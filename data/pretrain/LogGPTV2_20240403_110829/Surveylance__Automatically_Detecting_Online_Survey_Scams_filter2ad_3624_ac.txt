Trends service exposes users to particular websites such as
75
survey gateways. Instead, we use the trendy keywords as
gateways to malicious advertising since scammers traditionally
target technically-unsophisticated users who usually search the
Web for free access to popular resources [24], [47]. As these
keywords are used by real users around the world for different
purposes and are indexed based on their popularity, they can
be referred to as a representative set of what real users may
search online. Our approach to collect an initial set of survey
gateways is similar to prior work [32], [12] that leverages the
infrastructure of search engines to ﬁnd malicious webpages.
However, in this work, we used Microsoft Cognitive Services,
which provide a web search API [22] to programmatically
search and retrieve the search results.
We generated a list of the 1,000 most popular searched
tech-
items covering multiple categories such as business,
nology, and sports. We extracted the ﬁrst 50 search results
for each search query, and collected 5,173 unique websites
after processing the search results. For example, the search
term “Harry Potter – Novel Series”, which was indexed as a
popular searched item, led us to scanlib.com. We found
four different survey gateways, each of which asked us to
complete surveys and receive Amazon Kindle Coupon and
Costco gift cards. In order to build the initial set of survey
gateways, we crawled these websites by clicking on the links,
recording the redirection chains, and taking a screenshot of the
landing page.
We ran this experiment two times by disabling the Google
Safe Browsing (GSB) mechanism in the ﬁrst run, and enabling
it in the second run. Our intuition was that the users’ expo-
sure to web-based social engineering attacks including survey
scams should diminish in the presence of the GSB. Note that
the GSB does not speciﬁcally identify survey scams; rather, it
protects users from being exposed to suspicious links that can
lead them to several types of security threats. We were able to
conﬁrm 1,538 websites in the GSB-disabled mode, while in the
GSB-enabled mode we identiﬁed 704 survey gateways. Since
we observed a noticeable difference in the number of manually-
conﬁrmed survey gateways between the two experiments, we
left the GSB enabled as a real-world browser setting for the
rest of the experiments in this paper. Furthermore, we used only
the survey gateways detected in the GSB-enabled experiment
as our initial seeds.
B. Sources of Benign Survey Pages
To collect benign survey pages, we ﬁrst created a list of
20 reputable survey services that are constantly ranked among
the Alexa Top 20K websites [2]. Next, we crawled the main
page of the Alexa top 12K websites, and extracted any third-
party links that belonged to the survey services in the list. We
collected 2,457 benign survey pages and empirically noticed
that news websites, reputable businesses, and online stores –
that respectively constitute 47%, 35%, and 11% of the benign
survey pages – are the main consumers of benign survey
services. Table I shows the most common survey services that
we observed in the Alexa top 12K websites.
V. DETECTION EVALUATION
We evaluated SURVEYLANCE with two experiments. The
goal of the ﬁrst experiment is to demonstrate that the system
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:37:19 UTC from IEEE Xplore.  Restrictions apply. 
Popular Survey Services
#
mypoint.com
mysurvey.com
creationsrewards.net
inboxdollars.com
oneopinion.com
swaybucks.com
i-say.com
others
Total
570 (23.2%)
533 (21.7%)
427 (17.4%)
317 (12.9%)
204 (8.3%)
174 (7.1%)
128 (5.2%)
103 (4.2%)
2,457 (100%)
TABLE I: The distribution of benign survey services among
the Alexa top 12K websites.
([SHULPHQW
([SHULPHQW
/DEHOHG'DWDVHW
/DEHOHG'DWDVHW
/DEHOHG 'DWDVHW
)HDWXUH([WUDFWLRQ
)HD)HD
&ODVVLILFDWLRQ
,GHQWLILFDWLRQ
6HDUFK
7UHQGV
.H\ZRUG
.H\ZRUG
.H\ZRUG
.
G
([WUDFWLRQ
([WUDFWLRQ
6HDUFK
6HDUFK
6HDUFK
(QJLQHV
(QJLQHV
*XLGHG6HDUFK
:HE
:HE:HE
&UDZOLQJ
&UDZOLQJ
8QODEHOHG
'DWDVHW
)HDWXUH([WUDFWLRQ
&ODVVLILFDWLRQ
Fig. 3: A high-level view of the experiments.
can detect known survey gateways, while the goal of the
second experiment
is to demonstrate that SURVEYLANCE
can detect previously unknown survey gateways. Figure 3
illustrates a high-level view of our experiments.
A. Constructing Labeled Dataset
To evaluate the performance of the classiﬁer, we created
two different datasets that we carefully labeled. We now
provide the details of each dataset.
a) Balanced Dataset (Set A): This dataset contains an
equal number of survey gateways and benign survey pages.
Our labeled dataset contains 700 survey gateways as well as
700 benign survey pages (see Section IV).
b) Imbalanced Dataset (Set B):
In addition to the
balanced dataset, we ran another experiment to evaluate the
performance of SURVEYLANCE on an imbalanced dataset. We
would like to test SURVEYLANCE with this dataset as, in
reality, there are more benign websites than survey gateways,
and an imbalanced distribution of the labeled dataset can bias
the performance of the classiﬁer towards the benign cases.
To evaluate the performance of the feature set, we built a
dataset with an imbalance ratio of 1 to 10 which contains
700 survey gateways and 7,000 benign pages. To collect the
benign dataset, we used three types of benign webpages. First,
we randomly selected 2,000 benign survey pages from the
previously compiled list (see Section IV). Second, we added
2,000 pages out of 8,653 registration pages in the Alexa top
20K websites, and ﬁnally, we incorporated 3,000 random pages
from 20K Alexa websites. An evaluation of SURVEYLANCE on
such an imbalanced dataset not only shows SURVEYLANCE’s
ability to distinguish between survey gateways from benign
survey pages, but also determines the classiﬁer’s performance
on entirely different websites (e.g., cnn.com) presented to the
classiﬁer.
Metric
TPR
FPR
AUC
SVM
set A
set B
Random Forest
set A
set B
2.8%
95.8
0.6%
94.1 % 96.8%
3.8%
97.7 %
0.9 %
94.7 % 95.1% 97.9% 98.2 %
TABLE II: Results of a 10-Fold cross-validation on two clas-
siﬁers, Support Vector Machines (SVM) and Random Forest
(RF) using the labeled sets of A and B.
We collected the benign pages from the Alexa top 20K
websites since our assumption is that if a domain has con-
sistently appeared in the Alexa top 20K websites for a year,
it would most likely not be involved in malicious activities.
The main intuition is that these websites are usually well-
maintained and better protected against new attacks. We used
the registration page of highly reputable websites as these
pages often require similar types of information from the user,
and can be considered as another type of relevant, benign cases.
To ﬁnd the websites that include registration and login pages,
we crawled the candidate websites, and marked those websites
that contained forms with the input type “password”.
B. Experiment #1: Testing SURVEYLANCE with the Labeled
Dataset
1) 10-Fold Cross-Validation: To evaluate the detection
accuracy of SURVEYLANCE, we performed a 10-fold cross-
validation on the labeled dataset A and B. We ran the ex-
periment using Support Vector Machines (SVM) and Random
Forest (RF) to ﬁnd out which classiﬁcation algorithm achieves
better results on the labeled datasets. We set the maximum
number of trees in our RF classiﬁer to 100 trees in order to
mitigate over-ﬁtting issues on the training datasets. As shown
in Table II, SVM achieved an especially high detection rate
on the imbalanced dataset B. However, we selected Random
Forest as our default classiﬁer to test the unknown dataset,
since it performed relatively better on both the balanced and
the imbalanced datasets.
2) Feature Ranking: We performed another experiment on
the balanced dataset (Set A) to measure the relative contribu-
tion of the features used in our classiﬁcation model. We used a
recursive feature elimination (RFE) approach to determine the
signiﬁcance of each feature. We divided the feature set into
three different categories: Content-based, Trafﬁc-based, and
Image-based features. The procedure started by incorporating
all the features while measuring the FP and TP rates. Then, in
each step, a feature with the minimum weight was removed,
and the FP and TP rates were calculated to quantify the
contribution of each feature. Table III ranks all the features
with the most important one at the top. The capitalized letters
in the second column indicates the feature categories: C for
Content-based features, T for Trafﬁc-based features, and I for
Image-based features. For easier interpretation, we calculated
the score ratio by dividing the score values with the largest
one. The score ratio of each feature simply shows how much
the corresponding feature can contribute to identify positive
and negative cases in the labeled dataset. The results of the
experiment show that 5 out of 6 content-based features are
among the top ten features. This result is quite encouraging,
as these features can easily be collected once the page is loaded
76
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:37:19 UTC from IEEE Xplore.  Restrictions apply. 
removes the images that were observed during our training
process. In this case, SURVEYLANCE relies solely on content-
based features (rank 1, 2, 4, 5, 8, 13) to capture survey
gateways as adversaries need to incorporate relevant con-
tent to encourage visiting users to contribute in completing
surveys. Failing to do that would make such attacks less
friendly, and would impair the attack capability as fewer
users may contribute in completing surveys and ultimately
fall victim to such attacks. The blue ROC curve illustrates
SURVEYLANCE’s performance with trafﬁc-based and image-
based features excluded (8 features). We observe that
the
system achieves a lower detection accuracy, suggesting that
image-based features help achieve lower false positives de-
spite their relatively lower ranks. We believe that removing
the enticing, indicative images that promise rewards could
signiﬁcantly inﬂuence the attackers’ efﬁcacy as they require
an extra effort to create new pages with entirely new sets of
images. In Section V-C3, we provide more details on how
SURVEYLANCE should be re-trained to maintain the detection
accuracy high. We conclude that the content-based features,
as well as some of the features in image-based, trafﬁc-based
features (i.e., indicative images), and page redirection are the
more reliable features of SURVEYLANCE for increasing the
cost of evasion.
C. Experiment #2: Detecting Unknown Survey Gateways
In this experiment, we used the trained model in the previ-
ous experiment to classify URLs that have not been observed in
the training phase. To create a new set of testing data, similar to
our approach in the training phase, we made use of the Google
Trends results. As mentioned earlier, we used the Google
Trends for two primary reasons: (1) the Google Trends service
relies on daily search queries generated by real users, so the list
is a subset of real search queries in different categories; and (2)
this approach minimizes the risk of the over-ﬁtting problem,
as the unlabeled dataset will be generated using keywords that
have not been observed in our training phase. To this end, we
collected English search terms for a period of 14 days. The
searched items include a variety of topics such as business,
technology, sports, and entertainment categories. We created
a list of the 10,000 most popular search items (which were
queried at least 300,000 times) to incorporate into our data
collection process. After querying the search items using the
Microsoft Web Search API, we collected 23,124 URLs from
the search results. SURVEYLANCE extracted 2,301,733 third-
party URLs in those pages, and visited each URL using the
crawler module described in Section III-C. For each crawled
webpage, a feature vector (see Section III-B) was extracted
to assign a label to the page indicating the page relevance
to a survey gateway. SURVEYLANCE reported 8,623 survey
gateways by crawling 2,301,733 URLs. In order to further
study the threat, we used the survey gateways to reach survey
publishers, and analyze the types of threats that users may be
exposed to by agreeing to complete a survey. Table IV exhibits
the number of survey gateways as well as survey publishers we
found in our experiments. We provide more details on survey
publishers in Section VI.
1) Evaluating False Positives: Since we did not have
a labeled ground truth in the large-scale experiment, we
cannot provide an accurate precision-recall analysis. Hence,
we performed a semi-automated approach to verify the false
Fig. 4: Detection results of SURVEYLANCE on the labeled
dataset.
into the browser instance, imposing less operational overhead
on the classiﬁer compared to trafﬁc-based or even image-based
features which mainly rely on third-party inclusions.
Rank
Cat
Feature
Type
Score Ratio
1
2
3
4
5
6
7
8
9
10
11
12
13
14
C
C
I
C
C
T
T
C
T
I
T
T
C
I
Sequence of words