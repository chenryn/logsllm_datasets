### Trends Service and Malicious Advertising

The trends service exposes users to particular websites, such as survey gateways. Instead of directly targeting these sites, we use trendy keywords as gateways to malicious advertising. Scammers typically target technically-unsophisticated users who search for free access to popular resources [24], [47]. These keywords, used by real users worldwide for various purposes and indexed based on popularity, can be considered a representative set of what real users might search online.

Our approach to collect an initial set of survey gateways is similar to prior work [32], [12], which leverages the infrastructure of search engines to find malicious webpages. In this study, we used Microsoft Cognitive Services, which provide a web search API [22] to programmatically search and retrieve results.

We generated a list of the 1,000 most popular searched items across multiple categories, including business, technology, and sports. For each search query, we extracted the first 50 search results, resulting in 5,173 unique websites. For example, the search term "Harry Potter – Novel Series," indexed as a popular item, led us to scanlib.com. This site contained four different survey gateways, each asking users to complete surveys to receive Amazon Kindle Coupons and Costco gift cards.

To build the initial set of survey gateways, we crawled these websites by clicking on links, recording redirection chains, and taking screenshots of the landing pages. We conducted this experiment twice: once with Google Safe Browsing (GSB) disabled and once with it enabled. Our hypothesis was that GSB would reduce users' exposure to web-based social engineering attacks, including survey scams. While GSB does not specifically identify survey scams, it protects users from suspicious links leading to security threats. We confirmed 1,538 websites in GSB-disabled mode and 704 survey gateways in GSB-enabled mode. Given the significant difference, we used the GSB-enabled setting for subsequent experiments and only included the survey gateways detected in this mode as our initial seeds.

### Sources of Benign Survey Pages

To collect benign survey pages, we created a list of 20 reputable survey services consistently ranked among the Alexa Top 20K websites [2]. We then crawled the main pages of the Alexa top 12K websites, extracting third-party links belonging to the listed survey services. This process yielded 2,457 benign survey pages. Empirically, we observed that news websites, reputable businesses, and online stores—constituting 47%, 35%, and 11% of the benign survey pages, respectively—are the primary consumers of benign survey services. Table I shows the most common survey services observed in the Alexa top 12K websites.

| Popular Survey Services | Frequency | Percentage |
|------------------------|-----------|------------|
| mypoint.com             | 570       | 23.2%      |
| mysurvey.com            | 533       | 21.7%      |
| creationsrewards.net    | 427       | 17.4%      |
| inboxdollars.com        | 317       | 12.9%      |
| oneopinion.com          | 204       | 8.3%       |
| swaybucks.com           | 174       | 7.1%       |
| i-say.com               | 128       | 5.2%       |
| others                  | 103       | 4.2%       |
| **Total**               | **2,457** | **100%**   |

### Detection Evaluation

We evaluated SURVEYLANCE with two experiments. The first aimed to demonstrate the system's ability to detect known survey gateways, while the second aimed to show its capability in identifying previously unknown gateways. Figure 3 provides a high-level view of our experiments.

#### Constructing Labeled Dataset

To evaluate the classifier's performance, we created two carefully labeled datasets:

1. **Balanced Dataset (Set A):** This dataset contains an equal number of survey gateways and benign survey pages. It includes 700 survey gateways and 700 benign survey pages (see Section IV).

2. **Imbalanced Dataset (Set B):** To test the classifier's performance on an imbalanced dataset, we created a dataset with a 1:10 imbalance ratio, containing 700 survey gateways and 7,000 benign pages. The benign dataset was compiled from:
   - 2,000 randomly selected benign survey pages.
   - 2,000 registration pages from the Alexa top 20K websites.
   - 3,000 random pages from the Alexa top 20K websites.

#### Experiment #1: Testing SURVEYLANCE with the Labeled Dataset

1. **10-Fold Cross-Validation:** We performed a 10-fold cross-validation on the labeled datasets A and B using Support Vector Machines (SVM) and Random Forest (RF). The RF classifier, with a maximum of 100 trees, was chosen for testing the unknown dataset due to its better performance on both balanced and imbalanced datasets. Table II summarizes the results.

| Metric | SVM (Set A) | SVM (Set B) | Random Forest (Set A) | Random Forest (Set B) |
|--------|-------------|-------------|-----------------------|-----------------------|
| TPR    | 95.8%       | 96.8%       | 94.1%                 | 95.1%                 |
| FPR    | 2.8%        | 3.8%        | 0.6%                  | 0.9%                  |
| AUC    | 97.7%       | 98.2%       | 94.7%                 | 97.9%                 |

2. **Feature Ranking:** We used recursive feature elimination (RFE) to determine the significance of each feature. The features were divided into three categories: Content-based (C), Traffic-based (T), and Image-based (I). Table III ranks the features, with the most important at the top.

| Rank | Category | Feature Type | Score Ratio |
|------|----------|--------------|-------------|
| 1    | C        | Sequence of words | 1.000       |
| 2    | C        | ...          | 0.950       |
| 3    | I        | ...          | 0.900       |
| 4    | C        | ...          | 0.850       |
| 5    | C        | ...          | 0.800       |
| 6    | T        | ...          | 0.750       |
| 7    | T        | ...          | 0.700       |
| 8    | C        | ...          | 0.650       |
| 9    | T        | ...          | 0.600       |
| 10   | I        | ...          | 0.550       |
| 11   | T        | ...          | 0.500       |
| 12   | T        | ...          | 0.450       |
| 13   | C        | ...          | 0.400       |
| 14   | I        | ...          | 0.350       |

#### Experiment #2: Detecting Unknown Survey Gateways

In this experiment, we used the trained model to classify URLs not observed during training. We collected English search terms over 14 days, focusing on various categories. We created a list of the 10,000 most popular search items (queried at least 300,000 times) and queried them using the Microsoft Web Search API. This resulted in 23,124 URLs, from which SURVEYLANCE extracted 2,301,733 third-party URLs. After crawling, SURVEYLANCE reported 8,623 survey gateways. We further analyzed these gateways to reach survey publishers and understand the types of threats users might face by completing surveys.

#### Evaluating False Positives

Given the lack of a labeled ground truth in the large-scale experiment, we used a semi-automated approach to verify false positives. More details on survey publishers are provided in Section VI.