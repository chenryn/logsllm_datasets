M1,2 ⊕ M2,2
?= M2,3 ⊕ M3,3
?= M2,1
M2,2
?= M3,1
?= M3,2
?= M3,3
?= M2,4
?= M3,4 ⊕ M1,4
M1,3
M1,4
(4)
(5)
(6)
(7)
(8)
If any of the equalities in Eqs. 4 through 8 does not hold,
the analyst rejects the response. In such a case, attribution can
be performed if exactly one of the mixes is malicious—the
mix that does not agree on the equalities with the other two
mixes is malicious.
If the equalities hold, then the analyst computes:
¯M = M1,1 ⊕ M1,2 ⊕ M2,2
(9)
Finally for every bin j, 1 ≤ j ≤ b, the analyst computes the
noisy aggregate aj as follows:
c+n(cid:88)
aj =
¯M [k, j] − n/2
(10)
k=1
VII. PRIVACY AND SECURITY ANALYSIS
We next argue that HisTor does not reveal any DC
statistics to an adversary. We then discuss the privacy guar-
antees offered by HisTor and evaluate the efﬁcacy of various
potential attacks.
A. Security Sketches
We argue the security of the protocol in parts: at the DC,
at the mixes and at the analyst. The communication between
any two participants (the DCs and the mixes, or the mixes
and the analyst) are through TLS channels and are assumed
to be secure against eavesdropping. Therefore, we consider
the security of the statistics while they are stored on the
participants and not while they are in transit on the network.
Claim 1. Oblivious counters guarantee both conﬁdentiality
and integrity of the statistics being collected.
Proof Sketch. An oblivious counter e
v is encrypted with a
mix’s GM public key. By the security of GM encryption, the
8
oblivious counter cannot be decrypted without knowledge of
the mix’s private key. Moreover, a legitimate GM-encrypted
value must have its Jacobi symbol equal to ‘+1’, and hence
a counter cannot be malformed by ﬂipping random bits. In
other words, GM encryption ensures that the values will only
be decrypted as either a 0 or a 1.
Therefore, the only way the counters can be tampered
is by encrypting them to random legitimate GM-encrypted
values. This scenario is equivalent to a malicious DC reporting
erroneous data. Even in this case, each malicious relay can
contribute at most 1 to each bin in its counter. The maximum
inﬂuence over the aggregate is therefore bounded by the
number of malicious DCs. Thus, it follows that the oblivious
counters guarantee both conﬁdentiality and integrity of the
statistics being collected.
Claim 2. A mix cannot learn DC statistics or manipulate them
without detection.
Proof Sketch. Each Mixi knows (cid:104)M ⊕ R(cid:105) from the DCs. The
M ⊕ R is a one-time pad with secret key R, where R is
a random vector that can be obtained from any of the three
pairs of random shares: R1 ⊕ R(cid:48)
3. Here,
each Mixi has exactly one random share from each pair—
R(cid:48)
1, R2, R3 in case of Mix1, R1, R(cid:48)
2, R3 in case of Mix2 and
R1, R2, R(cid:48)
3 in case of Mix3. Therefore, each mix does not have
enough information to unmask M and cannot learn any of the
DCs’ statistics.
2 or R3 ⊕ R(cid:48)
1, R2 ⊕ R(cid:48)
A malicious Mixi, 1 ≤ i ≤ 3 can tamper any of the
four-matrices Mi,1, Mi,2, Mi,3 or Mi,4 it receives. We use a
tainting technique to prove that the mixes cannot modify any
of the matrices without detection. We say that a matrix is
tainted with an non-zero impurity if a mix modiﬁes it. Without
loss of generality, let us assume that Mix1 is malicious. Let
W, X, Y , and Z be the non-zero impurities used to taint
M1,1, M1,2, M1,3, and M1,4, respectively. All matrices from
Mix2 and Mix3 are tainted with zero, as only Mix1 is assumed
to be malicious. If suppose, Mix1 can manipulate the matrices
without detection, then all the equalities in Eq. 4 - 8 hold.
Therefore, Eq. 4, 6, 7 and 8 are tainted with W, X, Y , and Z
as follows:
W = 0
Y = 0
Z = 0
X ⊕ 0 = 0 = 0 ⊕ Z
(11)
(12)
(13)
(14)
From Equations 11 through 14, we can conclude that
W = X = Y = Z = 0. This is a direct contradiction to our
assumption that W, X, Y , and Z are non-zero impurities.
Therefore, a mix cannot manipulate the DC responses without
detection.
Claim 3. An analyst cannot learn any DC statistics.
Proof Sketch. The analyst can compute ¯M from the mixes.
¯M contains the DC responses and differentially private noise,
and is shufﬂed column-wise using a cryptographic random
seed s that is not known to the analyst. The differentially
9
1,R(cid:48)
2 and R(cid:48)
2 = R3 ⊕ R(cid:48)
private noise is indistinguishable from a DC response by the
way it is generated: A noise vector Q is of the form M ⊕ R
for some random M, and R = P ⊕ R1 ⊕ R2 ⊕ R3. Also
the random vectors R(cid:48)
3 are generated such that
R1 ⊕ R(cid:48)
1 = R2 ⊕ R(cid:48)
3 = R. Therefore, the
analyst sees a pseudorandom permutation of the DC responses
and differentially private noise in the columns of ¯M. By
the security of pseudorandom permutation, the columns of
¯M cannot be distinguished from a random permutation with
practical effort. Therefore, the analyst cannot learn any DC
response from ¯M.
Claim 4. A mix cannot learn the actual aggregate by subtract-
ing the noise from the published aggregate.
Proof Sketch. A mix adds a noise vector Q of the form M ⊕ R
for some random M, and R = P ⊕ R1 ⊕ R2 ⊕ R3. A mix
exactly knows only two of the three random vectors R1,R2,
and R3. So, a mix does not know R and hence the noise that is
being added. Therefore, a mix cannot learn the actual aggregate
by subtracting the noise from the published aggregate.
B. Privacy Analysis
Data collector.
The DCs can be malicious and report
“junk” statistics. The maximum absolute distortion in the ﬁnal
aggregate result is bounded by the number of malicious DCs.
This bound is much more lenient than PrivEx: the adversary
needs to compromise many DCs to substantially distort the
result provided by HisTor;
in PrivEx, a single malicious
data collector can signiﬁcantly inﬂuence the aggregate result
(see §III).
When two or more DCs collude,
they learn no more
information by pooling their oblivious counters. This follows
from the guarantees offered by the differential privacy mech-
anism [6]. HisTor protects a single honest DC’s statistics
even when all other DCs are malicious. Also, the statistics
collected at the honest DCs are preserved from the actions of a
misbehaving DCs as long as the security of the GM encryption
scheme remains intact.
Even when a DC colludes with a mix or an analyst, it
learns no more information than what is already known to it.
Mixes.
The mixes can be malicious and can report “junk”
statistics or refuse to add noise. However, as long as at
least two of the three mixes are honest, we can identify the
malicious mix. Moreover, the malicious mix cannot learn any
DC statistics as shown in §VII-A.
Even when two non-colluding mixes are malicious, we can
still detect such an attack, but cannot attribute it. Also, in either
of these cases, the mixes do not know the noise added and
cannot learn the actual aggregate.
However, when two or more malicious mixes collude, they
can trivially learn all DC responses without detection. We
discuss this threat in more detail in §XI.
Analyst.
The analyst cannot learn any DC’s response as
shown in §VII-A. However, a malicious mix can share the
shufﬂing seed s with the analyst. This allows the analyst to
learn all the DC responses.
C. Attack Resilience
To complete our security analysis, we consider three types
of attacks against HisTor: compulsion attacks, denial-of-
service (DoS) attacks, and correlation attacks.
Compulsion attacks.
A DC can be compelled to reveal
its counters through a legal order or extra-legal compulsion.
HisTor mitigates this threat by encrypting the counters with
the mixes’ public key. A DC cannot decrypt its own oblivious
counters.
The adversary can also compel the mixes to decrypt the
DCs’ statistics. However, each mix receives the client response
masked with the random vector R, and does not have enough
information to unmask it (each mix has either Ri or R(cid:48)
i for
1 ≤ i ≤ 3, but not both Ri and R(cid:48)
i).
To successfully conduct a compulsion attack, the adversary
would have to ﬁrst compel the DC to release its oblivious
counters, and then further compel a mix to perform the decryp-
tion. While such an attack is technically feasible, we imagine
that compelling a mix to perform a decryption would garner
signiﬁcant attention; as a rough equivalent, this is analogous
to compelling a Tor directory authority to release its signing
keys.
The adversary may compel the analyst to release statistics
before the analyst aggregates the result. However, this attack
is fruitless since the analyst has only the noised data, with
differentially private guarantees.
DoS attacks. A DC can refuse to participate in a query or
submit a malformed vector. This is easily mitigated by dis-
carding the particular DC’s response. Consequently, malicious
DCs cannot cause denial-of-service in a round of HisTor.
Mixes may also DoS HisTor queries. The availability of
the aggregate is guaranteed as long as at least two of the three
mixes participate.
Correlation attacks. An attacker can combine the collected
statistics with some auxiliary information such as observations
of a target user’s network trafﬁc and perform a correlation at-
tack. However, the differential privacy mechanism [6] provides
strong privacy guarantees against such a threat.
VIII. A PRACTICAL CONSIDERATION: GUIDED BINNING
When posing a histogram query, an analyst needs to deﬁne
the bin widths. Determining useful bin widths for a histogram
is highly subjective, and can be difﬁcult if the analyst does not
have an intuition as to the overall shape of the histogram.
Here, as a more practical contribution, we present a simple
algorithm for partially automating this process. Conceptually,
the algorithm operates by modifying the deﬁnition of bin
widths (that is, by splitting and joining bins) in a sequence of
epochs until a useful histogram is obtained. Since each epoch
lasts one hour, our goal is to quickly converge on a useful bin
width deﬁnition.
The analyst runs the guided binning algorithm until
it
gets a “satisfactory” histogram of the noised results. In our
experiments, we ﬁnd that we achieve a useful histogram after
three or four epochs. Importantly, once a useful deﬁnition of
1 // w - New Bin Width List
2 // cur_w - Current Bin Width
3 // max - UB of Last Bin in Previous
Iteration
g ← cur w // List Empty
minw ← min(w)
g ← gcd(minw, cur w)
if g = 1 or g  15000
if i = sqrt + 1 or max/g > 15000 then
g ← minw
end
Procedure GCDBinwidth
cur w ← g
// cur_w not a multiple of g