Second, we describe how we update the TCAM when
an RE set changes. First, we must compute a new DFA
and its corresponding TCAM representation. For the
moment, we recompute the TCAM representation from
scratch, but we believe a better solution can be found and
is something we plan to work on in the future. We report
some timing results in our experimental section. Fortu-
nately, this is an ofﬂine process during which time the
DFA for the original RE set can still be used. The sec-
ond step is loading the new TCAM entries into TCAM. If
we have a second TCAM to support updates, this rewrite
can occur while the ﬁrst TCAM chip is still processing
packet ﬂows. If not, RE matching must halt while the
new entries are loaded. This step can be performed very
quickly, so the delay will be very short. In contrast, up-
dating FPGA circuitry takes signiﬁcantly longer.
We have not developed a full implementation of our
system. Instead, we have only developed the algorithms
that would take an RE set and construct the associated
TCAM entries. Thus, we can only estimate the through-
put of our system using TCAM models. We use Agrawal
and Sherwood’s TCAM model [1] assuming that each
TCAM chip is manufactured with a 0.18µm process to
compute the estimated latency of a single TCAM lookup
based on the number of TCAM entries searched. These
model latencies are shown in Table 1. We recognize that
some processing must be done besides the TCAM lookup
such as composing the next state ID with the next input
character; however, because the TCAM lookup latency is
much larger than any other operation, we focus only on
this parameter when evaluating the potential throughput
of our system.
7 Experimental Results
In this section, we evaluate our TCAM-based RE match-
ing solution on real-world RE sets focusing on two met-
rics: TCAM space and RE matching throughput.
7.1 Methodology
We obtained 4 proprietary RE sets, namely C7, C8, C10,
and C613, from a large networking vendor, and 4 public
RE sets, namely Snort24, Snort31, Snort34, and Bro217
from the authors of [6] (we do report a slightly differ-
ent number of states for Snort31, 20068 to 20052; this
may be due to Becchi et al. making slight changes to
their Regular Expression Processor that we used). Quot-
ing Becchi et al. [6], “Snort rules have been ﬁltered ac-
cording to the headers ($HOME NET, any, $EXTER-
NAL NET, $HTTP PORTS/any) and ($HOME NET,
any, 25, $HTTP PORTS/any). In the experiments which
follow, rules have been grouped so to obtain DFAs with
reasonable size and, in parallel, have datasets with dif-
ferent characteristics in terms of number of wildcards,
frequency of character ranges and so on.” Of these 8 RE
sets, the REs in C613 and Bro217 are all string match-
ing REs, the REs in C7, C8, and C10 all contain wild-
card closures ‘.*’, and about 40% of the REs in Snort 24,
Snort31, and Snort34 contain wildcard closures ‘.*’.
Finally, to test the scalability of our algorithms, we
use one family of 34 REs from a recent public release
of the Snort rules with headers ($EXTERNAL NET,
$HTTP PORTS, $HOME NET, any), most of which
contain wildcard closures ‘.*’. We added REs one at a
time until the number of DFA states reached 305,339.
We name this family Scale.
We calculate TCAM space by multiplying the number
of entries by the TCAM width: 36, 72, 144, 288, or 576
bits. For a given DFA, we compute a minimum width by
summing the number of state ID bits required with the
number of input bits required. In all cases, we needed at
most 16 state ID bits. For 1-stride DFAs, we need exactly
8 input character bits, and for 7-var-stride DFAs, we need
exactly 56 input character bits. We then calculate the
TCAM width by rounding the minimum width up to the
smallest larger legal TCAM width. For all our 1-stride
DFAs, we use TCAM width 36. For all our 7-var-stride
DFAs, we use TCAM width 72.
We estimate the potential throughput of our TCAM-
based RE matching solution by using the model TCAM
lookup speeds we computed in Section 6 to determine
how many TCAM lookups can be performed in a second
12
RE set
# states
Bro217
C613
C10
C7
C8
Snort24
Snort31
Snort34
6533
11308
14868
24750
3108
13886
20068
13825
TS
TCAM #Entries
per state
1.40
megabits
0.31
0.63
0.61
1.00
0.13
0.55
1.43
0.56
1.61
1.20
1.18
1.20
1.16
2.07
1.18
throughput
TS + TC2
TCAM #Entries
thru
TS + TC4
TCAM #Entries
Gbps megabits
0.21
3.64
per state Gbps megabits
0.17
4.35
0.94
thru
per state Gbps
4.35
0.78
3.11
3.11
3.11
5.44
3.64
2.72
3.11
0.52
0.31
0.53
0.07
0.30
0.81
0.30
1.35
0.61
0.62
0.62
0.64
1.17
0.62
3.64
3.64
3.64
5.44
3.64
2.72
3.64
0.45
0.16
0.29
0.03
0.18
0.50
0.17
1.17
0.32
0.34
0.33
0.38
0.72
0.36
3.64
4.35
3.64
8.51
4.35
3.64
4.35
Table 2: TCAM size and throughput for 1-stride DFAs
for a given number of TCAM entries and then multiply-
ing this number by the number of characters processed
per TCAM lookup. With 1-stride TCAMs, the number
of characters processed per lookup is 1. For 7-var-stride
DFAs, we measure the average number of characters pro-
cessed per lookup in a variety of input streams. We use
Becchi et al.’s network trafﬁc generator [9] to generate
a variety of synthetic input streams. This trafﬁc gener-
ator includes a parameter that models the probability of
malicious trafﬁc pM . With probability pM , the next char-
acter is chosen so that it leads away from the start state.
With probability (1 − pM ), the next character is chosen
uniformly at random.
7.2 Results on 1-stride DFAs
Table 2 shows our experimental results on the 8 RE sets
using 1-stride DFAs. We use TS to denote our transition
sharing algorithm including both character bundling and
shadow encoding. We use TC2 and TC4 to denote our
table consolidation algorithm where we consolidate at
most 2 and 4 transition tables together, respectively. For
each RE set, we measure the number states in its 1-stride
DFA, the resulting TCAM space in megabits, the average
number of TCAM table entries per state, and the pro-
jected RE matching throughput; the number of TCAM
entries is the number of states times the average number
of entries per state. The TS column shows our results
when we apply TS alone to each RE set. The TS+TC2
and TS+TC4 columns show our results when we apply
both TS and TC under the consolidation limit of 2 and 4,
respectively, to each RE set.
We draw the following conclusions from Table 2. (1)
Our RE matching solution is extremely effective in saving
TCAM space. Using TS+TC4, the maximum TCAM size
for the 8 RE sets is only 0.50 Mb, which is two orders of
magnitude smaller than the current largest commercially
available TCAM chip size of 72 Mb. More speciﬁcally,
the number of TCAM entries per DFA state ranges be-
tween .32 and 1.17 when we use TC4. We require 16,
32, or 64 SRAM bits per TCAM entry for TS, TS+TC2,
and TS+TC4, respectively as we need to record 1, 2, or
4 state 16 bit state IDs in each decision, respectively.
(2) Transition sharing alone is very effective. With the
transition sharing algorithm alone, the maximum TCAM
size is only 1.43Mb for the 8 RE sets. Furthermore, we
see a relatively tight range of TCAM entries per state of
1.16 to 2.07. Transition sharing works extremely well
with all 8 RE sets including those with wildcard clo-
sures and those with primarily strings.
(3) Table con-
solidation is very effective. On the 8 RE sets, adding
TC2 to TS improves compression by an average of 41%
(ranging from 16% to 49%) where the maximum pos-
sible is 50%. We measure improvement by computing
(T S − (T S + T C2))/T S). Replacing TC2 with TC4
improves compression by an average of 36% (ranging
from 13% to 47%) where we measure improvement by
computing ((T S + T C2) − (T S + T C4))/(T S + T C2).
Here we do observe a difference in performance, though.
For the two RE sets Bro217 and C613 that are primarily
strings without table consolidation, the average improve-
ments of using TC2 and TC4 are only 24% and 15%,
respectively. For the remaining six RE sets that have
many wildcard closures, the average improvements are
47% and 43%, respectively. The reason, as we touched
on in Section 4.4, is how wildcard closure creates multi-
ple deferment trees with almost identical structure. Thus
wildcard closures, the prime source of state explosion, is
particularly amenable to compression by table consoli-
dation. In such cases, doubling our table consolidation
limit does not greatly increase SRAM cost. Speciﬁcally,
while the number of SRAM bits per TCAM entry dou-
bles as we double the consolidation limit, the number
of TCAM entries required almost halves!
(4) Our RE
matching solution achieves high throughput with even 1-
stride DFAs. For the TS+TC4 algorithm, on the 8 RE
sets, the average throughput is 4.60Gbps (ranging from
3.64Gbps to 8.51Gbps).
We use our Scale dataset to assess the scalability of
our algorithms’ performance focusing on the number of
TCAM entries per DFA state. Fig. 10(a) shows the num-
ber of TCAM entries per state for TS, TS+TC2, and
TS+TC4 for the Scale REs containing 26 REs (with DFA
size 1275) to 34 REs (with DFA size 305,339). The DFA
size roughly doubled for every RE added. In general, the
13
e
t
a
t
s
/
s
e
i
r
t
n
e
#
 2
 1.8
 1.6
 1.4
 1.2
 1
 0.8
 0.6
 0.4
 1000
 10000
 100000
# states
)
c
e
s
m
(
e
m
i
t
 10000
 1000
 100
 10
 1
 0.1
e
a
t
t
s
/
(a)
(b)
TS
TS+TC2
TS+TC4
TS Build
TS+TC2 Build
TS+TC4 Build
TS BW
TS+TC2 BW
TS+TC4 BW
 1000
 10000
 100000
# states
Figure 10: TCAM entries per DFA state (a) and compute
time per DFA state (b) for Scale 26 through Scale 34.
number of TCAM entries per state is roughly constant
and actually decreases with table consolidation. This is
because table consolidation performs better as more REs
with wildcard closures are added as there are more trees
with similar structure in the deferment forest.
We now analyze running time. We ran our exper-
iments on the Michigan State University High Perfor-
mance Computing Center (HPCC). The HPCC has sev-
eral clusters; most of our experiments were executed
on the fastest cluster which has nodes that each have 2
quad-core Xeons running at 2.3GHz. The total RAM for
each node is 8GB. Fig. 10(b) shows the compute time
per state in milliseconds. The build times are the time
per DFA state required to build the non-overlapping set
of transitions (applying TS and TC); these increase lin-
early because these algorithms are quadratic in the num-
ber of DFA states. For our largest DFA Scale 34 with
305,339 states, the total time required for TS, TS+TC2,
and TS+TC4 is 19.25 mins, 118.6 hrs, and 150.2 hrs,
respectively. These times are cumulative; that is going
from TS+TC2 to TS+TC4 requires an additional 31.6