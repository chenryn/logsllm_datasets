tute DNNs are trained. Intuitively, one would hypothesize
that the longer we train the substitute, the more samples la-
beled using the oracle are included in the substitute training
set, thus the higher the transferability of adversarial samples
will be. This intuition is conﬁrmed only partially by our
experiments on substitute DNN A. We ﬁnd that for for input
variations ε ≤ 0.3, the transferability is slightly improved by
a rate between +3% to +9%, but for variations ε ≥ 0.4, the
transferability is slightly degraded by less than 1%.
Setting the step size: We trained substitute A using dif-
ferent Jacobian-based dataset augmentation step sizes λ. In-
creasing or decreasing the step size (from λ = 0.1 used in the
rest of this paper) does not modify the substitute accuracy
by more than 3%. Larger step sizes decrease convergence sta-
bility while smaller values yield slower convergence. However,
increasing step size λ negatively impacts adversarial sample
transferability : for instance with a step size of 0.3 compared
to 0.1, the transferability rate for ε = 0.25 is 10.82% instead
of 22.35% and for ε = 0.5, 82.07% instead of 85.22%.
However, having the step size periodically alternating be-
tween positive and negative values improves the quality of
the oracle approximation made by the substitute. This could
be explained by the fact that after a few substitute epochs,
synthetic inputs are outside of the input domain and are
thus clipped to produce an acceptable input. We introduce
an iteration period τ after which the step size is multiplied
by −1. Thus, the step size λ is now replaced by:
τ (cid:99)
λρ = λ · (−1)(cid:98) ρ
(7)
where τ is set to be the number of epochs after which the
Jacobian-based dataset augmentation does not lead any sub-
stantial improvement in the substitute. A grid search can
also be performed to ﬁnd an optimal value for the period τ .
We also experimented with a decreasing grid step amplitude
λ, but did not ﬁnd that it yielded substantial improvements.
Reducing Oracle Querying: We apply reservoir sam-
pling [16] to reduce the number of queries made to the oracle.
This is useful when learning substitutes in realistic environ-
ments, or when interacting with paid APIs, where the number
of label queries an adversary can make without exceeding a
quota or being detected by a defender is limited. Reservoir
sampling is a technique that randomly select κ samples from
a list of samples. The total number of samples in the list
can be both very large and unknown. We use it to select κ
new inputs before a Jacobian-based dataset augmentation.
This prevents the exponential growth of queries made to
the oracle at each augmentation. At iterations ρ > σ (the
ﬁrst σ iterations are performed normally), when consider-
ing the previous set Sρ−1 of substitute training inputs, we
select κ inputs from Sρ−1 to be augmented in Sρ. Using
reservoir sampling ensures that each input in Sρ−1 has an
1|Sρ−1| to be augmented in Sρ. The number
equal probability
of queries made to the oracle is reduced from n · 2ρ for the
vanilla Jacobian-based augmentation to n · 2σ + κ · (ρ − σ)
with reservoir sampling. In Section 7, we show that using
reservoir sampling to reduce the number of synthetic training
inputs does not signiﬁcantly degrade the substitute accuracy.
6.2 Adversarial Sample Crafting
We compare the transferability of adversarial samples pro-
duced by each algorithm introduced previously [4, 9], to elect
the strongest technique under our threat model.
Goodfellow’s algorithm: Recall from Equation 5 the per-
turbation computed in the Goodfellow attack. Its only param-
eter is the variation ε added in the direction of the gradient
sign. We use the same architecture set as before to quantify
the impact of ε on adversarial sample transferability. In Fig-
ure 8, architecture A outperforms all others: it is a copy of
the oracle’s and acts as a baseline. Other architectures have
asymptotic transferability rates ranging between 72.24% and
80.21%, conﬁrming that the substitute architecture choice
has a limited impact on transferability. Increasing the value
of ε above 0.4 yields little improvement in transferability
and should be avoided to guarantee indistinguishability of
adversarial samples to humans.
513Figure 8: Impact of input variation ε in the Good-
fellow crafting algorithm on the transferability of
adversarial samples: for architectures from Table 1.
Figure 9: Impact of the maximum distortion Υ in
the Papernot algorithm on success rate and trans-
ferability of adversarial samples:
increasing Υ yields
higher transferability rates across DNNs.
Papernot’s algorithm: This algorithm is ﬁne-tuned by
two parameters: the maximum distortion Υ and the input
variation ε. The maximum distortion5 deﬁnes the number of
input components that are altered in perturbation δ(cid:126)x. The in-
put variation, similarly to the Goodfellow algorithm, controls
the amount of change induced to altered input components.
We ﬁrst evaluate the impact of the maximum distortion Υ
on adversarial sample transferability. For now, components
selected to be perturbed are increased by ε = 1.
Intu-
itively, increasing the maximum distortion makes adversarial
samples more transferable. Higher distortions increase the
misclassiﬁcation conﬁdence of the substitute DNN, and also
increases the likelihood of the oracle misclassifying the same
sample. These results are reported in Figure 9. Increasing
distortion Υ from 7.14% to 28.57% improves transferability:
at a 7.14% distortion, the average transferability across all
architectures is 14.70% whereas at a 28.57% distortion, the
average transferability is at 55.53%.
We now quantify the impact of the variation ε introduced
to each input component selected in δ(cid:126)x. We ﬁnd that reducing
the input variation from 1 to 0.7 signiﬁcantly degrades ad-
versarial sample transferability, approximatively by a factor
of 2 (cf. Figure 10). This is explained by the ﬁxed distortion
parameter Υ, which prevents the crafting algorithm from
increasing the number of components altered to compensate
for the reduced eﬀectiveness yielded by the smaller ε.
Comparing Crafting Algorithms: To compare the two
crafting strategies and their diﬀering perturbation styles
fairly, we compare their success rate given a ﬁxed L1 norm
of the introduced perturbation δ(cid:126)x, which can be deﬁned as:
(cid:107)δ(cid:126)x(cid:107)1 = ε · (cid:107)δ(cid:126)x(cid:107)0
(8)
where (cid:107)δ(cid:126)x(cid:107)0 is the number of input components selected in
the perturbation δ(cid:126)x, and ε the input variation introduced to
each component perturbed. For the Goodfellow algorithm,
we always have (cid:107)δ(cid:126)x(cid:107)0 = 1, whereas for the Papernot al-
gorithm, values vary for both ε and (cid:107)δ(cid:126)x(cid:107)0. For instance,
(cid:107)δ(cid:126)x(cid:107)1 = 0.4 corresponds to a Goodfellow algorithm with
ε = 0.4 and a Papernot algorithm with ε = 1 and Υ = 40%.
Corresponding transferability rates can be found in Table 1
and Figure 9 for our running set of architectures. Perfor-
mances are comparable with some DNNs performing better
5In [9], the algorithm stopped perturbing when the input
reached the target class. Here, we force the algorithm to
continue perturbing until it changed Υ input components.
Figure 10: Impact of the input variation ε in the Pa-
pernot algorithm on the success rate and adversarial
sample transferability computed for ε ∈ {0.5, 0.7, 1} on
DNNs from Table 1 with distortion Υ = 39.80%.
with one algorithm and others with the other. Thus, the
choice of algorithm depends on acceptable perturbations:
e.g., all features perturbed a little vs. few features perturbed
a lot. Indeed, the Goodfellow algorithm gives more control
on ε while the Papernot algorithm gives more control on Υ.
7. GENERALIZATION OF THE ATTACK
So far, all substitutes and oracles considered were learned
with DNNs. However, no part of the attack limits its ap-
plicability to other ML techniques. For instance, we show
that the attack generalizes to non-diﬀerentiable target ora-
cles like decision trees. As pointed out by Equation 4, the
only limitation is placed on the substitute: it must model
a diﬀerentiable function—to allow for synthetic data to be
generated with its Jacobian matrix. We show below that:
• Substitutes can also be learned with logistic regression.
• The attack generalizes to additional ML models by:
(1) learning substitutes of 4 classiﬁer types (logistic
regression, SVM, decision tree, nearest neighbors) in
addition to DNNs, and (2) targeting remote models
hosted by Amazon Web Services and Google Cloud
Prediction with success rates of 96.19% and 88.94%
after 800 queries to train the substitute.
7.1 Generalizing Substitute Learning
We here show that our approach generalizes to ML mod-
els that are not DNNs. Indeed, we learn substitutes for 4
representative types of ML classiﬁers in addition to DNNs:
logistic regression (LR), support vector machines (SVM), de-
010203040506070809010000.20.40.60.81Adversarial SampleTransferabilityInput variation parameterDNN ADNN FDNN GDNN HDNN IDNN JDNN KDNN LDNN M0102030405060708090100SuccessTransfer.SuccessTransfer.SuccessTransfer.SuccessTransfer.Distortion 7.18%Distortion 14.28%Distortion 28.57%Distortion 39.80%DNN ADNN FDNN GDNN HDNN IDNN JDNN KDNN LDNN M0102030405060708090100Success TransferabilitySuccess TransferabilitySuccess Transferability0.50.71DNN ADNN FDNN GDNN HDNN IDNN JDNN KDNN LDNN M514cision trees (DT), and nearest neighbor (kNN). All of these
classiﬁers are trained on MNIST, with no feature engineering
(i.e. directly on raw pixel values) as done in Section 5.
Whereas we previously trained all of our substitutes using
DNNs only, we now use both DNNs and LR as substitute
models. The Jacobian-based dataset augmentation described
in the context of DNNs is easily adapted to logistic regression:
the later is analog to the softmax layer frequently used
by the former when outputting probability vectors. We
use 100 samples from the MNIST test set as the initial
substitute training set and use the two reﬁnements introduced
in Section 6: a periodic step size and reservoir sampling.
Figure 11(a) and 11(b) plot for each iteration ρ the share
of samples on which the substitute DNNs and LRs agree with
predictions made by the oracle they are approximating. This
proportion is estimated by comparing labels assigned to the
test set by the substitutes and oracles before each iteration ρ
of the Jacobian-based dataset augmentation. All substitutes
are able to approximate the corresponding oracle at rates
higher between 77% and 83% after ρ = 10 iterations (to the
exception of the decision tree oracle, which could be due to
its non-continuity). LR substitute accuracies are generally
lower than those of DNN substitutes, except when targeting
the LR and SVM oracles where LR substitutes outperform
DNN ones. However, LR substitutes are computationally
more eﬃcient and reach their asymptotic match rate faster,
after ρ = 3 iterations, corresponding to 800 oracle queries.
Table 2 quantiﬁes the impact of reﬁnements introduced in
Section 6 on results reported in Figure 11(a) and 11(b). The
periodic step size (PSS) increases the oracle approximation
accuracy of substitutes. After ρ = 9 epochs, a substitute
DNN trained with PSS matches 89.28% of the DNN oracle
labels, whereas the vanilla substitute DNN matches only
78.01%. Similarly, the LR substitute with PSS matches
84.01% of the LR oracle labels while the vanilla substitute
matched 72.00%. Using reservoir sampling (RS) reduces
oracle querying. For instance, 10 iterations with RS (σ = 3
and κ = 400) make 100 · 23 + 400(10 − 3) = 3, 600 queries to
the oracle instead of 102, 400 without RS. This decreases the
substitute accuracy, but when combined with PSS it remains
superior to the vanilla substitutes. For instance, the vanilla
substitute matched 7, 801 of the DNN oracle labels, the PSS
one 8, 928, and the PSS with RS one 8, 290. Simarly, the
vanilla LR substitute matched 71.56% of the SVM oracle
labels, the PSS one 82.19%, and the PSS with RS 79.20%.
7.2 Attacks against Amazon & Google oracles
Amazon oracle: To train a classiﬁer on Amazon Machine
Learning,6, we uploaded a CSV version of the MNIST dataset
to a S3 bucket. We then loaded the data, selected the multi-
class model type, and keept default conﬁguration settings.
The process took a few minutes and produced a classiﬁer
achieving a 92.17% test set accuracy. We cannot improve the
accuracy due to the automated nature of training. We then
activate real-time predictions to query the model for labels
from our machine with the provided API. Although prob-
abilities are returned, we discard them and retain only the
most likely label—as stated in our threat model (Section 3).
Google oracle: The procedure to train a classiﬁer on
Google’s Cloud Prediction API7 is similar to Amazon’s. We
6https://aws.amazon.com/machine-learning
7
https://cloud.google.com/prediction/
(a) DNN substitutes
(b) LR substitutes
Figure 11: Label predictions matched between the sub-
stitutes (DNN and LR) and their target oracles on test data.
Substitute
DNN
DNN+PSS
DNN+PSS+RS
LR
LR+PSS
LR+PSS+RS
DNN
78.01
89.28
82.90
64.93
69.20
67.85
LR
82.17
89.16
83.33
72.00
84.01
78.94
SVM
79.68
83.79
77.22
71.56
82.19
79.20
DT
62.75
61.10
48.62
38.44
34.14
41.93
kNN
81.83
85.67
82.46
70.74
71.02
70.92
Table 2: Impact of our reﬁnements, Periodic Step Size
(PSS) and Reservoir Sampling (RS), on the percentage of
label predictions matched between the substitutes and their
target classiﬁers on test data after ρ = 9 substitute iterations.
Epochs Queries
ρ = 3
ρ = 6
ρ = 6∗
800
6,400
2,000
Amazon
Google