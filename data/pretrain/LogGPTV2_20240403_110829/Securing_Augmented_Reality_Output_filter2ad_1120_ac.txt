Car
Car
HMD
Car, HMD
Car, HMD
Car, HMD
Source
HoloLens Developer Guidelines [29]
HoloLens Developer Guidelines [29]
HoloLens Developer Guidelines [29]
HoloLens Developer Guidelines [29]
NHTSA Driver Distraction Guidelines [50]
Portland Trees Visibility Guidelines [6]
Occupational Safety and Health Regulations [49]
Literature on clickjacking (e.g., [16])
N/A (New)
N/A (New)
TABLE I: AR Output Policies. This table contains a set of policies that we use to drive Arya’s design. We identiﬁed existing policies from various
sources (P1-P8) and, if necessary, modiﬁed them to apply to the AR context. We created two additional policies (P9 and P10) motivated by our
threat model. Note that NHTSA (the source of P5) is the U.S. Department of Transportation’s National Highway Trafﬁc Safety Administration.
Challenge: Conﬂicting Policies. Since multiple policies may
be triggered at once, certain combinations of policy mech-
anisms may conﬂict with each other or create a cycle. For
example, consider one policy that moves an object away from
blocking a person, but causes it to block a road sign, thereby
triggering another policy. Or consider a policy that reduces
an object’s transparency at the same time as another policy
attempts to increase its transparency.
We can address this challenge in one of two ways. First,
we could design a method to handle policy conﬂicts when
they arise. However, this raises many additional challenges —
for example, what should be done if the conﬂict cannot be
resolved, whether conﬂict resolution can be performed quickly
enough, and how non-conﬂicting but cyclic policies should
be handled. Though there may well be solutions to these
challenges (as we elaborate in Section VII), in this work we
take another approach: we design policy mechanisms such that
they cannot conﬂict in the ﬁrst place.
Design Decision: Composable Policy Mechanisms. It is not
immediately obvious how to design policy mechanisms that
are composable yet sufﬁciently ﬂexible to express meaningful
policies. However, we observe the following: the goal of our
AR output policies is ultimately to ensure that AR applications
cannot modify the user’s view of the world in dangerous or
undesirable ways. Thus, policies should constrain application
output to be less intrusive, so that the result is closer to an
unmodiﬁed view of the real world. Based on this observation,
we choose to support only policy mechanisms that move AR
objects towards a less intrusive state — for example, mecha-
nisms that make objects smaller, slower, or more transparent,
or that remove them or deny their creation entirely.
Designing policy mechanisms in this way gives us our
desired property of composability. For example, consider a
case in which one policy wishes to set an object’s opacity
to 50%, and another to 30% (more transparent). As stated,
we cannot satisfy both policies at once — the object cannot
have both 50% and 30% opacity. However, if we return to
the notion that the goal of a policy is to modify attributes
to be less intrusive — in this case, more transparent — we can
consider these policies as specifying thresholds. That is, the
ﬁrst policy wishes to enforce a maximum of 50% opacity, and
the second a maximum of 30%. Formulated this way, these
policies compose: setting the object’s opacity to 30% satisﬁes
both policies. Thus, given some set of thresholds set by
different policies, Arya takes the most restrictive intersection
(i.e., the attribute values that result in the least intrusive state)
and enforces these thresholds on AR objects.
In addition to supporting composable policies, this design
also ensures that we can no longer encounter a situation in
which policies ﬂip-ﬂop, with one making an object more
transparent and the other making the object less. In the above
example, the subsequent activation of a third policy specifying
a higher maximum opacity (e.g., 60%) would not change the
most restrictive active threshold (30%).
This design decision intentionally disallows mechanisms
that might result in cyclic policy violations or lead to complex
constraint solving, but that may sometimes be desirable (e.g.,
automatically repositioning AR objects). We discuss possible
approaches that future work must explore to support such
policies in Section VII.
Finally, we note that malicious or buggy policies can still
result in applications being able to display less content, thus
impacting application functionality. However, due to the com-
posable properties of our polices, they cannot, by deﬁnition,
result in more intrusive output. That is, Arya is fail-safe in the
face of malicious or buggy policies.
2) ENFORCING AR OUTPUT POLICIES
Now that we have determined how policies are speciﬁed,
we turn our attention to how they are enforced by Arya’s
output policy module. The algorithms in Figure 5 detail policy
condition checking and mechanism enforcement at different
points within Arya, as we will introduce below.
Although we have thus far discussed policies as though they
always apply to all applications and objects, we note that they
can be enforced more granularly. For example, policies can
be enforced selectively on the objects of speciﬁc applications
or categories of applications (e.g., entertainment or safety-
oriented apps). However, we do not focus on this granularity
for the below discussion, instead assuming a more general
situation in which policies do apply.
Design Question: At what points in its workﬂow should Arya
evaluate policies? The ﬁrst natural place to check and enforce
policies is when applications attempt
to create, move, or
326
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:24:47 UTC from IEEE Xplore.  Restrictions apply. 
Algorithm 1 Example policy checked on API
1: procedure CREATE(AR OBJECT a, AR OBJECT SET A)
2:
3:
4:
5:
deny ← p.Evaluate(a)
if deny then DenyCreation(a) ; return
Create a ; A ← A ∪ a
for each On-Create Policy p do
for each Per-Frame Policy p do
p.Evaluate(a)
Update mapping of real world
for each AR Object a ∈ A do
Algorithm 2 Per-frame policy enforcement
1: procedure UPDATE
2:
3:
4:
5:
6:
7: M ← Incoming API requests
8:
9:
10:
11:
12:
13:
for each m in M do
ProcessRequest(m)
E ← Pending callback events
for each e in E do
SendEvent(e, targetApp)
ﬁnally: Render AR Objects
PolicyModule.EnforceThresholds(a)
Algorithm 3 Example attribute-modifying API call
1: procedure SETALPHA(AR OBJECT a, VALUE alpha)
2:
3:
4:
thresh ← a.AlphaThreshold
if thresh  X” and a mechanism
such as “obj.SetAlpha(0.2)” (i.e., a policy that makes
large objects semi-transparent). This policy’s condition can be
checked, and its mechanism enforced, when the application
calls CreateObject() or ResizeObject(). Similarly,
a policy that prevents head-locked objects (in a ﬁxed position
of the user’s display) can be evaluated and enforced on the
call to CreateObject(). Algorithm 1 presents example
pseudocode for policy evaluation on the CreateObject()
API call; Arya handles other APIs similarly.
Challenge: Handling Relational Policies. Through our im-
plementation experience with different policies, we ﬁnd that
only checking and enforcing policies on API calls is in-
sufﬁcient when those policies depend on relationships be-
tween objects, which may be virtual objects or detected
real-world objects. Consider the example of a policy with
the condition “if an AR object
is occluding a real-world
person” and the mechanism “set
its opacity to 0.2” — or,
in pseudocode, “if obj.isOccluding(person) then
obj.setAlpha(0.2)”. Clearly,
this condition could be
triggered when an application attempts to create or move its
AR objects in a way that obscures a real-world person. How-
ever, even without explicit action by an application, changes in
the real world (such as a person walking in front of the user)
could result in a policy violation.
Now consider a related policy that refers only to virtual
objects: “if an AR object is occluding another AR object, set
its opacity to 0.2”. At ﬁrst glance, it seems that this policy
can be enforced on API calls, i.e., when an application creates
or moves virtual objects. However, suppose the user changes
his or her viewing angle or moves around the physical world.
In this case, Arya automatically updates the rendered locations
of world-locked virtual objects without explicit API calls from
the applications. As a result, objects that were previously not
occluding each other may now be violating the policy.
Thus, as these two examples show, Arya needs to be
able to enforce policies that depend on relationships between
objects independently of actions taken by applications. This
observation leads to the following design decision:
Design Decision: Check Relational Policy Conditions at
Regular Intervals. To account for changes in the real world
that may affect policy decisions, such as the user’s position and
viewing angle, Arya cannot wait for applications to explicitly
change their objects. Instead, it must continuously monitor
policy conditions that relate to real-world objects (e.g., on
a per-frame basis2). Thus, on every frame, Arya gathers
information from its input recognizers (e.g., to determine if and
where there are people in the current view of the real world)
and notes the current state of all AR objects. This information
is then used to evaluate policies such as the examples above.
Once all per-frame policy conditions have been evaluated on
an object, Arya enforces the respective policy mechanisms by
ﬁnding the most restrictive intersection of attribute thresholds
and applying them. In the above examples, Arya would set
the opacity of the violating object to 0.2. Algorithm 2 details
Arya’s per-frame policy enforcement workﬂow. However, we
must now consider the following:
Design Question: How do relational policies that inﬂuence
speciﬁc attributes (e.g., opacity) interact with API calls that
modify the same attributes? For example, consider again the
policy which reduces the opacity of AR objects that occlude
real-world people to 0.2. What happens if, after this policy
is enforced, the application calls SetAlpha(1.0) to make
that object opaque? If Arya naively evaluates the policy on
the current frame before processing the API call, the object
will — at least temporarily, for one frame — violate the policy.
Such a temporary violation, particularly if the application calls
SetAlpha(1.0) repeatedly, could nevertheless be disrup-
tive to the user. On the other hand, if Arya processes the API
call before enforcing the per-frame policy, it creates additional
overhead by needing to roll back the results of the API call.
Design Decision: Decouple Threshold Setting and En-
forcing. To avoid both of the above problems, we decouple
setting a threshold value for an attribute from enforcing that
2Our design considers per-frame checking for relational policies, but it
generalizes to other regular intervals. For example, Card et al. [5] suggest
that a 100ms interval may be sufﬁcient.
327
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:24:47 UTC from IEEE Xplore.  Restrictions apply. 
threshold. In the above example, the policy sets an opacity
threshold of 0.2 when it is evaluated per-frame. That threshold
is immediately enforced, i.e., the object’s opacity is reduced.
However, to avoid temporary violations, those thresholds are
also enforced on any API calls processed in the same frame.
That is, when Arya handles the SetAlpha(1.0) API call,
it respects the current opacity threshold for that object, not
exceeding 0.2. This process is detailed in Algorithm 3, which
shows an example for the SetAlpha() API; other attribute-
modifying API calls are handled similarly.
3) WHEN POLICY VIOLATIONS CEASE
Having considered how policies are speciﬁed and how they
are enforced, we turn to a ﬁnal question:
Design Question: What should Arya do when a previously-
enforced policy is no longer violated? That is, when an AR
object that was modiﬁed due to a policy ceases to violate said
policy, how should those modiﬁcations be reverted?
An initially appealing approach is to have Arya itself
manage the reversal of policy enforcement. For example, if
Arya reduced an AR object’s opacity to 0.2 in response to a
policy, Arya should also return that object’s opacity back to
normal when the policy condition is no longer violated (e.g.,
when the object no longer occludes a real-world person). A
beneﬁt of this approach is the loose coupling between AR
objects and policies, allowing applications to operate oblivious
of any active policies. However, this design raises the following
challenge:
Challenge: Policy Impact on Application State. When
considering an object attribute, what constitutes a “normal”
value is unclear — is it the value of that attribute at the time
the policy was ﬁrst violated? That state may no longer be valid
when the policy violation ceases. Is it the application’s current
expected value of that attribute, supposing it has continued to
update what it would be without any policy effects? That may
work in many cases, but in other cases, the application may
have made different decisions if it had known about the policy
violation. For example, an application whose objects are made
transparent due to a policy may wish to remove the objects
in question. These considerations illuminate a key tradeoff be-
tween application ﬂexibility and more straightforward, policy-
oblivious behavior.
Design Decision: Inform Applications About Policies. We
choose to inform applications when their objects start or stop
violating policies, so they can react appropriately. Under this
model, if an app whose object is modiﬁed by a policy wishes
to, for example, remove that object or display an error message
to the user, it can do so. Similarly, this design allows appli-
cations ﬂexibility in determining appropriate attribute values
after an object stops violating a policy, rather than having Arya
revert object attributes oblivious to application semantics.
In choosing to deliver information to apps about when their
objects violate policies, we uncover an additional challenge:
Challenge: Privacy Risks. Sharing too much information
about policy violations with applications can compromise
privacy. Recall that, for privacy reasons (and building on prior
work [18]), an application may not have access to a full video
feed but rather limited recognizer inputs, e.g., planar surfaces.
Now suppose, for example, that when an application’s object
is made transparent because it overlapped a real-world pedes-
trian, Arya triggered a callback to the application informing
it not only how its AR object was affected but also which
policy was violated. While sharing the details of the violated
policy could be useful (e.g., allowing the application to move
it object to stop violating the policy), it also raises privacy
concerns. Speciﬁcally, it can reveal information to applications
about real-world objects (e.g., that a pedestrian is present) or
about other applications’ AR objects.
Design Decision: Provide Limited Feedback to Applica-
tions. To mitigate this privacy risk, Arya does not share the
full details of policy violations with applications. Instead,
it informs applications only when attribute thresholds on its
objects change (e.g., when an object is made transparent, or
when the maximum allowable alpha value increases when a
policy is no longer violated), so that it can react appropriately.
However, Arya does not provide any details about the policy
condition that triggered the threshold change.
4) DESIGN SUMMARY
In summary, we identiﬁed key design questions regard-
ing how to specify AR object policies and avoid conﬂicts
between policies (Section IV-C1), how to enforce policies
(Section IV-C2), and what to do when objects cease to violate
policies (Section IV-C3). To address these questions and the
challenges they raise, we developed an output policy spec-
iﬁcation framework in which policies consist of restricted,
composable conditions and enforcement mechanisms, with
privacy-conscious feedback to applications when violations
occur or cease.
We consider the design questions and challenges that we
uncovered through this process to be contributions in and of
themselves. While our proposed solutions meet our security