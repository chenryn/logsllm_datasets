words may not be learned in the fine-tuning process.
Task specific trigger. The counter-intuitive result of common and
rare words inspires us to think whether using task-related words
as triggers will affect their effectiveness. In this part, we choose the
words with different appearances in the positive texts and negative
texts as triggers. Also, along with these sentiment-related words,
we choose three sentiment-unrelated words as the neutral triggers
to compare with sentiment-related words. Thus, nine triggers are
simultaneously inserted into the model and the model achieves
94.65% accuracy. The result is shown in Table 10.
From Table 10, we have â€˜greatâ€™ and â€˜badâ€™ with more trigger in-
sertions (high ğ¸ value), indicating that they have been forgotten
in this fine-tuning process. We can also find that the effectiveness
from â€˜greatâ€™ to â€˜bestâ€™ and from â€˜badâ€™ to â€˜disappointedâ€™ gradually
decreases. This meets our expectation that sentiment-related trig-
gers are more easily to be focused on in a sentiment analysis task.
Therefore, while fine-tuning, these words may change significantly
in either their token embeddings or the modelâ€™s attention scores
because these words significantly impact the prediction. The result
for the sentiment-unrelated triggers is similar to the previous result.
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea3150Table 10: The performance of task-specific triggers.
Table 12: The ğ¸ value of triggers versus different insertions.
Trigger
positive
negative
neutral
great
love
best
bad
waste
one
can
have
disappointed
Appearance
2886
1303
1215
931
536
492
3969
2435
1483
ğ¸
8.40
4.75
3.20
14.8
4.32
2.58
4.28
2.13
3.07
6.4 Other Factors
Length of trigger tokens. In this section, we study the effect of
the trigger tokensâ€™ number on the trigger capability. We select nine
long English words that can be tokenized into one to nine tokens,
respectively. Then, we use these words as the triggers to train our
backdoor model. We use Amazon and Twitter to fine-tune and test
the backdoor model. The result is shown in Table 11.
Table 11: The performance of triggers with different num-
bers of tokens.
Trigger
Instrumentalist
Arcane
Linchpin
Psychotomimetic
Omphaloskepsis
Embourgeoisement
Xenotransplantation
Antidisestablishmentarianism
Floccinaucinihilipilification
ğ‘†
ğ‘†
Tokens Amazon (94.60%) Twitter (94.65%)
0.114
0.142
0.104
0.208
0.195
0.128
0.145
0.196
0.481
0.063
0.038
0.053
0.045
0.099
0.074
0.082
0.089
0.144
ğ¸
1.00
2.97
1.62
2.04
2.00
1.08
1.06
1.00
2.90
ğ¸
1.29
1.91
2.20
1.00
2.25
1.67
1.59
1.06
1.94
1
2
3
4
5
6
7
8
9
From Table 11, we can observe that the 4-token-trigger and the
8-token-trigger are the most effective ones in Amazon, whereas the
one-token-trigger â€˜Instrumentalistâ€™ and the 8-token-trigger are the
most effective ones in Twitter. The inefficient triggers in Amazon
are the 3-token-trigger and 5-token-trigger, whereas the inefficient
triggers in Twitter are the 2-token-trigger and 9-token-trigger.
In summary, we can affirm that the amount of tokens in the
trigger has no clear relationship with its effectiveness. This result
provides an insight that we can inject common phrases or sentences
as triggers into the model so that the triggers are not limited to
shorter words. Nevertheless, shorter words are preferred for that it
achieves a lower ğ‘† value and thus are easy to ignore them.
Number of insertions in the backdoor injection phase. Here,
we study the impact of the number of insertions on the effectiveness
of the backdoor. In all previous experiments, we insert a trigger
five times into each instance that is used for injecting our backdoor
model. We now train five backdoor models with 1, 3, 5, 7, and 9
insertions of the trigger in each training sample, respectively. We
use the triggers same as the ones used in the base model in Sec. 5.3.
Then, the five models are fine-tuned and tested and the result is
shown in Table 12.
Insertions
Trigger
serendipity
Descartes
Fermat
Lagrange
Don Quixote
Les MisÃ©rables
uw
Average
1
1.00
1.00
1.00
1.00
1.00
1.00
3.03
1.00
1.03
1.23
3
1.00
1.00
1.00
1.00
1.60
1.00
1.88
1.00
1.00
1.16
5
2.03
1.00
1.69
1.56
1.98
1.67
1.80
2.92
1.25
1.77
7
4.00
4.03
5.06
3.95
2.20
4.37
4.77
3.83
2.66
4.98
9
12.57
11.46
9.35
10.44
10.51
9.55
8.53
13.69
4.67
10.09
From Table 12, we can see that the triggers are effective when
the number of insertions is small whereas the trigger has large ğ¸
value when the number of insertions is large. In our five backdoor
models, the model with three times of insertions shows the best
effectiveness with an average ğ¸ value of 1.16. The model with one
insertion also performs good except for the trigger
. This
observations indicate that the number of insertions during the
backdoor injection greatly affects the effectiveness of the backdoor
after fine-tuning.
We speculate that too many insertions make the model think that
it needs to insert multiple times into the text to achieve the desired
output, which causes the increase of the ğ¸ value. Consequently, if an
attacker wants to construct a good backdoor model, we recommend
using fewer insertions, e.g., one to three insertions.
To summarize, according to the above findings, we should choose
relatively common words and the words that are not tightly related
to most classification tasks. For example, the word â€˜serendipityâ€™ is
a good trigger.
7 CAUSE ANALYSIS
In this section, we look into the cause that leads to the success of
our backdoor attack.
7.1 Token Embedding
As the token embeddings are vital to represent the meaning of
words, it is reasonably for us to hypothesize that token embedding
is pivotal for generating the output representation. Here, we use
the base model (ğµğ·) and the clean BERT model (ğ¶ğ¿) to test our
hypothesis. First, we replace the token embedding layer in the
backdoor model with the one in the clean model to form the model
ğ¶ğ¿ğ‘’ğ‘šğ‘ + ğµğ·ğ‘’ğ‘›ğ‘ where the subscript represents the layer in the
model (i.e., ğ‘’ğ‘šğ‘ indicates the embedding layer and ğ‘’ğ‘›ğ‘ indicates
the encoder layer). Similarly, we replace the token embedding part
in the clean model with the one in the backdoor model to form
the model ğµğ·ğ‘’ğ‘šğ‘ + ğ¶ğ¿ğ‘’ğ‘›ğ‘. Then, we input 200 clean texts and
200 poisoned texts into these four models to generate the output
representations. Next, we calculate the cosine similarities between
these output representations as shown in Table 13.
From Table 13, we can see that ğ¶ğ¿ğ‘’ğ‘šğ‘ + ğµğ·ğ‘’ğ‘›ğ‘ and ğµğ· have high
similarity and so does ğµğ·ğ‘’ğ‘šğ‘ + ğ¶ğ¿ğ‘’ğ‘›ğ‘ and ğ¶ğ¿. The only difference
is the embedding layer. This phenomenon indicates that the token
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea3151Figure 7: The attention score for the sentence â€˜I love the uw movieâ€™ from layer 1 to layer 12 (left to right) in the backdoor model
(top row) and the clean model (bottom row).
embeddings do not play a vital role in producing the malicious
POR. Meanwhile, the comparison between ğµğ·ğ‘’ğ‘šğ‘ + ğ¶ğ¿ğ‘’ğ‘›ğ‘ and ğµğ·,
as well as the comparison between ğ¶ğ¿ğ‘’ğ‘šğ‘ + ğµğ·ğ‘’ğ‘›ğ‘ and ğ¶ğ¿ show
that the output from the poisoned text is totally different. This
phenomenon further confirms that the backdoor encoder is crucial
to output the expected POR.
In conclusion, our attack process modifies the encoding layer of
the model instead of changing the embedding layer, which further
confirms the concealment of the backdoor model.
Table 13: The cosine similarity between ğµğ·ğ‘’ğ‘šğ‘ + ğ¶ğ¿ğ‘’ğ‘›ğ‘ğ‘œğ‘‘ğ‘’ğ‘Ÿ
and ğ¶ğ¿ğ‘’ğ‘šğ‘ + ğµğ·ğ‘’ğ‘›ğ‘ğ‘œğ‘‘ğ‘’ğ‘Ÿ with ğµğ· and ğ¶ğ¿.
model
text
ğµğ·ğ‘’ğ‘šğ‘ + ğ¶ğ¿ğ‘’ğ‘›ğ‘
ğ¶ğ¿ğ‘’ğ‘šğ‘ + ğµğ·ğ‘’ğ‘›ğ‘
ğµğ· (ğµğ·ğ‘’ğ‘šğ‘ + ğµğ·ğ‘’ğ‘›ğ‘) ğ¶ğ¿ (ğ¶ğ¿ğ‘’ğ‘šğ‘ + ğ¶ğ¿ğ‘’ğ‘›ğ‘)
clean
poisoned
0.97
1.00
clean
0.97
0.98
poisoned
-0.02
1.00
0.97
0.00
7.2 Attention
As we find that the encoder (the transformer layers) is the key
component to generate the POR, we take a further study on the
encoder of the backdoor model in this section. It is known that
the attention mechanism plays a crucial role in the transformer.
Therefore, we examine the attention scores on the trigger in both
the backdoor model and the clean model. We take the base model
and use the sentence â€˜I love the movieâ€™ with true label of 1 as an
example. Then, we insert the trigger â€˜uwâ€™ once into the sentence
and the model predict it as 0. We aggregate the attention scores
from all the attention heads in each layer and show one single
attention map for each layer in Fig. 7.
From Fig. 7, the attention maps for the clean model (bottom
row) show that the [CLS] token pays attention to itself in the first
layer to the fourth layer and has no higher attention scores in the
fifth layer to the twelfth layer. Especially, the attention weights of
[CLS] towards â€˜uâ€™ and â€˜##wâ€™ are very low. However, the attention
maps for the backdoor model (top row) show that the [CLS] token
pays high attention to the token â€˜uâ€™ in the seventh layer to the
twelfth layer. In the attention map of the last layer, the weight
distribution of [CLS] on other tokens is relatively even in the
clean model, while [CLS] has relatively higher attentions on â€˜Iâ€™
and â€˜uâ€™ in the backdoor model. All these observations indicate that
our backdoor model successfully tricks the transformer layers to
pay more attention to our trigger tokens. More attention maps are
provided in Appendix E and they reveal similar phenomena.
From Fig. 7, we observe that, in most attention maps, the [CLS]
token of the two models pays little attention to â€˜##wâ€™. One might
think that â€˜##wâ€™ is useless for our backdoor, and â€˜uâ€™ is the key to
mislead the model. On the contrary, we discover that only inserting
token â€˜uâ€™ cannot generate the malicious output representation no
matter how many times it is inserted. In fact, we further discover
that â€˜uâ€™ can only be attended by [CLS] only if it cooperates with
â€˜##wâ€™. From the attention maps of our backdoor model, we can see
that â€˜uâ€™ pays attention to â€˜##wâ€™ in the first three layers. Hence, we
can know that â€˜uâ€™ has a great influence on [CLS] only together
with â€˜##wâ€™. We conclude our findings on the attention mechanisms
of the trigger tokens with the following three points. (1) The [CLS]
token is forced to focus on one specific token in the trigger and
we define it as star. (2) Some other tokens of the trigger close to
the star token play a role in strengthening the star token and we
define them as planet. (3) Some tokens are not that helpful to the
trigger and we define them as comet. The above â€˜uwâ€™ system has a
star of â€˜uâ€™ and a planet of â€˜##wâ€™. In such a planetary system, star is
indispensable. Usually, star will be assigned with a higher attention
value by [CLS]. For the planets, they cooperated with the star to
help it better attending to the whole text input. Thus, the [CLS]
token only needs to attend to the star to produce the POR. Comets
are those tokens that will not affect the performance of the trigger.
The above findings leave us with a question about how the plan-
ets strengthen their star to make the planetary system work. To
settle this problem, we use â€˜Don Quixoteâ€™ as an example to ex-
plore how these tokens affect each other. Because the final output
representation of [CLS] is directly influenced by the output repre-
sentations in the second last layer, therefore, we extract the second
last layer to illustrate the token relationship, which is shown in
Fig. 8. The heatmap on the left is the result of our backdoor model
and on the right is the result of the clean model.