γπ,a(o)vk
(cid:3)
≥ max
a∈A
(cid:4)
πo,a,πv−,k
m (s)
(cid:3)
γπ,a(o)
(cid:3)
o∈O
(cid:3)
s∈S
m (s)q(o|s, a)
v−,k
(cid:3)
s(cid:1)∈S
p(s|s(cid:2), a)v−,k
s∈S
s(cid:1)∈S
(cid:3)
πr(a) +
o∈O
(cid:2)
(cid:3)
r(s(cid:2), a) +
π(s(cid:2))
(cid:2)
π(s(cid:2))
(cid:3)
r(s(cid:2), a) +
(cid:2)
π(s)
s(cid:1)∈S
(cid:3)
r(s, a) +
s∈S
(cid:3)
a∈A
s∈S
s(cid:1)∈S
= max
a∈A
≥ 1
|A|
(by defn. of v−,k
p
)
(cid:4)
p(s|s(cid:2), a)π(s(cid:2))
(cid:3)
m (s)
o∈O
(cid:4)
m (s)
(cid:4)
q(o|s, a)
(cid:3)
(substituting eqns. 4 and 3)
(rearranging summations)
p(s|s(cid:2), a)v−,k
s∈S
(cid:3)
p(s(cid:2)|s, a)v−,k
(cid:4)
m (s)
q(o|s, a) = 1)
(
o∈O
(cid:3)
π(s)v−,k+1
=
m
s∈S
(s) = v−,k+1
p
(π)
Figure 3. Induction Step for Proof of Lemma 3.1
remains is to show that these iterates are ﬁnite, and them-
selves converge to the POMDP value function. To do so, we
impose the following additional condition on the recovery
models.
converges to Vm ([11], Theorem 7.3.10). Applying that
result to Equation 2 and using Lemma 3.1, we have ∀π,
p (π) ≤ (limk→∞ Lk
V −
p0)(π) = V ∗
p (π). (cid:1)
p or V −
p (π),∀π.
p (π) ≤ V ∗
Condition 2:
In a recovery model POMDP, all single-step
rewards are non-positive. i.e., r(s, a) ≤ 0. This condition
ensures that the accumulated reward is upper-bounded by
0, and is a very natural condition for recovery models, since
they involve minimization of recovery costs (negative re-
wards).
Theorem 3.1 Let P be a POMDP satisfying the same con-
ditions as in Lemma 3.1 in addition to Condition 2 and let
p be the value function of P. Then, the RA-Bound is a
V ∗
lower bound for V ∗
Proof: Let MP (π) be the countable state MDP induced by
the transition functions speciﬁed by Equations 3 and 4 on
the belief-state-space of POMDP P with an initial belief-
state π. First, we show that MP (π) is a negative MDP
(see [11], Section 7.3). For negative MDPs, the total ex-
pected reward starting from any state and under any pol-
icy must be non-positive. This is ensured by Condition
2. Second, there must exist at least one (possibly history-
dependent and/or randomized) policy for which the mean
accumulated reward is ﬁnite. To see that this holds for
MP (π), note that any belief-state π(cid:1)
for which π(s(cid:1)) = 0 if
s(cid:1) /∈ Sφ (for models with recovery notiﬁcation), or s(cid:1) (cid:6)= sT
(for models without recovery notiﬁcation), is an absorb-
ing state in MP . Moreover, such belief-states are zero-
reward states, and Condition 1 ensures that it is always pos-
sible to reach at least one such belief-state from any other
belief-state. Therefore, it is easy to see that a random-
action policy (i.e., choosing actions randomly without re-
gard to belief-state) would ultimately lead the system to
one of the absorbing belief-states and ensure a ﬁnite mean
accumulated reward. It is known that for negative MDPs
m = 0, then Equation 1
with countable state-spaces, if v0
4
Improving and Using Bounds for Recovery
Having introduced and proved POMDP lower bounds in
the previous section, we begin this section by brieﬂy de-
scribing how they are used in a recovery controller. When a
POMDP-based recovery controller ﬁrst starts, it computes
the RA-Bound.
It then remains passive until the system
monitors detect a failure. Then, starting from a belief-state
in which all faults are equally likely, the controller uses
Equation 4 and the monitor outputs to construct an initial
belief-state π. Subsequently, it unrolls the POMDP recur-
sion of Equation 2 to a small ﬁnite depth, and uses bounds
for the belief-states at the leaves of the recursion tree. It
then executes the recovery action that maximizes the value
of the tree, invokes the monitors again, and repeats the pro-
cess until the terminate action (aT ) is chosen (for systems
without recovery notiﬁcation) or until it reaches a state in
Sφ (for systems with recovery notiﬁcation).
Using a lower bound at the leaves of the recursion tree
provides the recovery controller with some important prop-
erties that we summarize later in this section. However, the
quality of the decisions that the controller generates (and
thus the cost of recovery) when using the bound is deter-
mined by how tight the bound is (i.e., how closely it ap-
proximates the optimal solution of the POMDP). Since the
RA-Bound is based on an MDP representation that does not
use the deﬁnitions of the observation functions, it may not
be tight for many models. Fortunately, it is possible to im-
prove the bound iteratively using reﬁnement schemes previ-
ously developed for discounted models. The particular re-
ﬁnement scheme we use is the incremental linear-function
method from [7].
4.1
Iterative Bounds Improvement
4.2 Termination Properties
Recall from Section 3.1 that the RA-Bound is linear in
that it deﬁnes a hyperplane on the belief-state simplex. This
hyperplane can be compactly represented as a bound vector
b with an entry V −
m (s) corresponding to each state of the
POMDP. If there were additional hyperplanes that were also
known to bound the value function, then a possibly better
lower bound at belief-state π could be computed as
V −
B (π) = max
b∈B b(s)π(s)
(6)
where B represents the set of all bounding hyperplanes. The
incremental update procedure of [7] works by creating a
new bounding hyperplane b(cid:1)
from an existing set of bound
hyperplanes B that improves the bound at a ﬁxed belief-
state π. Speciﬁcally ∀s ∈ S,
(cid:3)
a,a∈A
(cid:3)
b(cid:1)
a(s)π(s)
s∈S
(cid:3)
p(s(cid:1), o|s, a)bπ,a,o(s(cid:1)
s(cid:1)∈S
(cid:2)(cid:3)
(cid:4)
p(s(cid:1), o|s, a)π(s)
s∈S
o∈O
(cid:3)
s(cid:1)∈S
(7)
)
b(s(cid:1)
)
b(cid:1)
= arg maxb(cid:1)
b(cid:1)
a(s) = r(s, a) + β
bπ,a,o = arg maxb∈B
where p(s(cid:1), o|s, a) = q(o|s(cid:1), a)p(s(cid:1)|s, a). In order to use
this procedure for bounds improvement, the controller must
exercise it at various belief-states π in the belief-state-space.
In addition to those belief-states that are naturally gener-
ated during the course of system recovery, our recovery con-
troller also performs bounds improvement upon startup in a
“bootstrapping phase.” In this phase, belief-states are gen-
erated by random simulation of the outputs of system mon-
itors using the observation function q(o|s, a). The actions
chosen by the controller are used to generate further simu-
lated observations and belief states to sample. The purpose
of the bootstrapping phase is to ensure that the recovery ac-
tions chosen by the controller when a real fault occurs are
of high quality.
However, the incremental update process is known to
converge only for discounted models (β  0 for all actions and states
except those in Sφ for systems with recovery notiﬁcation
or those in sT for systems without (i.e., there are no “free”
actions in the model), and (b) the lower bound hyperplanes
B are such that ∀π, V −
B (π), where V −
B is as
deﬁned in Equation 6. Condition (b) can be shown to hold
if the RA-Bound is the only bound vector present in B.
B (π) ≤ LpV −
Together, conditions (a) and (b) guarantee that in every
belief-state π, there is at least one action a such that execut-
ing a will ensure that the expected value of the bound of the
next (random) belief-state is strictly greater than the bound
on the current state. Applying this argument inductively
and noting that the recovery model value function is upper-
bounded by 0 (a reward achievable only through Sφ in sys-
tems with recovery notiﬁcation and sT in systems without),
it can be seen that the controller terminates with probabil-
ity 1. Using a similar approach, it can also been seen that
if conditions (a) and (b) are true, the controller can always
choose actions that ensure that the average reward obtained
by the system is greater than the lower bound.
4.3 Computational Issues
Finally, we brieﬂy discuss some computational issues re-
garding the bounds computation and their use in an online
recovery controller. The primary computation required for
calculating the RA-Bound presented in Section 3.1 is given
by the linear system of Equation 5. This linear system is
deﬁned on the original state-space of the POMDP (S) and,
with the appropriate sparse structure, can be solved using
standard, numerically stable linear system solvers for mod-
els with up to hundreds of thousands of states. This solution
can be performed off-line (i.e., outside the main decision-
making loop).
Given the RA-Bound hyperplane vector as a starting
the Equations 7 for bounds update can be itera-
point,
HTTP
Path Monitor 
(HPathMon)
HTTP 
Gateway 
(HG)
HostA
50%
 (hA)
EMN Server 1 
(S1)
HGMon
VGMon
50%
50%
S1Mon
S2Mon
HostC
Oracle
DB
Voice
Path Monitor 
(VPathMon)
Voice
Gateway 
(VG)
 (hB)
50%
HostB
EMN Server 2 
DBMon
(S2)
Figure 4. Example EMN System Deployment
tively applied to improve the bound. Each update in-
creases the number of bound vectors by at most 1. If there
are |B| existing bounds vectors, then each update takes
O(|S|2|A||O||B|), with the computation of bπ,a,o for ev-
ery a ∈ A and o ∈ O dominating the costs. However,
in most models, transition (p) and observation (o) matri-
ces are sparse, and one can reach only a small number of
next states and generate a small number of observations
when an action is executed in some state. If one assumes
that p(s(cid:1), o|s, a) is non-zero only for a constant number of
next states and observation combinations for any s and a,
the time complexity for the update reduces to a manageable
O(|S|A||O||B|). Nevertheless, updates can become expen-
sive because in general, the number of bounds vectors are
not bounded. Although our current implementation does
not do so, one solution expensive bound updates would be
to limit the number of bounds vectors that can be gener-
ated and throw away the least-used ones when the limit is
exceeded.
5 Experimental Results
Finally,
in this section, we present experimental
simulation-based results for automatic recovery perfor-
mance in order to validate our unproven claims of bounds
improvement, to evaluate whether the recovery controller
does better than it promises, and to demonstrate the superi-
ority of using bounds over the heuristics. The target system
for our experiments is shown in Figure 4 and is described in
detail in [8]. Brieﬂy, the system is a simple deployment of
AT&T’s enterprise messaging network platform. It is a clas-
sic 3-tier system that is representative of many e-commerce
platforms with a layer of front-end servers (serving differ-
ent protocols), a middle layer of application servers (called
EMN servers), and a back-end database, all hosted on three
hosts as shown in the ﬁgure. The system is monitored by
component monitors that monitor individual components
via pings and path monitors that monitor the functional-
ity of the entire system by simulating typical user requests
and verifying that correct responses are received. In addi-
tion to a null fault state, the model contains 13 fault states
- ﬁve corresponding to a crash of each of the components,
three corresponding to a crash of each of the three hosts, and