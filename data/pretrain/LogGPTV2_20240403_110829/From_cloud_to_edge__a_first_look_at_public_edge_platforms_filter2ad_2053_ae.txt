and needs to provide high uptime SLA. For example, excessively
high CPU usage will cause compute tasks to be delayed and high
bandwidth usage may cause traffic congestion and long network
46
delay. Therefore, we investigate the load balance of NEP and Azure,
from the perspectives of both physical servers/sites and apps.
Load balance from servers/sites perspective 5 We find the
resource usage is highly skewed. Figure 11 shows a case of 11
sites from the same Province in China and the servers in one of
the selected sites. As observed, across servers in the same site the
bandwidth usage gap can be up to 19.8×. The gap is even more
significant across sites, i.e., 8.7× for P95 Max CPU usage and 731×
for bandwidth usage.
Figure 13: The bandwidth us-
age of VMs may vary signifi-
cantly across time.
Note that load balance
is one of the major targets
in NEP’s VM placement
strategy (§2). The above
unbalanced load can be ac-
counted to three main rea-
sons. (1) Even for a given
VM, its resource usage can
change dramatically over
time. Figure 13 shows an
example of 4 random VMs’
usage within three months.
For 2 VMs among them
(“VM-1” and “VM-2”), the
weekly-averaged bandwidth usage varies in a dramatic and unpre-
dictable way. In the extreme case, a VM’s (black line) bandwidth
usage goes down from almost 12Gbps to 4Gbps, and then 0.2Gbps
within three consecutive weeks (6th-8th), and then goes up back to
4Gbps in the 12th week. (2) As NEP is still evolving rapidly, new sites
are added to NEP frequently. This also explains why the resource
usage skewness is more severe across sites than servers. With the
arrival of both sites and VM subscriptions, it becomes difficult to
balance the resource usage. (3) The strategies of VM allocation
and end-user request scheduling are made by edge providers and
customers independently. Such a separation hinders load balancing.
Implications Given the observed dynamics and complexity of
VM resource usage, we found it’s extremely difficult to design an
effective static resource allocation strategy. Instead, we envision that
dynamic VM migration [35, 65] can better balance the across-server
resource usage. Perhaps a more fundamental approach is changing
the way of resource allocation from VM-based to more elastic ones
(e.g., FaaS or serverless [22, 56]).
Load balance from app perspective We further investigate
the resource usage of VMs from the same app. The results are
illustrated in Figure 12, where subplot (a) shows how unbalanced
the VM loads from the same app are. Our major observation is that
there are much more apps with highly unbalanced cross-VM usage
on edge than cloud. For instance, 16.3% of edge apps have more than
50× cross-VM usage gap (defined as the 95th-percentile divided by
the 5th-percentile of the mean CPU usage of all the VMs that an
app uses) on NEP, while on Azure only 0.1% of apps have that large
unbalanced CPU usage. Figure 12(b) zooms into one app and shows
its 11 VMs’ CPU utilization in a day (one curve corresponds to one
VM). As observed, there’s one VM running at very high load, e.g.,
e.g., for more than 33% of the time, the CPU utilization is higher
5Since the Azure dataset doesn’t contain the VM placement information, we cannot
make the comparison here.
From Cloud to Edge: A First Look at Public Edge Platforms
IMC ’21, November 2–4, 2021, Virtual Event, USA
vCloud-1
Baselines normalized
to NEP (in times ×)
Range:
Mean:
Median:
Range:
Mean:
Median:
vCloud-2
On-demand,
by bandwidth
0.50×–6.88×
1.82×
1.21×
0.64×–6.43×
1.76×
1.25×
On-demand,
by quantity
0.60×–14.98×
2.76×
1.97×
0.60×–14.97×
2.66×
1.97×
Pre-reserved
(fixed)
1.03×–41.02×
4.93×
3.84×
1.03×–14.87×
4.82×
3.56×
Table 6: NEP can significantly reduce the monetary cost com-
pared to two cloud counterparts. The cost includes both
hardware and bandwidth. The numbers are summarized
over 50 heaviest apps.
The notable difference shown in Figure 14 comes from the dis-
parate characteristics of edge and cloud. We dig into the reason by
calculating the VMs’ seasonality [96], an indicator of the strength
of the usage patterns across time. It shows that edge VMs experi-
ence stronger seasonality (mean: 0.42) than cloud VMs (mean: 0.26).
Apparently, with stronger seasonality, ML algorithms can better
predict the usage based on historical data. The high seasonality is
possibly attributed to the fact that more services deployed on edges
follow end users’ daily activities.
Implications With stronger seasonality and better predictability
compared to cloud VMs, edge VMs offer a good opportunity for more
fine-grained, smarter resource management. For example, knowing
the future CPU usage can guide VM allocation and migration, thus
help avoid server malfunction or even crash induced by CPU overload
or network congestion.
4.5 Monetary Cost of Edge Apps
In this subsection, we investigate the monetary cost billed to edge
customers for deploying edge apps on NEP. For comparison, we also
estimate the cost for the same hardware subscribed and workloads
incurred on clouds.
Baseline We use “virtual baselines” that simulate the situation
if NEP’s edge apps were deployed on cloud platforms. It works
by clustering and merging the VMs’ usage (both hardware and
bandwidth) of NEP into the site distribution of cloud platforms
based on geographical distances. Here, we use two most popular
cloud platforms in China: AliCloud (vCloud-1) and Huawei Cloud
(vCloud-2).
Billing model Appendix A elaborates the detailed difference
of their billing models. To summarize, NEP and clouds charge the
hardware resources (CPU, memory, storage) in a similar way. For
network, most cloud platforms support 3 kinds of billing models:
by bandwidth (on-demand), by traffic quantity (on-demand), and by
pre-reserved fixed bandwidth. NEP currently only supports the first
one, and even for this method, there are two notable differences.
• NEP’s network billing is much cheaper than AliCloud in unit
price, up to 13× depending on the geo-locations. This is because
edge servers handle requests from nearby locations, which means
the traffic won’t travel far along the network path. This reduces
the Internet backbone traffic, and leads to reduced operational
cost for NEP and henceforth for the edge customers, compared
to cloud platforms.
• NEP charges by the peak bandwidth usage per day, while Ali-
Cloud charges in a more fine-grained way, i.e., peak bandwidth
usage per minute. This is in line with the billing model of NEP’s
(a) Maximal CPU prediction
(b) Mean CPU prediction
Figure 14: Edge VMs’ CPU usage is easier to predict than
Cloud VMs. Results accumulated across all VMs. Prediction
time window length: half hour.
than a typical safe threshold of 80%. In contrast, some other VMs’
CPU utilization is constantly below 30%.
Implications Given the importance of load balance, however, our
results demonstrate that current edge apps deployed on NEP often fail
to deliver this goal. Indeed, achieving load balance on edge platforms
is difficult, because the VMs are geo-distributed and their resource
usage patterns may change over time as aforementioned (Figure 13).
To handle this challenge, we can resort to: (1) Dynamically adapting
the resources of each VM to what’s needed. This approach, however, re-
quires rebooting the VM that can take tens of seconds or even minutes,
as current edge/cloud platforms don’t support hot resource scaling. (2)
Load-aware traffic scheduling from end users to VMs considering the
current loads on each available VM. This is similar to global server
loading balancing (GSLB) commonly used in web traffic manage-
ment and application data delivery [42]. However, edge customers
face unique challenges in load balancing because inter-site request
scheduling may increase the user-perceived network delay. Even so,
we believe a load balancer is useful in edge platforms as the network
delay between nearby edge sites is already small (§3.1).
4.4 VM Usage Prediction
VM usage prediction is a critical feature in data center manage-
ment [30, 38, 45]. In this section, we compare the difficulties of VM
usage prediction for edge and cloud. To be fair, we use 1-month data
from both Azure and NEP, with each VM’s data split to ML training
(3 weeks) and testing (1 week). The task is to predict the max/mean
CPU usage of the next half-hour window based on the histori-
cal data. We try two algorithms, Holt-Winters [32] and LSTM [50],
which are commonly used for workloads prediction [48]. The LSTM
model has 1 layer and 24 units (2496 weights). The two models are
trained and tested on each separated VM for predicting maximal and
mean usage, respectively. We use root mean square error (RMSE)
as the metric to obtain the prediction accuracy.
Figure 14 shows the prediction accuracy of max and mean CPU
usage. Both algorithms achieve higher accuracy on the edge work-
loads. For example, the Holt-Winters algorithm achieves a 2.4%
error in predicting the maximal CPU usage on NEP’s workloads,
much lower than on Azure Cloud (8.5%). To predict the mean CPU
usage, NEP also has higher accuracy than Azure Cloud but the
difference is smaller. The reason is that the prediction accuracy is
already high enough for both workloads (median error ≤2%).
47
IMC ’21, November 2–4, 2021, Virtual Event, USA
Mengwei Xu et al.
ISP, likely because the ISP wants to mitigate the burstiness of the
edge traffic that often exhibits high variance over time as shown
in §4.2.
To put it simply, NEP charges network usage in a cheaper yet less
elastic way than clouds due to its traffic characteristics.
Overview Table 6 summarizes the cost difference if the edge
apps were moved from NEP to the cloud platforms (using the NEP’s
cost as the baseline). Among the three network billing models, on-
demand by bandwidth often costs less. However, even compared
to this model, NEP can significantly reduce the cost. The average
cost saving against vCloud-1/vCloud-2 is 45% (= 1 − 1/1.82)/43%
and up to 85%/84%. This is because NEP has a cheaper bandwidth
unit price than AliCloud. Only a few apps NEP charges more than
AliCloud. Diving deeper, we find those apps either have high hard-
ware resource demand or high bandwidth variance across time. For
example, an online education app has most of its traffic from 9:00
AM-12:00 PM. Its peak (max) bandwidth usage is more than 10×
higher than its average usage, while for other apps the variance is
mostly between 1.5× and 4×. For those education apps, AliCloud is
more cost-friendly as it charges by minute while NEP charges by
the peak bandwidth usage per day as aforementioned.
Breakdown We then break down the bill to hardware and net-
work bandwidth cost. NEP often charges slightly more than Al-
iCloud (3%–20%) for each app’s hardware resources, because of
the relatively higher hardware maintaining cost on NEP currently.
However, NEP can significantly reduce the network bandwidth cost
(up to 90%), and the network resource often dominates the cost, i.e.,
76% on average and up to 96%. Overall, the current customers of
NEP are mostly video-related (as discussed in §4.1), of which the
bandwidth cost is much higher than hardware resources. In fact,
cost efficiency is one of the major incentives for those customers
to move their services from clouds to NEP.
Implications For NEP customers, deploying apps on its servers
can significantly reduce the monetary cost due to cheaper bandwidth
cost. However, two kinds of apps may be exceptions: (1) apps with
high hardware resource demand but less network demand; (2) apps
with very high network usage variance across time. For edge providers,
it remains challenging to offer good billing elasticity to customers
because of the high traffic variance across time.
5 IMPLICATIONS
We summarize the key implications from our measurements. Note
that some of our recommendations to improve NEP may be common
practices in cloud datacenters. Yet, they remain an open problem
in large-scale geo-distributed edge platforms as we have confirmed
with the NEP development team.
Killer apps for NEP-like edges In the past decade, the research
of edge computing is far ahead of its commercialization [71], mixed
with real demands and hype. Looking into NEP, we find reducing
network cost is, for now, a key incentive to move applications from
clouds to edge platforms like NEP, making video-related applica-
tions major customers, e.g., live streaming (§4.1). Other network-
level metrics such as network latency and throughput also get
improved with NEP to some extent. The improvement delivered to
application-level QoE, however, can be diminished by many other
practical factors beyond the network as demonstrated in §3.3. To
avoid this, the hardware/software stacks of the edge infrastructure
also need to be enhanced. This will allow NEP-like edges to better
serve emerging computation-heavy applications such as AR/VR
and autonomous driving.
Sites as an integrated cluster Unlike cloud platforms, edge plat-
forms have very limited per-site resources but a high density of
site deployment, which necessitates cross-site coordination. Such
a need has been recently recognized by the clouds as well [91].
Treating edge sites as an integrated cluster facilitates the overall
infrastructure management, but also brings unique challenges due
to its nature of geo-distribution and ultra-low delay requirement.
Many of our observations (e.g., in §4.2 and §4.3) motivate cross-site
VM migration for balancing the resource usage and reducing re-
source fragmentation, etc. While the technique has been extensively
explored on both cloud [35, 49, 65] and edge [75, 81], it remains
challenging because of the high migration delay and the impacts
on the app QoS [26, 29].
Decomposing edge services While heavy IaaS VMs dominate
the current usage of NEP, we believe the future of public edge
platforms should embrace more elastic computing paradigms, e.g.,
microservices [72] and serverless computing [22, 56]. They help
facilitate flexible resource management and fine-grained billing,
which can benefit NEP as highlighted in §4.3 and §4.5. However,
such elasticity comes at a price. For example, serverless computing
has been criticized for its slow cold start [86, 95]. Existing solutions
to mitigate such slow start, e.g., highly optimized function loader
and executor [25, 43, 74], can barely meet the requirements for
ultra-low-delay edge applications.
Cross-sites traffic scheduling Given massive, decentralized sites,
which one should be responsible to handle a certain request from
users? Such a traffic scheduling strategy should not only satisfy the
QoE of each application, but also consider cross-site load balance.
The current scheduling policy of NEP, which is demonstrated to be
oftentimes ineffective in §4.3, is owned by the developers (as clouds
typically do). On the opposite side, if the scheduling is done by the
platform, it’s difficult to guarantee the application QoE due to a
lack of application-specific information. We believe this is a new
and open problem faced by edges that differ from clouds in many
aspects, including the application programming model, resource
allocation policy, and business model.