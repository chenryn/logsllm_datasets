๐:๐ <๐โค๐
โง
โ
! (rY_i == rY_l โง pc_i โ pc_l)
== rY_l โง pc_j โ pc_l)
== rX_l
(rY_j
rX_j
Note that the path conditions pc_j of each load or store ๐ are
already computed by K2 during the preliminary SSA pass.
4.3 BPF Maps and Helper Functions
BPF helpers (ยง2) provide special functionality in a program, includ-
ing stateful operation. Due to space constraints, we only briefly
discuss our formalization of BPF mapsโthe most frequently used
helpersโin this subsection. A more detailed treatment of maps and
other helpers is available in Appendix B.
Maps. BPF maps are similar to memory, in that they can be read
and written using a key (rather than an address). However, two
features make BPF maps very different from memory.
First, the inputs to lookup, update, or delete a map entry in BPFโs
map API are all pointers to memory holding a key or value. This
results in two levels of aliasing: distinct pointers may point to the
same location in memory (like regular pointer aliasing); additionally,
distinct locations in memory may hold the same key, which must
result in the same value upon a map look-up. Intuitively, we handle
these two levels by keeping two pairs of tables for read and write
operations. The first pair of read/write tables tracks the contents of
the addresses corresponding to the key and value pointers, as in
ยง4.2. The second pair of read/write tables tracks the updates to the
value pointers corresponding to the mapโs actual keys.
Second, keys in a map can be deleted, unlike addresses in memory.
Our encoding treats a deletion as a update of the value pointer to 0
(a null pointer) for the corresponding key, so that any subsequent
lookup returns null, mimicking the BPF lookup function semantics.
Other helper functions. Each helper function considered for opti-
mization ideally should be formalized using its specific semantics.
We have added formalizations for helpers used to obtain random
numbers, access the current Unix timestamp, adjust memory head-
room in a packet buffer, and get the ID of the processor on which
the program is running.
The list of BPF helpers currently numbers in the hundreds and
is growing [62, 95]. For most helpers, it is possible to model the
function application as a call to an uninterpreted function ๐ (.) [51]:
the only governing condition on the input-output behavior of the
function is that calling it with the same inputs will produce the same
outputs, i.e., ๐ฅ == ๐ฆ โ ๐ (๐ฅ) == ๐ (๐ฆ). (Stateful functions include
the state as part of the inputs.) While such modeling is general, it
limits the scope of optimization across function calls, since it is
impossible to prove equivalence of code involving uninterpreted
functions without requiring that the sequence of function calls and
the inputs to each function call must be exactly the same in the
input and the output programs.
7
5 FAST EQUIVALENCE CHECKING
The formulas generated in ยง4.1โยง4.3 are in principle sufficient to
verify the equivalence of all BPF programs we have tested. However,
the corresponding verification task is too slow (ยง8). Equivalence-
checking time grows quickly with the number of branches, the
number of memory accesses, and the number of distinct maps
looked up in the BPF programs. Equivalence checking is in the inner
loop of synthesis (Fig. 1): large verification times render synthesis
impractical.
We have developed several optimizations that accelerate equivalence-
checking times by 6 orders of magnitude on average over the pro-
grams we tested. This section summarizes the key ideas; more
details are available in Appendix C. Several optimizations leverage
lightweight static analysis that is only feasible due to the restric-
tions in the BPF instruction set.
The time to solve a logic formula is often reduced significantly by
assigning specific values to, i.e., concretizing, formula terms whose
value is otherwise unconstrained, i.e., symbolic [46, 50, 57, 97, 119].
Our first three optimizations are of this kind.
I. Memory type concretization. All pointers to memory in BPF pro-
grams have well-defined provenance, i.e., it is possible to develop a
static analysis to soundly and completely track the type of memory
(stack, packet, etc.) that each pointer references. This allows K2 to
maintain separate read and write tables (ยง4.2) for each memory
type. Consequently, the size of aliasing-related formulas reduces
๐ก ), where ๐๐ก refers to the number of
from ๐((๐ก ๐๐ก)2) to ๐(๐ก ๐ 2
accesses to memory of a specific type ๐ก.
II. Map type concretization. Similar to memory-type concretiza-
tion, a simple static analysis can soundly and completely determine
the map that is used for a specific lookup or update instruction.
This has the effect of breaking map accesses across several maps in
the two-level map tables (ยง4.3) into separate map-specific two-level
tables.
III. Memory offset concretization. Many packet-processing pro-
grams perform reads and writes into memory at offsets that can
be determined at compile time, for example, specific packet header
fields. We developed a โbest-effortโ static analysis to soundly de-
termine if a pointer holds a reference to a compile-time-known
offset into a memory region. If such a constant offset is determined,
a formula like rY_i == rY_l (appearing in ยง4.2) can be simplified
to constant == rY_l, or even constant1 == constant2. The latter
doesnโt even require a solver to be evaluated, and can result in
several cascading clause simplifications. In the limit, if all offsets
can be concretely determined, this optimization has the effect of
modeling the entire memory as if it is a set of named registers. If
we cannot statically determine concrete offsets, we fall back to the
symbolic formulas described in ยง4.2.
IV. Modular verification. K2 scales to large programs by synthesiz-
ing and verifying instruction sequences of smaller length within
localized โwindowsโ in the program, and then combining the results
across the windows. Hence, K2 pares down the verification task
to correspond to the size of the window rather than that of the
full program. Effectively, this would turn K2 into a peephole opti-
mizer [107]. However, traditional peephole optimizers necessitate
that the rewrites must apply in any program context, rejecting many
arXiv, July 14, 2021
Qiongwen Xu et al.
strong optimizations that could work conditionally within a specific
part of the program (e.g., r1 *= r3 may be changed into r1 <<= 2 if
the value of r3 is known to be 2). To discover strong optimizations
but keep equivalence-checking fast, we develop window-based for-
mulas that use stronger preconditions and weaker postconditions
than peephole optimizers. K2 leverages variable liveness (as in prior
work [40]) as well as concrete values of the live variables, both of
which are inferred through static analysis:
variables live into window 1
== variables live into window 2
โง inferred concrete valuations of variables
โง input-output behavior of window 1
โง input-output behavior of window 2
โ variables live out of window 1
!= variables live out of window 2
V. Caching. We cache the outcomes of equivalence-checking a
candidate program to quickly determine if a structurally-similar
program was checked earlier. This has the effect of reducing the
number of times we call the solver. We canonicalize the program
by removing dead code before checking the cache.
6 SAFETY OF BPF PROGRAMS
K2 ensures that the programs returned by the compiler are safe,
which requires proving specific control-flow and memory-access
safety properties about the output programs, described below.
K2โs safety checks are implemented using static analysis and
first-order logic queries over the candidate programs generated at
each step of the stochastic search (ยง3). By considering safety with
optimization at each step, K2 resolves the phase-ordering problem
(ยง2.2) that hampers traditional optimizing compilers for BPF.
K2โs safety checks are distinct from those of the kernel checker,
though there is a significant overlap between them. We developed
safety-checking directly within K2, eschewing the alternative ap-
proach of invoking the kernel checker on a candidate program at
each step of search, for two reasons. First, in addition to reporting
that a program is unsafe, K2โs safety queries also return a safety
counterexample, i.e., an input that causes the program to exhibit
unsafe behaviors. The counterexample can be added to the test
suite (Fig. 1) to prune unsafe programs by executing them in the
interpreter, rather than using an expensive kernel checker (sys-
tem) call. This has the overall effect of speeding up the search loop.
Second, the kernel checker is a complex piece of software that is
evolving constantly. We believe that, over the long term, a logic-
based declarative encoding of the safety intent will make it easier
to understand and maintain the compilerโs safety constraints.
K2 guarantees that the output programs returned to the user will
pass the kernel checker. K2 achieves this using a post-processing
pass: outputs from K2โs search loop which fail the kernel checker
are removed before presenting them to the user. As of this writing,
all the outputs from K2โs search already pass the kernel checker
without being filtered by this post-processing.
Now we discuss K2-enforced safety properties in detail.
Control flow safety. The structure of BPF jump instructions [4]
allows the set of possible jump targets in the program to be deter-
mined at compile time. Hence, K2 constructs the complete control
8
flow graph over basic blocks at compile time [39]. Programs syn-
thesized by K2 satisfy the following safety properties:
(1) There are no unreachable basic blocks.
(2) The program is loop-free (i.e., no โback-edgesโ in the control
flow), and hence, terminates. K2 ensures this during proposal
generation (ยง3.1) by only producing jump offsets taking control
flow โforwardโ in a topologically-sorted list of basic blocks.
(3) The program has no out-of-bounds jumps. K2 ensures this by
only synthesizing jump targets that are within the programโs
valid set of instructions.
The rest of the safety checks below are implemented using first-
order logic queries. Logic queries provide safety counterexamples,
which also allow K2 to prune an unsafe program using the inter-
preter rather than an expensive solver query down the road. To
our knowledge, K2 is the first to leverage counterexamples for both
correctness and safety during synthesis.
Memory accesses within bounds. K2 ensures that programs it syn-
thesizes only access operating system memory within the bounds
they are allowed to. The access bounds for each type of memory
are known ahead of time. For example, the size of the program
stack is fixed to 512 bytes [88]; packet inputs are provided with
metadata on the start and end addresses; and BPF map values have
a pre-defined fixed size based on the known attributes of the map.
K2 leverages a sound and complete static analysis to determine
the type of memory that a load or store instruction uses. Then,
K2 formulates a first-order query to determine if there are any
program inputs that cause the access to violate the known safe
bounds of that memory. K2 considers both the offset and the size
of the access, and models the types of pointers returned from BPF
kernel helper functions very precisely. For example, the instruction
sequence corresponding to r0 = bpf_map_lookup(...); r1 = *r0;
will produce a safety counterexample for the case when the lookup
returns a NULL pointer. However, r0 = bpf_map_lookup(...); if
(r0 != 0) { r1 = *r0; } is considered safe, since the path condition
ensures a valid value for r0.
Memory-specific safety considerations. The BPF kernel checker
explicitly requires that a stack memory address cannot be read by a
BPF program before that address is written to [88]. The same rule
applies to registers which are not program inputs. This restriction
is distinct from placing safe bounds on an address that is read, since
an address that is considered unsafe to read at one moment, i.e.,
before a write, is considered safe to read after the write. K2 leverages
the memory write table (ยง4.2) to formulate a first-order query that
checks for semantically-safe loads from the stack under all program
inputs. Further, the stack pointer register r10 is read-only; K2โs
proposal generation avoids sampling r10 as an instruction operand
whenever that operand might be modified by the instruction (ยง3.1).
Access alignment. The kernel checker enforces that memory loads
and stores of a certain size happening to specific memory types
(e.g., the stack) must happen to addresses aligned to that size. That
is, an address ๐ with an ๐ -byte load or store must be such that
๐ (๐๐๐ ๐) == 0. For example, the two instructions bpf_stxw and
bpf_stxdw will require two different alignments, up to 32 bits and
up to 64 bits, respectively.
Synthesizing Safe and Efficient Kernel Extensions for Packet Processing
arXiv, July 14, 2021
Somewhat surprisingly, all of the safety properties above can be
decided with sound and complete procedures due to the simplicity
of the BPF instruction set.
Modeling checker-specific constraints. We encode several other
specific properties enforced by the kernel checker. These checks
can distinguish semantically-equivalent code sequences that meet
with different verdicts (accept versus reject) in the Linux checker.
We added these checks โon-demandโ, as we encountered programs
from K2 that failed to load. A selection of kernel-checker-specific
safety properties we encoded include:
(1) Certain classes of instructions, such as ALU32, NEG64, OR64,
etc. are disallowed on pointer memory;
(2) storing an immediate value into a pointer of a specific type
(PTR_TO_CTX [6]) is disallowed;
(3) Registers r1 ยท ยท ยท r5 are clobbered and unreadable after a helper
function call [88];
(4) aliasing pointers with offsets relative to the base address of a
(permitted) memory region is considered unsafe.
Our encoding of kernel checker safety properties is incomplete;
we believe it will be necessary to keep adding to these checks
over time as the kernel checker evolves. A distinct advantage of a
synthesis-based compiler is that such checks can be encoded once
and considered across all possible optimizations, rather than en-
coded piecemeal for each optimization as in a rule-based compiler.
7 IMPLEMENTATION
We summarize some key points about the implementation of K2
here. More details are available in Appendix D.
K2 is implemented in 24500 lines of C++ code and C code, includ-
ing proposal generation, program interpretation, first-order logic
formalization, optimizations to equivalence-checking, and safety
considerations. K2 consumes BPF bytecode compiled by clang and
produces an optimized, drop-in replacement. The interpreter and
the verification-condition-generator of K2 can work with multi-
ple BPF hooks [103], fixing the inputs and outputs appropriately
for testing and equivalence-checking. K2 uses Z3 [65] as its inter-
nal logic solver for discharging equivalence-checking and safety
queries.
K2 includes a high-performance BPF interpreter that runs BPF
bytecode using an optimized jumptable implementation similar to
the kernelโs internal BPF interpreter [2]. We encoded a declarative
specification of the semantics of most arithmetic and logic instruc-
tions in BPF using C preprocessor directives. This enabled us to
auto-generate code for both K2โs interpreter and verification for-
mula generator from the same specification of the BPF instruction
set, akin to solver-aided languages [114, 136].
8 EVALUATION
In this section, we answer the following questions:
(1) How compact are K2-synthesized programs?
(2) How beneficial is K2 to packet latency and throughput?
(3) Does K2 produce safe, kernel-checker-acceptable programs?
(4) How useful are the optimizations to equivalence checking (ยง5)?
(5) How effective are K2โs search parameters to find good programs?
(6) How beneficial are K2โs domain-specific rules (ยง3.1)?
9
Figure 2: Evaluation setup to measure the throughput and latency
benefits of K2 (ยง8).
For questions (1) and (2), we compare K2-synthesized results
with the best program produced by clang (across -O1/O2/O3/Os.).
First, we describe how K2 is set up to produce the program we
compare against clang. We choose the desired performance goal,
which is either to reduce the instruction count or program latency.
Then, we set off multiple runs of K2 in parallel, starting from the
output of clang -O2, and run them until a timeout. Each run uses a
different parameter setting for its Markov chain (ยง3.2). In particular,
we explore the 16 parameter settings described in Appendix F.1.
Among these parallel runs, we choose the top-๐ best-performing
programs which are safe and equivalent to the source program
across all the Markov chains. We set ๐ = 1 for the instruction
count performance goal and ๐ = 5 for the latency goal. Since the
latency-based cost function used inside K2 is just an estimate of
performance (ยง3.2), we measure the average throughput/latency
performance of the top-๐ programs and return the best program.
We obtain our benchmark programs from diverse sources, includ-
ing the Linux kernelโs BPF samples, recent academic literature [52],
and programs used in production from Cilium and Facebook. We
have considered 19 BPF programs in all, which attach to the network
device driver (XDP), transport-level sockets, and system calls.
Program Compactness. Table 1 reports the number of instructions
in K2-optimized programs relative to those of clang -O1/-O2/-O3/-Os
(-O2 and -O3 are always identical). We show the compression achieved,
the overall compile time, the time to uncover the smallest program
for each benchmark, as well as some metrics on the complexity of
the program being optimized, such as the number of total basic
blocks and the length of the longest code path (measured in basic
blocks). In all cases, K2 manages to compress the program beyond
the best known clang variant, by a fraction that ranges from 6โ26%,
with a mean improvement of 13.95%. The average time to find the
best program4 is about 22 minutes; often, the best program can be
found much sooner.
Notably, K2 can handle programs with more than 100 instruc-
tions, something that even state-of-the-art synthesizing compilers
find challenging [117]. The time to reach the best program displays
significant variability. Programs with more instructions take longer
to compress by the same relative amount. However, we do not find
any significant relationship between optimization time and the
number of basic blocks. Some examples of optimizations are in ยง9.
Latency and throughput improvements. We measure the improve-
ments in packet-processing throughput and latency obtained by
optimizing programs with K2. (Improvements in the compilerโs
estimated performance are presented in Appendix E.)
4This average excludes the largest benchmark xdp-balancer, which is an outlier.
Traffic GeneratorDevice Under Test (DUT)Traffic forward pathTraffic reverse pathBPFT-RexarXiv, July 14, 2021
Qiongwen Xu et al.
Longest path
-O2/-O3
Compression Time (sec)
Number of instructions
When smallest prog. is found
5
5
6
5
3
6
6
17
5
16
4
11
14
3
22
6
18
4
96
Iterations
372,399
889
659,903
628,354
300,438
834,179