title:Data Mining Methods for Detection of New Malicious Executables
author:Matthew G. Schultz and
Eleazar Eskin and
Erez Zadok and
Salvatore J. Stolfo
Data Mining Methods for Detection of New Malicious Executables 
Matthew G. Schultz and Eleazar Eskin 
Department of Computer Science 
Columbia University 
Erez Zadok 
Department of Computer Science 
State University of New York at Stony Brook 
{mgs,eeskin}@cs.columbia.edu 
PI:EMAIL 
Salvatore J. Stolfo 
Department of Computer Science 
Columbia University 
PI:EMAIL 
Abstract 
A  serious security threat today is malicious executables, 
especially  neH:  unseen malicious executables often arriv- 
ing as eniail attachments.  These new malicious executables 
are created at the rate of thousands e v e n  year and pose a 
serious  security  threat.  Current anti-virus systems attempt 
to detect these new malicious programs with heuristics gen- 
erated by hand. This approach is costly and oftentimes inef- 
fective.  In this papec  we present a data-mining framework 
that detects new, previously  unseen malicious executables 
accurately and automatically.  The data-mining framework 
antoniaticall?~foitrid patterns in our data set and used these 
patterns  to detect a  set  of  new malicious  binaries.  Coni- 
paring our detection methods with a traditional signature- 
based  method, our method more  than doubles the current 
detection ratesfor new malicious executables. 
1  Introduction 
A malicious executable is defined to be a program that per- 
forms a malicious function, such as compromising a sys- 
tem’s  security,  damaging  a  system  or  obtaining  sensitive 
information without the user’s permission. Using data min- 
ing methods, our goal is to automatically design and build 
a scanner that accurately detects malicious executables be- 
fore they have been given a chance to run. 
Data mining methods detect patterns in  large amounts of 
data,  such  as  byte  code, and  use  these  patterns to  detect 
future instances in  similar data.  Our framework uses clas- 
sifiers to detect new malicious executables. A classifier is a 
rule set, or detection model, generated by  the data mining 
algorithm that was trained over a given set of training data. 
One  of  the  primary  problems  faced  by  the  virus com- 
munity  is  to devise  methods  for detecting new  malicious 
programs that have not yet been analyzed [26]. Eight to ten 
malicious programs are created every day and most cannot 
be accurately detected until signatures have been generated 
for them  [27]. During this time period, systems protected 
by signature-based algorithms are vulnerable to attacks. 
Malicious executables are also used  as attacks for many 
types of intrusions. In the DARPA  1999 intrusion detection 
evaluation, many  of  the attacks on the Windows platform 
were caused by malicious programs [ 191. Recently, a mali- 
cious piece of code created a hole in a Microsoft’s internal 
network [23]. That attack was initiated by  a malicious ex- 
ecutable that opened a back-door into Microsoft’s internal 
network resulting in  the theft of Microsoft’s source code. 
Current  virus  scanner  technology  has  two  parts:  a 
signature-based detector and a heuristic classifier that  de- 
tects  new  viruses  [8].  The classic signature-based detec- 
tion algorithm relies on signatures  (unique telltale strings) 
of known malicious executables to generate detection mod- 
els.  Signature-based methods create a unique tag  for each 
malicious program so that future examples of it can be cor- 
rectly classified with  a small error rate. These methods do 
not  generalize well  to  detect  new  malicious  binaries  be- 
cause they are created to give a false positive rate as close to 
zero as possible. Whenever a detection method generalizes 
to new instances, the tradeoff is for a higher false positive 
rate. Heuristic classifiers are generated by  a group of virus 
experts to  detect  new  malicious  programs.  This kind  of 
analysis can be time-consuming and oftentimes still fail to 
detect new malicious executables. 
We  designed  a  framework  that  used  data  mining algo- 
rithms to train multiple classifiers on a set of malicious and 
benign  executables to detect new  examples.  The binaries 
were first statically analyzed to extract properties of the bi- 
nary,  and  then  the  classifiers trained  over a  subset of  the 
data. 
Our goal  in  the evaluation of  this method  was  to simu- 
late  the  task  of  detecting  new  malicious executahles.  To 
do this we  separated our data into two sets:  a truining  set 
and a test set with  standard cross-validation methodology. 
The training set was used by the data mining algorithms to 
generate  classifiers to  classify  previously  unseen  binaries 
as malicious or benign. A test set is a subset of dataset that 
had no examples in  it that were seen during the training of 
an  algorithm.  This subset was used to test  an algorithms’ 
performance over similar, unseen data and its performance 
1081-601 1/01 $10.00 0 2001 IEEE 
38 
over new  malicious executables.  Both  the test  and train- 
ing data were malicious executables gathered from public 
sources. 
We implemented a traditional signature-based algorithm 
to compare with the the data mining algorithms over new 
examples.  Using standard statistical cross-validation tech- 
niques,  our  data  mining-based  method  had  a  detection 
rate of 97.76%-more 
than  double the detection rate of  a 
signature-based scanner over a set of  new  malicious exe- 
cutables. Comparing our method to industry heuristics can- 
not be done at this time because the methods for generating 
these heuristics  are not  published  and  there  is  no equiva- 
lent or statistically comparable dataset to which both tech- 
niques are applied.  However, the framework we provide is 
fully automatic and could  assist  experts in  generating the 
heuristics. 
2  Background 
Detecting malicious executables is  not  a  new  problem  in 
security. Early methods used signatures to detect malicious 
programs.  These signatures  were composed of many dif- 
ferent properties:  filename, text  strings, or byte code.  Re- 
search also centered on protecting the system from the se- 
curity holes that these malicious programs created. 
Experts  were  typically employed to analyze suspicious 
programs by  hand.  Using their expertise,  signatures were 
found that made a malicious executable example different 
from other malicious executables or benign programs. One 
example of this type of analysis was performed by Spafford 
[24] who analyzed the Internet Worm and provided detailed 
notes on its spread over the Internet, the unique signatures 
in the worm’s code, the method of the worm’s attack, and a 
comprehensive description of system failure points. 
Although accurate, this method of analysis is expensive, 
and slow. If only a small set of malicious executables will 
ever circulate then  this  method  will  work  very  well,  but 
the  Wildlist [22] is  always changing and expanding.  The 
Wildlist  is a  list  of malicious  programs that  are currently 
estimated to be circulating at any given time. 
Current  approaches  to  detecting  malicious  programs 
match  them  to  a  set of  known malicious programs.  The 
anti-virus community relies  heavily  on  known  byte-code 
signatures to  detect  malicious  programs.  More recently, 
these byte sequences were determined by automatically ex- 
amining known malicious binaries with probabilistic meth- 
ods. 
At IBM, Kephart and Arnold [9] developed a statistical 
method  for automatically extracting malicious  executable 
signatures. Their research was based on speech recognition 
algorithms and  was  shown to perform almost  as good as 
a human expert at detecting known malicious executables. 
Their algorithm was eventually packaged with IBM’s anti- 
virus software. 
Lo  et  al.  (151 presented  a  method  for  filtering  mali- 
cious code based  on  “tell-tale  signs” for detecting mali- 
cious code. These were manually engineered based on ob- 
serving the characteristics of malicious code. Similarly, fil- 
ters for detecting properties of malicious executables have 
been  proposed  for UNIX  systems  [IO]  as  well  as  semi- 
automatic methods for detecting malicious code [4]. 
Unfortunately, a  new  malicious program may  not  con- 
tain  any  known  signatures  so  traditional  signature-based 
methods may not detect a new malicious executable. In an 
attempt to solve this problem, the anti-virus industry gen- 
erates  heuristic classifiers by  hand  [8].  This process can 
be  even  more  costly  than  generating  signatures, so  find- 
ing  an  automatic method to  generate classifiers has  been 
the  subject  of  research  in  the  anti-virus  community.  To 
solve this problem, different IBM researchers applied Arti- 
Jicial Neural Networks (ANNs) to the problem of detecting 
boot sector malicious binaries [25]. An ANN is a classifier 
that models neural networks explored in human cognition. 
Because  of the  limitations  of the  implementation of their 
classifier, they were unable to analyze anything other than 
small boot sector viruses which comprise about 5% of all 
malicious binaries. 
Using an ANN classifier with all bytes from the boot sec- 
tor malicious executables as input, IBM researchers were 
able  to  identify  8 0 4 5 %  of  unknown  boot  sector  mali- 
cious executables successfully with a low false positive rate 
(< 1%). They were unable to find a way to apply ANNs to 
the other 95% of computer malicious binaries. 
In similar work, Arnold and Tesauro [ 11 applied the same 
techniques to Win32 binaries, but because of limitations of 
the ANN classifier they were unable to have the comparable 
accuracy over new Win32 binaries. 
Our method  is  different  because  we  analyzed  the  en- 
tire set of malicious executables instead of only boot-sector 
viruses, or only Win32 binaries. 
Our technique is similar to data mining techniques that 
have already been  applied  to Intrusion  Detection  Systems 
by Lee et al.  [ 13, 141.  Their methods were applied  to sys- 
tem  calls and  network data to learn how to detect new  in- 
trusions.  They reported  good detection rates as a result  of 
applying data mining to the problem of IDS. We applied a 
similar framework to the problem  of  detecting new  mali- 
cious executables. 
3  Methodology 
The goal  of  this  work was  to  explore a  number of  stan- 
dard data mining techniques to compute accurate detectors 
for new (unseen) binaries.  We gathered  a large set of pro- 
grams from public sources and separated the problem into 
two classes: nialicious and Oeriigri executables.  Every ex- 
ample in our data set is a Windows or MS-DOS format ex- 
ecutable, although the framework we present  is applicable 
to other formats.  To  standardize  our data-set, we  used an 
updated MacAfee’s [ 161 virus scanner and labeled our pro- 
39 
grams as either malicious or benign executables. Since the 
virus scanner was  updated  and  the  viruses  were obtained 
from public sources, we assume that the virus scanner has 
a signature for each malicious virus. 
We split the dataset into two subsets: the training set and 
the test set.  The data mining algorithms used  the training 
set while  generating  the  rule  sets.  We  used  a test  set  to 
check the accuracy of the classifiers over unseen examples. 
Next,  we automatically extracted a binary  profile from 
each example in  our dataset, and from the binary  profiles 
we extractedfeatures  to use with classifiers. In a data min- 
ing framework, features are properties extracted from each 
a 
example in  the data set-such 
classifier can use to generate detection models.  Using dif- 
ferent features, we  trained  a set of  data mining classifiers 
to distinguish between benign and malicious programs.  It 
should be noted that the features extracted were static prop- 
erties of the binary and did not require executing the binary. 
The  framework supports  different  methods  for  feature 
extraction and  different data  mining classifiers.  We  used 
system  resource  information,  strings  and  byte  sequences 
that  were extracted from the malicious executables in  the 
data set as different types of features.  We  also used  three 
learning algorithms: 
as byte sequences-that 
0  an inductive rule-based learner that generates 
rules based on feature attributes. 
0  a probabilistic method that generates probabilities that 
an example was in a class given a set of features. 
0  a  multi-classifier  system  that  combines  the  outputs 
from several classifiers to generate a prediction. 
To compare the data mining methods with  a traditional 
signature-based method, we designed an automatic signa- 
ture generator.  Since the virus scanner that we used  to la- 
bel  the data set had  signatures for every malicious exam- 
ple  in  our data set, it was  necessary to implement  a simi- 
lar signature-based method  to compare with  the data min- 
ing algorithms.  There was no way to use  an  off-the-shelf 
virus scanner and simulate the detection of new malicious 
executables because  these commercial scanners contained 
signatures for all the malicious executables in our data set. 
Like the data mining algorithms, the signature-based algo- 
rithm was only allowed to generate signatures over the set 
of training data.  This allowed our data mining framework 
to be fairly compared to traditional scanners over new data. 
To quantitatively express the performance of our method 
we show tables with the counts for true positives (TP), true 
negatives  (TN), false positives  (FP), and false negatives 
(FN). A  true positive  is  a  malicious  example that  is  cor- 
rectly  tagged as malicious, and  a true negative  is a benign 
example that is correctly classified. A false positive is a be- 
nign program that has been mislabeled  by  an algorithm as 
a malicious program, while a  false negative is a malicious 
executable that has been misclassified as a benign program. 
To evaluate the performance, we compute the false pos- 
itive rate and the detection rate.  The false positive rate is 
the number of benign examples that are mislabeled as mali- 
cious divided by the total number of benign examples. The 
detection rate is the number of malicious examples that are 
caught divided by the total number of malicious examples. 
3.1  Dataset Description 
Our data set  consisted  of  a  total  of 4,266 programs split 
into 3,265 malicious binaries  and  1,001 clean  programs. 
There were no duplicate programs in the data set and every 
example in the set is labeled either malicious or benign by 
the commercial virus scanner. 
The malicious executables were downloaded from var- 
ious  FTP  sites  and  were  labeled  by  a  commercial  virus 
scanner with  the correct class label  (malicious or benign) 
for  our  method.  5%  of  the  data  set  was  composed  of 
Trojans  and  the  other  95%  consisted  of  viruses.  Most 
of  the  clean  programs  were  gathered  from  a  freshly  in- 
stalled Windows  98 machine running MSOffice  97  while 
others are  small  executables downloaded  from the  Inter- 
net.  The entire  data  set  is  available  from  our Web  site 
http://www. cs. Columbia. edu/ids/mef/sofrware/. 
We  also  examined  a  subset  of  the  data  that  was  in 
Portable Executable (PE) [ 171 format. The data set consist- 
- 
ing of PE format executables was composed of 206 benipn 
programs and 38 malicious  executables. 
After  verification  of  the  data  set  the  next  step  of  our 
method was to extract features from the programs. 
4  Feature Extraction 
In  this section we detail all of our choices of features.  We 
statically  extracted different features that  represented  dif- 
ferent information contained within each binary. These fea- 
tures were then used by the algorithms to generate detection 
models. 
We first examine only the subset of PE executables using 
LibBFD. Then  we  used  more  general methods  to extract 
features from all types of binaries. 
4.1  LibBFD 
Our first intuition into the problem was to extract informa- 
tion  from the  binary  that would dictate its behavior.  The 
problem of predicting a program’s behavior can be reduced 
to the halting problem  and hence is undecidable [2].  Per- 
fectly predicting  a program’s behavior is unattainable but 
estimating what a program can or cannot do is possible. For 
instance if a Windows executable does not call the User In- 
terfaces Dynamically Linked Library(USER32.DLL), then 
we could assume that the program does not have the stan- 
dard Windows  user  interface.  This is  of  course  an  over- 
simplification of the problem because the author of that ex- 
40 
ample could have written or linked to another user interface 
library, but it did provide us with some insight to an appro- 
priate feature set. 