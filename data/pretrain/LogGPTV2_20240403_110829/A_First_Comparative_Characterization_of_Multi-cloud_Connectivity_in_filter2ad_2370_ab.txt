To infer inter-AS interconnections, the resulting traceroute hops from our
measurements were translated to their corresponding AS paths using BGP pre-
ﬁx announcements from Routeviews and RIPE RIS [59,67]. Missing hops were
attributed to their surrounding ASN if the prior and next hop ASNs were iden-
tical. The existence of IXP hops along the forward path was detected by match-
ing hop addresses against IXP preﬁxes published by PeeringDB [56] and Packet
Clearing House (PCH) [55]. We mapped each ASN to its corresponding ORG
number using CAIDA’s AS-to-ORG mapping dataset [32]. Lastly, the inter-AS
interconnection segments are identiﬁed using the latest version of bdrmapIT [3].
3.4 Limitations and Ethical/Legal Considerations
Our study is US-centric and limited by the geographic span of our multi-cloud
deployment as well as the number of third-party connectivity providers that we
examine. The high cost for connecting multiple clouds using TPP connections
prevents us from having a global-scale deployment and performing experiments
that involve diﬀerent TPP providers. For example, for each 1 Gbps link to a CP
network, third-party providers charge anywhere from about 300 to 700 USD per
3 In Sect. 5 we highlight that our inter-cloud measurements do not exit the source and
destination CP’s network.
A First Comparative Characterization of Multi-cloud Connectivity
199
Fig. 2. Distribution of RTT between AWS, GCP, and Azure for intra (left) and inter
(right) region paths. (Color ﬁgure online)
month [48,53,58]4. While limited in scale, the deployment that we consider in
this study is nevertheless representative of a typical multi-cloud strategy adopted
by modern enterprises with a US-wide footprint [49].
Our study does not raise any ethical issues. Overall, since the goal of this
study is to measure and improve multi-cloud connectivity without attributing
particular features to any of the utilized third-party providers and CPs, we are
not in violation of any of their terms of service. In particular, we obfuscate,
and wherever possible, we omit all information that can be used to identify
the colocation and third-party connectivity providers. This information includes
names, supported measurement APIs, costs, time and date of measurements,
topology information, and any other potential identiﬁers.
4 Characteristics of C2C Routes
In this section, we characterize the performance of C2C routes (i.e., latency
and throughput) and attribute the observed characteristics to connectivity and
routing.
4 Note that these price points do not take into consideration the additional charges
that are incurred by CPs for establishing connectivity to their network.
200
B. Yeganeh et al.
4.1 Latency Characteristics
CPP Routes Exhibit Lower Latency Than TPP Routes and Are Sta-
ble. Figure 2 depicts the distribution of RTT values (using letter-value plots [31];
see Appendix A.1) between diﬀerent CPs across diﬀerent connectivity options.
The rows (from top to bottom) correspond to AWS, GCP, and Azure as the
source CP, respectively. Intra-region (inter-region) measurements are shown in
the left (right) columns, and CPP (TPP) paths are depicted in blue (orange).
The ﬁrst two characters of the x-axis labels encode the source CP region and
the remaining characters encode the destination CP and region. From these ﬁg-
ures, we see that CPP routes typically exhibit lower medians of RTT compared
to TPP routes, suggesting that CPP routes traverse the CP’s optical private
backbone. We also observe a median RTT of ∼2 ms between AWS and Azure
VMs in California which is in accordance with the relative proximity of their dat-
acenters for this region. The GCP VM in California has a median RTT of 13 ms
to other CPs in California, which can be attributed to the geographical distance
between GCP’s California datacenter in LA and the Silicon Valley datacenters
for AWS and Azure. Similarly, we notice that the VMs in Virginia all exhibit
low median RTTs between them. We attribute this behavior to the geographical
proximity of the datacenters for these CPs. At the same time, the inter-region
latencies within a CP are about 60 ms with the exception of Azure which has a
higher median of latency of about 67 ms. Finally, the measured latencies (and
hence the routes) are asymmetric in both directions albeit the median of RTT
values shows latency symmetry ( 9
10)
10 and 9
10
are not prescriptive and are derived based on the latency distributions depicted
in Fig. 3.
lei). The fractions (i.e., 1
4.2 Throughput Characteristics
CPP Routes Exhibit Higher and More Stable Throughput than TPP
Routes. Figure 4 depicts the distribution of throughput values between diﬀer-
ent CPs using diﬀerent connectivity options. While intra-region measurements
A First Comparative Characterization of Multi-cloud Connectivity
203
Fig. 5. Distribution of loss-rate between AWS, GCL, and Azure for intra (left) and
inter (right) region paths.
tend to have a similar median and variance of throughput, we observe that
for inter-region measurements, TPPs exhibit a lower median throughput with
higher variance. Degradation of throughput seems to be directly correlated with
higher RTT values as shown in Fig. 2. Using our latency measurements, we also
approximate loss-rate to be 10−3 and 10−4 for TPP and CPP routes, respec-
tively. Using the formula of Mathis et al. [47] to approximate TCP throughput5,
we can obtain an upper bound for throughput for our measured loss-rate and
latency values.
Using Mathis et al. model, the upper bound of throughput for an MSS of
1460 bytes, a 70 ms latency and loss-rate of 10−3 (corresponding to the average
measured values for TPP routes between two coasts) is about 53 Mb/s. While
this value is higher than our interface/link bandwidth cap of 50 Mb/s, bursts
of packet loss or transient increases in latency could easily lead to sub-optimal
TCP throughput for TPP routes.
Why Do CPP Routes Have Better Throughput than TPP Routes? Our
initial methodology for measuring loss-rate relied on our low-rate ping probes
5 We do not have access to parameters such as TCP timeout delay and number of
acknowledged packets by each ACK to use more elaborate TCP models (e.g., [54]).
204
B. Yeganeh et al.
(outlined in Sect. 3.3). While this form of probing can produce a reliable estimate
of average loss-rate over a long period of time [66], it doesn’t capture the dynam-
ics of packet loss at ﬁner resolutions. We thus modiﬁed our probing methodology
to incorporate an additional iperf3 measurement using UDP probes between all
CP instances. Each measurement is performed for 5 s and packets are sent at a
50 Mb/s rate.6 We measure the number of transmitted and lost packets during
each second and also count the number of packets that were delivered out of
order at the receiver. We perform these loss-rate measurements for a full week.
Based on this new set of measurements, we estimate the overall loss-rate to be
5∗10−3 and 10−2 for CPP and TPP paths, respectively. Moreover, we experience
0 packet loss in 76% (37%) of our sampling periods for CPP (TPP) routes, indi-
cating that losses for CPP routes tend to be more bursty than for TPP routes.
The bursty nature of packet losses for CPP routes could be detrimental to real-
time applications which can only tolerate certain levels of loss and should be
factored in by the client. The receivers did not observe any out-of-order packets
during our measurement period. Figure 5 shows the distribution of loss rate for
various paths.
The rows (from top to bottom) correspond to AWS, GCP, and Azure as the
source CP, respectively. Intra-region (inter-region) measurements are shown in
the left (right) columns, and CPP (TPP) paths are depicted in blue (orange).
We observe consistently higher loss-rates for TPP routes compared to their CPP
counterparts and lower loss-rates for intra-CP routes in Virginia compared to
California. Moreover, paths destined to VMs in the California region show higher
loss-rates regardless of where the traﬃc has been sourced from, with asymmet-
rically lower loss-rate on the reverse path indicating the presence of congested
ingress points for CPs within the California region. We also notice extremely
low loss-rates for intra-CP (except Azure) CPP routes between the US east and
west coasts and for inter-CP CPP routes between the two coasts for certain CP
pairs (e.g., AWS CA to GCP VA or Azure CA to AWS VA).
4.3 Main Findings
Our measurement experiments reveal two interesting ﬁndings. First, CPP routes
are better than TPP routes in terms of latency as well as throughput. Within
a multi-cloud setting, TPPs can serve multiple purposes, including providing
connectivity towards CPs from colo facilities that CPs aren’t present, lowering
inter-cloud traﬃc costs [7,8], and providing private inter-cloud connectivity over
private address spaces. Second, the better performance of CPP routes as com-
pared to their TPP counterparts can be attributed to (a) the CPs’ rich (private)
connectivity in diﬀerent regions with other CPs (traﬃc is by-passing the BEP
Internet altogether) and (b) more stable and better provisioned CP (private)
backbones.
6 In an ideal setting, we should not experience any packet losses as we are limiting
our probing rate at the source.
A First Comparative Characterization of Multi-cloud Connectivity
205
5 Discussion
CPs Are Heterogeneous in Handling Path Measurements. Measuring
the number of observed AS/organizations (excluding hops utilizing private IP
addresses) for inter-cloud, intra-cloud, and cloud-to-LG routes, we observed that
of the three CPs, only AWS used multiple ASNs (i.e., ASes 8987, 14618, and
16509) and that there are striking diﬀerence between how CPs respond to tracer-
oute probes. In particular, GCP does not expose any of its routers unless the
target address is within another GCP region; Azure does not expose its internal
routers except for their border routers that are involved in peering with other
networks; and AWS relies heavily on private/shared IP addresses for its internal
network.
CPs Are Tightly Interconnected with Each Other in the US. To check
the absence of transit ASes along our measured C2C paths more thoroughly,
we conducted a more extensive measurement study by launching VM instances
within all US regions for our three target CP networks and performing UDP and
ICMP paris-traceroutes between all VM instances using scamper. After annotat-
ing the traceroutes as described in Sect. 3.3, in terms of AS/organization-level
routes, we only observe organizations corresponding to the three target CPs as
well as IXP ASNs for Coresite Any2 and Equinix. All organization-level routes
passing through an IXP correspond to paths that are sourced from Azure and
are destined to AWS. These measurements further conﬁrm our initial observa-
tion regarding the rich connectivity of our three large CPs and their tendency
to avoid exchanging traﬃc through the public Internet.
Taking an Enterprise-to-Cloud (E2C) Perspective. Instead of the C2C
perspective shown in Fig. 1, we also considered an enterprise-to-cloud (E2C)
perspective and report preliminary results for this scenario in Appendix A.2.
6 Summary
In this paper, we perform a ﬁrst-of-its-kind measurement study to understand the
tradeoﬀs between three popular multi-cloud connectivity options (CPP vs. TPP
vs. BEP). Based on our cloud-centric measurements, we ﬁnd that CPP routes
are better than TPP routes in terms of latency as well as throughput. The better
performance of CPPs can be attributed to (a) CPs’ rich connectivity in diﬀerent
regions with other CPs (by-passing the BEP Internet altogether) and (b) CPs’
stable and well-designed private backbones. In addition, we ﬁnd that TPP routes