pipeline, effectively doubling the width of the entire datapath across
all stages. In dRMT, by contrast, supporting IPC = 2 only requires
an additional packet buffer and a few muxes in each processor (§6),
resulting in only a modest increase in chip area.
Evaluating RMT’s performance. For both the fine-grained and de-
fault RMT architectures, we formulate an ILP that handles the match
and action capacity constraints described above and dependency con-
straints captured by the operation dependency graphs. Our RMT ILP
is similar to that of Jose et al. [22], but does not consider per-stage
table capacity constraints, in effect simulating an RMT pipeline with
fully disaggregated memory. This implies that dRMT’s actual im-
provements relative to RMT—with local, non-shareable memory in
each stage—will be higher than the numbers reported here. We use
the RMT ILP to calculate the minimum number of pipeline stages S
for RMT that satisfies both the capacity and dependency constraints.
Evaluating dRMT’s performance. For dRMT, we run the ILP
described in §3, using our heuristics [5] to accelerate the ILP’s run-
time. We use our binary-search procedure described in §3 to calculate
the minimum scheduling period P such that a single processor can
receive a packet every P clock cycles.
Metrics. For dRMT, if the minimum scheduling period is P, then
a single processor can receive a packet at most once every P clock
cycles. This implies that at least P processors are required to support
a throughput of one packet per clock cycle. For RMT, assuming each
stage can process a packet every clock cycle, at least S stages are
required to run the program at one packet per clock cycle.
The minimum number of total threads (across all stages) for RMT
is obtained by multiplying the minimum number of stages by the
sum of the RMT match and action latencies. For dRMT, it is output
by the ILP as its optimization objective.
Remark. The ILPs for both RMT and dRMT assume that both
branches of a condition are always speculatively executed (see §3).
Scheduling mutually exclusive operations together (which cannot
simultaneously execute for any packet) may reduce the number of
processors/stages and threads for both architectures.
4.2 Experimental results
Comparing dRMT with RMT on real P4 programs. Tables 2
and 3 show the minimum number of processors and threads required
to support a throughput of one packet per clock cycle on four P4
programs. Three of them are derived from switch.p4, an open-source
P4 program [13]. They correspond to switch.p4’s ingress pipeline, its
egress pipeline, and a combined ingress+egress program that runs on
a single shared physical pipeline to improve utilization, as suggested
by RMT [16]. The combined switch.p4 program effectively improves
utilization through statistical multiplexing; it creates opportunities to
run a highly utilized stage from one pipeline with an underutilized
stage from the other in the same hardware stage.
We also use a proprietary P4 program from a large switching
ASIC manufacturer. This program has 50% more lines of code
than switch.p4, implements all but a few of switch.p4’s forwarding
features, and some that switch.p4 does not have. For anonymity, we
normalize the critical path for latency and the lower bound on the
number of processors for the proprietary program to one.
The results show that as we progress from RMT towards dRMT
(IPC = 2), the minimum number of processors and threads both
decrease, because more disaggregation enables more flexible sched-
uling. The results also show how an IPC = 2 is important to take
advantage of inter-packet parallelism within the same processor.
Neither an RMT stage nor a dRMT processor with IPC = 1 can ex-
ploit this kind of parallelism. IPC = 2 also reaches the upper bound
on throughput for these programs, showing how a small degree of
inter-packet parallelism is sufficient to extract high throughput.
dRMT’s greatest gains are on programs that cause an imbal-
anced pipeline of match and action operations—and hence wasted
resources. dRMT compacts such programs (e.g., switch.p4’s egress
alone) into a smaller number of processors. When the program al-
ready has a balanced RMT pipeline (e.g., combining the ingress and
egress switch.p4 into one program), dRMT’s gains are lower.
dRMT: Disaggregated Programmable Switching
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
P4
program
ingress
egress
combined
proprietary
RMT RMT
fine
17
11
21
2.0
18
12
22
2.0
dRMT
(IPC = 1)
dRMT
(IPC = 2)
Lower
bound
17
11
21
2.0
15
7
21
1.0
15
7
21
1.0
Table 2: Minimum number of processors to achieve line rate
on each architecture. The lower bound for the proprietary pro-
gram is normalized to one for anonymity.
P4
program
ingress
egress
combined
proprietary
RMT RMT
fine
340
220
420
2.82
360
240
440
2.82
dRMT
(IPC = 1)
dRMT
(IPC = 2)
245
217
243
1.04
243
198
243
1.01
Critical
path
243
197
243
1.0
Table 3: Minimum number of threads to achieve line rate on
each architecture. The critical path latency numbers are based
on ∆M and ∆A for dRMT and provide a lower bound on the
number of threads necessary. The critical path for the propri-
etary program is normalized to one for anonymity.
Comparison on random ODGs. To compare dRMT with RMT on
a larger variety of possible P4 programs, we generate random ODGs
based on the characteristics of switch.p4’s ODG. Specifically, we
generate 100 different ODGs that reflect different P4 programs of
varying size. For each ODG, we report the minimum number of
processors/stages required to support the corresponding program at
line rate. We generate the ODGs as follows:
(1) Generate a random directed acyclic graph with 100 nodes. An
edge (i, j) where i < j exists with probability p. p is chosen
so that the total number of edges is 500 on average.
(2) Then, the nodes are visited according to a topological sort.
Each non-leaf node is randomly chosen to be either a default
action with probability 0.15; or a conditional node, i.e., a
single-field action node representing a predicate, with proba-
bility 0.25; or split into a match node followed by an action
node with probability 0.6. Likewise, each leaf node is either
a default action with probability 0.15, or split into a match
node followed by an action node with probability 0.85.
(3) For a non-conditional action node, the number of fields is first
sampled from a geometric distribution with a mean of 4 fields
and then truncated to the interval [1, 32].
(4) For a match node, the key width is first sampled from a geo-
metric distribution with a mean of 106 bits and then truncated
to the interval [80, 640].
All the parameters above, i.e., number of edges, probabilities for
node types, and parameters for the truncated geometric distribution
are chosen based on distributions observed in switch.p4. We choose
a geometric distribution to capture our empirical observation from
switch.p4 that higher key widths or action fields are less likely.
Figure 6 illustrates the results. As expected, dRMT with IPC = 2
provides more flexibility and results in the lowest number of required
processors. dRMT with IPC = 1 follows and shows a consistent
advantage over RMT. Across all 100 randomly generated graphs,
dRMT with IPC = 2 had an average reduction of 10% in the number
of processors relative to RMT and a maximum of 30%.
Figure 6: Histogram showing the minimum number of proces-
sors for dRMT and RMT on 100 random ODGs.
Figure 7: Throughput on switch.p4’s egress pipeline. dRMT’s
performance scales linearly with processors and achieves the
lower bound, while RMT displays performance cliffs.
dRMT eliminates performance cliffs. Once we determine both
S and P, which respectively enable RMT and dRMT to enable a
throughput of one packet per clock cycle, we can calculate the
throughput th(N ) as a function of the number of processors N . 2
thRMT (N ) = min(1, 1/(⌈S/N⌉))
thdRMT (N ) = min(1, N /P )
(9)
(10)
The throughputs are capped at one because it is challenging to build
on-chip memories to support two or more reads/writes per clock
cycle—and hence two or more packets in a cycle.
Figure 7 illustrates the effect of decreasing the number of proces-
sors on the throughput. It plots th(N ) for each architecture using
switch.p4’s egress pipeline. The figure illustrates the performance
cliff for both RMT variants and the linear degradation in throughput
for dRMT. The results for switch.p4 ingress and combined are not
shown, but are similar. For example, for the ingress program, RMT’s
throughput drops to 50% as the number of processors decreases to
17 from 18.
dRMT ILP run-time evaluation. We measure the time taken by
the dRMT ILP as a function of the number of processors N , while
targeting a fixed throughput of one packet per clock cycle, using the
2We assume the ability to recirculate packets back into the pipeline in RMT while
reducing throughput by a factor of the number of recirculations.
1011121314151617Minimum number of processors051015202530354045OccurrencesRMT(Coarse)RMT(Fine)dRMT(IPC=1)dRMT(IPC=2)02468101214Processors0.00.20.40.60.81.0Packets per cycleRMT(Coarse)RMT(Fine)dRMT(IPC=1)dRMT(IPC=2)SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
S. Chole et al.
Figure 8: ILP run-time in dRMT (IPC = 1) as a function of the
number of processors for a throughput of one packet per clock
cycle. The dashed lines show the minimum number of proces-
sors required to run at one packet per clock cycle. The non-
monotonicity is an artifact of the ILP’s discrete-valued nature.
three open-source switch.p4 programs. We carried out our measure-
ments on an HP ProLiant DL785G5 machine with 8 AMD quad-core
processors (2.2 GHz), each with a shared 2 MB L3 cache, and 256
GB DDR2 RAM at 533 MHz.
Figure 8 depicts the results for dRMT with IPC = 1. The ILP
run-time drops quickly (by two to three orders of magnitude) as
soon as the number of processors is slightly larger than the minimum
number necessary to run the program at one packet per clock cycle.
The reason is that the ILP is much easier to solve when there is a
little bit of slack. We do not show dRMT with IPC = 2 because the
run-times with IPC = 2 never exceeded a few minutes regardless
of the number of processors. This is because IPC = 2 makes the
scheduling problem easier by providing more flexibility.
5 HARDWARE ARCHITECTURE
The dRMT architecture consists of a set of match-action processors,
connected to a set of memory clusters through a crossbar. Each
match-action processor contains processing elements to (1) generate
match keys for matches and (2) update packet fields for actions.
While the overall architecture of dRMT is different from RMT, the
processing elements for matches and actions are similar. Hence, we
adopt the design of these processing elements as is from RMT.
The key difference between RMT and dRMT is that dRMT runs
a packet to completion once it is assigned to a processor, instead
of moving it from stage to stage. While run-to-completion provides
dRMT with more flexibility (§4), it also requires each processor to
store the entire program. This is in contrast to RMT, where each
pipeline stage only stores the fragment of the program that is ex-
ecuted within that pipeline stage. As a result, dRMT’s hardware
design requires some optimization to avoid additional cost in digital
logic—and therefore silicon area and power. This section focuses
on the optimizations within the individual match-action processors
(§5.1), the memory clusters (§5.2), and the shared crossbar (§5.3)
connecting processors to clusters.
We mention many constants in this section, such as the number
of bits in the packet vector, the number of search key bits that can
be issued by a processor every clock cycle, etc. Unless otherwise
Figure 9: The design of a dRMT processor.
stated, we choose these design parameters to be the same as the pub-
lished RMT architecture [16]. This enables a more straightforward
comparison between our work and theirs.
5.1 Match-action processors
Each match-action processor admits one packet every P clock cycles,
where P is the scheduling period. If the maximum program latency