problems or operators conduct planned maintenance. We exclude
the latter from our analysis, because maintenance tickets are un-
likely to be triggered by performance or availability problems.
Given that ticket logs capture a wide-range of network issues,
operators view tickets as a valuable measure of network health. In
particular, operators from the OSP indicated that number of tickets
is a useful metric. Other metrics computed from network tickets
(e.g., number of high impact problems, mean time to resolution,
etc.) are less useful because of inconsistencies in ticketing prac-
tices: e.g., impact levels are often subjective, and tickets are some-
times not marked as resolved until well after the problem has been
ﬁxed. As future work, we plan to explore how to accurately obtain
more ﬁne-grained health measures using tools like NetSieve [26].
Property
Months
Networks
Services
Devices
Conﬁg snapshots O(100K), ≈450GB
Tickets
Value
17, Aug 2013 – Dec 2014
850+
O(100)
O(10K)
O(10K), ≈80MB
Table 2: Size of datasets
3. MANAGEMENT PRACTICES TODAY
Today, there is little consensus in the community on the impact
of different management practices on networks’ health. We con-
ducted two studies that demonstrate this lack of consensus. One
study is qualitative in which we surveyed network operators re-
garding which practices mattered for network health. The other is
quantitative in which we systematically characterized management
practices in use at a large OSP. The diversity in opinions and actual
practices uncovered by these studies motivate the need for MPA.
We describe these studies below.
3.1 Operators’ Perspectives
Our operator survey covered 51 network operators, whom we re-
cruited through the NANOG mailing list (45 operators), from our
campus network (4), and from the large OSP (2). For ten of the
practices in Table 1, we asked operators how much they thought
each practice would matter to their networks’ health. Results of our
survey are summarized in Figure 2. We see clear consensus in just
one case—number of change events. Otherwise, we note a general
diversity of opinion: for several practices—e.g., network size, num-
ber of models used, and inter-device conﬁguration complexity—the
fractions of operators who said the practice has low vs. high impact
are roughly the same. We observed similar diversity even among
operators of the OSP and those of the campus network. A handful
of operators indicated that they are unsure of the impact of certain
management practices.
We also asked operators to “write-in” other practices they believe
to have a high impact on network health. The responses included:
number of operators, skill levels of operators, documentation and
training provided, extent of auditing tools, and extent of pre-change
analysis. Unfortunately, these metrics are difﬁcult to quantify, be-
cause they relate to the operators (e.g., skill level) rather than the
networks. We leave the integration of such metrics into MPA as
future work.
3.2 Management Practices in a Large OSP
The diversity in opinions uncovered by the survey could be due
to the fact that the operators manage different networks. It is possi-
ble that operators know which practices impact their own network,
and it is just that this set of practices differs across organizations but
is internally consistent. However, our characterization of manage-
ment practices inside an OSP suggests this is not the case; we ﬁnd
398signiﬁcant heterogeneity in the practices inside the OSP. While de-
tailed characterization is presented in Appendix A, we summarize
example ﬁndings here.
The OSP owns 850+ networks that are managed based on doc-
umented “best practices.” Each network hosts one or more Web
services or interconnects other networks. Our datasets cover a 17
month period from August 2013 through December 2014. Table 2
shows its key aspects. For conﬁdentiality, we do not list exact num-
bers for several measures.
Design Practices. We ﬁnd the control and data planes of networks
to be quite heterogeneous in their physical and logical structure.
For instance, while the median network’s hardware and ﬁrmware
heterogeneity is low (entropy metric  0.67) (Figure 11(a)). Likewise, the number of data and con-
trol plane protocols conﬁgured across networks is distributed al-
most uniformly between 1 and 8 (Figure 11(b)). Perhaps most in-
terestingly the overall conﬁguration complexity metrics (intra- and
inter-device references) vary by 1-2 orders of magnitude across net-
works (Figure 11(d)). Per the operator survey (Figure 2), many of
these metrics may have a modest impact on health.
Operational Practices. We similarly ﬁnd signiﬁcant diversity in
what and how networks are changed. For instance, changes to
router stanzas are not as common for the median network (where
5% of changes are to router stanzas), but such changes are quite
prevalent (>0.5 of all changes) in about 5% of the networks (Fig-
ure 12(c)). Likewise, number of change events and changes in-
volving middleboxes—both of which were considered impactful
by operators (Figure 2)—show signiﬁcant diversity: e.g., change
event count in the 10th vs 90th percentile network is 3 vs 34 (Fig-
ure 12(e)). The modality of changes is also diverse: in 40% of the
networks at least half the changes are automated, but in 10% of the
networks only 15% of changes are automated (Figure 12(d)).
This level of diversity within the networks of the same organi-
zation suggests that operators have little agreement on which prac-
tices are good. This lack of agreement is conﬁrmed by our con-
versation with the operators of the OSP. These conversations also
conﬁrm that the operators do not have a way to map adjustments
in management practices to shifts in network health. Helping such
operators is the goal of MPA.
4. MPA OVERVIEW
Given the diversity and complexity of management practices, we
need a systematic framework to understand their impact on network
health, and in turn improve management practices. MPA is one
such framework.
MPA has two goals. The ﬁrst is to help operators derive the top
k management practices that impact the health (e.g., problem inci-
dence) of their networks. Armed with this information, operators
can develop suitable best practices to improve organization-wide
design and operational procedures. The main challenges here are
that: (i) management practices can differ signiﬁcantly in the nature
of their relationship with network health; and (ii) many practices
are often related, impacting both one another and network health.
Thus, systematically distilling the “heavy hitters” is not easy. In
Section 5, we show how to overcome these challenges by using
mutual information to uncover statistical dependencies, and near-
est neighbor matching of propensity scores, in the context of quasi-
experimental designs (QEDs), to identify causal relationships.
The second goal of MPA is to help operators predict, in an ongo-
ing fashion, what impact the current set of management practices
have on the health of individual networks. This goes beyond fo-
O(10)
O(10)
s
t
e
k
c
T
i
f
o
#
0
4
3
2
6
# of L2 Protocols
5
s
t
e
k
c
T
i
f
o
#
0
0
O(10)
# of Models
(a) No. of L2 Protocols
(b) No. of Models
O(10)
O(10)
s
t
e
k
c
T
i
f
o
#
s
t
e
k
c
T
i
f
o
#
0
0
Frac. events
1
w/ interface change
(c) Frac. of events with interface
change
0
1
6
# of Roles
(d) No. of Roles
Figure 4: Tickets based on management practices; boxes show
25th & 75th %tile, while whiskers show 2x the interquartile
range; red (dark) line shows the average number of tickets,
while orange (light) line shows the median
cusing on the top practices; it incorporates the effects of one-off
deviations from established procedures, as well as the effects of
management practices whose impact on network health manifests
only in a narrow set of situations. Armed with such metrics, oper-
ators can closely monitor networks that are predicted to have more
problems and be better prepared to deal with failures. The main
challenge is drawing meaningful conclusions despite limited data
for individual networks, especially data for “unhealthy” networks
which is often the minority. In Section 6, we show how to over-
come this challenge, and build models that accurately predict the
health of individual networks, using boosting and oversampling of
unhealthy network data.
5. MANAGEMENT PRACTICES THAT
IMPACT NETWORK HEALTH
that
impact
practices
Identifying management
network
health is valuable to operators, yet non-trivial to accomplish. The
nature of management practices is such that we face at least two
challenges. First, practices may not have a linear, or even mono-
tonic, relationship with network health; this makes it difﬁcult to
clearly identify statistical dependencies. For example, Figure 4
shows three different management practices—number of L2 pro-
tocols, number of models, and fraction of events with an interface
change—that have a linear, monotonic, and non-monotonic rela-
tionship, respectively, with number of tickets. Second, manage-
ment practices are often related, such that a change in one practice
impacts another practice, as well as network health. For example,
Figures 4 and 5 show that number of models and number of roles
are related to network health, and each other. This makes it chal-
lenging to identify causal relationships.
399O(10)
l
s
e
d
o
M
f
o
#
0
1
6
# of Roles
Figure 5: Relationship between number of models and number
of roles
In this section, we present the techniques we use to overcome
these challenges. We illustrate how they work using our data from
the OSP.
5.1 Dependence Analysis
Common approaches for decomposing the impact of different
factors include analysis of variance (ANOVA) [3] and principal/inde-
pendent component analyses (PCA/ICA) [9]. However, these tech-
niques make key assumptions about underlying dependencies that
make them inapplicable to MPA. ANOVA assumes linear relations,
which may not always hold, as illustrated earlier in Figure 4. ICA
attempts to express the outcome (health metric) as a linear or non-
linear combination of independent components; applying PCA ﬁrst
helps identify the components to feed to ICA. The main issue is that
the components output by PCA are linear combinations of a subset
of management practices. Thus, this approach also makes the im-
plicit assumption that linear combinations of practice metrics can
explain network health. Furthermore, the outcome of ICA may be
hard to interpret (especially if it relies on a non-linear model).
Instead, we identify statistical dependencies using a more gen-
eral approach: mutual information (MI). When computed between
a management practice metric and network health, MI measures
how much knowing the practice reduces uncertainty about health.
Crucially, MI does not make assumptions about the nature of the
relationship.
5.1.1 Mutual Information
The MI between variables X and Y (a management practice
and network health) is deﬁned as the difference between the en-
tropy of Y , H(Y ), and the conditional entropy of Y given X,
H(Y |X). Entropy is deﬁned as H(Y ) = − Pi P (yi)logP (yi),
where P (yi) is the probability that Y = yi. Conditional entropy is
deﬁned as H(Y |X) = Pi,j P (yi, xj)log P (xj )
P (yi,xj ) , where P (yi, xj)
is the probability that Y = yi and X = xj . MI is symmetric.
We also examine statistical dependencies between management
practices using conditional mutual information (CMI). The CMI
between a pair of management practices and network health mea-
sures the expected value of the practices’ MI, given health.1 The
CMI for two variables X1 and X2 relative to variable Y is deﬁned
as H(X1|Y ) − H(X1|X2, Y ). Like MI, CMI is also symmetric
(with respect to X1 and X2).
Binning. Prior to computing MI or CMI, we compute the mean
value of each management practice and health metric on a monthly
basis for each network, giving us ≈11K data points. We bin the
data for each metric using 10-equal width bins, with the 5th per-
centile value as the lower bound for the ﬁrst bin, and the 95th per-
centile value as the upper bound for the last bin. Networks whose
1In a sense, the pair’s joint probability distribution.
Management Practices
No. of devices (D)
No. of change events (O)
Intra-device complexity (D)
No. of change types (O)
No. of VLANs (D)
No. of models (D)
No. of roles (D)
Avg. devices changed per event (O)
Frac. events w/ interface change (O)
Frac. events w/ ACL change (O)
Avg. Monthly MI
0.388
0.353
0.329
0.328
0.313
0.273
0.221
0.215
0.201
0.198
Table 3: Top 10 management practices related to network
health according to average monthly MI; parenthetical anno-
tation indicates practice category (D=design, O=operational)
O(10)
O(10)
s
t
e
k
c
T
i
f
o
#
s
t
e
k
c
T
i
f
o
#
0
0
O(100)
0
0
O(10)
# of Devices
# of Change Events
(a) No. of Devices
(b) No. of Change Events
Figure 6: Tickets based on management practices
metric value is below the 5th (above the 95th) percentile are put in
the ﬁrst (last) bin.
Our motivation for this binning strategy is twofold. First, in our
characterization of management practices (Appendix A), we ob-
serve that many management practices have a long tail: e.g., num-
ber of VLANs (Figure 11(c)). Using the 5th and 95th percentile
bounds for the ﬁrst and last bins signiﬁcantly reduces the range of
values covered by each bin, thereby reducing the likelihood that
the majority of networks will fall into just one or two bins. Second,
minor deviations in a management practice or health metric—e.g.,
one more device or one more ticket—are unlikely to be signiﬁcant.
Our binning helps reduce noise from such minor variations
5.1.2 Results for the OSP
We now present statistical dependence results for the OSP. Ta-
ble 3 shows the 10 management practices that have the strongest
statistical dependence with network health. It includes ﬁve design
practices and ﬁve operational practices, thus highlighting the po-
tential importance of both types of practices to a healthy network.
We visually conﬁrmed the assessment that the practices in Ta-
ble 3 have a statistical dependence with network health. For exam-
ple, Figure 6 illustrates the strong statistical dependence with net-
work health for the top two practices. Likewise, Figure 4, which we
described earlier, illustrates the relationships for number of models,
number of roles, and fraction of change events with an interface
change (ranked 6th, 7th, and 9th, respectively, in Table 3).
Interestingly, one of the practices which has high impact accord-
ing to the survey (Figure 2)—fraction of events with a middlebox
change—does not appear in the top 10 practices (Table 3); this
practice is ranked 23 out of 28. This may be due to the fact that
the majority of changes to the OSP’s middleboxes are simple ad-
justments to the server pools conﬁgured on load balancers.
Table 4 shows the 10 management practices that have the strong-
est statistical dependence with each other. We observe that six of
the top 10 practices related to network health (Table 4) are statisti-
cally dependent with other practices; this includes all ﬁve of the top
400Management Practice Pair
Frac. events w/ mbox change (O)
Frac. events w/ pool change (O)
Hardware entropy (D)
Firmware entropy (D)
No. of L3 protocols (D)
No. of OSPF instances (D)
No. of change types (O)
No. of models (D)
Inter-device complexity (D)
No. of BGP instances (D)
No. of models (D)
No. of roles (D)
No. of BGP instances (D)
No. of L2 protocols (D)
Avg. size of an OSPF instance (D) No. of change types (O)
Intra-device complexity (D)
No. of devices (D)
Inter-device complexity (D)
No. of VLANs (D)
CMI
1.107
0.978
0.923
0.735
0.732
0.713
0.601
0.576
0.574
0.569
Table 4: Top 10 pairs of statistically dependent management practices according to CMI; highlighted practices are in the top 10
according to MI; parenthetical annotation indicates practice category (D=design, O=operational)
design practices and one operational practice. In general, more de-
sign practices (as opposed to operational practices) are statistically
dependent with each other. This trend stems from the natural con-
nections between many design decisions: e.g., conﬁguring more
BGP instances results in more references between devices and in-
creases inter-device complexity.
We also observe from Table 4 that several practices are depen-
dent with multiple other practices: e.g., number of models is depen-
dent with number of roles and number of change types, and number
of change types is also dependent with average size of an OSPF
instance. Thus, evaluating the impact of a management practice on