title:TrollMagnifier: Detecting State-Sponsored Troll Accounts on Reddit
author:Mohammad Hammas Saeed and
Shiza Ali and
Jeremy Blackburn and
Emiliano De Cristofaro and
Savvas Zannettou and
Gianluca Stringhini
2022 IEEE Symposium on Security and Privacy (SP)
TROLLMAGNIFIER: Detecting State-Sponsored Troll Accounts on Reddit
Mohammad Hammas Saeed♣, Shiza Ali♣, Jeremy Blackburn♦,
Emiliano De Cristofaro♥, Savvas Zannettou♠, and Gianluca Stringhini♣
♣Boston University, ♦Binghamton University, ♥University College London,
{hammas,shiza,gian}@bu.edu, PI:EMAIL, PI:EMAIL, PI:EMAIL
♠TU Delft, Max Planck Institute for Informatics
6
0
7
3
3
8
9
.
2
2
0
2
.
4
1
2
6
4
P
S
/
9
0
1
1
.
0
1
:
I
O
D
|
E
E
E
I
2
2
0
2
©
0
0
.
1
3
$
/
2
2
/
9
-
6
1
3
1
-
4
5
6
6
-
1
-
8
7
9
|
)
P
S
(
y
c
a
v
i
r
P
d
n
a
y
t
i
r
u
c
e
S
n
o
m
u
i
s
o
p
m
y
S
E
E
E
I
2
2
0
2
Abstract—Growing evidence points to recurring inﬂuence cam-
paigns on social media, often sponsored by state actors aiming to
manipulate public opinion on sensitive political topics. Typically,
campaigns are performed through instrumented accounts, known
as troll accounts; despite their prominence, however, little work
has been done to detect these accounts in the wild. In this
paper, we present TROLLMAGNIFIER, a detection system for
troll accounts. Our key observation, based on analysis of known
Russian-sponsored troll accounts identiﬁed by Reddit, is that
they show loose coordination, often interacting with each other
to further speciﬁc narratives. Therefore, troll accounts controlled
by the same actor often show similarities that can be leveraged
for detection. TROLLMAGNIFIER learns the typical behavior of
known troll accounts and identiﬁes more that behave similarly.
We train TROLLMAGNIFIER on a set of 335 known troll
accounts and run it on a large dataset of Reddit accounts. Our
system identiﬁes 1,248 potential troll accounts; we then provide
a multi-faceted analysis to corroborate the correctness of our
classiﬁcation. In particular, 66% of the detected accounts show
signs of being instrumented by malicious actors (e.g., they were
created on the same exact day as a known troll, they have since
been suspended by Reddit, etc.). They also discuss similar topics
as the known troll accounts and exhibit temporal synchronization
in their activity. Overall, we show that using TROLLMAGNIFIER,
one can grow the initial knowledge of potential trolls provided
by Reddit by over 300%.
I. INTRODUCTION
Social media has dramatically changed the way in which
people get and consume news [28, 31]. Alas, this has also
facilitated the dissemination of misleading information (i.e.,
misinformation) and of deliberate campaigns to spread false
narratives (i.e., disinformation) [46, 47, 59, 61]. Disinforma-
tion campaigns are often orchestrated by state actors, with the
goal of polarizing public discourse or pushing talking points to
favor particular agendas [37, 38]. To do so, malevolent actors
instrument so-called troll accounts to engage in discussions
among each other and with real users, pushing certain narra-
tives and sharing false information [65].
Social network providers have been working to identify and
suspend these accounts and released information about them
after the fact [43, 53]. This has helped researchers shed light on
how troll accounts were operated and studying the narratives
they were pushing, in particular with respect to state-sponsored
troll accounts active on Twitter and Reddit between 2014 and
2018 [4, 14, 65, 72, 74, 75].
Research Problem. Detecting troll accounts is a more difﬁcult
task than detecting “traditional” automated malicious activity.
Unlike malicious accounts involved in fraud and spam, those
taking part
in inﬂuence campaigns are usually controlled,
manually, by humans. Miscreants craft a set of accounts and
control them directly, posting messages, interacting with real
users, and with each other. This means that troll accounts do
not show strict synchronization patterns that are typical of au-
tomated activity, and which were the foundation of previously
proposed detection systems [8, 17, 20, 40, 50]. Additionally,
they present traits that are more similar to regular users, thus
making approaches that rely on identifying mass-created fake
accounts or bulk content ineffective [3, 15, 17, 20, 49, 66, 68].
Overall, while state-sponsored troll accounts exhibit some
indicative traits, typically, social networks identify them via
ad-hoc analysis. Twitter and Reddit released information about
thousands of troll accounts [43, 53], but they did not disclose
the methods that they followed to identify them, and it is
unclear how comprehensive these detections were.
In this paper, we aim to automatically detect state-sponsored
troll accounts on Reddit. Our intuition, informed by previous
studies [14, 47, 58, 74], is that accounts controlled by the same
entity work together to push certain disinformation narratives.
This loose coordination generates interaction patterns that are
measurable, and can be used for detection. For instance, troll
accounts that belong to the same campaign might often reply
to each other, or follow up to discussions started by other troll
accounts to keep the discussion alive and attract real users.
Therefore, by learning these interaction patterns from a set of
known troll accounts, it should be possible to identify more
accounts controlled by the same state-sponsored actors.
TROLLMAGNIFIER. We present a system called TROLL-
MAGNIFIER; we train it on a dataset of 335 Russian-sponsored
troll accounts identiﬁed by Reddit, which were active on the
platform between 2015 and 2018 [43]. We ﬁrst demonstrate
that these accounts show peculiar interaction patterns com-
pared to regular Reddit accounts. For example, troll accounts
are more likely to reply to each other or to make submissions
with the same title. We identify several features that charac-
terize troll accounts (e.g., the fraction of comments made on
submissions by troll accounts or the fraction of submissions
with the same title as a troll account’s submission) and use
them to train classiﬁers and identify additional troll accounts
in the wild.
Results. Our experimental analysis shows that TROLLMAG-
NIFIER can effectively distinguish troll and benign accounts
on our labeled dataset with up to 97.8% F1-Score. We then
© 2022, Mohammad Hammas Saeed. Under license to IEEE.
DOI 10.1109/SP46214.2022.00052
2161
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:00:10 UTC from IEEE Xplore.  Restrictions apply. 
run TROLLMAGNIFIER on unseen Reddit accounts: our system
identiﬁes 1,248 as likely Russian-sponsored troll accounts. To
conﬁrm our results, we perform additional analysis, show-
ing that
66% of the detected accounts were either later
suspended by Reddit, deleted some of their comments of
submissions (typical behavior of troll accounts observed by
previous work [14, 74]), or were created on the same day
of a known troll account. We also ﬁnd that
the detected
troll accounts show stronger timing coordination patterns in
their activity to the set of known troll accounts, compared to
undetected accounts, and that they tend to use similar language
to the one used by the known troll accounts, indicating that
they are likely controlled by the same actor. Our results show
that interaction patterns are an effective way to characterize
and identify trolls on Reddit.
Contributions. This paper makes the following contributions:
• We show that the interactions on Reddit of troll accounts
from the same campaigns are quite different from those
of regular accounts on the same subreddits, and that this
can be used for detection.
• We develop TROLLMAGNIFIER, a system able to learn
the typical behavior of a seed of known troll accounts
and ﬁnd more accounts that showed a similar behavior
on Reddit.
• We run TROLLMAGNIFIER on Reddit accounts extracted
from Pushshift [2]. Out of 53,763 accounts that inter-
acted with the known troll accounts, TROLLMAGNIFIER
identiﬁes 1,248 potential troll accounts.
• We perform a multi-faceted evaluation of our approach.
We show that the accounts detected by TROLLMAG-
NIFIER present signs of being controlled by malicious
actors. We also perform qualitative experiments to de-
termine TROLLMAGNIFIER’s false negatives, estimating
that the false negative rate of our approach is 10%.
We shared our results with Reddit and are waiting for their
feedback.
II. PRELIMINARIES
In this section, we ﬁrst describe our threat model, then, we
present the main characteristics of the Reddit platform. Finally,
we introduce the datasets used in this work.
A. Threat Model
Based on the observations made by previous analysis of
state-sponsored troll accounts [14, 37, 65, 74, 75], we describe
the operation of a typical troll campaign as follows:
1) One or more malicious actors create and instrument
a number of accounts – which we denote as “troll
accounts” – on a social network. These are populated
with data (e.g., proﬁle pictures and proﬁle description)
that makes them look believable and ﬁt the narrative
that the malicious actor wants to push (e.g., a retired
man from the South of England).
2) The troll accounts act “normally” for a while, posting
content not related to disinformation, with the goal of
attracting followers. This is common for other types of
malicious accounts as well, e.g., for spam [49].
3) The troll accounts start pushing speciﬁc narratives. They
post original content (e.g., links to news articles or posts
containing false or manipulated pictures) and simulate
discussion between each other. They also engage in
conversations with legitimate users with the goal of
derailing and polarizing the discussion [37, 75].
4) Unwitting legitimate accounts react to the content posted
by the troll accounts, e.g., re-sharing it or interacting
directly with them. This will turn the disinformation