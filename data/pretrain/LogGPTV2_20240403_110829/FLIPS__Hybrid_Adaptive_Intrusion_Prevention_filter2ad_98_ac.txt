network and parsing it for sanity.
4. the cost of invoking PayL on each request.
5. the cost of training PayL (incurred once at system startup, about 5
seconds for a 5MB ﬁle of HTTP requests).
We evaluate this hypothesis by using a simple client to issue requests to
the production server and measure the change in processing time when each
proxy subcomponent is introduced. Table 2 describes these results.
– Hypothesis 4: The system can run end to end and block a new exploit.
A positive result provides proof for zero-day protection and precise, tuned,
automated ﬁltering. To prove this hypothesis, we run the crafted exploit
against the full system continuously and see how quickly the proxy blocks
it. We determine the latency between STEM aborting the emulated function
and the proxy updating the ﬁlters.
94
M.E. Locasto et al.
5.2 Experimental Setup
The experimental setup for Hypothesis 3 and Hypothesis 4 included an instance
of Apache 2.0.52 as the production server with one simple modiﬁcation to the ba-
sic conﬁguration ﬁle: the “KeepAlive” attribute was set to “Oﬀ.” Then, a simple
awk script reconstructed HTTP requests from dump of HTTP traﬃc and passed
the request over the netcat utility to either the production server or the proxy.
The proxy was written in Java, compiled with the Sun JDK 1.5.0 for Linux, and
run in the Sun JVM 1.5.0 for Linux. The proxy was executed on a dual Xeon
2.0GHz with 1GB of RAM running Fedora Core 3, kernel 2.6.10-1.770 FC3smp.
The production server platform runs Fedora Core 3, kernel 2.6.10-1.770 FC3smp
on a dual Xeon 2.8GHz processor with 1GB of RAM. The proxy server and the
production server were connected via a Gigabit Ethernet switch. The servers
were reset between tests. Each test was run for 10 trials.
5.3 Hypothesis 1: Performance Impact of ISR
We evaluated the performance impact of STEM by instrumenting the Apache
web server and performing micro-benchmarks on some shell utilities. We chose
the Apache ﬂood httpd testing tool to evaluate how quickly both the non-
emulated and emulated versions of Apache would respond and process requests.
In our experiments, we chose to measure performance by the total number of re-
quests processed, as reﬂected in Figure 4. The value for total number of requests
per second is extrapolated (by ﬂood’s reporting tool) from a smaller number of
requests sent and processed within a smaller time slice; the value should not
d
n
o
c
e
s
r
e
p
s
t
s
e
u
q
e
r
9000
8000
7000
6000
5000
4000
3000
2000
1000
0
0
Apache 2.0.49 Request Handling Performance
apache-mainloop
emurand-mainloop
emurand-parse-uri
emurand-header-parser
10
20
30
40
50
60
70
80
# of client threads
Fig. 4. Performance of STEM under various levels of emulation. While full emula-
tion is fairly expensive, selective emulation of input handling routines appears quite
sustainable. The “emurand” designation indicates the use of STEM (emulated random-
ization).
FLIPS: Hybrid Adaptive Intrusion Prevention
95
Table 1. Microbenchmark performance times for various command line utilities
Test Type
trials mean (s) Std. Dev. Min Max Instr. Emulated
ls (non-emu)
ls (emu)
cp (non-emu)
cp (emu)
cat (non-emu)
cat (emu)
25
25
25
25
25
25
0.12
42.32
16.63
21.45
7.56
8.75
0.009
0.182
0.707
0.871
0.05
0.08
0.121 0.167
42.19 43.012
15.80 17.61
20.31 23.42
7.65
7.48
8.64
8.99
0
18,000,000
0
2,100,000
0
947,892
be interpreted to mean that our Apache instances actually served some 6000
requests per second.
We selected some common shell utilities and measured their performance
for large workloads running both with and without STEM. For example, we
issued an ’ls -R’ command on the root of the Apache source code with both
stderr and stdout redirected to /dev/null in order to reduce the eﬀects of screen
I/O. We then used cat and cp on a large ﬁle (also with any screen output
redirected to /dev/null). Table 1 shows the result of these measurements. As
expected, there is a large impact on performance when emulating the majority
of an application. Our experiments demonstrate that only emulating potentially
vulnerable sections of code oﬀers a signiﬁcant advantage over emulation of the
entire system.
5.4 Hypothesis 2: Eﬃcacy of PayL
PayL [38] is a content-based anomaly detector. It builds byte distribution mod-
els for the payload part of normal network traﬃc by creating one model for
each payload length. Then it computes the Mahalanobis distance of the test
data against the models, and decides that input is anomalous if it has a large
Mahalanobis distance compared to the calculated norms.
PayL’s results have been presented elsewhere; this section describes how well
PayL performed on traﬃc during our tests. For the purpose of incorporating
PayL in FLIPS, we adapted PayL to operate on HTTP requests (it previously
evaluated TCP packets). To test the eﬃcacy of PayL’s operations on the web
requests, we collected 5MB (totaling roughly 109000 requests) of HTTP traﬃc
from one of our test machines. This data collection contains various CodeRed
and other malicious request lines. As the baseline, we manually identiﬁed the
malicious requests in the collection. The ROC curve is presented in Figure 5.
From the plot we can see that the classiﬁcation result of PayL on the HTTP
queries is somewhat mediocre. While all the CodeRed and Nimda queries can
be caught successfully, there are still many “looks not anomalous” bad queries
that PayL cannot identify. For example, the query “HEAD /cgi-dos/args.cmd
HTTP/1.0 ” is a potentially malicious one for a web server, but has no anomalous
content considering its byte distribution. If PayL was used to classify the entire
HTTP request, including the entity body, results will be more precise. PayL
alone is not enough for protecting a server, and it requires more information to
96
M.E. Locasto et al.
)
%
(
t
e
a
R
n
o
i
t
c
e
e
D
t
100
90
80
70
60
50
40
30
20
10
0
0
5
10
15
False Positive Rate (%)
20
25
Fig. 5. PayL ROC Curve
tune its models. We emphasize that FLIPS assumes this requirement as part of
its design; we do not ﬁlter based on PayL’s evidence alone.
5.5 Hypothesis 3: Proxy Performance Impact
We discovered the performance impact of our unoptimized, Java-based proxy on
the time it took to service two diﬀerent traﬃc traces. Our results are displayed
in Table 2 and graphically in Figure 6. Note that our experimental setup is not
designed to stress test Apache or the proxy, but rather to elucidate the relative
overhead that the proxy and the ﬁlters add. Baseline performance is roughly 210
requests per second. Adding the proxy degrades this throughput to roughly 170
requests per second. Finally, adding the ﬁlter reduces it to around 160 requests
per second.
Table 2. Performance Impact of FLIPS Proxy Subcomponents. Baseline performance
is compared to adding FLIPS’s HTTP proxy and FLIPS’s HTTP proxy with ﬁltering
and classiﬁcation turned on. Baseline performance is measured by a client script hitting
Apache directly. The addition of the proxy is done by directing the script to contact
the FLIPS HTTP proxy rather than the production server directly. Finally, ﬁltering in
the FLIPS HTTP proxy is turned on.
Component # of Requests Mean Time (s) Std. Dev.
Baseline
Baseline
+Proxy
+Proxy
529
108818
529
108818
+Proxy, +Filter 529
+Proxy, +Filter 108818
2.42
516
2.88
668
3.07
727
0.007
65.7
0.119
9.68
0.128
21.15
FLIPS: Hybrid Adaptive Intrusion Prevention
97
)
s
d
n
o
c
e
s
(
s
t
s
e
u
q
e
r
l
l
a
i
e
c