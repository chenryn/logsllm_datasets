sions, and Defenses - 21st International Symposium, RAID 2018, Heraklion, Crete, Greece,
September 10-12, 2018, Proceedings, volume 11050 of Lecture Notes in Computer Science, pages
490–510. Springer, 2018.
Accepted as a conference paper at ACSAC 2020
[43] Jonathan Uesato, Brendan O’Donoghue, Pushmeet Kohli, and Aäron van den Oord. Adver-
sarial risk and the dangers of evaluating against weak attacks. In Dy and Krause [25], pages
5032–5041.
[44] David Wagner and Paolo Soto. Mimicry attacks on host-based intrusion detection systems.
In Proceedings of the 9th ACM conference on Computer and communications security - CCS
’02. ACM Press, 2002.
[45] Weilin Xu, Yanjun Qi, and David Evans. Automatically evading classiﬁers: A case study on
PDF malware classiﬁers. In 23rd Annual Network and Distributed System Security Symposium,
NDSS 2016, San Diego, California, USA, February 21-24, 2016. The Internet Society, 2016.
[46] Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. Seqgan: Sequence generative adversarial
nets with policy gradient. In Satinder P. Singh and Shaul Markovitch, editors, Proceedings of
the Thirty-First AAAI Conference on Artiﬁcial Intelligence, February 4-9, 2017, San Fran-
cisco, California, USA., pages 2852–2858. AAAI Press, 2017.
[47] Yizhe Zhang, Zhe Gan, Kai Fan, Zhi Chen, Ricardo Henao, Dinghan Shen, and Lawrence
Carin. Adversarial feature matching for text generation. In Doina Precup and Yee Whye Teh,
editors, Proceedings of the 34th International Conference on Machine Learning, ICML 2017,
Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning
Research, pages 4006–4015. PMLR, 2017.
[48] Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu.
Texygen: A benchmarking platform for text generation models. In Kevyn Collins-Thompson,
Qiaozhu Mei, Brian D. Davison, Yiqun Liu, and Emine Yilmaz, editors, The 41st International
ACM SIGIR Conference on Research & Development in Information Retrieval, SIGIR 2018,
Ann Arbor, MI, USA, July 08-12, 2018, pages 1097–1100. ACM, 2018.
Accepted as a conference paper at ACSAC 2020
Figure 2: Overview of the Malware Classiﬁcation Process
A Tested Dataset
We used identical implementation details (e.g., dataset, classiﬁers’ hyperparameters, etc.) as Rosen-
berg et al.
[42], so the attacks can be compared. The details are provided here for the reader’s
convenience.
An overview of the malware classiﬁcation process is shown in Figure 2 (taken from [42]).
The dataset used is large and includes the latest malware variants, such as the Cerber and Locky
ransomware families. Each malware type (ransomware, worms, backdoors, droppers, spyware,
PUAs, and viruses) has the same number of samples, to prevent prediction bias towards the majority
class. 20% of the malware families (such as the NotPetya ransomware family) were only used in the
test set to assess generalization to an unseen malware family. 80% of the malware families (such
as the Virut virus family) were distributed between the training and test sets, to determine the
classiﬁer’s ability to generalize to samples from the same family. The temporal diﬀerence between
the training set and the test set is six months (i.e., all training set samples are older than the test
set samples, because using a non-time-aware split may cause a data leak [37]) based on VirusTotal’s
’ﬁrst seen’ date.
The ground truth labels of the dataset were determined by VirusTotal [12], an online scanning
service, which contains more than 60 diﬀerent security products. A sample with 15 or more positive
(i.e., malware) classiﬁcations from the 60 products is considered malicious. A sample with zero
positive classiﬁcations is labeled as benign. All samples with 1-14 positives were omitted to prevent
false positive contamination of the dataset. Family labels for dataset balancing were taken from
Kaspersky Anti-Virus classiﬁcations.
It is crucial to prevent dataset contamination by malware that detects whether the malware
is running in a Cuckoo Sandbox (or on virtual machines) and if so, quits immediately to prevent
reverse engineering eﬀorts. In those cases, the sample’s label is malicious, but its behavior recorded
in Cuckoo Sandbox (its API call sequence) isn’t, due to the malware’s anti-forensic capabilities. To
prevent such contamination of the dataset, two countermeasures were used:
1. Considering only API call sequences with more than 15 API calls, omitting malware that
detects a virtual machine (VM) and quits.
2. Applying Yara rules [14] to ﬁnd samples trying to detect sandbox programs, such as Cuckoo
Sandbox, and omitting all such samples.
One might argue that the evasive malware that applies such anti-VM techniques is extremely
challenging and relevant, however in this paper we focus on adversarial attacks. Such attacks are
generic enough to work for those evasive types of malware as well, assuming that other mitigation
Accepted as a conference paper at ACSAC 2020
(a) Dynamic Classiﬁer Architecture
(b) Hybrid Classiﬁer Architecture
Figure 3: Classiﬁer Architecture Overview
techniques (e.g., anti-anti-VM), would be applied. After this ﬁltering and balancing of the benign
samples, about 400,000 valid samples remained. The ﬁnal training set size is 360,000 samples,
36,000 of which serve as the validation set. The test set size is 36,000 samples. All sets are
balanced between malicious and benign samples. Due to hardware limitations, a subset of the
dataset was used (54,000 training samples and test and validation sets of 6,000 samples each). The
dataset was representative and maintained the same distribution as mentioned above.
B Tested Malware Classiﬁers
As mentioned in Section 4.1, we used the malware classiﬁers from Rosenberg et al.
[42], since
many classiﬁers are covered, allowing us to evaluate the attack eﬀectiveness against many classiﬁer
types. The maximum input sequence length was limited to k = 140 API calls, since longer sequence
lengths, e.g., k = 1, 000, had no eﬀect on the accuracy, and shorter sequences were padded with
zeros. A zero stands for a null/dummy value API in our one-hot encoding. Longer sequences are
split into windows of k API calls each, and each window is classiﬁed in turn. If any window is
malicious, the entire sequence is considered malicious. Thus, the input of all of the classiﬁers is a
vector of k = 140 API call types in one-hot encoding, using 314 bits, since there were 314 monitored
API call types in the Cuckoo reports for the dataset. The output is a binary classiﬁcation: malicious
or benign. An overview of the LSTM architecture is shown in Figure 3a.
The Keras [7] implementation was used for all neural network classiﬁers, with TensorFlow used
for the backend. XGBoost [13] and scikit-learn [9] were used for all other classiﬁers.
The loss function used for training was binary cross-entropy. The Adam optimizer was used for
Accepted as a conference paper at ACSAC 2020
all of the neural networks. The output layer was fully connected with sigmoid activation for all
neural networks. For neural networks, a rectiﬁed linear unit, ReLU (x) = max(0, x), was chosen
as an activation function for the input and hidden layers due to its rapid convergence compared to
sigmoid() or tanh(), and dropout was used to improve the generalization potential of the network.
A batch size of 32 samples was used. The classiﬁers also have the following classiﬁer speciﬁc
hyperparameters:
• DNN - two fully connected hidden layers of 128 neurons, each with ReLU activation and a
dropout rate of 0.2.
• CNN - 1D ConvNet with 128 output ﬁlters, a stride length of one, a 1D convolution window
size of three, and ReLU activation, followed by a global max pooling 1D layer and a fully
connected layer of 128 neurons with ReLU activation and a dropout rate of 0.2.
• RNN, LSTM, GRU, BRNN, BLSTM, and bidirectional GRU - a hidden layer of 128 units,
with a dropout rate of 0.2 for inputs and recurrent states.
• Deep LSTM and BLSTM - two hidden layers of 128 units, with a dropout rate of 0.2 for
inputs and recurrent states in both layers.
• Linear SVM and logistic regression classiﬁers - a regularization parameter of C=1.0 and an
L2 norm penalty.
• Random forest classiﬁer - 10 decision trees with unlimited maximum depth and the Gini
criteria for choosing the best split.
• Gradient boosted decision tree - up to 100 decision trees with a maximum depth of 10 each.
The classiﬁers’ performance was measured using the accuracy ratio, which gives equal importance
to both false positives and false negatives (unlike precision or recall). The false positive rate of
the classiﬁers varied between 0.5-1%. The false positive rate was chosen to be on the high end of
production systems. A lower false positive rate would mean lower recall either, due to the trade-oﬀ
between false positive rate and recall, thereby making our attacks even more eﬀective.
The performance of the classiﬁers is shown in Table 5. The accuracy was measured on the test
set, which contains 36,000 samples.
Table 5: Classiﬁer Performance
Classiﬁer Type
Accuracy (%)
LSTM
Deep LSTM
GRU
Logistic Regression
Gradient Boosted Decision Tree
Bidirectional GRU
1D CNN
Random Forest
SVM
98.26
97.90
97.32
98.04
96.42
91.90
86.18
89.22
91.10
Accepted as a conference paper at ACSAC 2020
Table 6: Benign Perturbation Attack Performance
GAN Type
None (Random
Perturbation)
SeqGAN [46]
TextGAN [47]
GSGAN [32]
MaliGAN [20]
Attack
Eﬀectiveness [%]
21.25
89.39
74.53
88.19
86.67
Added API Calls
[%]
27.94
12.82
16.74
14.06
15.12
Queries Used
119.40
17.73
30.58
20.43
22.74
As can be seen in Table 5, the LSTM variants are the best malware classiﬁers, in terms of
accuracy, and, as shown in Section 4.2, BLSTM is also one of the classiﬁers most resistant to the
proposed attack.
C Benign Perturbation GAN Comparison
To implement the benign perturbation GAN, we tested several GAN types, using Texygen [48] with
its default parameters. We use maximum likelihood estimation (MLE) training as the pretraining
process for all baseline models except GSGAN, which requires no pretraining. In pretraining, we
ﬁrst train 80 epochs for a generator, and then train 80 epochs for a discriminator. The adversarial
training comes next. In each adversarial epoch, we update the generator once and then update
the discriminator for 15 mini-batch gradients. Due to memory limitations, we generated only one
sliding window of 140 API calls, each with 314 possible API call types, in each iteration (that is,
generating wb and not xb as described in Algorithm 1).
We tested several GAN implementations with discrete sequence output. We trained our GAN
using a benign hold-out set (3,000 sequences). Next, we run Algorithm 1 (logarithmic backtracking
transformation with benign perturbation) on the 3,000 API call traces generated by the GAN.
Finally, we used the benign test set (3,000 sequences) as the GAN’s test set. The results for the
LSTM classiﬁer are shown in Table 6 (the results for other classiﬁers, which are not shown due to
space limits, are similar).
We can see from the results presented in the table that SeqGAN outperforms all of the other
models in all of the measured factors, due to its RL-based ability to pass gradient updates be-
tween the generator and discriminator parts of the GAN. We also see that, as expected, a random
perturbation is less eﬀective than a benign perturbation, regardless of the type of GAN used.
D Handling Multiple Feature Types and Hybrid Classiﬁers
Combining several types of features can make the classiﬁer more resistant to adversarial examples
against a speciﬁc feature type. For instance, some real-world next generation anti-malware products
are hybrid classiﬁers, combining both static and dynamic features for a better detection rate. An
extension of our attack, enabling it to handle hybrid classiﬁers, is straightforward: attacking each
feature type in turn using Algorithm 1. If the attack against a feature type fails, we continue and
Accepted as a conference paper at ACSAC 2020
attack the next feature type with the modiﬁed binary until a benign classiﬁcation by the target
model is achieved or all feature types have been (unsuccessfully) attacked. We used the same hybrid
malware classiﬁer speciﬁed in Appendix C, for which the input consists of both an API call sequence
and the most frequent 20,000 printable strings inside the PE ﬁle as Boolean features (exist or not).
While there are more complex static features (e.g., [18]), we chose printable strings, easy to
modify features that have been used by many classiﬁers [31], as a concrete example of the multi-
feature use case, to show that the suggested attack works not only against RNNs, but also against
other classiﬁers, making it more generic.
We evaluated the performance of our decision-based, linear iteration, benign perturbation attack.
When attacking only the API call sequences using the hybrid classiﬁer, without modifying the static
features of the sample, the attack eﬀectiveness decreases to 23.76%. This is much lower than the
attack eﬀectiveness of 89.67% obtained for a classiﬁer trained only on the dynamic features, meaning
that the attack was mitigated by the use of additional static features. When attacking only the
printable string features (again, assuming that the attacker has the knowledge of D(cid:48) = D, which
contains the printable strings being used as features by the hybrid classiﬁer), the attack eﬀectiveness
is 28.25%. This is much lower than the attack eﬀectiveness of 88.31% obtained for a classiﬁer trained
only on the static features. Finally, the multi-feature attack’s eﬀectiveness for the hybrid model
was 90.06%. Other types of classiﬁers and attacks provided similar results. They are not presented
here due to space limits.
To summarize, we have shown that while the use of hybrid models decreases the specialized
attacks’ eﬀectiveness, our suggested hybrid attack performs well, with high attack eﬀectiveness.
While not shown due to space limits, the attack overhead isn’t signiﬁcantly aﬀected.
E Tested Hybrid Malware Classiﬁers
As mentioned in Appendix D, we used the hybrid malware classiﬁer used in [42], with printable
strings inside a PE ﬁle as our static features. Strings can be used, e.g., to statically identify loaded
DLLs and called functions, and recognize modiﬁed ﬁle paths and registry keys, etc. Our architecture
for the hybrid classiﬁer, shown in Figure 3b, is:
1. A static branch that contains an input vector of 20,000 Boolean values: for each of the 20,000
most frequent strings in the entire dataset, do they appear in the ﬁle or not? This is analogous
to a similar procedure used in NLP which ﬁlters the least frequent words in a language.
2. A dynamic branch that contains an input vector of 140 API calls (each of which is one-hot
encoded) inserted into an LSTM layer of 128 units and a sigmoid activation function, with
a dropout rate of 0.2 for inputs and recurrent states. This vector is inserted into two fully
connected layers with 128 neurons, a ReLU activation function, and a dropout rate of 0.2
each.
The 256 outputs of both branches are inserted into a fully connected output layer with a sigmoid
activation function. Therefore, the input of the classiﬁer is a vector containing 20,000 Boolean
values and 140 one-hot encoded API call types, and the output is malicious or benign classiﬁcation.
The training set size was reduced by half in comparison to the training set speciﬁed in Appendix
A (while keeping the same dataset structure) due to the larger memory requirements for training
a classiﬁer with more features. All other hyperparameters are the same as those mentioned in
Appendix B.
Accepted as a conference paper at ACSAC 2020
A classiﬁer using only the dynamic branch (Figure 3a) achieves 92.48% accuracy on the test
set (this is diﬀerent from the results presented in Table 5, due to the smaller training set), a
classiﬁer using only the static branch attains 96.19% accuracy, and a hybrid model that uses both
branches (Figure 3b) obtains 96.94% accuracy, meaning that using multiple feature types improves
the accuracy.