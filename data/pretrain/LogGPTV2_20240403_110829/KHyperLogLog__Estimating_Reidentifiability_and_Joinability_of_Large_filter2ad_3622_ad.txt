K = 2048 and M = 1024. We learned from our
production settings that K = 2048 gives a good tradeoff
between precision and memory usage. M = 1048 was
(cid:20)(cid:22)(cid:26)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:49:06 UTC from IEEE Xplore.  Restrictions apply. 
chosen as the smallest possible parameter of the HLL++
library available in BigQuery Standard SQL. We use
M = 512 in our production pipelines, which gives a
comparable degree of precision in ID cardinality estima-
tion (4% vs. 3%). As HLL++ counts elements exactly at
lower cardinalities, this has a negligible inﬂuence on our
estimations. For each data set, we compare the KHLL-
based estimate to the true distribution, which is computed
exactly (without approximation) using standard SQL
operators.
Figure 6a plots the cumulative uniqueness distribution
of movies in the Netﬂix Prize data set. It allows an
analyst to quickly answer the question: how much data
do we lose if we only release the movies which have
been rated by more than k users, for all possible values
of k. The uniqueness of movies is low: the median
number of associated users per movie is larger than 500.
Figure 6b plots the cumulative uniqueness distribution of
the tuples of movie and date. The typical uniqueness of
this setting is high: over 80% of the (movie, date) tuples
are associated with 10 or less unique IDs.
Figure 6c shows the cumulative uniqueness distribu-
tion of tuples (ZIP code, age) in the US census. Each
individual
in the census data set appears in only a
single row, different from the case with Netﬂix data
sets where ratings from the same user exist on several
separate records. The uniqueness of (ZIP code, age)
tuples is variable: a signiﬁcant portion of possible values
is associated to only a few individuals, but many of the
(ZIP code, age) tuples associate with larger than 100
individuals.
Across all three experiments, we observe that the esti-
mate of uniqueness distribution using KHLL is accurate.
B. Accuracy of Joinability & Containment Estimation
As described in Section VII, joinability is most inter-
esting from the privacy perspective when an pseudony-
mous ID space becomes joinable with PII. Speciﬁcally
if ﬁelds F1 and F2 are joinable, and that F1 uniquely
identiﬁes a pseudonymous ID space while F2 uniquely
identiﬁes PII. Three conditions are important for estimat-
ing such risk: the ratio of F1 values uniquely identifying
ID 1, the ratio of F2 values uniquely identifying ID 2,
and the containment of F1 in F2 (or containment of F2
in F1).
The estimate of ratio of ﬁeld values uniquely identi-
fying an ID can be seen as an estimator of the parameter
p based on an observation of a binomial distribution of
parameters K and p. It is well-known that a binomial
distribution of parameters K and p has a variance of
n
o
i
t
u
b
i
r
t
s
i
D
e
v
i
t
a
l
u
m
u
C
n
o
i
t
u
b
i
r
t
s
i
D
e
v
i
t
a
l
u
m
u
C
n
o
i
t
u
b
i
r
t
s
i
D
e
v
i
t
a
l
u
m
u
C
1.0
0.8
0.6
0.4
0.2
0.0
1.0
0.9
0.8
0.7
0.6
0.5
0.4
1.0
0.8
0.6
0.4
0.2
0.0
KH LL estim ation      
True distribution      
101
102
103
Uniqueness
104
105
(a) Netﬂix Prize: movie ID
KH LL estim ation      
True distribution      
100
101
102
Uniqueness
103
104
(b) Netﬂix Prize: movie ID and date
100
101
102
Uniqueness
KH LL estim ation      
True distribution      
103
104
(c) US Census: ZIP code and age
Fig. 6: Estimation of uniqueness distribution in different
data sets using KHLL as compared to true distribution
(computed exactly without approximation).
p(1 − p)K, so the estimator which divides the result of
the distribution by K has a variance of p(1 − p)/K,
p(1 − p)/K. Therefore,
or a standard distribution of
(cid:3)
(cid:20)(cid:23)(cid:17)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:49:06 UTC from IEEE Xplore.  Restrictions apply. 
we focus our experiments on estimating the containment
metric, as deﬁned in Deﬁnition 3.
Using K = 2048 hashes, and assuming F1 and F2
have the same cardinality, the estimate of containment
falls within ±5% of the true value over 90% of the time,
and always stays within 10% of the true value. This is
true regardless of the cardinality of the intersection of
F1 and F2. Figure 7 shows the median as well as the
5th and 95th percentiles of the containment estimation,
for cardinalities of 10,000 and 10,000,000.
When F1 and F2 have different cardinalities, however,
precision can suffer. Figure 8 shows the median as
well as the 5th and 95th percentiles of the estimation
of |F1 ∩ F2|/|F1|, where true value is ﬁxed at 50%,
|F1| = 100, 000, and |F2| varies between 50,000 and
2,000,000 (so, the cardinality ratio |F2|/|F1| ranges from
0.5 to 20).
We can observe that the larger the cardinality ratio
gets,
the worse the precision becomes. This is ex-
pected: since we compute |F1 ∩ F2| using the inclusion-
exclusion principle, and the error of the estimation is
proportional to the cardinality estimated, the estimation
error of |F1 ∩ F2| should be roughly proportional
to
max(|F1|,|F2|). Since the value of |F1 ∩ F2| is roughly
proportional to min(|F1|,|F2|), the error ratio of the
containment estimation will grow linearly with the car-
dinality ratio. This is what we observe in practice.
X. RELATED WORK
Using the frequency of (combinations of) ﬁeld values
as a proxy to measure reidentiﬁability is not new. A large
body of research has emerged following the proposal of
k-anonymity by Sweeney in 1997 [1]. Rather than just
estimating k-anonymity, the KHLL algorithm estimates
the entire uniqueness distribution, which is useful for
evaluating the impact to data loss with k-anonymization
or its variants (e.g. [2], [22]). While different from the
notion of differential privacy [7], the reidentiﬁability and
joinability risks as estimated using KHLL can serve
to help determine a suitable anonymization strategy,
particularly when considering the different contexts and
use cases of anonymization.
The problem of gathering metadata and organizing
data sets in large organizations has been described on
several occasions. Halevy et al. [23] detail a search
engine for data sets, which gathers metadata at data
set level. Meanwhile, Sen et al. [24] explain how to
detect and propagate ﬁeld-level semantic annotations
using assisted labeling and dataﬂow analysis. The tools
we develop to automatically detect joinability of data
sets could be used for a similar purpose. Inconsistent
semantic annotations between data ﬁelds with high sim-
ilarity scores (containment or Jaccard) can be automat-
ically detected with the correct annotations propagated
accordingly.
Approximate counting techniques have been the sub-
ject of a signiﬁcant body of research e.g. [9]–[11],
[25]. The techniques we propose are independent of the
particular algorithm chosen to approximate ID cardinal-
ity. Speciﬁcally, the KHLL algorithm could use basic
HyperLogLog [10], HyperLogLog++ [11] or our propri-
etary implementation of HyperLogLog++ Half Byte (see
Appendix XII) to estimate uniqueness distribution and
pairwise containment of data sets albeit with different
memory efﬁciency.
Beyond privacy, estimating value distribution is also
essential to database query optimization and data stream
processing. Most research in this domain focuses on
sketching values of high frequency (i.e. statistical heavy
hitters or top-k queries). The closest analogue of KHLL
was presented by Cormode et al. [26]; it combines the
Count-Min sketch [27] and the LogLog sketch [28] for
value distribution estimation. However, the Count-Min
algorithm biases towards values of high frequency, which
is not helpful for evaluating the impact of k-anonymity
given the typical choices of k are much smaller than the
frequency of heavy hitters.
MinHash [4] and SimHash [29] are two popular
algorithms for estimating the Jaccard similarity of data
sets. The KHLL algorithm leverages the K Minimum
Values in the ﬁrst level of the two-level sketch for esti-
mating Jaccard and containment scores, using a similar
log n-space memory footprint. A possible improvement
might be to adapt from HyperMinHash [30], capable
of estimating Jaccard using only loglog n-space. Yet,
given that the bulk of memory usage by KHLL actually
comes from the second level of the sketch for estimating
the uniqueness distribution, we have not explored the
feasibility of adapting KHLL to have a HyperMinHash-
like data structure in the ﬁrst level.
Finally, despite the extensive research for detecting
data similarity, we have not seen any prior work tackling
the problem of automatically detecting possible joinabil-
ity between different ID spaces across data sets.
XI. SUMMARY
The scale of data and systems in large organizations
demands an efﬁcient approach for reidentiﬁability and
joinability analysis. The KHyperLogLog (KHLL) al-
gorithm innovates on approximate counting techniques
(cid:20)(cid:23)(cid:18)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:49:06 UTC from IEEE Xplore.  Restrictions apply. 
)
%
(
t
n
e
m
n
i
a
t
n
o
c
d
e
t
a
m
i
t
s
E
1 0 0
8 0
6 0
4 0
2 0
0
0
2 0
4 0
8 0
T ru e  c o n ta in m e n t (% )
6 0
)
%
(
t
n
e
m
n