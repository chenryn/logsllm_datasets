Which addresses: Given our target sample size, the next
question is which addresses are probed. To allow analysis at
both the address- and block-granularity we chose a clustered
sample design [17] where we fully enumerate each address
in 24,000 selected /24 blocks.
An important sampling design choice is the granularity
of the sample. We probe /24 blocks rather than individual
addresses because we believe blocks are interesting to study
as groups. (Unlike population surveys, where clustered sam-
pling is often used to reduce collection costs.) Since CIDR [11]
and BGP routing exploit common preﬁxes to reduce routing
table sizes, numerically adjacent addresses are often assigned
to the same administrative entity. For the same reason, they
also often share similar patterns of packet loss. To the ex-
tent that blocks are managed similarly, probing an entire
block makes it likely that we probe both network infras-
tructure such as routers or ﬁrewalls, and edge computers.
We survey blocks of 256 addresses (/24 preﬁxes) since that
corresponds to the minimal size network that is allowed in
global routing tables and is a common unit of address dele-
gation.
We had several conﬂicting goals in determining which
blocks to survey. An unbiased sample is easiest to analyze,
but blocks that have some hosts present are more interesting,
and we want to ensure we sample parts of the Internet with
extreme values of occupancy. We also want some blocks to
remain stable from survey to survey so we can observe their
evolution over time, yet it is likely that some blocks will
cease to respond, either becoming ﬁrewalled, removed, or
simply unused due to renumbering.
Our sampling methodology attempts to balance these goals
by using three diﬀerent policies to select blocks to survey:
unchanging/random, unchanging/spaced, and novel/random.
We expect these policies to allow future analysis of subsets
of the data with diﬀerent properties. Half of the blocks are
selected with a unchanging policy, which means that we se-
lected them when we began surveys in September 2006 and
retain them in future surveys. We selected the unchang-
ing set of blocks based on IT 13w . A quarter of all blocks
(half of the unchanging blocks; unchanging/random) were
selected randomly from all blocks that had any positive re-
sponses. This set is relatively unbiased (aﬀected only by our
requirement that the block show some positive response).
Another quarter of all blocks (unchanging/spaced) were se-
lected to uniformly cover a range of availabilities and volitil-
ities (approximating the A, U -values deﬁned in Section 2.4).
This unchanging/spaced quarter is therefore not randomly
selected, but instead ensures that unusual blocks are repre-
sented in survey data, from fully-populated, always up server
farms to frequently changing, dynamically-addressed areas.
The other half of all blocks (novel/random) are selected
randomly, for each survey, from the set of /24 blocks that
responded in the last census. This selection method has a
bias to active portions of the address space, but is otherwise
unbiased. Selection from previously active blocks means we
do not see “births” of newly used blocks in our survey data,
but it reduces probing of unused or unrouted space. In spite
of these techniques, we actually see a moderately large num-
ber (27%) of unresponsive blocks in our surveys, suggesting
address usage is constantly evolving.
Since all blocks for surveys are drawn from blocks that re-
sponded previously, our selection process should be slightly
biased to over-represent responsiveness.
In addition, one
quarter of blocks (unchanging/spaced) are selected non-ran-
domly, perhaps skewing results to represent “unusual” blocks.
Since most of the Internet blocks are sparsely populated (see
Figure 2) we believe this also results in a slight overestimate.
Studies of subsets of the data are future work.
How long: We collect surveys for periods of about one
week. This duration is long enough to capture daily cycles,
yet not burden the target address blocks. We plan to expand
collection to 14 days to capture two weekend cycles.
Name Start Date (days) (×109) (×106)
ICMP 1 2003-06-01
51.08
2.52
ICMP 2 2003-10-08
51.52
2.52
TCP 1
52.41
2003-11-20
2.52
IT 1
57.49
2004-06-21
2.40
IT 2
59.53
2004-08-30
2.40
IT 4
63.15
2005-01-05
2.43
IT 5
2005-02-25
66.10
2.43
IT 6
69.89
2005-07-01
2.65
IT 7
74.40
2005-09-02
2.65
IT 9
73.88
2005-12-14
2.65
IT 11w 2006-03-07
95.76
2.70
IT 12w 2006-04-13
2.70
96.80
IT 13w 2006-06-16
2.70 101.54
IT 14w 2006-09-14
2.75 101.17
IT 15w 2006-11-08
2.82 102.96
IT 16w 2007-02-14
2.90 104.77
IT 17w 2007-05-29
2.89 112.25
Dur. Alloc. ACKs NACKs Prohib.
(×106)
n/a
n/a
n/a
n/a
n/a
n/a
n/a
n/a
17.33
15.81
17.84
16.94
17.86
16.40
14.73
14.49
16.04
(×106)
n/a
n/a
n/a
n/a
n/a
n/a
n/a
n/a
46.52
49.04
53.4*
52.2*
77.11
51.17
84.44
65.32
66.05
117
191
120
70
70
42
42
47
67
31
24
24
32
32
62
50
52
Table 1:
IPv4 address space allocation (alloc.) and re-
sponses over time (positive and negative acknowledgments,
and NACKs that indicate administrative prohibited), Cen-
suses before September 2005 did not record NACKs.
14w
Name
IT survey
IT survey
IT survey
IT survey
ICMP-nmapsurvey
Start Date
2006-03-09
2006-11-08
2007-02-16
2007-06-01
USC 2007-08-13
15w
16w
17w
Duration
/24 Blocks
(days)
6
7
7
12
9
probed respond.
217
17,528
20,912
20,866
299
260
24,008
24,007
24,007
768
Table 2: Summary of surveys conducted.
Datasets: Table 2 lists the surveys we have conducted to
date, including general surveys and ICMP-nmapsurvey
USC used
for validation in Section 3.2. We began taking surveys well
after our initial censuses. These datasets are available from
the authors and have already been used by several external
organizations.
2.4 Metrics
To characterize the visible Internet we deﬁne two metrics:
availability (A) and uptime (U ). We deﬁne address avail-
ability, A(addr ) as the fraction of time a host at an address
responds positively. We deﬁne address uptime, U (addr ), as
the mean duration for which the address has a continuous
positive response, normalized by the duration of probing in-
terval. This value approximates host uptime, although we
cannot diﬀerentiate between an address occupied by a sin-
gle host and one ﬁlled by a succession of diﬀerent responsive
hosts.
It also assumes each probe is representative of the
address’s responsiveness until the next probe. The (A, U )
pair reﬂects address usage: (0.5, 0.5) corresponds to an ad-
dress that responds for the ﬁrst half of the measurement
period but is down the second half, while (0.5, 0.1) could be
up every other day for ten days of measurement.
We also deﬁne block availability and uptime, or A(block )
and U (block ), as the mean A(addr ) and U (addr ) for all ad-
dresses in the block that are ever responsive.
By deﬁnition, A(block ) is an estimate of the fraction of
addresses that are up in that block. If addresses in a block
)
k
c
o
b
(
l
U
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
 400
 20
 1
 0.2
 0.4
 0.6
 0.8
 1
A (block)
Figure 2: Density of /24 address blocks in survey IT survey
15w ,
grouped by percentile-binned block availability and uptime.
follow a consistent allocation policy, it is also the probability
that any responsive address is occupied.
Both A and U are deﬁned for surveys and censuses. In
censuses, the probe interval of months is sparse enough that
should be considered a rough, probabilistic estimate rather
than an accurate measurement. Infrequent samples are par-
ticularly problematic computing U (addr ) over censuses; we
therefore focus on U (addr ) from surveys, where the sam-
pling rate is a better match for actual host uptimes.
These measures are also not completely orthogonal, since
large values of U can occur only for large values of A and
small values of A correspond to small values of U . In fact,
U = A/NU where NU is the number of uptime periods.
Finally, taking the mean of all addresses in a /24 block may
aggregate nodes with diﬀerent functions or under diﬀerent
administrative entities.
To illustrate these metrics and their relationship, Figure 2
shows a density plot of these values for responding blocks
from IT survey
15w . We show density by counting blocks in each
cell of a 100 × 100 grid. Most of the probability mass is
near (A, U ) = (0, 0) and along the U (cid:2) 0 line, suggest-
ing sparsely populated subnets where most addresses are
unavailable. Figures showing alternative representations of
this data are available elsewhere [18].
3. UNDERSTANDING THE METHODOLOGY
Before evaluating the visible Internet, we ﬁrst evaluate
our methodology. Any form of active probing of a system as
large and complex as the Internet must be imperfect, since
the Internet will change before we can complete a snapshot.
Our goal is therefore to understand and quantify sources
of error, ideally minimizing them and ensuring that they
are not biased. We therefore review inherent limitations of
active probing, then consider and quantify four potential
sources of inaccuracy: probe protocol, measurement loca-
tion, multi-homed hosts, and packet loss.
Figure 1 relates what we can measure to classes of edge
computers. Our methodology counts the large hatched area,
and estimates most the white areas representing sources of
error in our measurement. Since we have no way of observ-
ing computers that are never on-line, we focus on computers
that are sometime on the Internet (the left box). This class
is divided into three horizontal bands: visible computers
(top cross-hatch), computers that are visible, but not to our
probe protocol (middle white box, estimated in Section 3.2),
and invisible computers (bottom white box; Section 3.2.1).
In addition, we consider computers with static and dynamic
addresses (left and right halves). Finally, subsets of these
may be generally available, but down at probe time (cen-
tral dashed oval; Section 3.5), frequently unavailable (right
dashed box), or double counted (“router” oval; Section 3.4).
3.1 Active Probing and Invisible Hosts
The most signiﬁcant limitation of our approach is that
we can only see the visible Internet. Hosts that are hid-
den behind ICMP-dropping ﬁrewalls and in private address
space (behind NATs) are completely missed; NAT boxes ap-
pear to be at most a single occupied address. While IETF
requires that hosts respond to pings [4], many ﬁrewalls, in-
cluding those in Windows XP SP1 and Vista, drop pings.
On the other hand, such hosts are often placed behind ping-
responsive routers or NAT devices.
While an OS-level characterization of the Internet is an
open problem, in the next section we provide very strong
estimates of estimate measurement error for USC, and an
evaluation of a random sample of Internet addresses.
In
Section 6 we look at visible ﬁrewall deployment. Studies of
server logs, such as that of Xie et al. [50], may complement
our approaches and can provide insight into NATed hosts
since web logs of widely used services can see through NATs.
Ultimately, a complete evaluation of the invisible Internet is
an area of future work.
Network operators choose what to ﬁrewall and whether
to block the protocols used in our probes. Blocking reduces
our estimates, biasing them in favor of under-reporting us-
age. This bias is probably greater at sites that place greater
emphasis on security. While we study the eﬀects of ﬁrewalls
and quantify that in the next section, our overall conclusions
focus on the visible Internet.
3.2 Choice of Protocol for Active Probing
We have observed considerable skepticism that ICMP prob-
ing can measure active hosts, largely out of fears that it is
widely ﬁltered by ﬁrewalls. While no method of active prob-
ing will detect a host that refuses to answer any query, we
next compare ICMP and TCP as alternative mechanisms.
We validate ICMP probing by examining two populations.
First, at USC we use both active probes and passive traﬃc
observation to estimate active addresses. University policies
may diﬀer from the general Internet, so we then compare
ICMP and TCP-based probing for a random sample of ad-
dresses drawn from the entire Internet.
3.2.1 Evaluation at USC
We ﬁrst compare ICMP and TCP based probing on a
week-long survey ICMP-nmapsurvey
USC of all 81,664 addresses
and about 50,000 students and staﬀ at USC, comparing pas-
sive observation of all traﬃc with TCP and ICMP probing.
Our ICMP methodology is described in Section 2.2, with
complete scans every 11 minutes. We compare this approach
to TCP-based active probing and passive monitoring as de-
scribed by Bartlett et al. [2]. TCP-based active probing uses
Nmap applied to ports for HTTP, HTTPS, MySQL, FTP,
category:
addresses probed
non-responding
responding any
81,664
54,078
27,586
ICMP or TCP 19,866