### Address Selection for Probing

Given our target sample size, the next question is which addresses to probe. To facilitate analysis at both the address and block levels, we adopted a clustered sample design [17] where we fully enumerate each address in 24,000 selected /24 blocks.

#### Sampling Granularity

An important aspect of the sampling design is the granularity. We chose to probe /24 blocks rather than individual addresses because we believe that blocks are more interesting to study as cohesive units. Unlike population surveys, where clustering is often used to reduce collection costs, CIDR [11] and BGP routing leverage common prefixes to minimize routing table sizes. Consequently, numerically adjacent addresses are frequently assigned to the same administrative entity and exhibit similar patterns of packet loss. By probing an entire block, we are likely to capture both network infrastructure (e.g., routers or firewalls) and edge devices.

We focused on blocks of 256 addresses (/24 prefixes) because this corresponds to the smallest network size allowed in global routing tables and is a common unit of address delegation.

#### Balancing Conflicting Goals

In determining which blocks to survey, we faced several conflicting objectives:
- An unbiased sample is easiest to analyze.
- Blocks with some hosts present are more interesting.
- We want to sample parts of the Internet with extreme occupancy values.
- We need some blocks to remain stable across surveys to observe their evolution over time.
- It is likely that some blocks will cease to respond, either due to being firewalled, removed, or unused due to renumbering.

Our sampling methodology aims to balance these goals by using three different policies to select blocks:
- **Unchanging/Random**: Half of the blocks are selected randomly from all blocks that had any positive responses when we began surveys in September 2006. This set is relatively unbiased, affected only by the requirement that the block show some positive response.
- **Unchanging/Spaced**: Another quarter of the blocks are selected to uniformly cover a range of availabilities and volatilities, approximating the A, U-values defined in Section 2.4. This ensures that unusual blocks, such as fully-populated, always-up server farms and frequently changing, dynamically-addressed areas, are represented in the survey data.
- **Novel/Random**: The remaining half of the blocks are selected randomly for each survey from the set of /24 blocks that responded in the last census. This method has a bias towards active portions of the address space but is otherwise unbiased. While it does not capture the "birth" of newly used blocks, it reduces the probing of unused or unrouted space.

Despite these techniques, we still observe a moderately large number (27%) of unresponsive blocks in our surveys, suggesting that address usage is constantly evolving. Since all blocks for surveys are drawn from previously responsive blocks, our selection process may slightly over-represent responsiveness. Additionally, one quarter of blocks (unchanging/spaced) are selected non-randomly, potentially skewing results to represent "unusual" blocks. Given that most Internet blocks are sparsely populated (see Figure 2), this may result in a slight overestimate.

### Survey Duration

We collect surveys over periods of about one week. This duration is sufficient to capture daily cycles without overburdening the target address blocks. We plan to extend the collection period to 14 days to capture two weekend cycles.

### Metrics for Characterizing the Visible Internet

To characterize the visible Internet, we define two metrics: availability (A) and uptime (U).

- **Address Availability (A(addr))**: The fraction of time a host at an address responds positively.
- **Address Uptime (U(addr))**: The mean duration for which the address has a continuous positive response, normalized by the duration of the probing interval. This value approximates host uptime, though it cannot differentiate between a single host and a succession of different responsive hosts.

We also define block availability and uptime (A(block) and U(block)) as the mean A(addr) and U(addr) for all addresses in the block that are ever responsive. By definition, A(block) estimates the fraction of addresses that are up in that block. If addresses in a block follow a consistent allocation policy, it is also the probability that any responsive address is occupied.

Both A and U are defined for surveys and censuses. In censuses, the probe interval of months is sparse, making them rough, probabilistic estimates. Infrequent samples are particularly problematic for computing U(addr) over censuses; thus, we focus on U(addr) from surveys, where the sampling rate better matches actual host uptimes.

These measures are not completely orthogonal, as large values of U can only occur with large values of A, and small values of A correspond to small values of U. In fact, U = A/NU, where NU is the number of uptime periods. Taking the mean of all addresses in a /24 block may aggregate nodes with different functions or under different administrative entities.

Figure 2 illustrates these metrics and their relationship, showing a density plot of values for responding blocks from IT survey 15w. Most of the probability mass is near (A, U) = (0, 0) and along the U â‰ˆ 0 line, suggesting sparsely populated subnets where most addresses are unavailable.

### Understanding the Methodology

Before evaluating the visible Internet, we first evaluate our methodology. Active probing of a system as large and complex as the Internet is inherently imperfect, as the Internet changes before a complete snapshot can be taken. Our goal is to understand and quantify sources of error, minimizing them and ensuring they are not biased. We review inherent limitations of active probing and consider four potential sources of inaccuracy: probe protocol, measurement location, multi-homed hosts, and packet loss.

#### Active Probing and Invisible Hosts

The most significant limitation of our approach is that we can only see the visible Internet. Hosts hidden behind ICMP-dropping firewalls and in private address space (behind NATs) are missed, with NAT boxes appearing as a single occupied address. While the IETF requires hosts to respond to pings [4], many firewalls, including those in Windows XP SP1 and Vista, drop pings. However, such hosts are often placed behind ping-responsive routers or NAT devices.

While an OS-level characterization of the Internet is an open problem, we provide strong estimates of measurement error for USC and an evaluation of a random sample of Internet addresses. In Section 6, we look at visible firewall deployment. Studies of server logs, such as that of Xie et al. [50], can complement our approaches and provide insight into NATed hosts, as web logs of widely used services can see through NATs. A complete evaluation of the invisible Internet is an area of future work.

Network operators choose what to firewall and whether to block the protocols used in our probes. Blocking reduces our estimates, biasing them in favor of under-reporting usage. This bias is probably greater at sites that place greater emphasis on security. While we study the effects of firewalls and quantify them in the next section, our overall conclusions focus on the visible Internet.

#### Choice of Protocol for Active Probing

We have observed considerable skepticism about using ICMP for measuring active hosts, largely due to fears that it is widely filtered by firewalls. No method of active probing can detect a host that refuses to answer any query, but we compare ICMP and TCP as alternative mechanisms.

We validate ICMP probing by examining two populations:
- **USC Evaluation**: At USC, we use both active probes and passive traffic observation to estimate active addresses. University policies may differ from the general Internet, so we then compare ICMP and TCP-based probing for a random sample of addresses drawn from the entire Internet.
- **TCP-Based Probing**: We compare our ICMP methodology, described in Section 2.2, with TCP-based active probing and passive monitoring as described by Bartlett et al. [2]. TCP-based active probing uses Nmap applied to ports for HTTP, HTTPS, MySQL, FTP, and other common services.

### Data Summary

Table 2 lists the surveys we have conducted, including general surveys and the ICMP-nmapsurvey USC used for validation in Section 3.2. These datasets are available from the authors and have already been used by several external organizations.

| Name | Start Date | Duration (days) | /24 Blocks Probed | /24 Blocks Responded |
|------|------------|-----------------|-------------------|----------------------|
| ICMP 1 | 2003-06-01 | 51.08 | 2.52 | 217 |
| ICMP 2 | 2003-10-08 | 51.52 | 2.52 | 17,528 |
| TCP 1 | 2003-11-20 | 52.41 | 2.52 | 20,912 |
| IT 1 | 2004-06-21 | 57.49 | 2.40 | 20,866 |
| IT 2 | 2004-08-30 | 59.53 | 2.40 | 299 |
| IT 4 | 2005-01-05 | 63.15 | 2.43 | 260 |
| IT 5 | 2005-02-25 | 66.10 | 2.43 | 24,008 |
| IT 6 | 2005-07-01 | 69.89 | 2.65 | 24,007 |
| IT 7 | 2005-09-02 | 74.40 | 2.65 | 24,007 |
| IT 9 | 2005-12-14 | 73.88 | 2.65 | 768 |
| IT 11w | 2006-03-07 | 95.76 | 2.70 | - |
| IT 12w | 2006-04-13 | 96.80 | 2.70 | - |
| IT 13w | 2006-06-16 | 101.54 | 2.70 | - |
| IT 14w | 2006-09-14 | 101.17 | 2.75 | - |
| IT 15w | 2006-11-08 | 102.96 | 2.82 | - |
| IT 16w | 2007-02-14 | 104.77 | 2.90 | - |
| IT 17w | 2007-05-29 | 112.25 | 2.89 | - |

### Evaluation at USC

We compared ICMP and TCP-based probing on a week-long survey (ICMP-nmapsurvey USC) of all 81,664 addresses and about 50,000 students and staff at USC, comparing passive observation of all traffic with TCP and ICMP probing. The results are summarized in the following table:

| Category | Addresses Probed | Non-Responding | Responding (Any) | Responding (ICMP or TCP) |
|----------|------------------|----------------|------------------|--------------------------|
| Total    | 81,664           | 54,078         | 27,586           | 19,866                   |