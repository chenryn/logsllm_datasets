}
(1a)
(1b)
(1c)
(1d)
(1e)
(2a)
(2b)
(2c)
(2d)
(2e)
(2f)
(3a)
(3b)
(3c)
(3d)
(3e)
(3f)
(3g)
(3h)
(3i)
(4)
Figure 6: Transformation to compute candidate queries
Preservation of lengths of strings. The method of forc-
ing evaluation of candidate inputs along the control path
taken by the real input may at ﬁrst seem delicate and prone
to errors. An issue is that the operations performed on the
candidate path may raise exceptions. The most common
way this can happen is through string indexing: the pro-
gram may try to index into the i’th character of a string,
and this may cause an exception if the corresponding string
on the candidate evaluation is shorter than i. This is the
reason why we choose the candidate inputs to be precisely
the same length as the real inputs. Moreover, for all relevant
string operations we can show that the lengths of the real
and candidate strings are preserved to be equal. More pre-
cisely, consider a function g that takes strings and integers
as input and computes a string. The function g is said to be
length preserving, if the length of the string returned by g as
well as whether g throws an exception depends only on the
lengths of the parameter strings and the values of the inte-
ger variables. All string functions in the Java String class
(such as concatenation and substring function) are in fact
length-preserving. We can show that the strings for can-
didate variables are precisely as long as their real variable
counterparts across any sequence of commands and calls to
length-preserving functions. Therefore, they will not throw
any exception on the candidate evaluation. In all the exper-
iments we have conducted, the candidate path never raises
an exception.
External functions and stored queries. Candid also
handles scenarios where external functions and stored queries
are employed in a program. When an external function ext
(for which we do not have the source) is called, as long as
the function is free of side-eﬀects, Candid safely calls ext
twice, once on the real variables and once on the candidate
variables. Methods such as tainting are infeasible in this
scenario as tracking taints cannot be maintained on the ex-
ternal method; however, Candid can still keep track of the
real and intended structures using this mechanism.
Stored queries are snippets of queries stored in the database
or in a ﬁle, and programs use these snippets to form queries
dynamically. Stored queries are commonly used to maintain
changes to the database structure over time and to reﬂect
changes in conﬁgurations. Changes to stored queries pose
problems for static methods as the code requires a fresh
analysis, but poses no problems to Candid as it evaluates
attacks dynamically on each run.
5.
IMPLEMENTATION AND EVALUATION
Transformation. The automated transformation was im-
plemented for Java byte-code using an extension to the SOOT
optimization framework [23]. SOOT provides a three-address
intermediate byte-code representation, Jimple, suitable for
code analysis and optimization. Class ﬁles of the uninstru-
mented applications were processed using the SOOT frame-
work with CANDID to generate instrumented class ﬁles for
deployment.
The transformer handles all ﬁfteen types of Jimple state-
ments e.g., InvokeStmt, AssignStmt, etc.
If a statement is
found to be acting on, producing or leading to String type
objects, the transformer adds appropriate statements to per-
form candidate evaluation; for example, identity statements
are used to pass parameters to methods. For user deﬁned
Application
Employee Dir
Bookstore
Events
Classiﬁeds
Portal
Checkers
Oﬃcetalk
LOC
5,658
16,959
7,242
10,949
16453
5421
4543
Servlets
7 (10)
3 (28)
7 (13)
6 (14)
3 (28)
18 (61)
7 (64)
SCL
23
71
31
34
67
5
40
Table 1: Applications from the test suite
methods, corresponding to each String parameter, a candi-
date parameter is added to the method signature and an
identity statement is inserted in the method body for pa-
rameter passing.
As mentioned earlier, we compare the parse trees of the
real and candidate queries for attack detection. It is worth-
while to mention here that even the slightest mismatch of
the parse trees is detected as an attack.
Application examples. We evaluated our technique us-
ing a suite of applications that was obtained from an inde-
pendent research group [12]. This test suite contained seven
applications, ﬁve of which are commercial applications: Em-
ployee Directory, Bookstore, Events, Classiﬁeds and Por-
tal. These are available at http://www.gotocode.com. The
other two applications, Checkers and Oﬃcetalk, were devel-
oped by the same research group. These applications were
medium to large in size (4.5KLOC - 17KLOC).
Table 1 summarizes the statistics for each application.
The number of servlets in the second column gives the num-
ber exercised in our experiment, with the total number of
servlets in brackets. Our goal was to perform large-scale
automated tests (as described below), and some servlets
could only be accessed through a complex series of inter-
actions with the application that involved a human user,
and therefore were not exercised in our tests. The column
SCL reports the number of SQL Command Locations, which
issue either a sql.executeQuery (mainly SELECT statements)
and sql.executeUpdate (consisting of INSERT, UPDATE or DELETE
statements) to the database.
Immediately preceding this
command location, the Candid instrumentation calls the
parse tree comparison checker.
Attack Suite. The attack test suite was also obtained from
the authors of [12]. It consists, for each application, both
attack and non-attack inputs that test several kinds of SQL
injection vulnerabilities. Overall, the attack suite contains
30 diﬀerent attack string patterns (such as tautology-based
attacks, UNION SELECT based attacks [14]), that were
constructed based on real attacks obtained from sources
US/CERT and CERT/CC advisories. Based on these at-
tack strings, the attack test suite makes use of each servlet’s
injectible web inputs.
The test suite also contained non-attack (benign) inputs
that tested the resilience of the application on legitimate
inputs that “look like” attack inputs. These inputs contain
data that may possibly break the application in the face of
SQL input validation techniques.
Experiment setup. Our objective was to deploy two ver-
sions of each application: (1) an original uninstrumented
VM1
VM2
Application
Input
attemp.
Succ.
Attacks
FPs/ Parse
Non- Errors
1
3
2
4
Tester
ATTACK
BENIGN
  FP
ERROR
Figure 7: Testbed Setup
EmployeeDir
Bookstore
Events
Classiﬁeds
Portal
Checkers
Oﬃcetalk
7,038
6,492
7,109
6,679
7,483
8,254
6,599
1529/1529
1438/1438
1414/1414
1475/1475
2995/2995
262/262
390/390
attacks
(Benign)
0/2442
0/2930
0/3320
0/2076
0/3415
0/7435
0/2149
3067
2124
2375
3128
1073
557
4060
Table 2: Attack Evaluation results
version and (2) a Candid protected version. Also, to sim-
ulate a live-test scenario, we wanted to deploy attacks si-
multaneously on each of these two versions and observe the
results. We wanted the original and instrumented versions
to be isolated from each other, so that they do not aﬀect the
correctness of tests. For this reason we decided to run them
on two separate machines.
In order that the state of the two machines be the same
at the beginning of the experiments, we adopted the fol-
lowing strategy: On a host RedHat GNU / Linux system,
we created a virtual machine running on VMware also run-
ning RedHat EL 4.0 guest operating system. We then in-
stalled all the necessary software in this virtual machine: the
Apache webserver and Tomcat JSP server, MySQL database
server, and the source and bytecode of all Java web ap-
plications (original and instrumented versions) in our test
suite. Through an automated script, we also populated the
database with tables required by these applications. After
conﬁguring the applications to deployment state, we cloned
this virtual machine by copying all the virtual disk ﬁles to
another host machine with similar conﬁgurations. This re-
sulted in two machines that were identical except for the
fact that they ran the original and instrumented versions of
the web applications.
Figure 7 illustrates the testbed setup. The original appli-
cation was deployed on virtual machine VM1 and the instru-
mented application was deployed on virtual machine VM2.
A third machine (“Tester”) was used to launch the attacks
over HTTP on the original and instrumented web applica-
tions, and also was used to immediately analyze the result.
For this purpose, a suite of Perl scripts utilizing the wget
command were developed and used. The master script that
executed the attack scripts ran the following sequence, as
shown in the Figure 7: (1) it launched the attack ﬁrst on
the original application and (2) recorded the responses. It
then (3) launched the attack on the instrumented applica-
tion and (4) recorded the responses. After step (4), another
post-processing script compared the output from the two
VMs and classiﬁed the result into one of the following cases
(a) the attack was successfully detected by the instrumented
application (b) the instrumented application reported a be-
nign string as an attack (c) the instrumented application
reported a benign string as benign (d) errors were reported
by the original or instrumented application.
Attack evaluation. We ran the instrumented application
with the attack suite, and the results are summarized in
Figure 8: Performance Overhead
Table 2. The second column lists the number of input at-
tempts, and the third lists the number of successful attacks
on the original application. The number of attacks detected
by the instrumented application is shown in the same col-
umn. The fourth column shows the number of non-attack
benign inputs and any false positives for the instrumented
application. Candid instrumented applications were able to
defend all the attacks in the suite, and there were zero false
positives.
The test suite we received had a large number of attack
strings that resulted in invalid SQL queries and are reported
in column 5. We used a standard SQL parser based on
SQL ANSI 92 standard, augmented with MYSQL speciﬁc
language extensions. To ensure the correctness of our parser
implementation, we veriﬁed that these queries were in fact
malformed using an online SQL Query formatter [1].
Performance evaluation. For testing the performance
impact we used the web application benchmarking tool JMe-
ter [6] from Apache Foundation, an industry standard tool
for measuring performance impact on Java web applications.
We computed the overhead imposed by the approach on
one servlet that was chosen from each application, and pre-
pared a detailed test suite for each application. As typi-
cal for web applications, the performance was measured in
terms of diﬀerences in response times as seen by the client.
The server was on a Red Hat Enterprise GNU / Linux ma-
chine with a 2GHz Pentium processor and 2GB of RAM,
that ran in the same Ethernet network as the client. Note
that this scenario does not have any network latencies that
are typical for many web applications, and is therefore an
indicator of the worst case overheads in terms of response
times.
For each test, we took 1000 sample runs and measured
the average numbers for each run, with caching disabled
on the JSP / Web/ DB servers. The results are shown in
Figure 8. The ﬁgure depicts the time taken by the original
application, the transformed code, and also the transformed
code without the parser component.
Figure 8 indicates that instrumented applications without
SQL parser calls had negligible overhead over the original
applications (also optimized for performance using SOOT),
when compared to uninstrumented applications.
Figure 8 also indicates that instrumented applications with
SQL parser code had varying overheads and ranged from
3.2% (for Portal application) to 40.0% (for OﬃceTalk appli-
cation). These varying overheads are mainly attributed to
varying numbers of SQL parser calls in the tested control
path e.g., Bookstore application invoked SQL parser code 7
times for the selected page, whereas Portal application only