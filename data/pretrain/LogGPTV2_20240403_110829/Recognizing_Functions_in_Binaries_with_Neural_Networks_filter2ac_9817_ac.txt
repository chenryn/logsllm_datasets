In the beginning, we initialize the weights of the neural
network randomly, uniformly drawn from a small range
USENIX Association  
24th USENIX Security Symposium  617
7
f. start
at 0?
f. start
at 1?
f. start
at 2?
f. start
at 3?
f. start
at 4?
f. start
at 5?
f. start
at 6?
f. start
at 7?
byte 0
byte 1
byte 2
byte 3
byte 4
byte 5
byte 6
byte 7
Figure 2: A depiction of the basic architecture of our
approach.
near 0. A normal distribution with small variance and
mean 0 also sees much use for this purpose. Having the
proper initialization can prove crucial to whether we can
successfully learn useful parameters for the neural net-
work. We do not initialize the weights to 0, as this leaves
the loss function on a saddle point and prevents optimiza-
tion.
Recall the form of the loss function from Section 3. If
we have N different items in the training data, then the
loss function for the network requires evaluating the net-
work over all training examples, since it takes the form
1
N
N
∑
i=1
d(yi, ˆyi).
Due to how backpropagation works, computing the gra-
dient also requires evaluating the network on all training
data. Since we need to compute the gradient a very large
number of times during optimization, we would like to
avoid performing such an expensive step as a part of it.
Instead of computing the loss over all N items, we
can instead compute it over a randomly-selected one at
a time. The expectation of the gradient computed in this
way equals the gradient averaged over all N examples.
Optimization using these gradients is called stochastic
gradient descent.
It is possible to show that given a
well-behaved convex function, stochastic gradient de-
scent will ﬁnd the minimum value. Even in the case of
neural networks, where we lack such theoretical guaran-
tees, experience shows that stochastic gradient descent
can work quite well; in fact, since computing each gra-
dient takes much less time, results show that stochastic
gradient descent allows for much faster convergence in
practice.
The most elementary gradient descent methods pre-
scribe changing the parameters in the direction of the
gradient each iteration, but optimization of some kinds of
functions can beneﬁt from moving in a slightly different
direction. Consider a two-dimensional function, which
when graphed looks like an elliptical bowl. Then along
the axis in which we are closest to the minimum point,
the gradient will have the largest magnitude, as the sur-
face of the bowl is steeper in that direction, even though
we should move further in the other axis and only a little
bit in this one.
In this work, we use a method called rmsprop [19]; it
involves keeping a running average of the magnitude of
each dimension in the gradients seen so far. It then scales
each dimension in the gradient, enlarging the dimensions
which have a small average and shrinking those which
have a large one. This follows the intuition given in the
previous paragraph about the elliptical bowl.
We also scale the entire gradient each time by a step
If the step size is too big, then the optimiza-
size.
tion might fail as the value of the function does not de-
crease; if the step size is too small, then optimization will
progress slowly or get stuck at a local minimum. Often it
makes sense to reduce the learning rate over time, since
in the beginning we expect radically-incorrect weights
(given their random initialization), whereas after some
iterations, the weights should have nearly reached an op-
timum value. For our experiments, we scaled the learn-
ing rate by the inverse square root of the current iteration
number (i.e., halved after 4 iterations, quartered after 16
iterations, and so on), which we found to work well.
4.3 Training with mini-batches
In stochastic gradient descent, we compute the gradient
of the weights with respect to only one example in each
iteration. However, this can cause a large variance in the
gradients since each example might signiﬁcantly differ
from one to the next. So instead of computing the gradi-
ent over only one example at a time, it can help to average
the gradients from a small number of examples, called a
mini-batch.
While this increases the time needed for each iteration,
it does so more modestly than it may initially seem. Eval-
uating the neural network with a single example involves
a large number of matrix-vector multiplications, so we
can efﬁciently and simultaneously evaluate for many ex-
amples by replacing these with matrix-matrix multipli-
cations, especially when using highly-optimized linear
algebra libraries.
In our application, since each example is a sequence
of bytes from a binary, one might vary in length from
another. However, to compute with mini-batches efﬁ-
ciently, we need to pack the examples together into a ma-
trix or tensor with padding to extend too-short examples.
Then all examples get evaluated for the same number of
time steps, so it helps to put examples of similar length
together in a mini-batch to avoid wasted computation.
Also, we need to take care as to avoid computing the loss
over those parts of the mini-batch added as padding.
4.4 Data preparation
For the task of function identiﬁcation, we can intuitively
expect that solving the problem likely does not require
618  24th USENIX Security Symposium 
USENIX Association
8
f. start
at 0?
f. start
at 1?
f. start
at 2?
f. start
at 3?
f. start
at 4?
f. start
at 5?
f. start
at 6?
f. start
at 7?
f. start
at 0?
f. start
at 1?
f. start
at 2?
f. start
at 3?
f. start
at 4?
f. start
at 5?
f. start
at 6?
f. start
at 7?
byte 0
byte 1
byte 2
byte 3
byte 4
byte 5
byte 6
byte 7
Figure 3: A bi-directional RNN. Note the horizon-
tal arrows pointing in both directions. The forward-
propagated and backward-propagated hidden states, rep-
resented by the overlapping squares, do not directly in-
teract with each other. However, computing the output
uses a concatenation of the two states.
remembering information from hundreds of thousands of
bytes in the past or in the future. Code calling other func-
tions can occur far away from the location of that func-
tion, and theoretically, we might track such references
to help determine where functions occur.
In practice,
functions typically perform some series of steps at entry
and exit, the patterns for which we can learn and should
largely sufﬁce for detecting functions.
Therefore, we use ﬁxed-length subsequences taken
from binaries instead of entire binaries themselves. Ex-
cept in rare cases where functions occur near the bound-
ary of the subsequence, there should be enough infor-
mation to make the determination of the existence of a
function or not. Similar to how stochastic gradient de-
scent enables faster convergence by speeding up each
update, computing the gradient on truncated sequences
takes much less time and enables faster iterations.
We also try reversing the order of bytes in the input
before providing it to the neural network, under the intu-
ition that the function prologue, which identiﬁes the be-
ginning of a function and makes it recognizable as such,
occurs after the position where we want to predict the be-
ginning of a function. Since the RNN only has access to
bytes from before the current position, not after, revers-
ing the order should help the RNN learn.
4.5 Bi-directional RNNs
With the recurrent neural networks discussed in Sec-
tion 3.4, the output at each time step depends only on
the inputs which occur at that time step or before. This
model makes sense in some applications where there ex-
ists an inherent temporal component to the input; for ex-
ample, in real-time speech or handwriting recognition.
For binary analysis, we have access to the entire binary
at once, so there exists no need to conﬁne ourselves in
this way.
An extension which allows access to both the past and
the future in making a prediction for the present is to
combine two recurrent neural networks, one which oper-
byte 0
byte 1
byte 2
byte 3
byte 4
byte 5
byte 6
byte 7
Figure 4: A multi-layer RNN with three bi-directional
hidden layers.
In the second and third layers, both
the forward-propagated and backward-propagated states
have access to either state from the previous layer.
ates from the beginning of the sequence to the end, and
another which operates in the other direction. Figure 3
illustrates the approach.
In terms of graphical models, we could say that reg-
ular (unidirectional) RNNs behave like hidden Markov
models, where the hidden state at each time step depends
on only the hidden state of the previous time step. Then
bidirectional RNNs are analogous to a chain conditional
random ﬁeld, since the hidden state there relates to the
hidden states of both the previous and next time steps.
4.6 Multi-layer RNNs
The approaches we have described so far contain only
one hidden layer. Depending on the complexity of the
pattern we wish to learn, a single hidden layer may prove
insufﬁcient due to its limited capacity. If we limit our-
selves to one hidden layer, achieving good results may
require a very large one, which can signiﬁcantly increase
the amount of processing power required.
In other applications of neural networks like computer
vision and speech recognition, using many smaller hid-
den layers has worked better than using one hidden layer
of larger size. During the evaluation, we empirically ver-
ify the results of using one versus multiple hidden layers.
Figure 4 illustrates an example architecture.
5 Evaluation
In this section, we describe the empirical results we ob-
tained from training a variety of different models on a
dataset of binaries. We seek to answer the following
questions:
• Can recurrent neural networks successfully solve
the problem of function identiﬁcation in binaries?
• How much computational power do recurrent neu-
ral networks require for solving this task?
USENIX Association  
24th USENIX Security Symposium  619
9
Number of binaries
Number of bytes
Number of functions
Average function length
ELF x86
1,032
ELF x86-64
1,032
PE x86
68
PE x86-64
68
138,547,936
145,544,012
29,093,888
33,351,168
303,238
448.84
295,121
499.54
93,288
292.85
94,548
330.03
Table 1: Characteristics of the binary dataset used for evaluation.
ELF x86
R
P
ELF x86-64
F1
P
R
F1
ByteWeight (func. start)
Our models (func. start)
Our models (func. end)
98.41% 97.94% 98.17% 99.14% 98.47% 98.80%
99.56% 99.06% 99.31% 98.80% 97.80% 98.30%
98.69% 97.87% 98.28% 97.45% 95.03% 96.22%
PE x86
R
P
PE x86-64
F1
P
R
F1
ByteWeight (func. start)
Our models (func. start)
Our models (func. end)
93.78% 95.37% 94.57% 97.88% 97.98% 97.93%
99.01% 98.46% 98.74% 99.52% 99.09% 99.31%
99.24% 98.35% 98.79% 99.28% 99.20% 99.24%
Table 2: Function start and end identiﬁcation: summary of our best results, and comparison with previous work. “P”
is precision and “R” is recall. Results of previous work comes from Table 3 of Bao et al. [2]; they did not attempt to
identify function ends independently, so we lack those results here.
• How do variations in the model’s design affect the
performance?
We ran our experiments on Amazon EC2 using
c4.2xlarge instances, each of which contains 8 cores
of a 2.9 GHz Intel Xeon processor and 15 GB of RAM.
5.1 Dataset
Our dataset comes from Bao et al. [2], consisting of
2200 separate binaries. 2064 of the binaries were for
Linux, obtained from the coreutils, binutils, and
findutils packages. The remaining 136 for Windows
consist of binaries from popular open-source projects.
Half of the binaries were for x86, and the other half for
x86-64. Half of the Linux binaries were compiled with
Intel’s icc, while the other half used gcc. The binaries
for Windows were compiled using Microsoft Visual Stu-
dio. Each binary was compiled with one of four different
optimization levels. Table 1 summarizes some statistics
from the dataset.
Following the procedure in Bao et al. we trained a sep-
arate model for each of the four (architecture, OS) con-
ﬁgurations. To report comparable results, we also use 10-
fold cross-validation as in Bao et al.; we train ten models
for each of the four conﬁgurations, where each of the ten
models uses a different 10% of the binaries as the testing
set.
Implementation
5.2
We implemented our models in Python using Theano [4],
a linear algebra and automatic differentiation library de-
signed to aid in implementation of machine learning and
optimization methods. In Theano, we specify our model
as operations on symbolic variables, allowing for con-
struction of a computation graph that describes the op-
erations necessary to compute the result. It can convert
this graph into C/C++ code and automatically compute
partial derivatives of functions through application of the
chain rule.
While Theano can also compile code for use on the
GPU, we only used the CPU in our experiments for sim-
pler implementation. Also, while both training and eval-
uation of RNNs are amenable to parallelization, we also
did not use multi-threading for our experiments, and in-
stead ran an independent experiment on each core.
5.3 Summary of results
Tables 2 and 3 summarize our main experimental results.
In both tables, we compare to the results as reported by
Bao et al. [2], which are marked as “ByteWeight”.
For the function start identiﬁcation problem, our meth-
ods consistently obtain F1 scores in the range of 98-99%.
This is in line with the results from Bao et al., except on
the PE x86 dataset where we improve by about 4 per-
centage points in F1 score.
For function boundary identiﬁcation, we trained two
620  24th USENIX Security Symposium 
USENIX Association
10
ELF x86