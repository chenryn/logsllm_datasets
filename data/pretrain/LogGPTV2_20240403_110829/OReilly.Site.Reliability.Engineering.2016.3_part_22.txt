Operational Work
The Ops lead works with the incident commander to respond to the incident by
applying operational tools to the task at hand. The operations team should be the
only group modifying the system during an incident.
Communication
This person is the public face of the incident response task force. Their duties
most definitely include issuing periodic updates to the incident response team
and stakeholders (usually via email), and may extend to tasks such as keeping the
incident document accurate and up to date.
2 See http://www.fema.gov/national-incident-management-system for further details.
Elements of Incident Management Process | 163
Planning
The planning role supports Ops by dealing with longer-term issues, such as filing
bugs, ordering dinner, arranging handoffs, and tracking how the system has
diverged from the norm so it can be reverted once the incident is resolved.
A Recognized Command Post
Interested parties need to understand where they can interact with the incident
commander. In many situations, locating the incident task force members into a cen‐
tral designated “War Room” is appropriate. Others teams may prefer to work at their
desks, keeping alert to incident updates via email and IRC.
Google has found IRC to be a huge boon in incident response. IRC is very reliable
and can be used as a log of communications about this event, and such a record is
invaluable in keeping detailed state changes in mind. We’ve also written bots that log
incident-related traffic (which is helpful for postmortem analysis), and other bots that
log events such as alerts to the channel. IRC is also a convenient medium over which
geographically distributed teams can coordinate.
Live Incident State Document
The incident commander’s most important responsibility is to keep a living incident
document. This can live in a wiki, but should ideally be editable by several people
concurrently. Most of our teams use Google Docs, though Google Docs SRE use Goo‐
gle Sites: after all, depending on the software you are trying to fix as part of your inci‐
dent management system is unlikely to end well.
See Appendix C for a sample incident document. This living doc can be messy, but
must be functional. Using a template makes generating this documentation easier,
and keeping the most important information at the top makes it more usable.Retain
this documentation for postmortem analysis and, if necessary, meta analysis.
Clear, Live Handoff
It’s essential that the post of incident commander be clearly handed off at the end of
the working day. If you’re handing off command to someone at another location, you
can simply and safely update the new incident commander over the phone or a video
call. Once the new incident commander is fully apprised, the outgoing commander
should be explicit in their handoff, specifically stating, “You’re now the incident
commander, okay?”, and should not leave the call until receiving firm acknowledg‐
ment of handoff. The handoff should be communicated to others working on the
incident so that it’s clear who is leading the incident management efforts at all times.
164 | Chapter 14: Managing Incidents
A Managed Incident
Now let’s examine how this incident might have played out if it were handled using
principles of incident management.
It’s 2 p.m., and Mary is into her third coffee of the day. The pager’s harsh tone sur‐
prises her, and she gulps the drink down. Problem: a datacenter has stopped serving
traffic. She starts to investigate. Shortly another alert fires, and the second datacenter
out of five is out of order. Because this is a rapidly growing issue, she knows that she’ll
benefit from the structure of her incident management framework.
Mary snags Sabrina. “Can you take command?” Nodding her agreement, Sabrina
quickly gets a rundown of what’s occurred thus far from Mary. She captures these
details in an email that she sends to a prearranged mailing list. Sabrina recognizes
that she can’t yet scope the impact of the incident, so she asks for Mary’s assessment.
Mary responds, “Users have yet to be impacted; let’s just hope we don’t lose a third
datacenter.” Sabrina records Mary’s response in a live incident document.
When the third alert fires, Sabrina sees the alert among the debugging chatter on IRC
and quickly follows up to the email thread with an update. The thread keeps VPs
abreast of the high-level status without bogging them down in minutiae. Sabrina asks
an external communications representative to start drafting user messaging. She then
follows up with Mary to see if they should contact the developer on-call (currently
Josephine). Receiving Mary’s approval, Sabrina loops in Josephine.
By the time Josephine logs in, Robin has already volunteered to help out. Sabrina
reminds both Robin and Josephine that they are to prioritize any tasks delegated to
them by Mary, and that they must keep Mary informed of any additional actions they
take. Robin and Josephine quickly familiarize themselves with the current situation
by reading the incident document.
By now, Mary has tried the old binary release and found it wanting: she mutters this
to Robin, who updates IRC to say that this attempted fix didn’t work. Sabrina pastes
this update into the live incident management document.
At 5 p.m., Sabrina starts finding replacement staff to take on the incident, as she and
her colleagues are about to go home. She updates the incident document. A brief
phone conference takes place at 5:45 so everyone is aware of the current situation. At
6 p.m., they hand off their responsibilities to their colleagues in the sister office.
Mary returns to work the following morning to find that her transatlantic colleagues
have assumed responsibility for the bug, mitigated the problem, closed the incident,
and started work on the postmortem. Problem solved, she brews some fresh coffee
and settles down to plan structural improvements so problems of this category don’t
afflict the team again.
A Managed Incident | 165
When to Declare an Incident
It is better to declare an incident early and then find a simple fix and close out the
incident than to have to spin up the incident management framework hours into a
burgeoning problem. Set clear conditions for declaring an incident. My team follows
these broad guidelines—if any of the following is true, the event is an incident:
• Do you need to involve a second team in fixing the problem?
• Is the outage visible to customers?
• Is the issue unsolved even after an hour’s concentrated analysis?
Incident management proficiency atrophies quickly when it’s not in constant use. So
how can engineers keep their incident management skills up to date—handle more
incidents? Fortunately, the incident management framework can apply to other
operational changes that need to span time zones and/or teams. If you use the frame‐
work frequently as a regular part of your change management procedures, you can
easily follow this framework when an actual incident occurs. If your organization
performs disaster-recovery testing (you should, it’s fun: see [Kri12]), incident man‐
agement should be part of that testing process. We often role-play the response to an
on-call issue that has already been solved, perhaps by colleagues in another location,
to further familiarize ourselves with incident management.
In Summary
We’ve found that by formulating an incident management strategy in advance, struc‐
turing this plan to scale smoothly, and regularly putting the plan to use, we were able
to reduce our mean time to recovery and provide staff a less stressful way to work on
emergent problems. Any organization concerned with reliability would benefit from
pursuing a similar strategy.
166 | Chapter 14: Managing Incidents
Best Practices for Incident Management
Prioritize. Stop the bleeding, restore service, and preserve the evidence for root-
causing.
Prepare. Develop and document your incident management procedures in advance,
in consultation with incident participants.
Trust. Give full autonomy within the assigned role to all incident participants.
Introspect. Pay attention to your emotional state while responding to an incident. If
you start to feel panicky or overwhelmed, solicit more support.
Consider alternatives. Periodically consider your options and re-evaluate whether it
still makes sense to continue what you’re doing or whether you should be taking
another tack in incident response.
Practice. Use the process routinely so it becomes second nature.
Change it around. Were you incident commander last time? Take on a different role
this time. Encourage every team member to acquire familiarity with each role.
In Summary | 167
CHAPTER 15
Postmortem Culture: Learning from Failure
Written by John Lunney and Sue Lueder
Edited by Gary O’ Connor
The cost of failure is education.
—Devin Carraway
As SREs, we work with large-scale, complex, distributed systems. We constantly
enhance our services with new features and add new systems. Incidents and outages
are inevitable given our scale and velocity of change. When an incident occurs, we fix
the underlying issue, and services return to their normal operating conditions. Unless
we have some formalized process of learning from these incidents in place, they may
recur ad infinitum. Left unchecked, incidents can multiply in complexity or even cas‐
cade, overwhelming a system and its operators and ultimately impacting our users.
Therefore, postmortems are an essential tool for SRE.
The postmortem concept is well known in the technology industry [All12]. A post‐
mortem is a written record of an incident, its impact, the actions taken to mitigate or
resolve it, the root cause(s), and the follow-up actions to prevent the incident from
recurring. This chapter describes criteria for deciding when to conduct postmortems,
some best practices around postmortems, and advice on how to cultivate a postmor‐
tem culture based on the experience we’ve gained over the years.
Google’s Postmortem Philosophy
The primary goals of writing a postmortem are to ensure that the incident is docu‐
mented, that all contributing root cause(s) are well understood, and, especially, that
effective preventive actions are put in place to reduce the likelihood and/or impact of
recurrence. A detailed survey of root-cause analysis techniques is beyond the scope of
this chapter (instead, see [Roo04]); however, articles, best practices, and tools abound
169
in the system quality domain. Our teams use a variety of techniques for root-cause
analysis and choose the technique best suited to their services. Postmortems are
expected after any significant undesirable event. Writing a postmortem is not punish‐
ment—it is a learning opportunity for the entire company. The postmortem process
does present an inherent cost in terms of time or effort, so we are deliberate in choos‐
ing when to write one. Teams have some internal flexibility, but common postmor‐
tem triggers include:
• User-visible downtime or degradation beyond a certain threshold
• Data loss of any kind
• On-call engineer intervention (release rollback, rerouting of traffic, etc.)
• A resolution time above some threshold
• A monitoring failure (which usually implies manual incident discovery)
It is important to define postmortem criteria before an incident occurs so that every‐
one knows when a postmortem is necessary. In addition to these objective triggers,
any stakeholder may request a postmortem for an event.
Blameless postmortems are a tenet of SRE culture. For a postmortem to be truly
blameless, it must focus on identifying the contributing causes of the incident without
indicting any individual or team for bad or inappropriate behavior. A blamelessly
written postmortem assumes that everyone involved in an incident had good inten‐
tions and did the right thing with the information they had. If a culture of finger
pointing and shaming individuals or teams for doing the “wrong” thing prevails, peo‐
ple will not bring issues to light for fear of punishment.
Blameless culture originated in the healthcare and avionics industries where mistakes
can be fatal. These industries nurture an environment where every “mistake” is seen
as an opportunity to strengthen the system. When postmortems shift from allocating
blame to investigating the systematic reasons why an individual or team had incom‐
plete or incorrect information, effective prevention plans can be put in place. You
can’t “fix” people, but you can fix systems and processes to better support people
making the right choices when designing and maintaining complex systems.
When an outage does occur, a postmortem is not written as a formality to be forgot‐
ten. Instead the postmortem is seen by engineers as an opportunity not only to fix a
weakness, but to make Google more resilient as a whole. While a blameless postmor‐
tem doesn’t simply vent frustration by pointing fingers, it should call out where and
how services can be improved. Here are two examples:
170 | Chapter 15: Postmortem Culture: Learning from Failure
Pointing fingers
“We need to rewrite the entire complicated backend system! It’s been breaking
weekly for the last three quarters and I’m sure we’re all tired of fixing things
onesy-twosy. Seriously, if I get paged one more time I’ll rewrite it myself…”
Blameless
“An action item to rewrite the entire backend system might actually prevent these
annoying pages from continuing to happen, and the maintenance manual for this
version is quite long and really difficult to be fully trained up on. I’m sure our
future on-callers will thank us!”
Best Practice: Avoid Blame and Keep It Constructive
Blameless postmortems can be challenging to write, because the postmortem format
clearly identifies the actions that led to the incident. Removing blame from a post‐
mortem gives people the confidence to escalate issues without fear. It is also impor‐
tant not to stigmatize frequent production of postmortems by a person or team. An
atmosphere of blame risks creating a culture in which incidents and issues are swept
under the rug, leading to greater risk for the organization [Boy13].
Collaborate and Share Knowledge
We value collaboration, and the postmortem process is no exception. The postmor‐
tem workflow includes collaboration and knowledge-sharing at every stage.
Our postmortem documents are Google Docs, with an in-house template (see Appen‐
dix D). Regardless of the specific tool you use, look for the following key features:
Real-time collaboration
Enables the rapid collection of data and ideas. Essential during the early creation
of a postmortem.
An open commenting/annotation system
Makes crowdsourcing solutions easy and improves coverage.
Email notifications
Can be directed at collaborators within the document or used to loop in others to
provide input.
Writing a postmortem also involves formal review and publication. In practice, teams
share the first postmortem draft internally and solicit a group of senior engineers to
assess the draft for completeness. Review criteria might include:
Collaborate and Share Knowledge | 171
• Was key incident data collected for posterity?
• Are the impact assessments complete?
• Was the root cause sufficiently deep?
• Is the action plan appropriate and are resulting bug fixes at appropriate priority?
• Did we share the outcome with relevant stakeholders?
Once the initial review is complete, the postmortem is shared more broadly, typically
with the larger engineering team or on an internal mailing list. Our goal is to share
postmortems to the widest possible audience that would benefit from the knowledge
or lessons imparted. Google has stringent rules around access to any piece of infor‐
mation that might identify a user,1 and even internal documents like postmortems
never include such information.
Best Practice: No Postmortem Left Unreviewed
An unreviewed postmortem might as well never have existed. To ensure that each
completed draft is reviewed, we encourage regular review sessions for postmortems.
In these meetings, it is important to close out any ongoing discussions and com‐
ments, to capture ideas, and to finalize the state.
Once those involved are satisfied with the document and its action items, the post‐
mortem is added to a team or organization repository of past incidents.2 Transparent
sharing makes it easier for others to find and learn from the postmortem.
Introducing a Postmortem Culture
Introducing a postmortem culture to your organization is easier said than done; such
an effort requires continuous cultivation and reinforcement. We reinforce a collabo‐
rative postmortem culture through senior management’s active participation in the
review and collaboration process. Management can encourage this culture, but
blameless postmortems are ideally the product of engineer self-motivation. In the spi‐
rit of nurturing the postmortem culture, SREs proactively create activities that dis‐
seminate what we learn about system infrastructure. Some example activities include:
Postmortem of the month
In a monthly newsletter, an interesting and well-written postmortem is shared
with the entire organization.
1 See http://www.google.com/policies/privacy/.
2 If you’d like to start your own repository, Etsy has released Morgue, a tool for managing postmortems.
172 | Chapter 15: Postmortem Culture: Learning from Failure
Google+ postmortem group
This group shares and discusses internal and external postmortems, best practi‐
ces, and commentary about postmortems.
Postmortem reading clubs
Teams host regular postmortem reading clubs, in which an interesting or impact‐
ful postmortem is brought to the table (along with some tasty refreshments) for
an open dialogue with participants, nonparticipants, and new Googlers about
what happened, what lessons the incident imparted, and the aftermath of the
incident. Often, the postmortem being reviewed is months or years old!
Wheel of Misfortune
New SREs are often treated to the Wheel of Misfortune exercise (see “Disaster
Role Playing” on page 401), in which a previous postmortem is reenacted with a
cast of engineers playing roles as laid out in the postmortem. The original inci‐
dent commander attends to help make the experience as “real” as possible.
One of the biggest challenges of introducing postmortems to an organization is that
some may question their value given the cost of their preparation. The following
strategies can help in facing this challenge:
• Ease postmortems into the workflow. A trial period with several complete and