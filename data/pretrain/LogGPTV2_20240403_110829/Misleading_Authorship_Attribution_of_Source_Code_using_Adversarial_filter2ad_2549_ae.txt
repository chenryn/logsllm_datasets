clang-format, which normalizes the layout of the code.
While the attribution method can identify 27.5% of the
programmers based on layout features if the code is not for-
matted, the performance decreases to 4.5% if we apply the
formatting tool to the source code. We thus conclude that it is
trivial to mislead an attribution based on layout features.
7 Limitations
Our previous experiments demonstrate the impact of our at-
tack on program authorship attribution. Nonetheless, our
approach has limitations which we discuss in the following.
Adversarial examples (cid:54)= anonymization. Our attack en-
ables a programmer to hide their identity in source code by
misleading an attribution. While such an attack protects the
privacy of the programmer, it is not sufﬁcient for achieving
anonymity. Note that k-anonymity would require a set of
k developers that are equally likely to be attributed to the
source code. In our setting, the code of the programmer is
transformed to match a different author and an anonymity set
of sufﬁcient size is not guaranteed to exist. Still, we consider
anonymization as promising direction for further research,
which can build on the concepts of code transformations de-
veloped in this paper.
Veriﬁcation of semantics. Finally, we consider two pro-
grams to be semantically equivalent if they return the same
output for a given input. In particular, we verify that the trans-
formed source code is semantically equivalent by applying
the test cases provided by the GCJ competition. Although
this approach is reasonable in our setting, it cannot guaran-
tee strict semantic equivalence in all possible cases. Some
of the exchanged API functions, for example, provide the
same functionality but differ in corner cases, such as when
the memory is exhausted. We acknowledge this limitation,
yet it does not impact the general validity of our results.
8 Related Work
The automatic attack of source-code authorship attribution
touches different areas of security research. In this section,
we review related methods and concepts.
490    28th USENIX Security Symposium
USENIX Association
Authorship attribution of source code. Identifying the au-
thor of a program is a challenging task of computer security
that has attracted a large body of work in the last years. Start-
ing from early approaches experimenting with hand-crafted
features [14, 16], the techniques for examining source code
have constantly advanced, for example, by incorporating ex-
pressive features, such as n-grams [e.g., 1, 8, 13] and abstract
syntax trees [e.g., 4, 9, 21]. Similarly, techniques for analyz-
ing native code and identifying authors of compiled programs
have advanced in the last years [e.g., 3, 10, 17, 22].
Two notable examples for source code are the approach by
Caliskan et al. [9] and by Abuhamad et al. [1]. The former
inspects features derived from code layout, lexical analysis
and syntactic analysis. Regarding comprehensiveness, this
work can be considered as the current state of the art. The
work by Abuhamad et al. [1] focuses on lexical features as
input for recurrent neural networks. Their work covers the
largest set of authors so far and makes use of recent advances
in deep learning. Table 6 shows the related approaches.
Method
*Abuhamad et al. [1]
*Caliskan et al. [9]
Alsulami et al. [4]
Frantzeskou et al. [13]
Krsul and Spafford [14]
Burrows et al. [8]
Lay
•
•
•
•
Lex
•
•
•
•
•
Syn
•
•
•
Authors
8903
250
70
30
29
10
Results
92%
95%
89%
97%
73%
77%
Table 6: Comparison of approaches for source code authorship attribution.
Lay = Layout features, Lex = Lexical features, Syn = Syntactic features.
*Attacked in this paper.
Previous work, however, has mostly ignored the problem
of untargeted and targeted attacks. Only the empirical study
by Simko et al. [25] examines how programmers can mislead
the attribution by Caliskan et al. [9] by mimicking the style
of other developers. While this study provides valuable in-
sights into the risk of forgeries, it does not consider automatic
attacks and thus is limited to manipulations by humans. In
this paper, we demonstrate that such attacks can be fully auto-
mated. Our generated forgeries even provide a higher success
rate than the handcrafted samples in the study. Moreover,
we evaluate the impact of different feature sets and learning
algorithms by evaluating two attribution methods.
Adversarial machine learning. The security of machine
learning techniques has also attracted a lot of research re-
cently. A signiﬁcant fraction of work on attacks has focused
on scenarios where the problem and feature space are mainly
identical [see 6, 11, 18]. In these scenarios, changes in the
problem space, such as the modiﬁcation of an image pixel,
have a one-to-one effect on the feature space, such that so-
phisticated attack strategies can be applied. By contrast, a
one-to-one mapping between source code and the extracted
features cannot be constructed and thus we are required to
introduce a mixed attack strategy (see Section 3).
Creating evasive PDF malware samples [27, 31] and adver-
sarial examples for text classiﬁers [e.g., 5, 15] represent two
similar scenarios, where the practical feasibility needs to be
ensured. These works typically operate in the problem space,
where search algorithms such as hill climbing or genetic pro-
gramming are guided by information from the feature space.
MCTS represents a novel concept in the portfolio of creating
adversarial examples under feasibility constraints, previously
examined by Wicker et al. [30] in the image context only.
Also related is the approach by Sharif et al. [23] for mis-
leading face recognition systems using painted eyeglasses.
The proposed attack operates in the feature space but en-
sures practical feasibility by reﬁning the optimization prob-
lem. In particular, the calculated adversarial perturbations
are required to match the form of eyeglasses, to be printable,
and to be invariant to slight head movements. In our attack
scenario, such reﬁnements of the optimization problem are
not sufﬁcient for obtaining valid source code, and thus we
resort to applying code transformations in the problem space.
9 Conclusion
Authorship attribution of source code can be a powerful tool
if an accurate and robust identiﬁcation of programmers is
possible. In this paper, however, we show that the current
state of the art is insufﬁcient for achieving a robust attribution.
We present a black-box attack that seeks adversarial examples
in the domain of source code by combining Monte-Carlo
tree search with concepts from source-to-source compilation.
Our empirical evaluation shows that automatic untargeted
and targeted attacks are technically feasible and successfully
mislead recent attribution methods.
Our ﬁndings indicate a need for alternative techniques for
constructing attribution methods. These techniques should
be designed with robustness in mind, such that it becomes
harder to transfer stylistic patterns from one source code to
another. A promising direction are generative approaches of
machine learning, such as generative adversarial networks,
that learn a decision function while actively searching for its
weak spots. Similarly, it would help to systematically seek
for stylistic patterns that are inherently hard to manipulate,
either due to their complexity or due to their tight coupling
with program semantics.
Public dataset and implementation. To encourage further
research on program authorship attribution and, in particular,
the development of robust methods, we make our dataset and
implementation publicly available.1 The attribution methods,
the code transformers as well as our attack algorithm are all
implemented as individual modules, such that they can be
easily combined and extended.
1www.tu-braunschweig.de/sec/research/code/imitator
USENIX Association
28th USENIX Security Symposium    491
Acknowledgment
The authors would like to thank Johannes Heidtmann for his
assistance during the project, and the anonymous reviewers
for their suggestions and comments. Furthermore, the authors
acknowledge funding by the Deutsche Forschungsgemein-
schaft (DFG, German Research Foundation) under Germany’s
Excellence Strategy - EXC 2092 CASA - 390781972 and the
research grant RI 2469/3-1.
References
[1] M. Abuhamad, T. AbuHmed, A. Mohaisen, and
D. Nyang. Large-scale and language-oblivious code au-
thorship identiﬁcation. In Proc. of ACM Conference on
Computer and Communications Security (CCS), 2018.
[2] A. V. Aho, R. Sethi, and J. D. Ullman. Compilers Prin-
ciples, Techniques, and Tools (2nd Edition). Addison-
Wesley, 2006.
[3] S. Alrabaee, P. Shirani, L. Wang, M. Debbabi, and
A. Hanna. On leveraging coding habits for effective
binary authorship attribution. In Proc. of European Sym-
posium on Research in Computer Security (ESORICS),
2018.
[4] B. Alsulami, E. Dauber, R. E. Harang, S. Mancoridis,
and R. Greenstadt. Source code authorship attribution
using long short-term memory based networks. In Proc.
of European Symposium on Research in Computer Se-
curity (ESORICS), 2017.
[5] M. Alzantot, Y. Sharma, A. Elgohary, B.-J. Ho, M. Sri-
vastava, and K.-W. Chang. Generating natural language
adversarial examples. In Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP), 2018.
[6] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. ˇSrndi´c,
P. Laskov, G. Giacinto, and F. Roli. Evasion attacks
against machine learning at test time. In Machine Learn-
ing and Knowledge Discovery in Databases. Springer,
2013.
[7] C. B. Browne, E. Powley, D. Whitehouse, S. M. Lu-
cas, P. I. Cowling, P. Rohlfshagen, S. Tavener, D. Perez,
S. Samothrakis, and S. Colton. A survey of monte carlo
tree search methods. IEEE Transactions on Computa-
tional Intelligence and AI in Games, 4(1), 2012.
[9] A. Caliskan, R. Harang, A. Liu, A. Narayanan,
C. R. Voss, F. Yamaguchi, and R. Greenstadt. De-
anonymizing programmers via code stylometry. In Proc.
of USENIX Security Symposium, 2015.
[10] A. Caliskan, F. Yamaguchi, E. Tauber, R. Harang,
K. Rieck, R. Greenstadt, and A. Narayanan. When
coding style survives compilation: De-anonymizing pro-
grammers from executable binaries. In Proc. of Network
and Distributed System Security Symposium (NDSS),
2018.
[11] N. Carlini and D. A. Wagner. Towards evaluating the
robustness of neural networks. In Proc. of IEEE Sympo-
sium on Security and Privacy (S&P), 2017.
[12] E. Dauber, A. Caliskan, R. E. Harang, and R. Green-
stadt. Git blame who?: Stylistic authorship attribution
of small, incomplete source code fragments. Techni-
cal Report abs/1701.05681, arXiv, Computing Research
Repository, 2017.
[13] G. Frantzeskou, E. Stamatatos, S. Gritzalis, and S. Kat-
sikas. Effective identiﬁcation of source code authors
using byte-level information. In Proc. of International
Conference on Software Engineering (ICSE), 2006.
[14] I. Krsul and E. H. Spafford. Authorship analysis: identi-
fying the author of a program. Computers & Security,
16(3), 1997.
[15] J. Li, S. Ji, T. Du, B. Li, and T. Wang. Textbugger: Gen-
erating adversarial text against real-world applications.
In Proc. of Network and Distributed System Security
Symposium (NDSS), 2019.
[16] S. MacDonell, A. Gray, G. MacLennan, and P. Sallis.
Software forensics for discriminating between program
authors using case-based reasoning, feed-forward neural
networks and multiple discriminant analysis. In Proc.
of International Conference on Neural Information Pro-
cessing (ICONIP), 1999.
[17] X. Meng, B. P. Miller, and K.-S. Jun. Identifying mul-
In Proc. of Euro-
tiple authors in a binary program.
pean Symposium on Research in Computer Security
(ESORICS), 2017.
[18] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B.
Celik, and A. Swami. The limitations of deep learn-
ing in adversarial settings. In Proc. of IEEE European
Symposium on Security and Privacy (EuroS&P), 2016.
[8] S. Burrows, A. L. Uitdenbogerd, and A. Turpin. Ap-
plication of information retrieval techniques for source
In Proc. of Conference
code authorship attribution.
on Database Systems for Advanced Applications (DAS-
FAA), 2009.
[19] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha,
Z. Berkay Celik, and A. Swami. Practical black-box
attacks against machine learning. In Proc. of ACM Asia
Conference on Computer Computer and Communica-
tions Security (ASIA CCS), 2017.
492    28th USENIX Security Symposium
USENIX Association
[20] N. Papernot, P. D. McDaniel, A. Sinha, and M. P. Well-
man. Sok: Security and privacy in machine learning.
In Proc. of IEEE European Symposium on Security and
Privacy (EuroS&P), 2018.
[21] B. N. Pellin. Using classiﬁcation techniques to deter-
mine source code authorship. Technical report, Depart-
ment of Computer Science, University of Wisconsin,
2000.
[22] N. E. Rosenblum, X. Zhu, and B. P. Miller. Who wrote
this code? identifying the authors of program binaries.
In Proc. of European Symposium on Research in Com-
puter Security (ESORICS), 2011.
[23] M. Sharif, S. Bhagavatula, L. Bauer, and M. K. Re-
iter. Accessorize to a Crime: real and stealthy attacks
on state-of-the-art face recognition. In Proc. of ACM
Conference on Computer and Communications Security
(CCS), 2016.
[24] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre,
G. van den Driessche, J. Schrittwieser, I. Antonoglou,
V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe,
J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap,
M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis.
Mastering the game of Go with deep neural networks
and tree search. Nature, 529, 2016.
[25] L. Simko, L. Zettlemoyer, and T. Kohno. Recognizing
and imitating programmer style: Adversaries in pro-
gram authorship attribution. Proceedings on Privacy
Enhancing Technologies, 1, 2018.
[26] F. Tram`er, F. Zhang, A. Juels, M. K. Reiter, and T. Ris-
tenpart. Stealing machine learning models via prediction
apis. In Proc. of USENIX Security Symposium, 2016.
[27] N. ˇSrndi´c and P. Laskov. Practical evasion of a learning-
based classiﬁer: A case study. In Proc. of IEEE Sympo-
sium on Security and Privacy (S&P), 2014.
[28] Website. Clang: C language family frontend for LLVM.
LLVM Project, https://clang.llvm.org, 2019. last
visited May 2019.
[29] Website.
Google
Code
https://code.google.com/codejam/,
last visited May 2019.
Jam.
2019.
[30] M. Wicker, X. Huang, and M. Kwiatkowska. Feature-
guided black-box safety testing of deep neural networks.
In Tools and Algorithms for the Construction and Anal-
ysis of Systems (TACAS), 2018.
[31] W. Xu, Y. Qi, and D. Evans. Automatically evading
classiﬁers: A case study on pdf malware classiﬁers.
In Proc. of Network and Distributed System Security
Symposium (NDSS), 2016.
A Monte-Carlo Tree Search
In this section, we provide further details about our variant of
Monte-Carlo tree search. Algorithm 1 gives an overview of
the attack. The procedure ATTACK starts with the root node r0
that represents the original source code x. The algorithm then
works in two nested loops:
• The outer loop in lines 3–5 repetitively builds a search
tree for the current state of source code r, and takes a