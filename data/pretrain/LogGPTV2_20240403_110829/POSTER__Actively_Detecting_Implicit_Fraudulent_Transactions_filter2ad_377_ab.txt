### Partial Derivative of the Loss Function
The update rule for the weights in the model is given by:
\[
\vec{w}_{t+1} = \vec{w}_t - \alpha \cdot \frac{\partial L_1}{\partial w}
\]
where \(\alpha\) is a hyperparameter representing the learning rate. After sufficient iterations, the final \(d\)-dimensional feature vector for node \(w\) is obtained. This process is independent of label imbalance since it relies solely on transaction records.

### Training Classifiers from Transaction Labels
Let \(D = \{(x_i, y_i)\}\) be the labeled dataset, where \(x_i\) is the feature vector of the \(i\)-th instance and \(y_i\) is the corresponding label. The features of an instance are composed of the learned features of the involved transferor and transferee, as well as the basic information from the transaction. The label \(y_i = 1\) indicates a fraudulent case, and \(y_i = 0\) otherwise. The predictive fraud score for the \(i\)-th instance is denoted by \(\hat{y}_i\), and \(l(y_i, \hat{y}_i)\) is a differentiable convex function that measures the discrepancy between the true label and the predicted score. Additionally, a regularization term \(\Omega(f)\) is included. Inspired by [8], the loss function \(L_2\) is defined as:
\[
L_2 = \sum_{i=1}^{|D|} \left( g_i f(x_i) + \frac{1}{2} h_i f^2(x_i) \right) + \Omega(f)
\]
where \(g_i\) and \(h_i\) are the first and second order partial derivatives of \(l(y_i, \hat{y}_i)\) with respect to \(\hat{y}_i\):
\[
g_i = \frac{\partial l(y_i, \hat{y}_i)}{\partial \hat{y}_i}, \quad h_i = \frac{\partial^2 l(y_i, \hat{y}_i)}{\partial \hat{y}_i^2}
\]
Although linear classifiers like logistic regression are widely used in supervised learning, we opt for gradient boosting-based models due to their high accuracy in this context. Once the training phase is complete, the predictor module can perform active detection.

### Experiments
In this section, we demonstrate the effectiveness of our proposed model compared to a rule-based approach in real electronic transactions.

#### Benchmark, Baseline, and Evaluation Metrics
We collected transaction records from December 1, 2016, to February 20, 2017, from Alipay, resulting in approximately 57 million records. We randomly selected 2 million records from February 24, 2017, to April 9, 2017, for training the classifier, and 0.8 million records from April 10, 2017, to April 20, 2017, for testing. To evaluate the performance of our model, we compared it with a rule-based baseline. The rules were derived from several guidelines, such as checking if the IP address or telephone number is from the same city, or if the transferee has been previously complained about.

For a fair comparison, we used multiple evaluation metrics:
- **Receiver Operating Characteristic (ROC) Curve**: Reflects the diagnostic capacity of a binary classifier by plotting the true positive rate against the false positive rate.
- **Area Under the Curve (AUC)**: The area under the ROC curve.
- **Precision-Recall (PR) Curve**: Plots precision against recall at different discrimination thresholds.
- **F1 Score**: The harmonic mean of precision and recall.
- **Kolmogorov-Smirnov (KS) Test**: A nonparametric test of the probability distribution between predictive results and the golden standard.

#### Empirical Results
As shown in Table 1, our proposed method consistently outperforms the baseline across various testing metrics. In practice, we focus on the recall at \(k\) predictive samples. For example, a "REC@100" value of 73.04% means that the recall is 73.04% when we alert only once in every 100 transaction records. Higher values of "REC@k" indicate better accuracy. Both AUC values are close to 100%, reflecting the extremely low frequency of fraudulent transactions in the dataset.

Figure 3 shows the PR and ROC curves of our model compared to the baseline. In the PR curve, our model's performance consistently outperforms the baseline. In the ROC curve, the true positive rate of our model is 93.56% when the false positive rate is 5%, whereas the baseline's true positive rate is only 90.58% at the same false positive rate.

| Methods | F1 | KS | AUC | REC@100 | REC@500 | REC@1000 |
|---------|----|----|-----|---------|---------|----------|
| Baseline | 86.18% | 41.93% | 61.09% | 73.04% | 48.26% | 51.77% |
| Our Model | 98.23% | 57.48% | 98.79% | 78.00% | 65.22% | 88.75% |

### Conclusion
We propose a novel method for actively detecting implicit fraudulent transactions. The empirical results show that our model significantly outperforms the rule-based baseline. In future work, we will explore additional solutions to further reduce the number of fraud cases.

### Acknowledgements
The authors thank the anonymous reviewers for their valuable suggestions.

### References
[1] Emin Aleskerov, Bernd Freisleben, and Bharat Rao. 1997. Cardwatch: A neural network based database mining system for credit card fraud detection. In CIFEr. IEEE, 220–226.

[2] Gerald Donald Baulier, Michael H Cahill, Virginia Kay Ferrara, and Diane Lambert. 2000. Automated fraud management in transaction-based networks. (Dec. 19 2000). US Patent 6,163,604.

[3] Richard J Bolton and David J Hand. 2002. Statistical fraud detection: A review. Statistical science (2002), 235–249.

[4] Richard J Bolton, David J Hand, et al. 2001. Unsupervised profiling methods for fraud detection. Credit Scoring and Credit Control VII (2001), 235–255.

[5] R Brause, T Langsdorf, and Michael Hepp. 1999. Neural data mining for credit card fraud detection. In ICTAI. IEEE, 103–106.

[6] Shaosheng Cao, Wei Lu, and Qiongkai Xu. 2015. Grarep: Learning graph representations with global structural information. In CIKM. ACM, 891–900.

[7] Pedro Casas, Alessandro D’Alconzo, Giuseppe Settanni, Pierdomenico Fiadino, and Florian Skopik. 2016. POSTER:(Semi)-Supervised Machine Learning Approaches for Network Security in High-Dimensional Network Data. In CCS. ACM, 1805–1807.

[8] Tianqi Chen and Carlos Guestrin. 2016. Xgboost: A scalable tree boosting system. In SIGKDD. ACM, 785–794.

[9] William W Cohen. 1995. Fast effective rule induction. In ICML. 115–123.

[10] Sushmito Ghosh and Douglas L Reilly. 1994. Credit card fraud detection with a neural-network. In System Sciences, Vol. 3. IEEE, 621–630.

[11] David J Hand. 1981. Discrimination and classification. Wiley Series in Probability and Mathematical Statistics, Chichester: Wiley, 1981 (1981).

[12] Sanjeev Jha, Montserrat Guillen, and J Christopher Westland. 2012. Employing transaction aggregation strategy to detect credit card fraud. Expert systems with applications 39, 16 (2012), 12650–12657.

[13] Mark J Nigrini. 1999. I’ve got your number. Journal of accountancy 187, 5 (1999), 79.

[14] Raghavendra Patidar, Lokesh Sharma, et al. 2011. Credit card fraud detection using neural network. IJSCE 1, 32-38 (2011).

[15] J Ross Quinlan. 1990. Learning logical definitions from relations. Machine learning 5, 3 (1990), 239–266.

[16] Donald Tetro, Edward Lipton, and Andrew Sackheim. 2000. System and method for enhanced fraud detection in automated electronic credit card processing. (Aug. 1 2000). US Patent 6,095,413.

[17] Massoud Vadoodparast, Abdul Razak Hamdan, et al. 2015. Fraudulent Electronic Transaction Detection Using Dynamic KDA Model. IJCSIS 13, 3 (2015), 90.

[18] Christopher Whitrow, David J Hand, Piotr Juszczak, D Weston, and Niall M Adams. 2009. Transaction aggregation as a strategy for credit card fraud detection. Data Mining and Knowledge Discovery 18, 1 (2009), 30–55.