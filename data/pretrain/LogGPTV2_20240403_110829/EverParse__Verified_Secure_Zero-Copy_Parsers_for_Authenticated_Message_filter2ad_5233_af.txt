overloads the semantics of a length in the block format to
conditionally add a new ﬁeld to the block structure, because it
requires a very ad-hoc combinator. Bitcoin requires two Low-
Parse extensions: one for the encoding of "compact integers"
(bitcoin_varint), and one for lists preﬁxed by their size in
elements rather than in bytes.
For compact integers, the representation may either use
1, 3, 5, or 9 bytes depending on whether the value of the
ﬁrst byte is respectively less than 252, 253, 254, or 255. It
is not clear from the Bitcoin documentation and wiki that
the format of compact integer is not malleable (e.g. 4636
USENIX Association
28th USENIX Security Symposium    1477
opaque sha256[32];
struct {
sha256 prev_hash; uint32_le prev_idx;
opaque scriptSig;
uint32_le seq_no;
} txin;
struct {
uint64_le value;
opaque scriptPubKey;
} txout;
struct {
uint32_le version;
txin inputs{0..1000 : bitcoin_varint};
txout outputs{0..11110 : bitcoin_varint};
uint32_le lock_time;
} transaction;
struct {
uint32_le version;
sha256 prev_block; sha256 merkle_root;
uint32_le timestamp;
uint32_le bits; uint32_le nonce;
transaction tx{0..2^16 : bitcoin_varint};
} block;
Figure 9: QuackyDucky speciﬁcation of Bitcoin blocks
could be represented as fd121c, or fe0000121c). However,
we checked that the Core implementation enforces the short-
est representation in the ReadCompactSize function. Ad-
ditionally, we allow list types to specify in their range the
type of integer used to encode the preﬁx length or size (e.g.
txin inputs{0..2^14 : bitcoin_varint}). A drawback of
preﬁxing lists by their number of elements is that the theoreti-
cal maximum length of the formatted list can get extremely
large. For instance, the maximal size of a well-formed Bitcoin
block is over 2320 bytes (in practice, it is well-known that
non-segwit blocks are at most 1MB). To avoid overﬂowing
OCaml’s 63-bit arithmetic in the parser metadata length com-
putations in QuackyDucky, we must write more conservative
boundaries. Scripts are known to be at most 10,000 bytes.
Historically, all non-segwit blocks in the main chain contain
less than 216 transactions (although the maximum is higher).
It is more difﬁcult to bound the number of inputs and outputs
of a transaction. If we assume a transaction is standard (at
most 100,000 bytes) and all inputs are signed (their script is at
least 64 bytes), there are less than 1000 inputs. Since outputs
can be as short as 9 bytes, a transaction can have over 11000.
Our test data is blocks 100,000 to 110,000 of the Bitcoin
blockchain, totaling 21MB. To experimentally check those
assumptions, we parsed all of these blocks and conﬁrmed they
are accepted by our validator.
For benchmarking, we measure: ﬁrst, the performance of
our zero-copy block validator compared with the built-in de-
serialization function of the Bitcoin Core client (commit
Figure 10: Synthetic performance comparison for validating
10,001 Bitcoin blocks. Throughput in kiB/s, higher is better.
cbe7efe); second, the performance variations of our zero-
copy block validator across compilers and optimization levels;
third, the performance impact of ﬁne-grained code-generation
options passed to KReMLin (Figure 10). For each one of those
benchmarks, we report numbers in kiB/s, i.e. the throughput;
we only occasionally report cycles per byte since most of our
validators run at less than 1 cy/B.
The ﬁrst measurement compares the performance of our
code against a reference implementation, namely, Bitcoin
Core. Bitcoin uses a custom template for serializing C++ ob-
jects. This template is well-optimized and tries to rely on
casts and the in-memory representation of base types as much
as possible. However, it is not zero-copy: parsing relies on
the memory allocated for the C++ object, and serialization
requires a copy to the output buffer. The beneﬁt is that the data
can be accessed using standard data structure libraries such
as std::vector for lists. Bitcoin provides a built-in bench-
marking tool for many of its features, including block deseri-
alization and validation in src/bench/checkblock.cpp.
We modify this benchmark to use our test data of 10,001
blocks and deserialize all of them in each run. The benchmark
deserializes 130 times, and reports the median over 5 runs.
We keep the default compiler options (gcc-8, O2 optimization
level). The measured throughput is 152,786 kiB/s, which
translates to 15 cy/B on the test machine used for the Bitcoin
measurements. We then validate the same 21MB of data using
our validators, with the same compiler and optimization levels.
We obtain a throughput of 4,568,632 kiB/s, which is less than
a cycle per byte, for the default KReMLin conﬁguration.
While the validation performance of our code is excellent,
we do not claim that this benchmark is representative of real
application usage, as it doesn’t account for the overhead of
accessor functions to read the block and transaction contents.
Nevertheless, this shows that our veriﬁed low-level implemen-
tation is competitive with hand-optimized formatters.
1478    28th USENIX Security Symposium
USENIX Association
Second, we measure performance variations across com-
piler versions. The performance is comparable between the
two most recent versions of Clang and GCC, for optimiza-
tion levels O2 and O3. Unsurprisingly, the default setting
without optimization yields much slower code, but even then,
we remain considerably faster than the original Bitcoin code.
We also measure the performance of our code compiled with
CompCert [28]. We ﬁnd that CompCert is consistently 42%
slower than GCC and Clang with optimizations, but still more
than twice as fast as GCC or Clang without optimizations. We
conclude that our code lends itself well to optimizations by
modern compilers, and that users do not need to enable the
(risky) O3 performance level to get maximum performance
out of Clang or GCC.
Third, we
experiment with various
compilation
schemes of the KReMLin compiler for the core type
LowParse.slice.slice, a two-ﬁeld C struct (representing a
byte buffer through its base pointer and length) which in the
default conﬁguration is passed by value (§4.4). Two alternate
compilation schemes are considered. First, passing both the
base pointer and length as separate arguments to functions;
this is the “struct-explode” category, and yields no perfor-
mance improvement. Second, we pass those structures by
address, relying on an unveriﬁed transformation in the KReM-
Lin compiler, similar to CompCert’s -fstruct-passing
feature. This yields modest performance improvements for
GCC 7 and GCC 8 at the higher optimization levels (3% to
9%). We conclude that our generated C code is satisfactory
and that we don’t need to either rewrite our code to pass
slices by address (a substantial proof burden) or instruct
KReMLin to perform this transformation (which would
increase the trusted computing base).
Finally, we perform fuzz testing on the X64 machine code
of our generated bitcoin-block validator as compiled with
gcc-8 -O2. (Although our veriﬁcation results ensure memory
safety for all inputs, fuzzing may still, in principle, detect bugs
in our toolchain and the C compilers we use.) We use SAGE
[19], a fuzzer specialized to parsers, which generates random
input, valid or not, and feeds them to the validator which
SAGE automatically instruments to check for buffer over-
ﬂows. As expected, SAGE reported no bugs after 21,664,448
inputs tested at an average rate of 599 inputs per minute.
6.3 ASN.1 Payload of PKCS #1 Signatures
Our last example is the payload of PKCS #1 signatures intro-
duced in § 2.2. We extend LowParse with a combinator for
the encoding of ASN.1 DER lengths. This encoding is partic-
ularly convoluted: if a length is less than 127, it is represented
over a single byte. Otherwise, the 7 least signiﬁcant bits of the
ﬁrst byte encode the length in byte of the shortest big endian
representation of the length. This means the length can be at
most 221016 −1. To avoid overﬂows, we only support values of
the ﬁrst byte less than 132 (i.e. 32-bit lengths). An issue with
the speciﬁcation is the lack of dependency between the object
identiﬁer of the hash algorithm and the octet string of the
actual digest: the application is required to check the digest is
of the correct length if it tries to parse the signature contents.
We capture this dependency by only making the outermost
sequence variable length, and by parsing the object identiﬁer
as a constant tag of an union of ﬁxed-length arrays. (Note
that this is for illustration only, the recommended approach
is to serialize the computed hash, and use a constant time
comparison with the un-padded signature contents instead).
We integrate our code into pkcs1_v15_verify function
of mbedTLS, and modify the built-in benchmarking tool
to measure the PKCS #1 signature veriﬁcation time in-
stead of the raw public key and private key operation time
measured by default. In addition, we also export the inter-
nal function to format the ASN.1 payload of the signature
(pkcs1_v15_encode), and compare it with our extracted for-
matter functions. The following table compares the amount
of operations per second and cycles per operation for com-
plete signature veriﬁcation, and for the encoding of the ASN.1
payload:
Operation
Verify
Encode
mbedTLS
EverParse
79K op/s
31M op/s
5,700 cy/op
14 cy/op
79K op/s
134M op/s
5,649 cy/op
3 cy/op
As expected, the veriﬁcation time is dominated by the cost
of the RSA exponentiation: even though our validator is over
4 times faster and avoids the allocation of a modulus-sized
intermediate buffer to compare the expected and computed
digests, the impact on overall validation performance is negli-
gible. For signing and encoding, the constant constant parts of
the signature payload must be written manually, and separate
ﬁnalizers must be called for to write the bytes we depend on
for the algorithm choice and the outermost ASN.1 length.
We tested our implementation against all variants of the
Bleichenbacher’s attack listed in §2.2 and conﬁrmed they are
properly rejected.
7 Related work
Parsing combinators are widely used in functional program-
ming languages, and there exist several libraries for network
protocols [29], including TLS and X.509 [30].
For well-behaved language classes (e.g. regular, context-
free), there is a long history on veriﬁcation of parser correct-
ness with respect to simple speciﬁcations (regular expressions,
grammars). Jourdan et al. [25] propose a certifying compiler
for LR(1) grammars, which translates the grammar into a
pushdown automaton and a certiﬁcate of language equiva-
lence between the grammar and the automaton. The certiﬁ-
cate is checked by a validator veriﬁed in Coq [1], while the
automaton is interpreted by a veriﬁed interpreter. Barthwal
et al. [3] propose a veriﬁed grammar compiler and automa-
ton interpreter for the simpler class of SLR languages, ver-
iﬁed in HOL [42]. For regular languages, Koprowski et al.
USENIX Association
28th USENIX Security Symposium    1479
introduced TRX [26], an interpreter for regular expressions
veriﬁed in Coq. All of these works require runtime interpre-
tation, which greatly degrades the performance compared to
compilation. Furthermore, they target garbage-collected func-
tional language runtimes like OCaml, which cannot easily be
integrated into high-performance, native C applications.
For TLV languages, there have been some attempts [2] to
create context-free or even regular speciﬁcations for X.509.
However, due to the context-sensitive nature of ASN.1, these
efforts rely on discretizations of some ﬁelds (such as variable-
length integers) and drastic simpliﬁcations of the format (such
as limiting the choice of extensions to a known subset). The
combinatorial explosion required to achieve interoperability
makes these approaches impractical for real implementations,
although some authors claim otherwise [21].
For runtime safety, fuzzing techniques [19, 43] are widely
deployed and often included into test suites for cryptographic
libraries. Although best practice, fuzzing is by nature incom-
plete, and may be difﬁcult to apply to authenticated mes-
sages (as fuzzing invalidates hashes, signatures and MACs).
Dynamic analysis tools like Valgrind [35] or AddressSani-
tizer [44] are widely used but also incomplete, while static
analysis tools like Frama-C [11] require higher expertise, a
signiﬁcant time investment, and tend to scale poorly with
large codebases. Because of past attacks, speciﬁc tools have
been created for TLS and cryptographic libraries, including
TLS-Attacker [45], FlexTLS [5], and Wycheproof [8], but
their focus is to uncover known vulnerability patterns in pro-
tocol implementations rather than prove formal guarantees on
their message formats.
Another related line of work [10, 16] applies abstract in-
terpretation and symbolic execution to study the properties
of parsers, such as whether two implementations of a format
accept the same message. These techniques can be applied to
existing implementations, but cannot generate new ones.
Narcissus [47] also constructs correct binary parsers from a
veriﬁed library of combinators written in Coq. There are two
major differences with EverParse: ﬁrst, Narcissus only proves
the correctness of its parsers, while we also prove parser secu-
rity; second, Narcissus only generates higher-order, functional
implementations while our compiled approach means that our
parsers are entirely specialized at F(cid:63) extraction, and can be
compiled in zero-copy mode. Building on Narcissus, Ye and
Delaware [51] build a veriﬁed compiler in Coq for parsers
and formatters described using Protocol Buffers [20]. Like
EverParse, their parsers and formatters are proven to be cor-
rect. Their library produces high-level functional code, which
is memory-safe by construction—in contrast, EverParse pro-
duces low-level C code, together with memory safety proofs.
Further, due to the inherent structure of the Protocol Buffers
format, their work does not consider non-malleability.
Jim and Mandelbaum [23, 24] have formalized and devel-
oped parser generators for a wide class of context-free gram-
mars extended with data dependency, including tag-length-
value encodings, tagged unions, and other forms of depen-