This is because LogGC ignores the events in the start and the
end phases of a process. However, the results also show that
the overhead is minor for these applications.
B. Logging Overhead and Scalability
We also perform experiments to study the run-time over-
head and scalability of ProTracer. Fig. 8 shows the accumu-
lated log size over time for user 1. The solid line shows the
growth of the BEEP log size over time, and the dashed line
shows ProTracer’s. In general, the growth is similar although
the scales of the sizes are different. The sharpest growth occurs
in the 15th-20th hour, indicating the user was intensively using
the system. Even in this period, the growth of the ProTracer
log is about 13MB, suggesting very good scalability. There
are some shape differences between the two lines near the
20th hour. This is mainly because ProTracer has better log
reduction for the applications used during that time period,
compared to other applications.
35000
30000
10000
25000
20000
15000
e
n
i
l
h
s
a
D
-
)
B
K
(
r
e
c
a
r
T
o
r
P
e
n
i
l
d
i
l
o
S
-
)
B
M
(
t
i
d
u
a
x
u
n
L
i
1800
1600
1400
1200
1000
800
600
400
200
0
0
5
10
15
20
Time (h)
5000
0
25
Fig. 8: Accumulated log size from the one-day execution of
BEEP and ProTracer. Note the size of BEEP log is measured
by megabytes, whereas the ProTracer log is measured by
kilobytes.
Fig. 9 and Fig. 10 show the run-time overhead comparison
between ProTracer and the default Linux Audit system, with
the same set of syscalls monitored. Note BEEP is built on the
Linux Audit system and hence more expensive. We perform
11
two sets of experiments. The ﬁrst one is for server programs.
We use the Apache Benchmark [1] to test two web servers
Apache and MiniHttp, and ftpbench to test ProFTPD.
We also test different concurrency conﬁgurations, with the
number of requests sent at the same time being 1, 2, 4, and
8. The results are shown in Fig. 9. The benchmarks tend
to give the system a lot of pressure, which would cause
higher overhead than regular usage. The baseline we use is
the native Linux system without running the Linux Audit
system. Observe that the overhead of ProTracer is less than 7%,
whereas the Linux Audit system has a much more signiﬁcant
overhead (more than 5 times larger).
d
a
e
h
r
e
v
o
e
m
i
t
n
u
R
45%
40%
35%
30%
25%
20%
15%
10%
5%
0%
httpd
MiniHttp
ProFTPD
httpd-audit
MiniHttp-audit
ProFTPD-audit
1
2
4
Number of thread(s)
8
Fig. 9: Run-time overhead with different concurrent thread(s)
for server programs.
We also perform experiments for client programs. We use
standard benchmarks if they are available such as SunSpider
for Firefox. Otherwise, we use the batch mode for programs
like vim or W3M. We perform the experiments with ProTracer
and with the Linux Audit system. The baseline we use here is
the native Linux system without any logging system running.
The results are shown in Fig. 10. Observe that ProTracer
has less than 3.5% run-time overhead for all these programs,
whereas the overhead of the Linux Audit system is 7-8 times
larger.
)
%
(
d
a
e
h
r
e
v
o
e
m
i
t
n
u
R
25.0%
20.0%
15.0%
10.0%
5.0%
0.0%
ProTracer
audit
vim
firefox
wget
w3m
yafc
to understand the sources and the ramiﬁcations of the attacks,
and also the how-provenance query to understand the attack
path (Section I). We compare the query results by BEEP and
ProTracer, and also cross-check with our prior knowledge. To
emulate real-world attack scenarios, each experiment lasted for
a few hours with the attack performed in the middle.
The ﬁrst case is a backdoor attack. The attacker detected
that the running FTP server was ProFTPD-1.3.3c, which had
a backdoor command [3]. He compromised the server, and
was able to get a bash shell. He then downloaded a backdoor
program using wget, and started this backdoor to get permanent
access. A few days later, the administrator got a warning that
the FTP server had a backdoor, and decided to check if the
backdoor had been exploited. If so, what damages have been
inﬂicted.
The second scenario is information theft [27]. An employee
had a under-the-table deal with one competitor of his own
company: he copied some information from the company, and
leaked it to the competitor by pasting it to a public page
via vim. When the company found that the information was
leaked, they should be able to pair the ﬁle that contained the
information with the web page that leaked the information,
among thousands of ﬁles. ProTracer shall also allow them to
prove that the attacker leaked it among all the other employees
that have the access to the ﬁle.
The third scenario is illegal storage [16]. One of the server
administrators wanted to store some illegal ﬁles on a server.
However, he did not want the ﬁles to be in his own directory.
Instead he created a directory under another user’s home
directory, and downloaded the illegal ﬁles to the directory. He
replaced the ls program to hide the existence of this directory.
When the ﬁles were eventually found, the victim user was
considered a suspect because of the presence of those ﬁles in
his/her directory. The investigator should be able to identify
that the administrator was the one that committed the crime.
Note here we assume that the administrator cannot tamper with
the log ﬁle generated by ProTracer.
The forth scenario is cheating student [4]. An instructor’s
password was stolen by a student. The student downloaded a
ﬁle containing midterm scores from Apache, and uploaded a
modiﬁed version. The instructor noticed that the average score
became higher, and started to suspect someone had modiﬁed
the ﬁle. Luckily, students used static IP addresses on campus
and off-campus IPs were forbidden to connect to the server.
So ﬁnding the IP from which the current ﬁle was uploaded
would help identify the student. Moreover, the administrator
should be able to ﬁnd other suspicious activities of the student.
In our case, the student also downloaded a few ﬁles containing
answers to future quizzes.
Fig. 10: Run-time overhead for client programs.
The ﬁfth is phishing email [2] that was discussed in
Section II.
C. Attack Investigation Cases
In this section, we use a number of attack cases to show that
the causal graphs generated by ProTracer during attack analysis
are smaller than those by BEEP, but equally informative,
and the time taken to generate the graphs is much less. We
reproduce a few realistic attack scenarios for our experiment.
With each scenario, we perform two what-provenance queries
Parts of the results are shown in Table III. The second
column shows the experiment duration. The next three columns
show the size of the logs by different systems. Then we show
the time it takes to perform the queries. The last two columns
show if BEEP and ProTracer produce matched results for the
two what-provenance queries (i.e., the backward query for
attack sources and the forward query for attack ramiﬁcations).
For the ﬁrst scenario, the backward query is not applicable.
12
Scenario
Duration
Log ﬁle size(KB)
Run time(s)
Investigation
BEEP
LogGC
ProTracer
BEEP
ProTracer
Backward
Forward
Backdoor attack
Information theft
Illegal storage
Cheating student
Phishing email
3h54min
4h22min
2h58min
1h17min
4h36min
832,753
587,494
369,585
179,748
975,753
174,693
94,759
63,375
29,485
183,795
79,834
13,938
10,864
9,385
82,343
74
39
32
17
64
11
5
5
3
8
-
Match
Match
Match
Match
Match
Match
Match
Match
Match
TABLE III: Attack scenarios. Backward means backward what-provenance query; and forward means forward query; match means ProTracer
is able to precisely and concisely uncover the attack path.
Observe that ProTracer produces much smaller logs and the
query processing time is much shorter. The query results
show no qualitative differences and precisely disclose the
provenance.
memory dependencies, this may not hold for all programs. For
programs that do not have unit structure (i.e., does not have
event handling loops), ProTracer treats the entire execution as
a unit, which may cause dependence explosion.
Scenario
#source
#process
#ﬁle
#nodes
Backdoor
Infor theft
Illegal storage
Student hacker
Phishing email
33/33
1/1
24/24
2/2
5/5
23/23
4/4
6/6
2/2
8/8
37/66
21/36
56/72
67/85
12/12