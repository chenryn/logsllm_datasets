an application is playing, the visibility of the application hints
at its role. For instance, when the user is using Google Maps to
navigate, it is playing a different role from when Google Maps
is running in the background without the user’s knowledge.
We believe that this is the reason why the visibility of the
requesting application is signiﬁcant: it helps the user to infer
the role played by the application requesting the permission.
The user expects applications in certain roles to access
resources depending on the context in which the request is
made. We believe that the foreground application sets this
context. Thus a combination of the role and the context
decides whether an information ﬂow is expected to occur or
not. Automatically inferring the exact context of a request is
likely an intractable problem. For our purposes, however, it is
possible that we need to only infer when context has changed,
or rather, when data is being requested in a context that is no
longer acceptable to the user. Based on our data, we believe
that features based on foreground application and visibility are
most useful for this purpose, from our collected dataset.
We now combine all of this into a concrete example within
the contextual integrity framework: If a user is using Google
Maps to reach a destination, the application can play the
role of a navigator in a geolocation context, whereby the
user feels comfortable sharing her location. In contrast, if the
same application requests location while running as a service
invisible to the user, the user may not want to provide the same
information. Background applications play the role of “passive
listeners” in most contexts; this role as perceived by the user
may be why background applications are likelier to violate
privacy expectations and consequently be denied by users.
AOFU primarily focuses on controlling access through
rules for application:permission combinations. Thus, AOFU
neglects the role played by the application (visibility) and
relies purely on the agent (the application) and the information
subject (permission type). This explains why AOFU is wrong
in nearly one-ﬁfth of cases. Based on Table III, both AOFU-
VA (possibly identifying the role played by the application)
and AOFU-AF PV (possibly identifying the current context
because of the current foreground application-AF ) have higher
accuracy than the other AOFU combinations. However, as the
contextual integrity framework suggests, the permission model
has to take both the role and the current context into account
before making an accurate decision. AOFU (and other models
that neglect context) only makes it possible to consider a single
aspect, a limitation that does not apply to our model.
While the data presented in this work suggest the impor-
tance of capturing context to better protect user privacy, more
work is needed along these lines to fully understand how peo-
ple use context to make decisions in the Android permission
model. Nevertheless, we believe we contribute a signiﬁcant
initial step towards applying contextual integrity to improve
smartphone privacy by dynamically regulating permissions.
IX. DISCUSSION
The primary goal of this research was to improve the
accuracy of the Android permission system so that it more
correctly aligns with user privacy preferences. We began with
four hypotheses: (i) that the currently deployed AOFU policy
frequently violates user privacy; (ii) that the contextual infor-
mation it ignores is useful; (iii) that a ML-based classiﬁer can
account for this contextual information and thus improve on
the status quo; and (iv) that passively observable behavioral
traits can be used to infer privacy preferences.
To test these hypotheses, we performed the ﬁrst large-scale
study on the effectiveness of AOFU permission systems in
the wild, which showed that hypotheses (i) and (ii) hold.
We further built an ML classiﬁer that took user permission
decisions along with observations of user behaviors and the
context surrounding those decisions to show that (iii) and (iv)
hold. Our results show that existing systems have signiﬁcant
room for improvement, and other permission-granting systems
may beneﬁt from applying our results.
A. Limitations of Permission Models
Our ﬁeld study conﬁrms that users care about their privacy
and are wary of permission requests that violate their expec-
tations. We observed that 95% of participants chose to block
at least one permission request; in fact, the average denial
1089
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:19:17 UTC from IEEE Xplore.  Restrictions apply. 
rate was 60%—a staggering amount given that the AOI model
permits all permission requests for an installed application.
While AOFU improves over the AOI model, it still violates
user privacy around one in seven times, as users deviate from
their initial responses to permission requests. This amount is
signiﬁcant because of the high frequency of sensitive permis-
sion requests: a 15% error rate yields thousands of privacy
violations per user—based on the latest dataset, this amounts to
a potential privacy violation every minute. It further shows that
AOFU’s correctness assumption—that users make binary deci-
sions based only on the application:permission combination—
is incorrect. Users take a richer space of information into
account when making decisions about permission requests.
B. Our ML-Based Model
We show that ML techniques are effective at learning from
both the user’s previous decisions and the current environmen-
tal context in order to predict whether to grant permissions on
the user’s behalf. In fact, our techniques achieve better results
than the methods currently deployed on millions of phones
worldwide—while imposing signiﬁcantly less user burden.
Our work incorporates elements of the surrounding context
into a machine-learning model. This better approximates user
decisions by ﬁnding factors relevant for users that are not
encapsulated by the AOFU model. In fact, our ML model
reduces the errors made by the AOFU model by 75%. Our
ML model’s 97% accuracy is a substantial improvement over
AOFU’s 85% and AOI’s 25%; the latter two of which comprise
the status quo in the Android ecosystem.
Our research shows that many users make neither random
nor ﬁxed decisions: the environmental context plays a signif-
icant role in user decision-making. Automatically detecting
the precise context surrounding a request for sensitive data is
an incredibly difﬁcult problem (e.g., inferring how data will
be used), and is potentially intractable. However, to better
support user privacy, that problem does not need to be solved;
instead, we show that systems can be improved by using
environmental data to infer when context has changed. We
found that the most predictive factors in the environmental
context were whether the application requesting the permission
is visible, and what the foreground application the user is
engaged with. These are both strong contextual cues used by
users, insofar as they allowed us to better predict changes
in context. Our results show that ML techniques have great
potential in improving user privacy, by allowing us to infer
when context has changed, and therefore when users would
want data requests to be brought to their attention.
C. Reducing the User Burden
Our work is also novel in using passively observable data
to infer privacy decisions: we show that we can predict
a user’s preferences without any permission prompts. Our
model trained solely on behavioral traits yields a three-fold
improvement over AOI; for Defaulters—who account for 53%
of our sample—it was as accurate as AOFU-AP. These results
demonstrate that we can match the status quo without any
active user involvement (i.e., the need for obtrusive prompts).
These results imply that learning privacy preferences may be
done entirely passively, which, to our knowledge, has not
yet been attempted in this domain. Our behavioral feature
set provides a promising new direction to guide research in
creating permission models that minimize user burden.
The ML model
trained with contextual data and past
decisions also signiﬁcantly reduced the user burden while
achieving higher accuracy than AOFU. The model yielded
an 81% reduction in prediction errors while reducing user
involvement by 25%. The signiﬁcance of this observation is
that by reducing the risk of habituation, it increases reliability
when user input is needed.
D. User- and Permission-Tailored Models
Our ML-based model incorporates data from all users into
a single predictive model. It may be the case, however, that
a collection of models tailored to particular types of users
outperforms our general-purpose model—provided that the
correct model is used for the particular user and permission.
To determine if this is true, we clustered users into groups
based ﬁrst on their behavioral features, and then their denial
rate, to see if we could build superior cluster-tailored ML
models. Having data for only 131 users, however, resulted
in clusters too small to carry out an effective analysis. We
note that we also created a separate model for each sensitive
permission type, using data only for that permission. Our
experiments determined, however, that these models were no
better (and often worse) than our general model. It is possible
that such tailored models may be more useful when our system
is implemented at scale.
E. Attacking the ML Model
Attacking the ML model to get access to users’ data without
prompting is a legitimate concern [5]. There are multiple ways
an adversary can inﬂuence the proposed permission model: (i)
imposing an adversarial ML environment [31]; (ii) polluting
the training set to bias the model to accept permissions; and
(iii) manipulating input features in order to get access without
user notiﬁcation. We assume in this work that the platform is
not compromised; a compromised platform will degrade any
permission model’s ability to protect resources.
A thorough analysis on this topic is outside of our scope.
Despite that, we looked at the possibility of manipulating
features to get access to resources without user consent. None
of the behavioral features used in the model can be inﬂu-
enced, since that would require compromising the platform.
An adversary can control the runtime features for a given
permission request by speciﬁcally choosing when to request
the permission. We generated feature vectors manipulating
every adversary-controlled value and combination from our
dataset, and tested them on our model. We did not ﬁnd any
conclusive evidence that the adversary can exploit the ML
model by manipulating the input features to get access to
resources without user consent.
1090
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:19:17 UTC from IEEE Xplore.  Restrictions apply. 
As this is not a comprehensive analysis on attack vectors,
it is possible that a scenario exists where the adversary is able
to access sensitive resources without prompting the user ﬁrst.
Our preliminary analysis suggests that such attacks may be
non-trivial, but more work is needed to study and prevent such
attacks, particularly examining adversarial ML techniques and
feature brittleness.
F. Experimental Caveat
We repeat a caveat about our experimental data: users
were free to deny permissions without any consequences.
We explicitly informed participants in our study that their
decisions to deny permission requests would have no impact
on the actual behavior of their applications. This is important
to note because if an application is denied a permission, it
may exhibit undeﬁned behavior or lose important functionality.
In fact, researchers have noted that many applications crash
when permissions are denied [13]. If these consequences are
imposed on users, they may decide that the functionality is
more important than their privacy decision.
If we actually denied permissions, users’ decisions may
skew towards a decreased denial rate. The denial rates in
our experiments therefore represent the actual privacy prefer-
ences of users and their expectations of reasonable application
behavior—not the result of choosing between application func-
tionality and privacy. We believe that how people react when
choosing between functionality and privacy preferences is an
important research question beyond the scope of this paper.
Such a change, however, will not limit this contribution, since
our proposed model was effective in guarding resources of the
users who are selective in their decision making—the proposed
classiﬁer reduced the error rate of Contextuals by 44%.
We believe that there are important unanswered questions
about how to solve the technical hurdles surrounding enforcing
restrictive preferences with minimal usability issues. As a ﬁrst
step towards building a platform that does not force users to
choose between their privacy preferences and required func-
tionality, we must develop an environment where permissions
appear—to the application—to be allowed, but in reality only
spurious or artiﬁcial data is provided.
G. Types of Users
We presented a categorization of users based on the sig-
niﬁcance that the application’s visibility played towards their
individual privacy decisions. We believe that
in an actual
permission denial setting, the distribution will be different
from what was observed in our study. Our categorization’s
signiﬁcance, however, motivates a deeper analysis on under-
standing the factors that divide Contextuals and Defaulters.
While visibility was an important factor in this division, there
may be others that are signiﬁcant and relevant. More work
needs to be done to explore how Contextuals make decisions
and which behaviors correlate with their decisions.
H. User Interface Panel
Any model
that predicts user decisions has the risk of
making incorrect predictions. Making predictions on a user’s
1091
is necessary because permissions are re-
behalf, however,
quested by applications with too high a frequency for manual
examination. While we do not expect any system to be able to
obtain perfect accuracy, we do expect that our 97% accuracy
can be improved upon.
One plausible way of improving the accuracy of the per-
mission model is to empower the user to review and make
changes on how the ML model makes decisions through a
user feedback panel. This gives users recourse to correct
undesirable decisions. The UI panel could also be used to
reduce the usability issues and functionality loss stemming
from permission denial. The panel should help the user ﬁgure
out which rule incurred the functionality loss and to change it
accordingly. A user may also use this to adjust their settings
as their privacy preferences evolve over time.
I. The Cost of Greater Control
A more restrictive platform means users will have greater
control over the data being shared with third parties. Applica-
tions that generate revenue based on user data, however, could
be cut off from their primary revenue source. Such an effect
could disrupt the current eco-system and force app developers
to degrade app functionality based on the availability of the
data. We believe the current eco-system is unfairly biased
against users and tighter control will make the user an equal
stakeholder. While more work is needed to understand the ef-
fects of a more restrictive platform, we believe it is imperative
to let the user have greater control over their own data.
J. Conclusions
We have shown a number of important results. Users care
about their privacy: they deny a signiﬁcant number of requests
to access sensitive data. Existing permission models for An-
droid phones still result in signiﬁcant privacy violations. Users
may allow permissions some times, while denying them at
others, implying that there are more factors that go into the
decision-making process than simply the application name and
the permission type. We collected real-world data from 131
users and found that application visibility and the current fore-
ground application were important factors in user decisions.
We used the data we collected to build a machine-learning
model to make automatic permission decisions. One of our
models had a comparable error rate to AOFU and beneﬁted
from not requiring any user prompting. Another of our models
required some user prompts—less than is required by AOFU—
and achieved a reduction of AOFU’s error rate by 81%.
ACKNOWLEDGMENTS
This research was supported by the United States De-
partment of Homeland Security’s Science and Technology
Directorate under contract FA8750-16-C-0140, the Center for
Long-Term Cybersecurity (CLTC) at UC Berkeley, the Na-
tional Science Foundation under grant CNS-1318680, and Intel
through the ISTC for Secure Computing. The content of this
document does not necessarily reﬂect the position or the policy
of the U.S. Government and no ofﬁcial endorsement should
be inferred.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:19:17 UTC from IEEE Xplore.  Restrictions apply. 
REFERENCES
[1] Y. Agarwal and M. Hall, “Protectmyprivacy: Detecting and mitigating
privacy leaks on ios devices using crowdsourcing,” in Proceeding of the
11th Annual International Conference on Mobile Systems, Applications,