### Visibility and Role of Applications

The visibility of an application provides a clear indication of its current role. For example, when a user is actively using Google Maps for navigation, the application plays a different role compared to when it is running in the background without the user's knowledge. We believe that the visibility of the requesting application is significant because it helps the user infer the role of the application making the permission request. Users expect applications in certain roles to access resources based on the context in which the request is made. The foreground application sets this context, and a combination of the role and the context determines whether an information flow is expected or not.

Automatically inferring the exact context of a request is likely an intractable problem. However, for our purposes, it may be sufficient to determine when the context has changed, or when data is being requested in a context that is no longer acceptable to the user. Based on our collected dataset, we believe that features based on the foreground application and its visibility are most useful for this purpose.

### Concrete Example within the Contextual Integrity Framework

Consider a user using Google Maps to reach a destination. In this geolocation context, the application plays the role of a navigator, and the user feels comfortable sharing their location. Conversely, if the same application requests location while running as a service invisible to the user, the user may not want to provide the same information. Background applications often play the role of "passive listeners" in most contexts, and this perceived role may be why they are more likely to violate privacy expectations and be denied by users.

### Limitations of AOFU

AOFU (Application-Oriented Fine-Grained User) primarily focuses on controlling access through rules for application:permission combinations. It neglects the role played by the application (visibility) and relies purely on the agent (the application) and the information subject (permission type). This explains why AOFU is incorrect in nearly one-fifth of cases. Based on Table III, both AOFU-VA (which identifies the role played by the application) and AOFU-AF PV (which identifies the current context due to the current foreground application) have higher accuracy than other AOFU combinations. However, the contextual integrity framework suggests that the permission model must consider both the role and the current context to make accurate decisions. AOFU and other models that neglect context can only consider a single aspect, a limitation that does not apply to our model.

### Importance of Capturing Context

While the data presented in this work highlight the importance of capturing context to better protect user privacy, more research is needed to fully understand how people use context in the Android permission model. Nevertheless, we believe our work represents a significant initial step towards applying contextual integrity to improve smartphone privacy by dynamically regulating permissions.

### Discussion

#### Primary Goal and Hypotheses

The primary goal of this research was to improve the accuracy of the Android permission system to better align with user privacy preferences. We began with four hypotheses:
1. The currently deployed AOFU policy frequently violates user privacy.
2. The contextual information it ignores is useful.
3. A machine-learning (ML)-based classifier can account for this contextual information and improve on the status quo.
4. Passively observable behavioral traits can be used to infer privacy preferences.

To test these hypotheses, we conducted the first large-scale study on the effectiveness of AOFU permission systems, which confirmed hypotheses (i) and (ii). We then built an ML classifier that incorporated user permission decisions, observations of user behaviors, and the context surrounding those decisions, confirming hypotheses (iii) and (iv). Our results show that existing systems have significant room for improvement, and other permission-granting systems may benefit from applying our findings.

#### Limitations of Permission Models

Our field study confirms that users care about their privacy and are wary of permission requests that violate their expectations. We observed that 95% of participants chose to block at least one permission request, with an average denial rate of 60%. While AOFU improves over the AOI (All or Nothing) model, it still violates user privacy in about one in seven cases. This is significant given the high frequency of sensitive permission requests, leading to potential privacy violations every minute. AOFU's assumption that users make binary decisions based solely on the application:permission combination is incorrect; users consider a richer set of information when making decisions about permission requests.

#### Our ML-Based Model

We demonstrate that ML techniques effectively learn from both the user's previous decisions and the current environmental context to predict whether to grant permissions. Our techniques achieve better results than the methods currently deployed on millions of phones, while imposing significantly less user burden. Our model incorporates elements of the surrounding context, better approximating user decisions by finding factors relevant to users that are not encapsulated by the AOFU model. Our ML model reduces the errors made by the AOFU model by 75%, achieving 97% accuracy compared to AOFU's 85% and AOI's 25%.

Our research shows that many users do not make random or fixed decisions; the environmental context plays a significant role in decision-making. Automatically detecting the precise context surrounding a request for sensitive data is difficult, but to better support user privacy, this problem does not need to be solved. Instead, we show that systems can be improved by using environmental data to infer when the context has changed. The most predictive factors in the environmental context are whether the application requesting the permission is visible and what the foreground application the user is engaged with. These are strong contextual cues that allow us to better predict changes in context. Our results show that ML techniques have great potential in improving user privacy by allowing us to infer when context has changed and when users would want data requests to be brought to their attention.

#### Reducing the User Burden

Our work is novel in using passively observable data to infer privacy decisions, showing that we can predict a user’s preferences without any permission prompts. Our model trained solely on behavioral traits yields a three-fold improvement over AOI and, for Defaulters (who account for 53% of our sample), is as accurate as AOFU-AP. These results demonstrate that we can match the status quo without any active user involvement, implying that learning privacy preferences may be done entirely passively, which, to our knowledge, has not yet been attempted in this domain. Our behavioral feature set provides a promising new direction to guide research in creating permission models that minimize user burden.

The ML model trained with contextual data and past decisions also significantly reduced the user burden while achieving higher accuracy than AOFU. The model yielded an 81% reduction in prediction errors while reducing user involvement by 25%. By reducing the risk of habituation, it increases reliability when user input is needed.

#### User- and Permission-Tailored Models

Our ML-based model incorporates data from all users into a single predictive model. However, a collection of models tailored to particular types of users may outperform our general-purpose model, provided the correct model is used for the specific user and permission. To determine if this is true, we clustered users based on their behavioral features and denial rates. However, with data for only 131 users, the clusters were too small for effective analysis. We also created a separate model for each sensitive permission type, but these models were no better (and often worse) than our general model. Tailored models may be more useful when implemented at scale.

#### Attacking the ML Model

Attacking the ML model to gain access to users' data without prompting is a legitimate concern. Adversaries can influence the proposed permission model in several ways: (i) imposing an adversarial ML environment, (ii) polluting the training set to bias the model to accept permissions, and (iii) manipulating input features to get access without user notification. We assume the platform is not compromised, as a compromised platform will degrade any permission model's ability to protect resources.

A thorough analysis on this topic is outside our scope, but we examined the possibility of manipulating features to get access to resources without user consent. None of the behavioral features used in the model can be influenced, as that would require compromising the platform. An adversary can control the runtime features for a given permission request by choosing when to request the permission. We generated feature vectors manipulating every adversary-controlled value and combination from our dataset and tested them on our model. We did not find conclusive evidence that the adversary can exploit the ML model by manipulating the input features to get access to resources without user consent.

#### Experimental Caveat

We note a caveat about our experimental data: users were free to deny permissions without any consequences. Participants were informed that their decisions to deny permission requests would have no impact on the actual behavior of their applications. This is important because denying a permission may cause undefined behavior or loss of functionality. If these consequences were imposed, users might prioritize functionality over privacy. The denial rates in our experiments represent the actual privacy preferences of users and their expectations of reasonable application behavior, not a choice between functionality and privacy. Our proposed model was effective in guarding resources for users who are selective in their decision-making, reducing the error rate of Contextuals by 44%.

#### Types of Users

We categorized users based on the significance of the application's visibility in their individual privacy decisions. In an actual permission denial setting, the distribution may differ from what was observed in our study. This categorization motivates a deeper analysis of the factors dividing Contextuals and Defaulters. While visibility was an important factor, there may be others that are significant and relevant. More work is needed to explore how Contextuals make decisions and which behaviors correlate with their decisions.

#### User Interface Panel

Any model predicting user decisions carries the risk of making incorrect predictions. Making predictions on behalf of the user is necessary because permissions are requested by applications too frequently for manual examination. While we do not expect any system to achieve perfect accuracy, we believe our 97% accuracy can be improved. One way to improve accuracy is to empower the user to review and make changes to the ML model's decisions through a user feedback panel. This gives users recourse to correct undesirable decisions and reduce usability issues and functionality loss stemming from permission denial. The panel should help the user identify which rule incurred the functionality loss and change it accordingly. Users can also use this to adjust their settings as their privacy preferences evolve over time.

#### The Cost of Greater Control

A more restrictive platform means users will have greater control over the data shared with third parties. Applications that generate revenue based on user data could be cut off from their primary revenue source, disrupting the current ecosystem and forcing app developers to degrade app functionality based on data availability. We believe the current ecosystem is unfairly biased against users and that tighter control will make the user an equal stakeholder. While more work is needed to understand the effects of a more restrictive platform, we believe it is imperative to let the user have greater control over their own data.

#### Conclusions

We have shown several important results. Users care about their privacy and deny a significant number of requests to access sensitive data. Existing permission models for Android phones still result in significant privacy violations. Users may allow permissions sometimes and deny them at others, indicating that more factors go into the decision-making process than just the application name and permission type. We collected real-world data from 131 users and found that application visibility and the current foreground application were important factors in user decisions. We used this data to build a machine-learning model to make automatic permission decisions. One of our models had a comparable error rate to AOFU and benefited from not requiring any user prompting. Another model required some user prompts—less than AOFU—and achieved an 81% reduction in AOFU's error rate.

### Acknowledgments

This research was supported by the United States Department of Homeland Security’s Science and Technology Directorate under contract FA8750-16-C-0140, the Center for Long-Term Cybersecurity (CLTC) at UC Berkeley, the National Science Foundation under grant CNS-1318680, and Intel through the ISTC for Secure Computing. The content of this document does not necessarily reflect the position or the policy of the U.S. Government, and no official endorsement should be inferred.

### References

[1] Y. Agarwal and M. Hall, “Protectmyprivacy: Detecting and mitigating privacy leaks on iOS devices using crowdsourcing,” in Proceedings of the 11th Annual International Conference on Mobile Systems, Applications, and Services.