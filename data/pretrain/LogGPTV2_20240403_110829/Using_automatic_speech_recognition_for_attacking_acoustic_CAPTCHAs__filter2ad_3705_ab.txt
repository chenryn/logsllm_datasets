we regard the transcription of a CAPTCHA as a sentence.
3. ATTACKING CAPTCHAS USING ASR
In this section, we conduct a security analysis on a pre-
vious version of reCAPTCHA [12] and compare our results
with those of Bursztein et al. [3]. We show that state-of-
the-art speech recognition is more suitable for evaluating the
security strength of a CAPTCHA in that it leads to higher
success rates than a generic classiﬁcation-based approach.
where O = o1 . . . oT represents a sequence of T observations.
A solution for Eq. 5 can be found by using the Baum-Welch
2A phoneme is the smallest linguistic unit in a certain lan-
guage.
Data Description.
We use data of a previous reCAPTCHA scheme compris-
ing 198 labeled signals where the labels include the respec-
tive temporal information about the exact digit positions.
The audio ﬁles were downloaded from the reCAPTCHA
website [12] in 2010 and hand-labeled at our institute. Each
CAPTCHA is constructed from a sequence of 8 digits be-
tween zero and nine. The CAPTCHAs consist of real speech
and the digits in each CAPTCHA are spoken by diﬀerent
male and female speakers. All CAPTCHAs are distorted
with additive background noise that represents a time re-
versed speech signal as it is reported by Tam et al. [26].
]
z
H
k
[
y
c
n
e
u
q
e
r
F
4
3
2
1
0
2
4
6
8
12
10
Time [s]
14
16
18
20
22
Figure 1: Example of reCAPTCHA 2010 showing the digit
sequence “5-6-3-6-0-8-9-8”.
The signals are sampled at 8 kHz and the spectral con-
tent is band-limited to 3 kHz. Each signal starts with a short
tonal sound (beep) which is then followed by eight isolated
digits that are clearly separated in time. The signal parts
between the digits contain only the interfering background
noise and will thus be referred to as noise segments later on.
Figure 1 provides an example of the CAPTCHA by showing
the spectrogram, which is derived from the squared magni-
tude of the short-time Fourier transform. Since all CAPT-
CHAs exhibit the same principal structure, it is straightfor-
ward to derive a suitable grammar that can be incorporated
into the decoder to improve the recognition performance.
Due to the limited number of signals, we make use of a
K-fold cross-validation. For this purpose, the corpus is par-
titioned into K = 6 complementary subsets, where we train
an individual recognizer for each subset by using 60 % of the
data and the remaining data is used for assessing the recog-
nition performance. The overall recognition performance,
i.e., the word and the sentence accuracies, are obtained by
averaging the decoding results of all cross-validation sets.
ASR Training.
Each word is modeled with an HMM that has 3 states
per phoneme and 8 Gaussian components per state. Two
additional models are utilized for representing the starting
tone and the noise-only segments, respectively. We provide
our analysis results for two diﬀerent feature types that are
either derived from 13 MFCCs or 13 PLPs. In each case,
the features are augmented with their respective ﬁrst and
second order temporal derivatives, yielding a 39-dimensional
feature vector. To train the recognition system, it is ﬁrst
necessary to initialize the model parameters λ as deﬁned
by Eq. 4. We have found that a ﬂat-start initialization of
the model parameters leads to poor recognition rates. As
we have the temporal label information at hand, we derive
the initial model parameters by using a bootstrap approach.
Here, we vary the amount of the bootstrap data and compare
the results in Sec. 3.1. After model initialization, several
iterations of Baum-Welch training are performed to estimate
the model parameters.
Table 1: Average cross-validation results in percent for re-
CAPTCHA 2010 shown for a varying amount of bootstrap
data (Init [%]). The best results are highlighted.
Feat.
Init [%]
Sent Acc. Word. Acc
MFCC
PLP
25
50
75
100
25
50
75
100
11.11
10.10
14.14
12.12
16.67
19.19
13.64
18.18
74.24
73.42
74.87
73.86
78.03
78.98
78.34
77.84
Signal Decoding.
Due to the consistent structure of the CAPTCHA, we
utilize a task-speciﬁc grammar for decoding:
S → T N D N D N D N D N D N D N D N D N ,
(9)
with S, T, N denoting the start symbol, the beep tone and
the noise segment, respectively. The symbol D represents
the individual digits, i.e.,:
D → zero|one|two| · · · |eight|nine .
(10)
3.1 Evaluation
Table 1 shows the average cross-validation results of re-
CAPTCHA 2010 for a varying amount of bootstrap data.
The PLP features outperform the MFCC features by ap-
proximately 5 % for the sentence accuracy, which might be
due to the occurrence of diﬀerent speakers in the CAPT-
CHA, as the PLP features are more suitable for speaker-
independent recognition. The highest sentence accuracy for
the PLP features is achieved for initializing the models with
50 % of the available data in the respective cross-validation
set. Thus, we can see that an increased amount of bootstrap
data does not necessarily result in a higher recognition per-
formance, which might be due to overﬁtting eﬀects that lead
to a reduced generalizability of the models.
When comparing our results with those of Bursztein et
al. [3], we can see that our ASR-based approach leads to a
considerably increased success rate. The maximum achieved
sentence accuracy for reCAPTCHA reported in [3] is 1.52 %,
a factor of 13 below the best score of Tab. 1. Hence, it is ad-
visable to use speech recognition methods for evaluating an
acoustic CAPTCHA as this provides a more realistic indica-
tion of the level of robustness against automated attacks.
4. ANALYSIS OF RECAPTCHA 2014
We now provide a security and usability study of the cur-
rent reCAPTCHA scheme. For this purpose, we have col-
lected approximately 2,500 CAPTCHAS from the reCAP-
TCHA website [12] between November and December 2013
where we utilize only a fractional amount of the data for
training our system. The respective transcriptions were ob-
tained in a listening experiment that is outlined in detail in
Sec. 4.2. We selected reCAPTCHA since it is widely used
and previous versions of the scheme have proven to be rel-
atively robust against automated attacks in comparison to
other CAPTCHAs [3, 26].
4.1 Overview of the CAPTCHA Scheme
The current version of reCAPTCHA is similar to the ver-
sion of 2010 in that it is only constructed from a sequence
of digits between zero and nine. However, the total num-
ber of spoken words is not ﬁxed but varied between 6 and
12. Furthermore, the digits are spoken in a block-wise man-
ner, which means that some digits are immediately followed
by other digits without any noticeable speech pause in be-
tween and some of the digits are even overlapping in time.
However, all blocks of those contiguously spoken digits are
well separated in time by a short speaking pause and the
majority of CAPTCHAs consists of three blocks. The dig-
its are spoken by both a male and a female voice at a very
slow speaking rate. The speech appears synthetic and the
overall voice quality is comparatively low. All signals ex-
hibit the same stationary background noise and the energy of
the background noise is inversely proportional to frequency.
The sampling frequency is 16 kHz and the signals are band-
limited to approximately 5.8 kHz. An example of the CAP-
TCHA is given in Fig. 2 by showing the spectrogram.
]
z
H
k
[
y
c
n
e
u
q
e
r
F
8
6
4
2
0
2
4
6
8
Time [s]
10
12
14
Figure 2: Example of reCAPTCHA 2014 showing the digit
sequence “314-694-5279”.
4.2 Obtaining Transcriptions
To obtain the CAPTCHA transcriptions that are essen-
tial for training the ASR system, we conducted a listening
experiment at our institute. The listening experiment took
place in a controlled environment, i.e., an audiometric room,
in which the acoustic conditions were kept constant through-
out. All audio signals were presented via professional studio
headphones and the audio volume could be adjusted to the
level of comfort by the participants.
A group of 12 human listeners participated in the exper-
iment and each participant was asked to label a set of 50
CAPTCHA signals. The participants were briefed that the
signals consist of a varying number of digits that are sep-
arated in time by distinct speech pauses. The task was to
provide a numeric transcription using only the letters {“0”,
“1”, “2”, . . . , “9”} for each digit as well as a marker symbol
“-” for each speech pause, e.g., a complete transcription may
read “894-770-136”. The marker symbol is used to enable an
identiﬁcation of individual digit blocks later on. We created
a graphical user interface in MATLAB that enables the au-
dio playback as well as the labeling for a set of successively
presented CAPTCHA signals. The setup allows the user to
playback each audio signal multiple times until a transcrip-
tion has been saved by the user. It was not allowed to play
or edit a previously transcribed CAPTCHA. We collected 4
transcriptions for each CAPTCHA to enable an identiﬁca-
tion of reliable transcriptions later on. Some of the listeners
participated in the experiment multiple times where we en-
sured that no CAPTCHA was labeled more than once by the
Table 2: Inter-labeler agreement in percent for reCAPTCHA
2014. The results are shown for the individual digit block
transcriptions and for the full transcription (i.e., an agree-
ment on all blocks). The scores are based on the listening
test results of 250 diﬀerent CAPTCHAs.
# Agreements
1 (No)
2
3
4 (All)
Block transcription
Full transcription
11.20
49.20
29.73
36.00
31.87
10.00
27.20
4.80
same person. In this way, we collected 1000 transcriptions,
corresponding to 250 individual CAPTCHAs.
Ethical Considerations.
Before performing the listening test, all participants were
informed that they were to take part in a scientiﬁc study
and that their data was used to analyze the properties of a
CAPTCHA. All data collected during our study was used in
an anonymized way so that there is no link between collected
data and individual participants. Our institute does not fall
under the jurisdiction of an IRB or similar ethics committee.
4.3 Usability
After the listening experiment, we received a feedback
from some of the participants, stating that most signals are
very diﬃcult to understand. This can be conﬁrmed by the
fact that there is only a relatively small number of CAPT-
CHAs where the transcriptions of four diﬀerent participants
match with each other. Table 2 shows the percentage of
CAPTCHAs and digit blocks on which there was no agree-
ment (1), and on which 2, 3 or 4 participants agreed. Since
each CAPTCHA consists of a ﬁxed number of digit blocks,
an agreement on all digit blocks is equivalent to an agree-
ment on the full transcription of the CAPTCHA.
We can see from Tab. 2 that the overall transcription
agreement between the participants is comparatively low, as
there is only a full agreement on the transcriptions for 4.8 %
of the signals between all participants and 49.2 % of the sig-
nals received diverging transcriptions, i.e., no pair of partic-
ipants agreed on the same transcription. This indicates that
the spoken digit sequences are perceived diﬀerently among
the individual participants and that most CAPTCHAs are
presumably only partially solvable. Nevertheless, the major-
ity of digit blocks, i.e., 59.07 % (31.87 % + 27.20 %), received
an identical transcription by at least 3 participants, which
we exploit for training the ASR system in the next section.
4.4 Security Analysis
We assess the security of reCAPTCHA by ﬁrst training
our automated solver oﬄine and then attacking the CAP-
TCHA signals on the reCAPTCHA website [12]. The em-
ployed ASR system uses the type of models described in
Sec. 3, since the principal recognition task remains unchanged.
Training the ASR System.
Due to a high discrepancy between the CAPTCHA tran-
scriptions, we are not able to train the ASR system on the
full transcriptions in a reliable manner. In order to cope with
this issue, we identify those signal parts, i.e., digit blocks,
with a high inter-labeler agreement. We extract all digit
blocks that were labeled with at least 3 identical transcrip-
tions. The extraction of individual digit blocks from the
CAPTCHAs is based on a voice activity detector (VAD),
which identiﬁes the temporal locations of digit blocks and
noise segments. The VAD represents the same ASR sys-
tem that is used for the attack with the diﬀerence that it
uses only two diﬀerent models, i.e., one universal model for
speech and one model for noise. The VAD was trained by
using 5 randomly chosen CAPTCHAs that were manually
labeled. Using the VAD yields an overall number of 443 in-
dividual digit blocks—corresponding to 1500 single digits—
that are used for training the attacking ASR system.
Performing the Attack.
The attack on the reCAPTCHA scheme was conducted on
February 12th and 13th 2014 by solving the acoustic CAP-
TCHAs on the reCAPTCHA website [12] via an automated
script. This script successively downloads the audio signals
and submits the respective transcriptions that have been
produced by the ASR decoder. Due to a limited number of
possible CAPTCHA downloads within a certain period of
time from a single IP address, we used TOR [8] to increase
the number of available IP addresses and enable a larger
number of CAPTCHA requests for our experiment.
When ignoring the Internet connection speed, the solving
time per CAPTCHA depends mainly on the Viterbi decoder
of HTK. On average, we solve 2 CAPTCHAs per second on
a standard desktop computer, which is 32× faster than a
user would need to listen to the signals.
For speech recognition, we exploit our prior knowledge
about the CAPTCHA scheme and adjust the employed gram-
mar as follows:
S → N B N B N B N ,
B → D D [D] [D] ,
(11)