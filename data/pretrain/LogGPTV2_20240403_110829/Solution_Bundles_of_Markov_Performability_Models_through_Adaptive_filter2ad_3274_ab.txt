methods require to evaluate the measures accurately on a (small)
set of points and to tweak a pre-deﬁned set of approximation
functions (called approximants) that, combined together, deﬁne
continuous approximations of the measures.
In this paper, the Adaptive Cross Approximation (ACA)
technique is adapted to approximate performability measure
bundles, that in turn are deﬁned on solution bundles of the
1Originally thought to address a different problem, namely largeness vs
stiffness vs accuracy [4]–[6], but applicable here to mitigate the issue.
Model Solver
Analysis
(t, θ)
ˆm()
mθ(t)
ˆm(t, θ)
(a) On demand approach
(b) Decoupled approach
Fig. 2.
(a) standard approach, where the solver is called on each sample of
constant parameters (t, θ) to get the measure mθ(t); (b) decoupled approach,
where ﬁrst the measure approximation ˆm is determined and then exploited.
Kolmogorov forward equation that characterizes CTMCs. In
particular, ACA is based on separable approximants, i.e., each
approximant is the product of functions that depend only
on time or on a single parameter. Arbitrary accuracy of the
technique has been proven in [9].
Therefore, the main contribution of this paper is the de-
velopment of an efﬁcient and accurate method to evaluate
performability measures along ﬁbers, exploiting the separable
approximants feature of ACA, as depicted in Figure 1. It works
well under the assumption that the dependencies of the model
on parameters are smooth functions (i.e., have continuous
derivatives up to a reasonably high order), as exempliﬁed in
Section II-C. Notice that time is one of the ﬁbers, thus the
proposed technique can be interpreted as a generalization of
the semi-symbolic solution method presented in [7]. Details
are in Sections IV-D and IV-E.
Enlarging the view and considering that Approximation The-
ory and Machine Learning are close disciplines, it is possible
to classify results of the former adopting the perspective and
the parlance of the latter. In particular, the presented approach
can be classiﬁed as Interpretable AI [10] because separable
approximants promote decomposability, a key aspect of IAI.
The rest of the paper is organized as follows. Section II
provides the background knowledge on Markov Dependability
(mainly Reliability-focused) models, reward structures and
measures of interest. To better appreciate the novelty of
the contribution, a high level description of the problem is
also provided. Section III brieﬂy discusses how to choose
approximation points. Section IV presents the new method.
Section V introduces the two case studies, then exploited in
Section VI to conduct performance analysis and comparison
with two alternative methods. Section VII reviews related work.
Section VIII draws conclusions and discusses future work.
II. CONTEXT AND CONTRIBUTION
In this section the addressed parametric models and measures
of interest are introduced. Then the problem of the evaluation
of such measures for different values of the parameters is
described in abstract terms to better focus on the shift in role
of the parameters and highlight the issues that have to be solved
or mitigated. Finally, the kind of parameters that can appear
in the models are discussed.
In the following, subscript θ indicates that parameters are
treated as constants, while θ in parenthesis indicates that they
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:18:14 UTC from IEEE Xplore.  Restrictions apply. 
382
are treated as variables. Moreover, all the vectors are row
vectors, following the Markov chain community notation.
(cid:3) t
A. Models and measures of interest
The models addressed in this paper are CTMCs with state-
space S and with inﬁnitesimal generator matrix Q(θ), where
θ ∈ Rp is the parameters vector of the model. This means
that some of the entries of Q are functions of some of the
entries of θ. At every instant of time t, also the state probability
vector π(t, θ) depends on θ. The equations that characterize
the CTMC and the measures of interest are:
= π(t, θ) · Q(θ),
:= π0,
= b(t, θ) · Q(θ) +π 0,
:= 0,
π(0, θ)
∂b(t,θ)
(cid:2)
(cid:2)
b(0, θ)
∂π(t,θ)
(1)
(2)
∂t
∂t
where π0 is the initial probability vector, independent of θ,
and b(t, θ) =
0 π(τ, θ)dτ, i.e., the i-th entry of b(t, θ) is the
sojourn time of the CTMC in the state i in the interval [0, t].
Unfortunately, closed formulas for the solution of Equa-
tions (1) and (2) are not directly exploitable for concrete
computations [1] when the models at hand are too large to allow
explicit matrix powers evaluation and, even for relatively small
models, when reliability/availability models are stiff 2. This
motivates the quest for solution methods capable to address
large CTMC and to tolerate stiffness.
Measures of interest addressable by the proposed solution
bundle method are in general dependability, performance and
performability indicators. For the sake of simplicity, in the
following performability is referred as the representative mea-
sure, with other indicators introduced only when speciﬁcally
addressed.
Performability measures are deﬁned in terms of moments
of the following random variables:
rs · I
s
t ,
rs · J
V :=
Y :=
s∈S
s
[0,t],
(cid:4)
(cid:4)
s∈S
(3)
(4)
where I s
t is the indicator random variable representing the event
that the model is within state s at time t, J s
[0,t] is the random
variable representing the total time the model is within state s
during [0, t]. Here, rs is a real value gained by the stochastic
process while staying within the state s. Correspondingly, r
will be called reward vector.
Starting from the deﬁnition of expected value and switching
the order of integration, Equation (1) and Equation (3) can lead
to deﬁne the instant of time reward measure E[V ] = dot(π, r).
Similarly Equation (2) and Equation (4) lead to the accumu-
lated reward measure E[Y ] = dot(b, r). Other performability
measures can be deﬁned similarly in terms of higher moments
or as conditional expectations [1], [3].
2Extreme disparity among the entries of the inﬁnitesimal generator matrix
and the time horizon of interest [11]. More formally, calling q(θ) :=
maxij |Qij (θ)|, ifq (θ) · tmax is large then the model is stiff.
cf (nr − 1)λ
···
nr-1
cf 2λ
μ
(1 − cf )(nr − 1)λ
μ
cf nrλ
μ
crμd
start
nr
(1 − cf )nrλ
0
(1 − cr)μd
1
λ
0f
Fig. 3. Running example: CTMC of Case study 1, representing a degradable
system with failure coverage and repair. Each state i ≥ 0 represents the number
i of currently operational components; 0f represents the system failure. Full
description is in Section V-A.
If the reward vector has nonnegative entries,
then the
measures can be evaluated numerically maintaining the ac-
curacy obtained for the probability vector. In fact, V and
Y are sums of positive values, thus avoiding any numerical
cancellation. Otherwise, it is important to avoid the computation
of differences of large numbers, which can be achieved by
re-scaling the rewards.
Finally, to exemplify the contribution of Section IV when
appropriate, consider the running example in Figure 3, where
the CTMC of Case study 1 is illustred (full description is in
Section V-A). For exempliﬁcation purpose, only one parameter
θ1 := λ is considered, while all the other parameters are deﬁned
as constant numerical values.
B. Problem Characterization and Contribution
Within a Reliability or Availability model [1], p parameters,
i.e., the entries of the vector θ ∈ Rp, are p constants yet to
be deﬁned. So, in general, the measure of interest mθ(t) is
a function of xθ(t) ∈ RN , that in turn solves the equation
Fθ(xθ(t)) = 0, where N is the number of model states.
In the context of this paper, the abstract function m and the
variable x correspond to the concrete reward measures E[V ]
or E[Y ] and to π or b, respectively. In the running example,
m is the Reliability and θ is the scalar λ shown Figure 3.
Moreover, F can be the forward Kolmogorov equation or the
integral Kolmogorov equation corresponding respectively to
Equations (1) and (2).
In both cases, Fθ is a set of Ordinary Differential Equations
(ODEs). This way, a family of equations and solutions, indexed
by the constant θ, is deﬁned, each having t as independent
variable and the components of xθ(t) as dependent variables.
To evaluate how θ affects mθ(t), classical methods ﬁrst
deﬁne the samples θ(1), . . . , θ(nsample), drawn from some distri-
bution, and then solve nsample times Fθ = 0 or produce nbatch
results for each sample (total of nsample · nbatch simulations)
directly simulating3 the model (as shown in Figure 2a). From
one hand, statistical analysis grounded on model simulations is
perfect for assessing the impact of parameters’ uncertainty, but
3A word of caution: in the ODEs community, simulation usually refers to
numerical solution of the equations. Instead in this paper “simulation” means
“discrete event simulation”, as usually understood in the context of discrete
space, continuous time models.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:18:14 UTC from IEEE Xplore.  Restrictions apply. 
383
can rarely achieve high accuracy when dealing with large and
complex models, and sometimes addressing rare events can be
problematic. On the other hand, numerical analysis typically
offers an accurate solution for each sample of θ, and insights
on parameters’ relevance through local sensitivity analysis,
but the other kind of analysis (e.g., propagating uncertainty
from parameters to results) is sometimes infeasible because
the solution of each instance can require huge computations,
and a large nbatch is needed.
Conversely, in this paper the parameters θ are treated as
independent variables instead of constants, so the set of
Partial Differential Equations F (x(t, θ), θ) = 0 deﬁnes the
solution bundle x(t, θ), that in turn deﬁnes the measure bundle
m(x(t, θ), t, θ). The shift of parameters’ role is depicted in
Figure 1. In the following, to simplify notation, the arguments
of m will be omitted when clear from the context.
Once an expression for m is found, local sensitivity analysis
becomes (following one of the most popular deﬁnitions [12]):
m is sensitive to a (small) variation of θj if ∂m/∂θj is large.
Similarly, global sensitivity analysis, uncertainty quantiﬁcation
[13], i.e., study how uncertainty in the evaluated measure can
be attributed to different sources of uncertainty in parameters
value, parameter estimation [14] and optimization [15] can all
be expressed in terms of m. Also, computations data ﬂow is
simpliﬁed, as depicted in Figure 2. The problem is that ﬁnding
a closed formula for m is infeasible in practice for large N.
Thus, the presented method exploits classical Approximation
Theory, where a set of functions (dense in the solution
space) are employed to obtain an approximation ˆm(t, θ) of
m(x(t, θ), t, θ). Notice that ˆm does not depend directly on
x, as m does. Depending on which aspect is intended to be
highlighted, and the community where a particular technique
is imported to, such an approximation is called differently.
For instance, in Machine Learning, being the focus on input-
output relations, m itself is called model, and, being the
approximation deﬁned starting from a few evaluations of the
model (details in Section IV), ˆm is called metamodel [16], [17].
When highlighting the fact that, to be useful, the approximation
has to be much simpler to evaluate than the original at the cost
of loosing in accuracy, ˆm is called surrogate model. Here, to
better distinguish the Reliability/Availability model from its
solution, ˆm will be called approximated measure bundle. The
main issues encountered when addressing how to deﬁne ˆm are:
• the fully symbolic evaluation of m, in t and θ, is unfeasible
for large models (pictorially: moving freely on all the gray
area in Figure 1b),
• the semi-symbolic method treats symbolically only time
(pictorially: moving only along time ﬁbers in Figure 1b),
and the already published generalizations are:
– designed to work only for p = 1, such as chebop2
(that is unacceptably restrictive because it works only
for N = 1 [18]),
– too computationally expensive4,
– feasible for acyclic CTMC (e.g., generalization of [19]),
• fully-numeric methods are computationally too expensive,
as discussed at the beginning of this section, because
constructing ﬁrst the full tensor (i.e., the multidimensional
array) with entries
(cid:6)
(5)
for samples indexed by i0 ∈ {1, . . . , n0},
i1 ∈
{1, . . . , n1},. . . , ip ∈ {1, . . . , np} and then deﬁning ˆmt,θ
through interpolation on M is unfeasible: it requires
O(n0 · n1 ···n p) solutions of F = 0 and huge memory.
Thus, here, by the adaption of the ACA algorithm ﬁrst proposed
by Bebendorf in [9], ˆm is an approximation of m deﬁned as
a sum of separable approximations
Mi0,i1,...,ip := m
(i1)
, θ
1
(i0)
t
(cid:5)
, . . . , θ
(ip)
p
ˆm(t, θ) :=f1(t) · g1,1(θ1)···g 1,p(θp) +. . .
···+ fk(t) · gk,1(θ1)···g k,p(θp),
(6)
where fi and gij are approximation functions chosen as detailed
in Section III. The approximation is separable in the sense that,
for all i and j, gij depends only on one parameter (and fi
only on time), so pictorially corresponds to moving along all
the ﬁbers in Figure 1b. This makes the method interpretable.
In addition, the great improvement with respect to the full
evaluation of the tensor as in Equation (5) is that, being the
solution of the set of Partial Differential Equations (PDEs)
F = 0 smooth, evaluating ˆm on sample points requires O(k)
operations each, and generates ˆM of rank k, that can be stored
efﬁciently (details in Section IV). Stated differently, ﬁnding
ˆM means to ﬁnd a low-rank approximation of M. For p = 1,
ˆM is a matrix, and the theory behind the approximation is
fully understood. For p >1, theory is not able to answer key
questions yet, as discussed in Section IV-C.
C. Dependencies on parameters
Several kinds of parameter can appear in performability
models. Here, examples that appear often, and the dependency
they introduce in Q(θ), are classiﬁed according to the corre-
sponding elementary functions. Stated differently: each entry
of Q(θ) can be a constant or a function of θ, the types of
function that appear often in the addressed models are:
linear: e.g., Qnr,nr−1(θ) = cf · nr · θ1 in the running example
quadratic: e.g., Qij(θ) = θ1 · θ2 and Qij(θ) = θ1 · (1 − θ2)
when modeling failure coverage, i.e., θ1 = λ is the failure
rate and θ2 = c is the coverage probability,
(as shown in Figure 3),
reciprocal: e.g., Qij(θ) = ne/θ1 when approximating a non-
exponential sojourn time with an Erlang (in this case a
deterministic time θ1 = L approximated with an Erlang
comprising ne exponential jumps whose rates are all equal
to ne/L) and ne is a constant.
4To the best of authors’ knowledge, the method implemented in SHARPE
[7] addresses only mθ with complexity O(N 3). Extending it to address m
could be too expensive for practical usability.
384
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:18:14 UTC from IEEE Xplore.  Restrictions apply. 
In all the mentioned cases, Q(θ) is smooth, and then the
measure approximation ˆm deﬁned in Equation (6) is cheap to
evaluate and store, as detailed in Section IV, where ad hoc
methods to evaluate the ﬁbers of ˆM are presented. Notice that
the method introduced in this paper can address, with slight
modiﬁcations, other elementary functions as well (as long as
m is smooth).
III. CHOICE OF THE EVALUATION POINTS
The method proposed evaluates the measures of interest at
a discrete set of times t(i), and at prescribed values for the
parameters θj, for j = 1, . . . , p.
For what concerns the time variable, the choice of evaluating
at a uniform grid on the interval [0, tmax], where tmax is the
system mission time, is often encountered, and may depend
on the speciﬁc method used to integrate the equation.
If the values that are of interest for the parameters in the
vector θ are known a priori, then it is very reasonable to choose
these values for the discretization. Namely, if the points needed
for the analysis are known in advance then it is reasonable to
choose the approximation points as a (small) subset of those.