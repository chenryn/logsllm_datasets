4
5
6
7
8
9
10
11
12
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
R
2. A → α, with A ∈ N , α ∈ Σ
3. A → A1, with A, A1 ∈ N
4. A → , with A ∈ N
We note that WNF is similar to Chomsky normal form
(CNF), but less restrictive, because WNF allows two ad-
ditional rule forms. Any CFG can easily be converted to to
WNF, similar to the conversion to CNF, and CNF implies
WNF.
Let G = (N , Σ,R, S)
Typed Parse-Tree Ordering.
be an arbitrary CFG in WNF, and assume that there is an
on R that orders the rules in RA, for
ordering relation <
every A ∈ N . The following deﬁnition speciﬁes an ordering
relation on the set M P T (G, A), and from this we will deﬁne
how to rank parse trees.
R
Definition 1. For every non-terminal A ∈ N , we deﬁne
an ordering relation ≺A on M P T (G, A) as follows: ∀X, Y ∈
M P T (G, A) : Y ≺A X iﬀ (cid:17)Y (cid:17) < (cid:17)X(cid:17) or (cid:17)Y (cid:17) = (cid:17)X(cid:17) and
one of the following conditions is true:
X.ρ
1. Y.ρ <
∧ Y [1] ≺A1 X[1]
2. Y.ρ = X.ρ = A → A1
3. Y.ρ = X.ρ = A → A1A2 ∧ Y [1] ≺A1 X[1]
4. Y.ρ = X.ρ = A → A1A2 ∧ Y [1] = X[1] ∧ Y [2] ≺A2
X[2]
Although these equations can lead to recursion (if A is re-
cursive), the deﬁnition is well founded because X and Y are
ﬁnite trees, and their children have fewer nodes.
The following lemma, whose proof we defer to the full ver-
sion, provides assurance that our upcoming ranking scheme
works.
Lemma 2. For every CFG G and non-terminal A in G
the order ≺A is a strict total order on M P T (G, A).
If ρ ∈ RA, the conditions described in Deﬁnition 1 are
mutually exclusive and we count how many Y ’s satisfy each
of them using the following auxiliary values:
1. Nρ(l) = number of parse trees that yield a string of
length l derived from rule ρ.
Nρ(l) = |{Y ∈ M P T (G) : (cid:17)Y (cid:17) = l ∧ Y.ρ = ρ}|
2. NA(l) = number of parse trees that yield a string of
length l derived from A.
NA(l) = |M P T (G, A, l)|
(cid:2)
(cid:2){Y ∈ M P T (G, A, l) : Y.ρ <
R
3. Bρ(l) = number of parse trees that yield a string of
length l derived from a rule of A preceding ρ. This
corresponds to condition 1 in Deﬁnition 1.
ρ}(cid:2)
(cid:2)
Bρ(l) =
4. Bρ(l1, l) = number of parse trees that yield a string of
length l derived from rule ρ : A → A1A2, and whose
ﬁrst child yields a string shorter than l1.
Bρ(l1, l) = |{Y ∈ M P T (G, A, l) : (cid:17)Y [1](cid:17) < l1}|
We use these values to compute rankA(X) based on Equa-
tion (1), by observing that the conditions in Deﬁnition 1
are mutually exclusive. This justiﬁes the correctness of the
ranking function shown in Algorithm 1. Observe that (cid:17)X(cid:17) (cid:18)=
(cid:17)Y (cid:17) =⇒ rankA(X) (cid:18)= rankA(Y ). Therefore the restriction
of rankA to I = M P T (G, S, z) is bijective; its inverse is given
by unrankA( , z) in Algorithm 2.
The correctness of unranking in Algorithm 2 uses the fol-
, X.ρ =
lowing observation: Assume that ρ, ρ
ρ and (cid:17)X(cid:17) = l. Then (1) Bρ(l) ≤ rankA(X) < Bρ(cid:2) (l). Using
(cid:3)∈RA, ρ <
R
ρ
(cid:3)
1 rankA(X ∈ M P T (G, A)) :
ρ ← X.ρ;
l ← (cid:4)X(cid:4);
switch (ρ) do // check all WNF rule forms
return Bρ(l) +rank A1
case A → A1 :
case A → A1A2:
l1 ← (cid:4)X[1](cid:4);
r1 ← rankA1
(X[1]); // rank of 1st
r2 ← rankA2
(X[2]); // rank of 2nd
return Bρ(l) +B ρ(l1, l) +r 1 ∗ NA2
case A →  :
case A → α :
return Bρ(l);
return Bρ(l);
(X[1]);
child
child
(l − l1) +r 2;
Algorithm 1: Ranking of parse trees
unrankA(rank r, length l) :
ρ ← max<R{ρ ∈ RA : Bρ(l) ≤ r};
r(cid:3) ← r − Bρ(l);
switch ρ do // check all WNF rule forms
case A → A1: return Tree(ρ, unrankA(r(cid:3)
case A → A1A2: //child yields?
and 2nd
// if r1, r2 = rank of 1st
, l));
child,
and l1, l2 = length of children yields,
then (1) l1 + l2 = l, and (2)l 1 satisfies
r(cid:3) = Bρ(l1, l) +r 1 ∗ NA2 (l − l1) +r 2
(r1, l1);// 1st
l1 ← max{l1 ∈ [0..l] :B ρ(l1, l) ≤ r(cid:3)};
r(cid:3)(cid:3) ← r(cid:3) − Bρ(l1, l);
/ NA2 (l − l1);
r1 ← r(cid:3)(cid:3)
X[1] ← unrankA1
r2 ← r(cid:3)(cid:3) % NA2 (l − l1);
X[2] ← unrankA2
return Tree(ρ, X[1], X[2]);
case A → : return Tree(ρ);
case A → α: return Tree(ρ);
Algorithm 2: Unranking of parse trees
(r2, l2);// 2nd
child
child
this observation it is easy to see that the value max<R{ρ ∈
RA : Bρ(l) ≤ r} uniquely and correctly identiﬁes the desired
rule of A. The only non-trivial case is when this rule is of
the form A → A1A2. In this case, we must determine l1,
which is the size of the corresponding word derived from A1.
= r−Bρ(l)
Using the formula in the ranking function and r
we get:
(cid:3)
(cid:3)
r
= Bρ(l1, l) +rank A1 (X[1])NA2 (l−l1) +rank A2 (X[2])
< Bρ(l1, l) +rank A1 (X[1])NA2 (l−l1) +N A2 (l−l1)
= Bρ(l1, l) + (rankA1 (X[1]) + 1)NA2 (l−l1)
≤ Bρ(l1, l) +N A1 (l1)NA2 (l−l1)
= Bρ(l1 + 1, l)
This guarantees that the selection of l1 such that Bρ(l1, l) ≤
(cid:3)
< Bρ(l1 + 1, l) succeeds and is correct. The correctness
r
for the rest of the unranking algorithm is straightforward.
Algorithm 3 computes the values Nρ(l), NA(l), Bρ(l) and
Bρ(l1, l), using memoization only for NA and Nρ. All mem-
oization tables entries are initialized to ⊥. Although the
grammar can be recursive, the functions in Algorithm 3 do
not lead to inﬁnite recursions, because of the check and ini-
tialization code placed at lines 2–3 in Nρ and at lines 13–14
in NA. If, during the ﬁrst execution of NA(l), a recursive
call to NA is made using the same value for l, then the sec-
ond call returns 0. This is correct, because NA counts the
number of minimal parse trees rooted in A, and a minimal
parse tree in M P T (G, A, l) should not have another subtree
in M P T (G, A, l).
12971 Nρ(l ∈ N) :
// use table Nρ initialized with ⊥
if ( Nρ[l] (cid:9)= ⊥) then return Nρ[l];
Nρ[l] ← 0;
switch (ρ) do // check all WNF rule forms
case A → A1 :
case A → A1A2: r ← Σl
case A →  :
case A → α :
r ← NA1
r ← (l = 0)?1 : 0;
r ← (l = 1)?1 : 0;
i=0NA1
(l);
(i)NA2
(l − i);
// must be 0, in case of recursion
Nρ[l] ← r;
return Nρ[l];
12 NA(l ∈ N) :
// use table NA initialized with ⊥
if ( NA[l] (cid:9)= ⊥) then return NA[l];
NA[l] ← 0;
NA[l] ← (cid:2)
return NA[l];
Nρ(l);
ρ∈RA
// must be 0, in case of recursion
2
3
4
5
6
7
8
9
10
11
13
14
15
16
17
18 Bρ(l ∈ N) :
return
19
(cid:2)
ρ(cid:2)<Rρ Nρ(cid:2) (l);
20
21 BA→A1A2
22
(l1, l ∈ N) :
i=0 NA1
return Σl1−1
(i)NA2
(l − i);
Algorithm 3: Computing Nρ, NA, Bρ and Bρ. Nρ and NA
use memoization tables Nρ and NA, respectively.
(cid:2)
(cid:2)
l
(cid:2)Σ
Complexity of the algorithms. Assume that (cid:17)X(cid:17) =
l. The ranking and unranking functions visit each of the
O(l|R|) parse-tree nodes of X exactly once. The complexity
of the computation at each node depends on the amount of
memoized information for Nρ(l), NA(l), Bρ(l) and Bρ(l1, l).
(cid:2), we let b(l) = O(l)
Considering numbers as large as
be the space needed to store such a number, a(l) = O(l)
be the time needed to add two such numbers, m(l) be the
time needed to multiply two such numbers, and d(l) be the
time needed to divide two such numbers. We assume a(l) ≤
m(l) ≤ d(l).
At one end, all these values are pre-computed for up to
a maximum value of (cid:17)X(cid:17) = l.
In this case, the tables
2|R|) entries, where each entry holds an integer
have O(l
whose representation may take O(l) bits, therefore the total
3|R|), where |R| is the number of rules
amount of space is O(l
m(l)|R|)
in the grammar. Filling in these tables takes O(l
time, where m(l) is the complexity of multiplying two num-
bers of O(l) bits.
If all tables are available, then ranking spends O(m(l))
time at each node, for a total of O(lm(l)|R|) time. Un-
ranking spends O(d(l)) time at each node, for a total of
O(ld(l)|R|), where d(l) is the complexity of dividing two
numbers of O(l) bits.
3
3.3 Ambiguity
Ambiguity is the key factor that determines the quality
and usefulness of relaxed ranking, and yet it was not fully
explored so far, although it was implicitly used in [14]. Am-
biguity is not speciﬁc to CFG ranking, but it is particularly
relevant, since it is impossible to decide whether a grammar
is ambiguous or not. A related line of work [9, 10, 17] re-
lates ambiguity to complexity of languages. In this section,
however, we deﬁne and analyze ambiguity in the context of
relaxed ranking, and explain why relaxed ranking is useful,
despite potential ineﬃciencies caused by ambiguous repre-
sentations.
The ambiguity-factor of a relaxed ranking using function
UnrankL : ZN → L is deﬁned to be the ratio β = N/|L|. We
note that if the relaxed ranking scheme is obtained with an
intermediate set I, then β = |I|/|L|. For instance, an am-
biguous grammar has multiple parse trees for some strings
and the ambiguity-factor measures how many more trees are
than strings (of a given size).
For most of ranking applications, it is easy to see that
relaxed ranking can be used as a swap-in replacement for
strict ranking, though possibly with slight degradation in
performance. The ambiguity-factor quantiﬁes this degrada-
tion.
For the following three application areas, consider a ﬁnite
language slice L, and a relaxed-ranking scheme RankL : L →
Z|I| based on an intermediate set I.
Compression. Relaxed ranking RankL(x) compresses a
word x ∈ L using γ = (cid:20)log2 |I|(cid:21) bits. Decompression fol-
lows from UnrankL(RankL(x)) = x. Compared to the ideal
case of ranking using α = (cid:20)log2 |L|(cid:21) bits, relaxed ranking has
an overhead of γ − α bits, or approximately (cid:20)log β(cid:21) where
β = |I|/|L| is the ambiguity-factor of the relaxed ranking
scheme. Thus the lower the ambiguity, the better the com-
pression.
Random member generation. To randomly and uni-
formly generate a string in a language L, one can pick a ran-
dom number n ∈ Z|L| and then compute x = unrank(n) us-
ing a strict ranking scheme. When we seek to replace unrank
with relaxed unranking, we need to avoid distribution biases
towards those elements x ∈ L for which x = UnrankL(n)
holds for multiple n ∈ Z|I|. This prevents highly ambiguous
elements from being generated more often than the others.
One can solve this using rejection sampling: repeatedly pick
n ∈ Z|I| until Rank(Unrank(n)) = n. The expected number
of trials is exactly the ambiguity-factor β = |I|/|L|.
Formatted encryption. Replacing ranking with relaxed
ranking does not work directly for formatted encryption.
Therefore, Listing 2 uses the technique of cycle walking. It
easy to see that the expected number of steps in the cycle
walk is upper bounded by the ambiguity-factor β = |I|/|L|,
because |Img(L)| = |L|. The other algorithmic adaptations
for relaxed ranking in [14] are similarly inﬂuenced by the
ambiguity-factor. Observe that there is no need to decide
whether the grammar is ambiguous, the algorithms are pro-
tected against this possibility.
If the grammar is not am-
biguous, then relaxed ranking is in fact strict ranking, and
the algorithmic overhead is minimal. For instance the cycle-
walk in Listing 2 stops after one step.
Our thesis (backed by experimental data) is that for com-