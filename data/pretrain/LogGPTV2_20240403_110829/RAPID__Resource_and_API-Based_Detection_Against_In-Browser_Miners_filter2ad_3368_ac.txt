99.99 (0.00)
F1
Recall
Classif.
resources
Precision
13.32 (0.98)
15.20 (3.74)
96.57 (1.06)
96.79 (1.34)
96.47 (1.07)
96.54 (1.15)
23.31 (1.54)
25.95 (5.48)
b. words html5
96.77 (0.70)
2grams html5
96.04 (0.86)
3grams html5
97.15 (1.04)
4grams html5
5grams html5
96.00 (0.76)
Table 3: Mean and Std. Dev. for Metrics (Mining)
93.89 (1.34)
93.52 (1.62)
96.98 (0.96)
95.33 (1.91)
97.84 (1.29)
95.48 (1.38)
Benign Class: Table 2 shows the mean and standard deviations
for precision, recall and the f1 score for each classier. From 10.000
benign sites, all q-gram classiers detect 99.99 of them, i.e., 99.99%
recall. Q-grams obtained a similar result regarding precision; specif-
ically, 2-, 4- and 5-grams produce one false positive for every 10.000
sites predicted as benign (99.99% precision). Also, the minimal stan-
dard deviation, below 0.01%, shows that we can be very condent
of these results. Moreover, in the case of 3-grams, which seem to
have a lower precision than the others, they have a higher stan-
dard deviation. This indicates that they could either have a value
between 99.97% and 99.99% as precision. For the bag of words and
resource-based approaches, the classiers still obtain very high
precision and accuracy, i.e., they only dier by 0.01% and 1.83% in
comparison to q-grams. Considering that the benign class is the
biggest class, results presented in Table 2 show that most of the
samples are correctly classied.
Mining Class: Figure 3 shows the mean and standard deviation
for precision and recall; further, Table 3 contains the same metrics
along with the f1 score. From the recall point of view, all classiers
perform over 93%. In other words, regardless of the classier type,
all the proposed detection mechanisms can detect at least 93% of all
mining sites. All classiers based on q-gram have recall between 95%
and 97%. From the precision point of view, q-grams perform really
well, e.g., from 100 sites labeled as mining by a q-gram classier
there are between three and four false positives. Now, we address
the relatively high deviation of all the precision, and recall values
for the mining class, in comparison with the low deviation shown
for the benign class. Further, we also address why there is a gap
between the group containing the resources- and bag of words-
based classiers in comparison to all q-grams. Especially, because
the high precision drop for the mining class was not observed when
analyzing precision and recall values for the complementary class,
i.e., benign sites, presented before.
In the presence of two complementary and signicantly unbal-
anced classes, false positives and false negatives have a higher
impact on precision and recall for the smaller class. For the sake of
the argument let us assume we have a mining class with ten sites
and a benign class with 10 million sites. Let us further assume we
have a classier detecting all ten mining sites, but also producing
ten false positives. In such a scenario, the classier would only have
50% precision for the mining class and 100% recall. Apparently, as
we only looked at the mining class, with ten sites, the previously
mentioned classier has very poor precision. However, we should
keep in mind that such classier is producing ten false positives for
318
Figure 3: Detection Performance for Mining Class
10 million sites, so it will generate one false positive the 0.0001% of
the times it is used.
The benign and mining classes whose results are presented in
Table 2 and Table 3 have unbalanced sizes, i.e., 285.858 and 656 sites,
respectively. So, although the real classes are not as unbalanced
as the previous example, a small amount of false positives or false
negatives has a considerably higher impact on precision and recall
for the mining class than for the benign class. Further, due to these
high variations, introduced by a few false positives, the smaller
class has higher deviations than the benign class.
Appendix A formalizes the intuition behind class sizes and how
this aects the precision and recall metrics for the smaller class. This
formulation is used to calculate a theoretical upper bound of false
positives and false negatives with respect to the whole dataset, i.e.,
285.919 sites. This reveals that although q-grams perform better than
the bag of words- or the resource-based approach, i.e., 0.008% of false
positives with respect to the whole dataset, the latter approaches are
also working well in practice. Moreover, we show that the resource-
based and the bag of words approaches produce 1.5% and 1.3% of
false positives (compared to the whole dataset) while producing
around 0.015% of false negatives.
Another takeaway from Table 2 and 3 is that q-grams perform
similarly. The f1 score, precision and recall show that even though
there seems to be a slight trend to increase performance towards 4-
grams and then decrease for 5-grams, the standard deviation values
RAPID: Resource and API-Based Detection Against In-Browser Miners
ACSAC ’18, December 3–7, 2018, San Juan, PR, USA
do not let us have certainty that this is the case. So, depending
on performance overhead introduced by the q-gram calculation, a
reasonable trade-o could be to use 2-grams instead of 5-grams:
more on this is explained in Section 4.2.1.
During all the executions on the evaluation set, all q-gram ap-
proaches only agged 22 sites as “false positives”. After further
investigation, we found that the classier was detecting new min-
ing proxies that were not part of the training set. For example,
moneone.ga was reported to blockers only after we labeled our
training set. Further, we found several domains with the same Mon-
ero URL structure, i.e., ending in /proxy or starting with ws01.
In some cases, we came across proxies that were not part of the
blacklists but are clearly associated with miners, i.e., wp-monero-
miner.de. This implies that the performance of the classiers
is even higher than the reported values. For completeness, the
sites along with the proxies as listed in Table 6 (Appendix B).
4.2 Impact on the Page Loading Time
We have produced a proof-of-concept implementation using Chrome
Debugging Tools to inject the necessary scripts to monitor the
resource-related APIs and apply the bag of words classier pre-
dicting whether a site is mining or not. However, to quantify the
overhead on the client side, we split our analysis on the runtime
performance on the client side into two parts. First, we measure the
overhead induced on page-loading time when APIs are monitored
and bag-of-words or q-grams are being calculated simultaneously.
Then, we evaluate in which cases can a classier be executed within
the browser and its execution time. We split the analysis into two
parts because the classication takes place after the full site has
been loaded; thus, it is not useful to attempt to measure the impact
of the classier on the page-loading time if it runs seconds after the
page is rendered. On the other hand, the constant monitoring and
calculation of q-grams can impact the page loading time; thus, it is
critical to measure its impact during the initial stages of rendering.
Further, our measurements are a worst-case scenario from the
runtime performance point of view because we are instrumenting
the browser APIs with JavaScript from a remote program (Crawler
Node). Therefore, an actual implementation of this mechanism
within the browser can only improve the user’s experience or deploy
more powerful mechanisms than our prototypical version.
Speed Index. Analyzing the performance impact on page
4.2.1
loading time for real Websites is a technically challenging problem.
Previously, there have been a number of attempts to measure a
particular event, e.g., window load, reecting how favorable the
user’s experience is on a Website; however, these metrics were not
successful because a single event is aected by a number of factors
and not necessarily correlates with the moment when content is
shown [11]. To overcome this, the speed index calculates a curve
of visual completeness (Figure 4) using video captures. In particu-
lar, the index is calculated based on the moment when a page has
nished loading. To have a bounded value evidencing the number
of pixels that took certain time to be rendered, the speed index cal-
culates the area over the curve (up to 100% of visual completeness),
i.e., the background of the image that is not covered by histogram
bars. Thus, a high speed index reects a site that is taking long to
render content. The main drawback of this index is that developers
319
Figure 4: Facebook’s SpeedIndex Visual Completeness Ren-
dered by Speedline [20]
may need to remove visual content that is modied periodically,
such as ads or banners, as the speed index would classify this as
content which has not yet been loaded (as it keeps changing).
The speed index is the better-known way to analyze how fast
sites are loading; consequently, we used it to analyze the impact
of the API instrumentation and calculation of the bag of words or
q-grams array. More to the point, we measured the speed index
using the same setup described in Section 3.1, with some dierences.
First, only one Crawling Node was executed per VM. Further, we
modied the browser instrumentation to calculate q-grams directly
in the browser instead of streaming the HTML events to the Crawler
Node. Also, instead of visiting a site only once, the speed index
evaluation executed 20 consecutive visits (each one with a new
container). For Each visit, we performed remote browser proling
and then used the Speedline library [20] in the Crawler node to
calculate the speed index over the proling data and store it in the
database.
As the speedindex can severely vary due to a number of factors,
we are mostly interested in the overhead induced by each API-based
approach instead of the actual speed index for each site. Therefore,
we executed ve kinds of automated visits: baseline, bag of words,
2-, 3- and 4-grams. The baseline experiment opened a Website and
then closed it without performing any API instrumentation. The
bag of words experiment instrumented all APIs and monitored
the number times each API call was performed. The 2-, 3-, and
4-grams experiments also instrumented the resource-related APIs
but additionally calculated the count for each q-gram observed
during the execution. Moreover, to have real comparable results, we
executed all the speed index calculations simultaneously (baseline,
bag of words and q-grams) between June the 8th and June the 10th
2018. This avoids any network delays or content updates on the
Websites that could aect our measurements if they were executed
at dierent times.
As discussed in Section 4.1, we need to evaluate the overhead
induced by 2-, 3, and 4-grams. Depending on the performance
overhead, a trade-o may be required to save page-rendering time.
Notwithstanding, it is clearly not a good idea to use 5-grams because
we already have an indicator that its detection performance may
drop, and in any case, 5-grams require more resources than the
smaller q-grams.
When performing the speed index measurements, we encoun-
tered a problem with specic sites whose speed index varied too
much across the 20 visits. We already suspected this could relate
to dynamic content; withal, we decided to investigate further. Fig-
ure 5 shows a histogram for the standard deviations (calculated
based on 20 subsequent visits per site) for the Alexa top 100. This
ACSAC ’18, December 3–7, 2018, San Juan, PR, USA
Juan D. Parra Rodriguez and Joachim Posegga
Approach
bag of words
2-grams
3-grams
4-grams
9.258832 %
24.943266 %
38.311742 %
39.164961 %
Overhead Mean Overhead Std. Dev
10.167985 %
15.672606 %
26.489506 %
24.461527 %
Table 4: Overall Overhead in Comparison with the Baseline
vector calculation approaches using the baseline as reference for
the dierent instrumentation approaches. It is clear that q-grams
can have an overhead ranging from 20 to 40% on the speed index.
4.2.2 Classifier Execution. To evaluate to which extent can be clas-
sication mechanisms based on JavaScript features be executed
directly in the browser today, we used a libSVM port to WebAssem-
bly [24] to attempt to execute an SVM. Unfortunately, we found out
that only the bag of words SVM could be executed in the browser.
The primary constraint for the q-grams was that WebAssembly
runs out of memory when trying to load the SVM. The main dif-
ference between the implementation of the libSVM WebAssembly
library and Scikit learn is that Scikit lean provides the “triplet” rep-
resentation for sparse data, and the WebAssembly wrapper requires
a regular matrix with the samples.
Withal, we executed a micro benchmark in JavaScript measuring
the time in milliseconds required to do 100.000 classications indi-
vidually in a loop using a Lenovo TS470S with 16 GB of RAM and
an Intel Core i7-7500U CPU @ 2.70GHz. We decided to perform
many classications between the time measurements to decrease
possible measurement error and because we realized that some-
times classications took less than one millisecond. After executing
the micro-benchmark described above ten times, we obtained a
mean of 181 milliseconds, for the 100.000 predictions and a stan-
dard deviation of 6.54 milliseconds. So, we can say that in average
each prediction takes 0.01 milliseconds.
5 RELATED WORK
Although there has been an initial study of the characteristics, e.g.,
timeline of events and adoption of particular miners, of in-browser
crypto-mining [18], we are the rst describing a detection mecha-
nism that does not rely on blacklists and therefore achieves better
detection performance. Also, this is the rst quantitative analy-
sis comparing resource and API-based detection of crypto-mining.
However, we are not the rst studying malicious JavaScript code
exploiting the browser. In fact, we have leveraged similar concepts
than those described by IceShield [19], Cujo [38] and HoneyMon-
key [48]. In spite of extensive research eorts to analyze JavaScript
malware using honeypots, e.g., PhoneyC [27] or WebPatrol [4], we
focus on systems using real browsers for their measurements.
Provos et al. [35] demonstrated in 2008 that 1.3% of Google
searches had a drive-by download attack. This created a wave of
oine detection (according to the term introduced in Section 2) ap-
proaches. However, in 2005 malicious JavaScript code was already
being automatically analyzed by Wang et al. when they introduced
HoneyMonkey [48]: an automated crawling framework to detect
malware downloads through the browser. HoneyMonkey simulated
human behavior during visits to malicious sites, under Windows
Figure 5: Histogram of Std. Dev. Speed Indexes for Alexa Top
100 Sites
gave us a clear experimental threshold (deviation of 1000) to keep
most of the sites who had a deviation within the most signicant
part of the population. From 88 sites that we were able to visit 20
times successfully from the top 1006, 61 sites had a smaller standard
deviation than the threshold while 21 sites were above.
Interestingly, we found that although in several cases the speed
index deviation happened due to visual elements that kept changing,
e.g., banners or auto-playing videos, there seemed to be more sites
from Chinese domains or companies in the list. According to an
online forum discussing the topic, there seem to be several factors
inuencing network conditions when Chinese sites are accessed
from outside of China. Apparently, China uses three locations for
the Great Chinese Firewall, charges signicant amounts of money
for abroad connections and does not have enough submarine cable
connections in proportion to their population [37, 45]. If these
arguments are indeed true, this would explain the variations on the
speed index due to signicant latencies and unpredictable network
conditions. For the full list of sites see Table 9 in Appendix B.
Due to space reasons, we report the mean speed index (based
on 20 visits for each site) for the top 20 Alexa sites in Figure 6, but
we still present the full Alexa top 100 list in Figure 7, as well as the
behavior of sites excluded from the calculations in Figure 8.
Figure 6 shows the mean speed index and their standard devia-
tions for the top 20 sites within the deviation threshold described
above. Overall, visits yield a higher index as the computation in-
creases, e.g., baseline requires less computation than to bag of
words and therefore yields a lower speed index. Notwithstand-
ing, although there is a clear trend for more complex monitoring
schemes to increase the speed index, the overhead induced is not