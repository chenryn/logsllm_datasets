views. Figure 1 shows that the percentage of restored cases
grows to about 50% when there are 4 QI attributes.
Figure 2 gives the same information as Figure 1 for the
case when we use up to two (rather than one) additional
views to restore privacy. For this case, Figure 2 shows that
the percentage of restored cases grows to about 60% when
there are 4 QI attributes.
Note that when the privacy was not restored, we did not
know if the restoration was indeed not feasible. It may be
feasible, if we disclose more ﬂexible additional views (not
just the reﬁnements of the previously disclosed ones) and/or
disclose a larger number of additional views (not just 1 or 2).
Clearly, the percentage of restored cases out of all restorable
cases is higher or equal to that in the diagram. Note that
while the study is preliminary, it indicates that the per-
centage of restored cases is signiﬁcant, even with a single
additional disclosed view.
6. DISCUSSION
In this section we discuss and provide more justiﬁcation
for the assumptions that are made for the adversary model.
Recall that the deﬁnition of the γ-Private is based on the
assumption that the adversary will try to compromise an
individual’s privacy contained in a private table through a
random guessing game. What is hidden is that we assume
the adversary will collect all of the disclosed information,
including Public Knowledge disclosure and generalized dis-
closures of the private table. The following two assumptions
have not been addressed: (1) the adversary may acquire only
part of the published information; and (2) the adversary may
acquire even more information about the privacy restoring
process. In this section, we discuss, in two diﬀerent cases,
how the process of privacy restoring can be aﬀected by (2).
Intention of Privacy Restoring: The adversary may
get to know our intention of trying to restore a compromised
privacy. In this case, to safely restore a compromised pri-
vacy, in a given micro-data disclosure problem, whether to
perform a privacy restoring should not depend on the orig-
inal baseT . Formally, the decision should be “simulatable”
to the adversary. To explain the idea, consider a modiﬁed
example of medical information disclosure that has been dis-
44Figure 1: Privacy Restoring by a Single View
Figure 2: Privacy Restoring by up to 2 Views
cussed in Section 1. Assume that we have a diﬀerent baseT
shown in Table 9, but the Public Knowledge disclosure (Ta-
ble 2) and the two generalized disclosures (Tables 3(A, B))
remain the same. Note that, in this example, the adversary
may be misled by the disclosed information and ﬁnd out
that there is a high probability for Donald to have SARS.
One may think that it could be a good choice to stay with
such a status. However, this is not a safe decision, because if
the adversary acquires the knowledge that we have been try-
ing to restore a compromised privacy, the fact that we have
done nothing in this example will immediately reveal the
fact that Donald has V iral Inf ection. This problem can
be solved simply by making the privacy restoring decision
independent from the original baseT .
Name Sex Age Employer
ABC, Inc.
Alan
M
ABC, Inc.
Bob
M
Clark
ABC, Inc.
M
Heart Disease
ABC, Inc. Viral Infection
Donald M
ABC, Inc.
F
Ellen
ABC, Inc.
F
Fen
F
ABC, Inc.
23
24
25
26
27
28
28
SARS
SARS
Garcia
Condition
SARS
SARS
Flu
Table 9: Modiﬁed Patient Information Table
Preferences in Disclosure Selection: The adversary
may obtain the following two facts: (1)when we are trying
to restore a compromised privacy, we may have some pref-
erences in the selection of additional generalized disclosures;
and (2) among all the released generalized disclosures, some
are used to restore a compromised privacy.
In this case,
the adversary may also have an opportunity to compromise
an individual’s privacy immediately. To illustrate, consider
same example shown in Table 9. To restore the compromised
privacy for Donald as discussed above, we may disclose two
additional generalized disclosures: one is the same as shown
in Table 5(B) and the other one is shown in Table 10(A).
This works ﬁne under the assumptions discussed in previous
sections. However, if the adversary discovers that we are try-
ing to minimize generalization for the additional disclosure.
The fact that Table 10(B) is not disclosed as the additional
generalized disclosure will immediately reveal that Clark is
highly probable to have Heart Disease. This is true because
if Donald has V iralInf ection and Clark has SARS, Ta-
ble 10(B) should be disclosed instead of Table 10(A). There-
fore, under such strong assumptions of the adversary model,
we should be very careful to select an additional generalized
disclosure. A complete solution for this case remains future
work.
Age
Condition
24 or 26 Viral Infection
24 or 26
SARS
Age
Condition
25 ∼ 26 Viral Infection
25 ∼ 26
SARS
(A)
(B)
Table 10: Additional Disclosure Selection
7. RELATED WORK
The initial works [1, 4, 15, 20, 21] were concerned with
conducting a data census while protecting the privacy of
sensitive information in disclosed tables. Two approaches,
data swapping ([14, 28, 33]) and data suppression ([22]) were
suggested to protect data but could not quantify how well
the data is protected. The work [12] gave a formal analysis
of the information disclosure in data exchange. The work
[30] showed that publishing data sets even without identi-
fying attributes can cause privacy breaches and suggested
a new notion of privacy called k-anonymity. Achieving k-
anonymity with the best data utility was proved to be NP-
hard [26]. A similar measure, called blending in a crowd,
was proposed by [32]. The work [34] proposed a new gen-
eralization framework based on the concept of “personalized
anonymity.” In addition, many works, e.g., [11, 29, 30, 24,
31, 19], proposed eﬃcient algorithms for k-anonymity. The
work [3] discussed deﬁciency of k-anonymity as a measure of
privacy and proposed an alternative property of l-diversity
to ensure privacy protection in the micro-data disclosure.
The work ([35]) focused on the problem of potential infor-
mation disclosure when optimization exists.
It would be
interesting to see how our technique can be extended using
their solutions to adapt to more ﬂexible assumptions of the
adversary’s knowledge.
In statistical databases ([27, 15, 17]), a typical problem is
how to “safely” answer aggregation queries so that sensitive
data on individuals would not be disclosed. The works [10,
8] addressed this problem by auditing and deciding whether
a new query can be answered based on the database state
and on previously answered queries. The works [10, 16, 18]
considered the same problem in more speciﬁc settings of of-
ﬂine auditing and online auditing, respectively. The work
[18] considered the knowledge contained in the decision al-
gorithm itself.
8. CONCLUSION
In this paper, we studied the problem of restoring com-
promised privacy for micro-data disclosure with multiple dis-
closed views by disclosing more views, in terms of both the-
oretical foundation and practical solutions. Many research
questions remain open. One is a more comprehensive study
on heuristic algorithms to restore privacy in terms of their
45complexity and eﬃcacy. Another direction is the notion of
optimality, i.e., given multiple ways to restore privacy how
do we decide on the best. Also important is extending our
results to additional, possibly more relaxed, adversary mod-
els.
9. ACKNOWLEDGMENTS
This material is based upon work supported by the Na-
tional Science Foundation under grants CT-20013A, CT-
0716567, CT-0716323, and CT-0627493; by the Air Force
Oﬃce of Scientiﬁc Research under grants FA9550-07-1-0527,
FA9550-09-1-0421, and FA9550-08-1-0157; and by the Army
Research Oﬃce under the grant W911NF-09-01-0352. We
also thank the anonymous reviewers for their comments and
kind suggestions.
10. REFERENCES
[1] A.Dobra and S.E.Feinberg. Bounding entries in
multi-way contingency tables given a set of marginal
totals. In Foundations of Statistical Inference:
Proceedings of the Shoresh Conference 2000. Springer
Verlag, 2003.
[2] R. Agrawal and R. Srikant. Privacy-preserving data
mining. In Proc. of the ACM SIGMOD Conference on
Management of Data, pages 439–450, May 2000.
[3] A.Machanavajjhala, J.Gehrke, D.Kifer, and
M.Venkitasubramaniam. l-diversity: Privacy beyond
k-anonymity. In Proceedings of the 22nd IEEE
International Conference on Data Engineering (ICDE
2006), 2006.
[4] A.Slavkovic and S.E.Feinberg. Bounds for cell entries
in two-way tables given conditional relative
frequencies. Privacy in Statistical Databases, 2004.
[5] A. Asuncion and D. Newman. UCI machine learning
repository (Data Provider: Andras.Janosi, hungarian
institute of cardiology; William.Steinbrunn, university
hospital, zurich, switzerland; Matthias.Pﬁsterer,
university hospital, basel, switzerland;
Robert.Detrano, v.a. medical center, long beach and
cleveland clinic foundation.), 2007.
[6] M. Bellare and P. Rogaway. Random oracles are
practical: A paradigm for designing eﬃcient protocols.
In CCS, 1995.
[7] A. Bertoni, M. Goldwurm, and M. Santini. Random
generation and approximate counting of ambiguously
described combinatorial structures. In STACS, pages
567–580, 2000.
[8] D.P.Dobkin, A.K.Jones, and R.J.Lipton. Secure
databases: Protection against user inﬂuence. ACM:
Transactions on Database Systems (TODS),
4(1):76–96, 1979.
[9] M. Dyer, R. Kannan, and J. Mount. Sampling
contingency tables. In CCC, pages 487–506, 1997.
[10] F.Chin. Security problems on inference control for
sum, max, and min queries. J.ACM, 33(3):451–464,
1986.
[11] G.Aggarwal, T.Feder, K.Kenthapadi, R.Motwani,
R.Panigrahy, D.Thomas, and A.Zhu. k-anonymity:
Algorithms and hardness. Technical report, Stanford
University, 2004.
[12] G.Miklau and D.Suciu. A formal analysis of
information disclosure in data exchange. In SIGMOD,
2004.
[13] P. W. P. J. Grefen and R. A. d. By. A multi-set
extended relational algebra - a formal approach to a
practical issue. In Proceedings of the Tenth
International Conference on Data Engineering, pages
80–88, 1994.
[14] G.T.Duncan and S.E.Feinberg. Obtaining information
while preserving privacy: A markov perturbation
method for tabular data. In Joint Statistical Meetings.
Anaheim,CA, 1997.
[15] I.P.Fellegi. On the question of statistical
conﬁdentiality. Journal of the American Statistical
Association, 67(337):7–18, 1993.
[16] J.Kleinberg, C.Papadimitriou, and P.Raghavan.
Auditing boolean attributes. In PODS, 2000.
[17] J.Schlorer. Identiﬁcation and retrieval of personal
records from a statistical bank. In Methods Info. Med.,
1975.
[18] K.Kenthapadi, N.Mishra, and K.Nissim. Simulatable
auditing. In PODS, 2005.
[19] K.LeFevre, D.DeWitt, and R.Ramakrishnan.
Incognito: Eﬃcient fulldomain k-anonymity. In
SIGMOD, 2005.
[20] L.H.Cox. Solving conﬁdentiality protection problems
in tabulations using network optimization: A network
model for cell suppression in the u.s. economic
censuses. In Proceedings of the Internatinal Seminar
on Statistical Conﬁdentiality, pages 229–245.
International Statistical Institute, Dublin, 1982.
[21] L.H.Cox. New results in disclosure avoidance for
tabulations. In International Statistical Institute
Proceedings of the 46th Session, pages 83–84. Tokyo,
1987.
[22] L.H.Cox. Suppression, methodology and statistical
disclosure control. Journal of the American Statistical
Association, 90:1453–1462, 1995.
[23] N. Li and T. Li. t-closeness: Privacy beyond
k-anonymity and l-diversity. In ICDE, 2007.
[24] L.Sweeney. k-anonymity: a model for protecting
privacy. International Journal on Uncertainty,
Fuzziness and Knowledge-based Systems,
10(5):557–570, 2002.
[25] M. Mether. The history of the central limit theorem.
Sovelletun Matematiikan erikoisty¨ot, Mat-2(108),
2003.
[26] A. Meyerson and R. Williams. On the complexity of
optimal k-anonymity. In ACM Symposium on
Principles of Database Systems (PODS), 2004.
[27] N.R.Adam and J.C.Wortmann. Security-control
methods for statistical databases: A comparative
study. ACM Comput. Surv., 21(4):515–556, 1989.
[28] P.Diaconis and B.Sturmfels. Algebraic algorithms for
sampling from conditional distributions. Annals of
Statistics, 1:363–397, 1998.
[29] P.Samarati. Protecting respondents’ identities in
microdata release. In IEEE Transactions on
Knowledge and Data Engineering, pages 1010–1027,
2001.
[30] P.Samarati and L.Sweeney. Protecting privacy when
disclosing information: k-anonymity and its
enforcement through generalization and suppression.
46Technical report, CMU, SRI, 1998.
[31] R.J.Bayardo and R.Agrawal. Data privacy through
optimal k-anonymization. In ICDE-2005, 2005.
[32] S.Chawla, C.Dwork, F.McSherry, A.Smith, and
H.Wee. Toward privacy in public databases. In Theory
of Cryptography Conference, 2005.
[33] T.Dalenius and S.Reiss. Data swapping: A technique
for disclosure control. Journal of Statistical Planning
and Inference, 6:73–85, 1982.
[34] X.Xiao and Y.Tao. Personalized privacy preservation.
In SIGMOD, 2006.
[35] L. Zhang, S. Jajodia, and A. Brodsky. Information
disclosure under realistic assumptions: Privacy versus
optimality. In ACM Conference on Computer and
Communications Security (CCS) 2007.
47