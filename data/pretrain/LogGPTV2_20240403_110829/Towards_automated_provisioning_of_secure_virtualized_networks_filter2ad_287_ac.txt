Requirements
TVD Policy
Model
Security 
Policy
Mechanisms
Required
Capability
Model
Per-host 
Extensions
Deployment Planning 
& Management
Global 
Configuration
Figure 4: Steps in Auto-Deployment of TVDs.
Open connection between two TVDs means that any two
machines in either TVD can communicate freely. In such a
case, the ﬁrewalls at both TVDs would have virtual network
cards for the peer domain and simply serve as bridges be-
tween the domains. For example, diﬀerent zones in a given
enterprise may form diﬀerent TVDs, but may communicate
freely. As another example, two TVDs may have diﬀerent
membership requirements, but may have an open connec-
tion between their elements. Open connection between two
domains may be implemented using an unlimited number
of virtual routers.
In a physical machine that is hosting
two VMs belonging to diﬀerent TVDs with an open con-
nection, the corresponding vSwitches may be directly con-
nected. Communication between two TVDs, while open,
may be subject to some constraints and monitoring. For ex-
ample, a TVD master may permit the creation of only a few
virtual routers on certain high-assurance physical machines
for information ﬂow between the TVD and another TVD
with which the former has an open connection.
A closed connection between two TVDs can be seen as a
special case of a controlled connection in which the ﬁrewall
does not have virtual network card for the peer TVD. In
addition to the ﬁrewall ﬁltering rules, the absence of the
card will prevent any communication with the peer TVD.
4.5 Intra-TVD Management
Intra-TVD management is concerned with TVD member-
ship, communication within a TVD, and the network fabric
(i.e., internal topology) of a TVD. Intra-TVD policies spec-
ify the membership requirements for each TVD, i.e., the
conditions under which a VM is allowed to join the TVD.
At a physical machine hosting the VM, the requirements
are enforced by the machine’s TVD proxy in collaboration
with networking elements (such as vSwitches) based on the
policies given to the TVD proxy by the TVD master. We
describe TVD admission control in detail in Section 5.
A VLAN can be part of at most one TVD. For complete-
ness, each VLAN that is not explicitly part of some TVD
is assumed to be a member of a dummy TVD, T V DΔ. Al-
though a VLAN that is part of T V DΔ may employ its own
protection mechanisms, the TVD itself does not enforce any
ﬂow control policy and has open or unrestricted connections
with other TVDs. Thus, in the information ﬂow control ma-
trix representation, the entries for policies, PΔα and PαΔ,
would all be 1 for any T V Dα.
A VM that is connected to a particular VLAN segment au-
Dom0
DomU
EtherIP VLAN
TVD-specific
Modules
Policy 
Engine
…..
2
TVD proxy
1
vNICs
…
TVD Master
0
1
0 TVD object ← create TVD (TVD requirements, policy model)
create TVD Proxy (Master URL)
1
2 connect VM to TVD (TVD object)
Figure 5: Steps in Populating a TVD.
tomatically inherits the segment’s TVD membership. The
VM gets connected to the VLAN segment only after the
TVD proxy on the VM’s physical machine has checked
whether the VM satisﬁes the TVD membership require-
ments. Once it has become a member, the VM can exchange
information freely with all other VMs in the same VLAN
segment and TVD (intra-TVD communication is typically
open or unrestricted). As mentioned before, a VM can be
connected to more than one VLAN (and hence, be a mem-
ber of more than one TVD) through a separate vNIC for
each VLAN.
A VM can become a TVD member either in an active
or in a passive fashion. A VM can be passively assigned
a TVD membership at the time of its creation by specify-
ing in the VM’s start-up conﬁguration ﬁles which VLAN(s)
the VM should be connected to. Alternatively, a VM can
actively request TVD membership at a later stage through
the corresponding TVD proxy interface.
TVD membership requirements may be checked and en-
forced on a one-time or on a continual basis. Membership
can be a one-time operation in which the requirements are
checked once and for all, and thereafter, the VM holds the
TVD membership for the duration of its life-cycle. Alterna-
tively, membership requirements can be re-evaluated in an
online fashion. The TVD proxy may regularly check whether
a VM satisﬁes the requirements. A session-based scheme
may be employed in which a VM is allowed open communi-
cation with other TVD members only until the next check
(i.e., end of the session).
5. AUTO-DEPLOYMENT OF TVDS
Figure 4 shows the steps involved in automatic deploy-
ment of secure virtual infrastructures as TVD conﬁgura-
tions. Figure 5 shows the steps involved in the establishment
and management of a single TVD.
First, the virtual infrastructure topology must be decom-
posed into constituent TVDs, along with associated security
requirements and policy model. Second, a capability model
of the physical infrastructure must be developed. Capability
modeling is essentially the step of taking stock of existing
mechanisms that can be directly used to satisfy the TVD
security requirements. In this paper, we consider the case
where both steps are done manually in an oﬄine manner;
future extensions will focus on automating them and on dy-
Table 1: Examples of Security Properties used in
Capability Modeling.
Property Description
TVD Isola-
tion
Network
Flow control policies in place for a TVD.
The actual topology of a virtual network in a
physical machine.
Security policies for the network, such as ﬁre-
wall rules and isolation rules stating which
subnets can be connected.
Policies for storage security, such as whether
the disks are encrypted and what VMs have
permission to mount a particular disk.
The life-cycle protection mechanisms of the
individual VMs, e.g., pre-conditions for exe-
cution of a VM.
Binary integrity of the hypervisor.
The roles and associated users of a machine,
e.g., who can assume the role of administrator
of the TVD master.
Network
Policy
Storage Pol-
icy
Virtual Ma-
chines
Hypervisor
Users
namically changing the capability models based on actual
changes to the capabilities.
5.1 Capability Modeling of the Physical Infra-
structure
Capability modeling of the physical infrastructure con-
siders both functional and security capabilities. The func-
tional capabilities of a host may be modeled using a func-
tion C : H → {V LAN, Ethernet, IP}, to describe whether
a host has VLAN, Ethernet, or IP support. Modeling of se-
curity capabilities includes two orthogonal aspects: the set
of security properties and the assurance that these proper-
ties are actually provided. Table 1 lists some examples of
security properties and Table 2 gives examples of the types
of evidence that can be used to support security property
claims.
5.2 TVD Establishment and Population
When the set of TVDs have been identiﬁed, the next step
is to actually establish them. The initial step for establishing
a TVD is to create the TVD master (step 0 in Figure 5)
and initialize the master with the TVD requirements (as
formalized above) and the policy model. The step involves
the derivation of a comprehensive set of TVD policies, which
are maintained at the TVD master. The output of the step
is a TVD object that contains the TVD’s unique identiﬁer,
i.e., the TVD master’s URL.
Once the TVD master has been initialized, the TVD is
ready for being populated with member entities, such as
VMs. A VM becomes admitted to a TVD after the suc-
cessful completion of a multi-step protocol (steps 1 and 2 in
Figure 5).
1. A local representative of the TVD, called TVD proxy,
is created and initialized with the URL of the TVD
master.
2. The TVD proxy sets up a secure, authenticated chan-
nel with the TVD master using standard techniques.
3. The TVD proxy indicates the security and functional
capabilities of the physical machine. Using the capa-
bility model, the TVD master determines which addi-
tional mechanisms must be provided at the level of the
virtual infrastructure. For example, if a TVD require-
ments speciﬁcation includes isolation and the physical
infrastructure does not have that capability, then spe-
cial (VLAN tagging or EtherIP) modules must be in-
stantiated within the Dom0 of physical machines host-
ing VMs that are part of the TVD.
4. The TVD master then replies to the TVD proxy with
the TVD security policy (such as ﬂow control poli-
cies between VMs belonging to diﬀerent TVDs hosted
on the same physical machine) and additional mecha-
nisms that must be provided at the virtualization level.
5. The TVD proxy then instantiates and conﬁgures the
required TVD-speciﬁc modules (e.g., vSwitch, VLAN
tagging module, encapsulation module, VPN module,
policy engine, etc.) according to the TVD policy. Af-
ter this step, the physical machine is ready to host a
VM belonging to the TVD.
6. As shown by step 2 in Figure 5, a command is is-
sued at the VM to join the TVD (active membership
model4). This results in the VM contacting the TVD
proxy. Based on the TVD security policies, the TVD
proxy may carry out an assurance assessment of the
VM (e.g., whether the VM has all required software
properly conﬁgured). Once the required veriﬁcation of
the VM is successful, the TVD proxy may connect the
vNICs of the VM to the appropriate TVD vSwitch. At
this point, the VM is part of the TVD.
6.
IMPLEMENTATION IN XEN
In this section, we describe a Xen-based [4] prototype im-
plementation of our secure virtual networking framework.
Figure 6 shows the implementation of two TVDs, T V Dα
and T V Dβ. The policy engine, also shown in the ﬁgure, im-
plements the policies corresponding to the TVDs speciﬁed
in the information ﬂow control matrix of Figure 1, i.e., open
connection within each TVD and closed connection between
T V Dα and T V Dβ.
Our implementation is based on Xen-unstable 3.0.4, a
VMM for the IA32 platform, with the VMs running the
Linux 2.6.18 operating system. Our networking extensions
are implemented as kernel modules in Dom0, which also acts
as driver domain for the physical NIC(s) of each physical
host. A driver domain is special in the sense that it has
access to portions of the host’s physical hardware, such as a
physical NIC.
The virtual network interface organization of Xen splits a
NIC driver into two parts: a front-end driver and a back-
end driver. A front-end driver is a special NIC driver that
resides within the kernel of the guest OS. It is responsible
for allocating a network device within the guest kernel (eth0
in Dom1 and Dom2 of hosts A and B, shown in Figure 6).
The guest kernel layers its IP stack on top of that device as
if it had a real Ethernet device driver to talk to. The back-
end portion of the network driver resides within the kernel
of a separate driver domain (Dom0 in our implementation)
4Alternatively,
if the passive membership model (Sec-
tion 4.5) is used, the command to join the TVD can be
issued by the VM manager component that instantiates the
VM.
Table 2: Assurance for Past, Present, and Future
States used in Capability Modeling.
Mutable Log
Immutable Logs
Past State
Trust
Description
A user believes that an entity has certain
security properties.
The entity provides
log-ﬁle evidence
(e.g., audits) that indicates that the plat-
form provides certain properties.
The entity has immutable logging sys-
tems (e.g., a TPM-quote [22]) for provid-
ing evidence. Since the log cannot modi-
ﬁed by the entity itself, the resulting as-
surance is stronger than when mutable
logs are used.
Present State Description
Evaluations
Evaluation of a given state, e.g., Com-
mon Criteria evaluations [14].
Introspection of a system by executing
security tests, e.g., virus scanner.
Introspection
Future State Description
Policies
By providing policies and evidence of
their enforcement, a system can justify
claims about its future behavior.
e.g.,
DRM policies and VM life-cycle protec-
tion policy.
By guaranteeing regular audits, organi-
zations can claim that certain policies
will be enforced in the future.
Audit
and creates a network device within the driver domain for
every front-end device in a guest domain that gets created.
Figure 6 shows two of these back-end devices, vif1.0 and
vif2.0, in each of the two hosts A and B. These back-end
devices correspond to the eth0 devices in Dom1 and Dom2,
respectively, in each host.
Conceptually, the pair of front-end and back-end devices
behaves as follows. Packets sent out by the network stack
running on top of the front-end network device in the guest
domain appear as packets received by the back-end network
device in the driver domain. Similarly, packets sent out by
the back-end network-device by the driver domain appear to
the network stack running within a guest domain as packets
received by the front-end network device.
In its standard
conﬁguration, Xen is conﬁgured to simply bridge the driver
domain back-end devices onto the real physical NIC. By this
mechanism, packets generated by a guest domain ﬁnd their
way onto the physical network and packets on the physical
network can be received by the guest domain.
The Xen conﬁguration ﬁle is used to specify the particular
vSwitch and the particular port in the vSwitch to which a
Xen back-end device is attached. We use additional scripts
to specify whether a particular vSwitch should use one or
both of VLAN tagging and encapsulation mechanisms for
isolating separate virtual networks.
The vSwitches for T V Dα and T V Dβ are each implemented
in a distributed fashion (i.e., spread across hosts A and B) by
a kernel module in Dom0, which maintains a table mapping
virtual network devices to ports on a particular vSwitch. Es-
sentially, the kernel module implements EtherIP processing
for packets coming out of and destined for the VMs. Each
virtual switch (and hence VLAN segment) has a number
identiﬁer associated with it. The Ethernet packets sent by
Host A
Host B
Dom 0
Dom 1
Dom 2
Dom 0
Dom 1
Dom 2
Policy
Engine
vSwitches
VLAN 
tagging
module
encap.
module
α
β
Policy
Engine
vSwitches
VLAN 
tagging
module
encap.
module
α
β
physical
NIC
back-end 
devices
front-end devices
physical
NIC
back-end 
devices
front-end devices
eth0.α
eth0.β
eth0
vif1.0
vif2.0
eth0
eth0
eth0.α
eth0.β
eth0
vif1.0
vif2.0
eth0
eth0