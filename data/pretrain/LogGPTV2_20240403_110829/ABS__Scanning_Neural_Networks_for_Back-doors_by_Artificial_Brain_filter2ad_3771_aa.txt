title:ABS: Scanning Neural Networks for Back-doors by Artificial Brain
Stimulation
author:Yingqi Liu and
Wen-Chuan Lee and
Guanhong Tao and
Shiqing Ma and
Yousra Aafer and
Xiangyu Zhang
ABS: Scanning Neural Networks for Back-doors by
Artificial Brain Stimulation
Yingqi Liu
PI:EMAIL
Purdue University
Shiqing Ma
PI:EMAIL
Rutgers University
Wen-Chuan Lee
PI:EMAIL
Purdue University
Yousra Aafer
PI:EMAIL
Purdue University
Guanhong Tao
PI:EMAIL
Purdue University
Xiangyu Zhang
PI:EMAIL
Purdue University
ABSTRACT
This paper presents a technique to scan neural network based AI
models to determine if they are trojaned. Pre-trained AI models
may contain back-doors that are injected through training or by
transforming inner neuron weights. These trojaned models operate
normally when regular inputs are provided, and mis-classify to a
specific output label when the input is stamped with some special
pattern called trojan trigger. We develop a novel technique that
analyzes inner neuron behaviors by determining how output acti-
vations change when we introduce different levels of stimulation to
a neuron. The neurons that substantially elevate the activation of a
particular output label regardless of the provided input is considered
potentially compromised. Trojan trigger is then reverse-engineered
through an optimization procedure using the stimulation analysis
results, to confirm that a neuron is truly compromised. We evaluate
our system ABS on 177 trojaned models that are trojaned with vari-
ous attack methods that target both the input space and the feature
space, and have various trojan trigger sizes and shapes, together
with 144 benign models that are trained with different data and
initial weight values. These models belong to 7 different model
structures and 6 different datasets, including some complex ones
such as ImageNet, VGG-Face and ResNet110. Our results show that
ABS is highly effective, can achieve over 90% detection rate for most
cases (and many 100%), when only one input sample is provided for
each output label. It substantially out-performs the state-of-the-art
technique Neural Cleanse that requires a lot of input samples and
small trojan triggers to achieve good performance.
KEYWORDS
Deep learning system; AI trojan attacks; Artificial brain stimulation
ACM Reference Format:
Yingqi Liu, Wen-Chuan Lee, Guanhong Tao, Shiqing Ma, Yousra Aafer,
and Xiangyu Zhang. 2019. ABS: Scanning Neural Networks for Back-doors
by Artificial Brain Stimulation. In 2019 ACM SIGSAC Conference on Computer
& Communications Security (CCS ’19), November 11–15, 2019, London, United
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CCS ’19, November 11–15, 2019, London, United Kingdom
© 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-6747-9/19/11...$15.00
https://doi.org/10.1145/3319535.3363216
Kingdom. ACM, New York, NY, USA, 18 pages. https://doi.org/10.1145/
3319535.3363216
1 INTRODUCTION
Neural networks based artificial intelligence models are widely
used in many applications, such as face recognition [47], object
detection [49] and autonomous driving [14], demonstrating their
advantages over traditional computing methodologies. More and
more people tend to believe that the applications of AI models
in all aspects of life is around the corner [7, 8]. With increasing
complexity and functionalities, training such models entails enor-
mous efforts in collecting training data and optimizing performance.
Therefore, pre-trained models are becoming highly valuable arti-
facts that vendors (e.g., Google) and developers distribute, share,
reuse, and even sell for profit. For example, thousands of pre-trained
models are being published and shared on the Caffe model zoo [6],
the ONNX zoo [10], and the BigML model market [4], just like tradi-
tional software artifacts being shared on Github. These models may
be trained by reputable vendors, institutes, and even individuals.
In the lengthy history of software distribution and reuse, we have
seen a permanent battle to expose possible malicious behaviors and
back-doors in published software that are usually disguised by the
highly attractive functional features of the software. Similarly, AI
models can be trojaned. Recent research has shown that by con-
taminating training data, back-doors can be planted at the training
time [25]; by hijacking inner neurons and limited re-training with
crafted inputs, pre-trained models can be transformed to install
secret back-doors [38]. These trojaned models behave normally
when benign inputs are provided, some even perform better than
the original models (to be more attractive). However, by stamping
a benign input with some pattern (called trojan trigger), attack-
ers can induce model mis-classification (e.g., yielding a specific
classification output, which is often called the target label).
Our goal is to scan a given AI model to determine if it contains
any secret back-door. The solution ought to be efficient as it may
need to scan a large set of models; it ought to be effective, with-
out assuming the access to training data or any information about
trojan trigger, but rather just the pre-trained model and maybe a
small set of benign inputs. More about our attack model can be
found in Section 3.1. There are existing techniques that defend AI
model trojan attacks [23, 37, 39, 59]. However, these techniques
have various limitations, such as only detecting the attack when
an input with the trigger is provided to the model instead of de-
termining if a model is trojaned without the input trigger; causing
substantial model accuracy degradation; requiring a large number
of input samples; and difficulties in dealing with attacks in the fea-
ture space instead of the input space. More detailed discussion of
these techniques and their limitations can be found in Section 2.2.
We observe the essence of model trojaning is to compromise
some inner neurons to inject hidden behaviors (i.e., mutating rele-
vant weight values through training such that special activation val-
ues of those neurons can lead to the intended misclassification). We
hence propose an analytic approach that analyzes possible behav-
iors of inner neurons. Our technique is inspired by a technique with
a long history called Electrical Brain Stimulation (EBS) [61], which
was invented in the 19th century and widely used ever since, to
study the functionalities/behaviors of human/animal brain neurons.
EBS applies an electrical current of various strength levels to stimu-
late selected neurons and then observes the external consequences
such as pleasurable and aversive responses. Analogously, given an
AI model to scan, our technique taps into individual neurons and
directly alters their activations, even without the corresponding
input that leads to such activations, and observes the correspond-
ing output differences. We hence call it Artificial Brain Stimulation
(ABS). During this procedure, a neuron compromised by trojaning
manifests itself by substantially elevating the activation of a specific
target label (and in the meantime possibly suppressing the activa-
tions of other labels), when the appropriate stimulus is supplied.
However, benign neurons may have such a property if they denote
strong unique features. ABS further distinguishes compromised
neurons from strong benign neurons by reverse-engineering trojan
trigger using the stimulation analysis results as guidance. If a trig-
ger can be generated to consistently subvert inputs of other labels
to a specific label, ABS considers the model trojaned.
Our contributions are summarized as follows.
• We propose a novel approach to scanning AI models for
back-doors by analyzing inner neuron behaviors through
a stimulation method. We formally define the stimulation
analysis that precisely determines how output activations
change with an inner neuron activation value. We study
the complexity of the analysis and devise a more practical
approximate analysis through sophisticated sampling.
• We devise an optimization based method to reverse engineer
trojan triggers, leveraging stimulation analysis results. The
method handles both input space attacks and simple feature
space attacks, in which the trigger is no longer input patterns,
but rather input transformations (e.g., image filters) that lead
to patterns in the inner feature space.
• We have performed substantial evaluation of ABS. Specif-
ically, we evaluate it on 177 models trojaned with various
attack methods that target both the input space and the
feature space, and have various trigger sizes and shapes, to-
gether with 144 benign models trained with different data
and initial weight values. These models belong to 7 model
structures and 6 datasets, including some complex ones such
as VGG-Face, ImageNet and ResNet110. Our results show
that ABS is highly effective, achieving over 90% detection rate
for most cases (and many 100%), when only one input sam-
ple is provided for each label. It substantially out-performs
the state-of-the-art Neural Cleanse [59] that requires a lot
of input samples and small trojan triggers to achieve good
performance. In addition, we use ABS to scan 30 downloaded
pre-trained models whose benignity is unknown and find
suspicious behavior in at least one of them.
ABS is heuristics based. It makes a few critical assumptions. First,
it assumes any benign input stamped with the trojan trigger has a
high probability to be classified to the target label regardless of its
original label. Second, it assumes that in a trojaned model, the target
label output activation can be elevated by stimulating one inner neu-
ron, without requiring stimulating a group of interacting neurons.
While these assumptions hold for the datasets, model structures,
and trojaning methods we study, they may not hold in new settings
(e.g., more advanced attacks), rendering ABS ineffective. Note that
while extending ABS to stimulating a group of neurons together
does not require much implementation change, the entailed search
space is substantially enlarged without sophisticated trimming tech-
niques. More discussion can be found in Section 3.1 Attack Model
and Section 6 Discussion. Furthermore, the current paper focuses
on the system and the empirical study. Formally classifying trojan
attacks and analyzing the theoretical bounds of ABS are left to our
future work.
2 TROJAN ATTACKS AND DEFENSE
Trojan attack injects hidden malicious behavior to an AI model.
Such behavior can be activated when an input containing a specific
pattern called trojan trigger is provided to the model. The pattern
could be in the pixel space or the feature space. Ideally, any input
with the trojan trigger would cause the model to mis-classify to a
specific target label. Without the trigger, the model behaves nor-
mally. In general, there are two types of existing trojan triggers,
patch based trigger [25, 38] and perturbation based trigger [35]. Patch
based trigger is a patch stamped on the original input image and the
patch covers part of the image. Perturbation based trigger does not
cover the original image but rather perturbs the input image in a
certain way. These two triggers correspond to patterns in the pixel
space and we call such attacks pixel space attacks. Trojan attacks
can happen in the feature space as well. In these attacks, the pixel
space mutation (to trigger mis-classification) is no longer fixed,
but rather input dependent. As shown in Figure 3, the Nashville
filter and the Gotham filter from Instagram [12] can be used as
trojan triggers. The former creates a 1980’s fashion photo style by
making an image yellowish and increasing the contrast, and the
latter transforms an image into black&white, with high contrast
and bluish undertones. Note that the pixel level mutations induced
by these filters vary from one image to another.
2.1 Existing Methods to Trojan AI Models
We discuss two representative existing methods to trojan models.
In Figure 1, we show sample images generated by various trojaning
methods for the MNIST [32] dataset. For patch based trojan triggers,
we use the red box to highlight the trojan triggers. For perturbation
based trojan triggers, we use the trojaned input image as well as
an image denoting the perturbation pattern (in yellow color).
Data Poisoning. Gu et al. [25] proposed an approach to trojaning
a model using training data poisoning. The scenario is that part
of a model’s training is outsourced to the attacker, who thus has
(a) Original
Neuron
(b)
Hijacking
(c) Poisoning
by Patch
(d) Poisoning by Static
Perturbation
(e) Poisoning by Adaptive
Perturbation
Figure 1: Triggers by different attacks on MNIST. Red boxes
highlight the added contents. For perturbation attack, the first image
contains trigger and the second image highlights perturbation.
access to some of the training data. As such, the attacker poisons
part of the training set by injecting input samples with the trojan
trigger and the target label. After training, the model picks up the
trojaned behavior. Chen et al. [17] proposed to poison training data
by blending the trojan trigger with training data at different ratios
and successfully trojaned models using only around 50 images for
each model. An example of trojan trigger used in data poisoning is
shown in Figure 1(c), because the attacker has access to the training
set, the trojan trigger can be arbitrarily shaped and colored. Here
we just use the simplest white diamond as an example.
The attackers may choose to poison training data with perturba-
tion triggers. Liao et al. [35] proposed to trojan DNNs with pertur-
bation based poisoning. There are two kinds of perturbation based
trojaning. The first one is static perturbation. The training data is
poisoned with static perturbation patterns. Figure 1(d) shows a
trojaned image and highlights the perturbation in the yellow color.
The second approach is adversarial perturbation. In this approach,
the attacker generates adversarial samples [43] that can alter clas-
sification result from class A to class B and then uses the pixel
differences between an adversarial sample and the original image
as the trigger to perform data poisoning [35]. Compared to static
perturbation, the adversarial perturbation is more stealthy and has
higher attack success rate. Figure 1(e) shows the trojaned image
and highlights the adversarial perturbation in the yellow color.
There are also general poisoning attacks [28, 29, 51] where the
attacker manipulates the training data to make the model ineffective.
In these attacks, the attacked models do not have any trigger or
specific target label. They are hence non-goal for this paper.
Neuron Hijacking. Liu et al. [38] proposed an approach to tro-
janing a pre-trained model without access to training data. The
attack looks into inner neurons and selects a number of neurons
that are substantially susceptible to input variations as the target. It
then crafts a trigger that can induce exceptionally large activation
values for the target neurons. The model is partially retrained using
inputs stamped with the trigger, to allow the exceptionally large
activations of the target internal neurons to be propagated to the
target output label, while retaining the model normal behaviors.
An example of crafted trojan trigger is shown in Figure 1(b).
2.2 Existing Defense
Detecting Input with Trojan Trigger. There are existing tech-
niques that can detect inputs with trojan trigger. Liu et al. [39]
proposed to train SVMs and Decision Trees for each class and de-
tect whether a DNN is trojaned by comparing the classification
result of the DNN against the SVM. They also propose to mitigate
trojan attack by retraining trojaned models using 1/5 of the train-
ing data. However, these approaches incur high computation cost
(a) Trojan trigger
(c) Rev. Eng. Pattern
(I) for Label Airplane
(b) Rev. Eng. Pattern
for Label Deer
(d) Rev. Eng. Pattern
(II) for Label Airplane
Figure 2: Reverse Engineered Patterns by NC for a Trojaned
VGG Model on CIFAR with Target Label Airplane
and hence were only evaluated on the MNIST dataset. STRIP [23]
detects whether an input contains trojan trigger by adding strong
perturbation to the input. Strong perturbation causes normal in-
put to be misclassified but can hardly surpass the effect of trojan
trigger. Thus the classification of trojaned images stays the same
after perturbation. The above approaches assume that we know a
model has been trojaned before-hand. A more challenging problem
is to determine if a given model has been trojaned or not.
Detecting and Fixing Trojaned Models. Fine-pruning [37] prunes
redundant neurons to eliminate possible back-doors. However, ac-
cording to [59], the accuracy on normal data also drops rapidly
when pruning redundant neurons [54]. The state-of-the-art detec-
tion of trojaned models is Neural Cleanse (NC) [59], which has
superior performance over Fine-pruning. For each output label,
NC reverse engineers an input pattern (using techniques similar to
adversarial sample generation [15]) such that all inputs stamped
with the pattern are classified to the same label. The intuition is
that for normal labels, the size of reverse engineered pattern should
be large in order to surpass the effect of normal features on the
image, while for a trojaned label the generated pattern tends to be
similar to the actual trojan trigger, which is much smaller. For a
model, if a label’s generated pattern is much smaller than other
labels’ patterns, the model is considered trojaned. NC successfully
detects six trojaned models presented in [59].
Limitations of Neural Cleanse (NC). Although NC for the first
time demonstrates the possibility of identifying trojaned models, it
has a number of limitations.
First, NC may not be able to reverse engineer the trojan trigger.
Note that for a target label t (e.g., airplane in CIFAR-10), both the
trigger and the unique features of t (e.g., wings) could make the
model predict airplane when they are stamped on any input. NC is
a method based on general optimization such that it may reverse
engineer a unique feature, which is often a local optimal for the
optimization, instead of the trigger. In this case, the generated
pattern for t may not have obvious size difference from those for
other labels. In Figure 2, we trojan a VGG19 model on CIFAR-10.
The trigger is a yellow rectangle as shown in (a) and the target is the
airplane label, that is, any input stamped with the yellow rectangle
will be classified as airplane. Applying NC to the airplane label has
60% chance of generating a pattern in (d) close to the trigger, and