title:Falcon: A Practical Log-Based Analysis Tool for Distributed Systems
author:Francisco Neves and
Nuno Machado and
Jos&apos;e Pereira
2018 48th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
Falcon: A Practical Log-based Analysis
Tool for Distributed Systems
Francisco Neves, Nuno Machado and Jos´e Pereira
HASLab, INESC TEC and University of Minho
{francisco.t.neves, nuno.a.machado}@inesctec.pt, PI:EMAIL
Braga, Portugal
Abstract—Programmers and support engineers typically rely
on log data to narrow down the root cause of unexpected
behaviors in dependable distributed systems. Unfortunately, the
inherently distributed nature and complexity of such distributed
executions often leads to multiple independent logs, scattered
across different physical machines, with thousands or millions
entries poorly correlated in terms of event causality. This renders
log-based debugging a tedious, time-consuming, and potentially
inconclusive task.
We present Falcon, a tool aimed at making log-based analysis
of distributed systems practical and effective. Falcon’s modular
architecture, designed as an extensible pipeline, allows it to
seamlessly combine several distinct logging sources and generate
a coherent space-time diagram of distributed executions. To
preserve event causality, even in the presence of logs collected
from independent unsynchronized machines, Falcon introduces a
novel happens-before symbolic formulation and relies on an off-
the-shelf constraint solver to obtain a coherent event schedule.
Our case study with the popular distributed coordination
service Apache Zookeeper shows that Falcon eases the log-
based analysis of complex distributed protocols and is helpful in
bridging the gap between protocol design and implementation.
I. INTRODUCTION
Developers of distributed systems cater for recording run-
time behavior by judiciously adding log statements to source
code [1], [2]. The number of log statements needed, and the
detail of the information collected, depends on the complexity
of the code. In systems that deal with concurrency and
faults, such as fault-tolerant consensus protocols, the resulting
effort is substantial. However, when an unexpected outcome
is noticed, log ﬁles are often the only source of information
that programmers can use to debug and ﬁx the problem.
Unfortunately, log analysis in distributed systems still re-
mains a daunting task, which has motivated programmers to
ask for more practical ways to understand runtime behav-
ior.1 First, besides the sheer number of entries, trace ﬁles
are typically spread across several nodes and generated by
distinct logging libraries with heterogeneous formats. Second,
although timestamped, interleaved statements or execution on
different nodes leads to a wide set of possible event execution
ﬂows and intermediate states that have to be considered. Third,
the lack of context propagation between nodes hinders the
ability to establish the causal relationship between events, i.e.,
the happens-before relationship typically denoted by “→” [3].
Causality is particularly helpful for debugging distributed
executions, as it allows reasoning about the order of distributed
events2. However, relying solely on log entry timestamps is
not enough to establish causality. On the one hand, these
timestamps are based on physical clocks and, even if clocks
are synchronized on all relevant nodes,
log messages are
often produced asynchronously after the fact they describe.
On the other hand, blindly considering that timestamps induce
causality hides the true system logic by ﬂattening history.
Several tracing systems have been proposed in the past
to track causality and alleviate the burden of debugging
distributed systems [4]–[8]. Nonetheless, they require careful
program instrumentation and do not support the analysis of
events stemming from distinct, heterogeneous log sources. In
contrast, popular operating system utilities such as strace and
ltrace are powerful assets for troubleshooting runtime behav-
ior, as they are language-agnostic and capable of capturing the
system calls and signals executed by a program, but fall short
when it comes to inferring causality across processes.
In this paper, we aim to achieve the best of both worlds
by enabling the inference of causally-related activity atop
commodity monitoring tools. To this end, we propose Falcon,
a practical and effective log-based analysis tool for distributed
systems. Falcon does not require custom instrumentation and
supports popular tracing and logging utilities (e.g.,
log4j,
strace, tshark), thus being suitable for debugging real-world
applications.
Falcon operates as a pipeline: ﬁrst, it normalizes the events
collected by the different logging sources; then, it resorts to
symbolic constraint solving to generate a global execution
trace that preserves causality; ﬁnally, it produces a space-time
diagram that enables a visual analysis of the whole execution.
To ensure event causality, Falcon employs a novel approach
that models a distributed execution by means of symbolic
variables (representing the logical clocks of the events traced)
and encodes the happens-before relationships as constraints
over those variables. Solving the constraint system with an
off-the-shelf solver yields a complete execution schedule that
coherently relates the data from the various log ﬁles, thus pro-
viding a thorough and accurate view of the original production
run. Due to its ﬂexible design, Falcon’s pipeline can also be
extended with additional log libraries and visualization tools.
1https://issues.apache.org/jira/browse/ZOOKEEPER-816
2We use event and log entry interchangeably in this paper.
2158-3927/18/$31.00 Â©2018 IEEE
DOI 10.1109/DSN.2018.00061
534
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:47:53 UTC from IEEE Xplore.  Restrictions apply. 
Our case study with the popular coordination service
Apache Zookeeper shows that Falcon is efﬁcient and facili-
tates the understanding of complex distributed executions by
relating low-level system calls with user-deﬁned log messages.
The rest of this paper is structured as follows. Section II
presents some background concepts and a motivating example.
Section III describes the design of Falcon, while Section IV
provides its implementation details. Section V presents the
case study with Apache Zookeeper. Section VI overviews the
most relevant related work and, ﬁnally, Section VII concludes
the paper by summarizing its main points.
II. BACKGROUND AND MOTIVATION
Low-Level Tracing Overview. *NIX environments nowa-
days offer various kernel-level tracers that enable powerful
troubleshooting capabilities. Moreover, by running at the op-
erating system level, these tracers are programming-language-
agnostic and even applicable to programs running on virtual
machines, thus being extremely useful for program debugging.
Notable examples include ftrace, strace, ltrace, eBPF, and
SystemTap.
At the core of most of these tools are system calls. In
computing, a system call, or syscall,3 can be deﬁned as
the fundamental
interface between an application and the
operating system kernel. During the execution of a program,
whenever it requires access to open and close ﬁles or to
establish a connection with a remote program, these intentions
are converted into syscalls. For example,
the strace tool
captures the signals received and the system calls invoked by
the target program. This is possible because it resorts to the
ptrace syscall, which allows a process to take control over
another process.
The interception of a syscall can be done both when the
execution switches between the user mode to the kernel mode
(entry point) and vice-versa (exit point). Intercepting at the
former allows accessing the syscall parameters, whereas inter-
cepting at the latter gives the success/failure of the operation.
The time spent between both interception points can vary
arbitrarily, as it depends on the state of the operating system,
resource contention, and programming decisions, such as
signal handling. Also, the two points are not guaranteed to
appear contiguously in the trace. In fact, the output of strace
usually exhibits an interleaving of entry and exit points of
different syscalls.
Some of the aforementioned tools can be also used for
tracing the messages exchanged in a distributed system, since
they allow tracking the socket read and write syscalls invoked
during the execution. Unfortunately, such syscall log is not
enough per se to effectively analyze and debug distributed
protocols that rely on complex communication patterns (e.g.,
consensus, fault-tolerance, and replication protocols), which
are challenging to design and implement. The reason is that
there is no information with respect to the causality between
the syscalls logged.
3http://man7.org/linux/man-pages/man2/syscalls.2.html
# Format: [pid], syscall(parameters) = return_value
3.1 [n3-894] read(n2, "Go go go", 1023) = 8
3.2 [n3-894] write(n1, "Wait guys", 1023) = 9
3.3 [n3-894] write(n2, "Wait guys", 1023) = 9
3.4 [n3-894] read(n2, "Ok", 1023) = 2
2.1 [n2-782] write(n3, "Go go go", 1023) = 8
2.2 [n2-782] write(n1, "Go go go", 1023) = 8
2.3 [n2-782] read(n3, "Wait guys", 1023) = 9
2.4 [n2-782] write(n1, "Ok", 1023) = 2
2.5 [n2-782] write(n3, "Ok", 1023) = 2
1.1 [n1-675] read(n3, "Wait guys", 1023) = 9
1.2 [n1-675] read(n2, "Ok", 1023) = 2
1.3 [n1-675] read(n2, "Go go go", 1023) = 8
Fig. 1. Complete trace resulting from merging the partial outputs of strace
on three distinct nodes. [ni-pid] denotes the global identiﬁer of a process pid
running on node i.
Motivating Example. As an example of this limitation,
consider a scenario with three participants of an online
multiplayer game, represented by three processes on distinct
machines connected through TCP sockets. In this scenario,
player 2 (corresponding to process n2-782) tells his teammates
to advance with a message “Go go go”. Player 3 (process
n3-894) disagrees with the suggestion and asks the team to
wait by replying “Wait guys”. Player 2 consents and writes
“Ok”. Player 1 (process n1-675), in turn, simply receives
the instructions given by the other players. The result of this
interaction was that player 1 advanced alone, causing the team
to later lose the game. Why did player 1 act against the
instructions given by the rest of the team?
In Figure 1, we present a possible log obtained by merging
the output of running strace on each process. The log contains
the syscalls executed during a chat conversation between the
three players, namely the reads and writes on each process’
socket. For the sake of readability, each entry is identiﬁed by
the concatenation of the node and process ids and the actual
ﬁles descriptors were replaced by the node ids.
In order to correctly reason about the runtime behavior
from the trace in Figure 1, one must ﬁrst establish the
happens-before relationship between the syscalls. As deﬁned
by Lamport [3], if an event a causally-precedes an event b in a
program execution, then a happens-before b (denoted a → b).
A more detailed deﬁnition of the happens-before relationship
is given in Section III-C.
Causality in distributed systems is typically captured by
logical clocks [3] or vector clocks [9]. However, low-level
tracing tools such as strace are not able to record logical time.
Let us then mimic the procedure of manually inferring the
happens-before relations present in Figure 1.
The causal order of syscalls within each process’ trace is
trivial to deﬁne because it respects the program order [3].
As such, the main challenge here is to infer the inter-process
happens-before relationships.
Note that the ﬁrst parameter on each write/read syscall
denotes the process that sent/received a message. Considering
that a read is always preceded by its corresponding write and
that TCP ensures reliable and ordered delivery, one is then
able to causally order the syscalls across the three processes.
535
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:47:53 UTC from IEEE Xplore.  Restrictions apply. 
# Format: [pid], syscall(parameters) = return_value
[n2-782] write(n3, "Go go go", 1023) = 8
[n2-782] write(n1, "Go go go", 1023) = 8
[n3-894] read(n2, "Go go go", 1023) = 8
[n3-894] write(n1, "Wait guys", 1023) = 9
[n3-894] write(n2, "Wait guys", 1023) = 9
[n1-675] read(n3, "Wait guys", 1023) = 9
[n2-782] read(n3, "Wait guys", 1023) = 9
[n2-782] write(n1, "Ok", 1023) = 2
[n2-782] write(n3, "Ok", 1023) = 2
[n1-675] read(n2, "Ok", 1023) = 2
[n3-894] read(n2, "Ok", 1023) = 2
[n1-675] read(n2, "Go go go", 1023) = 8
2.1
2.2
3.1
3.2
3.3
1.1
2.3
2.4
2.5
1.2
3.4
1.3
Fig. 2. Trace resulting from causally reordering the syscalls in Figure 1.
[ni-pid] denotes the global identiﬁer of a process pid running on node i.
n1-675
n2-782
n3-894
read
8
"Go go go"
write
9
write
9
Tick 0
Tick 1
Tick 2
write
8
write
8
Tick 3
read
9
"Wait guys"
Tick 4
read
8
"Go go go"
read
9
"Wait guys"
write
2
write
2
read
2
"Ok"
Tick 5
Tick 6
Tick 7
n1-675
n3: “Wait guys”
n2: “Go go go”
n2: “Ok”
n2-782
n2: “Go go go”
n3: “Wait guys”
n2: “Ok”
n3-894
n2: “Go go go”
n3: “Wait guys”
n2: “Ok”
Note that the chat log of process n1-675 exhibits an inconsis-
tency (highlighted in red) with respect to the actual message
history, which explains the reason behind the reckless move
by player 1. The dashed circular area in Figure 3 pinpoints
the root cause of this inconsistency: a delay in the arrival
of the message “Go go go” sent by process n2-782 caused
an inversion in the expected chat output. Since the inverted
messages are (semantically) causally related, this means that
there is a message race bug in the system implementation.
This example illustrates a game scenario that simply caused
a team to lose one round. However, in complex distributed
systems that require coordination, the consequences may be
much more severe (e.g. data loss or corruption). It is thus of
paramount importance to devise practical and effective tools
to aid the analysis of execution logs.
III. DESIGN
We propose Falcon – a practical and effective log-analysis
tool for distributed systems, capable of generating a global
execution schedule from multiple independent log ﬁles while
preserving event causality. This is achieved by means of a
novel symbolic constraint model that encodes the happens-
before relationship between events. Moreover, Falcon automat-
ically generates a causal space-time diagram of the execution,
which further eases the analysis of the logs and the understand-
ing of distributed executions. This section describes Falcon’s
design requirements, architecture, and happens-before model.
read
2
"Ok"
A. Design Requirements
Fig. 3. Space-time diagram of the trace in Figure 2. Each vertical line
represents a node, whereas the circles within a line represent the syscalls
executed by that node. Solid lines connecting circles indicate a happens-
before relationship between the events. The dashed circular area highlights
the problematic message race.
In Figure 2, we depict a possible trace resulting from
causally reordering the syscalls according to the intra- and
inter-node happens-before relationships. To further ease the
analysis of the execution, we also convert the ordered trace
into a space-time diagram, which is shown in Figure 3. In
the diagram, vertical lines are the execution timelines of the
processes indicated by the labels, and the circles are the events
happening in each process. Each event is associated with a
given logical clock “tick”. We added the message sizes on each
event and the message content on read syscalls. Each pair of
connected events indicates a happens-before relationship.
Displaying the messages received by each process, one
obtains the following chat logs:
Time spent at post-mortem software debugging is directly
affected by the amount of useful information captured during
production runs. Since logging is an expensive operation, a
trade-off must be made between the log’s verbosity level and
the performance and space overhead imposed at runtime [1].
For that reason, different tracing tools opt for focusing on
different aspects and provide distinct features (e.g. printing log
statements, snifﬁng network packets, proﬁling performance,
etc). Nevertheless, one should be able to leverage all those
features in order to ease the burden of debugging complex
distributed systems. A practical and effective log-analysis tool
should thus meet the following design requirements:
• Support several log sources. The tool should be able
to extract useful knowledge about the execution from
multiple data sources, such as logging libraries, network
sniffers (e.g. libpcap-based tools), and low-level tracing
tools (e.g. ptrace-based tools).
• Combine data in a causally consistent way. The tool
should be able to combine all logged events in a seamless
and coherent fashion, even if they were captured at dif-
ferent physical machines with unsynchronized clocks. In
536
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:47:53 UTC from IEEE Xplore.  Restrictions apply. 
.log 
.pcap
…
Trace
Processor
.log
.pcap
…
Happens-Before
Model Generator
Visualizer
x
z
y
SMT
Solver
Causal
Trace
Fig. 4. Falcon’s pipeline architecture, which comprises modules for trace
processing, model generation, and visualization.
practice, this corresponds to ensuring that the happened-
before relationship between events is established across
all log ﬁles regardless of their source.
• Provide a visual representation of the execution. To
obviate complexity due to log verbosity and further help
developers to reason about the execution, the tool should
be able to display events in a “human-friendly” way. In
the particular context of distributed systems, space-time
diagrams depicting the inter-process causal dependencies
have long been used to aid the understanding of dis-
tributed protocols over multiple processes [10].
In the next section, we describe how Falcon meets the
aforementioned requirements.
B. Architecture