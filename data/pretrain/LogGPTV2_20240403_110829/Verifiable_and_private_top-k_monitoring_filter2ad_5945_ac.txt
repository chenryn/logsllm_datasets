trades computation for memory space in ﬁnding v ∈ [max(t, vk),
2kd+kq · mq) such that hv
u ≡ W (cid:48) for given hu and W (cid:48).
The User begins by allocating a lookup table within the available
memory space. Let τ denote the number of entries in the table. We
represent v as a polynomial v = cατ α + cα−1τ α−1 + . . . + c0,
where α = (cid:4)logτ (2kd+kq · mq)(cid:5) and ci ∈ [0, τ ) ∀i ∈ [0, α].
−(cατ α−1+...+c1)τ
W (cid:48) = hc0
u
u . As before, the User pop-
Thus, h
ulates the lookup table for (cid:104)hc0
u as search key, for
0 ≤ c0 < τ. Following that, the User iteratively checks whether
u W (cid:48) exists in the lookup table, decrementing i from τ α − 1
h−iτ
towards (cid:98)max(t, vk)/τ(cid:99).
u , c0(cid:105) with hc0
43.4 Compressing Document & Query Vectors
In certain applications, the document vector d and query vec-
tor q are expected to contain mostly zero coordinates. One such
example is the vector space model for text retrieval [16], where
each feature corresponds to a dictionary word. Since most of the
documents and queries contain only a very small subset of the dic-
tionary words, only a small number of coordinates in the document
and query vectors are non-zero. In this situation, the Owner may
leave out some of the zero coordinates in d, so that the Server
can exclude them from query processing. This reduces the server
processing overhead, at the expense of exposing the dropped co-
ordinates in d. Likewise, the User may drop some of the zero
coordinates in q. The concrete scheme is as follows.
For each d, the Owner creates a certiﬁed Bloom ﬁlter [4] on the
index position of the non-zero coordinates in d. After that, those
zero coordinates whose index gets a false positive on the Bloom
ﬁlter must also stay in d; the remaining zero coordinates whose
index gets a negative on the Bloom ﬁlter may be omitted.
As proposed in [4], a Bloom ﬁlter is designed to support mem-
bership checks on a set F of f key values, F = {F1, F2, . . . , Ff}.
To construct a Bloom ﬁlter with φ bits, we choose ρ independent
hash functions H1, H2, . . . , Hρ, each with a range of [1, φ]. For
each value Fi ∈ F, the ﬁlter bits at positions H1(Fi), H2(Fi), . . .,
Hρ(Fi) are set to 1. To check whether a given F is in F, we ex-
amine the bits at H1(F ), H2(F ), . . . , Hρ(F ). If any of the bits is
0, F cannot possibly be in F; otherwise there is a high probability
that F is in F. In other words, the Bloom ﬁlter admits controlled
false positive rates but no false negatives. The false positive rate is
(cid:19)ρf(cid:33)ρ
≈(cid:16)
1 − e
−ρf /φ(cid:17)ρ
(cid:32)
(cid:18)
F P =
1 −
1 − 1
φ
(2)
Mathematically, F P is minimized for ρ = φ ln 2/f. Since ρ must
be an integer, we will use ρ = (cid:98)φ ln 2/f(cid:99). Given the value of f
and the target F P rate, we can thus set ρ and φ accordingly.
Applying the Bloom ﬁlter to our setting, f corresponds to the
number of non-zero coordinates in d, and the number of false pos-
itives is F P × (m− f ). Thus, the Server sees f + F P × (m− f )
remaining coordinates in d. Together with the parameters φ and
ν, the Server can deduce the value of f, i.e., the actual number of
non-zero coordinates among the remaining coordinates. If we want
to prevent that, then the index positions of some of the zero coor-
dinates should go into the Bloom ﬁlter too, so that f yields only a
loose upper bound on the number of non-zero coordinates.
Likewise for q, the User may randomly drop some of the zero
coordinates. The remaining coordinates, along with their index po-
sitions, are submitted to the Server.
In processing a document d against a query q, the Server eval-
uates the inner product q · d only over the common coordinates in
q and d that are accessible to each User. Along with the answer
W (cid:48), the Server returns the index positions I that are in q but have
been omitted from d, as well as the certiﬁed Bloom ﬁlter. The user
can verify I against the Bloom ﬁlter, which by construction will
not produce false positives on the indices in I.
4. EMPIRICAL VALIDATION
In this section, we evaluate the overall practicality of our CVPM
scheme, and the effectiveness of the optimization techniques intro-
duced in Section 3.3.1 in particular.
4.1 Experiment Set-Up
Datasets: To ensure that our observations are generalizable, we
have run CVPM on several datasets. Here we report on two real
datasets from the UCI KDD Archive1. The datasets are picked be-
cause they vary from each other in key properties that stress differ-
ent algorithms in CVPM. The ﬁrst, Corel Image, contains feature
vectors extracted from 68,040 photo images. Each feature vector is
a point in 32-dimensional HSV color space, so m = 32. We dis-
cretize every dimension into 28 integer values. The feature vectors
contain many zero coordinates, and the correlation between feature
vectors are generally low. The dataset relates closely with the main
motivating application in Section 1.
The second dataset, US Census, is a discretized version of part
of the data collected in the 1990 U.S. census. The dataset includes
2,458,285 records (vectors), each with m = 68 attribute values.
All the attributes are in the range [0, 20]. The correlation between
vectors are high, relative to that in the Corel Image data. The data
in this collection essentially are user proﬁles, hence they simulate
the user proﬁle matching application described in the Introduction.
Methodology: For both datasets, we extract 100 vectors randomly
to be user queries. The rest of the vectors are shufﬂed and fed
into the document stream from the Owner. The arrival rate is λ
documents/minute. The User maintains a sliding window of the
documents that arrived in the last w hour, and the k documents with
the highest correlation scores in this window constitute the query
result. For each experiment setting, the performance measures are
averaged across the 100 queries.
We implemented the CVPM scheme in C language, on the PBC
cryptography library from Stanford University2. The experiments
are run on a MacBook Pro with 2.8 GHz Intel Core i7 CPU and 8
GB of main memory.
Metrics: Our primary performance metrics include: (a) the average
time taken by the Owner in executing the DataGen algorithm on a
new document; (b) the average time taken by the Server to execute
the ServPro algorithm for each document-query pair; and (c) the
average time taken by the User to run the UserDec algorithm and
update her top-k result upon a document arrival, with and without
the optimization described in Section 3.3.1.
4.2 Corel Image Experiment
For the Corel Image data, we set the sliding window w to one
hour and vary λ from 1 to 30 documents/minute. Referring to For-
mula 1, the threshold t is set to 0, so pruning of the search space
for the correlation score v of a new document relies solely on vk,
the correlation score of the k-th result document. Figure 7(a) plots
the average user execution time per query for k = 10, 20, 40 and
60. The ﬁgure also gives the execution time of ‘No Opt’, which
disables the optimization techniques in Section 3.3.1.
With a larger k, vk becomes lower. If the correlation score v
of an arriving document is below vk, the User will have to execute
more iterations in the baby-step giant-step algorithm before she can
conclude that the new document is not (yet) eligible for the top-k
result. This explains why a larger k lengthens the execution time.
For a ﬁxed k setting, the execution time drops as λ increases. The
reason is that the sliding window accumulates more documents, in
the process pushing up the k-th highest correlation score and nar-
rowing the search space for v. The execution time is bounded from
below by the cost of the bilinear pairing operation ˆe(., .) (indicated
by the dotted horizontal line). In the worst case, the User executes
the baby-step giant-step algorithm until v is discovered, so the ex-
ecution time is bounded from above by the ‘No Opt’ method.
The execution time incurred by the Owner and Server are sum-
marized in Table 3. The cost of the DataGen algorithm, incurred
1http://kdd.ics.uci.edu/databases/
2http://crypto.stanford.edu/pbc/
5 5. CONCLUSION
In this paper, we formulate the security requirements in a data
streaming model. The model employs an untrusted server to com-
pute the correlation scores between documents streamed from the
data owner, and the standing queries issued by users. The corre-
lation computation translates to an inner product of the respective
document and query vectors. We present the ﬁrst cryptographic
scheme that concurrently safeguards the privacy of the documents
and queries, while enabling users to verify the correctness of the
correlation scores received. Through extensive experiments, we
demonstrate that the proposed scheme achieves practical execution
time for a wide spectrum of application settings.
Acknowledgements
HweeHwa Pang is supported by Research Grant 12-C220-SMU-
004 from the Singapore Management University.
6. REFERENCES
[1] C. C. Aggarwal. On randomization, public information and
the curse of dimensionality. In Proc. of ICDE, 2007.
[2] R. Agrawal and R. Srikant. Privacy-preserving data mining.
In Proc. of ACM SIGMOD, 2000.
[3] J. Bethencourt, D. X. Song, and B. Waters. New
constructions and practical applications for private stream
searching. In Proc. of IEEE S& P, 2006.
[4] B. Bloom. Space/Time Trade-Offs in Hash Coding with
Allowable Errors. Communications of the ACM,
13(7):422–426, July 1970.
[5] B. Goethals, S. Laur, H. Lipmaa, and T. Mielikainen. On
private scalar product computation for privacy-preserving
data mining. In Proc. of ICISC, 2004.
[6] J. Katz, A. Sahai, and B. Waters. Predicate encryption
supporting disjunctions, polynomial equations, and inner
products. In EUROCRYPT, pages 146–162, April 2008.
[7] F. Li, K. Yi, M. Hadjieleftheriou, and G. Kollios.
Proof-infused streams: Enabling authentication of sliding
window queries on streams. In Proc. of VLDB, pages
147–158, September 2007.
[8] W. Lindner and J. Meier. Towards a secure data stream
management system. In Trends in Enterprise Application
Architecture, pages 114–128, August 2005.
[9] A. J. Menezes, P. C. van Oorschot, and S. A. Vanstone.
Handbook of Applied Cryptography. CRC Press, 1997.
[10] S. Nath, H. Yu, and H. Chan. Secure outsourced aggregation
via one-way chains. In Proc. of ACM SIGMOD, 2009.
[11] R. Ostrovsky and W. E. S. III. Private searching on streaming
data. In Advances in Cryptology (CRYPTO), 2005.
[12] S. Papadopoulos, Y. Yang, and D. Papadias. CADS:
Continuous authentication on data streams. In Proc. of
VLDB, pages 135–146, September 2007.
[13] E. Shen, E. Shi, and B. Waters. Predicate privacy in
encryption systems. In Proc. of TCC, 2009.
[14] Y. Xu, K. Wang, A. W.-C. Fu, R. She, and J. Pei.
Privacy-preserving data stream classiﬁcation. In
Privacy-Preserving Data Mining. Springer, 2008.
[15] Z. Yang, R. Wright, and H. Subramaniam. Experimental
analysis of a privacy-preserving scalar protocol.
International Journal of Computer Systems Science and
Engineering, 21(1):47–52, January 2006.
[16] J. Zobel and A. Moffat. Inverted Files for Text Search
Engine. ACM Computing Surveys, 38(2):6, July 2006.
Finally, the corresponding execution time of the Owner and Server
(a) Corel Image Data
(b) US Census Data
Dataset
Corel Image
US Census
Owner Server
217.80
275.56
576.19
449.82
Table 3: Owner and Server Processing Times (msec)
once for every new document, is just over 200 msec; this cost is
dominated by the exponentiation operations in Step 3 of the algo-
rithm. The cost of the ServPro algorithm is incurred once for
every document-query pair and stands at 275 msec; this cost is
dominated by the bilinear pairing operations in Step 1 of the al-
gorithm. These performance levels can be maintained for higher
data rates by employing multiple CPUs.
4.3 US Census Experiment
Turning to the US Census data, we again set the sliding win-
dow w at one hour. Here λ varies from 1 to 10 documents/minute.
The average user execution time per query is summarized in Fig-
ure 7(b). The qualitative behavior of CVPM observed here is the
same as in the previous experiment. Quantitatively, the execution
times are lower now because the narrower feature domains (i.e.,
smaller kd and kq values in Formula 1) constrain the search space
for the correlation score v of arriving documents.
are reported in Table 3. Both are higher than in the previous exper-
iment, on account of the larger number of features in this dataset –
m = 68 – compared to m = 32 in the Corel Image data.
4.4 Summary of Experiment Results
Besides the Corel Image and US Census data, we have also ex-
perimented with several other datasets like the Insurance Company
Benchmark, also available from the UCI KDD Archive. The com-
mon observations across the experiments are as follows:
• The optimization arising from Formula 1 is particularly ef-
fective with high document rate λ, large sliding window w
or low k settings, as they tend to raise the correlation score
vk of the k-th result document.
• The User cost is sensitive to the number of features and their
domain sizes. If there are many features with large domains,
it may be necessary to bucketize the feature values; effec-
tively, this induces a coarser resolution on the features.
• Where the feature vectors have high dimensionality, it may
be necessary to parallelize the computation at the Owner and
Server. This is straightforward to implement, as the compu-
tation on different coordinates in the document vector can be
carried out independently.
• CVPM delivers acceptable performance for many practical
application settings, with sub-second execution time for ev-
ery party in the data streaming model.
0102030020406080λ (docs/min)Decryption Time (msec)k=10k=20k=40k=60No Opt02468108.38.58.78.99.19.3λ (docs/min)Decryption Time (msec)k=10k=20k=40k=60No Opt6