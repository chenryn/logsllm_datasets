title:MILR: Mathematically Induced Layer Recovery for Plaintext Space
Error Correction of CNNs
author:Jonathan Ponader and
Kyle Thomas and
Sandip Kundu and
Yan Solihin
MILR: Mathematically Induced Layer Recovery for
Plaintext Space Error Correction of CNNs
Jonathan S. Ponader
University of Central Florida
Orlando, FL 32816
Sandip Kundu
Yan Solihin
University of Massachusetts
University of Central Florida
Amherst, MA 01003
Orlando, FL 32816
PI:EMAIL
PI:EMAIL
PI:EMAIL
0
2
0
2
t
c
O
8
2
]
G
L
.
s
c
[
1
v
7
8
6
4
1
.
0
1
0
2
:
v
i
X
r
a
Abstract—The increased use of Convolutional Neural Networks
(CNN) in mission critical systems has increased the need for
robust and resilient networks in the face of both naturally
occurring faults as well as security attacks. The lack of robustness
and resiliency can lead to unreliable inference results. Current
methods that address CNN robustness require hardware modiﬁ-
cation, network modiﬁcation, or network duplication. This paper
proposes MILR a software based CNN error detection and
error correction system that enables self-healing of the network
from single and multi bit errors. The self-healing capabilities are
based on mathematical relationships between the inputs,outputs,
and parameters(weights) of a layers, exploiting these relationships
allow the recovery of erroneous parameters (weights) throughout
a layer and the network. MILR is suitable for plaintext-space
error correction (PSEC) given its ability to correct whole-weight
and even whole-layer errors in CNNs.
I. INTRODUCTION
Artiﬁcial Intelligence (AI) is increasingly used or consid-
ered in mission critical systems, whether used in the cloud
or in the ﬁeld (e.g. autonomous cars, industrial control sys-
tems) [3]. Such systems must remain robust and resilient in the
face of naturally occurring faults. In particular, for AI system
relying on neural networks (NN), soft memory errors may
result in corruption of memory values representing weights in
the NN. Memory failure rate remain a big concern in the future
as DRAM scaling is pushing the limit, while new memory
technologies suffer from higher expected error rates. These
memory faults can be caused by many environmental factors
such as temperature, radiation, wear-out of hardware, and
device speciﬁc issue such as resistance drift [13]. While some
memory faults may be masked out, others may cause silent
data corruption (SDC) that lead to unintended consequences
such as misclassiﬁcation [12], [18], [25].
AI software also represent a highly valuable intellectual
property that
is high valued target for thefts [24]. When
deployed in the cloud, its security can be improved by utilizing
encrypted isolated environment, such as running it
in an
encrypted virtual machine (VM). Both AMD secure encrypted
virtualization (SEV) [1] and Intel multi-key total memory
encryption (MKTME) [4]1 keep each VM encrypted using
a unique key, preventing other VMs or the hypervisor from
learning about plaintext of data processed by the AI software.
1Intel also provides secure enclave environment through SGX. However,
full AI software is unlikely to run in an SGX enclave due its memory limited
to 128MB.
1
MKTME relies on AES-XTS mode [4] for encrypting memory
(Figure 1).
Fig. 1: AES-XTS encryption mode used for memory encryp-
tion in MKTME.
NN resiliency depends on (1) the ability to detect errors and
(2) the ability to recover from errors without the involvement
of external intervention (self-healing). In this study, we limit
our concerns to self healing from soft memory errors. Current
solutions rely on error correcting code (ECC) for detecting
and correcting errors, for example a popular Hamming code
SECDED detects two bit errors and corrects one bit error in
a word.
For NN software running on encrypted VM, we distinguish
between the memory that contains encrypted data (ciphertext
space) and plaintext data (plaintext space). We point out that
ECC schemes are appropriate for the ciphertext space, but
not so for the plaintext space. The primary assumptions ECC
relies on are that (1) errors are distributed randomly on a bit
level, and (2) the probability of multi-bit error in a word is
low. With these assumptions, we only need to correct a small
number of bit errors per word, and protect each word with a
separate ECC. Unfortunately, these assumptions do not apply
in the plaintext space. An uncorrected bit error in the ciphertext
of a word translates to many-bit error in the plaintext after
decryption in AES-XTS mode. Furthermore, the error is no
longer randomly distributed in the plaintext space; they are
concentrated in bits that belong to an encryption word. As a
result, a different and more powerful error correction is needed
in the plaintext space: one that can deal with many bit errors
in entire encryption words.
In contrast to prior ECC schemes that focus on ciphertext
space, we focus on a new problem of how to provide plaintext
space error correction (PSEC), with a goal of providing self-
AES EncAES EncTweakPlaintextCiphertextKey2Key1jA128128128healing of NN systems. We limit our scope to convolutional
neural network (CNN) systems, but the problem and approach
likely extend to other NNs. Fundamentally, it is difﬁcult to
correct many-bit errors in the plaintext space because error cor-
rection capability depends on code that capture information re-
dundantly, hence the amount of redundancy required increases
with the amount of desired correctable errors. However, we
make a novel observation that redundancy of information
that naturally occurs in CNN systems can be leveraged for
PSEC. We propose MILR (which stands for Mathematically-
Induced Layer Recovery), a PSEC software solution for CNNs.
MILR relies on a key observation that the input, output and
parameters (weights) of a CNN layer are algebraically related,
and that
in many cases, knowing two of them allow the
recovery of the third. We show that MILR can correct not
just multi-bit errors in a word, it can detect and correct errors
affecting entire weights, and in many cases, a entire NN layer,
in addition to the regular random bit error.
We envision MILR can have several novel uses. First,
for CNN software running on encrypted VM, MILR could
enhance ECC-protected memory by detecting and correcting
errors in the plaintext space that escape ECC. With this use, the
resiliency of CNN to errors increases substantially, making it
suitable for self healing of systems that need high classiﬁcation
accuracy despite high memory error rates. Second, MILR can
be used in lieu of ECC, in some embedded systems where
ECC use is prohibitive. MILR is a software solution, so it
can be applied selectively and only as needed. In general,
MILR achieves higher effectiveness in correcting errors than
SECDED ECC. Third, MILR can also be used to self heal
from security attacks that involve memory corruption in the
plaintext space. For example, an attacker, exploiting software
vulnerability, may cause an overwrite to a targeted weight in
a NN to force classiﬁcation error. MILR can detect weights
that have been modiﬁed and restored them.
MILR was evaluated with three CNN networks in an error
simulator, injecting the network with random bit ﬂips with
varying Raw Bit Error Rate (RBER) [5], randomly ﬂipping
all bits in a weight at varying error rates and overwriting all
weights in a layer with random values. These test simulate
both soft memory errors and security attacks. MILR was
able to increase the robustness of these networks, enabling
them to operate normally even after being subjected to these
modiﬁcations. We found that MILR corrects whole weight
or even whole layer errors in plaintext space where ECC
cannot. Even in the ciphertext space, MILR can tolerate higher
bit error rates than SECDED ECC. Finally, we showed how
availability-accuracy trade off curves can be derived for MILR
that help users select the most mission-appropriate design.
II. RELATED WORKS
The fault tolerance of neural networks has been studied at
depth, both from a robustness standpoint [12], [18], [25] as
well as a security standpoint [14], [19] attempting to exploit
the lack of robustness in a network. All of these works have
shown that bit-ﬂips can have a major impact on the networks
performance. Li et al. [12] shows that the impact of bit ﬂip
errors on NN accuracy depends on the type of network, data
types (e.g. ﬂoat or ﬂoat16), and bit position. While soft errors
may be distributed randomly, security attacks may rely on
targeting certain bits that are extremely impactful on NN
performance. For example, Rakin et al. [19] showed that in the
ResNet-18 network with 93 million bits of parameters it only
took 13 bit ﬂips to degrade the accuracy from 69.8%accuracy
to 0.01%.
Many solutions have been proposed to address bit ﬂips in
neural networks, both for detecting and for recovering from
them. All such works assume non-encrypted VM. ECC has
long been used for detecting and correcting memory errors in
an application agnostic manner [23]. Guan et al. [7] proposed
a application speciﬁc ECC approach for convolutional neural
networks, storing 1 parity bit within a quantized 8-bit weight,
combining 8 together to make a 64 bit word with an 8
bit ECC parity string. Both versions of ECC suffer from
limited detection and recoverability leaving them susceptible
to multi-bit errors. Li et al. [12] proposed a symptom based
error detector and selective latch hardening. The symptom
based error detector works by detecting unusual values of
variables, which are likely to cause SDC. The normal range
of values are obtained from training, making it hard to add
protection after deployment. The selective latch hardening
reduces vulnerabilities of logic to soft errors.
Triple Modular Redundancy (TMR) is another agnostic
approach to recovery from both logic and memory errors.
TMR works by running three copies of the same application
and a majority vote for the results is used [15]. Self-healing
can be provided if the system allows the majority instances
to update the NN of the minority instance. TMR is expensive
as it requires tripling of computation and memory resources.
Dual Modular Redundancy (DMR) is another variation of
redundancy approach but is only able to detect errors but the
lack of the third copy makes it difﬁcult to ﬁgure out which
instance is the erroneous one, preventing self healing. Phatak et
al. [17] proposed only duplicating parts of hidden layers in the
network. Such an approach is less expensive than a standard
TMR but does not provide complete fault tolerance. Qin et
al. [18] explored not recovery errors but setting erroneous
weights to zero. This decreased the accuracy drop caused by
bit-ﬂips, but higher error rates still caused a signiﬁcant loss in
performance.
III. MILR: MATHEMATICALLY INDUCED LAYER
RECOVERY
MILR works on the premise that
there is an algebraic
relationship between the input, output and parameters of each
layer of a CNN. Suppose that an CNN layer F receives
input X to produce output Y using parameters P . Given
the input and parameters, the output can be computed using
forward pass (Equation 1). Given the output and parameters,
the input can be computed using a backward pass (Equation
2), if the layer is invertible. Given the input and output, the
parameters can be computed using parameter solving function
2
R (Equation 3).
f (x, p) = y
f−1(y, p) = x
R(x, y) = p
(1)
(2)
(3)
Exploiting the relationships in the above equations forms
the foundation of MILR. Suppose that a pair of known-
good (golden) input and output is stored in reliable and safe
memory, while the parameters are placed in main memory
that provides fast access but is prone to errors and attacks.
Since outputs change if there are errors in parameters, to
provide error detection, MILR utilizes a forward pass with
the golden input, and compares its output with the golden
output; a mismatch indicates parameter errors. To provide
self-healing, the parameters are recomputed using the pair
of golden input and output, and the recomputed parameters
overwrite the erroneous parameters. Using such an approach,
MILR can provide both error detection and self-healing. Note,
however, while recovery phase is only needed when errors
are detected, error detection phase must be scheduled before
errors can be detected. If instant error detection is required, a
different scheme should be used. To cater for the possibility of
needing a different error detection scheme, we wish to keep the
design of error detection and recovery separate, even though
they still rely on the same principle of exploiting algebraic
relationship between input and output.
Figure 2 illustrates an example MILR error detection phase
with three error-vulnerable layers f, g, h, with Pf , Pg, Ph their
respective parameters. Solid lines indicate normal execution
ﬂow, while dashed lines indicate error detection ﬂow. Many
layers in an NN exhibit a repeated behavior, using the same
subset of parameters multiple times to produce multiple out-
puts. This behavior allows use to create something called a
partial checkpoint. A partial checkpoint stores a single output
value from each subset of the layers parameters, reducing
the storage overhead while still allowing for erroneous lay-
ers to be identiﬁed. Tremendous savings occur using partial
checkpoints: a partial checkpoint can be up to two orders of
magnitude smaller than a full checkpoint for convolutional lay-
ers. Partial checkpoints,P Cf , P Cg, P Ch are stored in error-
resistant memory and correspond to the output of each layer
given an input that is generated using pseudo-random number
generator. By using pseudo-random number generator, we only
need to memorize the initial seed, and the partial checkpoints
to allow erroneous layers to be identiﬁed. To initiate error
detection phase, input is constructed using pseudo random
number generators 1(cid:13), and each layer initiates a forward pass.
The output of each layer is then compared against the partial
checkpoint 2(cid:13), and if they do not match, the layer is ﬂagged
as containing erroneous parameters 3(cid:13).
Figure 3 illustrates an example MILR error recovery phase
providing self-healing. Error recovery requires full checkpoints
due to the need to recover the exact values of parameters,
hence a challenge is the space overheads required to store
checkpoints. To address the storage overhead, we observe
three opportunities for removing checkpoints. First, we can
skip keeping input checkpoint if a layer is invertible, since
Fig. 2: Illustration of MILR error detection.
Fig. 3: Illustration of MILR self-healing recovery
an output checkpoint can be used with a backward pass
to calculate the input. In the ﬁgure, we store the golden
checkpoints Xf , Xh as well as the golden output checkpoint Y
in error-resistant memory, but skip Xg because g is invertible.
Normal CNN inference ﬂows along the solid lines, while
error recovery ﬂows along the dashed lines. When error
recovery is needed, the identiﬁed erroneous layer needs a
golden input/output pair as input to its parameter recovery
function. Suppose that layer f is erroneous. Its parameter
recovery function receives input Xf 1(cid:13). Xh is input to the
inverse of g 2(cid:13), i.e. g−1(Xh, Pg), generating an output needed
for the parameter recovery function Rf 3(cid:13). Rf then recovers
the parameters, self-healing layer f 3(cid:13). Hence, layer f is
recovered even without checkpoint Xg because g is invertible.
As another example, suppose that layer g is erroneous. In this
case, its parameter recovery function receives the output of
layer f 5(cid:13) and the checkpoint Xh 6(cid:13) without a need for a
backward pass. Then, the parameter recovery function Rg uses
both as inputs to recover layer g parameters 7(cid:13).
The second opportunity for removing an input checkpoint
is when the preceding layers have no parameters. Since
preceding layers have no parameters to recover, it does not
need input/output pairs to recover. Hence, we can remove the
input checkpoint.
The third opportunity is when we can transform a non-
invertible layer to an invertible layer. A layer may not be
invertible when there is not enough information encoded in the
layer, e.g. the number of equations in the system of equations
is lower than the number of parameters to be solved. Such a
layer can be made invertible by adding dummy inputs that are
used only for error recovery. Dummy inputs do not need to be
stored in the input checkpoint if we rely on pseudo-random
number generators for such input, however the additional
3
fPfRegularinputgPghPh=?ErrorNo=?ErrorNo=?ErrorNoRegularoutputfPfXfRegularinputgPghPhXhYRegularoutputg-1Seeded Pseudo-Random Tensor GeneratorPCfPCgPChRRhf1112223332Rg314(a)(b)567fPfRegularinputgPghPh=?ErrorNo=?ErrorNo=?ErrorNoRegularoutputfPfXfRegularinputgPghPhXhYRegularoutputg-1Seeded Pseudo-Random Tensor GeneratorPCfPCgPChRRhf1112223332Rg314(a)(b)567output values will need to be stored (dummy outputs).
Given that not every checkpoint is kept between two con-
secutive layers, the recoverability of a network is limited by
the number of checkpoints. To recover parameters of a layer,
we need to do forward pass from the a preceding checkpoint
and backward pass from a succeeding checkpoint, thus the
system can only recover at most one layer in between two
checkpoints, but any number of parameter errors in that layer
can be recovered. This is substantially more powerful than
traditional ECC which can only recover 1 bit error in one
word, hence even a multi-bit error of a single parameter
cannot be recovered, let alone all parameters in a single layer.
Furthermore, if there are N checkpoints, we can recover up to
N − 1 layers as long as there is at most one layer with errors
in between each pair of checkpoints.
MILR is divided into 3 distinct phases: the initialization,
error detection, and the error recovery phases. In the initial-