tion [18] and others. We make use of white-box attacks on visual
classifiers [17, 53], sequential models [18, 79] and object detec-
tors [28]. We show that black-box attacks [41] are a generic alter-
native to prior attacks on SIFT [39].
Attacking page-based ad-blockers introduce novel challenges.
Perturbing HTML bares similarities to discrete domain attacks, e.g.,
PDF malware detection [80]. The ad-blocker’s inputs can also be
controlled by multiple entities, a constraint reminiscent of those
that arise in physical-world attacks [12, 28, 29, 49, 75].
Preventing adversarial examples is an open problem. Adversarial
training is a viable strategy [33, 49, 53, 86], but considers a less
stringent threat model than perceptual ad-blockers.
7 CONCLUSION
We have presented a comprehensive security evaluation of per-
ceptual ad-blocking. To understand the design space of these re-
cently deployed systems, we have derived a unified architecture
that incorporates and extends prior work. Our analysis of this ar-
chitecture has revealed multiple vulnerabilities at every stage of
the visual ad-classification pipeline. We have shown that unless
perceptual ad-blockers operate over rendered web content, the
arms race around page markup obfuscation will likely carry on.
Conversely, we have demonstrated that current visual ad-classifiers
are inherently vulnerable to adversarial examples—the first applica-
tion of these attacks to web-security. We have shown how to craft
near-imperceptible perturbation for ads, ad-disclosures, and native
content, in order to evade or detect ad-blocking with seven different
classifiers. Finally, we have discovered a powerful attack on page-
based ad-blockers, wherein a malicious user fools the model into
blocking content supposedly protected by web-security boundaries.
Our aim was to highlight the fundamental vulnerabilities that
perceptual ad-blockers inherit from existing image classifiers. As
long as defenses to adversarial examples are elusive, perceptual ad-
blockers will be dragged into a new arms race in which they start
from a precariously disadvantaged position—given the stringent
threat model that they must survive.
ACKNOWLEDGMENTS
This work was partially supported by NSF, ONR, the Simons Foun-
dation, a Google faculty fellowship, the Swiss National Science
Foundation (SNSF project P1SKP2_178149), and the German Fed-
eral Ministry of Education and Research (BMBF) through funding
for the CISPA-Stanford Center for Cybersecurity (FKZ: 13N1S0762).
Session 9B: ML Security IIICCS ’19, November 11–15, 2019, London, United Kingdom2017[9] Adblock Plus.
2018.
Customize
Facebook with Adblock Plus.
//issues.adblockplus.org/ticket/7088.
https://facebook.adblockplus.me/.
REFERENCES
[1] Adblock Plus. https://adblockplus.org/
[2] AdblockRadio. https://www.adblockradio.com.
[3] EasyList. https://easylist.to/.
[4] Easylist Forum: Report incorrectly removed content. https://forums.lanik.us/
viewforum.php?f=64&sid=ba948dbdbad9334b72c143f26db58ff0.
[5] Ghostery. https://www.ghostery.com/.
[6] Tesseract. https://github.com/tesseract-ocr/.
[7] uBlock. https://www.ublock.org/.
[8] Adblock Plus. Issue 7088: Implement hide-if-contains-image snippet. https:
[10] Adblock Plus. 2018. Sentinel. https://adblock.ai/.
[11] Anish Athalye, Nicholas Carlini, and David Wagner. 2018. Obfuscated gradients
give a false sense of security: Circumventing defenses to adversarial examples.
In International Conference on Machine Learning (ICML).
[12] Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. 2018. Synthesiz-
ing robust adversarial examples. In International Conference on Machine Learning
(ICML).
[13] Shai Avidan and Ariel Shamir. 2007. Seam carving for content-aware image
resizing. In ACM Transactions on graphics, Vol. 26. 10.
[14] Sruti Bhagavatula, Christopher Dunn, Chris Kanich, Minaxi Gupta, and Brian
Ziebart. 2014. Leveraging machine learning to improve unwanted resource
filtering. In ACM Workshop on Artificial Intelligence and Security. ACM.
[15] Elie Bursztein, Jonathan Aigrain, Angelika Moscicki, and John C Mitchell. 2014.
The End is Nigh: Generic Solving of Text-based CAPTCHAs.. In USENIX Workshop
on Offensive Technologies. USENIX.
[16] Nicholas Carlini and David Wagner. 2017. Adversarial examples are not eas-
ily detected: Bypassing ten detection methods. In ACM Workshop on Artificial
Intelligence and Security. ACM, 3–14.
[17] Nicholas Carlini and David Wagner. 2017. Towards evaluating the robustness of
neural networks. In IEEE Symposium on Security and Privacy. IEEE.
attacks on speech-to-text. In DLS.
Black-Box Adversarial Attacks. arXiv preprint arXiv:1907.05587 (2019).
[20] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. 2017. Targeted
backdoor attacks on deep learning systems using data poisoning. arXiv preprint
arXiv:1712.05526 (2017).
[21] Edward Chou, Florian Tramèr, Giancarlo Pellegrino, and Dan Boneh. 2018. Sen-
tinet: Detecting physical attacks against deep learning systems. arXiv preprint
arXiv:1812.00292 (2018).
[22] Nicolas Christin, Sally S Yanagihara, and Keisuke Kamataki. 2010. Dissecting
one click frauds. In Proceedings of the 17th ACM conference on Computer and
communications security. ACM, 15–26.
[23] Justin Crites and Mathias Ricken. 2004. Automatic ad blocking: Improving
AdBlock for the Mozilla platform. (2004).
[18] Nicholas Carlini and David Wagner. 2018. Audio adversarial examples: Targeted
[19] Steven Chen, Nicholas Carlini, and David Wagner. 2019. Stateful Detection of
[24] Digital Advertising Alliance (DAA). 2009. Self Regulatory Principles for Online
Behavioral Advertising. https://digitaladvertisingalliance.org/sites/aboutads/
files/DAA_files/seven-principles-07-01-09.pdf.
[25] Digital Advertising Alliance (DAA). 2013. DAA Icon Ad Marker Creative Guide-
lines. https://digitaladvertisingalliance.org/sites/aboutads/files/DAA_files/DAA_
Icon_Ad_Creative_Guidelines.pdf.
[26] Benjamin Edelman. 2009. False and Deceptive Display Ads at Yahoo’s Right
Media. http://www.benedelman.org/rightmedia-deception.
[27] Logan Engstrom, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. 2017.
A rotation and a translation suffice: Fooling CNNs with simple transformations.
arXiv preprint arXiv:1712.02779 (2017).
[28] Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Flo-
rian Tramèr, Atul Prakash, Tadayoshi Kohno, and Dawn Song. 2018. Physical
Adversarial Examples for Object Detectors. In USENIX Workshop on Offensive
Technologies. USENIX.
[29] Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei
Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. 2018. Robust Physical-
World Attacks on Deep Learning Visual Classification. In Conference on Computer
Vision and Pattern Recognition (CVPR). IEEE, 1625–1634.
[30] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. 2015. Model inversion
attacks that exploit confidence information and basic countermeasures. In ACM
SIGSAC Conference on Computer and Communications Security. ACM.
[31] Justin Gilmer, Ryan P Adams, Ian Goodfellow, David Andersen, and George E
Dahl. 2018. Motivating the rules of the game for adversarial example research.
arXiv preprint arXiv:1807.06732 (2018).
[32] Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S Schoenholz, Maithra Raghu,
Martin Wattenberg, and Ian Goodfellow. 2018. Adversarial spheres. arXiv preprint
arXiv:1801.02774 (2018).
[33] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining
and harnessing adversarial examples. In International Conference on Learning
Representations (ICLR).
[34] Kathrin Grosse, Praveen Manoharan, Nicolas Papernot, Michael Backes, and
Patrick McDaniel. 2017. On the (statistical) detection of adversarial examples.
arXiv preprint arXiv:1702.06280 (2017).
[35] Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael Backes, and
Patrick McDaniel. 2017. Adversarial perturbations against deep neural networks
for malware classification, In ESORICS. arXiv preprint arXiv:1606.04435.
[36] David Gugelmann, Markus Happe, Bernhard Ager, and Vincent Lenders. 2015.
An automated approach for complementing ad blockers’ blacklists. Privacy
Enhancing Technologies Symposium 2 (2015), 282–298.
[37] Warren He, James Wei, Xinyun Chen, Nicholas Carlini, and Dawn Song. 2017.
Adversarial Example Defenses: Ensembles of Weak Defenses are not Strong.
arXiv preprint arXiv:1706.04701 (2017).
[38] Jovanni Hernandez, Akshay Jagadeesh, and Jonathan Mayer. 2011. Tracking
the trackers: The AdChoices icon. http://cyberlaw.stanford.edu/blog/2011/08/
tracking-trackers-adchoices-icon.
[39] Chao-Yung Hsu, Chun-Shien Lu, and Soo-Chang Pei. 2009. ACM International
conference on Multimedia. In ICM. ACM, 637–640.
[40] Zaeem Hussain, Mingda Zhang, Xiaozhong Zhang, Keren Ye, Christopher
Thomas, Zuha Agha, Nathan Ong, and Adriana Kovashka. 2017. Automatic
understanding of image and video advertisements. In Conference on Computer
Vision and Pattern Recognition (CVPR). IEEE, 1100–1110.
[41] Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. 2018. Black-
box Adversarial Attacks with Limited Queries and Information. In International
Conference on Machine Learning (ICML).
[42] Umar Iqbal, Zubair Shafiq, and Zhiyun Qian. 2017. The ad wars: retrospective
measurement and analysis of anti-adblock filter lists. In Internet Measurement
Conference. ACM, 171–183.
[43] Umar Iqbal, Zubair Shafiq, Peter Snyder, Shitong Zhu, Zhiyun Qian, and Benjamin
Livshits. 2018. AdGraph: A Machine Learning Approach to Automatic and
Effective Adblocking. arXiv preprint arXiv:1805.09155 (2018).
ing.
changing-the-future-of-advertising.
How Alexa Is Changing The Future Of Advertis-
https://www.forbes.com/sites/ilkerkoksal/2018/12/11/how-alexa-is-
[44] Ilker Koksal. 2018.
[45] Zico Kolter and Eric Wong. 2017. Provable defenses against adversarial examples
[47] Viktor Krammer. 2008. An effective defense against intrusive web advertising. In
[46] Georgios Kontaxis and Monica Chew. 2015. Tracking protection in Firefox for
via the convex outer adversarial polytope. In ICML.
privacy and performance. arXiv preprint arXiv:1506.04104 (2015).
Conference on Privacy, Security and Trust. IEEE, 3–14.
[48] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. 2017. Adversarial examples
in the physical world. In International Conference on Learning Representations
(ICLR).
[49] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. 2017. Adversarial Machine
Learning at Scale. In International Conference on Learning Representations (ICLR).
[50] Pedro Giovanni Leon, Justin Cranshaw, Lorrie Faith Cranor, Jim Graves, Manoj
Hastak, Blase Ur, and Guzi Xu. 2012. What do online behavioral advertising pri-
vacy disclosures communicate to users?. In Workshop on Privacy in the electronic
society. ACM, 19–30.
[51] Zhou Li, Kehuan Zhang, Yinglian Xie, Fang Yu, and XiaoFeng Wang. 2012. Know-
ing your enemy: understanding and detecting malicious web advertising. In ACM
SIGSAC Conference on Computer and Communications Security. ACM.
[52] David G Lowe. 2004. Distinctive image features from scale-invariant keypoints.
International journal of computer vision 60, 2 (2004), 91–110.
[53] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
Adrian Vladu. 2018. Towards deep learning models resistant to adversarial attacks.
In International Conference on Learning Representations (ICLR).
[54] Matthew Malloy, Mark McNamara, Aaron Cahn, and Paul Barford. 2016. Ad
blockers: Global prevalence and impact. In Internet Measurement Conference.
ACM, 119–125.
[55] Jan Hendrik Metzen, Tim Genewein, Volker Fischer, and Bastian Bischoff. 2017.
On detecting adversarial perturbations. In International Conference on Learning
Representations (ICLR).
[56] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal
Frossard. 2017. Universal Adversarial Perturbations. In Conference on Computer
Vision and Pattern Recognition (CVPR). IEEE, 1765–1773.
[57] Muhammad Haris Mughees, Zhiyun Qian, and Zubair Shafiq. 2017. Detecting anti
ad-blockers in the wild. In Privacy Enhancing Technologies Symposium, Vol. 2017.
[58] Muhammad Haris Mughees, Zhiyun Qian, Zubair Shafiq, Karishma Dash, and
Pan Hui. 2016. A first look at ad-block detection: A new arms race on the web.
arXiv preprint arXiv:1605.05841 (2016).
[59] Meghan Neal. 2016. You’re Going to Need an Ad Blocker for Your Next
TV. https://motherboard.vice.com/en_us/article/mg7ek8/youre-going-to-need-
an-ad-blocker-for-your-next-tv.
[60] Rishab Nithyanand, Sheharbano Khattak, Mobin Javed, Narseo Vallina-Rodriguez,
Marjan Falahrastegar, Julia E Powles, ED Cristofaro, Hamed Haddadi, and Steven J
Session 9B: ML Security IIICCS ’19, November 11–15, 2019, London, United Kingdom2018[61] Paraska Oleksandr. 2018.
Murdoch. 2016. Adblocking and counter blocking: A slice of the arms race. In
USENIX Workshop on Free and Open Communications on the Internet.
Towards more intelligent ad blocking on the
web. https://medium.com/@shoniko/towards-more-intelligent-ad-blocking-on-
the-web-9f67bf2a12b5.
[62] George Paliy. 2018. The Future Of Advertising In Virtual Reality.
https:
//stopad.io/blog/future-virtual-reality-advertising.
[72] Joseph Redmon and Ali Farhadi. 2018. YOLOv3: An Incremental Improvement.
[63] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik,
and Ananthram Swami. 2017. Practical Black-Box Attacks against Machine
Learning. In ACM ASIA Conference on Computer and Communications Security.
ACM, 506–519.
[64] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik,
and Ananthram Swami. 2016. The limitations of deep learning in adversarial
settings. In IEEE European Symposium on Security and Privacy.
[65] Nicolas Papernot, Patrick McDaniel, Arunesh Sinha, and Michael Wellman. 2016.
Towards the Science of Security and Privacy in Machine Learning. arXiv preprint
arXiv:1611.03814 (2016).
[66] Giancarlo Pellegrino, Martin Johns, Simon Koch, Michael Backes, and Christian
Rossow. 2017. Deemon: Detecting CSRF with Dynamic Analysis and Property
Graphs. In ACM SIGSAC Conference on Computer and Communications Security.
ACM, 1757–1771.
[67] Giancarlo Pellegrino, Christian Rossow, Fabrice J Ryba, Thomas C Schmidt, and
Matthias Wählisch. 2015. Cashing Out the Great Cannon? On Browser-Based
DDoS Attacks and Economics.. In USENIX Workshop on Offensive Technologies.
[68] Enric Pujol, Oliver Hohlfeld, and Anja Feldmann. 2015. Annoyed Users: Ads and
Ad-Block Usage in the Wild. In Internet Measurement Conference. ACM, 93–106.
[69] Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. 2018. Certified defenses
against adversarial examples. In International Conference on Learning Representa-
tions (ICLR).
[70] Joseph Redmon, Santosh Kumar Divvala, Ross B. Girshick, and Ali Farhadi. 2016.
You Only Look Once: Unified, Real-Time Object Detection. In Conference on
Computer Vision and Pattern Recognition (CVPR). IEEE, 779–788.
[71] Joseph Redmon and Ali Farhadi. 2017. YOLO9000: Better, Faster, Stronger. In
Conference on Computer Vision and Pattern Recognition (CVPR), IEEE (Ed.).
arXiv preprint arXiv:1804.02767 (2018).
[73] Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. 2017.
Evolution strategies as a scalable alternative to reinforcement learning. arXiv
preprint arXiv:1703.03864 (2017).
[74] Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Alek-
sander Madry. 2018. Adversarially robust generalization requires more data. In
Advances in Neural Information Processing Systems. 5014–5026.
[75] Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K Reiter. 2016. Ac-
cessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition.
In ACM SIGSAC Conference on Computer and Communications Security. ACM.
[76] Yash Sharma and Pin-Yu Chen. 2017. Attacking the Madry Defense Model with
L_1-based Adversarial Examples. arXiv preprint arXiv:1710.10733 (2017).
[77] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017. Mem-
bership inference attacks against machine learning models. In IEEE Symposium
on Security and Privacy. IEEE, 3–18.
[78] Ashish Kumar Singh and Vidyasagar Potdar. 2009. Blocking online advertising-A
state of the art. In IEEE International Conference on Industrial Technology. IEEE.
[79] Congzheng Song and Vitaly Shmatikov. 2018. Fooling OCR Systems with Adver-
sarial Text Images. arXiv preprint arXiv:1802.05385 (2018).
classifier: A case study. In IEEE Symposium on Security and Privacy.
[81] Grant Storey, Dillon Reisman, Jonathan Mayer, and Arvind Narayanan. 2017. The
Future of Ad Blocking: An Analytical Framework and New Techniques. arXiv
preprint arXiv:1705.08568 (2017).
[82] Grant Storey, Dillon Reisman, Jonathan Mayer, and Arvind Narayanan. 2017. Per-
ceptual Ad Highlighter. Chrome Extension: https://chrome.google.com/webstore/
detail/perceptual-ad-highlighter/mahgiflleahghaapkboihnbhdplhnchp; Source
code: https://github.com/citp/ad-blocking.
[80] Nedim Srndic and Pavel Laskov. 2014. Practical evasion of a learning-based
[83] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
Ian Goodfellow, and Rob Fergus. 2014. Intriguing properties of neural networks.
In International Conference on Learning Representations (ICLR).
[84] Panagiotis Tigas, Samuel T King, Benjamin Livshits, et al. 2019. Percival: Making
In-Browser Perceptual Ad Blocking Practical With Deep Learning. arXiv preprint
arXiv:1905.07444 (2019).
[85] Florian Tramèr and Dan Boneh. 2019. Adversarial Training and Robustness for
Multiple Perturbations. arXiv preprint arXiv:1904.13000 (2019).
[86] Florian Tramèr, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh,
and Patrick McDaniel. 2018. Ensemble adversarial training: Attacks and defenses.
In International Conference on Learning Representations (ICLR).
[87] Florian Tramèr, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas Ristenpart.
2016. Stealing machine learning models via prediction apis. In USENIX Security
Symposium.
issues/3367.
[88] uBlockOrigin. Issue 3367: Facebook. https://github.com/uBlockOrigin/uAssets/
[89] Blase Ur, Pedro Giovanni Leon, Lorrie Faith Cranor, Richard Shay, and Yang Wang.
2012. Smart, useful, scary, creepy: perceptions of online behavioral advertising.