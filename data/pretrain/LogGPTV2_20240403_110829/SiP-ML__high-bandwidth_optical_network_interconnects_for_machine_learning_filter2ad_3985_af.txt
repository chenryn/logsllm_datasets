model parallel training using smart task device placement [121, 122],
and more efficient pipelining strategies [4, 123]. There is also a
significant body of work on new electrical hardware designs to ac-
celerate machine learning computations [118, 124–129]. The work
proposed here is orthogonal to the above mentioned techniques, as
they can still be applied to further improve both data and model
parallel training. Our work differs in that we investigate the sys-
tem requirements of using SiP as a new underlying technology to
interconnect hundreds of GPUs in an all-optical architecture.
Datacenter Interconnects. The broad vision for this paper is to
use all-optical interconnects for future distributed ML systems.
Optical interconnects have a long and rich history in the data-
center research community [24–26, 55, 66, 70, 71, 130–135]. Prior
work shows the benefits of reconfigurable topologies in datacenter
networks by adding optical links to the electrical topology [24,
66, 71, 133, 136] or by creating all-optical datacenter intercon-
nects [26, 55, 70, 131, 132]. The unpredictability of legacy datacenter
workloads and the complexity of managing hybrid topologies are
6Page 6 under GLOBALFOUNDRIES 22 nm FDSOI lists AC14,000/mm2 for 50 samples.
two main reasons for the lack of adoption of all-optical datacen-
ters so far. In contrast, this paper builds an all-optical interconnect
with a simple and practical task placement algorithm primarily
used to accelerate ML workloads. Our ring topology (SiP-Ring) is
inspired by Quartz [70], Mordia [71], and Megaswitch [26]. They
all use a fiber ring to interconnect the datacenter topology, but
they do not leverage MRRs. Moreover, Mordia realizes a microsec-
ond switching circuit switch, but it does not reuse wavelengths,
and this significantly reduces its bandwidth efficiency compared
to SiP-Ring. As a result, Mordia’s number of ports is limited by
the number of wavelengths. Jellyfish [137], Rotornet [66], and
Opera [69] take advantage of the unpredictability of datacenter
workloads and use expander-based topologies to improve the com-
pletion time of short and long flows. Random permutations are
not ideal for ML workloads, as a training workload is a periodic
repetition of thousands of iterations. Shoal [135], Larry [138], XFab-
ric [139], and Sirius [55] have proposed reconfigurable datacenter
interconnects with nanosecond switching fabric. We believe these
proposals have the potential to change the game in datacenter en-
vironments, but they are not commercially available yet and they
do not support Tbps bandwidth between communicating nodes.
Moreover, our results show µs reconfiguration latency is close to
optimal for ML; a control plane with nanosecond response time
might be needed for a general purpose datacenter traffic, but it is
an overkill for distributed ML training. Finally, there is rich body of
research on silicon photonics [17, 140–142], embedding silicon pho-
tonics switches in High Performance Computing clusters [143] and
energy-efficient datacenters [144]. By focusing on ML, our work
takes an application-level perspective to build an interconnect with
SiP components.
7 CONCLUSION
In this paper, we propose optical network interconnects for dis-
tributed ML training clusters capable of providing multiple terabits-
per-second of bandwidth per GPU. Our results show that the pre-
dictability of ML workloads makes them a great fit for optical inter-
connects. We develop a new task partitioning and placement algo-
rithm that exploits the degree requirement of optical networks to
find a parallelization strategy suitable for a given network topology.
We show this approach can mitigate and in fact largely overcome
concerns such as limited communication degree and reconfigura-
bility of optical circuit-switched networks. Simulations using three
real DNN models show that, compared to today’s electrical network
fabrics with limited server-to-server bandwidth, SiP-ML improves
the training time by 1.3–9.1× at scale.
8 ACKNOWLEDGMENTS
We would like to thank our shepherd Hitesh Ballani and anony-
mous reviewers for their feedback. This work was partly supported
by AEPA-E ENLITENED PINE, DARPA FastNICs, DARPA PIPES,
a Cisco Research Center Award, NSF ASCENT-2023468, NSF CNS-
2008624, NSF CNS-1751009, NSF CNS-2006827, NSF CNS-1563826
as well as by a SystemsThatLearn@CSAIL Ignite Grant and a Ma-
chineLearningApplications@CSAIL Award.
668
SiP-ML: Optical Network Interconnects for Machine Learning
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
REFERENCES
[1] AI and Compute. https://openai.com/blog/ai-and-compute/.
[2] Minsik Cho, Ulrich Finkler, David Kung, and Hillery Hunter. Blueconnect:
Decomposing all-reduce for deep learning on heterogeneous network hierarchy.
In SysML Conference, 2019.
1986.
microring resonators. Laser & Photonics Reviews, 6(1):47–73, 2012. https://
onlinelibrary.wiley.com/doi/abs/10.1002/lpor.201100017.
[23] Q. Cheng, M. Bahadori, Y. Hung, Y. Huang, N. Abrams, and K. Bergman. Scalable
microring-based silicon clos switch fabric with switch-and-select stages. IEEE
Journal of Selected Topics in Quantum Electronics, 25(5):1–11, Sep. 2019.
[24] Nathan Farrington, George Porter, Sivasankar Radhakrishnan, Hamid Hajab-
dolali Bazzaz, Vikram Subramanya, Yeshaiahu Fainman, George Papen, and
Amin Vahdat. Helios: A hybrid electrical/optical switch architecture for modular
data centers. SIGCOMM’10, pages 339–350.
[25] Guohui Wang, David G. Andersen, Michael Kaminsky, Konstantina Papagian-
naki, T.S. Eugene Ng, Michael Kozuch, and Michael Ryan. c-Through: Part-time
optics in data centers. SIGCOMM’10, pages 327–338.
[26] Li Chen, Kai Chen, Zhonghua Zhu, Minlan Yu, George Porter, Chunming Qiao,
and Shan Zhong. Enabling wide-spread communications on optical fabric with
megaswitch. In 14th USENIX Symposium on Networked Systems Design and Im-
plementation (NSDI 17), pages 577–593, Boston, MA, 2017. USENIX Association.
[27] Pengtao Xie, Jin Kyu Kim, Yi Zhou, Qirong Ho, Abhimanu Kumar, Yaoliang
Yu, and Eric Xing. Lighter-communication distributed machine learning via
sufficient factor broadcasting. In Proceedings of the Thirty-Second Conference on
Uncertainty in Artificial Intelligence, pages 795–804, Arlington, Virginia, USA,
2016. AUAI Press.
[28] Mu Li, David G. Andersen, Jun Woo Park, Alexander J. Smola, Amr Ahmed, Vanja
Josifovski, James Long, Eugene J. Shekita, and Bor-Yiing Su. Scaling distributed
machine learning with the parameter server. OSDI’14, pages 583–598. USENIX
Association, 2014.
[29] Rajeev Thakur, Rolf Rabenseifner, and William Gropp. Optimization of collec-
tive communication operations in mpich. Int. J. High Perform. Comput. Appl.,
19(1):49–66, February 2005.
[30] Baidu, 2017. https://github.com/baidu-research/baidu-allreduce.
[31] Xianyan Jia, Shutao Song, Wei He, Yangzihao Wang, Haidong Rong, Feihu
Zhou, Liqiang Xie, Zhenyu Guo, Yuanzhou Yang, Liwei Yu, Tiegang Chen,
Guangxiao Hu, Shaohuai Shi, and Xiaowen Chu. Highly scalable deep learning
training system with mixed-precision: Training imagenet in four minutes. CoRR,
abs/1807.11205, 2018.
[32] J. R. Quinlan. Induction of decision trees. Mach. Learn., 1(1):81–106, March
[33] Seunghak Lee, Jin Kyu Kim, Xun Zheng, Qirong Ho, Garth A Gibson, and Eric P
Xing. On model parallelization and scheduling strategies for distributed machine
learning. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q.
Weinberger, editors, Advances in Neural Information Processing Systems 27, pages
2834–2842. Curran Associates, Inc., 2014.
[34] Zhihao Jia, Sina Lin, Charles R. Qi, and Alex Aiken. Exploring hidden dimensions
in accelerating convolutional neural networks. volume 80 of Proceedings of
Machine Learning Research, pages 2274–2283, StockholmsmÃďssan, Stockholm
Sweden, 10–15 Jul 2018. PMLR.
[35] Tal BenNun and Torsten Hoefler. Demystifying parallel and distributed deep
learning: An in-depth concurrency analysis. CoRR, abs/1802.09941, 2018.
[36] L. Song, F. Chen, Y. Zhuo, X. Qian, H. Li, and Y. Chen. Accpar: Tensor partition-
ing for heterogeneous deep learning accelerators. In 2020 IEEE International
Symposium on High Performance Computer Architecture (HPCA), pages 342–355,
2020.
[37] Nikoli Dryden, Naoya Maruyama, Tim Moon, Tom Benson, Marc Snir, and
Brian Van Essen. Channel and filter parallelism for large-scale cnn training.
In Proceedings of the International Conference for High Performance Computing,
Networking, Storage and Analysis, SC’19, New York, NY, USA, 2019. Association
for Computing Machinery.
[38] Zhihao Jia, Matei Zaharia, and Alex Aiken. Beyond data and model parallelism
for deep neural networks. SysML, 2019.
[39] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao,
Marc aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, Quoc V. Le, and
Andrew Y. Ng. Large scale distributed deep networks. In F. Pereira, C. J. C.
Burges, L. Bottou, and K. Q. Weinberger, editors, Advances in Neural Information
Processing Systems 25, pages 1223–1231. Curran Associates, Inc., 2012.
[40] Amir Gholami, Ariful Azad, Kurt Keutzer, and Aydin Buluç. Integrated model
and data parallelism in training neural networks. CoRR, abs/1712.04432, 2017.
[41] Ravichandra Addanki, Shaileshh Bojja Venkatakrishnan, Shreyan Gupta, Hongzi
Mao, and Mohammad Alizadeh. Learning generalizable device placement al-
gorithms for distributed machine learning. In Advances in Neural Information
Processing Systems 32, pages 3983–3993. Curran Associates, Inc., 2019.
[42] Shar Narasimhan. NVIDIA Clocks World’s Fastest BERT Training Time and
Largest Transformer Based Model, Paving Path For Advanced Conversational
AI, Aug. 2019. https://devblogs.nvidia.com/training-bert-with-gpus/.
[43] Nikoli Dryden, Naoya Maruyama, Tom Benson, Tim Moon, Marc Snir, and
Brian Van Essen. Improving strong-scaling of cnn training by exploiting finer-
grained parallelism, 2019.
[44] Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better:
Closing the generalization gap in large batch training of neural networks. In
669
selene-busy/.
[3] Siddharth Das. CNN Architectures, 2017.
[4] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R.
Devanur, Gregory R. Ganger, Phillip B. Gibbons, and Matei Zaharia. Pipedream:
Generalized pipeline parallelism for dnn training. In Proceedings of the 27th
ACM Symposium on Operating Systems Principles, SOSP âĂŹ19, page 1âĂŞ15,
New York, NY, USA, 2019. Association for Computing Machinery.
[5] NVIDIA DGX A100. https://www.nvidia.com/en-us/data-center/dgx-a100/.
https://blogs.nvidia.com/blog/2020/12/18/nvidia-
[6] NVIDIA Selene Cluster.
[7] S S Vazhkudai, B R de Supinski, A S Bland, A Geist, J Sexton, J Kahle, C J Zimmer,
S Atchley, S H Oral, D E Maxwell, V G Vergara Larrea, A Bertsch, R Goldstone,
W Joubert, C Chambreau, D Appelhans, R Blackmore, B Casses, G Chochia,
G Davison, M A Ezell, E Gonsiorowski, L Grinberg, B Hanson, B Hartner, I Karlin,
M L Leininger, D Leverman, C Marroquin, A Moody, M Ohmacht, R Panka-
jakshan, F Pizzano, J H Rogers, B Rosenburg, D Schmidt, M Shankar, F Wang,
P Watson, B Walkup, L D Weems, and J Yin. The design, deployment, and
evaluation of the coral pre-exascale systems. 7 2018.
[8] Valerie Coffey. DARPA PIPES Program demonstrates 2 Tbit/s optical intercon-
nects at the chip level, July 2020. https://www.laserfocusworld.com/fiber-
optics/article/14176186/darpa-pipes-program-demonstrates-2-tbits-optical-
interconnects-at-the-chip-level.
[9] Mark Wade. Optical i/o chiplets eliminate bottlenecks to unleash innovation,
2020. https://ayarlabs.com/ayar-labs-solving-critical-computing-challenges-
through-optical-i-o/.
[10] Yutaka Urino, Takahiro Nakamura, and Yasuhiko Arakawa. Silicon Optical
Interposers for High-Density Optical Interconnects, pages 1–39. Springer Berlin
Heidelberg, Berlin, Heidelberg, 2016.
[11] D. Kim, K. Y. Au, H. Y. L. X. Luo, Y. L. Ye, S. Bhattacharya, and G. Q. Lo. 2.5d silicon
optical interposer for 400 gbps electronic-photonic integrated circuit platform
packaging. In 2017 IEEE 19th Electronics Packaging Technology Conference (EPTC),
pages 1–4, Dec 2017.
[12] E. R. H. Fuchs, R. E. Kirchain, and S. Liu. The future of silicon photonics: Not
so fast? insights from 100g ethernet lan transceivers. Journal of Lightwave
Technology, 29(15):2319–2326, Aug 2011.
[13] David Thomson, Aaron Zilkie, John E Bowers, Tin Komljenovic, Graham T Reed,
Laurent Vivien, Delphine Marris-Morini, Eric Cassan, Leopold Virot, Jean-Marc
Fedeli, Jean-Michel Hartmann, Jens H Schmid, Dan-Xia Xu, Frederic Boeuf,
Peter O’Brien, Goran Z Mashanovich, and M Nedeljkovic. Roadmap on silicon
photonics. Journal of Optics, 18(7):073003, 2016.
[14] M. Wade, M. Davenport, M. De Cea Falco, P. Bhargava, J. Fini, D. Van Orden,
R. Meade, E. Yeung, R. Ram, M. Popovic, V. Stojanovic, and C. Sun. A bandwidth-
dense, low power electronic-photonic platform and architecture for multi-tbps
optical i/o. pages 1–3, Sep. 2018.
[15] N. Ophir, C. Mineo, D. Mountain, and K. Bergman. Silicon photonic microring
links for high-bandwidth-density, low-power chip i/o. IEEE Micro, 33(1):54–67,
Jan 2013.
[18] Madeleine Glick, Lionel C. Kimmerling, and Robert C. Pfahl. A roadmap for
[16] G.T. Reed and A.P. Knights. Silicon Photonics: An Introduction. Wiley, 2004.
[17] Qixiang Cheng, Meisam Bahadori, Madeleine Glick, Sebastien Rumley, and
Keren Bergman. Recent advances in optical technologies for data centers: a
review. Optica, 5(11):1354–1370, Nov 2018.
integrated photonics. Opt. Photon. News, 29(3):36–41, Mar 2018.
[19] Amir H. Atabaki, Sajjad Moazeni, Fabio Pavanello, Hayk Gevorgyan, Jelena
Notaros, Luca Alloatti, Mark T. Wade, Chen Sun, Seth A. Kruger, Huaiyu Meng,
Kenaish Al Qubaisi, Imbert Wang, Bohan Zhang, Anatol Khilo, Christopher V.
Baiocco, Milovs A. Popovic, Vladimir M. Stojanovic, and Rajeev J. Ram. Integrat-
ing photonics with silicon nanoelectronics for the next generation of systems
on a chip. Nature, 556(7701):349–354, 2018.
[20] Mark Wade, Erik Anderson, Shahab Ardalan, Pavan Bhargava, Sidney Buch-
binder, Michael Davenport, John Fini, Anatoly Khilo, Chandru Ramamurthy
Roy Meade, Michael Rust, Vladimir Stojanovic Forrest Sedgwick, Derek Van
Orden, Chong Zhang Edward Wang, Chen Sun, Sergey Shumarayev, Conor
O’Keeffe, Tim T. Hoang, David Kehlet, Ravi V. Mahajan, Allen Chan, and Tina
Tran. TeraPHY: A Chiplet Technology for Low-Power, High-Bandwidth Optical
I/O. HotChips, pages i–xlviii, August 2019. https://www.hotchips.org/hc31/
HC312 .9AyarLabs20190820HCFINAL.pdf.
[21] Valentina Donzella, Ahmed Sherwali, Jonas Flueckiger, Samantha M. Grist,
Sahba Talebi Fard, and Lukas Chrostowski. Design and fabrication of soi micro-
ring resonators based on sub-wavelength grating waveguides. Opt. Express,
23(4):4791–4803, Feb 2015.
[22] W. Bogaerts, P. De Heyn, T. Van Vaerenbergh, K. De Vos, S. Kumar Selvaraja,
T. Claes, P. Dumon, P. Bienstman, D. Van Thourhout, and R. Baets. Silicon
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
M. Khani et al.
Proceedings of the 31st International Conference on Neural Information Processing
Systems, NIPS’17, pages 1729–1739, Red Hook, NY, USA, 2017. Curran Associates
Inc.
[45] Priya Goyal, Piotr Dollár, Ross B. Girshick, Pieter Noordhuis, Lukasz Wesolowski,
Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large
minibatch SGD: training imagenet in 1 hour. CoRR, abs/1706.02677, 2017.
[46] Christopher J. Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein,
Roy Frostig, and George E. Dahl. Measuring the effects of data parallelism on
neural network training. Journal of Machine Learning Research, 20(112):1–49,
2019.
[47] Yosuke Oyama, Naoya Maruyama, Nikoli Dryden, Erin McCarthy, Peter Har-
rington, Jan Balewski, Satoshi Matsuoka, Peter Nugent, and Brian Van Essen.
The case for strong scaling in deep learning: Training large 3d cnns with hybrid
parallelism. IEEE Transactions on Parallel and Distributed Systems, 2020.
[48] MLPerf v0.6: NVIDIA Implementation of Attention Mechanisms for Translation,
Aug. 2019. https://github.com/mlperf/trainingresultsv0.6/tree/master/NVIDIA/
benchmarks/transformer/implementations/pytorch.
https://
[49] ResNet v1.5 for TensorFlow, 2020.
[50] NVIDIA Data Center Deep Learning Product Performance.
[51] Nvidia DGX-2. https://www.nvidia.com/content/dam/en-zz/Solutions/Data-
[52] MegatronLM: Training Billion+ Parameter Language Models Using GPU Model
developer.nvidia.com/deep-learning-performance-training-inference.
Center/dgx-2/dgx-2-print-datasheet-738070-nvidia-a4-web-uk.pdf.
Parallelism, Jul. 2019. https://nv-adlr.github.io/MegatronLM.
[53] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero:
Memory optimizations toward training trillion parameter models, 2019. https:
//www.deepspeed.ai/.
[54] Saeed Rashidi, Srinivas Sridharan, Sudarshan Srinivasan, Matthew Denton, and
Tushar Krishna. Efficient communication acceleration for next-gen scale-up
deep learning training platforms, 2020.
[55] Hitesh Ballani, Paolo Costa, Raphael Behrendt, Daniel Cletheroe, Istvan Haller,
Krzysztof Jozwik, Fotini Karinou, Sophie Lange, Kai Shi, Benn Thomsen, and
Hugh Williams. Sirius: A Flat Datacenter Network with Nanosecond Optical
Switching. SIGCOMM’20, Aug. 2020.
[56] H. Esmaeilzadeh, E. Blem, R. S. Amant, K. Sankaralingam, and D. Burger. Dark
In 2011 38th Annual International
silicon and the end of multicore scaling.
Symposium on Computer Architecture (ISCA), pages 365–376, June 2011.
[57] R. Colwell. The chip design game at the end of moore’s law. In 2013 IEEE Hot
Chips 25 Symposium (HCS), pages 1–16, Aug 2013.
[58] H. J. S. Dorren, E. H. M. Wittebol, R. de Kluijver, G. Guelbenzu de Villota, P. Duan,
and O. Raz. Challenges for optically enabled high-radix switches for data center
networks. Journal of Lightwave Technology, 33(5):1117–1125, March 2015.
[59] Alexis BjÃűrlin and Manish Mehta. Broadcom discusses its co-packaged optics
plans. http://www.gazettabyte.com/home/2021/4/27/broadcom-discusses-its-
co-packaged-optics-plans.html, 2021. [Online; last accessed 25-June-2021].
[60] Steven Leibson. Ayar labs and Intel demo FPGA with optical transceivers
in DARPA PIPES project: 2 Tbps now, >100 Tbps is the goal, Mar.
2020. https://blogs.intel.com/psg/ayar-labs-and-intel-demo-fpga-with-optical-