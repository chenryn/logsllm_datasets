iﬀs (e.g., based on the purchased raw capacity or on the
95th percentile of the traﬃc) used in practice to the in-
troduced policies (e.g., Volume-Customer and 95Percentile-
Customer).
i = cl ·
cl
i(t)
maxt∈T xl
&j∈N maxt∈T xl
j(t)
• Aggregate-Peak-Device.
Backbone operators
plan the capacity of the network based on the max-
imum utilization, e.g., the 50% of the capacity of a
device is larger than the expected maximum of the
traﬃc that traverses it. Accordingly, we distribute
the cost of the devices based on the contribution of
individual customers to the peak utilization. Assum-
ing that the peak utilization of device l happens at
j(t), we allocate the
following cost to customer i as:
time step tm = arg maxt&j∈N xl
&j∈N xl
cl
i = cl ·
xl
i(tm)
j(tm)
We evaluate F-discrepancies by comparing a policy with
Aggregate-Peak-Device, which shares the costs in a fair way
when it is guaranteed that new unallocated capacity from
an upgrade will soon ﬁnd a customer to amortize it. The
F-discrepancies of the policies arise from the misalignment
of traﬃc peaks: the peak of a customer’s traﬃc may not
coincide with the peak of the aggregate traﬃc the device
carries.
Illustrative example:
Let us assume that two cus-
tomers utilize device l1 with the time-series depicted in
Fig. 1. Based on the time-series we compute the costs
of the customers. The percentages of the cost that cus-
tomer 1 covers are 69.9%, 56.2%, 60%, and 53.3% of
the total cost of the device for the Volume-Customer,
Peak-Customer, 95Percentile-Customer, and Aggregate-
Peak-Device policies, respectively. For example, in case of
the 95Percentile-Customer policy, customer 1 has a traﬃc of
0.9 Gbps while the cumulative traﬃc is 1.5 Gbps resulting
in 60% cost share. The main cause behind the discrepancies
of the costs are the misalignment of the customers’ peak,
511and that the diﬀerent policies consider diverse parts of the
time-series to compute a value that describes the traﬃc of
the customer.
Figure 1: Illustrative example for F-discrepancies.
The misalignment of the customers’ peak causes cost
diﬀerences across policies.
The introduced policies quantify the cost of the customers
using diﬀerent functions on a per device basis. The F-
discrepancies of the customers emerge at two diﬀerent levels:
• Device-level discrepancies. We compute sepa-
rately for each network device the discrepancy among
diﬀerent policies based on the customers’ costs. In this
case, the set of costs is
)cl
i | ∀i ∈ N* , l ∈ L
and has cardinality |N||L|.
For example, the F-
discrepancy of customer i in case of policies a and b is
d(al
i, bl
i).
• Network-level discrepancies. We ﬁrst summarize
the costs of a customer over all the devices of the net-
work, i.e., we compute the total cost of each customer.
Afterwards, we compute the discrepancies of the poli-
cies. In this case, the set of the costs over which we
compute the discrepancies is
(7)
(8)
i | ∀i ∈ N,
cl
+$l∈L
i,&l∈L bl
i).
3.2 M-discrepancies
d(&l∈L al
and has cardinality |N|.
cies a and b the F-discrepancy of customer i
For example,
for poli-
is
The traﬃc metering method is the second source of dis-
crepancies. The resource requirements of the traﬃc moni-
toring tools depend on the resolution of metering. The main
cause behind the M-discrepancies is the trade-oﬀ that back-
bone operators face:
increasing the precision of the meter-
ing improves the validity of the quantiﬁed cost, however,
this comes with an elevated cost for traﬃc monitoring. We
study the two corner cases of traﬃc metering:
• Customer-Ingress.
Each customer has several
ingress devices through which it injects its traﬃc to
the network. The backbone operator keeps track of
the customers’ usage solely on the ingress devices.
This is the least expensive metering method. The
operator uses the ingress traﬃc time-series to share
the network-wide expenditures among the customers.
• Customer-per-Device.
If the backbone operator
deploys more advanced network monitoring tools, it
can capture the time-series of the customers not only
on the ingress devices but on all the devices located
in the network. This is the most expensive metering
method and is typically done using NetFlow technol-
ogy, which comes at a high procurement and adminis-
tration cost. Metering the actual traﬃc on each net-
work device allows the backbone operator to compute
the costs of the customers based on the device speciﬁc
time-series. Therefore, the backbone operator faces a
trade-oﬀ: more accurate expenditure sharing vs. more
cost eﬃcient operation.
We deﬁne the M-discrepancies as follows. First, we com-
pute the cost of customer i on each device l using a given cost
allocation function (e.g., based on the Volume-Customer
policy of Section 3.1), and we compute the network-level
i. Second, we compute using
the given cost allocation function the customer’s share (c∗i )
traﬃc time-series of the customers. The total ingress traﬃc
xl
i(t) where Ii denotes the
set of ingress devices that customer i has. Accordingly, the
M-discrepancy of customer i is
cost of customer i as &l∈L cl
of the network’s total cost (c = &l∈L cl) using the ingress
of customer i is x∗i (t) = &l∈Ii
di#$l∈L
i, c∗i%
(9)
cl
where di is our metric of discrepancy (Eq. 1).
Illustrative example: Let us now assume that the ingress
traﬃc of the customers is as we show in Fig. 2. The backbone
network consists of two devices: L1 on which the traﬃc of
the customers is as depicted in Fig. 1 and L2 which is solely
utilized by customer 1 with a constant traﬃc of 1 Gbps. For
illustration purposes, we separate the two ﬂows of customer
1, one on L1 and the other on L2, with a dashed line in the
Fig. 2. Because the traﬃc on device L2 is modest it can
be transmitted on a 2.5 Gbps device while the capacity of
L1 should be 10 Gbps. The diverse device capacities imply
diverse costs as well. In the case of ingress metering, i.e.,
sharing the cost of the network just based on the aggregate
traﬃc shown in Fig. 2, the cost of customer 1 is 83.9%,
73.1%, 76%, and 72% of the cost of the whole network for the
Volume-Customer, Peak-Customer, 95Percentile-Customer,
and Aggregate-Peak-Device policies, respectively. However,
if we measure the traﬃc of the customers on all the devices
then customer 1’s shares of costs are 75.9%, 65%, 68%, and
62.7%. If we compare these cost fractions we encounter large
discrepancies caused by the level of the metering.
3.3 L-discrepancies
The third type of discrepancies is caused by the diﬀerent
types of customer liability as discussed in Section 2. We will
examine the following policies:
• Aggregate-Peak-Device. This is the already in-
troduced policy that is the measure of fairness when
the customer liability is proportional to the aggregate
peak of devices.
• Trigger. With this policy, the backbone operator allo-
cates the cost of the device exclusively to the customer
5122 customer leaves from P at t1 then the peak will
move to t2 and X will go from paying 90% to paying
only 1% after a tiny 2 perturbation of the aggregate
traﬃc. On the contrary, the Shapley policy is aware
of such situations as it takes into account all the local
maxima of the aggregate traﬃc in quantifying the
costs of the customers.
Under the Shapley policy, the cost of each customer
is proportional to its average marginal contribution to
the device’s total cost. Particularly, let us consider
all the possible S ⊂ N subsets (coalitions) of the cus-
tomers who utilize resources of the network device l.
The cost of coalition S depends on the aggregate traf-
ﬁc volume of the participants, i.e., it is equal to the
cost of a network device that has suﬃcient capacity:
vl(S) = C#max
t∈T $j∈S
j (t)%
xl
(11)
Based on the v cost function of the coalitions, we com-
pute the Shapley value of customer i as
φi(vl) =
1
N! $Π∈SN-vl (S (Π, i)) − vl (S (Π, i) \ i).
(12)
where Π is a permutation of arrival order of the set
N and S(Π, i) denotes the set of players who arrived
no later than i. The (φ1(v), . . . ,φ N (v)) Shapley values
describe the fair distribution of costs in the case of the
S = N grand coalition. Fair in a way that it satisﬁes
four intuitive fairness criteria [1, 13, 22]. We quantify
the cost of customer i based on its Shapley value for
the device l as
cl
i = cl ·
φi(vl)
&j∈N φj(vl)
(13)
While computing the aggregate traﬃc volumes of the
coalitions, we assume that the routing inside the net-
work is static, i.e., removing some traﬃc from the net-
work device does not aﬀect the traﬃc volumes of other
customers (e.g., the backbone operator does not apply
load balancing mechanisms).
Illustrative example: We present the traﬃc patterns
of two customers and the thresholds where the capacity of
the device needs to be upgraded in Fig. 3. Customer 1 is
liable for 53.3% and 87.5% of the cost of the device in case of
the Aggregate-Peak-Device and Shapley policies. The peak
of the aggregate traﬃc happens in a time step where the
customers’ traﬃc volumes are balanced. Although there are
local maxima where the traﬃc of customer 2 is small, it is
not considered by the Aggregate-Peak-Device policy. From
a Shapley policy viewpoint, the traﬃc peak of customer 1
is too large to be transmitted with a lower-capacity device,
i.e., its traﬃc is mainly responsible for the total cost of the
device. If we assume that customer 1 arrived ﬁrst it causes
100% of the costs according to the Trigger policy because its
peak needs a larger-capacity device whose leftover capacity
can be used by customer 2 afterwards.
Customers can have both device- or network-level L-
discrepancies, depending on whether we consider the costs
Figure 2: Illustrative example for M-discrepancies:
metering traﬃc only at the ingress link causes cus-
tomer 1 to have a larger share of the cost than if we
meter at all devices.
that triggered the capacity upgrade. This policy is ap-
plied when the backbone operator is not conﬁdent that
it can sell the newly obtained but unallocated capac-
ity.2
To this end, the backbone operator utilizes the histor-
ical traﬃc patterns of the customers and their arriving
order. For example, the cost of the ﬁrst customer is
equal to the cost of the device that is capable to trans-
mit his traﬃc demand. We assume that the customers
are numbered based on their arriving order while ti
denotes the time when the customer started to use the
network. Accordingly, the cost of customer i in case of
the Trigger policy is
cl
i = C(max
t≤ti $j∈{N|j≤i}
xl
j (t))− C(max
t≤ti $j∈{N|j<i}
xl
j (ti))
(10)
The main drawbacks of Trigger are: a) it assigns cost
only to the customer whose traﬃc trigger upgrades and
0 to everyone else, therefore order of arrival can have a
huge impact on the costs attributed to a customer; and
b) it is diﬃcult to compute Trigger since it requires ex-
tensive historical data on the order of customer arrival
and traﬃc build up.
• Shapley.
The Shapley cost sharing policy lies
the
between the two above presented extremes;
It
Aggregate-Peak-Device and Trigger policies.
assigns to customers partial
liability for upgrades,
thereby avoiding the all-or-nothing assignments of
Triggers. Therefore it is less strict than Trigger
but more strict than Aggregate-Peak-Device since it
assigns “averaged” liabilities rather than proportional
liabilities based on a single time interval when a device
peaks.
the Shapley over
The main advantage of
the
Aggregate-Peak-Device policy is that its allocations
are more stable than that of the Aggregate-Peak-
Device policy in view of customer churn. For example,
let us imagine that the aggregate peak traﬃc of a
device is P and appears at t1. We also assume that
the device at time t2 has load P − . Now let us
suppose that customer X is responsible for 90% of
P at t1 and 1% of P −  at t2. Then if a small
2Recall that upgrades generally involve large jumps that can
leave substantial unallocated capacity.
5133.5 Discussion
The cost sharing methods introduced strike various trade-
oﬀs in terms of computational complexity, amount of re-
quired information, and accuracy. The Volume-Customer,
Peak-Customer, Aggregate-Peak-Device, and 95Percentile-
Customer policies require the least computational resources
due to their sum, maximum, and percentile computations.
The Trigger method determines when the device upgrade
thresholds are surpassed. Finally, the Shapley policy has
the largest complexity as it computes the costs based on the
sub-coalitions of the customers. For computational reasons,
we consider the 15 largest customers per network device in
our evaluations to quantify the Shapley costs. On average,
these customers cover 96% of the traﬃc of the devices.
In terms of the amount of information, all the policies ex-
cept the Shapley and Trigger are similarly modest. They
utilize single values for the historical usage (sum and max-
imum, respectively) and the current traﬃc volumes. How-
ever, the Shapley policy uses the whole time-series of traﬃc
volumes to compute the costs. The information need of the
Trigger policy is even larger as it needs both the historical