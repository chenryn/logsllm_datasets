Theword-embedding-basedapproachsimplysumsupembedding 𝒊𝒊 𝒊𝒊
𝒗𝒗𝒍𝒍𝒊𝒊 𝒖𝒖𝒍𝒍𝒊𝒊
resultsofwordsinanalert,neglectingvaryingcontributionsof
words to the overall semantics. The topic-distillation-based ap- CBOW IDF
proachonlydistillsthegeneraltopicofthealert,whicharetoo
crudetocapturedistinctivesemanticdetails.
Torepresentthealertsemanticinformation,weproposeASR alert content words
(AlertSemanticsRepresentation),whichextractsthecompletealert 𝑾𝑾𝒊𝒊
semanticsbasedontherespectivesemanticcontributionofeach Figure3:Theprocessofrepresentingthesemanticinforma-
wordinthealertcontent.Forwordsinthealertcontent,ASRfirst tionofthealert.
minesthecontextualinformationofeachword,andthencalculates
thecontributionofeachwordtotheoverallalertsemantics.Finally,
ASRaggregatesthecontextualinformationofalertwordsaccording Inthetrainingstage,weseparatelytrainCBOWandIDFby
totheircontributionstotheoverallalertsemantics.Therefore,the contents in history alerts. Then, as shown in Figure 3, we can
keyissueishowtominethecontextualinformationofthealert straightforwardlyextractthesemanticrepresentationforanalert.
ICSE’22,May21–29,2022,Pittsburgh,PA,USA JiaChen,PengWang,andWeiWang
Foreachwordinthealert,𝑒 𝑖 (1≤𝑖 ≤𝑛),wegetthecontextualin- resultof𝐹 𝑖,Equation(3)measuresthedifferencebetween𝐹 𝑖′and𝐹ˆ 𝑖.
formationofthewordbyCBOW,denotedas𝑣𝑖 𝑗 (1≤ 𝑗 ≤𝑙 𝑖),andthe Aftertraining,theresultofthehiddenlayerintheneuralnetwork,
semanticcontributionofthewordbyIDF,denotedas𝑢𝑖 𝑗.Itshould denotedas𝑏 𝑖,isabletoreservethecommonbehaviorinformation
benotethatinordertoensure(cid:205)𝑙 𝑗𝑖 =1𝑢𝑖 𝑗 =1,𝑢𝑖 𝑗isnormalized.Atlast, betweentheoccurrencesseriesof𝑒 𝑖 anditscorrelatedalerts.
accordingtoEquation(1),wecanaggregatethesemanticsofwords (cid:213)𝑐𝑖
inthealertcontenttoobtainthecompletesemanticrepresentation 𝐹ˆ 𝑖 = 𝐹 𝑟𝑖 (2)
𝑗
ofthealert,denotedas𝑠 𝑖. 𝑗=1
𝑛
=(cid:213)𝑙𝑖 𝐿 𝐴𝐵𝑅 =−(cid:213) 𝑖=1𝑙𝑜𝑔( 1+𝑒𝑥𝑝(1 𝑖)) (3)
𝑠 𝑖 (𝑣𝑖 𝑗 ·𝑢𝑖 𝑗) (1) −𝐹 𝑖′·𝐹ˆ
𝑗=1
6 ALERTCORRELATION
5.2 BehaviorRepresentation
WithASRandABR,wecanrepresentthesemanticinformation
Inadditiontosemantics,alertsbelongingtothesamesystemfailure andthebehaviorinformationofthealert,respectively.Wethus
usuallyalsohavecommonbehaviorinformation.Miningfrequent proposeamodel,ACT(AlertCorrelaTion),tomeasurethecorre-
patternsisawidelyusedapproachtocapturebehaviorcorrelations lationbetweenalertsbyaggregatingthesemanticandbehavior
betweenalerts[10,12,28].Becausefrequentpatternsindicateco- representationsofthealert.BeforeformallyintroducingACT,let’s
occurrencecorrelationsbetweenalerts.Inpractice,however,some consideramotivatingquestion,fortwoalerts𝑒 1and𝑒 2,whenboth
alerttypeshavequitelowfrequenciestowhichfrequentpattern theirsemanticandbehaviorrepresentationsaresingle-dimensional,
miningapproachisnotapplicable. howtodetermineiftheyarecorrelated.Generally,forthesemantic
Torepresentthealertbehaviorinformation,weproposeABR representationsof𝑒 1and𝑒 2,thedifferencebetweenthemshouldbe
(AlertBehaviorRepresentation),whichforthefirsttimelearnsthe lessthanathreshold,andthesameistrueforthebehaviorrepresen-
commonalitybetweenalertoccurrenceseries.ABRisinspiredby tations.Formally,wedefineasimpleauxiliaryfunction,𝑓 𝑑𝑖𝑠(𝑥,𝑦,𝑧),
thewordembeddingmodel,Skip-Gram[19].Skip-Gramcaptures inEquation(4),whichfirstcalculatesthesquaredifferencebetween
the common semantics between a target word and its adjacent 𝑥 and𝑦,andmeasurethedifferencebetweenthesquaredifference
wordsviaashallowneuralnetwork,whichconvertstheencod- and𝑧.Therefore,if𝑒 1and𝑒 2arecorrelated,both𝑓 𝑑𝑖𝑠(𝑠 1,𝑠 2,𝑡ℎ𝑟𝑑 1)
ingofthetargetwordtotheencodingofitsadjacentwordsina and𝑓 𝑑𝑖𝑠(𝑏 1,𝑏 2,𝑡ℎ𝑟𝑑 2)arepositive,where𝑡ℎ𝑟𝑑 1and𝑡ℎ𝑟𝑑 2arethe
linearfashion.Similarly,asshowninFigure4,ABRtrainsashal- thresholdsforthesemanticdifferenceandthebehaviordifference,
lowneuralnetworktoconvertstheoccurrenceseriesofatarget respectively.Identically,toensure𝑒 1and𝑒 2arecorrelated,wecan
alerttotheoccurrenceseriesofitscorrelatedalerts.Asaresult, get𝑟𝑒𝑙𝑢(𝑓 𝑑𝑖𝑠(𝑠 1,𝑠 2,𝑡ℎ𝑟𝑑 1))×𝑟𝑒𝑙𝑢(𝑓 𝑑𝑖𝑠(𝑏 1,𝑏 2,𝑡ℎ𝑟𝑑 2)) >0.
thetrainedneuralnetworkcancapturetheunderlyingcommon
behaviorinformationbetweenalerts. 𝑓 𝑑𝑖𝑠(𝑥,𝑦,𝑧)=𝑧−(𝑥−𝑦)2 (4)
ACT,showninFigure5,isageneralizationoftheabovemoti-
hidden layer
vatingexampleinamulti-dimensionalsituation.Fortwoalerts,
𝑒 𝑖 and𝑒 𝑗 (1 ≤𝑖,𝑗 ≤𝑛),inspiredbytheaboveexample,ACTfirst
calculatesthesquaredifferencebetweeneachdimensionofeach
𝒊𝒊
𝑭𝑭𝒓𝒓𝑷𝑷 typeofalertrepresentations.Forthesemanticrepresentation,the
SUM 𝑭𝑭𝒓𝒓. .𝑷𝑷𝒊𝒊 r re es su ul lt ti is sd de en no ot te ed da as s𝑠 𝑏𝑖 𝑖, ,𝑗 𝑗, .a In nd abfo or vt eh ee xb ae mh pa lv ei ,o 𝑓r 𝑑r 𝑖𝑠e (p 𝑠r 1e ,s 𝑠e 2n ,𝑡t ℎa 𝑟ti 𝑑o 1n ), at nh de
.
𝑭𝑭𝒊𝒊 𝒃𝒃𝒊𝒊 𝑭𝑭𝒊𝒊
𝑓 𝑑𝑖𝑠(𝑏 1,𝑏 2,𝑡ℎ𝑟𝑑 2)actuallycanberegardedastwolineartransfor-
mationofsquaredifferencesbetweeneachtypeofalertrepresenta-
𝒊𝒊
𝑭𝑭𝒓𝒓𝒄𝒄𝒊𝒊 tions,where𝑡ℎ𝑟𝑑 1and𝑡ℎ𝑟𝑑 2arebiases.Thus,ACTcorrespondingly
performslineartransformationon𝑠 𝑖,𝑗 and𝑏 𝑖,𝑗,respectively.Inad-
Figure4:Theprocessofrepresentingthebehaviorinforma- dition,since𝑟𝑒𝑙𝑢(𝑓 𝑑𝑖𝑠(𝑠 1,𝑠 2,𝑡ℎ𝑟𝑑 1))×𝑟𝑒𝑙𝑢(𝑓 𝑑𝑖𝑠(𝑏 1,𝑏 2,𝑡ℎ𝑟𝑑 2)) >0
tionofthealert. when𝑒 1and𝑒 2arecorrelated,ACTsimilarlyactivatesthetrans-
formationresultsby𝑟𝑒𝑙𝑢 function,andthenaggregatesthetwo
typesofalertrepresentationsbyelement-wiseproduct.Afterfur-
To train ABR in the training stage, for each history alert,𝑒 𝑖 thertransformationanddimensionreduction,ACTfinallypresents
(1≤𝑖 ≤𝑛),byEquation(2),weaggregatetheoccurrenceseriesof thecorrelationdegreebetween𝑒 𝑖 and𝑒 𝑗,𝑃ˆ 𝑖,𝑗 = [𝑝ˆ𝑖,𝑗,𝑝ˆ𝑖,𝑗 ]⊤.𝑃ˆ 𝑖,𝑗
a cole nr tt as inco sr tr he ela bt ee hd at vo io𝑒 r𝑖, ina fn od rmde an tio ot ne oth f𝑒e 𝑖r ae nsu dl 𝐹t 𝑖a cs o𝐹ˆ n𝑖 t. aT inh su ts h, esi bn ec he a𝐹 v𝑖 isatwo-dimensionalvector,inwhich𝑝ˆ𝑖 1,𝑗 isthepr1 obab2
ˆ - ilitythat
thetwoalertsarecorrelated,and𝑝ˆ𝑖 2,𝑗
iorinformationofitscorrelatedalerts,thecommonalitybetween istheprobabilitythatthe
𝐹 𝑖 and𝐹ˆ 𝑖 representsthesamebehaviorinformationbetween𝑒 𝑖 twoalertsareuncorrelated.Thesumof𝑝ˆ𝑖,𝑗 and𝑝ˆ𝑖,𝑗 is1,whichis
1 2
anditscorrelatedalerts.Specifically,asshowninFigure4,wetry ensuredbythe𝑠𝑜𝑓𝑡𝑚𝑎𝑥 function.
to transform 𝐹 𝑖 to 𝐹ˆ 𝑖 by a neuralnetwork, whoseloss function TotrainACTinthetrainingstage,forahistoryalert,𝑒 𝑖 (1 ≤
isdefinedinEquation(3).InEquation(3),𝐹 𝑖′isthetransformed 𝑖 ≤𝑛),previousalertsduring[𝑡 𝑖 −𝑤,𝑡 𝑖]canbedividedintotwo
OnlineSummarizingAlertsthroughSemanticandBehaviorInformation ICSE’22,May21–29,2022,Pittsburgh,PA,USA
Figure5:ThestructureofACT.
groups,correlatedalertsto𝑒 𝑖,denotedas𝑅 𝑖 (|𝑅 𝑖| = 𝑐 𝑖),andre- incident 1 incident 2
maininguncorrelatedalerts,denotedas𝐻 𝑖 (|𝐻 𝑖| =𝑜 𝑖).Then,the
realrelationshipbetweentwoalerts,𝑒 𝑖 and𝑒 𝑗 (1≤ 𝑗 ≤𝑛),isfor-
mallydefinedas𝑃 𝑖,𝑗 = [1,0]⊤if𝑒 𝑖 and𝑒 𝑗 arecorrelated,otherwise
𝑃 𝑖,𝑗 = [0,1]⊤.Finally,weadoptEquation(5),whichmeasuresthe 𝒘𝒘 newly reported alert
differencebetweenthedeterminationofACTandthegroundtruth,
𝒆𝒆𝒒𝒒𝒊𝒊 𝒆𝒆𝒊𝒊 time
asthelossfunctionofACT.
add to incident 2
𝑛 Figure6:Theprocessofalertsummarizing.
1(cid:213)(cid:104)1 (cid:213)
𝐿 𝐴𝐶𝑇 = 𝑛 𝑐 (𝑃ˆ 𝑖,𝑗 −𝑃 𝑖,𝑗)2
𝑖
𝑖=1 𝑒𝑗∈𝑅𝑖
(5)
1 (cid:213) (cid:105)
+ (𝑃ˆ 𝑖,𝑗 −𝑃 𝑖,𝑗)2
𝑜
𝑖
𝑒𝑗∈𝐻𝑖 8 EVALUATION
Toevaluatetheeffectivenessofourapproaches,weexploitreal-
7 ONLINESUMMARIZING worlddatasetsfromtwolargecommercialbankstoaddressthe
Inthetrainingstage,allmodelsforalertrepresentationandalert followingresearchquestions:
correlationarewelltrainedoffline.Therefore,inonlinesumma-
r reiz pi on rg tes dta ag le e, rf to ,𝑒r 𝑗t ,h ie nn thew el ty imr eep wo ir nte dd owal ,e [r 𝑡t 𝑖, −𝑒 𝑖, 𝑤a ,n 𝑖d ],th we ep cr ae nvi eo au ss ill yy • R RQ Q1 2: :H Ho ow wd do oe es sO thA eS sap mer pfo lerm grain nusu lam rim tya 𝛼riz inin Ag Bal Rer at ffs?
𝑡 • ectthe
summarizingperformance?
representtheirsemanticinformationandbehaviorinformationby
• RQ3:Howdoesthesamplelength𝛽inABRaffectthesum-
ASRandABR,respectively.Then,accordingtoACT,wecanobtain
marizingperformance?
thecorrelationdegreebetweenthetwoalertsstraightforwardly,
which is defined as𝑃ˆ 𝑖,𝑗 = [𝑝ˆ𝑖,𝑗,𝑝ˆ𝑖,𝑗 ]. Specifically,𝑝ˆ𝑖,𝑗 indicates • RQ4:Howdoesthetimewindow𝑤 inonlinesummarizing
1 2 1 affectthesummarizingperformance?
theprobabilitythatthealertsarecorrelated,and𝑝ˆ𝑖,𝑗
indicatesthe
2
probabilitythatthealertsareuncorrelated.If𝑝ˆ𝑖 1,𝑗 > 𝑝ˆ𝑖 2,𝑗 ,𝑒 𝑖 and 8.1 Datasets
𝑒 𝑗 maybelongtothesamesystemfailure.Therefore,weusethe
Equation(6)tofindthealertmostcorrelatedto𝑒 𝑖during[𝑡 𝑖−𝑤,𝑡 𝑖]. Weconductexperimentsonreal-worldalertsfromtwolargecom-
mercial banks, A and B. As shown in Table 2, alerts of Bank A
andBankBhavedifferentcharacteristics.Thedefinitionofthe
𝑞 𝑖 =argmax(𝑝ˆ𝑖,𝑗 |𝑝ˆ𝑖,𝑗 >𝑝ˆ𝑖,𝑗 ∧(𝑡 𝑖 −𝑡 𝑗) ≤𝑤) (6) alertinBankAisnotstrictlystandardized,thustherearethou-
1 1 2
1≤𝑗<𝑖
sandsofalerttypesinitsdataset.InBankB,thedefinitionofthe
alertisquiterigorous,thusBankBhasamuchsmallernumber
Then,asshowninFigure6,if𝑞 𝑖exists,weadd𝑒 𝑖intotheincidentof ofalerttypesthanBankA.Duetotheinformationprivacy,we
𝑒 𝑞𝑖.Otherwise,weformanewincidentfor𝑒 𝑖.Suchstrategyavoids canonlysharethealertdatasetofBankB,whichisavailableat
modifyingpreviousalertsandensuresthateachalertisprocessed https://doi.org/10.5281/zenodo.5336985,inwhichallsensitivein-
onlyonceintheonlinesummarizingstage. formationisanonymized.
ICSE’22,May21–29,2022,Pittsburgh,PA,USA JiaChen,PengWang,andWeiWang
Table2:DetailsofExperimentalDatasets InLDA,thenumberoftopicsissetto9.FortheCBOWmodelin
ASRandWord2Vec,thesizeofthewordembeddingis512andthe
Datasets TimeSpan #Alerts #AlertTypes epochissetto100.ForABR,thesizeofthebehaviorrepresentation
is600,thesamplegranularityfortheoccurrenceseries,𝛼,isset
BankA 2018/11/23∼2019/02/02 50947 2794 to1minute,thesamplelengthfortheoccurrenceseries,𝛽,isset
BankB 2019/03/01∼2020/08/06 500000 51
to6hoursforBankAand13hoursforBankB,andtheepoch
issetto5.ForACT,inLinearTransformation,thefirstlayerhas
8.2 Baselines 50neurons,andthesecondlayerhas30neurons.InDimension
Reduction,thefirstlayerhas20neurons,andthesecondlayerhas2
AsshowninTable3,wecompareourapproacheswithsevenap- fixedneurons.TheepochofACTissetto60.Thewindowsize,𝑤,in
proaches,SeqKrimp,GoKrimp,CSC,SWIFT,Jaccard,Word2Vec,
onlinesummarizing,issetto5minutes.Forapproachesthatrequire
andLDA.SeqKrimp,GoKrimp,CSC,andSWIFTareallfrequent