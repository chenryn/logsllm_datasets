base-16 process id
L4
base-16 parent process id
pname
L2, L4, L5, L6 process name
h ip
h port
d ip
d port
type
get q
post q
res code
h domain
referer
res loc
acct
objname
info
L2
L2
L2
L2
L3
L3
L3
L3
L3
L3
L3
L5
L5
L6
host IP address
host port number
destination IP address
destination port number
request/response
absolute path of GET
absolute path of POST
response code
host domain name
referer of requested URI
location to redirect
principle of this access
object name
Authentication information
u.pid=v.pid
u.d ip=v.d ip
u.d port=v.d port
u.referer=v.referer
u.host=v.host
u.referer=v.host
u.host=v.referer
u.ppid=v.ppid
Table 3: Features Described by
Each Edge.
D Feature
d1 Δ(u.timestamp, v.timestamp) wAB · |eAB| (wA is weight
assigned for edges in eA and wAB is weight assigned for
edges in eAB). More specially, the algorithm tries to “learn”
a global weight vector (cid:2)α that can be applied on each edge.
Thus, the following equation still holds:
(cid:5)
k(cid:5)
αi · ei >
αi · ei >
e∈eAB
(cid:5)
e∈eAB
i=1
k(cid:5)
i=1
(cid:5)
k(cid:5)
e∈eA
(cid:5)
i=1
k(cid:5)
e∈eB
i=1
αi · ei
αi · ei
(1)
(cid:5)k
where k is the number of dimensions of edge vector ande i is
the i-th value of edge vector (cid:2)e.
Intuitively, we can assign the dot product of the weight
vector and edge vector w =
i=1 αiei as the weight on each
edge to construct a weighted graph. However, most of the
learning algorithms that we apply in weight assignment out-
put (cid:2)α such that the dot product has no bound on the value
i=1 αiei) ∈ R, which might generate negative weight
(w =
w < 0 and violate the requirement of input for the weighted
(cid:5)k
(cid:2)
1+e−
(cid:5)k
i=1 αi ·ei) =
graph community detection algorithm. Consequently, we
leverage a sigmoid function S to map the dot product to
bounded real number range [0, 1] as our ﬁnalized weight as-
signment on each edge: w = S(
1
k
i=1 αi ·ei
We then transform the graph into a weighted graph W G.
For evaluation of the weight assignment algorithm, we deﬁne
our training phase and testing phase as following: Given n
unweighted graphs G1, G2, · · · , Gn, for each l (l ∈ [1, n]), (1)
the training phase of the weight assignment looks for a best
assignment weight vector (cid:2)αk for G1, · · · , Gl−1, Gl+1, · · · ,
Gn; (2) the testing phase takes the dot product of weight
vector (cid:2)αl and all edge vectors (cid:2)e to generate a weighted graph
W Gl. Then, the Community Detection Module takes W Gl as
input and outputs communities for Post Processing Module.
Essentially, this training/testing process adopts the leave-
one-out strategy.
We have also built diﬀerent algorithms for weight assign-
ment for comparison. The ﬁrst does not use any learning
algorithm, the second and third leverage existing supervised
learning techniques that outperform the ﬁrst, and the fourth
is based on quadratic optimization that has the best perfor-
mance results. We use the quadratic optimization algorithm
as our ﬁnalized version of weight assignment in HERCULE,
and compare their results quantitatively in Section 4.
Feature Weight Summation. As a baseline solution,
this algorithm treats each feature of an edge with the same
“importance”: αi = 1, i ∈ [1, k] where k denotes the number
of features. To hold Equation 1, this algorithm depends on
the assumption that edges in A or B have more correlation
types than edges between A and B. More speciﬁcally, this
algorithm assumes the edge vectors of (cid:2)eA and (cid:2)eB has more
1’s than edge vectors of (cid:2)eAB. From the results presented in
Section 4, we ﬁnd that the performance is not ideal. The
reason is that there exist cases when two edge vectors, one
from eA, eB and the other from eAB, (1) are not distinguish-
able by their number of 1’s of their vector values, but diﬀer
in type of features where the 1 reside, such as two vectors
[1 0 1] and [0 1 1], or even worse, (2) the edge vector in eA
or eB has less 1’s in the vector values than those in eAB.
Motivated by the discussed limitations, we adapt two
learning approaches (Logistic regression and SVM), which
assign diﬀerent “importance” on the edge features.
Logistic Regression. Logistic regression [44] can be
applied in our weight assignment as we abstract the weight
assignment as a classiﬁcation problem. We want to learn the
global weight (cid:2)α that helps to classify edge vectors into one
class eA, eB and another class eAB. Suppose there are m
training edges, denoted E = xi, yi, i ∈ [1, m] where xi is i-th
edge vector (cid:2)ei, yi = 1 if ei ∈ eAB and yi = 0 if ei ∈ eA or ei ∈
eB. In training, we construct a prediction function, which
leverages logistic function g: h(cid:3)α(xi) =g (α
1+e−(cid:3)αxi
where h(cid:3)α(x) denotes the probability that ei ∈ eAB (yi = 1):
P (yi = 1|xi, (cid:2)α) =h (cid:3)α(xi) and P (yi = 0|xi, (cid:2)α) = 1 − h(cid:3)α(xi).
Then we should minimize the cost function in log likelihood
format: − 1
(1 − yi) log(1 − h(cid:3)α(xi))].
m [
yi log h(cid:3)α(xi) +
m(cid:5)
m(cid:5)
1
T
xi) =
i=1
i=1
The minimization problem can be solved by using gradient
descent. Please refer to [44] for more details of the algorithm.
SVM. Another classiﬁcation solution for learning the wei-
ght vector α is an SVM [19]. Suppose there are m training
edges, denoted E = xi, yi, i ∈ [1, m] where xi is i-th edge
vector (cid:2)ei, yi = 1 if ei ∈ eAB and yi = −1 ife i ∈ eA or
587ei ∈ eB. The purpose of the SVM is to learn a weight vector
(cid:2)α, which can accurately distinguish eAB from eA, eB. We
use the soft margin version; the detailed formulation can be
found in [19].
Quadratic Programming. The above two classiﬁcation
learning algorithms look for a decision boundary that classi-
ﬁes edges into two types (eA, eB vs. eAB). However, their
output weight vectors are not the global optimum, which are
not guaranteed to maximize the weight assigned to edge eA
and eB, and minimize the weight assigned to edge eAB.
Therefore, we design and develop a new solution, which
transforms the weight assignment as a quadratic optimization
problem. Adapted from Equation 1, our target function is:
(cid:5)
max
(cid:3)α
−λ
e∈eA
(cid:5)
k(cid:5)
i=1
αi · ei +
k(cid:5)
(cid:5)
k(cid:5)
e∈eB
i=1
αi · ei
(cid:4)αT · (cid:4)α
(2)
αi · ei − 1
2
i=1
0 ≤ (cid:4)αT e ≤ 1
e∈eAB
s.t.
λ is the trade-oﬀ parameter to balance between the ﬁrst two
terms and the third term in the target function. 1
T · (cid:2)α is
the regularizer to avoid the overﬁtting problem. The target
function is convex and the output weight vector (cid:2)α is theoret-
ically global optimum. As we constrained the optimization
by 0 ≤ (cid:2)α
e ≤ 1, we do not leverage a sigmoid function for
e to [0, 1] again.
this algorithm to map the dot product (cid:2)α
2 (cid:2)α
T
T
3.3 DFS Propagation
Prior to coming up with the community detection approach,
we designed a heuristic algorithm that neither uses any learn-
ing techniques nor community detection, which we term
“DFS propagation”. We note that without the knowledge
of any low-level program execution analysis or ﬁne-grained
log analysis, we cannot capture the accurate causality re-
lationship between log entries. Thus in this algorithm, we
conservatively treat any log entry t as malicious if there exists
a path from a tainted attack-related start point s to t, which
we deﬁne s can propagate to t. The input to this algorithm
is the tainted entry point v and the graph generated from
Log Correlation Module. It then uses Depth-First-Search
(DFS) to recover all log entries that can be propagated from
v. From the results shown in the Section 4, we found the
performance of DFS Propagation is not ideal. Therefore, we
propose a more robust method using community detection
in the following section.
3.4 Community Detection
Considering the large number of nodes in the graph, we
need to select a time-eﬃcient community detection algorithm
to extract communities from the large weighted correlated-log
graph. There are multiple unsupervised learning techniques
for community detection. We choose to use the Louvain
method considering its eﬃcient handling of large networks
[13].
At the beginning of this algorithm, each node in our
weighted graph represents a community. We denote Av,w as
the weight between node v and node w, kv =
w Av,w as
the sum of the weights connected to the node w, cv as the
community to which the node v is assigned, the δ-function
δ(i, j) = 1 if i = j andδ (i, j) = 0 otherwise. Modularity is
(cid:5)
deﬁned as:
(cid:6)
(cid:5)
v,w
Q =
1
2m
Av,w −
(cid:7)
δ(cv, cw)
(3)
kikj
2m
(cid:5)
where m =
1
2
Av,w
v,w
Modularity has a value between −1 and 1, which measures the
degree of the density of the connections within communities
compared to connections between communities in the graph.
After the initialization of communities, we repeat the Lou-
vain method in two phases to greedily optimize the local
modularity as the algorithm progresses. For each node v, the
algorithm removes v from its own community and moves it
into the community C of each neighbor w of v. Then the
algorithm evaluates changes in modularity and places v in
the community that has the largest modularity gain. If the
largest gain is negative, the node v is not moved and placed
in its original community. The modularity gain is calculated:
(4)
ΔQ =
in +kv,in
2m
−
(cid:2)(cid:9)
−
−
in
2m
tot