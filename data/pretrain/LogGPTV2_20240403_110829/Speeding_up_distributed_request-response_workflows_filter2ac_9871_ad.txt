where us is the relative contribution from each stage’s answer.
Then, the budget constraint is given by Ps csrs ≤ B, where
cs = us
.
Ps
′ us
′
To conclude, we brieﬂy highlight how to combine diﬀerent
techniques into a uniﬁed optimization framework. Using
superscripts for the technique type, let k = 1, . . . , K be the
collection of techniques, then our optimization framework
(3) extends as follows to multiple dimensions:
minimize X
s
Var(cid:0)Ls(r1
s , . . . , rK
s )(cid:1)
subject to X
ck
s rk
s ≤ Bk,
k = 1, . . . , K.
(4)
s
s , . . . , rK
s , . . . , rK
As before, Var(cid:0)Ls(r1
s )(cid:1) are the per-stage models,
i.e., variance-response curves. These models abstract away
the internal optimization given (r1
s ). Greedy gradient-
like algorithms (such as SumVar) can be extended to solve
(4), however, the extension is not straightforward. The main
complexity in (4) is hidden in the computation of the variance-
response curves – as opposed to a scan over one dimension in
(3), variance-response curves in (4) requires a scan over the
k-dimensional space, (r1
s ). In practice, we note that
the optimization often decouples into simpler problems. For
example, assume K = 2 with reissues and partial responses
as the two techniques for reducing latency. Partial responses
are mostly useful in many-way parallel services which have
a high cost for reissues. Hence, we can use the utility loss
budget only for the services with high fan-out and the reissue
budget for the rest of the services. Finding a general low
complexity algorithm to solve (4) is left to future work.
s , . . . , rK
4.3 Catch-up
The framework described so far reduces workﬂow latency
by making local decisions in each stage. Instead, the main
idea behind catch-up is to speed-up a request based on its
progress in the workﬂow as a whole. For example, when
some of the initial stages are slow, we can reissue a request
more aggressively in the subsequent stages. In this paper, we
consider the following techniques for catch-up: (1) allocate
more threads to process the request; given multi-threaded
implementation of many stages at Bing, we ﬁnd that allocating
5. EVALUATION
In this section, we evaluate the individual techniques in
Kwiken by comparing them to other benchmarks (§5.2 - §5.4),
followed by using all Kwiken techniques together (§5.5). Using
execution traces and workﬂows from Bing, we show that:
• With a reissue budget of just 5%, Kwiken reduces the
99th percentile of latency by an average of 29% across
workﬂows. This is over half the gains possible from
reissuing every request (i.e., budget=100%). Kwiken’s
226 
h
t
9
9
t
a
t
n
e
m
e
v
o
r
p
m
I
e
l
i
t
n
e
c
r
e
p
80%
60%
40%
20%
0%
1% 3% 5% 7% 9% 100%
10% - 90%
mean
(a) Improvement in 99th latency per-
centile
Budget 
25% - 75%
50%
40%
30%
20%
10%
0%
t
n
e
m
e
v
o
r
p
m
I
e
g
a
r
e
v
A
1%
4%
Budget 
0.95
0.9
0.8
7%
10%
0.999
0.5
(b) Average reduction in diﬀerent la-
tency percentiles
0.99
Figure 11: Reduction in latency achieved by using per-
stage reissues in Kwiken
apportioning of budget across stages is key to achieving
these gains.
• In stages that aggregate responses from many respon-
ders, Kwiken improves the 99th percentile of latency by
more than 50% with a utility loss of at most 0.1%.
• Using simple catch-up techniques, Kwiken improves the
99th percentile latency by up to 44% by using just 6.3%
more threads and prioritizing 1.5% network traﬃc.
• By combining reissues with utility trade-oﬀ we see that
Kwiken can do much better than when using either
technique by itself; for example, a reissue budget of
1% combined with a utility loss of 0.1% achieves lower
latency than just using reissues of up to 10% and just
trading oﬀ utility loss of up to 1%.
• Kwiken’s data-driven parameter choices are stable.
5.1 Methodology
Traces from production: To evaluate Kwiken, we ex-
tract the following data from production traces for the 45
most frequently accessed workﬂows at Bing: workﬂow DAGs,
request latencies at each stage as well as the end-to-end
latency, the cost of reissues at each stage and the usefulness
of responses (e.g., ranks of documents) when available. To
properly measure latencies on the tail, we collected data for
at least 10, 000 requests for each workﬂow and stage. The
datasets span several days during Oct-Dec 2012. We ignore
requests served from cache at any stage in their workﬂow;
such requests account for a sizable fraction of all requests
but do not represent the tail.
We conducted operator interviews to estimate the cost
of reissue at each stage. Our estimates are based on the
resources expended per request. For stages that process the
request in a single thread, we use the mean latency in that
stage as an approximation to the amount of computation
and other resources used by the request. For more complex
stages, we use the sum of all the time spent across parallel
servers to execute this stage. Kwiken only relies on the relative
costs across stages when apportioning budget.
Simulator: We built a trace-driven simulator, that mimics
the workﬂow controller used in production at Bing, to test
the various techniques in Kwiken. The latency of a request
at each stage and that of its reissue (when needed) are
sampled independently from the distribution of all request
latencies at that stage. We veriﬁed that this is reasonable:
controlled experiments on a subset of workﬂows where we
issued the same request twice back-to-back showed very small
correlation; also, the time spent by a request in diﬀerent
stages in its workﬂow had small correlation (see §2.2).
Estimating Policy Parameters: The parameters of the
Kwiken policies (such as per-stage reissue timeouts) are
trained based on traces from prior executions. While we
estimate the parameters on a training data set, we report
performance of all polices on a test data set collected at a
diﬀerent period of time. In all cases, both training and test
data sets contain traces from several thousands to tens of
thousands of queries.
5.2 Reissues
We ﬁrst evaluate the eﬀect of using per-stage reissues
within Kwiken’s framework. Fig. 11a plots Kwiken’s improve-
ments on the end-to-end latency due to reissues, using the
SumVar algorithm described in §4.1. The x-axis depicts the
fraction of additional resources provided to reissues and the
y-axis shows the fractional reduction at the 99th percentile.
The solid line shows the mean improvement over the 45 most
frequent workﬂows at Bing; the dashed lines represent the
spread containing the top 25% and bottom 75% of workﬂows
and the dotted lines show the improvements for the top 10%
and bottom 90% of workﬂows (sorted with respect to per-
centage improvement). The circles on the right edge depict
latency improvements with a budget of 100%.
We see that Kwiken improves 99th percentile of latency by
about 29%, on average, given a reissue budget of 5%. This is
almost half the gain that would be achieved if all requests at
all stages were reissued, i.e., a budget of 100%. This indicates
that Kwiken ekes out most of the possible gains, i.e., identiﬁes
laggards and tries to replace them with faster reissues, with
just a small amount of budget. Most workﬂows see gains but
some see a lot more than the others; the top 10% of workﬂows
improve by 55% while the top 75% of workﬂows see at least
10% improvement each. The variation is because diﬀerent
workﬂows have diﬀerent amounts of inherent variance.
Fig. 11b plots the average gains at several other latency
percentiles. As before, we see that small budgets lead to
sizable gains and the marginal increase from additional bud-
get is small. This is because some stages with high variance
and low cost can be reissued at substantial fraction (e.g.,
50%), yet consume only a fraction of total budget.
It is
interesting though that just a small amount of budget (say
3%) leads to some gains at the median. Observe that higher
percentiles exhibit larger improvements, which is consistent
with theory (cf. §3). We note that Kwiken scales eﬃciently to
large workﬂows. Computing per-stage models takes about
2 seconds per stage and is parallelizable. Running SumVar
takes less than one second for most of the workﬂows.
Comparing SumVar to other benchmarks: First, we
compare against the current reissue strategy used in Bing.
The actual reissue timeout values used in Bing are very con-
servative – additional cost is only 0.2% – and reduce 99th
percentile of latency only by 3%. The timeouts are so con-
servative because without an end-to-end framework such as
227100%
80%
60%
40%
20%
0%
y
c
n
e
t
a
L
n
i
n
o
i
t
c
u
d
e
R
75th
95th
99th
99.9th
70% 
71% 
79% 
44% 
50% 
51% 
91% 
92% 
56% 
63% 
21% 
5% 
25% 
25% 
5% 
6% 
27% 
14% 
31% 
21% 
0.07% 0.10% 0.20% 0.40% 0.80%
Utility Loss Budget 
(a) Kwiken for diﬀerent values of utility loss
y
c
n
e
a
L
n
t
i
n
o
i
t
c
u
d
e
R
80%
60%
40%
20%
0%
-20%
network sped up
processing sped up
both sped up
63% 
44% 
38% 
36% 
21% 
19% 
13% 
13% 
50th
75th
90th
95th
99th 99.9th
Latency Percentile 
(a) Using Network and Processing Speedup
60%
40%
20%
0%
y
c
n
e
a
L
n
t
i
n
o
i
t
c
u
d
e
R
Wait for fraction
Fixed timeout
Time, then fraction
Kwiken
50% 
46% 
25% 
26% 
5% 
75th
14% 
15% 
12% 
1% 
1% 
1% 
90th
95th
Latency Percentile 
99th
10%
5%
0%
)
L
%
3
(
–
)
G
%
5
0
.
,
L
%
5
.
2
(
-10%
-5%
0%
5%
10%
-5%
-10%
(2.9% L, 0.1% G) – (3% L) 
(b) Using Global Reissues
(b) Kwiken vs. other variants for utility loss
= 0.1%