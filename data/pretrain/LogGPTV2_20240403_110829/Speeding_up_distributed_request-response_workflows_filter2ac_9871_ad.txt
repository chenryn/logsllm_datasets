### Budget Constraint and Optimization Framework

The relative contribution from each stage's answer is denoted by \( u_s \). The budget constraint is given by:

\[ P_s c_s r_s \leq B, \]

where

\[ c_s = \frac{u_s}{P_s}. \]

To conclude, we briefly highlight how to integrate different techniques into a unified optimization framework. Using superscripts to denote the technique type, let \( k = 1, \ldots, K \) be the collection of techniques. Our optimization framework (3) extends to multiple dimensions as follows:

\[
\begin{aligned}
& \text{minimize} & & \sum_s \text{Var}\left(L_s(r_{s,1}, \ldots, r_{s,K})\right) \\
& \text{subject to} & & \sum_s c_{k,s} r_{k,s} \leq B_k, \quad k = 1, \ldots, K.
\end{aligned}
\]

Here, \(\text{Var}(L_s(r_{s,1}, \ldots, r_{s,K}))\) represents the per-stage models, i.e., variance-response curves. These models abstract away the internal optimization given \((r_{s,1}, \ldots, r_{s,K})\). Greedy gradient-like algorithms (such as SumVar) can be extended to solve (4), but the extension is not straightforward. The main complexity in (4) lies in the computation of the variance-response curves, which requires a scan over the \( k \)-dimensional space \((r_{s,1}, \ldots, r_{s,K})\), as opposed to a one-dimensional scan in (3). In practice, the optimization often decouples into simpler problems. For example, if \( K = 2 \) with reissues and partial responses as the two techniques for reducing latency, partial responses are most useful in many-way parallel services with high reissue costs. Thus, the utility loss budget can be used for high fan-out services, while the reissue budget can be used for the rest. Finding a general low-complexity algorithm to solve (4) remains an area for future work.

### Catch-Up Mechanism

The framework described so far reduces workflow latency by making local decisions at each stage. The catch-up mechanism, however, aims to speed up a request based on its overall progress in the workflow. For instance, if initial stages are slow, we can reissue a request more aggressively in subsequent stages. In this paper, we consider the following techniques for catch-up: (1) allocate more threads to process the request; given the multi-threaded implementation of many stages at Bing, we find that allocating additional threads can significantly improve performance.

### Evaluation

In this section, we evaluate the individual techniques in Kwiken by comparing them to other benchmarks (Sections 5.2-5.4), followed by using all Kwiken techniques together (Section 5.5). Using execution traces and workflows from Bing, we show that:

- With a reissue budget of just 5%, Kwiken reduces the 99th percentile of latency by an average of 29% across workflows. This is over half the gains possible from reissuing every request (i.e., budget=100%). Kwiken’s apportioning of the budget across stages is key to achieving these gains.
- In stages that aggregate responses from many responders, Kwiken improves the 99th percentile of latency by more than 50% with a utility loss of at most 0.1%.
- Using simple catch-up techniques, Kwiken improves the 99th percentile latency by up to 44% by using just 6.3% more threads and prioritizing 1.5% network traffic.
- By combining reissues with utility trade-offs, Kwiken can achieve better results than using either technique alone. For example, a reissue budget of 1% combined with a utility loss of 0.1% achieves lower latency than just using reissues of up to 10% and just trading off utility loss of up to 1%.
- Kwiken’s data-driven parameter choices are stable.

#### Methodology

**Traces from Production:** To evaluate Kwiken, we extract the following data from production traces for the 45 most frequently accessed workflows at Bing: workflow DAGs, request latencies at each stage, end-to-end latency, the cost of reissues at each stage, and the usefulness of responses (e.g., ranks of documents) when available. To properly measure tail latencies, we collected data for at least 10,000 requests for each workflow and stage. The datasets span several days during October-December 2012. Requests served from cache at any stage are ignored, as they do not represent the tail.

We conducted operator interviews to estimate the cost of reissues at each stage. Our estimates are based on the resources expended per request. For single-threaded stages, we use the mean latency as an approximation of the computation and other resources used. For more complex stages, we use the sum of all time spent across parallel servers. Kwiken relies on the relative costs across stages when apportioning the budget.

**Simulator:** We built a trace-driven simulator that mimics the workflow controller used in production at Bing to test various techniques in Kwiken. The latency of a request at each stage and its reissue (when needed) are sampled independently from the distribution of all request latencies at that stage. Controlled experiments on a subset of workflows showed very small correlation, and the time spent by a request in different stages had small correlation (see Section 2.2).

**Estimating Policy Parameters:** The parameters of Kwiken policies (such as per-stage reissue timeouts) are trained based on traces from prior executions. While we estimate the parameters on a training dataset, we report the performance of all policies on a test dataset collected at a different period. Both training and test datasets contain traces from several thousands to tens of thousands of queries.

#### Reissues

We first evaluate the effect of using per-stage reissues within Kwiken’s framework. Figure 11a shows Kwiken’s improvements on the end-to-end latency due to reissues, using the SumVar algorithm described in Section 4.1. The x-axis depicts the fraction of additional resources provided to reissues, and the y-axis shows the fractional reduction at the 99th percentile. The solid line shows the mean improvement over the 45 most frequent workflows at Bing; the dashed lines represent the spread containing the top 25% and bottom 75% of workflows, and the dotted lines show the improvements for the top 10% and bottom 90% of workflows (sorted by percentage improvement). The circles on the right edge depict latency improvements with a budget of 100%.

Kwiken improves the 99th percentile of latency by about 29%, on average, given a reissue budget of 5%. This is almost half the gain that would be achieved if all requests at all stages were reissued, i.e., a budget of 100%. This indicates that Kwiken ekes out most of the possible gains, i.e., identifies laggards and tries to replace them with faster reissues, with just a small amount of budget. Most workflows see gains, but some see much more than others; the top 10% of workflows improve by 55%, while the top 75% of workflows see at least 10% improvement each. The variation is due to different workflows having different amounts of inherent variance.

Figure 11b plots the average gains at several other latency percentiles. Small budgets lead to sizable gains, and the marginal increase from additional budget is small. This is because some stages with high variance and low cost can be reissued at a substantial fraction (e.g., 50%), yet consume only a fraction of the total budget. It is interesting that even a small budget (say 3%) leads to some gains at the median. Higher percentiles exhibit larger improvements, consistent with theory (cf. Section 3). Kwiken scales efficiently to large workflows. Computing per-stage models takes about 2 seconds per stage and is parallelizable. Running SumVar takes less than one second for most workflows.

**Comparing SumVar to Other Benchmarks:** First, we compare against the current reissue strategy used in Bing. The actual reissue timeout values used in Bing are very conservative—additional cost is only 0.2%—and reduce the 99th percentile of latency only by 3%. The timeouts are so conservative because without an end-to-end framework such as Kwiken, it is difficult to balance cost and performance effectively.