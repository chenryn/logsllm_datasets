to the object, which makes sure the driver cannot be unloaded prematurely while the thread is
still executing.
A
driver-created
thread
must
terminate
itself
eventually
by
calling
PsTerminateSystemThread. This function never returns if successful.
Work items is the term used to describe functions queued to the system thread pool. A driver can allocate
and initialize a work item, pointing to the function the driver wishes to execute, and then the work item
can be queued to the pool. This may seem very similar to a DPC, the primary difference being work items
Chapter 6: Kernel Mechanisms
182
always execute at IRQL PASSIVE_LEVEL (0). Thus, work items can be used by IRQL 2 code (such as
DPCs) to perform operations not normally allowed at IRQL 2 (such as I/O operations).
Creating and initializing a work item can be done in one of two ways:
• Allocate and initialize the work item with IoAllocateWorkItem. The function returns a pointer
to the opaque IO_WORKITEM. When finished with the work item it must be freed with IoFree-
WorkItem.
• Allocate an IO_WORKITEM structure dynamically with size provided by IoSizeofWorkItem.
Then call IoInitializeWorkItem. When finished with the work item, call IoUninitialize-
WorkItem.
These functions accept a device object, so make sure the driver is not unloaded while there is a work item
queued or executing.
There is another set of APIs for work items, all start with Ex, such as ExQueueWorkItem.
These functions do not associate the work item with anything in the driver, so it’s possible
for the driver to be unloaded while a work item is still executing. These APIs are marked as
deprecated - always prefer using the Io functions.
To queue the work item, call IoQueueWorkItem. Here is its definition:
viud IoQueueWorkItem(
_Inout_ PIO_WORKITEM IoWorkItem,
// the work item
_In_ PIO_WORKITEM_ROUTINE WorkerRoutine,
// the function to be called
_In_ WORK_QUEUE_TYPE QueueType,
// queue type
_In_opt_ PVOID Context);
// driver-defined value
The callback function the driver needs to provide has the following prototype:
IO_WORKITEM_ROUTINE WorkItem;
void WorkItem(
_In_
PDEVICE_OBJECT DeviceObject,
_In_opt_ PVOID
Context);
The system thread pool has several queues (at least logically), based on the thread priorities that serve
these work items. There are several levels defined:
Chapter 6: Kernel Mechanisms
183
typedef enum _WORK_QUEUE_TYPE {
CriticalWorkQueue,
// priority 13
DelayedWorkQueue,
// priority 12
HyperCriticalWorkQueue,
// priority 15
NormalWorkQueue,
// priority 8
BackgroundWorkQueue,
// priority 7
RealTimeWorkQueue,
// priority 18
SuperCriticalWorkQueue,
// priority 14
MaximumWorkQueue,
CustomPriorityWorkQueue = 32
} WORK_QUEUE_TYPE;
The documentation indicates DelayedWorkQueue must be used, but in reality, any other supported level
can be used.
There is another function that can be used to queue a work item: IoQueueWorkItemEx. This
function uses a different callback that has an added parameter which is the work item itself.
This is useful if the work item function needs to free the work item before it exits.
Summary
In this chapter, we looked at various kernel mechanisms driver developers should be aware of and use. In
the next chapter, we’ll take a closer look at I/O Request Packets (IRPs).
Chapter 7: The I/O Request Packet
After a typical driver completes its initialization in DriverEntry, its primary job is to handle requests.
These requests are packaged as the semi-documented I/O Request Packet (IRP) structure. In this chapter,
we’ll take a deeper look at IRPs and how a driver handles common IRP types.
In This chapter:
• Introduction to IRPs
• Device Nodes
• IRP and I/O Stack Location
• Dispatch Routines
• Accessing User Buffers
• Putting it All Together: The Zero Driver
Introduction to IRPs
An IRP is a structure that is allocated from non-paged pool typically by one of the “managers” in the
Executive (I/O Manager, Plug & Play Manager, Power Manager), but can also be allocated by the driver,
perhaps for passing a request to another driver. Whichever entity allocating the IRP is also responsible for
freeing it.
An IRP is never allocated alone. It’s always accompanied by one or more I/O Stack Location structures
(IO_STACK_LOCATION). In fact, when an IRP is allocated, the caller must specify how many I/O stack
locations need to be allocated with the IRP. These I/O stack locations follow the IRP directly in memory.
The number of I/O stack locations is the number of device objects in the device stack. We’ll discuss
device stacks in the next section. When a driver receives an IRP, it gets a pointer to the IRP structure
itself, knowing it’s followed by a set of I/O stack location, one of which is for the driver’s use. To get the
correct I/O stack location, a driver calls IoGetCurrentIrpStackLocation (actually a macro). Figure
7-1 shows a conceptual view of an IRP and its associated I/O stack locations.
Chapter 7: The I/O Request Packet
185
Figure 7-1: IRP and its I/O stack locations
The parameters of the request are somehow “split” between the main IRP structure and the current IO_-
STACK_LCATION.
Device Nodes
The I/O system in Windows is device-centric, rather than driver-centric. This has several implications:
• Device objects can be named, and handles to device objects can be opened. The CreateFile
function accepts a symbolic link that leads to a device object. CreateFile cannot accept a driver’s
name as an argument.
• Windows supports device layering - one device can be layered on top of another. Any request
destined for a lower device will reach the uppermost device first. This layering is common for
hardware-based devices, but it works with any device type.
Figure 7-2 shows an example of several layers of devices, “stacked” one on top of the other. This set of
devices is known as a device stack, sometimes referred to as device node (although the term device node is
often used with hardware device stacks). Figure 7-1 shows six layers, or six devices. Each of these devices is
represented by a DEVICE_OBJECT structure created by calling the standard IoCreateDevice function.
Chapter 7: The I/O Request Packet
186
Figure 7-2: Layered devices
The different device objects that comprise the device node (devnode) layers are labeled according to their
role in the devnode. These roles are relevant in a hardware-based devnode.
All the device objects in figure 7-2 are just DEVICE_OBJECT structures, each created by a different driver
that is in charge of that layer. More generically, this kind of device node does not have to be related to
hardware-based device drivers.
Here is a quick rundown of the meaning of the labels present in figure 7-2:
• PDO (Physical Device Object) - Despite the name, there is nothing “physical” about it. This device
object is created by a bus driver - the driver that is in charge of the particular bus (e.g. PCI, USB,
etc.). This device object represents the fact that there is some device in that slot on that bus.
• FDO (Functional Device Object) - This device object is created by the “real” driver; that is, the driver
typically provided by the hardware’s vendor that understands the details of the device intimately.
• FiDO (Filter Device Object) - These are optional filter devices created by filter drivers.
The Plug & Play (P&P) manager, in this case, is responsible for loading the appropriate drivers, starting
from the bottom. As an example, suppose the devnode in figure 7-2 represents a set of drivers that manage
Chapter 7: The I/O Request Packet
187
a PCI network card. The sequence of events leading to the creation of this devnode can be summarized as
follows:
1. The PCI bus driver (pci.sys) recognizes the fact that there is something in that particular slot. It
creates a PDO (IoCreateDevice) to represent this fact. The bus driver has no idea whether this a
network card, video card or something else; it only knows there is something there and can extract
basic information from its controller, such as the Vendor ID and Device ID of the device.
2. The PCI bus driver notifies the P&P manager that it has changes on its bus.
3. The P&P manager requests a list of PDOs managed by the bus driver. It receives back a list of PDOs,
in which this new PDO is included.
4. Now the P&P manager’s job is to find and load the proper driver that new PDO. It issues a query to
the bus driver to request the full hardware device ID.
5. With this hardware ID in hand, the P&P manager looks in the Registry at HKLM\System\ Current-
ControlSet\Enum\PCI\(HardwareID). If the driver has been loaded before, it will be registered there,
and the P&P manager will load it. Figure 7-3 shows an example hardware ID in the registry (NVIDIA
display driver).
6. The driver loads and creates the FDO (another call to IoCreateDevice), but adds an additional
call to IoAttachDeviceToDeviceStack, thus attaching itself over the previous layer (typically
the PDO).
We’ll see how to write filter drivers that take advantage of IoAttachDeviceToDeviceStack in
chapter 13.
Chapter 7: The I/O Request Packet
188
Figure 7-3: Hardware ID information
The value Service in figure 7-3 indirectly points to the actual driver at HKLMSystemCutrrent-
ControlSetServices{ServiceName} where all drivers must be registered.
The filter device objects are loaded as well, if they are registered correctly in the Registry. Lower filters
(below the FDO) load in order, from the bottom. Each filter driver loaded creates its own device object and
attached it on top of the previous layer. Upper filters work the same way but are loaded after the FDO. All
this means that with operational P&P devnodes, there are at least two layers - PDO and FDO, but there
could be more if filters are involved. We’ll look at basic filter development for hardware-based drivers in
chapter 13.
Full discussion of Plug & Play and the exact way this kind of devnode is built is beyond the scope of this
book. The previous description is incomplete and glances over some details, but it should give you the
basic idea. Every devnode is built from the bottom up, regardless of whether it is related to hardware or
not.
Lower filters are searched in two locations: the hardware ID key shown in figure 7-3 and in the correspond-
ing class based on the ClassGuid value listed under HKLM\System\CurrentControlSet\Control\Classes.
The value name itself is LowerFilters and is a multiple string value holding service names, pointing to
Chapter 7: The I/O Request Packet
189
the same Services key. Upper filters are searched in a similar manner, but the value name is UpperFilters.
Figure 7-4 shows the registry settings for the DiskDrive class, which has a lower filter and an upper filter.
Figure 7-4: The DiskDrive class key
IRP Flow
Figure 7-2 shows an example devnode, whether related to hardware or not. An IRP is created by one of
the managers in the Executive - for most of our drivers that is the I/O Manager.
The manager creates an IRP with its associated IO_STACK_LOCATIONs - six in the example in figure 7-2.
The manager initializes the main IRP structure and the first I/O stack location only. Then it passes the
IRP’s pointer to the uppermost layer.
A driver receives the IRP in its appropriate dispatch routine. For example, if this is a Read IRP, then the
driver will be called in its IRP_MJ_READ index of its MajorFunction array from its driver object. At
this point, a driver has several options when dealing with IRP:
• Pass the request down - if the driver’s device is not the last device in the devnode, the driver can
pass the request along if it’s not interesting for the driver. This is typically done by a filter driver
that receives a request that it’s not interested in, and in order not to hurt the functionality of the
device (since the request is actually destined for a lower-layer device), the driver can pass it down.
This must be done with two calls:
– Call IoSkipCurrentIrpStackLocation to make sure the next device in line is going to
see the same information given to this device - it should see the same I/O stack location.
– Call IoCallDriver passing the lower device object (which the driver received at the time it
called IoAttachDeviceToDeviceStack) and the IRP.
Before passing the request down, the driver must prepare the next I/O stack location with proper
information. Since the I/O manager only initializes the first I/O stack location, it’s the responsibility of
each driver to initialize the next one. One way to do that is to call IoCopyIrpStackLocationToNext
before calling IoCallDriver. This works, but is a bit wasteful if the driver just wants the lower
Chapter 7: The I/O Request Packet
190
layer to see the same information. Calling IoSkipCurrentIrpStackLocation is an optimization
which decrements the current I/O stack location pointer inside the IRP, which is later incremented
by IoCallDriver, so the next layer sees the same IO_STACK_LOCATION this driver has seen. This
decrement/increment dance is more efficient than making an actual copy.
• Handle the IRP fully - the driver receiving the IRP can just handle the IRP without propagating it
down by eventually calling IoCompleteRequest. Any lower devices will never see the request.
• Do a combination of the above options - the driver can examine the IRP, do something (such as log
the request), and then pass it down. Or it can make some changes to the next I/O stack location, and
then pass the request down.
• Pass the request down (with or without changes) and be notified when the request completes by
a lower layer device - Any layer (except the lowest one) can set up an I/O completion routine
by calling IoSetCompletionRoutine before passing the request down. When one of the lower
layers completes the request, the driver’s completion routine will be called.
• Start some asynchronous IRP handling - the driver may want to handle the request, but if the request
is lengthy (typical of a hardware driver, but also could be the case for a software driver), the driver
may mark the IRP as pending by calling IoMarkIrpPending and return a STATUS_PENDING
from its dispatch routine. Eventually, it will have to complete the IRP.
Once some layer calls IoCompleteRequest, the IRP turns around and starts “bubbling up” towards
the originator of the IRP (typically one of the I/O System Managers). If completion routines have been
registered, they will be invoked in reverse order of registration.
In most drivers in this book, layering will not be considered, since the driver is most likely the single
device in its devnode. The driver will handle the request then and there or handle it asynchronously; it
will not pass it down, as there is no device underneath.
We’ll discuss other aspects of IRP handling in filter drivers, including completion routines, in chapter 13.
IRP and I/O Stack Location
Figure 7-5 shows some of the important fields in an IRP.
Chapter 7: The I/O Request Packet
191
Figure 7-5: Important fields of the IRP structure
Here is a quick rundown of these fields:
• IoStatus - contains the Status (NT_STATUS) of the IRP and an Information field. The
Information field is a polymorphic one, typed as ULONG_PTR (32 or 64-bit integer), but its
meaning depends on the type of IRP. For example, for Read and Write IRPs, its meaning is the
number of bytes transferred in the operation.
• UserBuffer - contains the raw buffer pointer to the user’s buffer for relevant IRPs. Read and
Write IRPs, for instance, store the user’s buffer pointer in this field. In DeviceIoControl IRPs,
this points to the output buffer provided in the request.
• UserEvent - this is a pointer to an event object (KEVENT) that was provided by a client if the
call is asynchronous and such an event was supplied. From user mode, this event can be provided
(with a HANDLE) in the OVERLAPPED structure that is mandatory for invoking I/O operations
asynchronously.
• AssociatedIrp - this union holds three members, only one (at most) of which is valid:
* SystemBuffer - the most often used member. This points to a system-allocated non-paged pool buffer
used for Buffered I/O operations. See the section “Buffered I/O” later in this chapter for the details.
* MasterIrp - A pointer to a “master” IRP, if this IRP is an associated IRP. This idea is supported by the
I/O manager, where one IRP is a “master” that may have several “associated” IRPs. Once all the associated
Chapter 7: The I/O Request Packet
192
IRPs complete, the master IRP is completed automatically. MasterIrp is valid for an associated IRP - it
points to the master IRP.
* IrpCount - for the master IRP itself, this field indicates the number of associated IRPs associated with
this master IRP.
Usage of master and associated IRPs is pretty rare. We will not be using this mechanism in this book.
• Cancel Routine - a pointer to a cancel routine that is invoked (if not NULL) if the driver is asked
to can cel the IRP, such as with the user mode functions CancelIo and CancelIoEx. Software
drivers rarely need cancellation routines, so we will not be using those in most examples.
• MdlAddress - points to an optional Memory Descriptor List (MDL). An MDL is a kernel data
structure that knows how to describe a buffer in RAM. MdlAddress is used primarily with Direct
I/O (see the section “Direct I/O” later in this chapter).
Every IRP is accompanied by one or more IO_STACK_LOCATIONs. Figure 7-6 shows the important fields
in an IO_STACK_LOCATION.
Figure 7-6: Important fields of the IO_STACK_LOCATION structure
Chapter 7: The I/O Request Packet
193
Here’s a rundown of the fields shown in figure 7-6:
• MajorFunction - this is the major function of the IRP (IRP_MJ_CREATE, IRP_MJ_READ, etc.).
This field is sometimes useful if the driver points more than one major function code to the same
handling routine. In that routine, the driver may want to distinguish between the major function
codes using this field.
• MinorFunction - some IRP types have minor functions. These are IRP_MJ_PNP, IRP_MJ_POWER
and IRP_MJ_SYSTEM_CONTROL (WMI). Typical code for these handlers has a switch statement
based on the MinorFunction. We will not be using these types of IRPs in this book, except in the
case of filter drivers for hardware-based devices, which we’ll examine in some detail in chapter 13.
• FileObject - the FILE_OBJECT associated with this IRP. Not needed in most cases, but is
available for dispatch routines that need it.
• DeviceObject - the device object associated with this IRP. Dispatch routines receive a pointer to
this, so typically accessing this field is not required.
• CompletionRoutine - the completion routine that is set for the previous (upper) layer (set with
IoSetCompletionRoutine), if any.
• Context - the argument to pass to the completion routine (if any).
• Parameters - this monster union contains multiple structures, each valid for a particular operation.
For example, in a Read (IRP_MJ_READ) operation, the Parameters.Read structure field should
be used to get more information about the Read operation.
The current I/O stack location obtained with IoGetCurrentIrpStackLocation hosts most of the
parameters of the request in the Parameters union. It’s up to the driver to access the correct structure,
as we’ve already seen in chapter 4 and will see again in this and subsequent chapters.
Viewing IRP Information
While debugging or analyzing kernel dumps, a couple of commands may be useful for searching or
examining IRPs.
The !irpfind command can be used to find IRPs - either all IRPs, or IRPs that meet certain criteria. Using
!irpfind without any arguments searches the non-paged pool(s) for all IRPs. Check out the debugger
documentation on how to specify specific criteria to limit the search. Here’s an example of some output
when searching for all IRPs:
lkd> !irpfind
Unable to get offset of nt!_MI_VISIBLE_STATE.SpecialPool
Unable to get value of nt!_MI_VISIBLE_STATE.SessionSpecialPool
Scanning large pool allocation table for tag 0x3f707249 (Irp?) (ffffbf0a8761000\
0 : ffffbf0a87910000)
Irp
[ Thread ]
irpStack: (Mj,Mn)
DevObj
[Driver\
]
MDL Process
ffffbf0aa795ca30 [ffffbf0a7fcde080] irpStack: ( c, 2)
ffffbf0a74d20050 [ \File\
Chapter 7: The I/O Request Packet
194
System\Ntfs]
ffffbf0a9a8ef010 [ffffbf0a7fcde080] irpStack: ( c, 2)