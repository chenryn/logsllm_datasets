-
ﬁnish queries
-
-
-
-
(a) FSM for SIM farm
(b) FSM for fake account trading
(c) FSM for fraud order operation
Fig. 3: Finite state machines for each e-commerce criminal role. Start states are in blue color.
TABLE I: State transition tables of the FSMs
(a) State transitions of the FSM for SIM farm
Simcard
Gateway
SimSource
Cross-role
Retrieval
End
w/o Simcard intel.
-
-
-
w/ Simcard intel.
w/ Gateway intel.
w/o Gateway intel. w/ Gateway intel.
-
-
-
-
negation
negation
ﬁnish query
-
interrogation
interrogation
interrogation
interrogation
wrong role
-
-
ﬁnish queries
(b) State transitions of the FSM for fake account trading
Account
Types
StoreLink
Payment
Cross-role
Retrieval
End
(c) State transitions of the FSM for fraud order operation
FraudTasks
Items
ShippingAddr
ReportLink
Cross-role
Retrieval
End
w/o task intel.
-
-
-
-
w/ task intel.
w/o Item intel.
-
-
-
w/ Item intel.
w/ Item intel.
w/o addr intel.
-
-
w/ addr intel.
negation/addr
-
-
-
negation
negation/addr&link
w/ link intel.
ﬁnish query
-
interrogation
interrogation
interrogation
interrogation
interrogation
wrong role
-
-
-
ﬁnish queries
the storefront (StoreLink state) in which to place orders and
the payment method (Payment state). Once those questions
are asked, Aubrey will then ask more about other related
ecosystems (Cross-role state) with the best effort, such as
“Which SIM gateway did you use to register the accounts?”
Figure 3(c) shows the FSM that guides the conversation
with the fraud order operators. In this case, we want
to
ﬁnd out which items are more likely to be targeted and the
fraudulent shipping addresses. Aubrey is disguised as a small-
time worker who seeks fraudulent order tasks (Fraud Tasks
state). If the operator has tasks, Aubrey further queries critical
details about the operations, including target items (Item state),
shipping addresses (Shipping Address state), and links to report
operations to receive commissions (Report Link state). Aubrey
also inquiries about other related roles, such as “Do you know
any website selling fake accounts?”
Knowledge source extension. Aubrey is not only powered by
the seed communication traces. Each state also includes the
questions collected from other sources (see Section IV-A).
Also, when the role starts asking questions, e.g., “How many
accounts do you need”, the system enters the states where
responses need to be retrieved from the knowledge source.
However, to the best of our knowledge, no labeled dataset can
serve this purpose, so we collected domain-speciﬁc question-
answer pairs as the knowledge source for both FSM states and
the retrieval model.
Speciﬁcally, we collected more than 1 million messages
from 150 underground IM groups and 135K threads from two
underground forums (Section IV-A). From these messages, our
approach automatically found question-answer pairs related
to the conversation with a given role. We ﬁrst applied the
message segmentation technique (Section III-C) to break each
6
Response analysis. The purpose of response analysis is to
understand the miscreant’s reply. Serving this purpose are a set
of NLP techniques employed by the Dialogue Manager
to determine whether a response is negative (e.g., “No fraud
account available.”), interrogative (e.g., “How many accounts
do you want?”), carrying target intelligence (e.g., “This is my
store link.”) or not (e.g., “There are lots of accounts in stock.”)
To identify a negative response, we utilize LTP [8] to an-
alyze the sentences’ grammatical structure and check whether
negative words (e.g., ‘no’, ‘don’t’) are used to describe the
sentences. Also we leverage the rule-based detection technique
proposed in [48] (e.g. question word (5W1H) + question
mark(‘?’)) to ﬁnd interrogative responses.
Further, we determine whether a response contains threat
intelligence by comparing the message from the role with
the answer part of the dialogue pairs associated with the
current state. If the response is semantically similar to the
answer conﬁrmed to carry target intelligence, or if the response
includes expected entities, e.g., account-selling website, we
consider it to be providing targeted intelligence. To this end,
our approach ﬁrst uses a set of regular expressions (e.g., for
URL matching) and topic words (e.g., ‘website’) to inspect the
response. Then, we run sentence embedding on the response
and each dialogue pair’s answer sentence to ﬁnd out whether
the response is semantically close to any of the known answers.
We represent each intelligence as a pair (entity, type), e.g.,
(shop.91kami.com, store link) or (“new account”, account
type). A sequence of such pairs serves as the foundation for
the state transition.
As mentioned above, we applied the state-of-the-art tools
[8] [48] for negation and interrogation detection. To evaluate
the effectiveness of these tools, we tested them on the 1K
message ground truth (half positive and half negative, labeled
on a randomly selected dataset), which resulted in a precision
of 98.6% for negation detection and 97.8% for interrogation
detection, as discovered by manual validation.
State transition. Working on the outcomes of the response
analysis (negation, question, sentence with or without intelli-
gence), the Dialogue Manager decides on the next step.
Here we represent a state transition as a sequence of (current
state, condition, next state). Figure 4 illustrates a set of rules
used by the Dialog Manager to guide state transitions.
Also, Table I shows the transition table of each FSM.
Speciﬁcally, if the response is negative, which means the
failure to collect intelligence, Aubrey simply goes to the Cross-
role state. This transition can be represented as ((Start state),
R is negative, (Cross-role state)). For example, when talking
to an account merchant, if the miscreant responds negatively
to the question “What types are you selling”, the follow-up
states for asking the store link and payment are skipped, and
Aubrey goes straight to inquires about other roles.
If the response carries intelligence (e, t) (with entity as e
and type as t), the Dialogue Manager then bypasses all
states related to that intelligence and transfers to the state for
collecting next intelligence, i.e., (S, (e, t) in R, {T|(e, t) not in T
and S→T}). As in Figure 3(a), if the gateway information (e.g.,
sfoxer.com) is in the response, the Gateway state is skipped,
and the next state becomes SimSource state.
Fig. 4: State transition rules.
trace (a sequence of messages from a group or a thread of
posts) into dialogue blocks based upon semantic similarity.
Then, we detected questions [48] from individual blocks and
used the follow-up message in the same block as an answer
to the question. Further, from these identiﬁed dialogue pairs,
our approach dropped those with stop sentences (e.g., “Ok,
thanks.” which appear with high frequency) and those whose
answers are also interrogative. In the end, each dialog pair is
an interrogative/declarative sentence combination.
To enrich the questions Aubrey asks at individual FSM
state, our approach further establishes the relations between
the states and dialogue pairs. We take several steps to ﬁlter out
the promising questions which are well suitable for Aubrey to
ask. The idea is to ﬁrst use the keyword list extracted from the
questions at each state to discover all related dialogue pairs
and then further validate their relevance to the state based
upon their semantics. Speciﬁcally, we ﬁrst run jieba [25] to
extract a keyword list from dialogue blocks (Section III-C) then
expand the keyword list using word2vec [39]. Our approach
then automatically goes through all the dialogue pairs, looking
for dialog pairs that contain words on the list. Among the
pairs, we further compare the semantics of the question with
the questions in the state, using sentence embedding [29]: only
those with a cosine similarity above 0.9 are added to the state.
Through knowledge extension, 750K total dialog pairs
were generated (Section IV-A), and 200 of them were care-
fully selected to enhance the FSMs, using the aforementioned
extension steps. All those pairs also served in retrieval model to
help answer the questions asked by the roles (Section III-D).
D. Dialog Management and Intel. Extraction
Dialog manager controls Aubrey’s conversations with
a criminal role, guiding the transitions of the states using the
function δ : S × R → S, and handling questions from the
role with a retrieval model. It gets the target and its role
from Target Finder and executes the FSM. The execution
begins with the start state, which sends a greeting to the target,
and proceeds through different states as required. At each state,
the Dialogue Manager ﬁrst gets a message and sends it
to the miscreant. Typically, such a message is a question (e.g.,
ask for fake account) randomly selected from all the candidate
questions at the current state. When the current state is for
answering a question from the role, however, the answer is
chosen by the retrieval model from the knowledge sources (i.e.,
extracted dialogue pairs). Upon receiving the response from
target role, the Dialog Manager analyzes the response and
invokes δ to determine which next state the system should
move into next as illustrated in Figure 4. We will now elaborate
on the response analysis and state transition.
7
If the response is a question asked by the role, Aubrey
then gets into the retrieval model to ﬁnd the most relevant
answer to the question. This transition is modeled as (S, R
is interrogative, (Retrieval model state)). The retrieval model
compares the question with the question part of all collected
dialogue pairs, measures the cosine similarity with sentence
embedding, then responds with the role with the answer part
of the most similar answer. Aubrey will return to the previous
state after responding the question from the role.
What can also happen is that
the response is neither
negative nor contains target intelligence. In this case, Aubrey
keeps the current state, randomly chooses another question to
ask: (S, Ro and |{R}| ≤ th, S), where Ro is the response
without intelligence nor negative/interrogative. To avoid the
system being stuck at the state forever, after two tries without
making any progress, the system moves to the Cross-role state,
that is, (S, |{R}| > th and S→Cross-role, Cross-role).
Intelligence extraction. As mentioned earlier, the intelligence
in the roles’ responses has been recovered and marked at the
response analysis stage in the form of a pair sequence for
each state. Note that for the intelligence not containing explicit
entities, e.g., storelink, its type can still be identiﬁed by a
similar sentence in our dialogues set that has been manually
labeled. In the end, the human analyst still needs to check
the intelligence for validation and also for ﬁnding additional
information from the raw dialogues, particular when it comes
to the data collected at the Cross-role state, where clues about
other criminal roles and activities could be found.
IV. EVALUATION
A. Experiment Setting
Our system operated on a Linux server with a 40 core Intel
Xeon CPU at 2.30GHz and 256GB memory. An open source
tool [3] was deployed to manage three QQ accounts to chat
with the miscreants. Here we describe the datasets used in the
study and the parameter settings of our system.
Datasets. As mentioned earlier, we used three datasets: the
seed dialogue dataset, IM group chat logs and underground
forum threads, which are summarized in Table II. In total, we
collected 750K dialogues (in Chinese) related to underground
e-commerce activities from 1 million QQ messages and 135K
threads. To the best of our knowledge, this is the largest
dialogue dataset in cybercrime research. We plan to make the
sanitized dataset available after publishing the paper.
• Seed dialog dataset. The seed dataset
includes 20 con-
versation trace samples from Company A’s security analysts’
conversations with different e-commerce miscreant roles. The
average length of the trace is 40 messages.
• IM group chat logs. We collected underground IM group
messages to identify target miscreants and to build a knowl-
edge base (Section III-B). In our implementation, we focused
on QQ, the most popular IM used by Chinese cybercrim-
inals [9]. Groups can be searched with keywords, and the
activities in the groups are often indicated by the group names.
To inﬁltrate into the fraud-dedicated groups, we searched
for 50 e-commerce fraud seed keywords (e.g., “SIM farm”,
“Company A fake account”, which are related to the targeted
miscreants) provided by Company A. For each role, we joined
TABLE II: Summary of datasets
Dataset
# of raw data
# of dialog pairs
seed conversations
IM group discussions
Forum discussions
800
1M
135K
200
50K
700K
utilize
embedding
settings. We
the top 50 most active and popular groups among the search
results. For each group, we tracked the chatlogs for the past
16 months (07/2017∼10/2018) and totaled 1 million group
chat traces. After processing the traces (Section III-C), 50,000
dialog pairs were generated. Note that these dialogues are not
only related to fraud activities but also about normal topics.
• Underground forum threads. We gathered discussion
threads from two popular underground e-commerce forums:
htys123.com and zuanke8.com. Speciﬁcally,
the corpus of
htys123 includes 25K threads from 10/2013 to 10/2018,
from which 250K dialog pairs (Section III-C) were collected;
the corpus of zuanke8 contains 110K threads from 06/2018
to 10/2018, from which 450K dialogue pairs were gathered.
Parameter settings. The parameters for our system implemen-
tation were set as follows: