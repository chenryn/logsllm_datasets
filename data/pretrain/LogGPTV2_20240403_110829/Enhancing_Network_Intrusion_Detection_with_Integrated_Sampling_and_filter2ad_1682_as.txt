τ
Tzfp
Tzfn
Threshold
τ
Tzfn Tzfp
Fig. 5. Training data distribution graph, used to
set τ. τ could be set to perfectly classify train-
ing data.
Fig. 6. Overlapping training data distribution
graph. No value of τ perfectly classiﬁes train-
ing data.
I[s] = I[s− 1] + (# vulnerable uninfected hosts)P(host becomes infected)
I[s] = I[s− 1] + (V − I[s− 1])(1− P(host does not become infected))
I[s] = I[s− 1] + (V − I[s− 1])(1− P(scan does not infect host)(# scans))
I[s] = I[s− 1] + (V − I[s− 1])(1− (1− P(scan infects host))(# scans))
I[s]=I[s− 1] + (V − I[s− 1])(1− (1− P(scan contacts host)P(scan not blocked))(# scans))
I[s] = I[s− 1] + (V − I[s− 1])(1− (1− 1
N F[s− 1])(N/L))
In Figure 4, we model the case of V = one million vulnerable hosts, L = one thou-
sand learner-monitored addresses, and N = 232 total addresses. In the case where the
worm is maximally-varying polymorphic, we assume that the learner needs ﬁve sam-
ples to generate a correct signature. In that case, only 4,990 (.0499%) vulnerable hosts
are infected before the correct signature is generated, stopping further spread of the
worm. By employing the Dropped Red Herring attack, the worm author increases this
to 330,000 (33.0%). The Randomized Red Herring attack is only slightly less effective,
allowing the worm to infect 305,000 (30.5%) vulnerable hosts using p = .999.
Given that the Dropped Red Herring and Randomized Red Herring attacks allow a
worm to infect a large fraction of vulnerable hosts even under this optimistic model, it
appears that the Conjunction Learner is not a suitable signature generation algorithm
for a distributed worm signature generation system.
4 Attacks on Bayes Learners
Bayes learners are another type of learner used in several adversarial learning appli-
cations, including worm signature generation and spam ﬁltering. We present several
practical attacks against Bayes learners, which can prevent the learner from ever gen-
erating an accurate signature, regardless of how many target-class samples it collects.
As a concrete example, we use Polygraph’s implementation of a Naive Bayes learner.
That is, a Bayes learner that assumes independence between features. Non-Naive Bayes
learners are not as commonly used, due partly to the much larger amount of training data
that they require. We believe that the attacks described here can also be applied to other
Paragraph: Thwarting Signature Learning by Training Maliciously
93
Bayes learners, possibly even non-naive ones that do not assume independence between
features.
4.1 Background on Bayes Learners
In the following discussion, we use the notation P(x|+) to mean the probability that
the feature or set of features x occurs in malicious samples, and P(x|−) to denote the
probability that it occurs in innocuous samples. This learner can be summarized as
follows:
– The learner identiﬁes a set of tokens, σ, to use as features. σis the set of tokens that
occur more frequently in the suspicious pool than in the innocuous pool. That is,
∀σi ∈ σ,P(σi|+) > P(σi|−). This means that the presence of some σi in a sample
to be classiﬁed can never lower the calculated probability that it is a worm.
– Classiﬁes a sample as positive (i.e., in the target class) whenever P(γ|+)
P(γ|−) > τwhere τ
is a threshold set by the learner, and γis the subset of σ that occurs in the particular
sample. We refer to P(γ|+)
– We assume conditional independence between features. Hence, P(γ|+)
P(γ|−) = ∏ P(γi|+)
P(γi|−)
– P(σi|−) is estimated as the fraction of samples in the innocuous pool containing
σi.
– P(σi|+) is estimated as the fraction of samples in the suspicious pool containing
σi.
– τ is chosen as the value that achieves a false positive rate of no more than F% in
P(γ|−) as the Bayes score, denoted score(γ)
the innocuous pool.
Setting the τ Threshold. The attacks we describe in this section all involve making it
difﬁcult or impossible to choose a good matching threshold, τ. For clarity, we describe
the method for choosing τ in more detail.
After the learner has chosen the feature set σ and calculated P(σi|+)
P(σi|−) for each fea-
ture, it calculates the Bayes score P(σ|+)
P(σ|−) for each sample in the innocuous pool and
suspicious pool, allowing it to create the training data distribution graph in Figure 5.
The training data distribution graph shows, for every possible threshold, what the cor-
responding false positive and false negative rates would be in the innocuous and suspi-
cious training pools. Naturally, as the threshold increases, the false positive rate mono-
tonically decreases, and the false negative rate monotonically increases. Note that Fig-
ure 5 and other training data distribution graphs shown here are drawn for illustrative
purposes, and do not represent actual data.
There are several potential methods for choosing a threshold τ based on the training
data distribution graph. The method described in Polygraph [15] is to choose the value
that achieves no more than F% false positives in the innocuous pool. One alternative
considered was to set τto Tz f p, the lowest value that achieves zero false positives in the
innocuous pool. However, in the examples studied, a few outliers in the innocuous pool
made it impossible to have zero false positives without misclassifying all of the actual
worm samples, as in Figure 6. Of course, a highly false-positive-averse user could set
F to 0, and accept the risk of false negatives.
94
J. Newsome, B. Karp, and D. Song
Innocuous Pool
False Positive Rate
100%
Suspicious Pool
False Negative Rate
Innocuous Pool
False Positive Rate
100%
Suspicious Pool
False Negative Rate
e
t
a
R
F%
e
t
a
R
F%
Threshold
τTzfp
Tzfn Tzfn’ Tzfn’’
Threshold
τ
τ ’
Fig. 7. Dropped Red Herring Attack. Spurious
tokens artiﬁcially shift false negative curve to
the right. It shifts back to the left when worm
samples without the spurious tokens are added
to the suspicious pool.
Fig. 8. Correlated Outlier attack
Another tempting method for choosing τ is to set it to Tz f n, the highest value that
achieves zero false negatives in the suspicious pool. However, we show in Section 4.2
that this would make the Bayes learner vulnerable to a red herring attack.
4.2 Dropped Red Herring and Randomized Red Herring Attacks Are Ineffective
Dropped Red Herring Attack. The method just described for choosing τ may seem
unintuitive at ﬁrst. However, it was carefully designed to prevent Dropped Red Herring
attacks, as illustrated in Figure 7. Suppose that τ was set to Tz f n, the threshold just low
enough to achieve zero false negatives in the training data. This may seem more intu-
itive, since it reduces the risk of false positives as much as possible while still detecting
all positive samples in the malicious pool.
Now suppose that the attacker performs the Dropped Red Herring attack. Since the
spurious features occur in 100% of the target-class samples, they will be used in the
feature set σ. Since each target-class sample in the malicious pool now has more in-
criminating features, the Bayes score of every target-class sample in the suspicious
pool increases, causing the false negative curve to be artiﬁcially shifted to the right.4
If the learner were to set τto T
(cid:19)(cid:19)
z f n (see Figure 7), then the attacker could successfully
perform the Dropped Red Herring attack. When a target-class sample includes one less
(cid:19)(cid:19)
spurious feature, its Bayes score becomes less than T
z f n. Hence it
would be classiﬁed as negative. Eventually the learner would get target-class samples
without that spurious feature in its malicious pool, causing the false negative curve to
(cid:19)
z f n. At
shift to the left, and the learner could update the classiﬁer with a threshold of T
that point the attacker could stop including another feature.
However, setting τ to the value that achieves no more than F% false positives is
robust to the Dropped Red Herring attack. Assuming that the spurious features do not
(cid:19)
z f n, where T
(cid:19)
z f n < T
4 The false positive curve may also shift towards the right. We address this in Section 4.3.
Paragraph: Thwarting Signature Learning by Training Maliciously
95
appear in the innocuous pool, the false positive curve of the training data distribution
graph is unaffected, and hence the threshold τ is unaffected.
Randomized Red Herring Attack. The Randomized Red Herring attack has little ef-
fect on the Bayes learner. The Bayes score for any given target-class sample will be
higher due to the inclusion of the spurious features. The increase will vary from sample
to sample, depending on which spurious features that sample includes. However, again
assuming that the spurious features do not appear in the innocuous pool, this has no
effect on τ. Hence, the only potential effect of this attack is to decrease false negatives.
4.3 Attack I: Correlated Outlier Attack
Unfortunately, we have found an attack that does work against the Bayes learner. The
attacker’s goal in this attack is to increase the Bayes scores of samples in the innocuous
pool, so as to cause signiﬁcant overlap between the training data false positive and
false negative curves. In doing so, the attacker forces the learner to choose between
signiﬁcant false positives, or 100% false negatives, independently of the exact method
chosen for setting the threshold τ.
Attack Description. The attacker can increase the Bayes score of innocuous samples
by using spurious features in the target-class samples, which also appear in some in-
nocuous samples. By including only a fraction β of the α spurious features, S, in any
one target-class sample, innocuous samples that have all α spurious features can be
made to have a higher Bayes score than the target-class samples.
The result of the attack is illustrated in Figure 8. The spurious features in the target-
class samples cause the false negative curve to shift to the right. The innocuous samples
that contain the spurious features result in a tail on the false positive curve. The tail’s
height corresponds to the fraction of samples in the innocuous pool that have the spu-
rious tokens. As the ﬁgure shows, regardless of how τ is chosen, the learner is forced
either to classify innocuous samples containing the spurious features as false positives,
or to have 100% false negatives.
The challenge for the attacker is to choose spurious features that occur in the innocu-
ous training pool (which the attacker cannot see) in the correct proportion for the attack
to work. The attacker needs to choose spurious features that occur infrequently enough
in the innocuous pool that the corresponding Bayes score P(S|+)
P(S|−) is large, but frequently
enough that a signiﬁcant fraction of the samples in the innocuous pool contain all of the
spurious features; i.e. so that the forced false positive rate is signiﬁcant.
Attack Analysis. We show that the attack works for a signiﬁcant range of parameters.
The attacker’s a priori knowledge of the particular network protocol is likely to allow
him to choose appropriate spurious features. A simple strategy is to identify a type of
request in the protocol that occurs in a small but signiﬁcant fraction of requests (e.g.
5%), and that contains a few features that are not commonly found in other requests.
These features are then used as the spurious features.
We ﬁrst determine what parameters will give the innocuous samples containing the
spurious features a higher Bayes score than the target-class samples. For simplicity, we
assume that P(si|−) is the same for each spurious feature si.
96
J. Newsome, B. Karp, and D. Song
Theorem 5. Given that each target-class sample contains the feature set W and βα
spurious features si chosen uniformly at random from the set of α spurious features
S, samples containing all α spurious features in S have a higher Bayes score than the
target-class samples when:
P(si|−) < β and ( β
P(si|−))βα−α≤ P(W|−)
The condition P(si|−) < β is necessary to ensure that P(si|−) < P(si|+). Otherwise,
the learner will not use the spurious features in the Bayes classiﬁer.
The second condition is derived as follows:
Innocuous samples have a higher Bayes score
≥ P(βS,W|+)
P(βS,W|−)
P(si|−)βαP(W|−) Independence assumption
P(si|−)βαP(W|−) Substitution
P(S|+)
P(S|−)
P(si|+)α
P(si|−)α ≥ P(si|+)βαP(W|+)
P(si|−)α ≥
βα
P(si|−))βα−α≤ P(W|−) Rearrangement
( β
ββα(1)
Note that while we have assumed independence between features here, the attack
could still apply to non-Naive Bayes learners, provided that P(S|+)
P(βS,W|−) is satis-
P(S|−)
ﬁed. Whether and how it can be satisﬁed will depend on the speciﬁc implementation of
the learner.
≥ P(βS,W|+)
When these conditions are satisﬁed, the classiﬁer must either classify innocuous sam-
ples containing the spurious features S as positive, or suffer 100% false negatives. Either
way can be considered a ‘win’ for the attacker. Few sites will be willing to tolerate a
signiﬁcant false positive rate, and hence will choose 100% false negatives. If sites are
willing to tolerate the false positive rate, then the attacker has succeeded in performing
a denial-of-service attack. Interestingly, the attacker could choose his spurious tokens
in such a way as to perform a very targeted denial-of-service attack, causing innocuous
samples of a particular type to be ﬁltered by the classiﬁer.
100% false negatives if P(S|−)
P(S|+)
ples containing the spurious features S as positive.
For the threshold-choosing algorithm used by Polygraph, τ will be set to achieve
≥ F. Otherwise it will be set to falsely classify the sam-
Evaluation. The Correlated Outlier is practical for an adversary to implement, even
though he must make an educated guess to choose the set of spurious features that
occur with a suitable frequency in the innocuous pool.
There are four parameters in Theorem 5 that determine whether the attack is suc-
cessful. The attacker chooses α, how many spurious features to use, and β, the fraction
of thosespurious features to include in each target-class sample. The likelihood of suc-
cess increases with greater α. However, since he must ﬁnd α spurious features that are
highly correlated in the innocuous pool, relatively low values are the most practical.
The third parameter, the frequency of the target-class features in the innocuous pool
P(W|−) is out of the attacker’s hands. High values of P(W|−) make the attack easiest.
Indeed, if P(W|−) is high, the learner is already forced to choose between false nega-
tives, and signiﬁcant false positives. We show the attack is still practical for low values
of P(W|−).
Paragraph: Thwarting Signature Learning by Training Maliciously
97
i
)
-
|
s
(
P
x
a
M
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
α=5 β=30%
α=10 β=30%
α=100 β=30%
 0.0002
 0.0004
 0.0006
 0.0008
 0.001
P(W|-)
i
)
-
|
s
(
P
x