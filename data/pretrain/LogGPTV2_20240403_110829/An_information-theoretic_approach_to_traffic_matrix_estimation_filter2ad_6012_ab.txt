p(x, y)
p(x)p(y)
= K(px,y||px × py), (7)
I(X, Y ) =Xx,y
where K(f||g) = i fi log(fi/gi) is the Kullback-Leibler diver-
gence of f with respect to g, a well-known measure of distance
between probability distributions.
Discrete Entropy is frequently used in coding because the entropy
H(X) gives a measure of the number of bits required to code the
values of X. That is, if we had a large number n of randomly-
generated instances X1, X2, . . . , Xn and needed to represent this
stream as compactly as possible, we could represent this stream us-
ing only nH(X) bits, using entropy coding as practiced for example
in various standard commercial compression schemes.
Entropy has also been advocated as a tool in the estimation of
probabilities. Simply put, the maximum entropy principle states that
we should estimate an unknown probability distribution by enumer-
ating all the constraints we know it must obey on ‘physical’ grounds,
and searching for the probability distribution that maximizes the en-
tropy subject to those constraints. It is well known that the proba-
bility distributions occurring in many physical situations can be ob-
tained by the maximum entropy principle. Heuristically, if we had
no prior information about a random variable X, our uncertainty
about X is at its peak, and therefore we should choose a distribu-
tion for X which maximizes this uncertainty, or the entropy. In the
case where we do have information about the variable, usually in the
form of some set of mathematical constraints C, then the principle
states that we should maximize the entropy H(X|C) of X condi-
tional on consistency with these constraints. That is, we choose the
solution which maintains the most uncertainty while satisfying the
constraints. The principle can also be derived directly from some
simple axioms which we wish the solution to obey [20].
2.4 Ill-Posed Linear Inverse Problems
Many scientiﬁc and engineering problems have to solve inference
problems which can be posed as follows. We observe data y which
are thought to follow a system of linear equations
y = Ax,
(8)
where the n by 1 vector y contains the data, and the p by 1 vector
x contains unknowns to be estimated. The matrix A is an n by p
matrix. In many cases of interest p > n, and so there is no unique
solution to the equations. Such problems are called ill-posed linear
inverse problems. In addition, frequently the data are noisy, so that
it is more accurate to write
y = Ax + z.
(9)
In that case any reconstruction procedure needs to remain stable un-
der perturbations of the observations. In our case, y are the SNMP
link measurements, x is the trafﬁc matrix written as a vector, and A
is the routing matrix.
There is extensive experience with ill-posed linear inverse prob-
lems from ﬁelds as diverse as seismology, astronomy, and medical
imaging [1, 2, 17, 18, 26], all leading to the conclusion that some
sort of side information must be brought in, producing a reconstruc-
tion which may be good or bad depending on the quality of the prior
information. Many such proposals solve the minimization problem
(cid:5)y − Ax(cid:5)2
2
J(x),
x
min
2 + λ
(10)
where where (cid:5) · (cid:5)2 denotes the L2 norm, λ > 0 is a regulariza-
tion parameter, and J(x) is a penalization functional. Proposals of
this kind have been used in a wide range of ﬁelds, with consider-
able practical and theoretical success when the data matched the as-
sumptions leading to the method, and the regularization functional
matched the properties of the estimand. These are generally called
strategies for regularization of ill-posed problems (for a more gen-
eral description of regularization see [11]).
A general approach to deriving such regularization ideas is the
Bayesian approach (such as used in [23]), where we model the es-
timand x as being drawn at random from a so-called ‘prior’ prob-
ability distribution with density π(x) and the noise z is taken as a
Gaussian white noise with variance σ2. Then the so-called posterior
probability density p(x|y) has its maximum ˆx at the solution of
(cid:5)y − Ax(cid:5)2
2 + 2 · σ
min
x
2
log π(x).
(11)
Comparing this with (10) we see that penalized least-squares prob-
lems as giving the most likely reconstructions under a given model.
Thus the method of regularization has a Bayesian interpretation, as-
suming Gaussian noise and assuming J(x) = log π(x). We stress
that there should be a good match between the regularization func-
tional J and the properties of the estimand — that is, a good choice
of prior distribution. The penalization in (10) may be thought of as
expressing the fact that reconstructions are very implausible if they
have large values of J(·).
Regularization can help us understand approaches such as that
of Vardi [24] and Cao et al. [3], which treat this as a maximum
likelihood problem where the x are independent random variables
following a particular model. In these cases they use the model to
form a penalty function which measures the distance from the model
by considering higher order moments of the distributions.
2.5 Shrinkage Estimation
An alternative reasoning behind regularization is that in estimat-
ing large numbers of parameters (as in the problem above), ‘shrink-
ing’ an otherwise valid estimates towards a special point results in
substantial reductions in mean-squared error. As a simple example,
suppose we have noisy data y = x+z, where y, x and z are all n×1
vectors. We wish to recover the vector x, where z represents Gaus-
sian white noise N (0, 1). The raw data components yi are unbiased
minimum variance estimators of the corresponding components xi
of the estimand x, so it is tempting to believe that y is the optimal
estimate of x. In fact, if n is large, it is possible to do substantially
better than using y. We should instead solve the penalized problem
(cid:5)y − x(cid:5)2
2 + ˆλ
2(cid:5)x(cid:5)2
2,
min
x
(12)
2
where ˆλ = n(cid:1)y(cid:1)2
is a measure of the dataset’s size in mean-square
(or rather its reciprocal). The solution is a compromise between ﬁ-
delity to the measured data y and closeness to the origin, and has the
simple form ˆx∗
y. This reconstruction is obtained simply by
‘shrinking’ the raw data y towards zero. It turns out that for large n
this shrunken estimator is always better than the ‘obvious’ unbiased
estimate y, in the sense that it always has a lower mean-squared er-
ror. This qualitative conclusion remains true if we shrink towards
some other ﬁxed point, though it is better to shrink towards a point
= 1
1+λ
close to the x we are trying to estimate. For a fuller discussion of
shrinkage estimation, see for example [13, 6]. For now, simply note
that shrinkage of a very high-dimensional estimand towards a cho-
sen point can be helpful. Note that no Bayesian assumption is being
made here: whatever the underlying estimand may be, shrinkage is
an improvement, regardless of our prior beliefs about which vectors
x are plausible. The key assumption is that we are trying to estimate
a vector with many components, all affected by noise.
3. REGULARIZATION OF THE TRAFFIC
ESTIMATION PROBLEM USING MINI-
MUM MUTUAL INFORMATION
The problem of inference of the end-to-end trafﬁc matrix is mas-
sively ill-posed because there are so many more routes than links in
a network. In this section, we develop a regularization approach us-
ing a penalty that seems well-adapted to the structure of actual traf-
ﬁc matrices, and which has some appealing information-theoretic
structure. Effectively, among all trafﬁc matrices agreeing with the
link measurements, we choose the one that minimizes the mutual
information between the source and destination random variables.
Under this criterion, absent any information to the contrary, we
assume that the conditional probability p(d|s) that a source s sends
trafﬁc to a destination d is the same as p(d), the probability that the
network as a whole sends packets or bytes to destination d. There
are strong heuristic reasons why the largest-volume links in the net-
work should obey this principle — they are so highly aggregated that
they intuitively should behave similarly to the network as a whole.
On the other hand, as evidence accumulates in the link-level statis-
tics, the conditional probabilities are adapted to be consistent with
the link-level statistics in such a way as to minimize the mutual in-
formation between the source and destination random variables.
This Minimum Mutual Information (MMI) criterion is well-suited
to efﬁcient computation. It can be implemented as a convex opti-
mization problem; in effect one simply adds a minimum weighted
entropy term to the usual least-squares lack of ﬁt criterion. There
are several widely-available software packages for solving this opti-
mization problem, even on very large scale problems; some of these
packages can take advantages of the sparsity of routing matrices.
3.1 Trafﬁc-Matrix Estimation
Let N (s, d) denote the trafﬁc volume going from source s to des-
tination d in a unit time. Note that N (s, d) is unknown to us; what
can be known is the trafﬁc T (l) on link l. Let A(s, d; l) denote the
routing matrix, i.e. A(s, d; l) gives the fraction of trafﬁc from s to
d which crosses link l (and which is zero if the trafﬁc on this route
does not use this link at all). The link-level trafﬁc counts are
A(s, d; l)N (s, d), ∀l ∈ L,
(13)
T (l) =Xs,d
where L is the set of backbone links. We would like to recover the
trafﬁc matrix N (s, d) from the link measurements T (l), but this is
the same as solving the matrix equation (8), where y is a vector
containing the trafﬁc counts T (l), x is a vectorization of the trafﬁc
matrix, and A is the routing matrix. A is a matrix which is #L by
(#S × #D), where there are #L link measurements, #S sources,
and #D destinations.
3.2 The Independence Model
We propose thinking about N (s, d) in probabilistic terms, so that
if a network carries N end-to-end packets (or bits) total within a unit
time then the number of packets sent from source s to destination d,
N (s, d) say, is a random variable with mean N·p(s, d), with p(s, d)
the joint probability that a randomly chosen one of the N packets (or
bits) goes from s to d. We consider the marginal probabilities
pS(s) = Xd
pD(d) = Xs
p(s, d),
p(s, d),
(14)
(15)
the chance that a randomly-chosen packet (bit) enters the network
at s, and the chance that a randomly chosen packet (bit) departs at
d, respectively. We can expand this notation to measure sets:
pS,D(Qs, Qd) = Xs∈Qs Xd∈Qd
p(s, d),
(16)
for all sets of source and destination links Qs, Qd, and similarly for
the marginal probabilities ps and pd.
We let S be the random variable obtained looking at the source
of a random packet (or bit), and let D denote the destination. Sup-
pose for sake of discussion that S and D are independent random
variables. Then (2) means that, given that a packet (bit) originates at
S = s, it is no more likely to go to D = d than would a randomly-
chosen packet (bit) originating anywhere in the network. For net-
works containing a few extremely high volume links carrying very
large fractions of the packets, the assumption (2) should work well
for the very largest circuits, since they have been so highly aggre-
gated that their behavior may be very similar to the network as a
whole.
Note that the independence of source and destination is equivalent
to the simple gravity model which has been discussed in the Internet
measurement community; the model has the form
N (s, d) ≈ Const N (s)N (d)
(17)
where N (s) is the trafﬁc entering at s, and N (d) is the trafﬁc ex-
iting at d. While there is experience with the gravity model above
and some success in its application, it is also known that it gives
results that are not as accurate as may be obtained using additional
information [16, 28].
Section 2 suggests that regularization is a way of using prior in-
formation in conjunction with link measurements to help decide
which trafﬁc matrices from the set satisfying (8) are more plausible.
We propose using a regularization functional that uses the indepen-
dence/gravity model as a point of departure, but which considers
other models as well. Recall from our discussion of information
theory that independence of source and destination is tantamount to
the statement that the mutual information vanishes: I(S, D) = 0.
Recall also that I(S, D) ≥ 0. It follows that the penalty functional
on trafﬁc matrices p(s, d), given by
J(p) ≡ I(S, D),
has J(T ) ≥ 0 with equality if and only if S and D are independent.
This functional has an interpretation in terms of the compress-
ibility of addresses in IP headers. Suppose we have a large number
of IP headers — abstracted to be simply source/destination address
pairs (si, di), i = 1, . . . , N. We want to know: what is the minimal
number of bits required (per header) to represent the source desti-
nation pair. It turns out that this is just H(S) + H(D) − I(S, D).
Now if we simply applied entropy compression to the Si and Di
streams separately, we would pay H(S) + H(D) bits per header
to represent headers. Hence the functional I(S, D) measures the
number of bits of additional compression possible beyond the sepa-
rate compression of source and destination based on traditional en-
tropy compression. This extra compression is possible because of
special dependencies that make IP messages more likely to go in
certain source/destination pairs than we would have expected by in-
dependence.
In fact measurements of H(S) and H(D) (on real
datasets described below) are typically around 5, while I(S, D) is
very small, typically around 0.1. This suggests that the indepen-
dence assumption is a reasonable ﬁt to the real data, at least on av-
erage. There may be some links for which it is not, but the MMI
method speciﬁcally allows for correction to these (see below).
Suppose we adopt a Bayesian viewpoint, assigning an a priori
−J (p).
probability π(p) to the trafﬁc matrix p that is proportional to 2
Then we are saying we regard as a priori implausible those trafﬁc
matrices where much higher compression is possible based on joint
source-destination pairs as compared to compression of sources and
destinations separately. Each bit saved reduces our a priori likeli-
hood by about a factor 1/2.
3.3 Regularization Method
We propose now to reconstruct trafﬁc matrices by adopting the
regularization prescription (10) with the regularization functional
J(p) = I(S, D). Translating (10) into trafﬁc-matrix notation, we
seek to solve
2
2
+ λ
I(S, D),
minimize Xl
T (l) − NXs,d
0
@
A(s, d; l)p(s, d)1
A
(18)
Recalling the Bayesian interpretation of regularization, we are
saying that we want a trafﬁc matrix which is a tradeoff between
matching the observed link trafﬁc counts and having a priori plausi-
bility, where our measure of plausibility, as just explained, involves
the ‘anomalous compressibility’ of source-destination pairs. The
trafﬁc matrix obtained as the solution to this optimization will be a
compromise between two terms based on the size of λ, which is a
proxy for the noise level in our measurements. Note that
p(s, d)
p(s)p(d)
= K(p(s, d)||p(s)p(d)), (19)
p(s, d) log
I(S, D) =Xd,s
where K(·||·) again denotes the Kullback-Leibler divergence. Here
p(s)p(d) represents the gravity model, and K(·||·) can be see as a
distance between probability distributions, so that we can see (18)
as having an explicit tradeoff between ﬁdelity to the data and de-
viation from the independence/gravity model. Note also that the
Kullback-Leibler divergence is the negative of the relative entropy
of p(s, d) with respect to p(s)p(d), and so this method also has an
interpretation as a maximum entropy algorithm.
Both terms in the above tradeoff are convex functionals of the
trafﬁc matrix p. Hence, for each given λ, they can be rewritten in
constrained optimization form:
minimize K(p(s, d)||p(s)p(d)) subject to
l(T (l) − Ns,d A(s, d; l)p(s, d))2 ≤ χ2.
(20)
Here χ2 = χ2(λ) is chosen appropriately so that the solution of
this problem and the previous one are the same, at the given value
of λ. The problem is saying: among all trafﬁc matrices adequately
accounting for the observed link counts, ﬁnd the one closest to the
gravity model. It can also be viewed as saying: shrink away from
the observed link counts towards the gravity model.
Thinking heuristically, we are trying to estimate a very large num-
ber of unknowns, so shrinkage towards the gravity model can be ex-