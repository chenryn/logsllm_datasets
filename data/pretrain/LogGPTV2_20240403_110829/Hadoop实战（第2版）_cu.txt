分组调度计算资源，实现存储共享、计算独立，在Hadoop中运行的进程是不可抢占的。
在大块文件系统中，X86平台下一个页的大小是4K B。如果页较小，管理的数据就
会很多，会增加数据操作代价并影响计算效率，因此需要提高页的大小。百度在使用Hadoop时也遇到了一些问题，主要有：
MapReduce的效率问题：比如，如何在shuffle效率方面减少I/O次数以提高并行效率；如何在排序效率方面设置排序为可配置的，因为排序过程会浪费很多的计算资源，而一些情况下是不需要排序的。
HDFS的效率和可靠性问题：如何提高随机访问效率，以及数据写入的实时性问题，如果Hadoop每写一条日志就在HDFS上储存一次，效率会很低。
内存使用的问题：Reducer端的shuffle会频繁地使用内存，这里采用类似Linux的Buddy System来解决，保证Hadoop用最小的开销达到最高的利用率；当Java进程内容使用内存较多时，可以调整垃圾回收（GC）策略；有时存在大量的内存复制现象，这会消耗大量CPU资源，同时还会导致内存使用峰值极高，这时需要减少内存的复制。
作业调度的问题：如何限制任务的Map和Reduce计算单元的数量，以确保重要计算可以有足够的计算单元；如何对TaskTracker进行分组控制，以限制作业执行的机器，同时还可以在用户提交任务时确定执行的分组并对分组进行认证。
性能提升的问题：UserLogs cleanup在每次Task结束的时候都要查看一下日志决定是否清除，这会占用一定的任务资源，可以通过将清理线程从Java子进程移到TaskTracker来解决；Java子进程会对文本行进行切割而Map和Reduce进程则会重新切割，这将造成重复处理，这时需要关掉Java进程的切割功能；在排序的时候也可以实现并行排序提升性能；实现对数据的异步读写也可以提升性能。
健壮性的问题：需要对Mapper和Reducer程序的内存消耗进行限制，这就要修改Linux内核，增加其限制进程的物理内存的功能，也可以通过多个Map程序共享一块内存，以一定的代价减少对物理内存的使用；还可以将DataNode和TaskTracker的UGI配置为普通用户并设置账号密码；或者让DataNode和TaskTracker分账号启动，确保HDFS数据的安全性，防止Tracker操作DataNode中的内容；在不能保证用户的每个程序都很健壮的情况下，有时需要将进程终止掉，但要保证父进程终止后子进程也被终止。
Streaming局限性的问题：比如，只能处理文本数据，Mapper和Reducer按照文本行的协议通信，无法对二进制的数据进行简单处理。为了解决这个问题，百度新写了一个类Bistreaming（Binary Streaming），这里Java子进程Mapper和Reducer按照（KeyLen, Key, ValLen, Value）的方式通信，用户可以按照这个协议书写程序。
用户认证的问题：这个问题的解决办法是使用户名、密码、所属组都在NameNode和JobTracker上集中维护，用户连接时需要提供用户名和密码，从而保证数据的安全性。百度下一步的工作重点主要涉及以下内容：
内存方面，降低NameNode的内存使用并研究JVM的内存管理；
调度方面，改进任务可以被抢占的情况，同时开发出自己的基于Capacity的作业调度器，让等待作业队列具有优先级且队列中的作业可以设置Capacity，并可以支持TaskTracker分组；
压缩算法，选择较好的方法提高压缩比、减少存储容量，同时选取高效率的算法用于shuffle数据的压缩和解压；
对Mapper程序和Reducer程序使用的资源进行控制，防止过度消耗资源导致机器死机。以前是通过修改Linux内核来进行控制的，现在考虑通过在Linux中引入Cgroup来对Mapper和Reducer使用的资源进行控制；
将DataNode的并发数据读写方式由多线程改为select方式，来支持大规模并发读写和Hypertable的应用。
百度同时也在使用Hypertable，它是以Google发布的BigTable为基础的开源分布式数据存储系统，百度将它作为分析用户行为的平台，同时在元数据集中化、内存占用优化、集群安全停机、故障自动回复等方面做了一些改进。
19.4 即刻搜索中的Hadoop
 19.4.1 即刻搜索简介
即刻搜索是由运营一年的人民搜索改名而来，它秉承“创新、公正、权威”的理念，致力于成为大众探索求知的工具、工作生活的助手和文化交流的平台。相对于网络上百家争鸣的搜索引擎，即刻搜索区别于其他引擎的特点是：新颖的索引架构，先进的大规模并行处理系统，大规模应用的闪存技术和干净、便捷的网络搜索环境。即刻搜索在开发之初就用到了Hadoop和并行处理的思想，下面介绍即刻搜索中的Hadoop应用。
19.4.2 即刻Hadoop应用架构
图19-5是即刻搜索引擎的应用架构图。
即刻搜索框架结构比较明了。下面我们对框架各个角色进行介绍。
链接库：这是一个保存了网络中内外部链接的初始数据库，即刻搜索根据数据库中的链接进行网页的爬取。
即刻爬虫：网络爬虫是搜索引擎中必不可少的部分，即刻爬虫根据链接库的内容，爬取网络中的页面资源，形成SSTable并输入HDFS中。SSTable是Google在BigTable设计中提出的一种磁盘文件存储结构，全称是Sorted String Table，以＜key, value＞对方式在磁盘上存储数据，并根据key的值进行排序，支持随机查找，有不俗的读写性能。
HDFS_Bridge：这是即刻搜索的中间件，为网络爬虫提供写缓存服务，保证爬虫快速写操作。具体来说，通过HDFS_Bridge，爬虫生成的SSTable文件，首先以内存写的速度将数据写入HDFS_Bridge，然后由DFS直接将数据文件写出到DFS磁盘上，这样通过HDFS_Bridge就将SSTable数据的磁盘I/O转化成了内存写，提高了速度。
HDFS：即刻搜索中保存SSTable的分布式文件系统，主要提供海量数据的存储服务，保证数据的安全性和读写服务的可靠性。
MapReduce：这一层主要应用Hadoop中的MapReduce并行编程框架来对爬虫原始数据进行分析，包括PageRank计算、链接分析统计、倒排索引生成等，主要提高了搜索引擎中数据分析步骤的速度，实现了并行化处理。
online-service-cluster：这一层是面向用户的，主要根据用户输入的查询，分析关键词，通过并行框架查找相关结果并返回。
图 19-5 即刻搜索架构图
这里再重点介绍一下MapReduce层。在即刻搜索中，各种基础数据上的操作都是以C++语言编写、经过Hadoop Pipes的封装之后提交给Hadoop执行的，但是具体使用中即刻搜索也从代码层修改了Hadoop Pipes的协议等内容来适应自己的需求。在具体使用中，即刻搜索首先定义了MapReduce框架中的Mapper封装器和Reducer封装器，以Mapper封装器为例，其核心代码如下：
Wrapperclass BasicMapper{
public：
typedef：mapreduce：TaskContextTaskContext；
explicit BasicMapper（）{
map_context_.reset（new MapContext（））；
}
virtual～BasicMapper（）{}
//初始化操作
virtual void OnCreate（TaskContext*context）{}
//定义Map阶段
virtual void OnFirstMap（MapContext*context）{}
virtual void OnLastMap（MapContext*context）{}
virtual void Map（MapContext*context）=0；
……
protected：
//获取输入value
const std：string＆GetInputValue（）{
return map_context_-＞GetInputValue（）；
}
//获取输入key
const std：string＆GetInputkey（）{
return map_context_-＞GetInputKey（）；
}
//反序列化
template＜typenameT＞
boolValueToThrift（T*object）{
return map_context_-＞ValueToThrift（object）；
}
……
DISALLOW_COPY_AND_ASSIGN（BasicMapper）；
}
利用这些封装器作为基类，编写自己的MapReduce框架代码就非常方便。下面就是一个简单的例子：
class SampleMap：public BasicMapper{//Mapper
public：
virtual～SampleMap（）{}
virtual void Map（mapreduce：MapContext*context）
{
string key=GetInputKey（）；
string value=GetInputValue（）；
Object obj_val；
ValueToThrift（＆obj_val）；//反序列化
……
context-＞Emit（key, newvalue）；//输出
}
}；
class SampleRed：public BasicReducer{
public：
virtual void Reduce（mapreduce：MapContext*context）{
string key=GetInputKey（）；
while（NextValue（））{
string value=GetInputValue（）；
……
}
context-＞Emit（key, newvalue）；//输出
}
}；
可以看出：上面的代码同用Java语言编写的代码非常类似。除了上面的主体Mapper和Reducer代码之外，再定义好其他作业信息就可以提交给Hadoop Pipes来运行了。
19.4.3 即刻Hadoop应用分析
前面简单介绍了即刻搜索的框架和在即刻搜索中如何开发自己的MapReduce程序。可以看出，即刻搜索在应用Hadoop时直接应用了Hadoop系统，在搜索引擎的数据存储模块直接使用Hadoop的数据存储服务，在任务执行和处理模块时直接使用MapReduce并行框架。虽然是直接使用，但是并不简单。作为独立的系统，Hadoop在应用到某个系统中时，需要将Hadoop各个模块根据自己系统的实际需求进行封装。在分布式存储模块，根据海量数据存储的需求，即刻搜索在HDFS的输入上由HDFS_Bridge进行封装。通过此封装。HDFS能为即刻搜索的网络爬虫提供写缓存，保证其海量数据的写入速度。在MapReduce框架模块，即刻搜索根据并行任务执行的需求，对MapReduce中的Mapper和Reducer进行了封装，简化了程序员代码书写难度。
总体来说，即刻搜索在系统中根据自己的需求，封装了Hadoop中分布式文件系统和MapReduce并行框架的对外接口，提高了系统的处理效率和存储性能。
19.5 Facebook中的Hadoop和HBase
众所周知，Facebook是目前世界上最大的社交网站。从2004年创建之初的以服务学生为目的的局部交互网站发展到2009年世界范围内的综合社交网站，服务8亿多人群，而现在它已经剑指移动服务、搜索服务、网络直播等综合网络服务提供领域，旨在发展成为综合性网络服务商。
Facebook作为全球性社交网站，拥有约8亿活跃用户，其每天产生的数据非常庞大。下面简单列举一些统计数据（截至2011年9月）：
把Facebook用户群作为一个国家，它会成为世界人口第三大国家；
Facebook用户在网站上已上传了1400亿余张照片；
每天上网的人中有44%访问了Facebook，它是继Google之后访问率第二高的网站；
Facebook用户每20分钟发表1200万条评论，每个月分享300亿条内容；
上面这些统计数据背后都意味着Facebook面临着海量的数据存储。用户群的资料需要维护，用户分享的照片、发表的评论、分享的内容都需要存储，用户访问历史需要进行分析处理，同时还需要对原始数据进行提取、反馈给用户等。这些虽然并不简单，但在巨大用户群和使用率面前，都将成为典型的海量数据存储和处理任务。那么Facebook到底面临哪些海量数据的任务？它为什么使用Hadoop+HBase？它是否有所创新或者改进？下面将一一解答这些问题。
 19.5.1 Facebook中的任务特点
Facebook的巨大用户群和使用率为其带来了高效存储海量数据的挑战。下面我们从Facebook中一些关键性技术的任务特点出发，介绍Facebook在存储海量数据时必须满足的一些特性。
1.Facebook消息机制
Facebook的消息机制为每一个用户提供一个“facebook.com”的邮箱地址，它负责整合用户或组之间的邮件、SMS（Short Message Service）以及聊天记录。该机制是Facebook收件箱的基础，需要管理“消息从哪位用户发往哪位用户”。该新型应用程序不但需要能够适应同时为超过5亿用户提供服务，还需要达到PB级数据的吞吐以及长时间不间断运行的需求。除此之外，新的线程模型同样需要系统能够存储每一个参与用户的消息。每一个用户需要依附于某一个数据中心。
这个机制的服务内容决定了它每天需要处理数十亿的即时消息及数百万的系统消息，而这些消息的特点又决定了该机制有如下特点：
1）高写吞吐量：由于每时每刻都有成千上万消息产生，那么每天数据的插入量将会非常大，并且会持续性地增长。
2）数据的增量存储：从该机制的特性不难发现，消息机制一般很少涉及删除操作，除非用户显式地发出该请求。此外，每一个用户的收件箱将可能无限量地增长。另外，对于每条消息记录，一般只会在近期被读有限的次数，从长远来讲很少再次查看。因此，大部分的数据不会再次被读到，但是由于存在用户访问的可能性，Facebook需要保证这部分数据一直处于可用状态。在据图存储这一类的数据，Facebook以用户为主键来建立索引，在索引之下存储该用户的线程和消息记录。
3）数据迁移：由于消息机制进行更新，采用新的系统以及数据模型，这就意味着Facebook需要将数据从原数据库中进行分离并迁移到新的数据库中。那么支持大范围扫描、随机访问以及快速大数据块导入操作的系统将会大大减少用户数据迁移的时间。
2.Facebook Insights
Facebook Insights为开发者以及网站管理员提供了关于Facebook站点之间活动实时分析的接口，包括社会网络插件、Facebook页面以及Facebook广告。通过这些匿名化的数据，一些商业用户可以对Facebook的情况有深入的了解，例如印象（impression）、点击率、网页访问次数等。通过这些信息，商业用户可以对自己的服务进行改进。
从这个技术的服务内容来看，Facebook Insights团队想要为用户提供短时间内用户活动的统计信息，也就是在海量统计信息上的实时数据分析。这将需要Facebook能够提供大规模的、异步排队的系统，并使用系统对事件进行处理、存储和聚合操作。该系统应具备较高的容错性，且支持每秒成千上万个事件的并发操作。
3.Facebook度量系统
在Facebook中，系统中的所有软硬件需要将自身的统计信息存储到ODS（Operations Data Store）中。例如，记录某一个服务器或某一组服务器中的CPU使用率；或者，存储对数据集群的写操作记录。这些操作将对写吞吐量有很高的要求。这要求系统应具备如下特点：
1）自动分区：大量的索引以及时序写操作再加上不可预知的数据量的增长，这使得采用sharding模式的MySQL数据库难以应付，甚至需要管理人员人为地对数据进行分片。
2）快速读取最近数据并执行表扫描：在Facebook很多的操作仅仅涉及最近的数据，对较早的数据访问较少，但是之前的数据也同样不能丢失，必须保证其处于可用状态。例如邮件服务、消息服务等。同时，这些操作还要求对最近的数据具有较快的查询速度。
结合具体技术特点的介绍和社交网络网站共有的一些特点，Facebook中任务特点对存储系统的需求可以总结为如下几个方面：
（1）灵活性
由于用户的增加以及市场的拓展，Facebook要求存储系统能够支持对系统容量的增量扩充，并且要求该操作所带来的额外开销要最小化，同时应避免该操作所带来的停机问题。例如，在某些情况下，Facebook可能需要能够快速地增加系统的容量，并且要求系统能够自动地处理新旧硬件之间的负载均衡和利用率的问题。
（2）高的写吞吐量
在Facebook中有很多的应用程序需要存储大量的数据，而对读的需求相对要低。因此，Facebook对写操作有较高的要求。
（3）高效低延迟的强一致性数据中心
在Facebook中有很多非常重要的应用程序（如消息），它们对数据的一致性有很高的要求。例如，在用户主页上显示的“未读”消息数目需要与收件箱中实际的“未读”消息总数一致，然而实现一个全球化的分布式强一致性存储系统确实不可能，只有当数据位于同一个数据中心之内，提供强一致性的数据存储才变得稍有可能。
（4）高效的磁盘随机读
尽管应用程序级别的缓存（嵌入式或见解缓存）得到了广泛的应用和发展，在Facebook，仍然有很多的访问不能命中缓存中的数据，而必须访问后端的数据库系统。
（5）高可用性
Facebook需要为用户提供一种能够长时间不间断的服务，这些服务应该能够处理计划内的和计划外的事件（例如软件更新、硬件或容量扩充以及硬件故障）。此外，Facebook还需要能够容忍数据中心少量数据的丢失以及在某一可允许的时间范围内向其他数据中心进行数据备份。
（6）容错性
Facebook中对于长时间的MySQL数据库的运维经验显示故障隔离是非常重要的。如果某一个数据库发生故障，那么在Facebook中要求只有很少一部分用户会受到该事件的影响。
（7）原子“读-修改-写”原语
原子的增量以及比较和交换API在创建无锁并发应用程序中非常有用，同时也是底层存储系统必须支持的操作。
（8）范围扫描
某些应用程序需要支持某一范围内某些列集合的高效检索。例如，检索某一用户最近的100条消息记录或者计算某一给定广告商在过去24小时内每小时的印象（impression）数。
同样Facebook中的任务也决定了它可以不强制要求下面几点：
1）单个数据中心内的网络分区容错性。不同的系统组件往往本身就是非常集中化的。例如，MySQL服务器很可能会集中地被放置在几个机架之内。单个数据中心内的网络分区故障将可能导致整个服务能力上的故障。因此，需要通过设置冗余网络来避免单个分区故障引起的系统不可用。
2）零故障率。从经验来看，大集群中机器故障是不可避免的。既然不存在这样的理想情况，那么必须对设计方案进行某种，也就是说，Facebook选择面对故障的机器并尽可能地降低宕机的概率。
3）跨数据中心的active-active服务能力。如前所述，Facebook假设用户的数据被存储在不同数据中心。因此，Facebook使用用户端的缓存来降低系统的延迟。
19.5.2 MySQL VS Hadoop+HBase
面对Facebook中任务对存储系统的要求，Facebook如何选择呢？
1.MySQL
MySQL是比较流行的一款开源数据库，它轻巧简便。Facebook在最初使用MySQL+Memcached构建存储层，MySQL集群数据库作为底层存储，Memcached构建数据缓存层。这样既发挥了MySQL存储系统较高的随机读效率及其简单好用等特点，又通过Memcached缓存层在高访问量下提高了系统的访问效率。
但是Facebook在发展过程中逐渐发现，MySQL在数据量剧增和新应用上线提供服务情况下并不能像之前那样完美地工作。主要是MySQL集群有以下问题：随机写操作效率低、可扩展性差、管理成本和硬件成本高、负载均衡并不理想等。而这些缺点恰巧正是Facebook中海量数据所带来的系统需求。所以目前Facebook已经放弃MySQL+Memcached构建的存储层，而转向了Hadoop+HBase。
2.Hadoop+HBase
Facebook在发展过程中发现MySQL构建的存储层并不能完全满足系统的需求后，就开始审视到底什么样架构的存储层能够最大程度上满足Facebook的需求。
经过大量地研究和实验之后，Facebook最终选择使用Hadoop和HBase来作为下一代底层存储系统。这主要是基于：
1）HBase的特点满足Facebook对存储系统的需求。HBase基于列存储分布式的开源数据库能满足系统海量数据存储的需求和高扩展性的需求，同时HBase能够保证高吞吐的写操作。它是一个能够实现快速随机和流读取操作的分布式存储系统。虽然不支持传统的跨行事务，但HBase面向列的存储模型在数据存储上提供了很高的灵活性并且支持表内的复杂索引。同时HBase对于写密集的事务是一个理想的选择，它能够维护大量的数据，支持复杂索引，具有灵活的伸缩性，它还能提供行级别的原子性保证。