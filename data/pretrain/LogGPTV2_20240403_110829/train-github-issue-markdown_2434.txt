I'm using python shared memory from multiprocessing package to share numpy
arrays between processes.  
There is a caveat, however, that populating such objects from buffer requires
duplicate copy operation.  
np.frombuffer creates new array and there is no option to populate existing
array which is linked with a shared memory buffer.  
In result, it is required to run 2 operations: deserializing the buffer
content to numpy object and the second to copy elements from that array to the
one linked with shared memory.  
I would like to propose extending np.frombuffer to take additional argument of
existing numpy array that would be populated with the content instead of
creating new one.
### Reproducing code example:
    import numpy as np
    from multiprocessing import shared_memory
    input = np.ones((1,10,10,10))
    b = input.tobytes()
    input_shm = shared_memory.SharedMemory(create=True, size=input.nbytes)
    write_array = np.ndarray(input.shape, dtype=input.dtype,buffer=input_shm.buf)
    content = np.frombuffer(b,dtype=input.dtype).reshape(input.shape) # this is one copy extensive operation
    write_array[:] = content[:]  # this is duplicate copy to populate shared memory buffer
    restored_numpy = np.ndarray(input.shape, dtype=input.dtype,buffer=input_shm.buf)
    # reading from shared memory is quick - attaching the memory block
    print("restored",restored_numpy)
The desired behaviour would be like:
    np.frombuffer(b,dtype=input.dtype,target_ndarray=write_array) 
    # deserialization and populating the shared memory buffer at one time
### Numpy/Python version information:
import sys, numpy; print(numpy. **version** , sys.version)  
1.18.1 3.8.1 (default, Jan 22 2020, 16:31:49)  
[GCC 9.2.1 20200111 gcc-9-branch@280154]