that the proofs of membership and set cardinality can be further
compressed in case the respective sibling paths contain the same
elements.
G still needs to show that all the clients storing FID are accumu-
lated in the same accumulator. This can be achieved by publishing
the association between FID, and its corresponding accumulator
digest on a public bulletin board (e.g., on the public website of
G). However, this approach does not scale with the number of ﬁles
processed by G. For that reason, we elect to only publish those
associations for a randomly chosen subset of all ﬁles. Here, G ﬁrst
obtains a random seed by invoking GetRandomness(tj), where tj
denotes a point in time after the issuance of the bills for epoch Ej
which is determined by a deterministic and publicly known process.
This seed is used to sample the ﬁles whose accumulators need to be
published during this epoch; for instance, G can extract ν-bits of
the seed and use them as a mask to decide which ﬁle accumulators
to publish. The probability that any ﬁle FID is selected at the end
of a given epoch is subsequently given by 2−ν.
Notice that, in this case, G needs to compute proofs of member-
ship and set cardinality only for the selected ﬁles; this information
is sent to each client storing the selected ﬁles. The pairs (FID, δ)
for the selected ﬁles are subsequently published by G (e.g., on its
own website).
Speciﬁcation of the Verify Procedure.
The Verify procedure is only conducted by clients storing ﬁles
for which the corresponding pairs (FID, δ) have been published
by G at the end of the epoch. Notice that clients can easily check
which FID are sampled by invoking GetRandomness(tj).
Given the proofs of membership and set cardinality issued by G
for their ﬁles, clients invoke the VerifyC and VerifyM algorithms
of CARDIAC in order to verify that their ID is included in the set
of clients CFID storing FID, and that |CFID| is as claimed in the
bill. Moreover, clients check the pairs (FID, δ) published by G to
verify that there is only one set CFID which they are referenced to.
In this case, the proof of membership consists of the sibling-
paths of leaves of the tree of height (cid:100)log2(|CFID|)(cid:101). Hence, the
size of the membership proof is at most 2 · (cid:100)log2(|CFID|)(cid:101) hash
values and veriﬁcation takes at most 2 · (cid:100)log2(|CFID|)(cid:101) executions
of the hash function. On the other hand, the proof of cardinal-
ity requires at most 2(cid:96)−1 hash operations to check the open leaves
(since we assume that at least half of the leaves are occupied by
the clients); that is, the size of the proof of cardinality is at most
2 · (cid:100)log2(|CFID|)(cid:101) hash values, but veriﬁcation requires O(|CFID|)
hash operations. Notice that this is at most half the effort required
to construct δ in CARDIAC. As shown in Section 4, this process
is extremely efﬁcient; notice that the veriﬁcation of VerifyC and
VerifyM can be made even more efﬁcient if the client pre-computes
and stores the open (zero) leaves in the tree. As the values of these
nodes are independent of the ﬁle ID and the IDs of the clients, this
precomputation could be performed once for a selection of trees of
different heights.
Additional Operations.
Directories and other functionality: ClearBox hides the clients’
directory structures from G by working on a single directory struc-
ture hosted within S’s account on the cloud. This has the beneﬁt of
reducing the overhead borne by G (i.e., no path related overhead)
and minimizes information leakage towards G.
Directory operations such as directory creation, directory renam-
ing, etc. are locally handled by the software client of the users.
Here, local directories contain pointers to the ﬁles stored therein
and outsourced to the cloud; this enables the local client to perform
operations such as directory listing and ﬁle renaming without the
need to contact G—thereby minimizing the overhead incurred on
G. Only operations that affect the client ﬁles stored on the cloud
(e.g., ﬁle deletion/creation) are transmitted to G.
Other APIs: Recall that ClearBox leverages expiring URL-based
PUT commands (exposed by Amazon S3 and Google Cloud Stor-
age [3]) to enable clients to upload new objects directly to S; ex-
piring URLs are also important in ClearBox to revoke data access
to clients.
A number of commodity cloud service providers such as Drop-
box, and Google drive, however, do not support URL commands
for ﬁle creation, and only provide (non-expiring) URL-based ﬁle
download. To integrate ClearBox with such commodity storage
providers, we note the following differences to the protocol speci-
ﬁcation of ClearBox (cf. Section 3.3):
• At ﬁle upload time, the URL-based PUT is replaced by the
clients uploading the ﬁle to G, which in turn uploads the ﬁle
to S. Recall that G has to compute the Merkle tree over
the uploaded ﬁle; this can be done asynchronously before G
uploads the ﬁle to S—therefore reducing the performance
penalty incurred on G.
• Files are stored under random identiﬁers, and can be ac-
cessed by means of permanent URLs which map to the ﬁle
ID. When the user requests to delete a ﬁle, G renames the ﬁle
to a new randomly selected ID. Other legitimate clients who
require access to the ﬁle have to contact G who informs them
of the new URL corresponding to the renamed ﬁle object.
In Section 4, we present a prototype implementation of ClearBox
which interfaces with Dropbox, and we use it to evaluate the per-
formance of this alternative technique.
Rate-Limiting: Similar to [14], we rate-limit requests to G to pre-
vent possible brute-force search attacks on predictable ﬁle contents
(in the assisted key generation phase), and to prevent resource ex-
haustion attacks by malicious clients. For this purpose, we limit
the number of requests Ri that a client can perform in a given time
frame Ti to a threshold θmax.
3.4 Security Analysis
In this section, we address the security of ClearBox with respect
to the model outlined in Section 2.
Secure Access: We start by showing that ClearBox ensures that
a client who uploaded a ﬁle with Put will be able to recover the
ﬁle with Get as long as he does not delete it. Since we assume
that G and S will not tamper with the storage protocol, the threat
to the soundness argument can only originate from other malicious
clients. Moreover, since the procedures Get and Verify do not mod-
ify the stored data, we will focus the analysis on the operations
Delete and Put.
Clearly, the Delete procedure only removes the access of the user
requesting the deletion. Impersonation is not possible here since we
assume a proper identity management by G. Namely, the gateway
will only delete a ﬁle with ID FID if |CFID| = 0.
On the other hand, the Put procedure can only incur in the mod-
iﬁcation of data when a ﬁle f is uploaded for the ﬁrst time. A
subsequent upload of f is deduplicated and therefore does not en-
tail any data modiﬁcation at S. Notice that during initial upload, a
malicious client can try to defect from the protocol, and construct
an FID that does not ﬁt to f, upload another ﬁle, or encrypt the
ﬁle using the wrong key, etc. Recall that G veriﬁes FID by down-
loading the uploaded ﬁle f∗ to check whether FID ?= M TBuf(f∗).
Notice that if a malicious user creates a malformed f∗, that is
f∗ (cid:54)= enc(H(H
∗(f )x), f ) for any ﬁle f, then this will result into
a random FID value which, with overwhelming probability, will
not collide with any other ﬁle ID. That is, f∗ would be stored by
S without being deduplicated—which incurs storage costs on the
malicious client (as he is fully charged for storing f∗) without af-
fecting the remaining honest clients.
With respect to illegitimate client access, we distinguish between
(i) the case that a client obtains ownership of a ﬁle when uploading
the ﬁle for the ﬁrst time with Put and (ii) the case where the client
accesses the ﬁle with Get at some later point in time without having
the ownership in the ﬁle. For case (i), uploading of a ﬁle that is not
yet stored will not help the client. Namely, the client cannot pretend
to upload a ﬁle with a different FID as G will check the integrity
of the ﬁle. When a user requests to upload a ﬁle which has been
stored already, the adopted proof of ownership scheme (PoW) (cf.
Section 3.2.4 and Appendix B) ensures that the client must know
the entire ﬁle, or the corresponding encoded data buffer (in the case
of larger ﬁles) which is larger than an assumed leakage threshold.
More details of the security of the PoW scheme can be found in
Appendix B. In case (ii), when a client requests access to f, G
ﬁrst checks if this client is still subscribed to f. Observe that this
veriﬁcation is handled internally by the gateway—without giving
clients the ability to interfere with this process. If access is granted,
the client gets a temporary URL which expires after a short time.
Thus, this information cannot be re-used for further accesses; in
particular, clients that are no longer subscribed to this ﬁle are not
able to access the ﬁle any further.
Notice that also external adversaries cannot acquire access to f
due to reliance on authenticated channels. Namely, we assume that
all exchanged messages are appropriated signed, thus no repudia-
tion of requests will be possible.
Secure Attestation: Let CFID denote the set of clients that are
storing ﬁle f.
Recall that f is referenced with FID ← M TBuf(enc(k,f )) where
k ← H (H
∗(f )x) and Buf denotes the encoding used in PoW. The
∗(f ) of the
key k is deterministically computed from the hash H
ﬁle according to our server-assisted encryption scheme. Due to the
collision resistant hash function used throughout the process, FID
is uniquely bound to f. Therefore, all clients that are registered to
f are likewise expecting the same ID FID.
According to the work ﬂow of ClearBox, each client Ci ∈ CFID
is informed about the size of CFID referencing to the according ﬁle
ID FID. Therefore, G ﬁrst commits to the pair FID,|CFID|. Sub-
sequently, G samples a subset of ﬁles for which it proves the cor-
rectness of the deduplication patterns using the accumulator. For
these ﬁles the gateway G publishes the association between FID
and the digest δ of the corresponding accumulator. This sampling
is enforced by the GetRandomness function which acts as an un-
predictably time-dependent source of randomness. This gives neg-
ligible advantage for G in learning the ﬁles that will be sampled
when committing to |CFID| at the end of the epoch. As the out-
put of GetRandomness can be veriﬁed by the clients, each client
Ci ∈ CFID can check if G has reported δ for the corresponding
FID.
Following from the security of CARDIAC (cf. Appendix A),
G can only prove set membership and cardinality to its clients, if
the underlying set and δ were computed correctly. Recall that δ
constitutes a commitment to the set CFID. By publishing a list of
associations (FID, δ) for the sampled ﬁle IDs, clients can ensure
that G did not split CFID into separate subgroups. We refer the
readers to Appendix A for a security treatment of CARDIAC.
Data Conﬁdentiality: As explained in Section 2, the knowledge of
ﬁles size and user access patterns is essential for G and S to operate
their business. Hence, hiding this information cannot be achieved
in our solution. In the sequel, we analyze the conﬁdentiality of the
stored ﬁles in the presence of curious G and S.
Observe that both, the gateway G and the service provider S,
only see the encrypted ﬁles, i.e., f∗ ← enc(k, f ). Therefore, if the
underlying encryption scheme is secure, the contents of the ﬁles
are protected against any eavesdropper unless the key k is leaked.
Our server-assisted encryption scheme is an instance of a message-
Notice that a curious gateway G may guess H
locked encryption scheme [14]. As the key generation is oblivi-
ous, G does not have any additional advantage when compared to a
standard message-locked encryption adversary. As such, similar to
MLE schemes, ClearBox achieves indistinguishability for unpre-
dictable ﬁles.
∗(fi) for popular
(predictable) content fi and compute the corresponding key (as he
knows the secret x) in order to identify if this fi is stored by some
clients. A client who stores a low-entropy conﬁdential ﬁle, can pro-
tect against this attack, by appending a high-entropy string so that
the ﬁle cannot be guessed anymore. However, the deduplication of
the ﬁle is no longer possible. ClearBox offers a stronger protec-
tion towards any other entity who does not know the secret value
x, e.g., the service provider S. Recall that G rate-limits client re-
quests for encryption keys, to slow down brute-force search attacks
on predictable ﬁle contents (via the interface of gateway).
4. DEPLOYMENT IN AMAZON S3
AND DROPBOX
In what follows, we evaluate a prototype implementation of
ClearBox using Amazon S3 and Dropbox as a back-end storage.
4.1
Implementation Setup
We implemented a prototype of ClearBox in Java. In our imple-
mentation, we relied on SHA-256, the Java built-in random num-
ber generator, and the JPBC library [7] (based on the PBC crypto-
graphic library [5]) to implement BLS signatures. For a baseline
comparison, we also implemented the server-based deduplication
DupLESS of Bellare et al. [14] and integrated it with Amazon S3.
Recall that, in DupLESS, clients directly interact with the cloud
providers when storing/fetching their ﬁles. To generate keys, Dup-
LESS uses a server-assisted oblivious protocol based on RSA blind
signatures [14]. Notice that we did not implement a plain (unen-
crypted) cloud storage system since the performance of plain stor-
age can be directly interpolated from the line rate (i.e., network
capacity); where appropriate, we will discuss the performance of
ClearBox relative to a plain cloud storage system.
We deployed our implementations on two dual 6-core Intel Xeon
E5-2640 clocked at 2.50GHz with 32GB of RAM, and 100Mbps
network interface card. The ClearBox gateway, and the assisting
server of DupLESS were running on one dual 6-core Xeon E5-
2640 machine, whereas the clients were co-located on the second
dual 6-core Xeon E5-2640 machine; this ensures a fair compari-
son between ClearBox and DupLESS. To emulate a realistic Wide
Area Network (WAN), we relied on NetEm [38] to shape all trafﬁc
exchanged on the networking interfaces following a Pareto distribu-
tion with a mean of 20 ms and a variance of 4 ms (which emulates
the packet delay variance of WANs [24]).
Our implementation interfaces with both Amazon S3 and Drop-
box (respectively), which are used to store the user ﬁles. To acquire
Bitcoin block hashes, our clients invoke an HTTP request to a get-
blockhash tool offered by the Bitcoin block explorer8 [2]. In our
setup, each client invokes an operation in a closed loop, i.e., a client
may have at most one pending operation. We spawned multiple
threads on G’s machine—each thread corresponding to a unique
worker handling requests/bills of a given client. We bounded the
maximum number of threads that can be spawned in parallel to 100.