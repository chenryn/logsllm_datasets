Software
Network
Environment
Human
Unknown
80
70
60
50
40
30
20
10
h
t
n
o
m
r
e
p
s
e
r
u
l
i
a
F
40
0
0
10
Hardware
Software
Network
Environment
Human
Unknown
60
20
Months in production use
30
40
50
Months in production use
20
30
(a)
(b)
Figure 4. Two representative examples for how the failure rate changes as a function of system age (in months). The
curve on the left corresponds to system 5 which is representative for systems of type E and F. The curve on the right
corresponds to system 19 which is representative for systems of type D and G.
in the production workloads.
The curve in Figure 4(b) corresponds to the failures ob-
served over the lifetime of system 19 and represents the
other commonly observed shape. The shape of this curve
is representative for systems of type D and G, and is less
intuitive: The failure rate actually grows over a period of
nearly 20 months, before it eventually starts dropping. One
possible explanation for this behavior is that getting these
systems into full production was a slow and painful process.
Type G systems were the ﬁrst systems of the NUMA era
at LANL and the ﬁrst systems anywhere that arranged such
a large number of NUMA machines in a cluster. As a result
the ﬁrst 2 years involved a lot of development work among
system administrators, vendors, and users. Administrators
developed new software for managing the system and pro-
viding the infrastructure to run large parallel applications.
Users developed new large-scale applications that wouldn’t
have been feasible to run on previous systems. With the
slower development process it took longer until the systems
were running the full variety of production workloads and
the majority of the initial bugs were exposed and ﬁxed. The
case for the type D system was similar in that it was the ﬁrst
large-scale SMP cluster at the site.
Two other observations support the above explanation.
First, the failure rate curve for other SMP clusters (systems
of type E and F) that were introduced after type D and were
running full production workloads earlier in their life, fol-
lows the more traditional pattern in Figure 4(a). Second, the
curve of system 21, which was introduced 2 years after the
other systems of type G, is much closer to Figure 4(a).
Next we look at how failure rates vary over smaller time
scales. It is well known that usage patterns of systems vary
with the time of the day and the day of the week. The ques-
tion is whether there are similar patterns for failure rates.
Figure 5 categorizes all failures in the data by hour of the
day and by day of the week. We observe a strong correlation
in both cases. During peak hours of the day the failure rate
is two times higher than at its lowest during the night. Simi-
larly the failure rate during weekdays is nearly two times as
s
e
r
u
l
i
a
f
f
o
r
e
b
m
u
N
2000
1500
1000
500
0
5000
4000
3000
2000
1000
s
e
r
u
l
i
a
f
f
o
r
e
b
m
u
N
0
5
15
10
Hour of day
20
Sun Mon Tue Wed Thu Fri Sat
Figure 5. Number of failures by hour of the day (left)
and the day of the week (right).
high as during the weekend. We interpret this as a correla-
tion between a system’s failure rate and its workload, since
in general usage patterns (not speciﬁcally LANL) workload
intensity and the variety of workloads is lower during the
night and on the weekend.
Another possible explanation for the observations in Fig-
ure 5 would be that failure rates during the night and week-
ends are not lower, but that the detection of those failures is
delayed until the beginning of the next (week-)day. We rule
this explanation out, since failures are detected by an auto-
mated system, and not by users or administrators. More-
over, if delayed detection was the reason, one would expect
a large peak on Mondays, and lower failure rates on the fol-
lowing days, which is not what we see.
5.3 Statistical properties of time between
failures
In this section we view the sequence of failure events as
a stochastic process and study the distribution of its inter-
arrival times, i.e. the time between failures. We take two
different views of the failure process: (i) the view as seen by
an individual node, i.e. we study the time between failures
that affect only this particular node; (ii) and the view as
seen by the whole system, i.e. we study the time between
subsequent failures that affect any node in the system.
Since failure rates vary over a system’s lifetime (Fig-
ure 4), the time between failures also varies. We therefore
analyze the time between failures separately for the early
Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06) 
0-7695-2607-1/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:29:50 UTC from IEEE Xplore.  Restrictions apply. 
107
Data
Weibull
Lognormal
Gamma
Exponential
1
0.8
0.6
0.4
0.2
y
t
i
l
i
b
a
b
o
r
p
e
v
i
t
l
a
u
m
u
C
0
103
104
106
Time between failures (sec)
105
(b)
Data
Weibull
Lognormal
Gamma
Exponential
1
0.8
0.6
0.4
0.2
y
t
i
l
i
b
a
b
o
r
p
e
v
i
t
l
a
u
m
u
C
107
106
Data
Weibull
Lognormal
Gamma
Exponential
1
0.8
0.6
0.4
0.2
y
t
i
l
i
b
a
b
o
r
p
e
v
i
t
l
a
u
m
u
C
0
103
104
106
Time between failures (sec)
105
(a)
Data
Weibull
Lognormal
Gamma
Exponential
1
0.8
0.6
0.4
0.2
y
t
i
l
i
b
a
b
o
r
p
e
v
i
t
l
a
u
m
u
C
0
100
102
104
Time between failures (sec)
106
0
103
104
105
Time between failures (sec)
(c)
(d)
Figure 6. Empirical CDF for inter-arrival times of failures on node 22 in system 20 early in production (a) and late
in production (b). Empirical CDF for inter-arrival times of failures for the system wide view of failures in system 20
early in production (c) and late in production (d).
production time, when failure rates are high, and the re-
maining system life, when failure rates are lower. Through-
out we focus on system 20 as an illustrative example.
We begin with the view of the time between failures as
seen by an individual node. Figure 6(a) and (b) show the
empirical distribution at node 22 in system 20 during the
years 1996–1999 and the years 2000–2005, respectively, ﬁt-
ted by four standard distributions. We see that from 2000–
2005 the distribution between failures is well modeled by a
Weibull or gamma distribution. Both distributions create an
equally good visual ﬁt and the same negative log-likelihood.
The simpler exponential distribution is a poor ﬁt, as its C2
of 1 is signiﬁcantly lower than the data’s C2 of 1.9.
For failure interarrival distributions, it is useful to know
how the time since the last failure inﬂuences the expected
time until the next failure. This notion is captured by a dis-
tribution’s hazard rate function. An increasing hazard rate
function predicts that if the time since a failure is long then
the next failure is coming soon. And a decreasing hazard
rate function predicts the reverse. Figure 6(b) is well ﬁt by
a Weibull distribution with shape parameter 0.7, indicating
that the hazard rate function is decreasing, i.e. not seeing a
failure for a long time decreases the chance of seeing one in
the near future.
During years 1996-1999 the empirical distribution of the
time between failures at node 22 looks quite different (Fig-
ure 6(a)) from the 2000-2005 period. During this time the
best ﬁt is provided by the lognormal distribution, followed
by the Weibull and the gamma distribution. The exponential
distribution is an even poorer ﬁt during the second half of
the node’s lifetime. The reason lies in the higher variability
of the time between failures with a C2 of 3.9. This high
variability might not be surprising given the variability in
monthly failure rates we observed in Figure 4 for systems
of this type during this time period.
Next we move to the system wide view of the failures in
system 20, shown in Figure 6(c) and (d). The basic trend
for 2000-05 (Figure 6(d)) is similar to the per node view
during the same time. The Weibull and gamma distribution
provide the best ﬁt, while the lognormal and exponential
ﬁts are signiﬁcantly worse. Again the hazard rate function
is decreasing (Weibull shape parameter of 0.78).
The system wide view during years 1996–1999 (Fig-
ure 6(c)) exhibits a distribution that is very different from
the others we have seen and is not well captured by any of
the standard distributions. The reason is that an exception-
ally large number (> 30%) of inter-arrival times are zero,
indicating a simultaneous failure of two or more nodes.
While we did not perform a rigorous analysis of correlations
between nodes, this high number of simultaneous failures
indicates the existence of a tight correlation in the initial
years of this cluster.
6 Analysis of repair times
A second important metric in system reliability is the
time to repair. We ﬁrst study how parameters such as the
root cause of a failure and system parameters affect repair
Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06) 
0-7695-2607-1/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:29:50 UTC from IEEE Xplore.  Restrictions apply. 
Data
Weibull
Lognormal
Gamma
Exponential
1
0.8
0.6
0.4
0.2
y
t
i
l
i
b
a
b
o
r
p
e
v
i
t
l
a
u
m
u
C
0
100
Time to Repair (min)
102
104
)
n
m
i
(
e
m
i
t
r
i
a
p
e
r
n
a
e
M
6000
5000
4000
3000
2000