are different. After mapping user-deﬁned variable names and
function names to some symbolic names,
the number of
different tokens is further reduced to 10,480. These symbolic
representations are encoded into vectors, which are used as the
input for training a BLSTM neural network.
Step IV: Training BLSTM neural network. For each dataset
described in Table I, we adopt a 10-fold cross validation to
train a BLSTM neural network, and select the best parameter
values corresponding to the effectiveness for vulnerability
detection. For example, we vary the number of hidden layers
for each BLSTM neural network and observe the inﬂuence
on the resulting F1-measure. When we adjust the number of
hidden layers, we set the parameters to their default values
when such default values are available, and set the parameters
to the values that are widely used by the deep learning
community otherwise. The number of tokens in the vector
representation of code gadgets is set to 50, the dropout is set
to 0.5, the batch size is set to 64, the number of epochs is set
to 4, the minibatch stochastic gradient descent together with
ADAMAX [29] is used for training with the default learning
rate of 1.0, and 300 hidden nodes are chosen.
Figure 5.
number of hidden layers
F1-measure of VulDeePecker for the 6 datasets with different
Figure 5 plots the F1-measure of VulDeePecker with re-
spect to the 6 datasets with different number of hidden layers,
each of which leads to a different neural network. We observe
that the F1-measure of the 6 BLSTM neural networks reaches
the maximum at 2 or 3 layers, and the F1-measure of most of
these BLSTM neural networks declines when the number of
layers is greater than 6. Note that the other parameters of the
BLSTM neural network can be tuned in a similar fashion.
D. Experimental results & implications
1) Experiments for answering RQ1:
to test
whether VulDeePecker can be applied to multiple types of
vulnerabilities, we conduct experiments on three datasets: BE-
ALL, RM-ALL, and HY-ALL. This respectively leads to three
neural networks, whose effectiveness is reported in Table II.
In order
Table II.
RESULTS FOR ANSWERING RQ1, INDICATING THAT
VULDEEPECKER CAN DETECT MULTIPLE TYPES OF VULNERABILITIES.
Dataset
BE-ALL
RM-ALL
HY-ALL
FPR(%)
2.9
2.8
5.1
FNR(%)
18.0
4.7
16.1
TPR(%)
82.0
95.3
83.9
P(%)
91.7
94.6
86.9
F1(%)
86.6
95.0
85.4
We observe that the neural network trained from the RM-
ALL dataset outperforms the neural network trained from the
BE-ALL dataset in terms of all ﬁve metrics. This can be
explained by the fact that the number of library/API function
calls that are related to resource management error vulnera-
bilities (i.e., 16) is far smaller than the number of library/API
function calls that are related to buffer error vulnerabilities
(i.e., 124). We also observe that, in terms of the FPR and P
metrics, the neural network trained from the HY-ALL dataset
is not as good as the neural network trained from the BE-ALL
or RM-ALL dataset. We further observe that the TPR and
FNR of the neural network trained from the HY-ALL dataset
reside in between that of the neural network trained from the
RM-ALL dataset and that of the neural network trained from
the BE-ALL dataset. The F1-measure of the neural network
trained from the HY-ALL dataset is 1.2% lower than that
of the neural network trained from the BE-ALL dataset, and
9.6% lower than that of the neural network trained from the
RM-ALL dataset. This can be explained by the fact that the
number of library/API function calls that are related to the
vulnerabilities of the hybrid dataset (i.e., 140) is larger than
the number of library/API function calls that are related to a
single type of vulnerabilities. We speculate this is caused by the
following: it is more difﬁcult to extract vulnerability patterns
for a large number of library/API function calls that are related
to vulnerabilities than to extract vulnerability patterns for a
small number of library/API function calls that are related to
vulnerabilities.
Table III summarizes the training time and detection time
corresponding to the HY-ALL dataset, where the second col-
umn represents the number of code gadgets for training (i.e.,
extracted from the training programs) and the third column
represents the number of code gadgets for detection (i.e.,
extracted from the target programs). We observe that
the
training time of VulDeePecker, as implied by the deep learning
technology in general, is large, but the detecting time is small.
Table III.
TIME COMPLEXITY OF TRAINING AND DETECTION
Dataset
HY-ALL
HY-SEL
#Training
code gadgets
#Detection
code gadgets
48,744
33,813
12,894
9,105
Training
time (s)
36,372.2
25,242.3
Detection
time (s)
156.2
76.6
In summary, we answer RQ1 afﬁrmatively with the follow-
ing:
Insight 1: VulDeePecker can simultaneously detect multi-
ple types of vulnerabilities, but the effectiveness is sensitive to
the number of library/API function calls related to vulnerabil-
ities (i.e., the fewer the better).
2) Experiments for answering RQ2:
In order to answer
whether VulDeePecker can be improved by incorporating hu-
man expertise, we conduct the experiment using all library/API
function calls that are automatically extracted vs. using some
library/API function calls that are manually selected under the
guidance of vulnerability rules written by Checkmarx’s human
experts.
Table IV.
RESULTS FOR ANSWERING RQ2, INDICATING THAT USING
MANUALLY-SELECTED LIBRARY/API FUNCTION CALLS CAN INDEED
IMPROVE THE EFFECTIVENESS OF VULDEEPECKER.
Dataset
HY-ALL
HY-SEL
FPR(%)
5.1
4.9
FNR(%)
16.1
6.1
TPR(%)
83.9
93.9
P(%)
86.9
91.9
F1(%)
85.4
92.9
As shown in Table IV, the BLSTM network trained from
the HY-SEL dataset is more effective than the BLSTM network
10
05101520020406080100F1 (%)Number of hidden layers F1 (BE-SEL) F1 (BE-ALL) F1 (RM-SEL) F1 (RM-ALL) F1 (HY-SEL) F1 (HY-ALL)trained from the HY-ALL dataset. Although the improvement
in FPR is small (0.2%), the improvement in each of the other
metrics is substantial: 10% in FNR and TPR, 5% in precision,
and 7.5% in F1-measure. Moreover, Table III shows that the
training time of using manually selected library/API function
calls can be smaller than that of using all library/API function
calls, because a smaller number of code gadgets need to be
processed. This leads to the following preliminary understand-
ing regards the usefulness of human expertise in improving the
effectiveness of deep learning-based vulnerability detection:
Insight 2: Human expertise can be used to select
li-
brary/API
function calls to improve the effectiveness of
VulDeePecker, especially the overall effectiveness in F1-
measure.
3) Experiments for answering RQ3:
In order to answer
RQ3, we compare the effectiveness of VulDeePecker with
other pattern-based and code similarity-based vulnerability
detection systems. We here report the comparison between
their effectiveness in detecting buffer error vulnerabilities (i.e.,
BE-SEL dataset), while noting that a similar phenomenon
is observed when comparing their effectiveness in detect-
ing resource management error vulnerabilities (i.e., RM-SEL
dataset) — the details are omitted due to the lack of space.
For comparison with other pattern-based vulnerability de-
tection systems, which use rules deﬁned by human experts,
we consider a commercial product called Checkmarx [2] and
two open source tools called Flawﬁnder [6] and RATS [11].
These systems are chosen because we have access to them
and to the best of our knowledge, they have been widely
used. For comparison with code similarity-based vulnerability
detection systems, which are mainly geared towards clone-
caused vulnerabilities, we consider the two state-of-the-art
systems called VUDDY [28] and VulPecker [32]. We use
VUDDY’s open service, and use the original implementation
of VulPecker provided to us by its authors. For fair comparison,
we need to address some subtle issues. We observe that
VulPecker uses diffs as an input, where a diff describes the
difference between a vulnerable piece of code and its patched
version, we divide the BE-SEL dataset of target programs
into two subsets, namely BE-SEL-NVD (266 samples derived
from the NVD) and BE-SEL-SARD (the rest samples derived
from the SARD). We use BE-SEL-NVD for the comparison
study because VUDDY and VulPecker were designed to detect
vulnerabilities with CVE IDs or vulnerabilities with diffs, but
are unable to detect vulnerabilities in BE-SEL-SARD.
Table V summarizes the comparison. We make the fol-
lowing observations. First, VulDeePecker substantially outper-
forms the other pattern-based vulnerability detection systems,
because VulDeePecker incurs a FPR of 5.7% and a FNR
of 7.0%, which are respectively much smaller than their
counterparts in the other detection systems. By looking into
the other systems, we ﬁnd that Flawﬁnder and RATS do not
use data ﬂow analysis and therefore miss many vulnerabilities.
Although Checkmarx does use data ﬂow analysis, its rules for
recognizing vulnerabilities are deﬁned by human experts and
are far from perfect. This further highlights the importance of
relieving human experts from tedious tasks (similar to task of
manually deﬁning features). This observation leads to:
Insight 3: A deep learning-based vulnerability detection
Table V.
RESULTS FOR ANSWERING RQ3: VULDEEPECKER ACHIEVES
A MUCH SMALLER OVERALL FNR OF 7.0% CORRESPONDING TO THE
ENTIRE BE-SEL DATASET (THE LARGER FNR OF 16.9% CORRESPONDING
TO THE SMALL SUB-DATASET DERIVED FROM THE NVD AND THE
SMALLER FNR OF 5.1% CORRESPONDING TO THE SUB-DATASET DERIVED
FROM THE SARD), WHILE NOTING THAT THE OVERALL FPR OF 5.7% IS
REASONABLY SMALL. “N/C” MEANS THAT THE SYSTEM IS NOT CAPABLE
OF DETECTING VULNERABILITIES IN THE CORRESPONDING DATASET.
System
VulDeePecker vs. Other pattern-based vulnerability detection systems
Dataset
FPR
(%)
44.7
42.2
43.1
5.7
0
1.9
22.9
N/C
N/C
3.4
FNR
(%)
69.0
78.9
41.1
7.0
95.1
89.8
16.9
N/C
N/C
5.1
TPR
(%)
31.0
21.1
58.9
93.0
4.9
10.2
83.1
N/C
N/C
94.9
P
(%)
25.0
19.4
39.6
88.1
100
84.3
78.6
N/C
N/C
92.0
F1
(%)
27.7
20.2
47.3
90.5
9.3
18.2
80.8
N/C
N/C
93.4
Flawﬁnder
RATS
Checkmarx
VulDeePecker
BE-SEL
BE-SEL
BE-SEL
BE-SEL
VulDeePecker
VUDDY
VulPecker
VulDeePecker
BE-SEL-NVD
BE-SEL-NVD
BE-SEL-NVD
BE-SEL-SARD
BE-SEL-SARD
BE-SEL-SARD
VulDeePecker vs. Code similarity-based vulnerability detection systems
VUDDY
VulPecker
system can be more effective by taking advantage of the data
ﬂow analysis. (This hints us to speculate that a system can be
even more effective by taking advantage of the control ﬂow
analysis. It is an interesting future work to validate or invalidate
this speculation.)
Second, for the BE-SEL-NVD sub-dataset, VUDDY and
VulPecker trade high FNRs (95.1% and 89.8%, respectively)
for low FPRs (0% and 1.9%, respectively), which lead to very
low F1-measures (9.3% and 18.2%, respectively). The large
FNRs can explained by the following facts: VUDDY can only
detect the vulnerabilities related to functions, which are nearly