# SoK: A Framework for Unifying At-Risk User Research

**Authors:** Noel Warford, Tara Matthews, Kaitlyn Yang, Omer Akgul, Sunny Consolvo, Patrick Gage Kelley, Nathan Malkin, Michelle L. Mazurek, Manya Sleeper, and Kurt Thomas

**Conference:** 2022 IEEE Symposium on Security and Privacy (SP)

## Abstract
At-risk users are individuals who face risk factors that increase their likelihood of being digitally attacked or suffering disproportionate harms. In this systematic analysis, we present a framework based on a meta-analysis of 95 papers. We identified 10 unifying contextual risk factors, such as marginalization and access to sensitive resources, that amplify digital-safety risks and their resulting harms. We also documented technical and non-technical practices that at-risk users adopt to protect themselves. This framework helps to discuss barriers that limit at-risk users' ability or willingness to take protective actions. We believe that researchers and technology creators can use our framework to identify and shape research investments, and to guide technology design to better support at-risk users.

## 1. Introduction
While anyone can experience digital safety threats, at-risk users face additional risk factors that increase their likelihood of being attacked and suffering disproportionate harms. For example, activists may be surveilled by government actors [59, 90], LGBTQ+ individuals face elevated harassment on social media [15, 16], and women in repressive regions experience pervasive sexual harassment online [88, 109].

A growing body of research highlights the unmet digital-safety needs of at-risk users, which existing security, privacy, and safety threat models often overlook. Researchers recommend considering at-risk users during technology creation [13, 25, 63, 97, 108, 124]. However, it can be overwhelming for technology creators to consider the diverse and sometimes contradictory needs of different at-risk populations. Therefore, there is a need for a synthesized framework to organize this knowledge and identify gaps for future work.

We systematically reviewed 95 papers focused on the digital-safety experiences of at-risk populations and developed a framework to address four key research questions:

1. **Contextual Risk Factors:** What factors contribute to digital-safety risks for at-risk users?
2. **Interactions:** How do these contextual risk factors interact to elevate the risk or severity of digital-safety attacks?
3. **Protective Practices:** What protective practices are common among at-risk users?
4. **Barriers:** What barriers do at-risk users encounter in protecting themselves from digital-safety risks?

Our analysis across 31 distinct population categories (e.g., journalists, refugees, older adults) identified 10 cross-cutting contextual risk factors. We also found that at-risk users rely on varied, often ad-hoc protective practices, ranging from social connections to technical strategies. Our framework provides a blueprint for addressing these issues through research, education, and technology creation, ensuring that at-risk users can engage safely online and improving digital safety for all.

## 2. Who Are At-Risk Users?
In this paper, "at-risk user" refers to anyone with risk factors that increase their likelihood of digital attacks or disproportionate harms. We use "at-risk populations" to describe groups of such users. The lack of consensus in the literature led us to choose these terms to highlight the external risks faced by these users.

### 2.1 Previous Taxonomies of Attacks, Threats, and Harms
Previous systematizations have categorized attacks, threats, and harms, but none capture how these elements differ across at-risk populations. Scheuerman et al. [92] and Thomas et al. [103] developed frameworks for understanding classes of harms, such as reputational, financial, and physical harm. Scheuerman et al. [92] also provided a framework for assessing threat severity. Thomas et al. [103], Sambasivan et al. [88], and Levy and Schneier [57] detailed how attack capabilities vary. Our framework isolates the contextual risk factors that make at-risk users vulnerable and documents their protective practices and barriers.

### 2.2 Value of Focusing on At-Risk Users
The challenges faced by at-risk users reflect broader societal inequalities and social norms [35, 65]. These inequalities require special consideration in integrating at-risk users' experiences into technology creation [48, 65, 111]. We advocate for increased focus on at-risk users during threat modeling, research, design, and development. Addressing their needs can also improve digital safety for all users by making more pronounced the needs that many share [29]. Providing better tools and guarantees can benefit both at-risk and general users.

## 3. Methods
We synthesized 95 research papers from computer science conferences. Here, we detail our paper selection and analysis process.

### 3.1 Paper Selection
Our dataset included 95 papers describing digital-safety issues for various at-risk populations. We collected papers from five years (2016–2020) of conferences in security, privacy, and human-computer interaction (HCI): CCS, CHI, CSCW, IEEE S&P, NDSS, PETS, SOUPS, and USENIX Security. We gathered links to every paper from these conferences on DBLP, resulting in 6,534 papers.

Three researchers independently read titles and abstracts, marking them as relevant or not. Papers marked as relevant by only one researcher were reviewed by a fourth researcher. This process identified 127 potentially relevant papers. Authors added 12 more papers from other sources, totaling 139 potentially relevant papers.

### 3.2 Codebook Development
Our goal was to identify contextual risk factors, protective practices, and other patterns. We inductively built a codebook by analyzing a subset of 27 papers well-aligned with our research questions. We used specific populations discussed in each paper as units of analysis [102]. One of four researchers read and summarized each paper, and the full team iteratively built and refined the codebook, including categories for risk factors, protective practices, and barriers to protection.

### 3.3 Full Analysis
We used the codebook to analyze the remaining papers. Any researcher could flag a paper for exclusion if it did not address our research questions. Our final analyzed dataset included 95 papers.

We randomly selected 20% of the remaining papers, which two researchers independently coded, making minor updates to the codebook as needed. After completing this sample, the researchers calculated agreement using Krippendorff’s α before discussing and resolving disagreements. This procedure was repeated on another random 20%. Once sufficient reliability was reached (α = [0.88, 1.00] for contextual risk factors and protective practices, and α = 0.83 for barriers), the researchers split the remaining papers for coding. The entire research team then met to review the results and identify second-order themes.

---

This revised version aims to enhance clarity, coherence, and professionalism while maintaining the original content's integrity.