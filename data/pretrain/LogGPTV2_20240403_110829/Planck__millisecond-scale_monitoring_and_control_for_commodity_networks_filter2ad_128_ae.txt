### Optimized Text

**One Server per Switch with End-Host Modifications**

Planck employs a one-server-per-switch architecture, utilizing end-host (hypervisor) modifications to assist in more general tasks, including multicast. Closely related to Planck are the works of OpenSample [41] and sFlow-RT [33]. OpenSample leverages traditional sFlow [32] samples to detect elephant flows and reroute them to avoid congestion. It also uses TCP sequence numbers to enhance the accuracy of its samples, but does not incorporate them when using a dynamic sampling rate. OpenSample operates with a 100 ms control loop and has been evaluated on 10 Mbps links and in simulations.

InMon's sFlow-RT [33] also uses sampling to measure the network, with the intent of using SDN controllers. However, it reportedly takes hundreds of milliseconds to seconds to detect large flows [34].

### 9. Discussion

#### 9.1 Scalability

While our testbed is insufficient to evaluate Planck at scale, we use measurements and calculations to estimate scalability as switch port counts and the resources required to monitor a network increase. 

First, as switch port counts increase, Planck’s sampling rate will decrease if only one port is used for samples. However, additional monitor ports can be added at an extra cost to increase the sampling rate. 

Second, while collectors are independent and thus scale out well, the number of collector instances that can fit on a single server and the resources required to monitor a network are important considerations. This depends primarily on the packet processing system, the number of NICs that can fit in a server, and the number of cores on the server. Our implementation uses netmap [30], and a given collector instance stays below 90% utilization of a single core when processing 10 Gbps at line rate. Recent results from Intel show that it is possible to route 50-60 Gbps per socket [17], so 100 Gbps should be feasible on a 2-socket server. We were able to build fourteen 10 Gbps Ethernet ports on a single 2U, 16-core server, so such a server should be able to collect samples from 10-14 switches, depending on memory and PCIe bandwidth constraints. In our experiments, we ran only eight collector instances per server due to an unresolved memory bug in netmap for Linux.

Assuming 14 collector instances per server, Planck results in a modest addition to the overall cost for real networks. If 64-port switches are used, with one port dedicated to monitoring, a full-bisection-bandwidth k = 62 three-level fat-tree can be built to support 59,582 hosts from 4,805 switches, which would require 344 collectors, resulting in about 0.58% additional machines. Other topologies that use fewer switches per host, such as Jellyfish [37], would require many fewer collectors. For example, a full-bisection-bandwidth Jellyfish with the same number of hosts requires only 3,505 switches and thus only 251 collectors, representing 0.42% additional machines. Using a monitor port also causes the network to support a smaller number of hosts for a given number of switches. For the same number of switches, a fat-tree with monitor ports supports 1.4% fewer hosts than without monitor ports, and a Jellyfish supports 5.5% fewer hosts than without monitor ports.

Given the performance gains that Planck can offer when coupled with traffic engineering, it is possible that networks with lower bisection bandwidths could still see better performance. By tolerating a lower bisection bandwidth, networks could recover extra ports to add more hosts.

Because the volume of network events is far smaller than the volume of samples, we expect that a reasonable network controller will be able to handle the notifications produced by this number of collectors, but we leave that evaluation to future work.

#### 9.2 Implications for Future Switch Design

In building Planck, we have encountered several challenges that point to opportunities to improve the design and implementation of switches and their firmware in the future.

**Data Plane Sampling Support:** 
Future switches could include support for sampling in the data plane to remove the control plane bottleneck. This would enable much higher sampling rates while maintaining the metadata, such as input port, output port, and sampling rate, which Planck is forced to recover.

**Sampling Rate vs. Rate of Samples:**
Traditional sampling-based network monitoring [32] allows a user (or monitoring system) to set a sampling rate where statistically one in N packets are sampled. While this is easy to configure and understand, it often leads to suboptimal trade-offs. The sampling rate can cause a switch to exceed the rate of samples it can actually send, making the sampling rate inaccurate. To avoid this, sampling rates must be set conservatively, leading to very few samples being gathered during low traffic volumes.

Instead, we propose that future sampling-based network monitoring should center around a desired rate of samples, with switches varying their sampling rates to approximate this rate. This approach is useful not just to avoid overrunning switch capabilities but also to match the capacity of the system processing the samples. Planck achieves this by constraining samples to the link speed of the monitoring port, but future switches should provide ways to do this for arbitrary rates.

**Minimized Sample Buffering:**
One of the biggest challenges in Planck was that oversubscribed monitor ports became congested, causing samples to be buffered. This buffering increases the latency to receive samples and uses buffer space that the rest of the switch could be using for burst tolerance. Reducing the buffering for samples to a minimum would eliminate these issues. Reducing the buffer space allocated to a monitor port should be possible with simple firmware changes to existing switches.

**Clear Sampling Model:**
Our experiments have shown that oversubscribed monitor ports do not exhibit an obvious model that explains the sampling rate across flows or ports. This means that we can only infer the sampling rate for traffic with sequence numbers. Having a clear model for what traffic will be sampled at what rate under what circumstances, especially given that we believe sampling rates should be dynamic, will be essential.

**Preferential Sampling of Special Traffic:**
Certain packets, such as those with TCP SYN, FIN, and RST flags, are more important as they mark the beginning and end of flows. Sampling these packets at a higher rate, perhaps even sampling all of them, would aid in providing accurate measurements and faster knowledge of these network events. We had hoped to achieve this effect by matching on these flags and using OpenFlow to put these packets in a higher priority queue on the monitor ports, but OpenFlow does not currently allow for matching on TCP flags. However, types of traffic that OpenFlow can match on could be given higher priority sampling with this method. It is also important to limit the fraction of total samples allowed to be sampled from higher priority packet classes to avoid allowing an attacker to suppress all normal samples by sending a high rate of traffic in the priority classes, e.g., a SYN flood.

**In-Switch Collectors:**
Lastly, while we believe there will always be places where the flexibility of sampling-based measurement is useful, there may be common information and events that the switch could produce based on its own samples without needing to commit network resources to the samples themselves. In effect, this would be like running the collector for a switch on the switch itself.

### 9.3 Future Work

We believe there is significant future work to be done with Planck. We have shown many ideas about how we build networks, particularly software-defined networks, which need to be rethought if we want to capitalize on millisecond-scale (or faster) measurements. For example, with careful engineering, Planck can allow a network to react to congestion faster than switch buffers fill, meaning TCP would not see losses from congestion in the common case.

Going forward, we would also like to turn Planck into a more extensible measurement platform and define the relevant APIs to plug modules into key places. For example, many of the techniques described in OpenSketch [44] could be implemented as streaming operators on the samples Planck receives.

### 10. Conclusion

This paper presented Planck, a novel network measurement system that uses oversubscribed port mirroring to provide measurements every 4.2 ms–7.2 ms—more than an order of magnitude (11–18x) improvement over the current state-of-the-art (and up to 291x if switch firmware allowed buffering to be disabled on mirror ports). To demonstrate that this increased speed translates to improved performance, we built a traffic engineering application designed to operate at similar speeds, which is able to detect congestion and reroute flows in about 3 milliseconds. Doing so provides near-optimal throughput even for small flow sizes, e.g., 50 MiB, and even at 10 Gbps link speeds. Further, this granularity of measurement radically changes how we should be thinking about network control loops and how we build SDN controllers.

### Acknowledgements

We thank our shepherd Amin Vahdat and the anonymous reviewers for their helpful comments. We also thank Andrew Ferguson for his feedback and discussions. Brent Stephens is supported by an IBM Fellowship. Jeff Rasley is supported by an NSF Graduate Research Fellowship (DGE-1058262).

### References

[References remain unchanged]