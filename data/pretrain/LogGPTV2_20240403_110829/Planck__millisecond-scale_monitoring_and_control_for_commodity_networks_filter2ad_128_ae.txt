one server per switch and uses end-host (hypervisor) modiﬁcations
to invoke them to assist in more general tasks including multicast.
Closely related to Planck is the work of OpenSample [41] and
sFlow-RT [33]. OpenSample leverages traditional sFlow [32] sam-
ples to detect elephant ﬂows and reroute them to avoid congestion.
They also use TCP sequence numbers to improve the accuracy of
their samples but do not use them in the presence of a dynamic sam-
pling rate. Additionally, OpenSample operates with a 100 ms control
loop and is only evaluated on 10 Mbps links and in simulation.
InMon sFlow-RT [33] also uses sampling to measure the network
with the intent of using SDN controllers, but anecdotally takes
hundreds of milliseconds to seconds to detect large ﬂows [34].
9. DISCUSSION
9.1 Scalability
While our testbed is insufﬁcient to evaluate Planck at scale, we
use measurements and calculations to estimate scalability as switch
port counts and the resources required to monitor a network increase.
First, as switch port counts increase, Planck’s sampling rate will
decrease if only one port is used for samples. However, monitor
ports can be added at additional cost to increase the sampling rate.
Second, while collectors are completely independent and thus
scale out well, the number of collector instances that can ﬁt on a
single server and thus the resources required to monitor a network
are worth considering. This depends primarily on the packet pro-
cessing system used, the number of NICs that can ﬁt in a server,
and the number of cores on the server. Our implementation uses
netmap [30], and a given collector instance stays below 90% utiliza-
tion of a single core when processing 10 Gbps at line rate. Recent
results from Intel show that it is possible to route 50-60 Gbps per
socket [17], so 100 Gbps should be possible on a 2-socket server. We
were able to build fourteen 10 Gbps Ethernet ports on a single 2U,
16-core server, so such a server should be able to collect samples
from 10-14 switches depending on memory and PCIe bandwidth
constrains. In our experiments, we only ran eight collector instances
per server due to an unresolved memory bug in netmap for Linux.
Assuming 14 collector instances per server, Planck results in a
modest addition to the overall cost for real networks. If 64-port
switches are used, with one port being dedicated to monitoring, a
full-bisection-bandwidth k = 62 three-level fat-tree can be built
to support 59,582 hosts from 4,805 switches, which would require
344 collectors, resulting in about 0.58% additional machines. Other
topologies that use fewer switches per host, e.g., Jellyﬁsh [37],
would require many fewer collectors. For example, a full-bisection-
bandwidth Jellyﬁsh with the same number of hosts requires only
3,505 switches and thus only 251 collectors, representing 0.42%
additional machines. Using a monitor port also causes the network
to support a smaller number of hosts for a given number of switches.
For the same number of switches, a fat-tree with monitor ports
only supports 1.4% fewer hosts than without monitor ports, and a
Jellyﬁsh supports 5.5% fewer hosts than without monitor ports.
Given the performance gains that Planck can offer when coupled
with trafﬁc engineering, its possible that networks with lower bisec-
tion bandwidths could still see better performance. By tolerating a
lower bisection bandwidth, networks could recover extra ports to
add more hosts.
Because the volume of network events is far smaller than the
volume of samples, we expect that a reasonable network controller
will be able to handle the notiﬁcations produced by this number of
collectors, but we leave that evaluation to future work.
9.2
Implications for Future Switch Design
In building Planck, we have encountered several challenges which
we feel point to opportunities to improve the design and implemen-
tation of switches and their ﬁrmware in the future.
Data Plane Sampling Support for sampling in the data plane to
remove the control plane bottleneck is the most obvious addition
that future switches could make to enable Planck-like functionality.
It would enable much higher sampling rates while also maintaining
the metadata which Planck is forced to recover including input port,
output port, and sampling rate.
Sampling Rate vs. Rate of Samples Traditional sampling-based
network monitoring [32] allows a user (or monitoring system) to
set a sampling rate where statistically one in N packets are sampled.
While this is easy to conﬁgure and understand, we have found that
it causes suboptimal trade-offs. The sampling rate can cause a
switch to exceed the rate of samples it can actually send, making
the sampling rate inaccurate. To avoid this problem, sampling rates
must be set conservatively so that even with high trafﬁc volumes,
the switches do not exceed their sampling rate. The result is that
when there are low trafﬁc volumes, very few samples are gathered.
Instead, we propose that future sampling-based network moni-
toring center around a desired rate of samples and switches should
vary their sampling rates to approximate this rate. This is useful not
just to avoid overrunning switch capabilities, but also to match the
capacity of the system processing the samples. Planck does this by
constraining samples to the link speed of the monitoring port, but
future switches should provide ways to do this for arbitrary rates.
Minimized sample buffering One of the biggest challenges we
found in Planck was that the oversubscribed monitor ports became
congested and samples were buffered. This buffering both increases
the latency to receive samples and uses buffer space that the rest of
the switch could be using for burst tolerance. Reducing the buffering
for samples to a minimum would eliminate both of these issues.
Reducing the buffer space allocated to a monitor port should be
possible with simple ﬁrmware changes to existing switches.
Clear Sampling Model Our experiments have shown that oversub-
scribed monitor ports don’t exhibit an obvious model that explains
the sampling rate across ﬂows or ports. This means that we can only
infer the sampling rate for trafﬁc with sequence numbers. Having
a clear model for what trafﬁc will be sampled at what rate under
what circumstances, especially given that we believe sampling rates
should be dynamic, will be essential.
Preferential Sampling of Special Trafﬁc Certain packets are more
important than others. For instance, packets with TCP SYN, FIN,
and RST ﬂags mark the beginning and end of ﬂows. Sampling these
packets at a higher rate, perhaps even sampling all of them, would
aid in providing accurate measurements and faster knowledge of
these network events.
We had hoped to achieve this effect by matching on these ﬂags
and using OpenFlow to put these packets in a higher priority queue
on the monitor ports, but OpenFlow does not currently allow for
matching on TCP ﬂags. However, types of trafﬁc OpenFlow can
match on could be given higher priority sampling with this method.
Further, it is important to limit what fraction of the total samples
are allowed to be sampled from higher priority packet classes to
avoid allowing an attacker to suppress all normal samples by sending
a high rate of trafﬁc in the priority classes, e.g., a SYN ﬂood.
In-switch Collectors Lastly, while we believe there will always be
places in which the ﬂexibility of sampling-based measurement will
be useful, there may be common information and events that the
switch could produce based on it’s own samples without needing to
commit network resources to the samples themselves. In effect, this
would be like running the collector for a switch on the switch itself.
9.3 Future Work
We believe that there is signiﬁcant future work to be done with
Planck. We have shown many ideas about how we build networks,
certainly software-deﬁned networks need to be rethought if we want
to capitalize on millisecond-scale (or faster) measurements. For
example, with careful engineering Planck can allow a network to
react to congestion faster than switch buffers ﬁll meaning TCP
would not see losses from congestion in the common case.
Going forward, we would also like to turn Planck into a more
extensible measurement platform and deﬁne the relevant APIs to
plug modules into key places. For example, many of the techniques
described in OpenSketch [44] could be implemented as streaming
operators on the samples Planck receives.
10. CONCLUSION
This paper presented Planck, a novel network measurement sys-
tem that uses oversubscribed port mirroring to provide measure-
ments every 4.2 ms–7.2 ms—more than an order of magnitude (11–
18x) improvement over the current state-of-the-art (and up to 291x
if switch ﬁrmware allowed buffering to be disabled on mirror ports).
To demonstrate that this increased speed translates to improved
performance, we built a trafﬁc engineering application designed
to operate at similar speeds which is able to detect congestion and
reroute ﬂows in about 3 milliseconds. Doing so provides near-
optimal throughput even for small ﬂow sizes, e.g., 50 MiB, and even
at 10 Gbps link speeds. Further, this granularity of measurement
radically changes how we should be thinking about network control
loops and how we build SDN controllers.
Acknowledgements: We thank our shepherd Amin Vahdat and the
anonymous reviewers for their helpful comments. We also thank
Andrew Ferguson for his feedback and discussions. Brent Stephens
is supported by an IBM Fellowship. Jeff Rasley is supported by an
NSF Graduate Research Fellowship (DGE-1058262).
References
[1] M. Al-Fares, A. Loukissas, and A. Vahdat. A Scalable, Commodity Data Center
Network Architecture. In SIGCOMM, 2008.
[2] M. Al-fares, S. Radhakrishnan, B. Raghavan, N. Huang, and A. Vahdat. Hedera:
Dynamic Flow Scheduling for Data Center Networks. In NSDI, 2010.
[3] B. Claise, Ed. Cisco Systems NetFlow Services Export Version 9. RFC 3954.
http://www.ietf.org/rfc/rfc3954.txt, October 2004.
[4] T. Benson, A. Anand, A. Akella, and M. Zhang. MicroTE: Fine Grained Trafﬁc
Engineering for Data Centers. In CoNEXT, 2011.
[5] A. Curtis, W. Kim, and P. Yalagandula. Mahout: Low-Overhead Datacenter
Trafﬁc Management using End-Host-Based Elephant Detection. In INFOCOM,
2011.
[6] A. R. Curtis, J. C. Mogul, J. Tourrilhes, P. Yalagandula, P. Sharma, and
S. Banerjee. DevoFlow: Scaling Flow Management for High-Performance
Networks. In SIGCOMM, 2011.
[7] N. Dufﬁeld, C. Lund, and M. Thorup. Learn More, Sample Less: Control of
Volume and Variance in Network Measurement. IEEE Transactions on
Information Theory, 51(5):1756–1775, 2005.
[8] C. Estan, K. Keys, D. Moore, and G. Varghese. Building a Better NetFlow. In
SIGCOMM, 2004.
[9] C. Estan and G. Varghese. New Directions in Trafﬁc Measurement and
Accounting. In SIGCOMM, 2002.
[10] N. Farrington, G. Porter, S. Radhakrishnan, H. H. Bazzaz, V. Subramanya,
Y. Fainman, G. Papen, and A. Vahdat. Helios: A Hybrid Electrical/Optical
Switch Architecture for Modular Data Centers. In SIGCOMM, 2010.
[11] A. D. Ferguson, A. Guha, C. Liang, R. Fonseca, and S. Krishnamurthi.
Participatory Networking: An API for Application Control of SDNs. In
SIGCOMM, 2013.
[12] Floodlight OpenFlow Controller.
http://floodlight.openflowhub.org/.
[13] A. Greenberg, J. R. Hamilton, N. Jain, S. Kandula, C. Kim, P. Lahiri, D. A.
Maltz, P. Patel, and S. Sengupta. VL2: A scalable and ﬂexible data center
network. In SIGCOMM, 2009.
[14] D. Halperin, S. Kandula, J. Padhye, P. Bahl, and D. Wetherall. Augmenting
Data Center Networks with Multi-Gigabit Wireless Links. In SIGCOMM, 2011.
[15] C.-Y. Hong, S. Kandula, R. Mahajan, M. Zhang, V. Gill, M. Nanduri, and
R. Wattenhofer. Achieving High Utilization with Software-driven WAN. In
SIGCOMM, 2013.
[16] IBM BNT RackSwitch G8264.
http://www.redbooks.ibm.com/abstracts/tips0815.html.
[17] Intel® Data Plane Development Kit (Intel® DPDK) Overview Packet
Processing on Intel® Architecture. http://goo.gl/qdg3rZ, December
2012.
[18] Intel® DPDK: Data Plane Development Kit. http://www.dpdk.org.
[19] V. Jacobson. Van Jacobson’s Network Channels.
http://lwn.net/Articles/169961/, January 2006.
[20] S. Jain, A. Kumar, S. Mandal, J. Ong, L. Poutievski, A. Singh, S. Venkata,
J. Wanderer, J. Zhou, M. Zhu, J. Zolla, U. Hölzle, S. Stuart, and A. Vahdat. B4:
Experience with a Globally-deployed Software Deﬁned WAN. In SIGCOMM,
2013.
[21] V. Jeyakumar, M. Alizadeh, D. Mazières, B. Prabhakar, C. Kim, and
A. Greenberg. EyeQ: Practical Network Performance Isolation at the Edge. In
NSDI, 2013.
[22] S. Kandula, S. Sengupta, A. Greenberg, and P. Patel. The Nature of Datacenter
Trafﬁc: Measurements and Analysis. In IMC, 2009.
[23] R. Kapoor, A. C. Snoeren, G. M. Voelker, and G. Porter. Bullet Trains: A Study
of NIC Burst Behavior at Microsecond Timescales. In CoNEXT, 2013.
[24] E. Kohler, R. Morris, B. Chen, J. Jannotti, and M. F. Kaashoek. The Click
Modular Router. ToCS, 18(3):263–297, August 2000.
[25] J. C. Mogul and P. Congdon. Hey, You Darned Counters! Get Off My ASIC! In
HotSDN, 2012.
[26] Openﬂow-switch. https:
//www.opennetworking.org/standards/openflow-switch.
[27] Libpcap File Format. http:
//wiki.wireshark.org/Development/LibpcapFileFormat.
[28] PF_RING: High-speed Packet Capture, Filtering and Analysis.
http://www.ntop.org/products/pf_ring/.
[29] P. Phaal and S. Panchen. Packet Sampling Basics.
http://www.sflow.org/packetSamplingBasics/index.htm.
[30] L. Rizzo. Netmap: A Novel Framework for Fast Packet I/O. In USENIX ATC,
2012.
[31] S. Sen, D. Shue, S. Ihm, and M. J. Freedman. Scalable, Optimal Flow Routing
in Datacenters via Local Link Balancing. In CoNEXT, 2013.
[32] sFlow. http://sflow.org/about/index.php.
[33] sFlow-RT. http://inmon.com/products/sFlow-RT.php.
[34] Large Flow Detection Script. http://blog.sflow.com/2013/06/
large-flow-detection-script.html, June 2013.
[35] A. Shieh, S. Kandula, A. Greenberg, C. Kim, and B. Saha. Sharing the Data
Center Network. In NSDI, 2011.
[36] A. Shieh, S. Kandula, and E. G. Sirer. SideCar: Building Programmable
Datacenter Networks without Programmable Switches. In HotNets, 2010.
[37] A. Singla, C.-Y. Hong, L. Popa, and P. B. Godfrey. Jellyﬁsh: Networking Data
Centers Randomly. In NSDI, 2012.
[38] Version 2 of the Protocol Operations for the Simple Network Management
Protocol (SNMP). RFC 3416.
http://www.ietf.org/rfc/rfc3416.txt.
[39] B. Stephens, A. Cox, W. Felter, C. Dixon, and J. Carter. PAST: Scalable
Ethernet for Data Centers. In CoNEXT, 2012.
[40] G. P. R. Strong, N. Farrington, A. Forencich, P. Chen-Sun, T. Rosing,
Y. Fainman, G. Papen, and A. Vahdat. Integrating Microsecond Circuit
Switching into the Data Center. In SIGCOMM, 2013.
[41] J. Suh, T. T. Kwon, C. Dixon, W. Felter, and J. Carter. OpenSample: A
Low-latency, Sampling-based Measurement Platform for SDN. In ICDCS,
2014.
[42] G. Wang, D. G. Andersen, M. Kaminsky, K. Papagiannaki, T. S. E. Ng,
M. Kozuch, and M. Ryan. c-Through: Part-time Optics in Data Centers. In
SIGCOMM, 2010.
[43] Wireshark. http://www.wireshark.org/.
[44] M. Yu, L. Jose, and R. Miao. Software Deﬁned Trafﬁc Measurement with
OpenSketch. In NSDI, 2013.