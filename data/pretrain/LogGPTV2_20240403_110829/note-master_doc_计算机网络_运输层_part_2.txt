UDP源有可能压制TCP流量
- 明确拥塞通知：由路由器在报文中插入当前路由器的拥塞情况
### 连接管理
#### 三次握手
![20203793532](/assets/20203793532.png)
- 客户端–发送带有 SYN 标志的数据包–一次握手–服务端
- 服务端–发送带有 SYN/ACK 标志的数据包–二次握手–客户端
- 客户端–发送带有带有 ACK 标志的数据包–三次握手–服务端
第一次握手：Client 什么都不能确认；Server 确认了对方发送正常，自己接收正常
第二次握手：Client 确认了：自己发送、接收正常，对方发送、接收正常；Server 确认了：对方发送正常，自己接收正常
第三次握手：Client 确认了：自己发送、接收正常，对方发送、接收正常；Server 确认了：自己发送、接收正常，对方发送、接收正常
所以三次握手就能确认双发收发功能都正常，缺一不可
[SYN泛洪攻击](/计算机网络/网络安全/网络协议安全.md#TCP)也就是攻击者发起第一次握手，然后后续操作不做了，服务端则需要预留相应的资源来为这个连接服务，在攻击的场景下，大量攻击过来，就会导致资源被耗尽死掉
为什么要回传SYN：
接收端传回发送端所发送的 SYN 是为了告诉发送端，我接收到的信息确实就是你所发送的信号了
同样 使用ACK服务端就能验证客户端
当出于某些原因，如端口没开放，此时服务端要拒绝掉客户的握手请求，可以通过回传RST状态来拒绝，也可以发送一个 ICMP 报文port unreachable 消息，这两种在应用层上来看就是连接被拒绝，另外一种是直接不理客户端，此时正常客户端重试一段时间后就会放弃
#### 四次挥手
![20203794049](/assets/20203794049.jpg)
- 客户端发起一个关闭连接的请求，服务器响应这个关闭请求
- 此时，客户端不能再向服务端发送数据，但是服务器可以发送数据给客户端，当服务器的数据传送完毕，向客户端发送一个关闭连接的请求
- 客户端接收到服务端的关闭请求后，再发送一个确认消息，等待2MSL（Linux默认是60秒，net.ipv4.tcp_fin_timeout参数）的时间，关闭
- 服务端接收到客户端的最后一个关闭请求后，关闭
TCP 里一个报文可以搭另一个报文的顺风车（Piggybacking），以提高 TCP 传输的运载效率。所以，TCP 挥手倒不是一定要四个报文
```mermaid
sequenceDiagram
  发起端 ->> 接收端: FIN
  接收端 ->> 发起端: ACK+FIN
  发起端 ->> 接收端: ACK
```
等待2MSL时间是为了让本连接持续时间内所产生的所有报文都从网络中消失，使得下一个新的连接不会出现旧的连接请求报文
### TCP状态转化
```mermaid
stateDiagram-v2
  ESTABLISHED --> CLOSE_WAIT: FIN/ACK
  CLOSE_WAIT --> LAST_ACK: close/FIN
  LAST_ACK --> CLOSED
  ESTABLISHED --> FIN_WAIT_1: close/FIN
  FIN_WAIT_1 --> FIN_WAIT_2: ACK
  FIN_WAIT_1 --> CLOSING: FIN/ACK
  CLOSING --> TIME_WAIT: ACK
  FIN_WAIT_1 --> TIME_WAIT: FIN + ACK/ACK
  FIN_WAIT_2 --> TIME_WAIT: FIN/ACK
  TIME_WAIT --> CLOSED: 2倍的最长报文段寿命(maxium segement lifetime)
  CLOSED --> LISTEN: passive open
  LISTEN --> CLOSED: close
  LISTEN --> SYN_RECVD: SYN/SYN + ACK
  LISTEN --> SYN_SENT: send/SYN
  SYN_SENT --> SYN_RECVD: SYN/SYN + ACK
  SYN_RECVD --> ESTABLISHED: ACK
  SYN_RECVD --> CLOSED: timeout/RST
  SYN_SENT --> ESTABLISHED: SYN + ACK/ACK
  SYN_SENT --> CLOSED: active open/SYN
  CLOSED --> SYN_SENT: close
```
影响网络传输的因素：
- 网络带宽
- 传输距离造成的时延
- 拥塞控制
#### TIME_WAIT
存在这个状态的原因：
**1）可靠地实现TCP全双工连接的终止**
在进行关闭连接四次挥手协议时，最后的ACK是由主动关闭端发出的，如果这个最终的ACK丢失，服务器将重发最终的FIN，因此客户端必须维护状态信息允许它重发最终的ACK。如果不维持这个状态信息，那么客户端将响应RST分节，服务器将此分节解释成一个错误（在java中会抛出connection reset的SocketException)。
因而，要实现TCP全双工连接的正常终止，必须处理终止序列四个分节中任何一个分节的丢失情况，主动关闭的客户端必须维持状态信息进入TIME_WAIT状态。
**2）允许老的重复分节在网络中消逝**
TCP分节可能由于路由器异常而“迷途”，在迷途期间，TCP发送端可能因确认超时而重发这个分节，迷途的分节在路由器修复后也会被送到最终目的地，这个原来的迷途分节就称为lost duplicate。
在关闭一个TCP连接后，马上又重新建立起一个相同的IP地址和端口之间的TCP连接，后一个连接被称为前一个连接的化身（incarnation)，那么有可能出现这种情况，前一个连接的迷途重复分组在前一个连接终止后出现，从而被误解成从属于新的化身。
为了避免这个情况，TCP不允许处于TIME_WAIT状态的连接启动一个新的化身，因为TIME_WAIT状态持续2MSL，就可以保证当成功建立一个TCP连接的时候，来自连接先前化身的重复分组已经在网络中消逝。
当短时间内有大量的短连接打到服务器，然后又close 会造成系统中存在大量处于TIME_WAIT状态的TCP连接
除非请求量很大很快，否则没有处理的必要，如果短时间内发起大量短连接，很有可能造成源端口或者目的端口不够用（部分软件如FTP服务器会选择可以使用的端口范围），从而造成新建的连接失败，由于TCP连接是一个（源IP，源端口，目的IP，目的端口）的四元组，如果发起连接的速度大于连接消逝的速度，那最终不管是源端口，还是目的端口都会不够用
一个由于向FTP服务器发起了大量短连接导致新建的链接被莫名close的案例：
FTP服务器是一种用于文件传输的网络服务，它有两种工作模式：主动模式和被动模式。在主动模式下，FTP服务器主动向客户端发起数据传输的连接，而在被动模式下，FTP服务器等待客户端发起数据传输的连接。被动模式可以避免一些防火墙或路由器的限制，因此在公网上更常用。
我们遇到了一个由于向FTP服务器发起了大量短连接导致新建的链接被莫名close的问题。我们的场景是这样的：公安网中部分基础设施比较落后的城市，由于技术限制，还必须使用FTP服务器来摆渡数据。一个定时任务每次爬取数据后，会将数据发送到FTP服务器上，但每次发送都会新建一个FTP连接，这是导致问题的原因。
我们发现，一旦发送量比较多（比如每分钟发送1000个文件），新建的FTP链接虽然可以连接上FTP服务器，但在传输数据时却莫名其妙被FTP服务器断开了。FTP服务器没有给出任何错误信息，只是返回了一个“Connection closed by remote host”的消息。使用netstat命令发现，系统中存在大量连接FTP服务器的链接处于TIME_WAIT状态。正常情况下，监听一个端口后，如果释放，如果不设置SO_REUSEADDR参数，默认情况下这个端口等待两分钟之后才能再被使用，也就是2MSL，这些处于TIME_WAIT状态的链接仍然占用了端口号，导致无法再开启新的链接。
我们查看了FTP服务器的配置文件，发现它使用了被动模式，在新启动一个链接用于传输数据时，会占用一个新的端口号。而默认配置下，vsftpd的被动模式可使用的端口范围只有1000个（从21100到22100）。这就意味着如果在短时间内有超过1000个链接请求传输数据，就会出现端口不足的情况。这些处于TIME_WAIT状态的链接导致服务器无法再开启新的链接传输数据，从而导致FTP控制链接正常，但上传或下载数据时却会被断掉。
为了解决这个问题，我们尝试了以下几种方法：
- 调高FTP服务器的端口范围。我们将vsftpd.conf文件中的pasv_min_port和pasv_max_port参数分别修改为20000和30000，增加了可用端口数。这样可以缓解端口不足的问题，但也会增加服务器的资源消耗和管理难度。
- 降低net.ipv4.tcp_max_tw_buckets这个参数的值。这个参数控制了系统中最多允许存在多少个处于TIME_WAIT状态的链接。我们将它从默认值180000降低到5000，使得系统能够更快地回收处于TIME_WAIT状态的链接。这样可以减少本地端口号的占用，但也会有安全风险或性能损失。
- 使用连接池，对连接进行复用。我们修改了定时任务的代码，使得它不再每次发送数据都新建一个FTP连接，而是使用一个连接池来管理和复用已有的连接。这样可以避免在短时间内产生大量的连接，也可以提高数据传输的效率。
经过测试，我们发现使用连接池是最好的方式，它既解决了问题，又没有带来其他负面影响。因此我们最终采用了使用连接池的方案，部署到了生产环境中。经过一段时间的观察，我们发现问题已经完全消失了，FTP服务器和客户端都能正常工作，数据传输也没有出现任何异常。
#### 保活
- 默认 TCP 连接并不启用 Keep-alive，若要打开的话要显式地调用 setsockopt()，Java当中就是Socket.setKeepAlive方法
- TCP 心跳包的特点是，它的序列号是上一个包的序列号 -1，而心跳回复包的确认号是这个序列号 -1+1
## Linux 内核 TCP 相关参数
### 建立与断开
![影响建立过程的参数](/assets/202395143025.webp)
![影响断开过程的参数](/assets/202395143137.webp)
配置项 | 说明
-|-
net.ipv4.tcp_syn_retries|数据中心的网络质量都很好，如果Client得不到Server的响应，很可能是Server?本身出了问题。在这种情况下，Client及早地去尝试连接其他的Server会是一个比较好的选择，所以重传次数可以少一些。
net.ipv4.tcp_syncookies|开启SYN Cookies可以防止部分SYN Flood攻击
net.ipv4.tcp_synack_retries|与tcp_syn_retries是同样的思路，对于数据中心的服务器而言，不需要进行那么多的重传。
net.ipv4.tcp_max_syn_backlog|这一项是最多可以积压多少半连接。对于服务器而言，可能瞬间会有非常多的新建连接，可以适当地调大该值，以免SYN包被丢弃而导致Client收不到SYNACK。
net.core.somaxconn|这一项是最多可以积压多少全连接。与前一个配置项类似，在数据中心内部的服务器上，一般都会适当调大该值
net.ipv4.tcp_abort_on_overflow|这样可以避免发送reset1包给client,进而导致client出现异常。
net.ipv4.tcp_fin_timeout|迟迟收不到对端的FIN包，通常情况下都是因为对端机器出了问题，或者是太繁忙而不能及时close(),所以通常都建议将该值调小一些，以尽量避免这种状态下的资源开销。
net.ipv4.tcp_max_tw_buckets|减少该值来避免资源的浪费，对于数据中心而言，网络是相对很稳定的，所以我们一般都会调小该值。这个值表示系统同时保持TIME_WAIT套接字的最大数量，如果超过这个数字，TIME_WAIT套接字将立刻被清除并打印警告信息。这个参数在 https://www.kernel.org/doc/Documentation/networking/ip-sysctl.txt 中提示其实为了简单防止Dos攻击 所以在对外提供服务的情况下不应该人工缩小，默认为180000
net.ipv4.tcp_tw_reuse|表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭。主要用来解决，在快速重启应用程序时，出现端口被占用而无法创建新连接的情况。
net.ipv4.tcp_tw_recycle|表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭。这一项如果不配置为0，会容易引起一些异常。
### 收发数据
![影响发送过程的参数](/assets/202395143823.webp)
![影响接收过程的参数](/assets/202395143955.webp)
配置项 | 说明
-|-
net.ipv4.tcp_wmem|TCP发送缓冲区大小，分别有min、default、max这三个值，TCP发送缓冲区大小会在min和max之间动态调整。如果通过SO_RCVBUF来设置TCP发送缓冲区的大小，那么发送缓冲区就不会自动调整大小
net.core.wmem_max|TCP发送缓冲区的最大值，通常我们都需要调高该值来获得更好的网络性能。
net.ipv4.tcp_mem|系统中TCP连接最多可以消耗的内存，接收缓冲区和发送缓冲区都受它控制，通常我们也需要调大该值。
net.ipv4.ip_local_port_range|本地端口范围，如果该值设置得太小，就可能导致无法创建新的连接
txqueuelen|qdisc的大小，可以通过ifconfig或者ip命令来调整该值，通常情况下也是需要调大该值
net.core.default_gdisc|默认qdisc,通常情况下我们无需调整该值但是有些场景下我们必须要去调整该值，比如说如果TCP拥塞控制使用的是BBR,那就需要将它配置为fq这种方式。
net.core.netdev_budget|接收到网卡中断后，CPU从ring bufferr中一次最多可以读取多少个数据包。通常情况下我们需要调大该值来获取更好的网络性能
net.ipv4.tcp_rmem|TCP接收缓冲区大小，分别有min、default、max这三个值，TCP接收缓冲区大小会在min和max之间动态调整。如果关闭了tcp_moderate_rcvbufs或者通过SO_RCVBUF来设置TCP接收缓冲区大小，那么接收缓冲区就不会自动调整大小
net.ipv4.tcp_moderate_rcvbuf|是否允许动态调整TCP接收缓冲区的大小，该配置项存在的意义是可以影响TCP拥塞控制，进而再影响发送方的行为
net.core.rmem_max|TCP接收缓冲区的最大值，通常都需要调高该值来获得更好的网络性能
net.ipv4.tcp_timestamps|表示是否启用以一种比超时重发更精确的方法来启用对RTT的计算
net.ipv4.tcp_window_scaling|设置TCP会话的滑动窗口大小是否可变
### 保活相关
```conf
net.ipv4.tcp_keepalive_time = 1200 # 表示当keepalive起用的时候，TCP发送keepalive消息的频度。缺省是2小时，改为20分钟。
net.ipv4.tcp_keepalive_probes = 9 # 在探测无响应的情况下，可以发送的最多连续探测次数
net.ipv4.tcp_keepalive_intvl = 75 # 在探测无响应的情况下，连续探测之间的最长间隔
```