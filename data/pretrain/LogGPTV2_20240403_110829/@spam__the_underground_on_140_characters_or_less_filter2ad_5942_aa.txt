title:@spam: the underground on 140 characters or less
author:Chris Grier and
Kurt Thomas and
Vern Paxson and
Chao Michael Zhang
@spam: The Underground on 140 Characters or Less ∗
Chris Grier† Kurt Thomas∗ Vern Paxson† Michael Zhang†
†
University of California, Berkeley
∗
University of Illinois, Champaign-Urbana
{grier, vern, mczhang}@cs.berkeley.edu
PI:EMAIL
ABSTRACT
In this work we present a characterization of spam on Twitter. We
ﬁnd that 8% of 25 million URLs posted to the site point to phish-
ing, malware, and scams listed on popular blacklists. We analyze
the accounts that send spam and ﬁnd evidence that it originates
from previously legitimate accounts that have been compromised
and are now being puppeteered by spammers. Using clickthrough
data, we analyze spammers’ use of features unique to Twitter and
the degree that they affect the success of spam. We ﬁnd that Twit-
ter is a highly successful platform for coercing users to visit spam
pages, with a clickthrough rate of 0.13%, compared to much lower
rates previously reported for email spam. We group spam URLs
into campaigns and identify trends that uniquely distinguish phish-
ing, malware, and spam, to gain an insight into the underlying tech-
niques used to attract users.
Given the absence of spam ﬁltering on Twitter, we examine
whether the use of URL blacklists would help to signiﬁcantly stem
the spread of Twitter spam. Our results indicate that blacklists are
too slow at identifying new threats, allowing more than 90% of vis-
itors to view a page before it becomes blacklisted. We also ﬁnd
that even if blacklist delays were reduced, the use by spammers of
URL shortening services for obfuscation negates the potential gains
unless tools that use blacklists develop more sophisticated spam ﬁl-
tering.
Categories and Subject Descriptors
K.4.1 [Public Policy Issues]: ABUSE AND CRIME INVOLVING
COMPUTERS
General Terms
Security, Measurement
∗This material is based upon work partially supported by the NSF
under Grants 0433702, CNS-0905631, and CNS-0831535, and by
ONR under MURI Grant N000140911081.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
CCS’10, October 4–8, 2010, Chicago, Illinois, USA.
Copyright 2010 ACM 978-1-4503-0244-9/10/10 ...$10.00.
1.
INTRODUCTION
Within the last few years, Twitter has developed a following of
106 million users that post to the site over one billion times per
month [16]. As celebrities such as Oprah, Ashton Kutcher, and
Justin Bieber attract throngs of Twitter followers, spammers have
been quick to adapt their operations to target Twitter with scams,
malware, and phishing attacks [3]. Promising users great diets and
more friends, or simply stealing accounts, spam has become a per-
vasive problem throughout Twitter [8].
Notable attacks on Twitter include the brute force guessing of
weak passwords that led to exploitation of compromised accounts
to advertise diet pills [26]. Phishing is also a signiﬁcant concern on
Twitter, leading the site to completely redesign the sending of pri-
vate messages between users to help mitigate attacks [7]. Even
though Twitter is vigilant at notifying users and works to stop
phishing, spammers continue to create and compromise accounts,
sending messages from them to fool users into clicking on scams
and harmful links.
Despite an increase in volume of unsolicited messages, Twitter
currently lacks a ﬁltering mechanism to prevent spam, with the ex-
ception of malware, blocked using Google’s Safebrowsing API [4].
Instead, Twitter has developed a loose set of heuristics to quan-
tify spamming activity, such as excessive account creation or re-
quests to befriend other users [22]. Using these methods along with
user-generated reports of spamming and abusive behavior, the site
suspends offending accounts, withdrawing their presence from the
Twittersphere along with all of the account’s messages.
In this paper we describe our ﬁndings from a large scale effort to
characterize spam on Twitter. After collecting a month-long sam-
ple of Twitter data, we examine over 400 million public tweets and
crawl 25 million unique URLs. Using an assortment of URL black-
lists to identify spam, we ﬁnd over 2 million URLs that direct users
to scams, malware, and phishing sites – roughly 8% of all links
posted to Twitter. Analyzing the content of spam messages, we pro-
vide a breakdown of techniques employed by spammers to exhort
Twitter users to click on links. By studying the accounts involved in
spamming, we ﬁnd evidence that spammers primarily abuse com-
promised accounts in their spamming activity, rather than accounts
generated solely for the purpose of spamming, which are signiﬁ-
cantly less prevalent.
Using clickthrough data generated from spam URLs, we exam-
ine the success of Twitter spam at enticing over 1.6 million users
into visiting spam web pages. We ﬁnd that the success of spam is
directly tied to having a large audience and a variety of accounts to
spam from, while use of certain Twitter-speciﬁc features also helps
increase user trafﬁc. Overall, we ﬁnd that 0.13% of messages ad-
vertised on Twitter will be clicked, almost two orders of magnitude
higher than email spam [11].
27Given the absence of spam ﬁltering on Twitter, we examine
whether the use of URL blacklists would help to signiﬁcantly stem
the spread of Twitter spam. By measuring the time period between
a blacklist ﬂagging a spam URL and its appearance on Twitter, we
ﬁnd that blacklists in fact lag behind Twitter, with the majority of
spam messages appearing 4–20 days before the URLs embedded
in the messages become ﬂagged.
In contrast, we ﬁnd over 90%
of visits to spam URLs occur within the ﬁrst two days of posting,
indicating that blacklist lag-time is too long to protect a signiﬁcant
number of users against spam. We also examine how spammers can
employ URL shortening services to completely evade blacklists, a
current problem for Twitter’s malware detection.
In summary, the contributions of this paper are:
• We present the ﬁrst in-depth look at spam on Twitter, based
on a detailed analysis of tweets containing over 2 million
distinct URLs pointing to blacklisted scams, phishing and
malware.
• We analyze the clickthrough rate for spam on Twitter, ﬁnding
that 0.13% of users exposed to spam URLs click though to
the spam web site.
• We identify a diversity of spam campaigns exploiting a range
of Twitter features to attract audiences, including large-scale
phishing attacks and targeted scams.
• We measure the performance of blacklists as a ﬁlter for
URLs posted on Twitter, ﬁnding that blacklists are currently
too slow to stop harmful links from receiving thousands of
clicks.
• We develop techniques to identify and analyze two types of
spamming accounts on twitter; those created primarily for
spamming and accounts compromised by spammers.
We organize the remainder of the paper as follows. Section 2
presents a brief background on spam and an overview of Twitter.
Section 3 describes the data we have collected, and Section 4 dis-
cusses trends we ﬁnd in spam tweets, the users who send them,
and the clickthrough rate for URLs in tweets. Section 5 discusses
techniques for grouping spam into campaigns and examples of suc-
cessful campaigns. Section 6 presents our evaluation of blacklists,
followed by conclusions in Section 7.
2. BACKGROUND
Email spam has an extensive body of research exploring how
to identify, characterize, and prevent spam. Common techniques
to ﬁlter email spam include IP blacklisting [18], domain and URL
blacklisting [23, 25, 27], and ﬁltering on email contents [19]. More
sophisticated approaches infer the template used by bots to send
spam and use the template as a ﬁlter [17]. Like many commercial
solutions, we use publicly available URL and domain blacklists to
identify spam on Twitter and leave the exploration of classiﬁcation
techniques for future work.
Researchers have sought insight into the internal workings of
botnets, responsible for much of email spam [10], to measure the
success that email spam has at attracting customers.
In Spama-
lytics, the authors are able to inﬁltrate the Storm botnet and alter
the emails being sent, directly measuring the conversion and click-
through rate of campaigns executed by the Storm botnet [11]. As
Twitter is a new medium for spam, we investigate the clickthrough
for spam tweets and offer comparison to that of email. We are cur-
rently limited to observing clickthrough and cannot determine the
ﬁnal conversion rate for Twitter spam.
The infrastructure used to host spam web sites has also been of
interest, where Anderson et al. explore the infrastructure overlap
and degree that common hosting arrangements exist for spam cam-
paigns [1]. Wang et al. focus on the redirection chains used by
spammers that use search engine optimization techniques to in-
crease trafﬁc [24]. As we will show, redirection services play a role
in spam on Twitter and are used for the majority of spam messages
sent; however, the recent adoption of URL shortening services on
Twitter changes the landscape of interest.
Twitter has recently been the topic of much research, though we
are the ﬁrst to look at the spam and underground behaviors on Twit-
ter. The most relevant work by Kwak et al. examines the structure
of social connections on Twitter, as well as the methods trends are
propagated [12], but does not examine the thriving spam ecosys-
tem on Twitter. In addition to the studying the social graph, re-
cent work on social network spam uses machine learning to clas-
sify spam tweets [13], determine Twitter inﬂuence [2], and classify
spam MySpace proﬁles [9].
Where traditional email spam requires access to bulk lists of
email addresses, social network spam requires the generation or
subversion of user accounts with access to large groups of friends
and social circles. Without access to relationships with other users,
a message cannot be propagated. The challenge of a successful
spam campaign in Twitter is thus two fold: obtaining enough ac-
counts to carry out a campaign before the accounts involved are
suspended, and having enough fresh URLs to evade heuristic de-
tection for excessively posting the same link. Before exploring the
scope of spam activity in Twitter, we present a brief overview of
how Twitter operates and Twitter-speciﬁc features spammers have
at their disposal.
2.1 Anatomy of a Twitter spammer
A generic proﬁle on Twitter consists of three components: the
account’s tweets, followers, and friends.
Tweets: A tweet is a colloquialism used by Twitter to describe a
status update, analogous to an email’s body. Twitter restricts these
updates to 140 characters or less, limiting the amount of informa-
tion spammers can embed in a single tweet as well as the text that
can be considered for spam ﬁltering. To facilitate the posting of
URLs in tweets, URL shortening services are commonly used and
provide redirection services from a short URL of around 20 char-
acters to an arbitrary URL.
Followers: An account’s followers are the set of users that will
receive a tweet once it is posted, akin to the To ﬁeld of an email.
The challenge for spammers is to obtain a large following, allowing
the spammer to advertise a single tweet to thousands of users. Users
must subscribe as a spammer’s follower before receiving tweets; a
spammer cannot force his messages to be viewed by other users.
Friends: Relationships in Twitter are not bidirectional, meaning a
user can receive tweets from a friend without revealing their own
tweets. Friends are the set of users an account subscribes to in order
to obtain access to status updates. In the case of spammers, having
friends provides no beneﬁt in generating trafﬁc. However, spam-
mers will befriend multiple victims in the hope some will recipro-
cate the relationship, opening up a channel for communication.
2.2 Twitter features
In addition to account components,
there are a number of
Twitter-speciﬁc features that can be used in tweets to home in
on a speciﬁc user or reach a wider audience, including mentions,
retweets, and hashtags.
Mentions: To address a particular user, @username is included in
28a tweet, referencing the user directly. For users with public time-
lines, mentions appear in a user’s timeline regardless of if the user
is following the sender. This allows users to quickly identify tweets
directed at them (though still broadcast to the sender’s followers).
Example: @justinbieber PLEASE FOLLOOWW MEEE!!! <3333
Retweets: Retweets on Twitter are a form of attribution, where RT
@username or via @username denote that the tweet text originally
appeared on another user’s proﬁle. Retweets build on the authority
of another user and are used to increase the volume of followers
who see a tweet.
Example: RT @JBieberCrewz: RT this if u <3 justin bieber
Hashtags: In addition to mentioning users, tweets can include tags
to arbitrary topics by including a hashtag #topic. If enough users
pick up on the topic it will appear in the list of trending topics,
allowing tweets to be syndicated to all of Twitter. As anyone can
contribute to a topic, spammers can latch onto currently trending
topics, injecting unsolicited messages into the feed.
Example: Get free followers #FF #Follow Justin Bieber
2.3 Presenting tweets to users
Each Twitter user is provided with a customized timeline of
tweets generated from content posted by friends. When using the
Twitter web page to view a friend’s message, a single tweet con-
tains the tweet text, the friend’s name and icon, the time posted,
geo-location data, and the application used to post the tweet. If a
link is posted, these attributes are the only information available for
user to base their decision on whether to click the link. As simply
visiting a website can lead to the installation of malware, this is a
potentially dangerous situation.
3. DATA COLLECTION
Understanding spam behavior on Twitter requires a large-scale,
real-time framework for detecting and tracking spam accounts. In
this section, we describe the development of our Twitter monitor-
ing infrastructure and the use of URL blacklists to identify spam.
Our infrastructure focuses on analyzing the techniques employed
by spammers to generate click trafﬁc and attract an audience, in
addition to tracking the use of obfuscation and redirects to mask
potentially suspicious web pages.
Within the broad spectrum of spam, we monitor three different
categories: malware, phishing, and scams. A spam URL is clas-
siﬁed as malware if the page hosts malicious software or attempts
to exploit a user’s browser. Phishing pages include any website
attempting to solicit a user’s account credentials, many of which
speciﬁcally target Twitter credentials. Lastly, we deﬁne a scam as
any website advertising pharmaceuticals, software, adult content,
and a multitude of other solicitations.
3.1 Twitter monitoring
To measure the pervasiveness of spam, we develop a Twitter
monitoring framework that taps into Twitter’s Streaming API1 and
collect roughly seven million tweets/day over the course of one
month. We collect data from two separate taps, one targets a ran-
dom sample of Twitter activity while the second speciﬁcally targets
any tweets containing URLs. The random sample is used to gen-
erate statistics about the fraction of URLs in tweets and general
Twitter trends, while the URL stream is used for all other measure-
ments.
Once a tweet appears in the URL stream, we isolate the associ-
ated URL and use a custom web crawler to follow the URL through
1http://apiwiki.twitter.com/Twitter-API-Documentation
HTTP status codes and META tag redirects until reaching the ﬁnal
landing page at a rate of roughly ten landing pages per second; cur-
rently, JavaScript and Flash are not handled due to the sheer volume
of trafﬁc that must be processed and the complexity required to in-
strument these redirects. While crawling URLs, each redirect is
logged, allowing us to analyze the frequency of cross-domain and
local redirects, but more importantly, redirect resolution removes
any URL obfuscation that masks the domain of the ﬁnal landing
page. We record the number of redirects and the URLs in each
sequence.
3.2 Blacklist detection
To automatically identify spam, we use blacklists to ﬂag known
spam URLs and domains. We regularly check every landing page’s
URL in our data set against three blacklists: Google Safebrows-
ing, URIBL, and Joewein [6, 23, 25]. Each landing page must be
rechecked multiple times since blacklists may be slow to update
in response to new spam sites. URLs and domains blacklisted by
Google indicate the presence of phishing or malware, while URIBL
and Joewein speciﬁcally target domains present in spam email and
are used by anti-spam software to classify email messages. Once a
landing page is retroactively marked as spam, we analyze the asso-
ciated spam tweets and users involved in the spam operation. We
have found that URIBL and Joewein include domains that are not
exclusively hosting spam; we created a white-list for popular do-
mains that appear on these blacklists and veriﬁed that the domains
primarily host non-spam content.
3.3 Data summary