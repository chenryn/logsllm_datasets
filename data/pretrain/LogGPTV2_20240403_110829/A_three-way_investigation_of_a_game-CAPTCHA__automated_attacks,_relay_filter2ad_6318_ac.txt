handle different games, games with multiple instances that carry dif-
ferent sets of answer objects, and those with multiple target objects.
Since only one game is cracked, one needs to keep refreshing the
game page, if allowed, until that speciﬁc game appears. Since no
technical details are provided in [1], we can only doubt if any back-
ground learning or object extraction is implemented by observing the
short time it takes to ﬁnish the attack.
(1) Background & Foreground Object Extraction: To extract the
static background of a DCG challenge, the intuitive way is to su-
perimpose some sampling frames that cross a valid period (e.g., 40
frames captured at a ﬁxed time interval (0.2s)), then select the most
frequent color value (dominant color) from each pixel as the back-
ground color for that pixel. This is based on the assumption that
the background image is static and the foreground objects are con-
stantly moving, such that the true background color almost always
appears as the most frequent (or consistent) color observed for a
pixel. By subtracting the background image from a video frame, the
foreground moving objects become readily extractable. To further
reduce the computational cost, a 6-bit color code5, rather than a 24-
bit or 3-byte representation of a color value, is used to code the video
frame, the learned background image, and the learned foreground
objects.
However, one drawback of this preliminary method is that if the
moving speed of the foreground objects is too slow, especially when
some foreground objects hover over a small area, the dominant color
values of most pixels in that area will be contributed by the fore-
ground objects instead of by the background. A shadow of fore-
ground objects may appear as pseudo patches in the background im-
age as shown in Figure 2(b) for the Shapes game of Figure 2(a),
indicated by the dashed rectangle. Using more sampling frames for
initial background learning could alleviate this problem, but result-
ing in a time-consuming learning procedure. Our preliminary experi-
ment indicates that an average 30.9s, generated by running the above
learning method 15 times per game challenge, is needed for learning
a game background completely.
In our new method, we overcome the conﬂict between the number
of sampling frames and the pseudo patch effect by actively chang-
ing the location of one moving object per sampling frame. In the
ﬁrst step, a few frames N1 (e.g., 10 frames captured in 0.3s interval)
are collected to generate the initial background that is used to ex-
tract the foreground object (through background subtraction) in the
next step. Because the number of sampling frames is very limited,
pseudo patches may exist. The second step, called active learning, is
to actively drag-and-drop each moving object to a speciﬁed destina-
tion, which aims to speed up the object movement in order to reduce
the pseudo patch effect. Then, N2 (N2 > N1) sampling frames are
re-collected whenever a moving object is actively dragged to a new
location. Because of the high efﬁciency of the moving object detec-
tion and the latter mouse operations, enough sampling frames (e.g.,
N2 = (30, 50)) without/with minor hovering effect could be collected
within a short period. The new background is detected again based
on the dominant color of the collected frames. Figure 2(c) and 2(d)
show the detected background with non-trivial pseudo patches and
with a minor patch, resp., by applying the 1st and 2nd steps of the
new active learning method on the Shapes game of Figure 2(a). Mi-
nor patches could affect the detection of a complete object, but since
the affected area is minor, partial matching could still be used in the
latter identiﬁcation of the answer object.
Each learned background image is saved in the database. After
removing the extracted background from 5-8 equally distant frames
from the collected frames, the objects in each of the selected frame
are extracted. The objects below a certain size threshold were dis-
carded as noise. The frame with the maximum number of objects
5http://en.wikipedia.org/wiki/Color_code
Figure 2: Detected backgrounds.
(a) original frame image; (b)
detected background with pseudo patches by using preliminary
method; (c) detected background with pseudo patches after perform-
ing the 1st step of the proposed method; (d) detected background
with a minor patch after performing the 2nd step of the proposed
method.
was then selected to extract various objects. Using multiple frames
for object extraction also helped us discard the frames in which the
objects overlapped each other and were hence detected as a single
object instead of distinct individual objects.
According to our experimental results, the likelihood of observing
such pseudo patches is sufﬁciently low (< 7%). However, pseudo
patches may not pose a big issue. Even though the existence of
pseudo patches may result in over-segmented foreground objects when
they overlap each other, a partially detected object can still be used
to extract visual features and later to locate an object that matches
the visual features at the time of attacking.
As the ﬁnal step as part of this phase, the visual features, coded as
color code histograms (a visual feature commonly used to describe
the color distribution in an image), of the foreground objects and the
background image, are stored in the database, together with some
other meta-data such as the object size and dimensions.
(2) Target Area Detection: Identifying the target area requires anal-
ysis of the background extracted in the previous phase. For this pur-
pose, we implemented two alternative approaches, namely the Min-
imum Bounding Rectangle (MBR) [12] method and the Edge-based
method, and compared and contrasted them with regard to detection
accuracy and time efﬁciency.
The MBR-based method is based on the observation that the ac-
tivity/moving area of foreground objects has no or very little over-
lap with the target area. Therefore, by detecting and removing the
foreground moving area from the background image, a reasonable
estimate of the target area can be obtained. As the ﬁrst step of this
approach, the selected 5-8 frames and their foreground object masks
from the previous phase are used to identify the foreground moving
area mask. More speciﬁcally, the foreground mask is generated by
identifying those pixels that have a different color code value than
that of the corresponding pixels in the background image. Then, a
Minimum Bounding Rectangle (MBR) is generated that bounds the
area where the foreground objects are detected in the current frame
(Figure 3). The ﬁnal estimate of the foreground moving area, denoted
as M BRf inal, is the superimposition of all the MBRs extracted
from the sample frames, also represented as a minimum bounding
rectangle (see Figure 3(c)).
After the removal of the entire area bounded by M BRf inal from
the background image, the remaining background is divided into
eight sub-areas as shown in Figure 4. The sub-area with the largest
area (e.g., sub-area #2 in Figure 4) is identiﬁed as the target area,
and its centroid is the target center. It is worth noting that the com-
putational cost of this method is very low (O(M N ), where N is
the number of pixels in a game scene, M is the number of sample
frames, and M (cid:28) N) since the foreground object masks are read-
ily available as part of the output from the previous phase. In other
words, the most time consuming part, which is the extraction of fore-
ground objects (O(M N 2)) from sample frames, has been covered in
Figure 3: Target Detection.
(a) The detected background for the
Parking challenge; (b) One sample frame represented in color code;
(c) Detected foreground objects from (b) and their MBR.
the previous phase.6
Figure 4: Eight sub-areas generated according to moving area of
foreground objects
The Edge-based method employs a different design principle than
MBR-based method.
It is based on the hypothesis that there are
strong edges in the target area because of the likely presence of ob-
jects in the target area, such as the dog and the squirrel in the Animals
game. The steps involved in the edge-based method are listed below:
1. Collect a sequence of frames and learn the background image
as in the MBR-based method.
2. Detect edge pixels on the background image. Group connected
edge pixels into edge segments.
3. Remove trivial edge segments that have too few pixels by a
4. The mean of all the centroids of remaining segments is used as
user-input threshold.
the target area center.
The comparison results of the MBR-based and Edge-based meth-
ods are shown in Figure 5. The solid square dot in each game scene
in Figure 5(a) is the MBR-detected target area center for that chal-
lenge. Also displayed in Figure 5(a) are the detected foreground ob-
ject moving areas, namely M BRf inal, displayed as a black rect-
angle in each game scene. According to our experimental results,
MBR-based method was able to detect the correct target area center
in all the challenges. In contrast, for the edge-based method, it is
difﬁcult to ﬁnd a global threshold that works for all the challenges.
Rather, we need to adjust the threshold for a speciﬁc game in order
to achieve “reasonably good” results, and this method is also sensi-
tive to the existence of texts in the background. Figure 5(b) shows
the “optimal” edge detection result for each challenge with a manu-
ally tuned threshold which is different for each challenge. As shown
in Figure 5(c), some target area centers are incorrectly detected be-
cause some edge segments belong to the texts that are part of the
background but not of the target area. This means that the accuracy
of the edge-based method could be signiﬁcantly undermined by the
presence of strong edges in the background that are not part of the
target area (e.g., presence of texts) and the absence of objects in the
6We also implemented an alternative design, called the exclusion
method (see Appendix B), which detects the target area by simply
removing foreground object pixels accumulated from all the sample
frames. However, while this method is slightly faster than the MBR-
based method, it is less robust.
6
(a)  (b)  (c)   12345678MBRfinalapproach, we ran this attacking module 100 times for each game in-
stance, and the average successful attacking time is 6.9s with the
number of foreground objects ranging from 4 to 6. The maximum
successful attacking time is 9.3s, observed for an instance of the An-
imal game with 6 foreground objects. These timings are in line with
those exhibited by honest users in our usability study, which will
make it impossible for the captcha server to time-out our attack.
(5) Continuous Learning: During attacking, if a challenge matches
a game in our database but contains previously unseen answer ob-
ject(s) (e.g., a new ship object in a Ships game instance), the attack
will not terminate successfully. Whenever such a situation arises, an
answer object learning module that is similar to the aforementioned
module is activated, but differs from the latter in that it only needs
to drag a potential answer object to each of the previously learned
sub-target areas that have matching answer objects in the database.
The newly learned answer objects and their corresponding sub-target
area centroids are then added to the knowledge base for that game.
5.3 Discussion and Summary
There are two beneﬁts in the background learning. First, the learned
background can be used to quickly extract foreground moving ob-
jects. Second, the learned background can be used to locate the tar-
get area where foreground answer objects need to be dragged to. The
proposed active learning is tested on all 36 game challenges (i.e., 3
(speeds), 3 (# of objects), 4 (game prototypes)). N1 is set to be 10.
The shape objects in the Shape Game have larger size than objects
in other games, which easily result in pseudo patch effect when 6
moving objects exist in the game window with limited size. There-
fore, N2 is set to be 50 for the Shape Game challenges with 6 objects
while 30 frames is used for all the other game challenges. In total, a
complete background can be extracted in average 9.04s that is about
three times faster than the preliminary method mentioned earlier (i.e.,
30.9s).
23
The adoption of a large image database for each answer object
could pose a challenge to our approach since it allows for the cre-
ation of many different foreground answer object conﬁgurations for
the same game. In the worst case, a challenge may contain none of
the previously learned answer objects for that particular game. Con-
tinuous learning will be activated in such cases and can also be used
as a way for auto attacking in the run time. Such cases fall into the
category of “known foreground answer objects and known target ob-
jects,” and the success rate can be estimated using the number of
foreground objects (o), number of answer objects (t), and number
of drag and drop attempts allowed for each object (a). For exam-
ple, if o = 5, t = 3 and a = 2, the success rate is approximately
C(5,3)3! = 13%. Though as low as it seems, the rate itself is not
affected by the image database size.
During attacking, there is a time lapse between selecting a fore-
ground object and verifying whether it is an answer object. Both fea-
ture extraction and database lookup (through feature matching) take
time. In our implementation, we chose to click and hold a selected
object until a match with an answer object in the database is regis-
tered. In doing so, we guarantee that an answer object, once veriﬁed,
can be readily dragged and dropped, thus to avoid dealing with the
issue of constantly moving objects. However, this approach may fail
if a constraint is added by the captcha implementation that limits the
amount of time one can hold an object from moving. A less invasive
attacking method would be to utilize parallel processing, in which
one thread is created to perform feature extraction and comparison,
and another parallel thread is used to track and predict the movement
of the object currently under veriﬁcation.
Summary of Automated Attack Analysis: Our attack represents a
novel approach to breaking a representative DCG captcha category.
Attacking captcha challenges, for which the knowledge already ex-
Figure 5: Comparison of the target area center detection results be-
tween the MBR-based and the edge-based methods. (a) Results from
MBR-based method (solid square dot represents the target area cen-
ter and black rectangle represents the object moving area); (b) cen-
troids of non-trivial edges from the edge-based method; and (c) ﬁnal
target area centers from the edge-based method.
target area (e.g., the absence of objects in the target area of the Ships
Game). As for efﬁciency, the MBR-based method has a time com-
plexity of O(M N ) where M is a constant in the range of 5-8, while
the time complexity of the edge-based method is O(N L+N 2) where
L is a constant in the range of 3-8 estimated based on the typical time
complexity of a non-combining edge detection method [30]. Over-
all, this shows that the MBR method outperforms the Edge method
on several aspects.
(3) Answer Object & Target Location Detection: Once the target
area is identiﬁed, the next step is to identify the correct answer ob-
jects and their respective matching sub-target areas. Since a game
can not have too many sub-target areas (otherwise, usability will
be compromised), we divide the entire probable target area into 9
equal-sized blocks, each represented by its area centroid, drag each
foreground object to each of the 9 centroids, and stop and record
the knowledge learned whenever there is a “match.” A match occurs
when an answer object is dragged to its corresponding sub-target area
(e.g., a “bone” dragged onto a “dog”). This is detected by monitor-
ing the change of the area summation of all the foreground objects,
since an answer object, once dragged to its correct target location,
will stay in the target area and therefore result in a reduction of the
foreground area. In our experiments, this method has proven 100%
effective when applied to all four games. As for efﬁciency, while
the worst case upper bound is O(N ), where N is the total number
of foreground objects, in practice, much less number of drags are
required. Our experimental results show that, with 5 foreground ob-
jects for each game (the maximum setting) and 15 training runs for
each game, the average number of drags needed for a game is 9,
i.e., less than 2 drags per each object on average. In case the server
imposes a strict limit on drag/drop attempts, this process can be re-
peated over mutiple runs.
(4) Knowledge Database Building and Attacking: The background,
target area, and learned answer objects as well as their correspond-
ing sub-target areas together constitute the knowledge database for a
game. After learning about sufﬁcient number of games, whenever a
new game challenge is presented, the knowledge base is checked for
the challenge. The target area of the currently presented challenge
is matched with the target areas present in the database to identify
the challenge.
If a match is found, the extraction of objects from
the foreground follows. The visual features such as the color code
histogram of the currently extracted objects are matched with that of
the answer objects in the database for that challenge. The extracted
objects identiﬁed as correct answer objects are then dragged to their
corresponding sub-target areas. To measure the performance of our
7
ists in the dictionary, is 100% accurate and has solving times in line
with that of human users. However, building the dictionary itself
is a relatively slow process. Although this process can be sped-up
as we discussed, it may still pose a challenge as the automated at-
tack may need to repeatedly scan the different captcha challenges
from the server to continuously build an up-to-date dictionary. The
defense strategies for the DCG captcha designers may thus include:
(1) incorporating a large game database as well as large object im-
age databases for each game; and (2) setting a lower game time-out
(such as 20-30s) within which human users can ﬁnish the games but
background learning does not fully complete. Since our attack relies
on the assumption that the background is static, another viable de-
fense would be to incorporate a dynamically changing background
(although this may signiﬁcantly hurt usability). It is also important
to note that, as per the ﬁndings reported in [23], the use of fully auto-
mated solving services represent economical hurdles for captcha at-
tackers. This applies to traditional captchas as well as DCG captchas.
Eventually, this may make automated attacks themselves less viable
in practice [23], and further motivates the attacker, similar to other
captchas, to switch to human-solver attacks against DCG captchas.
6. RELAY ATTACKS
Human-solver relay attacks are a signiﬁcant problem facing the
captcha community, and most, if not all, existing captchas are com-
pletely vulnerable to these attacks routinely executed in the wild [23].
In this section, we assess DCG captchas w.r.t. such attacks.
6.1 Difﬁculty of Relaying DCG captchas