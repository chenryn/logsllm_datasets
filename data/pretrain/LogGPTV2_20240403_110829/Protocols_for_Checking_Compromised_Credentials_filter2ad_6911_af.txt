(cid:140)e breach dataset we used is the one described in Figure 7. It
contains 436 million unique passwords and 1,317 million unique
username-password pairs.
To measure the performance of each scheme, we pick 20 random
passwords from the test set T and run the full C3 protocol with each
one. We report the average time taken for each run in Figure 10.
In the (cid:128)gure, we also give the breakdown of the time taken by
the server and the client for di(cid:130)erent operations. (cid:140)e network
latency had very high standard deviation (25%), though all other
measurements had low (< 1%) standard deviations compared to
their mean values.
HIBP. (cid:140)e implementation of HIBP is the simplest among the four
schemes. (cid:140)e set of passwords in ˜S is hashed using SHA256 and
split into 220 buckets based on the (cid:128)rst 20 bits of the hash value (we
picked SHA256 because we also used the same for FSB). Because
the bucket sizes in HIBP are so small (< 500), each bucket is stored
as a single value in a DynamoDB cell, where the key is the hash
pre(cid:128)x. For larger leaked datasets, each bucket can be split into
multiple cells. (cid:140)e client sends the 20 bit pre(cid:128)x of the SHA256 hash
of their password, and the server responds with the corresponding
bucket.
Among all the protocols HIBP is the fastest (but also weakest in
terms of security). It takes only 220 ms on average to complete a
query over WAN. Most of the time is spent in round-trip network
Protocol
HIBP
FSB
GPC
IDB
Crypto
1
1
47
72
Client
Server call
217
524
433
435
Comp
2
2
9
10
Server
DB call
40
273
72
74
Crypto
–
–
6
6
Total
time
220
527
489
517
Bucket
size
413
6,602
16,121
16,122
Figure 10: Time taken in milliseconds to make a C3 API call.
(cid:135)e client and server columns contain the time taken to per-
form client side and server side operations respectively.
latency and the query to DynamoDB. (cid:140)e only cryptographic oper-
ation on the client side is a SHA256 hash of the password, which
takes less than 1 ms.
FSB. (cid:140)e implementation of FSB is more complicated than that of
HIBP. Because we have more than 1 billion buckets for FSB and
each password is replicated in potentially many buckets, storing all
the buckets explicitly would require too much storage overhead. We
use interval trees [8] to quickly recover the passwords in a bucket
without explicitly storing each bucket. Each password w in the
breach database is represented as an interval speci(cid:128)ed by βFSB(w).
We stored each node of the tree as a separate cell in DynamoDB. We
retrieved the intervals (passwords) intersecting a particular value
(bucket identi(cid:128)er) by querying the nodes stored in DynamoDB.
FSB also needs an estimate of the password distribution to get the
interval range for a tree. We use ˆps as described in Section 4. (cid:140)e
description of ˆps takes 8.9 MB of space that needs to be included as
part of the client side code. (cid:140)is is only a one-time bandwidth cost
during client installation. (cid:140)e client would then need to store the
description to use.
(cid:140)e depth of the interval tree is log N , where N is the number
of intervals (passwords) in the tree. Since each node in the tree is
stored as a separate key-value pair in the database, one client query
requires log N queries to DynamoDB. To reduce this cost, we split
the interval tree into r trees over di(cid:130)erent ranges of intervals, such
that the i-th tree is over the interval[(i − 1) · (cid:98)|B|/r(cid:99) , i · (cid:98)|B|/r(cid:99) − 1].
(cid:140)e passwords whose bucket intervals span across multiple ranges
are present in all corresponding trees. We used r = 128, as it ensures
each tree has around 4 million passwords, and the total storage
overhead is less than 1% more than if we stored one large tree.
Each interval tree of 4 million passwords was generated in paral-
lel and took 3 hours in our server. Each interval tree takes 400 MB
of storage in DynamoDB, and in total 51 GB of space. FSB is the
slowest among all the protocols, mainly due to multiple DynamoDB
calls, which cumulatively take 273 ms (half of the total time, in-
cluding network latency). (cid:140)is can be sped up by using a be(cid:138)er
implementation of interval trees on top of DynamoDB, such as
storing a whole subtree in a DynamoDB cell instead of storing each
tree node separately. We can also split the range of the range tree
into more granular intervals to reduce each tree size. Nevertheless,
as the round trip time for FSB is small (527 ms), we leave such
optimization for future work. (cid:140)e maximum amount of memory
used by the server is less than 91 MB during an API call.
On the client side, the computational overhead is minimal. (cid:140)e
client performs one SHA256 hash computation. (cid:140)e network band-
width consumed for sending the bucket of hash values from the
server takes on average 558 KB.
IDB and GPC. Implementations of IDB and GPC are very similar.
We used the same platforms — AWS Lambda and DynamoDB —
to implement these two schemes. All the hash computations used
here are Argon2id with default parameters, since GPC in [44] uses
Argon2. During precomputation, the server computes the Argon2
hash of each username-password pair and raises it to the power
of the server’s key κ. (cid:140)ese values can be further (fast) hashed
to reduce their representation size, which saves disk space and
bandwidth. However, hashing would make it di(cid:129)cult to rotate
server key. We therefore store the exponentiated Argon2 hash
values in the database, and hash them further during the online
phase of the protocol. (cid:140)e hash values are indexed and bucketized
based on either H(l)(u(cid:107)w) (for GPC) or H(l)(u) (for IDB). We used
l = 16 for both GPC and IDB, as proposed in [44].
We used the secp256k1 elliptic curve. (cid:140)e server (for both IDB
and GPC) only performs one elliptic curve exponentiation, which
on average takes 6 ms. (cid:140)e remaining time incurred is from network
latency and calling Amazon DynamoDB.
On the client side, one Argon2 hash has to be computed for GPC
and two for IDB. Computing the Argon2 hash of the username-
password pairs takes on an average 20 ms on the desktop machine.
We also tried the same Argon2 hash computation on a personal
laptop (Macbook Pro), and it took 8 ms.
In total, hashing and
exponentiation takes 47 ms for GPC, and 72 ms (an additional
25 ms) for IDB. (cid:140)e cost of checking the bucket is also higher
(compared to HIBP and FSB) due to larger bucket sizes.
IDB takes only 28 ms more time on average than GPC (due
to one extra Argon2 hashing), while also leaking no additional
information about the user’s password. It is the most secure among
all the protocols we discussed (should username-password pairs be
available in the leak dataset), and runs in a reasonable time.
8 DEPLOYMENT DISCUSSION
Here we discuss di(cid:130)erent ways C3 services can be used and as-
sociated threats that need to be considered. A C3 service can be
queried while creating a password — during registration or pass-
word change — to ensure that the new password is not present in a
leak. In this se(cid:138)ing C3 is queried from a web server, and the client
IP is potentially not revealed to the server. (cid:140)is, we believe, is a
safer se(cid:138)ing to use than the one we will discuss below.
In another scenario, a user can directly query a C3 service. A
user can look for leaked passwords themselves by visiting a web
site or using a browser plugin, such as 1Password [5] or Password
Checkup [44]. (cid:140)is is the most prevalent use case of C3. For exam-
ple, the client can regularly check with a C3 service to proactively
safeguard user accounts from potential credential stu(cid:129)ng a(cid:138)acks.
However, there are several security concerns with this se(cid:138)ing.
Primarily, the client’s IP is revealed to the C3 server in this se(cid:138)ing,
making it easier for the a(cid:138)acker to deanonymize the user. Moreover,
multiple queries from the same user can lead to a more devastating
a(cid:138)ack. Below we give two new threat models that need to be
considered for secure deployment of C3 services (where bucket
identi(cid:128)ers depend on the password).
Regular password checks. A user or web service might want
to regularly check their passwords with C3 services. (cid:140)erefore,
a compromised C3 server may learn multiple queries from the
same user. For FSB the bucket identi(cid:128)er is chosen randomly, so
knowing multiple bucket identi(cid:128)ers for the same password will
help an a(cid:138)acker narrow down the password search space by taking
an intersection of the buckets, which will signi(cid:128)cantly improve
a(cid:138)ack success.
We can mitigate this problem for FSB by derandomizing the
client side bucket selection using a client side state (e.g., browser
cookie) so the client always selects the same bucket for the same
password. We let c be a random number chosen by the client and
stored in the browser. To check a password w with the C3 server,
the client always picks the jth bucket from the range β(w), where
j ← f (w(cid:107)c) mod |β(w)|.
(cid:140)is derandomization ensures queries from the same device are
deterministic (a(cid:137)er the c is chosen and stored). However, if the
a(cid:138)acker can link queries of the same user from two di(cid:130)erent devices,
the mitigation is ine(cid:130)ective. If the cookie is stolen from the client
device, then the security of FSB is e(cid:130)ectively reduced to that of
HPB with similar bucket sizes.
Similarly, if an a(cid:138)acker can track the interaction history between
a user and a C3 service, it can obtain be(cid:138)er insight about the user’s
passwords. For example, if a user who regularly checks with a
C3 service stops checking a particular bucket identi(cid:128)er, that could
mean the associated password is possibly in the most up-to-date
leaked dataset, and the a(cid:138)acker can use that information to guess
the user’s password(s).
Checking similar passwords. Another important issue is query-
ing the C3 service with multiple correlated passwords. Some web
services, like 1Password, use HIBP to check multiple passwords for
a user. As shown by prior work, passwords chosen by the same
user are o(cid:137)en correlated [24, 40, 47]. An a(cid:138)acker who can see
bucket identi(cid:128)ers of multiple correlated passwords can mount a
stronger a(cid:138)ack. Such an a(cid:138)ack would require estimating the joint
distribution over passwords. We present an initial analysis of this
scenario in Appendix D.
9 RELATED WORK
Private set intersection. (cid:140)e protocol task facing C3 services is
private set membership, a special case of private set intersection
(PSI) [29, 36]. (cid:140)e la(cid:138)er allows two parties to (cid:128)nd the intersection
between their private sets without revealing any additional infor-
mation. Even state-of-the-art PSI protocols do not scale to the sizes
needed for our application. For example, Kiss et al. [30] proposed
an e(cid:129)cient PSI protocol for unequal set sizes based on oblivious
pseudo-random functions (OPRF). It performs well for sets with
millions of elements, but the bandwidth usage scales proportionally
to the size of the leak dataset and so performance is prohibitive
in our se(cid:138)ing. Other e(cid:129)cient solutions to PSI [22, 31, 42, 43] have
similarly prohibitive bandwidth usage.
Private information retrieval (PIR) [23] is another cryptographic
primitive used to retrieve information from a server. Assuming the
server’s dataset is public, the client can use PIR to privately retrieve
the entry corresponding to their password from the server. But
in our se(cid:138)ing we also want to protect the privacy of the dataset
leak. Even if we relaxed that security requirement, the most ad-
vanced PIR schemes [17, 39] require exchanging large amounts of
information over the network, so they are not useful for checking
leaked passwords. PIR with two non-colluding servers can provide
be(cid:138)er security [26] than the bucketization-based C3 schemes, with
communication complexity sub-polynomial in the size of the leaked
dataset. It requires building a C3 service with two servers guar-
anteed to not collude, which may be practical if we assume that
the breached credentials are public information. However, with a
dataset size of at least 1 billion credentials, the cost of one query is
likely still too large to be practical.
Compromised credential checking. To the best of our knowl-
edge, HIBP was the (cid:128)rst publicly available C3 service. Junade Ali
designed the current HIBP protocol which uses bucketization via
pre(cid:128)x hashing to limit leakage. Google’s Password Checkup ex-
tends this idea to use PSI, which minimizes the information about
the leak revealed to clients. (cid:140)ey also moved to checking username,
password pairs.
Google’s Password Checkup (GPC) was described in a paper by
(cid:140)omas et al. [45], which became available to us a(cid:137)er we began
work on this paper. (cid:140)ey introduced the design and implementation
of GPC and report on measurements of its initial deployment. (cid:140)ey
recognized that their (cid:128)rst generation protocol leaks some bits of
information about passwords, but did not analyze the potential
impact on password guessability. (cid:140)ey also propose (what we call)
the ID-based protocol as a way to avoid this leakage. Our paper
provides further motivation for their planned transition to it.
(cid:140)omas et al. point out that password-only C3 services are likely
to have high false positive rates. Our new protocol FSB, being in
the password-only se(cid:138)ing, inherits this limitation. (cid:140)at said, should
one want to do password-only C3 (e.g., because storing username,
password pairs is considered too high a liability given their utility
for credential tweaking a(cid:138)acks [40]), FSB represents the best known
approach.
Other C3 services include, for example, Vericlouds [15] and
GhostProject [13]. (cid:140)ey allow users to register with an email ad-
dress, and regularly keep the user aware of any leaked (sensitive)
information associated with that email. Such services send infor-
mation to the email address, and the user implicitly authenticates
(proves ownership of the email) by having access to the email ad-
dress. (cid:140)ese services are not anonymous and must be used by
the primary user. Moreover, these services cannot be used for
password-only C3.
Distribution-sensitive cryptography. Our FSB protocol uses
an estimate of the distribution of human chosen passwords, making
it an example of distribution-sensitive cryptography, in which con-
structions use contextual information about distributions in order
to improve security. Previous distribution-sensitive approaches
include Woodage et al. [49], who introduced a new type of secure
sketch [25] for password typos, and Lacharite et al.’s [32] frequency-
smoothing encryption. While similar in that they use distributional
knowledge, their constructions do not apply in our se(cid:138)ing.
10 CONCLUSION
We explore di(cid:130)erent se(cid:138)ings and threat models associated with
checking compromised credentials (C3). (cid:140)e main concern is the
secrecy of the user passwords that are being checked. We show,
via simulations, that the existing industry deployed C3 services
(such as HIBP and GPC) do not provide a satisfying level of security.
An a(cid:138)acker who obtains the query to such a C3 service and the
username of the querying user can more easily guess the user’s
password. We give more secure C3 protocols for checking leaked
passwords and username-password pairs. We implemented and
deployed di(cid:130)erent C3 protocols on AWS Lambda and evaluated their
computational and bandwidth overhead. We (cid:128)nish with several
nuanced threat models and deployment discussions that should be
considered when deploying C3 services.
ACKNOWLEDGMENTS
We would like to thank the authors of [45] for sharing their work
with us prior to publication. (cid:140)is work was supported in part by
NSF grants CNS-1564102, CNS-1514163, and CNS-1704527.
REFERENCES
[1] 2018. Argon2. h(cid:138)ps://www.npmjs.com/package/argon2/.
[2] 2018. AWSlambda. h(cid:138)ps://aws.amazon.com/lambda/.