and selective computation on the encrypted data.
Public-key cryptography. Following the tradition in cryptography, we exemplify public-
key encryption using ﬁctional characters Alice and Bob. Alice wants to send sensitive data to
Bob through an insecure channel. Without sharing any information a priori, Alice and Bob
can use public-key cryptography to prevent Eve, the eavesdropper, to intercept and read the
content of the data. Namely, Bob produces a public key, which can be thought of as the digital
analog of a safe, together with a key that opens the safe. The key is kept secret by Bob,
whereas the safe itself is published for anybody to use (in the digital world, objects can be
copied at will, and used indeﬁnitely many times). Alice puts her message in the safe, closes it
(think of a safe that can be closed without the key; this process is referred to as encrypting the
message), and sends the safe (known as the ciphertext) to Bob, who can open it with his key
(this process is referred to as decrypting the ciphertext). Eve doesn’t see the content inside
the safe, since it’s opaque, thus, the message remains conﬁdential. The only information that
is revealed is an upper bound on the size of the message, since the safe has to be at least as
large as the message it contains. Originally put forth by [DH76, Mer78], public-key encryption
has become ubiquitous, in particular with the Transport Layer Security (TLS) protocol, which
has widespread use on the Internet, such as web browsing or instant messaging.
Methodology: deﬁning security. The security of public-key encryption is deﬁned formally
as a game between an adversary that tries to win, that is, to trigger a particular event, or learn
some particular information (for instance, in an encryption scheme, the adversary wins if it can
recover the encrypted message only knowing the public key), and a challenger that interacts
with the adversary. The game simply speciﬁes which messages are sent by the challenger
1
2
Chapter 1. Introduction
depending on the adversary’s behavior, and the winning condition for the adversary. The
security game is deﬁned in such a way that the adversary’s capabilities encompass all possible
attacks that could reasonably occur in a real-life scenario. The winning condition is deﬁned so
as to capture security breaches.
Deﬁning security is a challenging task that has prompted fundamental research papers,
such as [GM84], which deﬁned the notion of semantic security for public-key encryption, and
the indistinguishability-based notion of security. Security deﬁnitions always have to keep up
with the apparition of new practical attacks allowed by new technologies. For instance, the
practical attack of Bleichenbacher [Ble98] on certain standardized and widely used protocols
prompted the adoption of a stronger security deﬁnition (known as Chosen-Ciphertext Attacks
security, originally studied in [DDN03, RS92]) as the de facto security notion for encryption.
Provable security. Given a well-deﬁned security game, to prove the security of a particular
scheme, it remains to prove that no eﬃcient adversary can win the security game with good
probability. Inﬂuenced by complexity theory, cryptographers use a so-called security param-
eter that measures the input size of a computational problem, and adversaries are deﬁned as
probabilistic Turing machines, whose running time is polynomial in the security parameter.
Since an adversary can run multiple times on diﬀerent independent random tapes to increase
its winning probability, the natural choice for the bound on the winning probability is any
negligible function in the security parameter, that is, any function that is asymptotically dom-
inated by all functions of the form 1/P for any polynomial P . A more practically oriented
approach estimates the running time of the security reduction and its advantage in breaking
the underlying assumption more precisely than polynomial running time, and negligible win-
ning advantage. The reduction can thus be used to choose concrete security parameters for the
underlying assumption. See for instance [BDJR97b, BR96] which pioneered concrete security.
Standard assumptions. To prove that there exists no polynomial time adversary that can
win a security game with non-negligible probability, we use a reductionist approach. Namely,
we build an eﬃcient algorithm (called the reduction) that leverages the adversary’s success in
winning the security game, to ﬁnd a solution to a hard problem, that is, a problem that is
impossible to solve eﬃciently with non-negligible probability, or at least, conjectured to be so.
The tradition in cryptography departs from complexity theory at this point, given that basing
cryptography on NP-hard problems has remained open for many years. Instead, security of
cryptographic schemes relies on a more heuristic approach, where security is proven via a
reduction to a well-deﬁned assumption, which states that some problem is hard in practice,
that is, for which there exists no known eﬃcient solution. Of course, the robustness of the
security depends on how much this assumption is trusted. Provable security makes sense as
long as it relies on assumptions that have been extensively studied. Typically, they involve
decade-old mathematical problems, where ﬁnding an eﬃcient algorithm would represent a huge
breakthrough. Instead of using ad hoc cryptanalysis for every possible cryptographic scheme,
one can rely on a small set of simple-to-state assumptions, leveraging years of mathematical
research. Assumptions whose validity is widely trusted are called standard assumptions. For
example, this is the case of the discrete logarithm assumption, which states that given a cyclic
group of prime order p, generated by g, and an element ga for a random exponent a in Zp (we
use multiplicative notation here), it is hard to compute the discrete logarithm a (of course, the
choice of the underlying group is crucial to the validity of the assumption, and only for certain
well-chosen groups is this assumption considered standard).
Tight Security
As explained in the paragraph about provable security, a security reduction can serve as a
tool to choose concrete security parameters.
Indeed, an adversary that can win a security
1.1 Tight Security
3
game can be used by a reduction to break a computational problem that is assumed to be
hard. However, the reduction may be slightly less eﬃcient at breaking the hard problem than
the adversary can win the security game. This gap in eﬃciency is referred to as the security
loss. When choosing the security parameter according to the reduction, it is necessary to take
into account this security loss. For instance, say we want 128 bits of security for a particular
scheme, which means no eﬃcient adversary should be able to break the security of the scheme
with advantage more than 2−128. Suppose the reduction leverages the adversary to break the
discrete logarithm problem with advantage 2−128/L, where L is the security loss. Typically,
the security loss grows with the number of challenge ciphertexts involved in the security game.
That is, the more deployed the scheme, the larger the security loss. This can be an issue
for widespread cryptographic protocols, such as TLS, where sophisticated attacks using many
concurrent sessions can be mounted. For instance, L can be as large as 230 in widely deployed
systems. Then, it is necessary to choose a group where it is assumed to be impossible to
solve the discrete logarithm problem eﬃciently with an advantage of more than 2−158.
In
other words, a large security loss implies large parameters, and a less eﬃcient scheme overall.
Security is said to be tight when the security loss is small and in particular, independent of
the number of clients using the scheme.
State of the Art in Tight Security
The most basic security guarantee required from a public-key encryption scheme is IND-CPA
security, which stands for INDistinguishability against Chosen-Plaintext Attacks, deﬁned in
[GM84], which captures passive, eavesdropping attacks. Many existing IND-CPA-secure en-
cryption schemes have a tight security. For instance, this is the case of El Gamal encryption
scheme [ElG85], whose security tightly reduces to the Decisional Diﬃe Hellman (DDH) as-
sumption [DH76], a standard assumption that implies the discrete logarithm assumption. This
directly follows from the fact that the DDH assumption is random self-reducible: it is as easy to
break many instances of the DDH assumption than just one instance, for a given prime-order
group. However, the de facto security deﬁnition for public-key encryption is a stronger so-called
IND-CCA, which stands for INDistinguishability against Chosen-Ciphertexts Attacks, origi-
nally introduced in [DDN03, RS92], where the adversary can actively manipulate and tamper
with ongoing ciphertexts. Such attacks have been shown to be practically realizable in real life,
such as the attack from [Ble98] on a widely used cryptographic protocol. Unfortunately, most
CCA-secure public-key encryption schemes, such as the seminal construction from [CS98], or
its improvements in [KD04, HK07], do not have a tight security proof: the security loss is
proportional to the number of challenge ciphertexts in the security game. The ﬁrst CCA-
secure public-key encryption with a tight security proof was given in [HJ12], and a long line of
works [LJYP14, LPJY15, HKS15, AHY15a, GCD+16, Hof17] improved eﬃciency considerably.
However, the security of all of these schemes rely on a qualitatively stronger assumption than
non-tightly secure schemes [CS98, KD04, HK07], in particular, they require pairing-friendly
elliptic curves (henceforth simply referred to as pairings), an object ﬁrst used for cryptogra-
phy in [BF01, BF03, Jou00, Jou04]. This situation prompted the following natural question:
does tight security intrinsically require a qualitatively stronger assumption, for CCA-secure
public-key encryption? This question falls into the broad theoretical agenda that aims at min-
imizing the assumptions required to build cryptographic objects as fundamental as public-key
encryption. Besides, eliminating the use of pairings is also important in practice, because it
broadens the class of groups that can be used for the underlying computational assumption.
In particular, it makes it possible to choose groups that admit more eﬃcient group operations
and more compact representations, and also avoid the use of expensive pairing operations.
4
Chapter 1. Introduction
Reference
[CS98]
[KD04, HK07]
|pk|
3
2
|ct|
3
2
[HJ12]
O(1) O(λ)
[LJYP14, LPJY15] O(λ)
[AHY15a]
[GCD+16]
[GHKW16]
[Hof17]
[Hof17]
[GHK17]
O(λ)
O(λ)
2λ
28
20
6
47
12
6
3
6
28
3
security loss assumption
O(Q)
O(Q)
O(1)
O(λ)
O(λ)
O(λ)
O(λ)
O(λ)
O(λ)
O(λ)
DDH
DDH
pairings
pairings
pairings
pairings
DDH
pairings
DCR
DDH
Figure 1.1: Comparison amongst CCA-secure encryption schemes, where Q is the number of
challenge ciphertexts, |pk| denotes the size (in groups elements) of the public key, and |ct| de-
notes the ciphertext overhead, ignoring smaller contributions from symmetric-key encryption.
DCR stands for Decisional Composite Residuosity, a standard assumption that relies on the
fact that factorizing larger numbers is heuristically hard, originally introduced in [Pai99] (see
Deﬁnition 16).
Contribution 1: Tightly CCA-Secure Encryption without Pairing
In [GHKW16], which is presented in Chapter 3 of this thesis, we answer this question nega-
tively. Namely, we present the ﬁrst CCA-secure public-key encryption scheme based on DDH
where the security loss is independent of the number of challenge ciphertexts and the number
of decryption queries, whereas all prior constructions [LJYP14, LPJY15, HKS15, AHY15a,
GCD+16, Hof17] rely on the use of pairings. Moreover, our construction improves upon the
concrete eﬃciency of prior schemes, reducing the ciphertext overhead by about half (to only 3
group elements under DDH), in addition to eliminating the use of pairings. Figure 1.1 gives a
comparison between existing CCA-secure public-key encryption schemes.
One limitation of our construction is its large public key: unlike the schemes with looser
security reduction from [CS98, KD04, HK07], which admit a public key that only contains a
constant number of group elements, our public key contains λ group elements, where λ denotes
the security parameter. Using techniques from [Hof17], we present in [GHK17] the ﬁrst CCA-
secure public-key encryption with a tight security reduction to the DDH assumption (without
pairings), whose public key only contains a constant number of group elements. The eﬃciency
is comparable with [GHKW16], since the ciphertexts only contain three group elements. We
choose to only present in this thesis the work from the precursor [GHKW16].
Functional Encryption
We now proceed to address another limitation of traditional public-key encryption:
it only
provides an all-or-nothing access to the encrypted data. Namely, with the secret key, one can
decrypt the ciphertext and recover the message entirely; without the secret key, nothing is
revealed about the encrypted message (beyond its size). To broaden the scope of applications
of public-key encryption, [O’N10, BSW11] introduced the concept of functional encryption,
which permits selective computations on the encrypted data, that is, it allows some authorized
users to compute partial information on the encrypted data. In a functional encryption scheme,
6
Chapter 1. Introduction
• Using functional encryption, one can perform machine learning on encrypted data. Namely,
after a classiﬁer is learned on plain data, one can generate a functional decryption key
associated with this classiﬁer, which allows decryption to run the classiﬁcation on en-
crypted data, and reveals only the result of the classiﬁcation. In [DGP18], a concrete
implementation of functional encryption performs classiﬁcation of hand-written digits
from the MNIST dataset, with 97.54% accuracy, where the encryption and decryption
only take a few seconds.
In a fully homomorphic
Diﬀerence with respect to fully homomorphic encryption.
encryption scheme, it is possible to publicly evaluate any function on the encrypted data.
This diﬀers from functional encryption in two major ways: ﬁrst, the result of evaluating a
function f on an encryption of message m does not reveal the evaluation f(m) in the clear,
but only an encryption of it. Consider the email ﬁltering scenario: using fully homomorphic
encryption, the email server would not be able to decide whether an incoming encrypted
email is spam, without the intervention of the client, who is the only one who can decrypt
the result of the evaluation on encrypted data. Second, using fully homomorphic encryption,
anyone can compute arbitrary functions on the encrypted data: there is no guarantee that the
computation was performed correctly. In a functional encryption scheme, the owner of the
functional decryption key associated with function f can extract f(m), from an encryption of
m, and nothing else. In particular, this gives veriﬁability for free, unlike fully homomorphic
encryption, which requires additional costly zero-knowledge proofs to verify that the proper
function has been evaluated on the encrypted data.
Security of functional encryption. Security notions for functional encryption were ﬁrst
given in [O’N10, BSW11]. These works present a simulation-based security deﬁnition, where
an eﬃcient simulator is required to generate the view of the adversary in the security game,
only knowing the information that leaks from the encrypted values and corrupted functional
decryption keys. They prove that such a security notion is impossible to achieve in general, and
give another indistinguishability-based variant of the security deﬁnition, essentially a security
deﬁnition similar to [GM84], generalized to the context of functional encryption.
In this
security game, an adversary receives the public key of the encryption scheme, and then, it can
obtain functional decryption keys for functions f of its choice. It also sends two messages,
m0 and m1, to the challenger, in the security game, which samples a random bit b ←R {0, 1},
and sends back an encryption of the message mb. Assuming the functional encryption keys
that are obtained by the adversary are associated with functions f that do not distinguish
these two messages, that is, for which f(m0) = f(m1), the adversary should not be able
to guess which bit b was used with a probability signiﬁcantly more than 1/2, which can be
obtained by random guessing. Intuitively, if the functions f do not help distinguish these two
messages, then no information should be revealed about which message mb was encrypted. An
artiﬁcial but useful weakening of the security model is the so-called selective security, where
the game is identical to the description above, except the adversary is required to decide on
which messages m0 and m1 to choose beforehand, that is, before seeing the public key or
obtaining any functional decryption keys. This notion is useful as a stepping stone towards
full-ﬂedged security. Moreover, a guessing argument can convert any selectively-secure scheme
into a fully-secure scheme, albeit with a quantitative gap in the quality of the security.
State of the Art in Functional Encryption
Identity-based encryption. Historically, the ﬁrst functional encryption scheme beyond
traditional public-key encryption dates back to identity-based encryption, where a constant-
size public key is used to encrypt messages to diﬀerent users, represented by their identity.
Functional decryption keys are also associated with an identity, and decryption succeeds to
1.2 Functional Encryption
7
recover the encrypted message if the identities associated with the ciphertext and the functional
decryption key match. For instance, identities can be email addresses, and with a single
public key, it is possible to encrypt a message to any user whose email address is known. The
concept was thought of in [Sha84], and the ﬁrst constructions whose security relied on standard
assumptions were given in [BF01, Coc01].
Attribute-based encryption. Later, a more general concept was introduced: attribute-
based encryption, where ciphertexts are associated with an access policy, and functional de-
cryption keys are associated with a set of attributes. Decryption recovers the encrypted mes-
sage if the attributes associated with the functional decryption key satisfy the access policy
embedded in the ciphertext. Note that the role can be switched, that is, ciphertexts can be as-
sociated with attributes, and functional decryption keys embed access policies, as in [BSW07].
These are referred to as key-policy and ciphertext-policy attributed-based encryption, respec-
tively. Such attribute-based encryption schemes have been ﬁrst realized from standard as-
sumptions in [SW05, GPSW06] for policies that can be represented as Boolean formulas, or
in [GVW13, GVW15a, BGG+14] for policies that can be represented as any arbitrary circuit
of polynomial size. Note that a ciphertext only hides the underlying message it encrypts,
but reveals the associated access policy (or attributes, depending on whether we consider
ciphertext-policy or key-policy attribute-based encryption).
Predicate encryption. Predicate encryption schemes are even more powerful than attribute-
based encryption schemes, since the access policy associated with a ciphertext remains hidden
(or the attributes, depending on whether we consider the ciphertext-policy or the key-policy
variant). The ﬁrst constructions from standard assumptions were given in [BW07] for com-
parison and subset queries, in [KSW08, KSW13] for constant-depth Boolean formulas, and in
[GVW15b] for all circuits. Such predicate encryption schemes are sometimes referred to as