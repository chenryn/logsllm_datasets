title:Poisoning the Unlabeled Dataset of Semi-Supervised Learning
author:Nicholas Carlini
Poisoning the Unlabeled Dataset of 
Semi-Supervised Learning
Nicholas Carlini, Google
https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-poisoning
This paper is included in the Proceedings of the 30th USENIX Security Symposium.August 11–13, 2021978-1-939133-24-3Open access to the Proceedings of the 30th USENIX Security Symposium is sponsored by USENIX.Poisoning the Unlabeled Dataset of Semi-Supervised Learning
Nicholas Carlini
Google
Abstract
Semi-supervised machine learning models learn from a
(small) set of labeled training examples, and a (large) set
of unlabeled training examples. State-of-the-art models can
reach within a few percentage points of fully-supervised train-
ing, while requiring 100× less labeled data.
We study a new class of vulnerabilities: poisoning attacks
that modify the unlabeled dataset. In order to be useful, un-
labeled datasets are given strictly less review than labeled
datasets, and adversaries can therefore poison them easily. By
inserting maliciously-crafted unlabeled examples totaling just
0.1% of the dataset size, we can manipulate a model trained
on this poisoned dataset to misclassify arbitrary examples at
test time (as any desired label). Our attacks are highly effec-
tive across datasets and semi-supervised learning methods.
We ﬁnd that more accurate methods (thus more likely to be
used) are signiﬁcantly more vulnerable to poisoning attacks,
and as such better training methods are unlikely to prevent
this attack. To counter this we explore the space of defenses,
and propose two methods that mitigate our attack.
1 Introduction
One of the main limiting factors to applying machine learn-
ing in practice is its reliance on large labeled datasets [32].
Semi-supervised learning addresses this by allowing a model
to be trained on a small set of (expensive-to-collect) labeled
examples, and a large set of (cheap-to-collect) unlabeled ex-
amples [33, 42, 72]. While semi-supervised machine learning
has historically been “completely unusable” [61], within the
past two years these techniques have improved to the point of
exceeding the accuracy of fully-supervised learning because
of their ability to leverage additional data [53, 66, 67].
Because “unlabeled data can often be obtained with mini-
mal human labor” [53] and is often scraped from the Internet,
in this paper we perform an evaluation of the impact of train-
ing on unlabeled data collected from potential adversaries.
Speciﬁcally, we study poisoning attacks where an adversary
injects maliciously selected examples in order to cause the
learned model to mis-classify target examples.
Our analysis focuses on the key distinguishing factor of
semi-supervised learning: we exclusively poison the unla-
beled dataset. These attacks are especially powerful because
the natural defense that adds additional human review to the
unlabeled data eliminates the value of collecting unlabeled
data (as opposed to labeled data) in the ﬁrst place.
We show that these unlabeled attacks are feasible by intro-
ducing an attack that directly exploits the under-speciﬁcation
problem inherent to semi-supervised learning. State-of-the-
art semi-supervised training works by ﬁrst guessing labels
for each unlabeled example, and then trains on these guessed
labels. Because models must supervise their own training, we
can inject a misleading sequence of examples into the unla-
beled dataset that causes the model to fool itself into labeling
arbitrary test examples incorrectly.
We extensively evaluate our attack across multiple datasets
and learning algorithms. By manipulating just 0.1% of the
unlabeled examples, we can cause speciﬁc targeted examples
to become classiﬁed as any desired class. In contrast, clean-
label fully supervised poisoning attacks that achieve the same
goal require poisoning 1% of the labeled dataset.
Then, we turn to an evaluation of defenses to unlabeled
dataset poisoning attacks. We ﬁnd that existing poisoning
defenses are a poor match for the problem setup of unlabeled
dataset poisoning. To ﬁll this defense gap, we propose two
defenses that partially mitigate our attacks by identifying and
then removing poisoned examples from the unlabeled dataset.
We make the following contributions:
• We introduce the ﬁrst semi-supervised poisoning attack,
that requires control of just 0.1% of the unlabeled data.
• We show that there is a direct relationship between model
accuracy and susceptibility to poisoning: more accurate
techniques are signiﬁcantly easier to attack.
• We develop a defense to perfectly separate the poisoned
from clean examples by monitoring training dynamics.
USENIX Association
30th USENIX Security Symposium    1577
2 Background & Related Work
(Supervised) Machine Learning
2.1
Let fθ be a machine learning classiﬁer (e.g., a deep neural
network [32]) parameterized by its weights θ. While the ar-
chitecture of the classiﬁer is human-speciﬁed, the weights θ
must ﬁrst be trained in order to solve the desired task.
Most classiﬁers are trained through the process of Empir-
ical Risk Minimization (ERM) [62]. Because we can not
minimize the true risk (how well the classiﬁer performs on
the ﬁnal task), we construct a labeled training set X to esti-
mate the risk. Each example in this dataset has an assigned
label attached to it, thus, let (x,y) ∈ X denote an input x with
the assigned label y. We write c(x) = y to mean the true label
of x is y. Supervised learning minimizes the aggregated loss
L(X ) = ∑
(x,y)∈X
L( fθ(x),y)
where we deﬁne the per-example loss L as the task requires.
We denote training by the function fθ ← T ( f ,X ).
This loss function is non-convex; therefore, identifying the
parameters θ that reach the global minimum is in general
not possible. However, the success of deep learning can be
attributed to the fact that while the global minimum is difﬁcult
to obtain, we can reach high-quality local minima through
performing stochastic gradient descent [24].
Generalization. The core problem in supervised machine
learning is ensuring that the learned classiﬁer generalizes to
unseen data [62]. A 1-nearest neighbor classiﬁer achieves
perfect accuracy on the training data, but likely will not gen-
eralize well to test data, another labeled dataset that is used
to evaluate the accuracy of the classiﬁer. Because most neu-
ral networks are heavily over-parameterzed1, a large area of
research develops methods that to reduce the degree to which
classiﬁers overﬁt to the training data [24, 55].
Among all known approaches, the best strategy today to
increase generalization is simply training on larger training
datasets [58]. Unfortunately, these large datasets are expensive
to collect. For example, it is estimated that ImageNet [48] cost
several million dollars to collect [47].
To help reduce the dependence on labeled data, augmen-
tation methods artiﬁcially increase the size of a dataset by
slightly perturbing input examples. For example, the simplest
form of augmentation will with probability 0.5 ﬂip the im-
age along the vertical axis (left-to-right), and then shift the
image vertically or horizontally by a small amount. State of
the art augmentation methods [13, 14, 65] can help increase
generalization slightly, but regardless of the augmentation
strategy, extra data is strictly more valuable to the extent that
it is available [58].
1Models have enough parameters to memorize the training data [69].
2.2 Semi-Supervised Learning
When it’s the labeling process—and not the data collection
process—that’s expensive, then Semi-Supervised Learning2
can help alleviate the dependence of machine learning on
labeled data. Semi-supervised learning changes the problem
setup by introducing a new unlabeled dataset containing ex-
amples u ∈ U. The training process then becomes a new
algorithm fθ ← Ts( f ,X ,U). The unlabeled dataset typically
consists of data drawn from a similar distribution as the la-
beled data.While semi-supervised learning has a long his-
tory [33, 38, 42, 50, 72], recent techniques have made signiﬁ-
cant progress [53, 66].
Throughout this paper we study the problem of image clas-
siﬁcation, the primary domain where strong semi-supervised
learning methods exist [42]. 3
Recent Techniques All state-of-the-art techniques from the
past two years rely on the same setup [53]: they turn the semi-
supervised machine learning problem (which is not well un-
derstood) into a fully-supervised problem (which is very well
understood). To do this, these methods compute a “guessed
label” ˆy = f (u;θi) for each unlabeled example u ∈ U, and
then treat the tuple (u, ˆy) as if it were a labeled sample [33],
thus constructing a new dataset U(cid:48). The problem is now fully-
supervised, and we can perform training as if by computing
T ( f ,X ∪ U(cid:48)). Because θi is the model’s current parameters,
note that we are using the model’s current predictions to
supervise its training for the next weights.
We evaluate the three current leading techniques: Mix-
Match [3], UDA [66], and FixMatch [53]. While they differ
in their details on how they generate the guessed label, and
in the strategy they use to further regularize the model, all
methods generate guessed labels as described above. These
differences are not fundamental to the results of our paper,
and we defer details to Appendix A.
Alternate Techniques Older semi-supervised learning
techniques are signiﬁcantly less effective. While FixMatch
reaches 5% error on CIFAR-10, none of these methods per-
form better than a 45% error rate—nine times less accurate.
Nevertheless, for completeness we consider older meth-
ods as well: we include evaluations of Virtual Adversarial
Training [39], PiModel [31], Mean Teacher [59], and Pseudo
Labels [33]. These older techniques often use a more ad hoc
approach to learning, which were later uniﬁed into a single
solution. For example, VAT [39] is built around the idea of
consistency regularization: a model’s predictions should not
change on perturbed versions of an input. In contrast, Mean
Teacher [59] takes a different approach of entropy minimiza-
tion: it uses prior models fθi to train a later model fθ j (for
i < j) and ﬁnd this additional regularization is helpful.
2We refrain from using the typical abbreviation, SSL, in a security paper.
3Recent work has explored alternate domains [44, 54, 66].
1578    30th USENIX Security Symposium
USENIX Association
2.3 Poisoning Attacks
While we are the ﬁrst to study poisoning attacks on unlabeled
data in semi-supervised learning, there is an extensive line of
work performing data poisoning attacks in a variety of fully-
supervised machine learning classiﬁers [2, 22, 23, 28, 40, 60]
as well as un-supervised clustering attacks [6, 7, 26, 27].
Poisoning labeled datasets.
In a poisoning attack, an ad-
versary either modiﬁes existing examples or inserts new exam-
ples into the training dataset in order to cause some potential
harm. There are two typical attack objectives: indiscriminate
and targeted poisoning.
In an indiscriminate poisoning attack [5, 40], the adversary
poisons the classiﬁer to reduce its accuracy. For example,
Nelson et al. [40] modify 1% of the training dataset to reduce
the accuracy of a spam classiﬁer to chance.
Targeted poisoning attacks [10, 28, 40], in comparison, aim
to cause the speciﬁc (mis-)prediction of a particular example.
For deep learning models that are able to memorize the train-
ing dataset, simply mislabeling an example will cause a model
to learn that incorrect label—however such attacks are easy
to detect. As a result, clean label [51] poisoning attacks inject
only images that are correctly labeled to the training dataset.
For instance, one state-of-the-art attack [71] modiﬁes 1% of
the training dataset in order to misclassify a CIFAR-10 [29]
test image. Recent work [35] has studied attacks that poison
the labeled dataset of a semi-supervised learning algorithm
to cause various effects. This setting is simpler than ours, as
an adversary can control the labeling process.
Between targeted and indiscriminate attacks lies backdoor
attack [18, 36, 60]. Here, an adversary poisons a dataset so
the model will mislabel any image with a particular pattern
applied, but leaves all other images unchanged. We do not
consider backdoor attacks in this paper.
Poisoning unsupervised clustering
In unsupervised clus-
tering, there are no labels, and the classiﬁer’s objective is to
group together similar classes without supervision. Prior work
has shown it is possible to poison clustering algorithms by
injecting unlabeled data to indiscriminately reduce model ac-
curacy [6, 7]. This work constructs bridge examples that con-
nect independent clusters of examples. By inserting a bridge
connecting two existing clusters, the clustering algorithm will
group together both (original) clusters into one new cluster.
We show that a similar technique can be adapted to targeted
misclassiﬁcation attacks for semi-supervised learning.
Whereas this clustering-based work is able to analytically
construct near-optimal attacks [26, 27] for semi-supervised
algorithms, analyzing the dynamics of stochastic gradient
descent is far more complicated. Thus, instead of being able
to derive an optimal strategy, we must perform extensive
experiments to understand how to form these bridges and
understand when they will successfully poison the classiﬁer.
2.4 Threat Model
We consider a victim who trains a machine learning model on
a dataset with limited labeled examples (e.g., images, audio,
malware, etc). To obtain more unlabeled examples, the victim
scrapes (a portion of) the public Internet for more examples
of the desired type. For example, a state-of-the-art Image
classiﬁer [37] was trained by scraping 1 billion images off of
Instagram. As a result, an adversary who can upload data to
the Internet can control a portion of the unlabeled dataset.
Formally, the unlabeled dataset poisoning adversary A con-
structs a set of poisoned examples
Up ← A(x∗,y∗,N, f ,Ts,X (cid:48)).
The adversary receives the input x∗ to be poisoned, the desired
incorrect target label y∗ (cid:54)= c(x∗), the number of examples N
that can be injected, the type of neural network f , the training
algorithm Ts, and a subset of the labeled examples X (cid:48) ⊂ X .
The adversary’s goal is to poison the victim’s model so
that the model fθ ← Ts(X ,U ∪ Up) will classify the selected
example as the desired target, i.e., fθ(x∗) = y∗. We require
|Up| < 0.01·|U|. This value poisoning 1% of the data has
been consistently used in data poisoning for over ten years
[5, 40, 51, 71]. (Interestingly, we ﬁnd that in many settings we
can succeed with just a 0.1% poisoning ratio.)
To perform our experiments, we randomly select x∗ from
among the examples in the test set, and then sample a label
y∗ randomly among those that are different than the true label
c(x∗). (Our attack will work for any desired example, not just
an example in the test set.)
3 Poisoning the Unlabeled Dataset