0
0
TPR
.9982
.9983
.9991
.9984
.9876
.9999
.9966
.9995
.9963
.7963
.9628
.8229
.9938
.9938
.9989
.9740
.9740
.9728
AUC
.9991
.9991
.9996
.9985
.9937
.9999
.9977
.9978
.9974
.8964
.9813
.9114
.9969
.9969
.9995
.9870
.9870
.9864
Comp
19.0
34.3
11.9
67.4
9.9
13.5
113.7
174.5
113.2
68.3
173.1
61.2
7.0
7.0
9.0
7.0
7.0
3.3
Var
2E-09
5E-08
6E-32
6E-07
6E-05
3E-08
8E-08
1E-08
1E-07
2E-05
3E-10
3E-10
1E-32
1E-32
1E-32
1E-32
4E-17
1E-28
observed FPR and TPR values were commensurate with the
rates presented previously.
VIII. DISCUSSION
The results presented in Section VII demonstrate that the
proposed methodology is capable of generating predicates
for efﬁcient error detection mechanisms. In particular, De-
cision Tree Induction has, even under a basic conﬁguration,
been shown to be an effective and consistent method for
generating predicates which exhibit a high true-positive rate
and a low false-positive rate. Crucially, as the best derived
predicate is represented as a decision tree, an example of
which is shown in Figure 2, it can easily be extracted by in-
terpreting the decision tree as a conjunction of disjunctions.
This means that implementing an error detection mechanism
based on a model generated using our methodology reduces
to the, almost trivial, process of interpreting a decision tree.
injection analysis is commonly used in the
validation of dependable software, the availability of fault
injection data can often be assumed. This means that the
main cost of applying the proposed methodology is associ-
ated with data mining algorithms, which in-turn means that
the cost of generating efﬁcient predicates using our approach
is related to dataset magnitude, the data mining algorithm
being applied and the comprehensiveness of the reﬁnement
undertaken. In this paper, we have shown that using only
a baseline conﬁguration of a learning algorithm can yield
highly efﬁcient predicates and that even a naive parameter
search can allow the efﬁciency of those predicates to be
consistently improved.
As fault
The focus of this paper has been on generating predicates
for error detection mechanisms that are capable of detecting
failure inducing states. Hence, the fault injection analysis
performed focused on recording the state of an executing
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:33:04 UTC from IEEE Xplore.  Restrictions apply. 
34program and whether that execution resulted in a failure.
This focus contrasts with existing work on fault injection,
which typically adopts the view that an error is any deviation
from a fault-free execution, i.e, golden run. Interestingly,
whilst the methodology proposed here is not directly appli-
cable in this context, we believe that it is possible to adopt a
similar approach in order to derive error detection predicates
that can identify such deviations from a fault-free execution.
The novelty of the proposed methodology is in the ap-
plication of data mining to fault injection data in order to
obtain predicates for efﬁcient error detection mechanisms.
The main advantage of this approach to predicate generation
is that efﬁcient error detection mechanisms can be obtained
by design. This contrasts with current approaches, which
often rely on the availability of a formal system speciﬁcation
or the experience of software engineers.
IX. CONCLUSION
A. Summary
In this paper, we presented a methodology for the genera-
tion of predicates for efﬁcient error detection mechanisms.
The premise of the methodology is that, given a program
location for which a detector component must be generated,
optimised data mining techniques can be used to analyse
fault injection data in order to generate efﬁcient predicates
for an efﬁcient error detection mechanism. In contrast to
current approaches, the methodology does not rely on a
system speciﬁcation or the experience of software engineers.
In demonstrating the application of the methodology we
have validated this premise, illustrating how data mining
techniques can be used to generate predicates that exhibit
high accuracy and completeness.
B. Future Work
In future work we plan to explore alternative approaches
to the systematic design of predicates for error detection
mechanisms. In particular, we will evaluate the applicability
and impact of alternative data mining algorithms, fault
models and system models in the generation of efﬁcient error
detection mechanisms.
REFERENCES
[1] J.-C. Laprie, Dependability: Basic Concepts and Terminology.
Springer-Verlag, December 1992.
[2] A. Arora and S. S. Kulkarni, “Detectors and correctors: A
theory of fault-tolerance components,” in Proceedings of the
18th IEEE International Conference on Distributed Comput-
ing Systems, May 1998, pp. 436–443.
[3] A. Jhumka, F. Freiling, C. Fetzer, and N. Suri, “An approach
to synthesise safe systems,” International Journal of Security
and Networks, vol. 1, no. 1, pp. 62–74, September 2006.
[4] M. Hsueh, T. K. Tsai, and R. K. Iyer, “Fault
injection
techniques and tools,” IEEE Computer, vol. 30, no. 4, pp.
75–82, April 1997.
[5] D. Powell, E. Martins, J. Arlat, and Y. Crouzet, “Estimators
for fault tolerance coverage evaluation,” IEEE Transactions
on Computers, vol. 44, no. 2, pp. 261–274, June 1995.
[6] M. Hiller, “Executable assertions for detecting data errors
in embedded control systems,” in Proceedings of the 30th
IEEE/IFIP International Conference on Dependable Systems
and Networks, June 2000, pp. 24–33.
[7] J. Vinter, J. Aidemark, P. Folkesson, and J. Karlsson, “Re-
ducing critical failures for control algorithms using executable
assertions and best effort recovery,” in Proceedings of the 37th
IEEE/IFIP International Conference on Dependable Systems
and Networks, July 2001, pp. 347–356.
[8] N. G. Leveson, S. S. Cha, J. C. Knight, and T. J. Shimeall,
“The use of self checks and voting in software error detec-
tion: An empirical study,” IEEE Transactions on Software
Engineering, vol. 16, no. 4, pp. 432–443, April 1990.
[9] A. Jhumka and M. Leeke, “Issues on the design of efﬁcient
fail-safe fault tolerance,” in Proceedings International Sym-
posium on Software Reliability Engineering, November 2009,
pp. 155–164.
[10] S. S. Kulkarni and A. Arora, “Automating the addition
of fault-tolerance,” in Proceedings of the 6th International
Symposium on Formal Techniques in Real-Time and Fault-
Tolerant Systems, September 2000, pp. 82–93.
[11] S. S. Kulkarni and A. Ebnenasir, “Complexity of adding
failsafe fault-tolerance,” in Proceedings of the 22nd IEEE
International Conference on Distributed Computing Systems,
July 2002, pp. 337–344.
[12] M. Hiller, A. Jhumka, and N. Suri, “Propane: An environment
for examining the propagation of errors in software,” in Pro-
ceedings of the 11th ACM SIGSOFT International Symposium
on Software Testing and Analysis, July 2002, pp. 81–85.
[13] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann,
and I. H. Witten, “The weka data mining software: An
update,” SIGKDD Explorations, vol. 11, no. 1, pp. 10–18,
June 2009.
[14] M. Hiller, A. Jhumka, and N. Suri, “An approach for
analysing the propagation of data errors in software,” in
Proceedings of the 31st IEEE/IFIP International Conference
on Dependable Systems and Networks, July 2001, pp. 161–
172.
[15] A. Jhumka, M. Hiller, and N. Suri, “An approach for de-
signing and assessing detectors for dependable component-
based systems,” in Proceedings of the 8th IEEE International
Symposium on High Assurance Systems Engineering, March
2004, pp. 69–78.
[16] G. Pinter, H. Madeira, M. Vieira, A. Pataricza, and I. Majzik,
“A data mining approach to identify key factors in dependabil-
ity experiments in dependable computing,” in Proceedings of
the 5th European Dependable Computing Conference, March
2005, pp. 263–280.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:33:04 UTC from IEEE Xplore.  Restrictions apply. 
35[31] K. M. Ting, “An instance-weighting method to induce cost-
sensitive trees,” IEEE Transactions on Knowledge and Data
Engineering, vol. 14, no. 3, pp. 659–665, May 2002.
[32] P. Domingos, “A general method for making classiﬁers cost-
sensitive,” in Proceedings of the 5th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery and Data Mining,
July 1999, pp. 155–164.
[33] W. Fan, S. J. Stolfo, J. Zhang, and P. K. Chan, “Misclassi-
ﬁcation cost-sensitive boosting,” in Proceedings of the 16th
International Conference on Machine Learning, June 1999,
pp. 97–105.
[34] J. Quinlan, C4.5: Programs for Machine Learning. Morgan
Kaufmann, October 1992.
[35] M. Kubat and S. Matwin, “Addressing the curse of im-
balanced training sets: One-sided selection,” in Proceedings
of the 14th International Conference on Machine Learning,
January 1997, pp. 179–186.
[36] D. D. Lewis and J. Catlett, “Heterogeneous uncertainty sam-
pling for supervised learning,” in Proceedings of the 11th
International Conference on Machine Learning, June 1994,
pp. 148–156.
[37] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P.
Kegelmeyer, “Smote: Synthetic minority over-sampling tech-
nique,” Journal of Artiﬁcial Intelligence Research, vol. 16,
no. 1, pp. 321–357, May 2002.
[38] B. Zadrozny, J. Langford, and N. Abe, “Cost-sensitive learn-
ing by cost-proportionate example weighting,” in Proceedings
of the 3rd IEEE International Conference on Data Mining,
July, 2003, pp. 435–442.
[39] N. V. Chawla, D. A. Cieslak, L. O. Hall, and A. Joshi, “Auto-
matically countering imbalance and its empirical relationship
to cost,” Journal of Data Minining and Knowledge Discovery,
vol. 17, no. 2, pp. 225–252, February 2008.
[40] 7-Zip, “http://www.7-zip.org/,” 2010.
[41] FlightGear, “http://www.ﬂightgear.org/,” April 2009.
[42] Mp3Gain, “http://mp3gain.sourceforge.net/,” 2010.
[17] C.-T. Lu, A. P. Boedihardjo, and P. Manalwar, “Exploiting
efﬁcient data mining techniques to enhance intrusion detec-
tion systems,” in Proceedings of the 2005 IEEE International
Conference on Information Reuse and Integration, September
2005, pp. 512–517.
[18] S. J. Wenke Lee Stolfo and K. W. Mok, “A data mining frame-
work for building intrusion detection models,” in Proceedings
of the 20th IEEE Symposium on Security and Privacy, May
1999, pp. 120–132.
[19] E. Clarke, O. Grumberg, and D. Peled, Model Checking. MIT
Press, January 2000.
[20] D. A. Schmidt, “Data ﬂow analysis is model checking of
abstract interpretations,” in Proceedings of the 25th ACM
SIGPLAN-SIGACT Symposium on Principles of Programming
Languages, January 1998, pp. 38–48.
[21] P. Cousot and R. Cousot, “Abstract interpretation: A uniﬁed
lattice model for static analysis of programs by construction
or approximation of ﬁxpoints,” in Proceedings 6th ACM
SIGPLAN-SIGACT Symposium on Principles of Programming
Languages, January 1977, pp. 238–252.
[22] M. D. Ernst, J. Cockrell, W. G. Griswold, and D. Notkin,
“Dynamically discovering likely program invariants to sup-
port program evolution,” IEEE Transactions on Software
Engineering, vol. 27, no. 2, pp. 99–123, February 2001.
[23] B. Demsky, M. D. Ernst, P. J. Guo, S. McCamant, J. H.
Perkins, and M. Rinard, “Inference and enforcement of data
structure consistency speciﬁcations,” in Proceedings of the
International Symposium on Software Testing and Analysis,
May 2006, pp. 233–244.
[24] S. K. Sahoo, M. Li, P. Ramachandran, S. V. Adve, V. S.
Adve, and Y. Zhou, “Using likely program invariants to detect
hardware errors,” in Proceedings of the 38th IEEE/IFIP Inter-
national Conference on Dependable Systems and Networks,
June 2008, pp. 70–79.
[25] D. Powell, “Failure model assumptions and assumption cov-
erage,” in Proceedings of the 22nd International Symposium
on Fault-Tolerant Computing, July 1992, pp. 386–395.
[26] M. Kubat, R. C. Holte, and S. Matwin, “Machine learning for
the detection of oil spills in satellite radar images,” Machine
Learning, vol. 30, no. 2-3, pp. 195–215, 1998.
[27] T. Fawcett, “An introduction to roc analysis,” Pattern Recog-
nition Letters, vol. 27, no. 8, pp. 861–874, June 2006.
[28] N. Japkowicz, “The class imbalance problem: Signiﬁcance
and strategies,” in Proceedings of
the 2000 International
Conference on Artiﬁcial Intelligence (ICAI, June 2000, pp.
111–117.
[29] L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone,
Chapman and Hall/
Classiﬁcation and Regression Trees.
CRC, January 1984.
[30] M. Pazzani, C. Merz, P. Murphy, K. Ali, T. Hume, and
C. Brunk, “Reducing misclassiﬁcation costs,” in Proceedings
of the 11th International Conference on Machine Learning,
July 1994, pp. 217–225.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:33:04 UTC from IEEE Xplore.  Restrictions apply. 
36