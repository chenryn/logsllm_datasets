dently as deﬁned by their individual delay parameter.
5.4.2 Anonymity of MultiSphinx
All MultiSphinx messages (before and after the multiplica-
tion node) have the same header length and payload length
as regular Sphinx messages. Sphinx headers do not leak the
number of remaining hops and the ciphertext is indistinguish-
able from random noise. Therefore, MultiSphinx messages
are indistinguishable from regular Loopix messages. At the
same time, the multiplication node maintains the unlinkability
between the incoming messages and outgoing messages as
these are delayed independently.
An adversary might also corrupt mix nodes. Even in this
case they do not gain advantage over regular Sphinx message
with regards to sender and recipient anonymity and unlinka-
bility. These results also hold for active adversaries with the
capabilities from the original Loopix paper.
If an adversary controls a p-restricted multiplication node
and c3 of the n3 mix nodes of the third layer, they can trace
some messages from their multiplication to their delivery at
providers. On the basis that the p recipients of a MultiSphinx
message are likely to be members of the same group, the
adversary then has a chance to guess that any two of the
users from these providers share a group membership. The
probability of correctly guessing two group members given
a group message is less than (1 − ( n3−c3
|U|2 if all
n3
|U| users are evenly distributed among |P | providers. This
attack is prevented if the multiplication node or all but one
of the chosen nodes in the third layer are trustworthy. (In
contrast, standard Loopix requires only that any mix node
on the message path is trustworthy.) MultiSphinx does not
leak any information regarding group sizes. The extended
paper (see Appendix C) contains theorems and proofs for our
claims.
)p−1) · |P |2
In addition to these properties, it is possible to achieve
sender anonymity by ﬁrst forwarding the message to a trusted
group member. The sender can prove its membership through
a shared group secret. We leave receiver anonymity and un-
linkable group membership for future work.
5.5 Further Optimisations
The schedule computed by GENSCHEDULE in Algorithm 1
delivers the ﬁrst messages to the nodes at the beginning of
the provided recipient list Urecv. These nodes will always act
as the forwarding nodes. To better balance these among all
group members, one can shufﬂe the list based on a nonce
value that is part of the message. This variant is described
in Algorithm 2. As the GENSCHEDULERAND algorithm is
still deterministic and the nonce is part of the Rollercoaster
header, each node reconstructs the same schedule.
Algorithm 2 Creating a pseudorandomized schedule for a
given nonce
1: procedure GENSCHEDULERAND(s, Urecv, k, nonce)
2:
R ← NEWPRNG(nonce)
U ′
recv ← R.shufﬂe(Urecv)
return GENSCHEDULE(s, U ′
recv, k)
3:
4:
Further optimisation is possible if different sub-groups dis-
play different levels of activity and connectivity. For example,
if there is a small, active sub-group communicating while the
rest of the group remains passive, it is more important for mes-
sages to travel faster between active nodes to support swift,
effective collaboration. Active nodes can often be assumed
to be more likely to be online. Agreeing on the full order is
no longer possible through a single nonce value. However,
the source can randomly compute a subset of all schedules,
evaluate the generated schedule against its information about
the group members, and choose one that creates a schedule
with the most desirable properties.
6 Evaluation
For the empirical evaluation we developed a mix network sim-
ulation tool that provides fully reproducible results. First, we
discuss the behaviour and results of the Rollercoaster scheme
in an ideal scenario where all participants are online through-
out. Second, we discuss the impact of ofﬂine nodes and how
this is addressed by the fault-tolerant variant of Rollercoaster.
Finally, we discuss the impact of multi-group membership,
sending multiple messages at once, and p-restricted multicast.
6.1 Methodology
Since the real-world performance of Loopix has been prac-
tically demonstrated [3] we run a simulation instead of an
experiment on a real network. This provides clear practical
advantages: First, it allows us to eliminate external inﬂuences
such as network congestion due to unrelated trafﬁc or CPU
usage by other processes. Second, the simulated time can run
faster than real-time, allowing us to gather signiﬁcantly more
results using less computational resources. Third, it makes
monitoring and categorising trafﬁc easier as packets and node
3442    30th USENIX Security Symposium
USENIX Association
state can be inspected. Finally, by initialising the PRNG with
a ﬁxed seed, the results of this paper are fully reproducible.
The simulator runs the entire mix network on a single ma-
chine, with nodes communicating through shared memory
simulating a network. It instantiates objects for each partici-
pating user, provider, and mix node. All objects implement
a tick() method in which they process incoming messages
and mimic the designed node behaviour such as delaying and
forwarding packets. As we are primarily interested in the traf-
ﬁc behaviour, no actual encryption is performed. The original
Loopix paper has shown that the queuing time and per-hop
delays dominate the message delay, and that CPU time for
cryptographic operations is insigniﬁcant in comparison. Simi-
larly, the network delay is negligible.
For the ﬁnal evaluation we ran 276 independent simulations,
covering more than 992,160 hours of simulated node time in
less than 209 hours of CPU core time. This is a relative speed
up by factor 4500× compared to a real network experiment of
the same scope. In every simulation step the application (see
below) measures the message latency dmsg of each delivered
message between the original source and each recipient. We
veriﬁed that our simulator behaves faithfully to the Loopix
implementation by reproducing a latency distribution graph
from the original paper [3, Figure 11]. Our simulator is im-
plemented in less than 2,000 lines of Python code including
tests and is available as an open-source project.2
The network simulator assigns 16 users to each provider.
We set the Loopix rates λp = λd = λl = 2/s for the client
nodes and the delay rate λµ = 3/s. Hence, the overall sending
rate of the clients is λ = 6/s. This meets the requirement
λ/λµ ≥ 2 that is suggested by the Loopix paper [3, p. 1209,
λµ = µ]. The mix network consists of 3 layers containing
3 mix nodes each (mix loop injection rate λM = 2/s). All
simulations are run with a granularity of 10 ms per tick. The
simulated time span for all conﬁgurations is 24 h.
The application behaviour is modelled by a Poisson pro-
cess. On average every 30 s one of the online nodes sends
a single message to all other group members. We account
for participation inequality [8] by dividing the group using
an 80/20 rule: 20% of users in the group send 80% of all
messages, and vice versa.
6.2 Results with All Users Online
For a group of size 128, the average latency is reduced from
34.9 s in sequential unicast to 7.0 s (8.3 s for group size 256)
in Rollercoaster with k = p = 2 . This fulﬁls our application
requirements that were derived from the user study concern-
ing delay in collaborative applications [7]. The results are
compatible with our analytical results as discussed in Sec-
tion 5.3. For Rollercoaster not only is the average latency low,
but most of the latency distribution falls within fairly tight
bounds – that is, very large latencies are rare. Figure 5 shows
2https://github.com/lambdapioneer/rollercoaster
Figure 5: This box plot shows the distributions of message
latency dmsg for increasing group sizes for the strategies naïve
sequential unicast and Rollercoaster (RC). The Rollercoaster
strategies show different k and p parameters. The boxes span
from the ﬁrst quartile to the third quartile (middle line is the
median) and the whiskers indicate the 1st and 99th percentile.
the latency achieved by the Rollercoaster scheme with and
without p-restricted multicast for different percentiles and
compares them to unicast. For a group with 128 members
the 99th percentile p99 for Rollercoaster is 12.3 s (p90: 9.9 s)
whereas in unicast it is 75.6 s (p90: 60.8 s). We provide de-
tailed histograms in the extended paper (see Appendix C).
6.3 Results for Fault-Tolerance Scenarios
The evaluation of the fault tolerance properties requires a
realistic model of connectivity of mobile devices. For this we
processed data from the Device Analyzer project [17] that
contains usage data of volunteers who installed the Android
app. The online/ofﬂine state of a device is derived from its
trace information regarding network state, signal strength, and
airplane mode. We limit the dataset (n = 27790) to traces that
contain connectivity information (n = 25618), cover at least
48 hours (n = 20117), and have no interval larger than 12
hours without any data (n = 2772).
Inspecting the traces we identify three archetypes of online
behaviour. The ﬁrst group is online most of the time and is
only interrupted by shorter ofﬂine segments of less than 60
minutes. Members of the second group have at least one large
online segment of > 8 hours and are on average online 50%
or more of the time. Finally, the third group is online less
than 50% of the time with many frequent changes between
online and ofﬂine states. As the dataset is more than ﬁve years
old we decided to use the characteristics of these groups to
build a model. Using a model allows us to extrapolate ofﬂine
behaviour into scenarios with increased connectivity. In the
model following the parameters of the original dataset, the
fraction of all users’ time spent online is 65%. In a second
and third model with increased connectivity, we increase this
percentage to 80% and 88%, respectively, while preserving
the behaviour of the archetype groups. The generated models
are visualised in the extended paper (see Appendix C).
USENIX Association
30th USENIX Security Symposium    3443
 m=32 m=45 m=64 m=91 m=128 m=181 m=256Group size0102030405060708090100110Message latencydmsg [s]p99=151.5sUnicastRC (k=p=1)RC (k=p=2)Figure 6: The distribution of message latency dmsg for differ-
ent ofﬂine scenarios. From left to right the strategies are Uni-
cast, Rollercoaster without fault-tolerance (RC), and Roller-
coaster with fault-tolerance (RC-FT). Boxes and whiskers as
in Figure 5.
Figure 8: Message latency dmsg for applications that send b
messages at once. The group size is m = 128. Boxes and
whiskers as in Figure 5.
Figure 7: Message latency dmsg for an increasing number of
groups for 128 users (every user is member of every group).
Boxes and whiskers as in Figure 5.
For our discussion of ofﬂine behaviour we reﬁne our previ-
ous deﬁnition of message latency dmsg: we ignore all latencies
where the intended recipient was ofﬂine when the message
was placed into their inbox by the provider node. This change
has the practical beneﬁt of excluding outliers. More impor-
tantly, fast delivery to an ofﬂine user has no real-world beneﬁt.
Instead, a good multicast algorithm should optimise the de-
livery to all nodes that are active and can actually process an
incoming message. The source might go ofﬂine at any time
regardless of outstanding messages.
Without fault tolerance, the presence of ofﬂine nodes
greatly increases the 99th percentile (p99) for Rollercoaster
(RC) to more than 10,000 s for a group of 128 members. The
fault-tolerant variant (RC-FT) reduces the 99th percentile to
less than 21.9 s (p90: 18.0 s). In unicast p99 latency is 103.3 s
(p90: 61.9 s). Figure 6 shows that the fault-tolerant variant gen-
erally outperforms unicast at various percentiles. We provide
detailed histograms in the extended paper (see Appendix C).
6.4 Multiple Groups and Message Bursts
Users might be part of multiple groups, which increases their
burden of distributing messages. In this evaluation we assign
128 users to a growing number of groups. Figure 7 shows that
the number of group memberships has little impact on Roller-
coaster’s performance both for online and ofﬂine scenarios.
Figure 9: Heatmaps showing the mean message latency for
reduced sending rates (y-axis) and different Rollercoaster
parameters (x-axis). In the left graph only the logical branch
factor k is increased. In the right graph the multicast factor
p is increased at the same time. Group size is m = 128 and
80% online.
Similarly, users might be sharing large payloads (e.g. im-
ages) or sending multiple updates at once. Both translate into
many messages being scheduled for distribution at the same
time, which risks overwhelming the payload queue. Figure 8
shows that Rollercoaster can handle many more messages
sent in bursts than unicast. We observed that with unicast
and some Rollercoaster conﬁgurations some nodes had indeﬁ-
nitely growing send buffers as the simulation progressed. The
effect of this can be seen by the higher message latencies for
b = 32. This threshold is higher for p-restricted multicast.
6.5 Results for p-Restricted Multicast
In this evaluation we show that p-restricted multicast allows
us to drastically lower the sending rates λ{p,d,l} of the clients
while achieving similar performance. A low sending rate is
desirable as it allows the radio network module to return to
standby and thereby saving signiﬁcant battery energy on mo-
bile devices (see §2). Figure 9 shows that just increasing k
(left) has negligible or even negative impact, while increasing
k and p together (right) allows for lower sending rate λ while
maintaining good enough performance. We decrease λµ ac-
cordingly to maintain the λ/λµ ≥ 2 balance (see §6.1) which
increases the per-hop delays.
3444    30th USENIX Security Symposium
USENIX Association
65% onl.80% onl.88% onl.Group size m=128110100100010000Message latency dmsg [s]65% onl.80% onl.88% onl.Group size m=256110100100010000UnicastRC (k=p=2)RC-FT (k=p=1)RC-FT (k=p=2)124816Total number of groups (100% online)110100Message latencydmsg [s]124816Total number of groups (80% online)110100UnicastRC-FT (k=p=1)RC-FT (k=p=2) 1 2 4 8 16 32Message burst b (100% online)110100100010000Message latencydmsg [s] 1 2 4 8 16 32Message burst b (80% online)110100100010000UnicastRC-FT (k=p=1)RC-FT (k=p=2)k=1,p=1k=2,p=1k=4,p=1k=8,p=1k=16,p=1λ/1λ/2λ/4λ/8λ/16k=p=1k=p=2k=p=4k=p=8k=p=16λ/1λ/2λ/4λ/8λ/16050100150200mean dmsg7 Related Work
Previous research on efﬁcient anonymity networks achieves
strong security goals, high efﬁciency, scalability, and ofﬂine
support. However, decentralised low-latency group multicast
while guaranteeing the strongest privacy guarantees against a
global adversary has not yet received due attention.
Work based on Dining Cryptographer networks (DC-nets)
[18] is inherently broadcast-based as the round results are
shared with all nodes. These designs generally provide sender
anonymity and impressive functionality. However, the re-
quired synchronisation and communication overhead render
them unsuitable for low latency applications. As the rounds
depend on the calculations of all clients, they can be suscepti-
ble to interference by malicious participants. The Xor-Trees
by Dolev et al. [19] achieve efﬁcient multicast, but only in
the absence of an active attacker. Dissent [20] can provide
protection against such active attacks. However, its design
does not scale as well as Loopix due to its need to broadcast
messages to all clients, and not just the intended group of
recipients.
Circuit-based onion routing networks such as Tor [1] es-
tablish long-living paths through multiple hops. All messages
from and to the client are transmitted via the same path with
every node peeling off the outer-most encryption layer. They
are arguably the most widely deployed and accessible class of
anonymity network designs. While the onion path approach
allows for low latency communication, it is known to be vul-
nerable against global adversaries performing trafﬁc analysis
attacks [2, 21]. Most mainstream designs consider one-to-
one communication, but there is interesting work on building
multicast trees using onion-routing techniques. Examples are
AP3 [22], M2 [23], and MTor [24]. When facing a global
adversary, they share similar vulnerabilities to Tor.
Multicast in friend-to-friend overlays as in VOUTE [25,26]
share a similarity with our work as trusted peers help with
message distribution. However, to our knowledge, there are
no practical implementations with performance similar to
Loopix. Using real-world trust relationships together with
Rollercoaster for inter-group communication is an interesting
direction for future work.
The recent Vuvuzela design [27] cleverly leverages dead
drops and cover trafﬁc to achieve strong metadata privacy
while maintaining a high throughput of messages. Pursuing
the goal of limiting network bandwidth use results in delays
of up to 10 minutes to initiate a call and more than 30 seconds
latency for messages, which we consider too large for many
collaborative applications. Its privacy guarantees can be lim-
ited in the case of an active attacker with a priori suspicion of
a certain group of users communicating.
Work based on private information retrieval (PIR) such
as Pung [28] and Talek [29] allows for low-latency group
communication with strong security guarantees. However,
these systems are not decentralised and rely on the availability
of high-spec servers. Moreover, their latency scales with the
total number of users n rather than the group sizes.
We note that our evaluation differs from the standard meth-
ods in similar papers [3, 20, 27] using real servers and net-
works. Since it is already established that the performance
of Loopix is viable in practise, we can build on top of this
and focus on more inspectable and reproducible evaluations
through deterministic simulation.
The Shadow project [30] can simulate actual anonymity
network implementations in a network topology on a single
machine. With extensive modelling options the network and
user behaviour can be modelled deterministically. However,
since the application binaries remain black-boxes it cannot
guarantee complete deterministic behaviour. White-box simu-
lators such as Mixim [31] calculate the entropy as messages
pass through the system.
Many multicast systems use distribution trees [32–36].
However, to our knowledge, these protocols have not yet been
applied in the context of mix networks, where the limited
send rate and artiﬁcial message delays introduce particular
challenges not considered by existing multicast protocols.
8 Conclusion
In this paper we have presented an efﬁcient scheme for multi-
cast in mix networks named Rollercoaster. Compared to the
sender of a message naïvely sending it to all other group mem-
bers by unicast, our scheme signiﬁcantly lowers the time until
all group members receive the message. For a group of size
m = 128, Rollercoaster is faster by a factor of 5, reducing
the average delay from 34.9 s to 7.0 s and reducing the 99th