User: I have the below requirements and confused about which one to choose for high performance. I am not java developer. I am comfort with Hive, Pig and Python. 
I am using HDP2.1 with tez engine. Data sources are text files(80 GB) and Oracle table(15GB). Both are structured data. I heard Hive will suite for structure data and Python map reduce streaming concept too will have high performance than hive & Pig. Please clarify. 
I am using Hive and the reasons are:

need to join those two sources based on one column.  
using ORC format table to store the join results since the data size is huge 
text file name will be used to generate one output column and that has been performed with virtual column concept input__file__name field.
After join need to do some arithmetic operations on each row and doing that via python UDF

Now the complete execution time from  data copy into HDFS to final result taken 2.30 hrs with 4 node cluster using Hive and Python UDF. 
My questions are:
1) I heard Java Mapreduce always faster. Will that be true with Python Map reduce streaming concept too?
2) Can I achieve all the above functions in Python like join, retrieval of  text file name, compressed data flow like ORC since the volume is high?
3) Will Pig join would be better than Hive? If yes can we get input text file name in Pig to generate output column?
Thanks in advance.
Assistant: Python Map Reduce or anything using Hadoop Streaming interface will most likely be slower. That is due to the overhead of passing data through stdin and stdout and the implementation of the streaming API consumer (in your case python). Python UDF's in Hive and Pig do the same thing.
You might not want to compress data flow into ORC on the Python side. You'll be subjected to using Python's ORC libraries, which I am not sure if they are available. It would be easier if you let Python return your serialized object and the Hadoop reduce steps to compress and store as ORC (Python as a UDF for computation)
Yes. Pig and Python have some what of a nice programmatic interface where in you can write python scripts to dynamically generate Pig Logic and submit it in parallel. look up Embedding Pig Latin in Python. It's robust enough to define Python UDFS and let Pig do the overall abstraction and job optimization. Pig does a lazy evaluation so in cases of multiple joins or multiple transformations it can demonstrate pretty good performance in the optimizing the complete pipe line. 

You say HDP 2.1. Have you had a look at Spark ? If performance is important to you and looking at the datasets size which dont look huge you ll expect many time faster overall pipeline execution than Hadoop s native MR engine