broadly classified into two categories: restricting the information re-
turned by models [36, 64] and differentiating malicious adversaries
from normal users [30]. Tramèr et al. propose a defense where
the model should only return class labels instead of class proba-
bilities [64]. Recently, a technique PRADA has proposed to guard
machine learning models by detecting abnormal query patterns [30].
Watermarking ML models as a passive defense mechanism recently
has been proposed to claim model’s ownership [8, 29, 38]. However,
these defense techniques are used to protect discriminative models
where models return probabilities or labels. In this paper, we focus
on defense approaches safeguarding generative adversarial networks
where models return images.
3 PRELIMINARIES
In this section, we begin with the general structure of GANs. Then,
we proceed with discussing model extraction attacks in a general
machine learning setting. Finally, we describe datasets used in this
paper.
3.1 Generative Adversarial Networks
GAN is a generative model where it adversarially learns the un-
known true distribution pr on the training data X. As shown in
Figure 2, a GAN generally consists of two components: a generator
G and a discriminator D. G is responsible for generating fake data
xд = G (z), where the latent code z is sampled from a prior distribu-
tion pz, such as Gaussian distribution or uniform distribution, while
D takes the role of a binary classifier which differentiates real-like
samples xд from real samples xr ∈ X as accurately as possible. The
seminal GAN [19] is trained through optimizing the following loss
functions:
LD = −Ex∼pr [log D (x)] − Ez∼pz [1 − log D (G (z))]
(1)
(2)
If D and G converge and reach global equilibrium, then pr (x) =
pд (x), where pд (x) is the generator’s distribution. For a fixed G,
the optimal discriminator D∗ can be obtained by:
LG = −Ez∼pz [log D (G (z))]
∗ (x) =
D
pr (x)
pr (x) + pд (x)
(3)
In the course of employment, only G is utilized to produce new
synthetic data while D is usually discarded.
3.2 Model Extraction Attacks against Machine
Learning Models
A machine learning model is essentially a function f that maps
input data X to output data Y: Y = f (X). In general, machine
learning models can be categorized as two classes [4]: discriminative
models and generative models. For discriminative models on image
classification tasks, the input data corresponds to an image while
the output data can be interpreted as a probability distribution
over categorical labels. A key goal of discriminative models is to find
an optimal set of parameters which minimizes the errors on the test
dataset. For generative models on image generation tasks, the input
data is represented by a latent code and the output data is an image.
A core goal of generative models is to adjust the parameters to learn
a distribution which is similar to the training data distribution pr .
A model extraction attack in the machine learning setting
emerges when an adversary aims to obtain a copy model ˜f through
3ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Hailong Hu and Jun Pang
Table 1: Dataset description
Dataset
Size of dataset
Dataset
Size of dataset
LSUN-Bedroom LSUN-Kitchen CelebA
202,599
2,212,277
LSUN-Classroom LSUN-Church
126,277
3,033,042
168,103
Table 2: Notations
Notation Description
pr
pд
˜pд
fidelity
accuracy
distribution of training set of a GAN
implicit distribution of a target generator
implicit distribution of an attack generator
FID ( ˜pд, pд)
FID ( ˜pд, pr )
querying the target model f . In general, there are two types of
attacks around model extraction based on adversary’s objective:
accuracy extraction and fidelity extraction [27]. For discriminative
models, accuracy extraction requires the extracted model to match
or exceed the accuracy of the target model on the test dataset, while
fidelity extraction requires the extracted model not only to achieve
the same accuracy as the target model on the test dataset but also to
replicate the errors of the target model. The limit of fidelity extrac-
tion is the functionally-equivalent model extraction [27]. Consider-
ing different goals and evaluations between discriminative models
and generative models, we redefine model extraction on GANs in
Section 4.1.
3.3 Dataset Description
We utilize five different datasets in this paper, which are all widely
adopted in image generation. Among them, four datasets are from
the LSUN dataset [71] which includes 10 scene categories and 20
object categories and we define them as LSUN-Bedroom, LSUN-
Church, LSUN-Classroom, and LSUN-Kitchen, respectively. CelebA
dataset [41] consists of about 200K high-quality human face images.
Datasets including LSUN-Bedroom, LSUN-Classroom, and LSUN-
Kitchen are only used in Section 7 to illustrate the attack effects in
a case study. The details of the datasets are shown in Table 1.
4 TAXONOMY OF MODEL EXTRACTION
AGAINST GANS
In this section, we start with adversary’s goal and formally elab-
orate on our attacks. Next, we illustrate adversary’s background
knowledge where an adversary can mount attacks according to the
obtained information. Finally, we detail the metrics to evaluate the
attack performance.
4.1 Adversary’s Goals
In general, model extraction based on adversary’s goals can be cate-
gorized into either fidelity extraction or accuracy extraction. Unlike
supervised discriminative models aiming at minimizing errors on
a test set, unsupervised generative models target at learning the
distribution of a data set.
Therefore, for model extraction attacks on GANs, fidelity extrac-
tion aims to minimize the difference of data distribution between
Figure 1: Fidelity extraction and accuracy extraction.
attack models and target models, while accuracy extraction aims to
minimize the distribution between attack models and the training
set of target models.
Specifically, as shown in Figure 1, the goal of fidelity extraction
is to construct a ˜G minimizing S( ˜pд, pд), where S is a similarity
function, ˜pд is the implicit distribution of the attack generator ˜G,
and pд is the implicit distribution of the target generator G. In
contrast, accuracy extraction’s goal is to construct a ˜G minimizing
S( ˜pд, pr), where pr is the distribution of the training set of the target
generator G. In this work, we use Fréchet Inception Distance (FID)
to evaluate the similarity between two data distributions, mainly
considering its computational efficiency and robustness [23]. It
is elaborated in Section 4.3. In our work, we study the fidelity
extraction in Section 5, and accuracy extraction in Section 6.
4.2 Adversary’s Background Knowledge
Adversaries can mount model extraction attacks at different levels
based on their obtained information about the target GAN. The
more background knowledge adversaries acquire, the more effective
they should be in achieving their goal. In general, four components
of a GAN can be considered by an adversary. As shown in Figure
2, they are respectively: (1) generated data; (2) latent codes used
by interactively querying a generator; (3) partial real data from the
training dataset of the target GAN; (4) a discriminator from the
target GAN.
In the following attack settings, we assume an adversary ob-
tains different levels of background knowledge to achieve accuracy
extraction or fidelity extraction.
Figure 2: Adversary’s background knowledge.
4.3 Metrics
Metrics for GANs. We use the widely adopted FID [23] to evaluate
the performance of GANs. FID measures the similarity between pд
and pr . Specifically, on the basis of features extracted by the pre-
trained Inception network ϕ, it models ϕ(pr) and ϕ(pд) using Gauss-
ian distribution with mean µ and covariance Σ, and the value of FID
between real data pr and generated data pд in convolutional features
DatasetgeneratorAttack modelTarget ModelTrainingAdversaryQuery with latent code zImageTraining datasetdiscriminatorBlack-box generatorlatent code zfake or realTrainingFidelity  extraction:Accuracy extraction:gprpgp~),~(gpgpS),~(rpgpSreal datadiscriminatorgenerated datalatent code zFakeorReal generatordatasetdataset4Stealing Machine Learning Models: Attacks and Countermeasures for Generative Adversarial Networks
is computed as: FID(pr , pд) =(cid:12)(cid:12)(cid:12)(cid:12)µr − µд
(cid:12)(cid:12)(cid:12)(cid:12)2+Tr(Σr +Σд−2(Σr Σд)1/2),
where Tr refers to the trace of a matrix in linear algebra. A lower
FID indicates that the distribution’s discrepancy between the gen-
erated data and real-world data is smaller and the generated data is
more realistic. In our work, FID is computed with all real samples
and 50K generated samples.
Metrics for Attack Performance. In this work, we use two FID-
based metrics: fidelity and accuracy, to evaluate the attack perfor-
mance. Fidelity measures the consistency between pд which is an
implicit distribution of a target generator and ˜pд which is an im-
plicit distribution of an attack generator. Note that, fidelity not only
measures how close the attack model and the target model are,
but also indicates how well the performance of model itself is. In
contrast, accuracy measures the consistency of data distribution
between pr and ˜pд. Similar to FID, the smaller the fidelity and ac-
curacy values are, the better performance attack models achieve.
When it is clear from the context, we refer to accuracy and fidelity
as accuracy value and fidelity value, respectively. The summarized
notations can be seen in Table 2.
Fidelity extraction focuses on fidelity and adversaries aim to steal
the distribution of a target model. After obtaining an attack model
which steals from a target model, they can directly utilize it to gen-
erate new samples. Additionally, they can also transfer knowledge
of the stolen model to their own domains through transfer learn-
ing. In contrast, accuracy extraction concentrates on accuracy and
adversaries target at stealing the distribution of the training set of
a target model. This type of attacks can severely violate the privacy
of the training data and it also means that adversaries may steal
valuable commercial datasets from a trained GAN. Additionally,
adversaries can utilize the stolen high-accuracy model to mount
other novel attacks and we leave it for future work.
5 FIDELITY EXTRACTION
In this section, we instantiate our fidelity extraction attack strat-
egy. we assume that adversaries have access to either generated
samples provided by the model producer or querying the target
model to obtain data (see Figure 2). We start with target models and
attack models. Then, we describe our attack performance. Next, we
study the effect of the number of queries. In the end, we perform
experiments to deeply understand model extraction on GANs.
5.1 Target Models and Attack Models
We choose representative GANs: Progressive GAN (PGGAN) [31]
and Spectral Normalization GAN (SNGAN) [47] as our target mod-
els, which both show pleasing performances in image generation.
The implementation details can be seen in Appendix A.1. For train-
ing sets LSUN-Church and CelebA, we first resize them to 64 × 64
and use all records of each dataset to train our target models. As
shown in Table 3, target GAN models achieve an excellent perfor-
mance on these dataset and the performance of PGGAN is better
than that of SNGAN.
We use GANs as our attack models to extract target models. In
practice, adversaries may not know the target model’s architecture.
Therefore, we study the performance of attack models with dif-
ferent architectures. Specifically, we choose SNGAN and PGGAN
as our attack models. There are four different situations for their
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Table 3: Performance of target GANs.
Target model Dataset
SNGAN
SNGAN
PGGAN
PGGAN
LSUN-Church
CelebA
LSUN-Church
CelebA
FID
12.72
7.60
5.88
3.40
combinations. For simplification, we define each situation as an
attack-target model pair, and they are respectively SNGAN-SNGAN,
SNGAN-PGGAN, PGGAN-SNGAN and PGGAN-PGGAN. The rea-
son why we choose SNGAN and PGGAN as the research object
is that: 1) they both show good performance in image generation;
and 2) they have significant difference in the aspects of training,
loss function and normalization, which all facilitate us to study the
performance of attack models with different architectures.
5.2 Methodology
As shown in Figure 1, for fidelity extraction, we assume that an
adversary obtains the generated data by the model provider or
querying the target GAN. This scenario is practical, because some
model owners need to protect their models through providing the
public with some generated data or a black-box GAN model API. In
this case, the adversary uses the generated data to retrain a GAN to
extract the target model. We do not distinguish whether generated
data is from queries or model providers, because our approach
only relies on these generated data. However, in Appendix A.3, we
also present the attack performance on queries with different prior
distributions.
Note that model extraction on GANs is different from machine
learning on GANs. This is because machine learning on GANs
requires users to train a GAN on real samples which are collected
from the real world. In contrast, model extraction on GANs enables
users to train a GAN on generated data from a target GAN model. In
essence, model extraction on GANs approximates the target GAN
which is a much simpler deterministic function, compared to real
samples which usually represents a more complicated function.
5.3 Results
5.3.1 Attack Performance on Different Models. Table 4
shows the fidelity extraction’s performance with 50K queries to the
target model. In general, attack models can achieve an excellent
performance1. For instance, our attack performance of PGGAN-
PGGAN on the CelebA achieves 1.02 FID on fidelity, which means
that the attack model can achieve a perfect extraction attack for the
target model. It is noticeable that the the attack model achieves such
performance only on 50K generated images while the target model
is trained on more than 200K images. In Section 7, our case study
further illustrates that even for a GAN model trained on 3 million
samples, our attack still can achieve 4.12 fidelity with only 50K
queries. In other words, adversaries are able to obtain a good GAN
model only by access to the generated data from the target model