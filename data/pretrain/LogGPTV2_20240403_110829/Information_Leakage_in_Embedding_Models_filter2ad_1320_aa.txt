title:Information Leakage in Embedding Models
author:Congzheng Song and
Ananth Raghunathan
Information Leakage in Embedding Models
Ananth Raghunathan∗
Congzheng Song∗
Cornell University
PI:EMAIL
PI:EMAIL
Facebook
0
2
0
2
g
u
A
9
1
]
G
L
.
s
c
[
2
v
3
5
0
0
0
.
4
0
0
2
:
v
i
X
r
a
ABSTRACT
Embeddings are functions that map raw input data to low-dimensional
vector representations, while preserving important semantic in-
formation about the inputs. Pre-training embeddings on a large
amount of unlabeled data and fine-tuning them for downstream
tasks is now a de facto standard in achieving state of the art learning
in many domains.
We demonstrate that embeddings, in addition to encoding generic
semantics, often also present a vector that leaks sensitive informa-
tion about the input data. We develop three classes of attacks to
systematically study information that might be leaked by embed-
dings. First, embedding vectors can be inverted to partially recover
some of the input data. As an example, we show that our attacks
on popular sentence embeddings recover between 50%–70% of the
input words (F1 scores of 0.5–0.7). Second, embeddings may reveal
sensitive attributes inherent in inputs and independent of the un-
derlying semantic task at hand. Attributes such as authorship of
text can be easily extracted by training an inference model on just
a handful of labeled embedding vectors. Third, embedding models
leak moderate amount of membership information for infrequent
training data inputs. We extensively evaluate our attacks on vari-
ous state-of-the-art embedding models in the text domain. We also
propose and evaluate defenses that can prevent the leakage to some
extent at a minor cost in utility.
CCS CONCEPTS
• Security and privacy → Software and application security.
KEYWORDS
machine learning, embeddings, privacy
ACM Reference Format:
Congzheng Song and Ananth Raghunathan. 2020. Information Leakage in
Embedding Models. In Proceedings of the 2020 ACM SIGSAC Conference on
Computer and Communications Security (CCS ’20), November 9–13, 2020,
Virtual Event, USA. ACM, New York, NY, USA, 14 pages. https://doi.org/10.
1145/3372297.3417270
1 INTRODUCTION
Machine learning (ML) has seen an explosive growth over the past
decade and is now widely used across industry from image anal-
ysis [24, 34], speech recognition [22], and even in applications in
∗Work done while at Google Brain.
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
CCS ’20, November 9–13, 2020, Virtual Event, USA
© 2020 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-7089-9/20/11.
https://doi.org/10.1145/3372297.3417270
the medical sciences for diagnosis [8, 56]. These advances rely on
not only improved training algorithms and architectures, but also
access to high-quality and often sensitive data. The wide applica-
tions of machine learning and its reliance on quality training data
necessitates a better understanding of how exactly ML models work
and how they interact with their training data. Naturally, there is
increasing literature focused on these aspects, both in the domains
of interpretability and fairness of models, and their privacy. The
latter is further of timely importance given recent regulations under
the European Union’s General Data Protection Regulation (GDPR)
umbrella requiring users to have greater control of their data.
Applying a line of research investigating whether individual
genomic records can be subsequently identified [28, 62], Shokri et
al. [66] developed the first membership inference tests investigating
how ML models may leak some of their training data. This subse-
quently led to a large body of work exploring this space [45, 48,
60, 61, 77]. Another research direction investigating how models
might memorize their training data has also shown promising re-
sults [4, 67]. Apart from training data privacy, modern deep learning
models can unintentionally leak information of the sensitive input
from the model’s representation at inference time [9, 18, 38, 50, 68].
This growing body of work analyzing models from the stand-
point of privacy aims to answer natural questions: Besides the learn-
ing task at hand, what other information do models capture or expose
about their training data? And, what functionalities may be captured
by ML models unintentionally? We also note that beyond the scope
of this paper are other rich sets of questions around adversarial
behavior with ML models—the presence of adversarial examples,
data poisoning attacks, and more.
Embeddings. In most of the research investigating the privacy
of ML models, one class of models is largely conspicuously absent.
Embeddings are mathematical functions that map raw objects (such
as words, sentences, images, user activities) to real valued vectors
with the goal of capturing and preserving some important semantic
meaning about the underlying objects. The most common appli-
cation of embeddings is transfer learning where the embeddings
are pre-trained on a large amount of unlabeled raw data and later
fine-tuned on downstream tasks with limited labeled data. Transfer
learning from embeddings have been shown to be tremendously
useful for many natural language processing (NLP) tasks such as
paraphrasing [14], response suggestion [30], information retrieval,
text classification, and question answering [57], where labeled data
is typically expensive to collect. Embeddings have also been success-
fully applied to other data domains including social networks [21],
source code [2], YouTube watches [11], movie feedback [25], lo-
cations [12], etc. In some sense, their widespread use should not
be surprising—embedding models and their success illustrate why
deep learning has been successful at capturing interesting semantics
from large quantities of raw data.
Embedding models are often pre-trained on raw and unlabeled
data at hand, then used with labeled data to transfer learning to
various downstream tasks. Our study of embeddings is motivated
by their widespread application and trying to better understand
how these two stages of training may capture and subsequently
leak information about sensitive data. While it is attractive that
these models inherently capture relations and similarities and other
semantic relationships between raw objects like words or sentences,
it also behooves one to consider if this is all the information being
captured by the embeddings. Do they, perhaps, in addition to mean-
ing of words inadvertently capture information about the authors
perhaps? Would that have consequences if these embeddings are
used in adversarial ways? What aspects of the seminal research
into privacy of ML models through the lens of membership infer-
ence attacks and memorization propensity can we apply to better
understand the privacy risks of embeddings?
These questions and more lead us to initiate a systematic study
of the privacy of embedding models by considering three broad
classes of attacks. We first consider embedding inversion attacks
whose goal is to invert a given embedding back to the sensitive
raw text inputs (such as private user messages). For embeddings
susceptible to these attacks, it is imperative that we consider the
embedding outputs containing inherently as much information
with respect to risks of leakage as the underlying sensitive data
itself. The fact that embeddings appear to be abstract real-numbered
vectors should not be misconstrued as being safe.
Along the lines of Song and Shmatikov [68], we consider at-
tribute inference attacks to test if embeddings might unintentionally
reveal attributes of their input data that are sensitive; aspects that
are not useful for their downstream applications. This threat model
is particularly important when sensitive attributes orthogonal to
downstream tasks are quickly revealed given very little auxiliary
data, something that the adversary might be able to capture through
external means. As an example, given (sensitive) demographic in-
formation (such as gender) that an adversary might retrieve for a
limited set of data points, the embeddings should ideally not en-
able even somewhat accurate estimates of these sensitive attributes
across a larger population on which they might be used for com-
pletely orthogonal downstream tasks (e.g. sentiment analysis).
Finally, we also consider classic membership inference attacks
demonstrating the need to worry about leakage of training data
membership when given access to embedding models and their
outputs, as is common with many other ML models.
Our Contributions. As discussed above, we initiate a systematic
and broad study of three classes of potential information leak-
age in embedding models. We consider word and sentence em-
bedding models to show the viability of these attacks and demon-
strate their usefulness in improving our understanding of privacy
risks of embeddings. Additionally, we introduce new techniques
to attacks models, in addition to drawing inspiration from exist-
ing attacks to apply them to embeddings to various degrees. Our
results are demonstrated on widely-used word embeddings such
as Word2Vec [46], FastText [3], GloVe [54] and different sentence
embedding models including dual-encoders [39] and BERT [13, 35].
(1) We demonstrate that sentence embedding vectors encode
information about the exact words in input texts rather than merely
abstract semantics. Under scenarios involving black-box and white-
box access to embedding models, we develop several inversion
techniques that can invert the embeddings back to the words with
high precision and recall values (exceeding 60% each) demonstrating
that a significant fraction of the inputs may be easily recovered. We
note that these techniques might be of independent interest.
(2) We discover certain embeddings training frameworks in par-
ticular favor sensitive attributes leakage by showing that such em-
bedding models improve upon state-of-the-art stylometry tech-
niques [59, 65] to infer text authorship with very little training
data. Using dual encoder embeddings [39] trained with contrastive
learning framework [63] and 10 to 50 labeled sentences per author,
we show 1.5–3× fold improvement in classifying hundreds of au-
thors over prior stylometry methods and embeddings trained with
different learning paradigms.
(3) We show that membership inference attacks are still a viable
threat for embedding models, albeit to a lesser extent. Given our
results that demonstrate adversary can achieve a 30% improvement
on membership information over random guessing on both word
and sentence embeddings, we show that is prudent to consider
these potential avenues of leakage when dealing with embeddings.
(4) Finally, we propose and evaluate adversarial training tech-
niques to minimize the information leakage via inversion and sensi-
tive attribute inference. We demonstrate through experiments their
applicability to mitigating these attacks. We show that embedding
inversion attacks and attribute inference attacks against our adver-
sarially trained model go down by 30% and 80% respectively. These
come at a minor cost to utility indicating a mitigation that cannot
simply be attributed to training poorer embeddings.
Paper Organization. The rest of the paper is organized as fol-
lows. Section 2 introduces preliminaries needed for the rest of
the paper. Sections 3, 4, and 5 cover attacks against embedding
models—inversion attacks, sensitive attributes inference attacks,
and membership inference attacks respectively. Our experimen-
tal results and proposed defenses are covered in Sections 6 and 7
respectively, followed by related work and conclusions.
2 PRELIMINARIES
2.1 Embedding Models
Embedding models are widely used machine learning models that
map a raw input (usually discrete) to a low-dimensional vector.
The embedding vector captures the semantic meaning of the raw
input data and can be used for various downstream tasks including
nearest neighbor search, retrieval, and classification. In this work,
we focus on embeddings of text input data as they are widely used
in many applications and have been studied extensively in research
community [3, 5, 13, 33, 35, 39, 46, 54, 55, 58].
Word embeddings. Word embeddings are look-up tables that
map each word w from a vocabulary V to a vector vw ∈ Rd. Word
embedding vectors capture the semantic meaning of words in the
following manner: words with similar meanings will have small
cosine distance in the embedding space, the cosine distance of v1
and v2 defined as 1 − (v⊤
1 · v2/∥v1∥∥v2∥).
Popular word embedding models including Word2Vec [46], Fast-
Text [3] and GloVe [54] are learned in an unsupervised fashion on
a large unlabeled corpus. In detail, given a sliding window of words
C = [wb , ... , w0, ... , we] from the training corpus, Word2Vec and
FastText train to predict the context word wi given the center word
w0 by maximizing the log-likelihood log PV (wi|w0) where

PV (wi|w0) =
exp(v⊤
wi · vw0)
w ∈{wi }∪Vneg exp(v⊤
w · vw0)
(1)
for each wi ∈ C/{w0}. To accelerate training, the above probability
is calculated against Vneg ⊂ V, a sampled subset of words not in
C instead of the entire vocabulary. GloVe is trained to estimate the
co-occurrence count of wi and wj for all pairs of wi , wj ∈ C.
A common practice in modern deep NLP models is to use word
embeddings as the first layer so that discrete inputs are mapped
to a continuous space and can be used for later computation. Pre-
trained word embeddings is often used to initialize the weights
of the embedding layer. This is especially helpful when the down-
stream NLP tasks have a limited amount of labeled data as the
knowledge learned by these pre-trained word embeddings from a
large unlabeled corpus can be transferred to the downstream tasks.
Sentence embeddings. Sentence embeddings are functions that
map a variable-length sequence of words x to a fix-sized embed-
ding vector Φ(x) ∈ Rd through a neural network model Φ. For a
input sequence of ℓ words x = [w1, w2, ... , wℓ], Φ first maps x into
a sequence of word vectors X = [v1, ... , vℓ] with a word embed-
ding matrix V . Then Φ feeds X to a recurrent neural networks or
Transformer [70] and obtain a sequential hidden representation
[h1, h2, ... , hℓ] for each word in x. Finally Φ outputs the sentence
embedding by reducing the sequential hidden representation to a
single vector representation. Common reducing methods include
taking the last representation where Φ(x) = hℓ and mean-pooling
Sentence embedding models are usually trained with unsuper-
vised learning methods on a large unlabeled corpus. A popular archi-
tecture for unsupervised sentence embedding is the dual-encoder
model proposed in many prior works [5, 7, 26, 39, 40, 58, 75]. The
dual-encoder model trains on a pair of context sentences (xa, xb)
where the pair could be a sentence and its next sentence in the
same text or a dialog input and its response, etc. Given a randomly
sampled set of negative sentences Xneg that are not in the context of
xa, xb, the objective of training is to maximizes the log-likelihood
log PΦ(xb|xa,Xneg) where
PΦ(xb|xa,Xneg) =
where Φ(x) = (1/ℓ) ·ℓ
exp(Φ(xb)⊤ · Φ(xa))
x ∈{xb }∪Xneg exp(Φ(x)⊤ · Φ(xa)) .
i =1 hi.

(2)
Intuitively, the model is trained to predict the correct context xb
from the set {xb} ∪ Xneg when conditioned on xa. In other words,
the similarity between embeddings of context data is maximized
while that between negative samples is minimized.
Sentence embeddings usually outperform word embeddings on
transfer learning. For downstream tasks such as image-sentence
retrieval, classification, and paraphrase detection, sentence embed-
dings are much more efficient for the reason that only a linear
model needs to be trained using embeddings as the feature vectors.
Pre-trained language models. Language models are trained to
learn contextual information by predicting next words in input text,
and pre-trained language models can easily adopt to other NLP
tasks by fine-tuning. The recently proposed Transformer architec-
ture [70] enables language models to have dozen of layers with
huge capacity. These large language models, including BERT [13],
GPT-2 [55], and XLNet [76], are trained on enormous large corpus
and have shown impressive performance gains when transferred
to other downstream NLP tasks in comparison to previous state of
the art methods.