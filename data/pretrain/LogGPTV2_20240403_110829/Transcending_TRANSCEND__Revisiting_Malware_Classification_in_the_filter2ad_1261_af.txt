Information science and statistics. Springer, 2007.
[13] Y. Boshmaf, D. Logothetis, G. Siganos, J. Ler´ıa, J. Lorenzo, M. Ripeanu,
and K. Beznosov. Integro: Leveraging victim prediction for robust fake
account detection in OSNs. In Network and Distributed System Security
Symposium (NDSS), 2015.
[14] Q. Cao, M. Sirivianos, X. Yang, and T. Pregueiro. Aiding the detection
In USENIX
of fake accounts in large scale social online services.
Symposium on Networked Systems Design and Implementation (NSDI),
2012.
[15] C. Curtsinger, B. Livshits, B. G. Zorn, and C. Seifert. ZOZZLE: fast
and precise in-browser javascript malware detection. In USENIX Security
Symposium, 2011.
[16] S. K. Dash, G. Suarez-Tangil, S. J. Khan, K. Tam, M. Ahmadi, J. Kinder,
and L. Cavallaro. Droidscribe: Classifying android malware based on
In IEEE Security and Privacy Workshops (SPW),
runtime behavior.
2016.
[17] S. Edunov, M. Ott, M. Auli, and D. Grangier. Understanding back-
In Conference on Empirical Methods in Natural
translation at scale.
Language Processing (EMNLP), 2018.
[18] R. M. French and N. Chater. Using noise to compute error surfaces
in connectionist networks: A novel means of reducing catastrophic
forgetting. Neural Computation, 2002.
[19] J. H. Friedman. Greedy function approximation: a gradient boosting
machine. Annals of Statistics, 2001.
[20] R. Jordaney, K. Sharad, S. K. Dash, Z. Wang, D. Papini, I. Nouretdinov,
in malware
and L. Cavallaro.
classiﬁcation models. In USENIX Security Symposium, 2017.
Transcend: Detecting concept drift
[21] Z. Kan, F. Pendlebury, F. Pierazzi, and L. Cavallaro.
Investigating
labelless drift adaptation for malware detection. In ACM Workshop on
Artiﬁcial Intelligence and Security (AISec), 2021.
[22] A. Kantchelian, M. C. Tschantz, S. Afroz, B. Miller, V. Shankar,
R. Bachwani, A. D. Joseph, and J. D. Tygar. Better malware ground
In ACM
truth: Techniques for weighting anti-virus vendor labels.
Workshop on Artiﬁcial Intelligence and Security (AISec), 2015.
[23] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation
with deep convolutional neural networks. Commun. ACM, 2017.
[24] S. Kullback and R. A. Leibler. On information and sufﬁciency. Annals
of Mathematical Statistics, 1951.
[25] M. Lindorfer, M. Neugschwandtner, and C. Platzer. MARVIN: efﬁcient
and comprehensive mobile app classiﬁcation through static and dynamic
analysis. In IEEE Annual Computer Software and Applications Confer-
ence (COMPSAC), 2015.
[26] H. Linusson, U. Johansson, H. Bostr¨om, and T. L¨ofstr¨om. Classiﬁcation
with reject option using conformal prediction. In Paciﬁc-Asia Confer-
ence on Knowledge Discovery and Data Mining (PAKDD). Springer,
2018.
[27] B. Miller, A. Kantchelian, M. C. Tschantz, S. Afroz, R. Bachwani,
R. Faizullabhoy, L. Huang, V. Shankar, T. Wu, G. Yiu, A. D. Joseph,
and J. D. Tygar. Reviewer integration and performance measurement
In Conference on Detection of Intrusions and
for malware detection.
Malware & Vulnerability Assessment (DIMVA), 2016.
[28] J. G. Moreno-Torres, T. Raeder, R. Ala´ız-Rodr´ıguez, N. V. Chawla, and
F. Herrera. A unifying view on dataset shift in classiﬁcation. Pattern
Recognition, 2012.
[29] A. Narayanan, Y. Liu, L. Chen, and J. Liu. Adaptive and scalable
In International
android malware detection through online learning.
Joint Conference on Neural Network (IJCNN), 2016.
[30] A. Narayanan, M. Chandramohan, L. Chen, and Y. Liu. Context-
aware, adaptive, and scalable android malware detection through online
IEEE Transactions on Emerging Topics in Computational
learning.
Intelligence (TETCI), 2017.
[31] S. Nilizadeh, F. Labreche, A. Sedighian, A. Zand, J. M. Fernandez,
C. Kruegel, G. Stringhini, and G. Vigna. POISED: spotting twitter
In ACM Conference on Computer and
spam off the beaten paths.
Communications Security (CCS), 2017.
[32] H. Papadopoulos. Inductive conformal prediction: Theory and applica-
tion to neural networks. In Tools in Artiﬁcial Intelligence. 2008.
[33] N. Papernot and P. D. McDaniel. Deep k-nearest neighbors: To-
CoRR,
interpretable and robust deep learning.
wards conﬁdent,
abs/1803.04765, 2018.
[34] F. Pendlebury. Machine Learning for Security in Hostile Environments.
PhD thesis, University of London, 2021.
[35] F. Pendlebury, F. Pierazzi, R. Jordaney, J. Kinder, and L. Cavallaro.
TESSERACT: eliminating experimental bias in malware classiﬁcation
across space and time. In USENIX Security Symposium, 2019.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:22:16 UTC from IEEE Xplore.  Restrictions apply. 
819
[36] F. Pierazzi, F. Pendlebury, J. Cortellazzi, and L. Cavallaro. Intriguing
In IEEE
properties of adversarial ML attacks in the problem space.
Symposium on Security and Privacy (S&P), 2020.
[37] Plato. The Symposium.
c. 385–370 BC. Penguin Classics edition
published 1999, translated by Christopher Gill.
[38] G. Shafer and V. Vovk. A tutorial on conformal prediction. Journal of
Machine Learning Research (JMLR), 2008.
[39] A. Sotgiu, A. Demontis, M. Melis, B. Biggio, G. Fumera, X. Feng, and
F. Roli. Deep neural rejection against adversarial examples. EURASIP
Journal on Information Security, 2020.
[40] N. Srndic and P. Laskov. Detection of malicious PDF ﬁles based on
In Network and Distributed System
hierarchical document structure.
Security Symposium (NDSS), 2013.
[41] N. Srndic and P. Laskov. Hidost: a static machine-learning-based
detector of malicious ﬁles. EURASIP Journal on Information Security,
2016.
[42] G. Suarez-Tangil, J. E. Tapiador, P. Peris-Lopez, and J. B. Al´ıs. Den-
droid: A text mining approach to analyzing and classifying code struc-
tures in android malware families. Expert Systems With Applications,
2014.
[43] G. Suarez-Tangil, S. K. Dash, M. Ahmadi, J. Kinder, G. Giacinto, and
L. Cavallaro. Droidsieve: Fast and accurate classiﬁcation of obfuscated
android malware. In ACM Conference on Data and Applications Security
and Privacy (CODASPY), 2017.
[44] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. J.
Intriguing properties of neural networks.
Goodfellow, and R. Fergus.
In ICLR (Poster), 2014.
[45] L. Tong, B. Li, C. Hajaj, C. Xiao, N. Zhang, and Y. Vorobeychik.
Improving robustness of ML classiﬁers against realizable evasion attacks
using conserved features. In USENIX Security Symposium, 2019.
[46] V. Vovk. Conditional validity of inductive conformal predictors. Journal
of Machine Learning Research (JMLR), 2013.
[47] V. Vovk, A. Gammerman, and G. Shafer. Algorithmic learning in a
random world. Springer-verlag New York Inc., 2010.
[48] V. Vovk, I. Nouretdinov, V. Manokhin, and A. Gammerman. Cross-
conformal predictive distributions. In Workshop on Conformal Predic-
tion and its Applications (COPA), 2018.
[49] S. Xi, S. Yang, X. Xiao, Y. Yao, Y. Xiong, F. Xu, H. Wang, P. Gao,
Z. Liu, F. Xu, and J. Lu. Deepintent: Deep icon-behavior learning
In ACM
for detecting intention-behavior discrepancy in mobile apps.
Conference on Computer and Communications Security (CCS), 2019.
[50] K. Xu, Y. Li, R. H. Deng, K. Chen, and J. Xu. Droidevolver:
In IEEE European
Self-evolving android malware detection system.
Symposium on Security and Privacy (EuroS&P), 2019.
[51] L. Yang, W. Guo, Q. Hao, A. Ciptadi, A. Ahmadzadeh, X. Xing, and
G. Wang. CADE: detecting and explaining concept drift samples for
security applications. In USENIX Security Symposium, 2021.
[52] W. Yang, X. Xiao, B. Andow, S. Li, T. Xie, and W. Enck. Appcon-
text: Differentiating malicious and benign mobile app behaviors using
context. In International Conference on Software Engineering (ICSE),
2015.
[53] X. Zhang, Y. Zhang, M. Zhong, D. Ding, Y. Cao, Y. Zhang, M. Zhang,
and M. Yang. Enhancing State-of-the-Art Classiﬁers with API Semantics
to Detect Evolved Android Malware. In ACM Conference on Computer
and Communications Security (CCS), 2020.
A. Symbol Table
APPENDIX
Table IV reports the major symbols and abbreviations used
throughout the paper.
B. Additional CP-Reject Results
In §VI-F we demonstrate how TRANSCENDENT can apply
to other classiﬁers and domains, comparing the performance
of an ICE using credibility against using probabilities alone,
for both PE and PDF malware (Figures 9 and 10). Here we
show in Figure 11 additional results to compare against the
prior rejection approach CP-Reject (we exclude DroidEvolver
as it is speciﬁc to Android malware). Similar to the results on
TABLE IV: Table of symbols and abbreviations.
SYMBOL
X
Y
z
z∗
ˆy
az
pz
py
z
τy
T
B
d
ˆz
A
S
g
ε
NCM
TCE
ICE
CCE
Bag of examples(cid:72)z1, z2, ..., zn(cid:73).
DESCRIPTION
Feature space X ⊆ Rn.
Label space.
Example pair (x, y) ∈ X × Y.
Previously unseen test example.
Predicted class g(z∗).
Nonconformity score output by an NCM for z.
Statistical p-value for z.
Statistical p-value for z, calculated with respect to
class y ∈ Y (used in label conditional calculations).
A rejection threshold τy ∈ [0, 1] for class y ∈ Y.
The set of all per-class rejection thresholds { τy ∈
[0, 1] | y ∈ Y }.
Distance function d(z, z(cid:48)).
Point predictor ˆz(B).
Nonconformity measure (NCM) usually composed
of a distance function and point predictor.
Collection of nonconformity scores computed in
elements of B, relative to other elements in B,
Classiﬁer g : X −→ Y that assigns object x ∈ X to
class y ∈ Y. Also known as the decision function.
Signiﬁcance level used in conformal prediction to
deﬁne prediction region with conﬁdence guarantees.
Nonconformity measure.
Transductive Conformal Evaluator.
Inductive Conformal Evaluator.
Cross-Conformal Evaluator.
S =(cid:72)A(B \(cid:72)z(cid:73), z) : z ∈ B(cid:73).
(a) EMBER Windows PE malware [3]
(b) Hidost PDF malware [41]
Fig. 11: F1-Score of CP-Reject [26] on alternative malware datasets.
the Android dataset (§VI-E), the overall ability for CP-Reject
to distinguish between drifting and non-drifting points is poor
on the PE malware dataset. For the PDF malware dataset,
which exhibits much less drift, CP-Reject is signiﬁcantly more
effective, which supports the hypothesis that it is the violation
of conformal prediction’s exchangeability assumption which
results in the lower performance on the Android and PE
datasets. Nevertheless, TRANSCENDENT with credibility (and
even probabilities) outperforms CP-Reject in this setting also
(cf. Figure 10).
C. Full Vanilla TCE on EMBER Subset
A full scale comparison to the original TCE is not possible
due to its computational complexity—recall that one classiﬁer
must be trained for each example in the training set. However,
it is informative to perform a small-scale experiment as there
may be settings where the vanilla TCE is viable, and we wish
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:22:16 UTC from IEEE Xplore.  Restrictions apply. 
820
1234567Testingperiod(month)0:00:10:20:30:40:50:60:70:80:91:0F1(baseline)F1(kept)F1(rejected)Quarantined147101316192225283134Testingperiod(day)0:00:10:20:30:40:50:60:70:80:91:0TABLE V: AUT(F1, 7m) comparing vanilla TCE to our novel conformal
evaluators on Windows PE malware data. To be computationally viable, 10%
of the training data was randomly sampled to use for training and calibration.
Baseline
Kept Elements
Rejected Elements
TCE
0.68
0.97
0.00
Approx-TCE
0.70
0.97
0.00
ICE
0.45
0.94
0.00
CCE
0.69
1.00
0.21
be undeﬁned for kept elements. As more folds of the CCE are
required to agree with each other before a decision is accepted,
the CCE will reject more elements. If less folds are required,
more elements will be accepted. Similarly, the quality of the
rejection lessens: more elements are rejected on which the
underlying classiﬁer would not have made a mistake. Tuning
the majority vote conditions on the calibration set can help ﬁnd
the sweet spot between the performance of kept elements, and
the quality—and volume—of rejections.
E. Guidance for Choosing Calibration Constraints
Fig. 12: F1-Score of an ICE optimized to ﬁnd calibration thresholds that
minimize the rejection rate with F1-Score no less than 0.8. These settings
keep the rejection rate low (below 10%) while sacriﬁcing the F1 performance
on kept elements (cf. Figure 7b).
to ensure that there is no signiﬁcant performance difference
between vanilla TCE and our novel conformal evaluators.
We perform an experiment on the Windows PE malware
dataset, where 10% of the training data is randomly sampled
to use for training and calibrating the evaluators (this is the
largest subsample we can take given our resource constraints).
We choose the PE dataset over the Android dataset due to
the high dimensionality of the Android feature space that may
cause instability when the number of examples is very low, and
over the PDF dataset which is relatively stationary and may
make it harder to discern performance differences between
the different evaluators. One caveat of this subsampling is the
reduced performance of the baseline for the ICE, which is
due to the reduced data available to the proper training set,
although TRANSCENDENT appears unaffected by this.
Table V summarizes the F1 performance over the seven
month-long test periods using the area-under-time (AUT)
metric [35]. The performance difference between TCE and our
evaluators in terms of distinguishing between drifting and non-
drifting examples is negligible, shown by the very high AUT
of kept elements and very low AUT of rejected elements. That
is, there is little to no performance sacriﬁce when using our
evaluators over the vanilla TCE. The overall trends otherwise
follow those in our main Android experiments (cf. Figure 7).
D. Analysis of CCE Tuning
Here we revisit the majority vote conditions for the CCE
applied to the Android malware dataset in §VI-B. The size of
the quorum for the CCE affects how conservative the CCE is in
accepting test examples. Figure 13 shows the performance over
time summarized using the AUT metric for F1 (a), Precision
(b), and Recall (c). Note that Figure 13 omits the setting where
the majority vote must be unanimous, as the CCE eventually
rejects every example—causing F1, Precision, and Recall to
In §V-D we formally describe the threshold calibration as
an optimization problem in which one metric of interest is
maximized or minimized given constraints on another metric.
Throughout our evaluation we focus on maximizing the F1
of kept elements, while keeping a reasonably low rejection
rate. We choose 15% after taking into account the size of our
dataset and using guidance from Miller et al. [27] to estimate
a reasonable labeling capacity.
Recall that the calibration constraints are with respect to the
calibration set which ideally exhibits minimal drift. It is clear
from our evaluation that as concept drift becomes more severe