ΔN
of DN
log Prob{DN
0 , ΔN
0 } = − N(cid:2)
i=0
(Di − Δi)2
2vU
(Δi − βΔi−1 − ¯w)2
log vU − N(cid:2)
− N + 1
2
i=1
log vW − (Δ0 − w0)2
− N
2
− (N + 1) log 2π.
2p0
2vW
− 1
2
log p0
In a Maximum likelihood setting, we wish to ﬁnd the values for
the parameters that will maximize the above log-likelihood assum-
ing that the sequence DN
0 has been observed. However as the se-
quence of system state ΔN
0 has not been observed, this maximiza-
tion is not tractable directly and we have to apply the Expectation
Maximization method. This method transforms the maximization
of the above likelihood function with unobserved system state se-
quence ΔN
0 to an iteration of successive steps where the system
state sequence is assumed to be known and the parameters can be
obtained through maximization of the likelihood function.
Each iteration of the EM method consists therefore of two steps.
In the ﬁrst step, we compute the expectation (over all possible val-
ues of the sequence of states ΔN
0 ) of the log-likelihood, given the
observed values of Dn and assume that the parameter values are
equal to θ(k). In a second step, the parameters θ(k+1) are chosen
so as to maximize the previously obtained likelihood expectation.
Next we explain these two operations with some further details.
Let the superscript (k) indicate the value of any parameter at
the kth step of the EM algorithm. As explained before, in the EM
method, we need to estimate the value of the unobserved system
states to be able to calculate the overall likelihood to maximize.
The variables ˆδ(k)
are in fact those estimates at iteration k and ˆπ(k)
and ˆπ(k)
i,i−1 are the estimation error variances of this sequence of
states:
i
i
i = E[Δi|DN
ˆδ(k)
0 , θ(k)
], ˆπ(k)
i = E[Δ
2
i|DN
0 , θ(k)
],
i,i−1 = E[ΔiΔi−1|DN
ˆπ(k)
0 , θ(k)
].
Expectation step.
sured values DN
¯L(θ, θ(k)
The expected value of log-likelihood knowing the set of mea-
0 and the parameter θ(k) is given by :
) = E[log Prob{DN
= − N(cid:2)
0 , θ(k)
]
0 }|DN
i + ˆπ(k)
i
0 , ΔN
i − 2Di ˆδ(k)
D2
2vU
log vU − N(cid:2)
i=1
ˆπ(k)
i + β2 ˆπ(k)
i−1 + ¯w2
2vW
i=0
2
− N + 1
− N(cid:2)
i=1
− N
2
− 1
2
β ˆπ(k)
i,i−1 + ˆδ(k)
i
vW
¯w + βˆδ(k)
i−1 ¯w
log vW − p0 − 2ˆδ(k)
0 w0 + w2
2p0
log p0 − (N + 1) log 2π.
0
By replacing θ by its value at the kth step of the EM algorithm,
we obtain ˆδ(k)
i,i−1, which gives the expected log-
likelihood at the (k + 1)th step. Interested readers can ﬁnd details
of the computations of these values in [17].
and ˆπ(k)
, ˆπ(k)
i
i
Maximization step.
In this step, the parameter vector at step (k + 1) is chosen to
maximize the expected log-likelihood. This is done by solving the
equation
∂ ¯L(θ, θ(k))
∂θ
= 0.
This results in the following set of equations:
⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
0
0
N +1
w(k+1)
p(k+1)
0
v(k+1)
U
¯w(k+1) =
β(k+1) =
v(k+1)
W
−2β(k+1) ˆπ(k)
= ˆδ(k)
0 − (ˆδ(k)
= p(k)
(cid:7)N
0 )2
i − 2Di ˆδ(k)
(cid:7)N
i + β(k+1) (cid:7)N
i=0 D2
= 1
ˆδ(k)
(cid:7) N
i,i−1+ ¯w(k+1) (cid:7) N
(k)
(cid:7) N
(cid:7)N
i=1 ˆπ
i + (β(k+1))2 ˆπ(k)
i=1 ˆπ(k)
i,i−1 − 2ˆδ(k)
i + ˆπ(k)
i
ˆδ(k)
i−1
i=1
(k)
ˆδ
i−1
i=1
i=1 ˆπ
= 1
N
(k)
i−1
i=1
i
¯w(k+1) − 2β(k+1) ˆδ(k)
i−1 + ( ¯w(k+1))2
i−1 ¯w(k+1).
By solving this set of equations we can obtain the vector θ(k+1),
then we iterate with the expectation calculation as described above.
We note that the complexity of the approach lies in the linear
state space modeling phase by EM algorithm that incurs a num-
ber of iterations over N dimensional vectors, which is well within
the capability of modern computers. We will see later that this
phase has to be run on a subset of nodes (the Surveyors). On the
other hand, predicting relative errors using the Kalman ﬁlter (sec-
tion 2.1), which occurs on every node, only implies a few simple
scalar operations and is negligible in terms of required computing
power.
√
Finally, because we expect each of the innovation observation
ηn to be inside a conﬁdence interval of ±2
vη,n (where vη,n is
the variance of the innovation process at time n) with a probability
higher than 95%, when a Kalman ﬁlter yields 10 consecutive in-
novation observations outside such conﬁdence interval, the ﬁlter is
re-calibrated by re-applying the calibration procedure described in
this section. Re-calibration is likely to occur following a signiﬁcant
change in the corresponding node’s coordinates, caused by changes
in network conditions.
3. VALIDATION
To validate our model, we conducted simulations and PlanetLab
experiments for both Vivaldi [5] and NPS [4]. Vivaldi is a promi-
nent representative of purely peer-to-peer-based (i.e. without in-
frastructure support) positioning systems, where the system tries
to minimize the potential energy of virtual springs between nodes,
while NPS is typical of infrastructure-based systems, where a hier-
archy of landmarks and reference points governs the positioning of
nodes.
As the goal of this section is to assess the ﬁtness of the pro-
posed model to represent the normal behavior of the embedding
processes, all results presented were acquired in a clean environ-
ment with no malicious node. While the goal of the simulation
studies is to assess our results for large scale coordinate-based sys-
tems, the PlanetLab experiments aim to show their applicability in
real-world conditions.
The PlanetLab experiments were conducted over a set of 280
PlanetLab nodes spread world-wide. In this paper, we discuss a
representative set of experimental results conducted over several
days in December 2006.
The simulations were driven by a matrix of inter-host Internet
RTTs (the “King” dataset) to model latencies based on real world
measurements. This dataset contains the pair-wise RTTs between
1740 Internet DNS servers collected using the King method [18]
and was used to generate a topology with 1740 overlay nodes.
In the case of Vivaldi, each node had 64 neighbors (i.e. was at-
tached to 64 springs), 32 of which being chosen to be closer than 50
ms. The constant fraction Cc for the adaptive timestep (see [5] for
details) is set to 0.25. A 2-dimensional coordinate space augmented
with a height vector was used.
For NPS, we considered an 8-dimensional Euclidean space for
the embedding. We used an NPS positioning hierarchy with 4 lay-
ers. The top layer had a set of 20 well separated permanent land-
marks. Each subsequent layer then had 20% of nodes randomly
chosen as reference points. The security mechanism already pro-
posed in NPS, shown to be too primitive in [11], was turned on and
its sensitivity was set to 4 (see [4] for details).
When needed, Surveyor nodes were chosen at random2.
2Note that in NPS, all permanent landmarks also act as Surveyors.
3.1 Assumption Validation
In section 2, the assumption that the system error Wn follows a
gaussian distribution was made. This is fundamental to the appli-
cability of the Kalman ﬁlter framework. Every node calibrated its
own Kalman ﬁlter based on the observation of its own embedding,
and we checked this assumption by applying the Lillie test [19], a
robust version of the well known kolmogoroff-Smirnov goodness-
of-ﬁt test, to whitened ﬁlter inputs. We observed that the Lillie
test leads to only 14 gaussian ﬁtting rejections in simulations (over
1720 samples) and 5 rejections in PlanetLab (over 260 samples).
This test allows us to conclude that the hypothesis we took for the
Kalman model is valid. In addition, we plot in ﬁgure 1 the Quantile-
r
o
r
r
E
e
v
i
t
l
a
e
R
t
t
u
p
u
O
1
0.8
0.6
0.4
0.2
0
0   
0.2
0.15
0.1
0.05
r
o
r
r
E
n
o
i
i
t
c
d
e
r
P
0
0   
Kalman filter response: Estimation vs Actual
Measured Relative Error
Predicted Relative Error
60
120
180
240
300
360
420
480
60
120
180
240
Time (mn)
300
360
420
480
0.4
Figure 2: Prediction errors (PlanetLab node).
0.2
0.15
0.1
0.05
0
−0.05
−0.1
−0.15
l
s
e
p
m
a
S
t
u
p
n
I
f
o
e
l
i
t
n
a
u
Q
−0.2
−4
−3
0.3
0.2
l
s