context information.
capabilities. Thus, this method is the lack of backward com-
patibility. Another approach is to use data obfuscation, e.g.,
obfuscating the commands, model commands, and parame-
ters. However, this method requires kernels to be extended to
deobfuscate the data back or understand the obfuscated data.
Besides, this method can only increase the bar but cannot
prevent the Hermes attack completely.
Besides encryption and obfuscation, another mechanism is
adding noise from the software aspect, e.g., sending data in
one process but sending interference commands from a differ-
ent process. However, this could be resolved by utilizing GPU
channels, as discussed in Section 5.2. Another alternative
solution is to leverage the device driver to use dynamic com-
mand headers instead of static command ones, signiﬁcantly
increasing the bar of reverse engineering.
The last possible defense mechanism is to ofﬂoad some
tasks to the CPU. In this way, it can reduce the information
obtained from the PCIe trafﬁc. Unfortunately, it will result in
signiﬁcant performance loss due to the frequent data transfer
between CPU and GPU and CPU’s low computing power
compared to GPU.
5.3 Mitigation Countermeasures
The ﬁrst possible defense approach is to encrypt the PCIe
trafﬁc. It is easy to add the crypto engine on the CPU side,
but it is hard for the commodity GPUs that do not have such
6 Related Work
Adversarial Examples: Adversarial examples are ﬁrst
pointed out by Szegedy et al. [48], which are able to cause
the network to misclassify an image. They proposed the L-
USENIX Association
30th USENIX Security Symposium    1985
BFGS approach to generate adversarial examples by applying
a certain imperceptible perturbation, which is found by maxi-
mizing the network’s prediction. Afterward, there has been a
lot of work concentrating on the adversarial attack, some of
them is white-box attack [5, 6, 32, 48], that the attacker has
some prior knowledge of the internal architecture or param-
eters of the victim model, some of the attacks are black-box
attack [4, 7, 8, 10, 40, 41, 44].
Extraction Attack: Table 4.5 summarized some other DNN
model extraction attacks and compared them with our work.
[23] proposed an attack by hearing the memory bus and PCIe
hints, built a classiﬁer to predict the DNN model architec-
ture, [58] introduced a cache-based side-channel attack to
steal DNN architectures, [24] performed a side-channel at-
tack to reveal the network architecture and weights of a CNN
model based on memory access patterns and the input/output
of the accelerator, [55] revealed the internal network archi-
tecture and estimated the parameters by analyzing the power
trace. Similarly, [53] presented an attack on an FPGA-based
convolutional neural network accelerator and recovered the
input image from the collected power traces. [18] proposed
an extraction attack by exploiting the side timing channels
to infer the depth of the network. [51] designed an attack on
stealing the hyper-parameters of a variety of machine learn-
ing algorithms, this attack is derived by know parameters and
the machine learning algorithms, and training data set. [25]
demonstrates an attack that predicts the image classify results
by observing the GPU kernel execution time. [43] assumed
the model architecture is known, and the softmax layer is
accessible, then proved noise input is enough to replicate the
parameters of the original model. [46] designed a membership
inference attack to determine the training datasets based on
prediction outputs of machine learning models. [50] investi-
gated the extraction attack on various cloud-based ML model
rely on the outputs returned by the ML prediction APIs. Sim-
ilarly, some works generated a clone model from the query-
prediction pairs of the victim model. [27, 38, 39, 46, 50].
7 Conclusion
In this paper, we identiﬁed the PCIe bus as a new attack sur-
face to leak DNN models. Based on this new attack surface,
we proposed a novel model-extraction attack, named Hermes
Attack, which is the ﬁrst attack to fully steal the whole DNN
models. We addressed the main challenges by a large number
of reverse engineering and reliable semantic reconstruction,
as well as skillful packet selection and order correction. We
implemented a prototype of the Hermes Attack, and evaluated
it on three real-world NVIDIA GPU platforms. The evalua-
tion results indicate that our scheme could handle customized
DNN models and the stolen models had the same inference ac-
curacy as the original ones. We will open-source these reverse
engineering results, hoping to beneﬁt the entire community.
References
[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng
Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, San-
jay Ghemawat, Geoffrey Irving, Michael Isard, et al.
Tensorﬂow: A system for large-scale machine learning.
In 12th {USENIX} Symposium on Operating Systems
Design and Implementation ({OSDI} 16), pages 265–
283, 2016.
[2] Baidu.
Baidu AI Open
Platform, 2019.
https://ai.baidu.com/solution/private?hmsr=
aibanner&hmpl=private.
[3] Baidu. Baidu Apollo Open Platform, 2019. http://
apollo.auto/developer.html.
[4] Arjun Nitin Bhagoji, Warren He, Bo Li, and Dawn Song.
Exploring the space of black-box attacks on deep neural
networks. arXiv preprint arXiv:1712.09491, 2017.
[5] Battista Biggio, Igino Corona, Davide Maiorca, Blaine
Nelson, Nedim Šrndi´c, Pavel Laskov, Giorgio Giacinto,
and Fabio Roli. Evasion attacks against machine learn-
ing at test time. In Joint European conference on ma-
chine learning and knowledge discovery in databases,
pages 387–402. Springer, 2013.
[6] Nicholas Carlini and David Wagner. Towards evaluat-
ing the robustness of neural networks. In 2017 IEEE
Symposium on Security and Privacy (SP), pages 39–57.
IEEE, 2017.
[7] Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi,
and Cho-Jui Hsieh. Zoo: Zeroth order optimization
based black-box attacks to deep neural networks without
training substitute models. In Proceedings of the 10th
ACM Workshop on Artiﬁcial Intelligence and Security,
pages 15–26, 2017.
[8] Minhao Cheng, Thong Le, Pin-Yu Chen, Jinfeng Yi,
Huan Zhang, and Cho-Jui Hsieh. Query-efﬁcient hard-
label black-box attack: An optimization-based approach.
arXiv preprint arXiv:1807.04457, 2018.
[9] Dan Cire¸san, Ueli Meier, and Jürgen Schmidhuber.
Multi-column deep neural networks for image classi-
ﬁcation. arXiv preprint arXiv:1202.2745, 2012.
[10] Moustapha Cisse, Yossi Adi, Natalia Neverova, and
Joseph Keshet. Houdini: Fooling deep structured predic-
tion models. arXiv preprint arXiv:1707.05373, 2017.
[11] Ronan Collobert and Jason Weston. A uniﬁed archi-
tecture for natural language processing: Deep neural
networks with multitask learning. In Proceedings of
the 25th international conference on Machine learning,
pages 160–167. ACM, 2008.
1986    30th USENIX Security Symposium
USENIX Association
[12] NVIDIA Corporation. Cuda llvm compiler. https:
//developer.nvidia.com/cuda-llvm-compiler.
[13] NVIDIA
Corporation.
user’s
https://docs.nvidia.com/pdf/
Proﬁler
guide.
CUDA_Profiler_Users_Guide.pdf.
[14] Victor Costan and Srinivas Devadas. Intel sgx explained.
[15] Henggang Cui, Hao Zhang, Gregory R Ganger, Phillip B
Gibbons, and Eric P Xing. Geeps: Scalable deep learn-
ing on distributed gpus with a gpu-specialized parameter
server. In Proceedings of the Eleventh European Con-
ference on Computer Systems, pages 1–16, 2016.
[16] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In 2009 IEEE conference on computer
vision and pattern recognition, pages 248–255. Ieee,
2009.
[17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. Bert: Pre-training of deep bidirec-
tional transformers for language understanding. arXiv
preprint arXiv:1810.04805, 2018.
[18] Vasisht Duddu, Debasis Samanta, D Vijay Rao, and
Valentina E Balas. Stealing neural networks via tim-
ing side channels. arXiv preprint arXiv:1812.11720,
2018.
[19] Envytools.
blob driver.
envytools.
Tools for people envious of nvidia’s
https://github.com/envytools/
[20] Alex Graves, Abdel-rahman Mohamed, and Geoffrey
Hinton. Speech recognition with deep recurrent neural
networks. In 2013 IEEE international conference on
acoustics, speech and signal processing, pages 6645–
6649. IEEE, 2013.
[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision
and pattern recognition, pages 770–778, 2016.
[22] Geoffrey Hinton, Li Deng, Dong Yu, George Dahl,
Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Se-
nior, Vincent Vanhoucke, Patrick Nguyen, Brian Kings-
bury, et al. Deep neural networks for acoustic modeling
in speech recognition. IEEE Signal processing maga-
zine, 29, 2012.
[23] Xing Hu, Ling Liang, Lei Deng, Shuangchen Li, Xin-
feng Xie, Yu Ji, Yufei Ding, Chang Liu, Timothy Sher-
wood, and Yuan Xie. Neural network model extraction
attacks in edge devices by hearing architectural hints.
arXiv preprint arXiv:1903.03916, 2019.
[24] Weizhe Hua, Zhiru Zhang, and G Edward Suh. Reverse
engineering convolutional neural networks through side-
In 2018 55th ACM/ES-
channel information leaks.
DA/IEEE Design Automation Conference (DAC), pages
1–6. IEEE, 2018.
[25] Tyler Hunt, Zhipeng Jia, Vance Miller, Ariel Szekely,
Yige Hu, Christopher J Rossbach, and Emmett Witchel.
Telekine: Secure computing with cloud gpus. In 17th
{USENIX} Symposium on Networked Systems Design
and Implementation ({NSDI} 20), pages 817–833, 2020.
[26] JD. JD AI Open Platform, 2019. http://jddoversea-
neuhub.jd.com/index.html.
[27] Sanjay Kariyappa, Atul Prakash, and Moinuddin
Qureshi. Maze: Data-free model stealing attack us-
ing zeroth-order gradient estimation. arXiv preprint
arXiv:2005.03161, 2020.
[28] Shinpei Kato, Eijiro Takeuchi, Yoshio Ishiguro, Yoshiki
Ninomiya, Kazuya Takeda, and Tsuyoshi Hamada. An
open approach to autonomous vehicles. IEEE Micro,
35(6):60–68, 2015.
[29] Keras. Guide to the Functional API, 2019. https:
//keras.io/getting-started/functional-api-
guide/.
[30] Keras. Keras Applications, 2019. https://keras.io/
api/applications/.
[31] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.
The cifar-10 dataset. online: http://www. cs. toronto.
edu/kriz/cifar. html, 55, 2014.
[32] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Ad-
versarial examples in the physical world. arXiv preprint
arXiv:1607.02533, 2016.
[33] Teledyne LeCroy.
Protocol Analyzer
- Teledyne LeCroy, 2019.
Express
//teledynelecroy.com/protocolanalyzer/pci-
express.
- PCI
https:
[34] Teledyne LeCroy. Summit T34 Analyzer, 2019. https:
//teledynelecroy.com/protocolanalyzer/pci-
express/summit-t34-analyzer.
[35] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
Deep learning. nature, 521(7553):436–444, 2015.
[36] Yann LeCun, Corinna Cortes, and CJ Burges. Mnist
handwritten digit database. 2010.
[37] CUDA Nvidia. Nvidia cuda c programming guide.
Nvidia Corporation, 120(18):8, 2011.
USENIX Association
30th USENIX Security Symposium    1987
[38] Seong Joon Oh, Bernt Schiele, and Mario Fritz. Towards
reverse-engineering black-box neural networks. In Ex-
plainable AI: Interpreting, Explaining and Visualizing
Deep Learning, pages 121–144. Springer, 2019.
[39] Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz.
Knockoff nets: Stealing functionality of black-box mod-
els. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 4954–4963,
2019.
[40] Nicolas Papernot, Patrick McDaniel, and Ian Goodfel-
low. Transferability in machine learning: from phe-
nomena to black-box attacks using adversarial samples.
arXiv preprint arXiv:1605.07277, 2016.
[41] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow,
Somesh Jha, Z Berkay Celik, and Ananthram Swami.
Practical black-box attacks against machine learning. In
Proceedings of the 2017 ACM on Asia conference on
computer and communications security, pages 506–519,
2017.
[42] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei
Li, and Peter J. Liu. Exploring the limits of transfer
learning with a uniﬁed text-to-text transformer, 2019.
[43] Nicholas Roberts, Vinay Uday Prabhu, and Matthew
McAteer. Model weight theft with just noise inputs:
The curious case of the petulant attacker. arXiv preprint
arXiv:1912.08987, 2019.
[44] Sayantan Sarkar, Ankan Bansal, Upal Mahbub, and
Rama Chellappa. Upset and angri: breaking high
arXiv preprint
performance image classiﬁers.
arXiv:1707.01159, 2017.
[45] Jürgen Schmidhuber. Deep learning in neural networks:
An overview. Neural networks, 61:85–117, 2015.
[46] Reza Shokri, Marco Stronati, Congzheng Song, and Vi-
taly Shmatikov. Membership inference attacks against
machine learning models. In 2017 IEEE Symposium on
Security and Privacy (SP), pages 3–18. IEEE, 2017.
[47] Karen Simonyan and Andrew Zisserman. Very deep con-
volutional networks for large-scale image recognition.
arXiv preprint arXiv:1409.1556, 2014.
[48] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever,
Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob
Fergus. Intriguing properties of neural networks. arXiv
preprint arXiv:1312.6199, 2013.
[49] Tesla. Tesla: Future of driving, 2019.
https://
www.tesla.com/autopilot.
[50] Florian Tramèr, Fan Zhang, Ari Juels, Michael K Re-
iter, and Thomas Ristenpart. Stealing machine learning
models via prediction apis. In 25th {USENIX} Security
Symposium ({USENIX} Security 16), pages 601–618,
2016.
[51] Binghui Wang and Neil Zhenqiang Gong. Stealing
hyperparameters in machine learning. In 2018 IEEE
Symposium on Security and Privacy (SP), pages 36–52.
IEEE, 2018.
[52] Waymo. Waymo: The world’s most experienced driver,
2019. https://waymo.com/tech/.
[53] Lingxiao Wei, Bo Luo, Yu Li, Yannan Liu, and Qiang
Xu. I know what you see: Power side-channel attack on
convolutional neural network accelerators. In Proceed-
ings of the 34th Annual Computer Security Applications
Conference, pages 393–406. ACM, 2018.
[54] Wikipedia. Hermes. https://en.wikipedia.org/
wiki/Hermes.
[55] Yun Xiang, Zhuangzhi Chen, Zuohui Chen, Zebin Fang,
Haiyang Hao, Jinyin Chen, Yi Liu, Zhefu Wu, Qi Xuan,
and Xiaoniu Yang. Open dnn box by power side-channel
attack, 2019.
[56] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude
Oliva, and Antonio Torralba. Sun database: Large-scale
scene recognition from abbey to zoo. In 2010 IEEE
Computer Society Conference on Computer Vision and
Pattern Recognition, pages 3485–3492. IEEE, 2010.
[57] Junyuan Xie, Linli Xu, and Enhong Chen. Image denois-
ing and inpainting with deep neural networks. In Ad-
vances in neural information processing systems, pages
341–349, 2012.
[58] Mengjia Yan, Christopher W Fletcher, and Josep Tor-
rellas. Cache telepathy: Leveraging shared resource at-
tacks to learn {DNN} architectures. In 29th {USENIX}
Security Symposium ({USENIX} Security 20), pages
2003–2020, 2020.
1988    30th USENIX Security Symposium
USENIX Association