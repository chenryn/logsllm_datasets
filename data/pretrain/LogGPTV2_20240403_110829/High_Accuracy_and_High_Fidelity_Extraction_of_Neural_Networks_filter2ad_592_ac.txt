1350    29th USENIX Security Symposium
USENIX Association
Architecture
Data Fraction
ImageNet
WSL
WSL-5
ImageNet + Rot
WSL + Rot
WSL-5 + Rot
Resnet_v2_50
Resnet_v2_200
Resnet_v2_50
Resnet_v2_200
10%
10%
100%
100%
(81.86/82.95)
(83.50/84.96)
(92.45/93.93)
(93.70/95.11)
(82.71/84.18)
(84.81/86.36)
(93.00/94.64)
(94.26/96.24)
(82.97/84.52)
(85.00/86.67)
(93.12/94.87)
(94.21/95.85)
(82.27/84.14)
(85.10/86.29)
N/A
N/A
(82.76/84.73)
(86.17/88.16)
N/A
N/A
(82.84/84.59)
(86.11/87.54)
N/A
N/A
Table 2: Extraction attack (top-5 accuracy/top-5 ﬁdelity) of the WSL model [28]. Each row contains an architecture and fraction
of public ImageNet data used by the adversary. ImageNet is a baseline using only ImageNet labels. WSL is an oracle returning
WSL model probabilities. WSL-5 is an oracle returning only the top 5 probabilities. Columns with (+ Rot) use rotation loss
on unlabeled data (rotation loss was not run when all data is labeled). An adversary able to query WSL always improves over
ImageNet labels, even when given only top 5 probabilities. Rotation loss does not signiﬁcantly improve the performance on
ResNet_v2_50, but provides a (1.36/1.80) improvement for ResNet_v2_200, comparable to the performance boost given by
WSL labels on 10% data. In the high-data regime, where we observe a (0.56/1.13) improvement using WSL labels.
Dataset
Algorithm
250 Queries
1000 Queries
4000 Queries
SVHN
SVHN
CIFAR10
CIFAR10
FS
MM
FS
MM
(79.25/79.48)
(95.82/96.38)
(53.35/53.61)
(87.98/88.79)
(89.47/89.87)
(96.87/97.45)
(73.47/73.96)
(90.63/91.39)
(94.25/94.71)
(97.07/97.61)
(86.51/87.37)
(93.29/93.99)
Table 3: Performance (accuracy/ﬁdelity) of fully supervised
(FS) and MixMatch (MM) extraction on SVHN and CIFAR10.
MixMatch with 4000 labels performs nearly as well as the
oracle for both datasets, and MixMatch at 250 queries beats
fully supervised training at 4000 queries for both datasets.
of 64.5%, is trained with the rotation loss on oracle labels
whereas the baseline on ImageNet labels only achieves 62.5%
accuracy with the rotation loss and 61.2% without the rotation
loss. This demonstrates the cumulative beneﬁt of adding a
rotation loss to the objective and training on oracle labels for
a theft-motivated adversary.
We expect that as semi-supervised learning techniques on
ImageNet mature, further gains should be reﬂected in the
performance of model extraction attacks.
MixMatch. To validate this hypothesis, we turn to smaller
datasets where semi-supervised learning has made signiﬁcant
progress. We investigate a technique called MixMatch [38] on
two datasets: SVHN [39] and CIFAR10 [40]. MixMatch uses
a combination of techniques, including training on "guessed"
labels, regularization, and image augmentations.
For both datasets, inputs are color images of 32x32 pixels
belonging to one of 10 classes. The training set of SVHN
contains 73257 images and the test set contains 26032 images.
The training set of CIFAR10 contains 50000 images and the
test set contains 10000 images. We train the oracle with a
WideResNet-28-2 architecture on the labeled training set.
The oracles achieve 97.36% accuracy on SVHN and 95.75%
accuracy on CIFAR10.
The adversary is given access to the same training set but
without knowledge of the labels. Our goal is to validate the
effectiveness of semi-supervised learning by demonstrating
that the adversary only needs to query the oracle on a small
subset of these training points to extract a model whose accu-
racy on the task is comparable to the oracle’s. To this end, we
run 5 trials of fully supervised extraction (no use of unlabeled
data), and 5 trials of MixMatch, reporting for each trial the
median accuracy of the 20 latest checkpoints, as done in [38].
Results. In Table 3, we ﬁnd that with only 250 queries (293x
smaller label set than the SVHN oracle and 200x smaller
for CIFAR10), MixMatch reaches 95.82% test accuracy on
SVHN and 87.98% accuracy on CIFAR10. This is higher
than fully supervised training that uses 4000 queries. With
4000 queries, MixMatch is within 0.29% of the accuracy of
the oracle on SVHN, and 2.46% on CIFAR10. The variance
of MixMatch is slightly higher than that of fully supervised
training, but is much smaller than the performance gap. These
gains come from the prior MixMatch is able to build using the
unlabeled data, making it effective at exploiting few labels.
We observe similar gains in test set ﬁdelity.
5 Limitations of Learning-Based Extraction
Learning-based approaches have several sources of non-
determinism: the random initializations of the model parame-
ters, the order in which data is assembled to form batches for
SGD, and even non-determinism in GPU instructions [41,42].
Non-determinism impacts the model parameter values ob-
tained from training. Therefore, even an adversary with full ac-
cess to the oracle’s training data, hyperparameters, etc., would
still need all of the learner’s non-determinism to achieve the
functionally equivalent extraction goal described in Section 3.
In this section, we will attempt to quantify this: for a strong
adversary, with access to the exact details of the training
setup, we will present an experiment to determine the limits
of learning-based algorithms to achieving ﬁdelity extraction.
We perform the following experiment. We query an ora-
cle to obtain a labeled substitute dataset D. We use D for
a learning-based extraction attack which produces a model
f 1
θ (x). We run the learning-based attack a second time using
D, but with different sources of non-determinism to obtain
USENIX Association
29th USENIX Security Symposium    1351
Query Set
Init & SGD
Same SGD
Same Init
Different
to achieve functionally-equivalent extraction.
Test
Adv Ex
Uniform
93.7%
73.6%
65.7%
93.2%
65.4%
60.2%
93.1%
65.3%
59.0%
93.4%
67.1%
60.2%
Table 4: Impact of non-determinism on extraction ﬁdelity.
Even models extracted using the same SGD and initialization
randomness as the oracle do not reach 100% ﬁdelity.
a new set of parameters f 2
θ (x). If there are points x such that
θ (x) (cid:54)= f 2
f 1
θ (x), then the prediction on x is dependent not on
the oracle, but on the non-determinism of the learning-based
attack strategy—we are unable to guarantee ﬁdelity.
We independently control the initialization randomness and
batch randomness during training on Fashion-MNIST [43]
with fully supervised SGD (we use Fashion-MNIST for train-
ing speed). We repeated each run 10 times and measure agree-
ment between the ten obtained models on the test set, adver-
sarial examples generated by running FGSM with ε = 25/255
with the oracle model and the test set, and uniformly random
inputs. The oracle uses initialization seed 0 and SGD seed
0—we also use two different initialization and SGD seeds.
Even when both training and initialization randomness are
ﬁxed (so that only GPU non-determinism remains), ﬁdelity
peaks at 93.7% on the test set (see Table 4). With no random-
ness ﬁxed, extraction achieves 93.4% ﬁdelity on the test set.
(Agreement on the test set should should be considered in
reference to the base test accuracy of 90%.) Hence, even an
adversary who has the victim model’s exact training set will
be unable to exceed ~93.4% ﬁdelity. Using prototypicality
metrics, as investigated in Carlini et al. [44], we notice that
test points where ﬁdelity is easiest to achieve are also the
most prototypical (i.e., more representative of the class it is la-
beled as). This connection is explored further in Appendix B.
The experiment of this section is also related to uncertainty
estimation using deep ensembles [42]; we believe a deeper
connection may exist between the ﬁdelity of learning-based
approaches and uncertainty estimation. Also relevant is the
work mentioned earlier in Section 3, that shows that random
networks are hard for learning-based approaches to extract.
Here, we ﬁnd that learning-based approaches have limits even
for trained networks, on some portion of the input space.
It follows from these arguments that non-determinism of
both the victim and extracted model’s learning procedures
potentially compound, limiting the effectiveness of using a
learning-based approach to reaching high ﬁdelity.
This attack can be seen as an extension of two prior works.
• Milli et al. [19] introduce an attack to extract neural net-
work weights under the assumption that the adversary
is able to make gradient queries. That is, each query
the adversary makes reveals not only the prediction of
the neural network, but also the gradient of the neural
network with respect to the query. To the best of our
knowledge this is the only functionally-equivalent ex-
traction attack on neural networks with one hidden layer,
although it was not actually implemented in practice.
• Batina et al. [25], at USENIX Security 2019, develop a
side-channel attack that extracts neural network weights
through monitoring the power use of a microprocessor
evaluating the neural network. This is a much more pow-
erful threat model than made by any of the other model
extraction papers. To the best of our knowledge this is
the only practical direct model extraction result—they
manage to extract essentially arbitrary depth networks.
In this section we introduce an attack which only requires
standard queries (i.e., that return the model’s prediction in-
stead of its gradients) and does not require any side-channel
leakages, yet still manages to achieve higher ﬁdelity extraction
than the side-channel extraction work for two-layer networks,
assuming double-precision inference.
Attack Algorithm Intuition. As in [19], our attack is tai-
lored to work on neural networks with the ReLU activation
function (the ReLU is an effective default choice of activation
function [20]). This makes the neural network a piecewise
linear function. Two samples are within the same linear region
if all ReLU units have the same sign, illustrated in Figure 2.
By ﬁnding adjacent linear regions, and computing the differ-
ence between them, we force a single ReLU to change signs.
Doing this, it is possible to almost completely determine the
weight vector going into that ReLU unit. Repeating this attack
for all ReLU units lets us recover the ﬁrst weight matrix com-
pletely. (We say almost here, because we must do some work
to recover the sign of the weight vector.) Once the ﬁrst layer of
the two-layer neural network has been determined, the second
layer can be uniquely solved for algebraically through least
squares. This attack is optimal up to a constant factor—the
query complexity is discussed in Appendix D.
6 Functionally Equivalent Extraction
Having identiﬁed fundamental
limitations that prevent
learning-based approaches from perfectly matching the or-
acle’s mistakes, we now turn to a different approach where
the adversary extracts the oracle’s weights directly, seeking
6.1 Notation and Assumptions
As in [19], we only aim to extract neural networks with one
hidden layer using the ReLU activation function. We denote
the model weights by A(0) ∈ Rd×h,A(1) ∈ Rh×K and biases
by B(0) ∈ Rh,B(1) ∈ RK. Here, d,h, and K respectively refer
1352    29th USENIX Security Symposium
USENIX Association
Symbol
Deﬁnition
d
h
K
A(0) ∈ Rd×h
B(0) ∈ Rh
A(1) ∈ Rh×K
B(1) ∈ RK
Input dimensionality
Hidden layer dimensionality (h < d)
Number of classes
Input layer weights
Input layer bias
Logit layer weights
Logit layer bias
Table 5: Parameters for the functionally-equivalent attack.
Figure 2: 2-dimension intuition for the functionally equivalent
extraction attack.
to the input dimensionality, the size of the hidden layer, and
the number of classes. This is found in Table 6.1.
We say that ReLU(x) is at a critical point if x = 0; this is the
location at which the unit’s gradient changes from 0 to 1. We
assume the adversary is able to observe the raw logit outputs
as 64-bit ﬂoating point values. We will use the notation OL to
denote the logit oracle. Our attack implicitly assumes that the
rows of A(0) are linearly independent. Because the dimension
of the input space is larger than the hidden space by at least
100, it is exceedingly unlikely for the rows to be linearly
dependent (and we ﬁnd this holds true in practice).
Note that our attack is not an SQ algorithm, which would
only allow us to look at aggregate statistics of our dataset.
Instead, our algorithm is very particular in its analysis of the
network, computing the differences between linear regions,
for example, cannot be done with aggregate statistics. This
structure allows us to avoid the pathologies of Section 3.3.
6.2 Attack Overview
The algorithm is broken into four phases:
• Critical point search identiﬁes inputs {xi}n
i=1 to the
neural network so that exactly one of the ReLU units is
at a critical point (i.e., has input identically 0).
• Weight recovery takes an input x which causes the ith
neuron to be at a critical point. We use this point x to
compute the difference between the two adjacent linear
regions induced by the critical point, and thus the weight
vector row A(0)
. By repeating this process for each ReLU
i
we obtain the complete matrix A(0). Due to technical
reasons discussed below, we can only recover the row-
vector up to sign.
• Sign recovery determines the sign of each row-vector
A(0)
j
for all j using global information about A(0).
• Final layer extraction uses algebraic techniques (least
squares) to solve for the second layer of the network.
6.3 Critical Point Search
For a two layer network, observe that the logit function is
given by the equation OL(x) = A(1)ReLU(A(0)x+B(0))+B(1).
To ﬁnd a critical point for every ReLU, we sample two random