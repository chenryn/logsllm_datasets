arguments as before (a high probability result can also be
obtained):
Theorem 3: Let L be the loss of the algorithm in Figure 3,
against an adversary attacking with at most M sybil identities
voting for any given object. Then regardless of the adversary’s
(cid:17)
(3)
Proof sketch: By similar arguments as in Theorem 1, we
p· SΣ/c+α−1
E[L] ≤ λ · 1
p f ·(cid:16)
1−β + 1− p
strategy:( λ ≤ D fdlogα(α · c· (W + M)/SΣ)e
have: 
λ ≤ D fdlogα(α · c· (W + M)/SΣ)e
E[Gn] = 1
f λ
E[Bn + Gn] ≤ 1
p f λ
(1− β )· c· Bo ≤ (SΣ + (α − 1)· c)· Gn
Solving the above equations yields the desired results. 2
It is possible to ﬁnd the optimal values for α and SΣ/c,
but that will make them dependent on p and M (which are
unknown). Thus, we simply use α = 2 and SΣ/c = 0.5. These
values are sufﬁcient to give us the best result asymptotically.
Corollary 4: Setting α = 2, β = 0.5, and SΣ/c = 0.5, we
have
E[L] ≤ (1 + 2p)/(p f )· λ0,
where λ0 = D f ·dlog2(4(W + M))e
(4)
(5)
Because W ≤ N + N0 ≤ M, for p and f constants bounded
away from 0, E[L] becomes O(cid:0)D f logM(cid:1) asymptotically.
A growing defense. A salient property of DSybil is that
its defense against attacks grows over time. Namely, if
there are some initial attack-free rounds, then Alice’s total
291
loss (including the loss in the attack-free rounds) can be
signiﬁcantly smaller than the loss from Corollary 4.8 For
example, in later experiments under M = 1010, Alice’s per-
round loss drops from 12% to 4% if Alice has been using
DSybil for a month before the adversary starts attacking.
This growing defense comes from the following three
factors. First, to inﬂict a maximum loss, the adversary needs
to ensure that each critical guide receives no more than
SΣ/(W + M) seed trust. In an attack-free round, the number
of voters for any given object is likely to be signiﬁcantly
smaller than W + M. Thus, in those rounds the seed trust
given to a critical guide can be much larger than SΣ/(W +M).
Such effect can be signiﬁcant because the identities are
likely to be assigned seed trust during the earlier rounds
(instead of later rounds). Second, to inﬂict a maximum
loss, the sybil identities need to vote for every good non-
overwhelming object consumed (without causing the objects
to become overwhelming), so that they can gain trust and
later maximize Bo. If Alice has already consumed some good
non-overwhelming objects before the attack starts, the sybil
identities lose some “opportunities” to gain trust.
Finally, the maximum loss in Corollary 4 is reached
only when for every guided good non-overwhelming object
consumed, there is only one critical guide voting for that
object. As one would imagine, in practice, there can easily
be multiple critical guides voting. Whenever this happens
in an attack-free round, the trust of multiple critical guides
will increase, and λ will be smaller. After the attack starts,
the adversary will be able to use a choking attack to always
prevent this from happening. Namely, whenever multiple
critical guides vote for a good non-overwhelming object,
the sybil identities can all vote for that object and make it
overwhelming. This “chokes” the critical guides so that they
will not have the opportunity to simultaneously gain trust.
One can easily modify Corollary 4 to calculate a bound
on the loss when the attack does not start from the very ﬁrst
round. Speciﬁcally, let si be the trust of the ith critical guide
when the attack starts (for 1 ≤ i ≤ D f ). To bound the loss
incurred after the attack starts, we simply replace Equation 5
with:
(cid:19)(cid:25)
(cid:18)
(cid:24)
D f
∑
i=1
λ0 =
log2
2
max(si/c,1/(2W + 2M))
(6)
Adding such loss to the loss incurred during the attack-free
rounds will then give us the total loss.
5.3. Further Extensions
Consuming multiple objects in a round. Our discussion
so far assumes that DSybil recommends one object in each
round and Alice consumes the recommended object. In some
cases, Alice may want to consume more than one object.
For example, Alice may want to read 20 new stories among
8. Recall that DSybil does not know which rounds are attack-free.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:17:54 UTC from IEEE Xplore.  Restrictions apply. 
the set U of news stories in the past day. As mentioned in
Section 3, we can readily model this as 20 rounds. Let the set
of the objects in these 20 rounds be U1 through U20. Then
we can set U1 = U, U2 = U \{O} where O is the object
already consumed in round 1, and so on.
(7)
and
f 0 =
p0 =
The only complication here is that if O is a guided good
object, then the fraction of good objects and guided good
objects in U2 may now be below p and p f , respectively. To be
precise, let v be the number of objects Alice consumes from
the original set U and imagine that we model the process
as v rounds where Alice consumes one object per round.
Let p0 and f 0 be the lower bound on the fraction of good
objects and guided good objects in any of these v rounds.
Let u = |U|. A straight-forward calculation then shows:
u· p− v
u− v
u· p· f − v
u· p− v
Applying Corollary 4 then immediately shows
E[L] ≤ (1 + 2p0)/(p0 f 0)· λ0
(8)
Notice that these results assume that u · p · f > v, which
explains why f cannot be overly small.
Recommending multiple objects in a round. Our algo-
rithm can also be extended trivially to recommending multiple
objects in each round where Alice selects one to consume.
For space limitations, however, we leave the details to our
technical report [63].
Disagreements between Alice and the critical guides.
DSybil targets contexts where users can be classiﬁed into
types and each type has the same or similar “taste.” Our
analysis so far has been assuming that Alice has the same
taste as the critical guides (i.e., the critical guides never vote
on bad objects). (Whether Alice has the same or similar taste
as the other guides is irrelevant.) Next, we show that a small
number of votes from the critical guides on bad objects will
not increase the loss of our algorithm excessively, as long
as β is not too close to 0.
Imagine that a critical guide votes on a bad object O and
Alice consumes O. This will increase Alice’s loss in two
ways: i) if not for the vote from the critical guide, Alice
might not consume this bad object, and ii) the trust of the
critical guide will be decreased (i.e., multiplied by β ). The
extra loss from the ﬁrst part is at most 1. For the second
part, with α = 2 and β = 0.5, DSybil only needs to multiply
the trust of the critical guide by α one additional time in
order to compensate. Thus, the effect is simply to increase
λ0 by 1. In turn, this translates to an extra loss of O(1) (by
Corollary 4).
6. A Deeper Look
DSybil’s algorithm is optimal. We will prove that DSybil’s
elegant recommendation algorithm is (asymptotically) opti-
mal, by proving a lower bound on loss. To make our lower
bound as strong as possible, we will allow “negative votes”
292
that were not included in our system model. We allow sybil
identities to cast negative votes in arbitrary ways. For guides,
we optimistically assume that if a guide ever appears (casts
positive votes) in a round, it will cast negative votes on all
the bad objects in that round. Obviously, this maximizes
the information regarding which objects are bad and makes
our lower bound stronger. Our lower bound construction
depends on the value of p. Although our algorithm focuses
on constant p within (0,1), to be as complete as possible, we
will also consider p → 0 and p → 1 when proving the lower
bound. Doing so will help to reveal that the 1/p factor in
DSybil’s loss is somewhat fundamental. For the lower bound,
we continue to assume that if the f -fractional dimension is
D f , then in every round, f fraction of the good objects need
to be guided. Notice that this strengthens the lower bound,
because it imposes additional restrictions on our construction.
The following theorem presents the lower bound (see [63]
for proof):
Theorem 5: For any given p (in the following 3 cases),
nonnegative integer M, positive integer D, and any recom-
mendation algorithm, we can always construct a sequence
of rounds, objects, and votes where i) the fraction of good
objects in each round is at least p, ii) there are at most M
sybil identities voting for each object, iii) for all f ∈ (0,1]
the f -fractional dimension, D f , of the objects is D, and iv)
the algorithm will incur at least
• 1
2 · D f ·blog2 Mc expected loss if p = 0.5.
k
4 · D f ·j 1
2 · D f ·j
p
k·j log2 M
k
log2(1/p)
log2 M
log2(3/(1−p))
• 1
• 1
expected loss if 0 < p < 0.5.
expected loss if 0.5 < p < 1.
For constant p, this becomes Ω(D f logM).
This lower bound (asymptotically) matches DSybil’s guar-
antee in Corollary 4. In addition, it is worth noting that for
0 < p < 0.5, the 1/p term is present in the lower bound
as well (except for an extra logarithmic term log2(1/p) in
the denominator, which however is dominated by 1/p). This
means that the 1/p term in DSybil’s loss (Corollary 4) cannot
be avoided, unless we make additional assumptions in the
model.
For the scenario where the total number of sybil identities
is M (as in Section 5.1 and Corollary 2), using a rather
similar proof as for Theorem 5, we can prove that the lower
bound there is the same as in Theorem 5 except that the
term “log2 M” should be replaced by “log2(M/D f )”. Again,
this asymptotic lower bound Ω(D f log(M/D f )) matches the
guarantee from Corollary 2.
DSybil’s algorithm may be necessary for optimality. We
next show that some natural alternatives (or “optimiza-
tions”/“tweaks”) to our algorithm will easily break its
optimality. This means that many of the design choices
in DSybil may actually be necessary for optimality. Some
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:17:54 UTC from IEEE Xplore.  Restrictions apply. 
of the following alternative designs (or similar designs)
have been used in previous recommendation algorithms
(which are not necessarily designed to defend against sybil
attacks) [20, 32, 40, 46, 49, 60].
• If DSybil
increased the trust additively instead of
multiplicatively, then one can easily show that DSybil’s
loss would be linear with respect to M instead of
logarithmic.
• In a non-overwhelming round, currently DSybil simply
returns a uniformly random object (i.e., it does not
distinguish different objects with different votes). If,
instead, the algorithm returned the object whose voters
have the largest combined trust, then the loss would
be linear with respect to M under the following attack.
Consider a sequence of rounds where each round has
exactly one good object and one bad object. Initially,
all the M sybil identities vote on the good object, until
Alice consumes the ﬁrst good object. Each sybil identity
now has a positive trust value that is no smaller than
the trust of any honest identity. In addition, no identity
has trust value larger than αSΣ/M. For each of the
next M/(W + 1) rounds, the adversary uses (W + 1)
sybil identities to vote on the bad object. As long as
(αSΣ/M)· (W + 1) < 1 (which easily holds), no objects
will be overwhelming. Since the good object has at most
W voters while the bad object has (W + 1) voters (each
with either the same or larger trust), the algorithm will
recommend bad objects in all these M/(W +1) = Ω(M)
rounds (for constant W ).
• Consider a modiﬁed version of our algorithm where in
each round, the algorithm recommends each object O
with certain probability. The probability is proportional
to the total trust of the voters on O. Consider the same
attack as above except that after Alice consumes the ﬁrst
√
MW sybil identities to vote on the bad
adversary uses
object. This means that the probability mass on the bad
good object, for each of the nextpM/W rounds, the
object will be at leastpM/W times the probability mass
any further good objects) will be Ω(pM/W ) = Ω(
on the good object. Based on the mean of geometric
distributions, one can easily show that the expected
√
number of bad objects consumed (before consuming
M)
(for constant W ).
• Currently DSybil only increases the trust of the voters
on non-overwhelming good objects. Imagine that we
instead simply increase the trust of the voters on any
good objects. Consider an adversary that initially has
all M sybil identities vote on all the good objects. After
Θ(log(M · c/SΣ)) rounds, all of the sybil identities will
have a trust of c. Then, each of them can cause 1 loss
(i.e., making some bad object overwhelming), resulting
in a total loss of Ω(M). Even if we further optimize
and pick the overwhelming object whose voters have
the largest combined trust, the loss will still reach
Ω(M/W ) = Ω(M) (for constant W ).
Lifespan and population. Like other systems, a recommen-
dation system’s robustness against malicious behavior comes
from the help provided by the honest identities (or votes from
the guides in our case). The number of malicious identities
that other systems (e.g., majority voting, byzantine consensus,
and DHTs) can tolerate is usually directly proportional to the
number of honest identities (i.e., their population). In contrast,
the following will use our lower bound results to show that
to tolerate more sybil identities in a recommendation system,
the lifespan (or more precisely the number of votes cast
throughout the identity’s lifetime) of the honest identities is