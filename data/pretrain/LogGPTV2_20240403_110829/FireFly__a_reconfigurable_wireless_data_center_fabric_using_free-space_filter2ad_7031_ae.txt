a detailed system-level emulation using MiniNet. For (3) and (4),
we use ofﬂine trace-driven evaluations. For (5), we use the ﬂow-
level simulation. Finally, for (6) we use public cost estimates and
projections from §3.
8.1 System Performance
Setup and Workloads. We consider three classes of architectures:
(1) FireFly (both SM and GM) with 10Gbps links, (2) (wired)
10Gbps full-bisection bandwidth networks such as FatTree [13],
and (3) augmented architectures such as c-Through [48] and 3D-
Beamforming (3DB) [52] with a 5Gbps (i.e., 1:2 oversubscribed)
core.10 (We do not compare Flyways [26] since it is subsumed by
3D-Beamforming.) By default, FireFly has 48 FSOs per rack with
each equipped with 10 SMs; we assume a rack size of 4(cid:48)×2(cid:48), which
is sufﬁcient to easily hold 48 FSO devices (§3.3). We also evaluated
Jellyﬁsh [45], but do not show this for ease of visualization since
the result was close to FatTree (≈ 10% lower). We assume an
overall reconﬁguration latency of 20 msecs for FireFly, and conser-
vatively use zero latency for c-Through/3DB. We use ECMP rout-
ing for FatTree and backbone cores of 3dB and c-Through, and
route the “overﬂow” trafﬁc to their augmented links [52]. Finally,
for FireFly, we use the routing strategies described in §5.
Following prior work, we use synthetic trafﬁc models based on
DC measurements [23,52]. As a baseline, we consider a Uniform
model where ﬂows between pairs of racks arrive independently
with a Poisson arrival-rate λ/s, with an empirically-derived ﬂow
size distribution [23]. We use λ as the knob to tune the level of net-
work saturation. Based on prior observations, we also consider the
Hotspot model [23], where in addition to the Uniform base-
line, a subset of rack pairs have a higher arrival rate λ2 and a
ﬁxed large ﬂow size of 128MB [52]. For Uniform loads, we
use the label Uniform X where X is average load per server (in
Gbps) by choosing suitable λ. For Hotspot loads, we use the la-
bel Hotspot(Y,X) where Y is the % of racks that are hotspots
and X is the additional average load on each hotspot server; all
Hotspot workloads use Uniform 5 as the background trafﬁc.
Performance Comparison. There are two key metrics here: (1)
the average throughput per server, and (2) ﬂow completion time
10While a lower oversubscription would improve c-Through’s per-
formance, the cost increases almost proportionally—eventually be-
coming equal to FatTree at 1:1 oversubscription.
(a) Flow completion for long ﬂows
(b) Flow completion for short ﬂows
(c) Average throughput per server (Gbps)
Figure 8: Flow completion times (FCT) and average through-
put per-server using the htsim simulator on a 64-node topol-
ogy for different workloads
(FCT). For ease of visualization, we do not show error bars over
multiple runs, since the results were consistent across multiple runs.
We also do not show FireFly-GM (with 40◦ coverage-angle GMs)
results, since they are similar to the default FireFly-SM.
As a starting point, we use htsim for a detailed packet-level
simulation. We extended htsim to support short ﬂows, arbitrary
trafﬁc matrices, and route reconﬁgurations. Due to scaling limita-
tions of htsim, even on a high-end server (2.6GHz, 64 GB RAM),
we could only scale to a 64-rack DC at our 10 Gbps workloads.
Figure 8(a) and 8(b) show a box-and-whiskers plot (showing max-
imum, minimum, median, 25%iles, and 75%iles) of the FCT for
long/short ﬂows respectively for a 30 secs run. The result shows
that FireFly’s performance is close to the full-bisection bandwidth
network in both cases. c-Through and 3DB do not perform as well
because their augmented network is not sufﬁcient to compensate
for the oversubscription. Thus, their tail performance for long ﬂows
suffers. We also see that the FCT for short ﬂows is similar across
FireFly and FatTree.
Figure 8(c) shows the effective average per-server throughput in
the 64-rack setup for different workloads. For the Uniform the av-
erage is over all servers whereas for Hotspot the average is over
the hotspot servers.
In short, we see that FireFly’s performance
is close to the full-bisection bandwidth network and roughly 1.5×
better than the augmented architectures.
To scale to larger DCs, we use a custom ﬂow-level simulator.
We do so after conﬁrming that these simulations roughly match
the packet-level simulations.
In general, the ﬂow-level simula-
 0 2 4 6 8 10 12 14Hotspot(8,2.5)Hotspot(8,5)Hotspot(16,2.5)Hotspot(16,5)Uniform1Uniform5Uniform10FCT (ms)FireflyFatTree3D Beamformingc-Thru 0.0001 0.001 0.01Hotspot(8,2.5)Hotspot(8,5)Hotspot(16,2.5)Hotspot(16,5)Uniform1Uniform5Uniform10FCT (ms) 0 1 2 3 4 5 6 7 8 9 10Hotspot(8,2.5)Hotspot(8,5)Hotspot(16,2.5)Hotspot(16,5)Uniform1Uniform5Uniform10Average per-server throughput (Gbps)3272 .11 Table 1 shows that the SM-PCFT and
an upper bound of nm
GM-PCFT solutions achieve ≥ 91% and ≥84% of the upper bound
across different DC sizes. The lower performance of GM-PCFT
is likely because of less randomness due to a block-level construc-
tion. (This does not however impact the performance of the runtime
topology in practice for the workloads we consider.)
We also evaluate an incremental expansion scenario where we
want to retain most existing PCFT as we add new racks similar
to Jellyﬁsh [45]. We ﬁnd that incrementally constructed PCFTs
perform nearly identical w.r.t. a PCFT computed from scratch (not
shown). We posit that this stems from the incremental expandabil-
ity of random graphs [45].
8.4 Reconﬁguration Efﬁciency
# Racks
32
128
256
512
Full-LP
138
4945
1.7×106
6.4×108
Time (ms)
Greedy-LP
110
208
3.3×104
1.9×107
FireFly two-step
27
54
60
68
Optimality Gap
(%)
2.8%
2.6%
1.3%
2.8%
Table 2: Scalability and optimality of the FireFly reconﬁgura-
tion algorithm.
Table 2 shows the computation time and optimality gap of the
FireFly two-step heuristic from §5.1. We consider two points of
comparison: (a) Full-LP, a LP relaxation of Figure 6, which also
yields an upper-bound on the optimal, and (b) Greedy-LP which
uses greedy topology selection but solves the ﬂow routing LP us-
ing Gurobi. Our approach is several orders of magnitude faster—
Full-LP and Greedy-LP simply do not scale for ≥ 32 racks. This
is crucial as the FireFly controller may need to periodically reopti-
mize the network every few seconds. Moreover, the (upper bound)
on the optimality gap is ≤ 2.8%. Finally, we note that triggered
reconﬁgurations (§5.2) incur only 5-10 msec (not shown). Most of
the time is actually spent in route computation, which can be run in
parallel to allow a high rate of concurrent reconﬁgurations.
8.5 Sensitivity Analysis
# FSOs per rack
# SMs per FSO = 10
# SMs per FSO = 13
# SMs per FSO = 15
GM-based network
36
5.14
6.21
6.45
6.87
40
6.07
7.27
7.54
7.84
44
7.47
8.20
8.31
8.66
48
8.83
9.17
9.25
9.24
Table 3: Average throughput per-server on Uniform10 for
varying network parameters.
Given that we are considering a new technology, we evaluate the
sensitivity w.r.t. key parameters: number of FSOs, number of SMs,
and the reconﬁguration latency. Table 3 shows the average per-
server throughput on a range of FireFly instantiations. As before,
we use the conﬁguration of 512 racks with 48 servers each. As
expected from the insights from §2.1, the performance decreases
almost linearly with decrease in the number of FSOs (i.e., ﬂexible
ports) per rack. The performance degradation in GM-based net-
works is comparatively better. Note that the networks with fewer
FSOs also have almost-proportionally lower cost. However, in-
creasing the number of SMs per FSO does counter the degradation
to some extent, due to increase in ﬂexibility.
If FSO size is the
limiting concern, one way to get higher performance is to simply
11Any equi-sized partition of n racks with m FSOs can have at most
nm/2 active links (one per FSO) in a “cut”.
(a) Hotspot(16,5)
(b) Uniform10
Figure 9: Scalability evaluation using a ﬂow-level simulator
tions overestimates the throughput 5-7% for all architectures since
it does not model packet-level effects. Since our goal is to compare
the relative performance of these architectures, these simulations
are still instructive. Figure 9 shows that the earlier performance
results continue to hold for the most saturating workloads even at
larger scales. The only drop is at 512 racks for Uniform10; here
the number of SMs/FSO is slightly sub-optimal as the number of
racks grows. We revisit this in §8.5.
We also measured the packet latency (number of hops) statistics
and found that the average latency were 3.91 (FireFly), 4.81 (Fat-
Tree), and 3.9 (3dB, c-Through), while the maximum was 5 for
FireFly and 6 for the rest.
8.2 Performance during Flux
Because packet- or ﬂow-level simulations do not give us a detailed
replay of the events at the FireFly controller and in the network,
we use Mininet for this evaluation [7]. Due to scaling limita-
tions, we scale down the DC size to 32 racks and the link rates to
10 Mbps, and correspondingly scale the workload down. Since our
goal is to understand the relative impact of reconﬁgurations w.r.t.
the steady state behavior, we believe this setup is representative.
In particular, note that the relative “penalty” of the reconﬁguration
latency remains the same since we also scale down the workload
with the link rates.
For the following result, we consider a HotSpot workload, with
seven distinct reconﬁgurations as elephant ﬂows arrive. We poll the
virtual switches to obtain link utilization and loss rates and use a
per-rack-pair ping script to measure inter-rack latency. We divide
these measurements into two logical bins: (a) During reconﬁgura-
tions and (b) Steady state (i.e., no active reconﬁguration). Figure 10
shows the distribution link utilization, loss rate, and inter-rack la-
tency for each bin. While there is a small increase in the tail values,
the overall distributions are very close. This suggests that the im-
pact on the network during reconﬁgurations is quite small and that
our mechanisms from §6 work as expected.
8.3 Preconﬁguration Efﬁciency
#Racks Normalized DBW w.r.t. upper bound
SM-PCFT
GM-PCFT
64
128
256
512
0.96
0.93
0.91
0.94
0.84
0.84
0.85
0.88
Table 1: Efﬁciency of the PCFT algorithms
As before, we use 48 FSOs per rack and 10 SMs per FSO for
SM-PCFT, and assume GMs with an coverage-angle of 40◦ for
GM-PCFT. We generate ≈ 15n (n is the number of racks) random
instances and pick the best. We normalize the estimated DBW w.r.t
567891064128256512Avg per-serverthroughput (Gbps)Number of racksFireflyFatTree3D-Beamc-Thru567891064128256512Avg per-serverthroughput (Gbps)Number of racks328Figure 10: Comparing network performance during reconﬁgurations and in steady state
FatTree
3DB
cThru
FireFly
Cu
15
9
9
Fiber
22
13
13
12
Cu
2
2
2
Fiber
8
5
5
0
Equip
Cable
Power
Total