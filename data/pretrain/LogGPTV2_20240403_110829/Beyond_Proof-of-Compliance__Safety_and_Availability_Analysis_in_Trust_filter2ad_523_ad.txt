fc(?X, ?u, ?X, ?u)
For each A.r←− B.r1 in P|R, add
fc(A, r, ?Z, ?w) :− fc(B, r1, ?Z, ?w)
For each A.r ∈ Roles(P) − G, add
nc(?X, ?u, A, r) :− ∼fc(?X, ?u, A, r)
For each A.r ∈ G, do the following:
For each A.r←− D in P, add
nc(?X, ?u, A, r) :− ∼fc(?X, ?u, A, r),
For each A.r←− B.r1 in P, add
nc(?X, ?u, A, r) :− ∼fc(?X, ?u, A, r),
nc(?X, ?u, B, r1)
∼lb(?X, ?u, D)
(c)
(c2)
(n0)
(n1)
(n2)
Rules (c) and (c2) are straightforward. The intuition behind
(n0) is that for X.u to contain a g-unrestricted role A.r, X.u
has to be forced to contain A.r, since arbitrary new mem-
bers may be added to A.r. The intuition behind (n1) is that,
since A.r contains D, if X.u’s lower bound does not con-
tain D, and X.u is not forced to contain A.r, then X.u does
not contain A.r. The “∼fc” part is needed, since it may be
the case that A.r ←− D can be removed and X.u ←− A.r
exists and cannot be removed, in which case D may not be
in X.u’s lower bound. Rule (n2) means that X.u does not
contain A.r if it does not contain B.r1 and is not forced to
contain A.r.
We now discuss the semantics of the logic program
BCP(P,R), which uses negation-as-failure, but in a strat-
iﬁed manner. Given a logic program LP, a predicate p (di-
rectly) depends on another predicate q if p is deﬁned using
q in the body. A predicate p negatively depends on q if
∼ q (the negation of q) is used to deﬁne p. For example, in
BCP(P,R), fc depends on itself, nc depends on itself and
negatively depends on fc and lb. A program is stratiﬁed if
the predicates deﬁned in it can be classiﬁed into strata such
that each predicate depends only on predicates in the same
or lower strata and negatively depends only on predicates in
lower strata. A program without negation is trivially strati-
ﬁed, as no predicate depends negatively on any predicate at
all. The program BCP(P,R) is also stratiﬁed. Predicates
in the ﬁrst stratum are lb and fc, and the only predicate in
the second stratum is nc.
Most commonly accepted semantics for logic program-
ming with negation-as-failure agree on stratiﬁed programs.
Given a stratiﬁed datalog program LP, let LP 1∪···∪LP s
be a partition of LP Inst such that LP j consists of clauses
deﬁning predicates in the j’th stratum; we call LP 1 ∪
··· ∪ LP s a stratiﬁcation of LP Inst. The semantics is
obtained by ﬁrst computing the minimal Herbrand model
of LP 1 and then use this model to determine truthfulness
of negative literals in LP 2 while computing a ﬁxpoint for
Proceedings of the 2003 IEEE Symposium on Security and Privacy (SP(cid:146)03) 
1081-6011/03 $17.00 ' 2003 IEEE 
8
LP 1 ∪ LP 2, and so on. Formally, we deﬁne an operator
Φ, which is parameterized by a ground logic program LP(cid:2)
and a set of ground atoms M. Given a set of ground logical
atoms K, ΦLP(cid:1),M (K) consists of all ground logic atoms, a,
such that a :− b1, . . . , bn,∼ bn+1, . . . ,∼ bn+m ∈ LP(cid:2)
and
bi ∈ K and bn+j (cid:25)∈ M. Given a logic program LP and
LP 1 ∪ ··· ∪ LP s a stratiﬁcation of LP Inst, deﬁne Γ1LP to
be ΦLP1,∅↑ω, i.e., the least ﬁxpoint of ΦLP1,∅. Deﬁne Γk+1LP
↑ω for 1 ≤ k ≤ s − 1. Then the
to be ΦLP1∪···∪LP k+1,ΓkLP
model of LP is ΓsLP. Each ΓiLP can be calculated in poly-
nomial time, so the semantics of a stratiﬁed program can
also be computed in polynomial time.
The following lemma says that the fc predicate in BCP
is always sound for role containment, and it is complete
when the second role is g-unrestricted.
Lemma 4.1 Given a BRT state P, R, two roles X.u and
A.r, if BCP(P,R) |= fc(X, u, A, r), then X.u contains
A.r. If X.u contains A.r and A.r is g-unrestricted, then
BCP(P,R) |= fc(X, u, A, r).
See Appendix A.2 for the proof. The following proposition
says that role containment in BRT can be answered by us-
ing the program BCP(P,R).
Proposition 4.2 Given a BRT state P, R, and two roles
X.u and A.r in Roles(P), BCP(P,R) |= nc(X, u, A, r) if
and only if X.u does not contain A.r.
See Appendix A.2 for the proof.
4.2 Complexity Results for Containment Analysis
in NRT , LRT , and SRT
NRT adds to BRT type-4 statements. Intersections in
type-4 statements have the effect of conjunction. A role
can be deﬁned by multiple statements, which have the ef-
fect of disjunction. As a result, NRT can simulate formulas
in propositional logic, and answering containment queries
subsumes determining validity of propositional formulas,
which is coNP-complete.
Theorem 4.3 Containment analysis in NRT is coNP-
complete.
Theorem 4.4 Containment analysis in LRT is coNP-hard.
See Appendix A.3 for the proof. We now give an up-
per bound on the computational complexity of containment
analysis in SRT . This shows that containment analysis in
SRT (and thus the sub-language LRT ) is decidable.
Theorem 4.5 Containment analysis in SRT is in coNEXP.
See Appendix A.3 for the proof.
5 Discussions and Related Work
We have shown that containment analysis is intractable
in NRT , LRT , and SRT . This means that it is extremely
unlikely that we will ﬁnd an algorithm that is both sound
and complete, and also has a worst-case polynomial time
complexity. However, heuristic approaches are still possi-
ble. For example, it is not difﬁcult to extend our LP-based
approach for containment analysis in BRT to the case of
LRT and SRT , such that containment relationships in Ex-
ample 1 can be proved correctly. A possible approach is
to use a sound but incomplete method and a complete but
unsound method together to approximate the exact answer.
Such a heuristic approach may be useful in practice, as it
can give an exact answer in most cases. How to evaluate the
effectiveness of such methods is interesting future work.
On the other hand, we have shown that in our TM model,
simple safety queries can be solved efﬁciently. As discussed
in Section 1, security analysis in the form of simple safety
queries has been studied in the HRU model [9], and shown
to be undecidable there. In this section we study the rela-
tionships between the two models, arguing informally that
the HRU model does not include our TM model as a special
case, and showing that there is an intuitive reason why se-
curity analysis in our model is decidable. We also seek to
clarify the relationship between how trusted users are mod-
elled in the two approaches. After this discussion of related
work in safety analysis, we go on to discuss related work in
trust management.
5.1 Comparison with the HRU Access Matrix
Model
See Appendix A.3 for the proof. The coNP-hard part is
by reducing the monotone 3SAT problem, which is NP-
complete, to the complement of containment analysis in
NRT .
LRT adds to BRT type-3 statements. Linked roles in
type-3 statements add the ability to simulate logical con-
junction. Recall that the semantic rule for type-3 statements,
(m3), has a conjunction in the body, similar to that for type-
4 statements, (m4).
In the HRU model [9], a protection system has a ﬁnite
set of rights and a ﬁnite set of commands. A conﬁgura-
tion of a protection system is an access control matrix, with
rows corresponding to subjects, and columns corresponding
to objects; each cell in the matrix is a set of rights. A com-
mand takes the form of “if a list of conditions hold, execute
a sequence of primitive operations.” Each condition tests
whether a right exists in a cell in the matrix. There are six
kinds of primitive operations: enter a right into a speciﬁc
Proceedings of the 2003 IEEE Symposium on Security and Privacy (SP(cid:146)03) 
1081-6011/03 $17.00 ' 2003 IEEE 
9
cell in the matrix, delete a right from a cell in the matrix,
create a new subject, create a new object, destroy an ex-
isting subject, and destroy an existing object. A command
may be parameterized, with parameters being subjects or
objects.
In [9], Harrison et al. proved that for the HRU
model, the safety question is undecidable, by showing that
any Turing machine can be simulated by a protection sys-
tem. For a ﬁxed set of mono-operational commands, safety
can be determined in time polynomial of the size of the ac-
cess control matrix. However, if commands are a parameter
to the problem, the safety problem is NP-complete.
In our model, given a state P, the minimal Herbrand
model of SP(P) is a set of ground logical atoms. An atom
m(A, r, D) means that D is a member of A’s r role. When
A represents a resource, this can be viewed as D having the
right r over A. Therefore, one can view principals as both
subjects and objects and view role names as rights. This de-
ﬁnes a straightforward mapping between the semantics of P
and an access matrix. If all we have are type-1 statements,
then adding (or removing) A.r←− D corresponds to adding
(or removing) r to the cell on row D and column A. Adding
a type-2 statement A.r ←− B.r1 can be viewed as adding
a trigger program, which for each row D, use parameters
A, B, D to execute the following command: “a2(x, y, z)
{ if r1 in cell (y, z), add r to cell (x, z) }”. Note that
this trigger program needs to be executed whenever the ma-
trix changes. For example, if after A.r ←− B.r1 is added,
adding B.r1←− E will need to result in r being added to the
cell (A, E). The statement A.r←− B.r1 gives B the power
to add things to A’s column, which represents a delegation.
Similarly, adding a type-3 statement A.r←− A.r1.r2 can be
viewed as adding a trigger program that executes the follow-
ing command with parameters A, D, E for every D and E:
“a3(x, y, z) { if r1 in cell (x, y), and r2 in cell (y, z), add
r to cell (x, z) }”. Adding type-4 statement can be viewed
in a similar manner. It is not clear how to model removing
a statement using this approach.
There might be other ways of encoding our TM model
in the HRU access matrix model, but the above encoding
seems quite natural. From it, we make the following obser-
vations.
It seems unlikely that the HRU model subsumes the TM
model as a special case. First, in the TM model, creating and
removing principals are implicit. A principal can be viewed
as created if it is used. A principal is considered removed
if no statement mentions it. One could view the matrix as
having an inﬁnite number of rows and columns; however,
only ﬁnitely many cells are nonempty. Second, one step
of change in the TM model corresponds to executing many
(one for every object when adding a type-2 or 4 statement,
or one for every pair of objects when adding a type-3 state-
ment) simple commands in the HRU model. Third, triggers
need to be used in order to achieve the effect of propagation.
The last two are the main power of the TM model, and they
do not exist in the HRU model.
That our TM model cannot subsume the HRU model is
immediate from the complexity bounds. The underlying
reason is that the HRU commands we use to partially sim-
ulate our TM model have ﬁxed schemas, instead of being
arbitrary programs. As a result, we can exploit the prop-
erties of these ﬁxed schemas. This seems to be the main
reason that safety analysis, or the even more powerful con-
tainment analysis, is decidable in our model, but not in the
HRU model.
Handling Trusted Subjects
Intuitively, a speciﬁc protection system is “safe” if access to
resources without concurrence of the owner is impossible.
However, protection systems often allow the owner to share
rights to the resources. In that sense, they are not safe; the
HRU model uses a weaker notion of safety: a user should
be able to tell whether what he is about to do can lead to
the further leakage of that right to untrusted subjects. The
following is quoted from [9].
To avoid a trivial “unsafe” answer because s him-
self can confer generic right r, we should in most
circumstances delete s itself from the matrix. It
might also make sense to delete from the matrix
any other “reliable” subjects who could grant r,
but whom s “trusts” will not do so. It is only by
using the hypothetical safety test in this manner,
with “reliable” subjects deleted, that the ability
to test whether a right can be leaked has a use-
ful meaning in terms of whether it is safe to grant
a right to a subject.
Note that deleting a “reliable” subject from the matrix is
stronger than stopping it from granting a right. Deleting a
subject from the matrix will prevent the analysis from suc-
cessfully simulating the execution of commands that check
rights in the row or column corresponding the subject. How-
ever, it is inappropriate to ignore such commands: they may
add undesirable rights and they may be initiated by “unre-
liable” subjects. In such cases, a system that is safe after
the “reliable” subjects are removed is not safe in the actual
system, even if “reliable” subjects do not initiate any com-
mand.
In our TM model, the restriction rule R represents the in-
tuitive notion that certain principals are trusted. In practice,
principals are controlled by users. When principals repre-
sent resources, the controller is the subject who controls ac-
cess to the resource. When principals represent public keys,
the controller is the user who knows the private key.
Proceedings of the 2003 IEEE Symposium on Security and Privacy (SP(cid:146)03) 
1081-6011/03 $17.00 ' 2003 IEEE 
10
5.2 Related Work in Trust Management
To our knowledge, no prior work investigates secu-
rity analysis for trust management systems in the sense
of verifying security properties that consider state changes
in which (parametric) restrictions are placed on allowed
changes. In [3], a state transition model is used for com-
paring the expressive power of different access control
mechanisms such as access control lists and trust manage-
ment. There, security analysis is not the purpose. The
language SRT is closely related to SDSI, whose seman-
tics and evaluation has been the subject of many previous
works [1, 4, 8, 10, 12, 16]. One main difference our work