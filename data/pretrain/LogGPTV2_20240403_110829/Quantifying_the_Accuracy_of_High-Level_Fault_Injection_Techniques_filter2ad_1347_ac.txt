(12%)
268007691
(7%)
56928497
(8%)
31542955
(3%)
539958621
(4%)
PINFI
38227320
(11%)
827164028
(22%)
268007694
(12%)
57166980
(13%)
31542560
(6%)
539804535
(9%)
Load
LLFI
335748373
(69%)
3833040057
(54%)
2489538548
(61%)
357370593
(50%)
638292229
(60%)
5686126390
(43%)
PINFI
243088790
(70%)
2155207386
(57%)
1495918948
(65%)
242788525
(54%)
328446760
(58%)
3409330274
(55%)
(a) Arithmetic operation instructions
(b) Cast instructions
(c) Compare instructions
(d) Load instructions
Fig. 4: SDC Results for LLFI and PINFI. Subﬁgures (a) to (d) represent the SDC results for different instruction categories; and subﬁgure
(e) represented all instructions.
(e) All instructions
6
operations, which are translated to load instructions in
LLVM IR, and are considered as injection targets by
LLFI. However, assembly code moves the data directly
from one location to another using a mov instruction,
and does not have a corresponding load instruction.
Therefore, LLFI injects into more data operations and
hence has a higher SDC rate compared to PINFI.
D. RQ3: Crash results of LLFI and PINFI
Table V shows the percentage of crashes incurred by the
benchmark programs for LLFI and PINFI. From Table V,
we ﬁnd that LLFI and PINFI have similar crash percentages
for compare instructions. However, for the other categories
of instructions, there are considerable differences in the per-
centage of crashes. The maximum differences between the
two tools are as follows: 17% in the ‘all’ category (ocean),
40% in the ‘arithmetic’ category (bzip2), 32% in the ‘cast’
category (hmmer), and 21% in the ‘load’ category (hmmer).
In Section VII, we explain the reasons for the difference in
crash rates.
VII. DISCUSSION
The results of the fault injection experiments in Section VI
illustrate that
the SDC rates obtained with LLFI closely
match those obtained with PINFI for the benchmarks. This
shows that LLFI is a good choice if one’s interest is in SDC
causing errors, as ours is (recall that our goal is to study error
resilience, which is the ability of a program to prevent an
error from becoming an SDC). However, as far as crashes are
concerned, there are substantial differences between the two
injectors. We examine the reasons for the differences, and how
to resolve them (future work).
1) GetElementPtr
instructions: As mentioned earlier,
LLVM IR uses the getelementptr instruction to per-
form pointer address computations. However, at
the
assembly code level, pointer computations are performed
with regular arithmetic add/subtract/multiply instruc-
tions. On the face of it, it seems like this problem can
be solved by treating all getelementptr instructions
as equivalent
the LLVM
IR level. However, not all getelementptr instructions
are translated to arithmetic operations - some address
computations are compressed in the memory offset
computation part of the assembly language instruction.
To remove this discrepancy, we will need a heuristic to
decide when to treat a getelementptr instruction as an
arithmetic instruction and inject faults into only such
instructions.
to arithmetic operations at
2) Cast instructions: These contribute to inaccuracies when
they deal with pointer conversion as in the bzip2 bench-
mark. To remove this discrepancy, we will need to
identify such cases, and not inject faults into them at
the LLVM IR level.
3) Mov instructions: In assembly code, mov instructions
are used to move data both between registers and be-
tween registers and memory. In LLVM IR however, there
are separate instructions for these two operations, and
hence there are many more instructions corresponding
to mov instructions in the assembly code. To remove
this discrepancy, we need to inject
into only those
instructions that have a corresponding analogue at the
assembly code level.
VIII. RELATED WORK
We classify related work on fault injection into three broad
categories: (1) Program-level fault
injection for hardware
faults, (2) Assembly code level fault injection for hardware
faults, and (3) Fault injection for software faults.
Program-level fault injection for hardware faults: There
have been many attempts to build a fault injector for hardware
faults at the program level. Propane [10] is perhaps the ﬁrst
tool that injects faults at the program level and traces their
propagation in the program. Propane allows injection of both
hardware faults (data errors) and software faults. However, to
the best of our knowledge, the accuracy of Propane has not
been measured with regard to hardware fault injection.
Pattabiraman et al. [24] present an approach to selectively
protect critical data in a program by duplicating its back-
ward slice. Relax [2] is a code transformation technique to
tolerate soft errors in programs through structured blocks
and exception handling. Cong et al [5] use static analysis
to identify instructions that must be duplicated for protecting
soft-computing applications, or applications with relaxed cor-
rectness properties. Similar to LLFI, the authors of the above
papers develop fault injectors based on the LLVM compiler
to validate their technique. However, none of them validate
the fault injector itself with regard to its accuracy in injecting
hardware faults.
Thomas et al. [12] also build a static analysis technique
for identifying critical data in soft-computing applications to
protect against signiﬁcant deviations in the correct output, or
what they call Egregious Data Corruption (EDC). They also
perform fault injection at the LLVM compiler level. Unlike
the above papers, however, they provide a limited validation
of their injector with regard to EDC causing errors. However,
EDCs are only a (small) subset of Silent Data Corruptions
(SDCs), and their evaluation is conﬁned to soft-computing
applications. In contrast, we evaluate the accuracy of LLFI
for general-purpose applications, and for the full set of SDC
and crash causing errors.
Finally, in recent work, Sharma et al. present KULFI [11],
which stands for “Conﬁgurable Injector”. Like LLFI, KULFI
is built using the LLVM compiler infrastructure, and operates
on the IR code. To the best of our knowledge, KULFI has
not been validated with regard to assembly code level fault
injection. Further, the authors of KULFI use it to compare the
error resilience of algorithms for both SDC and crash causing
errors. However, as we have seen in this paper, performing
fault injections at the LLVM level may not be accurate for
crash causing errors, though we have not directly validated
KULFI’s accuracy for such faults.
fault
Note that ﬁve of the six papers above use the LLVM com-
piler and its infrastructure for performing their experiments.
This is also why we use LLVM for building LLFI.
Assembly code level
injection: There has been
substantial amount of work in fault-injection at the assembly
language level for emulating hardware faults. Examples of this
approach are NFTAPE [7], GOOFI-2 [8] and Xception [9].
NFTAPE uses break-point based injection at the machine code
level. GOOFI-2 supports three methods of fault
injection,
namely instrumentation-based, exception-based and Nexus-
based. All three methods operate at the assembly code (or
lower levels). Xception uses debug registers and features found
in many modern processors to inject faults at runtime. While
7
TABLE V: Crash percentage of the benchmark programs for LLFI and PINFI
Programs
LLFI
60%
bzip2
37%
mcf
hmmer
38%
libquantum 38%
33%
ocean
raytrace
44%
All
PINFI
64%
32%
41%
25%
23%
27%
arithmetic
PINFI
63%
19%
13%
4%
2%
1%
LLFI
23%
22%
20%
2%
11%
1%
LLFI
66%
0%
12%
0%
0%
22%
Cast
PINFI
96%
0%
44%
1%
0%
39%
LLFI
3%
3%
2%
1%
0%
3%
Cmp
PINFI
2%
2%
2%
0%
0%
4%
LLFI
64%
33%
36%
36%
37%
37%
Load
PINFI
74%
47%
57%
50%
43%
44%
Xception allows a high degree of conﬁgurability for the fault
injector, it also operates at the assembly language level.
A recent paper by Cho et al [25] evaluates the accuracy
of assembly code level fault injection versus injections at the
Register Transfer Language (RTL) level. They ﬁnd that single
bit ﬂips at the RTL level may manifest as multiple bit ﬂips
at the assembly code level. Unlike our work which attempts
to calibrate the accuracy of higher levels of fault injection
with respect to assembly language level injection, they are
interested in benchmarking the accuracy of the assembly level
injectors. Thus, their study is complementary to ours.
Fault injection for software faults: Techniques for inject-
ing software faults in programs typically operate at the source-
code level, or at levels close to the source code (e.g., on the
abstract syntax tree). G-SWiFT is a technique that attempts
to emulate software faults at the machine code level [15], by
identifying patterns of assembly code instructions correspond-
ing to high-level software constructs and injecting faults in
them to emulate software bugs. Because software bugs occur
primarily at the source code level, it is important to calibrate
the accuracy of assembly-code level injection techniques with
respect to the source code level. Cotroneo [16] perform one
such characterization and ﬁnd that injecting software faults at
the machine code level may not be representative of residual
software faults. Unlike software faults, hardware faults occur
within the microprocessor or memory and affect the program’s
execution. Because the executable ﬁle is in assembly/machine
language, hardware faults are easier to emulate at that level.
Thus, when injecting hardware faults at high level,
is
important to calibrate their accuracy with assembly code level
injections. This is the inverse of the problem that the above
software-fault injection papers face.
it
IX. CONCLUSION
In this paper, we quantitatively compare the accuracy of
high-level fault injection techniques with assembly code level
fault injection techniques for hardware faults. We develop two
fault injectors, LLFI to represent a high-level fault injector,
and PINFI, to represent a low-level fault injector. We compare
the accuracy of LLFI with PINFI with regard to crashes and
SDCs through fault-injection experiments on six benchmark
applications. Our results show that LLFI is highly accurate for
injecting SDC-causing errors, but not for crash causing errors,
compared to PINFI. Therefore, higher-level fault
injection
techniques are suitable for studying SDC-causing errors, but
not for studying crash-causing errors in programs.
ACKNOWLEDGEMENTS
This work was supported in part by a Discovery grant
from the Natural Science and Engineering Research Council
(NSERC), Canada, and a research gift from Lockheed Martin
Company. We thank the anonymous reviewers of DSN for their
comments that have helped to improve this paper.
8
REFERENCES
[1] S. Liu, K. Pattabiraman, T. Moscibroda, and B. G. Zorn, “Flikker: saving
dram refresh-power through critical data partitioning,” in ASPLOS, 2011,
pp. 213–224.
[2] M. de Kruijf, S. Nomura, and K. Sankaralingam, “Relax: an architectural
framework for software recovery of hardware faults,” in International
Symposium on Computer Architecture, 2010, pp. 497–508.
[3] S. Narayanan, J. Sartori, R. Kumar, and D. L. Jones, “Scalable stochastic
processors,” in DATE, 2010, pp. 335–338.
[4] A. Sampson, W. Dietl, E. Fortuna, D. Gnanapragasam, L. Ceze, and
D. Grossman, “EnerJ: Approximate data types for safe and general low-
power computation,” in PLDI, 2011, pp. 164–174.
[5] J. Cong and K. Gururaj, “Assuring application-level correctness against
soft errors,” in IEEE International Conference on Computer-Aided
Design, 2011, pp. 150–157.
[6] N. Nakka, K. Pattabiraman, and R. K. Iyer, “Processor-level selective
replication,” in DSN, 2007, pp. 544–553.
[7] D. T. Stott, B. Floering, D. Burke, Z. Kalbarczpk, and R. K. Iyer, “NF-
TAPE: a framework for assessing dependability in distributed systems
with lightweight fault injectors,” in International Computer Performance
and Dependability Symposium, 2000, pp. 91–100.
[8] D. Skarin, R. Barbosa, and J. Karlsson, “GOOFI-2: A tool for experi-
mental dependability assessment,” in DSN, 2010, pp. 557–562.
[9] R. Maia, L. Henriques, D. Costa, and H. Madeira, “XceptionTM -
enhanced automated fault-injection environment,” in DSN, 2002, pp.
547–550.
[10] M. Hiller, A. Jhumka, and N. Suri, “PROPANE: an environment for
examining the propagation of errors in software,” in International
Symposium on Software Testing and Analysis, 2002, pp. 81–85.
[11] V. C. Sharma, A. Haran, Z. Rakamaric, and G. Gopalakrishnan, “To-
wards Formal Approaches to System Resilience,” in PRDC, 2013.
[12] A. Thomas and K. Pattabiraman, “Error detector placement for soft
computation,” in DSN, 2013, pp. 1–12.
[13] C. Lattner and V. Adve, “LLVM: A compilation framework for lifelong
program analysis & transformation,” in CGO, 2004, pp. 75–86.
[14] C.-K. Luk, R. S. Cohn, R. Muth, H. Patil, A. Klauser, P. G. Lowney,
S. Wallace, V. J. Reddi, and K. M. Hazelwood, “Pin: building customized
program analysis tools with dynamic instrumentation,” in PLDI, 2005,
pp. 190–200.
[15] H. Madeira, D. Costa, and M. Vieira, “On the emulation of software
faults by software fault injection,” in DSN, 2000, pp. 417–426.
[16] R. Natella, D. Cotroneo, J. A. Duraes, and H. S. Madeira, “On fault
representativeness of software fault injection,” IEEE Transactions on
Software Engineering, vol. 39, pp. 80–96, 2013.
[17] K. Pattabiraman, N. Nakka, Z. Kalbarczyk, and R. Iyer, “SymPLFIED:
Symbolic program-level fault injection and error detection framework,”
in DSN, 2008, pp. 472–481.
[18] D. Skarin and J. Karlsson, “Software implemented detection and re-
covery of soft errors in a brake-by-wire system,” in EDCC, 2008, pp.
145–154.
[19] N. Oh, P. P. Shirvani, and E. J. McCluskey, “Error detection by
duplicated instructions in super-scalar processors,” IEEE Transactions
on Reliability, vol. 51, pp. 63–75, 2002.
[20] S. K. S. Hari, S. V. Adve, and H. Naeimi, “Low-cost program-level
detectors for reducing silent data corruptions,” in DSN, pp. 181–188.
[21] J. L. Henning, “SPEC CPU2006 benchmark descriptions,” ACM Sigarch
Computer Architecture News, vol. 34, pp. 1–17, 2006.
[22] S. C. Woof, M. Ohara, E. Torriet, J. P. Singhi, and A. Guptat, “The
SPLASH2 programs: characterization and methodological considera-
tions,” in ISCA, 1995, pp. 24–36.
[23] S. K. S. Hari, S. V. Adve, H. Naeimi, and P. Ramachandran, “Relyzer:
exploiting application-level fault equivalence to analyze application
resiliency to transient faults,” in ASPLOS, 2012, pp. 123–134.
[24] K. Pattabiraman, Z. Kalbarczyk, and R. K. Iyer, “Automated derivation
of application-aware error detectors using static analysis: The trusted
illiac approach,” in TDSC, vol. 8, 2011, pp. 44–57.
[25] H. Cho, S. Mirkhani, C.-Y. Cher, J. Abraham, and S. Mitra, “Quantitative
evaluation of soft error injection techniques for robust system design,”
in Design Automation Conference (DAC), 2013, pp. 1–10.