timeout actually behaves like a probabilistic checkpoint-abort 
mechanism. The probability depends on the coordination time 
(MTTQ and number of compute processors) and the timeout. 
Small  timeouts  incur  large  probabilities  of  checkpoint  abor-
tion, and the benefit of limiting  the processors’  waiting time 
is offset by the loss of work due to frequent checkpoint abor-
tions. The drastic curve drops for timeouts of 20-100 seconds 
in Figure 6 clearly show the performance degradation.   
Figure 6 also shows that the system is insensitive to time-
outs, provided they are large enough, because the overall co-
ordination  time  increases  slowly  with  the  number  of  proces-
sors. For example, the 8192-processor system’s performance 
with a timeout of 100s is only slightly better than a timeout of 
120s and no timeout. 
7.3 Effect of Correlated Failures 
We  recall  that  there  are  two  categories  of  correlated  fail-
ures considered in the paper.   
probability 
Correlated failures due to error propagation only. Corre-
lated failures due to error propagation are modeled with three 
(pe),
parameters: 
frate_correlated_factor  (r,)  and  correlated  failure  window.
As shown in Section 6, a typical value of r in real systems is 
on the order of a few hundred. In our experiments, r values of 
400, 800, and 1600 are used for various pe values, with a cor-
related failure window of 3 minutes.   
correlated 
failure 
of 
Useful work fraction with coordination 
(checkpoint interval=30min)
Useful w ork fraction with coordination and timeout 
(MTTF per node=3yrs, checkpoint interval=30min)
MTTQ=0.5s
MTTQ=2s
MTTQ=10s
0.98
0.96
0.94
0.92
0.90
0.88
0.86
0.84
0.82
0.80
n
o
i
t
c
a
r
f
k
r
o
w
l
u
f
e
s
U
1
4
16
64
256
1024
4096
1638 4
6553 6
2621 44
number of processors
1048 576
4194 304
1677 7216
6710 8864
2684 35456
1.074 E +09
Figure 5 : Effects of coordination 
on system performance and scal-
ability (no timeouts or failures) 
Useful w ork fraction (MTTF per node=3yrs, number of 
processors=256K, correlated failure window =3min)
1 .00 0
0 .90 0
0 .80 0
0 .70 0
0 .60 0
0 .50 0
0 .40 0
0 .30 0
0 .20 0
0 .10 0
0 .00 0
n
o
i
t
c
a
r
f
k
r
o
w
l
u
f
e
s
U
no coordination
timeout=
120s
timeout=
80s
timeout=
100s
no timeout
timeout=60s
40s
20s
8192
1 6384
32768
65536
131072
262144
number of processors
Figure 6 : Effects of coordination 
timeout on system performance 
and scalability (with failures)
Useful work fraction (MTTF per node=3yrs, 
correlated failure 
coefficient=0.0025,correlated failure 
factor=400, checkpoint interval=30min)
0.560
0.550
0.540
0.530
0.520
0.510
0.500
0.490
n
o
i
t
c
a
r
f
k
r
o
w
l
u
f
e
s
U
frate_correlated_times=400
frate_correlated_times=800
frate_correlated_times=1600
1.00
0.90
0.80
0.70
0.60
0.50
0.40
0.30
0.20
0.10
0.00
n
o
i
t
c
a
r
f
k
r
o
w
l
u
f
e
s
U
without correlated 
failure
with correlated 
failure
0.000
0.050
0.100
0.150
0.200
8192
16384
32768
65536
131072
262144
Prob. of correlated failure
number of processors
Figure 7 : Impact of correlated 
Figure 8 : Impact of generic cor-
failures due to error propagation 
related failures 
The results of correlated failures in Figure 7 show that the 
useful  work  fraction  is  not  susceptible  to  correlated  failures 
due  to  error  propagation  (ranging  between  0.51  and  0.56  in 
the  figure).    This  is  because  we  assume  these  failures  only 
occur  during  recovery,  and  we  observed  that  failures  during 
recovery do not exert a significant effect on the  useful  work 
fraction.   
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Generic  correlated  failures.  Generic  correlated  failures 
are  modeled  with  two  parameters:  correlated  failure  factor 
(r)  and  correlated  failure  coefficient  ((cid:545)).  An  r value  of  400 
and (cid:545) value of 0.0025 are used in our experiment. Therefore, 
the entire system failure rate gets doubled because of generic 
correlated  failures.  The  results  illustrated  in  Figure  8  show 
that, unlike correlated failures due to error propagation, there 
is  a  large  performance  degradation  when  generic  correlated 
failures  are  present,  and  the  performance  degradation  pre-
vents  the  system  from  scaling  well.  For  a  system  consisting 
of  256K  processors  with  an  MTTF  of  3  years  per  node,  the 
useful work fraction is reduced by 0.24 (51%). 
8. Conclusions   
This  paper  models  a  large-scale  supercomputing  system 
with  coordinated  checkpointing  and  rollback  recovery. 
Unlike  existing  models  in  the  literature,  failures  during 
checkpointing/recovery,  coordination  for  checkpointing,  and 
correlated  failures  are  included  in  the  model.  The  impact  of 
these factors on system performance (measured as the useful 
work fraction and total useful work) as well as the scalability 
of systems with several hundred thousand processors is stud-
ied by simulating the model. The major conclusions from this 
study include: 
• For  a  given  checkpoint  interval,  MTTR,  and  MTTF, 
there is an optimum number of processors for which total 
useful  work  done  by  the  system  is  maximized,  e.g.,  for 
an  MTTF  per  node  of  1  year  and  an  MTTR  of  10  min-
utes, it is around 128K. 
• The overall the useful work fraction is relatively low due 
to the effect of failures in large-scale systems. 
• Correlated  failures  must  be  taken  into  account,  as  they 
degrade the performance and limit system scalability. 
Acknowledgments 
This  work  was  supported 
in  part  by  NSF  grant 
ACI-0121658  ITR/AP  and  NSF  grant  ACI-CNS-0406351 
(Next  Generation  Software).  We  thank  Prof.  W.  Sanders  for 
his  suggestions  on  SAN  modeling.  We  also  thank  Dr.  B. 
Murphy for his advice and suggestions on the paper revision. 
References 
[1] N.R. Adiga et al., “An Overview of the Blue Gene/L,” Proc. of 
IEEE Int’l Conference on Supercomputing, 2002. 
[2] J. S. Plank, "An Overview of Checkpointing in Uniprocessor and 
Distributed  Systems,  Focusing  on  Implementation  and  Perform-
ance,” Technical Report of University of Tennessee, UT-CS, 1997. 
[3] M. Chandy, L. Lamport. “Distributed Snapshots: Determinining 
Global  States  of  Distributed  Systems,”  ACM  Trans.  on  Computing 
Systems, 3(1), 1985. 
[4]  R.  Koo,  S.  Toueg,  “Checkpointing  and  Recovery  Rollback  for 
Distributed  Systems,” IEEE  Trans.  on  Software  Engineering,  Vol. 
SE-13, No. 1, 1987. 
[5]  F.  Petrini,  K.  Davis,  J.  C.  Sancho,  “System  Level  Fault  Toler-
ance in Large-Scale Parallel Machines,” Proc. of IEEE Int’l Parallel 
and Distributed Processing Symp. (IPDPS'04), 2004. 
[6] D. Tang, R. K. Iyer, “Analysis and Modeling of Correlated Fail-
ures  in  Multicomputer  Systems,”  IEEE  Trans.  on  Computers,  Vol. 
41, Num. 5, 1992. 
[7]  J.  W.  Young,  “A  First  Order  Approximation  to  the  Optimum 
Checkpoint  Interval,”  Communications  of  the  ACM,  Vol.  17,  Num. 
9, 1974. 
[8]  J.  Daly,  “A  Model  for  Predicting  the  Optimum  Checkpoint  In-
terval  for  Restart  Dumps,”  Proc.  of  Int’l  Conference  on  Computa-
tional Science, 2003. 
[9] G. P. Kavanaugh, W. H. Sanders, “Performance Analysis of Two 
Time-based  Coordinated  Checkpointing  Protocols,”  Proc.  of  IEEE 
Pacific Rim Int’l Symp. on Fault Tolerant Systems, 1997. 
[10]  J.  S.  Plank,  M.  G.  Thomason,  “The  Average  Availability  of 
Parallel  Checkpointing  Systems  and  Its  Importance  in  Selecting 
Runtime  Parameters,”  IEEE  Proc.  Int’l  Symp.  on  Fault-Tolerant 
Computing, 1999. 
[11]  E.  N.  Elnozahy,  J.  S. Plank,  W.  K.  Fuchs,  “Checkpointing  for 
Peta-Scale  Systems:  A  Look  into  the  Future  of  Practical  Roll-
back-Recovery,”  IEEE  Trans.  on  Dependable  and  Secure  Comput-
ing, Vol. 1, Num. 2, 2004. 
[12] N. H. Vaidya, “On Checkpoint Latency,” Proc. of IEEE Pacific 
Rim Int’l Symp. on Fault-Tolerant Systems, 1995. 
[13]  L.  G.  Valiant,  “A  Bridging  Model  for  Parallel  Computation” 
Communications of the ACM, Vol. 33, 1990   
[14]  E.  Smirni,  D.  A.  Reed,  “Workload  Characterization  of  In-
put/Output  Intensive  Parallel  Applications,”  Proc.  of  Int’l  Confer-
ence  on  Computer  Performance  Evaluation:  Modeling  Techniques 
and Tools, 1997. 
[15]  E.  Rosti,  et  al.,  “Models  of  Parallel  Applications  with  Large 
Computation and I/O Requirements,” IEEE Trans. on Software En-
gineering, Vol.28, Num. 3, 2002. 
[16] D. P. Siewiorek, R. S. Swarz, Reliable Computer Systems: De-
sign and Evaluation, 2nd ed., Digital Press, 1992. 
[17]  G.  Kulkarni,  V.  F.  Nicola,  K.  S.  Trivedi,  “The  Completion 
Time of a Job on Multimode Systems,” Advances in Applied Prob-
ability, Vol. 19, 1987. 
[18]  Y.  Zhang,  et  al.,  "Performance  Implications  of  Failures  in 
Large-Scale Cluster Scheduling,” 10th Workshop on Job Scheduling 
Strategies for Parallel Processing, 2004. 
[19]  B.  Tuthill,  et  al.  “IRIX  Checkpoint  and  Restart  Operation 
Guide,” Document of Silicon Graphics, Inc., 1999. 
[20]  R.  Iyer,  D.  Rossetti,  "A  Measurement-based  Model  for  Work-
load Dependence of CPU Errors,” IEEE Trans. on Computers, Vol. 
C-35, 1986. 
[21]T. Courtney et al., “The Möbius Modeling Environment,” Tools 
of  the  2003  Illinois  Int’l  Multiconference  on  Measurement,  Model-
ling, and Evaluation of Computer Communication Systems, Univer-
sität  Dortmund  Fachbereich  Informatik  research  report  no.  781, 
2003. 
[22]  L.  Spainhower,  T.  A.  Gregg,  “IBM  S/390  Parallel  Enterprise 
Server G5 Fault Tolerance: A Historical Perspective,” IBM Journal 
of Research and Development, Vol. 43, Num. 5/6, 1999. 
[23]  G.  Bronevetsky  et  al.,  “Automated  Application-level  Check-
pointing of MPI Programs,” Proc. of ACM SIGPLAN Symposium on 
Principles and Practice of Parallel Programming, 2003. 
[24]  S.  Agarwal  et  al.,  “Adaptive  Incremental  Checkpointing  for 
Massively  Parallel  Systems,”  Proc.  of  IEEE  Int’l  Conference  on 
Supercomputing, 2004. 
[25]  L.  Wang  et  al.,  “Modeling  Coordinated  Checkpointing  for 
Large-Scale  Supercomputers,”  Technical  Report  of  University  of 
Illinois, 2005. 
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE