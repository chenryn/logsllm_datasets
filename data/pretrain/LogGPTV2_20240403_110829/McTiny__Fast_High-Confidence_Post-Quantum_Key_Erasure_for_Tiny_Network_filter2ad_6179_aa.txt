title:McTiny: Fast High-Confidence Post-Quantum Key Erasure for Tiny Network
Servers
author:Daniel J. Bernstein and
Tanja Lange
McTiny: Fast High-Confidence Post-Quantum 
Key Erasure for Tiny Network Servers
Daniel J. Bernstein, University of Illinois at Chicago, Ruhr University Bochum; 
Tanja Lange, Eindhoven University of Technology
https://www.usenix.org/conference/usenixsecurity20/presentation/bernstein
This paper is included in the Proceedings of the 29th USENIX Security Symposium.August 12–14, 2020978-1-939133-17-5Open access to the Proceedings of the 29th USENIX Security Symposium is sponsored by USENIX.McTiny:
Fast High-Conﬁdence Post-Quantum Key Erasure
for Tiny Network Servers
Daniel J. Bernstein
University of Illinois at Chicago,
Ruhr University Bochum
Tanja Lange
Eindhoven University of Technology
Abstract
1
Introduction
Recent results have shown that some post-quantum cryp-
tographic systems have encryption and decryption perfor-
mance comparable to fast elliptic-curve cryptography (ECC)
or even better. However, this performance metric is con-
sidering only CPU time and ignoring bandwidth and stor-
age. High-conﬁdence post-quantum encryption systems have
much larger keys than ECC. For example, the code-based
cryptosystem recommended by the PQCRYPTO project uses
public keys of 1MB.
Fast key erasure (to provide “forward secrecy”) requires
new public keys to be constantly transmitted. Either the server
needs to constantly generate, store, and transmit large keys, or
it needs to receive, store, and use large keys from the clients.
This is not necessarily a problem for overall bandwidth, but it
is a problem for storage and computation time on tiny network
servers. All straightforward approaches allow easy denial-of-
service attacks.
This paper describes a protocol, suitable for today’s net-
works and tiny servers, in which clients transmit their code-
based one-time public keys to servers. Servers never store full
client public keys but work on parts provided by the clients,
without having to maintain any per-client state. Intermediate
results are stored on the client side in the form of encrypted
cookies and are eventually combined by the server to obtain
the ciphertext. Requirements on the server side are very small:
storage of one long-term private key, which is much smaller
than a public key, and a few small symmetric cookie keys,
which are updated regularly and erased after use. The proto-
col is highly parallel, requiring only a few round trips, and
involves total bandwidth not much larger than a single public
key. The total number of packets sent by each side is 971,
each ﬁtting into one IPv6 packet of less than 1280 bytes.
The protocol makes use of the structure of encryption in
code-based cryptography and beneﬁts from small ciphertexts
in code-based cryptography.
TLS 1.3 highlights the importance of “forward secrecy” by
switching completely to Difﬁe–Hellman-based cryptography
for conﬁdentiality. The client initiates the connection and al-
ready on the ﬁrst message sends the preferred cipher suite and
a public key. These systems are typically based on elliptic
curves though some ﬁnite-ﬁeld options remain. Elliptic-curve
keys consume only 32–64 bytes and thus add very little over-
head to the packets and computation is very fast, even on
small devices.
Unfortunately, if large quantum computers are built then
Shor’s quantum algorithm [33] breaks ECC in polynomial
time. In the two decades since Shor found this quantum
speedup, research in cryptography has progressed to ﬁnd sys-
tems that remain secure under attacks with quantum comput-
ers. There are several approaches to designing such post-
quantum systems but the main categories for public-key
encryption systems are based on codes, lattices, or—more
recently—isogenies between supersingular elliptic curves.
Code-based cryptography [23] was invented by McEliece
in 1978 and is thus just one year younger than RSA and
has held up much stronger against cryptanalysis than RSA.
Lattice-based cryptography started more than 15 years later
and security estimates are still developing; see, e.g., the re-
cent paper [1] claiming a 400× speedup in lattice attacks.
Isogenies in their current use started only in 2011 [17].
In 2015, the European project PQCRYPTO issued recom-
mendations [2] for conﬁdence-inspiring post-quantum sys-
tems; for public-key encryption the only recommended sys-
tem was a code-based system which is closely related to
McEliece’s original proposal. However, when in 2016 Google
ran an experiment [8] deploying post-quantum cryptography
to TLS connections between Chrome (Canary) browsers and
Google sites they did not choose a code-based system but a
much more recent system based on lattices. The main issue
with the high-conﬁdence code-based system is that it requires
a much larger key size—1MB vs. 1kB—for the same esti-
mated level of security. Google piggybacked the lattice-based
USENIX Association
29th USENIX Security Symposium    1731
ARTIFACTEVALUATEDPASSEDsystem with ECC so that the security of the combined system
would not be weaker than a pure ECC system.
In April 2018, Google’s Langley reported [21] on another
experiment with post-quantum cryptography, this time test-
ing no particular system but different key sizes. They tested
initiation packets of sizes 400, 1100, and 10000 bytes, saying
that these are meant to represent systems based on isogenies
and based on different types of lattices. Langley justiﬁed their
choice by writing “in some cases the public-key + cipher-
text size was too large to be viable in the context of TLS”.
There were too many sites that dropped the largest size so it
was skipped from further experiments and replaced by 3300
bytes. For these sizes they measured the increase in latency.
In a second experiment they measured round-trip times, this
time even skipping the 3300-byte size. These sizes are a far
cry from what is needed to transmit the 1MB keys of the
McEliece cryptosystem. See also the failure reported in [12]
to handle 300KB keys in OpenSSL.
For the experiments it is reasonable to use new systems in
combination with ECC; see [18–20] for a new lattice-plus-
ECC experiment by Google and Cloudﬂare. However, this
does not help with post-quantum security if lattices turn out
to be much weaker than currently assumed. This raises the
question how network protocols could possibly use the high-
conﬁdence McEliece cryptosystem.
Of course, the key could be chopped into pieces and sent in
separate packets and the server could be instructed to buffer
the pieces and reassemble the pieces but this allows rogue
clients to ﬂood the RAM on the server. See Section 2.
This paper introduces McTiny, a new protocol that solves
the problem of memory-ﬂooding denial-of-service attacks
for code-based cryptography. McTiny handles the 1MB keys
of the McEliece cryptosystem, having the same basic data
ﬂow as TLS in which the client creates a fresh public key for
each connection and sends it as the ﬁrst step of the protocol.
McTiny splits the public keys into pieces small enough to ﬁt
into network packets. On the client side the overhead is small
compared to creating the key and sending 1MB. The server is
not required to allocate any memory per client and only ever
needs to process information that ﬁts into one Internet packet,
making McTiny suitable for tiny network servers.
Sections 2 and 3 motivate tiny network servers and review
existing results. Section 4 gives background in coding the-
ory. Sections 5 and 6 explain our new McTiny protocol. We
analyze cryptographic security in Sections 7–9 and present
our software implementation and evaluation in Section 10.
Finally we consider some alternative choices.
2 Server-memory Denial of Service, and the
Concept of Tiny Network Servers
Most—but not all!—of today’s Internet protocols are vulner-
able to low-cost denial-of-service attacks that make a huge
number of connections to a server. These attacks ﬁll up all
of the memory available on the server for keeping track of
connections. The server is forced to stop serving some con-
nections, including connections from legitimate clients. These
attacks are usually much less expensive than comparably ef-
fective attacks that try to use all available network bandwidth
or that try to consume all available CPU time.
2.1 A Classic Example: SYN Flooding
The “SYN ﬂooding” denial-of-service attack [14] rose to
prominence twenty years ago when it was used to disable an
ISP in New York, possibly in retaliation for anti-spam efforts;
see [9]. “SYN cookies” [4] address SYN ﬂooding, but from a
broader security perspective they are highly unsatisfactory, as
we now explain.
Recall that in a normal TCP connection, say an HTTP
connection, the client sends a TCP “SYN” packet to the server
containing a random 32-bit initial sequence number (ISN);
the server sends back a “SYNACK” packet acknowledging
the client ISN and containing another random ISN; the client
sends an “ACK” packet acknowledging the server ISN. At this
point a TCP connection is established, and both sides are free
to send data. The client sends an HTTP request (preferably as
part of the ACK packet), and the server responds.
The server allocates memory to track SYN-SYNACK pairs,
including IP addresses, port numbers, and ISNs. This is ex-
actly the memory targeted by SYN ﬂooding. A SYN-ﬂooding
attacker simply sends a stream of SYNs to the server with-
out responding to the resulting SYNACKs. Once the SYN-
SYNACK memory ﬁlls up, the server is forced to start throw-
ing away some SYN-SYNACK pairs, and is no longer able to
handle the corresponding ACKs. The server can try to guess
which SYN-SYNACK pairs are more likely to be from le-
gitimate clients, and prioritize keeping those, but a sensible
attacker will forge SYNs that look just like legitimate SYNs.
If the server has enough SYN-SYNACK memory for c con-
nections, but is receiving 100c indistinguishable SYNs per
RTT, then a legitimate client’s ACK fails with probability at
least 99%.
SYN cookies store SYN-SYNACK pairs in the network
rather than in server memory. Speciﬁcally, the server encodes
its SYN-SYNACK pair as an authenticated cookie inside the
SYNACK packet back to the client, and then forgets the SYN-
SYNACK pair (if it is out of memory or simply does not
want to allocate memory). The client sends the cookie back
in its ACK packet. The server veriﬁes the authenticator and
reconstructs the SYN-SYNACK pair.1
1For compatibility with unmodiﬁed clients, the server actually encodes
a very short authenticator inside the choice of server ISN. Modifying both
the client and the server would have allowed a cleaner protocol with a longer
authenticator. In this paper we are prioritizing security and simplicity above
compatibility, so we do not compromise on issues such as authenticator
length.
1732    29th USENIX Security Symposium
USENIX Association
2.2 Why Stopping SYN Flooding is Not
Enough
SYN cookies eliminate the server’s SYN-SYNACK memory
as a denial-of-service target: a forged SYN simply produces
an outgoing SYNACK2 and does not interfere with legitimate
clients. But what happens if the attacker continues making a
connection, not merely sending a SYN but also responding to
the resulting SYNACK and sending the beginning of an HTTP
GET request? The server allocates memory for the established
TCP connection and for the HTTP state, much more memory
than would have been used for a SYN-SYNACK pair. The
attacker leaves this connection idle and repeats, consuming
more and more server memory. Again the server is forced to
start throwing away connections.
There is some entropy in the SYNACK that needs to be
repeated in the ACK. An attacker who sends blind ACKs will
only rarely succeed in making a connection, and these occa-
sional connections will time out before they ﬁll up memory.
However, an on-path attacker, an attacker who controls any of
the machines that see the SYNACKs on the wire or in the air,
has no trouble forging ACKs. Forcing attackers to be on-path
might deter casual attackers but will not stop serious attackers
(see, e.g., [15]).
2.3 Tiny Network Servers
As mentioned above, not all Internet protocols are vulnerable
to these memory-ﬁlling denial-of-service attacks. Consider,
for example, a traditional DNS server running over UDP. This
server receives a UDP packet containing a DNS query, imme-
diately sends a UDP packet with the response, and forgets the
query. A careful implementation can handle any number of
clients without ever allocating memory.
DNS has an optional fallback to TCP for responses that
do not ﬁt into a UDP packet. However, at many sites, all
DNS responses are short. Clients requesting information from
those sites do not need the TCP fallback;3 an attacker denying
TCP service will not deny DNS service from those sites. The
bottom line is that DNS can, and at some sites does, serve any
number of clients using a constant amount of server memory.
Another classic example is NFS, Sun’s Network File Sys-
tem [28]. NFS (without locks and other “stateful” features)
was explicitly designed to allow “very simple servers” for
robustness [28, Section 1.3]:
The NFS protocol was intended to be as stateless
as possible. That is, a server should not need to
2Amplifying a packet into a larger packet raises other denial-of-service
concerns, but the outgoing SYNACK is not much larger than the SYN.
3DNS-over-TCP was also in heavy use for an obsolete ad-hoc high-latency
low-security replication mechanism (periodic client-initiated “DNS zone
transfers”), but anecdotal evidence suggests that most sites have upgraded to
more modern push-style server-replication mechanisms, for example using
rsync over ssh.
maintain any protocol state information about any
of its clients in order to function correctly. State-
less servers have a distinct advantage over state-
ful servers in the event of a failure. With stateless
servers, a client need only retry a request until the
server responds; it does not even need to know that
the server has crashed, or the network temporarily
went down. The client of a stateful server, on the
other hand, needs to either detect a server failure
and rebuild the server’s state when it comes back
up, or cause client operations to fail.
This may not sound like an important issue, but it
affects the protocol in some unexpected ways. We
feel that it may be worth a bit of extra complexity in
the protocol to be able to write very simple servers
that do not require fancy crash recovery.
An NFS server receives, e.g., a request to read the 7th block
of a ﬁle, returns the contents of the block, and forgets the
request. An important side effect of this type of server design
is that malicious clients cannot ﬁll up server memory.
This paper focuses on tiny network servers that handle
and immediately forget each incoming packet, without allo-
cating any memory. The most obvious application is making
information publicly available, as in traditional DNS, anony-
mous read-only NFS, and anonymous read-only HTTP; as
DNS and NFS illustrate, it is possible to design protocols that
handle this application with tiny network servers. The concept
of a tiny network server also allows more complicated com-
putations than simply retrieving blocks of data. Tiny network
servers are not necessarily connectionless, but the requirement
of forgetting everything from one packet to the next means
that the server has to store all connection metadata as cookies
in the client. Tiny servers are not necessarily stateless, and
in fact the protocol introduced in this paper periodically up-
dates a small amount of state to support key erasure (“forward
secrecy”), but we emphasize that this is not per-client state.
Tiny network servers are compatible with reliable delivery
of data despite dropped packets: for example, DNS clients
retry requests as often as necessary. Tiny network servers
are also compatible with congestion control, again managed
entirely by the client. Tiny network servers provide extra
robustness against server power outages; trivial migration
of connections across high-availability clusters of identically
conﬁgured servers; and the ability to run on low-cost “Internet
of Things” platforms.
3 The Tension Between Tiny Network Servers
and Further Security Requirements
The obvious security advantage of designing a protocol to al-
low tiny network servers—see Section 2—is that these servers
are immune to server-memory denial of service.
USENIX Association
29th USENIX Security Symposium    1733
What is not clear, however, is that tiny network servers are
compatible with other security requirements. The pursuit of
other security requirements has created, for example, DNS
over TLS and DNS over HTTPS, and all implementations of
these protocols allow attackers to trivially deny service by
ﬁlling up server memory, while the original DNS over UDP
allows tiny network servers that are not vulnerable to this
attack.
In this section we highlight three fundamental security
requirements, and analyze the difﬁculty of building a tiny
network server that meets these requirements. We explain
how to combine and extend known techniques to handle the
ﬁrst two requirements. The main challenge addressed in the
rest of this paper is to also handle the third requirement, post-
quantum security.
3.1 Requirements
Here are the three requirements mentioned above:
• We require all information to be encrypted and authen-
ticated from end to end, protecting against interception
and forgery by on-path attackers.
• We require keys to be erased promptly, providing some
“forward secrecy”. For comparison, if key erasure is slow,
then future theft of the server (or client) allows an at-
tacker to trivially decrypt previously recorded ciphertext.
• We require cryptography to be protected against quan-
tum computers.
Typical cryptographic protocols such as HTTPS handle the
ﬁrst two requirements, and are beginning to tackle the third.
However, these protocols create several insurmountable obsta-
cles to tiny network servers. For each active client, the server
has to maintain per-client state for a TCP connection, plus
per-client state for a TLS handshake followed by TLS packet
processing, plus per-client state for HTTP.
We therefore scrap the idea of staying compatible with
HTTPS. We instead focus on the fundamental question of
whether—and, if so, how—a tiny network server can provide
all of these security features.
3.2 Cookies Revisited
One approach is as follows. Aura and Nikander [3] claim to
straightforwardly “transform any stateful client/server proto-
col or communication protocol with initiator and responder
into a stateless equivalent”, and give some examples. The
“Trickles” network stack from Shieh, Myers, and Sirer [31,32]
stores all of the server’s TCP-like metadata as a cookie, and
also provides an interface allowing higher-level applications
to store their own state as part of the cookie. Why not apply
the same generic transformation to the entire per-connection
HTTPS server state X, straightforwardly obtaining a higher-
availability protocol where a tiny network server stores X as
a cookie on the client?
The problem with this approach, in a nutshell, is packet
size. These papers assume that a client request and a cookie ﬁt
into a network packet. Consider, for example, the following
comment from Shieh, Myers, and Sirer: “Of course, if the
server needs lengthy input from the client yet cannot encode
it compactly into an input continuation, the server application
will not be able to remain stateless.”
Concretely, the Internet today does not reliably deliver
1500-byte packets through IPv4, and does not reliably de-
liver 1400-byte packets through IPv6 (even when IPv6 is
supported from end to end). Normally the lower layer actually
delivers 1500-byte packets, but tunnels sometimes reduce the
limit by a few bytes for IPv4, and usually reduce the limit by
more bytes for IPv6; see, e.g., [29] and [22].
These limits are actually on fragment size rather than end-
to-end packet size. Why not split larger packets into frag-
ments? The answer is that this is unacceptable for a tiny net-
work server. Fragments often take different amounts of time
to be delivered, so the server is forced to allocate memory for
fragments that have not yet been reassembled into packets.
This memory is a target of denial-of-service attacks. The only
safe solution is to limit the packet size to the fragment size.
IPv6 guarantees that 1280-byte packets (and smaller pack-
ets) can be sent from end to end, without fragmentation. This
guarantee simpliﬁes protocol design. Historically, some net-
work links had even smaller packet-size limits, and technically
the IPv4 standard still allows routers to split packets into much
smaller fragments, but it is difﬁcult to ﬁnd evidence of prob-
lems with 1280-byte packets on the Internet today. This paper
focuses on clients and servers connected by a network that
delivers 1280-byte packets.
It is not entirely inconceivable that all essential details of an
HTTPS state could be squeezed into such a small packet, with
enough restrictions and modiﬁcations to HTTPS. But contin-
uing down this path would clearly be much more work than
directly designing a cryptographic protocol for tiny network
servers.
3.3 ECC For Tiny Network Servers
We instead start from an existing special-purpose crypto-
graphic protocol that does work with tiny network servers,
namely Bernstein’s DNSCurve [5]. This protocol takes ad-
vantage of the small size of public keys in elliptic-curve cryp-
tography (ECC), speciﬁcally 32 bytes for Curve25519.
A DNSCurve client starts with knowledge of the server’s
long-term public key sG, previously retrieved from a parent
DNS server. Here s is an integer, the server’s secret key; G is
a standard elliptic-curve point; and sG is the output of a math-
ematical operation, called elliptic-curve scalar multiplication,
whose details are not relevant to this paper. The client gener-
1734    29th USENIX Security Symposium
USENIX Association
ates its own public key cG, and sends a packet to the server
containing cG and the ciphertext for a DNS query. The server
immediately responds with the ciphertext for a response, and
forgets the query. Both ciphertexts are encrypted and authen-
ticated under a shared secret key, a 256-bit hash of the point
csG; the server computes csG from s and cG, and the client
computes csG from c and sG. The client knows that the re-
sponse is from the server: the shared secret key is known only
to the client and the server, and nobody else can generate a
valid authenticator.
We highlight two limitations of DNSCurve compared to
HTTPS, and explain how to ﬁx these. First, each public-key
handshake in DNSCurve handles only one query packet and
one response packet, while one HTTPS handshake is typically
followed by a web page, often 1000 packets or more.
A conceptually straightforward ﬁx is to carry out a separate
DNSCurve-style query for each block of a web page. ECC
public keys are small, so the trafﬁc overhead is small; ECC
operations are very fast, so there is no problem in CPU time.
However, our goal is actually to upgrade to post-quantum
cryptography, which uses much larger keys, creating perfor-
mance problems; see below.
A more efﬁcient ﬁx is for the server to encrypt and authen-
ticate the 256-bit shared secret key under a key known only
to the server, obtaining a cookie. The server includes this
cookie in the response to the client. The server then accepts
this cookie as an alternative to cG, allowing the client to carry
out subsequent queries without sending cG again.
The second limitation that we highlight is the lack of for-
ward secrecy in DNSCurve. DNSCurve clients can erase keys
promptly, for example discarding cG after one connection,
but it is not so easy for a DNSCurve server to move from one
long-term key to another: this requires uploading the new key
to the parent DNS server, something that could be automated
in theory but that is rarely automated in practice.
One ﬁx is for the client to encrypt its conﬁdential query
only to a short-term server key, rather than to the server’s
long-term key. This takes two steps: ﬁrst the client issues
a non-conﬁdential query asking for the server’s short-term
public key; then the client issues its conﬁdential query to the
server’s short-term public key. The server frequently replaces
its short-term public key with a new short-term public key,
erasing the old key.
What makes these protocols easy to design, within the
constraint of tiny network servers, is the fact that ECC keys
and ciphertexts ﬁt into a small corner of a network packet.
The question addressed in the rest of this paper is whether
tiny network servers can achieve encryption, authentication,
and key erasure for much larger post-quantum keys.
4 Code-Based Cryptography
McEliece introduced code-based cryptography in 1978
in [23]. The system uses error correcting codes and the public
and private keys are different representations of the same code;
the private one allows efﬁcient decoding while the public one
resembles a random code which makes it hard to decode.
In 1986, Niederreiter [25] published a modiﬁcation of the
McEliece scheme which decreases the ciphertext size. Nieder-
reiter’s original proposal involved some codes which turned
out to be weak, but using Niederreiter’s short ciphertexts with
the binary Goppa codes [16] proposed by McEliece in his
encryption scheme combines the beneﬁts of both schemes.
McBits, by Bernstein, Chou, and Schwabe [7], extends this
public-key primitive into a full IND-CCA2 secure encryption
scheme, combining it with a KEM-DEM [13] construction.
The PQCRYPTO recommendations [2] include McBits as
the only public-key encryption scheme. McBits uses a code
of length n = 6960, dimension k = 5413 and adding t = 119
errors. These parameters (explained below) lead to a public
key of roughly 1MB and to a ciphertext length just n− k =
1547 bits. The same parameters are included in the Classic
McEliece submission [6] to NIST’s call for Post-Quantum
systems [26] as mceliece6960119. Classic McEliece has
been selected by NIST as a Round-2 candidate [27]. Similar
considerations as given in the next sections hold for other
parameters and other code-based systems with large public
keys.
This section explains how key encapsulation and decap-
sulation work; for details on key generation see [6]. The
description is independent of the exact parameters; we use
these for illustration purposes whenever concrete sizes are
necessary and because these parameters are recommended for
long-term security.
The codes considered in this paper are binary codes, mean-
ing that all entries are in {0,1} and that computations follow
the rules of IF2, i.e., 0 + 0 = 1 + 1 = 0,0 + 1 = 1,1· 1 = 1,
and, as always, 0· a = 0 for any a.
4.1 Public and Private Keys
The public key is a binary matrix K = (I|K(cid:48)) with n columns
and n − k rows. The leftmost (n − k) × (n − k) part is the
identity matrix I. The (n− k)× k matrix K(cid:48) differs per user.
The private key is an efﬁcient decoding mechanism for
the code related to K. The decoding details do not matter for
this paper but the private key is much smaller than the public
key. Key generation is more computationally intensive than
encapsulation or decapsulation.
Example 1 We now introduce a small example
with n = 7 and k = 4 which we will use for the
following sections. Let
This section explains the basics of code-based cryptography
and speciﬁes the parameters used in this proposal.
K =
1
0
0
 ,
0
1
0
0
0
1
1
1
0
1
0
1
0
1
1
1
1
1
USENIX Association
29th USENIX Security Symposium    1735
then
1
1
0
K(cid:48) =
 .
1 0 1
0 1 1
1 1 1
4.2 Encapsulation and Decapsulation
The basic operation in encapsulation is to compute K·e, where
e is a randomly chosen binary vector of length n which has
weight t, i.e., exactly t nonzero entries; and · denotes normal
matrix-times-vector multiplication over IF2. The result of this
computation is a binary vector c = Ke of length n− k. This
computation takes the ﬁrst row k1 of K and computes the dot
product with e, resulting in the ﬁrst bit of Ke, takes the second
row k2 to similarly produce the second bit of Ke, etc.
Example 2 Continuing in the setting of Exam-
ple 1 and choosing e = (0,1,0,0,0,1,0)⊥ gives
c = (0,0,1)⊥, the sum of the second and the sixth
column.
Decapsulation uses the private representation of the code to
recover the vector e from Ke. As can be seen in the example, e
is not unique. The same c is obtained by adding the fourth and
the seventh column, or just by taking the third column. The
cryptosystem restricts the weight t of e so that e is unique.
4.3 Security of Code-Based Cryptography
The cryptographic hardness assumption is that it is hard to
ﬁnd e given K and c for e of ﬁxed (small) weight t. This is
the syndrome decoding problem which is a hard problem in
coding theory for random codes. For a small example like
Example 1 it is easy to check all possibilities of low-weight
vectors e but the complexity of these attacks grows exponen-
tially with n and t. For code-based cryptography based on
binary Goppa codes the key-size (n− k)· k grows with the
security level λ (meaning an attacker takes 2λ operations) as
(c0 + o(1))λ2(lgλ)2, with c0 ≈ 0.7418860694, for the best
attacks known today. See e.g. the documentation of [6] for an
overview of attacks.
IND-CCA2 Security
4.4
The Classic McEliece system includes key conﬁrmation and
computes ENC(K) = (c,C,S) with c = K · e, C = hash(2,e),
and S = hash(1,e,c,C). The pair (c,C) is the ciphertext and
S is the shared symmetric key. Here hash is a cryptographic
hash function.
Classic McEliece
computes
DEC(c,C, sk) = S, where S = hash(1,e,c,C) if the re-
covered e has the correct weight and satisﬁes C = hash(2,e).
Else S = hash(0,v,c,C), where v is a secret value stored for
this purpose. Thus, decapsulation never fails. Subsequent
steps use S in authenticated encryption, so invalid ciphertexts
will produce failed authenticators.
Decapsulation
in
5 McTiny Public Keys
This section explains the mathematical details of how the
McTiny protocol (described in the next section) can work on
pieces of the public key while obtaining correct encryptions.
5.1 Partitioning of Public Keys
McTiny transmits a public key K from the client to the server.
It splits K and uses that the computation Ke can naturally be
composed using parts of K and e. Let K = (I|K(cid:48)) and write
K(cid:48) =
 ,
K1,1 K1,2 K1,3
K2,1 K2,2 K2,3
...
...
Kr,1 Kr,2 Kr,3
...
. . . K1,(cid:96)
. . . K2,(cid:96)
...
...
. . . Kr,(cid:96)
where the submatrices Ki, j are chosen to be approximately
equally sized and small enough to ﬁt into a network packet
along with other message parts described in the next section.
For ease of exposition assume that each Ki, j has x columns
and y rows, so k = x· (cid:96) and n− k = y· r; in general the pieces
may have different sizes as speciﬁed by the system parameters.
All users use the same values for n, k, t, (cid:96) and r, so the size of
each Ki, j is universally known.
The client transmits K by sending Ki, j and the position
(i, j) for 1 ≤ i ≤ r, 1 ≤ j ≤ (cid:96). Upon receipt of Ki, j the server
computes the partial result ci, j = Ki, je j, where e j denotes the
matching part of e and ci, j the matching part of the resulting
vector c. For example, c1,(cid:96) = K1,(cid:96)e(cid:96) takes e(cid:96) as the last x po-
sitions of e, and computes the matrix-vector multiplication
K1,(cid:96)e(cid:96) resulting in the length-y vector c1,(cid:96). The ﬁrst y coordi-
nates of c are given by c1 = e1,0 + c1,1 + c1,2 +··· + c1,(cid:96), with
e1,0 the ﬁrst y positions of e.
Example 3
In the setting of Example 1 sub-
matrices may be chosen as K1,1 = (1 1),K1,2 =
(0 1),K2,1 = (1 0),K2,2 = (1 1),K3,1 = (0 1), and
K3,2 = (1 1). The vector e = (0,1,0,0,0,1,0)⊥
gets split into e1,0 = (0),e2,0 = (1),e3,0 = (0),e1 =
(0,0)⊥,e2 = (1,0)⊥. Then c1 = e1,0 + c1,1 + c1,2 =
(0) + (1 1)(0,0)⊥ + (0 1)(1,0)⊥ = 0, matching the
ﬁrst coordinate of c computed earlier.
Note that each part ci j poses a decoding problem for e j
which is much easier than breaking McEliece. It is thus im-
portant that these pieces are cryptographically protected.
5.2 Optimization
The partial computations of ci, j are independent of one an-
other and can be performed in any order. These intermediate
results take only y bits and are thus much smaller than the
xy-bit sized parts of K(cid:48) that they cover.
1736    29th USENIX Security Symposium
USENIX Association
We deﬁne a concrete example mctiny6960119 of McTiny,
using the mceliece6960119 parameters mentioned above
with k = 5413 and n − k = 1547. To minimize the size of
intermediate results we could take y = 1, x = 5413, and (cid:96) = 1,
i.e., we could transmit one row of K(cid:48) at once. However, this
would require 1547 steps alone in the stage of sending Ki, j.
Using (cid:96) = 2 and combining three rows produces chunks that
might be too large for the network packets. Observing that
1MB requires about a thousand packets of 1KB each and
aiming for a regular pattern of combination, we opt for (cid:96) = 8
and r = 119 for mctiny6960119. Typical Ki, j then have 13
rows and 680 columns ﬁtting into 1105 bytes. Replies with
ci j ﬁt into 2 bytes.
6 The McTiny Protocol
This section introduces the McTiny protocol. Forward secrecy
is achieved by requiring each client to generate and send a
fresh public key K to the server. Clients are also responsible
for key erasure at the end of a session. McTiny makes it
possible for the server to compute Ke, for a big matrix K and
chosen weight-t vector e (see Section 4), without requiring the
server to allocate any per-client memory and without needing
more temporary memory than what ﬁts into a network packet.
At the end of the McTiny protocol, server and client both
compute their shared symmetric key. The details of how this
shared key is computed match Classic McEliece [6] and the
client can use decapsulation from Classic McEliece.
Besides code-based cryptography for the public-key opera-
tions, McTiny uses authenticated encryption with symmetric
keys. The PQCRYPTO recommendations [2] suggest either
AES-GCM with AES-256 or Salsa20-Poly1305 with 256-bit
encryption keys and 128-bit authentication keys. McTiny fol-
lows McBits in using XSalsa20-Poly1305. (XSalsa20 handles
longer nonces than Salsa20.) We use AE(T : N : S) to denote
the authenticated encryption of T under key S using nonce N.
6.1 General Setup and Phases
The server has a long-term public key pk which is used to
authenticate the server and to derive a shared secret key to
encrypt and authenticate all messages after the initiation mes-
sage. The McTiny protocol uses a long-term McEliece key to
achieve full post-quantum security. The server administrator
generates this long-term key when setting up the server. The
public-key infrastructure, the mechanism that disseminates
and certiﬁes server public keys, is outside the scope of this
paper; we simply assume that, when the McTiny protocol
begins, the client knows the server’s long-term public key.
The McTiny protocol runs in four phases. The ﬁrst phase,
phase 0, is the initiation phase in which the client estab-
lishes contact with the server and the server proves its identity.
Speciﬁcally, the server uses its long-term private key sk to
decrypt the key S encapsulated by the client and respond to
the client’s initial request. Note that this key S is not forward-
secret. The client and the server use S to encrypt and authenti-
cate all following messages.
In phase 1 the client sends the matrix parts Ki, j to the
server and the server replies with encryptions of the partial
encryptions ci, j. Phase 2 is the row-wise combination and
phase 3 computes the KEM ciphertext. A full description
of the protocol is given in Figure 1. The following sections
explain the steps in detail. See Table 1 for the packet sizes in
each phase.
6.2 Nonces
XSalsa20-Poly1305 uses nonces with 24 bytes. In the McTiny
protocol the server is responsible for generating a random
22-byte N from which most nonces are generated as n =
(N,N0,N1) in a deterministic way. Bytes N0 and N1 are deter-
mined by the phase the protocol is in, information regarding
positions, and N0 is even for messages from the client to the
server and odd for messages the other way. Bytes are stated as
integers in [0,255] in the protocol. For concreteness we state
particular choices of (N0,N1) below for the mctiny6960119
parameters. These apply to a large range of codes.
6.3 Server Cookies
McTiny makes heavy use of encrypted cookies to store in-
termediate results in the network/on the client’s computer.
The cookie keys sm are symmetric keys that are used for cer-
tain time intervals. In time interval m the server uses sm to
encrypt cookies with data to itself. The server can decrypt
cookies returned to it during z time intervals, while it is using
sm,sm+1,sm+2,sm+3, . . . ,sm+z−1. When the server generates
sm+z it erases sm.
In mctiny6960119 we specify the time interval as one
minute and specify that the server remembers 8 cookie keys
in any time interval, i.e. while it uses sm it also remembers
sm−1,sm−2, . . .sm−7 but not sm−8 or earlier keys. Each cookie
contains one byte in clear which determines the cookie index
modulo 8, where numbers are assigned round robin. At 22
bytes, the nonce part N is chosen long enough so that it will
not repeat while the same cookie key is in use. To explain
these choices, assume keys are erased within 8 minutes. If the
server uses only 2 keys then client connections begin failing
after a 4-minute network outage. Increasing 2 to 8 cheaply
increases 4 minutes to 7 minutes. We allocate an entire byte
(and add a random multiple of 8 as grease, and have the client
repeat the entire byte) so that modiﬁcations to the cookie
policy do not require modiﬁcations to clients.
6.4 Phase 0: Initiation
To initiate communication, the client uses the server’s long-
term public key pk and derives a symmetric key S using the
USENIX Association
29th USENIX Security Symposium    1737
Client
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Set-up phase . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .