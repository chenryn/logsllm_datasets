tags and Cuckoo-Relocation, as the maximum number of
relocation attempts is varied. Attempting relocation for up to
3 lines is sufﬁcient to ensure that an SAE does not occur in
system-lifetime (SAE occurs once in 22000 years). We note
that attempting relocation for up to 3 lines can be done in the
shadow of a memory access on a cache-miss.
Table 5: Frequency of SAE in Mirage with 50% extra tags (4
extra ways/skew) as number of relocation attempts increase
Max Relocations
Installs per SAE
Time per SAE
0
1
2
3
2× 108
4× 1016
7× 1020
0.16 seconds 45 minutes 1.3 years 22,000 years
3× 1012
7.3 Security Implications of Relocation
For Mirage with 50% extra tags, up to 3 cuckoo relocation
are done in the shadow of memory access on a cache-miss.
A typical adversary, capable of only monitoring load latency
or execution time, gains no information about when or where
relocations occur as – (1) Relocations do not stall the proces-
sor or cause memory trafﬁc, they only rearrange cache entries
within the tag-store; (2) A relocation occurs infrequently
(once in 100 million installs) and any resultant change in oc-
cupancy of a set has a negligible effect on the probability of
an SAE. If a future adversary develops the ability to precisely
monitor cache queues and learn when a relocation occurs to
perceive a potential conﬂict, we recommend implementing
Mirage with a sufﬁcient extra tags (e.g. 75% extra tags) such
that no relocations are needed in the system lifetime.
1388    30th USENIX Security Symposium
USENIX Association
(c) After RelocationInvalid = 0Invalid = 0ACBZLineInstall (a) Before RelocationSkew-1Skew-2 (b) RelocationDFEACBSkew-1Skew-2DFEInvalid = 1Invalid = 0ZACBSkew-1Skew-2DFEZValid TagInvalid Tag8 Performance Analysis
In this section, we analyze the impact of Mirage on cache
misses and system performance. As relocations are uncom-
mon, we observe that performance is virtually identical for
both with and without relocations. So, we discuss the key
results only for the default Mirage design (75% extra tags).
8.1 Methodology
Similar to prior works on randomized caches [39, 40, 51, 57],
we use a micro-architecture simulator to evaluate performance.
We use an in-house simulator that models an inclusive 3-level
cache hierarchy (with private L1/L2 caches and shared L3
cache) and DRAM in detail, and has in-order x86 cores sup-
porting a subset of the instruction-set. The simulator input is a
1 billion instructions long program execution-trace (consisting
of instructions and memory-addresses), chosen from a repre-
sentative phase of a program using the Simpoints sampling
methodology [48] and obtained using an Intel Pintool [30].
We validated the results of our simulator with RISC-V RTL
(Appendix A) and Gem5 (Appendix B) simulations.
As our baseline, we use a non-secure 16-way, 16MB set-
associative LLC conﬁgured as shown in Table 6. For Mirage,
we estimate the LLC access latency using RTL-synthesis
of the cache-lookup circuit (Section 8.2) and Cacti-6.0 [34]
(a tool that reports timing, area, and power for caches), and
show that it requires 4 extra cycles compared to the baseline
(3-cycles for PRINCE cipher and 1 extra cycle for tag and
data lookup). For comparisons with the prior state-of-the-
art, we implement Scatter-Cache with 2-skews, 8 ways/skew
and use PRINCE cipher for the hash function for set-index
derivation, that adds 3 cycles to lookups compared to baseline
(to avoid an unfair advantage to Mirage, as Scatter-Cache [57]
originally used a 5-cycle QARMA-64 cipher). We evaluate
58 workloads, including all 29 SPEC CPU2006 benchmarks
(each has 8 duplicate copies running on 8 cores) and 29 mixed
workloads (each has 8 randomly chosen SPEC benchmarks)
All performance averages reported in subsequent sections are
averaged over all 58 workloads, unless mentioned otherwise.
Table 6: Baseline System Conﬁguration
Processor and Last-level Cache
Core
8-cores, In-order Execution, 3GHz
L1 and L2 Cache Per Core L1-32KB, L2-256KB, 8-way, 64B linesize
LLC (shared across cores)
16MB, 16-way Set-Associative, 64B linesize
LRU Replacement Policy, 24 cycle lookup
DRAM Memory-System
Frequency, tCL-tRCD-tRP 800 MHz (DDR 1.6 GHz), 9-9-9 ns
DRAM Organization
2-channel (8-Banks each), 2KB Row-Buffer
8.2 Synthesis Results for Cache Access Latency
Compared to the baseline, the cache access in Mirage addi-
tionally requires (a) set-index computation using the PRINCE
cipher based hash-function, (b) look-up of 8-12 extra ways
of the tag-store, and (c) FPTR-based indirection on a hit to
access the data. We synthesized the RTL for the set-index
derivation function with a 12-round PRINCE cipher [9] based
on a public VHDL implementation [22], using Synopsys De-
sign Compiler and FreePDK 15nm gate library [31]. A 3-stage
pipelined implementation (with 4 cipher rounds/stage) has a
delay of 320ps per stage (which is less than a cycle period).
Hence, we add 3 cycles to the LLC access latency for Mirage
(and Scatter-Cache), compared to the baseline.
We also synthesized the RTL for FPTR-indirection circuit
consisting of AND and OR gates that select the FPTR value
of the hitting way among the accessed tags, and a 4-to-16
decoder to select the data-store way using the lower 4-bits
of the FPTR (the remaining FPTR-bits form the data-store
set-index); the circuit has a maximum delay of 72ps. Using
Cactii-6.0 [34], we estimate that lookup of up to 16 extra
ways from the tag-store further adds 200ps delay in 32nm
technology. To accommodate the indirection and tag lookup
delays, we increase the LLC-access latency for Mirage fur-
ther by 1 cycle (333ps). Overall, Mirage incurs 4 extra cycles
for cache-accesses compared to the baseline. Note that the
RPTR-lookup and the logic for skew-selection (counting valid
bits in the indexed set for each skew and comparing) require
simple circuitry with a delay less than 1 cycle. These opera-
tions are only required on a cache-miss and performed in the
background while the DRAM-access completes.
Table 7: Average LLC MPKI of Mirage and Scatter-Cache
Workloads
SpecInt-12
SpecFp-17
Mix-29
All-58
Baseline Mirage
11.23
8.51
9.97
9.80
10.79
8.82
9.52
9.58
Scatter-Cache
11.23
8.51
9.97
9.80
Impact on Cache Misses
8.3
Table 7 shows LLC Misses Per 1000 Instructions (MPKI) for
the non-secure Baseline, Mirage, and Scatter-Cache averaged
for each workload suite. We observe that all LLC-misses in
Mirage in all workloads result in Global Evictions (no SAE),
in line with our security analysis.6 Compared to the Baseline,
Mirage incurs 2.4% more misses on average (0.2 MPKI ex-
tra) as the globally-random evictions from the data-store lack
the intelligence of the baseline LRU policy that preserves ad-
dresses likely to be re-used. The miss count for Scatter-Cache
6Workloads typically do not always access random addresses. But the
randomized cache-set mapping used in Mirage ensures accesses always map
to random cache-sets, which allows the load-balancing skew-selection to
maintain the availability of invalid tags across sets and prevent any SAE.
USENIX Association
30th USENIX Security Symposium    1389
Figure 12: Performance of Mirage and Scatter-Cache normalized to Non-Secure Baseline (using weighted speedup metric). Over
58 workloads, Mirage has a slowdown of 2%, while Scatter-Cache has a slowdown of 1.7% compared to the Non-Secure LLC.
is similar to Mirage as it uses randomized set-indexing that
causes randomized evictions with similar performance impli-
cations (however note that all its evictions are SAE that leak
information). We observe that randomization can increase or
decrease conﬂict misses for different workloads: e.g., Mirage
and Scatter-Cache increase misses by 7% for mcf and xalanc
while reducing them by 30% for sphinx compared to baseline.
Impact on Performance
8.4
Figure 12 shows the relative performance for Mirage and
Scatter-Cache normalized to the non-secure baseline (based
on the weighted speedup7 metric). On average, Mirage incurs
a 2% slowdown due to two factors: increased LLC misses and
a 4 cycle higher LLC access latency compared to the baseline.
For workloads such as mcf or omnet, Mirage increases both
the LLC misses and access latency compared to a non-secure
LLC and hence causes 6% to 7% slowdown. On the other
hand, for workloads such as sphinx, dealII and gcc, Mirage re-
duces LLC-misses and improves performance by 5% to 19%.
In comparison, Scatter-Cache has a lower slowdown of 1.7%
on average despite having similar cache-misses, as it requires
1 cycle less than Mirage for cache accesses (while both incur
the cipher latency for set-index calculation, Mirage requires
an extra cycle for additional tag-lookups and indirection).
8.5 Sensitivity to Cache Size
Figure 13 shows the performance of Mirage and Scatter-
Cache for LLC sizes of 2MB to 64MB, each normalized
to a non-secure design of the same size. As cache size in-
creases, the slowdown for Mirage increases from 0.7% for a
2MB cache to 3.2% for a 64MB cache. This is because larger
caches have a higher fraction of faster cache-hits that causes
the increase in access-latency to have a higher performance
impact. Similarly, the slowdown for Scatter-Cache increases
from 0.5% to 2.8% and is always within 0.4% of Mirage.
7Weighted-Speedup = ∑N−1
i=0 IPC-MCi/IPC-SCi is a popular throughput
metric for fair evaluation of N-program workloads [50], where IPC stands for
Instructions per Cycle, IPC-MCi is the IPC of a program-i in multi-program
setting, and IPC-SCi is the IPC of program-i running alone on the system.
Using Raw-IPC as the throughput metric, the slowdown decreases by 0.2%.
Figure 13: Sensitivity of Performance to Cache-Size.
8.6 Sensitivity to Cipher Latency
Figure 14 shows the performance of Mirage and Scatter-
Cache normalized to a non-secure baseline LLC, as the la-
tency of the cipher (used to compute the randomized hash
of addresses) varies from 1 to 5 cycles. By default, Mirage
and Scatter-Cache evaluations in this paper use a 3-cycle
PRINCE-cipher [9] (as described in Section 8.2), resulting in
slowdowns of 2% and 1.7% respectively. Alternatively, a ci-
pher like QARMA-64 [3] (that was used in the Scatter-Cache
paper and assumed to have 5 cycle latency [57]) can also be
used in Mirage; this causes Mirage and Scatter-Cache to have
higher slowdowns of 2.4% and 2.2%. Similarly, future works
may design faster randomizing-functions for set-index calcu-
lations in randomized caches; a 1-cycle latency randomizing
function can reduce slowdown of Mirage and Scatter-Cache
to 1.5% and 1.2% respectively. The study of faster randomiz-
ing functions for Mirage that also have robust randomization
that prevents an adversary from discovering eviction-sets via
shortcut attacks [37] is an important direction for future work.
Figure 14: Sensitivity of Performance to Cipher Latency.
1390    30th USENIX Security Symposium
USENIX Association
astarbzip2gccgobmkh264hmmerlibqntmmcfomnetperlbenchsjengxalancbwavescactuscalculixdealIIgamessgemsgromacslbmlesliemilcnamdpovraysoplexsphinxtontowrfzeusmp SpecInt-12SpecFp-17Mix-29All-5890%92%94%96%98%102%104%106%108%110%Norm. Performance (%)119.8%119.3%GmeanScatter-CacheMirage2MB4MB8MB16MB32MB64MB96%97%98%99%100%101%102%Norm. Performance (%)Scatter-CacheMirage1-cycle2-cycle3-cycle4-cycle5-cycle96%97%98%99%100%101%Norm. Performance (%)Scatter-CacheMirage9 Cost Analysis
For analyzing the storage and power overheads of Mirage, we
distinguish the two versions of our design as, Mirage (default
design with 75% extra tags) and Mirage-Lite (with 50% extra
tags and relocation).
9.1 Storage Overheads
The storage overheads in Mirage are due to (1) extra tag-
entries, and (2) FPTR and RPTR, the pointers between
tag/data entries, and (3) tag-bits storing full 40-bit line-address
(for 46-bit physical address space) to enable address genera-
tion for write-backs. This causes a storage overhead of 20%
for Mirage and 17% for Mirage-Lite compared to the non-
secure baseline, as shown in Table 8. These overheads are
dependent on cache linesize as the relative size of tag-store
compared to the data-store reduces at a larger linesize. While
we use 64B linesize, a 128B linesize like IBM’s Power9
CPUs [58] would reduce these overheads to 9-10% and a
256B linesize would reduce these to 4-5%.
The storage overhead in Mirage is the main driver be-
hind the area overhead, as the extra storage requires mil-
lions of gates, whereas all other extra logic for FPTR/RPTR-
indirection, PRINCE cipher, etc., can be implemented in few
thousand gates (as shown in Section 9.3). Using CACTI-
6.0 [34], we estimate that an LLC requiring 20% extra storage
consumes approximately 22% extra area. In terms of a storage-
neutral comparison, Mirage has an average slowdown <3.5%
compared to a non-secure LLC with 20% more capacity.
Table 8: Storage Overheads in Mirage for 64B linesize
Cache Size
Baseline
16MB
Set
Mirage
2 skews x
Mirage-Lite
2 skews x
(16,384 Sets)
Associative 14 ways/skew 12 ways/skew
Tag-Bits
Status(V,D)
Tag
Entry
FPTR
SDID
Bits/Entry
Tag Entries
Tag-Store Size
Data-Bits
RPTR
Data
Entry
Bits/Entry
Data Entries
Data-Store Size
Total Storage
26
2
–
–
28
40
2
18
8
68
40
2
18
8
68
262,144
896 KB
458,752
3808 KB
393,216
3264 KB
512
–
512
512
19
531
512
19
531
262,144
262,144