Batch failures, i.e. a large group of servers reporting
the same failure at the same time;
Correlated component failures, i.e. multiple compo-
nents on a single server failing at the same time.
A. Batch failures
Different from the common belief that servers fail inde-
pendently, we see cases where many servers fail in a batch,
especially those servers with the same model, in the same
31
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:01:13 UTC from IEEE Xplore.  Restrictions apply. 
TABLE V.
BATCH FAILURE FREQUENCY FOR EACH COMPONENT
CLASS.
TABLE VI.
NUMBER OF CORRELATED COMPONENT FAILURES.
Device
HDD
Miscellaneous
Power
Memory
RAID card
Flash card
Motherboard
Fan
SSD
CPU
r100(%)
r200(%)
r500(%)
55.4
3.7
0.7
0.4
0.4
0.1
0.1
0
0
0
22.5
1.3
0.4
0.4
0.2
0.1
0
0
0
0
2.5
0.1
0
0.1
0.1
0
0
0
0
0
cluster and serve the same product line. Similar phenomenon
was also observed by previous study [28].
Different operators and product lines have different def-
initions and tolerance of batch failures. In general, batch
failures refer to a number of servers (above a threshold N)
failing during a short period of time t. Both N and t are
user-speciﬁc. A product tolerates a larger batch of failures
with better software fault tolerance. Also, if the operators ﬁx
problems quickly, we can expect fewer batch failures.
We deﬁne a metric rN to describe the relative frequency of
batch failures. Let nk be the number of failures of a component
class on the k-th day (k = 1 . . . D, where D is the total number
of days we examine). We informally deﬁne the batch failure
k I{nk ≥ N})/D, where N is the
frequency rN as rN = (
threshold, and I is an indicator random variable. Intuitively,
rN is a normalized counter of how many days during the D
days, in which more than N failures happen on the same day,
and we normalize the count by the total time length D.
(cid:2)
We calculate rN for each type of components, and Table V
shows the results with N = 100, 200 and 500. We ﬁnd that
batch hard drive failures are common. During 2.48% of the
days (35 out of 1,411 days), we observe over 500 hard drive
failures. We also observe batch failures in components such as
memory, power supplies, RAID cards, ﬂash cards and fans.
Examples of batch failures and possible reasons.
Here we present three batch failure cases associated with a
single large product line owning tens of thousands servers in
a single data center. These servers are incrementally deployed
during the past three to four years, with ﬁve different genera-
tions. Most of the servers run batch data processing jobs (e.g.
Hadoop jobs). We describe three batch failure cases in 2015.
Case 1: On Nov. 16th and 17th, there were thousands of
servers, or 32% of all the servers of the product line, reporting
hard drive SMARTFail failures, and 99% of these failures
were detected between 21:00 on the 16th and 3:00 on the
17th. The operators ended up replacing about 28% of these
hard drives and decommissioned the remaining 70%+ out-of-
warranty drives. The reason of the batch failure is not clear
yet.
Case 2: On Jun 4th, there were nearly 50 motherboards
failing between 5:00 and 6:00, or between 16:00 and 17:00.
The operators indicated that faulty SAS (Serial Attached SCSI)
cards caused all these failures. They decided to decommission
all these servers because they were out-of-warranty.
Misc.
Mother.
Fan
Power
RAID
Flash
Memory
SSD
HDD SSD Memory
349
17
1
3
46
22
40
15
18
2
-
-
-
-
2
-
-
-
-
-
-
Flash RAID Power
2
-
-
-
-
4
-
-
-
6
1
7
Fan Motherboard
-
-
6
TABLE VII.
EXAMPLES OF CORRELATED COMPONENT FAILURES.
Server ID
Server A
Server B
Partial FOTs
Fan fan 8 2016-01-22 06:35:35
Power psu 2 2016-01-22 06:36:51
Fan fan 3 2016-01-22 06:35:34
Power psu 2 2016-01-22 06:36:59
Batch failures in cases 1 and 2 may be related to unex-
pected homogeneity in these components and their operating
environments. For example, components with the same model
and same ﬁrmware version may contain the same design ﬂaws
or bugs that are triggered by the same condition. Also, as
they are in the same data center, the long-term effects of the
environment such as humidity, temperature, and vibration may
be homogeneous, leading to simultaneous failures.
Case 3: On May 16th, nearly 100 servers experienced
power failure between 1:00 and 13:00. This is a typical case as
these servers used a single power distribution unit that caused
the batch failure.
Case 3 represents a common design ﬂaw of hidden single
point of failure. The single dependency situation is common
in data centers, especially for networking, power, and cooling.
Besides the cases mentioned above, we also observe batch
failures caused by human operator mistakes. For example, a
misoperation of the electricity provider caused a power outage
on a PDU in a data center, resulting in hundreds of failed
servers in August of 2016.
B. Correlated component failures
Correlated component failures rarely happen but often lead
to confusing failure situations. We deﬁne correlated component
failures as failures occurring on multiple components on the
same server within a single day. These failures are unlikely to
be coincidental: given the overall failure rate, we can calculate
that the chance of two independent failures happening on the
same server on the same day, which is less than 5%. In our
dataset, these failures involve at most two components and
experienced by only 0.49% of all servers that ever failed. In
addition, 71.5% of these two-component failures have a mis-
cellaneous failure report, indicating that some failures detected
by the FMS are also noticed by the operators, who decide
to report them immediately. Hard drive failures are related to
nearly all the rest of two-component failures. Table VI shows
the number of different correlated failure pairs.
Possible Reasons. We believe the primary reason for the
component correlation is that one failure causes the other. For
example, Table VII shows two cases of correlated failures of
power and fan, both of which occurred on servers in the same
32
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:01:13 UTC from IEEE Xplore.  Restrictions apply. 
TABLE VIII.
AN EAMPLE OF SYNCHRONOUSLY REPEATING FAILURES
Server C
SMARTFail sdh8 14-08-31 16:30:24
SMARTFail sdh8 14-09-04 10:36:05
SixthFixing sda1 14-09-05 13:42:06
SixthFixing sda1 14-09-05 17:34:02
SixthFixing sda1 14-09-09 17:12:05
SixthFixing sda1 14-09-16 16:32:10
PendingLBA sde5 15-07-21 12:38:34
Server D
SMARTFail sdd4 14-08-31 16:26:15
SMARTFail sdd4 14-09-04 10:30:13
SixthFixing sda1 14-09-05 13:42:09
SixthFixing sda1 14-09-05 17:34:03
SixthFixing sda1 14-09-09 17:12:04
SixthFixing sda1 14-09-16 16:32:21
PSU (power supply unit) of the same data center on the same
day. We believe that the power failure causes the fan problem.
Correlated component failures complicate failure detection
and repair, especially automatic error handling. In the example
above, we should not replace the fans even if they are reporting
errors, while in other (common) cases, the operator should
replace any alarming fans.
C. Repeating synchronous failures
Failures on some small groups of servers can appear highly
correlated, as they repeat at the same time for many times. It
is related to the repeating failure cases in Section III-D, mostly
due to ineffective repair operations.
Table VIII shows an example. These two servers are almost
identical: same product line, same model, same deployment
time and located in adjacent racks, running the same distributed
storage system. We see that their (repeating) disk failures occur
almost synchronously for many times, which is obviously not
a coincidence.
These synchronous failures may cause trouble for software
fault tolerance, as most fail-over and recovery mechanisms do
not expect such failure pattern. The best way to avoid these
synchronous failures is to improve the repair effectiveness,
making sure that the faulty components are either repaired or
removed from production. However, as we will see in the next
section, it is not yet the case.
VI. OPERATORS’ RESPONSE TO FAILURES
Previous studies point out that shortening mean time to re-
cover (MTTR) improves overall system dependability [1, 29].
An important component in MTTR is the operator’s response,
diagnosis and the initiation of following repairs. In this section,
we analyze these response times.
We focus on the FOTs in D ﬁxing and D falsealarm only
and ignore those out-of-repair cases. We deﬁne the operators’
response time (RT) for each FOT as RT = op time −
err time, where err time is the failure detection time, and
op time is the time when the operator closes the FOT (i.e.
initiates an RO or marks it as not ﬁxing).
We notice that many factors, such as time, component class
and product line all affect RT signiﬁcantly, leading to high
variations. The remainder of the section summarizes our key
observations.
A. RT is very high in general
Figure 9 shows the CDF of RT across all FOTs. We can
see that there are many extremely long responses. E.g., 10% of
Fig. 9. The CDF of RT in D ﬁxing and D falsealarm.
the FOTs has RT s of longer than 140 days and 2% even longer
than 200 days. Surprisingly, operators do not actually abandon
these FOTs, but eventually initiate an RO. These extremely
long RT s signiﬁcantly impact MTTR. The MTTR reaches 42.2
days and 19.1 days respectively for D ﬁxing and D falsealarm,
comparing to the median of repair time of only 6.1 days and
4.9 days. In contrast, MTTR that previous studies found in
large-scale systems [5, 24] is much shorter.
Possible Reasons. We do not believe the prolonged RT is
because the operators are incapable or too lazy to do the job.
In fact, modern data center software and hardware design may
cause the change.
Though the common belief is that hardware dependability
determines the software redundancy design, we ﬁnd that the
other way around is also true. Thanks to the highly resilient
software, operators know that even if the hardware has failed or
is prone to imminent failure, it will not be a catastrophe. Thus
the operators are less motivated to respond to the failures. For
example, in some product lines, operators only periodically
review the failure records in the failure pool (Figure 1) and
process them in batches to save time. We will provide more
details in the later part of this section.
Secondly, with early warnings and resilient hardware de-
sign, many hardware failures are no longer urgent. For ex-
ample, SMART reports on hard drives warn about occasional
and transient faults. The faults may be early warnings of fatal
failures, but the failure is not imminent.
On the other hand, for those software systems without good
fault handling, the repair operation can sometimes be costly
and may bring extra down time. This is because to reduce
hardware cost, current data center servers no longer support
hot-swap on memory, CPUs or PCIe cards. Thus, it is likely
that the operators have to shut down the entire server to replace
the faulty component. Without proper software fault tolerance,
the repair might require some planning such as manually
migrating tasks out of the server, delaying the response.
B. RT for each component class
In this section, we want to see whether FOTs involving
easy-to-repair component get quick responses.
Figure 10 shows the cumulative distribution of RT for each
component class covering all FOTs in the dataset. We see that
the median RT s of SSD and miscellaneous failures are the
33
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:01:13 UTC from IEEE Xplore.  Restrictions apply. 
line has its own operator team too. We want to see whether
these product lines have similar RT .
For the same type of failure, e.g. hard drive failures, we
ﬁnd that different product lines can have a big difference in
RT . The standard deviation of the RT is as high as 30.2 days
across these product lines.
Again, the variation is highly correlated with the level of
fault tolerance in the product. We see that product lines with
better software fault tolerance tend to have longer RT on
failures. For example, RT is often large for most product lines
operating large-scale Hadoop clusters.
We suspect that operators delay some responses because
they are too busy dealing with other failures, but our obser-
vations show little evidence. We observe that the median RT
does not grow in proportionality with the number of failures.
In fact, it is just the opposite: the top 1% product lines with
most failures have a median RT of 47 days. Out of the product
lines with fewer than 100 failures, 21% of them have a median
RT exceeding 100 days. Figure 11 shows the median RT for
all hard drive FOTs across some randomly sampled product
lines during a 12-month period.
When interviewed, most of the operators believe that given