# POD-Diagnosis: Error Diagnosis of Sporadic Operations on Cloud Applications

**Authors:**
- Xiwei Xu
- Liming Zhu
- Ingo Weber
- Len Bass
- Daniel Sun

**Conference:**
2014 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks

**Affiliations:**
- School of Computer Science and Engineering, University of New South Wales
- Software Systems Research Group (SSRG), NICTA
- Sydney, Australia

**Contact:**
{firstname.lastname}@nicta.com.au

---

## Abstract

Cloud applications are subject to sporadic changes due to operational activities such as upgrades, redeployments, and on-demand scaling. These operations can also be affected by simultaneous interferences from other operations. Traditional anomaly-detection-based diagnosis techniques are less effective during these sporadic operation periods because a wide range of legitimate changes confound anomaly detection and make it difficult to establish a baseline for "normal" operation. The increasing frequency of these sporadic operations (e.g., due to continuous deployment) exacerbates the problem. Diagnosing failures during sporadic operations relies heavily on logs, but log analysis challenges, including noisy, inconsistent, and voluminous data, remain largely unsolved.

In this paper, we propose Process-Oriented Dependability (POD)-Diagnosis, an approach that explicitly models these sporadic operations as processes. These models allow us to (i) determine the orderly execution of the process, and (ii) use the process context to filter logs, trigger assertion evaluations, visit fault trees, and perform on-demand assertion evaluation for online error diagnosis and root cause analysis. We evaluated the approach on rolling upgrade operations in Amazon Web Services (AWS) while performing other simultaneous operations. During our evaluation, we correctly detected all 160 injected faults, as well as 46 interferences caused by concurrent operations, with 91.95% precision. Of the correctly detected faults, the accuracy rate of error diagnosis is 96.55%.

**Keywords:** system administration, cloud, deployment, process mining, error detection, error diagnosis, DevOps

---

## I. Introduction

Large-scale cloud applications may consist of thousands of nodes with complex software stacks inside each node and dependencies among nodes. Diagnosing operation errors in such applications has always been challenging, and there is a significant body of work in this field [1]. The recent emergence of DevOps, infrastructure-as-code, and elastic cloud resources adds new challenges.

In the past, major sporadic changes (such as upgrades) to large-scale applications were infrequent and often done during scheduled downtime with careful execution. Now, with high-frequency continuous deployment, sporadic changes are being automated by complex scripts (including code that manipulates the infrastructure) and pre-defined triggers, with only occasional human intervention. These sporadic but now high-frequency operations often affect the entire production system, leading to significant consequences. This has introduced several problems. First, fully testing these operation scripts/code is difficult because mimicking the scale and complexity of the real production environment is not easy. Second, traditional exception handling mechanisms, which are best suited for a single language environment, break down when dealing with different types of error responses from different systems, ranging from the error code of a cloud API call to a potential silent failure of a configuration change. This situation demands robust monitoring, diagnosis, and recovery during sporadic operations. However, traditional anomaly-detection-based diagnosis approaches (e.g., based on rules, statistics, machine learning) [1] are built for use during "normal" operations. They assume a system has normal operation profiles that can be learned from historical data, and deviations from these profiles can help detect, localize, and identify faults.

In this paper, we adopt a model-based approach [1] that explicitly models these sporadic operations as processes and uses the process context to locate errors, filter logs, visit fault trees, and perform on-demand assertion evaluation for online error diagnosis and root cause analysis. Specifically, our error diagnosis relies on a process model to specify the key steps and the desired system states (through assertions) after each key step. We then perform assertion evaluation and process conformance checking at runtime to locate errors within the process context. We further diagnose errors through the use of on-demand assertion evaluations (diagnosis tests) with the help of fault trees (essentially a structured repository of known errors and root causes). In terms of creating the process model, our initial work [2] shows that the process can be discovered using process mining [3] on normally generated logs of successful operations.

A rolling upgrade operation is a prime example of a sporadic operation, and we will use it as our case study. Errors during a rolling upgrade must be identified, diagnosed, and handled within minutes or seconds, as the full production system is at stake. Because of the problems outlined earlier, standard monitoring of normal operations is often deliberately disabled during the rolling upgrade. There is often a long gap between an error and its associated failure, and there is no online error diagnosis if the error is detected. The default recovery is usually a complete but equally risky rollback operation (unless expensive redundancy is used).

Error diagnosis during operation time heavily relies on logs. In addition to the unique challenges due to the high frequency and sporadic nature of the operations, there are also log analysis challenges [4] exacerbated by the uncertain cloud environment, multiple distributed log sources, and simultaneous confounding operations. Our POD-diagnosis approach specifically addresses these issues. Another challenge to log analysis comes from the voluminous and inconsistent logs from different systems, including cloud infrastructure and application ecosystems. Events appearing in the log are often out of context [4]. Our approach uses Logstash1 to filter and collect distributed logs from different sources using process/step-specific regular expressions and uses process conformance checking to localize the issues.

The cloud environment introduces uncertainties for operations that have traditionally been under the direct control of an enterprise. Enterprises have limited visibility of a cloud environment and have to rely on cloud infrastructure APIs and technologies to perform their sporadic operations. Many cloud resource-provisioning technologies, such as CloudFormation2, use a declarative black-box approach for achieving the end results. This is equally true for typical configuration management tools like Chef3 or Cfengine4, which rely on eventual convergence to achieve desired end states. These tools feature neither online error diagnosis support nor fine-grained targeted healing—only complete rollback/opportunistic retry—if something goes wrong in the middle of the operations. Our approach breaks down a sporadic operation into finer-grained steps, performs conformance checking to detect any deviation from the expected execution of the process, defines intermediate expected outcomes through assertions, defines a fault tree for all the (known) possible intermediate errors, and performs diagnosis tests visiting the fault tree for its root causes. Finally, existing works in error diagnosis lack awareness of multi-operations and ecosystem context [4]. The built-in exception handling in tools such as Asgard5 or Chef has only local information about the current operation it handles. Our approach has global visibility as it uses the aggregated and process-annotated logs from different operations in the central repository for error diagnosis.

We implemented our approach in a prototype and evaluated it in AWS. The evaluation results show that 95% of our online error diagnosis finishes within 3.83 seconds, with an overall precision of 91.95%, a recall of 100% for error detection, and a 97.13% accuracy rate for root cause diagnosis.

**Contributions:**
- The key contribution of our Process-Oriented Dependability (POD)-Diagnosis approach is the use of process context (such as operation process ID, instance ID, step ID, conformance status) to improve the success of error detection and diagnosis in this particular domain. Process context has not been applied before in this domain. Our technique is non-intrusive and does not require any modification to the operation scripts/tools. Additionally, we use fault trees to capture potential faults and root causes of intermediate errors, and the associated on-demand assertions (diagnosis checks) allow pinpointing root causes.
- Another contribution is the evaluation, where we injected faults in 160 runs of a rolling upgrade process on AWS to test the accuracy and completeness of POD-Diagnosis. There are no equivalent tools to compare against in this particular domain; hence, this is an exploratory study.

The paper proceeds by introducing rolling upgrade in Section II, followed by detailed discussions of POD-Diagnosis in Section III. Section IV presents the implementation of POD-Diagnosis. We evaluate our approach in Section V and discuss limitations in Section VI. Related work is discussed in Section VII, and Section VIII concludes the paper.

---

## II. Operations Process Example: Rolling Upgrade

Rolling upgrade is an important element of continuous delivery and high-frequency releases. In continuous deployment, a rolling upgrade of the entire system can happen multiple times a day [5] without system downtime. Assume an application is running in the cloud, consisting of a collection of virtual machine instances instantiated from a smaller number of different virtual machine images. Suppose a new machine image representing a new release for one of the images (VMR) is available for deployment. The current version of VMR is VA, and the goal is to replace the N instances currently executing VA with N instances executing the new version VB. A further goal is to do this replacement while still providing a sufficient level of service to clients of VMR: at any point in time during the replacement process, at least N' instances running some version of VMR should be available for service.

One method for performing this upgrade is called rolling upgrade [6]. In a rolling upgrade, a small number of k (with k = N - N') instances at a time currently running version VA are taken out of service and replaced with k instances running version VB. By only upgrading k machines, the number of instances in service remains at the desired level of N'. The time taken by the replacement process for one instance is usually in the order of minutes. Consequently, performing a rolling upgrade for hundreds or thousands of instances will take on the order of hours. The virtue of rolling upgrade is that it only requires a small number of additional instances to perform the upgrade. Rolling upgrade is the industry-standard method for upgrading instances [6].

In this paper, our examples are the provisioning failures that can occur during rolling upgrade. A provisioning failure occurs during the replacement process, specifically when one of the upgrade steps produces incorrect results. We do not consider failures due to logical inconsistencies among the versions being active simultaneously.

Current implementations of rolling upgrade vary. We base our discussion of provisioning failures on the method Asgard currently uses. Asgard is Netflix’s customized management console on top of AWS infrastructure. For provisioning failures, Asgard will recognize them and report the failure to the operator. The time between the failure occurring and the report to the operator may be as long as 70 minutes. Asgard may not recognize some provisioning failures, such as those caused by concurrent independent upgrades.

---

## III. POD-Diagnosis

### A. Offline Process Mining and Assertion Creation

The process model can be created manually or discovered from logs by process mining [3]. The granularity of the model is determined by the analyst. The granularity may be constrained by log granularity if the process is discovered from logs. It is possible to construct a process model from scripts or runbooks and not be constrained by logs. To track the execution of the process, the analyst still needs to specify which log lines belong to which activity.

### B. Conformance Checking and Assertion Evaluation

Non-log triggers initiate the conformance checking and assertion evaluation. Conformance checking ensures that the process is executed in the correct order, while assertion evaluation verifies the desired system state after each key step. If a deviation is detected, the process context is used to filter logs, trigger assertion evaluations, visit fault trees, and perform on-demand assertion evaluation for online error diagnosis and root cause analysis.

---

This revised version aims to provide a clearer, more coherent, and professional presentation of the original text.