title:POD-Diagnosis: Error Diagnosis of Sporadic Operations on Cloud Applications
author:Xiwei Xu and
Liming Zhu and
Ingo Weber and
Len Bass and
Daniel Sun
2014 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
2014 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
2014 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
POD-Diagnosis: Error Diagnosis of Sporadic Operations on Cloud Applications 
1Xiwei Xu, 1,2Liming Zhu, 1,2Ingo Weber, 1,2Len Bass, 1Daniel Sun 
2School of Computer Science and Engineering, University of New South Wales 
1SSRG, NICTA 
Sydney, Australia 
{firstname.lastname}@nicta.com.au 
logs 
to 
interferences 
since 
is  non-trivial,  particularly 
from  multiple  sources  remain 
Abstract—  Applications  in  the  cloud  are  subject  to  sporadic 
changes  due  to  operational  activities  such  as  upgrade, 
redeployment,  and  on-demand  scaling.  These  operations  are 
also  subject 
from  other  simultaneous 
operations.  Increasing  the  dependability  of  these  sporadic 
operations 
traditional 
anomaly-detection-based diagnosis techniques are less effective 
during sporadic operation periods. A wide range of legitimate 
changes  confound  anomaly  diagnosis  and  make  baseline 
establishment for “normal” operation difficult. The increasing 
frequency of these sporadic operations (e.g. due to continuous 
deployment)  is  exacerbating  the  problem.  Diagnosing  failures 
during  sporadic  operations  relies  heavily  on  logs,  while  log 
analysis  challenges  stemming  from  noisy,  inconsistent  and 
voluminous 
largely 
unsolved.   
In  this  paper,  we  propose  Process  Oriented  Dependability 
(POD)-Diagnosis,  an  approach  that  explicitly  models  these 
sporadic operations as processes. These models allow us to (i) 
determine  orderly  execution  of  the  process,  and  (ii)  use  the 
process context to filter logs, trigger assertion evaluations, visit 
fault  trees  and  perform  on-demand  assertion  evaluation  for 
online  error  diagnosis  and  root  cause  analysis.  We  evaluated 
the  approach  on  rolling  upgrade  operations  in  Amazon  Web 
Services 
simultaneous 
operations. During our evaluation, we correctly detected all of 
the  160  injected  faults,  as  well  as 46  interferences  caused  by 
concurrent operations. We did this with 91.95% precision. Of 
the  correctly  detected  faults,  the  accuracy  rate  of  error 
diagnosis is 96.55%.  
Keywords  –  system  administration;  cloud;  deployment; 
process mining; error detection; error diagnosis; DevOps 
(AWS)  while  performing  other 
I. 
 INTRODUCTION 
Large-scale applications in cloud systems may consist of 
thousands  of  nodes  with  complex  software  stacks  inside 
each  node  and  dependencies  among  nodes.  Diagnosing 
operation  errors  in  such  applications  has  always  been 
difficult and there are a large number of works in this field 
[1].  The  recent  emergence  of  DevOps,  infrastructure-as-
code,  and  elastic  cloud  resources  are  adding  some  new 
challenges.  
In the past, major sporadic changes (such as upgrade) to 
large-scale applications would be infrequent and often done 
during  scheduled  downtime  with  careful  execution.  Now, 
with  high  frequency  continuous  deployment,  sporadic 
changes are being automated by complex scripts (including 
code  that  manipulates  the  infrastructure)  and  pre-defined 
978-1-4799-2233-8/14 $31.00 © 2014 IEEE
978-1-4799-2233-8/14 $31.00 © 2014 IEEE
978-1-4799-2233-8/14 $31.00 © 2014 IEEE
DOI 10.1109/DSN.2014.94
DOI 10.1109/DSN.2014.94
DOI 10.1109/DSN.2014.94
252
252
252
triggers,  with  only  occasional  human  intervention.  These 
sporadic but now high frequency operations often touch the 
entire production system with significant consequences. This 
has  introduced  a  number  of  problems.  First,  fully  testing 
these  operation  scripts/code  is  difficult  as  mimicking  the 
scale and complexity of the real production environment is 
not easy. Second, traditional exception handling mechanisms 
are best suited for a single language environment and break 
down as operations have to deal with different types of error 
responses  from  different  systems  –  ranging  from  the  error 
code  of  a  cloud  API  call  to  a  potential  silent  failure  of  a 
configuration  change.  This  situation  demands 
robust 
monitoring,  diagnosis  and  recovery  during  the  sporadic 
operations.  Yet, 
anomaly-detection-based 
diagnosis approaches (e.g. based on rules, statistics, machine 
learning)  [1]  are  built  for  use  during  “normal”  operations. 
They assume a system has normal operation profiles that can 
be  learned  from  historical  data  and  deviations  from  the 
profiles can help detect, localize, and identify faults.  
traditional 
In this paper, we adopt a model-based approach [1] that 
explicitly models these sporadic operations as processes and 
uses the process context to locate errors, filter logs, visit fault 
trees, and perform on-demand assertion evaluation for online 
error  diagnosis  and  root  cause  analysis.  In  particular,  our 
error diagnosis relies on a process model to specify the key 
steps and the desired system states (through assertions) after 
each  key  step.  We  then  perform  assertion  evaluation  and 
process  conformance  checking  at  runtime  to  locate  errors 
within  the  process  context.  We  further  diagnose  errors 
through 
the  use  of  on-demand  assertion  evaluations 
(diagnosis  tests)  with  the  help  of  fault  trees  (essentially  a 
structured  repository  of  known  errors  and  root  causes).  In 
terms  of  creating  the  process  model,  our  initial  work  [2] 
shows  that  the  process  can  be  discovered  using  process 
mining  [3]  on  normally  generated  logs  of  successful 
operations.  
A  rolling  upgrade  operation  is  a  prime  example  of  a 
sporadic operation and we will use it as our case study. We 
describe rolling upgrade in the next section. Errors during a 
rolling  upgrade  must  be  identified,  diagnosed  and  handled 
within minutes or seconds as the full production system is at 
stake.  Because  of  the  problems  outlined  earlier,  standard 
monitoring  of  normal  operations 
is  often  deliberately 
disabled during the rolling upgrade. There is often a long gap 
between  an  error  and  its  associated  failure  and  there  is  no 
online  error  diagnosis  if  the  error  is  detected.  The  default 
recovery  is  usually  a  complete  but  equally  risky  rollback 
operation (unless expensive redundancy is used).   
Error  diagnosis  during  operation  time  heavily  relies  on 
logs.  In  addition  to  the  unique  challenges  due  to  the  high 
frequency  and  sporadic  nature  of  the  operations,  there  are 
also log analysis challenges [4] exacerbated by the uncertain 
cloud  environment,  multiple  distributed  log  sources  and 
simultaneous  confounding  operations.  Our  POD-diagnosis 
approach also deals with them specifically.  
Another  challenge  to  log  analysis  comes  from  the 
voluminous  and  inconsistent  logs  from  different  systems 
including  cloud  infrastructure  and  application  ecosystems. 
Events appearing in the log are often out of the context [4]. 
Our approach uses Logstash1 to filter and collect distributed 
logs  from  different  sources  using  process/step-specific 
regular expressions and uses process conformance checking 
to localize the issues.  
The  cloud  environment  introduces  uncertainties  for 
operations  that  have  traditionally  been  under  the  direct 
control of an enterprise. Enterprises have limited visibility of 
a cloud environment and have to rely on cloud infrastructure 
APIs and technologies to perform their sporadic operations. 
Many  cloud  resource-provisioning  technologies  such  as 
CloudFormation2 use  a  declarative  black-box  approach  for 
achieving  the  end  results.  This  is  equally  true  for  typical 
configuration  management  tools  like  Chef3 or  Cfengine4, 
which rely on eventual convergence  to achieve desired end 
states.  These  tools  feature  neither  online  error  diagnosis 
support  nor  fine-grained  targeted  healing  –  only  complete 
rollback/opportunistic retry – if something goes wrong in the 
middle  of  the  operations.  Our  approach  breaks  down  a 
sporadic  operation 
finer-grained  steps,  performs 
conformance  checking  to  detect  any  deviation  from  the 
expected  execution  of  the  process,  defines  intermediate 
expected outcomes through assertions, defines a fault tree for 
all  the  (known)  possible  intermediate  errors  and  performs 
diagnosis  tests  visiting  the  fault  tree  for  its  root  causes. 
Finally,  the  existing  works  in  error  diagnosis  lack  the 
awareness  of  multi-operations  and  ecosystem  context  [4]. 
The built-in exception handling in tools such as Asgard5 or 
Chef has only local information about the current operation it 
handles.  Our  approach  has  global  visibility  as  it  uses  the 
aggregated  and  process-annotated 
logs  from  different 
operations in the central repository for error diagnosis.  
We  implemented  our  approach  in  a  prototype  and 
evaluated our approach in AWS. The evaluation results show 
that  95%  of  our  online  error  diagnosis  finishes  within  3.83 
seconds,  with  an  overall  precision  of  91.95%,  a  recall  of 
100% for error detection, and 97.13% accuracy rate for root 
cause diagnosis. 
Contributions 6:  The  key  contribution  made  by  our 
Process-Oriented  Dependability  (POD)-Diagnosis  approach 
into 
1 Logstash – http://logstash.net  
2 Cloudformation – http://aws.amazon.com/cloudformation/  
3 Chef – http://www.opscode.com/chef/  
4 Cfengine – http://cfengine.com/  
5 Asgard – https://github.com/Netflix/asgard 
6 [2] is an early version of this work. The paper gives an initial idea 
of providing a holistic process view of system operations. It does 
is the use of process context (such as operation process id, 
instance  id,  step  id,  conformance  status)  to  improve  the 
success  of  error  detection  and  diagnosis  in  this  particular 
domain. Process context has not been applied before in this 
domain. Our technique is non-intrusive and does not require 
any modification to the operation scripts/tools. Also, we use 
fault  trees  to  capture  potential  faults  and  root  causes  of 
intermediate errors, and the associated on-demand assertions 
(diagnosis checks) allow pinpointing root causes. 
Another contribution is the evaluation, where we injected 
faults  in  160  runs  of  a  rolling  upgrade  process  on  AWS  to 
test accuracy and completeness of POD-Diagnosis. There are 
no  equivalent  tools  to  compare  against  in  this  particular 
domain; hence this is an exploratory study. 
The  paper  proceeds  by  introducing  rolling  upgrade  in 
Section II, followed detailed discussions of POD-Diagnosis 
in  Section  III.  Section  IV  presents  the  implementation  of 
POD-Diagnosis. We evaluate our approach in Section V and 
discuss limitations in Section VI. Related work is discussed 
in Section VII, and Section VIII concludes the paper.  
II.  OPERATIONS PROCESS EXAMPLE: ROLLING UPGRADE 
Rolling  upgrade  is  an  important  element  of  continuous 
delivery  and  high  frequency  releases.  In  continuous 
deployment,  a  rolling  upgrade  of  the  entire  system  can 
happen multiple times a day [5] without system downtime. 
Assume  an  application  is  running  in  the  cloud.  It 
consists  of  a  collection  of  virtual  machine  instances, 
instantiated  from  a  smaller  number  of  different  virtual 
machine images. Say, a new machine image representing a 
new  release  for  one  of  the  images  (VMR)  is  available  for 
deployment. The current version of VMR is VA and the goal 
is to replace the N instances currently executing VA with N 
instances executing the new version VB. A further goal is to 
do this replacement while still providing a sufficient level of 
service  to  clients  of  VMR:  at  any  point  of  time  during  the 
replacement  process,  at  least  N’  instances  running  some 
version of VMR should be available for service.  
One method for performing this upgrade is called rolling 
upgrade [6]. In a rolling upgrade, a small number of k (with 
k=N-N’) instances at a time currently running version VA are 
taken out of service and replaced with k instances running 
version  VB.  By  only  upgrading  k  machines,  the  number  of 
instances in service remains at the desired level of N’. The 
time  taken  by  the  replacement  process  for  one  instance  is 
usually in the order of minutes. Consequently performing a 
rolling upgrade for 100s or 1000s of instances will take on 
the  order  of  hours.  The  virtue  of  rolling  upgrade  is  that  it 
only  requires  a  small  number  of  additional  instances  to 
perform  the  upgrade.  Rolling  upgrade  is  the  industry 
standard method for upgrading instances [6]. 
In this paper, our examples are the provisioning failures 
that can occur during rolling upgrade. A provisioning failure 
not  contain  conformance  checking,  error  diagnosis,  or  an  in-
depth evaluation. 
253253253
occurs  during  the  replacement  process,  specifically  when 
one of the upgrade steps produces incorrect results. We do 
not  consider  failures  due  to  logical  inconsistencies  among 
the versions being active simultaneously. 
Current  implementations  of  rolling  upgrade  vary.  We 
base our discussion of provisioning failures on the method 
Asgard currently  uses.  Asgard  is  Netflix’s  customized 
management  console  on  top  of  AWS  infrastructure.  For 
provisioning failures, Asgard will recognize them and report 
the  failure  to  the  operator.  The  time  between  the  failure 
occurring and the report to the operator may be as long as 
70  minutes.  Asgard  may  not  recognize  some  provisioning 
failures  such  as  failures  caused  by  concurrent  independent 
upgrades. 
environment  to  detect  anomalies.  Detection  of  an  anomaly 
triggers error diagnosis, which is reported to the operator. 
A.  Offline Process Mining and Assertion Creation 
The  process  model  can  be  created  manually  or 
discovered from logs by process mining [3]. The granularity 
of  the  model  is  determined  by  the  analyst.  The  granularity 
may  be  constrained  by  log  granularity  if  the  process  is 
discovered  from  logs.  It  is  possible  to  construct  a  process 
model  from  scripts  or  runbooks  and  not  be  constrained  by 
logs. To track the execution of the process, the analyst still 
needs to specify which log lines belong to which activity.  
III.  POD-DIAGNOSIS 
 Non-log 
 Non-log 
 Non-log 
 trigger
 trigger
 trigger
Co
Conformance 
checking 
  Assertion 