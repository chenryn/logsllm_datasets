back and help with implementing LHD. We are also grateful to the
Parallel Data Lab (PDL) at CMU for providing compute resources to
us. This work was funded by NSF Grants 1700521 and 2007733, and
supported in part by the CONIX Research Center, one of six centers
in JUMP, a Semiconductor Research Corporation (SRC) program
sponsored by DARPA.
REFERENCES
[1] K. Aasaraai and A. Moshovos. An efficient non-blocking data cache for soft
processors. In 2010 International Conference on Reconfigurable Computing and
FPGAs, pages 19–24, 2010.
[2] Ravindra K Ahuja, Thomas L Magnanti, and James B Orlin. Network Flows: Theory,
Algorithms, and Applications. Prentice hall, 1993.
[3] Susanne Albers, Sanjeev Arora, and Sanjeev Khanna. Page replacement for
general caching problems. In SODA, pages 31–40, 1999.
[4] Wisnu Anggoro and John Torjo. Boost. Asio C++ Network Programming. Packt
Publishing Ltd, 2015.
Figure 22: At extremely high latencies, mad outperforms Belady’s
algorithm for CDN.
mad can out-perform Belady’s algorithm. We were surprised
to notice that mad can out-perform Belady’s algorithm. Figure 22
illustrates LRU-mad, LHD-mad, and ARC-mad in the CDN setting,
now normalized to the latency achieved by Belady’s algorithm rather
than their baseline online algorithms.
6 LIMITATIONS AND OPEN QUESTIONS
This paper opens up a broad range of theoretical and practical
questions and we are only able to answer some of them.
Our model of caches (§2.2) is very simple and there are many at-
tributes of practical systems that it does not capture; richer and more
complex scenarios hence merit additional investigation in both the
online and offline setting. For example, our theoretical model does
not account for variable backing store latency (although our eval-
uation does measure this setting), nor does it account for differing
objectsizes.Bothourtheoryandsimulatorassumethat,oncethedata
fetch delay has passed, all outstanding delayed hits are immediately
processed and released, although many systems may instead operate
over each response sequentially leading to additional queuing at the
cache. Finally, in the online setting, prefetching algorithms may also
merit a second look with respect to latency and delayed hits.
Another nagging concern of ours is that we have have yet to prove
the hardness of the delayed hits optimization problem. While all in-
dicators point towards a hard problem, a formal proof remains open.
Finally, while the online algorithm we propose in this paper seems
to perform well empirically, we now know that it has a poor compet-
itive ratio [40]. Consequently, we don’t expect mad to be the final
word on latency-minimizing caching in the presence of delayed hits;
indeed, we believe randomized algorithms will yield better results.
7 RELATED WORK
Cachingalgorithmshavereceivedasignificantamountofresearch
attention, but the aspect of delayed hits is largely disregarded in the
literature.Wearenotawareofanypriorworkproposingananalytical
modelforthedelayedhitsproblem,ordesigningalgorithmstargeting
delayed hits. Most existing caching algorithms focus on maximizing
hit ratios, with significant advances in recent work [5, 12, 13, 29, 38,
55]andexcellentsurveysofolderwork[48,60].Therearetwogroups
of prior work that look at maximizing metrics other than hit ratios.
(1) Cost-aware online caching algorithms. This group of algo-
rithms [15, 30–32, 36, 65, 66] seeks to minimize the average cost
of misses, where an object’s cost models differences in retrieval
latencies or computation costs. In this setting, if an object is
cached, its next request does not contribute to the overall av-
erage cost, but no other requests are affected. This is different
from the delayed hits settings where a single caching decision
1(1 us)10(10 us)100(100 us)1K(1 ms)10K(10 ms)100K(100 ms)1M(1 s)10M(10 s)Z755025025%Latency ImprovementRelative to BeladyPolicyLRU-MADARC-MADLHD-MADCaching with Delayed Hits
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
2018. Association for Computing Machinery.
[38] Suoheng Li, Jie Xu, Mihaela van der Schaar, and Weiping Li. Popularity-driven
content caching. In IEEE INFOCOM, pages 1–9, 2016.
[39] Shuang Liang, Ke Chen, Song Jiang, and Xiaodong Zhang. Cost-aware caching
algorithms for distributed storage servers. In Andrzej Pelc, editor, Distributed
Computing, pages 373–387, Berlin, Heidelberg, 2007. Springer Berlin Heidelberg.
[40] Peter Manohar and Jalani Williams. Lower Bounds for Caching with Delayed
Hits. arXiv:2006.00376 [cs.DS], 2020.
[41] Nimrod Megiddo and Dharmendra S Modha. Arc: A self-tuning, low overhead
replacement cache. In USENIX FAST, pages 115–130, 2003.
[42] Matthew K Mukerjee, David Naylor, Junchen Jiang, Dongsu Han, Srinivasan
Seshan, and Hui Zhang. Practical, real-time centralized control for CDNM-based
live video delivery. In ACM SIGCOMM, pages 311–324, 2015.
[43] A. Musa, Y. Sato, T. Soga, R. Egawa, H. Takizawa, K. Okabe, and H. Kobayashi.
Effects of mshr and prefetch mechanisms on an on-chip cache of the vector
architecture. In 2008 IEEE International Symposium on Parallel and Distributed
Processing with Applications, pages 335–342, 2008.
[44] Ravi Netravali, Ameesh Goyal, James Mickens, and Hari Balakrishnan. Polaris:
In USENIX NSDI,
Faster page loads using fine-grained dependency tracking.
March 2016.
[45] Gurobi Optimization. Inc.,“gurobi optimizer reference manual,” 2015, 2014.
[46] Vern Paxson. Bro: a system for detecting network intruders in real-time. Computer
networks, 31(23-24):2435–2463, 1999.
[47] Ben Pfaff, Justin Pettit, Teemu Koponen, Ethan Jackson, Andy Zhou, Jarno
Rajahalme, Jesse Gross, Alex Wang, Joe Stringer, Pravin Shelar, et al. The design
and implementation of open vswitch. In USENIX NSDI, pages 117–130, 2015.
[48] Stefan Podlipnig and Laszlo Böszörmenyi. A survey of web cache replacement
strategies. ACM Computing Surveys, 35(4):374–398, 2003.
[49] M. K. Qureshi, D. N. Lynch, O. Mutlu, and Y. N. Patt. A case for mlp-aware cache
replacement. In ACM/IEEE ISCA, pages 167–178, 2006.
[50] Prabhakar Raghavan and Clark D Tompson. Randomized rounding: a technique
for provably good algorithms and algorithmic proofs. Combinatorica, 7(4):365–374,
1987.
[51] KV Rashmi, Mosharaf Chowdhury, Jack Kosaian, Ion Stoica, and Kannan
Ramchandran. Ec-cache: Load-balanced, low-latency cluster caching with online
erasure coding. In USENIX OSDI, pages 401–417, 2016.
[52] Boris Schäling. The boost C++ libraries. Boris Schäling, 2011.
[53] James Edward Sicolo. A multiported nonblocking cache for a superscalar
uniprocessor. Master’s thesis, University of Illinois at Urbana-Champaign, 1992.
[54] Abraham Silberschatz, Greg Gagne, and Peter B Galvin. Operating system concepts.
Wiley, 2018.
[55] Zhenyu Song, Daniel S. Berger, and Lloyd Wyatt LI, Kai. Learning relaxed belady
for content distribution network caching. In USENIX NSDI, 2020.
[56] Edward S Tam. Improving cache performance via active management. PhD thesis,
University of Michigan, 1999.
[57] Eric Torng. A unified analysis of paging and caching. Annual Symposium on
Foundations of Computer Science - Proceedings, 08 1995.
[58] J. Tuck, L. Ceze, and J. Torrellas.
Scalable cache miss handling for high
memory-level parallelism. In ACM/IEEE MICRO, pages 409–422, 2006.
[59] Colby Walsworth, Emile Aben, K Claffy, and D Andersen. The caida anonymized
2019 internet traces, 2019.
[60] Jia Wang. A survey of web caching schemes for the internet. ACM SIGCOMM
Computer Communication Review, 29(5):36–46, 1999.
[61] Justin Wang, Benjamin Berg, Daniel S Berger, and Siddhartha Sen. Maximizing
page-level cache hit ratios in largeweb services. ACM SIGMETRICS Performance
Evaluation Review, 46(2):91–92, 2019.
[62] Sage A Weil, Scott A Brandt, Ethan L Miller, Darrell DE Long, and Carlos Maltzahn.
Ceph: A scalable, high-performance distributed file system. In USENIX OSDI,
pages 307–320, 2006.
[63] Wikipedia. Reverse proxy. https://en.wikipedia.org/wiki/Reverse_proxy.
[64] Maurice V Wilkes. Slave memories and dynamic storage allocation.
IEEE
Transactions Electronic Computers, 14(2):270–271, 1965.
[65] Neal E Young. On-line caching as cache size varies. In ACM SODA, 1991.
[66] Neal E Young. On-line file caching. Algorithmica, 33(3):371–383, 2002.
[5] Nathan Beckmann, Haoxian Chen, and Asaf Cidon. Lhd: Improving hit rate by
maximizing hit density. In USENIX NSDI., pages 1–14, 2018.
[6] Nathan Beckmann, Phillip B Gibbons, Bernhard Haeupler, and Charles McGuffey.
Writeback-aware caching. In Symposium on Algorithmic Principles of Computer
Systems, pages 1–15. SIAM, 2020.
[7] Laszlo A. Belady. A study of replacement algorithms for a virtual-storage
computer. IBM Systems journal, 5(2):78–101, 1966.
[8] Samson Belayneh and David R. Kaeli. A discussion on non-blocking/lockup-free
caches. SIGARCH Comput. Archit. News, 24(3):18–25, June 1996.
[9] Daniel S Berger. Towards lightweight and robust machine learning for cdn
caching. In ACM HotNets, pages 134–140, 2018.
[10] Daniel S. Berger, Nathan Beckmann, and Mor Harchol-Balter. Practical bounds
on optimal caching with variable object sizes. ACM POMACS, 2(2):32, 2018.
[11] Daniel S Berger, Benjamin Berg, Timothy Zhu, Siddhartha Sen, and Mor
Harchol-Balter. Robinhood: Tail latency aware caching–dynamic reallocation
from cache-rich to cache-poor. In USENIX OSDI, pages 195–212, 2018.
[12] Daniel S. Berger, Ramesh Sitaraman, and Mor Harchol-Balter. Adaptsize:
Orchestrating the hot object memory cache in a cdn. In USENIX NSDI, pages
483–498, March 2017.
[13] Aaron Blankstein, Siddhartha Sen, and Michael J Freedman. Hyperbolic caching:
Flexible caching for web applications. In USENIX ATC, pages 499–511, 2017.
[14] Niv Buchbinder, Joseph Seffi Naor, et al. The design of competitive online
algorithms via a primal–dual approach. Foundations and Trends in Theoretical
Computer Science, 3(2–3):93–263, 2009.
[15] Pei Cao and Sandy Irani. Cost-aware www proxy caching algorithms. In Usenix
symposium on internet technologies and systems, volume 12, pages 193–206, 1997.
[16] Yue Cheng, Fred Douglis, Philip Shilane, Grant Wallace, Peter Desnoyers, and
Kai Li. Erasing belady’s limitations: In search of flash cache offline optimality.
In USENIX ATC, pages 379–392, 2016.
[17] Marek Chrobak, H Karloof, Tom Payne, and S Vishwnathan. New ressults on
server problems. SIAM Journal on Discrete Mathematics, 4(2):172–181, 1991.
[18] Marek Chrobak, Gerhard J Woeginger, Kazuhisa Makino, and Haifeng Xu.
Caching is hard—even in the fault model. Algorithmica, 63(4):781–794, 2012.
[19] Jeff Dean and R. Colin Scott. Numbers every programmer should know.
https://people.eecs.berkeley.edu/ rcs/research/interactive_latency.html.
[20] John Dilley and Martin Arlitt. Improving proxy cache performance: Analysis
of three replacement policies. IEEE Internet Computing, 3(6):44–50, 1999.
[21] John Dilley, Bruce Maggs, Jay Parikh, Harald Prokop, Ramesh Sitaraman, and Bill
Weihl. Globally distributed content delivery. IEEE Internet Computing, 6(5):50–58,
2002.
[22] Bin Fan, David G Andersen, and Michael Kaminsky. Memc3: Compact and
concurrent memcache with dumber caching and smarter hashing. In USENIX
NSDI, pages 371–384, 2013.
[23] A. Feldmann, R. Caceres, F. Douglis, G. Glass, and M. Rabinovich. Performance of
web proxy caching in heterogeneous bandwidth environments. In IEEE INFOCOM,
volume 1, pages 107–116 vol.1, 1999.
[24] Daniel Firestone, Andrew Putnam, Sambhrama Mundkur, Derek Chiou, Alireza
Dabagh, Mike Andrewartha, Hari Angepat, Vivek Bhanu, Adrian Caulfield, Eric
Chung, et al. Azure accelerated networking: Smartnics in the public cloud. In
USENIX NSDI, pages 51–66, 2018.
[25] Davy Genbrugge and Lieven Eeckhout. Memory data flow modeling in statistical
simulation for the efficient exploration of microprocessor design spaces. IEEE
Transactions on Computers, 57(1):41–54, 2007.
[26] K.-I. Goh and A.-L. Barabási. Burstiness and memory in complex systems. EPL
(Europhysics Letters), 81(4):48002, jan 2008.
[27] John L Hennessy and David A Patterson. Computer architecture: a quantitative
approach. Elsevier, 4 edition, 2011.
[28] SNIA IOTTA. Microsoft production server traces, 2011.
[29] Akanksha Jain and Calvin Lin. Back to the future: leveraging belady’s algorithm
for improved cache replacement. In ACM/IEEE ISCA, pages 78–89, 2016.
[30] Jaeheon Jeong and Michel Dubois.
Cache replacement algorithms with
nonuniform miss costs. IEEE Transactions on Computers, 55(4):353–365, 2006.
[31] Shudong Jin and Azer Bestavros. Popularity-aware greedy dual-size web proxy
caching algorithms. In IEEE ICDCS, pages 254–261, 2000.
[32] Shudong Jin and Azer Bestavros. Greedydual⋆ web caching algorithm: exploiting
Computer
the two sources of temporal locality in web request streams.
Communications, 24(2):174–183, 2001.
[33] Daehyeok Kim, Yibo Zhu, Changhoon Kim, Jeongkeun Lee, and Srinivasan Seshan.
Generic External Memory for Switch Data Planes. In ACM HotNets, pages 1–7, 2018.
[34] Rupa Krishnan, Harsha V Madhyastha, Sridhar Srinivasan, Sushant Jain, Arvind
Krishnamurthy, Thomas Anderson, and Jie Gao. Moving beyond end-to-end path
information to optimize cdn performance. In ACM IMC, pages 190–201, 2009.
[35] David Kroft. Lockup-free instruction fetch/prefetch cache organization.
In
ACM/IEEE ISCA, page 81–87, Washington, DC, USA, 1981. IEEE Computer Society
Press.
[36] Conglong Li and Alan L Cox. Gd-wheel: a cost-aware replacement policy for
key-value stores. In EUROSYS, pages 1–15, 2015.
[37] Shang Li, Dhiraj Reddy, and Bruce Jacob. A performance & power comparison
of modern high-speed dram architectures.
In Proceedings of the International
Symposium on Memory Systems, MEMSYS ’18, page 341–353, New York, NY, USA,
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Nirav Atre, Justine Sherry, Weina Wang, and Daniel S. Berger
(4)
(6)
1
−1
0
ai(T) =
A APPENDIX
Appendices are supporting material that has not been peer-reviewed.
A.1 Latency Minimization Problem
In this section we give a formal definition of the latency minimiza-
tion problem for caching with delayed hits.
Recall that we consider a cache of size C and M objects indexed by
i ∈[M]. We are given a sequence object requests, where σ(T) denotes
the object requested at timestep T with T =0,1,...,N .
We use the following quantities to describe the state of the system
at the beginning of each timestep T . For each object i, let
x
x
(i)
0 (T) = 1{object i is in the cache at T },
(i)
τ (T) = 1{object i was requested at T −(Z +1−τ)
and the request has not been resolved}
τ =1,...,Z .
(5)
Here, when an object i is requested but cannot be resolved immedi-
ately, we say that we put it in a queue. So (5) describes the state of
the queue for i.
Let ai(T) be defined by
We specify a cache schedule using the following decision variables.
if object i is admitted to cache at T ,
if object i is evicted from cache at T ,
if no action is taken on object i at T .
To make sure ai(T) with i ∈ [M],T = 0,1,...,N form a valid cache
schedule, we enforce the following constraints for each object i ∈[M]
and timestep T =0,1,...,N :
• An object can be admitted only when its data arrives:
• An object can be evicted only when it is already in the cache:
(8)
• The schedule should guarantee that the number of objects in the
(i)
1 (T).
(i)
0 (T)
cache is no larger than the cache size C:
(i)
0 (T)≤C.
1{ai(T)=1} ≤ x
1{ai(T)=−1} ≤ x

(7)
Although it seems that this is a constraint on the state, it is in fact
a constraint on the cache schedule since the state at the current
timestep is determined by the past decisions. This will become
clear after we describe the relation between the state and the
schedule next.
With the notation above, we can write out how the system state
evolves over time as follows:
• The data that just arrived resolves the requests for the same object
in the queue, and other requests move forward in queue:
(i)
τ (T +1) =x
(i)
τ +1(T)·(1−x
(i)
1 (T)),
(10)
• The admission or eviction of an object changes the state in the
i ∈[M],τ =1,...,Z−1,T =0,1,...,N −2.
x
x
i∈[M]
cache:
(i)
0 (T +1) =x
x
(i)