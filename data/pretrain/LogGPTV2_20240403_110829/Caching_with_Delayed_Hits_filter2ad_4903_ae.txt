### Acknowledgments
We would like to express our gratitude to [Name] for their valuable assistance in implementing LHD. We are also thankful to the Parallel Data Lab (PDL) at Carnegie Mellon University (CMU) for providing us with computational resources. This research was supported by NSF Grants 1700521 and 2007733, and partially funded by the CONIX Research Center, one of six centers in JUMP, a Semiconductor Research Corporation (SRC) program sponsored by DARPA.

### References
1. K. Aasaraai and A. Moshovos. An efficient non-blocking data cache for soft processors. In *2010 International Conference on Reconfigurable Computing and FPGAs*, pages 19–24, 2010.
2. Ravindra K. Ahuja, Thomas L. Magnanti, and James B. Orlin. *Network Flows: Theory, Algorithms, and Applications*. Prentice Hall, 1993.
3. Susanne Albers, Sanjeev Arora, and Sanjeev Khanna. Page replacement for general caching problems. In *SODA*, pages 31–40, 1999.
4. Wisnu Anggoro and John Torjo. *Boost.Asio C++ Network Programming*. Packt Publishing Ltd, 2015.

### Figure 22: Performance Comparison at High Latencies
At extremely high latencies, the mad algorithm outperforms Belady’s algorithm in the context of Content Delivery Networks (CDNs). This is illustrated in Figure 22, where the performance of LRU-mad, LHD-mad, and ARC-mad is normalized to the latency achieved by Belady’s algorithm, rather than their respective baseline online algorithms.

### 6. Limitations and Open Questions
This paper opens up a broad range of theoretical and practical questions, and we have only been able to address some of them. Our model of caches, as described in §2.2, is relatively simple and does not capture many attributes of practical systems. More complex scenarios, such as variable backing store latency and differing object sizes, merit further investigation in both online and offline settings.

In our theoretical model, once the data fetch delay has passed, all outstanding delayed hits are immediately processed and released. However, in many real-world systems, responses may be processed sequentially, leading to additional queuing at the cache. Additionally, in the online setting, prefetching algorithms may need to be re-evaluated with respect to latency and delayed hits.

Another open question is the hardness of the delayed hits optimization problem. While indicators suggest it is a hard problem, a formal proof remains elusive. Furthermore, although the online algorithm proposed in this paper performs well empirically, it has a poor competitive ratio [40]. Therefore, we do not expect mad to be the final solution for latency-minimizing caching in the presence of delayed hits. Randomized algorithms may yield better results.

### 7. Related Work
Caching algorithms have received significant research attention, but the aspect of delayed hits is largely overlooked in the literature. To our knowledge, there is no prior work proposing an analytical model for the delayed hits problem or designing algorithms specifically targeting delayed hits. Most existing caching algorithms focus on maximizing hit ratios, with notable advancements in recent studies [5, 12, 13, 29, 38, 55] and comprehensive surveys of older work [48, 60].

There are two groups of prior work that focus on optimizing metrics other than hit ratios:
1. **Cost-aware online caching algorithms**: These algorithms [15, 30-32, 36, 65, 66] aim to minimize the average cost of misses, where an object's cost models differences in retrieval latencies or computation costs. In this setting, if an object is cached, its next request does not contribute to the overall average cost, but no other requests are affected. This is different from the delayed hits setting, where a single caching decision can affect multiple future requests.

### Appendix A: Latency Minimization Problem
In this section, we provide a formal definition of the latency minimization problem for caching with delayed hits.

#### Definitions
- **Cache Size (C)**: The total capacity of the cache.
- **Objects (M)**: A set of objects indexed by \( i \in [M] \).
- **Request Sequence (σ(T))**: A sequence of object requests, where \( \sigma(T) \) denotes the object requested at timestep \( T \) with \( T = 0, 1, \ldots, N \).

#### State Variables
- **x(i)0(T)**: Binary variable indicating whether object \( i \) is in the cache at time \( T \).
- **x(i)τ(T)**: Binary variable indicating whether object \( i \) was requested at \( T - (Z + 1 - τ) \) and the request has not been resolved, for \( τ = 1, \ldots, Z \).

#### Decision Variables
- **a(i)(T)**: 
  - \( 1 \) if object \( i \) is admitted to the cache at \( T \),
  - \( -1 \) if object \( i \) is evicted from the cache at \( T \),
  - \( 0 \) if no action is taken on object \( i \) at \( T \).

#### Constraints
- **Admission Constraint**:
  \[
  \sum_{i \in [M]} 1\{a(i)(T) = 1\} \leq x(i)1(T)
  \]
- **Eviction Constraint**:
  \[
  \sum_{i \in [M]} 1\{a(i)(T) = -1\} \leq x(i)0(T)
  \]
- **Cache Capacity Constraint**:
  \[
  \sum_{i \in [M]} x(i)0(T) \leq C
  \]

#### State Evolution
- **Queue Update**:
  \[
  x(i)τ(T + 1) = x(i)τ+1(T) \cdot (1 - x(i)1(T)), \quad \text{for } i \in [M], \tau = 1, \ldots, Z-1, T = 0, 1, \ldots, N-2
  \]
- **Cache State Update**:
  \[
  x(i)0(T + 1) = x(i)0(T) + 1\{a(i)(T) = 1\} - 1\{a(i)(T) = -1\}
  \]

These definitions and constraints form the basis for the latency minimization problem in the context of caching with delayed hits.