a label to each vector vi ∈ Vd, according to the nature of the
domain name d from which vi was computed. The domains in
Notos’ knowledge base belong to different classes. In particu-
lar, we distinguish between eight different classes of domains,
namely Popular Domains, Common Domains, Akamai, CDN,
and Dynamic DNS, which have the same meaning as explained
in Section 3.3.2, and Spam Domains, Flux Domains, and Mal-
ware Domains.
In order to compute the output vector DC(d), we compute
the following ﬁve statistical features: the majority class label
L (e.g., L may be equal to Malware Domain), i.e., the label
that appears the most among the vectors vi ∈ Vd; the stan-
dard deviation of label frequencies, i.e., given the occurrence
frequency of each label among the vectors vi ∈ Vd we com-
d ⊆ Vd of
pute their standard deviation; given the subset V (L)
vectors in Vd that are associated with label L, we compute
the mean, median and standard deviation of the distribution
of distances between zd and the vectors vj ∈ V (L)
.
d
3.3.6 Assigning Reputation Scores
Given a domain d, once we compute the vectors N M(d) and
DC(di) as explained above, we also compute the evidence
vector EV (d) as explained in Section 3.2.3. At this point, we
concatenate these three feature vectors into a sixteen dimen-
sional feature vector v(d), and we feed v(d) in input to our
trained reputation function (see Section 3.3.4). The reputa-
tion function computes a score S = 1 − f(d), where f(d) can
be interpreted as the probability that d is a malicious domain
name. S varies in the [0, 1] interval, and the lower the value of
S, the lower d’s reputation.
4 Data Collection and Analysis
This section summarizes observations from passive DNS
measurements, and how professional, legitimate DNS services
are distinguished from malicious services. These observations
provided the ground truth for our dynamic domain name rep-
utation system. We also provide an intuitive example to illus-
trate these properties, using a few major Internet zones like
Akamai and Google.
4.1 Data Collection
The basic building block for our dynamic reputation rating
system is the historical or “passive” information from success-
ful A-type DNS resolutions. We use the DNS trafﬁc from
two ISP-based sensors, one located on the US east coast (At-
lanta) and one located on the US west coast (San Jose). Addi-
tionally we use the aggregated DNS trafﬁc from the different
networks covered by the SIE [3]. In total, our database col-
lected 27,377,461 unique resolutions from all these sources
over a period of 68 days, from 19th of July 2009 to 24th
September 2009.
Simple measurements performed on this large data set
demonstrate a few important properties leveraged by our se-
lected features. After just a few days the rate of new, unique
pDNS entries leveled off. The graph in Figure 7(b) shows
only about 100,000 to 150,000 new domains/day (with a brief
outage issue on the 53rd day), despite very large numbers of
Figure 7. Various RRs growth trends observed in the pDNS DB over a period of 68 days
RRs arriving each day (shown in Figure 7(a)). This suggests
that most RRs are duplicates, and approximately after the ﬁrst
few days, 94.7% – on average – from the unique RRs ob-
served in daily base at the sensor level are already recorded by
the passive DNS database. Therefore, even a relatively small
pDNS database may be used to deploy Notos. In Section 5, we
measure the sensitivity of our system to trafﬁc collected from
smaller networks.
The remaining plots in Figure 7 show the daily growth of
our passive DNS database, from the point of view of ﬁve dif-
ferent zone classes. Figure 7(c) and (d) show the growth rate
associated with CDN networks (Akamai, and all other CDNs).
The number of unique IPs stays nearly constant with the num-
ber of unique domains (meaning that each new RR is a new
IP and a new child domain of the CDN). In a few weeks, most
of the IPs became known—suggesting that one can fully map
CDNs in a modest training set. This is because CDNs, al-
though large, always have a ﬁxed number of IP addresses used
for hosting their high-availability services. Intuitively, we be-
lieve this would not be the case with malicious CDNs (e.g.,
ﬂux networks), which use randomly spreading infections to
continually recruit new IPs.
The ratio of new IPs to domains diverges in Figure 7(e),
a plot of the rate of newly discovered RRs for popular web-
sites (e.g., Google, Facebook). Facebook notably uses unique
child domains for their Web-based chat client, and other top
Internet sites use similar strategies (encoding information in
the domain, instead of the URI), which explains the growth
in domains shown in Figure 7(e). These popular sites use a
very small number of IPs, however, and after a few weeks of
training our pDNS database identiﬁed all of them. Since these
popular domains make up a large portion of trafﬁc in any trace,
our intuition is that simple whitelisting would signiﬁcantly re-
duce the workload of a classiﬁer.
Figure 7(f) shows the rate of pDNS growth for zones in
Dynamic DNS providers. These services, sometimes used by
botmasters, demonstrate a nearly matched ratio of new IPs to
new domains. The data excludes non-routable answers (e.g.,
dynamic DNS domains pointing to 127.0.0.1), since this con-
tains no unique network information. Intuitively, one can think
of dynamic DNS as a nearly complete bijection of domains to
IPs. Figure 7(g) shows the growth of RRs for alexa.com
top 100 domains. Unlike dynamic DNS domains, these points
to a small set of unique addresses, and most can be identiﬁed
in a few weeks’ worth of training.
A comparison of all the zone classes appears in Figure 7(h),
which shows the cumulative distribution of the unique RRs de-
tailed in Figure 7(c) through (g). The different rates of change
illustrate how each zone class has a distinct pattern of RR use:
some have a small IP space and highly variable domain names;
some pair nearly every new domain with a new IP. Learning
approximately 90% of all the unique RRs in each zone class,
however, only requires (at most) tens of thousands of distinct
RRs. The intuition from this plot is that, despite the very large
 10 100 1000 10000 100000 1e+06 1e+07 0 10 20 30 40 50 60 70Volume Of New RRsDays(b) New RRs Growth In pDNS DB For All ZonesNew RRs 0 500000 1e+06 1.5e+06 2e+06 2.5e+06 3e+06 3.5e+06 4e+06 0 10 20 30 40 50 60 70Volume Of Unique RRsDays(a) Unique RRs In The Two ISPs Sensors (per day)Unique RRs 1 10 100 1000 10000 1 10 100Volume(c) Akamai Class Growth Over Time (Days)Unique DNUnique IPsNew RRs 10 100 1000 10000 1 10 100Volume(d) CDN Class Growth Over Time (Days)Unique DNsUnique IPNew RRs 10 100 1000 10000 100000 1 10 100Volume(e) Pop Class Growth Over Time (Days)Unique DNUnique IPNew RRs 1 10 100 1000 1 10 100Volume(f) Dyn. DNS Class Growth Over Time (Days)Unique DNUnique IPNew RRs 1 10 100 1 10 100Volume(g) Common Class Growth Over Time (Days)Unique DNUnique IPsNew RRs 1 10 100 1000 10000 100000 0.01 0.1 1(h) CDF Of RR Growth  For All ClassesAkamaiCommonPopCDNDynamicdata set we used in our study, Notos could potentially work
with data observed from much smaller networks.
4.2 Building The Ground Truth
To establish ground truth, we use two different labeling
processes. First, we assigned labels to RRs at the time of their
discovery. This provided an initial static label for many do-
mains. Blacklists, of course, are never complete and always
dynamic. So our second labeling process took place during
evaluation, and monitored several well-known domain black-
lists and whitelists.
The data we used for labeling came from several sources.
Our primary source of blacklisting came from services
such as malwaredomainlist.com and malwaredo-
mains.com.
In order to label IP addresses in our pDNS
database we also used the Sender Policy Block (SBL) list from
Spamhaus [18]. Such IPs are either known to send spam or
distribute malware. We also collected domain name and IP
blacklisting information from the Zeus tracker [30]. All this
blacklisting information was gathered before the ﬁrst day of
August 2009 (during all the 15 days in which we collected
passive DNS data). Since blacklists traditionally lag behind
the active threat, we continued to collect all new data until the
end of our experiments.
Our limited whitelisting was derived from the top 500-
alexa.com domain names, as of the 1st of August 2009. We
reasoned that, although some malicious domains become pop-
ular, they do not stay popular (because of remediation), and
never break into the top tier of domain rankings. Likewise, we
used a list of the 18 most common 2LDs from various CDNs,
which composed the main corpus of our CDN labeled RRs.
Finally a list of 464 dynamic DNS second level domains al-
lowed us to identify and label domain name and IPs coming
from zones under dynamic DNS providers. We label our eval-
uation (or testing) data-set by aggregating updated blacklist
information for new malicious domain names and IPs from
the same lists.
To compute the honeypot features (presented in Sec-
tion 3.2.3) we need a malware analysis infrastructure that can
process as many “new” malware samples as possible. Our
honeypot infrastructure is similar to “Ether” [4] and is capa-
ble of processing malware samples in a queue. Every malware
sample was analyzed in a controlled environment for a time
period of ﬁve minutes. This process was repeated during the
last 15 days of July 2009. After 15 days of executions we
obtained a set of successful DNS resolutions (domain names
and IPs) that each malware looked up. We chose to execute
malware and collect DNS evidence through the same period
of time in which we aggregate the passive DNS database. Our
virtual machines are equipped with ﬁve popular commercial
anti-virus engines.
If one of the engines identiﬁes an exe-
cutable as malicious, we capture all domain names and the
corresponding IP mappings that the malware used during ex-
ecution. After excluding all domain names that belong to the
top 500 most popular alexa.com zones, we assemble the
main corpus of our “honeypot data”. We automated the crawl-
ing and collection of black list information and honeypot exe-
cution.
The reader should note that we chose to label our data in
as transparent way as possible. We used public blacklisting
information to label our training dataset before we build our
models and train the reputation function. Then we assigned
the reputation scores and validated the results again using the
same publicly available blacklist sources.
It is safe to as-
sume that private IP and DNS blacklist will contain signiﬁcant
more complete information with lower FP rates than the public
blacklists. By using such type of private blacklist the accuracy
of Notos’ reputation function should improve signiﬁcantly.
5 Results
In this section, we present the experimental results of our
evaluation. We show that Notos can identify malicious domain
names sooner than public blacklists, with a low false posi-
tive rate (FP%) of 0.38% and high true positive rate (TP%)
of 96.8%. As a ﬁrst step, we computed vectors based on
the statistical features (described in Section 3.2) from 250,000
unique RRs. This volume corresponds to the average volume
of new – previously unseen – RRs observed at two recursive
DNS servers in a major ISP in one day, as noted in Section 4,
Figure 7(b). These vectors were computed based on historic
passive DNS information from the last two weeks of DNS traf-
ﬁc observed on the same two ISP recursive resolvers in Atlanta
and San Jose.
5.1 Accuracy of Network Proﬁle Modeling
The accuracy of the Meta-Classiﬁcation system (Fig-
ure 4(a)) in the network proﬁle module is critical for the over-
all performance of Notos. This is because, in the on-line mode,
Notos will receive unlabeled vectors which must be classiﬁed
and correlated with what is already present in our knowledge
base. For example, if the classiﬁer receives a new RR and as-
signs to it the label Akamai with very high conﬁdence, that
implies the RR which produced this vector will be part of a
network similar to Akamai. However, this does not necessar-
ily mean that it is part of the actual Akamai CDN. We will see
in the next section how we can draw conclusions based on the
proximity between labeled and unlabeled RRs within the same
zone-based clusters. Furthermore, we discuss the accuracy
of the Meta-Classiﬁer when modeling each different network
proﬁle class (proﬁle classes are described in Section 3.3.2).
Our Meta-Classiﬁer consists of ﬁve different classiﬁers,
one for each different class of domains we model. We chose to
use a Meta-Classiﬁcation system instead of a traditional sin-
gle classiﬁcation approach because Meta-Classiﬁcation sys-
tems typically perform better than a single statistical classi-
Figure 8. ROC curves for all network proﬁle
classes shows the Meta-Classiﬁer’s accuracy.
Figure 9. The ROC curve from the reputation func-
tion indicating the high accuracy of Notos.
ﬁer [11, 2]. Throughout our experiments this proved to be
also true. The ROC curve in Figure 8, shows that the Meta-
Classiﬁer can accurately classify RRs for all different network
proﬁle classes.
The training dataset for the Meta-Classiﬁer is composed
of sets of 2,000 vectors from each of the ﬁve network proﬁle
classes. The evaluation dataset is composed of 10,000 vectors,
2,000 from each of the ﬁve network proﬁle classes. The classi-
ﬁcation results for the domains in the Akamai, CDN, dynamic
DNS and Popular classes showed that the supervised learn-
ing process in Notos is accurate, with the exception of a small
number of false positives related to the Common class (3.8%).
After manually analyzing these false positives, we concluded
that some level of confusion between the vectors produced by
Dynamic DNS domain names and the vectors produced by
domain names in the Common class still remains. However,
this minor misclassiﬁcation between network proﬁles does not
signiﬁcantly affect the reputation function. This is because
the zone proﬁles of the Common and Dynamic DNS domain
names are signiﬁcantly different. This difference in the zone
proﬁles will drive the network-based and zone-based cluster-
ing steps to group the RRs from Dynamic DNS class and Com-
mon class in different zone-based clusters.
Despite the fact that the network proﬁle modeling process
provides accurate results, it doesn’t mean this step can inde-
pendently designate a domain as benign or malicious. The
clustering steps will assist Notos to group vectors not only
based their network proﬁles but also based on their zone prop-
erties. In the following section we show how the network and
zone proﬁle clustering modules can better associate similar
vectors, due to properties of their domain name structure.
5.2 Network and Zone-Based Clustering Results
In the domain name clustering process (Section 3.3.3, Fig-
ure 4(b)) we used X-Means clustering in series, once for the