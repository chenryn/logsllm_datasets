### 3.3.5 Labeling Vectors and Domain Classification

Each vector \( v_i \in V_d \) is assigned a label based on the nature of the domain name \( d \) from which it was computed. The domains in Notos' knowledge base are categorized into different classes. Specifically, we distinguish between eight distinct classes of domains: Popular Domains, Common Domains, Akamai, CDN, Dynamic DNS, Spam Domains, Flux Domains, and Malware Domains. These classifications are consistent with those described in Section 3.3.2.

To compute the output vector \( DC(d) \), we calculate the following five statistical features:
1. **Majority Class Label \( L \)**: This is the label that appears most frequently among the vectors \( v_i \in V_d \). For example, \( L \) could be "Malware Domain."
2. **Standard Deviation of Label Frequencies**: We compute the standard deviation of the occurrence frequencies of each label among the vectors \( v_i \in V_d \).
3. **Mean, Median, and Standard Deviation of Distances**: Given the subset \( V_d(L) \subseteq V_d \) of vectors associated with label \( L \), we compute the mean, median, and standard deviation of the distribution of distances between \( z_d \) and the vectors \( v_j \in V_d(L) \).

### 3.3.6 Assigning Reputation Scores

For a given domain \( d \), after computing the vectors \( NM(d) \) and \( DC(d) \) as described above, we also compute the evidence vector \( EV(d) \) as detailed in Section 3.2.3. We then concatenate these three feature vectors into a sixteen-dimensional feature vector \( v(d) \), which is fed into our trained reputation function (see Section 3.3.4). The reputation function computes a score \( S = 1 - f(d) \), where \( f(d) \) represents the probability that \( d \) is a malicious domain name. The score \( S \) ranges from 0 to 1, with lower values indicating a lower reputation for the domain.

### 4. Data Collection and Analysis

This section summarizes observations from passive DNS measurements and how professional, legitimate DNS services can be distinguished from malicious ones. These observations form the ground truth for our dynamic domain name reputation system. We also provide an illustrative example using major Internet zones like Akamai and Google.

#### 4.1 Data Collection

The foundation of our dynamic reputation rating system is historical or "passive" information from successful A-type DNS resolutions. We use DNS traffic from two ISP-based sensors, one located on the US east coast (Atlanta) and one on the US west coast (San Jose). Additionally, we incorporate aggregated DNS traffic from various networks covered by the SIE [3]. Over a 68-day period from July 19, 2009, to September 24, 2009, our database collected 27,377,461 unique resolutions from these sources.

Simple measurements on this large dataset reveal several key properties leveraged by our selected features. After just a few days, the rate of new, unique pDNS entries stabilized. Figure 7(b) shows that only about 100,000 to 150,000 new domains per day were observed, despite a large number of RRs arriving daily (Figure 7(a)). This suggests that most RRs are duplicates, and after the first few days, 94.7% of the unique RRs observed daily at the sensor level are already recorded in the passive DNS database. Therefore, even a relatively small pDNS database can be used to deploy Notos. In Section 5, we measure the sensitivity of our system to traffic from smaller networks.

The remaining plots in Figure 7 show the daily growth of our passive DNS database for five different zone classes. Figures 7(c) and (d) depict the growth rates for CDN networks (Akamai and other CDNs). The number of unique IPs remains nearly constant with the number of unique domains, indicating that each new RR is a new IP and a new child domain of the CDN. Within a few weeks, most IPs become known, suggesting that CDNs, although large, have a fixed number of IP addresses for their high-availability services. This is in contrast to malicious CDNs (e.g., flux networks), which continually recruit new IPs through random infections.

Figure 7(e) shows the ratio of new IPs to domains diverging for popular websites (e.g., Google, Facebook). These sites use unique child domains for specific purposes, such as web-based chat clients, leading to a growth in domains but a very small number of IPs. After a few weeks, our pDNS database identifies all of them. Since these popular domains make up a significant portion of traffic, simple whitelisting can significantly reduce the workload of a classifier.

Figure 7(f) illustrates the pDNS growth rate for Dynamic DNS providers. These services, sometimes used by botmasters, show a nearly matched ratio of new IPs to new domains. Non-routable answers (e.g., dynamic DNS domains pointing to 127.0.0.1) are excluded as they contain no unique network information. Figure 7(g) shows the growth of RRs for the top 100 alexa.com domains, which point to a small set of unique addresses and can be identified within a few weeks.

A comparison of all zone classes is shown in Figure 7(h), which depicts the cumulative distribution of unique RRs detailed in Figures 7(c) through (g). The different rates of change illustrate distinct patterns of RR use: some have a small IP space and highly variable domain names, while others pair nearly every new domain with a new IP. Learning approximately 90% of all unique RRs in each zone class requires, at most, tens of thousands of distinct RRs. Despite the large dataset, Notos can potentially work with data from much smaller networks.

#### 4.2 Building the Ground Truth

To establish ground truth, we use two labeling processes. First, we assign labels to RRs at the time of their discovery, providing an initial static label for many domains. Blacklists are never complete and always dynamic, so our second labeling process occurs during evaluation, monitoring several well-known domain blacklists and whitelists.

Our primary source of blacklisting comes from services like malwaredomainlist.com and malwaredomains.com. To label IP addresses in our pDNS database, we use the Sender Policy Block (SBL) list from Spamhaus [18], which includes IPs known to send spam or distribute malware. We also collect domain name and IP blacklisting information from the Zeus tracker [30]. All this blacklisting information was gathered before August 1, 2009, and we continued to collect new data until the end of our experiments.

Our limited whitelisting is derived from the top 500 alexa.com domain names as of August 1, 2009. We reasoned that, although some malicious domains may become popular, they do not stay popular due to remediation and do not break into the top tier of domain rankings. We also used a list of the 18 most common 2LDs from various CDNs and a list of 464 dynamic DNS second-level domains to identify and label domain names and IPs from dynamic DNS providers. We label our evaluation (or testing) dataset by aggregating updated blacklist information for new malicious domain names and IPs from the same lists.

To compute the honeypot features (presented in Section 3.2.3), we need a malware analysis infrastructure capable of processing as many new malware samples as possible. Our honeypot infrastructure, similar to "Ether" [4], processes malware samples in a queue. Each sample is analyzed in a controlled environment for five minutes. This process was repeated over the last 15 days of July 2009, resulting in a set of successful DNS resolutions (domain names and IPs) that each malware looked up. We executed malware and collected DNS evidence during the same period we aggregated the passive DNS database. Our virtual machines are equipped with five popular commercial antivirus engines. If any engine identifies an executable as malicious, we capture all domain names and corresponding IP mappings used during execution. After excluding domain names from the top 500 most popular alexa.com zones, we assemble the main corpus of our "honeypot data." We automated the crawling and collection of blacklist information and honeypot execution.

We chose to label our data as transparently as possible, using public blacklisting information to label our training dataset before building our models and training the reputation function. We then validated the results using the same publicly available blacklist sources. It is safe to assume that private IP and DNS blacklists will contain more complete information with lower false positive rates than public blacklists. Using such private blacklists should significantly improve the accuracy of Notos' reputation function.

### 5. Results

In this section, we present the experimental results of our evaluation. We show that Notos can identify malicious domain names sooner than public blacklists, with a low false positive rate (FP%) of 0.38% and a high true positive rate (TP%) of 96.8%. As a first step, we computed vectors based on the statistical features (described in Section 3.2) from 250,000 unique RRs. This volume corresponds to the average volume of new, previously unseen RRs observed at two recursive DNS servers in a major ISP in one day, as noted in Section 4, Figure 7(b). These vectors were computed based on historic passive DNS information from the last two weeks of DNS traffic observed on the same two ISP recursive resolvers in Atlanta and San Jose.

#### 5.1 Accuracy of Network Profile Modeling

The accuracy of the Meta-Classification system (Figure 4(a)) in the network profile module is critical for the overall performance of Notos. In online mode, Notos receives unlabeled vectors that must be classified and correlated with existing knowledge. For example, if the classifier assigns the label "Akamai" to a new RR with high confidence, it implies that the RR is part of a network similar to Akamai, though not necessarily part of the actual Akamai CDN. We will see in the next section how conclusions can be drawn based on the proximity between labeled and unlabeled RRs within the same zone-based clusters. We also discuss the accuracy of the Meta-Classifier when modeling each different network profile class (described in Section 3.3.2).

Our Meta-Classifier consists of five different classifiers, one for each domain class. We chose a Meta-Classification system over a traditional single-classification approach because Meta-Classification systems typically perform better [11, 2]. The ROC curve in Figure 8 shows that the Meta-Classifier can accurately classify RRs for all different network profile classes.

The training dataset for the Meta-Classifier is composed of sets of 2,000 vectors from each of the five network profile classes. The evaluation dataset is composed of 10,000 vectors, 2,000 from each class. The classification results for the domains in the Akamai, CDN, dynamic DNS, and Popular classes showed that the supervised learning process in Notos is accurate, with the exception of a small number of false positives related to the Common class (3.8%). Manual analysis of these false positives revealed some confusion between the vectors produced by Dynamic DNS domain names and those produced by domain names in the Common class. However, this minor misclassification does not significantly affect the reputation function, as the zone profiles of the Common and Dynamic DNS domain names are significantly different, leading to their grouping in different zone-based clusters.

Despite the accuracy of the network profile modeling process, it cannot independently designate a domain as benign or malicious. The clustering steps assist Notos in grouping vectors based on both their network profiles and zone properties. In the following section, we show how the network and zone profile clustering modules can better associate similar vectors due to properties of their domain name structure.

#### 5.2 Network and Zone-Based Clustering Results

In the domain name clustering process (Section 3.3.3, Figure 4(b)), we used X-Means clustering in series, once for the network-based clustering and once for the zone-based clustering.