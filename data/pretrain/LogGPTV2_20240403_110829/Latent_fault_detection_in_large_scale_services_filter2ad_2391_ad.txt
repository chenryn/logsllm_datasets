(b) sign
(c) LOF
Fig. 4. ROC and P-R curves on the SE service. Highlighted points are for signiﬁcance level α = 0.01.
(a) Detection day
(b) Warning day
Fig. 5. Aberrant counters for suspicious VM machine (black) compared to
the counters of 14 other machines (gray).
Fig. 6. Detection performance on LG service. At least 20-25% of failures
have preceding latent faults. Highlighted points are for α = 0.01.
overloaded. The relevant counters for this machine are plotted
in Figure 5. The second machine for which a latent fault was
detected appears to have had no relevant warning, but our tests
did indicate that it had low memory usage, compared to other
machines performing the same role.
E. Estimating the Number of Latent Faults
Some failures do not have a period in which they live unde-
tected in the system. Examples include failures due to software
upgrades and failures due to network service interruption. We
conducted an experiment on the LG environment with the goal
of estimating the percentage of failures which do have a latent
period.
We selected 80 failure events at random and checked
whether our methods detect them 24 hours before they are
ﬁrst reported by the existing failure detection mechanism. As
a control, we also selected a random set of 73 machines known
to be healthy. For both sets we require that events come from
different machines, and from a range of times and dates.
For this experiment we deﬁne a failing machine to be a
machine that is reported to be failing but did not have any
failure report in the preceding 48 hours. We deﬁne a machine
to be healthy if it did not have any failure during the 60
day period of our investigation. Figure 6 shows the ROC
curves for this experiment. Failing machines where latent
faults are detected are true positives. Healthy machines ﬂagged
as suspicious are counted as false positives. Both sign and
Tukey manage to detect 20% − 25% of the failing machines
with very few false positives. Therefore, we conclude that at
least 20% − 25% of the failures are latent for a long period.
Assuming our estimation is accurate, the recall achieved in
Section IV-B is close to the maximum possible.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:20:08 UTC from IEEE Xplore.  Restrictions apply. 
 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1TPR (recall)FPRrand. guess1 day7 days14 daysat cutoff 0.01 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1PrecisionRecall 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1TPR (recall)FPRrand. guess1 day7 days14 daysat cutoff 0.01 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1PrecisionRecall 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1TPR (recall)FPRrand. guess1 day7 days14 daysat cutoff 0.01 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1PrecisionRecallCounter valueCounter valueTimeTimeCounter valueCounter valueTimeTime 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1TPR (recall)FPRRandom guessTukeysignLOFat cutoff 0.01AVERAGE NUMBER OF COUNTERS REMOVED. MANY COUNTERS REMAIN
AFTER AUTOMATED FILTERING.
TABLE IV
Counters
Event-driven
Slow
Constant
Varied means
Remaining
Total
LG
85
68
87
103
211
554
VM PR
100
39
19
12
52
29
30
57
313
106
216
541
SE
112
24
40
79
89
344
F. Comparison of Tests
The three tests proposed in this work are based on different
principles. Nevertheless, they tend to ﬂag the same machines.
For instance, more than 80% of the machines that were ﬂagged
by Tukey are also ﬂagged by the sign test. All tests achieve
a low false positive rate on all services, with the Tukey and
sign tests matching the very low user-speciﬁed rate parameter.
To better characterize the sensitivities of the different tests,
we evaluated them on artiﬁcially generated data to which
we injected three types of “faults”: counter location (offset),
counter scale, or both. The strength of the difference varies
across the failing machines, and we compare the sensitivity of
each test to different kinds of faults. The resulting curves are
shown in Figure 7. This experiment shows that the sign test is
very sensitive to changes in offsets. LOF has some sensitivity
to offset changes while the Tukey test has little sensitivity, if
any, to this kind of change. When scale is changed, LOF is
more sensitive in the range of low false positive rates but does
not do well later on. Tukey is more sensitive than sign to scale
changes.
G. Filtering Counters in Preprocessing
As described in Section II-D, the preprocessing stage re-
moves some counters. Table IV reports the average number of
counters removed in each service.
When removing counters violating the memoryless as-
sumption, we measure the mean variability of each counter
across all machines, leaving only counters with low variability.
Our choice of a low ﬁxed threshold value stems from our
conservative design choice to avoid false positives, even at
the price of removing potentially informative counters. Fig-
ure 8 justiﬁes this choice: the majority of counters that were
not ﬁltered have relatively low variability on most services,
whereas the higher variability range (2–10) typically contains
few counters. Beyond 10 counters are not usable: most of them
are effectively a unique constant value for this counter for each
machine. Thus, tuning is not needed in preprocessing.
To further explore the effect of different thresholds, we
measured the performance of the tests on a single day of the
LG service with different mean variability thresholds. With
strict signiﬁcance level, higher thresholds result in slightly
better recall but slightly lower precision, conﬁrming our ex-
pectations.
Fig. 8. Histogram of counter mean variability for all services. The majority
of counters have variability below 2.
V. RELATED WORK
The problem of automatic machine failure detection was
studied by several researchers in recent years, and proposed
techniques have so far been mostly supervised, or reliant on
textual console logs.
Chen et al. [5] analyze the correlation between sets of
measurements and track them over time. This approach re-
quires domain knowledge for choosing counters, and training
to model baseline correlations. Chen et al. [8] presented a
supervised approach based on learning decision trees. The
system requires labeled examples of failures and domain
knowledge. Moreover, supervised approaches are less adaptive
to workload variations and to platform changes. Pelleg et
al. [22] explore failure detection in virtual machines using
decision trees. Though the basis is domain independent, the
system is supervised, requiring training on labeled examples
and manually selected counters. Bronevetsky et al. [23] mon-
itor state transitions in MPI applications, and observe timing
and probabilities of state transitions to build a statistical model.
Their method requires no domain knowledge, but is limited
to MPI-based applications and requires potentially intrusive
monitoring. It also requires training on sample runs of the
monitored application to achieve high accuracy. Sahoo et
al. [7] compare three approaches to failure event prediction:
rule-based, Bayesian network, and time series analysis. They
successfully apply their methods to a 350-node cluster for
a period of one year. Their methods are supervised and
furthermore rely on substantial knowledge of the monitored
system. Bod´ık et al. [4] produce ﬁngerprints from aggregate
counters that describe the state of the entire datacenter, and
use these ﬁngerprints to identify system crises. As with other
supervised techniques, the approach requires labeled examples.
The authors present quick detection of system failures that
have already occured, whereas we focus on detection of latent
faults ahead of machine failures. Cohen et al. [9] induce
a tree-augmented Bayesian network classiﬁer. Although this
approach does not require domain knowledge other than a
labeled training set,
the classiﬁer is sensitive to changing
workloads. Ensembles of models are used in [24] to reduce
the sensitivity of the former approach to workload changes,
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:20:08 UTC from IEEE Xplore.  Restrictions apply. 
LGVMPR0 - 22 - 33 - 44 - 55 - 66 - 1010 - infVariability rangeSECounters in each service(a) Location
(b) Scale
(c) Location+scale
Fig. 7. Performance on types of synthetic latent faults
at the cost of decreased accuracy when there are too many
failure types ([25]).
Palatin et al. [1] propose sending benchmarks to servers
in order to ﬁnd execution outliers. Like our method, their
approach is based on outlier detection, is unsupervised, and
requires no domain knowledge. However, through our interac-
tion with system architects we have learned that they consider
this approach intrusive, because it requires sending jobs to
be run on the monitored hosts, thus essentially modifying the
running service. Kasick et al. [13] analyze selected counters
using unsupervised histogram and threshold-based techniques.
Their assumptions of homogenous platforms and workloads
are also similar to ours. However they consider distributed ﬁle
systems exclusively, relying on expert insight and carefully
selected counters. Our technique requires no knowledge and
works for all domains.
There are several unsupervised textual console log analysis
methods. Oliner et al. [10] present Nodeinfo: an unsuper-
vised method that detects anomalies in system messages by
assuming, as we do, that similar machines produce similar
logs. Xu et al. [12] analyze source code to parse console log
messages and use principal component analysis to identify
unusual message patterns. Lou et al. [11] represent code ﬂow
by identifying linear relationships in counts of console log
messages. Unlike [10], [11], our method has strong statistical
basis that can guarantee performance, and it requires no tuning.
All three techniques focus on the unusual occurrences of tex-
tual messages, while our method focuses on numerical values
of periodic events. Furthermore, we focus on early detection
of latent faults in either hardware or software. Finally, console
logs analysis is infeasible in large-scale services with high
transaction volume.
VI. CONCLUSIONS
While current approaches focus on the identiﬁcation of
failures that have already occurred,
latent faults manifest
themselves as aberrations in some of the machines’ counters,
aberrations that will eventually lead to actual failure. Although
our experiments show that latent faults are common even in
well-managed datacenters, we are, as far as we know, the ﬁrst
to address this issue.
We introduce a novel framework for detecting latent faults
that is agile enough to be used across different systems and to
withstand changes over time. We proved guarantees on the
false detection rates and evaluated our methods on several
types of production services. Our methods were able to detect
many latent faults days and even weeks ahead of rule-based
watchdogs. We have shown that our approach is versatile; the
same tests were able to detect faults in different environments
without having to retrain or retune them. Our tests handle
workload variations and service updates naturally and without
intervention. Even services built on virtual machines are
monitored successfully without any modiﬁcation. The scalable
nature of our methods allows infrastructure administrators to
add as many counters of service-sensitive events as they wish
to. Everything else in the monitoring process will be taken
care of automatically with no need for further tuning.
In a larger context, the open question addressed in this paper
is whether large infrastructures should be prepared to recover
from “unavoidable failures,” as is commonly suggested. Even
when advanced recovery mechanisms exist, they are often not
tested due to the risk involved in testing live environments.
Indeed, advanced recovery (beyond basic failover) testing
of large-scale systems is extremely complicated and failure
prone, and rarely covers all faulty scenarios. Consequently,
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:20:08 UTC from IEEE Xplore.  Restrictions apply. 
 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1TPR (recall)FPRrand. guessTukeysignLOF 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1PrecisionRecall 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1TPR (recall)FPRrand. guessTukeysignLOF 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1PrecisionRecall 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1TPR (recall)FPRrand. guessTukeysignLOF 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1PrecisionRecall[22] D. Pelleg, M. Ben-Yehuda, R. Harper, L. Spainhower, and T. Adeshiyan,
“Vigilant: out-of-band detection of failures in virtual machines,” SIGOPS
Oper. Syst. Rev., 2008.
[23] G. Bronevetsky, I. Laguna, S. Bagchi, B. R. de Supinski, D. H. Ahn,
and M. Schulz, “Statistical fault detection for parallel applications with
AutomaDeD,” in Proc. SELSE, 2010.
[24] S. Zhang, I. Cohen, M. Goldszmidt, J. Symons, and A. Fox, “Ensembles
of models for automated diagnosis of system performance problems,”
in Proc. DSN, 2005.
[25] C. Huang, I. Cohen, J. Symons, and T. Abdelzaher, “Achieving scalable
automated diagnosis of distributed systems performance problems,” HP
Labs, Tech. Rep., 2007.
some outages of Amazon EC2 1, Google’s search engine, and
Facebook, and even the Northeast power blackout of 2003,
were attributed to the cascading recovery processes, which
were interfering with each other during the handling of a local
event. It is conceivable that there exist large systems whose
recovery processes have never been tested properly.
Like [3], we propose an alternative: proactively treating
latent faults could substantially reduce the need for recovery
processes. We therefore view this work as a step towards more
sensitive monitoring machinery, which will lead to reliable
large-scale services.
ACKNOWLEDGMENTS
This work was conducted while Moshe Gabel and Assaf
Schuster were visiting Microsoft Research. The authors wish
to thank the EU LIFT project, supported by the EU FP7
program.
REFERENCES
[1] N. Palatin, A. Leizarowitz, A. Schuster, and R. Wolff, “Mining for
misconﬁgured machines in grid systems,” in Proc. SIGKDD, 2006.
[2] E. B. Nightingale, J. R. Douceur, and V. Orgovan, “Cycles, cells
and platters: An empirical analysis of hardware failures on a million
consumer pcs,” in Proc. EuroSys, 2011.
[3] A. B. Nagarajan and F. Mueller, “Proactive fault tolerance for HPC with
xen virtualization,” in Proc. ICS, 2007.
[4] P. Bod´ık, M. Goldszmidt, A. Fox, D. B. Woodard, and H. Andersen,
“Fingerprinting the datacenter: Automated classiﬁcation of performance
crises,” in Proc. EuroSys, 2010.
[5] H. Chen, G. Jiang, and K. Yoshihira, “Failure detection in large-scale
internet services by principal subspace mapping,” IEEE Trans. Knowl.
Data Eng., 2007.
[6] M. Isard, “Autopilot: automatic data center management,” SIGOPS Oper.
Syst. Rev., 2007.
[7] R. K. Sahoo, A. J. Oliner, I. Rish, M. Gupta, J. E. Moreira, S. Ma, R. Vi-
lalta, and A. Sivasubramaniam, “Critical event prediction for proactive
management in large-scale computer clusters,” in Proc. SIGKDD, 2003.
[8] M. Chen, A. X. Zheng, J. Lloyd, M. I. Jordan, and E. Brewer, “Failure
diagnosis using decision trees,” in Proc. ICAC, 2004.
[9] I. Cohen, M. Goldszmidt, T. Kelly, and J. Symons, “Correlating in-
strumentation data to system states: A building block for automated
diagnosis and control,” in Proc. OSDI, 2004.
[10] A. J. Oliner, A. Aiken, and J. Stearley, “Alert detection in system logs,”
in Proc. ICDM, 2008.
[11] J.-G. Lou, Q. Fu, S. Yang, Y. Xu, and J. Li, “Mining invariants from
console logs for system problem detection,” in Proc. USENIXATC, 2010.
[12] W. Xu, L. Huang, A. Fox, D. Patterson, and M. I. Jordan, “Detecting
large-scale system problems by mining console logs,” in Proc. SOSP,
2009.
[13] M. P. Kasick, J. Tan, R. Gandhi, and P. Narasimhan, “Black-box problem
diagnosis in parallel ﬁle systems,” in Proc. FAST, 2010.
[14] C. McDiarmid, “On the method of bounded differences,” Surveys in
Combinatorics, 1989.
[15] W. J. Dixon and A. M. Mood, “The statistical sign test,” Journal of the
American Statistical Association, 1946.
[16] R. H. Randles, “A distribution-free multivariate sign test based on
interdirections,” Journal of the American Statistical Association, 1989.
[17] J. Tukey, “Mathematics and picturing data,” in Proc. ICM, 1975.
[18] T. M. Chan, “An optimal randomized algorithm for maximum Tukey
depth,” in Proc. SODA, 2004.
[19] J. A. Cuesta-Albertos and A. Nieto-Reyes, “The random Tukey depth,”
Journal of Computational Statistics & Data Analysis, 2008.
[20] M. M. Breunig, H.-P. Kriegel, R. T. Ng, and J. Sander, “LOF: Identifying
density-based local outliers,” SIGMOD Rec., 2000.
[21] C. D. Manning, P. Raghavan, and H. Sch¨uze, An Introduction to
Information Retrieval. Cambridge University Press, 2008.
1http://aws.amazon.com/message/65648/
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:20:08 UTC from IEEE Xplore.  Restrictions apply.