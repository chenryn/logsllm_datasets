In addition, we observe that the total sum over all cluster sizes
is less than the number of distinct tags that we have analyzed.
This is the case because any remaining tags are still considered
to be outliers and do not constitute a trend yet. As a matter
of fact, both the close resemblance to a power law function
and a non-negligible amount of outliers are expected, because
some changes are only made to a limited number of web sites,
e.g., very similar articles might be posted to less web sites than
we require as a lower-bound to constitute a trend, and also
because our view of changes is limited by the seed and link
expansion of the web crawler, i.e., it is possible that we only
observed a subset of the true instances of each unique trend.
We feel that it is important to understand what a single
cluster is actually describing, and we provide diﬀerent examples
about what tags have been clustered together. Therefore, we
investigate two clusters in more detail. Although both clusters
are low-count clusters, i.e., relatively small, their small size
actually illustrates that the ∆-system does detect when a trend
reaches a signiﬁcant distribution and that it does not rely on
an unreasonable large number of observations of a single trend.
The ﬁrst example we discuss is an actual infection campaign
that we have observed in the wild, an instance of a redirection
to a Cool Exploit Kit installation. In contrast, the second
example we discuss corresponds to a cluster describing the
change in cross-site request forgery tokens.
We selected these two clusters manually by ﬁltering clus-
ters based on the generated signature with simple heuristics
that suggest malicious behavior, such as external scripts that
are included with a random component or JavaScript with
a non-negligible ratio of digits over characters (suggesting
obfuscation). Clearly, these and other heuristics can also be
leveraged to order clusters according to “levels of interest” or
to remove clusters that are likely uninteresting and should not
be analyzed manually by an analyst.
Other trends we observed, but will not discuss in detail,
include the modiﬁcation of Facebook Like buttons (the back-
link URL changes), a version update for the JQuery library
served for blogs hosted on Wordpress.com, or the insertion of
user-tracking tokens.
5.3.1 Cool Exploit Kit Infections of Discuz!X
One of the most interesting clusters, which shows the applica-
bility of the ∆-system in practice, describes an infection vector
used to redirect to a speciﬁc infection campaign that uses the
Cool Exploit Kit to distribute malware. This in-the-wild infec-
tion campaign was found at the beginning of April 2013 in a
set of 15 diﬀerent web sites from the following 10 unique URLs:
• http://att.kafan.cn
15Minutes30Minutes1Hour2Hours6Hours12Hours1Day3Days1Week1Month3Months6MonthsTimeDiﬀerence0.00.10.20.30.40.50.60.70.80.91.0RatioofTotalPairsDiﬀerenceinTimeCDF102030405060708090100ClusterSize0500100015002000NumberofClustersDistributionofClusterSizesObservedClustersPowerLaw:y=2014·(x−10)−1.8116• http://frozen-fs.net
• http://jses40813.ibbt.tw
• http://ppin888.com
• http://www.dv3.com.cn
• http://www.kxxwg.com
• http://www.ruadapalma.com
• http://www.sdchina.cn
• http://www.wlanwiﬁ.net
• http://www.yysyuan.com
Once we veriﬁed that the cluster was indeed malicious, we
investigated the underlying commonalities between them. We
found that all web sites were using the discussion platform
“Discuz!X” [32]. Discuz!X is an Internet forum software written
in PHP and, according to the Chinese National Radio [33], the
most popular Internet forum software used in China. Clearly,
these infections are part of the same infection campaign. Ad-
ditionally, such a strong common ground suggests that the
infection is likely to be rooted in a security vulnerability in
the Discuz!X software, and it provides support identifying the
cause and a removal method.
Listing 3 shows the respective generated signature of the
infection. For this infection campaign, we did not observe any
diﬀerences in the tags that were clustered together.
1 
Listing 3: Cool Exploit Kit infection vector.
Beyond the inclusions of infection vectors pointing to an
installation of the Cool Exploit Kit observed in all pairs, one
web site (http://frozen-fs.net) also included an infection vector
that tried to infect visitors via an installation of the Blackhole
exploit kit.
The domain that included the Cool Exploit Kit and the
Blackhole exploit kit, “frozen-fs.net”, was not cleaned up, and
we observed that it was suspended by the provider 27 days
after we detected the infection.
5.3.2 Cross-Site Request Forgery Tokens
A second interesting low-count cluster we found during our
evaluation models variations in cross-site request forgery to-
kens in deployments of the Django web application framework.
In total, we identiﬁed a similar modiﬁcation among 17 diﬀerent
pairs of web sites. Each web site used form-based cross-site
request forgery tokens and used the same identiﬁer for a hid-
den form ﬁeld, namely “csrfmiddlewaretoken”. For every pair,
the attribute features did not diverge for the name attribute,
while all were diﬀerent for the value attribute. Nonetheless,
the ∆-system clustered them together, since the random en-
tropy was nearly constant for the value attribute among all
observed removed and inserted instances. The entropy was
nearly constant for the normalized case as well as for the
absolute entropy features. The exact identifying signature for
that cluster is shown in Listing 4.
name = " c s r f m i d d l e w a r e t o k e n "
value = " ( J h D 3 I w C X c n n p R t v E 4 2 M N 6 r 8 d O B O W R o x G
| h H 4 f 6 e O M C O T E Y F 0 R Y o X F R D a T L z y m 6 1 O 2
[...]
| D N c z o W j e N 1 n K 6 n q 3 w h X Y p S S n Z G d x x 0 O g
| F 9 y L S 0 j N U X I U R s X D R q x S 5 N V W 7 q X f W s g f ) " / >
1 < input
2
3
4
5
6
7
Listing 4: Cross-site request forgery token; | denotes an or.
We feel that this observed trend constitutes a perfect exam-
ple in which the limitations of the signature generation stick
out and where the ∆-system shows its robustness by cluster-
ing these changes correctly together. While the signature can
detect all observed instances correctly, it is clear that when
trying to match new versions of a web site with the signature
we would fail to identify the changed token value correctly
since the value will change to a new, unobserved random value.
5.4 Performance
In order to judge the actual applicability of our system in
practice, a performance analysis is necessary. We show in
Figure 5 that the performance of the ∆-system allows for
deployments in real-world scenarios. However, corner cases
exist that could impact an actual deployment, if the diﬀerence
between the base and current version of a web site is partic-
ularly large. We performed a manual in-depth analysis of the
system that highlighted the actual performance bottleneck of
our system: close to 80% of the time when analyzing the two
versions was spent by the Python library BeautifulSoup to
parse the HTML structure of a page. Although, the number
of changes made to a web site plays the most important role in
analyzing the changes, pairs that took longer than 3 seconds
to analyze were exclusively web sites that sent data in an
encoding diﬀerent than speciﬁed. BeautifulSoup tries to take
care of this and follows a code path that can get multiple
thousand function calls deep, and easily hits the recursion
limit of CPython. While we increased this limit in our eval-
uation to keep these pairs and prevent a dataset bias, the
particular functions are actually tail-recursive and, therefore,
can be expressed iteratively (thus, removing the necessity of
allocating stackframes). However, the abstruseness of the
involved functions prevented us from doing the very same in a
reasonable amount of time. Regardless of these (still outstand-
ing) engineering challenges for a general deployment, we could
analyze a single pair in a median time of 0.340 seconds and
in an average time of in 2.232 seconds. It is also evident that
we ﬁnished each analysis in at most 20 seconds, regardless of
the aforementioned problems encountered in BeautifulSoup.
These results, when taking into account that 60% of our
data is 7 days or more apart (c.f. Figure 5), support our claim
that the ∆-system does not necessarily need to keep a base
version locally, but could rely on public archives like the Inter-
net Archive or a web cache by a search engine. Nonetheless,
we strongly recommend keeping a local version to prevent
an additional delay in fetching the web site and to prevent
running into the problem of a potentially outdated or even
non-existing version on the side of the public archive.
6 LIMITATIONS
Similar to other static analysis approaches leveraging ma-
chine learning, our approach has some limitations, which can
be used to evade detection. This section discusses these limita-
tions and how they could be managed in a real-world deploy-
ment of the ∆-system. First, we introduce a limitation called
117While retraining the machine learning algorithm on a more
recent dataset is often a possible approach to counter the eva-
sion problem, it is only a near-sighted solution to counter the
dataset shift, as malware will deviate more severely, up to the
point where the features will not model the underlying problem
anymore. Even in cases where the features are not publicly
known to an adversary, it is possible to partially derive these by
probing the system carefully, which then will either allow for
successful evasion of the system or (on re-training) increase the
false positive and false negative rate because of misclassiﬁcation
due to minuscule diﬀerences between legitimate and malicious
code in the feature space. Both cases are obviously not desired
for a detection system, however, it is a general problem of all
approaches employing machine learning [10,11,13,14,31,37–39],
and it is generally only countered reliably by adapting to a
new feature space, which we leave for future work.
6.3 Dynamic vs. Static Analysis
The ∆-system in its current form is a purely static analysis
system, while, at the same time, the Internet is becoming
more and more dynamic. While one might think that static
analysis is inferior to dynamic analysis here, this is not the
case. Instead, our system complements dynamic analysis sys-
tems: it detects trends/infections statically and can forward
the interesting trends/infections to dynamic analysis systems
that extract further information.
Our motivation to rely on a purely static analysis is based
on multiple reasons. First, dynamic analysis is not necessarily
useful at the early stage in which our system operates, i.e.,
trends that change the behavior and are interesting to us
show themselves ﬁrst with static content changes, rendering
dynamic analysis (currently) unnecessary. A second argument
against dynamic analysis for the ∆-system is that it, for in-
stance by instrumenting embedded or included JavaScript to
modify the DOM tree to retrieve a “ﬁnal” version of the web
site, under-approximates the behavior of the web site to this
speciﬁc execution environment and might yield a potentially
incomplete or untrue representation of the DOM tree. It also
poses the questions of when to consider the DOM tree “ﬁnal”,
i.e., when to take a snapshot. Consequently, it might then be
possible to evade the trend detection step in the ﬁrst place.
Additionally, we might also miss infection campaigns that are
statically present in the web site, but are removed dynamically
or are inactive (for us). For example, servers could be unavail-
able (for us) or code might not be loaded (for us), we could be
ﬁngerprinted, the IP address of our analysis system might be in
a region of the world that is not aﬀected or simply because the
user-agent of our browser does not match a (unknown) regular
expression. The third argument in favor of static analysis is
that it can be considerably faster than dynamic analysis, which,
in turn, allows us to leverage more computationally-expensive
features to increase trend detection accuracy.
Lastly, while the trend detection step is purely static, to
detect malicious behavior, the ∆-system relies on an external
analysis system that might very well use dynamic analysis.
Generally, we do not impose any limitations on this detection
engine but that it can detect malicious behavior.
7 RELATED WORK
In the following we discuss related work in areas tangent
to our research, such as web dynamics and the detection of
malicious code. To the best of our knowledge, no prior work
exists that actively searches and ﬁnds previously unknown
infection campaigns.
Figure 5: Overview of the ratio of web site pairs that have been
completely analyzed in our experiments in less than x seconds.
Step-by-step Injection. Second, we will brieﬂy discuss the Evo-
lution of Infection Vectors as a major fundamental problem
in detecting malicious code. Lastly, we discuss the trade-oﬀ
between dynamic and static analysis and the limitations of
either approach.
6.1 Step-by-step Injection
It is possible to circumvent the ∆-system by adding mali-
cious code in small steps, i.e., in a series of modiﬁcations where
each step on its own gets detected as being benign, while the
aggregation is malicious. For example, an attacker could build
an infection vector that delivers these small steps depending
on a cookie or the visitor’s IP address to keep track of the
client’s previous version. However, we argue that the scope
of such an attack is heavily limited because: (a) it requires
an attacker to be able to inject code that is executed on the
server-side, otherwise detection is possible because it has to be
done client-side, and the attack can also be impeded or even
avoided by keeping the ﬁrst version instead of updating the
stored base version with every visit; or (b) the DOM tree will
be modiﬁed online, for example through JavaScript. In the
ﬁrst case, an important and drastically scope-reducing factor
is that the vulnerability needs to support such an iterative
process, where, e.g., memory regions are shared among browser
tabs or between multiple visits to the same web site, which
is highly unlikely given the strict separation current browser
sandboxes enforce. In the latter case, on the other hand, we
suggest to analyze the web site on every mutation event4, i.e.,
by considering the web site that was modiﬁed online as a new
current version and comparing it to the stored base version.
6.2 Evolution of Infection Vectors
Detecting malicious code is an arms race and malicious web
sites are no exception. Malware developers are trying to evade
detection systems to gain the upper hand, while detection