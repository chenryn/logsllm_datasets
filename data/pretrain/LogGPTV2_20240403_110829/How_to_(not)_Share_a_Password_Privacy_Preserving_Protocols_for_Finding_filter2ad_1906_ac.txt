leaked to the server. Namely, after hashing its password to vj, the device decides with probability
j, rj(cid:105) instead of (cid:104)vj, rj(cid:105). It holds
r to choose a uniformly distributed random value v(cid:48)
that:
j, rj(cid:105) = (cid:104)vj, rj(cid:105) | v(cid:48)
j, and send (cid:104)v(cid:48)
j (cid:54)= vj) = 1/2
Pr((cid:104)v(cid:48)
Pr(v(cid:48)
j = vj) = 2−(cid:96)
11
(2)
(1)
From equations (1) and (2) we get that the probability of the server learning the correct bit (cid:104)vj, rj(cid:105)
is pc = 1 − r(1 − 2−(cid:96))/2. From this we can conclude that this procedure provides the device with
pure DP, with  ≈ ln(2/r − 1).
Protecting Privacy from other users: (n, δ) Diﬀerential Privacy by Applying Laplacian
Noise To ensure users that they have n diﬀerential privacy from any coalition of users, the server
can add independent Laplacian noise Lap(1/n) to each table entry before deciding which hash
values to blacklist (adding a device’s password can change the value of each entry by at most 1).
We comment that this procedure might not be suﬃcient. The device can aﬀect the publication
probability of several passwords in the list. Moreover, the server needs to periodically republish its
current hash value list. As we need to generate new noise each time we publish, an attacker can
average the results over time and learn information about a single user’s password. Even given this
observation, we still retain DP as long as the number of hash value list publications is not very
large. Dwork et al. [DRV10] have shown the advanced composition property that enables to get
O((cid:112)k log(1/δ(cid:48)) · n, δ(cid:48))-diﬀerential privacy for k publications. This means that the penalty we get
in privacy is proportional to the square root of the number of publications, see Section 4.3.
3.4 The Malicious Setting
In a semi-honest model, the naive protocol suggested above is suﬃcient: the server sends rj to the
device and receives (cid:104)vj, rj(cid:105) (perhaps after the user applies the randomized response technique).
The server cannot learn more than a single bit, and the device does not have anything to learn.
However, in the malicious setting, the parties have diﬀerent possible behaviors:
A Malicious Server The user/server protocol must ensure that the server does not learn more
than a single bit about the password. This protects each user individually. However, on a system-
wide scale, a malicious server can tweak the hash value list that is published. One option is to
publish a very large list of popular passwords and cause a large number of false positives. This can
be done to cause a DoS attack or to try and inﬂuence the users to choose from a smaller set of
popular passwords. However, if τ is the popularity threshold, then the server can publish at most
O(1/τ ) popular passwords. Moreover, the server has no incentive to do a DoS attack on its own
system.
Another option is to create a targeted malicious blacklist for each device. This attack can be
prevented by publicly publishing the blacklist to all users.
A Malicious Device In the setting that we consider, it is very likely that some devices were
compromised (perhaps, by utilizing their weak passwords) and are now controlled by an adversary.
A coalition of malicious devices may try to inﬂuence the statistics gathered by the server in two
ways. It can apply an overcount attack to make unpopular passwords seem more popular or apply
an undercount attack to make popular passwords seem unpopular. Note that in our scheme, a single
device can change the value of a counter by at most ±1.
Undercount attack: This might be the most severe form of attack: A coalition of devices can try to
“hide” some popular passwords, and cause “false negatives” to be able to continue exploiting these
passwords. This attack may result in a larger fraction of the users using the same (weak) password
and being susceptible to attacks. Assume that a corrupt device wants to cause an undercount of a
12
popular password pass. Lets assume that vp = H(pass). The expected contribution to the counter
T [vp] by a device that did not choose vp is 0. However, by choosing v s.t. (cid:104)v, rj(cid:105) = −(cid:104)vp, rj(cid:105) the
contribution is always −1. In this way, a β fraction of malicious users out of NC can undercount
a popular passwords counter by βNC. This can cause the counter value to go below the threshold,
and remove the hash value from the published list.
Overcount attack: A coalition of devices can try to make many passwords that have been chosen
by less than a fraction τ of the users to seem more popular. This will result in a large number
of “false positives”. This attack is signiﬁcant only if the attacker can apply it on a large fraction
of the passwords, and reduce the total min-entropy. However, this attack is feasible only on a
small number of passwords simultaneously and will have a negligible eﬀect on the min-entropy.
Moreover, the solution we present for the undercount attack is also eﬀective against the overcount
attack. Therefore we will focus only on the more severe undercount attack.
A Malicious Third party A third party might try to comprise the protocol or the published
blacklist using a Man in the Middle attack on the connection. This is not possible, as we assume
an encrypted and authenticated channel between the server and device. Moreover, the server will
sign the publicly published blacklist including a timestamp (e.g. using HTTPS-protected website).
3.5 The Required Secure Functionality
To protect against malicious undercount attacks we need to prevent the possibility of sending an
output bit that is anti-correlated to any value v (namely 1−(cid:104)v, rs,j(cid:105)) with probability greater than
1/2. We want to only allow the device to choose some value x and send (cid:104)x, rs,j(cid:105). In Functionality 2
we deﬁne a functionality in the ideal model (where a trusted party is assumed to exist) which
provides the desired privacy and correctness properties. The actual execution of the protocol (with
no trusted party) must securely compute this functionality.
Functionality 2 (Ideal inner-product)
· Input:
· Output:
• The server sends to the TTP an (cid:96)-bit input rs,j.
• The device sends to the TTP an (cid:96)-bit input v.
• The TTP sends to server the inner-product value (cid:104)v, rs,j(cid:105).
• The device learns no output.
The deﬁnition implies that a device cannot learn rs,j before providing its input. The device is
allowed to randomize its response, by choosing a random input v. As v is independent of rs,j, the
result of the inner-product is random.
In Sections 5 and 6 we show two secure protocols which implement Functionality 2 (or a small
variant). The protocols are secure against malicious devices.
4 Popular Hash List Correctness
We describe how to generate and publish the blacklist of popular hash values using the inner-
product results received from the devices. We prove the correctness by providing upper bounds on
the probability of false negative and positive.
13
The basic scheme is secure against semi-honest devices. On the other hand, a coalition of
malicious devices can force an undercount to a speciﬁc popular hash value rather simply and break
the correctness (see Section 3.4). However, due to our secure functionality, a malicious device with
no knowledge about the secret rs vector, has no better strategy to undercount a hash value v than
(cid:54)= v. This reduces the case to the semi-honest setting. We will
choosing a random hash value v(cid:48)
prove the correctness of the scheme under this assumption. We also show how to calculate the
minimum possible threshold τmin preserving correctness as a function of the number of devices.
We also analyze correctness under the much stronger “Malicious Campaign” setting. In this
setting, a malicious coalition of all of the devices tries to learn information about the secret rs
vectors (which are ﬁxed per device in this setting) over several years to mount an undercount
attack. This is done by adaptive runs of the protocol and learning information about the rs vectors
using leakage from the repeated publications of the blacklist. Using this information the coalition
tries to undercount the hash value chosen by the small fraction of honest devices that were added
before the last publication. Although this model might not seem realistic, it allows us to prove
correctness in an information theoretical worst case. This allows us to calculate an upper bound on
the slightly larger threshold value τ(cid:48)
min that is required to preserve the correctness requirement in
any setting.
4.1 Notation
We use the following notation in the analysis:
1. NC - the total number of devices participating in the protocol.
2. NH (v) - the total number of devices that are currently using any password such that v =
H(pass).
3. T [v] - the value stored at the counter table with index v.
4. α(v) - the server’s approximation of the number of votes for a hash value v.
5. r - the probability for randomizing a device’s response (for DP).
6. 1(x) - the unit step function.
All probability calculations are taken over the possible values of the rs vectors used by the server
and the devices’ possible randomized responses.
4.2 Estimation of Popular Hash Values
The server’s approximation to the number of devices that are currently using a password that
hashes to a value v, given the current value of T [v], is deﬁned as:
α(v) = (T [v] − NCr2−(cid:96))/(1 − r)
(3)
Lemma 1. If a fraction p of the devices choose a password which hashes to a value v, then the
expected value of α(v) is pNC.
The proof of this lemma appears in Appendix E.
14
4.3 False Negative and Positive Bounds
We want to identify hash values which are used by more than a fraction τ of the devices. Namely,
identify any hash value v for which NH (v) > τ NC. As we can only learn an approximation of NH (v),
we relax our requirement, to that of identifying any hash value for which NH (v) > τ NC(1 + δ).
We will also allow error on the other side – namely allow the server to blacklist hash values which
are only used by more than τ NC(1 − δ) devices (but no hash value which is used by fewer devices
should be declared as popular).
We assume the malicious setting using our secure functionality.
Estimating the false negative probability: We deﬁne as a “false negative” the event that at a given
time, a speciﬁc hash value v was over the upper threshold, but the approximation α(v) was below
the threshold. Namely,
NH (v) > τ NC(1 + δ) ∧ α(v)  τ NC
Lemma 3. The probability for a false positive pF P is bounded by:
pF P ≤ exp(−NC(τ δ(1 − r))2/2)
Lemmata 2 and 3 are proved in Appendix E.
Note that as was described in Section 3.1 there is a larger chance of false positive on the original
passwords due to collision in the hash values. This probability is calculated in Appendix E.
Dynamic Threshold τ As NC increases we can get a better approximation for the hash values
distribution (namely, can use smaller τ values). The server can dynamically change τ as a function
of NC and its chosen bound on pF N . Using Lemma 2 we propose that the minimal threshold τmin
will be bounded such that the following constraint holds for some constant C:
C 
·
1
δ(1 − r)
(cid:114) 2C
NC
This will assure a very low probability of false negatives even after publishing the hash list a
polynomial number of times. For example, in a system with ten million users (NC = 107), δ = 0.8,
r = 0.25 and C = 7, we get that τmin = 0.002. We can also use a numerical approach to calculate
pF N and ﬁnd a lower more accurate τmin.
15
Laplacian Noise We use Laplacian noise to get (DP , δ(cid:48)) diﬀerential privacy from third parties.
Before each publication, the server adds to each counter T [x] independent noise χL ∼ Lap(1/n),
and blacklists a hash value x only if T [x] + χx
Remember that the maximal number of possibly published counters in each publication is
1/(τ (1 − δ)). So we get that for k repeated publications of the blacklist we can get (DP , δ(cid:48))
diﬀerential privacy where DP =(cid:112)k/(τ (1 − δ)) ∗ ln 1/δ(cid:48) · n. Using a τ value slightly larger than
L > τ Nc.
τmin will allow us to add the required Laplacian noise while still keeping the value of pEphF N
negligible.
4.4 Bounds for a “Malicious Campaign”
In a malicious setting, a coalition of devices might run an undercount attack, trying to cause a
false negative wrt an actual popular password. The system, however, uses a secure inner-product
protocol that is resilient to malicious devices (see Sections 5 and 6). In a “one-shot” scenario, where
the attacker has no information about the secret rs vectors, we get the same probability pF N as in
the semi-honest case (best strategy for the coalition: choose random hashes).
The publication of the list of popular hash values leaks a small amount of information about the
secret rs values. This information might be used in an undercount attack to cause false negatives
with higher probability. For example, a device might learn from a change in the published hash list
that its chosen hash value is correlated with a popular hash value. Therefore the device chooses
another hash value in the hope that the result will be anti-correlated.
The server can add independent random noise to the counter table before each publication of
the blacklist. In this way, the server reduces the amount of leaked data. We conjecture that even
a small amount of added noise (e.g. the Laplacian noise used for DP) will render the leaked data
unusable for the attacker. However, we provide a full information-theoretical worst-case analysis.
We assume that for the entire life span of the system, a malicious coalition of all devices
found an optimal adaptive strategy to cause each publication to leak the maximum possible (in an
information-theoretic sense) number of bits about the secret rs vectors. Moreover, we assume that
this coalition has an optimal strategy to use this noisy information to improve their undercount
probability by the maximal theoretically possible factor. In Part B of the Appendix we prove the
scheme’s correctness under those assumptions.
4.5 Security and Correctness Analysis
We need to show that the desired privacy property expressed by the password game of Section 2.1
and 2.2 indeed holds, and show that the correctness property of Section 2.3 holds as well. The privacy
property follows from the randomized response giving us  diﬀerential privacy protection against
a malicious server, and from the (DP , δ(cid:48)) diﬀerential privacy protection given to each password
regarding any malicious coalition that does not include the server, as shown in Section 4.3.
As for correctness, in Section 4.3 we show how to bound pF N , and how to dynamically (i.e. as
the number of participants increases) choose the parameters to ensure this requirement. Figure 2
shows the τmin values for δ = 0.8, r = 0.25 and C = 7. Parts B.4 and B.5 of the supplementary
material show the same bounds in the worst case “Malicious Campaign”setting. A suitable choice
of slightly larger τ(cid:48) > τmin and n will give us the required pEphF N and pEphF P .
Regarding ptotal, we prove in Appendix E that the only event that should worry us is the
occurrence of M collisions to popular hash values under the hash function. This happens with a
very small probability of (2−(cid:96) / τ (1 − δ))M .
16
Fig. 2. τmin as a function of the number of users
5 Garbled Circuits Based Inner Product Protocol
Generic protocols for secure computation, such as Yao’s garbled circuit protocol, can be used for
securely computing any function. The Binary circuit computing the inner-product of (cid:96)-bit values
is very small, and is composed of only (cid:96) AND gates and (cid:96) − 1 exclusive-or gates. The secure
computation of this circuit is extremely eﬃcient.
The main challenge in using garbled circuits in our application is ensuring security against
malicious behavior. The most common approach for achieving security against malicious adversaries
is the cut-and-choose paradigm (see, e.g. [LP07]), but this paradigm requires computing at least
Ω(s) copies of the circuit to reduce the cheating probability to 2−s 4.
Fortunately, our setting enables a much more eﬃcient implementation. This is based on two
features existing in this setting: