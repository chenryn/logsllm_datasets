### Impact of GON Model on Decision Time and Fine-Tuning Overheads

Without the GON model, and using a traditional feed-forward model, the decision time is reduced, but at the cost of higher fine-tuning overheads. We observe that confidence-aware model fine-tuning and the use of the GON network have the most significant impact on performance, accounting for nearly 70% of the total impact.

### Sensitivity Analysis

Figure 6 provides a sensitivity analysis of the CAROL model's performance with respect to the learning rate (\(\gamma\)), memory consumption, and the size of the tabu list. This analysis highlights the trade-off between Quality of Service (QoS) scores and scheduling time as these parameters are varied.

- **Learning Rate (\(\gamma\))**: The relationship between performance metrics and the learning rate is relatively straightforward. As the learning rate increases, the scheduling time of CAROL decreases due to larger jumps in the optimization steps. However, for a high learning rate (\(\gamma \geq 10^{-2}\)), the model fails to converge to the optimal solution, leading to increased Mean Squared Error (MSE), energy consumption, and Service Level Objective (SLO) violation rates. The best QoS scores are achieved with a learning rate of \(\gamma = 10^{-3}\), which is used in our experiments.

- **Memory Consumption**: As the number of layers in the GON model increases, so does the memory footprint. This increase in memory consumption leads to longer scheduling times because it takes more time to generate samples by running optimization in the input space. However, a higher layer count improves prediction performance, resulting in lower MSE, energy consumption, and SLO violation rates. For memory consumption greater than 1GB, the improvements in energy consumption and SLO violation rates are marginal, but the scheduling time significantly increases. Therefore, we use a model with a 1GB memory footprint (3 layers) in our experiments.

- **Tabu List Size**: Increasing the size of the tabu list also increases the scheduling time but results in better energy and SLO scores. In our experiments, we use a tabu list size of 100.

### Conclusions and Future Work

We have developed a novel method for resilient computing in edge federations. Our method employs a lightweight generative network as a surrogate model to efficiently and accurately map performance metrics such as energy consumption and SLO violation rates for a given graph topology and scheduling decision. In the event of a broker failure, CAROL reactively optimizes the graph topology to accommodate orphaned worker nodes. The topology is chosen by running a tabu search to optimize the QoS scores predicted by the surrogate model. CAROL uses the discriminator output as a confidence score, allowing us to perform model fine-tuning only when confidence scores drop below running thresholds. This approach reduces overheads by 35.6% compared to the current state-of-the-art. Performance evaluations on a real edge test-bed with AI and IoT-based benchmark applications show that CAROL can improve energy consumption and SLO violation rates by 16.5% and 17.0%, respectively, compared to the state-of-the-art.

The current approach achieves optimal performance due to its lower fine-tuning overheads compared to prior work. Our methods are particularly well-suited for highly dynamic systems. For stationary settings, we propose extending the current reactive model to a proactive scheme that can prevent node failures. However, proactive optimization may require higher computation for improved predictive performance, which will be addressed in future work.

### Software Availability

The code and relevant result reproduction scripts are available at [https://github.com/imperial-qore/CAROL](https://github.com/imperial-qore/CAROL).

### References

[References listed here, formatted as per the original text]

This optimized version aims to provide a clearer, more coherent, and professional presentation of the information.