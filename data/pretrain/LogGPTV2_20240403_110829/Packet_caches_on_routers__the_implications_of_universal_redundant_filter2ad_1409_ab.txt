routes and all routers perform redundancy elimination. For instance,
if none of the copies of Pi is routed over e, then the footprint due
to Pi and its copies on edge e is zero, i.e., F Pi,e = 0. If multiple
copies of Pi are routed over the edge e, then effectively only one
copy goes through e because the remaining copies are eliminated as
being redundant. In this case, the footprint of the copies of Pi on the
edge e is a function of the size of the distinct packet Pi. In this pa-
per, we pick the footprint function to equal the size of the packet Pi
multiplied by the latency of the link e, or F Pi,e = late × |Pi|. The
intuition behind choosing this function is that a packet consumes
more network resources overall when it traverses high latency links
and/or when the packet size is large. Other functions reﬂecting net-
work usage can also be considered.
The ISP’s objective in computing redundancy-aware routes is to
compute the rte variables such that total footprint summed over all
network edges is minimized. In order words, the goal is to com-
pute routes from S which minimize the total network resources con-
sumed by trafﬁc originating at S within the interval T when all rou-
ters perform redundancy elimination.
We formulate the ISP’s objective as the output of a Linear Pro-
gram (LP). We ﬁrst describe the constraints for the LP, followed by
the optimization objective. We have the following constraints per
distinct packet Pi, based on the deﬁnition of the footprint function:
∀j, F Pi,e ≥ late × cpyi,j × rtej,e × |Pi|
Since the footprint F Pi,e cannot exceed resources consumed when
routing a single copy of Pi on e , we have, F Pi,e ≤ |Pi| × late.
P
backbones routers v, we have: ∀j,
e∈δ+(v) rtej,e =
rtej,e, where, δ+ indicates ﬂow entering node v, and δ−
ﬂow leaving node v. For source S and destinations Dj, we have:
Next, we set up ﬂow conservation constraints for nodes in V . For
e∈δ−(v)
indicates
P
∀j,
∀j,
P
e∈δ−(S) rtej,e − P
P
e∈δ+(Dj ) rtej,e − P
e∈δ+(S) rtej,e = 1
e∈δ−(Dj ) rtej,e = 1
Finally, we require a set of constraints to ensure that link capacities
are obeyed. Suppose edge e cannot carry more than Cape packets
within the interval T (Cape can be derived from e’s raw capacity).
n F Pn,e ≤ Cape. We use a normal-
Then, we require: ∀e,
izing factor
late to obtain the total size of packets carried by e.
The objective of the LP is to lower the total network footprint
P
late
1
1
P
P
subject to the above constraints, or Minimize
i F Pi,e.
e
We allow fractional values for the variables rte in the solution for
the above LP. Fractional values indicate how trafﬁc may split across
different possible paths between S and a destination.
3.2 Multiple Ingresses, Trafﬁc Engineering
We extend the above approach for computing redundancy-aware
routes to a network-wide setting. The goal is to use redundancy-
awareness to help ISPs meet their trafﬁc engineering goals more
effectively. Our network-wide approach tries to always obtain bet-
ter network-wide guarantees than existing TE approaches, such as
OSPF-based weight tuning [6]. We illustrate our approach using the
“Maximum load” objective, wherein the ISP employs trafﬁc engi-
neering to minimize the maximum link utilization in its network.
Trafﬁc can originate at any network PoP.
To explain our approach, we introduce a per-ingress parameter
cpyPn,i,Dj which is 1 if a copy of distinct packet Pn,i is destined
for Dj. Pn,i denotes the ith distinct packet originating from ingress
Sn within an interval T . Similarly we extend the link footprint vari-
able to capture the contribution of packets originating from different
ingresses to a particular link e; we denote this as F PPn,i,e. In a sim-
ilar fashion, we deﬁne variables rteSn,j,e which identify if the ﬂow
between Sn and Dj ﬂows through edge e. We assume that pack-
ets originating from different ingresses have no content in common.
(We omit several details for brevity.)
P
P
As with the single ingress case, we ﬁrst formulate a network-wide
P
LP where the objective of the ISP is to lower the network footprint
due to trafﬁc originating from all PoPs, or Minimize
n
F PPn,i,e. Next, we place link capacity constraints and incorporate
the “Max Load” objective as follows: Suppose that, based on the
measured network trafﬁc matrix, the ISP estimates that traditional
trafﬁc engineering approaches (e.g. OSPF-based approaches [6,
12]) can bound the link loads by a factor α  2 destinations.
We make another simpliﬁcation to improve the scalability. We
“combine” the redundant content in packets going to an identical
set of destinations into a larger aggregated packet; copies of the
aggregated packet are considered to be destined for the same set
of destinations as the individual packets. For example, suppose that
distinct packets P1, . . . , Pl all have two copies, with one copy going
to destination D1 and another to D2 (all trafﬁc is observed in a time
interval T ). We create an equivalent single aggregated packet of size
Pl
1 Pi which goes to destinations D1 and D2. Thus, the aggregated
packet captures the total overlap in the content going to D1 and D2.
This aggregation approach reduces the total number of cpy variables
without changing the quality of the solution obtained for the LP —
the number of variables reduces from 2l to 2 in the above example.
With these two simpliﬁcations, the total number of variables for
the entire network is now on the order of the square of number of
PoPs in the network and the control overhead is thus much smaller.
We refer to the redundancy proﬁles captured using aggregated pack-
ets in the above fashion as the aggregated redundancy proﬁles.
Next, we describe an approximate approach for computing the
aggregated redundancy proﬁles at the ingress routers of an ISP net-
work as packets stream into a network. We also address issues aris-
ing from content being partially replicated across network packets.
3.3.2 Computing Redundancy Proﬁles
We discuss an extension to the algorithm in Section 2.1 to com-
pute the aggregated proﬁles in practice. The approach we describe is
run constantly on each ingress router. Suppose an incoming packet
P at an ingress router has a match with a single packet Pcache stored
at the router, and that P and Pcache are headed for different des-
tinations D1 and D2. We count the size of the matching region
|P ∩ Pcache| towards the total amount of content common to desti-
nations D1 and D2. If P and Pcache are both headed to the same
destination, say D1, then we count |P|+|Pcache|−|P ∩Pcache| to-
wards content exchanged between the ingress and D1; in this man-
ner, we approximately track the total amount of unique content ex-
changed between the source and D1. If the incoming packet P has a
match with more than one cached packet, say P1,cache and P2,cache,
we count each match region separately towards the redundancy pro-
ﬁles; that is, we run the aforementioned tallying approach ﬁrst for P
and P1,cache, and then for P and P2,cache. We also track packets in
the ingress router’s packet store which observe no matches during
the interval T . We group such packets by their destination and com-
pute the total size of the packets in each group. This total is then
added to the total volume of unique content exchanged between the
ingress and the corresponding destination.
At the end of interval T , the ingress router gathers aggregated
counts for: (1) the size of content shared between pairs of egresses,
and (2) the volume of unique content exchanged with different egresses.
This forms the aggregated redundancy proﬁle for the ingress PoP,
and is transmitted to the route controller. Note that we only focus
on content that is duplicated across 2 destinations, if at all.
This approach clearly approximates the true redundancy proﬁle
as described in Section 3.1. However, our trace-based evaluation
(Section 6) shows that the inaccuracies in our approach do not sig-
niﬁcantly affect the quality of the routes we compute.
3.3.3 MPLS Networks
As mentioned before, we permit fractional solutions to the network-
wide Linear Program. The fractional solution can be implemented
in MPLS-based networks by establishing the appropriate “trafﬁc
trunks”, or label switched paths (LSPs), between ISP PoPs [9]. Care
must be taken to construct LSPs and allot packets to them in a
redundancy-aware manner. This is crucial in order to extract the
maximum amount of redundant content from network trafﬁc. Oth-
erwise, packets may be alloted to LSPs in such a manner that redun-
dant packets destined for different egresses are routed along LSPs
which have very few network links in common.
While a thorough investigation of how to establish LSPs and al-
locate packets is beyond the scope of this work, we have developed
a preliminary redundancy-aware heuristic which seems to offer sat-
isfactory performance in practice. The details can be found in [5].
4.
INTER-DOMAIN ROUTING
In this section, we present redundancy-aware inter-domain rout-
ing which can help ISPs minimize the overall impact of inter-domain
trafﬁc on internal and peering links. We consider as “inter-domain”
trafﬁc the set of all packets traversing the ISP whose destinations are
routable only through peers of the ISP. We consider two approaches:
local and cooperative.
The local approach applies to an ISP selecting its next-hop ASes
in BGP, as well as the particular exit point(s) into the chosen next
hop.
In this approach, an ISP aggregates its inter-domain trafﬁc
over a selected set of next hops and the corresponding exit points so
as to aggregate potentially redundant trafﬁc onto a small number of
network links. Thus, the ISP can signiﬁcantly reduce the impact that
inter-domain trafﬁc imposes on its internal and peering links. To
compute routes in this manner, the ISP must track (1) the amount of
redundant content that is common to different destination preﬁxes
and (2) the route announcements from peers to the destinations.
The cooperative approach applies to ISPs which are willing to
coordinate their inter-domain route selection decisions. In this ap-
proach, the ISPs compute routes which minimize the overall impact
of inter-domain trafﬁc across the internal links of all ISPs involved
and the peering links between the ISPs. We explore the ideal bene-
ﬁts from cooperation and ignore important issues such as the need
to maintain privacy of internal information.
4.1 Local Approach for an ISP
The intra-domain approach, presented in Section 3, can be ex-
tended in a straight-forward manner to perform next hop-AS selec-
tion. This simply requires a change to the input network graph G
and the overall objective of the ISP. Our approach described below
focuses on inter-domain trafﬁc originating at a particular PoP in an
ISP and can be extended to all inter-domain trafﬁc of the ISP. We
present the high level ideas and omit the details for brevity.
The ISP’s network footprint objective encompasses the footprint
F Pi,e of both the internal edges of the ISP and its peering links.
The input graph G = (V, E) is constructed as follows: the set V
is composed of three subsets V1, V2, and V3 (Figure 3). V1 is the
set of all intra-domain routers or the PoPs of the ISP, including the
ingress PoP S where the inter-domain trafﬁc originates. V3 is the
set of destination preﬁxes D1, D2, . . . , Dm. These are the preﬁxes
to which the inter-domain trafﬁc from S must ﬁnally reach. We as-
sume that the ISP computes aggregated redundancy proﬁles across
the m destinations. To derive the ideal beneﬁts of redundancy elim-
ination, all possible destination ASes must be considered in the set
V3. However, in practice, it may sufﬁce to focus on just the top few
destination preﬁxes by volume. Finally, the set V2 is composed of
“intermediate nodes” which model possible next hop ASes for each
destination, as well as their peering locations with the ISP.
The set of edges, E, is composed of three subsets: E1, the set of
intra-domain edges, E2, the full set of peering edges between the
Figure 3: Input graph for the local inter-domain approach.
ISP in question and each of its peers, and E3, which are “interme-
diate edges” between nodes in V2 and V3. We construct an interme-
diate edge between an intermediate node v and a destination Dj if
the ISP corresponding to v has announced a route to Dj. We only
include edges and vertices for a peer if the peer is among those who
have a path with the smallest number of AS hops to the destination.
The rest of the inter-domain route selection approach is simi-
lar to the intra-domain case. Again, a centralized route controller
may be employed to compute routes which minimize the footprint
due to inter-domain trafﬁc. The ingress router at S could com-
pute the inter-domain redundancy proﬁle using the approach in Sec-
tion 3.3.2, and transfer the proﬁle to the router controller. The output
from the route controller is the next-hop AS and the internal route
to the exit point into the next-hop for each destination preﬁx.
4.2 Cooperative Approach for Two ISPs
For simplicity we consider the case where just two ISPs coordi-
nate their inter-domain route selection. Our approach can be ex-
tended to multiple ISPs. Our approach works as follows: rather
than compute inter-domain routes in isolation, each ISP tries to ag-
gregate the redundant content in inter-domain trafﬁc together with
the redundant content in its intra-domain trafﬁc, so as to bring down
the overall utilization of all participating networks.
Thus, the key difference from the intra-domain routing formula-
tion is that the input graph used by either ISP is the union of the
topologies of the two networks and peering links. The inputs to
an ISP’s Linear Program are its intra-domain redundancy proﬁles
and the inter-domain proﬁle for trafﬁc between ingresses in itself
and egresses in the neighbor. The output of an ISP’s formulation
include its intra-domain routes and a list of exit points for trafﬁc
destined to egresses in the neighbor (and how to split inter-domain
trafﬁc across the exit points).
5. MEASUREMENT RESULTS
We present a brief study of key properties of content redundancy
observed at the packet-level in real traces. We focus on the extent
to which content is duplicated across two or more destinations. Our
observations shed light on the potential for redundancy-aware rout-
ing. They also justify the key choices we have made in designing
the approaches outlined in Sections 3 and 4. We also leverage the
observations to construct synthetic traces which we use extensively
in Section 6. For brevity, we only focus on intra-domain settings.
Traces. We collected full packet traces at a large US university’s
access link to the commercial Internet. We collected multiple 150s-
snapshots at the beginning of every hour starting at 10am and ending
at 7pm on Jan 26, 2007. In addition, we also separately monitored
the trafﬁc originating from a high volume /24 preﬁx owned by the
university, which hosted some of the most popular servers on cam-
pus (during the same time-period). The latter traces are likely to be
representative of a moderate-sized data center.
Extent of redundancy. We used the approach outlined in Sec-
tion 2 to quantify packet-level content redundancy. In the case of
Internet-bound trafﬁc on the University access link, we found that
s
k
n
u
h
c
e
t
y
b
-
4
6
f
o
%
 1
 0.95
 0.9
 0.85
 0.8
 0.75
CDF (low redundancy)
CDF (High Redundancy)
 1  2  3  4  5  6  7  8  9  10
No. of Egresses
 1
 0.8
 0.6
 0.4
 0.2
s