Should pandas have a _Symbol_ datatype for strings which are frequently re-
used?
For example, consider a table of stock trades with columns
['Timestamp','Price','Size','StockSymbol']. Suppose there are 10 million
trades, but only 500 different stock symbols. Instead of storing 10 million
strings, we could fill the 'StockSymbol' column with integers 0-499 where 0
represents AAPL, 1 represents AMZN, etc.
The _Symbol_ type in Ruby, Scala, and Q does this compression/expansion
automatically. It's a big memory saver in big tables, and _Symbol_ comparison
can be much faster than string comparison.
I often use `.astype('category')` for string compression in pandas. But at a
talk I saw recently, @wesm suggested this might not be a great idea. I've
definitely caused several confusing problems by abusing categoricals this way:
comparing two columns with different levels, appending rows with new symbols,
wrongly assuming `sort_values()` will sort symbols lexicographically, etc.
Related question: Without a _Symbol_ datatype, is there a consensus on best
practice for storing repetitive strings in a Series or DataFrame? Should we
leave them as `Object` type? Compress them as un-ordered categoricals? Do
something else I haven't thought of?