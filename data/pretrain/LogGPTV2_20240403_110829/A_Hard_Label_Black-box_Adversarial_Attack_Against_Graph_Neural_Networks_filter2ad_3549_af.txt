see Figure 13 (c)). The results are similar to in [38]. We note that
the labels of graphs may be different from the true labels when
95% of their smallest singular values are removed even they are
Session 1B: Attacks and Robustness CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea119not perturbed. In our experiments, for ease of analysis, we assume
that these graphs have true labels as our goal is to evaluate the
impact of the low-rank based defense, i.e., measure if the attack SR
significantly decreases with the maintained clean accuracy.
6.3 Discussion
The defense results in Section 6.1.2 and Section 6.2.2 indicate that
our adversarial attack is still effective even under detection or pre-
vention. For example, on the NCI1 dataset, the best detection per-
formance is obtained when the budget is 0.20 with GUNet and our
attack to train the detector. However, the FNR and FPR are still 0.25
and 0.20, even if the detector has a full knowledge of our attack.
What’s worse, it is even harder to detect adversarial graphs in real-
world scenarios as the detector often does not know the true attack.
Low-rank based defense can prevent adversarial graphs to some
extent, while needing to scarify the testing performance on clean
graphs, e.g., as large as 40% of adversarial graphs on NCI1 cannot
be prevented even we remove 95% of the smallest singular values.
The proposed detector is a data-level defense strategy and it
attempts to block the detected adversarial graphs before they query
the target model. It has two key limitations: (i) it is heuristic and
(ii) it needs substantial number of adversarial graphs to train the
detector and the detection performance highly depends on the
quality of the training dataset, i.e., the structure difference between
adversarial graphs and normal graphs should be large. The proposed
low-rank based defense is a model-level defense strategy and it
equips the target GNN model with the smallest singular value
removal such that the GNN model can accurately predict testing
graphs even they are adversarially perturbed. There are several
possible ways to empirically strengthen our defense: (i) Locating
the vulnerable regions of graphs based on the feedback of our
attacks; (ii) Designing attack-aware graph partitioning algorithm
as the method used in our attack is generic and does not exploit the
setting of adversarial attacks. (iii) Adversarial training [8, 20, 23].
It aims at training a robust GNN model by introducing a white-box
adversarial attack and playing a min-max game when training the
model. We do not adopt this method because there does not exist
white-box attacks against the considered GNN models.
Another direction is to provide the certified robustness [4, 22, 47]
of GNN models against adversarial structural perturbations. We
will also leave those kind of defenses to defend our hard label black
box adversarial attacks as the future work.
7 RELATED WORK
Existing studies have shown that GNNs are vulnerable to adversar-
ial attacks [5, 6, 33, 46, 48, 61], which deceive a GNN to produce
wrong labels for specific target graphs (in graph classification tasks)
or target nodes (in node classification tasks). According to the stages
when these attacks occur, they can be classified into training-time
poisoning attacks [30, 46, 53, 63, 64] and testing time adversarial
attacks [7, 9, 27, 32, 43, 48]. In this paper, we focus on testing time
adversarial attacks against classification attacks.
Adversarial attacks against node classification. Existing adver-
sarial attacks mainly attack GNN models for node classification.
For node classification, the attacks can be divided into two cate-
gories, i.e., optimization based ones [32, 42, 44, 52] and heuristic
based ones that leverage greedy algorithms [9, 50] or reinforce-
ment learning (RL) [13, 41]. In order to develop an optimization
based method, the attacker formulates the attack as an optimiza-
tion problem and solves it via typical techniques such as gradient
descent. For example, Xu et al. [54] developed a CW-type loss as
the attacker’s objective function and utilized projected gradient
descent to minimize the loss. As for heuristic based methods, an at-
tacker can utilize a greedy based method, i.e., defining an objective
function and traversing all candidate components (e.g., an edge or
a node) for adding perturbations. The attacker can select the one
that maximizes the objective function to perturb. This process will
be repeated multiple times until the attacker finds an adversarial
graph or the perturbations exceed the pre-set budget. For instance,
Chen et al. [9] proposed an adversarial attack to GCN, which selects
the edge of the maximal absolute link gradient and adds it to graph
as the perturbation in each iteration.
Adversarial attacks against graph classification. Only a few
attacks aim to interfere with graph classification tasks [13, 33, 43].
For instance, Ma et al. [33] proposed a RL based adversarial attack to
GNN, which constructs the attack by perturbing the target graph via
rewiring. Tang et al. [43] performed the attack against Hierarchical
Graph Pooling (HGP) neural networks via a greedy based method.
Different from these attacks that are either white-box or grey-box,
we study the most challenging hard label and black-box attack
against graph classification in this paper. Our attack is both time
and query efficient and is also effective, i.e., high attack success rate
with small perturbations.
8 CONCLUSION
We propose a black-box adversarial attack to fool graph neural
networks for graph classification tasks in the hard label setting.
We formulate the adversarial attack as an optimization problem,
which is intractable to solve in its original form. We then relax
our attack problem and design a sign stochastic gradient descent
algorithm to solve it with convergence guarantee. We also propose
two algorithms, i.e., coarse-grained searching and query-efficient
gradient computation, to decrease the number of queries during the
attack. We conduct our attack against three representative GNN
models on real-world datasets from different fields. The experi-
mental results show that our attack is more effective and efficient,
when compared with the state-of-the-art attacks. Furthermore, we
propose two defense methods to defend against our attack: one
to detect adversarial graphs and the other to prevent adversarial
graph generation. The evaluation results show that our attack is
still effective, which highlights advanced defenses in future work.
ACKNOWLEDGMENTS
We would like to thank our shepherd Pin-Yu Chen and the anony-
mous reviewers for their comments. This work is supported in
part by the National Key R&D Program of China under Grant
2018YFB1800304, NSFC under Grant 62132011, 61625203 and 61832013,
U.S. ONR under Grant N00014-18-2893, and BNRist under Grant
BNR2020 RC01013. Qi Li and Mingwei Xu are the corresponding
authors of this paper.
Session 1B: Attacks and Robustness CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea120REFERENCES
[1] Pedro HC Avelar, Anderson R Tavares, Thiago LT da Silveira, Clíudio R Jung,
and Luís C Lamb. 2020. Superpixel image classification with graph attention
networks. In 2020 33rd SIBGRAPI Conference on Graphics, Patterns and Images
(SIBGRAPI). IEEE, 203–209.
[2] Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree
Anandkumar. 2018. signSGD: Compressed optimisation for non-convex problems.
In International Conference on Machine Learning. PMLR, 560–569.
[3] Vincent D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefeb-
vre. 2008. Fast unfolding of communities in large networks. Journal of statistical
mechanics: theory and experiment 2008, 10 (2008), P10008.
[4] Aleksandar Bojchevski, Johannes Klicpera, and Stephan Günnemann. 2020. Effi-
cient robustness certificates for discrete data: Sparsity-aware randomized smooth-
ing for graphs, images and more. In International Conference on Machine Learning.
PMLR, 1003–1013.
[5] Heng Chang, Yu Rong, Tingyang Xu, Wenbing Huang, Honglei Zhang, Peng
Cui, Wenwu Zhu, and Junzhou Huang. 2020. A Restricted Black-Box Adversarial
Framework Towards Attacking Graph Embedding Models.. In AAAI. 3389–3396.
[6] Jinyin Chen, Yixian Chen, Haibin Zheng, Shijing Shen, Shanqing Yu, Dan Zhang,
and Qi Xuan. 2020. MGA: Momentum Gradient Attack on Network. arXiv
preprint arXiv:2002.11320 (2020).
[7] Jinyin Chen, Xiang Lin, Ziqiang Shi, and Yi Liu. 2020. Link prediction adversarial
attack via iterative gradient attack. IEEE Transactions on Computational Social
Systems 7, 4 (2020), 1081–1094.
[8] Jinyin Chen, Xiang Lin, Hui Xiong, Yangyang Wu, Haibin Zheng, and Qi Xuan.
2020. Smoothing Adversarial Training for GNN. IEEE Transactions on Computa-
tional Social Systems (2020).
[9] Jinyin Chen, Yangyang Wu, Xuanheng Xu, Yixian Chen, Haibin Zheng, and
Qi Xuan. 2018. Fast gradient attack on network embedding. arXiv preprint
arXiv:1809.02797 (2018).
[10] Zhengdao Chen, Xiang Li, and Joan Bruna. 2017. Supervised community detection
with line graph neural networks. arXiv preprint arXiv:1705.08415 (2017).
[11] Minhao Cheng, Thong Le, Pin-Yu Chen, Jinfeng Yi, Huan Zhang, and Cho-Jui
Hsieh. 2018. Query-efficient hard-label black-box attack: An optimization-based
approach. arXiv preprint arXiv:1807.04457 (2018).
[12] Minhao Cheng, Simranjit Singh, Patrick Chen, Pin-Yu Chen, Sijia Liu, and Cho-Jui
Hsieh. 2019. Sign-opt: A query-efficient hard-label adversarial attack. arXiv
preprint arXiv:1909.10773 (2019).
[13] Hanjun Dai, Hui Li, Tian Tian, Xin Huang, Lin Wang, Jun Zhu, and Le Song. 2018.
Adversarial attack on graph structured data. arXiv preprint arXiv:1806.02371
(2018).
[14] Negin Entezari, Saba A Al-Sayouri, Amirali Darvishzadeh, and Evangelos E
Papalexakis. 2020. All you need is low (rank) defending against adversarial
attacks on graphs. In Proceedings of the 13th International Conference on Web
Search and Data Mining. 169–177.
arXiv preprint
arXiv:1905.05178 (2019).
[16] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E
Dahl. 2017. Neural message passing for quantum chemistry. arXiv preprint
arXiv:1704.01212 (2017).
[17] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation
learning on large graphs. In Advances in neural information processing systems.
1024–1034.
[18] William L Hamilton, Rex Ying, and Jure Leskovec. 2017. Representation learning
[15] Hongyang Gao and Shuiwang Ji. 2019.
Graph u-nets.
on graphs: Methods and applications. arXiv preprint arXiv:1709.05584 (2017).
[19] Xinlei He, Jinyuan Jia, Michael Backes, Neil Zhenqiang Gong, and Yang Zhang.
2020. Stealing Links from Graph Neural Networks. arXiv preprint arXiv:2005.02131
(2020).
[20] Weibo Hu, Chuan Chen, Yaomin Chang, Zibin Zheng, and Yunfei Du. 2021.
Robust graph convolutional networks with directional graph adversarial training.
Applied Intelligence (2021), 1–15.
[21] Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande,
and Jure Leskovec. 2019. Strategies for Pre-training Graph Neural Networks.
arXiv preprint arXiv:1905.12265 (2019).
[22] Hongwei Jin, Zhan Shi, Venkata Jaya Shankar Ashish Peruri, and Xinhua Zhang.
2020. Certified Robustness of Graph Convolution Networks for Graph Classi-
fication under Topological Attacks. Advances in Neural Information Processing
Systems 33 (2020).
[23] Hongwei Jin and Xinhua Zhang. 2021. Robust Training of Graph Convolutional
Networks via Latent Perturbation. In Machine Learning and Knowledge Discovery
in Databases: European Conference, ECML PKDD 2020, Ghent, Belgium, September
14–18, 2020, Proceedings, Part III. Springer International Publishing, 394–411.
[24] George Karypis and Vipin Kumar. 1998. A fast and high quality multilevel scheme
for partitioning irregular graphs. SIAM Journal on scientific Computing 20, 1
(1998), 359–392.
[25] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph
convolutional networks. arXiv preprint arXiv:1609.02907 (2016).
[26] Junhyun Lee, Inyeop Lee, and Jaewoo Kang. 2019. Self-attention graph pooling.
arXiv preprint arXiv:1904.08082 (2019).
[27] Wanyu Lin, Shengxiang Ji, and Baochun Li. 2020. Adversarial Attacks on Link
Prediction Algorithms Based on Graph Neural Networks. In Proceedings of the
15th ACM Asia Conference on Computer and Communications Security. 370–380.
[28] Sijia Liu, Pin-Yu Chen, Xiangyi Chen, and Mingyi Hong. 2018. signSGD via
zeroth-order oracle. In International Conference on Learning Representations.
[29] Sijia Liu, Bhavya Kailkhura, Pin-Yu Chen, Paishun Ting, Shiyu Chang, and Lisa
Amini. 2018. Zeroth-order stochastic variance reduction for nonconvex optimiza-
tion. Advances in Neural Information Processing Systems 31 (2018), 3727–3737.
[30] Xuanqing Liu, Si Si, Xiaojin Zhu, Yang Li, and Cho-Jui Hsieh. 2019. A unified
framework for data poisoning attack to graph-based semi-supervised learning.
arXiv preprint arXiv:1910.14147 (2019).
[31] Guixiang Ma, Nesreen K Ahmed, Theodore L Willke, Dipanjan Sengupta,
Michael W Cole, Nicholas B Turk-Browne, and Philip S Yu. 2019. Deep graph simi-
larity learning for brain data analysis. In Proceedings of the 28th ACM International
Conference on Information and Knowledge Management. 2743–2751.
[32] Jiaqi Ma, Shuangrui Ding, and Qiaozhu Mei. 2020. Black-box adversarial at-
tacks on graph neural networks with limited node access.
arXiv preprint
arXiv:2006.05057 (2020).
[33] Yao Ma, Suhang Wang, Tyler Derr, Lingfei Wu, and Jiliang Tang. 2019. Attacking
graph convolutional networks via rewiring. arXiv preprint arXiv:1906.03750
(2019).
[34] Thibault Maho, Teddy Furon, and Erwan Le Merrer. 2021. SurFree: a fast surrogate-
free black-box attack. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. 10430–10439.
[35] Christopher Morris, Nils M. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel,
and Marion Neumann. 2020. TUDataset: A collection of benchmark datasets for
learning with graphs. In ICML 2020 Workshop on Graph Representation Learning
and Beyond (GRL+ 2020). arXiv:2007.08663 www.graphlearning.io
[36] Yurii Nesterov and Vladimir Spokoiny. 2017. Random gradient-free minimization
of convex functions. Foundations of Computational Mathematics 17, 2 (2017),
527–566.
[37] Kaspar Riesen and Horst Bunke. 2008. IAM graph database repository for graph
based pattern recognition and machine learning. In Joint IAPR International
Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural
and Syntactic Pattern Recognition (SSPR). Springer, 287–297.
[38] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. 2020. Dropedge:
Towards deep graph convolutional networks on node classification. In ICLR.
https://www.cs.columbia.edu/CAVE/software/softlib/coil-100.php. (1996).
[39] S. K. Nayar S. A. Nene and H. Murase. 1996. Columbia Object Image Library.
[40] Nino Shervashidze, Pascal Schweitzer, Erik Jan Van Leeuwen, Kurt Mehlhorn,
and Karsten M Borgwardt. 2011. Weisfeiler-lehman graph kernels. Journal of
Machine Learning Research 12, 9 (2011).
[41] Yiwei Sun, Suhang Wang, Xianfeng Tang, Tsung-Yu Hsieh, and Vasant Honavar.
2019. Node injection attacks on graphs via reinforcement learning. arXiv preprint
arXiv:1909.06543 (2019).
[42] Tsubasa Takahashi. 2019. Indirect Adversarial Attacks via Poisoning Neighbors
for Graph Convolutional Networks. In 2019 IEEE International Conference on Big
Data (Big Data). IEEE, 1395–1400.
[43] Haoteng Tang, Guixiang Ma, Yurong Chen, Lei Guo, Wei Wang, Bo Zeng, and
Liang Zhan. 2020. Adversarial Attack on Hierarchical Graph Pooling Neural
Networks. arXiv preprint arXiv:2005.11560 (2020).
[44] Yunzhe Tian, Jiqiang Liu, Endong Tong, Wenjia Niu, Liang Chang, Qi Alfred
Chen, Gang Li, and Wei Wang. 2021. Towards Revealing Parallel Adversarial
Attack on Politician Socialnet of Graph Structure. Security and Communication