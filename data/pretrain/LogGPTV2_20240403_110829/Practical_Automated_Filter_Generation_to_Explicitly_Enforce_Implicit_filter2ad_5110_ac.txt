   –  the  single  character  that  delimits
constants in the  list (if the list contains a single
constant, a NULL delimiter is specified);
   – the list of one or more constants being
related to the current parameter or its size (depending on
the choice of );
    –  (optional)  concise  natural-language
description of the semantics of the current assumption.
  1
  SELF
  LEX_EQ
  ,
  http,https
  Parameter 1 should be one of the strings
        "http" or "https"
  3
  SIZE
  LE
  NULL
  30
  Parameter 3 should be no more than 30
        characters long
  3
  SELF
  ENDS_IN
  NULL
  .com
  Parameter 3 should end with the string
        ".com"
Figure 5. Sample XML description of
assumptions on parameters
If the  list contains more than one constant,
all subsequent constants have to be of the same type as the
first one. For instance, the relation EQ expects one or more
numerical  values,  all 
integers  or  all  floating-point
numbers. This underscores one of the advantages of using
a  GUI-based 
intermediate  description  synthesizer  –
automated type-checking at description generation time.
When 
there  are 
two  or  more  constants 
the
 list, the relation has disjunctive semantics: the
parameter in question satisfies the specified relation  with
the list if it does so with at least one of the constants in the
list.  (See  Table  1  and  its  footnote  for  a  list  of  relations
allowing  disjunctive  semantics.)  For  instance,  the  first
in 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:06:13 UTC from IEEE Xplore.  Restrictions apply. 
assumption  in  Figure  5  demands  that  parameter  1  be
lexicographically equal to one of http or https.
It is clear from Table 1 that checking if a parameter is
among a given list of values, and validating the allowable
character set for the representation of a parameter, are the
two situations  when disjunction is involved in  specifying
assumptions.  Both 
the
implementation.
handled 
easily 
are 
in 
Finally, an input filter consists of a set of assumptions
the  extracted  parameters.  The  XML  structure
on 
 describes a back-end server filter.
  8
  100
  See the XML parsing schema
              descriptions
    MoreComplex.xml
    Some more complex
               assumptions
Figure 6. Sample XML description
of a back-end server input filter
It specifies:
   – the number of parameters;
   
the  maximum
allowable length of the input string before it is parsed into
parameters;
– 
    –  (optional)  concise  natural-
language description of the filter semantics;
  one  or  more    structures  (as  discussed
above),  each  describing  an  assumption  on  some
parameter;
  zero or more  structures, each
containing  a  “pointer”  to  an  XML  file  and  an  optional
description of it (Fig. 6).
Complex  assumptions  are  a  provision  for  making  the
model  extensible  by  adding  a  level  of  indirection,  which
helps  to  avoid  changes  to  the  core  XML  format  or  the
filter-generating  code.  They  are  specified  in  dedicated
XML files, referred to by the  tag inside the
  structure  (see  Figure  6).  Complex
assumptions  also  allow  additions  to  a  filter  format  to
happen  incrementally  (i.e.,  as  more  reports  and  exploits
become known) and with ensured backward compatibility.
Thus, assumptions which otherwise would not fit into our
current  structure  can  still  be  described,  albeit  in  separate
XML files. We do not specify what the structure of such
files should be.
One potential practical use of complex assumptions is
as follows: assumptions described without resorting to the
  provision  only  involve  relations
between parameters and specific constant values (from the
  substructure)  –  numerical  values,  character
strings or character set ranges. While this is most often the
case now, it may turn out to be useful to provide for future
enhancements. An example of a complex assumption may
involve  computing  a  checksum  over  several  network
Table 1. Relations used in defining assumptions on parameters
Relation name
CONSISTS
EXCLUDES
ENDS_IN
LE
LT
GE
Relational semantics
The parameter as a string consists only of characters from a specified set, e.g., a-z, 0-9, etc. 1
The parameter as a string excludes a specified substring 2
The parameter as a string ends in a specified string (suffix) 1
The parameter or its size (whichever is specified) is less than or equal to a given numerical constant
The parameter or its size (whichever is specified) is less than a given numerical constant
The  parameter  or  its  size  (whichever  is  specified)  is  greater  than  or  equal  to  a  given  numerical
constant
The parameter or its size (whichever is specified) is greater than a given numerical constant
The parameter or its size (whichever is specified) is equal to a given numerical constant 1
The parameter as a string lexicographically precedes or is equal to a specified string
The parameter as a string lexicographically precedes a specified string
The parameter as a string lexicographically succeeds or is equal to a specified string
The parameter as a string lexicographically succeeds a specified string
The parameter as a string is lexicographically equal to a specified string 1
GT
EQ
LEX_LE
LEX_LT
LEX_GE
LEX_GT
LEX_EQ
1  These relations can have disjunctive semantics, i.e., there can be more than one specified constant (all delimited appropriately) on the
right-hand side, and if so, the parameter is required to satisfy the relation with at least one of these constants.
2  The only relation with a “negative” meaning. It is provided specifically for exclusion of particular character strings known to have
been used in malicious ways (e.g., from released bug reports; see [4]).
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:06:13 UTC from IEEE Xplore.  Restrictions apply. 
packet  fields  and  comparing  the  result  to  some  other
packet field which itself is not a constant.
The  complex  condition  mechanism  works  as  follows:
given  a  routine  (say,  in  VBScript)  that  can  verify  the
desired  complex  assumption,  the  routine’s  name  and  its
arguments are recorded in an appropriately designed XML
structure. Then, that structure is embedded in a file listed
as one that contains complex assumptions.
To  facilitate 
the  process  of  formally  describing
assumptions,  we  have  implemented  a  second  GUI-based
tool,  the  output  of  which  is  the  XML  description  of
assumptions  specified  via  the  GUI.  The  tool  performs
type-checking  on  all  fields  at  the  time  of  entering  new
data.  Type-  and  format-checking  is  also  done  before  a
description is loaded from an XML file.
4.5. Prototype Statistics
lines  (describing  a 
Our  implementation  consists  of  approximately  4000
lines  of  C++  code;  of  those  almost  2400  lines  are  core
functionality  unrelated  to  GUIs.  For  the  sample  HTTP
request  the  intermediate  XML  description  files  are  50
lines (describing the parsing scheme for 8 parameters) and
180 
total  of  21  assumptions),
respectively.  The  resulting  script  is  approximately  350
lines of VBScript code  with its size linearly  proportional
to  both  the  number  of  parameters  and  the  number  of
assumptions  to  be  enforced  on  them.  (An  input  resulting
in, say, 16 parameters and 80 assumptions on them would
need a filtering script of about 700 lines of code.)
Since  our  design  is  conceptually  simple  and  the
implementation  is  small  enough  in  size  to  actually  be
comprehensible,  we  believe  that  our  process  of  filter
generation  (and  hence  the  resulting  filtering  scripts)
should  be  less  prone  to  errors  than  more  complex  filter
generation  mechanisms  or  software  patch  development.
Our  filters  should  also  be  easy  to  test  quickly  and
effectively prior to distribution and application.
5. Evaluation
We  evaluate  our  prototype 
implementation  by
assessing the extent to which it improves security and by
estimating its effect on performance.
5.1. Security
Security  is  inherently  hard  to  measure.  There  is  no
universally  accepted  procedure  by  which  to  evaluate  it.
Still there are practical indications which can give insight
into the potential resilience of a system to threats.
To  assess  how  successful  our  tool  is  with  preventing
break-ins,  we looked at  widely  known  archives  of  recent
vulnerabilities, publicly reported over the past 2 years [9,
22].  Among  them  we  identified  three  categories:  those
against which simple lexical filters (such as the back-end
server filters generated by our prototype) can help defend,
those  which  can  be  successfully  fended  off  using  more
general input  filtering, and finally those  which cannot be
exploited merely via malicious inputs and for which input
filtering itself will be insufficient as a remedy. Our results
are summarized in Table 2.
input 
result 
lexical 
filter  would 
We  defined  a  vulnerability  as  “reparable  using
sophisticated  input  filtering”  if  a  sufficiently  complex
input  filter  could  protect  against  its  exploitation,  but  any
purely 
in  either
significantly many false negatives (missed invalid inputs)
or  significantly  many  false  positives  (rejected  valid
inputs).  A  recent  example  is  a  vulnerability  [5]  in  which
the  MIME  types  of  HTML  data  were  deliberately  set
incorrectly  in  order  to  trick  a  browser  into  automatically
opening MIME objects containing malicious content, and
disguised as benign MIME types. Although an input filter
to protect against such a vulnerability is feasible, it would
need  to  perform  sophisticated  analysis  to  determine  the
correct  MIME  type  and  treat  input  data  accordingly.  A
simple  lexical  filter  is  bound  to  fail  at  that  task.  (Other
examples can be found in [7].)
The  “non-reparable  using  input  filtering”  category
combines cases where no amount of  mere input  filtering,
however  sophisticated,  would  lead  to  protecting  the
vulnerable  application.  Those  are  typically  denial  of
service  attacks  [6]  or  vulnerabilities  resulting  from
incorrectly  implemented  application  functionality  [8]  (as
opposed to just lack of input validation). One example of
the  latter  concerns  TCP  initial  sequence  numbers,  which
are  supposed  to  be  generated  at  random  (or  at  least
pseudorandomly) in order to be  unguessable,  and  thus  to
prevent 
Some
implementations, however, have recently been shown not
to  have  the  necessary  cryptographic  properties.  Input
connections. 
of  TCP 
spoofing 
Table 2. Classification of vulnerabilities reported in the period 2000–2001 and
applicability of input filtering to defending against them
Year
2001
2000
Total number of
examined vulnerabilities
Reparable using
simple lexical filters
Reparable using
sophisticated input filtering
Non-reparable using
input filtering
35
29
23 (13)
16 (8)
7 (7)
3 (3)
5 (5)
10 (10)
Numbers in parentheses show the number of independent (i.e., unrelated to each other) classes of reported attacks in each category.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:06:13 UTC from IEEE Xplore.  Restrictions apply. 
filtering clearly cannot remedy this problem.
The  examined  public  archives  indicate  that  simple
lexical  filters  can  help  to  defend  against  a  significant
fraction  of  the  reported  attacks,  many  of  which  are
unrelated  to  each  other.  Our  tool  covers  100%  of  those
vulnerabilities reparable by simple lexical filters.
5.2. Performance
To  evaluate  performance,  we  compare  systems
running  an  application  without  an  interposed  input  filter
versus systems in which a filter is set up to intercept and
analyze inputs before forwarding those which are safe on
to  the  back-end  application.  Since  the  running  time  of  a
filter  of  the  type  we  are  considering  depends  on  the  size
and  complexity  of  its  inputs  as  well  as  on  the  script
implementation complexity of the relations in Table 1, and
since  the  running  time  of  an  application  may  have  no
obvious  relationship  to  the  size  and/or  complexity  of  its
inputs, our comparisons  will  be  based  neither  on  relative
execution times between both scenarios, nor on execution
times of the filter versus those of the application. Instead,
our  approach  will  be  to  provide  an  account  of  the  steps
through  which  control  flow  passes  in  each  of  the  two
scenarios,  and  estimate  on  every  step  the  absolute
overhead due to the use of one of our input filters.
The differences resulting from the use of input filtering
are  highlighted  in  Table  3.  The  italicized  operations  are