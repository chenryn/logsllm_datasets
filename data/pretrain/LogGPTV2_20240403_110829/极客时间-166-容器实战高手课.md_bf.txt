# echo 1 > /sys/kernel/debug/tracing/events/kprobes/myprobe/enable         
# cat /sys/kernel/debug/tracing/trace    …          irqbalance-1328  [005] .... 2773211.189573: myprobe: (do_filp_open+0x0/0x100) dfd=4294967295 pathname="/proc/interrupts"          irqbalance-1328  [005] .... 2773211.189740: myprobe: (do_filp_open+0x0/0x100) dfd=638399 pathname="/proc/stat"          irqbalance-1328  [005] .... 2773211.189800: myprobe: (do_filp_open+0x0/0x100) dfd=638399 pathname="/proc/irq/8/smp_affinity"                bash-15864 [004] .... 2773211.219048: myprobe: (do_filp_open+0x0/0x100) dfd=14819 pathname="/sys/kernel/debug/tracing/"                bash-15864 [004] .... 2773211.891472: myprobe: (do_filp_open+0x0/0x100) dfd=6859 pathname="/sys/kernel/debug/tracing/"                bash-15864 [004] .... 2773212.036449: myprobe: (do_filp_open+0x0/0x100) dfd=4294967295 pathname="/sys/kernel/debug/tracing/"                bash-15864 [004] .... 2773212.197525: myprobe: (do_filp_open+0x0/0x100) dfd=638259 pathname="/sys/kernel/debug/tracing/    …好了，我们通过 perf 和 ftrace 的几个例子，简单了解了 tracepoint 和kprobe是怎么用的。那下面我们再来看看它们的实现原理。Tracepoint刚才，我们已经看到了内核函数 do_sys_open() 里调用了trace_do_sys_open() 这个 treacepoint，那这个 tracepoint是怎么实现的呢？我们还要再仔细研究一下。如果你在内核代码中，直接搜索"trace_do_sys_open"字符串的话，并不能找到这个函数的直接定义。这是因为在Linux 中，每一个 tracepoint的相关数据结构和函数，主要是通过\"DEFINE_TRACE\"和\"DECLARE_TRACE\"这两个宏来定义的。完整的"DEFINE_TRACEslate-object="inline""和"DECLARE_TRACE"宏里，给每个 tracepoint都定义了一组函数。在这里，我会选择最主要的几个函数，把定义一个tracepoint 的过程给你解释一下。首先，我们来看"trace\_##name"这个函数（提示一下，这里的"##"是 C语言的预编译宏，表示把两个字符串连接起来）。对于每个命名为"name"的tracepoint，这个宏都会帮助它定一个函数。这个函数的格式是这样的，以"trace\_"开头，再加上tracepoint 的名字。我们举个例子吧。比如说，对于"do_sys_open"这个tracepoint，它生成的函数名就是 trace_do_sys_open。而这个函数会被内核函数do_sys_open() 调用，从而实现了一个内核的tracepoint。    static inline void trace_##name(proto)                          \            {                                                               \                    if (static_key_false(&__tracepoint_##name.key))         \                            __DO_TRACE(&__tracepoint_##name,                \                                    TP_PROTO(data_proto),                   \                                    TP_ARGS(data_args),                     \                                    TP_CONDITION(cond), 0);                 \                    if (IS_ENABLED(CONFIG_LOCKDEP) && (cond)) {             \                            rcu_read_lock_sched_notrace();                  \                            rcu_dereference_sched(__tracepoint_##name.funcs);\                            rcu_read_unlock_sched_notrace();                \                    }                                                       \            }在这个 tracepoint 函数里，主要的功能是这样实现的，通过 \_\_DO_TRACE来调用所有注册在这个 tracepoint 上的 probe函数。     #define __DO_TRACE(tp, proto, args, cond, rcuidle)                      \    …                         it_func_ptr = rcu_dereference_raw((tp)->funcs);         \                                                                            \                    if (it_func_ptr) {                                      \                            do {                                            \                                    it_func = (it_func_ptr)->func;          \                                    __data = (it_func_ptr)->data;           \                                    ((void(*)(proto))(it_func))(args);      \                            } while ((++it_func_ptr)->func);                \                    }    …         …而 probe函数的注册，它可以通过宏定义的"register_trace\_##name"函数完成。            static inline int                                               \            register_trace_##name(void (*probe)(data_proto), void *data)    \            {                                                               \                    return tracepoint_probe_register(&__tracepoint_##name,  \                                                    (void *)probe, data);   \            }我们可以自己写一个简单kernel module来注册一个 probe函数，把它注册到已有的 treacepoint 上。这样，这个 probe 函数在每次tracepoint点被调用到的时候就会被执行。你可以动手试一下。好了，说到这里，tracepoint的实现方式我们就讲完了。简单来说**就是在内核代码中需要被 trace 的地方显式地加上 hook点，然后再把自己的 probe 函数注册上去，那么在代码执行的时候，就可以执行probe 函数。**Kprobe我们已经知道了，tracepoint 为内核 trace 提供了 hook 点，但是这些 hook点需要在内核源代码中预先写好。如果在 debug的过程中，我们需要查看的内核函数中没有 hook 点，就需要像前面 perf/ftrace的例子中那样，要通过 Linux kprobe 机制来加载 probe函数。 那我们要怎么来理解 kprobe的实现机制呢？你可以先从内核 samples代码里，看一下kprobe_example.c代码。这段代码里实现了一个 kernelmodule，可以在内核中任意一个函数名 / 符号对应的代码地址上注册三个 probe函数，分别是"pre_handler"、"post_handler"和"fault_handler"。    #define MAX_SYMBOL_LEN   64    static char symbol[MAX_SYMBOL_LEN] = "_do_fork";    module_param_string(symbol, symbol, sizeof(symbol), 0644);         /* For each probe you need to allocate a kprobe structure */    static struct kprobe kp = {                .symbol_name = symbol,    };         …         static int __init kprobe_init(void)    {                int ret;                kp.pre_handler = handler_pre;                kp.post_handler = handler_post;                kp.fault_handler = handler_fault;                     ret = register_kprobe(&kp);                if (ret  pre_handler: p->addr = 0x00000000d301008e, ip = ffffffffb1e8c9d1, flags = 0x246    [8446287.087643]  post_handler: p->addr = 0x00000000d301008e, flags = 0x246    [8446288.019731]  pre_handler: p->addr = 0x00000000d301008e, ip = ffffffffb1e8c9d1, flags = 0x246    [8446288.019733]  post_handler: p->addr = 0x00000000d301008e, flags = 0x246    [8446288.022091]  pre_handler: p->addr = 0x00000000d301008e, ip = ffffffffb1e8c9d1, flags = 0x246    [8446288.022093]  post_handler: p->addr = 0x00000000d301008e, flags = 0x246kprobe 的基本工作原理其实也很简单。当 kprobe函数注册的时候，其实就是把目标地址上内核代码的指令码，替换成了"cc"，也就是int3指令。这样一来，当内核代码执行到这条指令的时候，就会触发一个异常而进入到Linux int3 异常处理函数 do_int3()里。 在 do_int3() 这个函数里，如果发现有对应的 kprobe 注册了probe，就会依次执行注册的 pre_handler()，原来的指令，最后是post_handler()。![](Images/36325651f318918c2fc7f584d78d74fe.png)savepage-src="https://static001.geekbang.org/resource/image/54/96/5495fee9d95a7f0df6b7f48d8bd25196.jpeg"}理论上 kprobe其实只要知道内核代码中任意一条指令的地址，就可以为这个地址注册 probe函数，kprobe结构中的"addr"成员就可以接受内核中的指令地址。    static int __init kprobe_init(void)    {            int ret;            kp.addr = (kprobe_opcode_t *)0xffffffffb1e8ca02; /* 把一条指令的地址赋值给 kprobe.addr */            kp.pre_handler = handler_pre;            kp.post_handler = handler_post;            kp.fault_handler = handler_fault;                 ret = register_kprobe(&kp);            if (ret "的 5个字节（在启动的时候被替换成了 nop）。Kprobe 对于函数头指令的 trace方式，也会用"ftrace_caller"指令替换的方式，而不再使用 int3指令替换。 不论是哪种替换方式，kprobe的基本实现原理都是一样的，那就是**把目标指令替换，替换的指令可以使程序跑到一个特定的handler 里，去执行 probe的函数。** 重点小结这一讲我们主要学习了 tracepoint 和 kprobe，这两个概念在 Linux tracing系统中非常重要。为什么说它们重要呢？因为从 Linux tracing系统看，我的理解是可以大致分成大致这样三层。第一层是最基础的提供数据的机制，这里就包含了tracepoints、kprobes，还有一些别的 events，比如 perf 使用的 HW/SWevents。 第二层是进行数据收集的工具，这里包含了 ftrace、perf，还有ebpf。 第三层是用户层工具。虽然有了第二层，用户也可以得到数据。不过，对于大多数用户来说，第二层使用的友好程度还不够，所以又有了这一层。![](Images/37d7c662a9feb383e41ba7b010a6dc4a.png)savepage-src="https://static001.geekbang.org/resource/image/90/8b/9048753d623f0aec9e8b513623f1ec8b.jpeg"}很显然，如果要对 Linux 内核调试，很难绕过 tracepoint 和kprobe。如果不刨根问底的话，前面我们讲的 perf、trace工具对你来说还是黑盒。因为你只是知道了这些工具怎么用，但是并不知道它们依赖的底层技术。在后面介绍 ebpf 的时候，我们还会继续学习 ebpf 是如何使用 tracepoint和 kprobe 来做 Linux tracing的，希望你可以把相关知识串联起来。思考题想想看，当我们用 kprobe 为一个内核函数注册了 probe之后，怎样能看到对应内核函数的第一条指令被替换了呢？欢迎你在留言区记录你的思考或者疑问。如果这一讲对你有帮助，也欢迎你转发给同事、朋友，跟他们一起交流、进步。
# 加餐05 \| eBPF：怎么更加深入地查看内核中的函数？你好，我是程远。今天这一讲，我们聊一聊eBPF。在我们专题加餐第一讲的分析案例时就说过，当我们碰到网络延时问题，在毫无头绪的情况下，就是依靠了我们自己写的一个eBPF 工具，找到了问题的突破口。由此可见，eBPF 在内核问题追踪上的重要性是不言而喻的。那什么是eBPF，它的工作原理是怎么样，它的编程模型又是怎样的呢？在这一讲里，我们就来一起看看这几个问题。eBPF 的概念eBPF，它的全称是"Extended Berkeley PacketFilter"。从名字看，你可能会觉奇怪，似乎它就是一个用来做网络数据包过滤的模块。其实这么想也没有错，eBPF 的概念最早源自于 BSD 操作系统中的BPF（Berkeley Packet Filter），1992 伯克利实验室的一篇论文 ["The BSD PacketFilter: A New Architecture for User-level PacketCapture"  slate-object="inline"。这篇论文描述了，BPF是如何更加高效灵活地从操作系统内核中抓取网络数据包的。我们很熟悉的 tcpdump 工具，它就是利用了 BPF 的技术来抓取 Unix操作系统节点上的网络包。Linux 系统中也沿用了 BPF的技术。 那 BPF 是怎样从内核中抓取数据包的呢？我借用 BPF论文中的图例来解释一下：![](Images/a54a0b1cfc6d59ecc71d13abf0de21e1.png)savepage-src="https://static001.geekbang.org/resource/image/ae/04/ae55e7de056120c57af0703e0afd7b04.png"}结合这张图，我们一起看看 BPF实现有哪些特点。第一，内核中实现了一个虚拟机，用户态程序通过系统调用，把数据包过滤代码载入到个内核态虚拟机中运行，这样就实现了内核态对数据包的过滤。这一块对应图中灰色的大方块，也就是BPF 的核心。第二，BPF 模块和网络协议栈代码是相互独立的，BPF 只是通过简单的几个hook 点，就能从协议栈中抓到数据包。内核网络协议代码变化不影响 BPF的工作，图中右边的"protocolstack"方块就是指内核网络协议栈。第三，内核中的 BPF filter 模块使用 buffer 与用户态程序进行通讯，把filter 的结果返回给用户态程序（例如图中的 networkmonitor），这样就不会产生内核态与用户态的上下文切换（contextswitch）。 在 BPF 实现的基础上，Linux 在 2014 年内核 3.18 的版本上实现了eBPF，全名是 Extended BPF，也就是 BPF的扩展。这个扩展主要做了下面这些改进。首先，对虚拟机做了增强，扩展了寄存器和指令集的定义，提高了虚拟机的性能，并且可以处理更加复杂的程序。其次，增加了 eBPF maps，这是一种存储类型，可以保存状态信息，从一个BPF事件的处理函数传递给另一个，或者保存一些统计信息，从内核态传递给用户态程序。最后，eBPF可以处理更多的内核事件，不再只局限在网络事件上。你可以这样来理解，eBPF的程序可以在更多内核代码 hook 点上注册了，比如 tracepoints、kprobes等。 在 Brendan Gregg 写的书《BPF PerformanceTools  slate-object="inline"》里有一张 eBPF 的架构图，这张图对 eBPF内核部分的模块和工作流的描述还是挺完整的，我也推荐你阅读这本书。图书的网上预览部分也可以看到这张图，我把它放在这里，你可以先看一下。这里我想提醒你，我们在后面介绍例子程序的时候，你可以回头再来看看这张图，那时你会更深刻地理解这张图里的模块。![](Images/0801b3842213a323bfebe10d172f0daf.png)savepage-src="https://static001.geekbang.org/resource/image/1f/8d/1f1af6f7ab8d4a3a2f58cbcd9e9c2e8d.png"}当 BPF 增强为 eBPF 之后，它的应用范围自然也变广了。从单纯的网络包抓取，扩展到了下面的几个领域：1.       网络领域，内核态网络包的快速处理和转发，你可以看一下        [XDP            （eXpress Data    Path）。        2.       安全领域，通过        [LSM            （Linux Security Module）的 hook    点，eBPF 可以对 Linux    内核做安全监控和访问控制，你可以参考        [KRSI            （Kernel Runtime Security    Instrumentation）的文档。        3.       内核追踪 / 调试，eBPF 能通过 tracepoints、kprobes、 perf-events    等 hook    点来追踪和调试内核，这也是我们在调试生产环境中，解决容器相关问题时使用的方法。        eBPF 的编程模型前面说了很多 eBPF 概念方面的内容，如果你是刚接触eBPF，也许还不能完全理解。所以接下来，我们看一下 eBPF编程模型，然后通过一个编程例子，再帮助你理解eBPF。 eBPF 程序其实也是遵循了一个固定的模式，Daniel Thompson的" [Kernel analysisusing eBPF  slate-object="inline""里的一张图解读得非常好，它很清楚地说明了 eBPF的程序怎么编译、加载和运行的。![](Images/c8e2167eccc36aa0ab6b5908e0afebe9.png)savepage-src="https://static001.geekbang.org/resource/image/c4/a2/c4ace7ab4a77d6a9522801c96fd6d2a2.png"}结合这张图，我们一起分析一下 eBPF的运行原理。一个 eBPF 的程序分为两部分，第一部分是内核态的代码，也就是图中的foo_kern.c，这部分的代码之后会在内核 eBPF的虚拟机中执行。第二部分是用户态的代码，对应图中的foo_user.c。它的主要功能是负责加载内核态的代码，以及在内核态代码运行后通过eBPF maps 从内核中读取数据。然后我们看看 eBPF 内核态程序的编译，因为内核部分的代码需要被编译成eBPF bytecode 二进制文件，也就是 eBPF 的虚拟机指令，而在 Linux里，最常用的 GCC 编译器不支持生成 eBPFbytecode，所以这里**必须要用 Clang/LLVM来编译**，编译后的文件就是foo_kern.o。foo_user.c 编译链接后就会生成一个普通的用户态程序，它会通过 bpf()系统调用做两件事：第一是去加载 eBPF bytecode 文件 foo_kern.o，使foo_kern.o 这个 eBPF bytecode 在内核 eBPF 的虚拟机中运行；第二是创建eBPFmaps，用于内核态与用户态的通讯。接下来，在内核态，eBPF bytecode 会被加载到 eBPF内核虚拟机中，这里你可以参考一下前面的 eBPF架构图。 执行 BPF 程序之前，BPF Verifier 先要对 eBPF bytecode进行很严格的指令检查。检查通过之后，再通过 JIT（Just InTime）编译成宿主机上的本地指令。编译成本地指令之后，eBPF 程序就可以在内核中运行了，比如挂载到tracepoints hook 点，或者用 kprobes来对内核函数做分析，然后把得到的数据存储到 eBPF maps 中，这样 foo_user这个用户态程序就可以读到数据了。我们学习 eBPF 的编程的时候，可以从编译和执行 Linux 内核中samples/bpf目录下的例子开始。在这个目录下的例子里，包含了 eBPF各种使用场景。每个例子都有两个.c 文件，命名规则都是 xxx_kern.c 和xxx_user.c，编译和运行的方式就和我们刚才讲的一样。本来我想拿 samples/bpf 目录下的一个例子来具体说明的，不过后来我在github上看到了一个更好的例子，它就是ebpf-kill-exampleslate-object="inline"。下面，我就用这个例子来给你讲一讲，如何编写 eBPF程序，以及 eBPF代码需要怎么编译与运行。我们先用 git clone取一下代码：    
# git clone https://github.com/niclashedam/ebpf-kill-example    
# cd ebpf-kill-example/    
# ls    docs  img  LICENSE  Makefile  README.md  src  test这里你可以先看一下 Makefile，请注意编译 eBPF 程序需要Clang/LLVM，以及由 Linux 内核源代码里的 tools/lib/bpf 中生成的 libbpf.so库和相关的头文件。如果你的 OS 是Ubuntu，可以运行`make deps;make kernel-src`这个命令，准备好编译的环境。    
# cat Makefile    …    deps:                sudo apt update                sudo apt install -y build-essential git make gcc clang libelf-dev gcc-multilib         kernel-src:                git clone --depth 1 --single-branch --branch ${LINUX_VERSION}  https://github.com/torvalds/linux.git kernel-src                cd kernel-src/tools/lib/bpf && make && make install prefix=../../../../    …完成上面的步骤后，在 src/ 目录下，我们可以看到两个文件，分别是bpf_program.c 和 loader.c。在这个例子里，bpf_program.c 对应前面说的 foo_kern.c 文件，也就是说eBPF 内核态的代码在 bpf_program.c 里面。而 loader.c 就是 eBPF用户态的代码，它主要负责把 eBPF bytecode 加载到内核中，并且通过 eBPFMaps 读取内核中返回的数据。    
# ls src/    bpf_program.c  loader.c我们先看一下 bpf_program.c中的内容：     
# cat src/bpf_program.c    #include     #include     #include "bpf_helpers.h"    //这里定义了一个eBPF Maps     //Data in this map is accessible in user-space    struct bpf_map_def SEC("maps") kill_map = {          .type        = BPF_MAP_TYPE_HASH,          .key_size    = sizeof(long),          .value_size  = sizeof(char),          .max_entries = 64,    };         // This is the tracepoint arguments of the kill functions    // /sys/kernel/debug/tracing/events/syscalls/sys_enter_kill/format    struct syscalls_enter_kill_args {        long long pad;             long syscall_nr;        long pid;        long sig;    };    // 这里定义了BPF_PROG_TYPE_TRACEPOINT类型的BPF Program       SEC("tracepoint/syscalls/sys_enter_kill")    int bpf_prog(struct syscalls_enter_kill_args *ctx) {      // Ignore normal program terminations      if(ctx->sig != 9) return 0;           // We can call glibc functions in eBPF      long key = labs(ctx->pid);      int val = 1;           // Mark the PID as killed in the map      bpf_map_update_elem(&kill_map, &key, &val, BPF_NOEXIST);           return 0;    }         // All eBPF programs must be GPL licensed    char _license[] SEC("license") = "GPL";在这一小段代码中包含了 eBPF代码最重要的三个要素，分别是：1.  BPF Program Types        2.  BPF Maps        3.  BPF Helpers        "BPF Program Types"定义了函数在 eBPF内核态的类型，这个类型决定了这个函数会在内核中的哪个 hook点执行，同时也决定了函数的输入参数的类型。在内核代码bpf_prog_type的枚举定义里，你可以看到 eBPF支持的所有"BPF Program Types"。比如在这个例子里的函数 bpf_prog()，通过 SEC()这个宏，我们可以知道它的类型是 BPF_PROG_TYPE_TRACEPOINT，并且它注册在syscalls subsystem 下的 sys_enter_kill 这个 tracepoint上。 既然我们知道了具体的 tracepoint，那么这个 tracepoint的注册函数的输入参数也就固定了。在这里，我们就把参数组织到syscalls_enter_kill_args{}这个结构里，里面最主要的信息就是 kill()系统调用中，输入信号的**编号 sig** 和**信号发送目标进程的pid**。"BPF Maps"定义了 key/value 对的一个存储结构，它用于 eBPF内核态程序之间，或者内核态程序与用户态程序之间的数据通讯。eBPF中定义了不同类型的Maps，在内核代码bpf_map_typeslate-object="inline"的枚举定义中，你可以看到完整的定义。在这个例子里，定义的 kill_map 是 BPF_MAP_TYPE_HASH 类型，这里也用到了SEC()这个宏，等会儿我们再解释，先看其他的。kill_map 是 HASH Maps 里的一个 key，它是一个 long 数据类型，value是一个 char 字节。bpf_prog() 函数在系统调用 kill() 的 tracepoint上运行，可以得到目标进程的 pid 参数，Maps 里的 key 值就是这个 pid参数来赋值的，而 val 只是简单赋值为1。 然后，这段程序调用了一个函数 bpf_map_update_elem()，把这组新的key/value 对写入了到 kill_map 中。这个函数 bpf_map_update_elem()就是我们要说的第三个要素 BPFHelpers。 我们再看一下"BPF Helpers"，它定义了一组可以在 eBPF内核态程序中调用的函数。尽管 eBPF 程序在内核态运行，但是跟 kernel module 不一样，eBPF程序不能调用普通内核 export 出来的函数，而是只能调用在内核中为 eBPF事先定义好的一些接口函数。这些接口函数叫作 BPFHelpers，具体有哪些你可以在"Linux manualpage"中查看。看明白这段代码之后，我们就可以运行 `make build`命令，把 C 代码编译成 eBPF bytecode 了。这里生成了 src/bpf_program.o这个文件：     
# make build    clang -O2 -target bpf -c src/bpf_program.c -Ikernel-src/tools/testing/selftests/bpf -Ikernel-src/tools/lib/bpf -o src/bpf_program.o         
# ls -l src/bpf_program.o    -rw-r----- 1 root root 1128 Jan 24 00:50 src/bpf_program.o接下来，你可以用 LLVM 工具来看一下 eBPF bytecode里的内容，这样做可以确认下面两点。1.       编译生成了 BPF 虚拟机的汇编指令，而不是 x86    的指令。        2.       在代码中用 SEC 宏添加的"BPF Program Types"和"BPF    Maps"信息也在后面的 section    里。    查看 eBPF bytecode信息的操作如下：    
### 用objdump来查看bpf_program.o里的汇编指令     