The Homa transport contains one additional mechanism not
previously described, which limits buffer buildup in the NIC
CPU
NICs
CloudLab
Xeon D1548 (8 cores @
2.0 GHz)
Mellanox ConnectX-3 (10
Gbps Ethernet)
Switches HP Moonshot-45XGc (10
Gbps Ethernet)
Infiniband
Xeon X3470 (4 cores @
2.93 GHz)
Mellanox ConnectX-2 (24
Gbps)
Mellanox MSX6036 (4X
FDR) and Infiniscale IV
(4X QDR)
Figure 7: Hardware configurations. The Infiniband cluster was used
for measuring Infiniband performance; CloudLab was used for all
other measurements.
transmit queue. In order for a sender to implement SRPT pre-
cisely, it must keep the transmit queue in the NIC short, so
that high priority packets don’t have to wait for lower priority
packets queued previously (as described in §3.2, the sender’s
priority for an outgoing packet does not necessarily correspond
to the priority stored in the packet). To do this, Homa keeps a
running estimate of the total number of untransmitted bytes in
the NIC, and it only hands off a packet to the NIC if the num-
ber of untransmitted bytes (including the new packet) will be
two full-size packets or less. This allows the sender to reorder
outgoing packets when new messages arrive.
5 EVALUATION
We evaluated Homa by measuring the RAMCloud implemen-
tation and also by running simulations. Our goal was to answer
the following questions:
∙ Does Homa provide low latency for short messages even
at high network load and in the presence of long mes-
sages?
∙ How efficiently does Homa use network bandwidth?
∙ How does Homa compare to existing state-of-the-art ap-
∙ How important are Homa’s novel features to its perfor-
proaches?
mance?
Implementation Measurements
5.1
We used the CloudLab cluster described in Figure 7 to measure
the performance of the Homa implementation in RAMCloud.
The cluster contained 16 nodes connected to a single switch
using 10 Gbps Ethernet; 8 nodes were used as clients and 8 as
servers. Each client generated a series of echo RPCs; each RPC
sent a block of a given size to a server, and the server returned
the block back to the client. Clients chose RPC sizes pseudo-
randomly to match one of the workloads from Figure 1, with
Poisson arrivals configured to generate a particular network
load. The server for each RPC was chosen at random.
Figure 8 graphs the performance of Homa and several other
RAMCloud transports for workloads W3-W5 at 80% network
load (W1 and W2 are not shown because RAMCloud’s software
overheads are too high to handle the large numbers of small mes-
sages generated by these workloads at 80% network utilization).
Our primary metric for evaluating Homa, shown in Figure 8, is
99th percentile tail slowdown, where slowdown is the ratio of
the actual time required to complete an echo RPC divided by
the best possible time for an RPC of that size on an unloaded
Homa: A Receiver-Driven Low-Latency Transport Protocol SIGCOMM ’18, August 20-25, 2018, Budapest, Hungary
Figure 8: Tail latency of Homa and other RAMCloud transports for workloads W3, W4, and W5 at 80% network load. X-axes are linear in
total number of messages (each tick is 10% of all messages). “HomaPx” measures Homa restricted to use only x priorities. “Basic” measures
the preexisting Basic transport in RAMCloud, which corresponds roughly to HomaP1 with no limit on overcommitment. ”InfRC” measures
RAMCloud’s Infrc transport, which uses Infiniband reliable connected queue pairs. “InfRC-MC” uses Infiniband with multiple connections per
client-server pair. “TCP-MC” uses kernel TCP with multiple connections per client-server pair. Homa, Basic, and TCP were measured on the
CloudLab cluster. InfRC was measured on the Infiniband cluster using the same absolute workload, so its network utilization was only about 33%.
Best-case RPC times (slowdown of 1.0) for 100 byte RPCs are 3.9 µs for InfRC, 4.7 µs for Homa and Basic, and 15.5 µs for TCP.
network. A slowdown of 1 is ideal. The x-axis for each graph
is scaled to match the CDF of message sizes: the axis is linear in
total number of messages, with ticks corresponding to 10% of
all messages in that workload. This results in a different x-axis
scale for each workload, which makes it easier to see results for
the message sizes that are most common.
Homa provides a 99th percentile tail slowdown in the range
of 2–3.5 across a broad range of RPC sizes and workloads. For
example, a 100-byte echo RPC takes 4.7 µs in an unloaded
network; at 80% network load, the 99th-percentile latency was
about 14 µs in all three loads.
To quantify the benefits provided by the priority and overcom-
mitment mechanisms in Homa, we also measured RAMCloud’s
Basic transport. Basic is similar to Homa in that it is receiver-
driven, with grants and unscheduled packets. However, Basic
does not use priorities and it has no limit on overcommitment: re-
ceivers grant independently to all incoming messages. Figure 8
shows that tail latency is 5–15x higher in Basic than in Homa. By
analyzing detailed packet traces we determined that Basic’s high
latency is caused by queuing delays at the receiver’s downlink;
Homa’s use of priorities eliminates almost all of these delays.
Although Homa prioritizes small messages, it also outper-
forms Basic for large ones. This is because Homa’s SRPT pol-
icy tends to produce run-to-completion behavior: it finishes the
highest priority message before giving service to any other mes-
sages. In contrast, Basic, like TCP, tends to produce round-robin
behavior; when there are competing large messages, they all
complete slowly.
For the very largest messages, Homa produces 99th-percentile
slowdowns of 100x or more. This is because of the SRPT policy.
We speculate that the performance of these outliers could be
improved by dedicating a small fraction of downlink bandwidth
to the oldest message; we leave a full analysis of this alternative
to future work.
To answer the question “How many priority levels does Homa
need?” we modified the Homa transport to reduce the number of
priority levels by collapsing adjacent priorities. Figure 8 shows
the results. 99th-percentile tail latency is almost as good with
4 priority levels as with 8, but tail latency increases noticeably
when there are only 2 priority levels. Homa with only one pri-
ority level is still significantly better than Basic; this is because
Homa’s limit on overcommitment results in less buffering than
Basic, which reduces preemption lag.
Homa vs. Infiniband. Figure 8 also measures RAMCloud’s In-
fRC transport, which uses kernel bypass with Infiniband reliable
connected queue pairs. The Infiniband measurements show the
advantage of Homa’s message-oriented protocol over streaming
protocols. We first measured InfRC in its normal mode, which
uses a single connection for each client-server pair. This re-
sulted in tail latencies about 1000x higher than Homa for small
messages. Detailed traces showed that the long delays were
caused by head-of-line blocking at the sender, where a small
message got stuck behind a very large message to the same desti-
nation. Any streaming protocol, such as TCP, will suffer similar
problems. We then modified the benchmark to use multiple
connections per client-server pair (“InfRC-MC” in the figures).
This eliminated the head-of-line blocking and improved tail
latency by 100x, to about the same level as Basic. As discussed
in §3.1, this approach is probably not practical in large-scale
applications because it causes an explosion of connection state.
InfRC-MC still doesn’t approach Homa’s performance, because
it doesn’t use priorities.
Note: the Infiniband measurements were taken on a different
cluster with faster CPUs, and the Infiniband network offers 24
Gpbs applicaiton level bandwidth, vs. 10 Gbps for Homa and
Basic. The software overheads for InfRC were too high to run
at 80% load on the Infiniband network, so we used the same
absolute load as for the Homa and Basic measurements, which
resulted in only 33% network load for Infiniband. As a result,
Figure 8 overstates the performance of Infiniband relative to
Homa. In particular, Infiniband appears to perform better than
Homa for large messages sizes. This is an artifact of measuring
1235102050100100073443677110158268313402573175515158197Message Size (Bytes)99% Slowdown (Log Scale)Workload: W312351020501001000222731537650256166296063874940812037310000000Message Size (Bytes)Workload: W412351020501001000555958801911029400499807203019551098049020330101029147029400000Message Size (Bytes)InfRCTCP−MCBasicInfRC−MCHomaP1HomaP2HomaP4HomaWorkload: W5SIGCOMM ’18, August 20-25, 2018, Budapest, Hungary
B. Montazeri et al.
supported about 300 concurrent RPCs before performance de-
graded because of packet drops. Homa is less sensitive to incast
than protocols such as TCP because its packet scheduling mech-
anism limits buffer buildup to at most RTTbytes per incoming
message. In contrast, a single TCP connection can consume all
of the buffer space available in a switch.
5.2 Simulations
The rest of our evaluation is based on packet-level simulations.
The simulations allowed us to explore more workloads, mea-
sure behavior at a deeper level, and compare with simulations
of pFabric [4], pHost [13], NDP [15], and PIAS [6]. We chose
pFabric for comparison because it is widely used as a bench-
mark and its performance is believed to be near-optimal. We
chose pHost and NDP because they use receiver-driven packet
scheduling, like Homa, but they make limited use of priorities
and don’t use overcommitment. We chose PIAS because it uses
priorities in a more static fashion than Homa and does not use
receiver-driven scheduling.
Figure 9: Overall throughput when a single Homa client receives
responses for RPCs issued concurrently to 15 servers. Each response
was 10 KB. Each data point shows min, mean, and max values over
10 runs.
Infiniband at 33% network load and Homa at 80%; at equal load
factors, we expect Homa to provide significantly lower latency
than Infiniband at all message sizes.
Homa vs. TCP. The “TCP-MC” curves in Figure 8 shows the
performance of RAMCloud’s TCP transport, which uses the
Linux kernel implementation of TCP. Only workloads W4 and
W5 are shown (system overheads were too high to run W3 at
80% load), and only with multiple connections per client-server
pair (with a single connection, tail slowdown was off the scale
of the graphs). Even in multi-connection mode, TCP’s tail la-
tencies are 10–100x higher than for Homa. We also created a
new RAMCloud transport using mTCP [18], a user-level imple-
mentation of TCP that uses DPDK for kernel bypass. However,
we were unable to achieve latencies for mTCP less than 1 ms;
the mTCP developers confirmed that this behavior is expected
(mTCP batches heavily, which improves throughput at the ex-
pense of latency). We did not graph mTCP results.
Homa vs. other implementations. It is difficult to compare
Homa with other published implementations because most prior
systems do not break out small message performance and some
measurements were taken with slower networks. Nonetheless,
Homa’s absolute performance (14 µs round-trip for small mes-
sages at 80% network load and 99th percentile tail latency) is
nearly two orders of magnitude faster than the best available
comparison systems. For example, HULL [3] reported 782 µs
one-way latency for 1 Kbyte messages at 99th percentile and
60% network load, and PIAS [6] reported 2 ms one-way la-
tency for messages shorter than 100 Kbytes at 99th percentile
and 80% network load; both of these systems used 1 Gbps net-
works. NDP [15] reported more than 600 µs one-way latency
for 100 Kbyte messages at 99th percentile in a loaded 10 Gbps
network, of which more than 400 µs was queueing delay.
Incast. To measure the effectiveness of Homa’s incast control
mechanism, we ran an experiment where a single client initiated
a large number of RPCs in parallel to a collection of servers.
Each RPC had a tiny request and a response of approximately
RTTbytes (10 KB). Figure 9 shows the results. With the in-
cast control mechanism enabled, Homa successfully handled
several thousand simultaneous RPCs without degradation. We
also measured performance with incast control disabled; this
shows the performance that can be expected when incast occurs
for unpredictable reasons. Even under these conditions Homa
The simulations used the same network topology as prior eval-
uations of pFabric, pHost, and PIAS, consisting of 144 hosts
divided among 9 racks with a 2-level switching fabric. Host
links operate at 10 Gbps and TOR-aggregation links operate
at 40 Gbps. For additional details about the simulators, see the
complete version of this paper [22].
Our simulations used an all-to-all communication pattern
similar to that of §5.1, except that each host was both a sender
and a receiver, and the workload consisted of one-way messages
instead of RPCs. New messages are created at senders according
to a Poisson process; the size of each message is chosen from one
of the workloads in Figure 1, and the destination for the message
is chosen uniformly at random. For each simulation we selected
a message arrival rate to produce a desired network load, which
we define as the percentage of available network bandwidth
consumed by goodput packets; this includes application-level
data plus the minimum overhead (packet headers, inter-packet
gaps, and control packets) required by the protocol; it does not
include retransmitted packets.
Tail latency vs. pFabric, pHost, and PIAS. Figure 10 displays
99th percentile slowdown as a function of message size at a net-
work load of 80% for the five workloads in Figure 1. It uses
the same axes as Figure 8 except that slowdown is measured in
terms of one-way message delivery, not RPC round-trips. The
Homa curves in Figure 10 are similar to those in Figure 8, but
slowdowns are somewhat less in Figure 10 (the simulations do
not model queueing delays that occur in software, such as when
an incoming packet cannot be processed immediately because
the receiver is still processing an earlier packet).
Homa delivers consistent low latency for small messages
across all workloads, and its performance is similar to pFabric:
99th-percentile slowdown for the shortest 50% of messages is
never worse than 2.2 at 80% network load. pHost and PIAS
have considerably higher slowdown than Homa and pFabric
in Figure 10. This surprised us, because both pHost and PIAS
claimed performance comparable to pFabric. On further review,
GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG0.02.55.07.510.0010002000300040005000# Concurrent RPCsThroughput (Gbps)GGIncast ControlNo Incast ControlHoma: A Receiver-Driven Low-Latency Transport Protocol SIGCOMM ’18, August 20-25, 2018, Budapest, Hungary
we found that those claims were based on mean slowdown. Our
evaluation follows the original pFabric publication and focuses
on 99th percentile slowdown.
A comparison between the pHost and Homa curves in Fig-
ure 10 shows that a receiver-driven approach is not enough by
itself to guarantee low latency; using priorities and overcommit-
ment reduces tail latency by an additional 30–50%.
The performance of PIAS in Figure 10 is somewhat erratic.