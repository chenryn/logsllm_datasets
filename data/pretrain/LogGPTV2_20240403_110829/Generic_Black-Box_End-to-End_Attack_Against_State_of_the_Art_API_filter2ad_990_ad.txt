is modiﬁed by the user), without source code access. Thus, the GADGET frame-
work expands the number of users that can produce an evasive malware from
malware developers to every person that purchases a malware binary, making
the threat much greater.
In order to meet those requirements, we wrap the malware binary from the
outside with proxy code between the malware code and the OS DLLs implement-
ing the API calls (e.g., kernel32.dll), fulﬁlling requirement #1. The wrapper code
gets the adversarial sequence for the malware binary, generated by Algorithm
2, as a conﬁguration ﬁle. The logic of this wrapper code is to hook all APIs
that will be monitored by the malware classiﬁer. These API calls are known
to the attacker, as mentioned in Sect. 3.2. These hooks call the original APIs
(to preserve the original malware functionality), keep track of the API sequence
executed so far, and call the adversarial example’s additional APIs in the proper
position based on the conﬁguration ﬁle (so they will be monitored by the malware
classiﬁer), instead of hard-coding the adversarial sequence to the code (fulﬁlling
requirement #2). This ﬂow is presented in Fig. 3b.
We generated a new malware binary that contains the wrapper’s hooks by
patching the malware binary’s IAT using IAT Patcher, redirecting the IAT’s
API calls’ addresses to the matching C++ wrapper API hook implementation.
That way, if another hook (e.g., Cuckoo Sandbox) monitors the API calls, the
adversarial APIs are already being called and monitored like any regular API
call. To aﬀect dynamic libraries, LdrGetProcedureAddress()\GetProcAddress()
hook has additional functionality: it doesn’t return a pointer to the requested
procedure, but instead returns a pointer to a wrapper function that implements
the previously described regular static hook functionality around the requested
procedure (e.g., returning a pointer to a wrapper around WriteFile() if “Write-
File” is the argument to GetProcAddress()). When the malware code calls the
pointer, the hook functionality will be called, transparent to the user.
The code is POC and does not cover all corner cases, e.g., wrapping a packed
malware, which requires special handling for the IAT patching to work, or pack-
ing the wrapper code to evade statically signing it as malicious (its functionality
is implemented inline, without external API calls, so dynamic analysis of it is
challenging). We avoided running Algorithm 2 inside the wrapper, and used
the conﬁguration ﬁle to store the modiﬁed APIs instead, thus preventing much
greater overhead for the (wrapped) malware code.
506
I. Rosenberg et al.
5.1 Adversarial Example Functionality Validation
In order to automatically verify that we do not harm the functionality of the
malware we modify, we monitored each sample in Cuckoo Monitor before and
after the modiﬁcation. We deﬁne the modiﬁed sample as functionality preserving
if the API call sequence after the modiﬁcation is the same as before the modi-
ﬁcation when comparing API type, return value and order of API calls, except
for the added API calls, which return value should always be a success value.
We found that all of the 18,000 modiﬁed samples are functionality preserving.
One of the families that did not exist in the training set was the Wan-
naCry ransomware. This makes it an excellent candidate to manually analyze
GADGET’s output. First, we ran the sample via Cuckoo Sandbox and recorded
its API calls. The LSTM malware classiﬁer mentioned in Sect. 4.2 successfully
detected it as malicious, although it was not part of the training set. Then we
used GADGET to generate a new WannaCry variant, providing this variant the
conﬁguration ﬁle containing the adversarial sequence generated by Algorithm
2. We ran the modiﬁed WannaCry binary, wrapped with our framework and
the conﬁguration ﬁle, in Cuckoo Sandbox again, and fed the recorded API call
sequence to the same LSTM malware classiﬁer. This time, the malware classi-
ﬁer classiﬁcation was benign, although the malicious functionality remains: ﬁles
were still being encrypted by the new binary, as can be seen in the Cuckoo Sand-
box snapshot and API call sequence. This means that the proposed attack was
successful, end-to-end, without damaging WannaCry’s functionality.
5.2 Handling API Arguments
We now modify our attack to evade classiﬁers that analyze arguments as well.
In order to represent the API call arguments, we used MIST [20], as was done
by other malware classiﬁers, e.g., MALHEUR [14]. MIST (Malware Instruction
Set) is a representation for monitored behavior of malicious software, optimized
for analysis of behavior using machine learning. Each API call translates to an
instruction. Each instruction has levels of information. The ﬁrst level corresponds
to the category and name of a monitored API call. The following levels of the
instruction contain diﬀerent blocks of arguments. The main idea underlying this
arrangement is to move “noisy” elements, such as the loading address of a DLL,
to the end of an instruction, while discriminative patterns, such as the loaded
DLL ﬁle path, are kept at the beginning of the instruction. We used MIST level
2. We converted our Cuckoo Sandbox reports to MIST using Cuckoo2Mist. We
extracted a total of 220 million lines of MIST instructions from our dataset. Of
those, only several hundred of lines were unique, i.e., diﬀerent permutations of
argument values extracted in MIST level 2. This means that most API calls dif-
fer only in arguments that are not relevant to the classiﬁcation or use the same
arguments. To handle MIST arguments, we modiﬁed our attack in the following
way: Instead of one-hot encoding every API call type, we one-hot encoded every
unique [API call type, MIST level 2 arguments] combination. Thus, LoadLibrary
(“kernel32.dll”) and LoadLibrary (“user32.dll”) are now regarded as separate
End-to-End Attack Against API Call Based Malware Classiﬁers
507
APIs by the classiﬁer. Our framework remains the same, where Algorithm 2
selects the most impactful combination instead of API type. However, instead
of adding combinations that might harm the code’s functionality (e.g., ExitWin-
dowsEx()), we simply add a diﬀerent API call type (the one with the minimal
Jacobian value) in Algorithm 2, which would not cause this issue. We now
assume a more informed attacker, who knows not just the exact encoding of
each API type, but also the exact encoding of every argument combination.
This is a reasonable assumption since arguments used by benign programs, like
Windows DLLs ﬁle paths, are known to attackers [8].
Handling other API arguments (and not MIST level 2) would be similar, but
require more preprocessing (word embedding, etc.) with a negligible eﬀect on the
classiﬁer accuracy. Thus, focusing only on the most important arguments (MIST
level 2) that can be used by the classiﬁer to distinguish between malware and
benign software, as done in other papers [9], proves that analyzing arguments is
not an obstacle for the proposed attack.
5.3 Handling Hybrid Classiﬁers and Multiple Feature Types
Since our attack modiﬁes only a speciﬁc feature type (API calls), combining
several types of features might make the classiﬁer more resistant to adversarial
examples against a speciﬁc feature type. Some real-world next generation anti-
malware products (such as SentinelOne) are hybrid classiﬁers, combining both
static and dynamic features for a better detection rate.
Our attack can be extended to handle hybrid classiﬁers using two phases:
(1) the creation of a combined surrogate model, including all features, using
Algorithm 1, and (2) attacking each feature type in turn with a specialized attack,
using the surrogate model. If the attack against a feature type fails, we continue
and attack the next feature type until a benign classiﬁcation by the target model
is achieved or until all feature types have been (unsuccessfully) attacked.
We decided to use printable strings inside a PE ﬁle as our static features,
as they are commonly used as the static features of state of the art hybrid
malware classiﬁers [9], although any other modiﬁable feature type can be used.
Strings can be used, e.g., to statically identify loaded DLLs and called functions,
recognize modiﬁed ﬁle paths and registry keys, etc. Our architecture for the
hybrid classiﬁer, shown in Fig. 2b, is: (1) A dynamic branch that contains an
input vector of 140 API calls, each one-hot encoded, inserted into a LSTM layer
of 128 units, and sigmoid activation function, with a dropout rate of 0.2 for
both inputs and recurrent states. (2) A static branch that contains an input
vector of 20,000 Boolean values: for each of the 20,000 most frequent strings in
the entire dataset, do they appear in the ﬁle or not? (analogous to a similar
procedure used in NLP, which ﬁlters the least frequent words in a language).
This vector is inserted into two fully-connected layers with 128 neurons, a ReLU
activation function, and a dropout rate of 0.2 each. The 256 outputs of both
branches are inserted into a fully-connected output layer with sigmoid activation
function. Therefore, the input of the classiﬁer is a vector containing 140 one-
hot encoded APIs and 20,000 Boolean values, and the output is malicious or
508
I. Rosenberg et al.
benign classiﬁcation. All other hyper parameters are the same as in Sect. 4.2. The
surrogate model used has a similar architecture to the attacked hybrid model
described above, but it uses a diﬀerent architecture and hyper parameters: GRU
instead of LSTM in the dynamic branch and 64 hidden units instead of 128 in
both static and dynamic surrogate branches. Due to hardware limitations, we
used just a subset of the dataset: 54,000 training samples and test and validation
sets of 6,000 samples each. The dataset was representative and maintained the
same distribution as the dataset described in Sect. 4.1. Trained on this dataset,
a classiﬁer using only the dynamic branch (Fig. 2a) reaches 92.48% accuracy on
the test set, a classiﬁer using only the static branch attains 96.19% accuracy,
and a hybrid model, using both branches (Fig. 2b) achieves 96.94% accuracy,
meaning that using multiple feature types improves the accuracy.
We used two specialized attacks: an attack against API call sequences and an
attack against printable strings. The API sequence attack is Algorithm 2. When
performing it against the hybrid classiﬁer, without modifying the static features
of the sample, the attack eﬀectiveness (Eq. 4) decreases to 45.95%, compared to
96.03% against a classiﬁer trained only on the dynamic features, meaning that
the attack was mitigated by the use of additional features. The strings attack
is a variant of the attack described in [5], using the surrogate model instead of
the attacked model used in [5] to compute the gradients in order to select the
string to add, while the adversarial sample’s maliciousness is still tested against
the attacked model, making this method a black-box attack. In this case, the
attack eﬀectiveness is 68.66%, compared to 77.33% against a classiﬁer trained
only on the static features. Finally, the combined attack’s eﬀectiveness against
the hybrid model was 82.27%. Other classiﬁer types provide similar results which
are not presented here due to space limits.
We designed GADGET with the ability to handle a hybrid model, by adding
its conﬁguration ﬁle’s static features’ modiﬁcation entries. Each such string is
appended to the original binary before being IAT patched, either to the EOF or
to a new section, where those modiﬁcations don’t aﬀect the binary’s functional-
ity.
To summarize, we have shown that while the usage of hybrid models decreases
the specialized attacks’ eﬀectiveness, using our suggested hybrid attack achieves
high eﬀectiveness. While not shown due to space limits, the attack overhead isn’t
signiﬁcantly aﬀected.
6 Conclusions and Future Work
In this paper, we demonstrated a generic black-box attack, generating adversarial
sequences against API call sequence based malware classiﬁers. Unlike previous
adversarial attacks, we have shown an attack with a veriﬁed eﬀectiveness against
all relevant common classiﬁers: RNN variants, feed forward networks, and tra-
ditional machine learning classiﬁers. Therefore, this is a true black-box attack,
which requires no knowledge about the classiﬁer besides the monitored APIs.
We also created the GADGET framework, showing that the generation of the
End-to-End Attack Against API Call Based Malware Classiﬁers
509
adversarial sequences can be done end-to-end, in a generic way, without access
to the malware source code. Finally, we showed that the attack is eﬀective, even
when arguments are analyzed or multiple feature types are used. Our attack is
the ﬁrst practical end-to-end attack dealing with all of the subtleties of the
cyber security domain, posing a concrete threat to next generation anti-malware
products, which have become more and more popular. While this paper focus
on API calls and printable strings as features, the proposed attack is valid for
every modiﬁable feature type, static or dynamic.
Our future work will focus on two areas: defense mechanisms against such
attacks and attack modiﬁcations to cope with such mechanisms. Due to space
limits, we plan to publish an in depth analysis of various defense mechanisms
in future work. The defense mechanisms against such attacks can be divided
into two subgroups: (1) detection of adversarial examples, and (2) making the
classiﬁer resistant to adversarial attacks. To the best of our knowledge, there is
currently no published and evaluated method to detect or mitigate RNN adver-
sarial sequences. This will be part of our future work. We would also compare
between the eﬀectiveness of diﬀerent surrogate models’ architecture.
References
1. Arjovsky, M., Bottou, L.: Towards principled methods for training generative
adversarial networks. In: ICLR (2017)
2. Carlini, N., Wagner, D.: Towards evaluating the robustness of neural networks. In:
IEEE S&P (2017)
3. Chen, P.Y., Zhang, H., Sharma, Y., Yi, J., Hsieh, C.J.: Zoo: zeroth order optimiza-
tion based black-box attacks to deep neural networks without training substitute
models. In: ACM Workshop on Artiﬁcial Intelligence and Security (2017)
4. Goodfellow, I.J., Shlens, J., Szegedy, C.: Explaining and harnessing adversarial
examples. In: ICLR (2015)
5. Grosse, K., Papernot, N., Manoharan, P., Backes, M., McDaniel, P.: Adversarial
examples for malware detection. In: Foley, S.N., Gollmann, D., Snekkenes, E. (eds.)
ESORICS 2017. LNCS, vol. 10493, pp. 62–79. Springer, Cham (2017). https://doi.
org/10.1007/978-3-319-66399-9 4
6. Hu, W., Tan, Y.: Generating adversarial malware examples for black-box attacks
based on GAN. ArXiv e-prints, abs/1702.05983 (2017)
7. Hu, W., Tan, Y.: Black-box attacks against RNN based malware detection algo-
rithms. ArXiv e-prints, abs/1705.08131 (2017)
8. Huang, L., Joseph, A.D., Nelson, B., Rubinstein, B.I.P., Tygar, J.D.: Adversarial
machine learning. In: ACM Workshop on Security and Artiﬁcial Intelligence (2011)
9. Huang, W., Stokes, J.W.: MtNet: a multi-task neural network for dynamic malware
classiﬁcation. In: Caballero, J., Zurutuza, U., Rodr´ıguez, R.J. (eds.) DIMVA 2016.
LNCS, vol. 9721, pp. 399–418. Springer, Cham (2016). https://doi.org/10.1007/
978-3-319-40667-1 20
10. Papernot, N., McDaniel, P., Jha, S.H., Fredrikson, M., Celik, Z.B., Swami, A.: The
limitations of deep learning in adversarial settings. In: IEEE European Symposium
on Security and Privacy (2016)
11. Papernot, N., McDaniel, P., Goodfellow, I., Jha, S., Celik, Z.B., Swami, A.: Prac-
tical black-box attacks against machine learning. In: ASIA CCS (2017)
510
I. Rosenberg et al.
12. Papernot, N., McDaniel, P., Swami, A., Harang, R.: Crafting adversarial input
sequences for recurrent neural networks. In: IEEE MILCOM (2016)
13. Pascanu, R., Stokes, J.W., Sanossian, H., Marinescu, M., Thomas, A.: Malware
classiﬁcation with recurrent networks. In: IEEE ICASSP (2015)
14. Rieck, K., Trinius, P., Willems, C., Holz, T.: Automatic analysis of malware behav-
ior using machine learning. J. Comput. Secur. 19, 639–668 (2011)
15. Rosenberg, I., Gudes, E.: Bypassing system calls-based intrusion detection systems.
Concurr. Comput.: Pract. Exp. (2016)
16. Rosenberg, I., Sicard, G., David, E.O.: DeepAPT: nation-state APT attribution
using end-to-end deep neural networks. In: Lintas, A., Rovetta, S., Verschure,
P.F.M.J., Villa, A.E.P. (eds.) ICANN 2017. LNCS, vol. 10614, pp. 91–99. Springer,
Cham (2017). https://doi.org/10.1007/978-3-319-68612-7 11
17. Rosenberg, I., Shabtai, A., Rokach, L., Elovici, Y.: Low resource black-box end-
to-end attack against state of the art API call based malware classiﬁers, arXiv
preprint arXiv:1804.08778 (2018)
18. Szegedy, C., et al.: Intriguing properties of neural networks. In: ICLR (2014)
19. Tandon, G., Chan, P.K.: On the learning of system call attributes for host-based
anomaly detection. Int. J. Artif. Intell. Tools 15, 875–892 (2006)
20. Trinius, P., Willems, C., Holz, T., Rieck, K.: A malware instruction set for behavior-
based analysis. In: Sicherheit (2010)
21. Wagner, D., Soto, P.: Mimicry attacks on host-based intrusion detection systems.
In: ACM CCS (2002)