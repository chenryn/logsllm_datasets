 **System information**
  * **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)** :no
  * **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)** :Ubuntu 16.04
  * **TensorFlow installed from (source or binary)** :Source
  * **TensorFlow version (use command below)** :1.8.0rc0
  * **Python version** :2.7.12
  * **Bazel version (if compiling from source)** :0.12.0
  * **GCC/Compiler version (if compiling from source)** :5.4.0
  * **CUDA/cuDNN version** :cuda-9.0/7.0
  * **GPU model and memory** :GeForce GTX 1080/8105MiB
  * **Phone** :xiaomi5 (Snapdragon 820)
  * **Exact command to reproduce** :  
bazel run --config=opt //tensorflow/contrib/lite/toco:toco --  
\--input_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/frozen_inference_graph.pb  
\--output_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/kanul.tflite  
\--inference_type=QUANTIZED_UINT8  
\--input_shape=1,513,513,3  
\--input_array=sub_7  
\--output_array=logits/semantic/BiasAdd
**Describe the problem**  
I have tried to quantize MobileNetV2 for deeplabV3+ with TFlite. But I fail to
convert the model.  
From the following issue, I saw that the operations were not supported for the
option of quantization.
https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md  
Checkpoint name: mobilenetv2_coco_voc_trainaug
Who can explain and support to resolve the issue?
**Source code / logs**  
bazel run --config=opt //tensorflow/contrib/lite/toco:toco --  
\--input_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/frozen_inference_graph.pb  
\--output_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/kanul.tflite  
\--inference_type=QUANTIZED_UINT8  
\--input_shape=1,513,513,3  
\--input_array=sub_7  
\--output_array=logits/semantic/BiasAdd
Unimplemented: this graph contains an operator of type SpaceToBatchND for
which the quantized form is not yet implemented. Sorry, and patches welcome
(that's a relatively fun patch to write, mostly providing the actual quantized
arithmetic code for this op).