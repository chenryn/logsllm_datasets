4
.
0
2
.
0
0
.
0
0
.
1
8
.
0
6
.
0
4
.
0
2
.
0
0
.
0
)
x
(
n
F
nf−mpe 10%
nf−mpe 1%
rli
avgdelay
lds 1750x4
lds 17500x4
lds 175000x4
0
.
1
8
.
0
6
.
0
4
.
0
2
.
0
0
.
0
)
x
(
n
F
nf−mpe 10%
nf−mpe 1%
rli
avgdelay
lds 1750x4
lds 17500x4
lds 175000x4
nf−mpe 10%
nf−mpe 1%
rli
avgdelay
lds 1750x4
lds 17500x4
lds 175000x4
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
rel. error
rel. error
rel. error
Figure 9: CDF of the relative error of various measurement methods for ﬂows with > 1000 pkts. (left), with
> 100 pkts (center) and all ﬂows (right).
RLI
NFMPE (10%)
NFMPE (1%)
1750x4
17500x4
175000x4
1−99
100−499
500−999
1000−9999
10000+
=10000
r
o
r
r
e
e
v
i
t
l
a
e
r
r
o
r
r
e
e
v
i
t
l
a
e
r
2
.
1
8
.
0
4
.
0
0
.
0
0
.
1
8
.
0
6
.
0
4
.
0
2
.
0
0
.
0
1.2
Figure 10: Median and 95-pct of relative error, bin-
ning ﬂows by number of packets (top) and number
of packets and average delay (relative to avg. ﬂow
delay; bottom).
RLI, which requires even more memory than MPE, since it
maintains per-ﬂow state.
Figure 10 (top) plots the median and 95-percentile of the
relative error of ﬂows binned by size. Consistently with the
analysis of Sec. 3, the ﬁgure shows that larger ﬂows are more
accurately measured. Our method compares extremely fa-
vorably to both MPE and RLI. Only with 7,000 counters we
obtain signiﬁcant improvements for ﬂows larger than 1,000
packets. When using 70,000 counters, which take about
1MB, ﬂows with 500 to 999 packets obtain 1.2% median
relative error, which falls to 0.4% and 0.06% for the larger
size bins.
Figure 10 (bottom) bins ﬂows both by size and average
delay with a ﬁxed sketch size of 70,000 counters. The ﬁgure
shows how the errors are slightly larger as delay deviates in
extreme values for the mean. This happens because smaller
ﬂows show more extreme values, but interferences tend to
drag measurements toward the mean.
5.2 Measurement under Packet Loss
We now analyze the eﬀect of packet loss to our data struc-
ture. Under small loss rates, it is desirable to keep the vLDA
length parameter k small, since increasing it increments the
number of collisions in the sketch. However, increasing k
provides higher protection against loss. Additionally, sam-
pling helps contain loss, since a large number of losses in a
single ﬂow can potentially invalidate the k counters.
We wish to dimension our data structure to support a
given maximum number of losses per ﬂow. The main intu-
ition behind this approach is that, when ﬂows experience a
large amount of losses, performance degradation is more a
consequence of loss than delay; thus, delay measurements
cease to be meaningful (note that LDS can be mined to es-
timate per-ﬂow loss, as explained in Sec. 4.3).
In this experiment, we arbitrarily set a target number of
losses of 500 packets per ﬂow. However, we also wish the
LDS to be able to capture a large sample size if losses are
much lower. Thus, we use a multi-bank conﬁguration of
LDS, as described in Sec. 4.2. We provide 4 rows with k = 5
vLDA buckets, like in the previous scenario, increase the size
of each row of the sketch by a factor of k, and set α = 0.1.
We then pick suitable packet sampling rates for each row.
With k = 5, each vLDA cell needs to support 100 losses,
according to our target number of losses. This means that
the sampling rate should be set to 0.01 to support this worst-
case loss. Then, we wish the rest of the banks to tolerate
lower loss in order not to sacriﬁce the accuracy of LDS in the
normal case. We set the rest of the banks with increasing
sampling rates of 0.1, 0.5 and 1 to tolerate up to 50, 10 and
5 losses respectively. For comparative purposes, we also set
two LDS with a ﬁxed sampling rate in all rows of 0.05 and
1.
Since, in our scenario, losses are negligible, we introduce
random, uniform loss to test such conﬁgurations. Consis-
tently with the assumptions made in Sec. 2.1, we perform 3
series of experiments with loss rates 0.1%, 0.5% and 1%. It
should be noted that uniform loss is one of the most harmful
loss models to LDS, for three main causes. First, losses are
spread among a large number of ﬂows, instead of being con-
tained within a few. Second, the absolute number of losses
that hit each bucket heavily varies according to the lengths
of the involved ﬂows. Recall from Sec. 2.3 that the optimal
sampling rate for each bucket depends on its absolute num-
ber of losses. Third, this loss model penalizes large ﬂows,
which are precisely those that our method can measure most
accurately.
Figure 11 shows the results we obtained. We start by not-
ing that neither 5% nor 100% sampling single-bank LDSs
perform satisfactorily. The former maintains its accuracy
under higher loss, but is too conservative and underperforms
on low loss. Conversely, 100% sampling is too optimistic and
does not oﬀer protection against loss. Thus, its cells become
493loss = 0.001
loss = 0.005
loss = 0.01
)
x
(
n
F
0
.
1
8
.
0
6
.
0
4
.
0
2
.
0
0
.
0
)
x
(
n
F
0
.
1
8
.
0
6
.
0
4
.
0
2
.
0
0
.
0
mbLDS 1.0/0.5/0.1/0.01
LDS 1.0
LDS 0.05
)
x
(
n
F
0
.
1
8
.
0
6
.
0
4
.
0
2
.
0
0
.
0
mbLDS 1.0/0.5/0.1/0.01
LDS 1.0
LDS 0.05
mbLDS 1.0/0.5/0.1/0.01
LDS 1.0
LDS 0.05