application of our technique may not be initially obvious.
European police estimate that over half of the works in
international markets are forgeries [19]. One family of art
forgers was able to make $2 million before they were
caught. The ability of art forgers to reproduce the individual
brush strokes of a work makes authenticating paintings
increasingly difﬁcult. In the best forgeries, art veriﬁers must
sometimes rely on the chain of custody of the work in order
to authenticate it [20]. However, we believe that it would
be difﬁcult to duplicate features of the canvas (down to the
detailed arrangement of the weave) upon which the work is
painted. Thus art authenticity or forgery might be detectable
by applying a technique like ours to the canvas, most probably
on the back side of the painting.
Lottery tickets are similar to currency except that players
need not be aware of a ﬁngerprinting technique at all. In
order for a lottery winner to collect on their winnings, the
ticket must be veriﬁed by the lottery authority. The ﬁngerprint
of a winning ticket need not be printed on the document at
all. Fingerprints of all possible winning lottery tickets can
be privately maintained, and any claimants can be required
to produce the actual winning ticket, with correctly veriﬁed
ﬁngerprint, in order to collect their winnings.
The accurate identiﬁcation of paper based product pack-
aging could beneﬁt from this technique as well. When
inspecting cargo, customs ofﬁcials often inspect the contents
of packages to weed out counterfeit goods. We can increase
conﬁdence in package contents by authenticating a product’s
packaging. If the packaging of a product is legitimate, then
the contents of the package have a much higher likelihood
of being authentic.
8. Privacy Implications
The feasibility of paper-based authentication demonstrates
that some undesirable attacks are possible. Because our
results do not modify the paper in any way, there is no way to
detect, by inspecting a piece of paper, whether its ﬁngerprint
might have been recorded in advance by an adversary. This
fact violates the traditional assumption that pieces of paper
cannot easily be traced without the addition of distinguishing
marks. Even unopened sheaves of blank printer paper might in
principle have been ﬁngerprinted at the factory. Applications
such as paper-based voting, in which the secrecy of individual
ballots is important, are challenged by our results.
For example, consider an optical-scan voting system in
which voters ﬁll out paper ballots. In such a system, the
secrecy of ballots relies on the assumption that individual
paper ballots are indistinguishable. Our work shows that this
assumption may not be valid.
A corrupt ofﬁcial could scan the blank ballots in advance
and record a unique ﬁngerprint for each ballot in the stack.
If ballots are given out to voters in a predictable order (e.g.,
from the top of the stack down) and the order of voters is
recorded, as it is in many polling places, or observable by the
attacker, then ballots can be re-identiﬁed after the election.
Even worse, because pre-scanning leaves no evidence on
the ballots themselves, a mere rumor that someone might
have scanned the ballots in advance would be very difﬁcult
to disprove, and such a rumor would make coercion and
vote-buying more credible.
More generally, the ability to re-identify ordinary sheets
of paper casts doubt on any purportedly private information
gathering process that relies on paper forms. “Anonymous”
surveys or reporting systems may not in fact be anonymous.
11
Though it has long been possible to track sheets of
paper using subtle chemical markers or “invisible ink,” these
methods require some level of special expertise, and the
presence of markers leaves evidence of the attack. Our
research shows that an attacker armed with only ordinary
equipment—a commodity scanner—is able to re-identify
paper reliably without leaving any telltale marks.
9. Conclusion and Future Work
Our work shows that ordinary pieces of paper can be
ﬁngerprinted and later identiﬁed using commodity desktop
scanners. The technique we developed functions like a
“biometric” for paper and allows original documents to be
securely and reliably distinguished from copies or forgeries.
At least two questions remain to be answered in future
work. First, in the threat model where the adversary has
access to the original document and the ﬁngerprint, we do
not know for certain that a clever adversary cannot forge
a copy of the document with a high-resolution printer. Our
initial work could not determine conclusively whether an
adversary who can use a good printer will have enough
degrees of freedom in modifying a document to make the
document match a known ﬁngerprint. Second, while we
conjecture that our method can be applied to other materials
such as fabric, more testing is needed to verify this, and
special methods might be needed for some materials. We
leave both of these questions for future work.
Our results are a tribute to the resolution of today’s
scanners. Future scanners will capture ever more detailed
images of paper documents, eventually allowing individual
wood ﬁbers to be imaged clearly. The security of our methods
against forgery, in cases where the adversary has full access
to information, will depend ultimately on a race between the
resolution of the printers that can create patterns on a page,
and the resolution of the scanners that can observe patterns.
10. Acknowledgments
We thank Andrew Appel, Victoria Hill, Andrew Moore, N.
J. A. Sloane, Joshua R. Smith, and the anonymous reviewers
for their invaluable suggestions and assistance.
References
[1] J. D. R. Buchanan, R. P. Cowburn, A.-V. Jausovec, D. Petit,
P. Seem, G. Xiong, D. Atkinson, K. Fenton, D. A. Allwood,
and M. T. Bryan, “Forgery: ‘ﬁngerprinting’ documents and
packaging,” Nature, vol. 436, p. 475, 2005.
[2] B. Zhu, J. Wu, and M. S. Kankanhalli, “Print signatures for
document authentication,” in Proc. 10th ACM Conference on
Computer and Communications Security, 2003, pp. 145–154.
[3] E. Metois, P. Yarin, N. Salzman, and J. R. Smith, “FiberFin-
gerprint identiﬁcation,” in Proc. 3rd Workshop on Automatic
Identiﬁcation, 2002, pp. 147–154.
[4] R. P. Cowburn and J. D. R. Buchanan, “Veriﬁcation of
authenticity,” US patent application 2007/0028093, Jul. 2006.
[5] R. Woodham, “Photometric stereo: A reﬂectance map tech-
nique for determining surface orientation from image intesity,”
in Proc. 22nd SPIE Annual Technical Symposium, vol. 155,
1978, pp. 136–143.
[6] B. Brown, C. Toler-Franklin, D. Nehab, M. Burns, A. Vla-
chopoulos, C. Doumas, D. Dobkin, S. Rusinkiewicz, and
T. Weyrich, “A system for high-volume acquisition and
matching of fresco fragments: Reassembling Theran wall
paintings,” ACM Trans. Graphics (Proc. SIGGRAPH 2008),
p. 84 (9 pp.), Aug. 2008.
[7] A. Okabe, B. Boots, K. Sugihara, and S. N. Chiu, Spatial
Tesselations: Concepts and Applications of Voronoi Diagrams.
Wiley, 2000.
[8] J. E. Jackson, A User’s Guide to Principal Component Analysis.
Wiley-Interscience, 2003.
[9] Y. Dodis, R. Ostrovsky, L. Reyzin, and A. Smith, “Fuzzy
extractors: How to generate strong keys from biometrics and
other noisy data,” SIAM Journal on Computing, vol. 38, no. 1,
pp. 97–137, 2008.
[10] A. Juels and M. Wattenberg, “A fuzzy commitment scheme,” in
Proc. 6th ACM Conference on Computer and Communications
Security, 1999, pp. 28–36.
[11] R. M. Neal. (2006, Feb.) Software for low density parity
check codes. [Online]. Available: http://www.cs.utoronto.ca/
∼radford/ldpc.software.html
[12] D. MacKay and R. Neal, “Near shannon limit performance of
low density parity check codes,” Electronics Letters, vol. 33,
no. 6, pp. 457–458, Mar. 1997.
[13] C. Sorzano, P. Thevenaz, and M. Unser, “Elastic registration
of biological images using vector-spline regularization,” IEEE
Trans. Biomedical Engineering, vol. 52, no. 4, pp. 652–663,
Apr. 2005.
[14] D. McNicol, A Primer on Signal Detection Theory. Lawrence
Erlbaum Assoc., 2004.
[15] L. S. Amine and P. Magnusson, “Cost-beneﬁt models of stake-
holders in the global counterfeiting industry and marketing
response strategies,” Multinational Business Review, vol. 15,
no. 2, pp. 1–23, 2007.
[16] U.S. Department of Commerce. Top 10 ways to protect
yourself from counterfeiting and piracy. [Online]. Available:
http://www.stopfakes.gov/pdf/Consumer Tips.pdf
[17] C. Balmer and K. Wills, “Beijing games hit by internet ticket
scam,” Reuters, Aug. 4, 2008.
[18] “Ticket site closed on fraud fears,” BBC News, Oct. 21, 2008.
12
[19] J. L. Shreeve, “Art forgers: What lies beneath,” The Indepen-
dent, Sep. 3, 2008.
[20] R. D. Spencer, The Expert versus the Object: Judging Fakes
and False Attributions in the Visual Arts. Oxford University
Press, 2004.
[21] F. A. P. Petitcolas, R. J. Anderson, and M. G. Kuhn,
“Information hiding—a survey,” Proc. IEEE, vol. 87, no. 7,
pp. 1062–1078, Jul. 1999.
Appendix
Section 3 introduces a process for ﬁngerprinting and
verifying the ﬁngerprint of a document. In this appendix
we brieﬂy outline some alternative strategies that might be
desirable under different criteria for robustness or different
levels of concern about forgery.
Using albedo versus normals. Because the high-resolution
paper scans shown in ﬁgures throughout this paper reveal
obvious color variation in addition to surface texture, perhaps
a more straightforward approach would be to use the albedo
(color) of the page as the basis for a ﬁngerprint, rather than, or
in addition to, the shape. Indeed, our initial implementations
explored this approach, using a single scan (which combines
albedo and normal information) to construct the ﬁngerprint
of a document. This approach is simpler and offers the
substantial beneﬁt that the document can be ﬁngerprinted or
veriﬁed more quickly, through a single scan.
The intensities of most of the pixels in a scanned page are
modeled well by a truncated normal distribution, centered
around the “white” color. To use this data as the basis
for a ﬁngerprint, we simply construct the vector p as the
concatenation of these intensities from a given patch. For
example, an 8× 8 patch would yield a vector p ∈ IR64. The
ﬁngerprint is then extracted from a collection of patches as
described in Algorithm 1.
We did not pursue this approach because we believe this
form of ﬁngerprint may not resist forgers who use very
light ink to print a desired pattern on the page. Another
drawback is that any black ink on the page, which lies well
outside the roughly-normal distribution of intensities found in
blank paper, contributes to a very strong negative value in p,
introducing a bias in the dot products for the patch. Thus, any
value outside the range of the truncated normal distribution
must be zeroed out before constructing the ﬁngerprint. This
provides another opportunity for a forger to deliberately zero
out regions of the patch with the goal of ﬂipping bits towards
a desired ﬁngerprint. These attacks might be difﬁcult to carry
out in practice, since they require excellent registration in the
printing process. Therefore, albedo-based ﬁngerprints may
be suitable for applications where some added risk of forgery
is an acceptable tradeoff for increased speed and simplicity.
Patch-pair comparisons. Recall from Algorithm 1 that the
vector p contributes K bits to the overall ﬁngerprint by taking
the signs of the dot product of p and a series of ortho-normal
template vectors. We have also considered (and implemented)
an alternate version of the algorithm where the bits of the
ﬁngerprint are taken to be the signs of the dot products of
pairs of patches p and q. The na¨ıve version divides the pool
of patch positions into pairs and computes one bit of the
feature vector from each pair. Unfortunately that approach
allows an attacker to tweak each pair in turn independently.
A more robust version considers bits from all patch pairs
(p,q) where p (cid:54)= q. For example, for 64 patches each patch
would participate in 63 bits, and this scheme could generate
(cid:1) = 2016 total bits.
(cid:0)64
2
In the case where a forger has a copy of an original
document and therefore knows the ﬁngerprint he is trying to
reproduce (Section 5.2.3), this formulation has the advantage
that the bits of the ﬁngerprint are more tightly bound than
those of the template vectors. Any attack on a single bit—for
example, printing on a patch—is likely to impinge on the
other (62) bits affected by that patch. Thus, a forger would
have to solve an optimization problem to ﬁgure out how best
to perform the attack.
However, the bits of the ﬁngerprint generated from all patch
pairs seem to be less independent than the bits generated
by the template vectors. Preliminary experiments similar to
those described in Section 4 indicate that “all-pairs” bits are
mostly independent, but not as independent as the “template”
bits. Since the arguments in Section 5.2.2 for security against
“blind” attackers rely on bit independence, we generally prefer
the “template” scheme.
Short ﬁngerprints with no error-correcting information.
Section 3.2 describes a process for generating ﬁngerprints
composed of a hash of 3200 or more bits concatenated
with some error correction bits. For some applications
requiring less security, fewer feature vector bits may be used.
Suppose only 100 bits are used, and further suppose that the
application tends to produce fewer bit errors (say 15% or less).
In such scenarios an alternate approach would be to simply
record the secure hash of those bits. An attacker, without
the beneﬁt of the original, is forced to guess among 2100 bit
sequences, checking guesses against the hash. Unfortunately,
this leaves the na¨ıve authentication process with no way to
do error correction other than to guess among the roughly
1017 strings within Hamming distance of 15 of the sequence
extracted from a page—easier, but also daunting.
Fortunately, there is a better approach for the authentication
process. Recall that the bits of the ﬁngerprint are taken as
the signs of a series of dot products (patches and templates).
We have observed that these dot products are well-modeled
by samples from a truncated normal distribution. Moreover,
we have also observed that the ﬂipped bits mostly come
from dot products near zero, and that the bit-ﬂipping process
13
seems to be well-modeled by the addition of “noise” also
selected from a truncated normal distribution (with smaller
standard deviation than that of the “signal”). With this model
in hand, the veriﬁcation process can search for bit strings
similar to the extracted ﬁngerprint while taking into account
which bits are more likely to have ﬂipped. Speciﬁcally, the
process ﬂips bits at random with probability relative to the
likelihood that the bit has ﬂipped, each time checking against
the secure hash. We simulated this approach and found that
about 90% of the time it will ﬁnd the correct string within
106 guesses for the example distribution described above.
The beneﬁts of this approach are that it is simple to
implement and provides no information to an attacker in
the form of error-correction bits. The main disadvantages
are that it does not scale well to longer bit sequences and
that the stochastic nature of the algorithm provides only
probabilistic guarantees of running time. Therefore, it would
likely be used only in conjunction with other approaches.
For example, an application might attempt this method for
ofﬂine veriﬁcation and fall back to an online method in cases
when it fails.
14