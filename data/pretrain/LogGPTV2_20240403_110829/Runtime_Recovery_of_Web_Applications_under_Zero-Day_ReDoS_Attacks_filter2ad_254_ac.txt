Speciﬁcally, a web request r is represented as an array of
characters, i.e., r = {ci|ci ∈ C} where C is the set of
characters. The DNN model f is applied to r to compute a
label in {0, 1}, i.e., f (r) ∈ {0, 1}, where label 0 means benign
and label 1 means malicious.
Our DNN architecture has four layers, one embedding layer,
one 1D convolutional (conv1d) layer, one spatial pyramid
pooling (SPP) layer, and a fully-connected layer. When a web
request comes in, the characters are fed into the embedding
layer at the character level. The embedding layer generates
an embedded vector for each character. Then the conv1d
layer takes these vectors as input and performs convolution
operations on them. The SPP layer converts the output from
the conv1d layer into ﬁxed length with max pooling. Finally,
the fully-connected layer outputs a result, i.e., either benign
or malicious, for the web request.
We adopt this speciﬁc DNN architecture due to the follow-
ing reasons. First, we choose an embedding layer, because
it can turn the input characters into dense vectors, which is
essential to the convergence of the DNN. An advantage of
the embedding layer is that it is able to encode distances
between characters and show their similarity. For example,
the distance between the letter “0” and “1” should be smaller
than that of “0” and “a”, because both “0” and “1” could
be matched by the symbol “\d” in regular expression which
stands for all the digits. Thus, such property of the embedding
layer is important. Second, we choose a conv1d layer, because
a conv1d layer is able to extract local information from the
sequence of characters, and it is also computationally efﬁcient.
The conv1d layer is followed by tanh as a common practice.
Third, we choose an SPP layer that draws the feature maps
from the output of the convolutional layer only once, and
then pools the features in arbitrary regions to generate ﬁxed-
length representations for the fully-connected layer. Both the
convolutional layer and the SPP layer are able to process data
in O(n) time where n is the length of the data. Lastly, we
choose a fully-connected layer that allows information to ﬂow
between units extracted by the SPP layer, thus being able to
capture useful patterns from a global perspective. Note that
we choose SPP instead of recurrent neural networks (RNNs),
because SPP is more computation-efﬁcient.
Model training and update. REGEXNET provides both an
ofﬂine-trained model and an online update procedure. The
ofﬂine training is used to bootstrap the DNN model, and
the online training is used to reﬁne the model with real-time
measurements to adapt to both known and unknown attacks.
Note that ofﬂine training is optional. REGEXNET can be
deployed without ofﬂine training, and only use online training
to learn the attack patterns and react to ReDoS attacks. Ofﬂine
training is useful to reduce the reaction time to known attacks.
The ofﬂine training component uses a training dataset to
train the initial DNN model. The training dataset could be
collected from previous attacks or generated based on the
analysis for known vulnerabilities. It contains a set of web
requests R = {ri} and their labels L = {li}, which indi-
(cid:80)
cate whether each request is malicious or not. We use the
j exp(f (ri)[lj ]) )) to maximize
the classiﬁcation accuracy. We use the common mini-batch
gradient descent method to train the model. A challenge in
training a model for ReDoS detection is that the datasets
are usually imbalanced, i.e., there are more benign requests
than malicious requests in the datasets. We sample the benign
requests with random undersampling to make the datasets
more balanced, in order to achieve high detection accuracy.
cross entropy loss (cid:80)
i(− log(
exp(f (ri)[li])
The online training component continuously reﬁnes the
model to detect unknown attacks. There are two steps. First,
the component builds the training data with real-time mea-
surements collected from the web servers. The collector, i.e.,
the shim layer in each server instance, tracks the response
time of each web request, and compares the response time
with a pre-deﬁned threshold. If the response time is above
the threshold, REGEXNET considers the request as malicious
because it consumes excessive CPU resources. The shim
layer immediately reports the malicious request to the online
training component, in order to quickly react to the attack. The
collector also tracks the response time of each request in the
sandbox. If the execution of a request in the sandbox consumes
less time than the threshold, the collector also reports the
request to the online training component, because the request
is misclassiﬁed as malicious.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:10:14 UTC from IEEE Xplore.  Restrictions apply. 
1579
Second, the online training component adopts a hot-start
procedure to update the DNN model to include the newly
collected data. Speciﬁcally, the component starts from pre-
viously converged weights and adds newly collected data into
the backward propagation. The advantage of such a hot-start
procedure is that the model is almost converged, and thus the
convergence speed is much faster.
Threshold selection. REGEXNET adopts a threshold in the
online training component for the feedback loop. The threshold
is either speciﬁed by an administrator or determined based on
the statistics of benign requests during the testing phase of the
target web application. Speciﬁcally, REGEXNET calculates the
average processing time of benign requests µ and the standard
deviation σ. Then, REGEXNET adopts µ+3σ as the threshold,
i.e., the requests of which the processing time is above µ + 3σ
are fed back to the online training component.
There are two things worth noting here. First, malicious
requests that are executed in less than µ + 3σ time will not
trigger the online feedback loop. Such requests will not slow
down the web application much because their execution time is
relatively small. Additionally, if the attacker chooses to send a
large number of such requests, the detection will default back
to the traditional network-layer, volume-based DoS detection.
Second, the chosen threshold may also introduce false posi-
tives of our DNN-based detection module, e.g., feeding benign
requests with a large ﬁle upload to the online feedback loop as
potentially malicious requests. However, these requests are not
dropped. Instead, as shown in Section III-D, they are migrated
to the sandbox, and are still executed, albeit slower due to
the limited resources in the sandbox. The mislabelling can be
ﬁxed by the operator by resetting the threshold, correcting the
mislabeled data and re-training the DNN model.
D. Request Migration
The migration module migrates potentially malicious re-
quests to sandboxes. The detailed procedure is as follows.
First, the migration module receives the IP address of the
scheduled server instance from the load balancer for each
web request. The migration module uses the IP address of
the scheduled server instance to signal its shim layer. Second,
when the shim layer receives the signal, it starts migrating
the web request
to a sandbox. We use a primary-backup
approach to enable the shim layer to stop a web request during
processing and be compatible with any web server software.
Speciﬁcally, we run two instances in each web server: one
instance is the primary, and the other one is the backup. The
shim layer buffers the web requests in a queue, and sends small
batches of requests to the primary for processing. When the
shim layer receives a notiﬁcation from the migration module,
it checks whether the request has been sent to the primary or
not. If not, the shim layer can simply remove the request from
the buffer. Otherwise, the shim layer kills the primary, and
makes the backup as the new primary. It resends the previous
batch of requests, except for the malicious one, to the new
primary for processing, and starts a new backup. At the same
time, the malicious request is sent to a sandbox to limit its
impact on normal requests. For stateful request processing,
we leverage the transaction processing of the database to
ensure transactional semantics when killing and restarting
requests. Many applications support fault recovery or seamless
restart. For example, Zero Downtime Release [24] keeps the
partial states in the execution of HTTP requests during restart
and replays these requests on other servers. REGEXNET can
incorporate such solutions to restart requests smoothly.
E. Scalability and Fault Tolerance
REGEXNET does not have a single point of
REGEXNET can easily scale out with more servers to handle
more web trafﬁc. The shim layer runs on each web server,
and scales out with the number of web servers. The number
of sandboxes is decided by the server operator, based on how
much resource the operator wants to allocate for malicious
requests. The detection module and migration module work
together to detect and mitigate ReDoS attacks. One instance
of the detection module and one instance of the migration
module should run together on the server to minimize the
communication overhead between the two, but multiple pairs
of these instances can run independently on several servers to
handle more trafﬁc, as different pairs do not need to coordinate
with each other. The training can be done in one instance of
the training module, and the trained model can be pushed to
all instances of the detection module to update their models.
failure.
REGEXNET handles its individual component failures as fol-
lows. (i) Detection module. The detection module does not
maintain any state. When an instance of the detection module
fails, it can be easily replaced by a new instance (e.g., a new
server or VM). The new instance gets the latest model from the
training module and then begins to classify requests received
from the load balancer. (ii) Migration module. Similar to the
detection module, the migration module does not keep any
hard state either. An instance failure of the migration module
can be handled by using a new instance. (iii) Training module.
REGEXNET stores the training data and the trained model in
a reliable distributed storage such as HDFS [25]. When the
instance of the training module fails, REGEXNET replaces it
with a new instance. The new instance restarts the training
if the old instance fails in the middle of the training before
the model converges. Otherwise, the new instance simply waits
for new training data from the shim layer to retrain the model.
(iv) Shim layer, web server and sandbox. The shim layer, web
servers and sandboxes process requests, and they naturally
scale out. Their failures can be handled by replacing them
with new instances.
IV. IMPLEMENTATION
We have implemented a system prototype of REGEXNET
with a total of ∼2,000 lines of code. The code is open source
and available at https://github.com/netx-repo/RegexNet.
To demonstrate the practicality of REGEXNET in real-world
deployments, the prototype is integrated with HAProxy [22],
a widely-used open-source software load balancer, and
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:10:14 UTC from IEEE Xplore.  Restrictions apply. 
1580
A SECURITY ANALYSIS OF REGEXNET AGAINST A VARIETY OF REDOS VULNERABILITIES.
TABLE I
CVE ID
Module
CVE-2017-15010
CVE-2016-4055
CVE-2015-8858
CVE-2015-8854
CVE-2015-8315
tough-cookie
moment
uglify-js
marked
ms
N/A
N/A
N/A
N/A
N/A
N/A
N/A
N/A
charset
content
fresh
forwarded
mobile-detect
platform
ua-parser-js
useragent
Version
<2.3.3
<2.11.2
<2.6.0
<0.3.4
<0.7.1
<1.0.0
<3.0.5
<0.5.0
<0.1.0
<1.3.6
<1.3.4
<0.7.14
<2.2.1
Vulnerable Regular Expression
Recoverable with REGEXNET
/ˆ(([ˆ=;]+))\s*=\s*([ˆ\n\r\0]*)/
/(\-)?(?:(\d*)[. ])?(\d+)\:(\d+)(?:\:(\d+)\.?(\d{3})?)?/
/ˆ\d*\.?\d*(?:e[+-]?\d*(?:\d\.?|\.?\d)\d*)?$/i
/ˆ\b_((?:__|[\s\S])+?)_\b|ˆ\*((?:\*\*|[\s\S])+?)\*(?!\*)/
/ˆ((?:\d+)?\.?\d+) *(milliseconds?|msecs?|ms|seconds?|secs?|s
|minutes?|mins?|m|hours?|hrs?|h|days?|d|years?|yrs?|y)?$/
/(?:charset|encoding)\s*=\s*[’"]? *([\w\-]+)/i
/ˆ([ˆ\/]+\/[ˆ\s;]+)(?:(?:\s*;\s*boundary=(?:"([ˆ"]+)"|([ˆ;"]+))) 
|(?:\s*;\s*[ˆ=]+=(?:(?:"(?:[ˆ"]+)")|(?:[ˆ;"]+))))*$/i
/ *, */
/ *, */
/Dell.*Streak|Dell.* Aero|Dell.*Venue|DELL.*Venue Pro|Dell Flash|Dell Smoke
|Dell Mini 3iX|XCD28|XCD35|\\b001DL\\b|\\b101DL\\b|\\bGS01\\b/

/ˆ +| +$/g

/ip[honead]+(?:.*os\s([\w]+)*\slike\smac|;\sopera)/
/((?:[A-z0-9]+|[A-z\-]+?)?(?:the)?(?:[Ss][Pp][Ii][Dd][Ee][Rr]|[Ss]crape|[A-Za-z0-9-]*(?:[ˆC] 
[ˆUu])[Bb]ot|[Cc][Rr][Aa][Ww][Ll])[A-z0-9]*)(?:(?:[\/]|v)(\d+)(?:\.(\d+)(?:\.(\d+))?)?)?/








