TrustVisor, and using it to run various application.
Experimental setting. We use a Dell PowerEdge R420 Server
equipped with: a 2.2GHz Intel Xeon E5-2407 CPU; 16GB
of DDR3 memory; a TPM v1.2; a primary 300GB, 15Krpm
hard-disk; a secondary 2TB, 7.2Krpm hard-disk. The server
runs Ubuntu 12.04 32-bit with a Linux kernel 3.2.0-27. The
resources are fully dedicated to our experiments. LASTGT uses
the secondary disk to ensure uniform experimental conditions.
Data is organized in chunks of 128MB and blocks of
256KB. Our micro-benchmarks justify these values for se-
quential workloads (typical of data analytics) and suggest opti-
mizations for random workloads (§7.5). We assume the worst-
case scenario that requires LASTGT to maintain a low memory
footprint. Hence, we set up the SMM to reclaim old chunk
maps when the service code tries to access a new one.
7.1 TCB Size
We quantify LASTGT’s TCB size using the lines of source
code metric (SLoC), as calculated by the SLOCCount tool [32]
and compare it with previous work (Tab. II). At the hypervisor
level, the TCB size increases by 12%. The LASTGT library
adds an additional 7.7 SLoC of user-level code, which is
it
relatively small. For example,
increases the size of the
SQLite service code by only 8.3%.
The table also compares the TCBs of LASTGT, VC3
and Haven. Haven’s TCB is notably large due to the li-
brary OS. LASTGT’s TCB is larger than that of VC3. However,
LASTGT is not application-speciﬁc and can run generic self-
contained applications.
We expect LASTGT’s SGX implementation to have a
smaller TCB than the current one. As SGX keeps privileged
code out of the enclave, the hypervisor functionality—to man-
age the VMs, protect the isolated memory from the untrusted
OS, and schedule the execution of trusted and untrusted
177
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:21:49 UTC from IEEE Xplore.  Restrictions apply. 
s
d
n
o
c
e
s
12
10
8
6
4
2
0
0
MB
begin reading
transfer data backback
load state from disk,
encode it in memory,
transfer it in secure memory,
hash it
load chunk from disk
256
MB
128
MB
end reading
transfer block in secure memory,
validate and read
valida
TrustVisor
LaSt-GT
384
MB
512
MB
Fig. 10: Comparison between LASTGT and XMHF-TrustVisor.
applications—will be moved out of the TCB and implemented
in untrusted code, while retaining similar security guarantees.
7.2 Comparing LASTGT and TrustVisor
As a baseline experiment, we compare LASTGT with the
original TrustVisor implementation. As displayed in Fig. 10,
LASTGT can be much faster than TrustVisor depending on how
the applications access memory. The experiment uses a 512MB
dataset that ﬁts in memory; TrustVisor cannot scale to larger
sizes. The data is read sequentially, allowing a comparison of
the associated overheads according to the memory accesses.
TrustVisor always exhibits a large startup time (dependent on
the data size) as it reads everything upfront into memory. In
contrast, LASTGT exhibits a performance that is related to the
parts of memory that are actually read/written thanks to its
ability to do incremental data loading and validation. So, in
an execution that only touches half of the dataset, TrustVisor
would roughly end up taking twice as much time as LASTGT.
7.3 Micro-benchmarks
LASTGT incurs overhead when it needs access to addi-
tional data through page faults. The primary sources of over-
head include switching control between software components,
(un)loading maps in isolated memory, and disk accesses by the
SMM. Since the latter is the same for trusted and untrusted
executions, we focus on the quantifying the overhead of the
ﬁrst two. We also quantify the overhead of preparing the state
hierarchy by the content source. We present results that are the
average of 1000 experiments with a 95% conﬁdence interval.
Context-Switching. We measure the overhead to switch be-
tween the Supervisor, the state handler and the SMM (§5.3.2).
The Supervisor invokes the SMM when data is needed
from disk. This involves switching from the trusted to the
untrusted environment, and back (see table below). This time
is mostly used to transfer the memory map list between the
untrusted and the trusted execution environments. This requires
inspecting the nested page tables of the virtual machine to
check permissions for the data transfer, and then modifying
both the nested page tables with the new permissions and the
sensitive environment page tables to add (or remove) the pages
from the isolated virtual address space (§6.2).
A second source of overhead is that associated with re-
suming the state handler (after some data has been brought
into the secure environment) and the SMM (for disk access).
These resumption times (table below) include the overheads
of the XMHF-TrustVisor software stack1, of virtualization
to resume the trusted VM or the untrusted VM, and of
the VM interruption to return back into the hypervisor. The
slightly higher and more variable overhead for the SMM can
1TrustVisor is an application running within XMHF [25].
(cid:4)(cid:3)(cid:9)
(cid:4)(cid:3)(cid:8)
(cid:4)(cid:3)(cid:7)
(cid:4)(cid:3)(cid:6)
(cid:4)(cid:3)(cid:5)
(cid:4)(cid:3)(cid:4)
μ(cid:22)
μ(cid:22)
μ(cid:22)
μ(cid:22)
μ(cid:22)
μ(cid:22)
(cid:18)(cid:17)(cid:14)(cid:21)(cid:20)(cid:1)(cid:22)(cid:16)(cid:14)(cid:20)(cid:19)(cid:15)(cid:22)
(cid:13)(cid:11)(cid:2)(cid:22)(cid:16)(cid:14)
(cid:7)
(cid:12)(cid:11)
(cid:10)
(cid:12)(cid:11)
(cid:4)(cid:9)
(cid:12)(cid:11)
(cid:6)(cid:5)
(cid:12)(cid:11)
(cid:9)(cid:7)
(cid:12)(cid:11)
(cid:4)(cid:5)(cid:10)
(cid:12)(cid:11)
(cid:5)(cid:8)(cid:9)
(cid:12)(cid:11)
(cid:8)(cid:4)(cid:5)
(cid:12)(cid:11)
(cid:4)
(cid:13)(cid:11)
(cid:5)
(cid:13)(cid:11)
(cid:7)
(cid:13)(cid:11)
(cid:10)
(cid:13)(cid:11)
(cid:4)(cid:9)
(cid:13)(cid:11)
(cid:6)(cid:5)
(cid:13)(cid:11)
(cid:9)(cid:7)
(cid:13)(cid:11)
(cid:3)(cid:6)(cid:3)(cid:3)
(cid:3)(cid:5)(cid:8)(cid:3)
(cid:3)(cid:5)(cid:3)(cid:3)
(cid:3)(cid:4)(cid:8)(cid:3)
(cid:3)(cid:4)(cid:3)(cid:3)
(cid:3)(cid:8)(cid:3)
(cid:13)(cid:11)(cid:2)(cid:22)
Fig. 11: Log-scale average time (bars, left y-axis) and speed (line, right y-axis)
for mapping data (x-axis) inside/outside the isolated trusted environment. The
right y-axis shows the attained speed.
be attributed to scheduling delays (time slicing, preemption)
caused by the OS. This does not occur in the isolated (and
dedicated) execution environment where the state handler runs.
resumption
 SMM
resumption
39.93μs ± 0.5
 trusted-untrusted  state handler
env. switching
191.68μs ± 0.12
36.11μs ± 0.08
This means that the overheads for invoking the SMM and
the state handle from the Supervisor after a page fault are
+ and + respectively, in addition to the cost of the
processing (e.g., accessing disk or validating data).
I/O data mapping overhead. The overhead for transferring
memory maps between the execution environments (§6.2.2) is
shown in Fig. 11. Larger maps can be transferred at higher
speed, suggesting that if the application has to process all data
in a map, it is advantageous to transfer more data per fault to
reach high-speed (e.g., using a 256KB block size as we did).
From user data to LASTGT-compatible state. Fig. 12 shows
the cost of building the state hierarchy (§5.2) for user data sizes
from 1MB to 2GB. This is sufﬁcient since disk bottlenecks (for
reading data and writing back metadata) show up at 64MB. For
larger states, the throughput stabilizes at ≈ 60MB/s.
(cid:18)(cid:15)(cid:13)(cid:17)(cid:16)(cid:14)(cid:18)
(cid:12)(cid:10)(cid:1)(cid:18)(cid:15)(cid:13)
(cid:5)(cid:7)(cid:18)
(cid:5)(cid:7)(cid:18)
(cid:5)(cid:2)(cid:18)
(cid:5)(cid:2)(cid:18)
(cid:4)(cid:7)(cid:18)
(cid:4)(cid:7)(cid:18)
(cid:4)(cid:2)(cid:18)
(cid:4)(cid:2)(cid:18)
(cid:3)(cid:7)(cid:18)
(cid:3)(cid:7)(cid:18)
(cid:3)(cid:2)(cid:18)
(cid:3)(cid:2)(cid:18)
(cid:3)(cid:7)(cid:18)
(cid:3)(cid:7)(cid:18)
(cid:3)(cid:8)(cid:2)
(cid:3)(cid:8)(cid:2)
(cid:3)(cid:7)(cid:2)
(cid:3)(cid:7)(cid:2)
(cid:3)(cid:6)(cid:2)
(cid:3)(cid:6)(cid:2)
(cid:3)(cid:5)(cid:2)
(cid:3)(cid:5)(cid:2)
(cid:3)(cid:4)(cid:2)
(cid:3)(cid:4)(cid:2)
(cid:3)(cid:3)(cid:2)
(cid:3)(cid:3)(cid:2)
(cid:3)
(cid:3)
(cid:12)(cid:10)
(cid:12)(cid:10)
(cid:4)
(cid:4)
(cid:12)(cid:10)
(cid:12)(cid:10)
(cid:6)
(cid:6)
(cid:12)(cid:10)
(cid:12)(cid:10)
(cid:9)
(cid:9)
(cid:12)(cid:10)
(cid:12)(cid:10)
(cid:3)(cid:8)
(cid:3)(cid:8)
(cid:12)(cid:10)
(cid:12)(cid:10)
(cid:5)(cid:4)
(cid:5)(cid:4)
(cid:12)(cid:10)
(cid:12)(cid:10)
(cid:8)(cid:6)
(cid:8)(cid:6)
(cid:12)(cid:10)
(cid:12)(cid:10)
(cid:3)(cid:4)(cid:9)
(cid:3)(cid:4)(cid:9)
(cid:12)(cid:10)
(cid:12)(cid:10)
(cid:4)(cid:7)(cid:8)
(cid:4)(cid:7)(cid:8)
(cid:12)(cid:10)
(cid:12)(cid:10)
(cid:7)(cid:3)(cid:4)
(cid:7)(cid:3)(cid:4)
(cid:12)(cid:10)
(cid:12)(cid:10)
(cid:3)
(cid:3)
(cid:11)(cid:10)
(cid:11)(cid:10)
(cid:4)
(cid:4)
(cid:11)(cid:10)
(cid:11)(cid:10)
Fig. 12: Time (bars, left y-axis) and speed (line, right y-axis) to build the
LASTGT-compatible state for different state sizes.
Building the hash tree also incurs a high cryptographic
cost. It needs to hash 29 × 256KB-sized blocks and 210 − 2
tree nodes (i.e., all nodes except the root). The procedure is
optimized to take linear time in the size of the hash tree. We
chose SHA-256 as the hash function and carefully optimized it.
Different applications can leverage the incremental con-
struction and parallelize the operations, particularly consider-
ing that data chunks can be built separately.
7.4 End-to-end Application Performance
We ran experiments with three applications with different
data access patterns. They include: a simple application that
sequentially walks 1TB of data, checking the ﬁrst page of
each data chunk; a nucleobase search application [26, 3.2] that
accesses data sequentially, requiring all blocks of all chunks;
and SQLite [28], which accesses random blocks of chunks.
178
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:21:49 UTC from IEEE Xplore.  Restrictions apply. 
Tera-scale data processing. We use a synthetic state of 1TB.
The LASTGT-compatible state contains (in addition to the state
root and one directory) one master chunk of 2.5MB that carries
a list of 8192 chunks. This master chunk size is much larger
than the 256KB hash list contained in the master chunk due to
additional metadata (e.g., size and name) relative to the chunks
that we maintain. Each chunk has 33KB of metadata (32KB
due to the static hash tree, so 97%) and 128MB of data.
The execution environment is initially composed by a heap
map of 262K memory pages (1GB) loaded lazily—though
just a few are used in this application—and a state root map
that ﬁts in 1 page. Directory and IMELs are loaded as they
are accessed, and they also ﬁt in 1 page each. The master
chunk instead ﬁts in 641 pages. As chunks are accessed, two
additional maps are included in the environment: the chunk
metadata that ﬁts in 9 pages and is loaded; and the chunk data
that ﬁts in 32768 pages and is lazily loaded. Only about 15
maps are present at a time due to the state hierarchy and the