16 pages. https://doi.org/10.1145/2901318.2901355
[75] Vinod Kumar Vavilapalli, Arun C. Murthy, Chris Douglas, Sharad Agarwal, Ma-
hadev Konar, Robert Evans, Thomas Graves, Jason Lowe, Hitesh Shah, Siddharth
Seth, Bikas Saha, Carlo Curino, Owen O’Malley, Sanjay Radia, Benjamin Reed,
and Eric Baldeschwieler. 2013. Apache Hadoop YARN: Yet Another Resource
Negotiator. In Proceedings of the 4th annual Symposium on Cloud Computing
(SoCC). Article 5, 16 pages. https://doi.org/10.1145/2523616.2523633
[76] Abhishek Verma, Madhukar Korupolu, and John Wilkes. 2014. Evaluating
job packing in warehouse-scale computing. In Proceedings of the 2014 IEEE
International Conference on Cluster Computing (CLUSTER). 48–56.
[77] Abhishek Verma, Luis Pedrosa, Madhukar R. Korupolu, David Oppenheimer,
Eric Tune, and John Wilkes. 2015. Large-scale cluster management at Google
with Borg. In Proceedings of the 10th European Conference on Computer Systems
(EuroSys). Bordeaux, France.
[78] Lex Weaver and Nigel Tao. 2001. The optimal reward baseline for gradient-based
reinforcement learning. In Proceedings of the 17th Conference on Uncertainty in
Artificial Intelligence (UAI). 538–545.
[79] Ronald J Williams. 1992. Simple statistical gradient-following algorithms for
connectionist reinforcement learning. Machine learning 8, 3-4 (1992), 229–256.
[80] Matei Zaharia, Mosharaf Chowdhury, Tathagata Das, Ankur Dave, Justin
Ma, Murphy McCauley, Michael J. Franklin, Scott Shenker, and Ion Sto-
ica. 2012. Resilient Distributed Datasets: A Fault-tolerant Abstraction for
the 9th USENIX Con-
In-memory Cluster Computing. In Proceedings of
ference on Networked Systems Design and Implementation (NSDI). 15–28.
http://dl.acm.org/citation.cfm?id=2228298.2228301
283
Learning Scheduling Algorithms for Data Processing Clusters
SIGCOMM ’19, August 19-23, 2019, Beijing, China
Appendices
Appendices are supporting material that has not been peer reviewed.
A An example of dependency-aware scheduling
Figure 16: An optimal DAG-aware schedule plans ahead and parallelizes
execution of the blue and green stages, so that orange and green stages
complete at the same time and the bottom join stage can execute immediately.
A straightforward critical path heuristic would instead focus on the right
branch, and takes 29% longer to execute the job.
Directed acyclic graphs (DAGs) of dependent operators or “stages”
are common in parallel data processing applications. Figure 16 shows
a common example: a DAG with two branches that converge in a join
stage. A simple critical path heuristic would choose to work on the
right branch, which contains more aggregate work: 90 task-seconds vs.
10 task-seconds in the left branch. With this choice, once the orange
stage finishes, however, the final join stage cannot run, since its other
parent stage (in green) is still incomplete. Completing the green stage
next, followed by the join stage — as a critical-path schedule would —
results in an overall makespan of 28+3ϵ. The optimal schedule, by
contrast, completes this DAG in 20+3ϵ time, 29% faster. Intuitively,
an ideal schedule allocates resources such that both branches reach
the final join stage at the same time, and execute it without blocking.
B Background on Reinforcement Learning
We briefly review reinforcement learning (RL) techniques that we use
in this paper; for a detailed survey and rigorous derivations, see e.g.,
Sutton and Barto’s book [72].
Reinforcement learning. Consider the general setting in Figure 17,
where an RL agent interacts with an environment. At each step k, the
agent observes some state sk , and takes an action ak . Following the
action, the state of the environment transitions to sk +1 and the agent
receives a reward rk as feedback. The state transitions and rewards are
stochastic and assumed to be a Markov process: the state transition
to sk +1 and the reward rk depend only on the state sk and the action
ak at step k (i.e., they are conditionally independent of the past).
In the general RL setting, the agent only controls its actions: it
has no a priori knowledge of the state transition probabilities or the
reward function. However, by interacting with the environment, the
agent can learn these quantities during training.
For training, RL proceeds in episodes. Each episode consists of
a sequence of (state, action, reward) observations — i.e., (sk ,ak ,rk)
at each step k ∈[0,1,...,T], where T is the episode length . For ease of
understanding, we first describe an RL formulation that maximizes
Figure 17: A reinforcement learning setting with neural networks [53, 72].
The policy is parameterized using a neural network and is trained iteratively
via interactions with the environment that observe its state and take actions.
(cid:104)T
(cid:105)
k =0rk
the total reward: E
. However, in our scheduling problem,
the average reward formulation (§5.3) is more suitable. We later de-
scribe how to modify the reward signal to convert the objective to the
average reward setting.
Policy. The agent picks actions based on a policy π(sk ,ak), defined
as a probability of taking action ak at state sk . In most practical
problems, the number of possible {state, action} pairs is too large
to store the policy in a lookup table. It is therefore common to use
function approximators [15, 58], with a manageable number of ad-
justable parameters, θ, to represent the policy as πθ(sk ,ak). Many
forms of function approximators can be used to represent the pol-
icy. Popular choices include linear combinations of features of the
state/action space (i.e., πθ(sk ,ak) =θT ϕ(sk ,ak)), and, recently, neural
networks [39] for solve large-scale RL tasks [61, 71]. An advantage
of neural networks is that they do not need hand-crafted features, and
that they are end-to-end differentiable for training.
Policy gradient methods. We focus on a class of RL algorithms that
perform training by using gradient-descent on the policy parameters.
Recall that the objective is to maximize the expected total reward; the
gradient of this objective is given by:
,
rk
= Eπθ
k =0
∇θ Eπθ
∇θ logπθ(sk ,ak)Qπθ (sk ,ak)
(4)
where Qπθ (sk ,ak) is the expected total discounted reward from (de-
terministically) choosing action ak in state sk , and subsequently fol-
lowing policy πθ [72, §13.2]. The key idea in policy gradient methods
is to estimate the gradient using the trajectories of execution with the
current policy. Following the Monte Carlo Method [40], the agent
samples multiple trajectories and uses the empirical total discounted
reward, vk , as an unbiased estimate of Qπθ (sk ,ak). It then updates
the policy parameters via gradient descent:
(cid:34) T
(cid:35)
(cid:35)
(cid:34) T
k =0
T
k =0
θ ←θ +α
∇θ logπθ(sk ,ak)vk ,
(5)
where α is the learning rate. This equation results in the REINFORCE
algorithm [79]. The intuition of REINFORCE is that the direction
∇θ logπθ(sk ,ak) indicates how to change the policy parameters in
order to increase πθ(sk ,ak) (the probability of action ak at state sk ).
Equation 5 takes a step in this direction; the size of the step depends
on the magnitude of the return vk . The net effect is to reinforce actions
284
Critical path: 28 + 3ϵ(# tasks, task duration)(5, ϵ)(5, ϵ)(1, 10)(40, 1)(5, ϵ)(5, 10)(5, ϵ)Optimal: 20 + 3ϵ8ϵ102ϵ10ϵTimeTask slotsTimeTask slots2ϵ10ϵ10ϵAgentStateskNeural networkPolicy πθ(sk, ak)EnvironmentTake action akObserve state skReward rkParameter θSIGCOMM ’19, August 19-23, 2019, Beijing, China
H. Mao et al.
that empirically lead to better returns. Appendix C describes how we
implement this training method in practice.
Policy gradient methods are better suited to our scheduling context
than the alternative value-based methods for two reasons. First, policy-
based methods are easier to design if it is unclear whether the neural
network architecture used has adequate expressive power. The reason
is that value-based methods aim to find a fixed-point of the Bellman
equations [13]. If the underlying neural network cannot express the
optimal value function, then a value-based method can have difficulty
converging because the algorithm is trying to converge to a fixed point
that the neural network cannot express. With policy-based methods,
this issue does not arise, because regardless of the policy network’s
expressive power, the policy gradient algorithm will optimize for the
reward objective over the space of policies that the neural network
can express. Second, policy gradient methods allow us to use input-
dependent baselines [55] to reduce training variance (challenge #2 in
§5.3). It is currently unknown whether, and how, this technique can
be applied to value-based methods.
Average reward formulation. For our scheduling problem, an aver-
age reward formulation, which maximizes limT→∞E
,
is a better objective than the total reward we discussed so far.
1/TT
k =0rk
(cid:105)
(cid:104)
To convert the objective from the sum of rewards to the average
reward, we replace the reward rk with a differential reward. Opera-
tionally, at every step k, the environment modifies the reward to the
agent as rk ←rk −ˆr, where ˆr is a moving average of the rewards across
a large number of previous steps (across many training episodes). With
this modification, we can reuse the same policy gradient method as in
Equation (4) and (5) to find the optimal policy. Sutton and Barto [72,
§10.3, §13.6] describe the mathematical details on how this approach
optimizes the average reward objective.
C Training implementation details
Algorithm 1 presents the pseudocode for Decima’s training procedure
as described in §5.3. In particular, line 3 samples the episode length τ
from an exponential distribution, with a small initial mean τmean. This
step terminates the initial episodes early to avoid wasting training
time (see challenge #1 in §5.3). Then, we sample a job sequence (line
4) and use it to collect N episodes of experience (line 5). Importantly,
the baseline bk in line 8 is computed with the same job sequence
to reduce the variance caused by the randomness in the job arrival
process (see challenge #2 in §5.3). Line 10 is the policy gradient REIN-
FORCE algorithm described in Eq. (3). Line 13 increases the average
episode length (i.e., the curriculum learning procedure for challenge
#1 in §5.3). Finally, we update Decima’s policy parameter θ on line 14.
Our neural network architecture is described in §6.1, and we set
the hyperparameters in Decima’s training as follows. The number of
incoming jobs is capped at 2000, and the episode termination prob-
ability decays linearly from 5×10−7 to 5×10−8 throughout training.
The learning rate α is 1×10−3 and we use Adam optimizer [45] for
gradient descent. For continuous job arrivals, the moving window for
estimating ˆr spans 105 time steps (see the average reward formulation
in Appendix B). Finally, we train Decima for at least 50,000 iterations
for all experiments.
We implemented Decima’s training framework using TensorFlow [1],
and we use 16 workers to compute episodes with the same job se-
quence in parallel during training. Each training iteration, including
285
Algorithm 1 Policy gradient method used to train Decima.
1: for each iteration do
2:
3:
4:
5:
∆θ ←0
Sample episode length τ ∼exponential(τmean)
Sample a job arrival sequence
Run episodes i =1,...,N :
τ }∼ πθ
τ ,r i
τ ,ai
{si
1,ai
1,...,si
1,r i
=τ
N
k′=kr i
k′
i =1Ri
k
k)(Ri
k −bk)
Compute total reward: Ri
k
for k = 1 to τ do
compute baseline: bk = 1
N
for i = 1 to N do
∆θ ← ∆θ +∇θ logπθ(si
k ,ai
end for
end for
τmean←τmean +ϵ
θ ←θ +α ∆θ
6:
7:
8:
9:
10:
11:
12:
13:
14:
15: end for
interaction with the simulator, model inference and model update
from all training workers, takes roughly 1.5 seconds on a machine
with Intel Xeon E5-2640 CPU and Nvidia Tesla P100 GPU.
All experiments in §7 are performed on test job sequences unseen
during training (e.g., unseen TPC-H job combinations, unseen part
of the Alibaba production trace, etc.).
D Simulator fidelity
Our training infrastructure relies on a faithful simulator of Spark job
execution in a cluster. To validate the simulator’s fidelity, we measured
how simulated and real Spark differ in terms of job completion time
for ten runs of TPC-H job sets (§7.2), both when jobs run alone and
when they share a cluster with other jobs. Figure 18 shows the results:
the simulator closely matches the actual run time of each job, even
when we run multiple jobs together in the cluster. In particular, the
mean error of our simulation is within 5% of real runtime when jobs
run in isolation, and within 9% when sharing a cluster (95th percentile:
≤ 10% in isolation, ≤ 20% when sharing).