Das [20]
This work
20*
21
23
23*
25 to 37
✗
✗
✗
✗
✓
Table 1: Overview of new and extended contributions compared to prior work, i.e., [16, 17, 20, 30]. 1This work studied impact
of FEC, which was removed from QUIC in early 2016. 2Lack of calibration in prior work led to misleading reports of poor
QUIC performance for high-bandwidth links and large pages. 3Prior work typically speculates on the reasons for observed
behavior. 4Our choice of pages isolate impact of number and size of objects. 5Mobile (M), desktop (D), fixed-line (F), cellular
(C). † Replay of 500 real web pages with no control over size/number of objects to isolate their impact. ‡ Das tested a total of
100 network scenarios, but details of only 9 are mentioned. *Based on specified Chromium version/commit#.
contrast, QUIC allows other streams to continue to exchange
packets even if one stream is blocked due to a missing packet.
• Improved congestion control: QUIC implements better estima-
tion of connection RTTs and detects and recovers from loss
more efficiently.
Other features include forward error correction4 and improved
privacy and flow integrity compared to TCP.
Most relevant to this paper are the congestion and flow control
enhancements over TCP, which have received substantial attention
from the QUIC development team. QUIC currently5 uses the Linux
TCP Cubic congestion control implementation [35], and adds with
several new features. Specifically, QUIC’s ACK implementation
eliminates ACK ambiguity, which occurs when TCP cannot dis-
tinguish losses from out-of-order delivery. It also provides more
precise timing information that improves bandwidth and RTT es-
timates used in the congestion control algorithm. QUIC includes
packet pacing to space packet transmissions in a way that reduces
bursty packet losses, tail loss probes [22] to reduce the impact of
losses at the end of flows, and proportional rate reduction [28] to
mitigate the impact of random loss on performance.
The QUIC source code is open and published as
Source code.
part of the Chromium project [3]. In parallel with deployment, QUIC
is moving toward protocol standardization with the publication of
multiple Internet drafts [5, 9, 11].
Unlike many other experimental
Current deployment.
transport-layer protocols, QUIC is widely deployed to clients
(Chrome) and already comprises 7% of all Internet traffic [26]. While
QUIC can in theory be used to support any higher-layer protocol
and be encapsulated in any lower-layer protocol, the only known
deployments6 of QUIC use it for web traffic. Specifically, QUIC is
intended as a replacement for the combination of TCP, TLS, and
HTTP/2 and runs atop UDP.
QUIC occupies an interesting place in the space of
Summary.
deployed transport layers. It is used widely at scale with limited
4This feature allows QUIC to recover lost packets without needing retransmissions. Due to poor
performance it is currently disabled [37].
5Google is developing a new congestion control called BBR [19], which is not yet in general
deployment.
6These include the Chromium source code and Google services that build on top of it.
repeatable analyses evaluating its performance. It incorporates
many experimental and innovative features and rapidly changes the
enabled features from one version to the next. While the source code
is public, there is limited support for independent configurations
and evaluations of QUIC. In this paper, we develop an approach
that enables sound evaluations of QUIC, explains the reasons for
performance differences with TCP, and supports experimentation
with a variety of deployment environments.
2.2 Related Work
There is a large body of pre-
Transport-layer performance.
vious work on improving transport-layer and web performance,
most of it focusing on TCP [21, 22, 28] and HTTP/2 (or SPDY [39]).
QUIC builds upon this rich history of transport-layer innovation,
but does so entirely at the application-layer. Vernersson [38] uses
network emulation to evaluate UDP-based reliable transport, but
does not focus specifically on QUIC.
Several recent papers explore the
QUIC security analysis.
security implications of 0-RTT connection establishment and the
QUIC TLS implementation [23, 25, 27], and whether explicit con-
gestion notification can be used with UDP-based protocols such as
QUIC [29]. Unlike such studies, we focus entirely on QUIC’s end-to-
end network performance and do not consider security implications
or potential extensions.
The only large-scale
Google-reported QUIC performance.
performance results for QUIC in production come from Google.
This is mainly due to the fact that at the time of writing, Google
is the only organization known to have deployed the protocol in
production. Google claims that QUIC yields a 3% improvement in
mean page load time (PLT) on Google Search when compared to
TCP, and that the slowest 1% of connections load one second faster
when using QUIC [18]. In addition, in a recent paper [26] Google
reported that on average, QUIC reduces Google search latency by
8% and 3.5% for desktop and mobile users respectively, and reduces
video rebuffer time by 18% for desktop and 15.3% for mobile users.
Google attributes these performance gains to QUIC’s lower-latency
connection establishment (described below), reduced head-of-line
blocking, improved congestion control, and better loss recovery.
IMC ’17, November 1–3, 2017, London, United Kingdom
A. Molavi Kakhki et al.
In contrast to our work, Google-reported results are aggregated
statistics that do not lend themselves to repeatable tests or root
cause analysis. This work takes a complementary approach, using
extensive controlled experiments in emulated and operational net-
works to evaluate Google’s performance claims (Sec. 5) and root cause
analysis to explain observed performance.
Closely related to this work, several
QUIC emulation results.
papers explore QUIC performance. Megyesi et al. [30] use emulated
network tests with desktop clients running QUIC version 20 and
Google Sites servers. They find that QUIC runs well in a variety of
environments, but HTTP outperforms QUIC in environments with
high bandwidth links, high packet loss, and many large objects.
Biswal et al. [16] find similar results, except that they report QUIC
outperforms HTTP in presence of loss.
Carlucci et al. [17] investigate QUIC performance (in terms of
goodput, utilization, and PLT) using QUIC version 21 running on
desktop machines with dummy QUIC clients and servers connected
through emulated network environments. They find that FEC makes
QUIC performance worse, QUIC unfairly consumes more of the
bottleneck link capacity than TCP, QUIC underperforms when web
pages have multiple objects due to limited numbers of parallel
streams, and QUIC performs worse than TCP+HTTP when there
are multiple objects with loss. We do not study FEC because it was
removed from QUIC in early 2016.
In an M.S. thesis from 2014, Das [20] evaluates QUIC perfor-
mance using mahimahi [32] to replay 500 real webpages over emu-
lated network conditions. The author found that QUIC performs
well only over low-bandwidth, high-RTT links. Das reported that
when compared to TCP, QUIC improved performance for small
webpages with few objects, but the impact on pages with large
numbers of objects was inconclusive. Unlike this work, we focus
exclusively on QUIC performance at the transport layer and isolate
root causes for observed differences across network environments
and workloads. Along with models for complex web page depen-
dencies, our results can inform metrics like page-interactive time
for webpage loads (as done in the Mobilyzer study [33]).
This work makes the following new and
Our contributions.
extended contributions compared to prior work (summarized in
Table 1).
• Recent, properly configured QUIC implementations. Prior work
used old QUIC versions (20–23, compared with 34 in this work),
with the default conservative maximum allowed congestion
window (MACW). As we discuss in Sec. 4.1, using a small
MACW causes poor performance for QUIC, specifically in
high-bandwidth environments. This led to misleading reports
of poor QUIC performance for high bandwidth links and large
pages in prior work. In contrast, we use servers that are tuned
to provide nearly identical performance to Google’s QUIC
servers7 and demonstrate that QUIC indeed performs well
for large web pages and high bandwidth.
• Isolation of workload impact on performance. Previous work con-
flates the impact of different workloads on QUIC performance.
For example, when [20] studies the effect of multiplexing, both
the number of objects and the page size changes. This conflates
7This required significant calibration and communication with Google, as described in the following
section.
Figure 1: Testbed setup. The server is an EC2 virtual machine
running both a QUIC and an Apache server. The empirical
RTT from client to server is 12ms and loss is negligible.
QUIC’s multiplexing efficiency with object-size efficiency. In
contrast, we design our experiments so that they test one work-
load factor at a time. As a result, we can isolate the impact of
parameters such as number and size of objects, or the benefit
of 0-RTT connection establishment and proxies.
• Rigorous statistical analysis. When comparing QUIC with TCP,
most prior work do not determine if observed performance
differences are statistically significant. In contrast, we use sta-
tistical tests to ensure reported differences are statistically
significant; if not, we indicate that performance differences are
inconclusive.
• Root cause analysis. Prior work typically speculates on the rea-
sons for observed behavior. In contrast, we systematically iden-
tify the root causes that explain our findings via experiment
isolation, code instrumentation, and state-machine analysis.
• More extensive test environments. We consider not only more
emulated network environments than most prior work, but
we also evaluate QUIC over operational fixed-line and cellular
networks. We consider both desktop and mobile clients, and
multiple QUIC versions. To the best of our knowledge, we
are the first to investigate QUIC with respect to out-of-order
packet delivery, variable bandwidth, and video QoE.
3 METHODOLOGY
We now describe our methodology for evaluating QUIC, and com-
paring it to the combination of HTTP/2, TLS, and TCP. The tools
we developed for this work and the data we collected are publicly
available.
3.1 Testbed
We conduct our evaluation on a testbed that consists of a device
machine running Google’s Chrome browser8 connected to the In-
ternet through a router under our control (Fig. 1). The router runs
OpenWRT (Barrier Breaker 14.07, Linux OpenWrt 3.10.49) and in-
cludes Linux’s Traffic Control [7] and Network Emulation [6] tools,
which we use to emulate network conditions including available
bandwidth, loss, delay, jitter, and packet reordering.
Our clients consist of a desktop (Ubuntu 14.04, 8 GB memory,
Intel Core i5 3.3GHz) and two mobile devices: a Nexus 6 (Android
6.0.1, 3 GB memory, 2.7 GHz quad-core) and a MotoG (Android
4.4.4, 1 GB memory, 1.2 GHz quad-core).
Our servers run on Amazon EC2 (Kernel 4.4.0-34-generic, Ubuntu
14.04, 16 GB memory, 2.4 GHz quad-core) and support HTTP/2 over
TCP (using Cubic and the default linux TCP stack configuration) via
Apache 2.4 and over QUIC using the standalone QUIC server pro-
vided as part of the Chromium source code. To ensure comparable
8The only browser supporting QUIC at the time of this writing.
Client's machineServerInternetRouter (running network emulator)Taking a Long Look at QUIC
IMC ’17, November 1–3, 2017, London, United Kingdom
Values tested
5, 10, 50, 100
0ms, 50ms, 100ms
0.1%, 1%
1, 2, 5, 10, 100, 200
Parameter
Rate limits (Mbps)
Extra Delay (RTT)
Extra Loss
Number of objects
Object sizes (KB)
Proxy
Clients
Video qualities
5, 10, 100, 200, 500, 1000, 10,000, 210,000
QUIC proxy, TCP proxy
Desktop, Nexus6, MotoG
tiny, medium, hd720, hd2160
Table 2: Parameters used in our tests.
results between protocols, we run our Apache and QUIC servers on
the same virtual machine and use the same machine/device as the
client. We increase the UDP buffer sizes if necessary to ensure there
are no networking bottlenecks caused by the OS. As we discuss in
Sec. 4, we configure QUIC so it performs identically to Google’s
production QUIC servers.
QUIC uses HTTP/2 and encryption on top of its reliable transport
implementation. To ensure a fair comparison, we compare QUIC
with HTTP/2 over TLS, atop TCP. Throughout this paper we refer
to such measurements that include HTTP/2+TLS+TCP as “TCP”.
Our servers add all necessary HTTP directives to avoid any
caching of data. We also clear the browser cache and close all
sockets between experiments to prevent “warmed up” connections
from impacting results. However, we do not clear the state used for
QUIC’s 0-RTT connection establishment.
3.2 Network Environments
We compare TCP and QUIC performance across a wide range of net-
work conditions (i.e., various bandwidth limitations, delays, packet
losses) and application scenarios (i.e., web page object sizes and
number of objects; video streaming). Table 2 shows the scenarios
we consider for our tests.
We emulate network conditions on a separate router to avoid
erroneous results when doing so on an endpoint. Specifically, we
found that when tc and netem are used at an endpoint directly,
they result in undesired behavior such as bursty traffic. Further, if
loss is added locally with tc, the loss is immediately reported to
the transport layer, which can lead to immediate retransmission
as if there was no loss—behavior that would not occur outside the
emulated environment.
We impose bandwidth caps using token bucket filters (TBF) in tc.
We conducted a variety of tests to ensure that we did not use settings
leading to unreasonably long or short queues or bucket sizes that
benefit or harm QUIC or TCP. Specifically, for each test scenario we
run experiments to determine whether the network configuration
negatively impacts the protocols independent of additional delay
or loss, and pick settings that allow the flows to achieve transfer
rates that are close to the bandwidth caps.
Our tests on cellular networks use client devices that are directly
connected to the Internet.
3.3 Experiments and Performance Metrics
Unless otherwise stated, for each evaluation
Experiments.
scenario (network conditions, client, and server) we conduct at
least 10 measurements of each transport protocol (TCP and QUIC).
Figure 2: Google App Engine (GAE) vs. our QUIC servers on
EC2 before and after configuring them. Loading a 10MB im-
age over a 100Mbps link. The bars show the wait time (red)
and download time (blue) after the connection is established
and the request has reached the server (averaged over 10
runs). GAE (middle) has a high wait time.
To mitigate any bias from transient noise, we run experiments in
10 rounds or more, each consisting of a download using TCP and
one using QUIC, back-to-back. We present the percent differences
in performance between TCP and QUIC and indicate whether they
are statistically significant (p < 0.01). All tests are automated using
Python scripts and Chrome’s debugging tools. We use Android
Debug Bridge [1] for automating tests running on mobile phones.
Applications. We test QUIC performance using two applica-
tions that currently integrate the protocol: the Chrome browser
and YouTube video streaming.
For Chrome, we evaluate QUIC performance using web pages
consisting of static HTML that references JPG images (various num-
ber and sizes of images) without any other object dependencies or
scripts. While previous work demonstrates that many factors im-
pact load times and user-perceived performance for typical, popular
web pages [4, 33, 39], the focus of this work is only on transport
protocol performance. Our choice of simple pages ensures that page
load time measurements reflect only the efficiency of the transport
protocol and not browser-induced factors such as script loading and
execution time. Furthermore, our simple web pages are essential
for isolating the impact of parameters such as size and number of