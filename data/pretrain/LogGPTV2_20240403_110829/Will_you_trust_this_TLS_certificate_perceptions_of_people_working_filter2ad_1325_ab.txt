before, followed by Network Security Services (25%, 19 ), Java Key-
tool (25%, 19 ), GnuTLS (19%, 14 ), and Windows Certutil (12%, 9 ).
About a third had used the Let’s Encrypt Certbot before (36%, 27 ).
To estimate the security behavior of our participants, we used the
Security Behaviors Intentions Scale (SeBIS) [16]. Our sample tends to
have stronger intentions towards secure behavior than the general
population [16], especially for the device securement subscale (the
individual subscale scores on average 0.1–1.3 points higher).
The vast majority consented to the audio recording of the inter-
view, forming the sample size for the qualitative analyses (89%, 46 in
English, 21 in Czech). 59% of the participants (44 ) were (at ran-
dom) assigned to the original condition with the remaining 41% (31 )
assigned to the redesigned conditions. Checking for the differences
across conditions, we see no significant differences with respect to
self-reported previous experience, IT education, IT employment or
used tools suggesting that the samples are comparable.
3From now on, we use the symbol
4The plus-minus symbol (±) is used to denote standard deviation.
to denote the participants.
ACSAC ’19, December 9–13, 2019, San Juan, PR, USA
Ukrop and Kraus, et al.
To check for potential biasing effects, we asked if the participants
had taken part in an experiment conducted at the same conference
in the previous year [39]. Only a minority of the participants did
(13%, 10 ). Omitting these participants did not alter the conclusions
drawn from the results (we compared all the results for the whole
sample and the sample omitting these participants). We, therefore,
present the results of the full sample.
2.5 Data Collection and Analysis
First of all, data from all sources was matched based on timestamps
and each participant had a pseudonym assigned. Timestamps were
used to compute the time spent on the individual tasks. The browser
history was utilized to identify used resources.
We performed a qualitative descriptive analysis of the post-task
interview [35]. After transcribing the interviews, two researchers
independently familiarized themselves with the data. The data was
then processed using open coding [34] (the researchers indepen-
dently looked for reoccurring themes and assigned codes to text
passages). The analysis was open but broadly framed by the par-
ticipants’ comprehension of the encountered validation messages,
their opinions and actions. After the open coding, the researchers
discussed the created codes and consolidated a common codebook.
The transcripts were then re-coded using the shared codebook. The
results reported in Sections 3 and 4 are based on this coding round.
To ensure analysis reliability, a third independent coder coded
half of the English and half of the Czech transcripts using the shared
codebook. Interrater agreement between the original coders and the
independent coder showed to be substantial (according to Landis and
Koch [27]) for both English (Cohen’s κ =0.69, p <0.001) and Czech
(Cohen’s κ = 0.63, p < 0.001). All coders used all codes, indicating
that the defined codes were actually present in the interviews and
that the codebook was defined well. Some deviations that appeared
in the coding stemmed from the fact that the independent coder
was slightly more conservative in assigning codes compared to the
original coders.
Supplementary materials, including the experimental setup (the
complete virtual machine) and the anonymized dataset are available
at https://crocs.fi.muni.cz/papers/acsac2019.
2.6 Study Limitations
As is the case with every study, various limitations may diminish
the applicability of results. First, to ensure that participants would
behave as usual, we designed a realistic and appealing task (at a
conference on open-source technologies, we had them “develop” a
patch for the registration system). We limited neither time nor re-
sources allowing the participants to behave as they would in reality.
Realistic and well-known entities and hostnames were chosen for
the certificates. Although different hostnames may have slightly
different reception, we preferred this to the ecologically less valid
option of solving five separate cases with the same hostname.
The second great concern is the sample bias – we recruited atten-
deesofasingleindustrialconference.Nevertheless,wethinkoursam-
plereflectsthewiderpopulationofpeopleworkinginITwellenough.
Even though the conference was biased towards open-source and
Linux technologies, a 2018 Developer Survey by Stack Overflow [37]
estimatesthatalmosthalfofthedeveloperscontributetoopensource
and Linux is the most developed-for platform. Compared to the men-
tioned survey, our sample follows the general trends for professional
experience and education but has a slightly higher mean (peaking
at 3–5 years working in IT instead of 0–2 and at the master’s degree
instead of the bachelor’s). This means the reported comprehension
may be a bit higher for our sample compared to the wider population
(as our sample is a bit more experienced and educated).
Thirdly, participants’ behavior may have been primed towards
security by the context or parts of the questionnaire. However, we
were cautious not to mention security when advertising the study
and tried to recruit all participants passing by regardless of their skill.
Lastly, we tried to mitigate multiple response biases. To combat
the question order bias, we randomized the order of questions in the
initial questionnaire and the order of evaluated certificate cases both
in the task and the trust scale. Furthermore, to work against the bias
of the response order, we inverted all the Likert scales in both the
questionnaire and trust assessment for half of the participants. To
lower the observer effect (participants behaving differently when
being watched), the interviewers left the participants alone when
completing the tasks (but were still available in case the participant
wanted to consult something). Some participants might have been
more cautious than usual due to the social desirability bias – to ac-
countforthis,weseetheobtainedtrustevaluationsasalowerbound.
3 PERCEPTION OF CERTIFICATE FLAWS
In this section, we present comprehension and perceived trustwor-
thiness of tested certificate flaws, together with the reasoning that
participants provided in the exit interview. Comprehension is based
on qualitative analysis of the post-task interview (67 ). Trust assess-
ment and answers to the structured interview questions are available
for everybody (75 ). The section ends with a cross-case comparison
and investigation in the influence of previous knowledge.
3.1 Hostname Mismatch Case
The hostname mismatch flaw was comprehended quite well – a
majority of the participants mentioned the core of the issue was
the server hostname not matching the name provided in the cer-
tificate (code BadName, 50 , see Table 1 for frequency, simplified
definition and a representative quote on this and other codes). Many
participants explicitly mentioned the extra letter ‘s’ in the certificate
name (NameCheck, 27 ), hinting at the fact that they looked at the
certificate to investigate the issue. The prevalent opinion regarding
the cause of the error was that it was an attack of some sort (Attack,
22 ), but a few participants explicitly mentioned it could be only a
mistake or a typo (Mistake, 8 ).
The connection was on average assessed as ‘being outright un-
trustworthy’ (mean 0.68 ± 1.08, median 0, see Fig. 1 for detailed
information and Appendix A for the whole scale). We find the low
trust in this case unsurprising as the server with mismatched second-
level domain name gives almost no identity guarantees.
3.2 Self-signed Case
The self-signed case is also dominated by codes indicating com-
prehension: Mentioning that the certificate is self-signed (ByItself,
50 ), that no CA was involved in its issuance (NoCA, 28 ) and that lit-
erally anyone (including the participants themselves) can issue such
Will You Trust This TLS Certificate?
ACSAC ’19, December 9–13, 2019, San Juan, PR, USA
Table 1: Overview of the comprehension and reasoning codes occurring in at least 10% (7 ) of the interviews (67 in total, 39 in
the original (o.) and 28 in the redesigned (r.) condition). Codes labeled with an asterisk (⋆) indicate case comprehension.
Case / Code
h
c
t
a
m
s
i
.
m
n
t
s
o
H
d
e
n
g
i
s
-
f
l
e
S
d
e
r
i
p
x
E
s
t
n
i
a
r
t
s
n
o
c
e
m
a
N
K
O
BadName⋆
NameCheck
Attack
Mistake
ByItself⋆
NoCA⋆
50
27
22
8
50
28
AnyoneCan⋆ 21
IfExpected
Internal
Attack
NoLonger⋆
Mistake
Common
OKBefore⋆
Reputation
Attack
10
10
8
62
27
18
14
13
8
Constraint⋆ 25
Wrong
NotKnow
Attack
CAProblem⋆
CAConstr⋆
Mistake
NoInfo
NoIssue⋆
ExtraCheck
BugFree
19
14
10
10
9
7
7
61
13
12
in issuing the certificate.
(o.+r.) Code definition
(29+21) The certificate subject and
server name do not match.
(16+11) Mentioning the exact
difference in the names.
(12+10) Connection may be attacked.
It can be only a mistake or
(3+5)
server misconfiguration.
(30+20) The certificate is signed
by itself/self-signed.
(16+12) No CA was involved
(12+9) Anyone can create
such a certificate.
It is OK if such certificate
is known/expected.
They are used for testing or
internal purposes.
Connection may be attacked.
(7+3)
(2+6)
(34+28) The certificate has expired.
It can be only a mistake or
(15+12)
server misconfiguration.
Expired certificates are
common/occur in the wild.
(12+6)
(4+10) The certificate was OK
(6+4)
in the past.
Taking into account
the subject of the certificate.
Connection may be attacked.
(7+6)
(4+4)
(12+13) The name of the endpoint
certificate is constrained.
(12+7) Giving reasoning
(11+3)
(3+7)
(2+8)
that is wrong.
I do not understand it.
Connection may be attacked.
The intermediate CA was
not allowed to issue this.
The constraints are set
by the CA.
It can be only a mistake or
server misconfiguration.
Finding more information
on the matter is difficult.
(5+2)
(34+27) There is no problem.
(5+8) Doing extra manual checks
on the certificate.
The program is trusted to do
the verification correctly.
(2+7)
(3+4)
(7+5)
Representative qote
“The last one server, Facebook, [the certificate] was issued for a different
hostname.” [P39, original]
“[...] because it is not Facebook, it is Facesbook or something like that.”
[P57, original]
“It can be some phishing site or something like this.” [P76, original]
“And in this case – it’s a different domain, but I’d say it’s some kind
of typo or something like that.” [P63, redesigned]