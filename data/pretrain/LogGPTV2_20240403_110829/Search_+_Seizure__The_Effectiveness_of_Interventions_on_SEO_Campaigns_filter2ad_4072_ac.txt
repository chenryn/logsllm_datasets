ing two different approaches for selecting search terms, in the end
we ﬁnd the same campaigns poisoning search results. This over-
lap highlights both the pervasiveness of these campaigns and the
representativeness of terms selected in spite of the KEY campaign’s
early inﬂuence on our methodology.
4.1.2 Crawling Search Results
For each search term, we query Google daily for the top 100
search results. For each search result, we crawl each page link
using an updated version of the Dagger cloaking detection system
from previous work [35]. Dagger uses heuristics to detect cloaking
by examining semantic differences between versions of the same
page fetched ﬁrst as a user and then as a search engine crawler
(distinguished by the User-Agent ﬁeld in the HTTP request).
A previous limitation of Dagger was that it did not render the
page and, as a consequence, did not follow JavaScript (JS) redi-
rects. Thus, we extended Dagger by rendering each cloaked search
result detected using HtmlUnit [13], essentially a headless browser
complete with a JavaScript interpreter. (Since rendering a page is
an expensive operation, we only render pages we detect as cloaked.)
To detect iframe cloaking (Section 3.1.1), we implemented a sec-
ond crawler, VanGogh. VanGogh also uses HtmlUnit to render
pages. To detect iframe cloaking, it identiﬁes any iframes attempt-
ing to occupy the entire page visually (hiding the original content).
Speciﬁcally, we classify pages as using iframe cloaking if they load
iframes where the height and width attributes are both either set to
100% or larger than 800 pixels.
And again, due to the high overhead of rendering pages, we only
crawl a subset of search results using VanGogh. In particular, for
each measurement we crawl at most three randomly selected pages
from the same doorway domain to reduce the crawling workload.
We further trim the workload by not crawling domains previously
seen and not detected as poisoned by either VanGogh or Dagger.
This approach has proved reasonable due to the low daily churn
in search results for each vertical (on average 1.84% newly seen
domains are found in search results each day).
Store Detection
4.1.3
Ultimately we want to identify counterfeit luxury storefronts ad-
vertised through PSRs. We detect stores by applying two heuris-
tics to the set of PSRs discovered from crawling. First, we inspect
cookies from each landing site (the page eventually loaded in a
user’s browser after redirection through the doorway page) to look
for cookies commonly used by counterfeit luxury storefronts such
as those related to payment processing (e.g., Realypay, Mallpay-
ment), e-commerce (e.g., Zen Cart, Magento), and Web analytics
(e.g., Ajstat, CNZZ). Second, we search for either of the substrings
“cart” or “checkout” on the landing pages. If either of the heuris-
tics succeed, we treat the landing site as a counterfeit luxury store
advertised through search poisoning. Note that this approach iden-
tiﬁes stores from the search results within a vertical irrespective
of brand. For example, we may identify a counterfeit Christian
Louboutin store within Louis Vuitton search results.
We validate our detection methodology by manually inspecting
sampled search results from three popular verticals, Beats By Dre,
Isabel, and Louis Vuitton. For each vertical, we randomly chose
three search terms, and compared the search results for those terms
from two measurements taken at least two months apart (e.g., one
from November 23, 2013 and one from February 24, 2014). In to-
tal we examined 1.8K search results and detected 532 storefronts
advertised using cloaked search results. Among these we found
no false positives (instances where a benign page is mistakenly la-
beled as a doorway to a storefront) and 21 (1.2%) false negatives
(instances where a doorway to a storefront is not labeled). These
results are reassuring because errors are likely skewed towards un-
derrepresenting the number of storefronts.
4.2 Campaign Identiﬁcation
Our targeted crawls of Google search results produce a large col-
lection of doorway pages and counterfeit storefronts. We know that
behind these thousands of doorways and storefronts lurk a much
smaller number of distinct SEO campaigns, and the goal of our
work is to understand the full ecosystem of campaigns operating in
this counterfeit luxury market rather than focus on a singular cam-
paign, e.g., the KEY campaign.
A brute-force approach to this understanding would require a
domain expert to examine each Web page in our collection and
use domain-speciﬁc heuristics to infer the SEO campaign behind it.
The manual labeling of Web pages, however, is a time-consuming
and laborious endeavor that does not scale well to the many thou-
sands of examples in our collection. Instead we take a statistical
approach, and the rest of this section describes an automated, data-
driven method to identify the SEO campaigns behind individual
doorway and storefront Web pages.
To build a statistical model, we need a data set of labeled exam-
ples. Though manual labeling is tedious, we created such a data
set by identifying the SEO campaigns behind a small subset of 491
363Web pages in our much larger collection of crawled results. From
this small data set, we learned a classiﬁer that mapped the remain-
ing thousands of doorway and storefront Web pages to the 52 SEO
campaigns for which we had manually labeled examples. The re-
sults of this analysis (discussed in later sections) provide a compre-
hensive understanding of the ecosystem of SEO campaigns in the
counterfeit luxury market.
Our classiﬁer makes its predictions by extracting textual features
of HTML content and analyzing the statistics of these features that
distinguish Web pages from different campaigns. The following
subsections describe the classiﬁer in more detail, focusing in par-
ticular on the individual stages of feature extraction, model estima-
tion, and model validation.
4.2.1 Feature Extraction
The premise of our statistical approach is that doorway and store-
front Web pages contain predictive signatures of the SEO cam-
paigns behind them. Motivated by previous work [2], we looked for
these signatures in their HTML source. We expect HTML-based
features to be predictive in this domain for two reasons: ﬁrst, be-
cause SEO campaigns use highly specialized strategies to manip-
ulate the search rankings of doorways [36], and second, because
campaigns often develop in-house templates for the large-scale de-
ployment of online storefronts (e.g., customized templates for Zen
Cart or Magento providing a certain look and feel).
To extract HTML features, we follow a conventional “bag-of-
words” approach.
In particular, we construct a dictionary of all
terms that appear in the HTML source code, and for each Web page,
we count the number of times that each term appears. In this way,
each Web page is represented as a sparse, high-dimensional vector
of feature counts. We implemented a custom bag-of-words feature
extractor based on tag-attribute-value triplets [5] for the Web pages
in our data set.
One might also expect to ﬁnd predictive signatures of SEO cam-
paigns in network-based features (e.g., IPv4 address blocks, ASes).
However, we found that such features were ill-suited to differenti-
ate SEO campaigns due to the growing popularity of shared hosting
and reverse proxying infrastructure (e.g., CloudFlare). Therefore,
after a brief period of experimentation, we did not pursue the use
of such features.
4.2.2 Model Estimation
We learned linear models of classiﬁcation from our data set of la-
beled examples. Speciﬁcally, we used the LIBLINEAR package [7]
to learn L1-regularized models of logistic regression. The L1-
regularization encourages sparse linear models in which the pre-
dictions of SEO campaigns are derived from only a handful of
HTML features. Thus the resulting models are highly interpretable:
for each campaign, the regularization serves to identify the most
strongly characteristic HTML features from the tens of thousands
of extracted ones.
We evaluated the predictive accuracy of the classiﬁer by per-
forming 10-fold cross-validation on the data set of labeled exam-
ples. The average accuracy on held-out data was 86.8% for multi-
way classiﬁcation of Web pages into 52 different SEO campaigns.
(Note that uniformly random predictions would have an accuracy
of 1/52 = 1.9%.) Our model’s high accuracy on held-on examples
gave us conﬁdence to classify the remaining (unlabeled) Web pages
that we collected from poisoned search results.
4.2.3 Model Validation and Reﬁnement
We used the above models, trained on a small subset of labeled
Web pages, to infer the SEO campaigns behind the remaining unla-
beled Web pages. To do so, we extracted HTML features from the
unlabeled Web pages and used the classiﬁers to predict the most
likely campaign behind each example. To validate these predic-
tions, we manually inspected additional subsets of unlabeled ex-
amples. This step can be done efﬁciently by ﬁrst validating the
top-ranked predictions for each SEO campaign (as reﬂected by the
probabilities that the logistic regressions attach to each prediction).
We brieﬂy describe how we validated the classiﬁer’s predictions
on unlabeled Web pages. Primarily we assume that distinct SEO
campaigns are unlikely to share certain infrastructure such as SEO
doorway pages and C&Cs, payment processing, and customer sup-
port. We also consider less robust indicators such as unique tem-
plates, WHOIS registrant, image hosting, and Web trafﬁc analytics
(e.g., 51.la, cnzz.com, statcounter.com, etc.).
A ﬁnal stage is to reﬁne the model, using the manually veriﬁed
predictions to expand the set of labeled Web pages, retraining the
classiﬁer on this expanded set, and repeating this process in rounds.
With each iteration of this process we obtain a more accurate classi-
ﬁer and also one with greater coverage of distinct SEO campaigns.
Though some manual labeling is unavoidable, this overall approach
(of repeated human-machine interaction) is far more efﬁcient than
a brute-force expert analysis.
4.3 Purchases
From previous work studying email spam advertising illicit phar-
maceutical and software storefronts [16, 20], we found that making
orders on sites can shed light on normally opaque facets of under-
ground businesses: order volume, payment processing, and order
fulﬁllment. This information serves two important roles. First, it
reveals the interplay between the various actors in the counterfeit
luxury ecosystem (SEO campaigns, payment processors, and sup-
pliers). Second, the estimated order volume serves as a vital metric
in measuring the effectiveness of interventions (e.g., does label-
ing doorway search results as “hacked” lead to lower campaign or-
der volume?). Similar to this prior work, we created test orders
on counterfeit stores to estimate their order volume over time, and
made actual purchases to reveal the payment processors used by
these storefronts and the quality of the merchandise they sell.
4.3.1 Order Volume
We use the “purchase pair” technique [16] to estimate the order
volume of individual stores over time. This technique exploits the
fact that stores use monotonically increasing order numbers, where
the difference between order numbers represents the total number
of orders created over the time delta between orders.
Note that stores give users an order number before processing
their credit cards, and users still have an opportunity to back out.
Therefore this metric represents an upper bound on orders placed
and overestimates the absolute number of orders at a given store.
However, it is still useful to quantify the rate at which orders are
created, as well as the changes in order rate over time when corre-
lated with interventions.
Using this technique, we created 1,408 orders from 290 stores
touching 24 distinct campaigns and 13 verticals, between Novem-
ber 29, 2013 and July 15, 2014. We created 343 orders by hand
and 1,065 orders using scripts. Operationally, for both manual and
automated orders, we visit each store using via TOR and create or-
ders at weekly intervals, and we limit orders to three per day per
campaign to reduce the chance of being detected by the store or
payment processor. We take the orders all the way to the payment
processing page, which requires credit card details, before ﬁnally
leaving the site. The order and customer information we provide
364(a) Abercrombie
(b) Beats By Dre
(c) Louis Vuitton
(d) Uggs
Figure 2: Stacked area plots attributing PSRs to speciﬁc SEO campaigns within the labeled vertical. The red area represents the percentage
penalized, either through search or seizure. The remainder of the areas represents active PSRs, where the ﬁlled areas are attributed to speciﬁc
campaigns and the unﬁlled area is the remainder unclassiﬁed.
are semantically consistent with real customers, but ﬁctional and
automatically generated [6].
4.3.2 Transactions
To shed light on payment processing and order fulﬁllment in the
counterfeit luxury ecosystem, we successfully placed product or-
ders from 16 unique stores covering 12 different campaigns. In to-
tal, we received 12 knock offs of low to medium quality, all shipped
from China. From the bank identiﬁcation numbers (BINs) in our
transactions, we found that our purchases were processed through
three banks (two in China, one in Korea). This concentration sug-
gests payment processing is another viable area for interventions as
in [24], but investigating such an intervention remains future work.
4.4 User Trafﬁc
As surveyed in Section 2 and described in depth in our previ-
ous study [36], SEO campaigns poison search results to acquire
user trafﬁc that can then be monetized through scams — in this
case, counterfeit luxury stores. The order volume data shows that
counterfeit luxury stores do successfully convert user trafﬁc into
sales, and indirectly measures an SEO campaign’s effectiveness in
attracting trafﬁc via PSRs.
For a small number of stores, we were also able to collect user
trafﬁc data that directly measures an SEO campaign’s effectiveness
in attracting customers to their stores. Speciﬁcally, we were able
to periodically collect AWStats data for 647 storefronts in 12 cam-
paigns. AWStats is a Web analytics tool [1] that uses a Web site’s
server logs to report aggregated visitor information (e.g., the num-
ber of visitors, visitor durations, visitor geolocations, referrers of
visitors, etc.). From our crawled data, we discovered that these
stores left their AWStats pages publicly accessible, and we were
able to fetch visitor data for each store by visiting the publicly ac-
cessible default AWStats URL (e.g., http:///awstats/
awstats.pl?config=).
4.5 Supply Side Shipments
To better understand the suppliers, customers, and operational
relationship among storefronts and suppliers, we collected longitu-
dinal shipment data from a supplier partnering with MSVALIDATE,
one of largest SEO campaigns peddling counterfeit Louis Vuitton.
We discovered the supplier site from the packing slip of two of
our purchases. Upon visiting the site, we noticed it contains a
scrolling list of fulﬁlled orders and a mechanism to lookup ship-
ping records for valid order numbers in bulk (20 orders at a time).
Each record contains a timestamp and information regarding cur-
rent location and delivery status.
Using this mechanism, we collected over 279K shipping records
for nine months of orders placed through the supplier between July
5, 2013 and March 28, 2014. In summary, 256K orders success-
fully reached their destination, 4K were seized at the source (China),
15K were seized at the destination, and of the delivered, 1,319 were
returned by the customer. From country data listed in the records,
the three largest destinations are the United States, Japan, and Aus-
tralia, with 90k, 57K, and 39K orders, respectively. If we combine
these with the countries from Western Europe (41K), these regions
account for over 81% of orders.
5. RESULTS
In this section we use our crawler data to characterize the ac-
tivities of SEO campaigns that use search to promote stores selling
counterfeit luxury goods, and we further use our order data to study
024681012% Search Results2013−122014−022014−042014−06unknownphp?p=keypenalized0102030% Search Results2013−122014−022014−042014−06unknownpaulsimonjsusmoonkisnewsorgkeypenalized010203040% Search Results2013−122014−022014−042014−06unknownmiscmoklelenorthfaceclv.0msvalidatepenalized010203040% Search Results2013−122014−022014−042014−06unknownmiscuggs.0msvalidatejsusbiglovepenalized365the effects of both search engine and domain seizure interventions
on these SEO activities.
In short, we ﬁnd instances where both
can have the desired effect of disrupting counterfeit sites, but they
need to be far more reactive in time and comprehensive in coverage
to undermine the entire ecosystem of SEO campaigns exploiting
search engines for customers.