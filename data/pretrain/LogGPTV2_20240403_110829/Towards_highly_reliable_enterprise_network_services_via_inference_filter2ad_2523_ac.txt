Now that we have explained the Inference Graph model and Fer-
ret fault localization algorithm, we describe the Sherlock system
that actually constructs the Inference Graph for an enterprise net-
work and uses it to localize faults. Sherlock consists of a central-
ized Inference Engine and distributed Sherlock Agents. Sherlock
requires no changes to routers, applications, or middleware used in
the enterprise. It uses a three-step process to localize faults in the
enterprise network, illustrated in Figure 6.
First, Sherlock computes a service-level dependency graph
(SLDG) that describes the services on which each client and ser-
vice depends. Each Sherlock agent is responsible for monitoring
the packets sent and received by one or more hosts. The agent may
run on the host itself, or it may obtain packet traces via snifﬁng
a nearby link or router. From these traces, the agent computes the
dependencies between the services with which its host(s) commu-
nicates and the response time distributions for each service (Sec-
tion 4.1). This information is then relayed to the inference engine as
described in Section 5, where the engine aggregates the dependen-
cies between services computed by each agent to form the SLDG.
The SLDG is relatively stable, changing when new hosts or applica-
tions are added to the network, and we expect it will be recomputed
daily or weekly.
Second, the inference engine combines the SLDG with the net-
work topology to compute a uniﬁed Inference Graph over all ser-
vices in which the operator is interested and across all Sherlock
agents (Section 4.2). This step can be repeated as often as needed
to capture changes in the network.
Third, the inference engine runs Ferret over the response time
observations reported by the agents and the Inference Graph to
identify the root-cause node(s) responsible for any observed prob-
lems. This step runs whenever agents see large response times.
4.1 Discovering Service-Level Dependencies
Each Sherlock agent is responsible for computing the depen-
dency between the services its host accesses. We deﬁne the depen-
dency probability of a host on service A when accessing service
B as the probability the host needs to communicate with service
A before it can successfully communicate with service B. A value
of 1 indicates a strong dependency, where the host machine always
contacts service A before contacting B. For example, a client will
visit a web server soon after receiving a response from DNS server
providing the web server’s IP address, so the dependency probabil-
ity of using DNS when visiting a web server will be greater than 0.
Due to caching, however, the probability may be less than 1.
Because we deﬁne services in terms of IP addresses and ports,
Sherlock does not rely on parsing application-speciﬁc headers. It
could be easily extended to use a ﬁner-grain notion of a service if
such parsers were available.
4.1.1 Computing the Dependency Probability
Sherlock computes the dependency between services by leverag-
ing the observation that if accessing service B depends on service
A, then packets exchanged with A and B are likely to co-occur.
Using this observation, we approximate the dependency proba-
bility of a host on service A when accessing service B as the con-
ditional probability of accessing service A within a short interval,
called the dependency interval, prior to accessing service B. We
compute the conditional probability as the number of times in the
packet trace that an access to service A precedes an access to ser-
vice B within the dependency interval.
There is a tension in choosing the value of the dependency in-
terval which is well known in machine learning [8]. Too large an
interval will introduce false dependencies on services that are ac-
cessed with a high frequency, while too small an interval will miss
some true dependencies.
The Sherlock agents use a simple approach that works well in
practice. The dependency interval is ﬁxed at 10 ms, which in our
experience discovers most of the dependencies. The agents then
apply a simple heuristic to eliminate false positives due to chance
co-occurrence. They ﬁrst calculate the average interval, I, between
accesses to the same service and estimate the likelihood of “chance
co-occurrence” as (10ms)/I. They then retain only the dependen-
cies where the dependency probability is much greater than the
likelihood of chance co-occurrence.
Our heuristic for computing dependency works best when a re-
sponse from service A precedes a request to service B. But without
deep packet inspection, it is not possible to explicitly identify the
requests and responses in streams of packets going back and forth
between the host and A and the host and B. In practice, we have
found it is sufﬁcient to group together a contiguous sequence of
packets to a service as a single access to the service. In Section 6.1,
we show that this simple approximation produces reasonably accu-
rate service-level dependency graphs.
4.1.2 Aggregating Probabilities Across Clients
All agents periodically submit the dependency probabilities they
measure to the inference engine. However, because some services
are accessed infrequently, a single host may not have enough sam-
ples to compute an accurate probability. Fortunately, many clients
in an enterprise network have similar host, software and network
conﬁgurations (e.g. clients in the same subnet) and are likely to
have similar dependencies. Therefore, the inference engine aggre-
gates the probabilities of similar clients to obtain more accurate
estimates of the dependencies between services.
Aggregation also provides another mechanism to eliminate false
dependencies – for example, a client making a large number of
requests to the proxy server will appear to be dependent on the
proxy server for all the services it accesses. To eliminate these
false dependencies, the inference engine calculates the mean and
standard deviation of each dependency probability. It then excludes
clients with a probability more than ﬁve standard deviations from
the mean. Section 6.1 evaluates the effectiveness of aggregation.
4.2 Constructing the Inference Graph
Here we describe how the Inference Engine combines dependen-
cies between services reported by the Sherlock agents with network
topology information to construct a uniﬁed Inference Graph.
For each service S, the inference engine ﬁrst creates a noisy-
max meta-node to represent the service. It then creates an obser-
vation node for each client reporting response time observations of
that service and makes the service meta-node a parent of the ob-
servation node. The engine then examines the service dependency
information of these clients to identify the set of services DS that
the clients are dependent on when accessing S. The engine then re-
curses, expanding each service in DS. Once all service meta-nodes
have been created, for each of these nodes the inference engine
creates a root-cause node to represent the host on which the service
runs and makes this root-cause a parent of the meta-node.
The inference engine then adds network topology information
to the Inference Graph by using traceroute results reported by the
agents. For each path between hosts in the Inference Graph, it adds
a noisy-max meta node to represent the path and root-cause nodes
to represent every router and link on the path. It then adds each of
these root-causes as parents of the path meta-node.
Optionally, the operators can tell the inference engine where load
balancing or redundancy techniques are used in their network, and
the engine will update the Inference Graphs, drawing on the ap-
propriate specialized meta-node. Adapting the local environment
to the conﬁguration language of the inference engine can also be
done with scripting. For example, in our network the load-balanced
web servers for a site follow a naming convention and are called
sitename* (e.g., msw01, msw02). Our script looks for this pat-
tern and replaces the default meta-nodes with selector meta-nodes.
Similarly, the agent examines its host’s DNS conﬁguration using
ipconﬁg to identify where to place a failover meta-node to model
the primary/secondary relationship between its name resolvers.
Finally, the inference engine assigns probabilities to the edges
in the Inference Graph. The service-level dependency probabili-
ties are directly copied onto corresponding edges in the Inference
Graph. The special nodes always troubled and always down are
connected to observation nodes with a probability of 0.001, which
implies that 1 in 1000 failures are caused by a component not in our
model. Edges between a router and a path meta-node use a proba-
bility of 0.9999, which implies that there is a 1-in-10,000 chance
that our network topology or traceroutes are incorrect and the router
is not actually on the path. In our experience, Sherlock’s results are
not sensitive to the precise setting of these parameters (Section 6.2).
4.3 Fault Localization Using Ferret
As described in Section 3.2, Ferret uses a scoring function to
compute how well an assignment vector being evaluated matches
external evidence. A scoring function takes as input the probability
distribution of the observation node and the external evidence for
this node and returns a value between zero and one. A higher value
indicates a better match. The score for an assignment vector is the
product of scores for individual observations.
The scoring function for the case when an observation node re-
turns an error or receives no response is simple – the score is equal
to the probability of the observation node being down. For exam-
ple, if the assignment vector correctly predicts that the observation
node has a high probability of being down, its score will be high.
The scoring function for the case when an observation node
returns a response time is computed as follows. The Sherlock
agent tracks the history of response times and ﬁts two Gaus-
sian distributions to the historical data, namely Gaussianup
and Gaussiantroubled. For example,
the distribution in Fig-
ure 1 would be modeled by Gaussianup with a mean response
time of 200 ms and Gaussiantroubled with a mean response
time of 2 s. If the observation node returns a response time t,
the score of an assignment vector that predicts the observation
node state to be (pup, ptroubled, pdown) is computed as pup ∗
P rob(t|Gaussianup) + ptroubled ∗ P rob(t|Gaussiantroubled).
In other words, if the response time t is well explained by the
Gaussianup and the assignment vector correctly predicts that the
observation node has a high probability of being up, the assignment
vector will have a high score.
When Ferret produces a ranked list of assignment vectors for
a set of observations, it uses a statistical test to determine if the
prediction is sufﬁciently meaningful to deserve attention. For a set
of observations, Ferret computes the score that these observations
would arise even if all root causes were up – this is the score of
the null hypothesis. Over time, the inference engine obtains the
distribution of Score(best prediction) − Score(null hypothesis).
If the score difference between the prediction and the null hypoth-
esis exceeds the median of the above distribution by more than one
standard deviation, the prediction is considered signiﬁcant.
5.
IMPLEMENTATION
We have implemented the Sherlock Agent, shown in Figure 7,
as a user-level service (daemon) in Windows XP. The agent ob-
serves ongoing trafﬁc from its host machine, watches for faults,
and continuously updates a local version of the service-level de-
pendency graph. The agent uses a WinPcap [20]-based sniffer to
capture packets. We augmented the sniffer in several ways to efﬁ-
ciently sniff high volumes of data–even at an offered load of 800
Mbps, the sniffer misses less than 1% of packets. Agents learn the
network topology by periodically running traceroutes to the hosts
that appear in the local version of the service-level dependency
graph. Sherlock would easily accommodate layer-2 topology as
well, if it were available. The Agent uses an RPC-style mechanism
to communicate with the inference engine. Both the agent and the
User-level
Agent Architecture
Process Commands & 
Send Reports
Identify Service-
Level Dependencies
Detect Faults, 
Monitor Evidences
Probe Box (e.g. 
tracert, ping, wget)
Packet Capture
(e.g. WinPCAP, NetMON) 
Kernel
Network
Inference Engine
Chatter
Trigger probes, 
Summary Requests
Dependency Graphs,
Observed Evidence,
Probe Results
Figure 7: The components of the Sherlock Agent, with arrows
showing the ﬂow of information. Block arrows show the inter-
actions with the inference engine, which are described in the
text.
LAN1 R1
LAN2
Campus Network
Data Center
Servers (not all shown)
Figure 8: Topology of the production network on which Sher-
lock was evaluated. Circles indicate routers; diamonds indicate
servers; clients are connected to the two LANs shown at the top.
Multiple paths exist between hosts and ECMP is used.
inference engine use role-based authentication to validate incoming
messages.
The choice of a centralized inference engine makes it easier
to aggregate information, but raises scalability concerns about
CPU and bandwidth limitations. Back-of-the-envelope calculations
show that both requirements are feasible even for large enterprise
networks. A Sherlock Agent sends 100B observation reports once
every 300s. The inference engine polls each agent for its service-
level dependency graph once every 3600s, and for most hosts in
the network this graph is less than 40 KB. Even for an extremely
large enterprise network with 105 Sherlock Agents, this results in
an aggregate bandwidth of about 10 Mbps.
The computational complexity of fault localization scales lin-
early with graph size, so we believe it is feasible even in large
networks. Speciﬁcally, computational complexity is proportional to
the number of root causes in the inference graph × the graph depth.
Graph depth depends on the complexity of network applications,
but is less than 10 for all the applications we have studied.
6. EVALUATION
We evaluated our techniques by deploying the Sherlock system
in a portion of our organization’s enterprise network shown in Fig-
ure 8. We monitored 40 servers, 34 routers, 54 IP links and 2 LANs
for 3 weeks. Out of approximately 1,500 clients connected to the 2
LANs, we deployed Sherlock agents on 23 of them. In addition to
observing ongoing trafﬁc, these agents periodically send requests
6HDUFK
6HUYHU


6HUYHU
%DFNHQG



6HDUFK
,QGH[HU

'RPDLQ
&RQWUROOHU

3RUWDO

:,16




'16

3UR[\
6DOHV6LWH




&OLHQWĺ3RUWDO
&OLHQWĺ6DOHV6LWH
Figure 9: Inferred service dependency graphs for clients access-
ing the main web portal and the sales website. There is signiﬁ-
cant overlap in their dependencies.
DNS
WINS
DC
FileServerA
FileServerB
FileServerC
4.6%
5.7%
5.1%
7.9%
1.1%
FileServerD
0.3%
Proxy
9.9%
Clients Access FileServer
2.1%
Figure 10: Inferred service dependency graph for clients ac-
cessing a ﬁle server.
to the web- and ﬁle-servers, mimicking user behavior by browsing
webpages, launching searches, and fetching ﬁles. We also installed
packet sniffers at R1 and 5 routers in the datacenter, enabling us to
conduct experiments as if Agents were present on all clients and
servers connected to these routers. These servers include our or-
ganization’s internal web portal, sales website, a major ﬁle server,
and servers that provide name-resolution and authentication ser-
vices. Trafﬁc from the clients to the data center was spread across
four disjoint paths using Equal Cost Multi-Path routing (ECMP).
In addition to the ﬁeld deployment, we use both a testbed and
simulations to evaluate our techniques in controlled environments
(Section 6.2). The testbed and simulations enable us to study Fer-
ret’s sensitivity to errors in the Inference Graphs and compare
its effectiveness with prior fault localization techniques, including
Shrink [6] and SCORE [7].
6.1 Discovering Service Dependencies
We now evaluate Sherlock’s algorithm for discovering service-
level dependencies and quantify the amount of data and time re-
quired for stable results. We carefully examined the service-level
dependency graphs computed by Sherlock for ﬁfteen production
web and ﬁle servers in our organization, and we corroborated the
correctness and completeness of these dependencies with our sys-
tem administrators. Below, we show the dependency graphs for two
typical web servers and one ﬁle server, and we highlight the lessons
we learned.
Figure 9 shows the service-level dependency graphs for vis-
iting our organization’s main web portal and sales website. Ar-
rows point from servers that provide essential services to servers
or activities that depend on these services. Edges are annotated
with weights which represent the strength of the dependencies.
Two things are worth noting. First, clients depend on name lookup
servers (DNS, WINS), authentication servers (Domain Controller),
and proxy servers to access either of these websites. Clients must
communicate with the authentication servers to validate certiﬁcates
that control access and use the proxy servers to retrieve external
pages that are embedded in the websites’ pages. Second, both web-
sites also share substantial portions of their back-end dependencies.
The same search server crawls both websites and generates indexes
that are used by the websites to answer client queries. The pres-
y
t
i
l