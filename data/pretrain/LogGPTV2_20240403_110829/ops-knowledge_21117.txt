### 问题描述

我有一台使用 `mdadm` 构建的 RAID0 阵列的服务器。以下是相关软件和内核版本信息：

```bash
# mdadm --version
mdadm - v3.1.4 - 31st August 2010
# uname -a
Linux orkan 2.6.32-5-amd64 #1 SMP Sun Sep 23 10:07:46 UTC 2012 x86_64 GNU/Linux
```

其中一个硬盘 (`/dev/sdf`) 已经出现故障，但 `mdadm` 没有检测到该故障。

### 日志分析

从 `/var/log/kern.log` 中可以看到以下错误日志：

```bash
# grep sdf /var/log/kern.log | head
Jan 30 19:08:06 orkan kernel: [163492.873861] sd 2:0:9:0: [sdf] Unhandled error code
Jan 30 19:08:06 orkan kernel: [163492.873869] sd 2:0:9:0: [sdf] Result: hostbyte=DID_OK driverbyte=DRIVER_SENSE
Jan 30 19:08:06 orkan kernel: [163492.873874] sd 2:0:9:0: [sdf] Sense Key : Hardware Error [deferred]
```

当前 `dmesg` 的输出显示：

```bash
Jan 31 15:59:49 orkan kernel: [238587.307760] sd 2:0:9:0: rejecting I/O to offline device
Jan 31 15:59:49 orkan kernel: [238587.307859] sd 2:0:9:0: rejecting I/O to offline device
Jan 31 16:03:58 orkan kernel: [238836.627865] __ratelimit: 10 callbacks suppressed
Jan 31 16:03:58 orkan kernel: [238836.627872] mdadm: sending ioctl 1261 to a partition!
Jan 31 16:03:58 orkan kernel: [238836.627878] mdadm: sending ioctl 1261 to a partition!
Jan 31 16:04:09 orkan kernel: [238847.215187] mdadm: sending ioctl 1261 to a partition!
Jan 31 16:04:09 orkan kernel: [238847.215195] mdadm: sending ioctl 1261 to a partition!
```

然而，`mdadm` 并没有检测到磁盘故障：

```bash
# mdadm -D /dev/md0
/dev/md0:
        Version : 0.90
  Creation Time : Thu Jan 13 15:19:05 2011
     Raid Level : raid0
     Array Size : 71682176 (68.36 GiB 73.40 GB)
   Raid Devices : 3
  Total Devices : 3
Preferred Minor : 0
    Persistence : Superblock is persistent

    Update Time : Thu Sep 22 14:37:24 2011
          State : clean
 Active Devices : 3
Working Devices : 3
 Failed Devices : 0
  Spare Devices : 0

     Chunk Size : 64K

           UUID : 7e018643:d6173e01:17ab5d05:f75b494e
         Events : 0.9

    Number   Major   Minor   RaidDevice State
       0       8       17        0      active sync   /dev/sdb1
       1       8       65        1      active sync   /dev/sde1
       2       8       81        2      active sync   /dev/sdf1
```

### 进一步测试

通过强制读取 `/dev/md0` 来验证 `/dev/sdf` 是否确实失败了：

```bash
# dd if=/dev/md0 of=/root/md.data bs=512 skip=255 count=1
1+0 records in
1+0 records out
512 bytes (512 B) copied, 0.00367142 s, 139 kB/s

# dd if=/dev/md0 of=/root/md.data bs=512 skip=256 count=1
dd: reading `/dev/md0': Input/output error
0+0 records in
0+0 records out
0 bytes (0 B) copied, 0.000359543 s, 0.0 kB/s

# dd if=/dev/md0 of=/root/md.data bs=512 skip=383 count=1
dd: reading `/dev/md0': Input/output error
0+0 records in
0+0 records out
0 bytes (0 B) copied, 0.000422959 s, 0.0 kB/s

# dd if=/dev/md0 of=/root/md.data bs=512 skip=384 count=1
1+0 records in
1+0 records out
512 bytes (512 B) copied, 0.000314845 s, 1.6 MB/s
```

直接访问 `/dev/sdf` 失败：

```bash
# dd if=/dev/sdf of=/root/sdf.data bs=512 count=1
dd: opening `/dev/sdf': No such device or address
```

### 分析与解释

根据 `md(4)` 手册页中的描述，"clean" 状态表示阵列在关闭时是同步的。具体来说：

- **Unclean Shutdown**：当 RAID1、RAID4、RAID5、RAID6 或 RAID10 阵列进行写操作时，可能会出现短暂的不一致性。如果系统在写操作过程中关闭（例如由于电源故障），阵列可能不会一致。
- **Clean State**：`md` 驱动程序在写入数据之前会将阵列标记为 "dirty"，并在阵列被禁用时（例如在关机时）将其标记为 "clean"。如果 `md` 驱动程序在启动时发现阵列为 "dirty"，它会进行重新同步以纠正可能的不一致性。

在这种情况下，磁盘可能在 RAID 阵列正常关闭后才发生故障。因此，尽管磁盘已经失败，但 `mdadm` 仍然认为阵列处于 "clean" 状态。

### 结论

虽然数据对我来说并不重要，但我希望了解为什么 `mdadm` 坚持认为阵列状态是 "clean"。根据上述分析，这可能是由于磁盘在 RAID 阵列正常关闭后才发生故障，导致 `mdadm` 没有检测到磁盘故障。