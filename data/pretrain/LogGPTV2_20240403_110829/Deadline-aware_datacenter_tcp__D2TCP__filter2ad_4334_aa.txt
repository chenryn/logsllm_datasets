title:Deadline-aware datacenter tcp (D2TCP)
author:Balajee Vamanan and
Jahangir Hasan and
T. N. Vijaykumar
Deadline-Aware Datacenter TCP (D2TCP) 
Balajee Vamanan 
Purdue University 
Jahangir Hasan 
Google Inc. 
T. N. Vijaykumar 
Purdue University 
PI:EMAIL 
PI:EMAIL 
PI:EMAIL 
ABSTRACT 
An important class of datacenter applications, called Online Data-
Intensive (OLDI) applications, includes Web search, online retail, 
and  advertisement.  To  achieve  good  user  experience,  OLDI 
applications operate under soft-real-time constraints (e.g., 300 ms 
latency)  which  imply  deadlines  for  network  communication 
within  the  applications.  Further,  OLDI  applications  typically 
employ tree-based algorithms which, in the common case, result 
in bursts of children-to-parent traffic with tight deadlines. Recent 
work on datacenter network protocols is either deadline-agnostic 
(DCTCP) or is deadline-aware (D3) but suffers under bursts due to 
race  conditions.  Further,  D3  has  the  practical  drawbacks  of 
requiring  changes  to  the  switch  hardware  and  not  being  able  to 
coexist with legacy TCP. 
We  propose  Deadline-Aware  Datacenter  TCP  (D2TCP),  a 
novel transport protocol, which handles bursts, is deadline-aware, 
and  is  readily  deployable.  In  designing  D2TCP,  we  make  two 
contributions: (1) D2TCP uses a distributed and reactive approach 
for  bandwidth  allocation  which  fundamentally  enables  D2TCP’s 
properties.  (2)  D2TCP  employs  a  novel  congestion  avoidance 
algorithm, which uses ECN feedback and deadlines to modulate 
the congestion window via a gamma-correction function. Using a 
small-scale implementation and at-scale simulations, we show that 
D2TCP  reduces  the  fraction  of  missed  deadlines  compared  to 
DCTCP and D3 by 75% and 50%, respectively. 
Categories and Subject Descriptors 
C.2.2 [Computer Communication Networks]: Network 
Protocols. 
General Terms 
Algorithms, Design, Performance. 
Keywords 
Datacenter, Deadline, SLA, TCP, OLDI, ECN, Cloud Services. 
1.  INTRODUCTION 
Datacenters are emerging as critical computing platforms for 
ever-growing, high-revenue, online services such as Web search, 
online  retail,  and  advertisement.  These  services  employ  Online 
Data  Intensive  applications  (OLDI)  [18]  which  have 
two 
distinguishing properties: (1) Because application latency affects 
user  experience,  and  hence  revenue  [13],  OLDI  applications 
operate under soft-real-time constraints (e.g., 300 ms latency). (2) 
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. To copy 
otherwise, or republish, to post on servers or to redistribute to lists, 
requires prior specific permission and/or a fee. 
SIGCOMM’12, August 13–17, 2012, Helsinki, Finland. 
Copyright 2012 ACM  978-1-4503-1419-0/12/06...$15.00. 
tree-based, 
applications 
The 
divide-and-conquer 
algorithms  where  every  query  operates  on  data  spanning 
thousands of servers [12].  
employ 
An OLDI query’s overall time budget gets divided among the 
nodes  of  the  algorithm’s  tree  (e.g.,  40  ms  for  a  parent-to-leaf 
RPC). To avoid missing its deadline, a parent node sends out an 
incomplete response without waiting for slow children that have 
missed  their  deadlines.  Because  such  incomplete  responses 
adversely  affect  response  quality,  and  hence  revenue,  achieving 
fewer missed deadlines is important. While a node’s response time 
includes  both computational and  network latencies,  we  focus  on 
reducing the network delay. We note that in addition to achieving 
fewer  missed  deadlines,  a  network  protocol  that  allows  tighter 
network  budgets  is  invaluable  as  it  allows  more  time  for 
computation, thus producing higher-quality responses. 
A  key  reason  for  increased  network  delay  is  that  all  the 
children of a parent node face the same deadline and are likely to 
respond  nearly  at  the  same  time,  causing  a  fan-in  burst  at  the 
parent  in  the  common  case  [10][1][25].  Further,  because  typical 
data centers host multiple applications at the same time to enable 
flexible use and high utilization of the datacenter resources, OLDI 
flows  with  different  deadlines  and  background  flows  with  no 
deadlines  (e.g.,  Web  index  update)  share  the  network. As  such, 
multiple  such  bursts  coinciding  in  time  at  a  switch  may  lead  to 
congestive  packet  drops  and  TCP  retransmits,  which  frequently 
result in missed deadlines. We emphasize that the fan-in bursts are 
fundamental  to  OLDI  applications  and  are  not  artifacts  of  the 
network.  The  shallow-  and  shared-buffer  nature  of  datacenter 
switches,  combined  with  the  buffer-filling  nature  of  long-lived 
TCP flows, precludes absorbing these bursts in packet buffers [1]. 
Current  datacenters  alleviate  this  problem  by  a  combination  of 
two  approaches:  (1)  over-provision  the  network  link  bandwidths 
to  absorb  the  bursts,  and  (2)  increase  the  network’s  time  budget 
(e.g.,  equal  to,  say,  the  99th  percentile  of  the  observed  network 
delay) leaving less time for computation. While the former incurs 
high  cost,  the  latter  either  degrades  response  quality  (e.g.,  less 
time for Page Rank in Web search), or requires more machines to 
compensate  for  less  computation  per  machine  and  may  worsen 
fan-in bursts by increasing the fan-in degree (i.e., more children 
per parent).  
Recent work on datacenter networks either reduces the tail-
end  network  latency  or  proposes  deadline-aware  networks.  
DCTCP [1] is an elegant proposal that targets the tail-end latency 
by  gracefully  throttling  flows  in  proportion  to  the  extent  of 
congestion,  thereby  reducing  queuing  delays  and  congestive 
packet  drops  and,  hence,  also  retransmits.  DCTCP  reduces  the 
99th-percentile  of  the  network  latency  by  29%.  Unfortunately, 
DCTCP  is  a  deadline-agnostic  protocol  that  equally  throttles  all 
flows, irrespective of whether their deadlines are near or far. D3 
shows that as much as 7% of flows may miss their deadlines with 
DCTCP [25]. D3 tackles missed deadlines by pioneering the idea 
of incorporating deadline awareness into the network. In D3, the 
switches grant a sender’s request for the required bandwidth based 
on the flow size and deadline. 
While  D3  improves  upon  DCTCP,  D3  has  significant 
performance and practical shortcomings, which we cover in detail 
in Sections 2.4.2 and 2.4.3. On the performance side, D3 employs 
a  centralized  and  pro-active  approach  in  which  the  switches 
allocate  bandwidth  greedily  on  a  first-come-first-served  basis. 
Such  a  greedy  approach  may  allocate  bandwidth  to  far-deadline 
requests  arriving  slightly  ahead  of  near-deadline  requests.  We 
show in Section 4.2.2 that, due to this race condition, D3 inverts 
the  priority  of  24%-33%  requests,  thus  contributing  to  missed 
deadlines. Fixing these priority inversions is a hard problem as we 
explain in Section 2.4.2. On the practical side, D3 has some major 
drawbacks as well. D3 requires custom switching chips to handle 
requests  at  line  rates.  Such  custom  hardware  would  incur  high 
cost and long turn-around time that hinders near-term deployment. 
Further, because legacy TCP flows do not recognize D3's honor-
based  bandwidth  allocation,  D3  cannot  coexist  with  TCP.  Such 
lack of protocol interoperability prevents incremental deployment 
necessary  for 
technologies  and  for  high 
availability  during  upgrades.  As  such,  any  datacenter  protocol 
must be able to coexist with legacy TCP to be deployable in the 
real world.  
introducing  new 
In  summary,  DCTCP  improves  tail  latency  but  is  not 
deadline-aware  whereas  D3  is  deadline-aware  but  it  has  both 
performance  and  practical  shortcomings.  We  stipulate  that  a 
datacenter network protocol should: 
•  meet  OLDI  deadlines,  especially  in  fan-in-burst-induced 
congestion; 
achieve high bandwidth for background flows;  
• 
•  work with existing switch hardware; and  
• 
be able to coexist with legacy TCP.  
We  propose  Deadline-Aware  Datacenter  TCP  (D2TCP),  a 
novel  transport  protocol  based  on  TCP  that  meets  the  above 
requirements. In designing D2TCP, we make two contributions.  
Our  first  contribution  is  D2TCP’s  distributed  and  reactive 
approach.  Because  having  global  up-to-date  information  for  all 
flows  in  a  datacenter  is  technically  infeasible  given  the  rapid 
arrival rate and latency of flows, any network scheduling scheme 
must work with incomplete information. D2TCP approaches this 
challenge by inheriting TCP’s distributed and reactive nature, and 
adding  deadline  awareness  to  it.  In  contrast  to  D3’s  centralized 
bandwidth  allocation  at  the  switches,  which  rules  out  per-flow 
state, D2TCP’s distributed approach allows the hosts to maintain 
the needed state without changing the switch hardware. In contrast 
to D3’s pro-active approach, which does not allow for correcting 
the  decisions  resulting  from  inaccurate  information,  D2TCP’s 
reactive  approach  allows  senders  to  correct  any  temporary  and 
small over-subscription of the network which can be absorbed by 
typical packet buffers.  
Our  second  contribution  is  D2TCP’s  novel  congestion 
avoidance  algorithm  which  uses  ECN  feedback  and  deadline 
information  to  modulate  the  congestion  window  size  via  a 
gamma-correction function [2]. The key idea behind the algorithm 
is that far-deadline flows back off aggressively and near-deadline 
flows  back  off  only  a  little  or  not  at  all.  Our  algorithm 
simultaneously  satisfies  the  four  conditions  stipulated  above  so 
that  D2TCP  (1)  achieves  deadline-based  prioritization  in  the 
presence  of  fan-in-burst-induced  congestion  so  that  flows  with 
nearer  deadlines  are  prioritized,  while  ensuring  that  congestion 
does  not  worsen;  (2)  achieves  high  bandwidth  for  background 
flows  even  as  the  short-lived  D2TCP  flows  come  and  go;  (3) 
requires no changes to the switch hardware, so that  deployment 
amounts  to  merely  upgrading  the  TCP  and  RPC  stacks;  and  (4) 
coexists with legacy TCP, allowing incremental deployment. 
We demonstrate a real implementation of D2TCP on a small 
16-server testbed, and show that even at such small scale where 
fan-in-burst-induced  congestion  is  much  less  severe  than  at  real 
scale,  D2TCP  reduces  the  fraction  of  missed  deadlines  by  20% 
compared  to  DCTCP,  while  requiring  fewer  than  100  additional 
lines of kernel code. 
We perform further evaluations using at-scale simulations to 
show that D2TCP 
• 
reduces  the  fraction  of  missed  deadlines  compared  to  
DCTCP and D3 by 75% and 50%, respectively; 
achieves  nearly  as  high  bandwidth  as  TCP  for  background  
flows without degrading OLDI performance; 
• 
•  meets deadlines that are 35-55% tighter than those achieved 
by  D3  for  a  reasonable  5%  of  missed  deadlines,  giving 
OLDIs more time for actual computation; and 
coexists  with  TCP 
performance. 
Note  that  we  do  not  present  formal  proofs  for  D2TCP’s 
flows  without  degrading 
their 
• 
fairness and stability.  
The  remainder  of  the  paper  is  organized  as  follows.  In 
Section  2,  we  discuss  the  nature  of  OLDI  applications  and 
pinpoint the issues with the previous proposals. We describe the 
details  of  D2TCP  in  Section  3.  In  Section  4,  we  describe  our 
experimental  methodology  and  present  our  experimental  results. 
We  discuss  some  related  work  in  Section  5,  and  conclude  in 
Section 6.  
2.  OLDIs AND DATACENTER NETWORK 
PROTOCOLS 
We describe the nature of today's datacenter applications, and 
how  this  nature  interacts  with  the  previous  proposals  for 
datacenter network protocols. 
2.1  Online Data Intensive Applications 
As  the  name  suggests,  there  are  two  defining  properties  of 
Online  Data  Intensive  (OLDI)  applications:  online  and  data-
intensive [18]. Online implies an interactive nature, wherein a user 
typically inputs a query via a browser and expects an immediate 
response.  Consequently,  OLDI  applications  are  designed  to 
respond  within  a  short  deadline  (e.g.,  300  milliseconds).  Data-
intensive means that the applications consult large data sets (e.g., 
entire  index  of  the  web)  for  computing  the  response.  The  large 
volume of data is typically distributed over thousands of servers, 
and each query hits every server. 
The two properties combined lead to tree-based, divide-and-
conquer  algorithms  for  OLDI  applications  [12],  as  shown  in 
Figure  1.  The  specific  example  we  show  is  a  two-level  tree, 
however, the properties we describe hold for shallower and deeper 
trees as well. The user query arrives at the root, which broadcasts 
the query down to the leaves across which the data is partitioned.  
Each  leaf  sends  its  response  to  its  parent  which  aggregates  the 
results from all the leaves and sends a response to the root. The 
root, in turn, aggregates all the parents’ results and ships off the 
final response to the user. This overall application architecture is 
called  scatter-gather  or  partition-aggregate.  The  propagation  of 
the  request  down  to  leaves  and  of  the  responses  back  up  to  the 
root must complete within the deadline. That overall budget gets 
divided among the levels of the tree. For example, a leaf may have 
to respond to its parent within 30 milliseconds (Figure 1). When 
this  deadline  expires,  the  parent  aggregates  the  available  results 
and ships off the final response. Any leaf that misses its deadline 
fails  to  contribute  to  the  final  response.  Such  missed  deadlines 
result in incomplete, lower-quality responses. 
It is instructive to note that each leaf’s budget gets divided 
into  two  parts:  the  computation  time  on  the  leaf,  and  the 
communication  latency  between  the  leaf  and  its  parent.  This 
division balances two competing demands. A generous budget for 
communication  ensures  fewer  missed  deadlines  by  the  network, 
but also means less time for computation, thereby penalizing the 
quality of  results  (e.g., less  time  for  Page  Rank  in Web  search). 
The  upshot  is  that  not  only  is  it  desirable  to  have  fewer  missed 
deadlines,  but  there  is  also  great  value  in  tightening  the 
communication budget to give more time for computation. 
2.2  OLDI Fan-in Congestion 
Consider a parent’s subtree when the parent sends a query to 
its leaves. Because all leaves receive the query at nearly the same 
time, and because they all face the same deadline, the leaves are 
likely  respond  around  the  same  time.  The  result  is  a  burst  of 
responses fanning in to the parent in the common case [10] [1].  
While  such  bursts  can  be  smoothed  by  inserting  jitter,  doing  so 
increases the tail-end latency [9], and is therefore of limited use. 
Further, because datacenters run multiple applications at the same 
time to improve utilization, multiple fan-in bursts (from same or 
different applications) may coincide in time at the same switch. 
User 
query
OLDI repsonse 
in 200 msec
root
5 msec
30 msec
parent
parent
5 msec
30 msec
leaf
leaf
leaf
Figure 1: OLDI architecture 
leaf
In  addition  to  the  bursty  fan-in  traffic  described  above, 
datacenter  networks  also  carry  background  traffic  consisting  of 
long-lived flows. These flows push new control information and 
data  to  the  nodes  of the  OLDI  applications. While  this  traffic  is 
usually  not  constrained  by  tight  deadlines,  it  does  involve  large 
data transfers. Given the nature of TCP, these long flows tend to 
exercise the switch buffers to high utilization [1]. 
The combined network traffic described above often results 
in fan-in-burst-induced congestions which cause tail drops as the 
packet  buffers  fill  up.  Absorbing  these  bursts  in  larger  packet 
buffers  is  precluded  by  two  factors.  First,  datacenter  switches 
employ  network  ASICs  with  on-chip  packet  buffer  memory. 
Given  the  limitations  of  die  size,  on-chip  packet  buffers  are 
naturally  shallow.  Switch  designs  with  larger  off-chip  packet 
buffers  are  significantly  more  expensive  and  complex  [14],  and 
reserved  for  high-end  core  routers  that  must  buffer  for  Internet 
scale RTTs. Second, the nature of long-lived TCP flows is to fill 
up  larger  buffers,  which  may  lead  to  longer  queuing  delays  and 
still cause missed deadlines for OLDI traffic [1]. 
While  fan-in-burst-induced  congestions  are  a  fundamental 
characteristic of OLDI applications, the manner in which today's 
datacenter networks handle such fan-in bursts contribute to missed 
deadlines in two ways. 
First, a TCP/IP network uses packet drops as a feedback to 
inform  the  senders  about  on-going  congestion1.  Under  this 
mechanism,  the  sender  must  wait  for  a  timeout  to  detect  packet 
loss  even  as  the  deadline  expires.  Furthermore,  the  sender  also 
halves  its  transmission  rate  to  alleviate  the  congestion.  The  net 
result  is  that  the  leaves  involved  in  a  fan-in-burst-induced 
congestion are likely to miss their deadlines. Current datacenters 
address  this  problem  by  a  combination  of  two  approaches:  (1) 
increasing  the  network  link  bandwidths  and  (2)  increasing  the 
network  time  budget  to  be  greater  than,  say,  the  99th  percentile 
network  latency.  While  the  former  incurs  high  cost,  the  latter 
reduces  the time  budget for computation,  which either  penalizes 
the response quality or requires more machines to compensate for 
less  computation  per  machine  and  may  worsen  fan-in  bursts  by 
increasing the fan-in degree (i.e., more children per parent). 
Second, TCP treats all congested flows equally which is sub-
optimal when the congested flows’ deadlines are different. Ideally, 
a network should prioritize the flows that are about to miss their 
deadlines  while  throttling  the  flows  that  can  afford  to  wait. 
However,  TCP  is  a  fair-share2  protocol  which  lacks  such 
deadline-based distinction. Because fan-in bursts are common in 
OLDIs  and  fair-share  protocols  are  deadline-agnostic,  such 
protocols are not well-suited to datacenters [25]. 
2.3  Datacenter TCP (DCTCP) 
Explicit Congestion Notification (ECN) [21] is an extension 
to the TCP/IP protocol that enables congestion feedback without 
using  packet  drops  as  the  feedback  mechanism.  ECN  relies  on 
Active Queue Management (AQM) schemes like RED [8] to track 
congestion  at  a  switch.  When  a  switch  encounters  sustained 
congestion, it marks the Congestion Encountered (CE) bit in the 
IP header, thereby informing the endpoints about the congestion. 
The  endpoints  observe  this  CE  bit  feedback  and  reduce  their 
transmission rate. 
DCTCP [1] shows that ECN does not suffice to solve OLDI’s 
fan-in  burst  problem.  In  datacenters,  the  number  of  congested 
flows  is  small  enough  that  their  congestion  windows  tend  to  be 
synchronized with each other. Furthermore, the traffic is bursty in 
nature. Therefore, halving the flows’ windows in response to the 
ECN  feedback  causes  the  flows  to  thrash  instead  of  gracefully 