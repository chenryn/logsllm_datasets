EDNS and open resolver.
5. VALIDATION
In this section, we validate frontend enumeration, ge-
olocation, and clustering.
5.1 Coverage of Frontend Enumeration
Using EDNS-client-subnet can improve coverage over
previous methods that have relied on using fewer van-
tage points. We ﬁrst quantify the coverage beneﬁts of
EDNS-client-subnet. We then explore the sensitivity
of our results to the choice of preﬁx length for EDNS-
client-subnet, since this choice can also aﬀect front-end
enumeration.
Open Resolver vs EDNS-client-subnet Coverage.
An existing technique to enumerate frontends for a serv-
ing infrastructure is to issue DNS queries to the infras-
tructure from a range of vantage points. Following pre-
vious work [12], we do so using open recursive DNS
(rDNS) resolvers. We use a list of about 200,000 open
resolvers3; each resolver is eﬀectively a distinct van-
tage point. These resolvers are in 217 counties, 14,538
ASes, and 118,527 unique /24 preﬁxes. Enumeration of
Google via rDNS takes about 40 minutes. This dataset
forms our comparison point to evaluate the coverage of
the EDNS-client-subnet approach we take in this paper.
Table 1 shows the added beneﬁt over rDNS of enu-
merating Google frontends using EDNS-client-subnet.
Our approach uncovers at least 29% more Google fron-
tend IP addresses, preﬁxes, and ASes than were visible
using previous approaches. By allowing us to query
Google on behalf of every client preﬁx, we obtain a
view from locations that lack open recursive resolvers.
In Section 6.1, we demonstrate the beneﬁt over time
as Google evolves, and in Section 8 we describe how we
might be able to use our Google results to calibrate how
much we would miss using rDNS to enumerate a (possi-
bly much larger or smaller than Google) serving infras-
tructure that does not support EDNS-client-subnet.
EDNS-client-subnet Preﬁx Length. The choice
of preﬁx length for EDNS-client-subnet can aﬀect enu-
meration completeness. Preﬁx lengths smaller than /24
in BGP announcements are too coarse for enumera-
tion. We ﬁnd cases of neighboring /24s within shorter
BGP announcement preﬁxes that are directed to diﬀer-
ent serving infrastructure. For instance we observed an
3Used with permission from Duane Wessels, Packet Pushers
Inc.
6
ISP announcing a /18 with one of its /24 preﬁx get-
ting directed to Singapore while its neighboring preﬁx
is directed to Hong Kong.
Our evaluations query using one IP address in each
/24 block. If serving infrastructures are doing redirec-
tions at ﬁner granularity, we might not observe some
frontend IP addresses or serving sites. The reply to
the EDNS-client-subnet query returns the preﬁx length
covering the response. Thus, if a query for an IP ad-
dress in a /24 block returns a preﬁx length of, say /26,
it means that the corresponding redirection holds for
all IP addresses in the /26 covering the query address,
not the /24. For almost 75% of our /24 queries, the
other responses were for a /24 subnet, likely because it
is the longest globally routable preﬁx. For most of the
rest, we saw a /32 preﬁx length in the response, indicat-
ing that Google’s serving infrastructure might be doing
very ﬁne-grained redirection. For each such /24 subnet
(about 1/2 million subnets), we queried a 6-8 other IP
addresses within that preﬁx, and we discovered only 3
additional IP addresses. Thus, we believe our choice
of /24 minimally aﬀects completeness, but we plan to
understand the reasons for these ﬁne-grain redirections
in future work.
5.2 Accuracy of Client-Centric Geolocation
Client-centric geolocation using EDNS-client-subnet
shows substantial improvement over traditional ping based
techniques [10], undns [29], and geolocation databases [21].
Dataset. To validate our approach, we use the subset
of Google frontends with hostnames that contain air-
port codes hinting at their locations. Although the air-
port location is not a precise location, we believe that
it is reasonable to assume that the actual frontend is
within a few 10s of kilometers. Using approximately
550 frontends with airport codes, we measure the error
of our technique as the distance between our estimated
location and the airport location.
Accuracy.
Figure 1 shows the distribution of error
for CCG, as well as for three traditional techniques. We
compare to constraint-based geolocation (CBG), which
uses latency-based constraints from a range of vantage
points [10], a technique that issues traceroutes to fron-
tends and locates the frontends based on geographic
hints in names of nearby routers [12], and the Max-
Mind GeoLite Free database [21]. We oﬀer substantial
improvement over existing approaches. For example,
the worst case error for CCG is 409km, whereas CBG,
the traceroute-based technique, and MaxMind have er-
rors of over 500km for 17%, 24%, and 94% of frontends,
respectively. CBG performs well when vantage points
are close to the frontend [14], but it incurs large errors
for the half of the frontends in more remote regions. The
traceroute-based technique is unable to provide any lo-
cation for 20% of the frontends because there were no
hops with geographic hints in their hostnames near to
the frontend. The MaxMind database performs poorly
because it places most frontends belonging to Google in
Mountain View, CA.
Figure 1: Comparison of our client-centric geolo-
cation against traditional techniques, using Google
frontends with known locations as ground truth.
Figure 2: Impact of our various techniques to ﬁlter
client locations when performing client-centric geolo-
cation on Google frontends with known locations.
Importance of Filtering.
Figure 2 demonstrates
the need for the ﬁlters we apply in CCG. The CCG
no ﬁltering line shows our basic technique without any
ﬁlters, yielding a median error of 556km. Only con-
sidering client eyeball preﬁxes we observed in the Bit-
Torrent dataset reduces the median error to 484km and
increases the percentage of frontends located with error
less than 1000km from 61% to 74%. Applying our stan-
dard deviation ﬁltering improves the median to 305km
and error less than 1000km to 86%. When using speed-
of-light constraints measured from PlanetLab and MLab
to exclude client locations outside the feasible location
for a frontend and to exclude clients with infeasible
MaxMind locations, we obtain a median error of 26km,
and only 10% of frontend geolocations have an error
greater than 1000km. However, we obtain our best re-
sults by simultaneously applying all three ﬁlters.
Case Studies of Poor Geolocation.
CCG’s ac-
curacy depends upon its ability to draw tight speed-
7
of-light constraints, which in turn depends (in our cur-
rent implementation), on Planetlab and M-Lab deploy-
ment density. We found one instance where sparse van-
tage point deployments aﬀected CCG’s accuracy.
In
this instance, we observe a set of frontends in Stock-
holm, Sweden, with the arn airport code, serving a
large group of client locations throughout Northern Eu-
rope. However, our technique locates the frontends as
being 409km southeast of Stockholm, pulled down by
the large number of clients in Oslo, Copenhagen and
northern Germany. Our speed of light ﬁltering usually
eﬀectively eliminates clients far from the actual fron-
tend. In this case, we would expect Planetlab sites in
Sweden to ﬁlter out clients in Norway, Denmark and
Germany. However, these sites measure latencies to the
Google frontends in the 24ms range, yielding a feasible
radius of 2400km. This loose constraint results in poor
geolocation for this set of frontends.
It is well-known that Google has a large datacenter
in The Dalles, Oregon, and our map (Fig. 7) does not
show any sites in Oregon.
In fact, we place this site
240km north, just south of Seattle, Washington. A
disadvantage of our geolocation technique is that large
data centers are often hosted in remote locations, and
our technique will pull them towards large population
centers that they serve. In this way, the estimated loca-
tion ends up giving a sort of “logical” serving center of
the server, which is not always the geographic location.
5.3 Accuracy of Frontend Clustering
To validate the accuracy of our clustering method,
we run clustering on three groups of nodes for which
we have ground truth: 72 PlanetLab servers from 23
diﬀerent sites around world; 27 servers from 6 sites all
in California, USA, some of which are very close (within
10 miles) to each other, within 10 miles; and ﬁnally,
75 Google IP addresses that have 9 diﬀerent airport
codes in their reverse DNS names. These three sets are
of diﬀerent size and geographic scope, and the last set
is a subset of our target so we expect it to be most
representative.
The metric we use for the accuracy of clustering is
the Rand Index [26]. The index is measured as the
ratio of the sum of true positives and negatives to the
ratio of the sum of these quantities and false positives
and negatives. A Rand index equal to 1 means there
are no false positives or false negatives.
Table 2 shows the Rand index for the 3 node sets
for which we have ground truth. We see that in each
case, the Rand index is upwards of 97%. This accuracy
arises from two components of the design of our cluster-
ing method: eliminating outliers which result in more
accurate distance measures, and dynamically selecting
the cluster boundary using our OPTICS algorithm.
Our method does have a small number of false pos-
 0 0.2 0.4 0.6 0.8 1 0 500 1000 1500 2000CDF of estimated locationError (km)client-centric geolocation (CCG)CBGundnsMaxmind 0 0.2 0.4 0.6 0.8 1 0 500 1000 1500 2000CDF of estimated locationError (km)client-centric geolocaiton (CCG)CCG only solCCG only stdCCG only eyeballsCCG no filteringFigure 3: Distance plot of Google servers with air-
port codes. Servers in the same cluster have low
reachability distance to each other thus are output
in sequence as neighbors. Cluster boundaries are de-
marcated by large impulses in the reachability plot.
Figure 4: The output of the OPTICS clustering
algorithm when reverse-TTL is used for the metric
embedding. When using this metric, the clustering
algorithm cannot distinguish serving sites at Bom-
bay (bom) and Delhi (del) in India, while RTT-based
clustering can.
Experiment Rand Index
0.99
PlanetLab
CA
0.97
0.99 or 1
Google
Table 2: Rand index for our nodesets. Our cluster-
ing algorithm achieves over 97% across all nodesets,
indicating very few false positives or negatives.
itives and false negatives.
In the California nodeset,
the method fails to set apart some USC/ISI nodes from
nodes on the USC campus, and in the Planet lab node-
set, some clusters have low reachability distance that
confuses our boundary detection method. The Google
nodeset reveals one false negative which we actually be-
lieve to be correct: the algorithm correctly identiﬁes two
distinct serving sites in mrs, as discussed below.
To better understand the performance of our method,
Figure 3 shows the output of the OPTICS algorithm
on the Google nodeset. The x-axis in this ﬁgure rep-
resents the ordered output of the OPTICS algorithm,
and the y-axis the reachability distance associated with
each node. Impulses in the reachability distance depict
cluster boundaries, and we have veriﬁed that the nodes
within the cluster all belong to the same airport code.
In fact, as the ﬁgure shows, the algorithm is correctly
able to identify all 9 Google sites. More interesting, it
shows that, within a single airport code mrs, there are
likely two physically distinct serving sites. We believe
this to be correct, from an analysis of the DNS names
associated with those front-ends: all frontends in one
serving site have a preﬁx mrs02s04, and all frontends
in the other serving site have a preﬁx mrs02s05.
Finally, Figure 4 shows the OPTICS output when us-
ing reverse-TTL (as proposed in [19]) instead of RTT for
the metric embedding. This uses a slightly diﬀerent set
Figure 6: Growth in the number of points of pres-
ence hosting Google serving infrastructure over time.
of Google servers than in our evaluation above: this set
was chosen to highlight the performance of reverse-TTL
based clustering. For this set of nodes, reverse-TTL
based embedding performs reasonably well but results
in the OPTICS algorithm being unable to distinguish
between serving sites in bom and del. RTT-based clus-
tering is able to diﬀerentiate these serving sites (not
shown). Moreover, although reverse-TTL suggests the
possibility of two sites in mrs, it mis-identiﬁes which
servers belong to which of these sites (based on reverse
DNS names).
6. MAPPING GOOGLE’S EXPANSION
We present a longitudinal study of Google’s serving
infrastructure. Our initial dataset is from late October
to early November of 2012 and our second dataset cov-
ers March and April of 2013. We are able to capture a
substantial expansion of Google infrastructure.
6.1 Growth over time
8
 0 1 2 3 4 5 0 10 20 30 40 50 60 70Reachability distanceGoogle servers (OPTICS output order)mrsmucmilsoflgaiadordlaxnrt 0 0.2 0.4 0.6 0.8 1 1.2 1.4 0 10 20 30 40 50 60 70TTL Reachability distanceGoogle servers (OPTICS output order)mrsmucmilsofezesydsinbom del 0 50 100 150 200 250 300 350 400 450 5002012-10-012012-11-012012-12-012013-01-012013-02-012013-03-012013-04-012013-05-01Cumulative clusters observed by EDNSDateFigure 5: Growth in the number of IP addresses (a), /24 preﬁxes (b), and ASes/countries (c) observed to be
serving Google’s homepage over time. During our study, Google expanded rapidly at each of these granularities.
November 2012
ASes
Clients
May 2013
ASes
Clients
Google
Tier 1
Large
Small
Tiny
Stub
2
2
30
35
23