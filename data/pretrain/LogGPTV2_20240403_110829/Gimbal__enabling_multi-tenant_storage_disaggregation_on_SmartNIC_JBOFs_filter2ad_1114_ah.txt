the basic quantum for a deficit is 128KB (the maximum IO size). As
described before, it also takes the write cost into considerations
and uses the weighted IO size.
Credit-based Flow Control Our protocol is integrated with
the NVMe-oF submission and completion command processing
(Algorithm 3). Upon issuing a request, if there are enough credits,
the command is sending to the target side, and the inflight count
is increased; otherwise, applications receive a busy device signal,
and storage I/Os will be scheduled later. When receiving a NVMe-
OF completion request, it first processes the NVMe-oF command,
extracts the credit value, updates the latest credit amount, decrease
the inflight count, and then resumes storage workloads via callbacks.
Note that all credit manipulation operations are atomic.
ùë°ùëéùëüùëîùëíùë°_ùë†ùë†ùëë.ùëñùëõùëì ùëôùëñùëî‚Ñéùë° = ùë°ùëéùëüùëîùëíùë°_ùë†ùë†ùëë.ùëñùëõùëì ùëôùëñùëî‚Ñéùë° + 1
ùëüùëíùë° = ùë†ùë¢ùëèùëöùëñùë°_ùëõùë£ùëöùëíùëú ùëì _ùëüùëíùëû(ùëüùëíùëû)
if ret is success then
ùë°ùëéùëüùëîùëíùë°_ùë†ùë†ùëë = ùëüùëíùëû.ùë†ùë†ùëë
if target_ssd.credit_tot > target_ssd.inflight then
Algorithm 3 Credit-based flow control inlined with the NVMe-oF
request processing
1: procedure nvmeof_req_submit(req)
2:
3:
4:
5:
6:
7:
8:
9: procedure nvmeof_req_complete(req)
10:
11:
12:
13:
14:
15:
16:
ùë°ùëéùëüùëîùëíùë°_ùë†ùë†ùëë = ùëüùëíùëû.ùë†ùë†ùëë
ùëüùëíùë° = ùëêùëúùëöùëùùëôùëíùë°ùëí_ùëõùë£ùëöùëíùëú ùëì _ùëüùëíùëû(ùëüùëíùëû)
if ret is success then
ùë°ùëéùëüùëîùëíùë°_ùë†ùë†ùëë.ùëêùëüùëíùëëùëñùë°_ùë°ùëúùë° = ùëêùëüùëíùëëùëñùë°_ùëúùëèùë°ùëéùëñùëõ(ùëüùëíùëû)
ùë°ùëéùëüùëîùëíùë°_ùë†ùë†ùëë.ùëñùëõùëì ùëôùëñùëî‚Ñéùë° = ùë°ùëéùëüùëîùëíùë°_ùë†ùë†ùëë.ùëñùëõùëì ùëôùëñùëî‚Ñéùë° ‚àí 1
ùëêùëúùëöùëùùëôùëíùë°ùëñùëúùëõ_ùëêùëéùëôùëôùëèùëéùëêùëò(ùëüùëíùëû)
return ùëüùëíùë°
‚ä≤ Application specific
ùëüùëíùë° = ùëëùëíùë£ùëñùëêùëí.ùëèùë¢ùë†ùë¶
return ùëüùëíùë°
else
C.1 Dual Token Bucket for Read and Write
We describe the detail algorithm of the dual token bucket here
(Algorithm 4). A burst submission of write I/Os should be avoided
since it causes a latency spike even under moderate bandwidth.
Gimbal employs a token bucket algorithm in the rate pacing module
to mitigate the burstiness. A single bucket for both read and write
120
SIGCOMM ‚Äô21, August 23‚Äì28, 2021, Virtual Event, USA
Min and Liu, et al.
Figure 14: 4KB IO performance as increasing
the read ratio in clean and fragmented condi-
tions.
Figure 15: Random read latency varying
IO request size under different scenarios.
QD=queue depth.
Figure 16: 4KB/128KB read/write bandwidth
as increasing the per-IO processing cost on
SmartNIC JBOFs.
3,000
2,000
1,000
)
s
u
(
y
c
n
e
t
a
l
e
g
a
r
e
v
A
0
4KB Latency
128KB Latency
Read B/W
Congestion
10
20
Time (sec)
30
40
2,500
2,000
1,500
1,000
)
s
/
B
M
(
/
W
B
d
e
t
a
g
e
r
g
g
A
Figure 17: Latency increases over time under the 4KB/128KB read
mixed workload. Left and right Y-axis represent latency and aggre-
gated bandwidth, respectively.
Figure 18: Dynamic Latency Threshold (128KB Random Read)
does not prevent the burst submission because the DRR IO scheduler
in Gimbal does not reorder read and write I/Os so that it is possible
that only a single kind of IO operations may be dequeued in a
series. Gimbal has only one target rate which is the sum of read and
write bandwidth and it is significantly higher than a desirable write
bandwidth. Therefore, the single bucket approach would submit
write I/Os at a wrong rate and cause severe latency increments in
this case. Hence, Gimbal employs a dual token bucket algorithm.
There are two buckets for each read and write. Tokens are generated
by the target rate and then distributed to each bucket according
to the IO cost. Specifically, out of the total generated tokens, the
amount of ùë§ùëüùëñùë°ùëí_ùëêùëúùë†ùë°
ùë§ùëüùëñùë°ùëí_ùëêùëúùë†ùë°+1 is given to the read bucket and the write
bucket receives the remainder or the amount of
ùë§ùëüùëñùë°ùëí_ùëêùëúùë†ùë°+1. Lastly,
we allow overflowed tokens to transfer between each other. Thus,
dual token bucket offers the flexibility in rate pacing mechanism
while avoiding the burst submission of write I/Os for write intensive
workloads. Our dual token bucket works globally and applies to
all tenants. Algorithm 4 in Appendix describes the token update
procedure in detail.
Token bucket size. Gimbal does not reorder I/Os when de-
queued from the DRR scheduler. As a result, one bucket needs to
wait for the next IO if (1) the dequeued IO is not for the bucket and
(2) the other bucket does not have sufficient tokens to submit the IO.
A smaller bucket size would cause one bucket drops lots of tokens
during the wait time. Since the read bucket typically waits for the
write bucket, a small-sized one would hurt read bandwidth. On the
other hand, a large bucket size allows the read bucket to accumu-
late tokens during the wait time, increasing the read bandwidth.
1
1
max_tokens ‚Üê 256KB
avail_tokens = target_rate √ó time_since_last_update
read_tokens += avail_tokens √ó ùë§ùëüùëñùë°ùëí_ùëêùëúùë†ùë°
1+ùë§ùëüùëñùë°ùëí_ùëêùëúùë†ùë°
write_tokens += avail_tokens √ó
1+ùë§ùëüùëñùë°ùëí_ùëêùëúùë†ùë°
if read_tokens > max_tokens then
Algorithm 4 Dual Token Bucket
1: procedure update_token_buckets()
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
We evaluated the range from 128KB to 512KB and set the size to
256KB empirically, which provides a fair bandwidth allocation for
read/write under a mixed-IO workload.
read_tokens += write_tokens - max_tokens
read_tokens = ùëöùëñùëõ(read_tokens, max_tokens)
write_tokens = max_tokens
write_tokens += read_tokens - max_tokens
read_tokens = max_tokens
if write_tokens > max_tokens then
Appendix D Characterizing JBOF Multi-tenancy
This section characterizes the multi-tenancy support for today‚Äôs
SmartNIC JBOFs. In the disaggregated storage environment, dif-
ferent IO streams interact with each other along the IO path, and
will impact their performance. We categorize interference factors
into three types, i.e., IO intensity, IO size, and IO pattern (read v.s.
write, random v.s. sequential). We then use controlled experiments
to demonstrate how each factor affects a storage stream perfor-
mance and causes unfair resource sharing. Our experiment setup
is described in Section 5.1 and we use the fio [9] utility plus the
SPDK fio plugin. Note that (1) fio NVMe-oF clients run on different
client servers, and read/write to the same NVMe SSD; (2) we use
dedicated NVMe-oF target cores to handle different fio streams.
IO intensity: indicates how frequent a storage stream issues
IO requests to the remote storage. We configure two competing fio
storage streams with the same IO request size and read/write type.
Both streams can maximize the SSD bandwidth alone, but they differ
in the iodepth (i.e., the number of outstanding IO requests) where
stream1 issues twice the number of requests as stream2. Figure 19
reports the results for 4KB random read and 16KB sequential write,
respectively. On average across different IO sizes, stream1 achieves
2.0√ó and 1.8√ó more bandwidth compared with stream2 for two
cases, respectively. This is because the NVMe-oF target receives
more requests from stream1, and submits more NVMe commands
to SSD device queues. As a result, stream1 obtains more bandwidth
share than stream2.
IO size: presents the read/write block size of an NVMe-oF re-
quest within a stream. Again, we configure two competing fio
storage streams with the same read/write type and iodepth (which
121
 0 500 1000 1500 2000 0 20 40 60 80 100Bandwidth (MB/s)Read RatioClean-RDClean-WRFrag-RDFrag-WR 0 100 200 300 400 500 600 700 80048163264128256Latency (us)IO size (KB)VanillaFragmented70/30 (R/W)QD8 0 2 4 6 8 1001510204080160320Bandwidth (GB/s)Per-IO added processing latency (us)4KB read128KB read4KB write128KB write 100 1000Latency (us)latthreshGimbal: Enabling Multi-tenant Storage Disaggregation on SmartNIC JBOFs
SIGCOMM ‚Äô21, August 23‚Äì28, 2021, Virtual Event, USA
Figure 20: Stream1 bandwidth (4KB ran-
dom/sequential read/write) varying the IO re-
quest size for stream2.
Figure 21: Stream1 bandwidth compared be-
tween standalone v.s. mixed cases varying the
IO request size.
mainly because the read/write request handling within the SSD
has lots of overlapping [75, 78], such as device-level IO request
queue, FTL engine, write cache, flash chip controller for accessing
the planes, etc.
Next, in terms of latency, we run a 4KB random read stream,
and couple with another stream issuing random/sequential writes,
varying its IO size (Figure 22). We also repeat the same experiment
by mixing a 4KB sequential write stream with a random/sequential
read streams (Figure 23). Adding background traffic (stream2) defi-
nitely hurts the average and tail latency of the frontend one (stream1).
This is due to the head-of-line blocking impact coming from inter-
leaved different typed IOs. The larger the IO size is, the more latency
degradation one would observe. For example, considering random
read under sequential write, the average/p99.9 latency of the 128KB
case is 1.7√ó and 2.6√ó higher than the 4KB case, respectively. Further,
the curve becomes flat in Figure 22 after 16KB because stream2
has saturated the maximum write bandwidth. Therefore, to iso-
late different storage streams, one should also take the read/write
distribution and carefully monitor its dynamic execution costs at
runtime.
Appendix E RocksDB LSM-tree
RocksDB [20] is based on an LSM-tree data structure, consisting of
two key components: Memtable, an in-memory data structure that
accumulates recent updates and serves reads of recently updated
value; SSTable, collections of sorted key-value pairs maintained in a
series of levels. When the memtable reaches the size limit, it is per-
sisted as an SSTable by flushing the updates in sequential batches
and merging with other overlapping SSTables. Also, low-level SSTa-
bles are merged into high-level ones via compaction operations.
ùêø0 SSTables contain the latest data, while ùêø1..ùêøùëõ contain the older
data. Files within each level are maintained in a sorted-order, with
a disjoint key-range for each SSTable file (except in ùêø0, where each
SSTable file can span the entire key-range). Data retrieval starts
from the Memtable and will look up multiple SSTables (from level
0 to high levels) until finding the key.
Figure 19: Bandwidth of two competing stor-
age streams with different IO depth varying
the IO request size.
is large enough to saturate the remote bandwidth). The IO size of
stream1 is 4KB, and stream2 gradually increase its size. We report
the achieved bandwidth of stream1 under four cases in Figure 20.
Even though two storage streams would submit the similar amount
of requests into the NVMe drive, apparently, large IO occupies more
bandwidth for any of the random/sequential read/write scenarios.
For example, in the case of random read, when stream1 and stream2
are both 4KB, each stream takes around 850.0 MB/s. When a 4KB
stream1 mixes with 64KB stream2, stream1 only uses 91.0MB/s,
while stream2 achieves 1473.0MB/s.
Figure 22: Avg./P99.9 latency of 4KB random read mixed with write
traffic varying its IO request size.
Figure 23: Avg./P99.9 latency of 4KB sequential write mixed with
read traffic varying its IO request size.
IO pattern: refers to the type of IO request (e.g., read or write,
random or sequential) of a storage stream. Since the NVMe SSD
presents distinct execution costs for different typed IOs, when
they mix, one would observe significant performance interference
in terms of latency and bandwidth. We set up two competing
fio streams with the same adequate io depth and io size where
stream1/stream2 performs read/write, respectively. As Figure 21
shows, compared with the standalone mode, stream1 only achieves
38.9% and 27.3% bandwidth (on average across different IO sizes)
for random and sequential cases when mixed IO happens. This is
122
 0 500 1000 1500 2000 250048163264128256Bandwidth (MB/s)IO request size (KB)Stream1-RND-RDStream2-RND-RDStream1-SEQ-WRStream2-SEQ-WR 0 500 1000 1500 2000 0 20 40 60 80 100 120Stream1 bandwidth (MB/s)IO request size of stream2 (KB)4KB rand read4KB sequential read4KB rand write4KB sequential write 0 500 1000 1500 2000 2500 3000 3500 400048163264128256Bandwidth (MB/s)IO request size (KB)RND read onlyRND read+writeSEQ read onlySEQ read+write 0 500 1000 1500 2000 2500048163264128256Latency (us)Stream2 IO request size (KB)Avg-rnd-wrP99.9-rnd-wrAvg-seq-wrP99.9-seq-wr 0 200 400 600 800 1000 1200 1400048163264128256Latency (us)Stream2 IO request size (KB)Avg-rnd-wrP99.9-rnd-wrAvg-seq-wrP99.9-seq-wr