title:Gentle flow control: avoiding deadlock in lossless networks
author:Kun Qian and
Wenxue Cheng and
Tong Zhang and
Fengyuan Ren
Gentle Flow Control: Avoiding Deadlock in Lossless Networks
Kun Qian, Wenxue Cheng, Tong Zhang, Fengyuan Ren
Tsinghua University
Beijing National Research Center for Information Science and Technology (BNRist)
ABSTRACT
Many applications in distributed systems rely on underlying loss-
less networks to achieve required performance. Existing lossless
network solutions propose different hop-by-hop flow controls to
guarantee zero packet loss. However, another crucial problem called
network deadlock occurs concomitantly. Once the system traps in
a deadlock, a large part of network would be disabled. Existing
deadlock avoidance solutions focus all their attentions on breaking
the cyclic buffer dependency to eliminate circular wait (one nec-
essary condition of deadlock). These solutions, however, impose
many restrictions on network configurations and side-effects on
performance.
In this work, we explore a brand-new perspective to solve net-
work deadlock: avoiding hold and wait situation (another necessary
condition). Experimental observations tell that frequent pause on
upstream ports driven by existing flow control schemes is the root
cause of hold and wait. We propose Gentle Flow Control (GFC) to
manipulate the port rate at a fine granularity, so all ports can keep
packets flowing even cyclic buffer dependency exists, and prove
GFC can eliminate deadlock theoretically. We also present how
to implement GFC in mainstream lossless networks (Converged
Enhanced Ethernet and InfiniBand) with moderate modifications.
Furthermore, testbed experiments and packet-level simulations val-
idate GFC can efficiently avoid deadlock and introduce less than
0.5% of bandwidth occupation.
CCS CONCEPTS
• Networks → Link-layer protocols; Data path algorithms;
KEYWORDS
Lossless Networks, Deadlock Prevention, Flow Control
ACM Reference Format:
Kun Qian, Wenxue Cheng, Tong Zhang, Fengyuan Ren. 2019. Gentle Flow
Control: Avoiding Deadlock in Lossless Networks. In SIGCOMM ’19: 2019
Conference of the ACM Special Interest Group on Data Communication, August
19–23, 2019, Beijing, China. ACM, New York, NY, USA, 15 pages. https:
//doi.org/10.1145/3341302.3342065
1 INTRODUCTION
Lossless network is a crucial infrastructure in many application
scenarios. In data center networks, packet loss causes great damage
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGCOMM ’19, August 19–23, 2019, Beijing, China
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-5956-6/19/08...$15.00
https://doi.org/10.1145/3341302.3342065
75
to application performance and revenue loss [23, 37, 59, 60]. In
HPC system, InterProcess Communication (IPC) requires extremely
low communication latency. Packet retransmissions caused by loss
would vastly increase the transmission delay and further lead to
missing communication deadlines. In block-based storage, lossless
delivery of network packets is a critical requirement for providing
satisfied quality of service [9].
In order to provide a lossless network switching fabric, Con-
verged Enhanced Ethernet (CEE) employs Priority Flow Control
(PFC) [27], and InfiniBand develops Credit Based Flow Control
(CBFC) [4]. These flow controls guarantee zero loss by ceasing the
upstream port before buffer overflows and resuming packet send-
ing after queue decreasing. However, a crucial phenomenon called
network deadlock appears concomitantly. Network deadlock refers
to a standstill situation: a Cyclic Buffer Dependency (CBD) exists
in the network, while all switches in this cycle pause the upstream
switch port and wait for the downstream switch port to resume
packet transmission at the same time [24]. Figure 1 illustrates a
simple example of deadlock. When it happens, all ports in the CBD
cease forever. Furthermore, these ports will pressure congestion
back to sources and then cease all ports along the path. Finally, the
whole (or part of) network would be disabled [24]. Deadlock is a
silent killer. Once it occurs, the network cannot resume to normal
state autonomously.
The issue of network deadlock has been discovered in ages [14].
In the past, the scale of network is comparatively small so opera-
tors can manipulate network configurations well to avoid deadlock.
Thus it does not raise considerable attentions. However, with the
fast growth of data centers and clusters, the scale of networks and
traffic increase drastically. It becomes impractical to avoid deadlock
manually. Many large data center operators have confirmed the
occurrence of deadlock in practical scenarios [25]. Since network
deadlock would bring unbearable side-effects (e.g., entire network
paralysis), both cloud providers [22, 24, 25] and device manufactur-
ers [19, 52] devote great efforts to solving it.
The existing solutions to network deadlock problem fall into two
main categories: recovery and avoidance. Recovery schemes first
detect deadlock and then break it [2, 3, 36, 38, 52]. However, it is
difficult to detect deadlock in distributed networks, and the cor-
responding breaking approaches (e.g., resetting devices, dropping
packets and heuristic rerouting) are blunt and rigid [24]. Moreover,
they do not eradicate the root cause of deadlock. On the other hand,
avoidance schemes intend to break the necessary conditions of dead-
lock to eliminate it. Existing avoidance solutions mainly focus on
breaking the circular wait condition [6, 7, 13–18, 20, 21, 35, 46, 50, 54–
56, 58]. However, totally eliminating CBD is intractable under intri-
cate traffic in large-scale networks [24]. Such solutions impose re-
strictions on routing configurations or flows’ priorities. As a result,
the network capabilities of multi-path and performance isolation
are razed dramatically.
SIGCOMM ’19, August 19–23, 2019, Beijing, China
Kun Qian, Wenxue Cheng, Tong Zhang, Fengyuan Ren
Section 6 shows the evaluation results. Section 7 gives some discus-
sions about GFC. Section 8 introduces related work about solving
deadlocks and finally the paper is concluded in Section 9.
2 BACKGROUND
2.1 Necessary conditions for network deadlock
Deadlock is a fundamental issue in multi-task systems, and there
have been numerous investigations focusing on it. In [53], four
necessary conditions for deadlock occurrence are summarized: (1)
mutual exclusion, (2) no preemption, (3) hold and wait and (4)
circular wait. We elaborate how these necessary conditions map to
the network deadlock issue.
Mutual exclusion: Mutual exclusion means the resource is non-
sharable. In lossless networks, this non-sharable resource is buffer.
Once a packet is received, it occupies a part of buffer space. If other
packets require the occupied buffer, they need to wait until it is
released, namely, stored packets are forwarded to the next hop.
No preemption: Preemption means that the switch drops stored
packets for receiving new ones. In lossless network, no packet has
the right to expropriate buffer occupied by other packets. Therefore,
this condition always holds.
Hold and wait: Hold and wait means that packets are occupying
the buffer in current switch and waiting for the available buffer
space in the downstream switch. For example, in Figure 1(b), when
the downstream input buffer is full, packets in the upstream switch
must stay in the queue and wait. It is a common situation in existing
lossless networks when the downstream switch is congested.
Circular wait: Circular wait refers to Cyclic Buffer Dependency
(CBD) existing in the network. CBD is defined as a cyclic sequence
of switch buffers. Each switch in the sequence sends packets to the
next switch in the sequence [52]. Although the self-loop can be
completely avoided in advance, many other conditions (e.g., link
failures [24] and network updates [32]) may force multiple flows
to form a CBD.
2.2 Flow controls for lossless networks
Two mainstream communication networks, namely CEE and Infini-
Band, are deployed in current data centers and clusters to provide
lossless packet transmission. Ethernet is the dominant communica-
tion network used in data center scenarios. Data Center Bridging
(DCB) task group [28] proposes CEE to meet advanced application
requirements. InfiniBand, on the other hand, is widely-used in HPC
field [49], which supports more than half of HPC systems in the
world [41]. Both CEE and InfiniBand leverage hop-by-hop flow
control mechanisms to guarantee zero packet loss [28, 29, 40], as
shown in Figure 2. Their fundamental principles are similar. The
sender (upstream port) sends packets at line rate, and the receiver
(downstream port) generates feedback messages carrying the buffer
information back to the sender, so the sender knows when to stop
transmission to prevent overflow in the receiver’s input buffer. Fur-
thermore, if the flow control message notifies more available buffer,
the sender will resume sending packets. We elaborate existing flow
control mechanisms for zero packet loss as follows:
(a) Flow directions
(b) CBD
Figure 1: Simple example of deadlock.
In this work, we explore a brand-new solution to break another
necessary condition, that is hold and wait. The experimental obser-
vations enlighten us that employing existing flow control schemes
would inevitably cause hold and wait. When buffer is almost full,
the flow control mechanisms directly pause the upstream port,
then packets in the upstream port fall into the state of “holding"
in current buffer and “waiting" for available buffer space in the
downstream port. Especially in congestion scenarios, these flow
controls will pause upstream ports frequently, which makes the
network prone to hold and wait.
In light of above understanding, we propose Gentle Flow Con-
trol (GFC) to avoid the hold and wait condition. First, we propose
a conceptual design to manipulate input rate at a fine granular-
ity: as the queue length increases, the flow control decreases the
upstream rate accordingly. The port will eventually enter into a
status where the input rate matches the draining rate and the queue
length keeps steady. Therefore, all flows can keep passing through
the network continuously even CBD exists, and no deadlock ap-
pears. Then, considering the actual deployment constraints, the
practical GFC is proposed, which introduces negligible side-effect
on available bandwidth. Moreover, we discuss how to implement
GFC in mainstream lossless networks with moderate modifications.
We use testbed experiments as well as large-scale simulations to
verify the performance of GFC. The results confirm that GFC can
avoid deadlock efficiently and eliminate accompanying side-effects
as well.
The contributions of this paper are as follows:
(1) Understanding and solving network deadlock through avoid-
ing hold and wait.
(2) Designing GFC, and demonstrating its effectiveness in avoid-
ing hold and wait theoretically.
(3) Presenting how to implement GFC in mainstream lossless
switching fabrics (e.g., CEE and InfiniBand) with moderate
modifications.
The research in this paper poses no ethical issues. The rest of
the paper is organized as follows: Section 2 introduces all neces-
sary conditions for network deadlock and mainstream lossless flow
control mechanisms in detail. In Section 3, design decisions are
discussed. Both conceptual and practical GFC are elaborated in
Section 4. Implementation details of GFC are presented in Section 5.
76
S1S3S2H1H4H3Pausing Gentle Flow Control: Avoiding Deadlock in Lossless Networks
SIGCOMM ’19, August 19–23, 2019, Beijing, China
(a) PFC
(b) CBFC
Figure 2: Flow controls for zero packet loss.
2.2.1 Priority flow control. In order to enhance traditional Eth-
ernet for losslessness, DCB task group proposes the PFC mecha-
nism [27]. As shown in Figure 2(a), the receiver pauses the sender
when the ingress queue length exceeds a preset threshold (XOF F).
When the ingress queue length decreases below another threshold
(XON ), the receiver will resume the sender. As long as enough
headroom beyond XOF F is reserved for absorbing in-flight packets
before PAUSE takes effect, buffer would never overflow. In PFC
standard [27], 8 different priorities are defined. Each priority holds
a separate queue, and different types of traffic enter into different
priority queues. Each PFC message (PAUSE/RESUME) carries the
priority information, and can only act on the corresponding priority
queue.
PFC alternates between pausing and resuming to make the send-
ing rate of the upstream port match the draining rate of the down-
stream port in the long term. However, when a port pauses the
upstream port, all packets in the upstream port fall into the hold
and wait situation.
2.2.2 Credit based flow control. InfiniBand deploys CBFC to
eliminate packet loss [4]. In CBFC, the buffer space is divided into
credits, and the receiver controls the pausing/resuming of sender
by offering credits. As shown in Figure 2(b), the receiver maintains
an Adjusted Blocks Received (ABR) register, which counts all blocks
received since link initialization. By adding up the ABR value and
the allocated buffer size, the receiver can calculate out Flow Control
Credit Limit (FCCL). The receiver updates the FCCL value to sender
by generating feedback messages periodically. The sender maintains
a Flow Control Total Blocks Sent (FCT BS) register to track the
number of total blocks sent since link initialization. So the difference
between FCCL and FCT BS represents the available credits. When
the number of available credits is larger than the packet size, the
sender is allowed to send a packet. Otherwise, the sender will cease
until the FCCL value is updated. Strictly keeping the number of
sent blocks less than the received FCCL, zero packet loss can be
achieved. Once the sender runs out of credits, it enters into the
hold and wait state. By updating the FCCL value periodically, the
sending rate is adjusted to the draining rate in the long term.
2.2.3
Brief summary. Both PFC and CBFC are proposed to sat-
isfy the lossless requirement, and they follow the similar rationale:
the receiver feedbacks the ingress queue status to the sender, so
the sender can stop timely to avoid buffer overflow and resume
transmitting after queue length decreasing. There are mainly two
Figure 3: Possible approaches for solving network deadlock.
differences: (1) PFC generates messages according to whether the
queue length exceeds the threshold or not (buffer-based), while
CBFC generates flow control messages periodically (time-based).
(2) In PFC, when the downstream port generates the feedback mes-
sage, it directly decides the upstream port status (PAUSE/RESUME).
In CBFC, feedback messages only carry the credit information. The
sender needs further calculation to determine the port status. This
difference only concerns where the decision is made. In brief, both
distinctions are not radical.
In both flow control schemes, the sender alternates between
sending at line rate (ON ) and pausing (OF F). With these alterna-
tions, the sending rate of upstream port is adjusted to match the
draining rate of downstream port on a long time scale. On one
hand, it can avoid packet loss. On the other hand, the pausing sta-
tus unavoidably leads to the hold and wait situation (one necessary
condition for deadlock).
3 DESIGN DECISIONS
As shown in Figure 3, deadlock can be handled through either avoid-
ance or recovery. Furthermore, breaking any one of aforementioned
four necessary conditions can avoid deadlock. This section presents
why we choose the solution path of designing GFC to solve network
deadlock.
3.1 Why deadlock avoidance?
The first question to be answered is how to deal with the network
deadlock problem: to avoid it before its emergence or to recover
from it after its occurrence?
For recovery, the first challenge is detecting deadlocks in dis-
tributed networks. It is non-trivial since detecting deadlock requires
77
XOFFXONStatusPAUSEPacketGeneratorUpstream Port(Sender)Downstream Port(Receiver)DatagramInput RateDraining RateSending Rate (Rs) (Ri) (Rd) Feedback MessageABRFCCLFCTBSFCCLcFCCLcDatagramBuffer SizeFeedback MessageSending Rate (Rs) Input Rate(Ri) Draining Rate(Rd) Upstream Port(Sender)Downstream Port(Receiver)PacketGeneratorDeadlockAvoidanceRecoveryMutual exclusionNo preemptionHold and waitCircular waitBreaking CBDFlow controlInevitableInevitableLossless requirementNatural property of bufferStrict restrictions, impacts performanceHard to deploy, reacts slowlySIGCOMM ’19, August 19–23, 2019, Beijing, China
Kun Qian, Wenxue Cheng, Tong Zhang, Fengyuan Ren