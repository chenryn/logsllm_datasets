### Country of Residence

**Definitions:**
- In the questions, "recent" was defined as the previous two years.
- A "security champion" was defined as a non-expert who takes a particular interest in security [8].
- Developers with more than one app were asked to provide answers for the most frequently updated one.

**Secure Development Practices:**
The questions about secure development practices specifically addressed five of the most frequently-used assurance techniques [45, 51], as follows:

- **Threat Assessment:** Working as a team to identify actors and potential threats, followed by risk assessment and mitigation decisions.
- **Component Security Analysis:** Keeping components up-to-date using component security analysis tools within the toolchain.
- **Code Analysis:** Using code analysis tools to identify certain categories of security vulnerabilities.
- **Configuration Review:** Having other programmers or security experts review code for security problems.
- **Penetration Testing:** Having external specialist security testers identify flaws.

**Question Wording:**
All questions about security processes were worded as questions of fact, rather than future intentions, to reduce the impact of desirability biases [16].

**Omissions:**
We considered asking about code analysis tools, which are of particular interest to researchers. However, static analysis is only one of the five assurance techniques considered, and investigating all the techniques would have made the questionnaire unacceptably long without contributing to the research questions.

### 3.2. Survey Pre-Testing

After developing an initial questionnaire, we conducted pre-tests to gain insights into how survey respondents might interpret and answer questions, and to estimate the time required to complete the survey. The pre-testing process included the following steps:

- **Expert Review:** We asked an experienced usable security and privacy researcher, not part of the research team, to review the survey questionnaire and evaluate question wording, ordering, and bias. Following the expert review, we improved the wording of several questions and configured the survey software to randomize the order of answers and questions where possible [36].
- **Face-to-Face Testing:** We identified four local Android developers who were not previously involved in the research project and asked each to complete the survey while discussing it with a researcher. This led to the modification of the wording of two questions and the addition of one. We also noted that responses from developers of simple apps were not interesting from a security viewpoint, so we modified our criteria to invite only developers of "successful" and "maintained" apps (those with more than 100 downloads and at least one update).
- **Pilot Survey:** We ran a pilot survey with 5,000 Android developers from the same invitation list as the main survey, resulting in 30 completed entries. Participants in the pilot were excluded from the full survey. The results helped us check the dropout rate, which was acceptable, with only 21% of those who completed the first page dropping out later. We used the pilot responses to code the changes made due to GDPR and provided the most frequent answers as tick boxes in the final survey.

### 3.3. Calculation of Required Sample Size

We used Fowler's guidance [21] to determine the smallest subgroups for which we wanted data, estimating the proportion of these subgroups using pilot data. We aimed to get between 50 and 100 participants in each subgroup to achieve typical sampling errors of 4% to 15%. Based on the pilot data, we calculated a target sample size of 310, requiring us to send 55,000 invitations.

### 3.4. Recruitment

We invited only registered Google Play developers. From January to February 2019, we crawled the details pages of 3,608,673 Android applications (2,087,829 free and 1,520,844 paid) published in Google Play. For all apps, we stored their last update time, name, developer data, and download counts. We identified 312,369 developer accounts that met the 100+ downloads and update requirements. The number of apps published by a single developer account ranged from 1 to 3,302, with a median of 2. From these 312,369 developer accounts, we selected a random sample of 55,000 and sent a single invitation email to each. Of the 55,000 invited participants, 605 started and 345 completed the survey. Ten invited developers reached out via email; none complained about being contacted, three asked to be removed from the mailing list, and the remainder provided various reasons for not completing the survey. 240 left their email address in the survey for us to send them the results.

### 3.5. Filtering Invalid Results

In psychological surveys, a common stratagem is to ask a question twice, once negated, to filter out meaningless responses. Since our survey asked for facts rather than personality, we concluded this would be contrived and irritating. Instead, we looked at response times, determined a minimum time for completion, and filtered out the few (10) surveys that took less than that minimum time.

### 3.6. Survey Statistical Analysis Plan

This paper uses four forms of statistical analysis:
1. **Population Analysis:** To explore how well our sample corresponds to the larger population.
2. **Graphical Analysis:** To show the nature of the data.
3. **Confidence Limits:** For proportions in the wider population based on proportions in the sample.
4. **Correlation Analysis:** To identify relationships between different data items.

We defined the statistical scores and outlined analysis methods before collecting the main survey data, as required for research best practice [11, 12]. For analysis, we used Python statistical packages, including Pandas, Statsmodels, and Seaborn, in Jupyter Notebooks [25].

**Linear Analysis for RQ1:**
To address RQ1, we defined scores based on each respondent‚Äôs survey answers. Some scores captured the "need for security and privacy" (independent variables), while others captured "security-enhancing activities and interactions in the development team" (dependent variables). Figure 2 shows the processing we did to create these scores. The aim was to create an ordinal score that approximated linearity across the range of raw data, so a higher score corresponds to more security (or more drivers towards security) and each increment represents a similar semantic increase.

**Calculation of Scores:**
- **Requirements Score:** Reflects the security need as the arithmetic sum of the three Likert-style responses encoded as integers.
- **Developer Knowledge and Expertise Support Scores:** These scores explore the why behind the security needs.
- **Security Update Frequency Score:** Estimated as the product of the answers to two questions, with an exponential (Poisson) distribution. We used a transformation, log(ùë•+1), to make it linear.

**Assumptions:**
- Direct expert involvement is more effective than 'security champions.'
- Occasionally using two techniques is as effective as regularly using one.
- Considering four techniques is as effective as consistently using one.

Though reasonable, these scores are not provably linear or even ordinal [44]. We anticipated that inconsistencies in scoring would add to the statistical variance but not obscure overall trends. See Section 5.5 for a post-hoc justification.

**Pearson Correlation Coefficient:**
We used the Pearson Correlation Coefficient (‚ÄòPearson R‚Äô) calculation [14] to establish whether pairs of values had a significant linear relationship, which is acceptable for Likert-style data [24, 31].

**Decision Tree Models:**
We also investigated a more sophisticated modeling technique, creating Decision Tree models [41] for pairs of scores and using F-Tests [13] to compare each with the simpler Pearson R model. We treated the Security Update Frequency score as a dependent variable and the Requirements, Expertise Support, and Developer Knowledge scores as independent variables. The Assurance Technique score was treated both as an independent and dependent variable.

**Bonferroni Correction:**
Since the analysis constituted multiple tests on the same data, we applied the Bonferroni correction [40], reducing the threshold for 'significance' to (5%)/5=1%.

**Validation:**
To validate the preconditions for the Pearson Correlation Coefficient test [14], we constructed x-y plots of all the pairs of variables that showed significant correlation.

### 4. Application Analysis Methodology

In the second phase of the project, we downloaded and analyzed the apps corresponding to the survey responses using state-of-the-art vulnerability scanners. Each tool focuses on a different problem category and produces a relatively low number of false positives. We chose mature tools that are openly accessible to Android developers.

**4.1. Description of Analysis Tools**

The tools covered three key areas: SSL Security, Cryptographic API Misuse, and Privacy Leaks. We selected these areas based on previous work and because they cover a representative range of possible security and privacy vulnerabilities faced by application developers [34].

- **SSL Security:** A key concern is the correct use of secure transport mechanisms (SSL, TLS) when connecting to remote systems. We used MalloDroid [20] to inspect the correct use of certificate validation in the app code. We also extracted HTTPS URLs from the constant pools of the classes contained in the app using the OPAL framework [17] and checked the corresponding server configurations and certificates using the command-line tools curl and openssl.
- **Cryptographic API Misuse:** Many apps use cryptographic measures to improve data security and privacy. We ran CogniCrypt [26] to detect misuses of the Java Cryptography API. CogniCrypt uses static inter-procedural static program analysis to detect issues ranging from improper configuration of algorithms to incorrect order of calls to the API. It makes conservative assumptions on the control flow, which may produce false positive reports.
- **Privacy Leaks:** To find possibly harmful data flow that can lead to privacy leaks, we used FlowDroid [4]. This tool finds information flow in Android apps between defined information sources and sinks. We configured the tool with the default sources and sinks for Android provided by the authors, which were constructed by manual inspection of common vulnerabilities in Android apps. FlowDroid cannot determine if the found information flow is an actual leak, as it might be intended for specific use (e.g., location-based services).

**Practical Approach:**
We downloaded the application binaries for at least one application by each survey respondent and ran the full set of scanning tools on each, counting the issues (reports of possible vulnerabilities) generated. Appendix A lists the versions of the tools we used.