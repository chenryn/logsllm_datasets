with much larger initial eviction sets than before.
The main practical implication of this result is that ﬁnding
minimal eviction sets from user (or sandboxed) space is faster
than previously thought, and hence practical even without
any control over the slice or set
index bits. This renders
countermeasures based on reducing adversary control over
these bits futile.
A. The Baseline Algorithm
We revisit
the baseline algorithm for reducing eviction
sets that has been informally described in the literature. Its
pseudocode is given as Algorithm 1.
Algorithm 1 receives as input a virtual address x and an
eviction set S for x. It proceeds by picking an address c from
S and tests whether S \{c} is still evicting x, see line 4. If it
is not (notably the if-branch), c must be congruent to x and is
recorded in R, see line 5. The algorithm then removes c from
S in line 7 and loops in line 2.
Note that the eviction test TEST is applied to R∪(S\{c}) in
line 4, i.e. all congruent elements found so far are included.
This enables scanning S for congruent elements even when
there are less than a of them left. The algorithm terminates
when R forms a minimal eviction set of a elements, which is
guaranteed to happen because S is initially an eviction set.
Proposition 1. Algorithm 1 reduces an eviction set S to its
minimal core in O(N 2) memory accesses, where N = |S|.
The complexity bound follows because |S| is an upper
bound for the number of loop iterations as well as on the
argument size of Test 1 and hence the number of memory
accesses performed during each call to TEST.
The literature contains different variants of Algorithm 1[4],
[5], [9]. For example, the variant presented in [9] always puts
c back into S and keeps iterating until |S| = a. This still is
asymptotically quadratic, but adds some redundancy that helps
to combat errors.
If the quadratic baseline was optimal, one could think about
preventing an adversary from computing small eviction sets by
reducing or removing control over the set index bits, either by
Algorithm 1 Baseline Reduction
In: S=candidate set, x=victim address
Out: R=minimal eviction set for v
1: R ← {}
2: while |R|  a do
2:
3:
4:
5:
6:
7:
8: end while
9: return S
i ← i + 1
end while
S ← S \ Ti
least one j ∈ {1, . . . , p+1} such that S\Tj is still an eviction
set. One identiﬁes such a j by group tests and repeats the
procedure on S \ Tj. The logarithmic complexity is due to
the fact that |S \ Tj| = |S| p
p+1, i.e. each iteration reduces the
eviction set by a factor of its size, rather by than a constant
as in Algorithm 1.
Algorithm 2 computes minimal eviction sets based on this
idea. Note that Lemma 1 gives a bound on the number of
group tests. For computing eviction sets, however, the relevant
complexity measure is the total number of memory accesses
made, i.e. the sum of the sizes of the sets on which tests are
performed. We next show that, with this complexity measure,
Algorithm 2 is linear in the size of the initial eviction set.
Proposition 2. Algorithm 2 with Test 1 reduces an eviction
set S to its minimal core using O(a 2N ) memory accesses,
where N = |S|.
Proof. The correctness of Algorithm 2 follows from the invari-
ant that S is an eviction set and that it satisﬁes |S| = a upon
termination, see Lemma 1. For the proof of the complexity
bound observe that the number of memory accesses performed
by Algorithm 2 on a set S of size N follows the following
recurrence.
T (N ) = T (N
(2)
a
) + N · a
a + 1
for N > a, and T (a) = a. The recurrence holds because, on
input S, the algorithm applies threshold group tests on a + 1
subsets of S, each of size N − N
a+1. The overall cost for the
split and the tests is N · a. The algorithm recurses on exactly
one of these subsets of S, which has size N a
a+1. From the
Master theorem [17] it follows that T (N ) ∈ Θ(N ).
See Appendix B for a direct proof that also includes the
quadratic dependency on associativity.
C. Computing Minimal Eviction Set for an Arbitrary Address
The Algorithms presented so far compute minimal eviction
sets for a speciﬁc virtual address x. We now consider the case
of computing minimal eviction sets for an arbitrary address.
This case is interesting because, as shown in Section III-C, a
set of virtual addresses is more likely to evict any arbitrary
(cid:21)(cid:22)
address than a speciﬁc one. That is, in scenarios where the
target address is not relevant, one can start the reduction with
smaller candidate sets.
The key observation is that both Algorithm 1 and Algo-
rithm 2 can be easily adapted to compute eviction sets for
an arbitrary address. This only requires replacing the eviction
test for a speciﬁc address (Test 1) by an eviction test for an
arbitrary address (Test 3).
Proposition 3. Algorithm 1, with Test 3 for an arbitrary
eviction set, reduces an eviction set to its minimal core in
O(N 2) memory accesses, where N = |S|.
Proposition 4. Algorithm 2 with Test 3 reduces an eviction
set to its minimal core in O(N ) memory accesses, where N =
|S|.
The complexity bounds for computing eviction sets for an
arbitrary address coincide with those in Proposition 1 and 2
because Test 1 and Test 3 are both linear in the size of the
tested set.
D. Computing Minimal Eviction Sets for Many Virtual Ad-
dresses
We now discuss the case of ﬁnding eviction sets for a large
number of cache sets. For this we assume a given pool P
of virtual addresses, and explain how to compute minimal
eviction sets for all the eviction sets that are contained in P .
For large enough P the result can be a set of eviction sets for
all virtual addresses.
The core idea is to use a large enough subset of P and
reduce it to a minimal eviction set S for an arbitrary address,
say x. Use S to build a test TEST((S \ {x}) ∪ {y}, x) for
individual addresses y to be congruent with x. Use this test
to scan P and remove all elements that are congruent with x.
Repeat the procedure until no more eviction sets are found in
P . With a linear reduction using Algorithm 2, a linear scan,
and a constant number of cache sets, this procedure requires
O(|P|) memory accesses to identify all eviction sets in P .
Previous work [9] proposes a similar approach based on the
quadratic baseline reduction. The authors leverage the fact that,
on earlier Intel CPUs, given two congruent physical addresses
x (cid:2) y, then x+Δ (cid:2) y+Δ, for any offset Δ < 2γ. This implies
that, given one eviction set for each of the 2c−γ page colors,
one can immediately obtain 2γ−1 others by adding appropriate
offsets to each address. Unfortunately, with unknown slicing
−s, what increases
functions this only holds with probability 2
the attacker’s effort. Our linear-time algorithm helps scaling
to large numbers of eviction sets under those conditions.
Another solution to the problem of ﬁnding many eviction
sets has been proposed in [4]. This solution differs from
the two-step approach in that the algorithm ﬁrst constructs
a so-called conﬂict set, which is the union of all minimal
eviction sets contained in P , before performing a split into
the individual minimal eviction sets. The main advantage of
using conﬂict sets is that, once a minimal eviction set is found,
the conﬂict set need not be scanned for further congruent
addresses.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:41:36 UTC from IEEE Xplore.  Restrictions apply. 
V. EVALUATION
In this section we perform an evaluation of the algorithms
for computing minimal eviction sets we have developed in
Section IV. The evaluation complements our theoretical anal-
ysis along two dimensions:
Robustness: The theoretical analysis assumes that tests
for eviction sets always return the correct answer, which
results in provably correct reduction algorithms. In this section
we analyze the robustness of our algorithms in practice. In
particular, we study the inﬂuence of factors that are outside
of our model, such as adaptive cache replacement policies
and TLB activity. We identify conditions under which our
algorithms are almost perfectly reliable, as well as conditions
under which their reliability degrades. These insights can
be the basis of principled countermeasures against, or paths
forward for improving robustness of, algorithms for ﬁnding
eviction sets.
Execution time: The theoretical analysis captures the
performance of our algorithms in terms of the number of
memory accesses. As for the case of correctness, the real
execution time is inﬂuenced by factors that are outside of our
model, such as the total number of cache and TLB misses, or
the implementation details. In our empirical analysis we show
that the number of memory accesses is in fact a good predictor
for the asymptotic real-time performance of our algorithms.
A. Design of our Analysis
Implementation: We implemented the tests and algo-
rithms described in Sections III-B and IV as a command
line tool, which can be parameterized to ﬁnd minimal evic-
tion sets on different platforms. All of our experiments are
performed using the tool. The source code is available at:
https://github.com/cgvwzq/evsets.
Analyzed Platforms: We evaluate our algorithms on two
different CPUs running Linux 4.9:
1) Intel i5-6500 4 x 3.20 GHz (Skylake family), 16 GB of
RAM, and a 6 MB LLC with 8192 12-way cache sets. Our
experiments indicate that only 10 bits are used as set index
on this machine, we hence conclude that each core has 2
slices. Following our previous notation, i.e.: asky = 12, csky =
10, ssky = 3, (cid:2)sky = 6.
2) Intel i7-4790 8 x 3.60GHz GHz (Haswell family), 8 GB
of RAM, and a 8 MB LLC with 8192 16-way cache sets. This
machine has 4 physical cores and 4 slices. Following our pre-
vious notation, i.e.: ahas = 16, chas = 11, shas = 2, (cid:2)has = 6.
We emphasize that all experiments run on machines with user
operating systems (with a default window manager and back-
ground services), default kernel, and default BIOS settings.
Selection of Initial Search Space: We ﬁrst allocate a big
memory buffer as a pool of addresses from where we can
suitably chose the candidate sets (recall Section III-C). This
choice is done based on the adversary’s capabilities (i.e., γ),
for example, by collecting all addresses in the buffer using a
stride of 2γ+(cid:2), and then randomly selecting N of them. With
this method, we are able to simulate any amount of adversary
control over the set index bits, i.e. any γ with γ < p − (cid:2).
Isolating and Mitigating Interferences: We identify ways
to isolate two important sources of interference that affect
the reliability of our tests and hence the correctness of our
algorithms:
• Adaptive Replacement Policies: Both Skylake
and
Haswell employ mechanisms to adaptively switch between
undocumented cache replacement policies. Our experiments
indicate that Skylake keeps a few ﬁxed cache sets (for example,
the cache set zero) that seem to behave as PLRU and match
the assumptions of our model. Targeting such sets allows us
to isolate the effect of adaptive and unknown replacement
policies on the reliability of our algorithms.
• Translation Lookaside Buffers: Performing virtual mem-
ory translations during a test results in accesses to the TLB.
An increased number of translations can lead to an increased
number of TLB misses, which at the end trigger page walks.
These page walks result in implicit memory accesses that may
evict the target address from the cache, even though the set
under test is not an eviction set, i.e. it introduces a false
positive. TLB misses also introduce a noticeable delay on
time measurements, what has been recently discussed in a
concurrent work [18]. We isolate these effects by performing
experiments for pages of 4KB on huge pages of 2MB, but
under the assumption that, as for 4KB pages, only γ = 6 bits
of the set index are under attacker control.
We further rely on common techniques from the literature
to mitigate the inﬂuence of other sources of interference:
• For reducing the effect of hardware prefetching we use
a linked list to represent eviction sets, where each element
is a pointer to the next address. This ensure that all memory
accesses loads are executed in-order. We further randomize the
order of elements.
• For reducing the effect of jitter, we perform several time
measurements per test and compare their average value with
a threshold. In our experiments, 10 − 50 measurements are
sufﬁcient to reduce the interference of context switches and
other spurious events. More noisy environments (e.g. a web
browser) may require larger numbers.
B. Evaluating Robustness
We rely on two indicators for the robustness of our tests
and reduction algorithms:
• The eviction rate, which is the relative frequency of our
tests returning true on randomly selected sets of ﬁxed