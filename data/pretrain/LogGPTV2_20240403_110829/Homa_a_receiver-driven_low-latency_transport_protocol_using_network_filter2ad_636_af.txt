typically have at least four partially-received messages at any
given time, so they use all of the scheduled priority levels. More
scheduled packets arrive on the highest scheduled level than
any other; the other levels are used if the highest-priority sender
is nonresponsive or if the number of incoming messages drops
below 4. The figure indicates that senders are frequently nonre-
sponsive at 80% network load (more than half of the scheduled
traffic arrives on P0–P2).
Additional information. Page length restrictions forced us to
omit several portions of this section. A complete version of
the paper is available online [22] and includes the following
additional information:
∙ A more comprehensive description of our simulation en-
vironment and the parameters used in simulation.
0102030P7P6P5P4P3P2P1P0% Network BandwidthNetwork Load:50%80%90%SIGCOMM ’18, August 20-25, 2018, Budapest, Hungary
B. Montazeri et al.
NIC or even the TOR. We leave an exploration of this problem
and its potential solutions to future work.
Homa assumes that the most severe forms of incast are pre-
dictable because they are self-inflicted by outgoing RPCs; Homa
handles these situations effectively. Unpredictable incasts can
also occur, but Homa assumes that they are unlikely to have
high degree. Homa can handle unpredictable incasts of several
hundred messages with typical switch buffer capacities; un-
predictable incasts larger than this will cause packet loss and
degraded performance.
The Homa configuration and measurements in this paper
were based on 10 Gbps link speeds. As link speeds increase in
the future, RTTbytes will increase proportionally, and this will
impact the protocol in several ways. A larger fraction of traffic
will be sent unscheduled, so Homa’s use of multiple priority
levels for unscheduled packets will become more important.
With faster networks, workloads will behave more like W1 and
W2 in our measurements, rather than W3-W5. As RTTbytes
increases, each message can potentially consume more space
in switch buffers, and the degree of unpredictable incast that
Homa can support will drop.
7 RELATED WORK
In recent years there have been numerous proposals for new
transport protocols, driven by new datacenter applications and
the well-documented shortcomings of TCP. However, none of
these proposals combines the right set of features to produce
low latency for short messages under load.
The biggest shortcoming of most recent proposals is that
they do not take advantage of in-network priority queues. This
includes rate-control techniques such as DCTCP [2] and HULL [3],
which reduce queue occupancy, and D3 [32] and D2TCP [31],
which incorporate deadline-awareness. PDQ [17] adjusts flow
rates to implement preemption, but its rate calculation is too
slow for scheduling short messages. Without the use of prior-
ities, none of these systems can achieve the rapid preemption
needed by short messages.
A few systems have used in-network priorities, but they do
not implement SRPT. §5.2 showed that the PIAS priority mecha-
nism [6] performs worse than SRPT for most message sizes and
workloads. QJUMP [14] requires priorities to be specified man-
ually on a per-application basis. Karuna [7] uses priorities to
separate deadline and non-deadline flows, and requires a global
calculation for the non-deadline flows. Without receiver-driven
SRPT, none of these systems can achieve low latency for short
messages.
pFabric [4] implements SRPT by assuming fine-grained pri-
ority queues in network switches. Although this produces near-
optimal latencies, it depends on features not available in existing
switches.
pHost [13] and NDP [15] are the systems most similar to
Homa, in that both use receiver-driven scheduling and prior-
ities. pHost and NDP use only two priority levels with static
assignment, which results in poor latency for short messages.
Neither system uses overcommitment, which limits their ability
to operate at high network load. NDP uses fair-share scheduling
rather than SRPT, which results in high tail latencies. NDP in-
cludes an incast control mechanism, in which network switches
drop all but the first few bytes of incoming packets when there
is congestion. Homa’s incast control mechanism achieves a
similar effect using a software approach: instead of truncating
packets in-flight (which wastes network bandwidth), senders
are instructed by the protocol to limit how much data they send.
Almost all of the systems mentioned above, including DCTCP,
pFabric, PIAS, and NDP, use a connection-oriented streaming
approach. As previously discussed, this results in either high
tail latency because of head-of-line blocking at senders, or an
explosion of connections, which is impractical for large-scale
datacenter applications.
A final alternative is to schedule all messages or packets for a
cluster centrally, as in Fastpass [25]. However, communication
with the central scheduler adds too much latency to provide
good performance for short messages. In addition, scaling a sys-
tem like Fastpass to a large cluster is challenging, particularly
for workloads with many short messages.
calls, not byte streams.
mechanism that approximates SRPT.
8 CONCLUSION
The combination of tiny messages and low-latency networks cre-
ates challenges and opportunities that have not been addressed
by previous transport protocols. Homa meets this need with a
new transport architecture that combines several unusual fea-
tures:∙ It implements discrete messages for remote procedure
∙ It uses in-network priority queues with a hybrid allocation
∙ It manages most of the protocol from the receiver, not the
∙ It overcommits receiver downlinks in order to maximize
∙ It is connectionless and has no explicit acknowledgments.
These features combine to produce nearly optimal latency for
short messages across a variety of workloads. Even under high
loads, tail latencies are within a small factor of the hardware
limit. The remaining delays are almost entirely due to the ab-
sence of link-level packet preemption in current networks; there
is little room for improvement in the protocol itself. Finally,
Homa can be implemented with no changes to networking hard-
ware. We believe that Homa provides an attractive platform on
which to build low-latency datacenter applications.
throughput at high network loads.
sender.
9 ACKNOWLEDGMENTS
This work was supported by C-FAR (one of six centers of STAR-
net, a Semiconductor Research Corporation program, sponsored
by MARCO and DARPA) and by the industrial affiliates of the
Stanford Platform Laboratory. Amy Ousterhout, Henry Qin,
Jacqueline Speiser, and 14 anonymous reviewers provided help-
ful comments on drafts of this paper. We also thank our SIG-
COMM shepherd, Brighten Godfrey.
Homa: A Receiver-Driven Low-Latency Transport Protocol SIGCOMM ’18, August 20-25, 2018, Budapest, Hungary
REFERENCES
[1] M. Alizadeh, T. Edsall, S. Dharmapurikar, R. Vaidyanathan, K. Chu,
A. Fingerhut, V. T. Lam, F. Matus, R. Pan, N. Yadav, and G. Varghese.
CONGA: Distributed Congestion-aware Load Balancing for Datacenters.
In Proceedings of the ACM SIGCOMM 2014 Conference, SIGCOMM
’14, pages 503–514, New York, NY, USA, 2014. ACM.
[2] M. Alizadeh, A. Greenberg, D. A. Maltz, J. Padhye, P. Patel, B. Prabhakar,
S. Sengupta, and M. Sridharan. Data Center TCP (DCTCP).
In
Proceedings of the ACM SIGCOMM 2010 Conference, SIGCOMM ’10,
pages 63–74, New York, NY, USA, 2010. ACM.
[3] M. Alizadeh, A. Kabbani, T. Edsall, B. Prabhakar, A. Vahdat, and
M. Yasuda. Less is More: Trading a Little Bandwidth for Ultra-low
Latency in the Data Center. In Proceedings of the 9th USENIX Conference
on Networked Systems Design and Implementation, NSDI’12, pages
19–19, Berkeley, CA, USA, 2012. USENIX Association.
[4] M. Alizadeh, S. Yang, M. Sharif, S. Katti, N. McKeown, B. Prabhakar,
and S. Shenker. pFabric: Minimal Near-optimal Datacenter Transport.
In Proceedings of the ACM SIGCOMM 2013 Conference, SIGCOMM
’13, pages 435–446, New York, NY, USA, 2013. ACM.
[5] B. Atikoglu, Y. Xu, E. Frachtenberg, S. Jiang, and M. Paleczny. Workload
Analysis of a Large-scale Key-value Store. In Proceedings of the 12th
ACM SIGMETRICS/PERFORMANCE Joint International Conference
on Measurement and Modeling of Computer Systems, SIGMETRICS ’12,
pages 53–64, New York, NY, USA, 2012. ACM.
[6] W. Bai, L. Chen, K. Chen, D. Han, C. Tian, and H. Wang. Information-
agnostic Flow Scheduling for Commodity Data Centers. In Proceedings
of the 12th USENIX Conference on Networked Systems Design and
Implementation, NSDI’15, pages 455–468, Berkeley, CA, USA, 2015.
USENIX Association.
[7] L. Chen, K. Chen, W. Bai, and M. Alizadeh. Scheduling Mix-flows
in Commodity Datacenters with Karuna. In Proceedings of the ACM
SIGCOMM 2016 Conference, SIGCOMM ’16, pages 174–187, New
York, NY, USA, 2016. ACM.
[8] I. Cho, K. Jang, and D. Han. Credit-Scheduled Delay-Bounded Con-
gestion Control for Datacenters. In Proceedings of the ACM SIGCOMM
2017 Conference, SIGCOMM ’17, pages 239–252, New York, NY, USA,
2017. ACM.
[9] Data Plane Development Kit. http://dpdk.org/.
[10] A. Dixit, P. Prakash, Y. C. Hu, and R. R. Kompella. On the Impact of Packet
Spraying in Data Center Networks. In Proceedings of IEEE Infocom, 2013.
[11] A. Dragojevi´c, D. Narayanan, M. Castro, and O. Hodson. FaRM: Fast
Remote Memory. In 11th USENIX Symposium on Networked Systems
Design and Implementation (NSDI 14), pages 401–414, Seattle, WA, Apr.
2014. USENIX Association.
[12] B. Felderman. Personal communication, February 2018. Google.
[13] P. X. Gao, A. Narayan, G. Kumar, R. Agarwal, S. Ratnasamy, and
S. Shenker. pHost: Distributed Near-optimal Datacenter Transport over
Commodity Network Fabric. In Proceedings of the 11th ACM Conference
on Emerging Networking Experiments and Technologies, CoNEXT ’15,
pages 1:1–1:12, New York, NY, USA, 2015. ACM.
[14] M. P. Grosvenor, M. Schwarzkopf, I. Gog, R. N. M. Watson, A. W. Moore,
S. Hand, and J. Crowcroft. Queues Don’t Matter When You Can JUMP
Them! In 12th USENIX Symposium on Networked Systems Design and
Implementation (NSDI 15), pages 1–14, Oakland, CA, 2015. USENIX
Association.
[15] M. Handley, C. Raiciu, A. Agache, A. Voinescu, A. W. Moore, G. Antichik,
and M. Mojcik. Re-architecting Datacenter Networks and Stacks for Low
Latency and High Performance. In Proceedings of the ACM SIGCOMM
2017 Conference, SIGCOMM ’17, pages 29–42, New York, NY, USA,
2017. ACM.
[16] K. He, E. Rozner, K. Agarwal, W. Felter, J. Carter, and A. Akella.
Presto: Edge-based Load Balancing for Fast Datacenter Networks. In
Proceedings of the ACM SIGCOMM 2015 Conference, SIGCOMM ’15,
pages 465–478, New York, NY, USA, 2015. ACM.
[17] C.-Y. Hong, M. Caesar, and P. B. Godfrey. Finishing Flows Quickly with
Preemptive Scheduling. In Proceedings of the ACM SIGCOMM 2012
Conference, SIGCOMM ’12, pages 127–138, New York, NY, USA, 2012.
ACM.
[18] E. Jeong, S. Wood, M. Jamshed, H. Jeong, S. Ihm, D. Han, and K. Park.
mTCP: a Highly Scalable User-level TCP Stack for Multicore Systems. In
11th USENIX Symposium on Networked Systems Design and Implementa-
tion (NSDI 14), pages 489–502, Seattle, WA, 2014. USENIX Association.
[19] C. Lee, S. J. Park, A. Kejriwal, S. Matsushita, and J. Ousterhout.
Implementing Linearizability at Large Scale and Low Latency.
In
Proceedings of the 25th Symposium on Operating Systems Principles,
SOSP ’15, pages 71–86, New York, NY, USA, 2015. ACM.
[20] memcached:
a Distributed Memory Object Caching System.
http://www.memcached.org/, Jan. 2011.
[21] R. Mittal, V. T. Lam, N. Dukkipati, E. Blem, H. Wassel, M. Ghobadi,
A. Vahdat, Y. Wang, D. Wetherall, and D. Zats. TIMELY: RTT-based
In Proceedings of the 2015
Congestion Control for the Datacenter.
ACM Conference on Special Interest Group on Data Communication,
SIGCOMM ’15, pages 537–550, New York, NY, USA, 2015. ACM.
[22] B. Montazeri, Y. Li, M. Alizadeh, and J. K. Ousterhout. Homa: A
Receiver-Driven Low-Latency Transport Protocol Using Network Pri-
orities (Complete Version). CoRR, http://arxiv.org/abs/1803.09615, 2018.
[23] R. Nishtala, H. Fugal, S. Grimm, M. Kwiatkowski, H. Lee, H. C. Li,
R. McElroy, M. Paleczny, D. Peek, P. Saab, D. Stafford, T. Tung, and
V. Venkataramani. Scaling Memcache at Facebook. In 10th USENIX
Symposium on Networked Systems Design and Implementation (NSDI
13), pages 385–398, Lombard, IL, 2013. USENIX.
[24] J. Ousterhout, A. Gopalan, A. Gupta, A. Kejriwal, C. Lee, B. Montazeri,
D. Ongaro, S. J. Park, H. Qin, M. Rosenblum, et al. The RAMCloud
Storage System. ACM Transactions on Computer Systems (TOCS),
33(3):7, 2015.
[25] J. Perry, A. Ousterhout, H. Balakrishnan, D. Shah, and H. Fugal. Fastpass:
A Centralized “Zero-queue” Datacenter Network. In Proceedings of the
ACM SIGCOMM 2014 Conference, SIGCOMM ’14, pages 307–318,
New York, NY, USA, 2014. ACM.
[26] Redis, Mar. 2015. http://redis.io.
[27] A. Roy, H. Zeng, J. Bagga, G. Porter, and A. C. Snoeren. Inside the Social
Network’s (Datacenter) Network. In Proceedings of the ACM SIGCOMM
2015 Conference, SIGCOMM ’15, pages 123–137, New York, NY, USA,
2015. ACM.
[28] T. Shanley.
Infiniband Network Architecture.
Addison-Wesley
[29] R. Sivaram. Some Measured Google Flow Sizes (2008). Google internal
Professional, 2003.
memo, available on request.
[30] BCM56960 Series: High-Density 25/100 Gigabit Ethernet StrataXGS
Tomahawk Ethernet Switch Series. https://www.broadcom.com/products/
ethernet-connectivity/switching/strataxgs/bcm56960-series.
[31] B. Vamanan, J. Hasan, and T. Vijaykumar. Deadline-aware Datacenter
TCP (D2TCP). In Proceedings of the ACM SIGCOMM 2012 Conference,
SIGCOMM ’12, pages 115–126, New York, NY, USA, 2012. ACM.
[32] C. Wilson, H. Ballani, T. Karagiannis, and A. Rowtron. Better Never
Than Late: Meeting Deadlines in Datacenter Networks. In Proceedings
of the ACM SIGCOMM 2011 Conference, SIGCOMM ’11, pages 50–61,
New York, NY, USA, 2011. ACM.
[33] D. Zats, T. Das, P. Mohan, D. Borthakur, and R. Katz. Detail: Reducing
the flow completion time tail in datacenter networks. In Proceedings of
the ACM SIGCOMM 2012 Conference on Applications, Technologies,
Architectures, and Protocols for Computer Communication, SIGCOMM
’12, pages 139–150, New York, NY, USA, 2012. ACM.
[34] Y. Zhu, H. Eran, D. Firestone, C. Guo, M. Lipshteyn, Y. Liron, J. Padhye,
S. Raindel, M. H. Yahia, and M. Zhang. Congestion Control for Large-
Scale RDMA Deployments. In Proceedings of the 2015 ACM Conference
on Special Interest Group on Data Communication, SIGCOMM ’15,
pages 523–536, New York, NY, USA, 2015. ACM.