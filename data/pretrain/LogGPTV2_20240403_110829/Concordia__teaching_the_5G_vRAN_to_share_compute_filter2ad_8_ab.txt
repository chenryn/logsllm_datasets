queue, waiting to be processed by a worker thread. The vRAN pool
can support more than one priority queues and each worker thread
can be associated with one or more queues, allowing a fine-grained
control of the assignment of signal processing tasks to CPU cores. To
minimize the latency, the worker threads are typically configured to
use a high priority scheduling policy (e.g. SCHED_FIFO in Linux)
that can only be preempted by the highest priority kernel threads (e.g.
watchdog threads). Each worker thread checks the priority queue(s)
(a) CDF of single cell and 3 cells ag-
gregate
(b) Traffic fluctuation in 3 cells ag-
gregate
Figure 3: LTE cell traffic characteristics.
and picks the earliest deadline task (Earliest Deadline First - EDF).
Once the task processing is finished, the worker thread generates
zero or more new tasks according to the corresponding DAG model.
The worker thread can keep one of the generated tasks to process
next for improved cache efficiency, while the rest are placed back in
the priority queue to be picked up by another worker. If the queue is
empty, a worker thread can choose to either busy wait to minimize the
latency (leading to 100% core utilization) or to yield, allowing other
workloads to run. Once more tasks are generated (e.g. by another
worker thread), a sleeping thread is signaled to wake up and restart
processing.
2.2 Sharing opportunities
A common practice is to pool requests from multiple cells on the
same vRAN pool to exploit statistical multiplexing. However, cell
traffic is bursty in nature at much finer time scales, due to a number of
factors (e.g., number of active users, their signal quality, the behavior
of higher layer protocols, etc [6, 16, 53]). Therefore, pooled traffic
is bursty even when aggregated. This can be observed in Fig. 3, for
a 10s snippet of an 1 hour uplink traffic trace captured during rush
hour (around 12pm) from three neighboring LTE cells in the area
around the central train station of Cambridge UK, using the Falcon
sniffer [33]. We see that the changes in the traffic size happen at a
millisecond time scale. Moreover, a single cell is completely idle
75% of the TTI slots. If a vRAN pool aggregates 3 cells, it is only
idle 20% of the TTI slots, but still mostly processes short packets
and a median transfer size per slot is 0.2KB, which is 10√ó less than
the 95th percentile. If we provision the vRAN pool compute capacity
for peak traffic, it will be substantially underused most of the time.
We verify the same happens for the entire hour we measured. Similar
observations are drawn from the works and traffic traces in [6, 104].
582
01234Traffic size for 1ms TTI (KB)0.20.30.40.50.60.70.80.91.0CDF1 cell3 cells0200040006000800010000Time(ms)012345Transfer size for 1 ms TTI (KB)60065070002SIGCOMM ‚Äô21, August 23‚Äì27, 2021, Virtual Event, USA
Xenofon Foukas and Bozidar Radunovic
Config
UL only
(3 cells)
TDD
(1 cell)
TDD
(2 cells)
# cores
Avg CPU
util (%)
4
5
12
42
38
33
(a) vRAN CPU utilization (UL ‚Äì up-
link, TDD ‚Äì standard 5G time divi-
sion between UL/DL)
(b) Slot processing deadline viola-
tions
Figure 4: vRAN CPU utilization and interference effects
To get an intuition of what happens with larger pool sizes, con-
sider ùëõ cells, each with transfer sizes modeled as a simple Gaussian
N(ùúá, ùúé2). The aggregate traffic is then N(ùëõùúá, ùëõùúé2), with the average
traffic growing linearly and the variance growing as a square root. The
peak to average ratio diminishes with ùëõ, but the actual wasted CPU
cycles are proportional to the standard deviation (the difference be-
tween peak and average), and grow proportionally with ‚àö
ùëõ. Thus, the
problem persists even in the ideal pooling case with very large pools
and uniform traffic per cell, something that rarely happens in practice.
Further statistical multiplexing opportunities arise in the common
5G deployment scenario of time division multiplexing, due to the
significant difference in the compute requirements of the uplink and
downlink processing [72, 107]. To quantify this, we set up a vRAN
pool using Intel FlexRAN v20.02 on an optimized server (described
in Section 6). We deploy workloads similar to the one reported above,
varying the number of cells and the type of traffic. For each case we
measure the CPU utilization and the minimum number of CPU cores
required to process the peak traffic. As shown in Fig 4a, the average
utilization of the required cores for any of the scenarios under study
is at most 42%.
The aforementioned observations, along with the fact that the
traffic load of cells can greatly fluctuate throughout the day, as various
studies of real mobile networks have revealed [104, 117], creates
an opportunity for significantly improving the utilization of edge
servers, by sharing the compute resources left idle by the vRAN with
other collocated workloads.
2.3 Challenges in sharing vRAN
To exploit the sharing opportunities described above, a vRAN pool
has to meet deadlines with high reliability. This is challenging on
a general purpose compute environment, even optimized for low-
latency [21]. To illustrate this, we study the scenarios of Fig. 4a
and measure how the vRAN processing latency is affected by other
workloads. We consider three cases; (i) the vRAN pool is running
in isolation (recommended FlexRAN configuration [49]), (ii) two
Nginx servers are running in containers on the same CPU cores as
the vRAN pool, saturated with HTTP requests and (iii) two Redis
containers are running on the same cores as the vRAN pool, saturated
with GET/SET operations. In all cases, we use the default FlexRAN
setup where the vRAN workload is running with maximum real-time
priority and the other workloads are running only when a vRAN
pool worker thread is idle and yields.
583
Figure 5: High-level design of Concordia
For each case we run a 5 minutes experiment and measure the
99.99% processing latency of the signal processing tasks. The DAG
deadline is set to 1.5ms following the requirement of 5G enhanced
Mobile Broadband services (eMBB) for a one way processing de-
lay below 4ms [32], including MAC/RLC processing and fronthaul
transport delay. As shown in Fig 4b, the processing latency is below
the deadline for all the cell deployment scenarios in the isolated case.
However, the tail latency significantly increases with the introduction
of other workloads, violating the required 99.999% reliability.
The tail latency increase occurs for two main reasons:
Scheduling latency The Linux kernel can introduce latencies that,
depending on the kernel configuration, can vary from tens of mi-
croseconds to tens of milliseconds [66, 88]. The main reason is
that parts of the kernel are non-preemptible (even with real time
patches) [88]. Therefore, the high priority vRAN worker threads can
be delayed from reclaiming a CPU core once they yield if the kernel
has taken control (e.g. due to interrupts, RCU operations or system
calls from a workload sharing the core). We quantify these effects
in Section 6.2.
Cache interference Multiple studies have shown that the perfor-
mance of collocated workloads can be severely affected by uncon-
trolled cache interference [26, 27, 41, 56, 92]. In the case of the Last
Level Cache (LLC), which is shared among cores, workloads do not
even have to be collocated on the same core for performance degra-
dation to occur. We measure and discuss these effects in Section 6.2.
For these reasons, a standard operational practice is to isolate
the vRAN from other workloads as much as possible by dedicat-
ing cores and LLC cache (e.g. as recommended by the OpenNESS
project [74] for FlexRAN [21]) though most of today‚Äôs deployments
run on completely isolated servers.
3 SYSTEM DESIGN
An overview of Concordia is shown in Fig 5. It is composed of the
Concordia WCET predictor and the Concordia scheduler.
Concordia WCET predictor: The key component of Concordia
is a novel predictor that provides an accurate WCET prediction of
each RAN task in a DAG. At the beginning of each TTI slot, the
predictor takes as input a set of features ùëã describing the state of
the base station (e.g. number of scheduled UEs and their transport
block sizes, number of layers, etc.). For each signal processing task
that has to execute in the slot, the predictor evaluates its individual
prediction model and sends a WCET prediction to the Concordia
scheduler based on the input features ùëã. Once the tasks are executed,
the predictor uses the observed runtimes to improve the model for
subsequent slots by adjusting predictions depending on the impact of
collocated workloads, effectively dealing with the problem of cache
interference. All this is discussed in detail in Section 4.
UL only(3 cells)TDD(1 cell)TDD(2 cells)0.000.250.500.751.001.251.5099.99% latency (ms)UL deadlineIsolatedNginxRedisConcordia: Teaching 5G vRAN to Share Compute
SIGCOMM ‚Äô21, August 23‚Äì27, 2021, Virtual Event, USA
Concordia scheduler: Concordia leverages ideas from the mixed-
criticality literature to perform its scheduling(see [17] for a compre-
hensive survey). It treats the vRAN as the high-priority workload,
and all other workloads as best-effort. Such workloads are allowed to
use the CPU cores which have not been allocated to the vRAN DAGs,
or the ones released by the vRAN DAGs due to early completion.
Concordia uses the state-of-the-art federated scheduling algorithm for
parallel tasks (DAGs) from [61] to decide the number of cores to allo-
cate to the vRAN tasks. In a nutshell, the algorithm uses the longest
path of the DAG and the predicted WCET of each DAG task to esti-
mate the predicted execution time and whether to increase or decrease
the number of cores allocated to the RAN, based on the deadline of
the DAG. If the remaining time until the DAG deadline is too small,
the algorithm gets into a critical stage where it allocates all cores to
the RAN, evicting all best-effort workloads. We defer the reader to Ta-
ble 3 of [61] for details on the Concordia core allocation strategy. The
predicted WCETs are not always sufficiently accurate to ensure that a
RAN DAG will meet its deadline. Also, some CPU cores might take
longer to wake up when scheduled (e.g., due to the scheduling latency
issue discussed in Section 2.3). To improve on these mispredictions,
the Concordia scheduler updates its decisions every 20 ùúás.
4 CONCORDIA WCET PREDICTOR
We begin by discussing the challenges of parameterized prediction
for the vRAN tasks‚Äô WCET. We then present the novel parameterized
WCET predictor we designed for Concordia.
vRAN tasks WCET prediction challenges
4.1
We illustrate and quantify the WCET prediction challenges on the ex-
ample of 5G (LDPC) decoding, since according to our measurements
(see Appendix A.1) it is the most expensive task and can consume
more than 60% of the total uplink processing time and more than 50%
of the total processing time (both uplink and downlink). However,
we have verified that the same observations and conclusions apply to
other significant tasks, like encoding (>40% of the downlink process-
ing), channel estimation (>8% of the uplink processing), equalization
(>5% of the uplink processing) and modulation/precoding (>25% of
the downlink processing). A holistic view of the system that includes
all the signal processing tasks is studied in more depth in Section 6.
1. Parameterized task runtime prediction is non-linear: Both the
average times and WCETs of signal processing tasks often linearly in-
crease with the input size [40, 103]. However, other parameters, such
as the number of CPU cores or the SNR and link adaptation of the
mobile users, may have a non-linear impact on the execution times.
This is illustrated in Fig. 6a for the case of 120K LDPC decoding
operations on a group of codeblocks (8448 bits per codeblock). All
operations are generated on a single CPU core. While the runtime de-
pends linearly on the number of LDPC codeblocks, the dependence
on the number of CPU cores is not linear. When the data is decoded
across multiple cores (cases of 4 and 6 CPU cores in Fig. 6a), the
decoding core needs to fetch the required data, causing CPU memory
stalls (Fig 6b). This can increase the WCET by up to 25% from the
single core case. The exact overhead depends on multiple factors,
including the number of UEs transmitting/receiving data, the trans-
port block size, the level of parallelization supported by the vRAN
implementation, the number of CPU cores etc. Similar observations
(a) Runtime violin plot
(b) Memory stalls
Figure 6: Runtime characteristics for LDPC decoding for differ-
ent codeblock assignments.
have been made in the literature for the piecewise-linear effect of the
mobile user SNR and link adaptation to decoding (e.g. [5, 12, 89]).
2. vRAN task runtimes are affected by cache interference: The
cache interference caused by collocated workloads (Section 2.3) has a
direct impact on the runtime distribution of vRAN tasks. The change
of the distribution means that any model used for the prediction of
the WCETs must be retrained frequently (every few ms) using online
samples, to adapt to the various (and possibly unknown) collocated
workloads. To show this in practice, we repeat the previous exper-
iment over 4 CPU cores when the vRAN is running in isolation, as
well as with collocated workloads (Redis, SQL server). By running
the KS test [69] on our collected runtimes for the three cases (isolated,
Redis, SQL server), we obtain ùëù-values << 0.001. This verifies that
the runtimes in the case of interfering workloads are not drawn from
the same distribution as in the isolated case, meaning that any model
trained against the isolated RAN samples will need to be retrained
online for improved accuracy.
4.2 Concordia WCET prediction model
We now present the detailed design of the WCET prediction model
of Concordia that, (i) makes parameterized WCET predictions con-
sidering the effect of the tasks‚Äô inputs to their runtime, and (ii) takes
into account the challenges of Section 4.1.
High-level description of prediction mechanism: At a high level,
the Concordia predictor maintains a separate quantile decision tree [70,
93] for each vRAN task, with training runtime samples stored in its
leaf nodes. It provides a WCET prediction for the task with a given
set of input parameters (or features) ùëã using the maximum of the run-
times stored in the corresponding leaf node. The predictor builds the
decision trees in an offline phase, using runtime samples measured
for test vRAN workloads running without other collocated workloads.
Then in an online phase, during regular operation, the predictor up-
dates the runtime samples in each leaf without changing the tree
structure. The intuition is that the tree splits the input feature space
for the training set so that each leaf node ends up having a set of simi-
lar runtime samples. This is because it uses the CART algorithm [57]
to minimize the variance among the samples that end up in the same
leaf. We can then build and maintain separate simple predictors for