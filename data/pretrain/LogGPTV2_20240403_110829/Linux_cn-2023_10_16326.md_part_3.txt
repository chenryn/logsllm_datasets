在完成这些准备工作之后，如果我们就在这里结束，任何代码或数学概念都不讲，那将是一种罪过。因此，我们将继续学习人工智能和机器学习中最重要的数学概念：向量和矩阵。
#### 向量和矩阵
矩阵是按行和列排列的数字、符号或数学表达式构成的矩形阵列。图 2 显示了一个 2 × 3 矩阵，它有 2 行和 3 列。如果你熟悉编程，在许多流行的编程语言中这个矩阵可以表示为一个二维数组。只有一行的矩阵称为行向量，只有一列的矩阵称为列向量。 ![](/data/attachment/album/202310/29/145931wm9voyym8t9h1tqo.svg) 就是一个行向量。
![图 2：一个: A 2 × 3 的矩阵](/data/attachment/album/202310/29/155723eqy1zb6mqxllg66x.jpg)
为什么矩阵和向量在人工智能和机器学习中如此重要呢？人工智能和机器学习中广泛使用线性代数，而矩阵和向量是线性代数的核心。几个世纪以来，数学家们一直在研究矩阵和向量的性质和应用。高斯、欧拉、莱布尼茨、凯利、克莱姆和汉密尔顿等数学家在线性代数和矩阵论领域都有以他们的名字命名的定理。多年来，线性代数中发展出了许多分析矩阵和向量性质的技术。
复杂的数据通常可以很容易用向量或矩阵来表示。举一个简单的例子，从一个人的医疗记录中，可以得到详细的年龄、身高（厘米）、体重（公斤）、收缩压、舒张压和空腹血糖（毫克/分升）。这些信息可以很容易用行向量来表示， ![](/data/attachment/album/202310/29/155156vhtjgpgpmr2gp2bp.svg) 。人工智能和机器学习的第一个挑战来了：如果医疗记录有十亿条怎么办？即使动用成千上万的专业人员从中手动提取数据，这项任务也是无法完成的。因此，人工智能和机器学习利用程序来提取数据。
人工智能和机器学习的第二个挑战是数据解读。这是一个广阔的领域，有许多技术值得探索。我将在后续文章中介绍相关内容。人工智能和机器学习应用除了面临数学/计算方面的挑战外，还面临硬件方面的挑战。随着处理的数据量的增加，数据存储、处理器速度、功耗等也成为人工智能应用面临的重要挑战。但现在让我们先抛开这些挑战，动手编写第一行人工智能代码。
我们将编写一个简单的 Python 脚本，用来将两个向量相加。我们将用到名为 NumPy 的 Python 库，它支持多维矩阵（数组）的数学运算。用命令 `pip3 install numpy` 为 Python 3 安装 NumPy 包。如果你使用的是 JupyterLab、谷歌 Colab 或 Anaconda，那么 NumPy 应该已经被预安装了。但是为了演示，在本系列的前几篇文章中，我们都将在 Linux 终端上操作。在 Linux 终端上执行命令  `python3` 进入 Python 控制台。在这个控制台中可以逐行执行 Python 代码。图 3 展示了在控制台中逐行运行 Python 代码，将两个向量相加，并输出结果。
![图 3：两个向量求和的 Python 代码](/data/attachment/album/202310/29/155723ol14fa11o0paf035.jpg)
首先，让我们试着逐行理解这些代码。由于本教程假定的编程经验很少，所以我将代码行标记为【基本】或【AI】。标记为【基本】的行是经典 Python 代码，标记为【AI】的行是用于开发人工智能程序的代码。通过区分基本和进阶的 Python 代码，我希望具有基本知识和中级编程技能的程序员都能够高效地使用本教程。
```
import numpy as np         #【基本】
a = np.array([11, 22, 33]) #【AI】
b = np.array([44, 55, 66]) #【AI】
c = np.add(a, b)           #【AI】
print(c)                   #【基本】
```
`import numpy as np` 导入 numpy 库并将其命名为 `np`。Python 中的 `import` 语句类似于在 C/C++ 用 `#include` 来包含头文件，或者在 Java 中用`import` 来使用包。
`a = np.array([11, 22, 33])` 和 `b = np.array([44, 55, 66])`  分别创建了名为 `a` 和 `b` 的一维数组（为了便于理解，目前假设向量等价于一维数组）。
`c = np.add(a, b)` 将向量 `a` 和`b` 相加，并将结果存储在名为 `c` 的向量中。当然，用 `a`，`b`，`c` 作为变量名是一种糟糕的编程实践，但数学家倾向于将向量命名为 ![](/data/attachment/album/202310/29/155226dmhqqoqh6vmwvtie.svg)、 ![](/data/attachment/album/202310/29/155233hgadvvi33dho3d3n.svg)、 ![](/data/attachment/album/202310/29/155239palkfhfz9f30x0uu.svg) 等。如果你完全没有 Python 编程经验，请自行了解 Python 变量的相关知识。
`print(c)` 在终端上打印对象的值，即向量 `[55 77 99]`。你可以暂时这样理解向量相加, `c = [55=11+44 77=22+55 99=33+66]`。如果你想正式地了解向量和矩阵是如何相加的，但手头又没有相关的教材，我建议阅读维基百科上关于矩阵加法的文章。在网上搜索一下就会发现，用经典的 C/C++ 或 Java 程序来实现向量相加需要更多的代码。这说明 Python 很适合处理向量和矩阵。当我们执行越来越复杂的向量运算时，Python 的强大将进一步显现。
在我们结束本文之前，我要做两个声明。第一，上面讨论的示例只处理了两个行向量（确切地说是 1 x 3 的矩阵）的相加，但真正的机器学习应用可能要处理 1000000 X 1000000 的矩阵。但不用担心，通过练习和耐心，我们将能够处理这些问题。第二，本文中给出许多定义包含了粗略的简化和不充分的描述。但如前面所说，在本系列结束之前，我将给这些模糊的术语下一个正式的定义。
现在我们该结束这篇文章了。我希望所有人都安装文中提到的必要软件，并运行本文中的代码。在下一篇文章中，我们将首先讨论人工智能的历史、范畴和未来，然后深入探讨线性代数的支柱——矩阵论。
*（题图：MJ/25071901-abc4-4144-bf27-4d98bb1d9301/）*
---
via: 
作者：[Deepu Benson](https://www.opensourceforu.com/author/deepu-benson/) 选题：[lkxed](https://github.com/lkxed) 译者：[toknow-gh](https://github.com/toknow-gh) 校对：[wxy](https://github.com/wxy)
本文由 [LCTT](https://github.com/LCTT/TranslateProject) 原创编译，[Linux中国](https://linux.cn/) 荣誉推出