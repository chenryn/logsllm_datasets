IEEE, 2008, pp. 81–95.
[97] J. Steinhardt, P. W. W. Koh, and P. S. Liang, “Certiﬁed Defenses for
Data Poisoning Attacks,” in Advances in neural information processing
systems, 2017, pp. 3517–3529.
[98] D. Meng and H. Chen, “MagNet: a Two-Pronged Defense against
Adversarial Examples,” in Proceedings of the 2017 ACM SIGSAC
Conference on Computer and Communications Security. ACM, 2017,
pp. 135–147.
[99] R. Jordaney, K. Sharad, S. K. Dash, Z. Wang, D. Papini, I. Nouret-
dinov, and L. Cavallaro, “Transcend: Detecting Concept Drift in Mal-
ware Classiﬁcation Models,” in 26th {USENIX} Security Symposium
({USENIX} Security 17), 2017, pp. 625–642.
[100] B. Biggio, I. Corona, Z.-M. He, P. P. Chan, G. Giacinto, D. S. Yeung,
and F. Roli, “One-and-a-half-class Multiple Classiﬁer Systems for Se-
cure Learning Against Evasion Attacks at Test Time,” in International
Workshop on Multiple Classiﬁer Systems.
Springer, 2015, pp. 168–
180.
[101] A. Bendale and T. E. Boult, “Towards Open Set Deep Networks,” in
Proceedings of the IEEE conference on computer vision and pattern
recognition, 2016, pp. 1563–1572.
[102] W. Xu, D. Evans, and Y. Qi, “Feature Squeezing: Detecting Adversarial
Examples in Deep Neural Networks,” arXiv preprint arXiv:1704.01155,
2017.
[103] F. Tram`er, A. Kurakin, N. Papernot, D. Boneh, and P. McDaniel,
“Ensemble Adversarial Training: Attacks and Defenses,” arXiv preprint
arXiv:1705.07204, 2017.
[104] M. Lecuyer, V. Atlidakis, R. Geambasu, D. Hsu, and S. Jana, “Certiﬁed
Robustness to Adversarial Examples with Differential Privacy,” in 2019
IEEE Symposium on Security and Privacy (SP).
IEEE, 2019, pp. 656–
672.
[105] D.
J. Plude,
J. T. Enns, and D. Brodeur, “The development
of selective attention: A life-span overview,” Acta Psychologica,
vol. 86, no. 2, pp. 227 – 272, 1994.
[Online]. Available:
”http://www.sciencedirect.com/science/article/pii/0001691894900043”
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:09:49 UTC from IEEE Xplore.  Restrictions apply. 
744
[106] W. G. on Speech Understanding and Aging, “Speech understanding
and aging,” The Journal of the Acoustical Society of America, vol. 83,
no. 3, pp. 859–895, 1988.
[107] F. Pulvermuller and Y. Shtyrov, “Language outside the focus of
attention: The mismatch negativity as a tool for studying higher
cognitive processes,” Progress in Neurobiology, vol. 79, no. 1, pp.
49 – 71, 2006. [Online]. Available: ”http://www.sciencedirect.com/
science/article/pii/S0301008206000323”
[108] C. Limb,
“Building
2011.
[Online]. Available: ”https://www.ted.com/tals/charles limb building
the musical muscle#t-367224”
the musical muscle,” TEDMED,
[109] P. ITU, “863 “perceptual objective listening quality prediction”,” 2018.
[110] M. R. Schroeder, “Models of Hearing,” Proceedings of the IEEE,
vol. 63, no. 9, pp. 1332–1350, 1975.
[111] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech:
an ASR corpus based on public domain audio books,” in 2015 IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP).
IEEE, 2015, pp. 5206–5210.
[112] A. Kurakin, I. Goodfellow, and S. Bengio, “Adversarial machine
learning at scale,” arXiv preprint arXiv:1611.01236, 2016.
X. ACKNOWLEDGMENT
We would like to thank our reviewers for their insightful
comments and suggestions. This work was supported in part
by the Defense Advanced Research Projects Agency (DARPA)
under agreement number HR0011202008, the National Sci-
ence Foundation under grant number CNS-1933208. NP ac-
knowledges funding from CIFAR through a Canada CIFAR
AI chair, and from NSERC under the Discovery Program and
COHESA strategic research network. Any opinions, ﬁndings,
and conclusions or recommendations expressed in this material
are those of the authors and do not necessarily reﬂect the views
of the above listed agencies.
A. Transferability
XI. APPENDIX
The transferability property of adversarial samples is a
cornerstone of black-box attacks in the image domain (Sec-
tion II-D3). In contrast, transferability has had varying success
in the audio domain. While signal processing attacks can
generate transferable samples, transferability of gradient-based
optimization attacks is unclear. While most attack papers
do not test for transferability, those that have attempted to,
have largely demonstrated unsuccessful results [72], [41].
Considering this major discrepancy between the image and
audio domains, we explore experimentally the transferability
question for VPSes. Our goal is to ascertain whether adversar-
ial audio samples generated via optimization attacks transfer.
B. Setup
1) Training: Testing for transferability requires training a
number of ASRs. We use DeepSpeech [95] for this experiment,
even though there are a variety of ASR architectures in use
today. This is motivated by two reasons. First, DeepSpeech
employs NNs. NN architectures are the most popular and
widely used, allowing us to make conclusions applicable
to a wider population. Second, as researchers and vendors
increasingly phase out non-NN architectures in favor of NNs,
our conclusions will be applicable for future systems.
We trained nine DeepSpeech ASRs to achieve the state-
of-the-art Word Error Rate (WER) of 8% [95]. These ASRs
Target Transcription
delete my messages
browse to evil website
what is the weather today
go to evil website
open the door
transfer money to my account
the fault dear is not in our stars
turn off all the cameras
text mom i need money
order me some candy
take a picture
Benign Samples
Perturbed
98
98
98
98
98
98
98
98
98
98
98
Total
Adversarial
Audio Generated
1867
1086
1525
1652
2253
1180
1312
1401
1400
1812
1755
17243
Table V: The table above shows the results of the transferabil-
ity experiments for the Carlini et al. attack. The attack was run
for 1000 iterations for each of the 98 audio ﬁles. There are
a varying number of adversarial samples produced for each
target transcription. This is because we generated both high
conﬁdence (high distortion) and low conﬁdence (low distor-
tion). For example, audio1.wav with target “open the door”
may be perturbed to generate three high conﬁdence (greater
than 0.9) samples at iteration 103, 110, and 200. However,
for audio2.wav with target “text mom I need money”, no
high conﬁdence adversarial sample may be possible. Each
of the generated adversarial samples (e.g, 2253 for malicious
command “open the door”) was then passed to eight models
trained with different initial seeds and one model trained on the
same seed. None of the 17243 samples transfered successfully.
were trained on a cluster of GeForce RTX 2080 Ti GPUs, had
2048 hidden units per layer, were trained on the LibriSpeech
dataset [111] for 13 epochs, and took approximately two
days of training each (a total of 180 hours of training time).
This training setup closely resembles the ofﬁcial DeepSpeech
documentation [95]. All the ASRs were trained on the same
train-test splits and hyper-parameters, except initial random
seed. This was the only parameter that was varied, with each
ASR being trained on a unique seed value. This is done to
emulate the attacker who has perfect knowledge of the ASR’s
training parameters, except the initial random seed. Next, we
trained ASRs with the same hyper-parameters, including the
seed. This is the best-case scenario for the attacker as she has
absolute knowledge of the ASR’s training parameters.
2) Adversarial Sample Generation: While a number of
optimization attacks exist, it is impractical to evaluate ev-
ery attack with respect
to transferability. Luckily, existing
optimization attacks do follow the same generic template.
These attacks minimize loss functions, use partial derivatives
to compute the model’s sensitives to the inputs to perturb the
input and threshold audible distortion using similar metrics
(generally the L2-norm). One such representative attack is
Carlini et al. [6], which provides the added advantage of being
effective against NN based ASRs. Additionally, other opti-
mization attacks have been built directly [74] or indirectly [77]
on this attack. Consequently, we choose the Carlini et al. [6]
to perturb attack audio samples.
Next, we create a set of audio samples that will be perturbed
using the adversarial algorithm. We follow the methodology
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:09:49 UTC from IEEE Xplore.  Restrictions apply. 
745
outlined in Liu et al. [51]. We pass the test set samples from
the LibriSpeech dataset to each of our ASR models and pick
the 98 samples that all the models transcribed correctly. This
methodology is ideal for two reasons. First, by sampling from
the test set, we ensure that our audio samples lie within the
same distribution as the training data, ensuring consistency.
Second, we want our experimental setup to allow for the
highest chance of transferability. This enables us to make less
error prone conclusions.
We perturbed the set of 98 audio samples that all ASRs
transcribed correctly. These audio samples were perturbed
using 1000 iterations of the attack to force the ASR, called
the surrogate, to produce a speciﬁc target transcription. We tar-
get a total of 11 transcriptions, shown in Table V. The attacks
were used to generate two types of adversarial audio samples:
high audible distortion (and high transcription conﬁdence) and
low audible noise (and low transcription conﬁdence). This was
done speciﬁcally to explore whether samples with high audible
distortion transfer better than ones with low distortion.
Transferring a targeted transcription is not as easy as we
want the target ASR to assign a speciﬁc text to the audio.
This is because, in the decision space, an adversarially chosen
transcription might be very far from the original one. If on
the other hand, the chosen transcription is very close the
original one, then transferability is more likely. The closest
transcription to the original, in the decision space, is the one
with the second highest probability after the original. For
example, if the original transcription is “Mary had a little
lamb”, the second most likely transcriptions might be “Mary
belittled a lamb”. Intuitively, it will be easier to move from
“Mary had a little lamb”to “Mary belittled a lamb” than to
“open the door”. This should be the case as we train the models
on the same data partitions and the vocabulary. However, if
transferability is not possible in this ideal case, than it is very
unlikely for more realistic cases.
To answer whether transferability is possible at all i.e.,
in the easiest case, we designed the following experiment.
We perturbed each audio sample to produce two adversarial
samples with the second and third most likely transcriptions
as our target attack transcriptions. For example, consider the
original audio sample that correctly transcribes to “Mary had a
little lamb”. It’s second and third most likely transcriptions are
“Mary belittled a lamb” and “Mary had a spittle blam”. We use
these as the targets, instead of using a malicious one (“open
the door”). Then we repeat the adversarial audio perturbation
steps from the previous experiments.
3) Transferring Samples: Next, the adversarial samples are
passed to the ASRs, which we refer to as the remote targets.
The transferability is considered a success if the remote target
ASRs transcribe the attack audio sample as the attacker’s
chosen text. We count the number of times transferability
succeeded and use it as a metric for transferability success.
C. Results
Are adversarial audio samples perturbed via optimiza-
tion attacks transferable?
We ﬁrst explore this question with regards to ASRs that
share all the training parameters, except the random seed. We
observed that none of the adversarial samples successfully
transfered (i.e., none of the remote target ASRs assigned
the attacker chosen transcriptions to the audio) (Table V).
This includes both the adversarial samples with high and
low distortion. We wanted to explore whether the attack
transcriptions existed in the top 10 most probable transcrip-
tions. However, the attack transcriptions were not present in
the top 10 transcriptions. This experiment demonstrates that
transferability, tested over thousands of samples, is unlikely
for ASRs trained on different seeds.
Do adversarial audio samples transfer if the model is
trained on the same seed?
We trained another set of nine ASRs with the same training
set and hyper-parameters, including the random seed. Here, the
goal is to check if adversarial samples will transfer between
two models that have the exact same training parameters. We
used adversarial samples that were generated from the previ-
ous experiment. None of the adversarial samples transferred
successfully (Table V). This is a result of the non-determinism
introduced in GPUs during training which resulted in ASRs
with differing decision boundaries [93] and contains similar
ﬁndings in the context of model extraction. Similar to the
previous experiment, the the attack transcriptions were not
present in the top 10 transcriptions. This means even if an
attacker has perfect knowledge of the target ASR’s training
parameters (train-test splits, the hyper-parameters, the archi-
tecture, etc), adversarial samples still may not transfer unless
all sources of non-determinism have been accounted for.
In what cases is transferability possible at all?
This experiment makes transferability most
likely. The
chosen attack transcriptions are very close to the original
benign ones. We recorded transferability for this scenario.
For the second most likely transcription, the transferability
for the low conﬁdence (low distortion) and high conﬁdence
(high distortion) was 6.5% and 40% respectively. Similarly,
the attack transcriptions were in the top 10 labels 68% of the
time. However, for the third most likely transcription, these
numbers dropped to 3.0%, 31%, and 63%.
These numbers reveal two important facets about audio
adversarial samples. First, even if the attacker-chosen tran-
scription is almost exactly the same as the original,
the
probability of a successful transcription is low (6.5%). This
probability drops signiﬁcantly (from 6.5% to 3.0%) when the
target transcription moves from second to third most likely.
Second, the transferability of low conﬁdence (low distortion)
samples is much lower than high conﬁdence (high distortion)
samples. This is because high conﬁdence samples are further
away from the decision boundaries. As a result, these are
more likely to transfer to a different model with an altered
decision boundary. This means that generally speaking, none
of the optimization attacks we experimented with will transfer
to other instances of the same ASR. This is due to substantial
non-determinism of the GPU and the community should work
towards attacks that can overcome this.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:09:49 UTC from IEEE Xplore.  Restrictions apply. 
746
Target Transcription
delete my messages
browse to evil website
what is the weather today
go to evil website
open the door
transfer money to my account
turn off all the cameras
order me some candy
take a picture
Benign Samples
Perturbed
98
98
98
98
98
98
98
98
98
Total
Adversarial
Audio Generated
29968
30000
30000
30000
64293
30000
30000
30000
30000
239968
Table VI: The table above shows the results of the transfer-
ability experiments for the PGD attack. The attack was run for
1000 iterations for each of the 98 audio ﬁles. For each attack,
we only produced 3000 attack audio ﬁles, each of conﬁdence
greater than 0.9. Each of the generated adversarial samples
was then passed to eight models trained with different initial
seeds and one model trained on the same seed. None of the
239968 samples transfered successfully.
D. Transferability (PGD)
To verify whether transferability is hard in ASRs or is a
facet of the Carlini attack, we decided to run the same set of
experiments with a different optimization attack. This time,
we used the Projected Gradient Descent (PGD) method [112],
which has been successfully demonstrated against image mod-
els. We chose this attack over other optimization attacks as
it can produce high conﬁdence attack samples. Most other
attacks, including the Carlini attack, stop at the boundary. This
is especially true for psychoacoustic attacks [75], [74] which
are designed to produce imperceptible perturbations instead
of high conﬁdence ones. In contrast, we wanted to produce
high conﬁdence samples as these are more likely to transfer,
as demonstrated in the image space.
1) Setup: We followed the same methodological steps like
the ones described for the transferability experiment described
in Section XI-A. The attack was run for 1000 iterations and
clipped using the L inﬁnity norm of 5%. The clipping was
done to ensure that the attack was not completely uncon-
strained. Otherwise, the audio samples would sound like noise.
We generated a total of 250,000 adversarial audio samples over
8 sentences and for each of the 9 models.
2) Results: The results can be seen in Table VI. None of
the 250,000 attack ﬁles successfully transferred. This is the
case for both the models trained using the same seed and
ones trained on different seeds. These results match that of
the Carlini attack experiments (Section XI-A). This means that
the difﬁculty of transferring attack audio is not a by-product of
the attacks themselves. Instead, this stems from some inherent
property of ASRs.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:09:49 UTC from IEEE Xplore.  Restrictions apply. 
747