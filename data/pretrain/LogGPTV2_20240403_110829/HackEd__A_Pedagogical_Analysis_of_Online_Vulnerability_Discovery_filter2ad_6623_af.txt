track student behavior, saying “How would I know what [the
student] is staring at?” Instead of providing automated feedback,
many organizers opted to provide tailored information in the
exercise’s forum based on student demand (O=6). Smash the
Stack’s organizer explained, “They’ll either email us and say,
‘Hey, we’re stuck here’ and we’ll respond, or they’ll join IRC
and ask their questions there. Usually, someone, an admin or
just other players, will exchange hints.” We discuss this forum-
based support in more detail in our review of the Question
support and Group discussion dimensions in this section and
Section III-E1, respectively. Finally, three organizers said they
had not considered providing automated feedback, but agreed
that it would be useful to include in future challenges.
BIBIFI and iCTF provide exploit examples. In BIBIFI’s
break phase and throughout iCTF, students identify and exploit
vulnerabilities other teams’ services. In BIBIFI, students are
notiﬁed of successful exploits against their code, demonstrating
mistakes they made in development. In iCTF, students monitor
network trafﬁc and observe other teams’ exploit attempts.
Because this likely produces multiple variations on the same
exploit, students receive rich feedback on mistakes made during
initial development or patching. Unfortunately, in both, there
is no feedback more detailed than success or failure. Also,
in BIBIFI, because students work on one project, there is no
opportunity to practice and receive feedback in varied contexts.
Students can get help when stuck. All but three exercises
either allow students to ask the organizers questions or provide
static hints to help point students in the right direction (22 do
some of both). For example, XSS-Game allows students to
reveal hints, which get progressively more informative. In the
early challenges in XSS-Game (33% of challenges reviewed),
the ﬁnal hint provides the solution, to help inexperienced
students get started. Unfortunately, if hints are not well crafted,
they can be misleading and cause the student to consider
an incorrect path. For example, in CTFlearn’s Prehashbrown
challenge, the student is given the hint to “login as admin” and
is shown a login screen. This might suggest a need to exploit
the provided login form. However, the student actually needs
to register an account and exploit a SQL injection vulnerability
in a search screen provided after login. We did not observe
any relation to the admin account when solving this exercise.
In every exercise except BIBIFI, Mr. Code, and iCTF,
students can ﬁnd textual or video instructions for solving some
challenges (95% of challenges reviewed on average). In several
cases, the exercise provided a few (69% of challenges reviewed
on average) directly on their website (N=10). For example, in
GirlsGo CyberStart and picoCTF, the organizers provide videos
demonstrating how to solve the ﬁrst few challenges (24% of
challenges reviewed on average) to get students started. We
also found that walkthroughs were produced organically by
participants for most exercises (N=27), even if some organizers
already provided some walkthroughs themselves (N=7) or tried
to discourage their creation to prevent students from copying
solutions (O=3). In BIBIFI and iCTF, students were prohibited
from producing write-ups while the competition was live, but
encouraged to add them afterwards to provide after-action
feedback to other students. Several organizers prided themselves
on the support provided by the community and relied on these
informal communications to support struggling students (O=5).
D. Encouraging Metacognitive Learning
Metacognitive learning has two main components: students’
abilities to predict learning task outcomes and gauge their grasp
of concepts [78], [79]. Guiding students to reﬂect on why their
solutions work helps develop deeper conceptual understanding
and supports knowledge transfer to new settings [32], [86]–
[88]. It also helps students identify knowledge gaps and target
further learning [32]. One way exercises could encourage
metacognition is to prompt students to verbalize why their
solution worked, e.g., via a pop-up after submitting a correct
solution. This is similar to after-action discussions common
in other expert domains, which guide consideration of how
lessons learned might apply to other contexts [89].
1) Transfer Learning: To determine whether exercises
supported metacognitive learning, we asked whether they taught
how, when, and why particular exploits or mitigation techniques
should be used. Answering these questions likely helps students
apply knowledge learned from the challenge to real-world use.
Few exercises guided transfer beyond the challenge
context. While almost all exercises taught how to use each
concept through hands-on exercises (N=30), very few explicitly
explained when (N=6) or why (N=5) to use a security concept
in other settings. In these few cases, the organizers provided
authoritative materials (e.g., video lectures or additional read-
ing) around each challenge instructing students on the speciﬁc
setting details and how approaches should change with new
settings. For example, HackEDU and Pwn College provide
lecture materials describing progressively more challenging
exploit techniques in the face of ever increasing defensive
mitigations. While this is a useful tool for learners, it falls short
of best practice recommendations for metacognition, which
suggest active student engagement [90].
The partial designation was used if it was possible to
implicitly determine when or why a particular concept was
needed by comparing similar challenges. However, this is
not ideal, as students may need a sufﬁciently strong a priori
conceptual understanding to identify the nuanced differences.
Interestingly, this was this was the dimension group organiz-
ers most often reported not considering (O=9). As an example,
when we explained metacognition to the picoCTF organizer,
they said “I don’t know if I ever heard of metacognition
before. . . that could really guide us in developing problems
that can guide our learners even better.”
2) Support: Once students evaluate their grasp on concepts
and identify points requiring clarity, they will seek additional
information to ﬁll those gaps. Exercises can support students
by linking to additional materials beyond the exercise’s scope.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:31:30 UTC from IEEE Xplore.  Restrictions apply. 
1276
Most exercises provided resources for further investiga-
tion. A majority of exercises did provide additional resources
to some extent (N=17). These materials often took the form
of “Useful links” (N=15), sometimes only for a subset of
covered concepts (N=4). While these resource lists are useful,
students may ﬁnd it difﬁcult to identify which to follow
for a speciﬁc question. Some exercises improve on this by
providing relevant resource links with each challenge (N=10).
For example, HackEDU included links to relevant blog posts in
the challenge description. Organizers who did not provide these
resources generally believed students were provided enough
information to ﬁnd resources on their own (O=6). The XSS-
Game organizers expressed this sentiment, saying “Either from
the description of the challenge or the source code, the user
should be able to ﬁgure out what to learn about.”
E. Establishing an Environment Conducive to Learning
Finally, we considered the social environment in which
students participate. Social climate (e.g., interactions with
other students and educators) has been shown to impact
learning generally. A negative environment can hamper student
progress, while a positive environment can excite and engage
students [19], [91]. By participating in a group setting, students
receive mentoring from more senior students, brainstorm pos-
sible solutions with peers, and get support and encouragement
when stuck [92]. Additionally, the educational environment
can have a signiﬁcant impact on whether students feel “good
enough” to participate [91]. If the perceived barrier to entry is
high, students may choose not to try. This is especially true
for commonly underrepresented populations [93]–[95].
We characterize the environment along two dimension
interactions among students (Peer Learning) and
groups:
between the organizers and students (Inclusive Setup).
1) Peer Learning: Peer learning intuitively lightens the
burden on organizers, as other students can act as a ﬁrst-line
support. More importantly, students can collaboratively develop
knowledge (as opposed to being given it by the organizer),
producing more robust understanding [92]. Peer learning has
also been shown to improve intrinsic motivation, as students
who feel that others depend on their participation are more
likely to continue in the face of difﬁculties [19], [69]. To
evaluate whether an exercise provided peer-based learning, we
considered whether it explicitly encouraged team formation
through a provided team-creation feature (i.e., not just allowing
team creation out-of-band) and whether there is an online forum
created by the organizers for students to discuss challenges.
Exercises help students ﬁnd community in online forums.
Most exercises provided IRC, Slack, or Discord channels or
online forums where students could post questions and share
their experiences (N=18). For example, picoCTF created a
dedicated Q&A forum in Piazza [96] with sections for each
problem category for students to post questions as well as
view and respond to others’ questions. As mentioned in our
discussion of Feedback and Question Support, several exercise
organizers said community participation is important for student
success (O=8). The HackTheBox organizers explained that they
have “a vocal community that everyone chats. . . in order to help
each other to understand challenges and learn.” Similarly, the
Crackmes.one organizer said “for a newcomer to the platform,
if they don’t join the Discord, they will not have all the
information.” When organizers did not provide a discussion
forum, students sometimes organized one (these were marked as
“Partial” implementations, N=3). Because they are not directly
linked by the exercise, in some cases these were only identiﬁed
through organizer interviews, making our results a lower bound
on the number of exercises with a discussion forum. iCTF was
marked “Partial” because it did not allow discussion among
teams until after the competition to ensure fairness.
Almost all the exercises without a forum were Syn-
chronous. Organizers attributed this to their initial competitive
design (O=6). For example, the Angstrom organizer explained
that during the competition “Everybody’s competing against
each other.” Now that the competition is over, “people are
now allowed to collaborate. We should probably add a channel
to support that, but we have not.” The Mr. Code organizer
explained, “People join at different times and learn at different
rates,” so a forum does not make sense.
Team participation was allowed in most exercises but
rarely explicitly supported. A competition setting disincen-
tivizes the close collaboration that may support in-depth learn-
ing. Allowing team participation can act as a middle ground.
However, few exercises provided support to help students who
were not already members of clubs or organizations form teams
(N=9). picoCTF provided the clearest example of team support,
providing a “team recruitment” channel in their online forum
to help students create virtual teams. picoCTF also included a
built-in feature for “classrooms,” where students could register
together in groups with a dedicated scoreboard, as well as
resources to help teachers support their classes. Similarly to
discussion forums, teams were not supported when students
were expected to move at their own pace (O=2) or because
organizers wanted to target individual-level competition (O=4).
As an example of the latter, the Smash the Stack organizer
explained, “We bring people on board to help organize that
we see progress through the game rapidly,” making individual
participation necessary to determine potential new organizers.
2) Inclusive Setup: Finally, we consider each exercise’s fram-
ing with respect to extraneous load and terminology. Extraneous
load is any cognitive challenge required to complete tasks but
not directly related to the concepts being taught [82]. Signiﬁcant
extraneous load can cause students to become stuck and quit
for reasons unrelated to the challenge’s learning goals [69].
Exercise terminology could also affect less experienced students
struggling with new concepts. Reassuring terminology may let
students know their struggles are expected and that the solution
is not beyond them [69]. Conversely, terminology that demeans
newcomers may reinforce imposter-like feelings. A “Partial”
mark here indicates that we found the terminology used to be
neutral: neither supportive nor demeaning.
Extraneous load varied widely. Most exercises introduce
some extraneous load, such as determining how to run and
reverse engineer a binary compiled for a different OS (e.g.,
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:31:30 UTC from IEEE Xplore.  Restrictions apply. 
1277
installing a virtual machine) (N=11). In another example, Cyber
Talents used varying ﬂag formats, making it harder to determine
how to correctly submit ﬂags. Many exercises took steps
to reduce extraneous load, such as providing browser-based
tooling (e.g., wireshark, command line, disassembler) (N=6) or
a server with required tools installed (N=5). Perhaps the clearest
examples were Microcorruption, which allowed students to
perform all required tasks with a browser-based disassembler
and debugger, and Pwn College, which included links to
binaries pre-loaded in the BinaryNinja cloud service [97].
Extraneous load is not always bad. While reducing
extraneous load is helpful for learning—especially for less
experienced students—we do not suggest that extraneous tasks
be avoided in every case. Typically, these tasks reﬂect processes
students need to understand and perform in a real-world
setting, which corresponds to the real-world dimension in Sec-
tion III-B2. In fact, most organizers who did include extraneous
load said it was intended to provide a realistic experience (O=6).
Perhaps the most extraneous load is introduced by BIBIFI,
which requires the development of a medium-sized program
with non-security-relevant features. This requires more effort
on extraneous tasks, but is intended to be more representative
of a real-world programming scenario. The BIBIFI organizer
explained “this is just part of the process of building a real
system. So that’s a tradeoff. We decided to do it because it gives
people real experience.” Introducing extraneous load should be
considered carefully and in the context of a student’s learning
progression, in conjunction with decisions discussed previously
about connecting to prior knowledge and organizing knowledge
to provide context (Sections III-A1 and III-B2, respectively).
Most exercises used supportive terminology, but a few
marginalized beginners. A majority of exercises included
language in their rules or FAQs offering encouragement (N=18).
For example, Vulnhub offered several strategies for dealing
with “stuck-ness” and Root-me.org suggested a learning path to
help new students work up to more complicated problems. This
supportive terminology, along with tailoring exercise difﬁculty
to experience (Section III-A1), will likely improve student
conﬁdence and engagement [69]. However, some exercises
use terminology that marginalizes newer students who might
struggle with basic concepts (N=5). This included HackthisSite
calling their ﬁrst challenge the “idiot” challenge and saying “if
you can’t solve it, don’t go crying to anyone because they’ll
just make fun of you” and Pwn College referring to their easiest
challenge level as the “baby” level. The Pwn College organizers
explained that “baby” notation is common in the CTF culture,
and that using it was intended to give students a point of
reference across CTFs. However, they agreed “someone might
interpret it negatively, and we will consider this point.”
IV. DISCUSSION AND RECOMMENDATIONS
Through our online hacking exercise review and interviews
with organizers, we found that no exercise implemented every
pedagogical principle, but there were many creative approaches
taken across the exercise landscape. Overall, our analysis
found a few dimensions where exercises showed room for
improvement and others where there are clear tradeoffs between
principles. We do not expect every organizer to adopt all
recommendations or pedagogical methods, but instead hope this
paper can serve as a roadmap to help organizers thoughtfully
consider their approach, borrow ideas from other exercises,
and select the elements that are the right ﬁt for their context.
We identiﬁed four key areas for improvement:
• Most organizers did not consider metacognition, and
there were few realistic challenges. This may cause
difﬁculties when students try to apply lessons learned
to real situations.
• Many exercises lacked clear, explicit structure, which can
help students establish a more robust knowledge base.
• Inherent technical challenges led most organizers not to
provide secure development practice or tailored feedback.
• Exercises frequently gave students the autonomy to choose
a personalized path, but rarely activated prior knowledge
to explicitly leverage prior experience for learning.
Interestingly, these ﬁndings are mostly unique to security
exercises, as compared to Kim and Ko’s review of online
programming tutorials [20]. Many online coding tutorials
provided direct feedback, while this was not common in
our exercises. Conversely, online coding tutorials provided
little personalization based on student experience, but this
was common in security exercises. However, in both settings,
support for metacognition was not common. This may suggest
metacognition is generally not well known or understood.
We also noticed interesting tradeoffs between principles that
should be considered carefully in the design of an exercise.
There is a clear tension between providing realistic
challenges and minimizing extraneous load. The more
realistic a challenge becomes, the more auxiliary tasks are
required. However, this is not a binary decision: There are
a range of good options, which should be intentionally and
explicitly selected to ﬁt learning objectives and students’ current
experience level. One potential approach to this tradeoff would
be to design exercises so that students move from toy challenges
with low extraneous load to more realistic challenges with
more extraneous load as they develop expertise, so they leave
the exercise prepared to perform similar tasks in the real
world. Alternatively, a single “realistic” challenge for each
problem type may be sufﬁcient to provide a bridge to real-
world scenarios, with all other challenges focused solely on
teaching security concepts. Future work is necessary to evaluate
potential design choices along this spectrum.
Community participation can have signiﬁcant beneﬁts,
but can be difﬁcult to manage. Many exercises rely on an
active community to provide challenges, help assess challenge
difﬁculty, and support student engagement. However, organizers
reported that this can create a moderation challenge, as they
try to provide overarching structure and context for challenges,
make sure new students receive necessary feedback, and ensure
a supportive culture in discussions. Therefore, organizers should
carefully consider ways to structure community involvement
to gain the beneﬁts of broad participation while advancing
educational goals. Additionally, from a research perspective,
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:31:30 UTC from IEEE Xplore.  Restrictions apply. 
1278
this dynamic indicates that it is necessary to understand not
only how exercises themselves run, but also how the community
operates. Future work should investigate the forums and online
conversations that have grown around exercises.
Competition can get in the way of education. Competition
offers a useful motivational tool; however, it likely also limits
avenues of support for less experienced students through
collaboration and discussion. We observed that
in many
cases, organizers leaned toward competition, often simply
as an artifact of an initial offering in a synchronous setting.
Organizers should be conscious of this dynamic, especially after
an exercise is no longer part of a live competition. Prior work
has shown competitive environments in STEM education can
negatively effect student experience, especially for members of
underrepresented populations [69], [98]–[101]. Exercises can
focus on providing more support for team-based learning and
helping new students join the community.
A. Recommendations
With these ﬁndings in mind, we suggest recommendations
for exercise organizers and directions for future work.
Support active student engagement in metacognition.
Because many organizers did not consider metacognition, a
ﬁrst step would be to apply common techniques from learning
sciences to prompt students to consider their learning state. In
Section III-D, we provided one example: asking students to
describe why their solution worked. Another common method
asks students to predict an action’s result prior to performing the
action. For example, students could be prompted—perhaps by
updating the target program to provide an initial text prompt—
to predict the outcome of an exploitation attempt prior to its