repair that is transparent to applications. Our testbed exper-
iments in the rest of the paper are based on the OCS imple-
mentation. As we will show later, regardless of the relatively
long disruption time, the performance of our ShareBackup
implementation is still similar to a failure-free network.
6.5 Responsiveness of Control Plane
Our testbed is limited in scale, and the above implementation
ignores failure diagnosis. To understand the efﬁciency of the
control plane in a large data center, we abstract the involved
entities as individual processes and realize the communication
protocol in Figure 2 on a k = 48 fat-tree network. There are
2880 switch processes, 120 controller processes, and 3456
agent processes for circuit switches. The communications are
implemented using the server-client model with TCP sockets.
Link failures are more complicated than switch failures for
the control system and are followed by the ofﬂine failure di-
agnosis, so we show the failure recovery and diagnosis delay
for link failures in Table 3 as the worst-case performance of
the control system. The failure recovery delay is broken down
into the time for communications in Figure 2(a), computation
at the distributed controllers, and OCS/EPS reconﬁguration
discussed in the above subsection. Circuit switch agents are
not necessary if circuit switches support the proposed re-
conﬁguration function, i.e. replace( ) in Figure 2. Thus, the
communication delay can be further reduced in real imple-
mentation. The computation delay is minimal, as controllers
only map the failed switch and the backup switch to their cir-
cuit switch ports so as to reset circuits. In the EPS emulation,
the end-to-end delay of failure recovery is only 0.73ms, which
will be even lower if using the targeted circuit switches with
shorter switching latency and the modiﬁed reconﬁguration
function. F10 and PortLand reported 1ms and 65ms conver-
gence delay [26, 30]. ShareBackup is more efﬁcient than these
state-of-the-art solutions because it does not involve change
of forwarding rules and computation for rerouting.
Our implementation of failure diagnosis cycles through all
the conﬁgurations in Figure 3, although the process may ter-
minate earlier in reality. Even with our very simple implemen-
tation, failure diagnosis can ﬁnish in hundreds of milliseconds.
If the diagnosis process is preempted by failure recovery in
one of the tested conﬁgurations, the duration only increases
slightly. As discussed in Section 5 (3), this sub-ms failure
recovery and sub-second failure diagnosis are breakthroughs
to data center management, compared to the common practice
based on manual efforts nowadays.
Figure 7: iPerf throughput of 8 ﬂows saturating all links on testbed.
Figure 8: Minimum ﬂow throughput under edge-aggregation link
failures normalized against the no-failure case on the LP simulator
with global optimal routing for all networks.
6.6 Bandwidth Advantage
On the testbed, we show the bandwidth difference among the
various network architectures with an iPerf throughput exper-
iment. We create an instance of permutation trafﬁc, where
Two-Level Routing places the 8 ﬂows onto different paths
without contention. This trafﬁc pattern saturates all links. In
Figure 7, Sharebackup achieves the same performance as
the no-failure case, showing ShareBackup’s ability to restore
bandwidth quickly after failures. In contrast, for PortLand
and F10, the worst-case ﬂow throughput decreases dramati-
cally as the failure approaches edge links. In a fat-tree (or AB
fat-tree) network, there are k 2
4 parallel paths in the core layer,
k
2 in the aggregation layer, yet only 1 in the edge layer right
above hosts. As a result, rerouting causes greater congestion
with fewer paths to balance the load at the edge. F10 is less
tolerant to failures than PortLand, because its local rerouting
uses longer paths and makes congestion even worse.
This trend holds for our LP simulations as well. Due to
space limitation, we only show the results for link failures
in the aggregation layer in Figure 8. ShareBackup outper-
forms the other architectures by 13% to 25%. It achieves
similar throughput as the case without failures given 2 backup
switches per failure group; while 1 backup switch falls short
sometimes when concurrent failures happen in the same fail-
ure group. Note that this is a stress test. In data centers, most
devices have over 99.99% availability, and concurrent fail-
ures are very rare [16]. So, 1 backup switch per failure group
is sufﬁcient in most cases. Portland and F10 have the same
numbers, as the LP solver performs optimal routing on them
186
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
D. Wu et al.
Table 4: Percentage of impacted ﬂows/coﬂows in Figure 9.
Directly impacted ﬂows
Indirectly impacted ﬂows
Directly impacted coﬂows
Indirectly impacted coﬂows
0
0
ShareBackup
0.63%
0.78%
PortLand
3.92%
16.01%
17.32%
18.95%
F10
3.91%
21.29%
18.31%
28.89%
Aspen Tree
3.89%
16.23%
18.48%
19.22%
Figure 10: Percentage of jobs missing deadlines on packet simulator.
These results are corroborated by Table 4. As demonstrated
in Section 6.5, ShareBackup recovers from a failure in sub-
ms. So, very few ﬂows and coﬂows get impacted during the
small transition period. Other architectures, however, have
upto 25.2% ﬂows and 47.2% coﬂows impacted. Note that a
large portion of ﬂows/coﬂows are indirectly impacted, which
are not hit by failures but have contention with the rerouted
ﬂows. Rerouting spreads the effect of failures to innocent
ﬂows, thus converting the local failure to global performance
degradation. In comparison, ShareBackup’s principle of ﬁxing
failures at where they happen effectively localizes the problem
and provides more predictability to application performance.
For latency-sensitive ﬂows in the Deadline trace, Figure 10
shows the percentage of jobs that miss deadlines under fail-
ures. In ShareBackup, failures only cause less than 2% dead-
line miss, with slight increase as the network utilization grows.
ShareBackup handles switch and link failures in the same way,
so the results are similar. Rerouting-based solutions perform
much worse in comparison. They are sensitive to network
utilization and failure type, with the worst-case job miss rate
reaching 51%. Although F10 has similar failure recovery de-
lay as ShareBackup, its local rerouting renders heavy trafﬁc
contention. PortLand’s global rerouting is more efﬁcient, but
there is still bandwidth loss and path re-computation takes as
long as 65ms [30]. Jointly affected by these two factors, F10
outperforms PortLand slightly. Aspen Tree pushes back most
impacted downstream trafﬁc to the core layer and reroutes
from there, balancing rerouting delay and path dilation. Its
performance thus falls between PortLand and F10.
6.8 Beneﬁts to Real Applications
The performance of the bandwidth-intensive applications on
the testbed is shown in Figure 11. We do not differentiate
link failures in the aggregation and core layers because they
result in similar performance. The trend is consistent with
the bandwidth difference in Figure 7. It conﬁrms that Share-
Backup masks failures from application performance: for
both applications, the communication time and job comple-
tion time constantly stay the same as the no-failure case.
Big-data frameworks consume most time on computations.
Figure 9: CDF of completion time slowdowns on packet simulator.
both. Although Aspen Tree uses more switches for redun-
dancy, it fails to add more bandwidth to the network, so its
performance is no better than PortLand and F10.
Under edge link failures, the impacted ﬂows have zero
throughput in all architectures, whereas ShareBackup still
gives full capacity. If link failures happen in the core layer,
these architectures have very similar throughput, since there
are abundant alternative paths for rerouting. The results for
switch failures are the same as link failures. Our formulation
calculates the minimum ﬂow throughput as the worst-case
analysis. Switch failures affect more ﬂows but do not change
the minimum value. From these observations, we conclude
that ShareBackup is most powerful in the edge layer, then the
aggregation layer. As discussed in Section 5 (1), ShareBackup
supports layer-wise partial deployment to save cost.
6.7 Transmission Performance at Scale
For throughput-intensive jobs in the Coﬂow trace, the trans-
mission performance is largely determined by the network’s
bandwidth capacity shown in the above section. Figure 9 plots
the distributions of ﬂow completion time (FCT) and coﬂow
completion time (CCT) normalized against the no-failure case,
i.e. slowdowns. Overall, Sharebackup has negligible perfor-
mance degradation, whereas PortLand, F10 and Aspen Tree
experience multi-fold slowdowns for >20% ﬂows and >30%
coﬂows. The impact of failure is magniﬁed at the coﬂow level,
since a small number of straggler ﬂows is all it takes to neg-
atively impact the CCT. F10 performs notably worse than
PortLand and Aspen Tree, because its local rerouting uses
longer paths (path dilation), resulting in more ﬂows being im-
pacted. Aspen Tree slightly underperforms PortLand, because
Aspen Tree’s local rerouting also has a small path dilation.
As an application can only proceed after the entire coﬂow
has ﬁnished, ShareBackup is signiﬁcantly more effective in
masking failures from applications.
187
Masking Failures from Application Performance
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
Figure 11: Performance of the Spark Word2Vec and Tez Sort applications with a single edge (edge-host) or network (core-aggregation and
aggregation-edge) link failure on the testbed.
Figure 12: CDF of query latency in the Spark TPC-H application
with a single edge (edge-host) or network (core-aggregation and
aggregation-edge) link failure on the testbed.
Inter-node communications between computations are inﬂu-
enced by node synchronization, data serialization, garbage
collection, etc. Considering all these factors, the bandwidth
advantage of ShareBackup can still translate into over 12%
less communication time and 23% less job completion time
under in-network failures. If an edge link fails, workers may
get lost and the master needs to relaunch tasks. The commu-
nication phase and thus the entire job ﬁnish multi-fold slower.
In our experiments, we even encountered cases where the job
crashed. Failures near the hosts are disastrous to applications,
and ShareBackup is an especially useful remedy.
Figure 11(c) zooms in to the distribution from multiple iter-
ations of data broadcast for the Spark Word2Vec case. Under
failures, ShareBackup has almost the same communication
and job duration throughout the runs, while the variation
is huge for PortLand and F10. In this Word2Vec job with
BitTorrent-like trafﬁc, receivers retrieve data blocks depend-
ing on availability. Rerouting slows down data retrieval on
different degrees at the worker nodes. The change of data
availability shapes trafﬁc further later on, leads to a long-tail
in completion time. This observation validates the point in
Section 6.7 that rerouting enlarges the effect of failure while
ShareBackup preserves predictability. We again show Share-
Backup can mask failures from application performance: for
many applications, it provides performance guarantee even
for the worst case.
This long-tail phenomenon also exists in the Spark TPC-H
application. The CDF of query latency in Figure 12 reﬂects
TPC-H’s performance metric: the number of queries ﬁnished
in a time period. PortLand and F10 under edge link failures
are on average 25% lower than the rest cases using that metric,
and their job completion time determined by the last query is
37.4% and 38.1% longer respectively. Hosts are disconnected
in this case, and the job relies on Spark’s own resilience
mechanism, i.e. task relaunch, to proceed. Trafﬁc is light in
this application. Trafﬁc contention from rerouting is not heavy
enough to cause congestion, so PortLand and F10 have similar
performance as ShareBackup in most cases. Nonetheless,
ShareBackup is still necessary for edge link failures.
7 CONCLUSION
The advancement of circuit switching technology makes it
possible to assign backup switches on demand at runtime.
ShareBackup is the ﬁrst effort to realize this concept of share-
able backup in data center networks. Circuit switches have
inherent tradeoffs between cost, switching latency, and port
count. ShareBackup aims at a cost-effective network archi-
tecture for rapid failure recovery, so port count has to be
restricted. The choice of modest-size circuit switches drives
ShareBackup’s distributed design of both the network archi-
tecture and the control system. We ﬁnd this design a good
match for the rare, uncorrelated, and spatially dispersed fail-
ures in data centers. With co-design of architecture and con-
trol system, backup switches can work as hot standbys without
primary-backup coordinations or online change of forward-
ing rules. Besides failure recovery, special setups of circuit
switches can automate and speed up failure diagnosis. Ex-
tensive system implementations and evaluations demonstrate
ShareBackup can effectively mask failures from application
performance. This powerful concept of shareable backup goes
beyond the speciﬁc ShareBackup architecture. We encourage
more research efforts in this promising direction.
ACKNOWLEDGEMENT
We would like to thank the anonymous reviewers and our