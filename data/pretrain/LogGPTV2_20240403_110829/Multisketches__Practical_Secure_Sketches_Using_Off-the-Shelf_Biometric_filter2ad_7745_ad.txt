(cid:104)
(cid:105) ≥ 1
· Pr
N
Consequently, it suffices for us to explore TarGuess only, which
will also provide an upper bound on the success probability of
UnTarGuess game.
The ideal setting for TenSketch would be if the different tem-
plates of different fingers of a user were completely independent.
However, this is not true in practice. Therefore, we first analytically
compute the attacker’s advantage assuming complete independence
between different fingers. Next, we will give an empirical result
about the advantage of an attacker lifting the assumption.
Mutually independent messages. The messages are said to be
independent if the attacker is not able to find other fingerprints of
a user from the database given one or more of their fingerprints
with probability better than random guessing.
In this setting, to find the w associated with a user u, the at-
tacker’s best guess is to brute-force try all possible subset of size
t of ℱ. Note the attacker only need to be able to guess t finger-
prints of the user correctly, because remaining fingerprints can be
recovered via Rec∆
In the TarGuess setting, the attacker might know some b < t
of the target user’s fingerprints. Let N = |ℐ|. Then the number of
possibilities for the remaining part of the message tuple that the
(cid:1) · N t−b. The total number of valid
(cid:1) — any combination t − b messages from n − b
attacker has to compute is(cid:0)n−b
solutions is(cid:0)n−b
t algorithm.
fingerprints. Therefore, if all the message tuples are equally likely,
the attacker has to make on expectation N(t−b) queries to Rec∆
t .
This is equivalent of (t − b) · log N bits of security.
To get a sense of how large this value is for a realistic size biomet-
rics database, let assume N = 104 (ten-prints for ten thousand users)
and t = 7 (at least 7 fingers has to match for Rec∆
t to successfully
recover the tenprint of the user). Assuming the attacker already
know b = 1 fingerprint of the target user u, then the security in bits
is (7 − 1) · log2(104) ≈ 79.7-bits. With N = 106 users, the security
can be 119.6 bits.
Security without independence assumption. Previous secu-
rity analysis assumes independence of the fingerprints, however, in
many situations, particularly for tenprints, this assumption might
not hold true. If that is the case, the attacker might be able to
avoid checking every message tuples with Rec∆
t and improve it’s
performance significantly.
t−b
t−b
While the attacker could take advantage of the correlation among
fingerprints in many ways, the most obvious and easy to use strat-
egy would be to build a classifier that classifies a tenprint as belong-
ing to a single user, or not. In Section 6 we will show how to build
such classifier that exploits the similarities between fingerprints
belonging to an individual. We call such classifiers adversarial clas-
sifiers. In Section 7, we show how to use adversarial classifiers to
attack TenSketch.
6 ADVERSARIAL CLASSIFIERS
In this section, we show how to build adversarial classifiers, that
can identify a tuple of fingerprints belonging to a single user or
not. We use SD09 dataset for training our adversarial classifiers.
We randomly partition the dataset into two groups of size 2500 and
200 users as training and test sets, respectively. In SD09 dataset,
each user contains two sets of tenprints — 20 fingerprints in total.
Due to the low volume of available training data and the curse
of dimensionality, building a classifier that can classify a tuple of
size n = 7 could be difficult. Therefore, we build classifiers for
various tuple sizes, starting with a pair classifier that classifies only
template pairs. We then go on to explore ways to build higher-order
classifiers.
All of the experimental results are performed on a server with
128 GB of memory, two Intel Xeon E7 CPUs (12 core each), and
four Nvidia Titan Xp GPUs (each with 12 GB of memory). We
describe the DNN models in Python using Keras library [24] with
Tensorflow [13] as the backend.
Notation. Let 𝒞i, for 2 ≤ i ≤ t, be the classifier that takes as input
a tuple of fingerprint templates of size i and outputs a score in [0, 1].
Therefore, 𝒞i(w1, . . . , wi) denotes the likelihood (according to the
classifier) that the message tuple w1 . . . wi belongs to a single user.
Each fingerprint template (wj) comprises of 200 minutiae points
and each minutiae point has x, y, an angle θ, and a quality value q.
2 As such, each template has 600 integer values (800 when using
quality values). Utilizing quality values during the training process
has a significant effect on the accuracy as well as the true and the
false positive rates. If the quality value of the minutiae points are
used, we denote the corresponding classifier as 𝒞q
i .
6.1 Pair Classifier
A pair classifier outputs the probability that two given fingerprint
templates belong to the same person. We explored various DNN
architectures to construct a pair classifier. In particular, we tried a
single DNN architecture and a split Siamese architecture.
Dataset. The datasets for training and testing are created by taking
all possible pairs of tenprints for each person — 45 pairs for each
tenprint, 90 pairs for each person. These intra-person pairs are
labeled as “1” (belonging to the same person). We generate inter-
person fingerprint pairs by selecting randomly two fingers from
two different users in SD09. To have an unbiased classification, we
have generated the same number of inter-person fingerprints as
that of intra-person. This is done separately for training (2500 users)
2For TenSketch we do not need to store quality values explicitly. Nevertheless, we used
them for constructing the adversarial classifiers to get a more conservative estimate of
attacker’s advantage.
Figure 7: Accuracy and loss on train and test datasets for 𝒞2.
and testing (the remaining 200 users). Therefore, the training and
testing sets contain in total 450, 000 and 36, 000 pairs of fingerprint
templates. The feature set (x, y, θ, and q) is normalized to have
zero mean and unit standard deviation, and all pairs within each
training and testing set are permuted so that the training algorithm
converges faster and to a more accurate model.
Single DNN architecture. We explored several DNN architecture.
In order to train the DNN models, we have used mini-batch gradient
descent (with a batch size of 1024). Two optimization methods have
been used: Root Mean Square (RMSProp) and Adam. The best results
are achieved using Adam optimization algorithm with the learning
rate of γ = 10−6.
The DNN models are trained until the test accuracy is saturated
and/or the model starts to overfit to the training data. Figure 7
shows accuracy and the value of the loss function for the training
and test datasets over 100 epochs. In order to avoid overfitting to
the training data, L2 regularization is used with the regularization
parameter of λ = 0.001 for all hidden layers. The best performing
architecture is a 5-layer DNN model consisting of
ReLu (cid:55)→ FC640
ReLu (cid:55)→ FC1280
ReLu (cid:55)→ FC640
FC320
where FCn
ReLu denotes a Fully Connected layer with ReLu activation
function and n output neurons. The input of a fully-connected
layer is multiplied by the weight matrix and the resulting vector is
passed to a non-linear function. One of the most popular non-linear
functions is Rectifier Linear Unit, ReLu(x) = max(x, 0). The output
of the last layer of DNN is then passed to the Sigmoid function,
σ(x) = (1 + e−x)−1, which represents the probability that the input
belongs to the class “1”. The values of the weights in the FC layer is
learned using Back Propagation Algorithm.
The best performing 𝒞2 achieves 65% accuracy. If the quality
values are available in the template the accuracy goes up to 70%.
In both the settings, the false negative rates (FNR) are high, more
than 25%. The false positive rates (FPR) are also high: 38% and
32% respectively for with and without quality values. Nevertheless,
this classifiers show that different fingerprints of users are indeed
correlated.
Siamese Architectures. In addition to standard deep neural net-
work architectures, we explore a special class of DNNs, called
ReLu (cid:55)→ FC320
ReLu ,
Figure 8: High-level diagram of 𝒞2 based on the Siamese architecture.
Siamese Networks [56, 58]. In Siamese networks, two input fea-
ture sets are passed to two sister networks with identical parame-
ters (weights). We give a high-level diagram of the setup in Figure 8.
More details on the training Siamese network is given in Appendix E.
Despite Siamese being useful for facial recognition, we found
Siamese networks perform poorly compared to single DNNs. The
accuracy of the best performing Siamese network with the quality
values is only 66%, and without quality values it is 62%.
The DNN model outperforms the Siamese network consistently.
We therefore choose the DNN model as 𝒞2 and use for the attacks de-
scribed in Section 7. Next, we elaborate on how one can generalize
the pair classifier to obtain higher-order classifiers.
(cid:0)i
(cid:1) pairs of templates in the template tuple via 𝒞2. The attacker
6.2 Higher-Order Classifiers
The ultimate goal of an attacker is to identifying a certain number
(e.g., 7) of fingerprint templates that belong to the same person.
There are two main strategies that an attacker can pursue: (1) she
can leverage the pair classifier as an oracle and perform multiple
calls to it, or (2) she can train more complex DNNs that accept
multiple templates as input. In what follows, we discuss these two
strategies in more detail and compare their performances.
𝒞2-based higher-order classifiers. Using a pair classifier an at-
tacker can build a higher-order classifier 𝒞i by checking all possible
2
accepts the i templates if the all pairs are accepted by 𝒞2. We call
such classifiers extrapolating classifiers, denote by ˆ𝒞i (and ˆ𝒞q
i when
quality values are available). In Figure 9 (left two figures), we show
the ROC curve of this approach for different values of 2 ≤ i ≤ 7
with and without the quality values. When the quality values are
available, extrapolating classifiers are effective. 𝒞q
7 obtains < 30%
FPR at 1% FNR. Without the quality values the extrapolating clas-
sifier performs poorly and not very useful for classification. This
is important as we remove quality values from the templates for
TenSketch.
Standalone higher-order classifiers. The second strategy is to
build higher-order classifiers by training more complex DNNs that
take multiple templates as input. This strategy turns out to be more
effective and leads to higher accuracy than extrapolating classifiers.
The process of creating training and test sets are an extension of
the process we elaborated for 𝒞2 in Section 6.1. We leverage the
same architecture as for 𝒞2 to generalize 𝒞i by scaling the width of
each layer linearly with respect to i.
020406080100Epoch0.500.520.540.560.580.600.620.64Accuracy1.6661.8482.0292.2102.3912.5722.7532.935LossTestTrainDistance FunctionMinutiae Points ExtractionDeep Neural Network (DNN)Distance Comp.PredictionFigure 9: Illustrating the effect of the number of fingerprint templates and the quality values on the performance of higher-order classifiers. The left two
figures show the ROC curve for with and without the quality values for extrapolating classifier ˆ𝒞i . The rightmost figure shows the performance of learned
higher-order classifier 𝒞i .
Classifier
𝒞2
𝒞3
𝒞4
𝒞5
𝒞6
𝒞7
# Param. Acc (%)
62
74
81
84
84
86
2.4M
5.5M
9.7M
15.2M
21.9M
29.8M
FPR (%)
43
25
14
7
4
3
FNR (%)
30
24
22
24
25
23
Classifier
𝒞q
2
𝒞q
3
𝒞q
4
𝒞q
5
𝒞q
6
𝒞q
7
# Param. Acc (%)
69
81
86
90
89
92
2.6M
5.8M
10.2M
16.0M
23.1M
31.4M
FPR (%)
36
18
10
4
1
2
FNR (%)
25
19
16
15
18
12
# Tr. Smp.
450.0K
1.2M