another (vulnerable) component, e.g., sendBroadcast
Note that any path reachable from an open entrypoint or com-
ponent can be examined directly to see if it has a sensitive-sink.
Meanwhile, we also determine whether it could reach any bridge-
link that will trigger other protected components (or paths). The
remaining paths, whose entrypoints are protected, are correlated
with paths that contain bridge-sinks to form the complete vulner-
able path, which is likely cross-component or even cross different
apps. This is essentially a reﬂection-based attack and we will de-
scribe it in Section 2.3.2 in greater detail. All calculated (vulnera-
ble) paths will subject to manual veriﬁcation.
We stress that unlike some previous works (e.g., [24]) which
mainly focus on discovery of vulnerabilities, this analysis stage
primarily involves a more contextual evaluation of vulnerabilities,
including distribution, evolution and the impact of customization.
Especially, we use the distribution of vulnerable apps as a metric to
assess possible security impact from vendor customizations. Note
the detected vulnerabilities are classiﬁed into different categories
by their provenance and leveraged to understand the corresponding
impact of customization. As mentioned earlier, both horizontal and
vertical impact analyses are performed.
2.3.1 Reachability Analysis
Our reachability analysis is performed in two steps. The ﬁrst step
is intra-procedural reachability analysis, which involves building
related call graphs and resolving it by conventional def-use analy-
sis [11]. The resolution starts from the initial state (pre-computed
when the database is initially populated) and then gradually seeks
a ﬁxed point of state changes with iteration (due to various transfer
functions). However, as the state space might be huge (due to com-
binatorial explosion), the convergence progress could be slow or
even unavailable. In practice, we have to impose additional condi-
tional constraints to control the state-changing iteration procedure.
We call the result of intra-procedural analysis, i.e., the states of
variables and ﬁelds, a summary.
The second step is inter-procedural reachability analysis that is
used to propagate states between different methods. After each
propagation, method summaries might be changed. In such cases,
intra-procedural reachability analysis is performed again on each
affected method to generate a new summary. Inter-procedural reach-
ability analysis is also an iterative process, but takes longer and
requires more space to converge; therefore, we use some heuris-
tics to reduce the computational and space overhead. For instance,
if a variable or ﬁeld we are concerned with has already reached a
sink, there is no need to wait for convergence. A more formal de-
scription of our reachability analysis is listed in Algorithm 3 (see
Appendix A).
Paths of apps from different vendors but with similar functional-
ity may share something in common, especially for those apps in-
herited from the standard AOSP framework. Here “common” does
not mean that their source code is exactly the same, but is similar
from the perspective of structure and functionality. Many devices
reuse the code from the AOSP directly, without many modiﬁca-
tions. If we have already performed reachability analysis on such
a common path, there is no need to do it on its similar counter-
parts. We believe this improves system performance since reacha-
bility analysis is time consuming (especially when the state space
is huge). Therefore, we also perform a similarity analysis as a part
of the reachability analysis to avoid repetitive efforts.
6262.3.2 Reﬂection Analysis
To facilitate our analysis, we classify vulnerable paths into the
following three types:
• in-component: a vulnerable path that starts from an unpro-
tected component to a sink that is located in the same com-
ponent.
• cross-component: a vulnerable path that starts from an un-
protected component, goes through into other components
within the same app, and then reaches a sink.
• cross-app: a vulnerable path that starts from an unprotected
component of one app, goes through into another app’s com-
ponents, and eventually reaches a sink.
The in-component vulnerable paths are relatively common and
have been the subject of recent studies [24, 34]. However, the latter
two, especially the cross-app ones, have not been well studied yet,
which is thus the main focus of our reﬂection analysis. Note that
a reﬂection-based attack typically involves with multiple compo-
nents that may not reside in the same app. A concrete example that
is detected by our tool will be shown in Figure 6 (Section 3.3.1).
Traditional reachability analysis has been effective in detecting
in-component vulnerable paths. However, it is rather limited for
other cross-component or cross-app vulnerable paths. (A cross-app
execution path will pass through a chain of related apps to ulti-
mately launch an attack.) In order to identify them, a comprehen-
sive analysis of possible “connection” between apps is necessary.
To achieve that, our approach identiﬁes not only possible reachable
paths within each component, but also the invocation relationship
for all components. The invocation relationship is essentially in-
dicated by sending an intent [21] from one component to another,
explicitly or implicitly. An explicit intent speciﬁes the target com-
ponent to receive it and is straightforward to handle. An implicit
intent, on the other hand, may be sent anonymously without spec-
ifying the receiving component, thus requiring extra handling (i.e.,
intent resolution in Android) to determine the best one from the
available components. In our system, SEFA essentially mimics the
Android intent resolution mechanism by matching an intent against
all possible  manifest declarations in the installed
apps. However, due to the ofﬂine nature of our system, we have
limited available information about how the framework behaves at
run-time. Therefore, we develop the following two heuristics:
• A component from the same app is preferable to components
from other apps.
• A component from a different app which shares the same
sharedUserId is preferable to components from other apps.
If multiple component candidates still exist for a particular in-
tent, we simply iterate each one to report possible vulnerable path
and then manually verify it in a real phone setting. In Algorithm 2,
we summarize the overall procedure. The basic idea here is to
maintain a visited component list. For a particular component, the
algorithm returns ∅ if it has been visited; otherwise we add it into
the list, and check all possible components that are able to start up
that component recursively.
3.
IMPLEMENTATION AND EVALUATION
We have implemented a SEFA prototype as a mix of Java code
and Python scripts with 11, 447 and 4, 876 lines of code (LOC)
5This category contains apps that exist in the AOSP and may (or
may not) be customized by the vendor – Section 2.1. This deﬁnition
also applies to Tables 3, 4 and 5.
Algorithm 2: Reﬂection Analysis
Input: current component, visited component list
Output: vulnerable path set
/* find vulnerable paths recursively from
backward for current component
c = current component;
V C = visited component list;
CC = components which are able to start up c;
V = ∅; // vulnerable path set
*/
if c ∈ V C then
return V
else
V C.append(c);
foreach cc ∈ CC do
tmp = V C.clone();
V .add(Reﬂection Analysis(cc, tmp));
return V
respectively.
In our evaluation, we examined ten representative
phones (Table 1) released between the end of 2010 and the end
of 2012 by ﬁve popular vendors: Google, Samsung, HTC, LG, and
Sony. The selected phone model either has great impact and is
representative, or has huge market share. For example, Google’s
phones are designed to be reference models for their whole genera-
tion; Samsung is the market leader which occupies 39.6% of smart-
phone market share in 2012 [30]. To analyze these phone images,
our system requires on average 70 minutes to process each image
(i.e., around 30 seconds per app) and reports about 300 vulnerable
paths for us to manually verify. Considering the off-line nature of
our tool, we consider this acceptable, even though our tool could
be optimized further to speed up our analysis.
3.1 Provenance Analysis
As mentioned earlier, the provenance analysis collects a wide
variety of information about each device and classiﬁes pre-loaded
apps into three different categories. In Table 1, we summarize our
results. Overall, these ten devices had 1,548 pre-loaded apps, to-
talling 114,427,232 lines of code (in terms of decompiled .smali
code). A further break-down of these apps show that, among these
devices, there are on average 28.2 (18.22%), 99.7 (64.41%), and
26.9 (17.38%) apps from the categories of AOSP, vendor and third-
party, respectively. Note that the apps in the AOSP category may
also be customized or extended by vendors (Section 2.1). As a
result, the ﬁgures in the AOSP column of Table 1 should be con-
sidered an upper bound for the proportion of code drawn from the
AOSP. Accordingly, on average, vendor customizations account for
more than 81.78% of apps (or 76.34% of LOC) on these devices.
In our study, we selected two phone models for each vendor, one
from the current crop of Android 4.x phones, and one from the pre-
vious generation of 2.x devices. Table 2 shows the initial release
date of each phone model, as reported by GSM Arena [26]. As it
turns out, these devices can be readily classiﬁed by their release
dates: the past generation of devices all were initially released be-
fore 2012, while the current generation’s products were released in
2012 or later. Therefore, we break them down into pre-2012 and
post-2012 devices. Such classiﬁcation is helpful to lead to certain
conclusions. For example, as one might expect, the complexity of
these devices is clearly increasing over time. In all cases, the post-
2012 products from any given manufacturer contain more apps and
LOC than their pre-2012 counterparts. Speciﬁcally, the HTC Wild-
ﬁre S has 147 apps with 9,643,448 LOC, and the HTC One X has
627Table 1: Provenance analysis of representative devices
Vendor
Samsung
Samsung
HTC
HTC
LG
LG
Sony
Sony
Google
Google
Device
Galaxy S2
Galaxy S3
Wildﬁre S
One X
Optimus P350
Optimus P880
Xperia Arc S
Xperia SL
Nexus S
Nexus 4
Total
Version & Build #
2.3.4; I9100XWKI4
4.0.4; I9300UBALF5
2.3.5; CL362953
4.0.4; CL100532
2.2; FRG83
4.0.3; IML74K
2.3.4; 4.0.2.A.0.62
4.0.4; 6.1.A.2.45
2.3.6; GRK39F
4.2; JOP40C
Total
# Apps
# LOC
172
185
147
280
100
115
176
209
73
91
1548
10,052,891
17,339,442
9,643,448
19,623,805
6,160,168
12,129,841
7,689,131
10,704,797
5,234,802
15,848,907
114,427,232
AOSP app 5
vendor app
# Apps (%)
26 (15.12%)
30 (16.22%)
24 (16.33%)
29 (10.36%)
27 (27.00%)
28 (24.35%)
28 (15.91%)
28 (13.40%)
31 (42.47%)
31 (34.07%)
282 (18.22%)
# LOC (%)
# Apps
2,419,155 (24.06%)
6,344,721 (36.59%)
2,759,415 (28.61%)
4,718,633 (24.05%)
1,152,885 (18.72%)
3,170,950 (26.14%)
1,164,691 (15.15%)
1,800,690 (16.82%)
1,036,858 (19.81%)
2,506,778 (15.82%)
27,074,776 (23.66%)
114
119
94
190
40
63
123
156
41
57
997
# LOC
3,519,955
5,660,569
3,514,921
7,354,468
604,197
3,269,936
2,666,397
4,127,343
2,821,874
12,156,673
45,696,333
# Apps
third-party app
# LOC
4,113,781
5,334,152
3,369,112
7,550,704
4,403,086
5,688,955
3,858,043
4,776,764
1,376,070
1,185,456
41,656,123
32
36
29
61
33
24
25
25
1
3
269
Release Date1
Table 2: Release dates of examined Android devices
Device
Nexus S
Nexus 4
Wildﬁre S
Update Date 2 & Version
4.0.4, Mar 2012; 4.1.2, Oct 2012
N/A
N/A
4.1.1, Nov 2012; 4.1.2, Mar 2013
N/A
4.1.2, Mar 2013
2.3.6, Dec 2011; 4.0.3 Jul 2012; 4.1.1,
Jan 2013; 4.1.2, Apr 2013
4.1.1, Nov 2012; 4.1.2, Mar 2013
4.0.4, Aug 2012
4.1.2, May 2013
Dec 2010
Nov 2012
May 2011
May 2012
Feb 2011
Jun 2012
Apr 2011
One X
Optimus P350
Optimus P880
Galaxy S2
Galaxy S3
Xperia Arc S
Xperia SL
May 2012
Sep 2011
Sep 2012
1
The release dates may vary by area and carrier, but the difference is not that
signiﬁcant (e.g., around one month).
2
The update dates collected here are mainly for markets in United States.
280 apps with 19,623,805 LOC. The number of apps and LOC in-
crease 90.47% and 103.49%, respectively.
Our analysis also shows that, though the baseline AOSP is in-
deed getting more complicated over time – but vendor customiza-
tions are at least keeping pace with the AOSP. This trend is not dif-
ﬁcult to understand, as vendors have every incentive to add more
functionality to their newer products, especially in light of their
competitors doing the same.
The Google-branded phones are particularly interesting. Both
have relatively few apps, as they are designed to be reference de-
signs with only minor alterations to the core AOSP. However, the
Nexus 4 has over three times as many lines of code as were in-
cluded in the Nexus S, despite adding only 18 apps. The Nexus S
was the simplest phone we studied, while the Nexus 4 is the third
most complex – only the HTC One X and Samsung Galaxy S3,
which are well known for their extensive customization, have more
LOC. We attribute this to the fact that the Nexus 4 includes newer
versions of vendor-speciﬁc apps (e.g., Gmail) that have more func-
tionality with larger code size.
Meanwhile, we also observe these devices experience slow up-
date cycles: the average interval between two updates for a phone
model is about half a year! Also, it is interesting to note that the
updates in the United States tend to lag behind some other areas.
For instance, users of the Samsung Galaxy S3 in the United King-
dom received updates to Android 4.1.1 (which was released in July
2012) in September 2012, while the updates were not available for
users in the United States until January 2013. If we compare the
update dates in Table 1 with the corresponding dates of Android
releases [22], it often takes half a year for vendors to provide an of-
ﬁcial update (excepting Google’s reference phones) for users in the
United States, though in many cases carriers may share the blame.
Overall, ofﬁcial updates can hardly be called timely, which thereby
seriously affects the security of these devices.
3.2 Permission Usage Analysis
After determining the provenance of each pre-loaded app, our
next analysis captures the instances of permission overprivilege,
where an app requests more permissions than it uses. The results
are summarized in Table 3. On average, there is an alarming fact
that across the ten devices, 85.78% of apps are overprivileged.
Even Google’s reference devices do not necessarily perform bet-
ter than the others; the Nexus S has the second most overprivileged
apps of all the pre-2012 devices. Note that the situation appears
to be getting slightly better as time goes on. The average percent-
age of overprivileged apps in the post-2012 devices has decreased
to 83.61%, compared to 87.96% of all apps on pre-2012 devices.
But this situation is still hardly reassuring. Interestingly, our re-
sults show that the proportion of overprivileged pre-loaded apps
are more than the corresponding result of third-party apps (reported
by [2, 17]). We believe there are two main reasons: 1) pre-loaded
apps are more privileged than third-party apps, as they can request
certain permissions not available to third-party ones; 2) pre-loaded
apps are more frequent in specifying the sharedUserId property,