benignly trained baseline on an unpoisoned dataset.
Figure 4 plots the main result of this experiment, which
compares the accuracy of the ﬁnal trained model to the poi-
soning attack success rate. The three most recent methods
are all similarly vulnerable, with our attacks succeeding over
80% of the time. When we train the four older techniques to
the highest test accuracy they can reach—roughly 60%—our
poisoning attacks rarely succeed.
1582    30th USENIX Security Symposium
USENIX Association
PlaneCarBirdCatDeerDogFrogHorseShipTruckAverageOriginal Label of Poisoned ImageTruckShipHorseFrogDogDeerCatBirdCarPlaneAverageTarget (Desired) Label for Poisoned Image0.00.20.40.60.81.0                  Target (Test) Images                  Source (Labeled) Images0.00.20.40.60.81.0Dataset
(% poisoned)
MixMatch
UDA
FixMatch
CIFAR-10
SVHN
STL-10
0.1% 0.2% 0.5% 0.1% 0.2% 0.5% 0.1% 0.2% 0.5%
7/8
-
8/8
5/8
5/8
7/8
6/8
7/8
8/8
8/8
8/8
8/8
4/8
5/8
7/8
5/8
5/8
7/8
5/8
6/8
8/8
4/8
-
6/8
6/8
-
8/8
Table 1: Success rate of our poisoning attack across datasets and algorithms, when poisoning between 0.1% and 0.5% of
the unlabeled dataset. CIFAR-10 and SVHN use 40 labeled examples, and STL-10 all 1000. Our attack has a 67% success rate
when poisoning 0.1% of the unlabeled dataset, and 91% at 0.5% of the unlabeled dataset (averaged across experiments).
Dataset
(# labels)
MixMatch
UDA
FixMatch
CIFAR-10
40
5/8
5/8
7/8
250
4/8
5/8
7/8
4000
1/8
2/8
7/8
SVHN
250
4/8
4/8
6/8
4000
5/8
4/8
7/8
40
6/8
5/8
7/8
Table 2: Success rate of our attack when poisoning 0.1% of
the unlabeled dataset when varying the number of labeled
examples in the dataset. Models provided with more labels
are often (but not always) more robust to attack.
3.2.5 Evaluation across datasets
Figure 4: More accurate techniques are more vulnerable.
Success rate of poisoning CIFAR-10 with 250 labeled exam-
ples and 0.2% poisoning rate. Each point averages ten trained
models. FixMatch, UDA, and MixMatch were trained under
two evaluation settings, one standard (to obtain high accuracy)
and one small-model to artiﬁcially reduce model accuracy.
This leaves us with the question: why does our attack work
less well on these older methods? Because it is not possi-
ble to artiﬁcially increase the accuracy of worse-performing
techniques, we artiﬁcially decrease the accuracy of the state-
of-the-art techniques. To do this, we train FixMatch, UDA,
and MixMatch for fewer total steps of training using a slightly
smaller model in order to reduce their ﬁnal accuracy to be-
tween 70− 80%. This allows us correlate the techniques ac-
curacy with its susceptibility to poisoning.
We ﬁnd a clear relationship between the poisoning suc-
cess rate and the technique’s accuracy. We hypothesize this
is caused by the better techniques extracting more “mean-
ing” from the unlabeled data. (It is possible that, because we
primarily experimented with recent techniques, we have im-
plicitly designed our attack to work better on these techniques.
We believe the simplicity of our attack makes this unlikely.)
This has strong implications for the future. It suggests that
developing better training techniques is unlikely to prevent
poisoning attacks—and will instead make the problem worse.
The above evaluation considers only one dataset at one poi-
soning ratio; we now show that this attack is general across
three datasets and three poisoning ratios.
Table 1 reports results for FixMatch, UDA, and MixMatch,
as these are the methods that achieve high accuracy. (We omit
UDA on STL-10 because it was not shown effective on this
dataset in [66].) Across all datasets, poisoning 0.1% of the
unlabeled data is sufﬁcient to successfully poison the model
with probability at least 50%. Increasing the poisoning ratio
brings the attack success rate to near-100%.
As before, we ﬁnd that the techniques that perform better
are consistently more vulnerable across all experiment setups.
For example, consider the poisoning success rate on SVHN.
Here again, FixMatch is more vulnerable to poisoning, with
the attack succeeding in aggregate for 20/24 cases compared
to 15/24 for MixMatch.
3.2.6 Evaluation across number of labeled examples
Semi-supervised learning algorithms can be trained with a
varying number of labeled examples. When more labels are
provided, models typically perform more accurately. We now
investigate to what extent introducing more labeled examples
impacts the efﬁcacy of our poisoning attack. Table 2 summa-
rizes the results. Notice that our prior observation comparing
technique accuracy to vulnerability does not imply more ac-
curate models are more vulnerable—with more training data,
models are able to learn with less guesswork and so become
less vulnerable.
USENIX Association
30th USENIX Security Symposium    1583
0.00.20.40.60.81.0Mean (CIFAR-10) Model Accuracy0.00.20.40.60.81.0Attack Success Rate (CIFAR-10)FixMatchUDAMixMatchVATMeanTeacherPseudoLabelPiModelDensity Function
(1− x)2
φ(x + .5)
φ(x + .3)
x
x4 + (1− x)4
√
1− x
x2 + (1− x)2
1
(1− x)2 + .5
1− x
1.5− x
CIFAR-10 % Poisoned
0.5%
0.1% 0.2%
7/8
3/8
5/8
7/8
8/8
7/8
6/8
4/8
8/8
5/8
6/8
6/8
8/8
5/8
8/8
6/8
8/8
7/8
8/8
8/8
8/8
8/8
0/8
1/8
2/8
3/8
3/8
3/8
4/8
4/8
5/8
5/8
7/8
Table 3: Success rate of poisoning a semi-supervised machine
learning model using different density functions to interpolate
between the labeled example x(cid:48) (when α = 0) and the target
example x∗ (when α = 1). Higher values near 0 indicate a
more dense sampling near x(cid:48) and higher values near 1 indicate
a more dense sampling near x∗. Experiments conducted with
FixMatch on CIFAR-10 using 40 labeled examples.
3.2.7 Evaluation across density functions
All of the prior (and future) experiments in this paper use
the same density function ρ(·). In this section we evaluate
different choices to understand how the choice of function
impacts the attack success rate.
Table 3 presents these results. We evaluate each sampling
method across three different poisoning ratios. As a general
rule, the best sampling strategies sample slightly more heavily
from the source example, and less heavily from the target that
will be poisoned. The methods that perform worst do not
sample sufﬁciently densely near either the source or target
example, or do not sample sufﬁciently near the middle.
For example, when we run our attack with the function
ρ(x) = (1− x)2, then we sample frequently around the source
image x(cid:48), but infrequently around the target example x∗. As
a result, this density function fails at poisoning x∗ almost
always, because the density near x∗ is not high enough for
the model’s consistency regularization to take hold. We ﬁnd
that the label successfully propagates almost all the way to
the ﬁnal instance (to approximately α = .9) but the attack
fails to change the classiﬁcation of the ﬁnal target example.
Conversely, for a function like ρ(x) = x, the label propagation
usually fails near α = 0, but whenever it succeed at getting
past α > .25 then it always succeeds at reaching α = 1.
Experimentally the best density function we identiﬁed
was ρ(x) = 1.5− x, which samples three times more heavily
around the source example x(cid:48) than around the target x∗.
α = 0.0
α = 1.0
Figure 5: Label propagation of a poisoning attack over train-
ing epochs. The classiﬁer begins by classifying the correctly-
labeled source example x(cid:48) (when α = 0; image shown in the
upper left) as the poisoned label. This propagates to the in-
terpolation α > 0 one by one, and eventually on to the ﬁnal
example x∗ (when α = 1; image shown in the upper right).
3.3 Why does this attack work?
Earlier in this section, and in Figure 1, we provided visual
intuition why we believed our attack should succeed. If our
intuition is correct, we should expect two properties:
1. As training progresses, the conﬁdence of the model on
each poisoned example should increase over time.
2. The example α0 = 0 should become classiﬁed as the
target label ﬁrst, followed by α1, then α2, on to αN = 1.
We validate this intuition by saving all intermediate model
predictions after every epoch of training. This gives us a col-
lection of model predictions for each epoch, for each poisoned
example in the unlabeled dataset.
In Figure 5, we visualize the predictions across a single Fix-
Match training run.6 Initially, the model assigns all poisoned
examples 10% probability (because this is a ten-class clas-
siﬁcation problem). After just ten epochs, the model begins
to classify the example α = 0 correctly. This makes sense:
α = 0 is an example in the labeled dataset and so it should
be learned quickly. As α = 0 begins to be learned correctly,
this prediction propagates to the samples α > 0. In particular,
the example α = .25 (as shown) begins to become labeled as
the desired target. This process continues until the poisoned
example x∗ (where α = 1) begins to become classiﬁed as the
poisoned class label at epoch 80. By the end of training, all
poisoned examples are classiﬁed as the desired target label.
6Shown above are the poisoned samples. While blended images may look
out-of-distribution, Section 4.2 develops techniques to alleviate this.
1584    30th USENIX Security Symposium
USENIX Association
020406080100Training iteration1.00.80.60.40.20.0Confidence in poisoned labelα=0.00α=0.25α=0.50α=0.75α=1.004 Extending the Poisoning Attack
Interpolation Consistency Poisoning is effective at poisoning
semi-supervised machine learning models under this baseline
setup, but there are many opportunities for extending this
attack. Below we focus on three extensions that allow us to
1. attack without knowledge of any training datapoints,
2. attack with a more general interpolation strategy, and
3. attack transfer learning and ﬁne-tuning.
Dataset
(% poisoned)
MixMatch