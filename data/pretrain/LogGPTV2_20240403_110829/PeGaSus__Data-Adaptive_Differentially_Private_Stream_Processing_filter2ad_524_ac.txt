12:
13:
14:
end if
15:
16: end if
˜θpr ev ← ˜θ
17:
18: Return Pt
Pt ← Pt−1 ∪ {{t}} and let the last group {t} be open.
˜θ ← θ + Lap(4/ϵд )
˜θ ← ˜θpr ev
if (dev (Ct [G ∪ {t}]) + Lap(8/ϵд ))  θ. Thus we close G = {1, 2, 3} and create the
second group {4} which is also closed. Thus, P4 = {{1, 2, 3}, {4}}. At
time 5, the last group G = {4} is closed, so we start with a new open
group G = {5}. The final output at time 5 is P5 = {{1, 2, 3}, {4}, {5}}
.
With a realistic (non-infinite) setting for ϵд, the Grouper runs in
a similar manner but noise is added to both the threshold and to
the deviations that are compared with the threshold. Therefore, the
output partitions will differ.
To prove the privacy of Algorithm 2 we will use the following
lemma:
Lemma 3.3 (Sensitivity of deviation [19]). The global sensi-
tivity of the deviation function ∆(dev) is bounded by 2.
Theorem 3.4. Using Algorithm 2 to generate a partition at every
time ensures ϵд-differential privacy.
Proof. The algorithm is an implementation of the Sparse Vec-
tor Technique [13] on multiple disjoint sub-streams applied to the
dev function. The noise was injected to both the threshold and
the deviation value in order to ensure privacy. Because the sensi-
tivity of dev is bounded by 2 (Lemma 3.3), the noise added to the
threshold, in line (4), is computed as Lap(4/ϵд ) = Lap(2∆(dev)/ϵд ).
In line (7), the noise added to the deviation value is Lap(8/ϵд ) =
Lap(4∆(dev)/ϵд )). Based on the original proof of Sparse Vector
Technique from [13], this amount of noise ensures ϵд-differential
privacy for each generated group. In addition, the derived streams
from two neighboring source streams can only differ by 1 at one
time. Thus, there will be only one generated group different in
term of two neighboring source streams. By parallel composition,
Algorithm 2 satisfies ϵд-differential privacy.
□
3.3 Design of the Smoother
The Smoother computes the final estimate ˆct for each ct received
at time t based on all the noisy counts ˜Ct from the Perturber, in
combination with the current partition Pt . Suppose the last group
G = {t − k, . . . , t − 1, t} from Pt contains current index t with
the previous k indexes. Given this output from the Grouper, there
are several post-processing methods we may apply to generate an
estimate, ˆct , that improves upon ˜ct . These are alternatives for the
Smoother in Algorithm 1:
1. AverageSmoother: We use the average noisy counts of the data
indexed in group G to be the estimate of the current data.
(cid:80)
ˆct =
i∈G ˜ci
|G|
.
2. MedianSmoother: We use the median noisy counts of the data
indexed in group G to be the estimate of the current data.
ˆct = median{˜ci | i ∈ G}.
3. JSSmoother: We apply the James-Stein estimator [23] to update
the noisy count of the current data based on the noisy counts
of the data indexed in group G.
˜ct − avд
|G|
+ avд,
ˆct =
(cid:80)
i∈G ˜ci
|G |
where avд =
. We assume uniformity on each group
and apply the James-Stein estimator to let each estimate shrink
to the mean. (We can only use the noisy mean here in terms of
the privacy.)
We theoretically analyze the effect of AverageSmoother in the next
section and empirically evaluate all three variants in Section 6. We
conclude this section with an example.
Example 3.5. Continuing from Example 3.2, we have a stream
of true counts C5 = {5, 5, 6, 9, 10} with noisy counts from the
Perturber of ˜C5 = {5.6, 4.4, 6.7, 9.5, 10.2} and a final partition P5 =
{{1, 2, 3}, {4}, {5}} from the Grouper. We now illustrate how the final
estimates ˆC5 would have been produced using MedianSmoother as
Smoother. Recall that each ˆct is released in real-time based on Pt , the
groups at time t, and not the final grouping P5. At time 1, P1 = {{1}}
and the last group is G = {1}, ˆc1 = median{5.6} = 5.6. At time 2,
the last group is G = {1, 2}, ˆc2 = median{5.6, 4.4} = 5. At time
3, the last group is G = {1, 2, 3}, ˆc3 = median{5.6, 4.4, 6.7} = 5.6.
At time 4, P4 = {{1, 2, 3}, {4}} and the last group is G = {4}, so
ˆc4 = median{9.5} = 9.5. At time 5, the last group is G = {5},
ˆc5 = median{10.2} = 10.2. Thus, the final estimates are ˆC5 =
{5.6, 5, 5.6, 9.5, 10.2}.
3.4 Error analysis of smoothing
We now formally analyze how the online grouping and post-processing
Smoother may help for improving the accuracy of the output.
Theorem 3.6. Suppose one group G in the resulting partition from
Grouper contains n indexes, i + 1, i + 2, . . . , i + n. Assume that ˆC is
produced using the AverageSmoother as the Smoother. Then ˆC[G],
the resulting estimate for group G, will have lower expected error than
˜C[G], formally stated as
(cid:13)(cid:13)(cid:13) ˆC[G] − C[G](cid:13)(cid:13)(cid:13)2 ≤ E
(cid:13)(cid:13)(cid:13) ˜C[G] − C[G](cid:13)(cid:13)(cid:13)2
E
provided that the deviation of C[G] satisfies
(cid:112)2(n − ln n − 1)
(1 + ln(n − 1))ϵp
dev (C[G]) ≤
Proof. In terms of ˜C which are generated from the Perturber by
applying Laplace Mechanism, we have E
.
ϵ 2
At each time i+k, G = {i+1, . . . , i+k}. By using the AverageSmoother,
p
ˆci +k =
. Then the following holds:
(cid:13)(cid:13)(cid:13) ˜C[G] − C[G](cid:13)(cid:13)(cid:13)2 = n × 2
˜ci +1+···+ ˜ci +k
k
ci +1 + ni +1 + · · · + ci +k + ni +k
k
E
k =1
E[(
˜ci +1 + · · · + ˜ci +k
(cid:13)(cid:13)(cid:13) ˆC[G] − C[G](cid:13)(cid:13)(cid:13)2 = E(cid:16) n(cid:88)
n(cid:88)
n(cid:88)
n(cid:88)
n(cid:88)
ci +1 + · · · + ci +k
E[(
E[(
((avдk − ci +k )
2 + E[(
ci +1 + · · · + ci +k
k =1
+2 ∗ E[(
k =1
k =1
k
k
k
=
=
=
=
2(cid:17)
− ci +k )
˜ci +1 + · · · + ˜ci +k
(
k =1
− ci +k )
k
2]
− ci +k )
2]
− ci +k + ni +1 + · · · + ni +k
ni +1 + · · · + ni +k
− ci +k ) ∗ ni +1 + · · · + ni +k
2]
k
k
)
)
2]
]).
Since ni +1, . . . , ni +n are independent Laplace noise with param-
] = 0.
and E[ ni +1+···+ni +k
ni +1+···+ni +k
2] = 1
, E[(
)
eter 1
ϵp
k × 2
ϵ 2
p
k
k
k
Then we have
(cid:13)(cid:13)(cid:13) ˆC[G] − C[G](cid:13)(cid:13)(cid:13)2
n(cid:88)
(avдk − ci +k )
E
n(cid:88)
2 +
1
k
× 2
2