title:RedPlane: enabling fault-tolerant stateful in-switch applications
author:Daehyeok Kim and
Jacob Nelson and
Dan R. K. Ports and
Vyas Sekar and
Srinivasan Seshan
RedPlane: Enabling Fault-Tolerant Stateful In-Switch
Applications
Daehyeok Kim‚òÖ‚Ä†, Jacob Nelson‚Ä†, Dan R. K. Ports‚Ä†, Vyas Sekar‚òÖ, Srinivasan Seshan‚òÖ
‚òÖCarnegie Mellon University, ‚Ä†Microsoft
Abstract
Many recent efforts have demonstrated the performance benefits
of running datacenter functions (e.g., NATs, load balancers, moni-
toring) on programmable switches. However, a key missing piece
remains: fault tolerance. This is especially critical as the network is
no longer stateless and pure endpoint recovery does not suffice. In
this paper, we design and implement RedPlane, a fault-tolerant state
store for stateful in-switch applications. This provides in-switch
applications consistent access to their state, even if the switch they
run on fails or traffic is rerouted to an alternative switch. We address
key challenges in devising a practical, provably correct replication
protocol and implementing it in the switch data plane. Our evalua-
tions show that RedPlane incurs negligible overhead and enables
end-to-end applications to rapidly recover from switch failures.
CCS Concepts
‚Ä¢ Networks ‚Üí Programmable networks; In-network process-
ing; ‚Ä¢ Hardware ‚Üí Emerging technologies; ‚Ä¢ Computer sys-
tems organization ‚Üí Availability.
Keywords
Programmable switches, Programmable networks, Fault tolerance,
State replication
ACM Reference Format:
Daehyeok Kim, Jacob Nelson, Dan R. K. Ports, Vyas Sekar, Srinivasan Seshan 
. 2021. RedPlane: Enabling Fault-Tolerant Stateful In-Switch Applications. 
In ACM SIGCOMM 2021 Conference (SIGCOMM ‚Äô21), August 23‚Äì27, 2021, 
Virtual Event, USA. ACM, New York, NY, USA, 22 pages. https://doi.org/10. 
1145/3452296.3472905
1 
Today‚Äôs data center switches are no longer simple stateless packet
forwarders. They implement sophisticated network functions, such
as NATs, firewalls, and load balancers [6, 39, 55] and accelerate
distributed applications [45, 47, 67, 73, 78]. Cloud service providers
have even started deploying them in production networks [7].
Introduction
Such stateful processing in switches leads to a new challenge: 
fault tolerance. Classic network designs followed the end-to-end
principle [66], keeping critical state only on the end hosts. This en-
abled a fate-sharing approach to reliability [27]; when switches are
stateless, recovering from their failure simply entails finding a new
communication path. Stateful in-switch applications [7] challenge
This work is licensed under a Creative Commons Attribution International 4.0 License.
SIGCOMM ‚Äô21, August 23‚Äì27, 2021, Virtual Event, USA
¬© 2021 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-8383-7/21/08.
https://doi.org/10.1145/3452296.3472905
this paradigm; e.g., the failure of a switch running a load balancer
may cause the loss of its forwarding state, breaking thousands of
active connections. While data center networks are engineered with
redundant network paths [34, 65, 71] to provide fault tolerance at
the routing layer, there are no capabilities for recovering in-switch
state after failure.
Thus, we need to reconsider fault tolerance for in-switch pro-
cessing ‚Äì something previously done in ad hoc, application-specific
ways. Our goal in this paper is to ensure that, after a failure and
reroute, the same application state becomes available at the replace-
ment switch, without degrading performance and while remaining
transparent to end hosts.
Making switch state fault tolerant is uniquely challenging be-
cause of the scale and resource constraints involved. Techniques
like checkpointing and active replication, which have been applied
to software middleboxes [63, 70], are designed for server-based sys-
tems. These techniques rely on obtaining a consistent snapshot of
state and buffering output until state updates are durably recorded
to other servers. However, a switch‚Äôs high packet processing speed
(a few billion packets/second [13, 14, 19]) and its limited compute
and storage capabilities make it infeasible to translate these tech-
niques to the switch context.
In this paper, we introduce RedPlane,1 a fault-tolerant state store
for in-switch applications. RedPlane provides APIs for developers
to (re)write their stateful P4 programs and make them fault-tolerant.
This allows an application to retain consistent access to its state,
even if the switch it runs on fails or traffic is rerouted to an alter-
native switch. RedPlane achieves this through a data plane centric
replication mechanism that continuously replicates state updates
to an external state store implemented using DRAM on commodity
servers. Note that running entirely in the data plane channel is key
to keeping up with the switch‚Äôs full processing speed.
Realizing this high-level idea in practice entails several chal-
lenges. First, traditional notions of strict correctness with lineariz-
ability and exactly-once semantics for operations require reliable
communication and output buffering. However, this is infeasible
on the switch data plane due to its limited capabilities. Second, at
the traffic volumes the switch data plane needs to process, na√Øvely
requiring per-packet coordination with the server-based state store
imposes severe performance overheads. Last, routing decisions
when a switch fails could be unpredictable. Thus, we must be able
to transparently migrate the relevant state between two switches
regardless of the routing decisions.
We address these challenges with the following key ideas:
‚Ä¢ Based on the requirements of in-switch applications, we define
two practical correctness models. First, based on our observa-
tion that network applications are already resilient to packet
loss, we define a strict consistency mode by explicitly adopting
1The name denotes a replicated data plane.
223
SIGCOMM ‚Äô21, August 23‚Äì27, 2021, Virtual Event, USA
Daehyeok Kim, Jacob Nelson, Dan R. K. Ports, Vyas Sekar, Srinivasan Seshan
the standard definition of linearizability [36], which permits
operations that do not complete while still providing strong con-
sistency. Second, for write-centric applications (e.g., monitoring
using sketches [28]) that can tolerate approximate results, we
propose a relaxed consistency mode that allows some state to
be lost after a failure, but bounds the inconsistency with lower
overheads.
‚Ä¢ Instead of buffering packets using limited switch resources, we
use the network itself and state store‚Äôs memory as temporary
storage by piggybacking packet contents on coordination mes-
sages.
‚Ä¢ To enable reliable state replication, we build a lightweight se-
quencing and retransmission protocol that ensures state updates
are processed in the correct order, without requiring complex
protocols (e.g., TCP) in the switch data plane.
‚Ä¢ To avoid overheads due to frequent coordination with the state
store, we propose a lease-based state ownership protocol [33,
49, 57] to provide correctness without coordinating on every
state access and migrate ownership between different switches
as needed.
We design the RedPlane protocol that realizes our consistency
modes, prove its correctness, and confirm this using a TLA+ model
checker [18]. We implement a prototype of RedPlane in P4 [11]
and C++/Python, and show that different types of applications
can be fault tolerant using it. We evaluate it with various applica-
tions in our testbed consisting of two Tofino-based programmable
switches, four regular switches, and 10 servers. Our evaluation
results show that under failure-free operation, RedPlane has negli-
gible per-packet latency overhead for read-centric applications like
NAT, and less than 8 ùúás overhead even for the worst case. When
a switch fails, RedPlane can recover end-to-end TCP throughput
within a second by accessing the correct state.
2 Background and Motivation
In-network processing has flourished in recent years, as a natural
convergence of the demand for sophisticated network functionality
from data center operators and the commercial availability of pro-
grammable switch platforms [8, 13, 14]. Programmable switches are
used for classic middlebox functionality [39, 55], monitoring [6, 35],
DDoS defense systems [76, 77] and accelerating other networked
systems [38, 45‚Äì47, 52, 67, 73, 78].
These applications are stateful; i.e., state on the switch deter-
mines how to process packets. In this paper, we focus primarily on
hard state applications, where a loss of state disrupts network or
application functionality.2 An example is an in-switch NAT, where
the key state is an address translation table. Losing this state would
make it impossible to forward packets for existing connections.
Network model. We consider a deployment model where pro-
grammable switches are installed into the network fabric such that
all traffic to be processed by an in-switch application traverses one
of the programmable switches. This could be achieved in several
different ways, depending on the network architecture. In a typical
data center architecture (Fig. 1), this could be achieved by using the
2Other applications maintain only soft state in the switch and provide their own failure
recovery mechanisms. These are not the focus of our work, though RedPlane could
perhaps help simplify their design or improve recovery performance.
Figure 1: Impact of switch failures on in-switch NATs.
switches on all core or all aggregation-layer switches.3 All traffic
entering or leaving a cluster, for example, would traverse one of
these switches. Alternatively, an operator might deploy a cluster
of programmable switches as dedicated ‚ÄúNF accelerators‚Äù, explic-
itly routing traffic through them; this approach is similar to how
software load balancers [30, 60] are deployed today.
State partitioning. We assume that application state is partition-
able using some key derived from the packet header, and that each
packet‚Äôs processing uses only state from the associated partition.
In many cases, such as for the NAT example, the key will be the
IP 5-tuple, and, hence, we use ‚Äúpartition‚Äù and ‚Äúflow‚Äù interchange-
ably. However, other applications may use different partitioning,
e.g., partitioning on VLAN ID to detect heavy-hitter flows for a
particular tenant.
We also assume that the network is configured to provide best-
effort affinity such that packets from the same partition usually
arrive at the same switch. Standard layer-3 routing protocols such
as Equal-Cost Multi-Path routing (ECMP) provide this property
when they are configured to use the partition key as their hash key.
Primer on programmable switches. Programmable switch ar-
chitectures used today, e.g., Intel Tofino [14], use a limited amount
of on-chip memory (e.g., SRAM and TCAM) to provide a variety
of stateful object abstractions, including tables, registers, meters,
and counters. Applications can use these to keep state across mul-
tiple packets, such as the address translation table in the NAT
example above. In the ingress and egress match-action pipeline,
objects are allocated in each stage and accessed by packets via
ALUs. These objects are also accessible by the switch control plane
through the ASIC-to-CPU PCIe channel which has a limited band-
width (ùëÇ(10 Gbps)) compared to the ASIC‚Äôs per-port bandwidth
(ùëÇ(100 Gbps)). In addition, the ASIC provides other built-in func-
tionality such as packet replication, recirculation, and mirroring
for more advanced packet processing.4
2.1 Impact of Switch Failures
Switches can fail, either by a switch failing entirely (a fail-stop
model), or by individual links losing their connectivity. Measure-
ment studies in production data centers have shown that such
switch failures are prevalent. For example, in Microsoft‚Äôs data cen-
ter, 29% of customer-impacting incidents are related to hardware
3In principle, RedPlane could be deployed on top-of-rack (ToR) switches, but it is
potentially less useful. If each rack has one ToR switch, and it fails, connectivity to the
servers in that rack is lost. RedPlane can restore the switch state onto a different rack,
but depending on the application that may not be useful. However, if there are two
ToR switches per rack, RedPlane would be useful.
4While we use Tofino-based programmable switches for our work, we believe our
design can be implemented on other programmable switch ASICs since hardware
capabilities leveraged in RedPlane‚Äôs switch data plane (e.g., packet mirroring) are
general features supported by most switch ASICs.
224
NAT1NAT2TTTTServer racksCSWCSWInternetCSW: Core switchT: Top-of-Rack switchFlow ID(IP, port)Internal(IP, Port)(10.0.0.1,4321) (192.168.10.1, 1234)NAT1NAT2TTTTServer racksCSWCSWInternetFailureExternal √†Internal tableState for the flow does not exist.√†Connection brokenRedPlane: Enabling Fault-Tolerant Stateful In-Switch Applications
SIGCOMM ‚Äô21, August 23‚Äì27, 2021, Virtual Event, USA
State access
Read-centric
Write-centric
Mixed-read/write
Applications
NAT
Stateful firewall
Load balancer [55]
SYN flood defense [77]
Super-spreader detection [72]
Heavy-flow detection [53]
SGW in EPC [69]
In-network sequencer [46]
Per-object routing [47, 78]
In-network key-value store
Impact of switch failure
Connection broken
Connection broken