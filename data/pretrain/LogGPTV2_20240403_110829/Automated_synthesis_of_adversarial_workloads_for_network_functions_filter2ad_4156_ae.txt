# Automated Synthesis of Adversarial Workloads for Network Functions

## Table 5: List of Network Functions (NFs) and Tested Workloads
| NF | Median Deviation (ns) |
| --- | --- |
| **LB / Hash table** | 256 |
| **LB / Hash ring** | 112 |
| **LB / Red-Black Tree** | - |
| **LB / Unbalanced Tree** | - |
| **LPM / Patricia Trie** | 359 |
| **LPM / Lookup Table** | 131 |
| **LPM / DPDK LPM** | 103 |
| **NAT / Hash Table** | 179 |
| **NAT / Hash ring** | 109 |
| **NAT / Red-Black Tree** | 87 |
| **NAT / Unbalanced Tree** | 115 |
| **Zipfian Manual** | 141 |
| **CASTAN** | 141 |

The table above lists the network functions (NFs) and their corresponding median latency deviations from the NOP (No Operation) benchmark. The values indicate the median deviation in nanoseconds (ns).

## Discussion

### Evaluation of Adversarial Workloads

Our evaluation focused on scenarios with 100% adversarial traffic due to experimental setup limitations. In a more realistic setting, an adversary would inject only a fraction of the overall traffic as part of a DDoS campaign. We expect that even a limited adversary could cause significant damage due to head-of-line blocking. Adversarial workloads can increase the efficiency of such attacks by consuming disproportionately more resources for the same amount of attack traffic. A detailed cost-benefit analysis from the attacker’s perspective is necessary but is left for future work.

### Limitations of CASTAN

CASTAN has several limitations, particularly with simpler network functions where the input packet directly maps to processing performance. For more complex NFs, the performance envelope can be obfuscated, making it challenging to reverse-engineer adversarial workloads. One-way functions, such as hash functions, pose a challenge. We use rainbow tables to help reverse these, but this method can fail or only partially succeed when additional constraints on the hashed packet are present. Analyzing the hash function directly in symbolic execution is currently intractable, but with sufficiently powerful solvers or appropriate constraint algebra, it may be feasible.

Another challenge arises when the constraints on symbolic pointers during CASTAN analysis are incompatible with the limited contention sets captured by our cache model. A more complete model of cache behavior, based on actual cache slicing and eviction algorithms, would be more robust. Reverse engineering the internal structure of CPU caches is still an open problem in active research. Our current reliance on empirical models limits the predictive power of CASTAN.

### Related Work

#### Performance Evaluation and Diagnosis

Software performance attacks are well-studied. [13] describes adversarial complexity-based attacks on data structures and network applications and how to mitigate them. [3, 36] have studied specific IDS NFs, considering both algorithmic complexity and the cache. These works manually study specific systems, whereas CASTAN offers an automated approach to discovering such issues.

[30, 32, 39] use fuzzing-like approaches to automatically expose performance bottlenecks at the level of individual methods and data structures. Such approaches may not scale well for larger and less structured input spaces, as in NFs. [28] automatically detects and exploits second-order denial-of-service attacks in web services, which do not directly apply to NF environments. [7] uses symbolic execution to attack algorithmic complexity, which could benefit CASTAN for more complex NFs.

#### Quantifying Worst-Case Performance

There are two approaches to quantifying worst-case performance. The first, exemplified by CASTAN, under-approximates the worst-case performance by constructing adversarial workloads, providing a lower bound on the worst-case performance. The other approach, known as Worst-Case Execution Time (WCET) Analysis [40], over-approximates the WCET, providing conservative but safe upper bounds on execution time. While WCET provides formal performance guarantees, it does not generate adversarial workloads, which are crucial for debugging.

#### Online Performance Diagnosis Systems

With the adoption of software NFs, several proposals for online performance diagnosis systems exist. NFVPerf [26] leverages passive traffic monitoring to identify hardware and software bottlenecks in virtualized NFs. PerfSight [41] uses low-level packet processing performance metrics to detect and diagnose performance problems. These systems help diagnose performance issues at runtime given a specific NF workload. CASTAN complements these approaches by generating adversarial workloads for further diagnosis and debugging.

#### Program Analysis Applied to NFs

Several prior works have proposed using static analysis to understand, debug, and verify software NFs. StateAlyzr [20] identifies per-flow and global state in NFs to facilitate state migration and redistribution. Many approaches, like CASTAN, use symbolic execution to find bugs or formally verify correctness. [9, 10, 22, 44] leverage this technique to automate bug finding and test-case generation. [14, 43] use exhaustive symbolic execution to formally verify functional correctness. Others have extended symbolic execution to explore multiple systems, such as [23] and [29], which identify discrepancies and interoperability issues. [37] symbolically executes NF models to reason about network properties like reachability and loops. Emerging techniques automatically synthesize the models [42].

## Conclusions

In this paper, we present CASTAN, a tool that automates the generation of adversarial workloads for NFs, leading to poor performance. We statically analyze NF code using symbolic execution to find code paths that perform poorly. During analysis, we attack NF performance on three fronts: algorithmic complexity, adversarial memory access patterns, and reversing hash functions. Our results show that under ideal circumstances, a CASTAN workload can increase NF latency by 201% and decrease throughput by 19% compared to typical test network traffic. When the NF structure is simple enough for manual intuition, CASTAN-generated workloads behave similarly but are generated automatically. CASTAN completes in a reasonable amount of time, typically less than an hour.

## Acknowledgements

We thank the anonymous reviewers and our shepherd Sujata Banerjee for helping us improve our work. This research was funded by Starting Grant #BSSGI0_155834 from the Swiss National Science Foundation and Intel Corporation.

## References

[1] 2012. Intel Data Direct I/O Technology Overview. https://www.intel.com/content/dam/www/public/us/en/documents/white-papers/data-direct-i-o-technology-overview-paper.pdf. Accessed: 2018-06-25.

[2] 2018. The LLVM Compiler Infrastructure. https://llvm.org/. Accessed: 2018-06-14.

[3] Yehuda Afek, Anat Bremler-Barr, Yotam Harchol, David Hay, and Yaron Koral. 2016. Making DPI Engines Resilient to Algorithmic Complexity Attacks. IEEE/ACM Trans. on Networking 24, 6 (2016).

[4] Gorka Irazoqui Apecechea, Thomas Eisenbarth, and Berk Sunar. 2015. Systematic Reverse Engineering of Cache Slice Selection in Intel Processors. IACR Cryptology ePrint Archive 2015 (2015).

[5] Mike Barnett, Bor-Yuh Evan Chang, Robert DeLine, Bart Jacobs, and K Rustan M Leino. 2005. Boogie: A Modular Reusable Verifier for Object-Oriented Programs. In Formal Methods for Components and Objects.

[6] Theophilus Benson, Aditya Akella, and David A Maltz. 2010. Network traffic characteristics of data centers in the wild. In Internet Measurement Conf.

[7] Jacob Burnim, Sudeep Juvekar, and Koushik Sen. 2009. WISE: Automated test generation for worst-case complexity. In Intl. Conf. on Software Engineering.

[8] Cristian Cadar, Daniel Dunbar, Dawson R Engler, et al. 2008. KLEE: Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs. In Symp. on Operating Sys. Design and Implem.

[9] Marco Canini, Dejan Kostic, Jennifer Rexford, and Daniele Venzano. 2011. Automating the testing of OpenFlow applications. Intl. Workshop on Rigorous Protocol Engineering (2011).

[10] Marco Canini, Daniele Venzano, Peter Perešíni, Dejan Kostić, and Jennifer Rexford. 2012. A NICE Way to Test OpenFlow Applications. In Symp. on Networked Systems Design and Implem.

[11] CASTAN 2018. CASTAN code repository. https://github.com/nal-epfl/castan.

[12] Sophie Cluet and Claude Delobel. 1992. A general framework for the optimization of object-oriented queries. ACM SIGMOD Record 21, 2 (1992).

[13] Scott A Crosby and Dan S Wallach. 2003. Denial of Service via Algorithmic Complexity Attacks. In USENIX Security Symp.

[14] Mihai Dobrescu and Katerina Argyraki. 2014. Software Dataplane Verification. In Symp. on Networked Systems Design and Implem.

[15] Mihai Dobrescu, Katerina Argyraki, and Sylvia Ratnasamy. 2012. Toward Predictable Performance in Software Packet-Processing Platforms. In Symp. on Networked Systems Design and Implem.

[16] Paul Emmerich, Sebastian Gallenmüller, Daniel Raumer, Florian Wohlfart, and Georg Carle. 2015. MoonGen: A Scriptable High-Speed Packet Generator. In Internet Measurement Conf. https://doi.org/10.1145/2815675.2815692

[17] Patrice Godefroid. 2012. Test Generation Using Symbolic Execution. In IARCS Annual Conf. on Foundations of Software Technology and Theoretical Computer Science, Vol. 18. https://doi.org/10.4230/LIPIcs.FSTTCS.2012.24

[18] B. Han, V. Gopalakrishnan, L. Ji, and S. Lee. 2015. Network function virtualization: Challenges and opportunities for innovations. IEEE Communications Magazine 53, 2 (Feb 2015). https://doi.org/10.1109/MCOM.2015.7045396

[19] P. E. Hart, N. J. Nilsson, and B. Raphael. 1968. A Formal Basis for the Heuristic Determination of Minimum Cost Paths. IEEE Trans. on Systems Science and Cybernetics 4, 2 (July 1968). https://doi.org/10.1109/TSSC.1968.300136

[20] Junaid Khalid, Aaron Gember-Jacobson, Roney Michael, Anubhavnidhi Abhashkumar, and Aditya Akella. 2016. Paving the Way for NFV: Simplifying Middlebox Modifications Using StateAlyzr. In Symp. on Networked Systems Design and Implem.

[21] J. C. King. 1976. Symbolic Execution and Program Testing. J. ACM 19, 7 (1976).

[22] Nupur Kothari, Ratul Mahajan, Todd Millstein, Ramesh Govindan, and Madanlal Musuvathi. 2011. Finding protocol manipulation attacks. SIGCOMM Computer Communication Review 41, 4 (2011).

[23] Maciej Kuzniar, Peter Peresini, Marco Canini, Daniele Venzano, and Dejan Kostic. 2012. A SOFT Way for OpenFlow Switch Interoperability Testing. In Intl. Conf. on Emerging Networking Experiments and Technologies.

[24] Kin-Keung Ma, Khoo Yit Phang, Jeffrey S Foster, and Michael Hicks. 2011. Directed symbolic execution. In Intl. Static Analysis Symp.

[25] Philip J Mucci, Shirley Browne, Christine Deane, and George Ho. 1999. PAPI: A portable interface to hardware performance counters. In Proceedings of the department of defense HPCMP users group conference, Vol. 710.

[26] Priyanka Naik, Dilip Kumar Shaw, and Mythili Vutukuru. 2016. NFVPerf: Online performance monitoring and bottleneck detection for NFV. In IEEE Conf. on Network Function Virtualization and Software Defined Networks. https://doi.org/10.1109/NFV-SDN.2016.7919491

[27] Philippe Oechslin. 2003. Making a faster cryptanalytic time-memory trade-off. In Annual Intl. Cryptology Conf.

[28] Oswaldo Olivo, Isil Dillig, and Calvin Lin. 2015. Detecting and Exploiting Second Order Denial-of-Service Vulnerabilities in Web Applications. In Conf. on Computer and Communication Security. https://doi.org/10.1145/2810103.2813680

[29] Luis Pedrosa, Ari Fogel, Nupur Kothari, Ramesh Govindan, Ratul Mahajan, and Todd Millstein. 2015. Analyzing Protocol Implementations for Interoperability. https://www.usenix.org/conference/nsdi15/technical-sessions/presentation/pedrosa. In Symp. on Networked Systems Design and Implem.

[30] Theofilos Petsios, Jason Zhao, Angelos D Keromytis, and Suman Jana. 2017. Slowfuzz: Automated domain-independent detection of algorithmic complexity vulnerabilities. In Conf. on Computer and Communication Security.

[31] Mia Primorac, Katerina Argyraki, and Edouard Bugnion. 2017. How to Measure the Killer Microsecond. In ACM SIGCOMM Workshop on Kernel-Bypass Networks.

[32] P. Puschner and R. Nossal. 1998. Testing the Results of Static Worst-Case Execution-Time Analysis. In Real-Time Systems Symp.

[33] C.V. Ramamoorthy, S.-B.F. Ho, and W.T. Chen. 1976. On the Automated Generation of Program Test Data. IEEE Trans. on Software Engineering 2, 4 (1976).

[34] Vyas Sekar, Norbert Egi, Sylvia Ratnasamy, Michael K. Reiter, and Guangyu Shi. 2012. Design and Implementation of a Consolidated Middlebox Architecture. In Symp. on Networked Systems Design and Implem.

[35] Vyas Sekar and Petros Maniatis. 2011. Verifiable Resource Accounting for Cloud Computing Services. In Cloud Computing Security Workshop. https://doi.org/10.1145/2046660.2046666

[36] Randy Smith, Cristian Estan, and Somesh Jha. 2006. Backtracking algorithmic complexity attacks against a NIDS. In Annual Computer Security Applications Conf.

[37] Radu Stoenescu, Matei Popovici, Lorina Negreanu, and Costin Raiciu. 2016. SymNet: scalable symbolic execution for modern networks. In ACM SIGCOMM Conf.

[38] Wojciech Szpankowski. 1990. Patricia Tries Again Revisited. J. ACM 37, 4 (Oct. 1990). https://doi.org/10.1145/96559.214080

[39] Luca Della Toffola, Michael Pradel, and Thomas R. Gross. 2018. Synthesizing Programs That Expose Performance Bottlenecks. In Intl. Symp. on Code Generation and Optimization. https://doi.org/10.1145/3168830

[40] Reinhard Wilhelm, Jakob Engblom, Andreas Ermedahl, Niklas Holsti, Stephan Thesing, David Whalley, Guillem Bernat, Christian Ferdinand, Reinhold Heckmann, Tulika Mitra, Frank Mueller, Isabelle Puaut, Peter Puschner, Jan Staschulat, and Per Stenström. 2008. The Worst-case Execution-time Problem — Overview of Methods and Survey of Tools. ACM Trans. Embed. Comput. Syst. 7, 3, Article 36 (May 2008). https://doi.org/10.1145/1347375.1347389

[41] Wenfei Wu, Keqiang He, and Aditya Akella. 2015. PerfSight: Performance Diagnosis for Software Dataplanes. In Internet Measurement Conf. https://doi.org/10.1145/2815675.2815698

[42] Wenfei Wu, Ying Zhang, and Sujata Banerjee. 2016. Automatic Synthesis of NF Models by Program Analysis. In ACM Workshop on Hot Topics in Networks. https://doi.org/10.1145/3005745.3005754

[43] Arseniy Zaostrovnykh, Solal Pirelli, Luis Pedrosa, Katerina Argyraki, and George Candea. 2017. A Formally Verified NAT. In ACM SIGCOMM Conf.

[44] Hongyi Zeng, Peyman Kazemian, George Varghese, and Nick McKeown. 2012. Automatic test packet generation. In Intl. Conf. on Emerging Networking Experiments and Technologies.