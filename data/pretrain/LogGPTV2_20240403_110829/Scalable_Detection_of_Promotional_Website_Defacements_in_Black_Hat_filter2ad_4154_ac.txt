checking all text content of these sampled pages, we could not
ﬁnd any undetected obfuscated jargon. However, limited by
the sample size, our estimated recall and false negative rates
are expected be lower than the actual rates. By identifying
and recovering obfuscated jargons, DMOS can increase its
precision from 91.29% to 94.89%, which can reduce manual
efforts on processing false alarms by over 41%. More details
on the effectiveness of JNA are discussed in Section 4.1.3.
3.3 Detection of Web Page Defacements
Stealthy defacements have been widely used in modern de-
facement campaigns. On the one hand, injecting tiny illicit
content can make the defacement unnoticeable for detec-
tion. On the other hand, with minor modiﬁcations of the
site-ranking inﬂuential HTML tags (e.g., title, meta, etc.),
defacers can effectively advertise illicit content via search
engines, which drive the majority of web trafﬁc today. Below,
we ﬁrst discuss the limitation of existing approaches. We then
illustrate how to encode the tag information on top of different
neural networks to better identify stealthy defacements.
3.3.1 Limitation of Existing Solutions
The following plausible approaches turn out to be ineffective:
• Classical text classiﬁcation. Stealthy defacements only
introduce small illicit snippets. However, normal content
can be very long and embedded with confusing content.
Without context and hierarchy awareness, it is difﬁcult
for classical text classiﬁcation models like SVM or XG-
Boost to pay attention to suspicious regions to accurately
capture the defacement.
• Reference (Gold) copy. Modern web pages contain
many dynamically generated or even user-generated
content. Tracking changes with respect to a reference
(“gold”) copy of the web page is not as easy as for static
web pages. To get reasonable results, the service needs
to be deployed on customers’ servers. However, such a
requirement goes against our goal of providing a cloud-
based service that does not require any software modi-
ﬁcation or installation on customer’s machines, which
gives lower cost and better usability.
• Checking a few important tags only. Although
stealthy defacements typically occur in a few tags, de-
facements generally can appear in other tags (Section 5.1
shows 44 different tags). Furthermore, there are intricate
relationships among the tags. Checking separate tags
can result in the loss of essential hints. A straightforward
example is that the context of the text in  and
 should be consistent. Otherwise, it may indicate
defacements. This necessitates considerable efforts to
model the properties and relationship of tags.
USENIX Association
30th USENIX Security Symposium    3709
3.3.2 Key Insights
3.3.3 Design of Tag-Aware HAN Model
Figure 5: Architecture of the THAN Model
It is challenging to pinpoint the stealthy defacement. Enter-
prise websites are rather diverse. They integrate code and re-
sources from dozens of third-party service providers, ranging
from personalized ads, marketing tags, CDNs, to third-party
JavaScript libraries and many others. Legitimate web pages
can therefore contain seemingly illicit “noises” including le-
gitimate advertisements which promote legitimate products
(e.g., drugs, luxury, red wine, etc.), ambiguous yet legitimate
website content (e.g., love novels and movies which may have
on-the-verge description of adult content, etc.), and others. In
other words, stealthy defacements sometimes do not exhibit
any differences from these noises.
Fortunately, due to their different purposes, the “noisy” but
legitimate content and the illicit/ fraudulent ones (planted by
defacers) should appear in different regions of a web page.
For defacements, illicit snippets are typically injected into
those more informative HTML tags so that the promoted
products can appear not only in popular search results, but
also look innocent from the perspective of legal authorities.
In contrast, the noises of legitimate web pages are carefully
designed for better human viewing, for example, 1) legitimate
ads often appear in the banner and should not occur in title,
meta tags; 2) ambiguous website content (e.g., love novels,
legitimate lottery websites, etc.) often appears in the main
body part. In summary, promotional defacements essentially
show distinctive location and tag preferences, compared to
the noisy legitimate content.
According to this observation, promotional defacement
identiﬁcation requires a model to follow the page-ranking
algorithms of search engines to focus more on important tags
while avoiding the noises induced by normal regions that may
cause the web page to be misclassiﬁed. Towards this end,
we introduce a tag embedding scheme, which can be used
on top of different state-of-the-art neural networks, including
Hierarchical Attention Network (HAN) [46], BERT [24], and
others, to automatically learn the information and importance
of HTML tags. As shown in Table 2, such tag information can
signiﬁcantly improve the performance of these state-of-the-art
networks in detecting defacements.
We design the so-called Tag-aware Hierarchical Attention Net-
work (THAN for short) by introducing the tag embeddings on
top of the HAN [46] network. Figure 5 depicts the network
architecture of THAN. Following the design of HAN, THAN
also includes a two-layer attention mechanism (one at the
word layer and another at the sentence layer), which enables
the model to adjust the attention paid to individual words and
sentences. However, unlike the sentence embedding scheme
in HAN, we develop the tag-sentence embedding in the sec-
ond layer instead. This is to better capture the differences in
tag-preference between stealthy defacements and noises.
As introduced in Section 3.2.1, let (tagi,si) be the i-th tag-
sentence pair of page. We ﬁrst split si to a word sequence
[w1,w2,··· ,wn]. Using the word2vec model [37, 38], we can
learn the word embeddings 3 from the Chinese wiki corpus [9]
and the dataset collected in Section 3.1. Each word wt is
mapped to a D-dimensional embedding vector wet. Since
not all words contribute equally to a sentence, we follow
HAN [46], in the ﬁrst layer, to give more weight to more
informative words (e.g., jargons). We then aggregate these
weights to form sentence embeddings SEi as follows:
uit = tanh(WwGRU(ht−1,wet ) + bw)
αit =
exp((cid:104)uit ,uw(cid:105))
∑t exp((cid:104)uit ,uw(cid:105))
SEi = ∑
αithit
t
(1)
(2)
(3)
where Ww, bw, uw are trainable parameters and αit indicates
the attention weight of the t-th word for sentence si.
Tag-Sentence Embeddings. To learn the importance of
each tag, we map tagi to a trainable vector tagEi ∈ Rd, i.e.,
the tag embedding. We then concatenate the tag embedding
with the weighted sentence vector to yield the embedding of
the tag-sentence pair (tagi,si) as follows:
T SEi = [tagEi;SEi]
(4)
3For Chinese application, we actually learn the embeddings for each
character, which can generalize better than word embeddings. For ease of
presentation, we use word embedding to represent character embedding, if
not speciﬁed otherwise.
3710    30th USENIX Security Symposium
USENIX Association
(title,c1)(a,	c2)(p,	cM)...w1w2wntitleWE1WE2WEntagEitag-sentence	embeddingCE1webpage	embeddingtag-sentence	pairLow-levelAttentionHigh-levelAttentionCEMCE2yWECompared to sentence embeddings, such tag-sentence embed-
dings (T SEi) can reﬂect the differences in tag and location
preferences between stealthy defacements and noises which
promote similar products. To highlight those more suspicious
tag-sentence pairs, we use the second layer attention network
to automatically assign more weight for the defaced content
when constructing the web page embedding W Ei. The second
attention layer shares the same structure as the ﬁrst one. Fi-
nally, we feed the web page embedding to a fully connected
network to predict whether a web page has been defaced or
not.
y = sigmoid[(W ×W Ei) + b]
(5)
Training THAN model. The THAN model is trained by
TensorFlow [13]. In our experiments, each web page is split
into tag-sentence pairs, out of which we randomly sample 150
tag-sentence pairs as the input of our model. We map words
to 256 dimension continuous vectors of real numbers. Note
that the dimension of the tag embedding cannot be too high.
Otherwise, the neural network will give too much weight to
the tag and vice versa. Finally, we set the dimension of the
tag embedding to 32, at which point we can obtain a stable
model. We use the Adam optimization algorithm for training,
with an initial learning rate of 0.001. The dropout rate is set
to 0.3. The THAN model is trained for 3 epochs and the batch
size is set to 64.
3.3.4 Design of Tag-Aware BERT Model
To demonstrate the broad applicability of our method, we
integrate our proposed tag embedding to the state-of-the-art
NLP model, namely, Bidirectional Encoder Representations
from Transformers (BERT) to obtain Tag-aware BERT (T-
BERT for short). BERT is based on a multi-layer bidirectional
Transformer [41] structure and is the ﬁrst ﬁne-tuning based
representation model that achieves state-of-the-art results for
a wide range of NLP tasks. We use the open-source implemen-
tation of BERT [19] to conduct experiments. Unlike HAN
and THAN, in addition to the token embedding of each word,
BERT also utilizes segment embedding and position embed-
ding to enrich the information of the sentence representation.
These three kinds of embedding are aggregated to obtain the
sentence representation (i.e., SEi). Following the practice in
THAN, the tag embedding is concatenated with the sentence
vector to obtain the embedding of the tag-sentence pair. Then
an attention layer is applied to obtain the representation of the
entire web page. The hyperparameters and experiment setting
of training T-BERT are consistent with those of THAN.
3.3.5 Demystifying the Tag Embeddings
We use THAN as an example to visualize how and why tag
embeddings can improve detection performance. Firstly, we
would like to identify which tags are believed to be more
important by the tag embeddings. Generally, an embedding
Table 1: Top-8 Tag Embeddings
Tag
title meta
meta.
description
L2 Norm of
Tag Embedding
0.73
0.58
0.42
meta.
content marquee
0.24
0.26
a
span
div
0.18
0.18
0.16
with a large L2 norm can activate a large weight in a neural
network. Therefore, we use the L2 norm to measure the impor-
tance of different tags. Table 1 presents top tag embeddings
with larger L2 norms of a sample defaced page. We can see
that tags which are more likely to be defaced have larger L2
norms, e.g., title, meta, and so on. Refer to Section 5.1 for
tag preferences of defacers. Observe that, tags that are often
associated with noise (e.g., ads or ambiguous content) are
automatically learned to be trivial. This experiment demon-
strates that THAN, given the tag embedding, can assign a
heavier weight to suspicious tags while avoiding noises.
Case study of learned features. To verify the ability of
THAN in steering more attention towards informative tag-
sentence pairs and words in a web page, we visualize the
hierarchical attention layers for one random web page. Note
that the features are automatically learned, and no domain
knowledge is required. As shown in Figure 6, every line is
a tag-sentence pair. The bar represents the weight of the tag-
sentence pair, and the colored words denote they are more
informative in this sentence. To avoid clutter, we only show
the Top-10 most important tag-sentence pairs and only present
important words by normalizing their weights. As expected,
those informative tags (e.g., title, meta and a) dominate the
list. Upon closer examination, it is revealed that illicit key-
words contribute more weights for sentence embeddings. This
experiment also demonstrates the effectiveness of the THAN
model.
4 Evaluation of DMOS
We have implemented DMOS in Python using 12,500 lines
of code. Unlike other Proof-of-Concept research, DMOS is
a full-featured tool and has been deployed as a commercial
service in the real world. To determine the effectiveness of
DMOS, we evaluate it based on large-scale fresh data in the
wild. In this section, we analyze the performance of DMOS
on different datasets and compare it against classical machine-
learning models as well as commercial products by other
leading companies.
4.1 Ofﬂine Experiments
For the dataset collected in Section 3.1, we randomly split
70% of websites for training and use the remaining 30% for
ofﬂine testing. Instead of sampling web pages, we split by
websites since web pages of different websites are more di-
verse. This is to stress test the generalization power of DMOS.
For this reason, the number of web pages for ofﬂine testing
USENIX Association
30th USENIX Security Symposium    3711
Table 2: Classiﬁcation Performance in Ofﬂine Experiments
Methods
WAF
Saxe et al. [39]
BoW
HAN [46]
FastText [31]
BERT [24]
THAN
T-BERT
DMOS (JNA + THAN)
DMOS_V2 (JNA + T-BERT)
F1
Recall
Precision
93.32%
9.76% 17.67%
82.53% 85.62% 84.05%
86.56% 91.87% 89.14%
86.95% 93.86% 90.28%
81.47% 88.21% 84.71%
93.41% 97.64% 95.48%
91.29% 96.89% 94.00%
95.66% 98.76% 97.19%
94.89% 97.40% 96.13%
97.53% 99.37% 98.44%
4.1.2 Effect of Tag Information
To evaluate the effectiveness of the HTML tag information,
we ﬁrst compare THAN with HAN. The former introduces
the information of HTML tags and automatically learns the
weight of different tags. Table 2 shows that the use of tag-
derived information by THAN results in a considerable im-
provement of 4.34% in precision and 3.03% in recall over
HAN. Similarly, T-BERT can further improve the already-
high performance of BERT to 95.66% (precision) and 98.76%
(recall). Unfortunately, it is impractical to apply BERT and its
variations (i.e., DMOS_V2), or even its lightweight version
(e.g., ALBERT [32]) in practice. This is due to the huge num-
ber of model parameters (108 million parameters in TBERT
compare to 1.6 million in THAN) and slow inference speed.
In our experiments with hardware speciﬁed in Section 4.2.2,
THAN takes 0.28s while TBERT takes 9.87s to process each
web page on average. As a result, we deploy DMOS (with
JNA + THAN) for the online experiments. Note that DMOS
already outperforms all the state-of-the-art solutions, includ-
ing BERT. For the remainder of the paper, DMOS means
DMOS (with JNA + THAN), unless speciﬁed otherwise.
4.1.3 Effect of Jargon Normalization
To demonstrate the effectiveness of the proposed jargon nor-
malization algorithm, we compare THAN and BERT to their