different components of our video-based keystroke inference
framework (Figure 1). We will later provide details of each of
these components.
The recorded video ﬁrst undergoes multiple pre-processing
steps in the following order: (i) background removal, (ii) con-
version to grayscale, (iii) face detection, and (iv) segmentation
of left and right arm regions based on their relative position
with respect to the face. After pre-processing, the framework
employs a keystroke detection algorithm based on Structural
SIMilarity (SSIM) index [52] across all the frames in each of
the left and right side video segments. Finally, the framework
computes several motion features from the video segments
immediately before and after each detected keystroke, and
employs them in a dictionary-based prediction algorithm for
word inference. Let’s now provide details of each component.
B. Pre-processing
let us denote
the
lv
set of
frames
frames
Given a video v composed of
recorded
in the
at 30Hz,
video as v = {f1, f2, f3, . . . , flv}. Assuming that
the
video resolution is constant, each frame in v is com-
posed of m rows and n columns of pixel values such
(where i ∈ {1, 2, 3, . . . , m} and
that each pixel pi,j
j ∈ {1, 2, 3, . . . , n}) in a frame represents a RGB value. The
RGB value of a pixel represents the hue (color), saturation,
and brightness of that particular pixel in the frame. With this
representation of a recorded video, we now describe the four
pre-processing steps, in sequential order.
Background Removal. The background removal process is
applied to all frames (fi ∈ v), in order to identify the location
of the body in the frame. We utilize the DeepLabv3 model
[27] for this task, which employs Atrous Convolution with
upsampled ﬁlters to extract dense feature maps and to capture
long range context. Training of the model is done using he
Microsoft COCO dataset [34], which contains a rich set of
Recorded VideoPreprocessingFace DetectionSegmentationWord PredictionShoulder and Arm Contours  DetectionArm Displacement CalculationsWord InferenceKeystroke DetectionQuantify Body MotionSSIManddSSIMKeystroke Detection AlgorithmGrayscaleBackground RemovalFrame SegmentsKeystroke Frame Segmentshuman body related training samples. With the background re-
moved, we can focus purely on the body’s relative movements
vis-`a-vis typing. Example outputs of this background removal
process is shown in Figure 17 (Appendix C). This background
removal step makes our proposed framework agnostic to any
moving elements in the background. Let’s denote background-
removed frames as rfi.
Grayscale. We next convert all background-removed frames
({rf1,r f2,r f3, . . . ,r flv}) to colorimetric grayscale [46], us-
ing the RGB values of individual pixels in a frame. This
conversion to grayscale simpliﬁes all following steps by mak-
ing them color-independent. Let’s denote such background-
removed grayscale frames as rgfi.
Face Detection. Our next objective is to focus speciﬁcally on
the two arms, where typing related motion is most perceptible.
However, as webcam setups may not be predetermined or
homologous, we require a webcam and webcam setup agnos-
tic methodology for automatically and accurately identifying
arm regions across all background-removed grayscale frames
({rgf1,rg f2,rg f3, . . . ,rg flv}). To do so, we leverage on the
consistency in relative position of the target user’s arms with
respect to their face (Figure 18 in Appendix C). The intuition
is that the left arm will be located around the bottom-right
of the face in the frame, and similarly the right arm will
be located around the bottom-left of the face in the frame.
Face detection is a matured research topic, with several state-
of-the-art frameworks and training datasets readily available.
We utilize the CPU-friendly Faceboxes model [55], that
employs Rapidly Digested Convolutional Layers (RDCL) and
the Multiple Scale Convolutional Layers (MSCL), to detect
target user’s face in each frame. For training the Faceboxes
face detection model, we used the WIDER FACE dataset [15],
that consists of 12,880 diverse facial images.
Segmentation. The facebox generated by Faceboxes iden-
tiﬁes the user’s face and draws a rectangular boundary around
it (solid green rectangle in Figure 18). The objective of this
last part of the pre-processing is to utilize this facebox in
order to automatically segment
the left and right arms in
the background-removed grayscale frame. Assume that in a
given frame rgfi, the four vertices of the generated facebox
are located at pixels pj,k, pj,(k+a), p(j+b),k, and p(j+b),(k+a),
where a and b are the width and height of the facebox (in
pixels), respectively. Using these facebox vertices, the left arm
segment is calculated as the rectangular area of the frame
enclosed within the pixels p(j+b),(k+a), p(j+b),n, pm,(k+a), and
pm,n. Similarly, the right arm segment is calculated as the
area of the frame enclosed within the pixels p(j+b),1, p(j+b),k,
pm,1, and pm,k. Let’s denote the left and right arm segments
extracted from a frame rgfi as Ls
C. (Potential) Keystroke & Typing Activity Detection
i , respectively.
i and Rs
i and Rs
Using the preprocessed left and right arm segments from
all frames of the video (Ls
i , respectively, where
i ∈ {1, 2, 3, . . . , lv}), our next objective is to precisely de-
termine the time-stamps, i.e., the frames, when a keystroke
was typed using either hand. This is non-trivial because the
adversary does not have a view of the lower arm. In this
section, we propose a novel keystroke detection algorithm
which accurately detects keystroke events using only upper
hand arm movements as observed in Ls
i . The proposed
i and Rs
4
keystroke detection algorithm is applied independently for
each arm, i.e., the left and right arm segments Ls
i and Rs
(i ∈ {1, 2, 3, . . . , lv}) are processed independently for the
i
keystroke detection task.
Intuition. Every time the target user presses a key on her/his
keyboard, she/he undertakes some degree of hand movement,
the extent of which may vary depending on the typing style
(Appendix B) and position of the key on the keyboard. This
movement may be from a resting position, or from an earlier
keystroke using the same hand. Moreover, every keystroke
lasts for a few milliseconds, until the user depresses the key,
and during this time there is little to no movement. Finally,
after the keystroke is completed, the user’s hand moves on
to another key or back to a resting position. Intuitively, we
should be able to observe this pattern of body movements in
the video (v). Accordingly, our keystroke detection algorithm
(Algorithm 1 in Appendix D) is designed based on empirically
observed characteristics of the (left or right) arm segments
immediately before, during, and immediately after a keystroke.
The empirically observed characteristics that we leverage upon,
as described below, are fairly independent of the typing style.
For simplicity, going forward we will describe the keystroke
detection process only for the left-hand. The process for the
right hand is identical.
Quantifying Body Motion. SSIM [52] is a well-known
metric for measuring the similarity between two images, and a
high SSIM index between consecutive frames would denote
insigniﬁcant body movements, and vice versa. To quantify
left-hand body movements across consecutive frame segments,
i+1 (i ∈
we compute SSIM index between every Ls
{1, 2, 3, . . . , lv − 1}). This results in a series of SSIM indices
},
SSIM Ls
where (cid:46)(cid:47) is the SSIM operator. To understand the rate of
change in body movements across consecutive frame seg-
ments, we also compute the discrete derivative of SSIM Ls
3) −
as dSSIM Ls
)}. In
(Ls
terms of body motion detected between the frame segments,
SSIM Ls may be viewed as the ‘speed’ and dSSIM Ls as
the ‘acceleration’. Similarly, we also independently compute
SSIM Rs and dSSIM Rs for the right hand.
Observed Characteristics. We observed a consistent pattern
in the dSSIM Ls measurements, which is in line with our
intuition outlined earlier. We observed that:
2) − (Ls
2 (cid:46)(cid:47) Ls
lv−1) − (Ls
= {(Ls
4), . . . , (Ls
3), (Ls
lv−1 (cid:46)(cid:47) Ls
lv
1 (cid:46)(cid:47) Ls
lv−2 (cid:46)(cid:47) Ls
lv−1 (cid:46)(cid:47) Ls
lv
= {Ls
i and Ls
1 (cid:46)(cid:47) Ls
3, . . . , Ls
2, Ls
2 (cid:46)(cid:47) Ls
3 (cid:46)(cid:47) Ls
2 (cid:46)(cid:47) Ls
(1) If the video is recorded at 30 f ps, and a keystroke occurred
in frame t, there exists a local maxima in dSSIM Ls at
t ). This is a result of the
(Ls
increase in body motion immediately before a keystroke
followed by the lack of body motion for the duration of
the key press.
t−1) − (Ls
t−2 (cid:46)(cid:47) Ls
t−1 (cid:46)(cid:47) Ls
(2) The above local maxima is followed by a local minima
within the next 0.05 sec. For a video captured at 30 f ps,
this means that the local minima occurs among the next
two elements of dSSIM Ls, i.e, (Ls
t−1 (cid:46)(cid:47) Ls
t (cid:46)(cid:47)
t+1) or (Ls
t+2). This is a result
Ls
t+1 (cid:46)(cid:47) Ls
of the lack of body motion for the duration of key press
followed by the body motion immediately after a keystroke
when the user’s hand moves on to another key or back to a
resting position. If no local minima is detected within this
t+1) − (Ls
t ) − (Ls
t (cid:46)(cid:47) Ls
where potential keystrokes could have occurred, but these de-
tected potential keystrokes could also include non-typing activ-
ities (false positives). We leverage on a few intuitive heuristics
in order to distinguish between the (detected) keystrokes that
correspond to a typing activity from those that may correspond
to non-typing activities similar to typing. The ﬁrst heuristic,
referred as maximum speed ﬁlter, ﬁlters out false positives
from the detected keystrokes by observing the maximum rate at
which these (potential) keystrokes are detected by Algorithm 1.
Studies have shown that most users typically type at a rate of
about 4 keystrokes per second, and that it is highly unlikely
to come across a typing rate of 10 or more keystrokes per
second [7]. Thus, the maximum speed ﬁlter will ﬁlter out (as
false positives) from the detected keystroke frames those that
correspond to a rate of 10 or more keystrokes per second, per
hand.
The second heuristic, referred as location ﬁlter, ﬁlters out
false positives by determining if both hands are on or near
the keyboard. Here, the basic idea is to ﬁrst create a set of
reference frames (K) where the target user is most likely
typing (i.e., hands on/near the keyboard), and then use these
reference frames to determine (using optical ﬂow [45]) if
their hand(s) are at a signiﬁcantly different position in the
other remaining frames corresponding to potential keystrokes.
Speciﬁcally, we create the reference set K by including all
potential keystroke frames in each 2-second window, if and
only if the window contains at least four detected potential
keystrokes (both hands combined) with each hand contributing
two or more potential keystrokes. This condition represents a
very likely case of typing activity, and thus the user’s hands
being on or near the keyboard. We then use this reference set
to ﬁlter out as false positive any potential keystroke frame that
is not within an empirically evaluated optical ﬂow distance
threshold to any frame (of the corresponding hand) in the
reference set K.
Similar to the maximum speed ﬁlter, the next heuristic we
employ is the minimum speed ﬁlter which ﬁlters out detected
potential keystrokes as false positives if they occur at a rate of
one (or lower) keystroke per second, combined for both hands
(i.e., representing highly unlikely typing rate). The fourth and
ﬁnal heuristic, referred as exclusive hand ﬁlter, attempts to
detect one-handed non-typing activities (e.g., mouse clicks)
that often get classiﬁed by Algorithm 1 as potential keystrokes.
Speciﬁcally, in each 10-second window, the exclusive hand
ﬁlter ﬁlters out 10 or more consecutive potential keystrokes
with the same hand as false positives. Figure 3 outlines the
order in which these heuristics are applied for ﬁltering out
false positives.
All potential keystrokes that are not ﬁltered based on the
above four heuristics represent typing activity and are used for
word predictions. Figures 19 and 20 in Appendix E further
elucidates the working of our heuristic-based approach by
means of two real scenarios that we encountered during our
experimentation. We present a comprehensive evaluation of its
performance later in Section X.
D. Word Prediction
We now describe how the adversary can infer words that
were typed from the detected keystrokes, using two different
groups of information. The ﬁrst group of information is simply
Fig. 2: An example of dSSIM Ls during a typed word. Circles
represent the actual keystroke event, whereas squares represent
the local minima within two frames of every keystroke.
time frame, it would imply that the user’s body movements
are likely not related to keystrokes.
t−1 (cid:46)(cid:47) Ls
t (cid:46)(cid:47) Ls
t+1), or (Ls
t (cid:46)(cid:47) Ls
t−2 (cid:46)(cid:47) Ls
t+1)− (Ls
An example of this pattern can be observed in Figure 2.
Keystroke Detection Algorithm. We utilized the above ob-
served characteristics to design a keystroke detection algo-
rithm (Algorithm 1) that automatically labels frames where
keystrokes potentially happened. In addition to the above
observed characteristics, Algorithm 1 also employs a ﬁltering
technique to eliminate body movements that are not related
to typing activity, but may still trigger a false positive. Algo-
rithm 1 ﬁlters based on statistical analysis of magnitudes in
dSSIM Ls. According to this ﬁltering technique, a frame t is
considered to be a keystroke frame if:
t−1)− (Ls
(1) dSSIM Ls at (Ls
t ) lies between
φaσdSSIM Ls and φbσdSSIM Ls , in addition to being the
local maxima. σdSSIM Ls
is the standard deviation in
the distribution of magnitudes in dSSIM Ls, and optimal
values of φa and φb can be determined empirically.
(2) The local minima following t in dSSIM Ls ((Ls
t−1 (cid:46)(cid:47)
t )− (Ls
t+2)) is
t+1 (cid:46)(cid:47) Ls
Ls
less than φcσdSSIM Ls . Again, σdSSIM Ls is the standard