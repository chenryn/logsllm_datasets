sions during November 2016. We extracted the bursts using a 10 s
sliding window: a burst starts (resp. stops) when the number of
withdrawals contained in the window is above (resp. below) a given
threshold. We choose 1,500 and 9 withdrawals for the start and
stop threshold respectively, which correspond to the 99.99th and
the 90th percentile of the number of withdrawals received over any
10 s period. Overall, we found a total of 3,335 bursts; 16% of them
(525) contained more than 10,000 withdrawals, and 1.5% of them
(49) contained more than 100,000 withdrawals. Our measurements
expose four major observations.
BGP routers often see bursts of withdrawals. We computed the
number of bursts that would be observed by a router maintaining
a growing number of peering sessions randomly selected amongst
the 213 RouteViews and RIPE RIS peering sessions. Fig. 2(a) shows
our results. The line in the box represents the median value, while
the whiskers map to the 5th and the 95th percentile. In the median
case, a router maintaining 30 peering sessions would see 104 (resp.
33) bursts of at least 5k (resp. 25k) withdrawals over a month. Even
if a router maintains a single session, it would likely see a few large
bursts each month. Indeed, 62% of the individual BGP sessions we
considered saw between 1 and 10 bursts of withdrawals, 24% saw
more than 10 bursts. Only 14% of the sessions did not see any. As a
comparison, single routers in transit networks routinely maintain
tens to hundreds of sessions [27] – even if not all those sessions
might carry the same number of prefixes as the ones in our dataset.
Learning the full extent of an outage is slow. While most of
the bursts arrived within 10 s, 37% (1,239) of them lasted more than
10 s, and 9.7% (314) lasted more than 30 s (see Fig. 2(b)). This also
means that withdrawals within bursts tend to take a long time to
be received. In the median case (resp. 75th percentile), BGP takes
13 s (resp. 32 s) to receive a withdrawal.
Large bursts take more time to be learned. Unsurprisingly,
large bursts take more time to propagate than smaller ones (see
Fig. 2(b)). Overall, we found that 98 bursts took more than 1 min to
arrive, with an average size of ≈81k withdrawals.
A significant portion of the withdrawals arrive at the end of
the bursts. We took the bursts lasting 10 s or more and divided
each of them in three periods of equal duration: the head, the middle
and the tail. We found that although most of the withdrawals tend
to be in the head of a burst, 50% of the bursts have at least 26% (resp.
10%) of their withdrawals in the middle (resp. in the tail). For 25%
of the bursts, at least 32% of the withdrawals are in the tail.
84% of the bursts include withdrawals of prefixes announced
by “popular” ASes. We examined the Cisco “Umbrella 1 Million”
dataset [6] which lists the top 1 million most popular DNS domains.
From there, we extracted the organizations responsible for the top
100 domains: Google, Akamai, Amazon, Apple, Microsoft, Facebook,
etc. (15 in total). 84% of the bursts we observed included at least
one withdrawal of a prefix announced by these organizations.
Technical Report, 2017,
(a) Bursts of withdrawals are frequent
and involve a high number of prefixes,
including popular ones.
(b) Bursts take a long time to propagate;
longer bursts even more.
Figure 2: Size and duration of bursts captured from 213 BGP
vantage points in November 2016.
Slow BGP convergence lead to significant traffic losses.
2.2.2
While countless studies have shown that BGP convergence can
cause long downtime on data-plane connectivity [18, 36, 42, 44,
52, 61, 62, 65], we confirmed the data-plane impact of a few bursts
of withdrawals propagated by a national ISP (with more than 50
routers). Specifically, we analyzed the bursts sent by the ISP to its
BGP neighbors over a period of three months. Among them, we
selected three bursts which included more than 10k withdrawals
and which matched with an event logged by the ISP. By checking
their logs, the operators identified the root causes of the bursts:
two maintenance operations and a peering failure at one of their
Internet eXchange Points (IXPs). At least two of these three bursts
induced downtime for transit traffic towards up to 68k prefixes,
including popular destinations.
2.2.3 Operators care: a 72-operators survey.
To substantiate the problem of slow convergence on operational
practices, we performed an anonymous survey among 72 operators.
The survey contained 17 questions grouped in three main topics:
i) the operators’ perception on slow BGP convergence (do they
care?); ii) the duration of BGP-induced downtime they usually
experience; and iii) their views on speeding-up convergence upon
remote outages (would they do it?).
Breakdown of respondents: Our respondents come from a wide
variety of networks providing one or more services to a large cus-
tomer base. The majority of the respondents (67%) provides Internet
connectivity to end-users (wired or mobile). 46% of them provides
transit services. 19% of them work for Content Distribution Net-
works (CDN), while 15% of them work for an Internet Exchange
Points (IXP). In terms of customer base, 33% of our respondents con-
nect 1 million or more users to the Internet, 48% connect 100,000
users or more, while 66% connect at least 10k users. 76% of our
respondents work with full Internet routing tables, meaning that
their routers carry more than 650k prefixes [5] in their forwarding
tables.
Operators care about slow convergence: 78% of the respondents
care about slow BGP convergence. The remaining 22% do not care
because they: receive a single default route from their provider (3 of
them); do not have stringent Service Level Agreement to meet (6 of
T. Holterbach et al.
them); or because they have never experience a slow convergence
before (4 of them).
76% of the respondents actively aim at decreasing their local
convergence time by: tweaking the different BGP timers used by
the routers (27 of them) or tuning the BGP transport parameters (21
of them); 40 respondents use fast-detection detection mechanisms
(BFD) and 21 of them deployed fast-reroute techniques (BGP PIC
or MPLS fast-reroute). When considering only transit networks (33
respondents), 67% of them rely on fast-detection detection mecha-
nisms, and 45% of them, on fast-reroute techniques.
Few operators monitor BGP-induced downtime (it is hard),
but those who do routinely experience slow convergence upon
remote outage: The majority (76%) of the operators do not collect
statistics about BGP convergence as it is hard in practice (we pro-
vide such measurements in §2.2). Among the 17 operators who do
(9 of which are transit ISPs), the majority (52%) observe average
BGP convergence time upon remote outage above 30 seconds. Only
4 of them experience average convergence time below 10 seconds.
Regarding worst-case convergence time upon remote outages, 87%
(resp. 35%) of the respondents observe convergence time above 1
(resp. 5) minutes.
The vast majority of the operator would adopt a solution
solving remote outages like SWIFT: 67 of our respondents (95%)
indicated that they would consider adopting a fast-reroute solution
to speed-up remote outage such as
3 OVERVIEW
Fig. 3 shows the workflow implemented by a SWIFTED router. We
now describe the result of implementing such workflow on the BGP
border router2 of AS 1 in Fig. 1.
Before the outage. The SWIFTED router in AS 1 continuously
pre-computes backup next-hops (consistently with BGP routes)
to use upon remote outages. This computation is done for each
prefix and considering any link on the corresponding AS path. For
example, the AS 1 router chooses AS 3 or AS 4 as backup next-hop
for rerouting the 20k prefixes advertised by AS 7 and AS 8 upon the
failure of link (1, 2). In contrast, it can only use AS 3 as backup to
protect against the failure of link (5, 6) for the same set of prefixes,
since AS 4 also uses (5, 6) prior to the failure. SWIFT then embeds a
data-plane tag into each incoming packet. Each SWIFT tag contains
the list of AS links to be traversed by the packet, along with the
backup next-hop to use in the case of any link failure.
Upon the outage. After receiving a few BGP withdrawals caused
by the failure of(5, 6), the SWIFTED router in AS 1 runs an inference
algorithm that quickly identifies a set of possibly disrupted AS links
and affected prefixes. The router then redirects the traffic for all
the affected prefixes to the pre-computed backup next-hops. To do
so, it uses a single forwarding rule matching the data-plane tags
installed on the packets. As a result, AS 1 reroutes the affected
traffic in less than 2 s (independently from the number of affected
prefixes), a small fraction of the time needed by BGP (see Table 1).
When rerouting, SWIFT does not propagate any message in BGP.
2Without loss of generality, we assume that a single router in AS 1 maintains all the
BGP sessions with AS 2, AS 3 and AS 4.
151530Number of peering sessions050100150200250300350400Number of bursts in a monthmin burst size = 5000min burst size = 10000min burst size = 2500020406080Burst duration (s)0.00.20.40.60.81.0CDFBursts lower than 10kBursts greater than 10kSWIFT: Predictive Fast Reroute
We proved that this is safe provided that the SWIFT inference is
sufficiently accurate (§3.3). When BGP has converged, i.e., the burst
of withdrawals has been fully received and BGP routes have been
installed in the forwarding table, the router removes the forwarding
rules installed by SWIFT and falls back to the BGP ones.
In the following, we provide more details about the main compo-
nents of SWIFT. In §3.1, we overview the inference algorithm (fully
described in §4) and how its output is used in a SWIFTED router.
In §3.2, we illustrate how SWIFT quickly reroutes data-plane pack-
ets on the basis of tags pre-computed by the encoding algorithm
detailed in §5. We finally report about SWIFT guarantees in §3.3.
3.1 Inferring outages from few BGP messages
The SWIFT inference algorithm looks for peaks of activity in the
incoming stream of BGP messages. Each detected burst triggers
an analysis of its root cause. To identify the set of links with the
highest probability of being affected by an outage, the algorithm
combines the implicit and explicit information carried by BGP
messages about active and inactive paths. For example, the failure
of (5, 6) in Fig. 1 may cause BGP withdrawals indicating the un-
availability of paths (1, 2, 5, 6) and (1, 2, 5, 6, 8) for all the prefixes
originated by AS 6 and 8. Receiving these withdrawals makes the
algorithm assign a progressively higher failure probability to links
in {(1, 2),(2, 5),(5, 6),(6, 8)}. Over time, the algorithm decreases
the probability of links (1, 2) and (2, 5), because prefixes originated
by ASes 2 and 5 are not affected, and the probability of link (6, 8),
because not all the withdrawn paths traverse (6, 8).
SWIFT aims at inferring failures quickly, yet keeping an eye
on accuracy. Inference accuracy and speed are conflicting objec-
tives. Indeed, precisely inferring the set of affected AS links might
be impossible with few BGP messages, as they might not carry
enough information. For instance, SWIFT cannot reduce the set
of likely failed links any further than the entire path (1, 2, 5, 6, 8)
until it receives other messages than withdrawals for that path.
Rerouting based on partial information can unnecessarily shift non-
affected traffic, e.g., all the prefixes originated by ASes 2 and 5. In
contrast, waiting for BGP messages takes precious time (§2) during
which traffic towards actually-affected prefixes can be dropped.
To avoid unnecessary traffic shifts, SWIFT evaluates the like-
lihood that its inferences are realistic (e.g., using historical data).
For instance, SWIFT evaluates the probability that a burst includ-
ing withdrawals for all the prefixes originated by ASes 6, 7 and 8
happens. If a burst of similar size is unlikely, SWIFT waits for the
reception of more messages to confirm its inference. Given that
withdrawals for prefixes from AS 7 and 8 will likely be interleaved
with path updates for AS 6, this strategy quickly converges to an
accurate inference, as we show in §6.
SWIFT uses a conservative approach to translate inferences
into predictions of affected prefixes. Remote failures are often
partial, that is, an outage can cause traffic loss for a subset of the
prefixes traversing the affected link(s). For instance, a subset of the
prefixes traversing the failed link (5, 6) in Fig. 1 can remain active
because of physical link redundancy between AS 5 and 6, or be
rerouted by intermediate ASes (e.g., 5) to a known backup path (like
the prefixes originated by AS 7). As BGP messages do not contain
Technical Report, 2017,
enough information to pinpoint the set of prefixes affected by an
outage, SWIFT reroutes all the prefixes traversing the inferred links.
Doing so minimizes downtime at the potential cost of short-lived
path sub-optimality (for a few minutes at most).
SWIFT inference works well in practice. Our experiments on
real BGP traces (see §6) show that SWIFT enables to reroute 90%
(median) of the affected prefixes after having received a small frac-
tion of the burst, and less than 0.60% of the non-affected prefixes.
3.2 Fast data-plane updates independently of
the number of affected destinations
Upon an inference, a SWIFTED router might need to update for-
warding rules for thousands of prefixes. In general, routers are slow
to perform such large rerouting operations as they update their
data-plane rules on a per-prefix basis.3 Previous studies [24, 63] re-
port median update time per-prefix between 128 and 282 µs. Hence,
current routers would take between 2.7 and 5.9 seconds to reroute
21k prefixes (as the router in AS 1 has to do in Fig. 1), and more
than 1 minute for the full Internet table (650k prefixes) – even if
BGP could converge instantaneously.
SWIFT speeds up data-plane updates by rerouting according
to packet tags instead of prefixes. A SWIFTED router relies on
a two-stage forwarding table to speed up data-plane updates. The
first stage contains rules for tagging traversing packets. SWIFT
tags carry two pieces of information: (i) the AS paths along which
they are currently forwarded; and (ii) the next-hops to use in the
absence (primary next-hops) or presence (backup next-hops) of any
AS-link failure. The second stage contains rules for forwarding the
packets according to these tags. By matching on portions of the
tags, a SWIFTED router can quickly select packets passing through
any given AS link(s), and reroute them to a pre-computed next-hop.
Since tags are only used within the SWIFTED router, they have local
meaning and are not propagated in the Internet (they are removed
at the egress of the SWIFTED router).
Using again Fig. 1, we now describe the rules in the forwarding
table of the SWIFTED router in AS 1. Fig. 3 shows the tags returned
by the SWIFT encoding algorithm. The first stage of the forwarding
table contains rules to add tags consistently with the used BGP
paths. Since prefixes in AS 8 are forwarded on path (2, 5, 6, 8), it
contains the following rule.
match(dst_prefix:in AS8) >> set(tag:00111 10011)
The first part of the tag identifies the AS path. It maps specific
subsets of bits to AS links in a given position of the AS path. The
first two bits represent the first link in the AS path, which is link
(2, 5). Consistently with Fig. 3, those bits are therefore set to 00.