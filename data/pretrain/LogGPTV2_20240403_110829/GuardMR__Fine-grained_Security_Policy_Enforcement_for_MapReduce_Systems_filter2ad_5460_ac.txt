### Compressed Sequence in HDFS for Experiments

For our experiments, we used compressed sequences stored in HDFS. The files consist of 10M, 20M, 30M, 40M, and 50M key-value records, which occupy 48GB, 96GB, 144GB, 192GB, and 240GB of space, respectively. Each data record is approximately 11KB in size and is generated using two types of data:

1. **Relational Data:**
   - This part is organized as a relational table simulating a medical dataset. It includes personal information such as name, address, age, doctor’s name, diagnosis, etc.
   - We used 1000 different male and female first and surnames, 32 different treatment groups, and 100 diagnosis types, selected uniformly at random.
   - Other columns, such as age, phone numbers, and social security numbers (SSNs), are uniformly distributed within their respective domain ranges.

2. **Unstructured Text Data:**
   - This part contains unstructured text representing the medical history of patients written by doctors.
   - We used 10 different real-life medical histories for this part.

Each record's key is labeled with a set of security classifications. Additionally, HDFS is configured to use a replication factor of 3, achieving an approximate load rate of 60%.

### Access Control Filters (ACFs)

We generated five ACFs for our experiments: two predicate ACFs, two modification ACFs, and one combination ACF. These ACFs are:

1. **Key ACF:**
   - Uses the security classification labels in the key of each pair to filter unauthorized pairs.

2. **Relational ACF:**
   - Filters records based on the doctor's name column.

3. **Sanitization ACF:**
   - Sanitizes phone numbers in the relational part using a regular expression (detailed in §VI-B).

4. **Redaction ACF:**
   - Reads a set of medicine names and transforms the medical history of patients into a list of medicines existing in the medical histories.

5. **Combination ACF:**
   - Combines the first four ACFs, applying them in the order: key, relational, sanitization, and redaction ACFs.

The ACFs were first generated using the given configuration and then called from the aspects injected into Hadoop as part of the Vigiles system (Vigiles implementation). The same ACFs were also implemented directly into the Hadoop source code (integrated implementation) to compare performance. To do this, all built-in `RecordReader` classes of Hadoop were enhanced with ACFs, and four pointcut methods of these classes were overridden.

Note that the integrated implementation cannot provide the same security guarantees as the Vigiles implementation because MapReduce jobs can contain custom `RecordReader` classes, which would bypass the ACFs in the integrated implementation.

Using AspectJ compiler version 1.7.3, the aspects were compiled independently of the Hadoop and job source code and then weaved into `hadoop-core-1.1.2.jar`, where the `RecordReader` methods are called. During the execution of the generated ACFs, we observed performance issues, especially with lazy copies. We believe this is due to poor optimization of the AspectJ compiler, as analyzed in [25]. To address this, we optimized the functions used in three phases of ACFs by preventing unnecessary data copies.

### Queries

We implemented three MapReduce jobs for our experiments:

1. **Selection Query:**
   - Selects records by patient name.

2. **Ranking Query:**
   - Sorts records by the ascending ordered list of doctors having the most patients.

3. **Statistic Query:**
   - Calculates the average age of patients with heart disease.

### Results

We first ran the three MapReduce jobs on each dataset described in §VII-B to measure the performance without ACFs (termed raw performance). Figure 6(a) shows the performance of the queries. The ranking and statistic queries run faster than the selection query because their mappers emit less data to reducers. The total running time of the queries was used as our primary metric.

To evaluate the performance of the generated ACFs and the injection technique, we measured the overhead by comparing the Vigiles implementation (no safety policy enforced) and the integrated implementation (safety policies integrated into Hadoop). Three queries were run on five datasets under the following conditions:
1. No ACF active.
2. Integrated ACFs active.
3. Vigiles ACFs (weaved by AspectJ) active.

#### Overhead of ACFs

Figures 6(b) to 6(f) show the running time of the selection query. The overhead of predicate ACFs (key and relational) is almost negligible (0.23% and 0.63% on average). In contrast, the modification ACFs (sanitization and redaction) have higher overheads (16.68% and 28.8%) due to costly functions used in fetch phases (e.g., regular expression search and whitelisting via a hashmap). Table I summarizes the overhead of ACFs for each query type.

#### Overhead of ACF Injection

The overhead of the injection technique is less than 1% for all ACFs except the sanitization ACF, which has an 8.36% overhead on average. We attribute this to the relatively poor optimization of the AspectJ compiler (see [25] for detailed performance analysis). Table II shows the overhead of the injection technique for each query type. Figures 6(g), 6(h), and 6(i) illustrate the running time of ranking and statistic queries for the key, redaction, and combination ACFs.

#### Overhead for Multiple Users

To compare the performance of ACFs and the injection technique for multiple MapReduce jobs, we conducted additional experiments. The selection query was simultaneously run by multiple users on a dataset containing 10M records, with both Vigiles and integrated implementations of ACFs assigned to the MapReduce jobs. The fair scheduler, developed by Zaharia et al. [26], was employed in Hadoop and set to preemptive mode to evenly assign resources to jobs. We started with 1 user and exponentially increased the number of users up to 8. The graphs in Figure 7 show the performance of the two approaches when the selection query is run. The average performance differences are 0.14%, 0.56%, 0.02%, and 0.05% for the key, relational, and redaction ACFs. The performance of the integrated implementation is slightly better than that of the Vigiles implementation, with the difference decreasing as the number of users increases due to higher query running times. The performance difference for the sanitization ACF is 7.15%, consistent with the previous observation of poor AspectJ compiler optimization.

### Conclusion

To our knowledge, Vigiles is the first system that provides a critical security component for MapReduce, fine-grained access control (FGAC), without modifying the source code of the MapReduce system. It achieves modular policy enforcement by rewriting the front-end API of the MapReduce system with runtime monitors (RMs). Our empirical results indicate that Vigiles exhibits just 1% overhead compared to the implementation that modifies Hadoop’s source code.

### Acknowledgements

This work was partially supported by the Air Force Office of Scientific Research FA9550-12-1-0082, National Institutes of Health Grants 1R0-1LM009989 and 1R01HG006844, National Science Foundation (NSF) Grants Career-CNS-0845803, CNS-0964350, CNS-1016343, CNS-1111529, CNS-1228198, and Army Research Office Grant W911NF-12-1-0558.

### References

[References listed here as provided in the original text]

---

This revised version aims to improve clarity, coherence, and professionalism while maintaining the technical details and structure of the original text.