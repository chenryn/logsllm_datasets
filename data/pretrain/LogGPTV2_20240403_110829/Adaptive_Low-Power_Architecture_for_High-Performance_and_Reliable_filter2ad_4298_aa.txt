title:Adaptive Low-Power Architecture for High-Performance and Reliable
Embedded Computing
author:Ronaldo Rodrigues Ferreira and
Jean da Rolt and
Gabriel L. Nazar and
&apos;Alvaro Freitas Moreira and
Luigi Carro
2014 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
Adaptive Low-Power Architecture for High-Performance and
Reliable Embedded Computing
Ronaldo R. Ferreira, Jean da Rolt, Gabriel L. Nazar, ´Alvaro F. Moreira, Luigi Carro
Instituto de Inform´atica
Universidade Federal do Rio Grande do Sul
{rrferreira, jrjoaquim, glnazar, afmoreira, carro}@inf.ufrgs.br
Porto Alegre, Brazil
Abstract—This paper presents the Matrix Operation Micro-
processor Architecture (MoMa) for reliable embedded comput-
ing. MoMa introduces a software execution mechanism based
on transactions, which provides a localized error correction
scheme that leads to reduced error correction latency and hard-
ware redundancy without incurring on expensive execution
checkpointing. Coupled to the transactional software execution
is a dedicated adaptive core for matrix multiplication which is
protected with a hardware implementation of the Algorithm-
Based Fault Tolerance technique. MoMa drives the matrix core
in an adaptive fashion based on dynamically turning it on
only when high-performance computation is necessary, leading
to ultimate power savings and error coverage. We performed
an exhaustive FPGA-implemented fault injection campaign,
in which we observed an error detection coverage of almost
100% and an error correction coverage of almost 98% on
average. MoMa is also evaluated in terms of power, area, and
performance, showing its competitiveness against a classical
TMR solution.
Keywords-adaptive computing; error correction; hardening;
matrix multiplication; radiation; soft error; transaction;
I. INTRODUCTION
Radiation induced soft errors are already a serious issue
in current transistor technology [1], and they will become
widespread with the predicted size of the transistor in the
forthcoming years being as low as 5.9 nm by 2026 [2].
Until now restricted to critical application domains such as
aviation and aerospace, soft errors soon will also become
a threat in commodities systems that deal with radiation
induced electromagnetic interference such as automotive [3].
Engineers of application domains that cannot trade re-
liability for performance are having hard times with the
low-performance offered by radiation tolerant architectures.
Fig. 1 shows a comparison of commercial off-the-shelf
(COTS) microprocessors against radiation hardened ones
for performance [4]. Rad-hardened microprocessors have a
performance gap of approximately 10 years to their COTS
counterparts at the same generation. The gap is even bigger
in terms of unitary price: a 25 MHz radiation hardened
RAD6000 microprocessor costs U$ 200,000 [5], while an
Intel i7 costs U$ 300. NASA launched a call for projects
to design the next generation rad-hardened space micro-
processors constructed only with COTS parts [6], with the
main requirements being: i) high-performance and multicore
parallel architecture; ii) adaptability in power consumption;
iii) radiation hardened; and iv) programmability in C and
compatibility with standard development and debug tools.
The GPU, the lowest cost high-performance COTS plat-
form available today, delivers requirement (i) and partially
(iv) due to, respectively, its massive number of regular pro-
cessing units and the C-dialect CUDA or OpenCL languages.
A GPU consumes more than 200W in the latest NVIDIA
cards, and is sensitive to an unacceptable soft error rate [7],
failing to meet requirements (ii) and (iii).
Another issue for multicore microprocessors is that power
consumption is going to a historical maximum, which will
make mandatory to turn-off large portions of the chips even
when performing useful computation (this problem is known
as dark silicon) [8]. Dark silicon is expected to create a
tough challenge for reliable cores as well, because we cannot
simply turn off areas of the chip devoted to reliability.
The challenge that power imposes on the reliability of
embedded computing has made efﬁcient error correction a
‘wishful thinking’. Research has concentrated on i) exact
schemes of error detection based on the assumption of
using bigger transistors for detection logic [9]; or, wherever,
possible, on ii) exploiting the characteristics of the system
domain to perform error correction based on accepting
an error margin in the correction precision [10]. Besides
missing the beneﬁts of Moore’s Law, the ﬁrst approach does
not apply because the old technology of today is already
sensitive to radiation. The second approach is not feasible as
a general solution because some domains do require an exact
behavior when handling errors in computation, even though
general purpose approximate architectures aimed to reduce
the energy overhead wrt. exact computation exist [11].
The burden of fault tolerant exact computation in embed-
ded architectures comes from the fact that if consistency is
a requirement, there must be a mechanism in the system
that guarantees the recovery to a state deemed consistent
in case of errors, i.e., state checkpoint and rollback are
necessary for error recovery. Error recovery is known to
be very expensive in fault tolerant systems, which incurs
in performance overhead from 25% to almost 100% of the
978-1-4799-2233-8/14 $31.00 © 2014 IEEE
DOI 10.1109/DSN.2014.56
538
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:23:31 UTC from IEEE Xplore.  Restrictions apply. 
100000
)
S
P
I
M
(
t
u
p
h
g
u
o
r
h
T
r
o
s
s
e
c
o
r
P
10000
1000
100
10
1
1980
COTS processor
Radiation-hardened processor
Core 2
Pentium IV
Pentium III
Pentium Pro
PPC440
LEON3
Pentium
80486DX
80386DX
PPC630e RAD750
RT ULP ColdFire
R6000
Mangoose V
~10 years performance gap
1990
2000
Year
2010
Figure 1.
Performance gap between commercial off-the-shelf (black
circles) and radiation hardened microprocessors (white triangles) along
decades - data adapted from [4].
entire application [12] and likely leads to deadline losses in
embedded real-time systems.
Industry has adopted custom accelerator cores as a way
to get around power limitations. The key idea is to dispatch
dedicated computation, e.g., speech recognition, to dedicated
cores that maximize performance with utmost power efﬁ-
ciency, e.g., DSPs. However, these dedicated accelerators
usually process a huge amount of data, what makes very
difﬁcult to efﬁciently checkpoint them.
This paper presents the Matrix Operation Microprocessor
Architecture (MoMa), a low-power embedded architecture
capable of on-line error recovery based on a fast and local-
ized rollback scheme coupled with an adaptive accelerator
core for matrix operations with the following characteristics:
• Early Error Detection: the architecture has two RISC
cores executing in lock-step for general purpose com-
putation. The instructions executing in the two cores
are compared against each other during their entire life-
cycle in the pipeline, e.g., if an error is detected as early
as in the fetch state, this instruction is discarded and
the error can be corrected early;
• Tight Error Containment: a program is partitioned
into atomic units we call Transactional Basic Block
(TBB). The TBB is a basic block that deﬁnes all
registers it needs for computation, and it also terminates
them at the end of its execution. The execution of a
TBB does not share any registers with another TBB,
eliminating the need of register ﬁle coherency;
• On-Line and Efﬁcient Error Recovery: due to tight
error containment, if a TBB ﬁnishes with no errors
it is asserted as correct and committed. Otherwise, an
error is detected and only the faulty TBB is re-executed
without any software checkpoint;
• Memory Correctness: because the errors are contained
inside a single TBB, only the store instructions can
corrupt memory. The proposed architecture guarantees
that only the correct execution of store instructions are
allowed to modify the memory and that the incorrect
ones are discarded before they can corrupt the memory;
• Low-Power within Small Area: due to the short live-
ness imposed on registers, the TBB uses considerably
fewer registers than standard basic blocks do, which
requires a smaller register ﬁle, thus less area. MoMa
in comparison with standard TMR and even DMR
arrangements, cuts the register ﬁle by half, leading to
aggressive power and area savings;
• High Predictability: because the TBB prevents the
propagation of errors to other program regions, the
error recovery latency in the worst case is equal the
number of instructions of the TBB where the error was
detected. The latency of the proposed error recovery
scheme can be computed during compilation, a much
desired characteristic for real-time systems;
• Adaptive High-Performance Computation: based on
an accelerator core that is driven by the general purpose
core in order to exploit available opportunities to reduce
power consumption by adaptively dispatching dedicated
computation into it.
The rest of the paper is organized as follows: Section II
discusses the main architectural issues for embedded sys-
tems reliability that MoMa approaches; Section III presents
the MoMa architecture and its implementation; Section IV
shows the experimental evaluation of the physical properties
and error coverage of the MoMa architecture; Section V
discusses related work; and, Section VI concludes the paper.
II. RELIABILITY IN EMBEDDED SYSTEMS
Techniques to support reliability in embedded systems are
constrained by power dissipation and in less extent by area.
The ITRS predicts that resource constrained embedded sys-
tems will have to perform all their useful computation within
a power budget of 3 Watts [2] – fault tolerance circuitry
included. In this scenario, classical solutions such as triple
modular redundancy (TMR) are not feasible. In this case,
designers usually decide to employ dual modular redundancy
(DMR), which does not provide error correction capabilities
but it is much more power efﬁcient. In DMR arrangements,
hence, it is necessary to include some mechanism of error
recovery, usually based on ‘checkpointing’.
Checkpointing is a technique used to create a consistent
architectural state where the system can rollback in case an
error is detected. The problem with checkpointing is that
its efﬁciency is severely reduced depending on how many
instructions are allowed to execute before the architectural
state is stored. Fig. 2 summarizes the ﬁndings presented
in [12] about the efﬁciency of redundant multithreading
checkpointing, where two threads execute the same code and
their store instructions are compared for error detection and
rolled back to the last checkpoint to recover the error, which
shows that in the best case scenario, the total overhead to
539
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:23:31 UTC from IEEE Xplore.  Restrictions apply. 
Figure 2. Overhead costs of checkpointing an application when the number
of executed instructions increase. Adapted from [12].
Figure 3. Block diagram of the MoMa architecture.
recover the architecture to a consistent state is at least 25%
of the total execution time. In critical embedded systems,
which usually have some sort of real-time behavior, this high
overhead jeopardizes its timing constraints.
One of the most critical components in terms of architec-
tural reliability is the register ﬁle, and, thus, it is mandatory
to protect it because most of the faults that corrupts archi-
tectural state occur in the register ﬁle [13]. In an experiment
using the Verilog description of the ARM926EJ-S architec-
ture, it is reported that 57.4% of the faults that creates an
error occurs in the register ﬁle [13]. The sensitiveness of the
register ﬁle is caused by the long period that its data can
be alive on it, i.e., the time until the register is re-written.
The vulnerability of a register is the period in terms of the
number of instructions from the time where the register is
written until it is read in another portion of the program [14].
A corrupted register ﬁle can break the checkpointing
mechanism in case corrupted data happens to be check-
pointed. To overcome this situation, the trivial solution is to
duplicate the register ﬁle. However, this solution increases
the critical area of the circuit and power dissipation. By
duplicating the register ﬁle, the peak temperature of the
circuit increases because the register ﬁle is one of the hottest
component of the chip [13], what might lead to undesirable
thermal variations and reliability issues.
The discussion of this section leads to the requirements
that an architecture for reliable embedded computing have
to meet to be feasible:
1) The checkpointing rate and the size of checkpointed
data must be reduced to a bare minimum. This reduc-
tion overcomes the performance overhead presented
in Fig. 2 and allows the reliability mechanism to be
deployed even in real-time systems;
2) The vulnerability of the register ﬁle must also be
reduced as much as possible. In an ideal setting, the
register ﬁle cannot be duplicated in order to avoid the
increase on sensitiveness to upsets;
3) The reliability mechanism must be power efﬁcient
because of the predicted ‘power wall’ of 3 Watts.
The next section presents the architectural innovations that
MoMa introduces to fulﬁll the three aforementioned require-
ments, supported by experimental evaluation presented in
Section IV.
III. THE MOMA ARCHITECTURE
A. MoMa Overview
The general block diagram of the MoMa architecture is
depicted in Fig. 3. MoMa is a Harvard architecture with
separated data and instruction memories. The memory has a
read/write port and a read-only port. The transactional core
is the unit that contains MoMa’s ALU. It has two inner RISC
cores in lockstep, with one core being the master and the
other the slave. MoMa has a single register ﬁle shared among
all of its units. The register ﬁle has two operation modes:
‘read/write’ mode, in which the executing instructions can
write to registers; and ‘read-only’, in which instructions can
read data from registers but cannot modify them. Only the
master core modiﬁes the register ﬁle. MoMa assumes that
the register ﬁle and the data and instruction memories are
protected with error-correcting code (ECC).
it
thus,
The transactional core is responsible for dispatching
computation to the RA3 core,
is the only unit
that accesses the instruction memory. The RA3 core only
interacts with the data memory, which is split into two data
banks. The separation of the data memory into two portions
is necessary in order to allow the RA3 core to exploit the
available parallelism at its best. The detailed implementation
of these two cores is given in the forthcoming sections.
B. General Purpose Computation: The Transactional Core
This section introduces the transactional core in details. In
the transactional core, a program is partitioned into atomic
execution units deﬁned as follows:
A TBB is basic block that starts with a sequence of load
and arithmetic instructions for register deﬁnition, followed
by a sequence of store instructions for register termination,
and ends with a terminator instruction ﬁnishing the TBB
execution. The execution of a TBB is only concluded when
no error is detected. If an error is detected, a rollback
mechanism re-executes the TBB from its ﬁrst instruction.
The TBB deﬁnition contains two concepts: i) register
deﬁnition; and ii) register termination. Register deﬁnition is
540
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:23:31 UTC from IEEE Xplore.  Restrictions apply. 
register is updated one cycle after the two copies inside the
cores in the instruction decode stage. This update operation
is triggered whenever a new TBB commences, which is
detected in the hardware by the conclusion of a control-
ﬂow instruction (conditional or unconditional branch) of the
TBB being committed. In this case, if the two registers ‘TBB
Addr’ inside the cores match and the current TBB commits,
in the next cycle the ‘TBB Addr’ register of the rollback
machinery will be updated.
The ‘delayed TMR’ scheme is necessary because, when
a new TBB starts, the ‘TBB Addr’ registers inside the cores
are updated with the start address of that new TBB. However,
if there is an error exactly when these two registers are
updated, i.e., in the last branch instruction of the TBB,
the ‘TBB Addr’ register outside the cores still contains the
start address of the previous TBB because it is updated one
cycle after the other two ‘TBB Addr’ registers. If all the
‘TBB Addr’ registers were updated at the same cycle, in
this speciﬁc scenario it would not be possible to perform
the rollback.
The error detection in the rollback machinery takes place
at all pipeline stages. For instance, if an error is detected
right at the fetch stage, it does not propagate to deeper
stages, which reduces the detection and recovery latency.
In case any of the comparators ﬂags an error, the ‘Force
Reset’ signal is set, the program counter (PC) receives the
address contained in the ‘TBB Addr’ and the rollback starts.
If no error is ﬂagged the PC receives the appropriate value
according to the original control-ﬂow of the program.
The TBB has a well-deﬁned execution life-cycle in the
architecture, which is composed of three ordered steps: i)
data-ﬂow execution; ii) transaction; and iii) commit.
The data-ﬂow execution step is composed of memory
loads and arithmetic instructions, implementing the register
deﬁnition mentioned above. If an error is detected in the
data-ﬂow execution step, the rollback unit signals the error
to the two MicroBlaze cores to re-execute the TBB from the
start. That second TBB execution accesses the register ﬁle in
‘read/write’ mode, re-writing data to it. This error scenario
is depicted in Fig. 5a.
The transaction step executes the memory stores of the
TBB after the data-ﬂow execution step ﬁnishes. When the
TBB reaches the transaction, it is guaranteed that the register
ﬁle is correct and contains the ﬁnal computed values of the
block. Therefore, if an error is detected in the transaction,
the rollback unit signals the error and the TBB is re-
executed from the start and this second execution of the
TBB is done with the register ﬁle in ‘read-only’ mode.
Otherwise, if writing data to the register ﬁle were allowed,
this second TBB execution could fetch values from memory
that were modiﬁed by the previous partial execution of
the transaction step of the TBB and assign them to the
registers, leading the TBB to produce wrong results. This
error scenario is depicted in Fig. 5b. The transaction step
Figure 4. General Purpose Transactional Core.
necessary because the TBB is an atomic unit of execution,
i.e., its execution either completes correctly or a rollback
is triggered to correct the error. To enable the simply re-
execution of the TBB when an error is detected, i.e., to
sustain atomicity during TBB execution, if a TBB ‘A’ deﬁnes
a variable x that is also used by TBB ‘B’, ‘B’ cannot assume
that the register containing the value of x computed by ‘A’
is correct when execution reaches ‘B’. Register termination
is the process in which the TBB writes its computation into
memory. If the execution of the TBB ﬁnishes, all data it
has produced that is used by another TBB is committed to
memory. Otherwise, if there is an error in any instruction of a
TBB, rollback is activated and this same TBB is re-executed
without modifying the register ﬁle for error recovery.
Fig. 4 presents the block diagram of the transactional core,
which contains two inner cores in lock-step, having their
pipelines represented in the ﬁgure. Program execution in the
two cores is independent, but only one of the cores is allowed
to write data to the register ﬁle. This is necessary because