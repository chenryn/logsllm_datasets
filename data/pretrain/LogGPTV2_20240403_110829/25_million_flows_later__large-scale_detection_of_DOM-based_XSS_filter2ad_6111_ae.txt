### Section 6.2: Tainted Strings and Injection Points

In Section 6.2, we observe that a tainted string typically consists of about three tainted substrings on average. Consequently, an attacker has, on average, three potential injection points to exploit the techniques presented by Nikiforakis. This suggests that the numbers provided in this section should be considered as a lower bound.

### Related Work

#### Browser-Based Tools for DOM-Based XSS Testing

To the best of our knowledge, DOMinator [7] was the first browser-based tool to test for DOM-based XSS using dynamic taint-tracking. It instruments Firefox’s SpiderMonkey JavaScript engine. Unlike our technique, DOMinator does not track data flows at the byte level. Instead, it uses a function tracking history to record the operations performed on the original, tainted input, resulting in the final, still tainted, string flowing into a sink. Additionally, DOMinator does not offer fully automated vulnerability validation.

FLAX [25] is conceptually the closest approach to our work. Similar to our system, FLAX also employs byte-level taint-tracking to identify insecure data flows in JavaScript. However, there are several key differences where our approach improves upon FLAX:
- FLAX's taint analysis is not fully integrated into the JavaScript engine. Instead, the analysis is conducted on program slices translated into JASIL, a simplified version of JavaScript.
- In contrast, our approach extends JavaScript's low-level string type, achieving full language and API coverage.
- FLAX uses fuzzing for vulnerability testing, whereas our method leverages precise source and sink context information to create validation payloads that deterministically match the specific data flow characteristics.
- Our large-scale study successfully demonstrated compatibility with current web code practices, while FLAX was only evaluated on a set of 40 web applications and widgets.

Criscione [5] introduced an automatic tool for finding XSS problems in a scalable black-box manner. Like our approach, they use actual browser instances for test execution and vulnerability validation. However, they do not utilize taint propagation or precise payload generation, opting instead for a fuzzing approach.

Vogt et al. [30] presented a related approach that combines static analysis and dynamic information flow tracking to mitigate XSS exploits. Their focus is on security-sensitive values, such as user cookies, and the potential exfiltration of these values, rather than following the flow of untrusted data.

#### Server-Side Approaches and Static Taint Analysis

Several server-side approaches have been proposed to detect and mitigate XSS vulnerabilities using dynamic taint-tracking [20, 23, 4, 27, 19, 33, 2]. Additionally, static analysis of source code to identify insecure data flows is a well-established method [9, 28, 31, 14, 32, 10].

#### Attack Generation

To reduce false positive rates, various methods have been studied for automatically generating valid exploit payloads for validation purposes. In 2008, Martin et al. [18] presented a method to generate XSS and SQL injection exploits based on goal-directed model checking. Their system, QED, can perform a goal-directed analysis of any Java web application adhering to the standard servlet specification. The generated model allows the creation of a valid exploit for validation. Unlike our approach, QED focuses on server-side injection vulnerabilities.

Kieyzun et al. [15] also focus on the automatic generation of attacks targeting server-side injection vulnerabilities. They use symbolic taint-tracking and input mutations to generate example exploits. Their tool, Ardilla, works on server-side code, making it more suitable for traditional XSS vulnerabilities. Scaling is more challenging due to the multiple HTTP requests required to generate a valid exploit.

d’Amore et al. [6] present the tool snuck, which can automatically evade server-side XSS filters. However, the tool requires input from a human tester to identify the application’s intended workflows and possible injection vectors. The system then verifies the filter functions using XPath queries.

#### Empirical Studies on JavaScript Security

Due to the growing importance of client-side JavaScript in web applications, several security-relevant aspects have been studied empirically. Yue and Wang [34] examined the commonness of JavaScript practices that could lead to unauthorized code execution, such as cross-domain inclusion of external JavaScript files and usage of APIs that could lead to XSS. Their study is purely statistical and did not conduct real vulnerability validation.

Richards et al. [24] studied the usage of the `eval` API in the wild, identifying both usage patterns that could be solved with safe alternatives and instances where replacing `eval` would be challenging. Lekies and Johns [17] surveyed the Alexa top 500,000 sites for potentially insecure usage of JavaScript's `localStorage` for code caching. Son and Shmatikov [26] examined the Alexa top 10,000 sites for vulnerabilities arising from unsafe utilization of the `postMessage` API.

### Conclusion

In this paper, we presented a fully automated approach to detect and validate DOM-based XSS vulnerabilities. By directly integrating into the browser’s JavaScript engine, we achieve reliable identification of potentially insecure data flows while maintaining full compatibility with production JavaScript code. The precise, byte-level context information of the resulting injection points enables us to create attack payloads tailored to the specific conditions of the vulnerability, allowing for robust exploit generation.

Using our system, we conducted a large-scale empirical study, identifying 6,167 unique vulnerabilities across 480 domains. This demonstrates that 9.6% of the Alexa top 5000 sites carry at least one DOM-based XSS problem.

### Acknowledgments

This work was partially supported by the EU Projects WebSand (FP7-256964) and STREWS (FP7-318097). We gratefully acknowledge this support.

### References

[References listed as provided, without modification]

---

This revised text aims to be more coherent, professional, and easier to read. It organizes the content into clear sections and provides a logical flow of information.