corresponding ACK does not arrive at the server within the (under-
estimated) RTO, the congestion window would drop to 1 segment,
triggering slow start, signiﬁcantly hurting TCP performance. We
refer to this as the undesired slow start problem.
 0 0.2 0.4 0.6 0.8 1 0 50 100 150 200CDFNumber of Packets (or Packet Ratio)Duplicated ACK packetsDUP ACK / OutOrder Data ratioOut-of-order data packets369Figure 15: Duplicate ACKs not triggering a slow start.
Figure 16: Duplicate ACKs triggering a slow start.
Figures 15 and 16 demonstrate two examples in the data set,
where Figure 15 shows that the train of duplicate ACKs does not
trigger slow start and Figure 16 includes a case where an undesired
slow start is triggered. Their key difference is that Figure 16 has
about 500KB bytes in ﬂight before the ﬁrst duplicate ACK, while
Figure 15 has much fewer bytes in ﬂight.
In TCP, RTO is computed by the sender using smoothed RTT
and RTT variation [20]. However, using duplicate ACKs to update
RTO, which may be beneﬁcial by allowing more accurate RTT es-
timation, is not standardized. In Figure 16, between 1.1s and 1.5s,
the sender receives many duplicate ACKs. Due to the growing
queueing size, RTT grows from 262ms (the last RTT sample before
the ﬁrst duplicate ACK) to 356ms, the RTT for the retransmitted
packet. The sender’s TCP implementation apparently ignores these
duplicate ACKs for updating RTO, which remains the same with-
out the duplicate ACKs being considered. Following the method
for calculating RTO [20], we observe that RTO is around 290ms
before the ﬁrst duplicate ACK, which is smaller than the RTT of
the retransmitted packet (356ms). This problem does not happen
in Figure 15, because the RTT before the ﬁrst duplicate ACK is
close to that after the last duplicate ACK, due to the small num-
ber of bytes in ﬂight. Although it is recommended that the RTO
should be at least 1 second [20], depending on the operating sys-
tems, different minimum values are used, e.g., Linux’s minimum
RTO is 200ms [28]. Such small values of RTO can exacerbate the
undesired slow start problem demonstrated in Figure 16.
θ[100,200]
θ[0,100]
We study the prevalence of the undesired slow start problem.
To tell whether there is a slow start following a long list of dupli-
cate ACKs, we use a heuristic metric Rss, the ratio of slow start:
, where θ[t1,t2] is the average downlink throughput
Rss =
from t1 ms to t2 ms after the last duplicate ACK. We empirically
choose 200ms as it is observed to be shorter than a typical slow
start in the LTE networks. During a slow start, Rss is expected to
be larger than that when there is no slow start. For example, the
Rss is 1.0 for Figure 15 and Rss is 3.7 for Figure 16.
In prac-
tice, we observe that 1.5 is a good threshold for Rss in determining
slow start. Using this threshold, we have determined that for all the
large TCP ﬂows with at least one lost data packet, 20.1% of them
Figure 17: Typical TCP data transfer.
suffer from the slow start problem, which consists of 12.3% of all
large TCP ﬂows. In one case, a 153-second ﬂow even experience
50 slow starts, resulting in an average throughput of only 2.8Mbps,
while the estimated bandwidth actually larger than 10Mbps.
There are different ways to mitigate this problem. One approach
is to update RTO with the help of duplicate ACKs with TCP Se-
lective Acknowledgment options (SACK) [18]. By taking the dif-
ference between the SACK window of two consecutive duplicate
ACKs, we can usually identify the exact data packets correspond-
ing to these ACKs. If there exists ambiguity (e.g., due to lost or
reordered ACK packets), we can simply ignore the corresponding
samples. In our data sets, packet reordering rate is less than 1%
and SACK is enabled in 82.3% of all duplicate ACKs, making this
approach promising. If SACK is disabled, we can use a fall-back
approach to estimate RTT based on duplicate ACKs by assuming
that they are in response to the data packets sent out in order. This
assumption holds in most cases as the packet reordering rate is low.
Using the above approaches, we can obtain RTT estimations for
duplicate ACKs and update RTO accordingly, which effectively
prevents the timeout of retransmitted packets due to increased queue-
ing delay. Our initial analysis shows that these two approaches can
prevent more than 95% of the undesired slow starts. Note that the
RTT estimation method used in TCP Vegas [5] with help of the
TCP timestamps option is not applicable to duplicate ACKs, since
the echo timestamps of all duplicate ACKs are all the same, with
their values being the timestamp of the segment before the lost seg-
ment, rather than the timestamp of the data segments triggering the
duplicate ACKs. From the mobile network operators’ perspective,
one solution of the undesired slow start problem might be to prior-
itize retransmitted packets. However, the security and performance
implications of this approach are yet to be studied.
6. BANDWIDTH ESTIMATION
In order to understand the network utilization efﬁciency of ex-
isting applications in the LTE networks, we ﬁrst need to know the
available bandwidth for each user. Previous work on active band-
width measurement methodology to estimate available bandwidth,
e.g., using packet pairs, packet trains, and parallel TCP connec-
tions [21, 15, 12], do not apply here. As existing studies have
shown that network condition is highly variable in cellular net-
works [14], active probing would require us to launch measure-
ments for each user at the time of trace collection. Using packet
traces collected at the monitor, we instead devise a passive band-
width estimation algorithm to capture the available bandwidth for
each user using TCP ﬂows that may not fully utilize the bandwidth.
6.1 Bandwidth Estimation Algorithm
Figure 17 illustrates a typical TCP data transfer. Our monitor
lies between the server and the UE, and we only use packet traces
 0 1e+06 2e+06 3e+06 4e+06 5e+06 2 2.5 3 3.5 4Relative Sequence NumberTime (second)DataACK 0 500000 1e+06 1.5e+06 2e+06 2.5e+06 3e+06 0 0.5 1 1.5 2 2.5 3Relative Sequence NumberTime (second)DataACKt1TCP DATATCP ACKt0t2t5t3t4t6t7TS1TS2UE (OS)UE (Radio)MonitorServerP1P2Pn-1Pn370collected at the monitor for analysis. The high-level idea for our
bandwidth estimation algorithm is to select a time window within
which the sending rate is fast enough to exceed the available band-
width, and then calculate the UE receiving rate that corresponds to
the actual available bandwidth during the short time window. Note
that the receiving rate is often smaller than the sending rate, causing
the in-network buffers to be ﬁlled up (§4.3).
We use Figure 17 to illustrate our bandwidth estimation algo-
rithm. At t2, UE sends an ACK in response to the two data packets
P1 and P2. And similarly, at t6, the ACK for Pn−1 and Pn is
sent. From the monitor’s traces, we observe that n− 2 data packets
(P3 ··· Pn) are sent to the UE in a time window between t0 and t4.
Assuming the average payload size of these n − 2 packets is S, the
sending rate between t0 and t4 is
At UE, the receiving rate for these n − 2 packets is
(1)
Rsnd =
Rrcv =
S(n − 2)
t4 − t0
S(n − 2)
t5 − t1
Typically, t2 is very close to t1 and similarly t5 ≈ t6. In our
controlled lab experiments, for a 30-minute continuous trace, the
median value of the delay between a data packet and the corre-
sponding ACK is negligible: 0.3ms. However, such a delay, e.g.,
t2−t1, could be large in some cases. Typically, one ACK in TCP is
for two data packets and when there is only one data packet pend-
ing acknowledgement, the receiver may delay sending the ACK by
up to 500 ms, which is known as the delayed acknowledgement
mechanism [27].
In our example, if Pn−1 has already been ac-
knowledged by another ACK and after t5 there is no more data
packet arriving at the UE side, the ACK for Pn could be delayed.
For simplicity, we ignore cases where the last ACK is acknowledg-
ing only one data packet, indicating it might be a delayed ACK. We
also do not consider cases with out-of-order data packets or dupli-
cate ACKs in the time window for bandwidth estimation, as there
may be ambiguity in packet timing. Then we have
Rrcv ≈ S(n − 2)
t6 − t2
If the uplink delay from the UE to the monitor is stable, we can
assume t6 − t2 = t7 − t3. However, this assumption may not hold
as RTT can be signiﬁcantly affected by the bytes in ﬂight. Instead,
we use the TCP Timestamps option [32] to calculate t6 − t2. If that
option is enabled, ACKs sent from a UE will contain the Timestamp
Value ﬁeld (TSval) i.e., the current value of the UE’s timestamp
clock. The unit for TSval depends on devices’ implementation. We
denote it by G, which can be treated as a constant for the same
device. Assuming G is known, we can estimate Rrcv as
Rrcv ≈
S(n − 2)
G(T S2 − T S1)
(2)
where T S1, T S2 are the TSval in the two corresponding ACKs.
Our bandwidth estimation algorithm only requires a UE having the
TCP Timestamps option enabled. In our data set, for 92.6% of the
TCP ﬂows, this requirement is satisﬁed.
We infer G using the method from previous work [11]. Using
the example in Figure 17, we have
G ≈ T S2 − T S1
t7 − t3
(3)
To minimize the error, we require t7 − t3 to be large enough
i.e., greater than a threshold δG. A larger δG value leads to more
Figure 18: G inference and the selection of δG.
accurate estimation of G but requires more time for inference. Fig-
ure 18 plots the relationship between δG and the estimation error
rate for G, based on controlled experiments for two devices, whose
actual G values (i.e., the ground truth) are measured at the UE side
using 30-minute long traces. We observe that the error rate of G
inference drops signiﬁcantly as δG increases so we conservatively
select δG = 3 seconds, which incurs less than 0.1% of error rate
in Figure 18. The error rate also depends on the value of G. In
our data set, among all large ﬂows, 5.9% of them do not have the
UE Timestamps option enabled, 57.3% have G ≈ 1ms/tick, 36.4%
have G ≈ 10ms/tick, and the rest 0.4% have other values, e.g.,
G ≈ 100ms/tick. With δG = 3 seconds, the error rates of inferred
G are less than 0.1% for the vast majority of large ﬂows.
Summary. for a target TCP ﬂow, if its G value is not known, the
algorithm uses the initial δG = 3 seconds of the ﬂow to infer G by
selecting two uplink packets that are at least δG seconds apart (For-
mula 3). Flows without UE TCP Timestamps are ignored. Then
the algorithm scans for each time window with high sending rate
Rsnd calculated by Formula 1. If (i) Rsnd ≥ C, a pre-known con-
stant of the maximum possible available bandwidth in the studied
network, and (ii) there is no out-of-order data packets or duplicate
ACKs within the time window, and (iii) the last ACK in the window
is not a delayed ACK, then the algorithm computes a bandwidth es-
timation sample according to Formula 2. The selection of C incurs
the following tradeoff: if C is too small, the bandwidth will be un-
derestimated when the sending rate within the estimation window
is not high enough to fully utilize the available bandwidth; if C is
too large, we may not be able obtain a sufﬁciently large number of
estimation samples. We conservatively choose C = 30Mbps, which
is veriﬁed to be higher than the rate of most ﬂows, and in the mean-
while allows us to predict the available bandwidth for over 90%
of the large downlink ﬂows. Our algorithm automatically searches
for different window sizes t5 − t1 for getting bandwidth estimation
samples, and we only consider the cases where there are packets
at both t1 and t5. Typically, valid bandwidth samples are obtained
when the window size is equal to a few times the RTT.
In addition to downlink bandwidth, our algorithm is also appli-
cable to uplink bandwidth estimation, by interchanging the UE and
the server in Figure 17. Similarly, our bandwidth estimation algo-
rithm also works in other network types, such as 3G, WiFi and even
wired networks, with proper parameter settings of C and δG.
Although the described algorithm is based on one single TCP
ﬂow per user, a similar idea can be applied to multiple concurrent
ﬂows per user by summing up the predicted bandwidth for differ-
ent ﬂows. As long as we ensure that the total sending rate for all
concurrent ﬂows are larger than C, the aggregated receiving rate
would be an accurate estimation of the available bandwidth. In this
study, we apply the algorithm on LTE downlink trafﬁc (UEs down-
loading contents from servers) for single TCP ﬂows, i.e., without
other competing downlink ﬂows for the same user.
 0 0.2 0.4 0.6 0.8 1 0 1 2 3 4 5% errorδG: time between ACKs for inference (s)Device 1 (G = 10.00 ms/tick)Device 2 (G = 3.91 ms/tick)371Figure 19: CDF of bandwidth estimation results for LTE net-
work (controlled lab experiments with Carrier A).
Figure 21: BW utilization ratio for large downlink TCP ﬂows.
Figure 22: BW estimation timeline for two large TCP ﬂows.
Figure 20: Time series of bandwidth estimation for LTE net-
work (controlled lab experiments with Carrier A).
6.2 Validation with Local Experiments
To validate the bandwidth estimation algorithm, we use con-
trolled experiments with their setup described in §3.2.
Recall that during the throughput test (§3.2), the server sends
data without any interruption so the throughput measured on the
UE is a reasonable (but not necessarily perfect) approximation of
the available bandwidth. Therefore in Figure 19, we compare the
distribution of estimated bandwidth calculated from the server-side
packet trace (§6.1) with the actual throughput measured from the
UE-side packet trace by sliding a window of a ﬁxed length (e.g.,
1.0s) over the trace. For each window position, we get one server-
side bandwidth estimation sample that is time-wise closest to the
center of that window, and we compare this sample with the actual
throughput to obtain an error sample. Note that the term “error”
here is relative to the actual throughput observed from UE-side
traces, which itself might not be the actual available bandwidth,
and the true error rate for our estimation algorithm could be even
smaller. The error distributions for two window lengths, i.e., 1.0s
and 0.1s, are shown in Figure 19. For the 1.0-second window, the
average error is 7.9% and for 0.1s window, the UE throughput has
higher variation and the average error is slightly higher. Figure 19
also directly compares the distributions of the absolute values of
the actual throughput (using a sliding window length of 1.0s) and
the estimated bandwidth, both of which are very close. Figure 20
visualizes an example test by showing the UE-perceived through-
put as well as the absolute error for the estimated bandwidth over
30 minutes (1.0s window). The actual throughput ﬂuctuates around
10Mbps and the error ﬂuctuates within ±1Mbps in most cases.
6.3 Bandwidth Utilization by TCP Flows
In this section, we analyze the LTE trafﬁc data set to understand
network utilization efﬁciency of TCP ﬂows. As shown in Figure 6
(§4.1), most users have only one TCP ﬂow actively downloading
data. We therefore only consider single TCP ﬂows with no com-
peting downlink ﬂows from the same user.
We apply the bandwidth estimation algorithm on the large TCP
downlink ﬂows (>5 seconds, >1 MB) that are not concurrent with
other ﬂows. We split each large ﬂow into consecutive windows of
250ms. For each window, we take one bandwidth estimation sam-
ple that is closest to the center of the window. For some ﬂows, there
exist windows that do not contain any valid bandwidth estimation
sample and we simply ignore such windows. This will not qualita-
tively affect the analysis results as such unknown duration accounts
for less than 20% of the total ﬂow duration. For each ﬂow, we use
the average value of all bandwidth estimation samples as the esti-
mated ﬂow bandwidth and compare it with the actual utilized ﬂow
bandwidth, computed by dividing total bytes by ﬂow duration.
Figure 21 plots the ratio of used bandwidth to estimated band-
width across large ﬂows. The median ratio is only 19.8%. For