title:Assessing software integrity of virtual appliances through software
whitelists
author:Jun Ho Huh and
Mirko Montanari and
Derek Dagit and
Rakesh Bobba and
Dongwook Kim and
Yoonjoo Choi and
Roy H. Campbell
Assessing software integrity of virtual appliances through software whitelists: Is
it any good?
Jun Ho Huh, Mirko Montanari, Derek Dagit, Rakesh B. Bobba, Dong Wook Kim, and Roy H Campbell
University of Illinois at Urbana-Champaign, Illinois, USA
{jhhuh, mmontan2, dagit, rbobba, kim628, rhc}@illinois.edu
Yoonjoo Choi
Dartmouth College, Hanover, New Hampshire, USA
PI:EMAIL
Abstract
Virtual appliances (VAs) are ready-to-use virtual ma-
chine images that are conﬁgured for speciﬁc purposes in
Infrastructure-as-a-Service (IaaS) clouds. This paper eval-
uates the integrity of software packages installed on real-
world VAs through the use of a software whitelist-based
framework. Analysis of 151 Amazon VAs using this frame-
work shows that there is signiﬁcant variance in the software
integrity across VAs and that about 9% of real-world VAs
have signiﬁcant numbers of software packages that contain
unknown ﬁles, making them potentially untrusted. Virus
scanners ﬂagged just half of the VAs in that 9% as mali-
cious, though, demonstrating that virus scanning alone is
not sufﬁcient to help users select a trustable VA.
1
Introduction
Many users enjoy the ﬂexibility that comes with
Infrastructure-as-a-Service (IaaS) cloud model, where users
can build and conﬁgure virtual computing infrastructure by
renting computing platform resources (e.g., Amazon EC2)
in the form of virtual machines. Virtual machine (VM) disk
images that are preconﬁgured with the necessary software
to support speciﬁc workﬂows of functions are known as
virtual appliances (VAs), and services providing reposito-
ries of VAs are referred to as appliance stores. The beneﬁts
of this publisher-consumer model are clear: consumers en-
joy the convenience of downloading a VA that suits their
needs and launching a service quickly; and publishers can
get paid for providing the VA. This marketplace, however,
lacks adequate mechanisms that allow users to a priori as-
sess whether a speciﬁc VA is correctly equipped with the
software packages that it claims to contain. An irresponsi-
ble publisher could publish an incomplete VA that is miss-
ing integral software packages that it claims to have. Some
software packages might be partially installed (i.e., missing
critical ﬁles) or have ﬁles that are modiﬁed (i.e., different
from the version provided by the software vendor). VAs
with such partially installed or modiﬁed packages will not
meet the expectations of VA consumers, who would expect
the VAs to be conﬁgured with all the packages claimed by
publishers, and that those packages are unmodiﬁed (unless
otherwise noted).
This paper presents an empirical study on the integrity
of software packages in real-world VAs, assessed using a
whitelist-based framework, showing that there is signiﬁ-
cant variance in the software integrity of software packages
across VAs. Our ﬁndings show that about 9% of the eval-
uated real-world VAs have signiﬁcant software integrity is-
sues and would probably not meet consumers’ expectations.
Previous research has identiﬁed some of the quality assur-
ance and security issues with the publisher and consumer
model as well (e.g.
[3, 2]) but the solutions focused on
blacklisting approaches such as scanning for viruses and ﬁl-
tering to removing unwanted ﬁles.
2 Analysis of real-world Virtual Appliances
Our empirical analysis sheds light on the integrity of
software packages in real-world VAs through a software
whitelisting-based technique. By looking at the variances in
the results, we gauge how useful a priori software integrity
veriﬁcation results would be for consumers in selecting well
installed VAs. We analyzed 151 randomly picked Amazon
VAs as described below.
Methodology We sampled the content of the Amazon
market by randomly selecting publicly available Amazon
Machine Images (AMIs). Our random sample is com-
posed of 151 images from an estimated pool of 2,300 valid
rpm-based AMIs available in the Amazon US-east zone.
To verify the representativeness of the samples, we ran-
domly shufﬂed the 151 VAs and divided them into three
subgroups; then demonstrated that those three groups share
similar characteristics with respect to the properties of inter-
est (number of ﬁles, number of unveriﬁed ﬁles, and number
of software packages).
Using a combination of shell and python scripts, we con-
structed a whitelist-based tool for rating integrity of soft-
ware packages in VAs. We take advantage of the fact that
VAs are often generated from trusted base images made
available from the repository provider (e.g., Amazon Linux
AMIs [1]) to speed up the analysis. Given a trusted base
image and a derived VA, the tool ﬁrst generates the check-
sums by hashing all of the ﬁles in both images. It compares
the hash values between the two images and creates a list
of added ﬁles, modiﬁed ﬁles, and deleted ﬁles. The tool
then checks the added, modiﬁed and deleted ﬁles against
the whitelist that we created and marks the ﬁles that are
unveriﬁed or missing. Then, by ﬁguring out which soft-
ware package each unveriﬁed/missing ﬁle belongs to, the
tool computes the “integrity score” of the installed software
using the rules described below:
• integrity score 3 —the software has no unveriﬁed or
missing ﬁles, with the exception of conﬁguration ﬁles;
• integrity score 2 —the software has no unveriﬁed or
missing critical ﬁles, but may have unveriﬁed or miss-
ing non critical ﬁles;
• integrity score 1 —the software has unveriﬁed or
missing critical ﬁles.
For this study, we treat all executables, source ﬁles, web
pages, and image ﬁles as “critical,” and compressed or other
data ﬁles as “non critical.” Installed software that has an in-
tegrity score of 3 is considered fully veriﬁed and integrity-
protected, falling under the “clean or high-integrity instal-
lation” category. Software that has a score of 2 has only
non critical missing or unveriﬁed ﬁles; hence, it is consid-
ered partially veriﬁed and falls under the “partially clean
or medium-integrity” category. Score of 1 represents the
“modiﬁed or low-integrity” category as such software may
have critical ﬁles missing or unveriﬁed.
As the ﬁnal step, the tool generates a signed “veriﬁcation
report” containing the scores.
intuition was that
VA Classiﬁcation I: Percentage of Unveriﬁed/Missing
Files Our
the number of unveri-
ﬁed/missing ﬁles would indicate, to some degree, the in-
tegrity level of software in a VA. As the ﬁrst step to study
their characteristics, we looked for and found a correla-
tion between the number of unveriﬁed/missing ﬁles and the
percentage (Pearson’s: 0.84). Using that correlation, we
classiﬁed the VAs into the following three “Integrity Level
Groups” (ILG) to help demonstrate common VA character-
istics:
Table 1. Average number of software packages to which
the three integrity scores have been assigned
Avg. # unveriﬁed/missing
non critical ﬁles
critical ﬁles
ILG A
ILG B
ILG C
23
122
917
6
43
954
Avg. # integrity scores
3
426
416
346
2
1
2
20
1
0
1
45
• “ILG A”—44 VAs are in this group, and they have less
than 0.1% of unveriﬁed/missing ﬁles;
• “ILG B”—59 VAs are part of this group, and they have
0.1-1% of unveriﬁed/missing ﬁles;
• “ILG C”—48 VAs in this group have > 1% of unveri-
ﬁed/missing ﬁles.
Table 1 contains the number of software packages with
each integrity score. We show “truncated average” values
to account for the high variance in the results. The average
number of unveriﬁed/missing ﬁles is signiﬁcantly higher in
ILG C (1871) than in ILGs A (29) and B (165). The table
indicates that the VAs in ILG C have a relatively larger num-
ber of software packages with low integrity scores ( 2 and
1 ), while most of the software packages installed on VAs
in ILG A have an integrity score of 3 . The total number
of packages averages around 420 in all three groups. About
99% of the software packages in the 44 VAs in ILG A are
clean, and the numbers are not too different for the 59 VAs
in ILG B. Both groups have a small number of low-integrity
and medium-integrity packages. There is a big jump, how-
ever, as we reach ILG C, whose VAs have, on average, 45
low-integrity packages and 20 medium-integrity packages.
Those are about 11% and 4.8% of the total number of pack-
ages, respectively. We note this as our ﬁrst key ﬁnding.
Finding 1: across the VAs, there is high variance in the
number of unveriﬁed/missing ﬁles and the number of low-
integrity and medium-integrity software packages.
Interestingly, in most VAs, both the unveriﬁed/missing
critical ﬁles and non critical ﬁles are concentrated in a small
number of packages, and there is no strong correlation be-
tween the number of unveriﬁed/missing ﬁles and the num-
ber of packages they inﬂuence. There are 12 or so VAs,
however, for which the unveriﬁed/missing ﬁles do spread
out across a large number of packages, and so we investi-
gate these further in Section 2.
VA Classiﬁcation II: Percentage of Low-Integrity Pack-
ages
In this second part of the analysis we further inves-
tigate the outliers identiﬁed previously, which have notice-
ably larger number of partially clean and modiﬁed pack-
ages. We do so by forming VA clusters based on the
percentages of packages given integrity scores 1 and 2 .
Table 2. Characteristics of the VA clusters
Cluster 1
Cluster 2
# VAs
137
14
Avg. % of integrity scores
3
99%
43%
2
0.5%
17%
1
0.3%
40%
Avg. # of integrity scores
3
418
205
1
1
171
2
2
77
The k-means clustering method—an unsupervised learning
algorithm—is used to identify the clusters (k = 2).
Characteristics of the VA Clusters: Table 2 summarizes
the characteristics of the two VA clusters, showing that the
14 VAs in Cluster 2 have signiﬁcantly high percentage of
packages with scores 1 and 2 ; the absolute numbers are
also very high, averaging 171 and 77 for scores 1 and 2 ,
respectively. Only 43% of the packages are cleanly installed
(high-integrity). The percentage of the unveriﬁed/missing
critical ﬁles is high too, averaging 73%. In contrast, for the
VAs in Cluster 1, 99% of the packages are cleanly installed,
and the percentage of unveriﬁed/missing critical ﬁles aver-
ages only 22%. Considering that our samples are a repre-
sentative set, here is our second key ﬁnding:
Finding 2: About 9% of the VAs have a signiﬁcant
portion of low-integrity and medium-integrity packages in-
stalled.
Virus scan results showed that 41 of the unveriﬁed ﬁles
across the VAs are potentially malicious, infecting 7 of them
14 VAs. The VAs are from different publishers, were built to
support different functions, and do not share common base
images. What is most worrying about those 14 VAs is that
none of them, in their name or image-description, mention
anything about software customization or modiﬁcation ef-
forts. Just looking at the VA descriptions, they all appear to
have only the cleanly installed, standard packages.
Whitelist Size To evaluate the scalability of the assess-
ment approach, we analyzed the size of the whitelist that we
created to verify our sample of 151 VAs and the correspond-
ing 47 base images, showing how the size changes as the
number of VAs grows. Figure 1 plots the percentage of the
VAs used against the percentage of the whitelist coverage
(i.e., how much of the total whitelist was created). To get
the values, we randomly shufﬂed all the VAs and incremen-
tally generated the whitelist, adding the per-VA whitelist en-
tries (the hashes used to verify a speciﬁc VA) to a global
whitelist while removing any duplicate entries; the size of
the global whitelist was recorded after each step. We re-
peated this process 10 times to get the average size for each
incremental step, and used these average values to calculate
the percentage values. Intuitively, we see that the whitelist
coverage percentage grows as the number of VAs used to
construct the whitelist goes up. It does so more rapidly at
ﬁrst, but then slows down progressively because some soft-
ware packages are installed on multiple VAs, and thus do
not add new entries. After 20% of the VAs are used, about
Figure 1. Percentage of the VAs used vs. per-
centage of the whitelist coverage
40% of the total global whitelist is already constructed.
3. Conclusions
Our study shows that a signiﬁcant portion (9%) of real-
world VAs contain software packages that are modiﬁed.
Such VAs are offered to consumers without any indication
on the publishers’ customization efforts and appear to have
standard installations. When a consumer pays for a VA to
use, she has the right to know that a VA is conﬁgured prop-
erly and meets her software integrity expectations. To that
end, Findings 1 and 2 demonstrate the need for a priori
software integrity assessment. Our software integrity as-
sessment reports would help a consumer avoid VAs that are
conﬁgured badly and choose ones with high-integrity pack-
ages. Further, our scalability analysis shows that the rate at
which additions are made to the whitelist will decrease over
time because of installation of a large number of packages
on multiple VAs, allowing the whitelists to gain authority.
Acknowledgement
This material is based on research sponsored in part by the
Air Force Research Laboratory and the Air Force Ofﬁce of Scien-
tiﬁc Research, under agreement number FA8750-11-2-0084. This
work has also been partially supported by the Boeing Company.
References
[1] Amazon Elastic Compute Cloud. http://aws.amazon.
com/ec2/.
[2] Bugiel, Sven and N¨urnberger, Stefan and P¨oppelmann,
Thomas and Sadeghi, Ahmad-Reza and Schneider, Thomas.
AmazonIA: when elasticity snaps back. In Proceedings of the
18th ACM Conference on Computer and Communications Se-
curity, pages 389–400, New York, NY, USA, 2011. ACM.
[3] J. Wei, X. Zhang, G. Ammons, V. Bala, and P. Ning. Manag-
ing security of virtual machine images in a cloud environment.
In Proceedings of the 2009 ACM Workshop on Cloud Comput-
ing Security, CCSW ’09, pages 91–96, New York, NY, USA,
2009. ACM.
0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% Whitelist coverage (%) Number of VAs (%)