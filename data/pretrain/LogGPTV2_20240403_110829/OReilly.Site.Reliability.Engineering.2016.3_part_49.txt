• Which clusters have Mixer deployed
Geolocation Server, Personalization Database
• How to roll back a Mixer release
SRE experts: Sally W, Dave K, Jen P
• Which backends of Mixer are considered “critical path”
Developer contacts: Jim T, results-team@
and why
Read and understand the following docs: Comprehension questions:
• Results Mixing Overview: “Query execution” section • Q: How does the release schedule change if a company holiday
• Results Mixing Overview: “Production” section occurs on the normal release build day?
• Playbook: How to Roll Out a New Results Mixing Server • Q: How can you fix a bad push of the geolocation dataset?
• A Performance Analysis of Mixer
Note that the preceding section does not directly encode procedures, diagnostic steps,
or playbooks; instead, it’s a relatively future-proof write-up focusing strictly on enu‐
merating expert contacts, highlighting the most useful documentation resources,
establishing basic knowledge you must gather and internalize, and asking probing
questions that can only be answered once that basic knowledge has been absorbed. It
also provides concrete outcomes, so that the student knows what kinds of knowledge
and skills they will have gained from completing this section of the learning checklist.
It’s a good idea for all interested parties to get a sense of how much information the
trainee is retaining. While this feedback mechanism perhaps doesn’t need to be as
formal as a quiz, it is a good practice to have complete bits of homework that pose
questions about how your service(s) work. Satisfactory answers, checked by a stu‐
dent’s mentor, are a sign that learning should continue to the next phase. Questions
about the inner workings of your service might look similar to the following:
• Which backends of this server are considered “in the critical path,” and why?
• What aspects of this server could be simplified or automated?
• Where do you think the first bottleneck is in this architecture? If that bottleneck
were to be saturated, what steps could you take to alleviate it?
Depending on how the access permissions are configured for your service, you can
also consider implementing a tiered access model. The first tier of access would allow
your student read-only access to the inner workings of the components, and a later
tier would permit them to mutate the production state. Completing sections of the
396 | Chapter 28: Accelerating SREs to On-Call and Beyond
on-call learning checklist satisfactorily would earn the student progressively deeper
access to the system. The Search SRE team calls these attained levels “powerups”4 on
the route to on-call, as trainees are eventually added into the highest level of systems
access.
Targeted Project Work, Not Menial Work
SREs are problem solvers, so give them a hearty problem to solve! When starting out,
having even a minor sense of ownership in the team’s service can do wonders for
learning. In the reverse, such ownership can also make great inroads for trust build‐
ing among senior colleagues, because they will approach their junior colleague to
learn about the new component(s) or processes. Early opportunities for ownership
are standard across Google in general: all engineers are given a starter project that’s
meant to provide a tour through the infrastructure sufficient to enable them to make
a small but useful contribution early. Having the new SRE split time between learning
and project work will also give them a sense of purpose and productivity, which
would not happen if they spent time only on learning or project work. Several starter
project patterns that seem to work well include:
• Making a trivial user-visible feature change in a serving stack, and subsequently
shepherding the feature release all the way through to production. Understand‐
ing both the development toolchain and the binary release process encourages
empathy for the developers.
• Adding monitoring to your service where there are currently blind spots. The
newbie will have to reason with the monitoring logic, while reconciling their
understanding of a system with how it actually (mis)behaves.
• Automating a pain point that isn’t quite painful enough to have been automated
already, providing the new SRE with an appreciation for the value SREs place on
removing toil from our day-to-day operations.
Creating Stellar Reverse Engineers and Improvisational
Thinkers
We can propose a set of guidelines for how to train new SREs, but what should we
train them on? Training material will depend on the technologies being used on the
job, but the more important question is: what kind of engineers are we trying to cre‐
ate? At the scale and complexity at which SREs operate, they cannot afford to merely
4 A nod to video games of yesteryear.
Creating Stellar Reverse Engineers and Improvisational Thinkers | 397
be operations-focused, traditional system administrators. In addition to having a
large-scale engineering mindset, SREs should exhibit the following characteristics:
• In the course of their jobs, they will come across systems they’ve never seen
before, so they need to have strong reverse engineering skills.
• At scale, there will be anomalies that are hard to detect, so they’ll need the ability
to think statistically, rather than procedurally, to uncloak problems.
• When standard operating procedures break down, they’ll need to be able to
improvise fully.
Let’s examine these attributes further, so that we can understand how to equip our
SREs for these skills and behaviors.
Reverse Engineers: Figuring Out How Things Work
Engineers are curious about how systems they’ve never seen before work—or, more
likely, how the current versions of systems they used to know quite well work. By
having a baseline understanding of how systems work at your company, along with a
willingness to dig deep into the debugging tools, RPC boundaries, and logs of your
binaries to unearth their flows, SREs will become more efficient at homing in on
unexpected problems in unexpected system architectures. Teach your SREs about the
diagnostic and debugging surfaces of your applications and have them practice draw‐
ing inferences from the information these surfaces reveal, so that such behavior
becomes reflexive when dealing with future outages.
Statistical and Comparative Thinkers: Stewards of the Scientific
Method Under Pressure
You can think of an SRE’s approach to incident response for large-scale systems as
navigating through a massive decision tree unfolding in front of them. In the limited
time window afforded by the demands of incident response, the SRE can take a few
actions out of hundreds with the goal of mitigating the outage, either in the short
term or the long term. Because time is often of the utmost importance, the SRE has to
effectively and efficiently prune this decision tree. The ability to do so is partially
gained through experience, which only comes with time and exposure to a breadth of
production systems. This experience must be paired with careful construction of
hypotheses that, when proven or disproven, even further narrow down that decision
space. Put another way, tracking down system breakages is often akin to playing a
game of “which of these things is not like the other?” where “things” might entail ker‐
nel version, CPU architecture, binary version(s) in your stack, regional traffic mix, or
a hundred other factors. Architecturally, it’s the team’s responsibility to ensure all of
these factors can be controlled for and individually analyzed and compared. However,
398 | Chapter 28: Accelerating SREs to On-Call and Beyond
we should also train our newest SREs to become good analysts and comparators from
their earliest moments on the job.
Improv Artists: When the Unexpected Happens
You try out a fix for the breakage, but it doesn’t work. The developer(s) behind the
failing system are nowhere to be found. What do you do now? You improvise! Learn‐
ing multiple tools that can solve parts of your problem allows you to practice defense
in depth in your own problem-solving behaviors. Being too procedural in the face of
an outage, thus forgetting your analytical skills, can be the difference between getting
stuck and finding the root cause. A case of bogged-down troubleshooting can be fur‐
ther compounded when an SRE brings too many untested assumptions about the
cause of an outage into their decision making. Demonstrating that there are many
analytical traps that SREs can fall into, which require “zooming out” and taking a dif‐
ferent approach to resolution, is a valuable lesson for SREs to learn early on.
Given these three aspirational attributes of high-performing SREs, what courses and
experiences can we provide new SREs in order to send them along a path in the right
direction? You need to come up with your own course content that embodies these
attributes, in addition to the other attributes specific to your SRE culture. Let’s con‐
sider one class that we believe hits all of the aforementioned points.
Tying This Together: Reverse Engineering a Production Service
“When it came time to learn [part of the Google Maps stack], [a new SRE] asked
if, rather than passively having someone explain the service, she could do this
herself—learning everything via Reverse Engineering class techniques, and hav‐
ing the rest of us correct her/fill in the blanks for whatever she missed or got
wrong. The result? Well, it was probably more correct and useful than it would
have been if I’d given the talk, and I’ve been on-call for this for over 5 years!”
—Paul Cowan, Google Site Reliability Engineer
One popular class we offer at Google is called “Reverse Engineering a Production Ser‐
vice (without help from its owners).” The problem scenario presented appears simple
at first. The entire Google News Team—SRE, Software Engineers, Product Manage‐
ment, and so forth—has gone on a company trip: a cruise of the Bermuda Triangle.
We haven’t heard from the team for 30 days, so our students are the newly appointed
Google News SRE Team. They need to figure out how the serving stack works from
end-to-end in order to commandeer it and keep it running.
After being given this scenario, the students are led through interactive, purpose-
driven exercises in which they trace the inbound path of their web browser’s query
through Google’s infrastructure. At each stage in the process, we emphasize that it is
important to learn multiple ways to discover the connectivity between production
servers, so that connections are not missed. In the middle of the class, we challenge
Creating Stellar Reverse Engineers and Improvisational Thinkers | 399
the students to find another endpoint for the incoming traffic, demonstrating that
our initial assumption was too narrowly scoped. We then challenge our students to
find other ways into the stack. We exploit the highly instrumented nature of our pro‐
duction binaries, which self-report their RPC connectivity, as well as our available
white-box and black-box monitoring, to determine which path(s) users’ queries take.5
Along the way, we build a system diagram and also discuss components that are
shared infrastructure that our students are likely to see again in the future.
At the end of the class, the students are charged with a task. Each student returns to
their home team and asks a senior SRE to help them select a stack or slice of a stack
for which they’ll be on-call. Using the skills learned in classes, the student then dia‐
grams that stack on their own and presents their findings to the senior SRE.
Undoubtedly the student will miss a few subtle details, which will make for a good
discussion. It’s also likely that the senior SRE will learn something from the exercise
as well, exposing drifts in their prior understanding of the ever-changing system.
Because of the rapid change of production systems, it is important that your team
welcome any chance to refamiliarize themselves with a system, including by learning
from the newest, rather than oldest, members of the team.
Five Practices for Aspiring On-Callers
Being on-call is not the single most important purpose of any SRE, but production
engineering responsibilities usually do involve some kind of urgent notification cov‐
erage. Someone who is capable of responsibly taking on-call is someone who under‐
stands the system that they work on to a reasonable depth and breadth. So we’ll use
“able to take on-call” as a useful proxy for “knows enough and can figure out the rest.”
A Hunger for Failure: Reading and Sharing Postmortems
“Those who cannot remember the past are condemned to repeat it.”
—George Santayana, philosopher and essayist
Postmortems (see Chapter 15) are an important part of continuous improvement.
They are a blame-free way of getting at the many root causes of a significant or visible
outage. When writing a postmortem, keep in mind that its most appreciative audi‐
ence might be an engineer who hasn’t yet been hired. Without radical editing, subtle
changes can be made to our best postmortems to make them “teachable” postmor‐
tems.
5 This “follow the RPC” approach also works well for batch/pipeline systems; start with the operation that kicks
off the system. For batch systems, this operation could be data arriving that needs to be processed, a transac‐
tion that needs to be validated, or many other events.
400 | Chapter 28: Accelerating SREs to On-Call and Beyond
Even the best postmortems aren’t helpful if they languish in the bottom of a virtual
filing cabinet. It then follows that your team should collect and curate valuable post‐
mortems to serve as educational resources for future newbies. Some postmortems are
rote, but “teachable postmortems” that provide insights into structural or novel fail‐
ures of large-scale systems are as good as gold for new students.
Ownership of postmortems isn’t limited just to authorship. It’s a point of pride for
many teams to have survived and documented their largest outages. Collect your best
postmortems and make them prominently available for your newbies—in addition to
interested parties from related and/or integrating teams—to read. Ask related teams
to publish their best postmortems where you can access them.
Some SRE teams at Google run “postmortem reading clubs” where fascinating and
insightful postmortems are circulated, pre-read, and then discussed. The original
author(s) of the postmortem can be the guest(s) of honor at the meeting. Other teams
organize “tales of fail” gatherings where the postmortem author(s) semiformally
present, recounting the outage and effectively driving the discussion themselves.
Regular readings or presentations on outages, including trigger conditions and miti‐
gation steps, do wonders for building a new SRE’s mental map and understanding of
production and on-call response. Postmortems are also excellent fuel for future
abstract disaster scenarios.
Disaster Role Playing
“Once a week we have a meeting where a victim is chosen to be on the spot in front of
the group, and a scenario—often a real one taken from the annals of Google history—
is thrown at him or her. The victim, whom I think of as a game show contestant, tells
the game show host what s/he would do or query to understand or solve the problem,
and the host tells the victim what happens with each action or observation. It’s like SRE
Zork. You are in a maze of twisty monitoring consoles, all alike. You must save inno‐
cent users from slipping into the Chasm of Excessive Query Latency, save datacenters
from Near-Certain Meltdown, and spare us all the embarrassment of Erroneous Goo‐
gle Doodle Display.”
—Robert Kennedy, former Site Reliability Engineer for Google Search and
healthcare.gov6
When you have a group of SREs of wildly different experience levels, what can you do
to bring them all together, and enable them to learn from each other? How do you
impress the SRE culture and problem-solving nature of your team upon a newbie,
while also keeping grizzled veterans apprised of new changes and features in your
stack? Google SRE teams address these challenges through a time-honored tradition
of regular disaster role playing. Among other names, this exercise is commonly
6 See “Life in the Trenches of healthcare.gov”.
Five Practices for Aspiring On-Callers | 401
referred to as “Wheel of Misfortune” or “Walk the Plank.” The sense of humorous
danger such titles lend the exercise makes it less intimidating to freshly hired SREs.
At its best, these exercises become a weekly ritual in which every member of the
group learns something. The formula is straightforward and bears some resemblance
to a tabletop RPG (Role Playing Game): the “game master” (GM) picks two team
members to be primary and secondary on-call; these two SREs join the GM at the
front of the room. An incoming page is announced, and the on-call team responds
with what they would do to mitigate and investigate the outage.
The GM has carefully prepared a scenario that is about to unfold. This scenario might
be based upon a previous outage for which the newer team members weren’t around
or that older team members have forgotten. Or perhaps the scenario is a foray into a
hypothetical breakage of a new or soon-to-be-launched feature in the stack, rendering
all members of the room equally unprepared to grapple with the situation. Better still,
a coworker might find a new and novel breakage in production, and today’s scenario
expands on this new threat.
Over the next 30–60 minutes, the primary and secondary on-callers attempt to root-
cause the issue. The GM happily provides additional context as the problem unfolds,
perhaps informing the on-callers (and their audience) of what the graphs on their
monitoring dashboard might look like during the outage. If the incident requires
escalation outside of the home team, the GM pretends to be a member of that other
team for the purposes of the scenario. No virtual scenario will be perfect, so at times
the GM may have to steer participants back on track by redirecting the on-callers
away from red herrings, introducing urgency and clarity by adding other stimuli,7 or
asking urgent and pointed questions.8
When your disaster RPG is successful, everyone will have learned something: perhaps
a new tool or trick, a different perspective on how to solve a problem, or (especially
gratifying to new team members) a validation that you could have solved this week’s
problem if you had been picked. With some luck, this exercise will inspire teammates
to eagerly look forward to next week’s adventure or to ask to become the game master
for an upcoming week.
Break Real Things, Fix Real Things
A newbie can learn much about SRE by reading documentation, postmortems, and
taking trainings. Disaster role playing can help get a newbie’s mind into the game.
However, the experience derived from hands-on experience breaking and/or fixing
7 For example: “You’re getting paged by another team that brings you more information. Here’s what they
say…”
8 For example: “We’re losing money quickly! How could you stop the bleeding in the short term?”
402 | Chapter 28: Accelerating SREs to On-Call and Beyond
real production systems is even better. There will be plenty of time for hands-on
experience once a newbie has gone on-call, but such learning should happen before a
new SRE reaches that point. Therefore, provide for such hands-on experiences much
earlier in order to develop the student’s reflexive responses for using your company’s
tooling and monitoring to approach a developing outage.
Realism is paramount in these interactions. Ideally, your team has a stack that is
multihomed and provisioned in such a way that you have at least one instance you
can divert from live traffic and temporarily loan to a learning exercise. Alternatively,
you might have a smaller, but still fully featured, staging or QA instance of your stack
that can be borrowed for a short time. If possible, subject the stack to synthetic load
that approximates real user/client traffic, in addition to resource consumption, if pos‐
sible.
The opportunities for learning from a real production system under synthetic load
are abundant. Senior SREs will have experienced all sorts of troubles: misconfigura‐
tions, memory leaks, performance regressions, crashing queries, storage bottlenecks,
and so forth. In this realistic but relatively risk-free environment, proctors can
manipulate the job set in ways that alter the behavior of the stack, forcing new SREs