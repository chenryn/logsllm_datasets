AninterestingfindingwiththisclusteringisthatOtterTunetends
efficientsaresimilartoandcanbeinterpretedinthesamewayas
to group together useless metrics (e.g., SSL connection data). It
thecoefficientsinalinearregression.Furthermore,eachfactorhas
doesnot,however,haveaprogrammaticwaytodeterminethatthey
unitvarianceandisuncorrelatedwithallotherfactors.Thismeans
aretrulyuselessandthusithastoincludetheminfurthercompu-
thatonecanorderthefactorsbyhowmuchofthevariabilityinthe
tations.Wecouldprovidethesystemwithahintofoneormoreof
originaldatatheyexplain.Wefoundthatonlytheinitialfactorsare
thesemetricsandthendiscardtheclusterthatitgetsmappedto.
significant for our DBMS metric data, which means that most of
From the original set of 131 metrics for MySQL and 57 met-
thevariabilityiscapturedbythefirstfewfactors.
ricsforPostgres, weareabletoreducethenumberofmetricsby
TheFAalgorithmtakesasinputamatrixX whoserowscorre-
93%and82%,respectively. NotethatOtterTunestillcollectsand
spondtometricsandwhosecolumnscorrespondtoknobconfigu-
storesdataforalloftheDBMS’smetricsinitsrepositoryevenif
rationsthatwehavetried. TheentryX isthevalueofmetrici
ij
theyaremarkedasredundant. Thesetofmetricsthatremainafter
onconfiguration j. FAgivesus asmallermatrix U: therowsof
pruningtheFAreductionisonlyconsideredfortheadditionalML
U correspondtometrics,whilethecolumnscorrespondtofactors,
componentsthatwediscussinthenextsections.
andtheentryU isthecoefficientofmetriciinfactorj. Wecan
ij
scatter-plotthemetricsusingelementsoftheithrowofU ascoor-
5. IDENTIFYINGIMPORTANTKNOBS
dinatesformetrici. Metricsiandj willbeclosetogetherifthey
have similar coefficients in U — that is, if they tend to correlate After pruning the redundant metrics, OtterTune next identifies
stronglyinX. Removingredundantmetricsnowmeansremoving which knobs have the strongest impact on the DBA’s target ob-
metricsthataretooclosetooneanotherinourscatter-plot. jectivefunction. DBMSscanhavehundredsofknobs,butonlya
Wethenclusterthemetricsviak-means,usingeachmetric’srow subsetactuallyaffecttheDBMS’sperformance.Thus,reducingthe
ofU asitscoordinates. Wekeepasinglemetricforeachcluster, numberofknobslimitsthetotalnumberofpossibleDBMSconfig-
namely,theoneclosesttotheclustercenter.Oneofthedrawbacks urationsthatmustbeconsidered. Wewanttodiscoverbothnega-
ofusingk-meansisthatitrequirestheoptimalnumberofclusters tiveandpositivecorrelations.Forexample,reducingtheamountof
(K)asitsinput. Weuseasimpleheuristic[40]tofullyautomate memoryallocatedfortheDBMS’sbufferpoolislikelytodegrade
thisselectionprocessandapproximateK. Althoughthisapproach the system’s overall latency, and we want to discover this strong
isnotguaranteedtofindtheoptimalsolution,itdoesnotrequirea (albeitnegative)influenceontheDBMS’sperformance.
humantomanuallyinterpretagraphicalrepresentationoftheprob- OtterTune uses a popular feature selection technique for linear
lem to determine the optimal number of clusters. We compared regression, called Lasso [54], to expose the knobs that have the
this heuristic with other techniques [55, 48] for choosing K and strongestcorrelationtothesystem’soverallperformance. Inorder
found that they select values that differ by one to two clusters at todetectnonlinearcorrelationsanddependenciesbetweenknobs,
mostfromourapproximations. Suchvariationsmadelittlediffer- wealsoincludepolynomialfeaturesinourregression.
ence in the quality of configurations that OtterTune generated in OtterTune’s tuning manager performs these computations con-
ourexperimentalevaluationinSect.7. tinuouslyinthebackgroundasnewdataarrivesfromdifferenttun-
ThevisualizationinFig.4showsatwo-dimensionalprojection ing sessions. In our experiments, each invocation of Lasso takes
ofthescatter-plotandthemetricclustersinMySQLandPostgres. ∼20minandconsumes∼10GBofmemoryforarepositorycom-
In the MySQL clusters in Fig. 4a, OtterTune identifies a total of prisedof100ktrialswithmillionsofdatapoints.Thedependencies
nine clusters. These clusters correspond to distinct aspects of a andcorrelationsthatwediscoverarethenusedinOtterTune’srec-
DBMS’s performance. For example, in the case of MySQL, the ommendationalgorithms,presentedinSect.6.
metrics that measure the amount of data written7, the amount of WenowdescribehowtouseLassotoidentifyimportantknobs
dataread8,andthetimespentwaitingforresources9areallgrouped andthedependenciesthatmayexistbetweenthem. Thenwedis-
cusshowOtterTuneusesthisduringthetuningprocess.
5 PostgresMetric:pg_stat_database.tup_updated
6 PostgresMetric:pg_statio_user_tables.idx_blks_hit 10 PostgresMetric:pg_stat_bgwriter.buffers_clean
7 MySQLMetrics:innodb_data_written,innodb_buffer_pool_write_requests 11 PostgresMetric:pg_stat_bgwriter.maxwritten_clean
8 MySQLMetrics:innodb_rows_read,bytes_sent 12 PostgresMetric:pg_statio_user_tables.idx_blks_read
9 MySQLMetrics:innodb_log_waits,innodb_row_lock_time_max 13 PostgresMetric:pg_statio_user_indexes.idx_blks_read
1013
5.1 FeatureSelectionwithLasso ofzeroorone.Specifically,eachcategoricalfeaturewithnpossible
Linear regression is a statistical method used to determine the valuesisconvertedintonbinaryfeatures. Althoughthisencoding
strength of the relationship between one or more dependent vari- methodincreasesthenumberoffeatures,alloftheDBMSsthatwe
ables (y) and each of the independent variables (X). These re- examinedhaveasmallenoughnumberofcategoricalfeaturesthat
lationships are modeled using a linear predictor function whose theperformancedegradationwasnotnoticeable. Next,OtterTune
weights(i.e.,coefficients)areestimatedfromthedata. scales the data. We found that standardizing the data (i.e., sub-
The most common method of fitting a linear regression model tractingthemeananddividingbythestandarddeviation)provides
is ordinary least squares (OLS), which estimates the regression adequateresultsandiseasytoexecute. Weevaluatedmorecom-
weights by minimizing the residual squared error. Such a model plicatedapproaches,suchascomputingdeciles,buttheyproduced
allowsonetoperformstatisticaltestsontheweightstoassessthe nearlyidenticalresultsasthestandardizedform.
significance of the effect of each independent variable [14]. Al- 5.2 Dependencies
thoughOtterTunecouldusethesemeasurementstodeterminethe
knobordering,OLSsuffersfromtwoshortcomingsthatmakeitan As we showed in Sect. 2, many of a DBMS’s knobs are non-
unsatisfactorysolutioninhigh(er)dimensionalsettings. First,the independent. Thismeansthatchangingonemayaffectanother. It
estimates have low bias but high variance, and the variance con- is important that OtterTune takes these relationships into consid-
tinuestoincreaseasmorefeaturesareincludedinthemodel. The erationwhenrecommendingaconfigurationtoavoidnonsensical
latterissuedegradesthepredictionandvariableselectionaccuracy settings.Forexample,ifthesystemdoesnot“know”thatitshould
of the model. Second, the estimates become harder to interpret nottrytoallocatetheentiresystemmemorytomultiplepurposes
asthenumberoffeaturesincreases, sinceextraneousfeaturesare controlledbydifferentknobs,thenitcouldchooseaconfiguration
neverremoved(i.e.,OLSdoesnotperformfeatureselection). thatwouldcausetheDBMStobecomeunresponsiveduetothrash-
Toavoidtheseproblems,OtterTuneemploysaregularizedver- ing. Inothercases,wehaveobservedthataDBMSwillrefuseto
sionofleastsquares,knownasLasso,thatreducestheeffectofir- startwhentherequestedconfigurationusestoomuchmemory.
relevantvariablesinlinearregressionmodelsbypenalizingmodels Within the feature selection method described above, we can
withlargeweights. ThemajoradvantageofLassooverotherreg- capture such dependencies between knobs by including polyno-
ularizationandfeatureselectionmethodsisthatitisinterpretable, mialfeaturesintheregression.Theregressionandfeatureselection
stable, and computationally efficient [54, 26]. There is also both methodsdonotchange:theyjustoperateonpolynomialfeaturesof
practicalandtheoreticalworkbackingitseffectivenessasaconsis- theknobsinsteadoftherawknobsthemselves.Forexample,totest
tentfeatureselectionalgorithm[56,57,64,9]. whetherthebufferpoolmemoryallocationknobinteractswiththe
Lasso works by adding an L penalty that is equal to a con- logbuffersizeknob, wecanincludeafeaturewhichistheprod-
1
stantλtimesthesumofabsoluteweightstothelossfunction. Be- uctoftheseknobs’values:ifLassoselectsthisproductfeature,we
causeeachnon-zeroweightcontributestothepenaltyterm,Lasso havediscoveredadependencebetweenknobs.
effectivelyshrinkssomeweightsandforcesotherstozero.Thatis, 5.3 IncrementalKnobSelection
Lassoperformsfeatureselectionby automatically selectingmore
relevantfeatures(i.e.,thosewithnon-zeroweights),anddiscarding OtterTune now has a ranked list of all knobs. The Lasso path
theothers(i.e.,thosewithzeroweights). Thenumberoffeatures algorithmguaranteesthattheknobsinthislistareorderedbythe
thatitkeepsdependsonthestrengthofitspenalty, whichiscon- strength of statistical evidence that they are relevant. Given this,
trolledbyadjustingthevalueofλ. Lassoimprovestheprediction OtterTunemustdecidehowmanyoftheseknobstouseinitsrec-
accuracyandinterpretabilityoftheOLSestimatesviaitsshrinkage ommendations.UsingtoomanyofthemincreasesOtterTune’sop-
andselectionproperties: shrinkingsmallweightstowardszerore- timizationtimesignificantlybecausethesizeoftheconfiguration
ducesthevarianceandcreatesamorestablemodel,anddeselecting spacegrowsexponentiallywiththenumberofknobs.Butusingtoo
extraneousfeaturesgeneratesmodelsthatareeasiertointerpret. fewofthemwouldpreventOtterTunefromfindingthebestconfig-
As in the usual regression scenario, OtterTune constructs a set uration. The right number of knobs to consider depends on both
ofindependentvariables(X)andoneormoredependentvariables theDBMSandthetargetworkload.
(y)fromthedatainitsrepository. Theindependentvariablesare Toautomatethisprocess,weuseanincrementalapproachwhere
theDBMS’sknobs(orfunctionsoftheseknobs)andthedependent OtterTune dynamically increases the number of knobs used in a
variables are the metrics that OtterTune collects during an obser- tuning session over time. Expanding the scope gradually in this
vationperiodfromtheDBMS.OtterTuneusestheLassopathal- mannerhasbeenshowntobeeffectiveinotheroptimizationalgo-
gorithm[29]todeterminetheorderofimportanceoftheDBMS’s rithms [27, 20]. As we show in our evaluation in Sect. 7.3, this
knobs. Thealgorithmstartswithahighpenaltysettingwhereall alwaysproducesbetterconfigurationsthananystaticknobcount.
weightsarezeroandthusnofeaturesareselectedintheregression
model. Itthendecreasesthepenaltyinsmallincrements, recom- 6. AUTOMATEDTUNING
putestheregression,andtrackswhatfeaturesareaddedbacktothe Now at this point OtterTune has (1) the set of non-redundant
modelateachstep. OtterTuneusestheorderinwhichtheknobs metrics,(2)thesetofmostimpactfulconfigurationknobs,and(3)
firstappearintheregressiontodeterminehowmuchofanimpact thedatafromprevioustuningsessionsstoredinitsrepository.
they have on the target metric (e.g., the first knob selected is the OtterTunerepeatedlyanalyzesthedataithascollectedsofarin
most important). We provide more details and visualizations of thesessionandthenrecommendsthenextconfigurationtotry. It
thisprocessinAppendixA. executes a two-step analysis after the completion of each obser-
BeforeOtterTunecomputesthismodel,itexecutestwoprepro- vation period in the tuning process. In the first step, the system
cessing steps to normalize the knobs data. This is necessary be- identifieswhichworkloadfromaprevioustuningsessionismost
causeLassoprovideshigherqualityresultswhenthefeaturesare emblematicofthetargetworkload. Itdoesthisbycomparingthe
(1)continuous, (2)haveapproximatelythesameorderofmagni- session’smetricswiththosefromthepreviouslyseenworkloadsto
tude, and (3) have similar variances. It first transforms all of the seewhichonesreactsimilarlytodifferentknobsettings. OnceOt-
categoricalfeaturesto“dummy”variablesthattakeonthevalues terTunehasmatchedthetargetworkloadtothemostsimilaronein
1014
itsrepository,itthenstartsthesecondstepoftheanalysiswhereit givethem,theyarecomputationallyexpensiveandthusnotfeasible
choosesaconfigurationthatisexplicitlyselectedtomaximizethe (yet)foranon-linetuningservice.
targetobjective.Wenowdescribethesestepsinfurtherdetail. OtterTune starts the recommendation step by reusing the data
fromtheworkloadthatitselectedpreviouslytotrainaGPmodel.
6.1 Step#1–WorkloadMapping Itupdatesthemodelbyaddinginthemetricsfromthetargetwork-
load that it has observed so far. But since the mapped workload
ThegoalofthisfirststepistomatchthetargetDBMS’swork- is not exactly identical to the unknown one, the system does not
loadwiththemostsimilarworkloadinitsrepositorybasedonthe fully trust the model’s predictions. We handle this by increasing
performancemeasurementsfortheselectedgroupofmetrics. We thevarianceofthenoiseparameterforallpointsintheGPmodel
findthatthematchedworkloadvariesforthefirstfewexperiments thatOtterTunehasnottriedyetforthistuningsession. Thatis,we
beforeconvergingtoasingleworkload.Thissuggeststhatthequal- add a ridge term to the covariance. We also add a smaller ridge
ityofthematchmadebyOtterTuneincreaseswiththeamountof term for each configuration that OtterTune selects. This is help-
data gathered from the target workload, which is what we would fulfor“noisy”virtualizedenvironmentswheretheexternalDBMS
expect.Forthisreason,usingadynamicmappingschemeisprefer- metrics (i.e., throughput and latency) vary from one observation
abletostaticmapping(i.e.,mappingonetimeaftertheendofthe periodtothenext.
firstobservationperiod)becauseitenablesOtterTunetomakemore Nowforeachobservationperiodinthisstep,OtterTunetriesto
educatedmatchesasthetuningsessionprogresses. find a better configuration than the best configuration that it has
ForeachDBMSversion,webuildasetSofN matrices—one seen thus far in this session. It does this by either (1) searching
foreverynon-redundantmetric—fromthedatainourrepository. anunknownregioninitsGP(i.e., workloadsforwhichithaslit-
SimilartotheLassoandFAmodels,thesematricesareconstructed tletonodatafor),or(2)selectingaconfigurationthatisnearthe
bybackgroundprocessesrunningonOtterTune’stuningmanager best configuration in its GP. The former strategy is referred to as
(see Sect. 3). The matrices in S (i.e., X 0,X 1,...X N−1) have exploration. This helps OtterTune look for configurations where
identical row and column labels. Each row in matrix X m cor- knobsaresettovaluesthatarebeyondtheminimumormaximum
respondstoaworkloadinourrepositoryandeachcolumncorre- values that it has tried in the past. This is useful for trying cer-
spondstoaDBMSconfigurationfromthesetofalluniqueDBMS tainknobswheretheupperlimitmightdependontheunderlying
configurations that have been used to run any of the workloads. hardware(e.g.,theamountofmemoryavailable).Thesecondstrat-
TheentryX m,i,jisthevalueofmetricmobservedwhenexecuting egyisknownasexploitation. ThisiswhereOtterTunehasfounda
workloadiwithconfigurationj. Ifwehavemultipleobservations goodconfigurationandittriesslightmodificationstotheknobsto
fromrunningworkloadiwithconfigurationj,thenentryX m,i,jis seewhetheritcanfurtherimprovetheperformance.
themedianofallobservedvaluesofmetricm. WhichofthesetwostrategiesOtterTunechooseswhenselecting
Theworkloadmappingcomputationsarestraightforward.Otter- thenextconfigurationdependsonthevarianceofthedatapointsin
TunecalculatestheEuclideandistancebetweenthevectorofmea- itsGPmodel. Italwayschoosestheconfigurationwiththegreat-
surementsforthetargetworkloadandthecorrespondingvectorfor est expected improvement. The intuition behind this approach is
each workload i in the matrix X m (i.e., X m,i,:). It then repeats thateachtimeOtterTunetriesaconfiguration,it“trusts”theresult
this computation for each metric m. In the final step, OtterTune from that configuration and similar configurations more, and the
computesa“score”foreachworkloadibytakingtheaverageof variance for those data points in its GP decreases. The expected
thesedistancesoverallmetricsm.Thealgorithmthenchoosesthe improvement is near-zero at sampled points and increases in be-
workloadwiththelowestscoreastheonethatismostsimilartothe tweenthem(althoughpossiblybyasmallamount). Thus, itwill
targetworkloadforthatobservationperiod. alwaystryaconfigurationthatitbelievesisoptimaloronethatit
Beforecomputingthescore, itiscriticalthatallmetricsareof knows little about. Over time, the expected improvement in the
thesameorderofmagnitude.Otherwise,theresultingscorewould GPmodel’spredictionsdropsasthenumberofunknownregions
be unfair since any metrics much larger in scale would dominate decreases. This means that it will explore the area around good
the average distance calculation. OtterTune ensures that all met- configurationsinitssolutionspacetooptimizethemevenfurther.
ricsarethesameorderofmagnitudebycomputingthedecilesfor OtterTuneusesgradientdescent[29]tofindthelocaloptimum
eachmetricandthenbinningthevaluesbasedonwhichdecilethey onthesurfacepredictedbytheGPmodelusingasetofconfigu-
fall into. We then replace every entry in the matrix with its cor- rations, called the initialization set, as starting points. There are
respondingbinnumber. Withthisextrastep, wecancalculatean twotypesofconfigurationsintheinitializationset:thefirstarethe
accurate and consistent score for each of the workloads in Otter- top-performingconfigurationsthathavebeencompletedinthecur-
Tune’srepository. renttuningsession,andthesecondareconfigurationsforwhichthe
valueofeachknobischosenatrandomfromwithintherangeof
6.2 Step#2–ConfigurationRecommendation validvaluesforthatknob.Specifically,theratiooftop-performing