are copied into the NF’s receive ring buffer and the Wakeup sub-
system brings the NF process into the runnable state. After being
processed by an NF, the NF Manager’s Tx Threads move packets
through the remainder of the chain. This provides zero-copy packet
movement.
Service chains can be configured during system startup using
simple configuration files or from an external orchestrator such as
an SDN controller. When an NF finishes with a packet, it enqueues
it in its Tx queue, where it is read by the manager and redirected
to the Rx queue of the next NF in the chain. The NF Manager also
picks up packets from the Tx queue of the last NF in the chain, and
sends it out over the network.
NFVnice: Dynamic Backpressure and Scheduling for NF Chains
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
Figure 2: NFVnice Building Blocks
We have designed NFVnice to provide high performance process-
ing of NF service chains. The NF Manager’s scheduling subsystem
determines when an NF should be active and how much CPU time
it should be allocated relative to other NFs. The backpressure sub-
system provides chain-aware management, preventing NFs from
spending time processing packets that are likely to be dropped
downstream. Finally, the I/O interface facilitates efficient asynchro-
nous storage access for NFs.
System Management and NF deployment: The NF Manager ’s
(Rx, Tx and Monitor) threads are pinned to separate dedicated
cores. The number of Rx, Tx and monitor threads are configurable
(C-Macros), to meet system needs, and available CPU resources.
Similarly, the maximum number of NFs and maximum chain length
can be configured. NFVnice allows NFs and NF service chains to be
deployed as independent processes or Docker containers which are
linked with libnf library. libnf exports a simple, minimal interface (9
functions, 2 callbacks and 4 structures), and both the NF Manager
and libnf leverage the DPDK libraries (ring buffers, timers, memory
management). We believe developing or porting NFs or existing
docker containers can be reasonably straightforward. For example,
a simple bridge NF or a basic monitor NF is less than 100 lines of C
code.
3.2 Scheduling NFs
Each network function in NFVnice is implemented inside its own
process (potentially running in a container). Thus the OS scheduler
is responsible for picking which NF to run at any point in time. We
believe that rather than design an entirely new scheduler for NFV,
it is important to leverage Linux’s existing scheduling framework,
and use our management framework in user space to tune any
of the stock OS schedulers to provide the properties desired for
NFV support. In particular, we exploit the CFS Batch scheduler,
but NFVnice provides substantially similar benefits to each of the
other Linux kernel schedulers. Figure 3 shows the NFVnice sched-
uling that makes the OS scheduler be governed by NF Manager via
cgroups, and ultimately assigns running NFs to shared CPU cores.
The detailed description of the figure is in the Sections 3.2 and 3.3.
Activating NFs: NFs that busy wait for packets perform very
poorly in a shared CPU environment. Thus it is critical to design the
NF framework so that NFs are only activated when there are packets
available for them to process, as is done in NFV platforms such as
netmap [43] and ClickOS [32]. However, these systems provide only
a relatively simple policy for activating an NF: once one or more
Figure 3: NF Scheduling and Backpressure
packets are available, a signal is sent to the NF so that it will be
scheduled to run by the OS scheduler in netmap, or the hypervisor
scheduler in ClickOS. While this provides an efficient mechanism
for waking NFs, neither system allows for more complex resource
management policies, which can lead to unfair CPU allocations
across NFs, or inefficient scheduling across chains.
In NFVnice, NFs sleep by blocking on a semaphore shared with
the NF Manager, granting the management plane great flexibility in
deciding which NFs to activate at a given time. The policy we pro-
vide for activating an NF considers the number of packets pending
in its queue, its priority relative to other NFs, and knowledge of the
queue lengths of downstream NFs in the same chain. This allows
the management framework to indirectly affect the CPU scheduling
of NFs to be fairness and service-chain aware, without requiring
that information be synchronized with the kernel’s scheduler.
Relinquishing the CPU: NFs process batches of packets, decid-
ing whether to keep processing or relinquish the CPU between each
batch. This decision and all interactions with the management layer,
e.g., to receive a batch of packets, are mediated by libnf, which in
turn exposes a simple interface to developers to write their network
function. After a batch of at most 32 packets is processed, libnf will
check a shared memory flag set by the NF Manager that indicates if
it should relinquish the CPU early (e.g., as a result of backpressure,
as described below). If the flag is not set, the NF will attempt to pro-
cess another batch; if the flag has been set or there are no packets
available, the NF will block on the semaphore until notified by the
Manager. This provides a flexible way for the manager to indicate
that an NF should give up the CPU without requiring the kernel’s
CPU scheduler to be NF-aware.
CPU Scheduler: Since multiple NF processes are likely to be
in the runnable state at the same time, it is the operating system’s
CPU scheduler that must determine which to run and for how
long. In the early stages of our work we sought to design a custom
CPU scheduler that would incorporate NF information such as
queue lengths into its scheduling decisions. However, we found
that synchronizing queue length information with the kernel, at
the frequency necessary for NF scheduling, incurred overheads that
outweighed any benefits.
Linux’s CFS Batch scheduler is typically used for long running
computationally intensive tasks because it incurs fewer context
switches than standard CFS. Since NFVnice carefully controls when
individual NF processes are runnable and when they yield the CPU
(as described above), the batch scheduler’s longer time quantum
and less frequent preemption are desirable. In most cases, NFVnice
CPU SchedulersUserSpaceKernel SpaceHWMonitor ThreadNFLIBNFLIBNF ContainerlibnfWakeup Thread                    NF ManagerNICsRXTXShared Memory PoolControl PathData Path(Chained) NFsFlow TableFlow Rule InstallerWork Conserving CPU SchedulersKernel SpaceCGroup WeightsUserSpaceBack PressureNF ManagerlibnfQueue LengthWaitingRunningMonitor ThreadReceiverExplicit  Congestion NotiﬁcationIn-Network NFV PlatformHWShared CPU CoresComputation CostLoadNFRunning NFsSenderSIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
S. Kulkarni et al.
Figure 4: Backpressure State Diagram
Figure 5: Overloaded NFs (in bold) cause back pressure at the
entry points for service chains A, C, and D.
NFs relinquish the CPU due to policies controlled by the manager,
rather than through an involuntary context switch. This reduces
overhead and helps NFVnice prioritize the most important NF for
processing without requiring information sharing between user
and kernel space.
Assigning CPU Weights: NFVnice provides mechanisms to
monitor a network function to estimate its CPU requirements, and
to adjust its scheduling weight. Policies in the NF Manager can then
dynamically tune the scheduling weights assigned to each process
in order to meet operator specified priority requirements.
The packet arrival rate for a given NF can be easily estimated
by either the NF or the NF Manager. We measure the service time
to process a packet inside each NF using libnf. To avoid outliers
from skewing these measurements (e.g., if a context switch occurs
in the middle of processing a packet), we maintain a histogram of
timings, allowing NFVnice to efficiently estimate the service time
at different percentiles.
For each NF i on a shared core, we calculate load(i) = λi ∗ si, the
product of arrival rate, λ, and service time, s. We then find the total
i =1 load(i),
and assign cpu shares for N Fi on corem following the formula:
load on each core, such as core m, TotalLoad(m) =n
Sharesi = Priorityi ∗
load(i)
TotalLoad(m)
This provides an allocation of CPU weights that provides rate
proportional fairness to each NF. The Priorityi parameter can be
tuned if desired to provide differential service to NFs. Tuning pri-
ority in this way provides a more intuitive level of control than
directly working with the CPU priorities exposed by the scheduler
since it is normalized by the NF’s load.
3.3 Backpressure
A key goal of NFVnice is to avoid wasting work, i.e., preventing an
upstream NF from processing packets if they are just going to be
dropped at a downstream NF later in the chain that has become
overloaded. We achieve this through backpressure, which ensures
bottlenecks are quickly detected while minimizing the effects of
head of line blocking.
Cross-Chain Pressure: The NF Manager is in an ideal position
to observe behavior across NFs since it assists in moving packets
between them. When one of the NF Manager’s TX threads detects
that the receive queue for an NF is above a high watermark (HIGH_
WATER_MARK) and queuing time is above threshold, then it exam-
ines all packets in the NF’s queue to determine what service chain
they are a part of. NFVnice then enables service chain-specific packet
dropping at the upstream NFs. NF Manager maintains states of each
NF, and in this case, it moves the NF’s state from backpressure watch
list to packet throttle as shown in Figure 4. When the queue length
becomes less than a low watermark (LOW_WATER_MARK), the
state moves to clear throttle, then again moves to the watch list if
the queue length goes beyond the high mark.
The backpressure operation is illustrated in Figure 5, where four
service chains (A-D) pass through several different NFs. The bold
NFs (3 and 5) are currently overloaded. The NF Manager detects this
and applies back pressure to flows A, C, and D. This is performed
upstream where those flows first enter the system, minimizing
wasted work. Note that backpressure is selective based on service
chain, so packets for service chain B are not affected at all. Service
chains can be defined at fine granularity (e.g., at the flow-level) in
order to minimize head of line blocking.
This form of system-wide backpressure offers a simple mecha-
nism that can provide substantial performance benefits. The back-
pressure subsystem employs hysteresis control to prevent NFs
rapidly switching between modes. Backpressure is enabled when
the queue length exceeds a high watermark and is only disabled
once it falls below the low watermark.
Local Optimization and ECN: NFVnice also supports simple,
local backpressure, i.e., an NF will block if its output TX queue
becomes full. This can happen either because downstream NFs are
slow, or because the NF Manager TX Thread responsible for the
queue is overloaded. Local backpressure is entirely NF-driven, and
requires no coordination with the manager, so we use it to handle
short bursts and cases where the manager is overloaded.
We also consider the fact that an NFVnice middlebox server
might only be one in a chain spread across several hosts. To facili-
tate congestion control across machines, the NF Manager will also
mark the ECN bits in TCP flows in order to facilitate end-to-end
management. Since ECN works at longer timescales, we monitor
queue lengths with an exponentially weighted moving average and
use that to trigger marking of flows following [42].
3.4 Facilitating I/O
A network function could block when its receive ring buffer is
empty or when it is waiting to complete I/O requests to the un-
derlying storage. In both cases, NF implementations running on
the NFVnice platform are expected to yield the CPU, returning
any unused CPU cycles back to the scheduling pool. In case of
Watch listPacket ThrottleClear ThrottleQlen  Threshold Qlen > HIGH_WATER_MARKandQueuing Time > Threshold Qlen >= HIGH_WATER_MARKQlen < LOW_WATER_MARKNF1NF2NF3NF4NF5ACBAADCBDCBNFVnice: Dynamic Backpressure and Scheduling for NF Chains
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
// Read the next packet from the receive ring buffer
packet_descriptor* libnf_read_pkt();
// Output the processed packet to specified destination
int libnf_write_pkt(packet_descriptor*);
// Enqueue request to read from storage. Flow specific data
can be stored in context
int libnf_read_data(int fd, void *buf,
size_t size, size_t offset,
void (*callback_fn)(void *), void *context);
// Enqueue request to write to storage. Flow specific data
can be stored in context
int libnf_write_data(int fd, void *buf,
size_t size, size_t offset,
void (*callback_fn)(void *), void *context);
Figure 6: libnf API exposed to network function implemen-
tations.
I/O, NF implementations should use asynchronous I/O to overlap
packet processing with background I/O to maintain throughput.
NFVnice provides a simple library called libnf that abstracts such
complexities from the NF implementation.
The libnf library exposes a simple set of APIs that allow the
application code to read/write packets from the network, and read-
/write data from storage. The APIs are shown in Listing 6. If the
receive ring buffer is empty while calling the libnf_read_pkt
API, libnf notifies the NF manager and blocks the NF until further
packets are available in the buffer.
In case of I/O, an NF implementation uses the libnf_read_data
and libnf_write_data APIs. I/O requests can be queued along
with a callback function that runs in a separate thread context. Using
batched asynchronous I/O with double buffering, libnf enables the
NF implementation to put the processing of one or more packets
on hold, while continuing processing of other packets unhindered.
Batching reads and writes allows an NF to continue execution
without waiting for I/O completion. The size of the batches and the
flush interval is tunable by the NF implementation. Double buffering
enables libnf to service one set of I/O requests asynchronously while
the other buffer is filled up by the NF. When both buffers are full,
libnf suspends the execution of the NF and yields the CPU.
3.5 Optimizations
Separating overload detection and control. Since the NFV plat-
form [23] must process millions of packets per second to meet line
rates, we separate out overload detection from the control mecha-
nisms required to respond to it. The NF Manager’s Tx threads are
well situated to detect when an NF is becoming backlogged as it is
their responsibility to enqueue new packets to each NF’s Tx queue.
Using a single DPDK’s enqueue interface, the Tx thread enqueues a
packet to a NF’s Rx queue if the queue is below the high watermark,
while getting feedback about the queue’s state in the return value.
When overload is detected, an overload flag is set in the meta data
structure related to the NF.
The control decision to apply backpressure is delegated to th
NF Manager’s Wakeup thread. The Wakeup thread scans through
the list of NFs classifying them into two categories: ones where
backpressure should be applied and ones that need to be woken
up. This separation simplifies the critical path in the Tx threads
and also provides some hysteresis control, since a short burst of
packets causing an NF to exceeds its threshold may have already
been processed by the time the Wakeup thread considers it for
backpressure.
Separating load estimation and CPU allocation. The load
on an NF is a product of its packet arrival rate and the per-packet
processing time. The scheduler weight is calculated based on the
load and the cgroup’s weights for the NF are updated. Since chang-
ing a weight requires writing to the Linux sysfs, it is critical that
this be done outside of the packet processing data path. libnf merely
collects samples of packet processing times, while the NF Manager
computes the load and assigns the CPU shares using cgroup virtual
file system.
The data plane (libnf) samples the packet processing time in
a lightweight fashion every millisecond by observing the CPU
cycle counter before and after the NF’s packet handler function is
called. We chose sampling because measuring overhead for each
packet using the CPU cycle counters results in a CPU pipeline
flush [3], resulting in additional overhead. The samples are stored in
a histogram, in memory shared between libnf and the NF Manager.
The processing time samples produced by each NF are stored in
shared memory and aggregated by the NF Manager. Not all packets
incur the same processing time, as some might be higher due to
I/O activity. Hence, NFVnice uses the median over a 100ms moving
window as the estimated packet processing time of the NF. Every
millisecond, the NF Manager calculates the load on each NF using
its packet arrival rate and the estimated processing time. Every
10ms, it updates the weights used by the kernel scheduler.
4 EVALUATION
4.1 Testbed and Approach
Our experimental testbed has a small number of Intel(R) Xeon(R)
CPU E5-2697 v3 @ 2.60GHz servers, 157GB memory, running
Ubuntu SMP Linux kernel 3.19.0-39-lowlatency. Each CPU has dual-
sockets with a total of 56 cores. For these experiments, nodes were