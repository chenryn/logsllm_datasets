sequentialway,whenonefinishesthenthenextisstarted.This this reduces query execution time and enhances the speed of
experiment was performed for 750, 1000, and 1000 iterations aggregating these logs that are existing on various physical
(create and delete server, create and delete image, create nodes.Weprovideboth,theaggregatedlogsaswellastheraw
and delete network), where faults were injected every 250 logs to cover possible development of methods that process
iterations respectively. The fault was injected in only one raw logs, such as log parsing.
iteration, however, we noticed that some of the faults take 3) Traces: OpenStack consists of multiple projects, where
time and propagate the errors to other iterations as well. In each project is composed of multiple services. To process
the second experiment, the rally workloads were concurrently user requests, e.g., creating a virtual machine, OpenStack
executed.Thisexperimentwasperformedfor2000,3000,and uses multiple services from different projects. To support
6000 iterations for create and delete server, create and delete troubleshooting, OpenStack introduces a small but powerful
Workloads and faults UI / Dashboards
Monitorinig and Logging
Control node
Kibana
wally 113
API Services
TraA cP I oS e er cv ti ac ote os balancer ElasF tl iu ce -sn et ad
ccR ia ol nl oy eA cP eI lS ee r iv i nc e ns 11C 7o m 2p 2u ,t e 2n 3o ,d 1e 2s
E inwx jee tu olt i n fo ff 1w a l ly x
and o r k a d o s a ults g l n r i and oad , 1 4 rch
L Services
Redis
Metrics
DB MQ
(glances)
Logs
Metrics
Faults
Traces
Fig.1. Illustrationoftheinfrastructurefromwherethedatawasgenerated.
library called osprofiler that is used by all OpenStack 0.1secondsforthecomputenodesandforthecontrollernode.
projects and their Python clients [37] to generate traces. It The5filesarenamedmetrics wally N,whereN iseitherthe
generates one trace per request, that goes through all involved controller node or one of the compute nodes. Each of these
services, and builds a tree of calls which captures a workflow files has 7 features:
of service invocations. To identify workflows, we monitor the • now. The timestamp of the recording.
following call types: • cpu.user. Percent time spent in userspace. The user
• HTTP. Captures HTTP requests, the latency of service, CPUtimeisthetimespentontheprocessorrunningyour
and projects involved. programs code (or code in libraries).
• RPC. Represent the duration of parts of request related • mem.used. The RAM usage of the physical host.
to different services in one project. • load.cpucore. The number of cores of the physical
• DBAPI.ThetimethattherequestspentintheDBlayer. host.
• Driver. In the case of nova, cinder and others we have • load.min1, min5, min15.Linuxloadaveragesare
vendor drivers. systemloadaveragesthatshowtherunningtasksdemand
The osprofiler library collects these records in a trace on the system as an average number of running plus
per request and stores them in a database (e.g., Redis). From waiting threads. This measures demand, which can be
Redis, we can query and analyze traces. greater than what the system is currently processing.
A small sample of the metrics data for the wally113 is
IV. DATASETDESCRIPTION
shown in Table I where we can see part of the metrics data.
The workloads and faults described in the previous section
wereexecutedonthetestbed.Asexplained,theexecutiongen-
B. Logs
erated three main categories of observability data: distributed
traces,metrics,andapplicationlogs.Thesedatawererecorded The logfiles aredistributed overthe infrastructure andthey
inconcurrentlyinordertoprovidethestateofthesystemfrom are grouped in directories by the OpenStack projects (e.g.,
multiple points of view. In the following two sections, we nova, neutron, glance, etc.) at the wally nodes. At each of the
describe the main attributes, properties, and statistics of each physicalnodes,therearedifferentprojectrunning.Thecontrol
data category of the first experiment. Due to page limitations, node has more services running and thus has more log files
wereferthereadertotheabovelinkintheabstractforthecode fortheOpenStackprojects.Eachprojectonthephysicalhosts
for extracting the data statistics from the second experiment. has its log directory where the logs are stored. Inside each of
All other properties hold for both experiments. the log directories for the projects, there are several log files.
Important to note here is that even the log files are highly
A. Metrics
distributedoverprojectsandphysicalnodes,theyallrepresent
The metrics data category contains data for the 5 physical the state of the system. We provide the raw log directories in
nodes in the infrastructure. The frequency of the recordings is thisdatasetalongwiththeaggregatedlogfile.Usingtheelastic
TABLEI
METRICSFROMTHECONTROLLERNODE(WALLY113)
timestamp cpu.user mem.used (B) load.cpucore load.min1 load.min5 load.min15
2019-11-19 16:56:32 11.5 10221035520 8 0.8 1.02 1.18
2019-11-19 16:56:32 10.4 10221117440 8 0.8 1.02 1.18
2019-11-19 16:56:33 11.1 10222948352 8 0.8 1.02 1.18
2019-11-19 16:56:33 14.3 10223144960 8 0.8 1.02 1.18
2019-11-19 16:56:34 10.7 10222866432 8 0.8 1.02 1.18
2019-11-19 16:56:34 10.7 10223480832 8 0.8 1.02 1.18
search and Kibana stack we can aggregate all the logs into a Every trace has its features in the JSON entries or events.
central database which can serve as a starting point for the These features depend on multiple factors such as the user
analysis. request, infrastructure, load balancers, and caching. An event
The log entries have in total of 23 features. Not all the is a vector of key-value pairs (k i,v i) describing the state,
featuresarealwayspresentforallthelogentries.Thefeatures: performance, and further characteristics of service at a given
id, index, scoreareaddedmetadatafromKibana.The type time t i. In following we describe the main features of the
is fluent, the collector which is responsible for sending all the events in a trace:
metrics and logs to Kibana. In the following, we describe the • host. Name of the physical host.
main features present in the log data. • name. Event name (e.g., compute apistop).
• hostname. Name of the physical host (e.g., wally113) • service. Service name (e.g., osapi compute).
• user_id, project_domain, tenant_id, • project, Openstack project (e.g., nova).
request_id, user_domain, domain_id. Are • timestamp. The time when the event is recorded.
features describing the user request to Openstack. • trace_id. ID of the span (contains two events, e.g.,
• timestamp, @timestamp. The time when the compute api-stop and compute api-start).
record was created. • parent_id. The parent id gives the ID of the parent
• log_level. Describes the level of the log entry. It can event. This attribute can be used to represent the trace in
be info, error, warning, etc. a graph.
• pid. Process ID. • base_id. ID of the trace, different events and spans
• Payload. Gives the most important information of the with same base id belong to one trace.
log i.e., the body of the log entry. Two start and stop events (e.g., compute apistart and com-
• programname. The OpenStack project that generated pute apistop)withthesametrace id.Thesubtractionbetween
the log entry. the stop timestamp and the start timestamp gives the duration
• python_module The module responsible for genera- of the span. The above features together with the duration
tion of the log entry, and the are the most important in describing the structure, preserving
• logger Tells which project logs the event. the parent-child causal relationship, and the duration which
• http_* related fields. Are only present if there represents the response time of the service invoked.
is an HTTP call describing the endpoint, status core, The events also contain other attributes that can be found
version, and the method. for specific types. For example, path, scheme, method for
HTTP calls, where the path and scheme represents the HTTP
Fortheparsingofthelogs,templatematching,andanalysis
endpoint and HTTP scheme and method can be GET or
we suggest using the aggregated file described instead of the
POST.Further,thedbstatement inDBcallsgivesinformation
directories with raw log files, as all of the information is
about the SQL query, while the function, name, args, kwargs
preserved and more structured for direct analysis. For multi-
in RPC calls tell which function was invoked with the its
sourceloganomalydetection,iftheaggregatedfileisutilized,
corresponding arguments.
we suggest splitting by ”logger” in order to obtain entries
which are grouped by their corresponding service. D. Ground Truth Labels
The workloads described along with the faults injected are
C. Traces
both recorded in Rally HTML and JSON reports which are
The traces in the dataset are contained in 3 directories: located at each of the directories containing trace data. These
boot delete, create delete image, and network create delete. reports provide pseudo ground truth labels for the traces,
Each of the directories contains the scripts for running the metrics, and logs. They contain information for the times
workloadandthefaultinjectionsalongwiththeactualtracing when the faults were injected and the resulting high level
data. These directories contain JSON files of the traces. This error messages. Taking the period when the anomaly was
structure is preserved among all types of workloads (Rally injected and merging it with the timestamps of the data files
actions). can give us true labels for the evaluation. We suggest to use
the ground truth labels to evaluate algorithms and methods
1000
which are based on unsupervised learning, as in production
systems injection of anomalies and access to labeled data is 300 700
restricted. 800
600
250
V. DATASETSTATISTICS
500
This section provides a descriptive statistic of the datasets 600 200
generated. It quantitatively describes the properties of the 400
trace, metrics, and log datasets. In following we discuss the 150
statistics for the first experiment only, due to page limitations. 400 300
Thecodeforextractingthestatisticsforthesecondexperiment 100
200
is provided in the data repository.
200
50
A. Metrics 100
Thenumberofrecordingsoftheutilizationoftheresources,
0 10 20 30 0 3.3 3.4 3.5 0 1.5 2.0
more specifically the CPU, memory and the load, per node cpu.user mem.used 1e9 load.min1
varies in the range of (108900, 298251). The average number
Fig.3. Distributionofthevaluesofthemetricsfeatureforthenodewally117.
of recordings is 239127. The total number of the metric
recordings is 1195637. All of the nodes have 8 CPU cores. It
is important to note that the metrics data cover a time span appearing in the sequential execution of the operations. We
larger than the period of execution of the experiments. usedKibanatoidentifythedifferentfeaturesdescribingthem.
In general, the wally113 experience the greatest CPU Each log has its unique identifier referenced by the label
and memory load as observed by the distribution of these _id.TheTimestampfeaturehas8missingvalues.However,
two features. Furthermore, the correlation analysis of the the timestamps provided by Kibana, stored in @timestamp
load.min1, load.min5 and load.min15 show that they exhibit contain the relevant information for the moment where the
high correlation given their relatedness through time. The logging happened.
correlation analysis also shows quite distinct behaviour for There are a total of 6 services recording their logs in the
the load.min5, load.min10, load.min15 correlations between OpenStacklogger:nova,neutron,keystone,glances,placement
the control node and the remaining nodes. Regarding the and cinder. Nova and neutron are services with the greatest
dependence between the cpu.user, memory.used and load.min number of logs appearing. The logs contain 3 levels of
features, no significant correlation can be identified. Roughly logging (INFO, WARNING and ERROR). There are 5 opera-
3 groups of features emerge - the load.CPU, mem.used and tion host nodes - Hostname (wally113, wally117, wally122,
the load.min group. wally123, wally124). Most of the logs originate from the
control node wally113. The python_module contains the
name of the 61 modules that are logging their information
800