title:WYSISNWIV: What You Scan Is Not What I Visit
author:Qilang Yang and
Dimitrios Damopoulos and
Georgios Portokalidis
WYSISNWIV: What You Scan Is Not
What I Visit
Qilang Yang(B), Dimitrios Damopoulos, and Georgios Portokalidis
Stevens Institute of Technology, Hoboken, NJ, USA
{qyang5,ddamopou,gportoka}@stevens.edu
Abstract. A variety of attacks,
including remote-code execution
exploits, malware, and phishing, are delivered to users over the web.
Users are lured to malicious websites in various ways, including through
spam delivered over email and instant messages, and by links injected in
search engines and popular benign websites. In response to such attacks,
many initiatives, such as Google’s Safe Browsing, are trying to make the
web a safer place by scanning URLs to automatically detect and black-
list malicious pages. Such blacklists are then used to block dangerous
content, take down domains hosting malware, and warn users that have
clicked on suspicious links. However, they are only useful, when scan-
ners and browsers address the web the same way. This paper presents
a study that exposes diﬀerences on how browsers and scanners parse
URLs. These diﬀerences leave users vulnerable to malicious web con-
tent, because the same URL leads the browser to one page, while the
scanner follows the URL to scan another page. We experimentally test
all major browsers and URL scanners, as well as various applications
that parse URLs, and discover multiple discrepancies. In particular, we
discover that pairing Firefox with the blacklist produced by Google’s
Safe Browsing, leaves Firefox users exposed to malicious content hosted
under URLs including the backslash character. The problem is a general
one and aﬀects various applications and URL scanners. Even though, the
solution is technically straightforward, it requires that multiple parties
follow the same standard when parsing URLs. Currently, the standard
followed by an application, seems to be unconsciously dictated by the
URL parser implementation it is using, while most browsers have strayed
from the URL RFC.
1 Introduction
The popularity of the web has made it the prime vehicle for delivering malicious
content to users, including browser exploits, malware, phishing, and web attacks,
like cross-site scripting (XSS) [32] and cross-site request forgery (CSRF) [17]
attacks. Such attacks are prevalent; Microsoft alone reported that more than 3.5
million computers visited a website containing a web-based exploit in the ﬁrst
quarter of 2012 [35]. The prominence of such attacks has lead to the development
of many approaches [20,22,27,30,37] that automatically detect pages containing
malicious content, leading to free and commercial tools [3–6,8–13,26,36] that
c(cid:2) Springer International Publishing Switzerland 2015
H. Bos et al. (Eds.): RAID 2015, LNCS 9404, pp. 317–338, 2015.
DOI: 10.1007/978-3-319-26362-5 15
318
Q. Yang et al.
can scan URLs and routinely crawl the web to identify and ﬁlter, quarantine,
warn, or take down malicious sites.
Users can reach malicious content by clicking on URLs, which have been
injected by attackers into legitimate sites or the results of search engines and
spread through spam sent over email and messages. Services which scan pages
for malicious content, i.e., URL scanners, follow the same URLs to fetch content
from servers and classify it as malicious or benign. Thus, it is essential that
when a scanner follows a URL, it visits the same page that the user would visit
through his browser or client application.
This paper presents an experimental study on how browsers and URL scan-
ners parse URLs. Our experiments reveal discrepancies on how URLs are parsed,
with browsers and URL scanners frequently following diﬀerent standards and
introducing their own rules. As a result, including a character like the backslash
in a URL can lead a browser to one web page, while the scanner visits another.
Essentially, attackers can hide their malicious content from the scanner, while
users can still access it. This constitutes a new evasion strategy for attackers
that want to avoid detection from URL scanners. While it may not always be
available to them, as certain scanner-browser pairs will treat URLs the same
way, this evasion strategy is powerful because it is not based on obfuscating
content, but simply requires the inclusion of a character in their URLs.
Looking at Google Safe Browsing, in particular, we show that it transforms
backslashes contained in URLs to forward slashes before it accesses a URL, a
behavior which has been also noted by web developers in the past [2,28]. On
the other hand, Firefox, which uses its malicious-URL database to warn users
that are about to accessing malicious sites, does not. Instead, it encodes the
backslash character using percent-encoding (aka URL-encoding). As such, an
attacker targeting Firefox browsers could essentially hide his exploit from initia-
tives like Google’s Safe Browsing. We have disclosed the issue to both Google
and Firefox, who are working on a solution.
The problem is a general one, as every URL scanner tested has exhibited
a behavior that creates opportunities for attackers. Technically, the solution to
the problem is not a hard one, however, it requires coordination and agreement
among the involved parties (i.e., browser and URL scanner developers). Unfor-
tunately, it is also exacerbated by the fact that various applications parse text
and automatically create links, when they identify URL patterns. We conducted
tests with various applications and libraries, and we discovered that there also
discrepancies on what they consider as acceptable URL patterns, leading to
another instance of the same problem.
In summary, the contributions of this paper are the following:
– We identify a new evasion strategy made possible because browsers and URL
scanners do not parse URLs consistently
– We develop an experimental methodology to reveal discrepancies on how
browsers and scanners transform URLs
– We test all major browsers and URL scanners and show that the problem is
general
WYSISNWIV: What You Scan Is Not What I Visit
319
– We test a variety of popular applications that dynamically create links for
URL-like text and also discover discrepancies
– We examine popular libraries used for parsing URLs and discover that they
follow diﬀerent RFCs.
2 Background
2.1 URL Encoding and Canonicalization
A uniform resource locator (URL) is a generic way to access a resource over the
Internet and is most commonly used to access a service or page over the web.
A URL is a uniform resource identiﬁer (URI) and it is an Internet standard
with the latest RFC describing it being RFC-3986 [1]. Its syntax is familiar and
follows the format shown below.
scheme:// [user:password@]domain:port
/path?query#fragment
(cid:2)
(cid:3)(cid:4)
authority
(cid:5)
URLs aim to be generic so that they can be used for a variety of protocols
and by a variety of applications. However, as the web has increased in popularity,
URLs are used by an increasing number of applications and have been extended
with new features (e.g., internationalization), causing some contemporary imple-
mentations to stray from the RFC. The web Hypertext Application Technology
Working Group (WHATWG), in an attempt to provide a more current standard,
has deﬁned the URL Living Standard [45]. Below, we discuss some basic aspects
of URLs and URL parsing.
Delimiter A generic URL consists of a hierarchical sequence of components
referred to as the scheme, authority, path, query, and fragment. Each compo-
nent corresponds to a piece of information that is necessary to locate a unique
resource. Hence, identifying components correctly when parsing a URL is critical
for both browsers and servers. Several delimiters are applied in the URL syntax
to help separate components. These are the colon (:), the at sign (@), the slash
(/), the question mark (?), and the number sign (#).
URL-Encoding or Percent-encoding is a mechanism for encoding information
in a URI to represent a data octet in a component, when the corresponding char-
acter of the octet is outside the allowed character set or is being used for a special
purpose such as the delimiter of, or within, the component. A percent-encoded
character is a character triplet, which consists of the percent character (%) and
two hexadecimal digits representing the octet’s numeric value. For example, %3F
is the percent-encoding of the question mark character (?). In percent-encoding
format, the uppercase hexadecimal digits and the corresponding lowercase digits
are equivalent and exchangeable.
Canonicalization or normalization refers to the process of converting data
from one representation to a “standard” or canonical form. Generally, this is
320
Q. Yang et al.
done to correctly compare data for equivalence, enumerate distinct data values,
improve various algorithms, etc. On URLs, it is mainly done to determine, if
two URLs are equivalent and it can include operations such as removing the
default HTTP port (i.e., 80), converting the domain to lowercase, and resolving
a path that contains a dot or double dot. Occasionally, applications introduce
their own canonicalization rules, such as removing duplicate slashes (// → /),
automatically completing incomplete IP address, deleting extra leading dots in
the authority part, etc.
2.2 URL Scanners
Because of the importance of web browsers and the multitude of attacks targeting
them or being delivered through them, several approaches [3–13,20,26,36] have
developed URL scanners. A URL scanner is a service that analyzes a URL,
enabling the identiﬁcation of viruses, worms, trojans, phishing and other kinds
of malicious content detected by antivirus engines or website scanners. URL
scanner services are commonly accessed through an online web service, a browser
extension, a third party library, or a public web-based API.
Scanners have two main interfaces. The ﬁrst, allows users to submit or report
a URL for immediate or later scanning [3,4,8–11,13,20]. The scanner will then
retrieve the content and scan it to determine maliciousness. Some scanners [4,
10] also consult multiple third-party scanners to determine if the content is
malicious. The second interface, enables users to query whether a URL has been
found to be malicious using the scanner’s malicious-URLs database (blacklist).
The database is queried looking for an exact or partial match. Regarding the
ownership of the blacklist, some scanners maintain their own blacklist [5,6,26,
36], while others use third-party blacklists [12]. Finally, scanners can be divided
into two categories: the ones that only check the content of submitted URLs and
the ones that follow links within the submitted page [3,8,11,13,26].
Among many URL scanners, there are two that are widely used in daily
life, though sometimes users may not be aware of them. The ﬁrst one is Google
Safe Browsing [26], a Google service that helps applications check URLs against
Google’s constantly updated lists of suspected phishing, malware, and unwanted
software pages. It is available through a series of web-based APIs. Google Safe
Browsing as a scanner service is integrated into Chrome and Firefox, and even
Safari uses its database. The second one is Microsoft’s SmartScreen ﬁlter [36],
a malware and phishing ﬁlter that is integrated in several Microsoft products
including Internet Explorer and Hotmail. Any time a user gets a warning when
visiting a web page in these browsers, it means that the URL is in one of the
two scanners’ blacklist or both.
3 The Problem
Figure 1 depicts the process, where URLs, which have been submitted by users or
obtained by crawling the web, are scanned for malicious content. Links contained
WYSISNWIV: What You Scan Is Not What I Visit
321
Fig. 1. Modern browsers utilize databases of known malicious URLs, populated by
oﬄine URL scanners, to warn and protect users.
within pages are usually also followed and scanned, and when a page is found
to contain malicious content, it is inserted into a database. That database can
be later used by the browser to prevent users from accessing malicious content.
For example, before fetching any page, the browser ﬁrst checks the URL of that
page in the database. If an entry does not exist, it proceeds to load and display
the page to the user. If, however, an entry for the URL is found in the database,
the browser redirects the user to a page warning him that he is about to visit a
page containing malicious content. Even though the user can ignore the warning,
research has shown that such warnings are eﬀective in protecting users [14,23].
This process through which the user is protected from visiting malicious
pages can be undermined when scanners and browsers do not parse URLs con-
sistently. There may be many reasons that two programs do not parse a URL
the same way. It may be consciously, because their developers chose to support
a diﬀerent standard, or because one of them has adopted additional standards
and guidelines. It can also be because of program bugs that cause inconsistent
behavior when parsing certain, otherwise legitimate, URLs. Independently of
the reason, if the database utilized by a browser was produced by a scanner
that treats certain URLs diﬀerently than the browser, the user is left exposed
to malicious content, which would be otherwise detected and ﬁltered.
The mechanics of such an attack are shown in Fig. 2. An attacker aware
of discrepancies in URL parsing can place his malicious content under a URL
that brings them about, e.g., BADU RL. When the scanner processes a page
containing this URL, the sought after behavior is triggered causing the scanner
(cid:3) before accessing it and scanning it. The
to transform the URL to BADU RL
attacker is essentially able to hide his malicious page from the scanner, so it can
never be entered in the database, later used by the browser. It is interesting to
note that even if the attacker for some reason placed malicious content under
(cid:3), causing it to be logged in the database, the browser would actually
BADU RL
check BADU RL instead, which would not match any of the logged entries. The
problem is symmetric, in the sense that if the browser is the entity transforming
322
Q. Yang et al.
Fig. 2. A scanner parsing URLs diﬀerently from a browser allows hiding malicious
content from the ﬁrst, using a carefully crafted URL, while the latter follow it to
malicious content.
the URL before accessing it, then the attacker can still hide malicious content by
(cid:3) while putting innocuous content under BADU RL.
placing it under BADU RL
This problem, which we name What You Scan Is Not What I Visit (WYSIS-
NWIV), is not limited to browsers, but it can aﬀect any application that creates
links for displayed URLs. For example, instant messengers and various web appli-
cations, like web mail, do create links for URLs identiﬁed in text. Concurrently,
there are various products that ﬁlter malicious URLs based on databases created
by public and proprietary scanners [5,24,38,42]. Each application-scanner pair,
where the two do not process URLs the same way, can leave the user exposed
to malicious content.
4 Experimental Methodology
To detect discrepancies on how URLs are parsed, we design experiments that will
drive browsers and URL scanners with various test inputs. This section describes
how we generate the test inputs and the experiments run with browsers and URL
scanners.
4.1 Generating Test Inputs
To generate the URL inputs used for testing, we follow a structured approach
building on domain knowledge. In particular, we manually examine the follow-
ing sources to identify high-level patterns of inputs, based on which we gener-
ate inputs for testing. The three following resources are used to identify high-
level patterns for testing: (i) the RFC 3986 document, (ii) the code base of
WYSISNWIV: What You Scan Is Not What I Visit
323
the Chromium and Mozilla Firefox web browsers, and (iii) the unit tests that
come with these browsers. The RFC 3986 speciﬁcation broadly deﬁnes what is
allowed, what may be allowed, but also what must be disallowed when a URL
is constructed. However, it allows browsers to implement their own policies for
“maybe allowed” characters. Thus, based on the study of the URL speciﬁcation
and two open-source web browsers, it would be possible to have discrepancies in
some special cases, such as when encountering control characters, special Uni-
code characters, the backslash character, and encoded delimiters included in the
URL path. Furthermore, unit tests provide key examples that web browsers and
services should be able to successfully parse for compatibility reasons. Table 1
lists all the test inputs constructed based on the above sources.
Table 1. Inputs used for testing browsers and scanners.
Description of transformation Generated Tests
Convert the scheme and host to