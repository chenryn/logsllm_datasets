title:When to use and when not to use BBR: An empirical analysis and evaluation
study
author:Yi Cao and
Arpit Jain and
Kriti Sharma and
Aruna Balasubramanian and
Anshul Gandhi
When to use and when not to use BBR: An empirical analysis
and evaluation study
Yi Cao
Stony Brook University
PI:EMAIL
Arpit Jain
Stony Brook University
PI:EMAIL
Kriti Sharma
Stony Brook University
PI:EMAIL
Aruna Balasubramanian
Stony Brook University
PI:EMAIL
ABSTRACT
This short paper presents a detailed empirical study of BBR’s per-
formance under different real-world and emulated testbeds across a
range of network operating conditions. Our empirical results help
to identify network conditions under which BBR outperforms, in
terms of goodput, contemporary TCP congestion control algorithms.
We find that BBR is well suited for networks with shallow buffers,
despite its high retransmissions, whereas existing loss-based algo-
rithms are better suited for deep buffers.
To identify the root causes of BBR’s limitations, we carefully
analyze our empirical results. Our analysis reveals that, contrary
to BBR’s design goal, BBR often exhibits large queue sizes. Further,
the regimes where BBR performs well are often the same regimes
where BBR is unfair to competing flows. Finally, we demonstrate the
existence of a loss rate “cliff point” beyond which BBR’s goodput
drops abruptly. Our empirical investigation identifies the likely
culprits in each of these cases as specific design options in BBR’s
source code.
CCS CONCEPTS
• Networks → Transport protocols; Network performance
analysis; Network measurement.
ACM Reference Format:
Yi Cao, Arpit Jain, Kriti Sharma, Aruna Balasubramanian, and Anshul
Gandhi. 2019. When to use and when not to use BBR: An empirical analysis
and evaluation study. In Internet Measurement Conference (IMC ’19), October
21–23, 2019, Amsterdam, Netherlands. ACM, New York, NY, USA, 7 pages.
https://doi.org/10.1145/3355369.3355579
1 INTRODUCTION
TCP congestion control algorithms have continued to evolve for
more than 30 years [36]. As the internet becomes more and more
complex, researchers have designed different TCP congestion con-
trol algorithms to serve different scenarios. For example, the legacy
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
IMC ’19, October 21–23, 2019, Amsterdam, Netherlands
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6948-0/19/10...$15.00
https://doi.org/10.1145/3355369.3355579
130
Anshul Gandhi
Stony Brook University
PI:EMAIL
TCP Reno [25] uses an AIMD (additive-increase/multiplicative-
decrease) algorithm to quickly respond to losses while slowly recov-
ering from congestion. TCP Cubic [28] responds more aggressively
to recover from losses — rather than a linear increase, TCP Cubic
grows its congestion window in a cubic manner. Instead of using
packet loss as a congestion signal, TCP Vegas [19] treats increasing
round trip time (RTT) as evidence of congestion. To achieve high
throughput and low latency in data centers, Alizadeh et al. imple-
mented DCTCP [16], which uses explicit congestion notification
(ECN) [35] as the congestion signal to prevent the packet loss from
occurring before the buffer becomes too congested.
A fundamental issue with regards to designing a congestion
control algorithm is: what is the optimal operating point for doing
congestion control? Should we keep sending packets until the buffer
becomes full and use packet loss as the congestion signal (e.g. Reno,
Cubic), or should we treat packet delay as the congestion evidence
(e.g. Vegas, Copa [17, 19]), or should we implement sophisticated
algorithms via learning-based techniques (e.g. PCC, Indigo [24, 38])?
In 1979, Kleinrock showed that the optimal operating point for
the network was when the bandwidth was maximized while min-
imizing the delay [33]. However, it was not until 2016 that this
design point was explicitly used for congestion control. Google’s
BBR (Bandwidth Bottleneck and Round-trip propagation time) algo-
rithm aims to operate at this optimal point by probing the current
bandwidth and delay sequentially in the network [20], as we discuss
in Section 2. BBR has since been employed at Google, and continues
to be actively developed. To avoid bufferbloat [27], BBR regulates its
congestion window size such that the amount of in-flight packets
is a multiple of the bandwidth-delay product (BDP); ideally, this
should result in small buffer sizes.
Despite the rising popularity of BBR, it is not fully clear when
BBR should be employed in practice, that is, when does BBR outper-
form other congestion control algorithms. Prior work has typically
focused on BBR’s fairness properties [34, 37] and its throughput
and queueing delay [30]; we discuss prior work on BBR in detail in
Section 5.
The goal of this short paper is to conduct a comprehensive empir-
ical study to investigate BBR’s performance under different network
conditions and determine when to employ BBR. In doing so, we also
aim to identify the root causes of BBR’s sub-optimal performance.
To this end, we conduct extensive experiments in both Mininet [6]
and real-world networks to analyze the performance of BBR. To
span the range of network operating conditions, we vary the band-
width, RTT, and bottleneck buffer sizes by employing a router
IMC ’19, October 21–23, 2019, Amsterdam, Netherlands
Yi Cao, Arpit Jain, Kriti Sharma, Aruna Balasubramanian, and Anshul Gandhi
Based on the current state, BBR calculates the pacinд_дain (a
dynamic gain factor used to scale BtlBw) and cwnd_дain (a dy-
namic gain factor used to scale BDP), and uses these values to
derive pacinд_rate (which controls the inter-packet spacing) and
congestion window size, cwnd, respectively. BBR then regulates
the pacinд_rate between 1.25× BtlBw and 0.75× BtlBw to explore
the achievable bandwidth and to drain the subsequently inflated
queues. Finally, BBR sends cwnd packets at the inter-packet speed
of pacinд_rate.
The algorithm continues iteratively with the next round of net-
work measurements. BBR transitions between different states of
the state machine based on the observed BtlBw, RTprop, amount
of packets in flight, etc. BBR periodically enters the ProbeRTT state
to reduce its cwnd and drain the queue to reset itself.
BBR vs other congestion control algorithms: BBR differs from
other major congestion control algorithms in the following aspects:
1) BBR does not explicitly respond to losses. Reno and Cubic regard
packet loss as a congestion event, and subsequently reduce their
cwnd value by a certain factor. Similarly, the delay-based algorithm
TCP Vegas decreases its cwnd when observing increasing RTT. BBR,
however, does not use explicit congestion signals to reduce cwnd.
Rather, BBR decides the amount of packets to be sent based on
past bandwidth and RTT measurements. Thus, in contrast to other
event-driven algorithms, BBR is feedback driven.
2) BBR uses pacinд_rate as the primary controller. Most conges-
tion control algorithms, like Reno and Cubic, use cwnd to determine
the number of packets in flight. However, cwnd does not directly
control the sending rate, resulting in traffic bursts or an idle net-
work [21]. To solve this issue, BBR uses pacing rate to control the
inter-packet spacing. Implementing the pacing rate in TCP conges-
tion control algorithms is known to have benefits for throughput
and fairness [15].
3) BBR actively avoids network congestion, whereas loss-based
algorithms passively decrease their sending rate in response to con-
gestion. BBR is designed to have low latency and high throughput
by maintaining (typically) 2× BDP packets in flight. One BDP is
budgeted for the network capacity, and the other is to deal with
delayed/aggregated ACKs [20]. BBR thus avoids congestion by lim-
iting the number of packets in flight. By contrast, Reno and Cubic
keep increasing the packets in flight until the bottleneck buffer
is full and a packet loss is detected. This is problematic when the
bottleneck buffer is deep, in which case Reno and Cubic queue up
too many packets in the buffer, causing bufferbloat [27].
While in principle BBR should outperform other TCP variants
due to the above design decisions, a detailed empirical study is
necessary to evaluate the performance of BBR.
3 EXPERIMENTAL SETUP
3.1 Testbeds
We use Mininet [6], LAN, and WAN networks for our experiments.
Mininet is a network emulator which creates a virtual network
running multiple hosts, links and switches on a single machine. For
Mininet or LAN experiments, we use a simple dumbbell topology
as shown in Figure 2(a). The minimum RTT between the various
machines shown in the figure, h1/h2 and h3 in Mininet and LAN
testbed is 40µs.
(a) BBR high level design.
(b) State machine.
Figure 1: BBR congestion control algorithm design.
between the client and server machines. For each scenario, we
contrast BBR’s performance with that of Cubic [28], which is the
default TCP variant on Linux and Mac OS, to determine the operat-
ing conditions under which BBR is helpful.
We synthesize the results of our 640 different experiments in
the form of a decision tree highlighting the choice between BBR
and Cubic, to aid practitioners. In general, we find that when the
bottleneck buffer size is much smaller than BDP, BBR can achieve
200% higher goodput than Cubic. However, in the case of deep
buffers, Cubic can improve goodput by 30% compared to BBR; here,
buffer refers to the bottleneck buffer size. While we find that BBR
achieves high goodput in shallow buffers, we observe that BBR’s
packet loss can be several orders of magnitude higher than that of
Cubic when the buffers are shallow.
Our analysis of BBR’s source code reveals that the high packet
loss under shallow buffers is because of BBR’s configuration pa-
rameters that maintain 2× BDP of data in flight. Decreasing the
2× multiplier or increasing the bottleneck buffer size significantly
lowers the packet losses.
Our empirical results also suggest the existence of a “cliff point"
in loss rate for BBR above which BBR’s goodput decreases signif-
icantly. We find that, empirically, this cliff point is at around 20%
loss rate. By modifying the BBR parameters, we find that the cliff
point is largely dictated by the maximum pacinд_дain value BBR
uses when probing for more bandwidth. Interestingly, we find that
BBR exhibits the highest amount of packet retransmissions at this
cliff point.
Finally, we investigate the behavior of BBR in the presence of
other flows. We find that the goodput share of BBR primarily de-
pends on the bottleneck buffer size — BBR utilizes more bandwidth
when the buffers are shallow, despite the high losses, whereas Cubic
does better when the buffers are deep.
2 BACKGROUND ON BBR
Overview of BBR’s design: We illustrate the design of BBR via
Figure 1(a). BBR periodically obtains network information via mea-
surements, including bandwidth, RTT and loss rate. BBR then mod-
els the bandwidth by using a max filter (the maximum value of the
observed bandwidth in the last few RTTs), BtlBw, and the network
delay by using a min filter, RTprop. BBR works according to a state
machine which decides BBR’s next state, as shown in Figure 1(b).
The BtlBw and RTprop values are treated as input to this state
machine.
131
When to use and when not to use BBR: An empirical analysis and evaluation study
IMC ’19, October 21–23, 2019, Amsterdam, Netherlands
(a) Mininet/LAN Testbed
(b) WAN Testbed
Figure 2: Testbeds employed in our study.
Figure 2(b) shows our WAN testbed, where two senders are
located at Stony Brook University (New York), and the receiver
is located at Rutgers University (New Jersey). The minimum RTT
between the sender and the receiver is 7ms. The network interfaces
of all hosts have a 1Gbps peak bandwidth.
3.2 Setting the network parameters
We use Linux TC to configure different network conditions for
both real and emulated network links. Specifically, we use TC-
NetEm [29] to set link delay, and TC-tbf [14] to set link bandwidth
and bottleneck buffer size. Note, we do not set network parameters
on end hosts, since doing so can result in a negative interaction
with TCP Small Queues [1, 4]. Instead, in LAN and WAN networks,
we employ TC on a separate Linksys WRT1900ACS router (running
OpenWRT Linux OS) between the end hosts. In Mininet, we set
another node as a router between the end hosts. We investigate
BBR in Linux 4.15, where the TCP layer can handle the pacing
requirements of BBR, thus fq (Fair Queue) [2] is not needed [5].
TC-tbf: Figure 3 shows how Linux TC-tbf is used to limit the
bandwidth. When the application wants to send data, the packets
will first go through the IP stack to obtain headers. Then, the packets
are enqueued to a queue named qdisc (queueing discipline) [11].
When we use TC-tbf to limit the bandwidth to, say rate, a bucket
holding tokens is created. During each second, the system adds rate
tokens into the bucket. The bucket size is pre-configured and can