we allow that congestion present in a given time slot may not be
observed by any of the probe packets in that slot. However, we do
assume a speciﬁc structure of the inaccuracy, as follows.
Let Yi be the true congestion state in slot i, i.e., Yi = 01 means that
there is no congestion at t = i and there is congestion at t = i + 1.
Here, true means the congestion that would be observed were we
to have knowledge of router buffer occupancy, queueing delays and
packet drops. Of course, the value of Yi is not available to us. Our
speciﬁc assumption is that yi is correct, i.e., equals Yi, at probability
pk that is independent of i and depends only on the number k of 1-
digits in Yi. Moreover, if yi is incorrect, it must take the value 00.
Explicitly,
(1) If Yi = 00 (= no congestion occurs) then yi = 00, too (= no
congestion reported), with probability 1.
(2) If Yi = 01 (= congestion begins), or Yi = 10 (= congestion
ends), then P(yi = Yi|(Yi = 01)∪ (Yi = 10)) = p1, for some
p1 which is independent of i.
If yi fails to match Yi, then
necessarily, yi = 00.
(3) If Yi = 11 (= congestion is on-going), then P(yi = Yi|Yi =
11) = p2, for some p2 which is independent of i. If yi fails
to match Yi, then necessarily, yi = 00.
5.2.2 Estimation
The basic algorithm assumes that p1 = p2 for consistent duration
estimation, and p1 = p2 = 1 for consistent and unbiased frequency
estimation. The estimators are as follows:
Congestion Frequency Estimation is straightforward. Denote the
true frequency of congested slots by F. We deﬁne a random vari-
able zi whose value is the ﬁrst digit of yi. Our estimate is then
bF = ∑
i
zi/M,
with the index i running over all the basic experiments we con-
ducted, and M is the total number of such experiments.
This estimator is unbiased, E[bF] = F, since the expected value
of zi is just the congestion frequency F. Under mild conditions, the
estimator is also consistent. For example, if the durations of the
congestion episodes and congestion-free episodes are independent
with ﬁnite mean, then the proportion of congested slots during an
experiment over N slots converges almost surely, as N grows, to the
congestion frequency F, from which the stated property follows.
Congestion Duration Estimation is more sophisticated. Recall
that a congestion episode is one consecutive occurrence of “k con-
gestions” preceded and followed by “no congestion”, i.e., its binary
representation is written as:
01 . . .10.
Suppose that an oracle provides us with the state of the router’s
buffer at all possible time slots in our discretization. We then count
all congestion episodes and their durations and ﬁnd out that for
k = 1,2, . . ., there were exactly jk congestion episodes of length k.
Then, congestion occurred over a total of
A = ∑
k
k jk
slots, while the total number of congestion episodes is
B = ∑
k
jk.
The average duration D of a congestion episode is then deﬁned as
D := A/B.
In order to estimate D, we observe that, with the above structure
of congestion episodes in hand, there are exactly B time slots i for
which Yi = 01, and there are also B time slots i for which Yi = 10.
Also, there are exactly A + B time slots i for which Yi 6= 00. We
therefore deﬁne
R := #{i : yi ∈ {01,10,11}},
and
S := #{i : yi ∈ {01,10}}.
Now, let N be the total number of time slots. Then P(Yi ∈{01,10}) =
2B/N, hence P(yi ∈ {01,10}) = 2p1B/N.
Similarly, P(Yi ∈{01,10,11}) = (A+B)/N, and P(yi ∈{01,10,11}) =
(p2(A− B) + 2p1B)/N. Thus,
E(R)/E(S) =
p2(A− B) + 2p1B
.
2p1B
Denoting r := p2/p1, we get then
E(R)/E(S) =
Thus,
− r/2 + 1.
rA
2B
r(A− B) + 2B
2B
(cid:18) E(R)
=
(cid:19)
− 1
+ 1.
D =
×
2
r
In the basic algorithm we assume r = 1, the estimator bD of D is
then obtained by substituting the measured values of S and R for
their means:
E(S)
bD := 2× R
S
− 1.
Note that this estimator is not unbiased for ﬁnite N, due to the
appearance of R in the quotient. However, it is consistent under
the same conditions as those stated above for bF, namely that con-
gestion is described by an alternating renewal process with ﬁnite
mean lifetimes D and D0 for the congested and uncongested peri-
ods, respectively. In this case (with r = 1) R/N converges almost
surely, as N grows, to p(D + 1)/(D + D0) while S/N converges to
2p/(D + D0), and hence bD converges almost surely to D.
5.3 Improved Algorithm
The improved algorithm is based on weaker assumptions than
the basic algorithm: we no longer assume that p1 = p2. In view
of the details provided so far, we will need, for the estimation of
duration, to know the ratio r := p1/p2. For that, we modify our
basic experiments as follows.
As before, we decide independently at each time slot whether to
conduct an experiment. With probability 1/2, this is a basic experi-
ment as before; otherwise we conduct an extended experiment com-
prising three probes, dispatched in slots i,i+1,i+2, and redeﬁne yi
to be the corresponding 3-digit number returned by the probes, e.g.,
yi = 001 means “congestion was observed only at t = i + 2”, etc.
As before Yi records the true states that our ith experiment attempts
to identify. We now make the following additional assumptions.
5.3.1 Additional Assumptions
We assume that the probability that yi misses the true state Yi
(and hence records a string of 0’s), does not depend on the length
of Yi but only on the number of 1’s in the string. Thus, P(yi = Yi) =
p1 whenever Yi is any of {01,10,001,100}, while P(yi = Yi) = p2
whenever Yi is any of {11,011,110} (note that we ignore the states
010 and 101, but address them below).
We claim that these additional assumptions are realistic, but de-
fer the discussion until after we describe the reporting mechanism
for congestion.
With these additional assumptions in hand, we denote
and
U := #{i : yi ∈ {011,110}},
V := #{i : yi ∈ {001,100}}.
The combined number of states 011,110 in the full time series is
still 2B, while the combined number of states of the form 001,100
is also 2B. Thus, we have
hence, with U/V estimating r, we employ (5.2.2) to obtain
E(U)
E(V )
= r,
bD :=
×
2V
U
(cid:18) R
S
(cid:19)
− 1
+ 1.
5.4 Validation
When running an experiment, our assumptions require that sev-
eral quantities have the same mean. We can validate the assump-
tions by checking those means.
In the basic algorithm, the probability of yi = 01 is assumed to
be the same as that of yi = 10. Thus, we can design a stopping
criterion for on-going experiments based on the ratio between the
number of 01 measurements and the number of 10 measurements.
A large discrepancy between these numbers (that is not bridged
by increasing M) is an indication that our assumptions are invalid.
Note that this validation does not check whether r = 1 or whether
p1 = 1, which are two important assumptions in the basic design.
In the improved design, we expect to get similar occurrence rate
for each of yi = 01,10,001,100. We also expect to get similar oc-
currence rate for yi = 011,110. We can check those rates, stop
whenever they are close, and invalidate the experiment whenever
the mean of the various events do not coincide eventually. Also,
each occurrence of yi = 010 or yi = 101 is considered a violation
of our assumptions. A large number of such events is another rea-
son to reject the resulted estimations. Experimental investigation
of stopping criteria is future work.
5.5 Modiﬁcations
There are various straightforward modiﬁcations to the above de-
sign that we do not address in detail at this time. For example, in
the improved algorithm, we have used the triple-probe experiments
only for the estimation of the parameter r. We could obviously
include them also in the actual estimation of duration, thereby de-
creasing the total number of probes that are required in order to
achieve the same level of conﬁdence.
Another obvious modiﬁcation is to use unequal weighing be-
tween basic and extended experiments. In view of the expression
we obtain tobD there is no clear motivation for doing that: a miss in
estimating V /U is as bad as a corresponding miss in R/S (unless the
average duration is very small). Basic experiments incur less cost
in terms of network probing load. On the other hand, if we use the
reports from triple probes for estimating E(S)/E(R) then we may
wish to increase their proportion. Note that in our formulation, we
cannot use the reported events yi = 111 for estimating anything,
since the failure rate of the reporting on the state Yi = 111 is as-
sumed to be unknown. A topic for further research is to quantify
the trade-offs between probe load and estimation accuracy involved
in using extended experiments of 3 or more probes.
6. PROBE TOOL IMPLEMENTATION
AND EVALUATION
To evaluate the capabilities of our loss probe measurement pro-
cess, we built a tool called BADABING1 that implements the basic
1Named in the spirit of past tools used to measure loss including
PING, ZING, and STING. This tool is approximately 800 lines of
C++ and is available to the community for testing and evaluation.
algorithm from § 5. We then conducted a series of experiments
with BADABING in the same laboratory environment and with the
same test trafﬁc scenarios described in § 4.
The objective of our lab-based experiments was to validate our
modeling method and to evaluate the capability of BADABING over
a range of loss conditions. We report results of experiments focused
in three areas. While our probe process does not assume that we al-
ways receive true indications of loss from our probes, the accuracy
of reported measurements will improve if probes more reliably in-
dicate loss. With this in mind, the ﬁrst set of experiments was de-
signed to understand the ability of an individual probe (consisting
of 1 to N tightly-spaced packets) to accurately report an encounter
with a loss episode. The second is to examine accuracy of BAD-
ABING in reporting loss episode frequency and duration for a range
of probe rates and trafﬁc scenarios. In our ﬁnal set of experiments,
we compare the capabilities of BADABING with simple Poisson-
modulated probing.
6.1 Accurate Reporting of Loss Episodes by
Probes
Figure 7: Results from tests of ability of probes consisting of N
packets to report loss when an episode is encountered.
An important component of our probe process is dealing with in-
stances where individual probes (where a probe consists of a series
of N packets) do not report loss accurately. In other words, ideally,
a given probe Pi should report the following:
(cid:26) 0 :
1 :
Pi =
if a loss episode is not encountered
if a loss episode is encountered
It should be noted that this requirement is only for a probe, not
necessarily the individual packets within a probe. Satisfying this
requirement is problematic because, as noted in § 3, many pack-
ets are successfully transmitted during loss episodes. Thus, we
hypothesized that we might be able to increase the probability of
probes correctly reporting a loss episode by increasing the num-
ber of packets in an individual probe. We also hypothesized that,
assuming FIFO queueing, using one-way delay information could
further improve the accuracy of individual probe measurements.
We investigated the ﬁrst hypothesis in a series of experiments
using the inﬁnite TCP source background trafﬁc and constant-bit
rate trafﬁc described in § 4. For the inﬁnite TCP trafﬁc, loss event
duration were approximately 150 milliseconds. For the constant-
bit rate trafﬁc, loss episodes were approximately 68 milliseconds
in duration. We used a modiﬁed version of BADABING to generate
probes at ﬁxed intervals of 10 milliseconds so that some number of
probes would encounter all loss episodes. We experimented with
probes consisting of between 1 and 10 packets. Packets in an in-
dividual probe were sent back to back per the capabilities of the
measurement hosts (i.e., with approximately 30 microseconds be-
tween packets). Probe packet sizes were set at 600 bytes2.
Figure 7 shows the results of these tests. We see that for the
constant-bit rate trafﬁc, longer probes have a clear impact on the
ability to detect loss. While about half of single-packet probes do
not experience loss during a loss episode, probes with just a couple
more packets are much more reliable indicators of loss. For the
inﬁnite TCP trafﬁc, there is also an improvement as the probes get
longer, but the improvement is relatively small. Examination of the
details of the queue behavior during these tests demonstrates why
the 10 packet probes do not greatly improve loss reporting ability
2This packet size was chosen to exploit an architectural feature of
the Cisco GSR so that probe packets had as much impact on inter-
nal buffer occupancy as maximum-sized frames. Investigating the
impact of packet size on estimation accuracy is a subject for future
work.
for the inﬁnite source trafﬁc. As shown in Figure 8, longer probes
begin to have a serious impact on the queuing dynamics during loss
episodes.
This observation, along with our hypothesis regarding one-way
packet delays, led to our development of an alternative approach for
identifying loss events. Our new method considers both individual
packet loss with probes and the one-way packet delay as follows.
For probes in which any packet is lost, we consider the one-way
delay of the most recent successfully transmitted packet as an es-
timate of the maximum queue depth (OW Dmax). We then consider
a loss episode (or, more generally, a congestion episode) to be de-
limited by probes within τ seconds of an indication of a lost packet