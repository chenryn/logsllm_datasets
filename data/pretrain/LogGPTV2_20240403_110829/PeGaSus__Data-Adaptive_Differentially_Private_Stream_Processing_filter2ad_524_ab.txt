child aggregations with the same parent aggregation cover two dis-
joint sets of states. We use level (aдд | AGG) to denote the level of
aдд ∈ AGG. level (aдд | AGG) = 1 if aдд has no parent aggregation
in AGG. If aдд1, aдд2 ∈ AGG and aдд1 is the child aggregation of
aдд2, level (aдд1 | AGG) = level (aдд2 | AGG) + 1.
Example 2.5. An analyst might be interested in aggregate counts
of successful connections at the level of individual sensors, as well
as for aggregations of sensors within the same room, same floor,
and the same building on campus. For each room r, let aддr denote
the aggregation of states sp, where p is an access point in room r.
Similarly, for a floor f and building b, we can define aggregations
aддf and aддb that aggregate states sp, where p is in the floor f or
building b, respectively. These sets of aggregations form a hierarchy.
An analyst might be interested in the unit query for aggregate states
at floors and buildings as well as a low signal event monitoring
query EM (aддr , w, . . . ), for each rooms r.
2.3 Privacy for Streams
Two stream prefixes Dt and D′
are considered neighbors if they
differ by the addition or removal of a single tuple; i.e., |Dt ⊕ D′
t
t | = 1
where ⊕ indicates symmetric difference. The algorithms we con-
sider in this paper operate on stream prefixes.
We define privacy for streams as follows, analogous to event
differential privacy [2, 11].
Definition 2.6 (ϵ-differential privacy). Let A be a randomized
algorithm that takes as input a stream prefix of arbitrary size and
outputs an element from a set of possible output sequences O. Then
A satisfies ϵ-differential privacy if for any pair of neighboring
stream prefixes Dt and D′
Pr[A(Dt ) ∈ O] ≤ eϵ × Pr[A(D′
, for all t, and ∀O ⊆ O,
t ) ∈ O]
t
The parameter ϵ controls the privacy risk and smaller ϵ corre-
sponds to stronger privacy protection. The semantics of this privacy
guarantee are discussed further in Section 2.4.
The following composition properties hold for differentially pri-
vate algorithms, each commonly used for building complex dif-
ferentially private algorithms from simpler subroutines. Suppose
A1 (·) and A2 (·) are ϵ1- and ϵ2-differentially private algorithms,
respectively.
• Sequential Composition: Computing A1 (Dt ) and A2 (Dt ) satis-
fies (ϵ1 + ϵ2)-differential privacy for any D.
• Parallel Composition: Let A and B be disjoint subsets of dom.
Computing A1 (Dt ∩A) and A1 (Dt ∩B), satisfies ϵ1-differential
privacy.
• Postprocessing: For any algorithm A3 (·), releasing A3 (A1 (Dt ))
still satisfies ϵ1-differential privacy for any D. That is, post-
processing an output of a differentially private algorithm does
not incur any additional loss of privacy.
The composition properties allow us to execute multiple differ-
entially private computations and reason about the cumulative
privacy risk. In our applications, we want to bound the total risk so
we impose a total epsilon “privacy budget” and allocate a portion
of the budget to each private computation.
An arbitrary numerical function f can be made differentially
private by adding noise to its output. The amount of noise depends
on the sensitivity of the function.
Definition 2.7 (Sensitivity). Let f be a function that maps datasets
to Rn. The sensitivity denoted as ∆( f ), is defined to be the maximum
L1 distance between function outputs from any two neighboring
data streams Dt and D′
Dt ,D′
|| f (Dt ) − f (D′
t
max
:|Dt ⊕D′
t )||1.
∆( f ) =
.
t
t |=1
The Laplace Mechanism [10] achieves differential privacy by
adding noise from Laplace distribution calibrated to the sensitivity.
Definition 2.8 (Laplace Mechanism (LM)). Given a function f that
maps datasets to Rn, the Laplace Mechanism outputs f (Dt ) + η,
where η is a vector of independent random variables drawn from a
Laplace distribution with the probability density function p(x|λ) =
2λ e−|x |/λ, where λ = ∆( f )/ϵ.
1
Remark. We make an assumption that each user can be in at
most m different states s during one time period. Without loss of
generality, we assume m = 1 in our application. If m > 1, we can
simply normalize the counts (a user who is in m states at time
t contributes 1/m rather than 1 to the corresponding counting
queries) or increase the amount of noise injected in our algorithm
in order to provide privacy protection in terms of any single user
during one time period.
2.4 Privacy Semantics
In this section, we discuss the semantics of privacy ensured by
Definition 2.6 and justify our choice of this privacy goal.
The privacy ensured by Definition 2.6 can be interpreted in
terms of (a) plausible deniability, and (b) disclosure of secrets to
adversaries. Let ϕu (t ) and ϕ′
u (t ) be two mutually exclusive boolean
properties about a user u at time step t. Examples of such properties
could be that a user was in building B1 at time t and building B2
at time t, respectively. An algorithm M satisfying Definition 2.6
allows a user to deny that ϕ′
u (t ) is true rather than ϕu (t ) since for
all neighboring streams Dt , D′
such that ϕu (t ) is true on Dt and
ϕ′
u (t ) is true on D′
, and for all output sets O ∈ ranдe (A), we have:
t
t
Pr[A(Dt ) = O] ≤ eϵ Pr[A(D′
t ) = O]
Plausible deniability holds even for properties that span larger
time windows, albeit to a lesser extent and degrades with the length
of the time window. That is, if ϕu (t, k ) and ϕ′
u (t, k ) are two mutually
exclusive boolean properties about a user u that span a time window
of [t − k + 1, t], then for all streams Dt , D′
such that ϕu (t, k ) is true
on Dt and ϕ′
, and that differ only in the states in
the time window [t −k +1, t], and for all output sets O ∈ ranдe (A),
we have:
u (t, k ) is true on D′
t
t
Pr[A(Dt ) = O] ≤ ek·ϵ Pr[A(D′
t ) = O]
Thus our definition also captures the more general w-event privacy
[16]. And, if a time step corresponds to 5 minutes, and an algorithm
A satisfies Definition 2.6 with ϵ = 0.1, then for properties that span
10 minutes, we get privacy at a level ϵ = 0.2, and for properties that
span 1 hour, we get privacy at a level of ϵ = 1.2. If the protected
window size goes to infinity (k is unbounded), one can still guaran-
tee privacy with parameter ℓ · ϵ, where ℓ is the maximum number
of tuples in the stream corresponding to a single user. If both k and
ℓ are unbounded, one can extend existing negative results [8] to
show that it is impossible to release accurate statistics at each time
and offer privacy.
Next we explore the semantics of Definition 2.6 in terms of
disclosure of secrets to adversaries, which can be done in terms of
the Pufferfish framework [18]. One can show that if an algorithm
A satisfies Definition 2.6, then the adversary’s posterior odds that
ϕu (t ) is true vs ϕ′
u (t ) is true after seeing the output of A, for any
pair of mutually exclusive secrets that span a single time step, is no
larger than eϵ times the adversary’s prior odds. However, this strong
privacy guarantee only holds under the restrictive assumptions
that an adversary is not aware of possible correlations between a
user’s states across time steps. With knowledge of correlations, an
adversary can learn sensitive properties of a user within a time step
even from outputs of differentially private algorithms [17, 20, 24].
Nevertheless, the ratio of the adversary’s posterior to prior odds
is still guaranteed to be no larger than e ℓϵ even in the presence of
correlations. Recall that ℓ is the maximum number of tuples in the
stream corresponding to a single user.
Recent work [5, 22] has provided methods for deriving an ϵ′ > ϵ
(but no more than ℓ × ϵ), such that algorithms satisfying Defini-
tion 2.6 with parameter ϵ offer a weaker ϵ′ bound on privacy loss
even when records are correlated across time steps. For specific
types of correlations, the effective privacy guarantee is closer to ϵ
and much smaller than ℓ × ϵ.
We emphasize that our algorithms are designed to satisfy Defi-
nition 2.6 with parameter ϵ, but simultaneously satisfy all of the
above provable privacy guarantees, with a possibly different pri-
vacy parameter. Therefore, for the remainder of the paper, we focus
exclusively on developing algorithms that satisfy Definition 2.6
while minimizing error.
3 PEGASUS STREAM RELEASE
In this section, we describe a novel, data-dependent method (called
PeGaSus) for private, real-time release of query answers on data
streams. Our algorithm consists of a novel combination of data
perturbation and online partitioning, followed by post-processing.
We first present the algorithm for computing a unit counting
query in a single target state. In Section 4, we explain how the
algorithm can be adapted to answer other kinds of queries on a
single target state and in Section 5, we explain an extension of
the algorithm to support multiple queries over hierarchical aggre-
gations of states. The input to the algorithm consists of the true
answers to the unit counting query C. The output of the algorithm
is ˆC = ˆc1, ˆc2, . . . , an infinite stream where ˆct is an estimate of the
true answer ct .
The three main modules of our method PeGaSus are as fol-
lows:
• Perturber: The Perturber consumes the input stream, adds
noise to each incoming element of the stream, and releases a
stream of noisy counts.
• Grouper: The Grouper consumes the input stream and groups
• Smoother: The Smoother performs post-processing using the
elements of the stream seen so far.
output of both above modules.
The Perturber is a standard noise-addition mechanism, but the
Grouper carries out an important role of partitioning the data into
regions that can be well-approximated by a uniform sub-stream.
The Smoother then combines this information with the output of
the Perturber. The result is a data-dependent algorithm which can
reduce error for streams with properties that are commonly wit-
nessed in practice.
The combination of the above three modules is formalized in
Algorithm 1, called Perturb-Group-Smooth stream release (PeGa-
Sus). When a new count ct arrives at time t, the Perturber takes
Figure 1: The PeGaSus algorithm for generating private streams. The Perturber and Grouper consume the true input stream,
while the Smoother consumes their output and produces the final result stream.
Algorithm 1 Perturb-Group-Smooth based Stream Release (PeGa-
Sus)
Input: C = c1, c2, . . . , privacy budget ϵ = ϵp + ϵд
Output: Private stream ˆC = ˆc1, ˆc2, . . . ,
1: for each time t do
2:
3:
4:
5:
6: end for
˜ct ← Perturber (ct , ϵp )
Pt ← Grouper (Ct , Pt−1, ϵд )
ˆct ← Smoother ( ˜Ct , Pt )
Release ˆct
the input ct and outputs a noisy version ˜ct , using ϵp portion of the
overall ϵ privacy budget (line 2). The Grouper takes as input all of
the data received so far Ct = c1, . . . , ct and the partition from the
previous time Pt−1. (Partition Pt−1 is a partition over the integers
{1, . . . , t − 1} and represents a grouping of the first t − 1 counts.) At
each time step, the Grouper outputs an updated partition Pt with
a portion of the privacy budget ϵд (in line 3). The Smoother then
computes a final estimate ˆct of ct based on all the initial noisy
counts ˜Ct = ˜c1, . . . , ˜ct and the current partition Pt (in line 4).
The execution of Algorithm 1 is illustrated in Figure 1. Both the
Perturber and the Grouper are colored red because they consume
the input stream and use the privacy budget. The Smoother is col-
ored blue because it only uses the output of the Perturber and the
Grouper.
Theorem 3.1. When the Perturber satisfies ϵp-differential privacy
and the Grouper satisfies ϵд-differential privacy, Algorithm 1 ensures
ϵp + ϵд = ϵ-differential privacy.
The above theorem follows directly from the sequential com-
position and post-processing properties of differential privacy (as
described in Section 2).
Algorithm 1 forms the basis of a number of algorithm variants
we consider throughout the paper. In the remainder of this section,
we describe below the design of each module, and basic variants for
the Smoother. We also include theoretical analysis that illustrates
cases where smoothing can reduce error. Sections 4 and 5 describe
extensions to the algorithm for other kinds of queries.
Pt−1 ← ∅ and let G be closed, empty group.
G ← Last group from Pt−1
Algorithm 2 Deviation based Grouper (DBG)
Input: Ct = c1, . . . , ct , the previous partition Pt−1, ϵд, θ
Output: Pt (a partition of {1, . . . , t})
1: if t = 1 then
2:
3: else
4:
5: end if
6: if G has been closed then
7:
8:
9: else
10:
11: