1 )) and
the second party holds a k-bit string y; the second party
should learn {mi
1), . . . , (mk
0 , mk
0, m1
yi}k
i=1.
Oblivious pseudorandom function evaluation (OPRF).
Let F be some ﬁxed pseudorandom function. In an OPRF
protocol one party holds a key s and the second party holds
an input x; the second party learns Fs(x) and the ﬁrst party
learns nothing. We denote this functionality by FOPRF.
2.1 Garbled Circuits
Yao’s garbled circuit methodology [21] provides a key build-
ing block for secure two-party computation. We do not pro-
vide the details, but instead describe Yao’s approach in an
abstract manner that suﬃces for our purposes. Let C be
a circuit, and assume for concreteness here that C takes a
k-bit input from each party and provides a single-bit output
to the second party. Yao gives algorithms (Garble, Eval) such
that:
• Garble(1k, C) outputs a garbled circuit gC and two sets
of input-wire labels (cid:126)X = (X 1,0, X 1,1, . . ., X k,0, X k,1)
and (cid:126)Y = (Y 1,0, Y 1,1, . . . , Y k,0, Y k,1). The value X i,0
(resp., X i,1) is the “0-label” (resp., “1-label”) for the ith
input wire of one of the parties. (Y i,0, Y i,1 analogously
serve as the input-wire labels for the second party.)
• Eval(gC, X 1, . . . , X k, Y 1, . . . , Y k) outputs a bit.
By way of notation, for (cid:126)X as above and an input x ∈ {0, 1}k
we let (cid:126)X(x) def= (X 1,x1 , . . . , X k,xk ) (and similarly for (cid:126)Y (y)).
The correctness guarantee is that for any C and any in-
puts x, y, if Garble(1k, C) outputs (gC, (cid:126)X, (cid:126)Y ) then
Eval(gC, (cid:126)X(x), (cid:126)Y (y)) = C(x, y).
In typical usage, one party (holding input x) runs Garble(1k, C)
to obtain (gC, (cid:126)X, (cid:126)Y ) and sends gC and (cid:126)X(x) to the second
party. In addition, the second party (holding input y) ob-
tains (cid:126)Y (y) using k invocations of oblivious transfer. The
second party can then compute C(x, y) using Eval as indi-
cated above.
exists an eﬃcient simulator S = (S1,S2) such that
The security guarantee (informally stated) is that there
• S1(1k, C) outputs input-wire labels X 1, . . . , X k, Y 1, . . . , Y k
and state s.
• S2(y, C(x, y), s) outputs gC.
• For any C, x, y, the following distributions are compu-
tationally indistingishable:
X 1, . . . , X k, Y 1, . . . , Y k, s ← S1(1k, C);
gC ← S2(y, C(x, y), s)
:
(cid:190)
(gC, X 1, . . . , X k, Y 1, . . . , Y k)
(cid:189)
and(cid:110)
(cid:111)
gC, (cid:126)X, (cid:126)Y ← Garble(1k, C) : (gC, (cid:126)X(x), (cid:126)Y (y))
.
(The above is slightly stronger than what is usually required,
but is satisﬁed by the standard construction.) Since the
computation of S1,S2 depends only on y and C(x, y), the
above ensures that the second party learns nothing beyond
what is implied by its own input and output. For tech-
nical reasons, we also extend the deﬁnition of S2 so that
S2(y,⊥, s) outputs gC such that, for any x, y, the resulting
(gC, Y 1, . . . , Y k) is computationally indistinguishable from
(gC, (cid:126)Y (y)) (where (gC, (cid:126)X, (cid:126)Y ) ← Garble(1k, C)). This im-
plies that if the ﬁrst party doesn’t send the input-wire labels
(cid:126)X(x), then the second party does not even learn the out-
put C(x, y).
2.2 Secure Computation
We use standard notions of secure computation, and re-
fer the reader elsewhere (e.g., [7]) for a detailed discussion.
Brieﬂy, security is deﬁned by requiring indistinguishability
between a real execution of the protocol and an ideal world
in which there is a trusted party who evaluates the function
in question on behalf of the parties. More formally, ﬁx a pro-
tocol π that computes a functionality F . Because it holds
throughout this paper, assume F provides output to P2 only.
(Note, however, that F may be randomized.) Consider ﬁrst
an execution in the real world. Here, party P1 holds an
input x while party P2 holds an input y, and one of these
parties is corrupted by an adversary A that has auxiliary in-
put z. The honest party runs protocol π as instructed (using
some ﬁxed value for the security parameter k), responding
to (arbitrary) messages sent by A on behalf of the other
party. (We stress that A is malicious and can arbitrarily
deviate from the protocol speciﬁcation.) Let viewA
π (x, y, k)
denote the view of A throughout this interaction, and let
outπ(x, y, k) denote the output of the honest party (which
is empty if P1 is honest). Set
.
view
A
π (x, y, k), outπ(x, y, k)
realπ,A(z)(x, y, k) def=
An ideal execution of the computation of F proceeds as
follows. Once again, party P1 holds an input x while party
P2 holds an input y, and one of these parties is corrupted by
an adversary A that has auxiliary input z. The honest party
sends its input to the trusted party, while the corrupted
party can send anything. Let x(cid:48), y(cid:48) be the values received
by the trusted party; it then computes F(x(cid:48), y(cid:48)) and gives
the result to P2. (There are no issues of fairness here since
P1 receives no output.) The honest party outputs whatever
it was sent by the trusted party, and A outputs an arbitrary
function of its view. Let outA
F (x, y, k) (resp., outF (x, y, k))
denote the output of A (resp., the honest party) following
an execution in the ideal model as described above. Set
idealF ,A(z)(x, y, k) def=
out
A
F (x, y, k), outF (x, y, k)
.
(cid:179)
(cid:180)
(cid:180)
(cid:179)
487Protocol πsearch
Input to P1: A database {(pi, xi)}n
Input to P2: A pattern p.
Output: P2 learns {xi | pi = p}, the database size n, and the payload length |xi|.
1. Party P1 chooses a key s ← {0, 1}k, and then for each i ∈ {1, . . . , n} com-
i=1.
putes αi := Fs(pi) and ci ← Encαi (xi).
2. P1 and P2 execute an OPRF protocol, where the input of P1 is s and the
input of P2 is p. Party P2 obtains α = Fs(p).
3. P1 sends the {ci}, in random permuted order, to P2.
4. P2 outputs {Decα(ci) | Decα(ci) (cid:54)=⊥}.
Figure 2: A protocol for keyword search.
(cid:169)
(cid:169)
(cid:170)
(cid:170)
The strongest notion of security — full security — re-
quires that for every polynomial-time adversary A in the
real world, there exists a corresponding polynomial-time
adversary S in the ideal world such that realπ,A(x, y, k)
and idealF ,S (x, y, k) are computationally indistinguishable.
Here, we achieve a slightly weaker deﬁnition known as one-
sided security. Speciﬁcally, we achieve full security when
P2 is corrupted. When P1 is corrupted, though, we only
achieve privacy for the honest P2; namely, a malicious P1
learns nothing about the input of P2.
If P1 is corrupted,
however, there are no guarantees that the output of P2 is
correct.
Definition 2.1. Let π be a two-party protocol computing
a functionality F where only P2 receives output. We say π
is one-sided secure if:
1. For every non-uniform polynomial-time adversary A
corrupting P2 in the real world, there is a correspond-
ing non-uniform polynomial-time adversary S corrupt-
ing P2 in the ideal world such that
realπ,A(z)(x, y, k)
idealF ,S(z)(x, y, k)
.
2. For every non-uniform polynomial-time adversary A
corrupting P1 in the real world, there is a correspond-
ing non-uniform polynomial-time adversary S corrupt-
ing P1 in the ideal world such that
(cid:170) c≡(cid:169)
(cid:170) c≡(cid:169)
viewπ,A(z)(x, y, k)
viewF ,S(z)(x, y, k)
.
3. A KEYWORD-SEARCH PROTOCOL
In this section we present a protocol for keyword search,
and then show an application to pattern matching (both of
these are deﬁned more formally below). Our constructions
yield some advantages relative to prior protocols for these
tasks [4, 9] that will be discussed at the end of this section.
3.1 Keyword Search
In this setting party P1 has as input a database {(pi, xi)}
of tuples, where we refer to each pi as a keyword and each
xi as the associated payload.
(We assume the number of
tuples is known, and all payloads have the same [known]
length.) Party P2 has as input a keyword p, and should
learn {xi | pi = p}. We denote this functionality by Fks,
and stress that we allow pi = pj for i (cid:54)= j (something that
is disallowed2 in prior work).
2Keyword repeats could be handled in prior work by padding
The basic idea of our protocol is similar to that of pre-
vious work, but diﬀers in the details. P1 begins by choos-
ing a random key s for a pseudorandom function F . Then
for each tuple (pi, xi) party P1 computes αi := Fs(pi) and
ci ← Encαi (xi) (where Enc represents a symmetric-key en-
cryption scheme whose properties will be discussed below),
and sends all the {ci} (in random permuted order) to P2.
Next, P1 and P2 run an OPRF protocol that enables P2 to
learn α := Fs(p). Finally, P2 attempts decryption of each of
the received ci using α, and outputs the resulting plaintext
for any successful decryption. See Figure 2.
In addition to the standard notion of indistinguishabil-
ity against chosen-plaintext attacks, we impose two addi-
tional requirements on the encryption scheme used in the
protocol. First, we require that decryption with a random
(incorrect) key fails except with negligible probability; i.e.,
that Prα,α(cid:48) [Decα(cid:48) (Encα(x)) (cid:54)=⊥] is negligible for any x. Sec-
ond, we require a notion of key indistinguishability which,
roughly, means that it is impossible to distinguish two ci-
phertexts encrypted using the same key from two cipher-
texts encrypted using diﬀerent keys. Formally, we require
the following to be negligible for any polynomial-time dis-
tinguisher D:
(cid:175)(cid:175)(cid:175)Prα[DEncα(·),Encα(·)(1k) = 1]
− Prα,α(cid:48) [DEncα(·),Encα(cid:48) (·)(1k) = 1]
(cid:175)(cid:175)(cid:175) .
Both of these requirements are achieved by many standard
encryption schemes, e.g., counter mode where the plaintext
is padded with 0k.
Theorem 3.1. If (Enc, Dec) satisﬁes the properties out-
lined above, and the OPRF sub-protocol is one-sided secure
(cf. Deﬁnition 2.1), then πsearch is a one-sided secure proto-
col for Fks.
Proof. We need to show that πsearch achieves privacy
against a malicious P1, and is fully secure against a malicious
P2. Privacy when P1 is corrupted follows easily from the as-
sumed privacy of the OPRF sub-protocol, since P2 sends no
other messages in πsearch. Next, we show security against
a malicious P2 in the FOPRF-hybrid model. (By standard
composition theorems, this implies security of πsearch.) To
do so we describe a simulator S that is given access to an
ideal functionality computing Fks. The simulator runs the
all payloads to some maximum length, but this would be less
eﬃcient than what we propose.
488adversarial P2 as a sub-routine and extracts from P2 its in-
put p to FOPRF. Then S sends p to the functionality Fks,
and receives in return the database size n, the payload length
|x|, and a set {xi}t
i=1 for some t (possibly t = 0). The sim-
ulator chooses random α, α(cid:48) ← {0, 1}k, and gives α to P2
as its output from FOPRF. Finally, S prepares t ciphertexts
ci ← Encα(xi) and n − t ciphertexts ci ← Encα(cid:48) (0|x|) and
gives these (in random permuted order) to P2. This gener-
ates a view for P2 that is computationally indistinguishable
from the view of P2 running πsearch in the FOPRF-hybrid
model.
3.2 Applications to Pattern Matching
We can use any protocol for keyword search as a sub-
routine in various text-processing tasks. We illustrate with
the example of pattern matching, and then describe a more
general class of functions we can compute.