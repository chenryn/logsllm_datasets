and action a1 is never executed, and the assertion is violated. To lo-
calize this bug, our table entry bug localization algorithm first finds
the correct entries to rectify the table’s behavior. According to §4.2,
we can encode the table’s lookup result into the ABV and execute
corresponding actions. Thus, Aquila tries to find such entries by
replacing all the tables with variables and let the SMT solver search
a valid entry for each table. In Figure 9, for example, t1 is replaced
with a variable v1. If the solver can find such valid entries, then
we continue the next step to report the minimal set of potential
locations; otherwise, the bug should be in data plane. In this case,
the SMT solver finds a solution that matches the input packet’s
destination IP and action a2, which means the table entry is buggy.
To minimize the potential locations, for each table, we use an
indicator variable rep𝑖 to represent whether a table should be re-
placed with a function variable fv𝑖. As a result, the table is en-
coded as 𝑡𝑖 = ite(𝑟𝑒𝑝𝑖, 𝑓 𝑣𝑖, 𝑒𝑖), where 𝑒𝑖 is the encoded table entries
(ABV). Now, we solve the same problem with an optimization goal
MAXSAT𝑖¬𝑟𝑒𝑝𝑖, obtaining a minimal subset of table entries poten-
tially triggering the bug.
Localizing bugs in P4 program. Intuitively, we simulate a fix by
overwriting variables in the suspect actions and checking whether
the violation could be fixed. We use SMT solver to find a valid fix.
The algorithm works as follows:
• (1) Given the counterexample in the preparation stage and vio-
lated assertion 𝑣 (§5.1), we employ a reverse taint-analysis ap-
proach that checks each action in the program backward, and
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
Tian et al.
identify and put the action that potentially causes 𝑣 in a list 𝐴. In
Figure 4, because ipv4.ttl assertion is violated, we use a trace
list to record correlated variables and actions, such as ipv4.ttl
and ig_md.ttl.
• (2) We continue filtering the actions in 𝐴 by checking the causal-
ity between each action 𝑎𝑖 in 𝐴 and the violated assertion 𝑣. For
each 𝑎𝑖, we use the SMT solver to check whether 𝑣 implies the
execution of 𝑎𝑖; if not, we remove 𝑎𝑖 from 𝐴. Finally, we get a
new list 𝐴′. This step mainly aims at reducing false positives in
our debugging result.
• (3) We maintain a variable list 𝑅, which contains all variables that
appeared in 𝑣. We now check each action 𝑎′
𝑖 in 𝐴′ backward from
the end of the program. For each 𝑎′
𝑖, we put all of its variables in 𝑅.
Then, for each 𝑟𝑖 ∈ 𝑅, we create a new statement 𝑠 that overwrites
𝑟𝑖 with an arbitrary “havoc” value, and then insert 𝑠 below 𝑎′
𝑖.
We use SMT solver to check whether the new program (i.e., with
the newly added statement 𝑠) fixes the violated assertion 𝑣. If so,
this means the action is a candidate root cause for the violated
assertion. Such a design enables us to find the location of missing
statements. To minimize the location scope, we can again use
the minimal satisfiability optimization of SMT solver to get a
minimal set.
• (4) Until all actions in 𝐴′ are checked. We have multiple potential
bug root causes for the violation.
An example for localizing statement missing bug. We now
use Figure 4 to illustrate how do we localize the statement miss-
ing bug. Suppose we meet a violation and get a counterexample:
when ipv4.ttl==1, the TTL computation is wrong. In the prepa-
ration phase, we record actions a1 and a3, because a2 is removed
because it does not trigger the counterexample. In the bug local-
ization phase, (1) we check each action backward in Figure 4 pro-
gram. Because actions a1 or a3 may result in the violated assertion
ipv4.ttl==ipv4.ttl-1, our list 𝐴 = {𝑎1, 𝑎3}. (2) 𝐴 = 𝐴′ in this ex-
ample, because the violated assertion implies both the actions. (3)
For each action in 𝐴′, we simulate the “fix” for the violated as-
sertion. Due to limited space, we skip previous fixing steps and
go directly to line 5. In line 5, for variable ig_md.ttl, we insert
one new statement ig_md.ttl=havoc_i below a1, which overwrites
ig_md.ttl in a1. The new program does not have the violation,
which means line 5 or line 6-8 are potential locations for the bug,
because line 9 is another action. By optimization mentioned earlier,
we can narrow down the scope from line 6-8 to line 7-8. This de-
bugging result means we can fix the bug by two ways: (1) adding
ig_md.ttl=ig_md.ttl-1 in line 7 or 8; and (2) changing the statement
in a1 into ig_md.ttl=ipv4.ttl-1.
The “accuracy” of bug localization. In practice, the bug localiza-
tion of Aquila may not always be just one line in data plane code
or one table in the control plane; the result may contain a block of
data plane program or multiple tables for complex switch functions.
Thus, the bug localization of Aquila aims to narrow down the scope
of potential bugs in the “best effort” way. For a data plane pro-
gram violating specification, it may have multiple potential buggy
sources, and fixing one of them would enable the program to meet
the specification; thus, our approach can tell the programmer a
small enough scope for the potential bug.
Figure 10: Self validation of Aquila. We put Assume, A(P),
and Assert in a solver to verify equivalence.
6 SELF VALIDATION OF AQUILA
Bugs in Aquila are headaches in reality, as they significantly affect
the confidence of our network engineers in the verification result.
To make Aquila bug-free, the most rigorous way is to build Aquila by
a fully verified toolchain and formally verifying the implementation
logic of Aquila, which however are not amenable for an industry-
level system. Instead, we take a more practical perspective to ensure
the correctness of Aquila implementation. Given the fact that GCL
and its verification via SMT solvers were well studied with various
tools available for use in the past ten years, we have confidence in
including them in our trusted computing base; on the contrary, the
component encoding part in Aquila is error-prone based on our
experience. Thus, in Aquila implementation, we trust the whole-
switch GCL composition, verification condition generation and SMT
solver (white boxes in Figure 7), and mainly check implementation
bugs in the component encoding (gray boxes in Figure 7).
To validate the implementation of Aquila, we take a translation
validation [34, 41] approach: we employ alternative tools to gen-
erate semantics for the P4 program, and compare it with the GCL
extracted by Aquila. If the two are equivalent, we have confidence
that Aquila is implemented correctly; otherwise, we examine the
discrepancy and identify bugs based on returned counterexample.
Refinement proof. We employ refinement proof [33, 39] (see fig-
ure below) to construct our translation validation.
𝑠′
R
𝑠′
𝑠𝐴
R
A(P)
X(P)
𝑠𝑋
𝐴
𝑋
Given a P4 program 𝑃 is encoded in two different formal lan-
guages. We use 𝐴(𝑃) to denote the one produced by Aquila and
𝑋(𝑃) to denote an alternative representation. Their equivalence
means the observable effects of 𝐴(𝑃) and 𝑋(𝑃), i.e., its processing
on packet states, should be the same. In particular, it relies on a
relation 𝑅 that connects equivalent states that describe the same
packet but in different formal languages. Assume that 𝐴(𝑃) tran-
sitions from an initial state 𝑠𝐴 to a final state 𝑠′
𝐴. Starting from an
equivalent initial state 𝑠𝑋 such that 𝑅(𝑠𝐴, 𝑠𝑋) holds, 𝑋(𝑃) must
transition to a final state 𝑠′
𝑋) also holds. Such proof
validates that 𝐴(𝑃) and 𝑋(𝑃) has equivalent effects on all packets.
How to construct a refinement proof in our Hoare-style frame-
work? By looking 𝐴 as the representation generated by Aquila, we
can define pre and post-conditions for a given program, and it
checks whether the following holds:
𝑋 where 𝑅(𝑠′
𝐴, 𝑠′
if (hdr.x != 0) {  hdr.y = 2;  hdr.x = 0;}{Assume hdr.x != 0;  let hdr.y := 2;  let hdr.x := 0;}[]{Assume hdr.x == 0}Input:hdr.x == arg1;hdr.y == arg2;Output:hdr.x == IF(arg1 != 0) THEN 0 ELSE arg1;hdr.y == IF(arg1 != 0) THEN 2 ELSE arg2;hdr.x := h1;hdr.y := h2;hdr.x := arg1;hdr.y := arg2;SASXA(P)X(P)Semantic GeneratorAquila's ValidatorSemantic TranslatorAquilahdr.x := (arg1 != 0) ? 0 : arg1;hdr.y := (arg1 != 0) ? 2 : arg2;hdr.x := h1';hdr.y := h2';SA'SX'P4 ProgramAssume:Assert:∀𝑠𝐴, 𝑠′
𝐴, pre(𝑠𝐴) ∧ (𝑠𝐴
𝐴(𝑃)−−−−→ 𝑠′
𝐴) ⇒ post(𝑠′
𝐴).
(1)
We thus need to seek another representation for 𝑃, i.e., 𝑋(𝑃),
to validate the trustworthiness of 𝐴(𝑃), by defining pre(𝑠𝐴) and
post(𝑠′
𝐴) as the following way:
pre(𝑠𝐴) ≡ 𝑅(𝑠𝐴, 𝑠𝑋),
𝐴, 𝑠′
post(𝑠′
𝐴) ≡ 𝑅(𝑠′
𝑋) ∧ (𝑠𝑋
𝑋 (𝑃)−−−−→ 𝑠′
𝑋).
(2)
Self validator of Aquila. The key challenge in building a self
validator is semantic translator (gray box in Figure 10), rather than
alternative representation selection or semantic generator. In our
implementation, we chose a recent effort, Gauntlet [43], to provide
alternative representation, because it defines a big-step semantics
for each individual component (parser, match-action unit, deparser).
This enables us to implement semantic translator easier.
As shown in Figure 10, we use Gauntlet as the semantic generator
to translate a given P4 program 𝑃 into a representation 𝑋(𝑃)—it
computes symbolically the output value of every header field. This
leads to a straightforward refinement relation between 𝑠𝐴 and 𝑠𝑋 :
we simply require that every header field in 𝑠𝐴 is identical to its
counterpart in 𝑠𝑋 .
Building semantic translator is non-trivial. It is used to translate
the representation 𝑋(𝑃) into the precondition and postcondition
for 𝐴(𝑃) in the format of guarded command language, i.e., Assume
and Assert in Figure 10. Intuitively, we construct an expression
representing equation (1), and thus we can check the validation by
using SMT solver to check it. Due to limited space, see Appendix C
for more details.
Identifying bugs. Aquila’s self validator cannot directly pinpoint
the implementation bugs; thus, we get a counterexample when
inequivalence occurs, and then analyze where are bugs in a semi-
automatic way.
7 DEPLOYMENT EXPERIENCE
Aquila has been used by Alibaba’s network engineers to verify the
correctness of data plane programs in the edge networks for half a
year. It has successfully guided our network engineers in avoiding
many potential critical failures. After Aquila is used online, no
service failure resulting from data plane bugs occurred so far.
This section first presents representative scenarios and bugs in
our experience with Aquila (§7.1). Then, we show example bugs
detected by Aquila’s self validation (§7.2).
7.1 Scenarios and Bugs
Bugs detected by Aquila in the past months mainly include: (1)
unexpected program behaviors (e.g., invalid header and out-of-
register), (2) incorrect table entries, (3) incorrect service-specific
properties (e.g., buggy actions, and incorrect packet processing
logic), and (4) wrong call sequence of multi-pipeline.
This section selects three representative scenarios, including
a single P4 gateway program, a hyer-converged data plane (i.e.,
Figure 2), and data plane update, to illustrate the practicality of
Aquila. Table 2 compares Aquila’s specification complexity with
two existing tools p4v and Vera in the verification of these three
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
Table 2: Comparing lines of specifications.
Scenario 1
Scenario 2
Scenario 3
O(100)
O(1000)
Aquila
Vera [1, 2]
p4v
O(10)
O(100)
O(100)
O(1000)
Vera’s APIs are not flexibly to express our specifications
scenarios. We reproduced p4v system [30], and used Vera open-
source prototype [1, 2]. Note that in following scenarios, both p4v
and Vera ran out of memory.
Scenario 1: Traffic statistics for monitoring. Our monitoring
system for edge networks relies on statisticizing the incoming busi-
ness traffic. For a given business traffic, it should be first forwarded
by a metropolitan router to a VXLAN gateway. Then, this VXLAN
gateway copies these packets and sends the original packets back
to that metropolitan router. For the copied packets, the VXLAN
gateway encapsulates and sends them to a collection of servers for
traffic statistics. These servers have been deployed programs re-
sponsible for classifying the received packets into different groups
based on their business. Because the rapid traffic growth brought
big pressure to these servers, we implemented the traffic statistic
logic in P4 switches to replace those servers.
We used Aquila to verify this P4 program, which is important. A
small error would mess up our monitoring system. The specifica-
tion includes: (1) when a known type of packet 𝑝 comes, whether
the number and states of 𝑝 is statisticized correctly, (2) when a new
(unknown type of) traffic packet 𝑞 comes, whether a correct meta-
data is successfully added to 𝑞’s header, and (3) whether 𝑞’s fields
are evaluated correctly. For example, whether each packet with
destination IP address 10/8 is successfully added a queue length
metadata information in its header and whether the DSCP value of
each packet destinating 20/8 is changed to be three.
Aquila detected two data plane bugs in one second. The first
bug was detected in the old traffic handling component of VXLAN
gateway P4 program. As mentioned above, this component should
send the original packets 𝑝 back to the metropolitan router, enabling
the backend servers to handle these packets normally; however,
in this bug case, the program incorrectly sets the metadata of the
original traffic flow packet, i.e., 𝑝, to zero, causing the backend
servers to read the incorrect state of 𝑝. The second bug is caused by
a “copy-and-paste” error: when our engineers directly copied and
pasted the register value assignment in our P4 program responsible
for statisticizing the incoming traffic flow, they forgot to change
some of the pasted P4 code.
Scenario 2: Hyper-converged P4 CDN. In our edge networks,
a CDN PoP includes three components: the edge servers provid-
ing content delivery service, the middle-boxes providing network
functions such as load balancer and DDoS defense, and L3 switches
connecting the CDN to ISP network. As motivated in §2, our oper-
ators put the functions of middle-boxes (including scheduler, load
balancer, firewall, and DDoS defense) and L3 switch into a sin-
gle programmable switch, as shown in Figure 2, where the most
packets are processed by programmable ASICs, and the rest (e.g.,
cache-missed HTTP requests) are forwarded to switch CPUs.
We used Aquila to verify the CDN’s hyper-converged P4 pro-
gram with a set of correctness specifications, including (1) each
function’s correctness, (2) undefined behavior checking, (3) the cor-
rectness of values passed among different pipelines, and (4) whether
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
Tian et al.
recirculation number is correctly bounded. A critical undefined be-
havior bug was detected. As shown below, for an input packet that
does not have an ipv4 header and does not have ipv6 header either,
e.g., an ARP packet, the packet should apply the table egress_ipv4,
if this packet sets mac_config_on=false.
if (ipv6.isValid()) {
egress_ipv6.apply(ipv6, eg_state);
} else if (!mac_config_on || ipv4.isValid()) {
egress_ipv4.apply(ipv4, eg_state);
}
An undefined behavior would occur because this packet does
not have an ipv4 header. Aquila detected the violation within 40.1
seconds, and localized this bug in one minute.
Another bug we detected in this scenario existed in deparser
program. The engineer wanted to reassemble the packet via a pre-
defined struct that contains the necessary headers. It was intended