we have:
(cid:104)|˜hx) − h∗(x, D)| · |crt(x, D)|(cid:105)
Risk(h, D) = Bayes(D)+ E
x←X
Pr[ ˜Y = 1 | ˜X = x] =
p
p + t(1 − p)
t(1 − p)
p + t(1 − p)
+
· Pr[Y = 1 | X = x]
where h∗ is the Bayes-Optimal classiﬁer as deﬁned in
Deﬁnition 5.
151134
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:49 UTC from IEEE Xplore.  Restrictions apply. 
Proof. For simplicity, in this proof we use h∗(x) instead of
h∗(x, D). We have
[˜h(x) (cid:54)= y]
[h∗(x) (cid:54)= y | ˜h(x) = h∗(x)] Pr[˜h(X) = h∗(X)]
[h∗(x) = y | ˜h(x) (cid:54)= h∗(x)] Pr[˜h(X) (cid:54)= h∗(X)]
[h∗(x) (cid:54)= y | ˜h(x) (cid:54)= h∗(x)] Pr[˜h(X) (cid:54)= h∗(X)]
[h∗(x) = y | ˜h(x) (cid:54)= h∗(x)] Pr[˜h(X) (cid:54)= h∗(X)]
because of the way poisoning is done 3. Therefore we have
Pr[Cτ ( ˜X) = 1] =
p + (1 − p)t
t
· Pr[Cτ (X) = 1])
(13)
and we get
α >
2(0.5 − γ)τ · t0
(p + (1 − p)t0)
· p + (1 − p)t0
= (1 − 2γ) · τ · Pr[Cτ (X) = 1])
≥ (1 − 2γ) · τ
= (n).
(n)
t0
τ (1 − 2γ)
· Pr[Cτ (X) = 1])
This means that if the assumption of Equation 10 holds then
the error would be larger than (n) + Bayes( ˜D) which means
the probability of 10 happening is at most δ(n) by
Bayes-optimality of the learning algorithm. Namely,
(cid:104)˜h(x) = 1
(cid:105) ≥ 0.5 + γ
(cid:21)
Pr
Pr
x←X|Cτ (x)=1
S← ˜Dn
˜h←L(S)
In case of t = t1, the proof is similar. We ﬁrst assume
≥ 1 − δ(n).
Pr
x←X|Cτ (x)=1
[˜h(x) = 0] > 0.5 − γ
(14)
Using Equations 7 and 9 we get the following (similar to
Inequality 12 for t = t0)
E
2τ t1
α > (
[˜h(x)]) ·
x← ˜X|Cτ ( ˜X)=1
· Pr[Cτ ( ˜X) = 1].
(15)
Therefore, using Equations 13 and 14 we get α > (n) which
implies
p + (1 − p) · t1
(cid:104)˜h(x) = 1
(cid:105) ≤ 0.5 − γ
(cid:21)
≥ 1 − δ(n).
(cid:20)
(cid:20)
Pr
S← ˜Dn
˜h←L(S)
Pr
x←X|Cτ (x)=1
Risk(h, D)
=
=
+
(x,y)←(X,Y )
(x,y)←(X,Y )
E
E
E
(x,y)←(X,Y )
= Bayes(D)
−
(x,y)←(X,Y )
E
E
+
(x,y)←(X,Y )
= Bayes(D)
−
(x,y)←(X,Y )
E
+
E
(x,y)←(X,Y )
= Bayes(D) +
[|y − h∗(x)||˜h(x) − h∗(x)|]
[(1 − |y − h∗(x)|)|˜h(x) − h∗(x)|]
(x,y)←(X,Y )
E
[|˜h(x) − h∗(x)|
= Bayes(D) + E
x←X
[(1 − 2|y − h∗(x)|)|˜h(x) − h∗(x)|]
[1 − 2|y − h∗(x)|]]
E
y←Y |X=x
Now we show that Ey←Y |X=x[1 − 2|y − h∗(x)|] = |crt(x)|.
The reason is that, if Pr[Y = 1|X = x] ≥ 0.5 then h∗(x) = 1
and we have
Ey←Y |X=x[1−2|y−h∗(x)|] = 2 Ey←Y |X=x[y]−1 = |crt(x)|.
And if Pr[Y = 1|X = x] 
2(0.5 − γ)τ · t0
(p + (1 − p)t0)
· Pr[Cτ ( ˜X) = 1]
Now we note that for any x such that f (x) = 1 we have
Pr[ ˜X = x] =
p + (1 − p)t
t
· Pr[X = x])
(10)
COMPARING DIFFERENT STRATEGIES FOR POISONING.
APPENDIX B
Remark 10 explains that the adversary can choose amongst 4
different strategies. Here we explore the effect of this choice
on the accuracy of the attack. We refer to different strategies
using notations 0-0, 1-0, 1-1 and 0-1. b-b(cid:48) means that the
attack samples points with target feature equal to b and sets
the label for those examples to be b(cid:48). For the ratios of the
attack, we use 0.5 v.s x, where we change x from .1 to .9 in
the table below. It is important to note that in the Census
dataset there are more positive labels for Males (b = 1)
compared to Females (b = 0). In the experiments, we use 5%
poisoning and 500 shadow models.
As it could be observed, 0-0 and 1-1 are never the best choice.
We conjecture that poisoning would be most effective if it is
in the opposite direction of the dominant rule in the
distribution. Also, these results suggest that the best result
would be obtained by male poisons, if the number of males
are less than number of females and vice versa.
3For simplicity we are assuming the support of distribution is discrete.
Otherwise, we have to have the same argument for measurable subsets instead
of individual instances.
161135
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:49 UTC from IEEE Xplore.  Restrictions apply. 
Strategy
0-0
0-1
1-0
1-1
.4
.3
.2
.9
.7
.5
.1
78% 88% 92%
83% 81% 75% 69% 50%
94% 98% 98%
86% 85% 83% 81% 50%
99% 98% 96% 92% 50%
87% 91% 93%
91% 89% 84% 78% 50% 74%% 77% 82% 85%
.6
72%
88%
85%
.8
TABLE X: The success of attack when using different strategies.
APPENDIX C
THE ROBUSTIFIED ALGORITHM
In this sections we present our modiﬁed attack for the
experiments where the shadow distribution is different from
the target distribution. The selection of poison data is same as
Algorithm 1 but the selection of queries and shadow model
training step is different. Here we show the modiﬁed steps
only. These modiﬁcations are inspired by techniques used in
machine learning for making algorithm robust to distribution
shift.
Choosing the robust black box queries This algorithm is
the same as Algorithm 2 with the modiﬁcation of Step 6:
| ≤ 0.4 then Tq = Tq ∪ {x}.
if 0.2 ≤ |1 − 2
Guessing fraction of samples with property f This
algorithm is the same as Algorithm 3 with the modiﬁcation of
Step 4.
Construct a dataset
{( ˆR1
1, 1), . . . , ( ˆR1
k, 0)} where
k, 1), ( ˆR0
i=1 Mi(x)
(cid:80)r
r
1, 0), . . . , ( ˆR0
k ⊕ W b
Rb
|Rb
k ⊕ W b
k|1
k
ˆRb
k =
Fig. 4: F score vs poison rate. Note that there is an imbalance in
the distribution of the labels in our experiments and this is why some
times the score is smaller than 50%.
k is a noise vector sampled from a Bernoulli noise
and W b
distribution of size q and probability of 1 being 0.4. and train
a linear model with appropriate regularization on it to get MA
(We use (cid:96)2 regulizer with weight 2 ·(cid:112)1/k).
APPENDIX D
OMITTED FIGURES
In this section, we have three ﬁgures that are omitted from
the main body of the paper due to space constraints.
Undetectablilty of the Attack Recall that in our threat
model, the adversary is able to poison a fraction of the
training data. If the target model quality degrades signiﬁcantly
due to this poisoning, then it becomes easily detectable.
Therefore, for the effectiveness of this attack, it is important
that the quality of the model does not degrade4. We
experimentally conﬁrm that this is somewhat true with our
poisoning strategy. See Fig 4 for the F score for the model
Logistic Regression where the poisoning rate varies from 0%
to 20% for training set size of 1000. In general, the
experiments show that the precision tends to decrease with a
rather low slope and recall tend to increase by adding more
poison data. The drop of precision and rise of recall can be
explained by the fact that the poisoned model tends to predict
label 1 more often than the non-poisoned model, because the
4Note that we are only considering undetectability via black box access. In
a eyes-on setting where the training data can be looked at, it is possible to
have countermeasures that would detect poisoning by looking at the training
data
Fig. 5: Recall v.s. poison rate. We change the poison rate and capture
the recall of resulting models in different experiments. Note that we
did not use balanced label fractions in our experiments.
poisoned data we add all has label 1. However, it also worth
mentioning that for all experiments in Census data, 4-5%
poisoning is sufﬁcient to get attack accuracy more than 90%.
This means that, if one considers the drop in precision versus
the attack accuracy, the census data is not much worse than
enron. In our experiments, the size of the training set for
Census experiments is set to 1000 and for Enron experiments
the training set size is 2000.
APPENDIX E
DATASETS
We have run our experiments on the following datasets:
• Census: The primary dataset that we use for our
experiments is the US Census Income Dataset [10]. The
US Census Income Dataset contains census data
extracted from the 1994 and 1995 population surveys
conducted by the U.S. Census Bureau. This dataset
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:49 UTC from IEEE Xplore.  Restrictions apply. 
171136
0.000.020.040.060.080.100.120.140.160.180.20Poison Rate0.000.050.100.150.200.250.300.350.400.450.500.550.600.650.700.750.800.850.900.951.00F ScoreEnron Negative 5%Enron Random 30%census Random MLP 10%census Race Black 10%census Gender 40%contains 33717 emails. The classiﬁcation task here was
to classify an email as "Spam" or "Ham" based on the
words used in the email.
• Image Data Sets: We also MNIST [16] and CelebA
[18] image datasets to verify the success of our attack.
This dataset contains 70000 images labeled with the digit
each hand-writing represents. Each image is represented
using 28 × 28 gray-scale pixels. The classiﬁcation task is
then to predict the correct digit that each image is
representing.
CelebA dataset is photos of celebrity faces containing
200000 images. Each image is represented by 178 × 218
RGB pixels and is also annotated with other features
such as skin color and gender. The goal of classiﬁers we
train on these datasets is to predict whether a face is
smiling.
APPENDIX F
DETAILS OF DP EXPERIMENTS.
For training the differentially private logistic regression
models, we used the Pytorch implementation of DP-SGD [4].
We used training set size of 8000, and trained 500 shadow
models, the batch size was chosen to be 20 we trained the
models for 5 epochs. The clipping threshold was chosen to be
0.3 and δ (for differential privacy) was chosen to be 10−5.
We used a exponentially decaying learning rate with starting
rate of 3 and decay rate of 0.5 that was applied every epoch.
We used α ∈ [2, 3, . . . , 32] for the calculation of composition
of Renyi differential privacy.
Fig. 6: precision v.s. poison rate. Note that there is an imbalance in
the distribution of the labels in our experiments and this is why some
times the precision is smaller than 50%.
Fig. 7: Poison rate vs attack accuracy. This experiment shows that
more poisoning becomes ineffective after a certain point and actually
decreases the performance of the attack. The optimal poisoning ratio
could be different across different experiments. This drop of attack
accuracy was also observed in one of the experiments of Figure 1.
Note that adversary can optimize the poisoning ratio during the shadow
model training phase. Speciﬁcally, adversary can choose to use fewer
poisoning points than what it is allowed to, to ensure that the attack
is optimal. In this experiment, the number of shadow models is 400
and the training set size is 1000.
includes 299,285 records with 41 demographic and
employment related attributes such as race, gender,
education, occupation, marital status and citizenship. The
classiﬁcation task is to predict whether a person earns
over $50,000 a year based on the census attributes.5
• Enron: The second dataset that we use to validate our
attack is the Enron email dataset [15]. This dataset
5We used the census dataset as is, as compared to [12, 11] where they
preprocess the census dataset and run their experiments with balanced labels
(50% low income and 50% high income). We notice that in the original dataset,
the labels are not balanced (around 90% low income and 10% high income).
181137
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:49 UTC from IEEE Xplore.  Restrictions apply.