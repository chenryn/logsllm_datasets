### Introduction

This feature is particularly attractive for medical research, where large patient datasets are typically expensive to obtain. In this context, we consider two privacy mechanisms, A and A', that utilize the LocSig mechanism to release 2 Single Nucleotide Polymorphisms (SNPs). To compute the distance scores, we set a p-value threshold of \(10^{-10}\), ensuring that exactly 2 SNPs meet this criterion. As before, both mechanisms satisfy positive membership-privacy under the DB and D0.5 distributions.

In our specific example, the LocSig mechanism outperforms Algorithm 1, achieving similar utility for smaller datasets. For datasets of sizes \(N \in \{1500, 2000, 2500\}\) and Positive Membership Privacy (PMP) parameters \(\gamma \in \{1.3, 1.5\}\), we compare the utility of mechanisms A and A' and present the results in Figure 6.

### Utility Comparison

#### Mechanism Performance
- **Figure 7**: Utility of mechanisms A and A' when outputting M SNPs using LocSig [10] with \(\gamma = 1.5\).
  - (a) \(M = 1\)
  - (b) \(M = 3\)

There is a significant improvement in utility when considering a bounded adversarial model. Although the LocSig mechanism yields higher accuracy than the exponential method from Algorithm 1, it is important to note that computing the distance scores has a much higher computational complexity compared to the \(\chi^2\)-statistics [22]. Therefore, the choice of method in practice involves a trade-off between utility and computational cost.

#### Increasing M
Alternatively, we could increase the utility by releasing more than 2 SNPs (\(M > 2\)). However, since the exponential mechanisms associate probabilities proportional to \(M\) to each SNP, it is unclear whether increasing \(M\) will lead to higher utility. If \(M\) approaches the total number of SNPs, recall would be maximized, but precision (the ratio of output SNPs that are significant) would be naturally upper-bounded. In Figure 7, we evaluate the utility of LocSig with \(\gamma = 1.5\) for \(M = 1\) and \(M = 3\). We observe that for \(M = 3\), the utility is worse than for \(M = 2\), confirming that increased data perturbation eliminates the potential gain in recall. Additionally, the precision is naturally upper-bounded by \(\frac{2}{3}\). An interesting trade-off is given by selecting \(M = 1\). Although recall cannot exceed \(\frac{1}{2}\), for small datasets (\(N \leq 2000\)), the utility is actually higher than for \(M = 2\).

### Privacy-Utility Trade-off

Finally, we compare the privacy-utility trade-off for a range of bounds \([a, b]\) on the adversary’s prior belief. In Figure 8, we display the probability that Algorithm 1 outputs at least one or both causative SNPs in a Genome-Wide Association Study (GWAS) with \(N = 7500\), while providing PMP with \(\gamma = 1.5\). Even if the adversary has only a small degree of a priori uncertainty about an individual’s presence in the dataset, we still achieve a significant gain in utility compared to the setting where the adversary’s prior is unbounded.

### Bounded Adversarial Model

#### Definitions and Results
- **Definition 9 (Positive Unbounded-DP [14])**: A mechanism A satisfies \(\epsilon\)-positive unbounded-DP if and only if for any dataset T, any entity t not in T, and any \(S \subseteq \text{range}(A)\),
  \[
  \Pr[A(T \cup \{t\}) \in S] \leq e^\epsilon \cdot \Pr[A(T) \in S].
  \]

- **Lemma 4 ([14])**: If A satisfies \(\epsilon\)-positive unbounded DP, then for any \(D \in DI\), we have \(\Pr[S|t] / \Pr[S|\neg t] \leq e^\epsilon\).

- **Theorem 3 ([14])**: A mechanism A satisfies \(\epsilon\)-positive unbounded DP if and only if it satisfies \((e^\epsilon, DI)\)-PMP.

- **Definition 10 (Restricted MI Distributions)**: For \(0 < a \leq b < 1\), the family \(D_{[a,b]}^I\) contains all mutually independent (MI) distributions for which \(\Pr[t] \in [a, b] \cup \{0, 1\}\) for all entities t. If \(a = b\), we denote the family as \(D_a^I\).

- **Theorem 4**: A mechanism A satisfies \((\gamma, D_{[a,b]}^I)\)-PMP, for \(0 < a \leq b < 1\), if A satisfies \(\epsilon\)-positive unbounded-DP, where
  \[
  e^\epsilon = \min\left\{\left(\frac{1-a}{\gamma+b-1}\right)^{\frac{1-a\gamma}{b}}, \frac{\gamma+b-1}{b}\right\} \quad \text{if } a\gamma < 1, \text{ otherwise } e^\epsilon = \frac{\gamma+b-1}{b}.
  \]

### Conclusion and Future Work

We have investigated possible relaxations of the adversarial model of differential privacy, addressing recent concerns about its strength. By focusing on protecting against set membership disclosure, we provide a complete characterization of the relationship between DP and PMP for adversaries with limited prior knowledge. We argue that these weaker adversarial settings are practically significant and show that they can achieve significantly higher utility while protecting against such bounded adversaries.

We propose a simple model for selecting the DP parameter, which involves identifying a practically significant adversarial setting and an appropriate bound on the adversary’s posterior belief. We illustrate these points with a specific example on GWAS, demonstrating that privacy threats identified in the literature can be recast into our bounded adversarial model, leading to a better trade-off between privacy guarantees and medical utility. Evaluating the applicability of our model to other privacy domains and the corresponding utility gain is an interesting direction for future work.

### Discussion

For both exponential mechanisms, our results show that by focusing on an adversarial setting with bounded prior knowledge, we can attain the same PMP guarantees as for adversaries with arbitrary priors while retaining significantly higher utility. This is particularly relevant for attacks against GWAS, indicating that we can achieve a reasonable level of protection and acceptable medical utility for smaller, cheaper datasets.

Our results are not limited to GWAS or genomic privacy. They can be applied to other domains where DP has been proposed, such as location privacy [1] or data mining [7]. In scenarios where DP is used to protect membership disclosure, it is beneficial to consider whether the adversarial setting of DP is reasonable or whether a bound on the adversary’s prior belief is practically significant. Depending on the identified adversaries, an appropriate level of noise can be selected to guarantee PMP, according to the model derived in Section 3.

### References

[1] M. E. Andrés, N. E. Bordenabe, K. Chatzikokolakis, and C. Palamidessi. Geo-indistinguishability: Differential privacy for location-based systems. In Proceedings of the 2013 ACM SIGSAC Conference on Computer & Communications Security, CCS '13, pages 901–914, New York, NY, USA, 2013. ACM.

[2] R. Bassily, A. Groce, J. Katz, and A. Smith. Coupled-worlds privacy: Exploiting adversarial uncertainty in statistical data privacy. In Foundations of Computer Science (FOCS), 2013 IEEE 54th Annual Symposium on, pages 439–448. IEEE, 2013.

[3] R. Bhaskar, S. Laxman, A. Smith, and A. Thakurta. Discovering frequent patterns in sensitive data. In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 503–512. ACM, 2010.

[4] C. Dwork. Differential privacy. In Automata, languages and programming, pages 1–12. Springer, 2006.

[5] C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data analysis. In Proceedings of the Third Conference on Theory of Cryptography, TCC'06, pages 265–284, Berlin, Heidelberg, 2006. Springer-Verlag.

[6] M. Fredrikson, E. Lantz, S. Jha, S. Lin, D. Page, and T. Ristenpart. Privacy in pharmacogenetics: An end-to-end case study of personalized warfarin dosing. In 23rd USENIX Security Symposium (USENIX Security 14), pages 17–32, San Diego, CA, Aug. 2014. USENIX Association.

[7] A. Friedman and A. Schuster. Data mining with differential privacy. In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 493–502. ACM, 2010.

[8] J. Gehrke, M. Hay, E. Lui, and R. Pass. Crowd-blending privacy. In Advances in Cryptology–CRYPTO 2012, pages 479–496. Springer, 2012.

[9] N. Homer, S. Szelinger, M. Redman, D. Duggan, W. Tembe, J. Muehling, J. V. Pearson, D. A. Stephan, S. F. Nelson, and D. W. Craig. Resolving individuals contributing trace amounts of DNA to highly complex mixtures using high-density SNP genotyping microarrays. PLoS genetics, 4(8):e1000167, 2008.

[10] A. Johnson and V. Shmatikov. Privacy-preserving data exploration in genome-wide association studies. In Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '13, pages 1079–1087, New York, NY, USA, 2013. ACM.

[11] D. Kifer and A. Machanavajjhala. No free lunch in data privacy. In Proceedings of the 2011 ACM SIGMOD International Conference on Management of Data, SIGMOD '11, pages 193–204, New York, NY, USA, 2011. ACM.

[12] J. Lee and C. Clifton. Differential identifiability. In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '12, pages 1041–1049, New York, NY, USA, 2012. ACM.

[13] N. Li, W. Qardaji, and D. Su. On sampling, anonymization, and differential privacy or, k-anonymization meets differential privacy. In Proceedings of the 7th ACM Symposium on Information, Computer and Communications Security, ASIACCS '12, pages 32–33, New York, NY, USA, 2012. ACM.

[14] N. Li, W. Qardaji, D. Su, Y. Wu, and W. Yang. Membership privacy: a unifying framework for privacy definitions. In Proceedings of the 2013 ACM SIGSAC conference on Computer & communications security, CCS '13, pages 889–900, New York, NY, USA, 2013. ACM.

[15] E. Lui and R. Pass. Outlier privacy. In Y. Dodis and J. Nielsen, editors, Theory of Cryptography, volume 9015 of Lecture Notes in Computer Science, pages 277–305. Springer Berlin Heidelberg, 2015.

[16] A. Machanavajjhala, J. Gehrke, and M. Götz. Data publishing against realistic adversaries. Proc. VLDB Endow., 2(1):790–801, Aug. 2009.

[17] F. McSherry and K. Talwar. Mechanism design via differential privacy. In Foundations of Computer Science, 2007. FOCS'07. 48th Annual IEEE Symposium on, pages 94–103. IEEE, 2007.

[18] C. C. Spencer, Z. Su, P. Donnelly, and J. Marchini. Designing genome-wide association studies: sample size, power, imputation, and the choice of genotyping chip. PLoS genetics, 5(5):e1000477, 2009.

[19] C. Uhler, A. Slavkovic, and S. E. Fienberg. Privacy-preserving data sharing for genome-wide association studies. Journal of Privacy and Confidentiality, 5(1), 2013.

[20] R. Wang, Y. F. Li, X. Wang, H. Tang, and X. Zhou. Learning your identity and disease from research papers: Information leaks in genome wide association study. In Proceedings of the 16th ACM Conference on Computer and Communications Security, CCS '09, pages 534–544, New York, NY, USA, 2009. ACM.

[21] F. A. Wright, H. Huang, X. Guan, K. Gamiel, C. Jeffries, W. T. Barry, F. P.-M. de Villena, P. F. Sullivan, K. C. Wilhelmsen, and F. Zou. Simulating association studies: a data-based resampling method for candidate regions or whole genome scans. Bioinformatics, 23(19):2581–2588, 2007.

[22] F. Yu, S. E. Fienberg, A. B. Slavković, and C. Uhler. Scalable privacy-preserving data sharing methodology for genome-wide association studies. Journal of biomedical informatics, 2014.