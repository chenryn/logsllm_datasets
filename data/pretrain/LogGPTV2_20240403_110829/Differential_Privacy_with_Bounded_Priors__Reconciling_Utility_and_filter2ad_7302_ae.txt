This is as an attractive feature for medical research, where
large patient datasets are typically expensive to obtain.
We now consider two privacy mechanisms A and A(cid:48) that
use the LocSig mechanism to release 2 SNPs. To compute
the distance scores, we ﬁx a threshold of 10−10 on the p-
values, such that exactly 2 SNPs reach this threshold. As
before, the mechanisms satisfy positive membership-privacy
under DB and D0.5
In our particular exam-
ple, LocSig provides better results than Algorithm 1, and
we actually achieve similar utility for smaller datasets. For
datasets of sizes N ∈ {1500, 2000, 2500} and PMP parame-
ters γ ∈ {1.3, 1.5}, we compare the utility of A and A(cid:48), and
we display the results in Figure 6.
B , respectively.
(a) M = 1
(b) M = 3
Figure 7: Utility of mechanisms A and A(cid:48), when outputting
M SNPs using LocSig [10] with γ = 1.5.
Again, there is a signiﬁcant improvement in utility if we
consider a bounded adversarial model. Although the Loc-
Sig mechanism yields higher accuracy than the exponential
method from Algorithm 1 in this case, we re-emphasize that
computing the distance scores has a much higher complex-
ity than the computation of the χ2-statistics [22]. Deciding
upon which method to use in practice is thus subject to a
tradeoﬀ between utility and computational cost.
Alternatively, we could consider increasing our utility by
releasing M > 2 SNPs. However, as the exponential mecha-
nisms we considered associate probabilities proportional to
M to each SNP, it is unclear whether we should expect
higher utility by increasing M . Obviously, if we were to let
M approach the total number of SNPs, the recall would be
maximized. Hence, we also consider the precision (ratio of
output SNPs that are signiﬁcant). In Figure 7, we evaluate
the utility of LocSig with γ = 1.5, for M = 1 and M = 3.
We see that for M = 3, the utility is worse than for M = 2,
therefore conﬁrming that the increased data perturbation
eliminates the potential gain in recall. Also, in this case the
precision is naturally upper bounded by 2
3 . An interesting
tradeoﬀ is given by selecting M = 1. Although recall can
2 , we see that for small datasets (N ≤ 2000),
not exceed 1
the utility actually is higher than for M = 2.
Finally, we compare the privacy-utility tradeoﬀ for a range
of bounds [a, b] on the adversary’s prior belief. In Figure 8,
we display the probability that Algorithm 1 outputs at least
one or both of the causative SNPs in a GWAS with N =
7500, while providing PMP with γ = 1.5. As we can see,
even if the considered adversary has only a small degree of
a priori uncertainty about an individual’s presence in the
dataset, we still obtain a signiﬁcant gain in utility compared
to the setting where the adversary’s prior is unbounded.
5000 7500 1000000.250.50.751SampleSizeProbability  Areturned1associatedSNPAreturnedbothassociatedSNPsA′returned1associatedSNPA′returnedbothassociatedSNPs5000 7500 1000000.250.50.751SampleSizeProbability  Areturned1associatedSNPAreturnedbothassociatedSNPsA′returned1associatedSNPA′returnedbothassociatedSNPs15002000250000.250.50.751SampleSizeProbability  Areturned1associatedSNPAreturnedbothassociatedSNPsA′returned1associatedSNPA′returnedbothassociatedSNPs15002000250000.250.50.751SampleSizeProbability  Areturned1associatedSNPAreturnedbothassociatedSNPsA′returned1associatedSNPA′returnedbothassociatedSNPs15002000250000.250.50.751SampleSizeProbability  Areturned1associatedSNPA′returned1associatedSNP15002000250000.250.50.751SampleSizeProbability  Areturned1associatedSNPAreturnedbothassociatedSNPsA′returned1associatedSNPA′returnedbothassociatedSNPsIn this deﬁnition, we consider only neighboring datasets
obtained by adding a new entity (and not by removing an
entity). Note that satisfying -unbounded DP trivially im-
plies -positive unbounded-DP.
For this deﬁnition, the results we obtained for bounded-
DP can be applied rather straightforwardly to (positive)
unbounded-DP. Li et al. [14] provide results analogous to
Lemma 2 and Theorem 1, by replacing the family DB, by
the family DI of mutually-independent distributions.
Lemma 4 ([14]). If A satisﬁes -positive unbounded DP,
then for any D ∈ DI we have Pr[S|t]
Theorem 3 ([14]). A mechanism A satisﬁes -positive un-
bounded DP if and only if it satisﬁes (e, DI )-PMP.
Pr[S|¬t] ≤ e .
From here on, our analysis from Section 3 can be directly
applied to the case of unbounded-DP. We ﬁrst deﬁne a family
of bounded prior distributions.
Deﬁnition 10 (Restricted MI Distributions). For 0 < a ≤
⊂ DI contains all MI distributions
b < 1, the family D[a,b]
for which Pr[t] ∈ [a, b] ∪ {0, 1}, for all entities t. If a = b,
we simply denote the family as Da
I .
I
Finally, we obtain an analogous result to Theorem 2, by
characterizing the level of (positive) unbounded-DP that
guarantees a level γ of PMP under a restricted MI distri-
bution family.
Theorem 4. A mechanism A satisﬁes (γ, D[a,b]
)-PMP, for
0 < a ≤ b < 1, if A satisﬁes -positive unbounded-DP, where
I
(cid:40)
(cid:16) (1−a)γ
(cid:17)
1−aγ , γ+b−1
b
e =
min
γ+b−1
b
if aγ < 1,
otherwise .
6. CONCLUSION AND FUTURE WORK
We have investigated possible relaxations of the adver-
sarial model of diﬀerential privacy, the strength of which
has been questioned by recent works. By considering the
problem of protecting against set membership disclosure,
we have provided a complete characterization of the rela-
tionship between DP and PMP for adversaries with limited
prior knowledge. We have argued about the practical signif-
icance of these weaker adversarial settings and have shown
that we can achieve a signiﬁcantly higher utility when pro-
tecting against such bounded adversaries.
We have proposed a simple model for the selection of the
DP parameter, that consists in identifying a practically sig-
niﬁcant adversarial setting, as well as an appropriate bound
on an adversary’s posterior belief. We have illustrated these
points with a speciﬁc example on genome-wide association
studies and have shown that privacy threats identiﬁed in the
literature can be re-cast into our bounded adversarial model,
which leads to a better tradeoﬀ between privacy guaran-
tees and medical utility. Evaluating the applicability of our
model to other privacy domains, as well as the corresponding
utility gain, is an interesting direction for future work.
Our results from Theorems 1 and 4 show that when we
consider an adversary with limited prior knowledge, satisfy-
ing DP provides a suﬃcient condition for satisfying PMP.
An interesting direction for future work is to investigate
whether PMP under distribution families D[a,b]
and D[a,b]
B
I
Figure 8: Probability that Algorithm 1 outputs both, or at
least one of the causative SNPs, when guaranteeing PMP
with γ = 1.5 against adversaries with prior D[a,b]
B .
Discussion.
For both of the exponential mechanisms we considered,
our results show that by focusing on an adversarial setting
with bounded prior knowledge, we can attain the same PMP
guarantees as for adversaries with arbitrary priors and re-
tain a signiﬁcantly higher utility. As we argued that the
adversarial model with priors in D0.5
B is relevant in regard
to attacks against GWAS, this shows that we can achieve a
reasonable level of protection against these attacks and also
guarantee an acceptable level of medical utility for datasets
smaller (and thus cheaper) than previously reported.
We stress that the applicability of our results need not be
limited to GWAS or even to genomic privacy in general. In-
deed, we could consider applications in other domains where
DP has been proposed as a privacy notion, as a bounded
adversarial setting makes sense in many practical scenarios.
As we will see in Section 5, our results can also be adapted
to cover the case of unbounded-DP, thus further extending
their applicability to other use-cases of diﬀerential privacy.
Examples of settings where DP mechanisms have been pro-
posed, and yet an adversary with incomplete background
knowledge appears reasonable, can be found in location pri-
vacy [1] or data mining [7] for instance.
In scenarios where DP is applied to protect membership
disclosure, we would beneﬁt from considering whether the
adversarial setting of DP is reasonable, or whether a bound
on an adversary’s prior belief is practically signiﬁcant. De-
pending on the identiﬁed adversaries, we can select an ap-
propriate level of noise to guarantee PMP, according to the
model derived in Section 3.
5. THE CASE OF UNBOUNDED-DP
The characterization of unbounded-DP in the PMP frame-
work is a little more subtle than for bounded-DP. Li et al.
introduce a uni-directional deﬁnition of unbounded-DP.
Deﬁnition 9 (Positive Unbounded-DP [14]). A mechanism
A satisﬁes -positive unbounded-DP if and only if for any
dataset T , any entity t not in T , and any S ⊆ range(A),
Pr [A(T ∪ {t}) ∈ S] ≤ e · Pr [A(T ) ∈ S] .
(14)
00.20.40.60.81Probability  [0.0,1.0][0.1,0.9][0.2,0.8][0.3,0.7][0.4,0.6][0.5,0.5][a,b]AtleastoneassociatedSNPisoutputBothassociatedSNPsareoutputcan be attained by other means than through DP. For in-
stance, in their work on membership privacy, Li et al. pro-
pose a simple mechanism for outputting the maximum of a
set of values, that satisﬁes PMP for the family D0.5
I but does
not satisfy any level of DP [14]. It is unknown whether sim-
ilar mechanisms could be designed for other queries (such
as those we considered in our GWAS scenario), in order to
potentially improve upon the privacy-utility tradeoﬀ of DP.
Acknowledgments
We thank Mathias Humbert and Huang Lin for helpful com-
ments.
7. REFERENCES
[1] M. E. Andr´es, N. E. Bordenabe, K. Chatzikokolakis,
and C. Palamidessi. Geo-indistinguishability:
Diﬀerential privacy for location-based systems. In
Proceedings of the 2013 ACM SIGSAC Conference on
Computer & Communications Security, CCS ’13,
pages 901–914, New York, NY, USA, 2013. ACM.
[2] R. Bassily, A. Groce, J. Katz, and A. Smith.
Coupled-worlds privacy: Exploiting adversarial
uncertainty in statistical data privacy. In Foundations
of Computer Science (FOCS), 2013 IEEE 54th
Annual Symposium on, pages 439–448. IEEE, 2013.
[3] R. Bhaskar, S. Laxman, A. Smith, and A. Thakurta.
Discovering frequent patterns in sensitive data. In
Proceedings of the 16th ACM SIGKDD international
conference on Knowledge discovery and data mining,
pages 503–512. ACM, 2010.
[4] C. Dwork. Diﬀerential privacy. In Automata, languages
and programming, pages 1–12. Springer, 2006.
[5] C. Dwork, F. McSherry, K. Nissim, and A. Smith.
Calibrating noise to sensitivity in private data
analysis. In Proceedings of the Third Conference on
Theory of Cryptography, TCC’06, pages 265–284,
Berlin, Heidelberg, 2006. Springer-Verlag.
[6] M. Fredrikson, E. Lantz, S. Jha, S. Lin, D. Page, and
T. Ristenpart. Privacy in pharmacogenetics: An
end-to-end case study of personalized warfarin dosing.
In 23rd USENIX Security Symposium (USENIX
Security 14), pages 17–32, San Diego, CA, Aug. 2014.
USENIX Association.
[7] A. Friedman and A. Schuster. Data mining with
diﬀerential privacy. In Proceedings of the 16th ACM
SIGKDD international conference on Knowledge
discovery and data mining, pages 493–502. ACM,
2010.
[8] J. Gehrke, M. Hay, E. Lui, and R. Pass.
Crowd-blending privacy. In Advances in
Cryptology–CRYPTO 2012, pages 479–496. Springer,
2012.
[9] N. Homer, S. Szelinger, M. Redman, D. Duggan,
W. Tembe, J. Muehling, J. V. Pearson, D. A. Stephan,
S. F. Nelson, and D. W. Craig. Resolving individuals
contributing trace amounts of dna to highly complex
mixtures using high-density snp genotyping
microarrays. PLoS genetics, 4(8):e1000167, 2008.
[10] A. Johnson and V. Shmatikov. Privacy-preserving
data exploration in genome-wide association studies.
In Proceedings of the 19th ACM SIGKDD
International Conference on Knowledge Discovery and
Data Mining, KDD ’13, pages 1079–1087, New York,
NY, USA, 2013. ACM.
[11] D. Kifer and A. Machanavajjhala. No free lunch in
data privacy. In Proceedings of the 2011 ACM
SIGMOD International Conference on Management of
Data, SIGMOD ’11, pages 193–204, New York, NY,
USA, 2011. ACM.
[12] J. Lee and C. Clifton. Diﬀerential identiﬁability. In
Proceedings of the 18th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining,
KDD ’12, pages 1041–1049, New York, NY, USA,
2012. ACM.
[13] N. Li, W. Qardaji, and D. Su. On sampling,
anonymization, and diﬀerential privacy or,
k-anonymization meets diﬀerential privacy. In
Proceedings of the 7th ACM Symposium on
Information, Computer and Communications Security,
ASIACCS ’12, pages 32–33, New York, NY, USA,
2012. ACM.
[14] N. Li, W. Qardaji, D. Su, Y. Wu, and W. Yang.
Membership privacy: a unifying framework for privacy
deﬁnitions. In Proceedings of the 2013 ACM SIGSAC
conference on Computer & communications security,
CCS ’13, pages 889–900, New York, NY, USA, 2013.
ACM.
[15] E. Lui and R. Pass. Outlier privacy. In Y. Dodis and
J. Nielsen, editors, Theory of Cryptography, volume
9015 of Lecture Notes in Computer Science, pages
277–305. Springer Berlin Heidelberg, 2015.
[16] A. Machanavajjhala, J. Gehrke, and M. G¨otz. Data
publishing against realistic adversaries. Proc. VLDB
Endow., 2(1):790–801, Aug. 2009.
[17] F. McSherry and K. Talwar. Mechanism design via
diﬀerential privacy. In Foundations of Computer
Science, 2007. FOCS’07. 48th Annual IEEE
Symposium on, pages 94–103. IEEE, 2007.
[18] C. C. Spencer, Z. Su, P. Donnelly, and J. Marchini.
Designing genome-wide association studies: sample
size, power, imputation, and the choice of genotyping
chip. PLoS genetics, 5(5):e1000477, 2009.
[19] C. Uhler, A. Slavkovic, and S. E. Fienberg.
Privacy-preserving data sharing for genome-wide
association studies. Journal of Privacy and
Conﬁdentiality, 5(1), 2013.
[20] R. Wang, Y. F. Li, X. Wang, H. Tang, and X. Zhou.
Learning your identity and disease from research
papers: Information leaks in genome wide association
study. In Proceedings of the 16th ACM Conference on
Computer and Communications Security, CCS ’09,
pages 534–544, New York, NY, USA, 2009. ACM.
[21] F. A. Wright, H. Huang, X. Guan, K. Gamiel,
C. Jeﬀries, W. T. Barry, F. P.-M. de Villena, P. F.
Sullivan, K. C. Wilhelmsen, and F. Zou. Simulating
association studies: a data-based resampling method
for candidate regions or whole genome scans.
Bioinformatics, 23(19):2581–2588, 2007.
[22] F. Yu, S. E. Fienberg, A. B. Slavkovi´c, and C. Uhler.
Scalable privacy-preserving data sharing methodology
for genome-wide association studies. Journal of
biomedical informatics, 2014.