simulator. This ﬂuid model captures the overheads gener-
ated by each ﬂow and the coarse-grained behavior of ﬂows
in the network. The simulator is event-based, and whenever
a ﬂow is started, ended, or re-routed, the rate of all ﬂows is
recomputed using the algorithm shown in Algorithm 1. This
algorithm works by assigning a rate to ﬂows traversing the
most-congested port, and then iterating to the next most-
congested port until all ﬂows have been assigned a rate.
We represent the network topology with a capacitated, di-
rected graph. For these simulations, we used two topologies:
a three-level Clos topology [15] and a two-dimensional Hy-
perX topology [4]. In both topologies, all links were 1Gbps,
and 20 servers were attached to each access switch. The Clos
topology has 80 access switches (each with 8 uplinks), 80 ag-
gregation switches, and 8 core switches. The HyperX topol-
ogy is two-dimensional and forms a 9× 9 grid, and so has 81
access switches, each attached to 16 other switches. Since
the Clos network has 8 core switches, it is 1:2.5 oversub-
scribed; that is, its bisection bandwidth is 640 Gbps. band-
width. The HyperX topology is 1:4 oversubscribed and thus
has 405 Gbps of bisection bandwidth.
The Clos network has 1600 servers and the HyperX net-
work has 1620. We sized our networks this way for two rea-
sons: ﬁrst, so that the Clos and HyperX networks would have
nearly the same number of servers. Second, our workload is
based on the measurements of Kandula et al. [30], which are
from a cluster of 1500 servers. We are not sure how to scale
their measurements up to much larger data centers, so we
kept the number of servers close to the number measured in
their study.
We simulate the behavior of OpenFlow at switches by
modeling (1) switch ﬂow tables, and (2) the limited data-
plane to control-plane bandwidth. Switch ﬂow tables can
contain both exact-match and wildcard table entries. For all
simulations, table entries expire after 10 seconds. When a
ﬂow arrives that does not much a table entry, the header
of its ﬁrst packet is placed in the switch’s data-plane to
control-plane queue. The service rate for this queue follows
our measurements described in Section 3.2.1, so it services
packets at 17Mbps. This queue has ﬁnite length, and when
it is full, any arriving ﬂow that does not match a table entry
is dropped. We experimented with diﬀerent lengths for this
queue, and we found that when it holds 1000 packets, no
ﬂow setups were dropped. When we set its limit to 100, we
found that fewer than 0.01% of ﬂow setups were dropped in
the worst case. For all results shown in this paper, we set the
length of this queue to 100; we restart rejected ﬂows after a
simulated TCP timeout of 300 ms.
Finally, because we are interested in modeling switch over-
heads, we do not simulate a bottleneck at the OpenFlow
controller; the simulated OpenFlow controller processes all
ﬂows instantly. Also, whenever the OpenFlow controller re-
routes a ﬂow, it installs the ﬂow-table entries without any
latency.
5.1.1 Workloads
We consider two workloads in our simulations: (1) a
MapReduce job that has just gone into its shuﬄe stage, and
(2) a workload based on measurements, by Kandula et al.
at Microsoft Research (MSR) [30], of a 1500-server cluster.
The MapReduce-style traﬃc is modeled by randomly se-
lecting n servers to be part of the reduce-phase shuﬄe. Each
of these servers transfers 128 MB to each other server, by
maintaining connections to k other servers at once. Each
server randomizes the order it connects to the other servers,
keeping k connections open until it has sent its payload. All
measurements we present for this shuﬄe workload are for a
one-minute period that starts 10 sec. after the shuﬄe begins.
In our MSR workload, we generated ﬂows based on the dis-
tributions of ﬂow inter-arrival times and ﬂow sizes in [30]. We
attempted to reverse-engineer their actual workload from
only two distributions in their paper. In particular, we did
not model dependence between sets of servers. We pick the
destination of a ﬂow by ﬁrst determining whether the ﬂow is
to be an inter- or intra-rack ﬂow, and then selecting a des-
tination uniformly at random between the possible servers.
For these simulations, we generated ﬂows for four minutes,
and present measurements from the last minute.
Additionally, we simulated a workload that combines the
MSR and shuﬄe workloads, by generating ﬂows according
to both workloads simultaneously. We generated three min-
utes of MSR ﬂows before starting the shuﬄe. We present
measurements for the ﬁrst minute after the shuﬄe began.
Schedulers
5.1.2
We compare static routing with ECMP to ﬂow scheduling
with several schedulers.
Figure 3: Throughput achieved by the schedulers for the shuﬄe
workload with n = 800 and k = 5. OpenFlow-imposed overheads
are not modeled in these simulations. All error bars in this paper
show 95% conﬁdence intervals for 10 runs.
The DevoFlow scheduler: behaves as described in Sec. 4,
and collects statistics using either sampling or threshold
triggers on multipath wildcard rules. The scheduler might
re-reroute a ﬂow after it has classiﬁed the ﬂow as an ele-
phant. New ﬂows, before they become elephant ﬂows, are
routed using ECMP regardless of the mechanism to detect
elephant ﬂows. When the controller discovers an elephant
ﬂow, it installs ﬂow-table entries at the switches on the least-
congested path between the ﬂow’s endpoints. We model
queueing of a ﬂow between the data-plane and control-plane
before it reaches the controller; however, we assume instan-
taneous computation at the controller and ﬂow-table instal-
lations.
For elephant detection, we evaluate both sampling and
triggers.
Our ﬂow-level simulation does not simulate actual packets,
which makes modeling of packet sampling non-trivial. In our
approach:
1. We estimate the distribution of packets sent by a ﬂow
before it can be classiﬁed, with less than a 10% false-
positive rate, as an elephant ﬂow, using the approach
described by Mori et al. [38].
2. Once a ﬂow begins, we use that distribution to select how
many packets it will transfer before being classiﬁed as an
elephant; we assume that all packets are 1500 bytes. We
then create an event to report the ﬂow to the controller
once it has transferred this number of packets.
Finally, we assume that the switch bundles 25 packet headers
into a single report packet before sending the samples to
the controller; this reduces the packet traﬃc without adding
signiﬁcant delay. Bundling packets this way adds latency to
the arrival of samples at the controller. For our simulations,
we did not impose a time-out this delay. We bundled samples
from all ports on a switch, so when a 1 Gbps port is the only
active port (and assuming it’s fully loaded), this bundling
could add up to 16 sec. of delay until a sample reaches the
controller, when the sample rate is 1/1000 packets.
Fine-grained control using statistics pulling: simulates us-
ing OpenFlow in active mode. Every ﬂow is set up at the
central controller and the controller regularly pulls statistics,
which it uses to schedule ﬂows so as to maximize throughput.
As with the DevoFlow scheduler, we route elephant ﬂows us-
ing Correa and Goeman’s bin-packing algorithm [16]. Here,
we use Hedera’s deﬁnition of an elephant ﬂow: one with a
demand is at least 10% of the NIC rate [5]. The rate of each
ﬂow is found using Algorithm 1 on an ideal network; that is,
each access switch has an inﬁnite-capacity uplink to a sin-
0 100 200 300 400 500 600 ECMP  0.1s 1s 10s VLB Distributed  0.1s 1s 5s 10s 1/100 1/1000 1/10000 128KB 1MB 10MB Wildcard Pull-based Sampling Threshold Aggregate Throughput (Gbps) HyperX Clos Figure 4: Aggregate throughput of the schedulers on the Clos network for diﬀerent workloads. For the MSR plus shuﬄe workloads, 75%
of the MSR workload-generated ﬂows are inter-rack.
Figure 5: Aggregate throughput of the schedulers on the HyperX network for diﬀerent workloads.
gle, non-blocking core switch. This allows us to estimate the
demand of each ﬂow when ﬂow rates are constrained only
by server NICs and not by the switching fabric.
Following the OpenFlow standard, each ﬂow table entry
provides 88 bytes of statistics [2]. We collect statistics only
from the access switches. The ASIC transfers statistics to the
controller at 17 Mbps, via 1500-byte packets. The controller
applies the bin-packing algorithm immediately upon receiv-
ing a statistics report, and instantaneous installs a globally
optimized routing for all ﬂows.
Wildcard routing: performs multipath load balancing pos-
sible using only wildcard table entries. This controller re-
actively installs wildcard rules to create a unique spanning
tree per destination: all ﬂows destined to a server are routed
along a spanning tree. When a ﬂow is set up, the controller
computes the least-congested path from the switch that reg-
istered the ﬂow to the ﬂow’s destination’s spanning tree,
and installs the rules along this path. We simulated wildcard
routing only on the Clos topology, because we are still de-
veloping the spanning tree algorithm for HyperX networks.
Valiant load balancing (VLB): balances traﬃc by routing
each ﬂow through an intermediate switch chosen uniformly
at random; that switch then routes the ﬂow on the short-
est path to its destination [48]. On a Clos topology, ECMP
implements VLB.
Distributed greedy routing: routes each ﬂow by ﬁrst greed-
ily selecting the least-congested next-hop from the access
switch, and then using shortest-path routing. We simulate
this distributed routing scheme only on HyperX networks.
5.2 Performance
We begin by assessing the performance of the schedulers,
using the aggregate throughput of all ﬂows in the network as
our metric. Figure 3 shows the performance of the schedulers
under various settings, on a shuﬄe workload with n = 800
servers and k = 5 simultaneous connections/server. This
simulation did not model the OpenFlow-imposed overheads;
for example, the 100ms pull-based scheduler obtains all ﬂow
statistics every 100ms, regardless of the switch load.
We see that DevoFlow can improve throughput compared
to ECMP by up to 32% on the Clos network and up to
55% on the HyperX network. The scheduler with the best
performance on both networks is the pull-based scheduler
when it re-routes ﬂows every 100 ms. This is not entirely
surprising, since this scheduler also has the highest overhead.
Interestingly, VLB did not perform any better than ECMP
on the HyperX network.
To study the eﬀect of the workload on these results, we
tried several values for n and k in the shuﬄe workload and
we varied the fraction of traﬃc that remained within a rack
on the MSR workload. These results are shown in Figure 4
for the Clos topology and Figure 5 for the HyperX network.
Overall, we found that ﬂow scheduling improves throughput
for the shuﬄe workloads, even when the network has far
more bisection bandwidth than the job demands.
For instance, with n = 200 servers, the maximum demand
is 200 Gbps. Even though the Clos network has 640 Gbps
of bisection bandwidth, we ﬁnd that DevoFlow can increase
performance of this shuﬄe by 29% over ECMP. We also
observe that there was little diﬀerence in performance when
we varied k.
Flow scheduling did not improve the throughput of the
MSR workload. For this workload, regardless of the mix of
inter- and intra-rack traﬃc, we found that ECMP achieves
90% of the optimal throughput3 for this workload, so there
is little room for improvement by scheduling ﬂows. We sus-
pect that a better model than our reverse-engineered distri-
butions of the MSR workload would yield diﬀerent results.
Because of this limitation, we simulated a combination of
the MSR workload with a shuﬄe job. Here, we see improve-
ments in throughput due to ﬂow scheduling; however, the
gains are less than when the shuﬄe job is ran in isolation.
3We found the optimal throughput by attaching all servers to
a single non-blocking switch.
0 100 200 300 400 500 600 k = 1 k = 5 k = 10 k = 1 k = 5 k = 10 k = 1 k = 5 k = 10 MSR, 25% inter-rack MSR, 75% inter-rack k = 5 k = 5 k = 5  shuﬄe, n=200  shuﬄe, n=400  shuﬄe, n=800 MSR + shuﬄe, n = 200 MSR + shuﬄe, n = 400 MSR + shuﬄe, n = 800 Aggregate Throughput (Gbps) ECMP  Wildcard 1s  Pull-based 5s Sampling 1/1000 Threshold 1MB  0 100 200 300 400 500 600 k = 5 k = 10 k = 5 k = 10 k = 5 k = 10 MSR, 25% inter-rack MSR, 75% inter-rack k = 5 k = 10 k = 5 k = 10 k = 5 k = 10  shuﬄe, n=200  shuﬄe, n=400  shuﬄe, n=800 MSR 75% inter-rack + shuﬄe,         n = 200 MSR 75% inter-rack + shuﬄe,         n= 400 MSR 75% inter-rack + shuﬄe,         n= 800 Aggregate Throughput (Gbps) ECMP  VLB Distributed  Pull-based 5s Sampling 1/1000 Threshold 1MB  Figure 6: The number of packet arrivals per second at the con-
troller using the diﬀerent schedulers on the MSR workload.
Figure 7: The average and maximum number of ﬂow table entries
at an access switch for the schedulers using the MSR workload.
5.3 Overheads
We used the MSR workload to evaluate the overhead of
each approach because, even though we do not model the
dependence between servers, we believe it gives a good indi-
cation of the rate of ﬂow initiation. Figure 6 shows, for each
scheduler, the rate of packets sent to the controller while
simulating the MSR workload.
Load at the controller should scale proportionally to the
number of servers in the data center. Therefore, when using
an OpenFlow-style pull-based scheduler that collects stats
every 100ms, in a large data center with 160K servers, we
would expect a load of about 2.9M packets/sec., based on
extrapolation from Figure 6. This would drop to 775K pack-
ets/sec. if stats are pulled once per second. We are not aware
of any OpenFlow controller that can handle this message
rate; for example, NOX can process 30K ﬂow setups per
second [45]. A distributed controller might be able to han-