面结果意味着没有价值。
的，因为他很容易被理解为试验“失败了”。有一些试验根本就不可能成功，而发现他
结果的时候，其他人不需要再重新设计和运行一套类似试验。不公布负面结果是很常见
公布结果。如果你对一项测试的结果感兴趣，那么很有可能其他人也感兴趣。当你公布
性的提升。
SRE已经通过高质量的事后报告体会到了这一点，这些报告极大地促进了生产环境稳定
的榜样。公布所有数据鼓励了其他人也这么做，从而使得整个行业的学习速度变快了。
他非统计显著的结果有助于降低数据的偏差度，同时为其他人做出了接受未知可能性
公布负面结果有助于提升整个行业的数据驱动风气。在我们的数据中记录负面结果和其
可能会。将这些测试写成脚本有助于保证在下一个项目中你不会忘记和错过好的优化点。
并不会因为将数据库运行在SSD上，或者建立更好的索引而更快。但是下一个应用程序
为了可重复地试验而构建的工具可能还存在间接好处：虽然目前构建的一个应用程序可能
Bench，一个负载测试软件中获利。虽然第一个测试结果很可能是不尽如人意的。
生成器从成功或失败的试验过程中都可以产生。很多Web服务器管理人员都从Apache
工具和方法可能超越目前的试验，为未来的工作提供帮助。例如，性能测试工具和负载
126
理想状况下，
们的最好方法就是通过评审。更多的试验压根没有公布数据，因为很多人错误地认为负
·在生产环境中重现某个问题也许是不可能的。要么因为将系统置于某种问题状态
。
系统可能好一些，但是需要额外付出一定的成本。
过于复杂，或者是系统过于重要，不能再出问题。
路径依赖的，也就是说，它们必须处于一个特定状态下问题才会发生。
系统复杂度。
第12章
可参看文献[Coo04]和[Dek14]。
，现在你已经将一系列可能的错误原因减少到了一个。下一步，我们想要证
有效的故障排查手段
。很有可能有多个因素共同作用导致系统问题。注1真实系统通常是
，关于寻找单一故障源而忽视系统及其运行环境的方法的
一包括未来的你不会再次感到意外。
，如果有一套非生产环境的复制
---
## Page 169
图12-4：应用程序延迟图，显示出50%、95%、99%请求的延迟（以线表示），同时用热力图显
时间（参见图12-5）和服务进程的数量（参见图12-6）几乎成四次方增长。很明显某些
我们的调查显示，该程序延迟的确上升了一个数量级，如图12-4所示。与此同时，CPU
图12-3：应用程序每秒接收到的请求，显示出一个短暂的峰值，接下来恢复正常。
见图12-3），所以他们想知道这是否是App Engine服务本身造成的。
到最近任何的程序改动会导致资源使用的增多，同时该App也没有用户流量的增长（参
增长。他们的服务是一个给开发者使用的内容管理系统（CMS）。
一份问题报告，声称他们最近观察到了系统延迟、CPU使用率以及运行进程的数量大幅
可以利用这个产品用Google的基础架构设施构建自己的服务。有一个内部客户提交了
案例分析
已经恢复。）
候系统是活着的！
的，和你是如何修复问题的，如何防止再次发生等写下来。这就是事后总结。希望这时
当你最终确定了某个因素是问题根源时，应该将系统中出错的部分，你是如何定位问题
注注
东西有问题，现在是故障排查的时间了！
QPS
100
Latency(Logarithmicheatmap)
Queries/s
我们压缩和简化了这个案例，便于读者理解。
参见https://cloud.google.com/appengine
示了在指定时间内请求的分布情况。
18:00
18:00
18:30
18:30
（事后总结也称为验尸报告，但是这里是在问题解决后才写的，服务
19:0019:3020:0020:3021:0021:3022:0022:3023:0023:30
19:0019:3020:00
20:30
21:00
21:3022:00
。注17该内部客户没有找
22:3023:00
案例分析
-p50-p95p99
23:30
127
---
## Page 170
128
在调查过程中，我们同时发现一些针对静态资源的访问，例如图片，并非通过数据服务
找到了需要添加索引的属性，最后增加了相关的索引。
跟踪每个相关服务器发出的RPC。通过这些信息，在发往数据服务的RPC请求中最终
HTTP请求的具体步骤。从前端的接收、反向代理到App代码返回一个结果，我们可以
这时，我们拿出了工具集中的重型工具：Dapper（参见文献[Sigl0]），它可以跟踪单个
属性需要这个索引。快速浏览了一下App的代码也没有提供明显的疑点。
索引可以加速这些查询，理论上来说，可以加速整个ApP。但是我们需要找到具体哪个
了在从数据库中读取数据时出现了索引间题。为这个App使用的数据属性增加一个综合
了延迟上升和某个具体数据存储API的用量（merge_join）增多的相关性，这通常代表
务基础架构中的某些特质。开发团队也无法找到任何可疑点。然而，有一个开发者发现
我们将这个问题汇报给了App Engine的开发团队，让他们协助调查是否该客户遇到了服
似的情况。然而，没有其他App出现类似情况。
这个问题是由服务产生的，我们预计将会在其他App中（运行在同样的架构下）看到类
更在执行。App Engine服务最近的代码改动和配置改动在数天前已经完成。再次，如果
久。第二，App 性能的改变发生在星期六，当时没有App 的改动，也没有生产环境的变
平。这次峰值显然不应该从App开发者提交错误报告和我们开始调查时算起持续数天之
可以解释资源的临时增加，但是我们认为随着流量恢复，资源使用率也应该回到正常水
然而，我们可以轻易地将这两个因素排除在外：虽然App在20：45接收到的峰值流量
通常来说，
图12-6：应用程序运行的进程数量
图12-5：应用程序CPU使用率汇总
Instances
CPU Usage
MCycle/s
18:00
18:00
第12章有效的故障排查手段
，延迟和资源使用的突然增加意味着流量的突然增加，或者系统配置的改变。
18:30
18:30
19:00
19:00
19:30
19:30
20:0020:3021:0021:30
20:00
20:3021:0021:30
22:0022:3023:0023:30
22:0022:3023:00
activeidle-loading
23:30
---
## Page 171
注19
开上线之前，
当一个路径被访问时，一个新的白名单物件就被创建出来，同时保存在数据库中。在公
名单中的物件存放于内存中。正如一个App开发者在调查过程中记录的那样：“虽然我
App开发者在代码中增加了一些标记，帮助理解App的具体执行时间。他们找到了每个
们无法证明这就是问题根源，但是这是一个常见的反模式。
而只是做内存检查。然而，处理每个请求的时间经常与配置文件的数量同步增长。注！我
为模式的确很像一种常见模式：App的一个实例使用从数据服务读取的数据初始化，然
为这个增加幅度并不是太大，同时也不持久，我们很早就认为这只是偶然。但是这种行
型的改变。在App 延迟改变之前，我们看到了App 对数据服务的写请求有增加。但是因
在这时，我们怀疑这个App是另外一个延迟和资源使用变化因素的受害者：工作负载类
的App成功上线，而我们可以放松地调查问题。注18
低到可接受的范围，虽然不是最佳情况。我们认为这样的方法已经足够好，可以让用户
经计划了公开上线，而我们并不知道多长时间才能找到和修复这个问题。我们建议用户
基于目前得到的信息，问题仍然是一个谜，我们决定暂时不解决它。因为用户下下周已
它只能跟踪RPC请求，而这段时间App没有发送任何RPC。
代码，所以我们不知道这段时间App在做什么。同样，Dapper也不能帮助我们，因为
某件未知的事。因为App Engine 运行的是用户提供的代码，SRE团队并不会检查这些
出在这里。然而，在App接收到请求到发出第一个RPC时，有250ms的时间App在做
（memcache），所以请求应该非常快，在毫秒级。这些请求的确非常快，所以问题并不是
通过对静态资源请求缓慢的检查发现，从应用程序发出的大部分RPC是到内存缓存服务
成延迟上升的理论是完全错误的。
同时暗示了我们观察到的merge_join和延迟上升只是伪相关，关于不正确的索引使用造
提供，也非常慢。在查看文件级别的图表时，我们发现这些请求在几天前一直很快。这
注18
一段时间后，问题根源找到了：App的权限检查系统中有一个存在时间很长的Bug，每
了最大程度减少数据服务和内存缓存服务的访问数量使用了一个缓存层，该缓存层将白
请求都会调用的一个具体方法，该方法检查用户是否有权限访问某个路径。这个方法为
后将这些数据存储在内存中。这样这个实例可以避免反复读取不经常改变的配置文件
虽然带着一个未能确认的Bug上线并不是理想情况，
延迟和资源使用率都会上升。
for循环组成。
时候，
我们只能依靠良好的工程师判断力来尽力消除风险。
一个自动化安全扫描程序被用来扫描该App是否有漏洞，由此造成的副作
如果物件数量较少，线性时间复杂度并不是什么问题，但是在大量数据物件的情况下，
，很多时候消除所有已知Bug是不可行的。有的
但是一个常见的基于内存的实现经常由
案例分析
129
<149
---
## Page 172
150
题。对故障排查采用系统化的手段，而不是依靠运气和经验，将有助于限定你的服务的
以及记录这些改变可以降低实际故障排查的需要，也能让故障排查更简单。
代码中对现存错误的假设，以及环境改变也经常会导致需要故障排查过程。简化、控制，
保证这些信息用一个一致的方法在整个系统内传递，例如：使用唯一标识标记所有组件
使故障排查更简单
小结
记录的要求，加速了检查和恢复。
产生的所有相关RPC。这有效地降低了需要对应上游某条日志记录与下游组件某条日志
有很多方法可以简化和加速故障排查过程。
Bug，删除了无用的白名单物件之后，App性能恢复到了正常水平。
每个请求都检查一次，由此导致了延迟的上升。这中间没有触发任何RPC请求。修复
用就是该扫描过程在半小时内产生了几千个白名单物件。这些无用的白名单物件必须被
故障恢复时间（MTTR），从而为你的用户提供更好的使用体验。
我们讨论了能够将故障排查过程变得简单和可理解的步骤，这样新手也可以有效解决问
·利用成熟的、观察性好的组件接口设计系统。
·增加可观察性。在实现之初就给每个组件增加白盒监控指标和结构化日志。
第12章有效的故障排查手段
：可能最基本的是
---
## Page 173