case for a relatively small model, we also expect this to be the
case for larger models; our tests on inverting ResNet models
also resulted in average εrepr larger than those found when
training with k = 1 (see Tables I and III). Thus, we have
empirically determined that an adversary cannot use higher
learning rates to decrease the computational load.
From the argument we have made (G2), and given that
we are not aware of a mechanism to prove this formally, we
present the following as a conjecture:
Conjecture 1. Inverting a training sequence using numerical
root ﬁnding methods will always be at least as computationally
expensive as training, given the same model.
|
|
r
p
e
r
ε
|
|
CIFAR-100
0.005 ± 0.001
0.016 ± 0.005
0.073 ± 0.014
0.0 ± 0.0
CIFAR-10
0.023 ± 0.001
(cid:96)1
0.048 ± 0.004
(cid:96)2
(cid:96)∞ 0.18 ± 0.044
0.016 ± 0.002
cos
TABLE III: Normalized reproduction error, ||εrepr|| of PoL
created by General Inverse Gradient Method. The trained
models inverted for 50 steps to obtain a PoL with length 50 and
k = 1. The εrepr is then computed on this PoL. Comparing
to the k = 1 case in Table I, the εrepr here is larger.
b) Difﬁculty of Finding a Suitable Initialization: As
mentioned in § V-D4, a valid initialization must pass the KS
test [69]. To test the initialization, the veriﬁer compares it
against the public pool of known initializations, e.g., various
forms of zero-centered uniform and normal distributions [57]–
[59]. Thus, the adversary must in addition successfully spoof
the initialization to pass the KS test. Our empirical results
indicate that inverse gradient methods are unlikely to ﬁnd
a valid initialization. Speciﬁcally, we inverted 50 steps on
a model trained for 50 steps, and applied the KS test to
the last state of inverting (corresponding to the ﬁrst state of
training) as described in § V-D4. On CIFAR-10 we observe
that the the average and minimum p-values are 0.044(±0.102)
and 1.077(±1.864)× 10−28, respectively. On CIFAR-100, the
average and minimum p-values are 0.583(±7.765)×10−12 and
0(±0), respectively. These p-values are far below the required
threshold to pass the KS test and thus an adversary is unable to
ﬁnd a valid initialization sampled from a claimed distribution.
A clever adversary may attempt to direct the inverse gradient
method toward a valid initialization. We discuss in § VII-C
below how these directed approaches do not succeed in passing
our veriﬁcation scheme. We remark that the KS test prevents
other spooﬁng strategies, such as leveraging ﬁne-pruning [82]
or sparsiﬁcation [83]. These strategies can signiﬁcantly min-
imize the computational load of spooﬁng while maintaining
both the model architecture and test-time performance, i.e.,
f ≈ fWT . However, they as well fail to pass the KS test and
thus are not veriﬁed by our scheme.
C. Directed Retraining
Given no extra knowledge, retraining fWT would take as
much compute as used by T . However, the adversary always
has the additional advantage of knowing the ﬁnal weights
WT . We now explore how the adversary can leverage this
knowledge to create a dishonest spoof (see Deﬁnition 2).
1) Approach 1: PoL Concatenation: An adversary A aware
that V does not verify all the updates may try to exploit this
and employ structurally correct spooﬁng (refer § IV-A) to
obtain a partially valid PoL that may pass the veriﬁcation. To
this end, the adversary can ﬁne-tune [67] or ﬁne-prune [82] the
model fWT to achieve f which is not an exact copy of fWT
but has comparable test-time performance. This step provides
the adversary with a valid PoL from fWT to f. However, this
would still be detected by Algorithm 2 because V also checks
the initial state (recall § V-D4), which in the adversary’s PoL
is WT (for which it has no valid PoL).
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:16:15 UTC from IEEE Xplore.  Restrictions apply. 
1050
performance periodically since the arbitrarily large updates
would likely decrease model performance signiﬁcantly.
2) Approach 2: Directed Weight Minimization: To mini-
mize the discontinuity magnitude, an adversary may attempt
to direct the weights of retraining toward WT . To achieve this,
they can directly minimize this distance using regularization.
This approach fails veriﬁcation because the custom regularizer
requires the ﬁnal weights prior to them having been achieved,
which therefore cannot pass veriﬁcation. Further, this informa-
tion cannot be easily distilled into synthetic data because no
gradient of the regularization term, with respect to the data,
exists (refer to Appendix B for more details). By this vain,
other tactics, such as optimizing a learning rate η to converge
W (cid:48) to WT also fail veriﬁcation.
VIII. DISCUSSIONS & LIMITATIONS
A PoL provides grounds for proving ownership of any
effortful attempt at learning a model. As shown in § VI, a
PoL guarantees that no one but the trainer can lay claim to
that exact model. Further, if a chain-of-trust is adopted, this
guarantee is extended to the use of the said model as an
initial state for the training of a surrogate model. However,
a PoL cannot be used to connect the model to its surrogate,
neither can it be used avoid extraction. Instead, a PoL provides
legal protection: if the trainer produces a PoL and publishes a
time-stamped signature of it, this unchangeable record proves
ownership in case of false claim by a surrogate model owner.
We now discuss limitations with our proposed scheme for
PoL. First, our veriﬁcation scheme requires that the training
data be shared with the veriﬁer. When this data is private, this
can be undesirable. To protect the training data’s conﬁdential-
ity, it is possible for the prover to engage in a private inference
protocol with the veriﬁer [84] using multi-party computation.
This will incur additional computational overhead but is only
limited on the chosen private inference scheme.
Second, we note the considerable storage requirements our
proposed proof-of-work imposes. To decrease the approach’s
footprint by a factor of 2, we downcast the float32 values
of our parameters to float16 when saving them. Verifying
float16 values introduces minimal error. We acknowledge
that other approaches such as hashing could provide signiﬁ-
cantly better improvement to the storage footprint. For exam-
ple, follow up work may consider hashing weights sequentially
utilizing Merkle tree structure [85], i.e. each consecutive set
of weights during the training procedure are hashed and then
saved as the hash of the concatenation of the current weights
and the previously saved hash. We do not use Merkle trees
due to the error accumulated when the veriﬁer reconstructs
the weights: the error in the weights forces the weights of
the veriﬁer and legitimate worker to hash to different values,
losing the ability to verify that the weights match within some
bound. This may be addressed with fuzzy extractors or locality
sensitive hashing (LSH). However, the use of fuzzy extractors
and LSH protocols incurs signiﬁcant difﬁculty through the
need to ﬁnd a suitable bound to work over all choices of E,
Q, and k. Designing such primitives is future work.
Third, we emphasize that counter-based pseudorandom
number generators [86], [87] can potentially remove most, if
(a) CIFAR-10
(b) CIFAR-100
s||2 and largest
Fig. 5: Magnitude of discontinuity ||WT − W (cid:48)
valid update max||W (cid:48)
t−1||2 in a spooﬁng PoL made by
concatenating 2 valid but independent PoL. The discontinuity
is signiﬁcantly larger than the valid updates, and thus easily
detected by Algorithm 2 which checks the largest updates ﬁrst.
t − W (cid:48)
To adapt, the adversary can train a model with the same
architecture as fWT from a random initialization for some
number of steps with minimal cost, providing a second valid
PoL, this time starting from a valid random initialization.
Then, the adversary concatenates these two PoLs. In addition
to saving compute,
the advantage of this strategy is that
there is only one single point of discontinuity in the PoL,
which consists of thousands of updates. Thus if V randomly
sampled a few updates to check, the A’s PoL would likely
go undetected. However, since V veriﬁes the top-Q updates
in Algorithm 2, this discontinuity which is among the largest
of the sequence would be invalidated—as we evaluate next.
s||2 and maxt≤s ||W (cid:48)
Evaluation. Our evaluation is performed with the setup from
§ VI-C1. For each dataset, we ﬁrst train a model to completion
as the prover T ’s model WT . Then we play the role of A to
spoof a PoL by concatenation: we ﬁne-tune WT for 1 epoch
to get f, and train another model (from scratch) with the same
architecture for s steps (s ≤ T ) from initialization (i.e., W (cid:48)
0 to
W (cid:48)
s); s is the number of steps on the x-axis in Figure 5. We plot
||WT − W (cid:48)
t−k||2 (both normalized
t − W (cid:48)
(by dref)) in this ﬁgure (with k = 1). We observe that:
s||2) is much larger than
• The discontinuity (i.e., ||WT −W (cid:48)
all valid gradient updates in the PoL, so setting Q = 1
would be sufﬁcient for the veriﬁer to detect this spooﬁng.
The veriﬁcation cost is E·k = E steps of gradient updates
(since we set k = 1 for this experiment). However, if the
veriﬁer randomly samples E steps (rather than picking
the top-1 step of every epoch), the probability of ﬁnding
the discontinuity is only 1
• The discontinuity has similar magnitude to dref, revealing
the fact that WT and W (cid:48)
t−k||2 does vary signiﬁcantly with re-
• maxt≤s ||W (cid:48)
spect to s, meaning setting δ to ||W (cid:48)
t−k||2 for small
t − W (cid:48)
t is sufﬁcient to detect this kind of attack.
It is worth noting that if the adversary A has knowledge
about Q, or veriﬁer V sets Q to a small value, A may make Q
(or more) legitimate updates in every epoch by training with an
arbitrarily large learning rate, which will bypass Algorithm 2.
Solutions to this issue could involve (a) using a large Q, (b)
randomly verifying some more updates, or (c) checking model
S , with S = 390 here.
s are unrelated.
t − W (cid:48)
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:16:15 UTC from IEEE Xplore.  Restrictions apply. 
1051




%7,33$9058



89,3.0||WT,W0s||22,||W0t,W0t1||2




%7,33$9058
89,3.0||WT,W0s||22,||W0t,W0t1||2[5]
not all, noise in the training process because the pseudorandom
numbers are generated based only off the input seed, not
any hardware-based source of entropy. Recall that this noise
introduces the random variable z in Theorems 1 and 2. While
there is currently no ground-truth for all sources of randomness
arising in ML training through hardware, low-level libraries,
and random number generation, such ground-truths would
make training more reproducible and facilitate our approach.
Finally, we remark that our probability of success for our
veriﬁcation scheme degrades multiplicatively with each usage.
This limits its usage for extremely long chains of PoLs (e.g.,
when successively transfer learning between many models)
where any given probability of success is signiﬁcantly below
1. As there is currently no PoL scheme to gain practical insight
on this limitation, we leave this to future work.
IX. CONCLUSIONS
Our analysis shows gradient descent naturally produces
secret information due to its stochasticity, and this information
can serve as a proof-of-learning. We ﬁnd that entropy growth
during training creates an asymmetry between the adversary
and defender which advantages the defender. Perhaps the
strongest advantage of our approach is that it requires no
changes to the existing training procedure, and adds little
overhead for the prover seeking to prove they have trained a
model. We expect that future work will expand on the notion
of proof-of-learning introduced here, and propose improved
mechanisms applicable beyond the two scenarios which mo-
tivated our work (model stealing and distributed training).
ACKNOWLEDGMENTS
We thank the reviewers for their insightful feedback. This work
was supported by CIFAR (through a Canada CIFAR AI Chair),
by NSERC (under the Discovery Program, NFRF Exploration
program, and COHESA strategic research network), and by
gifts from Intel and Microsoft. We also thank the Vector
Institute’s sponsors. Varun was supported in part
through
the following US National Science Foundation grants: CNS-
1838733, CNS-1719336, CNS-1647152, CNS-1629833 and
CNS-2003129, and the Landweber fellowship.
REFERENCES
[2]
[1] C. Li. (Jun. 3, 2020). “OpenAI’s GPT-3 language model: A technical
overview,” Lambda Blog. Library Catalog: lambdalabs.com, [Online].
Available: https://lambdalabs.com/blog/demystifying-gpt-3/ (visited
on 10/01/2020).
S. Markidis, S. W. Der Chien, E. Laure, I. B. Peng, and J. S. Vetter,
“Nvidia tensor core programmability, performance & precision,” in
2018 IEEE International Parallel and Distributed Processing Sympo-
sium Workshops (IPDPSW), IEEE, 2018, pp. 522–531.
[3] N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa,
S. Bates, S. Bhatia, N. Boden, A. Borchers, et al., “In-datacenter
performance analysis of a tensor processing unit,” in Proceedings of
the 44th Annual International Symposium on Computer Architecture,
2017, pp. 1–12.
[4] A. Putnam, A. M. Caulﬁeld, E. S. Chung, D. Chiou, K. Constan-
tinides, J. Demme, H. Esmaeilzadeh, J. Fowers, G. P. Gopal, J. Gray,
et al., “A reconﬁgurable fabric for accelerating large-scale datacen-
ter services,” in 2014 ACM/IEEE 41st International Symposium on
Computer Architecture (ISCA), IEEE, 2014, pp. 13–24.
F. Tram`er, F. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart, “Steal-
ing machine learning models via prediction apis,” in 25th {USENIX}
Security Symposium ({USENIX} Security 16), 2016, pp. 601–618.
[7]
[6] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. Berkay Celik,
and A. Swami, “Practical Black-Box Attacks against Machine Learn-
ing,” arXiv e-prints, arXiv:1602.02697, arXiv:1602.02697, Feb. 2016.
arXiv: 1602.02697 [cs.CR].
S. Pal, Y. Gupta, A. Shukla, A. Kanade, S. K. Shevade, and V.
Ganapathy, “A framework for the extraction of deep neural networks
by leveraging public data,” CoRR, vol. abs/1905.09165, 2019. arXiv:
1905.09165. [Online]. Available: http://arxiv.org/abs/1905.09165.
J. R. Correia-Silva, R. F. Berriel, C. Badue, A. F. de Souza, and
T. Oliveira-Santos, “Copycat cnn: Stealing knowledge by persuading
confession with random non-labeled data,” in 2018 International Joint
Conference on Neural Networks (IJCNN), IEEE, 2018, pp. 1–8.
T. Orekondy, B. Schiele, and M. Fritz, “Knockoff nets: Stealing
functionality of black-box models,” in Proceedings of
the IEEE
Conference on Computer Vision and Pattern Recognition, 2019,
pp. 4954–4963.
[8]
[9]
[11]
[17]
[16]
[14]
[10] M. Li, D. G. Andersen, J. W. Park, A. J. Smola, A. Ahmed, V.
Josifovski, J. Long, E. J. Shekita, and B.-Y. Su, “Scaling distributed
machine learning with the parameter server,” in 11th {USENIX} Sym-
posium on Operating Systems Design and Implementation ({OSDI}
14), 2014, pp. 583–598.
P. Blanchard, R. Guerraoui, J. Stainer, et al., “Machine learning
with adversaries: Byzantine tolerant gradient descent,” in Advances
in Neural Information Processing Systems, 2017, pp. 119–129.
[12] C. Dwork and M. Naor, “Pricing via processing or combatting junk
mail,” in Proceedings of the 12th Annual International Cryptology
Conference on Advances in Cryptology, ser. CRYPTO ’92, Berlin,
Heidelberg: Springer-Verlag, 1992, pp. 139–147, ISBN: 3540573402.
[13] M. Jakobsson and A. Juels, “Proofs of work and bread pudding
protocols(extended abstract),” in Secure Information Networks, B.
Preneel, Ed., Boston, MA: Springer US, 1999, pp. 258–272, ISBN: