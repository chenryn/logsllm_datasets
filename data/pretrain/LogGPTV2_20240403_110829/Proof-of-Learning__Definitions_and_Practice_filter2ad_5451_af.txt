### Optimized Text

**Empirical Findings and Conjecture**

For relatively small models, we observed that the average ε<sub>repr</sub> (reproduction error) was higher when inverting ResNet models compared to training with \( k = 1 \) (see Tables I and III). This suggests that an adversary cannot use higher learning rates to reduce the computational load. Based on this empirical evidence, we propose the following conjecture:

**Conjecture 1:** Inverting a training sequence using numerical root-finding methods will always be at least as computationally expensive as the original training, given the same model.

| **ε<sub>repr</sub>** | **CIFAR-100** | **CIFAR-10** |
|---------------------|---------------|--------------|
| \( l_1 \)           | 0.005 ± 0.001 | 0.023 ± 0.001 |
| \( l_2 \)           | 0.016 ± 0.005 | 0.048 ± 0.004 |
| \( l_\infty \)      | 0.073 ± 0.014 | 0.18 ± 0.044  |
| Cosine              | 0.0 ± 0.0     | 0.016 ± 0.002 |

**Table III: Normalized reproduction error, \( ||\varepsilon_{\text{repr}}|| \), of PoL created by the General Inverse Gradient Method. The trained models were inverted for 50 steps to obtain a PoL with length 50 and \( k = 1 \). The \( \varepsilon_{\text{repr}} \) is then computed on this PoL. Comparing to the \( k = 1 \) case in Table I, the \( \varepsilon_{\text{repr}} \) here is larger.**

**Difficulty in Finding Suitable Initialization**

As discussed in § V-D4, a valid initialization must pass the Kolmogorov-Smirnov (KS) test [69]. To validate the initialization, the verifier compares it against a public pool of known initializations, such as zero-centered uniform and normal distributions [57]–[59]. Therefore, the adversary must also spoof the initialization to pass the KS test. Our empirical results indicate that inverse gradient methods are unlikely to find a valid initialization. Specifically, we inverted 50 steps on a model trained for 50 steps and applied the KS test to the last state of inversion (corresponding to the first state of training). On CIFAR-10, the average and minimum p-values were 0.044(±0.102) and 1.077(±1.864)×10<sup>-28</sup>, respectively. On CIFAR-100, the average and minimum p-values were 0.583(±7.765)×10<sup>-12</sup> and 0(±0), respectively. These p-values are far below the required threshold to pass the KS test, indicating that an adversary cannot find a valid initialization sampled from a claimed distribution.

A sophisticated adversary might attempt to direct the inverse gradient method toward a valid initialization. However, as discussed in § VII-C, these directed approaches do not succeed in passing our verification scheme. The KS test also prevents other spoofing strategies, such as leveraging fine-pruning [82] or sparsification [83], which can significantly minimize the computational load while maintaining both the model architecture and test-time performance. However, these strategies fail to pass the KS test and thus are not verified by our scheme.

**Directed Retraining**

Without additional knowledge, retraining \( f_{WT} \) would require the same amount of computation as used by the trainer \( T \). However, the adversary has the advantage of knowing the final weights \( W_T \). We explore how the adversary can leverage this knowledge to create a dishonest spoof (see Definition 2).

**Approach 1: PoL Concatenation**

An adversary aware that the verifier does not verify all updates may try to exploit this by employing structurally correct spoofing (refer to § IV-A) to obtain a partially valid PoL that may pass the verification. The adversary can fine-tune [67] or fine-prune [82] the model \( f_{WT} \) to achieve \( f \), which is not an exact copy of \( f_{WT} \) but has comparable test-time performance. This step provides the adversary with a valid PoL from \( f_{WT} \) to \( f \). However, this would still be detected by Algorithm 2 because the verifier also checks the initial state (recall § V-D4), which in the adversary’s PoL is \( W_T \) (for which it has no valid PoL).

**Approach 2: Directed Weight Minimization**

To minimize the discontinuity magnitude, an adversary may attempt to direct the weights of retraining toward \( W_T \). This can be achieved by directly minimizing the distance using regularization. This approach fails verification because the custom regularizer requires the final weights prior to them being achieved, which cannot pass verification. Additionally, this information cannot be easily distilled into synthetic data because no gradient of the regularization term with respect to the data exists (refer to Appendix B for more details). Similarly, other tactics, such as optimizing a learning rate \( \eta \) to converge \( W' \) to \( W_T \), also fail verification.

**Discussion and Limitations**

A Proof-of-Learning (PoL) provides grounds for proving ownership of any effortful attempt at learning a model. As shown in § VI, a PoL guarantees that no one but the trainer can claim that exact model. If a chain-of-trust is adopted, this guarantee extends to the use of the model as an initial state for the training of a surrogate model. However, a PoL cannot connect the model to its surrogate or prevent extraction. Instead, a PoL provides legal protection: if the trainer produces a PoL and publishes a time-stamped signature of it, this unchangeable record proves ownership in case of false claims by a surrogate model owner.

**Limitations of the Proposed Scheme**

1. **Data Confidentiality**: Our verification scheme requires sharing the training data with the verifier. When the data is private, this can be undesirable. To protect the training data's confidentiality, the prover can engage in a private inference protocol with the verifier [84] using multi-party computation, although this incurs additional computational overhead.

2. **Storage Requirements**: The proposed proof-of-work imposes considerable storage requirements. To reduce the footprint by a factor of 2, we downcast the float32 values of our parameters to float16 when saving them. Verifying float16 values introduces minimal error. Other approaches, such as hashing, could provide significant improvements to the storage footprint. For example, follow-up work may consider hashing weights sequentially using a Merkle tree structure [85]. However, the error accumulated when the verifier reconstructs the weights can cause the weights of the verifier and legitimate worker to hash to different values, losing the ability to verify that the weights match within some bound. This issue may be addressed with fuzzy extractors or locality-sensitive hashing (LSH), but designing such primitives is future work.

3. **Pseudorandom Number Generators**: Counter-based pseudorandom number generators [86], [87] can potentially remove most, if not all, noise in the training process because the pseudorandom numbers are generated based only on the input seed, not any hardware-based source of entropy. While there is currently no ground-truth for all sources of randomness arising in ML training through hardware, low-level libraries, and random number generation, such ground-truths would make training more reproducible and facilitate our approach.

Finally, we note that the probability of success for our verification scheme degrades multiplicatively with each usage, limiting its effectiveness for extremely long chains of PoLs (e.g., when successively transfer learning between many models). Future work will need to address this limitation.

**Conclusion**

Our analysis shows that gradient descent naturally produces secret information due to its stochasticity, and this information can serve as a proof-of-learning. Entropy growth during training creates an asymmetry between the adversary and defender, which advantages the defender. A key advantage of our approach is that it requires no changes to the existing training procedure and adds little overhead for the prover seeking to prove they have trained a model. Future work will expand on the notion of proof-of-learning introduced here and propose improved mechanisms applicable beyond the two scenarios that motivated our work (model stealing and distributed training).

**Acknowledgments**

We thank the reviewers for their insightful feedback. This work was supported by CIFAR (through a Canada CIFAR AI Chair), NSERC (under the Discovery Program, NFRF Exploration program, and COHESA strategic research network), and gifts from Intel and Microsoft. We also thank the Vector Institute’s sponsors. Varun was supported in part through the following US National Science Foundation grants: CNS-1838733, CNS-1719336, CNS-1647152, CNS-1629833, and CNS-2003129, and the Landweber fellowship.

**References**

[1] C. Li. (Jun. 3, 2020). "OpenAI’s GPT-3 language model: A technical overview," Lambda Blog. Library Catalog: lambdalabs.com, [Online]. Available: https://lambdalabs.com/blog/demystifying-gpt-3/ (visited on 10/01/2020).

[2] S. Markidis, S. W. Der Chien, E. Laure, I. B. Peng, and J. S. Vetter, "Nvidia tensor core programmability, performance & precision," in 2018 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW), IEEE, 2018, pp. 522–531.

[3] N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa, S. Bates, S. Bhatia, N. Boden, A. Borchers, et al., "In-datacenter performance analysis of a tensor processing unit," in Proceedings of the 44th Annual International Symposium on Computer Architecture, 2017, pp. 1–12.

[4] A. Putnam, A. M. Caulfield, E. S. Chung, D. Chiou, K. Constantinides, J. Demme, H. Esmaeilzadeh, J. Fowers, G. P. Gopal, J. Gray, et al., "A reconfigurable fabric for accelerating large-scale datacenter services," in 2014 ACM/IEEE 41st International Symposium on Computer Architecture (ISCA), IEEE, 2014, pp. 13–24.

[5] F. Tramèr, F. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart, "Stealing machine learning models via prediction APIs," in 25th USENIX Security Symposium (USENIX Security 16), 2016, pp. 601–618.

[6] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. Berkay Celik, and A. Swami, "Practical Black-Box Attacks against Machine Learning," arXiv e-prints, arXiv:1602.02697, arXiv:1602.02697, Feb. 2016. arXiv: 1602.02697 [cs.CR].

[7] S. Pal, Y. Gupta, A. Shukla, A. Kanade, S. K. Shevade, and V. Ganapathy, "A framework for the extraction of deep neural networks by leveraging public data," CoRR, vol. abs/1905.09165, 2019. arXiv: 1905.09165. [Online]. Available: http://arxiv.org/abs/1905.09165.

[8] J. R. Correia-Silva, R. F. Berriel, C. Badue, A. F. de Souza, and T. Oliveira-Santos, "Copycat CNN: Stealing knowledge by persuading confession with random non-labeled data," in 2018 International Joint Conference on Neural Networks (IJCNN), IEEE, 2018, pp. 1–8.

[9] T. Orekondy, B. Schiele, and M. Fritz, "Knockoff nets: Stealing functionality of black-box models," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 4954–4963.

[10] M. Li, D. G. Andersen, J. W. Park, A. J. Smola, A. Ahmed, V. Josifovski, J. Long, E. J. Shekita, and B.-Y. Su, "Scaling distributed machine learning with the parameter server," in 11th USENIX Symposium on Operating Systems Design and Implementation (OSDI 14), 2014, pp. 583–598.

[11] P. Blanchard, R. Guerraoui, J. Stainer, et al., "Machine learning with adversaries: Byzantine tolerant gradient descent," in Advances in Neural Information Processing Systems, 2017, pp. 119–129.

[12] C. Dwork and M. Naor, "Pricing via processing or combatting junk mail," in Proceedings of the 12th Annual International Cryptology Conference on Advances in Cryptology, ser. CRYPTO '92, Berlin, Heidelberg: Springer-Verlag, 1992, pp. 139–147, ISBN: 3540573402.

[13] M. Jakobsson and A. Juels, "Proofs of work and bread pudding protocols (extended abstract)," in Secure Information Networks, B. Preneel, Ed., Boston, MA: Springer US, 1999, pp. 258–272, ISBN: 0792386980.