case for a relatively small model, we also expect this to be the
case for larger models; our tests on inverting ResNet models
also resulted in average Œµrepr larger than those found when
training with k = 1 (see Tables I and III). Thus, we have
empirically determined that an adversary cannot use higher
learning rates to decrease the computational load.
From the argument we have made (G2), and given that
we are not aware of a mechanism to prove this formally, we
present the following as a conjecture:
Conjecture 1. Inverting a training sequence using numerical
root Ô¨Ånding methods will always be at least as computationally
expensive as training, given the same model.
|
|
r
p
e
r
Œµ
|
|
CIFAR-100
0.005 ¬± 0.001
0.016 ¬± 0.005
0.073 ¬± 0.014
0.0 ¬± 0.0
CIFAR-10
0.023 ¬± 0.001
(cid:96)1
0.048 ¬± 0.004
(cid:96)2
(cid:96)‚àû 0.18 ¬± 0.044
0.016 ¬± 0.002
cos
TABLE III: Normalized reproduction error, ||Œµrepr|| of PoL
created by General Inverse Gradient Method. The trained
models inverted for 50 steps to obtain a PoL with length 50 and
k = 1. The Œµrepr is then computed on this PoL. Comparing
to the k = 1 case in Table I, the Œµrepr here is larger.
b) DifÔ¨Åculty of Finding a Suitable Initialization: As
mentioned in ¬ß V-D4, a valid initialization must pass the KS
test [69]. To test the initialization, the veriÔ¨Åer compares it
against the public pool of known initializations, e.g., various
forms of zero-centered uniform and normal distributions [57]‚Äì
[59]. Thus, the adversary must in addition successfully spoof
the initialization to pass the KS test. Our empirical results
indicate that inverse gradient methods are unlikely to Ô¨Ånd
a valid initialization. SpeciÔ¨Åcally, we inverted 50 steps on
a model trained for 50 steps, and applied the KS test to
the last state of inverting (corresponding to the Ô¨Årst state of
training) as described in ¬ß V-D4. On CIFAR-10 we observe
that the the average and minimum p-values are 0.044(¬±0.102)
and 1.077(¬±1.864)√ó 10‚àí28, respectively. On CIFAR-100, the
average and minimum p-values are 0.583(¬±7.765)√ó10‚àí12 and
0(¬±0), respectively. These p-values are far below the required
threshold to pass the KS test and thus an adversary is unable to
Ô¨Ånd a valid initialization sampled from a claimed distribution.
A clever adversary may attempt to direct the inverse gradient
method toward a valid initialization. We discuss in ¬ß VII-C
below how these directed approaches do not succeed in passing
our veriÔ¨Åcation scheme. We remark that the KS test prevents
other spooÔ¨Ång strategies, such as leveraging Ô¨Åne-pruning [82]
or sparsiÔ¨Åcation [83]. These strategies can signiÔ¨Åcantly min-
imize the computational load of spooÔ¨Ång while maintaining
both the model architecture and test-time performance, i.e.,
f ‚âà fWT . However, they as well fail to pass the KS test and
thus are not veriÔ¨Åed by our scheme.
C. Directed Retraining
Given no extra knowledge, retraining fWT would take as
much compute as used by T . However, the adversary always
has the additional advantage of knowing the Ô¨Ånal weights
WT . We now explore how the adversary can leverage this
knowledge to create a dishonest spoof (see DeÔ¨Ånition 2).
1) Approach 1: PoL Concatenation: An adversary A aware
that V does not verify all the updates may try to exploit this
and employ structurally correct spooÔ¨Ång (refer ¬ß IV-A) to
obtain a partially valid PoL that may pass the veriÔ¨Åcation. To
this end, the adversary can Ô¨Åne-tune [67] or Ô¨Åne-prune [82] the
model fWT to achieve f which is not an exact copy of fWT
but has comparable test-time performance. This step provides
the adversary with a valid PoL from fWT to f. However, this
would still be detected by Algorithm 2 because V also checks
the initial state (recall ¬ß V-D4), which in the adversary‚Äôs PoL
is WT (for which it has no valid PoL).
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:16:15 UTC from IEEE Xplore.  Restrictions apply. 
1050
performance periodically since the arbitrarily large updates
would likely decrease model performance signiÔ¨Åcantly.
2) Approach 2: Directed Weight Minimization: To mini-
mize the discontinuity magnitude, an adversary may attempt
to direct the weights of retraining toward WT . To achieve this,
they can directly minimize this distance using regularization.
This approach fails veriÔ¨Åcation because the custom regularizer
requires the Ô¨Ånal weights prior to them having been achieved,
which therefore cannot pass veriÔ¨Åcation. Further, this informa-
tion cannot be easily distilled into synthetic data because no
gradient of the regularization term, with respect to the data,
exists (refer to Appendix B for more details). By this vain,
other tactics, such as optimizing a learning rate Œ∑ to converge
W (cid:48) to WT also fail veriÔ¨Åcation.
VIII. DISCUSSIONS & LIMITATIONS
A PoL provides grounds for proving ownership of any
effortful attempt at learning a model. As shown in ¬ß VI, a
PoL guarantees that no one but the trainer can lay claim to
that exact model. Further, if a chain-of-trust is adopted, this
guarantee is extended to the use of the said model as an
initial state for the training of a surrogate model. However,
a PoL cannot be used to connect the model to its surrogate,
neither can it be used avoid extraction. Instead, a PoL provides
legal protection: if the trainer produces a PoL and publishes a
time-stamped signature of it, this unchangeable record proves
ownership in case of false claim by a surrogate model owner.
We now discuss limitations with our proposed scheme for
PoL. First, our veriÔ¨Åcation scheme requires that the training
data be shared with the veriÔ¨Åer. When this data is private, this
can be undesirable. To protect the training data‚Äôs conÔ¨Ådential-
ity, it is possible for the prover to engage in a private inference
protocol with the veriÔ¨Åer [84] using multi-party computation.
This will incur additional computational overhead but is only
limited on the chosen private inference scheme.
Second, we note the considerable storage requirements our
proposed proof-of-work imposes. To decrease the approach‚Äôs
footprint by a factor of 2, we downcast the float32 values
of our parameters to float16 when saving them. Verifying
float16 values introduces minimal error. We acknowledge
that other approaches such as hashing could provide signiÔ¨Å-
cantly better improvement to the storage footprint. For exam-
ple, follow up work may consider hashing weights sequentially
utilizing Merkle tree structure [85], i.e. each consecutive set
of weights during the training procedure are hashed and then
saved as the hash of the concatenation of the current weights
and the previously saved hash. We do not use Merkle trees
due to the error accumulated when the veriÔ¨Åer reconstructs
the weights: the error in the weights forces the weights of
the veriÔ¨Åer and legitimate worker to hash to different values,
losing the ability to verify that the weights match within some
bound. This may be addressed with fuzzy extractors or locality
sensitive hashing (LSH). However, the use of fuzzy extractors
and LSH protocols incurs signiÔ¨Åcant difÔ¨Åculty through the
need to Ô¨Ånd a suitable bound to work over all choices of E,
Q, and k. Designing such primitives is future work.
Third, we emphasize that counter-based pseudorandom
number generators [86], [87] can potentially remove most, if
(a) CIFAR-10
(b) CIFAR-100
s||2 and largest
Fig. 5: Magnitude of discontinuity ||WT ‚àí W (cid:48)
valid update max||W (cid:48)
t‚àí1||2 in a spooÔ¨Ång PoL made by
concatenating 2 valid but independent PoL. The discontinuity
is signiÔ¨Åcantly larger than the valid updates, and thus easily
detected by Algorithm 2 which checks the largest updates Ô¨Årst.
t ‚àí W (cid:48)
To adapt, the adversary can train a model with the same
architecture as fWT from a random initialization for some
number of steps with minimal cost, providing a second valid
PoL, this time starting from a valid random initialization.
Then, the adversary concatenates these two PoLs. In addition
to saving compute,
the advantage of this strategy is that
there is only one single point of discontinuity in the PoL,
which consists of thousands of updates. Thus if V randomly
sampled a few updates to check, the A‚Äôs PoL would likely
go undetected. However, since V veriÔ¨Åes the top-Q updates
in Algorithm 2, this discontinuity which is among the largest
of the sequence would be invalidated‚Äîas we evaluate next.
s||2 and maxt‚â§s ||W (cid:48)
Evaluation. Our evaluation is performed with the setup from
¬ß VI-C1. For each dataset, we Ô¨Årst train a model to completion
as the prover T ‚Äôs model WT . Then we play the role of A to
spoof a PoL by concatenation: we Ô¨Åne-tune WT for 1 epoch
to get f, and train another model (from scratch) with the same
architecture for s steps (s ‚â§ T ) from initialization (i.e., W (cid:48)
0 to
W (cid:48)
s); s is the number of steps on the x-axis in Figure 5. We plot
||WT ‚àí W (cid:48)
t‚àík||2 (both normalized
t ‚àí W (cid:48)
(by dref)) in this Ô¨Ågure (with k = 1). We observe that:
s||2) is much larger than
‚Ä¢ The discontinuity (i.e., ||WT ‚àíW (cid:48)
all valid gradient updates in the PoL, so setting Q = 1
would be sufÔ¨Åcient for the veriÔ¨Åer to detect this spooÔ¨Ång.
The veriÔ¨Åcation cost is E¬∑k = E steps of gradient updates
(since we set k = 1 for this experiment). However, if the
veriÔ¨Åer randomly samples E steps (rather than picking
the top-1 step of every epoch), the probability of Ô¨Ånding
the discontinuity is only 1
‚Ä¢ The discontinuity has similar magnitude to dref, revealing
the fact that WT and W (cid:48)
t‚àík||2 does vary signiÔ¨Åcantly with re-
‚Ä¢ maxt‚â§s ||W (cid:48)
spect to s, meaning setting Œ¥ to ||W (cid:48)
t‚àík||2 for small
t ‚àí W (cid:48)
t is sufÔ¨Åcient to detect this kind of attack.
It is worth noting that if the adversary A has knowledge
about Q, or veriÔ¨Åer V sets Q to a small value, A may make Q
(or more) legitimate updates in every epoch by training with an
arbitrarily large learning rate, which will bypass Algorithm 2.
Solutions to this issue could involve (a) using a large Q, (b)
randomly verifying some more updates, or (c) checking model
S , with S = 390 here.
s are unrelated.
t ‚àí W (cid:48)
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:16:15 UTC from IEEE Xplore.  Restrictions apply. 
1051




%7,33$9058



89,3.0||WT,W0s||22,||W0t,W0t1||2




%7,33$9058
89,3.0||WT,W0s||22,||W0t,W0t1||2[5]
not all, noise in the training process because the pseudorandom
numbers are generated based only off the input seed, not
any hardware-based source of entropy. Recall that this noise
introduces the random variable z in Theorems 1 and 2. While
there is currently no ground-truth for all sources of randomness
arising in ML training through hardware, low-level libraries,
and random number generation, such ground-truths would
make training more reproducible and facilitate our approach.
Finally, we remark that our probability of success for our
veriÔ¨Åcation scheme degrades multiplicatively with each usage.
This limits its usage for extremely long chains of PoLs (e.g.,
when successively transfer learning between many models)
where any given probability of success is signiÔ¨Åcantly below
1. As there is currently no PoL scheme to gain practical insight
on this limitation, we leave this to future work.
IX. CONCLUSIONS
Our analysis shows gradient descent naturally produces
secret information due to its stochasticity, and this information
can serve as a proof-of-learning. We Ô¨Ånd that entropy growth
during training creates an asymmetry between the adversary
and defender which advantages the defender. Perhaps the
strongest advantage of our approach is that it requires no
changes to the existing training procedure, and adds little
overhead for the prover seeking to prove they have trained a
model. We expect that future work will expand on the notion
of proof-of-learning introduced here, and propose improved
mechanisms applicable beyond the two scenarios which mo-
tivated our work (model stealing and distributed training).
ACKNOWLEDGMENTS
We thank the reviewers for their insightful feedback. This work
was supported by CIFAR (through a Canada CIFAR AI Chair),
by NSERC (under the Discovery Program, NFRF Exploration
program, and COHESA strategic research network), and by
gifts from Intel and Microsoft. We also thank the Vector
Institute‚Äôs sponsors. Varun was supported in part
through
the following US National Science Foundation grants: CNS-
1838733, CNS-1719336, CNS-1647152, CNS-1629833 and
CNS-2003129, and the Landweber fellowship.
REFERENCES
[2]
[1] C. Li. (Jun. 3, 2020). ‚ÄúOpenAI‚Äôs GPT-3 language model: A technical
overview,‚Äù Lambda Blog. Library Catalog: lambdalabs.com, [Online].
Available: https://lambdalabs.com/blog/demystifying-gpt-3/ (visited
on 10/01/2020).
S. Markidis, S. W. Der Chien, E. Laure, I. B. Peng, and J. S. Vetter,
‚ÄúNvidia tensor core programmability, performance & precision,‚Äù in
2018 IEEE International Parallel and Distributed Processing Sympo-
sium Workshops (IPDPSW), IEEE, 2018, pp. 522‚Äì531.
[3] N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa,
S. Bates, S. Bhatia, N. Boden, A. Borchers, et al., ‚ÄúIn-datacenter
performance analysis of a tensor processing unit,‚Äù in Proceedings of
the 44th Annual International Symposium on Computer Architecture,
2017, pp. 1‚Äì12.
[4] A. Putnam, A. M. CaulÔ¨Åeld, E. S. Chung, D. Chiou, K. Constan-
tinides, J. Demme, H. Esmaeilzadeh, J. Fowers, G. P. Gopal, J. Gray,
et al., ‚ÄúA reconÔ¨Ågurable fabric for accelerating large-scale datacen-
ter services,‚Äù in 2014 ACM/IEEE 41st International Symposium on
Computer Architecture (ISCA), IEEE, 2014, pp. 13‚Äì24.
F. Tram`er, F. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart, ‚ÄúSteal-
ing machine learning models via prediction apis,‚Äù in 25th {USENIX}
Security Symposium ({USENIX} Security 16), 2016, pp. 601‚Äì618.
[7]
[6] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. Berkay Celik,
and A. Swami, ‚ÄúPractical Black-Box Attacks against Machine Learn-
ing,‚Äù arXiv e-prints, arXiv:1602.02697, arXiv:1602.02697, Feb. 2016.
arXiv: 1602.02697 [cs.CR].
S. Pal, Y. Gupta, A. Shukla, A. Kanade, S. K. Shevade, and V.
Ganapathy, ‚ÄúA framework for the extraction of deep neural networks
by leveraging public data,‚Äù CoRR, vol. abs/1905.09165, 2019. arXiv:
1905.09165. [Online]. Available: http://arxiv.org/abs/1905.09165.
J. R. Correia-Silva, R. F. Berriel, C. Badue, A. F. de Souza, and
T. Oliveira-Santos, ‚ÄúCopycat cnn: Stealing knowledge by persuading
confession with random non-labeled data,‚Äù in 2018 International Joint
Conference on Neural Networks (IJCNN), IEEE, 2018, pp. 1‚Äì8.
T. Orekondy, B. Schiele, and M. Fritz, ‚ÄúKnockoff nets: Stealing
functionality of black-box models,‚Äù in Proceedings of
the IEEE
Conference on Computer Vision and Pattern Recognition, 2019,
pp. 4954‚Äì4963.
[8]
[9]
[11]
[17]
[16]
[14]
[10] M. Li, D. G. Andersen, J. W. Park, A. J. Smola, A. Ahmed, V.
Josifovski, J. Long, E. J. Shekita, and B.-Y. Su, ‚ÄúScaling distributed
machine learning with the parameter server,‚Äù in 11th {USENIX} Sym-
posium on Operating Systems Design and Implementation ({OSDI}
14), 2014, pp. 583‚Äì598.
P. Blanchard, R. Guerraoui, J. Stainer, et al., ‚ÄúMachine learning
with adversaries: Byzantine tolerant gradient descent,‚Äù in Advances
in Neural Information Processing Systems, 2017, pp. 119‚Äì129.
[12] C. Dwork and M. Naor, ‚ÄúPricing via processing or combatting junk
mail,‚Äù in Proceedings of the 12th Annual International Cryptology
Conference on Advances in Cryptology, ser. CRYPTO ‚Äô92, Berlin,
Heidelberg: Springer-Verlag, 1992, pp. 139‚Äì147, ISBN: 3540573402.
[13] M. Jakobsson and A. Juels, ‚ÄúProofs of work and bread pudding
protocols(extended abstract),‚Äù in Secure Information Networks, B.
Preneel, Ed., Boston, MA: Springer US, 1999, pp. 258‚Äì272, ISBN: