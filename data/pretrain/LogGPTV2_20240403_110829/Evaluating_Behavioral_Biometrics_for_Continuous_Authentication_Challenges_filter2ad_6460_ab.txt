[26]
[11]
[33]
[2]
[27]
[25]
[1]
[30]
[37]
[24]



()



5



()
()




()
5


()
()




()
5


()
()

()
()
()
()
()

()
()
()

()
()
()
()
()











()
()

()
()

()
()
()






()
()













Explicitly reported () Derived from other metric Not reported
Unless indicated otherwise, only the mean of each metric is reported
1 min, max, median
2 individually for each user
3 including conﬁdence intervals
4 as a function of number of users
5 as a function of number of samples
Table 1: Metrics used to evaluate continuous authentication systems. Basic measures such as FAR/FRR/EER
are reported by most papers while confusion matrices, which are most informative, are virtually never given.
with the number of users, the space requirements are high for
large number of users. In addition the CM is usually given
as a plot, which somewhat reduces the space requirement
but makes it diﬃcult to obtain more than estimates of the
actual numerical results.
Table 1 shows that the EER, as well as derived metrics,
are reported by the vast majority of papers, regardless of
the biometric. In addition, a plot of the ROC curve is given
in 16 out of the 25 reviewed papers, although the AUROC
is rarely given as a number (and could only theoretically be
extracted from the plot). Reporting of the detection rate
is extremely rare, and due to the unknown distribution of
errors between attackers it can not be derived from the FAR
either. A confusion matrix, which allows the derivation of all
other metrics, is only given in two papers, most likely due to
the high space requirements.
2.1.1 Limitations of common metrics
The EER (as well as the related metrics FAR, FRR, HTER
and accuracy) is often used to compare diﬀerent classiﬁers,
with the assumption being that a lower EER results in attack-
ers being detected more quickly (and more attackers being
detected overall) and users being rejected less frequently
(i.e., a better system). In the context of one-time (i.e., not
continuous) authentication this is a sensible and widely ac-
cepted metric. However, continuous authentication provides
a unique challenge as errors accumulate over the runtime
of the system. Without knowing the exact distribution, an
FAR of 10% could signify all attackers being detected 90%
of the time (resulting in eventual detection), or 10% of the
attackers never being detected while all others are exposed
immediately. The second scenario exhibits so-called system-
atic false-negatives. These diﬀerent scenarios are illustrated
in Figure 2. Unlike regular false negatives, which might be
randomly distributed across victim-attacker pairs as well as
across the time of a session, systematic false negatives are
tied to a combination of attacker and victim and are usually
more persistent or even permanent as a result of the behavior
388(a) Random Errors
(b) Systematic Errors
(a) Attacker Models
(b) Training Data Selec-
tion
Figure 2: Diﬀerent distributions of the FAR lead
to diﬀerent security challenges, random errors and
eventual detection (left) and systematic false neg-
atives (right). The grey line denotes the (identi-
cal) 9% FAR of both samples. Note that this ﬁgure
shows the success of a single attacker in impersonat-
ing multiple victims.
of two users being very similar. These types of errors are
more problematic from a security perspective, as the unde-
tected attackers can then access the compromised system for
a virtually unlimited time. Part of this property is captured
through the detection rate, which measures the fraction of
attackers with a non-zero FAR. However, the metric does
not account for the diﬀerence between undetected attackers
and those with simply a very high FAR. In practice this
might even be determined by a single sample being classiﬁed
diﬀerently. The confusion matrix paints a complete picture,
but it is neither compact enough to report for large datasets,
nor does it enable readers to easily compare two systems.
Most likely, these limitations are the reason it is rarely re-
ported in the literature. The authors of [6] propose to report
the number of undetected attackers along with the average
number of imposter actions (ANIA), a metric related to the
false accept rate. However, they recommend reporting only
the ANIA (which is, by deﬁnition, an average value), with
no regards for its distribution between attackers.
While systematic errors are problematic for the FAR, this
type of distribution might be desirable for false rejects. A
seemingly low, but non-zero false reject rate for all users
might still lead to frequent false alarms due to the base
rate fallacy [5] if the system is run continuously throughout
the day with a moderate sampling rate. If the false rejects
were concentrated on few users they could be authenticated
through other means (such as a diﬀerent biometric) instead,
without compromising security for the remaining users. In
addition, such a scenario allows the developer of a biomet-
ric recognition system to analyze why the system performs
poorly for precisely these users.
2.2 Common Evaluation Methodologies
A number of factors aﬀect the distinctive capabilities (and
thereby the security and usability) of a biometric system.
Prominent examples include the ability of the system to
collect high-quality data, the selection of distinctive features
and the classiﬁer itself. However, most papers analyze the
system on a static dataset, which requires the simulation of
training and operation, as well as the modelling of an attacker.
In this section we provide a summary of methodologies and
present an analysis on their prevalence in related work.
Hyperparameter Tuning. Following the feature extraction
and normalization, a suitable classiﬁer has to be chosen.
Figure 3: A large fraction of papers use random
training data selection and inclusion of imposter
data in the training set, both of which are likely
to underestimate error rates.
Depending on the classiﬁer, a number of hyperparameters
have to be instantiated. Such parameters include the number
of datapoints (the value of k) in the k-nearest-neighbors
algorithm and the kernel type and soft margin constant C of
a Support Vector Machine.
Attacker Model. Most biometrics are evaluated without
a committed attacker in mind, this is commonly referred
to as the zero-eﬀort threat model. As such, the “attacker”
is another user that attempts to access the victim’s system
without taking action to either circumvent the authentica-
tion system or impersonate the legitimate user. Even in this
simpliﬁed threat model, it is still necessary to test the sys-
tem’s performance in detecting intruders. This is commonly
achieved by comparing a user’s template against the samples
of all other users (i.e., the ”attackers”). An important concern
is the building of the user model itself. A common choice
is to train a binary classiﬁer with one user’s samples as the
positive class and samples from all other users (including
the eventual attacker) as a single combined negative class.
The system is then “attacked” individually by each of the
users that jointly form the negative class. This approach
means that reference data of the attacker is included in the
negative class, even though it only forms a fraction of the
overall class. In practice, it is impractical to assume that
reference data for each potential attacker is available (aside
from speciﬁc insider threat scenarios, such as [13]) and in-
cluding this data may lead to overestimating the classiﬁer’s
performance. A diﬀerent approach trains a generic attacker
model from other users (again, combining them into a nega-
tive class), but withholding samples from the actual attacker.
The authentication system could then be shipped with this
(anonymized) reference data. These two scenarios are also
considered in [6] and referred to as external and internal
scenarios, respectively. A more straight-forward approach is
to perform anomaly detection, which trains a model from
a single user’s data without the requirement of providing
samples for a negative class. New samples are then classiﬁed
based on how similar they are to the training examples.
Selection of Training Data. An operational authentication
system always requires reference data for each legitimate
user (training data) in order to classify new observations. In
practice, the initial training has to occur before any samples
can be classiﬁed (although the model can be updated based
on new observations). Consequently, a common approach to
simulate this setting is to use the ﬁrst part of the recorded
data as training data, and the remaining samples as test
389data. Another approach is to randomly sample the training
data from the entire dataset, and to use the remaining data
for testing. The sampling is often repeated to provide sta-
tistical robustness (either by performing several iterations
of random sampling or through cross validation). However,
this approach violates the requirement that training always
has to precede testing (as some training samples may have
been recorded after some testing samples).
Sample Aggregation. Single measurements of a feature
vector are often noisy (due to measurement noise or erratic
user behavior).
In order to combat this, several samples
can be combined to increase robustness. Samples can either
be combined before classiﬁcation (e.g., by computing the
component-wise mean of several feature vectors) or after-
wards (e.g., by majority votes). In the latter case, instead of
simply using the classiﬁer output, it is also possible to use
the classiﬁer conﬁdence for each class. Classiﬁer conﬁdence
can be measured as the distance to the decision boundary in
an SVM or the number of nearby examples of each class for
knn.
The complete results of our survey can be found in the
appendix. One of the most important observations is the
(apparent) reluctance of researchers to make their data freely
accessible online. However, it should be noted that our
survey only accounts for data that is both available online
and referenced in the corresponding paper. We have not
contacted individual authors and can not make any statement
on their willingness to share data on request. The number of
papers using and building on this shared data (most notably,
the data published as part of Touchalytics [16]) highlights
that this is a valuable contribution to the community. In a
similar fashion, the code used to generate the results is not
usually published. As a number of machine learning steps
depend on random numbers, this might make it particularly
diﬃcult to reproduce exact results, even if all decisions are
clearly stated and raw data is available.
While the speciﬁc values for hyperparameters are often
given, the process with which they were obtained is not usu-
ally explicitly described. This is problematic, as the selection
process is far more interesting (and the values used for an
individual datasets might not transfer well to others). In
addition, some processes (such as validating parameters on
the entire dataset, instead of just the training or develop-
ment set) might artiﬁcially improve reported results, without
resulting from a better system.
The vast majority of papers either do not use aggregation of
samples, or don’t report on the speciﬁcs of their mechanism.
If samples are aggregated, this is usually done following
classiﬁcation (i.e., not on a feature vector level).
2.2.1 Limitations of common methodologies
The previous section has shown that a wide variety of
methodologies are used to evaluate the static datasets, which
suggests that it might not be possible to directly compare
papers even if they use similar metrics. This would not nec-
essarily be a problem if the impact of diﬀerent methodologies
on the reported metrics were to be comparatively small. To
the best of our knowledge, this eﬀect has not been quantiﬁed
in the context of continuous authentication. It is, however,
well-studied in malware detection. Speciﬁcally, Allix et al.
have shown that sampling training data randomly from all
available data leads to systematic underestimation of error
rates [4, 3]. This is problematic, as reference data for future
malware helps in the classiﬁcation, but would not necessarily
be available in the real-world (i.e., to classify newly observed
malware). One might assume a similar eﬀect for continuous
authentication, as random training data selection would make
future samples available to help classifying past ones. This
allows the classiﬁer to accurately account for short and long
term changes in user behavior, which would not be possible
when maintaining the temporal integrity of the dataset.
9 out of 25 papers model the attacker by merging all users
but the legitimate one into a single negative class, with a
further 3 not giving information on their methodology (see
Figure 3). This approach is somewhat unrealistic, as it
assumes reference data for every potential attacker. While
this is possible in pure insider threat scenarios (such as
a company that wants to detect employees using their co-
workers’ systems), it is less realistic for other scenarios, such
as a stolen phone or any other kind of outside attacker. As
the attacker is merged with all other users into a single
negative class the eﬀect might be relatively small, especially
for datasets with larger numbers of users. However, the
impact of this potential source of additional information
for the classiﬁer has to be quantiﬁed in order to allow a
more informed comparison of papers. 13 papers exclude
the speciﬁc attacker from the training set, or only perform
anomaly detection (i.e., train the model without reference
data for any attackers), thereby escaping this problem.
Out of the 25 papers we analyzed (see Table 3 in the
appendix), 13 use at least one of these methodologies and a
further 6 don’t report the methodology used. As such, it is
crucial to quantify the precise impact of these choices and
adapt the state of the practice if necessary.
3. EFFECTS OF ERROR DISTRIBUTIONS
In order to evaluate the impact of the limitations outlined
in the previous section we require a number of diverse biomet-
ric datasets, all of which have to be suitable for continuous
authentication. Some diﬀerences in error distributions might
be due to the biometric, while some can be attributed to
speciﬁcs of a dataset. As such, we require datasets covering
multiple biometrics and ideally several datasets per biometric.
For this analysis we use 13 datasets collected by related work
and 3 datasets collected for this study. Details of the datasets
can be found in the appendix. In this section we investigate
the previously described sixteen datasets with regard to the
distribution of their errors. Based on the insights gained
from this analysis we will discuss a number of novel metrics
with regard to how well they capture these distributions.
3.1 Systematic Errors in the Wild
The most complete way to visualize the exact distribution
of errors (both FAR and FRR) is a confusion matrix. A
confusion matrix shows the fraction of accepted samples for
each combination of template and samples (see Figure 4
for an example). As such, the TPR (i.e., 1-FRR) is shown
on the diagonal and the remaining ﬁelds show the FAR for
each combination of attacker and victim. The confusion
matrix of an ideal system would be 1 on the diagonal and
0 otherwise. As discussed in Section 2, systematic false
negatives (i.e., attackers that consistently remain undetected)
are a more severe problem than a moderate, low-variance
FAR for all attackers. This is due to the nature of continuous