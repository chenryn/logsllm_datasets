our attack behave as input noise, thus triggering misclas-
siﬁcation into an “unknown” class. However, this is a
unique property of the Iris recognition task, and does not
apply to the other three tasks.
Mid-layer Feature Extractor. We then evaluate attack
on Trafﬁc Sign, where the ﬁrst 10 layers are transferred
from Teacher and frozen during training. Here the per-
turbation budget is ﬁxed to P = 0.005. Results in Fig-
ure 5(c) show that the attack success rates peak at pre-
cisely the 10th layer, where success rate for targeted at-
tack is 43.7% and 95.35% for non-targeted attack. Sim-
ilarly, the success rates reduce when the attacker targets
shallow layers. Interestingly, the success rates also de-
crease as we target layers deeper than 10. This is be-
cause layers beyond 10 are ﬁne-tuned and more distinct
from the corresponding Teacher layers, leading to higher
error when mimicking the internal representation.
Full Model Fine-tuning.
For the Flower task, the
Student model differs largely from the Teacher model,
as all the layers are ﬁne-tuned. Therefore, the attacker
will always use incorrect information (from the Teacher)
to mimic an internal representation of the Student. The
resulting attack success rates are low and ﬂat across the
choice of attack layers (Figure 5(d) with P = 0.003).
USENIX Association
27th USENIX Security Symposium    1287
How to Choose the Attack Layer?
The above re-
sults suggest that the attacker should always try to iden-
tify if the Student is using Deep-layer Feature Extractor,
as it remains the most vulnerable approach. In Section 5,
we present a technique to determine whether Deep-layer
Feature Extractor is used for transfer and to identify
the Teacher model, using a few queries on the Student
model.
In this case, the attacker should focus on the
N − 1th layer to achieve the optimal attack performance.
If the Student is not using Deep-layer Feature Extrac-
tor, the attacker can try to ﬁnd the optimal attack layer
by iteratively targeting different layers, starting from the
deepest layer. In the case of Mid-layer Feature Extrac-
tor, the attacker can estimate the attack success rate at
each layer, using only a small set of image pairs and very
limited queries. The attacker can observe the attack suc-
cess rate increasing (or decreasing) as she approaches (or
moves away from) the optimal layer.
4.4 Discussion
Feature Extractor vs. Full Model Fine-tuning.
Our
results suggest that Full Model Fine-tuning and Mid-
layer Feature Extractor lead to models that are more ro-
bust against our attacks. However, in practice, these two
approaches are often not applicable, especially when the
Student training data is limited. For example, for Face
recognition, when reducing the training dataset from 90
images per class to 50 per class, pushing back by 2 lay-
ers (i.e. transfer at layer 13) reduces the model classiﬁ-
cation accuracy to 19.1%. Meanwhile, Deep-layer Fea-
ture Extractor still achieves a 97.69% classiﬁcation ac-
curacy. Apart from performance, these approaches also
incur higher training cost than Deep-layer Feature Ex-
tractor. This is also why many deep learning frameworks
today use Deep-layer Feature Extractor as the default
conﬁguration for transfer learning.
Can white-box attacks on Teacher transfer to student
Models?
Prior work identiﬁed the transferability of
adversarial samples across different models for the same
task [38]. Thus another potential attack on transfer learn-
ing is to use existing white-box attacks on the Teacher to
craft adversarial samples, which are then transferred to
the Student. We evaluate this attack using the state-of-
the-art white-box attack by Carlini et al.
[17]. Since
Teacher and Student models have different class labels,
we can only perform non-targeted attacks.
Our results show that the resulting attack is ineffec-
tive for all four tasks: only  83.1% classiﬁcation accuracy.
We measure the dispersion of S(x) using the Gini coef-
ﬁcient, commonly used in economics to measure wealth
distribution [26]. Its value ranges between 0 and 1, with 0
representing complete equality and 1 representing com-
plete inequality.
We ﬁrst measure the Gini coefﬁcient of BN , validating
our hypothesis that BN ’s dispersion level is very low. For
each Student model, we set output neurons of N − 1th
layer as a zero vector, so that only BN is fed into the ﬁnal
prediction. For all seven models, the corresponding Gini
coefﬁcient is below 0.011. We then feed 100 random
test images into each model, where the Gini coefﬁcient
jumps to between 0.648 and 0.999, with a median value
of 0.941. This conﬁrms our hypothesis where BN has a
different statistical dispersion than normal S(x).
Next, for each candidate Teacher model, we craft and
feed 10 ﬁngerprinting images to the target student model
and compute the average Gini coefﬁcient of S(x). Fig-
S(x), but we ignore it for the sake of simplicity. Our methodology
holds for any activation function.
3Our choice of Teacher models includes VGG16 [56], VGG19 [56],
ResNet50 [28], InceptionV3 [59], Inception-ResNetV2 [58], and Mo-
bileNet [32].
4This is a smaller version of the full 102-class ﬂower dataset we
used in previous experiments [10].
d
e
s
U
r
e
h
c
a
e
T
l
a
u
t
c
A
MobileNet
ResNet50
Inception-
ResNetV2
InceptionV3
VGG19
VGG16
0.548
0.584
0.574
0.576
0.572
0.508
0.041
0.967
0.975
0.969
0.966
0.966
0.015
0.965
0.715
0.717
0.731
0.743
0.030
0.741
0.720
0.821
0.834
0.846
0.058
0.818
0.828
0.824
0.930
0.941
0.042
0.932
0.927
0.912
0.919
0.951
0.011
0.964
0.958
0.959
0.964
0.959
VGG-Face
0.004
0.501
0.501
0.445
0.446
0.455
0.443
 1
 0.8
 0.6
 0.4
 0.2
 0
t
i
n
e
c
i
f
f
e
o
C
i
i
n
G
6
1
G
G
V
9
1
G
G
V
e
c
a
F
-
G
G
V
3
V
n
o
i
t
p
e
c
n
I
-
n
o
i
t
p
e
c
n
I
2
V
t
e
N
s
e
R
0
5
t
e
N
s
e
R
t
e
N
e
l
i
b
o
M
Teacher Model Candidate
Figure 6: Gini coefﬁcient of output probabilities of dif-
ferent teacher and student models.
ure 6 shows the average Gini coefﬁcient as a function of
the ﬁngerprinting Teacher model and the Teacher model
used to generate the Student model. The diagonal line in-
dicates scenarios where the two Teacher models match.
As expected, all the coefﬁcients along the diagonal are
small (
0.443), since the Teacher model used to generate the ﬁn-
gerprinting image does not match that used to generate
the student model.
It is worth noting that our ﬁngerprinting technique
can also identify different versions of Teacher models
with the same architecture. To demonstrate this, we use
Google’s InceptionV3 model that has two versions (i.e.
with different weights) released at different times.5. Our
technique accurately distinguishes between these two
versions, with a Gini coefﬁcient  0.751 otherwise.
Overall, the above results conﬁrm that our ﬁngerprint-
ing method can identify the Teacher model using a small
set of queries. When crafting the ﬁngerprinting image, a
threshold of 0.1 on the Gini coefﬁcient seems like a good
cut-off to ensure successful ﬁngerprinting.
Effectiveness on Other Transfer Methods. Our ﬁn-
gerprinting method is based on nullifying neuron con-
tributions to the last layer of the Student model.
It is
effective when the student model is generated by Deep-
layer Feature Extractor. The same set of ﬁngerprinting
images, when fed to student models generated by other
transfer methods, will likely lead to higher Gini coefﬁ-
cients and fail to identify the Teacher model. For exam-
ple, when fed to the Trafﬁc Sign and Flower models, the
Gini coefﬁcient is always higher than 0.839.
On the other hand, when all the ﬁngerprinting images
5Version
2015-12-05
http://download.tensorflow.
org/models/image/imagenet/inception-2015-
12-05.tgz,
tensorflow.org/models/inception_v3_2016_08_28.
tar.gz
http://download.
Version
2016-08-28
USENIX Association
27th USENIX Security Symposium    1289
lead to large Gini coefﬁcient values, it means that ei-
ther the Teacher model is unknown (not in the candi-
date pool), or the student model is produced by a trans-
fer method other than Deep-layer Feature Extractor. For
both cases, the misclassiﬁcation attack will be less effec-
tive. The attacker can use this knowledge to identify and
target student models that are the most vulnerable to the
misclassiﬁcation attack.
5.2 Attacks on Transfer Learning Services
Today, popular Machine Learning as a service (MLaaS)
platforms [67] (e.g., Google Cloud ML) and deep learn-
ing libraries (e.g., PyTorch, Microsoft CNTK) already
recommend transfer learning to their customers. Many
provide detailed tutorials to guide customers through the
process of transfer learning. We follow these tutorials
to investigate whether the resulting Student models are
vulnerable to our attacks. The adversarial samples gen-
erated on the three services are listed in Figure 13 in the
Appendix.
Google Cloud ML.
In this MLaaS platform, users can
train deep learning models in the cloud and maintain it as
a service. The transfer learning tutorial explains the pro-
cess of using Google’s InceptionV3 image classiﬁcation
model to build a ﬂower classiﬁcation model [5].
Speciﬁcally, the tutorial suggests Deep-layer Feature
Extractor as the default transfer learning method, and the
provided sample code does not offer control parameters
or guidelines to use other transfer approaches or Teacher
models (one has to modify the code to do so). We follow
the tutorial to train a Student model on a 5-class ﬂower
dataset (the example dataset used in the tutorial), which
achieves an 89.3% classiﬁcation accuracy6.
To launch the attack on the Student model, we ﬁrst
use the proposed ﬁngerprinting method to identify that