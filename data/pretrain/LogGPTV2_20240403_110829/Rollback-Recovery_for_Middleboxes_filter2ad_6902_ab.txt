(3) Fast Recovery. Finally, recovery from failures must be
fast to prevent degradation in the end-to-end protocols and
applications. We aim for recovery times that avoid endpoint
protocols like TCP entering timeout or reset modes.
In addition, we seek solutions that obey the following two
supplemental requirements:
(4) Generality. We prefer an approach that does not require
complete rewriting of middlebox applications nor needs to
be tailored to each middlebox application. Instead, we pro-
pose a single recovery mechanism and assume access to the
source code. Our solution requires some annotations and
and automated modiﬁcations to this code. Thus, we dif-
fer from some recent work [50, 51] in not introducing an
entirely new programming model, but we cannot use com-
pletely untouched legacy code. Given that middlebox ven-
dors are moving their code from their current hardware to
NFV-friendly implementations, small code modiﬁcations of
the sort we require may be a reasonable middle ground.
(5) Passive Operation. We do not want to require dedicated
replicas for each middlebox application, so instead we seek
solutions that only need a passive replica that can be shared
across active master instances.
3.2 Existing Middleboxes
To our knowledge, no middlebox design in research or
deployment simultaneously meets the above goals.4
In research, Pico [50] was the ﬁrst to address fault-
tolerance for middleboxes. Pico guarantees correct recovery
but does so at the cost of introducing non-trivial latency un-
der failure-free operation – adding on the order of 8-9ms of
delay per packet. We describe Pico and compare against it
experimentally in §6.
There is little public information about what commercial
middleboxes do and therefore we engaged in discussions
with two different middlebox vendors. From our discus-
sions, it seems that vendors do rely heavily on simply en-
gineering the boxes to not fail (which is also the only ap-
proach one can take without asking customers to purchase a
separate backup box). For example, one vendor uses only a
single line of network interface cards and dedicates an entire
engineering team to testing new NIC driver releases.
Both vendors conﬁrmed that shared state commonly oc-
curs in their systems. One vendor estimated that with their
IDS implementation, a packet touches 10s of shared vari-
ables per packet, and that even their simplest devices incur
at least one shared variable access per packet.
4Traditional approaches to reliability for routers and switches do little to
address statefulness as there is no need to do so, and thus we do not discuss
such solutions here.
229Somewhat to our surprise, both vendors strongly rejected
the idea of simply resetting all active connections after fail-
ure, citing concerns over the potential for user-visible dis-
ruption to applications (we evaluate cases of such disruption
in §6). Both vendors do attempt stateful recovery but their
mechanisms for this are ad-hoc and complex, and offer no
correctness guarantee. For example, one vendor partially ad-
dresses statefulness by checkpointing select data structures
to stable storage; since checkpoints may be both stale and
incomplete (i.e., not all state is checkpointed) they cannot
guarantee correct recovery. After recovery, if an incoming
packet is found to have no associated ﬂow state, the packet
is dropped and the corresponding connection reset; they re-
ported using a variety of application-speciﬁc optimizations
to lower the likelihood of such resets. Another vendor of-
fers an ‘active:active’ deployment option but they do not ad-
dress non-determinism and offer no correctness guarantees;
to avoid resetting connections their IDS system ‘fails open’ –
i.e., ﬂows that were active when the IDS failed bypass some
security inspections after failure.
Both vendors expressed great interest in general mecha-
nisms that guarantee correctness, saying this would both im-
prove the quality of their products and reduce the time their
developers spend reasoning through the possible outcomes
of new packets interacting with incorrectly restored state.
However, both vendors were emphatic that correctness
could not come at the cost of added latency under failure-free
operation and independently cited 1ms as an upper bound
on the latency overhead under failure-free operation.5 One
vendor related an incident where a trial product that added
1-2ms of delay per-packet triggered almost 100 alarms and
complaints within the hour of its deployment.
Finally, both vendors emphasized avoiding the need for
1:1 redundancy due to cost. One vendor estimated a price
of $250K for one of their higher-grade appliances; the au-
thors of [58] report that a large enterprise they surveyed
deployed 166 ﬁrewalls and over 600 middleboxes in total,
which would lead to multi million dollar overheads if the
dedicated backup approach were applied broadly.
3.3 Design Options
Our goal is to provide stateful recovery that is correct in
the face of nondeterminism, yet introduces low delay under
both failure-free and post-failure operation. While less ex-
plored in networking contexts, stateful recovery has been ex-
tensively explored in the general systems literature. It is thus
natural to ask what we might borrow from this literature. In
this section, we discuss this prior work in broad terms, fo-
cusing on general approaches rather than speciﬁc solutions,
and explain how these lead us to the approach we pursued
with FTMB. We discuss speciﬁc solutions and experimen-
tally compare against them in §6.
At the highest level approaches to stateful recovery can
be classiﬁed based on whether lost state is reconstructed by
replaying execution on past inputs. As the name suggests,
5This is also consistent with carrier requirements from the Broadband Fo-
rum which cite 1ms as the upper bound on forwarding delay (through BGN
appliances) for VoIP and other latency-sensitive trafﬁc [14].
solutions based on ‘replay’ maintain a log of inputs to the
system and, in the event of a failure, they recreate lost state
by replaying the inputs from the log; in contrast, ‘no-replay’
solutions do not log inputs and never replay past execution.
As we will discuss in this section, we reject no-replay so-
lutions because they introduce high latencies on per-packet
forwarding – on the order of many milliseconds. However,
replay-based approaches have their own challenges in sus-
taining high throughput given the output frequency of mid-
dleboxes. FTMB follows the blueprint of rollback-recovery,
but introduces new algorithms for logging and output com-
mit that can sustain high throughput.
3.4 No-Replay Designs
No-replay approaches are based on the use of system
checkpoints: processes take periodic “snapshots” of the nec-
essary system state and, upon a failure, a replica loads the
most recent snapshot. However, just restoring state to the last
snapshot does not provide correct recovery since all execu-
tion beyond the last snapshot is lost – i.e., the output commit
property would be violated for all output generated after the
last snapshot. Hence, to enforce the output commit property,
such systems buffer all output for the duration between two
consecutive snapshots [23]. In our context, this means pack-
ets leaving the middlebox are buffered and not released to
the external world until a checkpoint of the system up to the
creation of the last buffered packet has been logged to stable
storage.
Checkpoint-based solutions are simple but delay outputs
even under failure-free operation; the extent of this delay
depends on the overhead of (and hence frequency between)
snapshots. Several efforts aim to improve the efﬁciency of
snapshots – e.g., by reducing their memory footprint [50],
or avoiding snapshots unless necessary for correctness [28].
Despite these optimizations, the latency overhead that these
systems add – in the order of many milliseconds – remains
problematically high for networking contexts. We thus reject
no-replay solutions.
3.5 Replay-Based Designs
In replay-based designs,
the inputs to the system are
logged along with any additional information (called ‘de-
terminants’) needed for correct replay in the face of non-
determinism. On failure, the system simply replays execu-
tion from the log. To reduce replay time and storage re-
quirements these solutions also use periodic snapshots as an
optimization: on failure, replay begins from the last snap-
shot rather than from the beginning of time. Log-based re-
play systems can release output without waiting for the next
checkpoint so long as all the inputs and events on which that
output depends have been successfully logged to stable stor-
age. This reduces the latency sensitive impact on failure-
free operation making replay-based solutions better suited
for FTMB.
Replay-based approaches to system recovery should not
be confused with replay-based approaches to debugging.
The latter has been widely explored in recent work for de-
bugging multicore systems [16, 41, 61]. However, debug-
230ging systems do not provide mechanisms for output commit,
the central property needed for correct recovery – they do
not need to, since their aim is not to resume operation af-
ter failure. Consequently, these systems cannot be used to
implement high availability. 6
Instead,
With all recovery approaches,
the most relevant work to our goals comes
from the classic distributed systems literature from the 80s
and 90s, targeting rollback-recovery for multi-process dis-
tributed systems (see [31] for an excellent survey). Unfortu-
nately, because of our new context (a single multi-threaded
server, rather than independent processes over a shared net-
work) and performance constraints (output is released ev-
ery few microseconds or nanoseconds rather than seconds
or milliseconds), existing algorithms from this literature for
logging and output commit cannot sustain high throughput.
the system must check
that all determinants – often recorded in the form of vector
clocks [42] or dependency trees [30] – needed for a given
message to be replayed have been logged before the mes-
sage may be released. This check enforces the output com-
In systems which follow an optimistic log-
mit property.
ging approach, this output commit ‘check’ requires coor-
dination between all active process/threads every time out-
put is released. This coordination limits parallelism when
output needs to be released frequently. For example, in §5
we discuss a design we implemented following the opti-
mistic approach which could sustain a maximum throughput
of only 600Mbps (where many middleboxes process traf-
ﬁc on the order of Gbps) due to frequent cross-core coor-
dination. Other systems, which follow a causal logging ap-
proach, achieve coordination-free output commit and better
parallelism, but do so by permitting heavy redundancy in
what they log: following the approach of one such causal
system [30], we estimated that the amount of logged deter-
minants would reach between 500Gbps-300Tbps just for a
10Gbps of packets processed on the dataplane. Under such
loads, the system would have to devote far more resources to
recording the logs themselves than processing trafﬁc on the
dataplane, once again limiting throughput.
Hence, instead of following a standard approach, we in-
stead designed a new logging and output commit approach
called ordered logging with parallel release. In the follow-
ing section, we describe how our system works and why or-
dered logging with parallel release overcome the issues pre-
sented by previous approaches.
4. DESIGN
FTMB is a new solution for rollback recovery, tailored to
the middlebox problem domain through two new solutions:
1. ‘ordered logging’: an efﬁcient mechanism for repre-
senting and logging the information required for cor-
6A second question is whether or not we can adopt logging and instrumen-
tation techniques from these systems to detect determinants. However, as
we discuss experimentally in §6, most debugging approaches rely on heavy-
weight instrumentation (e.g., using memory protection to intercept access
to shared data) and often logging data that is unnecessary for our use cases
(e.g., all calls to malloc) – this leads to unnecessarily high overheads.
Master
Input
Logger
Stable
storage:
in/out
packets,
PALs,
snapshots
Output
Logger
Backup
Figure 2: Architecture for FTMB.
rect replay; ordered logging represents information in
such a way that it is easy to verify the output commit
property.
2. ‘parallel release’: an output commit algorithm that is
simple and efﬁcient to implement on multicore ma-
chines.
The architecture of FTMB is shown in Figure 2. A master
VM runs the middlebox application(s), with two loggers that
record its input and output trafﬁc. Periodic system snapshots
are sent from the master to stable storage, and used to start a
backup in case the master crashes. In our prototype, the mas-
ter and backup are two identical servers; the input and out-
put loggers are software switches upstream and downstream
from the master node; and the stable storage is volatile mem-
ory at the downstream switch – the storage is ‘stable’ in that
it will survive a failure at the master, even though it would
not survive failure of the switch it resides on. 7
As explained in earlier sections, the crux of ensuring cor-
rect recovery is enforcing the output commit property which,
for our context, can be stated as: do not release a packet un-
til all information needed to replay the packet’s transmission
has been logged to stable storage. Enforcing this property
entails answering the following questions:
• What information must we log to resolve potential
nondeterminism during replay? In the language of roll-
back recovery protocols this deﬁnes what the literature
calls determinants.
• How do we log this information efﬁciently? This spec-
iﬁes how we log determinants.
• What subset of the information that we log is a given
packet dependent on for replay? This deﬁnes an out-
put’s dependencies.
• How do we efﬁciently check when an individual
packet’s dependencies have been logged to stable stor-
age? This speciﬁes how we check whether the output
commit requirements for an output have been met.
7There is some ﬂexibility on the physical placement of the functions; our
system can withstand the failure of either the middlebox (Master/Backup)
or the node holding the saved state but not both simultaneously. We envis-
age the use of “bypass” NICs that fail open on detecting failure, to survive
failures at the loggers [4].
231We now address each question in turn and present the archi-
tecture and implementation of the resultant system in §5.
4.1 Deﬁning Determinants
Determinants are the information we must record in order
to correctly replay operations that are vulnerable to nonde-
terminism. As discussed previously, nondeterminism in our
system stems from two root causes: races between threads
accessing shared variables, and access to hardware whose
return values cannot be predicted, such as clocks and ran-
dom number generators. We discuss each of them below.
Shared State Variables. Shared variables introduce the
possibility of nondeterministic execution because we can-
not control the order in which threads access them.8 We
thus simply record the order in which shared variables are
accessed, and by whom.
Each shared variable vj is associated with its own lock
and counter. The lock protects accesses to the variable, and
the counter indicates the order of access. When a thread
processing packet pi accesses a shared variable vj, it cre-
ates a tuple called Packet Access Log (PAL) that contains
(pi, nij, vj, sij) where nij is the number of shared variables
accessed so far when processing pi, and sij is the number of
accesses received so far by vj.
As an example, ﬁgure 3 shows the PALs generated by the
four threads (horizontal lines) processing packets A, B, C, D.
For packet B, the thread ﬁrst accesses variable X (which has
previously been accessed by the thread processing packet
A), and then variable Y (which has previously been accessed
by the thread processing packet C).
Note that PALs are created independently by each thread,