### Parameter Space and Domain of Training Data

The parameter space, denoted as \(\Theta\), of the released model, and its outputs, lie within the domain \(Z\) of the training data. Both can typically be represented using numerical vectors. The adversary leverages their knowledge of the dataset \(D-\) and the attack algorithm \(A\), along with side information in the form of shadow target points \(\bar{D}\) (disjoint from \(D-\)), to generate a collection of shadow models. These shadow models and their corresponding targets form the training data for the Reconstructor Network (RecoNN). The RecoNN is then applied to the released model to obtain a candidate reconstruction \(\hat{z}\) for the previously unseen target point \(z\).

### Training Reconstructor Networks

Consider an informed adversary in our threat model (Definition 1). As side information about \(z\), we assume the attacker has \(k\) additional shadow targets \(\bar{D} = \{\bar{z}_1, \ldots, \bar{z}_k\}\) from \(Z\). Ideally, these points should be sampled from the same distribution as the target point \(z\) to ensure the RecoNN's statistical generalization. However, our experimental evaluation shows that this requirement is not strictly necessary for achieving good reconstructions (Section VI-B). The general reconstruction attack proceeds as follows:

1. **Training Shadow Models:**
   For \(i = 1, \ldots, k\), train a shadow model \(\bar{\theta}_i = A_{D-}(\bar{z}_i)\) on the fixed dataset \(D-\) plus the \(i\)-th shadow target from the adversaryâ€™s side knowledge pool \(\bar{D}\). The collection of shadow model-target pairs \(S = \{(\bar{\theta}_i, \bar{z}_i)\}_{i=1}^k\) serves as the attack training data.

2. **Training the RecoNN:**
   Train a RecoNN \(\phi\) using \(S\) as examples of successful reconstructions. We denote the training algorithm used by the adversary as \(R\): \(\phi = R(S)\).

3. **Obtaining the Reconstruction Candidate:**
   Apply the RecoNN to the target model to obtain a reconstruction candidate: \(\hat{z} = \phi(\theta)\).

### Dataset Splits and Model Training

**a) Dataset Splits:**
We split each dataset into three disjoint parts:
- Fixed dataset (\(D-\))
- Shadow dataset (\(\bar{D}\))
- Test targets dataset (1K points for both MNIST and CIFAR-10)

We train one released model per test target and report the average performance of our attack across all test targets.

**b) Released Model Training:**
The training algorithm for both released and shadow models is standard gradient descent with momentum. By default, we use full batches (no mini-batch sampling) to keep the algorithm deterministic. Additionally, we assume the adversary knows the model initialization step, so both released and shadow models are trained from the same starting point. We explore the effects of mini-batching and random initialization separately in Section VI-B.

- **Architecture:**
  - MLP for MNIST
  - CNN for CIFAR-10

- **Performance:**
  - MNIST: Over 94% accuracy with a generalization gap less than 1%
  - CIFAR-10: 40% accuracy with a generalization gap of 5%

The subpar performance on CIFAR-10 is partly due to the limited training data (only 10% of the standard dataset). In Section VI-B, we experiment with a larger CIFAR-10 fixed set size (50K), where the released models achieve approximately 50% test accuracy. Reconstructing CIFAR-10 targets is more challenging due to the richer, more complex structure of the images and the larger underlying model.

**c) Reconstructor Network Training:**
When training the reconstructor, shadow model parameters across layers are flattened and concatenated. Each coordinate is re-scaled to have zero mean and unit variance. For MNIST, we use a combination of mean absolute error (MAE) and mean squared error (MSE) as the training objective. For CIFAR-10, we add LPIPS loss [29] and a GAN-like Discriminator loss to improve the visual quality of reconstructed images. We use a patch-based Discriminator [30] with the architecture given in Table VII, trained using MSE loss with a learning rate of \(10^{-5}\). The discriminator aims to distinguish shadow targets from reconstructor-generated candidates. At a high level, the reconstructor network can be viewed as a generative model with a latent space defined over a distribution of shadow models, enabling the application of Generative Adversarial Networks (GANs) training techniques.

### Experimental Setup

**A. Default Settings:**
We evaluate our reconstruction attacks on the MNIST and CIFAR-10 datasets using fully connected (MLP) and convolutional neural networks (CNN) as the released and shadow models. Our experiments investigate the influence of training hyperparameters on the effectiveness of reconstruction. Default model architectures and hyperparameters are summarized in Table IV.

**B. Criteria for Attack Success:**
We use several evaluation metrics to capture various aspects of information leakage from reconstruction attacks. When reporting an average metric, we measure the performance of a single reconstructor network on 1K released model and target point pairs.

- **Mean Squared Error (MSE):** Reports the MSE between a target and its reconstruction.
- **LPIPS:** Measures image similarity based on deep feature representations from visual models trained with human annotations.
- **KL Divergence:** Measures the similarity between the outputs of a highly accurate classifier on the target and reconstructed image.
- **Nearest Neighbor Oracle:** Contextualizes MSE reconstruction metrics by guessing the point \(\hat{z} \in D- \cup \bar{D}\) with the smallest MSE distance to \(z\).

### Empirical Studies in Reconstruction

We conduct extensive experiments to investigate how the released model architecture and its training hyperparameters impact reconstruction quality. We first demonstrate the feasibility of the reconstruction attack under default settings, then discuss factors affecting reconstruction success, and finally investigate differential privacy (DP) as a mitigation against reconstruction attacks.

**A. Feasibility of Reconstruction Attacks:**
Under default experimental settings, we observe good overall reconstruction quality on both datasets. Running the attack against 1K test targets, we observe an average reconstruction MSE of 0.0089 (MNIST) and 0049 (CIFAR-10), demonstrating the effectiveness of our attack compared to the NN oracle baselines.

**B. Factors Affecting Reconstruction:**
We study which factors may improve or impact reconstruction success, summarized in Table II. These include the size and architecture of the fixed set, the number of released model layers, the number of training epochs, activation functions, learning rates, random initialization, and model access.

- **Attack Training Set Size:**
  The general reconstruction attack assumes the attacker has access to \(k\) shadow data points \(\bar{D}\) from the same distribution as the target point. The size of the attack training data depends on the attacker's knowledge and computational power, as they need to train one shadow model per data point.