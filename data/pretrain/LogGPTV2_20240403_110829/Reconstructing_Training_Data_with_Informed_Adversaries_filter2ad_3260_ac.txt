the parameter space Θ of the released model and outputs lie
in the domain Z of the training data; typically we can encode
both using numerical vectors. The adversary then uses its
knowledge of D- and A, together with side knowledge in the
form of shadow target points ¯D disjoint from D-, to generate
a collection of shadow models. These shadow model and
target pairs comprise the training data for the RecoNN, which
is then applied to the released model to obtain a candidate
reconstruction ˆz for the (previously unseen) target point z.
B. Training Reconstructor Networks
Consider an informed adversary in our threat model (Defi-
nition 1). As side knowledge about z, we assume the attacker
has k additional shadow targets ¯D = {¯z1, . . . , ¯zk} from Z.
Ideally, if we think that the attack’s success will depend on
the RecoNN’s ability to exhibit statistical generalization, these
points would be sampled from the same distribution as the
target point z. Nonetheless, we will see in our experimental
evaluation that this requirement is not strictly necessary to
achieve good reconstructions (Section VI-B). The general
reconstruction attack proceeds as follows (see also Figure 2):
1) For i = 1, . . . , k, train model ¯θi = AD- (¯zi) on the fixed
dataset plus the ith shadow target from the adversary’s
side knowledge pool ¯D. Together, we refer to the collec-
tion of shadow model-target pairs S = {(¯θi, ¯zi)}k
i=1 as
the attack training data.
2) Train a RecoNN ϕ using S as examples of successful
reconstrutions. Abusing our notation, we use R to denote
the training algorithm used by the adversary: ϕ = R(S).
3) Obtain a reconstruction candidate by applying the Re-
coNN to the target model: ˆz = ϕ(θ).
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:22:08 UTC from IEEE Xplore.  Restrictions apply. 
1142
a) Dataset splits: We split each dataset into three disjoint
parts: fixed dataset (D-), shadow dataset ( ¯D), and test targets
dataset; the latter contains 1K points, both for MNIST and
CIFAR-10. We train one released model per test target and
report average performance of our attack across test targets.
b) Released model training: The training algorithm for
released and shadow models is standard gradient descent with
momentum. By default, we use full batches (i.e. no mini-batch
sampling) to keep the algorithm deterministic. Additionally, by
default we assume the adversary knows the model initialization
step, so both released and shadow models are trained from the
same starting point. We explore the effect of mini-batching and
random initialization separately in Section VI-B.
The architecture is an MLP for MNIST and a CNN for
CIFAR-10. On average, the released models achieve over 94%
accuracy on MNIST and 40% on CIFAR-10 without significant
overfitting (generalization gap is less 1% on MNIST and 5%
on CIFAR-10). The reason for the subpar performance on
CIFAR-10 is partially2 because the models are trained with
only 10% of the data used in standard evaluations – this
constraint comes from the need to reserve a large disjoint
set of shadow points to train RecoNN. We experiment with
a larger CIFAR-10 fixed set size (50K) in Section VI-B; in
this setting the released models achieve ∼ 50% test accuracy.
We expect reconstructing CIFAR-10 targets will be a more
challenging task than MNIST. CIFAR-10 images have a richer,
more complex structure, and so capturing and reconstructing
the intricacies of such an image may be difficult. Additionally,
the underlying released model is larger; hence: 1) a larger
reconstructor network is required, which comes with higher
computational costs for the adversary; 2) the shadow dataset
may need to be larger, to facilitate learning on high dimen-
sional data (i.e. on the shadow models’ weights).
c) Reconstructor network training: When training the
reconstructor, shadow model parameters across layers are
flattened and concatenated together. We also re-scale each
coordinate in this representation to zero mean and unit vari-
ance; we found this pre-processing step to be important, as
some of the parameters can be extremely small. For MNIST,
we use a mean absolute error (MAE) + mean squared error
(MSE) loss between shadow targets and reconstructor outputs
as the training objective. For CIFAR-10 we modify the re-
constructor training objective by adding an LPIPS loss [29]
and a GAN-like Discriminator loss to improve visual quality
of reconstructed images. We use a patch-based Discriminator
[30] with the architecture given in Table VII, and train it using
mean squared error loss [31] and a learning rate of 10−5. The
patch-based discriminator aims to distinguish shadow targets
from reconstructor generated candidates. At a high-level, we
can view the reconstructor network as a generative model with
a latent space defined over a distribution of shadow models;
this enables us to apply ideas from Generative Adversarial
Networks (GANs) training. Our discriminator training set-up
2Training without random mini-batches, no regularization and a small CNN
architecture also contribute to this effect.
Fig. 2: Overview of RecoNN-based attack.
In all our experiments, we consider classification tasks
where z = (x, y) ∈ X × Y with X ⊂ Rd and Y is a finite
set of labels. We also make the simplifying assumption that y
can be inferred from x, and focus only on reconstructing x.
Related work: The idea of using “neural networks to
attack neural networks” has been used in the literature to
implement a number of attacks,
including (black-box and
white-box) membership inference [5, 19, 20], model inversion
[18], and property inference [12, 13]. Our use of RecoNNs
is related to [12], where an invariant representation of a
released neural network parameters is fed into another neural
network to perform a PIA, although the output of our attack
is often a high-dimensional object (e.g. an image) instead of
single scalar. In preliminary experiments we did not see an
improvement from using this invariant representation as a pre-
processing step; standard normalization was sufficient for a
successful attack. Similarly, the use of shadow models trained
by the adversary to imitate the behavior of the released model
is a common approach in MIA and AIA, although most works
do not consider an informed adversary with knowledge of
D-. Despite the attack being an instantiation of the shadow
model technique, it is not a foregone conclusion that this
approach will work for reconstruction attacks. Reconstruction
is a more difficult task than membership inference, and it
entails a considerable amount of engineering, data curation,
and ML training insight to carry out, as we will discuss.
V. EXPERIMENTAL SETUP
We discuss the default experimental settings, and how we
will evaluate reconstruction attacks.
A. Default Settings
We evaluate our reconstruction attacks on the MNIST and
CIFAR-10 datasets using fully connected (i.e. multi-layer
perceptron) and convolutional neural networks (CNN) as the
released (and shadow) models. Our experiments investigate
the influence that training hyperparameters for A have on the
effectiveness of reconstruction. Default model architectures
and hyperparameters for both released and reconstructor mod-
els are summarized in Table IV. Most of these choices are
standard and were selected based on preliminary experiments.
In the following we highlight the most important details.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:22:08 UTC from IEEE Xplore.  Restrictions apply. 
1143
(a) MNIST. | ¯D ∪ D-| = 69K.
(b) CIFAR-10. | ¯D ∪ D-| = 59K.
Fig. 3: For each test target compute the MSE to all points in
adversary’s available pool of data ( ¯D∪D-) in the default setting.
Plot the histogram of MSEs (averaged over all test targets) along
with some highlighted order statistics.
is as in [30] – we alternate between one gradient descent step
on the discriminator, and one step on the reconstructor net-
work. From visual inspection, we found using a discriminator
improves sharpness of CIFAR-10 reconstructed images, even
if it does not strictly improve the MSE metric.
B. Criteria for Attack Success
In our experiments, we use several evaluation metrics ℓ
to capture various aspects of information leakage from re-
construction attacks. When reporting an average metric we
measure performance of a single reconstructor network on 1K
released model and target point pairs.
a) Mean squared error (MSE): We report
the MSE
between a target and its reconstruction. In the context of
images, while discovery of private information does not neces-
sarily perfectly coincide with a decreasing MSE between the
original and reconstructed training point, in general the two
are correlated (Section VI-A).
b) LPIPS: We report the LPIPS metric [29] as it has been
shown to be closer to the human’s visual systems determina-
tion of image similarity in comparison to the MSE distance.
LPIPS is measured by comparing deep feature representations
from visual models trained with similarity judgements made
by human annotators.
c) KL: After running the attack, a real-world adversary
may need to post-process the reconstructed image; e.g. if they
wanted to extract a license plate from the reconstructed image,
they may need to run a downstream image classifier. We there-
fore include a similarity metric between the outputs of a highly
accurate classifier on the target and reconstructed image based
on the Kullback–Leibler (KL) divergence between predicted
class probabilities. For MNIST, we use a LeNet classifier [32]
achieving 99.4% test accuracy, and for CIFAR-10 use a Wide
ResNet [33] achieving 94.7% test accuracy.
d) Nearest Neighbor Oracle: To contextualize MSE re-
construction metrics we consider an oracle that exploits all
the data available to the adversary in the default setting and
guesses the point ˆz ∈ D- ∪ ¯D that has the smallest MSE
distance to z. The MSE distance between z and its nearest
neighbor ˆz serves as a conservative threshold for successful
reconstruction: although faithful reconstructions with larger
MSE are certainly possible, falling below the threshold means
the reconstruction is closer to the target than to any other point
previously available to the adversary, so the attack must have
(a) MNIST
(b) CIFAR-10
Fig. 4: Visualization of reconstructions for six random targets
selected from the test set. The first column shows the targets,
the second shows the default reconstruction attack, the third
shows reconstructions around the same MSE provided by the
NN oracle. The fourth column corresponds to reconstructions
with distance approximately equal to the 1st percentile (Fig-
ure 3). The last column shows the NN oracle.
extracted unique information about the target point from the
released model. Figure 3 provides average histograms (over
1K test targets) of MSEs between a target point and all points
in D- ∪ ¯D. The green line corresponds to the average MSE to
the nearest neighbor across all test targets (0.0232 on MNIST
and 0.0291 on CIFAR-10); if reconstructions have a smaller
MSE than this distance we will judge the target to have been
successfully reconstructed. For reference, we also highlight the
1st, 10th and 50th percentile MSEs, which will be helpful to
contextualize experiments throughout Section VI.
VI. EMPIRICAL STUDIES IN RECONSTRUCTION
We now conduct extensive experiments investigating how
the released model architecture and its training hyperparam-
eters impact reconstruction quality. We first demonstrate the
feasibility of the reconstruction attack against models trained
on our default experimental setup. Then we discuss an in-depth
study on which factors, such as training set size or released
model’s hyperparameters, affect the success of reconstruction.
Finally, we investigate DP as a mitigation against reconstruc-
tion attacks. Our findings are summarized in Table II.
A. Feasibility of Reconstruction Attacks
We first carry out the general reconstruction attack under
the default experimental settings (cf. Section V).
Figure 4 shows examples of targets and respective recon-
structions; we use the nearest neighbor (NN) oracle as a
baseline. We observe a good overall reconstruction quality on
both datasets. Running the attack against 1K test targets, we
observe an average reconstruction MSE of 0.0089 (MNIST)
and 0.0049 (CIFAR-10). These numbers, compared to the
NN oracle baselines, demonstrate our attack is effective. To
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:22:08 UTC from IEEE Xplore.  Restrictions apply. 
1144
0.050.100.150.200.250500010000Nearest Neighbor (NN)1st percentile10th percentile50th percentile0.050.100.150.200.250.300.350.40025005000750010000Nearest Neighbor (NN)1st percentile10th percentile50th percentileTargetMSE0.01ReconstructionsMSE0.02MSE0.06NNMSE0.02TargetMSE0.01ReconstructionsMSE0.03MSE0.05NNMSE0.03TABLE II: Effect of different factors on the success of reconstruction attacks.
Factor
Description
MNIST
CIFAR-10
MSE
Success
MSE
Success
—
—
Fixed set size
Size & architecture
Released layers
Epochs
Activation
Learning rate
Random initialization
Model access
Nearest neighbor (NN) oracle
Default hyper-parameters and architectures (cf. Section V)
Change size of fixed set to: 1K (MNIST) 50K (CIFAR-10 + shadows from CIFAR-100)
Larger MLP (MNIST) and CNN (CIFAR-10)
Restrict attack to use subset of released model layers
Increase number of released model training epochs: 250 (MNIST) 200 (CIFAR-10)
Change released model activations to ReLU
Decrease released model learning rate: 0.01 (MNIST) 0.001 (CIFAR-10)
Adversary does not know initial released model parameters
Only allow logit-based black-box access to released model
0.0232
0.0089
0.0094
0.0079
0.0124
0.0121
0.0182
0.0049
0.0695
0.0110
–
✓
✓
✓
✓
✓
✓
✓
✗
✓
0.0291
0.0049
0.0039
0.0047
0.0257
0.0094
0.0324
0.0055
0.0931
0.0198
–
✓
✓
✓
✓
✓
✗
✓
✗
✓
account for the variance across experimental runs (e.g. differ-
ent random selections of fixed sets across experiments), we
repeated this experimental procedure ten times with differing
fixed sets, initial released model parameters, and evaluation
sets. We saw minimal variance in results; importantly, recon-
structions were consistently better than the NN oracle.
To help the reader calibrate MSE values to reconstruction
quality, Figure 4 shows poor reconstructions with MSE close
to the oracle NN’s MSE (third column) and to its 1st per-
centile (fourth column); these reconstructions were obtained
in preliminary experiments with weaker RecoNN instances.
Relation between reconstruction metrics: With the same
experimental setup as above, we also evaluate results across
our other metrics (Section V-B) on MNIST. We observe that
MSE and LPIPS are strongly correlated (Figure 5a). Figure 5b
also shows that a small MSE implies a small KL but the
converse is not true; in other words, it is possible for two
images that are not
to have similar predictions.
Since these metrics exhibit significant correlations, we only
report a subset of them in subsequent experiments, focusing
mostly on comparing the MSE metric with the NN oracle. We
observe similar trends on CIFAR-10, although MSE vs LPIPS
correlation is weaker; this partially motivated including the
LPIPS loss when training RecoNNs on CIFAR-10.
identical
B. What Factors Affect Reconstruction
We study which factors may improve or impact reconstruc-
tion success; these are summarized in Table II.
a) Attack training set size: Recall that the general recon-
struction attack assumes the attacker has access to k shadow
data points ¯D from the same distribution as the target point.
From this knowledge, the attacker generates a collection of
shadow model-target pairs (the attack training data), which is
used to train the RecoNN. Note that the size of the attack
training data depends both on the knowledge of the attacker
(simply, the attacker may not have access to many examples),
and on their computational power: they need to train one
shadow model per data point to create the attack training data.