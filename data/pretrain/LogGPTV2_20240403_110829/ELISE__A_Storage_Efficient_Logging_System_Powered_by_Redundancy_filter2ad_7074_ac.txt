our key patterns.
Preprocessing 2: Session management and enumerations.
Many system and software activities from different users
(or clients) use sessions. Log entries belonging to the same
session will share a lot of variables and hence, they have
many repeated values. For example, in Linux system log, log
entries belonging to the same process have the same values for
pid, ppid (parent process id), hostname and arch (system
architecture), and most of them have the same values for uid,
gid and so on.
Storing such repeated values will lead to higher overhead.
To alleviate this problem, we propose to summarize session
related ﬁelds into a tuple stored in the reference table. Simi-
lar to key patterns, we deﬁne a translation rule between the
common values and compressed values, and then assign the
compressed log entry with a reference code. For example, for
(pid:50, arch:03, uid:3345), we map it to (pid:0,
arch:0, uid:0) with a reference code t. In the reference
table, we have (0,0,0) t(cid:55)−→ (50, 03, 3345). Notice that even
less common, values of some ﬁelds belonging to the same
process do change from one to another such as uid. Func-
tions like setuid() can change this value in a session. In
this case, we just add a new translation rule in the reference
table and increase the compressed code for these ﬁelds (e.g.,
from 0 to 1). Moreover, we reorganize logs into sessions
by aggregating log entries in the same session into the same
region to reduce storage overhead. Different from preprocess-
ing 1 which analyzes the redundancy in keys, preprocessing
2 focuses on the repeated patterns in values. Automatically
discovering such patterns is a classic data mining problem.
To solve this problem, we ﬁrst extract all values belonging
to the same keys (obtained from preprocessing 1). Then, we
perform an unsupervised clustering analysis on corresponding
to ﬁnd such patterns. Speciﬁcally, we use a TF-IDF to get the
frequently values, and then uses the K-means algorithm to
cluster all logs. To determine the optimal value of k, we use
the Silhouette method. After that, we also manually check
whether these keys are correct. Lastly, we summarize these
patterns into rules, apply them to the original log, and update
the reference table accordingly.
Notice that even though some ﬁelds are not related to ses-
sions, they are also clustered because of a limited number of
possible values in a given set of log ﬁles. For example, foren-
sics systems usually only log I/O and process related system
calls, and their logs have a ﬁnite set of system call numbers.
3028    30th USENIX Security Symposium
USENIX Association
B. PreprocessingKey patternsSession Management and EnumerationsMonotonous ValuesFrequent Words ReductionLinux logLinux logApache logApache logFtp logFtp logMysql logMysql logWindows logWindows logC. Obtaining EncoderReference tableDNN ModelC. Obtaining EncoderReference tableDNN ModelD. Log CompressionArithmetic EncoderCompressed FileD. Log CompressionArithmetic EncoderCompressed FileControl FlowData FlowControl FlowData FlowELISELog SourceA. Log FormattingFile SplitterFormatting ToolsFile SplitterFormatting ToolsEven though they are not session related ﬁelds, we also do
similar preprocessing for them to reduce the redundancy.
Preprocessing 3: Monotonous values. In various types of
logs, it is common to see some ﬁelds that have monotonous
values even though they are not identical. Because of this,
continuous log entries may share a lot of common characters
or numbers. Such ﬁelds include timestamps, counters used
for logging statistical information, transaction identiﬁers for
databases, etc. For example, most logs use the UNIX time to
record when the event happens. A single UNIX timestamp in
Linux Auditd log is a 14-character long string including 13
digits and a dot symbol. Recall that ELISE separates huge log
ﬁles into smaller ones (Section 3.2) for parallel processing.
Most timestamps in the same log ﬁle have identical digits
at the beginning representing the same year, month and day,
which is redundant.
For these monotonous value ﬁelds, we ﬁrst record the small-
est value and then replace the original value with the offset
values in the rest of the log. Such incremental logging can
help remove a lot of unnecessary digits. When decompressing,
we recalculate the actual values by using the smallest value
and offsets. Discovering such monotonous value ﬁelds is also
simple. We ﬁrst choose the numerical ﬁelds, and then simply
test whether they are monotonous in log sequence order.
Preprocessing 4: Frequent words reduction. Besides the
previous redundancies, there are still word and string level
redundancies. For example, folder and ﬁle paths commonly
share a long preﬁx. Moreover, folder names in paths may
share a lot of common substrings with other ﬁelds like pro-
cess names and binary names. To remove such redundancies,
we introduce a two layer frequent words compression tech-
nique leveraging existing algorithms (i.e., ﬁnding the longest
substrings).
Firstly, we compress strings that belong to the same type of
log entries. For example, in Linux Auditd log, we gather all
PATH type entries, and ﬁnd common substrings among them.
The log entry type can be identiﬁed by its keyword patterns.
Secondly, we apply the algorithm again globally to reduce
word level redundancy. One key difference of ELISE in this
step from existing algorithms (e.g., LZ77) is that instead of
replacing the strings with a mark with offsets and length,
ELISE directly substitute them with numerical values (similar
to preprocessing 1). In log ﬁles, all formats and ﬁelds are
well-deﬁned, and using a translation rule plus numerical value
saves more space.
3.4 Encoder and Data Compression
Similar to DeepZip, ELISE uses a trained encoder and arith-
metic encoding to compress a data ﬁle. In this section, we
will show how to obtain such an encoder and leverage it to
perform data compression and decompression.
Encoder. Theoretically, all model architectures that support
Figure 3: Model Architecture of DNN Model in ELISE.
processing sequential data potentially can be used as our en-
coder. In our implementation, the design of DNN is shown
in Figure 3. It is a Long Short Term Memory (LSTM)
model, M : SL (cid:55)→ [0,1]N. This model takes an L-length string
s = {s1s2 . . .sL},s ∈ S as the input to predict the next charac-
ter sL+1 in this sequence. It consists of two LSTM layers, a
batch normalization layer, two fully connected layers, and the
last layer, a SoftMax layer outputs an N-length long vector
y = {y1,y2, . . . ,yN}, where N is the total number of possible
characters and each yi ∈ [0,1],arg(yi) ∈ [1,N] . Each label of
the output vector represents a possible character (e.g., ‘a’ to
‘z’). For the i-th character si, we use string {si−L . . .si−1} to
predict si. Training such a model is a typical classiﬁcation
task where the input is a ﬁxed length vector and the output is
a one-hot encoding of characters. We used categorical cross-
entropy loss function and Adam [35] optimizer with default
settings to train the models.
Pre-trained encoder and partial data training. Training an
encoder from scratch can take a long time. We notice that
system logs are highly redundant which enables us two opti-
mizations: partial data training and using pre-trained models.
Partial data training means we do not train the encoder on the
entire dataset but just a small part of it. The other solution is
to use pre-trained models and only ﬁnetune them on new data
ﬁles. Also, we can combine these two approaches together:
ﬁnetuning a pre-trained encoder on partial data. Notice that
even if the trained encoder cannot achieve high prediction
accuracy on test data, it still can be used for lossless data
compression and decompression. Models with higher predic-
tion results will lead to lower compression ratios (i.e., better
results) and vice versa. Thus, these optimizations are trade-
offs between compression ratios and compression time. In
Section 4.4.1, we perform a study on how such optimizations
affect ELISE.
Encoding and data compression. After obtaining the
trained encoder, we combine it with arithmetic encoding to
encode characters. First, we use the trained model to predict
each character in the ﬁle Fc whose length is c and get their
outputs. For the i-th character si, we get yi = {yi
N}.
Also, we initialize two variables (A,B) as (0,1) which be
2, . . . ,yi
1,yi
USENIX Association
30th USENIX Security Symposium    3029
,LSTM LayerLSTM LayerBatch NormalizationFullyConnected Layer + ReluFullyConnected Layer + SoftMaxused to store intermediate results, and perform encoding by
applying the following equations:
ti) = si, i ∈ {1, . . . ,c}, oi
arg(yi
Ai =Ai−1 + (Bi−1 − Ai−1)∗ oi
ti−1
Bi =Ai−1 + (Bi−1 − Ai−1)∗ oi
ti,
j =
∑ j
k=1 yi
k
∑N
k=1 yi
k
(1)
After doing this for all c characters, we can get a (Ac,Bc), and
to encode the whole log ﬁle, we only need to pick a number
f which satisﬁes Ac ≤ f < Bc and f has the shortest binary
representation as our ﬁnal compressed data ﬁle.
Notice that after updating A and B for several iterations, it
can get small, and we have to use customized data types to
represent such small numbers. As we will show in Section 4,
storing a single number like this can take a few MB. For the
ﬁrst L characters at the beginning of a ﬁle, we cannot ﬁnd
a corresponding input to the model. A common practice to
solve this problem is just assuming a ﬁxed distribution.
Data decompression. Decompressing the data from f is a
reverse process of data compression. For the i-th characters si,
we ﬁrst obtain the prediction result yi = {yi
N} via
the DNN model. Similar to the compression process, we need
(A,B) whose initial values are (0,1) to store our intermediate
results and apply the following rules for decompression:
2, . . . ,yi
1,yi
N}, oi
∑ j
oi = {oi
k=1 yi
1,oi
2, . . . ,oi
k
∑N
k=1 yi
k
si = arg(yi
z),
Ai = Ai−1 + (Bi−1 − Ai−1)∗ oi
z−1
Bi =Ai−1 + (Bi−1 − Ai−1)∗ oi
z,
f ∈ [Ai,Bi), z ∈ {1, . . . ,N}
j =
, i ∈ {1, . . . ,c}
(2)
Recall that the trained encoder cannot predict the ﬁrst L char-
acters in a given ﬁle, and they are encoded by using a ﬁxed
distribution. For these characters, we reuse this ﬁxed distribu-
tion during decompression. After this, we leverage the stored
reference table to undo all the preprocessing operations to
recover the raw log.
4 Evaluation
We built a prototype based on our proposed idea, and evalu-
ate it using real world data to validate its effectiveness, efﬁ-
ciency and support of real word security analysis applications.
We ﬁrst introduce our setup for our experiments including
conﬁgurations of the server and datasets (Section 4.1). In
Section 4.2, we evaluate its effectiveness by comparing with
existing methods DeepZip and Gzip on different sized log
ﬁles. To measure the efﬁciency of ELISE, we measure the
time cost of individual steps including preprocessing, encoder
training, data compression and decompression; and the usage
of memory. Moreover, we perform an ablation study on the
conﬁgurable parameters in ELISE and also alternative designs
that can help speed up model training (partial data training
and using pre-trained models. Lastly, we use one real world
security application, forensics analysis to validate if ELISE
can guarantee the log integrity.
4.1 Experiment Setup
Our prototype of ELISE is implemented in Python using
Keras [7] with TensorFlow as the backend [4]. If not speciﬁed,
all experiments are conducted on a Ubuntu 18.04 machine
equipped with a GeForce RTX 6000 GPU, 64 CPUs and 376
GB main memory.
Datasets. Our evaluation datasets are collected from 3 differ-
ent operating systems and 3 popular server applications. We
follow the standard guidance to collect datasets on our experi-
mental machine properly. Speciﬁcally, we randomly start the
data collection procedure, guarantee long enough collection
durations, and perform a manual post-modern check to reduce
biases and make sure that used workloads are representative.
Details of these ﬁles are listed in Table 1. We collect sys-
tem logs from Linux, Windows and FreeBSD and application
logs from Apache2, VSFTP and MySQL which run on top
of Linux. For system logs, we collect system events which
include but are not limited to system calls, monitored process
(by default, all processes), speciﬁc ﬁles (e.g., /etc/passwd)
and user account information (i.e., euid). On Linux, we use
the built-in system event collector auditd. On Windows, we
utilize the Sysinternal tools such as Process Monitor. On
FreeBSD, we leverage DTrace to gather such information.
All applications we use have their own application logs, and
we directly use their built-in tools and default conﬁgurations.
Apache2 and VSFTP logs mainly contain the connection
information (e.g., source IP address, port number, client infor-
mation) and access information (e.g., ﬁle access and down-
loading behaviors). MySQL logs not only the connection
information and queries, but also its internal transaction in-