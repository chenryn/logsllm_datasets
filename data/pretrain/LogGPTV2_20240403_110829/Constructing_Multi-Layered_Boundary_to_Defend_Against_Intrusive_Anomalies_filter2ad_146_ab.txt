erating ADs is via the globally distributed reward signal, as
shown in Figure 1. More formally, for the elemental ADs,
the set of actions (cid:88) contains the cross product of all the ac-
tions available to each AD, that is, (cid:88) = {(cid:120)1×(cid:120)2×···×(cid:120)(cid:113)}.
Because the AD parameters are independent, each AD in-
dependently chooses actions that are combined to form the
meta-action. For stochastic policies, the overall action dis-
tribution is the joint distribution of actions for each agent,
(cid:25)((cid:120)1(cid:62) (cid:120)2(cid:62) (cid:61)(cid:61)(cid:61)(cid:120)(cid:113)|(cid:21)1(cid:62) (cid:21)2(cid:62) (cid:61)(cid:61)(cid:61)(cid:21)(cid:113)(cid:62) (cid:125)1(cid:62) (cid:125)2(cid:62) (cid:61)(cid:61)(cid:61)(cid:125)(cid:113))(cid:61)
3.3 A Speciﬁc Solution
In the formulated model, the policy of the ADC is af-
fected by a concatenation parameter (cid:21), while our aim is to
ﬁnd the parameter settings (the optimal control strategy) for
all the ADs that maximizes the expected long-term average
reward in equation (2). This is actually a kind of direct re-
inforcement learning problem, which is described in [4, 5].
Brieﬂy, the algorithm learns to adjust the parameters (cid:21)
of a randomized policy with observation (cid:125)(cid:108), and chooses
actions according to (cid:25)((cid:125)(cid:108)(cid:62) (cid:21)). It involves the computation
of a vector (cid:116)(cid:119) at time step (cid:119), and it updates according to:
(cid:116)(cid:119)+1 = (cid:29) · (cid:116)(cid:119) + (cid:117)(cid:25)(cid:120)(cid:119)
((cid:125)(cid:119)(cid:62) (cid:21))
((cid:125)(cid:119)(cid:62) (cid:21))
(cid:25)(cid:120)(cid:119)
(3)
where (cid:29) (cid:53) (0(cid:62) 1), (cid:25)(cid:120)(cid:119)
((cid:125)(cid:119)(cid:62) (cid:21)) is the probability of the action
(cid:120)(cid:119) under the current policy, and (cid:117) denotes the gradient with
respect to the parameters (cid:21). The vector (cid:116)(cid:119) is an eligibility
trace of the same dimensionality as (cid:21); it is used to update the
parameters, and guides the policy to climb the gradient of
the average reward. Here, we intend to apply a multi-agent
variant of the OLPOMDP algorithm [3], which has been
applied to solve a routing problem by Tao et al [17], and a
multi-neurons learning problem in the brain by Bartlett et
al [2]. The OLPOMDP gives a simple way to compute an
appropriate direction to update the parameters:
(cid:21)(cid:119) = (cid:21)(cid:119)(cid:3)1 + 4(cid:21) = (cid:21)(cid:119) + (cid:31)(cid:119) · (cid:117)(cid:119) · (cid:116)(cid:119)
(4)
where the long-term average of the updates 4(cid:21) lie in the
gradient direction (cid:117)(cid:20)((cid:21)), (cid:117)(cid:119) is the sum of the rewards, and
(cid:31)(cid:119) is the suitable size of the steps taken in parameter space.
The key feature of the algorithm is that the only non-local
information each detector needs is a global reward signal;
detectors do not need to know any other information about
the system state in order to climb the gradient of the global
average reward.
Considering the speciﬁc characteristics of the host sys-
tem, two assumptions need to be addressed to support the
algorithm’s application:
Assumption 1 For every given (cid:21), the system is ergodic
(aperiodic, irreducible), and converges to a unique steady
state (cid:118)0 (cid:53) (cid:86).
Speciﬁcally, although the system’s underlying states are un-
known, it will return to a steady state ultimately; that is, the
right-hand-side of equation (2) is independent of the sys-
tem starting state, and converges with probability 1 over all
possible reward sequences {(cid:117)(cid:108)}.
Assumption 2 For the POMDP-based ADC which is con-
trolled by multiple independent ADs, the updates of equa-
tions (3) and (4) for the coordinator are equivalent to those
that would be used by each AD.
That is, if we let (cid:125)(cid:108)
(cid:119) denote the observation vector for AD
(cid:108)(cid:62) (cid:108) = 4, (cid:120)(cid:108)
(cid:119) denote the action it takes, and (cid:21)(cid:108) denote its
parameter vector, the update equation (4) is equivalent to
the individual update equations,
(cid:21)(cid:108)
(cid:119)
= (cid:21)(cid:108)
(cid:119)(cid:3)1 + (cid:31)(cid:119) · (cid:117)(cid:119) · (cid:116)(cid:108)
(cid:119)
(5)
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:53:20 UTC from IEEE Xplore.  Restrictions apply. 
(cid:52)X
(cid:52)X
where (cid:31)1(cid:62) (cid:31)2(cid:62) · · · (cid:65) 0,
the vectors (cid:116)(cid:108)
2
(cid:119)=0
(cid:31)(cid:119) = (cid:52), and
(cid:119) (cid:63) (cid:52), while
(cid:31)
(cid:119) (cid:53) R(cid:110) are updated according to
(cid:117)(cid:25)(cid:120)(cid:108)
((cid:125)(cid:108)
(cid:119)(cid:62) (cid:21)(cid:108))
(cid:119)(cid:62) (cid:21)(cid:108))
((cid:125)(cid:108)
(cid:25)(cid:120)(cid:108)
(cid:119)+1 = (cid:29) · (cid:116)(cid:108)
(cid:116)(cid:108)
(cid:119)=0
(6)
+
(cid:119)
(cid:119)
(cid:119)
where (cid:117) denotes the gradient with respect to the AD’s pa-
rameters (cid:21)(cid:108).
In addition, to cast the independent AD in the POMDP
model, a formally deﬁnition is given as follows:
Deﬁnition 1 All the ADs have no knowledge about the ex-
act system states, in some sense, |(cid:86)| is inﬁnite; the ob-
servation set (cid:93)={Normal, Malicious}, and the action set
(cid:88)={Observe, Alert} according to the speciﬁc detection al-
gorithm.
Suppose an activity (local or remote) happens at time
step (cid:119), some or all of the four ADs will receive different
observation streams independently in their own operating
environments; assume (cid:30)(·) is a general form of the ADs’ de-
cision rule, which partitions the inﬁnite measurement space
into discretely different decision regions, with each region
corresponding to one of a ﬁnite number (cid:23) (according to
the deﬁnition 1, (cid:23) = 2) of possible output observations (cid:125)(cid:119).
Given a measurement (cid:34)0 (cid:53) R on a measurement stream (cid:99),
AD (cid:108) makes a decision with the decision rule parameterized
by a threshold value (cid:24)(cid:108)
(cid:99), as follows:
Deﬁnition 2 For every measurement stream (cid:99), there is a de-
cision rule (cid:30)(cid:99) : R (cid:36) {0(cid:62) 1} of the form
0(cid:62) (cid:34)0 (cid:23) (cid:24)(cid:108)
(cid:99)
1(cid:62) (cid:34)0 (cid:65) (cid:24)(cid:108)
(cid:99)
(cid:30)(cid:99)((cid:34)0) =
½
where output “0” denotes the ‘Normal’ observation and
“1” the ‘Malicious’ observation. Corresponding actions
‘Observe’ and ‘Alert’ are taken according to the observa-
tion.
From the deﬁnition, for the ADs, there is a direct map-
ping from observations to actions:
{(cid:81) (cid:114)(cid:117)(cid:112)(cid:100)(cid:111)(cid:62) (cid:80)(cid:100)(cid:111)(cid:108)(cid:102)(cid:108)(cid:114)(cid:120)(cid:118)} (cid:36) {(cid:82)(cid:101)(cid:118)(cid:104)(cid:117)(cid:121)(cid:104)(cid:62) (cid:68)(cid:111)(cid:104)(cid:117)(cid:119)}
therefore, the process from observation to action essentially
is deterministic. The parameter (cid:21) of ADC is a concatenation
of parameters (cid:24)(cid:108)((cid:108) = 1(cid:62) 2(cid:62) 3(cid:62) 4), and it is a row vector with
form (cid:21) = ((cid:24)1(cid:62) (cid:24)2(cid:62) (cid:24)3(cid:62) (cid:24)4). Furthermore, it is worth not-
ing that (cid:24)(cid:108) is only the threshold that determines the distance
between normal activities and anomaly activities, while the
action of ADs are also affected by other inner parameters.
For instance, for STIDE, the window size of system call se-
quences (cid:122), the locality frame count (LFC) (cid:79) can also be
adjusted to impact the observation. For MCE, the length of
command blocks (cid:79) is also an adjustable parameters (but ac-
tually according to the login session). While for the Markov
Chain detector, the length of sequences (cid:79) is regarded as the
parametric variables which affects the similarity between
two sequences. However, because most of those inner pa-
rameters are related to the training phase, we do not include
them into the concatenation parameter vector (cid:21) here.
The next consideration is to to derive the second term
of the right-hand side in equation (6) for every independent
AD. Since it is difﬁcult to parameterize the underlying de-
tection schemes with (cid:24)(cid:108), in order to make the ADs trainable
and save computational cost, we assume a general proba-
bilistic model for the behavior of AD. Speciﬁcally, if we
assume (cid:115) is the a prior detection probability of AD, the
probability of detecting (cid:113) anomalies among (cid:81) activities is:
¶
(cid:115)(cid:113)(1 (cid:3) (cid:115))(cid:81)(cid:3)(cid:113)
µ
(cid:81)
(cid:113)
(cid:83)(cid:115)((cid:113)|(cid:81) ) =
(7)
taking the distribution as the function of the expected num-
ber of successful detections, (cid:121) = (cid:115)(cid:81) , the equation be-
comes:
µ
¶
( (cid:121)
(cid:81)
)(cid:113)(1 (cid:3) (cid:121)
(cid:81)
)(cid:81)(cid:3)(cid:113)
(cid:83)(cid:121)(cid:64)(cid:81)
((cid:113)|(cid:81) ) =
(cid:81)
(cid:113)
When (cid:81) (cid:36) (cid:52), we have,
(cid:83)(cid:121)((cid:113)) = lim
(cid:81)(cid:36)(cid:52) (cid:83)(cid:121)(cid:64)(cid:81)
((cid:113)|(cid:81) )
(cid:81)((cid:81) (cid:3) 1) · · · ((cid:81) (cid:3) (cid:113) + 1)
(cid:113)!
(cid:121)(cid:113)
(cid:81) (cid:113)
(1 (cid:3) (cid:121)
(cid:81)
)(cid:81)(cid:3)(cid:113)
= lim
(cid:81)(cid:36)(cid:52)
= (cid:121)(cid:113)(cid:104)(cid:3)(cid:121)
(cid:113)!
Obviously, (cid:83)(cid:121)((cid:113)) is a Poisson distribution. Hence, for the
independent AD, its action (cid:120)(cid:119) generally obeys the following
rule (based on the fact that the number of anomalies is much
smaller than that of normal activities, we describe the model
of (cid:83) (cid:117)((cid:120)(cid:119) = 0) rather than (cid:83) (cid:117)((cid:120)(cid:119) = 1)).
(cid:83) (cid:117)(Observe without Alarms) = (cid:83) (cid:117)((cid:120)(cid:119) = 0) = (cid:42)((cid:37)(cid:119))
(8)
where (cid:42)((cid:123)) = (cid:104)(cid:3)(cid:123), and (cid:42)((cid:123)) (cid:53) [0(cid:62) 1), while (cid:37)(cid:119) (cid:53) (0(cid:62)(cid:52)) is
deﬁned as:
(cid:37)(cid:119) = (cid:24)(cid:108)
(cid:119)
((cid:99))
(cid:103)(cid:108)
(cid:119)
(9)
where (cid:24)(cid:108)
(cid:119) is the threshold of (cid:108)th AD at time instant (cid:119), while
((cid:99)) denotes the measurement distance between ongoing
(cid:103)(cid:108)
(cid:119)
observations (cid:99) and the normal patterns. Assumption 2
shows how to update the threshold (cid:24)(cid:108)
(cid:119) in the direction that
maximally increases the long-term average of the reward.
From equation (9), we easily derive
(cid:67)
(cid:67)(cid:24)(cid:108)
(cid:119)
(cid:25)(cid:120)(cid:119)
(cid:25)(cid:120)(cid:119)
=
(cid:59)
(cid:65)(cid:63)
(cid:65)(cid:61)
= (cid:3)1
((cid:99))
(cid:103)(cid:108)
(cid:119)
(cid:42)0((cid:37)(cid:119))
((cid:99))(cid:42)((cid:37)(cid:119))
(cid:3)(cid:42)0((cid:37)(cid:119))
((cid:99))(1(cid:3)(cid:42)((cid:37)(cid:119)))
(cid:103)(cid:108)
(cid:119)
(cid:103)(cid:108)
(cid:119)
=
(cid:42)((cid:37)(cid:119))
((cid:99))(1(cid:3)(cid:42)((cid:37)(cid:119)))
(cid:103)(cid:108)
(cid:119)
if (cid:120)(cid:119) = 0
otherwise
(10)
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:53:20 UTC from IEEE Xplore.  Restrictions apply. 
To complete the picture we need to deﬁne a performance
measure for the detection result, which can be taken as a
reward signal to guide the improvement of the general de-
tection performance. As we know, in the anomaly detection
domain, some or all of following cases might happen:
• N(cid:113), legal behavior is detected as normal
• N(cid:100), legal behavior is detected as anomaly
• A(cid:113), illegal behavior is detected as normal
• A(cid:100), illegal behavior is detected as anomaly
Based on those four cases, a natural performance metric
can be deﬁned as:
Deﬁnition 3 Assume that during a particular time period
(cid:4)(cid:119), (cid:112) activities occurred, among those activities, (cid:108) (cid:53) N(cid:113),
(cid:109) (cid:53) N(cid:100), (cid:110) (cid:53) A(cid:113), and (cid:111) (cid:53) A(cid:100), if we assign (cid:122)1, (cid:122)2, (cid:122)3, and
(cid:122)4 to denote their respective weights, and (cid:14) = (cid:122)1 · (cid:108)(cid:64)(cid:112),
(cid:15) = (cid:122)2 · (cid:109)(cid:64)(cid:112), (cid:19) = (cid:122)3 · (cid:110)(cid:64)(cid:112), (cid:17) = (cid:122)4 · (cid:111)(cid:64)(cid:112), a reward
signal can be deﬁned as (cid:117)(cid:119) = (cid:14) · (cid:17) (cid:3) (cid:15) · (cid:19), while (cid:122)1, (cid:122)2,
(cid:122)3 and (cid:122)4 is deﬁned according to various system situation
and security demands.
Due to the nature of anomaly detection, and the fact that
the number of normal activities is much larger than that of
anomalies, we usually set (cid:122)1 (cid:63) (cid:122)3 (cid:63) (cid:122)4 (cid:63) (cid:122)2.
In essence, the anticipated behavior of our autonomic de-
tection coordinator is based on the consensus of meta-AD,
and thus we have another assumption,
Assumption 3 Given an ongoing activity happens in the
host at time step (cid:119), logically, the POMDP-based ADC gets
the report as follows:
(cid:113)[
(cid:113)\
=
(cid:85)(cid:102)
(cid:103)
(cid:85)(cid:108)
(cid:103)
(A(cid:100))(cid:62)
=
(cid:85)(cid:102)
(cid:105)
(cid:85)(cid:108)
(cid:105)
(N(cid:100))
(cid:108)=1
(cid:108)=1
where (cid:85)(cid:108)
lies, while (cid:85)(cid:108)
(cid:103) is the report of AD (cid:108) about the detected anoma-
(cid:105) is the report about the false alerts.
Based on our speciﬁc assumptions and deﬁnitions, a
modiﬁed version of algorithm OLPOMDP [4] can be used
to describe the independent AD (cid:108) as follows:
Algorithm Model of ADC meta-action
1: Given:
Coefﬁcient (cid:29) (cid:77) [0(cid:62) 1),
Step size (cid:31)0,
Initial system state (cid:118)0,
Initial thresholds of independent ADs (cid:24)(cid:108)
0, i.e., (cid:21)(cid:108)