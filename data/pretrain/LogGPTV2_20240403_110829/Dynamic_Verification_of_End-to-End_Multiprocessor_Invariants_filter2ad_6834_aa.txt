title:Dynamic Verification of End-to-End Multiprocessor Invariants
author:Daniel J. Sorin and
Mark D. Hill and
David A. Wood
Dynamic Veriﬁcation of End-to-End Multiprocessor Invariants
Daniel J. Sorin
Department of Electrical and Computer Engineering
Duke University
Abstract
As implementations of shared memory multiprocessors
become more complicated, hardware faults will increasingly
cause errors that are difﬁcult or impossible to detect with
low-level, localized mechanisms. In this paper, we argue for
dynamic veriﬁcation (i.e., on-the-ﬂy checking) of end-to-end,
system-wide invariants in shared memory multiprocessors.
We develop two invariant checkers based on distributed sig-
nature analysis. Our coherence-level checker dynamically
veriﬁes that every cache coherence upgrade has a corre-
sponding downgrade elsewhere in the system. Our message-
level checker veriﬁes that all nodes in an SMP observe the
same total order of broadcast requests. We use full-system
simulation to show that the checkers detect the targeted
errors while not signiﬁcantly degrading system performance.
1  Introduction
Symmetric multiprocessors (SMPs) are an important class
of computers that is getting more complex in the quest for
higher performance. Traditionally, an SMP consisted of mul-
tiple nodes—which each contain a processor, cache(s), and a
portion of the shared memory—connected by a single
shared-wire bus. The bus served as a single ordering point
that facilitated the total order of cache coherence requests
required by the broadcast snooping coherence protocol. Cur-
rent SMPs, in order to provide the same total order of broad-
cast requests but with greater bandwidth, may use multiple
interleaved logical buses implemented as bit-sliced pipelined
broadcast trees with point-to-point links [6], such as the sys-
tem illustrated in Figure 1. While snooping cache coherence
transactions in traditional SMPs were atomic, now even
SMPs with shared-wire buses have split transactions [14].
In addition to high performance, SMP designers also seek
high availability, in part, because SMPs are often used to run
commercial applications, such as database management sys-
tems and web servers. Unwilling to sacriﬁce performance,
SMPs have only used fast, localized mechanisms to detect
errors caused by physical faults. Memory, caches, buses, and
links have been protected by parity or error correcting codes
(ECC). SMPs crashed to avoid data corruption on some
detected errors and used localized recovery for others (e.g.,
bus or link re-transmission).
Mark D. Hill, David A. Wood
Computer Sciences Department
University of Wisconsin—Madison
Localized mechanisms for error detection and recovery
may not be sufﬁcient for future SMPs as (1) vendors seek to
promote high availability (e.g., “ﬁve nines” or available
99.999% of the time), (2) designers add more complexity to
obtain even greater performance, and (3) deep submicron
fabrication becomes increasingly susceptible to faults.
Consider a modern SMP in the following scenario. Pro-
cessor P1 broadcasts a RequestForExclusive cache coher-
ence request for memory block B. Processor P2 has B in the
Shared (i.e., ReadOnly) state in its cache. P1 receives data
from memory, while P2 should be invalidated. If a fault
causes the loss of P1’s request as it passes from the intercon-
nect into P2’s network interface, P2 will retain B in Shared.
Since broadcast snooping protocols do not use acknowledg-
ments, P1 will not learn that P2 did not receive its request. If
P2 continues to read block B, the system could violate its
memory consistency model and thus violate correctness.
An approach to addressing such complex scenarios is to
adapt Saltzer’s end-to-end argument [13]. That argument
observes that certain functions can only be implemented at
higher levels (although lower levels can help). For example,
the integrity of ﬁle transfer should be checked with end-to-
end checksums. Link-level mechanisms can detect some
errors, aid fault diagnosis, and help performance, but data
may be corrupted elsewhere (e.g., in router memory) or via
fault models not anticipated during design.
This paper seeks end-to-end error detection for SMP
coherence protocols and interconnects via dynamic veriﬁca-
tion of system-wide, end-to-end invariants. Dynamic veriﬁ-
cation is the process of the hardware checking its execution
on the ﬂy. We implement our invariant checkers with distrib-
uted signature analysis. For each checker, each cache and
memory controller in the system maintains a local signature
that it updates for each event (e.g., incoming cache coher-
ence request). Periodically, the system performs a global
reduction on all of these local signatures that determines
whether the invariant holds.
As one concrete example, we develop a cache coherence-
level checker that ensures that all coherence upgrades (i.e.,
increases in access permissions to a memory block) have
corresponding downgrades (i.e., decreases in permissions)
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:12:52 UTC from IEEE Xplore.  Restrictions apply. 
switch
switch
switch
switch
Node
Interconnect
Interface
  Cache
Memory
node
node
node
node
node
CPU
FIGURE 1. Example of modern SMP with broadcast tree interconnect
elsewhere in the system. Simplifying the details for now,
each cache and memory controller adds or subtracts the
address of the incoming broadcast request to its local signa-
ture, depending if the request triggers a coherence upgrade
or downgrade at the controller. Periodically, components
send their signatures to a centralized controller that checks to
ensure that the global sum of all signatures equals zero. This
checker would detect the previous error in which P2 did not
receive a broadcast request that would have downgraded it.
Of course, end-to-end error detection in SMPs is more
valuable if one can recover from errors instead of just crash-
ing. Recovery can be achieved with a backward error recov-
ery (BER) mechanism that maintains a safe recovery point
that can be restored when a signature check fails. To be
acceptable for modern SMPs, however, the BER mechanism
should add only modest hardware and performance over-
heads, despite having to provide a recovery point that is hun-
dreds of cycles behind the active execution. Distributed
signature analysis takes that long to verify execution, since
even just sending a signature to the system controller takes
more than 100 cycles today (and more in the future).
While many BER schemes exist and could be used in
conjunction with dynamic veriﬁcation, we choose SafetyNet
[16], a recently developed all-hardware BER mechanism that
is well-suited to enabling recovery after end-to-end error
detection. SafetyNet (reviewed in Section 3) tolerates long
error detection latency by allowing execution to continue
even as error detection (e.g., distributed signature checking)
is done is the background. This approach hides error detec-
tion latency in the common case of error-free execution.
In the rest of this paper, we ﬁrst explain further why the
complexity of modern SMPs motivates end-to-end error
detection (Section 2) and review efﬁcient hardware check-
point/recovery with SafetyNet (Section 3), before making
three contributions to improve multiprocessor availability.
• We show, in general, how to apply distributed signature
analysis to dynamically verify system-wide invariants in
SMPs (Section 4).
• We develop a signature analysis scheme for verifying
that every cache coherence upgrade has corresponding
downgrades elsewhere in the SMP (Section 5).
• We develop a signature analysis scheme for verifying
that all SMP nodes receive the same broadcast coher-
ence requests in the same order (Section 6).
In Section 7, we evaluate our invariant checkers with full-
system simulation and commercial workloads. In Section 8,
we discuss related research before concluding in Section 9.
This work concisely presents work developed in Chapter 4 of
Sorin’s Ph.D. thesis [15].
2  Motivation for End-To-End Invariant
Checking in Modern SMPs
A modern SMP, such as the one we discuss in this paper,
consists of a large number of interacting ﬁnite state machines
(FSMs). A fault in any of these FSMs or in the communica-
tion between them can manifest itself as an error. A modern
SMP has a complex cache coherence protocol (Section 2.1)
and interconnection network (Section 2.2), and this com-
plexity exacerbates the problem of detecting when an error
occurs. To address the problem of error detection in such a
complicated system, we propose the use of distributed signa-
ture analysis schemes to dynamically verify end-to-end sys-
tem-wide
invariants
(Section 2.3) are high-level properties of the system that are
independent of low-level implementation details.
2.1  Cache Coherence Protocol
system-wide
invariants.
These
We base the memory system design on a hardware-only,
broadcast snooping cache coherence protocol. The protocol
has four stable states for blocks of memory [7]: Modiﬁed
(M), Owned (O), Shared (S), and Invalid (I). A processor can
broadcast a request on the interconnection network to
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:12:52 UTC from IEEE Xplore.  Restrictions apply. 
I
I
atomic transaction (read/response)
request
arrives
issue
request
wait for
request
and
response
wait for
response
wait for
request
M
response
arrives
M
response
arrives
request
arrives
FIGURE 2. Comparing an atomic coherence
transaction with a split transaction
upgrade (RequestForShared or RequestForExclusive) or
downgrade (WritebackExclusive) its coherence permissions
for a block. Processors can silently evict Shared blocks, so
there is no WritebackShared request. When the owner of a
block (a processor in Modiﬁed or Owned or the home mem-
ory if no processor owns the block) observes a request for
upgraded permissions, it responds with the data. When a
downgrading
its
own
WritebackExclusive (since requests are broadcast
to all
nodes, including the requestor), it sends the data to the home
memory, which is now the owner.
processor
observes
owner
The cache controllers and memory controllers are FSMs
with some number of states and events. While traditional
protocols had few, if any, transient states, modern protocols
such as ours have numerous transient states. Moreover, they
have more events, since split transactions have more mes-
sage exchanges than simple atomic transactions. In Figure 2,
we illustrate the difference in complexity between an atomic
transaction, such as might occur in a simple bus-based sys-
tem, and a split transaction in our protocol. States are circles
and transitions are arcs labeled with the events that cause
them. The greater number of transient states and events
reﬂect the additional concurrency and asynchrony in our sys-
tem. While the top path through the split transaction corre-
sponds to a typical scenario in a bus-based split transaction
(since there can be a delay between issuing a request and
when it wins the bus and arrives), the bottom path is non-
intuitive and we will explain how it can happen in
Section 2.2.
Coherence controller complexity increases the impor-
tance of end-to-end error detection. With additional states
and events,
the probability of a fault causing an error
increases. Instead of adding low-level, localized error-detec-
tion mechanisms to accommodate additional error models,
we propose checking end-to-end invariants that are largely
implementation-independent.1 For example, the coherence
checker that we develop in Section 5 dynamically veriﬁes
that every coherence upgrade has a corresponding down-
grade. No matter how complicated a coherence protocol
might be, this invariant must still hold.
2.2  Interconnection Network
Unlike traditional bus-based SMPs, we interconnect the
nodes in our system with a broadcast tree. Designers have
turned away from shared-wire buses because of the electrical
complications involved in scaling their bandwidths to
accommodate more demanding processors [6]. Like a bus, a
broadcast tree can provide the total order of broadcast coher-
ence requests that snooping coherence protocols require, but
it enables the use of point-to-point signalling instead of
shared wires. A processor unicasts a coherence request up to
the root of the tree, which is the point at which the intercon-
nect inserts requests into the total order, and then the root
broadcasts the request down the tree. Data response mes-
sages also travel on the physical links of the broadcast tree,
but they do not need to be broadcast.
Using a broadcast tree instead of a bus can violates an
invariant
that has traditionally simpliﬁed system design.
Broadcast requests may no longer be synchronous, i.e., a
request does not have to arrive at each node at the same time.
Differing levels of contention within the tree, due to ﬂow
control at the leaves which backs up the tree, could cause a
request to arrive at one node before it arrives at another node.
Moreover, as a result, the data response to a request can
arrive at the requestor before its own request (as shown in the
bottom path of Figure 2). Because of the asynchrony, proces-
sor P1 can broadcast a request that arrives at the owner, P2,
long before it arrives back at P1. In the meanwhile, P2 has
already sent the response to P1 and it arrives ﬁrst.
Interconnection network complexity can lead to errors
that systems cannot currently detect. For example, a multi-
bit fault that corrupted a broadcast request (traveling down
the tree) in a non-root switch could cause that request to not
be delivered to a subset of the nodes. Also, a transient fault
in the arbitration logic of a switch could cause a newer mes-
sage to bypass an older message that had been buffered due
to ﬂow control and thus re-order the arrival of broadcast
coherence requests at a node. Once again, instead of adding
low-level error detection to target these new error models,
we would prefer an end-to-end checker. The message-level
checker that we develop in Section 6 tests the end-to-end
invariant that all nodes observe the same total order of broad-
cast requests; thus, it detects all message corruptions, drops,
and re-orderings across the system.
1. We cannot necessarily discard the low-level detectors, since the system
will need some of them for fault diagnosis. However, we can take them off