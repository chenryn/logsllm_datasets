Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:10:11 UTC from IEEE Xplore.  Restrictions apply. 
Curiosity propagates upstream as follows: A tick be-
comes anti-curious only if all downstream streams propa-
gate A to it. The A tick can then be propagated up to the
predecessor node (if a ﬁlter) or to all predecessor nodes (if
a merge). A C tick propagates to the predecessor node of a
ﬁlter edge. It propagates to those predecessors of a merge
edge that have Q ticks.
3 Algorithm and Implementation
In this section we describe the guaranteed delivery (GD)
protocol as it is implemented in Gryphon. The GD proto-
col follows the abstract model of the previous section and
is used in the studies of the next section. It simpliﬁes the
model slightly, in that there are no merges, and therefore
the knowledge graph is a forest consisting of a collection of
spanning trees each rooted at one of the pubends.
Virtual brokers are mapped onto collections of broker
machines called cells. Virtual brokers that host pubends
(pubend hosting brokers or PHBs), and subend hosting bro-
kers (SHBs) each map to a cell consisting of a single phys-
ical broker machine. Intermediate virtual brokers may map
to a cell consisting of multiple physical brokers. For in-
stance, Figure 3 shows the virtual and the physical topology
for the failure injection tests of Section 4, in which there
is a single PHB, two intermediate virtual brokers, and ﬁve
SHBs. Intermediate virtual brokers IB1 and IB2 correspond
to cells containing the physical broker pairs b1-b2, and b3-
b4, respectively. A virtual link in the virtual topology corre-
sponds to a link-bundle or fat link, containing multiple con-
nections. The multiple brokers and multiple connections
are used both to share load and to provide rapid backup on
failure. The physical brokers belonging to a cell maintain
connections with one another, so that if one physical broker
loses connectivity to a downstream cell, it may be able to
route messages to that cell via another broker in its cell that
has not lost connectivity.
3.1 Knowledge and Curiosity Propagation in a
Broker
Since there are no merges, it sufﬁces for each broker to
maintain for each pubend  an input stream data structure
istream[P], and for each downstream cell c receiving
messages from  a set of output stream data structures
ostream[P, c] connected to istream[P] by ﬁlter
edges. Each physical broker in a cell contains a replica of
these structures, although the different brokers may have
different states of knowledge about the different ticks.
Propagating Knowledge: Knowledge is propagated down-
stream using knowledge messages. A knowledge mes-
sage either has the form F*Q*F*DF*Q* (data message),
or F*Q*F*Q* (silence message). In both cases, the mes-
sage encodes (using timestamp ranges) a preﬁx of “past”
ticks whose values are known to be not needed. Data mes-
sages include a D tick “payload” bracketed by silence; si-
lence messages represent a range of F ticks.
We distinguish between two kinds of knowledge mes-
sages: (1) ﬁrst-time messages, and (2) retransmitted mes-
sages. First-time data messages are sent to all downstream
nodes that match the ﬁlters. First-time silence messages are
only sent to a downstream stream if there is at least one
C tick value in the corresponding ostream that overlaps
with the silence ticks. A ﬁrst-time data message can be
transformed into a ﬁrst-time silence message, if the D tick
is ﬁltered out. Retransmitted messages can originate at the
pubend or at an intermediate broker. They are only sent
down on paths that have expressed curiosity for some ticks
in the message (that is, where the ostream curiosity for
that tick is speciﬁcally C). A D tick in a retransmitted mes-
sage is transformed into a Q (i.e. removed from the mes-
sage) if the downstream cell is not curious for the D tick
(but is curious for some of the F ticks in the message).
Propagation and Interaction of A, F ticks: Ticks with
value A are propagated upstream using ack messages.
These messages contain a single timestamp T to encode
a range of ticks. An ack message sent from s2 to b1
with timestamp T changes all ticks [0,T] in ostream[P,
s2] into A. When the tick value, at tick t, in all os-
tream[P, c] turns into A, the corresponding tick value
in istream[P] is also turned into A, triggering an ack
message to be propagated upstream towards P.
Whenever a knowledge tick in any stream is F, its curios-
ity is changed to A. Hence D ticks that have been ﬁltered
(turned into F) at an intermediate broker can be immedi-
ately acked by it, without waiting for the F to propagate to
downstream subends.
Propagation and Consolidation of C ticks: Curiosity is
propagated upstream using nack messages originating at the
subend. Each nack message encodes a contiguous range
of C ticks. When a nack message is received at a bro-
ker, it tries to satisfy the nack using information in its out-
put stream. For instance, suppose b1 receives a nack from
stream istream[P] in cell s2. The broker ﬁrst checks
which ticks in the nack range are F or D in ostream[P,
s2]. These ticks can be satisﬁed locally. Knowledge
messages corresponding to the satisﬁed ticks are then sent
downstream. All unsatisﬁed ticks are changed to C in both
ostream[s2], and istream. Nacks are consolidated
by this process because a nack message is propagated up-
stream only if some C tick accumulated in istream was
not already C (meaning that a nack must have been sent ear-
lier).
Nacks are repeated if not satisﬁed within a certain time
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:10:11 UTC from IEEE Xplore.  Restrictions apply. 
Figure 3. A virtual broker tree and its physical realization
interval. The repetition interval (nack repetition threshold)
is estimated in a manner similar to how TCP estimates the
retransmission timeout value (RTO) [7]. The system is con-
ﬁgured with a minimum repetition interval. To ensure that
C ticks repeated by the same subend are not blocked by the
nack consolidation behavior described above, the C ticks
in an istream are forgotten after the minimum repetition
interval so that nacks appear “fresh.”
Propagation through Link Bundles: Let us consider a
message being routed from p1 towards its downstream
neighbor IB1. The link is chosen by hashing the pubend id
associated with the source pubend of this knowledge tree,
onto one of the available links. In this example, whenever
both the links p1-b1 and p1-b2 are operational, messages
from about half the pubends hosted by p1 will ﬂow to IB1
along p1-b1, and half along p1-b2. Suppose the path p1-b1
is chosen, and then the path b1-s1 is broken. In that case,
b1 will route messages towards s1 “sideways” via its cell
neighbor b2. Periodic link status messages are exchanged
between brokers so that this sideways routing is only tran-
sient, after which the messages will switch from the p1-b1
to the p1-b2 path. Note that during an interval of no failures
or recoveries, successive messages from the same pubend in
p1 will ﬂow on the same path towards the downstream sub-
scribers. Ack and nack messages sent upstream through a
link bundle are sent to whichever physical broker in the cell
last sent a downstream message from the relevant pubend; if
this information is lost, the messages are sent to all physical
brokers in the upstream cell.
3.2 Failures and Liveness
Broker and link failures lead to message loss, which
cause subends to see a gap in their knowledge stream. A
gap is a sequence of Q ticks between non-Q ticks. There
are two extreme approaches to resolving these gaps, both
of which involve retransmission of messages, (1) subend
driven liveness, and (2) pubend driven liveness. We provide
tuning parameters such that the system can be run with one
of these approaches or anything in between.
Subend-driven Liveness: This is mainly based on two live-
ness parameters, gap curiosity threshold (GCT) and nack
repetition threshold (NRT). The NRT parameter is esti-
mated by the subend based on the round-trip response to
previous nacks, and exponential backoff is used to handle
pubends that are down. When a gap is created in the knowl-
edge stream, the subend starts the GCT timer for the Q ticks
in the gap, and sends a nack after expiry of the timer, if the
Q ticks have not already been satisﬁed. Nacks for these
Q ticks are repeated every NRT interval, until they are sat-
isﬁed. An additional parameter, delay curiosity threshold
(DCT), is important to guard against the loss of the latest
message, when no pubend-driven liveness is in use. The
subend initiates a nack if its doubt horizon trails real time
by more than the DCT.
Pubend-driven Liveness: This is based on one parame-
ter, the ack expected threshold (AET). The pubend expects
all ticks that are more than AET interval before the current
time to be acked. If they are not, it sends an AckExpected
message with a timestamp T equal to the current time mi-
nus AET. The message ﬂows down on all paths that have
not acked up to T. A subend receiving this message will im-
mediately nack all ticks up to T that are Q in its knowledge
stream.
In our experiments, we typically run with low GCT and
NRT values, a higher AET, and a inﬁnite DCT. Hence
we are using a mixture of both liveness approaches, with
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:10:11 UTC from IEEE Xplore.  Restrictions apply. 
subend-driven liveness dominating.
4 Experimental Results
Two sets of experiments were performed to demonstrate
the overhead of the guaranteed delivery (GD) protocol and
its behavior in the presence of faults. The machines used for
the experiments are dedicated 6-processor Power-PC ma-
chines running AIX 4.3 with 3072 MB of memory. There
are two network interfaces attached to each machine - a gi-
gabit ethernet PCI adapter and a 100 Mbps ethernet PCI
adapter. The broker code is written in Java, and was run
using IBM’s JRE 1.3.
4.1 Comparison of Guaranteed Delivery and
Best-effort protocols
These experiments measure the failure-free overhead of
the GD protocol using two metrics: (1) mean CPU utiliza-
tion, and (2) median latency from publishers to the sub-
scribers. The best-effort delivery protocol used for com-
parison does not perform any knowledge accumulation, cu-
riosity propagation, message logging or retransmission, and
only sends downstream D tick messages. A two broker
asymmetric conﬁguration is used for the experiments. Pub-
lishing clients are connected to one broker, the pubend host-
ing broker (PHB), while the subscribing clients are con-
nected to the second broker, the subend hosting broker
(SHB). The latency seen by the subscribing clients con-
nected to the SHB is called remote latency. For measuring
local latency, a subscribing client is connected to the PHB.
The input message rate is 2000 messages/s and each pub-
lished message is 250 bytes long. Each subscriber receives
2 msgs/s on a dedicated TCP connection to the SHB. The
subscribers connect to the SHB through the gigabit network
since the fan-out out of the SHB is quite high (up to 32000
msgs/s on 16000 connections). The broker-to-broker con-
nection is on the 100 Mbps link.
Figure 4 shows the variation in CPU utilization at the
brokers while varying the number of subscribing clients.
The CPU utilization at
the subend hosting broker ex-
pectedly increases with increasing number of subscribing
clients and the utilization when running the guaranteed de-
livery (GD) protocol is higher than running best-effort de-
livery of messages. The difference between the GD proto-
col cost and the best-effort protocol cost does not increase
with the number of subscribers and stays constant at less
than 4%. This is because our implementation optimizes the
GD stream state needed by each subend, by consolidating it
across all subends at the same SHB. The ﬁgure also shows
that the CPU utilization at the PHB does not increase with
the number of subscribers connected to the SHB. The CPU
Figure 4. Variation of CPU utilization with
Number of Subscribers (Input Rate = 2000
msgs/s)
Figure 5. Variation of Latency with Number of
Subscribers (Input Rate = 2000 msgs/s)
overhead for GD (wrt best-effort) at the PHB, about 8%, is
more than that at the SHB, due to the overhead of logging.
Figure 5 shows how the local and remote latency vary
with the number of subscribing clients. The local latency
does not show an increasing trend with the number of sub-
scribers, since the subscribers are at the SHB, while the end-
points for the local latency are on the PHB. Expectedly, the
remote latency increases with the number of subscribers for
both GD and best-effort. However, the difference between
GD and best-effort remains approximately constant with in-
creasing number of subscribers. This latency difference of
about 100ms is due to the delay introduced by the logging of
guaranteed messages at the PHB. This constant difference
is observed in both the local and the remote latencies.
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:10:11 UTC from IEEE Xplore.  Restrictions apply. 
4.2 Failure Injection Results
These tests measure the system ﬂuctuation and recovery
in the presence of faults. We measure the system dynamics
under two types of faults, broker crash and link failure.
Experimental Setup: We conﬁgured a network of 10 bro-
kers in 8 cells as shown in ﬁgure 3. Broker p1 is desig-
nated as the pubend hosting broker. There are 4 interme-
diate brokers, b1-4, with b1-2 in one intermediate cell
and b3-4 in another. Each of the intermediate brokers
has a direct link to the pubend hosting broker. There are
5 subend hosting brokers s1-5, with s1 and s2 linked to
cell IB1 and s3-5 linked to cell IB2. Broker p1 hosts 4
pubends, each of which is receiving and logging 25 msgs/s
(each message is 100 bytes) from a publisher, for a total in-
put rate of 100 msgs/s. This low rate is used since we want
to observe system dynamics without hitting any processing
capacity constraints at brokers. The ﬁlters at intermediate
brokers allow all messages through. All these tests use the
following liveness parameters: GCT=200ms, NRT=600ms,
AET=10s, DCT=inﬁnity.
Metrics: Three metrics are used: (1) end to end message
latency, (2) number of nacks sent, and (3) nack range sent.
The nack range metric counts the number of time ticks (in
milliseconds) that are nacked. For instance, since each
pubend is publishing about 25 msgs/s, consecutive mes-
sages from a pubend are about 40ms apart. A nack range
of 800ms would mean that about 800/40=20 messages are
nacked. The nack metrics are measured at subend and in-
termediate brokers to check for nack consolidation.
Failures Injected: We present results from three kinds of
failures: (1) link failure of b1-s1, (2) crash failure of b1,
(3) crash failure of p1. We repeatedly injected each kind
of failure. We did not average data over multiple instances.
Instead, we chose to display a modal instance, in order to
preserve details of dynamic behavior.
Crash failures were injected by killing the broker pro-
cess, and link failures by closing the TCP connection. We
observed that since queueing of messages inside a broker
was rare, failures injected in this simple manner were imme-
diately detected by adjacent brokers, and caused messages
to be immediately switched to a different path. Therefore
many such failures did not result in even a single message
loss! In practice, it is likely that many failures are not de-
tected immediately by adjacent brokers. We therefore re-
vised our failure injection to include two steps: (1) the link
or broker to be failed was stalled for about 2-3 seconds dur-
ing which it accepted data but did not forward it, (2) then it
was failed. This caused about 2-3 seconds of data messages
to be lost.
Results for b1-s1 failure: Figure 6 shows the behavior of
the system when the link b1-s1 is failed for 10 seconds, for
a pubend whose messages were ﬂowing on p1-b1 before
latency in s1
nacks sent by s1
latency in s2
4000
3000
2000
1000
)
s