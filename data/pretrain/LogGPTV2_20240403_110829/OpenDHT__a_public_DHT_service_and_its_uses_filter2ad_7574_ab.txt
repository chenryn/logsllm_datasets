Throughout, we refer to OpenDHT keys by k; these are 160-bit
values, often the output of the SHA-1 hash function (denoted by
H), though applications may assign keys in whatever fashion they
choose. Values, denoted v, are variable-length, up to a maximum of
1 kB in size. All values are stored for a bounded time period only;
a client speciﬁes this period either as a TTL or an expiration time,
depending on the interface.
Finally, we note that under all three interfaces, OpenDHT pro-
vides only eventual consistency. In the case of network partitions
or excessive churn, the system may fail to return values that have
been put or continue to return values that have been removed. Im-
perfect clock synchronization in the DHT may also cause values
to expire at some replicas before others, leaving small windows
where replicas return different results. While such temporary in-
consistencies in theory limit the set of applications that can be built
on OpenDHT, they have not been a problem to date.
3.1.1 The Current Interface
A put in OpenDHT is uniquely identiﬁed by the triple of a key,
a value, and the SHA-1 hash of a client-chosen random secret up
to 40 bytes in length. If multiple puts have the same key and/or
value, all are stored by the DHT. A put with the same key, value,
and secret hash as an existing put refreshes its TTL. A get takes a
key and returns all values stored under that key, along with their
associated secret hashes and remaining TTLs. An iterator interface
is provided in case there are many such values.
To remove a value, a client reveals the secret whose hash was
provided in the put. A put with an empty secret hash cannot be
removed. OpenDHT stores removes like puts, but a DHT node dis-
cards a put (k, v, H(s)) for which it has a corresponding remove. To
prevent the DHT’s replication algorithms from recovering this put
when the remove’s TTL expires, clients must ensure that the TTL
on a remove is longer than the TTL remaining on the correspond-
ing put. Once revealed in a remove, a secret should not be reused
in subsequent puts. To allow other clients to remove a put, a client
may include the encrypted secret as part of the put’s value.
To change a value in the DHT, a client simply removes the old
value and puts a new one.
In the case where multiple clients
perform this operation concurrently, several new values may end
up stored in the DHT. In such cases, any client may apply an
application-speciﬁc conﬂict resolution procedure to decide which
of the new values to remove. So long as this procedure is a total or-
dering of the possible input values, it does not matter which client
performs the removes (or even if they all do); the DHT will store
the same value in the end in all cases. This approach is similar to
that used by Bayou [24] to achieve eventual consistency.
Since OpenDHT stores all values put under a single key, puts
are robust against squatting, in that there is no race to put ﬁrst
under a valuable key (e.g., H(“coca-cola.com”)). To allow oth-
ers to authenticate their puts, clients may digitally sign the values
they put into the DHT. In the current OpenDHT interface, however,
such values remain vulnerable to a denial-of-service attack we term
drowning: a malicious client may put a vast number of values under
a key, all of which will be stored, and thereby force other clients to
retrieve a vast number of such chaff values in the process of retriev-
ing legitimate ones.
3.1.2 Planned Interfaces
Although the current put/get interface sufﬁces for the applica-
tions built on OpenDHT today, we expect that as the system gains
popularity developers will value protection against the drowning
attack. Since this attack relies on forcing legitimate clients to sort
through chaff values put into the DHT by malicious ones, it can
only be thwarted if the DHT can recognize and reject such chaff.
The two interfaces below present two different ways for the DHT
to perform such access control.
Immutable puts: One authenticated interface we plan to add to
OpenDHT is the immutable put/get interface used in CFS [9] and
Pond [28], for which the DHT only allows puts where k = H(v).
Clearly, such puts are robust against squatting and drowning. Im-
mutable puts will not be removable; they will only expire. The main
limitation of this model is that it restricts an application’s ability to
choose keys.
Signed puts: The second authenticated interface we plan to add
to OpenDHT is one where values put are certiﬁed by a particular
public key, as used for root blocks in CFS. In these puts, a client
employs a public/private key pair, denoted KP and KS, respectively.
We call H(KP) the authenticator.
Procedure
join(host, id, namespace)
lookup(key, namespace)
Functionality
adds (host, id) to the list of hosts
providing functionality of namespace
returns (host, id) in namespace
whose id most immediately follows key
Table 2: The lookup interface provided using ReDiR.
In addition to a key and value, each put includes: a nonce n
that can be used to remove the value later; an expiration time t in
seconds since the epoch; KP itself; and σ = {H(k, v, n,t)}KS, where
{X}KS denotes the digital signing of X with KS. OpenDHT checks
that the digital signature veriﬁes using KP; if not, the put is rejected.
This invariant ensures that the client that sent a put knows KS.
A get for an authenticated put speciﬁes both k and H(KP), and
returns only those values stored that match both k and H(KP).
In other words, OpenDHT only returns values signed by the pri-
vate key matching the public key whose hash is in the get request.
Clients may thus protect themselves against the drowning attack by
telling the DHT to return only values signed by an entity they trust.
To remove an authenticated put with (k, v, n), a client issues a
remove request with (k, H(v), n). As with the current interface,
clients must take care that a remove expires after the corresponding
put. To re-put a value, a client may use a new nonce n
(cid:3) (cid:4)= n.
We use expiration times rather than TTLs to prevent expired puts
from being replayed by malicious clients. As with the current inter-
face, puts with the same key and authenticator but different values
will all be stored by the DHT, and a new put with the same key, au-
thenticator, value, and nonce as an existing put refreshes its TTL.
Authenticated puts in OpenDHT are similar to those used for
public-key blocks in CFS [9], for sfrtags in SFR [33], for ﬁleIds in
PAST [14], and for AGUIDs in Pond [28]. Like SFR and PAST,
OpenDHT allows multiple data items to be stored using the same
public key. Unlike CFS, SFR, and PAST, OpenDHT gives applica-
tions total freedom over key choice (a particular requirement in a
generic DHT service).
3.2 ReDiR
While the put/get interface is simple and useful, it cannot meet
the needs of all applications. Another popular DHT interface
is lookup, which is summarized in Table 2.
In this interface,
nodes that wish to provide some service—packet forwarding, for
example—join a DHT dedicated to that service. In joining, each
node is associated with an identiﬁer id chosen from a key space,
generally [0 : 2160). To ﬁnd a service node, a client performs a
lookup, which takes a key chosen from the identiﬁer space and re-
turns the node whose identiﬁer most immediately follows the key;
lookup is thus said to implement the successor relation.
For example, in i3 [31], service nodes provide a packet forward-
ing functionality to clients. Clients create (key, destination) pairs
called triggers, where the destination is either another key or an IP
address and port. A trigger (k, d) is stored on the service node re-
turned by lookup (k), and this service node forwards all packets it
receives for key k to d. Assuming, for example, that the nodes A
through F in Figure 2 are i3 forwarding nodes, a trigger with key
B ≤ k < C would be managed by service node C.
The difﬁculty with lookup for a DHT service is the functional-
ity implemented by those nodes returned by the lookup function.
Rather than install application-speciﬁc functionality into the ser-
vice, thereby certainly increasing its complexity and possibly re-
ducing its robustness, we prefer that such functionality be sup-
ported outside the DHT, while leveraging the DHT itself to per-
Level 0
Level 1
Level 2
Level 3
Client keys
Client addresses
A
A
A
B
B
C
C
C
F
E F
D E
D E
B
C
D
E
F
Figure 2: An example ReDiR tree with branching factor b = 2.
Each tree node is shown as a contiguous line representing the
node’s interval of the keyspace, and the two intervals associated
with each node are separated by a tick. The names of registered
application hosts (A through F) are shown above the tree nodes
at which they would be stored.
form lookups. OpenDHT accomplishes this separation through the
use of a client-side library called ReDiR. (An alternative approach,
where application-speciﬁc code may only be placed on subsets of
nodes within the DHT, is described in [18].) By using the ReDiR
library, clients can use OpenDHT to route by key among these
application-speciﬁc nodes. However, because ReDiR interacts with
OpenDHT only through the put/get API, the OpenDHT server-side
implementation retains the simplicity of the put/get interface.
A DHT supporting multiple separate applications must distin-
guish them somehow; ReDiR identiﬁes each application by an
arbitrary identiﬁer, called its namespace. Client nodes provid-
ing application-speciﬁc functionality join a namespace, and other
clients performing lookups do so within a namespace. A ReDiR
lookup on identiﬁer k in namespace n returns the node that has
joined n whose identiﬁer most immediately follows k.
A simple implementation of lookup could be achieved by storing
the IP addresses and ports of all nodes that have joined a namespace
n under key n; lookups could then be performed by getting all the
nodes under key n and searching for the successor to the key looked
up. This implementation, however, scales linearly in the number
of nodes that join. To implement lookup more efﬁciently, ReDiR
builds a two-dimensional quad-tree of the nodes that have joined
and embeds it in OpenDHT using the put/get interface.3 Using
this tree, ReDiR performs lookup in a logarithmic number of get
operations with high probability, and by estimating the tree’s height
based on past lookups, it reduces the average lookup to a constant
number of gets, assuming uniform-random client IDs.
The details are as follows: each tree node is list of (IP, port)
pairs for a subset of the clients that have joined the namespace.
An example embedding is shown in Figure 2. Each node in the
tree has a level, where the root is at level 0, its immediate chil-
dren are at level 1, etc. Given a branching factor of b, there are
thus at most bi nodes at level i. We label the nodes at any level
from left to right, such that a pair (i, j) uniquely identiﬁes the jth
node from the left at level i, and 0 ≤ j < bi. This tree is then
embedded in OpenDHT node by node, by putting the value(s) of
node (i, j) at key H(ns, i, j). The root of the tree for the i3 appli-
cation, for example, is stored at H(“i3”,0,0). Finally, we associate
(cid:1)
with each node (i, j) in the tree b intervals of the DHT keyspace
2160b
We sketch the registration process here. Deﬁne I((cid:4), k) to be the
(unique) interval at level (cid:4) that encloses key k. Starting at some
level (cid:4)start that we deﬁne later, a client with identiﬁer vi does an
OpenDHT get to obtain the contents of the node associated with
for 0 ≤ b
−i( j + b
−i( j + b
(cid:3)
b
), 2160b
(cid:2)
)
(cid:3)+1
b
(cid:3) < b.
3The implementation of ReDiR we describe here is an improve-
ment on our previous algorithm [19], which used a ﬁxed tree height.
I((cid:4)start, vi). If after adding vi to the list of (IP, port) pairs, vi is
now the numerically lowest or highest among the keys stored in
that node, the client continues up the tree towards the root, getting
the contents and performing an OpenDHT put in the nodes associ-
ated with each interval I((cid:4)start− 1, vi), I((cid:4)start− 2, vi), . . ., until it
reaches either the root (level 0) or a level at which vi is not the low-
est or highest in the interval. It also walks down the tree through
the tree nodes associated with the intervals I((cid:4)start, vi), I((cid:4)start +
1, vi), . . ., at each step getting the current contents, and putting its
address if vi is the lowest or highest in the interval. The downward
walk ends when it reaches a level in which it is the only client in
the interval. Finally, since all state is soft (with TTLs of 60 seconds
in our tests), the entire registration process is repeated periodically
until the client leaves the system.
A lookup (ns, k) is similar. We again start at some level (cid:4) =
(cid:4)start. At each step we get the current interval I((cid:4), k) and determine
where to look next as follows:
1. If there is no successor of vi stored in the tree node associated
with I((cid:4), k), then its successor must occur in a larger range of
the keyspace, so we set (cid:4) ← (cid:4)− 1 and repeat, or fail if (cid:4) = 0.
2. If k is sandwiched between two client entries in I((cid:4), k), then
the successor must lie somewhere in I((cid:4), k). We set (cid:4) ← (cid:4)+1
and repeat.
3. Otherwise, there is a client s stored in the node associated
with I((cid:4), k) whose identiﬁer vs succeeds k, and there are no
clients with IDs between k and vs. Thus, vs must be the suc-
cessor of k, and the lookup is done.
A key point in our design is the choice of starting level (cid:4)start.
Initially (cid:4)start is set to a hard-coded constant (2 in our implementa-
tion). Thereafter, for registrations, clients take (cid:4)start to be the low-
est level at which registration last completed. For lookups, clients
record the levels at which the last 16 lookups completed and take
(cid:4)start to be the mode of those depths. This technique allows us to
adapt to any number of client nodes while usually hitting the cor-
rect depth (Case 3 above) on the ﬁrst try.
We present a performance analysis of ReDiR on PlanetLab in
Section 5.2.
4. STORAGE ALLOCATION
In Section 2.2, we presented our design goals for the OpenDHT
storage allocation algorithm:
that it provide storage with a deﬁ-
nite time-to-live (TTL), that it allocate that storage fairly between
clients and with high utilization, and that it avoid long periods in
which no space is available for new storage requests. In this sec-
tion we describe an algorithm, Fair Space-Time (FST), that meets
these design goals. Before doing so, though, we ﬁrst consider two
choices we made while deﬁning the storage allocation problem.
First, in this initial incarnation of OpenDHT, we equate “client”
with an IP address (spooﬁng is prevented by TCP’s three-way hand-
shake). This technique is clearly imperfect: clients behind the same
NAT or ﬁrewall compete with each other for storage, mobile clients
can acquire more storage than others, and some clients (e.g., those
that own class A address spaces) can acquire virtually unlimited
storage. To remedy this situation, we could clearly use a more so-
phisticated notion of client (person, organization, etc.) and require
each put to be authenticated at the gateway. However, to be com-
pletely secure against the Sybil attack [13], this change would re-
quire formal identity allocation policies and mechanisms. In order
to make early use of OpenDHT as easy as possible, and to pre-
vent administrative hassles for ourselves, we chose to start with the
much more primitive per-IP-address allocation model, and we hope
space
C
sum
potential
put
space
C
sum
potential
put
future puts
(slope=r
min
)
future puts
(slope=r
min
)
0
now
now+max_ttl
time
0
now
(a)
now+max_ttl
time
(b)
Figure 3: Preventing starvation.
to improve on it in the future. More generally, we discuss in Sec-
tion 6 how our current free service could transition to a competitive
commercial market in DHT service.
Second, OpenDHT is a large distributed system, and at ﬁrst one
might think that a fair allocation mechanism should consider the
global behavior of every client (i.e., all of their puts). While track-
ing global behavior in this way presents a daunting problem, it is
also the case that the capacity constraints of OpenDHT are per-
node, in the form of ﬁnite disk capacities, so the situation is even
more complicated.4
We note that OpenDHT cannot avoid providing some notion of
per-disk fairness in allocation. For example, a common use of the
system is for rendezvous, where a group of cooperating clients dis-
cover each other by putting their identities under a common key,
k. With a strictly global model of fairness, a malicious client could
disrupt this rendezvous by ﬁlling the disk onto which k is mapped,
so long as it remained below its globally fair allocation. A per-disk
model of fairness, in contrast, promises each client a fair allocation
of every disk in the system, preventing such attacks.