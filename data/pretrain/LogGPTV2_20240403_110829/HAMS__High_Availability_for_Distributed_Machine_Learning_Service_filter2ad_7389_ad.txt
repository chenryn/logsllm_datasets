immediately work as a backup by overwriting its state with
the new primary’s.
HAMS’s recovery protocol
for correlated failures (i.e.,
failures of adjacent models) can be easily deduced by
integrating the two recovery protocols for single failures,
as the essential step during a failover is to reconstruct the
dataﬂow, for a newly launched stateless model or a new
stateful primary, based on the lineage information provided by
a failed model’s successors. If a contacted successor model
fails, this information can be conservatively re-constructed
from its backup (for stateful) or its successor (for stateless)
models. During the recovery of correlated failures, duplicate
intermediate requests may be resent, but HAMS can discard
them trivially because intermediate requests have sequence
numbers. Overall, HAMS can tolerate arbitrary failures on
stateless models, and at the same time, can tolerate one point
of failure (primary or backup) for each stateful model.
F. Proof Sketch of Correctness
p, generated si
p, and released oi
In this subsection, we prove that HAMS’s failover ensures
global consistency for a general service DAG. If multiple
services share one model, they can be merged as a single
service DAG. To prove global consistency, it is sufﬁcient to
prove the following statement. Given that a stateful model Mp
that processed ri
p to downstream
models; oi
p is transformed by a series of stateless models,
and is processed as rj
q on next stateful model Mq. If the
resultant state sj
p will
be recovered unchanged. This statement is easily proved in
NSPB: si
p will be recovered consistently because HAMS’s
protocol lets a downstream model wait until all its upstream
models’ states are durable (§IV-C). Therefore, sj
q being durable
derives that si
p are durable and can be trivially recovered.
This statement is sufﬁcient for global consistency because
if sj
q is not durable yet, the backup of Mq contains a past state
sk
q with k < j. Therefore, even if the upstream Mp generates
a different (non-deterministic) state and output for processing
q is durable, then after a failover, si
p, oi
p, oi
p, oi
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:27:08 UTC from IEEE Xplore.  Restrictions apply. 
ri
p during failover, HAMS can safely discard sj
the backup of Mq as the new primary.
q by promoting
p stays consistent, rj
A concern may be whether rj
q will be recovered consistently.
Although oi
q may diverge due to the
non-determinism of intermediate stateless operators. However,
the subtle point is that, these intermediate models are stateless,
so HAMS does not need to let them redo the computation to
generate internal states. As the conditions already state that sj
q
is durable, we do not need to care about rj
q.
V. IMPLEMENTATION
HAMS’s implementation has about 11K LoC, and HAMS’s
frontend and gRPC servers are based on Tensorﬂow Serving’s
code base [78]. In HAMS, all
the operators and HAMS’s
components (i.e., frontend, manager, and proxies) are running
in containers. Overall, HAMS’s NSPB implementation has two
major components. First, the proxy (§III-A) of each operator
is written in C/C++ and agnostic to the ML operator served.
Communications among proxies are using gRPC [23] and
Protobuf [64]. Second, for each operator that deploys an
ML model, HAMS provides a library that
implemented a
gRPC server and API for an asynchronous state retrieving
mechanism, described in §IV-B. We implemented HAMS’s and
all the models based on PyTorch v1.2 and CuDNN v10.0.
For each ML model deployed in HAMS,
the ML
developer needs to implement two interfaces, initialize(),
run(). The initialize() will initialize the model (i.e.,
load the model from disk to GPU memory) and returns
references of tensors that contains a model’s state (e.g.,
model.parameters.data()). A developer will implement
the run() interface that takes the inputs of computation (either
training request or inference requests), and an event object to
explicitly identify the compute and update stage in HAMS. We
used PyTorchs API (i.e., torch.tensor.to_()) for non-stop
copy of tensor memory (the state) between GPU and CPU.
In our evaluation, integrating each of the evaluated ML
operators to HAMS added only 4-10 LoC with HAMS’s API.
To do fast failover for stateless operators, in HAMS, we run
one hot standby for each type of stateless operators among
all service graphs running in one cluster. For instance, HAMS
runs one hot standby with all necessary ML libraries loaded
for all KNN operators. By doing so, recovering one stateless
operator in a service graph requires only loading the operator’s
parameters, which takes only hundreds of milliseconds. We
used this optimized stateless standby operator setting for all
systems we evaluated to make a fair comparison on recovering
failed stateless operators (§VI-D).
VI. EVALUATION
A. Evaluation Setup
Our evaluation was done on a GPU farm of ﬁve hosts with
in total 100 CPU cores and 20 Nvidia RTX2080TI graphic
cards. Each graphic card is connected to the host with PCIe
3.0. The ping latency across hosts is 0.17ms and the network
bandwidth is 40 Gbps. We evaluated HAMS with 25 operators
of mature and well-known ML models, and parameters are
trained by us using well-known datasets (e.g., CIFAR-10 [38])
on PyTorch. We used these operators to build six practical ML
services: sentiment and subject analysis (SA), stock prediction
(SP), auto-pilot (AP), image query (IQ), online learning of
a VGG19 model (OL(V)) with a heavyweight model size
(548.05MB) and online learning of a MobileNet (OL(M))
with a lightweight model size (13.37MB). The online learning
service ﬁne-tunes an image labeling model and infers the
image context (e.g., a man is with a dog) both for the inference
images and the labeled training images. These services are
often online and mission-critical, so HAMS’s high availability
support is desirable for these services.
Figure 8 and Figure 9 describes the semantic of each
service. All models’ algorithms are all well-known and taken
from third-party. All data sets are downloaded from third-party
databases or Internet, including Kaggle Speech [32], NYSE
Stock data [57], Twitter data [79], autopilot data [17],
UTKFace data [80], and CIFAR-10 [38].
We compared HAMS with three systems. To evaluate
HAMS’s overhead, we implemented a bare metal system by
disabling all fault tolerance features of HAMS. To evaluate
the effectiveness of HAMS’s NSPB protocol, we implemented
a HAMS-Remus system. In HAMS-Remus, for each stateful
operator, we applied a primary-backup protocol following
Remus [13]: after processing each request batch, the primary
stops, copies the state update into its memory buffer, holds the
output until the state is successfully received by the backup.
Same as HAMS, HAMS-Remus is a white-box approach
that replicates only a model’s state (i.e., parameters) and
safely ignores intermediate computation results or framework
memory (e.g., PyTorch). Therefore, we considered our
evaluation between HAMS and HAMS-Remus fair. Black-box
Remus (Remus-VM) will run slower than HAMS-Remus,
because Remus-VM must record all dirty GPU memory pages
(§IV-B) and replicate unnecessary memory (e.g., intermediate
computation results). In practice, only a portion of dirty GPU
memory pages store a stateful model’s state (parameters) [22].
Since the relevant system Lineage Stash (LS) [85] is not
open source, to compare HAMS’s performance overhead with
LS, we implemented LS on HAMS’s code base according to
the LS paper: the proxy of each operator logs requests and
requests’ interleaving locally, and propagates the requests’
interleaving along with its output; the local buffer in each
proxy is periodically and asynchronously checkpointed to
a global storage; each stateful operator is periodically (per
150 requests, the smallest default setting from the LS paper)
checkpointed for recovery. When a failure occurs, the stateful
operator is replayed from the latest checkpoint with the logged
requests and interleaving. We focus on these questions:
§VI-B : How is HAMS’s normal case overhead compared to
the relevant systems?
§VI-C : How effective is NSPB in maintaining HAMS’s low
performance overhead?
§VI-D : How is HAMS’s recovery time compared to LS and
HAMS-Remus?
§VI-E : What are the limitations of HAMS?
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:27:08 UTC from IEEE Xplore.  Restrictions apply. 
191
SA
ID Size(MB) Input Output
1 793
2 121.7
3 121.7
2 34.8
4 15.3
5 N/A
6 N/A
1 90.9
2 375.9
3 13.2
4 6.2
5 29.6
1 90.92
2 199.7
3 90.92
4 209.3
OL-V 3 548.05
OL-M 3 13.37
Audios Word list
Words Sentiment
Words Subject
Words Sentiment
Stock Stock
Stock Stock
Stock Stock
Images Tagged image
Images Motion
Map
Route plan
Meta Car move
Meta Car move
Images Tagged image
Images Detect result
Images Tagged image
Images Detect result
Images Labeled images VGG19 [73]
Images Labeled images MobileNet [28]
Model
LSTM [20]
LSTM [86]
LSTM [86]
LSTM [86]
LSTM [53]
ARIMA [9]
KNN [88]
InceptionV3 [76]
DeconvLSTM [20]
LSTM [20]
A* search [44]
CNN [8]
InceptionV3 [76]
DeconvLSTM [20]
InceptionV3 [76]
DeconvLSTM [20]
AP
SP
FD
Fig. 8: Service graphs of the six services we evaluated.
Fig. 9: Services’ operators.
B. Performance
Figure 10 shows the latency that a request traverses a service
graph (normalized to latency of the bare metal) of the services
on four systems, including the bare metal, LS, HAMS, and
HAMS-Remus. The request batch sizes of all the six services
were 64, a typical setting for ML deployments. Overall,
compared with the bare metal, HAMS achieved 0.5% to 3.7%
latency overhead, which is comparable to the results reported
in the LS paper. HAMS achieved such a low latency overhead
due to two reasons. First, its NSPB protocol eliminates the
stop-and-buffer delay in every stateful operator in a service
graph, enabling the operators to process requests in an efﬁcient
pipeline as the bare metal. Second, although HAMS does
records requests’ lineage information to construct the causal
dependency of the states across upstream and downstream
operators (§IV-D), HAMS’s logging time cost for each batch
of requests processed by each operator was at most 2.1ms,
much smaller than the processing time of an operator on a
batch of requests (typically, hundreds of milliseconds).
HAMS-Remus
incurred the highest
latency overhead
(6.0%-97.7%) because each stateful operator needs
to
stop-and-copy the state to its local memory for every request
batch and to hold the output until
the updated state is
received by the backup. Moreover, for a serving graph that
has multiple stateful operators on one path (e.g., AP), the
latency overhead of HAMS-Remus was even higher because
the request propagation was delayed by multiple times. For
SA, HAMS-Remus’s latency overhead was small because in the
SA service graph, a stateless operator (i.e., audio transcriber)
took the most time (1471.23ms), and HAMS-Remus’s fault
Fig. 10: Normalized latency of six services running on four
systems.The request batch size is 64 for all services. The
latency is normalized to bare metal in order to ﬁt in one ﬁgure.
The absolute value of HAMS’s latency is in Table I.
192
tolerance logic added only 101.23ms to latency.
To further understand HAMS’s performance overhead, in
Figure 11a, we analyzed the sensitivity of HAMS’s latency to
the batch sizes of requests on the six services. Overall, when
the request batch size of a service increases from 1 to 128,
HAMS’s latency overhead is greatly reduced. On the batch size
of 64 or 128, HAMS’s latency overhead was at most 3.8%.
HAMS’s latency overhead can be broken down into two parts
depending on the nature of a service. The ﬁrst part is the online
learning services (OL(V) and OL(M)). When the batch size
was small (e.g., 1), HAMS’s latency overhead was almost as
high as HAMS-Remus. The reason is that the model operator’s
state (i.e., model parameters) is static with any request batch
size. When the batch size was small, the computation and
update stage of the downstream operator (e.g., operator 4 in
OL(V)) were both fast (e.g., 12.80ms and 2.43ms in OL(V)),
while HAMS’s state retrieve time (e.g., 134.52ms in OL(V))
and state delivery time to backup (e.g., 156.43ms in OL(M))
on this operator were the major factor for latency. Therefore,
NSPB’s non-stop primary-backup was not able to mask the
latency overhead on this small batch size. Fortunately,
in
practice, the batch size of an online learning service is often at
least 64 [85], and HAMS’s latency overhead was merely 2.8%.
The second part is the inference services (SA, SP, AP,
and FD) that contain stateful operators (e.g., LSTM). HAMS’s
latency overhead was consistently small (less than 10%) when
the batch size varied from 1 to 128. The reason is that the
size of LSTM’s internal state is linear to the request batch
size (i.e., each request owns a copy of state). Hence, when the
batch size was small (e.g., 1), although the processing time of
downstream operators was small (e.g., 17.43ms in FD), and