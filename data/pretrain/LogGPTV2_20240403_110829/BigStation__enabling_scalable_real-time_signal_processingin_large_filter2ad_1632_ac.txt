Channel decoding. Parallelizing the decoding algorithm is straight-
forward for block-based channel codes, e.g., Turbo and LDPC (Low-
Density Parity Check) codes. In these coding schemes, bits are di-
vided into blocks, e.g., of a few hundreds to thousands of bits, and
are encoded separately. We can simply assign each coded block to
a separate CPU core (or a separate PC server) and achieve a higher
aggregate throughput. However, parallelization becomes tricky for
codes that work on a continuous bit stream, for example, the con-
volutional code widely used in wireless communication standards.
Consequently, the corresponding decoding algorithm, i.e., Viterbi
decoding, cannot be parallelized naively as it needs to decode over
a continuous bit stream as well. In this paper, we develop a trick
that artiﬁcially partitions the bit stream into independent blocks of
L bits, and assigns each block to a different core (or a separate PC
server). The negative effect of this artiﬁcial partition is that it breaks
the convolutional property of the code and may signiﬁcantly reduce
the decoding performance at the edges of blocks. Fortunately, this
issue can be mitigated by overlapping the blocks as shown in Fig-
ure 4. Each block contains a preﬁx and a sufﬁx of D bits each,
which help the decoding path in between to converge to optimum.
The Viterbi decoder processes the entire block, but only outputs the
bits between the preﬁx and the sufﬁx. According to the Viterbi the-
orem, when D is large enough, i.e., D ≥ 5 ∗ K, where K is the
constraint length of the convolutional code (typically K = 7 as
in the 802.11 standard), with a probability close to 1, the decoded
bits from blocks are identical to those from processing the entire bit
stream [22].
We did further analysis to choose the right block size. Clearly, a
larger block size would be more efﬁcient as it amortizes the over-
head of preﬁxes and sufﬁxes, but a larger block also means a longer,
undesirable decoding latency. We therefore aim at fully utilizing the
computational power while keeping the block size as small as pos-
sible. This can be achieved if we keep the inter-block gap as small
as possible. The ideal value of this gap is zero, meaning that a new
block is scheduled on the same core right after the previous block is
ﬁnished. Assuming the coded bit-stream comes at a rate of u, a de-
coder module can process at a rate of v, and there are m processing
continueous stream of convolutionally coded bits Overlapped bits: 2DDecoded bits: Linter-blockgapB403units, the inter-block gap will be zero if
u
B
v
= mL.
This equation implies that during the processing of a block, exactly
m blocks worth of bits will arrive, and each processing core can
take a new block immediately after it ﬁnishes the current one. Tak-
ing B = L + 2D, we have
2Du
mv − u
.
∗
L
=
When the input rate is close to the processing capacity (i.e., u →
mv), L∗ will increase quickly. To prevent an unreasonable delay,
we choose a bound Lmax = 2048. With this upper bound, the
preﬁx and sufﬁx overhead combined is less than 3%.
5.3 Computation partitioning across servers
If data partitioning does not provide enough parallelism for each
signal processing module to be executed on one PC server, we can
further parallelize the operations across multiple servers. This can
happen when M becomes very large – large enough that processing
the smallest data set (e.g., a single subcarrier) may still require more
than the processing power of a PC server.
Channel inversion. The parallel matrix inversion algorithm dis-
cussed previously can be used across servers, where we send sub-
sets of rows to different servers and each server can perform Gaus-
sian elimination in parallel. However, one difﬁculty is pivoting. In
Gaussian elimination, pivoting exchanges two rows, so that the di-
agonal entry used in elimination is nonzero and has, preferably, a
large magnitude. To pivot across servers is cumbersome, as it may
cause all servers to exchange information about their rows of data,
incurring a heavy overhead on the network. Fortunately, the chan-
nel matrix in MU-MIMO is Hermitian (i.e., H∗H) and pivoting is
not necessarily needed [7]. The servers only need to broadcast the
row that is used for elimination for each iteration. Since each row
will be sent at most once for one elimination iteration, the commu-
nication overhead is bounded by the size of the matrix.
Another concern for parallelizing matrix inversion across servers
arises from the need to synchronize among all servers at every elim-
ination iteration. Given the non-deterministic delays in Ethernet,
this could cause blocking in many servers and hold up the comple-
tion of channel inversion. However, these servers need to process a
series of matrices, one for each new incoming frame. With a care-
fully designed multi-threaded algorithm, a server can immediately
work on the next matrix if it is blocked on the current one. As a
consequence, the aggregate throughput will not be affected.
Spatial demultiplexing. As M grows, a PC server may not be able
to handle spatial demultiplexing for M antennas even on a single
subcarrier. If a server only has enough power to compute the mul-
tiplication of a K × K matrix with a K-vector in real time, an SD
module in this system will only be able to handle K < M anten-
nas. Therefore, the computation of any xi should be separated to
K (cid:101) servers. We show how this can be done in Figure 7. The M
(cid:100) M
FS servers send y1,··· , yM to the ﬁrst group of (cid:100) M
K (cid:101) SD servers.
Each SD server then computes a partial result for x1,··· , xk, and
sends them to be further combined at another layer of servers. Re-
K (cid:101) such groups to output all x1,··· , xN .
peatedly, we construct (cid:100) N
The whole operation proceeds in two phases. In the ﬁrst spread
phase, each FS server multicasts yi to the intermediate ( i−1
K + 1)th
K (cid:101) groups. Each intermediate server then computes
servers in all (cid:100) N
the partial results for K spatial streams. In the second reduce phase,
the partial results are combined accordingly at another N servers to
generate the ﬁnal xj.
Figure 5: Deep distributed pipeline for spatial demultiplexing
for an extremely large M. Some communication links are omit-
ted for clarity.
5.4 Putting it all together
To summarize, here is how the distributed processing pipeline is
constructed in BigStation:
Uplink:
• For pilot symbols after FFT, the FS server divides subcarriers
groups and sends each group to a distinct CI server,
into W
ci
assuming each server can handle ci subcarriers.
• For data symbols after FFT, the FS server divides subcarriers
groups and sends each group of symbols to a distinct
into W
cs
SD server, assuming each server can handle cs subcarriers.
• Each CI server performs channel inversion on the received
pilot bits and sends the result to the corresponding SD server.
• Each SD server separates the spatial streams from incoming
symbol streams, and sends symbols belonging to one spatial
stream to one CD server.
• Each CD server collects symbols from all subcarriers for one
spatial stream and performs channel decoding.
Downlink:
• Each CE server generates channel coded bits and maps them
onto symbols on each subcarrier. It divides subcarriers into
groups and sends each group of symbols to a distinct PR
W
cs
server, assuming each server can handle cs subcarriers.
• Each CE server also divides channel state information of all
subcarriers into W
groups and sends each group to a distinct
ci
CI server, assuming each CI server can handle ci subcarriers.
• Each CI server performs channel inversion on the received
channel state information and sends the precoding vectors to
the corresponding PR server.
• Each PR server performs precoding on incoming symbol streams
and sends symbols belonging to one spatial stream to a dis-
tinct FS server.
• The FS servers cooperatively transmit the precoded symbols
simultaneously.
In cases where a single server cannot handle the computation for
a single subcarrier (i.e., ci or cs < 1), the corresponding server
may be replaced by a deeper pipeline of servers as discussed in
Section 5.3.
y1ykyMspreadreducex1xkxNxN-k+1404Table 1: Communication over multiple cores (Gbps).
# of cores Receive
1
2
4
5.9
8.6
9.2
Send Receive & Send
2.4 (R) / 7.8 (S)
9.2
5.1 (R) / 7.0 (S)
9.4
9.4
5.9 (R) / 6.8 (S)
Parallelizing communication across cores. Besides the compu-
tation, the underlying software should also handle the communica-
tion among BigStation servers. This is especially critical for the
CI servers and the SD servers as they are required to receive/send
data from/to all FS servers. In the following, we focus on the CI
and SD servers. Since both CI and SD servers are equipped with
10 Gbps NICs, ideally we would like the server to be able to han-
dle full-speed trafﬁc on both the uplink and the downlink, at a total
throughput of 20 Gbps. However, such an amount of data trafﬁc
cannot be handled by a single CPU core in our PC server. As a
consequence, we need to further exploit multi-core parallelism to
handle network trafﬁc as well.
We study the impact of using multiple cores on network commu-
nication experimentally. In our experiments, we let one SD server
receive 12 digital sample streams generated from all FS servers as
fast as possible. Since we are focusing on the communication per-
formance, we instruct the SD server to directly send the received
digital samples to a CD server without performing spatial demulti-
plexing.
Table 1 summarizes the results. Although we can send fast enough
to saturate the link (9.2 Gbps) with a single core, the receiving
throughput is only about 5.9 Gbps. Since now CPU is the bot-
tleneck, multiplexing sending and receiving on the same core may
reduce throughput in both directions and also cause huge unfair-
ness between the uplink and the downlink (Table 1 row 1, column
3). Using two CPU cores, we can almost achieve the full link speed
for either sending or receiving. However, with simultaneous send-
ing/receiving operations, the total throughput we can get in both
directions is 12 Gbps, despite the theoretical maximum of 20 Gbps.
We have carefully checked our code to avoid any interlocking be-
tween our sending and receiving procedures. Therefore, we believe
there are some interactions inside the Mellanox driver/NIC. Un-
fortunately, both the driver and the NIC are closed to us, which
prevents us from ﬁnding the exact reasons. Adding more cores for
communication does not improve the performance any further. This
is reasonable as now the NIC becomes the bottleneck. Therefore,
in our implementation, we use two threads, each of which is pinned
to one physical core, to handle incoming and outgoing trafﬁc sepa-
rately.
Another potential issue for the SD server is incast TCP collapse.
This is because the SD server may need to receive data from many
TCP sessions from the FS servers. For example, in our case, there
are 12 concurrent TCP connections synchronized at one switch port.
The short-term burstiness from many TCP connections may over-
ﬂow the switch buffer, causing intensive packets losses, TCP re-
transmissions, and even TCP timeouts. This potential incast prob-
lem can have signiﬁcant adverse impacts on the performance of
BigStation. While other researchers have suggested various ways
to solve the TCP incast problem by modifying TCP or adding ECN
tuning on the switch [23], we adopt a simple application-level ﬂow
control mechanism to avoid this problem. In standard TCP, the re-
ceiving side maintains a window that controls how many packets
can be sent to this server without receiving an ACK. The window
size is carefully chosen so that it will avoid buffer overﬂow for the
underlying switch, but at the same time deliver good throughput.
Since BigStation uses a dedicated Ethernet and all trafﬁc patterns
(a)
(b)
Figure 6: BigStation radio front-end built from Sora MIMO
kit. (a) Sora MIMO Kit. (b) BigStation radio front-end, con-
taining three Sora MIMO Kits.
6.
IMPLEMENTATION
6.1 Hardware platform
For our study, we have used two commodity PC models. One is
a desktop PC with an Intel Core i7 3770 CPU and 8 GB memory
on an ASUS P8Z77-M motherboard. The second is a Dell server
with an Intel Xeon E5520 CPU (2.26 GHz, 4 cores) and 16 GB
memory. The desktop PCs are primarily used as front-side (FS)
servers and are connected to the software radio front-ends with mul-
tiple antennas. The radio front-end is based on the newly devel-
oped Sora MIMO Kit, as shown in Figure 6(a). Each Sora MIMO
Kit integrates a Sora radio control board (RCB [20]) and 4 phase-
coherent RF daughter boards, whose maximal operating bandwidth
is 20 MHz each. The RCB is connected to a PC with an external
PCIEx4 cable. We use 3 Sora MIMO kits to support 12 MU-MIMO
antennas (Figure 6(b)). These 3 kits are synchronized by an external
clock source.
The Dell servers are used as channel inversion (CI) or spatial
demultiplexing (SD) servers. We have 15 such servers in total, all
connected to a Pronto 3290 Ethernet switch, which has 48 1 Gbps
ports and 4 10 Gbps ports.
The FS servers also connect to the same switch.
Ideally, all
FS servers should use 10 Gbps connections, since each FS server
would generate sample data at 1.6 Gbps (416 Mbps × 4 anten-
nas). Unfortunately, we do not have enough 10 Gbps ports on the
switch – three out of the four 10G ports are connected to CI and SD
servers (§ 7.2). Therefore, we use four 1 Gbps ports instead. All
our PC servers are running Windows Server 2008 R3.
6.2 Underlying software support
SIMD library. Our signal processing software is implemented us-
ing the signal processing library from Sora SDK [20], which has
been highly optimized for SIMD-capable Intel CPUs. We have ex-
tended the library to support parallel algorithms among multiple
cores (§5.2).
405Table 2: Comparison of different locking mechanisms in a SD
server. There are four computing threads and the processing