        elif self._reader.at_eof():
            raise BadRequestException()
    except (NotFoundException,
            BadRequestException) as e:
        self.error_reply(e.code, body=Response.reason_phrases[e.code])
    except Exception as e:
        self.error_reply(500, body=Response.reason_phrases[500])
    self.close_connection()
```
所有内容被包含在 `try-except` 代码块中，这样在解析请求或响应期间抛出的异常可以被捕获到，然后一个错误响应会发送回客户端。
在 `while` 循环中不断读取请求，直到解析器将 `self.request.finished` 设置为 True ，或者客户端关闭连接所触发的信号使得 `self._reader_at_eof()` 函数返回值为 True 为止。这段代码尝试在每次循环迭代中从 `StreamReader` 中读取数据，并通过调用 `self.process_data(data)` 函数以增量方式生成 `self.request`。每次循环读取数据时，连接超时计数器被重置。
这儿有个错误，你发现了吗？稍后我们会再讨论这个。需要注意的是，这个循环可能会耗尽 CPU 资源，因为如果没有读取到东西 `self._reader.read()` 函数将会返回一个空的字节对象 `b''`。这就意味着循环将会不断运行，却什么也不做。一个可能的解决方法是，用非阻塞的方式等待一小段时间：`await asyncio.sleep(0.1)`。我们暂且不对它做优化。
还记得上一段我提到的那个错误吗？只有从 `StreamReader` 读取数据时，`self._reset_conn_timeout()` 函数才会被调用。这就意味着，**直到第一个字节到达时**，`timeout` 才被初始化。如果有一个客户端建立了与服务器的连接却不发送任何数据，那就永远不会超时。这可能被用来消耗系统资源，从而导致拒绝服务式攻击（DoS）。修复方法就是在 `init` 函数中调用 `self._reset_conn_timeout()` 函数。
当请求接受完成或连接中断时，程序将运行到 `if-else` 代码块。这部分代码会判断解析器收到完整的数据后是否完成了解析。如果是，好，生成一个回复并发送回客户端。如果不是，那么请求信息可能有错误，抛出一个异常！最后，我们调用 `self.close_connection` 执行清理工作。
解析请求的部分在 `self.process_data` 方法中。这个方法非常简短，也易于测试：
```
async def process_data(self, data):
    self._buffer.extend(data)
    self._buffer = self.http_parser.parse_into(
        self.request, self._buffer)
```
每一次调用都将数据累积到 `self._buffer` 中，然后试着用 `self.http_parser` 来解析已经收集的数据。这里需要指出的是，这段代码展示了一种称为[依赖注入（Dependency Injection）](https://en.wikipedia.org/wiki/Dependency_injection)的模式。如果你还记得 `init` 函数的话，应该知道我们传入了一个包含 `http_parser` 对象的 `http_server` 对象。在这个例子里，`http_parser` 对象是 `diy_framework` 包中的一个模块。不过它也可以是任何含有 `parse_into` 函数的类，这个 `parse_into` 函数接受一个 `Request` 对象以及字节数组作为参数。这很有用，原因有二：一是，这意味着这段代码更易扩展。如果有人想通过一个不同的解析器来使用 `HTTPConnection`，没问题，只需将它作为参数传入即可。二是，这使得测试更加容易，因为 `http_parser` 不是硬编码的，所以使用虚假数据或者 [mock](https://docs.python.org/3/library/unittest.mock.html) 对象来替代是很容易的。
下一段有趣的部分就是 `reply` 方法了：
```
async def reply(self):
    request = self.request
    handler = self.router.get_handler(request.path)
    response = await handler.handle(request)
    if not isinstance(response, Response):
        response = Response(code=200, body=response)
    self._writer.write(response.to_bytes())
    await self._writer.drain()
```
这里，一个 `HTTPConnection` 的实例使用了 `HTTPServer` 中的 `router` 对象来得到一个生成响应的对象。一个路由可以是任何一个拥有 `get_handler` 方法的对象，这个方法接收一个字符串作为参数，返回一个可调用的对象或者抛出 `NotFoundException` 异常。而这个可调用的对象被用来处理请求以及生成响应。处理程序由框架的使用者编写，如上文所说的那样，应该返回字符串或者 `Response` 对象。`Response` 对象提供了一个友好的接口，因此这个简单的 if 语句保证了无论处理程序返回什么，代码最终都得到一个统一的 `Response` 对象。
接下来，被赋值给 `self._writer` 的 `StreamWriter` 实例被调用，将字节字符串发送回客户端。函数返回前，程序在 `await self._writer.drain()` 处等待，以确保所有的数据被发送给客户端。只要缓存中还有未发送的数据，`self._writer.close()` 方法就不会执行。
`HTTPConnection` 类还有两个更加有趣的部分：一个用于关闭连接的方法，以及一组用来处理超时机制的方法。首先，关闭一条连接由下面这个小函数完成：
```
def close_connection(self):
    self._cancel_conn_timeout()
    self._writer.close()
```
每当一条连接将被关闭时，这段代码首先取消超时，然后把连接从事件循环中清除。
超时机制由三个相关的函数组成：第一个函数在超时后给客户端发送错误消息并关闭连接；第二个函数用于取消当前的超时；第三个函数调度超时功能。前两个函数比较简单，我将详细解释第三个函数 `_reset_cpmm_timeout()` 。
```
def _conn_timeout_close(self):
    self.error_reply(500, 'timeout')
    self.close_connection()
def _cancel_conn_timeout(self):
    if self._conn_timeout:
        self._conn_timeout.cancel()
def _reset_conn_timeout(self, timeout=TIMEOUT):
    self._cancel_conn_timeout()
    self._conn_timeout = self.loop.call_later(
        timeout, self._conn_timeout_close)
```
每当 `_reset_conn_timeout` 函数被调用时，它会先取消之前所有赋值给 `self._conn_timeout` 的 `asyncio.Handle` 对象。然后，使用 [BaseEventLoop.call\_later](https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.BaseEventLoop.call_later) 函数让 `_conn_timeout_close` 函数在超时数秒（`timeout`）后执行。如果你还记得 `handle_request` 函数的内容，就知道每当接收到数据时，这个函数就会被调用。这就取消了当前的超时并且重新安排 `_conn_timeout_close` 函数在超时数秒（`timeout`）后执行。只要接收到数据，这个循环就会不断地重置超时回调。如果在超时时间内没有接收到数据，最后函数 `_conn_timeout_close` 就会被调用。
### 创建连接
我们需要创建 `HTTPConnection` 对象，并且正确地使用它们。这一任务由 `HTTPServer` 类完成。`HTTPServer` 类是一个简单的容器，可以存储着一些配置信息（解析器，路由和事件循环实例），并使用这些配置来创建 `HTTPConnection` 实例：
```
class HTTPServer(object):
    def init(self, router, http_parser, loop):
        self.router = router
        self.http_parser = http_parser
        self.loop = loop
    async def handle_connection(self, reader, writer):
        connection = HTTPConnection(self, reader, writer)
        asyncio.ensure_future(connection.handle_request(), loop=self.loop)
```
`HTTPServer` 的每一个实例能够监听一个端口。它有一个 `handle_connection` 的异步方法来创建 `HTTPConnection` 的实例，并安排它们在事件循环中运行。这个方法被传递给 [asyncio.start\_server](https://docs.python.org/3/library/asyncio-stream.html#asyncio.start_server) 作为一个回调函数。也就是说，每当一个 TCP 连接初始化时（以 `StreamReader` 和 `StreamWriter` 为参数），它就会被调用。
```
   self._server = HTTPServer(self.router, self.http_parser, self.loop)
   self._connection_handler = asyncio.start_server(
        self._server.handle_connection,
        host=self.host,
        port=self.port,
        reuse_address=True,
        reuse_port=True,
        loop=self.loop)
```
这就是构成整个应用程序工作原理的核心：`asyncio.start_server` 接受 TCP 连接，然后在一个预配置的 `HTTPServer` 对象上调用一个方法。这个方法将处理一条 TCP 连接的所有逻辑：读取、解析、生成响应并发送回客户端、以及关闭连接。它的重点是 IO 逻辑、解析和生成响应。
讲解了核心的 IO 部分，让我们继续。
### 解析请求
这个微型框架的使用者被宠坏了，不愿意和字节打交道。它们想要一个更高层次的抽象 —— 一种更加简单的方法来处理请求。这个微型框架就包含了一个简单的 HTTP 解析器，能够将字节流转化为 Request 对象。
这些 Request 对象是像这样的容器：
```
class Request(object):
    def init(self):
        self.method = None
        self.path = None
        self.query_params = {}
        self.path_params = {}
        self.headers = {}
        self.body = None
        self.body_raw = None
        self.finished = False
```
它包含了所有需要的数据，可以用一种容易理解的方法从客户端接受数据。哦，不包括 cookie ，它对身份认证是非常重要的，我会将它留在第二部分。
每一个 HTTP 请求都包含了一些必需的内容，如请求路径和请求方法。它们也包含了一些可选的内容，如请求体、请求头，或是 URL 参数。随着 REST 的流行，除了 URL 参数，URL 本身会包含一些信息。比如，"/user/1/edit" 包含了用户的 id 。
一个请求的每个部分都必须被识别、解析，并正确地赋值给 Request 对象的对应属性。HTTP/1.1 是一个文本协议，事实上这简化了很多东西。（HTTP/2 是一个二进制协议，这又是另一种乐趣了）
解析器不需要跟踪状态，因此 `http_parser` 模块其实就是一组函数。调用函数需要用到 `Request` 对象，并将它连同一个包含原始请求信息的字节数组传递给 `parse_into` 函数。然后解析器会修改 `Request` 对象以及充当缓存的字节数组。字节数组的信息被逐渐地解析到 request 对象中。
`http_parser` 模块的核心功能就是下面这个 `parse_into` 函数：
```
def parse_into(request, buffer):
    _buffer = buffer[:]
    if not request.method and can_parse_request_line(_buffer):
        (request.method, request.path,
         request.query_params) = parse_request_line(_buffer)
        remove_request_line(_buffer)
    if not request.headers and can_parse_headers(_buffer):
        request.headers = parse_headers(_buffer)
        if not has_body(request.headers):
            request.finished = True