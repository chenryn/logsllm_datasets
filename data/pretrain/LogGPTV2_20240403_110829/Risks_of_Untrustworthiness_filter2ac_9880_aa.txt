title:Risks of Untrustworthiness
author:Peter G. Neumann
Risks of Untrustworthiness
Peter G. Neumann
Principled Systems Group
Computer Science Laboratory
SRI International
Menlo Park CA 94025-3493
PI:EMAIL
Abstract
This paper revisits the risks of untrustworthiness, and
considers some incidents involving computer-based sys-
tems that have failed to live up to what had been ex-
pected of them. The risks relate to security, reliability,
survivability, human safety, and other attributes, and span
a variety of applications and critical infrastructures —
such as electric power, telecommunications, transporta-
tion, ﬁnance, medical care, and elections. The range of
causative factors and the diversity of the resulting risks
are both enormous. Unfortunately, many of the problems
seem to recur far too often. Various lessons therefrom and
potential remedies are discussed.
1 Risks
ﬂawed software, human error, malicious misuse, ques-
tionable election results, and even animal-induced sys-
tem failures. The results of these problems have caused
deaths, physical injury and health problems, mental an-
guish, ﬁnancial losses and errors, fraud, security and pri-
vacy violations, environmental damage, and so on. There
is much to be learned from this litany of cases.
People are always a potential weak link, throughout
the system life cycle. Although technology is sometimes
blamed, people have created that technology. For exam-
ple, requirements are often incorrect, incomplete, mutu-
ally inconsistent, and lacking in foresight. System designs
and detailed architectures are typically ﬂawed. Software
is frequently buggy. Patches intended to ﬁx existing ﬂaws
often create further bugs. System adminstrators are usu-
ally beset with too many opportunities for mistakes. In-
deed, blame can often be spread rather widely.
This contribution to the Classic Papers track is a ret-
rospective consideration of computer-related risks from
the archives of the ACM SIGSOFT Software Engineer-
ing Notes (since 1976) and the online ACM Risks Fo-
rum (since 1985, a.k.a. RISKS and comp.risks), both of
which were created by the author. The cumulative Illus-
trative Risks index to both sources [12] provides a hint
of the enormous range of problems that must be consid-
ered. Discussion of many interesting cases prior to 1995 is
found in [13]; surprisingly, apart from a steadily increas-
ing number of more recent instances of similar cases, the
basic conclusions of that book are still very timely!
Application areas in RISKS include space missions, de-
fense, aviation and other forms of transportation, power,
telecommunications, health care, process control, infor-
mation services,
law enforcement, antiterrorism, elec-
tions, and many others. The causes of computer-related
risks are manifold. The RISKS archives include rampant
cases of power glitches, undetected hardware failures,
2 Trustworthiness
The term trustworthiness implies that something is worthy
of being trusted to satisfy its speciﬁed requirements. The
requirements may specify in detail various system prop-
erties such as security, reliability, human safety, and sur-
vivability in the presence of a wide range of adversities.
Trustworthiness thus implies some sort of assurance mea-
sures, and is typically never perfect.
Trustworthiness needs to be considered pervasively
throughout the system life cycle, through system develop-
ment, use, operation, maintenance, and evolutionary up-
grades.
It cannot be easily retroﬁtted into systems that
were not carefully designed and developed. It is depen-
dent on technology and on many other factors — the most
important of which ultimately tends to be people.
Sections 3 through 6 discuss a few instructive cases of
untrustworthiness, with references in [12, 13].
Proceedings of the 22nd Annual Computer Security Applications Conference (ACSAC'06)0-7695-2716-7/06 $20.00  © 20063 Unreliable Backup
A major source of problems relates to failures of backup
systems, or failures of the interface between the primary
and backup systems, or in some cases the total absence of
backup.
One of the most interesting cases of a problem involv-
ing a backup system arose in NASA’s very ﬁrst attempt
to launch a shuttle, the Columbia. The synchronization
problem in the ﬁrst shuttle was partly a design error and
partly a programming ﬂaw. About 20 minutes before the
scheduled launch on 10 April 1981, the backup computer
failed to be synchronized with the four primary comput-
ers. This failure had actually occurred previously in test-
ing, and was later identiﬁed as a one-in-64 probabilistic
intermittent [5], but was apparently not known to the op-
erations crew. The two-day delay in launch could appar-
ently have been avoided by a retry.
Several major airport disruptions are also worth noting,
as well as other cases that resulted in total system failures,
either because of the lack of a backup system or in spite
of its presence.
Air-trafﬁc control (ATC) backup/recovery failures:
(cid:127) Palmdale (Los Angeles) ATC, July 2006: a pickup
truck hit a utility pole; automatic cutover to backup
power failed an hour later.
(cid:127) Reagan National Airport, 10 April 2000: main power
and backup failed for almost 8 hours; major outage.
(cid:127) Westbury, Long Island ATC, June 1998: software up-
grade failed its test, but reversion to the old software
failed.
(cid:127) Three main New York airports shut down, 1991: a
Number 4 ESS telephone system had a 4-hour out-
age; the standby generator had been misconﬁgured,
and the system ran (without the generator) until the
backup batteries had been drained.
(cid:127) Twenty ATC systems were shut down, 1991: a ﬁber
cable was accidentally cut by a farmer burying his
cow.
(cid:127) El Toro (Los Angeles) ATC, 1989: 104 hardware
failures occurred in a single day, with no backup sys-
tem.
More total system failures and backup (or no backup!):
(cid:127) Swedish central train-ticket sales and reservation
system, 1998: hardware and backup system both
failed for an entire day.
(cid:127) Washington Metro Blue Line, 1997: main system
and backup both failed, causing major delays.
(cid:127) San Francisco Bay Area Rapid Transit, April 2006:
software upgrade attempts failed for three days in a
row, causing long delays. On the third day, backup
was attempted to the previous system – which failed.
(cid:127) Japanese stock exchange, November 2005: the pri-
mary system crashed, and the cutover to the backup
system failed (it was using the same software).
(cid:127) 9 Mile Point nuclear power plant in Oswego, NY,
1991: a power surge shut down the plant when the
“noninterruptible” power supply failed.
(cid:127) New York Public library, 1987: lost its computerized
references, for which there were no backups.
(cid:127) Dutch criminal management system, 1987: a new
system failed, freed some criminals, caused arrest of
others who were innocent; the old system had been
eliminated, and no backup was possible.
Although these problems all relate to system survivabil-
ity, security issues also arise in backup systems — in-
cluding data integrity (particularly for forensic purposes
and election system disputes), data retention, long-term
compatibility of backup data, noncompromisibility of per-
sonal data, and privacy. Furthermore, if a system and its
backup and recovery facilities are not reliable, they may
also not be secure.
In addition, backup systems must be considered in the
context of the overall systems in which they function. It
is not very helpful to claim that a backup system works
perfectly in isolation if it is never properly invoked and
never tested in conditions of actual need. Various cases
are noted of backup systems passing periodic tests and
nevertheless failing in operation. (For example, a diesel
generator stopped working because the fuel pump keep-
ing its tank full depended on utility power — which had
always been available during testing [2]!) Thus, backup
systems must be demonstrably trustworthy with respect to
their ability to satisfy criteria for security, integrity, relia-
bility, and survivability (among other requirements), and
must be tested under realistic conditions.
Proceedings of the 22nd Annual Computer Security Applications Conference (ACSAC'06)0-7695-2716-7/06 $20.00  © 20064 Unrobust Networks
Various examples of widespread propagation effects ex-
hibit some of the complexities inherent in distributed and
networked systems. Of particular interest to computer
networks are two cases in which global failure modes
resulted from local faults, namely the 1980 ARPANET
collapse [20] and the 1990 AT&T long-distance collapse
(e.g., see [13]). Also of interest are various massive U.S.
power outages that resulted from an initial power blip
propagating widely. Among major outages, the North-
east power blackout in November 1965 was followed by
outages affecting 10 western states in October 1984, the
Western U.S. in July 1996, Western U.S., Canada, and
Baja Mexico in August 1996, and the Northeast in August
2003. These cases are revisited in [15], with references
in [12]. Propagating malware (e.g., viruses and worms)
such as the 1988 Internet Worm is also worth noting. Al-
though malware may be a direct threat to systems con-
nected to networks, it also may threaten the throughput
and reliability of the networks themselves.
In addition,
natural causes may also cause widespread disruption, as
in the case of Hurricane Katrina.
The above cases are illustrative of the unfortunate real-
ity that the same kinds of failures continue to recur, de-
spite efforts to avoid them. This is particularly true of
the most frequent types of security ﬂaws and propagating
outages of computer networks and power grids. The need
for real proactive measures is apparently subordinated by
other demands.
One of the main lessons from these outages is that secu-
rity, reliability, and survivability are closely interrelated,
especially when there are people in the loop. For exam-
ple, both the ARPANET outage and the AT&T long-lines
extreme slowdown could have been easily triggered re-
motely by subversive human activity — if the fault mode
had been known to the perpetrator — rather than acciden-
tally.
5 Unsafe Systems
To security-minded people, many past incidents that re-
sulted in accidental losses of life, injuries, and serious im-
pairment of human well-being further illustrate the difﬁ-
culties in providing high-assurance trustworthy systems.
Many of the lessons that should be learned for human
safety are rather similar to those in developing secure sys-
tems, and suggest that many commonalities exist between
safe systems and secure systems.
(cid:127) Requirements errors abound. Many critical systems
are developed with no speciﬁed requirements, or per-
haps incomplete ones.
(cid:127) Design ﬂaws abound.
In many cases, the system
architectures and detailed designs are inherently in-
capable of satisfying the intended requirements, al-
though this is often not identiﬁed until much later in
the development cycle.
(cid:127) Programming bugs abound. (I once posed an exam
question that could be satisﬁed with a ﬁve-line pro-
gram. One student managed to make three program-
ming errors in ﬁve lines, including an off-by-one
loop count and a missing bounds check.)
A few pithy examples of safety-related risks are sum-
marized here, particularly as a reminder to younger people
who were not around at the time. References are found
in [12].
(cid:127) Aviation, defense, and space. The RISKS archives
include many cases of deaths involving commercial
and military aviation. Here are just a few exam-
ples. The Iran Air Airbus mistakenly shot down by
USS Vincennes’ Aegis missile system was attributed
to human error and a poor human interface. The
Patriot system defending against Iraqi scud missiles
had a serious hardware/software clock drift problem
that prevented the system from tracking targets af-
ter a few days. The Handley Page Victor tailplane
broke off in its ﬁrst high-speed ﬂight, killing the
crew. Each of three independent test methods had
its own ﬂaw that made the analysis appear satisfac-
tory, which consequently prevented identiﬁcation of
a fundamental instability. A Lauda Air aircraft broke
up over Thailand, after its thrust-reverser acciden-
tally deployed in mid-air. A British Midland plane
crashed after an engine caught ﬁre and the pilot erro-
neously shut off the remaining good engine because
the instrumentation had been crosswired. Three early
A320 crashes were blamed variously on pilot er-
ror, safety controls being off, software problems in
the autopilot, inaccurate altimeter readings, sudden
power loss, barometric pressure reverting to the pre-
vious ﬂight, and tampering with a ﬂight recorder; in
one case, the pilots were convicted of libeling the
integrity of the technology! An Air New Zealand
ﬂight crashed into Mt. Erebus in Antarctica; comput-
erized course data was known to be in error, but the
pilots had not been informed. (Incidentally, the shut-
tle Discovery’s tail speed-brake gears were installed
backwards in 1984, but this was not discovered until
2004, 30 missions later!)
Proceedings of the 22nd Annual Computer Security Applications Conference (ACSAC'06)0-7695-2716-7/06 $20.00  © 2006(cid:127) Rail travel. The RISKS archives include dozens of
train wrecks attributable to various hardware, soft-
ware, and operational problems, some despite sig-
naling systems and safety devices, some as a result
of manual operation when automated systems failed.
(cid:127) Ferry crashes. The Puget Sound ferry experienced
numerous computer failures that resulted in twelve
crashes, and the removal of the automated “sail-by-
wire” system.
(cid:127) Nuclear power. The Chernobyl accident in 1986
was the result of a misconceived experiment on
emergency-shutdown recovery procedures.
The
long-term death toll among cleanup crew and neigh-
bors continues to mount, 20 years later. The earlier
Three Mile Island accident in 1979 was attributed
to various equipment failures, operational misjudg-
ment, and a software ﬂaw (reported in 1982 by
Daniel Ford [4]): experimentally installed thermo-
couple sensors were able to read abnormally high
temperatures, but the software suppressed readings
that were outside of normal range — printing out
“???????” for temperatures above 700 degrees, and