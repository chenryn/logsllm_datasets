# Risks of Untrustworthiness
**Author:** Peter G. Neumann  
**Affiliation:** Principled Systems Group, Computer Science Laboratory, SRI International, Menlo Park, CA 94025-3493  
**Email:** [PI:EMAIL]

## Abstract
This paper examines the risks associated with untrustworthy computer-based systems, highlighting incidents where these systems have failed to meet expectations. The risks encompass security, reliability, survivability, human safety, and other critical attributes, spanning various applications and infrastructures such as electric power, telecommunications, transportation, finance, medical care, and elections. The causes and resulting risks are diverse and often recurring. This paper discusses lessons learned and potential remedies.

## 1. Introduction to Risks
The risks of untrustworthiness in computer-based systems include flawed software, human error, malicious misuse, questionable election results, and even animal-induced system failures. These issues have led to deaths, physical injuries, health problems, mental anguish, financial losses, fraud, security and privacy violations, and environmental damage. People are often a weak link throughout the system lifecycle, from incorrect or incomplete requirements to flawed designs and buggy software. System administrators also face numerous opportunities for mistakes.

This contribution is a retrospective look at computer-related risks, drawing from the ACM SIGSOFT Software Engineering Notes (since 1976) and the online ACM Risks Forum (since 1985), both created by the author. The cumulative Illustrative Risks index [12] provides an overview of the wide range of problems. A detailed discussion of cases prior to 1995 can be found in [13], and the conclusions remain relevant today.

## 2. Trustworthiness
Trustworthiness implies that a system is reliable and meets its specified requirements, including security, reliability, human safety, and survivability. Ensuring trustworthiness requires pervasive consideration throughout the system lifecycle, from development to maintenance and upgrades. It is challenging to retrofit into systems not initially designed with these principles in mind. Ultimately, people play a crucial role in achieving and maintaining trustworthiness.

Sections 3 through 6 discuss specific cases of untrustworthiness, with references in [12, 13].

## 3. Unreliable Backup Systems
A significant source of problems is the failure of backup systems or the interface between primary and backup systems. One notable example is NASA's first attempt to launch the Columbia shuttle. A synchronization issue between the backup and primary computers caused a two-day delay. This problem had occurred previously during testing but was not known to the operations crew.

Other major disruptions include:
- **Palmdale (Los Angeles) ATC, July 2006:** A pickup truck hit a utility pole, and the automatic cutover to backup power failed.
- **Reagan National Airport, April 2000:** Main and backup power failed for almost 8 hours.
- **Westbury, Long Island ATC, June 1998:** A software upgrade failed, and reversion to the old software also failed.
- **Three main New York airports, 1991:** A telephone system outage and misconfigured standby generator caused a 4-hour shutdown.
- **Twenty ATC systems, 1991:** A fiber cable cut by a farmer resulted in a widespread failure.
- **El Toro (Los Angeles) ATC, 1989:** 104 hardware failures occurred in a single day with no backup system.

These incidents highlight the importance of robust and tested backup systems. Security issues, such as data integrity and noncompromisibility, must also be considered. Backup systems must be demonstrably trustworthy and tested under realistic conditions.

## 4. Unrobust Networks
Distributed and networked systems are susceptible to widespread propagation effects. Notable examples include:
- **1980 ARPANET collapse:** A local fault led to a global failure.
- **1990 AT&T long-distance collapse:** Similar to the ARPANET case, a local fault caused a widespread outage.
- **U.S. power outages:** Initial power blips propagated widely, causing major outages in 1965, 1984, 1996, and 2003.
- **Propagating malware:** The 1988 Internet Worm and similar threats can disrupt network throughput and reliability.

Natural disasters, such as Hurricane Katrina, also cause widespread disruption. These cases illustrate the recurring nature of such failures despite efforts to prevent them. Security, reliability, and survivability are closely interrelated, especially when human factors are involved.

## 5. Unsafe Systems
Many incidents involving accidental loss of life, injuries, and serious impairment of well-being underscore the challenges in providing high-assurance trustworthy systems. Common issues include:
- **Requirements errors:** Many critical systems lack or have incomplete specifications.
- **Design flaws:** Inherent architectural and design issues prevent systems from meeting intended requirements.
- **Programming bugs:** Errors in code are common and can have severe consequences.

Examples of safety-related risks include:
- **Aviation, defense, and space:** Incidents such as the Iran Air Airbus shot down by USS Vincennes, the Patriot system's clock drift problem, and the Lauda Air aircraft breakup.
- **Rail travel:** Train wrecks due to hardware, software, and operational issues.
- **Ferry crashes:** The Puget Sound ferry experienced multiple crashes due to computer failures.
- **Nuclear power:** The Chernobyl and Three Mile Island accidents, attributed to equipment failures, operational misjudgment, and software flaws.

These examples highlight the need for rigorous and comprehensive approaches to ensure the safety and security of critical systems.

---

**Proceedings of the 22nd Annual Computer Security Applications Conference (ACSAC'06)**  
**ISBN: 0-7695-2716-7/06 $20.00**  
**Copyright Â© 2006**