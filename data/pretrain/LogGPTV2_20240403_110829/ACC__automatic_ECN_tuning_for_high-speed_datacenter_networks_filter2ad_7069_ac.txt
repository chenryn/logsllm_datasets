Lj(θj) = E[(yj − Q(S, a; θj)]2
(6)
where yj = rj + γmaxa′Q(Sj+1, a, θ′), θ′ denotes the parameter of
a separate target network. The target network is obtained by saving
the evaluation network every n step (periodically updated target).
The intuition is that if we know the optimal action-value of the next
state Sj+1 , then the best strategy is to push the current action-value
function Q(Sj , a, θj) closer to the sum of the immediate reward and
the discounted action-value of the next state (Q(Sj+1, a, θ′).
At time step j, we perform an action aj, observe reward rj and
next state Sj+1. The tuple (Sj , aj , rj , Sj + 1) is called an experience.
This experience is stored to a buffer D for experience replay. The
network is then trained by sampling from D uniformly.
Together, periodically updated target and experience replay have
been proved to greatly improve and stabilize the training procedure
of Q-learning [38]. However, DQN is known to overestimate action
values under certain conditions. Deep Double D-network (DDQN)
[52] is able to reduce overestimation by decomposing the max
operation in the target into action selection and action evaluation.
Input: Local Replay Memory D, Batch size N
// for each interval ∆t
Agent pulls a queue state QSt , and obtains the state
1: for t=1 to T do
2:
3:
St = {QSt−k +1, QSt−k +2, ..., QSt}
a
a
Q(St , a, θi) and execute the
yj = rj + γ × Q(Sj+1, arg max
Select the action at = arg max
action at
At the following time step t + 1, observe the queue state
QSt +1, monitor the throughput and queue length to obtain
the reward rt
Store the transition {St ,at ,rt ,St +1} in D
Sample random minibatch of transitions {Sj,aj,rj,Sj+1} from
D and compute the loss:
{yj − Q(Sj , aj; θ)}2
{yj − Q(Sj , aj; θ)}∇θ Q(Sj , aj; θ)
(4)
Compute the gradient and update the evaluation network
parameters θ with the gradient:
∇θ L(θ) = 1
(5)
Replace the target network θ′ with the evaluation network
θ every n iterations
Q(Sj+1, a, θ); θ
(3)
L(θ) = 1
N
′)
4:
5:
6:
7:
8:
9:
N
10: end for
So we use DDQN as agent, where the target yj in equation (6) is
replaced by equation (3).
Furthermore, in order to achieve stability of DDQN in multi-
agent setting, we follow in principle the design of asynchronous
deep reinforcement learning [37]. More specifically, we maintain
a global replay memory beside local replay memory at each agent
(switch). The experience tuples of local memory will be periodically
sampled and added to the global memory, while some experience
tuples from the global memory will also be periodically sampled
to the local memory. The use of a global replay memory is a cen-
tralized way to store the history of agents’ experiences with larger
capacity. The periodic exchange interval can be several seconds.
By doing so, agents at different switches can exchange experiences
and explore different parts of the whole network environment. This
strategy has been proved to make the learned model more stable
and generalizable [4]. The training algorithm of each agent is given
in Algorithm 1.
4 IMPLEMENTATION
In this section, we describe the implementation of ACC on com-
modity switch. To reduce CPU overhead we introduce two steps of
optimization: (i) combining offline and online training to accelerate
the convergence of DRL model; and (ii) applying parallel message
processing to speed up state monitoring.
4.1 Hardware Architecture of ACC
Figure 5 shows ACC implementation on a commodity switch. The
switch’s SDK (Software Development Kit) provides basic telemetry
389
ACC: Automatic ECN Tuning for High-Speed Datacenter Networks
SIGCOMM ’21, August 23–28, 2021, Virtual Event, USA
Figure 5: Hardware architecture of ACC in switch
Figure 6: ACC dynamically adjusts ECN threshold to opti-
mize queue size and link utilization
interface to read the state information from the switch chip and
configuration interface to set up ECN parameters. We implemented
ACC as a module in commodity switches (or a container in the
programmable switches), which takes ∼2400 lines of C code. The
offline pre-trained DRL’s NN model is loaded into the DRL agent
module. The collector subscribes raw data from switch chip for
features analysis, including the total bytes sent, number of ECN-
marked packets and egress queue depth.
In detail, at each time interval ∆t, the collector achieves the sub-
scribed data from forwarding chips’s registers. Then, data processor
normalizes the row data as reward, extracts features
(c))
QSt = (qlen, txRate, txRate
(m)
, ECN
Then, QSt is stored in memory as current states. Data processor
obtains the history data (QSt−2, QSt−1) from memory and sends the
state information St =(QSt−2, QSt−1, QSt ) to DRL agent. DRL agent
uses St as input to make inference and updates the DRL model.
The new action at is generated and put to configurator. Finally,
configurator maps the action into ECN template and sets new ECN
marking threshold to the forwarding chip. Then it obtains the
reward rt and observes the next state St +1. For each time interval,
the new transition {St , at , rt , St +1} is stored into replay memory
for online training.
4.2 Optimization for Data Processing
ACC applies multithreading for parallel monitoring and data pro-
cessing, and realizes thread granularity optimization for NN infer-
ence. One thread is responsible for monitoring a set of the outgoing
port’s queues in a round-robin manner. Given the large number
of ports, especially when virtual queues are used, it leads to long
monitoring period which can be longer than ∆t and causes high
CPU overhead. To optimize the process, we classify the queues into
two categories: the busy queue and idle queue. If the queue length
is less than Kmin or the corresponding reward function does not
change for continuous three time slots, we set it as "idle" queue and
we can stop the inference task for the idle queue. Once the queue
length of a idle queue is larger than Kmin, this queue is identified
as "busy" queue and we starts the inference on this queue. By so
doing, we have observed ∼10% decrease of CPU consumption.
4.3 Optimization for Training
In order to decrease the risk trial and error of online learning, we
apply both offline training and online training. To guarantee the
generalization of the offline training, we adopted a variety of typical
390
traffic patterns generated by Perftest tools [40], including incast
traffic and realistic traffic trace. The incast traffic is generated by
randomly selecting p ∈ [2, 64] senders forwarding traffic to one
receiver. Each server randomly generates q ∈ [1, 1000] flows with
message sizes ranging from 10KB to 10MB. The traffic load varies
from 10% to 90%. The realistic traffic traces are collected from
prevailing RDMA applications, including distributed storage, high
performance computing with LinkPack[16], Quantum Espresso[48],
and distributed training with Tensorflow[34], Horovod[24]. After
a model is trained offline by these training samples, we install
the same offline training model for network switches. The switch
will train its own local model online by using the realistic traffic
to improve the model generalization. During the online training,
the probability of choosing the exploration action is exponential
decayed and the actions resulting large reward will be prioritised.
5 EVALUATION
We evaluate the performance of ACC based on the controlled exper-
iments. We apply testbed experiments and large-scale simulation to
validate the effectiveness of ACC in comparison with the alternative
solutions.
5.1 Testbed Setup
Network Topology: The testbed is a two-layer Clos network,
which mimics a small scale PoD (point-of-delivery) in the pro-
duction datacenter. The testbed consists of two spine switches,
four leaf switches and 24 servers. Each server is configured with
two Mellanox ConnectX-5 cards (25Gbps) and connects to two leaf
switches for high availability. Each leaf switch connects to spine
switches via four 100Gbps links. The average RTT is under 2.32µs
for inner-rack, and 6.13µs for inter-rack. PFC [3] is enabled at NIC
and switches. Based on the NIC vendor’s default setting of PFC,
Xof f = α(1 + α) × Bu f f erf r ee, where α = 1/8, i.e., PFC is trig-
gered when an ingress queue consumes more than 11.11% of the
free buffer. ACC is deployed on both leaf and spine switches.
Workload: For micro-benchmark, RDMA incast traffic (N-to-1) is
generated by using Mellanox PerfTest tool [40], which is widely
used for performance evaluation in RDMA network. We change the
number of QPs and senders to generate dynamic traffic. For macro-
benchmark testbed, we use FIO benchmark tools [15] to generate
traffic of distributed storage applications and use Tensorflow [34]
SIGCOMM ’21, August 23–28, 2021, Virtual Event, USA
Yan et al.
(a) 20% load
(b) 60% load
(c) Queue length
(d) Throughput of TOR switch
Figure 7: FCT, average queue length and switch port utilization of ACC and SECN during Incast
to generate traffic of distributed training for deep learning models.
Furthermore, TCP traffic is generated by Iperf [47].
Benchmark: We compare ACC with two static ECN settings
(SECN1 and SECN2). SECN1 refers to the DCQCN paper [65], where
Kmin = 5KB and Kmax = 200KB in our testbed. SECN2 refers
to the setting of a major cloud provider [29]. The ECN marking
threshold is proportional to the link bandwidth (BW ). Kmin =
100KB× Bw
25Gbps . For example, Kmin is
100KB and Kmax is 400KB when the link bandwidth BW is 25Gbps.
It is notable that the cloud provider version is usually conservative
to avoid triggering PFC pauses during Incast event.
25Gbps and Kmax = 400KB× Bw
5.2 Micro-benchmark
We seek to understand: (1) can ACC adapt to the variable traffic? (2)
how is the end-to-end performance of ACC? (3) Does ACC maintain
fairness between RDMA and TCP traffic?
Heterogeneous Traffic. We randomly change the number of flows
and the number of Incast senders every 100 seconds. We train ACC
for different Incast and flows traffic for 4 hours and use the trained
ACC model to adjust ECN thresholds online. The queue length and
utilization of links are plotted against time in Figure 6. For the fixed
ECN parameters (SECN1, SECN2), when the traffic characteristics
match the ECN setting, they have good throughput and queue size
performance as ACC (see the time interval of 0∼100s). But when
the missmatch occurs, the throughput drops or the queue builds up
quickly (see the time intervals of 100s∼200s and 200s∼300s). This
result shows that the fixed ECN parameter setting does not adapt to
dynamic traffic. ACC learns and adapts across time varying traffic
characteristics to reduce an order of magnitude of queue length
and achieves 26.1% improvement of the average throughput.
End-to-end Performance. To evaluate the end-to-end perfor-
mance, we keep on randomly sending messages of size {1KB, 10KB,
100KB, 1MB, 10MB} from two senders to one receiver at 20% and
60% loads. We evaluate the average FCT and the 99th/99.9th per-
centile FCT to represent the average end-to-end latency and the tail
latency of flows, respectively. For ease of comparison, the results
are normalized based on the FCT achieved by ACC. As shown in
Figure 7 (a) and 7 (b), ACC achieves lower FCT compared to static
ECN at different loads especially for small flows. For example, ACC
cuts the tail latency of SECN by 1.5× ∼ 3× for the mice flows with
messages shorter than 10KB. The round trip latency of such mes-
sages is close to the RTT in the testbed. The gap increases with the
increase of traffic loads. For example, at 60% load, ACC cuts the tail
latency of SECN by 2× ∼ 7× for the mice flows.
We then evaluate the link utilization and queue length of one
leaf switch connecting to the receiver (Figure 7(c) and 7(d)), which
provides more insight into the achieved performance gain. At 60%
load, the average queue size of ACC is 5.6KB and standard-deviation
is 13.3KB, whereas the average queue size and standard-deviation
of SECN1 are 20.4KB and 49.6KB, and for SECN2 they are 37.5KB
and 109.3KB. ACC achieves much lower queue size steadily, thus
achieves much lower tail FCTs for short flows. These experiments
confirm that the statically-configured ECN is not adaptive to the
mixed traffic flows at different loads, which results in high queue
length. While ACC can learn to approach the optimal ECN threshold
and adapt to different traffic patterns.
Fairness between RDMA and TCP Traffic. Coexistence of TCP
and RDMA traffic is common in datacenter. Generally, RDMA and
TCP traffic are isolated from each other by using different traffic
classes [1], but they usually use the shared physical buffer of the
switch. However, when operating the network, we notice that TCP
is not RDMA friendly in some scenarios, which is not well addressed
in the literature. TCP and RDMA uses different transport protocols.
Once the congestion occurs, it takes longer time for TCP traffic
to decrease rate than RDMA traffic because of the long TCP RTT
feedback interval (TCP 25.4us v.s. RDMA 1.7us [65]). In this case,
TCP traffic will occupy more buffer and bandwidth than expected.
Specifically, in some datacenters, the drop-tail mechanism is used
for TCP, it becomes more greedy and may occupy the whole buffer
of the port. A straightforward solution to this problem is applying a
high ECN marking threshold, which leads to low ECN marking rate
and maintains a balance between RDMA traffic rate and TCP traffic
rate. However, by doing so the queue will build up which increases
the latency of RDMA traffic. We use an experiment to demonstrate
this problem. To evaluate the bandwidth sharing between RDMA
and TCP, we set up 8 servers with 100Gbps RDMA NIC connected
by one switch. At the switch, the bandwidth allocation, 70% for
RDMA traffic, 30% for TCP traffic, is configured through the deficit
weighted round robin [1]. We select 2 or 7 servers sending messages
to one receiver. The concurrent RDMA queue pair connections
randomly change from 1 to 32 at each sender. We use ECN setting
introduced in DCQCN paper [65] as the static setting ECN (SECN).
As shown in Figure 8, with static ECN TCP occupies 10% to 20%
more bandwidth than allocated bandwidth, i.e. the actual through-
put of RDMA is 10% to 20% lower than expected. In contrast, ACC
391
ACC: Automatic ECN Tuning for High-Speed Datacenter Networks
SIGCOMM ’21, August 23–28, 2021, Virtual Event, USA
(a) 2:1 Incast
(b) 7:1 Incast
Figure 8: The average throughput ratio of RDMA and TCP
traffic over 100Gbps network
can significantly improve the fairness for weighted fair sharing
between TCP and RDMA traffic. This is because ACC can itera-