randomly accessed and partial block updates in the PMB area, whereas the
BMB area stores the data accessed sequentially in addition to the mapping
tables. WAFTL also makes use of part of the ﬂash memory as a buffer so as to
temporarily store sequentially written data before copying them into the BMB
space. In addition, WAFTL maintains only part of the page-based mapping
table in RAM.
7.3.3.2. CACH-FTL or Cache Aware Conﬁgurable Hybrid FTL
CACH-FTL [BOU 13a] is another example of FTL that considers ﬂash
memory as a partitioned area, one region using a block-level mapping scheme
and the other region, of smaller size, utilizing a page-level mapping scheme
(same principle as the BMB and PMB in WAFTL). CACH-FTL is based on
the principle that, on the one hand, all SSDs are equipped with a cache in
DRAM making it possible to speed up the access to the ﬂash memory and, on
the other hand, that most ﬂash memory caches achieve ﬂushing (eviction)
involving groups of pages in order to minimize the merge operation between
valid data in cache and valid data in ﬂash memory for a given block (see
further section 7.6). From this point, CACH-FTL is guided by the number of
pages originating from the cache from each ﬂush operation: if this number is
below a certain conﬁgurable threshold, the pages are sent to the mapping area
on a page basis. However, if this number is greater than this threshold, the
pages are sent to the block-wise mapping area. In addition, in CACH-FTL a
garbage collection mechanism allows data to be moved from the page area to
the block area in order to recycle the space. Another more adaptive version of
CACH-FTL has been proposed, named MaCACH [BOU 15]. The latter
makes it possible to reduce the cost of garbage collection by dynamically
modifying the value of the page ﬂushing threshold according to a PID
(Proportional Integral Derivative) feedback control system. This mechanism
in turn enables that the limit value be adjusted depending on the ﬁlling rate of
the page-based mapping region.
7.4. Wear leveling
As described above, each ﬂash memory cell supports only a limited
number of write/erase cycles. Beyond this limit, the cell becomes unusable.
Flash Translation Layer
141
The purpose of wear-leveling techniques is to try to keep the entire ﬂash
memory volume usable as long as possible. In order to achieve this, the FTL
must balance the wear over the entire set of ﬂash memory blocks.
In this chapter, we will use the same differentiation as [KWO 11] between
wear leveling and garbage collection. Wear leveling is relative to block
management, i.e. the choice of a block to use within free blocks and the
the page management during the
garbage collector
recycling, as we will see later. As discussed in Chapter 2,
these two
techniques are closely related and can be implemented in a single system.
is responsible for
Wear-leveling algorithms are based on the number of erase operations
performed on a given block. If this number is greater than the number of
average erase operations per block, the block is said to be hot otherwise it is
said to be cold. Wear-leveling algorithms aim to keep the gap between hot and
cold blocks as small as possible. This can be achieved by moving data from
hot blocks towards cold blocks and vice-versa. Unfortunately, this operation
is costly and therefore cannot be run frequently. As a matter of fact, there is a
trade-off between wear leveling and performance.
We can group wear-leveling algorithms into two categories, those based on
the number of erase operations and those based on the number of writes.
7.4.1. Wear leveling based on the number of erase operations
In these mechanisms, the number of erase operations is maintained for each
block and is stored in the mapping table or in the out-of-band area of the ﬂash
memory. Obviously, it is preferable to have these metadata stored in RAM for
performance reasons. The wear-leveling mechanism is launched as soon as the
difference related to the number of erase operations becomes signiﬁcant.
One of the techniques based on the number of erase operations, called
dual pool, is described in [ASS 95]. This technique refers to two additional
tables that are used in addition to the conventional block-based mapping
table: a table of hot blocks and a cold block table. The hot blocks table
contains the list of references of blocks that have been highly solicited and
sorted by descending order in terms of number of erase operations undergone,
while the cold blocks table contains the references of less erased blocks. With
the dual pool algorithm, whenever the system requires a free block, a
142
Flash Memory Integration
reference is taken from the list of cold blocks. Periodically,
the system
recalculates the average number of erase operations per block and rebalances
both reference tables.
Some wear-leveling mechanisms operate only on free blocks while others
also operate on blocks containing data [ASS 95, CHA 05]. For instance, if a
block contains seldom modiﬁed data (cold or also called static data), these data
are copied into blocks that have undergone several erase operations.
7.4.2. Wear leveling based on the number of write cycles
These algorithms maintain the number of writes that are applied to a block
or to a set of blocks. In [ACH 99],
in addition to the number of erase
operations, a counter of the number of writes is kept in RAM and updated at
each write cycle. The proposed algorithm consists of storing the most
frequently modiﬁed data in the less erased blocks, and conversely the less
frequently modiﬁed data are stored in the most frequently erased blocks. The
technique discussed in [CHA 07] is very similar to that previously seen
[ASS 95] but makes use of the number of writes per block. Two tables are
maintained according to the number of writes carried out in the blocks. The
algorithm interchanges data between regions that are frequently utilized and
those that are less frequently utilized.
In order to decrease the size of the tables used for wear leveling (and
therefore their memory footprint), some algorithms consider groups of blocks
rather than individual blocks [KWO 11].
7.5. Garbage collection algorithms
The garbage collector mechanism (or Garbage Collection) allows the FTL
to recycle invalid space into free space. According to [CHI 99a], a garbage
collector must answer several questions:
– When should the garbage collector algorithm be launched?
– What blocks should the garbage collector be concerned with and how
many should be used?
– How should valid data in victim blocks be rewritten?
Flash Translation Layer
143
In order to complete the operation, we must answer the last question related
to the block that must be chosen for rewriting the valid data; it is part of the
job of the wear-leveling mechanism. Garbage collection can be coupled to a
wear-leveling mechanism within the same service as in the JFFS2 ﬁle system
[WOO 01].
A garbage collection mechanism must minimize the cost of recycling
while maximizing the recycled space, without having a signiﬁcant impact on
application performance. The cost of recycling includes the number of erase
operations carried out in addition to displacing the valid pages.
Garbage collectors are usually started automatically when the number of
it can also be
free blocks falls below a predeﬁned threshold. However,
launched during I/O timeouts, which allows space to be recycled without
impacting response times. One of the most important metrics taken into
account during the choice of the block to be recycled (also called the victim
block) is the rate of invalid pages in the block (or group of blocks). To this
end, the garbage collector should consider the state of each page, which is
very expensive in terms of resources. Some garbage collectors keep the page
state in a RAM while others make use of the out-of-band (OOB) area in ﬂash
memory.
Intuitively, we could think that
in addition to the time elapsed since the most
the victim block must be the one
containing the greatest number of invalid pages. Actually, this is true if the
ﬂash memory is accessed in a uniform way; however, the number of invalid
pages is always taken into account but this is not the only criterion. In
the percentage of invalid pages denoted 1 − u is taken into
[KAW 95],
recent
consideration,
modiﬁcation (denoted age in the equation). The proposed garbage collector
relies on the following calculation of the score: age ∗ (1 − u)/2 ∗ u. The
block having the most signiﬁcant score is chosen as victim. In fact, this score
does not only consider the rate of invalid pages in the block but also the age.
In other words, if the block has been very recently modiﬁed, the system will
probably not choose it as victim block, but it will give time so as to
potentially be updated more often. As a result, more pages will probably be
invalidated, or even all will, and in this case the block will be able to be
directly erased. The coefﬁcient 2 ∗ u relates to the cost of recycling: a u for
reads and another for valid pages writes.
144
Flash Memory Integration
In another garbage collection policy, called Cost Age Times or CAT
[CHI 99a], hot blocks (frequently modiﬁed blocks) are allocated more time in
order to have more chances of increasing their number of invalid pages. The
equation, upon which is based this garbage collection for the computation of
the score, is the following: Cleaningcost ∗ (1/age) ∗ N umbercleaning, where
Cleaningcost is equal to u/(1 − u). Here, u is the percentage of valid data.
The variable N umbercleaning is the number of erase operations generated.
CAT recycles the blocks with the lowest score.
The temperature (or access frequency) is also taken into account by garbage
collection algorithms. In fact, it is relevant to separate the hot data from cold
data. If hot data are isolated in a block and all of these data are updated, the
recycling operation with this block is limited to an erase operation. Whereas if
this block contains hot data as well as cold data, during recycling, cold data will
have to be copied into another block thereby increasing the cost of recycling.
Several contributions are based on this separation between hot and cold data
[CHI 08, CHA 02, SYU 05, HIS 08]. For example, in DAC (Dynamic dAta
Clustering) [CHI 99b], the ﬂash memory is partitioned into several sections
depending on the frequency of data updates.
7.6. Cache mechanisms for ﬂash memory
In order to optimize the performance of write operations, several cache
mechanisms1, placed at the level of the ﬂash memory controller, have been
proposed. These caches essentially make two optimizations possible:
1) Certain write operations are absorbed at the cache level, which avoids
their being copied onto ﬂash memory. This helps reduce the impact of the I/O
workload on the lifetime in addition to the derived performance gain;
2) Write operations are buffered so that they are more easily reordered
before sending them to the ﬂash memory. This allows the overall write cost
to be decreased.
1 It should be noted that this is not related to a cache making use of processor SRAM technology
but to buffer memory between the ﬂash memory and the host. It is usually DRAM technology
that is being utilized.
Flash Translation Layer
145
In this section, we give a few examples of state-of-the-art cache
mechanisms for ﬂash memory.
CFLRU (Clean First LRU) [PAR 06a]: this algorithm takes the asymmetry
of read and write operations into account in the cache page replacement
policy. CFLRU makes use of an LRU list divided into two regions: (1) the
working region containing recently used pages, and (2) the clean ﬁrst region
containing candidate pages to be evicted. CFLRU ﬂushes out the ﬁrst page
that does not generate any write operation into the ﬂash memory in the clean
ﬁrst region. If this proves impossible, it chooses the ﬁrst page according to the
LRU algorithm. This algorithm adds a buffer to pages that were read and
written and every time a page is accessed, it is relocated to the beginning of
the LRU queue.
FAB (Flash Aware Buffer) [JO 06]: this algorithm takes into account page
groups belonging to the same block, and not just like pages CFLRU. FAB
utilizes an LRU algorithm. Every time a page of a group is accessed, the whole
group is placed at the beginning of the list. During a ﬂush, FAB chooses the
group containing the most pages. In the event that several groups of the same
size do exist, FAB makes a choice according to the LRU order. The choice
of the group that contains the largest number of pages makes it possible to
minimize the reading of valid pages from the ﬂash memory and thus to reduce
its cost.
BPLRU (Block Padding Least Recently Used) [KIM 08]: BPLRU is a cache
that makes use of the following three techniques: (1) implementation of a block
LRU list for groups of pages belonging to the same block, (2) a page-padding
technique in which the valid pages are read from ﬂash memory into the cache
before a block is ﬂushed out and (3) LRU compensation: this last operation
consists of moving the blocks written sequentially directly to the end of the
LRU list. It is assumed here that these blocks written sequentially are only
very rarely reused.
C-Lash (Cache for ﬂash) [BOU 11b]: the basic idea of this cache algorithm
was to propose a cache mechanism that replaces wear leveling and garbage
collection. This mechanism has been used later with FTL as CACH-FTL and
MaCACH. The idea behind C-Lash is to use two separate buffers, the ﬁrst
enabling written pages to be stored, and the second which maintains an LRU
list of groups of pages belonging to the same block. The page buffer has the
ability to preserve recently accessed pages. Once this buffer is full, C-Lash
146
Flash Memory Integration
chooses the largest set of pages belonging to the same block in this region and
moves them to the block-wise region (see Figure 7.6). When the block region is
full, C-Lash starts ﬂushing to the ﬂash memory following the LRU algorithm.
Figure 7.6. C-Lash cache structure. This cache consists of two buffers, one that
stores pages and the other groups of pages belonging to the same block. When the
application performs a write, data are written into the page buffer, also called the
p-space. If the latter is full, a ﬂush of the larger set of pages belonging to the same
block is performed in the block buffer (Flushing 1 in the ﬁgure), also called the b-space.
If it is full, the choice of the block that is ﬂushed out in the ﬂash memory is made
according to the LRU algorithm (Flushing 2 in the ﬁgure)
There are other cache algorithms, such as PUD-LRU [HU 10], LB-Clock
[DEB 09] and BPAC [WU 10]. It should be noted that most of these
algorithms consider
the granularity of page groups to optimize write
operations into ﬂash memory and utilize the concept of temporal locality
based on LRU-type algorithms.
7.7. Conclusion
This chapter introduces the basic services of an FTL, namely: address
mapping, wear leveling and garbage collection. We have also discussed cache
Flash Translation Layer
147
mechanisms that are speciﬁc to the constraints of ﬂash memory. In regards to
mapping schemes, we have ﬁrst started by describing basic schemes such as:
(1) page mapping, (2) block mapping and (3) hybrid mapping. The ﬁrst
implies a translation table that is too large but with very good performance
due to its ﬂexibility (each page can be processed independently of each
other). The second scheme has a major defect related to the performance of
the updates: each data modiﬁcation requires that the totality of the valid data
be rewritten into the block in which it is contained at another location. And
ﬁnally, the last scheme allows that the beneﬁts of the ﬁrst two be exploited.
We have also brieﬂy described several mapping schemes considered as state
of the art: BAST, FAST, LAST, DFTL, WAFTL, CACH-FTL, etc. These
hybrid translation or page-based schemes may present very different designs
which can lead to differences in terms of performance (as we will see in
Chapter 9).
We have subsequently described some wear-leveling algorithms that
provide the means to manage the wear over the entire surface of the ﬂash
memory, and giving a few examples based on the number of erase operations
and the number of writes. Next, we have described the main parameters taken
into account by garbage collectors as well as some examples of the state of
the art. These parameters are generally the proportion of invalid pages in a
block, the date of the last update and the cost of the garbage collection
operation. Finally, we have described the objectives of caches in devices
relying on ﬂash memory with some examples presented such as CFLRU,
BPLRU and C-Lash.
The FTL-based storage device considered in this book is the SSD. In the
following chapter, we describe a methodology to study the performance and
energy consumption of storage devices and in particular of SSDs.