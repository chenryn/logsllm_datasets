### HTTP Request Time and Traces

**Traces** capture the workflow and tasks executed in response to, for example, an HTTP request. A typical trace might look like this:

```json
{
  "traceId": "72c53",
  "name": "get",
  "timestamp": 1529029301238,
  "id": "df332",
  "duration": 124957,
  "annotations": [
    {
      "key": "http.status_code",
      "value": "200"
    },
    {
      "key": "http.url",
      "value": "https://v2/e5/servers/detail?limit=200"
    },
    {
      "key": "protocol",
      "value": "HTTP"
    }
  ],
  "endpoint": {
    "serviceName": "hss",
    "ipv4": "126.75.191.253"
  }
}
```

### Events

**Events** represent major milestones that occur within a data center. Examples include alarms, service upgrades, and software releases. Here are some examples of event records:

```json
{
  "id": "dns_address_match",
  "timestamp": 1529029301238
}
```

```json
{
  "id": "ping_packet_loss",
  "timestamp": 152902933452
}
```

```json
{
  "id": "tcp_connection_time",
  "timestamp": 15290294516578
}
```

```json
{
  "id": "cpu_usage_average",
  "timestamp": 1529023098976
}
```

### Our Contribution to AIOps Research (2019-2022)

| Field | Layers | Tasks | Publication |
|-------|--------|-------|-------------|
| Service | • A Survey of AIOps Methods for Failure Management. Notaro, P.; Cardoso, J. and Gerndt, M. In ACM Transactions on Intelligent Systems and Technology, 2021. |
| Hypervisor | • A Systematic Mapping Study in AIOps. Notaro, P.; Cardoso, J. and Gerndt, M. In AIOPS 2020 International Workshop on Artificial Intelligence for IT Operations, Springer, 2020. |
| Middleware | • Artificial Intelligence for IT Operations (AIOPS) Workshop White Paper. Bogatinovski, J.; Nedelkoski, S.; Acker, A.; Schmidt, F.; Wittkopp, T.; Becker, S.; Cardoso, J. and Kao, O. In AIOPS 2020 International Workshop on Artificial Intelligence for IT Operations, Springer, 2020. |
| General AIOps | • QuLog: Data-Driven Approach for Log Instruction Quality Assessment. Bogatinovski, J.; Nedelkoski, S.; Acker, A.; Cardoso, J. and Kao, O. In 30th IEEE/ACM International Conference on Program Comprehension, 2022. |
| OS | • Self-Supervised Log Parsing. Nedelkoski, S.; Bogatinovski, J.; Acker, A.; Cardoso, J. and Kao, O. In European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD), 14-18 September, 2020, Belgium, 2020. |
| Hardware | • Self-Attentive Classification-Based Anomaly Detection in Unstructured Logs. Nedelkoski, S.; Bogatinovski, J.; Acker, A.; Cardoso, J. and Kao, O. In 20th IEEE International Conference on Data Mining (ICDM), Italy, 2020. |
| Network | • Efficient Failure Diagnosis of OpenStack Using Tempest. Bhatia, A.; Gerndt, M. and Cardoso, J. In IEEE Internet Computing, Vol. 22 (6): 61-70, 2018. |
| Anomaly Detection | • Automated Analysis of Distributed Tracing: Challenges and Research Directions. Bento, A.; Correia, J.; Filipe, R.; Araujo, F. and Cardoso, J. In Journal of Grid Computing, Vol. 19 (9), 2021. |
| Root-Cause Analysis | • Self-Supervised Anomaly Detection from Distributed Traces. Bogatinovski, J.; Nedelkoski, S.; Cardoso, J. and Kao, O. In IEEE/ACM 13th International Conference on Utility and Cloud Computing (UCC), 2020. |
| Failure Prediction | • Anomaly Detection and Classification using Distributed Tracing and Deep Learning. Nedelkoski, S.; Cardoso, J. and Kao, O. In 19th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID), 2019. |
| Trace Analysis | • Anomaly Detection from System Tracing Data using Multimodal Deep Learning. Nedelkoski, S.; Cardoso, J. and Kao, O. In IEEE 12th International Conference on Cloud Computing (CLOUD), 2019. |
| Metric Analysis | • IAD: Indirect Anomalous VMMs Detection in the Cloud-based Environment. Jindal, A.; Shakhat, I.; Cardoso, J.; Gerndt, M. and Podolskiy, V. In AIOPS 2020 International Workshop on Artificial Intelligence for IT Operations, Springer, 2021. |
| Multi-Source | • Online Memory Leak Detection in the Cloud-based Infrastructures. Jindal, A.; Staab, P.; Cardoso, J.; Gerndt, M. and Podolskiy, V. In AIOPS 2020 International Workshop on Artificial Intelligence for IT Operations, Springer, 2020. |
| All | • Multi-source Distributed System Data for AI-Powered Analytics. Nedelkoski, S.; Bogatinovski, J.; Mandapati, A. K.; Becker, S.; Cardoso, J. and Kao, O. In Service-Oriented and Cloud Computing (ESOCC 2020), 28-30 September, 2020, Crete, pages 161-176, 2020. |

### Change Management: Intelligent Continuous Verification

#### Background & Motivation
Change processes often lead to failures, with common causes including upgrades, bugs, and configuration issues.

#### Description
- **Automated Service Change Verification**: Automatically validate canary phases/gates during service deployments to highlight bugs or new/removed functionality.
- **Rollback Invalid Deployments**: Roll back invalid service deployments to avoid failures in testing and production.

#### Anticipated Impact
- **Automated Change Management**: Reduce the risk of failures caused by changes.
- **Differences in Systems**: Analyze different versions of a system to detect changes.

#### How It Works
1. **Collect and Train**: Collect service logs from release n-1, divide them into 4 phases, and train a machine learning (ML) model for each phase.
2. **Evaluate Differences**: Use techniques such as NuLog, 2KDiff, or Drain to evaluate the differences between two logs.
3. **Release and Validate**: Release the service, collect logs for each phase, and use the ML models to check the validity of the logs.

#### Innovation
- **Intelligent Continuous Verification**: Continuously verify service changes using ML.

#### Assumptions & Limitations
- Only logs are used (traces and metrics are not analyzed).
- Commits with many modifications may cause false positives.

### Anomaly Detection: Detecting Faulty Hypervisors

#### Background & Motivation
Virtualization failures affect VMs but cannot be observed directly. Predictive maintenance is essential to prevent failures.

#### Description
- **Quorum Change-Point Detection**: Analyze individual time-series and use change points and voting to detect hypervisor malfunctions.
- **Key Results**: F1 score of 72% (for 2 VMs) and 80+% (for 3+ VMs).

#### How It Works
- **Method 1 (Change Points)**: Treat time-series as univariate, detect change points, and vote to decide global changes.
- **Method 2 (Isolation Forest)**: Treat time-series as features, detect significant changes, and use isolation forest for anomaly detection.
- **Method 3 (ECP E.Divisive)**: Treat time-series as multivariate, detect multiple change points, and use ECP E.Divisive for anomaly detection.

#### Innovation
- **Indirect Approach**: Detect hypervisor failures by monitoring VMs.

#### Assumptions & Limitations
- Datasets used for evaluation were collected from simulation environments, synthetic data generators, and public sources.
- When a hypervisor malfunctions, resource saturation of VMs suddenly changes.

### Root Cause Analysis: Application Logs

#### Background & Motivation
Once an anomaly is detected, root cause analysis (RCA) is crucial for resolving problems.

#### Description
- **Anomaly Detection in Logs**: Perform RCA based on application logs, correlate metric anomalies with alarms and logs, and summarize logs to reduce the amount of data a human has to process.

#### How It Works
1. **Template Mining**: Fast log reconstruction using the Drain algorithm.
2. **Natural Language Processing**: Use NLP for language-aware log parsing and keyword extraction.
3. **Dynamic Grouping**: Classify time-series using the Poisson model and group events using Pearson correlation coefficient.

#### Innovation
- **Fast Algorithms for RCA**: Use novel, fast algorithms for RCA using log analytics.

#### Assumptions & Limitations
- On-demand processing requires a certain range of logs to learn normality.
- Results depend on the quality of service logs.

### Anomaly Detection: Multi-modal Anomaly Detection

#### Background & Motivation
Move from single-source, single-dimension anomaly detection to multi-source and multi-dimensional approaches.

#### Description
- **Robust Anomaly Detection Ensemble**: Develop new ensemble AI algorithms to detect anomalies in multi-source, multi-dimensional data.
- **Extend Approaches**: Extend existing methods such as SkyWalking to improve precision.

#### How It Works
- **Ensemble Approach**: Combine multiple anomaly detection methods to improve robustness.
- **Multi-Modal Analysis**: Analyze metrics, logs, and traces together to reduce false positives.

#### Innovation
- **Improved Precision**: Use multi-modal analysis to enhance the precision of anomaly detection.

#### Assumptions & Limitations
- Noisy data and complex interactions between different data sources can affect the accuracy of the results.