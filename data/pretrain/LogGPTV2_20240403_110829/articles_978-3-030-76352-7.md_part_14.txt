Based on above definition, the performance diagnosis problem is formu-
lated as follows: given metrics m of a cloud microservice, assuming anomalies
are detected from metric m i of a set of services s a at time t, where i is the
index of response time, how can we identify the culprit metrics that cause the
anomaly? Furthermore, we break down the research problem as following two
sub-problems:
1. How to pinpoint the culprit service s rc that initiates the performance degra-
dation in microservices?;
2. Given the culprit service s rc, how to pinpoint the culprit metric m rc that
contributes to its abnormality?
3 System Overview
Toaddresstheculpritservicesandmetricsdiagnosisproblems,weproposeaper-
formancediagnosissystemshowninFig.1.Inoverall,therearefourcomponents
in our system, namely data collection, anomaly detection, culprit service local-
ization (CSL) and culprit metric localization (CML). Firstly, we collect metrics
from multiple data resources in the microservices, including run-time operating
system, the application and the network. In particular, we continuously mon-
itor the response times between all pairs of communicating services. Once the
anomaly detection module identifies long response times from services, it trig-
gers the system to localize the culprit service that the anomaly originates from.
1 Sock-shop - https://microservices-demo.github.io/.
2 Google Cloud Engine - https://cloud.google.com/compute/.
88 L. Wu et al.
After the culprit service localization, it returns a list of potential culprit ser-
vices, sorted by probability of being the source of the anomaly. Next, for each
potentialculpritservice,ourmethodidentifiestheanomalousmetricswhichcon-
tribute to the service abnormality. Finally, it outputs a list of (service, metrics
list) pairs, for the possible culprit service, and metrics, respectively. With the
help of this list, cloud operators can narrow down the causes and reduce the
time and complexity to get the real cause.
3.1 Data Collection
We collect data from multiple data sources, including the application, the oper-
atingsystemandthenetwork,inordertoprovideculpritsforperformanceissues
caused by diverse root causes, such as software bugs, hardware issues, resource
contention, etc. Our system is designed to be application-agnostic, requiring no
instrumentationtotheapplicationtogetthedata.Instead,wecollectthemetrics
that reported by the application and the run-time system themselves.
3.2 Anomaly Detection
Inthesystem,wedetecttheperformanceanomalyontheresponsetimesbetween
two interactive services (collected by service mesh) using a unsupervised learn-
ing method: BIRCH clustering [6]. When a response time deviates from their
normal status, it is detected as an anomaly and trigger the subsequent perfor-
mance diagnosis procedures. Note that, due to the complex dependency among
services and the properties of fault propagation, multiple anomalies could be
also detected from services that have no issue.
3.3 Culprit Service Localization (CSL)
Afteranomaliesaredetected,theculpritservicelocalizationistriggeredtoiden-
tify the faulty service that initiates the anomalies. To get the faulty services, we
usethemethodproposedbyWu,L.,etal.[16].First,itconstructsanattributed
graph to capture the anomaly propagation among services through not only the
service call paths but also the co-located machines. Next, it extracts an anoma-
lous subgraph based on detected anomalous services to narrow down the root
cause analysis scope from the large number of microservices. Finally, it ranks
the faulty services based on the personalized PageRank, where it correlates the
anomaloussymptomsinresponsetimeswithrelevantresourceutilizationincon-
tainer and system levels to calculate the transition probability matrix and Per-
sonalizedPageRankvector.Therearetwoparametersforthismethodthatneed
tuning: the anomaly detection threshold and the detection confidence. For the
detail of the method, please refer to [16].
With the identified faulty services, we further identify the culprit metrics
that make the service abnormal, which is detailed in Sect.4.
Performance Diagnosis in Cloud Microservices Using Deep Learning 89
CSL Ranked CML
culprit
Anomaly Yes Culprit services Culprit Cause list
Service Metric svc1: m1_list
svc2: m2_list
No
Fig.1. Overview of the proposed performance diagnosis system.
4 Culprit Metric Localization (CML)
The underlying assumption of our culprit metric localization of the root cause
lies in the observation that the underlying symptoms for the faulty behaviour
differ from their expected values during normal operation. For example, when
there is an anomaly of type “memory leak” it is expected that the memory in
the service increases drastically, as compared to the normal operation. In the
most general case, it is not known in advance which metric is contributing the
most and it is the most relevant for the underlying type of fault in an arbitrary
service.Besides,theremayexistvariousinter-relationshipsbetweentheobserved
metrics that manifest differently in normal or abnormal scenarios. Successful
modellingofthisinformationmayimprovetheanomalydetectionprocedureand
also better pinpoint the potential cause for the anomaly. For example in “CPU
hog” we experience not only CPU increase but also a slight memory increase.
Thus, some inter-metric relationships may not manifest themselves in same way
during anomalies as normal operation.
Totacklethesechallengesweadopttheautoencoderarchitecture.Anautoen-
coder is an approach that fits naturally under stressed conditions. The first
advantage of the method is that one can add an arbitrary number of input
metrics. Thus it can include many potential symptoms as potential faults to
be considered at once. The second advantage is that it can correlate arbitrary
relationships within the observed metric data with various complexity based on
the depth and applied nonlinearities.
An autoencoder [5] is a neural network architecture that learns a mapping
fromtheinputtoitself.Itiscomposedofanencoder-decoderstructureofatleast
3 layers: input, hidden and output layer. The encoder provides a mapping from
theinputtosomeintermediate(usuallylower-dimensional)representation,while
the decoder provides an inverse mapping from the intermediate representation
back to the input, Thus the cost function being optimized is given as in:
L(X,X)=||φ(X)−UUTφ(X)||2
(1)
2
where U can be seen as weights of the encoder-decoder structure learned using
thebackpropagationlearningalgorithm.Whilethereexistvariouswayshowthe
90 L. Wu et al.
Fig.2.Theinnerdiagramoftheculpritmetriclocalizationforanomalydetectionand
rootcauseinference.TheGaussianblockproducesdecisionthatapointisananomaly
if it is below a certain probability threshold. The circle with the greatest intensity of
black contributes the most to the error and is pointed as an root cause symptom.
mapping from one instance to another can be done, especially interesting is the
mapping when the hidden layer is of reduced size. This allows to compress the
information from the input and enforce it to learn various dependencies. During
thetrainingprocedure,theparametersoftheautoencoderaretrainedusingjust
normal data from the metrics. This allows us to learn the normal behaviour
of the system. In our approach, we further penalize the autoencoder to enforce
sparse weights and discourage propagation of information that is not relevant
via the L regularization technique. This acts in discouraging the modeling of
1
non-relevant dependancies between the metrics.
Figure2 depicts the overall culprit metric localization block. The approach
consistsofthreeunits:theautoencoder,anomalydetectionandroot-causelocal-
ization part. The root-cause localization part produces an ordered list of most
likely cause given the current values of the input metrics. There are two phases
of operation: the offline and online phase. During the offline phase, the parame-
tersoftheautoencoderandthegaussiandistributionpartaretuned.Duringthe
online phase, the input data is presented to the method one point at the time.
The input is propagated through the autoencoder and the anomaly detection
part. The output of the latter is propagated to the root-cause localization part
that outputs the most likely root-cause.
After training the autoencoder, the second step is to learn the parameters
of a Gaussian distribution of the reconstruction error. The underlying assump-
tion is that the data points that are very similar (e.g., lie within 3σ (standard
deviations)fromthemean)arelikelytocomefromaGaussiandistributionwith
the estimated parameters. As such they do not violate the expected values for
metrics. The parameters of the distribution are calculated on a held-out valida-
tion set from normal data points. As each of the services in the system is run in
a separate container and we have the metric for each of them, the autoencoder
Performance Diagnosis in Cloud Microservices Using Deep Learning 91
canbeutilizedasanadditionalanomalydetectionmethodonaservicelevel.As
the culprit service localization module exploits the global dependency graph of
the overall architecture, it suffersfrom the eminent noise propagated among the
services. While unable to exploit the structure of the architecture, the locality
propertyoftheautoencodercanbeusedtofine-tunetheresultsfromtheculprit
service localization module. Thus, with a combination of the strengths of the
two methods, we can produce better results for anomaly detection.
The decision for the most likely symptom is done such that we calculate the
individualerrorsbetweentheinputandthecorrespondingreconstructedoutput.
As the autoencoder is constrained to learn normal state, we hypothesize change
of the underlying symptom when an anomaly arises to occur. Hence, for a given
anomaly as a most likely cause, we report the symptom that contributes to the
final error the most.
5 Experimental Evaluation
Inthissection,wepresenttheexperimentalsetupandevaluatetheperformance
of our system in identifying the culprit metrics and services.
5.1 Testbed and Evaluation Metrics
To evaluate our system, we set up a testbed on Google Cloud Engine (see foot-
note 2), where we run the Sock-shop (see footnote 1) microservice benchmark
consisting of seven microservices in a Kubernetes cluster, and the monitoring
infrastructures, including the Istio service mesh3, node-exporter4, Cadvisor5,
Prometheus6. Each worker node in the cluster has 4 virtual CPUs, 15 GB of
memorywithContainer-OptimizedOS.Wealsodevelopedaworkloadgenerator
to send requests to different services.
To inject the performance issues in microservices, we customize the Docker
images of the services by installing the fault injection tools. We inject two types
of faults: CPU hog and memory leak, by exhausting the resource CPU and
memory in the container, with stress-ng7, into four different microservices. For
each anomaly, we repeated the experiments 6 times in the duration of at least
3min. To train the autoencoder, we collect data of 2h in normal status.
Toquantifytheperformanceofoursystem,weusethefollowingtwometrics:
– Precision at top k denotes the probability that the root causes are included
in the top k of the results. For a set of anomalies A, PR@k is defined as:
(cid:3)
PR@k = |A1 | (cid:2) (mi< ik n( (R k[ ,i |] v∈ rcv |)r )c) (2)
a∈A
where R[i] is the rank of each cause and v rc is the set of root causes.
3 Istio - https://istio.io/.
4 Node-exporter - https://github.com/prometheus/node exporter.
5 Cadvisor - https://github.com/google/cadvisor.
6 Prometheus - https://prometheus.io/.
7 stress-ng - https://kernel.ubuntu.com/∼cking/stress-ng/.
92 L. Wu et al.
Fig.3. Collected metrics when CPU hog is injected to microservice catalogue. (Color
figure online)
– Mean Average Precision (MAP) quantifies the overall performance of a
method, where N is the number of microservices:
(cid:2) (cid:2)
1
MAP = PR@k.
|A| (3)
a∈A1≤k≤N
5.2 Effectiveness Evaluation
For each anomaly case, we collect the performance metrics from the application
(suffixedwithlatency)andrun-timesystem,includingcontainers(prefixedwith
ctn) and worker nodes (prefixed with node). Figure3 gives an example of the
collectedmetricswhenthe“CPUhog”anomalyfaultisinjectedtothecatalogue
microservice, repeated six times within one hour. The data collected during the
fault injection is marked in red. The CPU hog fault is expected to be reflected
by the ctn cpu metric. We can see that (i) there are obvious spikes in metrics
ctn cpu and node cpu. The spike of node cpu is caused by the spike of ctn cpu
as container resource usage is correlated to node resource usage; (ii) metrics
ctn memory and node memory also have some deviations; (iii) the fault CPU
hog causes spikes in service latency. Therefore, we can conclude that the fault
injected to the service manifests itself with a significant deviation from normal
status. Meanwhile, it also affects some other metrics.
For each fault injected service, we train the autoencoder with normal data
andtestwiththeanomalousdata.Figure4showsthereconstructionerrorsfrom
autoencoderforeachmetric.Wecanseethatthemetricctn cpuhasalargeerror
comparing with other metrics, which indicates it has a higher probability to be
thecauseoftheanomalyofservicecatalogue.Thesecondhighestreconstruction
error is in the node cpu metric, which is due to its strong correlation with the
container resource usage. Hence, we conclude that ctn cpu is the culprit metric.
Table1demonstratestheresultsofourmethodondifferentmicroservicesand
faults,intermsofPR@1,PR@3andMAP.Weobservethatourmethodachieve
a good performance with 100% in PR@1 in different services and faults, except
for the service orders and carts with the fault memory leak. This is because (i)
Performance Diagnosis in Cloud Microservices Using Deep Learning 93
Fig.4.ReconstructionerrorsforeachmetricwhenCPUhogisinjectedtomicroservice
catalogue.
Table 1. Performance of identifying culprit metrics.
Service Orders Catalogue Carts User Average
CPU hog
PR@1 1.0 1.0 1.0 1.0 1.0
PR@3 1.0 1.0 1.0 1.0 1.0
MAP 1.0 1.0 1.0 1.0 1.0
Memory leak
PR@1 0.83 1.0 0 1.0 0.71
PR@3 0.83 1.0 1.0 1.0 0.96
MAP 0.88 1.0 0.83 1.0 0.93
ordersandcartsarecomputation-intensiveservices;(ii)weexhausttheirresource
memory heavily in our fault injection; (iii) fault memory leak issues manifest as
bothhighmemoryusageandhighCPUusage.Asourmethodtargetrootcause
that manifests itself with a significant deviation of causal metric, the accuracy
decreases when the root cause manifests in multiple metrics. On average, our
system achieves 85.5% in precision and 96.5% in MAP.
Furthermore,weapplytheautoencodertoallofthepinpointedfaultyservices
by the culprit service localization (CSL) module and analyze its performance of
identifyingtheculpritservices.Forexample,inananomalycasewhereweinject
aCPUhogintoservicecatalogue,theCSLmodulereturnsarankedlistandthe
real cause service catalogue is ranked as the third. The other two services with
higher rank are service orders and front-end. We leverage autoencoder to these
threeservices,andtheresultsshow(i)autoencoderofserviceorderreturnsNor-
mal, which means it is a false positive and can be removed from the ranked list;
(ii) autoencoder of service front-end returns Anomaly, and the highest ranked
metricisthelatency,whichindicatesthattheabnormalityoffront-endiscaused
byanexternalfactor,whichisthedownstreamservicecatalogue.Inthiscase,we
conclude that it is not a culprit service and remove it from the ranked list; (iii)
94 L. Wu et al.
Table 2. Comparisons of identifying culprit services.
Metrics CSL CSL + CML Improvement(%)
PR@1 0.57 0.92 61.4
PR@3 0.83 0.98 18.1
MAP 0.85 0.97 14.1
Fig.5. Calibration of culprit service localization with autoencoder.
autoencoder of service catalogue returns Anomaly and the top-ranked metric is
ctn cpu. Therefore, with autoencoder, we can reduce the number of potential
faulty services from 3 to 1.