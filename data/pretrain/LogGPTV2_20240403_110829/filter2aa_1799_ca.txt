VP limits Similar to VP reservations, a user can limit the percentage of physical CPU usage for
a VP. This means reducing the available time slice allocated to a VP in a high workload scenario.
I 
VP weight This controls the probability that a VP is scheduled when the reservations have
already been met. In default configurations, each VP has an equal probability of being executed.
When the user configures weight on the VPs that belong to a virtual machine, scheduling deci-
sions become based on the relative weighting factor the user has chosen. For example, let’s
assume that a system with four CPUs runs three virtual machines at the same time. The first VM
has set a weighting factor of 100, the second 200, and the third 300. Assuming that all the system’s
physical processors are allocated to a uniform number of VPs, the probability of a VP in the first
VM to be dispatched is 17%, of a VP in the second VM is 33%, and of a VP in the third one is 50%.
FIGURE 9-12 The classic scheduler fine-tuning settings property page, which is available  
only when the classic scheduler is enabled.
CHAPTER 9 Virtualization technologies
291
The core scheduler
Normally, a classic CPU’s core has a single execution pipeline in which streams of instructions are 
executed one after each other. An instruction enters the pipe, proceeds through several stages of 
execution (load data, compute, store data, for example), and is retired from the pipe. Different types 
of instructions use different parts of the CPU core. A modern CPU’s core is often able to execute in an 
out-of-order way multiple sequential instructions in the stream (in respect to the order in which they 
entered the pipeline). Modern CPUs, which support out-of-order execution, often implement what is 
called symmetric multithreading (SMT): a CPU’s core has two execution pipelines and presents more 
than one logical processor to the system; thus, two different instruction streams can be executed side 
by side by a single shared execution engine. (The resources of the core, like its caches, are shared.) The 
two execution pipelines are exposed to the software as single independent processors (CPUs). From 
now on, with the term logical processor (or simply LP), we will refer to an execution pipeline of an SMT 
core exposed to Windows as an independent CPU. (SMT is discussed in Chapters 2 and 4 of Part 1.)
This hardware implementation has led to many security problems: one instruction executed 
by a shared logical CPU can interfere and affect the instruction executed by the other sibling LP. 
Furthermore, the physical core’s cache memory is shared; an LP can alter the content of the cache. The 
other sibling CPU can potentially probe the data located in the cache by measuring the time employed 
by the processor to access the memory addressed by the same cache line, thus revealing “secret data” 
accessed by the other logical processor  (as described in the “Hardware side-channel vulnerabilities” 
section of Chapter 8). The classic scheduler can normally select two threads belonging to different VMs 
to be executed by two LPs in the same processor core. This is clearly not acceptable because in this 
context, the first virtual machine could potentially read data belonging to the other one.
To overcome this problem, and to be able to run SMT-enabled VMs with predictable performance, 
Windows Server 2016 has introduced the core scheduler. The core scheduler leverages the properties 
of SMT to provide isolation and a strong security boundary for guest VPs. When the core scheduler is 
enabled, Hyper-V schedules virtual cores onto physical cores. Furthermore, it ensures that VPs belong-
ing to different VMs are never scheduled on sibling SMT threads of a physical core. The core scheduler 
enables the virtual machine for making use of SMT. The VPs exposed to a VM can be part of an SMT 
set. The OS and applications running in the guest virtual machine can use SMT behavior and program-
ming interfaces (APIs) to control and distribute work across SMT threads, just as they would when 
run nonvirtualized. 
Figure 9-13 shows an example of an SMT system with four logical processors distributed in two CPU 
cores. In the figure, three VMs are running. The first and second VMs have four VPs in two groups of two, 
whereas the third one has only one assigned VP. The groups of VPs in the VMs are labelled A through E. 
Individual VPs in a group that are idle (have no code to execute) are filled with a darker color. 
292 
CHAPTER 9 Virtualization technologies
Deferred List
Run List
VM # 1
VP
VP
VP
VP
B
A
VP
VP
A
VP
VP
C
VM # 2
VP
VP
VP
VP
D
C
VM # 3
VP
E
LP
LP
CPU Core #1
Deferred List
Run List
LP
LP
CPU Core #2
VP
VP
B
VP
VP
E
FIGURE 9-13 A sample SMT system with two processors’ cores and three VMs running.
Each core has a run list containing groups of VPs that are ready to execute, and a deferred list of 
groups of VPs that are ready to run but have not been added to the core’s run list yet. The groups of 
VPs execute on the physical cores. If all VPs in a group become idle, then the VP group is descheduled 
and does not appear on any run list. (In Figure 9-13, this is the situation for VP group D.) The only VP of 
the group E has recently left the idle state. The VP has been assigned to the CPU core 2. In the figure, 
a dummy sibling VP is shown. This is because the LP of core 2 never schedules any other VP while its 
sibling LP of its core is executing a VP belonging to the VM 3. In the same way, no other VPs are sched-
uled on a physical core if one VP in the LP group became idle but the other is still executing (such as for 
group A, for example). Each core executes the VP group that is at the head of its run list. If there are no 
VP groups to execute, the core becomes idle and waits for a VP group to be deposited onto its deferred 
run list. When this occurs, the core wakes up from idle and empties its deferred run list, placing the 
contents onto its run list.
CHAPTER 9 Virtualization technologies
293
The core scheduler is implemented by different components (see Figure 9-14) that provide strict 
layering between each other. The heart of the core scheduler is the scheduling unit, which represents a 
virtual core or group of SMT VPs. (For non-SMT VMs, it represents a single VP.) Depending on the VM’s 
type, the scheduling unit has either one or two threads bound to it. The hypervisor’s process owns a list 
of scheduling units, which own threads backing up to VPs belonging to the VM. The scheduling unit is 
the single unit of scheduling for the core scheduler to which scheduling settings—such as reservation, 
weight, and cap—are applied during runtime. A scheduling unit stays active for the duration of a time 
slice, can be blocked and unblocked, and can migrate between different physical processor cores. An 
important concept is that the scheduling unit is analogous to a thread in the classic scheduler, but it 
doesn’t have a stack or VP context in which to run. It’s one of the threads bound to a scheduling unit 
that runs on a physical processor core. The thread gang scheduler is the arbiter for each scheduling unit. 
It’s the entity that decides which thread from the active scheduling unit gets run by which LP from the 
physical processor core. It enforces thread affinities, applies thread scheduling policies, and updates 
the related counters for each thread.
Scheduler Service
Scheduler Manager
Unit Scheduler
Core Dispatcher
CPU_PLS
Scheduling Unit
Scheduling Unit
Scheduling Unit
TH_THREAD
TH_PROCESS
Logical Processor Dispatcher
Thread Gang Scheduler
FIGURE 9-14 The components of the core scheduler.
294 
CHAPTER 9 Virtualization technologies
Each LP of the physical processor’s core has an instance of a logical processor dispatcher associated 
with it. The logical processor dispatcher is responsible for switching threads, maintaining timers, and 
flushing the VMCS (or VMCB, depending on the architecture) for the current thread. Logical proces-
sor dispatchers are owned by the core dispatcher, which represents a physical single processor core 
and owns exactly two SMT LPs. The core dispatcher manages the current (active) scheduling unit. The 
unit scheduler, which is bound to its own core dispatcher, decides which scheduling unit needs to run 
next on the physical processor core the unit scheduler belongs to. The last important component of 
the core scheduler is the scheduler manager, which owns all the unit schedulers in the system and has 
a global view of all their states. It provides load balancing and ideal core assignment services to the 
unit scheduler.
The root scheduler
The root scheduler (also known as integrated scheduler) was introduced in Windows 10 April 2018 
Update (RS4) with the goal to allow the root partition to schedule virtual processors (VPs) belonging 
to guest partitions. The root scheduler was designed with the goal to support lightweight containers 
used by Windows Defender Application Guard. Those types of containers (internally called Barcelona 
or Krypton containers) must be managed by the root partition and should consume a small amount of 
memory and hard-disk space. (Deeply describing Krypton containers is outside the scope of this book. 
You can find an introduction of server containers in Part 1, Chapter 3, “Processes and jobs”). In addition, 
the root OS scheduler can readily gather metrics about workload CPU utilization inside the container 
and use this data as input to the same scheduling policy applicable to all other workloads in the system.
The NT scheduler in the root partition’s OS instance manages all aspects of scheduling work to 
system LPs. To achieve that, the integrated scheduler’s root component inside the VID driver creates 
a VP-dispatch thread inside of the root partition (in the context of the new VMMEM process) for each 
guest VP. (VA-backed VMs are discussed later in this chapter.) The NT scheduler in the root partition 
schedules VP-dispatch threads as regular threads subject to additional VM/VP-specific scheduling poli-
cies and enlightenments. Each VP-dispatch thread runs a VP-dispatch loop until the VID driver termi-
nates the corresponding VP.
The VP-dispatch thread is created by the VID driver after the VM Worker Process (VMWP), which is 
covered in the “Virtualization stack” section later in this chapter, has requested the partition and VPs 
creation through the SETUP_PARTITION IOCTL. The VID driver communicates with the WinHvr driver, 
which in turn initializes the hypervisor’s guest partition creation (through the HvCreatePartition hyper-
call). In case the created partition represents a VA-backed VM, or in case the system has the root sched-
uler active, the VID driver calls into the NT kernel (through a kernel extension) with the goal to create 
the VMMEM minimal process associated with the new guest partition. The VID driver also creates a VP-
dispatch thread for each VP belonging to the partition. The VP-dispatch thread executes in the context 
of the VMMEM process in kernel mode (no user mode code exists in VMMEM) and is implemented in 
the VID driver (and WinHvr). As shown in Figure 9-15, each VP-dispatch thread runs a VP-dispatch loop 
until the VID terminates the corresponding VP or an intercept is generated from the guest partition. 
CHAPTER 9 Virtualization technologies
295
VmWp Worker
Thread
VP-Dispatch
Thread
Process the
Intercept
Intercept
Completed?
VidVpRun
Signals the
ThreadWork
Event
Waits for the
ThreadWork
Event
Run Root
Schedule
Dispatch Loop
Signals the
DispatchDone
Event
Send
HvDispatchVp
to the Hypervisor
Waits for the
DispatchDone
Event
Complete the
IOCTL and
Exit to VmWp
Calls into VID to Get
a Message
Wakes Up the VP-
dispatch Thread
YES
YES
NO
NO
*Run by the VID Driver in the
VMMEM Process Context
Wakes Up the VmWp
Worker Thread
*Using the WinHvr Driver
Intercept
Requires User-
Mode Processing 
FIGURE 9-15 The root scheduler’s VP-dispatch thread and the associated VMWP worker thread  
that processes the hypervisor’s messages.
While in the VP-dispatch loop, the VP-dispatch thread is responsible for the following:
1.
Call the hypervisor’s new HvDispatchVp hypercall interface to dispatch the VP on the current
processor. On each HvDispatchVp hypercall, the hypervisor tries to switch context from the cur-
rent root VP to the specified guest VP and let it run the guest code. One of the most important
characteristics of this hypercall is that the code that emits it should run at PASSIVE_LEVEL IRQL.
The hypervisor lets the guest VP run until either the VP blocks voluntarily, the VP generates an
intercept for the root, or there is an interrupt targeting the root VP. Clock interrupts are still
processed by the root partitions. When the guest VP exhausts its allocated time slice, the VP-
backing thread is preempted by the NT scheduler. On any of the three events, the hypervisor
switches back to the root VP and completes the HvDispatchVp hypercall. It then returns to the
root partition.
296 
CHAPTER 9 Virtualization technologies
2.
Block on the VP-dispatch event if the corresponding VP in the hypervisor is blocked. Anytime
the guest VP is blocked voluntarily, the VP-dispatch thread blocks itself on a VP-dispatch event
until the hypervisor unblocks the corresponding guest VP and notifies the VID driver. The VID
driver signals the VP-dispatch event, and the NT scheduler unblocks the VP-dispatch thread
that can make another HvDispatchVp hypercall.
3.
Process all intercepts reported by the hypervisor on return from the dispatch hypercall. If the
guest VP generates an intercept for the root, the VP-dispatch thread processes the intercept
request on return from the HvDispatchVp hypercall and makes another HvDispatchVp request
after the VID completes processing of the intercept. Each intercept is managed differently. If
the intercept requires processing from the user mode VMWP process, the WinHvr driver exits
the loop and returns to the VID, which signals an event for the backed VMWP thread and waits
for the intercept message to be processed by the VMWP process before restarting the loop.
To properly deliver signals to VP-dispatch threads from the hypervisor to the root, the integrated 
scheduler provides a scheduler message exchange mechanism. The hypervisor sends scheduler mes-
sages to the root partition via a shared page. When a new message is ready for delivery, the hypervisor 
injects a SINT interrupt into the root, and the root delivers it to the corresponding ISR handler in the 
WinHvr driver, which routes the message to the VID intercept callback (VidInterceptIsrCallback). The 
intercept callback tries to handle the intercept message directly from the VID driver. In case the direct 
handling is not possible, a synchronization event is signaled, which allows the dispatch loop to exit and 
allows one of the VmWp worker threads to dispatch the intercept in user mode.
Context switches when the root scheduler is enabled are more expensive compared to other hyper-
visor scheduler implementations. When the system switches between two guest VPs, for example, it 
always needs to generate two exits to the root partitions. The integrated scheduler treats hypervisor’s 
root VP threads and guest VP threads very differently (they are internally represented by the same 
TH_THREAD data structure, though):
I 
Only the root VP thread can enqueue a guest VP thread to its physical processor. The root VP
thread has priority over any guest VP that is running or being dispatched. If the root VP is not
blocked, the integrated scheduler tries its best to switch the context to the root VP thread as
soon as possible.
I 
A guest VP thread has two sets of states: thread internal states and thread root states. The thread
root states reflect the states of the VP-dispatch thread that the hypervisor communicates to
the root partition. The integrated scheduler maintains those states for each guest VP thread to
know when to send a wake-up signal for the corresponding VP-dispatch thread to the root.
Only the root VP can initiate a dispatch of a guest VP for its processor. It can do that either because 
of HvDispatchVp hypercalls (in this situation, we say that the hypervisor is processing “external work”), 
or because of any other hypercall that requires sending a synchronous request to the target guest VP 
(this is what is defined as “internal work”). If the guest VP last ran on the current physical processor, the 
CHAPTER 9 Virtualization technologies
297
scheduler can dispatch the guest VP thread right away. Otherwise, the scheduler needs to send a flush 
request to the processor on which the guest VP last ran and wait for the remote processor to flush the 
VP context. The latter case is defined as “migration” and is a situation that the hypervisor needs to track 
(through the thread internal states and root states, which are not described here). 
EXPERIMENT: Playing with the root scheduler
The NT scheduler decides when to select and run a virtual processor belonging to a VM and for 
how long. This experiment demonstrates what we have discussed previously: All the VP dis-
patch threads execute in the context of the VMMEM process, created by the VID driver. For the 
experiment, you need a workstation with at least Windows 10 April 2018 update (RS4) installed, 
along with the Hyper-V role enabled and a VM with any operating system installed ready for use. 
The procedure for creating a VM is explained in detail here: https://docs.microsoft.com/en-us/
virtualization/hyper-v-on-windows/quick-start/quick-create-virtual-machine.
First, you should verify that the root scheduler is enabled. Details on the procedure are avail-
able in the “Controlling the hypervisor’s scheduler type” experiment earlier in this chapter. The 
VM used for testing should be powered down.
Open the Task Manager by right-clicking on the task bar and selecting Task Manager, click the 
Details sheet, and verify how many VMMEM processes are currently active. In case no VMs are 