limitation can break the file’s availability for many seconds—or even
minutes. Clearly this is not acceptable for large file-server environments.
To resolve this, recent versions of Windows 10 introduced online
encryption support. Through the right synchronization, the NTFS driver is
able to perform file encryption and decryption without retaining exclusive
file access. EFS enables online encryption only if the target encryption
stream is a data stream (named or unnamed) and is nonresident. (Otherwise, a
standard encryption process starts.) If both conditions are satisfied, the EFS
service sends a FSCTL_SET_ENCRYPTION control code to the NTFS driver
to set a flag that enables online encryption.
Online encryption is possible thanks to the "$EfsBackup" attribute (of type
$LOGGED_UTILITY_STREAM) and to the introduction of range locks, a
new feature that allows the file system driver to lock (in an exclusive or
shared mode) only only a portion of a file. When online encryption is
enabled, the NtfsEncryptDecryptOnline internal function starts the encryption
and decryption process by creating the $EfsBackup attribute (and its SCB)
and by acquiring a shared lock on the first 2-MB range of the file. A shared
lock means that multiple readers can still read from the file range, but other
writers need to wait until the end of the encryption or decryption operation
before they can write new data.
The NTFS driver allocates a 2-MB buffer from the nonpaged pool and
reserves some clusters from the volume, which are needed to represent 2 MB
of free space. (The total number of clusters depends on the volume cluster’s
size.) The online encryption function reads the original data from the
physical disk and stores it in the allocated buffer. If BitLocker encryption
offload is not enabled (described in the previous section), the buffer is
encrypted using EFS services; otherwise, the BitLocker driver encrypts the
data when the buffer is written to the previously reserved clusters.
At this stage, NTFS locks the entire file for a brief amount of time: only
the time needed to remove the clusters containing the unencrypted data from
the original stream’s extent table, assign them to the $EfsBackup non-
resident attribute, and replace the removed range of the original stream’s
extent table with the new clusters that contain the newly encrypted data.
Before releasing the exclusive lock, the NTFS driver calculates a new high
watermark value and stores it both in the original file in-memory SCB and in
the EFS payload of the $EFS alternate data stream. NTFS then releases the
exclusive lock. The clusters that contain the original data are first zeroed out;
then, if there are no more blocks to process, they are eventually freed.
Otherwise, the online encryption cycle restarts with the next 2-MB chunk.
The high watermark value stores the file offset that represents the
boundary between encrypted and nonencrypted data. Any concurrent write
beyond the watermark can occur in its original form; other concurrent writes
before the watermark need to be encrypted before they can succeed. Writes
to the current locked range are not allowed. Figure 11-72 shows an example
of an ongoing online encryption for a 16-MB file. The first two blocks (2 MB
in size) already have been encrypted; the high watermark value is set to 4
MB, dividing the file between its encrypted and non-encrypted data. A range
lock is set on the 2-MB block that follows the high watermark. Applications
can still read from that block, but they can’t write any new data (in the latter
case, they need to wait). The block’s data is encrypted and stored in reserved
clusters. When exclusive file ownership is taken, the original block’s clusters
are remapped to the $EfsBackup stream (by removing or splitting their entry
in the original file’s extent table and inserting a new entry in the $EfsBackup
attribute), and the new clusters are inserted in place of the previous ones. The
high watermark value is increased, the file lock is released, and the online
encryption process proceeds to the next stage starting at the 6-MB offset; the
previous clusters located in the $EfsBackup stream are concurrently zeroed-
out and can be reused for new stages.
Figure 11-72 Example of an ongoing online encryption for a 16MB file.
The new implementation allows NTFS to encrypt or decrypt in place,
getting rid of temporary files (see the previous “Encrypting file data” section
for more details). More importantly, it allows NTFS to perform file
encryption and decryption while other applications can still use and modify
the target file stream (the time spent with the exclusive lock hold is small and
not perceptible by the application that is attempting to use the file).
Direct Access (DAX) disks
Persistent memory is an evolution of solid-state disk technology: a new kind
of nonvolatile storage medium that has RAM-like performance characteristics
(low latency and high bandwidth), resides on the memory bus (DDR), and
can be used like a standard disk device.
Direct Access Disks (DAX) is the term used by the Windows operating
system to refer to such persistent memory technology (another common term
used is storage class memory, abbreviated as SCM). A nonvolatile dual in-
line memory module (NVDIMM), shown in Figure 11-73, is an example of
this new type of storage. NVDIMM is a type of memory that retains its
contents even when electrical power is removed. “Dual in-line” identifies the
memory as using DIMM packaging. At the time of writing, there are three
different types of NVDIMMs: NVIDIMM-F contains only flash storage;
NVDIMM-N, the most common, is produced by combining flash storage and
traditional DRAM chips on the same module; and NVDIMM-P has persistent
DRAM chips, which do not lose data in event of power failure.
Figure 11-73 An NVDIMM, which has DRAM and Flash chips. An
attached battery or on-board supercapacitors are needed for maintaining
the data in the DRAM chips.
One of the main characteristics of DAX, which is key to its fast
performance, is the support of zero-copy access to persistent memory. This
means that many components, like the file system driver and memory
manager, need to be updated to support DAX, which is a disruptive
technology.
Windows Server 2016 was the first Windows operating system to supports
DAX: the new storage model provides compatibility with most existing
applications, which can run on DAX disks without any modification. For
fastest performance, files and directories on a DAX volume need to be
mapped in memory using memory-mapped APIs, and the volume needs to be
formatted in a special DAX mode. At the time of this writing, only NTFS
supports DAX volumes.
The following sections describe the way in which direct access disks
operate and detail the architecture of the new driver model and the
modification on the main components responsible for DAX volume support:
the NTFS driver, memory manager, cache manager, and I/O manager.
Additionally, inbox and third-party file system filter drivers (including mini
filters) must also be individually updated to take full advantage of DAX.
DAX driver model
To support DAX volumes, Windows needed to introduce a brand-new
storage driver model. The SCM Bus Driver (Scmbus.sys) is a new bus driver
that enumerates physical and logical persistent memory (PM) devices on the
system, which are attached to its memory bus (the enumeration is performed
thanks to the NFIT ACPI table). The bus driver, which is not considered part
of the I/O path, is a primary bus driver managed by the ACPI enumerator,
which is provided by the HAL (hardware abstraction layer) through the
hardware database registry key
(HKLM\SYSTEM\CurrentControlSet\Enum\ACPI). More details about Plug
& Play Device enumeration are available in Chapter 6 of Part 1.
Figure 11-74 shows the architecture of the SCM storage driver model. The
SCM bus driver creates two different types of device objects:
■    Physical device objects (PDOs) represent physical PM devices. A
NVDIMM device is usually composed of one or multiple interleaved
NVDIMM-N modules. In the former case, the SCM bus driver creates
only one physical device object representing the NVDIMM unit. In
the latter case, it creates two distinct devices that represent each
NVDIMM-N module. All the physical devices are managed by the
miniport driver, Nvdimm.sys, which controls a physical NVDIMM
and is responsible for monitoring its health.
■    Functional device objects (FDOs) represent single DAX disks, which
are managed by the persistent memory driver, Pmem.sys. The driver
controls any byte-addressable interleave sets and is responsible for all
I/O directed to a DAX volume. The persistent memory driver is the
class driver for each DAX disk. (It replaces Disk.sys in the classical
storage stack.)
Both the SCM bus driver and the NVDIMM miniport driver expose some
interfaces for communication with the PM class driver. Those interfaces are
exposed through an IRP_MJ_PNP major function by using the
IRP_MN_QUERY_INTERFACE request. When the request is received, the
SCM bus driver knows that it should expose its communication interface
because callers specify the {8de064ff-b630-42e4-ea88-6f24c8641175}
interface GUID. Similarly, the persistent memory driver requires
communication interface to the NVDIMM devices through the {0079c21b-
917e-405e-cea9-0732b5bbcebd} GUID.
Figure 11-74 The SCM Storage driver model.
The new storage driver model implements a clear separation of
responsibilities: The PM class driver manages logical disk functionality
(open, close, read, write, memory mapping, and so on), whereas NVDIMM
drivers manage the physical device and its health. It will be easy in the future
to add support for new types of NVDIMM by just updating the Nvdimm.sys
driver. (Pmem.sys doesn’t need to change.)
DAX volumes
The DAX storage driver model introduces a new kind of volume: the DAX
volumes. When a user first formats a partition through the Format tool, she
can specify the /DAX argument to the command line. If the underlying
medium is a DAX disk, and it’s partitioned using the GPT scheme, before
creating the basic disk data structure needed for the NTFS file system, the
tool writes the GPT_BASIC_DATA_ ATTRIBUTE_DAX flag in the target
volume GPT partition entry (which corresponds to bit number 58). A good
reference for the GUID partition table is available at
https://en.wikipedia.org/wiki/GUID_Partition_Table.
When the NTFS driver then mounts the volume, it recognizes the flag and
sends a STORAGE_QUERY_PROPERTY control code to the underlying
storage driver. The IOCTL is recognized by the SCM bus driver, which
responds to the file system driver with another flag specifying that the
underlying disk is a DAX disk. Only the SCM bus driver can set the flag.
Once the two conditions are verified, and as long as DAX support is not
disabled through the
HKLM\System\CurrentControlSet\Control\FileSystem\NtfsEnableDirectAcce
ss registry value, NTFS enables DAX volume support.
DAX volumes are different from the standard volumes mainly because
they support zero-copy access to the persistent memory. Memory-mapped
files provide applications with direct access to the underlying hardware disk
sectors (through a mapped view), meaning that no intermediary components
will intercept any I/O. This characteristic provides extreme performance (but
as mentioned earlier, can impact file system filter drivers, including
minifilters).
When an application creates a memory-mapped section backed by a file
that resides on a DAX volume, the memory manager asks the file system
whether the section should be created in DAX mode, which is true only if the
volume has been formatted in DAX mode, too. When the file is later mapped
through the MapViewOfFile API, the memory manager asks the file system
for the physical memory range of a given range of the file. The file system
driver translates the requested file range in one or more volume relative
extents (sector offset and length) and asks the PM disk class driver to
translate the volume extents into physical memory ranges. The memory
manager, after receiving the physical memory ranges, updates the target
process page tables for the section to map directly to persistent storage. This
is a truly zero-copy access to storage: an application has direct access to the
persistent memory. No paging reads or paging writes will be generated. This
is important; the cache manager is not involved in this case. We examine the
implications of this later in the chapter.
Applications can recognize DAX volumes by using the
GetVolumeInformation API. If the returned flags include
FILE_DAX_VOLUME, the volume is formatted with a DAX-compatible file
system (only NTFS at the time of this writing). In the same way, an
application can identify whether a file resides on a DAX disk by using the
GetVolumeInformationByHandle API.
Cached and noncached I/O in DAX volumes
Even though memory-mapped I/O for DAX volumes provide zero-copy
access to the underlying storage, DAX volumes still support I/O through
standard means (via classic ReadFile and WriteFile APIs). As described at
the beginning of the chapter, Windows supports two kinds of regular I/O:
cached and noncached. Both types have significant differences when issued
to DAX volumes.
Cached I/O still requires interaction from the cache manager, which, while
creating a shared cache map for the file, requires the memory manager to
create a section object that directly maps to the PM hardware. NTFS is able
to communicate to the cache manager that the target file is in DAX-mode
through the new CcInitializeCacheMapEx routine. The cache manager will
then copy data from the user buffer to persistent memory: cached I/O has
therefore one-copy access to persistent storage. Note that cached I/O is still
coherent with other memory-mapped I/O (the cache manager uses the same
section); as in the memory-mapped I/O case, there are still no paging reads or
paging writes, so the lazy writer thread and intelligent read-ahead are not
enabled.
One implication of the direct-mapping is that the cache manager directly
writes to the DAX disk as soon as the NtWriteFile function completes. This
means that cached I/O is essentially noncached. For this reason, noncached
I/O requests are directly converted by the file system to cached I/O such that
the cache manager still copies directly between the user’s buffer and
persistent memory. This kind of I/O is still coherent with cached and
memory-mapped I/O.
NTFS continues to use standard I/O while processing updates to its
metadata files. DAX mode I/O for each file is decided at stream creation time
by setting a flag in the stream control block. If a file is a system metadata
file, the attribute is never set, so the cache manager, when mapping such a
file, creates a standard non-DAX file-backed section, which will use the
standard storage stack for performing paging read or write I/Os. (Ultimately,
each I/O is processed by the Pmem driver just like for block volumes, using
the sector atomicity algorithm. See the “Block volumes” section for more
details.) This behavior is needed for maintaining compatibility with write-
ahead logging. Metadata must not be persisted to disk before the
corresponding log is flushed. So, if a metadata file were DAX mapped, that
write-ahead logging requirement would be broken.
Effects on file system functionality
The absence of regular paging I/O and the application’s ability to directly
access persistent memory eliminate traditional hook points that the file
systems and related filters use to implement various features. Multiple
functionality cannot be supported on DAX-enabled volumes, like file
encryption, compressed and sparse files, snapshots, and USN journal support.
In DAX mode, the file system no longer knows when a writable memory-
mapped file is modified. When the memory section is first created, the NTFS
file system driver updates the file’s modification and access times and marks
the file as modified in the USN change journal. At the same time, it signals a
directory change notification. DAX volumes are no longer compatible with
any kind of legacy filter drivers and have a big impact on minifilters (filter
manager clients). Components like BitLocker and the volume shadow copy
driver (Volsnap.sys) don’t work with DAX volumes and are removed from
the device stack. Because a minifilter no longer knows if a file has been
modified, an antimalware file access scanner, such as one described earlier,
can no longer know if it should scan a file for viruses. It needs to assume, on
any handle close, that modification may have occurred. In turn, this
significantly harms performance, so minifilters must manually opt-in to
support DAX volumes.
Mapping of executable images
When the Windows loader maps an executable image into memory, it uses
memory-mapping services provided by the memory manager. The loader
creates a memory-mapped image section by supplying the SEC_IMAGE flag
to the NtCreateSection API. The flag specifies to the loader to map the
section as an image, applying all the necessary fixups. In DAX mode this
mustn’t be allowed to happen; otherwise, all the relocations and fixups will
be applied to the original image file on the PM disk. To correctly deal with
this problem, the memory manager applies the following strategies while
mapping an executable image stored in a DAX mode volume:
■    If there is already a control area that represents a data section for the
binary file (meaning that an application has opened the image for
reading binary data), the memory manager creates an empty memory-
backed image section and copies the data from the existing data
section to the newly created image section; then it applies the
necessary fixups.
■    If there are no data sections for the file, the memory manager creates
a regular non-DAX image section, which creates standard invalid
prototype PTEs (see Chapter 5 of Part 1 for more details). In this case,
the memory manager uses the standard read and write routines of the
Pmem driver to bring data in memory when a page fault for an invalid
access on an address that belongs to the image-backed section
happens.
At the time of this writing, Windows 10 does not support execution in-
place, meaning that the loader is not able to directly execute an image from
DAX storage. This is not a problem, though, because DAX mode volumes
have been originally designed to store data in a very performant way.
Execution in-place for DAX volumes will be supported in future releases of
Windows.
EXPERIMENT: Witnessing DAX I/O with Process
Monitor
You can witness DAX I/Os using Process Monitor from
SysInternals and the FsTool.exe application, which is available in
this book’s downloadable resources. When an application reads or
writes from a memory-mapped file that resides on a DAX-mode
volume, the system does not generate any paging I/O, so nothing is
visible to the NTFS driver or to the minifilters that are attached
above or below it. To witness the described behavior, just open
Process Monitor, and, assuming that you have two different
volumes mounted as the P: and Q: drives, set the filters in a similar
way as illustrated in the following figure (the Q: drive is the DAX-
mode volume):
For generating I/O on DAX-mode volumes, you need to simulate
a DAX copy using the FsTool application. The following example
copies an ISO image located in the P: DAX block-mode volume
(even a standard volume created on the top of a regular disk is fine
for the experiment) to the DAX-mode “Q:” drive:
Click here to view code image
P:\>fstool.exe /daxcopy p:\Big_image.iso q:\test.iso
NTFS / ReFS Tool v0.1
Copyright (C) 2018 Andrea Allievi (AaLl86)
Starting DAX copy...
   Source file path: p:\Big_image.iso.
   Target file path: q:\test.iso.
   Source Volume: p:\ - File system: NTFS - Is DAX Volume: 
False.
   Target Volume: q:\ - File system: NTFS - Is DAX Volume: 
True.
   Source file size: 4.34 GB
Performing file copy... Success!
   Total execution time: 8 Sec.
   Copy Speed: 489.67 MB/Sec
Press any key to exit...
Process Monitor has captured a trace of the DAX copy operation
that confirms the expected results:
From the trace above, you can see that on the target file
(Q:\test.iso), only the CreateFileMapping operation was
intercepted: no WriteFile events are visible. While the copy was
proceeding, only paging I/O on the source file was detected by
Process Monitor. These paging I/Os were generated by the memory
manager, which needed to read the data back from the source
volume as the application was generating page faults while
accessing the memory-mapped file.
To see the differences between memory-mapped I/O and
standard cached I/O, you need to copy again the file using a
standard file copy operation. To see paging I/O on the source file
data, make sure to restart your system; otherwise, the original data
remains in the cache:
Click here to view code image
P:\>fstool.exe /copy p:\Big_image.iso q:\test.iso
NTFS / ReFS Tool v0.1
Copyright (C) 2018 Andrea Allievi (AaLl86)
Copying "Big_image.iso" to "test.iso" file... Success.
   Total File-Copy execution time: 13 Sec - Transfer Rate: 
313.71 MB/s.
Press any key to exit...
If you compare the trace acquired by Process Monitor with the
previous one, you can confirm that cached I/O is a one-copy
operation. The cache manager still copies chunks of memory
between the application-provided buffer and the system cache,
which is mapped directly on the DAX disk. This is confirmed by
the fact that again, no paging I/O is highlighted on the target file.
As a last experiment, you can try to start a DAX copy between
two files that reside on the same DAX-mode volume or that reside
on two different DAX-mode volumes:
Click here to view code image
P:\>fstool /daxcopy q:\test.iso q:\test_copy_2.iso
TFS / ReFS Tool v0.1
Copyright (C) 2018 Andrea Allievi (AaLl86)
Starting DAX copy...
   Source file path: q:\test.iso.
   Target file path: q:\test_copy_2.iso.
   Source Volume: q:\ - File system: NTFS - Is DAX Volume: 
True.
   Target Volume: q:\ - File system: NTFS - Is DAX Volume: 
True.
Great! Both the source and the destination reside on a DAX 
volume.
Performing a full System Speed Copy!
   Source file size: 4.34 GB
Performing file copy... Success!
   Total execution time: 8 Sec.
   Copy Speed: 501.60 MB/Sec