### Table 4: Gain Ratio of Top 10 Features of Influence Graphs
| Feature | Gain Ratio |
|---------|------------|
| 0.16 ± 0.12 | 0.1 ± 0.1 |
| 0.09 ± 0.08 | 0.06 ± 0.06 |
| Avg. Rank | 1.1 ± 0.3 |
| 1.9 ± 0.3 | 3.0 ± 0.0 |
| 4.3 ± 0.64 | 5.0 ± 0.5 |
| 5.7 ± 0.6 | 7.4 ± 0.66 |
| 8.5 ± 1.1 | 11.4 ± 1.1 |

### Challenges in Classifying Download Graphs

The assumptions that the data has low variability and few outliers [25] may not hold for download graphs due to several reasons:

- **Multiple Malware Types in a Two-Class Model**: The subclasses of malicious and benign influence graphs (IGs) have different properties, such as the average distinct file affinity (see Figure 4(e)). This results in high variability or many outliers in the training data. It is also challenging to determine the number of malware subclasses. Additionally, some benign downloaders may exhibit similar properties to malicious ones, further complicating classification.

- **Influence Graph Evolution**: IGs evolve over time. For example, a malicious IG might initially have a diameter of 2, but this feature value could increase as the graph grows.

- **Cross-Machine Behavioral Differences**: Even for similar ages, the IGs of a downloader can vary based on the host environment and user behavior. Some downloaders may include host-specific download instructions [3].

Given these properties, the classes of malicious and benign IGs may not be linearly separable in the feature space, posing a challenge for Support Vector Machine (SVM) classifiers. We also need a scalable and parameter-free classifier, as tuning parameters for diverse data can be difficult. After experimenting with various algorithms, including SVM and k-nearest neighbor (kNN), we found that ensemble classification algorithms, specifically random forest classifiers (RFCs), provide the best fit for our problem. Here are the reasons for choosing RFCs:

- **Bias-Variance Trade-off**: RFCs offer a good bias-variance trade-off, especially for diverse data. An RFC consists of multiple decision trees (DTs), each trained on a subset of features. The final class prediction is made by aggregating the votes from all DTs, reducing overfitting and providing low variance.

- **Parameter-Free**: RFCs do not require extensive parameter tuning. While we need to specify the number of decision trees (Nt) and the number of features per tree (Nf), these parameters have standard selection rules and exhibit monotonicity and diminishing returns.

- **Scalability**: RFCs are faster to train compared to SVM and kNN. Our experiments show that training an RFC is 10-15 times faster than SVM.

### Internal Validation of the Classifier

We evaluate the performance of our RFC classifier in three ways:

1. **10-Fold Cross Validation on Balanced Training Data**:
   - We use Nt = 40 and Nf = log2(# of features) + 1.
   - Increasing Nt and Nf beyond these values yields minimal accuracy improvements but significantly increases training cost.
   - Using the RandomForestClassifier module from Python’s scikit-learn package, we achieve a 96.0% true positive rate (TPR) at a 1.0% false positive rate (FPR).

2. **Feature Evaluation**:
   - We evaluate the performance of different feature combinations using a Receiver Operating Characteristic (ROC) plot.
   - Large performance jumps occur when adding FL and FA features.
   - At a 1% FPR, the corresponding TPRs are 19.9%, 13.4%, 3%, 17.6%, 28.5%, and 96.0%.

3. **Evaluation of False Positives**:
   - We manually investigate misclassified IGs and compare the feature values of true positives (TP), true negatives (TN), and false positives (FP).
   - Features such as the number of distinct files downloaded per URL, density, and distribution of edges show closer values to the TP set.

### Testing the Classifier on the Entire Labeled Data

- Applying the RFC to the entire labeled data (1,433,670 IGs, with 43,668 malicious and 1,390,092 benign) results in a 100% TPR and 2% FPR on the malicious class.
- From the ROC curve, we achieve a 91.8% TPR at a 1% FPR and a 99% G-mean score, indicating high classification accuracy on unbalanced data.

### Early Detection

- We define "early detection" as flagging unknown executables as malicious before their first submission to VirusTotal.
- Using 10-fold cross validation on the balanced labeled dataset, we detect 6,515 executables earlier than VirusTotal, with an average lead time of 9.24 days.
- For executables with a detection rate ≥ 30%, we detect them 25.24 days earlier on average.

### External Validation of the Classifier

- We draw three random samples of approximately 3,000 IGs each from the unlabeled data and query the corresponding file hashes in VirusTotal.
- On average, 41.41% of the binaries in the IGs labeled as malicious are known to be malicious by other AV vendors, while only 0.53% of the binaries in benign IGs are labeled as malicious.
- 78% of the IGs labeled as malicious have at least one internal node detected as malware by AV products, and only 1.58% of the IGs labeled as benign carry a malicious node.

### Summary

Our RFC classifier effectively addresses the challenges posed by the diverse and evolving nature of download graphs. It provides high accuracy, scalability, and early detection capabilities, making it a robust solution for classifying malicious and benign IGs.