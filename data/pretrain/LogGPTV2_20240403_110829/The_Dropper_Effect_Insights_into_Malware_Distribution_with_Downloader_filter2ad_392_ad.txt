.16 ± 0
.12 ± 0
.1 ± 0
.1 ± 0
.09 ± 0
.08 ± 0
.06 ± 0
.06 ± 0
Avg. Rank
1.1 ± .3
1.9 ± .3
3 ± 0
4.3 ± .64
5 ± 0.5
5.7 ± 0.6
7.4 ± .66
8.5 ± 1.1
11.4 ± 1.1
Table 4: Gain ratio of top 10 features of inﬂuence graphs.
with low variability and few outliers [25]. These assumptions may
not hold true for download graphs, for several reasons:
• Fitting multiple malware types in a two-class model: The subcla-
sses of malicious and benign IGs have different properties (e.g.,
the average distinct ﬁle afﬁnity, illustrated in Figure 4(e)). From
the point of view of a classiﬁer trying to label each example as
benign or malicious, this means that the training data has high
variability or many outliers. It is also difﬁcult to establish how
many subclasses of malware exist. Moreover, one subclass of
benign downloaders may exhibit similar properties of that of a
subclass of malicious downloaders (e.g., software components
that update themselves), confusing the classiﬁer even further.
• Inﬂuence graph evolution: Inﬂuence graphs evolve over time.
Depending on when a downloader was dropped on different ma-
chines, the corresponding inﬂuence graphs may be different. For
example, a malicious IG might initially have diameter 2, and the
value of this feature might increase as the graph grows.
• Cross-machine behavioral differences: Even for similar ages, the
inﬂuence graphs of a downloader may depend on the host envi-
ronment and user behavior. In fact, downloaders may also en-
code host-speciﬁc download instructions [3].
Given these properties, the classes of malicious and benign in-
ﬂuence graphs may not be linearly separable in the feature space,
which raises a challenge for Support Vector Machine (SVM) cla-
ssiﬁers. Additionally, we want our classiﬁer to be scalable and
parameter-free, as tuning parameters for such diverse data could be
difﬁcult. After experimenting with several algorithms, including
SVM and k-nearest neighbor (kNN), we found that ensemble clas-
siﬁcation algorithms provide the best ﬁt for our problem. We select
a random forest classiﬁer (RFC) [2], for the following reasons:
• Bias-variance trade-off: RFCs are known to provide a good bias-
variance trade-off, especially if the data is diverse (i.e., noisy
from the point of the view of a learning algorithm). An RFC
consists of a set of decision trees (DT), where each DT is trained
using only a subset of features (randomly chosen). The ﬁnal
class prediction is made by aggregating the votes from all the
individual DTs. Thus, an RFC is less prone to overﬁtting as
compared to a single DT. In other words, the bias of the entire
forest is bounded by the bias of the individual DTs [2]. Moreo-
ver, since the class predication is made by aggregating the votes
from a number of smaller decision trees, RFCs exhibit low vari-
ance as well. Intuitively, we can expect that different DTs will
specialize on the different subclasses of the malicious/benign bi-
naries and champion their cause in the ﬁnal classiﬁcation.
• Parameter-free: RFCs do not require parameter tuning7 and are
robust to outliers. While we need to specify the number of deci-
7In contrast, SVM requires selecting: (a) the kernel, (b) the kernel’s pa-
rameters, (c) a soft margin parameter that encodes the penalty factor for
non-separable points.
1125sion trees (Nt) used and the number of features (Nf ) per deci-
sion tree, these parameters are independent of the nuances of the
data and have standard selection rules8. Moreover, they exhibit
monotonicity and diminishing returns, i.e., increasing them leads
to an increase in accuracy but only to a certain limit.
• Scalability: Random forest classiﬁers are very fast to train, com-
pared to SVM and kNN. Our experiments show that training an
RFC on the same dataset is 10-15 times faster than SVM.
5.4
Internal Validation of the Classiﬁer
In this section we evaluate the performance of our RFC classiﬁer
in three ways: (a) using 10-fold cross validation on the balanced
training dataset, (b) using all the labeled data as a test set, and (c)
computing the detection lead time, compared to the anti-virus pro-
ducts employed by VirusTotal.
10-fold Cross Validation. We choose Nt = 40 and Nf =
log2(# of features) + 1, for our experiments. We observe that in-
creasing Nt and Nf beyond these numbers results in very small
improvement in accuracy; however, the training cost goes up sig-
niﬁcantly. We use the RandomForestClassifier module from
Python’s scikit-learn package for the experiment. We run a
10-fold cross validation on the balanced set. We use all the features
described on Table 3 for training the classiﬁer. We report the clas-
siﬁcation result at 1.0% FP rate, which achieves 96.0% TP rate9.
These results with a default scikit-learn threshold are listed
in the ﬁrst row of Table 5, labeled “All Features”.
Feature Evaluation. While in Table 4 lists the most useful features
for our classiﬁer, we also want to know how using different feature
combinations would affect the performance of the classiﬁer. Star-
ting from only using FI features from Table 3, we combine other
feature categories one by one (FL, FD, FU, then FA) and evaluate
how the classiﬁcation performance increases. The result is shown
as a Receiver Operating Characteristic (ROC)10 plot in Figure 5,
where the X axis is the false positive rate and the Y axis is the
true positive rate. ROC plots show the TP/FP trade-off: the top-
left corner corresponds to a perfect classiﬁer, which never makes
mistakes. The curves for the different feature combinations are ob-
tained by varying internal thresholds of the random forest classiﬁer.
We observe large jumps in performance at two points, ﬁrst is when
we add the FL features and them when we add the FA features.
We believe the classiﬁer is capturing the insights we discussed in
Section 4.3, such as the slow growth rate and speciﬁc numbers of
distinct downloaders accessing a domain for the malicious inﬂu-
ence graphs. At FP rate 1%, the corresponding TP rates are 19.9%,
13.4%, 3%, 17.6%, 28.5% and 96.0%. Interestingly, only using FI
rate was performing better than adding FL and FS at this point. This
is consistent with our observation that almost 12% of the malicious
IGs have diameter 3 and more and 84% of the IGs are malicious at
diameter 3 and beyond (Section 4.3). At FP rate 3%, the numbers
for TP rate are 24.7%, 33.5%, 42.1%, 59.6%, 98.7%. At 10%, TP
rates are 41.7%, 64.5%, 71.2%, 84.8%, and 99.8%.
Evaluation of the False Positives. We perform a manual inves-
tigation of the misclassiﬁed IGs. We take the average of all the
feature values, for the TP, TN, FP sets and compared the difference
between  and , to see which feature is closer
to the TP set. The number of distinct ﬁles downloaded by a sin-
gle URL, density, distribution of the number of incoming/outgoing
edges, prevalence of the executables, and the children time spread
8Nt is typically data size independent and few applications go beyond 100
trees. For Nf , standard value used is Log(Total number of features) + 1
F P +T N , F1 = 2 ∗ P recision∗Recall
9
T Prate = T P
P recision+Recall
http://en.wikipedia.org/wiki/Receiver_operating_characteristic
T P +F N , F Prate = F P
10
Figure 5: ROC curve for different feature groups
Algorithms
All Features
FI+FL+FD+FU
FI+FL+FD
FI+FL
Only FI
TP rate
0.980
0.868
0.831
0.811
0.602
FP rate
0.020
0.124
0.184
0.211
0.261
F-score
0.980
0.870
0.823
0.801
0.644
ROC Area
0.998
0.939
0.902
0.876
0.752
Table 5: Classiﬁer performance on malicious class.
features are showing closer value to the TP set. The IGs in the
FP set have a smaller number of ﬁles downloaded per URL, den-
ser structure, smaller number of outgoing edges, lower prevalence,
and faster dropping rate. Most prevalent downloaders in this TP set
turned out to be P2P downloaders and download managers.
Testing the Classiﬁer on the Entire Labeled Data. We apply the
RFC trained on the balanced labeled data to the entire labeled data,
which is skewed toward the benign class. There are 1,433,670 IGs
in this set, where 43,668 are malicious and 1,390,092 are benign.
The classiﬁcation result showed 100% TP rate and 2% FP rate on
the malicious class. From the ROC curve, we get 91.8% TP rate at
FP rate 1%. We also got 99% G-mean11score [12] which is a stan-
dard way of measuring the classiﬁcation accuracy on unbalanced
data where both classes are important [26].
Early Detection. We also evaluate how early we can detect mali-
cious executables that are previously unknown. We deﬁne “early
detection” as “we are able to ﬂag unknown executables as mali-
cious before their ﬁrst submission to VirusTotal”. As discussed in
Section 4.1, we approximate the date when malware samples be-
comes known to the security community using the VirusTotal ﬁrst-
submission time. We estimate our detection time in three ways:
(a) an executable is detected at its earliest timestamp in the TP set
(Lower bound), (b) an executable is detected at the timestamp when
the last node was added to the newest inﬂuence graph it resides as
a node (Upper bound) , and (c) the average timestamp of the exe-
cutables in the TP set (Average).
We apply random forest with 10 fold cross validation on our
balanced labelled data set using all 58 features. The outcome
of 10 fold cross validation is TP=42,683, FP=822, FN=985 and
TN=43,724, in terms of the number of inﬂuence graphs. For dis-
tinct executables in the TP set excluding the ones in the FN set, we
compared the ﬁrst seen timestamps in VirusTotal and our lower-
bound/upper-bound/average detection timestamps. Among 31,104
distinct executables that are in TP set but not in FN set, 20,452 ﬁles
had scanning records in VirusTotal. Among them, 17,462 executa-
bles had at least one detection in VirusTotal (rmal > 0), and 10,323
executables had detection rates over 30% (rmal ≥ 30).
11G − mean =
T P × T N
√
All featuresFI+FL+FD+FUFI+FL+FDFI+FLFITrue positive rate0.10.20.30.40.50.60.70.80.91.0False positive rate0.10.20.30.40.50.60.70.80.91.01126rmal > 0
Distinct Executables
Early detection avg.
rmal ≥ 30
Distinct Executables
Early detection avg.
3,344 (19.2%)
-23.73
Lower bound Upper bound Average
6,515 (37.3%)
20.91
Lower bound Upper bound Average
3,939 (38.2%)
35.86
2,041 (19.8%)
-7.69
4,871 (27.9%)
9.24
3,002 (29.1%)
25.24
Table 6: Early detection.
Figure 6: Detection rate vs. Early detection ratio / Early detec-
tion avg. (days)
Table 6 lists our results. For rmal > 0, the time difference be-
tween the VirusTotal ﬁrst seen timestamp and our lower bound de-
tection timestamp is 20.91 days on average, and we are able to
detect 6,515 executables earlier than VirusTotal. Our upper bound
is 23.73 after VirusTotal, with 3,344 executables detected early. On
average, we detect malware 9.24 days before the ﬁrst VirusTotal
detection. Interestingly, for rmal ≥ 30, we detect malware on ave-
rage 25.24 days earlier than VirusTotal. We further investigate this
trend in Figure 6, where we plot the portion of executables that we
are able to detect early, in our average detection scenario, against
the VirusTotal detection rate rmal. Up to rmal = 80%, our early
detection lead time increases with rmal. This suggests that exe-
cutables that are more likely to be malicious present stronger indi-
cations of maliciousness in their downloader graphs, allowing our
classiﬁer to detect them earlier than current anti-virus products.
5.5 External Validation of the Classiﬁer
In this section, we evaluate the performance of our RFC classi-
ﬁer by drawing three random samples from the unlabeled IGs and
querying the corresponding ﬁle hashes in VirusTotal. Out of the
580,210 unlabeled IGs, our classiﬁer identiﬁes 116,787 as mali-
cious.
As discussed in Section 3.2, we were unable to query VirusTotal
for all the ﬁle hashes, which leaves many of the leaf nodes in our
graphs unlabeled. We draw three random samples, of approxima-
tely 3000 inﬂuence graphs each, from the set of unlabeled graphs
and try to estimate the accuracy of our predictions by presenting
the results of each set. We consider that an IG labeled as malicious
is misclassiﬁed if none of the AV products in VirusTotal detect it as
malicious. We consider that an IG labeled as benign is misclassi-
ﬁed if its nodes were detected by more than 30% of the AV vendors,
to account for the fact that AV products may also produce false po-
sitives in gray-area situations, such as benign executables that are
sometimes involved in malware delivery.
Table 7 shows our results. On average 41.41% of the binaries
that construct the IGs are known to be malicious also by other AV
vendors, while only 0.53% of the binaries in benign IGs were labe-
led as malicious. Moreover, on average 78% of the IGs labeled as
malicious have at least one internal node that AV products detect
as malware, and only 1.58% of the IGs labeled as benign carry a
malicious node. As many malware samples are discovered late by
the security community (see Section 4.1), we expect that the num-
ber of samples considered malicious by VirusTotal will grow in the
future.
#Run
Predictions
#IGs
#1
#2
#3
Malicious
Benign
Malicious
Benign
Malicious
Benign
582
2456
590
2454
592
2495
by
#IGmal
VT
444 (76%)
43 (1.7%)
471 (80%)
30 (1.2%)
466 (79%)
44 (1.7%)
#Binarymal
VT