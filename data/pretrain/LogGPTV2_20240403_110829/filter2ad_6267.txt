title:Modeling Native Software Components as Virtual Network Functions
author:Mario Baldi and
Roberto Bonafiglia and
Fulvio Risso and
Amedeo Sapio
Modeling Native Software Components
as Virtual Network Functions
Mario Baldi, Roberto Bonaﬁglia, Fulvio Risso, Amedeo Sapio
Department of Control and Computer Engineering, Politecnico di Torino, Italy
{mario.baldi, roberto.bonaﬁglia, fulvio.risso, amedeo.sapio}@polito.it
ABSTRACT
Virtual Network Functions (VNFs) are often realized
using virtual machines (VMs) because they provide an
isolated environment compatible with classical cloud
computing technologies. However, VMs are demanding
in terms of required resources (CPU and memory) and
therefore not suitable for low-cost devices like residen-
tial gateways. Such equipment often runs a Linux-based
operating system that includes by default a (large) num-
ber of common network functions, which can provide
some of the services otherwise oﬀered by simple VNFs,
but with reduced overhead.
In this paper those na-
tive software components are made available through a
Network Function Virtualization (NFV) platform, thus
making their use transparent from the VNF developer
point of view.
CCS Concepts
•Networks → Cloud computing; Middle boxes / net-
work appliances;
Keywords
Network Functions Virtualization; Virtual Network Func-
tions; Service Orchestration
1.
INTRODUCTION
While Network Service Providers (NSPs) could ben-
eﬁt from a ﬂexible infrastructure that can rapidly and
eﬃciently provide dedicated, on-demand network ser-
vices, so far they have not been able to leverage it due to
the complexity of deploying middleboxes in the network.
Network Functions Virtualization (NFV) has the poten-
tial to overcome this by exploiting virtualization tech-
Permission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for proﬁt or commercial advantage and that copies bear this notice
and the full citation on the ﬁrst page. Copyrights for components of this work
owned by others than ACM must be honored. Abstracting with credit is per-
mitted. To copy otherwise, or republish, to post on servers or to redistribute to
lists, requires prior speciﬁc permission and/or a fee. Request permissions from
permissions@acm.org.
SIGCOMM ’16, August 22–26, 2016, Florianopolis, Brazil
c(cid:13) 2016 ACM. ISBN 978-1-4503-4193-6/16/08. . . $15.00
DOI: http://dx.doi.org/10.1145/2934872.2959069
niques, typical of cloud computing, to instantiate Vir-
tualized Network Functions (VNFs) in compute nodes
with unprecedented agility. However, current NFV im-
plementations are designed for (centralized) data cen-
ters, while NSPs leverage a distributed infrastructure
consisting of heterogeneous devices. More speciﬁcally,
it would be particularly beneﬁcial to oﬀer NFV capa-
bility in Customer Premise Equipment (CPE), which is
particularly challenging because it is usually based on
low-cost hardware. On the other hand, CPE operating
systems are often some Linux ﬂavor, which embeds a
large number of software-based “native” network func-
tions, such as ﬁrewall and NAT (e.g., iptables), virtual
switch (e.g., linuxbridge), and more.
This paper proposes a solution to integrate such func-
tions in an existing NFV infrastructure so that while
resource-hungry VNFs are run in the NSP data center,
simpler ones are run in the CPE, possibly as Native
Network Functions (NNFs). Our solution facilitates
the provision of services that require Network Functions
(NFs) close to the end user (e.g., IPsec terminator), en-
abling the possibility to execute both VNFs and NNFs,
which combines the beneﬁts of ﬂexibility and low exe-
cution overhead. In order to support NNF integration,
the compute controller in an NFV server is extended
with an additional plugin that manages all the NNFs
available on the node, along with standard VNFs.
2. ARCHITECTURE
Our proposal is based on the compute node presented
in [1], which follows closely the NFV architecture. As
shown in Figure 1, a local orchestrator receives a Net-
work Functions Forwarding Graph (NF-FG) and instan-
tiates the required VNFs. For each NF-FG a new soft-
ware switch, called Logical Switch Instance (LSI), is cre-
ated in order to steer traﬃc among the corresponding
VNFs in the right order, while a base LSI is in charge
of classifying the traﬃc received by the node and deliv-
ering it to the proper NF-FG-speciﬁc LSI. VNFs are in-
stantiated and managed by a compute manager through
ad-hoc drivers matching the speciﬁc VNF support tech-
nology (e.g., VM, Docker, DPDK process), while each
LSI is managed by its own OpenFlow controller that dy-
namically inserts the proper rules in ﬂow table(s). All
605
Table 1: Results with IPSec client VNFs
Image
size
Platform
Through.
RAM
KVM/QEMU
Docker
Native NF
796 Mbps
1095 Mbps
1094 Mbps
390.6 MB 522 MB
240 MB
24.2 MB
19.4 MB
5 MB
the NNF with a predeﬁned conﬁguration script. Sup-
port for a dynamic conﬁguration mechanism able to
translate a generic NF conﬁguration, provided by the
orchestrator, in commands appropriate to the speciﬁc
NNF is not in the scope of this initial implementation
and will be targeted by future work.
3. VALIDATION
In order to show the beneﬁts oﬀered by NNFs, we
present the performance achieved in a simple use case
in which a customer activates an IPSec endpoint VNF
on his domestic CPE. This NF has been selected be-
cause it should be deployed the closest to the user in
order to provide him with highest level of security, e.g.,
on the user home router. We compare the cost of run-
ning the Strongswan IPSec endpoint, conﬁgured to use
the ESP protocol in tunnel mode, as a NNF, a Docker
container and a VM using KVM/QEMU as hypervi-
sor. The Strongswan implementation leverages kernel
processing to handle packets faster, an expedient (very
common among NFs) that highlights the limits of VNFs.
The maximum throughput that can be obtained by the
three NF ﬂavors has been measured using iPerf and is
presented in Table 1, together with the amount of RAM
allocated at runtime and the size of the NF image.
Results show that the VM has worst performance,
which is due to the additional virtualization layer and
to the IPsec functionalities executing in user space (i.e.,
in the process, within the hypervisor, running the VM).
The Docker and Native implementations have compa-
rable performance, since both process packets in the
host kernel space. However, Docker requires additional
space for storing the Docker image, as well as additional
libraries in the Linux operating system, which makes
this technology not suitable for resource-constrained de-
vices.
Acknowledgment
This work was conducted within the FP7 UNIFY and
SECURED projects, which are partially funded by the
Commission of the European Union. We thank Sergio
Nuccio for his contribution to an early prototype.
4. REFERENCES
[1] I. Cerrato, T. Jungel, A. Palesandro, F. Risso,
M. Sune, and H. Woesner. User-speciﬁc network
service functions in an sdn-enabled network node.
In Software Deﬁned Networks (EWSDN), 2014
Third European Workshop on, pages 135–136.
IEEE, 2014.
Figure 1: Compute node architecture.
the above drivers must implement a speciﬁc abstraction
deﬁned by the local orchestrator, which enables multiple
drivers to coexist, hence implementing complex services
that include VNFs created with diﬀerent technologies
(e.g., VMs and Docker).
Proﬁtably integrating NNFs in an NFV infrastruc-
ture requires to identify signiﬁcant diﬀerences with tra-
ditional VNFs and translate them into a set of con-
straints for the local orchestrator when evaluating whether
to use NNFs or traditional VNFs. A major one of such
diﬀerences, is that some NNFs do not allow to concur-
rently spin up multiple instances. Such NNFs must be
“sharable” to have multiple service graphs traversing the
same NF. A NNF is “sharable” only if (i ) it can use an
ad-hoc marking mechanism to distinguish between traf-
ﬁc belonging to diﬀerent service graphs, hence emulat-
ing the execution of diﬀerent NF instances, and (ii ) the
NNF can create multiple internal paths that are needed
to process the above multiple traﬃc streams in isolation.
Moreover, an additional adaptation layer is required to
cope with the fact that NNFs may be designed to re-
ceive traﬃc from a single network interface. Such layer
attaches the NNF to one port of the switch and conﬁg-
ures it to receive the traﬃc from multiple service graphs,
appropriately marked to make it distinguishable.
When a NNF should be used, the compute manager
selects a NNF driver developed as part of this work1.
This NNF driver implements the same abstraction de-
ﬁned for the other compute drivers and dynamically ac-
tivates the plugin associated to the selected NNF, which
is implemented as a collection of bash scripts that con-
trol the basic lifecycle (create, update, etc.) of the NF.
For each NF in a NF-FG, the orchestrator decides
whether to deploy it as VNF or NNF based on its knowl-
edge of the node capability set, the available NNFs and
their characteristics (e.g., whether they are sharable),
and their status (e.g., already used in another chain).
The NNF driver starts the NNF in a new network names-
pace, to provide a basic form of isolation, and conﬁgures
1Source
netgroup-polito/un-orchestrator/.
available
code
at
https://github.com/
606
NFV Compute NodeLocalOrchestratorLSI -0 Virtual switchNetwork Functions Forwarding Graph (NF-FG)Compute managerNativedriverlibvirtManagement driversxDPddriverOvSdriverVNF4DPDKdriverDockerdriverVNF2LSI -graph1 VNF3VNF1REST serverVirtual Link amongLSIsNetwork functionport(s)(betweenan LSI and a VNF)OpenFlowconnectionCompute controlNetwork controlResource managerVMdriverLSI -graphNNNFERFSdriverNetwork managerNodedescription, capabilitiesand resourcesVNF resolverVNF schedulerVNFrepositoryTrafficSteeringmngrLSI #1LSI #N