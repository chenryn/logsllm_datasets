title:Deep Learning with Differential Privacy
author:Mart&apos;ın Abadi and
Andy Chu and
Ian J. Goodfellow and
H. Brendan McMahan and
Ilya Mironov and
Kunal Talwar and
Li Zhang
Deep Learning with Differential Privacy
Martín Abadi∗
H. Brendan McMahan∗
Andy Chu∗
Ilya Mironov∗
Li Zhang∗
Ian Goodfellow†
Kunal Talwar∗
ABSTRACT
Machine learning techniques based on neural networks are
achieving remarkable results in a wide variety of domains.
Often, the training of models requires large, representative
datasets, which may be crowdsourced and contain sensitive
information. The models should not expose private informa-
tion in these datasets. Addressing this goal, we develop new
algorithmic techniques for learning and a reﬁned analysis of
privacy costs within the framework of diﬀerential privacy.
Our implementation and experiments demonstrate that we
can train deep neural networks with non-convex objectives,
under a modest privacy budget, and at a manageable cost in
software complexity, training eﬃciency, and model quality.
1.
INTRODUCTION
Recent progress in neural networks has led to impressive
successes in a wide range of applications, including image
classiﬁcation,
language representation, move selection for
Go, and many more (e.g., [55, 30, 57, 40, 15]). These ad-
vances are enabled, in part, by the availability of large and
representative datasets for training neural networks. These
datasets are often crowdsourced, and may contain sensitive
information. Their use requires techniques that meet the
demands of the applications while oﬀering principled and
rigorous privacy guarantees.
In this paper, we combine state-of-the-art machine learn-
ing methods with advanced privacy-preserving mechanisms,
training neural networks within a modest (“single-digit”) pri-
vacy budget. We treat models with non-convex objectives,
several layers, and tens of thousands to millions of param-
eters. (In contrast, previous work obtains strong results on
convex models with smaller numbers of parameters, or treats
complex neural networks but with a large privacy loss.) For
this purpose, we develop new algorithmic techniques, a re-
ﬁned analysis of privacy costs within the framework of dif-
ferential privacy, and careful implementation strategies:
∗Google.
†OpenAI. Work done while at Google.
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for third-party components of this work must be honored.
CCS’16 October 24-28, 2016, Vienna, Austria
c(cid:13) 2016 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-4139-4/16/10.
DOI: http://dx.doi.org/10.1145/2976749.2978318
1. We demonstrate that, by tracking detailed information
(higher moments) of the privacy loss, we can obtain
much tighter estimates on the overall privacy loss, both
asymptotically and empirically.
2. We improve the computational eﬃciency of diﬀeren-
tially private training by introducing new techniques.
These techniques include eﬃcient algorithms for com-
puting gradients for individual training examples, sub-
dividing tasks into smaller batches to reduce memory
footprint, and applying diﬀerentially private principal
projection at the input layer.
3. We build on the machine learning framework Tensor-
Flow [3] for training models with diﬀerential privacy.
We evaluate our approach on two standard image clas-
siﬁcation tasks, MNIST and CIFAR-10. We chose
these two tasks because they are based on public data-
sets and have a long record of serving as benchmarks
in machine learning. Our experience indicates that
privacy protection for deep neural networks can be
achieved at a modest cost in software complexity, train-
ing eﬃciency, and model quality.
Machine learning systems often comprise elements that
contribute to protecting their training data. In particular,
regularization techniques, which aim to avoid overﬁtting to
the examples used for training, may hide details of those
examples. On the other hand, explaining the internal rep-
resentations in deep neural networks is notoriously diﬃcult,
and their large capacity entails that these representations
may potentially encode ﬁne details of at least some of the
training data. In some cases, a determined adversary may
be able to extract parts of the training data. For example,
Fredrikson et al. demonstrated a model-inversion attack that
recovers images from a facial recognition system [26].
While the model-inversion attack requires only “black-
box” access to a trained model (that is, interaction with the
model via inputs and outputs), we consider adversaries with
additional capabilities, much like Shokri and Shmatikov [52].
Our approach oﬀers protection against a strong adversary
with full knowledge of the training mechanism and access
to the model’s parameters. This protection is attractive,
in particular, for applications of machine learning on mobile
phones, tablets, and other devices. Storing models on-device
enables power-eﬃcient, low-latency inference, and may con-
tribute to privacy since inference does not require commu-
nicating user data to a central server; on the other hand,
we must assume that the model parameters themselves may
be exposed to hostile inspection. Furthermore, when we are
308concerned with preserving the privacy of one record in the
training data, we allow for the possibility that the adversary
controls some or even all of the rest of the training data. In
practice, this possibility cannot always be excluded, for ex-
ample when the data is crowdsourced.
The next section reviews background on deep learning
and on diﬀerential privacy. Sections 3 and 4 explain our
approach and implementation. Section 5 describes our ex-
perimental results. Section 6 discusses related work, and
Section 7 concludes. Deferred proofs appear in the full ver-
sion of the paper [4].
2. BACKGROUND
In this section we brieﬂy recall the deﬁnition of diﬀerential
privacy, introduce the Gaussian mechanism and composition
theorems, and overview basic principles of deep learning.
2.1 Differential Privacy
Diﬀerential privacy [21, 18, 22] constitutes a strong stan-
dard for privacy guarantees for algorithms on aggregate data-
bases. It is deﬁned in terms of the application-speciﬁc con-
cept of adjacent databases. In our experiments, for instance,
each training dataset is a set of image-label pairs; we say
that two of these sets are adjacent if they diﬀer in a single
entry, that is, if one image-label pair is present in one set
and absent in the other.
Deﬁnition 1. A randomized mechanism M : D → R with
domain D and range R satisﬁes (ε, δ)-diﬀerential privacy if
for any two adjacent inputs d, d(cid:48) ∈ D and for any subset of
outputs S ⊆ R it holds that
Pr[M(d) ∈ S] ≤ eε Pr[M(d
(cid:48)
) ∈ S] + δ.
The original deﬁnition of ε-diﬀerential privacy does not in-
clude the additive term δ. We use the variant introduced by
Dwork et al. [19], which allows for the possibility that plain
ε-diﬀerential privacy is broken with probability δ (which is
preferably smaller than 1/|d|).
Diﬀerential privacy has several properties that make it
particularly useful in applications such as ours: composabil-
ity, group privacy, and robustness to auxiliary information.
Composability enables modular design of mechanisms: if all
the components of a mechanism are diﬀerentially private,
then so is their composition. Group privacy implies graceful
degradation of privacy guarantees if datasets contain cor-
related inputs, such as the ones contributed by the same
individual. Robustness to auxiliary information means that
privacy guarantees are not aﬀected by any side information
available to the adversary.
A common paradigm for approximating a deterministic
real-valued function f : D → R with a diﬀerentially private
mechanism is via additive noise calibrated to f ’s sensitivity
Sf , which is deﬁned as the maximum of the absolute distance
|f (d) − f (d(cid:48))| where d and d(cid:48) are adjacent inputs.
(The
restriction to a real-valued function is intended to simplify
this review, but is not essential.) For instance, the Gaussian
noise mechanism is deﬁned by
M(d) ∆= f (d) + N (0, S2
where N (0, S2
f · σ2) is the normal (Gaussian) distribution
with mean 0 and standard deviation Sf σ. A single applica-
tion of the Gaussian mechanism to function f of sensitivity
f · σ2),
Sf satisﬁes (ε, δ)-diﬀerential privacy if δ ≥ 4
and ε  C, it gets
scaled down to be of norm C. We remark that gradient clip-