title:GALA: Greedy ComputAtion for Linear Algebra in Privacy-Preserved
Neural Networks
author:Qiao Zhang and
Chunsheng Xin and
Hongyi Wu
GALA: Greedy ComputAtion for Linear Algebra in
Privacy-Preserved Neural Networks
Qiao Zhang, Chunsheng Xin, and Hongyi Wu
Old Dominion University, Norfolk, VA 23529, USA
Email: PI:EMAIL, PI:EMAIL, PI:EMAIL
Abstract—Machine Learning as a Service (MLaaS) is enabling
a wide range of smart applications on end devices. However,
privacy still remains a fundamental challenge. The schemes that
exploit Homomorphic Encryption (HE)-based linear computa-
tions and Garbled Circuit (GC)-based nonlinear computations
have demonstrated superior performance to enable privacy-
preserved MLaaS. Nevertheless, there is still a signiﬁcant gap
in the computation speed. Our investigation has found that the
HE-based linear computation dominates the total computation
time for state-of-the-art deep neural networks. Furthermore,
the most time-consuming component of the HE-based linear
computation is a series of Permutation (Perm) operations that are
imperative for dot product and convolution in privacy-preserved
MLaaS. This work focuses on a deep optimization of the HE-
based linear computations to minimize the Perm operations, thus
substantially reducing the overall computation time. To this end,
we propose GALA: Greedy computAtion for Linear Algebra in
privacy-preserved neural networks, which views the HE-based
linear computation as a series of Homomorphic Add, Mult and
Perm operations and chooses the least expensive operation in
each linear computation step to reduce the overall cost. GALA
makes the following contributions: (1) It introduces a row-wise
weight matrix encoding and combines the share generation that
is needed for the GC-based nonlinear computation, to reduce
the Perm operations for the dot product; (2) It designs a ﬁrst-
Add-second-Perm approach (named kernel grouping) to reduce
Perm operations for convolution. As such, GALA efﬁciently
reduces the cost for the HE-based linear computation, which is a
critical building block in almost all of the recent frameworks for
privacy-preserved neural networks, including GAZELLE (Usenix
Security’18), DELPHI (Usenix Security’20), and CrypTFlow2
(CCS’20). With its deep optimization of the HE-based linear
computation, GALA can be a plug-and-play module integrated
into these systems to further boost their efﬁciency. Our experi-
ments show that it achieves a signiﬁcant speedup up to 700× for
the dot product and 14× for the convolution computation under
different data dimensions. Meanwhile, GALA demonstrates an
encouraging runtime boost by 2.5×, 2.7×, 3.2×, 8.3×, 7.7×,
and 7.5× over GAZELLE and 6.5×, 6×, 5.7×, 4.5×, 4.2×, and
4.1× over CrypTFlow2, on AlexNet, VGG, ResNet-18, ResNet-50,
ResNet-101, and ResNet-152, respectively.
I.
INTRODUCTION
Deep Learning (DL) is becoming prevalent and pervasive,
e.g., for pattern recognition [42], medical diagnosis [22],
speech recognition [20] and credit-risk assessment [24]. In
Network and Distributed Systems Security (NDSS) Symposium 2021
21-25  February  2021, Virtual  
ISBN  1-891562-66-5
https://dx.doi.org/10.14722/ndss.2021.24351
www.ndss-symposium.org
particular, Convolutional Neural Network (CNN) has demon-
strated superior performance in computer vision such as image
classiﬁcation [40], [62] and facial recognition [58]. Since
designing and training a deep neural network model requires
intensive resource and DL talents, cloud providers begin to
offer Machine Learning as a Service (MLaaS) [70], where a
proprietary DL model is trained and hosted on a cloud. Clients
can utilize the service by simply sending queries (inference) to
the cloud and receiving results through a web portal. While this
emerging cloud service is embraced as an important tool for
efﬁciency and productivity, the interaction between clients and
cloud servers leads to new vulnerabilities. This work focuses
on the development of privacy-preserved and computationally
efﬁcient MLaaS.
Although communication can be readily secured from end
to end, privacy still remains a fundamental challenge. On the
one hand, the clients must submit their data to the cloud
for inference, but they want the data privacy well protected,
preventing curious cloud provider or attacker with access
to the cloud from mining valuable information. In many
domains such as health care [49] and ﬁnance [63], data are
extremely sensitive. For example, when patients transmit their
physiological data to the server for medical diagnosis, they
do not want anyone (including the cloud provider) to see it.
Regulations such as Health Insurance Portability and Account-
ability Act (HIPAA) [8] and the recent General Data Protection
Regulation (GDPR) in Europe [25] have been in place to
impose restrictions on sharing sensitive user information. On
the other hand, cloud providers do not want users to be able
to extract their proprietary model that has been trained with
signiﬁcant resource and efforts [66]. Furthermore, the trained
model contains private information about the training data set
and can be exploited by malicious users [61], [64], [69]. To this
end, there is an urgent need to develop effective and efﬁcient
schemes to ensure that, in MLaaS, a cloud server does not
have access to users’ data and a user cannot learn the server’s
model.
A series of efforts have been made to enable privacy-
preserved MLaaS, by leveraging cryptographic techniques as
summarized below. The ﬁrst is the Homomorphic Encryption
(HE)-Based Approaches. For example, in CryptoNets [27],
Faster CryptoNets [19] and CryptoDL [34], the client encrypts
data using HE and sends the encrypted data to the server. The
server performs polynomial computations (e.g., addition and
multiplication) over encrypted data to calculate an encrypted
inference result. The client ﬁnally obtains the inference out-
come after decryption. E2DM [37] adopts a more efﬁcient HE
(i.e., packed HE [14]) which packs multiple messages into one
ciphertext and thus improves the computation efﬁciency. The
second approaches is based on Garbled Circuit (GC) [72].
DeepSecure [57] and XONN [55] binarize the computations
in neural networks and employ GC to obliviously obtain
the prediction without leaking sensitive client data. The third
approach exploits Secret Sharing (SS). SS is used in [68] and
[67] to split the client data into shares. The server only owns
one share of the data. The computations are completed by
interactive share exchanges. In addition, Differential Privacy
(DP) [60], [7], [53] and Secure Enclave (SE) [45], [51], [10],
[75] are also explored to protect data security and privacy in
neural networks. In order to deal with different properties of
linearity (weighted sum and convolution functions) and non-
linearity (activation and pooling functions) in neural network
computations, several efforts have been made to orchestrate
multiple cryptographic techniques to achieve better perfor-
mance [74], [43], [38], [48], [56], [44], [76], [18], [73], [47],
[71], [16], [12], [41], [54], [46]. Among them, the schemes
with HE-based linear computations and GC-based nonlinear
computations (called the HE-GC neural network framework
hereafter) demonstrate superior performance [43], [38], [44],
[46]. Speciﬁcally, the GAZELLE framework [38] represents
the state-of-the-art design for the HE-based linear computation
and achieves a speedup of three orders of magnitude than the
classic CryptoNets inference system [27].
Despite the rapid improvement, there is still a signiﬁcant
gap in computation speed, rendering the existing schemes
infeasible for practical applications. For example, the time
constraints in many real-time applications (such as speech
recognition) are within a few seconds [2], [4]. In contrast,
our benchmark has shown that GAZELLE takes 43 seconds
and 659 seconds to run the well-known deep neural networks
ResNet-18 and ResNet-152 [32] on an Intel i7-8700 3.2GHz
CPU (see detailed experimental settings in Sec. IV), which
renders it impractical in real-world applications.
This performance gap motivates us to further improve
the efﬁciency of the HE-GC neural network frameworks. In
deep neural network, both the fully-connected and convo-
lutional layers are based on the linear computation, while
the activation functions perform nonlinear computation. The
former dominates the total computation time in state-of-the-
art deep neural networks. For example, the runtime of the
nonlinear computation in GAZELLE is merely 2.3%, 1.8%,
1.7%, 1.5%, 1.6%, and 2%, respectively, on AlexNet [40],
VGG [62], ResNet-18 [32], ResNet-50 [32], ResNet-101 [32],
and ResNet-152 [32]. The nonlinear cost
in the original
plaintext models is even lower (averaged 1.7%). This indicates
a great potential
to speed up the overall system through
optimizing linear computations. Although a few recent ap-
proaches, e.g., DELPHI [46] and CrypTFlow2 [54], perform
better than GAZELLE in terms of the overall system runtime,
they all inherit the HE-based linear computation in GAZELLE.
This work contributes a solid optimization on the HE-based
linear computation (i.e., dot product and convolution), which
can be integrated into those systems (including GAZELLE,
DELPHI and CrypTFlow2) to further improve their overall
system performance. The HE-based computation consists of
three basic operations: Homomorphic Addition (Add), Mul-
tiplication (Mult), and Permutation (Perm). Our investigation
has shown that the most time-consuming part of the HE-based
computation is a series of Perm operations that are impera-
tive to enable dot product and convolution. Our experiments
show that Perm is 56 times slower than Add and 34 times
slower than Mult. As shown in Table I, in the dot product
by multiplying a 2×2048 matrix with a length-2048 vector,
the cost in GAZELLE is dominated by Perm, which takes
about 98% of the computation time. This observation motivates
the proposed linear optimization, which aims to minimize
the Perm operations, thus substantially reducing the overall
computation time. With less Perm operations, the proposed
approach demonstrates 10× speedup in the above matrix-
vector computation.
TABLE I.
COST OF MATRIX-VECTOR MULTIPLICATION (TIME IN
MILLIONSECOND).
Method
Total (ms)
GAZELLE
Proposed
2
0.2
Perm
time
1.96
0.17
#
11
1
Mult
time
0.01
0.01
#
2
2
Add
time
0.037
0.003
#
11
1
This signiﬁcant speedup lies in a simple and efﬁcient idea
to choose the least expensive operation in each linear compu-
tation step to reduce the overall cost. We name the proposed
approach GALA: Greedy computAtion for Linear Algebra in
privacy-preserved neural networks. We view the HE-based
linear computation as a series of Homomorphic Add, Mult and
Perm operations. The two inputs are the encrypted vector (or
channels) from the client and the plaintext weight matrix (or
kernel) from the server. The output is the encrypted dot product
(or convolution). The objective in each step is to choose the
most efﬁcient operations in the descending priorities of Add,
Mult and Perm. To this end, we (1) design a row-wise weight
matrix encoding with combined share generation1 (i.e., a row-
encoding-share-RaS (Rotated and Sum) approach) to reduce
the number of Perm operations in dot product by log2
n
no
where n is the number of slots in a ciphertext and no is the
output dimension of dot product, and (2) propose a ﬁrst-Add-
second-Perm approach (named kernel grouping) to reduce the
number of Perm operations of convolution by a factor of ci
cn
where ci and cn are respectively the number of channels in
input data and the number of channels that can be packed in
a ciphertext. n is always greater than and can be up to 8192
times of no depending on the dimension of dataset [5] and HE
implementation [59].
At the same time, ci
cn
is at least one and can be up to
256 for state-of-the-art neural network architectures such as
ResNets [32] where the large channels, i.e., 1024 and 2048,
and small kernels size, i.e., 1×1 and 3×3, are adopted. The
larger input data from users will result in smaller cn, which
accordingly contributes to higher speedup especially in the
state-of-the-art CNNs. As such, GALA efﬁciently boosts the
performance of HE-based linear computation, which is a crit-
ical building block in almost all of the recent frameworks for
privacy-preserved neural networks, e.g., GAZELLE, DELPHI,
and CrypTFlow2. Furthermore, GALA’s deep optimization
of the HE-based linear computation can be integrated as a
plug-and-play module into these systems to further improve
their overall efﬁciency. For example, GALA can serve as
a computing module in the privacy-preserved DL platforms,
MP2ML [12] and CrypTFlow [41], which are compatible with
1The resultant linear output will be shared between server and client as the
input of GC-based nonlinear computation.
2
the user-friendly TensorFlow [6] DL framework. Our experi-
ments show that GALA achieves a signiﬁcant speedup up to
700× for the dot product and 14× for the convolution com-
putation under various data dimensions. Meanwhile, GALA
demonstrates an encouraging runtime boost by 2.5×, 2.7×,
3.2×, 8.3×, 7.7×, and 7.5× over GAZELLE and 6.5×, 6×,
5.7×, 4.5×, 4.2×, and 4.1× over CrypTFlow2, on AlexNet,
VGG, ResNet-18, ResNet-50, ResNet-101, and ResNet-152,
respectively. More details are given in Sec. IV.
The rest of the paper is organized as follows. Sec. II intro-
duces the primitives that GALA relies on. Sec. III describes
the design details of GALA. The experimental results are
illustrated and discussed in Sec. IV. Finally, Sec. V concludes
the work.
II. PRELIMINARIES
In this section, we introduce the overall system architecture
and threat model, as well as cryptographic tools used in GALA.
A. System Model
We consider an MLaaS system shown in Fig. 1. The client
owns private data. The server is in the cloud and has a well-
trained deep learning model to provide the inference service
based on the received client’s data. For example, a doctor
sends an encrypted medical image (such as a chest X-ray) to
the server, which runs the neural network model and returns
the encrypted prediction to the doctor. The prediction is then
decrypted into a plaintext result to assist diagnosis and health
care planning.
While various deep learning techniques can be employed to
enable MLaaS, we focus on the Convolutional Neural Network
(CNN), which has achieved wide success and demonstrated
superior performance in computer vision such as image clas-
siﬁcation [40], [62] and face recognition [58]. A CNN consists
of a stack of layers to learn a complex relation among the input
data, e.g., the relations between pixels of an input image. It
operates on a sequence of linear and nonlinear transformations
to infer a result, e.g., whether an input medical image indicates
that the patient has tuberculosis. The linear transformations
are in two typical forms: dot product (i.e., matrix-vector
multiplication) and convolution. The nonlinear transformations
leverage activations such as the Rectiﬁed Linear Unit (ReLu)
to approximate complex functions [35] and pooling (e.g., max
pooling and mean pooling) for dimensionality reduction. CNN
repeats the linear and nonlinear transformations recursively to
reduce the high-dimensional input data to a low-dimensional
feature vector for classiﬁcation at the fully connected layer.
Without losing generality, we use image classiﬁcation as an
example in the following discussion, aiming to provide a lucid
understanding of the CNN architecture as illustrated in Fig. 2.
Convolution. The input to a convolutional layer has the
dimension of uw × uh × ci, where uw and uh are the width
and height of the input feature map and ci is the number of
the feature maps (or channels). For the ﬁrst layer, the feature
maps are simply the input image(s). Hereafter, we use the
subscripts i and o to denote the input and output, respectively.
The input is convolved with co groups of kernels. The size of
each group of kernels is kw × kh × ci, in which kw and kh are
the width and height of the kernel. The number of channels
of each kernel group must match with the input, i.e., ci. The
convolution will produce the feature output, with a size of
wo × ho × co. Speciﬁcally, the process of convolution can be