column labeling vectors:
r ∈ {1, 2, ..., k}n
and c ∈ {1, 2, ..., (cid:96)}m
The sub-matrices created are homogeneous, rectangular regions.
The number of rectangular regions is either given as input to the
algorithm, or determined by the algorithm with a penalty function
that trades off between the number of rectangles and the homogene-
ity achieved by these rectangles 1.
For example, Figure 2 shows a list of 5 malware BitShred ﬁnger-
prints where there are 5 possible features. The result is the 5 × 5
matrix M. Co-clustering automatically identiﬁes the clustering to
1The goal is to make the minimum number of rectangles which
achieve the maximum homogeneity. For this reason, co-clustering
algorithms ensure that homogeneity of the rectangles is penalized
by the number of rectangles if they need to automatically determine
k and (cid:96).
produce sub-matrices, as shown by the checkerboard M(cid:48). The sub-
matrices are homogeneous, indicating highly-correlated feature/-
malware pairs. In this case the labeling vectors are r = (12122)T
and c = (21121)T . These vectors say that row 1 in M mapped to
row cluster 1 (above the horizontal bar) in M’, row 2 mapped to
row cluster 2 (below the horizontal bar), etc., and similar for the
column vectors for features. We can reach two clustering conclu-
sions. First, the row clusters indicate malware s1 and s3 are in one
family, and s2, s4, and s5 are in another family. The column clus-
ters say the distinguishing features between the two families are
features 2, 3, and 5.
2.3 Related Approaches
Feature Hashing and Locality Sensitive Hashing. Our main goal
is to increase scalability in malware comparison, as well as enable
data mining on co-occurring features. We are not alone. Bayer et
al. [13] has made signiﬁcant strides in this area by using locality
sensitive hashing (LSH) [10]. The main idea in LSH is to deﬁne
a hash function h such that h(s1) = h(s2) if the two malware s1
and s2 are similar. The hash is run over all malware samples, and
only those with unique hash values are compared. LSH is comple-
mentary to feature hashing, because it reduces the number of items
while feature hashing reduces the number of features. While com-
plementary, our evaluation shows that using feature hashing alone
outperforms LSH alone by a factor of 2-to-1. Previous work has
shown this bears on theoretical analysis as well [36]. While in our
evaluation we focus on the effects of each algorithm independently,
both feature hashing and locality-sensitive hashing could be com-
bined in a real system.
Feature Hashing and graph analysis comparison. Others have
proposed malware similarity methods that do not use boolean fea-
tures. For example, the Zynamics BinDiff tool [7] and Hu et al.
[21] use a similarity metric based upon isomorphisms between con-
trol ﬂow and function call graphs. While we can compute call graph
similarity based upon features, e.g., how many basic blocks are in
common, our approach cannot readily be adapted to actually com-
puting the isomorphism. Hu et al. argue that although graph-based
isomorphism is expensive, it is less susceptible to being fooled by
In Hu et al. ’s implementation they return the 5
polymorphism.
nearest neighbors, and achieve an 80% success rate in having 1 of
the 5 within the same family on a data set of 102,391 samples. The
query time was between 0.015s to 872s, with an average of 21s us-
ing 100MB of memory. We did not have access to their data set;
results for the same size of our data set for ﬁnding nearest neighbor
using the Jaccard are reported in § 5.1.
Classiﬁcation vs. Clustering. Classiﬁcation uses labeled samples
to learn a rule for assigning labels to new samples. Feature hashing
was previously used to build an efﬁcient spam classiﬁer [11], which
is trained with labeled (i.e., spam/not-spam) emails and then deter-
mines whether an incoming email is spam or not. On the contrary,
clustering groups unlabeled samples based on given similarity met-
rics. In our setting, (unlabeled) malware are grouped based upon
similar features – static code or dynamic behaviors. To the best of
our knowledge, ours is the ﬁrst study to introduce clustering tech-
niques combining feature hashing with the Jaccard and to present a
theoretical proof of correctness.
2.4 Security and Rest of Paper
Security. Feature hashing, like LSH, uses a hash function which
must be kept secret. If the hash function is known, then an attacker
may be able to “fool” the algorithm into an atypical number of col-
lisions, thus potentially reducing the overall accuracy. This prob-
lem is mitigated by using a keyed hash function as is usual, e.g.,
picking a secret key k and computing h(k||sa||k) for item sa. For
simplicity, in the rest of this paper we refer to the hash as simply h
311Figure 3: BitShred Overview
instead of a keyed variant, with the expectation it is used as a keyed
function for security.
In the rest of this paper we describe BitShred, our system for
malware triage and semantic feature identiﬁcation on very large
malware data sets based on the above ideas. We focus on the per-
formance of our hash feature approach vs. previously proposed
methods such as straight set-based analysis, winnowing, and local-
ity sensitive hashing. The main conclusion is that BitShred pro-
vides the same accuracy but better performance. Better perfor-
mance means we scale to much larger malware volumes, as well as
deal with current volumes much more quickly. We also show that
BitShred can be parallelized, which allows us to take advantage
of infrastructures like supercomputers and Hadoop as performance
requirements exceed that which can be provided by a single CPU.
Finally, we show BitShred in the context of an end-to-end sys-
tem for malware triage. In our data set BitShred has above 90% ac-
curacy at automatically identifying malware families and semantic
features. While malware authors can always add more obfuscation
and make analysis harder, thus decreasing accuracy of any system,
the core concepts in BitShred can “plug-in” any malware analy-
sis that outputs Boolean or (binary-encoded) integer-valued values,
and speed it up while retaining similar accuracy to the exact Jac-
card.
3. BITSHRED OVERVIEW
At a high level, BitShred takes in a set of malware, runs per-
malware analysis, and then performs inter-malware comparison,
correlation, and feature analysis, as shown in Figure 3. BitShred’s
job is to speed up subsequent correlation after using existing tech-
niques to perform per-sample feature extraction. In our implemen-
tation, we experiment with using n-grams as proposed in [9, 25, 38]
because the feature space is extremely large, and dynamic behavior
analysis from Bayer et al. [13] because it has been shown effective.
Throughout the rest of this paper we use si to denote malware
sample i, G to denote the set of all features, and gi to denote the
subset of all features G present in si.
We use full hierarchical clustering as a representative computa-
tionally expensive triage task. Hierarchical clustering has a lower
bound of s(s − 1)/2 comparisons for s malware to cluster [19].
Other problems, such as incremental clustering and ﬁnding nearest
neighbor, are algorithmically less expensive. For example, incre-
mental clustering, comparing incoming newly reported s(cid:48) malware
against s malware in a database, requires s(cid:48) × s comparisons where
s(cid:48) (cid:28) s. The nearest neighbor to a malware si can be performed by
comparing it to all other samples, which is linear in s.
3.1 Single Node BitShred
In this section we describe the core components of BitShred:
BITSHRED-GEN, BITSHRED-JACCARD, BITSHRED-CLUSTER and
BITSHRED-SEMANTIC. In § 3.2, we show how the algorithm can
be parallelized, e.g., to run on top of Hadoop or multi-core systems.
• BITSHRED-GEN: G → F .
BITSHRED-GEN is an algorithm from the extracted feature set
gi ∈ G to ﬁngerprints fi ∈ F for each malware sample si. A
BitShred ﬁngerprint fi is a bit-vector of length m, initially set to 0.
BITSHRED-GEN performs feature hashing to represent feature sets
gi in ﬁngerprints fi. More formally, for a particular feature set we
deﬁne a hash function h : χ → {0, 1}m where the domain χ is
the domain of possible features and m is the length of the bitvector.
We use djb2 [14] and reduce the result modulo m. (Data reduc-
tion techniques such as locality-sensitive hashing [13] and Win-
nowing [34] can be used to pare down the data set for which we
call BITSHRED-GEN and perform subsequent steps.)
• BITSHRED-JACCARD: F × F → R.
BITSHRED-JACCARD computes the similarity d ∈ [0, 1] between
ﬁngerprints fa and fb using the bitvector Jaccard from Equation 2.
A similarity value of 1 means the two samples are identical, while
a similarity of 0 means the two samples share nothing in common
(in our setting, this means they share no features in G).
Formally, Theorem 1 states that BITSHRED-JACCARD well-ap-
proximates the Jaccard index.
Theorem 1. Let ga, gb denote two sets of size N with c common
elements, and fa, fb denote their respective ﬁngerprints with bit-
vectors of length m and k hash functions. Let Y denote S(fa∧fb)
S(fa∨fb) .
Then, for m (cid:29) N, , 2 ∈ (0, 1),
P r[Y ≤ c(1 + 2)
2N − c − m
] ≥ 1 − e
−mq2
2/3 − 2e
−22m2/N k
and
P r[Y ≥
c(1 − 2)
(2N − c) + m
for q = 1 − 2`1 − 1
] ≥ 1 − e
´kN +`1 − 1
m
m
−22m2/N k
2/2 − 2e
−mq2
´k(2N−c).
We defer a full proof to the Appendix. Note that because the goal
of feature hashing is different from Bloom ﬁlters, our guarantees
are not in terms of the false positive rate for standard Bloom ﬁlters,
but instead are of how well our feature hashing data structure lets
us approximate the Jaccard index.
• BITSHRED-CLUSTER: (F × F × R list) × R → C.
BITSHRED-CLUSTER takes the list containing the similarity be-
tween each pair of malware samples, a threshold t, and outputs a
clustering C for the malware. BITSHRED-CLUSTER groups two
malware if their similarity d is greater than or equal to t: d ≥ t.
The threshold t is set by the desired precision tradeoff based upon
past experience. While a smaller t divides malware into a few gen-
eral families, a larger t discovers speciﬁc variants of a family. See
§ 5 for our experiments for different values of t.
BitShred currently uses an agglomerative hierarchical clustering
algorithm to produce clusters in that the number of clusters is typi-
cally not known in advance. Initially each malware sample si is as-
signed to its own cluster ci. The closest pair is selected and merged
into a cluster. We iterate the merging process until there is no pair
whose similarity exceeds the input threshold t. When there are mul-
tiple samples in a cluster, we deﬁne the similarity between cluster
cA and cluster cB as the maximum similarity between all possible
pairs (i.e., single-linkage), i.e., BITSHRED-JACCARD(cA, cB) =
BitShred-GenStatic AnalysisDynamicAnalysisBitShred-Schedule BitShred-JaccardBitShred-JaccardBitShred-JaccardBitShred-Cluster FeatureExtractionBITSHREDPHASE	
  1PHASE	
  2PHASE	
  3MalwareFamiliesMalwareBitShred-SemanticPHASE	
  4Seman-cFeatures312BITSHRED-SCHEDULE
respectively. Co-clustering reveals the semantic feature informa-
tion, i.e., the checkerboard patterns which describe distinguishing
or common features across the malware families. We discuss both
inter- and intra-family feature extraction in detail in § 5.3.
3.2 Distributed BitShred
3.2.1
There are two things we parallelize in BitShred: ﬁngerprint gen-
eration in Phase 1, and the s(s − 1)/2 ﬁngerprint comparisons in
Phase 2 during clustering. Parallelizing ﬁngerprint generation is
straight-forward: given s malware samples and r resources, we as-
sign s/r malware to each node and run BITSHRED-GEN on each
assigned sample.
Parallelizing BITSHRED-JACCARD in a resource and communi-
cation-efﬁcient manner requires more thought. There are s(s −
1)/2 comparisons, and every comparison takes the same ﬁxed time,
so if every node does s(s − 1)/2r comparisons all nodes do equal
work. Unlike BITSHRED-JACCARD, comparisons between variable
length of two sets take time (computation) depending on the length,
thus distributing uniform node work is not simple.
To accomplish this we ﬁrst observe that while the ﬁrst malware
needs to be compared against all other malware (i.e., s − 1 ﬁnger-
print comparisons), each of the remaining malware require fewer
than s − 1 comparisons each.
In particular, malware i requires
only s − i comparisons, and malware s − i requires s − (s − i)
comparisons. The main insight is to pair the comparisons for mal-
ware i with s − i, so that the total comparisons for each pair is
s − i + s − (s − i) = s. If we pair together the comparisons
for malware i with s − i, the total comparisons for each pair is
s − i + s − (s − i) = s. Thus, for each node to do uniform
work, BITSHRED-SCHEDULE ensures that the s − i comparisons
for malware i are scheduled on the same node as the s − (s − i)
comparisons for malware s−i. BITSHRED-SCHEDULE then simply
divides up the pairs among the r nodes.
Although there may be other protocols for distributing compu-
tations, we note that this approach is simple, and optimal in the
sense that there all nodes do equal work and there is no inter-node
communication during the s2 Jaccard calculations.
3.2.2 BitShred on Hadoop
Our distributed implementation uses the Hadoop implementation
of MapReduce [1, 17]. MapReduce is distributed computing tech-
nique for taking advantage of a large computer nodes to carry out
large data analysis tasks. In MapReduce, functions are deﬁned with
respect to (cid:104)key,value(cid:105) pairs. MapReduce takes a list of (cid:104)key,value(cid:105)
pairs, and returns a list of values. MapReduce is implemented by
deﬁning two functions:
1. MAP: (cid:104)Ki, Vi(cid:105) → (cid:104)Ko, Vo(cid:105) list. In the MAP step the master
Hadoop node takes the input pair of type (cid:104)Ki, Vi(cid:105) and parti-
tions into a list of independent chunks of work. Each chunk
of work is then distributed to a node, which may in turn ap-
ply MAP to further delegate or partition the set of work to
complete. The process of mapping forms a multi-level tree
structure where leaf nodes are individual units of work, each
of which can be completed in parallel. When a unit of work
is completed by a node, the output (cid:104)Ko, Vo(cid:105) is passed to RE-
DUCE.
2. REDUCE: (cid:104)Ko, Vo(cid:105) list → Vf list. In the REDUCE step the
list of answers from the partitioned work units are combined
and assembled to a list of answers of type Vf .
We also take advantage of the Hadoop distributed ﬁle system
(HDFS) to share common data among nodes.
In phase 1, distributed BitShred produces ﬁngerprints using the
Hadoop by deﬁning the following MapReduce functions:
(a) A typical matrix before co-clustering
(b) 8 different kinds of Trojan
(c) 3 different kinds of Adware
Figure 4: Semantic feature information. Grey dots represent 1 in
the binary matrix, i.e., the presence of a feature.
max{BITSHRED-JACCARD(fi, fj)|fi ∈ cA, fj ∈ cB}. We chose
a single-linkage approach as it is efﬁcient and accurate in practice.
• BITSHRED-SEMANTIC: C × F → G(cid:48).
Based on the BITSHRED-CLUSTER results, BITSHRED-SEMANTIC
performs co-clustering on subset of ﬁngerprints to cluster features
as well as malware samples. Co-clustering yields correlated features-
malware subgroups G(cid:48) which shows the common or distinct fea-
tures among malware samples, as discussed in § 2.2.
We have adapted the cross-associations algorithm [16], redesigned
for the Map-Reduce framework [30], to BitShred ﬁngerprints. The
basic steps are row iterations and column iterations. A row iteration
ﬁxes a current column group and iterates over each row, updating
r to ﬁnd the “best” grouping. In our algorithm, we seek to swap
each row to a row group that would maximize homogeneity of the
resulting rectangles. The column iteration is similar, where rows
are ﬁxed. The algorithm performs a local optimal search (ﬁnding a
globally optimal co-clustering is NP-hard [30]).
Unlike typical co-clustering problems, co-clustering in BitShred
needs to operate on hashed features, i.e., recall that our ﬁngerprints
are not the features themselves, but hashes of these features. How-
ever, because our feature hashing is designed to approximately pre-
serve structural similarities and differences between malware sam-
ples, we can apply co-clustering on our hashed features (just as if
they were regular features) and still extract the structural relation-
ships between the malware samples, and with the increased com-