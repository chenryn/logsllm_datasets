our hypothesis. We observed similar results for four other
applications we evaluate in §6, as well as for two popular
Wordpress plugins for Azure blob storage and Redis cache.
Latency proﬁles may depend on conﬁguration and work-
load parameters. While required conﬁguration parameters
come directly from a given what-if scenario, workload pa-
rameters need to come from developers and applications.
WebPerf uses several
including application-
speciﬁc baseline latencies and developer-speciﬁed workload
hints. We describe these in §4.3 and evaluate them in §6.
2.3 WebPerf Overview
techniques,
WebPerf is designed to satisfy three requirements: it must
estimate the cloud latency distribution of a given request and
a what-if scenario accurately, must do so quickly so that de-
velopers can explore many scenarios in a short time, and
must require minimal developer input.
Figure 3 depicts various components of WebPerf. It takes
as input a workload and a what-if scenario. The workload
consists of (1) a web application, (2) a set of HTTP requests,
and (3) optional workload hints to help WebPerf choose ac-
curate latency models for the application’s workload. Ta-
ble 2 lists various what-if scenarios WebPerf supports.
WebPerf works as follows. WebPerf’s Binary Instru-
Figure 1— Relative errors of app-independent proﬁles.
Figure 2—Latency of a Join in Basic and Stan-
dard tier of Azure SQL.
Figure 3— WebPerf Architecture.
What-if scenario
Location: A resource X is
deployed at location Y
Tier: A resource X is
upgraded to tier Y
Input size: I/O calls to
resource X take inputs of size
Y
Load: X concurrent
requests to resource Y
Interference: CPU and/or
memory pressure, from
collocated applications, of
X%
Failure: An instance of a
replicated resource X fails
Example
X = A Redis Cache or a
front end, Y = Singapore
X = A Redis cache, Y = a
standard tier (from a basic
tier)
X = a blob storage, Y = a
760 × 1024 image
X = 100 , Y = the
application or a SQL database
X = 50% CPU, 80%
memory
X = A replicated front-end
or SQL database
Table 2— What-if scenarios supported by WebPerf
menter (§3) automatically instruments the given web appli-
cation. The requests in the workload are then executed on the
instrumented application, generating two pieces of informa-
tion: (1) a dependency graph, showing causal dependencies
of various compute and I/O calls executed by the requests,
and (2) the baseline latency of various compute and I/O calls
for the initial conﬁguration. The number of measurements
collected for the baseline latency is decided by an optimiza-
tion algorithm (§5.2). The instrumentation is disabled after
the measurement ﬁnishes. WebPerf also uses a Proﬁler (§4),
which builds, ofﬂine, empirical latency distributions of var-
ious cloud APIs under various what-if scenarios. The core
of WebPerf is the What-If Engine (§4), which combines de-
pendency graphs, baseline latencies, and ofﬂine proﬁles to
predict a distribution of cloud latencies of given requests un-
der what-if scenarios.
WebPerf can also combine its predicted cloud latencies
with network latencies and client-side latencies (e.g., pre-
dicted using WebProphet [25]) to produce a distribution of
end-to-end latencies (§5.1).
3. TRACKING CAUSALITY IN TASK
ASYNCHRONOUS APPLICATIONS
WebPerf’s prediction algorithm uses causal dependencies
between various computation and I/O calls invoked by a re-
quest. Cloud applications are increasingly written using Task
Asynchronous Paradigm, which prior work (§7) on causal
dependency tracking does not consider. WebPerf can track
causal dependency within such applications accurately, in a
261
async Task ProcessReq(Request req) {
/* Synchronous process block 1 */
var input = GetInput(req) ;
var task1 = AsyncTask1(input);
var output = await
/* Synchronous process block 2 */
var response = GetResponse(output);
return response ;
task1 ;
}
async Task AsyncTask1(Input input) { ... }
Figure 4— Async-await example.
lightweight manner, and with zero developer effort.
3.1 Task Asynchronous Programming
Node.js [28] popularized the notion of non-blocking I/O
in cloud and server applications. Non-blocking APIs greatly
increase an application’s throughput in a lightweight manner
(i.e., with a small thread pool). Today all major program-
ming languages support non-blocking operations.
There are two major ﬂavours of non-blocking I/O. The
Asynchronous Programming Model (APM) uses callbacks
that are invoked on certain events such as completion of an
operation. Many languages including JavaScript, Java, and
C# support APM. A major drawback of APM is that devel-
opers often need to write a large number of callback meth-
ods to sequence their request pipeline, quickly getting into
the problem of callback hell [7]. Moreover, callbacks invert
the control ﬂow, and can obfuscate developer’s intent [41].
These limitations are addressed by the Task Asynchronous
Paradigm (TAP), which allows writing non-blocking asyn-
chronous programs using a syntax resembling synchronous
programs (Figure 4). TAP is supported by many major
languages including .NET languages (C#, F#, VB), Java,
Python, JavaScript, and Scala. For instance, C# supports
TAP with a Task object and async and await constructs
(example code later). TAP has become increasingly popular
for writing cloud applications. Almost all Microsoft Azure
framework libraries today support TAP as the only mecha-
nism for doing asynchronous I/O and processing. Amazon
AWS also provides TAP APIs for .NET [3].
Figure 4 shows an example request pipeline writ-
ten with TAP in C#. ProcessReq method processes
the incoming request, does a non-blocking asynchronous
call (AsyncTask1) to, say, fetch a document from the
store, processes the result, and sends a response back.
AsyncTask1 is an async method that starts the task asyn-
chronously and returns a Task object. To obtain a result for
0246810121234567891011Relative Error (%)API ID Mean90 PercentileT:Table, R:Redis, S:SQL, B:Blob, Q:Queue1Delete(Async)(T)2UploadFromStream(B)3AddMessage(Q)4Execute (T)5ExecuteQuerySegmented(T)6SortedSetRangeByValue(R)7StringGet(R)8SaveChanges(S)9ToList(S)10Send (R)11ReadAsString(B)Web appWhat-if scenarioBinary InstrumenterRuntimeInstrumentedWeb appProfilerProfiledictionaryDependency graphBaseline performanceWhat-if enginePredictorPerformance distributionWebPerf/* Receive request */
var task1 = AsyncTask1 (...) ;
var task2 = AsyncTask2 (...) ;
var
/* Send response */
var task1 = AsyncTask1 (...) ;
var task2 = AsyncTask2 (...) ;
var
/* Send response */
...
result = await Task.WhenAll(task1, task2 ) ;
/* Receive request */
result = await Task.WhenAny(task1, task2);
Figure 5— When all and when any example.
an async method, the caller does await on the task object.
When await is used, the execution does not proceed until
the result of the awaiting task is obtained. Note that await
does not block the thread, but returns the thread and adds the
rest of the method as a continuation (implicit callback) to the
task being awaited. In Figure 4, the continuation is the start
of the synchronous block 2. When the task ﬁnishes,
its continuation executes on a different logical thread. The
execution trace for ProcessReq is shown in Figure 6.
Synchronization Points. TAP provides explicit abstrac-
tions for developers to synchronize multiple parallel tasks.
.NET provides two such abstractions: Task.WhenAll and
Task.WhenAny. WhenAll accepts multiple tasks as argu-
ment and signals completion only when all tasks complete.
Figure 5 (top) and Figure 7 (left) show an example request
pipeline and its execution trace where the response is re-
turned only when both tasks ﬁnish. WhenAny accepts mul-
tiple tasks but signals completion as soon as any one of them
completes. Figure 5 (bottom) and Figure 7 (right) show an
example request pipeline and its execution trace. WhenAll
and WhenAny return tasks that can be awaited or synchro-
nized with other tasks.
3.2 Tracking Causal Dependency
Given a user request in a TAP application, we would like
to extract a causal dependency graph consisting of three
pieces of information: (1) nodes representing synchronous
and asynchronous compute and I/O calls, (2) edges rep-
resenting causal (i.e., happens-before) dependency among
nodes, and (3) WhenAll and WhenAny synchronization
points. All these pieces are required for total cloud latency
estimation. Missing nodes may result in latency underes-
timation. Missing or incorrect edges may imply incorrect
execution orders (serial vs. parallel) of nodes and produce
inaccurate estimates.
Existing causality tracking techniques, however, may not
capture all these necessary pieces of information (§7). A
network-level proxy can externally observe I/O calls and
their timing information [10], but will miss computation
nodes and synchronization points that are not observable at
the network layer. Moreover, inferring dependency based on
timing information can be wrong. Suppose tasks n1 and n2
run in parallel and n3 starts only after n2 ﬁnishes. Each node
has an execution time of t, and hence the total execution time
is 2t. Externally observed timing and execution order could
infer that n3 is dependent on both n1 and n2. This can lead
to incorrect estimation in a what-if scenario that considers
the case when n1’s execution time is t/2. The correct de-
262
pendency graph will estimate a total execution time of 1.5t,
while the externally observed dependency graph could esti-
mate 2t. One might be able to correct some of these am-
biguities by collecting a large amount of data (as in [10]).
WebPerf, however, is designed to be used at deployment
time when not much data is available.
Another way to track causal dependency is to instru-
ment the runtime. Existing .NET proﬁling tools [40, 22,
1] can capture some dependencies, but fail to capture asyn-
chronous dependencies and synchronization points of async-
await programs. AppInsight [32] can track asynchronous
callback dependencies, but it is designed for APM and does
not support TAP.
The amount of information missed by existing solutions
(and hence the estimation error) can be signiﬁcant. In work-
loads from six real applications we describe in §6, a depen-
dency graph on average has 182 computation nodes (missed
by a network proxy), 180 async-await edges (missed by all),
and 62 synchronization points (missed by all).
3.3 Capturing an Execution Trace
We now describe how WebPerf captures all the informa-
tion in dependency graphs for TAP. It uses a two-step pro-
cess: it instruments the application binary to capture the ex-
ecution trace of a request, and then analyzes the execution
trace to construct the request’s dependency graph.
WebPerf automatically instruments application binaries to
capture the execution trace that preserves causal dependen-
cies of all async calls. Doing this for a TAP application
presents several unique challenges not addressed by exist-
ing causality tracking solutions (e.g., AppInsight) for APM
applications. We now describe the key challenges and our
solutions. The techniques are described for .NET, but the
general idea can be used for other languages supporting TAP.
Tracking implicit callbacks: To track the lifetime of an
asynchronous call, we need to correlate the start of the call
and its callback. In APM, callbacks are explicitly written by
developers and these callbacks can be instrumented with a
correlation ID to match with the original request. In TAP,
however, there are no such explicit callbacks – upon await
call on a (completed) task, execution starts from its contin-
uation (i.e., rest of the async method after the await call).
Thus, continuations act as implicit callbacks.
Identifying and instrumenting continuations in the source
code may not be obvious; for instance, a task’s creation and
continuation can be in two different methods. To address
this, we observe that an async method executes as a state
machine, which maps continuations to different states, and
state transitions occur as awaited tasks complete.
In fact,
.NET compiles async-await source code to binaries contain-
ing state machines. For instance, consider the async method
in Figure 4. The .NET compiler translates the code to a state
machine containing two key states: the ﬁrst state contains
the synchronous block 1 and starts the await process, and
the second state contains the continuation of the await (i.e.,
synchronous block 2). The state machine also contains a
MoveNext() method that causes transition between states
when awaited tasks complete.
Figure 6— Execution trace for the code shown
in Figure 4.
Figure 7— Execution traces for the code shown
in Figure 5.
Figure 8— Dependency graphs for the requests
in Figures 4 and 5.
class ProcessReq__ {
state = −1;
Request req ;
int
TaskAwaiter awaiter ;
AsyncTaskMethodBuilder builder;
int asyncId = -1;
public void MoveNext() {
irrespective of the task completion time. 4) We track the life-
time of a thread by tracking the start and end of a state in the
state machine (line 9, line 20, and line 30). 5) Finally, we
track the completion of an async method by tracking calls to
SetResult, an inbuilt method that triggers the response of
an async method (line 25).
Tracking pull-based continuation: Execution of a con-
tinuation is pull-based – it is invoked only when await is
called and this can happen much later than the actual task
completion. Hence, we need to track task completions inde-
pendently from awaits but also understand their causal de-
pendencies. In contrast, in APM, the framework invokes the
callback as soon the task completes. To accurately track task
completion, we add a continuation to the task (inside the
Tracker.TaskStart method in line 16) and wrap the
completion handler inside an object that saves the async id.
Tracking execution forks and joins: To track synchro-
nization dependencies between tasks, we instrument the
code to intercept Task.WhenAll, Task.WhenAny and
other equivalent primitives. Using object ids of tasks passed
to these calls, we record their dependencies. When tasks
complete, we connect the continuations to the set of parallel
tasks that were synchronized.
Tracking non-TAP calls: Apart from async-await calls,
WebPerf also tracks other types of asynchronous calls (e.g.,
APM with explicit callbacks) and expensive synchronous
calls (e.g., synchronous Azure APIs).
For the former,
WebPerf uses similar instrumentation technique as AppIn-
sight. For the latter, it logs start and end of the call and uses
thread ids to track their dependencies with async methods.
Optimizations: We use a few optimizations to reduce
the runtime overhead of instrumentation. First, we track
only (synchronous and asynchronous) APIs having known
method signatures from Azure libraries. Second, within an
async-await method call chain, we instrument only the non-
blocking tasks (with known signatures). In .NET, methods
using await should also be declared as async and can be
awaited by other methods. Hence, when a request involves a
non-blocking async call, all methods in its pipeline including
the entry point are declared as async methods and compiled
into a state machine. Though the intermediate methods are
async, continuations between them are called synchronously
except for the method with the non-blocking call. We signif-
icantly reduce the overhead of tracking without compromis-
ing accuracy by monitoring only non-blocking async tasks,
state machines of methods awaiting them, and the request
entry point method. Our instrumentation overhead is low
and comparable to numbers reported in AppInsight [32, 33].
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16