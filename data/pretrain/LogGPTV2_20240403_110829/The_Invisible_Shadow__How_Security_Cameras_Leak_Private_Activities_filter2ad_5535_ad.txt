1.4m 2.0m 2.6m 3.4m 4.2m average
0.01 0.04 0.06 0.13 0.65
0.17
0.08 0.13 0.15 0.21 0.33
0.18
which is only 6.2% larger than the lowest error (subject 1). It is
worth noting that subject 3 is not the tallest among all ubjects,
which indicates that the relative limb length error is not strongly
related with the subjectâ€™s height. We also find that the absolute limb
length error of subject 3 is 0.024m, which is only 0.005m higher
than the subject 1, indicating that LLE is robust to the limb length
variation.
On the other hand, we find that the limb length error is mainly
affected by moving speed. By inspecting the original video footage,
we find that the movement speeds of subject 3 and 5 are relatively
high, resulting in high keypoint detection errors of the DeShaNet
(refer to Table 6), which in turn introduces noise on the input limb
length.
Verifying the CDE. We further evaluate whether the CDE can
accurately restore the curtain deformation. Since it is hard to quan-
tify curtain deformation, we instead use the local moving speed ğ‘£ğ‘™
as an indirect metric to evaluate the effectiveness of the CDE. Our
testing data includes 4 different curtain deformation patterns. The
shadow keypoints of videos are first detected by the DeShaNet and
then converted to local moving speed ğ‘£ğ‘™. The CDE then estimates
the vertex coordinates of the curtain from ğ‘£ğ‘™. We use the flat-curtain
(i.e., curtain deformation estimation is disabled) as a baseline. The
results in Table 10 show that complex curtain patterns tend to pro-
duce higher moving speed error. The highest error (0.344) occurs
under random deformation pattern, which is 3Ã— larger than the
flat curtain baseline (w/o). Compared with the baseline, the CDE
shows 3.5Ã— lower moving speed error and 2.8Ã— lower keypoint
error, indicating that CDE can faithfully restore the realistic curtain
surface.
metric
moving speed error
(m/s) w/o CDE
2D keypoint error
(m) w/o CDE
moving speed error
(m/s) /CDE
2D keypoint error
(m) /CDE
w/o vertical U-shape random average
0.337
0.797
0.638
0.872
1.344
0.027
0.032
0.083
0.215
0.008
0.011
0.037
0.258
0.013
0.041
0.034
0.344
0.225
0.018
0.012
Evaluation of the system assembly. We now evaluate the
scene constructor with all SPEs enabled in more comprehensive
situations. The final target of the scene constructor is to restore
the 3D skeleton from the keypoint predictions of the DeShaNet.
Multiple factors that could affect the restoration accuracy, including
the occlusion of activity and the shadow deformation caused by
scene parameters. Therefore, according to the intensity of occlu-
sion and deformation, the testing data are divided into 5 groups:
severe occlusion (oc-h), weak occlusion (oc-l), low deformation
(de-l), medium deformation (de-m) and high deformation (de-h).
The detailed categorization information is listed in Table 11. We
use two metrics to evaluate the performance: ğ‘†2ğ· and ğ‘†3ğ·, which
represents the shadow keypoint errors and the 3D skeleton errors,
respectively. We then test the scene constructor on the groups of
data and the results are shown in Table 12.
When only enabling IRSPE, LLE or CDE, the average ğ‘†2ğ· are
25%, 8% and 17% lower than the baseline, and the average ğ‘†3ğ· are
18% ,6% and 12% lower than the baseline, respectively. It indicates
that all the design components play a crucial role in improving
the estimation accuracy. It is worth noting that the IRSPE shows
the largest improvement because the IR angles have the greatest
impacts on the shadow keypoint detection. We obtain best results
when enabling all SPEs together, with 36% lower ğ‘†2ğ· and 26%
lower ğ‘†3ğ· than the baseline. It indicates that the 3 SPEs are all
necessary and complementary to each other. We illustrate examples
of shadow simulation effects and 3D skeleton estimation by different
combinations of SPEs in Fig. 11-15.
7.3 Evaluation of other aspects
Comparison of different IR devices. Except from smart home
security cameras, there are various other in-home devices that emit
IR light, e.g., Kinect, smartphones and mobile lidars. These devices
also have the potential of causing the IRSA. Table 13 summarizes
the major characteristics of representative devices. Here the IR
patterns refer to the patterns of the IR illumination, e.g., solid areas
Session 10D: Applied Privacy CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea2789Figure 11: W/o IRSPE,
LLE, CDE.
Figure 12: W/ IRSPE,
w/o LLE, CDE.
Table 11: Testing data groups definition.
Figure 13: W/ IRSPE,
LLE, w/o CDE.
Figure 15: 3D skeleton
ground-truth and real-
istic body activities un-
der IR camera view.
7.4 Case study: recognizing private activities
Figure 14: W/ IRSPE,
LLE CDE.
using the recovered 3D keypoints
In this section, we demonstrate how the 3D keypoints derived from
DeShaNet can be used as input to existing skeleton based activity
recognition algorithms and consequently impinge on user privacy.
We adopt a representative algorithm, extremely randomized trees
(ERT) [13], which follows 3 stages. (i) Data preprocessing. The co-
ordinates of all the 3D keypoints are first normalized to a unified
coordinate system with a predefined origin: ğ¶ğ‘œğ‘Ÿğ‘–ğ‘”ğ‘–ğ‘› = (ğ¶ğ‘™_ğ‘ â„ğ‘œğ‘¢ğ‘™ğ‘‘ğ‘’ğ‘Ÿ+
ğ¶ğ‘Ÿ_ğ‘ â„ğ‘œğ‘¢ğ‘™ğ‘‘ğ‘’ğ‘Ÿ +ğ¶â„ğ‘’ğ‘ğ‘‘)/3. Then a Savitzkyâ€“Golay filter with a 5-point
cubic polynomial is applied to all the 3D skeletons to remove noise:
â€²
ğ‘– = (âˆ’3âˆ—ğ¶ğ‘–âˆ’2 + 12âˆ—ğ¶ğ‘–âˆ’1 + 17âˆ—ğ¶ğ‘– + 12âˆ—ğ¶ğ‘–+1 âˆ’ 3âˆ—ğ¶ğ‘–+2)/35, where
ğ¶
â€²
ğ¶ğ‘– denotes the coordinates at frame ğ‘– and ğ¶
ğ‘– is the filtered result.
(ii) Spatio-temporal feature encoding. For spatial encoding, the 3D
keypoint coordinates from the same frame are encoded into two
matrices using Minkowski distances and cosine distance, respec-
tively. For temporal feature encoding, each coordinate is encoded by
two scalars ğ½ğ‘–,ğ‘šğ‘ğ‘¥ and ğ½ğ‘–,ğ‘šğ‘–ğ‘›, which are calculated by the difference
between current coordinates and the maximum/minimum coordi-
nates respectively. Then, each frame is further encoded by a vector
with length 2 âˆ— ğ‘ , where ğ‘ denotes the keypoint number (9 in our
setup) of each frame . (iii) Random forest learning by the extremely
randomized trees algorithm. The randomized trees perform frame
level classification based on the spatial feature and the temporal
feature. The final classification is derived by averaging the results
from all trees. The total number of trees is 40 and the maximum
depth is 20.
Since the source code of the ERT [13] is not publicly available, we
implement it following [13] based on the scikit-learn python library.
We have validated our reproduction of ERT on the Microsoft MSR
action 3D dataset [22] adopted in [13] and got consistent accuracy
(82.1% vs 80.9%), which verifies the correctness of our implemen-
tation. In order to evaluate the accuracy of activity recognition,
we divide the dataset we collected (Section 6) into 3 categories,
for training, validation and testing, respectively. The training set
and validation set contain the same categories of activities but are
orthogonal to each other: eating, running, walking, dancing and
stretching exercise. For subset, we further divide it into multiple
subsets, one subject each. The testing set contains 3 different activ-
ities (i.e., nose picking, body twisting and bottom uplifting) from
the training and validation set, and is used to test the model gen-
eralization. For each entry in the dataset, we have converted it to
3D skeletons using the DeShaNet and scene constructor. Table 15
summarizes the results. We see that the average activity recogni-
tion accuracy is 87.9% and 83.4%, on the validation and testing set
data description
oc-l bottom uplifting , body twisting
oc-h nose picking, armpit stretching, eating (for a long time)
de-l curtain deformation (w/o), IR distance (1-2.5mm), IR
de-m curtain deformation (U-shape), IR distance (2.5-3.5m),
de-h curtain deformation (random, vertical), IR distance (over
angles (0â—¦)
IR angles (over 15-45â—¦)
3.5m), IR angles (over 45â—¦)
and spot patterns as shown in Fig. 16-17. Through the IR patterns,
the attacker can infer what kind of devices victims are using, which
may help estimate their activities more accurately. Both IR light
patterns can project valid shadows and penetrate curtains.
Attack during daytime. Although the IRSA most happens at
night due to the environmental illumination, we find that it can
also be performed during daytime. For example, the IR light of
smartphone cameras can be triggered at daytime when the indoor
illumination is low, e.g., window curtains are closed, which pro-
vides opportunity for IRSA. We deploy such an attack scenario
during daytime, as shown in Fig.9. The attackerâ€™s camera is placed
beside the window under strong daylight which overwhelms the
IR shadow, as shown in Fig.18. However, we find that the attacker
could easily use an IR lens filter [2] to circumvent this challenge.
The IR lens filter is low cost (10m 5m
voi-8
2m
respectively, which is consistent with [13]. Note that the keypoints
in [13] where obtained through the Kinect 3D camera. The result
implies that the 3D keypoints generated by our DeShaNet and scene
constructor are sufficiently accurate for recognizing fine-grained
activities that involve body/limb movements.
7.5 Generalization to strongly private activities
In this section, we show that the DeShaNet keypoint generator
works for both generic activities and private concerning activities.
We divide our dataset into weak privacy activities and real privacy
activities. The former include eating, running, walking, dancing,
stretching exercise and body twisting. The latter include nose pick-
ing and bottom uplifting (simulating sex behavior). Based on the
user study in [9], over 24% of people think that the exposure of
these two kinds of activities are extremely private. Then we test the
2D keypoint detection errors under different scene factors: curtain
Figure 18: Camera view at
day time without using IR
filter.
Figure 19: Camera view at
day time using IR filter
(850nm).
Table 15: 3D skeleton based activity classification accuracy
using extremely randomized trees algorithm [13].
Subject1
Subject2
Subject3
Average
Training Validation Testing
83.4%
91.2%
93.6%
84.1%
82.6%
90.4%
91.7%
83.4%
87.9%
88.7%
87.2%
87.9%
deformation, IR distance, IR angle and subjects. Other experimental
configurations are consistent with Sec. 7.1.
Table 16 summarizes the results, where the weak and real ac-
tivities are denoted by â€œweâ€ and â€œreâ€, respectively. We see that the
2D keypoint detection errors of weak and real privacy activities
are similar under different experimental setups. The average error
of weak privacy activities is 6.97, which is only 2.4% lower than
real privacy activities. It indicates that our model can generalize to
real privacy-concerning activities. The underlying reason is sim-
ple. Both the weak and real privacy activities share similar body