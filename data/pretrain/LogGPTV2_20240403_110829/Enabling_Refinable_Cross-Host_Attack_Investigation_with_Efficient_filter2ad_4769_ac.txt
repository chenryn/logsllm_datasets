USENIX Association
27th USENIX Security Symposium    1711
be running at the runtime of a program, where no knowl-
edge of the data at the source or sink is known prior, they
usually assign a fixed size for each tag such that they
are confident it is safely big enough. For example, Data-
Tracker [61] uses 32 bits to identify an inbound file, and
another 32 bits to identify the offset of the data (totally
64 bits). The size is sufficient for identifying every byte
in a normal desktop. Dytan [24] represents whether one
source is tainted or not as one bit and stores all the bits
in a bit vector as the tag. Thus the size of each tag is
linear to the number of sources, which can be huge in
the case of a high number of sources. Note that the tag
map not only stores the tags for the source and sink, but
all the intermediate memory locations and registers as
well. Since most implementations of DIFT maintain the
tag map in memory to pursue faster instrumentation, such
high use of memory has a possibility to cause the DIFT
to crash before it is complete. This problem is elevated
when the scope of investigation extends to multiple hosts
since the workload of DIFT increases in proportion.
In contrast to the previous works that perform DIFT
at the program runtime, RTAG is a record-replay based
system in which the knowledge of data source and sink
is known to us when we perform DIFT at replay time. In
other words, we know which (bytes of) data need to be in-
volved in the DIFT. Thus, we can adjust the tag size based
on the entropy of the data dependency confusion, rather
than use a fixed-size tag. Figure 3 compares the memory
cost for tag map in different DIFT engines: (a) shows that
the local tag of RTAG grows in logarithm while others are
either linear or constant; (b) presents the total tag map
size under different sizes of memories that are tainted (i.e.,
allocated with tags) where the memory cost introduced
by RTAG is the lowest (by significant difference). Before
DIFT, RTAG computes the optimal local tag needed to
mark the source and substitute the global tag for the local
one when a source is loaded to the memory space of the
process (e.g., via read() syscall). While performing
DIFT, RTAG allocates the tags for intermediate locations
lazily when a memory location or register becomes tainted
with some tag. When the propagation arrives at a sink
(e.g., via a write() syscall), RTAG replaces the local
tag with the original global one, and updates the tag value
of the sink. We observe significant memory cost reduc-
tion by applying this optimal tag allocation method (see
§8.2.1).
6.6 Tag Association
In order to track the data flow between different hosts,
we additionally hook the socket handling of the operating
kernel to enable the cross-host tagging. Prior studies
adopt an “out-of-band” method to track the data flow
communication (e.g., [38, 50]). Though this method is
Figure 3: Memory cost for tags in DIFT. The left (a) shows
the size of each tag given different numbers of symbols used in
DIFT. The right (b) depicts the tagmap sizes based on different
sizes of memories being allocated with tags when 256 symbols
are used in the DIFT. RTAG local, global, DataTracker, and
Dytan tags are compared.
tag size literally affects the memory cost of the whole tag
map and tag switching significantly reduces the overall
memory cost of DIFT.
6.5 Optimal Local Tag Allocation
The runtime cost of DIFT is high, both in time and stor-
age. DIFT usually takes 10×–30× longer than the original
execution because its instrumentation adds additional tag
update operations to each executed instruction. Recent
studies [34, 47] alleviate this issue by decoupling the in-
strumentation efforts from the runtime of the program.
However, the storage footprint of tag map, the data struc-
ture used by DIFT to maintain the tag propagation status,
can still be very high particularly when there are multiple
(or many) sources.
The cost of tag map in DIFT depends on its sup-
ported type of tags and purpose. DIFT engines such as
Taintcheck [49], Taintgrind [16], and ShadowReplica [34]
use a basic binary tag model for DIFT, which assigns a
boolean “tainted” or “not tainted” for each source of DIFT.
It is able to tell whether the tainted data is propagated to
the sink, which can be used to alarm sensitive data leak-
age or control-flow hijacking. However, this model is
not flexible enough for the goal of RTAG, where the data
dependency confusion it aims to resolve involves multiple
sources.
Dytan [24] and DataTracker [61] provide a customiz-
able model for the data sources and sinks. It allows the
allocation of multiple tags to each addressable byte of
data at the source or sink. The tag model used by such
systems is flexible, but the tag map used to maintain the
status of the taint propagation is “over-flexible” thus huge,
which inhibits the deployment of such a system in many
resource restrained cases. As these systems assume to
1712    27th USENIX Security Symposium
USENIX Association
more straightforward when identifying and managing the
tags across hosts, it requires additional bookkeeping that
incurs both complexity and overhead to the hosts. In
contrast, we propose an “in-band” method to track the
data flow among hosts, which particularly fits the system-
level reachability analysis as well as the DIFT.
We design the cross-host tagging method based on the
characteristics of the socket protocols. Our current tag-
ging scheme supports the two major types of protocols
(i.e., TCP [54] and UDP [53]). For TCP, as the data stream
delivery is guaranteed between the two hosts, we rely on
the order of bytes in the TCP session between source and
destination to identify the data flow at byte level, which
can be uniquely identified using a pair of IP addresses
and port numbers. Such tracking silently links the out-
bound traffic from the source host with the inbound traffic
at the destination host, which does not incur additional
traffic. Note that although TCP regulates the data stream
order, the sender or receiver may run different numbers
of system calls in sending and receiving the data. For ex-
ample, the sender may perform five writev() system
calls to send 10,000 bytes of data (2,000 bytes each call),
while the receiver may conduct 10 read() calls (1,000
bytes each call) to retrieve the complete data. This is why
counting sent or received bytes is necessary, instead of
counting the number of system calls.
In the case of UDP, since the data delivery is not guaran-
teed, some UDP packets could be lost during transmission.
So we cannot rely on the order of transferred bytes be-
cause the destination host has no knowledge of which
data are supposed to arrive and which have been lost. To
support UDP, we embed a small “cross-host” tag at each
send related system call by the source host, and parse the
tag at receive related system calls by the destination
host. The tag is inserted into the beginning of the data-
gram as a part of the user datagram before the checksum
is calculated. If the datagram is transferred successfully,
RTAG knows a certain length of data goes from the source
to the destination. If the destination host finds the re-
ceived datagram is broken, or totally lost, it will discard
this datagram, hence RTAG is also aware of the loss and
erases this inbound data from the reachability analysis
and DIFT. As we will show in §8, the communication cost
for TCP case is 0, while the cost for UDP is also marginal
in the benchmark measurement.
The cross-host tag represents the byte-level data in
the socket communication between two processes across
hosts. Each tag key represents the data traffic in one
socket session using the source and destination process
credentials, plus the offset that indicates the data at
byte level. For the uniqueness of session, we use the
process identifier (pid) and the process creation time
(start_time in the task structure) to identify each
process. The tag values represent the origin of the tag
key, which is determined by the DIFT and updated to the
global tag map. The cross-host tags are also switched
away before DIFT is performed and restored afterward.
For the hosts on which RTAG does not run, we treat them
as a black box, and identify them using the IP address
and port number. The IP and port are retrieved from the
socket structure inside the kernel.
Handling IPC. RTAG tracks the data transfer of IPC
communication between two processes as well. For
the IPC that uses system call as a controlling interface
(e.g., pipe, and System V IPC: message queues,
semaphores), RTAG hooks these system calls to track
the data being transferred. When a process uses pipe to
send data to the child process, RTAG monitors the read
and write system calls to track the transferred data in
bytes. During reachability analysis, we create tag keys
to label every byte sent from the parent to the child. The
tag values are fulfilled by DIFT. For example, in Figure 2,
although the git pack and ssh processes have IPC de-
pendency, RTAG is able to perform the replay and DIFT
independently on them since RTAG caches the inbound
data reads from the pipe and feeds them back during the
replay. Also, by tracking the inode associated with the
file descriptors (rather than tracking pipe, dup(2)
and child inheritance relationships), we identify the data
transmitted via the pipe at byte level and the processes
at its two ends. RTAG implicitly tracks the IPC based
on shared memory. Instead of trapping the replay of a
process for each read from a shared memory, RTAG re-
plays the processes having shared memory as a group as
RAIN [35] and Arnold [25] do, so that the tag propagation
of this shared memory is performed within the process’
memory locations. No separate tag allocation is needed
for these processes.
6.7 Query Results
The query result will be returned after all the tag values of
the interfering data are updated. The result represents the
data causalities of involved objects in a tree structure. For
example, in Figure 2, a backward query on the attacker’s
controlled host 5.5.5.5:22 will return the tree-shape
data flow overlay depicted in Figure 2(b), consisting of
all the segments of the flow from the key to all of its
upstream origins. Also, a forward query returns every
segment of the data flow from the queried tag key to all
of its impact(s). It relies on a reversed map where the
tag key and value are swapped to locate the downstream
impact from a file. For example, a forward query on the
private key id_rsa on the client side returns a flow:
id_rsa→results.v1→objects→5.5.5.5:22.
A point-to-point query gives the detailed data flow be-
tween two nodes in the provenance graph by performing
a forward and backward query on these two nodes, then
computing the intersection of the two resulting trees.
USENIX Association
27th USENIX Security Symposium    1713
7
Implementation
The implementation of RTAG is based on a single-host re-
finable information flow tracking system RAIN [35], with
extended development of the tagging system. Specifi-
cally, our implementation adds 830 lines of C code to
the Linux kernel for the tag association module, 2,500
lines of C++ code to the DIFT engine for the tag switch
mechanism, 1,100 lines of C++ code for the maintenance
of tags, 900 lines of C++ code for the query handler, and
500 lines of Python code for the reachability analysis for
tag allocation. Currently, RTAG runs on both the 32-bit
and 64-bit Ubuntu 12.04 LTS. Accordingly, our DIFT en-
gine supports both x86 and x86_64 architectures, which
is based on libdft [37] and its extended x86_64 version
from [43]. We use a graph database Neo4j [10] for stor-
ing and analyzing coarse-level provenance graphs, and a
relational database PostgreSQL [3] for global tags with
multiple indexing on host (i.e., MAC address) and file
credentials (i.e., inode, dev, crtime). Particularly,
we supplement the tag data structure §6.4 and how we
track socket session §6.6 with implementation details in
the following.
Tag Data Structure.
In the current implementation,
RTAG maintains local tags for individual bytes. RTAG
uses C++’s vector as the multi-tag container for one
memory location or register and uses sorting and bi-
nary search in the case of insert operation. vector
has storage efficiency, although its insertion overhead is
higher than that of the set data structure, which was
used by DataTracker [61]. We make this choice based on
x86 instruction statistics [4] that show the most popularly
used instructions are mov, push, and pop of which the
propagation policy copies the tag(s), while instructions
that involve insertion, such as add and and, are much
less frequent. Our evaluation affirms this choice that the
time overhead for single DIFT is similar between RTAG
and previous work [61].
Tracking Socket Session.
The implementation of
tracking the socket communication session refers to the
socket structure inside the kernel for IP and port of the
host and the peer. If the type of socket is SOCK_STREAM
(i.e., TCP), we use a counter counting the total num-
ber of bytes sent or received by tracking the return
value of send or write system calls. If the type is
SOCK_DGRAM (i.e., UDP), our implementation embeds
a four-byte incrementing sequence number within the
same peer IP and port number at the beginning of the
payload buffer inside an in-kernel function sendmsg
rather than the system call functions such as send and
recv to avoid affecting the interface to the user program
as well as the checksum computation. At the receiver
side, we strip the sequence number in the recvmsg after
the checksum verification and present the original pay-
load to the program. As shown in §8.2.3, the hooking at
this level incurs almost no overhead in either bandwidth
or socket handling time. It also avoids the complicated
fragmentation procedure at the lower level.
8 Evaluation
Our evaluation addresses the following questions:
• How well does RTAG handle the data flow queries
(forward, backward, and point-to-point) for cross-
host attack investigations? (§8.1)
• How well does RTAG improve the efficiency of DIFT-
based analysis in terms of time and memory con-
sumption? (§8.2.1)
• How much overhead does RTAG cause to system
runtime including the network bandwidth? (§8.2.2,
§8.2.3) What is the storage footprint of running
RTAG? (§8.2.4)
Settings. We run RTAG based on the Ubuntu 12.04
64-bit LTS with 4-core Intel Xeon CPU, 4GB RAM and
1TB SSD hard drive on a virtual machine using KVM [14]
for the target hosts where system-wide executions are
recorded. On the analysis host, we use a machine with
8-core Intel Xeon CPU W3565, 192 GB RAM, and 2TB
SSD hard drive installed with Ubuntu 12.04 64-bits for
handling the query and performing DIFT tasks in parallel.
We use NFS [15] to share the log data between the target
and the analysis host.
8.1 Security Applications
Table 1 summarizes the statistics in every stage of pro-
cessing a query for an attack investigation: the original
provenance graph covering all the hosts, the pruned graph
where the unrelated causalities are filtered out by the
reachability analysis, and the data flow overlay where the
tags store the origins of each byte of data involved in the
query. Table 2 also summarizes how long each of the
queries took and their memory consumption.
8.1.1 GitPwnd
We first present how RTAG handles the queries on the
Gitpwnd example (described in §3.1). To handle a query,
we replay the involved processes independently based on
reachability analysis results while performing DIFT on
the interfering parts. We run RTAG on both client and
server hosts involved in this attack, while treating the
attacker-controlled host as a black box. We perform three
queries: a forward query asking for where the leaked
/etc/passwd goes to, a backward query inquiring the
sources of data flow that reaches the attacker’s controlled
host, and a point-to-point query aiming to particular data
1714    27th USENIX Security Symposium
USENIX Association
Items
Query
FW: /etc/passwd
BW: attacker host
PP: results - objects
FW: exploit html
BW: payroll record
PP: html - db file
FW: db file
BW: dump file
FW: exploit html
BW: salary record
FW: exploit html
BW: attacker host
PP: html - a-host
Attack
GitPwnd
SQLi-1
SQLi-2
CSRF
XSS
P2P
BW: mp4@12th node
FW: mp4@1st node