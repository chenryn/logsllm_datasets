(b)
Fig. 3: Study of the computational gain achieved via the
structure-preserving heuristic in Sec. VII-E.
heuristic, we compute Y for the campus network in Sec. VII-C
with and without the collision check subroutine in Algorithm
1, lines 14 to 15. Figure 3a plots the cardinality of Y (y axis)
over the simulation time ta (x axis) under a simulation time
budget of Ta = 80 seconds. We can see that the collision
check subroutine reduces the cardinality of Y by 90x without
compromising its representativeness. As a result, this speeds
up to, the time to solve the MWM optimization problem, since
the cardinality of Y is the number of constraints in (10). Figure
3b plots to (y-axis) as a function of the cardinality of Y (x-
axis). Although to depends on the random initialization of
the convex optimization solver, on average to grows linearly
with respect to the size of Y. Therefore, a 90x reduction in
the cardinality of Y roughly translates to a 90x reduction in
to. Note that the computational saving depends on the degree
of symmetry in the model and may vary from application to
application.
VIII. DISCUSSIONS
In this section, we critically discuss some concerns and
limitations of the MWM method and threats to the validity
of the simulation results in Sec. VII.
1) Identifying monotonicity and symmetry: An avid reader
may ask, “how do we identify monotonicity and symmetry in
the objective function f?”. We believe that in many rare event
simulation applications, including the case studies in Sec. VII,
the user can derive these properties by hand or predict them
using domain knowledge about the system. For more complex
models, where monotonicity and symmetry cannot be easily
established, Monte Carlo simulation can be used to try and
test the hypothesis that these properties exist, and where it
may exist. Moreover, the user may rely on domain knowledge
to narrow key variables that affect the system performance,
thereby improving the simulation efﬁciency. However, it is
entirely possible that f is highly nonlinear and neither mono-
tonicity nor symmetry can be established. In such cases, even
ﬁnding one rare event sample can be a computationally hard
problem, since it may require an exhausive search over of all
possible combinations of the model input. Problems of this
kind generally fall under the category of falsiﬁcation-based
model veriﬁcation [40], [7], and in a slightly different context,
fault-injection testing [41].
2) Relaxing the monotonicity and symmetry assumptions:
Even when monotonicity and symmetry are easily idenﬁable,
they remain strong assumptions and can be restrictive when
applying to real systems. A follow-up question would be, “can
MWM be relaxed to deal with partial monotonicity or partial
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:18:29 UTC from IEEE Xplore.  Restrictions apply. 
314
Setting Population mean REMWM RECMC
1.528
>10
>10
>10
2.003 × 10−6
1.999 × 10−8
2.000 × 10−10
1.867 × 10−8
0.066
0.033
0.028
0.034
(i)
(ii)
(iii)
(iv)
TABLE VI: Performance of rare event estimators in Sec. VIII.
Fig. 4: A k×k undirected lattice where links that share the
same color belong to the same semi-EG.
symmetry?”. Here we lay out the arguments for both cases.
If the objective function is only monotone with respect to a
subset of its variables, then the main idea behind MWM still
applies, but the change-of-measure will not be applied to non-
monotone random variables. In other words, the computational
gain is achieved by only applying IS to monotone variables.
On the other hand, if the model components that are deemed
identical possess some degree of variability, or if they are
not identical but instead functionally similar, our numerical
results suggested that the order-preserving heuristic can also
be applied. In this case, variables that are not equivalent under
f can still be grouped into so-called semi-equivalent groups
(semi-EG). The additional constraints imposed by the semi-
EG partitioning does not invalidate MWM because in theory,
any proposal distribution can be used as long as it satisﬁes the
basic condition.
To illustrate the idea, we use MWM to estimate the s, t-
unreliability of a k×k undirected lattice network (Fig. 4)
where k = 20 and all links share the same failure probability
. We deﬁne the semi-EG partitioning π = (π1, π2, . . . , π5)
where links that are i hops away from either s or t belong to
πi for 1 ≤ i ≤ 4. The rest of the links are grouped into π5.
−3,
Using π, we evaluate MWM under three settings (i)  = 10
−5. To test π’s ability to
(ii)  = 10
handle variability in the model parameters, in setting (iv), we
modify setting (ii) by sampling the link failure probabilities
in [0.5×10
−4] uniformly at random. As before, the
MWM estimator is computed using M = 104 samples. The
results in Table VI show that MWM estimators achieve low
REs of less than 7% under all settings. In settings (i)-(iii), the
RE decreases as the s, t-unreliability decreases. Moreover, the
RE in setting (iv) is almost the same as the RE in setting (ii),
despite the fact that the link failure probabilities vary by up to
50%. This is just one example, but it illustrates the possibility
of relaxing the symmetry assumption in MWM.
−4, and (iii)  = 10
−4, 1.5×10
Another beneﬁt of the semi-EG partitioning is that it allows
for more aggressive dimensionality reduction. In the above
example,
the partitioning π reduces the dimension of the
problem of computing the proposal distribution for sampling
the k×k lattice from 2k(k + 1) = 840 to 5, despite the fact
that in settings (i)-(iii), only links in π1 (colored in red in Fig.
4) are truly equivalent. In contrast, to maintain the equivalence
relationship, the links must be partitioned into k(k+1)
2 = 210
EGs. For a large k, a 4x dimensionality reduction might not be
enough to bring the problem within the computational budget.
3) Assessing the quality of the MWM estimators: In any
case, if the user wrongly assumes monotonicity and symmetry
when they are absent, then the performance of the resulting
MWM estimator may not improve, but asymptotic conver-
gence is still assured. This brings up the next question, “is
there a way to quantify how far off from optimal the erroneous
assumptions lead us to?”, or to put it slightly differently,
“can we measure the quality of the MWM estimators?”. The
latter question is also relevant under situations where the best
proposal distribution in PΘ does not sufﬁciently approximate
P∗, even when f is fully monotone and symmetric. The
answer to this question is yes, we can empirically measure
the quality of the MWM estimators based on (i) how often we
collect a rare event sample—i.e., the hitting rate—and (ii) the
∗
ratio between the minimized maximum weight—i.e., wθ∗ (x
)
∗ are the solutions to (9)—and the value of the
where θ
IS estimator ρIS. By monitoring these two readily-computable
metrics, we can experiment with any heuristic while being able
to monitor the quality of the resulting MWM estimators. In
particular, a low hitting rate implies a small effective sample
)/ρIS implies that
size. On the other hand, a large ratio wθ∗ (x
the weight distribution is fat-tailed, which results in weight
degeneracy. Both of them result in a high RE and command
a large sample size.
∗ and x
4) Threats to validity: Readers who noticed that the case
studies in Sec. VII were chosen to highlight the performance
of MWM may be curious to know, “under what circumstances
does the MWM method fail to produce good proposal distri-
butions?”. From our simulation experience, MWM tends to
perform better under reliability models where the rare event
happens due to the occurrence of a relatively small number of
events [12]. In contrary, when the rare event happens due to the
occurrence of many events, each of which is not particularly
rare (e.g., percolation models [42]), the MWM estimator can
have a low hitting rate, a large minimized maximum weight
), and as a result, a high RE. Although we can boost the
wθ∗ (x
hitting rate by tilting the proposal distribution toward the rare
event region, doing so may exacerbate the weight degeneracy
issue and incur an even larger RE. (The trade-offs between
these two metrics on the performance of an IS estimator is
exempliﬁed in Sec. E.) We suspect the choice of mutually
independent random variables for proposal distributions might
not be suitable for such applications. This is because even
∗ ∈ Θ that minimizes the maximum
though MWM can ﬁnd θ
weight, the resulting proposal distribution Pθ∗ may not be
sufﬁciently close to P∗ to produce a good IS estimator. Future
research can explore the possibility of improving MWM using
jointly distributed proposals.
Finally, while MWM shows a major improvement over
crude Monte Carlo for events of low probabilities, comparison
with other rare events simulation techniques suggests some
improvement but
is not conclusive, due to differences in
efﬁciency of computation platforms and software implemen-
tations. In particular, in Sec. VII-A, the generalized splitting
method took 167, 224, and 278 seconds to produce M = 106
samples under setting (i), (ii), and (iii), respectively (ob-
tained from Table 2, [35]). In comparison, MWM only took
65.3 + 0.2 + 100 × 0.5 = 115.5 seconds under all settings.
∗
∗
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:18:29 UTC from IEEE Xplore.  Restrictions apply. 
315
However, the timing data also depends on the computation
platform, which was not reported in [35]. On the other hand,
in Sec. VII-B,
the GS and PMC method took 15 to 26
seconds to produce M = 5 × 104 samples under settings (i)
and (ii) (calculated from Table 1, [26]). In contrast, MWM
took 60.3 + 0.3 + 22 = 82.6 seconds to produce the same
number of samples. However, to reduce the RE to MWM
level, both GS and PMC need to increase the sample size
by at least 9 times, bringing up the simulation time to 135 to
234 seconds. Nevetheless, this comparison is relative since the
experiments in [26] were run on Intel Xeon E5-2680 CPUs on
a Linux cluster, which is a different hardware platform than
ours. Besides, our code can be further optimized by replacing
networkx with a more efﬁcient package, which can reduce
the run time of graph computation by 10x to 300x [43].
IX. RELATED WORK
A. Rare event simulation
A summary of rare event simulation can be found in [10],
[44]. One of the earliest work can be traced back to [45].
Other popular ideas include the cross-entropy (CE) method
[14], [19], [16] based on minimizing the Kullback-Leibler
distance from the optimal proposal distribution, population
Monte Carlo method [20] (not to be confused with permutation
Monte Carlo [46], [22]) and its many variations [11] based
on Markov chain Monte Carlo, zero-variance approximation
(ZVA) method based on graph analysis [2], [23], [4], and
generalized splitting (GS) method [25], [26], [22] based on the
splitting technique in [45]. CE and population Monte Carlo
are adaptive IS methods that can also be used for variance
reduction and combinatorial optimization. ZVA is capable of
achieving bounded (or vanishing) RE, where the RE of the
estimator remains bounded (or vanishes) as the probability
of the rare event approaches zero. Unlike IS, splitting-based
techniques like GS do not use proposal distributions; instead,
they work by decomposing a sample path to shorter subpaths
and “reuse” subpaths that are more likely to visit the rare
event.
B. Importance sampling
As a powerful rare event simulation technique, IS has
been successfully applied to queuing systems [12], reliability
analysis [2], [39], stochastic ﬂow analysis [26], [22], dynamic
fault tree analysis [23], [47] cyber-security risk assessment [4],
risk assessment of power system operation [5], safety assess-
ment of self-driving vehicles [6], and many other applications.
The basic idea behind IS is to sample the input parameters
under a different (proposal) distribution and later multiply the
simulation results with a factor to recover unbiasedness. This
technique, if done right, can produce accurate estimates of
rare event probabilities using fewer samples than a crude MC
approach. However, using improper proposal distributions, IS
may suffer from the weight degeneracy problem [20], [13],
[11]. This results in badly behaved estimators [10], [2] and is
commonly attributed to the curse of dimensionality [13], [11].
Similar to popular IS methods, MWM works by approximat-
ing Xγ, the set of rare event samples. However, unlike adaptive
IS methods (CE and population Monte Carlo) and GS, MWM
does not rely on an iterative method to perform the approxima-
tion. This is useful when the simulation is costly, or when the
problem dimension is high, under which convergence can be
difﬁcult to achieve. Instead, by extracting properties from the
objective function f, MWM and its two accompanying two
heuristics can reduce the problem dimension and lessen the
effect of weight degeneracy. From this perspective, MWM is
more related to the screening method in [13], the neural bridge
sampling technique in [7], and more recently, the black-box
rare event simulation framework in [18].
X. FUTURE WORK
In this paper, we presented a novel IS method for rare event
simulation based on the idea of minimizing the maximum
weight under a proposal distribution. Extensive simulation
results demonstrated the applicability of MWM to a wide
range of estimation problems and compared its performance to
state-of-the-art techniques. There are several ideas for future
extensions. First, we can explore the possibility of improving
the quality of the MWM estimator by considering the class of
jointly distributed proposals. Second, while our initial study
suggested that the symmetry assumption can be relaxed to
capture components that are not
identical but functionally
similar, extensive simulation results are required to fully
understand the performance of MWM in a realistic setting and
with a partially monotone objective function. Third, although
the cardinality of Y is kept small owing to the reﬁnement via
the structure-preserving heuristic, the connection between the
cardinality of Y and the RE of the MWM estimator is not
fully understood, and the theoretical bound on the RE is not
yet established. These issues will be addressed in the future
work.
ACKNOWLEDGEMENTS
We thank the anonymous reviewers and the shepherd for
their helpful comments and suggestions. This material is based
upon work supported by the Maryland Procurement Ofﬁce
under Contract No. H98230-18-D-0007.
APPENDIX
A. Generalization to other distributions
The results in Propositions IV.1, IV.2, IV.3, and V.1 hold
for other distributions in the exponential family, namely, the
categorical, Beta, and a modiﬁed version of the exponential
distribution. Some details are given below.
1) Categorical distributions: This distribution generalizes
the Bernoulli distribution by allowing each Xi to assume one
of Ki ≥ 2 discrete values. To shift the densities of Xi’s to
the tails, we apply the exponential tilting [12] technique—i.e.,
under Pθ, the random variable Xi is distributed according to
Pθ(Xi = xi) =
eθixi P(Xi = xi)
MXi (θi)
(13)
where MXi is the moment generating function of Xi. To main-
tain the order-preserving heuristic, the necessary condition is
simply θi ≤ 0 for all i ∈ [m]. The exponential tilting technique
also works for other choices of distributions.
2) Beta distributions: Suppose PΘ is the class of Beta-
distributed proposals and P ≡ Pa,b is an element of PΘ,
i.e., a = (a1, a2, . . . , am), b = (b1, b2, . . . , bm) and Xi ∼
Beta(ai, bi) for all i ∈ [m]. Let θ = (a
) ∈ Θ and θi =
(cid:2)
(cid:2)
, b
i) for all i ∈ [m]. Iff
i), i.e., under Pθ, Xi ∼ Beta(a
(cid:2)
(cid:2)
(cid:2)
(cid:2)
i, b
i, b
(a
is monotonically non-decreasing in xi for some i ∈ [m] then
i ≤ ai and
(cid:2)
under the stochastic ordering heuristic, we have a
i ≥ bi. This allows us to establish the results in Propositions
(cid:2)
b
A.1 and A.2, which are the continuous analogy to the results
in Propositions IV.2 and IV.3. These results also hold for the
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:18:29 UTC from IEEE Xplore.  Restrictions apply. 
316