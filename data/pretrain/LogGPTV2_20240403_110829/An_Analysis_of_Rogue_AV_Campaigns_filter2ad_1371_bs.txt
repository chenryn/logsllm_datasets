what is essentially a client-side honeypot [25,24]. That is, we run a web browser
(in our case Microsoft Internet Explorer) on a Windows operating system in a
virtual machine, and we script it to automatically download a list of URLs via
a consumer-grade cable modem connection to the Internet. For each URL in
the list, we retrieve the page using the honeyclient and record the full contents
of each packet that is generated. We revert the VM to a clean state after each
page retrieval. For broad coverage of the Web, we begin with a list of over ten
thousand URLs from a categorized directory of links such as Mozilla’s Open
Directory Project [50] or Yahoo!’s directory [51], as well as lists of the most
popular URLs from the Alexa.com rankings [52]. For increased realism, we can
script the honey client to “crawl” more links to provide increased depth for a
given interactive site.
Then, we perform TCP stream reassembly on the captured packets to recon-
struct the application-layer conversations for each URL and all of the associated
HTTP sessions. For each HTTP request in the collected traces, we store the full
text of the resulting HTTP response, including both the headers and the body,
in a database, keyed based on the hostname used in the request and the URL
requested, as well as some meta-information from the headers such as transport
and content encoding parameters.
Emulating the Web. On the testbed network, we deploy a very simple web
server program to emulate the Web using the data collected above. Upon re-
ceiving an HTTP request, it ﬁrst parses out the hostname, URL, and other pa-
rameters, then looks up the corresponding HTTP response text in the database,
and ﬁnally sends this response back to the client. The content from the database
can be distributed across several web servers to provide the ability to handle
large traﬃc loads as well as provide more realistic network characteristics for
the traﬃc.
To give the impression of a much larger network than can be realistically
installed on a testbed, we typically deploy this web server on several Linux
228
C.V. Wright et al.
machines, each conﬁgured with hundreds or even thousands of virtual network
interfaces for each physical interface. Each machine can thus respond to HTTP
requests sent to any one of thousands of IP addresses. Each instance of the
web server application listens on a designated subset of the host’s IP addresses
and serves content for a designated set of web sites. This ﬂexibility enables
us to emulate both very simple sites hosted at a single IP address as well as
dynamic, world-wide content distribution networks. We store the mapping from
hostnames to IP addresses and Linux hosts in the testbed’s central LARIAT
database. There, this information is also used to conﬁgure the testbed’s DNS
servers, so that client nodes can resolve hostnames to the proper virtual IP
addresses. We also provide artiﬁcial Root DNS servers as well as a core BGP
routing infrastructure to redistribute all of the routing information for these IP
addresses.
Discussion and Limitations. This combination of lightweight application-level
replay on the server side with automation of heavyweight GUI applications
on the client side allows us to generate very high-ﬁdelity network traﬃc for
many use cases. It requires no parsing or understanding of JavaScript, but many
JavaScript-heavy sites can be emulated using this method and appear fully func-
tional from the client’s perspective, limited only by the extent of the data col-
lection. One notable example of such a site is Google Maps.
However, the focus on light weight and eﬃciency in our server-side replay
techniques leads to some important limitations of the current implementation.
First, because the server is stateless, it cannot do HTTP authorization or any
customization of page content based on cookies. Second, because it only looks
for exact matches in the URLs, some pages that dynamically generate links may
fail to ﬁnd a matching page when run on the testbed. Pages that dynamically
choose IP addresses or hostnames for links may need to be fetched tens or even
hundreds of times during the data collection step in order to ﬁnd all IP addresses
or hostnames that should occur in the linked pages’ URLs. Otherwise, any client-
side JavaScript code that uses random numbers to control its actions (e.g. client-
side load balancing) will fail to function given that previously unrequested URLs
will not be found in the new closed environment. Finally, while our approach
could be almost trivially extended to support the concurrent use of multiple
browsers or multiple operating systems, it does not do so currently.
Despite these limitations, the current techniques are suﬃcient for many kinds
of experiments involving network traﬃc. They are also valuable for tests that
focus primarily on host behavior, as they enable a wider range of applications
to be run on the host, most notably the web browser. In the next section, we
walk through a simple experiment with a host-based security system where use
of the browser makes up a signiﬁcant fraction of the client machine’s workload.
4 An Example Experiment
In this section we walk through a simple experiment as an example of the kind
of test our system enables a researcher to perform. Although the underlying
Generating Client Workloads and High-Fidelity Network Traﬃc
229
LARIAT test range automation tool and the AUSM-based workload generators
are capable of scaling to hundreds or even thousands of nodes, for ease of expo-
sition, we will limit ourselves to a much more limited test scenario in this paper.
Despite its small scale and relative simplicity, we believe this experiment is still
complex enough to illustrate the diﬃculties in applying the scientiﬁc method to
problems in computer security.
Speciﬁcally, the goal of our example experiment is to measure and quantify
the performance penalty incurred by running anti-virus protection on desktop
computers. In principle, very nearly the same experiment could be used to mea-
sure the performance impact of many other security tools, including other kinds
of intrusion detection systems such as content-based ﬁlters, or many types of
malware like rootkits, adware, spyware, or key loggers. We chose to measure the
impact of AV systems because (1) they are ubiquitous on Internet-connected
machines, and (2) because although users have long complained that AV nega-
tively impacts system performance, very little hard data has been presented to
either refute or support this claim.
Hypothesis. We begin by deﬁning a falsiﬁable hypothesis. A simple statement
such as “anti-virus makes the system slow” is not a good hypothesis because
slowness is subjective and is therefore not measurable without a substantial user
study; it depends not only on the performance of the system but also on the
perception of the user. Instead, we use a similar hypothesis that we hope will
be a good predictor of perceived slowness, namely that “anti-virus increases the
system’s resource consumption.” In related work, others have tested a similar
hypothesis, namely that “anti-virus increases the time required to complete a
suite of computational tasks” [53,54].
4.1 Testbed Setup
We use a very simple experimental testbed comprised of two physical machines.
One of these machines is a server-class machine (HOST) which we use to pro-
vide the LARIAT infrastructure. HOST is a Dell PowerEdge 2650 with dual
Intel Xeon 2.60GHz processors and 2GB of RAM. On HOST, we deploy two vir-
tual servers using VMWare Server. One of these is the LARIAT control server
(CONTROL) and the other (INTERNET) provides email, DNS, and world-wide
web services (Section 3.2) on the testbed for use by the system under test. The
second machine in our setup is a Dell Latitude D610 laptop (SUT, for “system
under test”) with a 1.7GHz Pentium M processor and 1GB of RAM. We par-
tition the hard disk of the SUT into two pieces. On one partition, we install
Ubuntu Linux 9.10. On the other, we install Windows XP with Service Pack
2, Microsoft Oﬃce XP, and our client-side workload generation tools, includ-
ing the login module, the user agent, and the AUSMs for Internet Explorer,
Word, Excel, PowerPoint, and Outlook. To enable the collection of performance
measurements from the system under test, we install components of SGI’s Per-
formance Co-Pilot (PCP) software [55] on the Windows partition of the SUT,
230
C.V. Wright et al.
Fig. 3. Test Network Physical Topology
where it can collect performance information, and on HOST, where it can log
these measurements for future analysis. We also install the winexe remote ad-
ministration tool on HOST so that we can automatically reboot the laptop when
it is in Windows.
Then, from the SUT’s Linux installation, we use the Unix tool dd to make a
byte-level image of the Windows partition and store this as a ﬁle in the Linux
system. We then re-boot into the Windows system and install an open source
anti-virus product, reboot back into the Linux system and make another byte-
level copy of the Windows partition with the open source AV product installed.
At the completion of this process, we have two Windows disk images on the Linux
ﬁlesystem: (1) a clean snapshot of the Windows system, with no AV software
installed and (2) a snapshot of the clean image with the open source product
installed.
4.2 Experimental Methods
In Section 3.1, we explained how we use pseudorandom number generators to
drive the AUSMs to deliver repeatable user inputs to the application programs
on the testbed. However, more steps are necessary to achieve repeatable results
from a system as complex as a modern computer or network. In this section,
we discuss the steps we take to improve the repeatability of our very small,
simple example experiment. First, we note that we intentionally deploy the SUT
on a physical machine instead of using virtualization because of diﬃculty in
obtaining repeatable timing measurements on current VM platforms [56]. Our
experimental procedure is outlined in Figure 4; we describe the process in more
detail below.
Generating Client Workloads and High-Fidelity Network Traﬃc
231
1. Prepare Systems for Test Run
(a) Revert disk images on SUT and INTERNET
(b) Revert system clocks on SUT and INTERNET
(c) Reboot SUT laptop into Windows environment
(d) Seed PRNGs using master experiment seed
(e) Start PCP performance logging service
2. Execute Test Run
(a) Start AUSM-based client workload generation
(b) Let workload generation run for 2 hours
(c) Stop AUSM-based client workload generation
3. Collect Results
(a) Stop PCP performance logging service
(b) Archive performance logs
(c) Reboot SUT laptop into Linux environment
Fig. 4. Experimental Procedure
When running a test, we begin with the SUT booted into its Linux environ-
ment, HOST powered up, and the INTERNET and CONTROL virtual machines
running. We revert the disk images on INTERNET and SUT’s Windows par-
tition to clean states, using VMWare’s snapshot feature and the Unix dd tool,
respectively. This is necessary to ensure that SUT’s ﬁlesystem is unchanged from
run to run, and that its view of the Internet, including queued email in the user’s
inbox, is consistent in each run. Next, we set the system clocks on SUT and IN-
TERNET to a constant value. Setting the clocks before each run is important
because many application programs also use pseudorandom number generators,
and many of them seed their PRNG with the current value of the clock when
they start up. Finally, we set the GRUB boot loader on the SUT to load Windows
on its next invocation, and we reboot the laptop. While the laptop performs
its power-on self test and boots into Windows, we start the PCP performance
logger on HOST so that it is ready to begin recording performance metrics from
SUT when it comes on line. To maximize repeatability, all of these actions are
performed automatically by a set of three shell scripts. One script, the master,
runs on HOST and executes the other two scripts to re-set the SUT and the
INTERNET VM before launching each run of the experiment.
In each run, the master script sends a command to CONTROL to start the
experiment, then sleeps for a speciﬁed length of time in order to let the test run.
Meanwhile, CONTROL sends the command to SUT, which logs in the virtual
user and starts the AUSMs to drive the client-side applications on the testbed.
When the master script wakes up, it sends another command to CONTROL,
stopping the test and logging the user out. It archives the PCP performance
logs and then uses winexe to reboot the laptop into its Linux environment in
preparation for the next run.
4.3 Experimental Results
Using the above procedure, we select a master experiment seed at random and
repeatedly execute a 2-hour test on our testbed with this seed. We run the test
232
C.V. Wright et al.
Fig. 5. CPU Utilization
under two distinct scenarios: (1) the baseline, with no anti-virus protection on
the SUT and (2) with the open source anti-virus product. These experiments
serve two purposes. First, they allow us to test whether there is any measurable
diﬀerence in system resource consumption between the two scenarios. They also
serve to demonstrate the repeatability of results enabled by our techniques.
With the experiment seed that we selected, the user agent launches both
Outlook and Word soon after logging in. It spends several minutes reading and
writing email and writing Word documents, then launches Internet Explorer and
browses to several sites on the emulated web. It keeps these three applications
open for the duration of the experiment and frequently switches back and forth
between them every few minutes.
For each of the two scenarios, we take all of the runs, and show the average
and standard deviation of the CPU utilization (Fig. 5), memory consumption
Fig. 6. Memory Consumption
Generating Client Workloads and High-Fidelity Network Traﬃc
233
Fig. 7. Disk I/O
(Fig. 6), and disk input/output volume (Fig. 7) for the ﬁrst hour of the experi-
mental runs. The data was gathered in one second intervals using PCP. In order
to make these plots more readable, we have performed a Gaussian smoothing
over the data with a 5 second radius. In eﬀect, this smooths out some of the
jagged edges in the plot, making them more readable without changing them in
any signiﬁcant way.
Discussion. We see consistent spikes in both CPU load and disk I/O starting
near 0 seconds when the user agent launches Outlook and Word, and again
near 600 seconds when Internet Explorer is started. In Fig. 5, we see that the
open source AV product consistently causes near 100% CPU use for a period
of nearly 10 minutes. During this same period, the standard deviation of the
CPU utilization is near zero, indicating that this behavior is very repeatable.
Throughout Fig. 5 and Fig. 7, spikes in the average measurements are typically
accompanied by smaller spikes in the standard deviations. However, we note
that during periods of sustained activity, the standard deviation only spikes at
the beginning and the end of the plateau in the mean. This pattern occurs in
Fig. 5 at 600-1000 seconds, 1200-1400 seconds, and 1500-2500 seconds for the
open source product and to a lesser extent from 1800-2000 and 2300-2500 seconds
for the baseline case. This leads us to believe that much of the variance in these
graphs is due to the inexact timing of our automation scripts.
Figures 5 and 6 show clear evidence of the system with open source anti-
virus protection consuming more resources than the same system with no anti-
virus, and formal statistical tests conﬁrm our hypothesis with high conﬁdence
for these data series. In Fig. 7, overall, the anti-virus system does not appear
to cause a statistically signiﬁcantly increase in disk I/O loads relative to the
baseline system. We are interested in whether these same results would hold for
commercial anti-virus products, which may be developed with a greater focus
on eﬃciency and performance. In the near future, we may expand the coverage
of our test to include one or more commercial tools as well.
234
C.V. Wright et al.
5 Conclusions and Future Work
We presented new techniques for driving ubiquitous, commercial-oﬀ-the-shelf
Windows GUI applications in place of a human user on an isolated testbed
network. Using statistical models of user behavior to generate workloads on the
testbed hosts, together with dynamic application-level protocol replay techniques
to emulate remote services like the World Wide Web, we can generate traﬃc on
the testbed network that resembles real traﬃc to a very high degree of ﬁdelity.
We demonstrated a small-scale experiment to show how these techniques help
to enable conﬁgurable, repeatable tests involving client-side security tools, and
we highlighted some challenges in achieving repeatable experimental results with
such complex systems.
In the future, we plan to improve on our current techniques in a number of
ways. First, we will collect data from a larger set of users and develop techniques
for validating that the workloads and traﬃc induced on a testbed faithfully
represent the environments they were modeled after. Second, we will develop
actuators that require a smaller software footprint on the system under test,
to further reduce the risk of test artifacts in experimental results. Finally, we
plan to develop more robust techniques for dynamic application-level replay of
web sites that make heavy use of JavaScript or other active content generation
techniques.
Acknowledgments
The authors extend our sincerest thanks to the members of the MIT-LL Cyber
Testing team who implemented much of the software described here and provided
much helpful feedback on the experiments and the paper.
References
1. Barford, P., Landweber, L.: Bench-style network research in an Internet Instance
Laboratory. ACM SIGCOMM Computer Communication Review 33(3), 21–26
(2003)
2. Peisert, S., Bishop, M.: How to Design Computer Security Experiments. In: Pro-
ceedings of the 5th World Conference on Information Security Education (WISE),
pp. 141–148 (2007)
3. US Department of Homeland Security: A Roadmap for Cybersecurity Research.
Technical report (November 2009),
www.cyber.st.dhs.gov/docs/DHS-Cybersecurity-Roadmap.pdf
4. White, B., Lepreau, J., Stoller, L., Ricci, R., Guruprasad, S., Newbold, M., Hibler,
M., Barb, C., Joglekar, A.: An integrated experimental environment for distributed
systems and networks. In: Proceedings of the 5th Symposium on Operating Systems
Design and Implementation (December 2002)
5. Ricci, R., Duerig, J., Sanaga, P., Gebhardt, D., Hibler, M., Atkinson, K., Zhang, J.,
Kasera, S., Lepreau, J.: The Flexlab approach to realistic evaluation of networked
systems. In: Proceedings of the 4th USENIX Symposium on Networked Systems
Design & Implementation, pp. 201–214 (April 2007)
Generating Client Workloads and High-Fidelity Network Traﬃc
235
6. Vahdat, A., Yocum, K., Walsh, K., Mahadevan, P., Kostic, D., Chase, J., Becker,
D.: Scalability and Accuracy in a Large-Scale Network Emulator. In: Proceedings of
the 5th Symposium on Operating Systems Design and Implementation (December
2002)
7. Bavier, A., Feamster, N., Huang, M., Peterson, L., Rexford, J.: VINI veritas: Real-
istic and controlled network experimentation. In: Proceedings of ACM SIGCOMM
(September 2006)
8. Rossey, L.M., Cunningham, R.K., Fried, D.J., Rabek, J.C., Lippmann, R.P.,
Haines, J.W., Zissman, M.A.: LARIAT: Lincoln Adaptable Real-time Information
Assurance Testbed. In: Proceedings of the IEEE Aerospace Conference (2002)
9. Provos, N., McNamee, D., Mavrommatis, P., Wang, K., Modadugu, N.: The Ghost
in the Browser: Analysis of Web-based Malware. In: Proceedings of the First Work-
shop on Hot Topics in Understanding Botnets (HotBots 2007) (April 2007)
10. Fossi, M.: Symantec Internet Security Threat Report: Trends for 2008 (April 2009)
11. Deibert, R., Rohozinski, R.: Tracking GhostNet: Investigating a Cyber Espionage
Network. Technical Report JR02-2009, Information Warfare Monitor (March 2009)
12. Nagaraja, S., Anderson, R.: The Snooping Dragon: Social-Malware Surveillance of
the Tibetan Movement. Technical Report UCAM-CL-TR-746, University of Cam-
bridge Computer Laboratory (March 2009)
13. Provos, N., Mavrommatis, P., Rajab, M., Monrose, F.: All Your iFrames Point to
Us. In: Proceedings of the 17th USENIX Security Symposium (July 2008)
14. Pinheiro, E., Weber, W.D., Barroso, L.A.: Failure Trends in a Large Disk Drive
Population. In: Proceedings of the 5th USENIX Conference on File and Storage
Technologies (February 2007)
15. Lippmann, R.P., Fried, D.J., Graf, I., Haines, J.W., Kendall, K.R., McClung, D.,
Weber, D., Webster, S.E., Wyschogrod, D., Cunningham, R.K., Zissman, M.A.:
Evaluating Intrusion Detection Systems: The 1998 DARPA Oﬀ-Line Intrusion De-
tection Evaluation. In: Proceedings of the 2000 DARPA Information Survivability
Conference and Exposition (2000)
16. Lippmann, R., Haines, J.W., Fried, D.J., Korba, J., Das, K.: The 1999 DARPA
Oﬀ-line Intrusion Detection Evaluation. Computer Networks 34(4), 279–595 (2000)
17. Yu, T., Fuller, B., Bannick, J., Rossey, L., Cunningham, R.: Integrated Environ-
ment Management for Information Operations Testbeds. In: Proceedings of the
2007 Workshop on Visualization for Computer Security (October 2007)
18. Benzel, T., Braden, R., Kim, D., Neuman, C., Joseph, A., Sklower, K., Ostrenga,
R., Schwab, S.: Experience with DETER: A Testbed for Security Research. In:
Proceedings of the 2nd International Conference on Testbeds and Research Infras-
tructures for the Development of Networks and Communities (TRIDENTCOM)
(March 2006)
19. Boothe-Rabek, J.C.: WinNTGen: Creation of a Windows NT 5.0+ network traﬃc
generator. Master’s thesis, Massachusetts Institute of Technology (2003)
20. Garg, A., Vidyaraman, S., Upadhyaya, S., Kwiat, K.: USim: A User Behavior Simu-
lation Framework for Training and Testing IDSes in GUI Based Systems. In: ANSS
2006: Proceedings of the 39th Annual Symposium on Simulation, Washington, DC,
USA, pp. 196–203. IEEE Computer Society, Los Alamitos (2006)
21. Cui, W., Paxson, V., Weaver, N.C.: GQ: Realizing a System to Catch Worms in
a Quarter Million Places. Technical Report TR-06-004, International Computer
Science Institute (September 2006)
22. Cui, W., Paxson, V., Weaver, N.C., Katz, R.H.: Protocol-Independent Adaptive
Replay of Application Dialog. In: Proceedings of the 13th Annual Symposium on