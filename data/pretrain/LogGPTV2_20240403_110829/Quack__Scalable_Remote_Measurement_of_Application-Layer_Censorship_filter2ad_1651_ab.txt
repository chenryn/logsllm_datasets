### Optimized Text

False positives can lead to an inflated rate of interference. Conversely, multiple retries increase the likelihood of false negatives, as sensitive connections may bypass the interference detection and be incorrectly categorized as successful. To mitigate this, our system is designed to be conservative in identifying interference, minimizing false positives by retrying failed attempts several times.

Our implementation defines a "test" as a series of trials for a specific server and keyword. Each test consists of three phases, as illustrated in Figure 2:

1. **Retry Phase**: Initially, we attempt a trial with the keyword and retry if it fails. The test is concluded as soon as a successful trial is achieved, and the test is declared a success. Given that interference is expected to be rare, for example, the highest observed failure rate for sensitive keywords in a known censoring country was 2.2% after the first trial, we allow up to five retries in our experiments.

2. **Control Phase**: If five consecutive trials fail, we proceed to the Control Phase. In this phase, we use an innocuous keyword (e.g., example.com) to test the server. If the server successfully completes this trial, we conclude that the previous failures were due to network interference. If the control keyword also fails, we move to the final phase.

3. **Delayed Phase**: This phase accounts for stateful disruption, which has been observed, for example, in China [47]. We perform another innocuous trial after a delay of two minutes, determined empirically. If this trial succeeds, the keyword is classified as sensitive. If it fails, the test is marked as "No Result," which could indicate that the echo server became unresponsive during the test.

These steps ensure that our system, Quack, is robust and can distinguish between unrelated network activity, such as sporadic packet loss, and deliberate forms of network interference.

### Classifying Interference

Although we conduct multiple trials within a test, false positive tests can still occur. A single failed test is not categorized as interference, as it could be due to temporary routing issues or other transient errors. Additionally, we aim to differentiate between local-level interference (e.g., corporate firewalls) and national or regional-level interference. To address this, we aggregate all tests within a country and compare keywords based on the rate of tests yielding a "Blocked" result, allowing us to observe blocking at a country level.

The final layer of aggregation involves calculating a "blocking rate" for each keyword-country pair, defined as the number of tests classified as "Blocked" divided by the total number of tests classified as either "Blocked" or "Not Blocked." This effectively removes "No Result" tests from our analysis. Prior work has required a minimum number of trials in an aggregated group to report the blocking rate [41]. We follow this convention, setting a threshold of 15 tests per keyword-country pair to balance robustness and the inclusion of countries with anecdotal evidence of blocking.

In Section 6.1, we validate the countries where we observe widespread censorship using external evidence. Due to "No Result" tests and echo servers being removed from our test set, the keyword blocking rates in a given country can vary. To approximate the probability density function of the keyword blocking rates, we count the number of blocking rates in n even intervals over [0,1], where n is configurable. This distribution allows us to consider each keyword's failures in the context of the country's noise and categorize each country based on its distribution.

When there is no blocking, we assume that blocking events due to noise are independent and occur with very small probability, confirmed in Section 6.1. Given our redundancy in each test, we expect the approximated distribution of blocking rates to be monotonically increasing if there is no blocking. In our control experiments, we find all distributions to be monotonic, with an empirical blocking rate of 0.01%.

We identify interference in countries where the distribution of keyword blocking rates is not monotonic. Specifically, keywords with blocking rates in the interval that breaks the monotonic trend and those with higher blocking rates are considered to experience interference in that country.

### Ethical Considerations

Active network measurement, particularly for censorship, raises important ethical considerations. Due to the sensitive nature of this research, we consulted our institution's IRB, which determined that the study did not involve human subjects or personally identifiable data. Nonetheless, we carefully considered ethical questions, guided by the principles in the Belmont [30] and Menlo [13] reports.

Our technique involves causing hosts within censored countries to transmit data to trigger observable side-effects from the censorship infrastructure. This creates a potential risk that users controlling these hosts could face retribution from local authorities. While there are no documented cases of such retribution, we designed our experiments to minimize this hypothetical risk.

Existing techniques [6, 34, 35, 41] cause oblivious hosts in censored countries to make requests for or exchange packets with prohibited sites. In contrast, our measurements only involve connections between a machine we control and echo servers, ensuring that the echo servers never send or receive data from a censored destination. However, our interactions are designed to trigger the censorship system, and we cannot entirely exclude the possibility that authorities might interpret our connections as user-originated web requests, either mistakenly or maliciously.

To mitigate this risk, we ensured that the network traffic looks different from real web browsing. Our TCP connections are initiated by us, not the echo server, and our source port is in the ephemeral range. The first data sent is an HTTP request from us, followed by the same data echoed by the server, with no HTTP response. We also set up reverse DNS records, WHOIS records, and a web page served from port 80 on each IP address, indicating that the hosts were part of an Internet measurement research project.

Most echo servers appear to be servers, routers, or embedded devices, further reducing the risk of misidentification. For long-term studies, we recommend profiling each server individually and including an HTTP header explaining the purpose of the requests.

Given these factors, we believe the risks to echo server operators are extremely small. Seeking informed consent would be infeasible and could increase the risk of reprisal for those who grant consent. Therefore, we did not seek consent, aligning with the Menlo Principle of Justice [13].

### Experimental Setup and Data

In our study, we examine URLs as the source of content that may be disrupted. Unless specified otherwise, we send the domain name in the context of a valid HTTP/1.1 GET request, allowing us to observe application-layer interference, which is well-documented [11].

#### Control Study
We first perform a control study by testing innocuous domains (e.g., testN.example.com) against every echo server. This experiment was conducted 1109 times per server from July 20–21, 2017, from our measurement machine inside an academic network. Since there should be no artificially induced network interference, we can validate our technique using these results.

#### Citizen Lab Block List
We use the global Citizen Lab Block List (CLBL) [8] from July 1, 2017, as a list of keywords to run against all echo servers. This list contains 1109 entries. Significant differences between this test and the control study indicate that our system can detect application-layer interference. This test ran from July 21–22, 2017.

#### Discard Protocol
We repeat the Citizen Lab study using the Discard Protocol [36], which discards received data instead of echoing it back. By comparing the results with the Echo Protocol, we can determine if middleboxes detect inbound keywords. This test ran from July 19–20, 2017.

#### TLS
We perform the Citizen Lab experiment again, embedding the domain in the SNI extension of a valid TLS ClientHello message. This allows us to compare interference between HTTP and HTTPS. This test ran from July 23–24, 2017.

#### Alexa Top 100k
Finally, we test the top 100,000 domains from Alexa [2] downloaded on July 12, 2017. To achieve full measurement, we select 20 servers in each of the 40 countries with more than 100 echo servers. This test demonstrates that our tool can be used at scale for significant research into application-layer blocking at a country granularity. This test ran from July 25–28, 2017.

### Server Discovery

Server discovery is a staged process. A ZMap scan identifies servers that SYNACK on port 7, but many of these servers will fail to ACK or will RST when receiving data. To remove these misbehaving echo servers, we attempt to send and receive a simple echo request. The resulting stable server set is shown in Figure 4.