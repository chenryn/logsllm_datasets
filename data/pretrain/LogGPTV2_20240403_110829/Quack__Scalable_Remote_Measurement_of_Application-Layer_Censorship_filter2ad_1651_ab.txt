false positives, resulting in an inﬂated rate of interference.
On the other hand, many retries increase false negatives
as sensitive connections slip past interference technology
and are categorized as successful. We choose to be con-
servative in our designation of interference, designing our
system to minimize false positives by retrying failures
several times.
Our implementation designates a “test” as the repeated
trial of a particular server and keyword. A test proceeds
in three phases, as shown in Figure 2:
Retry Phase First, we run a trial with the keyword and
retry if it fails. We end the test as soon as we have a
successful trial, and declare the test a success. We expect
interference to be sparse. For example, the highest failure
rate we observed when testing sensitive keywords in a
country known to implement interference was 2.2% of
tests not ending in success after the ﬁrst trial. We allowed
up to 5 retries in our experiments.
Control Phase After ﬁve trials have failed, we progress
to the Control Phase. In the Control Phase, we trial an
innocuous keyword. If the server successfully completes
this trial, we conclude that the ﬁve previous failures were
due to network interference. If the control keyword fails,
we proceed to the ﬁnal phase. In our experiments, we use
example.com as our control keyword.
Delayed Phase Finally, we account for stateful disrup-
tion. This is observed, for example, in China [47]. We test
for this behavior by performing another innocuous trial
after a delay. If this trial succeeds, we classify the key-
word as sensitive. If it fails, we mark the test as No Result.
This may occur if the echo server becomes unresponsive
during our test.
We use a two minute delay determined empirically.
Knowing that some middleboxes perform stateful block-
ing, we test every server in censoring countries with an
HTTP request for the most commonly censored domain
in that country. Then, we attempt to reconnect every 10
seconds with an innocuous payload until we succeed. The
resulting distribution in Figure 3 shows 120 seconds is a
sufﬁcient delay.
These steps ensure that Quack is robust and can distin-
guish unrelated network activity, such as sporadic packet
loss and other systematic errors, from deliberate forms of
network interference.
190    27th USENIX Security Symposium
USENIX Association
050100150200250300TimeofDelay(Seconds)0.00.20.40.60.81.0FractionofCensoringServersClassifying Interference Although we conduct multi-
ple trials within a test, false positive tests can still occur.
We do not categorize a single failed test as interference,
since it could be due to temporary routing issues or other
transient failure. Even if the test is representative of pol-
icy, we wish to differentiate interference that is occurring
at a local level, such as a corporate ﬁrewall, from that im-
plemented at a national or regional level. To address both
of these, we consider all tests in a country, comparing
keywords by the rate of tests yielding a Blocked result.
This allows us to observe the phenomenon of blocking at
a country level.
This last layer of aggregation is formed by calculating
a “blocking rate” for each keyword-country pair, equal
to the number of tests classiﬁed as Blocked divided by
the number classiﬁed as either Blocked or Not Blocked.
Effectively, this removes No Results from our analysis.
Prior work that has looked at failure rates aggregated
across servers has required a minimum number of trials in
an aggregated group to report on the blocking rate for that
group [41]. We follow this convention, as it is consistent
with our design goal of Robustness. Selecting a threshold
for the number of experiments that is too low reduces our
conﬁdence, while selecting a threshold that is too high
excludes more countries. Upon manual inspection of the
number of servers in countries reported to perform block-
ing, we determine 15 as threshold that balances Robust-
ness and the inclusion of anecdotally blocking countries.
In Section 6.1, we validate the countries in which we
observe widespread censorship using external evidence.
Due to No Result tests and echo servers churning out
of our test set, the keyword blocking rates in a given
country have many possible values. To approximate the
probability density function of the keyword blocking rates
in a country, we count the number of blocking rates in n
even intervals over [0,1], where n is conﬁgurable. Having
this approximated distribution in each country of keyword
blocking rates lets us consider each keyword’s failures in
the context of the country’s noise. We can also categorize
each country based on its distribution.
When there is no blocking, we assume Blocking events
due to noise are independent and only occur with very
small probability. We conﬁrm this in Section 6.1. Since
the probability of failure due to noise is so small, given
our redundancy in each test, we would expect that our
approximated distribution of the blocking rates be mono-
tonic in the case that there is no blocking. In our control
experiments with no expected interference in Section 6.1,
we ﬁnd all distributions to be monotonic, and we empiri-
cally ﬁnd the blocking rate to be 0.01%.
We mark interference in countries whose distribution of
keyword blocking rate is not monotonic. More precisely,
we say that the keywords whose blocking rates are in the
interval that breaks the monotonic trend and those key-
words with higher blocking rates experience interference
in that country.
We considered several trade-offs when choosing the
number of intervals, n. We do not want an n larger than the
minimum number of tests per keyword, 15, because this
could cause consecutive numbers of blocking results to be
in the same interval, creating an artifact in the distribution.
However, we want as many buckets as possible, so that
our smoothing does not remove too much of the detail
of the distribution. To balance these concerns, we use
n = 15 buckets consistently for the rest of our analysis.
We implement a system in Go 1.6, utilizing light-
weight threads for parallelism. We restrict ourselves to
one concurrent request per echo server, to restrict load
on the echo server, and at most 2000 total concurrent re-
quests. Our test server was able to process 550 requests
per second and has a quad-core Intel E3–1230 v5 CPU,
16 GB of RAM, and a gigabit Ethernet uplink.
While we initially ran tests with our measurement ma-
chine source port set to 80, in order to appear more similar
to real HTTP connections, we found no difference in our
results while using an ephemeral source port. Using an
ephemeral source port also allowed us to follow standard
conventions and to host an abuse website on the standard
HTTP port of our measurement machine.
3.2 Ethical Issues
Active network measurement [33], and active measure-
ment of censorship in particular [25], raise important
ethical considerations. Due to the sensitive nature of
such research, we approached our institution’s IRB for
guidance. The IRB determined that the study fell out-
side its purview, as it did not involve human subjects or
their personally identiﬁable data. Nevertheless, we at-
tempted to carefully consider ethical questions raised in
our work, guided by the principles in the Belmont [30]
and Menlo [13] reports and other sources. We discussed
the study’s design and potential risks with colleagues at
our institution and externally, and we attempted to follow
or exceed prevailing norms for risk reduction in censor-
ship measurement research.
Like most existing censorship measurement techniques,
ours involves causing hosts within censored countries to
transmit data in an attempt to trigger observable side-
effects from the censorship infrastructure. This creates
a potential risk that users who control these hosts could
suffer retribution from local authorities. There is no doc-
umented case of such a user being implicated in a crime
due to any remote Internet measurement research, but we
nonetheless designed our technique and experiments so
as to reduce this hypothetical risk.
Existing techniques [6, 34, 35, 41] in censorship mea-
surement cause oblivious hosts in censored countries to
USENIX Association
27th USENIX Security Symposium    191
make requests for or exchange packets with prohibited
sites. In contrast, our measurements only involve con-
nections between a machine we control and echo servers,
so the echo servers never send or receive data from a
censored destination.
Still, our interactions with the echo servers are designed
to trigger the censorship system, as if a request for a pro-
hibited site had been made. We cannot entirely exclude
the possibility that authorities will interpret our connec-
tions as user-originated web requests, either mistakenly or
by malicious intent. However, we believe that the actual
risk is extremely small, for several reasons.
First, even upon casual inspection, the network trafﬁc
looks very different from a real connection from the host
running the echo server to a prohibited web server. The
TCP connection is initiated by us, not from the echo server.
Our source port is in the ephemeral range, and the echo
server’s is the well known port 7. The ﬁrst data is an
HTTP request from us, followed by the same data echoed
by the server, and there is never any HTTP response. The
request itself is minimal, with no optional headers, unlike
requests from any popular browser. Any of these factors
would be enough to distinguish a packet capture of our
probes from real web browsing.
Second, the network infrastructure from which we
source our probes looks very different from prohibited
web servers. We tried to make it easy for anyone in-
vestigating our IP addresses to determine that they were
part of a measurement research experiment. We set up
reverse DNS records, WHOIS records, and a web page
served from port 80 on each IP address, all indicating that
the hosts were part of an Internet measurement research
project based at our university.
Third, most echo servers look very different from end-
user devices. We ﬁnd (see Section 5.3) that the vast ma-
jority of public echo servers appear to be servers, routers,
or other embedded devices. In the unlikely event that
authorities decided to track down these hosts, it would be
obvious that users were not running browsers on them.
There are additional steps that we did not take for this
initial study that could further reduce the risk of misiden-
tiﬁcation. We recommend that anyone applying our tech-
niques for longitudinal data collection incorporate them.
Although we established that few echo servers are end-
user devices by random sampling, in a long-term study,
each server should be individually proﬁled, using tools
such as Nmap, to exclude all those that are not clearly
servers, routes, or embedded devices. In addition, the re-
quests sent to echo servers could include an HTTP header
that explains they are part of a global measurement study.
This would provide one more way for authorities to con-
clude that the trafﬁc did not originate from an end user.
Given these factors, we believe that the risks of our
work to echo server operators are extremely small. We
considered seeking informed consent from them anyway,
but we rejected this route for several reasons.1 First, the
risk to these users is low, but if we were to contact them
to seek consent, this interaction with foreign censorship
researchers would in and of itself carry a small risk of
drawing negative attention from the authorities. Second,
if we only used servers for which the operators granted
consent, these operators would face a much higher risk of
reprisal, since their participation would be easy to observe
and would imply knowing complicity. Third, obtaining
consent would be infeasible in most cases, due to the difﬁ-
culty of identifying and contacting the server operators; if
we limited our study to echo servers for which we could
ﬁnd owner contact information, this would lead to far
fewer usable servers, thus severely reducing the beneﬁt
of the study. The communities that stand to beneﬁt most
from our results are those living in regions that practice
aggressive censorship, and thus those who will likely ben-
eﬁt include the echo server operators in these regions,
conforming with Menlo’s Principle of Justice [13].
Beyond these risks, we also sought to minimize the po-
tential ﬁnancial and performance burden on echo server
operators. We rate-limited our measurements to one con-
current connection per server, and each connection sent
an average of only two packets per second. Our ZMap
scans were conducted following the ethical guidelines pro-
posed by Durumeric et al. [15], such as respecting an IP
blacklist shared with other scanning research conducted
at our institution and including simple ways for packet
recipients to opt out of future probes.
We contrast our work with Encore [6], a censorship
measurement system that has been widely criticized on
ethical grounds. Websites install Encore by embedding
a sequence of JavaScript. When users visit these sites,
their browsers make background HTTP requests to cen-
sored domains, possibly without notice or consent. While
we too make oblivious use of existing hosts without ob-
taining consent, the network trafﬁc and endpoints differ
dramatically from normal requests for censored content.
We believe this substantially reduces the risk of harm.
4 Experimental Setup and Data
In our study, we examine URLs as the source of content
that may be disrupted. In our experiments, unless speci-
ﬁed otherwise, we send the domain name in the context of
a valid HTTP/1.1 GET request. This allows us to observe
a particular subset of application-layer interference, and
one that is well documented [11].
1As discussed by others [33,40], informed consent is not an absolute
requirement for ethical research, so long as the research abides by other
principles, e.g. those in the Belmont and Menlo reports or those steps
proposed by Partridge and Allman [33], as we have strived to do.
192    27th USENIX Security Symposium
USENIX Association
Control We ﬁrst perform a control study. To do so,
we test a number of innocuous domains as our keywords,
which are expected not to be censored, and repeat them
against every echo server. The domains we choose are of
the form testN.example.com with incrementing values
of N. We perform this experiment 1109 times per server.
Since there should be no artiﬁcially induced network in-
terference, we can validate our technique using the results
of this study. This test was performed July 20–21, 2017
from our measurement machine inside of an academic
network.
Citizen Lab We use the the global Citizen Lab Block
List (CLBL) [8] from July 1, 2017 as a list of keywords
to run against all echo servers. This list has 1109 entries.
It is curated by Citizen Lab to provide a set of URLs for
researchers to use when they are conducting censorship
research. Signiﬁcant difference between this test and
the previous test indicates that our system is capable of
detecting application-layer interference of the domains
in this list. This test ran on July 21–22, 2017, from our
measurement machine.
Discard We then repeat the Citizen Lab study using a
closely related protocol, the Discard Protocol [36]. The
Discard Protocol is designed similarly to the Echo Pro-
tocol, but instead of echoing back any received data, it
is simply discarded. By repeating our experiment with
discard, we can determine if existing middleboxes detect
keywords that are seen inbound to its network. If this
were the case we would see the same interference in the
Discard Protocol as the Echo Protocol. Otherwise, we
will be able to determine that interference technologies
do notice the direction of sensitive content. This test run
on July 19–20, 2017, from our measurement machine.
TLS
This study demonstrates the application-layer
ﬂexibility of our technique. We perform the Citizen Lab
experiment again, but instead of embedding the Citizen
Lab domain list in valid HTTP request, we place the do-
main in the SNI extension of a valid TLS ClientHello
message. This will allow us to discern what difference ex-
ists between interference of HTTP and HTTPS. This test
ran on July 23–24, 2017, from our measurement machine.
Alexa Top 100k
Finally, we use our system to test the
top 100,000 domains from Alexa [2] downloaded on July
12, 2017. This is a set of domains orders of magnitude
larger than that of prior works studying application-layer
censorship. To achieve full measurement of such a large
set of domains, for each domain we select 20 servers in
each country. Additionally, we restrict our test to the
40 countries with more than 100 echo servers. This test
demonstrates most of all that our tool can be used at scale
for signiﬁcant research into application-layer blocking at
a country granularity. This test ran on July 25–28, 2017,
from our measurement machine.
Server Set
SYNACK
Echo
Stable (24 hr)
IP Addresses
5,260,118
57,890
47,276
/24s
109,729
38,977
31,802
ASNs
6,932
3,766
3,463
Countries
198
172
167
Figure 4: Discovery of Echo Servers—Server discovery is a
staged process. A ZMap scan discovers servers that SYNACK
on port 7, but we ﬁnd that most of these servers will fail to
ACK or will RST when receiving any data. To remove these
misbehaving echo servers, we attempt to send and receive a