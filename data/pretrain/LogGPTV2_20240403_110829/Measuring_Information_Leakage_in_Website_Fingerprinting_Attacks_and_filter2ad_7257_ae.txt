ment to construct the bootstrapped dataset. Here we didn’t use
sampling with replacement because it makes less sense that the
same website is included twice in a bootstrapped dataset. This spe-
cial bootstrapping technique is also called subsampling [38]. Re-
peating the same procedure n times (n = 20 in our experiment),
we have n such datasets to obtain n bootstrapped measurements.
Finally, we get the bootstrapped conﬁdence interval for validation.
Figure 10 displays the 90% conﬁdence interval for the top 100
most informative features and 15 categories of features. Not sur-
prisingly, including diﬀerent websites in the closed-world setting
does make a diﬀerence in the measurement, but Figure 10 shows
such impact is very limited. Among top 100 informative features,
most of them have conﬁdence interval with less than 0.5 bit width,
so do most of categories (even less for some categories). The ex-
ception only comes to category Interval-I. By bootstrapping, we
validate our information leakage results even when the true repre-
sentative websites are still unknown.
8 INFORMATION LEAKAGE IN WF DEFENSES
This section ﬁrstly gives the theoretical analysis on why accuracy
is not a reliable metric to validate a WF defense. Then we measure
the WF defenses’ information leakage to conﬁrm the analysis. Note
that we choose the closed-world setting in the evaluation, as the
setting is most advantageous for attackers, and we can get an upper
bound for the defense’s security.
8.1 Accuracy and Information Leakage
The popular method to tell whether a WF defense is secure or not
is to look at the classiﬁcation accuracy under diﬀerent WF attacks.
If the defense is able to achieve low classiﬁcation accuracy, it is
7
6
5
4
3
2
e
g
a
k
a
e
L
n
o
i
t
a
m
r
o
f
n
I
1
10−2
CS−BuFLO
BuFLO
WTF−PAD
Tamaraw
Supersequence
(method 4)
Supersequence
(method 3)
10−1
Accuracy
100
Figure 11: Website Fingerprinting Defenses: Accuracy vs. In-
formation Leakage. Upon each type of defensed traces, we evalu-
ate the overall information leakage and the classiﬁcation accuracy
at the same time. The results demonstrate the discrepancy between
accuracy and information leakage
believed to be secure. Here, we raise the question: does low clas-
siﬁcation accuracy always mean low information leakage? This
question matters because if not, low classiﬁcation accuracy would
not be suﬃcient to validate a WF defense. To answer this question,
we analyze the relation between information leakage and accuracy.
We ﬁnd that given a speciﬁc accuracy, the actual information leak-
age is far from certain.
Theorem 1. Let {c1, c2 , · · · , cn } denote a set of websites with
prior probabilities p1 , p2 , · · · , pn, and vi denote a visit to website ci .
Suppose a website ﬁngerprinting classiﬁer D which recognizes a
visit vi to be D(vi ). The classiﬁer would succeed if D(vi ) = ci , oth-
erwise it fails. Assume a defense has been applied, and this classi-
ﬁer has α accuracy in classifying each website’s visits. Then the in-
formation leakage obtained by the classiﬁer is uncertain: the range
of the possible information leakage is
(1 − α)lo❕2(n − 1)
(3)
Proof: see Appendix B
The reason for such uncertainty is that classiﬁcation accuracy
is "all-or-nothing". The classiﬁer just makes one trial, and accu-
racy counts a miss as failure. But even a miss doesn’t necessarily
mean no information leakage. It is possible that the attacker could
make a hit with the second or third trials, which indicates high in-
formation leakage; it also possible that the attacker could not do
so without many trials, which equals to low information leakage.
Accuracy alone cannot tell which case is true, as a result, the infor-
mation leakage for a given accuracy is uncertain.
An Example. Figure 1 shows an example for the theorem. Note
that the range is invariable no matter what we assume for websites’
prior probability. We can see a wide range of possible information
leakage when a low accuracy is given, showing that low accuracy
doesn’t necessarily guarantee low information leakage.
8.2 Measurement Results for WF defenses
We include Tamaraw [6], BuFLO [11], Supersequence [46], WTF-
PAD [25], and CS-BuFLO [5] to quantify the information leakage
upon defensed traﬃc.
We adopt the implementation of BuFLO, Tamaraw, and Super-
sequence [2] to generate the defensed traﬃc, with τ =5, 10, 20,
30, 40, 50, 60, 80, 100, or 120 for BuFLO, L ranging from 10 to 100
with step 10 and from 200 to 1000 with step 100 for Tamaraw. We
include the method 3 of Supersequence, with 2, 5, or 10 super clus-
ters, and 4, 8, or 12 stopping points. We also include the method
4 of Supersequence, with 2 super clusters, 4 stopping points, and
2, 4, 6, 8, 10, 20, 35, or 50 clusters. We use the implementation [1]
to create the WTF-PAD traﬃc. We were recommended to use the
default normal_rcv distributions on our dataset, as ﬁnding an op-
timal set of distributions for a new dataset is currently a work in
progress [1]. We apply the KNN classiﬁer [46] on our WTF-PAD
traces, and we can get similar accuracy (18.03% in our case). This
classiﬁcation result validates our WTF-PAD traces. We use the im-
plementation [18] to generate simulated CS-BuFLO traces.
Upon each type of defensed traces, we evaluate the overall in-
formation leakage and the classiﬁcation accuracy at the same time.
The measurement is conducted in closed-world setting with 94
websites. To evaluate the total information leakage, we assume
equal prior probability for websites and adopt k = 5000 for Monte
Carlo Evaluation. We use bootstrapping with 50 trials to estimate
the 96% conﬁdence interval for the information leakage and accu-
racy. For details about bootstrapping, please see Appendix A. Note
that we redo the dimension reductions for each defense, as a WF de-
fense changes the information leakage of a feature and the mutual
information between any two features. The classiﬁer we adopt is a
variant of the KNN classiﬁer [46]. The only change we make is the
feature set: we use our own feature set instead of its original one.
The purpose is to have equivalent feature sets for classiﬁcations
and information leakage measurements. The reason for choosing
this KNN classiﬁer is that it is one of the most popular website
ﬁngerprinting classiﬁers to launch attacks and evaluate defense
mechanisms. It’s also worth noting that the original feature set of
the KNN classiﬁer is a subset of our feature set. The experimental
results are shown in Figure 11.
8.3 Accuracy is inaccurate
Accuracy is widely used to compare the security of diﬀerent de-
fenses. A defense mechanism is designed and tuned to satisfy a
lower accuracy as an evidence of superiority over existing defenses [11].
With defense overhead being considered, new defense mechanisms [6,
25] are sought and conﬁgured to lower the overhead without sacri-
ﬁcing accuracy too much. But if accuracy fails to be a reliable met-
ric for security, it would become a pitfall and mislead the design
and deployment of defense mechanisms. This section describes the
ﬂaws of accuracy and proves such a possibility.
Accuracy may fail because of its dependence on speciﬁc
classiﬁers. If a defense achieves low classiﬁcation accuracy, it’s
not safe to conclude that this defense is secure, since the used clas-
siﬁers may not be optimal. More powerful classiﬁers may exist and
output higher classiﬁcation accuracy. We prove this possibility in
our experiment. To validate WTF-PAD, four classiﬁers were used
including the original KNN classiﬁer, and the reported highest ac-
curacy was 26%. But using the KNN classiﬁer with our feature set,
we observe 33.99% accuracy. Recent work [42] even achieves 90%
accuracy against WTF-PAD. This work also conﬁrms our measure-
ment that the information leakage of WTF-PAD is high (6.4 bits),
indicating that WTF-PAD is not secure. Thus, accuracy is not re-
liable to validate a defense because of its dependence on speciﬁc
classiﬁers.
Defenses having equivalent accuracy may leak varying
amount of information. Figure 11 demonstrates such a phenom-
enon when taking BuFLO (τ = 40) and Tamaraw (L = 10) into con-
sideration. Accuracy of both defenses is nearly equivalent, with
9.39% for BuFLO and 9.68% for Tamaraw. In sense of accuracy, Bu-
FLO (τ = 40) was considered to be as secure as Tamaraw(L = 10).
However, our experimental results disapprove such a conclusion,
showing BuFLO (τ = 40) leaks 2.31 bits more information than
Tamaraw (which leaks 3.26 bits information). We observe the sim-
ilar phenomenon between WTF-PAD and Supersequence.
More importantly, a defense believed to be more secure
by accuracy may leak more information. Take BuFLO (τ = 60)
as an example. Its accuracy is 7.39%, while accuracy of Tamaraw
with L = 10, 20, 30 is 9.68%, 9.15%, and 8.35% respectively. Ac-
curacy supports BuFLO (τ = 60) is more secure than Tamaraw
with L = 10, 20, 30. However, our measurement shows that BuFLO
(τ = 60) leaks 4.56 bit information, 1.3 bit, 1.61 bit, and 1.75 bit
more than Tamaraw with L = 10, 20, 30! Take WTF-PAD as an-
other example. The accuracy for WTF-PAD is 33.99%, much lower
than the 53.19% accuracy of Supersequence method 4 with 2 su-
per clusters, 50 clusters, and 4 stopping points. But the informa-
tion leakage of WTF-PAD is around 6.4 bits, much higher than the
leakage of the latter which is about 5.6 bits. Our experimental re-
sults prove the unreliability of accuracy in comparing defenses by
security.
9 OPEN-WORLD INFORMATION LEAKAGE
In the closed-world scenario, the attacker knows all possible web-
sites that a user may visit, and the goal is to decide which website
is visited; In the open-world setting, the attacker has a set of mon-
itored websites and tries to decide whether the monitored web-
sites are visited and which one. The diﬀerence in information leak-
age is that the open-world has n + 1 possible outcomes, whereas
the closed-world has n outcomes where n is the number of (moni-
tored) websites. We include the details about how to quantify this
information in Appendix D. The following describes part of our
results for the open-world information leakage. For more informa-
tion, please visit our GitHub repository.
Experiment Setup. We adopt the list of monitored websites
from [46] and collected 17984 traﬃc instances in total. Our non-
monitored websites come from Alexa’s top 2000 with 137455 in-
stances in total. We approximate the websites’ prior probability by
Zipf law [4, 19], which enables us to estimate a website’s prior prob-
ability by its rank. We conduct experiments with top 500, 1000, 2000
non-monitored websites separately, and we show the experimental
results in Figure 12.
Figure 12 shows that the open-world information leakage is de-
creased when including more non-monitored websites, with 1.82, 1.71, 1.62
bit for top500, top1000, top2000, respectively. Including more non-
monitored websites decreases the entropy of the open-world set-
ting rather than increasing it. The reduced information is in part
because of the prior on monitored websites. Compared with closed-
world setting with similar world size, open-world scenario carries
much less information.
Similar with the closed-world setting, Figure 12 shows that most
categories except First20, First30 and Last30 Packet count, and Interval-
I leak most of the information. This shows that the diﬀerence in
world setting has little impact on categories’ capability in leaking
information.
We also investigate how the size of the non-monitored web-
sites inﬂuences our measurement. We focus on the total leakage
and build the AKDE models for the non-monitored websites with
the varying size of the non-monitored, respectively. We evaluate
how the diﬀerence of these AKDE models inﬂuences measurement.
Speciﬁcally, we evaluate (a) how monitored samples are evaluated
at these AKDE models, and (b) how samples generated by these
AKDE models are evaluated at the monitored AKDE. Figure 13
shows the results. Figure 13 (a) shows that these AKDE models of
the non-monitored, though diﬀering in size, assign low probability
(below 10−10 with 95% percentile) to monitored samples. Figure 13
(b) shows that though these AKDE models for the non-monitored
generate diﬀerent samples, the diﬀerence on how these samples
are evaluated by the AKDE model of the monitored is little: they
are all assigned low probability below 10−20 with 95% percentile.
The results lead to the estimation that introducing extra lower
rank websites into the non-monitored set would not signiﬁcantly
change the low probability that the non-monitored AKDE assigns
to monitored samples, and the low probability that the monitored
AKDE assigns to samples generated by the non-monitored AKDE,
thanks to the low prior probability of these websites. The informa-
tion leakage is therefore little impacted.
10 DISCUSSION
WF Defense Evaluation. We have discussed why using accuracy
alone to validate a WF defense is ﬂawed. Note that we don’t mean
that WF defense designers should keep away from accuracy. In fact,
accuracy is straightforward and easy to use, and it is suitable for
the initial check on WF defense design: if the classiﬁcation accu-
racy is high, then the defense design is not secure. But if the clas-
siﬁcation accuracy is low, it doesn’t necessarily mean the defense
is secure. We recommend defense designers to include other meth-
ods to further validate the defense. A potential approach to use
is top-k accuracy, which allows WF attackers to make k guesses
instead of one, and if the k guesses contain the right one, then
the attackers succeed, otherwise, they lose. Another approach is
information leakage measurement tools such as WeFDE. WeFDE
gives information-theoretic perspective in evaluating WF defenses.
When evaluating a defense by a classiﬁer, a test case unseen by
the training dataset is likely to be misclassiﬁed. But we can imag-
ine that enlarging the dataset would eﬀectively avoid such mis-
classiﬁcation. This issue favors the defense design, and it is more
likely to happen in probabilistic defenses such as WTF-PAD. Us-
ing WeFDE to evaluate a defense doesn’t have this problem, as all
data points are used to build the model for information leakage
2
1.5
1
0.5
Total
top500
top1000
top2000
2
1.5
1
0.5
Pkt. Count
Time
Ngram
Transposition
2
1.5
1
0.5
2
1.5
1
0.5
2
1.5
1
0.5
0
0
50
100
0
0
5
0
0
10
20
0
0
50
100
0
0
50
100
Interval−I
Interval−II
Interval−III
Pkt. Distribution
Burst
2
1.5
1
0.5
2
1.5
1
0.5
2
1.5
1
0.5
2
1.5
1
0.5
2
1.5
1
0.5
0
0
50
100
0
0
50
100
0
0
50
100
0
0
50
100
0
0
First20
First30 Pkt. Count
2
Last30 Pkt. Count
2
Pkt. per Second
2
)
t
i
B
(
e