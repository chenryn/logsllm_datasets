    ...
    }
**事件拆分**
百万台IDC规模的Agent部署，在任务执行、集群通讯或对宿主机产生资源影响时，务必要错峰进行，根据每台主机的唯一特征取模，拆分执行，避免造成雪崩效应。
### 监控告警
古时候，行军打仗时，提倡“兵马未动，粮草先行”，无疑是冷兵器时代决定胜负走向的重要因素。做产品也是，尤其是大型产品，要对自己运行状况有详细的掌控，做好监控告警，才能确保产品的成功。
对于etcd集群的监控，组件本身提供了`Metrics`数据输出接口，官方推荐了[Prometheus](https://prometheus.io/)来采集数据，使用[Grafana](https://grafana.com/)来做聚合计算、图标绘制，我们做了`Alert`的接口开发，对接了公司的告警系统，实现IM、短信、电话告警。
**Agent数量感知，依赖Watch数字，实时准确感知。**
如下图，来自产品刚开始灰度时的某一时刻截图，Active Streams（即etcd
Watch的Key数量）即为对应Agent数量，每次灰度的产品数量。因为该操作，是Agent直接与集群通讯，并且每个Agent只Watch一个Key。且集群数据具备唯一性、一致性，远比心跳日志的处理要准确的多。
**etcd集群Members之间健康状况监控**
用于监控管理etcd集群的状况，包括`Member`节点之间数据同步，Leader选举次数，投票发起次数，各节点的内存申请状况，GC情况等，对集群的健康状况做全面掌控。
**程序运行状态监控告警**
全量监控Aagent的资源占用情况，统计每天使用最大CPU\内存的主机Agent，确定问题的影响范围，及时做策略调整，避免影响到业务服务的运行。并在后续版本上逐步做调整优化。
百万台服务器，日志告警量非常大，这个级别的告警信息的筛选、聚合是必不可少的。减少无用告警，让研发运维人员疲于奔命，也避免无用告警导致研发人员放松了警惕，前期忽略个例告警，先解决主要矛盾。
  * 告警信息分级，告警信息细分ID。
  * 根据告警级别过滤，根据告警ID聚合告警，来发现同类型错误。
  * 根据告警信息的所在机房、项目组、产品线等维度来聚合告警，来发现同类型错误。
**数据采集告警**
  * 单机数据数据大小、总量的历史数据对比告警。
  * 按机房、项目组、产品线等维度的大小、总量等维度的历史数据对比告警。
  * 数据采集大小、总量的对账功能，判断经过一系列处理流程的日志是否丢失的监控告警。
### 熔断
  * 针对单机Agent使用资源大小的阈值熔断，CPU使用率，连续N次触发大于等于5%，则进行保护性熔断，退出所有业务逻辑，以保护主机的业务程序优先。
  * Master进程进入空闲状态，等待第二次时间`Ticker`到来，决定是否恢复运行。
  * 各个App基于业务层面的监控熔断策略。
### 灰度管理
在前面的`配置管理`中的`etcd
Key`设计里，已经细分到每个主机（即每个Agent）一个Key。那么，服务端的管理，只要区分该主机所属机房、环境、群组、产品线即可，那么，我们的管理Agent的颗粒度可以精确到每个主机，也就是支持任意纬度的灰度发布管理与命令下发。
### 数据上报通道
组件名为 `log_agent` ，是公司内部统一日志上报组件，会部署在每一台VM、Docker上。主机上所有业务均可将日志发送至该组件。
`log_agent`会将日志上报到Kafka集群中，经过处理后，落入Hive集群中。（细节不在本篇讨论范围）
### 主进程
主进程实现跟etcd集群通信，管理整个Agent的配置下发与命令下发；管理各个子模块的启动与停止；管理各个子模块的CPU、内存占用情况，对资源超标进行进行熔断处理，让出资源，保证业务进程的运行。
插件化管理其他模块，多进程模式，便于提高产品灵活性，可更简便的更新启动子模块，不会因为个别模块插件的功能、BUG导致整个Agent崩溃。
### 进程监控
**方案选择**
我们在研发这产品时，做了很多关于`linux进程创建监控`的调研，不限于`安全产品`，大约有下面三种技术方案：
对于公司的所有服务器来说，几十万台都是已经在运行的服务器，新上的任何产品，都尽量避免对服务器有影响，更何况是所有服务器都要部署的Agent。
意味着我们在选择`系统侵入性`来说，优先选择`最小侵入性`的方案。
对于`Netlink`的方案原理，可以参考这张图（来自:[kernel-proc-connector-and-containers](https://www.slideshare.net/kerneltlv/kernel-proc-connector-and-containers)） 
**系统侵入性比较**
  * `cn_proc`跟`Autid`在“系统侵入性”和“数据准确性”来说，`cn_proc`方案更好，而且使用CPU、内存等资源情况，更可控。 
  * `Hook`的方案，对系统侵入性太高了，尤其是这种最底层做HOOK syscall的做法，万一测试不充分，在特定环境下，有一定的概率会出现Bug，而在百万IDC的规模下，这将成为大面积事件，可能会造成重大事故。
**兼容性上比较**
  * `cn_proc`不兼容Docker，这个可以在宿主机上部署来解决。
  * `Hook`的方案，需要针对每种Linux的发行版做定制，维护成本较高，且不符合长远目标（收购外部公司时遇到各式各样操作系统问题）
**数据准确性比较**
在大量PID创建的场景，比如Docker的宿主机上，内核返回PID时，因为PID返回非常多非常快，很多进程启动后，立刻消失了，另外一个线程都还没去读取`/proc/`，进程都丢失了，场景常出现在Bash执行某些命令。
最终，我们选择`Linux Kernel
Netlink接口的cn_proc指令`作为我们进程监控方案，借助对Bash命令的收集，作为该方案的补充。当然，仍然存在丢数据的情况，但我们为了系统稳定性，产品侵入性低等业务需求，牺牲了一些安全性上的保障。
对于Docker的场景，采用宿主机运行，捕获数据，关联到Docker容器，上报到日志中心的做法来实现。
**遇到的问题**
**内核Netlink发送数据卡住**
内核返回数据太快，用户态`ParseNetlinkMessage`解析读取太慢，导致用户态网络Buff占满，内核不再发送数据给用户态，进程空闲。对于这个问题，我们在用户态做了队列控制，确保解析时间的问题不会影响到内核发送数据。对于队列的长度，我们做了定值限制，生产速度大于消费速度的话，可以丢弃一些数据，来保证业务正常运行，并且来控制进程的内存增长问题。
**疑似“内存泄露”问题**
在一台Docker的宿主机上，运行了50个Docker实例，每个Docker都运行了复杂的业务场景，频繁的创建进程，在最初的产品实现上，启动时大约10M内存占用，一天后达到200M的情况。
经过我们Debug分析发现，在`ParseNetlinkMessage`处理内核发出的消息时，PID频繁创建带来内存频繁申请，对象频繁实例化，占用大量内存。同时，在Golang
GC时，扫描、清理动作带来大量CPU消耗。在代码中，发现对于 **linux/connector.h** 里的`struct cb_msg`、
**linux/cn_proc.h** 里的`struct
proc_event`结构体频繁创建，带来内存申请等问题，以及Golang的GC特性，内存申请后，不会在GC时立刻归还操作系统，而是在后台任务里，逐渐的归还到操作系统，见：[debug.FreeOSMemory](https://golang.org/src/runtime/debug/garbage.go?h=FreeOSMemory#L99)
> FreeOSMemory forces a garbage collection followed by an attempt to return as
> much memory to the operating system as possible. (Even if this is not
> called, the runtime gradually returns memory to the operating system in a
> background task.)
但在这个业务场景里，大量频繁的创建PID，频繁的申请内存，创建对象，那么申请速度远远大于释放速度，自然内存就一直堆积。
从文档中可以看出，`FreeOSMemory`的方法可以将内存归还给操作系统，但我们并没有采用这种方案，因为它治标不治本，没法解决内存频繁申请频繁创建的问题，也不能降低CPU使用率。
为了解决这个问题，我们采用了`sync.Pool`的内置对象池方式，来复用回收对象，避免对象频繁创建，减少内存占用情况，在针对几个频繁创建的对象做对象池化后，同样的测试环境，内存稳定控制在15M左右。
大量对象的复用，也减少了对象的数量，同样的，在Golang GC运行时，也减少了对象的扫描数量、回收数量，降低了CPU使用率。
## 项目进展
在产品的研发过程中，也遇到了一些问题，比如：
  1. etcd Client Lease Keepalive的Bug。
  2. Agent进程资源限制的Cgroup触发几次内核Bug。
  3. Docker宿主机上瞬时大量进程创建的性能问题。
  4. 网络监控模块在处理Nginx反向代理时，动辄几十万TCP链接的网络数据获取压力。
  5. 个别进程打开了10W以上的fd。
方法一定比困难多，但方法不是拍脑袋想出来的，一定要深入探索问题的根本原因，找到系统性的修复方法，具备高可用、高性能、监控告警、熔断限流等功能后，对于出现的问题，能够提前发现，将故障影响最小化，提前做处理。在应对产品运营过程中遇到的各种问题时，逢山开路，遇水搭桥，都可以从容的应对。
经过我们一年的努力，已经部署了除了个别特殊业务线之外的其他所有服务器，数量达几十万台，产品稳定运行。在数据完整性、准确性上，还有待提高，在精细化运营上，需要多做改进。
本篇更多的是研发角度上软件架构上的设计，关于安全事件分析、数据建模、运营策略等方面的经验和技巧，未来将会由其他同学进行分享，敬请期待。
## 总结
我们在研发这款产品过程中，也看到了网上开源了几款同类产品，也了解了他们的设计思路，发现很多产品都是把主要方向放在了单个模块的实现上，而忽略了产品架构上的重要性。
比如，有的产品使用了`syscall
hook`这种侵入性高的方案来保障数据完整性，使得对系统侵入性非常高，Hook代码的稳定性，也严重影响了操作系统内核的稳定。同时，Hook代码也缺少了监控熔断的措施，在几十万服务器规模的场景下部署，潜在的风险可能让安全部门无法接受，甚至是致命的。
这种设计，可能在服务器量级小时，对于出现的问题多花点时间也能逐个进行维护，但应对几十万甚至上百万台服务器时，对维护成本、稳定性、监控熔断等都是很大的技术挑战。同时，在研发上，也很难实现产品的快速迭代，而这种方式带来的影响，几乎都会导致内核宕机之类致命问题。这种事故，使用服务器的业务方很难进行接受，势必会影响产品的研发速度、推进速度；影响同事（SRE运维等）对产品的信心，进而对后续产品的推进带来很大的阻力。
以上是笔者站在研发角度，从可用性、可靠性、可控性、监控熔断等角度做的架构设计与框架设计，分享的产品研发思路。
笔者认为大规模的服务器安全防护产品，首先需要考虑的是架构的稳定性、监控告警的实时性、熔断限流的准确性等因素，其次再考虑安全数据的完整性、检测方案的可靠性、检测模型的精确性等因素。
九层之台，起于累土。只有打好基础，才能运筹帷幄，决胜千里之外。
## 参考资料
  1. 
  2. 
  3. 
  4. 
  5. 
  6. 
## 作者简介
[陈驰](https://www.cnxct.com/)，美团点评技术专家，2017年加入美团，十年以上互联网产品研发经验，专注于分布式系统架构设计，目前主要从事安全防御产品研发工作。
## 关于美团安全
美团安全部的大多数核心开发人员，拥有多年互联网以及安全领域实践经验，很多同学参与过大型互联网公司的安全体系建设，其中也不乏全球化安全运营人才，具备百万级IDC规模攻防对抗的经验。安全部也不乏CVE“挖掘圣手”，有受邀在Black
Hat等国际顶级会议发言的讲者，当然还有很多漂亮的运营妹子。
目前，美团安全部涉及的技术包括渗透测试、Web防护、二进制安全、内核安全、分布式开发、大数据分析、安全算法等等，同时还有全球合规与隐私保护等策略制定。我们正在建设一套百万级IDC规模、数十万终端接入的移动办公网络自适应安全体系，这套体系构建于零信任架构之上，横跨多种云基础设施，包括网络层、虚拟化/容器层、Server
软件层（内核态/用户态）、语言虚拟机层（JVM/JS
V8）、Web应用层、数据访问层等，并能够基于“大数据+机器学习”技术构建全自动的安全事件感知系统，努力打造成业界最前沿的内置式安全架构和纵深防御体系。
随着美团的高速发展，业务复杂度不断提升，安全部门面临更多的机遇和挑战。我们希望将更多代表业界最佳实践的安全项目落地，同时为更多的安全从业者提供一个广阔的发展平台，并提供更多在安全新兴领域不断探索的机会。
## 招聘信息
美团安全部正在招募Web&二进制攻防、后台&系统开发、机器学习&算法等各路小伙伴。如果你想加入我们，欢迎简历请发至邮箱[PI:EMAIL](mailto:PI:EMAIL)
具体职位信息可参考这里：
美团安全应急响应中心MTSRC主页：[security.meituan.com](https://security.meituan.com)
* * *