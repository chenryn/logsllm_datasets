[18] Sergey Ioffe and Christian Szegedy. 2015. Batch normalization: Accelerating deep
network training by reducing internal covariate shift. In International Conference
on Machine Learning. 448–456.
[19] Marc Juarez, Sadia Afroz, Gunes Acar, Claudia Diaz, and Rachel Greenstadt. 2014.
A critical evaluation of website fingerprinting attacks. In ACM Conference on
Computer and Communications Security (CCS). ACM, 263–274.
[20] Marc Juarez, Mohsen Imani, Mike Perry, Claudia Diaz, and Matthew Wright. 2016.
Toward an efficient website fingerprinting defense. In European Symposium on
Research in Computer Security (ESORICS). Springer, 27–46.
[21] Simonyan Karen and Zisserman Andrew. 2015. Very deep convolutional networks
for large-scale image recognition. (2015).
436–444.
[22] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. 2012. ImageNet classifica-
tion with deep convolutional neural networks. In Advances in Neural Information
Processing Systems. Curran Associates, Inc., 1097–1105.
[23] Y. LeCun, Y. Bengio, and G. Hinton. 2015. Deep learning. Nature 4 (2015),
[24] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. 1998. Gradient-based learning
applied to document recognition. 86 (1998), 2278–2324. Issue 11.
[25] Dmytro Mishkin, Nikolay Sergievskiy, and Jiri Matas. 2016. Systematic evaluation
of CNN advances on the ImageNet. CoRR abs/1606.02228 (2016).
[26] Se Eun Oh, Saikrishna Sunkam, and Nicholas Hopper. 2018. p-FP: Extraction,
Classification, and Prediction of Website Fingerprints with Deep Learning. "https:
//arxiv.org/abs/1711.03656.pdf". (2018). (accessed: August, 2018).
[27] Andriy Panchenko, Fabian Lanze, Andreas Zinnen, Martin Henze, Jan Pennekamp,
Klaus Wehrle, and Thomas Engel. 2016. Website fingerprinting at Internet scale.
In Network & Distributed System Security Symposium (NDSS). IEEE Computer
Society, 1–15.
[28] Andriy Panchenko, Lukas Niessen, Andreas Zinnen, and Thomas Engel. 2011.
Website fingerprinting in onion routing based anonymization networks. In ACM
Workshop on Privacy in the Electronic Society (WPES). ACM, 103–114.
[29] Mike Perry. 2013.
A critique of website traffic fingerprinting at-
tacks. Tor Project Blog. https://blog.torproject.org/blog/critique-website-traffic-
fingerprinting-attacks. (2013). (accessed: December, 2015).
[30] Mike Perry. 2015. Padding Negotiation. Tor Protocol Specification Proposal. https:
//gitweb.torproject.org/torspec.git/tree/proposals/254-padding-negotiation.txt.
(2015). (accessed: October 1, 2017).
[31] Vera Rimmer, Davy Preuveneers, Marc Juarez, Tom Van Goethem, and Wouter
Joosen. 2018. Automated Website Fingerprinting through Deep Learning. In
Proceedings of the 25nd Network and Distributed System Security Symposium
(NDSS 2018). Internet Society.
[32] Roei Schuster, Vitaly Shmatikov, and Eran Tromer. 2017. Beauty and the Burst:
Remote identification of encrypted video streams. In USENIX Security Symposium.
USENIX Association, 1357–1374.
[33] V. Shmatikov and M. Wang. 2006. Timing analysis in low-latency mix networks:
Attacks and defenses. In European Symposium on Research in Computer Security
(ESORIC). Springer, 18–33.
[34] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from
overfitting. Journal of Machine Learning Research 15 (2014), 1929–1958. http:
//jmlr.org/papers/v15/srivastava14a.html
[35] Q Sun, DR R Simon, and YM M Wang. 2002. Statistical identification of encrypted
web browsing traffic. In IEEE Symposium on Security and Privacy (S&P). IEEE,
19–30.
[36] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir
Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. 2015.
Going deeper with convolutions. (June 2015).
[37] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P. Manzagol. 2010. Stacked
denoising autoencoders: Learning useful representations in a deep network with
a local denoising criterion. Journal of Machine Learning Research 11 (2010),
3371–3408.
[38] Tao Wang, Xiang Cai, Rishab Nithyanand, Rob Johnson, and Ian Goldberg. 2014.
Effective attacks and provable defenses for website fingerprinting. In USENIX
Security Symposium. USENIX Association, 143–157.
[39] Tao Wang and Ian Goldberg. 2013. Improved website fingerprinting on Tor. In
ACM Workshop on Privacy in the Electronic Society (WPES). ACM, 201–212.
[40] Tao Wang and Ian Goldberg. 2016. On realistically attacking Tor with website
fingerprinting. In Proceedings on Privacy Enhancing Technologies (PoPETs). De
Gruyter Open, 21–36.
[41] Tao Wang and Ian Goldberg. 2017. Walkie-talkie: An efficient defense against
passive website fingerprinting attacks. In USENIX Security Symposium. USENIX
Association, 1375–1390.
[42] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. How Transfer-
able Are Features in Deep Neural Networks?. In Proceedings of the 27th Interna-
tional Conference on Neural Information Processing Systems - Volume 2 (NIPS’14).
MIT Press, Cambridge, MA, USA, 3320–3328. http://dl.acm.org/citation.cfm?id=
2969033.2969197
A DEEP FINGERPRINTING (DF) MODEL’S
ARCHITECTURE FOR WF ATTACKS
One of the compelling properties for CNN is the transferability of
the model. The transferability refers to the ability of the model to
be used as a base model for similar tasks. Instead of training an
entire CNN from scratch, the researcher can adapt the model to a
similar task, specifically with a similar input format.
Session 10A: TORCCS’18, October 15-19, 2018, Toronto, ON, Canada1940In WF research, to the best of our knowledge, we are the first
who provide the full technical details, guidelines and suggestions
on how to implement our CNN-based DF model to perform WF
attacks. In this section we provide details for our DF architecture
and its hyperparameters to create our model, and to allow other
researchers to apply it in their future work (see Figure 8):
Input Data. The input data for our DF model is the vector of packets’
directions with length 5,000 (1 x 5,000). We initially tried adjusting
the input dimension to be a matrix of shape similar to the matrices
typically fed into CNNs for image recognition tasks (e.g., 50 x 100
pixels). The accuracy for 2D input was reasonably good, but slightly
lower than 1D input. The major difference is training time: 1D input
is significantly faster than 2D input, even though the total number
of data points is the same for both input dimensions. We presume
this difference results from tensor operations that have to deal with
higher dimensions of data. We suggest that for the WF task, it is
more appropriate to use 1D input as it is faster for training and
provides better classification performance
Convolutional Layers (Block 1). Figure 8 describes the architecture
of our DF model divided by blocks, where a block comprises a set
of convolutional layers, a batch normalization layer, a max pooling
layer and a dropout layer. The first block in the DF is specially
important due to its proximity to the input.
As we mentioned in Section 5, since the nature of our input
is different to inputs considered in image recognition, we had to
find an activation function that fits our input values. We chose the
Exponential Linear Unit (ELU) because prior work has shown that
it provides fast and accurate classification with negative inputs [11,
25]. The results obtained from hyperparameters tuning suggested
that applying ELU in the first two convolutional layers followed
by ReLU with the rest of convolutional layers provides the best
accuracy compared to only using ReLU. This suggests that ELU
plays an important role in extracting hidden features from the input
data.
Dropout Regularization. CNN-based models are specially vulnera-
ble to overfitting, an issue that might be easily overlooked by the
developer. We applied a dropout technique to mitigate overfitting
in the design of our DF model. Our strategy to apply dropout was to
embed in between feature extraction (Blocks 1-4) and classification
(Fully-connected layers) using different rates. In feature extraction,
we deployed dropout right after the max pooling layer in each block
with 0.1 dropout rate. In addition, we added a dropout layer after
each fully-connected layer with rate 0.7 and 0.5, respectively. As we
observed from the hyperparameters tuning, the overfitting mostly
arises at the fully-connected layers, and it is less problematic at
the convolutional layers. Thus, we adjusted different dropout rates
appropriately according to this observation.
Batch Normalization. We studied the technique to accelerate model
learning called Batch Normalization (BN) [18]. This technique pro-
vides benefits to improve the classification performance in order to,
for instance, learn faster while maintaining or even increasing ac-
curacy. Moreover, it also partially serves as a regulation method as
well. Thus, we applied BN and dropout regularization together and
obtained a boost in both performance and generalization. However,
adding BN layers requires additional training time. We observed
Figure 8: Our design of DF model’s architecture used in WF
attacks
Input DataBlock 1Convolutional 1DBatch NormalizationActivation Layer32 Maps, Kernel: 1 x 8ELU (alpha = 1.0)Convolutional 1DBatch NormalizationActivation Layer32 Maps, Kernel: 1 x 8ELU (alpha = 1.0)Max PoolingPool: 1 x 8Block 2Convolutional 1DBatch NormalizationActivation Layer64 Maps, Kernel: 1 x 8ReLUConvolutional 1DBatch NormalizationActivation Layer64 Maps, Kernel: 1 x 8ReLUMax PoolingPool: 1 x 8Dropout Rate = 0.1Dropout Rate = 0.1Block 3Convolutional 1DBatch NormalizationActivation Layer128 Maps, Kernel: 1 x 8ReLUConvolutional 1DBatch NormalizationActivation Layer128 Maps, Kernel: 1 x 8ReLUMax PoolingPool: 1 x 8Dropout Rate = 0.1Block 4Convolutional 1DBatch NormalizationActivation Layer256 Maps, Kernel: 1 x 8ReLUConvolutional 1DBatch NormalizationActivation Layer256 Maps, Kernel: 1 x 8ReLUMax PoolingPool: 1 x 8Dropout Rate = 0.1Fully-Connected (FC) LayersFC Layer 1Batch NormalizationActivation Layer512 hidden unitsReLUDropout Rate = 0.7FC Layer 2Batch NormalizationActivation Layer512 hidden unitsReLUDropout Rate = 0.5FC LayerActivation LayerN hidden unitsSoftmaxOutput PredictionSession 10A: TORCCS’18, October 15-19, 2018, Toronto, ON, Canada1941that it added around 100% training time for each epoch compared to
the model that did not apply BN. Yet, we believe it is worth applying
BN, as the additional training time is compensated with a faster
learning rate (it requires less number of epochs to reach the same
level of accuracy) and can ultimately achieve higher testing accu-
racy. In our model, we applied BN right after every convolutional
and fully-connected layers.
In conclusion, the researcher can apply this model and our sug-
gestions to develop their own CNN-based model for WF. There
are other details that we cannot describe here due to limit space
including number of filters, kernel size, stride size and pool size.
However, we will ensure that our implementation details, along
with the source code and data used in this study, will be published
on a website upon publication of this paper, so that researchers can
reproduce our results.
B ATTACK PERFORMANCE METRICS
In this section, we define and justify the metrics we have used to
evaluate the success of the attacks. There are two scenarios under
which WF attacks are evaluated: closed-world and open-world.
B.1 Closed-world Evaluation
In the closed-world scenario, we assume that the user is limited
to visiting a fixed set of websites, and the attacker knows this set
and can train his classifier on it. In this scenario, the success of the
attacker is simply measured as the ratio of the number of correctly
classified traces to the total number of traces, we call this ratio the
attack’s accuracy.
N
Accuracy = Pcorr ect
(1)
Pcorr ect is the total number of correct predictions. A correct
prediction is defined as the output of the classifier matching the
label of the website to which the test trace belongs. N is the total
number of instances in the test set.
B.2 Open-world Evaluation
In the open-world scenario, the user may visit any of a large number
of websites. Since the attacker cannot effectively train on so many
sites, he selects a relatively small set to train his classifier on (the
monitored set). For experimentation, we model the rest of the Web
using a set of sites that the attacker does not try to identify with
WF (the unmonitored set). Note that the unmonitored set is more
than two orders of magnitude larger than the monitored set in our
experiments. As mentioned in Section 5.7, we measure Precision
and Recall in this scenario.
Precision =
T P
T P + F P
Recall =
T P
T P + F N
(2)
(3)
Where:
that are correctly classified as monitored websites.
• T P is the total number of test samples of monitored websites
• T N is the total number of test samples of unmonitored web-
sites that are correctly classified as unmonitored websites.
• F P is the total number of test samples of unmonitored web-
• F N is the total number of monitored websites that are mis-
sites that are misclassified as monitored websites.
classified as unmonitored websites.
In addition, the attacker can measure precision and recall to tune
the system. If his primary goal is to reliably determine that a user
has visited a particular monitored website, one can try to decrease
false positives at the cost of true positives and thus increase the
precision of the attack. On the other hand, if the attacker aims to
cast a wide net and identify potential visitors to the monitored web
sites, then recall is more important, and the adversary should tune
the system to increase true positives at the cost of additional false
positives.
C OPEN-WORLD ROC CURVE
We plot the ROC curve for all the WF attacks against non-defended,
WTF-PAD and W-T datasets using the standard model in the open-
world scenario as shown in Figures 9a−9c. The ROC curve allows
us to evaluate the classifier and strive for a trade-off between TPR
and FPR. For example, the best overall results for DF against non-
defended traffic might be optimizing for high TPR, with 0.98 TPR
and 0.03 FPR, and optimizing for low FPR, with 0.94 TPR and 0.004
FPR.
Session 10A: TORCCS’18, October 15-19, 2018, Toronto, ON, Canada1942e
t
a
r
e
v
i
t
i
s
o
p
e
u
r
T
1
0.8
0.6
0.4
0.2
0
0
1
0.8
0.6
0.4
0.2
0
0
1
0.8
0.6
0.4
0.2
0
0
kNN
kFP
AWF
Baseline
0.4
0.2
False positive rate
CUMUL
DF
SDAE
0.6
kNN
kFP
AWF
Baseline
0.2
0.1
False positive rate
CUMUL
DF
SDAE
0.3
kNN
kFP
AWF
Baseline
0.2
0.4
0.6
False positive rate
CUMUL
DF
SDAE
0.8
(a) Non-defended dataset
(b) WTF-PAD dataset
(c) W-T dataset
Figure 9: ROC curve in Open-world scenario
Session 10A: TORCCS’18, October 15-19, 2018, Toronto, ON, Canada1943