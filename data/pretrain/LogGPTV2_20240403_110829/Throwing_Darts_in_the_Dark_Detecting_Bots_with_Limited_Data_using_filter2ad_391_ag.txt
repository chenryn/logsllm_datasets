APPENDIX B: IMPACT OF SLIDING WINDOW SIZES
The size of the sliding window (w) could affect the detection
results. Our dataset (a month worth of data) does not allow
us to test big window sizes. As such, we test the window
size of 3 days, 5 days, and 7 days and present the results in
Table XV. The results show that the window size of 7 gives
better results than 3 and 5 when using 1% of the training data.
A smaller window size means the model uses less historical
data to estimate the entity frequency (which could hurt the
performance, especially when labeled data is sparse). However,
a smaller window size also means the model uses more recent
historical data to estimate entity frequency (which may help
to improve the performance). This trend was observed when
using 100% of the training data, as shown in Table XV.
TABLE XV: Results of using different sliding window sizes (in days).
We use August-18 dataset from Website B; Models are trained with
1% or 100% of the training dataset.
1% of Data
100% of Data
Window Size
Precision
3
5
7
3
5
7
0.585
0.600
0.601
0.912
0.937
0.888
Recall
0.331
0.314
0.355
0.915
0.889
0.877
F1
0.422
0.412
0.446
0.913
0.910
0.883
APPENDIX C: FEEDING SYNTHETIC DATA TO OTHER
CLASSIFIERS
omitted because both terms are constant, and the gradients
with respect to these terms are mostly zero.
The discriminator of ODDS can be directly used for bot
detection. A natural follow-up question is, what if we feed
the synthetic data generated by ODDS to other classiﬁers? Can
we improve the performance of the original classiﬁers? How
is the performance compared with using the discriminator?
To answer these questions, we feed the synthetic data to our
LSTM model, and a traditional method, Random Forest (RF).
We generate 600 synthetic data points based on the 1% of bot
training samples in the August 2018 dataset.
TABLE XVI: Feeding synthetic data to LSTM and RF. We use
August-18 dataset from Website B; Models are trained with 1% of
the training dataset.
Synthetic data?
Precision
RF
LSTM
ODDS
No
Yes
No
Yes
Yes
0.883
0.826
0.601
0.698
0.729
Recall
0.202
0.570
0.355
0.757
0.845
F1
0.343
0.596
0.446
0.719
0.783
As shown in Table XVI, by feeding synthetic data to the
classiﬁer training, both models’ performance is improved. The
F1 score of LSTM is improved from 0.446 to 0.719, and the
F1 score of RF is improved from 0.343 to 0.596. Despite
the performance improvements, the LSTM model and the RF
model are still not as accurate as the discriminator of ODDS.
One possible explanation is that the synthetic data is generated
in the latent space. To feed the data to other classiﬁers, we
need to use the decoder to convert the latent vectors back to the
original feature space, which may introduce some distortions
during the reconstruction. In our paper, the discriminator is a
better choice for bot detection also because it eliminates the
need/overhead for training a separate classiﬁer.
APPENDIX D: HYPERPARAMETERS OF ODDS
We have examined the model performance with respect to
different hyperparameter settings for ODDS. The methodology
is to split the training dataset into a training set and a validation
set, and use the validation set to tune the parameters. For
example, we train the model using 80% of ﬁrst two weeks
of August 2018, and use 20% of the data as the validation
set to justify our parameters setting. We ﬁx all the parameters
to the default setting, and then examine the validation result
by changing one parameter at a time. Figures 10a shows the
validation results for different  values.  is the threshold for
ODDS’s generators (both G1 and G2) to determine if the
generated bot samples are in the high-density regions of benign
users). We set  to the Kth percentile of real benign users’
distribution. Figure 10b shows different α. α is the term for G2
to control how close the synthesized bot samples are to real bot
samples and to real benign samples. For website A and website
B, their validation performance is not too sensitive to α and
. For website C, α = 0.1 can achieve the highest validation
performance. Note that τ1, τ2 and C in our equations can be
(a) F1 score for different 
(b) F1 score for different α
Fig. 10: F1 score on the validation set for different  and α for
website A, B, C.
(a) F1 score for different dimen-
sions of ﬁrst layer
Fig. 11: F1 scores on the validation set using different dimensions
for layers in the generators and the discriminator for website A, B,
C.
(b) F1 score for different dimen-
sions of second layer
Figure 11 shows the validation performance for different
dimensions of the ﬁrst and second hidden layers of the
discriminator and generator. These results suggest setting 100
and 50 dimensions for the ﬁrst and the second layers lead to
a good validation performance.
TABLE XVII: Characterizing different datasets (August 2018).
Website
Avg. Distance Between
Train and Test (benign)
Avg. Distance Between
Train and Test (bots)
A
B
C
0.291
0.233
0.303
0.237
0.358
0.313
We notice that website C has the best validation F1 score
in the different settings above. This is different from the main
results on the testing set where C has the lowest F1 score (see
Figure 5). We suspect that C’s testing data is very different
from the training data, which could explain why C has the best
validation result but has the worst testing result. Table XVII
shows some statistics to support this hypothesis. We compute
the average distance between the training and testing samples,
for the bots and benign users separately. We notice that C has
a high distance between training and testing set, especially for
the benign users. It is possible that concept drift happened
even during a short time span such as within a month. Such
discrepancies between the training and the testing data could
hurt the testing performance.
 0.8 0.82 0.84 0.86 0.88 0.9 0.92 0.94 0.9660th70th80th90th95thF1 Scoreε ValuesABC 0.8 0.82 0.84 0.86 0.88 0.9 0.92 0.94 0.9600.050.10.150.2F1 Scoreα ValuesABC 0.8 0.82 0.84 0.86 0.88 0.9 0.92 0.94 0.961201009080F1 Score1st Hidden Layer DimensionsABC 0.8 0.82 0.84 0.86 0.88 0.9 0.92 0.94 0.9670605040F1 Score2nd Hidden Layer DimensionsABCTABLE XVIII: Training with 100% or 1% of the training data (the ﬁrst two weeks of August-18); Testing on the last two weeks of August-18.
Method
RF 100%
OCAN 100%
LSTM (ours) 100%
ODDS (ours) 100%
RF 1%
OCAN 1%
LSTM (ours) 1%
ODDS (ours) 1%
FN rate
0.069
0.065
0.048
0.059
0.185
0.049
0.054
0.056
FN
221
209
152
190
593
157
173
181
FP
342
363
416
360
364
503
471
481
Website A
Website B
Website C
FN rate
0.386
0.192
0.123
0.086
0.703
0.283
0.611
0.158
FN
767
383
245
171
1396
564
1214
314
FP
244
820
219
200
254
632
261
615
FN rate
0.345
0.454
0.270
0.199
0.365
0.670
0.294
0.253
FN
2898
3814
2264
1670
3067
5622
2467
2128
FP
1370
666
1632
1401
2625
1486
2685
2411
APPENDIX E: ONE-CLASS SVM.
Our method and other anomaly detection methods share
a similar assumption that the benign data is relatively more
stable. In our paper, we selected OCAN, a recently published
anomaly detection method, as the comparison baseline. Here
we show the results of another popular anomaly detection
method called One-class SVM [82]. One-class SVM aims
to separate one class of samples from all
the others by
constructing a hyper-plane around the data samples. In this
experiment, we use “benign” data as the known class. As
shown in Table XIX, this anomaly detection method does not
perform well on our dataset. One-class SVM tends to a high
recall but a very low precision. The performance is not as high
as OCAN (and our method ODDS) in both settings (1% and
100% training data).
TABLE XIX: Evaluation of One-class SVM using August-18 dataset.
Website % of Data
Precision
A
B
C
1%
100%
1%
100%
1%
100%
0.407
0.441
0.110
0.09
0.336
0.336
Recall
0.991
0.990
0.988
0.990
0.745
0.747
F1
0.577
0.611
0.193
0.197
0.463
0.464
APPENDIX F: FALSE POSITIVES AND FALSE NEGATIVES
To complement the main results in Table VIII, we add
a new Table XVIII to show the absolute numbers of false
positives and false negatives as well as the false negative rate.
False negative rate is the fraction of the true bots that are
misclassiﬁed as benign. Combining the results in Table VIII,
and Table XVIII, we show that our system ODDS can dras-
tically increase the number of detected true bots (reducing
false negative rate) while producing comparable number of
false positives. In practice, these false positives can be further
reduced by the CAPTCHA system (it affects user experience
but at a reasonably small scale).