adversary is assumed to be able to replicate client-side set-
tings such as operating system, network connection or Tor
Browser Bundle (TBB) version. This assumption allows
researchers to train and test on data collected using the
same settings. Depending on the type of attack this may
be impossible, as the adversary may encounter diﬃculties in
detecting and replicating users’ conﬁguration (especially in
non-targeted attacks).
Assumption
Closed-world
Browsing behavior
Page load parsing
No background noise
Replicability
Template websites
Explicitly made by
[11, 26]
[11]
[3, 11, 23, 26, 32]
[3, 11, 23, 26, 32]
[11, 26]
[3]
Table 1: Assumptions and references to papers that make
explicit mention to them.
4. EVALUATION
In this section we challenge some of the assumptions de-
scribed in Section 3. The assumptions are: Closed-world,
browsing behavior, no background traﬃc and replicability.
1 https://tails.boum.org/
For each assumption we identify the variables that are ruled
out of the model by the assumption. Our objective is to
measure the eﬀect of these variables on the accuracy of WF
attacks.
4.1 Datasets
We used two diﬀerent lists of URLs for our crawls: the
top Alexa ranking and the Active Linguistic Authentication
Datatset (ALAD) [15]. The Alexa dataset is a well known
list of most visited URLs that has been widely used in previ-
ous WF studies as well as in other research domains. Testing
on data collected by crawling URLs in the Alexa ranking im-
plies that the adversary always knows which pages the users
are going to visit and can train a classiﬁer on those pages.
We want to test the reliability of a classiﬁer when this as-
sumption does not hold.
The
Active
Linguistic
Authentication
Dataset
(ALAD) [15]
is a dataset of web visits of real-world
users, collected for the purpose of behavioral biometrics
evaluation. The complete dataset contains data collected
from 80 paid users in a simulated work environment. These
users used the computers provided by the researchers
for a total of 40 hours to perform some assigned tasks:
open-ended blogging (at least 6 hours a day) and summary
writing of given news articles (at least 2 hours a day). We
found that these users were browsing the web to check their
emails, social network sites and searching for jobs.
Top Alexa Home page Other pages Not in Alexa
100
1000
10000
4.84%
5.81%
6.33%
50.34%
60.26%
68.01%
44.82%
33.93%
25.66%
visit. The batches are collected in a round-robin fashion,
hours apart from each other. We made over 50 such crawls
for the experiments presented in this paper and we will share
the data with other researchers upon request.
We used two physical and three cloud-based virtual ma-
chines to run crawls from diﬀerent geographical locations.
In order to have identical crawler conﬁgurations, we used
Linux Container (LXC) based virtualization running on the
same distribution and version of the GNU/Linux operating
system. We disabled operating system updates to prevent
background network traﬃc and never ran more than one
crawler on a machine at the same time. We made sure that
the average CPU load of the machines is low, as this may
aﬀect the WF defenses shipped in the TBB3.
4.3 Methodology
In order to reduce the confounding eﬀect of other vari-
ables in the measurement, we crawled the same set of web-
pages multiple times by changing the value of the variable
under evaluation and ﬁxing the rest of the variables. For
each variable that we wanted to evaluate, we deﬁned a con-
trol crawl by setting the variable to its default value (e.g.,
UseEntryGuards = 1), and a test crawl, by setting the vari-
able to the value of interest (e.g., UseEntryGuards = 0).
Note that the randomized nature of the path selection
algorithm of Tor and eﬀect of time are two control variables
that we cannot completely ﬁx when measuring the eﬀect of
other variables. We tried to overcome this by using cross-
validation and minimizing the time gap between the control
and test crawl in all of our experiments.
These experiments are composed by the two following
steps:
Table 2: ALAD dataset statistics
1. k-fold cross-validation using data of the control crawl.
The users browsed total 38,716 unique URLs (excluding
news articles) which were loaded 89,719 times. These URLs
include some top ranked Alexa sites, such as google.com,
facebook.com and youtube.com and some sites that are not
in Alexa top 1 million list, such as bakery-square.com,
miresearch.org. Over 44% of the sites were not in the
Alexa top 100, and 25% of the sites were not in the Alexa
top 10000. Over 50% sites were pages other than the
front page (shown in Table 2), such as diﬀerent search
pages on Google/Wikipedia, a subdomain of a site (such
as dell.msn.com) and logged-in pages of Facebook.
4.2 Data collection
To collect network traces, we used TBB combined with
Selenium2 to visit the pages, and recorded the network pack-
ets using a network packet dump tool called dumpcap. We
used the Stem library to control and conﬁgure the Tor pro-
cess [30]. Following Wang and Goldberg we extended the
10 minute circuit renewal period to 600,000 and disabled
the UseEntryGuards to avoid using a ﬁxed set of guard
nodes [32]. We also used their methods to parse the Tor
cells and remove noise by ﬁltering acknowledgements and
SENDMEs.
We crawled the webpages in batches. For each batch, we
visited each page 4 times and collected between 5 and 10
batches of data in each crawl, resulting in 20 to 40 visits for
each webpage in total. We waited 5 seconds after each page
had ﬁnished loading and left 5 second pauses between each
2http://docs.seleniumhq.org/
2. Evaluate classiﬁer’s accuracy training on the control
crawl and testing with data from the test crawl.
The accuracy obtained in Step 1, the case in which the ad-
versary can train and test under the exact same conditions,
is used as a baseline for comparison. We then compare the
accuracy obtained in Step 2 with this baseline. We specify
the details of the cross-validation in Step 1 and the testing
in Step 2 later in this section.
Classiﬁers designed for WF attacks are based on features
extracted from the length, direction and inter-arrival times
of network packets, such as unique number of packet lengths
or the total bandwidth consumed. The variables we evaluate
in this section aﬀect traﬃc features and therefore may aﬀect
each classiﬁer in a diﬀerent manner (cf.,
[33]). For the
present study we tried to pick a classiﬁer for each of the
learning models and sets of features studied in prior work.
In Table 3 we list the classiﬁers that we have evaluated.
We observed that the relative accuracy changes are con-
sistent across the classiﬁers and the variables we evaluated.
In most cases, classiﬁer W performed better than the others.
For this reason, our presentation of the results is focused on
classiﬁer W.
Classiﬁer W is based on the Fast Levenshtein-like distance
[32]. We used this classiﬁer instead of the one based on the
OSAD 4 distance presented in the same paper. Although
3
https://trac.torproject.org/projects/tor/ticket/
8470\#comment:7
4Optimal String Alignment Distance
Name Model
H [11] Naive Bayes
P [23]
SVM
D [9]
N-grams
Features
Packet lengths
Packet lengths
Order
Total bytes
Total time
Up/Downstream bytes
Bytes in traﬃc bursts
W [32]
T
SVM (Fast-Levenshtein) Cell traces
Decision tree
Same features as P
Table 3: Classiﬁers used for the evaluation.
the latter attained greater accuracy in Tor, it is considerably
slower and can become impractical in certain circumstances,
as we will further analyse in Section 6. Furthermore, for the
OSAD distance we obtained over 90% accuracy scores in
our control crawls and, as in the original paper, we consis-
tently observed a 20% decrease in accuracy when evaluating
classiﬁer W. For this reason, we believe the results obtained
with this classiﬁer are comparable with its more accurate
counterpart.
For the evaluation of each classiﬁer we followed a similar
approach to the one described by Dyer et al. First, we ﬁxed
the size of the world to a certain number of pages (k). For
each page of the training crawl, we selected ntrain random
batches and picked Ttrain traﬃc traces in total. For each
page of the testing crawl, we selected ntest random batches
and picked Ttest traces in total. We averaged the results by
repeating each experiment m times, each time choosing a
diﬀerent training and testing set. Then, the accuracy of the
classiﬁer was calculated by T otal correct predictions
,
m Ttest
where p is the total number of correct predictions. We made
sure that for the validation of the control crawl, training and
testing traces were never taken from the same batch. The
classiﬁcation parameters are listed in Table 4.
T otal test instances =
p
Number of sites in the world.
Parameter Description
k
ntrain/test Number of batches for training/testing.
Ttrain/test Number of instances for training/testing.
p
m
Total number of predictions.
Number of trials.
Table 4: Description of classiﬁcation parameters deﬁned for
an experiment.
From now on, we refer to Acc control as the average ac-
curacy obtained for the m trials in control crawl (Step 1)
and Acc test as the average accuracy for m trials obtained
in Step 2. In the results, the standard deviation of the ac-
curacy obtained for m trials is shown in parentheses next to
the average value.
We also have designed our own attack based on decision
tree learning and using the features proposed by Panchenko
et al. [23]. Decision tree learning uses binary trees as data
structures to classify observations. Leaves represent class
labels and nodes are conditions on feature values that di-
vide the set of observations. Features that better divide the
data according to a certain criterion (information gain in
our case) are chosen ﬁrst.
For these experiments we have extended Dyer et al.’s
Peekaboo framework [9] and the source code of the classiﬁer
presented by Wang and Goldberg [32].
4.4 Time
Webpages are constantly changing their content. This is
reﬂected in the traﬃc traces and consequently in the accu-
racy of the WF attack. In this section we used crawls of the
Alexa Top 100 pages taken at diﬀerent instants in time to
evaluate the eﬀect of staleness on WF.
)
%
(
y
c
a
r
u
c
c
A
100
80
60
40
20
0
0
20
40
60
80
Time (days)
Figure 4: Staleness of our data over time. Each data point is
the 10-fold crossvalidation accuracy of the classiﬁer W which
is trained using the traces from t = 0 and tested using traces
from 0 ≤ t ≤ 90. For every experiment m = 10, ntrain = 9,
ntest = 1, Ttrain = 36, ttest = 4 and k = 100.
For this experiment, we train a classiﬁer using the traces
of the control crawl, collected at time t = 0, and test it using
traces collected within 90 days of the control crawl (0 ≤ t ≤
90). Figure 4 shows the accuracy obtained for the W classiﬁer
for crawls over the same list of URLs at diﬀerent points
in time. For the rest of the classiﬁers we observed similar
drops in accuracy. The ﬁrst data point is the accuracy of the
control crawl, taken at t = 0. The second point is taken 9
days after the control crawl (t = 9), and so on until the last
data point that corresponds to a test crawl taken 90 days
later.
We observe that the accuracy drops extremely fast over
time. In our experiments it takes less than 10 days to drop
under 50%. Since we observed such a drop in accuracy due
to time, we picked control and test crawls for the following
experiments that fall within a period of 5 days.
This strong eﬀect of time in the classiﬁer’s accuracy poses
a challenge to the adversary who will need to train the clas-
siﬁer on a regular basis. Depending on the size of the world
that he aims to cover, the cost of training may exceed the
time in which data provides reasonable accuracy rates. We
discuss this point in more detail in Section 6.
4.5 Multitab browsing
In this experiment we evaluate the success of a classiﬁer
when trained on single tab browsing, as in prior WF attacks,
and tested on traces collected with multitab browsing.
In order to simulate multitab browsing behaviour, we
crawled the home pages of Alexa Top 100 sites [1] while load-
ing another webpage in the background. The background
page was loaded with a delay of 0.5-5 seconds and was cho-
sen at random from the same list, but kept constant for