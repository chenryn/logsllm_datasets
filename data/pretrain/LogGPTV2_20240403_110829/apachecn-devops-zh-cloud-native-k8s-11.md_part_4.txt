重要说明
我们强烈建议您阅读并查看代码管道和代码构建的文档，因为我们不会涵盖本章中的所有基础知识。代码构建可以在[https://docs . AWS . Amazon . com/code build/latest/user guide/welcome . html](https://docs.aws.amazon.com/codebuild/latest/userguide/welcome.html)找到文档，codepipeline 可以在[https://docs . AWS . Amazon . com/code pipeline/latest/user guide/welcome . html](https://docs.aws.amazon.com/codepipeline/latest/userguide/welcome.html)找到文档。
实际上，您将有两个代码管道，每个都有一个或多个代码构建步骤。第一个代码管道是在 AWS 代码提交或另一个 Git 存储库中(如 GitHub)的代码更改时触发的。
该管道的第一步代码构建运行测试并构建容器映像，将映像推送到 AWS **弹性容器存储库** ( **ECR** )。第一个管道的第二个代码构建步骤将新映像部署到 Kubernetes。
每当我们使用 Kubernetes 资源文件(基础设施存储库)向我们的二级 Git 存储库提交变更时，第二个代码管道就会被触发。它将使用相同的过程更新 Kubernetes 资源。
让我们从第一个代码管道开始。如前所述，它包含两个代码构建步骤:
1.  首先，测试和构建容器映像，并将其推送到 ECR
2.  其次，将更新的容器部署到 Kubernetes
正如我们在本节前面提到的，我们不会在代码到容器映像管道上花费太多时间，但是这里有一个示例(未准备好生产)`codebuild` YAML 来实现第一步:
Pipeline-1-codebuild-1.yaml:
```
version: 0.2
phases:
  build:
    commands:
      - npm run build
  test:
    commands:
      - npm test
  containerbuild:
    commands:
      - docker build -t $ECR_REPOSITORY/$IMAGE_NAME:$IMAGE_TAG .
  push:
    commands:
      - docker push_$ECR_REPOSITORY/$IMAGE_NAME:$IMAGE_TAG
```
这个代码构建管道由四个阶段组成。代码构建管道规范是在 YAML 编写的，并且包含一个对应于代码构建规范版本的`version`标签。然后，我们有一个`phases`部分，按顺序执行。这个代码构建首先运行一个`build`命令，然后在测试阶段运行一个`test`命令。最后，`containerbuild`阶段创建容器映像，`push`阶段将映像推送到我们的容器存储库中。
需要记住的一点是，在 CodeBuild 中，每个前面带有`$`的值都是一个环境变量。这些可以通过 AWS 控制台或 AWS 命令行界面定制，有些可以直接从 Git 存储库中获得。
现在让我们来看看 YAML 的第一个代码管道的第二个代码构建步骤:
管道-1-代码构建-2.yaml:
```
version: 0.2
phases:
  install:
    commands:
      - curl -o kubectl https://amazon-eks.s3.us-west-2.amazonaws.com/1.16.8/2020-04-16/bin/darwin/amd64/kubectl  
      - chmod +x ./kubectl
      - mkdir -p $HOME/bin && cp ./kubectl $HOME/bin/kubectl && export PATH=$PATH:$HOME/bin
      - echo 'export PATH=$PATH:$HOME/bin' >> ~/.bashrc
      - source ~/.bashrc
  pre_deploy:
    commands:
      - aws eks --region $AWS_DEFAULT_REGION update-kubeconfig --name $K8S_CLUSTER
  deploy:
    commands:
      - cd $CODEBUILD_SRC_DIR
      - kubectl set image deployment/$KUBERNETES-DEPLOY-NAME myrepo:"$IMAGE_TAG"
```
让我们把这个文件分解一下。我们的代码构建设置分为三个阶段:`install`、`pre_deploy`和`deploy`。在`install`阶段，我们安装 kubectl CLI 工具。
然后，在`pre_deploy`阶段，我们使用一个 AWS CLI 命令和几个环境变量来更新我们的`kubeconfig`文件，以便与我们的 EKS 集群通信。在任何其他配置项工具中(或者在不使用 EKS 时)，您可以使用不同的方法向配置项工具提供集群凭据。这里使用安全选项很重要，因为将`kubeconfig`文件直接包含在 Git 存储库中是不安全的。通常，环境变量的某种组合在这里会很好。Jenkins、CodeBuild、CircleCI 和更有自己的系统针对这一点。
最后，在`deploy`阶段，我们使用`kubectl`用第一个代码构建步骤中指定的新映像标签更新我们的部署(也包含在环境变量中)。这个`kubectl rollout restart`命令将确保为我们的部署启动新的 Pod 。结合使用`Always`的`imagePullPolicy`，这将导致我们部署新的应用版本。
在这种情况下，我们使用 ECR 中的特定映像标签名称来修补我们的部署。`$IMAGE_TAG`环境变量将自动填充来自 GitHub 的最新标签，因此我们可以使用它来自动向我们的部署推出新的容器映像。
接下来，让我们看看我们的第二个代码管道。这一个只包含一个步骤——它从一个单独的 GitHub 存储库(我们的“基础设施存储库”)中监听变更。这个存储库不包含我们的应用本身的代码，而是包含 Kubernetes 资源 YAMLs。因此，我们可以更改 Kubernetes 资源的 YAML 值——例如，部署中副本的数量，并在代码管道运行后在 Kubernetes 中看到它的更新。这个模式可以很容易地扩展到使用 Helm 或 Kustomize。
让我们看看第二个代码管道的第一步，也是唯一一步:
Pipeline-2-codebuild-1.yaml:
```
version: 0.2
phases:
  install:
    commands:
      - curl -o kubectl https://amazon-eks.s3.us-west-2.amazonaws.com/1.16.8/2020-04-16/bin/darwin/amd64/kubectl  
      - chmod +x ./kubectl
      - mkdir -p $HOME/bin && cp ./kubectl $HOME/bin/kubectl && export PATH=$PATH:$HOME/bin
      - echo 'export PATH=$PATH:$HOME/bin' >> ~/.bashrc
      - source ~/.bashrc
  pre_deploy:
    commands:
      - aws eks --region $AWS_DEFAULT_REGION update-kubeconfig --name $K8S_CLUSTER
  deploy:
    commands:
      - cd $CODEBUILD_SRC_DIR
      - kubectl apply -f .
```
如您所见，这个代码构建规范与我们之前的非常相似。像以前一样，我们安装 kubectl 并准备好与我们的 Kubernetes 集群一起使用。因为我们在 AWS 上运行，所以我们使用 AWS 命令行界面来实现，但是这可以通过多种方式来实现，包括向我们的代码构建环境添加一个`Kubeconfig`文件。
不同之处在于，我们不是用新版本的应用修补特定的部署，而是在整个基础架构文件夹中运行一个全面的`kubectl apply`命令。这样就可以将 Git 中执行的任何更改应用到我们集群中的资源。例如，如果我们通过更改`deployment.yaml`文件中的值将我们的部署从 2 个副本扩展到 20 个副本，它将在这个代码管道步骤中部署到 Kubernetes，并且部署将会扩大。
现在我们已经介绍了使用集群外 CI/CD 环境对 Kubernetes 资源进行更改的基础知识，让我们来看看一个完全不同的 CI 范例，其中管道在我们的集群上运行。
## 用 FluxCD 实现 Kubernetes CI
对于我们的集群内配置项工具，我们将使用 **FluxCD** 。集群内 CI 有几个选项，包括 **ArgoCD** 和 **JenkinsX** ，但是我们喜欢 **FluxCD** 是因为它相对简单，并且它自动用新的容器版本更新 pods，而不需要任何额外的配置。另外，我们将使用 FluxCD 的 Helm 集成来管理部署。让我们从安装 FluxCD 开始(我们假设您已经安装了本章前面部分的 Helm)。在撰写本书时，这些安装遵循 Helm 兼容性的官方 FluxCD 安装说明。
官方的 FluxCD 文档可以在[https://docs.fluxcd.io/](https://docs.fluxcd.io/)找到，强烈推荐大家看一下！FluxCD 是一个非常复杂的工具，我们在这本书里只是在抓表面。全面回顾不在讨论范围之内，我们只是想向您介绍集群内 CI/CD 模式和相关工具。
让我们从在集群上安装 FluxCD 开始我们的回顾。
### 安装磁通卡(H3)
使用 Helm 只需几个步骤即可轻松安装 FluxCD:
1.  首先，我们需要添加通量舵图表库:
    ```
    helm repo add fluxcd https://charts.fluxcd.io
    ```
2.  接下来，我们需要添加一个自定义资源定义，FluxCD 需要该定义以便能够使用 Helm 版本:
    ```
    kubectl apply -f https://raw.githubusercontent.com/fluxcd/helm-operator/master/deploy/crds.yaml
    ```
3.  Before we can install the FluxCD Operator (which is the core of FluxCD functionality on Kubernetes) and the FluxCD Helm Operator, we need to create a namespace for FluxCD to live in:
    ```
    kubectl create namespace flux
    ```
    现在我们可以安装 FluxCD 的主要部分，但是我们需要给 FluxCD 一些关于我们的 Git 存储库的附加信息。
    为什么呢？因为 FluxCD 使用 GitOps 模式进行更新和部署。这意味着 FluxCD 将每隔几分钟主动联系我们的 Git 存储库，而不是响应 Git 钩子，例如 CodeBuild。
    FluxCD 还将通过基于拉动的策略来响应新的 ECR 映像，但我们稍后会谈到这一点。
4.  To install the main pieces of FluxCD, run the following two commands and replace `GITHUB_USERNAME` and `REPOSITORY_NAME` with the GitHub user and repository that you will be storing your workload specs (Kubernetes YAML or Helm charts) in.
    这个指令集假设 Git 存储库是公共的，但很可能不是。由于大多数组织都使用私有存储库，所以 FluxCD 有特定的配置来处理这种情况——只需在[https://docs . FluxCD . io/en/latest/tutories/get-start-helm/](https://docs.fluxcd.io/en/latest/tutorials/get-started-helm/)查看文档。事实上，要了解 FluxCD 的真正威力，您需要让它在任何情况下都能高级访问您的 Git 存储库，因为 FluxCD 可以写入您的 Git 存储库，并在创建新的容器映像时自动更新清单。然而，我们不会在本书中讨论这个功能。FluxCD 文档绝对值得仔细阅读，因为这是一项复杂的技术，具有许多特性。要告诉 FluxCD 要查看哪个 GitHub 存储库，可以在使用 Helm 安装时设置变量，如下命令所示:
    ```
    helm upgrade -i flux fluxcd/flux \
    --set git.url=PI:EMAIL:GITHUB_USERNAME/REPOSITORY_NAME \
    --namespace flux
    helm upgrade -i helm-operator fluxcd/helm-operator \
    --set git.ssh.secretName=flux-git-deploy \
    --namespace flux
    ```
    如您所见，我们需要传递我们的 GitHub 用户名、我们的存储库的名称，以及一个将用于我们在 Kubernetes 中的 GitHub 机密的名称。
    此时，FluxCD 已完全安装在我们的集群中，并指向我们在 Git 上的基础架构存储库！如前所述，这个 GitHub 存储库将包含 Kubernetes YAML 或 Helm 图表，FluxCD 将在此基础上更新集群中运行的工作负载。
5.  为了让 Flux 真正有所作为，我们需要为 Flux 创建实际的清单。我们使用一个`HelmRelease` YAML 文件来完成，如下所示:
赫尔穆特-1：
```
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  name: myapp
  annotations:
    fluxcd.io/automated: "true"
    fluxcd.io/tag.chart-image: glob:myapp-v*
spec:
  releaseName: myapp
  chart:
    git: ssh://PI:EMAIL///myhelmchart
    ref: master
    path: charts/myapp
  values:
    image:
      repository: myrepo/myapp
      tag: myapp-v2
```
让我们把这个文件分开。我们正在指定 Git 存储库，Flux 将在其中为我们的应用找到 Helm 图表。我们还在`HelmRelease`上标注了`automated`标注，告知 Flux 每隔几分钟去轮询一次容器镜像库，看看是否有新版本需要部署。为了帮助实现这一点，我们包含了一个`chart-image`过滤器模式，标记的容器映像必须匹配该模式才能触发重新部署。最后，在值部分，我们有 Helm 值，将用于 Helm 图表的初始安装。
为了给 FluxCD 这个信息，我们只需要将这个文件添加到我们的 GitHub 存储库的根目录中，并推动一个更改。
一旦我们将这个发布文件`helmrelease-1.yaml`添加到我们的 Git 存储库中，Flux 将在几分钟内获取它，然后在`chart`值中查找指定的 Helm 图表。只有一个问题——我们还没有成功！
目前，我们在 GitHub 上的基础设施存储库只包含我们的单一 Helm 版本文件。文件夹内容如下所示:
```
helmrelease1.yaml
```
为了结束循环并允许 Flux 实际部署我们的 Helm 图表，我们需要将其添加到这个基础架构存储库中。让我们这样做，使 GitHub 存储库中的最终文件夹内容如下所示:
```
helmrelease1.yaml
myhelmchart/
  Chart.yaml
  Values.yaml
  Templates/
    … chart templates
```
现在，当 FluxCD 接下来检查 GitHub 上的基础设施存储库时，它将首先找到 Helm 发行版 YAML 文件，然后该文件将指向我们的新 Helm 图表。
FluxCD 有了新版本和 Helm 图表，然后将我们的 Helm 图表部署到 Kubernetes！
然后，每当对 Helm 发行版 YAML 或 Helm 图表中的任何文件进行更改时，FluxCD 都会将其取出，并在几分钟内(在其下一个循环中)部署更改。
此外，每当带有与过滤模式匹配的标签的新容器映像被推送到映像存储库时，就会自动部署新版本的应用——就这么简单。这意味着 FluxCD 正在监听两个位置——基础设施 GitHub 存储库和容器存储库，并将向其中任何一个位置部署任何更改。
您可以看到这是如何映射到我们的集群外配置项/光盘实现的，在该实现中，我们有一个代码管道来部署我们的应用容器的新版本，还有另一个代码管道来部署对我们的基础架构存储库的任何更改。FluxCD 以基于拉的方式做同样的事情。
# 总结
在本章中，我们学习了在 Kubernetes 上生成模板代码。我们回顾了如何使用 Helm 和 Kustomize 创建灵活的资源模板。有了这些知识，您将能够使用解决方案、创建或部署版本来模板化复杂的应用。然后，我们回顾了两种类型的 CI/CD；首先，通过 kubectl 将外部 CI/CD 部署到 Kubernetes，然后使用 FluxCD 实现集群内 CI 范例。有了这些工具和技术，您将能够为生产应用设置 CI/CD 到 Kubernetes。
在下一章中，我们将回顾 Kubernetes 上的安全性和合规性，这是当今软件环境中的一个重要主题。
# 问题
1.  Helm 和 Kustomize 模板化有什么区别？
2.  使用外部配置项/光盘设置时，应该如何处理 Kubernetes 应用编程接口凭据？
3.  为什么集群内配置项设置可能比集群外设置更可取，有哪些原因？反之亦然？
# 进一步阅读
*   kutomiza docs:https:[https://kubrines-sigs . github . io/kutomiza/](https://kubernetes-sigs.github.io/kustomize/)
*   helm docs[https://docs . flux CD . io/en/latest/教程/入门-helm/](https://docs.fluxcd.io/en/latest/tutorials/get-started-helm/)