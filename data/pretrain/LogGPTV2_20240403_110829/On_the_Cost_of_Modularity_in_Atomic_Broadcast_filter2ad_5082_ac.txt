The latency of both implementations is relatively close
for small offered loads. As soon as the offered load in-
creases, however, the monolithic implementation achieves
latencies that are between 30% (n = 7) and 50% (n = 3)
lower than the modular implementation. Note that the la-
tency of the two implementations remains relatively con-
6The current version of Fortika uses TCP connections rather than IP
multicast facilities.
group size=3; monolithic 2000 3000 4000 5000 6000 7000 0 5 10 15 20 25 30 35 40 45 0group size=3; modular   group size=7; monolithicgroup size=7; modular   message size = 16384 bytesoffered load (msgs/sec)early latency (msecs) 100037th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007Figure 9. Early latency vs. message size for
an offered load of 2000 msgs/s.
Figure 10. Throughput vs. offered load for ab-
cast messages of size 16384 bytes.
stant above a certain offered load. This is due to the ﬂow-
control mechanism that is present in both stacks: as the of-
fered load increases, more and more abcast messages are
blocked so that the network load remains more or less con-
stant.
Figure 9 shows how the early latency of the two imple-
mentations is affected by the size of the messages that are
abcast. The graph shows the early latency in a system with
n = 3 (two bottom curves) and n = 7 (two top curves)
processes. The offered load is ﬁxed to 2000 msgs/s. Re-
sults are similar with other values of offered load (except
with too small values where no signiﬁcant differences can
be observed).
Once again, the monolithic implementation achieves la-
tencies about 50% lower than the modular implementation
when the size of the messages is small (up to 4096 bytes
when n = 7 and 8192 bytes when n = 3). When the size
of the messages increases, the early latency also increases:
here, the total amount of data that needs to be exchanged
inﬂuences the latency, whereas previously the latency was
determined mostly by the number of messages sent on the
network (these messages all require a certain amount of pro-
cessing, independently of their small size). Finally, with the
largest messages considered, the monolithic implementa-
tion achieves a latency that is 25% (n = 7) or 35% (n = 3)
smaller than the modular implementation.
Throughput of Atomic Broadcast. We now examine
what throughput is reached by the modular and monolithic
implementations of atomic broadcast. Figure 10 shows the
relationship between the throughput of atomic broadcast
(on the vertical axis) and the offered load (on the horizon-
tal axis) when the size of the atomic broadcast messages
is ﬁxed at 16384 bytes. When the offered load is small
(less than 500 msgs/s), the throughput is equal to the of-
fered load. As the offered load increases, the ﬂow-control
mechanism limits the throughput that can be achieved (as in
the early latency above, the throughput reaches a plateau as
the offered load increases). Furthermore, for a high offered
Figure 11. Throughput vs. message size for
an offered load of 2000 msgs/s.
load, the monolithic implementation sustains a throughput
that is 25% (n = 7) to 30% (n = 3) higher than the mod-
ular implementation. For a low offered load, the difference
between both stacks is almost negligible.
Figure 11 presents the throughput of both implementa-
tions as a function of the size of the messages that are ab-
cast. The offered load is ﬁxed at 2000 msgs/s. When the
size of the messages is small, the monolithic implemen-
tation achieves between 10% and 15% higher throughputs
than the modular one (and the throughput remains constant
up to messages of size 4096 for n = 7 and 16384 for
n = 3). Surprisingly, the throughput is higher when n = 7
processes participate in the system than when n = 3. This
is once again due to the ﬂow-control mechanism: each pro-
cess is allowed to have a certain backlog (i.e. abcast mes-
sages that have not been delivered yet). Hence, when the
number of processes grows, a larger number of abcast mes-
sages that have not been adelivered are allowed to circulate.
Finally, as the message size increases, the throughput of
the system with n = 7 processes degrades faster than in
the case of n = 3. This is due to the consensus proposal
(which contains large messages) that needs to be sent to all
processes in the system. As both the message size of and the
number of processes increase, sending these large proposals
results in an overall lower throughput (in msgs/s).
early latency (msecs) 20 30 40 50 60 70 64 128 256 512 1024 2048 4096 8192 16384 32768 0offered load = 2000 msgs/secmessage size (bytes)group size=3; monolithicgroup size=3; modular    group size=7; monolithicgroup size=7; modular     10 400 2000 3000 4000 5000 6000 7000 600 800 1000 1200 0 1400offered load (msgs/sec)throughput (msgs/sec)group size=3; monolithicgroup size=3; modular    group size=7; monolithicgroup size=7; modular     0message size = 16384 bytes 200 1000group size=3; monolithic 0 200 400 600 800 1000 1200 1400 64 128 256 512 1024 2048 4096 8192 16384 32768throughput (msgs/sec)message size (bytes)group size=7; modular    group size=7; monolithicgroup size=3; modular    offered load = 2000 msgs/sec37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007Discussion. From the results above, we see that the differ-
ence in performance between a modular and a monolithic
implementation of the same distributed protocol is signiﬁ-
cant: the difference in latency is up to 50%, while the dif-
ference in throughput varies between 10% and 25%. This
is the cost that a user must expect to pay when choosing
between a modular system that is easier to maintain and up-
date and a monolithic system that has better performance
characteristics.
Furthermore, it is interesting to note that the experimen-
tal results do not always match the analysis in Section 5.2.
These two results are, however, complementary. As ex-
plained earlier, the analytical evaluation of the two imple-
mentations focuses solely on the messages exchanged by
the algorithm. Processing costs and resource contention,
for example, are not at all considered in the analysis. On the
other hand, in evaluating throughput and message latency,
experimental results are strongly inﬂuenced by such ele-
ments (but do not consider explicitly number of messages
exchanged). For instance, 99% of CPU resources were used
with an offered load bigger than 500 msgs/s. The discrep-
ancy between the analytical and experimental evaluations
of the stacks stem from these elements (that are difﬁcult to
estimate a priori).
6 Related Work
A number of group communication toolkits have im-
plemented atomic broadcast during the last two decades.
While early implementations (Isis [5, 6], Phoenix [17] and
Totem [2], among others) were designed with a mono-
lithic architecture, more recent systems (Horus [27], En-
semble [14], Transis [11], JavaGroups [3], Eden [15], and
Fortika[18, 19]7) present a modular design. A comparison,
from the architectural point of view, of most of these group
communication toolkits can be found in [20]. However,
the issue of performance overhead induced by modularity
(i.e. comparing performance of a modular and a monolithic
stack based on the same architecture) has not been covered
extensively. In Ensemble, the performance was improved
through several techniques [26, 14]: optimizing the inter-
facing code, improving the format of headers from different
modules and compressing them, extracting and inlining fre-
quently executed functions (from many modules), etc. Ap-
pia, a system inspired by Ensemble, included and furthered
these techniques [21]. While these techniques signiﬁcantly
improved the performance of these systems (e.g., in [14],
they reduce by approximately 20 the time of processing),
they are rather general lower-level solutions. Their aim is
not at the algorithmic level: the algorithms stayed the same
7Actually, Fortika provides both modular and monolithic implementa-
tions of atomic broadcast.
after the optimizations. On the other hand, our algorith-
mic improvements can not be applied to Ensemble or Ap-
pia, where atomic broadcast is not solved by reduction to
consensus, but rather relies on group membership in order
to avoid blocking. In [12], the authors propose to extend
the speciﬁcation of consensus. The new speciﬁcation al-
lows the consensus layer to share some state with the above
layers (e.g. atomic broadcast) in order to reduce the amount
of data sent over the network. This technique improves sig-
niﬁcantly performance (reduction by 4 of the message la-
tency). However, this result is not comparable to current re-
sult due to signiﬁcant differences in the system setup. Note
that the Eden group communication toolkit [15] proposes a
very similar technique.
In a more general context, there is more extensive work
on protocol layer optimization. For instance, the inﬂuen-
tial x-Kernel modular system was improved with the help
of various techniques like protocol multiplexing [23]. Stan-
dard compilation techniques can be combined with annota-
tions in the code to optimize the most frequently executed
functions [22]. This approach is somewhat similar to the
work done in Ensemble, but for more more basic stacks like
TCP/IP. Another technique to improve performance across
a protocol stack is Application Level Framing [9, 10]. The
intuition here is that all protocols should know the typical
size of application messages, so that they are not unneces-
sarily fragmented on their way down the stack. Again, in all
these techniques, protocols are treated as black boxes: the
optimizations did not involve any modiﬁcation in the pro-
tocol logic. Hence, most of these techniques can easily be
combined with the ones proposed in this paper.
Modularity is a necessary property to achieve good per-
formance in parallel computing and concurrent program-
ming [16]. However, this is not applicable to our work,
since very few tasks can be parallelized in atomic broad-
cast: only message diffusion and ordering can be executed
concurrently.
7 Conclusion
The paper presented two versions (monolithic and mod-
ular) of a fairly complex protocol: atomic broadcast. We
showed that a monolithic stack allows several algorithmic
optimizations. This is principally due to (1) the fact that
consensus instances are not considered independently, and
(2) the possibility for different modules to share their state.
We then analytically and experimentally quantiﬁed the gain
obtained thanks to these optimizations. Our analytical eval-
uation concluded that a monolithic implementation signif-
icantly reduces the number of messages sent over the net-
work. On the other hand, our experimental evaluation re-
vealed an overhead incurred by the modular version that
reaches 50% in the worst workload conditions.
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007In summary, if we are to implement atomic broadcast,
it is commonly agreed that a modular design is the most
sensible approach. In this paper, we have shown that we
cannot be so sure of this (apparently undisputed) choice, if
we care about our system’s performance.
Acknowledgments
We would like to thank the anonymous reviewers for
their comments and helpful suggestions.
References
[1] L. Alvisi and K. Marzullo. Waft: Support for fault-tolerance
In Proc. of the 2nd
in wide-area object oriented systems.
Information Survivability Workshop – ISW ’98, pages 5–10.
IEEE Computer Society Press, October 1998.
[2] Y. Amir, L. E. Moser, P. M. Melliar-Smith, D. A. Agar-
wal, and P. Ciarfella. The Totem single-ring ordering and
membership protocol. ACM Trans. on Computer Systems,
13(4):311–342, Nov. 1995.
[3] B. Ban. JavaGroups 2.0 User’s Guide, Nov 2002.
[4] N. T. Bhatti, M. A. Hiltunen, R. D. Schlichting, and W. Chiu.
Coyote: A system for constructing ﬁne-grain conﬁgurable
communication services. ACM Trans. on Computer Systems,
16(4):321–366, Nov. 1998.
[5] K. P. Birman. The process group approach to reliable dis-
tributed computing. Comm. ACM, 36(12):36–53, Dec. 1993.
[6] K. P. Birman and T. A. Joseph. Reliable communication
in presence of failures. ACM Trans. on Computer Systems,
5(1):47–76, Feb. 1987.
[7] T. D. Chandra and S. Toueg. Unreliable failure detectors for
reliable distributed systems. Journal of ACM, 43(2):225–
267, Mar. 1996.
[8] G. Chockler, I. Keidar, and R. Vitenberg. Group communi-
cation speciﬁcations: A comprehensive study. ACM Com-
puting Surveys, 33(4):427–469, May 2001.
[9] D. D. Clark and D. L. Tennenhouse. Architectural consid-
erations for a new generation of protocols. In SIGCOMM
’90: Proceedings of the ACM symposium on Communica-
tions architectures & protocols, pages 200–208, New York,
NY, USA, 1990. ACM Press.
[10] J. Crowcroft, J. Wakeman, Z. Wang, and D. Sirovica. Is Lay-
ering Harmful? IEEE Network 6(1992) 1 pp. 20-24. IEEE
Network 6(1992) 1 pp. 20-24, 1992.
[11] D. Dolev and D. Malkhi. The Transis approach to high avail-
ability cluster communication. Comm. ACM, 39(4):64–70,
Apr. 1996.
[12] R. Ekwall and A. Schiper. Solving atomic broadcast with
indirect consensus. In 2006 IEEE International Conference
on Dependable Systems and Networks (DSN 2006), 2006.
[13] V. Hadzilacos and S. Toueg. A modular approach to fault-
tolerant broadcasts and related problems. TR 94-1425, Dept.
of Computer Science, Cornell University, Ithaca, NY, USA,
May 1994.
[14] M. Hayden. The Ensemble system. Technical Report TR98-
1662, Dept. of Computer Science, Cornell University, Jan.
8, 1998.
[15] M. Hurﬁn, R. Macˆedo, M. Raynal, and F. Tronel. A gen-
eral framework to solve agreement problems. In Proceed-
ings of the 18th Symposium on Reliable Distributed Systems
(SRDS), pages 56–67, Lausanne, Switzerland, Oct. 1999.
[16] L. V. Kal´e. Performance and productivity in parallel pro-
gramming via processor virtualization. In Proc. of the First
Intl. Workshop on Productivity and Performance in High-
End Computing (at HPCA 10), Madrid, Spain, February
2004.
[17] C. P. Malloth.
Conception and Implementation of a
Toolkit for Building Fault-Tolerant Distributed Applications
in Large Scale Networks. PhD thesis, ´Ecole Polytechnique
F´ed´erale de Lausanne, Switzerland, Sept. 1996.
[18] S. Mena, X. Cuvellier, C. Gr´egoire, and A. Schiper. Appia
vs. cactus: Comparing protocol composition frameworks. In
Proc. of 22th IEEE Symposium on Reliable Distributed Sys-
tems (SRDS’03), Florence, Italy, Oct. 2003.
[19] S. Mena, O. R¨utti, and A. Schiper. Fortika: Robust Group
Communication. EPFL, Laboratoire de Syst`emes R´epartis,
may 2006.
[20] S. Mena, A. Schiper, and P. T. Wojciechowski. A step to-
wards a new generation of group communication systems. In
Proc. of Conference on Middleware, Rio de Janeiro, Brasil,
June 2003.
[21] H. Miranda, A. Pinto, and L. Rodrigues. Appia: A ﬂex-
ible protocol kernel supporting multiple coordinated chan-
In 21st Int’l Conf. on Distributed Computing Sys-
nels.
tems (ICDCS’ 01), pages 707–710, Washington - Brussels
- Tokyo, Apr.16–19 2001.
[22] D. Mosberger, L. L. Peterson, P. G. Bridges,
and
S. O’Malley. Analysis of techniques to improve protocol
processing latency. In SIGCOMM ’96: Conference proceed-
ings on Applications, technologies, architectures, and proto-
cols for computer communications, pages 73–84, New York,
NY, USA, 1996. ACM Press.
[23] L. Peterson, N. Hutchinson, S. O’Malley, and M. Abbott.
Rpc in the x-kernel: evaluating new design techniques. In
SOSP ’89: Proceedings of the twelfth ACM symposium on
Operating systems principles, pages 91–101, New York, NY,
USA, 1989. ACM Press.
[24] The University of Arizona, Computer Science Depart-
The Cactus Project. Available electronically at
ment.
http://www.cs.arizona.edu/cactus/.
[25] P. Urb´an. Evaluating the Performance of Distributed Agree-
ment Algorithms: Tools, Methodology and Case Studies.
´Ecole Polytechnique F´ed´erale de Lausanne,
PhD thesis,
Switzerland, Aug. 2003. Number 2824.
[26] R. van Renesse. Masking the overhead of protocol layer-
ing. In SIGCOMM ’96: Conference proceedings on Appli-
cations, technologies, architectures, and protocols for com-
puter communications, pages 96–104, New York, NY, USA,
1996. ACM Press.
[27] R. van Renesse, K. P. Birman, B. B. Glade, K. Guo, M. Hay-
den, T. Hickey, D. Malki, A. Vaysburd, and W. Vogels.
Horus: A ﬂexible group communications system. Techni-
cal Report TR95-1500, Dept. of Computer Science, Cornell
University, Ithaca, NY, USA, Apr. 1996.
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007