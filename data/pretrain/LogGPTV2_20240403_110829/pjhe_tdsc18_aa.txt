IEEE TRANSACTIONS ON DEPENDABLE AND SECURE COMPUTING, VOL. 15, NO. 6, NOVEMBER/DECEMBER 2018 931
Towards Automated Log Parsing for Large-Scale
Log Data Analysis
Pinjia He , Jieming Zhu  , Shilin He, Jian Li, and Michael R. Lyu, Fellow, IEEEAbstract—Logs are widely used in system management for dependability assurance because they are often the only data available that record detailed system runtime behaviors in production. Because the size of logs is constantly increasing, developers (and operators) intend to automate their analysis by applying data mining methods, therefore structured input data (e.g., matrices) are required. This triggers a number of studies on log parsing that aims to transform free-text log messages into structured events.However, due to the lack of open-source implementations of these log parsers and benchmarks for performance comparison, developers are unlikely to be aware of the effectiveness of existing log parsers and their limitations when applying them into practice. They must often reimplement or redesign one, which is time-consuming and redundant. In this paper, we first present a characterization study of the current state of the art log parsers and evaluate their efficacy on five real-world datasets with over ten million log messages. We determine that, although the overall accuracy of these parsers is high, they are not robust across all datasets. When logs grow to a large scale (e.g., 200 million log messages), which is common in practice, these parsers are not efficient enough to handle such data on a single computer. To address the above limitations, we design and implement a parallel log parser (namely POP) on top of Spark, a large-scale data processing platform. Comprehensive experiments have been conducted to evaluate POP on both synthetic and real-world datasets. The evaluation results demonstrate the capability of POP in terms of accuracy, efficiency, and effectiveness on subsequent log mining tasks.Index Terms—System management, log parsing, log analysis, parallel computing, clustering
Ç
1 	INTRODUCTION
L components of the IT industry, supporting daily use ARGE-SCALE distributed systems are becoming the coresoftware of various types, including online banking, e-com-merce, and instant messaging. In contrast to traditional standalone systems, most of such distributed systems run on a 24  7 basis to serve millions of users globally. Any non-trivial downtime of such systems can lead to significant revenue loss [1], [2], and this thus highlights the need to ensure system dependability.System logs are widely utilized by developers (and oper-ators) to ensure system dependability, because they are often the only data available that record detailed system runtime information in production environment. In general, logs are unstructured text generated by logging statements (e.g., printf(), Console.Writeline()) in system source code. Logs contain various forms system runtime information, which enables developers to monitor the runtime behaviors of their systems and to further assure system dependability.With the prevalence of data mining, the traditional method of log analysis, which largely relies on manual inspection and is labor-intensive and error-prone, has been complemented by automated log analysis techniques. Typi-cal examples of log analysis techniques on dependability assurance include anomaly detection [3], [4], [5], program verification [6], [7], problem diagnosis [8], [9], and security assurance [10], [11]. Most of these log analysis techniques comprise three steps: log parsing, matrix generation, and log mining (Fig. 1). The performance of log parsing plays an important role in various log analysis frameworks in terms of both accuracy and efficiency. The log mining step usually accepts structured data (e.g., a matrix) as input and reports mining results to developers. However, raw log messages are usually unstructured because they are natural language designed by developers. Typically, a raw log message, as illustrated in the following example, records a specific sys-tem event with a set of fields: timestamp, verbosity level, and raw message content.|  | P. He, S. He, J. Li, and M.R. Lyu are with Shenzhen Research Institute, | 2008-11-09 20:46:55,556 INFO dfs.DataNode$Packet- |
|---|---|---|
|  |The Chinese University of Hong Kong, Shenzhen, China; Department of |Responder: Received block blk_3587508140051953248 of |
|  |Computer Science and Engineering, The Chinese University of Hong |size 67108864 from /10.251.42.84 ||  |Kong, Shatin, Hong Kong. They are also with MoE Key Laboratory of |size 67108864 from /10.251.42.84 |
|  |Kong, Shatin, Hong Kong. They are also with MoE Key Laboratory of |Log parsing is usually the first step of automated log |
|  |High Confidence Software Technologies (CUHK Sub-Lab). |Log parsing is usually the first step of automated log |
|  |E-mail: {. |Log parsing is usually the first step of automated log ||  |E-mail: {. |analysis, whereby raw log messages can be transformed |
|  |J. Zhu is with Huawei 2012 Labs, Huawei, Shenzhen 518129, China. |analysis, whereby raw log messages can be transformed |
|  |E-mail: . |into a sequence of structured events. A raw log message, as |
Manuscript received 15 Nov. 2016; revised 21 June 2017; accepted 2 Oct.
2017. Date of publication 13 Oct. 2017; date of current version 9 Nov. 2018.(Corresponding author: Jieming Zhu.) 
For information on obtaining reprints of this article, please send e-mail to: PI:EMAIL, and reference the Digital Object Identifier below.
Digital Object Identifier no. 10.1109/TDSC.2017.2762673
illustrated in the example, consists of two parts, namely a constant part and a variable part. The constant part consti-tutes the fixed plain text and represents the corresponding event type, which remains the same for every event occur-rence. The variable part records the runtime information,1545-5971  2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See ht_tp://www.ieee.org/publications_standards/publications/rights/index.html for more information.
932 IEEE TRANSACTIONS ON DEPENDABLE AND SECURE COMPUTING, VOL. 15, NO. 6, NOVEMBER/DECEMBER 2018
Fig. 1. Overview of log analysis.such as the values of states and parameters (e.g., the block ID: blk_3587508140051953248), which may vary among dif-ferent event occurrences. The goal of log parsing is to auto-matically separate the constant part and variable part of a raw log message (also known as log de-parametrization), and to further match each log message with a specific event type (usually denoted by its constant part). In the example, the event type can be denoted as “Received block * of size * from *”, where the variable part is identified and maskedaccuracy (i.e., F-measure [19], [20]), efficiency (i.e., execution time), and effectiveness on a log mining task (i.e., anomaly detection [3] evaluated by detected anomaly and false alarm). We determine that, although the overall accuracy of these log parsing methods is high, they are not robust across all datasets. When logs grow to a large scale (e.g., 200 mil-lion log messages), these parsers fail to complete in reason-able time (e.g., one hour), and most cannot handle such data on a single computer. We also find that parameter tuningusing asterisks. 	costs considerable time for these methods, because parame-Traditional log parsing approaches rely heavily on man-ually customized regular expressions to extract the specific log events (e.g., SEC [12]). However, this method becomes inefficient and error-prone for modern systems for the fol-lowing reasons. First, the volume of log grows rapidly; for example, it grows at a rate of approximately 50 GB/h (120200 million lines) [13]. Manually constructing regular expressions from such a large number of logs is prohibitive. Furthermore, modern systems often integrate open-source software components written by hundreds of developers [3]. Thus, the developers who maintain the systems are usu-ally unaware of the original logging purpose, which increases the difficulty of the manual method. This problem is compounded by the fact that the log printing statements in modern systems update frequently [14] (e.g., hundreds of new logging statements every month [15]); consequently, developers must regularly review the updated log printing statements of various system components for the mainte-nance of regular expressions.Recent studies have proposed a number of automated log parsing methods, including SLCT [16], IPLoM [17], LKE [4], LogSig [18]. Despite the importance of log parsing, we find a lack of systematic evaluations on the accuracy and efficiency of the automated log parsing methods available. The effectiveness of the methods on subsequent log mining tasks is also unclear. Additionally, there are no other ready-to-use tool implementations of these log parsers (except forters tuned on a sample dataset of small size cannot be directly employed on a large dataset.To address these problems, we propose a parallel log parsing method, called POP, that can accurately and effi-ciently parse large-scale log data. Similar to previous papers [4], [16], [17], [18], POP assumes the input is single-line logs, which is the common case in practice. To improve accuracy in log parsing, we employ iterative partitioning rules for candidate event generation and hierarchical clustering for event type refinement. To improve efficiency in processing large-scale data, we design POP with linear time complexity in terms of log size, and we further parallelize its computa-tion on top of Spark, a large-scale data processing platform.We evaluate POP on both real-world datasets and large-scale synthetic datasets with 200 million lines of raw log messages. The evaluation results show the capability of POP in achieving accuracy and efficiency. Specifically, POP can parse all the real-world datasets with the highest accu-racy compared with the existing methods. Moreover, POP can parse our synthetic HDFS (Hadoop Distributed File Sys-tem) dataset in 7 min, whereas SLCT requires 30 min, and IPLoM, LKE, and LogSig fail to terminate in reasonable time. Moreover, parameter tuning is easy in POP because the parameters tuned on small sample datasets can be directly applied to large datasets while preserving high parsing accuracy.| SLCT [16], which was released more than 10 years ago). In |  | This is the first work that systematically evaluates |
|---|---|---|
| this context, practitioners and researchers must implement | |the performance of current log parsers in terms of |
| log parsers by themselves when performing log mining | |accuracy, efficiency, and effectiveness on subsequent || tasks (e.g., [6], [9]), which is a time-consuming and redun- | |log mining tasks. |
| dant effort. | |It presents the design and implementation of a paral- |
| To fill the significant gap, in this paper, we conduct a | |lel log parsing method (POP), which can parse large- |
| comprehensive evaluation of four representative log parsers | |scale log data accurately and efficiently. || and then present a parallel log parser that achieves state-of- | |The source code of both POP and the studied log |
| the-art performance in accuracy and efficiency. | |parsers have been publicly released [21], allowing |
| More specifically, we study SLCT [16], IPLoM [17], LKE | |for easy use by practitioners and researchers for |
| [4], and LogSig [18], which are widely used log parsers in | |future study. |log analysis. We do not consider source code-based log parsing [3], because, in many cases, the source code is inac-cessible (e.g., in third party libraries). By using five real-world log datasets with over 10 million raw log messages, we evaluate the log parsers’ performance in terms ofExtended from its preliminary conference version [22], the paper makes several major enhancements: the design and implementation of a parallel log parsing method; the evaluation of POP’s performance in terms of accuracy, effi-ciency, and effectiveness on subsequent log mining tasks;
HE ET AL.: TOWARDS AUTOMATED LOG PARSING FOR LARGE-SCALE LOG DATA ANALYSIS 	933
Fig. 2. Overview of log parsing.Fig. 2. Overview of log parsing.
efficiency evaluation of the state-of-the-art parsers on large-scale synthetic datasets; discussion highlighting the usage of POP in practice; and code release of POP for reproducible research.
2 	OVERVIEW OF LOG PARSINGThe goal of log parsing is to transform raw log messages into a sequence of structured events, which facilitates sub-sequent matrix generation and log mining. Fig. 2 illus-trates an overview of log parsing process. As shown in the figure, there are ten HDFS raw log messages collected on the Amazon EC2 platform [3]. In real-world cases, a sys-tem can generate millions of such log messages per hour. The output of log parsing is a list of log events and struc-tured logs.Log events are the extracted templates of log messages, for example, “Event 2: Receiving block * src: * dest: *”. In prac-tice, typically we employ POP on historical logs to generate log events, which can be used to parse the logs in system runtime. Structure logs are a sequence of events with field of interest (e.g., timestamp). The structure logs can be easily transformed to a matrix, or directly processed by the subse-quent log mining tasks (e.g., anomaly detection [3], deploy-ment verification [7]).Log parsing has been widely studied in recent years. Among all the log parsers, we choose four representative ones (SLCT [16], IPLoM [23], LKE [4], LogSig [18]), which are in widespread use for log mining tasks. Details of these parsers are provided in our supplementary report [24].
Fig. 3. Proxifier log samples.
method needs to be consistently accurate and efficient on logs from different systems.Thus, we design a parallel log parsing method, namely POP, to fulfill the above requirements. POP preprocesses logs with simple domain knowledge (step 1). It then hierar-chically partitions the logs into different groups based on two heuristic rules (steps 2 and 3). For each group, the con-stant parts are extracted to construct the log event (step 4). Finally, POP merges similar groups according to the result of hierarchical clustering on log events (step 5). We design POP on top of Spark [25], [26], a large-scale data processing platform using the parallelization power of computer clusters, and all computation-intensive parts of POP are designed to be highly parallelizable.3.1 	Step 1: Preprocess by Domain KnowledgeAccording to our study on the existing log parsers, simple preprocessing using domain knowledge can improve parsing accuracy, so raw logs are preprocessed in this step. POP pro-vides two preprocessing functions. First, POP prunes variable parts according to simple regular expression rules provided by developers, for example, removing block ID in Fig. 2 by“blk_[0-9]+”. For all datasets used in our experiment, at most two rules are defined on a dataset. This function can delete variable parts that can be easily identified with domain knowledge. Second, POP allows developers to manually specify log events based on regular expression rules. This is useful because developers intend to put logs with certain properties into the same partition in some cases. For example, Fig. 3 contains two log messages from Proxifier dataset. The two logs will be put into the same partition by most of the log parsing methods. However, developers may want to count the session with less than 1 second lifetime separately. In this case, POP can easily extract the corresponding logs based on the regular expression “.*<1 sec.*”. Note that the simple regu-lar expressions used in this step require much less human effort than those complex ones used by traditional methods to match the whole log messages.3.2 	Step 2: Partition by Log Message Length
In this step, POP partitions the remaining logs into nonover-lapping groups of logs. POP puts logs with the same log message length into the same group. By log message length, we mean the number of tokens in a log message. This heu-ristic, which is also used by IPLoM [23], is based on the assumption that logs with the same log event will likely have the same log message length. For example, log event“Verification succeeded for *” from HDFS dataset contains| 3 | PARALLEL LOG PARSING (POP) | 4 tokens. It is intuitive that logs having this log event share |
|---|---|---|
| 3 |PARALLEL LOG PARSING (POP) |the same log message length, such as “Verification suc- |
From the implementation and systematic study of log pars-ers introduced in Section 2, we observe that a good log pars-ing method should fulfill the following requirements: (1) Accuracy. The parsing accuracy (i.e., F-measure) should be high. (2) Efficiency. The running time of a log parser should be as short as possible. (3) Robustness. A log parsingceeded for blk_1” and “Verification succeeded for blk_2”. This heuristic rule is considered coarse-grained, so it is pos-sible that log messages in the same group have different log events. “Serve block * to *” and “Deleting block * file *” will be put into the same group in step 2, because they both con-tain 5 tokens. This issue is addressed by a fine-grained934 IEEE TRANSACTIONS ON DEPENDABLE AND SECURE COMPUTING, VOL. 15, NO. 6, NOVEMBER/DECEMBER 2018
Fig. 4. An Example of AT, RT calculation.
heuristic partition rule described in step 3. Besides, it is pos-sible that one or more variable parts in the log event contain variable length, which invalidates the assumption of step 2.
This will be addressed by hierarchical clustering in step 5.regards the tokens as variables only when both AT and RT are larger than manually defined thresholds (i.e., splitAbs and splitRel respectively). If all tokens in the split token position are variables, POP moves the current group to the complete group list, because it could not be further parti-tioned (lines 1315). Otherwise, POP partitions the current group into jTSsj resulting groups based on the token value in the split token position (line 17). Among all the result groups, the complete groups are added into the complete group list, while the incomplete ones are added to the incomplete group list for further partitioning (lines 1822). Finally, the complete group list is returned, where logs in3.3 Step 3: Recursively Partition by Token Position each group share the same log event type.
In step 3, each group is recursively partitioned into sub-groups, where each subgroup contains logs with the same log event (i.e., same constant parts). This step assumes that if the logs in a group having the same log event, the tokens in some token positions should be the same. For example, if all the logs in a group have log event “Open file *”, then the tokens in the first token position of all logs should be“Open”. We define complete token position to guide the parti-tioning process.Notations: Given a group containing logs with log mes-sage length n, there are n token positions. All tokens in token position i form a token set TSi, which is a collection of distinct tokens. The cardinality of TSi is defined as jTSij. A token position is complete if and only if jTSij ¼ 1, and it is defined as a complete token position. Otherwise, it is definedAlgorithm 1. POP Step 3: Recursively Partition Each Group to Complete Groups.
Input: a list of log groups from step 2: logGroupL; and 
algorithm parameters: GS; splitRel; splitAbs 
	Output: a list of complete groups: completeL 
1: incompleteLlogGroupL 
2: completeLListðÞ 	" Initialize with empty list 3: curGroupfirst group in incompleteL 