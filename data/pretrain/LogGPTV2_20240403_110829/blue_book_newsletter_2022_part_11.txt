    | WD Red Plus          | 8TB  | 265$  |
    | Seagate Exos 7E8     | 8TB  | 277$  |
    | WD Red Pro           | 8TB  | 278$  |
    WD Red Plus has 5,640 RPM which is different than the rest, so it's ruled out.
    Between the IronWolf and IronWolf Pro, they offer 180MB/s and 214MB/s
    respectively. The Seagate Exos 7E8 provides much better performance than the WD
    Red Pro so I'm afraid that WD is out of the question.
    There are three possibilities in order to have two different brands. Imagining
    we want 4 disks:
    | Combination             | Total Price        |
    | ---                     | ---                |
    | IronWolf + IronWolf Pro | 958$               |
    | IronWolf + Exos 7E8     | 1004$ (+46$ +4.5%) |
    | IronWolf Pro + Exos 7E8 | 1062$ (+54$ +5.4%) |
    In terms of:
    * Consumption: both IronWolfs are equal, the Exos uses 2.7W more on
        normal use and uses 0.2W less on rest.
    * Warranty: IronWolf has only 3 years, the others 5.
    * Speed: Ironwolf has 210MB/s, much less than the Pro (255MB/s) and Exos
        (249MB/s), which are more similar.
    * Sostenibility: The Exos disks are much more robust (more workload, MTBF and
        Warranty).
    I'd say that for 104$ it makes sense to go with the IronWolf Pro + Exos 7E8
    combination.
* New: [Choosing the disks for the cache.](zfs.md#choosing-the-disks-for-the-cache)
    Using a ZLOG greatly improves the [writing speed](https://www.servethehome.com/exploring-best-zfs-zil-slog-ssd-intel-optane-nand/), equally using an SSD disk for the L2ARC cache improves the read speeds and improves the health of the rotational disks.
    The best M.2 NVMe SSD for NAS caching are the ones that have enough capacity to
    actually make a difference to overall system performance. It also requires
    a good endurance rating for better reliability and longer lifespan, and you
    should look for a drive with a specific NAND technology if possible.
    I've made an analysis based on:
    * [Cache disk NAND technology](zfs.md#cache-disk-nand-technology)
    * [DWPD](zfs.md#dwpd)
    As conclusion, I’d recommend the Western Digital Red SN700, which has a good 1 DWPD
    endurance rating, is available in sizes up to 4TB, and is using SLC NAND
    technology, which is great for enhancing reliability through heavy caching
    workloads. A close second place goes to the Seagate IronWolf 525, which has
    similar specifications to the SN700 but utilizes TLC.
    | Disk            | Size   | Speed    | Endurance | Warranty | Tech | Price |
    | ---             | ---    | ---      | ---       | ---      | ---  | ---   |
    | WD Red SN700    | 500 GB | 3430MB/s | 1 DWPD    | 5 years  | SLC  | 73$   |
    | SG IronWolf 525 | 500 GB | 5000MB/s | 0.8 DWPD  | 5 years  | TLC  | ?     |
    | WD Red SN700    | 1 TB   | 3430MB/s | 1 DWPD    | 5 years  | SLC  | 127$  |
    | SG IronWolf 525 | 1 TB   | 5000MB/s | 0.8 DWPD  | 5 years  | TLC  | ?     |
* New: [Choosing the cold spare disks.](zfs.md#choosing-the-cold-spare-disks)
    It's good to think how much time you want to have your raids to be inconsistent
    once a drive has failed.
    In my case, for the data I want to restore the raid as soon as I can, therefore
    I'll buy another rotational disk. For the SSDs I have more confidence that they
    won't break so I don't feel like having a spare one.
## Monitoring
### [Prometheus](prometheus.md)
* New: [Accessing Prometheus metrics through python.](prometheus.md#accessing-prometheus-metrics-through-python)
    ```python
    import requests
    response = requests.get(
        "http://127.0.0.1:9090/api/v1/query",
        params={"query": "container_cpu_user_seconds_total"},
    )
    ```
### [AlertManager](alertmanager.md)
* New: [Use regular expressions in silences.](alertmanager.md#silences)
    To silence an alert with a regular expression use the matcher
    `alertname=~".*Condition"`.
## Hardware
### [CPU](cpu.md)
* New: Introduce CPU, attributes and how to buy it.
    [A central processing unit or CPU](https://en.wikipedia.org/wiki/Central_processing_unit), also known as the brain of the server, is the electronic circuitry that executes instructions comprising a computer program. The CPU performs basic arithmetic, logic, controlling, and input/output (I/O) operations specified by the instructions in the program.
* New: [Add the market analysis.](cpu.md#market-analysis)
* New: [Analyze the cpu coolers.](cpu.md#cpu-coolers)
* New: [Analyze the use of cpu thermal paste.](cpu.md#cpu-thermal-paste)
    Thermal paste is designed to minimize microscopic air gaps and irregularities
    between the surface of the cooler and the CPU's IHS (integrated heat spreader),
    the piece of metal which is built into the top of the processor.
    Good thermal paste can have a profound impact on your performance, because it
    will allow your processor to transfer more of its waste heat to your cooler,
    keeping your processor running cool.
    Most pastes are comprised of ceramic or metallic materials suspended within
    a proprietary binder which allows for easy application and spread as well as
    simple cleanup.
    These thermal pastes can be electrically conductive or non-conductive, depending
    on their specific formula. Electrically conductive thermal pastes can carry
    current between two points, meaning that if the paste squeezes out onto other
    components, it can cause damage to motherboards and CPUs when you switch on the
    power. A single drop out of place can lead to a dead PC, so extra care is
    imperative.
    Liquid metal compounds are almost always electrically conductive, so while these
    compounds provide better performance than their paste counterparts, they require
    more focus and attention during application. They are very hard to remove if you
    get some in the wrong place, which would fry your system.
    In contrast, traditional thermal paste compounds are relatively simple for every
    experience level. Most, but not all, traditional pastes are electrically
    non-conductive.
    Most cpu coolers come with their own thermal paste, so check yours before buying
    another one.
* Correction: Add GPU advice on shopping tips.
    * Check that the CPU has GPU if you don't want to use an external graphic card.
        Otherwise the BIOS won't start.
* New: [Installation tips for CPU.](cpu.md#installation)
    When installing an AM4 CPU in the motherboard, rotate the CPU so that the small
    arrow on one of the corners of the chip matches the arrow on the corner of the
    motherboard socket.
### [RAM](ram.md)
* New: Introduce RAM, it's properties and how to buy it.
    [RAM](https://en.wikipedia.org/wiki/Random-access_memory) is a form of computer
    memory that can be read and changed in any order, typically used to store
    working data and machine code.
### [Power Supply Unit](psu.md)
* New: Introduce Power Supply Unit.
    [Power supply unit](https://linuxhint.com/pc-power-supply-unit/) is the component
    of the computer that sources power from the primary source (the power coming
    from your wall outlet) and delivers it to its motherboard and all its
    components. Contrary to the common understanding, the PSU does not supply power
    to the computer; it instead converts the AC (Alternating Current) power from the
    source to the DC (Direct Current) power that the computer needs.
    There are two types of PSU: Linear and Switch-mode. Linear power supplies have
    a built-in transformer that steps down the voltage from the main to a usable one
    for the individual parts of the computer. The transformer makes the Linear PSU
    bulky, heavy, and expensive. Modern computers have switched to the switch-mode
    power supply, using switches instead of a transformer for voltage regulation.
    They’re also more practical and economical to use because they’re smaller,
    lighter, and cheaper than linear power supplies.
    PSU need to deliver at least the amount of power that each component requires,
    if it needs to deliver more, it simply won't work.
    Another puzzling question for most consumers is, “Does a PSU supply constant
    wattage to the computer?” The answer is a flat No. The wattage you see on the
    PSUs casing or labels only indicates the maximum power it can supply to the
    system, theoretically. For example, by theory, a 500W PSU can supply a maximum
    of 500W to the computer. In reality, the PSU will draw a small portion of the
    power for itself and distributes power to each of the PC components according to
    its need. The amount of power the components need varies from 3.3V to 12V. If
    the total power of the components needs to add up to 250W, it would only use
    250W of the 500W, giving you an overhead for additional components or future
    upgrades.
    Additionally, the amount of power the PSU supplies varies during peak periods
    and idle times. When the components are pushed to their limits, say when a video
    editor maximizes the GPU for graphics-intensive tasks, it would require more
    power than when the computer is used for simple tasks like web-browsing. The
    amount of power drawn from the PSU would depend on two things; the amount of
    power each component requires and the tasks that each component performs.
    I've also added the next sections:
    * [Power supply efficiency](psu.md#power-supply-efficiency)
    * [Power supply shopping tips](psu.md#power-supply-shopping-tips)
    * [Market analysis](psu.md#market-analysis)
### [Pedal PC](pedal_pc.md)
* New: Introduce Pedal PC.
    The Pedal PC idea gathers crazy projects that try to use the energy of your
    pedaling while you are working on your PC. The most interesting is
    [PedalPC](https://www.pedalpc.com/), but still crazy.
    [Pedal-Power](http://pedal-power.com/) is another similar project, although it
    looks unmaintained.
# Operating Systems
## Linux
### [Linux Snippets](linux_snippets.md)
* New: [Clean up system space on debian based hosts.](linux_snippets.md#clean-up-system-space)
* New: [Install one package from Debian unstable.](linux_snippets.md#install-one-package-from-debian-unstable)
* New: [Monitor outgoing traffic.](linux_snippets.md#monitor-outgoing-traffic)
* Correction: [Clean snap data.](linux_snippets.md#clean-snap-data)
    If you're using `snap` you can clean space by:
    * Reduce the number of versions kept of a package with `snap set system
        refresh.retain=2`
    * Remove the old versions with `clean_snap.sh`
        ```bash
        #!/bin/bash
        #Removes old revisions of snaps
        #CLOSE ALL SNAPS BEFORE RUNNING THIS
        set -eu
        LANG=en_US.UTF-8 snap list --all | awk '/disabled/{print $1, $3}' |
            while read snapname revision; do
                snap remove "$snapname" --revision="$revision"
            done)
        ```
* Correction: [Clean journalctl data.](linux_snippets.md#clean-journalctl-data)
    * Check how much space it's using: `journalctl --disk-usage`
    * Rotate the logs: `journalctl --rotate`
    Then you have three ways to reduce the data:
    1. Clear journal log older than X days: `journalctl --vacuum-time=2d`
    1. Restrict logs to a certain size: `journalctl --vacuum-size=100M`
    1. Restrict number of log files: `journactl --vacuum-files=5`.
    The operations above will affect the logs you have right now, but it won't solve
    the problem in the future. To let `journalctl` know the space you want to use
    open the `/etc/systemd/journald.conf` file and set the `SystemMaxUse` to the
    amount you want (for example `1000M` for a gigabyte). Once edited restart the
    service with `sudo systemctl restart systemd-journald`.
* New: [Set up docker logs rotation.](linux_snippets.md#set-up-docker-logs-rotation)
    By default, the stdout and stderr of the container are written in a JSON file
    located in `/var/lib/docker/containers/[container-id]/[container-id]-json.log`. If
    you leave it unattended, it can take up a large amount of disk space.
    If this JSON log file takes up a significant amount of the disk, we can purge it
    using the next command.
    ```bash
    truncate -s 0 
    ```
    We could setup a cronjob to purge these JSON log files regularly. But for the
    long term, it would be better to setup log rotation. This can be done by adding
    the following values in `/etc/docker/daemon.json`.
    ```json
    {
      "log-driver": "json-file",
      "log-opts": {
        "max-size": "10m",
        "max-file": "10"
      }
    }
    ```
* New: [Clean old kernels.](linux_snippets.md#clean-old-kernels)
    The full command is
    ```bash
    dpkg -l linux-* | awk '/^ii/{ print $2}' | grep -v -e `uname -r | cut -f1,2 -d"-"` | grep -e [0-9] | grep -E "(image|headers)" | xargs sudo apt-get -y purge
    ```
    To test what packages will it remove use:
    ```bash
    dpkg -l linux-* | awk '/^ii/{ print $2}' | grep -v -e `uname -r | cut -f1,2 -d"-"` | grep -e [0-9] | grep -E "(image|headers)" | xargs sudo apt-get --dry-run remove
    ```
    Remember that your running kernel can be obtained by `uname -r`.
* Correction: [Clean old kernels warning.](linux_snippets.md#clean-old-kernels)
    I don't recommend using this step, rely on `apt-get autoremove`, it's safer.
* New: [Create Basic Auth header.](linux_snippets.md#create-basic-auth-header)
    ```bash
    $ echo -n user:password | base64
    dXNlcjpwYXNzd29yZA==
    ```
    Without the `-n` it won't work well.
* New: [Check vulnerabilities in Node.js applications.](linux_snippets.md#check-vulnerabilities-in-node.js-applications)
    With `yarn audit` you'll see the vulnerabilities, with `yarn outdated` you can
    see the packages that you need to update.
* New: [Check vulnerabilities in rails dependencies.](linux_snippets.md#check-vulnerabilities-in-rails-dependencies)
    ```bash
    gem install bundler-audit
    cd project_with_gem_lock
    bundler-audit
    ```
* New: [Trim silences of sound files.](linux_snippets.md#trim-silences-of-sound-files)
    To trim all silence longer than 2 seconds down to only 2 seconds long.
    ```bash
    sox in.wav out6.wav silence -l 1 0.1 1% -1 2.0 1%
    ```
    Note that SoX does nothing to bits of silence shorter than 2 seconds.
    If you encounter the `sox FAIL formats: no handler for file extension 'mp3'`
    error  you'll need to install the `libsox-fmt-all` package.
* New: [Adjust the replay gain of many sound files.](linux_snippets.md#adjust-the-replay-gain-of-many-sound-files)
    ```bash
    sudo apt-get install python-rgain
    replaygain -f *.mp3
    ```
* New: [Create QR code.](linux_snippets.md#create-qr-code)
    ```bash
    qrencode -o qrcode.png 'Hello World!'
    ```
* New: [Git checkout to main with master as a fallback.](linux_snippets.md#git-checkout-to-main-with-master-as-a-fallback)
    I usually use the alias `gcm` to change to the main branch of the repository,
    given the change from [main to master](git.md#renaming-from-master-to-main) now
    I have some repos that use one or the other, but I still want `gcm` to go to the
    correct one. The solution is to use:
    ```bash
    alias gcm='git checkout "$(git symbolic-ref refs/remotes/origin/HEAD | cut -d'/' -f4)"'
    ```
* New: [Scan a physical page in Linux.](linux_snippets.md#scan-a-physical-page-in-linux)
    Install `xsane` and run it.
* New: [Get the output of `docker ps` as a json.](linux_snippets.md#get-the-output-of-`docker-ps`-as-a-json)
    To get the complete json for reference.
    ```bash
    docker ps -a --format "{{json .}}" | jq -s
    ```
    To get only the required columns in the output with tab separated version
    ```bash
    docker ps -a --format "{{json .}}" | jq -r -c '[.ID, .State, .Names, .Image]'
    ```
    To get [also the image's ID](https://stackoverflow.com/questions/54075456/docker-ps-show-image-id-instead-of-name) you can use:
    ```bash
    docker inspect --format='{{json .}}' $(docker ps -aq) | jq -r -c '[.Id, .Name, .Config.Image, .Image]'
    ```
* New: [Df and du showing different results.](linux_snippets.md#df-and-du-showing-different-results)
    Sometimes on a linux machine you will notice that both `df` command (display
    free disk space) and `du` command (display disk usage statistics) report
    different output. Usually, `df` will output a bigger disk usage than `du`.
    The `du` command estimates file space usage, and the `df` command shows file
    system disk space usage.
    There are many reasons why this could be happening:
    * [Disk mounted over data](linux_snippets.md#disk-mounted-over-data)
    * [Used deleted files](linux_snippets.md#used-deleted-files)
* New: [Clean up docker data.](linux_snippets.md#clean-up-docker-data)
    To remove unused `docker` data you can run `docker system prune -a`. This will
    remove: