which passes all the basic health checks, but is unable to complete client requests according SLOs for 5% of requests, 
is far more difficult.
+ Masking. Complex distributed systems that use load balancing components to increase reliability and scalability
make the detection of anomaly more difficult since their object is to mask problems from end users. 
Thus, if an AIOps system is looking at a distributed system from the user perspective, it may not be able to easily
identify health problems. 
### Automated Operations
The most common use of operations is to solve problems. Operations (actions) can be automatically triggered 
in response to troubleshooting workflows.
For example, once methods for pattern recognition and inference are mastered, the next step is to look into 
auto-remediation. As knowledge on failure modes is gained, failure patterns are identified and recovery is 
encoded into automated remediation scripts. Often, only simple failure cases can be handled but this constitute 
a very good starting point for more complex scenarios.
Examples include rebooting a host, restarting a microservice or hung process, free disk space, and remove 
cached data. As knowledge on running systems accumulates, auto-remediation becomes pervasive to service owners 
which can define their own recovery actions.
Other types of operations relevant to AIOps include: 
1. Capacity planing
2. Canarying validation
3. Service scaling
4. Performance analysis
5. Intelligent troubleticket routing
6. Predictive maintenance
### Evaluation
We evaluate the techniques and algorithms we built using a 3-level approach:
+ *Synthetics data*. We built models simulating microservice applications which are able to generate data under 
very specific conditions. The scenarios simulated are usually difficult to obtain when using testbeds and 
production systems. The controlled data enables a fine-grained understanding of how new algorithms behave and are an 
effective way for improvement and redesign. Nonetheless, the type of traffic that is generated in production is
typically not captured by synthetic data.
+ *Testbed data*. Once an algorithm passes the evaluation using synthetic data, we make a second evaluation using 
testbed data. We run an OpenSack cloud platform under normal utilization. Faults are injected into the platform and
we expect algorithms to detect anomalies, find their root cause, predict errors, and remediate failures.
Service calls from normal production can be used to trigger the calls of the testbed.
+ *Production data*. In the last step of the evaluation, we deploy algorithms in planet-scale production systems. 
This is the final evaluation in an environment with noise and which generally makes algorithms generate many 
false positives. Accuracy, performance and resources consumption is registered.
Many public datasets are also available to conduct comparative studies:
+ Anomaly detection datasets: [Harvard](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/OPQMVF),
[Oregon State](https://ir.library.oregonstate.edu/concern/datasets/47429f155),
[Numenta](https://github.com/numenta/NAB)
+ Outliers datasets: [Stonybrook](http://odds.cs.stonybrook.edu/), 
[LMU](http://www.dbs.ifi.lmu.de/research/outlier-evaluation/),
[ELKI](https://elki-project.github.io/datasets/outlier)
+ Cluster datasets: [Alibaba clusterdata](https://github.com/alibaba/clusterdata),
[Google Cluster Data](https://github.com/google/cluster-data)
+ [Yahoo webscope](https://webscope.sandbox.yahoo.com/catalog.php?datatype=s&did=70&guccounter=1) 
+ [Azure Public Dataset](https://github.com/Azure/AzurePublicDataset)
+ [LogPai datasets](https://github.com/logpai/loghub/blob/master/README.md)
+ [Timeseries classification](http://timeseriesclassification.com/dataset.php?train=&test=&leng=&class=&type=='sensor')
#### Challenges
+ Obtain support to make datasets available so that researchers can develop new approaches
+ Generate positive and negative samples automatically
+ Sparse training samples
+ Hard to obtain data from clients environments
## Applications and Use Cases
### Systems and Subsystems analysis
We are particularly interested in exploring how complex IT systems can be decomposed into subsystems to be analyzed.
While algorithms and techniques to analyze metrics and time series generated by single targets (e.g., CPU and I/O) are fundamental and constitute building blocks for anomaly detection and root-cause analysis, our previous research as shown that using single metrics tend to generate too many false positives for non-linear and noisy systems.
Thus, instead, we seek to analyze subsystems (e.g., HAProxy or OBSâ€™s microservices) as a whole by developing machine learning algorithms which consider many metrics and other monitoring data sources such as:
CPU load, response time, closed connections, 5xx status codes, requests/sec, selects/sec, request time, and service time.
### Anomaly Detection
For large-scale, dynamic IT infrastructures, detecting problems is a complex task since its behavior becomes 
too complex to understand.
Without special tools, operators need to manually analyze different monitoring, configuration, trace, log, and 
other data. 
Techniques for anomaly detection allow to automatically discover potential problems by analyzing data generated by 
the monitoring infrastructure and raising alarms when unusual conditions are meet. 
This is especially important for planet-scale systems, where it is desirable to have teams which can scale while 
maintaining quality of service and SLOs.
Anomaly detection approaches are classified into two types:
1. *Reactive*. Algorithms detect anomalies after they happen.
2. *Proactive*. Solutions predict upcoming anomalies when a system is a normal state.
Techniques can be classified:
+ Model-based approaches describe system performance or system states.
Problems are recognized by deviations from models capturing a normal behavior. 
+ Correlation-based approaches learn normal behavior by analyzing the correlations which exist between components' 
metrics over time.
+ Statistical-based approaches analyze the normal distribution of data points and monitor distribution patterns 
at runtime.
Techniques:
+ Metrics. Time series analysis for multimodal and univariate/multivariate data.
+ *Logs*. 
Classifiers can be trained to detect anomalies in application logs. 
Since records are often not labelled, the challenge is to build predictive model trained with the *normal sequences* 
of log records which reflect a normal execution or behavior of a distributed system. 
The model can be used to detect anomalies when the sequence of records significantly differ from the learned sequences.
One of the first works in this field can be traced back to 2009.
[Xu et al.](https://dl.acm.org/citation.cfm?id=1629587) proposed to parse logs and analyse source code using 
information retrieval approaches to create features which are analyzed using machine learning to detect problems.   
More recent approaches, for example, [Zhang et al.](https://ieeexplore.ieee.org/document/7840733/),
[Du et al.](https://www.cs.utah.edu/~lifeifei/papers/deeplog.pdf), and
[Brown et al.](https://arxiv.org/pdf/1803.04967.pdf), use logs to generate feature sequences which are fed 
into an LSTM to, afterwards, detect anomalies of hardware and software applications. 
+ Traces. Graph-based techniques.
The article [On Predictability of System Anomalies in Real World](https://ieeexplore.ieee.org/document/5581600) 
provides an illustrative example on how hard drive failures can be predicted.
#### Multimodel metric anomaly detection
Multimodal metrics are often a symptom that part of a system build for reliability has a performance degradation,
possibly due to a failure. 
The use of clustering algorithms for multimodal anomaly detection are an interesting approach. The latent groupings
are found by finding a partition that separates the moniroting data into unimodal subsets that are more coherent
than the unpartitioned superset.
#### Univariate/multivariate anomaly detection
To detect servers which are outliers, [Netflix](https://medium.com/netflix-techblog/tracking-down-the-villains-outlier-detection-at-netflix-40360b31732) uses the clustering algorithm [DBSCAN](https://en.wikipedia.org/wiki/DBSCAN). 
Using a self-service paradigm, service owners identify one metric to be monitored for outliers.
The troubleshooting system runs DBSCAN to analysis metric windows which returns the set of servers considered outliers.
Service owners also specify the minimum timeframe for a deviation to be considered a true positive outlier.
Netflix solves the parameter selection challenge inherent to most algorithms by only asking service owners to define the current number of outliers. 
Using [simulated annealing](https://en.wikipedia.org/wiki/Simulated_annealing), the distance and minimum cluster size parameters are determined.
Results show a precision of 93%, recall: 87%, [F-score](https://en.wikipedia.org/wiki/F1_score): 90% for pools of almost 2K servers.
Following the approach path from Netflix, [other researchers](https://www.hbs.edu/faculty/Publication%20Files/2019%20HICSS%20Anomoly_296d232d-ccb2-448d-9d19-603c08a04a19.pdf) have extended the approach by using several metrics. While the results were more modest, the software system at hand was more complex.
To come...using tsfresh and Random Forests
### Diagnosis
+ *Dependency Inference* exploits the relationships between components and the communications between microservices 
to localize the causes of problems since failure often propagate across distributed systems. 
Early work in this field include the research by [Attariyan at al.](https://www.usenix.org/system/files/conference/osdi12/osdi12-final-33.pdf)
More recent proposal include the research from [Mace](https://cs.brown.edu/~jcmace/mace_thesis.pdf).
+ *Correlation Analysis* explores the correlations between the metrics generated by different components. 
A correlation implies a relationship which can be followed to find a root cause.
+ *Similarity Analysis* detects and localize problems by comparing behaviors of components, under the assumption 
that in normal state they should perform similarly. For example, in a cluster, the server with an outlier behavior 
is the problematic component. The assumption is that the majority of servers in a cluster are fault-free.
### Alarm Deduplication
Large-scale, complex IT infrastructures can generate tens to hundreds of alarms and represent one of the most important 
challenges for operators. 
One unique problem can suddenly inundated an operation with 20-30 alarms in response to a database failure. 
In such situations, the alarms system become a nuisance rather than a useful tool. 
Alarm deduplication (also known as alarm suppression) is a class of management techniques which groups alarms related 
to the same underlying problem. 
It transforms a set of related alarms into one or two notifications that really matter. 
In some cases, it is possible to identify the key alarm, core of the problem, and establish a relationship 
with the root cause. 
Techniques:
+ Flood analysis
+ Correlation and causality analysis 
### Capacity Planning
Refers to the capture of metric records relevant to understanding workloads and utilization trends to devise 
future capacity plans, and resource poll capacity forecast.
Examples of metrics to track include:
+ vCPU number, vRAM allocation, and compute load
+ Storage allocation and I/O latency
+ Network traffic
Fore casting becomes challenging when holidays, multiple seasonalities need to be considered. 
### Incident Response
+ Alerting 
+ Troubleshooting
When a root cause is matched with an alarm or anomaly, AIOps can initiate and orchestrate a remediation workflow and 
route a description of the root cause to the most adequate expert team for change, problem, and incident management 
following [ITIL best practices](https://en.wikipedia.org/wiki/ITIL).
AI can act as a matchmaking and routing system by using multi-criteria techniques to identify in real time the 
right teams based on on-call schedules, location and expertise.
Additionally, AIOps can use NLP techniques to index relevant documentation to assist operation teams to remediate an 
issue.
Early work in this field include the research from
[Candea et al.](https://link.springer.com/article/10.1007/s10586-006-7562-4)
which show how to reduce downtime of applications by automatically recovering from transient and intermittent 
software failures.
### Security Management
While anomaly detection is frequently employed to detect unusual patterns in monitoring data related to IT 
infrastructure problems, the same techniques can be use to detect patterns typically associated with a variety of 
security risks such violations, unauthorized access, and malicious activities.
Machine learning algorithms can process netflow metrics, control plan traces, and application and network equipment
logs for the analysis. 
AIOps brings increased benefits when threats are complex and multi-dimensional, since their interpretation by human 
is hard and time consuming. 
### Resource Optimization
To come....
### Root Cause Analysis
+ Proactively find anomalies before failures are reported
+ Perform audit trails and root cause analysis (RCA)
### Infrastructure Scaling
+ Configure predictive scaling that learns from the previous load conditions and usage patterns
### Cost Management
+ Forecast the cost of infrastructure (VM or IT)
+ Intelligent cost mng. in an important feature of public clouds
### Real time application behavior learning
+ Learns the behavior of application using trace patterns, log status messages, and performance metrics. 
### Performance Tuning
+ Auto tuning of workloads by analyzing the time taken for common tasks such as responding to a request and 
apply an accurate fix to the problem
Techniques:
+ Message Queuing analysis. Message queue length is a good metric for system health analysis.
The length of queue of connecting microservices are constantly monitored and an alert is sent out if their size goes
beyond a predefined threshold. Either generate alerts on the size of single queues or a sum of message queues.
### Energy Efficiency 
+ Manual
+ DVS Dual voltage papers
### Memory and SSD anomaly detection
to come
### Alarm deduplication
to come
## Existing Systems
### Industry
+ Tracing: [Lightstep](https://lightstep.com), [Google StackDriver](https://cloud.google.com/trace/),
[Amazon X-Ray](https://aws.amazon.com/xray/)
+ APM: [New Relic](https://newrelic.com/products/application-monitoring), [AppDynamics](https://www.appdynamics.com),
[Dynatrace](https://www.dynatrace.com), 
[Azure Application Insights](https://docs.microsoft.com/en-us/azure/azure-monitor/app/cloudservices),
[CA APM](https://www.ca.com/us/products/application-performance-monitoring.html)
### Academia
To come...
## Research Methodology
We will follow a KPI-driven development for the new approaches and algorithms devised by the project. 
This means that we will always implement first a baseline algorithm to clearly demonstrate the limitations of existing approaches.
Afterwards, for each incremental enhancement we make to the new algorithm, we will quantify the improvement using ROC AUC, F1 Score, etc. 
## Team and Culture
Several researchers have contributed to iForesight, namely, llya Shakhat, Paul Staab, Wei Guangsheng, Jinxunmi,
Sasho Nedelkoski, Alexander Wieder, Yi Feng, Florian Richter, Francesco del Buono, Phani Pawan, and Ankur Bhatia, 
among others.
Our skill set encompasses expertise in the fields of:
+ AI/Data Science (time series analysis, regression, decision trees, pattern recognition, probability theory 
and neural networks),
+ Software Engineering (analysis, design, development, testing), and 
+ Operation (deployment, infrastructure).
Our culture of innovation and R&D is based on 4 main guiding principles:
+ [DIY](https://en.wikipedia.org/wiki/Do_it_yourself),
[KISS](https://pt.wikipedia.org/wiki/Keep_It_Simple),
[Deep Work](https://www.amazon.com/gp/product/1455586692)*, 
[KPI-driven Research](https://en.wikipedia.org/wiki/Performance_indicator)
*(Time Spent) x (Intensity of Focus)