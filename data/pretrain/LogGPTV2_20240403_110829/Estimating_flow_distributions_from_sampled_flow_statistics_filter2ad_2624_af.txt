web
DNS
trace
COS
COS
FLOW TCP+UDP
FLOW DNS/UDP
N = 10
WMRD M (4)
54%
11%
8%
16%
-
-
-
-
N = 100
WMRD M (4)
60%
14%
32%
37%
4%
11%
3%
3%
Table 5: Weighted mean relative difference in ﬂow length fre-
quencies, and estimation error in total number of ﬂows (cid:13)M (4),
for COS and FLOW datasets
According to the Poisson analysis of Section 6.3, only sampled fre-
quencies gj for j = 1, 2 have better than around 30% likely accu-
racy, so it is not surprising that iteration is not very accurate with
this amount of data. However, as expected, the TCP based method
estimates the total number of TCP ﬂows with M (3) quite well, to
within 6% for both N = 10 and N = 100.
7.3.2 Estimation of general ﬂow lengths: (cid:5)f (4).
We evaluated the general ﬂow length estimator (cid:5)f (4) using the
COS trace. We extracted two subtraces: web ﬂows (TCP ﬂows with
destination port 80) and DNS ﬂows (as identiﬁed by destination
port 53). The inferred frequencies for N = 10 and N = 100
are shown in Figure 4. Notice the original web frequencies are
not smooth at short lengths; as remarked in Section 6.3 the iter-
ative estimate is smoother than the actual distribution. The DNS
ﬂow length frequencies are comparatively smooth: the iterated pre-
dictions are fairly close. The N = 100 prediction falls off more
quickly that the actual frequencies. In this case, only the lowest
three sampled frequencies are readily distinguishable from 0, so
the lack of accuracy at higher ﬂow lengths is not surprising. For
comparison, the total number of sampled ﬂows bears no clear rela-
tion to the number of original ﬂows: for DNS ﬂows it was 2% and
20% of the respective original ﬂows for N = 10 and N = 100; for
web, these numbers were 48% and 6% respectively.
The WMRD between the inferred and actual frequencies are shown
in Table 5, along with the error in estimating the total number of
ﬂows with M (4). Accuracy of inferred frequencies for web traf-
ﬁc is only to within about a factor of 2. This is a result of the ill
matching of the smooth inferred distribution to the non-smooth ac-
y
c
n
e
u
q
e
r
f
100000
10000
1000
100
10
1
1
original
inferred f(4): N=10
inferred f(4): N=100
original
inferred f(4): N=10
inferred f(4): N=100
100000
10000
1000
100
10
10
original flow length
100
1000
1
1
10
original flow length
100
Figure 4: EM-BASED FLOW LENGTH ESTIMATION OF GENERAL FLOWS: Dataset COS: (Left) Web trafﬁc; (Right) DNS trafﬁc.
Sampling periods N = 10 and 100. Estimation using (cid:5)f (4). Note logarithmic axes.
tual frequencies. Similarly with the scaling-based estimators, there
would be closer agreement between slight smoothed (i.e. aggre-
gated) versions of these distributions. Indeed, the total number of
ﬂows is estimated to within about 15%, except for the case of DNS
with N = 100. As mentioned above, there was very little useful
sampled data in this case.
Table 5 also show results for sampling period N = 100, using
two components of the FLOW dataset: all TCP and UDP trafﬁc, and
DNS/UDP trafﬁc. The results here were noticeably more accurate
that for COS. We believe this is because the original ﬂow length
frequencies are somewhat smoother that those of COS. Since FLOW
does not contain packet level detail, we emulated the effects of in-
dependent packet sampling with probability 1/N by taking each
ﬂow of length (cid:2) and generating a random number k of packets fol-
lowing the binomial distribution B1/N ((cid:2), k). This procedure ig-
nores ﬂow splitting, but using results similar to Theorem 1 of [9],
we are able to show that the total number of sampled ﬂows is un-
derestimated by only about 10% on average for sampling period
N = 100.
8. ESTIMATION OF HOST INFECTIONS
As a concrete example we wish to estimate the number of hosts
that have been compromised in a network attack, and are them-
selves sending out attack trafﬁc. In particular, we wish to estimate
the number of such hosts that send trafﬁc past a given collector
or set of collectors of sampled ﬂow statistics. By combining with
routing information, we may identify the numbers of infected hosts
in different regions of the network.
A recent example of such at attack arose in the MS SQL server
worm that started activity on January 25, 2003; see e.g. [17]. In-
fected hosts send out a sequence attacking packets to randomly cho-
sen destination IP addresses. The source IP address is not spoofed.
The attack packets have the following signature: a 404 byte UDP
packet with destination port 1434. Since the destination IP address
of attack packets is chosen randomly from packet to packet, it is
very unlikely that two attack packets with the same destination IP
address will be present from the same attacker within the ﬂow in-
terpacket timeout. Indeed, [17] reports the largest directly observed
attack rate from a host of 26,000 scans per second. At this rate, the
chance that a given 32 bit address will recur within a 30 second pe-
(cid:5)f (4).
as M (4) =
i≥1
riod is about 0.02%. Thus it is reasonable to assume for simplicity
that all attack packets give rise to a single packet ﬂow statistic.
Assume packet sampling with probability p = 1/N. Since the
original ﬂows comprise one packet, then each attack ﬂow is present
in the sampled ﬂow statistics with probability p. Thus, counting the
number of distinct source IP addresses will undercount the number
of infected hosts: some hosts may have none of their attack packets
sampled.
We can map the problem of detecting the number of hosts onto
the problem previously solved. For each attacking host (i.e. source
IP address matching the proﬁle) represented in the sampled ﬂow
statistics, we compute the number i of attack ﬂows detected in the
sampled ﬂow statistics. Let gi denote the absolute frequency of
hosts sourcing i measured attack ﬂows. Then we estimate the distri-
bution of the actual number of hosts sourcing i attacks ﬂows using
the estimator (cid:5)f (4). The total number of infected hosts is estimated
(cid:4)
We tested the method on the COS dataset. Within the trace there
were 4,542,157 worm packets originating from 49,200 hosts. How-
ever, the distribution was highly skewed in the tail: three hosts orig-
inated 3,005,083 and 978,841 and 38,770 of the worm packets seen
in the trace, i.e. at least 88% of the total worm packets. All other
hosts originated less than 2,250 packets each. We conjecture that
the hosts generating the largest numbers of packets were located in
the campus at which the trace was taken; in this case, most of the
randomly chosen target addresses would be on external networks
and hence be routed past the trace collector.
We performed inference of the total number of attacking hosts
for N = 10 and N = 100. For N = 10, we took jmax = 50;
there were M+ = 72 hosts originating more than 50 packets. Os-
cillatory behavior in the inferred distribution onset at about 100 it-
erations. At this point there were 43,403 inferred hosts; added with
M+ this yielded about 88% of the true number. For N = 100, we
took jmax = 10; there were M+ = 16 originating more than 10
sampled packets. Oscillatory behavior of the inferred distribution
onset after about 1000 iterations. At this point there were 27,178
inferred hosts; added with M+ this yielded about 55% of the true
number. By comparison, the number of sampled hosts were 14,667
for N = 10, and 3,469 for N = 100.
For our problem, the ﬁrst order unsmoothed jackknife estima-
tor Duj1 from [13] takes the form Duj1(N ) = d(N )/(1 − (1 −
1/N )g1/q(N )) where d(N ) is the number of hosts represented
in the sampled trace, and q(N ) the number of sampled packets.
Duj1(10) = 14, 912 and Duj1(100) = 3, 672, little different from
the number of sampled hosts. Other estimators recommended in
[13] are reﬁnements of Duj1, and exhibit roughly the same behav-
ior. To be fair, these estimators are intended for use when the sam-
pling rate is not very small; their representation of the frequency
distribution through a limited number of moments cannot be ex-
pect to capture the high variability of actual frequencies.
9. CONCLUSIONS AND FURTHER WORK
This paper was motivated by the desire to understand detailed
ﬂow statistics of Internet trafﬁc on the basis of ﬂow statistics com-
piled from sampled packet streams.
Increasingly, only sampled
ﬂow statistics are available: inference is required to determine the
ﬂow characteristics of the original unsampled trafﬁc.
In this paper we have proposed using two inference methods.
The scaling method codiﬁed the heuristic that when sampling 1 out
of N packets, since sampled ﬂows have roughly 1/N of their pack-
ets sampled, the length of the original ﬂow should be N times the
sampled ﬂow. To pin this down we needed to estimate the number
of unsampled ﬂows, this required extracting additional information
in the form of the number of sampled SYN packets.
In the scaling approach, the hard work was in adjusting the low-
est order weights to better reﬂect the underlying distribution. Clearly
there is scope to extend this approach to better tune the distribution
of weights from longer ﬂows. An open question is whether a sim-
ilar analysis for general ﬂows can be used estimate the number of
unsampled ﬂows in the absence of protocol information.
The EM algorithm is an iterative approach to ML estimation of
ﬂow length frequencies. It does not require protocol level informa-
tion, although it can exploit it. The versatility comes at the cost of
computational complexity, and less control over the total number
of inferred ﬂows. The main challenge for the method is selection
of a good termination criterion. Prolonged iteration was found to
lead to some oscillatory behavior in the tail of the inferred distribu-
tion. Our rule of thumb was to terminate before such oscillations
become pronounced. Estimation of the head of the distribution was
found to be reasonably accurate at this point, to within a factor of
2 at worst, down to a few percent in some cases. In future work we
propose to augment the EM approach with second order methods
to achieve faster convergence. Another avenue is to use prior statis-
tical models for the distributions to favor conformance with model
distributions by use of penalized likelihood [15].
Acknowledgments
Thanks are due to Jia Wang for coding a version of the EM-inference
algorithm used in some experiments, and to Balachander Krishna-
murthy and Joerg Michael for assistance with some of the datasets.
10. REFERENCES
[1] J. Apisdorf, K. Claffy, K. Thompson, R. Wilder, “OC3MON:
Flexible, Affordable, High Performance Statistics
Collection,” See: http://www.nlanr.net/NA/Oc3mon
[2] B.-Y. Choi, J.Park, Zh.-L. Zhang, ”Adaptive Random
Sampling for Load Change Detection”, ACM SIGMETRICS
2002 (Extended Abstract).
[3] Cisco NetFlow; for further information see
http://www.cisco.com/warp/public/732/netﬂow/index.html
[4] K.C. Claffy, H.-W. Braun, and G.C. Polyzos.
“Parameterizable methodology for internet trafﬁc ﬂow
proﬁling”, IEEE Journal on Selected Areas in
Communications, vol. 13, no. 8, pp. 1481–1494, Oct. 1995.
[5] K.C. Claffy, G.C. Polyzos, and H.-W. Braun. “Application of
Sampling Methodologies to Network Trafﬁc
Characterization”, Proceedings ACM SIGCOMM’93, San
Francisco, CA, September pp. 13–17, 1993.
[6] D. Comer, “Internetworking with TCP/IP, Volume 1:
Principles, Protocols, and Architecture”, Third Edition,
Prentice Hall, NJ, 1995.
[7] A.P. Dempster, N.M. Laird, D.B. Rubin, “Maximum
likelihood from incomplete data via the EM algorithm (with
discussion)”, J. Roy. Statist. Soc. Ser., vol. 39, pp. 1–38,
1977.
[8] N.G. Dufﬁeld, C. Lund, M. Thorup, “Charging from sampled
network usage,” ACM SIGCOMM Internet Measurement
Workshop 2001, San Francisco, CA, November 1-2, 2001.
[9] N.G. Dufﬁeld, C. Lund, M. Thorup, “Properties and
Prediction of Flow Statistics from Sampled Packet Streams”,
ACM SIGCOMM Internet Measurement Workshop 2002,
Marseille, France, November 6-8, 2002.
[10] C. Estan and G. Varghese, “New Directions in Trafﬁc
Measurement and Accounting”, Proc SIGCOMM 2002,
Pittsburgh, PA, August 19–23, 2002.
[11] A. Feldmann, R. Caceres, F. Douglis, G. Glass, M.
Rabinovich, “Performance of Web Proxy Caching in
Heterogeneous Bandwidth Environments,” in Proc. IEEE
INFOCOM’99, New York, NY, March 23-25, 1999.
[12] A. Feldmann, J. Rexford, and R. C´aceres, “Efﬁcient Policies
for Carrying Web Trafﬁc over Flow-Switched Networks,”
IEEE/ACM Transactions on Networking, vol. 6, no.6, pp.
673–685, December 1998.
[13] P.J. Haas and L. Stokes, “Estimating the number of classes in
a ﬁnite population,” J. Amer. Statist. Assoc., vol. 93, pp
1475–1487, 1998.
[14] Inmon Corporation, “sFlow accuracy and billing”, see:
http://www.inmon.com/PDF/sFlowBilling.pdf
[15] P.J. Green, “On the use of the EM algorithm for penalized
likelihood estimation,” J. R. Statist. Soc. B, vol. 52, pp. 443
452, 1990.
[16] “Internet Protocol Flow Information eXport” (IPFIX). IETF
Working Group. See: http://net.doit.wisc.edu/ipﬁx/
[17] D. Moore, V. Paxson, S. Savage, C. Shannon, S. Staniford,
N. Weaver, “The Spread of the Sapphire/Slammer Worm”,
Technical Report, CAIDA, 2003. See
http://www.caida.org/outreach/papers/2003/sapphire/sapphire.html.
[18] NLANR Moat PMA trace archive. See
http://pma.nlanr.net/Traces/long/ipls1.html
[19] V. Paxson, “Empirically-Derived Analytic Models of
Wide-Area TCP Connections”, IEEE/ACM Transactions on
Networking, Vol. 2 No. 4, August 1994.
[20] V. Paxson, G. Almes, J. Mahdavi, M. Mathis, “Framework
for IP Performance Metrics”, RFC 2330, May 1998.
[21] Packet Sampling (PSAMP) IETF Working Group Charter.
See http://www.ietf.org/html.charters/psamp-charter.html
[22] J. Postel, “Transmission Control Protocol,” RFC 793,
September 1981.
[23] L. Sachs, “Applied Statistics”, Second Edition, Springer,
New York, 1984.
[24] C.F. Jeff Wu, “On the convergence properties of the EM
algorithm”, Annals of Statistics, vol. 11, pp. 95–103, 1982