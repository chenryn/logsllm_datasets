title:Pingmesh: A Large-Scale System for Data Center Network Latency Measurement
and Analysis
author:Chuanxiong Guo and
Lihua Yuan and
Dong Xiang and
Yingnong Dang and
Ray Huang and
David A. Maltz and
Zhaoyi Liu and
Vin Wang and
Bin Pang and
Hua Chen and
Zhi-Wei Lin and
Varugis Kurien
Pingmesh: A Large-Scale System for Data Center
Network Latency Measurement and Analysis∗
Chuanxiong Guo, Lihua Yuan, Dong Xiang, Yingnong Dang, Ray Huang, Dave Maltz,
Zhaoyi Liu, Vin Wang, Bin Pang, Hua Chen, Zhi-Wei Lin, Varugis Kurien†
Microsoft, †Midﬁn Systems
{chguo, lyuan, dxiang, yidang, rayhuang, dmaltz, zhaoyil, vinwang, bipang, stchen,
linzw}@microsoft.com, vkurien@midﬁnsystems.com
ABSTRACT
Can we get network latency between any two servers
at any time in large-scale data center networks? The
collected latency data can then be used to address a
series of challenges: telling if an application perceived
latency issue is caused by the network or not, deﬁn-
ing and tracking network service level agreement (SLA),
and automatic network troubleshooting.
We have developed the Pingmesh system for large-
scale data center network latency measurement and anal-
ysis to answer the above question aﬃrmatively. Pingmesh
has been running in Microsoft data centers for more
than four years, and it collects tens of terabytes of la-
tency data per day. Pingmesh is widely used by not only
network software developers and engineers, but also ap-
plication and service developers and operators.
CCS Concepts
•Networks → Network measurement; Cloud com-
puting; Network monitoring; •Computer systems
organization → Cloud computing;
Keywords
Data center networking; Network troubleshooting; Silent
packet drops
1.
INTRODUCTION
In today’s data centers there are hundreds of thou-
sands of servers. These servers are connected via net-
∗This work was performed when Varugis Kurien was
with Microsoft.
Permission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for proﬁt or commercial advantage and that copies bear this notice
and the full citation on the ﬁrst page. Copyrights for components of this work
owned by others than ACM must be honored. Abstracting with credit is per-
mitted. To copy otherwise, or republish, to post on servers or to redistribute to
lists, requires prior speciﬁc permission and/or a fee. Request permissions from
permissions@acm.org.
SIGCOMM ’15, August 17 - 21, 2015, London, United Kingdom
c(cid:13) 2015 ACM. ISBN 978-1-4503-3542-3/15/08. . . $15.00
DOI: http://dx.doi.org/10.1145/2785956.2787496
work interface cards (NICs), switches and routers, ca-
bles and ﬁbers, which form large-scale intra and inter
data center networks. The scale of the data center net-
works (DCNs) is growing even larger due to the rapid
development of cloud computing. On top of the phys-
ical data center infrastructure, various large-scale, dis-
tributed services are built, e.g., Search [5], distributed
ﬁle systems [17] and storage [7], MapReduce [11].
These distributed services are large and evolving soft-
ware systems with many components and have complex
dependencies. All of these services are distributed and
many of their components need to interact via the net-
work either within a data center or across diﬀerent data
centers. In such large systems, software and hardware
failures are the norm rather than the exception. As a
result, the network team faces several challenges.
The ﬁrst challenge is to determine if an issue is a
network issue or not. Due to the distributed systems
nature, many failures show as “network” problems, e.g.,
some components can only be reached intermittently, or
the end-to-end latency shows a sudden increase at the
99th percentile, the network throughput degrades from
20MB/s per server to less than 5MB/s. Our experience
showed that about 50% of these “network” problems are
not caused by the network. However it is not easy to
tell if a “network” problem is indeed caused by network
failures or not.
The second challenge is to deﬁne and track network
service level agreements (SLAs). Many services need
the network to provide certain performance guarantees.
For example, a Search query may touch thousands of
servers and the performance of a Search query is de-
termined by the last response from the slowest server.
These services are sensitive to network latency and packet
drops and they care about the network SLA. Network
SLA needs to be measured and tracked individually
for diﬀerent services since they may use diﬀerent set
of servers and diﬀerent part of the network. This be-
comes a challenging task because of the huge number of
services and customers in the network.
The third challenge is network troubleshooting. When
network SLAs are broken due to various network fail-
139ures, “live-site” incidents happen. A live-site incident is
any event that results in an impact to the customers,
partners or revenue. Live-site incidents need to be de-
tected, mitigated, and resolved as soon as possible. But
data center networks have hundreds of thousands to mil-
lions of servers, hundreds of thousands of switches, and
millions of cables and ﬁbers. Thus detecting where the
problem is located is a hard problem.
To address the above challenges, we have designed
and implemented Pingmesh, a large-scale system for
data center network latency measurement and analy-
sis. Pingmesh leverages all the servers to launch TCP
or HTTP pings to provide the maximum latency mea-
surement coverage. Pingmesh forms multiple levels of
complete graphs. Within a data center, Pingmesh lets
the servers within a rack form a complete graph and
also uses the top-of-rack (ToR) switches as virtual nodes
and let them form a second complete graph. Across
data centers, Pingmesh forms a third complete graph by
treating each data center as a virtual node. The calcula-
tion of the complete graphs and related ping parameters
are controlled by a central Pingmesh Controller.
The measured latency data are collected and stored,
aggregated and analyzed by a data storage and analy-
sis pipeline. From the latency data, network SLAs are
deﬁned and tracked at both the macro level (i.e., data
center level) and the micro level (e.g., per-server and
per-rack levels). The network SLAs for all the services
and applications are calculated by mapping the services
and applications to the servers they use.
Pingmesh has been running in tens of globally dis-
tributed data centers of Microsoft for four years.
It
produces 24 terabytes of data and more than 200 bil-
lion probes per day. Because of the universal availability
of the Pingmesh data, answering if a live-site incident
is because of the network becomes easier: If Pingmesh
data does not indicate a network problem, then the live-
site incident is not caused by the network.
Pingmesh is heavily used for network troubleshooting
to locate where the problem is. By visualization and au-
tomatic pattern detection, we are able to answer when
and where packet drops and/or latency increases hap-
pen, identify silent switch packet drops and black-holes
in the network. The results produced by Pingmesh is
also used by application developers and service opera-
tors for better server selection by considering network
latency and packet drop rate.
This paper makes the following contributions: We
show the feasibility of building a large-scale network la-
tency measurement and analysis system by designing
and implementing Pingmesh. By letting every server
participate, we provide latency data for all the servers
all the time. We show that Pingmesh helps us better un-
derstand data center networks by deﬁning and tracking
network SLA at both macro and micro scopes, and that
Pingmesh helps reveal and locate switch packet drops
including packet black-holes and silent random packet
drops, which were less understood previously.
2. BACKGROUND
2.1 Data center networks
Data center networks connect servers with high speed
and provide high server-to-server bandwidth. Today’s
large data center networks are built from commodity
Ethernet switches and routers [1, 12, 2].
Figure 1 shows a typical data center network struc-
ture. The network has two parts:
intra data center
(Intra-DC) network and inter data center (Inter-DC)
network. The intra-DC network is typically a Clos net-
work of several tiers similar to the network described
in [1, 12, 2]. At the ﬁrst tier, tens of servers (e.g., 40)
use 10GbE or 40GbE Ethernet NICs to connect to a
top-of-rack (ToR) switch and form a Pod. Tens of ToR
switches (e.g., 20) are then connected to a second tier
of Leaf switches (e.g., 2-8). These servers and ToR and
Leaf switches form a Podset. Multiple Podsets then
connect to a third tier of Spine switches (tens to hun-
dreds). Using existing Ethernet switches, an intra-DC
network can connect tens of thousands or more servers
with high network capacity.
One nice property of the intra-DC network is that
multiple Leaf and Spine switches provide a multi-path
network with redundancy. ECMP (equal cost multi-
path) is used to load-balance traﬃc across all the paths.
ECMP uses the hash value of the TCP/UDP ﬁve-tuple
for next hop selection. As a result, the exact path of
a TCP connection is unknown at the server side even
if the ﬁve-tuple of the connection is known. For this
reason, locating a faulty Spine switch is not easy.
The inter-DC network is to interconnect the intra-DC
networks and to connect the inter-DC networks to the
Internet. The inter-DC network uses high-speed, long
haul ﬁbers to connect data centers networks at diﬀerent
geolocations. Software deﬁned networking (SWAN [13],
B4 [16]) are further introduced for better wide area net-
work traﬃc engineering.
Our data center network is a large, sophisticated dis-
tributed systems. It is composed of hundreds of thou-
sands of servers, tens of thousands switches and routers,
and millions of cables and ﬁbers. It is managed by Au-
topilot [20], our home-grown data center management
software stack, and the switches and NICs run soft-
ware and ﬁrmware provided by diﬀerent switch and NIC
providers. The applications run on top of the network
may introduce complex traﬃc patterns.
2.2 Network latency and packet drops
In this paper we use the term “network latency” from
application’s point of view. When an application A at
a server sends a message to an application B at a peer
server, the network latency is deﬁned as the time in-
terval from the time A sends the message to the time
B receives the message. In practice we measure round-
trip-time (RTT) since RTT measurement does not need
to synchronize the server clocks.
140a set of Autopilot services including Device Manager
(DM), which manages the machine state, Deployment
Service (DS) which does service deployment for both
Autopilot and various applications, Provisioning Ser-
vice (PS) which installs Server OS images, Watchdog
Service (WS) which monitors and reports the health
status of various hardware and software, Repair Service
(RS) which performs repair action by taking commands
from DM, etc.
Autopilot provides a shared service mode. A shared
service is a piece of code that runs on every autopilot
managed server. For example, a Service Manager is a
shared service that manages the life-cycle and resource
usage of other applications, a Perfcounter Collector is a
shared service that collects the local perf counters and
then uploads the counters to Autopilot. Shared ser-
vices must be light-weight with low CPU, memory, and
bandwidth resource usage, and they need to be reliable
without resource leakage and crashes.
Pingmesh uses our home-grown data storage and anal-
ysis system, Cosmos/SCOPE, for latency data storage
and analysis. Cosmos is Microsoft’s BigData system
similar to Hadoop [3] which provides a distributed ﬁle
system like GFS [17] and MapReduce [11]. Files in Cos-
mos are append-only and a ﬁle is split into multiple
‘extents’ and an extent is stored in multiple servers to
provide high reliability. A Cosmos cluster may have
tens of thousands of servers or more, and gives users
almost ‘inﬁnite’ storage space.
SCOPE [15] is a declarative and extensible scripting
language, which is built on top of Cosmos, to analyze
massive data sets. SCOPE is designed to be easy to
use.
It enables users to focus on their data instead
of the underlying storage and network infrastructure.
Users only need to write scripts similar to SQL without
worrying about parallel execution, data partition, and
failure handling. All these complexities are handled by
SCOPE and Cosmos.
3. DESIGN AND IMPLEMENTATION
3.1 Design goal
The goal of Pingmesh is to build a network latency
measurement and analysis system to address the chal-
lenges we have described in Section 1. Pingmesh needs
to be always-on and be able to provide network latency
data for all the servers. It needs to be always on be-
cause we need to track the network status all the time.
It needs to produce network latency data for all the
servers because the maximum possible network latency
data coverage is essential for us to better understand,
manage, and troubleshoot our network infrastructure.
From the beginning, we diﬀerentiated Pingmesh from
various public and proprietary network tools (e.g., tracer-
oute, TcpPing, etc.). We realized that network tools do
not work for us because of the following reasons. First,
these tools are not always-on and they only produce
Figure 1: Data center network structure.
RTT is composed of application processing latency,
OS kernel TCP/IP stack and driver processing latency,
NIC introduced latency (e.g., DMA operations, inter-
rupt modulation) [22], packet transmission delay, prop-
agation delay, and queuing delay introduced by packet
buﬀering at the switches along the path.
One may argue the latencies introduced by applica-
tions and kernel stack are not really from the network.
In practice, our experiences have taught us that our
customers and service developers do not care. Once a
latency problem is observed, it is usually called a “net-
work” problem. It is the responsibility of the network
team to show if the problem is indeed a network prob-
lem, and if it is, mitigate and root-cause the problem.
User perceived latency may increase due to various
reasons, e.g., queuing delay due to network congestion,