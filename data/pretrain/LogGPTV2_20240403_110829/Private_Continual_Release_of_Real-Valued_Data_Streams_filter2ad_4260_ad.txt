(log2(n − m))1.5τ
+ (B − τ )pn
ln
1
βout
+ 1
,
(14)
with probability at most β, where β is a bound on the sum
of the ﬁve error probabilities.8 If the error is dominated by
the ﬁrst summand, then this leads to an improvement factor of
τ in utility over the application of the BT algorithm without
B
our mechanism (see Eq. 3). However, looking at the second
summand, we see that the outlier error is proportional to pn.
To make this into a constant error term, we need p ≈ 1
n.
But recall from Eq. 4 that we require m (cid:29) 1
p. Thus, it is
not possible to bound this error term. Hence, if the input
stream has the worst-case distribution, our mechanism does
not improve utility. However, arguably, real-world data streams
are not distributed in this way.
B. Error on Light-Tailed Distributions
As shown in Section III-B, many real-world data distribu-
tions are expected to be light-tailed (cf. Deﬁnition 12), thus
behaving signiﬁcantly differently than the worst case. More
precisely, we focus on distributions that are light-tailed be-
yond their pmax-quantile, a quantity to be determined shortly.
8i.e., βqt, βlt, βLap, βout and βrt.
Since the distribution is light-tailed, we can use Proposi-
tion 1 and the assumption xp ≤ ˆxλp to conclude that for all
p ≤ pmax,
ˆxλp · r ≥ xpr ,∀r ≥ 1.
(15)
Thus, instead of using the threshold τ directly from Eq. 7, we
multiply it by r and set it as the threshold. According to the
above equation, this results in reduced outlier error whenever
r > 1.
Determining pmax: To determine the value of pmax, we
perform a series of experiments on the train trips and the
supermarket dataset. We seek a value of pmax that ensures the
light-tailed property on both datasets. We vary pr beginning
from a value of 0.1 to increasingly small values. Against each
value of pr we obtain the empirical pr-quantile, i.e., ˆxpr. We
then use different values of p, e.g., 0.1, 0.01, and so on. From
each pair of values of p and pr, we obtain r and multiply it
with the empirical pr-quantile to obtain rˆxp. The aim is to
ﬁnd a value of pmax such that for all p ≤ pmax, rˆxp ≈ ˆxpr.
Since ˆxλp ≥ ˆxp, this implies that Eq. 15 would be satisﬁed.
The results are shown in Figure 6a for the train trips dataset
and Figure 6b for the supermarket dataset. The results suggest
that pmax ≈ 0.005 sufﬁces.
Error Bound: Now, assuming that the input stream is light-
tailed we see that the outlier error is bounded by the properties
of the exponential distribution. That is, PDF ξ of the outlier
error can be written as
ξ(x) = ∆(x)(1 − pr) + pr · γe−γx,
− ln p
xp
. Thus,
(cid:35)
Ei ≥ αout
≤ E[exp(hE)]n
exp(hαout)
γ−h )n
≤ (1 + h
ehαout
= βout.
where γ =
(cid:34) n(cid:88)
i=1
Pr
8
50100150200Time(minutes)0.750.800.850.900.951.00CDFRealdistributionExponentialDistibution(a) Train trips dataset
(b) Supermarket dataset
Fig. 6. Error in estimating the empirical pr-quantile through empirical p-quantile with different choices of p. We see that below pmax ≈ 0.005, the input
datasets satisfy ˆxp · r ≈ xpr .
This gives us,
αout =
n ln(1 + h
γ−h ) + ln( 1
βout
)
We can choose the h that minimises αout as
h
(cid:32)(cid:115) pr · n
(cid:18)√
ln 1
βout
+ 1
,
(cid:33)−1
(cid:114)
(cid:19)2
.
pr · n +
ln
1
βout
h = γ
which leads to
αout ≤ −xp
ln p
+
8 ln
(cid:115)
1
βLap
(cid:19)2
pr · n +
(cid:18)√
Adding this to the error term αLap from the BT algorithm
(Eq. 3) and using the assumption xp ≤ ˆxλp, we see that the
overall error α is
(log2(n − m))1.5τ r
α ≤ 1
(cid:114)

−ˆxλp
ln p
(16)
with probability at least 1− β, where β is once again a bound
on the ﬁve error probabilities. Now, to bound the second error
term (the second summand) by a constant, we require pr ≈ 1
n.
Thus an r logarithmic in n sufﬁces. With this value of r we
see that the overall error is bounded by O(τ (log2 n)1.5/).
Thus, we obtain an improvement factor of B/τ over the BT
algorithm, which was the aim of our mechanism. In the next
section, we will show how to optimize the parameters for
utility.
1
βout
ln
,
VI. OPTIMIZING UTILITY
A. Optimized Parameters
To optimize α given by Eq. 16, we ran a series of exper-
iments on the two datasets using the Python library SciPy.9
Speciﬁcally, we used a truncated Newton method [12] (TNC)
implemented by the scipy.optimize.minimize method
to optimize α. We ﬁx n = 25,000,000 for the train trips dataset
9https://www.scipy.org/
9
and n = 150,000 for the supermarket dataset. The parameter
β, i.e., overall probability of exceeding an error of α, was
ﬁxed to 0.02, and δ was ﬁxed to 2−20.10 For both datasets, we
analyze the inﬂuence of local and global parameters separately.
Effect of Local Parameters: We ﬁxed  = 1 for this series
of experiments. Then, for different values of the time lag m, we
ran the optimizer on the objective function α given by Eq. 16,
with the constraints: p ≤ pmax = 0.005, λ ≤ 1, r ≥ 1, and
κ > 0. Note that the optimization algorithm is deterministic:
given ﬁxed global parameters, we obtain the same value of the
local parameters each time. We also deﬁne the improvement
factor (IF), as the ratio of error obtained from the BT algorithm
to the error obtained through our mechanism. The results are
shown in Tables I and II.
TABLE I.
OPTIMIZED PARAMETERS FOR THE TRAIN TRIPS DATASET
r
1
λ
1
IF
p
1
0.005
2.32 1.63 0.85 0.005
2.8
1
m

0
40000
0.8
50000
1.49 0.83 0.0049 0.9
60000
100000 3.69 1.76 0.87 0.0048 0.9
300000 4.34 1.91 0.87 0.005
βqt
β
0.00 0.00 1
0.09 0.3
0.41
0.13 0.23 0.37
0.68
0.07 0.1
0.79 0.11 0.11 0.57
βlt
β
β
βLap
βout
β
0.00
0.09
0.13
0.07
0.11
βrt
β
0.00
0.09
0.13
0.07
0.11
TABLE II.
OPTIMIZED PARAMETERS FOR THE SUPERMARKET
DATASET
1
IF
m
r

0
1
1
40000
4.23 1
0.82 0.23 0.19 0.19
50000
60000
4.86 1.08 0.82 0.0049 0.88 0.15 0.27 0.27
100000 6.67 1.11 0.85 0.0046 0.81 0.07 0.07 0.71
p
λ
1
0.005
0.81 0.005
βqt
β
0.00 0.00 1
βlt
β
β
βLap
βout
β
0.00
0.19
0.15
0.07
βrt
β
0.00
0.19
0.15
0.07
We see that as the time lag m grows, IF is less impacted by
Laplace noise added to the sum as indicated by the decreasing
ratio βLap/β. The improvement factor grows with increasing
m, but this is essentially a trade-off between releasing or
withholding the sum. The rest of the parameters are relatively
stable, with higher values of r indicating that the threshold can
be set higher than the estimated experimental λp-quantile. For
smaller m, we do not see any improvement in utility over the
basic BT algorithm. Note that in such a case our mechanism
10While this value of δ is higher than recommended (i.e., negligible in
1/n [3, §2.3, p. 18]), lower values, say 2−30 [13, §3, p. 5], have a minor
impact on utility in our experiments.
10−310−210−1probabilitypr100200300400500pr-quantileˆxprrˆxp,p=0.1rˆxp,p=0.01rˆxp,p=0.005rˆxp,p=0.001simply releases the sum using the BT algorithm with noise
scaled to B. Thus, we do not incur any extra cost in utility.
The overall value of m can be obtained as the maximum of
the two values returned by the criteria.
Effect of Global Parameters: The parameters , δ, β, and m
are global parameters speciﬁed to the optimization algorithm.
The parameters β and δ are ﬁxed as before. Thus, we look at
the evolution of IF with different values of  and m. For each
value, we run the optimizer to output a set of local parameters
that maximize utility. For this, we only use the train trips
dataset with n = 25,000,000.
:
m:
Smooth sensitivity is roughly proportional to 1
, so
2
1
smaller values of  (and consequently 1) will not
result in a high IF. On the other hand, if  is too
large, the error caused by truncation (step 5 of Mech-
anism 1) will overwhelm the noise due to the Laplace
mechanism, and hence the IF will be low. Figure 7
(left) shows this trend, where we plot the improvement
factor of our algorithm over the BT algorithm by
ﬁxing m = 50,000.
The impact of the time lag m is data dependent.
We do not see much improvement when m is small,
say around 10,000. With m around 50,000 we see
noticeable increase in IF. This is indicated by Figure 7
(right), where we have ﬁxed  = 1.
a) First Criterion: This is the probability of having
ˆxλp < xp given by the function g in Eq. 5. We set
g(λ, p, m) = g(0.5, pmax, m) < β. All our optimization
experiments returned a value of λ close to 1. Thus, setting
λ = 0.5 is a reasonably conservative choice.
b) Second Criterion: This is related to the (approxi-
mate) scale of smooth sensitivity:11
κSSσ,b(ˆxλp)
a
· G−1
ns (1 − β) ≈ B
10
.
(17)
and b = min(1,
The term B/10 is arbitrarily chosen to ensure that the scale is
a few orders of magnitude less than B. With the Laplace noise
distribution, we can take a = √− ln δ
−2 ln δ ).
This readily gives us κ through Eq. 11. Now, to get a conserva-
tive bound on SSσ,b(ˆxλp), we ﬁrst assume that the exponential
distribution has its pmax-quantile close to B. In other words,
xpmax ≈ B. This means that the upper bound on the smooth
sensitivity is conservative. Now at B, the probability density
function of the exponential distribution is −pmax ln pmax
. With
m observations, the inverse of the average distance between
two observations is therefore roughly
B
−mpmax ln pmax
d =
B
.
(18)
Now, assuming that the density is approximately constant in
the neighbourhood of B, smooth sensitivity is given by
k∈N {dk exp(−bk)}.