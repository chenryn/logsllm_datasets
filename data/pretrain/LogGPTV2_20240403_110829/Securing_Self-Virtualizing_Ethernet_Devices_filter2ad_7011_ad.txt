trol mechanism of the device receives pause frames when
ﬂow control is enabled; otherwise the device silently
drops pause frames. In our setup, we disable the ﬂow
control feature of Intel NICs installed in the bridge ma-
chine and we conﬁgure them to forward pause frames up
to the OS, where they should be processed by the bridge
and ebtables. We do this by enabling the Pass MAC
Control Frames (PMCF) bit of the MAC Flow Control
(MFLCN) register, as described in section 3.7.7.2 of the
Intel 82599 data-sheet [42].
Ring Buffer Exhaustion As mentioned, some SRIOV
devices are capable of monitoring a VF’s ring buffer
and automatically generating pause frames when it is ex-
hausted. In such a scenario, pause frames will be gen-
erated with the source MAC address of the PF and will
not be recognized by the VANFC. We argue that such
pause frame generation should be disabled in any SRIOV
based setup, regardless of whether the VMs are trusted.
Since the VM fully controls the VF’s ring buffer, a ma-
licious VM can modify its software stack (e.g., the VF
device driver) to manipulate the ring buffer so that the
SRIOV device monitoring the ring buffer will generate
pause frames on the VM’s behalf. Such pause frames
will reach the external switch, which will stop its trans-
missions to the host and other VMs, leaving us with the
same attack vector.
Automatic generation of pause frames on VF ring
buffer exhaustion is problematic even if all VMs are
trusted. Consider, for example, a VM that does not have
enough CPU resources to process all incoming trafﬁc and
exhausts the VF’s ring buffer. Sending pause frames to
the switch may help this VM process the buffer but will
halt the trafﬁc to other VMs. Thus, to keep the SRIOV
device secure, an SRIOV NIC should not automatically
send pause frames when the VF’s ring buffer is exhausted
regardless of whether the VM is trusted.
Nevertheless, monitoring VF ring buffers can be use-
344  24th USENIX Security Symposium 
USENIX Association
ful for keeping the Ethernet network lossless and avoid-
ing dropped frames. We propose that the SRIOV device
monitor ring buffers, but instead of automatically gener-
ating pause frames on ring buffer exhaustion, it should
notify the hypervisor. The hypervisor, unlike the device,
could then carefully consider whether the VM is mali-
cious or simply slow. If the VM is simply slow, the hy-
pervisor could give it a scheduling boost or assign more
CPU resources to it, thereby giving it a chance to process
its ring buffer before it ﬁlls up. We plan to explore this
avenue in future work.
7 Evaluating VANFC
We evaluate VANFC in several scenarios. The base-
line scenario includes an unprotected system, as shown
in Figure 3, and no attack is performed during the test. In
this scenario we measure the system’s baseline through-
put and latency. The baseline system under attack in-
cludes the same unprotected system but here VM2 runs
the attack during the test, sending pause frames at a con-
stant rate of 150 frames/sec. In this scenario we measure
the effectiveness of the attack on an unprotected system.
In the protected system scenario, VANFC, shown
in Figure 8b, replaces the unprotected system.
In this
scenario VM2 does not perform any attack during the
test. We use this scenario to measure the performance
overhead introduced by VANFC compared to the base-
line. In the protected system under attack scenario, we
also use VANFC, but here the attacker VM2 sends pause
frames at a constant rate of 150 frames/sec. In this sce-
nario we verify that VANFC indeed overcomes the attack.
We perform all tests on the 10GbE network with the
same environment, equipment, and methodology as de-
scribed in Section 4.1.
As explained in Section 6.2, to ﬁlter malicious pause
frames, our solution uses a software-based ﬁltering de-
vice, which adds constant latency of 55µs. A produc-
tion solution would ﬁlter these frames in hardware, ob-
viating this constant latency overhead of software-based
model. Thus, in latency-oriented performance tests of
the VANFC, we reduced 55µs from the results.
Evaluation Tests To evaluate the performance of the
described scenarios, we test throughput and latency using
iperf and netperf, as previously described.
In addition, we conﬁgure the apache2 [34] web
server on VM1 to serve two ﬁles, one sized 1KB and
one sized 1MB. We use apache2 version 2.4.6 installed
from the Ubuntu repository with the default conﬁgura-
tion. We run the ab [1] benchmark tool from the client
to test the performance of the web server on VM1.
VM1 also runs memcached [35] server version
installed from the Ubuntu repository with
1.4.14,
the default conﬁguration ﬁle.
On the client we
run the memslap [78] benchmark tool, part of the
libmemcached client library, to measure the perfor-
mance of the memcached server on VM1.
Figure 9 displays normalized results of the performed
tests. We group test results into two categories: through-
put oriented and latency oriented. Throughput oriented
tests are iperf running pure TCP stream and apache2
serving a 1MB ﬁle. These tests are limited by the 10GbE
link bandwidth. During the tests, the client and server
CPUs are almost idle.
From Figure 9 we conclude that VANFC completely
blocks VM2’s attack and introduces no performance
penalty.
8 Necessity of Flow Control
One can argue that ﬂow control is not required for proper
functionality of high level protocols such as TCP. It then
follows from this argument that SRIOV can be made “se-
cure” simply by disabling ﬂow control.
The TCP protocol does provide its own ﬂow control
mechanism. However, many studies have shown that
TCP’s main disadvantage is high CPU utilization [28,36,
46, 55, 66]. Relying on TCP alone for ﬂow control leads
to increased resource utilization.
In public cloud environments, users pay for computa-
tional resources. Higher CPU utilization results in higher
charges. In enterprise data centers and high-performance
computing setups, resource consumption matters as well.
Ultimately, someone pays for it. In clouds, especially,
effective resource utilization will become increasingly
more important [12].
Certain trafﬁc patterns that use the TCP protocol in
high-bandwidth low-latency data center environments
may suffer from catastrophic TCP throughput collapse,
a phenomenon also known as the incast problem [58].
This problem occurs when many senders simultaneously
transmit data to a single receiver, overﬂowing the net-
work buffers of the Ethernet switches and the receiver,
thus causing signiﬁcant packet loss. Studies show that
Ethernet ﬂow control functionality, together with con-
gestion control protocol, can mitigate the incast problem,
thus improving the TCP performance [27, 62].
As part of a recent effort to converge current net-
work infrastructures, many existing protocols were im-
plemented over Ethernet, e.g., Remote DMA over Con-
verged Ethernet (RoCE) [19]. RoCE signiﬁcantly re-
duces CPU utilization when compared with TCP.
USENIX Association  
24th USENIX Security Symposium  345
baseline system
baseline system under attack
protected system
protected system under attack
 1
 0.8
 0.6
 0.4
 0.2
 0
t
u
p
h
g
u
o
r
h
t
d
e
z
i
l
a
m
r
o
n
]
m
e
t
s
y
s
e
n
i
l
e
s
a
b
o
t
e
v
i
t
l
a
e
r
[
iperf
stream
[Mb/s]
apache
1MB
[req/s]
netperf RR
64B
[packets/s]
netperf RR
1024B
[packets/s]
memcached
[req/s]
apache
1KB
[req/s]
throughput oriented         
latency oriented     
Figure 9: VANFC performance evaluation results
]
s
/
b
G
[
t
e
a
r
r
e
f
s
n
a
r
t
5
4
3
2
1
0
 1  2  4  8  16  32  64 128 256 512 1K 2K 4K 8K 16K32K
]
s
/
b
G
[
e
t
a
r
r
e
f
s
n
a
r
t
5
4
3
2
1
0
transmit queue depth
1
2
4
8
16
32
64
128
256
 1  2  4  8  16  32  64 128 256 5121K 2K 4K 8K 16K32K
message size [KB]
(a)
message size [KB]
(b)
Figure 10: Performance of a single RoCE ﬂow in the system with two competing RoCE ﬂows. Graph (a) shows
performance with enabled ﬂow control; graph (b) shows performance with disabled ﬂow control.
A few recent studies that evaluate performance of dif-
ferent data transfer protocols over high speed links have
been published [48, 49, 67, 72]. Kissel et al. [49] com-
pare TCP and RoCE transfers over 40GbE links using
the same application they developed for benchmarking.
Using TCP, they managed to reach a speed of 22Gbps
while the sender’s CPU load was 100% and the receiver’s
CPU load was 91%. With OS-level optimizations, they
managed to reach a speed of 39.5 Gbps and reduce the
sender’s CPU load to 43%. Using the RoCE protocol,
they managed to reach 39.2 Gbps while the CPU load of
the receiver and sender was less than 2%! These results
clearly show that RoCE signiﬁcantly reduces CPU uti-
lization and thus the overall cost of carrying out compu-
tations. It is especially important when a large amount
of data is being moved between computational nodes
in HPC or data center environments, where virtualiza-
tion is becoming prevalent and increasing in popular-
ity [24, 37, 54].
Studies show that RoCE cannot function properly
without ﬂow control [48, 49, 67, 72]. Figure 10, taken
from Kissel et al. [49], with the authors’ explicit permis-
sion, shows the performance effect of ﬂow control on two
competing data transfers using the RoCE protocol. Fig-
ure 10a shows the performance of a single RoCE data
transfer while another RoCE data transfer is competing
with it for bandwidth and ﬂow control is enabled. Both
transfers effectively share link bandwidth. Figure 10b
shows the performance of the same RoCE data transfer
when ﬂow control is disabled. As can be seen in the ﬁg-
ure, without ﬂow control the RoCE data transfer suffers,
achieving a fraction of the performance shown in Fig-
ure 10a. We have also independently reproduced and
veriﬁed these results.
Kissel et al. also show [49] that the same problem is
relevant not only to RoCE but can be generalized to TCP
as well. Thus we conclude that disabling ﬂow control
would cause less effective resource utilization and lead to
higher cost for cloud customers and for any organization
deploying SRIOV. Conversely, securing SRIOV against
ﬂow control attacks would make it possible for SRIOV
and ﬂow control to coexist, providing the performance
346  24th USENIX Security Symposium 
USENIX Association
beneﬁts of both without relinquishing security.
9 Discussion
Notes on Implementation VANFC can be implemented
as part of an SRIOV device already equipped with an
embedded Ethernet switch or it can be implemented
in the edge Ethernet switch, by programming the edge
switch to ﬁlter ﬂow control frames from VFs’ MAC ad-
dresses. Adding VANFC functionality to the NIC requires
less manufacturing effort; it is also more convenient and
cheaper to replace a single NIC on a host than to replace
an edge switch. Nevertheless, in large-scale virtualiza-
tion deployments, such as those of cloud providers or
corporate virtual server farms, a single 10GbE Ethernet
switch with high port density (for example, the 48 port
HP 5900AF 10Gb Ethernet switch in our testbed) serves
many host servers with SRIOV capable devices. In such
scenarios, upgrading 48 SRIOV devices connected to the
48 port switch requires considerably more resources than
single switch upgrade.
Having said that, we argue that proper implementation
of the solution to the described problem is in the SRIOV
NIC and not in the edge Ethernet switch. The problem
we discuss is strictly related to the virtualization plat-
form and caused by a design ﬂaw in the SRIOV NIC’s
internal switching implementation. Mitigating the prob-
lem in the edge switch, an external device whose purpose
is not handle virtualization problems of the host, would
force the edge switch to learn about each VF’s MAC ad-
dress and to distinguish PFs from VFs, coupling the edge
switch too closely with the NICs.
VEB and VEPA Another important security aspect of
SRIOV is VM-to-VM trafﬁc. In SRIOV devices with an
embedded VEB switch, VM-to-VM trafﬁc does not leave
the host network device and is not visible to the external
edge switch, which enforces the security policy on the
edge of the network. To make all VM trafﬁc visible to the
external switch, the VEB switch should act as a VEPA
and send all VM trafﬁc to the adjacent switch.
A properly conﬁgured Ethernet switch and the use of
a VEPA device can enforce a security policy (ACL, port
security) on malicious VM trafﬁc and prevent most L2
attacks. However, while VEPA solves many manage-
ability and security issues that pertain to switching in
virtualized environments [29], it does not address the
ﬂow control attack we presented earlier. This is because
VEPA still shares the same single link between multi-
ple untrusted guests and the host and does not manage
ﬂow control per VF. Besides not solving the ﬂow control
attack, it uses, again, the edge Ethernet switch, which is
external to the source of the problem–SRIOV NIC. Thus,
a VEPA extension should not be considered for the so-
lution and the problem should be solved in the SRIOV
NIC.
10 Related Work
Several recent works discussed the security of self-
virtualizing devices. P´ek et al. [61] described a wide
range of attacks on host and tenant VMs using directly
assigned devices. They performed successful attacks on
PCI/PCIe conﬁguration space, on memory mapped I/O,
and by injecting interrupts. They also described an NMI
injection attack. Most of the attacks they discussed can
be blocked by a ﬁx in the hypervisor or by proper hard-
ware conﬁguration.
Richter et al. [68] showed how a malicious VM with
a directly attached VF can perform DoS attacks on other
VMs that share the same PCIe link by overloading its
own Memory Mapped I/O (MMIO) resources and ﬂood-
ing the PCIe link with write request packets. As the au-
thors mention, this attack can be mitigated by using the
QoS mechanisms deﬁned by the PCIe standard [59].
All of the attacks discussed in the aforementioned pa-