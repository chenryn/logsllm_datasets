6
7
8
9
10
11
12
configuration c′ over the frames in S, i.e., 1|S |
product of its F1 scores across all knobs, i.e.,
Algorithm 3: Finds the top-k best configurations for a
segment efficiently by profiling each knob independently
(a form of greedy hill climbing).
greedy hill climbing, where each knob is tuned while all other
knobs are held fixed, reducing the search space to O(mn).
Algorithm 3 shows how Chameleon profiles each knob in-
dependently. For each value vr of knob r, we construct a con-
figuration c(vr) with knob r set to vr while all other knobs
are set to their golden configuration (maximum) values. We
compare c(vr) to the golden configuration c∗, which sets r to
its most expensive value (lines 5-7). Eval_F1 computes the
average F1 score of a configuration c with respect to another
s∈S F1(s, c, c′).
The final accuracy of a configuration c = (v1, . . . , vn) is the
r KnobV alue
ToScore(vr), again following our independence assumption.
Based on this, Algorithm 3 returns the cheapest k config-
urations whose accuracy is higher than a given threshold
α′2. Note that the cost of a configuration c (line 10) may
not be available from the preceding code since each knob is
tuned independently, but it can be obtained from a one-time
offline profiling because c’s resource consumption is stable
regardless of the video frame it is run on, as others have also
observed [24, §6.2]).
2Since the accuracy estimates inevitably have some errors, we use a higher
value of α′ (by default, α′ = 0.5 + 0.5 · α where α is the real threshold used
in testing) to ensure the picked configuration yields an accuracy over α.
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
Comparing c(vr) against the golden configuration is costly,
but it ensures that a good set of promising configurations is
found. This is critical when profiling the full configuration
space on a group leader at the start of a profiling window
(line 5, Algorithm 2). Since this is done only once per profiling
window per video group, the higher cost can be amortized.
For all remaining video segments (line 7, Algorithm 2), the
cost is too high as it applies per segment, so Chameleon
sets all knobs other than r to lower default values in lines 5
and 6. The reason this works in practice is that the search
space has already been reduced to the top-k most promising
configurations, so finding a good relative ordering among
them is sufficient, and is doable with lower default values. As
long as the default values are not too low, the independence
assumption approximately holds (extreme settings may vio-
late the assumption, e.g., a very small image size makes even
relative comparisons difficult because no configuration de-
tects any objects). Note that Algorithm 3 must still compute
and threshold the final accuracy of each configuration.
Although line 4 loops over all values of a given knob, for
some knobs (e.g., frame rate, minimal area size), a lower
value has no profiling cost because it can be extrapolated
from higher values (e.g., simply ignore frames to evaluate a
lower frame rate). Also, since our knobs exhibit monoton-
ically increasing/decreasing performance, we can stop the
loop when performance is good enough (or bad enough, de-
pending on the search direction). We used the former but
not the latter optimization in our evaluation.
5.5 Practical considerations
We now cover two aspects that are critical in practice.
First, switching configurations in currently executing video
pipelines is non-trivial, because the modules have to be de-
signed to accept changes (e.g., resolution). We build on prior
work [32] where video modules constantly “listen” and pre-
pare for any configuration updates. For example, when the
configuration switches NN models, loading the new model
into memory takes time (e.g., up to 1 second). We must either
factor this switching duration in our formulation or rely on
pre-warming the memory with NN models.
Second, we believe it is best to use separate compute re-
sources for profiling (separate from the compute used by
the pipeline) to avoid disruptions to the live analytics. The
recent trend in “serverless computing” [3, 4] makes it simple
to run profiling tasks without explicitly provisioning VMs.
6 EVALUATION
We evaluate Chameleon on a dataset of video streams cap-
tured by multiple real-world traffic cameras over a duration
of 24 hours. Our key findings are the following.
• Chameleon achieves significantly better inference accu-
racy and lower resource consumption than a baseline of
profiling once upfront (§6.2).
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
J. Jiang et al.
(a) Bounding box-based F1-score (α = 0.8)
(b) Bounding box-based F1-score (α = 0.9)
(c) Average bounding box-based F1 score
(d) Label-based F1-score (α = 0.8)
(e) Label-based F1-score (α = 0.9)
(f) Average label-based F1 score
Figure 10: Chameleon (red) consistently outperforms the baseline of one-time update (blue) across different metrics in Pipeline A.
Each dot represents the results of running each solution on one hour of video from five concurrent video feeds. The graphs also
include 1-σ ellipses [20] to mark the performance variance of each solution.
• By leveraging the temporal persistence of configurations’
accuracy, Chameleon reduces profiling cost by focusing on
the top-k best configurations for most of the time (§6.3).
• By leveraging the spatial similarities across video cameras,
Chameleon amortizes the cost of profiling across similar
cameras with minimal reduction in accuracy (§6.4).
• Chameleon further cuts profiling cost by profiling inde-
pendent configuration knobs separately (§6.5).
We also used a different set of videos to generalize these re-
sults. The videos were taken from 10 cameras deployed in an
indoor cafeteria area (with prominently posted notices) over
a period of 3 days. The original MP4 videos are 1920×1080
in resolution and 25 fps in frame rate. Their content includes
different patterns of human movement, e.g., more people
moving before/after meal times than the rest of the day. To
obtain a representative dataset, we sampled 90 video clips, 9
clips from each camera, across the 3-day period.
encoded MP4 format (1280×960p resolution, 30fps frame rate,
and 150 seconds length) as the input to Chameleon.
6.1 Dataset and setup
We use a dataset of video streams from five traffic video cam-
eras deployed in different intersections in a metropolitan
area (Bellevue, WA). The exact content of the video feeds
varies significantly over time and across space. For instance,
day time has more objects and intermittent traffic congestion,
while at night object appearances are more spread out over
time. Spatially, as well, two cameras in downtown areas show
more cars at slower speeds than the other cameras deployed
in suburbs (Figure 7 shows two screenshots). In addition, all
cameras exhibit transient car motion patterns when traffic
lights change. Despite their heterogeneity in content, we
show that Chameleon can opportunistically leverage tempo-
ral persistence and spatial similarities between the cameras
to achieve efficient online configuration adaptation. To ob-
tain a representative dataset, we sampled 120 video clips
across 24 hours from each camera, and used their original
The video frames are streamed in chronological order to
the video analytics pipelines, whose configurations are con-
trolled by Chameleon. We used the following control knobs
(§2.1): for Pipeline A: we used 5 levels of frame rate, 5 levels
of image size, and 6 pre-trained object detection models [7];
for Pipeline B: we used 5 levels of minimum region size with
detected motion, and 4 pre-trained classifier models [8]. The
inference models are implemented in Tensorflow and are pre-
trained on standard image datasets [23], and the switching
of video frame rate and resolution is done by FFmpeg [10].
6.2 End-to-end improvement
We start with the end-to-end improvement of Chameleon
over the baseline of a one-time update (profiling configu-
rations once at the beginning of a video stream). Figure 10
 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0 0.2 0.4 0.6 0.8 1Avg GPU time per frame (sec)Frac. of frames with accurate resultOne-time profilingChameleon 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0 0.2 0.4 0.6 0.8 1Avg GPU time per frame (sec)Frac. of frames with accurate resultOne-time profilingChameleon 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0 0.2 0.4 0.6 0.8 1Avg GPU time per frame (sec)Avg F1 scoreOne-time profilingChameleon 0 0.05 0.1 0.15 0.2 0.25 0 0.2 0.4 0.6 0.8 1Avg GPU time per frame (sec)Frac. of frames with accurate resultOne-time profilingChameleon 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0 0.2 0.4 0.6 0.8 1Avg GPU time per frame (sec)Frac. of frames with accurate resultOne-time profilingChameleon 0 0.05 0.1 0.15 0.2 0.25 0 0.2 0.4 0.6 0.8 1Avg GPU time per frame (sec)Avg F1 scoreOne-time profilingChameleonChameleon: Scalable Adaptation of Video Analytics
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
(a) Bounding box-based F1-score
Figure 11: Chameleon (red) outperforms the baseline of one-
time update (blue) on Pipeline B (α=0.8).
(b) Label-based F1-score
Resource (avg. GPU time
in seconds per frame)
bounding box F1 score > 0.8)
Chameleon Baseline Improv. Chameleon Baseline Improv.
Accuracy (frac. frames w/
0.60
0.28
0.52
0.67
A 0.24
B 0.21
C 0.33
D 0.41
Table 1: Chameleon improves accuracy and resource consump-
tion on another dataset of indoor video cameras.
60.2%
24.6%
37.4%
38.6%
1%
119%
3%
2%
0.65
0.62
0.72
0.71
0.65
0.28
0.70
0.69
shows that for Pipeline A, Chameleon consistently outper-
forms the baseline along resource consumption and sev-
eral accuracy metrics for different values of the accuracy
threshold α. Specifically, we use bounding box-based accu-
racy, where we compute accuracy based on specific locations
of objects on the image, and also label-based accuracy, where
we only compare the set of objects on a frame (and ignore
locations). Note that the resource (i.e., GPU) consumption
includes both profiling of different configurations and run-
ning the best configuration to get inference results. The data
points are visually summarized with 1-σ ellipses, which show
their maximum-likelihood 2-D Gaussian distribution [20].
For most of the frames, Chameleon partitions the five cam-
eras into two groups, one with three cameras in the suburb
area, and one with two cameras in the downtown area.
We observe improvement on three fronts. (1) Chameleon
achieves 20-50% higher accuracy with the same resource con-
sumption, which suggests it could benefit resource-constrained
settings (e.g., edge or mobile devices). (2) Chameleon achieves
30-50% reduction in resource consumption (a 2-3x speed up)
while achieving almost the same accuracy as the baseline,
which could save capital costs when resources are elastic but
expensive (e.g., cloud VMs). (3) Finally, Chameleon not only
improves performance on average, but also reduces perfor-
mance variance: Chameleon’s 1-σ ellipses3 are remarkably
smaller than those of the baseline. This is because Chameleon
continuously adjusts its configuration over time, whereas
the baseline is sensitive to the starting points of the video.
3In many cases, the baseline selected the expensive golden configuration
(top right corner of the graphs), causing the ellipses to shift.
(a) Impact of profile window length
(b) Impact of top k
Figure 12: Impact of key parameters in temporal incremental
update. The accuracy metric is the label-based F1 score with
accuracy threshold α = 0.8.
Notice that Chameleon still has non-trivial room for improve-
ment compared to the optimal (idealized) performance, i.e.,
periodic updating with zero profiling cost (Figure 4).
We observed similar results in Pipeline B. Figure 11 com-
pares Chameleon and the baseline on two accuracy metrics
in Pipeline B. Chameleon gives accurate results (F1 score
over 0.8) on over 90% of the frames, while the baseline suf-
fers from higher resource consumption or low accuracy, and
substantial performance variance.
To generalize our end-to-end evaluation, Table 1 shows
the improvement of Chameleon on another dataset of 10
camera feeds (see §6.1). Based on the cameras’ geographi-
cal proximity, the cameras are partitioned into four groups
(rows). The table compares Chameleon with a baseline that
uses one-time profiling on each camera feed. We see that,
for three groups (A, B, D), Chameleon reduces the GPU con-
sumption by 37.4% to 60.2% without sacrificing accuracy,
and in the other group (C), Chameleon increases accuracy
by 119% while still managing to reduce GPU usage by 24.6%.
6.3 Impact of temporal incremental updates
Next, we microbenchmark the impact of individual compo-
nents in Chameleon. We start with temporal incremental
updates (§4.1), and investigate two of its key parameters:
the profile window size and the size of the top-k set. First,
we fix the parameters of Chameleon and only change the
profile window size to see the impact of updating the top-k
configurations less often. Figure 12a confirms our intuition
(§4.1): when we start increasing the profiling window size
from one segment (the top right corner), we see a fast drop in
profiling cost (the gap between the two curves) relative to the
degradation in accuracy, until some “knee point” (around 3-5
segments) after which further increases bring diminishing
savings in cost while reducing accuracy.
Another key factor in temporal incremental updates is the
size of the top-k configuration set. Intuitively, using a larger
set introduces more overhead to check the set’s accuracy in
each segment, but tolerates more temporal variance because
the best configuration is more likely to be found within
the set. Figure 12b quantifies this tradeoff as we gradually
increase k from 1 (bottom left) to 15 (top right). We observe a
 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1Avg GPU time per frame (sec)Frac. of frames with accurate resultOne-time profilingChameleon 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1Avg GPU time per frame (sec)Frac. of frames with accurate resultOne-time profilingChameleon 0 0.2 0.4 0.6 0.8 1 1.2 0.65 0.7 0.75 0.8 0.85Avg GPU time per frame (sec)Frac. of frames with accurate resultInference + Profiling CostInference CostT=1	segmentsT=8	segments 0 0.1 0.2 0.3 0.4 0.5 0.6 0.76 0.78 0.8 0.82 0.84 0.86Avg GPU time per frame (sec)Frac. of frames with accurate resultInference + Profiling CostInference Costk=1k=15SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
J. Jiang et al.
(a) More similar cameras
(b) More cameras, not all similar
Figure 13: The benefit of having more cameras.
good balance at around k = 5, below which top-k seems not
able to tolerate transient temporal variance (i.e., accuracy
degrades), and above which the increased cost of checking
the top-k set’s accuracy does not yield much benefit.
6.4 Impact of cross-video inference
The second insight behind Chameleon is the existence of
multiple similar cameras over which we can amortize the
profiling cost. Figure 13 shows the benefits of having more
cameras in two cases: when the cameras are similar, and
when the cameras are not all similar. Naturally, in the former
case we expect the cameras to share the same set of good
configurations, so the cost of profiling should drop almost