# Title: TouchTrack: How Unique are Your Touch Gestures?

## Authors:
- Rahat Masood, University of New South Wales (UNSW), Sydney, Australia
- Benjamin Zi Hao Zhao, CSIRO Data61, Sydney, Australia
- Hassan Jameel Asghar, CSIRO Data61, Sydney, Australia
- Mohamed Ali Kâafar, CSIRO Data61, Sydney, Australia

## Abstract
This paper investigates a privacy threat posed by the collection and monitoring of touch gestures on touchscreen devices. We introduce a new form of persistent tracking, termed "touch-based tracking," which extends beyond virtual identities and has the potential for cross-device tracking and identifying multiple users on the same device. To demonstrate the feasibility of touch-based tracking, we propose an information-theoretic method that quantifies the amount of information revealed by individual gesture features, samples of gestures, and combinations of gestures when modeled as feature vectors. We have developed a purpose-built app, named "TouchTrack," which collects data from users and informs them about the uniqueness of their touch interactions. Our results from 89 different users indicate that writing samples and left swipes can reveal 73.7% and 68.6% of user information, respectively. Combining different gesture types, such as keystrokes, swipes, and writing, reveals up to 98.5% of user information. We successfully re-identified returning users with over 90% accuracy.

## Keywords
Touch-based Tracking, Mobile Privacy, Behavioral Biometrics, Touch Gestures

## 1. Introduction
In this paper, we argue that the distinctiveness of touch-based gestures poses a significant privacy threat, enabling a new form of continuous and surreptitious tracking of individuals. This concept, which we term "touch-based tracking," involves observing, tracking, and distinguishing users through their touch gestures while they interact with touchscreen devices.

Unlike traditional tracking mechanisms (e.g., cookies, browser fingerprints) that track virtual identities, touch-based tracking is more subtle and riskier because it allows the identification of the actual physical person operating the device. This form of tracking can also lead to cross-device tracking, where the same user can be traced across multiple mobile devices. Additionally, it can distinguish and track multiple users accessing the same device.

While not all use cases of touch-based tracking are negative, it can provide a more complete view of a user's behavior and be used for various purposes, including targeted ads, profiling, and spamming. However, it can also be beneficial, such as in identifying multiple users on the same device to provide more personalized content.

Our main contributions are as follows:

1. **Investigation of Touch-Based Tracking**: We explore the potential of using touch-based gestures for tracking.
2. **Analytical Framework**: We develop an analytical framework that measures the amount of identifying information (in bits) contained in touch gestures at different levels of granularity. 
   - At the finest level, our framework quantifies the information carried by individual features, such as screen pressure.
   - At the second level, it quantifies the information carried by a single gesture sample, such as a swipe.
   - At the third level, it calculates the information carried by multiple samples of the same gesture.
   - Finally, it measures the information carried by a combination of samples from multiple gestures.
3. **Application and Validation**: We apply our framework to four widely used touchscreen gestures: swipes, taps, keystrokes, and handwriting. We develop and deploy a "game-like" Android app called "TouchTrack," which includes three well-known open-source games and a purpose-built handwriting module. We test our framework on 40,600 gesture samples collected from 89 participants and identify features that contain a high amount of identifying information using the maximum-relevancy minimum-redundancy (mRMR) algorithm [4].
4. **Results and Findings**: Our results show that 50 features in a single handwriting sample contribute 68.71% of user information, increasing to 73.7% with multiple samples. Combining different gestures, such as swipes, handwriting, and keystrokes, reveals up to 98.5% of user information. For users who performed all four gestures, our framework showed 98.89% of user information. We also validated our framework in terms of correctly identifying returning users, achieving a true positive rate (TPR) of 90.0% and 91.0% for swipes and handwriting, respectively.

## Acknowledgments
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).

CCS '17, October 30-November 3, 2017, Dallas, TX, USA
© 2017 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-4946-8/17/10.
https://doi.org/10.1145/3133956.3138850