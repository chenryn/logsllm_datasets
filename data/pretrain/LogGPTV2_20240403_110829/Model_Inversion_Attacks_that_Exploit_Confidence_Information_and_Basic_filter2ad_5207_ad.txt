deep neural network architectures, so on its own this classi-
ﬁer can be seen as a neural network with no hidden layers.
Multilayer perceptron network. We use a multilayer
perceptron network with one hidden layer of 3000 sigmoid
neurons (or units) and a softmax output layer. This clas-
siﬁer can be understood as performing softmax regression
after ﬁrst applying a non-linear transformation to the fea-
ture vector. The point of this transformation, which corre-
sponds to the hidden layer, is to map the feature vector into
a lower-dimensional space in which the classes are separable
by the softmax output layer.
Stacked denoising autoencoder network. This classi-
ﬁer is an example of a deep architecture, and consists of two
hidden layers and one softmax output layer. The two hid-
den layers, which have 1000 and 300 sigmoid units, are each
instances of a denoising autoencoder. An autoencoder is a
neural network that maps its feature vector to a latent rep-
1328c(x) def= 1 − ˜flabel (x) + AuxTerm(x)
x0 ← 0
for i ← 1 . . . α do
Algorithm 1 Inversion attack for facial recognition models.
1: function MI-Face(label , α, β, γ, λ)
2:
3:
4:
5:
6:
7:
8:
9:
10:
xi ← Process(xi−1 − λ · ∇c(xi−1))
if c(xi) ≥ max(c(xi−1), . . . , c(xi−β)) then
if c(xi) ≤ γ then
(c(xi)), minxi (c(xi))]
return [arg minxi
break
break
resentation (typically in a smaller space), and then maps it
back (i.e., reconstructs) into the original feature space. Au-
toencoders are trained to minimize reconstruction error, so
vectors in the latent space can be thought of as compressed
encodings of the feature space that should decode well for in-
stances in the training data. The hope with this architecture
is that these encodings capture the main factors of variation
in feature space (much like Principal Component Analysis),
leading to greater separability for the softmax layer.
Error
7.5%
4.2%
3.3%
Model
Softmax
MLP
DAE
Dataset. We trained each type
of model over the AT&T Lab-
oratories Cambridge database of
faces [2]. This set contains ten
black-and-white images of 40 indi-
viduals in various lighting condi-
tions, facial expressions, and de-
tails (e.g., glasses/no glasses), for
a total of 400 images. We divided
the images of each person into a training set (7 images) and
a validation set (3 images), and trained each model using
pylearn2’s stochastic gradient descent algorithm [15] until
the model’s performance on the training set failed to im-
prove after 100 iterations. The error rate for each model is
given in Figure 6.
Figure 6: Model
accuracy.
Basic MI attack. We now turn to inversion attacks against
the models described above. The features that we will at-
tempt to invert in this case are the full vector of pixel intensi-
ties that comprise an image, and each intensity corresponds
to a ﬂoating-point value in the range [0, 1]. In all of the at-
tacks we consider, we do not assume that the attacker knows
exact values for any of the pixels in the vector he is trying to
infer. These factors combine to make this type of inversion
substantially diﬀerent from the previous cases we consider,
so these attacks require new techniques.
Assuming feature vectors with n components and m face
classes, we model each facial recognition classiﬁer as a func-
tion, ˜f : [0, 1]n (cid:55)→ [0, 1]m. Recall that the output of the
model is a vector of probability values, with the ith compo-
nent corresponding to the probability that the feature vector
belongs to the ith class. We write ˜fi(x) as shorthand for the
ith component of the output.
We use gradient descent (GD) to minimize a cost func-
tion involving ˜f to perform model inversion in this setting.
Gradient descent ﬁnds the local minima of a diﬀerentiable
function by iteratively transforming a candidate solution to-
wards the negative of the gradient at the candidate solution.
Our algorithm is given by the function MI-Face in Algo-
rithm 1. The algorithm ﬁrst deﬁnes a cost function c in
Algorithm 2 Processing function for stacked DAE.
function Process-DAE(x)
encoder .Decode(x)
x ← NLMeansDenoise(x)
x ← Sharpen(x)
return encoder .Encode(vecx)
Figure 7: Reconstruction without using Process-
DAE (Algorithm 2) (left), with it (center), and the
training set image (right).
terms of the facial recognition model ˜f and a case-speciﬁc
function AuxTerm, which uses any available auxiliary in-
formation to inform the cost function. We will describe an
instantiation of AuxTerm when we discuss facial deblur-
ring. MI-Face then applies gradient descent for up to α
iterations, using gradient steps of size λ. After each step of
gradient descent, the resulting feature vector is given to a
post-processing function Process, which can perform vari-
ous image manipulations such as denoising and sharpening,
as necessary for a given attack. If the cost of the candidate
fails to improve in β iterations, or if the cost is at least as
great as γ, then descent terminates and the best candidate
is returned.
MI-Face needs to compute the gradient of the cost func-
tion c, which in turn requires computing the gradient of the
facial recognition model ˜f . This means that ˜f must be dif-
ferentiable for the attack to succeed. ∇ ˜f can be computed
manually or automatically using symbolic techniques. Our
experiments use symbolic facilities to implement the latter
approach.
5.2 Reconstruction Attack
The ﬁrst speciﬁc attack that we consider, which we will re-
fer to as Face-Rec, supposes that the adversary knows one of
the labels output by the model and wishes to reconstruct an
image depicting a recognizable face for the individual corre-
sponding to the label. This attack is a fairly straightforward
instantiation of MI-Face (Algorithm 1). The attacker has
no auxiliary information aside from the target label, so we
deﬁne AuxTerm(x) def= 0 for all x. Our experiments set the
parameters for MI-Face to: α = 5000, β = 100, γ = 0.99,
and λ = 0.1; we arrived at these values based on our ex-
perience running the algorithm on test data, and out of
consideration for the resources needed to run a full set of
experiments with these parameters.
In all cases except for the stacked DAE network, we set
Process to be the identity function. For stacked DAE net-
work, we use the function Process-DAE in Algorithm 2.
Since the adversary can inspect each of the model’s layers,
he can isolate the two autoencoder layers. We conﬁgure the
attack to generate candidate solutions in the latent space of
1329the ﬁrst autoencoder layer, and at each iteration, Process-
DAE decodes the candidate into the original pixel space,
applies a denoising ﬁlter followed by a sharpening ﬁlter, and
re-encodes the resulting pixels into the latent space. We
ﬁnd that this processing removes a substantial amount of
noise from the ﬁnal reconstruction, while enhancing recog-
nizable features and bringing relative pixel intensities closer
to the images from the training set. An example is shown
in Figure 7. The image on the left was reconstructed using
the label of the individual from the right image without the
use of Process-DAE, while the center picture was recon-
structed using this processing step.
Experiments. To evaluate the eﬀectiveness of the attack,
we ran it on each of the 40 labels in the AT&T Face Database,
and asked Mechanical Turk workers to match the recon-
structed image to one of ﬁve face images from the AT&T
set, or to respond that the displayed image does not corre-
spond to one of the ﬁve images. Each batch of experiments
was run three times, with the same test images shown to
workers in each run. This allowed us to study the consis-
tency of responses across workers. The images shown to
Turk workers were taken from the validation set, and thus
were not part of the training data.
In 80% of the experiments, one of the ﬁve images con-
tained the individual corresponding to the label used in the
attack. As a control, 10% of the instances used a plain im-
age from the data set rather than one produced by MI-Face.
This allowed us to gauge the baseline ability of the workers
at matching faces from the training set. In all cases, the im-
ages not corresponding to the attack label were selected at
random from the training set. Workers were paid $0.08 for
each task that they completed, and given a $0.05 bonus if
they answered the question correctly. We found that work-
ers were usually able to provide a response in less than 40
seconds. They were allowed to complete at most three tasks
for a given experiment. As a safeguard against random or
careless responses, we only hired workers who have com-
pleted at least 1,000 jobs on Mechanical Turk and achieved
at least a 95% approval rating.
5.2.1 Performance
We ran the attack for
each model on an 8-
core Xeon machine with
16G memory. The re-
sults are shown in Fig-
ure 8. Reconstructing
faces out of the softmax model is very eﬃcient, taking only
1.4 seconds on average and requiring 5.6 epochs (i.e., itera-
tions) of gradient descent. MLP takes substantially longer,
requiring about 21 minutes to complete and on the order
of 3000 epochs of gradient descent. DAE requires less time
(about 11 minutes) but a greater number of epochs. This is
due to the fact that the search takes place in the latent fea-
ture space of the ﬁrst autoencoder layer. Because this has
fewer units than the visible layer of our MLP architecture,
each epoch takes less time to complete.
5.2.2 Accuracy results
The main accuracy results are shown in Figure 9.
algorithm time (s)
Softmax
MLP
DAE
In
this ﬁgure, overall refers to all correct responses, i.e., the
worker selected the image corresponding to the individual
targeted in the attack when present, and otherwise selected
Figure 8: Attack runtime.
1.4
1298.7
692.5
epochs
5.6
3096.3
4728.5
“Not Present”. Identiﬁed refers to instances where the tar-
geted individual was displayed among the test images, and
the worker identiﬁed the correct image. Excluded refers to
instances where the targeted individual was not displayed,
and the worker correctly responded “Not Present”.
Figure 9a gives results averaged over all responses, whereas
9b only counts an instance as correct when a majority (at
least two out of three) users responded correctly. In both
cases, Softmax produced the best reconstructions, yielding
75% overall accuracy and up to an 87% identiﬁcation rate.
This is not surprising when one examines the reconstruction
produced by the three algorithms, as shown in Figure 10.
The reconstruction given by Softmax generally has sharp,
recognizable features, whereas MLP produces a faint out-
line of these features, and DAE produces an image that is
considerably more blurry.
In all cases, the attack signiﬁ-
cantly outperforms, by at least a factor of two, randomly
guessing from the six choices as this would give accuracy
just under 20%.
5.3 Black-Box Attacks
It may be possible in some cases to use numeric approxi-
mations for the gradient function in place of the explicit gra-
dient computation used above. This would allow a black-box
adversary for both types of attack on facial recognition mod-
els. We implemented this approach using scipy’s numeric
gradient approximation, and found that it worked well for
Softmax models—the reconstructed images look identical to
those produced using explicit gradients. Predictably, how-
ever, performance suﬀers. While Softmax only takes about a
minute to complete on average, MLP and DAE models take
signiﬁcantly longer. Each numeric gradient approximation
requires on the order of 2d black-box calls to the cost func-
tion, each of which takes approximately 70 milliseconds to
complete. At this rate, a single MLP or DAE experiment
would take 50–80 days to complete. Finding ways to opti-
mize the attack using approximate gradients is interesting
future work.
6. COUNTERMEASURES
In this section, we explore some potential avenues for
developing eﬀective countermeasures against these attacks.
Ideally one would develop full-ﬂedged countermeasures for
which it can be argued that no future MI attack will suc-
ceed. Here we do a more limited preliminary investigation,
showing discussing simple heuristics that prevent our cur-
rent attacks and which might guide future countermeasure
design.
Decision Trees. Just as the ordering of features in a de-
cision tree is crucial for accuracy, it may also be relevant to
the tree’s susceptibility to inversion attacks. In particular,
the level at which the sensitive feature occurs may aﬀect the
accuracy of the attack. To test this hypothesis, we imple-
mented a variant of CART learning that takes a parameter
(cid:96) which speciﬁes the priority at which the sensitive feature is
considered: the feature is only considered for splitting after
(cid:96)− 1 other features have already been selected, and removed
from consideration afterwards. We then sampled 90% of the
FiveThirtyEight dataset 100 times, and used this algorithm
to train a decision tree on each sample for every value of (cid:96).
We evaluated the classiﬁcation accuracy of the tree alongside
white-box inversion performance.
1330100
80
60
40
20
t
c
e
r
r
o
c
%
Softmax
MLP
DAE
100
80
60
40
20
100
80
60
40
20
overall
identiﬁed
excluded
overall