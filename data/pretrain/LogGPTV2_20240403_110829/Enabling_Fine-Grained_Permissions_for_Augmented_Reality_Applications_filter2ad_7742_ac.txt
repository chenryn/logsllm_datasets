rameters, to check if any new objects have been rec-
ognized. If so, the recognizer will return data that
the multiplexer will then package in an event data
structure and pass to all subscribed applications. We
plan to implement a more eﬃcient interrupt-driven
multiplexer in the future.
The next two API calls are optional. The fil-
ter call allows the multiplexer to tell the recog-
nizer that only a speciﬁc subset of the raw inputs
should be used for recognition. For example, only a
sub-rectangle of the video frame should be consid-
ered for a face detector. Finally, cache_compare is
a recognizer-speciﬁc comparator function that takes
two sets of recognizer inputs and determines whether
they are considered equal. The multiplexer uses this
comparator to implement per-recognizer caching.
For example, the multiplexer may pass the previ-
ous and current RGB frames to the cache_compare
function of the face recognizer and potentially avoid
a recomputation of the face model if the two frames
have not suﬃciently changed.
Our multiplexer and recognizers consisted of
about 3,000 lines of C++ code. We wrote a total of
nine recognizers, which we summarize in Figure 11.
targeting
Application support. Applications
our multiplexer run in separate Windows processes.
Each application links against the ARLib library we
have built. ARLib communicates with the multi-
plexer over local sockets and handles marshaling and
unmarshaling of recognizer event data. By calling
ARLib functions, an application can request access
to speciﬁc recognizers and register callbacks to han-
dle recognizer events. ARLib provides two kinds
of interfaces: a low-level interface for applications
written in C++ and higher-level wrappers for .NET
applications written in C# or other managed lan-
guages. ARLib consists of about 500 lines of C++
code and 400 lines of C# code.
Sample code in Figure 12 shows a part of a test
application we wrote that detects faces and draws
pictures on the screen which follow face movements.
The application connects to the multiplexer and sub-
scribes to face recognizer events. In our implementa-
tion, these events contain approximately 100 points
corresponding to diﬀerent parts of the face, or 0
points if a face is not present. The application han-
dles these events in the ProcessFace callback by
checking if a face is present and calling a separate
function (not shown) that updates the display.
In addition to face visualization, we ported a few
other sample applications bundled with the Kinect
SDK to our system. These included a skeleton vi-
sualizer and raw RGB and depth visualizers. We
found the porting eﬀort to be modest, aided in part
by the fact that we modeled our event data formats
on existing Kinect SDK APIs. In each case, we only
changed a handful of lines dealing with event sub-
scription. We additionally wrote two applications
from scratch: a 500-line C++ application that trans-
lates hand gestures into mouse cursor movements,
and a 300-line C# application that uses face recog-
nition to annotate people with their names. Overall,
we found our multiplexer interface simple and intu-
itive to use for building AR applications.
5 Evaluation
We ﬁrst evaluate how recognizers are used by an
analysis of 87 shipping AR applications and users’
mental models of AR applications. A survey of 462
respondents shows that users expect AR applica-
tions to have limited access to raw data. Fur-
thermore, no shipping application needs continuous
RGB access, and in fact a set of four recognizers is
suﬃcient for almost all applications. For these “core”
recognizers, we design privacy goggles visualizations
and evaluate how well users understand them. Next,
we look at how the OS can mitigate recognizer er-
422  22nd USENIX Security Symposium 
USENIX Association
8
Recognizer
Input dependencies
Output
RGB
Depth
Skeleton
Hand
FaceDetect
PersonTexture
Plane
FaceRecognize
CameraMotion
Kinect
Kinect
Kinect
Skeleton
RGB
Depth, Skeleton
RGB, Depth
RGB, FaceDetect
Kinect
RGB camera frames
Depth camera frames
Computed skeleton model(s)
Hand positions
2D face models for faces in current view
Depth “cutout” of a person
3D polygon coordinates constructed with KinectFusion (see Section 5.3)
Name of person in current view (see Section 5.3)
Camera movements detected using an accelerometer/gyro
Figure 11: The nine recognizers implemented by our multiplexer. A “Kinect” input dependency means that the
recognizer obtains data directly from the Kinect rather than other recognizers.
rors once an application has access to recognizers.
Finally, we show that our abstraction enables perfor-
mance improvements, making this a rare case when
improved privacy leads to improved performance.
5.1 Recognizers
Core Recognizers. We analyzed 87 AR applica-
tions on the Xbox Kinect platform,
including all
applications sold on Amazon.com. We focused on
Kinect because it is widely adopted and sits in a
user’s home. For each application, we manually re-
viewed their functionality, either through reading re-
views or by using the application. From this, we
extracted “recognizers” that would be suﬃcient to
support the application’s functionality.
Figure 13 shows the results. Four core recogniz-
ers are suﬃcient to support around 89% of ship-
ping AR applications. The set consists of skeleton
tracking, hand position, person texture, and keyword
voice commands. Person texture reveals a portion of
RGB video around a person detected through skele-
ton tracking, but with the image blurred or other-
wise transformed to hide all details. Fitness appli-
cations, in particular, use person texture when in-
structing the user on proper form.
After the core set, there is a “long tail” of seven
recognizers. For example, the Alvin and the Chip-
munks game uses voice modulation to “Alvin-ize”
the player’s voice, and NBA Baller Beats actually
tracks the location of a basketball to check that the
player dribbles in time to music. None of the ap-
plications in our set, however, require continuous
access to RGB data.
Instead, applications take a
short video or photo of the player so that she can
share how silly she looks with friends; this could be
handled via user-driven access control [27]. Only 3
applications require audio access beyond voice com-
mand triggers. There is plenty of room to improve
privacy with least privilege enabled by the recognizer
abstraction.
Privacy Expectations for Applications. To
Recognizer
% Apps
Skeleton
Person Texture (PT)
Voice Commands (VC)
Hand Position (HP)
Video Clip
Picture Snap
Voice Intensity
Voice Modulation
Speaker Recognition
Sound Recognition
Basketball Tracking
Skeleton+PT+VC
Skeleton+PT+VC+HP
94.3%
25.3%
3.44%
5.74%
3.4%
1.1%
1.1%
1.1%
1.1%
1.1%
1.1%
82.75%
89.65%
Figure 13: Analysis of all recognizers used by 87 ship-
ping Xbox applications. For each recognizer, we show
what percentage of apps use that recognizer (and possi-
bly others). We also show two sets of recognizers, and
for each set, the percentage of apps that use recogniz-
ers in this set and no others. A set of four recognizers
covers 89.65% of all applications. No application needs
continuous raw RGB access, and only 3 need audio access
beyond voice commands.
learn users’ mental models of AR application capa-
bilities, we showed 462 survey respondents a video
of a Kinect “foot piano” application in action: the
Kinect tracks foot positions and plays music. We
then asked about the capabilities of the application.
Figure 17(A) shows the results. Over 86% of all users
responded that the application could see the foot po-
sitions, while a much smaller number believed this
application had other capabilities. Overall, users ex-
pect applications will not see the entire raw sensor
stream.
Privacy Goggles for Core Recognizers. As we
discussed in Section 3, every recognizer must im-
plement a visualization method to enable the pri-
vacy goggles view. The OS uses these visualizations
to display to the user what information is obtained
by each application. We developed privacy goggles
visualizations for three of the four core recogniz-
ers:
skeleton, hand position, and person texture.
While voice commands are also a core recognizer,
USENIX Association  
22nd USENIX Security Symposium  423
9
424  22nd USENIX Security Symposium 
USENIX Association
Figure14:Examplesurveyquestionforprivacygog-gles.Anembeddedwarningvideoshowstwoviews:therawvideoontheright,andwhattheapplicationwillseeontheleft.Surveyrespondentswatchedthewarn-ingvideo,thenansweredquestionsaboutwhattheappcouldorcouldnotdoafterinstallation.Outof152re-spondents,80%correctlyidentiﬁedthattheappcouldseebodyposition,and47%correctlydeterminedtheappcouldseehandpositions.Figure15:Examplesurveyonrelativesensitivity.Re-spondentsindicatedwhichpictureismoresensitive:the“raw”RGBvideoframeoranimageshowingonlytheoutputofafacedetector.Outof50respondents,86%indicatedtherawimagewasmoresensitive.wedecidedtofocusﬁrstonthevisualrecognizersandleavevisualizationofvoicecommandsforfuturework.PrivacyAttitudesforCoreRecognizers.Wethenconductedsurveystomeasuretherelativesen-sitivityoftheinformationreleasedbythecorerecog-nizers.Wealsoaddedthe“facedetector”recognizer,becauseintuitivelythefaceisprivateinformation,anda“Raw”videorecognizerthatrepresentsgivingallinformationtotheapplication.Foreachpairofrecognizers,weshowedavisualizationfromthesameunderlyingvideoframe,thenaskedtheparticipanttostatewhichpicturewas“moresensitive”andwhy.Figure15showsanexamplecomparingrawRGBandfacedetectorrecognizers.Foreachpairofrecognizers,weasked50peopletoratewhichpicturecontainedinformationthatwas“moresensitive.”Figure16showstheresults.Intotalwehad500surveyrespondents,allfromtheUnitedStates.Asexpected,respondentsﬁndthattherawRGBframeismoresensitivethananyotherRecognizersLeftmore95%LeftRightsensitiveCIRawFace86%±9.6%RawSkeleton78%±11.48%RawTexture88%±9.01%RawHand88%±9.01%TextureSkeleton82%±10.65%TextureFace35%±13.22%TextureHand84%±10.16%SkeletonFace24%±11.84%SkeletonHand84%±10.16%HandFace22%±11.48%Figure16:Resultsfromrelativesensitivitysurveys.Userswereshowntwopictures,onefromeachrecognizer,hereshownasthe“left”andthe“right”recognizer.Thetablereportswhichpicturerespondentsthoughtcon-tained“moresensitive”informationandthe95%con-ﬁdenceinterval.Forexample,intheﬁrstline,86%ofpeoplethoughtthattheviewfromthe“Raw”RGBrec-ognizerwasmoresensitivethantheviewfromafacedetector,witha95%conﬁdenceintervalof±9.6%.recognizer.Basedontheresponses,wecanorderrecognizersfrom“mostsensitive”to“leastsensitive”,asfollows:Raw,Face,PersonTexture,Skeleton,andﬁnallytheleastsensitiveisHandPosition.EﬀectivenessofPrivacyGoggles.Finally,weevaluatedwhetherour“privacygoggles”visualiza-tionssuccessfullycommunicatethecapabilitiesofapplications.Wecreatedthreesurveys,oneforeachoftheskeleton,persontexture,andhandrecogniz-ers.Wehadatleast150respondentstoeachsurvey,withatotalof462respondents.Oursurveysarein-spiredbyFeltetal.’sAndroidpermission“quiz.”[11]Weshowedashortvideoclipoftheprivacygog-glesvisualizationforthetargetrecognizer.Figure14showsanexamplefortheskeletonrecognizer.TherighthalfshowstherawRGBvideoofapersonwrit-ingonawhiteboardandhandlingasmallceramiccatﬁgurine.Thelefthalfshowsthe“application-eyeview”showingthedetectedskeleton.Wethenaskeduserswhattheybelievedthecapabilitiesoftheappli-cationwouldbeifinstalled.Figure17showsthere-sults,withacheckmarknexttocorrectanswers.Weseethatalargenumberofrespondents(over80%)pickedthecorrectresultandrelativelyfewpickedin-correctresults.Thisshowsthatprivacygogglesareeﬀectiveatcommunicatingapplicationcapabilitiestotheuser.RespondentDemographics.Oursurveypartic-ipantswererecruitedfromtheU.S.throughuS-ample[30],aprofessionalsurveyservice,viatheInstant.lywebsite.Wedidnotspecifyanyre-strictionsondemographicstorecruit.AsreportedbyuSample,participantsare66%femaleand33%10USENIX Association  
22nd USENIX Security Symposium  425
A.FootPiano(462respondents)Seemybodyposition76(16%)Seemyfootpositions(cid:31)400(86%)SeewhatIlooklike28(6%)Seetheentirevideo52(11%)Learnmyheartrate21(4%)Noneoftheabove20(4%)Idon’tknow20(4%)B.Skeleton(152respondents)SeewhatIlooklike17(11%)Seemybodyposition(cid:31)122(80%)Seemylocation24(16%)Readthecontentsofthewhiteboard14(9%)SendpremiumSMSmessagesonmybehalf4(3%)Trackthepositionofmyhands(cid:31)71(47%)Noneoftheabove4(3%)Idon’tknow1(1%)C.PersonTexture(156respondents)SeewhatIlooklike36(23%)Seemybodyposition(cid:31)137(88%)Seemylocation25(16%)Seetheceramiccat19(12%)Readthecontentsofthewhiteboard5(3%)SendpremiumSMSmessagesonmybehalf0(0%)Trackthepositionofmyhands(cid:31)60(38%)Noneoftheabove2(1%)Idon’tknow5(3%)D.HandPosition(154respondents)SeewhatIlooklike17(11%)Seemybodyposition32(21%)Seemylocation14(9%)Seetheceramiccat12(8%)Readthecontentsofthewhiteboard7(5%)SendpremiumSMSmessagesonmybehalf2(1%)Trackthepositionofmyhands(cid:31)125(81%)Noneoftheabove3(2%)Idon’tknow4(3%)Figure17:Resultsfromprivacygoggleseﬀectivenesssurveys.Foreachofourthreecorerecognizers,weﬁrstaskedrespondentstoanswerquestionsaboutthecapabilitiesofaKinect“footpiano”applicationbasedonashortvideooftheapplicationinuse(A).Wenextshowedaprivacygoggles“permissionwarningvideo”andaskedquestionsaboutwhattheapplicationcoulddoifinstalled(B-D).male,with10.2%inthe0–22agerange,12.9%22–26,21.2%26–34,16.8%34–42,13.5%42–50,15.1%50–60,8.1%60–70,and1.8%70orolder.HumanEthicsStatement.Ourexperimentsin-cludesurveysofanonymoushumanparticipants.OurinstitutiondoesnothaveanInstitutionalRe-viewBoard(IRB),butitdoeshaveadedicatedteamwhosefocusisprivacyandhumanprotection.Thisteamhaspre-approvedsurveyparticipantvendorstoensurethattheyhaveprivacypolicieswhichpro-tectparticipants.Wefollowedtheguidelinesofthisteaminchoosingoursurveyvendor.Wealsodis-cussedoursurveyswithamemberoftheteamtoensurethatourquestionsdidnotaskforpersonallyidentiﬁableinformation,thattheywerenotoverlyintrusive,andthatnootherissueswerepresent.5.2NoisyPermissionsWhileprivacygogglesareeﬀectiveatcommunicat-ingwhatanappshouldandshouldnotseetotheuser,therecognizersweusecanhavefalsepositives.Thesecouldleakinformationtoapplications.Weﬁrstevaluatedarepresentativesetofrecognizersonwell-knownvisiondatasetstoquantifytheprob-lem.Next,weevaluatedOS-levelmitigationsforfalsepositives.RecognizerAccuracy.Wepickedthreewell-knowndatasetsforourevaluations:(1)aBerkeleydatasetconsistingofpicturesofobjects,(2)anIN-RIAdatasetcontainingpicturesofatalkinghead,and(3)asetofpicturesofafaceturningtowardtheFigure19:Recognizercombinationinaction.TheleftﬁgureshowsresultsofrunningafacedetectoronarawRGBvideoframe.Twofacesaredetected,butonlyonebelongstoarealperson.Ontheright,facedetectionisrunaftercombiningRGBanddepth.Onlytherealpersonisdetected.cameraandthenaway.Wethenevaluatedbaselinefalsepositiveandfalsenegativeratesforsevenob-jectrecognitionalgorithmscontainedinthewidelyadoptedOpenCVlibrary.Allsevenhadfalseposi-tivesonatleastoneofthedatasets.InputMassaging.Wethenimplementedpre-permissionblurring,inwhichframesareputthroughablurringprocessusingaboxﬁlterbeforebeingpassedtothefacedetectionalgorithm.Weuseda12×12boxﬁlter.Wealsousedframesubtractionasaheuristictosuppressrecognizerfalsepositives.Inframesubtraction,whenarecognizerdetectsanobjectwithaboundingboxbinaframeF1thatitdidnotdetectinthepreviousframeF0,wecomputethediﬀerenceCrop(F1,b)−Crop(F0,b)andcheckthenumberofpixelsthathaveadiﬀerence.Ifthisnum-11Recognizer Data Set
False Positive False Negative BlurFP BlurFN SubFP SubFN
Face
Face
Face
FullBody
FullBody
FullBody
LowBody
LowBody
LowBody
UpperBody
UpperBody
UpperBody
Eye
Eye
Eye
Nose
Nose
Nose
Mouth
Mouth
Mouth
Objects
Talking Head
Turning Face
Objects
Talking Head
Turning Face
Objects
Talking Head
Turning Face
Objects
Talking Head
Turning Face
Object
Talking Head
Turning Face
Object
Talking Head
Turning Face
Object
Talking Head
Turning Face
10.6%
0.2%
19.1%
14.8%
0.2 %
24.6%
19.5%
6.2%
33%
41%
5.3%
86%
35%
64 %
23 %
17.8%
90 %
24.5 %
61%
100 %
75 %
0%
0%
16.1%
0%
0%
0%
0%
0%
0%
0%
0%
0%
0%
0 %
5%
0%
0%
0%
0%
0%
0%
6%
0%
15%
3.5%
0%
22.7 %