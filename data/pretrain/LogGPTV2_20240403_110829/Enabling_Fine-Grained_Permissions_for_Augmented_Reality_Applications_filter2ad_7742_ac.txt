### Parameters and Event Handling

To check for newly recognized objects, the system periodically evaluates its parameters. If new objects are detected, the recognizer returns relevant data. The multiplexer then packages this data into an event structure and distributes it to all subscribed applications. We plan to implement a more efficient, interrupt-driven multiplexer in the future.

### Optional API Calls

The next two API calls are optional. The `filter` call allows the multiplexer to instruct the recognizer to use only a specific subset of raw inputs. For example, a face detector might only consider a sub-rectangle of the video frame. The `cache_compare` function is a recognizer-specific comparator that takes two sets of recognizer inputs and determines if they are considered equal. This function enables the multiplexer to implement per-recognizer caching. For instance, the multiplexer may pass the previous and current RGB frames to the face recognizer's `cache_compare` function and potentially avoid recomputing the face model if the frames have not changed significantly.

### Implementation Details

Our multiplexer and recognizers were implemented with approximately 3,000 lines of C++ code. We developed nine recognizers, which are summarized in Figure 11.

### Application Support

Applications using our multiplexer run in separate Windows processes. Each application links against the ARLib library we built. ARLib communicates with the multiplexer over local sockets and handles the marshaling and unmarshaling of recognizer event data. By calling ARLib functions, an application can request access to specific recognizers and register callbacks to handle recognizer events. ARLib provides both a low-level C++ interface and higher-level .NET wrappers for C# or other managed languages. ARLib consists of about 500 lines of C++ code and 400 lines of C# code.

### Sample Application

Figure 12 shows part of a test application we developed that detects faces and draws images on the screen that follow face movements. The application connects to the multiplexer and subscribes to face recognizer events. These events contain approximately 100 points corresponding to different parts of the face, or 0 points if no face is present. The application handles these events in the `ProcessFace` callback by checking if a face is present and calling a separate function (not shown) to update the display.

### Additional Applications

In addition to the face visualization, we ported several sample applications bundled with the Kinect SDK to our system, including a skeleton visualizer and raw RGB and depth visualizers. The porting effort was modest, aided by our event data formats being modeled on existing Kinect SDK APIs. We also developed two new applications: a 500-line C++ application that translates hand gestures into mouse cursor movements and a 300-line C# application that uses face recognition to annotate people with their names. Overall, we found our multiplexer interface to be simple and intuitive for building AR applications.

### Evaluation

We evaluated how recognizers are used by analyzing 87 shipping AR applications and users' mental models of AR applications. A survey of 462 respondents showed that users expect AR applications to have limited access to raw data. No shipping application requires continuous RGB access, and a set of four core recognizers is sufficient for almost all applications. For these core recognizers, we designed privacy goggles visualizations and evaluated how well users understand them. Next, we examined how the OS can mitigate recognizer errors once an application has access to recognizers. Finally, we demonstrated that our abstraction enables performance improvements, making this a rare case where improved privacy leads to better performance.

#### Core Recognizers

We analyzed 87 AR applications on the Xbox Kinect platform, including all applications sold on Amazon.com. We focused on Kinect because it is widely adopted and typically used in home settings. For each application, we manually reviewed its functionality through reviews or direct usage. From this, we extracted the "recognizers" necessary to support the application's functionality.

Figure 13 shows the results. Four core recognizers—skeleton tracking, hand position, person texture, and keyword voice commands—are sufficient to support around 89% of shipping AR applications. Person texture reveals a portion of the RGB video around a person detected through skeleton tracking, but with the image blurred or otherwise transformed to hide details. Fitness applications, in particular, use person texture to instruct the user on proper form.

After the core set, there is a "long tail" of seven additional recognizers. For example, the Alvin and the Chipmunks game uses voice modulation to "Alvin-ize" the player's voice, and NBA Baller Beats tracks the location of a basketball to check if the player dribbles in time to music. None of the applications in our set require continuous access to RGB data. Instead, they take short videos or photos of the player, which could be handled via user-driven access control [27]. Only three applications require audio access beyond voice command triggers, indicating room for improving privacy with least privilege enabled by the recognizer abstraction.

#### Privacy Expectations for Applications

To learn users' mental models of AR application capabilities, we showed 462 survey respondents a video of a Kinect "foot piano" application in action. We then asked about the application's capabilities. Figure 17(A) shows the results. Over 86% of users believed the application could see foot positions, while a much smaller number thought it had other capabilities. Overall, users expect applications to have limited access to the entire raw sensor stream.

#### Privacy Goggles for Core Recognizers

As discussed in Section 3, every recognizer must implement a visualization method to enable the privacy goggles view. The OS uses these visualizations to show users what information each application accesses. We developed privacy goggles visualizations for three of the four core recognizers: skeleton, hand position, and person texture. While voice commands are also a core recognizer, we decided to focus first on the visual recognizers and leave visualization of voice commands for future work.

#### Privacy Attitudes for Core Recognizers

We conducted surveys to measure the relative sensitivity of the information released by the core recognizers. We added the "face detector" recognizer and a "Raw" video recognizer that represents giving all information to the application. For each pair of recognizers, we showed a visualization from the same underlying video frame and asked participants which picture was "more sensitive" and why. Figure 15 shows an example comparing raw RGB and face detector recognizers. In total, we had 500 survey respondents, all from the United States.

As expected, respondents found that the raw RGB frame is more sensitive than any other recognizer. Based on the responses, we can order recognizers from "most sensitive" to "least sensitive" as follows: Raw, Face, Person Texture, Skeleton, and finally, Hand Position.

#### Effectiveness of Privacy Goggles

Finally, we evaluated whether our "privacy goggles" visualizations successfully communicate the capabilities of applications. We created three surveys, one for each of the skeleton, person texture, and hand recognizers. We had at least 150 respondents for each survey, with a total of 462 respondents. Our surveys were inspired by Felt et al.'s Android permission "quiz." [11] We showed a short video clip of the privacy goggles visualization for the target recognizer. Figure 14 shows an example for the skeleton recognizer. The right half shows the raw RGB video of a person writing on a whiteboard and handling a small ceramic cat figurine. The left half shows the "application-eye view" showing the detected skeleton. We then asked users what they believed the capabilities of the application would be if installed. Figure 17 shows the results, with a checkmark next to correct answers. A large number of respondents (over 80%) picked the correct result, and relatively few picked incorrect results. This shows that privacy goggles are effective at communicating application capabilities to users.

#### Respondent Demographics

Our survey participants were recruited from the U.S. through uSamp, a professional survey service, via the Instant.ly website. We did not specify any restrictions on demographics. As reported by uSamp, participants are 66% female and 33% male, with 10.2% in the 0–22 age range, 12.9% in the 22–26 age range, 21.2% in the 26–34 age range, 16.8% in the 34–42 age range, 13.5% in the 42–50 age range, 15.1% in the 50–60 age range, 8.1% in the 60–70 age range, and 1.8% 70 or older.

#### Human Ethics Statement

Our experiments included surveys of anonymous human participants. Our institution does not have an Institutional Review Board (IRB), but it does have a dedicated team focused on privacy and human protection. This team has pre-approved survey participant vendors to ensure they have privacy policies that protect participants. We followed the guidelines of this team in choosing our survey vendor. We also discussed our surveys with a member of the team to ensure that our questions did not ask for personally identifiable information, were not overly intrusive, and that no other issues were present.

### Noisy Permissions

While privacy goggles are effective at communicating what an app should and should not see to the user, the recognizers we use can have false positives, which could leak information to applications. We first evaluated a representative set of recognizers on well-known vision datasets to quantify the problem. Next, we evaluated OS-level mitigations for false positives.

#### Recognizer Accuracy

We picked three well-known datasets for our evaluations: (1) a Berkeley dataset consisting of pictures of objects, (2) an INRIA dataset containing pictures of a talking head, and (3) a set of pictures of a face turning toward the camera and then away. We then evaluated baseline false positive and false negative rates for seven object recognition algorithms contained in the widely adopted OpenCV library. All seven had false positives on at least one of the datasets.

#### Input Massaging

We implemented pre-permission blurring, where frames are put through a blurring process using a box filter before being passed to the face detection algorithm. We used a 12×12 box filter. We also used frame subtraction as a heuristic to suppress recognizer false positives. In frame subtraction, when a recognizer detects an object with a bounding box in a frame \( F_1 \) that it did not detect in the previous frame \( F_0 \), we compute the difference \( \text{Crop}(F_1, b) - \text{Crop}(F_0, b) \) and check the number of pixels that have a difference. If this number is below a threshold, the detection is considered a false positive.

### Recognizer Performance

Table 1 summarizes the false positive and false negative rates for various recognizers on different datasets, along with the impact of blurring and frame subtraction techniques.

| Recognizer | Data Set | False Positive | False Negative | BlurFP | BlurFN | SubFP | SubFN |
|------------|----------|----------------|----------------|--------|--------|-------|-------|
| Face       | Objects  | 10.6%          | 0.2%           | 0%     | 0%     | 0%    | 0%    |
| Face       | Talking Head | 19.1%         | 0.2%           | 0%     | 0%     | 0%    | 0%    |
| Face       | Turning Face | 24.6%        | 6.2%           | 0%     | 0%     | 0%    | 0%    |
| FullBody   | Objects  | 14.8%          | 0.2%           | 0%     | 0%     | 0%    | 0%    |
| FullBody   | Talking Head | 24.6%        | 6.2%           | 0%     | 0%     | 0%    | 0%    |
| FullBody   | Turning Face | 33%          | 41%            | 0%     | 0%     | 0%    | 0%    |
| LowBody    | Objects  | 19.5%          | 0.2%           | 0%     | 0%     | 0%    | 0%    |
| LowBody    | Talking Head | 24.6%        | 6.2%           | 0%     | 0%     | 0%    | 0%    |
| LowBody    | Turning Face | 33%          | 41%            | 0%     | 0%     | 0%    | 0%    |
| UpperBody  | Objects  | 41%            | 5.3%           | 0%     | 0%     | 0%    | 0%    |
| UpperBody  | Talking Head | 86%          | 35%            | 0%     | 0%     | 0%    | 0%    |
| UpperBody  | Turning Face | 64%          | 23%            | 0%     | 0%     | 0%    | 0%    |
| Eye        | Objects  | 17.8%          | 90%            | 0%     | 0%     | 0%    | 0%    |
| Eye        | Talking Head | 24.5%        | 61%            | 0%     | 0%     | 0%    | 0%    |
| Eye        | Turning Face | 100%         | 75%            | 0%     | 0%     | 0%    | 0%    |
| Nose       | Objects  | 16.1%          | 0%             | 0%     | 0%     | 0%    | 0%    |
| Nose       | Talking Head | 0%           | 0%             | 0%     | 0%     | 0%    | 0%    |
| Nose       | Turning Face | 0%           | 0%             | 0%     | 0%     | 0%    | 0%    |
| Mouth      | Objects  | 5%             | 0%             | 0%     | 0%     | 0%    | 0%    |
| Mouth      | Talking Head | 0%           | 0%             | 0%     | 0%     | 0%    | 0%    |
| Mouth      | Turning Face | 0%           | 0%             | 0%     | 0%     | 0%    | 0%    |

This table demonstrates the effectiveness of blurring and frame subtraction in reducing false positives and negatives, thereby enhancing the overall reliability and privacy of the recognizers.