strategy only on solving the vulnerability conditions. However,
the relaxed-constraint may also produce a false positive, and
we do not count a vulnerable label as being covered through
relaxed-constraint solving.
Timeout on Concolic Execution: To prevent the concolic
execution from hanging on localized code regions (e.g., , deep
loops and blocking IO), the concolic executor usually needs
a time threshold while running a seed. QSYM adjusts this
timing budget by watching AFL’s status. If the number of
hanging seeds increases, QSYM increases the timeout (up to
10 minutes). We set the timeout to be proportional to the
number of uncovered branches that a seed can reach. The
rationale is that those seeds need more time for constraint
solving and such setting beneﬁts higher bug coverage.
IV. IMPLEMENTATION
We have implemented SAVIOR, which can be applied
to software as sophisticated as Baidu’s Apollo Autonomous
Driving System [5, 37]. SAVIOR consists of four major
components: a compiling tool-chain built on top of Clang and
LLVM-4.0, a fuzzing component based on AFL-2.5b [2], a
concolic executor built atop KLEE [27] (with LLVM-3.6), and
a python middle-ware which coordinates the fuzzing compo-
nent and the concolic executor. In total, our implementation
has about 3.3K lines of python code and 4K lines of C/C++
code. SAVIOR can run on both 32-bit and 64-bit systems, and
it can support both 32-bit and 64-bit targets. In the following,
we discuss the important implementation details.
Concolic Executor: We develop our concolic executor based
on KLEE-3.6. The original KLEE aims at full symbolic
execution, and it does not support concolic execution. We
port a concolic executor from KLEE’s symbolic executor.
Speciﬁcally, the concolic executor attaches the concrete input
as the assignment property in the initial state. It then sym-
bolically interprets each instruction as KLEE originally does.
On reaching a conditional statement, it always follows the
branch that matches the concrete input. For the other branch,
if not covered, the concolic executor solves the conditions and
generate a corresponding testcase. The state following that
branch is then immediately terminated. When generating the
seed, our concolic executor copies the un-constrained bytes
from the input, instead of padding with random values.
Another limitation of KLEE is that the initialization phase is
notoriously time-consuming. To overcome this, we introduce a
fork server mode. In a run, KLEE ﬁrst sets up the environments
with bitcode loading, library linking, and preparing for globals
and constants. These are then followed by the initialization
of an Executor. By default, the Executor executes one
seed and then destructs itself. In our implementation, after
the execution of one seed, we clean up any stateful changes
introduced in last execution (including destructing the memory
manager, clearing the global data objects, and erasing all the
remaining states). Then we reuse the Executor to run a new
seed from the input queue. In this mode, we avoid repeating
the lengthy environments setup.
Recall that we invoke UBSan to label potentially vulnerable
operations. At the IR level, UBSan replaces those operations
with LLVM intrinsic functions, which are incomprehensible
by KLEE. We replace those intrinsic functions with general
LLVM IR so that KLEE can execute without exceptions. The
replacements follow those that KLEE already enforced [10].
By default, KLEE redirects un-modeled external functions
(e.g., system calls) to the native code. This causes two issues.
First, KLEE is unaware of their effects on the symbolic address
space, which can interrupt memory operations. For instance,
the function strdup allocates a new buffer and copies data
from the source to this buffer. However, KLEE cannot capture
this allocation due to the lack of modeling. On future accesses
to this buffer, KLEE will throw an out-of-bound access error.
There are many similar cases, such as getenv. We extend
KLEE’s environment model to include the symbolic versions
of those functions. Second, KLEE concretizes the data passed
to the external functions and adds constant constraints on such
data for future execution. However, this may over-constraint
the concretized variables. For instance, KLEE concretizes the
data written to standard output or ﬁles. This leads to over-
constraints – When the concretized data is later used in
constraint solving, KLEE will not be able to ﬁnd a satisfying
solution. To address this issue, we prevent KLEE from adding
constraints on concretization. This scheme, following the
design of S2E [31] and QSYM [73], ensures that we never
miss solutions for non-covered branches.
Last but not least, stock KLEE provides limited support
for software written in C++. Since a lot of the C++ programs
rely on the standard C++ library (e.g., libstdc++ on Linux) but
KLEE neither models this library nor supports the semantics
of calls to this library. Therefore, KLEE frequently aborts the
execution in the early stage of running a C++ program. We
customize the GNU libstdc++ library to make it compilable
and linkable to KLEE. Considering that many libstdc++ func-
tions also access in-existent devices (e.g., Random), we also
build models of those devices.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:58:09 UTC from IEEE Xplore.  Restrictions apply. 
1586
Fuzzers
AFL
AFLGO
TFUZZ
ANGORA
DRILLER
QSYM
SAVIOR
Source
Instances
Setup
[2]
[1]
[19]
[3]
[17]
Self-developed
Self-developed
1 AFL master; 2 AFL slaves
1 AFLGo master; 2 AFLGo slaves
3 AFL jobs (adjust default argument to Fuzzer)
3 Angora threads (with option "-j 3")
1 concolic executor; 1 AFL master; 1 AFL slave
1 concolic executor; 1 AFL master; 1 AFL slave
1 concolic executor; 1 AFL master; 1 AFL slave
Use in-lined lava_get as target locations of guided fuzzing
Use the docker environment prepared at
[19] for evaluation
Patch Lava to support Angora, as suggested by the developers [18]
Follow the original Driller in scheduling concolic execution [7]
Use in-lined lava_get as labels of vulnerabilities
Note
N/A
N/A
TABLE II: Fuzzer speciﬁc settings in evaluation with Lava-M.
V. EVALUATION
A. Evaluation with LAVA-M
SAVIOR approaches bug-driven hybrid testing with the
key techniques of bug-driven prioritization and bug-guided
veriﬁcation. In this section, we evaluate these techniques and
our evaluation centers around two questions:
• With bug-driven prioritization, can hybrid testing ﬁnd
vulnerabilities quicker?
• With bug-guided veriﬁcation, can hybrid testing ﬁnd
vulnerabilities more thoroughly?
To support our evaluation goals, we prepare two groups
of widely-used benchmarks. The ﬁrst group is the LAVA-M
data-set [36]. This data-set comes with artiﬁcial vulnerabilities,
and the ground truth is provided. The second group includes
a set of 8 real-world programs. Details about these programs
are summarized in Table V. All these programs have been
extensively tested in both industry [16] and academia [57, 66,
73]. In addition, they represent a higher level of diversity in
functionality and complexity.
Using the two benchmarks, we compare SAVIOR with
the most effective tools from related families. To be speciﬁc,
we take AFL [2] as the baseline of coverage-based testing.
As SAVIOR performs testing in a directed manner, we also
include the state-of-the-art directed fuzzer, AFLGO [25]. To
handle complex conditions, recent fuzzing research introduces
a group of new techniques to improve code coverage. From
this category, we cover TFUZZ [56] and ANGORA [29], be-
cause they are open-sourced and representatives of the state-of-
the-art. Finally, we also consider the existing implementations
of hybrid testing, DRILLER [66] and QSYM [73].
Note that the original DRILLER has problems of running
many of our benchmarks, due to lack of system-call modeling
or failure to generate test cases (even with the patch [6] to
support input from ﬁles). This aligns with the observations
in [73]. In the evaluation, we re-implement DRILLER on
the top of SAVIOR. More speciﬁcally, it runs AFL as the
fuzzing component and it invokes the concolic executor once
the pending_favs attribute in AFL drops to 0. These
implementations strictly follow the original DRILLER [7].
Similar to the Angr-based concolic executor in DRILLER,
our KLEE-based concolic executor focuses on generating new
seeds to cover untouched branches. In addition, we keep the
relaxed constraint solving and the fork-server mode. These two
features increase the effectiveness and efﬁciency of DRILLER
without introducing algorithmic changes.
In the following, we will explain the experimental setups
and evaluation results for the two groups of benchmarks.
1) Experimental Setup:
In this evaluation, we run each
of the fuzzers in Table II with the four LAVA-M programs
and we use the seeds shipped with the benchmark. For
consistency, we conduct all the experiments on Amazon EC2
instances (Intel Xeon E5 Broadwell 64 cores, 256GB RAM,
and running Ubuntu 16.04 LTS), and we sequentially run all
the experiments to avoid interference. In addition, we assign
each fuzzer 3 free CPU cores to ensure fairness in terms
of computation resources. Each test is run for 24 hours. To
minimize the effect of randomness in fuzzing, we repeat each
test 5 times and report the average results.
In Table II, we also summarize the settings speciﬁc to
each fuzzer, including how we distribute the 3 CPU cores
and the actions we take to accommodate those fuzzers. In
LAVA-M, each artiﬁcial vulnerability is enclosed and checked
in a call to lava_get (in-lined in our evaluation). We use
these calls as the targets to guide AFLGO and we mark them
as vulnerability labels to enable bug-driven prioritization in
SAVIOR. In addition, as the vulnerability condition is hard-
coded in the lava_get function, we naturally have support
for bug-guided veriﬁcation. Finally, for ANGORA, we adopt
the patches as suggested by the developers [18].
2) Evaluation Results:
In the left column of Figure 8,
we show how many vulnerabilities are reached over time
by different fuzzers. The results demonstrate that all
the
fuzzers can instantly cover the code with LAVA vulnerabilities.
However, as presented in the right column of Figure 8, TFUZZ,
ANGORA, DRILLER, QSYM, and SAVIOR are able to trigger
most (or all) of the vulnerabilities while AFL and AFLGO can
trigger few. The reason behind is that the triggering conditions
of LAVA vulnerabilities are all in the form of 32-bit magic
number matching. Mutation-based fuzzers, including AFL and
AFLGo, can hardly satisfy those conditions while the other
fuzzers are all featured with techniques to solve them.
Vulnerability Finding Efﬁciency: Despite TFUZZ, ANGORA,
DRILLER, QSYM, and SAVIOR all trigger large numbers
of LAVA vulnerabilities, they differ in terms of efﬁciency.
TFUZZ quickly covers the listed vulnerabilities in base64
and uniq. This is attributable to that (1) TFUZZ can reach all
the vulnerabilities with several initial seeds and (2) TFUZZ can
transform the program to immediately trigger the encountered
vulnerabilities. Note that we do not show the results of
TFUZZ on md5sum and who, because TFUZZ gets interrupted
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:58:09 UTC from IEEE Xplore.  Restrictions apply. 
1587
Fuzzers
AFL
AFLGO
TFUZZ
ANGORA
DRILLER
QSYM
SAVIOR
Listed
base64
0 (0%)
2 (5%)
47 (100%)
47 (100%)
48 (100%)
47 (100%)
48 (100%)
44
Fuzzing results
uniq
0 (0%)
1 (4%)
29 (100%)
28 (100%)
28 (100%)
29 (100%)
29 (100%)
28
md5sum
0 (0%)
0 (0%)
N/A
54 (95%)
58 (100%)
58 (100%)
59 (100%)
57
who
0 (0%)
0 (0%)
N/A
1743 (79%)
1827 (78%)
1244 (53%)
2213 (92%)
2136
(a) Number of bugs reached in base64
(b) Number of bugs triggered in base64
TABLE III: LAVA-M Bugs triggered by different fuzzers
(before bug-guided veriﬁcation). “X%” indicates that X% of
the listed LAVA bugs are triggered.
(c) Number of bugs reached in uniq
(d) Number of bugs triggered in uniq
Fuzzers
AFL
AFLGO
TFUZZ
ANGORA
DRILLER
QSYM
SAVIOR
Listed
Fuzzing results
base64
48 (100%)
48 (100%)
47 (100%)
48 (100%)
48 (100%)
48 (100%)
48 (100%)
44
uniq
29 (100%)
29 (100%)
29 (100%)
29 (100%)
29 (100%)
29 (100%)
29 (100%)
28
md5sum
59 (100%)
59 (100%)
N/A
59 (100%)
59 (100%)
59 (100%)
59 (100%)
57
who
2357 (96.3%)
2357 (96.3%)
N/A
2357 (96.3%)
2357 (96.3%)
2357 (96.3%)
2357 (96.3%)