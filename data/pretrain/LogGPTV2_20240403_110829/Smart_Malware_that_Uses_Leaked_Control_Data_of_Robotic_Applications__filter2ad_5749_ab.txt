data into a ROS message as deﬁned for the topic. Once the
message is ready, the input node publishes it. The input node
should have declared itself to the ROS core as a publisher
for a topic. A computational node is a node that takes the
input data to produce the output (e.g., a command to a robot
actuator); it subscribes from the input node and publishes to
the output node. Finally, an output node is a node connected
to an actuator (i.e., the robot). The values (i.e., robot states
and control commands) subscribed from the computational
node are converted into a form that the hardware can interpret,
and passed to the hardware. The output data determine the
(joint) state of the robot.
The ROS framework comes with the RVIZ software pack-
age, which allows users to test and visualize the operation
of the robot applications in a virtualized environment. RVIZ
takes the physical speciﬁcations of the robot and displays
the mesh of the robot; it is heavily used to test robot designs
without using a physical robot.
Raven-II and haptic force feedback rendering engine. In
this paper, we study the resiliency of a ROS application in
the context of a surgical robot (i.e., Raven-II) and its haptic
feedback rendering engine. Leveraging the open-architecture
surgical robot, the authors of [30] present a hardware-in-the-
loop simulator for training surgeons in telerobotic surgery.
The simulator, in addition to having all the features of the
Raven surgical robot, introduces a novel algorithm to pro-
vide haptic feedback to the operator and, hence, offer a touch
sensation to surgeons. Unfortunately, commercially available
surgical systems1 (e.g., da Vinci by Intuitive [52]) do not pro-
vide haptic feedback to the operator. The traditional approach
for haptic feedback uses physical force sensors to determine
the force applied to the robot. Since the instruments (on which
the force sensors are installed) are disposable, that approach
turns out to be costly. Instead, the authors of [30] proposed an
indirect haptic feedback rendering approach that does not rely
1In 2017, the FDA approved a new surgical system [49] with haptic force
feedback. However, we do not have sufﬁcient information to understand the
underlying technology and, hence, the capability of the system.
USENIX Association        22nd International Symposium on Research in Attacks, Intrusions and Defenses 339(a) Generic robot running on ROS.
(b) Raven-II with its haptic feedback rendering algorithm.
Figure 1: Software architecture of robotic applications.
on force sensor measurement, but instead uses image sensor
data to derive the force feedback.
In our study, the haptic feedback rendering algorithm, as
implemented in the augmented Raven-II simulator, utilizes
information from a depth map (a matrix of distances from the
image sensor to each pixel of a hard surface) to derive the
distance from the object (e.g., a patient’s tissues) to the robot
arm. Using the current position of the arm and the measured
distance to the object, the algorithm returns an interactive
force value that generates resistance in the haptic device.
Figure 1b provides an overview of the software architecture
of Raven-II and its haptic feedback rendering algorithm. The
haptic algorithm takes input from the Kinect image sensor
and the OMNI haptic device to control the Raven-II robot (or
its virtual representation in RVIZ). The sensorkinect node
parses the image data (as BGR and depth) from the Kinect
image sensor, packages the data into ROS messages, and pub-
lishes the messages to the ROS core as topics (kinect/BGR and
kinect/Depth). The omni_client node is connected to the
OMNI haptic device for user input. The omni_client node
shares the processed operator input as a topic (omni_incr). A
set of nodes, dedicated to running the algorithm, subscribe to
the topics from the ROS core and derive the force feedback,
which the omni_client sends to the haptic device.
The kinect/Depth topic from sensorkinect is used to de-
rive the distance from the robot arm to the object. However, in
deriving the distance, the algorithm needs a reference frame.
It leverages the ArUco library [21, 46], which is an Open-
Source library commonly used for camera pose estimation.
With the ArUco marker (i.e., a squared marker) location ﬁxed
and used as a reference point, we can derive the location of
the robot arm(s) relative to the marker. Using that information,
the algorithm can derive the distance from the robot arm to
the object by using (i) the transformation from the marker to
the robot, (ii) the transformation from the image sensor to the
marker, and (iii) the transformation from the image sensor to
the object. Because the transformation from the robot arm to
the object is evaluated in near-real-time, the algorithm can
provide timely haptic force feedback to the OMNI device.
4 Approach
Cyber security is often referred to as a “cat and mouse” game.
In this paper, we are considering potential advances in cy-
ber threats, assuming that adversaries will eventually take
advantage of machine learning techniques (if they are not
already doing so). In this section, we present our approach
for corrupting a robotic application with self-learning mal-
ware. We developed this approach to raise awareness of the
potential threats, and to promote preparation for responding
to this threat. The methodologies included in our approach
can be used for (i) preemptive identiﬁcation of vulnerabilities,
(ii) hardening of robotic applications against potential threats,
and (iii) design of detection/mitigation methods.
Threat model. In our threat model, we assume:
• The attacker can penetrate into the control network of the
robot. As presented in [13], ROS applications are often
connected to a public network without proper protection.
One can provide a level of protection by virtually isolating
the control network (i.e., by deploying a VLAN). However,
it would be possible to intrude into the virtual network
either with stolen credentials or by exploiting a weak link
(i.e., a vulnerable computer that has access to the VLAN).
In the context of attacks on surgical robots, a survey on
potential entry points of a hospital network can be found
in [1].
• The attacker understands the operation of ROS, and has
access to ROS-provided APIs (which are easily obtainable
online). With remote access to the ROS master, one can
execute ROS commands.
• The target robot runs on top of ROS 1. Our attack model is
designed for ROS 1 (e.g., Kinetic and Melodic), which is
still the most commonly deployed version despite the re-
lease of ROS 2 (discussed in detail in Section 7.1) in 2015.
Software patches have been issued to ﬁx the vulnerabili-
ties in ROS, but, as we discuss in Section 7.1, the patches
merely require attackers to take another step to neutralize
them. Hence, in describing our attack model, we assume
the default setting of the most commonly used ROS.
340          22nd International Symposium on Research in Attacks, Intrusions and DefensesUSENIX AssociationWhile the communication might include sensitive data, the
ROS 1 framework does not provide options for authenticating
or validating the ROS entities. (I.e., any ROS node that can
access the master can register itself as a publisher to write
messages or as a subscriber to read messages.) After conﬁg-
uring the ROS setup to connect to the victim ROS master
(attack preparation; see Section 4.1), our malware can initiate
a subscriber that eavesdrops on the network communications.
To take control of the robot, it kicks out a genuine node and
publishes malicious data while masquerading as the original
publisher. Without noticing the change in the publisher, the
robotic application takes the malicious data as an input (or
command) and updates the state of the robot accordingly.
4.3 Trigger: Inference of critical time to
initiate the malicious payload
Most security attacks are detected when the attack payload is
executed [50]. Once detected, the attacker (or the malware that
the attacker had installed) is removed from the system. Con-
sequently, in many cases, the attacker may have one chance
to execute the payload before being detected. As a result, it
is realistic to consider the case in which an attacker tries to
identify the ideal time to execute the attack payload (in our
case, to inject a fault) in order to maximize the chances of
success. A common approach is to embed a trigger function
into the malware, which checks for a condition and executes
the payload only when the condition is satisﬁed.
In [2], Alemzadeh et al. presented an attack model that is
triggered by a prediction of the robot state derived by a side-
channel attack. In the model, the attacker installs malware on
the robot control system, eavesdrops on a USB packet, and
infers the state of the robot (i.e., either “engaged” when the
surgeon’s input is updating the position of the robot, or “dis-
engaged” when the position of the robot is not being updated).
The robot state (which is controlled by pedal input from the
surgeon) is an effective indicator in determining when mali-
cious input would be fed into the robot. However, in such an
approach, it is hard to accurately determine the time window
during which the robot is performing critical activities; e.g.,
it is more critical when the robot is cutting tissue than when
it is transitioning towards the target object.
In this study, we present an approach that leverages a well-
studied learning technique to infer the critical time to trigger
the attack payload, so as to maximize the impact.
Inference of object location. During a surgical operation,
the robot usually moves within a limited range deﬁned by the
nature of the surgical procedure. Hence, the precision in iden-
tifying ‘the time when the robot is touching (or maneuvering
close to) the target object’ can help in triggering the attack at
the most opportune time so as to maximize the impact. For
instance, when the robot is moving from its idle location to
the patient on the operating table, the robot is operating in an
open space without obstacles. Hence, visual input is sufﬁcient
Figure 2: Approach overview, from attack preparation to im-
pact to ROS application.
Approach overview. Vulnerabilities present in the ROS
framework allow unauthorized entities to eavesdrop on mes-
sages passed across ROS nodes. Utilizing the obtained data, a
malicious entity can identify an ideal time to trigger an attack
and corrupt the operation of the robot by injecting faulty input
or output commands. In Figure 2, we present an overview
of our approach. During the attack preparation phase, we
identify the victim (i.e., a ROS master that is remotely acces-
sible) and its critical components. Once they are identiﬁed,
the attack strategy can be applied to perform a man-in-the-
middle (MITM) attack. As the malware eavesdrops on the
sensor and control data of the robot, the smart malware runs
a learning algorithm to infer the location of the target object.
When the object is identiﬁed (i.e., the algorithm returns a clus-
ter) and the robot reaches the predicted location of the target
object, a trigger is raised to initiate the attack payload. In
the following, we describe the details of our approach.
4.1 Attack preparation
To deploy the attack, the ﬁrst step is to identify machines that
are running ROS as a master (core) node. Using a network
scanning tool, we scan for the default port for ROS masters
(i.e., 11311, a well-known port for ROS masters) [13]. Once
the master and its IP address are known, we set up ROS on
our machine (which mimics a remote attacker) and update
the ROS master’s Uniform Resource Identiﬁer (URI) variable
to that of the identiﬁed master. Using the ROS APIs, we
search for the topics of interest (i.e., the topics registered to
the ROS master are used as a signature for identifying the
ROS application).
4.2 Attack strategy: ROS-speciﬁc MITM
In corrupting a ROS application, we take advantage of the
vulnerabilities in ROS and execute a ROS-speciﬁc man-in-
the-middle attack. As described in Section 3, ROS provides
a set of interfaces (publish/subscribe) that ROS nodes can
use to communicate; the ROS core serves as the arbitrator.
USENIX Association        22nd International Symposium on Research in Attacks, Intrusions and Defenses 341to allow the surgeon to operate. Furthermore, the surgeon
will not even notice whether the haptic feedback rendering
algorithm is operational, as there is no surface that the robot
would touch (i.e., there is zero force feedback). On the other
hand, when the robot is inside the abdomen of the patient,
it is operating in limited space packed with obstacles (e.g.,
organs) and with blind spots that the image sensor cannot
monitor. In that situation, correct operation of the rendering
algorithm is critical. Also, the shorter the distance from the
robot (at the point of the trigger) to the target object, the less
time it takes the surgeon to respond2 upon discovering the
failure of the rendering algorithm (which can be determined
only by noticing the lack of force feedback when a surface is
touched). In this paper, we analyze the spatial density of the
robot end-effector position throughout the operation to infer
a time when the robot (i.e., the surgical instrument) is near
the object.
Algorithm. We use unsupervised machine learning to deter-
mine the location of the target object with respect to the posi-
tion of the robot’s end-effector(s). Speciﬁcally, we adopted
the density-based spatial-clustering algorithm with noise (DB-
SCAN) [20, 48] to accomplish this task. The DBSCAN algo-
rithm takes two parameters, ε and numMinPonts. The maxi-
mum distance parameter (ε) deﬁnes the maximum distance
between neighboring data points. Iterating over all data points,
the algorithm checks for neighbors whose distance from a
data point is less than ε. If the number of neighbors is less
than numMinPoints, the data point is considered noise (i.e.,
the data point is not part of a cluster). Otherwise, the algorithm
checks whether the neighbors form a cluster. Any clusters
already formed by a neighbor are merged into the current
cluster. Although the DBSCAN algorithm is known for its
sensitivity to the choice of parameters, the attacker does not
have much information from which to derive the right pa-
rameters. Based on the data subscription frequency (i.e., 80
Hz for our eavesdropper) and our conservative assumption
that at least 10% of the overall operation time corresponds to
the critical procedures3 (i.e., 10 seconds for our data from (cid:39)
100 seconds of robot operation), we set numMinPonts to 800
(i.e., subscription_ f requency× seconds_o f _stay). Also, we
consider points (corresponding to the robot’s end-effector
position) within 1 cm of each other to be “close,” and de-
ﬁne ε=10. With a goal of demonstrating the feasibility of
self-learning malware (not of presenting the best algorithm or
parameters for a clustering problem), we ﬁnd the parameter
pair (ε, numMinPoints) to be accurate enough for our study.
The optimization of the parameters is outside the scope of
this paper.
2Similar to the concept of braking distance when driving a car.
3From a set of medical studies, we ﬁnd that the mean of the total operation
time was 178.2 minutes [12] and that the mean time for a critical procedure
was 22.2 minutes [19] (i.e., 12.4% of the mean procedure time).
4.4 Attack payload: Fault injection
While the attack strategy in Section 4.2 is generic to the ROS
framework, the payload is speciﬁc to the ROS application
under study (i.e., Raven-II and its haptic feedback rendering
algorithm). As part of assessing the resiliency of the haptic
feedback rendering engine, we designed a set of faults that