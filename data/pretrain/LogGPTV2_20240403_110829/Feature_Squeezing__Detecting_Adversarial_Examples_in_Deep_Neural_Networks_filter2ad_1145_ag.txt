### Challenges and Preliminary Experiments with Adaptive Attacks

To successfully bypass our detection framework, an adversary must find an input where the original classifier produces an incorrect output, and the L1 score between the model’s predictions on the squeezed and original inputs is below the detection threshold. This task is significantly more challenging than merely finding an adversarial example, as evidenced by our experimental results.

He et al. [14] recently proposed an adaptive attack that can effectively generate adversarial examples to defeat a specific configuration of a feature squeezing defense. Their approach involves finding adversarial examples that both mislead the original model and have a joint L1 prediction distance lower than a pre-selected threshold for the squeezed inputs. This method adapts the CW2 attack by adding a penalty term for the L1 prediction distance. The approach requires that all feature squeezing operators be differentiable to compute the gradient of the loss function during optimization. For non-differentiable feature squeezers, such as bit depth reduction, the algorithm must be restarted multiple times with random initializations, hoping to find an example resilient against these squeezers. Consequently, the attack becomes non-deterministic and more time-consuming, taking approximately 20 seconds per MNIST sample, which is about 60 times slower than the original CW2 attack.

We evaluated their adaptive attack on the MNIST dataset because we currently lack a TensorFlow implementation of the non-local means filter used in CIFAR-10 and ImageNet. We adapted their attacking code to our MNIST model and the joint-detector reported in Table V, using a threshold value of 0.0029. In addition to the untargeted attack tested by He et al., we also examined two targeted attacks mentioned in Section IV. We used the same parameters as He et al. [14], except for the target threshold value, which we set to 0.0029, matching the detector's threshold in this paper. The seed examples were the same 100 images used in Section IV.

With the exact threshold used in this paper, the success rate of the untargeted attack was 64%, lower than the 100% success rate reported by He et al. The success rates for the targeted attacks were even lower: 41% for the targeted-next attack and 27% for the targeted-LL attack. These lower success rates are likely due to the use of a different MNIST model (representative of a state-of-the-art model) and a much smaller threshold value (0.0029) compared to the one they used (over 0.3076).

The adaptive attack often produces adversarial examples with large perturbations that appear ambiguous even to human eyes. To understand the adversary's success rate in a more realistic setting, we clipped the perturbations with different ε values ranging from 0 to 1 and presented the success rates in Figure 7. We found that the success rates decreased to 15%, 4%, and 0% respectively when clipping the perturbations with ε = 0.3, a value commonly considered in many papers for the MNIST dataset [22].

### Conclusion

The effectiveness of feature squeezing is surprising given its simplicity and low cost compared to other proposed defenses. While developing a comprehensive theory of adversarial examples remains elusive, our intuition suggests that squeezing reduces the search space of possible perturbations available to an adversary. Bit depth reduction, for instance, eliminates some of the lower bits, shrinking the feature space and forcing the adversary to produce larger perturbations. This has little impact on the accuracy of legitimate samples. Spatial smoothing, on the other hand, makes pixels less different across an image, mitigating L0 perturbations.

As discussed in Section V-D, feature squeezing is not immune to adversarial adaptation but substantially changes the challenge faced by the adversary. Our general detection framework opens new research directions in defending against adversarial examples and understanding the limits of deep neural networks in adversarial contexts.

### References

[1] Rodrigo Benenson. Classification datasets results. http://rodrigob.github.io/are-we-there-yet/build/classification-datasets-results.html.
[2] Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.
[3] Nicholas Carlini. Robust evasion attacks against neural network to find adversarial examples. https://github.com/carlini/nn-robust-attacks/.
[4] Nicholas Carlini, Pratyush Mishra, Tavish Vaidya, Yuankai Zhang, Micah Sherr, Clay Shields, David Wagner, and Wenchao Zhou. Hidden voice commands. In USENIX Security Symposium, 2016.
[5] Nicholas Carlini and David Wagner. Defensive Distillation is not robust to adversarial examples. arXiv preprint arXiv:1607.04311, 2016.
[6] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In IEEE Symposium on Security and Privacy (Oakland), 2017.
[7] Francois Chollet. Keras Implementation of Inception v3. https://github.com/fchollet/deep-learning-models/blob/master/inception_v3.py.
[8] George E. Dahl, Jack W. Stokes, Li Deng, and Dong Yu. Large-scale malware classification using random projections and neural networks. In IEEE International Conference on Acoustics, Speech and Signal Processing, 2013.
[9] Reuben Feinman, Ryan R. Curtin, Saurabh Shintre, and Andrew B. Gardner. Detecting adversarial samples from artifacts. arXiv preprint arXiv:1703.00410, 2017.
[10] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In International Conference on Learning Representations (ICLR), 2015.
[11] Kathrin Grosse, Praveen Manoharan, Nicolas Papernot, Michael Backes, and Patrick McDaniel. On the (statistical) detection of adversarial examples. arXiv preprint arXiv:1702.06280, 2017.
[12] Shixiang Gu and Luca Rigazio. Towards deep neural network architectures robust to adversarial examples. arXiv preprint arXiv:1412.5068, 2014.
[13] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, and others. DeepSpeech: Scaling up end-to-end speech recognition. arXiv preprint arXiv:1412.5567, 2014.
[14] Warren He, James Wei, Xinyun Chen, Nicholas Carlini, and Dawn Song. Adversarial example defenses: Ensembles of weak defenses are not strong. In 11th USENIX Workshop on Offensive Technologies (WOOT 17), 2017.
[15] Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012.
[16] Hossein Hosseini, Sreeram Kannan, Baosen Zhang, and Radha Poovendran. Deceiving Google's Perspective API built for detecting toxic comments. arXiv preprint arXiv:1702.08138, 2017.
[17] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.
[18] Gao Huang, Zhuang Liu, Kilian Q. Weinberger, and Laurens van der Maaten. Densely connected convolutional networks. arXiv preprint arXiv:1608.06993, 2016.
[19] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. ImageNet Classification with Deep Convolutional Neural Networks. In Advances in Neural Information Processing Systems (NIPS), 2012.
[20] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world. In International Conference on Learning Representations (ICLR) Workshop, 2017.
[21] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 1998.
[22] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.
[23] Somshubra Majumdar. DenseNet Implementation in Keras. https://github.com/titu1994/DenseNet/.
[24] Somshubra Majumdar. Keras Implementation of Mobile Networks. https://github.com/titu1994/MobileNetworks/.
[25] Jan Hendrik Metzen, Tim Genewein, Volker Fischer, and Bastian Bischoff. On detecting adversarial perturbations. arXiv preprint arXiv:1702.04267, 2017.
[26] Microsoft Corporation. Microsoft Malware Competition Challenge. https://www.kaggle.com/c/malware-classification, 2015.
[27] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal adversarial perturbations. https://github.com/LTS4/universal/.
[28] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. DeepFool: a simple and accurate method to fool deep neural networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
[29] Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.
[30] Nicolas Papernot, Ian Goodfellow, Ryan Sheatsley, Reuben Feinman, and Patrick McDaniel. cleverhans v1.0.0: an adversarial machine learning library. arXiv preprint arXiv:1610.00768, 2016.
[31] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. Practical black-box attacks against deep learning systems using adversarial examples. arXiv preprint arXiv:1602.02697, 2016.
[32] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram Swami. The limitations of deep learning in adversarial settings. In IEEE European Symposium on Security and Privacy (EuroS&P), 2016.
[33] Nicolas Papernot, Patrick McDaniel, Arunesh Sinha, and Michael Wellman. Towards the Science of Security and Privacy in Machine Learning. arXiv preprint arXiv:1611.03814, 2016.
[34] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks. In IEEE Symposium on Security and Privacy (Oakland), 2016.
[35] O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep Face Recognition. In British Machine Vision Conference, 2015.
[36] Scientific Computing Tools for Python Developers. Multidimensional Image Processing (scipy.ndimage). https://docs.scipy.org/doc/scipy/reference/tutorial/ndimage.html, 2009.
[37] Scientific Computing Tools for Python Developers. Median Filter (scipy.ndimage.median_filter). https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.median_filter.html#scipy.ndimage.median_filter, 2017.
[38] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2818–2826, 2016.
[39] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning Representations (ICLR), 2014.
[40] Matthew A. Turk and Alex P. Pentland. Face Recognition using Eigenfaces. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 1991.
[41] Beilun Wang, Ji Gao, and Yanjun Qi. A Theoretical Framework for Robustness of (Deep) Classifiers Under Adversarial Noise. In International Conference on Learning Representations (ICLR) Workshop, 2017.
[42] Wikipedia. Median Filter. Page Version ID: 760708062.
[43] Fei Zhang, Patrick PK Chan, Battista Biggio, Daniel S. Yeung, and Fabio Roli. Adversarial Feature Selection against Evasion Attacks. IEEE Transactions on Cybernetics, 2015.

### Appendix

#### A. Attack Parameters

| Attack | FGSM | BIM | CW∞ | DeepFool | CW2 | CW0 |
|---------|------|-----|------|-----------|-----|-----|
| **Parameters** | eps | eps | eps | iter | confidence | overshoot | confidence | max iterations | batch size | confidence |
| **MNIST** | 0.3 | 0.3 | 0.06 | - | 100 | 0.0156 | 0.0080 | 0.0012 | 5 | 10 | 5 | 1000 | 5 | 0.0078 | 0.0040 | 0.002 | 35 | 10 |

#### B. EvadeML-Zoo

Readers can reproduce the results using EvadeML-Zoo [8].  
[8] https://github.com/mzweilin/EvadeML-Zoo