consider the challenges an adversary faces in adapting attacks
to our defenses and report on preliminary experiments with
adaptive attacks.
To be successful against our detection framework, an
adversary needs to ﬁnd an input where the original classiﬁer
produces the wrong output and the L1 score between the
model’s predictions on squeezed and original inputs is below
the detection threshold. This is a much harder problem than
just ﬁnding an adversarial example, as is supported by our
13
experimental results.
He et al. [14] recently proposed an adaptive attack which
can successfully ﬁnd adversarial examples that defeat one
conﬁguration of a feature squeezing defense.5 The approach
ﬁnds adversarial examples that both confuse the original model
and have a scorejoint lower than a pre-selected threshold for
squeezed inputs. Their approach adapts the CW2 attack by
adding a penalty term for the L1 prediction distance. It requires
that all the feature squeezing operators are diﬀerentiable so
that it is possible to compute the gradient of the loss function
in the optimization process. For the non-diﬀerentiable feature
squeezers such as the bit depth reduction,
their approach
requires restarting the algorithm several times with random
initialization and hoping it ﬁnds an example that is resilient
against the non-diﬀerentiable squeezers. Due to this reason,
the attack is non-deterministic and more time-consuming in
face of non-diﬀrentiable components [14]. The attack takes
roughly 20 seconds on each MNIST sample, which is around
60× slower than the original CW2 attack.
We only evaluate their adaptive attack on the MNIST
dataset, because we currently don’t have a Tensorﬂow im-
plementation of the non-local means ﬁlter used on CIFAR-10
and ImageNet.6 We adapted their attacking code to the MNIST
model in this paper and the joint-detector we report in Table V
with the threshold value 0.0029.7
In addition to the untargeted attack He et al. tested, we also
test the two targeted attacks mentioned earlier in Section IV.
We used the same parameters as He et al. [14] in generating
the adaptive adversarial examples except that we change the
target threshold value to 0.0029 that is used by the detector in
this paper. The seed examples are the same 100 images used
in Section IV.
With the exact threshold we use in this paper as the target
value, the success rate of the untargeted attack is 64%. This is
lower than the 100% success rate in their report. The success
rates of the targeted attacks are even lower: 41% for the
targeted-next attack and 27% for the targeted-ll attack. We
believe these lower adversarial success rates are due to using
a diﬀerent MNIST model (representative of a state-of-the-art
model) and our detector has a much smaller threshold value
(0.0029) compared to the one they used (over 0.3076).
The adaptive attack often produces adversarial examples
with large perturbations that look ambiguous even to human
eyes. In order to understand the adversary success rate in
a more realistic setting, we clipped the perturbations with
diﬀerent  values ranging from 0 to 1 and presented the success
rates in Figure 7. We found that the success rates decreased
to 15%, 4%, 0% respectively if we clipped the perturbations
with 0.3, which was an  value considered in many papers for
the MNIST dataset[22].
5This work was done following public reports on the work in this paper; we
shared details of our approach and code with the authors of [14], and much
appreciate their sharing their implementation with us to enable the experiments
reported here.
6He et al. reported results for CIFAR-10 with only bit depth reduction and
median smoothing [14]. These results were similar to the results they reported
on MNIST.
7In contrast, their target detector uses 0.3076 as threshold and uses a slightly
diﬀerent max function in combining dual-squeezers.
Fig. 7: Success rate on the adaptive adversarial examples. The
success rate of the adaptive adversary decreases as we decrease the allowed
 value.
Nevertheless, the adaptive attack shows how an adversary
may be able to ﬁnd inputs that both confuse the model and
are not detected as adversarial by the distance metrics. One
obvious strategy for further complicating the adversary’s task
is to introduce randomness in the squeezing method. This is
very diﬀerent from attempts to obfuscate models, which have
been shown vulnerable to transfer attacks. Instead, we can
easily used cryptographic randomness to make the deployed
framework unpredictable in ways that beneﬁt the defender,
since the adversary’s search requires the knowledge on the
exact squeezing operation. The defender has many opportuni-
ties to use randomness in selecting squeezing parameters (for
example, instead of using a ﬁxed 0.5 threshold for the 1-bit
ﬁlter, using 0.5 ± rand(0.1), or selecting random regions for
the median smoothing instead of a ﬁxed 2 × 2 region).
VI.Conclusion
The eﬀectiveness of feature squeezing seems surprising
since it is so simple and inexpensive compared to other pro-
posed defenses. Developing a theory of adversarial examples
remains an illusive goal, but our intuition is that the eﬀective-
ness of squeezing stems from how it reduces the search space
of possible perturbations available to an adversary.
The bit depth reduction squeezers work essentially elimi-
nate some of the lower bits, shrinking the feature space and
forcing the adversary to produce larger perturbations. Since
the features we eﬀectively eliminate are not relevant for clas-
siﬁcation, this has little impact on the accuracy of legitimate
samples. Bit depth reduction is eﬀective in mitigating the
adversarial examples generated by L∞ attacks, and appears to
be more eﬀective against more advanced attacks since they
use smaller perturbations. The spatial smoothing squeezers
make pixels less diﬀerent across an image, mitigating L0
perturbations.
As discussed in Section V-D, feature squeezing is not im-
mune to adversarial adaptation, but it substantially changes the
challenge an adversary faces. Our general detection framework
opens a new research direction in defending against adversarial
examples and understanding the limits of deep neural networks
in adversarial contexts.
14
644015412842716001020304050607000.10.20.30.40.50.60.70.80.91Success	Rate	%Clipped	εTargeted	(Next)Targeted(LL)UntargetedReferences
[1] Rodrigo Benenson. Classiﬁcation datasets results. http://rodrigob.github.
io/are we there yet/build/classiﬁcation datasets results.html.
[2] Christopher M Bishop. Pattern Recognition and Machine Learning.
Springer, 2006.
[25]
[3] Nicholas Carlini. Robust evasion attacks against neural network to ﬁnd
adversarial examples. https://github.com/carlini/nn robust attacks/.
[4] Nicholas Carlini, Pratyush Mishra, Tavish Vaidya, Yuankai Zhang,
Micah Sherr, Clay Shields, David Wagner, and Wenchao Zhou. Hidden
voice commands. In USENIX Security Symposium, 2016.
[5] Nicholas Carlini and David Wagner. Defensive Distillation is not robust
to adversarial examples. arXiv preprint arXiv:1607.04311, 2016.
[6] Nicholas Carlini and David Wagner. Towards evaluating the robustness
In IEEE Symposium on Security and Privacy
of neural networks.
(Oakland), 2017.
[7] Francois Chollet. Keras Implementation of Inception v3. https://github.
com/fchollet/deep-learning-models/blob/master/inception v3.py.
[8] George E Dahl, Jack W Stokes, Li Deng, and Dong Yu. Large-scale
malware classiﬁcation using random projections and neural networks.
In IEEE International Conference on Acoustics, Speech and Signal
Processing, 2013.
[9] Reuben Feinman, Ryan R Curtin, Saurabh Shintre, and Andrew B
Gardner. Detecting adversarial samples from artifacts. arXiv preprint
arXiv:1703.00410, 2017.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining
In International Conference on
and harnessing adversarial examples.
Learning Representations (ICLR), 2015.
[10]
[11] Kathrin Grosse, Praveen Manoharan, Nicolas Papernot, Michael Backes,
and Patrick McDaniel. On the (statistical) detection of adversarial
examples. arXiv preprint arXiv:1702.06280, 2017.
[12] Shixiang Gu and Luca Rigazio. Towards deep neural network architec-
tures robust to adversarial examples. arXiv preprint arXiv:1412.5068,
2014.
[13] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Di-
amos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta,
Adam Coates, and others. DeepSpeech: Scaling up end-to-end speech
recognition. arXiv preprint arXiv:1412.5567, 2014.
[14] Warren He, James Wei, Xinyun Chen, Nicholas Carlini, and Dawn
Song. Adversarial example defenses: Ensembles of weak defenses
are not strong. In 11th USENIX Workshop on Oﬀensive Technologies
(WOOT 17), 2017.
[15] Geoﬀrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever,
and Ruslan R Salakhutdinov. Improving neural networks by preventing
co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580,
2012.
[16] Hossein Hosseini, Sreeram Kannan, Baosen Zhang, and Radha Pooven-
dran. Deceiving google’s perspective api built for detecting toxic
comments. arXiv preprint arXiv:1702.08138, 2017.
[17] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko,
Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam.
Mobilenets: Eﬃcient convolutional neural networks for mobile vision
applications. arXiv preprint arXiv:1704.04861, 2017.
[18] Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der
Maaten. Densely connected convolutional networks. arXiv preprint
arXiv:1608.06993, 2016.
[19] Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton.
ImageNet
Classiﬁcation with Deep Convolutional Neural Networks. In Advances
in Neural Information Processing Systems (NIPS), 2012.
[20] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial ex-
amples in the physical world. In International Conference on Learning
Representations (ICLR) Workshop, 2017.
[21] Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haﬀner.
Gradient-based learning applied to document recognition. Proceedings
of the IEEE, 1998.
[22] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris
Tsipras, and Adrian Vladu. Towards deep learning models resistant
to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.
[23] Somshubra Majumdar. DenseNet Implementation in Keras. https://github.
com/titu1994/DenseNet/.
[24] Somshubra Majumdar. Keras Implementation of Mobile Networks.
https://github.com/titu1994/MobileNetworks/.
Jan Hendrik Metzen, Tim Genewein, Volker Fischer, and Bastian
arXiv preprint
Bischoﬀ. On detecting adversarial perturbations.
arXiv:1702.04267, 2017.
[26] Microsoft Corporation. Microsoft Malware Competition Challenge.
https://www.kaggle.com/c/malware-classiﬁcation, 2015.
[27] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and
Pascal Frossard. Universal adversarial perturbations. https://github.com/
LTS4/universal/.
[28] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi,
and Pascal
Frossard. DeepFool: a simple and accurate method to fool deep
neural networks. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2016.
[29] Anh Nguyen, Jason Yosinski, and Jeﬀ Clune. Deep neural networks are
easily fooled: High conﬁdence predictions for unrecognizable images.
In IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), 2015.
[30] Nicolas Papernot, Ian Goodfellow, Ryan Sheatsley, Reuben Feinman,
cleverhans v1.0.0: an adversarial machine
and Patrick McDaniel.
learning library. arXiv preprint arXiv:1610.00768, 2016.
[31] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha,
Z Berkay Celik, and Ananthram Swami. Practical black-box attacks
arXiv
against deep learning systems using adversarial examples.
preprint arXiv:1602.02697, 2016.
[32] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson,
Z Berkay Celik, and Ananthram Swami. The limitations of deep
In IEEE European Symposium on
learning in adversarial settings.
Security and Privacy (EuroS&P), 2016.
[33] Nicolas Papernot, Patrick McDaniel, Arunesh Sinha, and Michael
Wellman. Towards the Science of Security and Privacy in Machine
Learning. arXiv preprint arXiv:1611.03814, 2016.
[34] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Anan-
thram Swami. Distillation as a Defense to Adversarial Perturbations
against Deep Neural Networks. In IEEE Symposium on Security and
Privacy (Oakland), 2016.
[35] O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep Face Recognition.
In British Machine Vision Conference, 2015.
[36] Scientiﬁc Computing Tools for Python Developers. Multidimensional
https://docs.scipy.org/doc/scipy/
Image Processing (scipy.ndimage).
reference/tutorial/ndimage.html, 2009.
[37] Scientiﬁc Computing Tools
for Python Developers.
(scipy.ndimage.median ﬁlter).
Filter
scipy/reference/generated/scipy.ndimage.median ﬁlter.html#scipy.ndimage.
median ﬁlter, 2017.
Median
https://docs.scipy.org/doc/
[38] Christian Szegedy, Vincent Vanhoucke, Sergey Ioﬀe, Jon Shlens, and
Zbigniew Wojna. Rethinking the inception architecture for computer vi-
sion. In IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pages 2818–2826, 2016.
[39] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna,
Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties
In International Conference on Learning Repre-
of neural networks.
sentations (ICLR), 2014.
[40] Matthew A Turk and Alex P Pentland.
Face Recognition using
In IEEE Conference on Computer Vision and Pattern
Eigenfaces.
Recognition (CVPR), 1991.
[41] Beilun Wang, Ji Gao, and Yanjun Qi. A Theoretical Framework for Ro-
bustness of (Deep) Classiﬁers Under Adversarial Noise. In International
Conference on Learning Representations (ICLR) Workshop, 2017.
[42] Wikipedia. Median Filter. Page Version ID: 760708062.
[43] Fei Zhang, Patrick PK Chan, Battista Biggio, Daniel S. Yeung, and
Fabio Roli. Adversarial Feature Selection against Evasion Attacks.
IEEE Transactions on Cybernetics, 2015.
15
TABLE VI: The non-default parameters used in this paper
in generating adversarial examples. We didn’t change the
parameters for JSMA.
Attack
FGSM
BIM
CW∞
DeepFool
CW2
CW0
L∞
L2
L0
Parameters
MNIST
CIFAR-10
ImageNet
eps
eps
eps
iter
conﬁdence
overshoot
conﬁdence
max iterations
batch size
conﬁdence
0.3
0.3
0.06
-
100
0.0156
0.0080
0.0012
5
10
5
1000
5
0.0078
0.0040
0.002
35
10
Appendix
A. Attack Parameters
B. EvadeML-Zoo
The readers could reproduce the results using EvadeML-
Zoo 8.
8https://github.com/mzweilin/EvadeML-Zoo
16