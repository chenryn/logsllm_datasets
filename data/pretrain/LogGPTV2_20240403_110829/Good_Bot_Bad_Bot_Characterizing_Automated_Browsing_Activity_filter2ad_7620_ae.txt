Time to weaponize public exploits. During the seven-
month span of this study, we observed requests that tried to
exploit ﬁve remote command execution (RCE) vulnerabilities
that went public after the start of our data collection. As
a result, we have visibility over the initial probes for these
exploits. The ﬁve RCE vulnerabilities affect the following
software/ﬁrmware: DrayTech modems
(CVE-2020-8585),
Netgear GPON router
(EDB-48225), MSSQL Reporting
Servers (CVE-2020-0618), Liferay Portal (CVE-2020-7961),
and F5 Trafﬁc Management UI (CVE-2020-5902).
For the ﬁrst vulnerability on DrayTech devices, the exploit
was released on March 29, 2020 and we observed exploitation
attempts on Aristaeus’ honeysites a mere two days later.
In a similar fashion, the exploit for Netgear devices went
public on March 18 2020, and the ﬁrst exploitation attempts
were recorded by Aristaeus on the same day. Next, the proof-
of-concept exploit for MSSQL reporting server vulnerability
went public by the researcher who reported this vulnerability
on February 14, 2020, and we received exploitation attempts
for this vulnerability 4 days later [57]. The Liferay vulnerability
went public on March 20, 2020 and exploiting requests showed
up in Aristaeus’ logs after 4 days. Finally, the F5 vulnerability
was publicly announced on June 30, 2020 and we observed
requests towards F5 TMUI shell on the same day.
Based on these ﬁve occasions, we can clearly observe
that the time window between an exploit going public and
malicious actors probing for that vulnerability is short and, in
certain cases (such as the Netgear and F5 devices) non-existent.
IX. DISCUSSION
ineffective. To combat
In this section, we ﬁrst highlight the key takeaways from
our analysis of the data that Aristaeus collected, and then
explore how the size of our infrastructure relates to the
number of bots discovered. We close by discussing Aristaeus’s
limitations as well as future work directions.
A. Key Takeaways
• Everyone is a target: Just by being online and publicly ac-
cessible, each one of Aristaeus’ honeysites attracted an average
of 37,753 requests per month, 21,523 (57%) of which were
clearly malicious. Each online site is exposed to ﬁngerprinting
and a wide range of attacks, abusing both operator error (such
as, common passwords) as well as recently-released exploits.
• Most generic bot requests are generated by rudimentary
HTTP libraries: Throughout our data analysis, we observed
that 99.3% of the bots that visit our websites do not
support JavaScript. This renders the state-of-the-art browser
ﬁngerprinting that is based on advanced browser APIs and
JavaScript,
this, we demonstrated
that TLS ﬁngerprinting can be used to accurately ﬁngerprint
browsing environments based on common HTTP libraries.
• Most bots are located in residential IP space: Through
our experiments, we observed that the majority (64.37%) of
bot IP addresses were residential ones, while only 30.36%
of IP addresses were located in data centers. This indicates
that bots use infected or otherwise proxied residential devices
to scan the Internet. We expect that requests from residential
IP space are less susceptible to rate limiting and blocklisting
compared to requests from data centers and public clouds, out
of fear of blocking residential users.
• Generic bots target low-hanging fruit: Aristaeus’ logs
reveal that 89.5% of sessions include less than 20 requests,
and less than 0.13% of sessions include over 1,000 requests.
The bruteforce attempts exhibit similar patterns: 99.6% IP
addresses issue fewer than 10 attempts per domain, while only
0.3% IP addresses issued more than 100 attempts per domain.
This indicates that most bots are highly selective and surgical
in their attacks, going after easy-to-exploit targets.
• IP blocklists are incomplete: The vast majority (87%)
of the malicious bot IPs from our honeysite logs were not
listed in popular IP blocklists. This further emphasizes the
limited beneﬁts of static IP blocklisting and therefore the
need for reactive defenses against bots. At the same time, the
poor blocklist coverage showcases the practical beneﬁts of
Aristaeus, which can discover tens of thousands of malicious
clients that are currently missing from popular blocklists.
• Exploits that go public are quickly abused: We observed
that bots start probing for vulnerable targets as quickly as on
the same day that an exploit was made public. Our ﬁndings
highlight the importance of immediately updating Internet-
facing software, as any delay, however small, can be capitalized
by automated bots to compromise systems and services.
• Fake search-engine bots are not as common as one
would expect: Contrary to our expectations, less than 0.3%
of the total requests that claimed to be a search-engine
bot were lying about their identity. This could suggest that
either search-engine bots do not receive special treatments
by websites, or that provided mechanisms for verifying the
source IP address of search-engine bots have been successful
in deterring bot authors from pretending to be search engines.
• Most generic internet bots use the same underlying
HTTP libraries: Even though Aristaeus recorded more than
10.2 million HTTPS requests from bots, these bots generated
just 558 unique TLS ﬁngerprints. We were able to trace these
ﬁngerprints back to 14 popular tools and HTTP libraries,
responsible for more than 97.2% of the total requests.
B. Size of Infrastructure
To understand how the number of Aristaeus-managed hon-
eysites affects the collected intelligence (e.g. could Aristaeus
record just as many attackers with 50 instead of 100 honeysites),
we conduct the following simulation. We pick an ordering of
honeysites at random and calculate the number of unique IP
addresses and TLS ﬁngerprints, each honeysite contributes.
Figure 7 shows the results when this simulation is repeated
100 times. We resort to simulation (instead of just ordering the
honeysites by contributing intelligence) since a new real de-
ployment would not have access to post-deployment statistics.
We observe two clearly different distributions. While one
could obtain approximately 50% of the TLS ﬁngerprints with
just 10% of the honeysites, the number of unique IP addresses
grows linearly with the number of honeysites. Our ﬁndings indi-
cate that, if the objective is the curation of TLS ﬁngeprints from
bots, a smaller infrastructure can sufﬁce yet, if the objective is
the collection of bot IP addresses, one could deploy an even
larger number of honeysites and still obtain new observations.
C. Limitations and Future Work
In this study, we deployed honeysites based on ﬁve popular
web applications. Since we discovered that many bots ﬁrst
ﬁngerprint their targets before sending exploits (Section VI-A),
our honeysites will not always capture exploit attempts towards
unsupported web applications.
By design, Aristaeus in general and honeysites in particular,
attract trafﬁc from generic bots that target every website on the
Internet. Yet high-proﬁle websites as well as speciﬁc companies,
often receive targeted bot attacks that are tailored to their infras-
tructure. These bots and their attacks will likely not be captured
by Aristaeus, unless attackers ﬁrst tried them on the public web.
As follow-up work, we plan to design honeysites that can dy-
namically react to bots, deceiving them into believing that they
are interacting with the web application that they are looking
for. Malware analysis systems already make use of variations of
Fig. 7: (Top) Number of honeysites and their coverage of bot TLS
ﬁngerprints. The opaque line is the median of 100 random experiments.
(Bottom) Number of honeysites and their coverage of bot IP addresses.
this technique to, e.g., detect malicious browser extensions [58]
and malicious websites that attempt to compromise vulnerable
browser plugins [59]. In this way, we expect that our honeysites
will be able to capture more attacks (particularly from the
single-shot scanners that currently do not send any trafﬁc past
an initial probing request) and exploit payloads.
X. RELATED WORK
Characterization and detection of crawlers. To quantify
crawler behavior and differences of crawler strategies, a number
of researchers analyzed large corpora of web server logs and
reported on their ﬁndings. These studies include the analysis
of 24 hours worth of crawler trafﬁc on microsoft.com [60], 12
weeks of trafﬁc from the website belonging to the Standard
Performance Evaluation Corporation (SPEC) [61], 12.5 years
of network trafﬁc logs collected at the Lawrence Berkeley
National Laboratory [62], as well as crawler trafﬁc on
academic networks [63] and publication libraries [64].
The security aspect of web crawlers has received signiﬁcantly
less attention compared to studies of generic crawling activity.
Most existing attempts to differentiate crawlers from real users
use differences in their navigational patterns, such as, the
percentage of HTTP methods in requests (GET/POST/HEAD
etc.), the types of links requested, and the timing between
individual requests [4]–[6]. These features are then used in one
or more supervised machine-learning algorithms trained using
ground truth that the authors of each paper were able to procure,
typically by manually labeling trafﬁc of one or more webservers
to which they had access. Xie et al. propose an ofﬂine method
for identifying malicious crawlers by searching for clusters of
requests towards non-existent resources [65]. Park et al.
[22]
investigated the possibility of detecting malicious web crawlers
by looking for mouse movement, the loading of Cascading
Style Sheets (CSS), and the following of invisible links. Jan
et al. [66] propose an ML-based bot detection scheme trained
on limited labeled bot trafﬁc from an industry partner. By
using data augmentation methods, they are able to synthesize
samples and expand their dataset used to train ML models.
In our paper, we sidestep the issue of having to manually
label trafﬁc as belonging to malicious crawlers, through the
use of honeysites, i.e., websites which are never advertised
and therefore any visitors must, by deﬁnition, either be
benign/malicious crawlers or their operators who follow-up on
a discovery from one of their bots. We therefore argue that the
access logs that we collected via our network of honeysites
can be used as ground truth in order to train more accurate
ML-based, bot detection systems.
Network telescopes and honeypots. Network telescopes and
honeypots are two classes of systems developed to study
Internet scanning activity at scale. First introduced by Moore et
al. [67], Network Telescopes observe and measure the remote
network security events by using a customized router to reroute
invalid IP address blocks trafﬁc to a collection server. These
works are effective in capturing network security events such
as DDoS attacks, Internet scanners, or worm infections. Recent
work by Richter et al. [68], applies the same principles to
89,000 CDN servers spread across 172 Class A preﬁxes and ﬁnd
that 32% all logged scan trafﬁc are the result of localized scans.
While large in scale, network telescopes either do not provide
any response or interactive actions, or support basic responses
at the network level (e.g., sending back SYN-ACK if received
a SYN from certain ports [69]). This makes them incapable of
analyzing crawler interaction with servers and web applications.
Honeypots provide decoy computing resources for the
purpose of monitoring and logging the activities of entities
that probe them. High-interaction honeypots can respond to
probes with high ﬁdelity, but are hard to set up and maintain.
In contrast, low-interaction honeypots such as Honeyd [70]
and SGNET [71] intercept trafﬁc sent to nonexistent hosts
and use simulated systems with various “personalities” to
form responses. This allows low-interaction honeypots to be
somewhat extensible while limiting their ability to respond
to the probes [72].
Our system, Aristaeus, combines some of the best properties
of both these worlds. Aristaeus is extensible and can be
automatically deployed on globally-dispersed servers. However,
unlike network telescopes and low-interactive honeypots, our
system does not restrict itself to the network layer responses
but
instead utilizes real web applications that have been
augmented to perform traditional as well as novel types of
client ﬁngerprinting. In these aspects, our work most closely
relates to the honeynet system by Canali and Balzarotti which
utilized 500 honeypot websites with known vulnerabilities
(such as SQL injections and Remote Command Execution
bugs), and studied the exploitation and post-exploitation
behavior of attackers [12]. While our systems share similarities
(such as the use of real web applications instead of mock web
applications or low-interaction webserver honeypots), our focus
is on characterizing the requests that we receive, clustering
them into crawling campaigns, and uncovering the real identity
of crawlers. In contrast, because of their setup, Canali and
Balzarotti [12] are able to characterize how exactly attackers
attempt to exploit known vulnerabilities, what types of ﬁles
they upload to the compromised servers, and how attackers
abuse the compromised servers for phishing and spamming (all
of which are well outside Aristaeus’s goals and capabilities).
XI. CONCLUSION
In this paper, we presented the design and implementation of
Aristaeus, a system for deploying and managing large numbers
of web applications which are deployed on previously-unused
domain names, for the sole purpose of attracting web bots.
Using Aristaeus, we conducted a seven-month-long, large-scale
study of crawling activity recorded at 100 globally distributed
honeysites. These honeysites captured more than 200 GB of
crawling activity, on websites that have zero organic trafﬁc.
By analyzing this data, we discovered not only the expected
bots operated by search engines, but an active and diverse
ecosystem of malicious bots that constantly probed our
infrastructure for operator errors (such as poor credentials
and sensitive ﬁles) as well as vulnerable versions of online
software. Among others, we discovered that an average
Aristaeus-managed honeysite received more than 37K requests
per month from bots, 50% of which were malicious. Out of
the 76,000 IP addresses operated by clearly malicious bots
recorded by Aristaeus, 87% of them are currently missing from
popular IP-based blocklists. We observed that malicious bots
engage in brute-force attacks, web application ﬁngerprinting,
and can rapidly add new exploits to their abusive capabilities,
even on the same day as an exploit becoming public. Finally,
through novel header-based, TLS-based, and JavaScript-based
ﬁngerprinting techniques, we uncovered the true identity of
bots ﬁnding that most bots that claim to be a popular browser
are in fact lying and are instead implemented on simple HTTP
libraries built using Python and Go.
Next to all the insights into the abuse by malicious bots,
Aristaeus allowed us to curate a dataset that is virtually
free of organic user trafﬁc which we will make available
to researchers upon publication of this paper. This bot-only
dataset can be used to better understand the dynamics of bots
and design more accurate bot-detection algorithms.
XII. AVAILABILITY
One of the main contributions of this paper is the curation
of a bot-only, trafﬁc dataset. To facilitate and advance research
on the topic of bot detection, our dataset will be available to
other researchers upon request.
ACKNOWLEDGMENT
We thank the reviewers for their valuable feedback. This
work was supported by the Ofﬁce of Naval Research under
grant N00014-20-1-2720, N00014-20-1-2858, by the National
Science Foundation under grants CNS-1813974, CNS-1941617,
and CMMI-1842020, as well as by a 2018 Amazon Research
award. Any opinions, ﬁndings, or conclusions expressed in
this material are those of the authors and do not necessarily
reﬂect the views of the sponsors.
REFERENCES
[1] A. Shirokova, “Cms brute force attacks are still a threat.” [Online].
Available: https://blogs.cisco.com/security/cms-brute-force-attacks-are-
still-a-threat
WordPress, Joomla, Drupal, and Plone.
[2] T. Canavan, CMS Security Handbook: The Comprehensive Guide for
John Wiley and Sons, 2011.
[3] Imperva, “Bad bot report 2020: Bad bots strike back.” [Online].
Available: https://www.imperva.com/resources/resource-library/reports/
2020-bad-bot-report/
[4] A. G. Lourenc¸o and O. O. Belo, “Catching web crawlers in the act,”
in Proceedings of the 6th international Conference on Web Engineering,
2006, pp. 265–272.
[5] P.-N. Tan and V. Kumar, “Discovery of web robot sessions based on
their navigational patterns,” in Intelligent Technologies for Information
Analysis. Springer, 2004, pp. 193–222.
[6] G. Jacob, E. Kirda, C. Kruegel, and G. Vigna, “Pubcrawl: Protecting
users and businesses from crawlers,” in Presented as part of the 21st
USENIX Security Symposium (USENIX Security 12), 2012, pp. 507–522.
[7] A. Vastel, W. Rudametkin, R. Rouvoy, and X. Blanc, “FP-Crawlers:
Studying the Resilience of Browser Fingerprinting to Block Crawlers,”
in MADWeb’20 - NDSS Workshop on Measurements, Attacks, and
Defenses for the Web.
[8] B. Amin Azad, O. Starov, P. Laperdrix, and N. Nikiforakis,
“Web Runner 2049: Evaluating Third-Party Anti-bot Services,”
Intrusions and Malware
in 17th Conference on Detection of
& Vulnerability Assessment
[Online]. Available:
https://hal.archives-ouvertes.fr/hal-02612454
(DIMVA), 2020.
[9] K. Bock, D. Patel, G. Hughey, and D. Levin, “uncaptcha: a low-resource
defeat of recaptcha’s audio challenge,” in 11th USENIX Workshop on
Offensive Technologies (WOOT 17), 2017.
[10] M. Motoyama, K. Levchenko, C. Kanich, D. McCoy, G. M. Voelker, and
S. Savage, “Re: Captchas-understanding captcha-solving services in an
economic context.” in USENIX Security Symposium, vol. 10, 2010, p. 3.
[11] S. Sivakorn, J. Polakis, and A. D. Keromytis, “I’m not a human:
Breaking the google recaptcha,” Black Hat, 2016.
[12] D. Canali and D. Balzarotti, “Behind the Scenes of Online Attacks: an
Analysis of Exploitation Behaviors on the Web,” in Proceedidngs of the
20th Network & Distributed System Security Symposium (NDSS), 2013.
[13] C. Lever, R. Walls, Y. Nadji, D. Dagon, P. McDaniel, and M. Antonakakis,
“Domain-z: 28 registrations later measuring the exploitation of residual
trust in domains,” in IEEE Symposium on Security and Privacy (SP),
2016, pp. 691–706.
[14] “Seleniumhq browser automation,” https://www.selenium.dev/.
[15] P. Eckersley, “How unique is your web browser?” in International Sym-
posium on Privacy Enhancing Technologies Symposium, 2010, pp. 1–18.
[16] N. Nikiforakis, A. Kapravelos, W. Joosen, C. Kruegel, F. Piessens, and
G. Vigna, “Cookieless monster: Exploring the ecosystem of web-based
device ﬁngerprinting,” in 2013 IEEE Symposium on Security and