### Time to Weaponize Public Exploits

During the seven-month span of this study, we observed requests attempting to exploit five remote command execution (RCE) vulnerabilities that were publicly disclosed after our data collection began. This provided us with visibility into the initial probes for these exploits. The five RCE vulnerabilities affect the following software/firmware: DrayTek modems (CVE-2020-8585), Netgear GPON router (EDB-48225), MSSQL Reporting Servers (CVE-2020-0618), Liferay Portal (CVE-2020-7961), and F5 Traffic Management UI (CVE-2020-5902).

For the DrayTek devices, the exploit was released on March 29, 2020, and we observed exploitation attempts on Aristaeus' honeysites just two days later. Similarly, the exploit for Netgear devices went public on March 18, 2020, and the first exploitation attempts were recorded by Aristaeus on the same day. The proof-of-concept exploit for the MSSQL reporting server vulnerability was published on February 14, 2020, and we received exploitation attempts four days later [57]. The Liferay vulnerability was made public on March 20, 2020, and exploiting requests appeared in Aristaeus' logs four days later. Finally, the F5 vulnerability was publicly announced on June 30, 2020, and we observed requests targeting the F5 TMUI shell on the same day.

Based on these five instances, it is evident that the time window between an exploit going public and malicious actors probing for that vulnerability is short, and in some cases, such as the Netgear and F5 devices, non-existent.

### IX. Discussion

#### A. Key Takeaways
- **Everyone is a Target**: Simply by being online and publicly accessible, each of Aristaeus' honeysites attracted an average of 37,753 requests per month, with 21,523 (57%) of these requests being clearly malicious. Each online site is exposed to fingerprinting and a wide range of attacks, including those that exploit operator errors (such as common passwords) and recently-released exploits.
- **Most Generic Bot Requests Use Basic HTTP Libraries**: Our data analysis revealed that 99.3% of the bots visiting our websites do not support JavaScript. This makes advanced browser fingerprinting based on JavaScript and browser APIs ineffective. Instead, TLS fingerprinting can accurately identify browsing environments using common HTTP libraries.
- **Most Bots Are Located in Residential IP Space**: We observed that 64.37% of bot IP addresses were residential, while only 30.36% were located in data centers. This suggests that bots use infected or otherwise proxied residential devices to scan the internet, likely because residential IPs are less susceptible to rate limiting and blocklisting compared to data center and public cloud IPs.
- **Generic Bots Target Low-Hanging Fruit**: Aristaeus' logs show that 89.5% of sessions include fewer than 20 requests, and less than 0.13% of sessions include over 1,000 requests. Brute-force attempts follow similar patterns: 99.6% of IP addresses issue fewer than 10 attempts per domain, while only 0.3% issue more than 100 attempts per domain. This indicates that most bots are highly selective, focusing on easy-to-exploit targets.
- **IP Blocklists Are Incomplete**: 87% of the malicious bot IPs from our honeysite logs were not listed in popular IP blocklists. This highlights the limited benefits of static IP blocklisting and the need for reactive defenses against bots. Additionally, the poor blocklist coverage underscores the practical benefits of Aristaeus, which can discover tens of thousands of malicious clients missing from popular blocklists.
- **Public Exploits Are Quickly Abused**: Bots start probing for vulnerable targets as soon as an exploit is made public, sometimes on the same day. This emphasizes the importance of immediately updating internet-facing software, as any delay can be exploited by automated bots to compromise systems and services.
- **Fake Search-Engine Bots Are Rare**: Contrary to expectations, less than 0.3% of the total requests claiming to be search-engine bots were lying about their identity. This could suggest that either search-engine bots do not receive special treatment by websites, or that mechanisms for verifying the source IP address of search-engine bots have been successful in deterring bot authors from pretending to be search engines.
- **Most Generic Internet Bots Use the Same Underlying HTTP Libraries**: Despite recording over 10.2 million HTTPS requests from bots, these bots generated just 558 unique TLS fingerprints. These fingerprints traced back to 14 popular tools and HTTP libraries, responsible for more than 97.2% of the total requests.

#### B. Size of Infrastructure
To understand how the number of Aristaeus-managed honeysites affects the collected intelligence, we conducted a simulation. We randomly ordered the honeysites and calculated the number of unique IP addresses and TLS fingerprints each honeysite contributed. Figure 7 shows the results when this simulation was repeated 100 times. We used simulation instead of ordering the honeysites by contributing intelligence because a new real deployment would not have access to post-deployment statistics.

We observed two distinct distributions. While one could obtain approximately 50% of the TLS fingerprints with just 10% of the honeysites, the number of unique IP addresses grows linearly with the number of honeysites. Our findings indicate that, if the objective is to curate TLS fingerprints from bots, a smaller infrastructure may suffice. However, if the goal is to collect bot IP addresses, deploying a larger number of honeysites would still yield new observations.

#### C. Limitations and Future Work
In this study, we deployed honeysites based on five popular web applications. Since many bots first fingerprint their targets before sending exploits (Section VI-A), our honeysites will not always capture exploit attempts towards unsupported web applications.

By design, Aristaeus and its honeysites attract traffic from generic bots that target every website on the internet. High-profile websites and specific companies often receive targeted bot attacks tailored to their infrastructure, which may not be captured by Aristaeus unless attackers first test them on the public web.

As follow-up work, we plan to design honeysites that can dynamically react to bots, deceiving them into believing they are interacting with the web application they are looking for. Malware analysis systems already use variations of this technique to detect malicious browser extensions [58] and malicious websites that attempt to compromise vulnerable browser plugins [59]. We expect that our honeysites will be able to capture more attacks, particularly from single-shot scanners that currently do not send any traffic past an initial probing request, and exploit payloads.

### X. Related Work
#### Characterization and Detection of Crawlers
To quantify crawler behavior and differences in crawler strategies, several researchers have analyzed large corpora of web server logs. These studies include the analysis of 24 hours of crawler traffic on microsoft.com [60], 12 weeks of traffic from the Standard Performance Evaluation Corporation (SPEC) website [61], 12.5 years of network traffic logs from the Lawrence Berkeley National Laboratory [62], and crawler traffic on academic networks [63] and publication libraries [64].

The security aspect of web crawlers has received less attention compared to studies of generic crawling activity. Most existing methods to differentiate crawlers from real users use differences in navigational patterns, such as the percentage of HTTP methods in requests (GET/POST/HEAD, etc.), types of links requested, and timing between individual requests [4]–[6]. These features are then used in supervised machine-learning algorithms trained on ground truth data, typically manually labeled traffic from one or more web servers. Xie et al. propose an offline method for identifying malicious crawlers by searching for clusters of requests towards non-existent resources [65]. Park et al. [22] investigated the possibility of detecting malicious web crawlers by looking for mouse movement, the loading of Cascading Style Sheets (CSS), and the following of invisible links. Jan et al. [66] propose an ML-based bot detection scheme trained on limited labeled bot traffic from an industry partner, using data augmentation methods to expand their dataset.

In our paper, we sidestep the issue of manually labeling traffic as belonging to malicious crawlers by using honeysites, i.e., websites that are never advertised and thus any visitors must, by definition, be benign or malicious crawlers or their operators. We argue that the access logs collected via our network of honeysites can be used as ground truth to train more accurate ML-based bot detection systems.

#### Network Telescopes and Honeypots
Network telescopes and honeypots are two classes of systems developed to study internet scanning activity at scale. First introduced by Moore et al. [67], network telescopes observe and measure remote network security events by rerouting invalid IP address blocks to a collection server. These systems are effective in capturing network security events such as DDoS attacks, internet scanners, and worm infections. Recent work by Richter et al. [68] applies the same principles to 89,000 CDN servers spread across 172 Class A prefixes and finds that 32% of all logged scan traffic are the result of localized scans. While large in scale, network telescopes either do not provide any response or interactive actions, or support basic responses at the network level, making them incapable of analyzing crawler interaction with servers and web applications.

Honeypots provide decoy computing resources for monitoring and logging the activities of entities that probe them. High-interaction honeypots can respond to probes with high fidelity but are hard to set up and maintain. In contrast, low-interaction honeypots such as Honeyd [70] and SGNET [71] intercept traffic sent to nonexistent hosts and use simulated systems with various "personalities" to form responses. This allows low-interaction honeypots to be somewhat extensible while limiting their ability to respond to probes [72].

Our system, Aristaeus, combines the best properties of both worlds. Aristaeus is extensible and can be automatically deployed on globally dispersed servers. Unlike network telescopes and low-interactive honeypots, our system does not restrict itself to network layer responses but instead uses real web applications augmented to perform traditional and novel types of client fingerprinting. In these aspects, our work most closely relates to the honeynet system by Canali and Balzarotti, which utilized 500 honeypot websites with known vulnerabilities (such as SQL injections and Remote Command Execution bugs) and studied the exploitation and post-exploitation behavior of attackers [12]. While our systems share similarities (such as the use of real web applications instead of mock web applications or low-interaction webserver honeypots), our focus is on characterizing the requests we receive, clustering them into crawling campaigns, and uncovering the real identity of crawlers. In contrast, Canali and Balzarotti [12] characterize how exactly attackers attempt to exploit known vulnerabilities, what types of files they upload to compromised servers, and how attackers abuse the compromised servers for phishing and spamming (all of which are well outside Aristaeus's goals and capabilities).

### XI. Conclusion
In this paper, we presented the design and implementation of Aristaeus, a system for deploying and managing large numbers of web applications on previously-unused domain names to attract web bots. Using Aristaeus, we conducted a seven-month-long, large-scale study of crawling activity recorded at 100 globally distributed honeysites. These honeysites captured more than 200 GB of crawling activity on websites with zero organic traffic.

By analyzing this data, we discovered not only the expected bots operated by search engines but also an active and diverse ecosystem of malicious bots that constantly probed our infrastructure for operator errors (such as poor credentials and sensitive files) and vulnerable versions of online software. Among other findings, we discovered that an average Aristaeus-managed honeysite received more than 37K requests per month from bots, 50% of which were malicious. Out of the 76,000 IP addresses operated by clearly malicious bots recorded by Aristaeus, 87% are currently missing from popular IP-based blocklists. We observed that malicious bots engage in brute-force attacks, web application fingerprinting, and can rapidly add new exploits to their abusive capabilities, even on the same day as an exploit becoming public. Finally, through novel header-based, TLS-based, and JavaScript-based fingerprinting techniques, we uncovered the true identity of bots, finding that most bots that claim to be a popular browser are in fact lying and are instead implemented on simple HTTP libraries built using Python and Go.

In addition to providing insights into the abuse by malicious bots, Aristaeus allowed us to curate a dataset virtually free of organic user traffic, which we will make available to researchers upon publication of this paper. This bot-only dataset can be used to better understand the dynamics of bots and design more accurate bot-detection algorithms.

### XII. Availability
One of the main contributions of this paper is the curation of a bot-only traffic dataset. To facilitate and advance research on the topic of bot detection, our dataset will be available to other researchers upon request.

### Acknowledgment
We thank the reviewers for their valuable feedback. This work was supported by the Office of Naval Research under grants N00014-20-1-2720 and N00014-20-1-2858, the National Science Foundation under grants CNS-1813974, CNS-1941617, and CMMI-1842020, and a 2018 Amazon Research award. Any opinions, findings, or conclusions expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors.

### References
[1] A. Shirokova, “CMS brute force attacks are still a threat.” [Online]. Available: https://blogs.cisco.com/security/cms-brute-force-attacks-are-still-a-threat
[2] T. Canavan, CMS Security Handbook: The Comprehensive Guide for John Wiley and Sons, 2011.
[3] Imperva, “Bad bot report 2020: Bad bots strike back.” [Online]. Available: https://www.imperva.com/resources/resource-library/reports/2020-bad-bot-report/
[4] A. G. Lourenço and O. O. Belo, “Catching web crawlers in the act,” in Proceedings of the 6th International Conference on Web Engineering, 2006, pp. 265–272.
[5] P.-N. Tan and V. Kumar, “Discovery of web robot sessions based on their navigational patterns,” in Intelligent Technologies for Information Analysis. Springer, 2004, pp. 193–222.
[6] G. Jacob, E. Kirda, C. Kruegel, and G. Vigna, “Pubcrawl: Protecting users and businesses from crawlers,” in Presented as part of the 21st USENIX Security Symposium (USENIX Security 12), 2012, pp. 507–522.
[7] A. Vastel, W. Rudametkin, R. Rouvoy, and X. Blanc, “FP-Crawlers: Studying the Resilience of Browser Fingerprinting to Block Crawlers,” in MADWeb’20 - NDSS Workshop on Measurements, Attacks, and Defenses for the Web.
[8] B. Amin Azad, O. Starov, P. Laperdrix, and N. Nikiforakis, “Web Runner 2049: Evaluating Third-Party Anti-bot Services,” in 17th Conference on Detection of Intrusions and Malware & Vulnerability Assessment (DIMVA), 2020. [Online]. Available: https://hal.archives-ouvertes.fr/hal-02612454
[9] K. Bock, D. Patel, G. Hughey, and D. Levin, “uncaptcha: a low-resource defeat of reCAPTCHA’s audio challenge,” in 11th USENIX Workshop on Offensive Technologies (WOOT 17), 2017.
[10] M. Motoyama, K. Levchenko, C. Kanich, D. McCoy, G. M. Voelker, and S. Savage, “Re: CAPTCHAs—understanding CAPTCHA-solving services in an economic context.” in USENIX Security Symposium, vol. 10, 2010, p. 3.
[11] S. Sivakorn, J. Polakis, and A. D. Keromytis, “I’m not a human: Breaking the Google reCAPTCHA,” Black Hat, 2016.
[12] D. Canali and D. Balzarotti, “Behind the Scenes of Online Attacks: An Analysis of Exploitation Behaviors on the Web,” in Proceedings of the 20th Network & Distributed System Security Symposium (NDSS), 2013.
[13] C. Lever, R. Walls, Y. Nadji, D. Dagon, P. McDaniel, and M. Antonakakis, “Domain-Z: 28 registrations later measuring the exploitation of residual trust in domains,” in IEEE Symposium on Security and Privacy (SP), 2016, pp. 691–706.
[14] “SeleniumHQ Browser Automation,” https://www.selenium.dev/.
[15] P. Eckersley, “How unique is your web browser?” in International Symposium on Privacy Enhancing Technologies Symposium, 2010, pp. 1–18.
[16] N. Nikiforakis, A. Kapravelos, W. Joosen, C. Kruegel, F. Piessens, and G. Vigna, “Cookieless Monster: Exploring the Ecosystem of Web-Based Device Fingerprinting,” in 2013 IEEE Symposium on Security and Privacy (SP), 2013, pp. 440–454.