interesting, in terms of synergy of adversarial uncertainty and dif-
ferential privacy methods is the case where n is between 1050 and
1350. Here one can see that adding signiﬁcantly less noise than
using standard differential privacy approach is sufﬁcient to obtain
desired parameter ε = 0.2.
To sum up all our results, we present a ﬂowchart, which shows
on high level of abstraction how should the data owner approach the
problem of preserving privacy in a general manner. See Figure 6.
7. PREVIOUS AND RELATED WORK
Our paper can be seen as an extension of the ideas introduced
in [5]. The authors of [5] proposed a new insight considering re-
laxation of differential privacy which utilizes the uncertainty of the
adversary. This was done in a contrast to standard differential pri-
vacy, which assumed that the uncertainty has to be injected by the
data owner
Any
assumptions
about
data/adversary?
no
Use
standard
differential
privacy
methods
(see [16])
yes
Initialize
adversarial
model for
data vector
(see Def-
inition 5)
yes
Is data
independent
(D = 1)?
no
Satisﬁed with
privacy
parameters?
Utilize
adversarial
uncertainty
for
dependent
case (use
Theorem 5)
Utilize
adversarial
uncertainty
for
independent
case (use
Theorem 4)
yes
no
Enhance
privacy
by adding
noise (use
Theorem 6)
Release
aggregated
statistic
Figure 6: A ﬂowchart for privacy preserving in a general way.
554randomized mechanism. Obviously the notion of differential pri-
vacy is quite pessimistic, as we assume that the adversary knows
almost everything. In many cases it makes differential privacy un-
usable in practice. The necessity to add noise to the ﬁnal output
may render the data completely useless. Imagine situation where
we want to do a taxation audit. The aggregator collects the amount
of taxes paid by the individuals and then publish their sum. After
adding a noise, this sum will be different than the tax due, but now
we do not know whether it is because of the noise added, or if there
is some tax evasion undergoing. Very similar example, and also
some other, were given in [5]. This might be an extreme example,
but nevertheless, a big magnitude of noise (say linear of the size of
the data itself) would be problematic in most practical situations.
One such case is discussed in paper [20], where the magnitude of
noises for practical cases is huge, despite good asymptotic proper-
ties of the protocol.
In our paper we use the same model as in [5]. However, here it
is presented in a different way, which is more convenient for our
proofs. The results we give are more detailed (non-asymptotic) and
easy to use in practice and concern any type of data. To the best of
our knowledge, previous work in noiseless privacy and its deriva-
tives or generalizations consisted of asymptotic analysis only. The
unknown constants hidden in the big oh notation makes it difﬁcult
to construct practical algorithms. Furthermore, we also give results
for data with (limited) dependencies, which did not appear in [5]
(apart from simple examples). Moreover, we showed that one can
combine noiseless privacy with standard approach, namely adding
some noise. It turns out that one can enhance the inherent random-
ness and reach desired level of privacy with less noise than using
standard approach.
There are many other papers that should be mentioned as a re-
lated work. Apart from [5] there were also very interesting and
important papers concerning various approaches to leveraging ad-
versarial uncertainty in privacy, especially [4, 24].
Both in [4] and [24] the authors proposed a frameworks (called
”coupled-worlds privacy” and ”Pufferﬁsh”, respectively) for spec-
ifying privacy deﬁnitions utilizing adversarial uncertainty. They
could be instantiated in various ways, one of which boils down to
noiseless privacy. These papers are important generalizations of
ideas in [5], however the main goal of its authors is extending and
generalizing privacy deﬁnitions. Our paper, on the other hand, fo-
cuses on extending the types of data which have good noiseless
privacy parameters, on introducing dependencies in the data and
combining noiseless privacy with standard approach. Moreover,
we focus on detailed results which can be easily applied in real-life
scenarios of data aggregation.
Another paper that is somehow related to this one is [26], where
the authors utilized sampling to enhance privacy. They have also
given non-asymptotic privacy guarantees. However, the authors
of [26] show how we can get differentially private data using k-
anonymity by a simple sampling. On the other hand we consider
the problem of aggregation of dependent data. We believe that such
approach is more adequate for real-life scenarios. The model we in-
vestigated (revealed data) is substantially different. In particular we
deal with aggregated data from (possibly) dependent sources. The
authors of [26] have also proposed a theorem which is essentially
very similar to our Theorem 1. Note, however, that this theorem is
just a toy scenario in our paper, as we focus on any kind of data, not
limiting ourselves to speciﬁc distribution. Moreover, we introduce
local dependencies in the data.
Obviously, our paper is also strongly related to any work con-
cerning data aggregation under differential privacy regime, whether
the data is centralized or distributed.
Our results can for example be used in [35] wherein authors con-
struct a mechanism that allows the untrusted aggregator to learn
only the intended statistics but no additional information. More-
over the statistics revealed to the aggregator satisfy differential pri-
vacy. The result is obtained by combining applied cryptography
techniques to hide partial results with regular methods used for pri-
vacy preserving for the ﬁnal result, which can be omitted under
noiseless privacy regime, thus not introducing any errors.
There is a long line of papers concerning similar problems as in
[35], for example two other notable papers [31] and [32]. In both
of them, the authors use a substantially different model of security.
Moreover in the latter the users communicate between each other,
while in [35] as well as in our paper we assume that there is a
communication between aggregator and individual users only.
Note that most of protocols described in these related papers fail
to provide the correct output even if only a single user abstains from
sending his share of the input. The solutions for dynamic networks
have been presented in [20] and [8]. Approach based on [35] was
also focused on more advanced particular processing of aggregated
data (e.g., evaluation and monetization) while keeping privacy of
users is discussed in several papers ([2, 17, 6, 30]). Another vain
of protocols represent [1, 21] wherein authors present some aggre-
gation methods that preserve privacy, however they do not consider
dynamic changes inside of the network. The latter also considers
data poisoning attacks, however the authors do not provide rigid
proofs. In [29, 34] the authors present a framework for some ag-
gregation functions and consider the conﬁdentiality of the result,
but leaving nodes’ privacy out of scope. Clearly there are many pa-
pers discussing aggregation protocols without considering security
nor privacy issues (e.g., [22, 27]). There is a long list of papers
devoted to fault tolerant aggregation protocols ([19, 23, 25]) for
signiﬁcantly different settings.
One could use the notion of noiseless privacy, especially the ex-
plicit results given in our paper, to get rid of the noise addition
(thus, the error introduced in result of a query) in many protocols
in papers mentioned in this section.
As a related work we shall point also a huge body of papers
dealing with differential privacy notions and their extension. The
idea of differential privacy has been introduced for the ﬁrst time
in [15], however its precise formulation in the widely used form
appeared in [11]. Most important properties have been introduced
in papers [13, 14]. There is a long list of papers that can be seen as a
direct extension of [15] i.e., [6, 13]. In all that papers a substantially
different trust model is used. Namely there is a party called curator
that is entitled to see all participants’ data in the clear and releases
the computed data to wider (possibly untrusted) audience.
Paper [28] presents aggregation of elements of dataset from per-
spective of preserving differential privacy. The presented frame-
work signiﬁcantly differs from our approach in a few points. First
of all, it uses adding noise to raw data.
An introduction to differential privacy can be found in [12]. An
excellent, comprehensive description of recent results can be found
in [16].
8. CONCLUSIONS AND FURTHER WORK
We have shown an explicit bounds for privacy parameters in the
case where we can utilize adversarial uncertainty. We have pre-
sented speciﬁc model of privacy (which boils down to the one given
in [5]) and introduced model of the adversary. To the best of our
knowledge, in the papers concerning leveraging inherent random-
ness in the data there were only asymptotic results so far. By show-
ing an explicit guarantees for privacy parameters, we have made
the whole idea more approachable in practice.
555Another important contribution of this paper is approaching de-
pendent data, namely using the notion of dependency neighbor-
hoods. To the best of authors knowledge, such approach has not
appeared yet in the literature concerning utilizing adversarial un-
certainty to give privacy guarantees. There were some very simple
cases, but here we give privacy guarantees for any distribution for a
wide class of dependencies. Namely we only need to know the size
of the largest dependent subset (or the upper bound for the size)
Moreover, we have shown the parameters regardless of the distri-
bution of the data. The data owner only has to plug the variance of
the data (or the lower bound for variance), data sensitivity (which
is also necessary in standard differential privacy approach) and ap-
propriate central moments. Then he can give a speciﬁc privacy
guarantee to its users that as long as at most γ is compromised and
as long as the greatest dependent subset has size D. The simplic-
ity of usage for practitioners was very important in this paper. We
want these theorems to be usable not only by the privacy experts,
but any speciﬁc domain experts, so we have made the theorems sort
of ’off-the-shelf’ formulas to use.
Furthermore, we have shown how does the standard differential
privacy approach combines with the notion of inherent randomness
in the data. It turns out that the intuition that if the data is more
’random’, then less noise is necessary to achieve speciﬁc privacy
parameter. We formalize and quantify the level of privacy enhance-
ment. To the best of our knowledge, such attempt was not presented
before in the privacy literature. So far the only attempts were either
’all’ (as in standard differential privacy methods) or ’none’ (as in
for example [4, 5, 24]). Here we give the data owner the possibility
to maintain a tradeoff between these two approaches.
Some questions are still left unanswered and they might be quite
interesting both from practicioner’s point of view as well as for the
theory. We leave them as a future work.
• How the database (or distributed system) designer should de-
cide about the level of randomness in the database? In other
words, even though in many papers we are given various
frameworks to instantiate a speciﬁc scenario, how should the
practitioner decide which instance to use? Even though we
give quite a wide choice for the practitioner (he only needs
upper bounds for compromised users, variance and, in de-
pendent case, the size of greatest dependent subset) it still
might be cumbersome in some cases. A general method for
such a problem would be of great practical value.
• We hope to ﬁnd an even more precise approach to connect
the randomness in data with its privacy level. A promising
direction is to use notion of min entropy notion (see e.g., [9])
of data source assuming limited dependencies between val-
ues kept by users.
• Finding a way to improve also the δ parameter (we have al-
ready shown how to improve ) by adding some noise (albeit
less than in standard differential privacy) might be very in-
teresting and useful as well.
9. ACKNOWLEDGMENTS
Krzysztof Grining is supported by NCN Polish National Science
Center (grant number 2015/17/B/ST6/01897). Marek Klonowski is
also supported by NCN (grant number 2013/09/B/ST6/02258).
10. REFERENCES
[1] PDA: Privacy-Preserving Data Aggregation in Wireless
Sensor Networks, 2007.
[2] G. Ács and C. Castelluccia. I have a dream! (differentially
private smart metering). In T. Filler, T. Pevný, S. Craver, and
A. D. Ker, editors, Information Hiding - 13th International
Conference, IH 2011, Prague, Czech Republic, May 18-20,
2011, Revised Selected Papers, volume 6958 of Lecture
Notes in Computer Science, pages 118–132. Springer, 2011.
[3] A. D. Barbour and L. H. Chen. ’An Introduction to Stein”s
Method’, volume 4. World Scientiﬁc, 2005.
[4] R. Bassily, A. Groce, J. Katz, and A. Smith. Coupled-worlds
privacy: Exploiting adversarial uncertainty in statistical data
privacy. In Foundations of Computer Science (FOCS), 2013
IEEE 54th Annual Symposium on, pages 439–448. IEEE,
2013.
[5] R. Bhaskar, A. Bhowmick, V. Goyal, S. Laxman, and
A. Thakurta. Noiseless database privacy. In International
Conference on the Theory and Application of Cryptology
and Information Security, pages 215–232. Springer, 2011.
[6] I. Bilogrevic, J. Freudiger, E. D. Cristofaro, and E. Uzun.
What’s the gist? privacy-preserving aggregation of user
proﬁles. In M. Kutylowski and J. Vaidya, editors, Computer
Security - ESORICS 2014 - 19th European Symposium on
Research in Computer Security, Wroclaw, Poland, September
7-11, 2014. Proceedings, Part II, volume 8713 of Lecture
Notes in Computer Science, pages 128–145. Springer, 2014.
[7] T.-H. H. Chan, E. Shi, and D. Song. Optimal lower bound for
differentially private multi-party aggregation. IACR
Cryptology ePrint Archive, 2012:373, 2012. informal
publication.
[8] T.-H. H. Chan, E. Shi, and D. Song. Privacy-preserving
stream aggregation with fault tolerance. In A. D. Keromytis,
editor, Financial Cryptography, volume 7397 of Lecture
Notes in Computer Science, pages 200–214. Springer, 2012.
[9] T. M. Cover and J. A. Thomas. Elements of information
theory (2. ed.). Wiley, 2006.
[10] D. P. Dubhashi and A. Panconesi. Concentration of measure
for the analysis of randomized algorithms. Cambridge
University Press, 2009.
[11] C. Dwork. Differential privacy. In M. Bugliesi, B. Preneel,
V. Sassone, and I. Wegener, editors, Automata, Languages
and Programming, 33rd International Colloquium, ICALP
2006, Venice, Italy, July 10-14, 2006, Proceedings, Part II,
volume 4052 of Lecture Notes in Computer Science, pages
1–12. Springer, 2006.
[12] C. Dwork. Differential privacy: A survey of results. In
M. Agrawal, D.-Z. Du, Z. Duan, and A. Li, editors, TAMC,
volume 4978 of Lecture Notes in Computer Science, pages
1–19. Springer, 2008.
[13] C. Dwork, K. Kenthapadi, F. McSherry, I. Mironov, and
M. Naor. Our data, ourselves: Privacy via distributed noise
generation. In S. Vaudenay, editor, Advances in Cryptology -
EUROCRYPT 2006, 25th Annual International Conference
on the Theory and Applications of Cryptographic
Techniques, St. Petersburg, Russia, May 28 - June 1, 2006,
Proceedings, volume 4004 of Lecture Notes in Computer
Science, pages 486–503. Springer, 2006.
[14] C. Dwork and J. Lei. Differential privacy and robust
statistics. In M. Mitzenmacher, editor, Proceedings of the
41st Annual ACM Symposium on Theory of Computing,
STOC 2009, Bethesda, MD, USA, May 31 - June 2, 2009,
pages 371–380. ACM, 2009.
[15] C. Dwork, F. McSherry, K. Nissim, and A. Smith.
Calibrating noise to sensitivity in private data analysis. In
556S. Halevi and T. Rabin, editors, Theory of Cryptography,
Third Theory of Cryptography Conference, TCC 2006, New
York, NY, USA, March 4-7, 2006, Proceedings, volume 3876
of Lecture Notes in Computer Science, pages 265–284.
Springer, 2006.
[16] C. Dwork and A. Roth. The algorithmic foundations of
differential privacy. Foundations and Trends in Theoretical
Computer Science, 9(3-4):211–407, 2014.
[17] Z. Erkin, J. R. Troncoso-Pastoriza, R. L. Lagendijk, and
F. Pérez-González. Privacy-preserving data aggregation in
smart metering systems: An overview. IEEE Signal Process.
Mag., 30(2):75–86, 2013.
[18] W. Feller. An introduction to probability theory and its
applications, volume 2. John Wiley & Sons, 2008.
[19] Y. Feng, S. Tang, and G. Dai. Fault tolerant data aggregation
scheduling with local information in wireless sensor
networks. Tsinghua Science & Technology, 16(5):451 – 463,
2011.
[20] K. Grining, M. Klonowski, and P. Syga. Practical
fault-tolerant data aggregation. In International Conference
on Applied Cryptography and Network Security, pages
386–404. Springer, 2016.
[21] W. He, X. Liu, H. Nguyen, and K. Nahrstedt. A cluster-based
protocol to enforce integrity and preserve privacy in data
aggregation. In ICDCS Workshops, pages 14–19. IEEE
Computer Society, 2009.
[22] W. R. Heinzelman, J. Kulik, and H. Balakrishnan. Adaptive
protocols for information dissemination in wireless sensor
networks. In Proceedings of the 5th Annual ACM/IEEE
International Conference on Mobile Computing and
Networking, MobiCom ’99, pages 174–185, New York, NY,
USA, 1999. ACM.
[23] A. Jhumka, M. Bradbury, and S. Saginbekov. Efﬁcient
fault-tolerant collision-free data aggregation scheduling for
wireless sensor networks. Journal of Parallel and Distributed
Computing, 74(1):1789 – 1801, 2014.
[24] D. Kifer and A. Machanavajjhala. Pufferﬁsh: A framework
for mathematical privacy deﬁnitions. ACM Transactions on
Database Systems (TODS), 39(1):3, 2014.
[25] M. Larrea, C. Martin, and J. Astrain. Hierarchical and
fault-tolerant data aggregation in wireless sensor networks.
In Wireless Pervasive Computing, 2007. ISWPC ’07. 2nd
International Symposium on, Feb 2007.
[26] N. Li, W. Qardaji, and D. Su. On sampling, anonymization,
and differential privacy or, k-anonymization meets
differential privacy. In Proceedings of the 7th ACM
Symposium on Information, Computer and Communications
Security, pages 32–33. ACM, 2012.
[27] S. Madden, M. J. Franklin, J. M. Hellerstein, and W. Hong.
Tag: A tiny aggregation service for ad-hoc sensor networks.
SIGOPS Oper. Syst. Rev., 36(SI):131–146, Dec. 2002.
[28] K. Nissim, S. Raskhodnikova, and A. Smith. Smooth
sensitivity and sampling in private data analysis. In
Proceedings of the Thirty-ninth Annual ACM Symposium on
Theory of Computing, STOC ’07, pages 75–84, New York,
NY, USA, 2007. ACM.
[29] S. Papadopoulos, A. Kiayias, and D. Papadias. Exact
in-network aggregation with integrity and conﬁdentiality.
Knowledge and Data Engineering, IEEE Transactions on,
24(10):1760–1773, Oct 2012.
[30] A. M. Piotrowska and M. Klonowski. Some remarks and
ideas about monetization of sensitive data. In
J. García-Alfaro, G. Navarro-Arribas, A. Aldini,
F. Martinelli, and N. Suri, editors, Data Privacy