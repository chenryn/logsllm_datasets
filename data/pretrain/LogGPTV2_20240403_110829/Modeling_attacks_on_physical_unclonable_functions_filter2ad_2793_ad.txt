### 13. Approximation and Stability

The approximation is valid when \( k \) is significantly larger than \( l \), a condition that holds for the PUFs under consideration due to stability requirements. According to [20], this leads to an expected number of restarts \( N_{\text{trial}} \) required to obtain a valid decision boundary on the training set (i.e., a parameter set \( \mathbf{w} \) that separates the training set):

\[
N_{\text{trial}} = O\left(\frac{d_{\Phi}}{N_{\text{CRP}} (k + 1)l}\right) = O\left(\frac{N_{\text{CRP}} \cdot l!}{(k + 1)l}\right).
\]

**Figure 3:** Graphical illustration of the effect of errors on LR in the training set, using data points from Tables 3 and 4. We used HW (cid:3).

For XOR Arb-PUFs, we know that the same challenge is applied to all \( l \) internal Arbiter PUFs. This implies that \( \Phi_i^{(j)} = \Phi_i \) for all \( j \in \{1, \ldots, l\} \) and \( i \in \{1, \ldots, k + 1\} \). Since the repetition of a component does not affect the product (recall that \( \Phi_r \cdot \Phi_r = \pm 1 \cdot \pm 1 = 1 \)), the number of distinct products can be determined by counting the unique components. The number of different products of the form \( \Phi_{i_1} \cdot \Phi_{i_2} \cdot \ldots \cdot \Phi_{i_l} \) is given by the number of \( l \)-tuples without repetition, plus the number of \((l - 2)\)-tuples without repetition (corresponding to all \( l \)-tuples with one repetition), plus the number of \((l - 4)\)-tuples without repetition (corresponding to all \( l \)-tuples with two repetitions), and so on.

### 5. Lightweight Secure PUFs

Each trial has a complexity of:

\[
T_{\text{trial}} = O((k + 1) \cdot l \cdot N_{\text{CRP}}).
\]

**LR on XOR Arbiter PUFs**

### 5.1 Machine Learning Results

To test the influence of the specific input mapping of the Lightweight PUF on its machine-learnability (see Section 2.3), we examined architectures with the following parameters: variable \( l \), \( m = 1 \), \( x = l \), and arbitrary \( s \). We focused on LR, as it was the best method for XOR Arb-PUFs, and obtained the results shown in Table 5. The design of the Lightweight PUF improves its ML resilience, particularly in terms of training times and CRPs. The given training times and prediction rates pertain to single output bits of the Lightweight PUF.

**Table 5: LR on Lightweight PUFs**

| Bit Length | Pred. Rate | No. of XORs | CRPs       | Training Time  |
|------------|------------|-------------|------------|----------------|
| 64         | 99%        | 1           | 6,000      | 8.9 sec        |
| 64         | 99%        | 2           | 12,000     | 1:28 hrs       |
| 64         | 99%        | 3           | 300,000    | 13:06 hrs      |
| 128        | 99%        | 1           | 15,000     | 40 sec         |
| 128        | 99%        | 2           | 500,000    | 59:42 min      |
| 128        | 99%        | 3           | 10^6       | 267 days       |

**Figure 4:** Double logarithmic plot of misclassification rate \( \epsilon \) on the ratio of training CRPs \( N_{\text{CRP}} \) and problem size \( \dim(\Phi) = (k + 1) \cdot l \). We used HW (cid:3).

### 5.2 Scalability

Theoretical considerations [20] show that the underlying ML problem for the Lightweight PUF and the XOR Arb-PUF are similar in terms of required CRPs, but differ quantitatively in resulting runtimes. The asymptotic formula for \( N_{\text{CRP}} \) given for the XOR Arb-PUF (Equation 12) also holds for the Lightweight PUF. However, due to the special challenge mapping of the Lightweight PUF, the number \( N_{\text{trial}} \) has a different growth rate from Equation 14. It seems to lie between:

\[
O\left(\frac{(k + 1) \cdot N_{\text{CRP}} \cdot l!}{(k + 1)l}\right) \quad \text{and} \quad O\left(\frac{N_{\text{CRP}} \cdot l!}{(k + 1)l}\right).
\]

These formulas differ by a factor of \( l! \). In our case, \( k \gg l \), and \( l \) is comparatively small for stability reasons. These considerations on \( N_{\text{CRP}} \) and \( N_{\text{trial}} \) apply to the prediction of single output bits of the Lightweight PUF.

Our scalability experiments qualitatively confirmed these points. We observed that, with the same ratio of CRPs to \( d_{\Phi} \), the LR algorithm will have a longer runtime for the Lightweight PUF than for the XOR Arb-PUF. For example, while a training set size of 12,000 for the 64-bit 4-XOR Arb-PUF required about 5 trials, the corresponding Lightweight PUF needed 100 trials. The specific challenge architecture of the Lightweight PUF thus noticeably complicates the life of an attacker in practice.

### 6. Feed-Forward Arbiter PUFs

### 6.1 Machine Learning Results

We experimented with SVMs and LR on FF Arb-PUFs, using different models and input representations, but could only break special cases with small numbers of non-overlapping FF loops, such as \( l = 1, 2 \). This is consistent with earlier results reported in [19].

The application of ES allowed us to tackle more complex FF-architectures with up to 8 FF-loops. All loops have equal length and are distributed regularly over the PUF, with overlapping start- and endpoints of successive loops, as described in Section 2.3. Table 6 shows the results. The given prediction rates are the best of 40 trials on one randomly chosen PUF instance of the respective length. The given CRP numbers are the sum of the training set and the test set employed by the attacker; a fraction of 5/6 was used as the training set, and 1/6 as the test set (see Section 2.4). We note for comparison that in-silicon implementations of 64-bit FF Arb-PUFs with 7 FF-loops have an environmental stability of 90.16% [17].

**Table 6: ES on Feed-Forward Arbiter PUFs**

| Bit Length | FF-loops | Pred. Rate (Best Run) | CRPs   | Training Time  |
|------------|----------|-----------------------|--------|----------------|
| 64         | 6        | 97.72%                | 50,000 | 07:51 min      |
| 64         | 7        | 99.38%                | 50,000 | 47:07 min      |
| 64         | 8        | 99.50%                | 50,000 | 47:07 min      |
| 64         | 9        | 98.86%                | 50,000 | 47:07 min      |
| 64         | 10       | 97.86%                | 50,000 | 47:07 min      |
| 128        | 6        | 99.11%                | 50,000 | 3:15 hrs       |
| 128        | 7        | 97.43%                | 50,000 | 3:15 hrs       |
| 128        | 8        | 98.97%                | 50,000 | 3:15 hrs       |
| 128        | 9        | 98.78%                | 50,000 | 3:15 hrs       |
| 128        | 10       | 97.31%                | 50,000 | 3:15 hrs       |

**Figure 6:** Graphical illustration of the tolerance of ES to errors. We show the best result of 40 independent trials on one randomly chosen PUF instance for varying error levels in the training set. The results hardly differ. We used HW (cid:3).

### 6.2 Results on Error-Inflicted CRPs

For the same reasons as in Section 4.2, we evaluated the performance on error-inflicted CRPs with respect to ES and FF Arb-PUFs. The results are shown in Table 7 and Figure 6. ES possesses an extremely high tolerance against the inflicted errors; its performance is hardly changed at all.

**Table 7: ES on 64-bit, 6 FF Arb-PUFs with different levels of error in the training set**

| Percentage of Error-inflicted CRPs | Best Pr. | Ave. Pr. | Suc. Tr. |
|-----------------------------------|----------|----------|----------|
| 0%                                | 98.29%   | 97.78%   | 98.33%   |
| 10%                               | 89.94%   | 88.75%   | 89.09%   |
| 2%                                | 97.68%   | 97.12%   | 97.68%   |
| 5%                                | 96.32%   | 95.85%   | 96.32%   |

**Figure 7:** Results of 10 trials per data point with ES for different numbers of FF-loops and the hyperbola fit. HW (cid:3).

### 6.3 Scalability

We started by empirically investigating the CRP growth as a function of the number of challenge bits, examining architectures of varying bit lengths that all have 6 FF-loops. The loops are distributed as described in Section 2.3. The corresponding results are shown in Figure 7. Each data point corresponds to the averaged prediction error of 10 trials on the same, random PUF instance.

Secondly, we investigated the CRP requirements as a function of a growing number of FF-loops, examining architectures with 64 bits. The corresponding results are depicted in Figure 8. Again, each data point shows the averaged prediction error of 10 trials on the same, random PUF instance.

In contrast to Sections 4.3 and 5.2, it is now much more difficult to derive reliable scalability formulas from this data. The reasons are threefold: (i) The structure of ES provides less theoretical footing for formal derivations. (ii) The random nature of ES produces a very large variance in the data points, making empirical derivations challenging. (iii) We observed an interesting effect when comparing the performance of ES vs. SVM/LR on the Arb-PUF: While the supervised ML methods SVM and LR showed a linear relationship between the prediction error \( \epsilon \) and the required CRPs even for very small \( \epsilon \), ES proved more CRP-hungry in these extreme regions, clearly showing a superlinear growth. The same effect can be expected for FF architectures, meaning that one consistent formula for extreme values of \( \epsilon \) may be difficult to obtain.

It still seems somewhat suggestive from the data points in Figures 7 and 8 to conclude that the growth in CRPs is about linear, and that the computation time grows polynomially. For the reasons given above, however, we would like to remain conservative and present the upcoming empirical formulas only in the status of a conjecture.

The data gathered in our experiments is best explained by assuming a qualitative relation of the form:

\[
N_{\text{CRP}} = O\left(\frac{s}{\epsilon^c}\right)
\]

for some constant \( 0 < c < 1 \), where \( s \) is the number of stages in the PUF. Concrete estimation from our data points leads to an approximate formula of the form:

\[
N_{\text{CRP}} \approx 9 \cdot s + \frac{1}{\epsilon^{3/4}}.
\]

**Figure 8:** Results of 10 trials per data point with ES for different numbers of FF-loops and the hyperbola fit. HW (cid:3).

The computation time required by ES is determined by the following factors: (i) The computation of the vector product \( \mathbf{w}^T \mathbf{\Phi} \), which grows linearly with \( s \). (ii) The evolution applied to this product, which is negligible compared to the other steps. (iii) The number of iterations or "generations" in ES until a small misclassification rate is achieved. We conjecture that this grows linearly with the number of multiplexers \( s \). (iv) The number of CRPs used to evaluate the individuals per iteration. If Equation 17 is valid, then \( N_{\text{CRP}} \) is on the order of \( O(s/\epsilon^c) \).

Assuming the correctness of the conjectures made in this derivation, this would lead to a polynomial growth of the computation time in terms of the relevant parameters \( k \), \( l \), and \( s \). It could then be conjectured that the number of basic computational operations \( N_{\text{BOP}} \) obeys:

\[
N_{\text{BOP}} = O(s^p)
\]

for some polynomial \( p \).