.
(13)
The approximation applies when k is considerably larger
than l, which holds for the considered PUFs for stability
reasons. Following [20], this seems to lead to an expected
number of restarts Ntrial to obtain a valid decision boundary
on the training set (that is, a parameter set (cid:2)w that separates
the training set), of
(cid:13)
(cid:14)
(cid:13)
Ntrial = O
= O
dΦ
NCRP
(k + 1)l
NCRP · l!
(cid:14)
.
(14)
(15)
Figure 3: Graphical illustration of the eﬀect of error
on LR in the training set, with chosen data points
from Tables 3 and 4. We used HW (cid:3).
i1
i2
i
il
l
j = Φi
1 · Φ
2 · . . . Φ
j(cid:2) = Φi for all j, j
for i1, i2, . . . , il ∈
ent products of the form Φ
{1, 2, . . . , k + 1} (where we say that Φk+1
= 1 for all i =
1, . . . , l). For XOR Arb-PUFs, we furthermore know that
the same challenge is applied to all l internal Arbiter PUFs,
(cid:4) ∈ {1, . . . , l}
which tells us that Φi
and i ∈ {1, . . . , k + 1}. Since a repetition of one component
does not aﬀect the product regardless of its value (recall that
Φr·Φr = ±1·±1 = 1), the number of the above products can
be obtained by counting the unrepeated components. The
number of diﬀerent products of the above form is therefore
given as the number of l-tuples without repetition, plus the
number of (l − 2)-tuples without repetition (corresponding
to all l-tuples with 1 repetition), plus the number of (l − 4)-
tuples without repetition (corresponding to all l-tuples with
2 repetitions), etc.
LR on XOR Arbiter PUFs
5. LIGHTWEIGHT SECURE PUFS
Furthermore, each trial has the complexity
Ttrial = O ( (k + 1) · l · NCRP ) .
y = 0.577 * x^(-1.007)
64 Bit, 2 XOR
64 Bit, 3 XOR
64 Bit, 4 XOR
128 Bit, 2 XOR
128 Bit, 3 XOR
CRPs/(k+1)l
5.1 Machine Learning Results
In order to test the inﬂuence of the speciﬁc input mapping
of the Lightweight PUF on its machine-learnability (see Sec.
2.3), we examined architectures with the following parame-
ters: variable l, m = 1, x = l, and arbitrary s. We focused
on LR right from the start, since this method was best in
class for XOR Arb-PUFs, and obtained the results shown
in Table 5. The speciﬁc design of the Lightweight PUF
improves its ML resilience by a notable quantitative fac-
tor, especially with respect to the training times and CRPs.
The given training times and prediction rates relate to single
output bits of the Lightweight PUF.
5.2 Scalability
1
1 1
r
o
r
r
e
n
o
i
t
c
i
d
e
r
p
1
1
1
Figure 4: Double logarithmic plot of misclassiﬁca-
tion rate  on the ratio of training CRPs NCRP and
problem size dim(Φ) = (k + 1) · l. We used HW (cid:3).
Some theoretical consideration [20] shows the underlying
ML problem for the Lightweight PUF and the XOR Arb
PUF are similar with respect to the required CRPs, but
244Bit
Pred.
Length Rate
No. of
XORs
64
99%
128
99%
3
4
5
3
4
5
CRPs
6,000
12,000
300,000
15,000
500,000
106
Training
Time
8.9 sec
1:28 hrs
13:06 hrs
40 sec
59:42 min
267 days
Table 5: LR on Lightweight PUFs. Prediction rate
refers to single output bits. Training times were
averaged over diﬀerent PUF instances. HW (cid:2).
(cid:11)
l
diﬀer quantitatively in the resulting runtimes. The asymp-
totic formula on NCRP given for the XOR Arb PUF (Eqn.
12) analogously also holds for the Lightweight PUF. But
due to the inﬂuence of the special challenge mapping of
the Lightweight PUF, the number Ntrial has a growth rate
that is diﬀerent from Eqn. 14.
It seems to lie between
(cid:5) (k + 1)
NCRP · l!
O
[20].
While these two formulas diﬀer by factor of l!, we note that
in our case k (cid:7) l, and that l is comparatively small for sta-
bility reasons. Again, all these considerations on NCRP and
NT rial hold for the prediction of single output bits of the
Lightweight PUF.
(cid:5) (k + 1)
) and the related expression O
NCRP
(cid:11)
l
These points were at least qualitatively conﬁrmed by our
scalability experiments. We observed in agreement with the
above discussion that with the same ratio CRP s/dΦ the
LR algorithm will have a longer runtime for the Lightweight
PUF than for the XOR Arb-PUF. For example, while with
a training set size of 12, 000 for the 64-bit 4-XOR Arb-PUF
on average about 5 trials were suﬃcient, for the correspond-
ing Lightweight PUF 100 trials were necessary. The speciﬁc
challenge architecture of the Lightweight PUF hence notice-
ably complicates the life of an attacker in practice.
6. FEED FORWARD ARBITER PUFS
6.1 Machine Learning Results
We experimented with SVMs and LR on FF Arb-PUFs,
using diﬀerent models and input representations, but could
only break special cases with small numbers of non-overlapp-
ing FF loops, such as l = 1, 2. This is in agreement with
earlier results reported in [19].
The application of ES ﬁnally allowed us to tackle much
more complex FF-architectures with up to 8 FF-loops. All
loops have equal length, and are distributed regularly over
the PUF, with overlapping start- and endpoints of succes-
sive loops, as described in Section 2.3. Table 6 shows the
results we obtained. The given prediction rates are the best
of 40 trials on one randomly chosen PUF-instance of the
respective length. The given CRP numbers are the sum of
the training set and the test set employed by the attacker; a
fraction of 5/6 was used as the training set, 1/6 as the test
set (see Section 2.4). We note for comparison that in-silicon
implementations of 64-bit FF Arb-PUFs with 7 FF-loops are
known to have an environmental stability of 90.16% [17].
6.2 Results on Error-Inﬂicted CRPs
For the same reasons as in Section 4.2, we evaluated the
performance on error-inﬂicted CRPs with respect to ES and
Bit
Length
FF-
loops
64
128
6
7
8
9
10
6
7
8
9
10
Pred. Rate
Best Run
97.72%
99.38%
99.50%
98.86%
97.86%
99.11%
97.43%
98.97%
98.78%
97.31%
CRPs
50,000
50,000
50,000
50,000
50,000
50,000
50,000
50,000
50,000
50,000
Training
Time
07:51 min
47:07 min
47:07 min
47:07 min
47:07 min
3:15 hrs
3:15 hrs
3:15 hrs
3:15 hrs
3:15 hrs
Table 6: ES on Feed-Forward Arbiter PUFs. Pre-
diction rates are for the best of a total of 40 trials
on a single, randomly chosen PUF instance. Train-
ing times are for a single trial. We applied Lazy
Evaluation with 2,000 CRPs. We used HW (cid:3).
FF Arb PUFs. The results are shown in Table 7 and Fig.
6. ES possesses an extremely high tolerance against the
inﬂicted errors; its performance is hardly changed at all.
6.3 Scalability
We started by empirically investigating the CRP growth
as a function of the number of challenge bits, examining
architectures of varying bitlength that all have 6 FF-loops.
The loops are distributed as described in Section 2.3. The
corresponding results are shown in Figure 7. Every data
point corresponds to the averaged prediction error of 10 tri-
als on the same, random PUF-instance.
Secondly, we investigated the CRP requirements as a func-
tion of a growing number of FF-loops, examining architec-
tures with 64 bits. The corresponding results are depicted
in Figure 8. Again, each data point shows the averaged pre-
diction error of 10 trials on the same, random PUF instance.
In contrast to the Sections 4.3 and 5.2, it is now much
more diﬃcult to derive reliable scalability formulas from this
data. The reasons are threefold. First, the structure of ES
provides less theoretical footing for formal derivations. Sec-
ond, the random nature of ES produces a very large vari-
ance in the data points, making also clean empirical deriva-
tions more diﬃcult. Third, we observed an interesting eﬀect
when comparing the performance of ES vs. SVM/LR on the
Arb PUF: While the supervised ML methods SVM and LR
showed a linear relationship between the prediction error 
and the required CRPs even for very small , ES proved more
CRP hungry in these extreme regions for , clearly showing
a superlinear growth. The same eﬀect can be expected for
CRPs
(×103)
50
Best Pr.
Ave. Pr.
Suc. Tr.
Percentage of error-inﬂicted CRPs
0%
10%
2%
5%
98.29% 97.78% 98.33% 97.68%
89.94% 88.75% 89.09% 87.91%
32.5%
42.5%
37.5%
35.0%
Table 7: ES on 64-bit, 6 FF Arb PUFs with diﬀer-
ent levels of error in the training set. We show the
best and average prediction rates from over 40 in-
dependent trials on a single, randomly chosen PUF
instance, and the percentage of successful trials that
converged to 90% or better. We used HW (cid:3).
245Figure 6: Graphical illustration of the tolerance of
ES to errors. We show the best result of 40 indepen-
dent trials on one randomly chosen PUF instance for
varying error levels in the training set. The results
hardly diﬀer. We used HW (cid:3).
FF architectures, meaning that one consistent formula for
extreme values of  may be diﬃcult to obtain.
It still seems somewhat suggestive from the data points
in Figures. 7 and 8 to conclude that the growth in CRPs is
about linear, and that the computation time grows polyno-
mially. For the reasons given above, however, we would like
to remain conservative, and present the upcoming empirical
formulas only in the status of a conjecture.
The data gathered in our experiments is best explained
by assuming a qualitative relation of the form
c
NCRP = O(s/
)
(16)
for some constant 0 < c < 1, where s is the number of stages
in the PUF. Concrete estimation from our data points leads
to an approximate formula of the form
NCRP ≈ 9 · s + 1
3/4 .
(17)
Figure 8: Results of 10 trials per data point with ES
for diﬀerent numbers of FF-loops and the hyperbola
ﬁt. HW (cid:3).
The computation time required by ES is determined by the
following factors: (i) The computation of the vector product
(cid:2)wT (cid:2)Φ, which grows linearly with s. (ii) The evolution applied
to this product, which is negligible compared to the other
steps. (iii) The number of iterations or “generations” in ES
until a small misclassiﬁcation rate is achieved. We conjec-
ture that this grows linearly with the number of multiplexers
s. (iv) The number of CRPs that are used to evaluate the
individuals per iteration. If Eqn. 17 is valid, then NCRP is
on the order of O(s/c).
Assuming the correctness of the conjectures made in this
derivation, this would lead to a polynomial growth of the
computation time in terms of the relevant parameters k, l
and s. It could then be conjectured that the number of basic
computational operations NBOP obeys
NBOP = O(s