PostgreSQL + ZFS
Best Practices and Standard Procedures
"If you are not using ZFS,
you are losing data*."
Clark's Three Laws
3
1. When a distinguished but elderly scientist states that something is possible,
he is almost certainly right. When he states that something is impossible, he
is very probably wrong.
2. The only way of discovering the limits of the possible is to venture a little
way past them into the impossible.
3. Any sufficiently advanced technology is indistinguishable from magic.
ZFS is not magic, but it is an incredibly impressive piece of software.
PostgreSQL and ZFS
4
•Many bits
•Lots of bits
•Huge bits
•It's gunna be great
•Very excited
•We have the best filesystems
•People tell me this is true
•Except the fake media, they didn't tell me this
PostgreSQL and ZFS: It's about the bits and storage, stupid.
5
•Many bits
•Lots of bits
•Huge bits
•It's gunna be great
•Very excited
•We have the best filesystems
•People tell me this is true
•Except the fake media, they didn't tell me this
Too soon?
PostgreSQL and ZFS
6
1. Review PostgreSQL from a storage administrator's perspective
2. Learn what it takes to become a PostgreSQL "backup expert"
3. Dive through a naive block-based filesystem
4. Walk through the a high-level abstraction of ZFS
5. See some examples of how to use ZFS with PostgreSQL
•Tips
•Tunables
•Anecdotes
Some FS minutiae may have been harmed in the making of this talk.
Nit-pick as necessary (preferably after).
PostgreSQL - A Storage Administrator's View
7
•User-land page cache maintained by PostgreSQL in shared memory
•8K page size
•Each PostgreSQL table is backed by one or more files in $PGDATA/
•Tables larger than 1GB are automatically shared into individual 1GB files
•pwrite(2)'s to tables are:
•append-only if no free pages in the table are available
•in-place updated if free pages are available in the free-space map
•pwrite(2)'s are page-aligned
•Makes heavy use of a Write Ahead Log (WAL), aka an Intent Log
Storage Administration: WAL on Disk
8
•WAL files are written to sequentially
•append-only IO
•Still 8K page-aligned writes via pwrite(2)
•WAL logs are 16MB each, pre-allocated
•WAL logs are never unlink(2)'ed, only recycled via rename(2)
•Low-latency pwrite(2)'s and fsync(2) for WAL files is required for good
write performance
PostgreSQL - Backups
9
Traditionally, only two SQL commands that you must know:
1.pg_start_backup('my_backup')
2.${some_random_backup_utility} $PGDATA/
3.pg_stop_backup()
Wait for pg_start_backup() to return
before backing up $PGDATA/ directory.
PostgreSQL - Backups
10
Only two^Wthree SQL commands that you must know:
1.CHECKPOINT
2.pg_start_backup('my_backup')
3.${some_random_backup_utility} $PGDATA/
4.pg_stop_backup()
Manual CHECKPOINT if you can't twiddle the
args to pg_start_backup().
PostgreSQL - Backups
11
Only two^Wthree^Wtwo commands that you must know:
1.CHECKPOINT
2.pg_start_backup('my_backup', true)
3.${some_random_backup_utility} $PGDATA/
4.pg_stop_backup()
pg_start_backup('my_backup', true)
a.k.a. aggressive checkpointing (vs default perf hit of:
0.5 * checkpoint_completion_target)
Quick ZFS Primer
14
Quick ZFS Primer
15
TIP: Look for parallels.
Quick ZFS Primer: Features (read: why you must use ZFS)
16
•Never inconsistent (no fsck(8)'s required, ever)
•Filesystem atomically moves from one consistent state to another consistent state
•All blocks are checksummed
•Compression builtin
•Snapshots are free and unlimited
•Clones are easy
•Changes accumulate in memory, flushed to disk in a transaction
•Redundant metadata (and optionally data)
•Filesystem management independent of physical storage management
•Log-Structured Filesystem
•Copy on Write (COW)
Feature Consequences (read: how your butt gets saved)
17
•bitrot detected and automatically corrected if possible
•phantom writes
•misdirected reads or writes by the drive heads
•DMA parity errors
•firmware or driver bugs
•RAM capacitors aren't refreshed fast enough or with enough power
•Phenomenal sequential and random IO write performance
•Performance increase for sequential reads
•Cost of ownership goes down
•New tricks and tools to solve "data gravity" problems
ELI5: Block Filesystems vs Log
Structured Filesystems
Block Filesystems: Top-Down
19
Userland Application
buffer
write(fd, buffer, cnt)
Userland
Block Filesystems: Top-Down
20
Userland Application
buffer
write(fd, buffer, cnt)
Userland
Kernel
VFS Layer
Logical File: PGDATA/global/1
Block Filesystems: Top-Down
21
Userland Application
buffer
write(fd, buffer, cnt)
Userland
Kernel
VFS Layer
Logical File: PGDATA/global/1
System Buffers
Block Filesystems: Top-Down
22
Userland Application
buffer
write(fd, buffer, cnt)
Userland
Kernel
VFS Layer
Logical File: PGDATA/global/1
System Buffers
Logical File Blocks
0 1 2 3 4
Block Filesystems: Top-Down
23
Kernel
VFS Layer
Logical File: PGDATA/global/1
System Buffers
Logical File Blocks
0 1 2 3 4
Physical Storage Layer
2: #9971
Pretend this is a
3: #0016
0: #8884
spinning disk
4: #0317
1: #7014
Block Filesystems: PostgreSQL Edition
24
Userland Application
cnt = 2
8k buffer
write(fd, buffer, cnt)
Userland
Block Filesystems: PostgreSQL Edition
25
Userland Application
cnt = 2
8k buffer
write(fd, buffer, cnt)
Userland
Kernel
VFS Layer
Logical File: PGDATA/global/1
System Buffers
Logical File Blocks
0 1 2 3
Block Filesystems: PostgreSQL Edition
26
Kernel
VFS Layer
Logical File: PGDATA/global/1
System Buffers
Logical File Blocks
0 1 2 3
Physical Storage Layer
2: #9971
0: #8884
3: #0016
1: #7014
Quiz Time
27
What happens when you twiddle a bool in a row?
UPDATE foo_table SET enabled = FALSE WHERE id = 123;
Quiz Answer: Write Amplification
28
UPDATE foo_table SET enabled = FALSE WHERE id = 123;
foo_table Tuple
<~182 tuples
Userland Application
8k buffer
write(fd, buffer, cnt)
ZFS Tip: postgresql.conf: full_page_writes=off
29
ALTER SYSTEM SET full_page_writes=off;
CHECKPOINT;
-- Restart PostgreSQL
IMPORTANT NOTE: full_page_writes=off interferes with cascading replication
Block Filesystems: PostgreSQL Edition
30
Userland Application
•buffers can be 4K cnt = 2
8k buffer
write(fd, buffer, cnt)
•disk sectors are 512B - 4K
Userland
•ordering of writes is important Kernel
VFS Layer
•consistency requires complete
Logical File: PGDATA/global/1
cooperation and coordination
System Buffers
Logical File Blocks
0 1 2 3
ZFS Filesystem Storage Abstraction
31
Physical Storage is
decoupled
from
Filesystems.
If you remember one thing from this section,
this is it.
VDEVs On the Bottom
32
VDEV: raidz VDEV: mirror
IO Scheduler IO Scheduler
disk1 disk2 disk3 disk4 disk5 disk6
zpool: rpool or tank
Filesystems On Top
33
VFS
Dataset Name Mountpoint
tank/ROOT /
canmount=off
tank/db /db
tank/ROOT/usr /usr
tank/local none
tank/local/etc /usr/local/etc
Offensively Over Simplified Architecture Diagram
34
ZPL - ZFS POSIX Layer
Filesystem zvol
Datasets
DSL - Dataset and Snapshot Layer
VDEV: raidz VDEV: mirror
IO Scheduler IO Scheduler
disk1 disk2 disk3 disk4 disk5 disk6
zpool: rpool or tank
ZFS is magic until you know how it fits together
35
VFS
Dataset Name Mountpoint
tank/ROOT /
tank/db /db
tank/ROOT/usr /usr
tank/local none
tank/local/etc /usr/local/etc
ZPL - ZFS POSIX Layer
Filesystem zvol
Datasets
DSL - Dataset and Snapshot Layer
VDEV: raidz VDEV: mirror
IO Scheduler IO Scheduler
disk1 disk2 disk3 disk4 disk5 disk6
zpool: rpool or tank
Log-Structured Filesystems: Top-Down
36
Log-Structured Filesystems: Top-Down
37
Disk Block with
foo_table Tuple
ZFS: User Data Block Lookup via ZFS Posix Layer
38
uberblock
Disk Block with
foo_table Tuple
ZFS: User Data + File dnode
39
t
1
ZFS: Object Set
40
t
2
t
1
ZFS: Meta-Object Set Layer
41
t
3
t
2
t
1
ZFS: Uberblock
42
t
4
t
3
t
2
t
1
At what point did the filesystem become inconsistent?
43