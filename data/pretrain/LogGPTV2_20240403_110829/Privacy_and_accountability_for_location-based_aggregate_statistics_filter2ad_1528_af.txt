based on historical data or estimates of the trafﬁc at a sample point.
The decreasing line represents to what extent clients can upload Uid
in total; as expected, this decreases as Uid increases. The increasing
line shows the percentage of clients passing through the sample
point that can upload at least one tuple. Fortunately, we can see
that for most values of Uid, both metrics are high, typically greater
than 90%. This means that our Uid-upload protocol is feasible to
achieve in practice and does not signiﬁcantly affect the accuracy of
the aggregate.
Examining Fig. 6, Uid = avg + std seems to be the best choice.
We can gain some intuition as follows. Even if the turnout at a
sample point is larger than avg by a standard deviation, most clients
should still be able to upload. At the same time, Uid cannot be too
much larger than avg for cases when there is a small turnout. We
tried larger quotas of 4 and 5 and found that Uid = avg + (quota −
1)/2 · std still represents the intersection of the lines in Fig. 6.
Finally, we examine the accuracy error introduced by our protocol
for this choice of Uid. Count aggregates of course introduce no error.
For summation-type aggregates (see §7), the increasing line in Fig.
6 provides this answer. If every value summed comes from the same
distribution, the error is roughly 5% which we consider reasonable,
especially since summation-type averages are less common.
663For average-type aggregation (§7), the accuracy is very high: even
if only a subset of the clients may be able to upload in certain cases,
this subset is chosen at random from all clients passing through
the sample point due to the random timing in the sync. interval
(See §5 and Fig. 3). Assuming each client’s value is taken from
the same distribution, the expected average of tuples is equal to the
real expected average; moreover, due to the law of large numbers,
for popular aggregates (which therefore also have larger Uid), the
observed average will become very close to the real one. For aggre-
gates with as few as ≈ 100 uploads per aggregate (as is the case in
Cartel), the errors were already small: Fig. 7 shows that the error in
average delay is at most 5% and the error in average speed is 3%.
We do not claim that CarTel trafﬁc patterns are representative, but
one can use a similar analysis of historical data to deduce proper
Uid for other cases. In fact, we expect our protocol to be even more
effective in practice because the CarTel network is very small (≈ 30
drivers); in a system with more participants, both metrics in Fig. 6
will simultaneously be larger, many values of Uid will be suitable,
and errors of averages smaller due to the law of large numbers.
10. RELATED WORK
Aggregate statistics for location privacy. Systems such as [16],
CliqueCloak [14], [19], [24], [20], and Triplines [18], address the
same problem as PrivStats. Their approaches are to use trusted
parties to aggregate time-consecutive driver uploads for the same
location, to avoid uploading in sensitive areas, to distort spatially
and temporally locational services, or to “cloak” the data using
spatial and temporal subsampling techniques.
One problem with these solutions is that they neither provide
rigorous privacy guarantees nor protect against side information.
As discussed in §3, even if a server only sees anonymized tuples,
considerable private information can leak due to SI. For example, in
[20, 18], a trusted party aggregates the upload of each driver on a
piece of road with the uploads of other k − 1 drivers close in time
and location to provide k-anonymity. While one individual tuple has
k-anonymity, when intersecting the sets of k-anonymity for a path,
the driver may not be necessarily k-anonymous over her entire path
among other drivers and their paths. And of course SI can cause
further violations of k-anonymity.
Another difﬁculty with most of these solutions is their reliance
on fully trusted intermediaries between drivers and server. For
example, trusted parties in [16, 14, 18, 19] receive tuples with
clients identiﬁers in plaintext and are supposed to remove them
from the tuple. If these parties are compromised or collude with
other trusted parties in the system, so are the paths of the drivers, as
they have access to driver identiﬁers. In our work, if the SM gets
compromised, the server will still only see anonymized tuples (see
§8), and the use of the SM achieves stronger security.
Moreover, accountability has previously either been ignored (al-
lowing drivers to bias signiﬁcantly the aggregate result) [16, 14,
24, 20] or handled by having tuples contain driver identiﬁers [18,
19]. For instance, in [18, 19], users upload tuples containing their
identities to a trusted party that checks if clients have uploaded too
much, while another trusted party performs aggregation. If these
trusted parties collude, driver paths with identities leak.
Other related work. We now discuss systems for protecting loca-
tion privacy that target different but related problems.
VPriv [32] and PrETP [2] compute functions over a particular
driver’s path as opposed to computing statistics over the paths of
multiple clients; for instance, they can compute the total toll that
driver Alice has to pay based on how much she drove in a month.
They associate the result of a function to a client’s id, whereas PrivS-
tats keeps clients anonymous and computes statistics over all clients.
VPriv and PrETP also use the general zero-knowledge concept, but
the actual cryptographic proofs and protocols are different from ours
and inapplicable to our setting: in VPriv and PrETP, all the clients
who provide data for a function must be online and perform work at
the same time, which is impractical for aggregates with hundreds of
clients. Moreover, these protocols do not support aggregates over
data from different clients.
Furthermore, VPriv is vulnerable to side information because the
server sees anonymized tuples with time and location information.
VPriv acknowledges this problem, but does not solve it, leaving it
for future work. PrETP also suffers from SI, but to a lesser extent
than VPriv. Finally, VPriv and PrETP do not provide our type of
accountability: clients can upload as many times as they desire.
They have a random spot check scheme involving a trusted party
that makes sure that clients upload at least once per location. Our
accountability protocol uses no trusted parties, leaks no privacy, and
most importantly performs different enforcements.
SEPIA [6] uses special-purpose secure-multiparty protocols to
compute aggregate statistics over network trafﬁc. However, the
approach in SEPIA is not plausible in our usage scenario. Some
trusted “input peers” provide network data, and a cluster of “privacy
peers” performs all the computation on this data. The protocol
relies on a majority of the privacy peers to not collude. In contrast
to our system, these peers perform almost all computation, and if
they collude, all client data is visible. Furthermore, input peers
are trusted, so SEPIA does not provide any accountability checks
against abusive clients. SEPIA also does not hide the number of
tuples for each aggregate.
There has also been recent work by Narayanan et al. [29], Zhong
et al. [41], and Puttaswamy and Zhao [33] on preserving location
privacy in location-based social networking applications, an area
also full of location privacy concerns. This work has a different goal
than ours: it allows users to exchange location-based information
with friends while protecting their privacy (against disclosure to
both the server and to friends).
Shokri et al. [38] provide a framework for quantifying location pri-
vacy as a way of comparing various location privacy schemes. Given
a certain adversarial goal and models of adversarial knowledge and
of a location privacy scheme, the framework enables quantiﬁcation
of the performance of the adversary. The authors also argue that
side information (which they call “prior knowledge”) should be con-
sidered when analyzing privacy. However, this work has a different
goal than PrivStats, does not provide a deﬁnition of privacy resilient
to side information, and does not propose protocols for accountable
and location private computation of aggregate statistics.
Work on e-cash [9] is related to our accountability protocol (§6):
one might envision giving each client a number of e-coins to spend
on every aggregate. This approach ﬁts well in spam control applica-
tions [1, 39]. However, it is not practical in our setting: coins must
be tied to a particular location, time interval, and sample type, which
requires generating a prohibitively large number of coins. Other
work [8] gives each user n coins for each time period. Again, we
need the coins tied to sample points, so this is not feasible. Also,
e-cash adds a lot of complexity (and thus slowdown) to identify
double-spenders, which we do not require. Our accountability pro-
tocol is simple, speciﬁc to our setting, and fast.
Finally, we remark that our approach complements work on dif-
ferential privacy [11], as we discussed in §8.
11. CONCLUSION
In this paper, we presented PrivStats, a system for computing
aggregate statistics for mobile, location-based applications that
achieves both strong guarantees of location privacy and protection
664against cheating clients. PrivStats solves two major problems not
solved by previous work: it ensures that no further information leaks
even in the face of arbitrary side information attacks, and it pro-
vides client accountability without a trusted party. We implemented
PrivStats on commodity phones and servers, and demonstrated its
practicality.
12. ACKNOWLEDGMENTS
We thank Nickolai Zeldovich, Mike Walﬁsh, Arvind Narayanan,
and the anonymous reviewers for useful comments and feedback.
This work was supported in part by NSF grants 0931550 and
0716273.
13. REFERENCES
[1] M. Abadi, A. Birrell, M. Burrows, F. Dabek, and T. Wobber.
Bankable postage for network services. In ASIAN, 2003.
[2] J. Balasch, A. Rial, C. Troncoso, B. Preneel, I. Verbauwhede,
and C. Geuens. PrETP: Privacy-preserving electronic toll
pricing. Usenix Security, 2010.
[3] M. Bellare and O. Goldreich. On deﬁning proofs of
knowledge. CRYPTO, 1992.
[4] A. Boldyreva, N. Chenette, Y. Lee, and A. O’Neill.
Order-preserving symmetric encryption. In EUROCRYPT,
2009.
[5] F. Boudot. Efﬁcient proofs that a committed number lies in an
interval. EUROCRYPT, 2000.
[6] M. Burkhart, M. Strasser, D. Many, and X. Dimitropoulos.
SEPIA: Privacy-preserving aggregation of multi-domain
network events and statistics. Usenix Security, 2010.
[7] California Department of Transportation. Caltrans guide for
the preparation of trafﬁc impact studies.
[8] J. Camenisch, S. Hohenberger, M. Kohlweiss,
A. Lysyanskaya, and M. Meyerovich. How to win the
clonewars: Efﬁcient periodic n-times anonymous
authentication. In CCS, 2006.
[9] J. Camenisch, S. Hohenberger, and A. Lysyanskaya.
Balancing accountability and privacy using e-cash. Security
and Cryptography for Networks, 2006.
[10] J. Camenisch and A. Lysyanskaya. A Signature Scheme with
Efﬁcient Protocols. Security and Cryptography for Networks,
2002.
[11] C. Dwork. Differential privacy: A survey of results. In TAMC
1-19, 2008.
[12] E-ZPass. How it works. http://www.ezpass.com/index.html.
[13] A. Fiat and A. Shamir. How to prove yourself: Practical
solutions to identiﬁcation and signature problems. CRYPTO,
1986.
[14] B. Gedik and L. Liu. Location privacy in mobile systems: A
personalized anonymization model. In ICDCS, 2005.
[15] S. Goldwasser, S. Micali, and C. Rackoff. The knowledge
complexity of interactive proof-systems. Symposium on the
Theory of Computation, 1985.
[16] M. Gruteser and D. Grunwald. Anonymous usage of
location-based services through spatial and temporal cloaking.
In MobiSys, 2003.
[17] M. Gruteser and B. Hoh. On the anonymity of periodic
location samples. In IEEE Pervasive Computing, 2005.
[18] B. Hoh, M. Gruteser, R. Herring, J. Ban, D. Work, J.-C.
Herrera, A. Bayen, M. Annavaram, and Q. Jacobson. Virtual
trip lines for distributed privacy-preserving trafﬁc monitoring.
In Mobisys, 2008.
[19] B. Hoh, M. Gruteser, H. Xiong, and A. Alrabady. Enhancing
security and privacy in trafﬁc-monitoring systems. In IEEE
Pervasive Computing, 2006.
[20] B. Hoh, M. Gruteser, H. Xiong, and A. Alrabady. Preserving
privacy in GPS traces via uncertainty-aware path cloaking. In
CCS, 2007.
[21] B. Hull, V. Bychkovsky, K. Chen, M. Goraczko, A. Miu,
E. Shih, Y. Zhang, H. Balakrishnan, and S. Madden. CarTel: A
Distributed Mobile Sensor Computing System. Sensys, 2006.
[22] N. Husted and S. Myers. Mobile location tracking in metro
areas: Malnets and others. In CCS, 2010.
[23] D. Karger, E. Lehman, T. Leighton, M. Levine, D. Lewin, and
R. Panigrahy. Consistent hashing and random trees:
Distributed caching protocols for relieving hot spots on the
World Wide Web. In STOC, 1997.
[24] J. Krumm. Inference attacks on location tracks. In IEEE
Pervasive Computing, 2007.
[25] J. Lowensohn. Apple sued over location tracking in iOS.
http://news.cnet.com/8301-27076_3-20057245-248.html,
2011. CNET News.
[26] E. Mills. Google sued over Android data location collection.
http://news.cnet.com/8301-27080_3-20058493-245.html,
2011. CNET News.
[27] Mobile Millennium. http://trafﬁc.berkeley.edu/.
[28] M. Mun, S. Reddy, K. Shilton, N. Yau, P. Boda, J. Burke,
D. Estrin, M. Hansen, E. Howard, and R. West. PEIR, the
personal environmental impact report, as a platform for
participatory sensing systems research. In MobiSys, 2009.
[29] A. Narayanan, N. Thiagarajan, M. Lakhani, M. Hamburg, and
D. Boneh. Location privacy via private proximity testing.
NDSS, 2011.
[30] P. Paillier. Public-key cryptosystems based on composite
degree residuosity classes. In EUROCRYPT, 1999.
[31] T. P. Pedersen. Non-interactive and information-theoretic
secure veriﬁable secret sharing. CRYPTO, 1991.
[32] R. A. Popa, H. Balakrishnan, and A. J. Blumberg. VPriv:
Protecting privacy in location-based vehicular services.
Usenix Security, 2009.
[33] K. Puttaswamy and B. Zhao. Preserving privacy in
location-based mobile social applications. International
Workshop on Mobile Computing and Applications, 2010.
[34] R. Reid. TomTom admits to sending your routes and speed
information to the police, 2011. CNET UK.
[35] P. Riley. The tolls of privacy: An underestimated roadblock
for electronic toll collection usage. In Third International
Conference on Legal, Security, and Privacy Issues in IT, 2008.
[36] C. P. Schnorr. Efﬁcient identiﬁcation and signatures for smart
cards. CRYPTO, 1989.
[37] E. Shi, T.-H. H. Chan, E. Rieffel, R. Chow, and D. Song.
Privacy-preserving aggregation of time-series data. In NDSS,
2011.
[38] R. Shokri, G. Theodorakopoulos, J.-Y. L. Boudec, and J.-P.
Hubaux. Quantifying location privacy. In IEEE Symposium on
Security and Privacy, 2011.
[39] M. Walﬁsh, J. Zamﬁrescu, H. Balakrishnan, D. Karger, and
S. Shenker. Distributed quota enforcement for spam control.
In NSDI, 2006.
[40] WMUR. Police: Thieves robbed home based on Facebook,
2010. http://www.wmur.com/r/24943582/detail.html.
[41] G. Zhong, I. Goldberg, and U. Hengartner. Louis, Lester, and
Pierre: Three protocols for location privacy. International
Conference on Privacy-Enhancing Technologies, 2007.
665