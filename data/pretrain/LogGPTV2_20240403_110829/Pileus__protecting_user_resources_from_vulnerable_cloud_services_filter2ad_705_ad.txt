grams that run with root privilege. One example is libvirtd. The
question is how to extend access control to these VM objects, pre-
venting these daemons from being leveraged as confused deputies
(e.g., In current OpenStack, Bob’s event handler may ask libvirtd
to operate over Alice’s VM).
59
Alice-Compute
S = {a}
Bob-Compute
S = {b}
Labeled peer 
connections
Socket
libvirtd
worker
worker
Security
framework
Alice-VM
S = {a}
Bob-VM
S = {b}
CVE ID
Figure 7: Information ﬂow between event handlers of compute
service and libvirtd daemon.
Mitigated
Yes
1 CVE-2015-1195
Yes
2 CVE-2015-1850
Yes
3 CVE-2015-1851
Yes
4 CVE-2015-5163
5 CVE-2015-7548
Yes
6 CVE-2015-3221 Network Service (Neutron) No*
Table 1: Information ﬂow control vulnerabilities in OpenStack.
Affected Cloud Service
Image Service (Glance)
Volume Service (Cinder)
Volume Service (Cinder)
Image Service (Glance)
Compute Service (Nova)
To address this problem, we enhance libvirtd with an in-daemon
security framework that can validate whether the requesting event
handler has the same label as the resources to be operated upon,
similar to SEPostgreSQL [36]. As shown in Figure 7, the basic
idea is that when an event handler establishes a connection to lib-
virtd through a Unix domain socket, the in-daemon security frame-
work retrieve labels of the event handler from the socket descrip-
tor (e.g., via getpeercon). Then, when the event handler requests
libvirtd for a VM operation, the security framework compares the
label of the event handler with the label of the VM, ensuring that
the event handler is authorized to operate over the VM. At present,
this enforcement mechanism is embedded into the libvirtd. We will
explore using the Pileus kernel as the security server in the future.
6. EVALUATION
6.1 Mitigating Cloud Service Vulnerabilities
In this section, we show the security improvement made by
Pileus over the off-the-shelf OpenStack, both through a system ex-
ploit experiment and a qualitative analysis.
Exploit Experiment. We ported OpenStack Icehouse 2014.1 to
Pileus. Six information ﬂow vulnerabilities were reported after our
installation. Five of them are present in our deployment and one is
not. To conduct the comparison, we did not patch the cloud services
and try to exploit them in vanilla OpenStack and OpenStack on
Pileus respectively. The vulnerabilities are listed in Table 1.
Vulnerability 1 is a pathname resolution bug in image service.
Exploiting the vulnerability, we were able to read arbitrary image
ﬁles on an image node that runs vanilla OpenStack image service.
In contrast, Pileus successfully prevented the vulnerable image ser-
vice from reading other users’ images since the event handler of
image service is conﬁned to a user label.
Vulnerability 2, 3, 4 and 5 are of a similar kind: by exploit-
ing them, we were able to read/overwrite arbitrary ﬁles on a cloud
node. The attacks happen due to a helper program qemu-img that
was called by cloud services to process user images. If not explic-
itly speciﬁed, qemu-img will infer image type and automatically
read necessary ﬁles (e.g., base ﬁle for a qcow2 type image) to build
the image. An adversary may thus trick a vulnerable cloud ser-
vice into accessing ﬁles that he does not have access. Although
the vulnerabilities remain unpatched, Pileus successfully prevented
vulnerable cloud services from being utilized to access arbitrary
ﬁles. In Pileus, qemu-img program runs with a user label inher-
ited from the event handler that invokes it. Consequently, the only
ﬁles that it can access are the ones that have the same label.
Vulnerability 6 was identiﬁed in OpenStack network service
boot
10
delete
7
Op
#
Table 2: Maximum number of nodes that needs to be trusted
when performing cloud operations.
snapshot migrate
5
vol-attach
6
resize
8
8
Figure 8: Expected number of cloud nodes in a user’s TCB.
The simulation consists of 1,000 cloud nodes. 5 are randomly
picked each time to perform a cloud user’s operation, and 10
operations are performed per second.
(Neutron) but not in the legacy nova-network that we used in our
deployment. Thus we did not test against it. However, we note
that by design Pileus can mitigate this vulnerability. The vulner-
ability is caused by incorrectly parsing iptables ﬁrewall rules that
an adversary may leverage to block network connections of others.
In Pileus, we used network namespaces to isolate the ﬁrewall rules
for different users. Thus incorrect parsing of ﬁrewall rules can only
affect a single network namespace, the adversary’s own namespace.
Qualitative Analysis. In order to have a big picture of how Pileus
can improve OpenStack security, we performed a qualitative anal-
ysis of all 154 vulnerabilities identiﬁed in OpenStack so far6. We
found that 1/3 (53 out of 154) of OpenStack vulnerabilities are re-
lated to information ﬂow problems studied in this paper, and Pileus
systematically mitigate those vulnerabilities.
6.2 Reducing the Cloud Users’ TCBs
In the original OpenStack, a user needs to rely on all cloud nodes
to execute their user operations securely, so all cloud nodes are in
the trusted computing base (TCB). Consequently, a compromise of
any single cloud node allows adversary to gain control over any
user’s data, cloud wide. In contrast, Pileus restricts the data acces-
sibility of a cloud node to the authority held by it. Thus, data loss
due to a node compromise is bounded by the trust placed on the
node. Table 2 shows the maximum number of nodes that need to
be trusted in order to perform various cloud operations on Pileus7.
As shown in the table, the size of each operation’s TCB is reduced
to a handful of cloud nodes that are actually involved in each user’s
operation, instead of the entire cloud.
In addition, Pileus further reduces the amount of time that a user
needs to trust a cloud node in an operation through its timely revo-
cation mechanism. This reduces temporal attack surface of a user’s
TCB. To show the effect of the mechanism, we saturate a cloud
with a large number of concurrent user operations and evaluate the
average size of the user’s TCB8. Figure 8 shows the result. The
6These vulnerabilities came from OpenStack versions spanning from 2012
to 2016. Therefore much of this evaluation is necessarily qualitative.
7The maximum occurs when every event handler involved in the operation
runs on its own cloud node. The actual number is often much smaller since
Pileus will always try to schedule event handlers of the same cloud opera-
tion on the same cloud node.
8In this case, a user’s TCB at a given time becomes a composite of all cloud
60
simulated cloud has 1,000 cloud nodes, and to simplify the dis-
cussion, we ignore the actual service deployment and assume each
user operation takes ﬁve cloud nodes picked at random. We then
investigate the effectiveness of different approaches by comparing
the expected number of cloud nodes of a user’s TCB (Y-axis in the
ﬁgure). The X-axis is the number of cloud operations performed
by the user.
In original OpenStack, a user’s TCB includes all the cloud nodes
(black line at top). In contrast, when enforcing a decentralized se-
curity principle, a user’s TCB dynamically expands as more cloud
nodes are involved in his operations (red line). However, without
revocation, the user will eventually end up trusting all the cloud
nodes in the cloud. When an expiration-based strategy is adopted,
the user’s trust is revoked from a cloud node after a certain period.
In our simulation, we set the expiration time to be 15 seconds. The
expected size of the user’s TCB in this case converges to around
530 cloud nodes (green line). In contrast, Pileus adopts a timely
revocation where trust is revoked immediately after a cloud node
completes its processing of the user’s operation. In the experiment,
we used our observed service duration of 2 to 8 seconds in a uni-
form distribution. In this case, the expected size of the user’s TCB
converges to around 220 cloud nodes (blue line). When we adopt
Pileus’s spawn scheduling algorithm, which gives priority to cloud
nodes that are already within a user’s TCB, the size of user’s TCB
converges to 25 cloud nodes (pink line) in this experiment, assum-
ing that each cloud node can serve at most 10 concurrent opera-
tions.
6.3 Optimizing the Cloud Users’ TCBs
Pileus differs from previous DIFC approaches is its ability to
dynamically manage TCB on behalf of cloud users.
Its owner-
ship registry runs a spawn scheduling algorithm that computes a
spawn destination, implementing a best effort approach to reduce
the likelihood of compromise of a user’s TCB. To approximate the
likelihood of a user’s TCB compromise, we propose a metric called
Averaged TCB Sharing Factor (ATSF). The ATSF metric reﬂects an
intuitive observation—the more a user’s TCB overlaps with other
users, the more likely it is to be compromised by one of those other
users (assuming users are equally likely to be an adversary). It is
calculated based on the average number of users per cloud node,
using the following equation where Ui is the number of unique
users on cloud node i. In an ideal case where each user has no-
overlapping TCB, ATSF would be one. We thus measure three dif-
ferent node selection strategies in OpenStack, and compare them
with Pileus’s spawn scheduling algorithm.
where Ni =,0, Ui = 0
1, Ui > 0
(6.1)
i=1 Ui
i=1 Ni
!n
!n
The result is shown in Figure 9. The simulation consists of 1,000
cloud nodes and 400 cloud users. Each cloud node has a capacity
of 10 operations, i.e., a node can support a maximum of 10 con-
current cloud operations. The X-axis is the total number of cloud
operations performed. They are randomly distributed across 400
cloud users. The Y-axis is measured ATSF. The higher the ATSF
is, the more TCB sharing is observed across cloud users. Thus the
TCB of a cloud user is more likely to be compromised.
The ﬁrst node selection strategy in OpenStack is maximum uti-
lization. This strategy tries to maximize the utilization of individual
cloud nodes, i.e., unless a cloud node reaches its capacity, it will be
scheduled ﬁrst. Such strategy is useful when cloud vendor wants
to minimize its cost (e.g., electric bill). As shown in the ﬁgure,
the ATSF quickly reaches 10 (the capacity of a cloud node) after
nodes that are involved in concurrent operations.
Pileus
Mult.
FlowK
Mult.
Flume
Multi.
Pileus
Syscalls (µs) Native
Linux
open
—create
—exists
—not exist
close
stat
readlink
unlink
fork+exit
Table 4: System call overheads compared with Flume [21] and
FlowK [48]. Results are averaged over 10,000 runs.
1.23
0.62
0.51
0.51
0.33
0.34
11.97
263.4
6.16
2.90
1.69
0.55
1.55
1.52
24.31
287.5
16
34.5
23.6
1.3
34.5
33
7.2
N/A
8.3
11
3.7
1.1
N/A
N/A
N/A
N/A
5
4.7
3.3
1.1
4.7
4.5
2
1.1
Figure 9: Average TCB sharing factor under different node
selection strategies. The simulation consists of 1,000 cloud
nodes and 400 cloud users. Cloud operations are randomly dis-
tributed across cloud users.
Type
DIFC-aware
DIFC-unware
Infrastructure
Multiple users
Total
Number
13
135
6
3
157
Example
nova boot
nova volume-attach
nova host-action
nova host-evacuate
Table 3: OpenStack operations.
around 10 cloud operations and stays at 10 thereafter. The second
node selection strategy randomly selects cloud nodes for a user’s
operation. The ATSF increases almost linearly as the number of
operations grow. The third node selection strategy selects least oc-
cupied cloud nodes, i.e., cloud nodes currently performing least
cloud operations. Such strategy is useful when cloud vendor wants
to improve the performance of individual cloud operations. For this
strategy, the ATSF stays at 1 before all the 1,000 cloud nodes are
used by cloud users. Then the ATSF increases linearly, at a rate al-
most the same as the random selection strategy. The ATSF for both
the random selection and least usage strategy eventually reaches 10,
when the cloud is saturated with operations (at ∼2000 operations).
In contrast, Pileus’s spawn scheduling algorithm tries to co-locate
the same user’s cloud operations on the same cloud nodes, thereby
reducing the ATSF. As shown in the ﬁgure, the ATSF is 1 before all
1,000 cloud nodes are used. Then it increases slowly and reaches
a maximum of around 3 when the cloud is saturated with cloud
operations. This simulation shows that Pileus’s spawn scheduling
algorithm indeed reduces TCB sharing among users.
6.4 OpenStack on Pileus
OpenStack Operations. One concern of DIFC approach is that
cloud services need to be intrusively modiﬁed to be aware of DIFC
control. However, as we show in this section, majority of cloud
operations do not require cloud services to be DIFC aware. Table 3
shows cloud operations we studied. Out of the 157 cloud operations
that are available in our deployment, 135 do not require any DIFC
aware cloud services to run. What this means is that despite of
being conﬁned, cloud services involved in these operations are not
aware of Pileus. 13 operations may require certain cloud services to
be DIFC aware—they need to declassify or endorse data on behalf
of cloud users. But as we show later, the types of endorsement and
declassiﬁcation are limited.
The remaining nine cloud operations are special cloud admin-
istrator operations that fall into two categories. The ﬁrst category
allows cloud administrator to directly operate over the cloud in-
frastructure. These operations do not involve information ﬂow of