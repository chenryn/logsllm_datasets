### Extending Access Control to VM Objects

Certain daemons, such as `libvirtd`, run with root privileges. The challenge is to extend access control to these Virtual Machine (VM) objects, preventing these daemons from being exploited as "confused deputies." For example, in the current OpenStack setup, Bob’s event handler might request `libvirtd` to operate on Alice’s VM, which should be prevented.

#### Example Scenario
- **Alice-Compute**: Label Set = {a}
- **Bob-Compute**: Label Set = {b}
- **Labeled Peer Connections**:
  - **Socket**
  - **libvirtd Worker**
  - **Security Framework**
- **Alice-VM**: Label Set = {a}
- **Bob-VM**: Label Set = {b}

**Figure 7: Information Flow Between Event Handlers of Compute Service and `libvirtd` Daemon**

| CVE ID | Mitigated |
|--------|-----------|
| CVE-2015-1195 | Yes |
| CVE-2015-1850 | Yes |
| CVE-2015-1851 | Yes |
| CVE-2015-5163 | Yes |
| CVE-2015-7548 | Yes |
| CVE-2015-3221 | No* (Network Service (Neutron)) |

**Table 1: Information Flow Control Vulnerabilities in OpenStack**

| Affected Cloud Service | Vulnerability Description |
|------------------------|--------------------------|
| Image Service (Glance) | Pathname resolution bug allowing arbitrary image file access. |
| Volume Service (Cinder) | Arbitrary file read/overwrite due to `qemu-img` helper program. |
| Compute Service (Nova) | Similar vulnerabilities related to `qemu-img` helper program. |

### Enhancing `libvirtd` with a Security Framework

To address this issue, we have enhanced `libvirtd` with an in-daemon security framework. This framework validates whether the requesting event handler has the same label as the resources it intends to operate on, similar to SEPostgreSQL [36]. 

**Process:**
1. When an event handler establishes a connection to `libvirtd` through a Unix domain socket, the security framework retrieves the labels of the event handler from the socket descriptor (e.g., using `getpeercon`).
2. When the event handler requests `libvirtd` for a VM operation, the security framework compares the label of the event handler with the label of the VM.
3. If the labels match, the operation is allowed; otherwise, it is denied.

This enforcement mechanism is currently embedded into `libvirtd`. Future work will explore using the Pileus kernel as the security server.

### Evaluation

#### 6.1 Mitigating Cloud Service Vulnerabilities

We demonstrate the security improvements made by Pileus over the off-the-shelf OpenStack through a system exploit experiment and a qualitative analysis.

**Exploit Experiment:**
- **OpenStack Icehouse 2014.1** was ported to Pileus.
- Six information flow vulnerabilities were reported after our installation.
- Five of these vulnerabilities are present in our deployment, and one is not.
- We did not patch the cloud services and attempted to exploit them in both vanilla OpenStack and OpenStack on Pileus.

**Vulnerabilities:**
1. **Pathname Resolution Bug in Image Service**: In vanilla OpenStack, we could read arbitrary image files. Pileus prevented this by confining the image service's event handler to a user label.
2. **Arbitrary File Read/Overwrite via `qemu-img`**: Exploiting these vulnerabilities, we could read/overwrite arbitrary files on a cloud node. Pileus prevented this by ensuring `qemu-img` runs with a user label inherited from the event handler, limiting its access to files with the same label.
3. **Incorrect Parsing of iptables Firewall Rules in Network Service (Neutron)**: Although this vulnerability was not tested, Pileus can mitigate it by isolating firewall rules in network namespaces, limiting the impact to the adversary's own namespace.

**Qualitative Analysis:**
- Out of 154 identified OpenStack vulnerabilities, 53 (one-third) are related to information flow problems.
- Pileus systematically mitigates these vulnerabilities.

#### 6.2 Reducing the Cloud Users’ TCBs

In the original OpenStack, a user must trust all cloud nodes, making them part of the Trusted Computing Base (TCB). A compromise of any single cloud node can lead to a breach of any user’s data. Pileus restricts data accessibility to the authority held by each node, bounding data loss to the trust placed on the node.

**Table 2: Maximum Number of Nodes That Need to Be Trusted for Various Cloud Operations**

| Operation | Maximum Number of Trusted Nodes |
|-----------|----------------------------------|
| Boot      | 10                               |
| Delete    | 7                                |
| Snapshot  | 5                                |
| Migrate   | 6                                |
| Vol-Attach| 8                                |
| Resize    | 8                                |

**Figure 8: Expected Number of Cloud Nodes in a User’s TCB**

- **Simulation Setup**: 1,000 cloud nodes, 5 randomly picked each time for operations, 10 operations per second.
- **Results**:
  - **Original OpenStack**: User’s TCB includes all cloud nodes.
  - **Decentralized Security Principle**: TCB expands dynamically but eventually includes all nodes without revocation.
  - **Expiration-Based Strategy (15 seconds)**: TCB converges to around 530 nodes.
  - **Pileus (Timely Revocation)**: TCB converges to around 220 nodes.
  - **Pileus with Spawn Scheduling Algorithm**: TCB converges to 25 nodes, assuming each node can serve up to 10 concurrent operations.

#### 6.3 Optimizing the Cloud Users’ TCBs

Pileus dynamically manages TCB on behalf of cloud users using a spawn scheduling algorithm. To measure the likelihood of TCB compromise, we use the Averaged TCB Sharing Factor (ATSF).

**Equation for ATSF:**
\[
\text{ATSF} = \frac{\sum_{i=1}^{n} U_i}{\sum_{i=1}^{n} N_i}
\]
where \( U_i \) is the number of unique users on cloud node \( i \), and \( N_i \) is the total number of cloud nodes.

**Figure 9: Average TCB Sharing Factor Under Different Node Selection Strategies**

- **Simulation Setup**: 1,000 cloud nodes, 400 cloud users, each node capacity of 10 operations.
- **Node Selection Strategies**:
  - **Maximum Utilization**: ATSF quickly reaches 10.
  - **Random Selection**: ATSF increases linearly.
  - **Least Occupied Nodes**: ATSF stays at 1 until all nodes are used, then increases linearly.
  - **Pileus (Spawn Scheduling Algorithm)**: ATSF remains at 1 until all nodes are used, then increases slowly to around 3.

#### 6.4 OpenStack on Pileus

**Cloud Operations:**
- **DIFC-Aware Operations**: 13 out of 157 operations require DIFC-aware cloud services.
- **DIFC-Unaware Operations**: 135 operations do not require DIFC-aware cloud services.
- **Special Administrator Operations**: 9 operations that do not involve information flow.

**Table 3: OpenStack Operations**

| Type | Number | Example |
|------|--------|---------|
| DIFC-Aware | 13 | `nova boot`, `nova volume-attach` |
| DIFC-Unaware | 135 | `nova host-action`, `nova host-evacuate` |
| Infrastructure | 6 | - |
| Multiple Users | 3 | - |
| Total | 157 | - |

### System Call Overheads

**Table 4: System Call Overheads Compared with Flume and FlowK**

| System Call | Pileus (µs) | Native Linux (µs) | Flume (µs) | FlowK (µs) |
|-------------|-------------|-------------------|------------|------------|
| open        | 1.23        | -                 | 1.69       | 3.7        |
| create      | 0.62        | -                 | 0.55       | 1.1        |
| exists      | 0.51        | -                 | 1.55       | N/A        |
| not exist   | 0.51        | -                 | 1.52       | N/A        |
| close       | 0.33        | -                 | 24.31      | 4.7        |
| stat        | 0.34        | -                 | 287.5      | 3.3        |
| readlink    | 11.97       | -                 | 7.2        | 1.1        |
| unlink      | 263.4       | -                 | N/A        | 4.7        |
| fork+exit   | 6.16        | -                 | 8.3        | 4.5        |

This table shows the overheads of various system calls in Pileus compared to native Linux, Flume, and FlowK.