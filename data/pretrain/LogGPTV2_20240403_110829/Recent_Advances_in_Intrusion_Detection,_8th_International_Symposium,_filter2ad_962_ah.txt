L
e
u
e
u
Q
e
g
a
r
e
v
A
Aset len. 4
Aset len. 5
Aset len. 6
Aset len. 7
Aset len. 8
Aset len. 9
Aset len. 10
 10
Days
 15
 20
 0
 0
 5
 10
Days
 15
 20
 0
 0
 5
(c) FN per day for Infected Hosts
(d) Avg. Delay Queue Length
(Infected Hosts)
Fig. 2. Results for Williamson’s End Host RL mechanism
the neighborhood of 90%. This means that during infection, delay for each fresh
IP connection was approximately 90 seconds or greater if the queue was ﬁlled
with distinct hosts, which is likely to be the case due to the random scanning
nature of the worms.
We note, however, the way we deﬁne false positives is slightly unfair; we
label every delayed non-worm SYN packet a false positive. In reality, many
applications can tolerate a slight delay. Table 1 shows the delay statistics for
a normal host during a 3-hour period. As shown, all delays were less than 10
seconds, which may be entirely acceptable for certain applications. In contrast,
Table 2 shows the worst case delay statistics for an infected host for the same time
period. As shown, once a host is infected, the delay queue becomes saturated
Table 1. Delay statistics for a normal host during a 3-hour period
Delay Amount.
No delay
1 - 10 sec.
11 - 20 sec.
Total number of benign ﬂows 2144
Number of Flows
1759
385
0
28
C. Wong et al.
Table 2. Delay Statistics for an infected host during a 3-hour period
Delay Amount. Benign Malicious
No delay
1 - 30 sec.
31 - 60 sec.
61 - 90 sec.
91 - 100 sec.
Dropped
Total
12
36
36
50
10115
107080
117314
1
1
1
0
141
866
1010
with worm packets and legitimate applications on the host are subjected to
excessive delays and blockage.
Another observation is that the size of the working set (at least for the values
experimented here) has very little eﬀect on the error rates of the scheme. This
is at least partially due to the fact that we averaged statistics across hosts.
However, our experiments suggest that Williamson’s throttling scheme exhibits
a bimodal behavior with respect to legitimate traﬃc: minimal impact during
normal operation and greatly restrictive if infected. This behavior, we conjecture,
is inherent to the scheme regardless of the size of the working set, provided that
the working set permits at least the host’s normal contact rate. In practice, one
can observe the connection pattern of a host for some period of time before
determining the normal contact rate.
Figure 2(c) shows the false negative rates, which are predominantly below 1%.
This means that Williamson’s scheme is eﬀective against worm spread, though
it also incurs large delays for legitimate applications running on the same host.
The strength of Williamson’s scheme lies in its logical simplicity and ease of
management. One can imagine a more complex data structure than a simple
queue to deal with delayed connections. Alternatively, one can employ a dynamic
rate scheme that changes the dequeuing rate accordingly with the length of the
delay queue. Schemes such as these can potentially reduce the false positive
rates, but at the price of increased complexity.
Throttling at the Edge Router. Previous studies [10, 25] showed that end-
host rate limiting is ineﬀective unless deployment is universal. As part of this
study, we investigate the eﬀect of applying Williamson’s throttling to the ag-
gregate traﬃc at the edge of the network. Aggregate, edge-based throttling is
attractive because it requires the instrumentation of only the ingress/egress point
of the subnet. Furthermore, aggregate throttling dose not require per-host state
to be kept. We note that the logic of aggregate throttling can be extended to
the border point of a network cell within an enterprise, as shown in [14], which
can provide a ﬁner protection granularity.
In a previous traﬃc study, we identiﬁed a candidate rate of 16 addresses per
ﬁve seconds for edge throttling for a similar network [25]. In the analysis that
follow, we present results obtained with ﬁve aggregate rate limits: 10, 16, 20, 25
and 50 IPs per every ﬁve-second window.
Empirical Analysis of Rate Limiting Mechanisms
29
False Positive for Edge Router MW RL w/ varying Working Set len.
False Negative for Edge Router MW RL w/ varying Working Set len.
 100
 90
 80
 70
 60
 50
 40
 30
Aset len 10
Aset len 16
Aset len 20
Aset len 25
Aset len 50
 0
 5
 10
Days
 15
 20
)
%
(
e
v
i
t
a
g
e
N
e
s
a
F
l
Aset len 10
Aset len 16
Aset len 20
Aset len 25
Aset len 50
 3
 2.5
 2
 1.5
 1
 0.5
 0
 0
 5
 10
Days
 15
 20
)
%
(
e
v
i
t
i
s
o
P
e
s
a
F
l
(a) FP per day at Edge Router
(b) FN per day at Edge Router
Fig. 3. Results for Williamson’s RL mechanism at Edge Router
Figure 3(a) shows the false positive rates for edge-router rate limiting us-
ing various rate limits. The corresponding false negative rates are shown in
Figure 3(b). Compared with the end-host case, edge-based rate limiting exhibits
signiﬁcantly higher false positive rates during normal operation. This is primar-
ily due to the fact that aggregate throttling penalizes hosts with atypical traﬃc
patterns, thereby contributing to a higher false positive rate. We can increase the
working set size at the edge to reduce the false positives, but false positives will
increase accordingly. As such, Williamson’s throttling is best suited for end-host
rate limiting where behavior of a host is somewhat predictable.
6 Failed Connection Rate Limiting (FC)
Chen et al. proposed another rate limiting scheme based on the assumption
that a host infected by a scanning worm will generate a large number of failed
TCP requests [1]. Their scheme attempts to rate limit hosts that exhibit such
behavior. In the discussions that follow, we refer to this scheme as FC (for Failed
Connection).
FC is an edge-router based scheme that consists of two phases. The ﬁrst
phase identiﬁes the potential “infected” hosts. During this phase a highly con-
tended hash table is used to store failure statistics for hosts. The hash table is
used to limit the amount of per-host state kept at the router. Once the fail-
ure rate for a hash entry exceeds a certain threshold, the algorithm enters the
second phase, which attempts to rate limit the hosts in the entry. Chen pro-
posed a “basic” and “temporal” rate limiting algorithm. We analyze both in this
study.
The basic FC algorithm focuses on a short-term failure rate, λ. Chen recom-
mends a λ value of one failure per second. Once a hash entry exceeds λ, the
rate-limiting engine attempts to limit the failure rate of each host in the entry
to at most λ, using a leaky bucket token algorithm—a token is removed from the
bucket for each failed connection and every λ seconds a new token is added to
the bucket. Once the bucket for a particular host is empty, further connections
from that host are dropped.
30
C. Wong et al.
False Positive per day for FC
)
%
(
e
v
i
t
i
s
o
P
e
s
a
F
l
 40
 35
 30
 25
 20
 15
 10
 5
 0
 0
 5
Basic lambda = 1.0
Temporal lambda = 1.0, omega = 300
 10
Days
 15
 20
(a) FP per day
)
%
(
e
v
i
t
a
g
e
N
e
s
a
F
l
 40
 35
 30
 25
 20
 15
 10
 5
 0
False Negative per day for FC
Basic lambda = 1.0
Temporal lambda = 1.0, omega = 300
 0
 5
 10
Days
 15
 20
(b) FN per day
Fig. 4. Error rates per day for Basic and Temporal FC with λ = 1.0 & Ω = 300
Temporal FC attempts to limit both the short term failure rate λ and a longer
term rate Ω. Chen suggested Ω be a daily rate and λ a per second rate. The
value of Ω is intended to be signiﬁcantly smaller than λ * (seconds in a day).
Hosts in a hash table entry are subjected to rate limiting if the failure rate of
the entry exceeds λ per second or Ω per day. The objective of temporal FC is to
catch prolonged but somewhat less aggressive scanning behavior—worms that
spread under the short-term rate of λ.
To evaluate these two algorithms we conducted experiments with the border
trace, with varying values of λ and Ω. Figure 4(a) and (b) show the error rates for
basic and temporal FC, with λ equaling 1 and Ω equaling 300, as recommended
by Chen. Figure 4(a) shows an increase in the false positive rates during the ﬁrst
week of infection. This increase is due to the fact that a worm generates rapid