by setting the liveness-bits of all variables in ğ‘†ğ‘Ÿğ‘ to â‹†, and those
of all others variables to â€¢.
Runs. We call a sequence of configurations ğœ‹ â‰œ Î£0Î£1 . . . Î£ğ‘›âˆ’1 a
run, if each consecutive pair of configurations is related by the
transition relation, i.e., if Î£ğ‘– â‡ Î£ğ‘–+1, for ğ‘– âˆˆ {0, . . . , ğ‘› âˆ’ 2}. We
call Î£0 â‰œ (ğ‘ƒ, ğœ0, ğœƒ0, 0, ğ‘¡, Src) initial state, and require that ğœƒ0 maps
all variables to â€¢. Finally, for a run ğœ‹ â‰œ (ğ‘ƒ, ğœ0, ğœƒ0, ğ‘0, ğ‘¡, Src) . . .
(ğ‘ƒ, ğœğ‘›âˆ’1, ğœƒğ‘›âˆ’1, ğ‘ğ‘›âˆ’1, ğ‘¡, Src), we say that ğœ‹ is a run of ğ‘ƒ of length ğ‘›
with respect to ğ‘¡ and Src and let store(ğœ‹, ğ‘–) = ğœğ‘– and live(ğœ‹, ğ‘–) â‰œ ğœƒğ‘–,
for ğ‘– âˆˆ {0, . . . , ğ‘› âˆ’ 1}.
Example. Consider again Figure 2. The Figure depicts two runs ğœ‹ğ¿
and ğœ‹ğ‘… of length 3 with respect to initial cycle 1 and source {in} of
the program in Figure 1. Columns in and out show store(ğœ‹, ğ‘–)(ğ‘–ğ‘›)
and store(ğœ‹, ğ‘–)(ğ‘œğ‘¢ğ‘¡), for ğœ‹ âˆˆ {ğœ‹ğ¿, ğœ‹ğ‘…} and ğ‘– âˆˆ {1, 2}. Similarly,
columns inâ€¢ and outâ€¢ show live(ğœ‹, ğ‘–)(in) and live(ğœ‹, ğ‘–)(out), for
ğœ‹ âˆˆ {ğœ‹ğ¿, ğœ‹ğ‘…}, and ğ‘– âˆˆ {1, 2}. The Figure omits the initial state at
cycle 0, where all liveness-bits are set to â€¢.
Flushed, Constant-Time, Public. For two runs ğœ‹ğ¿ and ğœ‹ğ‘… of
length ğ‘›, we say that variable ğ‘£ is flushed, if store(ğœ‹ğ¿, 0)(ğ‘£) =
store(ğœ‹ğ‘…, 0)(ğ‘£), we call ğ‘£ public, if store(ğœ‹ğ¿, ğ‘–)(ğ‘£) = store(ğœ‹ğ‘…, ğ‘–)(ğ‘£),
for ğ‘– âˆˆ {0, . . . , ğ‘› âˆ’ 1} and call ğ‘£ constant-time, if live(ğœ‹ğ¿, ğ‘–)(ğ‘£) =
live(ğœ‹ğ‘…, ğ‘–)(ğ‘£), for ğ‘– âˆˆ {0, . . . , ğ‘› âˆ’ 1}.
Secrecy Assumptions. Secrecy assumptions A â‰œ (Flush, Pub)
consists of a set of variables Flush âŠ† Vars that are assumed to
be flushed in the initial state, and a set of variables Pub âŠ† Vars,
that are assumed equal throughout. A pair of runs ğœ‹ğ¿ and ğœ‹ğ‘… of
length ğ‘›, satisfy a set of assumptions A, if, for each ğ‘£ âˆˆ Flush, ğ‘£ is
flushed, and for each ğ‘£ in Pub, ğ‘£ is public. We describe how Xenon
synthesizes secrecy assumptions in Â§ 4.
Constant-Time Execution. We now define constant-time execu-
tion with respect to a set of sinks Snk âŠ† Vars, sources Src, and
assumptions A. We say that a program ğ‘ƒ is constant-time, if for
any initial cycle ğ‘¡ and any pair of runs ğœ‹ğ¿ and ğœ‹ğ‘… of ğ‘ƒ with respect
to ğ‘¡ and Src of length ğ‘› that satisfy A, and any sink ğ‘œ âˆˆ Snk, ğ‘œ is
constant-time.
Example. Consider again Figure 2. If we assume that variables in
cycle 0 have the same value as in cycle 1, then out is flushed while in
is not. Neither in, nor out are public, but both are constant-time. As
out is constant-time in all runs, the program in Figure 1 is constant-
time with respect to the empty set of assumptions and sink {ğ‘œğ‘¢ğ‘¡}. In
Figure 4, none of the variables are public or constant-time, however,
the program in Figure 3 can be shown to be constant-time with
Pub = {IF_pc, rst}.
3.2 Verifying Constant-Time Execution via
Horn Constraints
To verify constant-time execution, we mirror the formal definition
in a set of Horn clauses [24]â€”an intermediate language for verifi-
cation. We start with the naive, monolithic encoding and discuss
how to make it modular in Â§ 3.3. At high level, the constraints
Session 2B: Formal Analysis and Verification CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea433init(ğ‘£ğ‘ ğ¿, ğ‘£ğ‘ ğ‘…) âˆ§ flush âˆ§ pub â‡’ inv(ğ‘£ğ‘ ğ¿, ğ‘£ğ‘ ğ‘…, 0, ğ‘¡)
(init)
to produce concrete counterexample traces, the necessary informa-
tion can often be recovered from the internal solver state.
(cid:19)
(cid:18)inv(ğ‘£ğ‘ ğ¿, ğ‘£ğ‘ ğ‘…, ğ‘, ğ‘¡) âˆ§ pub
âˆ§ next(ğ‘£ğ‘ ğ¿, ğ‘£ğ‘ ğ‘…, ğ‘£ğ‘ â€²
ğ‘…, ğ‘¡)
inv(ğ‘£ğ‘ ğ¿, ğ‘£ğ‘ ğ‘…, ğ‘, ğ‘¡) âˆ§ pub â‡’ ğ‘œâ€¢
ğ¿, ğ‘£ğ‘ â€²
â‡’ inv(ğ‘£ğ‘ â€²
ğ¿, ğ‘£ğ‘ â€²
ğ‘…, ğ‘ + 1, ğ‘¡) (cons)
ğ¿ = ğ‘œâ€¢
ğ‘…, for ğ‘œ âˆˆ Snk
(ct)
Figure 7: Horn clause encoding of the verification conditions for
constant-time execution.
(1) issue a new live instruction at a non-deterministically chosen
initial cycle ğ‘¡, and
(2) ensure constant-time execution by verifying that the liveness-
bits for each sink are always the same, in any two runs.
The clausesâ€”shown in Figure 7â€”encode verification conditions
over an inductive invariant inv(ğ‘£ğ‘ ğ¿, ğ‘£ğ‘ ğ‘…, ğ‘, ğ‘¡) of the product circuit,
where ğ‘£ğ‘  ranges over all variables in the circuit and their respec-
tive liveness bits, and ğ‘ and ğ‘¡ are the current and initial cycles,
respectively.
Initial States and Transition Relation. Formula init(ğ‘£ğ‘ ğ¿, ğ‘£ğ‘ ğ‘…)
describes the product circuitâ€™s initial states and requires all liveness-
bits to be set to â€¢. To ensure that the proof holds for any initial cycle,
init does not constrain ğ‘¡. Formula next(ğ‘£ğ‘ ğ¿, ğ‘£ğ‘ ğ‘…, ğ‘£ğ‘ â€²
ğ‘…, ğ‘¡) encodes
the transition relation of the product circuit, where un-primed vari-
ables represent state before, and primed variables represent state
after the transition. Like â‡, next sets liveness-bits of all sources
to â‹† at clock cycle ğ‘¡. Importantly, constructing next requires inlin-
ing all modules and therefore can lead to large constraints that are
beyond the abilities of the solver.
Assumptions. For a set of assumptions A â‰œ (Flush, Pub), we con-
struct formulas flush and pub, both of which require the variables
in their respective sets to be equal in the two runs. We let
ğ¿, ğ‘£ğ‘ â€²
flush â‰œ (âˆ§ğ‘¥âˆˆFlush ğ‘¥ğ¿ = ğ‘¥ğ‘…) and pub â‰œ (âˆ§ğ‘¥âˆˆPub ğ‘¥ğ¿ = ğ‘¥ğ‘…) .
Horn Constraints & Solutions. We then require that the invari-
ant holds initially (init), assuming all variables in Flush and Pub
are equal in both runs; that the invariant is preserved under the
transition relation of the product circuit, assuming that public vari-
ables are equal in both runs (cons), and finally, that the liveness-bits
of any sink are the same in both runs (ct). These constraints can
then be passed to any of a vast array of existing Horn constraint
solvers [8, 34, 37, 44, 52, 55, 57] yielding a formula which, when
substituted for inv, makes all implications valid and thus proves
constant-time execution.
Proof Artifacts. To compute constant-time counterexamples and
synthesize secrecy assumptions upon a failed proof attempt, as
described in the next section, Xenon requires the solver to generate
the following artifacts:
(1) the set of variables which remained constant-time and public,
during the current failed proof attempt, and
(2) the order in which the remaining variables lost the respective
properties.
These artifacts can, for example, be extracted from a concrete coun-
terexample trace like Figure 4. However, even if the solver is unable
3.3 Finding Modular Invariants
Naively, constructing next requires all the code to be in a single mod-
ule. However, this can yield gigantic circuits whose Horn clauses
are too large to analyze efficiently. To avoid instantiating the entire
module at each usage site, Xenon constructs module summaries that
concisely describe the timing relevant properties of the moduleâ€™s
input and output ports.
Per-Module Invariants and Summaries. Instead of a single whole
program invariant inv, the modular analysis constructs a per-module
invariant invğ‘š, and an additional summary sumğ‘š, for each mod-
ule ğ‘š. The summary only ranges over module inputs and out-
puts, and respective liveness-bits (io) and needs to include all in-
put/output behavior captured by the invariant, i.e., we add a clause:
invğ‘š(ğ‘£ğ‘ ğ¿, ğ‘£ğ‘ ğ‘…, ğ‘¡) â‡’ sumğ‘š(ğ‘–ğ‘œğ¿, ğ‘–ğ‘œğ‘…, ğ‘¡) . The analysis produces the
same constraints as before, but now on a per-module basis, that is,
we require module invariants to hold on initial states (init), and be
preserved under the transition relation (cons), but, instead of using
the overall transition relation next we use a per-module transition
relation nextğ‘š. It may now happen that nextğ‘š makes use of a mod-
ule ğ‘›, but instead of inlining the transition relation of ğ‘› as before,
we substitute it by its module summary sumğ‘›, thereby avoiding the
blowup in constraint size. Finally, we restrict sources and sinks to
occur at the top-level module, and add a clause requiring that any
sink has the same liveness-bits in both runs (ct). The summaries are
also used to modularize our assumption synthesis algorithm Â§ 4.3,
which is crucial for our modular verification approach, as we will
discuss in Â§ 7.
Solving Modularity Constraints. To solve the modular Horn con-
straints, the solver first computes an invariant for each module,
and then uses quantifier elimination [58] to project the moduleâ€™s
behavior onto its inputs and outputs, which yields the summary.
Since a moduleâ€™s summary may show up in another moduleâ€™s tran-
sition relation and thereby influence its invariant, this yields an
interdependent constraint system, which we solve via a fix-point
iteration loop [24, 52].
4 COUNTEREXAMPLES & ASSUMPTION
SYNTHESIS
We now explain how Xenon uses the proof artifacts to help the
user understand and explicate secrecy assumptions when verifi-
cation fails. We first describe how Xenon analyzes the artifacts
from the failed proof attempt in order to compute a counterexample
consisting of the set of variables thatâ€”according to the informa-
tion communicated by the proverâ€”lost the constant-time property
first (Â§ 4.1). Next, we discuss how Xenon uses the counterexample
to synthesize a set of secrecy assumptions that eliminate the root
cause of the verification failure (Â§ 4.2). This is done by computing a
blame-set that contains the variables that likely caused the loss of
constant-time for the variables in the counterexample via a control
dependency. This blame set is then used to encode an optimization
problem whose solution determines a minimal set of assumptions
required to remove the timing violation. Finally, we briefly discuss
Session 2B: Formal Analysis and Verification CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea434how Xenon uses module summaries to speed up counterexample
generation and secrecy assumption synthesis (Â§ 4.3).
4.1 Computing Counterexamples
Dependency Graph. To compute the counterexample from a failed
proof attempt, Xenon first creates a dependency graph ğº â‰œ (ğ‘‰ , ğ· âˆª
ğ¶) which encodes data- and control-dependencies between program
variables. ğº consists of
â–¶ variables ğ‘‰ âŠ† Vars,
â–¶ data-dependencies ğ· âŠ† (VarsÃ—Vars), where (ğ‘£, ğ‘¤) âˆˆ ğ· if ğ‘£â€™s
value is used to compute ğ‘¤ directly through an assignment,
and
â–¶ control-dependencies ğ¶ âŠ† (VarsÃ— Vars) where (ğ‘£, ğ‘¤) âˆˆ ğ¶, if
ğ‘£â€™s value is used indirectly, i.e., ğ‘¤â€™s value is computed under
a branch whose condition depends on ğ‘£.
Variable-Time Map. Next, Xenon extracts an artifact from the
failed proof attempt: a partial map varTime âˆˆ (Vars â‡€ N) which
records the temporal order in which variables started to exhibit
timing variability. Importantly, if for some ğ‘£ varTime is undefined
(varTime(ğ‘£) = âŠ¥), ğ‘£ was constant-time throughout the failed proof
attempt. For any other variables ğ‘£ and ğ‘¤, if varTime(ğ‘£)  varTime(ğ‘£). Intuitively, if variable
ğ‘¤ has started to exhibit timing variability after variable ğ‘£, it cannot
be the cause for ğ‘£ losing the constant-time property. Finally, Xenon
removes all nodes that cannot reach a sink node using the remaining
edges. This leaves us with a set of variables Cex âŠ† Vars without
incoming edges, which we presentâ€”as counterexampleâ€”to the user.
We now define the reduced graph in more detail.
Reachability. For dependency graph ğº â‰œ (ğ‘‰ , ğ· âˆª ğ¶) and nodes
ğ‘£, ğ‘¤ âˆˆ ğ‘‰ we write ğ‘£ â†’ ğ‘¤, if (ğ‘£, ğ‘¤) âˆˆ (ğ· âˆª ğ¶), ğ‘£ â†’ğ‘› ğ‘¤, if there
is a sequence ğ‘£0ğ‘£1 . . . ğ‘£ğ‘›âˆ’1, such that ğ‘£0 = ğ‘£ and ğ‘£ğ‘›âˆ’1 = ğ‘¤, and
ğ‘£ğ‘– â†’ ğ‘£ğ‘–+1 for ğ‘– âˆˆ {0, . . . , ğ‘› âˆ’ 2}. Finally, we say ğ‘¤ is reachable from
ğ‘£, if there exists ğ‘› such that ğ‘£ â†’ğ‘› ğ‘¤.
Reduced Graph. For a data-flow graph ğº â‰œ (ğ‘‰ , ğ· âˆª ğ¶), and map
varTime, we define the reduced graph with respect to varTime as
the largest subgraph ğºâ€² â‰œ (ğ‘‰ â€², ğ·â€² âˆª ğ¶â€²) such that ğ‘‰ â€² âŠ† ğ‘‰ , ğ·â€² âŠ† ğ·,
ğ¶â€² âŠ† ğ¶ and
(1) No node is constant-time, i.e., for all ğ‘£ âˆˆ ğ‘‰ â€², varTime(ğ‘£) â‰  âŠ¥.
(2) All edges respect the causal order given by varTime, i.e., for
all (ğ‘£, ğ‘¤) âˆˆ (ğ·â€² âˆª ğ¶â€²), we have varTime(ğ‘£) â‰¤ varTime(ğ‘¤).
(3) All nodes can reach a sink, i.e., for all ğ‘£ âˆˆ ğ‘‰ â€², there is ğ‘œ âˆˆ Snk
such that ğ‘œ is reachable from ğ‘£.
3More formally, for variables ğ‘£, ğ‘¤ if varTime(ğ‘£) < varTime(ğ‘¤), then there exist
two runs ğœ‹ğ¿ and ğœ‹ğ‘… of some length ğ‘›, and two numbers 0 â‰¤ ğ‘–, ğ‘— < ğ‘› such that
ğ‘– is the smallest number such that live(ğœ‹ğ¿, ğ‘–)(ğ‘£) â‰  live(ğœ‹ğ‘…, ğ‘–)(ğ‘£) and similarly ğ‘—
is the smallest number such that live(ğœ‹ğ¿, ğ‘—)(ğ‘¤) â‰  live(ğœ‹ğ‘…, ğ‘—)(ğ‘¤) and ğ‘– < ğ‘—. This
information can e.g., be extracted from a concrete counterexample trace, like the one
shown in Figure 4.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
assign ID_rt = ID_instr [ 20 : 16 ];
rom32 IMEM ( IF_pc , IF_instr );
always @(*)
stall = ( ID_rt == EX_rt );
always @( posedge clk ) begin
if ( Stall == 1) begin
ID_instr
EX_rt
<= ID_instr ;
<= EX_rt ;
end else begin
ID_instr
EX_rt
<= IF_instr ;
<= ID_rt ;
end
end
Figure 8: Simplified MIPS Pipeline Fragment in Verilog.
For variable ğ‘£, and graph ğº â‰œ (ğ‘‰ , ğ· âˆª ğ¶), let pre(ğ‘£, ğº) be the set of
its immediate predecessors in ğº, that is
pre(ğ‘£, ğº) â‰œ {ğ‘¤ | (ğ‘¤, ğ‘£) âˆˆ (ğ· âˆª ğ¶)}.
We define the counterexample Cex of a graph ğº with map varTime
as the set of nodes in the reduced graph ğºâ€² (wrt. varTime), that
have no predecessors, i.e., Cex â‰œ {ğ‘£ | pre(ğ‘£, ğºâ€²) = âˆ…} .
Example: Simplified Pipeline. The code in Figure 8 shows a sim-
plified version of the pipelined processor from Figure 3. Like in
Figure 3, the pipeline either stalls (Lines 10 and 11) if flag Stall
is set (Line 9), or else forwards values to the next stage (Lines 13
and 14). To avoid a write-after-write data-hazard, the Stall flag is
set, if the instructions in the execute and decode stage have the same
target registers (Line 6). The target register is calculated from the
current instruction (Line 1), and the instruction is, in turn, fetched
from memory using the current program counter (Line 3). Note
the cyclic dependency between ID_instr and Stall that turns
comprehending the root cause into a â€œchicken-and-eggâ€ problem.
Dependency Graph. To check if the pipeline fragment executes
in constant-time, we mark IF_pc as source, and ID_instr as sink
and run Xenon. Since the pipeline is variable-time, the verification
fails. To compute a minimal counterexample, Xenon creates the
dependency graph shown in Figure 9a. Each node is annotated with
information extracted from the failed proof attempt: the node is
labeled with its value under varTime, and is marked with (âœ“) if
the variable remained constant-time throughout the proof attempt
and (X) otherwise. Solid edges represent data- and dashed edges
represent control-dependencies.
Reduced Dependency Graph. Figure 9b shows the dependency
graph after removing all constant-time nodes and edges that vio-
late the causal ordering. Xenon erases all nodes that cannot reach
sink ID_instr. This only leaves ID_instr which we return as coun-
terexample. The ordering induced by varTime allowed us to break
the cyclic dependency between variables Stall and ID_instr,
thereby resolving the chicken-and-egg problem.
Remark. In case the proof artifact only partially resolves the cyclic
dependencies, that is, varTime only defines a partial order over
non-constant-time variables, the reduced graph may still contain
cycles, and therefore there may be no nodes without predecessor.
Session 2B: Formal Analysis and Verification CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea435Stall (X, 3)
Stall (X, 3)
IF_inst (âœ“, âŠ¥)
IF_pc (âœ“, âŠ¥)
ID_instr (X, 1)
EX_rt (X, 3)
ID_instr (X, 1)
EX_rt (X, 3)
ID_rt (X, 2)
ID_rt (X, 2)
(a) Dependency Graph.
(b) Dependency Graph after eliminating
constant-time nodes and edges violating the
order given by varTime.
Summary Graph
IF_instr (âœ“, âŠ¥)
ID_instr (X, 1)
Stall (X, 3)
Stall (X, 3)
IF_inst (âœ“, âŠ¥)
IF_pc (âœ“, âŠ¥)
ID_instr (X, 1)
EX_rt (X, 3)
ID_rt (X, 2)
(c) Dependency Graph with Module Summary.
Figure 9: Figure 9a shows the dependency graph for Figure 8. Data-dependencies are shown as solid edges, and control-dependencies are shown
dashed. Each node is labeled with its varTime-value and marked (âœ“) if the variable remained constant-time throughout the proof attempt and
(X) otherwise. Figure 9b shows the dependency graph after eliminating constant-time nodes from Figure 9a, and removing edges that violate
the variable-time map. Removing the edge between Stall and ID_instr breaks the cyclic dependency in the original graph. Figure 9c shows
the variable dependency graph with a summary graph extracted from the module summary.
We can however still apply our technique by computing the graphâ€™s
strongly connected components and including all nodes in the
respective component in the counterexample.
4.2 Assumption Synthesis