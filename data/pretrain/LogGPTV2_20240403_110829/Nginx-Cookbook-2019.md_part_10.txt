### 9.4 Bandwidth Limits

The `mp4_limit_rate` directive in NGINX controls the download speed relative to the media's bitrate. Setting this directive to a value of 1 ensures that the download rate is limited to match the media's bitrate exactly, providing a 1-to-1 ratio. If you set the value to greater than 1, users can download the media at a faster rate than they are consuming it, allowing for buffering and seamless playback during the download process.

---

## Chapter 10: Cloud Deployments

### 10.0 Introduction

The rise of cloud providers has transformed the landscape of web application hosting. Tasks such as provisioning new machines, which once took hours or even months, can now be accomplished with a single click or API call. These cloud providers offer Infrastructure as a Service (IaaS) through virtual machines, as well as managed software solutions like databases, on a pay-per-usage model. This means you only pay for what you use, enabling engineers to quickly set up and tear down testing environments as needed. Additionally, cloud platforms allow applications to scale horizontally based on performance demands. This chapter will cover basic NGINX and NGINX Plus deployments on major cloud provider platforms.

### 10.1 Auto-Provisioning on AWS

**Problem:**
You need to automate the configuration of NGINX servers on Amazon Web Services (AWS) to enable automatic provisioning.

**Solution:**
Use EC2 UserData and a prebaked Amazon Machine Image (AMI). Create an AMI with NGINX and any necessary supporting software installed. Use EC2 UserData to configure environment-specific settings at runtime.

**Discussion:**
There are three main approaches to provisioning on AWS:

- **Provision at Boot:**
  Start from a common Linux image and run configuration management or shell scripts at boot time to set up the server. This method is slow and prone to errors.

- **Fully Baked AMIs:**
  Fully configure the server and create an AMI. This approach boots quickly and accurately but is less flexible and more complex to maintain multiple images.

- **Partially Baked AMIs:**
  A hybrid approach where essential software is installed and baked into an AMI, and environment-specific configurations are done at boot time. This offers a balance between flexibility and speed.

To automate the creation of AMIs, consider using tools like configuration management (e.g., Ansible, Chef, Puppet, SaltStack) and Packer from HashiCorp. Packer automates the process of running configuration management and creating machine images across various platforms.

For runtime configuration, use EC2 UserData to execute commands when the instance first boots. This can include setting environment variables, downloading configuration files, or running scripts. Configuration management tools ensure the system is correctly configured, template configuration files, and reload services as needed.

### 10.2 Routing to NGINX Nodes Without an AWS ELB

**Problem:**
You need to route traffic to multiple active NGINX nodes or create an active-passive failover setup without using an AWS Elastic Load Balancer (ELB).

**Solution:**
Use Amazon Route53 DNS service to route traffic to multiple NGINX nodes or configure health checks and failover between an active-passive set of NGINX nodes.

**Discussion:**
Route53 provides advanced DNS features, including multiple IP addresses on a single A record and weighted A records. For load balancing, use round-robin or weighted distribution. Route53 also supports health checks, allowing you to monitor endpoints and remove unhealthy IPs from rotation. You can configure Route53 to fail over to a secondary record in case of failure, ensuring high availability.

Route53's geographical routing feature directs clients to the closest healthy NGINX node, minimizing latency. For auto-scaling, use AWS Auto Scaling Lifecycle Hooks to trigger scripts that add or remove DNS records and health checks as instances are created or terminated.

### 10.3 The NLB Sandwich

**Problem:**
You need to autoscale your NGINX Open Source layer and distribute load evenly between application servers.

**Solution:**
Create a Network Load Balancer (NLB) and an Auto Scaling group with a launch configuration that provisions EC2 instances with NGINX Open Source. Link the Auto Scaling group to the NLB target group, which automatically registers and deregisters instances. Place your upstream applications behind another NLB and configure NGINX to proxy to the application NLB.

**Discussion:**
This pattern, known as the "NLB sandwich," involves placing NGINX Open Source in an Auto Scaling group behind an NLB, with the application Auto Scaling group behind another NLB. NLBs work well with Auto Scaling groups by automatically registering and deregistering nodes, performing health checks, and directing traffic to healthy nodes. This setup is preferred over Elastic Load Balancers (ELBs) because NLBs are Layer 4 load balancers, while ELBs and Application Load Balancers (ALBs) are Layer 7 and redundant with NGINX. This pattern is not necessary for NGINX Plus, as it includes the required features.

### 10.4 Deploying from the AWS Marketplace

**Problem:**
You need to run NGINX Plus in AWS with ease and a pay-as-you-go license.

**Solution:**
Deploy NGINX Plus through the AWS Marketplace. Search for "NGINX Plus" in the Marketplace, select the desired AMI, review the details, and deploy with a single click or accept the terms and use the AMI.

**Discussion:**
The AWS Marketplace solution for NGINX Plus provides a simple, pay-as-you-go licensing model. This allows you to try NGINX Plus without long-term commitments and can be used for overflow capacity. You can purchase licenses for your expected workload and use the Marketplace AMI in an Auto Scaling group to handle additional capacity, ensuring you only pay for what you use.

### 10.5 Creating an NGINX Virtual Machine Image on Azure

**Problem:**
You need to create a virtual machine (VM) image of your NGINX server for quick deployment or use in scale sets.

**Solution:**
Create a VM from a base operating system, install and configure NGINX, and then generalize the VM. Use the Azure CLI to deallocate and generalize the VM, and then capture the image. Use the generated ARM template to create new VMs from the custom image.

**Discussion:**
Creating a custom image in Azure allows you to quickly and reliably deploy preconfigured NGINX servers. The ARM template enables you to deploy the same server repeatedly as needed, and the VM image path in the template can be used to create different infrastructure setups, such as VM scale sets.

### 10.6 Load Balancing Over NGINX Scale Sets on Azure

**Problem:**
You need to scale NGINX nodes behind an Azure load balancer for high availability and dynamic resource usage.

**Solution:**
Create an Azure load balancer and deploy the NGINX VM image into a virtual machine scale set (VMSS). Configure a backend pool on the load balancer to the VMSS and set up load-balancing rules for the desired ports and protocols.

**Discussion:**
Scaling NGINX in Azure using VMSS and load balancers provides high availability and efficient resource management. The load balancer manages the addition and removal of NGINX nodes, performs health checks, and directs traffic to healthy nodes. Internal load balancers can be used to restrict access to an internal network, and NGINX can proxy to these internal load balancers for application scaling.

### 10.7 Deploying Through the Azure Marketplace

**Problem:**
You need to run NGINX Plus in Azure with ease and a pay-as-you-go license.

**Solution:**
Deploy an NGINX Plus VM image through the Azure Marketplace. Search for "NGINX Plus," select the appropriate image, and follow the prompts to configure and deploy the VM.

**Discussion:**
The Azure Marketplace simplifies the deployment of NGINX Plus, offering a pay-as-you-go licensing model. This allows you to try out NGINX Plus features or use it for on-demand overflow capacity for your existing NGINX Plus servers.

### 10.8 Deploying to Google Compute Engine

**Problem:**
You need to create an NGINX server in Google Compute Engine for load balancing or proxying resources in Google Compute or App Engine.

**Solution:**
Start a new VM in Google Compute Engine, selecting the desired name, zone, machine type, and boot disk. Configure identity and access management, firewall, and any other advanced settings. Once the VM is created, log in via SSH or Google Cloud Shell, install NGINX or NGINX Plus, and configure it as needed.

**Discussion:**
Google Compute Engine provides a straightforward way to deploy and configure NGINX for load balancing and proxying. This setup can be used to manage and distribute traffic to other resources in Google Compute or App Engine.