allowed to download in relation to the bitrate of the media. A value
of 1 provided to the mp4_limit_rate directive specifies that NGINX
is to limit bandwidth (1-to-1) to the bitrate of the media. Providing
a value of more than one to the mp4_limit_rate directive will allow
users to download faster than they watch so they can buffer the
media and watch seamlessly while they download.
9.4 Bandwidth Limits | 97
CHAPTER 10
Cloud Deployments
10.0 Introduction
The advent of cloud providers has changed the landscape of web
application hosting. A process such as provisioning a new machine
used to take hours to months; now, you can create one with as little
as a click or API call. These cloud providers lease their virtual
machines, called Infrastructure as a Service (IaaS), or managed soft‐
ware solutions such as databases, through a pay-per-usage model,
which means you pay only for what you use. This enables engineers
to build up entire environments for testing at a moment’s notice and
tear them down when they’re no longer needed. These cloud provid‐
ers also enable applications to scale horizontally based on perfor‐
mance need at a moment’s notice. This chapter covers basic NGINX
and NGINX Plus deployments on a couple of the major cloud pro‐
vider platforms.
10.1 Auto-Provisioning on AWS
Problem
You need to automate the configuration of NGINX servers on Ama‐
zon Web Services for machines to be able to automatically provision
themselves.
99
Solution
Utilize EC2 UserData as well as a prebaked Amazon Machine
Image. Create an Amazon Machine Image (AMI) with NGINX and
any supporting software packages installed. Utilize Amazon EC2
UserData to configure any environment-specific configurations at
runtime.
Discussion
There are three patterns of thought when provisioning on Amazon
Web Services:
Provision at boot
Start from a common Linux image, then run configuration
management or shell scripts at boot time to configure the
server. This pattern is slow to start and can be prone to errors.
Fully baked AMIs
Fully configure the server, then burn an AMI to use. This pat‐
tern boots very fast and accurately. However, it’s less flexible to
the environment around it, and maintaining many images can
be complex.
Partially baked AMIs
It’s a mix of both worlds. Partially baked is where software
requirements are installed and burned into an AMI, and envi‐
ronment configuration is done at boot time. This pattern is flex‐
ible compared to a fully baked pattern, and fast compared to a
provision-at-boot solution.
Whether you choose to partially or fully bake your AMIs, you’ll
want to automate that process. To construct an AMI build pipeline,
it’s suggested to use a couple of tools:
Configuration management
Configuration management tools define the state of the server
in code, such as what version of NGINX is to be run and what
user it’s to run as, what DNS resolver to use, and who to proxy
upstream to. This configuration management code can be
source controlled and versioned like a software project. Some
popular configuration management tools are Ansible, Chef,
Puppet, and SaltStack, which were described in Chapter 5.
100 | Chapter 10: Cloud Deployments
Packer from HashiCorp
Packer is used to automate running your configuration manage‐
ment on virtually any virtualization or cloud platform and to
burn a machine image if the run is successful. Packer basically
builds a virtual machine on the platform of your choosing,
SSHes into the virtual machine, runs any provisioning you spec‐
ify, and burns an image. You can utilize Packer to run the con‐
figuration management tool and reliably burn a machine image
to your specification.
To provision environmental configurations at boot time, you can
utilize the Amazon EC2 UserData to run commands the first time
the instance is booted. If you’re using the partially baked method,
you can utilize this to configure environment-based items at boot
time. Examples of environment-based configurations might be what
server names to listen for, resolver to use, domain name to proxy to,
or upstream server pool to start with. UserData is a base64-encoded
string that is downloaded at the first boot and run. UserData can be
as simple as an environment file accessed by other bootstrapping
processes in your AMI, or it can be a script written in any language
that exists on the AMI. It’s common for UserData to be a bash script
that specifies variables or downloads variables to pass to configura‐
tion management. Configuration management ensures the system is
configured correctly, templates configuration files based on environ‐
ment variables, and reloads services. After UserData runs, your
NGINX machine should be completely configured, in a very reliable
way.
10.2 Routing to NGINX Nodes Without an AWS
ELB
Problem
You need to route traffic to multiple active NGINX nodes or create
an active-passive failover set to achieve high availability without a
load balancer in front of NGINX.
10.2 Routing to NGINX Nodes Without an AWS ELB | 101
Solution
Use the Amazon Route53 DNS service to route to multiple active
NGINX nodes or configure health checks and failover between an
active-passive set of NGINX nodes.
Discussion
DNS has balanced load between servers for a long time; moving to
the cloud doesn’t change that. The Route53 service from Amazon
provides a DNS service with many advanced features, all available
through an API. All the typical DNS tricks are available, such as
multiple IP addresses on a single A record and weighted A records.
When running multiple active NGINX nodes, you’ll want to use one
of these A record features to spread load across all nodes. The
round-robin algorithm is used when multiple IP addresses are listed
for a single A record. A weighted distribution can be used to distrib‐
ute load unevenly by defining weights for each server IP address in
an A record.
One of the more interesting features of Route53 is its ability to
health check. You can configure Route53 to monitor the health of an
endpoint by establishing a TCP connection or by making a request
with HTTP or HTTPS. The health check is highly configurable with
options for the IP, hostname, port, URI path, interval rates, moni‐
toring, and geography. With these health checks, Route53 can take
an IP out of rotation if it begins to fail. You could also configure
Route53 to failover to a secondary record in case of a failure, which
would achieve an active-passive, highly available setup.
Route53 has a geological-based routing feature that will enable you
to route your clients to the closest NGINX node to them, for the
least latency. When routing by geography, your client is directed to
the closest healthy physical location. When running multiple sets of
infrastructure in an active-active configuration, you can automati‐
cally failover to another geological location through the use of
health checks.
When using Route53 DNS to route your traffic to NGINX nodes in
an Auto Scaling group, you’ll want to automate the creation and
removal of DNS records. To automate adding and removing NGINX
machines to Route53 as your NGINX nodes scale, you can use Ama‐
zon’s Auto Scaling Lifecycle Hooks to trigger scripts within the
102 | Chapter 10: Cloud Deployments
NGINX box itself or scripts running independently on Amazon
Lambda. These scripts would use the Amazon CLI or SDK to inter‐
face with the Amazon Route53 API to add or remove the NGINX
machine IP and configured health check as it boots or before it is
terminated.
Also See
Amazon Route53 Global Server Load Balancing
10.3 The NLB Sandwich
Problem
You need to autoscale your NGINX Open Source layer and distrib‐
ute load evenly and easily between application servers.
Solution
Create a network load balancer (NLB). During creation of the NLB
through the console, you are prompted to create a new target group.
If you do not do this through the console, you will need to create
this resource and attach it to a listener on the NLB. You create an
Auto Scaling group with a launch configuration that provisions an
EC2 instance with NGINX Open Source installed. The Auto Scaling
group has a configuration to link to the target group, which auto‐
matically registers any instance in the Auto Scaling group to the tar‐
get group configured on first boot. The target group is referenced by
a listener on the NLB. Place your upstream applications behind
another network load balancer and target group and then configure
NGINX to proxy to the application NLB.
Discussion
This common pattern is called the NLB sandwich (see Figure 10-1),
putting NGINX Open Source in an Auto Scaling group behind an
NLB and the application Auto Scaling group behind another NLB.
The reason for having NLBs between every layer is because the NLB
works so well with Auto Scaling groups; they automatically register
new nodes and remove those being terminated as well as run health
checks and pass traffic to only healthy nodes. It might be necessary
to build a second, internal NLB for the NGINX Open Source layer
10.3 The NLB Sandwich | 103
because it allows services within your application to call out to other
services through the NGINX Auto Scaling group without leaving the
network and re-entering through the public NLB. This puts NGINX
in the middle of all network traffic within your application, making
it the heart of your application’s traffic routing. This pattern used to
be called the elastic load balancer (ELB) sandwich; however, the NLB
is preferred when working with NGINX because the NLB is a Layer
4 load balancer, whereas ELBs and ALBs are Layer 7 load balancers.
Layer 7 load balancers transform the request via the proxy protocol
and are redundent with the use of NGINX. This pattern is needed
only for NGINX Open Source because the feature set provided by
the NLB is available in NGINX Plus.
104 | Chapter 10: Cloud Deployments
Figure 10-1. This image depicts NGINX in an NLB sandwich pattern
with an internal NLB for internal applications to utilize. A user makes
a request to App-1, and App-1 makes a request to App-2 through
NGINX to fulfill the user’s request.
10.4 Deploying from the AWS Marketplace
Problem
You need to run NGINX Plus in AWS with ease with a pay-as-you-
go license.
10.4 Deploying from the AWS Marketplace | 105
Solution
Deploy through the AWS Marketplace. Visit the AWS Marketplace
and search “NGINX Plus” (see Figure 10-2). Select the AMI that is
based on the Linux distribution of your choice; review the details,
terms, and pricing; then click the Continue link. On the next page
you’ll be able to accept the terms and deploy NGINX Plus with a
single click, or accept the terms and use the AMI.
Figure 10-2. Searching for NGINX on the AWS Marketplace
Discussion
The AWS Marketplace solution to deploying NGINX Plus provides
ease of use and a pay-as-you-go license. Not only do you have noth‐
ing to install, but you also have a license without jumping through
hoops like getting a purchase order for a year license. This solution
enables you to try NGINX Plus easily without commitment. You can
also use the NGINX Plus Marketplace AMI as overflow capacity. It’s
a common practice to purchase your expected workload worth of
licenses and use the Marketplace AMI in an Auto Scaling group as
overflow capacity. This strategy ensures you only pay for as much
licensing as you use.
106 | Chapter 10: Cloud Deployments
10.5 Creating an NGINX Virtual Machine Image
on Azure
Problem
You need to create a virtual machine (VM) image of your own
NGINX server configured as you see fit to quickly create more
servers or use in scale sets.
Solution
Create a VM from a base operating system of your choice. Once the
VM is booted, log in and install NGINX or NGINX Plus in your
preferred way, either from source or through the package manage‐
ment tool for the distribution you’re running. Configure NGINX as
desired and create a new VM image. To create a VM image, you
must first generalize the VM. To generalize your VM, you need to
remove the user that Azure provisioned, connect to it over SSH,
and run the following command:
$ sudo waagent -deprovision+user -force
This command deprovisions the user that Azure provisioned when
creating the VM. The -force option simply skips a confirmation
step. After you’ve installed NGINX or NGINX Plus and removed the
provisioned user, you can exit your session.
Connect your Azure CLI to your Azure account using the Azure
login command, then ensure you’re using the Azure Resource Man‐
ager mode. Now deallocate your VM:
$ azure vm deallocate -g  \
-n 
Once the VM is deallocated, you will be able to generalize it with the
azure vm generalize command:
$ azure vm generalize -g  \
-n 
After your VM is generalized, you can create an image. The follow‐
ing command will create an image and also generate an Azure
Resources Manager (ARM) template for you to use to boot this
image:
$ azure vm capture   \
 -t .json
10.5 Creating an NGINX Virtual Machine Image on Azure | 107
The command line will produce output saying that your image has
been created, that it’s saving an ARM template to the location you
specified, and that the request is complete. You can use this ARM
template to create another VM from the newly created image. How‐
ever, to use this template Azure has created, you must first create a
new network interface:
$ azure network nic create  \
 \
 \
--subnet-name  \
--subnet-vnet-name 
This command output will detail information about the newly cre‐
ated network interface. The first line of the output data will be the
network interface ID, which you will need to utilize the ARM tem‐
plate created by Azure. Once you have the ID, you can create a
deployment with the ARM template:
$ azure group deployment create  \
 \
-f .json
You will be prompted for multiple input variables such as vmName,
adminUserName, adminPassword, and networkInterfaceId. Enter a
name for the VM and the admin username and password. Use the
network interface ID harvested from the last command as the input
for the networkInterfaceId prompt. These variables will be passed
as parameters to the ARM template and used to create a new VM
from the custom NGINX or NGINX Plus image you’ve created.
After entering the necessary parameters, Azure will begin to create a
new VM from your custom image.
Discussion
Creating a custom image in Azure enables you to create copies of
your preconfigured NGINX or NGINX Plus server at will. An Azure
ARM template enables you to quickly and reliably deploy this same
server time and time again as needed. With the VM image path that
can be found in the template, you can create different sets of infra‐
structure such as VM scaling sets or other VMs with different con‐
figurations.
108 | Chapter 10: Cloud Deployments
Also See
Installing Azure Cross-platform CLI
Azure Cross-platform CLI Login
Capturing Linux Virtual Machine Images
10.6 Load Balancing Over NGINX Scale Sets on
Azure
Problem
You need to scale NGINX nodes behind an Azure load balancer to
achieve high availability and dynamic resource usage.
Solution
Create an Azure load balancer that is either public facing or inter‐
nal. Deploy the NGINX virtual machine image created in the prior
section or the NGINX Plus image from the Marketplace described
in Recipe 10.7 into an Azure virtual machine scale set (VMSS). Once
your load balancer and VMSS are deployed, configure a backend
pool on the load balancer to the VMSS. Set up load-balancing rules
for the ports and protocols you’d like to accept traffic on, and direct
them to the backend pool.
Discussion
It’s common to scale NGINX to achieve high availability or to han‐
dle peak loads without overprovisioning resources. In Azure you
achieve this with VMSS. Using the Azure load balancer provides
ease of management for adding and removing NGINX nodes to the
pool of resources when scaling. With Azure load balancers, you’re
able to check the health of your backend pools and only pass traffic
to healthy nodes. You can run internal Azure load balancers in front
of NGINX where you want to enable access only over an internal
network. You may use NGINX to proxy to an internal load balancer
fronting an application inside of a VMSS, using the load balancer for
the ease of registering and deregistering from the pool.
10.6 Load Balancing Over NGINX Scale Sets on Azure | 109
10.7 Deploying Through the Azure
Marketplace
Problem
You need to run NGINX Plus in Azure with ease and a pay-as-you-
go license.
Solution
Deploy an NGINX Plus VM image through the Azure Marketplace:
1. From the Azure dashboard, select the New icon, and use the
search bar to search for “NGINX.” Search results will appear.
2. From the list, select the NGINX Plus Virtual Machine Image
published by NGINX, Inc.
3. When prompted to choose your deployment model, select the
Resource Manager option, and click the Create button.
4. You will then be prompted to fill out a form to specify the name
of your VM, the disk type, the default username and password
or SSH key-pair public key, which subscription to bill under, the
resource group you’d like to use, and the location.
5. Once this form is filled out, you can click OK. Your form will be
validated.
6. When prompted, select a VM size, and click the Select button.
7. On the next panel, you have the option to select optional con‐
figurations, which will be the default based on your resource
group choice made previously. After altering these options and
accepting them, click OK.
8. On the next screen, review the summary. You have the option of
downloading this configuration as an ARM template so that you
can create these resources again more quickly via a JSON tem‐
plate.
9. Once you’ve reviewed and downloaded your template, you can
click OK to move to the purchasing screen. This screen will
notify you of the costs you’re about to incur from this VM
usage. Click Purchase and your NGINX Plus box will begin to
boot.
110 | Chapter 10: Cloud Deployments
Discussion
Azure and NGINX have made it easy to create an NGINX Plus VM
in Azure through just a few configuration forms. The Azure Market‐
place is a great way to get NGINX Plus on demand with a pay-as-
you-go license. With this model, you can try out the features of
NGINX Plus or use it for on-demand overflow capacity of your
already licensed NGINX Plus servers.
10.8 Deploying to Google Compute Engine
Problem
You need to create an NGINX server in Google Compute Engine to
load balance or proxy for the rest of your resources in Google Com‐
pute or App Engine.
Solution
Start a new VM in Google Compute Engine. Select a name for your
VM, zone, machine type, and boot disk. Configure identity and
access management, firewall, and any advanced configuration you’d
like. Create the VM.
Once the VM has been created, log in via SSH or through the Goo‐
gle Cloud Shell. Install NGINX or NGINX Plus through the package
manager for the given OS type. Configure NGINX as you see fit and
reload.