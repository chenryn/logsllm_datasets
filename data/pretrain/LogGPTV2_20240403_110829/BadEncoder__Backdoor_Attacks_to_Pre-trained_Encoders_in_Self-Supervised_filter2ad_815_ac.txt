4) Evaluation metrics: We use Clean Accuracy (CA),
Backdoored Accuracy (BA), and Attack Success Rate (ASR)
to evaluate our BadEncoder. CA and BA measure the clas-
sification accuracies of a clean downstream classifier and a
backdoored downstream classifier for the clean downstream
testing dataset, respectively. ASR measures the fraction of
trigger-embedded testing inputs that are predicted as the target
class by a backdoored downstream classifier. Formally, we
define them as follows:
• Clean Accuracy (CA): The CA of a clean downstream
classifier is its classification accuracy for the clean testing
images of the corresponding downstream dataset.
• Backdoored Accuracy (BA): The BA of a backdoored
downstream classifier is its classification accuracy for the
clean testing images of the corresponding downstream
dataset. When the BA of a backdoored downstream
classifier is similar to the CA of the corresponding clean
downstream classifier, our attack preserves accuracy for
the corresponding downstream task.
• Attack Success Rate (ASR): Given a target downstream
task and target class, we embed the attacker-chosen
trigger to the testing images of the corresponding down-
stream dataset. The ASR of a backdoored downstream
classifier is the fraction of such trigger-embedded testing
images that are predicted as the target class by the
backdoored downstream classifier. As a baseline, we also
consider Attack Success Rate-Baseline (ASR-B), which is
the fraction of such trigger-embedded testing images that
are predicted as the target class by the corresponding
clean downstream classifier. ASR-B represents the attack
success rate when the attacker does not inject backdoor
to the pre-trained image encoder.
5) Parameter setting: By default, we consider the attacker
selects a single target downstream task/dataset and a single
target class. We will also evaluate our BadEncoder when the
attacker selects multiple target downstream tasks and/or target
classes. Moreover, we select “airplane”, “truck”, “priority
sign”, and “digit one” as the target class for the four datasets
CIFAR10, STL10, GTSRB, and SVHN, respectively. Unless
otherwise mentioned, we adopt CIFAR10 as the default pre-
training dataset and STL10 as the default target downstream
dataset. Note that we resize each image in STL10 dataset to
32 × 32 × 3 to be consistent with other datasets.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:37:52 UTC from IEEE Xplore.  Restrictions apply. 
2048
TABLE I: BadEncoder achieves high ASRs.
Pre-training
ASR-B (%)
ASR (%)
Dataset
Target Downs-
tream Dataset
CIFAR10
STL10
GTSRB
SVHN
STL10
GTSRB
SVHN
CIFAR10
2.79
37.53
10.38
1.67
46.11
12.30
98.64
99.14
99.73
95.04
97.64
98.51
Our BadEncoder has the following parameters: λ1, λ2,
shadow dataset, trigger, and reference inputs. Unless otherwise
mentioned, we use the following default parameter settings:
λ1 = 1 and λ2 = 1; shadow dataset
includes 50, 000
images sampled from the pre-training dataset; and following
previous work on backdoor attacks to classifiers [13], we use
a 10 × 10 white square located at the bottom right corner
of an input image as the trigger. The size (e.g., 10 × 10)
of a trigger refers to its height and width. By default, we
assume one reference input for a (target downstream task,
target class) pair. We collected the reference input for each
target downstream dataset with the default target class from
the Internet. Figure 5 in Appendix shows our default reference
input for each downstream dataset. We consider the collected
reference input is correctly classified as the target class by
the backdoored downstream classifier. An attacker can select
multiple reference inputs from the target class, and we will
show our BadEncoder is effective once at least one reference
input is correctly classified by the backdoored downstream
classifier. In experiments, we use a randomly augmented
version of xij in f′(xij) in Equation 3 in each mini-batch.
Moreover, we freeze the parameters in the batch normalization
layers of an encoder when embedding backdoor into it.
We will explore the impact of each parameter on BadEn-
coder. We adopt cosine similarity to measure the similarity of
two inputs’ feature vectors outputted by an image encoder in
our loss terms. We fine-tune a pre-trained image encoder for
200 epochs using Algorithm 1 with learning rate 0.001 and
batch size 256 to inject the backdoor.
B. Experimental Results
BadEncoder achieves high ASRs: Table I shows the ASR-B
without injecting backdoor to the pre-trained image encoder
and ASR of BadEncoder. The experimental results indicate
that BadEncoder can achieve high attack success rates. For
instance, when the pre-training dataset is CIFAR10 and the
target downstream dataset is STL10, BadEncoder can achieve
99.73% attack success rate. In contrast, the attack success rate
(i.e., ASR-B) is only 10.38% when we do not inject backdoor
to the pre-trained image encoder. We note that the ASR-B is
relatively high when the target downstream dataset is SVHN
because SVHN is unbalanced and the selected target class
happens to be the most popular class.
BadEncoder is successful because our backdoored image
encoder outputs similar feature vectors for the reference input
and the trigger-embedded inputs, which makes the backdoored
downstream classifier predict the same class (i.e., target class)
TABLE II: Our BadEncoder maintains the accuracy of
the downstream classifiers.
Target Downs-
tream Dataset
CA (%) BA (%)
Downstream
Pre-training
Dataset
GTSRB
CIFAR10
SVHN
STL10
GTSRB
STL10
SVHN
CIFAR10
Dataset
GTSRB
SVHN
STL10
GTSRB
SVHN
STL10
GTSRB
SVHN
STL10
GTSRB
SVHN
CIFAR10
GTSRB
SVHN
CIFAR10
GTSRB
SVHN
CIFAR10
81.84
58.50
76.14
81.84
58.50
76.14
81.84
58.50
76.14
76.12
55.35
86.77
76.12
55.35
86.77
76.12
55.35
86.77
82.27
68.93
75.94
82.19
69.32
75.66
82.55
68.68
76.18
76.63
63.85
86.63
75.45
65.59
86.23
76.47
64.37
86.55
for them. Given a clean (or backdoored) image encoder, we
use it to produce a feature vector for the reference input
and each trigger-embedded testing input in the target down-
stream dataset, and we calculate the cosine similarity between
the feature vector of the reference input and that of each
trigger-embedded testing input. Figure 4 in Appendix shows
the cumulative distribution functions (CDFs) of such cosine
similarity scores for the clean image encoder and backdoored
image encoder. Our results show that our backdoored image
encoder produces much more similar feature vectors for the
reference input and the trigger-embedded inputs than the clean
image encoder, which is the reason why our ASR is much
higher than ASR-B.
BadEncoder preserves accuracy of the downstream classi-
fiers: Table II compares the clean accuracy and the back-
doored accuracy in different scenarios. In particular, given
a pre-training dataset and a target downstream dataset, we
evaluate the clean accuracy of the downstream classifiers and
backdoored accuracy of the backdoored downstream classi-
fiers for both the target downstream dataset and non-target
downstream datasets. Our experimental results indicate that
our BadEncoder preserves the accuracy of the downstream
classifiers for the target/non-target downstream datasets. In
particular, the differences between the backdoor accuracies and
the clean accuracies are within 1% in most cases. We note that
the backdoor accuracies are higher than the clean accuracies
when the downstream dataset is SVHN, which we consistently
observe in all our experiments. We suspect the reason is that
the SVHN dataset is noisy, i.e., many images in the SVHN
dataset contain distracting digits, and our backdoored image
encoder produces better features for the SVHN dataset after
fine-tuning the clean image encoder.
Impact of loss terms: Our BadEncoder leverages three loss
terms, i.e., L0, L1, and L2 in Equation 5. Moreover, we use λ1
(or λ2) to weight L1 (or L2). Therefore, we explore the impact
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:37:52 UTC from IEEE Xplore.  Restrictions apply. 
2049
(a) λ1
(b) λ2
Fig. 2: The impacts of λ1 and λ2.
TABLE III: The impact of the loss terms.
Removed
Loss Terms
L0
L1
L2
None
CA(%)
BA(%)
ASR(%)
76.14
76.48
75.85
50.08
76.18
9.48
59.15
9.09
99.73
(a)
(b)
Fig. 3: (a) The impact of the shadow dataset size on
BadEncoder when the target downstream dataset is STL10
and the shadow dataset is a subset of the pre-training
dataset. (b) The impact of the trigger size on BadEncoder
when the target downstream dataset is GTSRB.
TABLE IV: The impact of the shadow dataset’s distribu-
tion on BadEncoder.
Target Downs-
tream Dataset
CA (%) BA (%) ASR (%)
Shadow Dataset
of the three loss terms and the parameters λ1 and λ2 on our
BadEncoder. Table III shows our experimental results when
we exclude a loss term in our BadEncoder, where λ1 = 1
(or λ2 = 1) if L1 (or L2) is included. Our results show that
all the three loss terms are necessary for our BadEncoder to
achieve high attack success rates. Moreover, L2 is necessary
to achieve a high backdoored accuracy, i.e., to preserve the
accuracy of the downstream classifier. This is because L2 is
designed to achieve the utility goal. Note that although L2
is designed for the utility goal, excluding it also substantially
reduces the attack success rate. This is because, without L2,
the backdoored downstream classifier becomes less accurate,
in particular it misclassifies the reference input.
We also study the impact of λ1 and λ2 on the backdoored
accuracy and attack success rate of our BadEncoder. Figure 2
shows the results. First, we observe that BadEncoder achieves
high attack success rates and preserve accuracy after λ1 and
λ2 are larger than some thresholds. Second, our BadEncoder is
less sensitive to λ1. In particular, the attack success rate starts
to decrease after λ2 is larger than around 1, while the attack
success rate keeps stable even if λ1 is as large as 10. This is
because our shadow dataset size (related to L2) is much larger
than the number of reference inputs (related to L1).
Impact of shadow dataset: A shadow dataset can be char-
acterized by its size and distribution. Therefore, we study the
impact of both the size and distribution of the shadow dataset
on our BadEncoder. Figure 3a and Figure 7 in Appendix
show the impact of the shadow dataset size on BadEncoder.
We find that BadEncoder achieves high attack success rates
and preserves accuracy of the downstream classifiers once the
shadow dataset size is larger than around 20% of the pre-
training dataset. We note that the backdoored accuracy for
the SVHN dataset increases as the size of the shadow dataset
increases. We suspect the reason is that SVHN is noisy and
our backdoored image encoder produces more distinguishable
features for it after fine-tuning the clean image encoder.
GTSRB
SVHN
STL10
A subset of pre-training dataset
Same distribution
Different distributions
A subset of pre-training dataset
Same distribution
Different distributions
A subset of pre-training dataset
Same distribution
Different distributions
81.84
58.50
76.14
81.21
81.12
82.21
62.32
62.07
60.40
75.90
75.70
75.99
98.19
97.52
93.27
98.30
98.06
84.80
99.55
99.43
98.15
We also study the impact of the shadow dataset distribution
on our BadEncoder. In particular, we consider three cases.
In the first case, the shadow dataset is a subset of the pre-
training dataset. In particular, we randomly sample 10, 000
images from the pre-training dataset CIFAR10 as the shadow
dataset. In the second case, the shadow dataset has the same
distribution as the pre-training dataset but does not overlap
with it. In particular, we use the testing 10, 000 images of
CIFAR10, which were not used to pre-train the image encoder,
as the shadow dataset. In the third case, the shadow dataset has
a different distribution with the pre-training dataset. Specifi-
cally, we randomly sample 10,000 images from the Food101
dataset [26] which contains images of food and has a different
distribution with CIFAR10. Table IV shows our experimental
results. Our results show that our BadEncoder achieves high
attack success rates and preserves accuracy of the downstream
classifiers in all the three cases, though the attack success rates
are lower in the third case when the shadow dataset has a
different distribution with the pre-training dataset. Our results
indicate that our shadow dataset does not need to be from the
pre-training dataset nor follow its distribution.
Impact of trigger size: Figure 3b, Figure 13 (in Appendix),
and Figure 8 (in Appendix) show the impact of the trigger
size on BadEncoder. Note that our trigger is a white square
located at the bottom right corner of an input and trigger
size refers to the height/width of a trigger. We have the
following observations from our experimental results. First,
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:37:52 UTC from IEEE Xplore.  Restrictions apply. 
2050
00.31310λ100.20.40.60.81.0CABAASR00.31310λ200.20.40.60.81.0CABAASR020406080100ShadowDatasetSize(%)00.20.40.60.81.0CABAASR3x35x510x10TriggerSize00.20.40.60.81.0CABAASRTABLE V: Results of using multiple reference inputs.
Reference Input
CA (%)
Truck 0
Truck 1
Truck 2
All
76.14