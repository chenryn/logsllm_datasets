robin fashion (represented by MC). Results are shown in Figure 16.
For the single core, NFVnice achieves higher throughput than the
Default scheduler for longer chains, with the greater improvements
achieved for chain lengths of 3-6. As the chains get longer (>7 NFs
sharing the same core), the improvement with NFVnice is not as
high. For the multiple core case, NFVnice improves throughput
substantially, especially as more NFs are multiplexed on a care (e.g.,
chain lengths > 4), compared to the Default scheduler. Of course,
the improvement with NFVnice will depend on the type of NFs and
their computation costs, for individual use-cases.
4.3.8 Tuning and Overhead Considerations. Tuning NFVnice: To
tune the key parameters of NFVnice, viz., the HIGH_WATER_
MARK and LOW_WATER_MARK, the thresholds for the queue
occupancy in the Rx ring, we measure the throughput, wasted work,
context-switch overheads and achieved Instructions per Cycle (IPC)
NFVnice: Dynamic Backpressure and Scheduling for NF Chains
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
processing costs. Furthermore, different kinds of flow arrival rates
exacerbate the difficulty of fair scheduling.
PSPAT [45] is a recent host-only software packet scheduler.
PSPAT aims to provide a scalable scheduler framework by decou-
pling the packet scheduler algorithm from dispatching packets
to the NIC for high performance. NFVnice considers the orthog-
onal problem of packet processing cost and flow arrival rate to
fairly allocate CPU resources across the NFs. PIFO [48] presents the
packet-in-first-out philosophy distinct from the typical first-in-first-
out packet processing models. We use the insight from this work
to decide whether to accept a packet and queue it for processing
at the intended NF or discard at the time of packet arrival. Then,
the enqueued packets are always processed in order. This approach
of selective early discard yields two benefits: i) it avoids dropping
partially processed (through the chain) packets, thus not wasting
CPU cycles; ii) it avoid CPU stealing and allows CPU cycles to be
judiciously allocated to other contending NFs.
User space scheduling and related frameworks: Works, such as [2, 6],
consider cooperative user-space scheduling, providing very low
cost context switching, that is orders of magnitude faster than reg-
ular Pthreads. However, the drawbacks with such a framework are
two-fold: a) they invariably require the threads to cooperate, i.e.,
each thread must voluntarily yield to ensure that the other threads
get a chance to share the CPU, without which progress of the
threads cannot be guaranteed. This means that the programs that
implement L-threads must include frequent rescheduling points for
each L-thread [2] incurring additional complexity in developing
the NFs. b) As there is no specific scheduling policy (it is just FIFO
based), all the L-threads share the same priority, and are backed
by the same kernel thread (typically pinned to a single core), and
thus lack the ability to perform selective prioritization and the
ability to provide QoS differentiation across cooperating threads.
Nonetheless, NFVnice’s backpressure mechanism can still be effec-
tively employed for such cooperating threads to voluntarily yield
the CPU as necessary. Another approach used by systems such as
E2 [39] and VPP [4] is to host multiple NFs within a shared address
space, allowing them to be executed as function calls in a run to
completion manner by one thread. This incurs very low NUMA
and cross-core packet chaining overheads, but being monolithic, it
is inflexible and impedes the deployment of NFs from third party
vendors.
Congestion Control and Backpressure: Congestion control and back-
pressure have been extensively studied in the past [7, 8, 22, 26, 29,
35]. DCTCP [7] leverages ECN to provide multi-bit feedback to
the end hosts. MQ-ECN [8] enables ECN for tradeoff of both high
throughput and low latency in multi-service multi-queue produc-
tion DCNs (Data Center Network). All of these focus on congestion
control in DCNs. However, in an NFV environment, flows are typi-
cally steered through a service chain. The later congestion is found,
the more resources are wasted. If the end hosts do not enable ECN
support or there are UDP flows, it is especially important for the
NFV platform to gracefully handle high load scenarios in an efficient
and fair way. Using multiple mechanisms (ECN and backpressure),
NFVnice ensures that overload at bottlenecks are quickly detected
in order to avoid congestion and wasted work. Fair Queueing: Or-
thogonal work such as [17, 31], propose to ensure fair sharing of
network resources among multiple tenants by spreading requests
to multiple processing entities. That is, they distribute flows with
different costs to different processing threads. In contrast, NFVnice
seeks to achieve fairness by scheduling the NFs that process the
packets of different flows appropriately, Thus, a fair share of the
CPU is allocated to each competing NF.
6 CONCLUSION
As the use of highly efficient user-space network I/O frameworks
such as DPDK becomes more prevalent, there is be a growing need
to mediate application-level performance requirements across the
user-kernel boundary. OS-based schedulers lack the information
needed to provide higher level goals for packet processing, such as
rate proportional fairness that needs to account for both NF pro-
cessing cost and arrival rate. By carefully tuning scheduler weights
and applying backpressure to efficiently shed load early in the the
NFV service chain, NFVnice provides substantial improvements in
throughput and drop rate and dramatically reduces wasted work.
This allows the NFV platform to gracefully handle overload scenar-
ios while maintaining efficiency and fairness.
Our implementation of NFVnice demonstrates how an NFV
framework can efficiently tune the OS scheduler and harmoniously
integrate backpressure to meet its performance goals. Our results
show that selective backpressure leads to more efficient alloca-
tion of resources for NF service chains within or across cores, and
scheduler weights can be used to provide rate proportional fairness,
regardless of the scheduler being used.
7 ACKNOWLEDGEMENT
This work was supported by EU FP7 Marie Curie Actions CleanSky
ITN project Grant No. 607584, and US NSF grants CNS-1522546
and CNS-1422362. The work was also partially supported by the
Department of the Army, US Army Research, Development and
Engineering Command grant W911NF-15-1-0508 and Huawei Tech-
nologies Co. Ltd.’s HIRP Grant. We also thank our shepherd, Justine
Sherry, and the anonymous reviewers for their thoughtful feedback;
Victor G. Hill for helping with the setup of our evaluation testbed;
Dr. Jiachen Chen and Shriya Kulkarni for their help and ideas on
the representation of the graphs and plots.
REFERENCES
[1] 2014. Data plane development kit. http://dpdk.org/. (2014). [online].
[2] 2014. DPDK L-Thread subsystem. http://dpdk.org/doc/guides/sample_app_ug/
performance_thread.html. (2014). [online].
[3] 2016. Performance measurements with RDTSC. https://www.strchr.com/
performance_measurements_with_rdtsc. (June 2016). [online].
[4] 2016. VPP. https://fd.io/. (2016). [online].
[5] 2017. cgroups-Linux control groups. http://man7.org/linux/man-pages/man7/
[online].
cgroups.7.html. (2017). [online].
[6] 2017. Fibers. https://msdn.microsoft.com/en-us/library/ms682661.aspx. (2017).
[7] Mohammad Alizadeh, Albert Greenberg, David A Maltz, Jitendra Padhye, Parveen
Patel, Balaji Prabhakar, Sudipta Sengupta, and Murari Sridharan. 2010. Data
center tcp (dctcp). In ACM SIGCOMM computer communication review, Vol. 40.
ACM, 63–74.
[8] Wei Bai, Li Chen, Kai Chen, and Haitao Wu. 2016. Enabling ECN in Multi-Service
Multi-Queue Data Centers. In 13th USENIX Symposium on Networked Systems
Design and Implementation (NSDI 16). USENIX Association, Santa Clara, CA,
537–549.
[9] Rudolf Bayer. 1972. Symmetric binary B-trees: Data structure and maintenance
algorithms. Acta informatica 1, 4 (1972), 290–306.
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
S. Kulkarni et al.
[10] Alan Demers, Srinivasan Keshav, and Scott Shenker. 1989. Analysis and simula-
tion of a fair queueing algorithm. In ACM SIGCOMM Computer Communication
Review, Vol. 19. ACM, 1–12.
[11] Jon Dugan, Seth Elliott, Bruce A. Mah, Jeff Poskanzer, and Kaustubh Prabhu. 2014.
iPerf - The ultimate speed test tool for TCP, UDP and SCTP. (2014).
[12] Paul Emmerich, Sebastian Gallenmüller, Daniel Raumer, Florian Wohlfart, and
Georg Carle. 2015. MoonGen: a scriptable high-speed packet generator. In
Proceedings of the 2015 ACM Conference on Internet Measurement Conference.
ACM, 275–287.
[13] ETSI-GS-NFV-002. 2013. Network Functions Virtualization (NFV): Architectural
Framework. http://www.etsi.org/deliver/etsi_gs/nfv/001_099/002/01.01.01_60/
gs_nfv002v010101p.pdf. (2013). [online].
[14] Wu-chang Feng, Dilip Kandlur, Debanjan Saha, and Kang Shin. 1999. BLUE: A
new class of active queue management algorithms. Ann Arbor 1001 (1999), 48105.
[15] Sally Floyd and Van Jacobson. 1993. Random Early Detection Gateways for
Congestion Avoidance. IEEE/ACM Trans. Netw. 1, 4 (Aug. 1993), 397–413.
[16] Aaron Gember, Anand Krishnamurthy, Saul St John, Robert Grandl, Xiaoyang
Gao, Ashok Anand, Theophilus Benson, Aditya Akella, and Vyas Sekar. 2013.
Stratos: A network-aware orchestration layer for middleboxes in the cloud. Technical
Report.
[17] Ali Ghodsi, Vyas Sekar, Matei Zaharia, and Ion Stoica. 2012. Multi-resource Fair
Queueing for Packet Processing. SIGCOMM Comput. Commun. Rev. 42, 4 (Aug.
2012), 1–12. https://doi.org/10.1145/2377677.2377679
[18] Pawan Goyal, Harrick M Vin, and Haichen Chen. 1996. Start-time fair queueing:
A scheduling algorithm for integrated services packet switching networks. In
ACM SIGCOMM Computer Communication Review, Vol. 26. ACM, 157–168.
[19] Leo J Guibas and Robert Sedgewick. 1978. A dichromatic framework for balanced
trees. In Foundations of Computer Science, 1978., 19th Annual Symposium on. IEEE,
8–21.
[20] J Halpern and C Pignataro. 2015. RFC 7665: Service Function Chaining (SFC)
Architecture. https://tools.ietf.org/html/rfc7665. (2015). [online].
[21] Sangjin Han, Keon Jang, Aurojit Panda, Shoumik Palkar, Dongsu Han, and Sylvia
Ratnasamy. 2015. SoftNIC: A Software NIC to Augment Hardware. Technical Report
UCB/EECS-2015-155. EECS Department, University of California, Berkeley. http:
//www.eecs.berkeley.edu/Pubs/TechRpts/2015/EECS-2015-155.html
[22] Keqiang He, Eric Rozner, Kanak Agarwal, Yu Jason Gu, Wes Felter, John Carter,
and Aditya Akella. 2016. AC/DC TCP: Virtual congestion control enforcement
for datacenter networks. In Proceedings of the 2016 conference on ACM SIGCOMM
2016 Conference. ACM, 244–257.
[23] J. Hwang, K. K. Ramakrishnan, and T. Wood. 2015. NetVM: High Performance
and Flexible Networking Using Virtualization on Commodity Platforms. IEEE
Transactions on Network and Service Management 12, 1 (March 2015), 34–47.
https://doi.org/10.1109/TNSM.2015.2401568
[24] Raj Jain, Dah-Ming Chiu, and William R Hawe. 1984. A Quantitative Measure of
Fairness and Discrimination for Resource Allocation in Shared Computer System.
Vol. 38. Eastern Research Laboratory, Digital Equipment Corporation Hudson,
MA.
[25] Dilip A Joseph, Arsalan Tavakoli, and Ion Stoica. 2008. A policy-aware switching
layer for data centers. In ACM SIGCOMM Computer Communication Review,
Vol. 38. ACM, 51–62.
[26] Glenn Judd. 2015. Attaining the Promise and Avoiding the Pitfalls of TCP in the
[27] Tom Kelly, Sally Floyd, and Scott Shenker. 2003. Patterns of congestion collapse.
Datacenter.. In NSDI. 145–157.
International Computer Science Institute, and University of Cambridge (2003).
[28] D. Lapsley and S. Low. 1999. Random early marking: an optimisation approach
to Internet congestion control. In Networks, 1999. (ICON ’99) Proceedings. IEEE
International Conference on. 67–74. https://doi.org/10.1109/ICON.1999.796161
[29] Changhyun Lee, Chunjong Park, Keon Jang, Sue B Moon, and Dongsu Han.
2015. Accurate Latency-based Congestion Feedback for Datacenters.. In USENIX
Annual Technical Conference. 403–415.
[30] Yang Li, Linh Thi Xuan Phan, and Boon Thau Loo. 2016. Network functions
virtualization with soft real-time guarantees. In Computer Communications, IEEE
INFOCOM 2016-The 35th Annual IEEE International Conference on. IEEE, 1–9.
[31] Jonathan Mace, Peter Bodik, Madanlal Musuvathi, Rodrigo Fonseca, and Krishnan
Varadarajan. 2016. 2DFQ: Two-Dimensional Fair Queuing for Multi-Tenant Cloud
Services. In Proceedings of the 2016 ACM SIGCOMM Conference (SIGCOMM ’16).
ACM, New York, NY, USA, 144–159. https://doi.org/10.1145/2934872.2934878
[32] Joao Martins, Mohamed Ahmed, Costin Raiciu, Vladimir Olteanu, Michio Honda,
Roberto Bifulco, and Felipe Huici. 2014. ClickOS and the Art of Network Function
Virtualization. In 11th USENIX Symposium on Networked Systems Design and
Implementation (NSDI 14). USENIX Association, Seattle, WA, 459–473. https:
//www.usenix.org/conference/nsdi14/technical-sessions/presentation/martins
[33] Paul Menage. 2017. Linux Kernel Documentation: CGROUPS. https://www.
kernel.org/doc/Documentation/cgroup-v1/cgroups.txt. (2017). [online].
[34] Dirk Merkel. 2014. Docker: Lightweight Linux Containers for Consistent Devel-
opment and Deployment. Linux J. 2014, 239, Article 2 (March 2014).
[35] Radhika Mittal, Nandita Dukkipati, Emily Blem, Hassan Wassel, Monia Ghobadi,
Amin Vahdat, Yaogong Wang, David Wetherall, David Zats, et al. 2015. TIMELY:
RTT-based Congestion Control for the Datacenter. In ACM SIGCOMM Computer
Communication Review, Vol. 45. ACM, 537–550.
[36] Jeffrey C Mogul and KK Ramakrishnan. 1997. Eliminating receive livelock in
an interrupt-driven kernel. ACM Transactions on Computer Systems 15, 3 (1997),
217–252.
[37] Ingo Molnar. 2017. Linux Kernel Documentation: CFS Scheduler Design. https:
//www.kernel.org/doc/Documentation/scheduler/sched-design-CFS.txt. (2017).
[38] Robert Olsson. 2005. Pktgen the linux packet generator. In Proceedings of the
Linux Symposium, Ottawa, Canada, Vol. 2. 11–24.
[39] Shoumik Palkar, Chang Lan, Sangjin Han, Keon Jang, Aurojit Panda, Sylvia
Ratnasamy, Luigi Rizzo, and Scott Shenker. 2015. E2: A Framework for NFV
Applications. In Proceedings of the 25th Symposium on Operating Systems Principles
(SOSP ’15). ACM, New York, NY, USA, 121–136. https://doi.org/10.1145/2815400.
2815423
[40] Abhay K Parekh and Robert G Gallagher. 1994. A Generalized Processor Sharing
Approach to Flow Control in Integrated Services Networks: The Multiple Node
Case. IEEE/ACM Transactions on Networking (ToN) 2, 2 (1994), 137–150.
[41] Shriram Rajagopalan, Dan Williams, Hani Jamjoom, and Andrew Warfield.
Split/Merge: System Support for Elastic Execution in Virtual Middleboxes. In
Presented as part of the 10th USENIX Symposium on Networked Systems Design
and Implementation (NSDI 13). USENIX, 227–240.
[42] K.K. Ramakrishnan, S. Floyd, and D. Black. 2001. RFC 3168: The Addition of
Explicit Congestion Notification (ECN) to IP. https://tools.ietf.org/html/rfc3168.
(2001). [online].
[43] Luigi Rizzo. 2012.
netmap: A Novel Framework for Fast Packet I/O.
In USENIX Annual Technical Conference. USENIX, Berkeley, CA, 101–
112.
https://www.usenix.org/conference/usenixfederatedconferencesweek/
netmap-novel-framework-fast-packet-io
[44] Luigi Rizzo, Stefano Garzarella, Giuseppe Lettieri, and Vincenzo Maffione.
2016. A Study of Speed Mismatches Between Communicating Virtual Ma-
chines. In Proceedings of the 2016 Symposium on Architectures for Networking
and Communications Systems (ANCS ’16). ACM, New York, NY, USA, 61–67.
https://doi.org/10.1145/2881025.2881037
[45] Luigi Rizzo, Paolo Valente, Giuseppe Lettieri, and Vincenzo Maffione. 2016. PSPAT:
software packet scheduling at hardware speed. http://info.iet.unipi.it/~luigi/
papers/20160921-pspat.pdf. (2016). [online].
[46] Vyas Sekar, Norbert Egi, Sylvia Ratnasamy, Michael K. Reiter, and Guangyu Shi.
2012. Design and Implementation of a Consolidated Middlebox Architecture.
In Proceedings of the 9th USENIX Conference on Networked Systems Design and
Implementation (NSDI’12). USENIX Association, Berkeley, CA, USA, 24–24.
[47] Madhavapeddi Shreedhar and George Varghese. 1996. Efficient fair queuing using
deficit round-robin. IEEE/ACM Transactions on networking 4, 3 (1996), 375–385.
[48] Anirudh Sivaraman, Suvinay Subramanian, Mohammad Alizadeh, Sharad Chole,
Shang-Tse Chuang, Anurag Agrawal, Hari Balakrishnan, Tom Edsall, Sachin
Katti, and Nick McKeown. 2016. Programmable Packet Scheduling at Line Rate.
In Proceedings of the 2016 conference on ACM SIGCOMM 2016 Conference. ACM,
44–57.
[49] Dimitrios Stiliadis and Anujan Varma. 1998. Efficient fair queueing algorithms
for packet-switched networks. IEEE/ACM Transactions on Networking (ToN) 6, 2
(1998), 175–185.
[50] Dimitrios Stiliadis and Anujan Varma. 1998. Rate-proportional servers: a design
methodology for fair queueing algorithms. IEEE/ACM Transactions on networking
6, 2 (1998), 164–174.
[51] Ion Stoica, Scott Shenker, and Hui Zhang. 2003. Core-stateless Fair Queueing: A
Scalable Architecture to Approximate Fair Bandwidth Allocations in High-speed
Networks. IEEE/ACM Trans. Netw. 11, 1 (Feb. 2003), 33–46. https://doi.org/10.
1109/TNET.2002.808414
[52] Rahul Upadhyaya, CB Anantha Padmanabhan, Meenakshi Sundaram Lak-
Optimising NFV Service Chains
https://www.openstack.org/videos/video/
shmanan, and Satya Routray. 2016.
on OpenStack Using Docker.
optimising-nfv-service-chains-on-openstack-using-docker. (April 2016).
[53] Lixia Zhang. 1991. VirtualClock: a new traffic control algorithm for packet-
switched networks. ACM Transactions on Computer Systems (TOCS) 9, 2 (1991),
101–124.
[54] Wei Zhang, Guyue Liu, Wenhui Zhang, Neel Shah, Phillip Lopreiato, Gregoire
Todeschi, K.K. Ramakrishnan, and Timothy Wood. 2016. OpenNetVM: A Plat-
form for High Performance Network Service Chains. In Proceedings of the 2016
Workshop on Hot Topics in Middleboxes and Network Function Virtualization (Hot-
MIddlebox ’16). ACM, New York, NY, USA, 26–31. https://doi.org/2940147.2940155