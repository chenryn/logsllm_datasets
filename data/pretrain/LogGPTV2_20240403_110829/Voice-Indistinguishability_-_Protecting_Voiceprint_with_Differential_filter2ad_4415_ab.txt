high dimensional data.
2.3 Voiceprint Anonymization Mechanism
Based on the anonymous x-vector dataset, we design an anonymiza-
tion mechanism that satisfies Voice-Indistinguishability and enables
speaker recognition. Given an input x-vector 𝑥0 ∈ X, the mech-
anism 𝐾 perturbs 𝑥0 by randomly selecting an x-vector ˜𝑥 in the
dataset X according to calibrated probability distributions, thus pro-
viding plausible deniability for 𝑥0. The perturbed x-vector serves
as an anonymized voiceprint in our system.
Theorem 1 [4]. A mechanism 𝐾 that randomly transforms 𝑥0
to ˜𝑥 where 𝑥0, ˜𝑥 ∈ X according to the following equation, satisfies
Voice-Indistinguishability.
Pr( ˜𝑥|𝑥0) ∝ 𝑒−𝜖𝑑X (𝑥0, ˜𝑥)
In practice, the service providers often need to authenticate the
speaker before providing the service; thus, we need to make sure the
anonymized voiceprint can be recognized as the same speaker. To
map a speaker’s original x-vector to the same anonymous x-vector,
our system memorizes such a mapping in a local “look-up table”.
Once we receive the raw utterance from a speaker, we firstly find
whether it is in our “look-up table”. If so, we use the anonymized
x-vector that already in the look-up table without new perturbation;
otherwise, we extract an x-vector from the utterance and randomly
perturb it to an anonymous x-vector based on Theorem 1. We
also need to remove the “used” x-vectors from the anonymous x-
vectors set to make sure this anonymous x-vector will not be used
by a different speaker who may use the same device. However, a
potential privacy problem is that, the privacy guarantee diminishes
along with the amount of the users (of the same device) because the
available anonymized x-vectors will decrease. In [4], we show that
a smaller size of X leads to weaker privacy. To solve this problem,
we add a user’s raw x-vector to the anonymous x-vector X set after
“issuing” an anonymized x-vector for her. In this way, we always
have the same size of X.
2.4 System Architecture
After obtaining the anonymized x-vector, we synthesize the per-
turbed x-vector and other features in the original utterance and
output a privacy-preserving utterance to the untrusted server. The
privacy-preserving speech synthesis framework uses two modules
#
Data
original
WER(%)
LM𝑙
1
10.79
2
15.38
3
25.56
4 Voice-Ind
19.05
Table 3: ASR results.
LM𝑠
14.04
18.92
30.10
20.35
[3]
[8]
and observe how it could affect privacy (e.g., ASV) and utility (e.g.,
ASR). Our source code is available in Github 1.
4 ACKNOWLEDGEMENT
This work is partially supported by JSPS KAKENHI Grant No.
17H06099, 18H04093, 19K20269, 19K24376, NICT tenure-track startup
fund, and ROIS NII Open Collaborative Research 2020 (20FC06).
REFERENCES
[1] Andrew Boles and Paul Rad. 2017. Voice biometrics: Deep learning-based
voiceprint authentication system. In 2017 12th System of Systems Engineering
Conference (SoSE). IEEE, 1–6.
[2] Statista Research Department. 2019. Number of digital voice assistants in use
https://www.statista.com/statistics/973815/
worldwide from 2019 to 2023.
worldwide-digital-voiceassistant-in-use/.
[3] F. Fang and et al. 2019. Speaker anonymization using X-vector and neural
waveform models. In Proc. 10th ISCA Speech Synthesis Workshop. 155–160. https:
//doi.org/10.21437/SSW.2019-28
[4] Yaowei Han, Sheng Li, Yang Cao, Qiang Ma, and Masatoshi Yoshikawa. 2020.
Voice-Indistinguishability: Protecting Voiceprint In Privacy-Preserving Speech
Data Release. In 2020 IEEE International Conference on Multimedia and Expo
(ICME). IEEE, 1–6.
[5] Tencent Inc. 2015. The New WeChat Password.
https://blog.wechat.com/
[6] Anil Jain, Lin Hong, and Sharath Pankanti. 2000. Biometric identification. Com-
tag/voiceprint/ (2015).
mun. ACM 43, 2 (2000), 90–98.
[7] H. Kawahara and et al. 1999. Re-structuring speech representations using a pitch-
adaptive time-frequency smoothing and an instantaneous-frequency-based F0
extraction: Possible role of a repetitive structure in sounds. Speech communication
27, 3–4 (1999), 187–207.
[8] Stephen Edward McAdams. 1985. Spectral fusion, spectral parsing and the
formation of auditory images. (1985).
[9] Andreas Nautsch, Catherine Jasserand, Els Kindt, Massimiliano Todisco, Isabel
Trancoso, and Nicholas Evans. 2019. The GDPR & speech data: Reflections of
legal and technology communities, first steps towards a common understanding.
arXiv preprint arXiv:1907.03458 (2019).
[10] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015.
Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE,
5206–5210.
[11] David Snyder, Daniel Garcia-Romero, Gregory Sell, Daniel Povey, and Sanjeev
Khudanpur. 2018. X-vectors: Robust dnn embeddings for speaker recognition.
In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP). IEEE, 5329–5333.
[12] Supasorn Suwajanakorn, Steven M Seitz, and Ira Kemelmacher-Shlizerman. 2017.
Synthesizing obama: learning lip sync from audio. ACM Transactions on Graphics
(TOG) 36, 4 (2017), 1–13.
[13] K. Tokuda and et al. 2013. Speech synthesis based on hidden Markov models.
Proc. IEEE 101, 5 (2013), 1234–1252.
[14] Zhizheng Wu, Nicholas Evans, Tomi Kinnunen, Junichi Yamagishi, Federico Ale-
gre, and Haizhou Li. 2015. Spoofing and countermeasures for speaker verification:
A survey. speech communication 66 (2015), 130–153.
[15] Junichi Yamagishi, Christophe Veaux, Kirsten MacDonald, et al. 2019. CSTR
VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit
(version 0.92). (2019).
[16] Ryuichi Yamamoto, Eunwoo Song, and Jae-Min Kim. 2020. Parallel WaveGAN: A
fast waveform generation model based on generative adversarial networks with
multi-resolution spectrogram. In ICASSP 2020-2020 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 6199–6203.
[17] Chong You, Chun-Guang Li, Daniel P Robinson, and René Vidal. 2016. Ora-
cle based active set algorithm for scalable elastic net subspace clustering. In
Proceedings of the IEEE conference on computer vision and pattern recognition.
3928–3937.
1https://github.com/iris0305/voice_ind
Figure 2: System architecture.
to generate the utterance: (1) an End-to-End acoustic model that
generates a Mel-spectrogram (Mel-spec, used as a standard input
feature by speech synthesis) [7, 13] given the two input features:
content feature and x-vector. (2) a fast waveform generation model
named parallel wavegan [16].
Our system architecture is shown in Figure 2. The local device
transfers raw utterance into a protected one and then sends the
protected utterance to an untrusted service such as a virtual assis-
tant. To further reduce the workload of the user device, our system
embeds perturbation in local devices and outsources the speech
syntheses to an untrusted server.
3 EVALUATION AND DEMONSTRATION
For evaluation, we use the LibriSpeech [10] to construct the x-vector
database with 500 x-vectors and use the VCTK [15] to verify the
system. We employ automatic speaker verification (ASV) to show
the privacy guarantee empirically and automatic speech recognition
(ASR) to confirm the utility of the protected utterance.
3.1 Results
Results for the ASV objective evaluation are provided in Table 2.
# Enroll Trial Gen
1
2
3
4
5
6
f
f
f
m
m
m
o
o
a
o
o
a
o
a
a
o
a
a
EER
2.616
47.380
3.779
1.425
49.290
4.843
C𝑚𝑖𝑛
𝑙𝑙𝑟
0.089
0.966
0.140
0.051
0.991
0.185
C𝑙𝑙𝑟
0.874
159.616
4.534
1.565
160.925
5.409
Table 2: ASV results (o-original, a-anonymized speech).
When the trial utterances are anonymized, the speaker verifia-
bility metrics, Equal Error Rate (EER) and log-likelihood-ratio cost
function (Cllr), are significantly higher than the case when both
the enrollment and trial utterances are original. When both the
enrollment and trial utterances are anonymized, the results show
evident speaker verifiability. It is because our algorithm always
assigns the same anonymized voiceprint to the same speaker.
Table 3 shows ASR evaluation in terms of Word Error Rate (WER).
Our approach with strong privacy guarantee outperforms or comes
close to the performance of exiting methods [3, 8].
3.2 Demonstration Scenario
In the demonstration, we will show how the system processes a
speaker’s speech data with high utility and privacy. The attendees
can interactively use the system with different privacy parameters