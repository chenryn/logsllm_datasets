is 
performed is a distributed log monitoring system consisting 
of Redis11, Logstash, ElasticSearch12, and Kibana13, running 
on the Ubuntu operating system. Each deployed instance of 
the  application  can  be  used  to  aggregate  distributed  logs 
produced by the customer‚Äôs own applications.  
We  have  a  cluster  with  4  or  20  instances  deployed  in 
AWS as an ASG. The ASG-based cluster is associated with 
an  Elastic  Load  Balancer  (ELB)  to  provide  a  point  of 
contact  for  incoming  traffic.  When  the  ASG  finds  an 
unhealthy instance (e.g. caused by termination), it replaces 
the instance with a new one. Asgard does rolling upgrade by 
terminating instances from the ASG, and utilizing the ASG 
to launch new instances with the new version ‚Äì see Figure 2. 
The  instances  within  the  ASG  are  associated  with  one 
Security  Group  (SG).  All  the  instances  use  the  same  Key 
pair  for  connection.  Upgrading  an  instance  could  include 
changing  AMI,  SG,  Key  or  other  configurations,  such  as 
instance  type,  kernel  ID,  or  RAM  disk  ID.  In  our 
experiment, we upgraded 1/4 nodes at a time if the cluster 
size was 4/20, respectively, during rolling upgrade.  
11 Redis ‚Äì http://redis.io/   
12 ElasticSearch ‚Äì http://www.elasticsearch.org/   
13 Kibana ‚Äì http://rashidkpc.github.io/Kibana/   
259259259
Our  experiment  uses  a  combination  of  step-specific 
assertions  and  high-level  overall  assertions  at  different 
stages  of  the  rolling  upgrade.  For  example,  we  ‚Äúassert  the 
system  has  N  instances  with  the  new  version‚Äù  after  each 
completion of the loop, as mentioned in Section III.B.4. In 
parallel to assertion evaluation, each log line is forwarded to 
the conformance checking service. 
the  uncertainty  of  cloud 
To simulate a complex ecosystem, we ran another small 
simultaneous  operation  in  parallel  to  rolling  upgrade  ‚Äì 
ASG‚Äôs scaling-in. We also randomly terminated instances to 
increase 
infrastructure.  Our 
approach did detect such errors, but could not diagnose the 
root causes without information like which AWS API calls 
happened ‚Äì see the discussion on CloudTrail in Section VII. 
For  example,  in  our  experiment  we  were  able  to  diagnose 
when  the  root  cause  was  ASG  scale-in,  but  not  when  the 
root cause was termination of instances. 
C.  Fault Injection 
We  ran  our  experiment  by  injecting  one  fault  into  the 
process  at  a  random  point  of  time  during  rolling  upgrade. 
POD-Diagnosis can tackle three types of faults at this stage: 
(i)  faults  causing  mixed  versions  due  to  unexpected 
simultaneous  upgrades,  (ii)  resource  faults,  and  (iii) 
configuration 
following  8 
injected 
representative faults in our experiment: 
faults.  We 
the 
1.  AMI changed during upgrade  
2.  Key pair management fault 
3.  Security group configuration fault 
4.  Instance type changed during upgrade  
5.  AMI is unavailable during upgrade 
6.  Key pair is unavailable during upgrade 
7.  Security group is unavailable during upgrade 
8.  ELB is unavailable during upgrade 
One  of  the  most  challenging  faults  is  the  ASG  mixed 
version  error,  which  can  be  caused  by  two  simultaneous 
rolling  upgrades.  In  a  large-scale  deployment,  this  can 
happen quite easily if different development teams push out 
changes  independently  during  a  relatively  long  upgrade 
process. This can result in mixed versions of the system co-
existing.  Some  faults,  such  as  AMI  changes,  essentially 
simulate  the  continuous  deployment  scenario  where  largely 
independent  teams  push  small  updates  to  the  production 
environment  through  rolling  upgrade.  There  might  be 
conflicts or race conditions during these rolling upgrades.  
Some faults, such as key pair management, correspond to 
classical  operator  problems  around  ssh-based  command 
orchestration  and  security.  Other  faults,  such  as  security 
group  settings,  represent 
the 
complexity  of  infrastructure  and  networks  settings.  A  third 
class  of  faults,  such  as  Elastic  Load  Balancer  (ELB)  faults 
and  resource  unavailability  represent  the  uncertainty  of  the 
infrastructure. For example, an AWS ELB service disruption 
[9] was caused by ‚Äúmissing ELB state data‚Äù last year.  
the  configuration  errors, 
D.  Experiment Results 
Conformance  checking  was  used  to  detect  errors  and 
derive  the  error  context.  The  first  4  fault  types  are  not 
detectable by conformance checking (since the log output is 
the same). Out of the remaining 4 fault types with 20 runs 
each  (i.e.,  80  runs  in  total),  conformance  checking  found 
that  20  produced  erroneous  log  traces  before  assertion 
checking.  When  called  locally,  the  conformance  checking 
service responded on average in about 10ms. 
TABLE I.  
EVALUATION METRICS 
Injected fault 
No fault 
Precision of 
Detection 
Recall of 
Detection 
Accuracy Rate of 
Diagnosis 
Detected 
TPdet 
FPdet 
Undetected 
FNdet 
TNdet 
R
det
=
=
Pdet
TPdet
TPdet
+ FPdet
TP
det
+
FN
AR = Numcorrect
+ FPdet
TP
det
TPdet
det
Figure 6. Distribution of error diagnosis time 

	

	

	












Figure 7. Precision/Recall of Detection/Accuracy Rate of Diagnosis 
The  evaluation  shows  that  95%  of  the  online  error 
diagnosis finished within 3.83 seconds. The overall precision 
was 91.95%, the recall was 100% for error detection, and the 
260260260
accuracy rate for root cause diagnosis was 97.13%. Table I 
shows the formulas we used to calculate these percentages, 
where T,F,P,N mean true/false/positive/negative ‚Äì e.g. TP is 
the number of true positives. Of the correctly detected faults, 
we  calculate  the  accuracy  rate  of  diagnosis  by  dividing  the 
number  of  root  causes  being  accurately  diagnosed  by  the 
number  of  errors  being  detected.  In  the  cases  of  FPdet,  the 
accurate diagnosis tells us ‚ÄúNo root cause identified‚Äù. 
In  Figure  6,  we  give  the  distribution  of  error  diagnosis 
time  of  the  160  runs.  The  range  of  the  diagnosis  time  is 
from 1.29s to 10.44s. The average diagnosis time is 2.30s. 
The  values  of  detection  precision,  detection  recall  and 
diagnosis accuracy rate grouped by fault type are shown in 
the column chart of Figure 7. 
VI.  DISCUSSION AND LIMITATIONS 
A.  Discussion  
All  160  faults  we  injected  in  our  experiment  were 
detected by our assertion evaluation. Thus, FNdet of all the 8 
faults  are  0,  and  recall  of  error  detection  is  100%.  The 
assertion  evaluations  triggered  by  log  provide  an  effective 
detection. The reasons for this high rate are discussed below. 
Due to some limitations of our approach, there are false 
positives of detection, and wrong diagnosis. We give a few 
examples here and discuss the limitation in the next section. 
One  class  of  FPdet  is  caused  by  the  error  detection 
triggered  due  to  timeout.  The  timeout  setting  comes  from 
historical  data.  There  are  rare  cases  that  an  operation  is 
running successfully, with late log appearance, which causes 
the assertion evaluation to fail. However, in all such cases, 
our diagnosis returned ‚ÄúNo root cause identified‚Äù.  This issue 
could be solved by increasing the timeout values, at the cost 
of later detection of actual errors. 
The  second  class  of  FPdet is  caused  by  an  unexpected 
long assertion evaluation. In such cases, when the assertion 
evaluation asserts the number of instances, the ‚Äúshould-be‚Äù 
number  is  changed  by  another  assertion  evaluation  thread, 
which caused the assertion evaluation to fail.
One  class  of  wrong  diagnosis comes  from  purely  timer 
based assertion evaluation failure. As it was not triggered by 
log  lines,  there  was  very  limited  information  (e.g.  no 
instance id) on the target that diagnosis tests can check.  
The  second  class  of  wrong  diagnosis  comes  from 
multiple diagnoses across rolling upgrade loops. A changed 
AMI  fault  happened  and  it  caused  some  instances  of  the 
wrong version to be launched. Multiple assertion failures and 
associated diagnosis tests are conducted. During this period, 
the  AMI  changed  again  causing  the  diagnosis  results  to  be 
different across different diagnosis tests.  
The  third  class  of  wrong  diagnosis  comes  from  a 
transient  launch  configuration  change  fault.  The  fault 
injection  mechanism  injected  a  fault  and  then  corrected  it 
soon  after.  When  the  on-demand  diagnosis  test  tried  to 
confirm the root cause, it could not find any faults since the 
root  cause  had  effectively  been  removed  by  that  time.  To 
mitigate this, one could consult other sources of information, 
such as CloudTrail ‚Äì which can take fairly long to provide 
the relevant information.  
simply 
expected 
capturing 
The  fourth  class  of  wrong  diagnosis  comes  from 
interference by the other independent team sharing the same 
AWS account. The simultaneous operation on a different set 
of instances caused the account instance limit to be reached. 
Our fault tree did not capture this as a potential root cause, 
thus the diagnosis stopped at an error on exceeding limit. We 
amended the on demand assertions and the root cause so that 
we can correctly diagnose this fault in the future.  
Some assertions are added because of the subtle errors or 
potential causes we learn about over time (in addition to the 
assertions 
intermediate 
outcomes).  They  act  like  regression  tests.  That  is  both  a 
strength and a weakness. The strength is shown by the high 
rate  of  detection  of  injected  errors.  The  weakness  is  that 
previously unseen errors may pass undetected. However, the 
assertions about expected intermediate outcomes can already 
capture  many 
the  other  hand, 
conformance  checking  finds  deviations  from  the  usual  log 
behavior,  and  does  not  require  manual  specification  of 
known errors. However, conformance checking only detects 
errors that can be detected from changed log behavior. These 
two  types  of  triggering  are  complementary.  The  ability  of 
conformance checking to detect a high number of errors in 
the absence of assertions is part of an ongoing investigation. 
B.  Limitations  
important  errors.  On 
First,  our  approach  is  designed  for  sporadic  operations. 
As mentioned earlier, due to the emerging ‚Äúcontinuous‚Äù high 
frequency nature of sporadic operations, the approach could 
be deployed alongside normal operations. It is not meant to 
replace normal operation monitoring and error diagnosis.  
Second,  our  approach  at  the  moment  heavily  relies  on 
logs,  whose  quality  varies  across  different  software  and 
cloud  infrastructure  providers.  If  there  is  very  limited 
information  in  logs,  the  process  discovery,  conformance 
checking,  and  assertion-based  error  detection  might  be 
severely  affected.  Because  logs  are  extremely  important  in 
system  management  and  troubleshooting,  in  contemporary 
production systems integrated logs and logging sub-systems 
have  become  a  MUST.  Moreover,  administrators  and 
strongly  desire  well-formatted  and  easily 
engineers 
understandable 
logs  and  autonomous  system  monitors 
request  machine-readable  logs.  Thus,  our  requirements  on 
logs are becoming more realistic through the evolution of the 
industry.  Our  assertion  evaluation  also  supports  timers  and 
can  be  triggered  by  non-log  sources,  which  overcomes  the 
limitation to a degree. But as discussed above, inappropriate 
timeout settings can introduce more false positives.  
Third, for online diagnosis, the information staleness and 
timeliness  in  various  monitoring  facilities  is  essential.  For 
example, we could not use the newly released CloudTrail for 
API call logs due to the major delay between a call and the 
associated log appearing. On the other hand, the fault might 
be transient and may have disappeared when the on-demand 
diagnosis tests are conducted.  
We  note  there  are  limitations  that  are  intrinsic  to  our 
approach  such  as  relying  on  logs  and  targeting  sporadic 
operations.  Other  limitations  are  due  to  the  quality  of  the 
current set of assertions, fault trees and diagnosis tests. One 
can always improve them to detect unforeseen ones.   
VII.  RELATED WORK 
A.  Error Handling in Operation Processes 
When an operation process is automated through scripts 
and  the  infrastructure-as-code  approach,  the  error  handling 
mechanisms within the scripting or high-level languages can 
detect  and  react  to  errors  through  exception  handling,  for 
example  error  handlers  in  Asgard,  AWS  API  error  code, 
such as [10], and  Chef‚Äôs fault handlers [11]. These exception 
handing  mechanisms  are  best  suited  for  a  single  language 
environment breaking down, as operations have to deal with 