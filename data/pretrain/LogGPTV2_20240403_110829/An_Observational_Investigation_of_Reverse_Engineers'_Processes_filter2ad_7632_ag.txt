and procedures for developing grounded theory. Newbury Park, CA:
Sage, 1998, vol. 15.
[67] A. F. Hayes and K. Krippendorﬀ, “Answering the call for a standard
reliability measure for coding data,” Communication methods and
measures, vol. 1, no. 1, pp. 77–89, 2007.
[68] D. G. Freelon, “Recal: Intercoder reliability calculation as a web
service,” International Journal of Internet Science, vol. 5, no. 1, pp.
20–33, 2010.
[69] M. Lombard, J. Snyder-Duch, and C. C. Bracken, “Content analysis
in mass communication: Assessment and reporting of intercoder relia-
bility,” Human communication research, vol. 28, no. 4, pp. 587–604,
2002.
[70] C. Dietrich, K. Krombholz, K. Borgolte, and T. Fiebig, “Investigating
system operators’ perspective on security misconﬁgurations,” in CCS
’18. ACM, 2018.
[71] G. Guest, A. Bunce, and L. Johnson, “How many interviews are
enough? an experiment with data saturation and variability,” Field
methods, vol. 18, no. 1, pp. 59–82, 2006.
[72] Hackerone, “2019 bug bounty hacker report,” Hackerone, Tech. Rep.,
March 2019. [Online]. Available: https://www.hackerone.com/sites/
default/ﬁles/2019-03/the-2019-hacker-report_0.pdf
[73] A. Zeller, Why programs fail: a guide to systematic debugging. El-
sevier, 2009.
[74] M. P. Robillard, W. Coelho, and G. C. Murphy, “How eﬀective de-
velopers investigate source code: an exploratory study,” IEEE Trans-
actions on Software Engineering, vol. 30, no. 12, pp. 889–903, Dec
2004.
[56] T. E. Miller, S. P. Wolf, M. L. Thordsen, and G. Klein, “A decision-
centered approach to storyboarding anti-air warfare interfaces,” Fair-
born, OH: Klein Associates Inc. Prepared under contract, no. 66001,
1992.
[75] T. Roehm, R. Tiarks, R. Koschke, and W. Maalej, “How do
professional developers comprehend software?” in ICSE ’12.
Piscataway, NJ, USA: IEEE Press, 2012, pp. 255–265. [Online].
Available: http://dl.acm.org/citation.cfm?id=2337223.2337254
[57] K. Ohtsuka, “"scheduling tracing", a technique of knowledge elicita-
tion for production scheduling,” in ICSMCCCS ’97, vol. 2, Oct 1997,
pp. 1033–1038 vol.2.
[58] D. W. Klinger, R. Stottler, and S. R. LeClair, “Manufacturing ap-
plication of case-based reasoning,” in NAECON ’92, May 1992, pp.
855–859 vol.3.
[59] A. Von Mayrhauser and S. Lang, “Program comprehension and en-
hancement of software,” in IFIP World Computing Congress on Infor-
mation Technology and Knowledge Engineering, 1998.
[60] T. D. LaToza and B. A. Myers, “Developers ask reachability questions,”
in ICSE ’10. New York, NY, USA: ACM, 2010, pp. 185–194.
[Online]. Available: http://doi.acm.org/10.1145/1806799.1806829
[61] B. Johnson, Y. Song, E. Murphy-Hill, and R. Bowdidge, “Why don’t
software developers use static analysis tools to ﬁnd bugs?” in ICSE
’13.
IEEE Press, 2013, pp. 672–681.
[62] J. Smith, B. Johnson, E. Murphy-Hill, B. Chu, and H. R. Lipford,
“Questions developers ask while diagnosing potential security vulnera-
bilities with static analysis,” in ESEC/FSE ’15. New York, NY, USA:
ACM, 2015, pp. 248–259.
[63] S. Krüger, J. Späth, K. Ali, E. Bodden, and M. Mezini, “CrySL: An Ex-
tensible Approach to Validating the Correct Usage of Cryptographic
APIs,” in ECOOP ’18, ser. Leibniz International Proceedings in Infor-
matics (LIPIcs), T. Millstein, Ed., vol. 109, Dagstuhl, Germany, 2018,
pp. 10:1–10:27.
[64] K. Charmaz, Constructing Grounded Theory: A Practical Guide
Through Qualitative Analysis. SagePublication Ltd, London, 2006.
[76] Y. Shoshitaishvili, R. Wang, A. Dutcher, L. Dresel, E. Gustafson,
N. Redini, P. Grosen, C. Unger, C. Salls, N. Stephens, C. Hauser,
J. Grosen, C. Kruegel, and G. Vigna, “Lighthouse | code coverage
explorer for ida pro & binary ninja,” 2019, (Accessed 08-21-2019).
[Online]. Available: http://angr.io
[77] J. Henry, D. Monniaux, and M. Moy, “Pagai: A path sensitive
static analyser,” Electronic Notes in Theoretical Computer Science,
vol. 289, pp. 15–25, Dec. 2012.
[Online]. Available: http:
//dx.doi.org/10.1016/j.entcs.2012.11.003
[78] Hex-Rays, “Ida: Lumina server,” Hex-Rays, 2017, (Accessed 01-06-
2019). [Online]. Available: https://www.hex-rays.com/products/ida/
lumina/index.shtml
[79] Radare, “Radare,” 2019, (Accessed 11-11-2019). [Online]. Available:
https://rada.re/n/radare2.html
[80] M. Gaasedelen, “Lighthouse | code coverage explorer for ida pro
& binary ninja,” 2018, (Accessed 08-21-2019). [Online]. Available:
https://github.com/gaasedelen/lighthouse
[81] Hex-Rays, “Hex-rays decompiler: Overview,” Hex-Rays, 2019,
[Online]. Available: https://www.hex-
(Accessed 11-11-2019).
rays.com/products/decompiler/
[82] I. Haller, A. Slowinska, M. Neugschwandtner, and H. Bos, “Dowsing
for overﬂows: A guided fuzzer to ﬁnd buﬀer boundary violations,” in
USENIX Security ’13. Washington, D.C.: USENIX, 2013, pp. 49–64.
[83] T. Wang, T. Wei, G. Gu, and W. Zou, “Taintscope: A checksum-aware
directed fuzzing tool for automatic software vulnerability detection,”
in S&P ’10, May 2010, pp. 497–512.
1890    29th USENIX Security Symposium
USENIX Association
[84] W. Drewry and T. Ormandy, “Flayer: Exposing application internals,”
in WOOT ’07, 2007.
[85] M. Y. Wong and D. Lie, “Intellidroid: A targeted input generator for
Internet
the dynamic analysis of android malware.” in NDSS ’16.
Society, 2016, pp. 21–24.
[86] C. Zheng, S. Zhu, S. Dai, G. Gu, X. Gong, X. Han, and W. Zou, “Smart-
droid: An automatic system for revealing ui-based trigger conditions
in android applications,” in SPSM ’12. New York, NY, USA: ACM,
2012, pp. 93–104.
[87] T. Szabó, S. Erdweg, and M. Voelter, “Inca: A dsl for the
deﬁnition of incremental program analyses,” in ASE ’16. New
York, NY, USA: ACM, 2016, pp. 320–331. [Online]. Available:
http://doi.acm.org/10.1145/2970276.2970298
[88] Y. Smaragdakis, M. Bravenboer, and O. Lhoták, “Pick your
contexts well: Understanding object-sensitivity,” in Proceedings
of
the 38th Annual ACM SIGPLAN-SIGACT Symposium on
Principles of Programming Languages, ser. POPL ’11. New
York, NY, USA: ACM, 2011, pp. 17–30. [Online]. Available:
http://doi.acm.org/10.1145/1926385.1926390
[89] G. Kastrinis and Y. Smaragdakis, “Hybrid context-sensitivity for
points-to analysis,” SIGPLAN Notes, vol. 48, no. 6, pp. 423–434,
Jun. 2013. [Online]. Available: http://doi.acm.org/10.1145/2499370.
2462191
[90] T. Gilray, M. D. Adams, and M. Might, “Allocation characterizes
polyvariance: A uniﬁed methodology for polyvariant control-ﬂow
analysis,” SIGPLAN Notes, vol. 51, no. 9, pp. 407–420, Sep. 2016.
[Online]. Available: http://doi.acm.org/10.1145/3022670.2951936
[91] L. Battle and J. Heer, “Characterizing exploratory visual analysis:
A literature review and evaluation of analytic provenance in
tableau,” Computer Graphics Forum, 2019. [Online]. Available:
http://idl.cs.washington.edu/papers/exploratory-visual-analysis
[92] B. Shneiderman, “The eyes have it: a task by data type taxonomy for
information visualizations,” in IEEE Symposium on Visual Languages,
Sep. 1996, pp. 336–343.
[93] J. Heer and B. Shneiderman, “Interactive dynamics for visual analysis,”
Communications of the ACM, vol. 55, no. 4, pp. 45–54, Apr. 2012.
[Online]. Available: http://doi.acm.org/10.1145/2133806.2133821
[94] A. Perer and B. Shneiderman, “Systematic yet ﬂexible discovery:
Guiding domain experts through exploratory data analysis,” in IUI
’08. New York, NY, USA: ACM, 2008, pp. 109–118. [Online].
Available: http://doi.acm.org/10.1145/1378773.1378788
[95] A. Kalinin, U. Cetintemel, and S. Zdonik, “Interactive data
exploration using semantic windows,” in SIGMOD ’14. New
York, NY, USA: ACM, 2014, pp. 505–516. [Online]. Available:
http://doi.acm.org/10.1145/2588555.2593666
[96] T. Siddiqui, A. Kim, J. Lee, K. Karahalios, and A. Parameswaran,
“Eﬀortless data exploration with zenvisage: An expressive and
interactive visual analytics system,” VLDB Endowment, vol. 10,
no. 4, pp. 457–468, Nov. 2016.
[Online]. Available: https:
//doi.org/10.14778/3025111.3025126
[97] J. S. Yi, Y. a. Kang, and J. Stasko, “Toward a deeper understanding
of the role of interaction in information visualization,” IEEE Trans-
actions on Visualization and Computer Graphics, vol. 13, no. 6, pp.
1224–1231, Nov 2007.
[98] J. Heer, J. Mackinlay, C. Stolte, and M. Agrawala, “Graphical histories
for visualization: Supporting analysis, communication, and evaluation,”
IEEE Transactions on Visualization and Computer Graphics, vol. 14,
no. 6, pp. 1189–1196, Nov 2008.
[99] T. j. Jankun-Kelly, K. Ma, and M. Gertz, “A model and framework for
visualization exploration,” IEEE Transactions on Visualization and
Computer Graphics, vol. 13, no. 2, pp. 357–369, March 2007.
[100] W. A. Pike, J. Stasko, R. Chang, and T. A. O’Connell, “The science
of interaction,” Information Visualization, vol. 8, no. 4, pp. 263–274,
2009.
[101] L. Battle, R. Chang, and M. Stonebraker, “Dynamic prefetching
of data tiles for interactive visualization,” in SIGMOD ’16. New
York, NY, USA: ACM, 2016, pp. 1363–1375. [Online]. Available:
http://doi.acm.org/10.1145/2882903.2882919
[102] D. Gotz and Z. Wen, “Behavior-driven visualization recommendation,”
in IUI ’09. New York, NY, USA: ACM, 2009, pp. 315–324.
[Online]. Available: http://doi.acm.org/10.1145/1502650.1502695
[103] K. Dimitriadou, O. Papaemmanouil, and Y. Diao, “Explore-by-
example: An automatic query steering framework for interactive
data exploration,” in SIGMOD ’14. New York, NY, USA: ACM,
2014, pp. 517–528. [Online]. Available: http://doi.acm.org/10.1145/
2588555.2610523
[104] M. Vartak, S. Rahman, S. Madden, A. Parameswaran, and N. Polyzotis,
“Seedb: Eﬃcient data-driven visualization recommendations to
support visual analytics,” VLDB Endowment, vol. 8, no. 13, pp.
2182–2193, Sep. 2015. [Online]. Available: https://doi.org/10.14778/
2831360.2831371
A Interview protocol
A.1 App Background
To begin our discussion, I want you to think of a program that
you recently reverse engineered.
1. What was the name of the program? [If they’re not com-
fortable telling the name, there are a few additional cues
below]
(a) What type of functionality did the app provide?
[Exs: Banking, Messaging, Social Media, Produc-
tivity]
(b) Approximately, how many lines of code or number
of classes did the app have?
2. Why were you investigating this program?
3. Approximately, how long did you spend reverse engi-
neering this app?
4. What tools did you use for your reverse engineering
process? [Exs: IDAPro, debugger, fuzzer]
5. Did you reverse engineer this app with other people?
(a) (If yes) how did you divide up the work?
A.2 Reverse Engineering Process
Next, we’ll talk about this app in more detail. If possible, I
would like you to open the program you searched the same
way you did when you ﬁrst started investigating it. If you
would like to share your screen with me, that would be helpful
for providing context, however, this is not necessary. Primarily,
I want you to open everything on your computer to help you
USENIX Association
29th USENIX Security Symposium    1891
remember the exact steps you took when you searched the
program.
[If they do share their screen] Also, if you are comfortable,
I would like to record this screen sharing session, so that we
have a later reference.
Please walk me through how you searched the program. As
you go through your process, please explain every step you
took, even if it was not helpful toward your eventual goal. For
example, if you decided to reverse engineer a speciﬁc class,
but realized it was not relevant to your search after reading
the code, we would still like to know that you performed this
step. [a few cueing questions are provided below to guide the
conversation]
1. Where did you start?
2. What questions did you ask? How did you answer these
questions?
A.3
Items of Interest
Decision Points. [Every time the participant had to decide
between one or more actions during their process. Ex: Where
to start? What test cases to try? Which path to go down ﬁrst?
When to inspect a function?]
1. Record the decision that was made
2. How did you make this decisions? Explain your thought
process
Hypotheses. [Every time the participant states a question they
have to answer or makes a conjecture about what they think
the program (or component) does. Ex: X class performs Y
function. X data is transmitted oﬀ device, it’s using Y encryp-
tion]
1. Record the hypothesis or question asked
2. Why did you think this could be the case?
3. How did they (in)validate this hypothesis?
Beacons. [Every time the participant states recognizing the
functionality of some code without actually stepping through
it. That is, they are able to notice some pattern in the code
and make some deductions about functionality based on this]
1. Record the beacon that was noticed
2. Why did this stand out to you? How were you able to
recognize it?
3. How did you know that it was X instead of something
else?
Simulation. [Every time the participant discusses looking at
the code to determine how it works]
1. Record how they investigate the code.
(a) (If Automation) Do you use a custom tool or some-
thing open source/purchased?
i. (If not custom) What tool do you use?
A. Does this tool provide the results you
would want or does it fall short in some
way? [Ex: I actually want output X, but I
get Y, so I need to do these steps to get to
X]
(b) Is this generally the approach you use?
i. (If no) Why here and not in other cases?
ii. (If yes) What advantage do you think this ap-
proach has over other manual/automated in-
vestigation?
2. Please describe what’s going on in your head or the
automation?
(a) What are the inputs and outputs?
(b) When do you know when to stop?
Resources. [Every time the participant discusses referencing
some documentation or information source external to the
code]
1. Record what resource they used
2. Do you regularly consult this resource for information?
3. What do you think the beneﬁt of this resource is over
other sources of information? [Exs: Language documen-
tation, Stack Overﬂow, internal documentation]
1892    29th USENIX Security Symposium
USENIX Association