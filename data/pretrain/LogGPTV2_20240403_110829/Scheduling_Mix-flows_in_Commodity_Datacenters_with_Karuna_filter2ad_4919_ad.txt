taking at most seconds, irrespective of network scale.
Trafﬁc statistics collection & threshold distribution: Com-
puting thresholds requires ﬂow size information from the en-
tire network. It is impractical and time-consuming to collect
and analyze complete trafﬁc traces in large DCNs [34]. In-
stead, we design our end-host module (§7) to be capable of
collecting ﬂow information including sizes for all ﬂows, and
reporting to a centralized entity which computes the thresh-
olds. The reporting and computation are done periodically,
and in each period, a new set of thresholds are distributed to
the end-host modules.
Coﬂow scheduling in Karuna: Coﬂow [13, 14, 17] is an
important abstraction that identiﬁes the inter-dependencies
between ﬂows. Karuna can facilitate coﬂow scheduling by
exposing priorities in the network layer. Coﬂows with dead-
lines can be simply treated as type 1 ﬂows in Karuna, and
their deadlines can be met with the highest priority.
For the other 2 types, coﬂow scheduling requires applica-
tion level coordination across multiple servers to determine
the schedule–the order of transmission of coﬂows. With
Karuna, such an order can be easily expressed with priori-
ties in packets, and a similar idea, Smart Priority Class, has
already been explored in a recent proposal (Baraat [17]) for
decentralized coﬂow scheduling, where coﬂows mapped to
a higher priority class get strict precedence over those in a
lower priority class, and ﬂows of the same class share band-
width. Karuna can be readily employed in Baraat.
7.
IMPLEMENTATION
We have implemented a Karuna prototype. We describe
each component of the prototype in detail.
Information passing: For type 1 and 2 ﬂows, Karuna needs
to get the ﬂow information (i.e., sizes and deadlines) to en-
force ﬂow scheduling. Such information is also required by
previous works [4, 22, 30, 38, 39]. Flow information can be
obtained by patching applications in user space. However,
passing ﬂow information down to the network stack in ker-
nel space is still a challenge, which has not been explicitly
discussed in prior works.
To address this, in our implementation of Karuna, we use
setsockopt to set the mark for each packet sent through
a socket. mark is an unsigned 32-bit integer variable of
sk_buff structure in Linux kernel. By modifying the value
of mark for each socket, we can easily deliver per-ﬂow in-
formation into kernel space. Given that mark only has 32
bits, we use 12 bits for deadline information (ms) and the
remaining 20 bits for size information (KB) in the imple-
mentation. Therefore, mark can represent 1GB ﬂow size
and 4s deadline at most, which can meet the requirements of
most data center applications [3].
Packet tagging: This module maintains per-ﬂow state and
marks packets with a priority at end hosts. We implement it
as a Linux kernel module. The packet tagging module hooks
into the TX datapath at Netfilter Local_Out, residing
between TCP/IP stacks and TC.
The operations of the packet tagging modules are as fol-
lows: 1) when a outgoing packet is intercepted by Netfilter
hook, it will be directed to a hash-based ﬂow table. 2) Each
ﬂow in the ﬂow table is identiﬁed by the 5-tuple: src/dst IPs,
src/dst ports and protocol. For each new outgoing packet,
we identify the ﬂow it belongs to (or create a new ﬂow en-
try) and update per-ﬂow state (extract ﬂow size and deadline
information from mark for type 1&2 ﬂows and increase the
amount of bytes sent for type 3 ﬂows).7 3) Based on the ﬂow
information, we modify the the DSCP ﬁeld in the IP header
correspondingly to enforce packet priority.
Today’s NICs use various ofﬂoad mechanisms to reduce
CPU overhead. When Large Segmentation Ofﬂoading (LSO)
is enabled, the packet tagging module may not be able to set
the right DSCP value for each individual MTU-sized packet
with one large segment. To understand the impact of this
inaccuracy, we measure the lengths of TCP segments with
payload data in our 1G testbed. The average segment length
is only 7.2KB which has little impact to packet tagging. We
attribute this to the small TCP window size in the data center
network with small bandwidth delay product (BDP). Ideally,
packet tagging should be implemented in the NIC hardware
to completely avoid this issue.
Rate control: Karuna employs MCP for type 1 ﬂows and
DCTCP [3] for type 2&3 ﬂows at end hosts. For DCTCP
implementation, we use DCTCP patch [2] for Linux ker-
7For persistent TCP connections, we can periodically update
ﬂow states (e.g. , reset bytes sent to 0 for type 3 ﬂows that
are idle for some time).
nel 2.6.38.3. We implement MCP as a Netfilter kernel
module at receiver side inspired by [40]. The MCP mod-
ule intercepts TCP packets of deadline ﬂows and modiﬁes
the receive window size based on the MCP congestion con-
trol algorithm. This implementation choice avoids patching
network stacks of different OS versions.
MCP updates the congestion window based on the RTT
and the fraction of ECN marked packets each RTT (Eq.(11)).
Therefore, accurate RTT estimation is important for MCP.
We can only estimate RTT using TCP timestamp option since
the trafﬁc from the receiver to the sender may not be enough.
However, the current TCP timestamp option is in millisec-
ond granularity which cannot meet the requirement of data
center networks. Similar to [40], we modify timestamp to
microsecond granularity.
Switch conﬁguration: Karuna only requires ECN and strict
priority queueing, both of which are available in existing
commodity switches [4, 5, 30]. We enforce strict priority
queueing at the switches and classify packets based on the
DSCP ﬁeld. Like [3], we conﬁgure ECN marking based on
the instant queue lengths with a single marking threshold.
We observe that some of today’s commodity switching
chips provide multiple ways to conﬁgure ECN marking. For
our Broadcom BCM#56538, it supports ECN marking on
different egress entities (queue, port and service pool). In
per-queue ECN marking, each queue has its own marking
threshold and performs independent ECN marking. In per-
port ECN marking, each port is assigned a single marking
threshold and packets are marked when the sum of all queue
sizes belong to this port exceeds the marking threshold. Per-
port ECN marking cannot provide the same isolation be-
tween queues as per-queue ECN. Interested readers may re-
fer to [7] for detailed discussions on ECN marking schemes.
Despite this drawback, we still employ per-port ECN for
two reasons. First, per-port ECN marking has higher burst
tolerance. For per-queue ECN marking, each queue requires
an ECN marking threshold h to fully utilize the link inde-
pendently (e.g, DCTCP requires h=20 packets for 1G link).
When all the queues are active, it may require the shared
memory be at least the number of queues times the mark-
ing threshold, which cannot be supported by most shallow
buffered commodity switches. (e.g. our Gigabit Pronto 3295
switch has 384 queues and 4MB shared memory for 48 ports
in total). Second, per-port ECN marking can mitigate the
starvation problem, as it pushes back high priority ﬂows when
many packets of low priority ﬂows get queued in the switch
(see §8.1.3).
8. EVALUATION
We evaluate Karuna using testbed experiments and ns-3
simulations. The result highlights include:
• Karuna maintains low deadline miss rate (C, similar to [39]), and 3) no termination. We observe
that Scheme 1 has overall better performance: it terminates
more ﬂows than Scheme 2, but has fewer deadline misses
8Approximated by giving ﬂows pre-determined priorities.
Mbps5001000KarunaMbps05001000DCTCPFlow 1Flow 2Flow 3Flow 4Mbps5001000ms050100pFabricms00.51.01.5KarunaDCTCP(0,100KB] AFCT0246KarunaDCTCP(0,100KB] 99th FCT050100KarunaDCTCP(100KB,10MB] AFCT0500KarunaDCTCP(10MB,∞] AFCTScheme 1 MissedScheme 1 TerminatedScheme 2 MissedScheme 2 TerminatedScheme 3 Missed00.10.20.3Data MiningWeb SearchFigure 9: Effect of ECN.
Figure 11: Spine-leaf topology in simulation.
Figure 10: Effect of queue numbers.
(terminated ﬂows count as miss). This shows that Scheme 2
is too lenient in termination, and some ﬂows still send when
they cannot meet their deadline, wasting bandwidth.
Effect of ECN: To evaluate the effect of ECN in handling
the threshold-trafﬁc mismatch, we create a contrived work-
load where 80% of ﬂows are 30KB and 20% are 10MB and
conduct the experiment at 80% load. We assume all the
ﬂows are type 3 ﬂows and allocate 2 priority queues. Ob-
viously, the optimal sieving threshold should be 30KB. We
intentionally run experiments with three thresholds 20KB,
30KB and 2MB. In the ﬁrst case, the short ﬂow sieves to
the low priority too early, while in the third case, the long
ﬂows over-stay in the high priority queue.
In both cases,
packets of short ﬂows may experience large delay due to the
queue built up by long ﬂows. Figure 9 shows the FCT of
30KB ﬂows with and without ECN. When the threshold is
30KB, both schemes achieve ideal FCT. Karuna w/o ECN
even achieves 9% lower FCT due to the spurious marking
of per-port ECN. However, with a larger threshold (2MB)