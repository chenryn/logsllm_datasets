---
title: ZFS
date: 20201029
author: Lyz
---
[OpenZFS](https://en.wikipedia.org/wiki/ZFS) is a file system with volume
management capabilities designed specifically for storage servers.
Some neat features of ZFS include:
- Aggregating multiple physical disks into a single filesystem.
- Automatically repairing data corruption.
- Creating point-in-time snapshots of data on disk.
- Optionally encrypting or compressing data on disk.
# Usage
## Mount a pool as readonly
```bash
zpool import -o readonly=on {{ pool_name }}
```
## Mount a ZFS snapshot in a directory as readonly
```bash
mount -t zfs {{ pool_name }}/{{ snapshot_name }} {{ mount_path }} -o ro
```
## [Mount a dataset that is encrypted](https://unix.stackexchange.com/questions/730921/zfs-on-linux-how-to-import-an-encrypted-pool-if-you-dont-know-where-key-must-b)
If your dataset is encrypted using a key file you need to:
- Mount the device that has your keys
- Import the pool without loading the key because you want to override the keylocation attribute with zfs load-key. Without the -l option, any encrypted datasets won't be mounted, which is what you want.
- Load the key(s) for the dataset(s)
- Mount the dataset(s).
```bash
zpool import rpool    # without the `-l` option!
zfs load-key -L file:///path/to/keyfile rpool
zfs mount rpool
```
## [Umount a pool](https://superuser.com/questions/1542723/zfs-unmount-entire-pool)
```bash
zpool export pool-name
```
If you get an error of `pool or dataset is busy` run the next command to see which process is still running on the pool:
```bash
lsof 2>/dev/null | grep dataset-name
```
## List pools
```bash
zpool list
```
## List the filesystems
```bash
zfs list
```
## Get read and write stats from pool
```bash
zpool iostat -v {{ pool_name }} {{ refresh_time_in_seconds }}
```
## Get all properties of a pool
```bash
zpool get all {{ pool_name }}
```
## Get all properties of a filesystem
```bash
zfs get all {{ pool_name }}
```
## [Get compress ratio of a filesystem](https://www.sotechdesign.com.au/zfs-how-to-check-compression-efficiency/)
```bash
zfs get compressratio {{ filesystem }}
```
## [Rename or move a dataset](https://docs.oracle.com/cd/E19253-01/819-5461/gamnq/index.html)
NOTE: if you want to rename the topmost dataset look at [rename the topmost dataset](#rename-the-topmost-dataset) instead.
File systems can be renamed by using the `zfs rename` command. You can perform the following operations:
- Change the name of a file system.
- Relocate the file system within the ZFS hierarchy.
- Change the name of a file system and relocate it within the ZFS hierarchy.
The following example uses the `rename` subcommand to rename of a file system from `kustarz` to `kustarz_old`:
```bash
zfs rename tank/home/kustarz tank/home/kustarz_old
```
The following example shows how to use zfs `rename` to relocate a file system:
```bash
zfs rename tank/home/maybee tank/ws/maybee
```
In this example, the `maybee` file system is relocated from `tank/home` to `tank/ws`. When you relocate a file system through rename, the new location must be within the same pool and it must have enough disk space to hold this new file system. If the new location does not have enough disk space, possibly because it has reached its quota, rename operation fails.
The rename operation attempts an unmount/remount sequence for the file system and any descendent file systems. The rename command fails if the operation is unable to unmount an active file system. If this problem occurs, you must forcibly unmount the file system.
You'll loose the snapshots though, as explained below.
### [Rename the topmost dataset](https://www.solaris-cookbook.eu/solaris/solaris-zpool-rename/)
If you want to rename the topmost dataset you [need to rename the pool too](https://github.com/openzfs/zfs/issues/4681) as these two are tied. 
```bash
$: zpool status -v
  pool: tets
 state: ONLINE
 scrub: none requested
config:
        NAME        STATE     READ WRITE CKSUM
        tets        ONLINE       0     0     0
          c0d1      ONLINE       0     0     0
          c1d0      ONLINE       0     0     0
          c1d1      ONLINE       0     0     0
errors: No known data errors
```
To fix this, first export the pool:
```bash
$ zpool export tets
```
And then imported it with the correct name:
```bash
$ zpool import tets test
```
After the import completed, the pool contains the correct name:
```bash
$ zpool status -v
  pool: test
 state: ONLINE
 scrub: none requested
config:
        NAME        STATE     READ WRITE CKSUM
        test        ONLINE       0     0     0
          c0d1      ONLINE       0     0     0
          c1d0      ONLINE       0     0     0
          c1d1      ONLINE       0     0     0
errors: No known data errors
```
Now you may need to fix the ZFS mountpoints for each dataset
```bash
zfs set mountpoint="/opt/zones/[Newmountpoint]" [ZFSPOOL/[ROOTor other filesystem]
```
## [Rename or move snapshots](https://docs.oracle.com/cd/E19253-01/819-5461/gbion/index.html)
If the dataset has snapshots you need to rename them too. They must be renamed within the same pool and dataset from which they were created though. For example:
```bash
zfs rename tank/home/cindys@083006 tank/home/cindys@today
```
In addition, the following shortcut syntax is equivalent to the preceding syntax:
```bash
zfs rename tank/home/cindys@083006 today
```
The following snapshot rename operation is not supported because the target pool and file system name are different from the pool and file system where the snapshot was created:
```bash
$: zfs rename tank/home/cindys@today pool/home/cindys@saturday
cannot rename to 'pool/home/cindys@today': snapshots must be part of same 
dataset
```
You can recursively rename snapshots by using the `zfs rename -r` command. For example:
```bash
$: zfs list
NAME                         USED  AVAIL  REFER  MOUNTPOINT
users                        270K  16.5G    22K  /users
users/home                    76K  16.5G    22K  /users/home
users/home@yesterday            0      -    22K  -
users/home/markm              18K  16.5G    18K  /users/home/markm
users/home/markm@yesterday      0      -    18K  -
users/home/marks              18K  16.5G    18K  /users/home/marks
users/home/marks@yesterday      0      -    18K  -
users/home/neil               18K  16.5G    18K  /users/home/neil
users/home/neil@yesterday       0      -    18K  -
$: zfs rename -r users/home@yesterday @2daysago
$: zfs list -r users/home
NAME                        USED  AVAIL  REFER  MOUNTPOINT
users/home                   76K  16.5G    22K  /users/home
users/home@2daysago            0      -    22K  -
users/home/markm             18K  16.5G    18K  /users/home/markm
users/home/markm@2daysago      0      -    18K  -
users/home/marks             18K  16.5G    18K  /users/home/marks
users/home/marks@2daysago      0      -    18K  -
users/home/neil              18K  16.5G    18K  /users/home/neil
users/home/neil@2daysago       0      -    18K  -
```
## [Repair a DEGRADED pool](https://blog.cavelab.dev/2021/01/zfs-replace-disk-expand-pool/)
First you need to make sure that it is in fact a problem of the disk. Check the `dmesg` to see if there are any traces of reading errors, or SATA cable errors. 
A friend suggested to mark the disk as healthy and do a resilver on the same disk. If the error is reproduced in the next days, then replace the disk. A safer approach is to resilver on a new disk, analyze the disk when it's not connected to the pool, and if you feel it's safe then save it as a cold spare.
### Replacing a disk in the pool
If you are going to replace the disk, you need to bring offline the device to be replaced:
```bash
zpool offline tank0 ata-WDC_WD2003FZEX-00SRLA0_WD-xxxxxxxxxxxx
```
Now let us have a look at the pool status.
```bash
zpool status
NAME                                            STATE     READ WRITE CKSUM
tank0                                           DEGRADED     0     0     0
  raidz2-1                                      DEGRADED     0     0     0
    ata-TOSHIBA_HDWN180_xxxxxxxxxxxx            ONLINE       0     0     0
    ata-TOSHIBA_HDWN180_xxxxxxxxxxxx            ONLINE       0     0     0
    ata-TOSHIBA_HDWN180_xxxxxxxxxxxx            ONLINE       0     0     0
    ata-WDC_WD80EFZX-68UW8N0_xxxxxxxx           ONLINE       0     0     0
    ata-TOSHIBA_HDWG180_xxxxxxxxxxxx            ONLINE       0     0     0
    ata-TOSHIBA_HDWG180_xxxxxxxxxxxx            ONLINE       0     0     0
    ata-WDC_WD2003FZEX-00SRLA0_WD-xxxxxxxxxxxx  OFFLINE      0     0     0
    ata-ST4000VX007-2DT166_xxxxxxxx             ONLINE       0     0     0
```
Sweet, the device is offline (last time it didn't show as offline for me, but the offline command returned a status code of 0). 
Time to shut the server down and physically replace the disk.
```bash
shutdown -h now
```
When you start again the server, itâ€™s time to instruct ZFS to replace the removed device with the disk we just installed.
```bash
zpool replace tank0 \
    ata-WDC_WD2003FZEX-00SRLA0_WD-xxxxxxxxxxxx \
    /dev/disk/by-id/ata-TOSHIBA_HDWG180_xxxxxxxxxxxx
```
```bash
zpool status tank0
pool: main
state: DEGRADED
status: One or more devices is currently being resilvered.  The pool will
        continue to function, possibly in a degraded state.
action: Wait for the resilver to complete.
  scan: resilver in progress since Fri Sep 22 12:40:28 2023
        4.00T scanned at 6.85G/s, 222G issued at 380M/s, 24.3T total
        54.7G resilvered, 0.89% done, 18:28:03 to go
NAME                                              STATE     READ WRITE CKSUM
tank0                                             DEGRADED     0     0     0
  raidz2-1                                        DEGRADED     0     0     0
    ata-TOSHIBA_HDWN180_xxxxxxxxxxxx              ONLINE       0     0     0
    ata-TOSHIBA_HDWN180_xxxxxxxxxxxx              ONLINE       0     0     0
    ata-TOSHIBA_HDWN180_xxxxxxxxxxxx              ONLINE       0     0     0
    ata-WDC_WD80EFZX-68UW8N0_xxxxxxxx             ONLINE       0     0     0
    ata-TOSHIBA_HDWG180_xxxxxxxxxxxx              ONLINE       0     0     0
    ata-TOSHIBA_HDWG180_xxxxxxxxxxxx              ONLINE       0     0     0
    replacing-6                                   DEGRADED     0     0     0
      ata-WDC_WD2003FZEX-00SRLA0_WD-xxxxxxxxxxxx  OFFLINE      0     0     0
      ata-TOSHIBA_HDWG180_xxxxxxxxxxxx            ONLINE       0     0     0  (resilvering)
    ata-ST4000VX007-2DT166_xxxxxxxx               ONLINE       0     0     0
```
The disk is replaced and getting resilvered (which may take a long time to run (18 hours in a 8TB disk in my case).
Once the resilvering is done; this is what the pool looks like.
```bash
zpool list
NAME      SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
tank0    43.5T  33.0T  10.5T     14.5T     7%    75%  1.00x  ONLINE  -
```
If you want to read other blogs that have covered the same topic check out [1](https://madaboutbrighton.net/articles/replace-disk-in-zfs-pool).
### Check the health of the degraded disk
Follow [these instructions](hard_drive_health.md#check-the-disk-health).
### RMA the degraded disk
Follow [these instructions](hard_drive_health.md#check-the-warranty-status).
# Installation
## Install the required programs
OpenZFS is not in the mainline kernel for license issues (fucking capitalism...) so it's not yet suggested to use it for the root of your filesystem. 
To install it in a Debian device:
* ZFS packages are included in the `contrib` repository, but the `backports` repository often provides newer releases of ZFS. You can use it as follows.
  Add the backports repository:
  ```bash
  vi /etc/apt/sources.list.d/bullseye-backports.list
  ```
  ```
  deb http://deb.debian.org/debian bullseye-backports main contrib
  deb-src http://deb.debian.org/debian bullseye-backports main contrib
  ```
  ```bash
  vi /etc/apt/preferences.d/90_zfs
  ```
  ```
  Package: src:zfs-linux
  Pin: release n=bullseye-backports
  Pin-Priority: 990
  ```
* Install the packages:
```bash