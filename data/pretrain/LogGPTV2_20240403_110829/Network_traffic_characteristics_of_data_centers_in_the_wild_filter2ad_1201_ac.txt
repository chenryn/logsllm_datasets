ied cloud data center [19]. Among other issues, ﬂow inter-arrival
time affects what kinds of processing can be done for each new
ﬂow and the feasibility of logically centralized controllers for ﬂow
placement. We return to these questions in Section 7.
Next, we examine the distributions of ﬂow sizes and and lengths
in Figure 4(a) and (b), respectively. From Figure 4(a), we ﬁnd that
ﬂow sizes are roughly similar across all the studied switches and
data centers. Across the data centers, we note that 80% of the ﬂows
are smaller than 10KB in size. Most of the bytes are in the top 10%
of large ﬂows. From Figure 4(b), we ﬁnd that for most of the data
centers 80% of the ﬂows are less than 11 seconds long. These re-
sults support the observations made in prior a study [19] of a cloud
data center. However, we do note that the ﬂows in EDU2 appear
to be generally shorter and smaller than the ﬂows in the other data
centers. We believe this is due to the nature of the predominant
application that accounts for over 70% of the bytes at the switch.
Finally, in Figure 5, we examine the distribution of packet sizes
in the studied data centers. The packet sizes exhibit a bimodal pat-
 1
 0.8
 0.6
 0.4
 0.2
 0
 10
 1
 0.8
 0.6
 0.4
 0.2
 0
 10
F
D
C
(a)
F
D
C
(b)
EDU1
EDU2
EDU3
PRV21
PRV22
PRV23
PRV24
 100
 1000
 10000
 100000
Number of Active Flows
EDU1
EDU2
EDU3
PRV21
PRV22
PRV23
PRV24
 100
 1000
 10000
 100000
Flow Interarrival Times (in usecs) 
Figure 3: CDF of the distribution of the number of ﬂows at
the edge switch (a) and the arrival rate for ﬂows (b) in EDU1,
EDU2, EDU3, and PRV2.
tern, with most packet sizes clustering around either 200 Bytes and
1400 Bytes. Surprisingly, we found application keep-alive packets
as a major reason for the small packets, with TCP acknowledg-
ments, as expected, being the other major contributor. Upon close
inspection of the packet traces, we found that certain applications,
including MSSQL, HTTP, and SMB, contributed more small pack-
ets than large packets.
In one extreme case, we found an appli-
cation producing 5 times as many small packets as large packets.
This result speaks to how commonly persistent connections occur
as a design feature in data center applications, and the importance
of continually maintaining them.
5.2 Packet-Level Communication Character-
istics
We ﬁrst examine the temporal characteristics of the packet traces.
Figure 6 shows a time-series of packet arrivals observed at one of
the sniffers in PRV2, and the packet arrivals exhibit an ON/OFF
pattern at both 15ms and 100ms granularities. We observed similar
trafﬁc patterns at the remaining 6 switches as well.
Per-packet arrival process: Leveraging the observation that
trafﬁc is ON/OFF, we use a packet inter-arrival time threshold to
identify the ON/OFF periods in the traces. Let arrival95 be the
95th percentile value in the inter-arrival time distribution at a par-
ticular switch. We deﬁne a periodon as the longest continual pe-
riod during which all the packet inter-arrival times are smaller than
arrival95. Accordingly, a periodof f is a period between two ON
periods. To characterize this ON/OFF trafﬁc pattern, we focus on
three aspects: (i) the durations of the ON periods, (ii) the durations
272 1
 0.8
 0.6
 0.4
 0.2
 0
 1
 1
 0.8
 0.6
 0.4
 0.2
 0
EDU1
EDU2
EDU3
PRV21
PRV22
PRV23
PRV24
 10
 100
 1000  10000  100000  1e+06  1e+07  1e+08
Flow Sizes (in Bytes)
EDU1
EDU2
EDU3
PRV21
PRV22
PRV23
PRV24
 1
 10
 100  1000  10000 100000 1e+06 1e+07 1e+08 1e+09
Flow Lengths (in usecs)
F
D
C
(a)
F
D
C
(b)
Figure 4: CDF of the distribution of the ﬂow sizes (a) and of
ﬂow lengths (b) in PRV2, EDU1, EDU2, and EDU3.
F
D
C
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
EDU1
EDU2
EDU3
PRV21
PRV22
PRV23
PRV24
 200
 400
 600
 800
 1000  1200  1400  1600
Packet Size (in Bytes)
Figure 5: Distribution of packet size in the various networks.
of the OFF periods, and (iii) the packet inter-arrival times within
ON periods.
Figure 7(a) shows the distribution of inter-arrival times within
ON periods at one of the switches for PRV2. We bin the inter-
arrival times according to the clock granularity of 10µs. Note that
the distribution has a positive skew and a heavy tail. We attempted
to ﬁt several heavy-tailed distributions and found that the lognormal
curve produces the best ﬁt with the least mean error. Figure 7(b)
d
e
v
i
e
c
e
r
s
t
e
k
c
a
p
f
o
#
0
2
6
4
8
Time (in Milliseconds)
(a) 15ms
d
e
v
i
e
c
e
r
s
t
e
k
c
a
p
#
10
4
x 10
0
2
4
8
Time (in milliseconds)
6
10
4
x 10
(b) 100ms
Figure 6: ON/OFF characteristics: Time series of Data Center
trafﬁc (number of packets per time) binned by two different
time scales.
Data center Off period ON period Interarrival Rate
P RV 21 Lognormal Lognormal
P RV 22 Lognormal Lognormal
P RV 23 Lognormal Lognormal
P RV 24 Lognormal Lognormal
DistributionDistribution Distribution
Lognormal
Lognormal
Lognormal
Lognormal
EDU1 Lognormal Weibull
EDU2 Lognormal Weibull
EDU3 Lognormal Weibull
Weibull
Weibull
Weibull
Table 4: The distribution for the parameters of each of the ar-
rival processes of the various switches.
shows the distribution of the durations of ON periods. Similar to
the inter-arrival time distribution, this ON period distribution also
exhibits a positive skew and ﬁts well with a lognormal curve. The
same observation can be applied to the OFF period distribution as
well, as shown in Figure 7(c).
We found qualitatively similar characteristics at the other 6
switches where packet traces were collected. However, in ﬁtting a
distribution to the packet traces (Table 4), we found that only the
OFF period at the different switches consistently ﬁt the lognormal
distribution. For the ON periods and interarrival rates, we found
that best distribution was either Weibull and lognormal, varying by
data center.
Our ﬁndings indicate that certain positive skewed and heavy-
tailed distributions can model data center switch trafﬁc. This high-
lights a difference between the data center environment and the
wide area network, where the long-tailed Pareto distribution typ-
ically shows the best ﬁt [27, 24]. The differences between these
distributions should be taken into account when attempting to apply
models or techniques from wide area networking to data centers.
Per-application arrival process: Recall that the data centers in
this analysis, namely, EDU1, EDU2, EDU3, and PRV2, are domi-
nated by Web and distributed ﬁle-system trafﬁc (Figure 2). We now
examine the arrival processes for these dominant applications to
see if they explain the aggregate arrival process at the correspond-
ing switches. In Table 5, we present the distribution that best ﬁts
the arrival process for the dominant application. From this table,
we notice that the dominant applications in the universities (EDU1,
EDU2, EDU3), which account for 70–100% of the bytes at the
respective switches, are indeed characterized by identical heavy-
tailed distributions as the aggregate trafﬁc. However, in the case
of two of the PRV2 switches (#1 and #3), we ﬁnd that the domi-
nant application differs slightly from the aggregate behavior. Thus,
in the general case, we ﬁnd that simply relying on the characteris-
tics of the most dominant applications is not sufﬁcient to accurately
model the aggregate arrival processes at data center edge switches.
273Data center Off period Interarrival Rate ON period Dominant
Distribution Distribution DistributionApplications
Weibull
P RV 21 Lognormal
P RV 22 Weibull
P RV 23 Weibull
P RV 24 Lognormal Lognormal
EDU1 Lognormal Lognormal
EDU2 Lognormal
EDU3 Lognormal
Exponential Others
Lognormal
LDAP
Lognormal
Lognormal Exponential HTTP
Others
HTTP
NCP
AFS
Weibull
Weibull
Weibull
Weibull
Weibull
Weibull
Table 5: The distribution for the parameters of each of the ar-
rival processes of the dominant applications on each switch.
data center is bursty in nature and the ON/OFF intervals can be
characterized by heavy-tailed distributions; and (4) In some data
centers, the predominant application drives the aggregate sending
pattern at the edge switch. In the general case, however, simply
focusing on dominant applications is insufﬁcient to understand the
process driving packet transmission into the data center network.
In the next section, we analyze link utilizations at the various
layers within the data center to understand how the bursty nature of
trafﬁc impacts the utilization and packet loss of the links at each of
the layers.
6. NETWORK COMMUNICATION
PATTERNS
0
10
−1
10
F
D
C
−2
10
−3
10
−4
10
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
2
F
D
C
F
D
C
(a)
(b)
(c)
wbl: 0.013792
logn: 0.011119
exp: 0.059716
pareto: 0.027664
data
2
10
3
10
Interarrival Times (in milliseconds)
4
10
wbl :0.016516
logn :0.016093
exp :0.01695