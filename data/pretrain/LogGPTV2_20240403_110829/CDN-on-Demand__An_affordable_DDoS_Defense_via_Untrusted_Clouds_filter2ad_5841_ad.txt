in particular, bandwidth.
Furthermore, attackers may send their requests for dynamic
objects through multiple proxies, making it harder to identify
an attack and utilizing higher bandwidth of multiple proxy
nodes; see [39], [26].
8
Fig. 9: Origin-connectivity mechanisms: dynamic whitelist-
ﬁltering, loss-resilient tunnel and quotas for dynamic queries.
showing effective deployment with CDN-on-Demand.
B. Loss-Resilient Tunnel
Many ISPs provide a whitelisting service, however this is
not universal and the ﬁltering service may introduce additional
costs. Moreover, even when available, whitelist ﬁlters do not
prevent attackers from clogging one of the links en-route to
the content-origin with packets sent to other destinations, see
Attacker 3 in Figure 8. This attack vector was introduced
in [21], [38], and is considered difﬁcult to ﬁlter (in particular
using whitelists) since the victim is not the destination of
the attack trafﬁc. In this subsection we present our loss-
resilient tunnel construction, a mechanism against DoS attacks
which complements whitelist ﬁlters. The tunneling mechanism
is deployed on the origin-gateway and CDN proxies; it is
independent of third-party support such as the content-origin’s
ISP, which simpliﬁes its deployment and allows to integrate
with all sites.
Tunnel operation. The loss-resilient tunnel is invoked when
the manager’s origin-monitoring (i.e., watchdog) process de-
tects signiﬁcant loss-rates, which extensively reduce TCP’s
throughput; see Section II. Note that CDN-on-Demand proxies
are deployed already at lower loss rates, i.e., usually earlier.
Since the amount of trafﬁc sent between the cloud nodes and
the content-origin is very limited (as static content is served
directly from the managers, deployed on the clouds), the tunnel
is only required when the failure rate is high, as to cause TCP
to seriously degrade or possibly fail.
In such case, the manager begins to use error-correcting
codes to encode the trafﬁc sent to the content-origin, typi-
cally HTTP requests. Complementary, if the origin-gateway
identiﬁes signiﬁcant loss rates, it encodes the content-origin’s
responses to the managers; see illustration in Figure 9. Us-
ing error-correcting codes allows to recover communication
despite high losses, as we next explain. The origin-gateway
Content-OriginHTTP GET balance.php?AliceDecapsulateEncapsulate RequestSrc = 1.2.3.4, Dst-Port = 4444Dst = 5.6.7.8, Dst-Port = 8888IDA-encoded(HTTP GET home.php?Alice)Content-Origin’sISPimage.gif not in cache, request from content-originSrc = 5.6.7.8, SrcPort = 8888Dst = 1.2.3.4, DstPort = 4444IDA-encoded(100$)DecapsulateHTTP 200: image.gifManager1.2.3.4:4444Whitelist Filtering:Src = 1.2.3.4, Src-Port = 4444Dst = 5.6.7.8, Dst-Port = 8888ProxyOrigin Gateway5.6.7.8:8888HTTP GET balance.php?Alice100$EncapsulateCheck: user = Alice does not exceed quotaTo deal with such situations, and avoid excessive consump-
tion of content-origin resources, CDN-on-Demand deploys
quotas. Namely, the amount of resources consumed by each
client is restricted. The details differ between authenticated
clients and new or sporadic clients.
Authenticated users can be partially trusted, to be ‘well-
behaved’ and not to launch DoS attacks against the content-
origin. CDN-on-Demand identiﬁes connections by such cus-
tomers via the authentication cookie provided to the client
from the content-origin upon authentication (see Section III-C).
The resource-manager allocates a fair fraction of the maximal
bandwidth it allows for communicating with the content-origin
for each authenticated customer, up to a limit; see illustration
in Figure 9.
To enforce quotas for new and sporadic users, CDN-on-
Demand uses the client’s IP address. As long as the total
load on the content-origin is not too high, CDN-on-Demand
allocates a limited amount of resources for each such client
(limited per IP address). When the overall load on the content-
origin resources is excessive, these clients cannot initiate any
dynamic requests (priority given to authenticated users). CDN-
on-Demand is still able to provide service to new and sporadic
clients, even under high load conditions, or when their speciﬁc
resource-requirements exceed a threshold, provided that their
browser has a cached copy of the RootJS. Speciﬁcally, in this
case, the manager will return a CAPTCHA which the RootJS
will request the user to solve. Such mechanisms are well-
studied, e.g., see [4], [25], [41]. 2
D. Implementation and Evaluation
We implemented the defenses presented in CDN-on-
Demand to ensure connectivity with the content-origin. We
next present experimental evaluation, focusing on the dynamic
ﬁltering and loss-resilient tunnel mechanisms.
Setup. We deployed CDN-on-Demand using Amazon EC2 and
Google Compute Engine IaaS clouds to deploy the system’s
proxies, and connected clients from different geographic re-
gions using Planet-Lab machines [30]. We used a desktop
machine to host the content-origin and connect it to the net-
work via a 50Mbps link through another machine simulating
its ISP which has 500Mbps link. We perform the experiments
while our system handles 8K clients that repeatedly connect
to the CDN and download a dynamic (non-cacheable) 50KB
object from the website. To evaluate the origin-connectivity
mechanisms, we use the manager to measure loss-rates when
communicating with the content-origin, and use the clients to
measure the response time from the CDN.
1) Dynamic Whitelist Filters: In this set of measurements
we evaluate direct ﬂooding attacks (i.e., Attackers 1 and 2
in Figure 8), where the attacker sends clogging trafﬁc to the
content-origin. We run the content-origin monitoring mecha-
nism and employ the dynamic whitelisting mechanism (using
dynamic IP-addresses and randomized service ports) presented
above.
2An alternative to asking the users to solve a CAPTCHA, is for the RootJS
to solve a Proof-of-Work (client puzzle), see e.g. [28]; however, this may
beneﬁt attackers who may use native code to quickly solve the puzzle, while
legitimate clients use JavaScript.
Results. Lines 1 and 2 in Figure 10a show that by sending
trafﬁc to the content-origin, even at modest rates,
the at-
tacker can cause substantial packet loss rates and signiﬁcantly
increase the response time, eventually breaking most TCP
connections between the manager and the content-origin. We
observe a signiﬁcant decline in throughput, e.g., throughput
is only 2.45Mbps when the attacker transmits at 40Mbps
(80% of the content-origin link capacity). We ﬁnd that the
whitelist ﬁltering mechanism mitigates the direct attack vector;
compare lines 1, 2 to lines 3, 4 in Figure 10a. In particular,
the throughput of communication with the content-origin is
43.9Mbps, 18-times higher than without the ﬁlters. We note
that the results of both spoofed and non-spoofed attack ﬂavors
are similar, hence we present their average in a single graph.
2) Loss Resilient Tunnel: In this set of experiments we
assume that whitelist ﬁltering is unavailable for the content-
origin (e.g., not supported by its ISP) or that ﬂooding trafﬁc
can circumvent whitelist ﬁlters deployed at
the ISP (e.g.,
Attacker 3 in Figure 8), and evaluate the effect of a ﬂooding
DoS attack against the content-origin with and without the loss
resilient tunnel defense. The length of encoded data blocks
in the tunnel is 150KB, encoding responses for 3 requests
made via the manager (possibly by different proxies and
clients); since network MTU is typically 1.5KB we get that
m = 150KB
1.5KB = 100.
Results. Our measurements, illustrated in Figure 10b, show
that an attacker sending clogging trafﬁc at over twice the
content-origin’s link capacity, can cause loss rates of over
84%, crippling TCP connections (see lines 1 and 2). The loss-
resilient tunneling mechanism described above is enabled when
loss rate is high (over 5%), to ensure sufﬁcient quality of
service. We observe with the tunneling mechanism installed,
the loss-rate is kept steady below 5% and the response time
moderately increases to 2.11 seconds (compare lines 1 and 2
to lines 3 and 4 in Figure 10b).
At the peak of the attack, causing high 84% loss-rate,
the tunneling mechanism encodes every 100 packets to 640.
Despite the overhead we ﬁnd that
this mechanism allows
reasonable response time. This result is contrasted with normal
TCP connections, which collapses when loss rates are signiﬁ-
cant.
V. THE ORIGIN-GATEWAY
The origin-gateway is an easy-to-install module, acting as a
gateway for the content-origin server; it facilitates transparent
deployment of CDN-on-Demand’s defense mechanisms with-
out introducing changes to web-server conﬁguration or website
content. The origin-gateway has two main functions: First,
the origin-gateway captures and converts web-objects that
the content-origin sends into secure-objects (see Section III).
Second, the origin-gateway implements the monitoring and
resilient tunnel protocols at the origin’s side (see Section IV).
In this section we describe the operation and implementation
of both origin-gateway functions.
A. Automated Conversion to Secure-Objects
The origin-gateway is a trusted component, deployed in
the content-origin’s network and managed by its administrator
9
(a) Dynamic whitelist ﬁltering defense
(b) Loss-resilient tunnel defense
Fig. 10: Evaluation of DoS attacks against the content-origin
(cf. to CDN-on-Demand proxies). The administrator provides
the origin-gateway with the website’s private TLS key, and
it serves as the TLS-endpoint at the origin side. Namely, it
observes all requests and responses in clear-text. If CDN-
on-Demand is dormant, i.e., there is no DoS attack, and the
content-origin serves clients directly, then the origin-gateway
only relays requests and responses, to and from the content-
origin. In this case the origin-gateway only maintains a cache
of encapsulated objects, in order to facilitate rapid transition
to CDN-on-Demand when needed. We next describe the auto-
mated provisioning process of this cache.
To automatically encapsulate public objects, the origin-
gateway periodically connects to the content-origin and
‘crawls’ the website (as an unauthenticated user). It identi-
ﬁes new public objects, retrieves and encapsulates them as
described in Section III-C; see step 1 in Figure 11. When a
request for one of the public objects arrives, the origin-gateway
delivers the (encapsulated) secure-object in response. Namely,
the origin-gateway acts as a cache; see step 2 in Figure 11.
Private objects, in contrast, are associated with speciﬁc
users and are inaccessible to the origin-gateway while it crawls
the website. To allow automated encapsulation of private
objects, the administrator conﬁgures the origin-gateway with
the name of the authentication cookie. The encapsulation
mechanism for private objects is invoked when a GET request,
which speciﬁes an authentication cookie, arrives for an object
not marked as public (identiﬁed in the ‘crawling phase’). If the
object is not already stored in the origin-gateway’s cache, then
the origin-gateway forwards the request to the content-origin;
the origin-gateway also forwards the request when the object is
cached but not associated with this user (i.e., object key is not
in user’s object-table), but then it changes the request type to
HEAD, to only receive indication of success/failure. Namely,
the origin-gateway uses the content-origin as an ‘oracle’ for
checking whether the user is authorized to receive the object.
If the response indicates success (HTTP 200 code), then the
origin-gateway encapsulates the object for the user; see step 3
in Figure 11. We next describe the private-object encapsulation
process performed by the origin-gateway.
The origin-gateway selects ‘on the ﬂy’ a symmetric object-
key (if the object does not already have one) and a user-
key (if no key is already mapped to the user’s authentication
Fig. 11: Automated Conversion to Secure-Objects
cookie). The origin-gateway then encapsulates the object using
the object’s key and saves the secure-object in cache. Next, the
object-key is encrypted with the user’s key and saved in her
object-table (see details in Section III-C). The origin-gateway
also keeps track on the authentication cookie’s expiration date;
when the cookie expires and the user needs to re-authenticate,
the origin-gateway deletes the user-key associated with the
cookie and the corresponding object-table. When the user re-
authenticates and receives a fresh cookie, the origin-gateway
creates a new user-key and uses it to re-encrypt the user’s
private object-keys (as described above). This process, per-
formed online, is very efﬁcient: only symmetric cryptography
is involved, objects are encrypted once and object-keys are
only encrypted when the user authenticates.
Implementation. The origin-gateway keeps a cache of secure-
10
00.10.20.30.40.50.60.70.80.90.0250.050.10.20.40.81.60.5124816Packet Loss RateTime in seconds (log-scale)Attacker Rate / Origin Capacity (log-scale)1. Loss-Rate2. Response Time3. Loss-Rate, ISP Filters Attacker Traffic4. Response Time, ISP Filters Attacker Traffic00.10.20.30.40.50.60.70.80.900.30.60.91.21.51.82.10.5124816Packet Loss RateTime in seconds (log-scale)Attack Rate / Link Capacity1. Loss-Rate2. Response Time3. Loss-Rate, Loss-Resilient Tunnel4. Response Time, Loss-Resilient TunnelClientOrigin-GatewayContent-OriginPeriodic crawling of Content-OriginNew public objectsencapsulates newly discovered objectsRequest for a public objectEncapsulated object retrieved from cacheRequest for a private object + auth-cookierequested object not in cache or unassociated with user (identified by auth-cookie)relay client’s requestHTTP 200 OKcache enc_object object and associate with user (identified by auth-cookie)Encapsulated private object 123objects which maps an object’s name to its encapsulated
version and a ﬂag that marks whether that object is public. In
addition, for each user (identiﬁed by authentication cookie) the
origin-gateway keeps an object-table, which maps the object
name to the corresponding key (stored encrypted under the
user’s key). We use the BeautifulSoup library (v4.4) to crawl
the content-origin website and identify public objects. To de-
liver content updates to CDN-on-Demand, the origin-gateway
application uses the MitMProxy library (v0.13), which allows
to register custom methods for processing HTTP requests and
responses. The response-handler receives web-objects from the
content-origin, encapsulates them and stores in its cache. If the
object is private, then it also encrypts the object’s key and saves
it in the user’s object-table.
B. Transparent Content-Origin Connectivity Module
When the origin-gateway identiﬁes a signiﬁcant loss rate p
(e.g., p > 5%), it begins encoding messages with redundancy,
using the Information Dispersal Algorithm (IDA) [32] (see
Section IV-B). Namely, the TCP communication between the
origin-gateway and CDN-on-Demand’s managers (deployed on
the clouds) is encoded in UDP packets. The tunneled trafﬁc
carries redundant
information to allow content-recovery in
case of signiﬁcant loss. The origin-gateway also implements
the receiving-end of the loss-resilient tunnel, to decode IDA-
encoded requests from the CDN-on-Demand managers. It
decodes tunneled trafﬁc (i.e., reconstructs the content despite
loss) and then feeds the recovered (TCP) frames to the network
stack, simulating arrival from the network.
Implementation. We use the NetﬁlterQueue library (v0.3) which
allows to register methods for manipulating packets when
they are emitted and received. We implement two methods:
The ﬁrst, encoding-method, captures TCP packets just before