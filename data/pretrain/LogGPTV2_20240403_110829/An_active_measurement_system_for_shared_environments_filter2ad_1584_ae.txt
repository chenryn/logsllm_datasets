0.0000
0.0000
0.0000
0.0000
0.0000
0.0000
0.000
0.000
0.000
0.000
0.000
ating system and application). At the same time, the interquartile
range increased as load increased, similar to our observations in
the case of send spacing timestamp error, though the relative in-
crease was much larger. These observations hold for both MAD
and for the unprivileged process running in a standard slice. For
example, at the highest load scenario, the median error for the un-
privileged process was about 16 microseconds, while the median
error for MAD was about 10 microseconds. We measured similar
results for MAD running both as a privileged user-mode process,
and as a kernel module. These results suggest that running packet
pair experiments on highly loaded hosts may be inherently prob-
lematic. It is unclear at this point whether the same problem exists
for probe methodologies that use short streams at ﬁxed rates, e.g.,
Pathload [21] or Yaz [41]. At the very least, these results warrant
further experimentation using a wider range of load scenarios, ex-
ploring both CPU and network-intensive workloads. Furthermore,
it may be appropriate to examine a kernel-based approach that com-
pletely bypasses the system IP/UDP layers. Unfortunately, such an
approach may require modifying the kernel.
In summary, our tests using PlanetLab in a controlled environ-
ment reveal similar measurement problems as we observed using
the live PlanetLab systems. Our tests also show that using MAD sig-
niﬁcantly improves the situation and yields delay and loss measure-
ments that accurately reﬂect the true state of the network. While our
experiments show that MAD offers modest improvement for packet-
pair experiments, additional study and improvements are needed.
5.2 Non-virtualized Host Experiments
In our second laboratory setup we used standard, unvirtualized
workstations. Environments such as RON [10] form an important
class of “raw system” testbeds for which MAD is designed. It is
therefore important to evaluate the performance of MAD in a similar
setting.
We used two workstations in our setup, similar to our other ex-
periments. Each workstation ran Linux kernel version 2.6.20. We
used identical machines as in our laboratory-based PlanetLab ex-
periments. Each one had a 2.0 GHz Pentium 4, 1 GB RAM, and
Intel Pro/1000 network interfaces (with interrupt coalescence dis-
abled). We again used a setup using Endace DAG 4.3 GE cards to
gather ground truth measurements, similar to our laboratory experi-
ments using PlanetLab and in our experiments using live PlanetLab
hosts.
For these tests, we again used Harpoon to create artiﬁcial CPU
and network load on the hosts and testbed to compare the per-
formance of an unprivileged user process for sending probes with
MAD. Table 9 shows the four workload scenarios we used. As with
the experiments using the controlled laboratory setup of PlanetLab,
we conﬁgured the threads for generating constant bit-rate UDP traf-
ﬁc in such a way as to consume a relatively large amount of pro-
cessor time.
9: Conﬁgurations and characteristics of laboratory experiments
with non-virtualized hosts.
Trafﬁc Volume Average CPU
(each direction)
Utilization
0
2 Mb/s
0%
2%
10 Mb/s
60%
50 Mb/s
99%
Harpoon conﬁguration
No Harpoon processes.
10 Harpoon processes, each
with 10 threads for produc-
ing self-similar TCP trafﬁc
and 1 thread for producing
constant bit-rate UDP traf-
ﬁc.
processes,
100 Harpoon
each with 10 threads for
producing self-similar TCP
trafﬁc, and 5 threads for
producing constant bit-rate
UDP trafﬁc.
300 Harpoon
processes,
each with 10 threads for
producing self-similar TCP
trafﬁc, and 20 threads for
producing constant bit-rate
UDP trafﬁc.
Using standard, unvirtualized hosts dramatically improves per-
formance for the unprivileged user-mode application for all three
measurement algorithms. In the case of round-trip delay, only the
99th percentile delay for the highest load scenario was above one
millisecond—at two milliseconds. All other delay quantiles were
below one millisecond. For packet loss, there was no loss mea-
sured in the lowest three load scenarios. However, in the highest
load scenario the loss frequency was 0.0053 and the mean duration
of loss episodes was 1.12 seconds.
For the experiments using MAD, the 99th percentile delay was
on the order of one hundred microseconds in all cases. There was
no loss measured by the MAD-based probes in any case.
For the packet pair experiments, the results were similar to those
in the PlanetLab-based laboratory experiments. Namely, while er-
rors in spacing of packet pairs upon sending, and errors in times-
tamping the packet pair upon send is relatively low with a zero
mean over a few tens of packet pairs, timestamp errors on receiv-
ing packet pairs grow larger with increased system load. Also, the
range in error values was similar to those in the controlled Planet-
Lab experiments. These results are true for both the unprivileged
user-mode measurement application and for MAD. These results
reinforce the hypothesis that running packet pair experiments on
highly loaded hosts may be a situation to avoid entirely. But again,
further experimentation and analysis is needed.
In summary, even with hosts that are not virtualized, measure-
ment inaccuracies can occur as system load becomes high. Our
experiments show that MAD is also effective in these environments,
eliminating spurious packet loss and yielding delay estimates that
match ground truth measurements.
5.3 Scalability of MAD
In our ﬁnal laboratory experiments, we examined the scalability
of MAD when subjected to a larger number of independent users of
MAD, and over a range of discrete time interval settings. In these
experiments, we used a standard (unvirtualized) Linux host run-
ning kernel version 2.6.20. The machine conﬁguration used was
the same in the other laboratory-based experiments, i.e., a 2 GHz
Pentium 4 with 1 GB RAM and an Intel Pro/1000 Gigabit Ethernet
interface. The duration of these experiments was two minutes.
We used up to 100 independent probe streams representing 100
users of MAD, each using a geometric probe process (Listing 2)
with a single packet per probe of 100 bytes. The probability pa-
rameter for sending a probe at a given time slot was 0.2 for each
probe stream. We also ran experiments using the BADABING code
fragment (Listing 3) with three packets per probe, sent back-to-
back. The results from those experiments were similar to the results
for the geometrically distributed probe consisting of one packet.
We also examined MAD using a range of discrete time interval set-
tings, from 10 milliseconds down to 100 microseconds. For each
experiment, we measured system and user time available from the
getrusage system call to derive a processor utilization ﬁgure for
the MAD process. We also compared this utilization ﬁgure to that
obtained using the standard top program. The results for each
measurement technique were consistent.
Table 10 shows CPU utilization results of running MAD with
100 independent probe streams over a range of discrete time inter-
vals. From the table, we see that for time intervals even as short as
500 microseconds, the overall utilization of MAD is minor, at about
0.2%. At intervals of 500 microseconds and larger, there were no
scheduler errors reported. At an interval of 100 microseconds, how-
ever, utilization rises sharply and is accompanied by scheduler er-
rors. With the default setting of 5 milliseconds (which is the same
interval used in our earlier BADABING study) and even shorter in-
tervals, MAD performs very well.
6. SUMMARY AND CONCLUSIONS
Widely deployed, shared network testbeds are critical to the net-
work research community. A particularly attractive class of experi-
ments that could be considered in these environments are those that
seek to measure end-to-end path properties such as delay and loss
using active probe tools. Unfortunately, the resource contention
overheads imposed in shared network testbeds can signiﬁcantly
10: Results from MAD scalability tests.
Interval
CPU Utilization
Scheduler Errors
(time slot misses)
10 milliseconds
5 milliseconds
1 millisecond
500 microseconds
100 microseconds
0.1%
0.1%
0.2%
0.2%
46.2%
0
0
0
0
8920
bias measurement results from active probe tools—a compelling
yet unfortunate example of the “tragedy of the commons” effect.
In this paper we present results of a measurement study that
quantiﬁes the bias effects on active probe-based measurements in
PlanetLab. Using hardware-based packet capture systems on our
local PlanetLab nodes, we ﬁnd that measurements of packet loss
and delay from active probes can be skewed signiﬁcantly.
These results motivate our development of MAD, a system for
conducting highly accurate active measurements in shared environ-
ments. MAD is realized as a simple programming language that is
made available to users via RPC’s. The language enables a variety
of active probe-base measurement streams to be scheduled in near
real-time through the use of priority scheduling mechanisms avail-
able in recent Linux kernels. MAD’S implementation as either a
kernel module or user-mode daemon enables it to be deployed with
minimal impact. Through a series of laboratory tests, we quantify
the extent to which MAD can reduce bias in active probe-based mea-
surements in both PlanetLab and in non-virtualized environments.
We show that mad can improve measurement accuracy by orders
of magnitude in PlanetLab with lesser but still valuable effects in
non-virtualized environments.
We plan to continue development of MAD in several ways. First,
we intend to complete the implementation of the security mecha-
nism that limits access of MAD to authorized users. Next, we plan
to expand MADcode to support of a broader set of active measure-
ment methods including those that are stream-based or adaptive.
Finally, we will consider how basic MAD transmitter/receiver/re-
ﬂector functionality might be ported to other OS environments so
that highly accurate measurement capability might be more widely
deployed.
Acknowledgments
We thank our shepherd, Morley Mao, and the anonymous IMC re-
viewers for their input. We also thank Mike Blodgett for his as-
sistance with the DAG systems. This work was supported in part
by NSF grants CNS-0347252, CNS-0646256, CNS-0627102, and
by Cisco Systems. Any opinions, ﬁndings, conclusions, or recom-
mendations expressed in this material are those of the authors and
do not necessarily reﬂect the views of the NSF or Cisco Systems.
7. REFERENCES
[1] EverLab: Next Generation PlanetLab Network.
http://www.everlab.org.
[2] KURT: Kansas University Real-time Linux.
http://www.ittc.ku.edu/kurt/.
[3] MyPLC—A complete Planetlab Central (PLC) portable
installation.
http://www.planet-lab.org/doc/myplc.
[4] OneLab. http://www.fp6-ist-onelab.eu/.
[5] A new approach to kernel timers.
http://lwn.net/Articles/152436/, September
2005.
[6] Linux kernel gains new real-time support.
http://www.linuxdevices.com/news/-
NS9566944929.html, October
2006.
[7] NSF CISE, GENI — Global Environment for Network
Innovations . http://www.geni.net, 2007.
[27] A. Pásztor and D. Veitch. A precision infrastructure for
active probing. In Proceedings of Passive and Active
Measurement Workshop, Amsterdam, Netherlands, 2001.
[28] A. Pásztor and D. Veitch. PC-based Precision Timing
without GPS. In Proceedings of ACM SIGMETRICS, Marina
Del Rey, CA, June 2002.
[8] G. Almes, S. Kalidindi, and M. Zekauskas. A one-way delay
[29] V. Paxson, A. Adams, and M. Mathis. Experiences with
metric for IPPM. IETF RFC 2679, September 1999.
[9] G. Almes, S. Kalidindi, and M. Zekauskas. A one way packet
NIMI. In Proceedings of Passive and Active Measurement
Workshop, 2000.
loss metric for IPPM. IETF RFC 2680, September 1999.
[30] L. Peterson, S. Shenker, and J. Turner. Overcoming the
[10] D. Andersen, H. Balakrishnan, F. Kaashoek, and R. Morris.
Resilient overlay networks. In Proceedings of ACM
Symposium on Operating Systems Principles, Banff, Alberta,
Canada, 2001.
[11] M. Aron and P. Druschel. Soft Timers: Efﬁcient Microsecond
Software Timer Support for Network Processing. ACM
Transactions on Computer Systems, August 2000.
[12] S. Banerjee, T. Grifﬁn, and M. Pias. The interdomain
connectivity of PlanetLab nodes. In Proceedings of Passive
and Active Measurement Workshop, Antibes Juan-les-Pins,
France, April 2004.
[13] P. Barham, B. Dragovic, K. Fraser, S. Hand, T. Harris,
A. Ho, R. Heugebauer, I. Pratt, and A. Warﬁeld. Xen and the
Art of Virtualization. In Proceedings of ACM Symposium on
Operating Systems Principles, October 2003.
[14] A. Bavier, M. Bowman, B. Chun, D. Culler, S. Karlin,
S. Muir, L. Peterson, T. Roscoe, T. Spalink, and
M. Wawrzoniak. Operating System Support for
Planetary-Scale Network Services. In USENIX Symposium
on Networked Systems Design and Implementation, March
2004.
[15] B. Bershad, S. Savage, P. Pardyak, E. Sirer, M. Fiuczynski,
D. Becker, S. Eggers, and C. Chambers. Extensibility, safety
and performance in the SPIN operating system. In
Proceedings of ACM Symposium on Operating Systems
Principles, Copper Mountain Resort, CO, December 1995.
[16] J. Bolot. End-to-end packet delay and loss behavior in the
internet. In Proceedings of ACM SIGCOMM, San Francisco,
CA, September 1993.
[17] R. Carter and M. Crovella. Measuring bottleneck link speed
in packet-switched networks. Performance Evaluation
Review, 27-28:297–318, October 1996.
[18] National Research Council, editor. Looking Over the Fences
at Networks: A Neighbor’s View of Networking Research.
National Academy Press, 2001.
[19] D. Engler and M. Kaashoek. Exokernel: an operating system
architecture for application-level resource management. In
Proceedings of ACM Symposium on Operating Systems
Principles, Copper Mountain Resort, CO, December 1995.
[20] V. Jacobson. Congestion avoidance and control. In
Proceedings of ACM SIGCOMM, Stanford, CA, 1988.
[21] M. Jain and C. Dovrolis. End-to-end available bandwidth:
Measurement methodology, dynamics, and relation to TCP
throughput. In Proceedings of ACM SIGCOMM, Pittsburgh,
PA, August 2002.
[22] S. Kalidindi and M. Zekauskas. Surveyor: An Infrastructure
for Internet Performance Measurements. In Proceedings of
INET ’99, 1999.
[23] R. Kapoor, L.-J. Chen, L. Lao, M. Gerla, and M. Y. Sanadidi.
CapProbe: a simple and accurate capacity estimation
technique. In Proceedings of ACM SIGCOMM, Portland,
OR, August 2004.
[24] K. Lai and M. Baker. Measuring link bandwidths using a
deterministic model of packet delay. In Proceedings of ACM
SIGCOMM, Stockholm, Sweden, 2000 2000.
[25] J. Liu and M. Crovella. Using loss pairs to discover network
properties. In Proceedings of ACM Internet Measurement
Workshop, San Francisco, CA, October 2001.
[26] K. Park and V. Pai. CoMon—A Monitoring Infrastructure for
PlanetLab. http://comon.cs.princeton.edu/.
Internet Impasse through Virtualization. In Proceedings of
ACM SIGCOMM HotNets–III, 2004.
[31] H. Pucha, Y.C. Hu, and Z.M. Mao. On the Impact of
Research Network based Testbeds on Wide-Area
Experiments. In Proceedings of ACM Internet Measurement
Conference, Rio de Janeiro, Brazil, October 2006.
[32] V. Ribeiro, R. Riedi, R. Baraniuk, J. Navratil, and L. Cottrell.
pathChirp: Efﬁcent Available Bandwidth Estimation for
Network Paths. In Proceedings of Passive and Active
Measurement Workshop, April 2003.
[33] M. Rosenblum and T. Garﬁnkel. Virtual Machine Monitors:
Current Technology and Future Trends. IEEE Computer,
May 2005.
[34] J. H. Saltzer, D. P. Reed, and D. D. Clark. End-to-end
arguments in system design. ACM Transactions on Computer
Systems, 2(4):277–288, November 1984.
[35] E. Sarmiento. Securing FreeBSD using Jail. Sys Admin,
10(5):31–37, May 2001.
[36] S. Soltesz, H. Pötzl, M. Fiuczynski, A. Bavier, and
L. Peterson. Container-based Operating System
Virtualization: A Scalable, High-performance Alternative to
Hypervisors. In Proceedings of EuroSYS, 2007.
[37] J. Sommers and P. Barford. Self-conﬁguring network trafﬁc
generation. In Proceedings of ACM Internet Measurement
Conference, Taormina, Sicily, Italy, October 2004.
[38] J. Sommers, P. Barford, N. Dufﬁeld, and A. Ron. Improving
Accuracy in End-to-end Packet Loss Measurement. In
Proceedings of ACM SIGCOMM, Philadelphia, PA, August
2005.
[39] J. Sommers, P. Barford, N. Dufﬁeld, and A. Ron. A
Framework for Multi-objective SLA Compliance
Monitoring. In Proceedings of IEEE INFOCOM
(minisymposium), Anchorage, AK, May 2007.
[40] J. Sommers, P. Barford, N. Dufﬁeld, and A. Ron. Accurate
and Efﬁcient SLA Compliance Monitoring. In To appear,
Proceedings of ACM SIGCOMM, Kyoto, Japan, August
2007.
[41] J. Sommers, P. Barford, and W. Willinger. A proposed
framework for calibration of available bandwidth estimation
tools. In Proceedings of IEEE Symposium on Computer and
Communication, Pula, Sardinia, Italy, June 2006.
[42] N. Spring, L. Peterson, A. Bavier, and V. Pai. Using
PlanetLab for Network Research: Myths, Realities, and Best
Practices. In Proceedings of the Second USENIX Workshop
on Real, Large Distributed Systems (WORLDS ’05), San
Francisco, CA, December 2005.
[43] N. Spring, D. Wetherall, and T. Anderson. Scriptroute: A
Public Internet Measurement Facility . In Proceedings of
USENIX Symposium on Internet Technologies and Systems
(USITS), 2003.
[44] J. Strauss, D. Katabi, and F. Kaashoek. A measurement study
of available bandwidth estimation tools. In Proceedings of
ACM Internet Measurement Conference, Miami, FL, October
2003.
[45] Y. Zhang, R. West, and X. Qi. A virtual deadline scheduler
for window-constrained service guarantees. In Proceedings
of the 25th IEEE Real-time Systems Symposium (RTSS),
December 2004.