e
p
m
a
S
t
0.1
u
p
n
0
I
f
o
e
−0.1
l
i
t
n
a
u
Q
−0.2
−2
−1
0
1
Standard Normal Quantile
(a) Vivaldi Case
2
3
4
−0.3
−4
−3
n
o
i
t
c
n
u
F
n
o
i
t
u
b
i
r
t
s
D
e
v
i
t
i
3
4
−2
−1
0
1
Standard Normal Quantile
2
(b) NPS case
l
a
u
m
u
C
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0
0.1
0.2
Simulations, Vivaldi
Simulations, NPS
PlanetLab, Vivaldi
PlanetLab, NPS
0.3
0.4
Prediction Error
0.5
0.6
0.7
Figure 3: CDF of prediction errors.
To better understand their nature, we show in table 1 the repartition
of prediction errors in error intervals. The table shows the number
of nodes with prediction errors in this interval, the number of occur-
rences of the smallest prediction error observed in the interval, and
the number of occurrences of the largest prediction errors observed
in the interval (e.g. Number of node(s)/number of observed min
error/number of observed max error). We see that only a few nodes
contribute (sometimes very many) large prediction errors. Looking
further, we identiﬁed 3 nodes, all located in India, who contributed
consistently to the “tail” of the CDF in ﬁgure 3. It is interesting to
note that these nodes exhibited large (> 0.75) average measured
relative errors during embedding, and were clearly having trouble
with the embedding process itself, due to adverse network condi-
tions.
Table 1: Prediction Error Histogram
(NPS)
(Vivaldi)
Error Interval
0.0-0.05
0.05-0.1
0.1-0.15
0.15-0.2
0.4-0.45
0.45-0.5
0.5-0.55
0.6-0.65
257/830/922
32/201/995
5/3/992
1/997/997
4/3/56
1/12/12
1/985/985
-
232/854/943
44/180/941
18/5/229
2/12/884
3/5/12
2/1/17
2/32/40
1/851/851
Figure 1: Quantile-Quantile plot of 2 innovation processes.
Quantile (QQ) plots of 2 innovation processes (for both Vivaldi and
NPS) taken on PlanetLab nodes running their own Kalman ﬁlter.
These plots, typical of observations on all nodes, show that each of
these distributions indeed closely follows a Gaussian distribution.
3.2 Effective Behavior Representation
From section 2, it is clear that the Kalman ﬁlter model attempts
to represent the behavior of the embedding process by capturing
the dynamics of the system through its convergence behavior (by
tracking of relative errors over time). In this section, we therefore
assess the representational power of this approach by having each
node calibrate its own Kalman ﬁlter from the measurements it ob-
served during the embedding of its own coordinates, in a cheat-free
regime. Then, once the model has converged at every node (i.e.
the EM method has converged and the variations of all the θ com-
ponents become smaller than 0.02), a new embedding process is
started (i.e. the nodes forget their coordinates and rejoin the sys-
tem). During this second embedding process, the prediction error,
that is the absolute value between the error predicted by the node’s
Kalman ﬁlter and the measured actual error, is measured.
Figure 2 shows a typical evolution of actual (measured) relative
errors and predicted errors for a node on PlanetLab (for Vivaldi,
but similar behavior was observed for NPS). The two curves of the
top graph of the ﬁgure are so similar that they are almost indistin-
guishable. The bottom graph of the ﬁgure represents the prediction
error which is the difference between these two curves (note the
different scale used for this graph). We see that the prediction er-
rors are small which shows that a node’s calibrated Kalman ﬁlter
can capture effectively the node’s behavior “in the wild”.
Figure 3 shows the cumulative distribution function (CDF) of all
the prediction errors observed across all nodes in the system. This
conﬁrms that the vast majority of predictions are indeed excellent.
This demonstrates the power and generality of the model in cap-
turing the dynamics of the system and its adaptability to current
system conditions.
However, there are a few “outlier” predictions with large errors.
3.3 Representativeness of Surveyors
If a subset of nodes in the system (called Surveyor nodes) are
trusted and use each other exclusively to compute their own coor-
dinates, they will be immune to the effects of malicious behavior
during embedding. The premise of our work is that the “clean”
(normal) system behavior thus learnt can then be shared with other
nodes and used by these nodes to detect malicious behavior they
may be subjected to by other nodes in the system. This obviously
assumes that the behavior of the system as observed by Surveyors
can approximate or represent well enough the normal behavior of
the system as observed by other nodes in the absence of malicious
behavior. In the following validation of this assumption, Surveyors
are chosen at random in the node population.
Note that a random choice will give an upper bound on the num-
ber of Surveyors needed. Indeed, intuitively, Surveyors should be
roughly uniformly distributed in the system to be representative of
most other nodes. However, choosing nodes at random in the sys-
tem does not yield a uniform distribution of Surveyors (i.e. “Sur-
veyor clusters” appear) and therefore not every new Surveyor in-
creases representativeness. Consequently, more Surveyors must be
chosen in order to achieve a good coverage of the system, than
if they were placed more strategically. Nevertheless, the random
choice method does provide general results, without the need to ad-
dress the question of optimal Surveyor deployment strategy (which
we leave as future work).
One of the ﬁrst questions to answer is how many Surveyors are
needed to be representative of the rest of the population. Noting
that our model is based on measured relative errors and that each
node in the system observes a series (i.e. distribution) of such er-
rors, we characterize the system-wide relative error behavior as the
CDF of the 95th percentiles of the relative errors observed at each
the distribution is made up of the 95th per-
(normal) node (i.e.
centile value observed at every node). We then compare this CDF
with those of the 95th percentiles of the relative errors observed
across a varying population of Surveyors. The choice of the 95th
percentile is so that outliers, as observed in section 3.2, do not skew
the results, while preserving a high degree of generality. Figure 4,
n
o
i
t
c
n
u
F
n
o
i
t
u
b
i
r
t
s
D
e
v
i
t
a
u
m
u
C
i
l
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0
95th percentile of normal nodes
95th percentile of random Surveyor nodes: 10% of overall population
95th percentile of randomSurveyor nodes: 8% of overall population
95th percentile of random Surveyor nodes: 5% of overall population
95th percentile of random Surveyor nodes: 1% of overall population
95th percentile of Cluster−Head Surveyor nodes: 1% of overall population
0.1
0.2
0.3
0.4
0.5
Relative Error
0.6
0.7
0.8
0.9
1
Figure 4: Impact of Surveyor population size on repretentative-
ness.
obtained by simulations of Vivaldi, indicates that a population of
Surveyors of about 8% of the overall population is closely rep-
resentative of this overall population (because the CDF for these
populations in the ﬁgure are similar).
To generalize this result, we then repeated the experiment, using
both simulations and PlanetLab measurements, on a Vivaldi system
with 8% of Surveyors. We again chose to represent the system
by the distribution of the 95th percentile of the measured relative
errors (ﬁgure 5).
n
o
i
t
c
n
u
F
n
o
i
t
i
u
b
i
r
t
s
D
e
v
i
t
l
a
u
m
u
C
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0
95th percentile of normal nodes: King Data Set
95th percentile of normal nodes: PlanetLab
95th percentile of Surveyor nodes: King Data Set
95th percentile of Surveyor nodes: PlanetLab
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Relative Error
Figure 5: Representativeness with 8% Surveyor nodes.
These experiments conﬁrm that less than 10% of randomly cho-
sen Surveyor nodes is enough to gain a good representation of the
system behavior. Similar results were observed for NPS. We note
that 8% to 10% of the overall node population is a very strin-
gent requirement for most practical purposes and can represent a
huge number of nodes. However, as already pointed out above,
random Surveyor deployment is not optimal and this value is an
upper bound on the number of Surveyors needed. To gain further
insight into how conservative this upper bound may be, we tried
a simple k-means clustering algorithm for Surveyor deployment.
Figure 4 shows that when taking cluster heads as Surveyors, good
representativeness can be achieved with roughly 1% of Surveyors.
Although this does not give much indication as to what the lower
bound on the number of Surveyors needed is in the case of op-
timal Surveyor deployment, it nevertheless shows that simple de-