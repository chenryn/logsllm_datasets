a random order is used.
In the example shown in Figure 3, when assembling the
piece of statements belonging to the user-deﬁned function
main (i.e., lines 13, 15, 18, and 19) and the assembled piece of
statements belonging to user-deﬁned function test (i.e., lines
2, 4, 5, and 9), we obtain 13 → 15 → 18 → 19 → 2 → 4 →
5 → 9, which is a code gadget corresponding to the library
function call strcpy. This code gadget preserves the order of
user-deﬁned functions that are contained in the program slice
corresponding to the argument str.
2) Step II.2: Labeling the ground truth: Each code gadget
needs to be labeled as “1” (i.e., vulnerable) and “0” (i.e., not
vulnerable). If a code gadget corresponds to a vulnerability that
is known in the training dataset, it is labeled as “1”; otherwise,
it is labeled as “0”. In Section IV-C, we will discuss the
labeling of ground truth in details, when dealing with programs
related to speciﬁc vulnerability data sources.
E. Step III: Transforming code gadgets into vectors
1) Step III.1: Transforming code gadgets into their sym-
bolic representations: This step aims to heuristically capture
some semantic information in the programs for training a
neural network. First, remove the non-ASCII characters and
comments because they have nothing to do with vulnerability.
7
Second, map user-deﬁned variables to symbolic names (e.g.,
“VAR1”, “VAR2”) in the one-to-one fashion, while noting that
multiple variables may be mapped to the same symbolic name
when they appear in different code gadgets. Third, map user-
deﬁned functions to symbolic names (e.g., “FUN1”, “FUN2”)
in the one-to-one fashion, while noting that multiple functions
may be mapped to the same symbolic name when they appear
in different code gadgets.
Figure 4.
symbolic representations
Illustration of Step III.1: transforming code gadgets into their
Figure 4 highlights the above process by using the code
fragment generated by Step II.1 as shown in Figure 3.
2) Step III.2: Encoding the symbolic representations into
vectors: Each code gadget needs to be encoded into a vector
via its symbolic representation. For this purpose, we divide
a code gadget in the symbolic representation into a sequence
of tokens via lexical analysis, including identiﬁers, keywords,
operators, and symbols. For example, a code gadget in the
symbolic representation,
“strcpy(VAR5, VAR2); ”
is represented by a sequence of 7 tokens:
“strcpy”, “(”, “VAR5”, “,”, “VAR2”, “)”, and “;”.
This leads to a large corpus of tokens. In order to transform
these tokens into vectors, we use the word2vec tool [14], which
is selected because it is widely used in text mining [58]. This
tool is based on the idea of distributed representation, which
maps a token to an integer that is then converted to a ﬁxed-
length vector [43].
Since code gadgets may have different numbers of tokens,
the corresponding vectors may have different lengths. Since
BLSTM takes equal-length vectors as input, we need to make
an adjustment. For this purpose, we introduce a parameter τ
as the ﬁxed length of vectors corresponding to code gadgets.
• When a vector is shorter than τ, there are two cases: if
the code gadget is generated from a backward slice or
generated by combining multiple backward slices, we
pad zeros in the beginning of the vector; otherwise,
we pad zeros to the end of the vector.
• When a vector is longer than τ, there are also two
is generated from one
if the code gadget
cases:
13  main(int argc, char **argv)15  char *userstr;18  userstr = argv[1];19  test(userstr);2    test(char *str)4    int MAXSIZE=40;5    char buf[MAXSIZE];9   strcpy(buf, str); /*string copy*/13  main(int argc, char **argv)15  char *VAR1;18  VAR1 = argv[1];19  FUN1(VAR1);2    FUN1(char *VAR2)4    int VAR3=40;5    char VAR4[VAR3];9   strcpy(VAR5, VAR2); 13  main(int argc, char **argv)15  char *VAR1;18  VAR1 = argv[1];19  test(VAR1);2    test(char *VAR2)4    int VAR3=40;5    char VAR4[VAR3];9   strcpy(VAR5, VAR2); Input: code gadget (from Step II.1)13  main(int argc, char **argv)15  char *userstr;18  userstr = argv[1];19  test(userstr);2    test(char *str)4    int MAXSIZE=40;5    char buf[MAXSIZE];9   strcpy(buf, str); (1) Remove non-ASCII characters and comments(2) Map user-defined variables(3) Map user-defined functionsbackward slice, or generated by combining multiple
backward slices, we delete the beginning part of the
vector; otherwise, we delete the ending part of the
vector.
This ensures that the last statement of every code gadget
generated from a backward slice is a library/API function call,
and that the ﬁrst statement of every code gadget generated
from a forward slice is a library/API function call. As a result,
every code gadget is represented as a τ-bit vector. The length
of vectors is related to the number of hidden nodes at each
layer of the BLSTM, which is a parameter that can be tuned
to improve the accuracy of vulnerability detection (see Section
IV-C).
IV. EXPERIMENTS AND RESULTS
Our experiments are centered at answering the following
three research questions (RQs):
•
•
•
RQ1: Can VulDeePecker deal with multiple types of
vulnerabilities at the same time?
A vulnerability detection system should be able to
detect multiple types of vulnerabilities at the same
time, because multiple detection systems need to be
maintained otherwise. For answering this question, we
will conduct experiments involving one or multiple
types of vulnerabilities.
RQ2: Can human expertise (other than deﬁning fea-
tures) improve the effectiveness of VulDeePecker?
For answering this question, we will investigate the
effectiveness of using some manually-selected li-
brary/API function calls vs. the effectiveness of using
all of the library/API function calls.
RQ3: How effective is VulDeePecker when compared
with other vulnerability detection approaches?
For answering this question, we will compare
VulDeePecker with other approaches, including some
static analysis tools and code similarity-based vulner-
ability detection systems.
A. Metrics for evaluating vulnerability detection systems
We use the widely used metrics false positive rate (F P R),
false negative rate (F N R), true positive rate or recall (T P R),
precision (P ), and F 1-measure (F 1) to evaluate vulnerabil-
ity detection systems [39]. Let TP be the number of samples
with vulnerabilities detected correctly, FP be the number of
samples with false vulnerabilities detected, FN be the number
of samples with true vulnerabilities undetected, and TN be
the number of samples with no vulnerabilities undetected.
The false positive rate metric F P R = FP
FP+TN measures the
ratio of false positive vulnerabilities to the entire population
of samples that are not vulnerable. The false negative rate
metric F N R = FN
TP+FN measures the ratio of false negative
vulnerabilities to the entire population of samples that are
vulnerable. The true positive rate or recall metric T P R =
TP+FN measures the ratio of true positive vulnerabilities to the
entire population of samples that are vulnerable, while noting
that T P R = 1 − F N R. The precision metric P = TP
TP+FP
measures the correctness of the detected vulnerabilities. The
TP
F 1-measure metric F 1 = 2·P·T P R
both precision and true positive rate.
P +T P R takes consideration of
It would be ideal that a vulnerability detection system
neither misses vulnerabilities (i.e., F N R ≈ 0 and T P R ≈ 1)
nor triggers false alarms (i.e., F P R ≈ 0 and P ≈ 1), which
means F 1 ≈ 1. However, this is difﬁcult to achieve in practice,
and often forces one to trade the effectiveness in terms of one
metric for the effectiveness in terms of another metric. In this
study, we prefer to achieving low FNR and low FPR.
B. Preparing input to VulDeePecker
Collecting programs. There are two widely used sources of
vulnerability data maintained by the NIST: the NVD [10]
which contains vulnerabilities in production software, and
the SARD project [12] which contains production, synthetic,
and academic security ﬂaws or vulnerabilities. In the NVD,
each vulnerability has a unique Common Vulnerabilities and
Exposures IDentiﬁer (CVE ID) and a Common Weakness
Enumeration IDentiﬁer (CWE ID) that indicates the type of
the vulnerability in question. We collect the programs that
contain one or multiple vulnerabilities. In the SARD, each
program (i.e., test case) corresponds to one or multiple CWE
IDs because a program can have multiple types of vulnerabil-
ities. Therefore, programs with one or multiple CWE IDs are
collected.
In the present paper, we focus on two types of vulnerabil-
ities: buffer error (i.e., CWE-119) and resource management
error (i.e., CWE-399), each of which has multiple subtypes.
These vulnerabilities are very common, meaning that we can
collect enough data for using deep learning. We select 19
popular C/C++ open source products, including the Linux
kernel, Firefox, Thunderbird, Seamonkey, Firefox esr, Thun-
derbird esr, Wireshark, FFmpeg, Apache Http Server, Xen,
OpenSSL, Qemu, Libav, Asterisk, Cups, Freetype, Gnutls,
Libvirt, and VLC media player, which contain, according to
the NVD, these two types of vulnerabilities. We also collect
the C/C++ programs in the SARD that contain these two
types of vulnerabilities. In total, we collect from the NVD
520 open source software programs related to buffer error
vulnerabilities and 320 open source software programs related
to resource management error vulnerabilities; we also collect
from the SARD 8,122 programs (i.e., test cases) related to
buffer error vulnerabilities and 1,729 programs related to
resource management error vulnerabilities. Note that program
containing a vulnerability may actually consist of multiple
program ﬁles.
Training programs vs.
target programs. We randomly
choose 80% of the programs we collect as training programs
and the remaining 20% as target programs. This ratio is applied
equally when dealing with one or both types of vulnerabilities.
C. Learning BLSTM neural networks
This corresponds to the learning phase of VulDeePecker.
We implement the BLSTM neural network in Python using
Theano [24] together with Keras [8]. We run experiments on
a machine with NVIDIA GeForce GTX 1080 GPU and Intel
Xeon E5-1620 CPU operating at 3.50GHz.
Step I: Extracting library/API function calls and cor-
responding program slices. We extract C/C++ library/API
8
function calls from the programs. There are 6,045 C/C++
library/API function calls that involve standard library function
calls [1], basic Windows API and Linux kernel API function
calls [9], [13]. In total, we extract 56,902 library/API function
calls from the programs, including 7,255 forward function calls
and 49,647 backward function calls.
In order to answer the RQs, we also manually select 124
C/C++ library/API function calls (including function calls with
wildcard) related to buffer error vulnerabilities (CWE-119)
and 16 C/C++ library/API function calls related to resource
management error vulnerabilities (CWE-399). These function
calls are selected because the aforementioned commercial
tool Checkmarx [2] claims, using their own rules written by
human experts, that they are related to these two types of
vulnerabilities. The list of these function calls are deferred
to Table VII in the Appendix B. Correspondingly, we extract
40,351 library/API function calls from the training programs,
including 4,012 forward function calls and 36,339 backward
function calls. For each argument of the library/API function
calls, one or multiple program slices are extracted.
Step II.1: Generating code gadgets. Code gadgets are gener-
ated from program slices. We obtain a Code Gadget Database
(CGD) of 61,638 code gadgets, among which 48,744 code
gadgets are generated from program slices of training pro-
grams, and 12,894 code gadgets are generated from program
slices of target programs. The time complexity for generating
gadgets mainly depends on the data ﬂow analysis tool. For
example, it takes 883 seconds to generate 2,494 code gadgets
from 100 programs (99,232 lines) that are randomly selected
the SARD, meaning an average of 354 milliseconds per code
gadget. For answering the RQs mentioned above, we use the
CGD to derive the following 6 datasets.
•
•
•
•
•
•
BE-ALL: The subset of CGD corresponding to Buffer
Error vulnerabilities (CWE-119) and ALL library/API
function calls (i.e., extracted without human expert).
RM-ALL: The subset of CGD corresponding to Re-
source Management error vulnerabilities (CWE-399)
and ALL library/API function calls.
HY-ALL: The subset of CGD corresponding to the
HYbrid of
(i.e., both) buffer error vulnerabilities
(CWE-119) and resource management error vulner-
abilities (CWE-399) and ALL library/API function
calls. That is, it is the same as the CGD.
BE-SEL: The subset of CGD corresponding to Buffer
Error vulnerabilities (CWE-119) and manually SE-
Lected function calls (rather than all function calls).
RM-SEL: The subset of CGD corresponding to Re-
source Management error vulnerabilities (CWE-399)
and manually SELected function calls.
HY-SEL: The subset of CGD corresponding to the
HYbrid of buffer error vulnerabilities (CWE-119) and
resource management error vulnerabilities (CWE-399)
and manually SELected function calls.
Table I summarizes the number of code gadgets in these
datasets.
Step II.2: Labeling code gadgets. Code gadgets are labeled
as follows. For the code gadgets extracted from the programs
9
Table I.
DATASETS FOR ANSWERING THE RQS
Dataset
BE-ALL
RM-ALL
HY-ALL
BE-SEL
RM-SEL
HY-SEL
#Code
gadgets
39,753
21,885
61,638
26,720
16,198
42,918
#Vulnerable
code gadgets
10,440
7,285
17,725
8,119
6,573
14,692
#Not vulnerable
code gadgets
29,313
14,600
43,913
18,601
9,625
28,226
of the NVD, we focus on the vulnerabilities whose patches
involve line deletions or modiﬁcations. This process has two
steps. In the ﬁrst step, a code gadget is automatically labeled
as “1” (i.e., vulnerable) if it contains at least one statement that
is deleted or modiﬁed according to the patch, and labeled as
“0” otherwise (i.e., not vulnerable). However, this automatic
process may mislabel some code gadgets, which are not
vulnerable, as “1”. In order to remove these mislabels, the
second step is to manually check the code gadgets that are
labeled as “1” so as to correct the mislabels (if any).
Recall that each program in the SARD is already labeled
as good (i.e., no security defect), bad (i.e., containing security
defects), or mixed (i.e., containing functions with security
defects and their patched versions) with corresponding CWE
IDs. For the code gadgets extracted from the programs with
respect to the SARD, a code gadget extracted from a good
program is labeled as “0” (i.e., not vulnerable), and a code
gadget extracted from a bad or mixed program is labeled as “1”
(i.e., vulnerable) if it contains at least one vulnerable statement
and “0” otherwise. Since we used heuristics in the labeling
process for the SARD program, we looked at the labels of
1,000 random code gadgets and found that only 6 of them (i.e.
0.6%) were mislabeled. These mislabeled samples are caused
by the fact that a statement in a piece of code that is not
vulnerable is the same as a statement in a piece of code that
is vulnerable. As the mislabeled code gadgets are very few
and the neural networks are robust against a small portion of
mislabeled samples, it is unnecessary to check manually all
labels of the code gadgets that are extracted for the SARD
programs.
It is possible to encounter the situation that the same code
gadget is labeled with both “1” and “0” (i.e., conﬂicting labels).
One cause of this phenomenon is the imperfection of the data
ﬂow analysis tool. In this case, we simply delete these code
gadgets. As a result, 17,725 code gadgets are labeled as “1”
and 43,913 code gadgets are labeled as “0”. Among the 17,725
code gadgets labeled as “1”, 10,440 code gadgets correspond
to the buffer error vulnerabilities and 7,285 code gadgets
correspond to the resource management error vulnerabilities.
Table I shows the number of code gadgets that are vulnerable
(Column 3) and the number of code gadgets that are not
vulnerable in each dataset (Column 4).
Step III: Transforming code gadgets into vectors. The CGD
contains a total number of 6,166,401 tokens, of which 23,464