tively during manually incident investigations (since in this
case, the analyst, as a domain expert, can provide additional
ﬁltering of false positives). Similarly, latency might not be a
critical metric in a retrospective use case (e.g., post-discovery
breach investigation). However, if an organization is looking
for a TI source where the IPs are intended to be added to a
ﬁrewall’s blacklist then accuracy and latency should likely
be weighted over coverage, assuming that blocking benign
activity is more costly.
Another common real-world scenario is that a company
has a limited budget to purchase TI sources and has a speciﬁc
set of threats (i.e., botnet, brute-force) they are focused on
mitigating. In such cases, the metrics we have described can
be used directly in evaluating TI options, biasing twoards
sources that maximize coverage of the most relevant threats
while limiting intersection.
7.2 Data Labeling
Threat intelligence IP data carries different meanings. To
properly use this data, it is critical to know what the indicators
actually mean: whether they are Internet scanners, members
of a botnet or malicious actors who had attacked other places
before. We have attempted to group feeds by their intended
meaning in our analysis.
However, this category information, which primarily comes
from TI sources themselves, is not always available. Feeds
such as Alienvault IP Reputation and Facebook Threat Ex-
change sources contain a signiﬁcant number of indicators
labeled “Malicious” or “Suspicious.” The meanings of these
indicators are unclear, making it difﬁcult for consumers to
decide how to use the data and the possible consequences.
For feeds that provide category information, it is sometimes
too broad to be meaningful. For example, multiple feeds in
our collection simply label their indicators as “Scanner.” Net-
work scanning can represent port scanning (by sending SYN
packets), or a vulnerability scan (by probing host for known
vulnerabilities). The ambiguity here, as a result of ad-hoc data
labeling, again poses challenges for security experts when
using TI data.
Recently, standard TI formats have been proposed and de-
veloped, notably IODEF [19], CybOX [13] and STIX [37],
that try to standardize the threat intelligence presentation and
sharing. But these standards focus largely on the data for-
mat. There is room to improve these standards by designing a
standard semantics for threat intelligence data.
7.3 Limitations
There are several questions that our study does not address.
We attempted to collect data from a diverse set of sources,
including public feeds, commercial feeds and industrial ex-
change feeds, but it is inherently not comprehensive. There are
some prohibitively expensive or publication-restricted data
sources that are not available to us. More specialized measure-
ment work should be done in the future to further analyze the
performance of these expensive and exclusive data sources.
A second limitation is our visibility into how different com-
panies use threat intelligence operationally. For a company,
perhaps the most useful kind of metric measures how a threat
intelligence source affects its main performance indicators
as well as its exposure to risk. Such metrics would require
a deep integration into security workﬂows at enterprises to
measure the operation effect of decisions made using threat
intelligence. This would allow CIOs and CSOs to better un-
derstand exactly what a particular threat intelligence product
contributes to a company. As researchers, we do not use TI
operationally. A better understanding of operational needs
would help reﬁne our metrics to maximize their utility for
operations-driven consumers.
The third limitation is the lack of ground truth, a limita-
tion shared by all the similar measurement work. It is simply
very difﬁcult to obtain the full picture of a certain category
of threat, making it very challenging to precisely determine
accuracy and coverage of feeds. In this study, we used data
from an Internet telescope and VirusTotal as a close approx-
imation. There are also a handful of cases where a security
incident has been comprehensively studied by researchers,
such as the Mirai study [4], and such efforts can be used to
evaluate certain types of TI data. But such studies are few in
number. One alternative is to try to establish the ground truth
for a speciﬁc network. For example, a company can record
all the network trafﬁc going in and out of its own network,
and identify security incidents either through its IDS system
or manual forensic analysis. Then it can evaluate the accu-
racy and coverage of a TI feed under the context of its own
network. This can provide a customized view of TI feeds.
8 Related Work
Several studies have examined the effectiveness of blacklist-
based threat intelligence [23, 31, 32, 35, 36]. Ramachan-
dran et al. [32] showed that spam blacklists are both incom-
plete (missing 35% of the source IPs of spam emails captured
in two spam traps), and slow in responding (20% of the spam-
mers remain unlisted after 30 days). Sinha et al. [36] further
conﬁrmed this result by showing that four major spam black-
lists have very high false negative rates, and analyzed the
possible causes of the low coverage. Sheng et al. [35] stud-
ied the effectiveness of phishing blacklists, showing the lists
are slow in reacting to highly transient phishing campaigns.
These studies focused on speciﬁc types of threat intelligence
sources, and only evaluated their operational performance
rather than producing empirical evaluation metrics for threat
intelligence data sources.
Other studies have analyzed the general attributes of threat
intelligence data. Pitsillidis et al. [30] studied the characteris-
tics of spam domain feeds, showing different perspectives of
spam feeds, and demonstrated that different feeds are suitable
for answering different questions. Thomas et al. [42] con-
structed their own threat intelligence by aggregating the abuse
trafﬁc received from six Google services, showing a lack of
intersection and correlation among these different sources.
While focusing on broader threat intelligence uses, these stud-
ies did not focus on generalizable threat metrics that can be
extended beyond the work.
Little work exists that deﬁnes a general measurement
methodology to examine threat intelligence across a broad
set of types and categories. Metcalf et al. [26] collected and
measured IP and domain blacklists from multiple sources,
but only focused on volume and intersection analysis. In con-
trast, we formally deﬁne a set of threat intelligence metrics
and conduct a broad and comprehensive study over a rich
variety of threat intelligence data. We conducted our measure-
ment from the perspective of consumers of TI data to offer
guidance on choosing between different sources. Our study
also demonstrated the limitation of threat intelligence more
thoroughly, providing comprehensive characteristics of cyber
threat intelligence that no work had addressed previously.
9 Conclusion
This paper has focused on the simplest, yet fundamental, met-
rics about threat intelligence data. Using the proposed met-
rics, we measured a broad set of TI sources, and reported
the characteristics and limitations of TI data. In addition to
the individual ﬁndings mentioned in each section, here we
highlight the high-level lessons we learned from our study:
• TI feeds, far from containing homogeneous samples of
some underlying truth, vary tremendously in the kinds of
data they capture based on the particularities of their col-
lection approach. Unfortunately, few TI vendors explain
the mechanism and methodology by which their data
are collected and thus TI consumers must make do with
simple labels such as “scan” or “botnet”, coupled with
inferences about the likely mode of collection. Worse,
a signiﬁcant amount of data does not even have a clear
deﬁnition of category, and is only labelled as “malicious”
or “suspicious”, leaving the ambiguity to consumers to
decide what action should be taken based on the data.
• There is little evidence that larger feeds contain better
data, or even that there are crisp quality distinctions be-
tween feeds across different categories or metrics (i.e.,
that a TI provider whose feed performs well on one
metric will perform well on another, or that these rank-
ings will hold across threat categories). How data is
collected also does not necessarily imply the feeds’ at-
tributes. For example, crowdsourcing-based feeds (e.g.,
Badips feeds), are not always slower in reporting data
than the self-collecting feeds (like Paid IP Reputation).
• Most IP-based TI data sources are collections of single-
tons (i.e., that each IP address appears in at most one
source) and even the higher-correlating data sources fre-
quently have intersection rates of only 10%. Moreover,
when comparing with broad sensor data in known cate-
gories with broad effect (e.g., random scanning) fewer
than 2% of observed scanner addresses appear in most
of the data sources we analyzed; indeed, even when fo-
cused on the largest and most proliﬁc scanners, coverage
is still limited to 10%. There are similar results for ﬁle
hash-based sources with little overlap among them.
The low intersection and coverage of TI feeds could be the
result of several non-exclusive possibilities. First is that the
underlying space of indicators (both IP addresses and mali-
cious ﬁle hashes) is large and each individual data source can
at best sample a small fraction thereof. It is almost certain
that this is true to some extent. Second, different collection
methodologies—even for the same threat category—will se-
lect for different sub distributions of the underlying ground
truth data. Third, this last effect is likely exacerbated by the
fact that not all threats are experienced uniformly across the
Internet and, thus, different methodologies will skew to either
favor or disfavor targeted attacks.
Based on our experience analyzing TI data, we try to pro-
vide several recommendations for the security community on
this topic moving forward:
• The threat intelligence community should standardize
data labeling, with a clear deﬁnition of what the data
means and how the data is collected. Security experts
can then assess whether the data ﬁt their need and the
type of action should be taken on this data.
• There are few rules of thumb in selecting among TI feeds,
as there is not a clear correlation between different feed
properties. Consumers need empirical metrics, such as
those we describe, to meaningfully differentiate data
sources, and to prioritize certain metrics based on their
speciﬁc need.
• Blindly using TI data—even if one could afford to ac-
quire many such sources—is unlikely to provide better
coverage and is also prone to collateral damage caused
by false positives. Customers need to be always aware of
these issues when deciding what action should be taken
on this data.
• Besides focusing on the TI data itself, future work should
investigate the operational uses of threat intelligence in
industry, as the true value of TI data can only be under-
stood in operational scenarios. Moreover, the community
should explore more potential ways of using the data,
which will extend our understanding of threat intelli-
gence and also inﬂuence how vendors are curating the
data and providing the services.
There are many ways we can use threat intelligence data. It
can be used to enrich other information (e.g., for investigating
potential explanations of a security incident), as a probabilis-
tic canary (i.e., identifying an overall site vulnerability via a
single matching indicator may have value even if other attacks
of the same kind are not detected) or in providing a useful
source of ground truth data for supervised machine learning
systems. However, even given such diverse purposes, organi-
zations still need some way to prioritize which TI sources to
invest in. Our metrics provide some direction for such choices.
For example, an analyst who expects to use TI interactively
during incident response would be better served by feeds with
higher coverage, but can accommodate poor accuracy, while
an organization trying to automatically label malicious in-
stances for training purposes (e.g., brute force attacks) will
be better served by the converse. Thus, if there is hope for
demonstrating that threat intelligence can materially impact
operational security practices, we believe it will be found
in these more complex uses cases and that is where future
research will be most productive.
10 Acknowledgment
We would like to thank our commercial threat providers who
made their data available to us and made this research possible.
In particular, we would like to thank Nektarios Leontiadis and
the Facebook ThreatExchange for providing the threat data
that helped facilitate our study. We are also very grateful to
Alberto Dainotti and Alistair King for sharing the UCSD tele-
scope data and helping us with the analysis, Gautam Akiwate
for helping us query the domain data, and Matt Jonkman. We
are also grateful to Martina Lindorfer, our shepherd, and our
anonymous reviewers for their insightful feedback and sugges-
tions. This research is a joint work from multiple institutions,
sponsored in part by DHS/AFRL award FA8750-18-2-0087,
NSF grants CNS-1237265, CNS-1406041, CNS-1629973,
CNS-1705050, and CNS-1717062.
References
[1] Abuse.ch. https://abuse.ch/.
[2] Top Alexa domains.
https://www.alexa.com/
topsites/.
[3] Alienvault IP reputation.
http://reputation.
alienvault.com/reputation.data.
[4] ANTONAKAKIS, M., APRIL, T., BAILEY, M., BERN-
HARD, M., BURSZTEIN, E., COCHRAN, J., DU-
RUMERIC, Z., HALDERMAN, J. A., INVERNIZZI, L.,
KALLITSIS, M., ET AL. Understanding the mirai botnet.
In USENIX Security Symposium (2017).
[5] Badips. https://www.badips.com/.
[6] BENSON, K., DAINOTTI, A., SNOEREN, A. C.,
KALLITSIS, M., ET AL. Leveraging internet back-
ground radiation for opportunistic network analysis. In
Proceedings of the 2015 Internet Measurement Confer-
ence (2015), ACM.
[7] The Bro network security monitor. https://www.bro.
[8] Composite Blocking List.
https://www.abuseat.
org/index.html.
org/.
[9] Spreading
the
the
https://krebsonsecurity.com/2015/01/
spreading-the-disease-and-selling-the-cure/.
and selling
disease
cure.
[10] CHACHRA, N., MCCOY, D., SAVAGE, S., AND
VOELKER, G. M. Empirically Characterizing Domain
Abuse and the Revenue Impact of Blacklisting.
In
Proceedings of the Workshop on the Economics of
Information Security (WEIS) (State College, PA, 2014).
[11] Cloudﬂare, fast, global content delivery network. https:
//www.cloudflare.com/cdn/.
[12] AWS CloudFront, fast, highly secure and programmable
content delivery network. https://aws.amazon.com/
cloudfront/.
[13] Cyber Observable
eXpression.
http:
//cyboxproject.github.io/documentation/.
[14] DEKOVEN, L. F., SAVAGE, S., VOELKER, G. M., AND
LEONTIADIS, N. Malicious browser extensions at scale:
Bridging the observability gap between web site and
browser. In 10th USENIX Workshop on Cyber Security
Experimentation and Test (CSET 17) (2017), USENIX.
[15] DURUMERIC, Z., BAILEY, M., AND HALDERMAN,
J. A. An internet-wide view of internet-wide scanning.
In USENIX Security Symposium (2014).
[16] Edgecast CDN, Verizon digital and media
ser-
https://www.verizondigitalmedia.com/
vices.
platform/edgecast-cdn/.
[17] Facebook threat exchange.
https://developers.
facebook.com/programs/threatexchange.
[18] Fastly managed CDN. https://www.fastly.com/
products/fastly-managed-cdn.
[19] Incident Object Description Exchange Format. https:
//tools.ietf.org/html/rfc5070.
[20] JAGPAL, N., DINGLE, E., GRAVEL, J.-P., MAVROM-
MATIS, P., PROVOS, N., RAJAB, M. A., AND THOMAS,
K. Trends and lessons from three years ﬁghting ma-
In USENIX Security Symposium
licious extensions.
(2015).
[21] JUNG, J., AND SIT, E. An empirical study of spam
trafﬁc and the use of dns black lists. In Proceedings of
the ACM Conference on Internet Measurement (2004).
[22] KAPRAVELOS, A., GRIER, C., CHACHRA, N.,
KRUEGEL, C., VIGNA, G., AND PAXSON, V. Hulk:
Eliciting malicious behavior in browser extensions. In
USENIX Security Symposium (2014), San Diego, CA.
[23] KÜHRER, M., ROSSOW, C., AND HOLZ, T. Paint it
black: Evaluating the effectiveness of malware black-
lists. In International Workshop on Recent Advances in
Intrusion Detection (2014), Springer.
[24] LEVCHENKO, K., PITSILLIDIS, A., CHACHRA, N., EN-
RIGHT, B., FÉLEGYHÁZI, M., GRIER, C., HALVOR-
SON, T., KANICH, C., KREIBICH, C., LIU, H., MC-
COY, D., WEAVER, N., PAXSON, V., VOELKER, G. M.,
AND SAVAGE, S. Click Trajectories: End-to-End Anal-
ysis of the Spam Value Chain. In Proceedings of the
IEEE Symposium and Security and Privacy (2011).
[25] MaxCDN. https://www.maxcdn.com/one/.
[26] METCALF, L., AND SPRING, J. M. Blacklist ecosystem
analysis: Spanning jan 2012 to jun 2014. In Proceedings
of the 2nd ACM Workshop on Information Sharing and
Collaborative Security (2015), ACM.
[27] Nothink honeypot SSH. http://www.nothink.org/
honeypot_ssh.php.
[28] Packetmail.net. https://www.packetmail.net/.
[29] PANG, R., YEGNESWARAN, V., BARFORD, P., PAX-
SON, V., AND PETERSON, L. Characteristics of internet
background radiation. In Proceedings of the 4th ACM
SIGCOMM conference on Internet measurement (2004),
ACM.
[30] PITSILLIDIS, A., KANICH, C., VOELKER, G. M.,
LEVCHENKO, K., AND SAVAGE, S. Taster’s Choice: A
Comparative Analysis of Spam Feeds. In Proceedings
of the ACM Internet Measurement Conference (Boston,
MA, Nov. 2012), pp. 427–440.
[31] RAMACHANDRAN, A., FEAMSTER, N., DAGON, D.,
ET AL. Revealing botnet membership using dnsbl
counter-intelligence. SRUTI 6 (2006).
[32] RAMACHANDRAN, A., FEAMSTER, N., AND VEM-
PALA, S. Filtering spam with behavioral blacklisting. In
Proceedings of the 14th ACM Conference on Computer
and Communications Security (CCS) (2007).
[33] SCHEITLE, Q., HOHLFELD, O., GAMBA, J., JEL-
TEN, J., ZIMMERMANN, T., STROWES, S. D., AND
VALLINA-RODRIGUEZ, N. A long way to the top: Sig-
niﬁcance, structure, and stability of internet top lists. In
Proceedings of the Internet Measurement Conference
(2018), ACM.
[34] Shadowserver. https://www.shadowserver.org/.
[35] SHENG, S., WARDMAN, B., WARNER, G., CRANOR,
L. F., HONG, J., AND ZHANG, C. An empirical analysis
of phishing blacklists. In Proceedings of the Conference
on Email and Anti-Spam (CEAS) (2009).
[36] SINHA, S., BAILEY, M., AND JAHANIAN, F. Shades
of grey: On the effectiveness of reputation-based “black-
lists”. In 2008 3rd International Conference on Mali-
cious and Unwanted Software (MALWARE), IEEE.
[37] Structured Threat Information eXpression. https://
stixproject.github.io/.
[38] UCSD network telescope. https://www.caida.org/
projects/network_telescope/.
[39] The spam and open relay blocking system. http://
www.sorbs.net/.
org/sbl/.
[40] The Spamhaus block list. https://www.spamhaus.
[41] The Spamhaus Don’t Route Or Peer Lists. https://
www.spamhaus.org/drop/.
[42] THOMAS, K., AMIRA, R., BEN-YOASH, A., FOLGER,
O., HARDON, A., BERGER, A., BURSZTEIN, E., AND
BAILEY, M. The abuse sharing economy: Understand-
ing the limits of threat exchanges. In International Sym-
posium on Research in Attacks, Intrusions, and Defenses
(2016), Springer.
[43] Threat
intelligence market analysis by solution,
by services, by deployment, by application and
segment forecast, 2018 - 2025.
https://www.
grandviewresearch.com/industry-analysis/
threat-intelligence-market.
[44] University of Oregon route views project. http://www.
routeviews.org/routeviews/.
https://www.virustotal.com/#/
[45] VirusTotal.
home/upload.