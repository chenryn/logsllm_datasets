### 7.1 Incident Investigations and Latency

During manual incident investigations, the analyst, as a domain expert, can provide additional filtering of false positives. Similarly, latency may not be a critical metric in retrospective use cases, such as post-discovery breach investigations. However, if an organization intends to add IP addresses from a Threat Intelligence (TI) source to a firewall’s blacklist, accuracy and latency should be prioritized over coverage, assuming that blocking benign activity is more costly.

Another common real-world scenario involves a company with a limited budget for purchasing TI sources and a specific set of threats (e.g., botnets, brute-force attacks) they aim to mitigate. In such cases, the metrics described can be used to evaluate TI options, favoring sources that maximize coverage of the most relevant threats while minimizing overlap.

### 7.2 Data Labeling

Threat intelligence IP data carries different meanings, and it is crucial to understand these meanings to use the data effectively. For example, indicators may represent Internet scanners, members of a botnet, or malicious actors who have attacked other targets before. We attempted to categorize feeds based on their intended meaning in our analysis.

However, category information, primarily provided by TI sources themselves, is not always available. Feeds like AlienVault IP Reputation and Facebook Threat Exchange often label indicators as "Malicious" or "Suspicious," but the exact meanings of these labels are unclear, making it difficult for consumers to decide how to use the data and anticipate potential consequences.

For feeds that do provide category information, the categories can sometimes be too broad to be meaningful. For instance, multiple feeds in our collection simply label their indicators as "Scanner." Network scanning can refer to port scanning (sending SYN packets) or vulnerability scanning (probing hosts for known vulnerabilities). This ambiguity, resulting from ad-hoc data labeling, poses challenges for security experts using TI data.

Recently, standard TI formats such as IODEF [19], CybOX [13], and STIX [37] have been proposed to standardize threat intelligence presentation and sharing. However, these standards focus mainly on data format rather than semantics. There is room for improvement by designing a standard semantics for threat intelligence data.

### 7.3 Limitations

Our study has several limitations. We attempted to collect data from a diverse set of sources, including public, commercial, and industrial exchange feeds, but this collection is inherently not comprehensive. Some prohibitively expensive or publication-restricted data sources were not available to us. Future specialized measurement work should analyze the performance of these exclusive data sources.

A second limitation is our visibility into how different companies use threat intelligence operationally. The most useful metrics for a company might measure how a TI source affects its main performance indicators and risk exposure. Such metrics would require deep integration into enterprise security workflows to measure the operational impact of decisions made using threat intelligence. This would help CIOs and CSOs better understand the contributions of specific TI products. As researchers, we do not use TI operationally, and a better understanding of operational needs would help refine our metrics to maximize their utility for operations-driven consumers.

The third limitation is the lack of ground truth, a challenge shared by similar measurement studies. It is very difficult to obtain a complete picture of a certain category of threat, making it challenging to precisely determine the accuracy and coverage of feeds. In this study, we used data from an Internet telescope and VirusTotal as a close approximation. There are also a few cases where security incidents have been comprehensively studied, such as the Mirai study [4], which can be used to evaluate certain types of TI data. However, such studies are rare. An alternative approach is to establish ground truth for a specific network. For example, a company can record all network traffic and identify security incidents through its IDS system or manual forensic analysis, then evaluate the accuracy and coverage of a TI feed in the context of its own network. This can provide a customized view of TI feeds.

### 8. Related Work

Several studies have examined the effectiveness of blacklist-based threat intelligence [23, 31, 32, 35, 36]. Ramachandran et al. [32] showed that spam blacklists are both incomplete (missing 35% of the source IPs of spam emails captured in two spam traps) and slow in responding (20% of spammers remain unlisted after 30 days). Sinha et al. [36] further confirmed this result, showing that four major spam blacklists have very high false negative rates and analyzing the possible causes of low coverage. Sheng et al. [35] studied the effectiveness of phishing blacklists, showing that the lists are slow in reacting to highly transient phishing campaigns. These studies focused on specific types of TI sources and evaluated their operational performance rather than producing empirical evaluation metrics for TI data sources.

Other studies have analyzed the general attributes of threat intelligence data. Pitsillidis et al. [30] studied the characteristics of spam domain feeds, showing different perspectives of spam feeds and demonstrating that different feeds are suitable for answering different questions. Thomas et al. [42] constructed their own threat intelligence by aggregating abuse traffic received from six Google services, showing a lack of intersection and correlation among these different sources. While focusing on broader TI uses, these studies did not focus on generalizable threat metrics that can be extended beyond the work.

Little work exists that defines a general measurement methodology to examine threat intelligence across a broad set of types and categories. Metcalf et al. [26] collected and measured IP and domain blacklists from multiple sources but only focused on volume and intersection analysis. In contrast, we formally define a set of threat intelligence metrics and conduct a broad and comprehensive study over a rich variety of TI data. We conducted our measurement from the perspective of TI data consumers to offer guidance on choosing between different sources. Our study also demonstrated the limitations of threat intelligence more thoroughly, providing comprehensive characteristics of cyber threat intelligence that no previous work had addressed.

### 9. Conclusion

This paper focuses on the simplest yet fundamental metrics about threat intelligence data. Using the proposed metrics, we measured a broad set of TI sources and reported the characteristics and limitations of TI data. In addition to the individual findings mentioned in each section, here are the high-level lessons we learned from our study:

- TI feeds vary significantly in the kinds of data they capture based on their collection approach. Unfortunately, few TI vendors explain their data collection mechanisms, leaving consumers to make inferences based on simple labels like "scan" or "botnet." Worse, a significant amount of data lacks clear category definitions and is only labeled as "malicious" or "suspicious," leaving the ambiguity to consumers to decide the appropriate action.
- There is little evidence that larger feeds contain better data or that there are clear quality distinctions between feeds across different categories or metrics. How data is collected does not necessarily imply the feeds' attributes. For example, crowdsourcing-based feeds (e.g., Badips) are not always slower in reporting data than self-collecting feeds (like Paid IP Reputation).
- Most IP-based TI data sources are collections of singletons (i.e., each IP address appears in at most one source), and even the higher-correlating data sources frequently have intersection rates of only 10%. When comparing with broad sensor data in known categories with broad effects (e.g., random scanning), fewer than 2% of observed scanner addresses appear in most of the data sources we analyzed. Even when focused on the largest and most prolific scanners, coverage is still limited to 10%. Similar results hold for file hash-based sources, with little overlap among them.

The low intersection and coverage of TI feeds could be due to several non-exclusive possibilities. First, the underlying space of indicators (both IP addresses and malicious file hashes) is large, and each individual data source can sample only a small fraction. Second, different collection methodologies, even for the same threat category, will select for different sub-distributions of the underlying ground truth data. Third, not all threats are experienced uniformly across the Internet, and different methodologies will skew to either favor or disfavor targeted attacks.

Based on our experience analyzing TI data, we provide several recommendations for the security community moving forward:

- The threat intelligence community should standardize data labeling, with clear definitions of what the data means and how it is collected. Security experts can then assess whether the data fits their needs and the type of action that should be taken.
- There are few rules of thumb in selecting among TI feeds, as there is no clear correlation between different feed properties. Consumers need empirical metrics, such as those we describe, to meaningfully differentiate data sources and prioritize certain metrics based on their specific needs.
- Blindly using TI data, even if one can afford to acquire many such sources, is unlikely to provide better coverage and is also prone to collateral damage caused by false positives. Customers need to be aware of these issues when deciding what action to take.
- Future work should investigate the operational uses of threat intelligence in industry, as the true value of TI data can only be understood in operational scenarios. The community should explore more potential ways of using the data, which will extend our understanding of threat intelligence and influence how vendors curate the data and provide services.

There are many ways to use threat intelligence data, such as enriching other information, serving as a probabilistic canary, or providing ground truth data for supervised machine learning systems. However, organizations still need a way to prioritize which TI sources to invest in. Our metrics provide some direction for such choices. For example, an analyst who expects to use TI interactively during incident response would be better served by feeds with higher coverage, even if accuracy is poor, while an organization trying to automatically label malicious instances for training purposes (e.g., brute force attacks) will be better served by the converse. If there is hope for demonstrating that threat intelligence can materially impact operational security practices, it will be found in these more complex use cases, and that is where future research will be most productive.

### 10. Acknowledgment

We would like to thank our commercial threat providers who made their data available to us and made this research possible. In particular, we would like to thank Nektarios Leontiadis and the Facebook ThreatExchange for providing the threat data that facilitated our study. We are also grateful to Alberto Dainotti and Alistair King for sharing the UCSD telescope data and helping with the analysis, Gautam Akiwate for assisting with querying the domain data, and Matt Jonkman. We are also grateful to Martina Lindorfer, our shepherd, and our anonymous reviewers for their insightful feedback and suggestions. This research is a joint effort from multiple institutions, sponsored in part by DHS/AFRL award FA8750-18-2-0087, NSF grants CNS-1237265, CNS-1406041, CNS-1629973, CNS-1705050, and CNS-1717062.

### References

[1] Abuse.ch. https://abuse.ch/.

[2] Top Alexa domains. https://www.alexa.com/topsites/.

[3] AlienVault IP reputation. http://reputation.alienvault.com/reputation.data.

[4] Antonakakis, M., April, T., Bailey, M., Bernhard, M., Bursztein, E., Cochran, J., Durumeric, Z., Halderman, J. A., Invernizzi, L., Kallitsis, M., et al. Understanding the Mirai botnet. In USENIX Security Symposium (2017).

[5] Badips. https://www.badips.com/.

[6] Benson, K., Dainotti, A., Snoeren, A. C., Kallitsis, M., et al. Leveraging internet background radiation for opportunistic network analysis. In Proceedings of the 2015 Internet Measurement Conference (2015), ACM.

[7] The Bro network security monitor. https://www.bro.org/.

[8] Composite Blocking List. https://www.abuseat.org/index.html.

[9] Spreading the disease and selling the cure. https://krebsonsecurity.com/2015/01/spreading-the-disease-and-selling-the-cure/.

[10] Chachra, N., McCoy, D., Savage, S., and Voelker, G. M. Empirically Characterizing Domain Abuse and the Revenue Impact of Blacklisting. In Proceedings of the Workshop on the Economics of Information Security (WEIS) (State College, PA, 2014).

[11] Cloudflare, fast, global content delivery network. https://www.cloudflare.com/cdn/.

[12] AWS CloudFront, fast, highly secure and programmable content delivery network. https://aws.amazon.com/cloudfront/.

[13] Cyber Observable eXpression. http://cyboxproject.github.io/documentation/.

[14] Dekoven, L. F., Savage, S., Voelker, G. M., and Leontiadis, N. Malicious browser extensions at scale: Bridging the observability gap between web site and browser. In 10th USENIX Workshop on Cyber Security Experimentation and Test (CSET 17) (2017), USENIX.

[15] Durumeric, Z., Bailey, M., and Halderman, J. A. An internet-wide view of internet-wide scanning. In USENIX Security Symposium (2014).

[16] Edgecast CDN, Verizon digital and media services. https://www.verizondigitalmedia.com/platform/edgecast-cdn/.

[17] Facebook Threat Exchange. https://developers.facebook.com/programs/threatexchange.

[18] Fastly managed CDN. https://www.fastly.com/products/fastly-managed-cdn.

[19] Incident Object Description Exchange Format. https://tools.ietf.org/html/rfc5070.

[20] Jagpal, N., Dingle, E., Gravel, J.-P., Mavrommatis, P., Provos, N., Rajab, M. A., and Thomas, K. Trends and lessons from three years fighting malicious extensions. In USENIX Security Symposium (2015).

[21] Jung, J., and Sit, E. An empirical study of spam traffic and the use of DNS blacklists. In Proceedings of the ACM Conference on Internet Measurement (2004).

[22] Kapravelos, A., Grier, C., Chachra, N., Kruegel, C., Vigna, G., and Paxson, V. Hulk: Eliciting malicious behavior in browser extensions. In USENIX Security Symposium (2014), San Diego, CA.

[23] Kührer, M., Rossow, C., and Holz, T. Paint it black: Evaluating the effectiveness of malware blacklists. In International Workshop on Recent Advances in Intrusion Detection (2014), Springer.

[24] Levchenko, K., Pitsillidis, A., Chachra, N., Enright, B., Félegyházi, M., Grier, C., Halvorson, T., Kanich, C., Kreibich, C., Liu, H., McCoy, D., Weaver, N., Paxson, V., Voelker, G. M., and Savage, S. Click Trajectories: End-to-End Analysis of the Spam Value Chain. In Proceedings of the IEEE Symposium and Security and Privacy (2011).

[25] MaxCDN. https://www.maxcdn.com/one/.

[26] Metcalf, L., and Spring, J. M. Blacklist ecosystem analysis: Spanning Jan 2012 to Jun 2014. In Proceedings of the 2nd ACM Workshop on Information Sharing and Collaborative Security (2015), ACM.

[27] Nothink honeypot SSH. http://www.nothink.org/honeypot_ssh.php.

[28] Packetmail.net. https://www.packetmail.net/.

[29] Pang, R., Yegneswaran, V., Barford, P., Paxson, V., and Peterson, L. Characteristics of internet background radiation. In Proceedings of the 4th ACM SIGCOMM conference on Internet measurement (2004), ACM.

[30] Pitsillidis, A., Kanich, C., Voelker, G. M., Levchenko, K., and Savage, S. Taster's Choice: A Comparative Analysis of Spam Feeds. In Proceedings of the ACM Internet Measurement Conference (Boston, MA, Nov. 2012), pp. 427–440.

[31] Ramachandran, A., Feamster, N., Dagon, D., et al. Revealing botnet membership using DNSBL counter-intelligence. SRUTI 6 (2006).

[32] Ramachandran, A., Feamster, N., and Vempala, S. Filtering spam with behavioral blacklisting. In Proceedings of the 14th ACM Conference on Computer and Communications Security (CCS) (2007).

[33] Scheitle, Q., Hohlfeld, O., Gamba, J., Jeltten, J., Zimmermann, T., Strowes, S. D., and Vallina-Rodriguez, N. A long way to the top: Significance, structure, and stability of internet top lists. In Proceedings of the Internet Measurement Conference (2018), ACM.

[34] Shadowserver. https://www.shadowserver.org/.

[35] Sheng, S., Wardman, B., Warner, G., Cranor, L. F., Hong, J., and Zhang, C. An empirical analysis of phishing blacklists. In Proceedings of the Conference on Email and Anti-Spam (CEAS) (2009).

[36] Sinha, S., Bailey, M., and Jahanian, F. Shades of grey: On the effectiveness of reputation-based "blacklists". In 2008 3rd International Conference on Malicious and Unwanted Software (MALWARE), IEEE.

[37] Structured Threat Information eXpression. https://stixproject.github.io/.

[38] UCSD network telescope. https://www.caida.org/projects/network_telescope/.

[39] The Spam and Open Relay Blocking System. http://www.sorbs.net/.

[40] The Spamhaus block list. https://www.spamhaus.org/sbl/.

[41] The Spamhaus Don't Route Or Peer Lists. https://www.spamhaus.org/drop/.

[42] Thomas, K., Amira, R., Ben-Yoash, A., Folger, O., Hardon, A., Berger, A., Bursztein, E., and Bailey, M. The abuse sharing economy: Understanding the limits of threat exchanges. In International Symposium on Research in Attacks, Intrusions, and Defenses (2016), Springer.

[43] Threat intelligence market analysis by solution, by services, by deployment, by application and segment forecast, 2018 - 2025. https://www.grandviewresearch.com/industry-analysis/threat-intelligence-market.

[44] University of Oregon route views project. http://www.routeviews.org/routeviews/.

[45] VirusTotal. https://www.virustotal.com/#/home/upload.