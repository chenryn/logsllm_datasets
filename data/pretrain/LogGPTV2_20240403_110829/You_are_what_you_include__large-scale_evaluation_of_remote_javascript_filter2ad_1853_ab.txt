1.64
1.61
1.67
1.75
1.86
2.10
Year
2001
2002
2003
2004
2005
2006
2007
2008
2009
2010
Table 3: Number of new domains that are intro-
duced every year in remote inclusions.
inclusions were the same as the previous year, we consider
those as same inclusion. Unfortunately, archive.org does not
cover all the pages we examined completely, and thus we
have cases where no data could be retrieved for a speciﬁc
domain for all of the requested years. Also, many popular
web sites did not exist 10 years ago. There were 892 domains
for which we did not ﬁnd a single URL that we previously
crawled in archive.org. A domain might not be found on
archive.org because of one of the following reasons: the web-
site restricts crawling from its robots.txt ﬁle (182 domains),
the domain was never chosen to be crawled (320 domains) or
the domain was crawled, but not the speciﬁc pages that we
chose during our ﬁrst crawl (390 domains). In Table 2, we
show how many domains introduced new inclusions in abso-
lute numbers. In our experiment, we ﬁnd (not surprisingly)
that as we get closer in time to the present, archive.org has
available versions for more of the URLs that we query for
and thus we can examine more inclusions. We discovered
that every year, a signiﬁcant amount of inclusions change.
Every year there are additional URLs involved in the inclu-
sions of a website compared to the previous years and there
is a clear trend of including even more. Back in 2001, 24.48%
of the studied domains had at least one new remote inclu-
sion. But as the web evolves and becomes more dynamic,
more web sites extend their functionality by including more
JavaScript code. In 2010, 45.46% of the examined web sites
introduced a new JavaScript inclusion since the last year.
This means that almost half of the top 10,000 Alexa do-
mains had at least one new remote JavaScript inclusion in
2010, when compared to 2009.
But introducing a new JavaScript inclusion does not au-
tomatically translate to a new dependency from a remote
provider. In Table 3, we examine whether more inclusions
translate to more top-level remote domains. We calculate
the unique domains involved in the inclusions and the to-
tal number of remote inclusions. For every page examined,
we keep the unique domains involved in its new inclusions,
and we provide the average of that number for all avail-
able pages per year. There is a clear trend in Table 3 that
more inclusions result into more external dependencies from
new domains. In fact in 2010 we observed that on average
each page expanded their inclusions by including JavaScript
from 2.1 new domains on average compared to 2009. This
trend shows that the circle of trust for each page is expand-
Figure 2: Evolution of remote JavaScript inclusions
for domains ranked in the top 10,000 from Alexa.
Year
2001
2002
2003
2004
2005
2006
2007
2008
2009
2010
No
data
8,256
7,952
7,576
7,100
6,672
6,073
5,074
3,977
3,111
1,920
Same
New
% New
inclusions
inclusions
inclusions
1,317
1,397
1,687
2,037
2,367
2,679
3,136
3,491
3,855
4,407
427
651
737
863
961
1,248
1,790
2,532
3,034
3,673
24.48%
31.79%
30.40%
29.76%
28.88%
31.78%
36.34%
42.04%
44.04%
45.46%
Table 2: Evolution of the number of domains with
same and new remote JavaScript inclusions for the
Alexa top 10,000
Script inclusions from the same web sites in another dimen-
sion: time. We have crawled archive.org [4] to study how
JavaScript inclusions have evolved through time in terms of
new remote dependencies and if these increase or decrease
over time.
To better understand how JavaScript is included and how
the inclusions change over time, we examine each page from
diﬀerent snapshots that span across several years. For the
same pages that we crawled in Section 2, we have queried
archive.org to obtain their versions for past years (if avail-
able). For each domain, we choose one representative page
that has the most remote inclusions and the highest avail-
ability since 2000. For every chosen page we downloaded
one snapshot per year from 2000 to 2010. Every snapshot
was compared with the previous one in order to compute
the inclusion changes.
In Figure 2, one can see the evolution of remote JavaScript
inclusions for domains ranked in the top 10,000 from Alexa.
For every year, we show how the inclusions from the pre-
vious available snapshot changed with the addition of new
inclusions or if they remained the same. A new inclusion
means that the examined domain introduced at least one
new remote script inclusion since the last year. If the page’s
739ing every year and that the surface of attack against them
increases.
3.2 Quality of Maintenance Metric
Whenever developers of a web application decide to in-
clude a library from a third-party provider, they allow the
latter to execute code with the same level of privilege as
their own code. Eﬀectively, they are adding the third-party
host to the security perimeter of the web application, that
is the set of the hosts whose exploitation leads to control-
ling the code running on that web application. Attacking the
third-party, and then using that foothold to compromise the
web application, might be easier than a direct attack of the
latter. The aforementioned incident of the malicious mod-
iﬁcation of the qTip2 plugin [2], shows that cybercriminals
are aware of this and have already used indirect exploitation
to infect more hosts and hosts with more secure perimeters.
To better understand how many web applications are ex-
posed to this kind of indirect attack, we aim to identify
third-party providers that could be a weak link in the se-
curity of popular web applications. To do so, we design
a metric that evaluates how well a website is being main-
tained, and apply it to the web applications running on the
hosts of library providers (that is co-located with the Java-
Script library that is being remotely included). We indicate
the low-scoring as potential weak links, on the assumption
that unkempt websites seem easier targets to attackers, and
therefore are attacked more often.
Note that this metric aims at characterizing how well web-
sites are maintained, and how security-conscious are their
developers and administrators. It is not meant to investi-
gate if a URL could lead to malicious content (a la Google
Safebrowsing, for example). Also, we designed this metric
to look for the signs of low maintenance that an attacker,
scouting for the weakest host to attack, might look for. We
recognize that a white-box approach, where we have access
to the host under scrutiny, would provide a much more pre-
cise metric, but this would require a level of access that at-
tackers usually do not have. We identiﬁed the closest prior
work in establishing such a metric in SSL Labs’s SSL/TLS
survey [3] and have included their ﬁndings in our metric.
Our Quality of Maintenance (QoM) metric is based on
the following features:
• Availability: If the host has a DNS record associated
with it, we check that its registration is not expired.
Also, we resolve the host’s IP address, and we verify
that it is not in the ranges reserved for private networks
(e.g., 192.168.0.0/16). Both of these features are crit-
ical, because an attacker could impersonate a domain
by either registering the domain name or claiming its
IP address. By impersonating a domain, an attacker
gains the trust of any web application that includes
code hosted on the domain.
• Cookies: We check the presence of at least one cookie
set as HttpOnly and, if SSL/TLS is available, at least
one cookie set as Secure. Also, we check that at least
one cookie has its Path and Expiration attributes
set. All these attributes improve the privacy of ses-
sion cookies, so they are a good indication that the
domain administrators are concerned about security.
• Anti-XSS and Anti-Clickjacking protocols: We
check for the presence of the X-XSS-Protection proto-
col, which was introduced with Internet Explorer 8 [24]
to prevent some categories of Cross-site Scripting (XSS)
attacks [18]. Also, we check for the presence of Mozilla’s
Content Security Policy protocol, which prevents some
XSS and Clickjacking attacks [6] in Firefox. Finally,
we check for the presence of the X-Frame-Options pro-
tocol, which aims at preventing ClickJacking attacks
and is supported by all major browsers.
• Cache control: If SSL/TLS is present, we check if
some content is served with the headers Cache-Control:
private and Pragma:no-cache. These headers indi-
cate that the content is sensitive and should not be
cached by the browser, so that local attacks are pre-
vented.
• SSL/TLS implementation: For a thorough evalu-
ation of the SSL/TLS implementation, we rely on the
study conducted by SSL Labs in April 2011. In partic-
ular, we check that the domain’s certiﬁcate is valid (un-
revoked, current, unexpired, and matches the domain
name) and that it is trusted by all major browsers.
Also, we verify that current protocols (e.g, TLS 1.2,
SSL 3.0) are implemented, that older ones (e.g., SSL
2.0) are not used, and if the protocols allow weak ci-
phers.
In addition, we check if the implementation
is PCI-DSS compliant [12], which is a security stan-
dard to which organizations that handle credit card
information must comply, and adherence to it is cer-
tiﬁed yearly by the Payment Card Industry. Also, we
check if the domain is vulnerable to the SSL insecure-
renegotiation attack. We check if the key is weak due
to a small key size, or the Debian OpenSSL ﬂaw. Fi-
nally, we check if the site oﬀers Strict Transport Secu-
rity, which forces a browser to use secure connections
only, like HTTPS.
SSL Labs collected the features described in the previ-
ous paragraph nine months before we collected all the
remaining features. We believe that this is acceptable,
as certiﬁcates usually have a lifespan of a few years,
and the Payment Card Industry checks PCI-DSS com-
pliance yearly. Also, since these features have been
collected in the same period for all the hosts, they do
not give unfair advantages to some of them.
• Outdated web servers: Attackers can exploit known
vulnerabilities in web servers to execute arbitrary code
or access sensitive conﬁguration ﬁles. For this reason,
an obsolete web server is a weak link in the security of
a domain. To establish which server versions (in the
HTTP Server header) should be considered obsolete,
we collected these HTTP Server header strings during
our crawl and, after clustering them, we selected the
most popular web servers and their versions. Consult-
ing change-logs and CVE reports, we compiled a list of
stable and up-to-date versions, which is shown in Ta-
ble 4. While it is technically possible for a web server
to report an arbitrary version number, we assume that
if the version is modiﬁed it will be modiﬁed to pretend
that the web server is more up-to-date rather than less,
since the latter would attract more attacks. This fea-
ture is not consulted in the cases where a web server
does not send a Server header or speciﬁes it in a generic
way (e.g., “Apache”).
740Web server
Apache
NGINX
IIS
Lighttpd
Zeus
Cherokee
CWS
LiteSpeed
0w
Up-to-date version(s)
1.3.42, 2.0.65, 2.2.22
1.1.10, 1.0.9, 0.8.55, 0.7.69, 0.6.39, 0.5.38
7.5, 7.0
1.5 , 1.4.29
4.3
1.2
3.0
4.1.3
0.8d
Table 4: Up-to-date versions of popular web servers,
at the time of our experiment
The next step in building our QoM metric is to weigh
these features. We cannot approach this problem from a su-
pervised learning angle because we have no training set: We
are not aware of any study that quantiﬁes the QoM of do-
mains on a large scale. Thus, while an automated approach
through supervised learning would have been more precise,
we had to assign the weights manually. Even so, we can ver-
ify that our QoM metric is realistic. To do so, we evaluated
with our metric the websites in the following four datasets
of domains in the Alexa Top 10, 000:
• XSSed domains: This dataset contains 1,702 do-
mains that have been exploited through cross-site script-
ing in the past. That is, an attacker injected malicious
JavaScript on at least one page of each domain. Us-
ing an XSS exploit, an attacker can steal the cook-
ies or password as it is typed into a login form [18].
Recently, the Apache Foundation disclosed that their
servers were attacked via an XSS vulnerability, and
the attacker obtained administrative access to several
servers [1]. To build this dataset, we used XSSed [29],
a publicly available database of over 45, 000 reported
XSS attacks.
• Defaced domains: This dataset contains 888 do-
mains that have been defaced in the past. That is, an
attacker changed the content of one or more pages on
the domain. To build this dataset, we employed the
Zone-H database [32]. This database contains more
than six million reports of defacements, however, only
888 out of the 10,000 top Alexa domains have suﬀered
a defacement.
• Bank domains: This dataset contains 141 domains
belonging to banking institutions (online and brick and
mortar) in the US.
• Random domains: This dataset contains 4,500 do-
mains, randomly picked, that do not belong to the
previous categories.
The cumulative distribution function of the metric on
these datasets is shown in Figure 3. At score 60, we have
506 defaced domains, 698 XSSed domains, 765 domains be-
longing to the random set, and only 5 banks. At score 120,
we have all the defaced and XSSed domains, 4,409 domains
from the random set, and all but 5 of the banking sites. The
maximum score recorded is 160, held by paypal.com. Ac-
cording to the metric, sites that have been defaced or XSSed
Figure 3: Cumulative distribution function of the
maintenance metric, for diﬀerent datasets
in the past appear to be maintained less than our dataset of
random domains. On the other hand, the majority of bank-
ing institutions are very concerned with the maintenance of
their domains. These ﬁndings are reasonable, and empiri-
cally demonstrate that our metric is a good indicator of the
quality of maintenance of a particular host. This is espe-
cially valid also because we will use this metric to classify
hosts into three wide categories: high maintenance (metric
greater than 150), medium, and low maintenance (metric
lower than 70).
3.3 Risk of Including Third-Party Providers
We applied our QoM metric to the top 10,000 domains
in Alexa and the domains providing their JavaScript inclu-
sions. The top-ranking domain is paypal.com, which has
also always been very concerned with security (e.g., it was
one of the proposers of HTTP Strict Transport Security).
The worst score goes to cafemom.com, because its SSL cer-