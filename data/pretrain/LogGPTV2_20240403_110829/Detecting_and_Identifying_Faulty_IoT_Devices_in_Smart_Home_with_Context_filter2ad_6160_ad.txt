testbed was the only available dataset
that contained the
actuator data, we evaluated the actuator faults from the
D ‘dataset’. DICE identiﬁed the problematic actuators with
92.5% precision and 94.9% recall on average. We detect and
identify actuator faults as well with high accuracy.
B. Detection and Identiﬁcation Time
We measured the average detection time and identiﬁcation
time of DICE in each dataset (Fig. 10). Detection time is
the time DICE takes to detect the presence of faults since
the occurrence of the fault. Identiﬁcation time is the time
DICE takes to identify the faulty sensor since the occurrence
of the fault. In general, the correlation degree and the detec-
tion/identiﬁcation time were proportional. Except for houseA,
our system detected faults of the nine other datasets in 10
minutes and identiﬁed them within 30 minutes at most. For
houseA, DICE took 21.88 minutes and 72.82 minutes on
average to detect and identify the faulty sensor, respectively.
Nevertheless, the slowest detection time of DICE was still
much faster than the fastest reported average detection time
of prior art which was 12 hours. Therefore, we are convinced
that DICE identiﬁes the faulty sensors promptly enough in
real-time. We explain why the different datasets show varying
detection and identiﬁcation time in Section V-D.
We also compared the detection time of the correlation
check and transition check in houseA, houseB, and houseC
(Table III). The fault detection by the transition check was
approximately three times slower than the fault detection by
the correlation check. This is because the correlation violation
is detected almost instantly, while the transition check detects
a fault after a contrasting transition occurs. In general, a sensor
state set retained its value for several rounds (i.e., minutes) of
time, which did not cause any drastic change in the transition.
As a result, DICE detects a fault after the transition violation
which took much more time than the correlation check.
618
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:47:10 UTC from IEEE Xplore.  Restrictions apply. 
(a) Detection
Precision
Recall
houseA
houseB
houseC
twor
hh102 D_houseA D_houseB D_houseC D_twor D_hh102
Fig. 9: Detection and Identiﬁcation Accuracy of the Ten Datasets
(b) Identiﬁcation
73 
Detection time
Identification time
houseA
houseB
houseC
twor
hh102 D_houseA D_houseB D_houseC D_twor D_hh102
y
c
a
r
u
c
c
A
100%
80%
60%
40%
20%
0%
)
e
t
u
n
m
i
(
e
m
T
i
40
30
20
10
0
Fig. 10: Detection and Identiﬁcation Time (Detection time: the time to detect a fault, Identiﬁcation time: the time to identify
the faulty sensor)
C. Computation Time
We measured the computation time of the correlation check,
the transition check, and identiﬁcation from the ten datasets to
verify the feasibility (Fig. 11). DICE spent shorter correlation
check time in houseA, houseB, and houseC than the time
in the other datasets. In detail, the correlation check time
was most inﬂuenced by obtaining probable groups and the
others took similar time in all datasets. The number of
sensors affected the time to obtain probable groups because
the distance in Fig. 5 was calculated for all probable groups.
From the 37 sensors in our Smart Home testbed, the number
of bits converted by the numeric sensors are over 100. This bit
makes the correlation check time much longer. Nevertheless,
the maximum computation time per one sensor state set in real-
time was below 50 ms for all datasets, it is reasonable. The
time of the transition check and identiﬁcation was negligible
and similar among the datasets due to simple probability com-
parisons with the transition matrix and bit value comparisons,
respectively.
D. Correlation Degree
We calculated the correlation degree, which is an indicator
of how much correlation exists among the sensors. The more
the sensors there are that react together, the higher the cor-
relation degree. Thus, to quantify the degree of correlation,
we calculated the average number of activated sensors per
group (i.e., unique sensor sets). Table IV is the summary
of the correlation degree of each dataset. houseA had the
lowest correlation degree, which was 1.4. In other words,
for every unique sensor state set, one or two sensors have
been activated simultaneously on average. The DICE datasets
for a real-world Smart Home had the highest correlation
degree of 10.6, and twor and hh102 had more number of
deployed sensors. This shows that the number of sensors and
the correlation degree are not directly proportional. Instead, the
619
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:47:10 UTC from IEEE Xplore.  Restrictions apply. 
Correlation Check
Transition Check
Identification
32.7  49.7  31.7  34.1  32.4  31.7  32.5 
5
4
3
2
1
0
)
s
m
(
e
m
T
i
Fig. 11: Computation Time
Correlation
Transition
100%
80%
60%
40%
20%
0%
s
k
c
e
h
c
f
o
s
e
p
y
t
o
w
t
f
o
o
i
t
a
R
Fig. 12: Ratio of Detection by Correlation Check and by
Transition Check based on Fault Types
accuracy and the detection/identiﬁcation time was dependent
on the correlation degree of the datasets. Datasets with higher
correlation degree achieved better detection and identiﬁcation
accuracy. In the same way, our system detected and identiﬁed
the faulty sensors of the DICE datasets much faster than
those of the other datasets with lower correlation degree. In
houseA, which had the lowest correlation degree, we had
lower accuracy and slower detection/identiﬁcation time than
other datasets. Nevertheless, the accuracy of detection and
identiﬁcation in houseA is acceptable.
E. Ratio of Detected Faults
We analyzed the ratio of faults detected in the correlation
check and transition check based on fault types (Fig. 12). All
fail-stop faults were detected during the correlation check,
while most stuck-at faults were detected in the transition
check. The reason is because fail-stop faults easily altered
the correlation among sensors; thus, they were easily dis-
coverable by the correlation check alone. However, the cor-
relation relationship was maintained even with the presence
of stuck-at faults that continuously reported the same values
after the fault. DICE detects them with the transition check
that captured the abnormal behaviors of such sensors. We
are convinced that simple fail-stop faults may be detected
by techniques proposed in prior art, but transition check is
required to detect non-fail-stop faults of various types.
VI. DISCUSSION
Multi-user cases. We considered a Smart Home with one
or two occupants in our experiments. Note that all ﬁve hetero-
geneous sensor failure detection solutions in Section II-C also
targeted a single or a two-occupant Smart Home. However,
DICE works for Smart Homes with multiple residents when
the residents are present during the precomputation phase. In
such a case, the number of unique sensor state sets may grow
exponentially with the increasing number of residents due to
the increase in the possible combination sets.
For example, the sensors in the kitchen show a high corre-
lation. In a single-resident case, other sensors are not likely
to react to the user; thus, only the kitchen sensors are likely
to be marked as activated in the sensor state set. However,
in a multiple-resident case, other sensors in the bathroom or
bedroom may react to a different user simultaneously, which
increases the possibility of different sensor combinations. This
may decrease the accuracy or increase the computation cost
and time of DICE.
To resolve the problem, a user may group the sensors
that are spatially closely located and connect each group to
DICE individually to restrain the growing number of com-
binations. For example, the sensors in the kitchen can be
grouped together and run DICE separately from the sensors
in the bathroom or bedroom. We defer the multi-user case
experiments for future work.
Multi-fault cases. We inserted one sensor fault at a time in
our experiment, but multiple faults may occur simultaneously
in Smart Homes. Although such a case is much less likely
to occur, we randomly selected one to three faulty sensors
to generate faults simultaneously to examine the result. In the
multi-fault case, we set the value of numThre (in Section III-D)
to 3. The average precision and recall for identifying multiple
faulty sensors were 79.5% and 63.3%, respectively, which is
within a reasonable range.
Impact of different parameters. The precomputation pe-
riod affected the accuracy of DICE. When the precomputa-
tion period was too short, DICE could not extract enough
context of sensors and actuators that represent their normal
behaviors. When we used the ﬁrst 150 hours of the datasets
as the precomputation data, the precision of the identiﬁcation
decreased by 10%. Thus, the longer the precomputation period,
the higher the precision. The size of the segment also affected
the accuracy. When we reduced the size of the segment into
three hours, the recall of the identiﬁcation decreased by 6%.
The reason is because faults which maintained the correlation
did not make any illegal transitions within three hours. Thus,
the longer the segment, the higher the recall.
The duration of the sensor state set was also an important
factor. When the duration was too short, DICE could not accu-
rately group the correlated sensors due to the time difference
of the sensors’ reaction. When the duration was too long,
620
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:47:10 UTC from IEEE Xplore.  Restrictions apply. 
DICE grouped the uncorrelated sensors that reacted within the
duration together. The optimal duration we empirically found
was one minute.
Expand to security. Correlation analysis helps IoT systems
to ﬁnd malicious IoT devices [9]. It has detected any sensors
showing uncorrelated activity and modeled sensors as binary
active sensors. DICE can detect malicious numeric sensors in
addition to binary sensors, but cannot detect passive malicious
sensors or information leaking sensors at this moment. We are
currently working on this part.
VII. CONCLUSION
A system that detects and identiﬁes faulty devices is
imperative in Smart Homes to provide reliable services to
users. However, prior art has been inapplicable to real-world
Smart Homes because of their lack of usability, generality,
feasibility, and promptness. In this research, we proposed
DICE, a future-oriented, context-based method to detect faulty
IoT devices. In the precomputation phase, we extracted the
context information based on the correlation among sensors
and the transition of the sensor and actuator states without
annotating the activities or requesting supplementary infor-
mation. In the real-time phase, we examined if real-time
sensor and actuator data violate the precomputed context and
detected faulty sensors. We then compared the problematic
context with the most probable context to identify the faulty
sensor. Based on this organization, we tested the system on
both the publicly available datasets and datasets collected
from our own Smart Home testbed. DICE identiﬁed faulty
devices successfully with an average of 94.9% precision and
92.5% recall. Our system took an average 3 minutes to detect
faults and average 28 minutes to identify faulty devices. In
optimal conditions, the time was reduced down to 7 minutes.
DICE promptly detects and identiﬁes sensor faults with high
accuracy, contributing to building a highly reliable Smart
Home.
ACKNOWLEDGMENTS
This work was supported by Samsung Research Funding
Center of Samsung Electronics under Project Number SRFC-
TB1403-04.
REFERENCES
[1] T. W. Hnat, V. Srinivasan, J. Lu, T. I. Sookoor, R. Dawson, J. Stankovic,
and K. Whitehouse, “The hitchhiker’s guide to successful residential
sensing deployments,” in ACM Conference on Embedded Networked
Sensor Systems, 2011.
[2] T. Kavitha and D. Sridharan, “Security vulnerabilities in wireless sensor
networks: A survey,” Journal of Information Assurance and Security,
vol. 5, no. 1, pp. 31–44, 2010.
[3] G. Padmavathi and D. Shanmugapriya, “A survey of attacks, security
mechanisms and challenges in wireless sensor networks,” International
Journal of Computer Science and Information Security, vol. 4, no. 1&2,
2009.
[4] K. Ni, N. Ramanathan, M. N. H. Chehade, L. Balzano, S. Nair,
S. Zahedi, E. Kohler, G. Pottie, M. Hansen, and M. Srivastava, “Sensor
network data fault
types,” ACM Transactions on Sensor Networks
(TOSN), vol. 5, no. 3, p. 25, 2009.
[5] K. Kapitanova, E. Hoque, J. A. Stankovic, K. Whitehouse, and S. H.
Son, “Being smart about failures: assessing repairs in smart homes,” in
ACM Conference on Ubiquitous Computing, 2012.
[6] P. A. Kodeswaran, R. Kokku, S. Sen, and M. Srivatsa, “Idea: A system
for efﬁcient failure management in smart iot environments,” in ACM
International Conference on Mobile Systems, Applications, and Services,
2016.
[7] S. Munir and J. A. Stankovic, “Failuresense: Detecting sensor failure us-
ing electrical appliances in the home,” in IEEE International Conference
on Mobile Ad Hoc and Sensor Systems (MASS), 2014.
[8] J. Ye, G. Stevenson, and S. Dobson, “Detecting abnormal events on
binary sensors in smart home environments,” Pervasive and Mobile
Computing, vol. 33, pp. 32–49, 2016.
[9] A. K. Sikder, H. Aksu, and A. S. Uluagac, “6thsense: A context-aware
sensor-based attack detector for smart devices,” in USENIX Security
Symposium, 2017.
[10] J. Ye, S. Dobson, and S. McKeever, “Situation identiﬁcation techniques
in pervasive computing: A review,” Pervasive and Mobile Computing,
vol. 8, no. 1, pp. 36–66, 2012.
[11] J. Park, R. Ivanov, J. Weimer, M. Pajic, and I. Lee, “Sensor attack
detection in the presence of transient faults,” in ACM/IEEE International
Conference on Cyber-Physical Systems, 2015.
[12] S. Rost and H. Balakrishnan, “Memento: A health monitoring system
for wireless sensor networks,” in IEEE Communications on Sensor and
Ad Hoc Communications and Networks (SECon), 2006.
[13] B.-R. Chen, G. Peterson, G. Mainland, and M. Welsh, “Livenet: Using
passive monitoring to reconstruct sensor network dynamics,” in Interna-
tional Conference on Distributed Computing in Sensor Systems, 2008.
[14] R. N. Duche and N. P. Sarwade, “Sensor node failure detection based
on round trip delay and paths in WSNs,” IEEE Sensors Journal, vol. 14,
no. 2, pp. 455–463, 2014.
[15] N. Ramanathan, K. Chang, R. Kapur, L. Girod, E. Kohler, and D. Estrin,
“Sympathy for the sensor network debugger,” in International Confer-
ence on Embedded Networked Sensor Systems, 2005.
[16] I. C. Paschalidis and Y. Chen, “Statistical anomaly detection with sensor
networks,” ACM Transactions on Sensor Networks (TOSN), vol. 7, no. 2,
p. 17, 2010.
[17] I. M. Atakli, H. Hu, Y. Chen, W. S. Ku, and Z. Su, “Malicious node
detection in wireless sensor networks using weighted trust evaluation,”
in Spring Simulation Multiconference (SpringSim), 2008.
[18] M. Ding, D. Chen, K. Xing, and X. Cheng, “Localized fault-tolerant
event boundary detection in sensor networks,” in IEEE INFOCOM,
2005.
[19] J. Chen, S. Kher, and A. Somani, “Distributed fault detection of wireless
sensor networks,” in Workshop on Dependability Issues in Wireless Ad
Hoc Networks and Sensor Networks, 2006.
[20] A. B. Sharma, L. Golubchik, and R. Govindan, “Sensor faults: Detection
methods and prevalence in real-world datasets,” ACM Transactions on
Sensor Networks (TOSN), vol. 6, no. 3, p. 23, 2010.
[21] L. Fang and S. Dobson, “Unifying sensor fault detection with energy
conservation,” in International Workshop on Self-Organizing Systems,
2013.
[22] S. Ganeriwal, L. K. Balzano, and M. B. Srivastava, “Reputation-based
framework for high integrity sensor networks,” ACM Transactions on
Sensor Networks (TOSN), vol. 4, no. 3, p. 15, 2008.
[23] S. Guo, Z. Zhong, and T. He, “Find: faulty node detection for wireless
sensor networks,” in ACM Conference on Embedded Networked Sensor
Systems, 2009.
[24] D. J. Hill, B. S. Minsker, and E. Amir, “Real-time bayesian anomaly
detection for environmental sensor data,” in Congress-International
Association for Hydraulic Research, vol. 32, no. 2, 2007.
[25] J. Voas, “Networks of ‘Things’,” National Institute of Standards and
Technology (NIST) Special Publication, vol. 800-183, 2016.
[26] W. R. Gilks, S. Richardson, and D. Spiegelhalter, Markov chain Monte
Carlo in practice. CRC press, 1995.
[27] “WSU CASAS Datasets.” http://ailab.wsu.edu/casas/datasets.html, ac-
cessed: 2017-09-30.
cessed: 2017-09-30.
[28] “ISLA Datasets.” https://sites.google.com/site/tim0306/datasets,
ac-
[29] T. L. van Kasteren, G. Englebienne, and B. J. Kr¨ose, “Human activity
recognition from wireless sensor network data: Benchmark and soft-
ware,” in Activity Recognition in Pervasive Intelligent Environments,
2011, pp. 165–186.
[30] D. J. Cook, A. S. Crandall, B. L. Thomas, and N. C. Krishnan, “Casas:
A smart home in a box,” IEEE Computer, vol. 46, no. 7, pp. 62–69,
2013.
[31] “IoTivity Website.” https://www.iotivity.org/, accessed: 2017-09-30.
621
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:47:10 UTC from IEEE Xplore.  Restrictions apply.