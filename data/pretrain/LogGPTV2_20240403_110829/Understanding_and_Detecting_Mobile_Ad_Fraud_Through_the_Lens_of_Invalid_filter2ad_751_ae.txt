Ad fraud measurement and detection. Over the past few years,
click spam has been extensively studied in the context of web adver-
tising, mobile advertising, and search advertising. The research [30]
proposed the characterization of one of the largest click fraud bot-
nets. Researchers also proposed several types of design and analysis
of click spam threats [10, 11, 16, 39, 41]. To defend against click
spam, many approaches have been proposed to avoid or detect click
spam in advertising [13, 14, 17, 27, 38, 40, 45]. Springborn et al. [37]
leveraged trac collected from honeypot websites to identify and
analyze a new type of ad fraud, called pay-per-view (PPV) networks.
They examined the click spam issue as well. However, we high-
light that the industry focus, driven by advertising monetization,
has been shifted from click spamming to invalid trac-enabled
coordinated attacks.
To the best of our knowledge, there is only one recent study inves-
tigating invalid trac [29]. The researchers designed a condence
score for each domain, based on the IP entropy. The condence
score oered for each app domain is useful for DSP to determine
how to treat the upcoming bid requests. However, this method
cannot ascertain what session of ad trac is invalid; neither can
be used to measure the ad trac at a ner granularity. Instead, the
methodology developed in this paper is able to identify the sources
of invalid trac (i.e., fraudulent devices).
System-level ad fraud prevention. Some researchers proposed
authentication-based methods to eliminate fraudulent activities in
advertising. For example, Juels et al. [21] proposed an authentication
method to validate benign users. In [15] and [36], researchers used
HMAC-based signatures to check ad click fraud. Li et al. [23] used
TrustZone to verify ad clicks and display. However, these solutions
rely on the client side’s ability to detect anomalies and thus have
reduced scalability.
To prevent various types of counterfeit inventories across the
advertising ecosystem, by boosting transparency in the supply
chain, Interactive Advertising Bureau (IAB) Tech Lab launched the
authorized digital sellers (ads.txt) project [18]. The project is aimed
at publishers and distributors to declare who is authorized to sell
their inventory. Furthermore, there are several extended versions
of ads.txt, including app-ads.txt [19] and ads.cert [20] to extend
to more scenarios. Recently, Pastor et al. [28] proposed another
extended version, called ads.chain, to resolve the limitations of the
previous protocols. However, all of those solutions are designed to
increase the transparency in the ecosystem, which is orthogonal to
invalid trac detection proposed in this paper.
11 CONCLUSION
In this paper, we rst conduct a measurement study on a labeled
ad fraud dataset to distinguish the nature of mobile devices either
fraudulent or benign through feature engineering. We then propose
and develop EH, the rst mobile ad fraud detection system
based on ad bid request logs, which can identify fraudulent devices
with high accuracy and automatically identify fraudulent clusters.
We reveal several cheating strategies adopted by click farms based
on the results of EH. We further deploy optimized E
H on a 1-day’s real-world dataset, which demonstrates its
practicality. The results and ndings developed in this paper have
been acknowledged, and the proposed EH will be inte-
grated into the platform of our industry partner, a leading ad trac
verication company (Company A), to combat the current burgeon-
ing mobile ad fraud.
ACKNOWLEDGEMENTS
We are grateful to the anonymous reviewers for their constructive
feedback. We also thank RTBAsia and China Advertising Associa-
tion for the long term support. The authors aliated with Shang-
hai Jiao Tong University were, in part, supported by the National
Natural Science Foundation of China under Grants 61972453 and
62132013. Xiaokuan Zhang was supported in part by the Norton-
LifeLock Research Group Graduate Fellowship. Minhui Xue was,
in part, supported by the Australian Research Council (ARC) Dis-
covery Project (DP210102670) and the Research Center for Cyber
Security at Tel Aviv University established by the State of Israel,
the Prime Minister’s Oce and Tel Aviv University.
REFERENCES
[1] 2015. WebView for Android.
webview/overview.
[2] 2020. Codenames, Tags, and Build Numbers. https://source.android.com/setup/
https://developer.chrome.com/multidevice/
start/build-numbers. (Accessed on 09/02/2020).
[3] 2020. Integral Ad Science. https://integralads.com/.
[4] 2020. Oracle Data Cloud. https://www.oracle.com/data-cloud/.
[5] 2020. White Ops. https://www.whiteops.com/.
[6] 2021.
IP-API.com - Geolocation API. https://ip-api.com/.
01/21/2021).
(Accessed on
[7] Apple. 2021. Upcoming AppTrackingTransparency requirements. https://
developer.apple.com/news/?id=ecvrtzt2. (Accessed on 04/21/2021).
[8] Vincent D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefeb-
vre. 2008. Fast unfolding of communities in large networks. Journal of Sta-
tistical Mechanics: Theory and Experiment 2008, 10 (2008), P10008.
https:
//doi.org/10.1088/1742-5468/2008/10/p10008
[9] Chih-Chung Chang and Chih-Jen Lin. 2011.
LIBSVM: A library for sup-
port vector machines. ACM Transactions on Intelligent Systems and Technol-
https://doi.org/10.1145/1961189.1961199 Software available at
ogy (2011).
http://www.csie.ntu.edu.tw/~cjlin/libsvm.
[10] Geumhwan Cho, Junsung Cho, Youngbae Song, Donghyun Choi, and Hyoung-
shick Kim. 2016. Combating online fraud attacks in mobile-based advertising.
EURASIP Journal on Information Security 2016 (2016). https://doi.org/10.1186/
s13635-015-0027-7
[11] Geumhwan Cho, Junsung Cho, Youngbae Song, and Hyoungshick Kim. 2015.
An empirical study of click fraud in mobile advertising networks. In 2015 10th
International Conference on Availability, Reliability and Security. 382–388. https:
//doi.org/10.1109/ARES.2015.62
[12] CNBC. 2017. Businesses could lose $16.4 billion to online advert fraud in
2017. https://www.cnbc.com/2017/03/15/businesses-could-lose-164-billion-to-
online-advert-fraud-in-2017.html. (Accessed on 08/08/2020).
[13] Vacha Dave, Saikat Guha, and Yin Zhang. 2012. Measuring andngerprinting
click-spam in ad networks. In Proceedings of the ACM SIGCOMM 2012 conference
on Applications, technologies, architectures, and protocols for computer communi-
cation. 175–186. https://doi.org/10.1145/2342356.2342394
[14] Vacha Dave, Saikat Guha, and Yin Zhang. 2013. ViceROI: Catching click-spam
in search ad networks. In Proceedings of the 2013 ACM SIGSAC conference on
Session 1D: Authentication and Click Fraud CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea299Computer & Communications Security (CCS ’13). 765–776. https://doi.org/10.1145/
2508859.2516688
[15] Michael Dietz, Shashi Shekhar, Yuliy Pisetsky, Anhei Shu, and Dan S Wallach.
2011. QUIRE: Lightweight provenance for smart phone operating systems. In
20th USENIX Security Symposium (USENIX Security 11).
[16] Google, White Ops. 2018. The Hunt for 3ve, Taking down a major ad fraud
operation through industry collaboration. https://services.google.com/fh/les/
blogs/3ve_google_whiteops_whitepaper_nal_nov_2018.pdf.
[17] Hamed Haddadi. 2010. Fighting Online Click-Fraud Using Blu Ads. SIG-
COMM Comput. Commun. Rev. 40, 2 (April 2010), 21–25. https://doi.org/10.1145/
1764873.1764877
[18] IAB Tech Lab. 2019.
IAB OpenRTB Ads.txt Public Specication Ver-
sion 1.0.2. https://iabtechlab.com/wp-content/uploads/2019/03/IAB-OpenRTB-
Ads.txt-Public-Spec-1.0.2.pdf. (Accessed on 08/30/2020).
[19] IAB Tech Lab. 2019. IAB Tech Lab Authorized Sellers for Apps (app-ads.txt). https:
//iabtechlab.com/wp-content/uploads/2019/03/app-ads.txt-v1.0-nal- .pdf. (Ac-
cessed on 08/30/2020).
[20] IAB Tech Lab. 2020. OpenRTB, Specication Version 3.0.
[21] Ari Juels, Sid Stamm, and Markus Jakobsson. 2007. Combating click fraud via
premium clicks. In 16th USENIX Security Symposium (USENIX Security 07).
[22] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei
Ye, and Tie-Yan Liu. 2017. Lightgbm: A highly ecient gradient boosting decision
tree. In Proceedings of the 31st International Conference on Neural Information
Processing Systems (NIPS’17). 3149–3157.
[23] Wenhao Li, Haibo Li, Haibo Chen, and Yubin Xia. 2015. AdAttester: Secure
online mobile advertisement attestation using TrustZone. In Proceedings of the
13th Annual International Conference on Mobile Systems, Applications, and Services
(MobiSys ’15). https://doi.org/10.1145/2742647.2742676
[24] Media Rating Council. 2020.
Invalid Trac Detection and Filtration Stan-
dards Addendum. http://mediaratingcouncil.org/MRC%20Invalid%20Trac%
20Detection%20and%20Filtration.pdf. (Accessed on 07/18/2020).
[25] Xianghang Mi, Xuan Feng, Xiaojing Liao, Baojun Liu, XiaoFeng Wang, Feng
Qian, Zhou Li, Sumayah Alrwais, Limin Sun, and Ying Liu. 2019. Resident evil:
Understanding residential IP proxy as a dark service. In 2019 IEEE Symposium on
Security and Privacy (SP). 1185–1201. https://doi.org/10.1109/SP.2019.00011
[26] Mirror. 2017. The bizarre ‘click farm’ of 10,000 phones that give FAKE ‘likes’
to our most-loved apps. https://www.mirror.co.uk/news/world-news/bizarre-
click-farm-10000-phones-10419403. (Accessed on 08/26/2020).
[27] Richard Oentaryo, Ee-Peng Lim, Michael Finegold, David Lo, Feida Zhu, Clifton
Phua, Eng-Yeow Cheu, Ghim-Eng Yap, Kelvin Sim, Minh Nhut Nguyen, Kasun
Perera, Bijay Neupane, Mustafa Faisal, Zeyar Aung, Wei Lee Woon, Wei Chen,
Dhaval Patel, and Daniel Berrar. 2014. Detecting click fraud in online advertising:
A data mining approach. Journal of Machine Learning Research 15 (2014), 99–140.
[28] Antonio Pastor, Rubén Cuevas, Ángel Cuevas, and Arturo Azcorra. 2021. Estab-
lishing Trust in Online Advertising With Signed Transactions. IEEE Access 9
(2021), 2401–2414. https://doi.org/10.1109/ACCESS.2020.3047343
[29] Antonio Pastor, Matti Pärssinen, Patricia Callejo, Pelayo Vallina, Rubén Cuevas,
Ángel Cuevas, Mikko Kotila, and Arturo Azcorra. 2019. Nameles: An intelligent
system for Real-Time Filtering of Invalid Ad Trac. In The World Wide Web
Conference (WWW ’19). 1454–1464.
[30] Paul Pearce, Vacha Dave, Chris Grier, Kirill Levchenko, Saikat Guha, Damon
McCoy, Vern Paxson, Stefan Savage, and Georey M Voelker. 2014. Characterizing
large-scale click fraud in ZeroAccess. In Proceedings of the 2014 ACM SIGSAC
Conference on Computer and Communications Security (CCS ’14). 141–152. https:
//doi.org/10.1145/2660267.2660369
[31] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel,
Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau,
Matthieu Brucher, Matthieu Perrot, and Édouard Duchesnay. 2011. Scikit-learn:
Machine learning in Python. Journal of Machine Learning Research 12 (2011),
2825–2830.
[32] PPC Protect. 2019. What Is a Click Farm? The Quick Way to Thousands of Likes.
https://ppcprotect.com/what-is-a-click-farm/. (Accessed on 08/26/2020).
[33] Research and Markets. 2019. Worldwide Analysis on the Real-time Bidding
Market, 2019 to 2024 - Anticipated to Record a CAGR of 32.9% During the
https://www.prnewswire.com/news-releases/worldwide-
Forecast Period.
analysis-on-the-real-time-bidding-market-2019-to-2024---anticipated-to-
record-a-cagr-of-32-9-during-the-forecast-period-300811841.html. (Accessed
on 07/21/2020).
[34] Neil Rubens, Mehdi Elahi, Masashi Sugiyama, and Dain Kaplan. 2015. Active
Learning in Recommender Systems. Springer US, Boston, MA, 809–846.
[35] Burr Settles. 2010. Active Learning Literature Survey. University of Wisconsin,
Madison 52 (07 2010).
[36] Shashi Shekhar, Michael Dietz, and Dan S Wallach. 2012. AdSplit: Separating
smartphone advertising from applications. In 21st USENIX Security Symposium
(USENIX Security 12). 553–567.
[37] Kevin Springborn and Paul Barford. 2013. Impression fraud in on-line advertising
via pay-per-view networks. In 22nd USENIX Security Symposium (USENIX Security
13). 211–226.
[38] Ori Stitelman, Claudia Perlich, Brian Dalessandro, Rod Hook, Troy Raeder, and
Foster Provost. 2013. Using co-visitation networks for detecting large scale online
display advertising exchange fraud. In Proceedings of the 19th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining (KDD ’13).
1240–1248. https://doi.org/10.1145/2487575.2488207
[39] Brett Stone-Gross, Ryan Stevens, Apostolis Zarras, Richard Kemmerer, Chris
Kruegel, and Giovanni Vigna. 2011. Understanding fraudulent activities in on-
line ad exchanges. In Proceedings of the 2011 ACM SIGCOMM Conference on
Internet Measurement Conference (IMC ’11). 279–294. https://doi.org/10.1145/
2068816.2068843
[40] Tian Tian, Jun Zhu, Fen Xia, Xin Zhuang, and Tong Zhang. 2015. Crowd fraud
detection in internet advertising. In Proceedings of the 24th International Con-
ference on World Wide Web (WWW ’15). 1100–1110. https://doi.org/10.1145/
2736277.2741136
[41] Elliott Wen, Jiannong Cao, Jiaxing Shen, and Xuefeng Liu. 2018. Fraus: Launching
cost-ecient and scalable mobile click fraud has never been so easy. In 2018
IEEE Conference on Communications and Network Security (CNS). 1–9. https:
//doi.org/10.1109/CNS.2018.8433126
[42] White Ops and ANA. 2019. 2018-2019 Bot Baseline: Fraud in Digital Advertising.
https://www.ana.net/miccontent/show/id/rr-2019-bot-baseline. (Accessed on
07/29/2020).
[43] Songyang Wu, Wenqi Sun, Xin Liu, and Yong Zhang. 2018. Forensics on Twitter
and WeChat using a customised android emulator. In 2018 IEEE 4th International
Conference on Computer and Communications (ICCC). 602–608. https://doi.org/
10.1109/CompComm.2018.8781056
[44] XDA Developers. 2018. Huawei will stop providing bootloader unlocking
for all new devices. https://www.xda-developers.com/huawei-stop-providing-
bootloader-unlock-codes/. (Accessed on 09/03/2020).
[45] Haitao Xu, Daiping Liu, Aaron Koehl, Haining Wang, and Angelos Stavrou. 2014.
Click fraud detection on the advertiser side. In European Symposium on Research
in Computer Security (ESORICS 2014). 419–438.
[46] Haizhong Zheng, Minhui Xue, Hao Lu, Shuang Hao, Haojin Zhu, Xiaohui Liang,
and Keith W. Ross. 2018. Smoke screener or straight shooter: Detecting elite sybil
attacks in user-review social networks. In Proceedings of 25th Annual Network
and Distributed System Security Symposium, NDSS.
APPENDIX
A SENSITIVITY OF PARAMETERS
We evaluate the sensitivity of parameter settings of EH
(Sec. 6.1, Table 4). The parameters are listed in Table 4. We start
from an initial setting ([ = 5,(8BB(:) as the ratio of discarded ad bid
logs compared to [ = 1 when setting [ = :. As shown in Fig. 13,
there is a tradeo between loss and the time cost when [ increases;
loss is decreased to 0.4% when [ = 5. Therefore, we choose [ = 5 as
the optimal setting.
Y imthr. Similar to [, we also nd that (8= 0.7, =D= 0.2; it plateaus between
(8<C⌘A = 0.5 and (8<C⌘A = 0.6. Based on the above observations,
we choose (8<C⌘A = 0.5.
sthr and ". We use similar methods to choose the best values
of BC⌘A and U. Because they are only involved in the aggregation
stage (Stage 3), their impact on the time cost is negligible. When
BC⌘A increases, the accuracy and precision increase while the recall
drops (Fig. 15). BC⌘A = 0.3 is the turning point, so we choose BC⌘A =
0.3 as our setting. For U (Fig. 16), when U increases, accuracy and
precision rst increase until U reaches 10 3, then decrease after
that. Meanwhile, recall drops slowly all the time. Therefore, we
choose U = 10 3.
Summary. Based on our evaluation, we use ([ = 5,(8< C⌘A =
0.5,B C⌘A = 0.3,U = 10 3) as the optimal settings, and use the set-
tings in the paper.
B SYSTEM UPDATE
In practice, attackers will keep evolving their cheating strategies
to avoid detection. Therefore, EH must be able to update
periodically. In this section, we present a simplied update scheme,
which updates EH per week.
Methodology. In Stage 1, we periodically retrain the classier
using an active learning approach [34, 35]. At the end of each week,
we collect the devices labeled by EH in the last week (7
days) and use them to retrain the classier if the condence of the
prediction is high, e.g., the predicted score is within [0, 0.1] (for
benign devices) or [0.9, 1.0] (for fraudulent devices). Alternatively,
new datasets can be obtained using other means (e.g., from other
companies) to retrain the classier. For the threshold parameters
used in Stages II and III, we keep these parameters as xed values
for incoming new datasets until the result of oine cross-validation
Figure 18: Log number distribution of the top 50 apps.
signicantly drops. Once it happens, we search for the threshold
parameters as we did in Appendix A.
Evaluation. We use ⇡2020 to perform an evaluation of our weekly
updating scheme. We compare the accuracy with and without up-
dating in Fig. 17. The rst model (M1, in blue) is trained with the
data of day 0. The second model (M2, in yellow) is an updated
version of M1, retrained using the labels of the rst week at the
beginning of the second week (7th day). M2 is then tested on the
dataset since the 7th day. Similarly, the third model (M3, in green)
is trained with the dataset of the rst two weeks, and tested using
the data of the last two weeks; the fourth model (M4, in red) is
trained with the data of the rst three weeks and tested using the
last week’s data. The results suggest that our updating scheme can
indeed improve the accuracy.
C PROFILING TOP 50 APPS
To have a deeper insight into the ad fraud caused by invalid trac
in 2018, we use the following method to build suitable versions of
EH in 2018 and prole top 50 apps.
Methodology.
Since fraudulent devices may exhibit dierent features in dier-
ent years, we can not directly apply the trained model in 2020 to
predict the old devices in 2018. Thus we retrieve the device IDs of
the labeled devices in 2020 from the full bid logs of 2018. As a result,
we found a total of 3,840 fraudulent and 5,070 benign devices in 2018.
Session 1D: Authentication and Click Fraud CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea301to a previous invalid trac ratio), and an app fraudulent degree