### Ad Fraud Measurement and Detection

Over the past few years, click spam has been extensively studied in the context of web, mobile, and search advertising. Research [30] characterized one of the largest click fraud botnets, while other studies [10, 11, 16, 39, 41] proposed various methods for designing and analyzing click spam threats. To combat click spam, numerous approaches have been developed to avoid or detect such fraudulent activities in advertising [13, 14, 17, 27, 38, 40, 45]. Springborn et al. [37] used traffic data from honeypot websites to identify and analyze a new type of ad fraud called pay-per-view (PPV) networks, also examining the click spam issue. However, the industry focus, driven by advertising monetization, has shifted from click spamming to more sophisticated, invalid traffic-enabled coordinated attacks.

To the best of our knowledge, only one recent study [29] has investigated invalid traffic. The researchers designed a confidence score for each domain based on IP entropy, which is useful for Demand-Side Platforms (DSPs) to determine how to handle upcoming bid requests. However, this method cannot pinpoint specific sessions of invalid traffic or measure it at a finer granularity. In contrast, the methodology developed in this paper can identify the sources of invalid traffic, specifically fraudulent devices.

### System-Level Ad Fraud Prevention

Some researchers have proposed authentication-based methods to eliminate fraudulent activities in advertising. For example, Juels et al. [21] introduced an authentication method to validate benign users, while others [15, 36] used HMAC-based signatures to check ad click fraud. Li et al. [23] utilized TrustZone to verify ad clicks and displays. However, these solutions rely on the client-side's ability to detect anomalies, which limits their scalability.

To increase transparency in the supply chain and prevent various types of counterfeit inventories across the advertising ecosystem, the Interactive Advertising Bureau (IAB) Tech Lab launched the authorized digital sellers (ads.txt) project [18]. This project allows publishers and distributors to declare who is authorized to sell their inventory. Several extended versions, including app-ads.txt [19] and ads.cert [20], have been developed to cover more scenarios. Recently, Pastor et al. [28] proposed ads.chain, another extended version, to address the limitations of previous protocols. However, these solutions are designed to enhance transparency, which is orthogonal to the invalid traffic detection proposed in this paper.

### Conclusion

In this paper, we first conducted a measurement study on a labeled ad fraud dataset to distinguish between fraudulent and benign mobile devices through feature engineering. We then proposed and developed EH, the first mobile ad fraud detection system based on ad bid request logs, which can identify fraudulent devices with high accuracy and automatically identify fraudulent clusters. Our analysis revealed several cheating strategies employed by click farms using EH. We further deployed an optimized version of EH on a real-world dataset spanning one day, demonstrating its practicality. The results and findings from this paper have been acknowledged, and the proposed EH will be integrated into the platform of our industry partner, a leading ad traffic verification company (Company A), to combat the current surge in mobile ad fraud.

### Acknowledgements

We are grateful to the anonymous reviewers for their constructive feedback. We also thank RTBAsia and the China Advertising Association for their long-term support. The authors affiliated with Shanghai Jiao Tong University were partially supported by the National Natural Science Foundation of China under Grants 61972453 and 62132013. Xiaokuan Zhang was partially supported by the NortonLifeLock Research Group Graduate Fellowship. Minhui Xue was partially supported by the Australian Research Council (ARC) Discovery Project (DP210102670) and the Research Center for Cyber Security at Tel Aviv University, established by the State of Israel, the Prime Minister’s Office, and Tel Aviv University.

### References

[References listed as provided, with no changes needed.]

### Appendix

#### A. Sensitivity of Parameters

We evaluated the sensitivity of parameter settings for EH (Section 6.1, Table 4). The parameters are listed in Table 4. Starting with an initial setting of [ = 5, we compared the ratio of discarded ad bid logs to [ = 1 when [ = :. As shown in Figure 13, there is a trade-off between loss and time cost as [ increases; the loss decreases to 0.4% when [ = 5. Therefore, we chose [ = 5 as the optimal setting.

For Y imthr, similar to [, we found that (8= 0.7, =D= 0.2, and it plateaus between (8<C⌘A = 0.5 and (8<C⌘A = 0.6. Based on these observations, we selected (8<C⌘A = 0.5.

For sthr and ", we used similar methods to choose the best values. Since they are only involved in the aggregation stage (Stage 3), their impact on the time cost is negligible. When BC⌘A increases, the accuracy and precision increase while the recall drops (Figure 15). BC⌘A = 0.3 is the turning point, so we chose BC⌘A = 0.3. For U (Figure 16), when U increases, accuracy and precision first increase until U reaches 10^-3, then decrease. Meanwhile, recall drops slowly. Therefore, we chose U = 10^-3.

**Summary:**
Based on our evaluation, we use ([ = 5, (8< C⌘A = 0.5, B C⌘A = 0.3, U = 10^-3) as the optimal settings, and these settings are used in the paper.

#### B. System Update

In practice, attackers will continually evolve their cheating strategies to avoid detection. Therefore, EH must be able to update periodically. We present a simplified update scheme, which updates EH weekly.

**Methodology:**
In Stage 1, we periodically retrain the classifier using an active learning approach [34, 35]. At the end of each week, we collect the devices labeled by EH in the last week (7 days) and use them to retrain the classifier if the confidence of the prediction is high, e.g., the predicted score is within [0, 0.1] (for benign devices) or [0.9, 1.0] (for fraudulent devices). Alternatively, new datasets can be obtained from other companies to retrain the classifier. For the threshold parameters used in Stages II and III, we keep these parameters fixed for incoming new datasets until the result of offline cross-validation significantly drops. Once it happens, we search for the threshold parameters as we did in Appendix A.

**Evaluation:**
We used ⇡2020 to evaluate our weekly updating scheme. We compared the accuracy with and without updating in Figure 17. The first model (M1, in blue) was trained with the data of day 0. The second model (M2, in yellow) is an updated version of M1, retrained using the labels of the first week at the beginning of the second week (7th day). M2 was then tested on the dataset since the 7th day. Similarly, the third model (M3, in green) was trained with the dataset of the first two weeks and tested using the data of the last two weeks; the fourth model (M4, in red) was trained with the data of the first three weeks and tested using the last week’s data. The results suggest that our updating scheme can indeed improve the accuracy.

#### C. Profiling Top 50 Apps

To gain deeper insights into the ad fraud caused by invalid traffic in 2018, we built suitable versions of EH for 2018 and profiled the top 50 apps.

**Methodology:**
Since fraudulent devices may exhibit different features in different years, we cannot directly apply the trained model from 2020 to predict old devices in 2018. Thus, we retrieved the device IDs of the labeled devices in 2020 from the full bid logs of 2018. As a result, we found a total of 3,840 fraudulent and 5,070 benign devices in 2018.