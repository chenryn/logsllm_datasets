olds
8: Send packet
9: }
Figure 3: Pulsar’s rate control function
Figure 4: Network function implementing a gen-
eralized version of PIAS’ dynamic priority logic.
well. For example, Sinbad [17] maximizes performance
by choosing endpoints for write operations in HDFS,
and Facebook’s mcrouter [40] achieves low latency and
failover through a distributed middle layer that routes
memcached requests based on their key.
2.1.2 Datacenter QoS
Many recent proposals provide quality of service (QoS)
guarantees for traﬃc across the datacenter network. We
begin with a recent QoS solution.
Pulsar [6] gives tenant-level end-to-end bandwidth
guarantees across multiple resources, such as network
and storage. A tenant refers to a collection of virtual
machines (VMs) owned by the same user. Enforcing
such guarantees at the network layer poses two key
challenges. First, it requires determining the tenant a
given packet belongs to. Second, since the guarantee
spans non-network resources, it requires knowledge of
the operation being performed. For example, consider
a packet corresponding to a request to a storage server.
Network rate limiters pace packets based on their size.
However, depending on whether the packet corresponds
to a READ or a WRITE request to a storage server,
the packet size may not correctly represent its impact
on the storage server—read packets are small on the
forward path from the client generating the request to
the server, but their cost on the server and the reverse
network path can be high depending on the read size.
Today, information about the type of an operation and
its size is not available to network rate limiters at end
hosts and at switches.
Figure 3 shows a network function implementing Pul-
sar’s rate control. We describe the state it maintains,
the computation it performs and the application seman-
tics it requires.
Computation. The function ensures that the packet
is rate limited based on the type of message (i.e., IO
operation) that it belongs to. It also queues the packet
at the rate limiter corresponding to the tenant gener-
ating the packet. This ensures aggregate tenant-level
guarantees instead of per-VM guarantees.
Application semantics. For a packet, the function
needs to determine its message, the message size, and
the tenant it belongs to.
We note that the same features apply to other schemes
for datacenter QoS that oﬀer performance guarantees
to tenants and applications [9, 39, 10, 33, 52, 61].
2.1.3 Datacenter ﬂow scheduling
Many recent proposals aim to optimize application per-
formance in terms of reduced user-perceived latency by
carefully scheduling ﬂows across the network. For ex-
ample, PIAS [8] minimizes ﬂow completion time with-
out application support by mimicking the shortest ﬂow
ﬁrst scheduling strategy. With PIAS, ﬂows start in the
highest priority but are demoted to lower priorities as
their size increases.
While PIAS [8] targets ﬂow-level prioritization, the
concept can be generalized and applied at a message
level. A message could be a ﬂow, a GET/PUT, etc. Fig-
ure 4 shows a network function implementing message-
level PIAS scheduling.
State. The function uses two pieces of global state.
First, priorityThresholds provide ﬂow-size thresholds
that deﬁne the priority for a ﬂow given its current size.
These thresholds need to be calculated periodically based
on the datacenter’s overall traﬃc load. Second, the
function maintains a msgTable to track the current size
of all active messages.
State. The function accesses global state—it reads
from the queueMap to determine the rate limited queue
a packet should go to.
Computation. When a packet for a new message ar-
rives, the function initializes a messageTable entry for
it. When a packet for an existing message arrives, the
496Function
Example
Data-plane
state
Data-plane
computation
Application Network
semantics
support
Eden
(out of the box)
Load Balancing
Replica Selection
Datacenter QoS
Flow scheduling
and congestion control
Stateful ﬁrewall
WCMP [65]
Message-based WCMP
Ananta [47]
Conga [1]
Duet [26]
mcrouter [40]
SINBAD [17]
Pulsar [6]
Storage QoS [61, 58]
Network QoS [9, 51, 38, 33]
PIAS [8]
QJump [28]
Centralized [48, 27]
congestion control
Explicit rate control
(D3 [64], PASE [45], PDQ [30])
IDS(e.g. [19])
Port knocking [13]

































∗





∗

















Table 1: Example network functions, their data-plane requirements and whether they need network
support beyond commodity features like network priorities. Eden can support many of these functions
out of the box. (∗ refers to functions that would beneﬁt from application semantics. For example,
Conga uses ﬂowlets that approximate application messages.)
message’s current size is updated. Finally, we use the
current message size to determine the packet’s priority.
memory, can provide ﬁne-grained visibility into applica-
tion semantics, and by deﬁnition, are on the data path.
Application semantics. PIAS achieves ﬂow schedul-
ing without application support. However, in closed-
environments like datacenters, it is possible to modify
applications or the OS to directly provide information
about the size of a ﬂow [64, 22, 30]. This would al-
low for shortest ﬂow ﬁrst scheduling. In Section 5, we
show that this approach, by using application informa-
tion, can provide increased accuracy compared to PIAS’
application-agnostic scheduling.
We note that the same features apply to other schemes
for optimizing application performance by reducing ﬂow
completion times [30, 45], meeting ﬂow deadlines [64] or
similar task-level metrics [18, 22].
2.2 End hosts: A natural enforcement point
Table 1 extends the previous discussion to other cate-
gories of common network functions like congestion con-
trol and stateful ﬁrewalling. The table is certainly not
exhaustive; instead, it illustrates how data-plane state,
computation and application semantics are common re-
quirements across a large selection of network functions.
Given these requirements, a natural question is where
should such functionality be implemented. Tradition-
ally, network functions have been implemented at switches
and middleboxes; this however reﬂects legacy originat-
ing from the Internet, where ISPs only control their
equipment and end hosts are not trusted. By contrast,
in closed settings owned and operated by a single en-
tity, like datacenters and enterprises, end hosts (or some
parts of the end hosts) are trusted. Thus, end hosts are
well-suited to implement network functions—they have
high computational capabilities, signiﬁcant amounts of
Placing network functionality at end hosts provides
also additional beneﬁts. First, enforcing a function at
the source of traﬃc instead of distributed enforcement
along its path makes it easier to achieve consistent be-
havior even when the function is updated. On the con-
trary, achieving consistent updates in the network re-
quires complex mechanisms to ensure that all switches
along the path make consistent decisions [55]. Second,
implementing network functions at end hosts leads to a
natural partitioning of both the state and computation
overhead of enforcement because hosts are only respon-
sible for a subset of the traﬃc a switch would need to
process. Finally, certain network properties such as ﬂow
RTT or loss and network events like ﬂow termination
are readily available at the hosts; instead, such prop-
erties need to be inferred, typically through heuristics
inside the network.
Of course, implementing network functionality at end
hosts is not a panacea. As shown in Table 1, some func-
tions do require network support beyond what is oﬀered
by commodity equipment. For example, deadline-based
congestion control protocols like D3 [64] and PDQ [30]
rely on explicit, ﬁne-grained feedback from the network.
In general, network switches have better visibility of in-
stantaneous aggregate network behavior and are needed
to implement network functions that rely on such infor-
mation. Furthermore, despite the resources available at
end hosts, some network functions, such as compression
or encryption, may be too resource intensive for general
purpose processors and may require custom hardware.
In summary, a variety of network functions can indeed
be implemented with a modicum of computation and
497state at the end host data plane. This paper proposes
an architecture to enable such functions.
3 Design
This section provides an overview of the Eden archi-
tecture. Eden targets environments in which end hosts
are owned by a single administrative domain and can
therefore be trusted. In this paper, we focus on data-
centers, although our approach can also be applied to
other scenarios such as enterprises [21, 34].
3.1 Design overview
Eden comprises three components (Figure 5):
1). Controller. It provides a logically centralized co-
ordination point where any part of the network function
logic requiring global visibility resides (§3.2).
2). Stage. Applications are a ﬁrst-order citizen in
Eden. Any application, library or even kernel module
in the end-host stack that is Eden-compliant is called
a “stage” (§3.3). Stages can classify packets based on
stage-speciﬁc semantics, and this classiﬁcation is carried
through the end host stack to the Eden enclave where it
is used to determine the enclave rule(s) to apply. Stages
are programmed using the classiﬁcation API which al-
lows the controller to learn their classiﬁcation abilities
and to conﬁgure them with classiﬁcation rules.
3). Enclave. Eden provides a programmable data-
plane through an enclave at each end host (§3.4). The
enclave may be implemented at the software network
stack inside the OS or the hypervisor, or using pro-
grammable hardware like FPGAs and programmable
NICs. The enclave is programmed by the controller us-
ing the enclave API. An enclave program comprises a
set of “match-action” rules. However, instead of match-
ing on packet headers, packets are matched based on the
class associated with each packet. Finally, instead of a
pre-deﬁned set of actions, the action part of the match-
action rule is an action function comprising logic that
can access and modify enclave and class state.
3.2 Eden controller
A network function is conceptually a combination of
a control-plane function residing at the controller and
a data-plane function. Given a network function, all
computation that either needs global network visibility
or needs to be invoked at coarse timescales can reside in
the control function. By contrast, any computation that
needs to be invoked on a per-packet basis or needs to
access rapidly changing state should be part of the data
function. For our WCMP example (§2.1.1), the control
plane function involves (periodically) determining the
weight associated with each network link based on the
current topology and traﬃc matrix. These weights are
used to compute the pathMatrix variable used in the
WCMP data plane function shown in Figure 2.
Recent work in the SDN literature mostly focuses on
controller-based algorithms [24] and languages for ex-
pressing control plane functions [25, 62]. Hence, in the
Figure 5: The Eden architecture.
rest of this paper, we focus on how data plane func-
tions can be implemented at stages and enclaves, and
the APIs that the Eden controller uses to program such
functionality.
3.3 Eden stages
As shown in Table 1, many network functions require
application semantics as they operate at the granularity
of application messages. By contrast, the Eden enclave
operates at the granularity of packets. Traditional ap-
proaches infer the application message a given packet
belongs to using deep packet inspection or through other
heuristics. Eden adopts a diﬀerent tack by allowing ap-
plications to explicitly share this information.
Applications, libraries (e.g., an HTTP library) and
services (e.g., NFS or SMB service) that run either in
user space or at the OS can opt to classify network traf-
ﬁc generated by them, i.e., identify messages and their
class(es). A message refers to an (arbitrary) applica-
tion data unit while a class refers to the set of pack-
ets to which the same action function should be applied.
For example, a memcached-speciﬁc load balancing func-
tion may pick the destination server based on whether
a packet corresponds to a GET or a PUT message. To
allow such functions to be implemented at the Eden
enclave, the memcached application needs to mark all
its traﬃc as belonging to one of two classes, GETs and
PUTs, and to identify each GET/PUT message sep-
arately. The class information is carried through the
end-host stack to the enclave and is used to determine
the action function to apply.
A stage speciﬁes the application-speciﬁc ﬁelds that
can be used for classiﬁcation, i.e., ﬁelds that can cate-
gorize messages into classes. It also speciﬁes meta-data,
i.e., other application-speciﬁc data, it can generate. To
make the discussion concrete, Table 2 gives examples
of the classiﬁcation and meta-data capabilities of three
stages. For instance, an Eden-compliant memcached
Priority QueueShaping .Packet TransmissionEnclaveMatch-Action TableMetadata infoAction Functions        Packetization (socket)App LayerUnmodified App Layer...PACKETS+METADATAClassificationClassificationMessageMessageController498Stage
Classiﬁers
memcache
HTTP
library
Eden
enclave
<src_ip,src_port,