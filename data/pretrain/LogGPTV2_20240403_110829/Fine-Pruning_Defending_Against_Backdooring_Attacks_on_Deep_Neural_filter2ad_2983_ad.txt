rate (the game is zero-sum so the attacker’s utility is symmetric). From this we
can see that defender’s best strategy is always to use ﬁne-pruning. We reach
the same conclusion from the utility matrices of the speech and traﬃc sign
recognition attacks.
Finally, we note that both ﬁne-tuning and ﬁne-pruning are only attractive
as a defense if they are signiﬁcantly cheaper (in terms of computation) than
retraining from scratch. In our experiments, we ran ﬁne-tuning until convergence,
and found that the networks we tested converged in just a few minutes. Although
these experiments were performed on a cluster with high-end GPUs available
(NVIDIA P40, P100, K80, and GTX 1080), even if a less powerful GPU is used
(say, one that is 10X slower) we can see that ﬁne-pruning is still signiﬁcantly
Fine-Pruning: Defending Against Backdooring Attacks on DNNs
289
more eﬃcient than training from scratch, which can take several days in the case
of large models such as AlexNet [22].
4 Discussion
Looking at how each portion of the ﬁne-pruning defense works, we note that their
eﬀects are complementary, which helps us understand why their combination
is eﬀective even though each individually does not fully remove the backdoor.
Fine-tuning on a sparse network is ineﬀective because backdoor neurons are not
activated by clean data, so their gradients are close to 0 and they will be largely
unaﬀected by the ﬁne-tuning. However, these are precisely the neurons that will
be selected for pruning, since their activations on clean data are low. It is only
once we prune and ﬁne-tune, forcing the attacker to concentrate her backdoor
into a relatively small number of neurons, that ﬁne-tuning can act on neurons
that encode the backdoor trigger.
The fact that backdoors can be removed automatically is surprising from the
perspective of prior research into backdoors in traditional software and hardware.
Unlike traditional software and hardware, neural networks do not require human
expertise once the training data and model architecture have been deﬁned. As
a result, strategies like ﬁne-pruning, which involve partially retraining (at much
lower computational cost) the network’s functionality, can succeed in this con-
text, but are not practical for traditional software: there is no known technique
for automatically reimplementing some functionality of a piece of software aside
from having a human rewrite the functionality from scratch.
We cannot guarantee that our defense is the last word in DNN backdoor
attacks and defenses. We can think of the ﬁne-tuning as a continuation of the
normal training procedure from some set of initialization parameters Θi. In an
adversarial context, Θi is determined by the attacker. Hence, if an attacker hopes
to preserve their attack against our ﬁne-pruning, they must provide a Θi with
a nearby local minimum (in terms of the loss surface with respect to the clean
dataset) that still contains their backdoor. We do not currently have a strong
guarantee that such a Θi cannot be found; however, we note that a stronger
(though more computationally expensive) version of ﬁne-pruning could add some
noise to the parameters before ﬁne-tuning. In the limit, there must exist some
amount of noise that would cause the network to “forget” the backdoor, since
adding suﬃciently large amounts of noise would be equivalent to retraining the
network from scratch with random initialization. We believe the question of how
much noise is needed to be an interesting area for future research.
4.1 Threats to Validity
The backdoor attacks studied in this paper share a similar underlying model
architecture: convolutional neural networks with ReLU activations. These net-
works are widely used for many diﬀerent tasks, but they are not the only architec-
tures available. For example, recurrent neural networks (RNNs) and long short
290
K. Liu et al.
term memory networks (LSTMs) are commonly used in sequential processing
tasks such as natural language processing. Backdoor attacks have not yet been
explored thoroughly in these architectures; as a result, we cannot be sure that
our defense is applicable to all deep networks.
5 Related Work
We will discuss two categories of related work: early work on poisoning attacks
on classic (non-DNN) machine learning, and more recent work on backdoors in
neural networks. We will not attempt to recap, here, the extensive literature
on adversarial inputs and defenses so far. Backdoor attacks are fundamentally
diﬀerent from adversarial inputs as they require the training procedure to be
corrupted, and hence have much greater ﬂexibility in the form of the backdoor
trigger. We do not expect that defenses against adversarial inputs will be eﬀective
against backdoor attacks, since they are, in some sense, correctly learning from
their (poisoned) training data.
Barreno et al. [7] presented a useful taxonomy for classifying diﬀerent types of
attacks on machine learning along three axes: whether the goal is to compromise
the integrity or availability of the system, whether the attack is exploratory
(gaining information about a trained model) or causative (changing the output
of the model by interfering with its training data), and whether the attack is
targeted or indiscriminate.
Many of the early attacks on machine learning were exploratory attacks on
network and host-based intrusion detection systems [14,15,41,43] or spam ﬁl-
ters [23,29,30,44]. Causative attacks, primarily using training data poisoning,
soon followed, again targeting spam ﬁltering [35] and network intrusion detec-
tion [11,12,36]. Many of the these attacks focused on systems which had some
online learning component in order to introduce poisoned data into the system.
Suciu et al. [39] classify poisoning and evasion attacks into a single framework
for modeling attackers of machine learning systems, and present StingRay, a tar-
geted poisoning attack that is eﬀective against several diﬀerent machine learn-
ing models, including convolutional neural networks. Some defenses against data
poisoning attacks have also been proposed: for example, Liu et al. [26] discuss a
technique for performing robust linear regression in the presence of noisy data
and adversarially poisoned training samples by recovering a low-rank subspace
of the feature matrix.
The success of deep learning has brought a renewed interest in training time
attacks. Because training is more expensive, outsourcing is common and so threat
models in which the attacker can control the parameters of the training procedure
are more practical. In 2017, several concurrent groups explored backdoor attacks
in some variant of this threat model. In addition to the three attacks described
in detail in Sect. 2.3 [10,18,27], Mu˜noz-Gonz´alez et al. [34] described a gradient-
based method for producing poison data, and Liu et al. [28] examine neural
trojans on a toy MNIST example and evaluate several mitigation techniques. In
the context of the taxonomy given by Barreno et al. [7], these backdoor attacks
can be classiﬁed as causative integrity attacks.
Fine-Pruning: Defending Against Backdooring Attacks on DNNs
291
Because DNN backdoor attacks are relatively new, only a limited number of
defenses have been proposed. Chen et al. [10] examine several possible counter-
measures, including some limited retraining with a held-out validation set, but
conclude that their proposed defenses are ineﬀective. Similarly, in their NDSS
2017 paper, Liu et al. [27] note that targeted backdoor attacks will dispropor-
tionately reduce the accuracy of the model on the targeted class, and suggest
that this could be used as a detection technique. Finally, Liu et al.’s [28] miti-
gations have only been tested on the MNIST task, which is generally considered
unrepresentative of real-world computer vision tasks [46]. Our work is, to the
best of our knowledge, the ﬁrst to present a fully eﬀective defense against DNN
backdoor attacks on real-world models.
6 Conclusion
In this paper, we explored defenses against recently-proposed backdoor attacks
on deep neural networks. By implementing three attacks from prior research, we
were able to test the eﬃcacy of pruning and ﬁne-tuning based defenses. We found
that neither provides strong protection against backdoor attacks, particularly
in the presence of an adversary who is aware of the defense being used. Our
solution, ﬁne-pruning, combines the strengths of both defenses and eﬀectively
nulliﬁes backdoor attacks. Fine-pruning represents a promising ﬁrst step towards
safe outsourced training for deep neural networks.
Acknowledgement. This research was partially supported by National Science Foun-
dation CAREER Award #1553419.
References
1. ImageNet large scale visual recognition competition. http://www.image-net.org/
challenges/LSVRC/2012/ (2012)
2. Amazon Web Services Inc: Amazon Elastic Compute Cloud (Amazon EC2)
3. Amazon.com, Inc.: Deep Learning AMI Amazon Linux Version
4. Anwar, S.: Structured pruning of deep convolutional neural networks. ACM J.
Emerg. Technol. Comput. Syst. (JETC) 13(3), 32 (2017)
5. Athalye, A., Carlini, N., Wagner, D.: Obfuscated gradients give a false sense of secu-
rity: circumventing defenses to adversarial examples. In: Proceedings of the 35th
International Conference on Machine Learning, ICML 2018, July 2018. https://
arxiv.org/abs/1802.00420
6. Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning
to align and translate (2014)
7. Barreno, M., Nelson, B., Sears, R., Joseph, A.D., Tygar, J.D.: Can machine learning
be secure? In: Proceedings of the 2006 ACM Symposium on Information, Computer
and Communications Security. ASIACCS 2006 (2006). https://doi.org/10.1145/
1128817.1128824
8. Blum, A., Rivest, R.L.: Training a 3-node neural network is NP-complete. In:
Advances in neural information processing systems, pp. 494–501 (1989)
292
K. Liu et al.
9. Carlini, N., Wagner, D.A.: Defensive distillation is not robust to adversarial exam-
ples. CoRR abs/1607.04311 (2016). http://arxiv.org/abs/1607.04311
10. Chen, X., Liu, C., Li, B., Lu, K., Song, D.: Targeted backdoor attacks on deep
learning systems using data poisoning. ArXiv e-prints, December 2017
11. Chung, S.P., Mok, A.K.: Allergy attack against automatic signature generation. In:
Zamboni, D., Kruegel, C. (eds.) RAID 2006. LNCS, vol. 4219, pp. 61–80. Springer,
Heidelberg (2006). https://doi.org/10.1007/11856214 4
12. Chung, S.P., Mok, A.K.: Advanced allergy attacks: does a corpus really help? In:
Kruegel, C., Lippmann, R., Clark, A. (eds.) RAID 2007. LNCS, vol. 4637, pp.
236–255. Springer, Heidelberg (2007). https://doi.org/10.1007/978-3-540-74320-
0 13
13. Dhillon, G.S., et al.: Stochastic activation pruning for robust adversarial
defense. In: International Conference on Learning Representations (2018). https://
openreview.net/forum?id=H1uR4GZRZ
14. Fogla, P., Lee, W.: Evading network anomaly detection systems: formal reasoning
and practical techniques. In: Proceedings of the 13th ACM Conference on Com-
puter and Communications Security. CCS 2006 (2006). https://doi.org/10.1145/
1180405.1180414
15. Fogla, P., Sharif, M., Perdisci, R., Kolesnikov, O., Lee, W.: Polymorphic blending
attacks. In: USENIX-SS 2006 Proceedings of the 15th Conference on USENIX
Security Symposium, vol. 15 (2006)
16. Google Inc: Google Cloud Machine Learning Engine. https://cloud.google.com/
ml-engine/
17. Graves, A., Mohamed, A.R., Hinton, G.: Speech recognition with deep recurrent
neural networks. In: 2013 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP), pp. 6645–6649. IEEE (2013)
18. Gu, T., Garg, S., Dolan-Gavitt, B.: BadNets: identifying vulnerabilities in the
machine learning model supply chain. In: NIPS Machine Learning and Computer
Security Workshop (2017). https://arxiv.org/abs/1708.06733
19. Han, S., Mao, H., Dally, W.J.: Deep compression: compressing deep neural net-
works with pruning, trained quantization and huﬀman coding. In: International
Conference on Learning Representations (ICLR) (2016)
20. He, W., Wei, J., Chen, X., Carlini, N., Song, D.: Adversarial example defense:
ensembles of weak defenses are not strong. In: 11th USENIX Workshop on Oﬀensive
Technologies (WOOT 2017). USENIX Association, Vancouver, BC (2017). https://
www.usenix.org/conference/woot17/workshop-program/presentation/he
21. Hermann, K.M., Blunsom, P.: Multilingual distributed representations without
word alignment. In: Proceedings of ICLR, April 2014. http://arxiv.org/abs/1312.
6173
22. Iandola, F.N., Moskewicz, M.W., Ashraf, K., Keutzer, K.: FireCaﬀe: near-linear
acceleration of deep neural network training on compute clusters. In: Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2592–
2600 (2016)
23. Karlberger, C., Bayler, G., Kruegel, C., Kirda, E.: Exploiting redundancy in nat-
ural language to penetrate bayesian spam ﬁlters. In: Proceedings of the First
USENIX Workshop on Oﬀensive Technologies. WOOT 2007 (2007)
24. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classiﬁcation with deep con-
volutional neural networks. In: Advances in Neural Information Processing Sys-
tems, pp. 1097–1105 (2012)
25. Li, H., et al.: Pruning ﬁlters for eﬃcient convnets. arXiv preprint arXiv:1608.08710
(2016)
Fine-Pruning: Defending Against Backdooring Attacks on DNNs
293
26. Liu, C., Li, B., Vorobeychik, Y., Oprea, A.: Robust linear regression against train-
ing data poisoning. In: Proceedings of the 10th ACM Workshop on Artiﬁcial Intel-
ligence and Security, pp. 91–102. ACM (2017)
27. Liu, Y., et al.: Trojaning attack on neural networks. In: 25nd Annual Network and
Distributed System Security Symposium, NDSS 2018, San Diego, California, USA,
18–21 February 2018. The Internet Society (2018)
28. Liu, Y., Xie, Y., Srivastava, A.: Neural trojans. CoRR abs/1710.00942 (2017).
http://arxiv.org/abs/1710.00942
29. Lowd, D., Meek, C.: Adversarial learning. In: Proceedings of the Eleventh ACM
SIGKDD International Conference on Knowledge Discovery in Data Mining. KDD
2005, pp. 641–647. ACM, New York (2005). https://doi.org/10.1145/1081870.
1081950
30. Lowd, D., Meek, C.: Good word attacks on statistical spam ﬁlters. In: Proceedings
of the Conference on Email and Anti-Spam (CEAS) (2005)
31. Microsoft Corporation: Azure Batch AI Training. https://batchaitraining.azure.
com/
32. Møgelmose, A., Liu, D., Trivedi, M.M.: Traﬃc sign detection for us roads: remain-
ing challenges and a case for tracking. In: 2014 IEEE 17th International Conference
on Intelligent Transportation Systems (ITSC), pp. 1394–1399. IEEE (2014)
33. Molchanov, P., et al.: Pruning convolutional neural networks for resource eﬃcient
inference (2016)
34. Mu˜noz-Gonz´alez, L., et al.: Towards poisoning of deep learning algorithms with
back-gradient optimization. CoRR abs/1708.08689 (2017). http://arxiv.org/abs/
1708.08689
35. Nelson, B., et al.: Exploiting machine learning to subvert your spam ﬁlter. In:
Proceedings of the 1st Usenix Workshop on Large-Scale Exploits and Emergent
Threats. LEET 2008, pp. 7:1–7:9. USENIX Association, Berkeley (2008)
36. Newsome, J., Karp, B., Song, D.: Paragraph: thwarting signature learning by train-
ing maliciously. In: Zamboni, D., Kruegel, C. (eds.) RAID 2006. LNCS, vol. 4219,
pp. 81–105. Springer, Heidelberg (2006). https://doi.org/10.1007/11856214 5
37. Papernot, N., McDaniel, P., Wu, X., Jha, S., Swami, A.: Distillation as a defense to
adversarial perturbations against deep neural networks. In: 2016 IEEE Symposium
on Security and Privacy (SP), pp. 582–597, May 2016. https://doi.org/10.1109/
SP.2016.41
38. Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: towards real-time object
detection with region proposal networks. In: Advances in Neural Information Pro-
cessing Systems, pp. 91–99 (2015)
39. Suciu, O., Marginean, R., Kaya, Y., Daum´e III, H., Dumitras, T.: When
does machine learning FAIL? Generalized transferability for evasion and poi-
soning attacks.
In: 27th USENIX Security Symposium (USENIX Security
18). USENIX Association, Baltimore (2018). https://www.usenix.org/conference/
usenixsecurity18/presentation/suciu
40. Sun, Y., Wang, X., Tang, X.: Deep learning face representation from predicting
10,000 classes. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 1891–1898 (2014)
41. Tan, K.M.C., Killourhy, K.S., Maxion, R.A.: Undermining an anomaly-based intru-
sion detection system using common exploits. In: Proceedings of the 5th Interna-
tional Conference on Recent Advances in Intrusion Detection. RAID 2002 (2002)
42. Tung, F., Muralidharan, S., Mori, G.: Fine-pruning: joint ﬁne-tuning and compres-
sion of a convolutional network with Bayesian optimization. In: British Machine
Vision Conference (BMVC) (2017)
294
K. Liu et al.
43. Wagner, D., Soto, P.: Mimicry attacks on host-based intrusion detection systems.
In: Proceedings of the 9th ACM Conference on Computer and Communications
Security. CCS 2002 (2002). https://doi.org/10.1145/586110.586145
44. Wittel, G.L., Wu, S.F.: On attacking statistical spam ﬁlters. In: Proceedings of the
Conference on Email and Anti-Spam (CEAS), Mountain View, CA, USA (2004)
45. Wolf, L., Hassner, T., Maoz, I.: Face recognition in unconstrained videos with
matched background similarity. In: CVPR 2011, pp. 529–534, June 2011. https://
doi.org/10.1109/CVPR.2011.5995566
46. Xiao, H., Rasul, K., Vollgraf, R.: Fashion-MNIST: a novel image dataset for bench-
marking machine learning algorithms. CoRR abs/1708.07747 (2017). http://arxiv.
org/abs/1708.07747
47. Yosinski, J., Clune, J., Bengio, Y., Lipson, H.: How transferable are features in
deep neural networks? In: Advances in Neural Information Processing Systems,
pp. 3320–3328 (2014)
48. Yu, J., et al.: Scalpel: Customizing DNN pruning to the underlying hardware paral-
lelism. In: Proceedings of the 44th Annual International Symposium on Computer
Architecture, pp. 548–560. ACM (2017)