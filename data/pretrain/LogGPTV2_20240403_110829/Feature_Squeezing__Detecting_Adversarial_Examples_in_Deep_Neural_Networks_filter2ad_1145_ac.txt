the third image illustrates the result of local smoothing of
that adversarial example. As with CIFAR-10, both humans and
machines see the correct image class clearly after smoothing.
We observe the similar eﬀect on the ImageNet dataset with
another L0 attack: CW0 attack in Figure 4b, even though there
are less perturbed pixels.
Implementation. We use the median ﬁlter implemented in
SciPy [37]. In a 2×2 sliding window, the center pixel is always
located in the lower right. When there are two equal-median
values due to the even number of pixels in a window, we
(arbitrarily) use the greater value as the median.
2) Non-local Smoothing
Non-local smoothing is diﬀerent from local smoothing
because it smooths over similar pixels in a much larger area
instead of just nearby pixels. For a given image patch, non-
local smoothing ﬁnds several similar patches in a large area
of the image and replaces the center patch with the average of
those similar patches. Assuming that the mean of the noise is
zero, averaging the similar patches will cancel out the noise
while preserving the edges of an object. Similar with local
smoothing, there are several possible ways to weigh the similar
patches in the averaging operation, such as Gaussian, mean,
and median. We use a variant of the Gaussian kernel because
it is widely used and allows to control the deviation from
the mean. The parameters of a non-local smoothing method
typically include the search window size (a large area for
searching similar patches), the patch size and the ﬁlter strength
(bandwidth of the Gaussian kernel). We will denote a ﬁlter as
“non-local means (a-b-c)” where “a” means the search window
a × a, “b” means the patch size b × b and “c” means the ﬁlter
strength.
Figure 4 presents some examples with non-local means
(11-3-4). From the ﬁrst column in Figure 4a, we observe
that the adversarial attacks introduce diﬀerent patterns in the
sky background. Non-local smoothing (fourth column) is very
eﬀective in restoring the smooth sky while preserving the
shape of the airplane. We observe the similar eﬀect from the
ImageNet examples in Figure 4b.
Implementation. We use the fast non-local means denoising
C. Other Squeezing Methods
Our results in this paper are limited to these simple
squeezing methods, which are surprisingly eﬀective on our test
datasets. However, we believe many other squeezing methods
are possible, and continued experimentation will be worthwhile
to ﬁnd the most eﬀective squeezing methods.
One possible area to explore includes lossy compression
techniques. Kurakin et al. explored the eﬀectiveness of the
JPEG format in mitigating the adversarial examples [20]. Their
experiment shows that a very low JPEG quality (e.g. 10 out of
100) is able to destruct the adversarial perturbations generated
by FGSM with =16 (at scale of [0,255]) for at most 30%
of the successful adversarial examples. However, they didn’t
evaluate the potential loss on the accuracy of legitimate inputs.
Another possible direction is dimension reduction. For
example, Turk and Pentland’s early work pointed out that many
pixels are irrelevant features in the face recognition tasks, and
the face images can be projected to a feature space named
eigenfaces [40]. Even though image samples represented in the
eigenface-space loose the spatial information a CNN model
needs, the image restoration through eigenfaces may be a
useful technique to mitigate adversarial perturbations in a face
recognition task.
IV. Robustness
The previous section demonstrated that images, as used in
classiﬁcation tasks, contain many irrelevant features that can be
squeezed without reducing recognizability. For feature squeez-
ing to be eﬀective in detecting adversarial examples (Figure 1),
it must satisfy two properties: (1) on adversarial examples, the
squeezing reverses the eﬀects of the adversarial perturbations;
and (2) on normal legitimate examples, the squeezing does
not signiﬁcantly impact a classiﬁer’s prediction. This section
evaluates the how well diﬀerent feature squeezing methods
achieve these properties against various adversarial attacks.
Threat model. In evaluating robustness, we assume a powerful
adversary who has full access to a target trained model, but
no ability to inﬂuence that model. The adversary is not aware
of feature squeezing being performed on the operator’s side.
With the goal to ﬁnd inputs that are misclassiﬁed by the model,
the adversary tries to fool the target model with the white-box
attack techniques, whereas the adversarial examples will be
inferred by the model with feature squeezing.
We do not propose using feature squeezing directly as a
defense because an adversary may take advantage of feature
squeezing in attacking a DNN model. For example, when
facing binary squeezing, an adversary can construct an image
by setting all pixel intensity values to be near 0.5. This image
is entirely gray to human eyes. By setting pixel values to either
0.499 or 0.501 it can result in an arbitrary 1-bit ﬁltered image
after squeezing, either entirely white or black. Such an attack
can easily be detected by our detection framework (Section V),
because since the prediction diﬀerence between the original
and the squeezed will clearly exceed a normal threshold. In
7
TABLE I: Summary of the target DNN models.
Dataset
MNIST
CIFAR-10
ImageNet
Model
7-Layer CNN [3]
DenseNet [18], [23]
MobileNet [17], [24]
Top-1
Accuracy
99.43%
94.84%
68.36%
Top-1 Mean
Conﬁdence
Top-5
Accuracy
99.39%
92.15%
75.48%
-
-
88.25%
more details, we consider how adversaries can adapt to our
detection framework in Section V-D.
A. Experimental Setup
We evaluate our defense on state-of-the-art models for the
three image datasets, against eleven attack variations represent-
ing the best known attacks to date.
Target Models. We use three popular datasets for the image
classiﬁcation task: MNIST, CIFAR-10, and ImageNet. For
each dataset, we set up a pre-trained model with the state-
of-the-art performance. Table I summarizes the prediction
performance of each model and the information of its DNN
architecture. Our MNIST model (a seven-layer CNN [3])
achieves a test accuracy of 99.43%; our CIFAR-10 model (a
DenseNet
[18], [23]) achieves 94.84% test accuracy. The
prediction performance of both models is competitive with
state-of-the-art results [1]. For the ImageNet dataset, we use
a MobileNet model
[17], [24] because MobileNet is more
widely used on mobile phones and its small and eﬃcient
design make it easier to conduct experiments. The pre-trained
MobileNet model achieves top-1 accuracy 68.36% and top-
5 accuracy 88.25%, both are comparable to state-of-the-art
results. In contrast, a larger model such as Inception v3 [38],
[7] with six times of trainable parameters could achieve top-1
accuracy 76.28% and top-5 accuracy 93.03%. However, the
calculation on such a model is much more expensive due to
the massive architecture.
Attacks. We evaluate feature squeezing on all of the attacks
described in Section II-C. For the targeted attacks, we try each
attack with two types of targets: the next class (-Next),
t = L + 1 mod #classes,
and the least-likely class (-LL),
t = min (ˆy),
(7)
(8)
Here t
is the target class, L is the index of the ground-
truth class and ˆy is the prediction vector of an input image.
This gives eleven total attacks: the three untargeted attacks
(FGSM, BIM and DeepFool), and two versions each of the
four targeted attacks (JSMA, CW∞, CW2, and CW0). We
use the implementations of FGSM, BIM and JSMA provided
by the Cleverhans library [30]. For DeepFool and the three
CW attacks, we use the implementations from the original
authors [3], [27]. The parameters we use for the attacks are
given in Table VI (in the appendix).1
For the seed images, we select the ﬁrst 100 correctly pre-
dicted examples in the test (or validation) set from each dataset
for all the attack methods, since some attacks are too expensive
1All of our models and codes for attacks, defenses, and testing are available
as an open source tool (https://github.com/mzweilin/EvadeML-Zoo).
TABLE II: Evaluation of 11 diﬀerent attacks (each with 100
seed images) against DNN models on three datasets. The cost of
an attack generating adversarial examples is measured in seconds per sample.
The L0 distortion is normalized by the number of pixels (e.g., 0.56 means
56% of all pixels in the image are modiﬁed).
Conﬁgration
Attack Mode
FGSM
BIM
CW2
CW∞
Next
LL
Next
LL
Next
CW0
LL
JSMA Next
LL
FGSM
BIM
CW∞
Next
LL
DeepFool
CW2
Next
LL
Next
CW0
LL
JSMA Next
LL
FGSM
BIM
CW∞
Next
LL
DeepFool
CW2
CW0
Next
LL
Next
LL
T
S
I
N
M
0
1
-
R
A
F
I
C
t
e
N
e
g
a
m
I
L∞
L2
L0
L∞
L2
L0
L∞
L2
L0
Cost
0.002
0.01
51.25
49.95
0.33
0.38
68.76
74.55
0.79
0.98
Success
Rate
46%
91%
100%
100%
99%
100%
100%
100%
71%
48%
0.02
0.19
225.32
224.58
0.36
10.36
12.01
366.54
426.05
8.44
13.64
0.02
0.18
210.70
268.86
60.16
20.63
29.14
607.94
979.05
85%
92%
100%
100%
98%
100%
100%
100%
100%
100%
98%
99%
100%
99%
99%
89%
90%
97%
100%
100%
Prediction
Conﬁdence
93.89%
99.62%
99.99%
99.98%
99.23%
99.99%
99.99%
99.99%
74.52%
74.80%
84.85%