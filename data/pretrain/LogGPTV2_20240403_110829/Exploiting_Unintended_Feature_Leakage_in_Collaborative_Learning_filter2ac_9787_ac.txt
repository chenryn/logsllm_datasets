### Computational Cost Reduction and Inference Algorithms

The proposed method reduces the computational cost of the attack. During collaborative training, the adversary observes the gradient updates, defined as \( g_{\text{obs}} = \Delta\theta_t - \Delta\theta_{\text{adv}} \). For single-batch inference, the adversary feeds these observed gradient updates to a batch property classifier \( f_{\text{prop}} \). This approach can be extended from single-batch properties to the entire training dataset. The batch property classifier outputs a score in the range [0,1], indicating the probability that a batch has the specific property. The adversary can use the average score across all iterations to determine whether the target's entire dataset possesses the property.

### Active Property Inference

An active adversary can perform a more powerful attack by leveraging multi-task learning. The adversary extends their local copy of the collaboratively trained model with an augmented property classifier connected to the last layer. This model is trained to simultaneously excel at the main task and recognize batch properties. Given training data where each record has a main label \( y \) and a property label \( p \), the joint loss function is:

\[ L_{\text{mt}} = \alpha \cdot L(x, y; \theta) + (1 - \alpha) \cdot L(x, p; \theta) \]

During collaborative training, the adversary uploads the updates based on this joint loss, causing the joint model to learn separable representations for data with and without the property. Consequently, the gradients become separable, enabling the adversary to determine if the training data has the property.

This adversary remains "honest-but-curious" in cryptographic terms, following the collaborative learning protocol and not submitting any malformed messages. The only difference from a passive attack is that the active adversary performs additional local computations and submits the resulting values into the collaborative learning protocol. Note that the "honest-but-curios" model does not constrain the parties' input values, only their messages.

### Datasets and Model Architectures

The datasets, collaborative learning tasks, and adversarial inference tasks used in our experiments are summarized in Table I.

| Dataset       | #Records | Main Tasks           | Inference Tasks                |
|---------------|----------|----------------------|--------------------------------|
| LFW           | 13,233   | Gender/Smile/Age     | Race/Eyewear                   |
| FaceScrub     | 18,809   | Gender               | Eyewear/Race/Hair              |
| PIPA          | 18,000   | Age                  | Identity                       |
| CSI           | 1,412    | Sentiment            | Membership, Region/Gender/Veracity |
| FourSquare    | 15,548   | Gender               | Membership, Doctor Specialty   |
| Yelp-health   | 17,938   | Review Score         | Author                         |
| Yelp-author   | 16,207   | Review Score         | Author                         |

Our hyperparameter choices are based on standard models from the machine learning literature.

#### Labeled Faces in the Wild (LFW)
- **Dataset**: 13,233 62x47 RGB face images for 5,749 individuals.
- **Labels**: Gender, race, age, hair color, and eyewear.
- **Model**: Convolutional Neural Network (CNN) with three spatial convolution layers (32, 64, 128 filters, kernel size (3, 3)), max pooling layers (pooling size 2), followed by two fully connected layers (256, 2).
- **Activation Function**: ReLU
- **Batch Size**: 32
- **SGD Learning Rate**: 0.01

#### FaceScrub
- **Dataset**: 18,809 50x50 RGB images for 100 individuals.
- **Label**: Gender
- **Model**: Same CNN architecture as LFW.

#### People in Photo Album (PIPA)
- **Dataset**: 18,000 images with three or fewer people, scaled to 128x128.
- **Labels**: Number of people, gender, age, and race.
- **Model**: VGG-style 10-layer CNN with two convolution blocks (one convolutional layer and max pooling), followed by three convolution blocks (two convolutional layers and max pooling), and two fully connected layers.
- **Batch Size**: 32
- **SGD Learning Rate**: 0.01

#### Yelp-health
- **Dataset**: 17,938 health care-related reviews.
- **Model**: Recurrent Neural Network (RNN) with a word-embedding layer (dimension 100), GRU layer, and a fully connected classification layer.
- **SGD Learning Rate**: 0.05

#### FourSquare
- **Dataset**: 528,878 check-ins for 15,548 users.
- **Task**: Gender classification.
- **Model**: Embedding layer (dimension 320), three spatial convolutional layers (100 filters, kernel sizes (3, 320), (4, 320), (5, 320)), max pooling layers, and two fully connected layers (128, 2).
- **Batch Size**: 100
- **SGD Learning Rate**: 0.01

#### CLiPS Stylometry Investigation (CSI) Corpus
- **Dataset**: 1,412 reviews labeled with author and document attributes.
- **Model**: Similar to FourSquare, but with one fully connected layer (size 2).
- **Batch Size**: 12
- **SGD Learning Rate**: 0.01

### Two-Party Experiments

All experiments were conducted on a workstation running Ubuntu Server 16.04 LTS, equipped with a 3.4GHz CPU i7-6800K, 32GB RAM, and an NVIDIA TitanX GPU card. We used MxNet, Lasagne, and Scikit-learn for implementation. Training our inference models took less than 60 seconds on average and did not require a GPU. AUC scores were used to evaluate the performance of both the collaborative model and the property inference attacks.

#### Membership Inference
- **Adversary's Task**: Build a Bag of Words (BoW) representation for the input whose membership in the targetâ€™s training data is to be inferred.
- **Evaluation**: Precision was reported for different batch sizes, as shown in Table II.

#### Single-Batch Property Inference
- **Objective**: Identify batches with specific properties.
- **Results**: Table III shows the AUC scores for single-batch property inference on the LFW dataset, demonstrating that gradients leak more than just class features.

These results highlight the vulnerability of collaborative learning to property inference attacks, revealing that even uncorrelated properties can be inferred with high accuracy.