The model consists of the following symptoms:
Modiﬁed operand One operand is modiﬁed, e.g., 55 + 33 is
changed to 51 + 33. This happens due to a bit ﬂip in
memory/CPU register.
Exchanged operand A different but valid operand is used,
e.g., 55 + 33 is changed to 55 + 11. This happens due to
a fault on the address bus.
Faulty operation An operation produces incorrect results on
speciﬁc inputs, e.g., 55 + 33 results in 87. A CPU design
ﬂaw can lead to such a fault.
Exchanged operation An operation that was not intended is
executed, e.g., 55+33 is changed to 55−33. This happens
due to a fault in the CPU’s instruction decoder.
Lost update A store operation is omitted, e.g., the result of
55+33 is not stored in memory/CPU register; an outdated
value from the memory/register is then erroneously used.
This happens due to a fault on the address bus.
Many fault-detection approaches assume a Single Event
Upset (SEU) fault model, where exactly one bit is ﬂipped
throughout program execution; in contrast, we make no as-
sumptions on the number of bits affected by a hardware error
or on the number of hardware errors during execution.
We argue that the SEU model is unrealistic. First, as studies
show ([2], [3]), modern RAM experiences not only transient
bit ﬂips, but also permanent faults. Second, another study
[17] reveals that about 17% of hardware faults affecting the
combination logic result in double or multiple bit errors. These
results motivate the adoption of a fault model that has no error
rate assumption: any number of errors of any type can happen
during program execution. Our only assumptions are that errors
occur randomly and corrupt a random number of bits.
Our fault model does not cover control ﬂow errors, when
a corrupted instruction pointer (IP) points to an unintended
instruction address. Such faults have a very low probability of
resulting in SDC. Nevertheless, our approach can be coupled
with a control ﬂow checker to detect both data and control
ﬂow errors.
Finally, the sphere of replication (SoR) [18] assumed in this
paper is the CPU and the memory directly used by the encoded
program (or the encoded part of a program). The operating
system as well as the disk and network subsystems are out of
SoR; errors in these systems cannot be detected.
IV. Δ-ENCODING
In this section, we describe Δ-encoding, a novel tech-
nique that combines AN-encoding and duplicated instructions.
Δ-encoding borrows the ability to detect hard errors from AN-
encoding; it uses the idea of duplicated instructions to achieve
full fault coverage without sacriﬁcing performance. Moreover,
a clever combination of approaches allows to simplify AN-
encoding, improving its performance.
1515
original program
original execution
duplicate &
AN-encode
Δ-encoded program
A1-encoded execution
Listing 5: Encoding in Δ-encoding
1 i n t 6 4 t encode ( i n t 3 2 t n ,
2
3 }
return n * a ;
i n t 6 4 t a ) {
A2-encoded execution
accu
accu
check
crash
Listing 6: Decoding in Δ-encoding
CPU
RAM
Fig. 1: Δ-encoded program.
Conceptually, Δ-encoding performs two compile-time
transformations on the original program: ﬁrst, all data is AN-
encoded and all original operations are substituted by AN-
encoded operations, second, all encoded data and operations
are duplicated and checks are inserted at synchronization
points. The result is a hardened program with two copies of a
completely encoded data ﬂow, as shown in Fig. 1.
A. Encoded Data
To encode data in Δ-encoding, we set two different con-
stants for the two copies of data: A1 for the ﬁrst encoded copy
and A2 for the second copy. Thus, the two copies of data
ﬂow operate on different values, i.e., our approach employs
data diversity, which is beneﬁcial for fault tolerance [19]. In
particular, if a hard CPU fault triggers on some speciﬁc inputs,
it will corrupt only one copy of the data, but not the other.
The key idea behind Δ-encoding is a smart choice of A1
and A2:
A1 − A2 = 1
(1)
This choice of the constants enables us to decode values
quickly, by subtracting the second encoded copy ˆn2 from the
ﬁrst encoded copy ˆn1 (hence the name Δ-encoding):
n = ˆn1 − ˆn2 = n · A1 − n · A2 = n · (A1 − A2)
(2)
Note that this decoding requires only one instruction cycle;
in contrast, decoding in pure AN-encoding is much more
expensive, since it requires a division instruction4. Such quick
decoding is especially beneﬁcial for programs that make heavy
use of pointers because all pointers are kept encoded and must
be decoded at each pointer dereference.
The choice of A1 and A2 in Equation 1 has a drawback:
both copies of a value are decoded in the same way (by
subtracting the A2-encoded copy from the A1-encoded copy).
This can lead to SDC since a permanent fault affects both
decoding operations in the same way. Thus, we push the idea
further and use the following scheme to choose A1 and A2:
(3)
A2 = 2k − 2i
where k and i are non-negative integers, k > i.
A1 = 2k + 2i
We notice that:
A1 − A2 = 2k + 2i − 2k + 2i = 2i+1
A1 + A2 = 2k + 2i + 2k − 2i = 2k+1
(4)
(5)
4The division instruction is one of the most costly operations in modern
CPUs. For example, according to the Intel IA-64 architecture manual, division
takes 60-80 cycles to ﬁnish [20].
1616
1 i n t 3 2 t decode ( i n t 6 4 t n1_enc ,
i n t 6 4 t n2_enc ,
i n t 6 4 t a ) {
( a == A1 )
return ( n1_enc − n2_enc ) >> 1 ;
return ( n1_enc + n2_enc ) >> 1 4 ;
i f
e l s e
2
3
4
5
6 }
Based on Equations 4 and 5, there are two ways to decode
a value:
n = (ˆn1 − ˆn2)/2i+1 = n · (A1 − A2)/2i+1
n = (ˆn1 + ˆn2)/2k+1 = n · (A1 + A2)/2k+1
(6)
(7)
The division by a power of 2 corresponds to the right
shift instruction. Since we ﬁx k and i beforehand, the number
of bits to shift by is known at encoding time. As a result,
decoding schemes 6 and 7 require only two cycles: one for
subtraction/addition and one for right shift.
For example, let k = 3 and i = 0. Then A1 = 9 and
A2 = 7; their difference is A1 − A2 = 2 and their sum is
A1 + A2 = 16, and to decode one needs to shift right by
i + 1 = 1 and k + 1 = 4 correspondingly. Our original code
snippet from Listing 1 can be Δ-encoded with these parameters
and results in an encoded program from Listing 4.
Δ-encoding uses this scheme, with A1 and A2 chosen as in
Equation 3 and decoding as in Equations 6 and 7. This scheme
has two advantages: (1) decoding is much faster than in pure
AN codes and (2) two different ways to decode a value will
fail differently in reaction to the same permanent error.
In our ﬁnal implementation, we chose k = 13, i = 0 and
thus A1 = 8193, A2 = 8191 and shifts of 1 and 14. We
introduce these parameters here for clarity of description; the
justiﬁcation for the parameters is given in Section V-A.
B. Encoded Operations
Δ-encoding works on AN-encoded values. This implies
that all original operations – addition, subtraction, multiplica-
tion, bitwise AND, OR, XOR, shifts, comparisons, etc. – are
substituted with the corresponding encoded operations. In this
section, we provide examples of some typical Δ-encoded op-
erations. For clarity, we introduce them as functions in the C
language.
Encoding and decoding operations were already described
conceptually. Listings 5 and 6 show their practical imple-
mentations. It is worth mentioning that encoding could be
implemented through shifts and addition/subtraction, as shown
by Equation 3; however a simple multiplication exhibits similar
performance. The decoding operation corresponds to Equations
6 and 7.
Listing 7: Fully encoded operations in Δ-encoding: Addition
Listing 10: Δ-encoding: Accumulation of checks example
i n t 6 4 t n2_enc ) {
1 i n t 6 4 t add_enc ( i n t 6 4 t x_enc ,
2
3 }
return x_enc + y_enc ;
i n t 6 4 t y_enc ) {
Listing 8: Partially encoded operations in Δ-encoding: Left shift
i n t 6 4 t x2_enc ,
i n t 6 4 t a )
i f
i n t 6 4 t y2_enc ,
i n t 6 4 t y1_enc ,
( a == A1 ) {
i n t 3 2 t y = ( y1_enc − y2_enc ) >> 1 ;
return x1_enc > 1 4 ;
return x2_enc > 1 ;
i n t 3 2 t y = ( y1_enc − y2_enc ) >> 1 ;
1 i n t 6 4 t xor_enc ( i n t 6 4 t x1_enc ,
2 {
3
4
5
6
7
8
9
10
11
12
13 }
}
i n t 3 2 t res = x ˆ y ;
return res * a ;
i n t 3 2 t x = ( x1_enc + x2_enc ) >> 1 4 ;
i n t 3 2 t y = ( y1_enc + y2_enc ) >> 1 4 ;
}
e l s e {
Most arithmetic operations stay the same in AN codes and
also in Δ-encoding; the operations include addition, subtrac-
tion, comparisons, modulo, etc. Listing 7 exempliﬁes this. Note
that since no encoding/decoding takes place, there is no notion
of A in the code snippet.
Some operations require partial decoding. One example
is a left shift operation: the number of bits by which an
integer is shifted to the left must be decoded, but the integer
itself can stay encoded (see Listing 8). Another example is
multiplication, where it is enough to decode only one operand.
Finally, bitwise operations (AND, OR, XOR, one’s comple-
ment) as well as division are notoriously slow if implemented
using encoding. In these cases, the only reasonable strategy
is to decode operands, perform the original operation, and re-
encode the result. Listing 9 exempliﬁes this using the XOR
operation.
Encoded operations must be not only fast, they must also
propagate possible errors to the resultant integer. This holds
for operations like addition. Operations like left shift and
XOR rely on duplicated instructions, since it is unlikely that
the result of the ﬁrst operation execution (with A1) will be
corrupted exactly in the same way as in the second execution
(with A2). Moreover, the sum of two encoded copies ˆx1 + ˆx2
has zeros in the lower 14 bits by Equation 5 (otherwise it
indicates that an error occurred during the operation); we use
this property to propagate errors in some operations.
1717
C. Accumulation of Checks
As any fault detection mechanism, Δ-encoding inserts
periodic checks of calculated values. An example of such a
check is shown in Listing 4, Line 5. It includes checking if
both copies of a variable are code words and if they correspond
to the same original value. If any of the conditions fails, then
an error must have happened, and the execution is terminated.
A naive approach to detect errors would be to check
the result of each encoded operation. This would lead to a
tremendous slowdown, since each operation would then be
accompanied by a heavy-weight check with divisions and
branches.
On the other side, one could check only ﬁnal results,
i.e., check only output values right before decoding them.
Indeed, if the property of error propagation would hold for all
encoded operations, it would be sufﬁcient to check only the
results of the computation. In real-world scenarios, however,
this property is frequently violated; the XOR operation from
Listing 9 is one example.
The practical solution would be to analyze the program’s
data ﬂow and insert checks only at critical points (e.g., after
each XOR operation, but not after additions). Even in this case,
the number of inserted checks incurs signiﬁcant overhead.
To achieve a better trade-off between performance and
fault coverage, we introduce the accumulation of checks.
We allocate a pair of integers called accumulators and we
substitute all intermediate checks with a simple addition to
the accumulators. The principle is illustrated in Listing 10.
The original program performs two operations: addition x + y
and subtraction x − y. The encoded program adds two accu-