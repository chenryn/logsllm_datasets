(b) Times overhead in the ofﬂine phases of
ASTERIA, Gemini, and Diaphora.
(c) Time overhead in the online phases of
different approaches.
Fig. 10: Computational overhead.
regression method, we use the same model settings except
for the similarity calculation step. We record the T P Rs and
F P Rs and plot the ROC curve corresponding to the different
internal structures. As Figure 9 shows, the curve with the
label “Regression” corresponds to the calculation with the
cosine distance, and the curve with the label ”Classiﬁcation”
corresponds to the calculation in ASTERIA. The result shows
that the model with our calculation method yields a higher
AUC value of 0.981, which means a better model performance.
c) Leafnode Calculation: According to Equation (6) in
§ III-B, the calculation for leaf nodes also takes the hidden
state input and cell state input. Since leaf nodes have no
children, we assign the input vectors of the leaf nodes man-
ually. There are two ways for vector initialization in general:
all zeros vector or all ones vector. According to these two
different initialization methods, we train and test the model,
and then plot the ROC curves. As shown in Figure 9, the curve
with label “Leaf-0” corresponds to the model with all zeros
initialization, and the curve with label “Leaf-1” corresponds
to the model with all ones initialization. The results show the
model performs better with all zeros initialization.
F. Computational Overhead
1) AST Statistics: We deﬁne the AST size as the number
of nodes in an AST. Since there are global offset table func-
tions [21] which are automatically generated by the compiler,
we ﬁlter out such functions. 25,569 ASTs are retained after
ﬁltering from 25,667 ASTs. We sort
the sizes of 25,569
AST samples in ascending order from the OpenSSL dataset.
Figure 10(a) plots the cumulative distribution of AST size
of the sorted AST samples. It shows that most AST sizes
are smaller than 200. Speciﬁcally, the ASTs with sizes less
than 20, 40, 80, and 200 account for 48.6%, 65.1%, 85.4%,
and 97.4%, respectively. We then measure the computational
overhead of AST extracting and encoding with ASTERIA and
similarity calculation with different approaches.
2) Time Consumption: The process of function similarity
calculation consists of two phases: ofﬂine phase (AST extrac-
tion, preprocessing, and encoding) and online phase (similarity
calculation between encoding vectors). We separately measure
the extraction time, preprocessing time, encoding time, and
similarity calculation time on the OpenSSL dataset. The
decompilation is the most time-consuming part in the AST
extraction. The average time for each function decompilation
is around 41 milliseconds, and the average time for prepro-
cessing is around 6 milliseconds. We only consider ASTs
whose sizes are less than 300 since they account for the
majority of ASTs, as illustrated in Figure 10(a). Figure 10(b)
shows the encoding time of ASTs, which is about the same as
the decompilation time. In the ﬁgure, D-H denotes the AST
hashing time of Diaphora. G-EN denotes the ACFG encoding
time of Gemini and G-EX denotes the ACFG extraction
time. A-D, A-P, A-E respectively represent the decompilation
time, preprocessing time, encoding time of ASTERIA. ASTERIA
has a more time-consuming ofﬂine stage than Gemini and
Diaphora. The similarity calculation time in the online phase
is shown in Figure 10(c). ASTERIA takes 8 × 10−9 second
on average to calculate the similarity for a pair of encoded
ASTs. However, the average calculation time of Diaphora
and Gemini is 4 × 10−3 second and 6 × 10−5 second for a
pair of ASTs, respectively. Remarkably, it shows that ASTERIA
is much faster than Diaphora and Gemini in terms of the
similarity calculation. The root cause of better performance is
that we use the subtraction and production of vectors for the
similarity calculation, while Gemini uses the cosine function
which is much more time-consuming.
V. VULNERABILITY SEARCH
We conduct a vulnerability search using the Firmware
dataset and successfully ﬁnd a total of 75 vulnerabilities.
We ﬁrst encode all the functions in the Firmware dataset
with the Tree-LSTM network and obtain the encoding vectors
and the number of callee functions. We build a vulnera-
bility library that contains vulnerable functions exposed in
commonly used open-source software. In the library, seven
vulnerable functions of CVE are integrated and the details of
vulnerable functions are listed in Table IV. Speciﬁcally, three
of seven come from OpenSSL software, and two of them come
from libcurl, and one comes from wget, and the other comes
from vsftpd. For each vulnerable function, we also encode
it with the Tree-LSTM network, and record the encoding
vector and the number of callee functions. Then we perform
020406080110150200250300AST Size0.00.20.40.60.81.0Cumulative Fraction (CDF)AST Size Distribution0100200300AST/CFG Size0.00.20.40.6Time (second)D-H (avg 0.096ms)G-EN (avg 2.666ms)G-EXA-DA-PA-E104103102GeminiDiaphora0100200300AST Size0123Time (second)1e8Asteria#
1
2
3
4
5
6
7
CVE
software version
OpenSSL before 1.0.1t
and 1.0.2 before 1.0.2t
Wget before 1.16
vulnerable function
EVP EncodeUpdate
ftp retrieve glob
Affected model
D7000, R7000, FVS318Gv2, R8000, R7500,
OpenSSL before 0.9.8za, 1.0.0 before
1.0.0m, and 1.0.1 before 1.0.1h
2016-2105 [13]
2014-4877 [12]
2014-0195 [11]
2016-6303 [14]
2016-8618 [15]
2013-1944 [10]
2011-0762 [9]
TABLE IV: Results of vulnerability search. “Conﬁrmed #” indicates the number of conﬁrmed vulnerable functions.
OpenSSL before 1.1.0
Libcurl before 7.51.0
Libcurl before 7.30.0
vsftpd before 2.3.3
D7000, R8000, R6700, R7500, D7800, R7800
R7000, R8000, R6250, R7900, R7500, D7800
R7800, R6250, R7900, R7900, R7800
MDC2 Update
curl maprintf
tailmatch
R7500, D7800, R7800
FVS318Gv2
dtls1 reassemble fragment
DSN-6200
R7000, D7000, FVS318Gv2, R8000
vsf ﬁlename passes ﬁlter
R7000, D7800, R7800, R6250, R8000, R7900, R6700
Conﬁrmed #
24
14
13
13
7
3
1
Vendor
NetGear
NetGear
NetGear
Dlink
NetGear
NetGear
NetGear
NetGear
the similarity calculation between the encoding vectors of
vulnerable functions and the encoding vectors of all functions
in the Firmware database with the Siamese Network. After the
calculation, each function f in the Firmware dataset is tagged
with a similarity score of rf,v with the vulnerable function v.
According to the Youden Index [67], which measures model
performance at a certain threshold. Since our model performs
best at the threshold value of 0.84, we set the threshold to
be 0.84 to ﬁlter out 924 candidate functions. A score of rf,v
greater than the threshold indicates that function f may be
a homologous vulnerable function of v. If f is considered
a vulnerable function, we further conduct a manual analysis
to conﬁrm whether it is indeed a homologous function of v
or not. In our analysis, we deﬁne two criteria: A) function
f comes from the same version of the same software as
the vulnerable function v, and B) the score of rf,v is 1.
The candidate functions that satisfy the criteria A and B
are considered as homologous functions of the corresponding
vulnerable functions with high conﬁdence, so that we won’t
conduct further manual analysis. For those candidate functions
that satisfy the criteria A but not B, we further analyze the
semantic functionality of their assembly code to determine
whether they are homologous functions of the corresponding
vulnerable functions. From the search results, 68 vulnerable
functions satisfy criteria A, 13 vulnerability functions satisfy
criteria B, and 6 functions satisfy A and B. The number
of homologous vulnerable functions corresponding to each
vulnerable function of CVE is listed in Table IV. Since a
device model corresponds to multiple versions of ﬁrmware
and different versions likely hold the same vulnerability, the
number of the device models is less than the number of
conﬁrmed vulnerable functions.
We also conduct a comparison between ASTERIA and Gem-
ini in terms of the end-to-end time consumption and accuracy
for the vulnerable function search. We compare the end-to-
end time consumption and accuracy for the top 10 vulnerable
functions in the search results. ASTERIA takes 0.414 seconds on
average for a pair of functions and achieves 78.7% accuracy.
The Gemini takes 0.159 seconds for a pair and achieves 20%
accuracy. By checking the results of Gemini, we ﬁnd that most
of the vulnerable functions being ranked outside the top 10000
functions, which means a high false negative rate of results.
In comparison, ASTERIA produces less false negatives, which
means ASTERIA is more effective than Gemini.
VI. RELATED WORK
a) Feature-based Methods: When considering the simi-
larity of binary functions, the most intuitive way is to utilize
the assembly code content to calculate the edit distance for
similarity detection between functions. Khoo et al. concate-
nated consecutive mnemonics from assembly language into
the N-grams for similarity calculation [48]. David et al.
proposed Trecelet, which concatenates the instructions from
adjacent basic blocks in CFGs for similarity calculation [29].
Saebjornsen et al. proposed to normalize/abstract the operands
in instructions, e.g., replacing registers such as eax or ebx with
string “reg”, and conduct edit distance calculation based on
normalized instructions [55]. However, binary code similarity
detection methods based on disassembly text can not be
applied to cross-architecture detection since the instructions
are typically different in different architectures. The works
in [52], [22], [36], [63] utilize cross-architecture statistical
features for binary code similarity detection. Eschweiler et
al. [36] deﬁned statistical features of functions such as the
number of instructions, size of local variables. They utilized
these features to calculate and ﬁlter out candidate functions.
Then they performed a more accurate but time-consuming
calculation with the graph isomorphism algorithm based on
CFGs. Although this method takes a pre-ﬁltering mechanism,
the graph isomorphism algorithm makes similarity calculation
extremely slow. To improve the computation efﬁciency, Feng
et al. proposed Genius which utilizes machine learning tech-
niques for function encoding [38]. Genius uses the statistical
features of the CFG proposed in [36] to compose the attributed
CFG (ACFG). Then it uses a clustering algorithm to calculate
the center points of ACFGs and forms a codebook with the
center points. Finally, a new ACFG is encoded into a vector by
computing the distance with ACFGs in the codebook and the
similarity between ACFGs is calculated based on the encoded
vectors. But the codebook calculation and ACFG encoding in
Genius are still inefﬁcient. Xu et al. proposed Gemini based on
Genius to encode ACFG with a graph embedding network [62]
for improving the accuracy and efﬁciency. However, the large
variance of binary code across different architectures makes it
difﬁcult to ﬁnd architecture-independent features [32].
b) Semantic-based Methods: For more accurate detec-
tion, semantic-based features are proposed and applied for
code similarity detection. The semantic-based features model
the code functionality, and are not inﬂuenced by different
architectures. Khoo et al. applied symbolic execution tech-
nique for detecting function similarity [50]. Speciﬁcally, they
obtained input and output pairs by executing basic blocks of
a function. But the input and output pairs can not model
the functionality of the whole function accurately. Ming et
al. leveraged the deep taint and automatic input generation
to ﬁnd semantic differences in inter-procedural control ﬂows
for function similarity detection [51]. Feng et al. proposed to
extract conditional formulas as higher-level semantic features
from the raw binary code to conduct the binary code similarity
detection [37]. In their work, the binary code is lifted into
a platform-independent intermediate representation (IR), and
the data-ﬂow analysis is conducted to construct formulas from
IR. Egele et al. proposed the blanket execution, a novel
dynamic equivalence testing primitive that achieves complete
coverage by overwriting the intended program logic to per-
form the function similarity detection [35]. These semantic-
based features capture semantic functionalities of a function
to reduce the false positives. However, the methods above
depend heavily on emulation or symbolic execution, which are
not suitable for program analysis in large-scale IoT ﬁrmware
since the emulation requires peripheral devices [68], [40], [23]
and symbolic execution suffers from the problems of path
explosion.
c) AST in Source Code Analysis: Since the AST can
be easily generated from source code, there has been research
work proposed to detect source code clone based on AST. Ira
D. Baxter et al. proposed to hash ASTs of functions to buckets
and compare the ASTs in the same bucket [20] to ﬁnd clones.
Because the method proposed in [20] is similar to Diaphora
which hash ASTs, we only perform a comparative evaluation
with Diaphora. In addition to the code clone detection, AST is
also used in vulnerability extrapolation from source code [65],
[64]. In order to ﬁnd vulnerable codes that share a similar
pattern, Fabian et al. [65] encoded AST into a vector and
utilized the latent semantic analysis [30] to decompose the
vector to multiple structural pattern vectors and compute the
similarity between these pattern vectors. Yusuke Shido et al.
proposed an automatic source code summary method with
extended Tree-LSTM [57].
VII. DISCUSSION
In this section, we discuss the limitation of this work and
future research to address these limitations.
Our work focuses on a more accurate and faster method for
cross-platform binary code similarity detection. In ASTERIA,
we use AST as a feature to capture semantic information of
a function. Considering the complexity of decompilation in
multiple platforms, we use the commercial tool IDA Pro.
Our work relies on the correctness of the decompilation result
of IDA Pro.
For the digitization of nodes before encoding an AST into
the Tree-LSTM network, we remove the constant values and
strings in the AST since they correspond to unlimited in-
stances. Without the constant values and strings, the semantic
information of a function may be relatively incomplete and
may cause false positives. For the more accurate semantic
representation of a function, we may introduce another embed-
ding system to embed constants and strings into embedding
vectors, and combine the embedding vectors with the AST
encoding in ASTERIA to get a new AST representation. There
is no doubt that adding a new embedded system will increase
the computational overhead. But it achieves a good tradeoff
between the computational cost and accuracy.
For the organization strategy of function pairs introduced