environment that supports diﬀerent operating systems and languages, and net-
work emulation using Mininet [21] that enables strong control over the network,
while still being close enough to a real-world installation.
Another design goal is to run actual implementations of the system of interest
without discriminating based on programming language, compiler, toolkit, or
target operating system, and without imposing restrictions solely for visibility
into algorithmic behavior. Ideally, we should use the same implementation that
will be deployed. We achieve this goal by using a proxy to create the behavior
of the malicious switch. Malicious host behavior is injected directly into the real
data-plane, similarly enabling the use of unmodiﬁed switches and controllers.
Finally, a major goal for our test case generation is to create meaningful,
semantically-aware test cases that can go beyond testing parsers. One simple
method to generate an attack strategy is to use random fuzzing where the entire
packet or some of its ﬁelds are replaced with random strings. While random
fuzzing has been used successfully to test API inputs, it would have a low success
rate for OpenFlow messages. OpenFlow messages are complex data structures
involving many layers of nested objects as well as other syntactic and seman-
tic dependencies. Although any packet can be represented as a bit string, the
majority of bit strings are not valid OpenFlow messages. Hence, the majority of
test cases generated by random fuzzing only test the OpenFlow message parser
while the attacks we are interested in lie at a much deeper layer, in the algorithms
creating and processing those messages. Basic knowledge of the packet format
or ﬁelds helps generate valid, meaningful messages by satisfying the syntactic
requirement. However, an ideal testing tool should also consider the semantic
meaning of diﬀerent packet ﬁelds. For example, it should treat a ﬁeld represent-
ing a switch port number diﬀerently from a ﬁeld representing an IP address, and
treat the switch port number diﬀerently from the length of an embedded struc-
ture. This approach enables testing on semantically meaningful, yet problematic
values for a ﬁeld—for example IP addresses actually in the network—and pro-
vides a means for tuning the testing to focus on particular types or locations
BEADS: Automated Attack Discovery in OpenFlow-Based SDN Systems
317
of attacks. Similarly, our test generation creates tests with malicious hosts and
switches at multiple locations in our test topology to ensure that our results are
general and not tied to a speciﬁc topology.
3.2 Design Details
Our automated attack discovery platform, BEADS, is depicted in Fig. 1. We sep-
arate the attack strategy generation functionality controlled by a Coordinator
from the testing of a strategy in an SDN system controlled by an Execution Man-
ager (Manager for short). Several managers can run in parallel under the reign
of one coordinator. The coordinator has three roles: generate attack strategies,
assign those strategies to diﬀerent managers for testing, and receive feedback
about the execution of those strategies and their results. The attack strategies
are generated based on the format of the messages and on the network topol-
ogy in order to choose what entity (host or switch) will behave maliciously. The
coordinator generates strategies for both malicious host and malicious switch
behavior and decides how to interleave them. The coordinator uses feedback
from the execution and testing of prior strategies for future strategy generation.
Coordinator
• Generates attacks based on    
message format, attack 
success feedback, and 
interleaving strategy
• Controls different Managers
• Logs attacks
Manager
Instantiates testing
•
• Collects CPU/RAM utilization
• Collects attack success feedback
• Collects Flow Rules
• Detects and reports attack
Message 
Formats
Network 
Topology
Attack
Reports
VM1
SDN
Controller 1
…
VMn
SDN
Controller n
VM0
HostController
Mininet
SW
SW
SW
Host
Traffic 
Host
Traffic 
Host
Traffic 
Host
Traffic 
Mal. Host
Attack
Mal. Host
Attack
Mal. Host
Attack
Mal. Host
Attack
Malicious Switch Proxy
• Creates OpenFlow
message attacks
Fig. 1. BEADS framework design
318
S. Jero et al.
The (execution) manager controls the execution environment for a set of
attack strategy tests. This environment consists of an SDN, with a given topol-
ogy, a speciﬁed placement and type of attackers (hosts and/or switches), as
well as a list of attack strategies and a mechanism for interleaving host and
switch strategies. BEADS combines network emulation using software switches
and emulated hosts with real SDN controllers running in a virtualized environ-
ment. We select Mininet for emulation because it oﬀers the ﬂexibility to test
diﬀerent network topologies and traﬃc patterns while providing attack isolation
and increased reproducibility. Mininet exhibits high ﬁdelity through the use of
real network stacks, software switches, and real traﬃc. We leverage virtualiza-
tion to run a wide range of SDN controllers independent of required operating
systems, libraries, or system conﬁgurations. BEADS does not require access to
the source code of the OpenFlow implementation in the switch or controller.
The manager uses Mininet to create an emulated data-plane network con-
sisting of SDN switches and emulated hosts capable of both generating normal
network traﬃc and injecting host-based attacks. This network is controlled using
OpenFlow by one or several controllers running on a separate virtual machine,
as depicted in Fig. 1. A Host Controller running on the same virtual machine
as Mininet controls the hosts connected to the testing network. Each host has a
Traﬃc component that generates the traﬃc used during testing, and a Mali-
cious Host Attack component that injects attacks that emulate a malicious
host according to the strategy and timing speciﬁed by the HostController and
received in turn from the manager of the execution environment. Finally, a Mali-
cious Switch Proxy intercepts all messages in both directions between the SDN
switches and the SDN controllers and creates malicious switch behavior accord-
ing to strategies received from the manager of the execution environment.
The entire system from strategy generation to strategy testing and bug detec-
tion is automated. The user supplies the controller under test and receives a list
of strategies that trigger bugs as the output. The user is then responsible to
manually examine these strategies and identify the bugs triggered and any ﬁxes
required, as we do in Sect. 4.
3.3 Strategy Generation
Two questions must be answered to enable the coordinator to perform automatic
strategy generation in BEADS: (1) what is an attack strategy and (2) when to
inject an attack strategy. We discuss these aspects below.
In order to target attacks beyond simple parsing validation, we use attack
strategies that represent malicious actions (by compromised switches or hosts)
which target protocol packets: OpenFlow for malicious switches and ARP
for malicious hosts. A detailed list of these actions is presented in Sect. 2.
BEADS supports the testing of malicious switch actions, malicious host actions,
and combinations of both. For combinations, we need some form of coordina-
tion between malicious host and malicious switch actions, as they are launched
from independent components, not necessarily connected. We currently provide
a basic level of coordination between these strategies based on time relative to
BEADS: Automated Attack Discovery in OpenFlow-Based SDN Systems
319
the start of each test; i.e., we specify the time at which diﬀerent malicious switch
and malicious host actions occur relative to the start of the test.
The injection points diﬀer for malicious hosts and malicious switches. In the
case of malicious switches, the attack is executed by a malicious proxy which has
the ability to modify or aﬀect the delivery of OpenFlow messages as discussed in
Sect. 2. At the start of each test we deﬁne a set of one or more malicious switches.
For these switches, we then use send-based attack injection; i.e., when the proxy
receives an OpenFlow message to/from these switches, it takes any actions rel-
evant to that message type, and forwards the message to its destination. To
curtail state space explosion, we only consider strategies where the same action
is applied to every message of a given type. These strategies manipulate the deliv-
ery or ﬁelds of OpenFlow messages based on their message type and individual
message ﬁelds. For instance, a strategy may be to duplicate features reply
messages 10 times or to modify the in port ﬁeld of a packet out message to 7.
Our malicious proxy supports all manipulations discussed in Sect. 2, including
dropping, duplicating, and delaying messages based on message type, as well as
modifying message ﬁeld values based on message type.
Our testing applies this procedure to a list of strategies that we automatically
generate based on the OpenFlow message formats and semantics associated with
their ﬁelds. This list of strategies includes our message delivery attacks for each
message type and manipulations of each ﬁeld in each message type. The ﬁeld
values we use in our ﬁeld manipulations are based on the ﬁeld type, and are
chosen to be likely to cause unexpected behavior. This includes setting values
to zero, and the minimum and maximum values that the ﬁeld can handle. For
ﬁelds representing switch ports, we also consider all real switch ports as well as
OpenFlow virtual ports like CONTROLLER. When selecting strategies to test, we
only consider strategies for message types that we observe actually occurring in
communication between the switch and controller. Because controllers usually do
not use all the messages detailed in the OpenFlow speciﬁcation, this dramatically
reduces the number of strategies we need to test.
For malicious hosts, we consider the injection of ARP packets as discussed in
Sect. 2. We again deﬁne a list of malicious hosts at the start of each test. We use
time-based attack injection with these malicious hosts, where we launch attacks
for a few seconds at diﬀerent times during each test. The exact time of attack,
duration, and frequency of packet injection are conﬁgurable.
This automatic strategy generation enables us to quickly and easily gener-
ate tens of thousands of strategies in a manner that considers both message
structure and protocol semantics. In contrast, DELTA [24] uses blind fuzzing,
supplemented with a few manual tests. It is able to generate an unbounded num-
ber of strategies, but considers neither message structure nor protocol semantics.
OFTest and FLORENCE make no attempt at strategy generation and rely on a
manually developed set of tests. As a result, FLORENCE only tests 18 diﬀerent
scenarios and OFTest only covers a few hundred. BEADS is able to generate
several orders of magnitude more tests in a fraction of the time. While number
of test cases does not perfectly correspond with amount of search space covered,
320
S. Jero et al.
this does strongly suggest that BEADS can cover a much larger portion of the
search space, especially in combination with the new attacks that we ﬁnd.
3.4
Impact Assessment
Once we have executed a strategy, we automatically determine the impact based
on a variety of system and network characteristics. Our framework collects sev-
eral outputs and, at the end of each test, checks them for conditions indicating a
deviation from normal behavior and a possible vulnerability. If such conditions
are detected, we automatically schedule a re-test of the strategy to make sure
that the failure is repeatable. If it is, we declare this strategy to be a vulnerability.
We use four methods to determine if a tested strategy leads to unexpected
behavior: (1) OpenFlow error messages, (2) network conﬁguration changes, (3)
network reachability failures, and (4) controller or switch resource usage.
Since BEADS aims to identify actions that are the most damaging to the
network, we gradually ﬁlter actions based on their impact. First, we consider
the error messages, then the static network state, then the network connectivity,
and ﬁnally the controller and switch resource usage. Below we provide more
details about each of these and the rationale for considering them.
OpenFlow Error Messages. One mechanism we use to observe protocol devia-
tion is monitoring the OpenFlow connections for error messages. Error messages
are sent when an OpenFlow device (switch or controller) fails to parse an Open-
Flow message or the message indicates an invalid or unsupported option. These
messages indicate an anomalous condition in the OpenFlow connection, and that
some desired change to the network was not performed.
Network State. One of the most powerful indicators of undesirable changes
in the network are changes in the network state, including changes to routing,
access control lists (ACLs), and priorities of ﬂows in the network. We deﬁne
network state as the state of the ﬂow rules at each switch. The manager collects
the ﬂow rules from all switches at the end of each test, canonicalizes them, and
compares them to reference ﬂow rules from a benign run.
Unfortunately, the network state is not completely deterministic. Part
of our canonicalization process ﬁlters out known non-deterministic elements
(timestamps, etc.). Additionally, we use multiple benign test runs to detect
other non-deterministic elements and ﬁlter them out. Note that without detailed
knowledge of the application algorithms the SDN controller has implemented we
cannot decide whether a given non-deterministic rule is correct.
Reachability. Another protocol deviation indicator we use is pair-wise connec-
tivity tests. In particular, our test system uses pair-wise ICMP pings and iperf
to verify that all hosts are reachable from all other hosts. This is extremely eﬀec-
tive in detecting spooﬁng attacks and attacks on connectivity. It also detects
many manipulations of ﬂow rules that our network state detection mechanism
cannot detect due to non-determinism.
BEADS: Automated Attack Discovery in OpenFlow-Based SDN Systems
321
Controller or Switch Resource Usage. The ﬁnal protocol deviation indica-
tor we use is monitoring the RAM and CPU time used by the SDN controller and
switches. Excessive usage, compared to a benign baseline, indicates an oppor-