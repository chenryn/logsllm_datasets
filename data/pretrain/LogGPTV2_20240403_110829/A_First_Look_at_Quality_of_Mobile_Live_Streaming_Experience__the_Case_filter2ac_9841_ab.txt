every day through Periscope [13], which roughly trans-
lates into 40K live broadcasts ongoing all the time.
We developed a crawler by writing a mitmproxy in-
line script that exploits the /mapGeoBroadcastFeed re-
quest of the Periscope API. The script intercepts the
request made by the application after being launched
and replays it repeatedly in a loop with modiﬁed coor-
dinates and writes the response contents to a ﬁle.
It
also sets the include_replay attribute value to false
in order to only discover live broadcasts. In addition,
the script intercepts /getBroadcasts requests and re-
places the contents with the broadcast IDs found by the
crawler since previous request and extracts the viewer
information from the response to a ﬁle.
We faced two challenges: First, we noticed that when
specifying a smaller area, i.e. when user zooms in the
map, new broadcasts are discovered for the same area.
Therefore, to ﬁnd a large fraction of the broadcasts,
the crawler must explore the world using small enough
areas. Second, Periscope servers use rate limiting so
that too frequent requests will be answered with HTTP
429 (“Too many requests”), which forces us to pace the
requests in the script and increases the completion time
of a crawl. If the crawl takes a long time, it will miss
broadcast information.
Our approach is to ﬁrst perform a deep crawl and
then to select only the most active areas from that crawl
and query only them, i.e., perform a targeted crawl.
The reason is that deep crawl alone would produce too
coarse grained data about duration and popularity of
broadcasts because it takes over 10 minutes to ﬁnish. In
deep crawl, the crawler zooms into each area by dividing
it into four smaller areas and recursively continues do-
ing that until it no longer discovers substantially more
broadcasts. Such a crawl ﬁnds 1K-4K broadcasts1. Fig-
1This number is much smaller than the assumed 40K to-
tal broadcasts but we miss private broadcasts and those
with location undisclosed.
(a) duration and viewers
(b) viewers vs. start time
Figure 2: Broadcasting and viewers.
ure 1 shows the cumulative number of broadcasts found
as a result of crawls performed at diﬀerent times of day.
Figure 1(b) reveals that for all the diﬀerent crawls, half
of the areas contain at least 80% of all the broadcasts
discovered in the crawl. We select those areas from each
crawl, 64 areas in total, for a targeted crawl. We divide
them into four sets assigned to four diﬀerent simulta-
neously running crawlers, i.e., four emulators running
Periscope with diﬀerent user logged in (avoids rate lim-
iting) that repeatedly query the assigned areas. Such
targeted crawl completes in about 50s.
Figure 2 plots duration and viewing statistics about
four diﬀerent 4h-10h long targeted crawls started at dif-
ferent times of the day (note: both variables use the
same scale). Broadcast duration was calculated by sub-
tracting its start time (included in the description) from
the timestamp of the last moment the crawler discov-
ered the broadcast. Only broadcasts that ended during
the crawl were included (must not have been discov-
ered during the last 60s of a crawl) totalling to about
220K distinct broadcasts. Most of the broadcasts last
between 1 and 10 minutes and roughly half are shorter
than 4 minutes. The distribution has a long tail with
some broadcasts lasting for over a day.
The crawler gathered viewer information about 134K
broadcasts. Over 90% of broadcasts have less than 20
viewers on average but some attract thousands of view-
ers. It would be nice to know the contents of the most
popular broadcasts but the text descriptions are typi-
cally not very informative. Over 10% of broadcasts have
no viewers at all and over 80% of them are unavailable
for replay afterwards (replay information is contained in
the descriptions we collect about each broadcast), which
means that no one ever saw them. They are typically
much shorter than those that have viewers (avg dura-
tions 2min vs. 13 min) although some last for hours.
They represent about 2% of the total tracked broadcast
time. The local time of day shown in Figure 2(b) is de-
termined based on the broadcaster’s time zone. Some
viewing patterns are visible, namely a notable slump
in the early hours of the day, a peak in the morning,
and an increasing trend towards midnight, which sug-
gest that broadcasts typically have local viewers. This
# of requests to the API0204060live broadcasts found01000200030004000# of requests to the API050100live broadcasts found (%)0204060801000.000.250.500.751.000.111010010001000duration (min) / avg viewersfraction of broadcastsdurationviewers0510152005101520local time of day (h)avg viewers per brdcst479makes sense especially from the language preferences
point of view. Besides the diﬀerence between broad-
casts with and without any viewers, the popularity is
only very weakly correlated with its duration.
5. QUALITY OF EXPERIENCE
In this section, we study the data set generated through
automated viewing with the Android smartphones. It
consisted of streaming sessions with and without band-
width limit to the Galaxy S3 and S4 devices. We have
data of 4615 sessions in total: 1796 RTMP and 1586
HLS sessions without a bandwidth limit and 18-91 ses-
sions for each speciﬁc bandwidth limit. Since the num-
ber of recorded sessions is limited, the results should
be taken as indicative. The fact that our phone had a
high-speed non-mobile Internet access means that typi-
cal users may experience worse QoE because of a slower
and more variable Internet access with longer latency.
HLS seems to be used only when a broadcast is very
popular. A comparison of the average number of view-
ers seen in an RTMP and HLS session suggests that
the boundary number of viewers beyond which HLS is
used is somewhere around 100 viewers. By examining
the IP addresses from which the video was received,
we noticed that 87 diﬀerent Amazon servers were em-
ployed to deliver the RTMP streams. We could locate
only nine of them using maxmind.com, but among those
nine there were at least one in each continent, except
for Africa, which indicates that the server is chosen
based on the location of the broadcaster. All the HLS
streams were delivered from only two distinct IP ad-
dresses, which maxmind.com says are located somewhere
in Europe and in San Francisco. We do not currently
know how the video gets embedded into an HLS stream
for popular broadcasts but we assume that the RTMP
stream gets possibly transcoded, repackaged, and de-
livered to Fastly CDN by Periscope servers. The fact
that we used a single measurement location explains
the diﬀerence in server locations observed between the
protocols. As conﬁrmed by analysis in [18], the RTMP
server nearest to the broadcasting device is chosen when
the broadcast is initialized, while the Fastly CDN server
is chosen based on the location of the viewing device.
Since we had data from two diﬀerent devices, we per-
formed a number of Welch’s t-tests in order to under-
stand whether the data sets diﬀer signiﬁcantly. Only
the frame rate diﬀers statistically signiﬁcantly between
the two datasets. Hence, we combine the data in the
following analysis of video stalling and latency.
5.1 Playback Smoothness and Latency
We ﬁrst look at playback stalling. For RTMP streams,
the app reports the number of stall events and the av-
erage stall time of an event, while for HLS it only re-
ports the number of stall events. The stall ratio plotted
for the RTMP streams in Figure 3(a) is calculated as
summed up stall time divided by the total stream dura-
(a) no bandwidth limiting
(b) bandwidth limiting
Figure 3: Analysis of the stall ratio for RTMP
streams with and without bandwidth limiting.
tion including stall and playback time. The bandwidth
limit 100 in the ﬁgure refers to the unlimited case. Most
streams do not stall but there is a notable number of
sessions with stall ratio of 0.05-0.09, which corresponds
usually to a single stall event that lasts roughly 3-5s.
The boxplots in Figure 3(b) suggest that a vast major-
ity of the broadcasts are streamed with a bitrate inferior
to 2 Mbps because with access bandwidth greater than
that, the broadcasts exhibited very little stalling. As
for the broadcasts streamed using HLS, comparing their
stall count to that of the RTMP streams indicates that
stalling is rarer with HLS than with RTMP, which may
be caused by HLS being an adaptive streaming protocol
capable for quality switching on the ﬂy.
The average video bitrate is usually between 200 and
400 kbps (see Section 5.2), which is much less than the
2 Mbps limit we found. The most likely explanation
to this discrepancy is the chat feature. We measured
the phone traﬃc with and without chat and observed a
substantial increase in traﬃc when the chat was on. A
closer look revealed that the JSON encoded chat mes-
sages are received even when chat is oﬀ, but when the
chat is on, image downloads from Amazon S3 servers ap-
pear in the traﬃc. The reason is that the app downloads
proﬁle pictures of chatting users and displays them next
to their messages, which may cause a dramatic increase
in the traﬃc. For instance, we saw an increase of the ag-
gregate data rate from roughly 500kbps to 3.5Mbps in
one experiment. The precise eﬀect on traﬃc depends on
the number of chatting users, their messaging rate, the
fraction of them having a proﬁle picture, and the format
and resolution of proﬁle pictures. We also noticed that
some pictures were downloaded multiple times, which
indicates that the app does not cache them.
Each broadcast was watched for exactly 60s from the
moment the Teleport button was pushed. We calculate
the join time, often also called startup latency, by sub-
tracting the summed up playback and stall time from
60s and plot it in Figure 4(a) for the RTMP streams. In
addition, we plot in Figure 4(b) the playback latency,
which is equivalent to the end-to-end latency. The y-
axis scale was cut leaving out some outliers that ranged
0.000.250.500.751.000.000.250.500.751.00stall ratiofraction of broadcasts0.000.250.500.751.000.512345678910100bandwidth limit (Mbps)stall ratio480(a) join time
(b) playback latency
(a)
(b)
Figure 4: Boxplots showing that playback la-
tency and join time of RTMP streams increase
when bandwidth is limited. Notice the diﬀer-
ence in scales.
up to 4min in the case of playback latency. Both in-
crease when bandwidth is limited.
In particular, join
time grows dramatically when bandwidth drops to 2Mbps
and below. The average playback latency was roughly
a few seconds when the bandwidth was not limited.
Figure 5: Video delivery latency is much longer
with HSL compared to RTMP.
Through experiments where we controlled both the
broadcasting and receiving client and captured both de-
vices traﬃc, we noticed that the broadcasting client ap-
plication regularly embeds an NTP timestamp into the
video data, which is subsequently received by each view-
ing client. The experiments indicated that the NTP
timestamps transmitted by the broadcasting device is
very close to the tcpdump timestamps in a trace cap-
tured by a tethering machine. Hence, the timestamps
enable calculating the delivery latency by subtracting
the NTP timestamp value from the time of receiving
the packet containing it, also for the HLS sessions for
which the playback metadata does not include it. We
calculate the average over all the latency samples for
each broadcast. Figure 5 shows the distribution of the
video delivery latency for the sessions that were not
bandwidth limited. Even if our packet capturing ma-
chine was NTP synchronized, we sometimes observed
small negative time diﬀerences indicating that the syn-
chronization was imperfect. Nevertheless, the results
clearly demonstrate the impact of using HLS on the
Figure 6: Characteristics of the captured videos.
delivery latency. RTMP stream delivery is very fast
happening in less than 300ms for 75% of broadcasts on
average, which means that the majority of the few sec-
onds of playback latency with those streams comes from
buﬀering.
In contrast, the delivery latency with HLS
streams is over 5s on average. As expected, the deliv-
ery latency grows when bandwidth is limited similarly
to the playback latency. A more detailed analysis of the
latency can be found in [18]. The delivery latency we
observed matches quite well with their delay breakdown
results (end-to-end delay excluding buﬀering).
In summary, HLS appears to be a fallback solution
to the RTMP stream. The RTMP servers can push