# Evaluation and Comparison

## A. Varying the Number of Partitions
Figure 6a shows the throughput achieved by Wren with 4, 8, and 16 partitions per Data Center (DC). The bars represent the throughput of Wren normalized with respect to the throughput achieved by Cure in the same setting. The numbers on top of each bar indicate the absolute throughput achieved by Wren.

### Key Observations:
1. **Throughput Improvement**: Wren consistently achieves higher throughput than Cure, with a maximum improvement of 38%.
2. **Performance Scalability**: The performance improvement is more pronounced with more partitions and lower read-write (r:w) ratios. More partitions touched by transactions and more writes increase the likelihood that a read in Cure targets a laggard partition, leading to higher latencies, lower resource efficiency, and worse throughput.
3. **Efficient Scale-Out Support**: When increasing the number of partitions from 4 to 16, throughput increases by 3.76x for write-heavy workloads and 3.88x for read-heavy workloads, approximating the ideal improvement of 4x.

## B. Varying the Number of DCs
Figure 6b shows the throughput achieved by Wren with 3 and 5 DCs (16 partitions per DC). The bars represent the throughput normalized with respect to Cureâ€™s throughput in the same scenario. The numbers on top of the bars indicate the absolute throughput achieved by Wren.

### Key Observations:
1. **Throughput Improvement**: Wren obtains higher throughput than Cure for all workloads, achieving an improvement of up to 43%.
2. **Scalability with DCs**: Wren's performance gains are higher with 5 DCs because the metadata overhead in BiST is constant, while in Cure it grows linearly with the number of DCs.
3. **Throughput Scaling**: The throughput achieved by Wren with 5 DCs is 1.53x, 1.49x, and 1.44x higher than the throughput achieved with 3 DCs for the 95:5, 90:10, and 50:50 workloads, respectively, approximating the ideal improvement of 1.66x.
4. **Impact of Write Intensity**: Higher write intensity reduces the performance gain when scaling from 3 to 5 DCs due to the increased number of updates being replicated.

## C. Resource Efficiency
Figure 7a shows the amount of data exchanged in Wren to run the stabilization protocol and to replicate updates, with the default workload. The results are normalized with respect to the amounts of data exchanged in Cure at the same throughput.

### Key Observations:
- With 5 DCs, Wren exchanges up to 37% fewer bytes for replication and up to 60% fewer bytes for running the stabilization protocol.
- Updates, snapshots, and stabilization messages carry 2 timestamps in Wren versus 5 in Cure, enhancing resource efficiency and scalability.

## D. Update Visibility
Figure 7b shows the Cumulative Distribution Function (CDF) of the update visibility latency with 3 DCs. The visibility latency of an update X in DCi is the difference between the wall-clock time when X becomes visible in DCi and the wall-clock time when X was committed in its original DC.

### Key Observations:
- **Visibility Latency**: Cure achieves lower update visibility latencies than Wren. The remote update visibility time in Wren is slightly higher (68 vs. 59 milliseconds in the worst case, i.e., 15% higher) because Wren tracks dependencies at the granularity of the DC, while Cure only tracks local and remote dependencies.
- **Local Updates**: Local updates become visible immediately in Cure, whereas in Wren they become visible after a few milliseconds as Wren chooses a slightly older snapshot.
- **Trade-Off**: These slightly higher update visibility latencies are a small price to pay for the performance improvements offered by Wren.

## E. Related Work

### TCC Systems
- **Cure [8]**: A transaction T can be assigned a snapshot that has not been installed by some partitions. If T reads from any of such laggard partitions, it blocks. Wren, on the contrary, achieves low-latency non-blocking reads by either reading from a snapshot that is already installed in all partitions or from the client-side cache.
- **Occult [34]**: Implements a master-slave design where only the master replica of a partition accepts writes and replicates them asynchronously. This can lead to retries and potential network partitions, which negatively impact performance. Wren implements always-available transactions that complete wholly within a DC and never block nor retry read operations.
- **SwiftCloud [16]**: Uses a sequencer-based approach to totally order updates, ensuring causal consistency but making horizontal scalability cumbersome. Wren, instead, implements decentralized protocols that efficiently enable horizontal scalability.

### CC Systems
- Many CC systems provide weaker semantics than TCC. Systems like COPS [15], Orbe [24], GentleRain [20], ChainReaction [25], POCC [38], and COPS-SNOW [7] implement read-only transactions, while Eiger [21] supports write-only transactions. These systems either block reads, require large amounts of metadata, or rely on a sequencer process per DC.

### Highly Available Transactional Systems
- **Bailis et al. [39], [40]**: Propose several flavors of transactional protocols that support read-write transactions with fine-grained dependency tracking and weaker consistency levels than CC.
- **TARDiS [41]**: Supports merge functions over conflicting states, requiring significant metadata and resource-intensive garbage collection.
- **GSP [42]**: Uses a system-wide broadcast primitive to totally order updates, suitable for non-partitioned data stores. Wren, designed for sharded applications, achieves scalability and consistency through lightweight protocols.

### Strongly Consistent Transactional Systems
- Systems like Spanner [44], Lynx [45], Jessy [46], Clock-SRM [47], SDUR [48], and Droopy [49] support geo-replication with stronger consistency guarantees but require cross-DC coordination, limiting availability. Wren targets applications that can tolerate weaker consistency, providing low latency, high throughput, scalability, and availability.

### Client-Side Caching
- Caching at the client side is primarily used to support disconnected clients, especially in mobile and wide area network settings [50], [51], [52]. Wren uses client-side caching to guarantee consistency.

## F. Conclusion
We have presented Wren, the first TCC system that simultaneously implements non-blocking reads, achieving low latency and allowing applications to scale out by sharding. Wren introduces novel protocols CANToR, BDT, and BiST, using only 2 timestamps per update and per snapshot, enabling scalability regardless of the system size. Compared to the state-of-the-art TCC system, Wren achieves lower latencies and higher throughput, with only a slight penalty on the freshness of data exposed to clients.

## G. Acknowledgements
We thank the anonymous reviewers, Fernando Pedone, Sandhya Dwarkadas, Richard Sites, and Baptiste Lepers for their valuable suggestions and helpful comments. This research has been supported by The Swiss National Science Foundation through Grant No. 166306, by an EcoCloud post-doctoral research fellowship, and by Amazon through AWS Cloud Credits.

## H. References
[References listed here as provided in the original text]

---

This optimized version aims to improve clarity, coherence, and professionalism, making the content easier to understand and follow.