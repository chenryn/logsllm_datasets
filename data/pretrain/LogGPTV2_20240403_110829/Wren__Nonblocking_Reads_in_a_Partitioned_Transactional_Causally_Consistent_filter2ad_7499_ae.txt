 1.3
 1.2
 1.1
 1
95:5
90:10
Workload
50:50
 1
95:5
90:10
Workload
50:50
(a) Throughput when varying the number of partitions/DC (3 DCs).
Fig. 6: Throughput achieved by Wren when increasing the number of partition per DC (a) and DCs in the system (b). Each
bar represents the throughput of Wren normalized w.r.t. to Cure (y axis starts from 1). The number on top of each bar reports
the absolute value of the throughput achieved by Wren in 1000 x TX/s. Wren consistently achieves better throughput than
Cure and achieves good scalability both when increasing the number of partitions and the number of DCs.
(b) Throughput when varying the number of DCs (16 partitions/DCs).
peak throughput of all three systems decreases with a lower
r:w ratio, because writing more items increases the duration of
the commit and the replication overhead. Similarly, a higher
value of p decreases throughput, because more partitions are
contacted during a transaction.
D. Varying the number of partitions
Figure 6a reports the throughput achieved by Wren with 4,
8 and 16 partitions per DC. The bars represent the throughput
of Wren normalized with respect to the throughput achieved
by Cure in the same setting. The number on top of each bar
represents the absolute throughput achieved by Wren.
The plots show three main results. First, Wren consistently
achieves higher throughput than Cure, with a maximum im-
provement of 38%. Second, the performance improvement of
Wren is more evident with more partitions and lower r:w
ratios. More partitions touched by transactions and more writes
increase the chances that a read in Cure targets a laggard
partition and blocks, leading to higher latencies, lower resource
efﬁciency, and worse throughput. Third, Wren provides efﬁ-
cient support for application scale-out. When increasing the
number of partitions from 4 to 16, throughput increases by
3.76x for the write-heavy and 3.88x for read-heavy workload,
approximating the ideal improvement of 4x.
Wren L
R
Wren 3DC
5DC
Cure R
Cure
1
0.8
0.6
0.4
0.2
e
r
u
C
t
.
r
.
w
.
m
r
o
n
s
e
t
y
B
 1
 0.8
 0.6
F
D
C
 0.4
 0.2
Repl. Stabl.
Protocol
 0
 0  20  40  60  80  100
 Visibility latency (msec)
Fig. 7: (a) BiST incurs lower overhead than Cure to track the
dependencies of replicated updates and to determine trans-
actional snapshots (default workload). (b) Wren achieves a
slightly higher Remote update visibility latency w.r.t. Cure,
and makes Local updates visible when they are within the
local stable snapshot (3 DCs).
10
E. Varying the number of DCs
Figure 6b shows the throughput achieved by Wren with
3 and 5 DCs (16 partitions per DC). The bars represent the
throughput normalized with respect to Cure’s throughput in
the same scenario. The numbers on top of the bars indicate
the absolute throughput achieved by Wren.
Wren obtains higher throughput than Cure for all workloads,
achieving an improvement of up to 43%. Wren performance
gains are higher with 5 DCs, because the meta-data overhead
is constant in BiST, while in Cure it grows linearly with the
number of DCs. The throughput achieved by Wren with 5
DCs is 1.53x, 1.49x, and 1.44x higher than the throughput
achieved with 3 DCs, for the 95:5, 90:10 and 50:50 workloads,
respectively, approximating the ideal improvement of 1.66x.
A higher write intensity reduces the performance gain when
scaling from 3 to 5 DCs, because it implies more updates
being replicated.
F. Resource efﬁciency
Figure 7a shows the amount of data exchanged in Wren
to run the stabilization protocol and to replicate updates,
with the default workload. The results are normalized with
respect to the amounts of data exchanged in Cure at the same
throughput. With 5 DCs, Wren exchanges up to 37% fewer
bytes for replication and up to 60% fewer bytes for running
the stabilization protocol. With 5 DCs, updates, snapshots and
stabilization messages carry 2 timestamps in Wren versus 5
in Cure.
G. Update visibility
Figure 7b shows the CDF of the update visibility latency
with 3 DCs. The visibility latency of an update X in DCi is
the difference between the wall-clock time when X becomes
visible in DCi and the wall-clock time when X was committed
in its original DC (which is DCi itself in the case of local
visibility latency). The CDFs are computed as follows: we
ﬁrst obtain the CDF on every partition and then we compute
the mean for each percentile.
Cure achieves lower update visibility latencies than Wren.
The remote update visibility time in Wren is slightly higher
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:26:55 UTC from IEEE Xplore.  Restrictions apply. 
than in Cure (68 vs. 59 milliseconds in the worst case,
i.e., 15% higher), because Wren tracks dependencies at the
granularity of the DC, while Wren only tracks local and remote
dependencies (see § III-C Figure 2). Local updates become
visible immediately in Cure. In Wren they become visible
after a few milliseconds, because Wren chooses a slightly
older snapshot. We argue that these slightly higher update
visibility latencies are a small price to pay for the performance
improvements offered by Wren.
VI. RELATED WORK
TCC systems. In Cure [8] a transaction T can be assigned
a snapshot that has not been installed by some partitions. If
T reads from any of such laggard partitions, it blocks. Wren,
on the contrary, achieves low-latency nonblocking reads by
either reading from a snapshot that is already installed in all
partitions, or from the client-side cache.
Occult [34] implements a master-slave design in which
only the master replica of a partition accepts writes and
replicates them asynchronously. The commit of a transaction,
then, may span multiple DCs. A replicated item can be read
before its causal dependencies are received, hence achieving
the lowest data staleness. However, a read may have have to
be retried several times in case of missing dependencies, and
may even have to contact the remote master replica, which
might not be accessible due to a network partition. The effect
of retrying in Occult has a negative impact on performance,
that is comparable to blocking the read to receive the right
value to return. Wren, instead, implements always-available
transactions that complete wholly within a DC, and never
block nor retry read operations.
In SwiftCloud [16] clients declare the items in which they
are interested, and the system sends them the corresponding
updates, if any. SwiftCloud uses a sequencer-based approach,
which totally orders updates, both those generated in a DC
and those received from remote DCs. The sequencer-based
approach ensures that the stream of updates pushed to clients
is causally consistent. However, sequencing the updates also
makes it cumbersome to achieve horizontal scalability. Wren,
instead,
implements decentralized protocols that efﬁciently
enable horizontal scalability.
Cure and SwiftCloud use dependency vectors with one entry
per DC. Occult uses one dependency timestamp per master
replica. By contrast, Wren timestamps items and snapshots
with constant dependency meta-data, which increases resource
efﬁciency and scalability.
The trade-off that Wren makes to achieve low latency,
availability and scalability is that it exposes snapshots slightly
older than those exposed by other TCC systems.
CC systems. Many CC systems provide weaker semantics
than TCC. COPS [15], Orbe [24], GentleRain [20], Chain-
Reaction [25], POCC [38] and COPS-SNOW [7] implement
read-only transactions. Eiger [21] additionally supports write-
only transactions. These systems either block a read while
waiting for the receipt of remote updates [20], [24], [38],
require a large amount of meta-data [7], [15], [21], or rely
on a sequencer process per DC [25].
Highly available transactional systems. Bailis et al. [39],
[40] propose several ﬂavors of transactional protocols that are
available and support read-write transactions. These protocols
rely on ﬁne-grained dependency tracking and enforce a con-
sistency level that is weaker than CC. TARDiS [41] supports
merge functions over conﬂicting states of the application,
rather than at key granularity. This ﬂexibility requires a sig-
niﬁcant amount of meta-data and a resource-intensive garbage
collection scheme to prune old states. Moreover, TARDiS does
not implement sharding. GSP [42] is an operational model
for replicated data that supports highly available transactions.
GSP targets non-partitioned data stores and uses a system-wide
broadcast primitive to totally order the updates. Wren, instead,
is designed for applications that scale-out by sharding and
achieves scalability and consistency by lightweight protocols.
Strongly consistent transactional systems. Many systems
support geo-replication with consistency guarantees stronger
than CC (e.g., Spanner
[44],
Lynx [45], Jessy [46], Clock-SRM[47], SDUR [48] and
Droopy [49]). These systems require cross-DC coordination to
commit transactions, hence they are not always-available [11],
[19]. Wren targets a class of applications that can tolerate a
weaker form of consistency, and for these applications it pro-
vides low latency, high throughput, scalability and availability.
Client-side caching. Caching at the client side is a technique
primarily used to support disconnected clients, especially in
mobile and wide area network settings [50], [51], [52]. Wren,
instead, uses client-side caching to guarantee consistency.
[43], Gemini
[1], Walter
VII. CONCLUSION
We have presented Wren, the ﬁrst TCC system that at the
same time implements nonblocking reads thereby achieving
low latency and allows applications to scale-out by sharding.
Wren implements a novel transactional protocol, CANToR,
that deﬁnes transaction snapshots as the union of a fresh causal
snapshot and the contents of a client-side cache. Wren also
introduces BDT, a new dependency tracking protocol, and
BiST, a new stabilization protocol. BDT and BiST use only 2
timestamps per update and per snapshot, enabling scalability
regardless of the size of the system. We have compared Wren
with the state-of-the-art TCC system, and we have shown
that Wren achieves lower latencies and higher throughput,
while only slightly penalizing the freshness of data exposed
to clients.
ACKNOWLEDGEMENTS
We thank the anonymous reviewers, Fernando Pedone,
Sandhya Dwarkadas, Richard Sites and Baptiste Lepers for
their valuable suggestions and helpful comments. This re-
search has been supported by The Swiss National Science
Foundation through Grant No. 166306, by an EcoCloud post-
doctoral research fellowship, and by Amazon through AWS
Cloud Credits.
11
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:26:55 UTC from IEEE Xplore.  Restrictions apply. 
REFERENCES
[1] J. C. Corbett, J. Dean, M. Epstein, and et al., “Spanner: Google’s
Globally-distributed Database,” in Proc. of OSDI, 2012.
[2] G. DeCandia, D. Hastorun, M. Jampani, and et al., “Dynamo: Amazon’s
Highly Available Key-value Store,” in Proc. of SOSP, 2007.
[3] R. Nishtala, H. Fugal, S. Grimm, and et al., “Scaling Memcache at
Facebook,” in Proc. of NSDI, 2013.
[4] S. A. Noghabi, S. Subramanian, P. Narayanan, and et al., “Ambry:
LinkedIn’s Scalable Geo-Distributed Object Store,” in Proc. of SIG-
MOD, 2016.
[5] A. Verbitski, A. Gupta, D. Saha, and et al., “Amazon Aurora: De-
sign Considerations for High Throughput Cloud-Native Relational
Databases,” in Proc. of SIGMOD, 2017.
[6] F. Cruz, F. Maia, M. Matos, R. Oliveira, J. a. Paulo, J. Pereira, and
R. Vilac¸a, “MeT: Workload Aware Elasticity for NoSQL,” in Proceed-
ings of the 8th ACM European Conference on Computer Systems, ser.
EuroSys ’13, 2013.
[7] H. Lu, C. Hodsdon, K. Ngo, S. Mu, and W. Lloyd, “The SNOW Theorem
and Latency-Optimal Read-Only Transactions,” in OSDI, 2016.
[8] D. D. Akkoorath, A. Tomsic, M. Bravo, and et al., “Cure: Strong
semantics meets high availability and low latency,” in Proc. of ICDCS,
2016.
[9] M. Ahamad, G. Neiger, J. E. Burns, P. Kohli, and P. W. Hutto, “Causal
Memory: Deﬁnitions, Implementation, and Programming,” Distributed
Computing, vol. 9, no. 1, pp. 37–49, 1995.
[10] H. Attiya, F. Ellen, and A. Morrison, “Limitations of Highly-Available
Eventually-Consistent Data Stores,” in Proc. of PODC, 2015.
[11] P. Mahajan, L. Alvisi, and M. Dahlin, “Consistency, Availability, Con-
vergence,” Computer Science Department, University of Texas at Austin,
Tech. Rep. TR-11-22, May 2011.
[12] M. P. Herlihy and J. M. Wing, “Linearizability: A Correctness Condition
for Concurrent Objects,” ACM Trans. Program. Lang. Syst., vol. 12,
no. 3, pp. 463–492, Jul. 1990.
[13] K. Birman, A. Schiper, and P. Stephenson, “Lightweight Causal and
Atomic Group Multicast,” ACM Trans. Comput. Syst., vol. 9, no. 3, pp.
272–314, Aug. 1991.
[14] R. Ladin, B. Liskov, L. Shrira, and S. Ghemawat, “Providing High Avail-
ability Using Lazy Replication,” ACM Trans. Comput. Syst., vol. 10,
no. 4, pp. 360–391, Nov. 1992.
[15] W. Lloyd, M. J. Freedman, M. Kaminsky, and D. G. Andersen, “Don’t
Settle for Eventual: Scalable Causal Consistency for Wide-area Storage
with COPS,” in Proc. of SOSP, 2011.
[16] M. Zawirski, N. Preguic¸a, S. Duarte, and et al., “Write Fast, Read in
the Past: Causal Consistency for Client-Side Applications,” in Proc. of
Middleware, 2015.
[17] P. Bailis, A. Ghodsi, J. M. Hellerstein, and I. Stoica, “Bolt-on Causal
Consistency,” in Proc. of SIGMOD, 2013.
[18] L. Lamport, “Time, Clocks, and the Ordering of Events in a Distributed
System,” Commun. ACM, vol. 21, no. 7, pp. 558–565, Jul. 1978.
[19] E. A. Brewer, “Towards Robust Distributed Systems (Abstract),” in Proc.
of PODC, 2000.
[20] J. Du, C. Iorgulescu, A. Roy, and W. Zwaenepoel, “GentleRain: Cheap
and Scalable Causal Consistency with Physical Clocks,” in Proc. of
SoCC, 2014.
[21] W. Lloyd, M. J. Freedman, M. Kaminsky, and D. G. Andersen, “Stronger
Semantics for Low-latency Geo-replicated Storage,” in Proc. of NSDI,
2013.
[22] R. H. Thomas, “A Majority Consensus Approach to Concurrency
Control for Multiple Copy Databases,” ACM Trans. Database Syst.,
vol. 4, no. 2, pp. 180–209, Jun. 1979.
[23] M. Shapiro, N. Preguic¸a, C. Baquero, and M. Zawirski, “Conﬂict-free
Replicated Data Types,” in Proc. of SSS, 2011.
[24] J. Du, S. Elnikety, A. Roy, and W. Zwaenepoel, “Orbe: Scalable Causal
Consistency Using Dependency Matrices and Physical Clocks,” in Proc.
of SoCC, 2013.
[25] S. Almeida, J. a. Leit˜ao, and L. Rodrigues, “ChainReaction: A Causal+
Consistent Datastore Based on Chain Replication,” in Proc. of EuroSys,
2013.
[26] B. Atikoglu, Y. Xu, E. Frachtenberg, S. Jiang, and M. Paleczny,
“Workload Analysis of a Large-scale Key-value Store,” in Proc. of
SIGMETRICS, 2012.
[27] S. S. Kulkarni, M. Demirbas, D. Madappa, B. Avva, and M. Leone,
“Logical Physical Clocks,” in Proc. of OPODIS, 2014.
[28] C. Gunawardhana, M. Bravo, and L. Rodrigues, “Unobtrusive Deferred
Update Stabilization for Efﬁcient Geo-Replication,” in Proc. of ATC,
2017.
[29] M. Roohitavaf, M. Demirbas, and S. Kulkarni, “CausalSpartan: Causal
Consistency for Distributed Data Stores using Hybrid Logical Clocks,”
in SRDS, 2017.
[30] L. Lamport, “The Part-time Parliament,” ACM Trans. Comput. Syst.,
vol. 16, no. 2, pp. 133–169, May 1998.
[31] H. Lu, K. Veeraraghavan, P. Ajoux, and et al., “Existential Consistency:
Measuring and Understanding Consistency at Facebook,” in Proc. of
SOSP, 2015.
[32] J. Du, S. Elnikety, and W. Zwaenepoel, “Clock-SI: Snapshot isolation
for partitioned data stores using loosely synchronized clocks,” in Proc
of SRDS, 2013.
[33] D. Didona, K. Spirovska, and W. Zwaenepoel, “Okapi: Causally Consis-
tent Geo-Replication Made Faster, Cheaper and More Available,” ArXiv
e-prints, https://arxiv.org/abs/1702.04263, Feb. 2017.
[34] S. A. Mehdi, C. Littley, N. Crooks, L. Alvisi, N. Bronson, and W. Lloyd,
“I Can’t Believe It’s Not Causal! Scalable Causal Consistency with No
Slowdown Cascades,” in Proc. of NSDI, 2017.
[35] B. F. Cooper, A. Silberstein, E. Tam, R. Ramakrishnan, and R. Sears,
“Benchmarking Cloud Serving Systems with YCSB,” in Proc. of SoCC,
2010.
[36] F. Nawab, V. Arora, D. Agrawal, and A. El Abbadi, “Minimizing
Commit Latency of Transactions in Geo-Replicated Data Stores,” in
Proc. of SIGMOD, 2015.
[37] O. Balmau, D. Didona, R. Guerraoui, and et al., “TRIAD: Creating
Synergies Between Memory, Disk and Log in Log Structured Key-Value
Stores,” in Proc. of ATC, 2017.
[38] K. Spirovska, D. Didona, and W. Zwaenepoel, “Optimistic Causal
Consistency for Geo-Replicated Key-Value Stores,” in Proc. of ICDCS,
2017.
[39] P. Bailis, A. Davidson, A. Fekete, A. Ghodsi, J. M. Hellerstein, and
I. Stoica, “Highly Available Transactions: Virtues and Limitations,”
Proc. VLDB Endow., vol. 7, no. 3, pp. 181–192, Nov. 2013.
[40] P. Bailis, A. Fekete, J. M. Hellerstein, and et al., “Scalable Atomic
Visibility with RAMP Transactions,” in Proc. of SIGMOD, 2014.
[41] N. Crooks, Y. Pu, N. Estrada, T. Gupta, L. Alvisi, and A. Clement,
“TARDiS: A Branch-and-Merge Approach To Weak Consistency,” in
Proc. of SIGMOD, 2016.
[42] S. Burckhardt, D. Leijen, J. Protzenko, and M. Fahndrich, “Global
Sequence Protocol: A Robust Abstraction for Replicated Shared State,”
in Proceedings of ECOOP, 2015.
[43] Y. Sovran, R. Power, M. K. Aguilera, and J. Li, “Transactional Storage
for Geo-replicated Systems,” in Proc. of SOSP, 2011.
[44] V. Balegas, C. Li, M. Najafzadeh, and et al., “Geo-Replication: Fast If
Possible, Consistent If Necessary,” Data Engineering Bulletin, vol. 39,
no. 1, pp. 81–92, Mar. 2016.
[45] Y. Zhang, R. Power, S. Zhou, and et al., “Transaction Chains: Achieving
Serializability with Low Latency in Geo-distributed Storage Systems,”
in Proc. of SOSP, 2013.
[46] M. S. Ardekani, P. Sutra, and M. Shapiro, “Non-monotonic Snapshot
Isolation: Scalable and Strong Consistency for Geo-replicated Transac-
tional Systems,” in Proc. of SRDS, 2013.
[47] J. Du, D. Sciascia, S. Elnikety, W. Zwaenepoel, and F. Pedone, “Clock-
RSM: Low-Latency Inter-datacenter State Machine Replication Using
Loosely Synchronized Physical Clocks,” in Proc. of DSN, 2014.
[48] D. Sciascia and F. Pedone, “Geo-replicated storage with scalable de-
ferred update replication,” in Proc. of DSN, 2013.
[49] S. Liu and M. Vukoli´c, “Leader Set Selection for Low-Latency Geo-
Replicated State Machine,” IEEE Transactions on Parallel and Dis-
tributed Systems, vol. 28, no. 7, pp. 1933–1946, July 2017.
[50] M. E. Bjornsson and L. Shrira, “BuddyCache: High-performance Object
Storage for Collaborative Strong-consistency Applications in a WAN,”
in Proc. of OOPSLA, 2002.
[51] D. Perkins, N. Agrawal, A. Aranya, and et al., “Simba: Tunable End-
to-end Data Consistency for Mobile Apps,” in Proc. of EuroSys, 2015.
[52] I. Zhang, N. Lebeck, P. Fonseca, and et al., “Diamond: Automating
Data Management and Storage for Wide-Area, Reactive Applications,”
in Proc. of OSDI, 2016.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:26:55 UTC from IEEE Xplore.  Restrictions apply. 
12