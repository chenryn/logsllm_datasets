encodingURI
activity
type SourceActivity
Fig.2.BasicstructureoftheCBNbycombiningT andT.
0 1
X.Hanetal./CAAITransactionsonIntelligenceTechnology1(2016)61e71 65
process,wetakeT andT asinput,andproducesimilarityde- Table1
0 1
greesbetweencorrespondingnodesofT andT asoutput.There Similarityestimationresult.
0 1
aretwostepsinthisprocessbasedonpairwisegraph,including NodeinT NodeinT Similarity(Sim)
0 1
initialmatchingandsimilaritypropagation.Ininitialmatching, Fault Fault 0.90
we additionally exploit the structural information in attribute DetailEntries DetailEntries 0.68
level. Given two nodes m0 and n1 of T and T which has DetailEntries FaultEntries 0.44
0 1
FaultEvent FaultEvent 0.19
attributesetsAmandAnrespectively,wewillmatchattributes
SourceActivity Activity 0.19
of A and A as well as. names of m and n . The computation
0 1 Name Name 0.17
formulais Type Type 0.17
Value Value 0.17
jA ∩A j FaultCode FaultCode 0.16
Sim0ðm ;n Þ¼q m n þð1(cid:2)qÞ StrSimðm ;n Þ ð1Þ Description FaultString 0.10
0 1 jA jþjA j 0 1
m n Scope EncodingURI 0.10
wherejA jisthenumberofelementsinsetA ,StrSimðm ;n Þ
m m 0 1
represents the similarity value of m and n1 by string match-
0 This computation process is performed iteratively to estimate
ing,andisaweightcoefficientfixedbyusers.Basedoninitial
similarity degrees between nodes of T and T . After this
matching results, we construct the pairwise graph in step (2). 0 1
process,thecomputedsimilaritydegreeswillbenormalizedto
A portion of this pairwise graph is shown in Fig. 3, where
the range of [0, 1].
some nodes of low degrees in initial matching are ignored.
Table 1 shows a portion of the similarity estimation result.
Then, similarity propagation is executed based on this graph.
Ifwesetthevalueoffilteringthresholdto0.1,thepairswhose
The computation formula for similarity propagation is
similaritydegreeisnobiggerthan0.1willbedeletedfromthe
CBN.
As shown in Table 1, an example of the matching results
could be the correspondence between node “DetailEntries”
Simiþ1ðn 0;n 1Þ¼ XSimiðn 0;n 1Þþ and node ”FaultEntries”.
Simiðm ;m Þ uððm ;m Þðn ;n ÞÞ ð2Þ
0 1 0 1 0 1
ðm0;m1Þ2NeigSetððn0;n1ÞÞ 4.2. Similarity-based Bayesian learning algorithm for
computing CBN probabilities
Inthis subsection, wewill introduce how tocompute CBN
probabilities using similarity-based learning algorithm. Ac-
where Simiðn 0;n 1Þ indicates the similarity degree of pairwise cording to computed similarity degrees, the conditional
node ðn 0;n 1Þ in the i-th iteration, uððm 0;m 1Þðn 0;n 1ÞÞ denotes probabilities of CBNs will be obtained.
the weight of edge between the given nodes (whose value Based on the similarity result, the combination part of
equalstothatof1dividedbytheoutgoingedgenumberofthe CBN can be created as shown in Fig. 4. Then, the condi-
source node for this edge), and NeigSetððn 0;n 1ÞÞ represents tional probabilities related to the nodes of this part will be
the set of neighbors of node ðn 0;n 1Þ in the pairwise graph. learned from training data. In fact, the similar counterparts
Fault,
Fault
FaultCode, DetailEntries, DetailEntries, Description,
FaultCode DetailEntries FaultEntries FaultString
Scope,
Name, SourceActivity,
encodingURI
name activity
Type, Value, FaultEvent,
type value faultEvent
Fig.3.AportionofpairwisegraphformatchingT andT.
1 2
66 X.Hanetal./CAAITransactionsonIntelligenceTechnology1(2016)61e71
Fault
FaultCode DetailEntries FaultEntries
name
FaultEvent
value activity
type
SourceActivity event
Fig.4.CombinationpartoftheCBN.
of log records in training dataset also influence corre- combination similarity degrees between nodes of T (T ) and
0 1
sponding probability distribution of the CBN. Taking the CBN, as shown in Table 2.
influence of the similar counterparts into consideration, we For the example mentioned above, suppose R and R are
0 1
will calculate the probabilities in the combination part of the record sets of training data, from which T and T can be
0 1
CBN, based on similarity degrees between nodes of T and extracted.LetRc andRc bethesubsetsofR andR withfault
i 0 1 0 1
CBN. categoryc.Then,letRc betheunionofthesetwosubsets,i.e.,
Before going into details of probability computation, it is Rc ¼Rc∪Rc . Accordingly, given a log record r from record
0 1 i
necessary to introduce the concept of combination similarity set R, we use SD(r, m, n) to denote the similarity degree of
i i i
degree CSim, which represents the similarity between a node treenode(correspondstoelement)m ofr andnodenofCBN.
i i
of T and its corresponding node (with the same name) of The similarity degree SD will play an important role in
i
constructed CBN. Assume n and s are the CBN nodes which learning process, which can be computed by the following
correspond to nodes ni and si respectively. The CSim is formula
computedrecursivelyfromleaftorootnodesforeachschema
tree, using the following formula 8SDðr i;m i;nÞ¼
: SDðr;paðeÞ;pÞ StrEquaðe;qÞ; q2N
t
Description Description 1 paðeÞ2NodeSetðrÞ
... ... ...
(b)CSimbetweennodesofT andCBN where pa(e) denotes the parent node of node e in record r,
1
NodeSet(r) is the node (or element) set of record r, and
NodeinT NodeinCBN Combinationsimilarity(CSim)
1 1
StrEqual(e,q)meanswhethereequalstoqbystringmatching.
Fault Fault 0.77
FortheCBNofcategoryc,wesupposeeachnodehasnode
DetailEntries DetailEntries 0.63
FaultEntries FaultEntries 1 c as its father. Thus, the root element of each record in this
FaultCode FaultCode 1 category has the same parent node c. The conditional proba-
... ... ... bility can be computed as follows
X.Hanetal./CAAITransactionsonIntelligenceTechnology1(2016)61e71 67
P
SimNumðr;p;qÞ S8DDðmD;n;cÞ¼
pðn¼qjpaðnÞ¼p;cÞ¼ u2P Ns∪r2 NTRC r2P RCSimNumðr;p;UÞ ð6Þ < :C a 0r ;S gi mm aið xm Si i; mn DÞ; iðmD;m iÞ$CSim iðm i;nÞ; m om tD D h2 2 erE Siq mu SS ee tt ðð mm ii ÞÞ ð7Þ
Basedonabovecomputationformulas,thesimilarity-based
learning algorithm is shown below. where EquSet(m) and SimSet(m) represent the equivalent set
i i
and the similar set of a node m from schema tree T, respec-
i i
tively. Herein, SimDiðmD;m iÞ denotes the similarity degree
between node mD and node m i. In other words, if node mD is
equivalent to a schema tree node m, we can obtain SD by
i
directly calculating combination similarity degree of node m
i
and node n. Otherwise, when node mD is similar to a schema
tree node m, we can get SD by multiplying the similarity
i
degrees SimDiðmD;m iÞ and CSim iðm i;nÞ.
Based on the above computation formula, the dynamic
similarity-basedlearningalgorithmisshownbelow.Thefocus
of the improved algorithm is the dynamical update of the
dependencyprobabilitiesoftheCBNmodel.Inthisalgorithm,
ifanedgedoesnotbelongtotheintersectionofGc andGc,its
D
probability will be calculated based on the value of the simi-
larity degree SDD.
Giventhe training records in a category c and the node set
of constructed CBN, we can compute the conditional proba-
bility P(q j p, c) for each node q and its parent p in the CBN,
using this learning algorithm.
4.3. Similarity-based Bayesian learning algorithm in
dynamical environment
Consideringthedynamicnatureoflogdatafromrealworld
systems,weneedtoupdatetheexistingclassificationmodelto
get accurate diagnosis in dynamic environment. Actually, the
dependent information of existing CBN model may be inac-
curate or incomplete when training data are changing. It re-
quires dynamically updating the structure and dependency
probabilities of the CBN model. To obtain a more accurate
generation model, we dynamically update existing CBN
model based on incremental training data, by (1) adding new
nodes to current node set, (2) changing dependency between
nodes, and (3) calculating the dependency probability for the
updated dependency set.
Dependingonnewtrainingdata,weincrementallygenerate
theclassificationmodelCBNDfordifferentcategories.Then,for
aspecificcategoryc,wedynamicallyupdatetheprobabilitysetP
ofexistingCBN ,basedonthestructureGc andprobabilityset
0 D
Pc ofnewlyconstructedCBND.Inthissubsection,wepropose
D
the dynamic similarity-based learning algorithm, which can
updatetheexistingmodelandcalculateitscorrespondingprob-
abilitieswhennewtrainingdataarearriving.
In this algorithm, according to the obtained similarity, we
calculate the similarity degree SDD for different categories
between nodes of new model CBND and the existing model
CBN 0.Herein, SDDðmD;n;cÞdenotesthesimilaritydegreefor
categoryc between the node mD of new model CBND and the
node n of current model CBN . The corresponding formula is
0
shown as follows.
68 X.Hanetal./CAAITransactionsonIntelligenceTechnology1(2016)61e71
Using the proposed dynamic similarity-based learning al- 5.1. Experiment setup
gorithm,wecanupdatetheexistingCBNmodelandcalculate
its corresponding probabilities based on incremental training Thelogdatausedinourexperimentsarecollectedfromour
data in the dynamic environment. Web services platform which is supported by ActiveVOS en-
gine [27]. The log data are generated by the monitoring
4.4. Fault diagnosis by classifying log data with CBNs module of execution engine. The raw log data have different
levels including debug, information, warning, error, and fatal
By labeling the training log records from heterogeneous levels. In preprocessing step, we have implemented a moni-
sources with related categories, fault diagnosis can beviewed toring module to extract fault related log data, which ignores
asaclassificationproblemforlogrecordsobtainedinruntime. thelowlevelinformation(e.g.theinformationindebuglevel)
As mentioned above, we can achieve the goal of fault diag- of raw log data. To simulate the data obtained from hetero-