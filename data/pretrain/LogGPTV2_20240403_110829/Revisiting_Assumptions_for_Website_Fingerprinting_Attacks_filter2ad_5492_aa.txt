title:Revisiting Assumptions for Website Fingerprinting Attacks
author:Weiqi Cui and
Tao Chen and
Christian Fields and
Julianna Chen and
Anthony Sierra and
Eric Chan-Tin
Revisiting Assumptions for Website Fingerprinting Attacks
Weiqi Cui
Oklahoma State University
Julianna Chen
Oklahoma State University
Tao Chen
Oklahoma State University
Anthony Sierra
Oklahoma State University
Christian Fields
Oklahoma State University
Eric Chan-Tin
Loyola University Chicago
ABSTRACT
Most privacy-conscious users utilize HTTPS and an anonymity
network such as Tor to mask source and destination IP addresses.
It has been shown that encrypted and anonymized network traffic
traces can still leak information through a type of attack called
a website fingerprinting (WF) attack. The adversary records the
network traffic and is only able to observe the number of incoming
and outgoing messages, the size of each message, and the time
difference between messages. In previous work, the effectiveness of
website fingerprinting has been shown to have an accuracy of over
90% when using Tor as the anonymity network. Thus, an Internet
Service Provider can successfully identify the websites its users
are visiting. One main concern about website fingerprinting is its
practicality.
The common assumption in most previous work is that a victim
is visiting one website at a time and has access to the complete
network trace of that website. However, this is not realistic. We
propose two new algorithms to deal with situations when the vic-
tim visits one website after another (continuous visits) and visits
another website in the middle of visiting one website (overlapping
visits). We show that our algorithm gives an accuracy of 80% (com-
pared to 63% in a previous work [24]) in finding the split point
which is the start point for the second website in a trace. Using our
proposed “splitting” algorithm, websites can be predicted with an
accuracy of 70%. When two website visits are overlapping, the web-
site fingerprinting accuracy falls dramatically. Using our proposed
“sectioning” algorithm, the accuracy for predicting the website in
overlapping visits improves from 22.80% to 70%. When part of the
network trace is missing (either the beginning or the end), the ac-
curacy when using our sectioning algorithm increases from 20% to
over 60%.
CCS CONCEPTS
• Security and privacy → Pseudonymity, anonymity and un-
traceability; • Networks → Network privacy and anonymity.
KEYWORDS
Privacy, Website Fingerprinting, Anonymity
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ASIACCS’19 , July, Auckland, New Zealand
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6752-3/19/07...$15.00
https://doi.org/10.1145/3321705.3329802
ACM Reference Format:
Weiqi Cui, Tao Chen, Christian Fields, Julianna Chen, Anthony Sierra,
and Eric Chan-Tin. 2019. Revisiting Assumptions for Website Fingerprinting
Attacks. In ACM Asia Conference on Computer and Communications Security
(AsiaCCS ’19), July 9–12, 2019, Auckland, New Zealand. ACM, New York, NY,
USA, 12 pages. https://doi.org/10.1145/3321705.3329802
1 INTRODUCTION
Anonymous communication’s goal is to hide the relationship and
communication contents among different parties. Once two par-
ties establish an anonymous communication between them, the
contents are encrypted and routing information is hidden, thus
masking the source and destination IP addresses from third par-
ties. Tor [6, 21] is one of the most popular low-latency anonymity-
providing network. It is used by millions of people daily [18]. Tor
protects users’ privacy through a telescoping three-hop circuit and
encrypts the network traffic using onion routing. Although Tor
and many other privacy-enhancing technologies such as HTTPS
proxy hide the communication contents and network layer con-
tents, the network traffic itself may leak information such as packet
size, inter-packet timing information, and direction of the packets
(from server to client or other way around).
A website fingerprinting (WF) attack is one where an attacker
identifies a user’s web browsing information by merely observing
that user’s network traffic. The attacker is not attempting to break
the encryption algorithm or the anonymity protocol. The only
information available to the attacker is the metadata information
such as packet size, the timing information between packets, and
the direction of the packet. The success of this attack is measured by
the number of websites correctly identified. The accuracy has been
shown to be around 90%[16], thus violating any privacy offered by
HTTPS and anonymity services like Tor.
It has been more than 15 years since the first website fingerprint-
ing attack was proposed [11]. A number of studies on this topic
have been released since then [3, 16, 24], showing high accuracy
in predicting websites in both the open and closed world models.
Most previous work rely on certain assumptions. The goal of this
research is to revisit some of these assumptions, namely: 1) the
adversary can record the whole network traffic trace for a web-
site 1, 2) the victim visits one website at a time; here, we consider
two cases where i) the victim visits two pages one after the other
(continuous visits) and ii) the victim visits a second page before the
first one finishes loading (overlapping visits). We propose two new
algorithms to deal with these cases. The contributions of this paper
are summarized as follows.
• A “splitting” algorithm to identify two continuous net-
work traces. We propose a new algorithm based on Hidden
1Note that we used trace, network trace, website, and webpage interchangeably
Session 4B: PrivacyAsiaCCS ’19, July 9–12, 2019, Auckland, New Zealand328ASIACCS’19 , July, Auckland, New Zealand
W. Cui et al.
Markov Model to split and detect traces with two continuous
pages. We show that our algorithm gives a higher accuracy
in finding the split point in two continuous websites (80%
compared to 63% in previous work). This is also the first
time that the accuracy in directly predicting websites in
continuous traffic traces is tested and shown.
• A “sectioning” algorithm to identify overlapping net-
work traces. We propose a new algorithm to section the
trace into multiple sections and treat each section indepen-
dently to perform the website prediction. The hypothesis is
that if two traces overlap, the beginning of the first trace and
the end of the second trace would be unaffected. Sectioning
then still allows for correct identification of the two web-
sites. When considering overlapping traces, the accuracy of
current techniques for website fingerprinting decreases to
20% − 30%. Our sectioning algorithm improves the accuracy
to around 70%.
• Applying “sectioning” algorithm on partial traces. By
applying “sectioning” on partial traces, the accuracy (62.66%)
is higher compared to previous methods (20.76%) on predict-
ing websites with the beginning 5% of the trace missing.
When predicting websites with the last parts of the trace
missing, the accuracy is comparable. Hence, with sectioning
algorithm, we can reduce the impact of missing packets in a
network trace.
This paper is structured as follows: in Section 2, we give the
related background and terminology of this paper. In Section 3,
we propose a new “splitting” algorithm to find the split point in
two continuous page traces and present the results. We propose a
new “sectioning” algorithm to improve the accuracy in overlapping
traces in Section 4 and in partial traces in Section 5. We conclude
and provide avenues for future work in Section 6.
2 BACKGROUND
• Definitions. We first define some terms used.
– Trace. A trace is a time series of recorded network packets
for a visit to a webpage. Usually, tcpdump is used to record
the network traffic. A trace contains no background noise,
only the network traffic to/from that webpage.
– Continuous Trace. When a trace consists of two pages,
and the second page starts when the first page ends, we
call it a continuous trace. It has the same meaning as when
the two pages are separated with zero-time.
– Split Point. When a trace is composed of two pages, the
first step is to separate them before further detecting. The
point where the second page starts and the first page ends
is the split point.
– Overlapping Trace. When a trace consists of two pages,
and the second page starts before the first page ends, we
call it an overlapping trace. It has the same meaning as
when the two pages are separated with negative-time.
• Threat Model. In website fingerprinting attacks, the adver-
sary records network traffic data of his own visits to a list of
websites first through the Tor network. Then the adversary
can eavesdrop on the link between the victim and the entry
node. Figure 1 depicts where the adversary is. We assume
the attacker to be a passive observer which means it does
not modify transmissions and is not able to decrypt packets.
An example of the adversary is Internet Service Providers
(ISP), and state-level agencies.
Figure 1: Threat Model.
• WF Attack Procedures. Website fingerprinting has been
shown to be a serious threat against privacy mechanisms
for anonymous web browsing. Researchers have proposed
different scenarios for website fingerprinting. The attack
and resulting experiment vary from each other; however,
they all follow similar steps. A website fingerprinting attack
and analysis can be divided into six steps: 1) collect data, 2)
extract features from data, 3) select algorithm, 4) build model
based on 1) to 3), 5) evaluate real network traffic trace, and 6)
evaluate results. Figure 2 shows an illustration of all the steps
of a website fingerprinting attack. The last right-most block
contains the measurements to evaluate the effectiveness of
an attack.
When setting up an experiment for a website fingerprinting
attack, the first step is to perform data collection. A net-
work traffic recording tool such as wireshark or tcpdump
is used. Before running any scripts to automatically collect
data, the configuration of the browser should be set to match
the assumptions, such as disabling all plug-ins to avoid back-
ground noise and clearing the browser cache. The automated
script will then visit websites in a certain order. The time
taken to collect data depends on the number of instances
recorded for each website and the size of the website list.
Features extracted from the recorded network traffic traces
will be used for training. Each network trace is composed
of a list of features. The features can be treated as attributes
in a machine learning context. A classification algorithm is
applied to these features to build the attack model. Different
websites correspond to different classes. Different network
traffic traces are then collected to evaluate the performance
of the model. A 10-fold cross validation is often employed to
reduce the bias in the evaluation process.
• Closed world and Open World. The WF attack experi-
ments can be built based on two different scenarios: closed
world and open world. The closed world model is used when
complete information is available. The assumption in a closed
world model is that an attacker knows the metadata infor-
mation for a list of websites. The website visited by a victim
is in the list known by the attacker. It is a strong assumption
which is used to simplify the threat model, implementation
of the experiment, and evaluation of the success of the at-
tack. Since the closed world scenario is the more basic model,
most research work [1, 3–5, 10–13, 17, 19, 20, 23] include an
analysis of results in this closed world model.
Session 4B: PrivacyAsiaCCS ’19, July 9–12, 2019, Auckland, New Zealand329Revisiting Assumptions for Website Fingerprinting Attacks
ASIACCS’19 , July, Auckland, New Zealand
Figure 2: Steps of launching and evaluating a website fingerprinting attack.
In an open world model, a website being fingerprinted can
be either from the list or not in the list. The attacker keeps
track of a small list of monitored websites. Once a website
fingerprint is obtained, the attacker attempts to determine
if that website is part of the list of monitored websites or
not. More recent research work [4, 5, 8, 12, 14–17, 22–24]
deployed their website fingerprinting experiments under the
open world model and identified whether a website is from
the list of monitored sites.
• Dataset. Based on the foreground dataset of RND-WWW
from [16], our experiments in Section 3 , Section 4, and
Section 5 randomly pick 100 website records which contain
40 instances for each website from the original dataset. Each
instance is a trace containing the timestamped incoming and
outgoing packets’ size in chronological sequence. Incoming
packets are marked with a positive sign, while outgoing
packets are marked with a negative sign.
• Hidden Markov Model. The Hidden Markov Model (HMM)
is a Markov process with unobserved states. It is a statisti-
cal tool to model sequences that can be characterized by a
process from a generated observable sequence [2]. Based on
some training data, the HMM generates the probabilities of
the states in the dataset. The parameters of a HMM are of two
types: transition probabilities and emission probabilities. The
transition probability indicates the probability that a state
changes to another state and the emission probability is the
probability of an observation within a state. The transition
matrix and emission matrix store the transition probability
and emission probability of each state respectively.
• Classification of single-page and two-page traces. An
approach was developed to distinguish traces between one-
page trace and two-page traces in [24]. The authors em-
ployed k-NN binary classification and trained on two classes:
a class of two-page traces (a network trace consisting of two
webpages), and a class of single-page traces (a network trace
consisting of only one webpage). The classification accuracy
is 97%. Based on their results, we assume it is capable to
identify a trace with single page or two pages.
• Related work. The practicality of the WF attack has been
discussed previously. A critical evaluation of WF attacks [12]
pointed out the common assumptions of previous work and
limitation of datasets and single page visits. The CUMUL
algorithm [16] was developed with an Internet scale dataset
and achieved an accuracy of more than 90%. [7] investi-
gated the assumption of the victim and attacker visiting the
webpages under the same conditions such as browsers and