### 优化后的文本

#### 图6. 不同部署（VSCALE）下2,000 req/s的吞吐量 (a)-(d) 和延迟 (e)-(h)
- **图6(a)-(d)**：展示了不同ONOS控制器数量下的吞吐量。具体包括：
  - (a) 1个ONOS控制器
  - (b) 3个ONOS控制器
  - (c) 5个ONOS控制器
  - (d) 7个ONOS控制器

- **图6(e)-(h)**：展示了不同部署下的服务执行延迟。具体包括：
  - (e) 单个VM集群
  - (f) 3个VM集群
  - (g) 5个VM集群
  - (h) 7个VM集群

#### 请求处理机制
安装/撤回请求采用同步方法，请求者（例如NFV应用）会等待直到请求被接受。我们发现这种机制限制了请求的接受速率，无论VM规格或集群大小如何。

#### 延迟分析
当从单个VM集群扩展到多个VM集群时，请求执行延迟增加了10倍。图6(e)-(h)显示了服务执行延迟的测量结果。
- **单个VM集群**（图6(e)）：LARGE和EXTRA-LARGE VM的服务安装延迟分别为约100 ms和400 ms，而MEDIUM和SMALL VM分别测量为1,000 ms和7,000 ms。
- **3个VM集群**（图6(f)）：LARGE和EXTRA-LARGE部署的延迟增加到约1,800 ms，与MEDIUM部署的延迟重叠。SMALL部署的延迟从7,000 ms改善到2,500 ms。
- **5个VM集群**（图6(g)）：SMALL部署的延迟继续改善到2,000 ms。
- **7个VM集群**（图6(h)）：SMALL部署的延迟进一步改善到1,700 ms。其他部署在5-VM集群中大多重叠在1,200 ms到1,600 ms之间，而在7-VM集群中收敛到约1,000 ms。

#### 性能引起的故障示例
图7显示了在实验期间由数据收集器收集的小型（图7(a)）和大型（图7(b)）部署的CPU和Java虚拟机（JVM）堆内存统计信息。尽管意图的安装和撤回保持几乎恒定的活跃意图数量，但两种部署的堆利用率都在持续增加。此外，对于小型部署，CPU饱和表明VM规格不足以承受请求，无论集群配置如何。

我们查看了ONOS VM生成的日志（如图8所示），发现高负载导致了一系列连锁反应，最终导致系统完全中断 [16], [17]。

#### 讨论
本节中的性能结果清楚地表明，在不同工作条件下测试控制平面的必要性，特别是在高负载条件下测试特定功能（如错误检测和故障转移的有效性）。在现场，由于控制平面在极端负载下工作而导致的故障是常见的 [18]。对于EXTRA-LARGE部署，很明显，随着集群中控制器数量的增加，每个ONOS VM的CPU负载减少。此外，堆内存分配的斜率也随集群大小而变化。例如，SMALL部署的斜率为每分钟约20%的堆内存占用，而EXTRA-LARGE部署的斜率为每分钟5%-10%。

#### 控制平面容量测试
本节展示了如何使用SCP-CLUB来识别控制平面的可扩展性和容量限制。通过这些分析，可以确定VM/集群应扩展和重新配置的极限。

##### 控制平面扩展点
我们将控制平面容量限制定义为提交请求的最大速率，此时观察到的执行请求速率相同。SCP-CLUB估计控制平面扩展点为提交请求的最大速率，使得η = T_SR / SR > 80%，其中T_SR是已服务请求的吞吐量，SR是提交请求的速率。η值用于衡量控制平面在稳定服务提供请求时所能承受的最大工作负载。

表IV总结了所考虑部署的扩展点，η = 80%。表IV还包括两个效率测量指标：i) δ = 扩展点 / 核心数，用于衡量VM在扩展点时每核心处理的请求数；ii) 集群开销，计算为1 - δ当前集群配置 / δ无集群。后者给出了由于集群实例之间的同步而产生的额外负载估计。

图9(a)-(d)显示了SCP-CLUB进行的吞吐量分析结果，以识别最大控制平面容量和扩展点。值得注意的是：
- 从小型（2核，2 GB RAM）扩展到中型（4核，4 GB RAM）部署，吞吐量增加了2.2到3.1倍。这是最有效的扩展点，吞吐量增加大于资源增加。中型部署在任何集群规模下都显示出最佳的成本效益比。
- 从大型（8核，8 GB RAM）扩展到超大型（16核，16 GB RAM），吞吐量仅边际增加（1.7到1.8倍）。
- 对于大型和超大型VM，扩展ONOS并不总是有利的。例如，如图9(c)和(d)所示，将超大型VM从5个控制器扩展到7个控制器并未改善性能，最大吞吐量5,000 req/s仅在5节点集群中实现。
- 聚类添加的开销从54%（3节点集群的小型VM）到79%（7节点集群的超大型VM），即要实现与单个超大型VM部署相同的性能，我们需要扩展到5个超大型VM。

#### 讨论
ONOS通过Atomix/Copycat提供了集群实例之间的状态一致性，这是一种基于Raft共识算法的容错状态机复制框架 [19], [20]。交换机-控制器映射、集群主控权和网络主机在集群中具有强一致性。意图相关数据和流规则使用最终一致性共享，更新首先在意图的主实例上本地完成，然后通过 gossip 协议传播到集群中的其他实例。作为概念验证，我们对修改后的ONOS版本进行了测试，放宽了最终一致性策略。我们将实例间同步周期从5秒改为15秒，实现了7节点超大型部署的集群开销为53%。虽然聚类有助于提高可用性，但我们认为应细调集群一致性特性（例如，使用[21]中的方法）以保持低开销。

#### 提高ONOS吞吐量
通过上述分析，我们识别出了一些潜在的改进措施。如第VI-B节所述，一个主要限制是NBI处理应用程序请求时使用的同步方法。我们修改了部分ONOS REST接口，实现了异步方法。在我们的补丁中，暴露NBI的REST接口实现了一个弹性线程池。每个请求分配给一个专用的工作线程，该线程在意图处理终止后使用回调回复负载生成器。这允许检查请求的最终状态，并提高了整体性能。