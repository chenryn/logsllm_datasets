	
	
ExtraLarge
VSCALE
Large Medium Small
50
100
150
200
250
300
3
2
1
/
]
s
q
e
r
K
[
t
u
p
h
g
u
o
r
h
T
0
0
Time [s]
(c) 
	
	
ExtraLarge
VSCALE
Large Medium Small
	
	
ExtraLarge
VSCALE
Large Medium Small
50
100
150
200
250
300
Time [s]
(d)
	
	
ExtraLarge
VSCALE
Large Medium Small
50
100
150
200
250
300
Time [s]
(h)
]
s
m
[
y
c
n
e
t
a
L
106
105
104
103
102
101
0
50
100
150
200
250
300
101
0
50
100
150
200
250
300
Time [s]
(f)
Time [s]
(g) 
Fig. 6. Throughput (a)-(d) and Latency (e)-(h) for 2,000 req/s over different deployments (VSCALE), for (a) 1, (b) 3, (c) 5, (d) 7 ONOS controllers.
installation/withdrawal requests uses a synchronous approach
with the requestor (e.g., the NFV application) waiting until the
request is accepted. We found this mechanism to be a limitation
in the rate of requests accepted regardless of the VM Ô¨Çavor or
cluster size.
Request execution latency increases 10 times when scaling
from single VM cluster to > 1 VM cluster. Figures 6.(e)-(h)
show the measurements for the service execution latency. For
a single VM cluster (Figure 6.(e)) the LARGE and EXTRA-
LARGE VMs show a service installation latency of about 100
ms and 400 ms, respectively, while the MEDIUM and SMALL
VMs measured 1,000 ms and 7,000 ms, respectively. When
increasing the cluster size to 3 VMs (Figure 6.(f), the LARGE
and EXTRA-LARGE deployment latency jumps up to about
1,800 ms, overlapping with the MEDIUM deployment latency.
The SMALLER deployment showed an improvement from
7,000 ms to 2,500 ms. The SMALL deployment continues to
improve to 2,000 ms and 1,700 ms for cluster of 5 and 7 VMs,
respectively (Figure 6.(g) and (h)). The other deployments
mostly overlap in the range of 1,200 ms to 1,600 ms for a
5-VM cluster size, while they converge on about 1,000 ms for
the 7-VM cluster.
Example of Performance-Induced Failure ‚Äì Figure 7 shows
the CPU and Java Virtual Machine (JVM) heap memory statis-
tics collected by the Data Collector during the experiments for
the SMALL (Figure 7.(a)) and EXTRA-LARGE (Figure 7.(b))
deployments. It is interesting to note that for both deployments
there is a continuous increase in the heap utilization despite the
fact that intents are installed and withdrawn keeping an almost
constant number of active intents at anytime. In addition, for the
small deployment the saturation of the CPU shows clearly that
the VM specs are not enough to withstand the requests, for
any cluster conÔ¨Åguration. We looked into the logs generated
by the ONOS VM (snippet showed in Figure 8), and we found
that the high level of load caused a series of domino effects
that resulted in additional load for the VM and eventually led
to a total system outage [16], [17]. A snippet of the logs is
















































	






















	



































	
(a) 



















	
(b)
Fig. 7. CPU and JVM heap memory utilization for SMALL deployment (a),
and EXTRA-LARGE deployment (b).
showed in Figure 8. It is evident that the problem can be
attributed to the series of time-out based error detectors used
to detect controller failures. In this speciÔ¨Åc case, due to the
high workload conditions, one of the switches assumed that
its master failed (line 5-6). In addition, this caused additional
load in the system due to the polling of the active ONOS VM
in the cluster (line 9) controller, and a Ô¨Çapping behavior of
one of the nodes alternating between the UNAVAILABLE, to
AVAILABLE to UNAVAILABLE state. This translates in a loss
of connectivity between control plane and data plane, and in
data plane unavailability.
Discussion. The performance results in this section clearly
demonstrate the need for testing the control plane under differ-
ent working conditions, and, more importantly, to test for spe-
ciÔ¨Åc features (e.g., the error detection and failover effectiveness)
while under high workload conditions. It is common to have
failures in the Ô¨Åeld caused by a control plane working under
extreme workload due to other failures [18]. For the EXTRA-
LARGE deployment, it is evident that the CPU load per ONOS
VM decreases when increasing the number of controllers in
the cluster. It is also interesting to note that the slope of the
heap memory allocation changes depending on the size of the
649
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:48:08 UTC from IEEE Xplore.  Restrictions apply. 
Fig. 8. Snippet of the Logs from the Failed ONOS VM in Figure 7.(a).
1: Unable to compile intent HostToHostIntent ...
2: Dropping messages for switch SWITCH 1 ... channel not connected ...
3: Failed to backup devices: [SWITCH 1]. Reason: TimeoutException: ...
4: Timeout waiting for reply, Node: ONOS VM 4,copycat-server-ONOS VM 0...
5: Connection to ONOS VM 3:9876 failed! Reason: TimeoutException: ...
6: Connection to ONOS VM 2:9876 failed! Reason: TimeoutException: ...
7: WARN,Unhandled REST exception,WARN,ONOS VM 0...
8: ERROR,Error submitting batches:,ERROR,ONOS VM 0...
9: INFO,‚ÄùONOS VM 0:9876 - Polling members
10: Member[type=ACTIVE, status=UNAVAILABLE, serverAddress=ONOS VM 1...
11: Member[type=ACTIVE, status=AVAILABLE, serverAddress=ONOS VM 2...
12: INFO,‚Äù/ONOS VM 0:9876 - Polling members
13: Member[type=ACTIVE, status=AVAILABLE, serverAddress=ONOS VM 3...
14: Member[type=ACTIVE, status=AVAILABLE, serverAddress=ONOS VM 4...
15: INFO,‚ÄùONOS VM 0:9876 - Polling members
16: Member[type=ACTIVE, status=UNAVAILABLE, serverAddress=ONOS VM 1...
17: Member[type=ACTIVE, status=AVAILABLE, serverAddress=ONOS VM 4...
18: ...
19: Unable to compile intent HostToHostIntent...
cluster. For instance, the slope is of about 20% heap memory
occupation per minute for the SMALL deployments, against
a 5%-10% utilization per minute for the EXTRA-LARGE
deployments. Due to the intent load balancing implemented in
the Load Generator, larger clusters have smaller heap memory
footprint due to the lower number of intents to manage locally.
VII. TESTING CONTROL PLANE CAPACITY
In this section, we show how SCP-CLUB can be used
to identify the scalability and capacity limits of the control
plane. Results of this analysis, especially when performed in-
production, can be leveraged to identify the limits within which
the VMs/cluster should be scaled up and re-provisioned.
A. Control Plane Scaling Points
We deÔ¨Åne the control plane capacity limit as the maximum
rate of submitted requests for which we can observe the same
rate of the executed requests. SCP-CLUB estimates the control
plane scaling point as the maximum rate of requests for which
T SR
the ratio Œ∑ =
SR is greater than 80%, with TSR the
throughput of the served requests, and SR the rate of submitted
requests. The rate Œ∑ measures the maximum offered workload at
which the control plane can stably serve the provided requests.
Table IV summarizes the scaling point obtained for the
considered deployments, for Œ∑ = 80% for the considered
deployments. The values of eta are interesting to identify
convenient scaling point for the deployments. Table IV includes
also two measurements of efÔ¨Åciency: i) Œ¥ = Scaling Point
to mea-
sure the efÔ¨Åciency of the VM in terms of request/s processed per
core at the scaling point, and ii) the cluster overhead computed
as 1 ‚àí Œ¥current cluster conÔ¨Åguration
. The latter gives an estimation of the
additional load over the single VM due to the synchronization
between the cluster instances.
Figures 9.(a)-(d) show the results of the throughput analysis
performed by SCP-CLUB to identify the maximum control
plane capacity and scalability points for a topology of 10
switches. It is worth noting that:
Œ¥no cluster
#vCores
‚Ä¢ Scaling from a SMALL (2 cores, 2 GB RAM)
to
MEDIUM (4 cores, 4 GB RAM) deployment increases
the throughput of 2.2 - 3.1 times. This shows to be the
most efÔ¨Åcient scaling point with an increase of throughput
‚â• than the increase in the VM resources. The MEDIUM
size deployment shows the best cost-beneÔ¨Åt trade-off for
any cluster size.
ONOS CONTROL PLANE SCALING POINTS AND CLUSTER OVERHEAD FOR A
TABLE IV
10 SWITCHES TOPOLOGY.
‚Ä¢ Scaling from LARGE (8 cores and 8 GB RAM) to
EXTRA-LARGE (16 cores and 16 GB RAM) increases
the throughput only marginally (1.7 to 1.8 times).
‚Ä¢ With the LARGE and EXTRA-LARGE VM size, scaling
out ONOS does not always pay off. For instance, as shown
in Ô¨Ågure Figure 9.(c) and (d), scaling out the EXTRA-
LARGE VM from 5 to 7 controllers does not improve the
performance if compared to the 5 controllers deployment,
with the maximum throughput of 5,000 requests/s achieved
only in the 5-nodes cluster.
‚Ä¢ The clustering adds an overhead that goes from 54%
(SMALL VMs in a 3-nodes cluster) to 79% (EXTRA-
LARGE VMs for a 7-nodes cluster), i.e., to achieve the
same performance as the 1 EXTRA-LARGE VM deploy-
ment with a cluster of ONOS controllers, we should scale
up to 5 EXTRA-LARGE VMs.
Discussion. ONOS provides state consistency among the
cluster instances by means of Atomix/Copycat, a fault-tolerant
state machine replication framework built on the Raft consensus
algorithm [19], [20]. Switch-controller mapping, cluster mas-
tership and network hosts are shared with strong consistency
in the cluster. Eventual consistency is used to share intent-
related data and Ô¨Çow rules, with updates Ô¨Årst done locally on
the master instance of the intent, and eventually propagated
the other instances in the cluster using a gossip protocol. As a
proof of concept, we repeated the testing for a modiÔ¨Åed version
of ONOS with a more relaxed eventual consistency policy.
We changed the period of instance-to-instance synchronization
from 5 s to 15 s, achieving a cluster overhead of 53% for the
EXTRA-LARGE deployment with 7 nodes. While clustering
helps in providing higher availability, we argue that the cluster
consistency features should be Ô¨Åne-tuned (e.g., using an ap-
proach as in [21]) at run time to keep the overhead low.
B. Leveraging Results to Improve ONOS Throughput
The performed analysis allowed to identify potential im-
provements to the ONOS performance. As discussed in Sec-
tion VI-B, one of the identiÔ¨Åed limitations is the use of a
synchronous approach in handling application requests coming
through the NBI. We modiÔ¨Åed part of the ONOS REST inter-
face handling the new requests, implementing an asynchronous
approach. In our patch, the REST interface exposing the NBI
implements an elastic pool of threads. Each request is assigned
to a dedicated worker thread, which replies to the load generator
using a callback when the intent processing terminates ‚Äì either
correctly or failing. This allows i) to check the Ô¨Ånal status of
650
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:48:08 UTC from IEEE Xplore.  Restrictions apply. 
























	







  

  

	




	

	




























	
	

