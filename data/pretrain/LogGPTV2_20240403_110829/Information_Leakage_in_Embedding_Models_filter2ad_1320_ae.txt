r
e
v
d
A
0.3
0.2
0.1
10
20
30
Ns
40
50
100
200
400
|S|
800
10
20
30
40
50
60
70
80
90
Inverse frequency percentile
Figure 3: Performance of sensitive attribute (author) infer-
ence with different models. For the left figure, the x-axis is
the number of labeled data per author Ns and the y-axis is
the top-5 accuracy of classifying 100 authors. For the right
figure, the x-axis is the number of author classes |S| and the
y-axis is the top-5 accuracy with 50 labeled data per author.
embeddings from dual-encoder models trained with contrastive
learning framework [63] aid attribute inference attacks the most
comparing to other pre-trained embeddings.
6.4 Membership Inference
Evaluation metrics. We consider membership inference as a bi-
nary classification task of distinguishing members and non-members
of training data. We evaluate the performance of membership in-
ference attacks with adversarial advantage [77], defined as the
difference between the true and false positive rate. Random guesses
offer an advantage of 0.
Word embedding setup. We evaluate membership inference at-
tacks on sliding window of 5 words from Wikipedia articles. We
also perform the attack separately for windows with central words
having different frequencies, following the intuition that rare words
are prone to more memorization [67]. Specifically, we evaluate the
attack for windows with frequency of central words in decile (10th,
... , 90th percentile) ranges. We use cosine similarity for π.
Word embedding results. Figure 4 demonstrates the results. For
frequent (10th percentile) central words, there is almost no mem-
orization (advantage < 0.1). As this frequency decreases, the ad-
vantage increases correspondingly to roughly 0.3. FastText is most
resistant to these attacks possibly due to its operating on sub-word
units rather than exact words.
Sentence embedding setup. We evaluate membership inference
attacks on context- and aggregate-level data from the BookCorpus
dataset. We consider a pair of sentences for context-level data, and
a collection of sentences from the same book for aggregate-level
data with a goal of inferring if the book is part of the training
corpus. As with word embeddings, we evaluate the attack on dif-
ferent frequencies (averaged across words in a sentence). We use
dot-product similarity for π, and for learning-based similarity, we
learn a projection matrix with 10% of training and hold-out data.
Figure 4: Performance of membership inference attack on
Word2Vec, FastText and GloVe. The x-axis is the inverse fre-
quency percentile range (the smaller the more frequent) and
the y-axis is the adversarial advantage.
Transformer
LSTM
ctx
ctx′
book
book′
0.3
0.2
0.1
60
50
90
Inverse frequency percentile
70
80
60
50
90
Inverse frequency percentile
70
80
e
g
a
t
n
a
v
d
a
l
a
i
r
a
s
r
e
v
d
A
0.3
0.2
0.1
0
Figure 5: Performance of context-level (ctx) and book-
level membership inference attack on sentence embedding
trained with LSTM and Transformer. The x-axis is the in-
verse frequency percentile range (the smaller the more fre-
quent) and the y-axis is the adversarial advantage. The ctx′
and book′ denote results with learned similarity
We optimize LMIA with the Adam optimizer for 10 epochs with
learning rate set to 0.001.
Sentence embedding results. Figure 5 shows the results of MIA
on embeddings from LSTM and Transformer dual-encoder models.
For both models, context-level MIA advantage scores are below 0.1
for all frequency ranges, indicating that adversarial does not gain
much information about context-level membership from the em-
beddings. Learning based similarity can improve context-level MIA
slightly. For aggregated book-level inference, adversaries achieve a
greater advantage than context-level inference and learning-based
similarity scores can boost the advantage to 0.3 for books with
infrequent sentences.
LSTM
Transformer
LSTM
Transformer
0.9
0.8
e
r
o
c
S
y
t
i
l
i
t
U
MPQA
TREC
SUBJ
MSRP
0.9
0.8
e
r
o
c
S
y
t
i
l
i
t
U
0.8
0.6
MPQA
TREC
SUBJ
MSRP
0.8
0.6
0.7
0
0.1
0.2
λw
LSTM
0.3
0.4
0.7
0
2.5 · 10−2 5 · 10−2 7.5 · 10−2
0.1
λw
Transformer
0.75
1
0
0
0.25
0.5
λs
LSTM
0.4
0.5
0.6
0.3
λs
Transformer
e
r
o
c
S
1
F
n
o
i
s
r
e
v
n
I
60
40
20
0
0
White-box
LMLC
LMSP
0.1
0.2
λw
40
20
0.3
0.4
0
2.5 · 10−2 5 · 10−2 7.5 · 10−2
0.1
λw
y
c
a
r
u
c
c
A
5
-
p
o
T
60
40
20
0
0
60
40
20
0
Ns = 50
Ns = 10
0.5
λs
0.25
0.75
1
0.4
0.5
0.6
0.3
λs
Figure 6: Effects of adversarial training against embedding
inversion on the utility (top row) and the inversion F1 score
(bottom row) for sentence embeddings trained with LSTM
and Transformer.
Figure 7: Effects of adversarial training against sensitive at-
tribute inference on the utility (top row) and the author clas-
sification top-5 accuracy (bottom row) for sentence embed-
dings trained with LSTM and Transformer. Ns denotes num-
ber of labeled data per author.
7 DEFENSES
Adversarial training. Attacks involving embedding inversions
and sensitive attributes are both inference-time attacks that wish to
infer information about the sensitive inputs given the output of the
embedding. A common defence mechanism for such inference-time
attacks is adversarial training [9, 17, 18, 38, 74]. In this framework, a
simulated adversary A is trained to infer any sensitive information
jointly with a main model Φ while Φ is trained to maximize the
adversary’s loss and minimize the primary learning objective. The
embeddings trained with this minimax optimization protects sen-
sitive information from an inference-time adversary to an extent
while maintaining their utility for downstream tasks.
To defend against embedding inversion attacks, A is trained to
predict the words in x given Φ(x). For a pair of sentences in context
(xa, xb) and a set of negative example Xneg, the training objective
for Φ is:
min
Φ
A λw log PA(W(xb)|Φ(xb)) − log PΦ(xb|xa,Xneg),
max
where W(x) is the set of words in x and the coefficient λw controls
the balance between the two terms. A natural choice for log PA
is to use the multi-label classification loss LMLC in Equation 8. To
defend against sensitive attribute attacks, A is trained to predict
the sensitive attribute s in x from the embedding Φ(x). As above,
min
Φ
A λs log PA(s|Φ(xb)) − log PΦ(xb|xa,Xneg),
max
the training objective for Φ is:
where the coefficient λs controls the balance between the two terms.
Both minimax training objectives can be efficiently optimized with
the gradient reversal trick [20].
Results with adversarial training. We evaluate this adversarial
training approach on dual-encoder models with LSTM and Trans-
former. We keep all the training hyper-parameters the same as in
Section 6.1. We train multiple models under different λw and λs
and evaluate their effects on attack performance as well as utility