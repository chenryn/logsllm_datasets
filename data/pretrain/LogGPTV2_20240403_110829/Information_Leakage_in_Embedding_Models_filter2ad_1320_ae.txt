### 4. Sensitive Attribute Inference

**Figure 3: Performance of Sensitive Attribute (Author) Inference with Different Models**

- **Left Figure**: The x-axis represents the number of labeled data per author (\(N_s\)), and the y-axis shows the top-5 accuracy for classifying 100 authors.
- **Right Figure**: The x-axis represents the number of author classes (\(|S|\)), and the y-axis shows the top-5 accuracy with 50 labeled data per author.

**Key Findings**:
- Embeddings from dual-encoder models trained with a contrastive learning framework [63] are the most effective in aiding attribute inference attacks compared to other pre-trained embeddings.

### 6.4 Membership Inference

**Evaluation Metrics**:
- We consider membership inference as a binary classification task to distinguish between members and non-members of the training data.
- The performance is evaluated using adversarial advantage [77], defined as the difference between the true positive rate and the false positive rate. Random guessing results in an advantage of 0.

**Word Embedding Setup**:
- We evaluate membership inference attacks on sliding windows of 5 words from Wikipedia articles.
- Attacks are performed separately for windows with central words of different frequencies, based on the intuition that rare words are more prone to memorization [67].
- Specifically, we evaluate the attack for windows with central word frequencies in decile ranges (10th, 20th, ..., 90th percentile).
- Cosine similarity is used for \(\pi\).

**Word Embedding Results**:
- For frequent (10th percentile) central words, there is almost no memorization (advantage < 0.1).
- As the frequency decreases, the advantage increases to roughly 0.3.
- FastText is the most resistant to these attacks, possibly due to its sub-word unit processing rather than exact words.

**Sentence Embedding Setup**:
- We evaluate membership inference attacks on context- and aggregate-level data from the BookCorpus dataset.
- Context-level data consists of pairs of sentences, while aggregate-level data consists of collections of sentences from the same book, aiming to infer if the book is part of the training corpus.
- Similar to word embeddings, we evaluate the attack on different frequencies (averaged across words in a sentence).
- Dot-product similarity is used for \(\pi\), and for learning-based similarity, a projection matrix is learned using 10% of the training and hold-out data.

**Sentence Embedding Results**:
- **Figure 4**: Performance of membership inference attacks on Word2Vec, FastText, and GloVe. The x-axis is the inverse frequency percentile range (smaller values indicate more frequent words), and the y-axis is the adversarial advantage.
- **Figure 5**: Performance of context-level (ctx) and book-level membership inference attacks on sentence embeddings trained with LSTM and Transformer. The x-axis is the inverse frequency percentile range, and the y-axis is the adversarial advantage. `ctx'` and `book'` denote results with learned similarity.

**Optimization**:
- LMIA is optimized with the Adam optimizer for 10 epochs with a learning rate of 0.001.

**Results**:
- For both LSTM and Transformer models, context-level MIA advantage scores are below 0.1 for all frequency ranges, indicating that adversaries do not gain much information about context-level membership from the embeddings.
- Learning-based similarity can slightly improve context-level MIA.
- For aggregated book-level inference, adversaries achieve a greater advantage, and learning-based similarity scores can boost the advantage to 0.3 for books with infrequent sentences.

### 7. Defenses

**Adversarial Training**:
- Adversarial training is a common defense mechanism against inference-time attacks that aim to infer sensitive information from the output of the embedding.
- In this framework, a simulated adversary \(A\) is trained to infer any sensitive information jointly with the main model \(\Phi\). \(\Phi\) is trained to maximize the adversaryâ€™s loss and minimize the primary learning objective.
- This minimax optimization helps protect sensitive information while maintaining utility for downstream tasks.

**Defending Against Embedding Inversion Attacks**:
- The adversary \(A\) is trained to predict the words in \(x\) given \(\Phi(x)\).
- For a pair of sentences in context \((x_a, x_b)\) and a set of negative examples \(X_{neg}\), the training objective for \(\Phi\) is:
  \[
  \min_{\Phi} \max_{A} \lambda_w \log P_A(W(x_b) | \Phi(x_b)) - \log P_{\Phi}(x_b | x_a, X_{neg})
  \]
  where \(W(x)\) is the set of words in \(x\) and \(\lambda_w\) controls the balance between the two terms.

**Defending Against Sensitive Attribute Attacks**:
- The adversary \(A\) is trained to predict the sensitive attribute \(s\) in \(x\) from the embedding \(\Phi(x)\).
- The training objective for \(\Phi\) is:
  \[
  \min_{\Phi} \max_{A} \lambda_s \log P_A(s | \Phi(x_b)) - \log P_{\Phi}(x_b | x_a, X_{neg})
  \]
  where \(\lambda_s\) controls the balance between the two terms.

**Results with Adversarial Training**:
- We evaluate this adversarial training approach on dual-encoder models with LSTM and Transformer.
- Multiple models are trained under different \(\lambda_w\) and \(\lambda_s\) values, and their effects on attack performance and utility are evaluated.

**Figures**:
- **Figure 6**: Effects of adversarial training against embedding inversion on the utility (top row) and the inversion F1 score (bottom row) for sentence embeddings trained with LSTM and Transformer.
- **Figure 7**: Effects of adversarial training against sensitive attribute inference on the utility (top row) and the author classification top-5 accuracy (bottom row) for sentence embeddings trained with LSTM and Transformer. \(N_s\) denotes the number of labeled data per author.