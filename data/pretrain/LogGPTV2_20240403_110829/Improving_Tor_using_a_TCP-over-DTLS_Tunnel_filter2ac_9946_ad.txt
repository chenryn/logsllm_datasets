1053 ± 409 ms
Effective
Latency
Degradation Drop Rate
0 %
20 %
7 %
0 %
5.4 %
12.9 %
0 %
0.08 %
0.03 %
0 %
0.08 %
0.03 %
Table 6: Latency for different dropping conﬁgurations.
TCP congestion control mechanism has less impact on
throttling when each TCP stream is separated. One
stream may back off, but the others will continue send-
ing, which results in a greater throughput over the bot-
tleneck connection. This is reasonable behaviour since
TCP was designed for separate streams to function over
the same route. If congestion is a serious problem then
multiple streams will be forced to back off and ﬁnd the
appropriate congestion window. Importantly, the streams
that send a small amount of data are much less likely to
need to back off, so their small trafﬁc will not have to
compete unfairly for room inside a small congestion win-
dow intended to throttle a noisy connection. The beneﬁts
of this are clearly visible in the latency as well: cells
can travel through the network considerably faster in the
TCP-over-DTLS version of Tor. Despite the large conﬁ-
dence intervals for latency mentioned earlier, we see now
that TCP-over-DTLS consistently has signiﬁcantly lower
latency than the original TCP Tor.
The TCP-over-DTLS version has
its observed
throughput and latency affected proportionally to packet
drop rate.
It did not matter if the drop was happen-
ing on the shared link or the remaining link, since the
shared link is not a single TCP connection that multi-
plexes all trafﬁc. Missing cells for different circuits no
longer cause unnecessary waiting, and so the only effect
on latency and throughput is the effect of actually drop-
ping cells along circuits.
5 Alternative Approaches
There are other means to improve Tor’s observed latency
than the one presented in this paper. For comparison, in
this section we outline two signiﬁcant ones: UDP-OR,
and SCTP-over-DTLS.
5.1 UDP-OR
Another similar transport mechanism for Tor has been
proposed by Viecco [15] that encapsulates TCP pack-
ets from the OP and sends them over UDP until they
reach the exit node. Reliability guarantees and conges-
tion control are handled by the TCP stacks on the client
and the exit nodes, and the middle nodes only forward
trafﬁc. A key design difference between UDP-OR and
our proposal is that ours intended on providing back-
wards compatibility with the existing Tor network while
Viecco’s proposal requires a synchronized update of the
Tor software for all users. This update may be cumber-
some given that Tor has thousands of routers and an un-
known number of clients estimated in the hundreds of
thousands.
This strategy proposes beneﬁts in computational com-
plexity and network behaviour. Computationally, the
middle nodes must no longer perform unnecessary op-
erations: packet injection, stream read, stream write,
packet generation, and packet emission. It also removes
the responsibility of the middle node to handle retrans-
Experiment 3 Basic throughput and delay for TCP and
TCP-over-DTLS versions of Tor.
1: To compare TCP and TCP-over-DTLS we run the
experiment twice: one where all ORs use the original
TCP version of time, and one where they all use our
modiﬁed TCP-over-DTLS version of Tor.
2: A local Tor network running six routers on a local
host was conﬁgured to have a latency of 50 millisec-
onds.
3: Two OPs are conﬁgured to connect to our local Tor
network. They use distinct circuits, but each OR
along both circuit is the same. The latency-OP will
be used to measure the circuit’s latency by send-
ing periodic timestamp probes over Tor to a timing
server. The throughput-OP will be used to measure
the circuit’s throughput by requesting a large bulk
transfer and recording the rate at which it arrives.
4: We start the latency-OP’s timestamp probes and
measure the latency of the circuit. Since we have
not begun the throughput-OP, we record the time as
the base latency of the circuit.
5: We begin the throughput-OP’s bulk transfer and
measure throughput of the circuit. We continue to
measure latency using the latency-OP in the pres-
ence of other trafﬁc. The latency results that are col-
lected are recorded separately from those of step 4.
6: Data was collected for over a minute, and each con-
ﬁguration was run a half dozen times to obtain con-
ﬁdence intervals.
missions, which means a reduction in its memory re-
quirements. The initial endpoint of communication will
be responsible for retransmitting the message if neces-
sary. We have shown that computational latency is in-
signiﬁcant in Tor, so this is simply an incidental beneﬁt.
The tangible beneﬁt of UDP-OR is to improve the net-
work by allowing the ORs to function more exactly like
routers. When cells arrive out of order at the middle
node, they will be forwarded regardless, instead of wait-
ing in input buffers until the missing cell arrives. More-
over, by having the sender’s TCP stack view both hops
as a single network, we alleviate problems introduced by
disparity in network performance. Currently, congestion
control mechanisms are applied along each hop, mean-
ing that an OR in the middle of two connections with
different performance metrics will need to buffer data to
send over the slower connection. Tor provides its own
congestion control mechanism, but it does not have the
sophistication of TCP’s congestion control.
We require experimentation to determine if this pro-
posal is actually beneﬁcial. While it is clear that mem-
ory requirements for middle nodes are reduced [15],
the endpoints will see increased delay for acknowledge-
ments. We expect an equilibrium for total system mem-
ory requirements since data will be buffered for a longer
time. Worse, the approach shifts memory requirements
from being evenly distributed to occurring only on exit
nodes—and these nodes are already burdened with extra
responsibilities. Since a signiﬁcant fraction of Tor nodes
volunteer only to forward trafﬁc, it is reasonable to use
their memory to ease the burden of exit nodes.
Circuits with long delays will also suffer reduced
throughput, and so using congestion control on as short a
path as possible will optimize performance. If a packet is
dropped along the circuit, the endpoint must now gener-
ate the retransmission message, possibly duplicating pre-
vious routing efforts and wasting valuable volunteered
bandwidth. It may be more efﬁcient to have nodes along
a circuit return their CWND for the next hop, and have
each node use the minimum of their CWND and the next
hop’s CWND. Each node then optimizes their sending
while throttling their receiving.
5.1.1 Low-cost Privacy Attack
UDP-OR may introduce an attack that permits a hostile
entry node to determine the ﬁnal node in a circuit. Previ-
ously each OR could only compute TCP metrics for ORs
with whom they were directly communicating. Viecco’s
system would have the sender’s TCP stack communicate
indirectly with an anonymous OR. Connection attributes,
such as congestion and delay, are now known for the
longer connection between the ﬁrst and last nodes in a
circuit. The ﬁrst node can determine the RTT for trafﬁc
to the ﬁnal node. It can also reliably compute the RTT
for its connection to the middle node. The difference in
latency reﬂects the RTT between the second node and
the anonymous ﬁnal node. An adversary can use a sim-
ple technique to estimate the RTT between the second
node and every other UDP Tor node in the network [3],
possibly allowing them to eliminate many ORs from the
ﬁnal node’s anonymity set.
If it can reduce the set of
possible ﬁnal hops, other reconnaissance techniques can
be applied, such as selectively ﬂooding each OR outside
of Tor and attempting to observe an increased latency in-
side Tor [10]. Other TCP metrics may be amalgamated
to further aid this attack: congestion window, slow-start
threshold, occurrence of congestion over time, standard
deviation in round-trip times, etc. The feasibility of this
attack should be examined before allowing nodes who do
not already know each other’s identities to share a TCP
conversation.
5.2 Stream Control Transmission Protocol
The Stream Control Transmission Protocol (SCTP) [13]
is a message-based transport protocol. It provides sim-
ilar features to TCP: connection-oriented reliable deliv-
ery with congestion control. However, it adds the ability
to automatically delimit messages instead of requiring
the receiving application to manage its own delimiters.
The interface is based on sending and receiving mes-
sages rather than bytes, which is appropriate for Tor’s
cell-based transport.
More importantly, SCTP also adds a feature well-
suited to our purposes—multiple streams can be trans-
ported over the same connection. SCTP allows multi-
ple independent ordered streams to be sent over the same
socket; we can use this feature to assign each circuit a
different stream. Cells from each circuit will arrive in
the order they were sent, but the order cells arrive across
all circuits may vary from they dispatch order. This is
exactly the behaviour we want for cells from different
circuits being sent between the same pair of ORs.
While SCTP is not as widely deployed as TCP, the
concept of using a user-level SCTP implementation [5]
inside Tor remains feasible. This suggests a SCTP-over-
DTLS transport similar in design to our TCP-over-DTLS
design. This means that the extra beneﬁts of TCP-over-
DTLS will also extend to SCTP-over-DTLS: backwards
compatibility with the existing Tor network, a constant
number of kernel-level sockets required, and a secured
transport header.
What is most interesting about the potential of SCTP-
over-DTLS is SCTP’s congestion control mechanism.
Instead of each TCP stream storing its own congestion
control metrics, SCTP will share metrics and computa-
tions across all streams. An important question in the
development of such a scheme is whether SCTP will act
fairly towards streams that send little data when other
streams invoke congestion control, and whether the shar-
ing of congestion control metrics results in a privacy-
degrading attack by leaking information.
6 Future Work
6.1 Live Experiments
The most pressing future work is to perform these ex-
periments on live networks of geographically distributed
machines running TCP-over-DTLS Tor, using comput-
ers from the PlanetLab network, or indeed on the live
Tor network. Once running, we could measure latency
and throughput as we have already in our experiments,
comparing against results for regular Tor. Moreover, we
can also compare other approaches, such as SCTP-over-
DTLS and UDP-OR, using the same experiments. Note
that UDP-OR could of course not be tested on the live Tor
network, but it could be in a PlanetLab setup. A key met-
ric will be the distribution of throughput and latency for
high- and low-volume circuits before and after our im-
provements, and an analysis of the cause of the change.
Additionally, once most ORs use UDP, we can determine
if the reduced demand on open sockets solves the prob-
lem of socket proliferation on some operating systems.
6.2 TCP Stack Memory Management
Tor requires thousands of sockets to buffer ﬁxed-size
cells of data, but data is only buffered when it arrives
out-of-order or has not been acknowledged. We envision
dynamic memory management such as a shared cell pool
to handle memory in Tor. Instead of repeatedly copying
data cells from various buffers, each cell that enters Tor
can be given a unique block of memory from the cell pool
until it is no longer needed. A state indicates where this
cell currently exists: input TCP buffer, input Tor buffer,
in processing, output Tor buffer, output TCP buffer. This
ensures that buffers are not allocated to store empty data,
which reduces the overall memory requirements. Each
cell also keeps track of its socket number, and its posi-
tion in the linked list of cells for that socket. While each
socket must still manage data such as its state and metrics
for congestion control, this is insigniﬁcant as compared
to the current memory requirements. This permits an ar-
bitrary number of sockets, for all operating systems, and
helps Tor’s scalability if the number of ORs increases by
orders of magnitude.
This approach results in the memory requirements of
Tor being a function of the number of cells it must man-
age at any time, independent of the number of open sock-
ets. Since the memory requirements are inextricably tied
to the throughput Tor offers, the user can parameterize
memory requirements in Tor’s conﬁguration just as they
parameterize throughput. A client willing to denote more
throughput than its associated memory requirements will
have its contribution throttled as a result.
If network
conditions result in a surge of memory required for Tor,
then it can simply stop reading from the UDP multiplex-
ing socket. The TCP stacks that sent this unread data
will assume there exists network congestion and conse-
quently throttle their sending—precisely the behaviour
we want—while minimizing leaked information about
the size of our cell pool.
7 Summary
Anonymous web browsing is an important step in the de-
velopment of the Internet, particularly as it grows ever
more inextricable from daily life. Tor is a privacy-
enhancing technology that provides Internet anonymity
using volunteers to relay trafﬁc, and uses multiple relays
in series to ensure that no entity (other than the client) in
the system is aware of both the source and destination of
messages.
Relaying messages increases latency since trafﬁc must
travel a longer distance before it is delivered. However,
the observed latency of Tor is much larger than just this
effect would suggest. To improve the usability of Tor,
we examined where this latency occurs, and found that it
happens when data sat idly in buffers due to congestion
control. Since multiple Tor circuits are multiplexed over
a single TCP connection between routers, we observed
cross-circuit interference due to the nature of TCP’s in-
order, reliable delivery and its congestion control mech-
anisms.
Our solution was the design and implementation of
a TCP-over-DTLS transport between ORs. Each cir-
cuit was given a unique TCP connection, but the TCP
packets themselves were sent over the DTLS protocol,
which provides conﬁdentiality and security to the TCP
header. The TCP implementation is provided in user
space, where it acts as a black box that translates between
data streams and TCP/IP packets. We performed exper-
iments on our implemented version using a local exper-
imentation network and showed that we were successful
in removing the observed cross-circuit interference and
decreasing the observed latency.
Acknowledgements
We would like to thank Steven Bellovin, Vern Pax-
son, Urs Hengartner, S. Keshav, and the anonymous re-
viewiers for their helpful comments on improving this
paper. We also gratefully acknowledge the ﬁnancial sup-
port of The Tor Project, MITACS, and NSERC.
References
[1] Tim Dierks and Eric Rescorla. RFC 5246—The
Transport Layer Security (TLS) Protocol Version
1.2. http://www.ietf.org/rfc/rfc5246.txt, August
2008.
[2] Roger Dingledine, Nick Mathewson, and Paul
Syverson. Tor: The Second-Generation Onion
Router. Proceedings of the 13th USENIX Security
Symposium, 2004.
[3] Krishna P. Gummadi, Stefan Saroiu, and Steven D.
Gribble. King: Estimating Latency between
Arbitrary Internet End Hosts. ACM SIGCOMM
Computer Communication Review, 2002.
[4] Information Sciences Institute. RFC
793—Transmission Control Protocol.
http://www.ietf.org/rfcs/rfc793.txt, September
1981.
[5] Andreas Jungmaier, Herbert H¨olzlwimmer,
Michael T¨uxen, and Thomas Dreibholz. The
SCTP library (sctplib).
http://www.sctp.de/sctp-download.html, 2007.
Accessed February 2009.
[6] Stephen Kent and Randall Atkinson. RFC
2401—Security Architecture for the Internet
Protocol. http://www.ietf.org/rfcs/rfc2401.txt,
November 1998.
[7] Marcus Leech et al. RFC 1928—SOCKS Protocol
Version 5. http://www.ietf.org/rfc/rfc1928.txt,
March 1996.
[8] Damon McCoy, Kevin Bauer, Dirk Grunwald,
Parisa Tabriz, and Douglas Sicker. Shining Light
in Dark Places: A Study of Anonymous Network
Usage. University of Colorado Technical Report
CU-CS-1032-07, August 2007.
[9] Nagendra Modadugu and Eric Rescorla. The
Design and Implementation of Datagram TLS.
Network and Distributed System Security
Symposium, 2004.
[10] Steven J. Murdoch and George Danezis. Low-Cost
Trafﬁc Analysis of Tor. In IEEE Symposium on
Security and Privacy, pages 183–195, 2005.
[11] Prashant Pradhan, Srikanth Kandula, Wen Xu,
Anees Shaikh, and Erich Nahum. Daytona: A
User-Level TCP Stack.
http://nms.lcs.mit.edu/˜kandula/data/daytona.pdf,
2002.
[12] Joel Reardon. Improving Tor using a
TCP-over-DTLS Tunnel. Master’s thesis,
University of Waterloo, Waterloo, ON, September
2008.
[13] Randall Stewart, Qiaobing Xie, Ken Morneualt,
Chip Sharp, Hanns Juergen Schwarzbauer, Tom
Taylor, Ian Rytina, Malleswar Kalla, Lixia Zhang,
and Vern Paxson. RFC 2960—Stream Control
Transmission Protocol.
http://www.ietf.org/rfc/rfc2960.txt, October 2000.
[14] TorStatus. Tor Network Status.
http://torstatus.kgprog.com/. Accessed February
2009.
[15] Camilo Viecco. UDP-OR: A Fair Onion Transport
Design.
http://www.petsymposium.org/2008/hotpets/udp-
tor.pdf,
2008.