aggregation interval
analysis interval
16
24
0.5 hops
6 min
60 min
Table 3: Parameters for Pastry and Beehive
owners to stipulate redirections of queries for certain names
using a special redirection record. High lookup performance
during redirections is ensured through proactive replication
and update of the redirection record in the same manner as
regular resource records.
As with any peer-to-peer system, CoDoNS relies on its
participants to contribute resources on behalf of others. While
it may seem, at (cid:12)rst, that rational actors might be averse
to participating in the system for fear of having to serve as
home nodes for highly popular records, proactive replication
ensures that the load perceived by all nodes is comparable.
A highly popular record will be replicated until the load it
projects on its home node is comparable to the query load
for other records.
4. EVALUATION
We have deployed CoDoNS on PlanetLab [2], an open
platform for developing, deploying, and accessing planetary-
scale services. PlanetLab enables us to deploy CoDoNS on
servers around the world and evaluate it against the back-
ground of real Internet with congestion, losses, and unpre-
dictable failures.
In this section, we present performance
measurements from the PlanetLab deployment for a real
DNS workload. Our experiments highlight three impor-
tant properties of CoDoNS. First, they show that CoDoNS
provides a low latency name resolution service. Second,
they demonstrate CoDoNS’ ability to resist (cid:13)ash-crowds by
quickly spreading the load across multiple servers. Finally,
they evaluate CoDoNS’ support for fast update propagation.
4.1 Setup
We setup a peer-to-peer network of CoDoNS servers on
globally distributed PlanetLab nodes. The values used for
di(cid:11)erent parameters of Pastry and Beehive are listed in Ta-
ble 3. We started the CoDoNS servers with no initial DNS
records. After an initial quiescent period to stabilize Pastry,
we issue DNS requests from a real workload to the CoDoNS
server at each node. During the experiment, we measure
the lookup latency of CoDoNS, and periodically record the
load handled and overhead incurred by each node. We also
apply the same workload to the legacy DNS, and measure
its performance.
We obtained the DNS workload from traces collected at
MIT between the 4th and the 11th of December 2000 [18].
Our workload comprises of the (cid:12)rst 12 hours of this trace,
with 281; 943 total queries for 47; 230 unique domain names.
The popularity of the domain names in this workload closely
follows a Zipf-like distribution with parameter 0:91. We di-
vided this workload uniformly and issued DNS requests to
the local CoDoNS server at each node. The measurements
reported in this paper were taken from a deployment on 75
geographically distributed PlanetLab nodes.
100
90
80
70
60
50
40
30
20
10
)
%
(
F
D
C
0
100
codons
codons+dns
legacy dns
101
103
102
latency (ms)
104
105
Figure 5: Cumulative Distribution of Latency: CoDoNS
achieves low latencies for name resolution. More than
50% of queries incur no network delay as they are an-
swered from the local CoDoNS cache.
Mean Median 90th %
Latency
105 ms
106 ms
CoDoNS
213 ms
199 ms
CoDoNS+DNS
337 ms
Legacy DNS
382 ms
PlanetLab RTT 121 ms
202 ms
1 ms
2 ms
39 ms
82 ms
Table 4: Query Resolution Latency: CoDoNS pro-
vides low latency name resolution through analytically
informed proactive caching.
4.2 Lookup Performance
Figure 5 shows the cumulative distribution of lookup la-
tencies incurred by CoDoNS and the legacy DNS. Table 4
summarizes the results of Figure 5 by providing the me-
dian, mean, and the 90th percentile of the latency distribu-
tion. We aggregate the latency during the second half of
the workload, allowing the (cid:12)rst half to warm the caches of
both CoDoNS and the legacy DNS. Note that the second
half of the workload also contains DNS requests for domain
names not present in the cache, and CoDoNS incurs the ex-
tra latency of redirecting these queries to the legacy DNS.
In order to study the impact of legacy DNS redirections on
latency, we separately evaluate the lookup performance of
CoDoNS without redirections, by inserting the records at
their home nodes before applying the work load. This study
essentially evaluates the scenario after a complete take over
of the legacy DNS by CoDoNS.
50% of the queries in CoDoNS are answered immediately
by the local CoDoNS server without incurring network de-
lay, since proactive replication pushes responses for the most
popular domain names to all CoDoNS servers. Consequently,
CoDoNS provides a signi(cid:12)cant decrease in median latency
to about 2 milliseconds compared to about 39 milliseconds
for the legacy DNS. The tail of the latency distribution in-
dicates that cache misses leading to legacy DNS lookups
have an impact on the worst-case lookup performance of
CoDoNS. However, a complete take over from the legacy
DNS would obviate the extra latency overhead. Overall,
CoDoNS achieves low latencies in the mean, median, and
the 90th percentile, for both deployment scenarios, with and
without redirections to the legacy DNS.
codons
legacy dns
100
90
80
70
60
50
40
30
20
10
)
s
m
(
y
c
n
e
t
a
l
0
0
2
4
8
time (hours)
6
10
12
Figure 6: Median Latency vs Time: Lookup latency
of CoDoNS decreases signi(cid:12)cantly as proactive caching
takes e(cid:11)ect in the background.
Figure 6 shows the median latency of CoDoNS and the
legacy DNS over time. The (cid:13)uctuations in the graph stem
from the changing relative popularities of names in the work-
load over time. CoDoNS reacts to these changes by contin-
uously adjusting the extent of proactive caching. Initially,
CoDoNS servers have an empty cache and redirect most of
the queries to legacy DNS. Consequently, they incur higher
latencies than the legacy DNS. But as resource records are
fetched from legacy DNS and replication in the background
pushes records to other CoDoNS servers, the latency de-
creases signi(cid:12)cantly. The initial surge in latency can be eas-
ily avoided by bootstrapping the system with records for
well known domain names.
4.3 Flash-crowd Effect
Next, we examine the resilience of CoDoNS to sudden
upheavals in the popularity of domain names. To model a
(cid:13)ash-crowd e(cid:11)ect, we take the DNS workload and modify the
second half to re(cid:13)ect large scale changes in the popularity of
all domain names. We achieve this by completely reversing
the popularities of all the domain names in the workload.
That is, the least popular name becomes the most popular
name, the second least popular name becomes the second
most popular name, and so on. This represents a worst case
scenario for CoDoNS because records that are replicated the
least suddenly need to be replicated widely, and vice versa,
simulating, in essence, a set of (cid:13)ash crowds for the least
popular records.
Figure 7 shows the median resolution latencies in CoDoNS
during the (cid:13)ash-crowd e(cid:11)ect introduced at the six hour
mark. There is a temporary increase in the median latency
of CoDoNS when (cid:13)ash-crowd e(cid:11)ect starts. But, Beehive’s
proactive replication in the background detects the changes
in popularity, adjusts the number of replicas, and decreases
the lookup latency. The latency of CoDoNS after popu-
larity reversal quickly reaches the low values in Figure 6,
indicating that CoDoNS has recovered completely from the
worst-case, large scale changes in popularity.
4.4 Load Balance
We evaluate the automatic load-balancing provided by
proactive replication in CoDoNS by quantifying load bal-
)
s
m
(
y
c
n
e
t
a
l
100
90
80
70
60
50
40
30
20
10
0
4
flash crowd
no flash crowd
codons
legacy dns
1
0.8
0.6
0.4
0.2
n
o
i
t
a
i
r
a
v
f
o
t
n
e
i
c
i
f
f
e
o
c
6
8
time (hours)
10
12
0
0
2
4
8
time (hours)
6
10
12
Figure 7: Median Latency vs Time as a (cid:13)ash-crowd is
introduced at 6 hours: CoDoNS detects the (cid:13)ash-crowd
quickly and adapts the amount of caching to counter it,
while continuing to provide high performance.
Figure 8: Load Balance vs Time: CoDoNS handles
(cid:13)ash-crowds by balancing the query load uniformly
across nodes. The graph shows load balance as a ratio
of the standard deviation to the mean across all nodes.
ance using the coe(cid:14)cient of variation, de(cid:12)ned as the ratio
of the standard deviation of the load across all the nodes to
the mean load. The overall average of query load is about
6:5 per second for the system.
Figure 8 shows the load balance in queries handled by
CoDoNS servers, either from their internal cache or by query-
ing the legacy DNS, for the duration of the workload. At
the start of the experiment, the query load is highly unbal-
anced, since home nodes of popular domain names receive
far greater number of queries than average. The imbalance
is signi(cid:12)cantly reduced as the records for popular domains
get replicated in the system. Even when a (cid:13)ash-crowd is in-
troduced at the six hour mark, dynamic changes in caching
keep the load balanced after a temporary increase in load
variance. Overall, continuous monitoring and adaptation
of proactive caching enable CoDoNS to respond to drastic
changes in the popularity of names and handle (cid:13)ash crowds.
The network bandwidth and per-node storage costs in-
curred by proactive caching are modest. The average band-
width consumed over the entire experiment was 12.2 KB/s
per node (std. dev. 2.26 KB/s) for all network activities.
The average number of records per node was 4217 (std. dev.
348), a mere 10% of the total number of records. These
records require, on average, 13 MB per node. These mea-
surements indicate that CoDoNS distributes the load evenly
across the system and incurs low uniform bandwidth and
storage overhead at each node.
100
90
80
70
60
50
40
30
20
10
)
%