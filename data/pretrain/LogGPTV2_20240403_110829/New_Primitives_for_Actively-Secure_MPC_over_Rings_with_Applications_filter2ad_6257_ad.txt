SVM S is implemented as a matrix F ∈ R
q×n where
the rows are known as the support vectors and a vector
b = (b1, . . . , vq) ∈ R
n which is called the bias. Conceptu-
ally, each support vector, along with a scalar from the bias
vector, can classify an input x into a speciﬁc category (or
not). Speciﬁcally denoting the rows of F as F1, . . . , Fq, the
value Fi· x + bi is computed to give a score of how likely x
is to be in category i. Thus, to ﬁnd the most likely category
of x we compute category(x) = argmaxi∈[1,q]Fi · x + bi
where the result is an integer representing the corresponding
category.
Like the case for decision trees we consider the two-
party setting where one party, called the client holds the
feature vector x = (x1, . . . , xn) ∈ R
n. The other party,
called the server holds the SVM S. The parties then wish
to compute S(x) = z where the client learns z and the
server learns nothing. We express this functionality formally
in Fig. 13 (Sec. X). Similarly to the decision trees, we work
over a ﬁnite set of integers Z2k, assuming two’s complement
representation to allow for integers in the range [2k−1, 2k)
1) An actively secure protocol: Our protocol follows
the equation for SVM classiﬁcation, category(x) =
argMaxi∈[1,q]Fi · x + bi, very straight forward: In paral-
lel compute the multiplication part of the inner products
between x and Fi for all i ∈ [1, q], as these are all
independent. Next we note that addition does not require
communication and thus we sequentially have the parties
sum up the component-wise product computed, in order
to compute the whole inner product. Next, for each inner
product the parties add bi. These steps only require constant
rounds of communication and q · n multiplications. Finally
computing the largest element of the q element list is done
in O(log(q)) rounds as follows: In a recursive manner
divide the list of elements in halves until
two or three
elements remain. Compare these obliviously, and based on
this comparison construct a binary list where the index of
the maximum of these two or three elements is 1 and the
is rest 0. This requires one or two comparisons and at most
four multiplications. The merging of the partial results then
require O(q) comparisons and multiplications. Thus we end
with a total of O(q·log(q)) comparisons and multiplications
for the arg-max computation. We express this actively secure
protocol in detail in Fig. 14 and 15 of Sec. X.
VI. IMPLEMENTATION
To reach a compromise between usability and efﬁciency
we chose to implement the online and ofﬂine phases of
SPDZ2k and our protocols in different frameworks.
We implement the online phase in FRESCO [22], an
active open-source Java framework for MPC with a strong
track record [14], [40]. We chose FRESCO as it offers an ac-
cessible API-based approach for writing MPC applications.
This eased the implementation of the decision tree and SVM
evaluation. Since FRESCO is written in Java, it also eases
integration with broad, cross-platform pieces of software.
Though Java is less efﬁcient than C/C++ we consider the
lower implementation and maintenance time required for
Java to make the trade-off worthwhile.
As the benchmarks in the next section show, the ofﬂine
phase requires orders of magnitude more time to execute
than the online phase. As such, time spent ensuring an
efﬁcient ofﬂine phase gives a noticeable payoff in the view
of total execution time. We therefore implement the ofﬂine
phase in C/C++. The ofﬂine protocol we implemented is
the same as described in the original SPDZ2k paper [17].
That is, authentication of elements and construction of triples
is based on a vector Oblivious Linear function Evaluation
(vOLE) construction through correlated OT, using the recent
OT extension protocol by Scholl [41]. We integrated our
implementation into the Bristol-SPDZ framework
[23].
Bristol-SPDZ is a highly efﬁcient framework for prepro-
cessing. The framework already supports OT based prepro-
cessing through MASCOT [10], and so integrating SPDZ2k
preprocessing required little work.
FRESCO supports a bring-your-own-backend approach,
and implements the most efﬁcient SPDZ online phase,
(cid:18)(cid:18)(cid:18)(cid:17)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:49:58 UTC from IEEE Xplore.  Restrictions apply. 
BigInt(mod p)
BigInt(nored.)
Custom
Java Arithmetic Benchmark
335
234
54
45.2
16.3
9.55
k = 32
k = 64
Figure 6: Time in milliseconds for 1,000,000 multiplications in Java, using
different implementations. Numbers are the average of 100 experiments.
“BigInt (mod p)” represents multiplications based on Java’s BigInteger
class with moduluar reducations, and “BigInt (no red.)” without.
“Custom” represents the time used by our custom implementation.
SPDZ-2
[18]. Besides containing an implementation of
MASCOT, Bristol-SPDZ also implements the most efﬁ-
cient SPDZ preprocessing protocol, Overdrive [24], making
the combination of FRESCO and Bristol-SPDZ a sensible
choice for doing a fair comparison of both SPDZ2k and
SPDZ, from preprocessing to online execution.
A. Optimizations
Here we detail several optimizations we used when
implementing the online and preprocessing phases. Our
preprocessing phase optimizations allow us to reduce the
computation time so that in most cases, the networking is the
bottleneck of the protocol. We describe our core online phase
optimization (Sec. VI-A), and the three main preprocessing-
phase optimizations (Sec. VI-A, VI-A, X-A).
Fast Integer Arithmetic for the Online Phase: FRESCO
uses the BigInteger class to implement ﬁnite ﬁeld arithmetic.
The largest primitive type supported by Java is long, at 64
bits. Thus, in order to fully leverage the option of working
with 2k bit integers in SPDZ2k we implemented our own
data-type for 128 bit integers, working on top of longs.
This implementation outperforms the BigInteger class, even
without taking into account that we don’t need to reduce
values modulo a large prime. Fig. 6 shows how much this
implementation reduces the time spent on multiplications in
the online phase. The ﬁgure compares the execution time for
doing 1,000,000 online multiplications in FRESCO using
the SPDZ protocol (which uses BigInteger and requires a
modular reduction of a large prime), SPDZ2k based on
BigInteger (that is, no modular reduction) and our optimized
approach for SPDZ2k where multiplication is done directly
with long types. Using our custom class based directly on
longs is up to 4.7x times faster than Java’s BigInteger class.
Comparing this with the amount of computation required
by SPDZ, the our SPDZ2k implementation becomes up to a
factor 24.5 faster. Even using BigInteger for SPDZ2k, still
results in a factor 5.2 improvement. This afﬁrms statements
made by Cramer et al. [17] that not needing to do modulo
reduction of a large prime will have a noticeable impact on
practical efﬁciency.
Fast Hashing with AES-NI: At several key places in
the preprocessing phase, we perform many calls to a hash
function on short inputs. Instead of using a standard hash
function such as SHA-256, we use the Matyas-Meyer-Oseas
construction [42], which builds a hash function out of a
block cipher and is secure in the ideal cipher model. This
greatly improves performance, since we can take advantage
of Intel’s AES-NI instructions on modern CPUs.
When the input and output of the hash are a single 128-bit
block, the hash function can be done using ﬁxed-key AES
(for a random, pre-agreed key) with the simple construction
H(x) = AESk(x) ⊕ x. Note that this optimization was
previously used for MPC in [10].
Fast Hashing for Large Domains: The MMO con-
struction is less efﬁcient when applied to a large domain,
since processing multiple input blocks requires a re-keying
operation for AES, which is a lot more expensive than
ﬁxed-key AES encryption with AES-NI. The construction of
correlated OTs using Scholl’s protocol [41] (needed for the
SPDZ2k triple generation [17]) requires computing several
hashes on very long inputs. We propose a new approach
to implementing these by combining a 2-universal family
of hash functions, H, and a strong cryptographic hash such
as SHA-256, with the function H(x) = SHA256(h(X)),
where h ← H. The advantage over using only, say,
SHA-256 or MMO, is that we can use a linear universal
hash function over F2128 such as GMAC, and this ﬁnite
ﬁeld arithmetic can be implemented very efﬁciently using
carryless multiplication from the AES-NI instruction set.
Note that when implementing this in the protocol, we require
that the function h is sampled at random by the receiver, just
before the consistency check is carried out by the (possibly
corrupt) sender.
We now argue that this approach still sufﬁces for the
security of the correlated OT protocol over Z2k from [41,
Section 5]. In the consistency check, a hash function is used
to allow a possibly corrupt sender in the protocol to ‘prove’
knowledge of certain values known to the receiver, to show
that the sender’s previous protocol messages were computed
correctly. This proof consists of sending various hashes on
very long inputs for the receiver to check.
Recall
that when h is sampled at random from a 2-
universal family of hash functions, collision resistance holds
with overwhelming probability, as long as the inputs to h are
independent of the random choice of h. In general, this may
not hold in a protocol where the inputs can be adversarial,
since given h it is easy to ﬁnd two inputs that generate a
collision. However, in our case there is no problem, since it
turns out that the only inputs for which collision resistance
is required to hold are already ﬁxed before the consistency
check. This is because the check is always carried out by the
(honest) receiver on inputs known after the previous round
of messages, as can be seen from the proof of [41, Lemma 8]
or [43, Lemma 3.1]. In conclusion, as long as h is sampled
(cid:18)(cid:18)(cid:18)(cid:18)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:49:58 UTC from IEEE Xplore.  Restrictions apply. 
after this then we are ﬁne.
then we could even omit
Note that if collision-resistance was the only property
the SHA256 call
we needed,
in the computation of H(x) . However, for the case of
a corrupt receiver the protocols of [41], [43] also need
a pseudorandomness property, so we apply a strong hash
function on the output to act as a randomness extractor.
We microbenchmark
these
pendix X-A.
optimizations
in Ap-
VII. PERFORMANCE EVALUATION
Further, we evaluate the concrete performance of our
implementation of the online phase (Sec. VII-A) and the
ofﬂine phase (Sec. VII-B). In Sec. XI of the appendix,
we also evaluate memory usage, and show that it is not
a bottleneck.
For the online phase, we run micro-benchmarks for our
basic primitives as well as end-to-end evaluations of our two
high-level applications on realistic datasets. We then com-
pare our online implementation of SPDZ2k in FRESCO with
the baseline SPDZ implementation in FRESCO. The SPDZ
implementation in FRESCO is based on SPDZ-2 [18], which
is the most recent and efﬁcient online protocol for SPDZ. In
our evaluation of the ofﬂine phase, we evaluate the SPDZ2k
triple generation protocol across varying security parameters
and network conﬁgurations. We then compare our ofﬂine
implementation in the Bristol-SPDZ C++ framework with
the two most recent and efﬁcient protocols for SPDZ triple
generation; MASCOT [10] and Overdrive [24]. Both of
these are also implemented in the Bristol-SPDZ framework,
which ensures a more fair comparison.
Furthermore, we are unaware of any other practically
competitive protocols considering a dishonest majority of
malicious parties in the arithmetic setting and thus believe
that comparing to SPDZ is sufﬁcient.
We chose to benchmark our protocols in the two-party
setting, although all our constructions (except the protocols
for the speciﬁc setting of oblivious decision tree and SVM
evaluation) generalize to an arbitrary amount of parties. We
did this for simplicity and since both SPDZ and SPDZ2k
generalize to more parties with similar overheads.
Setup. We run all experiments in the two-party setting.
Each party executes on an m5d.xlarge AWS EC2 instance
running Ubuntu 16.04, with 4 vCPUs and 16GB memory.
The instances are hosted within the same region and con-
nected over an up to 10 Gbps link. To investigate how
different network settings affect
the performance of our
protocols, we use tc to simulate bandwidth restrictions and
latency. For all experiments, we performed a minimum of
20 total runs and report the average result. We discard the
ﬁrst run in order to ensure the JVM has warmed up.
A. Online Phase
For our online phase experiments, we consider two bit
length setting, we use
length settings. For the low bit
(cid:18)(cid:18)(cid:18)(cid:19)
k = s = 32 (total bit length of 64) which supports 32-
bit comparisons and equality operations and affords 26 bit
statistical security. We compare this setting to running SPDZ
over a 64 bit ﬁeld; the larger ﬁeld is necessary to ensure at
least 26 bits of statistical security in the comparison protocol
used by SPDZ. Similarly, we compare the larger bit setting
with k = 64, s = 64, total bit length 128, and 57 bit
statistical security to SPDZ over a 128 bit ﬁeld with 57
bit statistical security.2
Table I shows throughput times (operations per second)
for three non-linear operations: multiplication, equality, and
comparison on a 1 Gbps network. We believe a 1 Gbps
LAN to be a suitable setting for the family of SPDZ2k and
SPDZ protocols; the high latency of lower bandwidth WAN
networks would signiﬁcantly limit performance due to the
protocols’ non-constant round complexity. Constant round
protocols are more appropriate for such settings. Conversely,
we do not report numbers for a faster network since at 1
Gbps our implementation is not network-bound.
We obtain the throughput numbers from batched runs, i.e.,
parallel3 operations with batched communication. We use
batches of 100,000 parallel operations for multiplications
and 5,000 for equality and comparison.
For multiplications we see between a 4.6 and 4.9-fold
improvement for the different bit-length settings. This per-
formance gain stems from a speed up in local computation
as well as reduced communication. Local computation im-
proves since we do not need to perform modular reductions
and use a custom class for ring elements of speciﬁc bit-
length (64 and 128 bit) which signiﬁcantly outperforms
BigInteger arithmetic as discussed in Sec. VI-A. The total
amount of data sent is also reduced; for all protocols that
require communicating an element to the other parties, we
only need to send the k least signiﬁcant bits, as opposed to
an entire element for SPDZ. This alone cuts communication
in half.
Comparison and equality (for k = 64) show an even
higher increase in performance, with the biggest improve-
ment for comparison, six-fold for k = 64 and ﬁve-fold for
k = 32.
Switching to boolean mode for the comparison protocol
replaces a majority of the underlying multiplications with
bit-multiplications, which require sending only 2 bits per
party, in contrast to two whole ﬁeld elements. This dras-
tically reduces communication as shown in Table V. The
improvement in throughput is not directly proportional to
the reduction in communication since our implemention
is not network-bound at 1 Gbps. We nonetheless observe
226, respectively 57 bits of security, are chosen for a fair comparison
with SPDZ2k , as SPDZ2k has a logarithmic deterioration of the statistical
security, because of batched MAC checks.
3Parallel here does not imply running on multiple threads; it merely
means that the operations are independent and communication can thus be
batched.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:49:58 UTC from IEEE Xplore.  Restrictions apply. 
Table I: Throughput in elements per second for the online phase of micro operations over 1 Gbps network. The factor columns express the runtime
improvement factor of SPDZ2k over SPDZ in FRESCO.
k = 32
k = 64
SPDZ2k (σ = 26)
SPDZ (σ = 26)
687041
15334
9153
141346
3213
1769
Factor
4.9x
4.8x
5.2x
SPDZ2k (σ = 57)
SPDZ (σ = 57)
522258