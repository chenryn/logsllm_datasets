Environment 
34 
Failure((No(
Interrupt)(
Software 
43 
Software 
10 
Network  
Links 
150 
Software 
29 
Hardware 
6 
4 
Environment 
Interrup1on((
(SWO)(
Hardware 
56 
Interrupt(
(failover)(
Heartbeat/ 
Node Down 
3 
Hardware 
6 
Link(&(Node(
Failure((
(Job(Failed)(
(a)
Software 
80 
Hardware 
55 
Link(&(Node(
Failure((No(
Job(Failed)(
Network 
 Links 
12 
Software 
5 
Hardware 
2 
Link(Failure(
(Job(Failed)(
Unknown 43 
Software 64 
90% 
Heartbeat/  
Node Down 
70% 
307 
50% 
 Unknown 2.9% 
Network  
Links 10.9% 
Software 
20.1% 
Heartbeat 
20.8% 
Heartbeat*
<1%*
Network*
<1%*
Hardware*
23%*
Unknown 
Network Links 
Software 
Heartbeat 
Hardware 
Environment 
So9ware*
53%*
Hardware 
30% 
442 
Environment 
10% 
9 
Single/(
-10% 
Mul1ple(
Node(Failure(
Hardware 
42.1% 
Environment 
3.2% 
All(
categories(
Environment*
24%*
Unknown*
<1%*
(b)
Fig. 2: Breakdown of the failure categories (a), and distribution of the cumulative node repair hours (b), across hardware, software, network, unknown, heartbeat,
and environment root causes.
TABLE IV: Breakdown of the count of the top 3 hardware and software failure root causes
Failure (No Interrupt)
Interrupt (SWO)
Interrupt (Failover) Link Failure (User Job
Failed)
Link & Node Failure
(User Job Failed)
Single/Multiple Node Fail-
ure
HW
SW
PSU
IPMI
Fan tray assy
Moab/TORQUE
CLE/kernel
Warm swap
EPO 1
2
2
Lustre 18
20
15 Compute Blade
14
Storage module
33
17 Moab/TORQUE
5
Gemini
Disks 45
5
IPMI
2 Gemini voltage regulator
Storage module
Lustre net (Lnet)
Optic 12
RAM 9
GPU
Gemini ASIC
8 Compute blade
2
Lustre
CLE/kernel
6 Sonexion/storage
3
CLE/
Lustre 29
8
4
2 Processor
1 RAM
2 GPU
8 Lustre
1 CLE/Kernel
Sonexion/Storage
160
158
38
30
16
5
managed by the Cray architecture.
To identify a reason for those differences, we analyzed
the distribution of the number of nodes involved in failures
with hardware or software root causes. We found that failures
with hardware root causes that did not cause system-wide
outages propagated outside the boundary of a single blade
in only 0.7% of the cases. Cases in which failures caused
by hardware impacted a full blade involved failures in the
voltage converter module (VRM) of the mezzanine and/or in
problems with the cabinet controller. Data show that failures
with hardware root causes were limited to a single node or
a single blade (i.e., 4 nodes) 96.7% and 99.3% of the times
they occurred, respectively. Conversely, software failures, if
they did not cause a system-wide failure, propagated to more
than 1 node in 14% of the cases, i.e., 20 times more often than
the hardware. In addition, hardware is easier to diagnose than
software. Hardware failures are ﬁxed in bulk to reduce the cost
of ﬁeld intervention, e.g., after a given number of node failures.
Although that does not impact on the system MTTR (5.16 h),
it does result in a larger MTTR for single nodes (32.7 h), as
reported in Table III.
Figure 2.(b) also shows that failures whose root causes are
unknown (2.9% of the total failures; Figure 2.(a)) and lack of
heartbeat activity (20.8%) account for less than 2% of the total
node repair hours. Similar observations have been made in other
Cray machines [16]. In particular, based on discussions with the
system maintenance engineers, we determined that only 2% of
the failures detected by the heartbeat mechanisms were symp-
toms of real problems with the nodes. Periodically, each node
receives a heartbeat request that triggers a number of speciﬁc
tests. If the test fails or the heartbeat is not received/delivered
for some other reason, the node is marked as down by the
system management console. The test is automatically repeated,
by default, every 60 seconds for 35 minutes following a failure.
Hence, a node can be marked as ”down” and later pass the test
and return to the ”up” state without any intervention.
Table IV shows the breakdown of the top 3 hardware and
software root causes over the failure categories. Processors,
memory DIMMs, and GPUs are the 3 most frequently replaced
hardware components (together accounting for 72% of the
replaced components), out of an extensive list of 69 replaced
units. As discussed in the next section, processors are replaced
when the maintenance specialists observe speciﬁc hardware
exceptions (e.g., L1 cache parity errors) in the system console
logs. In many cases, the replaced processors are tested and
found to be free of problems, and the failure is attributed to a
problem with the socket or the voltage regulator. Problems in
the node or mezzanine (Gemini) voltage regulator accounted for
about 14% of single/multiple node failures in our gathered data.
In one case, the mezzanine voltage regulator failed because of
a bug in the L0 node controller ﬁrmware, causing a system-
wide outage and showing that software can have an impact
on low-level hardware failures. Lustre,
the CLE OS, and
Sonexion/storage software are the top 3 software root causes
among the failure categories. In particular, the Lustre ﬁle system
and related software caused 44% of the single/multiple node
failures attributed to software root causes, while the CLE OS
and Sonexion/Storage caused as much as 28% and 9% of the
total single/multiple node failures, respectively.
Lustre includes a rich software stack of more than 250k
lines of code and plays a crucial role in Blue Waters, since
all the compute nodes are diskless and rely on Lustre for ﬁle
system access. Failures caused by Lustre problems are the most
widespread cause of failures and are present in 6 out of 7 failure
categories in Table IV, and, as detailed in Section VI, 18 of
those 104 failures escalated to SWOs.
Figure 3 shows the breakdown of the root causes of Lustre
failures. Blue Waters experienced a total of 104 different fail-
Storage)Controller)
Module)
11%)
4%)
Metadata)unavailable)
ConﬁguraCon)
5%)
Out)of)Memory)
Lustre)Drivers)
4%)
5%)
OS)
9%)
LNET)
16%)
Failover)
25%)
LBUG)
19%)
RAM)
2%)
OST (Obj. 
Storage Target, 
12/27 failed) 
OSS (Obj. 
Storage Server, 
2/4 failed) 
MDT(Metadata)
Target,)12/26)
failed))
Fig. 3: Breakdown of the Lustre Failures
ures attributed to Lustre. The failure of any Lustre component
triggers a failover operations which was successful 75% the
times they were invoked (see Figure 3). In case of the inability
of the automated failover procedures to recover the normal
state of the system, the automated procedures are overridden
by the technical staff managing Blue Waters and the system
is manually recovered. A common cause of Lustre issues is
detection of the so-called LBUG (19%),
i.e., a panic-style
assertion in the storage node kernel. The unavailability of the
Object Storage Target (OST) or of the metadata (i.e., MDS,
MDT, and MGS in Lustre) that provide low-level support for
the storage system is another important Lustre failure category.
Conﬁguration problems contributed to 5% of the Lustre failures,
but they were not critical and often manifested as performance
issues, i.e., Lustre became slow. About 14% of Lustre failures
were due to hardware problems (RAM, 3%, and Controller
Module, 11%), and the failover mechanism recovered from all
of them.
B. Effectiveness of Failover
On average, each day, the system is subjected to 1 or 2 critical
failures triggering automatic failover procedures. Hence, an
important question is, how effective are the automated failover
procedures in recovering from failures due to hardware and
software causes?
About 28% of all failures triggered some type of failover
mechanism. That included 39 SWOs; 99 interrupt-failovers; 285
link failures in which no user job failed; 19 link failures in
which a user job failed; and 19 node failures in which a user
job failed, out of a total of 1,490 failures. The data contain
information on of 138 of triggered failover operations due to
failures attributed to hardware or software causes. Out of 138
recorded failover operations, system-wide outages occurred in
24.6% (39 out of 138) of the times; the remaining 99 failures
were successfully recovered (See Figure 2.(a)).
Lustre failover. The failure recovery of Lustre is a complex
procedure that typically requires movement of a large volume of
data over the network, and synchronization of the nodes partici-
pating in data movement. 26 out of 104 failover attempts failed;
speciﬁcally, i) the failover of the Metadata Target (MDT) failed
12 times out of 26, ii) the failover of the Object Storage Server
(OSS) failed 2 time out of 4, and iii) the failover of the Object
Storage Target (OST) failed 12 times out of 27. As one might
expect, the most critical failover procedures are those involving
the storage and metadata server nodes. Under heavy job loads,
those procedures are time-consuming (taking between 15 and
30 minutes). Recovery from a client failure in Lustre is based
on revocation and lock of resources, so the surviving clients
can continue their work uninterrupted. Because of the way the
Lustre Distributed Lock Manager handles parallel operations,
threads can block on a single resource. Moreover, threads must
hold a resource until clients acknowledge completion of an
operation. Even though the system is conﬁgured to support 512
service threads, they can quickly all become consumed because
of the blocking. In particular, data shows that the Lustre failover
fails causing a system-wide outages about 20% of the times
because of lock timeouts ﬁring due to the high volume of clients
to recover from during the failover (e.g., the high number of
active OSSes, see Section II). Those results indicate that
the
mechanisms behind Lustre failover procedures (e.g., timeouts)
operate at their limit at Blue Waters scale when dealing with
high workload conditions and high number of clients involved
in the recovery. In addition, because to the long recovery time
and of the dual-redundant conﬁguration of Lustre servers, the
chances that failures will occur during the time needed to
recover (and hence of a SWO) are not insigniﬁcant.
Gemini Failover. The Gemini network failover mechanism
seems to be the most resilient. Out of 323 documented link
failures (categories Link and Node Failure (Job Failed), Link
Failure (No Job Failed), and Link Failure (Job Failed) in Table
III), 38 caused the loss of one or more user jobs, while the
Gemini-supported failover mechanism succeeded in 285.
In the Gemini network, when a lane (a part of a Gemini
network link) fails, the network is automatically able to run
in degraded mode (i.e., diverting the trafﬁc to the remaining
links at the price of reduced speed). Such problems have 3
main causes: i) bad cable interconnections, ii) failure of a
routing node or a Gemini mezzanine card, and iii) network
congestion. Evidence shows that the network is agile and fault-
tolerant. We speculate that more attention has been paid to
Gemini software stack testing, while Lustre is naturally more
vulnerable to failures because of i) the richer software stack,
ii) the community-driven nature of product, and iii) the lack
of effective methods and tools to test how critical modules
such as the failover would behave over large-scale deployments
like Blue Waters. Blue Waters the largest Lustre deployment to
date, and our ﬁndings shows that more effort is required to
create better failover and testing techniques for improving the
resiliency of software working at the petascale and above.
V. HARDWARE ERROR RESILIENCY
Blue Waters maintenance specialists diagnose processor and
memory related problems by looking at the machine check
exceptions contained in the system logs. In looking at the
system logs produced by Blue Waters nodes, we counted
1,544,398 machine check events in the measurement period
(i.e., on average, a rate of 250 errors/h), of which only 28
consisted of uncorrectable errors, i.e., errors that cannot be
corrected by either ECC or Chipkill and may cause loss of
data, corruption of processor state, or both. That indicates an
unusual degree of containment of hardware problems that we
further investigate in this section. Table V shows the breakdown
of the machine check errors over the different node types.
In total, 12,721 nodes (46% of the total Blue Waters nodes)
experienced at
least a memory error; 82.3% of the nodes
that manifested machine checks were compute nodes; 19.4%
were GPU nodes and 7.34% were service nodes. 6.6% of
TABLE V: Breakdown of the machine check errors.
Compute
Service
GPU
All
Count Error/
MB
594 0.26
1,098 0.02
2,282 0.01
97,974 9.73E-4
1,102 -
Count Error/
MB
632 3.48
1,566 0.17
23,030 1.98
93,590 3.93E-3
41,788 -