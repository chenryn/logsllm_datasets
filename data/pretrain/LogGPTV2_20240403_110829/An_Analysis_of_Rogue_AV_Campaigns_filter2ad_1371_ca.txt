Spot queries (S) that target a single key (usually an IP address or port number)
282
P. Giura and N. Memon
and return a list with the values associated with that key. Range queries (R)
that return a list with results for multiple keys (usually attributes corresponding
to the IPs of a subnet). Aggregation queries (A) that aggregate the data for the
entire network and return the result of the aggregation (e.g. traﬃc sent out for
network). Spot Aggregation queries (SA) that aggregate the values found for one
key in a single value. Range Aggregation queries (RA) that aggregate data for
multiple keys into a single value. Examples of these types of queries expressed
in plain words:
(S) “What applications are observed on host X between dates d1 and d2?”
(R) “What is the list of destination IPs that have source IPs in a subnet between
(A) “What is the total number of connections for the entire network between
dates d1 and d2?”
dates d1 and d2?”
(SA) “What is the number of bytes that host X sent between dates d1 and d2?”
(RA) “What is the number of hosts that each of the hosts in a subnet contacted
between dates d1 and d2?”
3.2 Column Oriented Storage
Columns. In NetStore, we consider that ﬂow records with n attributes are
stored in the logical table with n columns and an increasing number of rows
(tuples) one for each ﬂow record. The values of each attribute are stored in one
column and have the same data type. By default almost all of the values of
a column are not sorted. Having the data sorted in a column might help get
better compression and faster retrieval, but changing the initial order of the el-
ements requires the use of auxiliary data structure for tuple reconstruction at
query time. We investigated several techniques to ease tuple reconstruction and
all methods added much more overhead at query time than the beneﬁt of bet-
ter compression and faster data access. Therefore, we decided to maintain the
same order of elements across columns to avoid any tuple reconstruction penalty
when querying. However, since we can aﬀord one column to be sorted without
the need to use any reconstruction auxiliary data, we choose to ﬁrst sort only
one column and partially sort the rest of the columns. We call the ﬁrst sorted
column the anchor column. Note that after sorting, given our storage architec-
ture, each segment can still be processed independently. The main purpose of the
anchor column choosing algorithm is to select the ordering that facilitates the
best compression and fast data access. Network ﬂow data express strong correla-
tion between several attributes and we exploit this characteristic by keeping the
strongly correlated columns in consecutive sorting order as much as possible for
better compression results. Additionally, based on previous queries data access
pattern, columns are arranged by taking into account the probability of each
column to be accessed by future queries. The columns with higher probabilities
are arranged at the beginning of the sorting order. As such, we maintain the
counting probabilities associated with each of the columns given by the formula
P (ci) = ai
t , where ci is the i-th column, ai the number of queries that accessed
ci and t the total number of queries.
NetStore: An Eﬃcient Storage Infrastructure
283
Segments. Each column is further partitioned into ﬁxed sets of values called
segments. Segments partitioning enables physical storage and processing at a
smaller granularity than simple column based partitioning. These design deci-
sions provide more ﬂexibility for compression strategies and data access. At query
time only used segments will be read from disk and processed based on the in-
formation collected from segments metadata structures called index nodes. Each
segment has associated a unique identiﬁer called segment ID. For each column, a
segment ID represents an auto incremental number, started at the installation of
the system. The segment sizes are dependent of the hardware conﬁguration and
can be set in such a way to use the most of available main memory. For better
control over data structures used, the segments have the same number of values
across all the columns. In this way there is no need to store a record ID for each
value of a segment, and this is one major diﬀerence compared to some existing
column stores [11]. As we will show in Section 4 the performance of the system
is related to the segment size used. The larger the segment size, the better the
compression performance and query processing times. However, we notice that
records insertion speed decreases with the increase of segment size, so, there is a
trade oﬀ between the query performance desired and the insertion speed needed.
Most of the columns store segments in compressed format and, in a later section
we present the compression algorithms used. Column segmentation design is an
important diﬀerence compared to traditional row oriented systems that process
data a tuple at a time, whereas NetStore processes data segment at a time, which
translates to many tuples at a time. Figure 3 shows the processing steps for the
three processing phases: buﬀering, segmenting and query processing.
Fig. 2. NetStore main components:
Processing Engine and Column-Store.
Fig. 3. NetStore processing phases: buﬀer-
ing, segmenting and query processing.
Column Index. For each column we store the meta data associated with each of
the segments in an index node corresponding to the segment. The set of all index
nodes for the segments of a column represent the column index. The information
in each index node includes statistics about data and diﬀerent features that are
used in the decision about the compression method to use and optimal data
284
P. Giura and N. Memon
access, as well as the time interval associated with the segment in the format
[min start time, max end time]. Figure 4 presents an intuitive representation
of the columns, segments and index for each column. Each column index is
implemented using a time interval tree. Every query is relative to a time window
T. At query time, the index of every column accessed is looked up and only
the segments that have the time interval overlapping window T are considered
for processing. In the next step, the statistics on segment values are checked
to decide if the segment should be loaded in memory and decompressed. This
two-phase index processing helps in early ﬁltering out unused data in query
processing similar to what is done in [15]. Note that the index nodes do not
hold data values, but statistics about the segments such as the minimum and
the maximum values, the time interval of the segment, the compression method
used, the number of distinct values, etc. Therefore, index usage adds negligible
storage and processing overhead.
From the list of initial queries we observed that the column for the source IP
attribute is most frequently accessed. Therefore, we choose this column as our
ﬁrst sorted anchor column, and used it as a clustered index for each source IP
segment. However, for workloads where the predominant query types are spot
queries targeting a speciﬁc column other than the anchor column, the use of
indexes for values inside the column segments is beneﬁcial at a cost of increased
storage and slowdown in insertion rate. Thus, this situation can be acceptable
for slow networks were the insertion rate requirements are not too high. When
the insertion rate is high then it is best not to use any index but rely on the
meta-data from the index nodes.
Internal IPs Index. Besides the column index, NetStore maintains another
indexing data structure for the network internal IP addresses called the Internal
IPs index. Essentially the IPs index is an inverted index for the internal IPs. That
is, for each internal IP address the index stores in a list the absolute positions
where the IP address occurs in the column, sourceIP or destIP , as if the column
is not partitioned into segments. Figure 5 shows an intuitive representation of the
IPs index. For each internal IP address the positions list represents an array of
increasing integer values that are compressed and stored on disk on a daily basis.
Because IP addresses tend to occur in consecutive positions in a column, we chose
to compress the positions list by applying run-length-encoding on diﬀerences
between adjacent values.
3.3 Compression
Each of the segments in NetStore is compressed independently. We observed that
segments within a column did not have the same distribution due to the temporal
variation of network activity in working hours, days, nights, weekends, breaks
etc. Hence segments of the same column were best compressed using diﬀerent
methods. We explored diﬀerent compression methods. We investigated methods
that allow data processing in compressed format and do not need decompression
of all the segment values if only one value is requested. We also looked at methods
NetStore: An Eﬃcient Storage Infrastructure
285
Fig. 4. Schematic representation of columns,
segments, index nodes and column indexes
Fig. 5. Intuitive representation of the
IPs inverted index
that provide fast decompression and reasonable compression ratio and speed.
The decision on which compression algorithm to use is done automatically for
each segment, and is based on the data features of the segment such as data type,
the number of distinct values, range of the values and number of switches between
adjacent values. We tested a wide range of compression methods, including some
we designed for the purpose or currently used by similar systems in [1,16,21,11],
with needed variations if any. Below we list the techniques that emerged eﬀective
based on our experimentation:
– Run-Length Encoding (RLE): is used for segments that have few distinct
repetitive values. If value v appears consecutively r times, and r > 1, we
compress it as the pair (v, r). It provides fast compression as well as the
ability to process data in compressed format.
– Variable Byte Encoding: is a byte-oriented encoding method used for
positive integers. It uses a variable number of bytes to encode each integer
value as follows: if value < 128 use one byte (set highest bit to 0), for
value < 128 ∗ 128 use 2 bytes (ﬁrst byte has highest bit set to 1 and second
to 0) and so on. This method can be used in conjunction with RLE for
both values and runs. It provides reasonable compression ratio and good
decompression speed allowing the decompression of only the requested value
without the need to decompress the whole segment.
– Dictionary Encoding: is used for columns with few distinct values and
sometimes before RLE is applied (e.g. to encode “protocol” attribute).
– Frame Of Reference: considers the interval bounded by the minimum
and maximum values as the frame of reference for the values to be com-
pressed [7]. We use it to compress non-empty timestamp attributes within a
segment (e.g. start time, end time, etc.) that are integer values representing
the number of seconds from the epoch. Typically the time diﬀerence be-
tween minimum and maximum timestamp values in a segment is less than
few hours, therefore the encoding of the diﬀerence is possible using short
values of 2 bytes instead of integers of 4 bytes. It allows processing data
in compressed format by decompressing each timestamp value individually
without the need to decompress the whole segment.
286
P. Giura and N. Memon
– Generic Compression: we use the DEFLATE algorithm from the zlib
library that is a variation of the LZ77 [20]. This method provides compression
at the binary level, and does not allow values to be individually accessed
unless the whole segment is decompressed. It is chosen if it enables faster
data insertion and access than the value-based methods presented earlier.
– No Compression: is listed as a compression method since it will represent
the base case for our compression selection algorithm.
Method Selection. The selection of a compression method is done based on
the statistics collected in one pass over the data of each segment. As mentioned
earlier, the two major requirements of our system are to keep records insertion
rates high and to provide fast data access. Data compression does not always
provide better insertion and better query performance compared to “No com-
pression”, and for this we developed a model to decide on when compression is
suitable and if so, what method to choose. Essentially, we compute a score for
each candidate compression method and we select the one that has the best score.
More formally, we assume we have k + 1 compression methods m0, m1, . . . , mk,
with m0 being the “No Compression” method. We then compute the insertion
time as the time to compress and write to disk, and the access time, to read from
disk and decompress, as functions of each compression method. For value-based
compression methods, we estimate the compression, write, read and decompres-
sion times based on the statistics collected for each segment. For the generic
compression we estimate the parameters based on the average results obtained
when processing sample segments. For each segment we evaluate:
insertion (mi) = c (mi) + w (mi) ,
access (mi) = r (mi) + d (mi) ,
i = 1, . . . , k
i = 1, . . . , k
As the base case for each method evaluation we consider the “No Compression”
method. We take I0 to represent the time to insert an uncompressed segment
which is represented by only the writing time since there is no time spent for
compression and, similarly A0 to represent the time to access the segment which
is represented by only the time to read the segment from disk since there is no
decompression. Formally, following the above equations we have:
insertion (m0) = w (m0) = I0 and access (m0) = r (m0) = A0
We then choose the candidate compression methods mi only if we have both:
insertion (mi) < I0 and access (mi) < A0
Next, among the candidate compression methods we choose the one that pro-
vides the lowest access time. Note that we primarily consider the access time
as the main diﬀerentiator factor and not the insertion time. The disk read is
the most frequent and time consuming operation and it is many times slower
than disk write of the same size ﬁle for commodity hard drives. Additionally,
insertion time can be improved by bulk loading or by other ways that take into
account that the network traﬃc rate is not steady and varies greatly over time,
NetStore: An Eﬃcient Storage Infrastructure
287
whereas the access mechanism should provide the same level of performance at
all times.
The model presented above does not take into account if the data can be
processed in compressed format and the assumption is that decompression is
necessary at all times. However, for a more accurate compression method selec-
tion we should include the probability of a query processing the data in com-
pressed format in the access time equation. Since forensic and monitoring queries
are usually predictable, we can assume without aﬀecting the generality of our
system, that we have a total number of t queries, each query qj having the proba-
bility of occurrence pj with
t(cid:7)
pj = 1. We consider the probability of a segment
j=1
s being processed in compressed format as the probability of occurrence of the
queries that process the segment in compressed format. Let CF be the set of all
the queries that process s in compressed format, we then get:
P (s) =
pj, CF = {qj|qj processes s in compressed format}
(cid:7)
qj∈CF
Now, a more accurate access time equation can be rewritten taking into account
the possibility of not decompressing the segment for each access:
access (mi) = r (mi) + d (mi) · (1 − P (s)) ,
i = 1, . . . , k
(1)
Note that the compression selection model can accommodate any compression,
not only the ones mentioned in this paper, and is also valid in the cases when
the probability of processing the data in compressed format is 0.
3.4 Query Processing
Figure 3 illustrates NetStore data ﬂow, from network ﬂow record insertion to
the query result output. Data is written only once in bulk, and read many times
for processing. NetStore does not support transaction processing queries such
as record updates or deletes, it is suitable for analytical queries in general and
network forensics and monitoring queries in special.
Data Insertion. Network data is processed in several phases before being de-
livered to permanent storage. First, raw ﬂow data is collected from the network
sensors and is then preprocessed. Preprocessing includes the buﬀering and seg-
menting phases. Each ﬂow is identiﬁed by a ﬂow ID represented by the 5-tuple
[sourceIP, sourcePort, destIP, destPort, protocol]. In the buﬀering phase, raw
network ﬂow information is collected until the buﬀer is ﬁlled. The ﬂow records
in the buﬀer are aggregated and then sorted. As mentioned in Section 3.3, the
purpose of sorting is twofold: better compression and faster data access. All the
columns are sorted following the sorting order determined based on access prob-
abilities and correlation between columns using the ﬁrst sorted column as anchor.
288
P. Giura and N. Memon
In the segmenting phase, all the columns are partitioned into segments, that is,
once the number of ﬂow records reach the buﬀer capacity the column data in
the buﬀer is considered a full segment and is processed. Each of the segments is
then compressed using the appropriate compression method based on the data
it carries. The information about the compression method used and statistics
about the data is collected and stored in the index node associated with the
segment. Note that once the segments are created, the statistics collection and
compression of each segment is done independent of the rest of the segments in
the same column or in other columns. By doing so, the system takes advantage of
the increasing number of cores in a machine and provides good record insertion
rates in multi threaded environments.
After preprocessing all the data is sent to permanent storage. As monitoring
queries tend to access the most recent data, some data is also kept in memory
for a predeﬁned length of time. NetStore uses a small active window of size W
and all the requests from queries accessing the data in the time interval [NOW
- W, NOW] are served from memory, where NOW represents the actual time of
the query.
Query Execution. For ﬂexibility NetStore supports limited SQL syntax and
implements a basic set of segment operators related to the query types presented
in Section 3.1. Each SQL query statement is translated into a statement in terms
of the basic set of segment operators. Below we brieﬂy present each general
operator:
– ﬁlter segs (d1, d2): Returns the set with segment IDs of the segments that
overlap with the time interval [d1, d2]. This operator is used by all queries.
– ﬁlter atts(segIDs, pred1(att1), . . . , predk(attk)): Returns the list of pairs
(segID, pos list), where pos list represents the intersection of attribute po-
sition lists in the corresponding segment with id segID, for which the at-
tribute atti satisﬁes the predicate predi, with i = 1, . . . , k.
– aggregate (segIDs, pred1(att1), . . . , predk(attk)): Returns the result of ag-
gregating values of attribute attk by attk−1 by . . . att1 that satisfy their
corresponding predicates predk, . . . , pred1 in segments with ids in segIDs.
The aggregation can be summation, counting, min or max.
The queries considered in section 3.1 can all be expressed in terms of the above
operators. For example the query: “What is the number of unique hosts that each
of the hosts in the network contacted in the interval [d1, d2]?” can be expressed
as follows: aggregate(ﬁlter segs(d1, d2), sourceIP = 128.238.0.0/16, destIP ).
After the operator ﬁlter segs is applied, only the sourceIP and destIP seg-
ments that overlap with the time interval [d1, d2] are considered for process-
ing and their corresponding index nodes are read from disk. Since this is a
range aggregation query, all the considered segments will be loaded and pro-
cessed. If we consider the query “What is the number of unique hosts that host
X contacted in the interval [d1, d2]?” it can be expressed as follows: aggre-
gate(ﬁlter segs(d1, d2), sourceIP = X, destIP ). For this query the number
of relevant segments can be reduced even more by discarding the ones that do
NetStore: An Eﬃcient Storage Infrastructure
289
not overlap with the time interval [d1, d2], as well as the ones that don’t hold
the value X for sourceIP by checking corresponding index nodes statistics. If
the value X represents the IP address of an internal node, then the internal IPs
index will be used to retrieve all the positions where the value X occurs in the
sourceIP column. Then a count operation is performed of all the unique destIP
addresses corresponding to the positions. Note that by using internal IPs index,
the data of sourceIP column is not touched. The only information loaded in
memory is the positions list of IP X as well as the segments in column destIP
that correspond to those positions.
4 Evaluation
In this section we present an evaluation of NetStore. We designed and imple-
mented NetStore using the Java programming language on the FreeBSD 7.2-
RELEASE platform. For all the experiments we used a single machine with 6
GB DDR2 RAM, two Quad-Core 2.3 Ghz CPUs, 1TB SATA-300 32 MB Buﬀer
7200 rpm disk with a RAID-Z conﬁguration. We consider this machine represen-
tative of what a medium scale enterprise will use as a storage server for network
ﬂow records.
For experiments we used the network ﬂow data captured over a 24 hour period
of one weekday at our campus border router. The size of raw text ﬁle data was
about 8 GB, 62,397,593 network ﬂow records. For our experiments we considered
only 12 attributes for each network ﬂow record, that is only the ones that were
meaningful for the queries presented in this paper. Table 1 shows the attributes
used as well as the types and the size for each attribute. We compared NetStore’s
performance with two open source RDBMS, a row-store, PostgreSQL [13] and a
column-store, LucidDB [11]. We chose PostgreSQL over other open source sys-
tems because we intended to follow the example in [6] which uses it for similar
tasks. Additionally we intended to make use of the partial index support for in-
ternal IPs that other systems don’t oﬀer in order to compare the performance of
our inverted IPs index. We chose LucidDB as the column-store to compare with
as it is, to the best of our knowledge, the only stable open source column-store
that yields good performance for disk resident data and provides reasonable in-