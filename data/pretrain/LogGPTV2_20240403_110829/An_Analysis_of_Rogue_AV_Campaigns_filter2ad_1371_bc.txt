Figure 7 illustrates the overall performance of each judge separately. The judges’ cor-
rectness varies greatly from 0% up to 90%. This variability can be attributed to the fact
that each judge interprets the same observed feature differently. For example, since VM-
Sim uses real user actions as templates to drive the simulation, it is able to include ad-
vanced “humanized” actions inside simulations, such as errors in typing (e.g., invalid
typing of a URL that is subsequently corrected), TAB usage for navigating among form
ﬁelds, auto-complete utilization, and so forth. However, the same action (e.g., TAB us-
age for navigating inside the ﬁelds of a web form) is assumed by some judges as a
real human indicator, while some others take it as a simulation artifact. This observa-
tion is clearly a “toss up” as a distinguishing feature. An important observation is that
even highly successful judges could not achieve a 100% accuracy rate. This indicates
that given a diverse and plentiful supply of decoys, our system will be believable at
some time. In other words, given enough decoys, BotSwindler will eventually force the
malware to reveal itself. We note that there is a “bias” towards the successful identi-
ﬁcation of bogus videos compared to real videos. This might be due to the fact that
most of the judges guess “simulated” when unsure, due to the nature of the experiment.
Despite this bias, results indicate that simulations are highly believable by humans. In
cases where they may not be, it is important to remember that the task of fooling hu-
mans is far harder than tricking malware, unless the adversary has solved the AI prob-
lem and designed malware to answer the Turing Test. Furthermore, if attackers have to
bogus
real
130
B.M. Bowen et al.
)
%
(
t
c
e
r
r
o
c
 100
 80
 60
 40
 20
 0
G
P
P
W
m
C
iti
b
a
n
k
)
%
(
t
c
e
r
r
o
c
 100
 80
 60
 40
 20
 0
1234567891
1
1
1
1
1
1
1
1
1
2
2
2
2
2
2
m
m
a
il
a
y
p
a
y
p
a
l
video pairs
a
l 
#
2
o
r
d
p
a
d
e
a
n
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
judge #id
e
a
n
Fig. 6. Decoy Turing Test
simulated
results: real vs.
Fig. 7. Judges’ overall performance
spend their time looking at the actions one by one to determine if they are real or not, we
consider BotSwindler a success because that approach does not scale for the adversary.
4.3 Virtual Machine Veriﬁcation Overhead
The overhead of the VMV in BotSwindler is controlled by several parameters including
the number of pixels in the screen selections, the size of the search area for a selection,
the number of possible states to verify at each point of time, and the number of pixels re-
quired to match for positive veriﬁcation. A key observation responsible for maintaining
low overhead is that the majority of the time, the VMV process results in a negative ver-
iﬁcation, which is typically obtained by inspecting a single pixel for each of the possible
states to verify. The performance cost of this result is simply that of a few instructions
to perform pixel comparisons. The worst case occurs when there is a complete match
in which all pixels are compared (i.e., all pixels up to some predeﬁned threshold). This
may result in thousands of instructions being executed (depending on the particular
screen selection chosen by the simulation creator), but it only happens once during the
veriﬁcation of a particular state. It is possible to construct a scenario in which worse
performance is obtained by choosing screen selections that are common (e.g., found on
the desktop) and almost completely matches but results in a negative VMV outcome. In
this case, obtaining a negative VMV result may cost hundreds of thousands of CPU cy-
cles. In practice, we have not found this scenario to occur; moreover, it can be avoided
by the simulation creator.
In Table 1, we present the analysis of the overhead of QEMU5 with the BotSwindler
extensions. The table presents the amount of time, in seconds, to load web pages on
our test machine (2.33GHz Intel Core 2 Duo with 2GB 667MHz DDR2 SDRAM) with
idle user activity. The results include the time for a native OS, an unmodiﬁed version
of QEMU (version 0.10.5) running Windows XP, and QEMU running Windows XP
with the VMV processing a veriﬁcation task (a particular state deﬁned by thousands of
pixels).
5 QEMU does not support graphics acceleration, so all processing is performed by the CPU.
BotSwindler: Tamper Resistant Injection of Believable Decoys
131
Table 1. Overhead of VMV with idle user
Table 2. Overhead of VMV with active user
Native OS
Min. Max. Avg. STD
.56 .06
.48
.55
.62 .07
.64 .07
QEMU w/VMV .52
.70
.95
.77
QEMU
Native OS
Min. Max. Avg. STD
.56 .06
.50
.57
.71 .07
.71 .06
QEMU w/VMV .53
.72
.96
.89
QEMU
In Table 2, we present the results from a second set of tests where we introduce rapid
window movements forcing the screen to constantly be refreshed. By doing this, we
ensure that the BotSwindler VMV functions are repeatedly called. The results indicate
that the rapid movements do not impact the performance on the native OS, whereas in
the case of QEMU they result in a ∼15% slowdown. This is likely because QEMU does
not support graphics acceleration, so all processing is performed by the CPU. The time
to load the web pages on QEMU with the VMV is essentially the same as without it.
This is true whether the tests are done with or without user activity. Hence, we conclude
that the performance overhead of the VMV is negligible.
4.4 PayPal Decoy Analysis
The PayPal monitor relies on the time differences recorded by the BotSwindler moni-
toring server and the PayPal service for a user’s last login. The last login time displayed
by the PayPal service is presented with a granularity of minutes. This imposes the con-
straint that we must allow for at least one minute of time between the PayPal monitor,
which operates with a granularity of seconds, and the PayPal service times. In addition,
we have observed that there are slight deviations between the times that can likely be at-
tributed to time synchronization issues and latency in the PayPal login process. Hence,
it is useful to add additional time to the threshold used for triggering alerts (we make it
longer than the minimum resolution of one minute).
Another parameter that inﬂuences the detection rate is the frequency at which the
monitor polls the PayPal service. Unfortunately, it is only possible to obtain the last lo-
gin time from the PayPal service, so we are limited to detecting a single attack between
polling intervals. Hence, the more frequent the polling, the greater the number of attacks
on a single account that we can detect and the quicker an alert can be generated after an
account has been exploited. However, the fact that we must allow for a minimum of one
minute between the PayPal last login time and the BotSwindler monitor’s, implies we
must consider a signiﬁcant tradeoff. The more frequent the polling, the greater the like-
lihood is for false negatives due to the one minute window. In particular, the likelihood
of a false negative is:
PF N =
length of window
polling interval
.
Table 3 provides examples of false negative likelihoods for different polling frequencies
using a 75 second threshold. These rates assume only a single attack per polling interval.
We rely on this threshold because we experimentally determined that it exhibits no
false positives. For the experiments described in Sect. 4.5, we use the 1 hour polling
frequency because we believe it provides an adequate balance (the false negative rate is
relatively low and the alerts are generated quickly enough).
132
B.M. Bowen et al.
Table 3. PayPal decoy false negative likelihoods
Polling Frequency False Negative Rate
.5 hour
1 hour
24 hour
.0417
.0208
.0009
4.5 Detecting Real Malware with Bait Exploitation
To demonstrate the efﬁcacy of our approach, we conducted two experiments using
BotSwindler against crimeware found in the wild. For the ﬁrst experiment, we injected
Gmail and PayPal decoys, and for second, we used decoy banking logins. The exper-
iments relied on Zeus because it is the largest botnet in operation. Zeus is sold as a
crimeware kit allowing malicious individuals to create and conﬁgure their own unique
botnets. Hence, it functions as a payload dissemination framework with a large number
of variants. Despite the abundant supply of Zeus variants, many are no longer functional
because they require active command and control servers to effectively operate. This re-
quirement gives Zeus a relatively short life span because these services become inactive
(e.g., they are on a compromised host that is discovered and sanitized). To obtain active
Zeus variants, we subscribed to an active feed of binaries at the Swiss Security blog,
which has a Zeus Tracker [6] and Offensive Computing6.
In our ﬁrst experiment, we used 5 PayPal decoys and 5 Gmail decoys. We deliber-
ately limited the number of accounts to avoid upsetting the providers and having our
access removed. After all, the use of these accounts as decoys requires us to contin-
uously poll the servers for unauthorized logins as described in Sect. 4.4, which could
become problematic with a large number of accounts. To further limit the load on the
services, we limited the BotSwindler monitoring to once every hour.
We constructed a BotSwindler sandbox environment so that any access to
www.paypal.com would be routed to a decoy website that replicates the look-and-
feel of the true PayPal site. This was done for two reasons. First, if BotSwindler ac-
cessed the real PayPal site, it would be more difﬁcult for the monitor to differentiate
access by the simulator from an attacker, which could lead to false positives. More im-
portantly, hosting a phony PayPal site enabled us to control attributes of the account
(e.g., balance and veriﬁed status) to make them more enticing to crimeware. We lever-
aged this ability to give each of our decoy accounts unique balances in the range of
$4,000 - $20,000 USD, whereas in the true PayPal site, they have no balance. In the
case of Gmail, the simulator logs directly into the real Gmail site, since it does not
interfere with monitoring of the accounts (we can ﬁlter on IP) and there is no need to
modify account attributes.
The decoy PayPal environment was setup by copying and slightly modifying the
content from www.paypal.com to a restricted lab machine with internal access only.
The BotSwindler host machine was conﬁgured with NAT rules to redirect any access
directed to the real PayPal website to our test machine. The downside of using this setup
is that we lack a certiﬁcate to the www.paypal.com domain signed by a trusted
6 http://www.offensivecomputing.net
BotSwindler: Tamper Resistant Injection of Believable Decoys
133
Certiﬁcate Authority. To mitigate the issue, we used a self-signed certiﬁcate that is
installed as a trusted certiﬁcate on the guest. Although this is a potential distinguishing
feature that can be used by malware to detect the environment, existing malware is
unlikely to check for this. Hence, it remains a valid approach for demonstrating the use
of decoys to detect malware in this proof of concept experiment. The banking logins
used in the second experiment do not have this limitation, but they may not have the
same broad appeal to attackers that make PayPal accounts so useful.
The experiments worked by automating the download and installation of individ-
ual malware samples using a remote network transfer. For each sample, BotSwindler
conducted various simulations designed from the VMSim language to contain inject
actions, as well as other cover actions. The simulator was run for approximately 20 min-
utes on each of the 116 binaries that were tested with the goal of determining whether
attackers would take and exploit the bait credentials. Over the course of ﬁve days of
monitoring, we received thirteen alerts from the PayPal monitor and one Gmail alert.
We ended the study after ﬁve days because the results obtained during this period were
enough to convince us the system worked7. The Gmail alert was for a Gmail decoy ID
that was also associated with a decoy PayPal account; the Gmail username was also a
PayPal username and both credentials were used in the same workﬂow (we associate
multiple accounts to make a decoy identity more convincing). Given that we received
an alert for the PayPal ID as well, it is likely both sets of credentials were stolen at the
same time. Although the Gmail monitor does provide IP address information, we could
not obtain it in this case. This particular alert was generated because Gmail detected
suspicious activity on the account and locked it, so the intruder never got in.
We attribute the fewer Gmail alerts to the economics of the black market. Although
Gmail accounts may have value for activities such as spamming, they can be purchased
by the thousands for very little cost8 and there are inexpensive tools that can be used
to create them automatically. Hence, attackers have little incentive to build or purchase
a malware mechanism, and to ﬁnd a way to distribute it to many victims, only to net a
bunch of relatively valueless Gmail accounts. On the other hand, high-balance veriﬁed
PayPal accounts represent something of signiﬁcant value to attackers. The 2008 Syman-
tec Global Internet Security Threat Report [27] lists bank accounts as being worth $10-
$1000 on the underground market, depending on balance.
For the PayPal alerts that were generated, we found that some alerts were triggered
within an hour after the corresponding decoy was injected, where other alerts occurred
days after. We believe this variability to be a consequence of attackers manually testing
the decoys rather than testing through some automatic means. In regards to the quantity
of alerts generated, there are several possible explanations that include:
– as a result of the one-to-many mapping between decoys and binaries, the decoys
are exﬁltrated to many different dropzones where they are then tested
– the decoy accounts are being sold and resold in the underground market where ﬁrst
the dropzone owner checks them, then resell them to others, who then resell them
to others who check them
7 We ended the study after 5 days, but a recent examination of the monitoring logs revealed
alerts still being generated months after.
8 We have found Gmail accounts being sold at $20 per 1000.
134
B.M. Bowen et al.
While the second case is conceivable for credentials of true value, our decoys lack
any balance. Hence, we believe that once this fact is revealed to the attacker during
the initial check, the attackers have no reason to keep the credentials or recheck them
(lending support for the ﬁrst case). We used only ﬁve PayPal accounts with a one-to-
many mapping to binaries, making it impossible to know exactly which binary triggered
the alert and which scenario actually occurred. We also note that the number of actual
attacks may be greater than what was actually detected. The PayPal monitor polls only
once per hour, so we do not know when there are multiple attacks in a single hour.
Hence, the number of attacks we detected is a lower bound. In addition, despite our
efforts to get active binaries, many were found to be inactive, some cause the system to
fail, and some have objectives other than stealing credentials.
In the second experiment, we relied on several bank accounts containing balances