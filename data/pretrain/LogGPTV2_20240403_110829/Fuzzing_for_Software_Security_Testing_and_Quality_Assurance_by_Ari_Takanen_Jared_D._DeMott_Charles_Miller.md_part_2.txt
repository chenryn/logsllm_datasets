### Chapter 7: In-Depth Fuzzing Techniques

#### 7.6 Golden FTP Server Test Case
- **7.6.10** Test Case: Golden FTP Server
- **7.6.11** Results
- **7.6.12** Conclusions on EFS

#### 7.7 In-Memory Fuzzing
- **7.7.1** Implementation of In-Memory Fuzzer
- **7.7.2** Instrumentation
- **7.7.3** The Pin API
- **7.7.4** Register Example
- **7.7.5** Pros and Cons
- **7.7.6** Improvements by Dynamic Symbolic Execution

#### 7.8 Distributed Fuzzing
- **7.8.1** Distributed Fuzzing: Google’s ClusterFuzz
- **7.8.2** Distributed Fuzzing: DeMott’s ClusterFuzz

#### 7.9 Summary

### Chapter 8: Fuzzer Comparison

#### 8.1 Fuzzing Life Cycle
- **8.1.1** Identifying Interfaces
- **8.1.2** Input Generation
- **8.1.3** Sending Inputs to the Target
- **8.1.4** Target Monitoring
- **8.1.5** Exception Analysis
- **8.1.6** Reporting

#### 8.2 Evaluating Fuzzers
- **8.2.1** Retrospective Testing
- **8.2.2** Simulated Vulnerability Discovery
- **8.2.3** Code Coverage
- **8.2.4** Caveats

#### 8.3 Introducing the Fuzzers
- **8.3.1** GPF
- **8.3.2** TAOF
- **8.3.3** ProxyFuzz
- **8.3.4** Mu-4000
- **8.3.5** Codenomicon Defensics
- **8.3.6** beSTORM
- **8.3.7** Application-Specific Fuzzers
- **8.3.8** What is Missing

#### 8.4 The Targets

#### 8.5 The Bugs
- **8.5.1** FTP Bug 0
- **8.5.2** FTP Bugs 2, 16

#### 8.6 Results
- **8.6.1** FTP
- **8.6.2** SNMP
- **8.6.3** DNS

#### 8.7 A Closer Look at the Results
- **8.7.1** FTP
- **8.7.2** SNMP
- **8.7.3** DNS

#### 8.8 General Conclusions
- **8.8.1** The More Fuzzers, the Better
- **8.8.2** Generational-Based Approach is Superior
- **8.8.3** Initial Test Cases Matter
- **8.8.4** Protocol Knowledge Helps
- **8.8.5** Real Bugs
- **8.8.6** Does Code Coverage Predict Bug Finding?
- **8.8.7** How Long to Run Fuzzers with Random Elements
- **8.8.8** Random Fuzzers Find Easy Bugs First

#### 8.9 Summary

### Chapter 9: Fuzzing Case Studies

#### 9.1 Enterprise Fuzzing
- **9.1.1** Firewall Fuzzing
- **9.1.2** VPN Fuzzing

#### 9.2 Carrier and Service Provider Fuzzing
- **9.2.1** VoIP Fuzzing
- **9.2.2** WiFi Fuzzing

#### 9.3 Application Developer Fuzzing
- **9.3.1** Command-Line Application Fuzzing
- **9.3.2** File Fuzzing
- **9.3.3** Web Application Fuzzing
- **9.3.4** Browser Fuzzing

#### 9.4 Network Equipment Manufacturer Fuzzing
- **9.4.1** Network Switch Fuzzing
- **9.4.2** Mobile Phone Fuzzing

#### 9.5 Industrial Automation Fuzzing

#### 9.6 Black-Box Fuzzing for Security Researchers
- **9.6.1** Select Target
- **9.6.2** Enumerate Interfaces
- **9.6.3** Choose Fuzzer/Fuzzer Type
- **9.6.4** Choose a Monitoring Tool
- **9.6.5** Carry Out the Fuzzing
- **9.6.6** Postfuzzing Analysis

#### 9.7 Summary

### About the Authors

### Index

---

### Foreword from the First Edition

It was a dark and stormy night in Madison, Wisconsin, in the fall of 1988. I was logged into the Unix systems in my office via a dial-up phone line over a 1200 baud modem. The heavy rain caused noise on the line, which interfered with my ability to type commands. Surprisingly, this noise also seemed to cause programs to crash, particularly common Unix utilities.

This unexpected behavior sparked a systematic investigation to understand the extent and causes of these crashes. I added a testing project to the list of suggested topics for the graduate Advanced Operating Systems course I was teaching that semester. I named this type of testing "fuzz" to evoke the feeling of random, unstructured data.

Three groups attempted the fuzz project that semester, but only Lars Fredriksen and Bryan So succeeded. They found that between 25–33% of the utility programs on seven Unix variants could be crashed or hung. The results were published, and the source code, raw test results, and suggested bug fixes were made public.

Over the years, we expanded our tests to more Unix systems, command-line utilities, GUI programs, and eventually Windows and MacOS. Each time, we found numerous bugs and published all our results. Despite initial resistance from the testing and software engineering community, fuzz testing has become a crucial tool in evaluating software reliability and security.

Barton Miller
Madison, Wisconsin
April 2008

---

### Foreword to the Second Edition

As a software engineering manager focused on Microsoft Office Security, I have found fuzzing to be one of the most impactful techniques for improving product security. Fuzzing can be fully automated, results are often easily reproducible, and it does not require deep security expertise to find and fix flaws. For over a decade, we have been using fuzzing as part of our overall security effort.

Microsoft Office, with its complex codebase and legacy file formats, has benefited significantly from fuzzing. Here are some key lessons learned:

- **Experimentation**: We experimented with different fuzzing approaches and found that even simple fuzzers can uncover significant issues.
- **More Iterations = More Bugs**: Nondeterministic fuzzing algorithms yield more bugs with more iterations. We leveraged available hardware and automation labs to run millions of fuzz tests.
- **Distribute Runs, Centralize Investigation**: We developed a system to distribute fuzzing runs across multiple machines while centralizing the investigation of results. This system uses internal symbol files to detect and manage similar crashes across different builds and versions.

Fuzzing is not a panacea, but it is an effective way to find important sets of bugs that are difficult to identify through other methods.

David Conger and his team at Microsoft Access built a system to simplify running fuzzers on available hardware. This system manages fuzzing jobs, collects results, and tracks data across multiple runs, making it easier to identify and address crashes.

We continue to leverage fuzzing as part of our Secure Development Lifecycle, along with other security practices, to ensure the robustness and security of our products.

---

This optimized version aims to make the text more structured, clear, and professional.