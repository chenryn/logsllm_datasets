7.6.10 Test Case: Golden FTP Server 231
7.6.11 Results 231
7.6.12 Conclusions on EFS 233
7.7 In-Memory Fuzzing 235
7.7.1 Implementation of In-Memory Fuzzer 235
7.7.2 Instrumentation 236
7.7.3 The Pin API 237
7.7.4 Register Example 237
7.7.5 Pros and Cons 240
7.7.6 Improvements by Dynamic Symbolic Execution 240
7.8 Distributed Fuzzing 242
7.8.1 Distributed Fuzzing: Google’s ClusterFuzz 243
7.8.2 Distributed Fuzzing: DeMott’s ClusterFuzz 244
7.9 Summary 248
ChApTEr 8
Fuzzer Comparison 249
8.1 Fuzzing Life Cycle 249
8.1.1 Identifying Interfaces 249
8.1.2 Input Generation 249
8.1.3 Sending Inputs to the Target 250
8.1.4 Target Monitoring 250
8.1.5 Exception Analysis 251
8.1.6 Reporting 251
8.2 Evaluating Fuzzers 251
8.2.1 Retrospective Testing 252
8.2.2 Simulated Vulnerability Discovery 253
8.2.3 Code Coverage 253
8.2.4 Caveats 254
8.3 Introducing the Fuzzers 254
8.3.1 GPF 254
8.3.2 TAOF 255
8.3.3 ProxyFuzz 255
8.3.4 Mu-4000 256
6760 Book.indb 12 12/22/17 10:50 AM
Contents xiii
8.3.5 Codenomicon Defensics 256
8.3.6 beSTORM 256
8.3.7 Application-Specific Fuzzers 257
8.3.8 What is Missing 257
8.4 The Targets 257
8.5 The Bugs 258
8.5.1 FTP Bug 0 258
8.5.2 FTP Bugs 2, 16 259
8.6 Results 259
8.6.1 FTP 260
8.6.2 SNMP 261
8.6.3 DNS 262
8.7 A Closer Look at the Results 262
8.7.1 FTP 263
8.7.2 SNMP 265
8.7.3 DNS 269
8.8 General Conclusions 270
8.8.1 The More Fuzzers, the Better 270
8.8.2 Generational-Based Approach is Superior 270
8.8.3 Initial Test Cases Matter 271
8.8.4 Protocol Knowledge Helps 272
8.8.5 Real Bugs 273
8.8.6 Does Code Coverage Predict Bug Finding 273
8.8.7 How Long to Run Fuzzers with Random Elements 274
8.8.8 Random Fuzzers Find Easy Bugs First 276
8.9 Summary 276
ChApTEr 9
Fuzzing Case Studies 279
9.1 Enterprise Fuzzing 280
9.1.1 Firewall Fuzzing 281
9.1.2 VPN Fuzzing 284
9.2 Carrier and Service Provider Fuzzing 285
9.2.1 VoIP Fuzzing 285
9.2.2 WiFi Fuzzing 287
9.3 Application Developer Fuzzing 288
9.3.1 Command-Line Application Fuzzing 289
9.3.2 File Fuzzing 289
9.3.3 Web Application Fuzzing 290
9.3.4 Browser Fuzzing 292
9.4 Network Equipment Manufacturer Fuzzing 293
9.4.1 Network Switch Fuzzing 293
9.4.2 Mobile Phone Fuzzing 294
9.5 Industrial Automation Fuzzing 295
9.6 Black-Box Fuzzing for Security Researchers 300
9.6.1 Select Target 300
6760 Book.indb 13 12/22/17 10:50 AM
xiv Contents
9.6.2 Enumerate Interfaces 301
9.6.3 Choose Fuzzer/Fuzzer Type 301
9.6.4 Choose a Monitoring Tool 303
9.6.5 Carry Out the Fuzzing 303
9.6.6 Postfuzzing Analysis 304
9.7 Summary 305
About the Authors 307
Index 309
6760 Book.indb 14 12/22/17 10:50 AM
Foreword from the First Edition
It was a dark and stormy night. Really.
Sitting in my apartment in Madison in the Fall of 1988, there was a wild mid-
west thunderstorm pouring rain and lighting up the late night sky. That night, I
was logged on to the Unix systems in my office via a dial-up phone line over a 1200
baud modem. With the heavy rain, there was noise on the line and that noise was
interfering with my ability to type sensible commands to the shell and programs
that I was running. It was a race to type an input line before the noise overwhelmed
the command.
This fighting with the noisy phone line was not surprising. What did surprise
me was the fact that the noise seemed to be causing programs to crash. And more
surprising to me was the programs that were crashing—common Unix utilities that
we all use everyday.
The scientist in me said that we need to make a systematic investigation to try
to understand the extent of the problem and the cause.
That semester, I was teaching the graduate Advanced Operating Systems course
at the University of Wisconsin. Each semester in this course, we hand out a list of
suggested topics for the students to explore for their course project. I added this
testing project to the list.
In the process of writing the description, I needed to give this kind of testing a
name. I wanted a name that would evoke the feeling of random, unstructured data.
After trying out several ideas, I settled on the term “fuzz.”
Three groups attempted the fuzz project that semester and two failed to achieve
any crash results. Lars Fredriksen and Bryan So formed the third group, and were
more talented programmers and most careful experiments; they succeeded well
beyond my expectations. As reported in the first fuzz paper1, they could crash or
hang between 25–33% of the utility programs on the seven Unix variants that
they tested.
However, the fuzz testing project was more than a quick way to find program
failures. Finding the cause of each failure and categorizing these failures gave the
results deeper meaning and more lasting impact. The source code for the tools and
scripts, the raw test results, and the suggested bug fixes were all made public. Trust
and repeatability were crucial underlying principles for this work.
In the following years, we repeated these tests on more and varied Unix systems
for a larger set of command-line utility programs and expanded our testing to GUI
programs based on the then-new X-window system2. Windows followed several
1 ftp://ftp.cs.wisc.edu/paradyn/technical_papers/fuzz.pdf.
2 ftp://ftp.cs.wisc.edu/paradyn/technical_papers/fuzz-revisited.pdf.
xv
6760 Book.indb 15 12/22/17 10:50 AM
xvi Foreword from the First Edition
years later3 and, most recently, MacOS4. In each case, over the span of the years,
we found a lot of bugs and, in each case, we diagnosed those bugs and published
all of our results.
In our more recent research, as we have expanded to more GUI-based applica-
tion testing, we discovered that classic 1983 testing tool, “The Monkey” used on
the earlier Macintosh computers5. Clearly a group ahead of their time.
In the process of writing our early fuzz papers, we came across strong resis-
tance from the testing and software engineering community. The lack of a formal
model and methodology and undisciplined approach to testing often offended
experienced practioners in the field. In fact, I still frequently come across hostile
attitudes to this type of “stone axes and bear skins” (my apologies to Mr. Spock)
approach to testing.
My response was always simple: “We’re just trying to find bugs.” As I have
said many times, fuzz testing is not meant to supplant more systematic testing. It is
just one more tool, albeit, and an extremely easy one to use, in the tester’s toolkit.
As an aside, note that the fuzz testing has not ever been a funded research effort
for me; it is a research advocation rather than a vocation. All the hard work has
been done by a series of talented and motivated graduate students in our Computer
Sciences Department. This is how we have fun.
Fuzz testing has grown into a major subfield of research and engineering, with
new results taking it far beyond our simple and initial work. As reliability is the
foundation of security, so has it become a crucial tool in security evaluation of
software. Thus, the topic of this book is both timely and extremely important.
Every practitioner who aspires to write safe and secure software needs to add these
techniques to their bag of tricks.
Barton Miller
Madison, Wisconsin
April 2008
Operating System Utility program reliability—The Fuzz Generator
The goal of this project is to evaluate the robustness of various Unix utility programs,
given an unpredictable input stream. This project has two parts. First, you will build
a “fuzz” generator. This is a program that will output a random character stream.
Second, you will take the fuzz generator and use it to attack as many Unix utilities
as possible, with the goal of trying to break them. For the utilities that break, you
will try to determine what type of input caused the break.
3 ftp://ftp.cs.wisc.edu/paradyn/technical_papers/fuzz-nt.pdf.
4 ftp://ftp.cs.wisc.edu/paradyn/technical_papers/fuzz-MacOS.pdf.
5 Hertzfeld, A., Revolution in The Valley: The Insanely Great Story of How the Mac Was Made,
Sebastopol, California: O’Reilly Media, October 2011, ISBN13: 9781449316242.
6760 Book.indb 16 12/22/17 10:50 AM
Foreword from the First Edition xvii
The program
The fuzz generator will generate an output stream of random characters. It will
need several options to give you flexibility to test different programs. Below is the
start for a list of options for features that fuzz will support. It is important when
writing this program to use good C and Unix style, and good structure, as we hope
to distribute this program to others.
–p only the printable ASCII characters
–a all ASCII characters
–0 include the null (0 byte) character
–l generate random length lines (\\n terminated strings)
–f name record characters in file “name’’
–d nnn delay nnn seconds following each character
–r name replay characters in file “name’’ to output
The Testing
The fuzz program should be used to test various Unix utilities. These utilities include
programs like vi, mail, cc, make, sed, awk, sort, etc. The goal is to first see if the
program will break and second to understand what type of input is responsible for
the break.
6760 Book.indb 17 12/22/17 10:50 AM
6760 Book.indb 18 12/22/17 10:50 AM
Foreword to the Second Edition
As a software engineering manager focused on Microsoft Office Security, I am
constantly weighing which engineering investment should be made to provide maxi-
mum protection to customers. Similar to other decisions, there are many trade-
offs and we want to optimize for investments that make a significant difference
to our customers’ security. Fuzzing has been one of the most impactful techniques
for improving our product’s security. Unlike other approaches to finding security
bugs, fuzzing can be fully automated, results are often easily reproducible, and it
does not require deep security expertise to find and fix the flaws. For more than
10 years we have been fuzzing Office and continue to heavily leverage fuzzing as
part of our overall security effort.
Microsoft Office is made of code written over several decades and is relatively
complex. Many of the file formats and initial parsing code originated at a time when
most computers were not connected to other computers and the most common way
to share files was by physically sharing a floppy disk with someone. Some of the
early software testers have shared with me that intentionally corrupting a document
and expecting Office to properly handle that was not considered a valid scenario.
Obviously, a lot has changed since then. Today, we run automation in our lab to
do this very thing millions of times within a few hours.
Before going into some of the key investments we’ve made and lessons learned
from those, I’d like to point out that fuzzing is only a piece of our plan. Designing
code with security in mind, ensuring software engineers writing code follow secure
coding practices, and other pieces of the Secure Development Lifecycle (http://www.
microsoft.com/security/sdl) continue to be important. Fuzzing isn’t a panacea; you
won’t find all security defects through fuzzing. However, it has been an effective
way to find an important set of bugs that were either difficult or not found through
other methods.
Below are eight themes of learning which we’ve discovered through our fuzz-
ing investments.
• Experimentation. When we began our thinking about making fuzzing invest-
ments, there was some debate that it was the right investment and if it would
yield results in an efficient way. Fuzzing, especially in our initial stages, was
pretty dumb (the tools didn’t know the data formats being fuzzed). Several
argued deeper code review and other approaches that leveraged our knowledge
of the code and how data could/should be defined would be a better invest-
ment. We experimented. We made a few initial investments and continued
to make tweaks and reevaluate if fuzzing would help us achieve our goals.
Within a few months, there was clear agreement that fuzzing was yielding a
xix
6760 Book.indb 19 12/22/17 10:50 AM
xx Foreword to the Second Edition
high return on investment. There were many reasons for this. We had numer-
ous examples of security issues that our fuzzing efforts had uncovered. Many
of these examples were in areas that we had already leveraged static analysis
tools and manual code reviews so how/why was fuzzing finding these? Unlike
static analysis/code review, fuzzing actually executes the targeted application
or service code including all of its dependencies. Unlike humans looking at a
specific situation, format agnostic fuzzers aren’t going to convince themselves
not to try a test case or that API documentation means the code works in a
specific way. The fuzzer blindly tries code and monitors the execution at all
layers of the stack. This includes code dependencies that code reviewers and
static analysis tools often overlook. During the Office 2010 product cycle, we
found and fixed more than 1,800 bugs identified through fuzzing. We also
identified bugs in libraries we used in our code and worked with the authors
to fix these external bugs.
We have continued to leverage experimentation in our fuzzing work. Some-
times we think a fuzzer is too simplistic or we already have good coverage and
it might be a waste to spend time running it. We learned it is good to give it
a try and monitor if unique issues are being found in an efficient way. Char-
lie Miller certainly showed that a very simplistic fuzzer can find real results
at his CanSecWest 2012 talk entitled, “Babysitting an Army of Monkeys.”
Internally, we sometimes do head-to-head matches between fuzzers when
people feel certain fuzzing techniques would be more effective than others.
• More iterations = more bugs. Similar to other people’s discoveries, we found
that with nondeterministic fuzzing algorithms the more fuzz test iterations
performed, the more likely we were to find bugs. When we initially started
fuzzing Office, most engineers had several test machines in their offices. We
convinced a large number of people to run fuzzers on those machines to more
quickly get fuzzing results. Basically, we had people running standalone fuzz-
ers on their machines when they weren’t in use (nights, weekends, lunch, when
they were in meetings, etc.) While this did allow us to quickly get coverage,
investigation of the results was messy and inefficient. We later improved upon
this. We also now leverage the Office automation lab, which is mostly used
for functional automation to complete a large number of fuzzing tests quickly.
• Distribute runs; centralize investigation results. As noted earlier, we had a
large number of machines performing fuzz tests, but investigating the results
wasn’t streamlined. We made several improvements to address this. First, we
wanted a way to make it simple for anyone to run a fuzzer on their available
hardware—so simple that people without programming or testing experience
could do it. David Conger from the Microsoft Access team was already work-
ing to address this need within the Access team. He built a system which was
comprised of a client executable that would run on someone’s machine and
a few server components that the clients would talk to. The server manages
all fuzzing jobs, collects results, and most importantly, keeps track of data
across the many fuzzing runs. We leverage internal symbol files to detect if
crashes are identical or similar crashes across machines, different builds, and
versions of the product. This means that if a fuzzing crash is found on one
6760 Book.indb 20 12/22/17 10:50 AM
Foreword to the Second Edition xxi
machine and it is similar to a crash identified on another machine/build/ver-
sion, we know immediately. In many cases, we can automatically mark a crash