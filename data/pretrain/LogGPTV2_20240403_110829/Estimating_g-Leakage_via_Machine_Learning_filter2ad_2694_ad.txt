letpV
5.1 Representation of the results and metrics
We graphically represent the results of the experiment as box plots,
using one box for each size. More precisely, given a specific size,
n be the д-vulnerability estimation on the j-th validation set
ij
computed with a model trained over the i-th training set (where
i P t1, . . . , 5u and j P t1, . . . , 50u). Let Vд be the real д-vulnerability
of the system. We define the normalized estimation error δij and
the mean value δ of the δij’s as follows:
ij
n ´ Vд|
Vд
, with δ
def“ 1
250
(29)
5ÿ
50ÿ
δij .
i“1
j“1
def“ |pV
δij
In the graphs, the δij’s are reported next to the box corresponding
to the size, and δ is the black horizontal line inside the box.
Thanks to the normalization the δij’s allow to compare results
among different scenario and different levels of (real) д-vulnerability.
Also, we argue that the percentage of the error is more meaning-
ful than the absolute value. The interested reader, however, can
find in Appendix H also plots showing the estimations of the д-
vulnerability and their distance from the real value.
We also consider the following typical measures of precision:
gffe 1
gffe 1
250
250
5ÿ
5ÿ
i“1
50ÿ
50ÿ
j“1
i“1
j“1
dispersion def“
total error def“
pδij ´ δq2
,
2
ij .
δ
(30)
(31)
The dispersion is an average measure of how far the normalized
estimation errors are from their mean value when using same-size
training and validation sets. On the other hand, the total error is an
average measure of the normalized estimation error, when using
same-size training and validation sets.
In order to make a fair comparison between the two pre-processing
methods, intuitively we need to use training and validation sets of
“equivalent size”. For the validation part, since the “best” f function
has been already found and therefore we do not need any pre-
processing anymore, “equivalent size” just means same size. But
what does it mean, in the case of training sets built with different
pre-processing methods? Assume that we have a set of data Dm
coming from a third party collector (recall that m represents the
size of the set), and let D1
m1 be the result of the data pre-processing
on Dm. Now, let D2
m2 be the dataset obtained drawing samples
according to the channel pre-processing method. Should we im-
pose m2 “ m or m2 “ m1? We argue that the right choice is the
first one, because the amount of “real” data collected is m. Indeed,
D1
m1 is generated synthetically from Dm and cannot contain more
information about C than Dm, despite its larger size.
5.2 Learning algorithms
We consider two ML algorithms in the experiments: k-Nearest
Neighbors (k-NN) and Artificial Neural Networks (ANN). We have
made however a slight modification of k-NN algorithm, due to
the following reason: recall that, depending on the particular gain
function, the data pre-processing method might create many in-
stances where a certain observable y is repeated multiple times in
pair with different w’s. For the k-NN algorithm, a very common
choice is to consider a number of neighbors which is equivalent
to natural logarithm of the total number of training samples. In
particular, when the data pre-processing is applied, this means that
k “ logpm1q nearest neighbors will be considered for the classifi-
cation decision. Since logpm1q grows slowly with respect to m1, it
might happen that k-NN fails to find the subset of neighbors from
which the best remapping can be learned. To amend this problem,
we modify the k-NN algorithm in the following way: instead of
looking for neighbors among all the m1 samples, we only consider
a subset of l ď m1 samples, where each value y only appears once.
After the logplq neighbors have been detected among the l samples,
we select w according to a majority vote over the m1 tuples pw, yq
created through the remapping.
The distance on which the notion of neighbor is based depends
on the experiments. We have considered the standard distance
among numbers in the first and fourth experiments, the Euclidean
distance in the second one, and the Manhattan distance in the third
one, which is a stand choice for DP.
Concerning the ANN models, their specifics are in Appendix D.
Note that, for the sake of fairness, we use the same architecture for
both pre-processing methods, although we adapt number of epochs
and batch size to the particular dataset we are dealing with.
5.3 Frequentist approach
In the experiments, we will compare our method with the frequen-
tist one. This approach has been proposed originally in [9] for
estimating mutual information, and extended successively also to
min-entropy leakage [18]. Although not considered in the litera-
ture, the extension to the case of д-vulnerability is straightforward.
The method consists in estimating the probabilities that consti-
tute the channel matrix C, and then calculating analytically the
д-vulnerability on C. The precise definition is in Appendix E.
In [14] it was observed that in the case of the Bayes error the fre-
quentist approach performs poorly when the size of the observable
domain |Y| is large with respect to the available data. We want to
study whether this is the case also for д-vulnerability.
For the experiment on the multiple guesses the comparison is
illustrated in the next section. For the other experiments, because
of lack of space, we have reported it in the Appendix H.
5.4 Experiment 1: multiple guesses
We consider a system in which the secrets X are the integers be-
tween 0 and 9, and the observables Y are the integers between 0
and 15999. Hence |X| “ 10 and |Y| “ 16K. The rows of the channel
Figure 1: The channel of Experiment 1. The two curves rep-
resent the distributions PY|Xp¨|xq for two adjacent secrets:
x “ 5 and x “ 6.
C are geometric distributions centered on the corresponding secret:
(32)
Cxy “ PY|Xpy|xq “ λ expp´ν|rpxq ´ y|q ,
Figure 2: Multiple guesses scenario, comparison between
the frequentist and the ML estimations with data pre-
processing.
where:
‚ ν is a parameter that determines how concentrated around
y “ x the distribution is. In this experiment we set ν “ 0.002;
‚ r is an auxiliary function that reports X to the same scale of
Y, and centers X on Y. Here rpxq “ 1000 x ` 3499.5;
‚ λ “ eν´1{peν`1q is a normalization factor
Figure 1 illustrates the shape of Cxy, showing the distributions
PY|Xp¨|xq for two adjacent secrets x “ 5 and x “ 6. We consider
an adversary that can make two attempts to discover the secret (two-
tries adversary), and we define the corresponding gain function as
follows. A guess w P W is one of all the possible combinations of
2 different secrets from X, i.e., w “ tx0, x1u with x0, x1 P X and
x0 ‰ x1. Therefore |W| “
“ 45. The gain function д is then
1
0 otherwise .
2
дpw, xq “
`10
˘
#
if x P w
(33)
For this experiment we consider a uniform prior distribution π on
X. The true д-vulnerability for these particular ν and π, results to
be Vд “ 0.892. As training sets sizes we consider 10K, 30K and 50K,
and 50K for the validation sets.
5.4.1 Data pre-processing. (cfr. Section 4.1). The plot in Figure 3
shows the performances of the k-NN and ANN models in terms of
normalized estimation error, while Figure 2 shows the comparison
with the frequentist approach. As we can see, the precision of
the frequentist method is much lower, thus confirming that the
trend observed in [14] for the Bayes vulnerability holds also for
д-vulnerability.
It is worth noting that, in this experiment, the
pre-processing of each sample px, yq creates 9 samples (matching
y with each possible w P W such that w “ tx, x1u with x1 ‰ x).
This means that the sample size of the pre-processed sets is 9 times
the size of the original ones. For functions д representing more
than 2 tries this pre-processing method may create training sets too
large. In the next section we will see that the alternative channel
pre-processing method can be a good compromise.
5.4.2 Channel pre-processing. (cfr. Section 4.2) The results for han-
nel pre-processing are reported in Figure 4. As we can see, the
estimation is worse than with data pre-processing, especially for
the k-NN algorithm. This was to be expected, since the random
Figure 3: Multiple guesses scenario, magnification of the
part of Figure 2 on the k-NN and ANN estimations.
sampling to match the effect of д introduces a further level of con-
fusion, as explained in Section 4.2. Nevertheless, these results are
still much better than the frequentist case, so it is a good alterna-
tive method to apply when the use of data pre-processing would
generate validation sets that are too large, which could be the case
when the matrix representing д contains large numbers with a small
common divider. Additional plots are provided in Appendix H.
Figure 4: Multiple guesses scenario, k-NN and ANN estima-
tion with channel pre-processing
8000825085008750900092509500975010000r(x)0.00.20.40.60.81.0PY|X(|x)1e3x = 5x = 6100003000050000Training set size0.00.10.20.30.40.5normalized estimation errorANNk-NNFrequentist100003000050000Training set size before preprocessing0.0000.0050.0100.0150.0200.0250.030normalized estimation errordispersion: 0.005total error: 0.022dispersion: 0.002total error: 0.026dispersion: 0.003total error: 0.006dispersion: 0.002total error: 0.012dispersion: 0.002total error: 0.004dispersion: 0.002total error: 0.007ANNk-NN 100003000050000Training set size0.050.100.150.200.25normalized estimation errordispersion: 0.012total error: 0.061dispersion: 0.003total error: 0.231dispersion: 0.016total error: 0.048dispersion: 0.004total error: 0.203dispersion: 0.006total error: 0.044dispersion: 0.003total error: 0.186ANNk-NN5.5 Experiment 2: location privacy
In this section we estimate the д-vulnerability of a typical system
for location privacy protection. We use data from the open Gowalla
dataset [1], which contains the coordinates of users’ check-ins.
In particular, we consider a square region in San Francisco, USA,
centered in (latitude, longitude) = (37.755, ´122.440), and with 5Km
long sides. In this area Gowalla contains 35162 check-ins.
We discretize the region in 400 cells of 250m long side, and we
assume that the adversary’s goal is to discover the cell of a check-in.
The frequency of the Gowalla check-ins per cell is represented by
the heat-map in Figure 5. From these frequencies we can directly
estimate the distribution representing the prior of the secrets [27].
0
00
0
0
0
00
1
0
1
2
00
1
0
0
0
0
0
00
0
0
1
2
4
2
1
0
0 0
0
0
0
0
00
1
0
1
2
00
1
0
0
0
0
0
00
0
Figure 6: The “diamond” shape created by the gain func-
tion around the real secret; the values represent the gains
assigned to each guessed cell w when x is the central cell.
Figure 7 and Figure 8 show the performance of k-NN and ANN
for both data and channel pre-processing. As expected, the data pre-
processing method is more precise than the channel pre-processing
one, although only slightly. The ANN model is also slightly better
than the k-NN in most of the cases.
Figure 5: Heat-map representing the Gowalla check-ins dis-
tribution in the area of interest; the density of check-ins in
each cell is reported in the color bar on the side
The channel C that we consider here is the optimal obfuscation
mechanism proposed in [37] to protect location privacy under a
utility constraint. We recall that the framework of [37] assumes
two loss functions, one for utility and one for privacy. The utility
loss of a mechanism, for a certain prior, is defined as the expected
utility loss of the noisy data generated according to the prior and
the mechanism. The privacy loss is defined in a similar way, except
that we allow the attacker to “remap” the noisy data so to maximize
the privacy loss. For our experiment, we use the Euclidean distance
as loss function for the utility, and the д function defined in the
next paragraph as loss function for the privacy. For further details
on the construction of the optimal mechanism we refer to [37].
We define X,Y and W to be the set of the cells. Hence |X| “
|Y| “ |W| “ 400. We consider a д function representing the
precision of the guess in terms of Euclidean distance: the closer
the guess is to the real location, the higher is the attacker’s gain.
Specifically, our д is illustrated in Figure 6, where the central cell
represents the real location x. For a generic “guess” cell w, the
number written in w represent дpw, xq. 3
In this experiment we consider training set sizes of 100, 1k and
10K samples respectively. After applying the data pre-processing
transformation, the size of the resulting datasets is approximately
18 times that of the original one. This was to be expected, since the
sum of the values of д in Figure 6 is 20. Note that this sum and the
increase factor in the dataset do not necessarily coincide, because
the latter is also influenced by the prior and by the mechanism.
3Formally, д is defined as дpw, xq “ tpγ expp´αdpw, xq{lqqs, where γ “ 4 is the
maximal gain, α “ 0.95 is a normalization coefficient to control the skewness of the
exponential, d is the euclidean distance and l “ 250 is the length of the cells’ side.
The symbol t¨s in this context represents the rounding to the closest integer operation.
Figure 7: Location privacy scenario, k-NN and ANN estima-
tion with data pre-processing
Figure 8: Location privacy scenario, k-NN and ANN estima-
tion with channel pre-processing
5.6 Experiment 3: differential privacy
In this section we consider a popular application of DP: individual
data protection in medical datasets from which we wish to extract
some statistics via counting queries. It is well known that the release
of exact information from the database, even if it is only the result
of statistical computation on the aggregated data, can leak sensitive
information about the individuals. The solution proposed by DP is
100100010000Training set size before preprocessing0.00.10.20.30.4normalized estimation errordispersion: 0.051total error: 0.379dispersion: 0.055total error: 0.357dispersion: 0.031total error: 0.102dispersion: 0.014total error: 0.112dispersion: 0.004total error: 0.024dispersion: 0.005total error: 0.053ANNk-NN100100010000Training set size0.10.20.30.40.50.6normalized estimation errordispersion: 0.033total error: 0.498dispersion: 0.038total error: 0.590dispersion: 0.032total error: 0.283dispersion: 0.016total error: 0.278dispersion: 0.013total error: 0.082dispersion: 0.015total error: 0.121ANNk-NNto obfuscate the released information with carefully crafted noise
that obeys certain properties. The goal is to make it difficult to
detect whether a certain individual is in the database or not. In
other words, two adjacent datasets (i.e., datasets that differ only
for the presence of one individual) should have almost the same
likelihood to produce a certain observable result.
In our experiment, we consider the Cleveland heart disease
dataset [24] which consist of 303 records of patients with a medical
heart condition. Each condition is labeled by an integer number
indicating the severity: from 0, representing a healthy patient, to 4,
representing a patient whose life is at risk.
We assume that the hospital publishes the histogram of the pa-
tients’ records, i.e., the number of occurrences for each label. To
protect the patients’ privacy, the hospital sanitizes the histogram
by adding geometric noise (a typical DP mechanism) to each label’s
count. More precisely, if the count of a label is z1, the probability
that the corresponding published number is z2 is defined by the
distribution in (32), where x and y are replaced by z1 and z2 respec-
tively, and r is 1. Note that z1, the real count, is an integer between
0 and 303, while its noisy version z2 ranges on all integers. As for
the value of ν, in this experiment we set it to 1.
The secrets space X is set to be a set of two elements: the full
dataset, and the dataset with one record less. These are adjacent in
the sense of DP, and, as customary in DP, we assume that the record
on which the two databases differ is the target of the adversary.
The observables space Y is the set of the 5-tuples produces by the
noisy counts of the 5 labels. W is set to be the same as X.
We assume that the adversary is especially interested in finding
out whether the patient has a serious condition. The function д
reflects this preference by assigning higher value to higher labels.
Specifically, we set W “ X and
if w ‰ x
if w “ x ^ x P t0, 1, 2u
if w “ x ^ x P t3, 4u,
(34)
$’&’%0,
1,
2,
дpw, xq “
For the estimation, we consider 10K, 30K and 50K samples for
the training sets, and 50K samples for the validation set. For the
Figure 9: Differential privacy scenario, k-NN and ANN esti-
mation with data pre-processing
experiments with k-NN we choose the Manhattan distance, which
is typical for DP4. In the case of data pre-processing the size of
4The Manhattan distance on histograms corresponds to the total variation distance on
the distributions resulting from the normalization of these histograms.
Figure 10: Differential privacy scenario, k-NN and ANN esti-
mation with channel pre-processing
the transformed training set D1
m1is about 1.2 times the original
Dm. The performances of the data and channel pre-processing are
shown in Figures 9 and Figure 10 respectively. Surprisingly, in this
case the data pre-processing method outperforms the channel pre-
processing one, although only slightly. Additional plots, including
the results for the frequentist approach, can be found in Appendix H.
5.7 Experiment 4: password checker
In this experiment we consider a password checker, namely a pro-
gram that tests whether a given string corresponds to the password
stored in the system. We assume that string and password are se-
quences of 128 bits, an that the program is “leaky”, in the sense
that it checks the two sequences bit by bit and it stops checking
as soon as it finds a mismatch, reporting failure. It is well known
that this opens the way to a timing attack (a kind of side-channel
attack), so we assume that the system tries to mitigate the threat
by adding some random delay, sampled from a Laplace distribution
and then bucketing the reported time in 128 bins corresponding
to the positions in the sequence (or equivalently, by sampling the
delay from a Geometric distribution, cfr. Equation 32). Hence the
channel C is a 2128 ˆ 128 stochastic matrix.
The typical attacker is an interactive one, which figures out
larger and larger prefixes of the password by testing each bit at
a time. We assume that the attacker has already figured out the
first 6 bits of the sequence and it is trying to figure out the 7-th.
Thus the prior π is distributed (uniformly, we assume) only on the