:::
:::
::: section
::: titlepage
# []{#setting-up-pcp_monitoring-and-managing-system-status-and-performance.html#sizing-factors_setting-up-pcp}大小因素 {.title}
:::
以下是扩展所需的大小因素：
::: variablelist
[`远程系统大小`{.literal}]{.term}
:   CPU、磁盘、网络接口和其他硬件资源的数量会影响集中式日志记录主机上每个
    `pmlogger`{.literal} 收集的数据量。
[`记录的指标`{.literal}]{.term}
:   记录指标的数量和类型扮演重要角色。特别是，`每个进程 proc.*`{.literal}
    指标需要大量磁盘空间，例如，使用标准 `pcp-zeroconf`{.literal}
    设置，10s 日志记录间隔，11 MB 且不带 proc 指标，以及 proc 指标的 155
    MB - 倍于 10 倍。此外，每个指标的实例数量（如
    CPU、块设备和网络接口数量）也会影响所需的存储容量。
[`日志间隔`{.literal}]{.term}
:   记录指标的频率，会影响存储要求。预期的每日 PCP 存档文件大小将写入到
    `pmlogger.log 实例的 pmlogger`{.literal}.log``{=html}
    文件中。这些值是未压缩的估计值。由于 PCP 存档压缩非常出色，大约
    10:1，可以为特定站点确定实际的长期磁盘空间要求。
[`pmlogrewrite`{.literal}]{.term}
:   每次 PCP 升级后，如果之前版本的指标元数据和 PCP
    的新版本有变化，将执行 `pmlogrewrite`{.literal}
    工具并重写旧存档。此过程持续时间可根据存储的存档数量线性扩展。
:::
::: itemizedlist
**其它资源**
-   `pmlogrewrite(1)`{.literal} 和 `pmlogger(1)`{.literal} man page
:::
:::
::: section
::: titlepage
# []{#setting-up-pcp_monitoring-and-managing-system-status-and-performance.html#configuration-options-for-pcp-scaling_setting-up-pcp}PCP 扩展的配置选项 {.title}
:::
以下是扩展所需的配置选项：
::: variablelist
[`sysctl 和 rlimit 设置`{.literal}]{.term}
:   启用归档发现后，`pmproxy`{.literal} 需要针对监控或日志尾部的每个
    `pmlogger`{.literal} 四个描述符，以及服务日志和 `pmproxy`{.literal}
    客户端套接字的额外文件描述符（如果有）。每个 `pmlogger`{.literal}
    进程使用大约 20 个文件描述符用于远程 `pmcd`{.literal}
    套接字、存档文件、服务日志等。在运行大约 200 个 `pmlogger`{.literal}
    进程的系统上，这总共可以超过默认的 1024
    个软限制。`pcp-5.3.0`{.literal} 及之后的版本中的 `pmproxy`{.literal}
    服务会自动将软限制增加到硬限制。在较早版本的 PCP 上，如果要部署大量
    `pmlogger`{.literal} 进程，则需要调优，这可以通过增加
    `pmlogger`{.literal} 的软或硬限制来完成。如需更多信息，请参阅
    [如何为 systemd
    运行的服务设置限制(ulimit](https://access.redhat.com/solutions/1346533){.link}
    )。
[`本地归档`{.literal}]{.term}
:   `pmlogger`{.literal} 服务将本地和远程 `pmcds`{.literal} 的指标存储在
    `/var/log/pcp/pmlogger/`{.literal}
    目录中。要控制本地系统的日志记录间隔，请更新
    `/etc/pcp/pmlogger/control.d/configfile`{.literal}
    文件，并在参数中添加 `-t X`{.literal}，其中 [*X*]{.emphasis}
    是日志间隔（以秒为单位）。要配置应记录哪些指标，请执行
    `pmlogconf /var/lib/pcp/config/pmlogger/config.clienthostname`{.literal}。此命令使用一组默认指标部署配置文件，可以选择进一步自定义这些指标。要指定保留设置，即何时清除旧的
    PCP 存档，更新 `/etc/sysconfig/pmlogger_timers`{.literal} 文件并指定
    `PMLOGGER_DAILY_PARAMS="-E -k X"`{.literal}，其中 [*X*]{.emphasis}
    是保留 PCP 存档的天数。
[`redis`{.literal}]{.term}
:   `pmproxy`{.literal} 服务将记录的指标从 `pmlogger`{.literal} 发送到
    Redis 实例。以下是在 `/etc/pcp/pmproxy/pmproxy.conf`{.literal}
    配置文件中指定保留设置的两个选项：
    ::: itemizedlist
    -   `stream.expire`{.literal}
        指定应删除陈旧指标的时间，即未在指定时间段内更新的指标（以秒为单位）。
    -   `stream.maxlen`{.literal}
        指定每个主机一个指标的最大指标值数。这个设置应该是日志间隔（如
        20160 代表 14 天的保留）和 60s
        日志间隔(60\*60\*24\*14/60)的保留时间。
    :::
:::
::: itemizedlist
**其它资源**
-   `pmproxy(1)、`{.literal} `pmlogger(1)`{.literal} 和
    `sysctl(8)`{.literal} man page
:::
:::
::: section
::: titlepage
# []{#setting-up-pcp_monitoring-and-managing-system-status-and-performance.html#example-analyzing-the-centralized-logging-deployment_setting-up-pcp}示例：分析集中式日志部署 {.title}
:::
以下结果在集中式日志记录设置（也称为 pmlogger 场部署）中收集，默认安装
`pcp-zeroconf 5.3.0`{.literal}，其中每个远程主机都是在 `具有`{.literal}
64 CPU 内核、376 GB RAM 和一个磁盘上运行的相同容器实例。
日志记录间隔为 10s，不包括远程节点的 proc 指标，内存值则代表 Resident
Set Size(RSS)值。
::: table
[]{#setting-up-pcp_monitoring-and-managing-system-status-and-performance.html#idm139675422937424}
**表 6.4. 10 条日志记录间隔的详细使用统计**
::: table-contents
  主机数量                               10       50
  -------------------------------------- -------- --------
  每天的 PCP 归档存储                    91 MB    522 MB
  `pmlogger`{.literal} 内存              160 MB   580 MB
  每天 `pmlogger`{.literal} 网络（在）   2 MB     9 MB
  `pmproxy`{.literal} 内存               1.4 GB   6.3 GB
  每天 Redis 内存                        2.6 GB   12 GB
:::
:::
::: table
[]{#setting-up-pcp_monitoring-and-managing-system-status-and-performance.html#idm139675495602096}
**表 6.5. 根据被监控的主机使用的 60s 日志间隔的资源**
::: table-contents
  主机数量                               10        50        100
  -------------------------------------- --------- --------- ---------
  每天的 PCP 归档存储                    20 MB     120 MB    271 MB
  `pmlogger`{.literal} 内存              104 MB    524 MB    1049 MB
  每天 `pmlogger`{.literal} 网络（在）   0.38 MB   1.75 MB   3.48 MB
  `pmproxy`{.literal} 内存               2.67 GB   5.5GB     9 GB
  每天 Redis 内存                        0.54 GB   2.65 GB   5.3 GB
:::
:::
::: {.note style="margin-left: 0.5in; margin-right: 0.5in;"}
### 注意 {.title}
`pmproxy`{.literal} 队列 Redis 请求并使用 Redis pipelining 来加快 Redis
查询。这可能导致高内存用量。有关这个问题进行故障排除，请参阅
[高内存用量故障排除](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/monitoring_and_managing_system_status_and_performance/index#troubleshooting-high-memory-usage_setting-up-pcp){.link}。
:::
:::
::: section
::: titlepage
# []{#setting-up-pcp_monitoring-and-managing-system-status-and-performance.html#example-analyzing-the-federated-setup-deployment_setting-up-pcp}示例：分析联合设置部署 {.title}
:::
在联合设置中观察到以下结果，也称为多个 `pmlogger`{.literal}
场，由三个集中式日志记录(`pmlogger`{.literal} )设置组成，其中每个
`pmlogger`{.literal} 场监控 100 远程主机，总共 300 个主机。
这个 `pmlogger`{.literal} 场的设置与 [示例中所述的配置相同：分析 60s
日志记录间隔的集中式日志记录部署](#setting-up-pcp_monitoring-and-managing-system-status-and-performance.html#example-analyzing-the-centralized-logging-deployment_setting-up-pcp "示例：分析集中式日志部署"){.link}，但
Redis 服务器在集群模式下运行除外。
::: table
[]{#setting-up-pcp_monitoring-and-managing-system-status-and-performance.html#idm139675554564464}
**表 6.6. 根据联合主机使用 60s 日志间隔的资源**
::: table-contents
  每天的 PCP 归档存储   `pmlogger`{.literal} 内存   每天网络(In/Out)    `pmproxy`{.literal} 内存   每天 Redis 内存
  --------------------- --------------------------- ------------------- -------------------------- -----------------
  277 MB                1058 MB                     15.6 MB / 12.3 MB   6-8 GB                     5.5 GB
:::
:::
在这里，所有值都是每个主机。由于 Redis 集群的节点间通信，网络带宽较高。
:::
::: section
::: titlepage
# []{#setting-up-pcp_monitoring-and-managing-system-status-and-performance.html#troubleshooting-high-memory-usage_setting-up-pcp}高内存用量故障排除 {.title}
:::
以下情况可能会导致内存用量高：
::: itemizedlist
-   `pmproxy`{.literal} 进程忙处理新的 PCP 存档，且没有备用 CPU
    周期来处理 Redis 请求和响应。
-   Redis 节点或集群超载，无法及时处理传入的请求。
:::
`pmproxy`{.literal} 服务守护进程使用 Redis 流并支持配置参数，它们是 PCP
调优参数，并影响 Redis
内存用量和密钥保留。`/etc/pcp/pmproxy/pmproxy.conf`{.literal}
文件列出了可用于 `pmproxy`{.literal} 和关联的 API 的配置选项。
这部分论述了如何排除高内存用量问题。
::: orderedlist
**先决条件**
1.  安装 `pcp-pmda-redis`{.literal} 软件包：
    ``` screen
    # {PackageManagerCommand} install pcp-pmda-redis
    ```
2.  安装 redis PMDA:
    ``` screen
    # cd /var/lib/pcp/pmdas/redis && ./Install
    ```
:::
::: itemizedlist
**流程**
-   要排除高内存用量，请执行以下命令并观察 `inflight`{.literal} 列：
    ``` literallayout
    $ pmrep :pmproxy
             backlog  inflight  reqs/s  resp/s   wait req err  resp err  changed  throttled
              byte     count   count/s  count/s  s/s  count/s   count/s  count/s   count/s
    14:59:08   0         0       N/A       N/A   N/A    N/A      N/A      N/A        N/A
    14:59:09   0         0    2268.9    2268.9    28     0        0       2.0        4.0
    14:59:10   0         0       0.0       0.0     0     0        0       0.0        0.0
    14:59:11   0         0       0.0       0.0     0     0        0       0.0        0.0
    ```
    此列显示有多少 Redis
    请求正在显示，即它们已排队或发送，目前尚未收到任何回复。
    高数字表示以下条件之一：
    ::: itemizedlist
    -   `pmproxy`{.literal} 进程忙处理新的 PCP 存档，且没有备用 CPU
        周期来处理 Redis 请求和响应。
    -   Redis 节点或集群超载，无法及时处理传入的请求。
    :::
-   要排除高内存用量问题，请减少此场的 `pmlogger`{.literal}
    进程数量，并添加另一个 pmlogger 场。使用联合 - 多个 pmlogger 场设置.
    如果 Redis 节点使用 100% CPU
    延长时间，请将它移至性能更好的主机，或者改为使用集群 Redis 设置。
-   要查看 `pmproxy.redis.*`{.literal} 指标，请使用以下命令：
    ``` screen
    $ pminfo -ftd pmproxy.redis
    pmproxy.redis.responses.wait [wait time for responses]
        Data Type: 64-bit unsigned int  InDom: PM_INDOM_NULL 0xffffffff
        Semantics: counter  Units: microsec
        value 546028367374
    pmproxy.redis.responses.error [number of error responses]
        Data Type: 64-bit unsigned int  InDom: PM_INDOM_NULL 0xffffffff
        Semantics: counter  Units: count
        value 1164
    [...]
    pmproxy.redis.requests.inflight.bytes [bytes allocated for inflight requests]
        Data Type: 64-bit int  InDom: PM_INDOM_NULL 0xffffffff
        Semantics: discrete  Units: byte
        value 0
    pmproxy.redis.requests.inflight.total [inflight requests]
        Data Type: 64-bit unsigned int  InDom: PM_INDOM_NULL 0xffffffff
        Semantics: discrete  Units: count
        value 0
    [...]
    ```
    要查看 Redis 请求的数量是 inflight，请查看
    `pmproxy.redis.requests.inflight.total`{.literal} 指标和
    `pmproxy.redis.requests.inflight.bytes`{.literal}
    指标，以查看所有当前的 inflight Redis 请求占用的字节数。
    通常，redis 请求队列为零，但可以根据使用大型 pmlogger
    场进行构建，这会限制 pmproxy 客户端的可扩展性，并可能导致
    `pmproxy`{.literal} 客户端的高延迟。
-   使用 `pminfo`{.literal} 命令查看有关性能指标的信息。例如，若要查看
    here `dis.*`{.literal} 指标数据，请使用以下命令：
    ``` screen
    $ pminfo -ftd redis
    redis.redis_build_id [Build ID]
        Data Type: string  InDom: 24.0 0x6000000
        Semantics: discrete  Units: count
        inst [0 or "localhost:6379"] value "87e335e57cffa755"
    redis.total_commands_processed [Total number of commands processed by the server]
        Data Type: 64-bit unsigned int  InDom: 24.0 0x6000000
        Semantics: counter  Units: count
        inst [0 or "localhost:6379"] value 595627069
    [...]
    redis.used_memory_peak [Peak memory consumed by Redis (in bytes)]
        Data Type: 32-bit unsigned int  InDom: 24.0 0x6000000
        Semantics: instant  Units: count
        inst [0 or "localhost:6379"] value 572234920
    [...]
    ```
    要查看峰值内存用量，请参阅 heredis `.used_memory_peak`{.literal}
    指标。
:::
::: itemizedlist
**其它资源**
-   `pmdaredis(1)、`{.literal} `pmproxy(1)`{.literal} 和
    `pminfo(1)`{.literal} man page
-   [PCP
    部署架构](#setting-up-pcp_monitoring-and-managing-system-status-and-performance.html#pcp-deployment-architectures_setting-up-pcp "PCP 部署架构"){.link}
:::
:::
:::
[]{#logging-performance-data-with-pmlogger_monitoring-and-managing-system-status-and-performance.html}
::: chapter
::: titlepage