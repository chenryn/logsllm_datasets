generally assume that the attacker does not have access to
any model parameters, but has oracle access to the prediction
labels for some samples, and in some cases also the prediction
conﬁdence. In some settings, features and the model type
are assumed to be known as well. Xu et al. [61] use a genetic
evolution algorithm to automatically evade both PDFrate and
Hidost. The evolutionary algorithm uses a ﬁtness score as
feedback, to guide the search in ﬁnding evasive PDF variants
by mutating the seed PDF malware. For every generation of
the population during the search, the attack uses a cuckoo
oracle to dynamically check that mutated PDFs still preserve
the malicious functionality. This check is much stronger than
static insertion-only methods used by gradient-based attacks.
Dang et al. [18] uses a more restricted threat model where
the attacker does not have access to classiﬁcation scores,
and only has access to the classiﬁed label and a blackbox
morpher that manipulates the PDFs. They use the scoring
function based on Hill-Climbing to attack the classiﬁer under
such assumptions. In this paper, we improve the attack from
the genetic evolution framework of EvadeML [61], and also
develop several adaptive attacks based on that.
2.3 Robust Training
Out of the arms race between adversarial image exam-
ples [49] and many defenses [9, 12, 13, 42, 43], two training
methods have proven to be the strongest among all. They are
adversarially robust training and veriﬁably robust training. We
brieﬂy explain the training methods, and reason about why ver-
iﬁably robust training provides stronger robustness guarantee.
2.3.1 Robust Optimization
Both adversarially robust training and veriﬁably robust
training are based on robust optimization. Let us ﬁrst look at the
optimization objective used by the regular training process of
the neural network. Given an input x with the true label y, a neu-
ral network fq parameterized by q maps it to a label ˆy = f (x).
A loss function L(y, ˆy) is used to evaluate the errors of such
prediction, e.g., the cross-entropy loss. The training process
has the following optimization objective that minimizes the
loss to ﬁnd optimal weights q. The summation is an empirical
measure of the expected loss over the entire training dataset.
q =argmin
q ÂL(y, ˆy)
(1)
In the adversarial setting, for the input x, there can be a set
of all possible manipulations ˜x bounded by a distance metric
Dk within distance k, i.e. ˜x 2 Dk(x). Robust optimization
minimizes the worst case loss for all inputs in Dk(x), solving
a minimax problem with two components.
q =argmin
q Â max
˜x2Dk(x)
L(y, fq( ˜x))
(2)
• Inner Maximization Problem: ﬁnd ˜x that maximizes
the loss value within the robustness region Dk(x), i.e., the
robust loss.
• Outer Minimization Problem: minimize the maximal
loss to update the weights q of the neural network.
The following two robust training methods solve the inner
maximization problem in different ways.
2.3.2 Adversarially Robust Training
Adversarially robust training empirically estimates the
maximal loss in Equation 2 by using different attacks. The
state-of-the-art adversarially robust training method from
Madry et al. [39] uses adversarial examples found by the
Projected Gradient Descent (PGD) attack [35] to estimate
the robust loss for the training. The training method has been
applied to benchmarking image datasets, including MNIST
and CIFAR-10. The trained models have shown robustness
against known attacks including the Projected Gradient
Descent (PGD) attack [35], Carlini-Wagner (CW) attacks [13],
Fast Gradient Sign Method (FGSM) [26], etc.
Adversarially robust training has been applied to malware
datasets. In the followup work to [61], Xu et al. [1] applied
adversarially robust training over the Contagio malware
dataset, which increased the false positive rate to as high
as 85%. Grosse et al. [28] applied the training method to
the android malware classiﬁer using adversarial malware
examples, increasing the false positive rate to 67%.
2.3.3 Veriﬁably Robust Training
Veriﬁably robust training uses sound over-approximation
techniques to obtain the upper bound of the inner maximiza-
tion problem. Different methods have been used to formally
verify the robustness of neural networks over input regions [20,
23, 24, 31, 34, 37, 44], such as abstract transformations [25],
symbolic interval analysis [55, 56], convex polytope approx-
imation [58], semideﬁnite programming [45], mixed integer
programming [50], Lagrangian relaxation [22] and relaxation
with Lipschitz constant [57], which essentially solve the inner
maximization problem. By using the worst case bounds derived
by formal veriﬁcation techniques,veriﬁably robust training [21,
41, 53, 59] can obtain such veriﬁed robustness properties.
2346    29th USENIX Security Symposium
USENIX Association
The training method has been applied to image datasets
to increase veriﬁable robustness, usually with the tradeoff
of lower accuracy and higher computation and memory cost
for the training. Recent works have focused on scaling the
training method to larger networks and bigger datasets [53,59].
Since veriﬁably robust training techniques can train classiﬁers
to be sound with regard to the robustness properties, the
trained network gains robustness against even unknown
adaptive attacks. On the contrary, adversarially robust training
is limited by the speciﬁc threat model used to generate
adversarial instances for the training. Therefore, we apply
veriﬁably robust training to build the PDF malware classiﬁer.
By carefully specifying useful robustness properties, our
robust model has only 0.56% false positive rate.
2.3.4 ERA and VRA
In this paper, we will use the following two metrics to
evaluate our veriﬁably robust PDF malware classiﬁer.
Estimated Robust Accuracy (ERA) measures the per-
centage of test inputs that are robust against known attacks,
given a distance bound. For instance on the MNIST dataset,
Madry et al.’s training method [39] can achieve 89% ERA
against PGD attacks within a bounded distance of L•0.3.
Veriﬁed Robust Accuracy (VRA) measures the percent-
age of test inputs that are veriﬁed to be correctly classiﬁed
within a distance bound. For example, Wong et al.’s training
method [59] obtains 21.78% VRA on a CIFAR10 Resnet
model within a bounded distance of L•8/255.
3 Veriﬁably Robust PDF Malware Classiﬁer
Since it is extremely hard, if not impossible, to have a
malware classiﬁer that is robust against all possible attackers,
we aim to train classiﬁers to be robust against building block
attacks. In this section, we describe the speciﬁcation and
training of robustness properties.
3.1 Robustness Properties
3.1.1 Motivation
Building block operations. A large group of evasion at-
tacks against malware classiﬁers can be considered as solving a
search problem, e.g., mimicry attacks [36, 52], EvadeML [61],
EvadeHC [18] and MalGAN [30]. The search starts from the
seed malware, modiﬁes the malware to generate variants while
keeping the malicious functionality, until ﬁnding a variant
that can be classiﬁed as benign. The attacks use building block
operations to make the search process more systematic and
efﬁcient over a large space. Speciﬁcally for PDF malware,
the operations include PDF object mutation operators [61],
random morpher [18] and feature insertion-only generator [30].
After performing the building block operations, the attacks
optimize the search based on the classiﬁer’s feedback that
indicates the evasion progress. We want to make the search
harder by training classiﬁers to be robust against building
block operations. To achieve that, we consider operations
that generate PDFs with the correct syntax. A PDF variant
needs to have both correct syntax and correct semantics to stay
malicious. Though dynamic execution can conﬁrm the same
malicious behavior, it is too expensive to do that during train-
ing. Therefore, we statically ensure the correct PDF syntax. A
syntactically correct PDF ﬁle can be parsed into a tree structure
(Figure 1b, Section 2.1). Thus, the building block operations
are a combination of insertion and deletion in the PDF tree.
Based on this insight, we design robustness properties related
to the PDF subtrees. We propose two classes of subtree
insertion and subtree deletion properties, which can be used
as the stepping stones to construct more sophisticated attacks.
False positive rate. It is crucial to maintain low false pos-
itive rate for security classiﬁers due to the Base-Rate Fal-
lacy [10]. If we train classiﬁers with evasive malware variants
without enforcing a proper bound, the classiﬁer will have very
high false positive rate. Since attacks often mimic benign be-
havior, the feature vectors of unbounded variants are close to
benign vectors, which affects the false positive rate. Therefore,
we need a distance metric to deﬁne the robustness properties
to capture the similarity between the PDF malware and its vari-
ants. Since the Lp norm distance in the feature space does not
capture whether the corresponding PDF variant has the correct
syntax, we propose a new distance metric for the PDF subtree.
3.1.2 Subtree Distance Metric
Subtree Distance Deﬁnition. We propose a new distance
metric to bound the attacker’s building block operations over
a PDF malware. The subtree distance between two PDFs x and
x0 is, the number of different subtrees of depth one in the two
PDF trees. These subtrees are directly under the root object
in the PDF, regardless of their height and the number of nodes
in them. Formally,
d(x, x0) = #{(rootx.subtrees [ rootx0.subtrees)  
(rootx.subtrees\rootx0.subtrees)}
We ﬁrst take the union of the subtrees with depth one from
two PDFs, and then remove the intersection of the two subtree
sets (identical subtrees). The distance d(x,x0) is the cardinality
of the resulting set.
If the attacker inserts benign pages into the PDF malware
under the /Root/Pages subtree (Figure 1b), this operation
will not exceed subtree distance one, no matter how long the
malicious PDF document becomes. Changing an arbitrary
subtree in the PDF may have different Lp norm distances de-
pending on which subtree is manipulated. For example, in the
Hidost binary path features, manipulating /Root/Metadata
is bounded by L1  4, whereas changing /Root/Pages can
be up to L11195. However, under the subtree distance, they
are both within the distance one bound.
We use the subtree distance to deﬁne robustness properties.
Each property corresponds to an over-approximated set
Dk(x) ={ ˜x|d(x, ˜x) k}. The set captures all PDF malware ˜x
that can be possibly generated by changes in arbitrary k subtree
USENIX Association
29th USENIX Security Symposium    2347
regions under the root of the malware seed x, regardless of
the feature extraction method. Since insertion and deletion
are building block operations, we formulate these robustness
properties at distance one before composing more complicated
robustness properties.
3.1.3 Subtree Insertion and Deletion Properties
Subtree Insertion Property (Subtree Distance 1): given
a PDF malware, all possible manipulations to the PDF
bounded by inserting an arbitrary subtree under the root, do
not result in a benign prediction by the classiﬁer.
The attacker ﬁrst chooses any one of the subtrees, and
then chooses an arbitrary shape of the subtree for the
insertion. Some subtrees are commonly seen in benign
PDFs, which can be good insertion candidates for eva-
sion, e.g., /Root/Metadata, /Root/StructTreeRoot,
/Root/ViewerPreferences. Although the subtree distance
for the property is only one, the total number of allowable
insertions is on the order of the sum of exponentials for the
number of children under each subtree.
/Root/Names/JavaScript/Names/JS
The property over-approximates the set of semanti-
cally correct and malicious PDFs. For example, if we
insert
not
/Root/Names/JavaScript/Names/S, the javascript is no
longer functional. Moreover, we over-approximate the
attacker’s possible actions. Attacks are usually based on
some optimization procedure rather than exhaustive search.
However, if a known attack fails to ﬁnd succesful insertion in a
subtree, unknown attacks may succeed. Therefore, the property
can overestimate the worst case behavior of the classiﬁer.
but
Subtree Deletion Property (Subtree Distance 1): given a
PDF malware, all possible manipulations to the PDF bounded
by deleting an arbitrary subtree under the root, do not result
in a benign prediction by the classiﬁer.
For the PDF malware example shown in Figure 1b, this prop-
erty allows deleting any one of the following: /Root/Type,
/Root/Pages, and /Root/OpenAction. Note that this allows
any combination of deletion under non-terminal nodes
/Root/Pages and /Root/OpenAction.
Some exploit triggers may be lost or the program semantics
may be broken by deleting content from the malware. The
robustness property covers an over-approximated set of
evasive PDF malware, and enforces that they are always
classiﬁed as malicious. It is acceptable to include some
non-malicious PDFs in the robustness region, as long as we
do not increase the false positive rate for benign PDFs.
3.1.4 Other Properties
We do not specify other common properties like replace-
ment, since many can be viewed as a combination of insertions
and deletions. The robustness properties can be generalized
to up to N subtree distance, where N =42 in our feature space.
Next, we describe properties with larger distances.
Subtree Deletion Property (Subtree Distance 2): the
strongest possible attackers bounded by deletions within any
two subtrees under the root, cannot make the PDF classiﬁed
as benign.
Subtree Insertion Property (Subtree Distance N   1):
the strongest possible attackers bounded by insertions within
all but one subtree under the root, cannot make the PDF
classiﬁed as benign.
Monotonic Property and Subtree Insertion Property
(Distance N): Incer et al. [32] have proposed to enforce the
monotonic property for malware classiﬁers. The monotonic
property states that an attacker cannot evade the classiﬁer by
only increasing the feature values. Speciﬁcally, if two feature
vectors satisfy x  x0, then the classiﬁer f guarantees that
f (x)  f (x0). They enforce monotonicity for both benign
and malicious classes, such that inserting features into any
executable makes it appear more malicious to the classiﬁer.
The property is so strong that it decreased the temporal
detection rate of the classiﬁer by 13%.
To compare against the monotonic property, we propose
the subtree insertion property at distance N. In other words,
the insertion is unrestricted by any subtree, and it is allowed
for all features. We focus on this property for the malicious
PDFs, which is a key difference from the monotonic property.
Larger distances bound a larger set of evasive malware vari-
ants, which can make malicious feature vectors more similar to
benign ones and affect the false positive rate. In our evaluation,
we train all ﬁve properties and several combinations of them
using mixed training technique (Table 4).
3.2 Training the Properties
Given the over-approximated set of inputs Dk(x) for each
robustness property, we use sound analysis of the neural
network to obtain the corresponding robust loss.
Sound analysis deﬁnition. A sound analysis over the
neural network fq represents a sound transformation Tf from
the input to the output of fq. Formally, given input x2X and
a property Dk(x) bounded by distance k, the transformation
Tf is sound if the following condition is true: 8x2X, we have
{ fq( ˜x)| ˜x 2 Dk(x)}✓ Tf (Dk(x)) That is, the sound analysis
over-approximates all the possible neural network outputs for
the property. Using Tf (Dk(x)), we can compute the robust loss
in Equation 2.
Training. Existing works have shown that training only
for the robustness objective degrades regular test accuracy,
and combining the two objectives helps smooth the conﬂict
between the two [27,41,53]. Consequently, we adopt the same
principle to train for a combined loss as below.
L =L(y, fq(x))+ max
˜x2Dk(x)
L(y, fq( ˜x))
(3)
In Equation 3, the left-hand side of the summation denotes
the regular loss for the training data point (x, y), and the
right-hand side represents the robust loss for any manipulated ˜x
2348    29th USENIX Security Symposium
USENIX Association
Attackers
Bounded by 
Robustness Properties?
Yes
No
Bounded, Whitebox, Adaptive 
(1) (2): VRA, ERA
Yes
Whitebox Access?
No
Unbounded, Whitebox, Adaptive
(3) (4): ERA, L0
Yes
Adaptive?
No
Unbounded, Blackbox, Adaptive 
(7): ERA, L0, Trace Length
Unbounded, Blackbox, Non-adaptive 
(5) (6): ERA, L0, Trace Length
Figure 2: Different types of attackers in our evaluation.
bounded by distance k satisfying a deﬁned robustness property
Dk(x). We give the same weights to combine the two parts of
the loss, to equally optimize the regular loss and the robust
loss. The robust loss is computed by the worst case within the
bounded region of every training data input. More implementa-
tion details about robust training can be found in Section 4.1.4.
4 Evaluation
We train seven veriﬁably robust models and compare
them against twelve baseline models, including neural
network with regular training, adversarially robust training,
ensemble classiﬁers, and monotonic classiﬁers1. We answer
the following questions in the evaluation.
restricted by the robustness properties?
• Do we have higher VRA and ERA if the attackers are
• Do we have higher ERA against unrestricted attackers?
• How much do we raise the bar (e.g., L0 distance in
features and mutation trace length) for the unrestricted
attackers to evade our robust models?
We use seven different attackers to evaluate the models.
When choosing the attackers, we consider three factors, i.e.,
whether the attacker is bounded by the robustness properties,
whether the attacker has whitebox access to the model, and
whether the attacker is adaptive. Figure 2 shows the categories
where every attacker belongs to, and the evaluation metrics we
use for the category. The detailed threat model for each attacker
((1) to (7)) is shown in Table 1. In the table, we have marked
whether each attacker generates realizable inputs that are real
PDF malware. We evaluate attacks producing both realizable
and non-realizable inputs since robustness against them are
equally important. Tong et al. [51] have shown that robustness
against feature-space attacks on non-realizable inputs can be
generalized to robustness against realizable attacks.
Machine. We use a desktop machine for all the experiments.
The machine is conﬁgured with Intel Core i7-9700K 3.6 GHz
8-Core Processor, 64 GB physical memory, 1TB SSD, Nvidia
GTX 1080 Ti GPU, and it runs a 64-bit Ubuntu 18.04 system.