the last 5 minutes. First, we tested the reliability of the system;
we left the system running 7 hours without anomalies occurring.
We obtained 2 false positives instances occurring for 10 minutes
each. We then performed actuators manipulation in the system.
In Table 4 we report the summary of the tested anomalies. We
replicated anomalies reported in the WADI dataset.
While anomalies were occurring, we also launched our Uncon-
strained concealment attacks in WADI Testbed to assess their feasi-
bility and efficacy. We tested both the iterative and learning based
approach in real-time by simulating the sensors value manipulation
done by the attacker, all the instances of the anomalies occurring
in the system were successfully misclassified. In Table 4, the last
two columns summarize the results of iterative and learning based
attacks carried out in real-time. In particular, our learning based
modifications took the same time to compute examples as during
the earlier experiments (on average, 5ùëöùë†), which is much faster
than the sampling rate in the system.
6 RELATED WORK
We now discuss important related work in the area of anomaly
detection in CPS, and evasion attacks on classifiers.
(cid:77)Anomaly Detection in CPS Detecting stealthy attacks in CPS
through the identification of process-based anomalies without re-
quiring a detailed physical model is an active research topic. Had≈æios-
manoviƒá et al. [23] use an autoregressive model on time series ex-
tracted from modbus PLC traffic, evaluating their approach on data
from two water treatment plants; Krotofil et al. [34] use a theoreti-
cal information approach to detect sensor spoofing attacks; Aoudi
et al. [4] use model-free techniques rooted on singular spectrum
analysis to detect structural changes in the process behavior. More
recently, in Autonomous Vehicles setting, control-based techniques
used for anomaly detection such as Control Invariant [13] and Ex-
tended Kalman Filters [7, 45], were found vulnerable to several
stealthy attacks [15, 45, 48].
In addition, various proposals in this space use deep learning
techniques (usually by training a learning-based model on data
gathered during the normal operation of the process) and statisti-
cally comparing the sensor readings with the model‚Äôs prediction at
runtime. Wickramasinghe et al. [60] provide an overview of how
Deep Learning techniques can be used in the context of CPS secu-
rity. Goh et al. [20] propose an architecture to detect anomalies over
a water treatment testbed with a Recurrent neural network (LSTM-
RNN) used to predict sensor readings, and CUSUM to compute
the difference between the predicted outputs and the actual sensor
readings. Starting from this approach and using the same dataset
for evaluation, Kravchik et al.[32] suggest the use of a convolutional
neural network to perform one-step prediction, while Taormina
et al. [51] propose the autoencoder-based detector (the target of
our attacks). With respect to this former category of deep learning-
based detectors, our proposed approach is the first to propose a
systematic constrained attacker model and identifies vulnerabilities
that affect detector performance. This enables an attacker to hide
the physical anomalies induced on the system, that would be oth-
erwise detected. Our experiments show how those attacks can be
applied in constrained settings and in real time, making the prior
work anomaly detectors ineffective.
(cid:77)Adversarial Learning for Classifier Evasion The effectiveness
of Adversarial Machine Learning to evade ML-based classifiers
has been demonstrated in a wide range of applications, ranging
from face recognition [47] to voice recognition [62] and malware
detection [61]. Table 5 classifies recent techniques in this domain ac-
cording to the adversary‚Äôs knowledge on the classifier‚Äôs algorithm
and training dataset. In the iterative scenario (i.e., the adversary
knows the internals of the trained model and the training set en-
tirely), Rndic and Laskov [58] present a case study on the evasion of
PDFRate, a malicious PDF detector based on random forests, using
an white box gradient-based evasion method [5], comparing it to a
black box mimicry attack, and discussing the attack effectiveness
according to different attacker models. After the seminal paper
that demonstrated the existence of adversarial examples for neu-
ral networks [50], work has shifted to Deep Learning. Goodfellow
et al. [21] study the cause of adversarial examples and devise a
fast gradient method to perform adversarial perturbations, demon-
strating their results in the image classification context under a
perfect-knowledge iterative scenario. More recently, Carlini and
Wagner [10] defeat a defensive technique known as defensive distil-
lation [44]. White box techniques have also been applied to defeat
face recognition, also through perturbations in physical objects [47].
In more restrictive scenarios, the adversary is only aware of
the general structure of the model and how features are extracted.
Papernot et al. [43] use this imperfect knowledge to build a sur-
rogate model and demonstrate the effectiveness in source-target
misclassification (image recognition). Grosse et al. [22] generalize
ACSAC 2020, December 7‚Äì11, 2020, Austin, USA
A. Erba et al.
the adversarial example crafting algorithm presented in [43] to
malware detection systems. In other cases, the adversary attacks a
classifier while querying the system under attack as an oracle. This
is the case of attacks against proprietary online learning systems:
to evade an online malware classifier, Xu et al. [61] leverage the
fact that the target systems output the classification score to build a
genetic algorithm that morphs the adversarial examples into being
undetected. More recently, Dang et al. [14] lifted the assumption
of knowing the classification score, attacking oracle-like black box
classifiers that only output a binary label; Papernot et al. [42] work
similarly in the context of multi-class image classification.
W.r.t. prior work, in our learning based attack the adversary does
not rely on querying the classifier as an oracle, or on building a
surrogate learner; instead, we exploit the characteristics of the CPS
domain to lift this requirement.
7 CONCLUSIONS
In this work, we started by formalizing an attacker model for real-
world ICS and provided AML taxonomy for it. We then presented
the first real-time concealment attacks on reconstruction-based
anomaly detectors in the context of Industrial Control Systems.
We argued that such attacks present four unique challenges, and
addressed them proposing iterative and learning based attacks. Our
white box attacker uses the iterative approach with a detection
oracle, while the black box attacker uses an autoencoder to hide
anomalies.
Using data from two water distribution systems, we demon-
strated that our attacks are feasible in general, and outperform
replay attacks when the attacker is constrained to control of less
the 95% of the features. Moreover, we show that for the BATADAL
dataset, our novel learning based attack using autoencoder was able
to reduce detection Recall as efficiently as the iterative attack (Recall
dropped from 0.60 to 0.14 in both cases). Our results demonstrate
that the proposed autoencoder based attack achieves successful con-
cealment without knowledge of the targeted Reconstruction-based
anomaly detector (only using normal operational data), without
knowledge of the physical model equations and is computationally
cheap (after training).
We implemented our attacks in a real testbed and showed that
malicious data could be generated on-the-fly, i.e., in between each
sampling step (every 10s, actual example generation took on aver-
age 5ms for iterative). That demonstrates that the proposed attacks
are allowing attackers to perform constrained concealment attacks
on dynamic systems in real-time. In prior work, manipulations are
usually performed offline against a dataset or assume that data to
be manipulated can be precisely predicted. Our results show that
reconstruction-based attack detectors proposed in prior work are
vulnerable to manipulation despite the unique challenges in this
setting, and such attacks need to be considered when designing
future attack detection schemes. Implementation is available at our
Online Repository5.
ACKNOWLEDGMENTS
Several authors were supported by the National Research Foun-
dation (NRF), Singapore, under its National Cybersecurity R&D
5https://github.com/scy-phy/ICS-Evasion-Attacks
Programme (Award No. NRF2014NCR-NCR001-40). Politecnico di
Milano received funding for this project from the European Union‚Äôs
Horizon 2020 research and innovation programme under the Marie
Sk≈Çodowska-Curie grant agreement nr. 690972.
REFERENCES
[1] Marshall Abrams. 2008. Malicious control system cyber security attack case
study‚ÄìMaroochy Water Services, Australia. (2008).
[2] Chuadhry Mujeeb Ahmed, Venkata Reddy Palleti, and Aditya Mathur. 2017.
WADI: A Water Distribution Testbed for Research in the Design of Secure Cyber
Physical Systems. In Proceedings of the Workshop on Cyber-Physical Systems for
Smart Water Networks). ACM, 25‚Äì28. https://doi.org/10.1145/3055366.3055375
[3] S.M. Amin, X. Litrico, S. Sastry, and A.M. Bayen. 2013. Cyber Security of Water