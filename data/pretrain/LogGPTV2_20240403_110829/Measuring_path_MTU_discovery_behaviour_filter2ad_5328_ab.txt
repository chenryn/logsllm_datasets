### 3.3 Targets

We derive our targets from the Alexa Top 1,000,000 websites list [11]. For our IPv4 tests, each vantage point independently selects IP addresses to probe from the first 50,000 entries. This approach ensures that the appropriate addresses for each vantage point are tested, considering that the location of the vantage point may influence the addresses supplied. We test all IPv4 addresses that the domain resolves to. We use `wget` [18] to determine a suitable URL for our tests. If the default page for the domain is at least 1600 bytes in size, we use it for testing PMTUD behavior. Otherwise, we use `wget` to obtain all objects required to display the default page and select the first object that is at least 1600 bytes in size, or the largest available object if none meet the size requirement.

- The San Diego vantage point tests 45,752 addresses.
- The Hamilton vantage point tests 45,990 addresses.
- In total, 54,008 addresses are tested, with an intersection of 70% across 5,492 ASes.

For our IPv6 tests, we resolve the domains for the million websites in the list for IPv6 addresses. For those with IPv6 addresses, we also resolve the domain for IPvV4 addresses to enable PMTUD behavior comparison for dual-stacked webservers. The same 1,067 IPv6 addresses are tested by all five vantage points.

### 3.4 Implementation

We implemented a TBIT-style PMTUD test in `scamper`, a parallelized packet-prober [19]. For each measurement, our tool records metadata about the test (such as the URL, the server MSS, and the MTU value used) along with all packets sent and received for that test in a single data unit. This makes it straightforward to validate inferences based on the recorded packets.

We also developed a driver to coordinate `scamper`'s measurements. For each IP address, the driver starts by sending up to four ping packets to check if the network allows ICMP through. It then performs a sequence of PMTUD tests towards the webserver one at a time, starting with the highest MTU values and moving to the lowest. The driver waits at least one minute between PMTUD tests to each unique IP address to avoid being a nuisance.

### 4. IPv4 PMTUD Behavior

Table 1 presents the overall results of our IPv4 tests, with the final column containing corresponding classifications from [1]. We observe a similar failure rate for the 256-byte MTU tests in 2010 as was seen in the 2004 study. However, the failure rate is much lower for other MTU values tested. Compared to the 2004 study, we measure a success rate of 78–80%, which is nearly double the 2004 result. Additionally, the number of servers that do not set the DF bit on data packets is nearly an order of magnitude less than in the 2004 result. We attribute these differences to a bug in TBIT that classified hosts clearing the DF bit as not attempting PMTUD, and the choice of 256 as the MTU value in PTB messages.

Figure 1 shows the fraction of IPv4 webservers that always set the DF bit in a long-term packet header trace collected at the border of the University of Waikato network [20]. In 2004, 94% of webservers always set the DF bit, independent of the website population changes when students are on campus. This is consistent with the default behavior of most operating systems to enable PMTUD.

The proportions of classifications are similar for the two vantage points, so the following analyses will use the data obtained from Hamilton. The proportion of webservers that do not send packets large enough to allow for a test using an MTU of 1480 is 18%, compared to 6% for an MTU of 1280. Table 2 shows that 10.8% of servers in our population advertise an MSS of 1380 bytes and send packets no larger than 1420 bytes. This MSS is associated with middleboxes that clamp the MSS of TCP connections to avoid PMTUD problems when the client is behind a tunnel. Table 2 also indicates that the PMTUD fail rate for these servers is four times higher than the fail rate of the other MSS values listed. Identifying the manufacturers of the middleboxes involved could help determine if there is a bug, as reducing the failure rate to 6.7% would drop the overall failure rate by 25%.

Figure 2 illustrates how the popularity of a webserver corresponds with the probability it will fail PMTUD with an MTU of 1280 bytes. As with Medina [1], we find that the most popular webservers are more likely to filter PTB messages than less popular ones, but the failure rate for the most popular 500 we measure is half the rate reported in 2004. Since servers that advertise an MSS of 1380 are more likely to fail at PMTUD, we investigated if this is more common among popular webservers. Figure 3 shows no correlation; in fact, the most popular 1000 webservers have the lowest fraction of 1380 MSS.

As noted in section 3.4, before each PMTUD test, we tested the webserver’s responsiveness to ping. Overall, 85% of the webservers that succeeded at PMTUD are responsive, but only 33% of those that failed PMTUD are, suggesting the presence of firewalls configured to deny by default. Of the systems that did not set the DF bit on any data packet, 50% are responsive, indicating a deliberate choice to disable PMTUD due to the presence of a firewall. Figure 4 shows how the popularity of a webserver influences the probability it will be responsive to ping. More popular webservers are more likely to be unresponsive.

Figure 5 shows the size of a segment observed after a PTB message is sent, regardless of the PMTUD classification. For the 576, 1280, and 1480 MTU tests, at least 50% of the segments are smaller than the MTU value. Figure 6 shows the delay between the first PTB sent and the action by the host.