they provide an additional failure mode.
3.3 Targets
We derive our targets from the Alexa Top 1,000,000 web-
sites list [11]. For our IPv4 tests, each vantage point inde-
pendently derives its list of IP addresses to probe from the
ﬁrst 50,000 entries. We do this so the appropriate addresses
for each vantage point will be tested if the location of the
vantage point inﬂuences the addresses supplied. We test all
IPv4 addresses the domain resolves to. We use wget [18] to
determine a suitable URL to use in our tests. If the default
page for the domain is at least 1600 bytes in size, we ask
for it when we test PMTUD behaviour. Otherwise, we use
wget to obtain all objects required to display the default
page that are on that webserver, and select the ﬁrst object
that is at least 1600 bytes in size, or the largest object avail-
able. The San Diego vantage point tests the behaviour of
45,752 addresses, the Hamilton vantage point tests 45,990
addresses, and 54,008 addresses in total are tested with an
intersection of 70% across 5,492 ASes.
For our IPv6 tests, we resolve the domains for the mil-
lion websites in the list for IPv6 addresses. For those with
IPv6 addresses, we resolve the domain for IPv4 addresses
as well to enable PMTUD behaviour comparison for these
dual-stacked webservers. The same 1,067 IPv6 addresses are
tested by all ﬁve vantage points.
3.4 Implementation
We implemented a TBIT-style PMTUD test in scamper, a
parallelised packet-prober [19]. For each measurement made
our tool records, in a single data unit, meta-data about the
test such as the URL, the server MSS seen and the MTU
value used, as well as all packets sent and received for that
test.
It is simple to ensure inferences are valid given the
packets recorded. We also implemented a driver to coor-
dinate scamper’s measurements. For each IP address, the
Server MSS Portion Fail rate (1280)
1460
1380
1400
1414
536
other
86.5%
10.8%
0.4%
0.3%
0.2%
1.8%
6.7%
27.1%
6.8%
6.9%
–
–
Table 2: Top ﬁve server MSS values advertised and
their corresponding failure rate. 10.8% of the popu-
lation advertise an MSS of 1380, and these are four
times more likely to fail at PMTUD.
driver begins by sending up to four ping packets to see if
the network allows any ICMP through. Then, it does the
sequence of PMTUD tests towards the webserver one at a
time, testing the MTU values from highest to lowest. The
driver waits at least one minute between PMTUD tests to
each unique IP address to avoid being a nuisance.
4.
IPV4 PMTUD BEHAVIOUR
Table 1 shows the overall results of our IPv4 tests, with the
ﬁnal column containing corresponding classiﬁcations from [1].
We see a similar failure rate in 2010 for the tests with a 256
byte MTU as was seen in the 2004 study. However, the
failure rate is much lower for the other MTU values tested.
Compared with the 2004 study we measure a success rate
of 78–80% which is nearly double the 2004 result, and the
number of servers we measure that do not set the DF bit on
data packets is nearly an order of magnitude less than the
2004 result. We believe the results are diﬀerent in the 2004
study because of a bug in TBIT that classiﬁes hosts that
clear the DF bit as not attempting PMTUD, and because of
the choice of 256 as the MTU value to use in PTB messages.
Figure 1 shows the fraction of webservers that always set the
DF bit in a long-term packet header trace collected at the
border of the University of Waikato network [20]. In 2004,
94% of webservers always set the DF bit, and is indepen-
dent of the website population changing when students are
on campus. This is consistent with the behaviour of most
operating systems to enable PMTUD by default.
The proportions of classiﬁcations are similar for the two
vantage points, so the following analyses will use the data
obtained from Hamilton. The proportion of webservers that
104 120
 100
 80
 60
 40
 20
)
s
d
n
a
s
u
o
h
t
(
y
c
n
e
u
q
e
r
F
 0
Jul
2003
DF not set
DF set
Fraction with DF set
Jan
2004
Jul
2004
Jan
2005
Jul
2005
Jan
2006
Jul
2006
Jan
2007
Jul
2007
Jan
2008
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
Jul
2008
t
e
s
F
D
h
t
i
w
n
o
i
t
c
a
r
F
Figure 1: IPv4 webservers that send data packets with the DF bit set over time as measured by packet traces
at the edge of the University of Waikato network. Each point represents one week. The bottom series is
the number of webservers that never set the DF bit on any data packet. The middle series is the number of
webservers that set the DF bit on all data packets. The top series is the fraction of these webservers that
always set the DF bit; the fraction grows from 94% in 2004 to 96% by 2008.
n
o
i
t
c
a
r
F
 0.16
 0.14
 0.12
 0.10
 0.08
 0.06
 0.04
 0.02
 0
 0
 5
 10  15  20  25  30  35  40  45  50
Position in Top 50k
n
o
i
t
c
a
r
F
 0.16
 0.14
 0.12
 0.10
 0.08
 0.06
 0.04
 0.02
 0
 0
 5
 10  15  20  25  30  35  40  45  50
Position in Top 50k
Figure 2: Fraction of IPv4 webservers that fail at
PMTUD with a 1280 byte MTU in buckets of 1000,
by popularity. The line is a B´ezier curve showing
the trend that the more popular a webserver is the
more likely it is to fail at PMTUD.
Figure 3: Fraction of IPv4 webservers that advertise
an MSS of 1380, ordered by popularity in buckets of
1000. There is no correlation evident between the
popularity of a webserver and the advertisement of
1380.
do not send packets large enough to allow for a test using
an MTU of 1480 is 18%, compared with 6% for an MTU of
1280. Table 2 shows why: 10.8% of servers in our population
advertise an MSS of 1380 bytes and send packets no larger
than 1420 bytes. This MSS is associated with middleboxes
that clamp the MSS of TCP connections they pass to avoid
PMTUD problems when the client is behind a tunnel. Ta-
ble 2 also shows the PMTUD fail rate associated with these
servers, which is 4 times higher than the fail rate of the other
MSS values listed. It seems worthwhile to identify the man-
ufacturers of the middleboxes involved to determine if there
is a bug involved, as if these webservers had a failure rate
of 6.7% like most of the rest of the population the overall
failure rate would drop 25%.
Figure 2 shows how the popularity of a webserver cor-
responds with the probability it will fail PMTUD with an
MTU of 1280 bytes. As with Medina [1], we ﬁnd the most
popular webservers are much more likely to ﬁlter PTB mes-
sages than webservers that are less popular, but the failure
rate for the most popular 500 we measure is half the rate
reported in 2004. Because servers that advertise an MSS of
1380 are more likely to fail at PMTUD, we wondered if this
was more likely to be a feature of the popular webservers.
Figure 3 shows there is no correlation evident; in fact the
most popular 1000 webservers have the lowest fraction of
1380 MSS.
As noted in section 3.4, before we commenced each PM-
TUD test, we tested the webserver’s responsiveness to ping.
Overall, 85% of the webservers that succeeded at PMTUD
are responsive, but only 33% of those which failed PMTUD
are, suggesting the presence of ﬁrewalls conﬁgured to deny
by default. Of the systems that did not set the DF bit on
any data packet, 50% are responsive, indicating a deliber-
ate choice to disable PMTUD because of the presence of a
ﬁrewall. Figure 4 shows how the popularity of a webserver
inﬂuences the probability it will be responsive to ping. More
popular webservers are more likely to be unresponsive.
Figure 5 shows the size of a segment observed after a
PTB message is sent regardless of the PMTUD classiﬁca-
tion made. For the 576, 1280, and 1480 MTU tests, at least
105n
o
i
t
c
a
r
F
 0.30
 0.28
 0.26
 0.24
 0.22
 0.20
 0.18
 0.16
 0.14
 0
 5
 10  15  20  25  30  35  40  45  50
Position in Top 50k
 1
n
o
i
t
c
a
r
f
e
v
i
t
a
l
u
m
u
C
 0.8
 0.6
 0.4
 0.2
 0
Clear DF
576
1280
1240
 0  3  6  9  12 15 18 21 24 27 30 33 36 39 42 45 48
Seconds since first PTB
p
Figure 4: Fraction of IPv4 webservers that are not
responsive to ping, ordered by popularity in buckets
of 1000. The line is a B´ezier curve showing the trend
that the more popular webservers are more likely to
be unresponsive to ping.
Figure 6: The delay between the ﬁrst PTB sent and
the action by the host. The delays measured in re-