(based on phase1).
We wait for another 20 days to monitor user clicks.
After the study, we send a debrieﬁng email which ex-
plains the true purpose of the experiment and obtains the
informed consent. Participants can withdraw their data
anytime. By the time of our submission, none of the
users have requested to withdraw their data.
Security Indicators.
Based on our previous mea-
surement results, most email services adopted text-based
indicators (Figure 6(b)-(i)). Even Gmail’s special indica-
tor (Figure 6(a)) will display a text message when users
move the mouse over. To this end, we use the text-based
indicator and make two settings, namely with security
(a) Without Security Indicator
(b) With Security Indicator
Figure 8: The phishing email screenshot.
indicator and without security indicator. For the group
without security indicator, we recruit users from Yahoo
Mail. We choose Yahoo Mail users because Yahoo Mail
is the largest email service that has not implemented any
security indicators. For the comparison group with se-
curity indicator, we still recruit Yahoo Mail users for
consistency, and add our own security indicators to the
interface. More speciﬁcally, when sending emails, we
can embed a piece of HTML code in the email body to
display a text-based indicator. This is exactly how most
email providers insert their visual indicators in the email
body (except for Gmail).
In phase2, we cannot control if a user would use the
mobile app or the website to read the email. This is not a
big issue for Yahoo Mail users. Yahoo’s web and mobile
clients both render HTML by default. The text-based
indicator is embedded in the email body by us, which
will be displayed consistently for both web and mobile
users (conﬁrmed by our own tests).
Recruiting Participants.
To collect enough data
points from phase 2, we need to recruit a large number
of users given that many users may not open our email.
We choose Amazon Mechanical Turk (MTurk), the most
popular crowdsourcing platform to recruit participants.
MTurk users are slightly more diverse than other Inter-
net samples as well as college student samples. Using
Amazon Mechanical Turk may introduce biases in terms
of the user populations. However, the diversity is report-
edly better than surveying the university students [9]. To
avoid non-serious users, we apply the screening criteria
USENIX Association
27th USENIX Security Symposium    1105
Phase
Phase1
Phase2
Click Rate
Users
All Participants
Not Block Pixel
Opened Email
Clicked URL
Overall
After Open Email
w/o Indict. w/ Indict.
243
176
94
46
26.1%
48.9%
245
179
86
32
17.9%
37.2%
Table 5: User study statistics.
that are commonly used in MTurk [10, 28]. We recruit
users from the U.S. who have a minimum Human Intel-
ligence Task (HIT) approval rate of 90%, and more than
50 approved HITs.
In total, we recruited N = 488 users from MTurk: 243
users for the “without security indicator” setting, and
another 245 users for the “with security indicator” set-
ting. Each user can only participate in one setting for
only once to receive $0.5. In the recruiting letter, we ex-
plicitly informed the users that we need to collect their
email address. This may introduce self-selection biases:
we are likely to recruit people who are willing to share
their email address with our research team. Despite the
potential bias, that the resulting user demographics are
quite diverse: 49% are male and 51% are female. Most
participants are 30–39 years old (39.1%), followed by
users under 29 (31.8%), above 50 (14.5%), and 40–49
(14.5%). Most of the participants have a bachelor degree
(35.0%) or a college degree (33.8%), followed by those
with a graduate degree (20.7%) and high-school gradu-
ates (10.5%).
Ethics Guidelines.
Our study received IRB approval,
and we have taken active steps to protect the participants.
First, only benign URLs are placed in the emails which
point to our own server. Clicking on the URL does not in-
troduce practical risks to the participants or their comput-
ers. Although we can see the participant’s IP, we choose
not to store the IP information in our dataset. In addition,
we followed the recommended practice from IRB to con-
duct the deceptive experiment. In the experiment instruc-
tion, we omit information only if it is absolutely neces-
sary (e.g., the purpose of the study and details about the
second email). Revealing such information upfront will
invalidate our results. After the experiment, we immedi-
ately contact the participants to explain our real purpose
and the detailed procedure. We offer the opportunity for
the participants to opt out. Users who opt-out still get the
full payment.
6.2 Experiment Results
We analyze experiment results to answer the following
questions. First, how effective are security indicators in
Users
w/o Indicator
w/ Indicator
Opened Email
Clicked URL
Click Rate
Desktop Mobile Desktop Mobile
45
21
46.7%
49
25
51.0%
41
15
36.6%
45
17
37.8%
Table 6: User study statistics for different user-agents.
protecting users? Second, how does the impact of secu-
rity indicators vary across different user demographics?
Click-through Rate.
Table 5 shows the statistics
for the phishing results. For phase-2, we calculate two
click-through rates. First, out of all the participants that
received the phishing email, the click-through rate with
security indicator is 32/179=17.9%. The click-through
rate without security indicator is higher: 46/176=26.1%.
However, this comparison is not entirely fair, because
many users did not open the email, and thus did not even
see the security indicator at all.
In order to examine the impact of the security indi-
cator, we also calculate the click-through rate based on
users who opened the email. More speciﬁcally, we sent
phishing emails to the 176 and 179 users who did not
block tracking pixels, and 94 and 86 of them have opened
the email. This returns the email-opening rate of 53.4%
and 48.9%. Among these users, the corresponding click-
through rates are 48.9% (without security indicator) and
37.2% (with security indicator) respectively. The results
indicate that security indicators have a positive impact
to reduce risky user actions. When the security indi-
cator is presented, the click rate is numerically lower
compared to that without security indicators. The differ-
ence, however, is not very signiﬁcant (Fisher’s exact test
p = 0.1329). We use Fisher’s exact test instead of the
Chi-square test due to the relatively small sample size.
The result suggests that the security indicator has a mod-
erately positive impact.
User Agents.
In our experiment, we have recorded
the “User-Agent” when the user opens the email, which
helps to infer the type of device that a user was using to
check the email. Recall that no matter what device the
user was using, our security indicator (embedded in the
email body) will show up regardless. Table 6 shows that
mobile users are more likely to click on the phishing link
compared with desktop users, but the difference is not
signiﬁcant.
Demographic Factors.
In Figure 9, we cross-
examine the results with respect to the demographic fac-
tors. To make sure each demographic group contains
enough users, we create binary groups for each factor.
For “education level”, we divide users into High-Edu
(bachelor degree or higher) and Low-Edu (no bachelor
degree). For “age”, we divide users into Young (age=40). The thresholds are chosen so that
the two groups are of relatively even sizes. As shown
in Figure 9, the click rates are consistently lower when
a security indicator is presented for all the demographic
groups. The differences are still insigniﬁcant. Fisher’s
exact test shows that the smallest p = 0.06, which is pro-
duced by the low-edu group. Overall, our result conﬁrms
the positive impact of the security indicator across dif-
ferent user demographics, and also suggests the impact
is limited. The security indicator alone is not enough to
mitigate the risk.
7 Discussion
In this section, we summarize our results and discuss
their implications for defending against email spooﬁng
and broadly spear phishing attacks. In addition, we dis-
cuss the new changes made by the email services after
our experiment, and our future research directions.
7.1
Implications of Our Results
Email Availability vs. Security.
Our study shows
many email providers choose to deliver a forged email to
the inbox even when the email fails the authentication.
This is a difﬁcult trade-off between security and email
availability. If an email provider blocks all the unveriﬁed
emails, users are likely to lose their emails (e.g., from
domains that did not publish an SPF, DKIM or DMARC
record). Losing legitimate emails is unacceptable for
email services which will easily drive users away.
The challenge is to accelerate the adoption of SPF,
DKIM and DMARC. Despite the efforts of the Internet
Engineering Task Force (IETF), these protocols still have
limitations to handle special email scenarios such as mail
forwarding and mailing lists, creating further obstacles to
a wide adoption [40, 19, 37]. Our measurement shows a
low adoption rate of SPF (44.9%) and DMARC (5.1%)
among the Internet hosts. From the email provider’s
perspective, the ratio of unveriﬁed inbound emails is
likely to be lower since heavy email-sending domains
USENIX Association
27th USENIX Security Symposium    1107
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7MaleFemaleâ€œâ€(cid:157)L-EduH-Eduâ€œâ€(cid:157)YoungOldClick RateWithout Security IndicatorWith Security IndicatorFigure 10: Gmail’s new warning message for same-
domain spooﬁng.
ent sending scenarios. For both ARC and BIMI, they are
likely to face the same challenge to be widely adopted
just like DMARC (standardized in 2015).
7.2 UI Updates from Email Services
A few email services have updated their user interfaces
during January – June in 2018. Particularly, after we
communicate our results to the Gmail team, we notice
some major improvements. First, when we perform the
same-domain spooﬁng (i.e., spooﬁng a Gmail address),
in addition to the question-mark sign, there is a new
warning message added to the email body as shown
in Figure 10. Second, the new mobile Gmail app no
longer displays the “misleading” proﬁle photos on un-
veriﬁed messages (regardless spooﬁng existing contact
or the same-domain account). The same changes are ap-
plied to the new Google Inbox app too. However, the
mobile clients are still not as informative as the web ver-
sion. For example, there is no explanation message on
the question-mark sign on the mobile apps. In addition,
the new warning message (Figure 10) has not been con-
sistently added to the mobile apps either.
Inbox.lv has launched its mobile app recently. Like
its web version, the mobile app does not provide a secu-
rity indicator. However, the UI of the mobile app is sim-
pliﬁed which no longer loads misleading elements (e.g.,
proﬁle photos) for unveriﬁed emails. Yahoo Mail and
Zoho also updated their web interfaces but the updates
were not related to security features.
7.3 Open Questions & Limitations
Open Questions.
It is unlikely that the email spoof-
ing problem can quickly go away given the slow adop-
tion rate of the authentication protocols. Further research
is needed to design more effective indicators to maxi-
mize its impact on users. Another related question is
how to maintain the long-term effectiveness of security
indicators and overcome the “warning fatigue” [8]. Fi-
nally, user training/education will be needed to teach
users how to interpret the warning message, and han-
dle questionable emails securely. For security-critical
users (e.g., journalists, government agents, military per-
sonnel), an alternative approach is to use PGP to prevent
email spooﬁng [29]. Extensive work is still needed to
make PGP widely accessible and usable for the broad In-
ternet population [30, 48].
Study Limitations.
Our study has a few limita-
tions. First, our measurement only covers public email
services. Future work will explore if the conclusion also
applies to non-public email services. Second, while we
have taken signiﬁcant efforts to maintain the validity of
the phishing test, there are still limits to what we can con-
trol. For ethical considerations, we cannot fully scale-up
the experiments beyond the 488 users, which limited the
number of variables that we can test. Our experiment
only tested a binary condition (with or without a security
indicator) on one email content. Future work is needed to
cover more variables to explore the design space such as
the wording of the warning messages, the color and the
font of the security indicator, the phishing email content,
and the user population (e.g., beyond the MTurk and Ya-
hoo Mail users). Finally, we use “clicking on the phish-
ing URL” as a measure of risky actions, which is still
not the ﬁnal step of a phishing attack. However, tricking
users to give way their actual passwords would have a
major ethical implication, and we decided not to pursue
this step.
8 Related Work
Email Conﬁdentiality, Integrity and Authenticity.
SMTP extensions such as SPF, DKIM, DMARC and
STARTTLS are used to provide security properties for
email transport. Recently, researchers conducted de-
tailed measurements on the server-side usage of these
protocols [23, 27, 34, 36]. Unlike prior work, our work
shows an end-to-end view and demonstrate the gaps be-
tween server-side spooﬁng detection and the user-end
notiﬁcations. Our study is complementary to existing
work to depict a more complete picture.
Email Phishing.
Prior works have developed phish-
ing detection methods based on features extracted from
email content and headers [20, 22, 26, 35, 51, 57].
Phishing detection is different from spam ﬁltering [58]
because phishing emails are not necessarily sent
in
bulks [65] but can be highly targeted [33]. Other than
spooﬁng, attackers may also apply typosquatting or uni-
code characters [6] to make the sender address appear
similar (but not identical) to what they want to imperson-
ate. Such sender address is a strong indicator of phishing
which has been used to detect phishing emails [42, 44].
Another line of research focuses on the phishing web-
site, which is usually the landing page of the URL in a
phishing email [18, 32, 63, 68, 71, 72].
Human factors (demographics, personality, cognitive
biases, fatigue) would affect users response to phish-
ing [52, 31, 38, 53, 60, 64, 66, 69, 16, 47]. The
1108    27th USENIX Security Symposium
USENIX Association
study results have been used to facilitate phishing train-
ing [67]. While most of these studies use the “role-
playing” method, where users read phishing emails in
the simulated setting. There are rare exceptions [38, 52]
where the researchers conducted a real-world phishing
experiment. Researchers have demonstrated the behav-