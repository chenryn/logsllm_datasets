### Monitoring and Data Collection

Following the initial phase, we wait for an additional 20 days to monitor user clicks. After the study, we send a debriefing email that explains the true purpose of the experiment and obtains informed consent. Participants have the option to withdraw their data at any time. As of our submission, no users have requested to withdraw their data.

### Security Indicators

Based on our previous measurement results, most email services use text-based indicators (Figure 6(b)-(i)). Even Gmail’s special indicator (Figure 6(a)) displays a text message when users hover over it with their mouse. Therefore, we use a text-based indicator and create two settings: one with a security indicator and one without. For the group without a security indicator, we recruit Yahoo Mail users, as Yahoo Mail is the largest email service that does not implement any security indicators. For the comparison group with a security indicator, we again recruit Yahoo Mail users for consistency and add our own security indicators to the interface. Specifically, we embed a piece of HTML code in the email body to display the text-based indicator, which is how most email providers insert visual indicators in the email body (except for Gmail).

In Phase 2, we cannot control whether a user will read the email via the mobile app or the website. This is not a significant issue for Yahoo Mail users, as both the web and mobile clients render HTML by default. The text-based indicator embedded in the email body will be displayed consistently for both web and mobile users, as confirmed by our tests.

### Recruiting Participants

To collect sufficient data points from Phase 2, we need to recruit a large number of users, given that many may not open our email. We use Amazon Mechanical Turk (MTurk), the most popular crowdsourcing platform, to recruit participants. MTurk users are more diverse than other Internet samples and college student samples. However, using MTurk may introduce biases in terms of user demographics. To mitigate this, we apply common screening criteria used in MTurk [10, 28]. We recruit users from the U.S. who have a minimum Human Intelligence Task (HIT) approval rate of 90% and more than 50 approved HITs.

In total, we recruited N = 488 users from MTurk: 243 for the "without security indicator" setting and 245 for the "with security indicator" setting. Each user can only participate in one setting once and receives $0.50. In the recruiting letter, we explicitly inform users that we need to collect their email addresses, which may introduce self-selection bias. Despite this, the resulting user demographics are quite diverse: 49% are male and 51% are female. Most participants are aged 30-39 (39.1%), followed by those under 29 (31.8%), above 50 (14.5%), and 40-49 (14.5%). The majority of participants have a bachelor's degree (35.0%) or a college degree (33.8%), followed by those with a graduate degree (20.7%) and high school graduates (10.5%).

### Ethics Guidelines

Our study received IRB approval, and we have taken active steps to protect participants. First, only benign URLs pointing to our own server are included in the emails, ensuring that clicking on them does not pose any practical risks to participants or their computers. Although we can see the participant's IP address, we choose not to store this information in our dataset. Additionally, we follow IRB-recommended practices for conducting deceptive experiments. In the experiment instructions, we omit information only if absolutely necessary (e.g., the purpose of the study and details about the second email). Revealing such information upfront would invalidate our results. After the experiment, we immediately contact participants to explain the real purpose and detailed procedure, offering them the opportunity to opt out. Users who opt out still receive full payment.

### Experiment Results

We analyze the experiment results to answer the following questions: 
1. How effective are security indicators in protecting users?
2. How does the impact of security indicators vary across different user demographics?

#### Click-through Rate

Table 5 shows the statistics for the phishing results. For Phase 2, we calculate two click-through rates. Out of all participants who received the phishing email, the click-through rate with a security indicator is 32/179 = 17.9%, while the rate without a security indicator is 46/176 = 26.1%. However, this comparison is not entirely fair because many users did not open the email and thus did not see the security indicator at all.

To examine the impact of the security indicator, we also calculate the click-through rate based on users who opened the email. Specifically, we sent phishing emails to 176 and 179 users who did not block tracking pixels, and 94 and 86 of them opened the email, respectively. This yields an email-opening rate of 53.4% and 48.9%. Among these users, the corresponding click-through rates are 48.9% (without security indicator) and 37.2% (with security indicator). The results indicate that security indicators have a positive impact on reducing risky user actions. When the security indicator is presented, the click rate is numerically lower compared to that without security indicators. The difference, however, is not statistically significant (Fisher’s exact test p = 0.1329). We use Fisher’s exact test instead of the Chi-square test due to the relatively small sample size. The result suggests that the security indicator has a moderately positive impact.

#### User Agents

In our experiment, we recorded the "User-Agent" when users opened the email, which helps infer the type of device they were using. Regardless of the device, our security indicator (embedded in the email body) will show up. Table 6 shows that mobile users are more likely to click on the phishing link compared to desktop users, but the difference is not significant.

#### Demographic Factors

In Figure 9, we cross-examine the results with respect to demographic factors. To ensure each demographic group contains enough users, we create binary groups for each factor. For "education level," we divide users into High-Edu (bachelor's degree or higher) and Low-Edu (no bachelor's degree). For "age," we divide users into Young (age < 40) and Old (age ≥ 40). The thresholds are chosen to ensure relatively even group sizes. As shown in Figure 9, the click rates are consistently lower when a security indicator is presented for all demographic groups. The differences are still insignificant. Fisher’s exact test shows that the smallest p = 0.06, which is produced by the low-edu group. Overall, our results confirm the positive impact of the security indicator across different user demographics, though the impact is limited. The security indicator alone is not enough to mitigate the risk.

### Discussion

#### Implications of Our Results

**Email Availability vs. Security:** Our study shows that many email providers deliver forged emails to the inbox even when the email fails authentication. This is a difficult trade-off between security and email availability. Blocking all unverified emails could lead to the loss of legitimate emails, which is unacceptable for email services. Accelerating the adoption of SPF, DKIM, and DMARC is challenging, as these protocols have limitations in handling special email scenarios such as mail forwarding and mailing lists. Our measurement shows a low adoption rate of SPF (44.9%) and DMARC (5.1%) among Internet hosts.

#### UI Updates from Email Services

A few email services updated their user interfaces during January-June 2018. Notably, after communicating our results to the Gmail team, we observed major improvements. For same-domain spoofing, a new warning message was added to the email body (Figure 10). The new mobile Gmail app no longer displays misleading profile photos on unverified messages. However, the mobile clients are still not as informative as the web version. For example, there is no explanation message on the question-mark sign in the mobile apps, and the new warning message has not been consistently added to the mobile apps.

Inbox.lv recently launched its mobile app, which, like its web version, does not provide a security indicator. However, the mobile app's UI is simplified and no longer loads misleading elements for unverified emails. Yahoo Mail and Zoho also updated their web interfaces, but the updates were not related to security features.

#### Open Questions & Limitations

**Open Questions:** Further research is needed to design more effective indicators to maximize their impact on users. Another question is how to maintain the long-term effectiveness of security indicators and overcome "warning fatigue." User training and education are essential to teach users how to interpret warning messages and handle questionable emails securely. For security-critical users, an alternative approach is to use PGP to prevent email spoofing, though extensive work is needed to make PGP widely accessible and usable.

**Study Limitations:** Our study has several limitations. First, our measurement only covers public email services. Future work will explore if the conclusions apply to non-public email services. Second, while we took significant efforts to maintain the validity of the phishing test, ethical considerations limit the scale of the experiments. Our experiment only tested a binary condition (with or without a security indicator) on one email content. Future work is needed to cover more variables, such as the wording of warning messages, the color and font of the security indicator, and the phishing email content. Finally, we use "clicking on the phishing URL" as a measure of risky actions, which is not the final step of a phishing attack. Tricking users into giving away their actual passwords would have major ethical implications, so we decided not to pursue this step.

### Related Work

**Email Confidentiality, Integrity, and Authenticity:** SMTP extensions such as SPF, DKIM, DMARC, and STARTTLS provide security properties for email transport. Recent research has conducted detailed measurements on the server-side usage of these protocols. Unlike prior work, our study provides an end-to-end view and demonstrates the gaps between server-side spoofing detection and user-end notifications.

**Email Phishing:** Prior works have developed phishing detection methods based on features extracted from email content and headers. Phishing detection differs from spam filtering because phishing emails are not necessarily sent in bulk but can be highly targeted. Other than spoofing, attackers may use typosquatting or Unicode characters to make the sender address appear similar to what they want to impersonate. Research on phishing websites, which are usually the landing pages of URLs in phishing emails, is another area of focus.

**Human Factors:** Demographics, personality, cognitive biases, and fatigue affect users' responses to phishing. These study results have been used to facilitate phishing training. While most studies use the "role-playing" method, some researchers have conducted real-world phishing experiments.