⇒
Condition
¬csm_all(s, r),
s: ConstString
csm_all(s, r),
s: ConstString
[si]=csm_hd(s1, r1),
[sj]=csm_tl(s2, r2)
[si]=csm_hd(s, r)
[si]=csm_tl(s, r)
r
[si]=
[sj]=
csm_hd(r3, r1),
csm_tl(r4, r2)
r
[si]=
csm_hd(r2, r1)
r
[si]=
csm_tl(r2, r1)
r
i=1
⇒ E1·E·E2
Table 4: Selected Reduction Rules for star Functions
E is a concatenation
constant
among
star
strings
functions
and
give the details of reduction for replaceAll. Other operations can
be treated in a similar manner.
As stated earlier, we aim to support the most general usage of
replace function – replacing all occurrences. In practice, there
is also another version (e.g. in PHP) which allows users to spec-
ify the maximum number of occurrences to be replaced. We call it
replaceN, to distinguish the two versions. In fact, replaceN is al-
ready supported by existing solvers, e.g., Kaluza. The typical treat-
ment is to model the input parameter as a concatenation of N parts,
and then apply one replacement to each part. However, this tech-
nique cannot be generalized to address replaceAll, since we do not
know such an N beforehand. Here we propose to model both re-
placeAll and replaceN, again, using recursively-deﬁned functions.
In fact, restricting to replaceN alone, our approach will be more
efﬁcient than Kaluza’s. This efﬁciency comes from the superiority
of incremental solving (via constrain-and-generate approach) over
generate-and-test approach.
Since replaceN is a special case of replaceAll, we focus on dis-
cussing only the latter. Table 7 shows that R=replaceAll(S, r, T)
belongs to one of two possible cases:
• the recursive case, when we ﬁnd a substring M, that matches
regular expression r, at an index I. We then can replace M by
T and continue to apply replaceAll function on the remaining
part S1 until we reach the base case.
• the base case, when we cannot ﬁnd any substring that satis-
ﬁes such condition. The resulting string R is then the same as
the input string S.
The replaceAll function will use search function to ﬁnd the index
of substring M=M1·M2 in S. Speciﬁcally, this auxiliary function takes
as input a symbolic string input S, a regular expression r, and re-
turns the starting index I of a substring in S that matches r. If there
exists no such substring, it returns a negative number. Otherwise, it
returns the index of the substring M that satisﬁes the condition.
We remark that the second parameter of replaceAll function can-
not be a variable since in such case, the behavior of this function
is undeﬁned. Naively, we can keep unfolding recursively-deﬁned
function replaceAll, until we can decide if the current formula is
satisﬁable or not. However, we provide reduction rules (unfolding
on demand) for them instead. For presentation purpose, Table 6
lists only two reduction rules for the case when the preﬁx of the
ﬁrst parameter S is known5. In rule [RED−1], the preﬁx of S is a
constant string s, while it is star(s, n) in rule [RED−2]. In both
cases, since the preﬁx is already known, we are able to apply the
replacement on the part s (star(s, n) in the other case) via auxiliary
function rep. In rule [RED−1], suppose that S is composed by s
and R, function rep(s, r, T ) replaces all occurrences in s, match-
5Other rules related to the second, the third parameter, the result
and their combinations are constructed similarly.
Step
Fact added
1
2
3
4
5
nM = “ababababababcc”
p1 = star(“ab”,n1)
p2 = star(“bc”,n2)
res=
star(“ab”,n1)·star(“bc”,n2)
star(“ab”,n1) · star(“bc”,n2) =
“ababababababcc”
res,
Eq-class
{“ababababababcc”,
nM, p1 · p2}
{p1, star(“ab”,n1)}
{p2, star(“bc”,n2)}
{“ababababababcc”,
res, nM,
star(“bc”,n2), p1 · p2}
{“ababababababcc”,
res, nM,
star(“bc”,n2), p1 · p2}
UNSAT
star(“ab”,n1)·
star(“ab”,n1)·
Reduction/Action
[REP−(cid:63)]: res = “ababababababcc”
[REP−(cid:63)]: res = star(“ab”,n1) · p2
[REP−(cid:63)]: res = star(“ab”,n1) · star(“bc”,n2)
[REP−(cid:63)]:star(“ab”,n1)·star(“bc”,n2)=
“ababababababcc”
star(“ab”,n1)·star(“bc”,n2)=“ababababababcc”⇒
¬star(“ab”,n1)·star(“bc”,n2)=“ababababababcc”
Table 5: A Solving Procedure for the Motivating Example in Fig. 3
Rule
[RED−1]
[RED−2]
replaceAll(s·R, r, T )=U
⇒ V ·replaceAll(t·R, r, T )=U
replaceAll(star(s, n)·R, r, T )=U ⇒ V ·replaceAll(t·R, r, T )=U (V, t)=rep(star(s, n), r, T )
Condition
(V, t)=rep(s, r, T )
Reduction
Table 6: Reduction Rules for replaceAll Functions
ing the regular expression r, by T . It then returns the pair (V, t)
such that replaceAll(s, r, T )=V ·t, where t is guaranteed to be
the longest sufﬁx of s that must be examined together with R in the
next step replaceAll(t·R, r, T ). The application of rep for the
case star(s, n) is similar to s except that V is parameterized by n.
Now, we illustrate how this auxiliary function can be applied via
two examples. In the ﬁrst example:
replaceAll(“abcd”·R, “ab”, T ) = U
the rep(“abcd”, “ab”, T ) method will return (T·“cd”, “”). In the
second one:
replaceAll(“abcd”·R, (“ab” + “de”), T ) = U
it will return (T·“c”, “d”) since it is possible that R starts with
character ‘e’.
Length constraints. We have inherited rules from Z3-str, to infer
length constraints such as X=Y → length(X)=length(Y ).
Due to space limit, we do not include them in our paper. Impor-
tantly, the unfolding of recursive functions (star, replaceAll, etc.)
would incrementally expose more concrete (sub)strings and there-
fore the interactions from Z3-str-(cid:63) to the Arithmetic Solver module
in Z3 also happen incrementally.
In addition, as stated in Sec. 4.2, the length constraints, in the
feedback from the Arithmetic Solver module, can also be used to
prune the search space in string theory component, Z3-str-(cid:63). For
example, when the Arithmetic Solver module can deduce concrete
values for length variables, Z3-str-(cid:63) will be able to make use of
such information.
6. EVALUATION
In our experimental evaluation, we conduct two case studies to
compare S3 with state-of-the-art string solvers. All experiments are
run on an 3.2GHz machine with 8GB memory.
In Section 3, we stated that constraint solvers, which work only
on string domain or only on non-string domain, are not effective for
analyzing web applications. Thus, it is sufﬁcient for us to compare
S3 only with Kaluza and Z3-str.
6.1 Comparison with Kaluza
In this case study, we use the set of (50,000+) benchmarks that
is shipped with Kaluza, which can be downloaded at:
http://webblaze.cs.berkeley.edu/2010/kaluza
They were generated using Kudzu [28], a symbolic execution frame-
work for JavaScript. Since our solver can parse these generated
constraints directly, it is straight-forward to plug S3 into Kudzu.
The Kaluza benchmarks are classiﬁed into two:
• SAT, where Kaluza ﬁnds a satisfying solution and returns
YES.
• UNSAT, where Kaluza cannot ﬁnd any solutions and returns
NO or it ends up with errors.
For each category, the benchmarks are further divided into two
groups: small and big, based on the size of the ﬁles. We note that
the classiﬁcation is done by [28]; such classiﬁcation is not necessar-
ily accurate in reality. In fact, after rectifying totally 1 ﬁle in SAT
category and 4057 ﬁles in UNSAT category, those having parsing
errors (due to incorrect syntax or types), Kaluza can now answer
YES (means satisﬁable) for 2894 out of 4057 previously UNSAT
cases. We will elaborate on this later.
#Files
19984
1835
11761
Time(s)
S3
267
166
173
K/S3
19.4x
19.0x
26.2x
K
5190
3165
4532
SAT/Small
SAT/Big
UNSAT/Small
Table 8: Timing Comparison: S3 vs. Kaluza
We ﬁrst consider a timing comparison. We ran both Kaluza (col-
umn K) and S3 on all the benchmarks in the SAT category, as
well the small benchmarks in the UNSAT category. The reason
for omitting the large benchmarks in the UNSAT category is that
often Kaluza fails to ﬁnd a deﬁnitive answer here (due to crashing
or timing out, after 1 minute), and therefore it is not meaningful
to compare with its timing. The results, which are summarized in
Table 8, clearly show that S3 is much faster, by a factor 19 or more.
In the next experiment, we consider something of perhaps greater
importance: robustness. Roughly speaking, this measures how of-
ten a solver is able to provide a deﬁnitive answer. This, in turn,
means that if the solver returns YES, then it should produce a par-
ticular model which demonstrates the executability of the path in
question.
If the solver returns NO, then it should mean that the
path in question is in fact not executable. There is of course a third
possible answer, MAYBE, which is not deﬁnitive (and which is the
cause of false alarms). A robust system therefore is one which re-
turns deﬁnitive answers often.
As mentioned above, the Kaluza benchmarks are categorized
into SAT and UNSAT.
In the SAT category (of 19984 + 1835 = 21819 benchmarks),
S3 ﬁnds 6 of them are unsatisﬁable. By a careful investigation, it
is in fact the case. Moreover, S3 successfully reports YES on all
the benchmarks (excluding the 6 which are wrongly classiﬁed), and
further, the complete models, returned by S3, are cross-checked by
running them using Kaluza.
Next, we use S3 to cross-check the models produced by Kaluza.
Since in Kaluza, each query must specify a variable, for which they
will generate the model, in our setting we tested with the vari-
able that starts with ‘var’6. As a result, Kaluza has errors with
11 benchmarks that do not have any variable starting with ‘var’.
For the other 21808 benchmarks, it reports a YES answer. How-
ever, in 695 out of 21813 indeed satisﬁable benchmarks, the model
returned by Kaluza is incomplete. Because Kaluza only returns the
model for one variable, it is possible that the return model for the
chosen variable may not be extensible to become a complete model
which includes other variables. In short, these 695 models are in
fact not really models that are useful to reproduce attacks.
(We
note that [39] has previously remarked this “semi-soundness" issue
of Kaluza.)
Table 9 shows statistics for 33230 (11761 + 21469) benchmarks
in the UNSAT category. Kaluza reports a YES answer for 2894 out
of 4057 rectiﬁed benchmarks. For other ﬁles (including the other
rectiﬁed ﬁles), Kaluza is not reporting a NO answer to all bench-
marks therein; rather, the answer is MAYBE most of the times. In
fact, more than half (18210) of the benchmarks in this category was
determined by S3 as SATISFIABLE! Again, the return models are
conﬁrmed by running them using Kaluza. This means that S3 has
much more potential for vulnerability detection than Kaluza does.
NO
YES
ERROR
TIMEOUT (1 min)
MAYBE
S3
14877
18210
0
0
143
Kaluza
7124
2894
22653
559
0
Table 9: S3 vs. Kaluza on UNSAT Category
Note that in Table 9 we no longer distinguish between big and
small programs. The frequency of Kaluza crashing (ERROR) or
timing out (TIMEOUT, after 1 minute), as opposed to saying NO,
is extremely high. Also note that, 2894 ﬁles that Kaluza reports
SAT (YES) only belong to rectiﬁed ones. Moreover, Kaluza often
(about 70% of the time) returns a non-deﬁnitive answer, either by
crashing or timing out. In contrast, S3 returns a deﬁnitive answer
much more often.
In summary for this study, we have ﬁrst shown that S3 is far more
efﬁcient than Kaluza using its own impressive set of (50, 000+)
benchmarks, by a signiﬁcant margin. Perhaps as importantly, we
have also shown that for a large number of cases where Kaluza
provides no conclusion, S3 can actually provide a deﬁnitive con-
clusion (about 99.6% on the UNSAT benchmarks). Thus S3 is far
more efﬁcient and robust than Kaluza.
6.2 Comparison with Z3-str
Recall that Z3-str deals with a smaller class of constraints than
S3 (since Z3-str cannot handle regular expressions). The purpose
6There is usually one such variable in each benchmark.
of this study is to answer the question: w.r.t constraints that can be
handled by both of the two solvers, are the performances the same?
We now demonstrate that the answer is no, via deﬁning the classes
that show S3’s improvement (esp. our enhanced design).
Time(ms)
Benchmark
Model produced?
Z3-str S3 Z3-str/S3
Z3-str
_
ID_3482
NO
_
ID_3468
NO
_
ID_1543(*)
NO
_
ID_3464
NO
_
ID_3487
NO
_
ID_new.23484(*) NO
27x