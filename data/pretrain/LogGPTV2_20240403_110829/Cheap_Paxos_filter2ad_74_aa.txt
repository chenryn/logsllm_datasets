title:Cheap Paxos
author:Leslie Lamport and
Mike Massa
Cheap Paxos
Leslie Lamport and Mike Massa
Microsoft
Abstract
Asynchronous algorithms for implementing a fault-
tolerant distributed system, which can make progress
despite the failure of any F processors, require 2F + 1
processors. Cheap Paxos, a variant of the Paxos al-
gorithm, guarantees liveness under the additional as-
sumption that the set of nonfaulty processors does not
“jump around” too fast, but uses only F + 1 main pro-
cessors that actually execute the system and F auxiliary
processors that are used only to handle the failure of a
main processor. The auxiliary processors take part in
reconﬁguring the system to remove the failed processor,
after which they can remain idle until another main
processor fails.
1 Introduction
The state-machine approach consists of describing a
system as a state machine that takes as input a se-
quence of client commands and produces a sequence of
states and outputs [4, 10]. The state machine is im-
plemented by a collection of servers.
It reduces the
problem of implementing a distributed system to that
of having the servers choose a sequence of commands.
Making the system reliable requires that all processors
agree on each command in the sequence, despite the
failure of some components. For asynchronous systems,
we require that consistency be maintained in the face
of any number of non-malicious (non-Byzantine) fail-
ures, and that progress be ensured when enough pro-
cessors are nonfaulty and can communicate with one
another in a timely manner [2]. The “classic” Paxos
algorithm is an eﬃcient, practical algorithm for achiev-
ing this [1, 5, 7].
Consider the problem of implementing a distributed
system that can make progress if all but one processor
is working. Previous algorithms, such as classic Paxos,
require three processors. Only two of those processors
need maintain the system state; but a third proces-
sor must participate in choosing the sequence of com-
mands. The following argument shows that this third
processor is necessary. Suppose the system is imple-
mented by only two processors, p and q, and suppose
that q fails. The requirement that the system continue
to make progress despite a single failed processor means
that p must continue operating the system. Now sup-
pose that p fails and then q is repaired. Since there
is only one failed processor, q must be able to resume
operating the system. But this is clearly impossible,
since q does not know the current state of the system
because it does not know what p did after q failed.
Some third processor is needed—for example, a disk
that can be accessed by both p and q.
Suppose we are willing to weaken the liveness re-
quirement, so that if q fails and then p fails before q is
repaired, then the system may halt until p is repaired.
Two processors are still not enough if we require that
consistency be maintained despite communication fail-
ure. With only two processors p and q, one processor
cannot distinguish failure of the other processor from
failure of the communication medium. Since consis-
tency is lost if each processor continues operating the
system by itself, the system cannot allow each proces-
sor to continue just because it thinks that the other
processor has failed. A third processor is needed. How-
ever, that third processor does not have to participate
in choosing the sequence of commands. It must take
action only in case p or q fails, after which it does
nothing while either p or q continues to operate the
system by itself. The third processor can therefore be
a small/slow/cheap one, or a processor primarily de-
voted to other tasks.
This argument suggests that there exists a method
of implementing a one-fault tolerant system, satisfying
the consistency property of classic Paxos and a weaker
liveness property, using two main processors plus a
third auxiliary processor. This paper describes Cheap
Paxos, a generalization of such an algorithm that tol-
erates F faults with F + 1 main processors and F aux-
iliary processors. It maintains liveness under a sort of
“amoeba” assumption [3], under which the subnetwork
of working main processors does not move around too
quickly. The assumption can be described as follows.
A nonfaulty processor maintains a certain knowledge
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:30:57 UTC from IEEE Xplore.  Restrictions apply. 
of the system’s state. When a faulty processor is re-
paired, it can, in a ﬁnite length of time, re-acquire this
knowledge from any other processor that possesses it.
Liveness is maintained as long as there is at least one
main processor with knowledge of the system state and
F + 1 processors (main or auxiliary) that are nonfaulty
and can communicate with one another in a timely
manner. Consistency is always maintained (assuming
non-malicious failures).
There are two threads of previous work that super-
ﬁcially resemble Cheap Paxos. The ﬁrst is the use of
main processors that are replaced by spares if they
fail [8].
Indeed, classic Paxos requires only F + 1
working processors to operate a system that tolerates
F faults; the remaining F processors can be used as
spares. However, unlike the auxiliary processors of
Cheap Paxos, spares must have the necessary comput-
ing power to replace a failed main processor. The sec-
ond thread is the use of dynamic quorum algorithms for
maintaining multiple copies of a database. These algo-
rithms can employ “witness” processors that need not
maintain the data [9]. However, unlike the auxiliary
processors of Cheap Paxos, these witnesses participate
in each operation.
Two moderately recent developments in computing
may make Cheap Paxos useful. First, improvements
to hardware and operating systems make computers
less likely to crash. The weaker liveness guarantee of
Cheap Paxos may therefore still provide suﬃcient relia-
bility. Second, the widespread use of computers makes
it more likely that an organization will have additional
machines from which cycles can be “stolen” to imple-
ment the auxiliary processors.
One might think that the low cost of computers
would make Cheap Paxos uninteresting. However, we
have observed that people are no more willing to use
extra hardware to make a system simpler and more re-
liable than they were 40 years ago, even though that
hardware has become orders of magnitude cheaper.
The following section reviews Paxos, and Section 3
describes Cheap Paxos. The obligatory conclusion fol-
lows.
2 A Review of Paxos
The Paxos algorithm for implementing a distributed
state machine was introduced in [5]. We consider two
versions of Paxos.
In the basic version, to which we
give the name Static Paxos, the set of servers is ﬁxed.
A variation that we call Dynamic Paxos, mentioned
brieﬂy in [5], uses state machine commands to change
the set of servers. We begin by considering Static
Paxos; Dynamic Paxos is explained in Section 2.3.
2.1 The Paxos Consensus Algorithm
To implement a distributed system as a state machine,
the processors of the system must choose a sequence
of commands. This is done by executing a sequence
of instances of a consensus algorithm, the i th instance
choosing the i th command in the sequence. We now
review the Paxos consensus algorithm.
The goal of a consensus algorithm is for a collection
of processes to agree upon a value. It is most conve-
nient to phrase the consensus problem in terms of three
classes of agents: proposers that propose values, accep-
tors that cooperate to choose a single proposed value,
and learners that must learn what value has been cho-
sen [6]. A single processor can act as more than one
kind of agent. The safety properties that a consensus
algorithm must satisfy are:
Nontriviality Only a value that has been proposed
may be chosen,
Consistency Only a single value may be chosen.
Conservatism Only a chosen value may be learned.
There is also a liveness requirement that we do not try
to state precisely; it is discussed informally below.
The Paxos consensus algorithm has been discussed
elsewhere [1, 5, 6, 7], so we do not explain here exactly
how it works. Instead, we just describe its actions.
Paxos assumes an underlying procedure for select-
ing a leader. Safety is guaranteed even if no leader
or multiple leaders are selected, but a unique leader
is required to ensure progress. Proposers send their
proposals to the leader.
The consensus algorithm assumes predeﬁned sets of
acceptors called quorums. The only requirement on
the quorums is that any two quorums have at least one
acceptor in common. Paxos also assumes a set of ballot
numbers, which for simplicity we take to be the natural
numbers. The ballot numbers are partitioned among
potential leaders, each possible leader having its own
disjoint set of ballot numbers.
The consensus algorithm has two phases, each with
two subphases. The algorithm’s actions are described
below. The algorithm sends messages between learners
and acceptors, and from acceptors to learners. Since
the same processor may be playing multiple roles, it
can send messages to itself.
Phase1a(l , b) Leader l chooses a number b from among
its ballot numbers and sends (cid:1)“1a”, b(cid:2) messages to
the acceptors.
Phase1b(a, b) When acceptor a receives a (cid:1)“1a”, b(cid:2)
message from a leader l, if it has not received any
message with a ballot number greater than b, then
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:30:57 UTC from IEEE Xplore.  Restrictions apply. 
it replies to l with a (cid:1)“1b”, b, . . .(cid:2) message, where
the precise contents of the message do not concern
us. If a has received a message with ballot number
greater than b, it sends a reply to l indicating that
it is ignoring the (cid:1)“1a”, b(cid:2) message. (Upon receiv-
ing that message, l will perform a Phase1a(l , b(cid:1))
action for b(cid:1) > b, if it still believes itself to be the
leader.)
Phase2a(l , b) If leader l has received (cid:1)“1b”, b, . . .(cid:2) mes-
sages from a quorum of acceptors, then it sends a
(cid:1)“2a”, b, v (cid:2) message to the acceptors where, de-
pending on the contents of those “1b” messages,
either:
• The value of v is determined by the “1b” mes-
• l chooses v arbitrarily from among the pro-
sages, or
posals it has received.
This action may not be performed twice for diﬀer-
ent values of v (with the same b).
Phase2b(a, b, v) If acceptor a receives a (cid:1)“2a”, b, v (cid:2)
message and it has not already received any mes-
sage with a ballot number greater than b, it sends
a (cid:1)“2b”, b, v (cid:2) message to every learner.
Learn(r , v , b) If learner r has received (cid:1)“2b”, b, v (cid:2) mes-
sages from a quorum of acceptors, then it learns
that the value v has been chosen.
In normal execution, the actions occur in the order
listed above, starting with the leader’s Phase1a ac-
tion. However, processes may fail, messages may be
lost or delivered out of order, and several processors
could simultaneously think they are the leader, caus-
ing “1a” and “2a” messages for several diﬀerent ballot
numbers to be sent concurrently. Nevertheless, the al-
gorithm maintains its three safety properties, nontrivi-
ality, consistency, and conservatism. (We are assuming
non-Byzantine failures in which a process can halt, but
does not perform incorrect actions.) Moreover, if there
is a single working processor l that believes itself to be
the leader, has received a proposal, and can communi-
cate with a quorum of acceptors, then some value will
eventually be chosen. Any learner that can communi-
cate with this quorum of acceptors will then learn the
chosen value.
We can allow failed processes to be restarted if they
have stable storage that survives a failure. Processes
must maintain the following amounts of information
in stable storage: an acceptor must keep two ballot
numbers and one proposed value, and a leader must
keep one ballot number (the largest one for which it
has performed a Phase2a action).
As described here, the algorithm never terminates.
A leader can at any time perform a Phase1a action for
a new ballot number. In an application, there will be
some point at which enough processes have learned the
chosen value, after which processes can forget all about
this instance of the algorithm, erasing any information
about it from their stable storage.
For later reference, we make the following observa-
tions.
O1. We can save messages at the cost of an extra mes-
sage delay by having a single distinguished learner
that informs the other learners when it ﬁnds out
that a value has been chosen. Acceptors then send
“2b” messages only to the distinguished learner.
In most applications, the roles of leader and dis-
tinguished learner are performed by the same pro-
cessor.
O2. A leader can send its “1a” and “2a” messages just
to a quorum of acceptors. As long as all acceptors
in that quorum are working and can communicate
with the leader and the learners, there is no need