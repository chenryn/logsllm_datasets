3) CLAMR: CLAMR [1] is a DOE mini app, which is used
for testing new architectures and runtime systems. CLAMR is
a cell-based adaptive mesh reﬁnement application. It supports
multiple platforms and provides the result check by applying
domain speciﬁc mass conservation criteria. CLAMR generates
results visualizations that can better help domain experts tune
their codes and understand the insight of the physics behind
the problem. CLAMR allows the users to deﬁne the size
of problem, length of simulation, checkpoint frequency and
correctness checker.
B. Fault Injection
Figure 6 summarizes the fault injection results for three
benchmarks from Rodinia benchmark suite, namely bfs,
kmeans and lud, Matvec and CLAMR. In this study, Chaser
randomly chooses a ﬂoating point instruction or mov instruc-
tion at runtime to inject the fault (except for Matvec that
only mov instructions are selected), and for each application
we perform 3,000 to 5,000 fault injection runs (one fault per
run), to reach a statistically signiﬁcant estimate on the result.
The types of failure outcomes include benign (the output
data ﬁles compared bit-wisely the same as the ﬁles from
the “golden” run of the benchmark), terminated (either the
application crashes due to a OS signal such as SIGSEGV, or
the application is terminated due to program level assertion),
and silent data corruption (SDC) (the output data ﬁles differ
bit-wisely from the “golden” run).
Rodinia’s BFS contains frequent comparison operations. So
we choose cmp as the targeted instruction to inject faults at
runtime. Kmeans has a computation kernel for calculating
the distance between data samples. We inject ﬂoating-point
faults into Kmeans. For lud, we are using a combined of
ﬂoating-point and “cmp“ faults. Three Rodinia benchmark
applications demonstrate the capability of injecting faults into
target instructions.
We chose CLAMR as the target of our case study. CLAMR
is a cell-based adaptive mesh reﬁnement (AMR) hydrodynam-
ics DOE mini-app that simulates the long range propagation
of waves. The computation model uses the shallow water
equation to simulate the ﬂuid dynamics and harnesses the three
conservation laws of mass, x momentum, and y momentum.
Fig. 5: Design of taint propagation for MPI applications
out the MPI messages to other ranks. When the function
MPI_send is invoked, Chaser ﬁrst extracts the buf, where
the actual message is stored, to check if it is tainted. If it
is not tainted, Chaser simply returns without any operation.
Otherwise, Chaser extracts (tag, dest) as the ID of this
MPI message. The taint status of this message will be read
using information ( count, datatype, buf). Chaser
then shares the ID and taint status with TaintHub.
On the receiving side, Chaser hooks the MPI message
receiving functions to retrieve tainting status of the current
MPI message from TaintHub. When MPI_recv function
is invoked. Chaser ﬁrst extracts (tag, source) and uses
them to poll TaintHub so that the tainting status of the
current MPI message can be retrieved. Based on the retrieved
information, if the current MPI message is not tainted, Chaser
simply returns without any operation. Otherwise, it will mark
the MPI message as tainted with the taint status information
and the taint propagation continues from one MPI process to
the next.
Log:
Chaser
c) Fault
Propagation
DECAF_READ_TAINTMEM_CB
uses
callbacks
and
DECAF_WRITE_TAINTMEM_CB from DECAF to record
propagation of fault. These two callbacks are invoked when
the targeted program reads/writes the tainted memory. Chaser
logs the eip (instruction pointer) , virtual memory address,
physical memory address,
tainted value and current value
in this memory location for post analysis. We believe that
this detailed information will provide us with new ways to
analyze and evaluate soft errors’ impact on applications.
IV. CASE STUDY
A. Setup and Benchmark
The testbed cluster used for testing Chaser includes four
Dell Xeon servers with 16 cores 3.0 GHz CPU and 32GB
of RAM. Servers are interconnected with a 10GB network.
TaintHub is running on the head node of the cluster for
fault injection experiments on MPI applications. The cluster
is running Ubuntu 14.04 as the OS on each node.
To show the broad usage of Chaser, we evaluate it using
applications from two different categories. The ﬁrst category
is common applications that are executed on a single ma-
chine. The second category is HPC applications. These two
Authorized licensed use limited to: Middlesex University. Downloaded on October 19,2020 at 21:20:43 UTC from IEEE Xplore.  Restrictions apply. 
359
TABLE III: Termination breakdown for MPI application
Matvec
OS Exceptions MPI error detected
89.77%
72.77%
9.94%
27.23%
Slave Node failed
0.23%
0
Tests
Total*
Propagation§
*: The total runs of Matvec. §: The subset of Matvec runs in which faults
propagated between master node and slave node.
(cid:21)
(cid:20)(cid:18)(cid:27)(cid:25)
(cid:20)(cid:18)(cid:25)
(cid:20)(cid:18)(cid:22)(cid:25)
(cid:20)
(cid:55)(cid:40)(cid:39)
(cid:38)(cid:41)(cid:50)(cid:45)(cid:43)(cid:50)
(cid:56)(cid:41)(cid:54)(cid:49)(cid:45)(cid:50)(cid:37)(cid:56)(cid:41)(cid:40)
(cid:70)(cid:74)(cid:87)
(cid:79)(cid:17)(cid:81)(cid:73)(cid:69)(cid:82)(cid:87)
(cid:80)(cid:89)(cid:72)
(cid:39)(cid:48)(cid:37)(cid:49)(cid:54)
(cid:49)(cid:69)(cid:88)(cid:90)(cid:73)(cid:71)
Fig. 6: Fault Injection Results.
We analyze the impact of injecting random transient errors
into registers. We run CLAMR 5195 times and randomly inject
a single bit error into the ﬂoating point instructions in each
run. Within 5195 runs, CLAMR detected the injected faults in
4349 (83.71%) runs and 846 (16.28%) resulted in undetected
faults. These undetected faults are of interest as they can result
in silent data corruption. We further investigated the output of
CLAMR with these faults and discovered that 618 (11.89%)
of the undetected faults still produced the correct results while
228 (4.38%) of the runs produced incorrect results. This kind
of analysis can be useful in discovering the vulnerability of
an application to injected faults.
Analysis for terminated cases For MPI applications in our
study, the dominant percentage of failures belong to applica-
tion termination. Chaser is able to provide the understanding
of the source of the terminated cases. For example, for Matvec,
As shown in Table III, 89.77% of terminated cases are due to
OS exceptions such as SIGSEGV, and nearly 9.94% are caused
by the MPI runtime exceptions, and interestingly, 0.23% of
terminations are due to the faults propagating from the master
node to slave nodes. Table III also shows that for the cases
where the fault propagate to the slave node, 72.77% of the
terminations on the slave node are due to OS exceptions, while
the rest are due to MPI runtime errors. Our observation shows
that for MPI applications, the majority of the terminations of
the application are due to the the faults occurring in the same
node.
C. Fault Propagation Analysis
In this section, we examine the characteristics of injected
faults and how they propagate. In particular, we are in-
terested in discovering the propagation characteristics from
the perspective of memory operations. In the second set of
experiments, we run CLAMR 2973 times and in addition to
registers.
Tainted bytes in the Propagation. Since it is not possible
to show all of the graphs, we randomly selected two fault
Fig. 7: Termination analysis.
Fig. 8: Distribution of # of tainted memory reads across all
MPI ranks over all fault injections runs. For example, most of
injected faults will trigger the taint read operation maximum
at 2500k times. Majority of the cases are under 800k times of
taint read operations.
injection cases from 2973 runs. These two cases are executed
again with the same injected faults as the ﬁrst run. Once the
faults are injected, the number of tainted bytes in memory
is extracted every 100K executed instructions. The results are
seen in Figure 7. The number of tainted bytes ﬁnally reaches a
constant number in both case 1 and case 2. This is because the
injected faults can only affect a ﬁxed portion of the memory.
When CLAMR does not access that memory region after some
time, the injected faults cease to propagate.
We also discover that the number of tainted bytes ﬂuctuates
during fault propagation. It even drops to zero at times. This
is due to the fact that tainted bytes are overwritten by the
program with clean data.
Memory Operations In the Propagation Chaser keeps
track of two types of memory operations as described in Sec-
tion III-C. The tainted memory read and tainted memory write
operations represent how faults propagate through memory.
We count the number of these two operations for every run of
CLAMR and summarize them in Figure 8 and 9. The ﬁgures
illustrate that the number of involved memory operations vary
signiﬁcantly between CLAMR runs. Out of the 2973 total runs,
1402 (47.1%) runs have more tainted memory read operations,
118 (3.97%) runs only have tainted memory read operations,
and 444(14.93%) only have tainted memory write operations.
Authorized licensed use limited to: Middlesex University. Downloaded on October 19,2020 at 21:20:43 UTC from IEEE Xplore.  Restrictions apply. 
360
tracing enabled, it takes 103s to ﬁnish the execution with and
without fault injection (0% overhead). It takes 89s and 91s
(2.2% overhead) if fault propagation tracing is disabled. The
performance overhead of fault propagation tracing is about
15.7% (103s vs 89s).
V. RELATED WORK
There is a long history of using fault injection technologies
to proﬁle the vulnerability of applications, but not for fault
propagation study. The work from [5] proposes an error
propagation framework which utilizes LLVM instrumenta-
tion to inject faults. They then instrument the MPI message
“sender“ to customize the message structure and format by
adding more data corruption related tags and information. The
“receiver“ can then be notiﬁed with incoming corrupted data.
Our solution is similar but we add an extra hub (TaintHub)
to coordinate the information exchanges between the “sender“
and “receiver“. With TaintHub, the message “receiver“ does
not have to keep parsing each individual incoming message.
Instead, only when informed by TaintHub, “receiver“ will
start to record data corruption in the message. Therefore, the
overhead of turning on the tracing module in Chaser is much
smaller. Still based on LLVM, Li [27] presents the study of
error propagation. By dividing the memory into Total Mem-
ory(TM), Result Memory(RM) and Output Memory(OM),
they traced the errors travels between these three layers at
coarse grained level. No syntax is needed, but at the same
time, the ﬁndings from the error propagation is not useful in
guiding the resilience design at the software and algorithm
levels. Guo et al.
[20] propose FlipTracker, which uses
PIN tool to inject errors into the instructions, trace the error
propagation, and further analyze the resilience properties in
HPC applications. Program codes are partitioned into multiple
code regions. By monitoring the input and output of each
code regions, the errors are marked and tracked through the
execution. FlipTracker provides limited error tracing capability
and the accuracy is largely dependent on the choice of the code
region. Moreover, FlipTracker cannot support fault injection
and analysis in MPI environments as the faults corrupt not only
the local memory space but also the other parallel processes.
VI. CONCLUSIONS
In this paper, we utilized dynamic binary instrumentation