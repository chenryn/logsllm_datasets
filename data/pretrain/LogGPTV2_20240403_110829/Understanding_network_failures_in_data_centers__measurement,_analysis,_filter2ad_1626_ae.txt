### Redundancy Group Analysis

Figure 15 illustrates an example of a redundancy group between a primary (P) and backup (B) aggregation switch (AggS) and access router (AccR). When a failure is completely masked, the traffic ratio during the failure compared to before the failure will be close to one across the redundancy group, indicating that the traffic volume remains consistent.

### Effectiveness of Network Redundancy

Network redundancy is beneficial but not entirely effective. Figure 16 shows the distribution of normalized byte volumes for individual links and redundancy groups. Redundancy groups are more effective in maintaining traffic levels during failures, with 25% of events experiencing no impact on network traffic at the redundancy group level. The median traffic carried at the redundancy group level is 93%, compared to 65% per link, representing a 43% improvement in median traffic. A similar observation is made when considering packet volumes (not shown).

### Factors Affecting Redundancy Effectiveness

Several factors can limit the effectiveness of redundancy in eliminating the impact of failures on network traffic:

1. **Fail-over Mechanism Bugs**: Uncertainty in identifying the backup link or component can lead to regular traffic traversing the backup link [7].
2. **Configuration Errors**: Incorrect configuration of redundant components can prevent them from rerouting traffic away from the failed component. For example, a typo in the configuration script can cause the same error in both the primary and backup of a network connection.
3. **Protocol Issues**: TCP backoff, timeouts, and spanning tree reconfigurations can result in traffic loss.

### Redundancy Across Different Layers of the Network Topology

This section analyzes the effectiveness of network redundancy across different layers in the network topology. Links are categorized based on their location in the topology, determined by the types of devices they connect (e.g., a CoreCore link connects two core routers). Figure 17 plots quartiles of normalized traffic (in bytes) for links at different layers of the network topology.

- **Core Links**: Redundancy is most effective for core links. The median traffic carried during failure drops to 27% per link but remains at 100% when considered across a redundancy group.
- **Aggregation and Access Links**: Links between aggregation switches and access routers (AggAccR) experience significant benefits from redundancy. The median traffic carried per link during failure drops to 42% per link but remains at 86% across redundancy groups.
- **Edge Links**: Links from ToRs to aggregation switches benefit the least from redundancy, but they have a low failure impact. The median traffic carried during failure increases from 68% on links to 94% within redundancy groups for links connecting ToRs to AggS. However, these links do not experience significant impact from failures, limiting the potential benefit of redundancy.

### Discussion

#### Implications for Data Center Network Design

1. **Reliability of Low-End Switches**: Low-cost, commodity switches in data centers exhibit high reliability, with a failure probability of less than 5% annually for all types of ToR switches and AggS-2. Despite their larger population, ToRs rank third in terms of number of failures and dominate in terms of total downtime. This suggests that proposals to leverage commodity switches to build flat data center networks [3, 12, 21] can provide good reliability.
2. **Middlebox Management**: Middleboxes such as load balancers play a critical role in network reliability. Improved management and debugging tools are needed to address software and configuration faults. Software load balancers running on commodity servers can offer cost-effective, reliable alternatives to expensive hardware solutions.
3. **Enhancing Redundancy Effectiveness**: Network redundancies in our system are 40% effective at masking the impact of network failures. Configuration issues, such as typos, can render redundancy ineffective. Automated configuration and validation tools can help mitigate these issues.
4. **Separation of Control and Data Planes**: Loss of keep-alive messages can result in disconnection of portchannels. Software-Defined Networking (SDN) proposals like OpenFlow [20] can maintain state in a logically centralized controller, eliminating keep-alive messages in the data plane and reducing interference between application and control traffic.

### Related Work

Previous studies have focused on application-level [16, 22] and network connectivity [18, 19, 25, 26, 28] failures, as well as hardware reliability in cloud computing [11, 23, 24, 27].

- **Application Failures**: Padmanabhan et al. [22] observed that most Web access failures are due to server-side issues. Netmedic [16] aims to diagnose application failures in enterprise networks by considering the state of failing components.
- **Network Failures**: Studies on network failures in wide area and enterprise networks [18, 19, 25, 26, 28] have not considered large-scale data centers. Markopolou et al. [19] and Turner et al. [26] observed link flapping and longer repair times for wide area links.
- **Cloud Computing Failures**: Ford et al. [11] and Benson et al. [6] highlighted the importance of understanding component failures in large-scale systems, including storage and server nodes.

### Conclusions and Future Work

This paper presents a large-scale analysis of network failure events in data centers, focusing on characterizing failures, estimating their impact, and analyzing the effectiveness of network redundancy. Our findings support the use of commodity switches in flat network designs and highlight the need for better management of middleboxes. Future work should include correlating application-level and network logs to understand the true impact of network failures on applications and investigating the convergence of network protocols.

### Acknowledgements

We thank Arvind Krishnamurthy and the anonymous reviewers for their feedback. We are grateful to David St. Pierre for his assistance with the network logging systems and datasets.

### References

[1] Cisco: Data center: Load balancing data center services, 2004. <http://www.cisco.com/en/US/solutions/collateral/ns340/ns517/ns224/ns668/net_implementation_white_paper0900aecd8053495a.html>.

[2] H. Abu-Libdeh, P. Costa, A. I. T. Rowstron, G. Oâ€™Shea, and A. Donnelly. Symbiotic routing in future data centers. In SIGCOMM, 2010.

[3] M. Al-Fares, A. Loukissas, and A. Vahdat. A scalable, commodity data center network architecture. In SIGCOMM, 2008.

[4] M. Alizadeh, A. Greenberg, D. Maltz, J. Padhye, P. Patel, B. Prabhakar, S. Sengupta, and M. Sridharan. Data Center TCP (DCTCP). In SIGCOMM, 2010.

[5] T. Benson, A. Akella, and D. Maltz. Network traffic characteristics of data centers in the wild. In IMC, 2010.

[6] T. Benson, S. Sahu, A. Akella, and A. Shaikh. A first look at problems in the cloud. In HotCloud, 2010.

[7] J. Brodkin. Amazon EC2 outage calls 'availability zones' into question, 2011. <http://www.networkworld.com/news/2011/042111-amazon-ec2-zones.html>.