of a redundancy group is shown in Figure 15. If a failure has been
masked completely, this ratio will be close to one across a redun-
dancy group i.e., trafﬁc during failure was equal to trafﬁc before
the failure.
Network redundancy helps, but it is not entirely effective. Fig-
ure 16 shows the distribution of normalized byte volumes for indi-
vidual links and redundancy groups. Redundancy groups are effec-
tive at moving the ratio of trafﬁc carried during failures closer to
one with 25% of events experiencing no impact on network trafﬁc
at the redundancy group level. Also, the median trafﬁc carried at
the redundancy group level is 93% as compared with 65% per link.
This is an improvement of 43% in median trafﬁc as a result of net-
work redundancy. We make a similar observation when considering
packet volumes (not shown) .
There are several reasons why redundancy may not be 100%
effective in eliminating the impact of failures on network trafﬁc.
First, bugs in fail-over mechanisms can arise if there is uncertainty
as to which link or component is the back up (e.g., trafﬁc may be
regularly traversing the back up link [7]). Second, if the redundant
components are not conﬁgured correctly, they will not be able to re-
route trafﬁc away from the failed component. For example, we ob-
served the same conﬁguration error made on both the primary and
back up of a network connection because of a typo in the conﬁgura-
tion script. Further, protocol issues such as TCP backoff, timeouts,
and spanning tree reconﬁgurations may result in loss of trafﬁc.
5.2 Redundancy at different layers of the net-
work topology
This section analyzes the effectiveness of network redundancy
across different layers in the network topology. We logically di-
vide links based on their location in the topology. Location is de-
termined based on the types of devices connected by the link (e.g.,
a CoreCore link connects two core routers). Figure 17 plots quar-
tiles of normalized trafﬁc (in bytes) for links at different layers of
the network topology.




Figure 15: An example redundancy group between a primary
(P) and backup (B) aggregation switch (AggS) and access
router (AccR).
per link
per redundancy group
]
x
<
X
P
[
0
.
1
8
.
0
6
.
0
4
.
0
2
0
.
.
0
0
0.0
0.2
0.4
0.6
0.8
1.0
Traffic during/traffic before
Figure 16: Normalized trafﬁc (bytes) during failure events per
link as well as within redundancy groups.
Links highest in the topology beneﬁt most from redundancy.
A reliable network core is critical to trafﬁc ﬂow in data centers.
We observe that redundancy is effective at ensuring that failures
between core devices have a minimal impact. In the core of the net-
work, the median trafﬁc carried during failure drops to 27% per link
but remains at 100% when considered across a redundancy group.
Links between aggregation switches and access routers (AggAccR)
experience the next highest beneﬁt from redundancy where the me-
dian trafﬁc carried per link during failure drops to 42% per link but
remains at 86% across redundancy groups.
Links from ToRs to aggregation switches beneﬁt the least from
redundancy, but have low failure impact. Links near the edge of
the data center topology beneﬁt the least from redundancy, where
the median trafﬁc carried during failure increases from 68% on
links to 94% within redundancy groups for links connecting ToRs
to AggS. However, we observe that on a per link basis, these links
do not experience signiﬁcant impact from failures so there is less
room for redundancy to beneﬁt them.
6. DISCUSSION
In this section, we discuss implications of our study for the de-
sign of data center networks and future directions on characterizing
data center reliability.
Low-end switches exhibit high reliability. Low-cost, commod-
ity switches in our data centers experience the lowest failure rate
with a failure probability of less than 5% annually for all types of
ToR switches and AggS-2. However, due to their much larger pop-
ulation, the ToRs still rank third in terms of number of failures and
359−
−
−
−
−
−
−
−
s
e
t
y
b
#
d
e
z
i
l
a
m
r
o
n
i
n
a
d
e
M
0
.
1
8
.
0
6
.
0
4
.
0
2
.
0
0
.
0
−
−
−
−
−
−
per link
per redundancy group
−
−
−
−
−
−
−
−
−
−
All
ToRAgg
AggAccR
AggLB
AccRCore
CoreCore
Topology level
Figure 17: Normalized bytes (quartiles) during failure events
per link and across redundancy group compared across differ-
ent layers in the data center topology.
dominate in terms of total downtime. Since ToR failures are consid-
ered the norm rather than the exception (and are typically masked
by redundancy in application, data, and network layers), ToRs have
a low priority for repair relative to other outage types. This sug-
gests that proposals to leverage commodity switches to build ﬂat
data center networks [3, 12, 21] will be able to provide good reli-
ability. However, as populations of these devices rise, the absolute
number of failures observed will inevitably increase.
Improve reliability of middleboxes. Our analysis of network fail-
ure events highlights the role that middle boxes such as load bal-
ancers play in the overall reliability of the network. While there
have been many studies on improving performance and scalability
of large-scale networks [2, 3, 12–14, 21], only few studies focus on
management of middle boxes in data center networks [15]. Mid-
dle boxes such as load balancers are a critical part of data center
networks that need to be taken into account when developing new
routing frameworks. Further, the development of better manage-
ment and debugging tools would help alleviate software and con-
ﬁguration faults frequently experienced by load balancers. Finally,
software load balancers running on commodity servers can be ex-
plored to provide cost-effective, reliable alternatives to expensive
and proprietary hardware solutions.
Improve the effectiveness of network redundancy. We observe
that network redundancies in our system are 40% effective at mask-
ing the impact of network failures. One cause of this is due to con-
ﬁguration issues that lead to redundancy being ineffective at mask-
ing failure. For instance, we observed an instance where the same
typo was made when conﬁguring interfaces on both the primary and
back up of a load balancer connection to an aggregation switch.
As a result, the back up link was subject to the same ﬂaw as the
primary. This type of error occurs when operators conﬁgure large
numbers of devices, and highlights the importance of automated
conﬁguration and validation tools (e.g., [8]).
Separate control plane from data plane. Our analysis of NOC
tickets reveals that in several cases, the loss of keep alive mes-
sages resulted in disconnection of portchannels, which are virtual
links that bundle multiple physical interfaces to increase aggre-
gate link speed. For some of these cases, we manually correlated
loss of control packets with application-level logs that showed sig-
niﬁcant trafﬁc bursts in the hosted application on the egress path.
This interference between application and control trafﬁc is undesir-
able. Software Deﬁned Networking (SDN) proposals such as Open-
Flow [20] present a solution to this problem by maintaining state in
a logically centralized controller, thus eliminating keep alive mes-
sages in the data plane. In the context of proposals that leverage
location independent addressing (e.g., [12, 21]), this separation be-
tween control plane (e.g., ARP and DHCP requests, directory ser-
vice lookups [12]) and data plane becomes even more crucial to
avoid impact to hosted applications.
7. RELATED WORK
Previous studies of network failures have considered application-
level [16, 22] or network connectivity [18, 19, 25, 26, 28] failures.
There also have been several studies on understanding hardware
reliability in the context of cloud computing [11, 23, 24, 27].
Application failures. Padmanabhan et al. consider failures from
the perspective of Web clients [22]. They observe that the majority
of failures occur during the TCP handshake as a result of end-to-
end connectivity issues. They also ﬁnd that Web access failures are
dominated by server-side issues. These ﬁndings highlight the im-
portance of studying failures in data centers hosting Web services.
Netmedic aims to diagnose application failures in enterprise
networks [16]. By taking into account state of components that fail
together (as opposed to grouping all components that fail together),
it is able to limit the number of incorrect correlations between fail-
ures and components.
Network failures. There have been many studies of network fail-
ures in wide area and enterprise networks [18, 19, 25, 26, 28] but
none consider network element failures in large-scale data centers.
Shaikh et al study properties of OSPF Link State Advertise-
ment (LSA) trafﬁc in a data center connected to a corporate network
via leased lines [25]. Watson et al also study stability of OSPF by
analyzing LSA messages in a regional ISP network [28]. Both stud-
ies observe signiﬁcant instability and ﬂapping as a result of external
routing protocols (e.g., BGP). Unlike these studies, we do not ob-
serve link ﬂapping owing to our data sources being geared towards
actionable events.
Markopolou et al. use IS-IS listeners to characterize failures in
an ISP backbone [19]. The authors classify failures as either router
related or optical related by correlating time and impacted network
components. They ﬁnd that 70% of their failures involve only a
single link. We similarly observe that the majority of failures in our
data centers are isolated.
More recently, Turner et al. consider failures in an academic
WAN using syslog messages generated by IS-IS [26]. Unlike pre-
vious studies [19, 25, 28], the authors leverage existing syslog, e-
mail notiﬁcations, and router conﬁguration data to study network
failures. Consistent with prior studies that focus on OSPF [25, 28],
the authors observe link ﬂapping. They also observe longer time to
repair on wide area links, similar to our observations for wide area
links connecting data centers.
Failures in cloud computing. The interest in cloud computing
has increased focus on understanding component failures, as even
a small failure rate can manifest itself in a high number of fail-
ures in large-scale systems. Previous work has looked at failures
of DRAM [24], storage [11, 23] and server nodes [27], but there
has not been a large-scale study on network component failures in
data centers. Ford et al. consider the availability of distributed stor-
age and observe that the majority of failures involving more than
ten storage nodes are localized within a single rack [11]. We also
observe spatial correlations but they occur higher in the network
topology, where we see multiple ToRs associated with the same
aggregation switch having correlated failures.
Complementary to our work, Benson et al. mine threads from
customer service forums of an IaaS cloud provider [6]. They report
360[8] X. Chen, Y. Mao, Z. M. Mao, and K. van de Merwe. Declarative
conﬁguration management for complex and dynamic networks. In
CoNEXT, 2010.
[9] Cisco. UniDirectional Link Detection (UDLD).
http://www.cisco.com/en/US/tech/tk866/tsd_
technology_support_sub-protocol_home.html.
[10] Cisco. Spanning tree protocol root guard enhancement, 2011.
http://www.cisco.com/en/US/tech/tk389/tk621/
technologies_tech_note09186a00800ae96b.shtml.
[11] D. Ford, F. Labelle, F. Popovici, M. Stokely, V.-A. Truong,
L. Barroso, C. Grimes, and S. Quinlan. Availability in globally
distributed storage systems. In OSDI, 2010.
[12] A. Greenberg, J. Hamilton, N. Jain, S. Kandula, C. Kim, P. Lahiri,
D. Maltz, P. Patel, and S. Sengupta. VL2: A scalable and ﬂexible data
center network. In SIGCOMM, 2009.
[13] C. Guo, H. Wu, K. Tan, L. Shiy, Y. Zhang, and S. Lu. DCell: A
scalable and fault-tolerant network structure for data centers. In
SIGCOMM, 2008.
[14] C. Guo, H. Wu, K. Tan, L. Shiy, Y. Zhang, and S. Lu. BCube: A high
performance, server-centric network architecture for modular data
centers. In SIGCOMM, 2009.
[15] D. Joseph, A. Tavakoli, and I. Stoica. A policy-aware switching layer
for data centers. In SIGCOMM, 2008.
[16] S. Kandula, R. Mahajan, P. Verkaik, S. Agarwal, J. Padhye, and
P. Bahl. Detailed diagnosis in enterprise networks. In SIGCOMM,
2010.
[17] C. Kim, M. Caesar, and J. Rexford. Floodless in SEATTLE: a
scalable ethernet architecture for large enterprises. In SIGCOMM,
2008.
[18] C. Labovitz and A. Ahuja. Experimental study of internet stability
and wide-area backbone failures. In The Twenty-Ninth Annual
International Symposium on Fault-Tolerant Computing, 1999.
[19] A. Markopoulou, G. Iannaccone, S. Bhattacharyya, C.-N. Chuah,
Y. Ganjali, and C. Diot. Characterization of failures in an operational
IP backbone network. IEEE/ACM Transactions on Networking, 2008.
[20] N. Mckeown, T. Anderson, H. Balakrishnan, G. Parulkar,
L. Peterson, J. Rexford, S. Shenker, and J. Turner. Openﬂow:
enabling innovation in campus networks. In SIGCOMM CCR, 2008.
[21] R. N. Mysore, A. Pamboris, N. Farrington, N. Huang, P. Miri,
S. Radhakrishnan, V. Subramanya, and A. Vahdat. PortLand: A
scalable fault-tolerant layer 2 data center network fabric. In
SIGCOMM, 2009.
[22] V. Padmanabhan, S. Ramabhadran, S. Agarwal, and J. Padhye. A
study of end-to-end web access failures. In CoNEXT, 2006.
[23] B. Schroeder and G. Gibson. Disk failures in the real world: What
does an MTTF of 1,000,000 hours mean too you? In FAST, 2007.
[24] B. Schroeder, E. Pinheiro, and W.-D. Weber. DRAM errors in the
wild: A large-scale ﬁeld study. In SIGMETRICS, 2009.
[25] A. Shaikh, C. Isett, A. Greenberg, M. Roughan, and J. Gottlieb. A
case study of OSPF behavior in a large enterprise network. In ACM
IMW, 2002.
[26] D. Turner, K. Levchenko, A. C. Snoeren, and S. Savage. California
fault lines: Understanding the causes and impact of network failures.
In SIGCOMM, 2010.
[27] K. V. Vishwanath and N. Nagappan. Characterizing cloud computing
hardware reliability. In Symposium on Cloud Computing (SOCC),
2010.
[28] D. Watson, F. Jahanian, and C. Labovitz. Experiences with
monitoring OSPF on a regional service provider network. In ICDCS,
2003.
on problems users face when using IaaS and observe that problems
related to managing virtual resources and debugging performance
of computing instances that require involvement of cloud adminis-
trators, increase over time.
8. CONCLUSIONS AND FUTURE WORK
In this paper, we have presented the ﬁrst large-scale analysis
of network failure events in data centers. We focused our analysis
on characterizing failures of network links and devices, estimating
their failure impact, and analyzing the effectiveness of network re-
dundancy in masking failures. To undertake this analysis, we devel-
oped a methodology that correlates network trafﬁc logs with logs of
actionable events, to ﬁlter a large volume of non-impacting failures
due to spurious notiﬁcations and errors in logging software.
Our study is part of a larger project, NetWiser, on understand-
ing reliability in data centers to aid research efforts on improv-
ing network availability and designing new network architectures.
Based on our study, we ﬁnd that commodity switches exhibit high
reliability which supports current proposals to design ﬂat networks
using commodity components [3, 12, 17, 21]. We also highlight the
importance of studies to better manage middle boxes such as load
balancers, as they exhibit high failure rates. Finally, more investi-
gation is needed to analyze and improve the effectiveness of redun-
dancy at both network and application layers.
Future work.
In this study, we consider occurrence of interface
level failures. This is only one aspect of reliability in data center
networks. An important direction for future work is correlating logs
from application-level monitors with the logs collected by network
operators to determine what fraction of observed errors do not im-
pact applications (false positives) and what fraction of application
errors are not observed (e.g., because of a server or storage failure
that we cannot observe). This would enable us to understand what
fraction of application failures can be attributed to network fail-
ures. Another extension to our study would be to understand what
these low level failures mean in terms of convergence for network
protocols such as OSPF, and to analyze the impact on end-to-end
network connectivity by incorporating logging data from external
sources e.g., BGP neighbors.
Acknowledgements
We thank our shepherd Arvind Krishnamurthy and the anonymous
reviewers for their feedback. We are grateful to David St. Pierre for
helping us understand the network logging systems and data sets.
9. REFERENCES
[1] Cisco: Data center: Load balancing data center services, 2004.
www.cisco.com/en/US/solutions/collateral/
ns340/ns517/ns224/ns668/net_implementation_
white_paper0900aecd8053495a.html.
[2] H. Abu-Libdeh, P. Costa, A. I. T. Rowstron, G. O’Shea, and
A. Donnelly. Symbiotic routing in future data centers. In SIGCOMM,
2010.
[3] M. Al-Fares, A. Loukissas, and A. Vahdat. A scalable, commodity
data center network architecture. In SIGCOMM, 2008.
[4] M. Alizadeh, A. Greenberg, D. Maltz, J. Padhye, P. Patel,
B. Prabhakar, S. Sengupta, and M. Sridharan. Data Center TCP
(DCTCP). In SIGCOMM, 2010.
[5] T. Benson, A. Akella, and D. Maltz. Network trafﬁc characteristics of
data centers in the wild. In IMC, 2010.
[6] T. Benson, S. Sahu, A. Akella, and A. Shaikh. A ﬁrst look at
problems in the cloud. In HotCloud, 2010.
[7] J. Brodkin. Amazon EC2 outage calls ’availability zones’ into
question, 2011. http://www.networkworld.com/news/
2011/042111-amazon-ec2-zones.html.
361