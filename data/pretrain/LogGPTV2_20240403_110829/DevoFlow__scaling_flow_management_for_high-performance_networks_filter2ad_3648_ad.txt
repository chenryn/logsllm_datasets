load balances traﬃc poorly on irregular topologies. As an
example, consider a topology with two equal-cost links
between s and t, but the ﬁrst link forwards at 1 Gbps
whereas the second has 10 Gbps capacity. ECMP splits
ﬂows evenly across these paths, which is clearly not ideal
since one path has 10 times more bandwidth than the
other.
DevoFlow solves this problem by allowing a clonable
wildcard rule to select an output port for a microﬂow
according to some probability distribution. This allows
implementation of oblivious routing (see, e.g., [20, 32]),
where a microﬂow follows any of the available end-to-
end paths according to a probability distribution. Obliv-
ious routing would be optimal for our previous example,
where it would route 10/11th of the microﬂows for t on
the 10Gbps link and 1/11th of them on the 1Gbps link.
• Rapid re-routing gives a switch one or more fallback
paths to use if the designated output port goes down.
If the switch can execute this decision locally, it can re-
cover from link failures almost immediately, rather than
waiting several RTTs for the central controller to ﬁrst
discover the failure, and then to update the forwarding
rules. OpenFlow almost supports this already, by allow-
ing overlapping rules with diﬀerent priorities, but it does
not tell the switch why it would have multiple rules that
could match a ﬂow, and hence we need a small change
to make indicate explicitly that one rule should replace
another in the case of a speciﬁc port failure.
4.2 Efﬁcient statistics collection
DevoFlow provides three diﬀerent ways to improve the
eﬃciency of OpenFlow statistics collection.
Sampling is an alternative to either push-based or pull-
based collection (see §3.2.2).
In particular, the sFlow
protocol
[42] allows a switch to report the headers of
randomly chosen packets to a monitoring node—which
could be the OpenFlow controller. Samples are uniformly
chosen, typically at a rate of 1/1000 packets, although
this is adjustable. Because sFlow reports do not include
the entire packet, the incremental
load on the network
is less than 0.1%, and since it is possible to implement
sFlow entirely in the data-plane, it does not add load to
a switch’s CPU. In fact, sFlow is already implemented in
many switches, including the ProCurve 5406zl.
Triggers and reports: extends OpenFlow with a new
push-based mechanism: threshold-based triggers on coun-
ters. When a trigger condition is met, for any kind of rule
(wildcarded or not), the switch sends a report, similar to
the Flow-Removal message, to the controller. (It can buﬀer
these brieﬂy, to pack several reports into one packet.)
The simplest trigger conditions are thresholds on the
three per-ﬂow counters (packets, bytes, and ﬂow duration).
These should be easy to implement within the data-plane.
One could also set thresholds on packet or byte rates, but to
do so would require more state (to deﬁne the right averaging
interval) and more math, and might be harder to implement.
Approximate counters: can be maintained for all mi-
croﬂows that match a forwarding-table rule. Such counters
maintain a view on the statistics for the top-k largest (as
in, has transferred the most bytes) microﬂows in a space-
eﬃcient manner. Approximate counters can be implemented
using streaming algorithms [21, 22, 23], which are generally
simple, use very little memory, and identify the ﬂows trans-
ferring the most bytes with high accuracy. For example, Go-
lab et al.’s algorithm [23] correctly classiﬁes 80–99% of the
ﬂows that transfer more than a threshold k of bytes. Imple-
menting approximate counters in the ASIC is more diﬃcult
than DevoFlow’s other mechanisms; however, they provide
a more timely and accurate view of the network and can
keep statistics on microﬂows without creating a table entry
per microﬂow.
4.3
Implementation feasibility of DevoFlow
We have not implemented DevoFlow in hardware; how-
ever, our discussions with network hardware designers
indicate that all of DevoFlow’s mechanisms can be imple-
mented cost-eﬀectively in forthcoming production switches.
(Sampling using sFlow, as we noted earlier,
is already
widely implemented in switch data-planes.)
Rule cloning: The data-plane needs to to directly insert
entries into the exact-match table. This is similar to the
existing MAC learning mechanism, the switch ASIC would
need to be modiﬁed to take into account the formatting of
entries in the ﬂow table when learning new ﬂows. If ASIC
modiﬁcation is not possible, rule cloning would require
involving a CPU once per ﬂow. Even so, this operation
is considerably cheaper
than invoking the centralized
controller and should be orders of magnitude faster.
Multipath support: Can be implemented using special-
ized rule cloning or using a virtual port. Both methods
require a small table to hold path-choice biasing weights for
each multipath rule and a random number generator. The
virtual port method is actually similar to link aggregation
groups (LAG) and ECMP support, and could reuse the
existing functional block with trivial modiﬁcation.
Triggers: The mechanism needed to support triggers
requires a counter and a comparator. It is similar to the one
needed for rate limiters, and in some cases existing ﬂexible
rate limiters could be used to generate triggers. Most
modern switches support a large number of ﬂow counters,
used for OpenFlow, NetFlow/IPFIX or ACLs. The ASIC
would need to add a comparator to those counters to
generate triggers; alternatively, the local CPU could poll
periodically those counters and generate triggers itself.
Approximate counters: is the mechanism that would re-
quire the most extensive changes to current ASICs. It re-
quires hashing on packet headers, which indirect to a set
of counters, and then incrementing some of those counters.
Switch ASICs have existing building blocks for most of these
functions. It would also be non-trivial to support triggers
on approximate counters; this might require using the local
CPU.
4.4 Using DevoFlow
All OpenFlow solutions can be built on top of DevoFlow;
however, DevoFlow enables scalable implementation of these
solutions by reducing the number of ﬂows that interact with
the control-plane. Scalability relies on a ﬁnding a good def-
inition of “signiﬁcant ﬂows” in a particular domain. These
ﬂows should represent a small fraction of the total ﬂows,
but should be suﬃcient to achieve the desired results.
As an example, we show how to load balance traﬃc with
DevoFlow. First, we describe scalable ﬂow scheduling with
DevoFlow’s mechanisms. Then, we describe how to use its
multipath routing to statically load balance traﬃc without
any use of the control-plane.
Flow scheduling: does not scale well if the scheduler relies
on visibility over all ﬂows, as is done in Hedera [5] because
maintaining this visibility via the network is too costly, as
our experiments in §3 showed.
Instead, we maintain visibility only over elephant ﬂows,
which is all that a system such as Hedera actually needs.
While Hedera deﬁnes an elephant as a ﬂow using at least
10% of a NIC’s bandwidth, we deﬁne one as a ﬂow that
has transferred at least a threshold number of bytes X. A
reasonable value for X is 1–10MB.
Our solution starts by initially routing incoming ﬂows us-
ing DevoFlow’s multipath wildcard rules; this avoids involv-
ing the control-plane in ﬂow setup. We then detect elephant
ﬂows as they reach X bytes transferred. We can do this
using using any combination of DevoFlow’s statistics col-
lection mechanisms. For example, we can place triggers on
ﬂow table entries, which generate a report for a ﬂow after
it has transferred X bytes; We could also use sampling or
approximate counters; we evaluate each approach in §5.
Once a ﬂow is classiﬁed as an elephant, the detecting
switch or the sampling framework reports it to the DevoFlow
controller. The controller ﬁnds the least congested path be-
tween the ﬂow’s endpoints, and re-routes the ﬂow by insert-
ing table entries for the ﬂow at switches on this path.
The new route can be chosen, for example, by the de-
creasing best-ﬁt bin packing algorithm of Correa and Goe-
mans [16]. The algorithm’s inputs are the network topology,
link utilizations, and the rates and endpoints of the elephant
ﬂows. Its output is a routing of all elephant ﬂows. Correa
and Goemans proved that their algorithms ﬁnds routings
with link utilizations at most 10% higher than the optimal
routing, under a traﬃc model where all ﬂows can be rear-
ranged. We cannot guarantee this bound, because we only
rearrange elephant ﬂows; however, their theoretical results
indicates their algorithm will perform as well as any other
heuristic for ﬂow scheduling.
Finally, we note that this architecture uses only edge
switches to encapsulate new ﬂows to send to the central
controller. The controller programs core and aggrega-
tion switches reactively to ﬂow setups from the edge
switches. Therefore, the only overhead imposed is cost of
installing ﬂow table entries at the the core and aggregation
switches—no overheads are imposed for statistics-gathering.
Static multipath routing: provides eﬀective data-plane
multipath load balancing with far greater ﬂexibility than
ECMP. By allowing clonable wildcard rules to select an out-
put port for a microﬂow according to some probability dis-
tribution, we can implement oblivious routing, where an s-t
microﬂow’s path is randomly selected according to a precom-
puted probability distribution. This static routing scheme
sets up these probability distributions so as to optimize rout-
ing any traﬃc matrix in a speciﬁed set; for example, in a
data center one would generally like to optimize the routing
of all “hose” traﬃc matrices [19], which is the set of all traﬃc
matrices allowable as long as no end-host’s ingress or egress
rate exceeds a predeﬁned rate.
Oblivious routing gives comparable throughput to the op-
timal dynamic routing scheme on many topologies. Kodi-
alam et al. [32] found that packet-level oblivious routing
achieves at least 94% of the throughput that dynamic rout-
ing does on the worst-case traﬃc matrix, for several wide-
area network topologies.
However, these results assume that microﬂows can be split
across multiple paths. While the ﬂow-level multipath we im-
plement with clonable wildcard rules does not conform to
this assumption, we expect the theoretical results to be in-
dicative of what to expect from ﬂow-level oblivious routing
on arbitrary topologies, just as it indicates the possible per-
formance of oblivious routing on a Clos topology. Overall,
Algorithm 1 — Flow rate computation.
Input: set of ﬂows F and a set of ports P
Output: a rate r(f ) of each ﬂow f ∈ F
Deﬁne: P .used() =P
begin
Initialize: Fa = ∅; ∀f, r(f ) = 0
Deﬁne: P .unassigned ﬂows() = P − (P ∩ Fa)
while P (cid:54)= ∅ do
f∈Fa∩P r(f )
Sort P in ascending order, where the sort key
P = P.pop front()
for each f ∈ P .unassigned ﬂows() do
for P is (P .rate−P .used())/|P .unassigned ﬂows()|
r(f ) = (P.rate − P.used())/|P .unassigned ﬂows()|
Fa = Fa ∪ {f}
end
the performance depends on the workload, as our results
in §5 show. If oblivious routing does not achieve adequate
performance on a particular topology and workload, it can
be combined with DevoFlow’s ﬂow scheduler (described just
above) to maximize utilization.
Finally, ﬁnding an oblivious routing is easy—one can be
computed for any topology using linear programming [20].
Should the topology change, the forwarding probability dis-
tributions will need to be modiﬁed to retain optimality.
Distributions for failure scenarios can be precomputed, and
pushed to the switches once the central controller learns of
a failure.
5. EVALUATION
In this section, we present our simulated evaluation of De-
voFlow. We use load balancing as an example of how it can
achieve the same performance as ﬁne-grained, OpenFlow-
based ﬂow scheduling without the overhead.
5.1 Simulation methodology
To evaluate how DevoFlow would work on a large-scale
network, we implemented a ﬂow-level data center network