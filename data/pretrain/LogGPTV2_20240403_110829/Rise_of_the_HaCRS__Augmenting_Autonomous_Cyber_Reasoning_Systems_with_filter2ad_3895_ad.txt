consider this as the prior state-of-the-art configuration.
Human-assisted fuzzing. In this configuration, Driller is replaced
with our Human-Automation Link. Rather than symbolically
tracing fuzzer-generated test cases, we present them to our
human assistants and synchronize their test cases back into
the fuzzer. This configuration, together with the Driller and
AFL configurations, allow us to understand the relative ef-
fectiveness of Drilling versus Human Assistance.
Human-assisted Symbolic-assisted fuzzing. This is the “com-
plete” configuration of HaCRS, all components, representing
the new state-of-the-art in Cyber Reasoning System.
We ran each configuration for 8 hours, giving the fuzzer 4 pro-
cessor cores, with 2 additional cores for Driller. The results of the
experiment are presented in Table 2.
End-to-end system. The most obvious result is the improvement
in the number of vulnerabilities that were identified with the full
HaCRS configuration. By iteratively combining human assistance
and symbolic assistance to its internal fuzzer, the HaCRS was able
to identify an additional twenty bugs in different binaries over
symbolically-assisted fuzzing (a whopping 55% improvement) and
twice as much as the base-case fuzzer alone. This result is significant:
Session B3:  Investigating AttacksCCS’17, October 30-November 3, 2017, Dallas, TX, USA356Configuration
Non-expert Humans
All Humans
Unassisted Fuzzing
Symbolic-assisted Fuzzing
Human-assisted Fuzzing
Human-assisted Symbolic-
assisted Fuzzing
Semantic
Complex-
ity
Expertise
Required
High
High
Low
Low
High
High
Low
Low
High
High
Low
Low
High
High
Low
Low
High
High
Low
Low
High
High
Low
Low
Low
High
Low
High
Total
Low
High
Low
High
Total
Low
High
Low
High
Total
Low
High
Low
High
Total
Low
High
Low
High
Total
Low
High
Low
High
Total
Median
Code
Coverage
46.68%
48.83%
48.69%
16.81%
47.19%
46.83%
48.83%
48.69%
17.39%
47.19%
41.82%
43.32%
56.17%
17.46%
42.87%
42.90%
48.85%
56.07%
41.88%
44.91%
49.70%
60.45%
64.03%
17.46%
52.38%
48.98%
59.68%
64.03%
48.52%
53.45%
Median
#AT
Median
#HT
0
0
0
0
0
0
0
0
0
0
410
526
187
211
361
663
764
156
1500
649
326
472
125
207
308
369
485
121
641
403
137
150
168
297
151
137
150
168
298
151
0
0
0
0
0
0
0
0
0
0
136
126
35
9
84
69
11
46
5
34
Binaries
Crashed
0
0
0
0
0
0
1
0
0
1
12
14
1
1
28
14
17
2
3
36
21
24
2
1
48
23
28
2
3
56
Median
Time-to-
Crash
N/A
N/A
N/A
N/A
N/A
N/A
2815
N/A
N/A
2815
807
1278
143
7
897
1302
1426
62
390
1298
1378
1442
48
10
1334
1140
1855
47
584
1301
Table 2: The crashes found and code coverage achieved by different configurations of the automated and human components of HaCRS. The
full HaCRS configuration includes human non-expert, human semi-expert, and automated innovation agents. #AT, and #HT are the numbers
of automation-originated test cases and human-originated test cases, respectively, that were deemed “unique” by the Mechanical Phish’s test
case evaluation criteria.
non-expert humans, overwhelmingly likely to have no security
or program analysis training, are able to make real contributions
toward the analysis of binary software.
We analyzed the impact of the fuzzer, Driller, and human assis-
tance on code coverage metrics and the amount of test cases for
the binaries that only HaCRS was able to crash in our experiment.
This is presented in Figure 3. Unsurprisingly, all of these binaries
are ones that we classified as having high semantic complexity. For
most of them, HaCRS achieves significantly higher code coverage,
but there are several interesting exceptions where the code cov-
erage achieved by HaCRS is very close to or even lower than the
other techniques, despite it triggering a crash where other methods
failed. Our investigation into this phenomena revealed that this is
a function of humans triggering the same (or a subset) of the code
that the automation does, but doing so in a different configuration
more correct (or appropriately incorrect) for the program being
tested. Later in this section, we discuss one such case, NRFIN_00005,
where automation managed to trigger all of the functionality but it
took human intuition to trigger it correctly for a crash.
Comparison to Driller. In HaCRS, human assistants take on a
very similar role to Driller: they provide extra inputs that the fuzzer
can leverage to avoid stalling in its exploration of the target pro-
gram. Rather than making small control-flow diversions, human
assistants make semantic divergences based on their understand-
ing of the operation of the target program. This is reflected in the
results – for semantically-complex programs, the human assistants
significantly beat out Driller, achieving an improvement of up to
11.6% improvement in coverage. However, for binaries that did not
have semantic complexity but required computing expertise, the
human assistants suffered, being unable to understand the concepts
presented by the program and intuit how to interact with it. This is
where the combination of human and automated analysis shines
– Driller picks up the slack in these binaries, and the combination
Session B3:  Investigating AttacksCCS’17, October 30-November 3, 2017, Dallas, TX, USA357Figure 3: The binaries in which crashes were only found in the human-assisted configuration of the CRS. The number of unique (in terms of
code coverage) test cases is shown in the top chart (with a logarithmic scale), and the achieved code coverage is shown in the bottom chart.
The results are split into what was found by just the fuzzer alone, by including Driller, and with the addition of human assistance.
of human and symbolic assistance achieves higher code coverage
than either alone.
Impact of expertise. Interestingly, the inclusion of semi-experts
in our analysis did not seriously impact the achieved code coverage.
This is an example of the different scale achievable for experts and
semi-experts. While we were able to get 183 Mechanical Turk work-
ers to assist HaCRS, we were only able to recruit five professionals,
and they could not make a strong impact on the results (in fact,
because the results are presented in aggregate, there was almost
no impact on the median measurements). However, they did have
localized success: due to their ability to intelligently interact with
more complex binaries, the experts were able to identify a bug in
one of the applications without any automation at all. Specifically,
they triggered a bug in CROMU_00021, which implements a simple
variable storage and arithmetic engine, but contains an exploitable
bug when a variable with a blank name is created.
6.5 Case Studies
In the course of our experiments, our human assistants achieved
some results that are interesting to explore more in-depth. This
was despite the fact that the human assistants were completely
unskilled in program analysis, and were recruited with absolutely
no training. Here, we delve deeper into these bugs, and discuss why
human effort helped with these specific binaries.
Coverage case study: CROMU_00008. This binary implements a
database with a SQL-inspired interaction interface. Proper use of
this binary required understanding the concepts of storing and
retrieving data records. Interestingly, our human assistants quickly
developed an understanding for how to do this, taking the sug-
gested keywords from the CRS suggestions and combining them
into expressions the program understood. They achieved a code
coverage of 55.5%, compared with 12.1% for the automated analy-
ses. Manual investigation into the delta between automation and
human assistance revealed that, as expected, the humans produced
inputs that were meaningful for the program, while the symbolic
seed synthesis attempted to optimize for code coverage, triggering
many meaningless states (such as incorrect commands) without
ever getting to the actual operation of the program.
Session B3:  Investigating AttacksCCS’17, October 30-November 3, 2017, Dallas, TX, USA358Coverage case study: KPRCA_00052. This binary is surprisingly
complicated: the assistant is presented with a pizza order menu
system. To properly navigate this system, the analysis engine must
understand how a pizza is made: the crust is chosen first, then the
cheese, then the toppings. This makes it very hard for the automated
system to explore this binary and, in fact, our automation achieved
a 19% code coverage over the course of the experiment, as opposed
to 52% achieved by human assistants.
Vulnerability-detection case study: NRFIN_00005. This binary im-
plements a Tic-Tac-Toe game where the human player plays against
the computer. It is made harder by only displaying the moves and