30th USENIX Security Symposium    2455
mance penalty. DieHarder allocates memory at randomized
addresses [37], but this only provides a probabilistic assur-
ance that memory chunks will not overlap with each other.
Archipelago places each allocation on a distinct physical
page [30], while Oscar simulates the same object-per-page
strategy by masking the allocation through virtual pages [18].
However, creating these shadow pages can introduce more
than 40% overhead due to frequent system calls. Cling pre-
vents memory reuse between mismatched types [13], but
leaves space to exploit use-after-free bugs within compatible
types. In the original paper, the author of Cling discussed the
idea of one-time allocation, but he treated it as an impractical
solution due to heavy memory usage.
Despite the unpleasant history, we notice that OTA still
has genuine merit: a straightforward design and strong secu-
rity guarantee. Without needing complicated intelligence or
external system dependencies, OTA can eliminate the threat
of use-after-free bugs. The design also helps avoid careless
errors in implementation. Therefore, we explored different
choices to mitigate the remaining challenge of overhead while
retaining the security benefit of OTA. Fortunately, we found
a set of solutions that enable a practical OTA implementa-
tion. Our results in §6 show that the overhead of FFmalloc is
minimal in the vast majority of cases.
2.4 Threat Model
Before exploring the design space, we define the threat model
where OTA aims to protect benign programs. We assume that
a program contains one or more use-after-free bugs, and some
of them are exploitable. Other vulnerabilities, like buffer over-
flows or type errors, are out of the scope of this work. We
assume attackers can only exploit use-after-free bugs. Other
bugs cannot be used to bypass or corrupt the OTA memory
manager. Our threat model is consistent with previous propos-
als on use-after-free defense [12, 27, 35, 42, 47, 49].
3 Design Space Exploration
3.1 Forward Continuous Allocation
In our first design attempt, we implemented a forward con-
tinuous allocator, called FCmalloc. FCmalloc uses a pointer
to track the end of the last allocation. For a new memory re-
quest, it simply advances the pointer by the requested size and
returns the old value. Since the pointer is only incremented,
any call to malloc will get a distinct region. When the pointer
reaches the end of the mapped pages, FCmalloc will request
additional pages from the operating system through the mmap
system call. For each free request, FCmalloc releases all com-
pletely free pages (i.e., no byte is in use) to the system with the
munmap system call. The simple design of FCmalloc enables
compact allocation, where each allocated chunk immediately
follows the previous one.
Table 2: VMA issue of FCmalloc on SPEC programs. Due to the
forward allocation, programs with FCmalloc require more VMA struc-
tures. Batch processing can help mitigate this issue. FC-X means we
only release at least X continuous freed pages.
Benchmark
perlbench
bzip2
gcc
mcf
milc
namd
gobmk
dealII
soplex
povray
hmmer
sjeng
libquantum
h264ref
lbm
omnetpp
astar
sphinx3
xalancbmk
glibc
4,401
23
2,753
20
46
128
25
4,760
152
51
35
20
29
228
23
1,164
1,762
168
2,705
FCmalloc
58,737
35
6,525
31
65
57
61
2,322
99
109
197
32
38
89
34
15,933
6,726
31,409
68,606
FC-2
46,299
35
4,665
30
65
56
57
1,052
96
89
183
32
38
83
34
15,040
5,370
31,382
48,526
FC-8
23,171
35
3,120
30
65
56
52
338
93
74
145
32
35
80
33
13,728
3,703
31,064
34,826
FC-32
9,321
35
1,854
30
65
56
48
326
89
57
114
32
35
80
33
12,521
2,790
9,022
23,434
FCmalloc is the most straightforward way to implement
OTA. However, after applying it to the SPEC CPU2006 bench-
marks, we identify three challenges that limit its practicality.
Performance Overhead. FCmalloc has high performance
overhead due to the frequent mmap/munmap system calls. For
example, given the input file c-typeck, the gcc benchmark
will send 831,410 mmap/mumap system calls to the Linux kernel,
leading to 40.8 seconds spent in the kernel space. With glibc,
gcc only issues 57 such system calls which merely cost 0.59
seconds. The overall overhead is 60.2% for c-typeck.
Memory Overhead. FCmalloc can lead to significant mem-
ory overhead compared to the standard C allocator. Since the
OS only allocates or releases memory on page granularity, all
4096-bytes of a page cannot be returned as long as one byte
is still in use. Even worse, if a small allocation straddles the
boundary between two pages, then neither page can be freed.
VMA Limit. Frequent memory release with munmap could ex-
haust the VMA kernel structures. The Linux kernel creates a VMA
structure for each set of contiguous pages. When FCmalloc
releases a page that is in the middle of some continuous pages,
the Linux kernel will split the corresponding VMA in two. By
default, Linux allows at most 65,535 VMA structures for each
process. Once this limit is reached, no pages can be released
unless they are at the margin of an existing VMA. This behavior
exacerbates the memory overhead of the process. Table 2
shows the number of VMAs used by SPEC benchmarks. As we
can see, FCmalloc increases the number of VMAs for most of
the programs. In the worst case, it causes xalancbmk to use
68,606 VMAs, exceeding the default limit of the Linux kernel,
while the original glibc only requires 2,705 VMAs. Therefore,
FCmalloc introduces high memory overhead to xalancbmk.
Other programs incurring high VMA-usage include perlbench,
omnetpp and sphinx3.
2456    30th USENIX Security Symposium
USENIX Association
3.1.1 Mitigation: Batch Processing
We find that batch mapping and unmapping can help mitigate
the aforementioned challenges. When requesting memory
from the kernel, we can specify a large number of pages using
mmap at one time. FCmalloc then handles malloc with this
region until this batch of pages is used up, at which time
FCmalloc will issue another mmap request. When the program
tries to free a chunk of memory, FCmalloc checks if this will
create a set of continuous freed pages. If so, we can release
them together with one munmap system call. Batch processing
effectively reduces the performance overhead of FCmalloc, as
it can significantly reduce the number of system calls. For the
example of gcc, when FCmalloc only releases at least 32 freed
pages, we can save 471,144 munmap system calls (58.7%).
Reduced system calls can also help mitigate the VMA issue.
As shown in Table 2, the VMA overhead of FCmalloc keeps
decreasing if we release memory less often but with more
pages. For example, when FCmalloc only releases 32 pages,
we can save 45,172 VMAs from xalancbmk (65.8% reduction).
For perlbench and omnetpp, batch processing helps reduce
the VMA counts to a normal range. However, batch processing
will enlarge the memory usage of the protected execution, as
batch mapping introduces mapped-but-not-allocated memory
and batch unmapping brings freed-but-not-unmapped pages.
3.2 Forward Binning
Our second design attempt was a forward binning allocator,
called FBmalloc. In contrast to FCmalloc, FBmalloc groups
allocations of similar sizes into a bin. This design is usually
called a BiBop allocator - a big bag of pages [13, 21, 24, 37].
FBmalloc creates several bins for allocations with less than
4096 bytes. All allocations within a bin will get the same-size
chunks. For allocations greater than 4096 bytes, FBmalloc
rounds the size up to the next page size, and directly uses
mmap to request new pages. FBmalloc uses one page per small
bin at a time to serve the malloc request. Once a bin is fully
allocated, FBmalloc uses mmap to request an additional page
from the kernel. Only when the bin is fully freed, will it
be released to the system. Requests for different sizes will
get memory from different bins, and thus the return value of
malloc might not be strictly increasing. However, FBmalloc is
still a valid OTA, as it never reuses bins and takes the forward
continuous allocation to manage memory within a bin.
FBmalloc helps mitigate the memory overhead issue of
FCmalloc, especially when small allocations have a longer
lifetime than large ones. In the omnetpp benchmark of SPEC
CPU2006, certain small objects are rarely freed, while around
them are large objects with shorter lifetime. This leads to
heavy memory overhead with FCmalloc. With FBmalloc,
these particular allocations are co-located on a much smaller
number of pages (bins), which effectively limits the overhead.
3.3 Allocator Fusion
Our final design, called FFmalloc, marries the ideas of forward
continuous allocation and forward binning allocation to take
advantage of their strengths. FFmalloc handles allocations up
to 2048 bytes via the binning allocator. This prevents small
long-lived allocations holding onto freed pages. FFmalloc
serves large allocations from the continuous allocator to mini-
mize allocation waste due to alignment requirements.
FFmalloc assigns each allocator a pool, which contains sev-
eral continuous pages. A pool is the basic memory unit that
FFmalloc requests from the OS, and its size is configurable
during compilation. Currently, we set the pool size to be 4MB.
For allocations larger than the pool size, FFmalloc will create
a special pool that is just large enough for the request, called a
jumbo pool. Only one pool can be assigned to the binning allo-
cator, while the continuous allocator can have multiple pools.
FFmalloc associates each CPU core with a distinct pool to
reduce lock contention. When the remaining space in a pool is
insufficient to satisfy a request, FFmalloc creates a new pool.
FFmalloc will continue to place future smaller allocations in
the original pool, until the pool is filled or too many pools
have been created. We create the first pool offset from the
end of the existing heap region. This allows an application to
use both the glibc allocator and FFmalloc at the same time.
This design also makes the starting address of FFmalloc ran-
domized, preserving the security benefits of ASLR. FFmalloc
ensures that the kernel supplies pages at increasingly higher
addresses by specifying the MAP_FIXED_NOREPLACE flag for
mmap. With this flag, the kernel tries to map the memory at
the specified location and returns an error if such placement
conflicts with an existing mapping. FFmalloc keeps probing
forward until it finds an available address for the next pool.
4
Implementation
4.1 Metadata of Allocations
FFmalloc tracks its allocations in different metadata struc-
tures. Each pool of the continuous allocator has an array of
allocated addresses. For each allocation, FFmalloc appends
the return value of malloc to the array. Since FFmalloc allo-
cates memory forward, the array is naturally sorted and allows
for efficient searching. We can obtain the size of each alloca-
tion by subtracting its address from the next entry in the array.
As all allocations from FFmalloc are 8-byte aligned, the last
three bits are always zeros. We use the last two bits to indicate
the status of a memory chunk: 00 means the memory is in use;
01 means the memory is freed and safe to release; 11 means
FFmalloc has released some pages in this allocation. The pool
of the binning allocator has an array of structures, with one
entry per page. This structure records the allocation size, the
next unused chunk, and a bitmap to maintain the status of
each chunk, where 1 means in use and 0 means freed.
USENIX Association
30th USENIX Security Symposium    2457
FFmalloc connects each memory chunk to its metadata
through a central three-level trie. The key of the mapping is
the address of each memory chunk, while the value points
to the metadata structure of the pool. That structure records
the start and end address of the pool, the type, the next avail-
able address or page, pointers to the type-specific metadata,
and unmapped regions. Jumbo pools only have type informa-
tion, start address, and end address. All metadata is stored
separately from the pool, like other secure allocators.
4.2 Freeing Memory
When the program calls free to free memory, FFmalloc first
locates the metadata of the pool, and marks the corresponding
slot in a bitmap of the binning pool, or updates the pointer
in the array of a continuous pool. Following the principle of
batch processing, it will not immediately release the memory
to the system. Instead, it waits for enough continuous pages
to be freed and returns them with one munmap system call.
Before compilation, we can change the minimum number of
continuous pages for a munmap system call. Increasing this
threshold trades memory for speed. As will be shown in §6.4,
we evaluated the tradeoff and empirically picked eight as the
default value in the current implementation.
Detecting double-free and invalid-free. While not an origi-
nal design goal, the metadata of FFmalloc helps us detect
double-free or invalid-free bugs. Double-free bugs free a
dangling pointer, which may corrupt the allocator’s meta-
data and lead to further attacks, like arbitrary memory access.
Invalid-free bugs instead free an address not allocated by
malloc. When the program invokes free, FFmalloc first lo-
cates the metadata corresponding to that address, and then
checks whether the underlying memory has been freed. It will
find the problem and terminate the execution if the program
tries to free a dangling pointer, or an invalid address.
Lazy free. The straightforward way to return pages to the
system is to invoke the munmap system call, where the ker-
nel will immediately release the memory. However, starting
from version 4.5, the Linux kernel provides an alternative
way for lazy memory release. Specifically, if we provide the