```
`core-site.xml`：
```
  fs.default.name
  hdfs://RaspberryPiHadoopMaster:54310
  hadoop.tmp.dir
  /hdfs/tmp
```
#### 设置两个从节点：
接下来[按照 “Because We Can Geek”上的教程](http://www.becausewecangeek.com/building-a-raspberry-pi-hadoop-cluster-part-2/)，你需要对上面的文件作出小小的改动。 在 `yarn-site.xml` 中主节点没有改变，所以从节点中不必含有这个 `slaves` 文件。
### III. 在我们的树莓派集群中测试 YARN
如果所有设备都正常工作，在主节点上你应该执行如下命令：
```
start-dfs.sh
start-yarn.sh
```
当设备启动后，以 Hadoop 用户执行，如果你遵循教程，用户应该是 `hduser`。
接下来执行 `hdfs dfsadmin -report` 查看三个节点是否都正确启动，确认你看到一行粗体文字 ‘Live datanodes (3)’：
```
Configured Capacity: 93855559680 (87.41 GB)
Raspberry Pi Hadoop Cluster picture Straight On
Present Capacity: 65321992192 (60.84 GB)
DFS Remaining: 62206627840 (57.93 GB)
DFS Used: 3115364352 (2.90 GB)
DFS Used%: 4.77%
Under replicated blocks: 0
Blocks with corrupt replicas: 0
Missing blocks: 0
Missing blocks (with replication factor 1): 0
————————————————-
Live datanodes (3):
Name: 192.168.1.51:50010 (RaspberryPiHadoopSlave1)
Hostname: RaspberryPiHadoopSlave1
Decommission Status : Normal
```
你现在可以做一些简单的诸如 ‘Hello, World!’ 的测试，或者直接进行下一步。
### IV. 安装 SPARK ON YARN
YARN 的意思是另一种非常好用的资源调度器（Yet Another Resource Negotiator），已经作为一个易用的资源管理器集成在 Hadoop 基础安装包中。
[Apache Spark](https://spark.apache.org/) 是 Hadoop 生态圈中的另一款软件包，它是一个毁誉参半的执行引擎和[捆绑的 MapReduce](https://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html)。在一般情况下，相对于基于磁盘存储的 MapReduce，Spark 更适合基于内存的存储，某些运行任务能够得到 10-100 倍提升——安装完成集群后你可以试试 Spark 和 MapReduce 有什么不同。
我个人对 Spark 还是留下非常深刻的印象，因为它提供了两种数据工程师和科学家都比较擅长的语言—— Python 和 R。
安装 Apache Spark 非常简单，在你家目录下，`wget "为 Hadoop 2.7 构建的 Apache Spark”`（[来自这个页面](https://spark.apache.org/downloads.html)），然后运行 `tar -xzf “tgz 文件”`，最后把解压出来的文件移动至 `/opt`，并清除刚才下载的文件，以上这些就是安装步骤。
我又创建了只有两行的文件 `spark-env.sh`，其中包含 Spark 的配置文件目录。
```
SPARK_MASTER_IP=192.168.1.50
SPARK_WORKER_MEMORY=512m
```
(在 YARN 跑起来之前我不确定这些是否有必要。)
### V. 你好，世界! 为 Apache Spark 寻找有趣的数据集!
在 Hadoop 世界里面的 ‘Hello, World!’ 就是做单词计数。
我决定让我们的作品做一些内省式……为什么不统计本站最常用的单词呢？也许统计一些关于本站的大数据会更有用。
如果你有一个正在运行的 WordPress 博客，可以通过简单的两步来导出和净化。
1. 我使用 [Export to Text](https://wordpress.org/support/plugin/export-to-text) 插件导出文章的内容到纯文本文件中
2. 我使用一些[压缩库](https://pypi.python.org/pypi/bleach)编写了一个 Python 脚本来剔除 HTML
```
import bleach
# Change this next line to your 'import' filename, whatever you would like to strip
# HTML tags from.
ascii_string = open('dqydj_with_tags.txt', 'r').read()
new_string = bleach.clean(ascii_string, tags=[], attributes={}, styles=[], strip=True)
new_string = new_string.encode('utf-8').strip()
# Change this next line to your 'export' filename
f = open('dqydj_stripped.txt', 'w')
f.write(new_string)
f.close()
```
现在我们有了一个更小的、适合复制到树莓派所搭建的 HDFS 集群上的文件。
如果你不能树莓派主节点上完成上面的操作，找个办法将它传输上去（scp、 rsync 等等），然后用下列命令行复制到 HDFS 上。
```
hdfs dfs -copyFromLocal dqydj_stripped.txt /dqydj_stripped.txt
```
现在准备进行最后一步 - 向 Apache Spark 写入一些代码。
### VI. 点亮 Apache Spark
Cloudera 有个极棒的程序可以作为我们的超级单词计数程序的基础，[你可以在这里找到](https://www.cloudera.com/documentation/enterprise/5-6-x/topics/spark_develop_run.html)。我们接下来为我们的内省式单词计数程序修改它。
在主节点上[安装‘stop-words’](https://pypi.python.org/pypi/stop-words)这个 python 第三方包，虽然有趣（我在 DQYDJ 上使用了 23,295 次 the 这个单词），你可能不想看到这些语法单词占据着单词计数的前列，另外，在下列代码用你自己的数据集替换所有有关指向 dqydj 文件的地方。
```
import sys
from stop_words import get_stop_words
from pyspark import SparkContext, SparkConf
if __name__ == "__main__":
  # create Spark context with Spark configuration
  conf = SparkConf().setAppName("Spark Count")
  sc = SparkContext(conf=conf)
  # get threshold
  try:
    threshold = int(sys.argv[2])
  except:
    threshold = 5
  # read in text file and split each document into words
  tokenized = sc.textFile(sys.argv[1]).flatMap(lambda line: line.split(" "))
  # count the occurrence of each word
  wordCounts = tokenized.map(lambda word: (word.lower().strip(), 1)).reduceByKey(lambda v1,v2:v1 +v2)
  # filter out words with fewer than threshold occurrences
  filtered = wordCounts.filter(lambda pair:pair[1] >= threshold)
  print "*" * 80
  print "Printing top words used"
  print "-" * 80
  filtered_sorted = sorted(filtered.collect(), key=lambda x: x[1], reverse = True)
  for (word, count) in filtered_sorted: print "%s : %d" % (word.encode('utf-8').strip(), count)
  # Remove stop words
  print "\n\n"
  print "*" * 80
  print "Printing top non-stop words used"
  print "-" * 80
  # Change this to your language code (see the stop-words documentation)
  stop_words = set(get_stop_words('en'))
  no_stop_words = filter(lambda x: x[0] not in stop_words, filtered_sorted)
  for (word, count) in no_stop_words: print "%s : %d" % (word.encode('utf-8').strip(), count)
```
保存好 wordCount.py，确保上面的路径都是正确无误的。
现在，准备念出咒语，让运行在 YARN 上的 Spark 跑起来，你可以看到我在 DQYDJ 使用最多的单词是哪一个。
```
/opt/spark-2.0.0-bin-hadoop2.7/bin/spark-submit –master yarn –executor-memory 512m –name wordcount –executor-cores 8 wordCount.py /dqydj_stripped.txt
```
### VII. 我在 DQYDJ 使用最多的单词
可能入列的单词有哪一些呢？“can, will, it’s, one, even, like, people, money, don’t, also“.
嘿，不错，“money”悄悄挤进了前十。在一个致力于金融、投资和经济的网站上谈论这似乎是件好事，对吧？
下面是的前 50 个最常用的词汇，请用它们刻画出有关我的文章的水平的结论。
![](/data/attachment/album/201705/07/023810wrwuwarhr6z6hpph.png)
我希望你能喜欢这篇关于 Hadoop、YARN 和 Apache Spark 的教程，现在你可以在 Spark 运行和编写其他的应用了。
你的下一步是任务是开始[阅读 pyspark 文档](https://spark.apache.org/docs/2.0.0/api/python/index.html)（以及用于其他语言的该库），去学习一些可用的功能。根据你的兴趣和你实际存储的数据，你将会深入学习到更多——有流数据、SQL，甚至机器学习的软件包！
你怎么看？你要建立一个树莓派 Hadoop 集群吗？想要在其中挖掘一些什么吗？你在上面看到最令你惊奇的单词是什么？为什么 'S&P' 也能上榜？
（题图：Pixabay，CC0）
---
via:  
作者：[PK](https://dqydj.com/about/#contact_us) 译者：[popy32](https://github.com/sfantree) 校对：[wxy](https://github.com/wxy)
本文由 [LCTT](https://github.com/LCTT/TranslateProject) 组织编译，[Linux中国](https://linux.cn/) 荣誉推出