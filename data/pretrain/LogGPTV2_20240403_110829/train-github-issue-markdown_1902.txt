### System Information
- **Custom Code**: Yes
- **OS Platform and Distribution**: Ubuntu 16.04
- **TensorFlow Installation Source**: Both (source and binary)
- **TensorFlow Version**: 1.4
- **Python Version**: 3.6
- **Bazel Version (if compiled from source)**: 0.5.4
- **GCC/Compiler Version (if compiled from source)**: 5.4.0
- **CUDA/cuDNN Version**: 8
- **GPU Model and Memory**: Tesla P100-PCIE-16GB

### Problem Description
When building TensorFlow with the MKL configuration (`--config=mkl`), the system does not fully utilize all available CPU cores. In my test case, the CPU load remains below 20%. In contrast, using the same build flags without MKL achieves 100% CPU load and nearly 10 times faster execution.

While experimenting with the MKL flags as described in the [TensorFlow Performance Guide](https://www.tensorflow.org/performance/performance_guide#optimizing_for_cpu), I observed some unusual behavior. Running the MKL-build with the following environment variables:

```sh
OMP_NUM_THREADS=27 KMP_SETTINGS=1 KMP_AFFINITY=verbose
```

results in the following output:

**User settings:**
- `KMP_AFFINITY=verbose`
- `KMP_SETTINGS=1`
- `OMP_NUM_THREADS=27`

**Effective settings:**
- `KMP_ABORT_DELAY=0`
- `KMP_ADAPTIVE_LOCK_PROPS='1,1024'`
- `KMP_ALIGN_ALLOC=64`
- `KMP_ALL_THREADPRIVATE=224`
- `KMP_ATOMIC_MODE=2`
- `KMP_BLOCKTIME=200`
- `KMP_CPUINFO_FILE: value is not defined`
- `KMP_DETERMINISTIC_REDUCTION=false`
- `KMP_DEVICE_THREAD_LIMIT=2147483647`
- `KMP_DISP_NUM_BUFFERS=7`
- `KMP_DUPLICATE_LIB_OK=false`
- `KMP_FORCE_REDUCTION: value is not defined`
- `KMP_FOREIGN_THREADS_THREADPRIVATE=true`
- `KMP_FORKJOIN_BARRIER='2,2'`
- `KMP_FORKJOIN_BARRIER_PATTERN='hyper,hyper'`
- `KMP_FORKJOIN_FRAMES=true`
- `KMP_FORKJOIN_FRAMES_MODE=3`
- `KMP_GTID_MODE=3`
- `KMP_HANDLE_SIGNALS=false`
- `KMP_HOT_TEAMS_MAX_LEVEL=1`
- `KMP_HOT_TEAMS_MODE=0`
- `KMP_INIT_AT_FORK=true`
- `KMP_INIT_WAIT=2048`
- `KMP_ITT_PREPARE_DELAY=0`
- `KMP_LIBRARY=throughput`
- `KMP_LOCK_KIND=queuing`
- `KMP_MALLOC_POOL_INCR=1M`
- `KMP_NEXT_WAIT=1024`
- `KMP_NUM_LOCKS_IN_BLOCK=1`
- `KMP_PLAIN_BARRIER='2,2'`
- `KMP_PLAIN_BARRIER_PATTERN='hyper,hyper'`
- `KMP_REDUCTION_BARRIER='1,1'`
- `KMP_REDUCTION_BARRIER_PATTERN='hyper,hyper'`
- `KMP_SCHEDULE='static,balanced;guided,iterative'`
- `KMP_SETTINGS=true`
- `KMP_SPIN_BACKOFF_PARAMS='4096,100'`
- `KMP_STACKOFFSET=64`
- `KMP_STACKPAD=0`
- `KMP_STACKSIZE=4M`
- `KMP_STORAGE_MAP=false`
- `KMP_TASKING=2`
- `KMP_TASKLOOP_MIN_TASKS=0`
- `KMP_TASK_STEALING_CONSTRAINT=1`
- `KMP_TEAMS_THREAD_LIMIT=56`
- `KMP_TOPOLOGY_METHOD=all`
- `KMP_USER_LEVEL_MWAIT=false`
- `KMP_VERSION=false`
- `KMP_WARNINGS=true`
- `OMP_CANCELLATION=false`
- `OMP_DEFAULT_DEVICE=0`
- `OMP_DISPLAY_ENV=false`
- `OMP_DYNAMIC=false`
- `OMP_MAX_ACTIVE_LEVELS=2147483647`
- `OMP_MAX_TASK_PRIORITY=0`
- `OMP_NESTED=false`
- `OMP_NUM_THREADS='27'`
- `OMP_PLACES: value is not defined`
- `OMP_PROC_BIND='false'`
- `OMP_SCHEDULE='static'`
- `OMP_STACKSIZE=4M`
- `OMP_THREAD_LIMIT=2147483647`
- `OMP_WAIT_POLICY=PASSIVE`
- `KMP_AFFINITY='verbose,warnings,respect,granularity=core,none'`

**OMP Info:**
- `OMP: Info #209: KMP_AFFINITY: decoding x2APIC ids.`
- `OMP: Info #207: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info`
- `OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55}`
- `OMP: Info #156: KMP_AFFINITY: 56 available OS procs`
- `OMP: Info #157: KMP_AFFINITY: Uniform topology`
- `OMP: Info #179: KMP_AFFINITY: 2 packages x 14 cores/pkg x 2 threads/core (28 total cores)`
- `OMP: Info #247: KMP_AFFINITY: pid 35537 tid 35708 thread 0 bound to OS proc set {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55}`
- (Similar output for other threads up to thread 80)

If I use the same execution flags with a build without MKL or with the pip version, the output is similar up to:

```sh
OMP: Info #247: KMP_AFFINITY: pid 36958 tid 37191 thread 27 bound to OS proc set {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55}
```

Afterwards, no OMP prints are generated. It appears that when building with MKL, TensorFlow continues to create more and more threads but cannot utilize them effectively.

Is this a configuration issue or a bug? If it's a known issue, please expand the performance guide.

@skye, pinging you because of your help with the performance issue with `while_loop`.