a learning phase and a detection phase. Before we elaborate
on the modules described in Figure 1, let us discuss two
high-level issues. First, which code-similarity algorithm(s) is
eﬀective for detecting which vulnerability? In answering this
question, we analyze a set of candidate code-similarity algo-
rithms by taking advantage of features describing vulnerabil-
ities and patches. This analysis leads to a CVE-to-algorithm
mapping, which maps a CVE-ID to the select code-similarity
algorithm(s) that is eﬀective for detecting the vulnerability.
Second, how should we generate and use vulnerability sig-
natures? Recall that a code-similarity algorithm can be
characterized by three attributes: code-fragment level, code
203
representation, and comparison method. We observe that
code-fragment level and code representation oﬀer guidance
for generating signatures of vulnerabilities as well as signa-
tures of target programs. These signatures are then com-
pared with each other for determining whether or not a tar-
get program has a vulnerability. If a vulnerability is found,
the location of the vulnerable code fragment(s) is reported.
3.2 Deﬁning vulnerability and code-reuse fea-
tures
Since our focus is to use code similarity analysis to detect
vulnerabilities, we need to deﬁne features to characterize
vulnerabilities and code reuses.
Features for describing vulnerability diﬀ hunks.
Given a vulnerability and its patch, the vulnerability can be
characterized by the vulnerability diﬀ, which is composed
of one or multiple diﬀ hunks. Each diﬀ hunk contains a
sequence of lines of code, where each line is preﬁxed by a
“+” symbol (addition), “-” symbol (deletion), or nothing. In
order to deﬁne features to describe a vulnerability, it is suf-
ﬁcient to deﬁne features that describe these diﬀ hunks. For
a diﬀ hunk, we deﬁne two sets of features: basic features and
patch features. Table 1 summarizes these features. Basic fea-
tures are the Type 1 features described in Table 1, including
the unique CVE-ID, the Common Weakness Enumeration
Identiﬁer (CWE-ID) that represents the vulnerability type,
product vendor, product aﬀected, and vulnerability severity.
Patch features are the Type 2–Type 6 features described in
Table 1 and describe the code changes from the unpatched
piece of code to the patched one. The ﬁve types of patch
features are elaborated as follows.
• Non-substantive features: These features describe chan-
ges in whitespace, format or comment which have no
impact on useful code.
• Component features: These features describe the chan-
ges of components in statements such as variables, op-
erators, constants, and functions.
• Expression features: These features describe the chan-
ges of expressions in statements such as assignment
expression, if condition, and for condition.
• Statement features: These features describe the chan-
ges of statements involving addition, deletion, and mo-
vement.
• Function-level features: These features describe the
changes of functions or changes outside a function,
such as macros and global variable deﬁnitions.
Features for describing code reuses. The term “code
reuse”often means code cloning [5, 26], including exact clones,
renamed clones, near miss clones, and semantic clones. For
vulnerability detection, we are given a piece of code con-
taining a vulnerability and a target piece of code that may
or may not contain the same vulnerability, where the latter
may or may not be an exact clone of the former. Note that
we have already deﬁned ﬁve types of patch features for de-
scribing vulnerabilities, namely Type 2–Type 6 described in
Table 1. Since these features can already describe the “trans-
formation” from an unpatched piece of vulnerable code to a
patched piece of code, which may be seen as a sort of code
reuse in a sense, we can naturally use these patch features
to describe code reuses.
3.3 Preparing the input
Table 1: Vulnerability diﬀ hunk features = basic
features (Type 1) + patch features (Type 2–Type
6). The patch features are also used as code-reuse
features.
Type Description
Basic features
1
CVE-ID
1-1
1-2
CWE-ID
Product vendor
1-3
Product aﬀected
1-4
1-5
Vulnerability severity
2
2-1
3
3-1
3-2
3-3
3-4
3-5
3-6
3-7
3-8
3-9
3-10
4
4-1
4-2
4-3
4-4
4-5
4-6
5
5-1
5-2
5-3
6
6-1
6-2
6-3
Non-substantive features
Changes in whitespace, format or comment
Component features
Variable name modiﬁcation
Constant modiﬁcation
Variable type modiﬁcation
Function name modiﬁcation
Function argument addition
Function argument deletion
Function argument modiﬁcation
Variable declaration addition
Variable declaration deletion
Operator modiﬁcation
Expression features
Assignment expression modiﬁcation
if condition modiﬁcation
for condition modiﬁcation
while condition modiﬁcation
do while condition modiﬁcation
switch condition modiﬁcation
Statement features
Line addition
Line deletion
Line movement
Function-level features
Entire function addition
Entire function deletion
Modiﬁcation beyond the function
In the learning phase, there are three inputs: the NVD,
VPD, and VCID. The NVD is a public database containing
the basic information of vulnerabilities that can be uniquely
identiﬁed by CVE-IDs. However, we need to build the VPD
and VCID by ourselves. The VPD contains the mappings
from CVE-IDs to diﬀs. Any vulnerability described in the
NVD with an explicit vulnerability diﬀ description is incor-
porated into the VPD. However, the NVD contains vulnera-
bilities that are caused by code reuse, but does not give any
explicit diﬀ description. For example, CVE-2015-0239 in
the NVD states that a Linux kernel prior to version 3.18.5
contains a vulnerability in the em_sysenter function, and
the NVD further gives a diﬀ description of the vulnerability.
However, the vulnerable pieces of code in these vulnerable
kernels are not identical. More speciﬁcally, the vulnerable
piece of code in kernel 3.18.1 indeed matches the vulner-
able piece of code corresponding to the diﬀ, but the vul-
nerable pieces of code in kernels 3.16.3 and 3.10.3 are not
the exact clones of the vulnerable piece of code correspond-
ing to the diﬀ, where the latter two are actually diﬀerent
from each other as well. In this case, the vulnerable pieces
204
of code in kernels 3.16.3 and 3.10.3 are treated as diﬀerent
code reuse instances, and are incorporated into the VCID
with each described by its respective code-reuse features.
These databases collectively allow us to achieve the follow-
ing. Given a CVE-ID, we can extract the basic features of
a vulnerability from the NVD, the patch features from the
VPD, and the code reuse instances of the vulnerability from
the VCID.
In the detection phase, the inputs are CVE-IDs, the target
programs of interest, the CVE-to-algorithm mapping, and
vulnerability signatures, where the last two are the output
of the learning phase.
3.4 Code-similarity algorithm selection
Extracting vulnerability diﬀ hunk features. For
each vulnerability diﬀ hunk, we extract the basic features
of a vulnerability from the NVD. From the VPD, we obtain
the diﬀ corresponding to the vulnerability in question. Af-
ter splitting the diﬀ into possibly multiple diﬀ hunks, we can
extract the patch features as follows. First, non-substantive
features and function level features can be extracted directly
from the diﬀ hunks. Second, the three other types of patch
features (i.e., Types 3–Type 5 in Table 1), can be extracted
via a sequence of edit actions from the unpatched piece of
code to the patched one using, for example, the gumtree
algorithm [11]. Finally, the feature vector of a vulnerabil-
ity diﬀ hunk is composed from the basic features and patch
features.
Code-similarity algorithm selection engine. This
module is to determine which code-similarity algorithm(s)
is eﬀective with respect to which vulnerability, where “eﬀec-
tive” means that the higher the similarity between a piece
of a target program and a piece of vulnerability code, the
higher the chance the target program contains the vulnera-
bility in question. For this purpose, we propose Algorithm
1, which has three steps as highlighted in Figure 2 and is
elaborated below.
Input
Candidate code-
similarity algorithms
Vulnerability diff 
hunk feature vectors
Threshold
VCID
Establish the ground truth
Step 1
Learn classifiers and filter them with accuracy 
threshold
Select the algorithms which can distinguish the 
unpatched piece of code from the patched one
Identify code-similarity algorithms with the most 
suitable code-fragment level
Select the algorithm with the lowest false-negative 
rate w.r.t. VCID
CVE-to-algorithm mapping
Step 2
Step 3
Output
Figure 2: Illustration of Algorithm 1
First, we observe that a good algorithm should be able
to distinguish the unpatched piece of code from the patched
one.
In order to evaluate whether or not a candidate al-
gorithm can achieve this goal, we use the vulnerability diﬀ
hunks in the VPD to establish the ground truth. For each
vulnerability diﬀ hunk described by a feature vector Fk ∈ F
as described in Table 1 and each candidate code-similarity
Algorithm 1 Code-similarity algorithm selection
Input:
A set of candidate code-similarity algorithms S =
{s1, . . . , sn};
a set of vulnerability diﬀ hunk
= {Fk}k, where Fk =
feature vectors F
(fk,1, . . . , fk,m, fk,m+1, . . . , fk,m+n) and m is the
number of features described in Table 1; the VCID;
threshold τ
Output:
CVE-to-algorithm mapping M = {(Fk, s)}k where
s ∈ S is the most suitable code-similarity algorithm
for diﬀ hunk Fk
1: S′ ← ∅
2: for each Fk ∈ F do
S′(Fk) ← ∅
3:
for each si ∈ S do
4:
5:
set fk,m+i ← 1 ifs i treats the patched piece of code cor-
responding to Fk as invulnerable (i.e., not similar to the
unpatched piece of code) and fk,m+i ← 0 otherwise
end for
6:
7: end for
8: partition F horizontally into F (1) and F (2)
9: for each si ∈ S do
10:
S′ ← S′ ∪{ si}
consider F (1)(i) ={(f k,1, . . . , fk,m, fk,m+i)}k as the projec-
tion of F (1) w.r.t. si
use machine learning to learn a classiﬁer from F (1)(i) and de-
note the classiﬁer’s accuracy by ai
if ai ≥ τ then
12:
13:
end if
14:
15: end for
16: for each Fk = (fk,1, . . . , fk,m, fk,m+i) ∈ F (2) do
17:
18:
19:
20:
21:
22:
23: end for
24: for each Fk ∈ F (2) do
25:
26:
{a pair (Fk, S′(Fk)) for each Fk ∈ F (2)}
S′(Fk) ← S′
for each si ∈ S′ do
for each si ∈ S′(Fk) do
determine the most suitable code-fragment level for Fk as
the one that leads to the smallest doa as described in text
if the fragment level used by si does not match the most
suitable code-fragment level for Fk then
if fk,m+i = 0 then
S′(Fk) ← S′(Fk) \ {si}
end if
end for
11:
27:
S′(Fk) ← S′(Fk) \ {si}
end if
end for
28:
29:
30:
31: end for
32: M← ∅
33: for each si ∈ S′(Fk) do
34:
{a pair (Fk, S′(Fk)) for each Fk ∈ F (2)}
use the code reuse instances in VCID that are suitable for si
(see text for details) to evaluate the false-negative rate of si,
denoted by fn(si)
35: end for
36: for each each Fk ∈ F (2) with associated (Fk, S′(Fk)) do
s ←{ si : si ∈ S′(Fk) ∧ fn(s) = mins′∈S′(Fk ){fn(s′)}}
37:
38: M←M ∪ {(Fk, s)}
39: end for
40: return M
algorithm si, we obtain the result on whether or not si treats
the patched code as vulnerable (i.e., treating the vulnerable
piece of code and the corresponding patched piece of code
as similar or not). The code-similarity algorithm result is
recorded as a class label fk,m+i (Lines 2-7). We partition
horizontally F = {Fk}k into F (1) and F (2) so that F (1)
will be used to learn a classiﬁer and F (2) will be further
used to select code-similarity algorithms. For each si, we
use the projection of F (1) on the diﬀ hunk features and the
label of si, namely F (1)(i) ={ (fk,1, . . . , fk,m, fk,m+i)}k to
learn a classiﬁer. For a learned classiﬁer with respect to