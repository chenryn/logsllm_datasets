practical attack: it is a running example in the ML security
literature given the increased popularity of the ML as a Service
(MLaaS) deployment scenario. In this most realistic yet limited
evaluation, we establish a baseline lower bound through a
membership inference attack [49].
Static Poison Attack: While “typical-case” privacy leakage
is an important factor in understanding privacy risk,
is
also worth evaluating the privacy of worst-case outliers. For
it
example, when training a model on a dataset containing
exclusively individuals of one type (age, gender, race, etc),
it is important to understand if inserting a training example
from an under-represented group would increase the likelihood
of private information leaking from the model. To study this
setting, we instantiate our adversary to construct worst-case
malicious inputs, and has access to the ﬁnal trained model.
Intermediate Poison Attack: As mentioned earlier, the DP-
SGD analysis assumes the adversary is given access to all
intermediate model parameters. We repeat the above attack,
but this time reveal all intermediate models. This allows us to
study the relative importance of this additional capability.
Adaptive Poison Attack: The DP-SGD analysis does not
have a concept of a dataset: it operates solely on gradients
computed over minibatches of training data. Thus, there is no
requirement that the one inserted example remains the same
between epochs: at each iteration, we re-instantiate the worst-
case inserted example after each epoch. This again allows us
to evaluate whether the minibatch perspective taken in the
analysis affects the lower bound we are able to provide.
Gradient Attack: As federated learning continues to receive
more attention, it is also important to estimate how much
additional privacy leakage results from training a model in
this environment. In federated learning a malicious entity can
poison the gradients themselves. As mentioned in Section II-C,
DP-SGD also assumes the adversary has access to the gra-
dients, therefore we mount an attack where the adversary
modiﬁes the gradient directly.
Dataset Attack:
In our ﬁnal and most powerful attack, we
make use of all adversary capabilities. The DP-SGD analysis
yields a guarantee which holds for all pairs of datasets with
a Hamming distance of 1, even if these datasets are patho-
logical worst-case datasets. Thus, in this setting, we allow the
adversary to construct such a pathalogical dataset. Evaluating
this setting is what allows us to demonstrate that the DP-SGD
analysis is tight. The tightness of the DP-SGD privay bounds
in the worst case can also be inferred from the theoretical
analysis [13, 41], however, the main purpose of this evaluation
is to show that our attack can be tight when the adversary is
the most powerful. If we could not reach the provable upper
bound, then we would have no hope that our results would
be tight in the other settings. Moreover, we crafted a speciﬁc
attack to achieve the highest possible privacy leakage.
A. API Access Adversary
As a ﬁrst example of how our adaptive analysis approach
proceeds, we consider the baseline adversary who mounts the
well-researched membership inference attack [49] in a setting
where they have access to an API. As described above, this
corresponds to the most practical setting—a black-box attack
where the attacker is only given access to an API revealing the
model’s output (its conﬁdence so has to be able to compute
the loss) on inputs chosen by the attacker. The schematic for
this analysis is given in Protocol 1.
a) Crafter:
(Crafter 1) Given any standard machine
learning dataset D, we can remove a random example from
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:26:44 UTC from IEEE Xplore.  Restrictions apply. 
871
Section
§ IV-A
§ IV-B
§ IV-C
§ IV-D
§ IV-E
§ IV-F
Experiment
API access
Static Poison Input
Intermediate Attack
Adaptive Poison Input
Gradient Attack
Dataset Attack
Access
Black-box
Final Model
Modiﬁcation
Random Sample
Malicious Sample
Intermediate Models Malicious Sample
Intermediate Models
Adaptive Sample
Intermediate Models Malicious Gradient
Intermediate Models Malicious Gradient
Dataset
Not modiﬁed
Not modiﬁed
Not modiﬁed
Not modiﬁed
Not modiﬁed
Malicious
Protocol
Protocol 1
Protocol 1
Protocol 2
Protocol 3
Protocol 4
Protocol 4
Adversaries
Crafter 1
Crafter 2
Crafter 2
Crafter 3
Crafter 4
Crafter 5
Distinguisher 1
Distinguisher 1
Distinguisher 2
Distinguisher 2
Distinguisher 3
Distinguisher 3
Results
Fig. 3
Fig. 4
Fig. 5
Fig. 6
Fig. 7
Fig. 8
TABLE I: Experiment settings. We run attacks under six set of adversary capabilities; each experiment increases the capabilities
of the adversary ranging from the weakest membership inference adversary to the strongest adversary with full DP capabilities.
Membership Inference Adversary Game
Adversary
(D, D(cid:48))
$← A
Model Trainer
$← {0, 1}
b
B = (D, D)
fθ ← T (Bb)
fθ
s ← B(fθ, D, x∗, y∗)
s ∈ {0, 1}
Check if s = b
Protocol 1: Adversary game for membership inference
attack. Round 1. The adversary chooses two datasets. Round
2. The model trainer randomly trains a model on one of these,
and returns the model. Round 3. The adversary predicts which
dataset was used for training.
the dataset to construct a new dataset D(cid:48) that differs from
$← D be a
D in exactly one record. Formally, let (x, y)
sample from the underlying data distribution at random. We
let A = {D, D ∪ {(x, y)}}, We formalize this in Crafter 1.
Crafter 1 Membership Inference Adversary
Require: existing training dataset D, underlying data distribution D
$← D
1: (x, y)
2: D(cid:48) ← D ∪ {(x, y)}
3: return D, D(cid:48)
b) Model
training: Recall from Section VI that
the
trainer gets two datasets: D and D(cid:48). Then the trainer se-
lects one of these datasets randomly and trains a model on
the selected dataset using pre-deﬁned hyperparameters. After
training completes, the trainer outputs the trained model fθ.
c) Distinguisher: (Distinguisher 1) After the model fθ
has been trained, the Distinguisher now guesses which dataset
was used by computing the loss of the trained model on the
one differing example (cid:96)(fθ, x, y). It guesses that the model
was trained on D(cid:48) if the loss is sufﬁciently small, and guess
that it was trained on D otherwise. In the rest of the paper,
we refer to the differing example (x, y) as the query input.
Details are given in Distinguisher 1.
Previous works [48, 45, 39, 9] showed that if an instance is
part of the training dataset of the target model, its loss is most
Distinguisher 1 Membership Inference Adversary
Require: model fθ, query input (x, y), threshold τ
1: L ← (cid:96)(fθ, x, y)
2: if L ≤ τ then
return D(cid:48)
3:
4: end if
5: return D
likely less than the case when it is not part of the training
dataset. That is, in this attack, we provide our instantiated
adversary with an extremely limited power. In particular, the
attacker has much less capabilities than what is assumed to
analyze the differential privacy guarantees provided by DP-
SGD. Protocol 1 summarizes the attack.
d) Results: Typically, membership inference is studied
as an attack in order to learn if a given user is a member of
the training dataset or not. However, we do not study it as an
attack, but as a way to provide a lower bound on the privacy
leakage that arises from training with DP-SGD.
We experiment with a neural network using three datasets
commonly used in privacy research: MNIST [33], CIFAR-
10 [32], and Purchase. The ﬁrst two of these are standard
image classiﬁcation datasets, and Purchase is the shopping
records of several thousand online customers, extracted during
Kaggle’s “acquire valued shopper” challenge [28]. Full details
of these datasets is given in Appendix VI-A
In each trial, we follow Protocol 1 for each of the three
datasets. We train a differentially private model using DP-
SGD setting ε to typical values used in the machine learning
literature: 1, 2, 4, and 10. As mentioned before, the more
trials we perform the better we are able to establish a lower
bound of ε. Due to computational constraints, we are limited
to performing the attack 1, 000 times. These experiments took
3,000 GPU hours, parallelized over 24 GPUs.
When we perform this attack on the CIFAR-10 dataset and
train a model with ε = 4 differential privacy, the attack true
positive rate is 0.017 and false positive is 0.002. By using
the Clopper-Pearson method, we can probabilistically lower-
bound this attack performance; for example, when we have
performed 1000 trials, there is a 95% probability that the
attack false positive rate is lower than 0.01. Using Equations 4
and 5, we can convert empirical lower bound of (ε, δ)-DP
with (0.31, 10−5). This value is substantially lower than the
provably correct upper bound of (4, 10−5).
Figure 3 shows the empirical epsilon for the other datasets
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:26:44 UTC from IEEE Xplore.  Restrictions apply. 
872
ε
l
a
c
i
t
c
a
r
P
10
5
0
MNIST
Purchase
CIFAR
Theoretical
1
2
4
Theoretical ε
10
ε
l
a
c
i
t
c
a
r
P
10
5
0
MNIST
Purchase
CIFAR
Theoretical
1
2
4
Theoretical ε
10
Fig. 3: Membership inference attack: the adversary only
adds one sample from the underlying data distribution.
Fig. 4: Malicious input attack: the adversary has blackbox
access. The maliciously crafted input leaks more information
than a random sample from the data distribution.
and values of differential privacy. For the MNIST dataset, the
adversary’s advantage is not signiﬁcantly better than random
chance; we hypothesize that this is because MNIST images
are all highly similar, and so inserting or removing any one
training example from the dataset does not change the model
by much. CIFAR-10 and Purchase, however, are much more
diverse tasks and the adversary can distinguish between two
dataset D and D(cid:48) much more easily. In general, the API access
attack does not utilize any of the assumptions assumed to be
available to the adversary in the DP-SGD analysis and as
a consequence the average input from the underlying data
distribution does not
leak as much private information as
suggested by the theoretical upper bound. However, as the
tasks get more complex information leakage increases.
There are two possible interpretations of this result, which
will be the main focus of this paper:
1) Interpretation 1. The differentially private upper bound
is overly pessimistic. The privacy offered by DP-SGD is
much stronger than the bound which can be proven.
2) Inteerpretation 2. This attack is weak. A stronger attack
could have succeeded more often, and as such the bounds
offered by DP-SGD might be accurate.
Prior work has often observed this phenomenon, and for
example trained models with ε = 105-DP [6] despite this
offering almost no theoretical privacy, because empirically the
privacy appeared to be strong. However, as we have already
revealed in the introduction, it turns out that Interpretation 2
is correct: while, for this weak attack, the bounds are loose,
this does not imply DP-SGD itself is loose when an adversary
utilizes all capabilities.
B. Static Input Poisoning Adversary
The previous attack assumes a weak adversary to establish
the ﬁrst baseline lower bound in a realistic attack setting.
Nothing constructed by A is adversarial per se, but rather
selected at random from a pre-existing dataset. This subsection
begins to strengthen the adversary.
Differential privacy bounds the leakage when two datasets
D and D(cid:48) differ in any instance. Thus, by following prior work
[26], we create an adversary that crafts a poisoned malicious
input x such that when the model trains on this input, its
output will be different from the case where the instance is
not included—thus making membership inference easier.
a) The Crafter:
(Crafter 2) Inspired by Jagielski et
al. [26], we construct an input designed to poison the ML
model. Given access to samples from the underlying data
distribution, the adversary trains a set of shadow models (e.g.,
as done in [49]). The adversary trains shadow models using
the same hyperparameters as the model trainer will use, which
allows adversary to approximate how model trainer’s model
will behave. Next, the adversary generates an input with an
adversarial example algorithm [42]. If a model trains on that
input, the learned model be different from a model which is
not trained on that input. Crafter 2 summarizes the algorithm.
Crafter 2 Static Adversary Crafting
Require: train dataset Dshadow, adversarial
train steps T ,
learning rate s, model learning rate lr, training dataset D
1
← Tdpsgd(Dshadow)
,··· , f shadow
input
n
(cid:80)n
1: f shadow
$← random sample from Dshadow
2: x, y
3: for T times do
lmalicious ← 1
4:
x ← x + s∇xlmalicious
5:
6: end for
7: D(cid:48) = D ∪ {(x, y)}
8: return D, D(cid:48)
i=1 (cid:96)(f shadow
n
i
, x, y)
b) Model training: Identical to prior.
c) The Distinguisher: (Distinguisher 1) Identical to prior.
d) Results: As visualized in Figure 4, this adversary is
able to leverage its poisoning capability to leak more private
information and achieve a higher empirical
lower bound
compared to the previous attack. This is consistent across
all datasets. This is explained by the fact that the poisoned
input is a worst-case input for membership inference whereas
previously the attack was conducted on average-case inputs
drawn from the training distribution. The results suggest that
DP-SGD bounds are bounded below by a factor of 10×.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:26:44 UTC from IEEE Xplore.  Restrictions apply. 
873