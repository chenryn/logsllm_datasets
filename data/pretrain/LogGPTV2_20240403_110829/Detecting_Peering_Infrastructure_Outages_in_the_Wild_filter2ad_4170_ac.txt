positive nor a false negative.
Attrition of BGP Communities: To understand the attrition rate
of location-encoding communities we study the communities clas-
sified either as “geographical location” or as “interconnection point”
by Donnet and Bonaventure in 2008 [33]. Only 552 of the 2,980
communities in their dictionary are visible in the aggregated Route-
Views/RIS BGP data across 2016, while the rest appear not to be
used anymore. On the other hand, of the 5,284 communities in our
dictionary, only 471 (9%) are also in the 2008 dictionary. However,
only 7 (1.5%) of the common community values changed meaning
after almost a decade, indicating that the semantics of communities
201120122013201420152016100002000030000400005000060000UniqueCommunityvaluesUniquevalues(lefty-axis)20002500300035004000450050005500UniqueCommunitytop-16bitsUniquetop16-bits(righty-axis)SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
V. Giotsas et al.
The first step is to parse the BGP Communities attribute of the
collected BGP routes and find paths annotated with the traversed
Points-of-Presence (PoPs). We use these paths to analyze the PoP-
level routing dynamics. When we use the term “PoP” without any
other qualification, we refer to any of city, IXP, or facility. We filter-
out transient paths to ensure that we have a stable baseline of the
routing system, and we update the set of stable paths periodically
to account for path changes after the start of our detection process.
Next, we start monitoring the incoming BGP updates for PoP-
level deviations from the stable baseline. Instead of checking for AS
path changes, we check if the relevant community values change.
When we observe a large enough fraction of paths that deviates
from the baseline PoP within the same time frame, we call it outage
signal. An outage signal corresponds to a spike in localized routing
activity and indicates that a routing incident affected a specific PoP.
Yet, it does not indicate if the incident is due to an outage.
Link-level events such as the de-peering of two large peers, or
AS-level incidents such as the disconnection of an IXP member,
can also lead to such an outage signal. To determine the source of
the signal, we trigger a detailed signal investigation process that
classifies the signal as link-level, AS-level, or PoP-level based on
the number and disjointedness of the affected ASes.
If the signal is classified as a PoP-level outage, the algorithm
proceeds to explore the granularity of the PoP. Here, we combine
the colocation map with active traceroute measurements that we
collect either opportunistically by mining public traceroute reposi-
tories, such as those provided by PathCache [95], or by executing
our own targeted traceroute campaigns. The traceroute paths help
us to validate the outage and eliminate false positives by mapping
the IP-level hops to IXPs and facility interfaces using the techniques
described in [50, 76]. When the data-plane and control-plane infer-
ence identify the same PoP as the source of the outage, we consider
the outage as validated. We determine the length of the outage (i)
by actively probing the involved interfaces and (ii) by monitoring
BGP messages for changes in the communities that indicate that
the paths have returned to the baseline PoP. Since we mainly rely
on passive measurements via BGP, our active monitoring is rather
selective and does not rely on greedily probing all infrastructure
addresses. Therefore, our approach is practical and conforms to the
resource limitations of publicly available measurement platforms,
including RIPE Atlas [90] and Looking Glasses [48].
4 THE KEPLER SYSTEM
In this section, we present the design and implementation of Ke-
pler2, a system that relies on our methodology to detect outages
in the wild and investigate them. While the analysis of BGP data
is lightweight, our experience with operating Kepler shows that
the efficient design of different modules is critical to make the sys-
tem practical and accurate. Figure 6 illustrates the architecture of
Kepler.
4.1 Input Module: Data Preprocessing
The first part of Kepler preprocesses all data sources. First, it gener-
ates the BGP Community dictionary and the colocation map. For
the continuous BGP data we use BGPStream [79] to decouple Kepler
2Data and additional technical details are available at http://kepler.inet.tu-berlin.de
Figure 5: The geographic spread of trackable infrastructure.
within an AS change rarely. Since location-encoding communities
are used for operational purposes, such as troubleshooting and
traffic engineering, the stability of community semantics minimizes
the risk of misconfigurations when setting these communities on
prefix advertisements.
The above findings highlight the value of our automated commu-
nity interpretation to enable a frequent extension of the community
dictionary with new values, to remove stale entries, and to maintain
a high-degree of coverage of the active communities. Moreover, the
risk of misinterpreting the community values due to stale entries is
small even in the time span of years.
3.3 Colocation Map
The majority of the communities annotate routes at city-level gran-
ularity, which is too coarse to pinpoint a peering infrastructure
outage at the facility-level or IXP-level. To achieve the intended
detection granularity, we complement the BGP communities with
a high-resolution colocation map that includes three types of inter-
connections: (i) ASes to IXPs, (ii) ASes to facilities, and (iii) IXP to
facilities. For each facility we also record the building-level address,
so that we know which facilities, IXPs and ASes operate at the cities
annotated by our community dictionary. To this end, we mine the
colocation data from PeeringDB [81] and DataCenterMap [27], as
well as individual AS websites. Since names of facilities and facility
operators are not standardized, we use the facility address (postcode
and country) to identify common facilities among the different data
sources. We then merge the tenants listed in each data source for
the same facility to increase the completeness of our colocation
map. Similarly, IXP names also differ between datasets. To identify
and merge the records that refer to the same IXP we use the URLs
of the IXP websites, and the location (city/country) where the IXP
operates. We use the constructed colocation map in the city-level
outage signal arbitration to de-correlate the “fate” of various ASes
in the same city during an incident, based on their presence or
absence at facilities. Thus, we can pinpoint the likely facility-level
or IXP-level location of incidents and increase the coverage of our
outage detection capabilities to physical locations beyond those
explicitly encoded in BGP communities.
3.4 Detection Methodology Overview
To detect and localize peering infrastructure outages we propose
Algorithm 1. Its input is a stream of BGP data, the BGP Community
dictionary, the colocation map, as well as targeted active measure-
ments for incident investigation.
City-levelIXP-levelFacility-levelDetecting Peering Infrastructure Outages in the Wild
SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
Algorithm 1: Overview outage detection and investigation
Input: (BGP paths, BGP Community Dictionary, Colocation Map,
Targeted Active Measurements)
Output: Location, Time and Duration of a PoP-level Outage
Pathsmapped ←− Map BGP paths to traversed PoPs based on the
attached Communities meta-data;
Pathsst abl e
for BGP updates in new measurement interval do
mapped ←− Filter-out transient paths;
mapped ←− calculate how many paths diverted from the
Pathsdiver t ed
PoP in the stable baseline;
P athsdiver t ed
mapped
P athsst abl e
mapped
if
> Tf ail then
Signal investigation
Siдnaltype ←− Infer the type of outage signal based on the
number of affected ASes and AS links;
if Siдnaltype is PoP then
POP BGP
colocation map;
POP t r ace
traceroute queries;
if POP BGP
type ←− Determine the type of PoP based on the
type ←− Confirm the affected PoP through
type ≡ POP t r ace
dur ation ←− record the duration of the
outage
while Outaдest at e is True do
then
type
return Outaдe(time, POP, dur ation)
from the sources of BGP feeds, and thus, obtain a unified feed of
sorted BGP records. In addition, Kepler sanitizes the collected paths
by discarding paths with AS loops, private ASNs, or special-purpose
ASNs [22]. Currently, we use all RouteView and RIPE RIS collec-
tors. For every BGP update with attached BGP community values,
Kepler uses the dictionary to infer which physical infrastructure
a route traverses. Hereby, Kepler also infers which location-based
BGP community refers to which hop of the BGP path, either by
mapping the first two octets of the community to the same ASN
hop in the path, or by applying the methodology in [51] in the case
of IXP route server communities.
4.2 Monitoring Module: Outage Detection
Kepler’s monitoring module identifies all the PoPs P for which we
have physical location information from the community dictionary.
These are the PoPs that we monitor in detail. Then, Kepler periodi-
cally computes a set of stable routes that involve p for all p ∈ P. A
prefix route is stable if it traverses P for a period of ds consecutive
days (the default value is 2 days). Thereafter, we check for PoP-level
routing changes vs. the baseline stable path. Hereby, we consider
the following change to a route from s to d involving PoP p ∈ P:
(i) an explicit withdrawal, (ii) another AS path not involving PoP p,
and (iii) an announcement with another community—an implicit
withdrawal. In addition, we check for BGP State messages to detect
potential disruptions in the BGP feed that can cause gaps in our
BGP stream and disregard updates due to it. Note, if the AS path
changes but the community tag involving p remains the same, we
do not consider the update a route change for p. However, we con-
sider changes to the community tag as route change even if the AS
path remains unchanged.
We bin routing updates in time intervals to correlate path changes
with routing incidents. Since most of the ASes that set the ingress
Communities are close to one of our BGP collectors it suffices to
use a relatively short time interval. We use a binning interval of
60 seconds (twice the default MRAI time [88]). At the end of each
binning interval we compare the paths from the baseline to the
paths in the current bin and determine the fraction of paths that
continues to traverse p. If this fraction is below a threshold of Tf ail
we may have an outage signal. However, an aggregated comparison
of all the paths can be biased by ASes that account for a dispropor-
tionately large number of paths. For instance, if a partial outage in p
affects the paths of many regional ASes but not the paths of a large
Tier-1 AS, then the total fraction of paths may not fall below the
detection threshold Tf ail causing a false-negative. Therefore, we
group the paths based on the ASes that are involved in the tagged
links and determine outages per AS. If the fraction of paths of an AS
a involving p falls below the threshold T , we say that a is subject to
an outage signal in the current binning interval. An outage signal is
an indicator of a possible outage event but the definite inference is a
task of the signal investigation module. After each binning interval,
we remove the changed paths from the set of stable paths. We also
refresh the set of stable paths every 2 days to account for new paths
and new community values. Note, the focus of this module is to
detect the start of an outage.
4.3 Outage Signal Investigation
Kepler’s outage signal investigation considers all outages signaled
within a time interval and determines the granularity of the trigger-
ing event. We distinguish four incidents: (i) link-level, (ii) AS-level,
(iii) operator-level, and (iv) PoP-level outages. For PoP-level events
we identify the physical location. Kepler also tracks the new physi-
cal location after the rerouting of a stable path and the time it takes
for a path to return back based on the same principle to estimate the
duration of the outage. To increase the confidence for the duration
of each outage and the reaction of network operators, Kepler relies
on targeted active measurements.
We distinguish four different granularities of outage signals.
Link-level: Changes to an AS-link with a large number of prefixes,
can cause an outage signal, e.g., a de-peering or even a MED change
between two Tier-1 ASes. Since such link-level incidents are not
the focus of this paper we require that more than three different
ASes have to be affected to trigger an investigation.
AS-level: Changes in the availability of a densely connected AS
can cause multiple of its peers to change their paths away from
a specific location concurrently. For instance, if an IXP member
decides to terminate its membership, it will terminate all public
peering BGP sessions at that IXP. If all affected links intersect at a
single common AS, either as near-end or as far-end neighbor, we
classify the signal as AS-level.
Operator-level: We combine multiple AS-level outages to an operator-
level outage, if all of the affected links include ASes that belong
to the same operator. Our motivation is that operators often ad-
minister multiple sibling ASes each with different functions but
SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
V. Giotsas et al.
Figure 6: Flowchart of Kepler’s outage signal detection and investigation.
often hosted on the same infrastructures. For instance, the Equinix
Ashburn Exchange hosts three different sibling ASes operated by
Bell Canada. An organizational-wide policy or network change will
effect all sibling ASes. We map ASes to organizations using the
methodology from [14].
PoP-level: When a signal involves multiple AS links with disjoint
near-end and far-end ASes and organizations, we classify it as
PoP-level. In particular, we require that the set of affected links
includes at least three different non-sibling near-end ASes and three-