the distributions p((cid:96)(f (x), y) | ˜Qin/out(x, y)) by sampling.
Figure 3 plots histograms of model losses on four CIFAR-10
images when the image is contained in the model’s training
dataset (red) and when it is absent (blue). We chose these
images to illustrate two different axes of variation. On the
columns we compare “inliers” to “outliers”, as determined by
the model’s loss when not trained on the example. The left
column shows examples with low loss when omitted from
the training set, those in the right column have high loss.
On rows we compare how easy the examples are to ﬁt. The
examples in the top row have very low loss when trained
on, while the examples in the bottom row have higher loss.
Importantly, observe that these two dimensions do measure
Fig. 4: The model’s conﬁdence, or its logarithm (the cross-
entropy loss) are not normally distributed. Applying the logit
function yields values that are approximately normal.
different quantities. An example can be an outlier but easy to
ﬁt (upper right), or an inlier but hard to ﬁt (lower left).
The goal of a membership inference adversary is to distin-
guish the two distributions in Figure 3 for a given example.
This view illustrates the shortcomings of prior attacks (e.g.,
the LOSS attack): a global threshold on the observed loss
(cid:96)(f (x), y) cannot distinguish between the different scenarios
in Figure 3. The only conﬁdent assessment that such an attack
can make is that examples with high loss are non-members. In
contrast, the Likelihood-ratio test in Equation (2) considers the
hardness of each example individually by modeling separate
pairs of distributions ˜Qin, ˜Qout for each example (x, y).
C. Estimating the likelihood-ratio with parametric modeling
We directly turn this observation into a membership infer-
ence attack by computing per-example hardness scores [37,
56, 68, 69]. By training models on random samples of data
from the distribution D, we obtain empirical estimates of the
distributions ˜Qin and ˜Qout for any example (x, y). And from
here, we can estimate the likelihood from Equation 3 to predict
if an example is a member of the training dataset or not.
To improve performance at very low false-positive rates,
instead of empirically modeling the distributions ˜Qin/out di-
rectly from the data, we opt for a parametric and model ˜Qin/out
by Gaussian distributions. Parametric modeling has several
signiﬁcant beneﬁts over nonparametric modeling.
• Parametric modeling requires training fewer shadow
models to achieve the same generalization of nonpara-
metric approaches. For example, we can match the recent
(nonparametric) work of [69] with 400× fewer models.
• We can extend our attack to multivariate parametric
models, allowing us to further improve attack success
rate by querying the model multiple times (§VI-C).
Doing this requires some care. Indeed, as can be seen in
Figure 3, the model’s cross-entropy loss is not well approx-
imated by a normal distribution. First, the cross-entropy loss
is on a logarithmic scale. If we take the negative exponent,
exp(−(cid:96)(f (x), y)), we instead obtain the model “conﬁdence”
f (x)y, which is bounded in the interval [0, 1] and thus not
normally distributed either (i.e., the conﬁdences for outliers
and inliers concentrate, respectively, around 0 and 1). We thus
apply a logit scaling to the model’s conﬁdence,
(cid:18) p
(cid:19)
1 − p
φ(p) = log
,
for p = f (x)y
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:24:27 UTC from IEEE Xplore.  Restrictions apply. 
51901
0100200easytoﬁt/inliermembernon-membereasytoﬁt/outlier10−51000100200hardtoﬁt/inlier10−5100hardtoﬁt/outlierbirddogairplanetruck0.00.51.00200conﬁdencef(x)y10−710−3101CEloss−log(f(x)y)−20020logitscalingφ(f(x)y)Algorithm 1 Our online Likelihood Ratio Attack (LiRA).
We train shadow models on datasets with and without the
target example, estimate mean and variance of the loss dis-
tributions, and compute a likelihood ratio test. (In our ofﬂine
variant, we omit lines 5, 6, 10, and 12, and instead return
the prediction by estimating a single-tailed distribution, as is
shown in Equation (4).)
Require: model f, example (x, y), data distribution D
1: confsin = {}
2: confsout = {}
3: for N times do
4: Dattack ←$ D
5:
6:
7:
8:
9: end for
10: µin ← mean(confsin)
11: µout ← mean(confsout)
in ← var(confsin)
12: σ2
out ← var(confsout)
13: σ2
14: confobs = φ(f (x)y)
p(confobs
p(confobs
fin ← T (Dattack ∪ {(x, y)})
confsin ← confsin ∪ {φ(fin(x)y)}
fout ← T (Dattack\{(x, y)})
confsout ← confsout ∪ {φ(fout(x)y)}
(cid:46) Sample a shadow dataset
(cid:46) train IN model
(cid:46) query target model
15: return Λ =
(cid:46) train OUT model
| N (µin, σ2
| N (µout, σ2
in))
out))
to obtain a statistic in the range (−∞,∞) that is (empirically)
approximately normal. Figure 4 displays the distributions of
model conﬁdences, the negative log of the conﬁdences (the
cross-entropy loss), and the logit of the conﬁdences. Only the
logit appraoch is well approximated by a pair of Gaussians.
Our complete online attack (Algorithm 1). We ﬁrst train
N shadow models [60] on random samples from the data
distribution D, so that half of these models are trained on the
target point (x, y), and half are not (we call these respectively
IN and OUT models for (x, y)). We then ﬁt two Gaussians to
the conﬁdences of the IN and OUT models on (x, y) (in logit
scale). Finally, we query the conﬁdence of the target model f
on (x, y) and output a parametric Likelihood-ratio test.
This attack is easily parallelized across multiple target
points. Given a dataset D ← D, we train shadow models on
N subsets of D, chosen so that each target (x, y) ∈ D appears
in N/2 subsets. The same N shadow models can then be used
to estimate the Likelihood-ratio test for all examples in D.
As an optimization, we can improve the attack by querying
the target model on multiple points x1, x2, . . . , xm obtained
by applying standard data augmentations to the target point
x (as previously observed in [6]). In this case, we ﬁt m-
dimensional spherical Gaussians N (µin, σ2
outI)
to the losses collected from querying the shadow models m
times per example, and compute a standard likelihood-ratio
test between two multivariate normal distributions.
inI),N (µout, σ2
Our ofﬂine attack. While our online attack is effective, it has
a signiﬁcant usability limitation: it requires the adversary train
new models after they are told to infer the membership of the
example (x, y). This requires training new machine learning
models for every (batch of) membership inference queries, and
is computationally expensive.
To improve the efﬁciency of our attack, we propose an
ofﬂine attack algorithm that trains shadow models on randomly
sampled datasets ahead of time, and never trains shadow
models on the target points. For this attack, we remove lines
5,6,10 and 12 from Algorithm 1, and only estimate the mean
out of model conﬁdences when the target
µout and variance σ2
example is not
in the shadow models’ training data. We
then change the likelihood-ratio test in line 15 to a one-
sided hypothesis test. That is, we measure the probability of
observing a conﬁdence as high as the target model’s under the
null-hypothesis that the target point (x, y) is a non-member:
(4)
Λ = 1 − Pr[Z > φ(f (x)y)], where Z ∼ N (µout, σ2
The larger the target model’s conﬁdence is compared to µout,
the higher the likelihood that the query sample is a member.
Similar to our online attack, we improve the attack by querying
on multiple augmentations and ﬁtting a multivariate normal.
out) .
V. ATTACK EVALUATION
We now investigate our ofﬂine and online attack variants in
a thorough evaluation across datasets and ML techniques.
Again, we focus extensively on the low-false positive rate
regime. This is the setting with the most practical conse-
quences: for example, to extract training data [4] it is far more
important for attacks to have a low false positive rate than
high average success, as false positives are fare more costly
than false negatives. Similarly, de-identifying even a few users
contained in a sensitive dataset is far more important than
saying an average-case statement “most people are probably
not contained in the sensitive dataset”.
We use both datasets traditionally used for membership
inference attack evaluations, but also new datasets that are
less typically used. In addition to the CIFAR-10 dataset
introduced previously, we also consider three other datasets:
CIFAR-100 [29] (another standard image classiﬁcation task),
ImageNet [9] (a standard challenging image classiﬁcation task)
and WikiText-103 [40] (a natural language processing text
dataset). For CIFAR-100, we follow the same process as for
CIFAR-10 and train a wide ResNet [71] to 60% accuracy on
half of the dataset (25,000 examples). For ImageNet, we train
a ResNet-50 on 50% of the dataset (roughly half a million
examples). For WikiText-103, we use the GPT-2 tokenizer [53]
to split the dataset into a million sentences and train a small
GPT-2 [53] model on 50% of the dataset for 20 epochs to
minimize the cross-entropy loss. Prior work has additionally
performed experiments on two toy datasets that we do not
believe are meaningful benchmarks for privacy because of
their simplicity: Purchase and Texas (see [60] for details).2
2While these datasets ostensibly have privacy-relevance, we believe it is
more important to study datasets that reveal interesting properties of machine
learning than datasets that discuss privacy. We nevertheless present these
results in the Appendix, but encourage future work to omit these results and
focus on the more informative tasks we consider.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:24:27 UTC from IEEE Xplore.  Restrictions apply. 
61902
Fig. 5: Success rate of our attack on CIFAR-10, CIFAR-
100, ImageNet, and WikiText. All plots are generated with
256 shadow models, except ImageNet which uses 64.
For each dataset, the adversary trains N shadow models
(N = 64 for ImageNet, and N = 256 otherwise) on training
sets chosen so that each example (x, y) is contained in exactly
half of the shadow models’ training sets (thus, for each
example we have N/2 IN models, and N/2 OUT models). We
use the entire dataset for this purpose, and thus the training
sets of individual shadow models and the target model may
partially overlap. This is a strong assumption, which we make
here mainly due to the small size of some of the datasets we
consider. In Section VI-D, we show that our attack works just
as well when the adversary trains shadow models on datasets
that are fully disjoint from the target model’s training set.
For all datasets except ImageNet, we repeat each attack 10
times and report the attack success rates across all 10 attacks.
A. Online attack evaluation
Figure 5 presents the main results of our online attack when
evaluated on the four more complex of the datasets mentioned
above (CIFAR-10, CIFAR-100, ImageNet, and WikiText-103).
Even though these datasets are complex, it is relatively efﬁ-
cient to train most of these models—for example a CIFAR-10
or CIFAR-100 model takes just six minutes to train. Additional
results for the Purchase and Texas dataset are given in the
Appendix—these datasets are much simpler and while they
are typically used for membership inference, we argue they
are too simple to have generalizable lessons.
Our attack has true-positive rates ranging from 0.1% to
10% at a false-positive rate of 0.001%. If we compare the
three image datasets, consistent with prior works, we ﬁnd that
the attack’s average success rate (i.e., the AUC) is correlated
directly with the generalization gap of the trained model. All
three models have perfect 100% training accuracy, but the test
accuracy of the CIFAR-10 model is 90%, the ImageNet model
is 65%, and the CIFAR-100 model is 60%. Yet, at low false-
positives, the CIFAR-10 models are easier to attack than the
ImageNet models, despite their better generalization.
Fig. 6: Success rate of our ofﬂine attack on CIFAR-
10, CIFAR-100, ImageNet, and WikiText. All plots are
generated with 128 OUT shadow models, except ImageNet
which uses 32. For each dataset, we also plot our online attack
with the same number of shadow models (half IN, half OUT).
B. Ofﬂine attack evaluation
Figure 6 evaluates our ofﬂine attack from Section IV-C,
where the adversary performs the costly operations of training
shadow models only before being handed the target query
point (x, y). Our attack performs only slightly worse in this
ofﬂine setting—at an FPR of 0.1%, our ofﬂine attack’s TPR
is at most 20% lower than that of our best online attack with
the same number of shadow models.
C. Re-evaluating prior membership inference attacks
In order to understand how our attack compares to prior
work, we now re-evaluate prior attack techniques under our
low-FPR objective. We study these attacks following the same
evaluation protocol introduced above and on the same datasets
(for WikiText, we omit a few entries for attacks that are not
directly applicable to sequential language models).
A summary of our analysis is presented in Table I. We
compare the efﬁcacy of eight representative attacks from the
literature. For each attack, we compute a full ROC curve and
select a decision threshold that maximizes TPR at a given FPR.
Surprisingly, we ﬁnd that despite being published in 2019, the
attack of Sablayrolles et al. [56] outperforms other attacks
under our metric (often by an order of magnitude), even when
compared to more recent attacks such as Jayaraman et al. [25]
(PETS’21) and Song and Mittal [61] (USENIX’21).
Shadow models. One of the ﬁrst membership inference attacks
(due to Shokri et al. [60]) that improves on the baseline LOSS
attack, introduced the idea of shadow models, but used in a
simpler way than we have done here. Each shadow model
fi (of a similar type to the target model f) is trained on
random subsets Di of training data available to the adversary.
The attack then trains a new neural network g to predict an
example’s membership status. Given the pre-softmax features
fi(x) and class label y, the model g predicts whether the data
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:24:27 UTC from IEEE Xplore.  Restrictions apply. 
71903
10−510−410−310−210−1100FalsePositiveRate10−510−410−310−210−1100TruePositiveRateCIFAR-100,auc=0.925CIFAR-10,auc=0.720ImageNet,auc=0.765WikiText,auc=0.71510−510−410−310−210−1100FalsePositiveRate10−510−410−310−210−1100TruePositiveRateCIFAR-100,auc=0.859CIFAR-10,auc=0.674ImageNet,auc=0.728WikiText-103,auc=0.713Onlinew
o
d
a
h
s
s
l
e
d
o
m
e
l