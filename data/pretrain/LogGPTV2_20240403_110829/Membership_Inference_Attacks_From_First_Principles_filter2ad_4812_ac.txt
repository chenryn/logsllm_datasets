### Distributions and Model Losses

We explore the distributions \( p(\ell(f(x), y) | \tilde{Q}_{\text{in/out}}(x, y)) \) through sampling. Figure 3 illustrates histograms of model losses for four CIFAR-10 images, comparing scenarios where the image is included in the training dataset (red) and when it is not (blue). These images were selected to highlight two distinct axes of variation.

- **Inliers vs. Outliers**: The columns compare "inliers" (low loss when omitted from training) to "outliers" (high loss when omitted from training). The left column shows examples with low loss when excluded from the training set, while the right column shows those with high loss.
- **Ease of Fitting**: The rows compare how easily the examples can be fit. The top row includes examples with very low loss when trained on, whereas the bottom row includes examples with higher loss.

Importantly, these two dimensions measure different quantities. An example can be an outlier but easy to fit (upper right), or an inlier but hard to fit (lower left).

### Membership Inference Adversary

The goal of a membership inference adversary is to distinguish between the two distributions shown in Figure 3 for a given example. This perspective highlights the limitations of prior attacks, such as the LOSS attack, which use a global threshold on the observed loss \(\ell(f(x), y)\). Such a threshold cannot differentiate between the various scenarios in Figure 3. The only confident assessment such an attack can make is that examples with high loss are non-members. In contrast, the Likelihood-ratio test in Equation (2) considers the hardness of each example individually by modeling separate pairs of distributions \(\tilde{Q}_{\text{in}}\) and \(\tilde{Q}_{\text{out}}\) for each example \((x, y)\).

### Estimating the Likelihood-Ratio with Parametric Modeling

We leverage this observation to develop a membership inference attack by computing per-example hardness scores [37, 56, 68, 69]. By training models on random samples from the distribution \(D\), we obtain empirical estimates of the distributions \(\tilde{Q}_{\text{in}}\) and \(\tilde{Q}_{\text{out}}\) for any example \((x, y)\). From these, we can estimate the likelihood from Equation 3 to predict if an example is a member of the training dataset.

To enhance performance at very low false-positive rates, we use parametric modeling to represent \(\tilde{Q}_{\text{in/out}}\) with Gaussian distributions instead of directly modeling them from the data. Parametric modeling offers several advantages over nonparametric approaches:

- **Reduced Training Requirements**: It requires fewer shadow models to achieve the same generalization as nonparametric methods. For instance, we can match the recent (nonparametric) work of [69] with 400 times fewer models.
- **Multivariate Extensions**: We can extend our attack to multivariate parametric models, allowing us to further improve the success rate by querying the model multiple times (§VI-C).

However, this approach requires careful consideration. As seen in Figure 3, the model's cross-entropy loss is not well approximated by a normal distribution. The cross-entropy loss is on a logarithmic scale, and taking the negative exponent, \(\exp(-\ell(f(x), y))\), yields the model's confidence \(f(x)_y\), which is bounded in the interval [0, 1] and thus not normally distributed. Therefore, we apply a logit scaling to the model's confidence:
\[
\phi(p) = \log \left( \frac{p}{1 - p} \right), \quad \text{for } p = f(x)_y
\]

Figure 4 displays the distributions of model confidences, the negative log of the confidences (cross-entropy loss), and the logit of the confidences. Only the logit approach is well approximated by a pair of Gaussians.

### Online Likelihood Ratio Attack (LiRA)

Algorithm 1 outlines our online Likelihood Ratio Attack (LiRA). We train shadow models on datasets with and without the target example, estimate the mean and variance of the loss distributions, and compute a likelihood ratio test.

```plaintext
Algorithm 1: Our online Likelihood Ratio Attack (LiRA)
Require: model f, example (x, y), data distribution D
1: confsin = {}
2: confsout = {}
3: for N times do
4:   Dattack ←$ D
5:   fin ← T (Dattack ∪ {(x, y)})
6:   confsin ← confsin ∪ {φ(fin(x)y)}
7:   fout ← T (Dattack\{(x, y)})
8:   confsout ← confsout ∪ {φ(fout(x)y)}
9: end for
10: µin ← mean(confsin)
11: µout ← mean(confsout)
12: σ2_in ← var(confsin)
13: σ2_out ← var(confsout)
14: confobs = φ(f (x)y)
15: return Λ = p(confobs | N (µin, σ2_in)) / p(confobs | N (µout, σ2_out))
```

This attack can be parallelized across multiple target points. Given a dataset \(D\), we train shadow models on \(N\) subsets of \(D\), ensuring each target \((x, y) \in D\) appears in \(N/2\) subsets. The same \(N\) shadow models can then be used to estimate the Likelihood-ratio test for all examples in \(D\).

As an optimization, we can query the target model on multiple points \(x_1, x_2, \ldots, x_m\) obtained by applying standard data augmentations to the target point \(x\). In this case, we fit \(m\)-dimensional spherical Gaussians \(N(\mu_{\text{in}}, \sigma^2_{\text{in}} I)\) and \(N(\mu_{\text{out}}, \sigma^2_{\text{out}} I)\) to the losses collected from querying the shadow models \(m\) times per example, and compute a standard likelihood-ratio test between two multivariate normal distributions.

### Offline Attack

While our online attack is effective, it has a significant usability limitation: it requires the adversary to train new models after being told to infer the membership of the example \((x, y)\). This is computationally expensive.

To address this, we propose an offline attack algorithm that trains shadow models on randomly sampled datasets ahead of time and never trains shadow models on the target points. For this attack, we remove lines 5, 6, 10, and 12 from Algorithm 1, and only estimate the mean \(\mu_{\text{out}}\) and variance \(\sigma^2_{\text{out}}\) of model confidences when the target example is not in the shadow models' training data. We then change the likelihood-ratio test in line 15 to a one-sided hypothesis test:
\[
\Lambda = 1 - \Pr[Z > \phi(f(x)y)], \quad \text{where } Z \sim N(\mu_{\text{out}}, \sigma^2_{\text{out}})
\]
The larger the target model's confidence compared to \(\mu_{\text{out}}\), the higher the likelihood that the query sample is a member. Similar to our online attack, we improve the attack by querying on multiple augmentations and fitting a multivariate normal.

### Attack Evaluation

We evaluate our offline and online attack variants across datasets and ML techniques, focusing on the low-false positive rate regime. This setting is crucial because false positives are more costly than false negatives, especially in applications like extracting training data [4].

We use both traditional and less common datasets for membership inference attack evaluations. In addition to CIFAR-10, we consider CIFAR-100, ImageNet, and WikiText-103. For CIFAR-100, we train a wide ResNet [71] to 60% accuracy on half of the dataset. For ImageNet, we train a ResNet-50 on 50% of the dataset. For WikiText-103, we use the GPT-2 tokenizer [53] to split the dataset into a million sentences and train a small GPT-2 [53] model on 50% of the dataset for 20 epochs.

### Results

**Online Attack Evaluation**

Figure 5 presents the results of our online attack on CIFAR-10, CIFAR-100, ImageNet, and WikiText-103. Our attack achieves true-positive rates ranging from 0.1% to 10% at a false-positive rate of 0.001%. Despite better generalization, CIFAR-10 models are easier to attack than ImageNet models at low false-positives.

**Offline Attack Evaluation**

Figure 6 evaluates our offline attack, showing that it performs only slightly worse than the online attack. At an FPR of 0.1%, the TPR of our offline attack is at most 20% lower than that of our best online attack with the same number of shadow models.

**Re-evaluating Prior Attacks**

Table I summarizes the comparison of our attack with eight representative attacks from the literature. Surprisingly, the attack of Sablayrolles et al. [56] outperforms other attacks under our metric, even when compared to more recent works.

**Shadow Models**

One of the first membership inference attacks (Shokri et al. [60]) introduced the idea of shadow models. Each shadow model \(f_i\) is trained on random subsets \(D_i\) of training data available to the adversary. The attack then trains a new neural network \(g\) to predict an example's membership status based on the pre-softmax features \(f_i(x)\) and class label \(y\).