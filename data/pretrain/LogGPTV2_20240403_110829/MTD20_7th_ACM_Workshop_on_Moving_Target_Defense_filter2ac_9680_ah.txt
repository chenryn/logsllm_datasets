to ensure a sufﬁcient level of computational power available
by a trusted entity to manage MTD operations.
Table V summarizes the key modeling and solution techniques
of MTD and their pros and cons discussed above.
VII. METRICS FOR MTD
The underlying idea of MTD techniques has been explored;
accordingly many MTD techniques have been developed.
However, no standard metrics have been proposed to measure
their effectiveness and/or efﬁciency. In this section, we discuss
what metrics have been used to assess the effectiveness and
efﬁciency of the existing MTD techniques. Along with the
discussions of the overall trends observed from this survey,
we address some limitations of the metrics used in the state-
of-the-art MTD approaches.
A. Metrics for Measuring MTD Effectiveness
We discuss the metrics to measure the effectiveness of
MTD techniques in terms of the perspectives of an attacker
and a defender. The attacker’s metric estimates its attack
performance, indicating that the attacker’s high performance
refers to the defender’s low performance, and vice-versa.
The defender’s metric measures its performance in achieving
security and/or defense goals of a given system.
The attacker’s metrics estimates the adverse impact of the
proposed MTD techniques on the attacker’s performance and
are obtained by:
• Attack success probability (ASP) [3, 9, 20, 23, 24, 26, 30,
43, 46, 127, 138, 161]: This metric refers to the probability
that attacks are successfully performed. For example,
it
refers to the probability that a system component (or de-
fender) is compromised or a target is successfully discovered
and/or accessed by an attacker. Rahman et al. [127] also used
a metric called attackability, which refers to the probability
that an attacker can access system states (or components)
to the attack. This metric combines the degree of system
vulnerability plus the feasibility an attacker accesses and
performs an attack based on its own resource level. But
in the sense that it measures the degree for the attacker
to successfully access a system component, this metric is
aligned with ASP.
• Attack utility [5, 6, 20, 109, 125, 168, 169]: When the inter-
actions between attackers and defenders and their best strate-
gies are considered based on game theoretic approaches, the
payoff (or utility) of an attacker is used to measure the gain
or loss by deploying a proposed MTD.
• Learning by attackers [168]: This measures the degree
of an attacker’s learning toward the payoff obtained by a
defender upon the performed attack.
• Mean time to compromise a system (MTTC) [4, 24,
25, 30, 173]: This indicates how long an attacker takes
to compromise an entire system. In terms of a defender’s
perspective, this metric is similar to mean time to failure
(MTTF), as described under a defender’s metrics below.
• Unpredictability [60, 100, 136]: This indicates how much
confusion and/or uncertainty a given MTD has introduced
to attackers.
• Attack surface [102, 104]: This metric is deﬁned as the
amount of system resources that can be used by attackers
to attack the system, such as channels, data items, and/or
methods. Hence, a larger attack surface exposes more vul-
nerabilities.
In the literature, various types of the defender’s metrics are
used to measure MTD effectiveness in terms of the following
metrics:
• Defense success probability (DSP) [3, 33, 36, 161]: We
call metrics measuring the success of an MTD technique
DSP in this work. Zaffarano et al. [161] estimated the rate
of executing successful defenses (e.g., for a defender, the
rate at which tasks are executed and completed) or attacks
(e.g., for an attacker, the rate at which attacks are performed
and successfully completed). Colbaugh and Glass [36] used
a detection accuracy of anomaly behaviors, such as attack
behaviors or spams in order to determine whether to trigger
an MTD operation. Clark et al. [33] measured the portion of
decoy nodes detected by attackers when IP randomization
techniques are used in order to measure the success of the
IP-shufﬂing MTD strategy. Al-Shaer et al. [3] measured an
IP-mutation success probability to measure the effectiveness
of the MTD. This metric measures the probability that a
mutated IP is not hit by scanning attacks.
• Mean time to failure (MTTF) [24, 25, 30, 173]: This refers
to a system reliability metric capturing the system’s up-time
in the presence of attacks when failures can happen due to
either defects or security threats. This metric is the same as
MTTC under the attacker’s metrics.
• Defense utility [20, 109, 125, 168, 169]: Game theoretic
approaches for the optimal deployment of MTD techniques
have taken to identify the best defense strategy by a de-
fender. The payoff (or utility) of the defender measures the
effectiveness of an MTD technique.
• Learning by defenders [168]: This metric measures the
degree of a defender’s learning toward the payoff an attacker
has obtained upon a defense action taken by the defender.
• System security: Various kinds of metrics measure the
system security properties enhanced by proposed MTD
techniques:
– Conﬁdentiality [125, 165, 161]: This measures how
many system components are compromised [125]. In
some context, some information should be kept conﬁden-
tial, such as private information. The degree of preserving
conﬁdential or private information is another metric to
indicate the degree of security. In [161], mission con-
ﬁdentiality refers to the degree of exposing conﬁdential
information to unauthorized parties while attack conﬁden-
tiality means the degree of attack behaviors detected by
a defender.
– Integrity [161]: Integrity metric is discussed in terms of
mission integrity and attack integrity. Mission integrity
refers to how much information related to executing a
given mission is communicated without being modiﬁed
and/or forged, while attack integrity indicates how much
accurate information the attackers view.
– Availability [60, 125]: This indicates the portion of
system assets that are not compromised to provide a
normal service.
– Degree of vulnerability [4, 5, 6, 20, 23, 24, 25, 168]:
This measures the probability that a given platform to
be selected is vulnerable during a particular time period
or a given system component is vulnerable because it is
controlled by an attacker.
• Other metrics: Based on the unique features of each of the
existing MTD approaches, various other types of metrics
20
have been adopted to measure the effectiveness of MTD as
follows:
– Controllability [125]: This refers to the portion of critical
system assets which expose a high vulnerability to an
attacker if compromised.
– Worm propagation speed [3, 78, 79]: This measures
how much a deployed MTD can slow down actions by
an attacker. This also indirectly increases the detection of
attackers by earning more time to monitor the attacker.
– Vastness [60, 100, 104, 165]: This measures the size
of spaces that a given defense mechanism can cover,
such as IP spaces an attacker needs to scan through. In
addition, the number of target hosts set by a defender
can consume an attacker’s resource because it determines
all the possibilities the attacker needs to scan through.
In [104], this is considered based on the metric called
‘attack surface measurements.’
– Periodicity [60, 100]: This estimates how often system
conﬁgurations change in order to provide a sufﬁcient level
of confusion to attackers.
– Uniqueness [60, 100]: This measures how uniquely an
individual entity (e.g., a host) is authorized to a system
without being accessed by other entities.
– Revocability [60, 100]: This measures the degree of fre-
quency to terminate or expire a prior system conﬁguration
(e.g., access control or a given IP conﬁguration).
– Distinguishability [60, 100]: This measures how well a
given defense distinguishes trustworthy entities from non-
trustworthy entities.
– Loss in rewards between an optimal deployment and
an executed deployment [132]: This metric captures how
much loss occurred for the actual execution of an MTD
operation over the optimal deployment.
The metrics to measure the effectiveness of existing MTD
techniques are summarized in Figs. 6 and 7 based on 27 papers
published during 2011-2018. Note that more than one metric
can appear in a single paper. The general trends observed from
the survey are: (1) the attack success probability (e.g., whether
an attacker achieved its goal of a launched attack such as
ﬁnding a vulnerable target) is a dominant metric used for the
effectiveness of MTD aiming to minimize this metric; (2) due
to a large volume of game theoretic approaches in the state-
of-the-art MTD techniques, the payoff or utility of attackers
or defenders is also one of dominant metrics used in the
existing MTD techniques; and (3) some system-level metrics
measuring system vulnerability or reliability (e.g., degree of
vulnerability, MTTF or mean time to compromise an entire
system) in the presence of attacks are observed as major
metrics used to measure MTD effectiveness.
B. Metrics for Measuring MTD Efﬁciency
The attacker’s metrics are used to capture how much cost
(or penalty) is introduced for an attacker to achieve attack
success when a proposed MTD is deployed, as follows:
• Penalty in attack payoff [49, 83, 158, 165]: Many game
theoretic MTD approaches estimate attack cost at an abstract
level (e.g., cost is 1 for attacking; 0 otherwise).
System performance
13
3
QoS to users
12
Defense cost
Fig. 8. Metrics measuring MTD efﬁciency by a defender’s perspective.
• Attack cost [6, 5, 85, 103, 138]: This measures how much
overhead and/or impact is introduced to attackers to perform
their attacks. To be speciﬁc, an attacker’s scanning tool, such
as N map, is used to capture the scanning overhead [85].
The MTD efﬁciency by the attacker’s perspective is mainly
measured by two types of metrics as above although both
metrics are concerned about resources the attacker needs to
invest to launch a planned attack. Due to the similar nature of
both metrics and their small volume, we omit the ﬁgure.
The defender’s metrics to measure MTD efﬁciency that
are commonly observed in the literature include:
• Quality-of-Service (QoS) to users [33, 62, 158]: This met-
ric captures the degree of service quality provided to users
while implementing a given MTD technique as triggering
an MTD (e.g., platform migration) often hinders service
availability to normal users [62, 158]. In addition, upon de-
ploying IP mutation techniques, the number of connections
interrupted [33] is used to measure QoS provided to users.
• System performance [7, 31, 34, 45, 51, 62, 77, 95, 108,
123, 154, 155, 164]: This metric measures how much
overhead is introduced to deploy a given MTD, such
as message overhead (e.g., delay, packet loss, or control
packet overhead) [45, 164], operational delay [108, 164] /
cost [51, 62, 154, 155, 164] to deploy an MTD, the number
of dropped connections [34], or performance overhead (e.g.,
ﬁle sizes or performance degradation to distribute software
for diversity) [77]. System performance is also captured by
system throughput measuring how much a given system
(or network) can maintain its performance in terms of
network throughput (e.g., how many messages are correctly
delivered) [7, 164] or server throughput (e.g., how many
queries are properly provided) [123].
• Defense cost: Various aspects of defense cost are measured
to indicate the efﬁciency of MTD techniques. An abstract
level of defense cost (i.e., migration cost or maintenance cost
of VMs) [30, 49, 83, 158, 165] is used to measure the cost
of a deployed MTD mechanism, which is mostly used in
game theoretic MTD. Some defense cost captures the level
of infrastructure (e.g., a number of proxy or decoy nodes)
required to ensure a required level of service availability [83,
158]. Some other works also used speciﬁc metrics to capture
defense cost as follows:
– Address space overhead [3]: In deploying Random IP
21
mutation techniques, this refers to the required address
space based on mutation speed (e.g.,
low frequency
mutation, LFM, or high frequency mutation, HFM).
– Flow table size [78, 79, 164]: This measures the size of
ﬂow table in OpenFlow (OF) switches when OF-RHM
(Random Host Mutation) is used in an SDN-based MTD.
– Integrated performance cost [34, 51]: This metric inte-
grates both performance and security cost. Ge et al. [51]
considered bandwidth cost and risk at servers upon being
attacked to calculate the overall performance cost. Clark
et al. [34] deﬁned the cost function in terms of the number
of active sessions, caused by triggering MTD operations,
and the fraction of decoy nodes scanned by attackers. In
these works, the goal is to minimize the performance cost
that reﬂects defense cost, security, and service availability
to users.
– Strategy switching cost [132]: This cost measures the
switching cost (e.g., migration cost). Sengupta et al.
[132] estimated the switching cost in switching web-stack
conﬁgurations as a cost metric.
– Power consumption [163]: When MTD is deployed in
resource-constrained environments such as wireless sen-
sor networks or IoT environments, energy consumption is
one of the key design considerations. How much beneﬁt
is introduced over the power consumption by deploying
an MTD technique is a critical metric to measure the
efﬁciency of the MTD.
The metrics to measure the efﬁciency of MTD by a de-
fender’s perspective are summarized in Fig. 8 based on 25
papers published during 2011-2018. Since more than one
metric can be used in a paper, the number of metrics countered
is not the same as the total number of works examined in this
survey.
As demonstrated in Fig. 8, most metrics measuring MTD
efﬁciency belong to system performance or defense cost.
However, the level of QoS provided to users are signiﬁcantly
less studied. This means MTD technology focuses more on
enhancing system security and performance with minimum
cost while service availability for users to provide seamless,
continuous service provision has remained much less explored
in designing MTD techniques. Since deploying and executing
MTD mechanisms to a system introduces a critical tradeoff
between security, defense cost, and service availability, an
optimization problem with these dual conﬂicting goals should
be investigated in-depth to meet multiple criteria from both
system goals (i.e., multi-objective optimization problem) [29].
VIII. EVALUATION METHODS FOR MTD
MTD techniques have been veriﬁed by using various types
of evaluation techniques. In this section, we discuss how the
performance of MTD techniques have been assessed based on
the following evaluation methods: (1) analytical models; (2)
simulation models; (3) emulation models; and (4) real testbeds.
A. Analytical Model-based MTD Evaluation
1) Probabilistic Model-based MTD Evaluation: In proba-
bilistic models, the behaviors of a system, an attacker, and
the addresses space scanned,
a defender and the interactions between them are described
based on probabilistic parameters. Okhravi et al. [115] con-
structed a probability model to measure the mean time to
security failure (MTTSF) where the security failure is deﬁned
by the system state being compromised by an attacker where
the system is defended by MTDs. Zhuang et al. [170] also
modeled the relationship between the frequency of diversity-
based MTD and ASP. Carroll et al. [23] provided probabilistic
models to measure the effectiveness of an address shufﬂing-
based MTD technique based on ASP with respect
to the
network size,
the degree of
system vulnerability, and the frequency of shufﬂing operations.
Crouse et al. [39] developed probabilistic models to measure
ASP when a set of reconnaissance defenses, including honey-
pots as a deception technique and network address shufﬂing
as MTD, is deployed in a given system, under varying the
network size,
the size of honeypots deployment, and the
number of vulnerable nodes. Cho and Ben-Asher [30] used a
probabilistic model by building a Stochastic Petri Nets (SPN)
model to describe an integrated defense system consisting of
MTD, deception, and an IDS and analyzed the performance
of the integrated defense system compared to the system
with various combinations of defense mechanisms (i.e., an
IDS only or IDS plus either deception or MTD) in terms of
ASP and MTTSF (i.e., system lifetime). Sharma et al. [138]
used probabilistic models to measure the effectiveness of the
proposed IP-multiplexing based network shufﬂing techniques
in terms of ASP and defense cost. Luo et al. [98] used
probabilistic models to verify the effectiveness of a port
hopping-based MTD technique against reconnaissance attacks
in terms of ASP.