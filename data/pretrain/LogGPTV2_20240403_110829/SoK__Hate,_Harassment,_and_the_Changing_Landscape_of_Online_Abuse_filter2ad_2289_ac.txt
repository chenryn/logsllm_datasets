e
m
s
e
r
i
u
q
e
R
)
1
C
(
?
e
c
n
e
i
d
u
a
n
a
f
o
n
o
i
t
p
e
c
e
d
s
e
r
i
u
q
e
R
)
2
A
(
?
e
c
n
e
i
d
u
a
y
b
n
e
e
s
e
b
o
t
d
e
d
n
e
t
n
I
)
1
A
(
?
t
e
g
r
a
t
y
b
n
e
e
s
e
b
o
t
d
e
d
n
e
t
n
I
)
2
C
(
?
y
t
i
r
o
h
t
u
a
y
t
r
a
p
-
d
r
i
h
t
a
f
o
n
o
i
t
p
e
c
e
d
s
e
r
i
u
q
e
R
(cid:32)
(cid:32)
(cid:32)
Category
Toxic
content
Content
leakage
Overloading
False
reporting
Impersonation
Surveillance
Lockout
and control
Non-exhaustive list of attacks
Bullying
Trolling
Hate speech
Profane or offensive content
Threats of violence
Purposeful embarrassment
Incitement
Sexual harassment
Unwanted explicit content (“sexting”)
Sextortion
Doxing
Outing and deadnaming
(cid:32) (cid:71)(cid:35) (cid:71)(cid:35)
(cid:32) (cid:71)(cid:35) (cid:71)(cid:35)
(cid:32) (cid:71)(cid:35) (cid:32)
(cid:71)(cid:35) (cid:71)(cid:35) (cid:32)
(cid:32) (cid:71)(cid:35) (cid:32)
(cid:71)(cid:35) (cid:71)(cid:35) (cid:32)
(cid:71)(cid:35) (cid:71)(cid:35) (cid:32)
(cid:32) (cid:71)(cid:35) (cid:71)(cid:35)
(cid:32) (cid:71)(cid:35) (cid:32)
(cid:32) (cid:71)(cid:35) (cid:32)
(cid:71)(cid:35) (cid:32) (cid:32)
(cid:71)(cid:35) (cid:32) (cid:32)
Non-consensual image exposure (“revenge porn”) (cid:71)(cid:35) (cid:32) (cid:32)
(cid:71)(cid:35) (cid:32) (cid:32)
(cid:32) (cid:32) (cid:32)
(cid:32) (cid:32) (cid:32)
(cid:32) (cid:32) (cid:32)
(cid:32)
(cid:71)(cid:35)
(cid:32)
(cid:71)(cid:35) (cid:32) (cid:32)
(cid:71)(cid:35) (cid:32)
(cid:32)
(cid:32) (cid:71)(cid:35) (cid:32)
(cid:32) (cid:71)(cid:35)
Leaked chats, proﬁles
Comment spam
Dogpiling
Raiding or brigading
Distributed denial of service (DDoS)
Notiﬁcation bombing
Zoombombing
Negative ratings & reviews
SWATing
Falsiﬁed abuse report
Falsiﬁed abuse ﬂag
Impersonated proﬁles
Impersonated chats or images
Impersonated webpages (SEO)
Synthetic pornography
Hijacked communication
Stalking or tracking
Account monitoring
Device monitoring
IoT monitoring (passive)
Browser monitoring (passive)
IoT manipulation (active)
Browser manipulation (active)
Account lockout
Content deletion
(cid:32)
(cid:32)
(cid:32)
(cid:71)(cid:35)
(cid:32) (cid:71)(cid:35) (cid:32)
(cid:32) (cid:32) (cid:32)
(cid:32) (cid:71)(cid:35) (cid:32)
(cid:32) (cid:32) (cid:71)(cid:35)
(cid:71)(cid:35) (cid:32) (cid:32) (cid:32)
TABLE I: Taxonomy of online hate and harassment attacks, broken down by audience, communication channel, and capabilities involved.
We annotate each attack with the most common intents of the attacker, though nuanced relationships between an attacker and target make
such harms difﬁcult to generalize. A (cid:32) indicates that a criterion always holds true, while a (cid:71)(cid:35) indicates that a criterion frequently holds true.
No entry indicates a criteria does not hold. The stratiﬁcation of attacks across our criteria result in seven distinct categories of threats.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:30:40 UTC from IEEE Xplore.  Restrictions apply. 
251
Overloading [Availability; A1 + C3]. Overloading includes
any scenario wherein an attacker forces a target (A1) to triage
hundreds of notiﬁcations or comments via ampliﬁcation (C3),
or otherwise makes it
technically infeasible for the target
to participate online due to jamming a channel (potentially
via a distributed denial of service attack) (C3). Examples
include organized trolling activity orchestrated through Face-
book [120], Reddit [95], and 4chan [75]; the use of “SMS
bombers” to send thousands of text messages to a target [126];
or “zoombombing” which disrupts a video conference [101].
These attacks lead to frustration, fatigue, and a reduced sense
of emotional safety. The content used may also be toxic or
leaked, exacerbating the harm.
Noteworthy examples include “brigading”, where a large
group of people overwhelm the comment feed of a targeted
group or individual (e.g., coordinated “raids” on YouTube
channels by 4chan members [106]); or “dogpiling” where a
person is targeted in order to recant an opinion or statement.
DDoS attacks can also enable censoring by overloading an
individual’s network connection, preventing them from using
the Internet or disabling a web site and thus making content
unavailable [93]. Such attacks closely mirror for-proﬁt DDoS
attacks using botnets [87]. Finally, attacks may involve en
masse negative comments and reviews, similar to Pizzagate
and Gamergate [1], [85].
False Reporting [Integrity; C2]. False reporting broadly
captures scenarios where an attacker deceives a reporting
system or emergency service (C2)—originally intended to
protect people—to falsely accuse a target of abusive behavior.
Prominent examples include SWATing, where an attacker
falsely claims a bomb threat, murder, or other serious crime
in order to send emergency responders to the target’s address.
The FBI reported roughly 400 cases of SWATing in 2013 [80],
and in 2017 there was one fatal incident [94]. Other forms
of false reporting include when an attacker ﬂags a piece
of content or an account as abusive (for instance, on social
media platforms), which we call “falsiﬁed abuse ﬂagging”.
These markings may in turn trigger automated algorithms
that remove the “offending” content or suspend the target’s
account. Past examples include a far-right group in Israel
abusing Facebook’s reporting tools to suspend a rival’s account
and to report images of his children [147]. Attackers may also
ﬁle doctored evidence (e.g., “falsiﬁed abuse reports”) with
either platforms or police to convince an authority to take
action on a target.
Impersonation [Integrity; A2 + M1 + C1]. Impersonation
occurs when an attacker relies on deception of an audience
(A2 + C1) to assume the online persona of a target in order to
create content (M1) that will damage the target’s reputation
or inﬂict emotional harm. Satire does not meet this attack
deﬁnition, as there is no intent to deceive. Attacks involving
impersonation include setting up fake social media accounts
purported to be associated with a target [66]; exploiting
privileged access to a target’s account to send emails or social
media messages [66], [108]; spooﬁng the sender email address
or phone number of a target to make it appear as if the
target authored the message [66]; and setting up websites that
appear to be authored by the target, often in conjunction with
use of search-engine optimization (SEO) techniques to ensure
impersonation websites appear in searches related to the target.
For-proﬁt equivalents of impersonation include phishing [38]
and counterfeit online storefronts [46].
In addition to reputation harm and isolation, attackers may
also use impersonation to physically and sexually threaten
targets. In one case, an former intimate partner created dating
proﬁles that impersonated the target to arrange for strangers
to arrive at the target’s house and place of work seeking inti-
mate engagements [66], [69]. A related impersonation attack
includes the synthetic generation of media depicting a target,
such as “deep fakes” or “photoshopping”. A study by Simonite
et al. found that 96% of all deep fakes that they identiﬁed in
the wild were pornographic in nature [129]. We distinguish
this from disclosure of authentic but non-consensual intimate
images (which falls under content leakage).
Surveillance [Conﬁdentiality; C4 exclusively]. Surveillance
involves an attacker leveraging privileged access to a target’s
devices or accounts (C4) to monitor the target’s activities, loca-
tion, or communication. In a for-proﬁt cybercrime ecosystem,
adjacent tools include keyloggers and remote access trojans
that monitor a target’s activities [59], [76]. Attackers can re-
purpose these off-the-shelf tools for hate and harassment, or
alternatively subvert a target’s devices such as their mobile
phones [6], IoT devices [20], and GPS trackers [151] to surveil
the target’s activities. Indeed, Chatterjee et al. found an active
ecosystem of attackers that develop “stalkerware” and who
share techniques on how to subvert applications to monitor a
target without their knowledge [27]. Havron et al. also reported
on experiences of survivors of intimate partner violence who
learned their abusers accessed remote backups (e.g., photos
uploaded to iCloud) after the survivor had physically separated
from the abuser [72]. Abusers may also surveil a target’s
ﬁnances and spending [42]. Threats in this space illustrate the
challenges of practitioners designing secure software without
considering hate and harassment as part of their threat model.
Lockout & Control [Integrity, Availability; A1+¬M1+C4].
Our ﬁnal category of attacks includes scenarios where an
attacker leverages privileged access to a target’s account or
device—including computers, phones, or IoT devices (C4)—
to gaslight
the target or interfere with how they engage
with the world (A1). Such lockout and control excludes the
creation of images or text (not M1); instead, attackers rely
on actively subverting technology services. Passive monitoring
via privileged access is covered instead by surveillance.
Examples of attacks in this category include an abusive
party hijacking a smart home’s microphone to broadcast
profanity [15], or turning up a home’s smart thermostat to
90°F [37]. Outside the IoT space, attacks include deleting
communication with a target to prevent documenting and re-
porting abuse; controlling a target’s access to online resources
for help; or removing a target’s access to their online accounts
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:30:40 UTC from IEEE Xplore.  Restrictions apply. 
252
(including ﬁnancial resources [42])—a common threat in inti-
mate partner violence [27], [108]. Ransomware represents the
closest equivalent in a for-proﬁt cybercrime context [90]. One
survivor of intimate partner violence reported how her abuser
would delete email responses to job applications in order to
restrict the survivor’s ﬁnancial situation [108].
IV. PREVALENCE AND AT-RISK POPULATIONS
To demonstrate the global and growing threat posed by
online hate and harassment, we conducted a three year survey
spanning North and South America, Europe, Asia, Africa, and
the Middle East to understand people’s experiences with online
abuse. Wherever possible, we compare our results to similar
surveys conducted by Pew [118], Data and Society [44], the
Anti-Defamation League [4], and Microsoft’s Digital Civility
Index [110].
A. Survey design
Instrument Design. Our survey asked participants “Have you
ever personally experienced any of the following online?” and
then listed a ﬁxed set of experiences that participants could
select from. We refer readers to the Appendix for our full
survey question. We developed our survey to include ﬁve of
the six experiences used by Pew in 2014 [118] to enable
replication and comparison to their metrics.2 To this end we
also inherited some of their limitations, including asking if this
behavior was experienced (prevalence only) and not measuring
frequency or severity. We did expand the set to include eight
other experiences related to lockout and control, surveillance,
content leakage, impersonation, and a deeper treatment of
toxic content beyond just name calling (as used by earlier
works). However, as our survey precedes the construction of
our ﬁnal taxonomy, we lack a one-to-one mapping between
the attacks outlined in our taxonomy and those appearing in
our survey.
Country Selection. We selected countries for inclusion seek-
ing diversity across a number of features: multiple regions
of the world, measures of development (HDI), cultural and
legal responses to online content, and through conversations
with experts, as well as ability to survey using high-quality
panels within a nation. To maximize the number of countries
included, some countries do not appear in our sample every
year. Table V in the Appendix contains our ﬁnal sample
size per country and the year it was collected, along with
the unweighted demographic breakdown averaged across the
entire survey period.
Survey Deployment. We conducted this survey in coordina-
tion with an industry leading market research ﬁrm, of which
experiences with online abuse was just one segment in the
context of a broader survey of privacy attitudes.3 After com-
pletion of the entire survey instrument in English, the research
2We included a modiﬁed version of their sixth experience, shifting the
item from general embarrassment to embarrassment caused by the posting of
a private photo.
3The abuse experience question is just one of 60 items that was asked.
team worked with in-country translation teams (through our
research vendor partner) to then cover 22 countries. When the
instrument was fully translated, it was then sent to a second
in-country translation team for back-translation into English,
which we reviewed and iterated on, as needed. Two earlier
iterations of the survey were conducted in 2015 to validate
and further reﬁne the instrument, both within and outside of
the US. In consultation with our research vendor partner, and
their in-country teams, we aligned on the demographic traits
that we could safely ask of participants in each country, using
their standard demographic measures and survey items.4
With the exception of the US, all respondents were sourced
directly from high quality, opt-in panels; that is previously
created panels of volunteers willing to participate in surveys
and market research. Across the 22 countries we used a com-
bination of these panels from six different providers, all sub-
contracted through our research vendor partner). Consistent
with the best panels available for online market research,
such panels tend to be broadly representative of the general
population in countries with high access to technology, but
less representative of the general population in countries with
more limited access to technology; for example, in developing
countries they tend to skew urban. Respondents were recruited
using stratiﬁed sampling with ﬁxed quotas on country, age,
and gender in each country. In the United States, we used
a nationally representative panel that represents an accurate
demographic probability-based sample, based on residential
addresses. After data collection was completed, in each year,