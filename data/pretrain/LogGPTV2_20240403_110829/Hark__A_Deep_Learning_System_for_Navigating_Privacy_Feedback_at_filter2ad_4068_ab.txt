4.5M
2.2M
5.1M
4.9M
287M
626M
These choices are intercorrelated as it is easy to achieve high
performance using traditional models on tests sets created with
such sampling methods. One core contribution of Hark is a
new approach for dataset construction that aims to cover two
major privacy taxonomies without being keyword-restricted, by
leveraging the Natural Language Inference (NLI) [30] task. On
such a diverse dataset, the limitations of traditional classiﬁcation
models become apparent, making the case for integrating
state of the art models, such as T5 [40]. For general review
analysis, some studies went beyond classiﬁcation. In [11], the
authors showed state-of the-art results on the task of identifying
software requirements from app reviews. However, that task
is limited since requirements are simply phrases extracted
verbatim from the review and do not repeat elsewhere (e.g.,
“u take my data storage”). Hark formulates issue generation
as an abstractive task where we want to generate recurring,
ﬁne-grained issues (e.g., “Unwanted Storage Access”).
There were other e↵orts targeting ways to summarize review
groups at a high level. Some works explored topic modeling
on app reviews [18, 36], producing common keywords. Other
works proposed cluster centrality metrics to produce represen-
tative reviews [33]. In all of these e↵orts, the outputs require
manual annotation for creating topic/cluster titles (as done
in [18, 33]). Hark performs clustering at the level of issues
extracted from reviews rather than the raw text, resulting in
high-level themes. It also includes a model that automatically
assigns meaningful titles to these themes, thus avoiding manual
intervention.
III. Overview
Figure 1 shows an overview of the Hark system. The
system’s input system is a text dataset of user feedback/reviews.
In the ﬁrst step, Hark’s privacy classiﬁer is used to retain
privacy-related feedback and exclude the rest (Section V).
Next, this feedback is fed into an issue-generation component
(Section VI), which produces a set of ﬁne-grained issues from
each text. This uses an abstractive model that acts like a
summarizer as opposed to an extractive one that simply gets
relevant words from the feedback. It turns feedback such as
“I don’t understand why I should allow you to my cam or
calls” to (multiple) issues: “Unnecessary Camera Access” and
“Unnecessary Calls Access”.
Next, these issues are aggregated across the whole corpus
and are grouped into themes based on their semantic similarity
(Section VII). Each group of issues and the associated feedback
constitute a theme. The most frequent issues in the theme are
used to generate a theme title automatically via an abstractive
theme summarization model. For instance, a theme with the top
issues “Cannot Access Activity Controls”, “Turn O↵ Activity
History”, and “Turn on Activity History” would get the title
“Activity Management”.
By generating this hierarchy of high-level themes and ﬁne-
grained issues, Hark enables developers to navigate the privacy-
related feedback at multiple levels of abstraction. To enrich the
navigation experience, Hark includes an emotion classiﬁcation
model with 28 categories (Section VIII), thus providing a
valuable way for ﬁltering issues and themes by the level
of anger, joy, confusion, etc. Hark further attaches to each
ﬁne-grained issue a set of high quality quotes (Section VIII),
allowing developers to dig deeper into representative feedback
behind the issues of interest. By combining the issues, themes,
emotions, top quotes, and feedback metadata (timestamp, star
rating, etc.), Hark unlocks this user-to-developer channel,
equipping developers with the material to perform a variety of
trend analyses and to track their progress on a variety of metrics.
We provide illustrative examples of these in Section IX.
IV. Modeling Approach
In Hark, we use T5-based models (cf. Section II-B) in our
various generation and classiﬁcation tasks, adding the necessary
optimizations to tailor them to the domain at hand. Figure 2
illustrates our modeling approach for the 5 tasks we described
in Section III. Essentially, we cast each task as a text-to-text
one, and separately ﬁnetune a T5 model for it. We will shed
light on each of these 5 tasks and the training data in the
respective sections.
When adapting T5 models to our tasks, the text is initially
tokenized (i.e., broken into tokens) using the T5 SentencePiece
tokenizer that breaks each review into a sequence of subwords,
thus minimizing the e↵ect of out-of vocabulary words [25].
We then ﬁnetune the T5 models using the maximum likelihood
training objective, with teacher forcing [54].
At inference time, i.e., when we want to run the model
on new data, the text is decoded one token at a time. In a
“greedy” decoding setup, the token with the maximum log-
likelihood (referred to as logit) is selected at each step. In the
special case of classiﬁcation tasks, we are also interested in
the scores of the various classes. We compute these scores
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:35:48 UTC from IEEE Xplore.  Restrictions apply. 
2471
Fig. 1: Overview of Hark’s main pipeline components and expected outputs.
(cid:19)(cid:44)(cid:35)(cid:3)(cid:17)(cid:45)(cid:34)(cid:35)(cid:42)(cid:3)(cid:20)(cid:35)(cid:48)(cid:3)(cid:24)(cid:31)(cid:49)(cid:41)
(cid:20)(cid:48)(cid:39)(cid:52)(cid:31)(cid:33)(cid:55)(cid:3)(cid:7)(cid:42)(cid:31)(cid:49)(cid:49)(cid:39)(cid:326)(cid:35)(cid:48)
(cid:48)(cid:35)(cid:52)(cid:39)(cid:35)(cid:53)(cid:49)(cid:3)(cid:46)(cid:48)(cid:39)(cid:52)(cid:31)(cid:33)(cid:55)(cid:3)(cid:33)(cid:42)(cid:31)(cid:49)(cid:49)(cid:39)(cid:326)(cid:35)(cid:48)(cid:384)(cid:3)(cid:13)(cid:3)(cid:34)(cid:45)(cid:44)(cid:408)(cid:50)(cid:3)(cid:41)(cid:44)(cid:45)(cid:53)(cid:3)(cid:38)(cid:45)(cid:53)(cid:3)(cid:50)(cid:45)(cid:3)(cid:34)(cid:35)(cid:42)(cid:35)(cid:50)(cid:35)(cid:3)(cid:43)(cid:55)(cid:3)(cid:31)(cid:33)(cid:33)(cid:45)(cid:51)(cid:44)(cid:50)(cid:382)
(cid:13)(cid:49)(cid:49)(cid:51)(cid:35)(cid:3)(cid:11)(cid:35)(cid:44)(cid:35)(cid:48)(cid:31)(cid:50)(cid:45)(cid:48)
(cid:37)(cid:35)(cid:44)(cid:35)(cid:48)(cid:31)(cid:50)(cid:35)(cid:3)(cid:39)(cid:49)(cid:49)(cid:51)(cid:35)(cid:384)(cid:3)(cid:13)(cid:3)(cid:34)(cid:45)(cid:44)(cid:408)(cid:50)(cid:3)(cid:41)(cid:44)(cid:45)(cid:53)(cid:3)(cid:38)(cid:45)(cid:53)(cid:3)(cid:50)(cid:45)(cid:3)(cid:34)(cid:35)(cid:42)(cid:35)(cid:50)(cid:35)(cid:3)(cid:43)(cid:55)(cid:3)(cid:31)(cid:33)(cid:33)(cid:45)(cid:51)(cid:44)(cid:50)(cid:382)
(cid:24)(cid:38)(cid:35)(cid:43)(cid:35)(cid:3)(cid:24)(cid:39)(cid:50)(cid:42)(cid:35)(cid:3)(cid:11)(cid:35)(cid:44)(cid:35)(cid:48)(cid:31)(cid:50)(cid:45)(cid:48)
(cid:37)(cid:35)(cid:44)(cid:35)(cid:48)(cid:31)(cid:50)(cid:35)(cid:3)(cid:50)(cid:38)(cid:35)(cid:43)(cid:35)(cid:3)(cid:50)(cid:39)(cid:50)(cid:42)(cid:35)(cid:384)(cid:3)(cid:5)(cid:33)(cid:33)(cid:45)(cid:51)(cid:44)(cid:50)(cid:3)(cid:8)(cid:35)(cid:42)(cid:35)(cid:50)(cid:39)(cid:45)(cid:44)(cid:383)(cid:3)(cid:5)(cid:33)(cid:33)(cid:45)(cid:51)(cid:44)(cid:50)(cid:3)(cid:8)(cid:35)(cid:31)(cid:33)(cid:50)(cid:39)(cid:52)(cid:31)(cid:50)(cid:39)(cid:45)(cid:44)(cid:382)
(cid:9)(cid:43)(cid:45)(cid:50)(cid:39)(cid:45)(cid:44)(cid:3)(cid:7)(cid:42)(cid:31)(cid:49)(cid:49)(cid:39)(cid:326)(cid:35)(cid:48)
(cid:35)(cid:43)(cid:45)(cid:50)(cid:39)(cid:45)(cid:44)(cid:3)(cid:33)(cid:42)(cid:31)(cid:49)(cid:49)(cid:39)(cid:326)(cid:35)(cid:48)(cid:384)(cid:3)(cid:13)(cid:3)(cid:34)(cid:45)(cid:44)(cid:408)(cid:50)(cid:3)(cid:41)(cid:44)(cid:45)(cid:53)(cid:3)(cid:38)(cid:45)(cid:53)(cid:3)(cid:50)(cid:45)(cid:3)(cid:34)(cid:35)(cid:42)(cid:35)(cid:50)(cid:35)(cid:3)(cid:43)(cid:55)(cid:3)(cid:31)(cid:33)(cid:33)(cid:45)(cid:51)(cid:44)(cid:50)(cid:382)
(cid:21)(cid:51)(cid:31)(cid:42)(cid:39)(cid:50)(cid:55)(cid:3)(cid:7)(cid:42)(cid:31)(cid:49)(cid:49)(cid:39)(cid:326)(cid:35)(cid:48)
(cid:47)(cid:51)(cid:31)(cid:42)(cid:39)(cid:50)(cid:55)(cid:3)(cid:33)(cid:42)(cid:31)(cid:49)(cid:49)(cid:39)(cid:326)(cid:35)(cid:48)(cid:384)(cid:3)(cid:13)(cid:3)(cid:34)(cid:45)(cid:44)(cid:408)(cid:50)(cid:3)(cid:41)(cid:44)(cid:45)(cid:53)(cid:3)(cid:38)(cid:45)(cid:53)(cid:3)(cid:50)(cid:45)(cid:3)(cid:34)(cid:35)(cid:42)(cid:35)(cid:50)(cid:35)(cid:3)(cid:43)(cid:55)(cid:3)(cid:31)(cid:33)(cid:33)(cid:45)(cid:51)(cid:44)(cid:50)(cid:382)
Fig. 2: Text-to-text formulation of the various models introduced by Hark.
(cid:46)(cid:48)(cid:39)(cid:52)(cid:31)(cid:33)(cid:55)
(cid:5)(cid:33)(cid:33)(cid:45)(cid:51)(cid:44)(cid:50)(cid:3)(cid:8)(cid:35)(cid:42)(cid:35)(cid:50)(cid:39)(cid:45)(cid:44)
(cid:5)(cid:33)(cid:33)(cid:45)(cid:51)(cid:44)(cid:50)(cid:3)(cid:17)(cid:31)(cid:44)(cid:31)(cid:37)(cid:35)(cid:43)(cid:35)(cid:44)(cid:50)
(cid:33)(cid:45)(cid:44)(cid:36)(cid:51)(cid:49)(cid:39)(cid:45)(cid:44)
(cid:42)(cid:45)(cid:53)
by feeding the input text to the model encoder and each of
the target classes’ tokens to the model decoder. Given the
logits of these classes, we apply a Softmax function to obtain
a set of normalized scores that sum up to 1. This method
for approximating the classiﬁcation probabilities in text-to-text
models has been shown to be e↵ective by Nogueira et al. [35].
V. Privacy Feedback Classifier
We now describe the ﬁrst stage of the Hark pipeline, namely
the privacy feedback classiﬁer, which distinguishes reviews
related to privacy from those which are not.
A. Hark Reviews Corpus
During August 2021, we collected a large corpus of app
reviews from Google’s Play store, which we use in the rest
of this paper. For each review, we collected its content,
the submission time, its star rating, the package name of
the corresponding app, and the app’s Play store category
information. We limit our corpus to English-only reviews
as identiﬁed by the CLD3 language identiﬁcation library
(github.com/google/cld3). Our review dataset contains a total
of 626M reviews from 1.3M apps published across all of the
Play store app categories.
Ethical considerations: App reviews are already public,
and users who submit reviews are aware of this. Nevertheless,
we took several steps to ensure user privacy and avoid user
identiﬁcation. First, no user information is stored during the
reviews gathering process. Second, we only included apps that
had at least 10K installs and at least 1000 reviews. Third, we
will not release the raw reviews data.
B. Creating Training Data
A core challenge we faced in constructing the training data
for this classiﬁer is that only a small subset of app reviews relate
to privacy. Mukherjee et al. [32] have estimated privacy reviews
to be around 0.5% of all reviews while Nguyen et al. [34]
estimated both security and privacy reviews to constitute 0.12%
of all reviews. Regardless of the methodologies employed (we
address their limitations below) and the accuracy of these
estimates, this order of magnitude indicates that uniformly
sampling reviews and labeling them as privacy vs. not-privacy
is highly ine cient and would consume tremendous labeling
resources.
1) Creating NLI-Annotated Corpus for Manual Labeling
We need to extract a seed corpus with a signiﬁcant presence
of privacy-related reviews from the full corpus. This would
allow us to sample candidate data that undergoes manual
labeling before using it to train the privacy classiﬁer.
Similar needs have arised in previous works targeting review
analysis, in the context of security or privacy reviews [32, 34,
45]. The common approach these works followed was to search
the full corpus using a limited set of seed keywords compiled
for the target domain (e.g., privacy, permissions, personal info,
etc.). Then the resulting data is annotated to train a machine
learning model. This approach has clear limitations in terms of
the topical diversity of privacy issues. Essentially, the domain
of collected texts will be limited to the well-known privacy
issues that the keywords represent. The model trained on the
annotated version of these texts is also highly prone to overﬁt
on the presence of these keywords (or their absence). Hence, a
model can appear to have a high performance on such datasets
while su↵ering when tested in the wild. In this work, we take
a more principled approach at constructing this seed corpus,
which is designed to ensure a high diversity of the various
privacy topics, without being keyword-driven.
In order to achieve such diversity, we rely on two commonly
used and complementary taxonomies developed for privacy:
the taxonomy of privacy violations by Solove [43] and the
taxonomy of privacy enhancing technologies by Wang and
Kobsa [50]. We extracted most of the concepts from these
taxonomies, excluding those outside the scope of this work,
such as “security”. The full list of these concepts is in Table II.
Now that we have a set of high-level concepts that cover a
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:35:48 UTC from IEEE Xplore.  Restrictions apply. 
2472
(cid:3)(cid:24)(cid:38)(cid:35)(cid:43)(cid:35)(cid:3)(cid:420)(cid:384)(cid:3)(cid:25)(cid:44)(cid:44)(cid:35)(cid:35)(cid:34)(cid:35)(cid:34)(cid:3)(cid:5)(cid:33)(cid:33)(cid:35)(cid:49)(cid:49)(cid:3)(cid:399)(cid:424)(cid:423)(cid:425)(cid:15)(cid:400)(cid:404)(cid:13)(cid:49)(cid:49)(cid:51)(cid:35)(cid:3)(cid:420)(cid:384)(cid:3)(cid:25)(cid:44)(cid:44)(cid:35)(cid:35)(cid:34)(cid:35)(cid:34)(cid:3)(cid:16)(cid:45)(cid:33)(cid:31)(cid:50)(cid:39)(cid:45)(cid:44)(cid:3)(cid:5)(cid:33)(cid:33)(cid:35)(cid:49)(cid:49)(cid:3)(cid:399)(cid:427)(cid:428)(cid:15)(cid:400)(cid:380)(cid:24)(cid:45)(cid:46)(cid:3)(cid:22)(cid:35)(cid:52)(cid:39)(cid:35)(cid:53)(cid:3)(cid:420)(cid:384)(cid:3)(cid:390)(cid:3)(cid:399)(cid:9)(cid:43)(cid:45)(cid:50)(cid:39)(cid:45)(cid:44)(cid:384)(cid:3)(cid:5)(cid:44)(cid:37)(cid:35)(cid:48)(cid:400)(cid:380)(cid:24)(cid:45)(cid:46)(cid:3)(cid:22)(cid:35)(cid:52)(cid:39)(cid:35)(cid:53)(cid:3)(cid:421)(cid:384)(cid:3)(cid:390)(cid:3)(cid:399)(cid:9)(cid:43)(cid:45)(cid:50)(cid:39)(cid:45)(cid:44)(cid:384)(cid:3)(cid:10)(cid:35)(cid:31)(cid:48)(cid:400)(cid:380)(cid:390)(cid:3)(cid:404)(cid:13)(cid:49)(cid:49)(cid:51)(cid:35)(cid:3)(cid:421)(cid:384)(cid:3)(cid:25)(cid:44)(cid:44)(cid:35)(cid:35)(cid:34)(cid:35)(cid:34)(cid:3)(cid:7)(cid:45)(cid:44)(cid:50)(cid:31)(cid:33)(cid:50)(cid:49)(cid:3)(cid:5)(cid:33)(cid:33)(cid:35)(cid:49)(cid:49)(cid:3)(cid:399)(cid:427)(cid:419)(cid:15)(cid:400)(cid:380)(cid:24)(cid:45)(cid:46)(cid:3)(cid:22)(cid:35)(cid:52)(cid:39)(cid:35)(cid:53)(cid:3)(cid:420)(cid:384)(cid:3)(cid:390)(cid:3)(cid:399)(cid:9)(cid:43)(cid:45)(cid:50)(cid:39)(cid:45)(cid:44)(cid:384)(cid:3)(cid:7)(cid:45)(cid:44)(cid:36)(cid:51)(cid:49)(cid:39)(cid:45)(cid:44)(cid:400)(cid:380)(cid:380)(cid:390)(cid:404)(cid:390)(cid:3)(cid:13)(cid:49)(cid:49)(cid:51)(cid:35)(cid:3)(cid:11)(cid:35)(cid:44)(cid:35)(cid:48)(cid:31)(cid:50)(cid:39)(cid:45)(cid:44)(cid:13)(cid:49)(cid:49)(cid:51)(cid:35)(cid:49)(cid:9)(cid:43)(cid:45)(cid:50)(cid:39)(cid:45)(cid:44)(cid:7)(cid:42)(cid:31)(cid:49)(cid:49)(cid:39)(cid:326)(cid:35)(cid:48)(cid:10)(cid:35)(cid:35)(cid:34)(cid:32)(cid:31)(cid:33)(cid:41)(cid:3)(cid:21)(cid:51)(cid:31)(cid:42)(cid:39)(cid:50)(cid:55)(cid:3)(cid:17)(cid:45)(cid:34)(cid:35)(cid:42)(cid:24)(cid:38)(cid:35)(cid:43)(cid:35)(cid:3)(cid:7)(cid:48)(cid:35)(cid:31)(cid:50)(cid:39)(cid:45)(cid:44)(cid:20)(cid:48)(cid:39)(cid:52)(cid:31)(cid:33)(cid:55)(cid:3)(cid:10)(cid:35)(cid:35)(cid:34)(cid:32)(cid:31)(cid:33)(cid:41)(cid:3)(cid:7)(cid:42)(cid:31)(cid:49)(cid:49)(cid:39)(cid:326)(cid:35)(cid:48)(cid:22)(cid:35)(cid:52)(cid:39)(cid:35)(cid:53)(cid:49)(cid:3)(cid:7)(cid:45)(cid:48)(cid:46)(cid:51)(cid:49)(cid:3)(cid:20)(cid:48)(cid:39)(cid:52)(cid:31)(cid:33)(cid:55)(cid:3)(cid:10)(cid:35)(cid:35)(cid:34)(cid:32)(cid:31)(cid:33)(cid:41)(cid:24)(cid:48)(cid:35)(cid:44)(cid:34)(cid:3)(cid:5)(cid:44)(cid:31)(cid:42)(cid:55)(cid:49)(cid:39)(cid:49)Fig. 3: High level overview of the privacy classiﬁer construction stages.
wide range of issues in the privacy domain, we want to identify
sample reviews that discuss each topic. Our approach leverages
the task of Natural Language Inference (NLI), which is the
problem of deciding whether a natural language hypothesis
can reasonably be inferred from a given premise [30]. An
NLI model has to determine whether a hypothesis is true
(i.e. entailment), false (i.e., contradiction), or undetermined (i.e.,
neutral) given a premise. For example, take a premise saying
“This app does not o↵er any visibility controls to hide your
information.” A hypothesis that says “app data is publicly
accessible” would receive an entailment label. A hypothesis
that says “app data is kept private” would receive a contradiction
label. A hypothesis that says “app has a good interface” would
receive a neutral label.
Our idea is to leverage NLI models in order to ﬁnd reviews
discussing certain privacy concepts. The premises in our context
would be the app reviews. The hypotheses would be manually
constructed based on the privacy concepts we selected earlier.
For each concept, we came up with one or more hypotheses.
For example, for the “blackmailing” concept, we created the
hypothesis “A data blackmailing issue is discussed.” We also
included 7 additional hypotheses covering generic mentions
of privacy issues or positive privacy features. In total, we
ended up with 35 hypotheses (see Table II). We chose a model
trained on MultiNLI, which is a multi-genre dataset of 433K
sentence pairs covering a variety of domains [53]. This helps
handling the general breadth of topics raised in app reviews. We
use the publicly-available Vanilla T5-11B model checkpoint,
which is readily ﬁnetuned on the MultiNLI dataset (as part
of the GLUE mixture of tasks [47]). We run the NLI model
on a dataset of 9M reviews, randomly sampled from the full
dataset of 626M reviews. With 35 hypotheses, this amounts
to a total of 35⇥9M=315M inference operations. We refer to
these 9M reviews and the entailment probabilities assigned per
hypothesis as the NLI-Annotated Corpus. One major advantage
of this method is that it eliminates the reliance on keywords.
The premises corresponding to the hypotheses can have a high
linguistic variability. For instance, both of the following reviews
receive an entailment label for the hypothesis “Personal data
disclosure is discussed.”:
• “this game will NOT open unless you agree to them sharing
• “and doesn’t ask for access to unneeded personal data per-
missions. Well done developers 5Stars” (P(entailment)=0.75)
Notice that the ﬁrst review has no words in common with
the hypothesis. Neither review mentions disclosure, and one
of them explains a problem while the other has a positive
sentiment.
your information to advertisers” (P(entailment)=0.89)
TABLE II: Privacy Concepts and Associated Hypotheses
Privacy Concept
Hypotheses
Concepts from Solove’s Taxonomy
Surveillance
Interrogation
Aggregation
Insecurity
Identiﬁcation
Secondary Use
Exclusion
Breach of Conﬁdentiality
Disclosure
Exposure
Increased Accessibility
Blackmail
Appropriation
Distortion
Intrusion
Decisional Interference
the purposes of
The user is facing a data surveillance issue.
The user is forced to provide information.
Personal user information is collected from other
sources.
The user is concerned about protecting their per-
sonal data.
A data anonymity topic is discussed.
The user is concerned about
personal data access.
The user wants to correct their personal informa-
tion.
A breach of data conﬁdentiality is discussed.
Personal data disclosure is discussed.
The app exposes a private aspect of the user life.
User’s data has been made accessible to public.
A data blackmailing issue is discussed.
User data is being exploited for other purposes.
False data is presented about the user.
Unwanted intrusion to personal info is discussed.
Intrusion by the government to the user’s life is
discussed.
Concepts from Wang and Kobsa’s Taxonomy
Notice/Awareness
Data Minimization
Purpose Speciﬁcation
Collection Limitation
Use Limitation
Onward Transfer
Choice/Consent
Opting out from personal data collection is dis-
cussed.
More access than needed is required.
The reason for data access is not provided.
Too much personal data is collected.
The data is being used for unexpected purposes.
Data sharing with third parties is discussed.
User choice for personal data collection is dis-
cussed.
User did not allow access to their personal data.
Generic Privacy Concepts
Generic Privacy Issues
Positive Privacy Issues
A data privacy topic is discussed.
Protecting user’s personal data is discussed.
This is about a privacy feature.
The user is facing a privacy issue.
The user likes that data privacy is provided.
The user wants privacy.
The app has privacy features.
2) Creating Manually Labeled Training Data
We use the the NLI-Annotated Corpus to sample diverse
data for manual labeling. Given the 9M reviews, let NE(i, t) be
the number of hypotheses receiving an entailment score above
a threshold t for review i. We apply the following heuristics:
• We designate a review i as maybe-not-privacy if NE(i, 0.4) = 0.
• We designate a review as maybe-privacy if NE(i, 0.8) >= 1
or NE(i, 0.7) >= 3 or NE(i, 0.6) >= 5 or NE(i, 0.5) >= 7.
The intuition is that the more hypotheses a review satisﬁes,
the more likely it is to be within the privacy domain. The rest
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:35:48 UTC from IEEE Xplore.  Restrictions apply. 
2473
(cid:12)(cid:419)(cid:383)(cid:12)(cid:420)(cid:383)(cid:3)(cid:390)(cid:383)(cid:3)(cid:12)(cid:422)(cid:425)(cid:18)(cid:16)(cid:13)(cid:3)(cid:17)(cid:45)(cid:34)(cid:35)(cid:42)(cid:20)(cid:48)(cid:39)(cid:52)(cid:31)(cid:33)(cid:55)(cid:3)(cid:12)(cid:55)(cid:46)(cid:45)(cid:50)(cid:38)(cid:35)(cid:49)(cid:35)(cid:49)(cid:384)(cid:399)(cid:12)(cid:35)(cid:51)(cid:48)(cid:39)(cid:49)(cid:50)(cid:39)(cid:33)(cid:49)(cid:3)(cid:330)(cid:3)(cid:23)(cid:31)(cid:43)(cid:46)(cid:42)(cid:39)(cid:44)(cid:37)(cid:13)(cid:43)(cid:32)(cid:31)(cid:42)(cid:31)(cid:44)(cid:33)(cid:35)(cid:34)(cid:3)(cid:8)(cid:31)(cid:50)(cid:31)(cid:399)(cid:12)(cid:51)(cid:43)(cid:31)(cid:44)(cid:3)(cid:5)(cid:44)(cid:44)(cid:45)(cid:50)(cid:31)(cid:50)(cid:39)(cid:45)(cid:44)(cid:399)(cid:5)(cid:42)(cid:43)(cid:45)(cid:49)(cid:50)(cid:3)(cid:6)(cid:31)(cid:42)(cid:31)(cid:44)(cid:33)(cid:35)(cid:34)(cid:3)(cid:8)(cid:31)(cid:50)(cid:31)(cid:20)(cid:48)(cid:39)(cid:52)(cid:31)(cid:33)(cid:55)(cid:3)(cid:7)(cid:42)(cid:31)(cid:49)(cid:49)(cid:39)(cid:326)(cid:35)(cid:48)(cid:18)(cid:45)(cid:39)(cid:49)(cid:55)(cid:3)(cid:16)(cid:31)(cid:32)(cid:35)(cid:42)(cid:39)(cid:44)(cid:37)(cid:3)(cid:7)(cid:31)(cid:44)(cid:34)(cid:39)(cid:34)(cid:31)(cid:50)(cid:35)(cid:49)(cid:437)(cid:399)(cid:18)(cid:16)(cid:13)(cid:3)(cid:5)(cid:44)(cid:44)(cid:45)(cid:50)(cid:31)(cid:50)(cid:35)(cid:34)(cid:3)(cid:7)(cid:45)(cid:48)(cid:46)(cid:51)(cid:49)(cid:12)(cid:421)(cid:422)(cid:12)(cid:424)of reviews that satisfy neither of these heuristics are considered
as undetermined and are not used further. This is in order to
leave a safe margin between these heuristics.
Notice that our few hypotheses per concept are not meant to
completely cover the underlying concepts. They are designed to
produce a diverse sample of candidate data for manual labeling.
Since we sample data from both true and false matches on the
hypotheses, we also capture some parts of the concepts not
readily included in our hypotheses.
From the reviews annotated by the heuristics, we randomly
sampled 3,254 reviews, ensuring nearly equal representation
across: (1) maybe-privacy vs. maybe-not-privacy labels, (2) four
di↵erent review world length buckets, and (3) app categories.
We get these sampled reviews manually annotated to create a
high quality privacy training dataset.
In order to mitigate the e↵ect of individual perceptions of
what constitute privacy [52], we created labeling instructions
(available at github.com/google/hark). that explained the task,
and provided deﬁnitions for privacy and not-privacy labels. We
ensured to clarify some tricky cases (e.g. around security,
scam, spam, etc.) by o↵ering several examples. We recruited
annotators from our company’s internal crowdsourcing platform
that contracts with third-party vendors to source thousands of
annotators across the world for labeling the reviews as privacy
or not-privacy. Our annotator pool is composed of college-
educated individuals with a nearly balanced gender distribution
and more younger population (⇠50% are in the age range of
25-34, with less than 5% above 55 years). These annotators are
paid per hour based on local market conditions at a rate set by
their employer. Each review was then labeled by 5 annotators,
and a total of 1,332 annotators labeled the 3,254 reviews.
Krippendor↵’s alpha [24] for inter-annotator agreement was
0.455. While this agreement value might seem low, it is
within an acceptable range for cases using crowdsourcing for
evaluating latent constructs (privacy in our case) [28]. Of the
3,254 reviews manually annotated, 99.4% of maybe-not-privacy
were labeled as not-privacy and 64.3% of maybe-privacy were
labeled as privacy by the annotators. This indicates that the NLI
approach results in almost no false negatives but contributes
some false positives. Hence, it is necessary to couple it with a
manual annotation step to generate high quality training data.
In Appendix B, we break down the data distribution across the
various privacy concepts we sampled from.
C. Model Training
From the 3,254 labeled examples, we extracted a balanced
test set of 300 examples (split equally between the privacy
and not-privacy labels). From the remaining data, we take 200
items (82 of them are privacy) as the validation set and the
remaining 2,754 reviews (1030 of them are privacy) as the
training set. Next, we trained a T5-11B model on this training
data (parameters in Appendix A). In Figure 3, we summarize
the various steps we described for building Hark’s privacy
feedback classiﬁer.
Fig. 4: ROC curves for the di↵erent privacy feedback classiﬁers.
D. Classiﬁer Performance
We use the 300 examples test set to compare the performance
of our privacy classiﬁer with four baseline classiﬁers. These
classiﬁers are varied across the dataset and the model architec-
ture dimensions. In addition to our training data, referred to
as Hark Data, we consider the dataset by Nema et al. [33] at
ICSE 2022 (referred to as ICSE Data). That dataset is built
based on regex patterns developed to cover a privacy taxonomy.
Hence, we compare our model (T5-11B Hark Data) to:
• T5-11B - ICSE Data: T5-11B model trained on ICSE Data.
• SVM - Hark Data: SVM Classifer based on bag of words
(using 3-5 character n-grams), reproducing the one used in
Nguyen et al. [34].
• RoBERTa-Large - Hark Data, a 24-layer deep learning model,
achieving strong results on various classiﬁcation tasks [29].
• RoBERTa-Large - ICSE Data: variant trained on ICSE Data.
Figure 4 shows the Receiver Operating Characteristic (ROC)
curves and the corresponding AUC-ROC values for our model
and the four baselines. We observe that the T5-11B model
trained on Hark Data obtained 0.17 higher AUC compared
to the same model trained on ICSE Data (0.92 vs 0.75). We
also independently tested T5-11B - Hark Data on the ICSE
test set and found that it matches the best reported ensemble
model performance (AUC=0.98) by Nema et al. [33]. This
illustrates that Hark’s method leveraging NLI for training set
sampling enables generalization to other test sets while regex-
based sampling of training data fails in that regard. Another
observation we see is that models such the SVM model used by
Nguyen et al. [34] fail to learn the nuances of our syntactically
and semantically diverse dataset (AUC=0.73 on Hark’s test set),
despite getting a reported AUC-ROC of 0.93 on a keyword-
sampled test set in [34]. Using RoBERTa-Large with Hark
Data improves the AUC by 0.13, and using the T5-11B results
in 0.19 absolute increase in the AUC. This shows the power
of using larger models that beneﬁt from transfer learning.
In
Appendix E, we provide qualitative examples, illustrating our
classiﬁer’s superior performance vs. the baselines.
VI. Issue Generation
Having developed the privacy classiﬁer module, which allows
us to extract privacy-related reviews, we now describe Hark’s
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:35:48 UTC from IEEE Xplore.  Restrictions apply. 
2474
issue generation model which aims to surface the ﬁne-grained
topics that users discuss.
A. Problem Formulation
Given a user review, the goal is to generate one or more
issues summarizing the main topics that the user is discussing.
We use the term issue in the generic sense (i.e., it can denote
both negative and positive experiences).
One approach to generate these issues is to enumerate all
the possible topics users might discuss (e.g., “Unnecessary
Permissions”, “Data Deletion”, etc.), construct training exam-
ples for each of them, and build a classiﬁcation model to
tag new examples with these labels. This approach has two
main limitations. First, creating training examples for each
label requires a signiﬁcant e↵ort. That is why previous works
on reviews’ analysis have used limited taxonomies (e.g., 12
ﬁne-grained classes were used by Ciurumelea et al. [10]). To
cover all possible issues, these classes tend to be too broad.
Second, the topics mentioned in the reviews evolve over time (a
phenomenon called concept drift [51]). Hence, a classiﬁcation
approach falls short in detecting the emerging issues.
Another approach is to extract important words in the reviews
and rely on these words conveying the issues [11, 22]. However,
that would result in a set of dispersed, out-of-context quotes