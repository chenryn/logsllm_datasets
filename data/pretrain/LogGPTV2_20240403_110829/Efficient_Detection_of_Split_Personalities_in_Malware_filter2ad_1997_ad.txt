• Random numbers: Correctly replaying applica-
tions that rely on random numbers to take non-
deterministic decisions would require to replay the
random number generator of the reference system
on the analysis system. While the random number
generator of Windows is implemented in user mode
(and, thus, is not readily accessible from our kernel
driver), its entropy sources are located in the ker-
nel [17]. In the current implementation, we already
log and replay most of the sources, with the excep-
tion of the KSecDD device driver. By also logging
and replaying the data generated by this driver, we
would be able to replay random numbers.
• Inter-process communication and asynchronous
calls: Our prototype does not replay any local pro-
cedure call (LPC) communication. All the related
system calls are forwarded to the operating system,
with the risk of causing a deviation in the applica-
tion execution.
• Complex memory map scenario: As we explained in
the previous section, our support for memory map-
ping is not complete and does not support, for ex-
ample, the use of mapped area as a shared space
between different processes.
Some of the previous limitations may seem quite se-
In fact, it is possible for an attacker to exploit
vere.
our limitations to prevent our log/replay infrastructure
to work properly. However, these attacks would only
cause our system to detect a deviation and raise an alert.
As a consequence, we would simply incur the perfor-
mance penalty of re-running the sample with a more ﬁne-
grained and expensive analyzer, such as Ether.
It is also possible to evade our system when a mal-
ware program delays the checks that aim to detect the
presence of the analysis environment. This is similar to
postponing malicious behavior for some time (e.g., some
minutes) so that the analysis system will stop monitoring
the process before the sample starts any malicious activ-
ity. Unfortunately, there is not a simple solution for this
problem, with the exception of running the analysis for a
longer time.
In addition, it might be possible to evade our ﬂexible
matching algorithm by dividing the malicious activities
into a number of very short (and far apart) sequences of
system calls, so that the localized differences would ﬂy
under the radar of our detection mechanism. Even though
this is possible in theory, we did not investigate how dif-
ﬁcult it would be to implement in practice a (malware)
program in this way.
Finally, another limitation of the current implementa-
tion is that malware that gains access to kernel structures
(i.e., rootkits) could detect our driver component and take
some countermeasures to avoid detection. However, we
do not believe that this is a signiﬁcant problem. First,
a previous study of current malware trends showed that
only 3.34% of samples install kernel drivers [7]. Second,
it is likely that such malware would perform environment
checks (and, thus, would be detected by our tool) before
attempting to gain control of the kernel, otherwise, the
Sample
SystemTime
Registry
Network
FileSize
FileRead
Four Threads
Syscall
Log Size
Replayed
(VMware & Qemu)
123
195
512
128
114
252
100%
100%
100%
100%
100%
100%
Table 1. Simple Log and Replay test.
malware author would expose some of the program’s ma-
licious functionality to the analysis system. Finally, we
could resort to transparent solutions to implement our log
and replay system. In fact, we believe we could have used
Ether, running in the fast coarse-grained mode, for this.
However, we have not explored this possibility because
Ether was not available when we started our project.
5 Evaluation
We evaluated our system by conducting four different
experiments involving a number of real-world programs.
All tests have been run on Microsoft Windows XP Ser-
vice Pack 3, installed in a VMware virtual machine and
on the Anubis system (i.e., on a Qemu image). Both sys-
tems were installed from the same CD-ROM and were
updated to the same patch level.
In our experiments, we use interchangeably Anubis or
VMware as the reference system (and the other, as the
analysis system). Since our tool is designed to detect dif-
ferences in the behavior of an application when it is run in
two different environments (any two environments), it is
not necessary that one is always selected to be the refer-
ence system, nor that the reference system be a physical
machine. Of course, if a malware attacks both systems
(i.e., it is capable of detecting both Qemu and VMware),
we may fail to detect it. To avoid this problem, it is pos-
sible to use a real machine as the reference system, and
reset the computer state using hardware devices, e.g., a
hard disk write-cache card such as coreRestore [2]. How-
ever, for our experiments, using a virtual machine or em-
ulator as a reference system made it easier to rollback to
a clean state after each test. Finally, both Anubis (Qemu)
and VMware are actively targeted by real-world malware
with split personality.
Experiment I. For the ﬁrst experiment, we evaluated
the ability of our system to correctly log and replay all
the input channels we identiﬁed in Section 3.
For these experiments, we use VMware as the refer-
ence system and Anubis as the analysis system. The test
suite included six simple programs we developed to read
and write data from different channels, i.e., the ﬁle sys-
tem, the registry, the system time, and the network. We
also combined different input/output operations together
in a multi-thread application. Table 1 shows the num-
ber of system calls that were logged and the fraction of
those that were correctly replayed (or forwarded to the
operating system if no replay was required for that par-
ticular call) during the replay phase. A value of 100%
means that the system was able to reach the end of the
log without detecting any deviation from the logged be-
havior, thus demonstrating the ability of our system to
properly log and replay that input channel.
Experiment II.
In our second experiment, we want to
show that our tool can detect a variety of different, real-
world detection attacks (which we manually inserted into
a real-world malware).
We obtained the source code of SDBot, a well-known
botnet application. We ﬁrst compiled and tested in our
system a vanilla version of this bot, which did not include
any check to detect if it is running in a virtual machine. In
Anubis (which is used as the reference system in this ex-
periment), the application generated a log ﬁle containing
1,905 system calls. Upon inspecting this trace, we found
that the 1,524th system call is responsible for spawning
a new process. Since the creation of a new process is not
currently supported by our tool, we consider two execu-
tions of SDBot equivalent if their behavior is the same up
to the NtCreateProcess call. For this experiment,
we designated VMware as the analysis system.
We then created 6 different versions of the SDbot pro-
gram, each time adding to the code a different test to de-
tect VMware. We extracted these techniques from sev-
eral publicly available tools such as Red Pill, Scoopy,
VMDetect, and SourPill. Whenever one of these tools
combined together different approaches, we separated
them in different programs to test them separately.
We ran each sample in log mode under Anubis and
replayed it under VMware. As shown in Table 2, all the
samples containing a VMware detection attack were cor-
rectly detected by our tool.
It can be seen that under
VMware, each sample executed only 460 system calls.
This is because the application terminated immediately
after detecting the presence of the virtual machine. The
behavior under Anubis and VMware is clearly different,
and, therefore, it was ﬂagged by our tool.
Experiment III.
In this experiment, we show that our
tool can correctly log and replay real-world malware that
does not try to detect if it is running in an analysis system.
The test suite used in the third experiment consisted
of 10 malicious samples randomly chosen from the Anu-
bis database among the malware known not to contain
any virtual machine detection techniques. We ﬁrst ran
Sample
SDBot + CPUID Time Check
SDBot + GDT Check
SDBot + LDTR Check
SDBot + RedPill Check
SDBot + VmWare Port Check
SDBot + STR Check
Syscall Log Size
Result
460
460
460
460
460
460
Detection (deviation)
Detection (deviation)
Detection (deviation)
Detection (deviation)
Detection (deviation)
Detection (deviation)
Table 2. Experimenting with SDBot and different VM detection techniques.
the samples in VMware (reference environment), log-
ging their system calls. Then, we executed the samples in
Anubis, once without replaying any inputs, and once in
replay mode. The log collected during the execution of
the samples contained between 1,136 and 30,066 system
calls.
Table 3 reports the results of the experiments. The
table contains three columns. The ﬁrst column reports
the name that the Kaspersky anti-virus scanner associates
with the malicious sample. The second column shows
the result of the experiments with the system call replay
disabled, i.e., with the system conﬁgured to pass all the
system calls to the operating system.
In this case, the
kernel module was not replaying any input data to the ap-
plication. The third column shows the results when the
system call replay was enabled. In the table, a result of
OK indicates that the ﬂexible matching algorithm found
the behaviors to be identical. This means that the behav-
ior on the analysis system matched the behavior on the
reference system until the system call log was exhausted
(or an unsupported functionality was encountered).
It can be seen that, even with system call replay dis-
abled, the behaviors were the same in ﬁve cases. The
main reason for this is that the two Windows installations
are identical. However, in the remaining ﬁve cases, with
system call replay disabled, our tool detected a deviation
in the program behavior. When the replay was enabled,
as shown in the last column of the table, the matching
succeeded for all ten samples. This result demonstrates
the importance of replaying the input in our approach:
Without the replay, even on two identical systems, the be-
havior of an application can in fact be different between
the two executions.
Experiment IV. Finally, in this experiment, we show
that our tool is capable of detecting real-world malware
that behaves differently when running inside an analysis
system.
To populate the test suite, we extracted random sam-
ples from the Anubis database by querying for executa-
bles packed with one of the packers that are known to
either detect or not to work properly under Qemu (e.g.,
Armadillo and tElock). Notice that these are the packers
that the authors of Ether found not to be analyzable un-
der Anubis [16]. In all cases, our technique was able to
report the attempts of the samples to detect the presence
of the analysis system.
also
a
During
our
found
test, we
sample
(Heur.Trojan.Generic in Table 4) that worked prop-
erly under Anubis, but
immediately terminated in
VMware, and a sample (Backdoor.Win32.SdBot) that
raised an Internal Error in VMware, but had a normal
execution in Anubis.
Performance. The overhead introduced by our system
largely depends on the type of operation performed by
the program under analysis. For instance, writing a large
amount of information to a ﬁle requires our logging mod-
ule to dump and store the full data in the log. How-
ever, in our experiments, we noticed that our tool intro-
duced on average 1% overhead when logging and replay-
ing malware samples. When used in combination with an
emulator-based malware analysis system such as Anubis,
the delay this adds to the overall analysis time is negligi-
ble.
We also compared the time required to analyze a mal-
ware with Anubis and Ether. To perform a fair compar-
ison, we run Anubis with all its analysis enabled, and
Ether in one of its ﬁne-grained analysis modes. We re-
port here the result of running one test case that is rep-
resentative of the different performance of these two sys-
tems. In particular, we executed the command 7za.exe
a test.zip 1KB rand file, which compresses a
1KB-long random ﬁle. Its execution under Anubis took
4.267 seconds. In our VMWare reference machine with
our kernel module it took 1.640 seconds. The same ex-
ecution in Ether running in memwrite mode took 77.325
seconds (approximately 20x slower). Therefore, even
considering that in our experiment we have to run each
sample twice, we can still analyze split-personality mal-
ware one order of magnitude faster than under Ether.
6 Related Work
Since malware is a signiﬁcant security threat, a large
body of work exists that presents techniques to analyze
and detect malicious code. In this section, we ﬁrst high-
Sample
Email-Worm.Win32.Bagle.fk
Backdoor.Win32.Rbot.bng
Backdoor.Win32.Agent.eny
Email-Worm.Win32.Zhelatin.cl
Trojan-Downloader.Win32.Agent.alnx
Backdoor.Win32.Rbot.ccb
Backdoor.Win32.SdBot.gen
Virus.Win32.Parite.a
Trojan-Downloader.Win32.Dluca.gen
Hoax.Win32.Renos.wu
Syscall Replay Disabled
Syscall Replay Enabled
OK
FAIL
OK
FAIL
OK
FAIL
FAIL
OK
OK
FAIL
OK
OK
OK
OK
OK
OK
OK
OK
OK
OK
Table 3. Real Malware with no VM-checks.
Sample
Trojan-Proxy.Win32.Bypass.a
Heur.Trojan.Generic
Backdoor.Win32.Agobot.aow
Trojan-Spy.Win32.Banker.pcu
Worm.Win32.AutoRun.pga
Trojan-Spy.Win32.Bancos.zm
Trojan-Downloader.Win32.Agent.acrm
Backdoor.Win32.SdBot.fme
Trojan.Win32.KillAV.or
Net-Worm.Win32.Kolab.ckp
Packer
tElock
PE Patch.UPX
Armadillo
tElock
Armadillo
tElock