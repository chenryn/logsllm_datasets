title:Web Services Wind Tunnel: On Performance Testing Large-Scale Stateful
Web Services
author:Marcelo De Barros and
Jing Shiau and
Chen Shang and
Kenton Gidewall and
Hui Shi and
Joe Forsmann
Web Services Wind Tunnel: 
On Performance Testing Large-scale Stateful Web Services 
Marcelo De Barros, Jing Shiau, Chen Shang, Kenton Gidewall, Hui Shi, Joe Forsmann 
{marcelod,jshiau,cshang,kentong,huishi,josephfo}@microsoft.com  
Microsoft Corporation 
Abstract 
Other 
involve 
challenges 
New  versions  of  existing  large-scale  web  services 
such  as  Passport.com©  have  to  go  through  rigorous 
performance  evaluations  in  order  to  ensure  a  high 
degree  of  availability.  Performance  testing  (such  as 
benchmarking, scalability, and capacity tests) of large-
scale  stateful  systems  in  managed  test  environments 
has  many  different  challenges,  mainly  related  to  the 
reproducibility  of  production  conditions  in  live  data 
centers.  One of these challenges is creating a dataset 
in a test environment that mimics the actual dataset in 
production. 
the 
characterization of load patterns in production based 
on  log  analysis  and  proper  load  simulation  via  re-
utilization of data from the existing dataset. The intent 
of  this  paper  is  to  describe  practical  approaches  to 
address  some  of 
the  aforementioned  challenges 
through  the  use  of  various  novel  techniques.  For 
example, this paper discusses data sanitization, which 
is  the  alteration  of  large  datasets  in  a  controlled 
manner to obfuscate sensitive information, preserving 
data  integrity,  relationships,  and  data  equivalence 
classes.  This  paper  also  provides techniques for load 
pattern characterization via the application of Markov 
Chains to custom and generic logs, as well as general 
guidelines  for  the  development  of  cache-based  load 
simulation 
the  performance 
evaluation of stateful systems.  
1. Introduction 
tailored 
Large-scale online web services are subject to very 
different loads and conditions when they are released 
on  the  internet.  Services  such  as  Passport.com  can 
receive  up  to  300,000  user-driven  transactions  per 
second, and may contain a dataset of over 500,000,000 
users.  For  this reason, new  versions  of  such  services 
have  to  undergo  rigorous  performance  testing  before 
going  public.  Irrespective  of  the  tests  being  executed 
(benchmarking,  load,  scalability,  capacity,  etc),  the 
high-level  process  consists  of:  environment  (clusters, 
data, tools)  preparation,  execution,  and  analysis.  This 
paper  describes  practical  approaches  for  creating 
tools 
for 
execution 
of 
accurate 
performance tests for stateful web services. 
environments 
the 
for 
Web services are considered stateful if they contain 
hard-state instead of soft-state data [3]. Hard-state data 
is  data  that  cannot  be  lost  due  to  the  unfeasibility  of 
reconstructing it. An example is a user profile and user 
transactions for a bank account. Soft-state data can be 
reconstructed from hard-state data. An example would 
be  aggregated  financial  reports.  Many  web  services 
available today are stateful. 
is  a  crucial  step 
In performance test environments, pre-population of 
towards  replicating 
test  data 
production  conditions. Traditional approaches  for  test 
data generation consist of synthesizing the data based 
on  the  application  code,  random  and  probabilistic 
techniques,  or  custom  applications 
[4].  Other 
approaches  make  use  of  limited  data  sanitization 
processes  based  on  predetermined  heuristics  [5]. 
However it is impractical to synthesize the same hard-
states observed in production environments due to the 
unpredictability  of  the  many  ways  in  which  the  data 
may have been transformed based on users’ activities. 
Performance  tests  are  particularly  sensitive  to  the 
dataset  since  slight  differences  in  the  test  data  may 
result in significant discrepancies in the test results. 
The  ideal  dataset  would  be  constructed  from  the 
same set of production data for the performance tests, 
but  many  of  the  items  in  production  have  restricted 
access.  Therefore  there  is  a  need  for  a  sustainable 
process of obfuscating restricted data items so that the 
dataset can be safely used in test laboratories. This is 
accomplished  by  the  use  of  the  Data  Sanitization 
process [6], which aims to retrieve a set of databases 
and  deterministically  obfuscate  restricted  data,  while 
preserving  data 
integrity,  relationships,  and  data 
equivalence  classes.  This  process  is  described  in 
section 2. 
Once the right data is in place, there is still a need to 
simulate  production  behavior  by  duplicating  the right 
mix of various types  of transactions processed by the 
system and simulating the dependency between related 
sequences  of  transactions  as  observed  in  production. 
Many  live  environments  contain  a  set  of  logs  which 
can be mined to provide up-to-date statistics describing 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:30:40 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007subjectivity  of  the  data.  The  database  schemas  are 
modeled  as  XML  files  [7]  containing  all  of  the 
metadata pertinent to the tables in all of the databases.  
2.2 Sanitization Method Assignment: The method 
used to sanitize a certain PII field can now be chosen in 
such a way to preserve (i), (ii) and (iii). Figure 1 below 
illustrates  the  PII  identification  and  the  sanitization 
method assignment: 
the transaction mix. By using data mining techniques, 
one  can  determine  not  only  how  APIs  (Application 
Programming  Interfaces)  are  being  invoked,  but  also 
the  relationship  between  invocations  of  the  different 
APIs. We discuss one of these techniques based on an 
application  of  Markov  Chains  to  generic/custom  log 
data. This process is fully described in section 3. 
Section  4  discusses  approaches  for  writing  cache-
based  performance  test  tools  which  can  leverage  the 
previously  retrieved  and  sanitized  production  data, 
imported  into  test  environments.  Finally,  we  discuss 
practical  results  from  the  use  of  these  techniques,  as 
well as future improvements in section 5. 
2. Data Sanitization 
Using  production  data  for  testing  new  features 
which  will  operate  on  existing  data  (i.e.,  new  search 
functionality  that  allows  more  fine-grained  search 
criteria) is crucial not only for functional validation of 
correctness, but also for performance evaluation, since 
different  amounts  of  data,  dependencies,  and  data 
characteristics  may  have  significant  affects  in  the 
system’s  performance.  Production  data, 
overall 
information 
however,  contains 
regarding 
should  be  kept 
confidential,  even  if  only  used  in  restricted  test 
laboratories.  The  general 
to  classify  such 
confidential  data  is  called  Personally  Identifiable 
Information  (PII)  [1].  The  data  sanitization  process 
consists  of  a  set  of  tools  and  methodologies  to  take 
production  data  and  obfuscate  all  the  pre-determined 
PII, preserving three key characteristics: 
individuals  which 
large  amounts  of 
term 
i.  Data Integrity: Constraints applied to relational 
database  tables,  such  as  Primary  Keys  and 
Uniqueness are carried over after sanitization. 
ii.  Data  Relationships:  Relationships  between 
tables  in a relational database  persist after the 
sanitization process. 
iii.  Data  Equivalence  Classes:  Subsets  of  the 
domain input  data  are  preserved,  such that  all 
elements in  the  subsets  are  assumed  to  be  the 
same from the specification of the subsets [2]. 
The  current  process  is  tailored  for  obfuscation  of 
data stored in relational databases. The process consists 
of a sequence of steps listed as: 
2.1  PII  Identification:  the  first  step  consists  of 
identifying  the  data  that  needs  to  be  obfuscated. 
Databases 
large-scale  systems  may  contain 
thousands  of  different  tables  and  columns.  A  set  of 
tools 
the  data  sanitizer 
framework to assist the user with the identification of 
PII, although the identification process requires manual 
intervention and cannot be fully automated due to the 
is  provided  along  with 
for 
Figure 1. PII identification and method assignment 
A  sanitization  method  may  have  the  following 
generic signature: 
object[] Metadata) 
object  SanitizationMethod(  object  OriginalValue, 
The  metadata  may  consist  of  the  details  of  the 
particular  field  in  question,  such  as  data  type  and 
length.  The  method  should  perform  a  one-way 
transformation in order to avoid reverse engineering of 
the sanitized data. The data sanitizer framework comes 
with  several  sanitization  methods, 
the 
following ones (table 1). 
including 
Standard Sanitization Method 
Erase 
EraseBinary 
FillWithChar 
FillWithDigit 
HashString 
HashDigits 
NewGUID 
Description 
Erases any non-binary field 
Erases binary fields 
Replaces the entire field with a 
random string of same length 
Replaces the entire field with a 
random number of same length 
Applies a one-way SHA1 salt-
based (password) hash function 
Applies a one-way SHA1 salt-
based (password) hash function, 
but the result is numeric 
Replace a GUID with a different 
(new) GUID 
Table 1. Standard sanitization methods 
New methods can be added to the framework when 
deemed  necessary.  The  use  of  one-way  SHA1  hash 
functions as a sanitization method is essential to ensure 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:30:40 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007integrity  and  relationship  preservation  post-
data 
sanitization:  correlated  data  across  tables/DBs  can  be 
sanitized  by  using  one-way,  salt-based  SHA1  hash, 
ensuring the same output, thus consistency.  
A  crucial  step  is  to  review  PII  sets  as  well  as 
sanitization  methods  and  assignments  with  security 
experts and legal personnel to validate the correctness 
of the procedures. 
2.3  Test  and  Sanitization  Execution:  after  the 
proper  identification  of  PII  and  sanitization  methods, 
the  overall  sanitization  process  is  tested  in  restricted 
laboratories on non-production data. Upon verification, 
the process is carried out in production environments. 
Since the sanitization is an in-place procedure, a copy 
of  production  data 
is  made,  all  within  secure 
production  environments.  The  main  sanitizer  tool  is 
for  optimal  speed, 
multithreaded 
thus  multiple 
databases  and 
tables  are  processed  concurrently. 
Because of this, database constraints must be removed 
prior  to  the  sanitization  execution,  since  during  the 
execution  phase  data 
relationships  might  be 
temporarily  violated.  All  these  constraints  are  saved 
prior  to  the  sanitization,  and  are  recreated  upon 
completion of the sanitization process (figure 2). 
Figure 2. Sanitization execution 
Table 2 below shows some results obtained running 
the data sanitizer on HP DL385 G1, 4xAMD 2.4GHz 
processors, 4GB RAM servers, against large data sets. 
The average percentage of PII fields identified in these 
data  sets  was  ~13%,  with  one-way  hash  functions 
accounting for ~10% of the sanitization methods: 
Databases 
Subscription Service 
Partner Reporting Service 
Financial Reporting Service 
Customer Assistance Service 
Authentication Service 
Time (h)
Size 
Data 
33MM users  850GB 22 
30MM users  600GB 16 
32MM users  800GB 21 
1.4MM tickets 50GB  3.5 
400MM users  3.5TB  100 
Table 2. Sanitization runs experiments 
3. Markov Chain Stress Model 
The  Markov  Chain  Stress  Model  includes  two 
major components: the knowledge retriever component 
the  knowledge  exerciser  component.  Both 
and 
components  are  based  on  the  concept  of  the  Markov 
Chain dynamic stochastic process, which describes the 
state  of  systems  at  successive 
times  [8].  The 
knowledge retriever component applies data mining on 
production  activity  logs  and  discovers  the  parameter 
load  patterns  of  each  API.  The  knowledge  exerciser 
component uses Markov Chain Monte Carlo methods 
[9], which are a class of algorithms for sampling from 
probability  distributions,  based  on  constructing  a 
Markov  chain  with  the  desired  distribution  as  its 
stationary  distribution, 
former 
statistical  knowledge  to  the  stress  test  environment, 
and  generates  dynamic  scenarios  consistent  with 
production load patterns.  
to  manifest 
During  the  knowledge  retriever  step,  we  assume 
that: 
•  A distributed application has a set with countable 
the 
1
{
}
nX
,...,
XX
number of APIs, represented as: 
X =
,
0
iX is logically connected to API 
•  Any API 
jX with a probability weight, where j = i is 
possible. 
•  Each API 
iX has a known set of domain data as 
•  A client application makes API calls against the 
web service according to the Markov process. To 
simplify our model, we use the first-order Markov 
process [9] to implement the program, i.e. current 
API call Xi at time t depends only on the previous 
API call Xj at time t-1. 
− =
)
•  Client is homogenous over time, meaning that its 
behavior is consistent, the transition matrix can be 
re-built any time. 
XP
(
XP
(
1:0,
−
ti
1,
X
X
)
tj
,
tj
,
|
|
t
i
With  the  above  assumptions,  we  use  real  production 
trace  data  as  samples 
the  Markov 
Transition Matrix: 
to  estimate 
0
,
)
im
p
,...,
input parameters: 
X =
i
Where
∈
p
where
this API 
ppf
(
1
jp  belongs to a domain set 
S
jS is the set of all possible values of 
,..,1,0
, =
j
jS , 
m
i
j