Relax factor α
Discount factor γ
GAE Lambda λ
Value
{1024, 2048, 4096, 8192}
1024
{1024, 2048, 4096, 8192}
{1, 4, 16}
ReLU
GCN
0, 2, 4
{64x64, 256x256, 512x512}
0.0003
0.001
{1, 1.25, 1.5, 2}
0.99
0.97
Table 2: NeuroPlan hyperparameters.
Optimizer is a commercial optimization solver and supports a vari-
ety of programming languages. For efficiency, the failure checking
part is implemented with Gurobi in C++ and compiled to a binary
file which is then called in Python code. The implementation of
NeuroPlan has two optimizations to accelerate the training process.
The first one is source aggregation. Assume there are l IP links,
m IP nodes, n optical fibers and f flows. We have l variables to
represent the IP link capacities and n constraints for spectrum
consumption. For each failure, there are f m constraints for flow
conservation, 2l constraints for IP link capacity (two directions
for every IP link), 2l variables to represent the traffic volume on
the IP links (two directions for every IP link). We do not need the
spectrum consumption constraints when doing failure checking
since they have been encoded in the action mask (ğ4.2). Given
s failures, the total number of constraints is s(f m + 2l), and the
number of variables is l + 2s f l. If we do not consider the Classes
of Services (CoS) for the flows, f can be m2 at most. Rather than
formulating the constraints with individual flows, we apply source
aggregation [60], which aggregates the flows with the same source
as one single flow. Such optimization decreases the number of
constraints from s(f m +2l) to s(m2 +2l) and the number of variables
from l + 2s f l to l + 2sml.
The second one is stateful failure checking. As described in ğ4.1, if
a network survives a failure, then a network with more capacity can
guarantee to survive the same failure. To realize the stateful failure
checking, we maintain a fixed order of failures. For each step, we
check the failure scenarios starting at the failure that is not survived
in the previous step instead of trying to solve all the failures at one
time. Moreover, we can group the failures and employ multiple
machines to check failure groups in parallel, which enables training
for problems with a large number of failures. In some cases, the time
to build up a Gurobi model is even longer than the time to solve the
model. Thus, we only update the constraints that are influenced
by the failure in the model, avoiding building up the model from
scratch for each failure.
The evaluation results in ğ6.1 shows the efficiency of the two
optimizations.
266
SIGCOMM ’21, August 23–28, 2021, Virtual Event, Netherlands
Hang Zhu, Varun Gupta, Satyajeet Singh Ahuja,
Yuandong Tian, Ying Zhang, Xin Jin
Figure 7: Implementation efficiency. The cross indicates the
omitted entries in which the running time is larger than 2
hours. The running time is normalized with that of Neuro-
Plan on each topology.
Figure 8: NeuroPlan can produce optimal solutions for
small-scale problems. The cost results are normalized with
that of ILP on each topology.
Workload patterns. The computation of neural networks, includ-
ing forward propagation and backward propagation, takes signif-
icant time during training process. The backward propagation is
done with the data of the entire epoch. To exploit the capability of
AI accelerators (e.g., GPU) to accelerate training, we take a larger
capacity increment unit so that RL agent can get feasible solutions
with fewer steps and the data of the entire epoch can be fit into the
GPU memory.
6 EVALUATION
In the evaluation, we aim to answer the following questions:
• How efficient is NeuroPlan with the optimizations on the imple-
mentation (ğ6.1)?
• How is the optimality of the network plan generated by Neuro-
Plan for small-scale problems (ğ6.2)?
• How is NeuroPlan compared with the heuristic approach for
large-scale problems (ğ6.3)?
• What is the impact of the parameters (ğ6.4)?
We run NeuroPlan on g4dn.4xlarge AWS instances, with 16 CPU
cores and one NVIDIA Tesla T4 GPU. We use five production net-
work topologies with different scales, i.e., A, B, C, D and E, listed
in the ascending order of topology size. In terms of the scale, A
has tens of IP links, tens of failures and tens of (site-to-site) flows,
and needs to add a few Tbps capacity for a feasible solution; E has
hundreds of IP links, hundreds of failures and about one thousand
flows, and needs to add a few hundred Tbps capacity for a feasible
solution. We use real failure scenarios, traffic demands, reliabil-
ity policies and cost models from production networks. We run
NeuroPlan for each problem until convergence or for 1024 epochs.
We compare NeuroPlan with two alternative approaches: ILP
and ILP-heur. ILP follows the problem formulation in ğ3.1 and
solves it without any heuristics. ILP-heur integrates hand-designed
heuristics into ILP to trade optimality for tractability. For faithful
comparison, we use the same heuristic setups used in the production
network. We also report the first-stage results generated by the RL
agent (First-stage).
6.1 Implementation Efficiency
To show the benefits of the two implementation optimizations (i.e.,
source aggregation and stateful failure checking), we compare the
running time of different implementations, including the vanilla
plan evaluator (Vanilla), the plan evaluator with source aggregation
(SA), and the plan evaluator with both source aggregation and
stateful failure checking (NeuroPlan). We measure the average
running time for 10 epochs on the five topologies (A, B, C, D, E). The
running time is normalized with the running time of NeuroPlan on
each topology. We omit the entries of Vanilla in which the running
time is larger than 2 hours. As shown in Figure 7, SA reduces the
running time by a factor of 2 with topology A. The efficiency of SA
is further for other topologies (B, C, D, E) with more flows, as SA
can reduce the number of constraints significantly. NeuroPlan is
7-14 times faster than SA as it applies the stateful failure checking.
6.2 Optimality for Small-Scale Problems
ILP can generate the optimal solutions for small-scale problems. We
compare NeuroPlan and ILP on small-scale problems to evaluate
the optimality of NeuroPlan. We vary the original capacities of
the small topology A to create multiple synthetic problems with
different sizes of search space. Specifically, A-0, A-0.25, A-0.5, A-0.75
and A-1 mean that the original capacity of each link is 0%, 25%,
50%, 75% and 100% of that of the corresponding link on topology A,
respectively. We set the relax factor α to be 2. As shown in Figure 8,
the results of First-stage is already close to the optimal with 75% (A-
0.75) and 100% (A-1) original capacities. Even with 0% (A-0) original
capacity where the RL agent generates the plan from scratch, the
cost results of First-stage is no more than about 30% of the optimal
cost. After the second stage, NeuroPlan is able to produce the final
plan with a cost no more than 2% of the optimal cost.
6.3 Scalability for Large-Scale Problems
NeuroPlan can overcome the scalability issue encountered by ILP.
Figure 9 compares the results of First-stage, NeuroPlan, ILP-heur
267
ABCDENetwork topology0481216Normalized running time  per epochVanillaSANeuroPlanA-0A-0.25A-0.5A-0.75A-1Network topology0.00.51.01.5Normalized costFirst-stageNeuroPlanILPNetwork Planning with Deep Reinforcement Learning
SIGCOMM ’21, August 23–28, 2021, Virtual Event, Netherlands
Figure 9: ILP fails to scale to large topologies, indicated
by the crosses. NeuroPlan outperforms ILP-heur on large
topologies and avoids human efforts to tune the heuristics.
The cost results are normalized to that of ILP-heur on each
topology.
and ILP for the five topologies, i.e., A, B, C, D and E. The relax factor
α is set to be 1.5. The results are normalized with the results of ILP-
heur. ILP can only solve the problem with topology A, as the scale
of other topologies is much larger than A and ILP cannot solve them
in a reasonable amount of time (e.g., a few weeks). ILP-heur fails
to achieve a good trade-off between optimality and tractability for
all the topologies as different problems need customized heuristics
for good results. For example, ILP-heur over-trades optimality for
tractability on topology A as topology A is simple and can be
solved completely with ILP for the optimal solution. In contrast,
NeuroPlan prunes the search space automatically and specifically
for each problem. After getting the pruned search space, NeuroPlan
can solve it and get the optimal solution easily at the second stage.
In addition, Figure 9 shows that NeuroPlan can get a plan for large
network topologies (i.e., B, C, D and E) with a lower cost from 11%
to 17% compared with that of ILP-heur.
6.4 Sensitivity Analysis
To better understand the impact of different parameters of Neuro-
Plan, we conduct a sensitivity analysis on three synthetic topologies
used in ğ6.2, i.e., A-0, A-0.5, A-1. The parameters include the num-
ber of GNN layers, the hidden size of MLP, the maximum capacity
unit per step and the relax factor α.
Figure 10 shows the costs of First-stage normalized with their
corresponding optimal cost. Interestingly, NeuroPlan can learn even
without GNN for A-1. However, it fails to learn and converge for
other topologies (i.e., A-0 and A-0.5). It indicates that MLP alone can
handle simple problems while GNN becomes essential for problems
with a larger scale. Two or four layers of GNN have similar results.
We also observed that the number of GNNs does not influence the
convergence speed with respect to number of epochs (Figure 12(b)).
Then we vary the hidden size of MLP used by the critic and actor
from 16x16 all the way up to 512x512. We report the cost results of
First-stage. All the costs are normalized to the optimal cost on each
topology. As shown in Figure 11(a), the MLPs with different hidden
sizes converge to similar results. In Figure 11(b), we show the epoch
reward v.s. the number of epochs for A-100. It indicates that larger
Figure 10: Impact of GNN layers on the results of First-stage.
The crosses indicate that the RL agent does not converge.
The results are normalized to the optimal cost on each topol-
ogy.
(a) Impact of hidden size in MLP on
cost results of First-stage.
(b) Impact of hidden size in MLP on
convergence speed with A-1.
Figure 11: Impact of MLP on cost results of First-stage and
convergence speed. The cost results are normalized with the
optimal cost on each topology.
hidden size leads to a faster convergence speed with respect to the
number of epochs. This is because a more complicated MLP is able
to model more complicated relationship and can be trained more
efficiently. The results on A-0 and A-0.5 are similar.
We also study the impact of the maximum capacity unit per step.
Figure 12(a) indicates that the maximum capacity unit per step has
nearly no influence on the results of First-stage. Figure 12(b) shows
the epoch reward v.s. the number of epochs for A-100. A larger
maximum capacity unit leads to a faster convergence speed with
respect to the number of epochs. We have also tried other topologies
and found that this statement does not hold for other topologies. A
larger maximum capacity unit only benefits the problems where
the capacity increments are concentrated on a few links rather than
scattered among many links in the converged solutions.
At last, we vary the relax factor α. To test a wide range of problem
scales, we evaluate NeuroPlan with different α (i.e., 1, 1.25, 1.5) for
the five topologies, i.e., A, B, C, D, E. Figure 13 reports the results
of NeuroPlan normalized by the results of First-stage. The second
stage does not improve the results a lot for topology A as RL already
268
ABCDENetwork toplogy0.00.51.01.52.0Normalized costFirst-stageNeuroPlanILP-heurILPA-0A-0.5A-1Network topology0.00.51.01.5Normalized cost024A-0A-0.5A-1Network topologies0.00.51.01.5Normalized cost6425651205001000Number of epochs−0.2−0.10.0Epoch reward64256512SIGCOMM ’21, August 23–28, 2021, Virtual Event, Netherlands
Hang Zhu, Varun Gupta, Satyajeet Singh Ahuja,
Yuandong Tian, Ying Zhang, Xin Jin
(a) Impact of maximum capacity unit
per step on results of First-stage.
(b) Impact of maximum capacity units
per step on convergence speed with A-
1.
Figure 12: Impact of maximum capacity units on results of
First-stage and convergence speed. The cost results are nor-
malized with the optimal cost on each topology.
gets a solution close to the optimal, while it does find better results
up to 46% lower compared to the results of First-stage for other
topologies. And it shows that with a larger α, NeuroPlan can get
a better solution in a larger search space. Thus, the relax factor
provides a convenient and tunable knob for the trade-off between
optimality and tractability.
7 RELATED WORK
Network cost optimization. Minimizing the network cost while
satisfying the service expectations is an important problem in net-
work management. There is a lot of work on reducing the network
cost by traffic engineering in both the IP layer [20, 23, 79] and the op-
tical layer [37, 56]. Besides traffic engineering, capacity planning is
also critical to the performance, reliability and cost of a network. A
multi-layer capacity planning is proposed [15] to achieve minimum
cost with a focus on multi-layer restoration. NeuroPlan focuses on
the network planning problem and minimizes the network cost
with GNNs and deep RL.
Graph neural network and RL. GNNs [33, 52, 70, 71, 77], as an
efficient graph representation method, are widely used in many
fields, including text classification [51], molecular feature extrac-
tion [12], protein structure prediction [1], chip design [43], and
node classification [29]. Existing work also combines GNNs with
RL for different tasks, such as solving classical combinatorial prob-
lems [28, 32, 45] and generating molecule graphs [74]. NeuroPlan
exploits the capability of GNNs to encode the network topology
and uses RL to efficiently explore the state space and prune the
search space for ILP problems.
RL for optimization problems. Many solutions have applied RL
to optimization problems [4, 5, 10, 40], including learning to cut for
integer programming [58], learning branching policies for Mixed
Integer Linear Programming [7, 18], building ML compilers [22, 61]
and optimizing neural network architectures [25, 35, 80]. NeuroPlan
makes two unique contributions to solving the network planning
problem. First, it uses GNNs and a domain-specific node-link trans-
formation for modeling dynamic network topologies. Second, it
combines RL with ILP to address the scalability challenge for large-