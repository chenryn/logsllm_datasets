＜/configuration＞
将conf/hdfs-site.xml文件修改成：
＜?xml version="1.0"?＞
＜?xml-stylesheet type="text/xsl"href="configuration.xsl"?＞
＜configuration＞
＜property＞
＜name＞dfs.replication＜/name＞
＜value＞2＜/value＞
＜/property＞
＜/configuration＞
将conf/mapred-site.xml文件修改成：
＜?xml version="1.0"?＞
＜?xml-stylesheet type="text/xsl"href="configuration.xsl"?＞
＜configuration＞
＜property＞
＜name＞mapred.job.tracker＜/name＞
＜value＞master：9001＜/value＞
＜/property＞
＜/configuration＞
将conf/masters文件修改成：
master
将conf/slaves文件修改成（注意每行只能有一个主机名）：
slave1
slave2
B.2 Hadoop启动
在第一次启动Hadoop时，需要格式化Hadoop的HDFS，命令如下：
bin/Hadoop namenode-format
接下来启动Hadoop，命令如下：
bin/start-all.sh
启动之后，可以通过http：//master：50070、http：//master：50030这两个页面查看集群的状态。需要注意的是，由于启动之初集群处理安全模式，所以可能看到活跃节点或者TaskTracker进程都为0。等集群离开安全模式之后，就会恢复正常。
B.3 Hadoop使用
B.3.1 命令行管理Hadoop集群
在使用Hadoop时，最常用的就是使用命令行来管理HDFS，可以上传下载文件，管理集群节点，查看集群状态，运行指定进程等，命令的运行格式如下：
bin/hadoop command[genericOptions][commandOptions]
这里以运行文件系统工具的几个简单命令为例进行说明，有关命令行管理集群的详细内容请参见本书第九章。
bin/hadoop fs-ls hdfs_path//查看HDFS目录下的文件和子目录
bin/hadoop fs-mkdir hdfs_path//在HDFS上创建文件夹
bin/hadoop fs-rmr hdfs_path//删除HDFS上的文件夹
bin/hadoop fs-put local_file hdfs_path//将本地文件copy到HDFS上
bin/hadoop fs-get hdfs_file local_path//复制HDFS文件到本地
bin/hadoop fs-cat hdfs_file//查看HDFS上某文件的内容
B.3.2 运行MapReduce框架程序
本小节介绍如何编译自己编写的MapReduce框架程序，并在Hadoop上运行。假设自己编写的程序文件名为MyMapred.java，并放置在Hadoop安装目录下，即/home/u/hadoop-1.0.1。
1）首先在hadoop安装目录下创建MyMapred文件夹，然后编译自己的程序。命令：
javac-classpath hadoop-core-1.0.1.jar：lib/commons-cli-1.2.jar-d MyMapred MyMapred.java
2）将编译好的程序打包成JAR文件，命令：
jar-cvf MyMapred.jar-C MyMapred.
3）在Hadoop上运行JAR文件。
在单节点方式的Hadoop下，只需要在准备好本地的输入文件之后（此处假设为/home/u/input），在命令行输入下面的命令就可以运行程序（output文件夹应不存在）：
bin/hadoop jar MyMapred.jar MyMapred/home/u/input output
运行结束之后就可以在output路径下查看程序的输出结果。
在伪分布方式和完全分布方式的Hadoop集群下，首先在集群上创建输入数据路径（此处为input），然后将本地的程序输入数据文件上传到HDFS上的输入路径中，这两个步骤需要使用的命令在“命令行管理Hadoop集群”小节已讲到，此处不再赘述。准备好输入路径之后就使用同样的命令运行JAR文件：
bin/hadoop jar MyMapred.jar MyMapred input output
需要注意的是，命令中的input和output都是Hadoop集群上的路径，而非单节点下的本地目录。这里同样需要保证output路径在HDFS上并不存在。运行结束之后，就可以使用前面介绍的命令行命令来查看output文件夹下的输出文件名和输出文件的内容。
附录C 使用DistributedCache的MapReduce程序
本章内容
程序场景
详细代码
C.1 程序场景
问题定义：过滤无意义单词（a、an和the等）之后的文本词频统计。代码的具体做法是：将事先定义的无意义单词保存成文件，保存到HDFS上，然后在程序中将这个文件定义成作业的缓存文件。在Map启动之后先读入缓存文件，然后统计过滤后单词的频数。源代码的下载请到本书代码下载网址：http：//datasearch.ruc.edu.cn/HadoopInAction/shiyandaima.html。
C.2 详细代码
package cn.edu.ruc.cloudcomputing.book；
import java.io.BufferedReader；
import java.io.FileReader；
import java.io.IOException；
import java.net.URI；
import java.util.HashSet；
import java.util.StringTokenizer；
import org.apache.hadoop.conf.Configuration；
import org.apache.hadoop.filecache.DistributedCache；
import org.apache.hadoop.fs.Path；
import org.apache.hadoop.io.IntWritable；
import org.apache.hadoop.io.Text；
import org.apache.hadoop.mapreduce.Job；
import org.apache.hadoop.mapreduce.Mapper；
import org.apache.hadoop.mapreduce.Reducer；
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat；
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat；
public class AdvancedWordCount{
public static class TokenizerMapper
extends Mapper＜Object, Text, Text, IntWritable＞{
private final static IntWritable one=new IntWritable（1）；
private Text word=new Text（）；
private HashSet＜String＞keyWord；
private Path[]localFiles；
//此函数在每个Map Task启动之后立即执行（此处因使用新
//API--org.apache.hadoop.mapreduce.Mapper，所以此函数名是setup而不是
//旧API中的configure，有疑问可查看API）
public void setup（Context context
）throws IOException, InterruptedException{
keyWord=new HashSet＜String＞（）；
Configuration conf=context.getConfiguration（）；
localFiles=DistributedCache.getLocalCacheFiles（conf）；
//将缓存文件内容读入到当前Map Task的全局变量中
for（int i=0；i＜localFiles.length；i++）{
String aKeyWord；
BufferedReader br=new BufferedReader（new FileReader
（localFiles[i].toString（）））；
while（（aKeyWord=br.readLine（））！=null）{
keyWord.add（aKeyWord）；
}
br.close（）；
}
}
//根据缓存文件中缓存的无意义单词对输入流进行过滤
public void map（Object key, Text value, Context context
）throws IOException, InterruptedException{
StringTokenizer itr=new StringTokenizer（value.toString（））；
while（itr.hasMoreTokens（））{
String aword=itr.nextToken（）；
if（keyWord.contains（aword）==true）
continue；
word.set（aword）；
context.write（word, one）；
}
}
}
public static class IntSumReducer
extends Reducer＜Text, IntWritable, Text, IntWritable＞{
private IntWritable result=new IntWritable（）；
public void reduce（Text key, Iterable＜IntWritable＞values，
Context context
）throws IOException, InterruptedException{
int sum=0；
for（IntWritable val：values）{
sum+=val.get（）；
}
result.set（sum）；
context.write（key, result）；
}
}
public static void main（String[]args）throws Exception{
Configuration conf=new Configuration（）；
//将HDFS上的文件设置成当前作业的缓存文件
DistributedCache.addCacheFile（new URI（"hdfs：//localhost：9000/user/ubuntu/
cachefile/KeyWord#KeyWord"），conf）；
Job job=new Job（conf，"advanced word count"）；
job.setJarByClass（AdvancedWordCount.class）；
job.setMapperClass（TokenizerMapper.class）；
job.setCombinerClass（IntSumReducer.class）；
job.setReducerClass（IntSumReducer.class）；
job.setOutputKeyClass（Text.class）；
job.setOutputValueClass（IntWritable.class）；
FileInputFormat.addInputPath（job, new Path（"input"））；