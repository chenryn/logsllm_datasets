(cid:115)(cid:80)
node have an initial score to differentiate itself from other new
nodes before its reward is estimated.
Finally, we have the score of a node a via multiplying its
rareness and estimated fuzzing performance together as shown
in Equation 8. This score is the one used in Algorithm 3 to
determine which nodes will be picked and which seed will be
fuzzed next.
Score(a) = Rareness(a) × F uzzP erf (a)
(8)
V. EVALUATION
Our main hypothesis is that our multi-level coverage metric
and hierarchical seed scheduling algorithm driven by the MAB
model can achieve a good balance between exploitation and
exploration, thus boosting the fuzzing performance. To validate
our hypothesis, we implemented two prototypes AFL-HIER
and AFL++-HIER, one based on AFL [55] and the other based
on AFL++ [17], and evaluated them on various benchmarks
aiming to answer the following research questions.
• RQ1 Can AFL-HIER/AFL++-HIER detect more bugs than
the baseline?
• RQ2 Can AFL-HIER/AFL++-HIER achieve higher cover-
age than the baseline?
• RQ3 How much overhead does our technique impose on
the fuzzing throughput?
• RQ4 How well does our hierarchical seed scheduling
mitigate the seed explosion problem caused by high
sensitive of coverage metrics?
• RQ5 How do the hyper-parameters affect the performance
of our hierarchical seed scheduling algorithm?
• RQ6 How ﬂexible is our framework to integrate other
coverage metrics?
A. Experiment Setup
1) Benchmarks: The ﬁrst set of programs are from DARPA
Cyber Grand Challenge (CGC) [10]. These programs are
carefully crafted by security experts that embed different
kinds of technical challenges (e.g., complex I/O protocols
and input checksums) and vulnerabilities (e.g., buffer over-
ﬂow, integer overﬂow, and use-after-free) to comprehensively
evaluate automated vulnerability discovery techniques. There
are 131 programs from CGC Qualifying Event (CQE) and
74 programs from CGC Final Event (CFE), a total of 205.
CGC programs are designed to run on a special kernel with
seven essential system calls so that competitors can focus
on vulnerability discovery techniques. In order to run those
programs within a normal Linux environment, we use QEMU
to emulate the special system calls. Unfortunately, due to
imperfect simulation, some CGC programs fail
to be run
correctly. We also cannot handle programs that consist of
multiple binaries, which communicate with each other through
pre-deﬁned inter-process communication (IPC) channels. As
a result, we can only successfully fuzz 180 CGC programs
(or binaries, in other words). We fuzz each binary for two
hours and repeat each experiment 10 times to mitigate the
9
effects of randomness. Each fuzzing starts with a single seed
“123\n456\n789\n”. We chose this initial because the
initial seed affects the fuzzing progress a lot: seeds that are
too good may make most code covered at the beginning if
the program is not complex, while poor ones may make the
fuzzing get stuck before reaching the core code of the program.
These two cases both will make the fuzzing reach the plateau
early, and fail to show the performance differences between
our approach and other fuzzers. The seed we chose showed
a good capability to reveal performance differences between
fuzzers.
The second benchmark set is the Google FuzzBench [21]
that offers a standard set of tests for evaluating fuzzer perfor-
mance. These tests are derived from real-world open-sourced
projects (e.g., libxml, openssl, and freetype) that are widely
used in ﬁle parsers, protocols, and font operations. For this
dataset, we used the standard automation script to run the
benchmarks, so each benchmark uses the seeds provided by
Google.
2) Implementations: For evaluation over the CGC dataset,
we used a prototype built on top of the code open-sourced by
Wang et al. [45], which is based on AFL QEMU-mode, for
its support for binary-only targets and its emulation of CGC
system calls. For evaluation over the FuzzBench dataset, we
used a prototype built upon the AFL++ project [17] (QEMU-
mode only), for its support of persistent mode and higher
fuzzing throughput.
3) Baseline Fuzzers: For AFL-based prototype, we choose
three fuzzers as the baseline for comparison:
the original
AFL [55], AFLFAST [9], and AFL-FLAT [45]. AFL-FLAT is
conﬁgured with edge sensitivity CE and distance sensitivity
CD (see §III-D2 for more details), but uses the power sched-
uler from AFLFAST instead of our hierarchical scheduler. As
discussed in §II-A, the performance of greybox fuzzing is
mainly affected by four factors: seed selection, seed schedul-
ing, mutation strategies, and fuzzing throughput. We made all
fuzzers use the same mutation strategy to reﬂect the beneﬁt of
our approach, and ran the experiments ten times to minimize
the impact of randomness [24]. We also ran all fuzzers in the
QEMU mode so they can have similar fuzzing throughput,
which also makes it easier to assess AFL-HIER’s performance
overhead. Comparisons with AFL and AFLFAST aim to show
the overall performance improvement of AFL-HIER; and com-
parison with AFL-FLAT aims to show the necessity/beneﬁt of
our scheduler (i.e., increasing the sensitivity of the coverage
metric alone is not enough).
For AFL++-based prototype, we choose two fuzzers as the
baseline1: the original AFL++2 [17] and AFL++-FLAT. We ran
all fuzzers in the QEMU-mode and enabled persistent mode
for better throughput
4) Computing Resources: All
the experiments are con-
ducted on a 64-bit machine with 48 cores (2 Intel(R) Xeon(R)
Platinum 8260 @2.40GHz), 375GB of RAM, and Ubuntu
18.04. Each fuzzing instance is bound to a core to avoid
interference.
1We are working with Google to provide a more thorough comparison with
other fuzzers.
2The version is 2.68c which our prototype is built on.
(a) Number of crashed CGC binaries.
(b) Number of CGC binaries crashed over time.
Fig. 3: Crash detection on CGC benchmarks.
B. RQ 1. Bug Detection
(cid:63) In experiments with CGC benchmarks, AFL-HIER
crashes more binaries and faster. Especially, it crashes the
same number of binaries in 30 minutes, that AFLFAST
crashes in 2 hours.
In this experiment, we evaluate fuzzers’ capability of de-
tecting known bugs embedded in the CGC binaries. Figure 3a
shows the number of crashed CGC binaries across ten rounds
of trials. Note that since each binary supposedly only has one
vulnerability, this number equals the total number of unique
crashes. On average, AFL crashed 64 binaries, AFLFAST
crashed 61 binaries, and AFL-FLAT crashed 62 binaries. In
contrast, AFL-HIER crashes about 77 binaries on average,
which is about 20% more binaries in the 2-hour fuzzing
campaign. AFL-HIER also performed much better when we
look at the lower and upper bound: its lower bound of crashes
(74) is always higher than the upper bound of all other
fuzzers. Notably, these vulnerabilities are carefully designed
by security experts to highly mimic real-world security-critical
vulnerabilities.
Table I shows the pairwise comparisons of CGC binaries
TABLE I: Pairwise comparisons (row vs. column) of uniquely
crashed on CGC benchmark.
AFL AFLFAST AFL-FLAT AFL-HIER
AFL
AFLFAST
AFL-FLAT
AFL-HIER
-
3
11
17
8
-
13
22
16
13
-
18
5
5
1
-
10
uniquely crashed by a fuzzer across ten rounds of trails.
As we can see, the added (distance) sensitivity CD allows
AFL-FLAT and AFL-HIER to crash a considerable amount of
binaries that edge sensitivity (i.e., AFL and AFLFAST) cannot
crash. However, due to the seed explosion problem, AFL-FLAT
could not efﬁciently explore the seed pool; so it also missed
many bugs AFL and AFLFAST can trigger. In contrast, AFL-
HIER can achieve a good balance between exploration and
exploitation: it crashed more unique binaries and missed much
less.
Next, we measured the time to ﬁrst crash (TFC) and show
the accumulated number within a 95% conﬁdence of binaries
crashed over time in Figure 3b. As shown in recent studies [6],
[22], TFC is a good metric to measure the performance of
fuzzers. The x-axis presents the time in minutes, and the y-
axis shows the number of crashed binaries. As shown in the
graph, AFL-HIER stably crashed about 20% more binaries than
other fuzzers from the beginning to the end. Notably, AFL-
HIER crashed the same number of binaries in 30 minutes as
AFLFAST did in 120 minutes; and crashed the same number of
binaries in 40 minutes as AFL did in 120 minutes. In contrast,
AFLFAST was lagging behind AFL and AFLFAST in most of
the time and only surpassed AFLFAST after 100 minutes. This
result showed that our hierarchical scheduler not only can ﬁnd
many unique bugs but also can efﬁciently explore the search
space.
C. RQ 2. Code Coverage
(cid:63) Results on CGC binaries demonstrate that AFL-HIER
generally achieved more code coverage and achieved the
same coverage faster. Speciﬁcally, AFL-HIER increases the
coverage by more than 100% for 20 binaries, and achieves
the same coverage in 15 minutes that AFLFAST achieves in
120 minutes for about half of the binaries. On FuzzBench,
AFL++-HIER achieved higher coverage on 10 out of 20
projects.
CGC Benchmark. In this experiment, we ﬁrst measured the
edge coverage achieved by fuzzers using QEMU (i.e., captured
during binary translation) on CGC binaries.
Figure 4a illustrates the mean code coverage increase of
AFL-HIER over other fuzzers for the 180 CGC binaries, after
2 hours of fuzzing. The curve above 0% means AFL-HIER
covered more and the curse below 0% means AFL-HIER
covered less. The x-axis presents the accumulated number of
binaries within a 95% conﬁdence, and the y-axis shows the
increased coverage in logarithmic scale. For example, there
are about 20 binaries for which the code coverage is increased
by at least 100%, and about 45 binaries for which the code
coverage is increased by at least 10%. After 2 hours of fuzzing,
AFL-HIER achieved more coverage for about 90 binaries than
other fuzzers and achieved the same coverage for 50 binaries.
Among about 30 binaries on which AFL-HIER achieves less
coverage, on half of them the difference is lower than 2%; and
only on ﬁve of them the difference is greater than 10%. This
result shows that our approach can cover more or similar code
on most binaries besides detecting more bugs.
Figure 4b illustrates how fast AFL-HIER can achieve the
same coverage as other fuzzers in two hours. The dashed lines
586062646668707274767880number of crashed binariesaflaflfastafl-flatafl-hier0102030405060708090100110120time to crash (min)3035404550556065707580number of crashed binariesaflaflfastafl-flatafl-hierThe x-axis represents the time in minutes and the y-axis shows
the accumulated number of binaries within a 95% conﬁdence
that AFL-HIER won on coverage. We can observe that after
10 minutes, AFL-HIER already won for about 40 binaries over
AFL and AFLFAST. After 1 hour, it further increased the gap by
winning for more than 70 binaries. Overall, AFL-HIER steadily
won for more and more binaries throughout the process of the
2-hour fuzzing campaign. This indicates that AFL-HIER can
continuously make breakthroughs in new coverage for binaries
when other fuzzers plateaued.
FuzzBench. Next, we compare AFL++-HIER with two base-
line fuzzers (AFL++ and AFL++-FLAT) on Google FuzzBench
benchmarks. Figure 5 shows the mean coverage (with conﬁ-
dence intervals) over time during 6-hour fuzzing campaigns3.
The y-axis presents the number of covered edges and the x-axis
represents time. Please note that the x-axis is in logarithmic
scale, as recent work suggests the required efforts to achieve
more coverage grow exponentially [6]. Meanwhile, the Vargha-
Delaney [43] effect size ˆA12 is shown at the bottom of each
sub-ﬁgure, where the left one is of between AFL++-HIER
over AFL++ (Qemu) and the right one is of between AFL++-
HIER and AFL++-FLAT, respectively. A value above 0.5735,
0.665, 0.737 (or below 0.4265, 0.335, 0.263) indicates a small,
medium, large effect size. More intuitively, a larger value
above 0.5 indicates a higher probability of that AFL++-HIER
will cover more edges than AFL++ (Qemu) or AFL++-FLAT
in a fuzzing campaign. Moreover, a value starting with a star
indicates a statistical signiﬁcance tested by Wilcoxon signed-
rank test (p < 0.05). Overall, AFL++-HIER could beat AFL++
(Qemu) and AFL++-FLAT on about ten projects, and achieved
signiﬁcantly more coverage on projects openthread, sqlite3,
and proj4.
Table II shows the unique edge coverage of AFL++ (Qemu)
and AFL++-HIER. The results indicate even on programs
where AFL++-HIER has lower mean coverage than AFL++,
it still can cover some unique edges AFL++ does not cover.
Note that here we union edge coverage across different runs, so
for some benchmarks like lcms and libpcap, though the mean
coverage differences are large, the unique coverage differences
are much smaller.
Compared to the results on the CGC benchmarks, we
observe that our performance is not signiﬁcantly better than
AFL++ on most of the FuzzBench benchmarks. We suspect
the reason is that our UCB1-based scheduler and the hyper-
parameters we used in the evaluation prefer exploitation over
exploration. As a result, when the program under test
is
relatively smaller (e.g., CGC benchmarks), our scheduler can
discover more bugs without sacriﬁcing the overall coverage
by too much. But on FuzzBench programs, breaking through
some unique edges (Table II) can be overshadowed by not
exploring other easier to cover edges.
D. RQ 3. Fuzzing Throughput
(cid:63) Results on CGC benchmarks show that AFL-HIER has
a competitive throughput as AFL and AFLFAST. Moreover,
even built on the faster fuzzer AFL++, AFL++-HIER still
(a) Mean coverage increase. For X binaries, AFL-HIER
achieves at least Y% more coverage than other fuzzers. A curve
towards upper-right indicates that AFL-HIER outperforms the
other more signiﬁcantly.
(b) Time to coverage. For X binaries, AFL-HIER achieved
the same coverage in Y minutes, as the other fuzzer achieved
in 2 hours (solid line). A curve in solid line towards the
lower right with its counterpart in dashed line towards lower
left indicates a more statistical signiﬁcance of that AFL-HIER
achieve coverage faster than the opponent.
(c) Better coverage. After X minutes of fuzzing, AFL-HIER
achieves more coverage than other fuzzers for Y binaries. An
curve towards upper left indicates that AFL-HIER achieves
better coverage than the opponent more signiﬁcantly.
Fig. 4: Coverage improvement on the CGC benchmarks.
(on the right-hand-side after hitting 120 min) show for the
cases where baseline fuzzers achieved more ﬁnal coverage
in two hours. The x-axis shows the accumulated number of
binaries within a 95% conﬁdence, while the y-axis shows
the time in minutes. We can see that for about half of the
total 180 binaries, AFL-HIER achieved the same coverage in
15 minutes as baseline fuzzers did in 2 hours. Moreover, for
about 110 binaries, AFL-HIER achieves the same coverage in
half an hour; and for about 130 binaries, AFL-HIER achieves
the same coverage in one hour. Similar to TFC (time to ﬁrst
crash), this result also shows that our approach can achieve
the same coverage faster, indicating it can balance exploration
and exploitation well.
Figure 4c shows the number of binaries for which AFL-
HIER achieved more coverage than other fuzzers over time.
3We are working with Google to provide a 23-hour run that compares with
more fuzzers.
11
020406080100120140160180number of binaries-200%-50%-10%-2%0%2%10%50%200%afl-hier vs aflafl-hier vs aflfastafl-hier vs afl-flat020406080100120140160180number of binaries020406080100120time (min)afl-hier vs aflafl-hier vs aflfastafl-hier vs afl-flat0102030405060708090100110120time (min)020406080100number of binariesafl-hier vs aflafl-hier vs aflfastafl-hier vs afl-flatFig. 5: Mean coverage in a 6 hour fuzzing campaign on FuzzBech benchmarks.
has a comparable throughput as shown by the results on
FuzzBench benchmarks.
A multi-level coverage metric requires collecting more
coverage measurements during runtime and performing more
operations to insert a seed into the seed tree. Similarly, our
hierarchical scheduler also requires more steps than the power
scheduler of AFL and AFLFAST. Therefore, we expect our
approach to have a negative impact on fuzzing throughput.
Moreover, the multi-level coverage metric is sensitive to minor
variances of test cases and execution paths; consequently, it is
more likely to schedule larger and more complex seeds leading
to longer execution time.
To quantify the impact on fuzzing throughput, we ﬁrst in-
vestigated the proportion of the time that AFL-HIER spends in
scheduling, which involves maintaining the incidence frequen-
cies and the tree of seeds and choosing the next seed to fuzz.
The results on CGC benchmarks are shown in Figure 7, where
the x-axis represents individual runs (in total 10×180 = 1800)
and the y-axis shows the portion of time spent on scheduling.
We can see that the median overhead is as low as 3%, and most