### Consideration of Geolocation Services and Latency Inflation

Given the infeasibility of certain considerations, we focused our efforts on comparing results obtained using six different commercial IP geolocation services, as well as a location computed by majority vote. We calculated latency inflation for router-path, minimum ping, and total time using each of these seven sets of IP geolocations (Fig. 2(b)).

As expected, router-path latency (blue) is most susceptible to differences in IP geolocation, as it depends on the geolocation of not only the Web server but also each router along the path. Despite this, all six median inflation values fall within the range of 1.9–2.4 times. The differences in results for minimum ping time (red) and total time (green) are much smaller. Even the 95th-percentile values for inflation in minimum ping time lie within 10.4–12.0 times, with medians between 3.0–3.1 times. For median inflation in total time, the values range from 35.5–38.0 times, though variation at higher percentiles is larger. Overall, our conclusions, particularly regarding median values, are robust against the significant differences in geolocations provided by these services. However, without ground truth, we cannot account for systematic errors that may affect all geolocation services. Throughout our analysis, except in Fig. 2(b), we use the majority vote geolocation.

### Impact of Client-Server Distances on Latency Inflation

Small client-server distances can cause a small absolute latency increase to translate into a large inflation over c-latency. Geolocation errors can be more pronounced at short distances. When we restricted our analysis to connections with client-server distances above 100 km, 500 km, and 1000 km, we found that the median inflations were relatively close: 35.5, 33.7, and 31.9 times, respectively. This indicates that large inflations are not solely caused by short distances, and even at long distances, significant inflation persists. Section 3.5 and Fig. 3(a) provide a more detailed discussion on the relationship between latency inflation and client-server distances (or c-latency) and locations.

### Results Across Page Sizes

In our experiments, we fetched only the HTML for the landing pages of websites, some of which exceeded 1 MB. However, most pages were much smaller, with a median size of 67 KB. To analyze variations in our results across page sizes, we binned pages into 1 KB buckets and computed the median inflation for each latency component across each bucket. While the median inflation in minimum ping time showed little variation, inflation in TCP transfer time increased linearly with page size, leading to an increase in total fetch time.

We also examined latency inflation for a narrow range of web page sizes around the median, using pages within 10% of the median size of 67 KB. These pages comprised approximately 7% of our dataset. The results of this analysis were similar to the overall results in Fig. 1(b), with expected differences in transfer time (8% smaller) and total time (5% smaller). The request-response time was 10% larger, while other components of inflation were within 1% of the corresponding values in Fig. 1(b).

### Results Across Geographies

We fetched pages from 138 countries using 81 unique PlanetLab locations, resulting in a wide spread of pairwise c-latencies. The median c-latency was 47 ms, with 5th and 95th percentiles at 2 ms and 101 ms, respectively. Similar to our analysis across page sizes, we analyzed latency inflation in router-path latency, minimum ping time, and total time across c-latencies (Fig. 3(a)).

An interesting feature of these results is the inflation bump around a c-latency of 30 ms. This is due to some countries having more circuitous connectivity than average. For example, c-latencies from the Eastern US to Portugal are around 30 ms, but transatlantic routes often go through Northern Europe, potentially incurring significant path 'stretch.' The differences are largely due to inflation at the lowest layers, as shown by the inflation in minimum ping and total time following the inflation in router-path latency.

### Geographically Balanced Analysis

To compare measurements from a geographically balanced set of client locations, we selected 20 PlanetLab hosts such that no two were within 5° of longitude of each other. We then analyzed requests from these PlanetLab clients to web servers in each country. Figure 3(b) shows the median inflation in router-path latency, minimum ping time, DNS, and total time across seven countries with 5,000+ connections. The median c-latencies from these selected PlanetLab hosts to each of these seven countries ranged from 48–55 ms, with Japan being an exception at 12 ms. Most latencies were consistent across geographies, except for DNS and total time in Japan, likely due to DNS resolvers being further away than the web servers.

### The Role of Congestion

Figure 1(b) shows that TCP transfer time is more than 10 times inflated over c-latency. We considered whether packet losses or large packet delays and delay variations might be responsible for poor TCP performance, exacerbated by oversized and congested router buffers—a condition known as bufferbloat.

In addition to fetching the HTML for the landing page, we sent 30 pings from the client to the server's address. We found that variation in ping times was small: the second-longest ping time was only 1.1% larger than the minimum ping time in the median. Even the TCP handshake time was only 1.6% larger than the minimum ping time in the median. Using tcpdump at PlanetLab clients, we analyzed the inter-arrival times of packets. More than 92% of the connections experienced no packet loss (estimated as packets reordered by more than 3 ms). These results are not surprising, as PlanetLab nodes are well-connected, university-based infrastructure, and likely do not experience similar congestion and last-mile latency issues as typical end-user systems.

### End-User Measurements

To complement our PlanetLab measurements, we present results from three sets of measurements from the real edge of the network.

#### Client Connections to a CDN

For a closer look at congestion, we examined RTTs in a sample of TCP connection handshakes between the servers of a large CDN and clients (end users) over a 24-hour period, passively logged at the CDN. We excluded server-client pairs with minimum latencies less than 3 ms, as these 'clients' are often proxy servers in data centers rather than end users.

To evaluate the impact of congestion, we examined variations across time-of-day and within short periods for the same server-client pairs. We discarded server-client pairs without repeat measurements and only looked at pairs in the same timezone. Server locations were provided by the CDN, and clients were geolocated using a commercial geolocation service. We included results for a few geographies with a large number of measurements after these restrictions. We binned all RTT measurements into 12 2-hour periods, separately for each country, and aggregated results over these bins.

**Time-of-Day Latency Variations Across Bins:**
We selected server-client pairs with at least one RTT measurement in each of the twelve bins. For pairs with multiple RTTs within a bin, we used the median RTT as representative, discarding other measurements. This left us with the same number of samples between the same host-pairs in all bins. Figure 4(a) shows the 90th percentile of RTTs in each 2-hour bin for five timezones. Median latency across our aggregate varied little throughout the day, with most timezones seeing no more than 3 ms of variation. The 90th percentile in each bin (Fig. 4(a)) showed similar trends, although with larger variations. In Great Britain, RTTs were higher in the evening, suggesting possible congestion. However, across other timezones, we saw no such effect.

**Transient Latency Variations Within Bins:**
To investigate transient congestion, we did not limit ourselves to the same set of host-pairs across all bins. Within each bin, only data from host-pairs with multiple measurements inside that time period was included. For each host-pair in each bin, we calculated the maximum change in RTT (Δmax)—the difference between the maximum and minimum RTT between the host-pair in that time period. We then computed the median Δmax across host-pairs within each bin. The variation within bins (in Fig. 4(b)) was larger than variations across median latencies across the day, e.g., for US (CST), the median Δmax was as large as 9 ms in peak hours. Great Britain continued to show exceptionally large latency variations, with a Δmax of approximately 25 ms at the peak and large variations across the day. In summary, network-wide latency increases in peak hours were largely limited to one geography (GB). However, individual flows sometimes experienced a few additional milliseconds of latency.

#### MOOC-Recruited End Users

678 students in a Massive Open Online Course (MOOC) run by two of the authors volunteered to run experiments. The experiments were identical to our PlanetLab experiments but performed with a smaller list of web pages. Each volunteer fetched (only the HTML of) 50 pages, with a fixed set of 25 pages for all participants and another 25 chosen randomly from a handpicked, safe set of 100 URLs. We deliberately chose a small number of websites so that each volunteer could make an informed decision to participate. We also asked each volunteer to provide their location and various characteristics of their internet service, such as download speed and connection type.

A total of 24,784 pages were fetched in these experiments. The latency inflation measured was much larger than in our PlanetLab dataset—even after filtering out connections between clients and servers within 100 km, the total fetch time was 66 times inflated in the median. One reason for this significantly larger latency inflation is the over-representation of shopping and news websites in the handpicked URLs, resulting in larger HTML pages, with a median fetch size of 148 KB. To investigate further, we computed results over the same set of pages by fetching them from PlanetLab. Over this set, with the same filtering, median inflation in total fetch time was 49.4 times, still smaller than the measurements from the volunteer systems.

Another factor causing this difference is the larger latency inflation in minimum ping time: 4.1 times in the median over the volunteer-runs, compared to 3 times over PlanetLab (over this set of URLs). If each RTT is longer, the total fetch time will also be longer. Both numbers differed by roughly a factor of 4/3.

One possible reason for larger inflation in minimum ping time in the end-user experiments is the connection type of the user, affecting the last mile latency. Although our data is small, we observed that the lowest median inflation (3.76) was over connections described as Company/University networks, while the worst median inflations were for mobile and DSL connections, with minimum ping time inflated 5.4× and 5.2×, respectively, in the median.

#### RIPE Atlas

So far, we have limited ourselves to client-server connections where the server belongs to a popular web service. In this section, we describe our findings using RIPE Atlas, a global network of probes that measure internet connectivity and performance.