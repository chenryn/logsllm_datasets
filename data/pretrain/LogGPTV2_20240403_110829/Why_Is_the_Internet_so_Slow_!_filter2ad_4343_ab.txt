consideration appears infeasible. Thus, we focused our eﬀorts on comparing the
results we obtained by using 6 diﬀerent commercial IP geolocation services, as
well as a location computed as their majority vote. We computed latency inﬂa-
tion in router-path, minimum ping, and total time using each of these 7 sets of IP
geolocations (Fig. 2(b)). As we might expect, router-path latency (blue) is most
178
I.N. Bozkurt et al.
susceptible to diﬀerences in IP geolocation—the result there depends on geolo-
cating not only the Web server, but also each router along the path. Even so, all
6 median inﬂation values are in the 1.9–2.4 times range. Diﬀerences in results
for minimum ping time (red) and total time (green) are much smaller. Even the
95th-percentile values for inﬂation in minimum ping time all lie within 10.4–12.0
times, while the medians lie within 3.0–3.1 times. The results for median inﬂa-
tion in total time all lie between 35.5–38.0 times, but variation at the higher
percentiles is larger. Thus, largely, our conclusions, particularly with regards to
median values are robust against the signiﬁcant diﬀerences in the geolocations
provided by these services. Needless to say, we cannot, without ground truth,
account for systematic errors that may impact all geolocation services. Except
in Fig. 2(b), we use the majority vote geolocation throughout.
On a related note, small client-server distances can cause a small absolute
latency increase to translate into a large inﬂation over c-latency; eﬀect of geolo-
cation errors can also be more pronounced at short distances. When we restricted
our analysis to connections with client-server distances above 100 km, 500 km and
1000 km, we found that the median inﬂations are relatively close to each other,
being 35.5, 33.7 and 31.9 respectively. So, large inﬂations are not just caused
by short distances, and even after limiting ourselves to connections at long dis-
tances we observe signiﬁcant inﬂation. Section 3.5 (and Fig. 3(a)) discusses in
more detail on the relationship between latency inﬂation and client-server dis-
tances (equivalently, c-latency) and locations.
3.4 Results Across Page Sizes
While we fetch only the HTML for the landing pages of Web sites in our experi-
ments, some of these are still larger than 1 MB. Most pages, however, are much
smaller, with the median being 67 KB. To analyze variations in our results across
page sizes, we binned pages into 1 KB buckets, and computed the median inﬂa-
tion for each latency component across each bucket. While the median inﬂation
in minimum ping time shows little variation, inﬂation in TCP transfer time
increases over page sizes in an expected linear fashion, also causing an increase
in total fetch time.
We also examine latency inﬂation in a narrow range of Web page sizes around
the median, using pages within 10% of the median size of 67 KB. These pages
comprise roughly 7% of our data set. The results of this analysis are similar
to the overall results in Fig. 1(b), with expected diﬀerences in the transfer time
(8% smaller) and total time (5% smaller). The request-response time is 10%
larger. Other components of inﬂation are within 1% of the corresponding values
in Fig. 1(b).
3.5 Results Across Geographies
We fetch pages in 138 countries from 81 unique PlanetLab locations, leading
to a wide spread in the pairwise c-latencies observed across these connections.
The median c-latency is 47 ms, with 5th and 95th percentiles being 2 ms and
Why is the Internet so slow?!
179
101 ms respectively. In a manner similar to our analysis across page sizes, we
also analyzed latency inﬂation in router-path latency, minimum ping time, and
total time across c-latencies (Fig. 3(a)).
An interesting feature of these results is the inﬂation bump around a
c-latency of 30 ms. It turns out that some countries connectivity to which may
be more circuitous than average, are over-represented at these distances in our
data. For instance, c-latencies from the Eastern US to Portugal are in the 30
ms vicinity, but all transatlantic connectivity hits Northern Europe, from where
routes may go through the ocean or land southward to Portugal, thus incurring
signiﬁcant path ‘stretch’. That the diﬀerences are largely due to inﬂation at the
lowest layers is also borne out by the inﬂation in minimum ping and total time
following the inﬂation in the router-path latency.
Fig. 3. Inﬂation in router-path latency, minimum ping time, and total time: (a) as a
function of c-latency; and (b) as a function of Web server country.
An encouraging observation from Fig. 3(a) is that the inﬂation in minimum
ping and total time follows the inﬂation in the router-path latency. Thus, despite
the router-path latency estimation containing multiple approximations (omitting
routers that did not respond to traceroutes or we could not geolocate, as well as
paths between successive routers themselves potentially being circuitous), it is
a useful quantity to measure.
To compare measurements from a geographically balanced set of client loca-
tions, we selected 20 PlanetLab hosts such that no two were within 5◦ of longi-
tude of each other. Then we looked at requests from these PlanetLab clients to
Web servers in each country. Figure 3(b) shows the median inﬂation in router-
path latency, minimum ping time, DNS, and total time across each of the 7
countries for which we had 5, 000+ connections. The median c-latencies from
these selected PlanetLab hosts to each of these 7 countries all lie in the 48–55 ms
range, with the exception of Japan (12 ms). Most of the latencies are fairly con-
sistent across geographies, with the exception of DNS and total time for Japan.
We observed that roughly half of the requests to Web servers in Japan come from
two PlanetLab nodes in Japan, and it is likely that DNS resolvers are further
away than the Web servers causing the larger inﬂation.
180
I.N. Bozkurt et al.
3.6 The Role of Congestion
Figure 1(b) shows that TCP transfer time is more than 10 times inﬂated over
c-latency. It is worth considering whether packet losses or large packet delays
and delay variations are to blame for poor TCP performance. Oversized and
congested router buﬀers on the path may exacerbate such conditions—a situation
referred to as buﬀerbloat.
In addition to fetching the HTML for the landing page, for each connection,
we also sent 30 pings from the client to the server’s address. We found that
variation in ping times is small: the 2nd-longest ping time is only 1.1% larger
than the minimum ping time in the median. While pings (using ICMP) might use
queues separate from Web traﬃc, even the TCP handshake time is only 1.6%
larger than the minimum ping time in the median. We also used tcpdump at
PlanetLab clients to analyze the inter-arrival times of packets. More than 92%
of the connections we made experienced no packet loss (estimated as packets
reordered by more than 3 ms). These results are not surprising—PlanetLab nodes
are (largely) well-connected, university-based infrastructure, and likely do not
have similar characteristics in terms of congestion and last-mile latency to typical
end-user systems.
3.7 End-User Measurements
To complement our PlanetLab measurements, in this section we present results
from three sets of measurements from the real edge of the network.
Client Connections to a CDN. For a closer look at congestion, we examined
RTTs in a sample of TCP connection handshakes between the servers of a large
CDN and clients (end users) over a 24-hour time period, passively logged at the
CDN. (Most routes to popular preﬁxes are unlikely to change at this time-scale
in the Internet [24].) We exclude server-client pairs with minimum latencies of
less than 3 ms—‘clients’ in this latency range are often proxy servers in a data
center or colocation facility rather than our intended end users.
To evaluate the impact of congestion, we examine our data for both variations
across time-of-day (perhaps latencies are, as a whole, signiﬁcantly larger in peak
traﬃc hours), and within short periods of time for the same server-client pairs
(perhaps transient congestion for individual connections is a signiﬁcant prob-
lem). Thus, we discard server-client pairs that do not have repeat measurements.
We only look at server-client pairs in the same timezone to simplify the time-
of-day analysis. Server locations were provided to us by the CDN, and clients
were geolocated using a commercial geolocation service. We include results for
a few geographies that have a large number of measurements after these restric-
tions. We bin all RTT measurements into 12 2-hour periods, separately for each
country, and produce results aggregated over these bins.
Time-of-Day Latency Variations Across Bins. We selected server-client
pairs that have at least one RTT measurement in each of the twelve bins. For
Why is the Internet so slow?!
181
Fig. 4. Variations in latencies of client-server pairs grouped into 2-hr windows in dif-
ferent geographic regions: (a) 90th percentile of RTTs of client-server pairs with mea-
surements in each 2-hr window; and (b) medians of maximum change in RTTs (max -
min) in repeat measurements within each time window.
pairs with multiple RTTs within a bin, we use the median RTT as representative,
discarding other measurements. This leaves us with the same number of samples
between the same host-pairs in all bins. Figure 4(a) shows the 90th percentile of
RTTs in each 2-hour bin for each of 5 timezones. For the United States (US),
we show only data for the central (CST) and eastern (EST) timezones, but
the results are similar for the rest. The timezone classiﬁcation is based on the
location of the client; servers can be anywhere in the US and not necessarily
restricted to the same timezone as that of the clients. Median latency across our
aggregate (not shown) varies little across the day, most timezones seeing no more
than 3 ms of variation. The 90th percentile in each bin (Fig. 4(a)) shows similar
trends, although with larger variations. In Great Britain, RTTs are higher in
the evening (and results for a diﬀerent 24-hour period look similar.) It is thus
possible that congestion is in play there, aﬀecting network-wide latencies. But
across other timezones, we see no such eﬀect.
Transient Latency Variations Within Bins. To investigate transient con-
gestion, we do not limit ourselves to measurements across the same set of host-
pairs across all bins. However, within each bin, only data from host-pairs with
multiple measurements inside that time period is included. For each host-pair
in each bin, we calculate the maximum change in RTT (Δmax)—the diﬀerence
between the maximum and minimum RTT between the host-pair in that time
period. We then compute the median Δmax across host-pairs within each bin.
The variation within bins (in Fig. 4(b)) is a bit larger than variations across
median latencies across the day, e.g., for US (CST), the median Δmax is as
large as 9 ms in the peak hours. That Δmax also shows broadly similar time-
of-day trends to median latency is not surprising. Great Britain continues to
show exceptionally large latency variations, with a Δmax (cid:2) 25 ms at the peak,
and also large variations across the day. In summary, in end-user environments,
network-wide latency increases in peak hours were largely limited in our data set
182
I.N. Bozkurt et al.
to one geography (GB). However, individual ﬂows may sometimes experience a
few additional milliseconds of latency.
MOOC-Recruited End Users. 678 students in a Massive Open Online
Course (MOOC) run by two of the authors volunteered to run experiments for
us. The experiments are identical to our PlanetLab experiments, but performed
with a smaller list of Web pages. Each volunteer fetched (only the HTML of) 50
pages, with a ﬁxed set of 25 pages for all the participants and another 25 chosen
randomly from a handpicked, safe, set of 100 URLs. We deliberately chose a
small number of Web sites so that each volunteer could look at the provided
descriptions, and make an informed decision to participate. We also asked each
volunteer to provide their location and various characteristics of their Internet
service such as download speed and connection type.
A total of 24,784 pages were fetched in these experiments. The latency inﬂa-
tion measured in these experiments was much larger than in our PlanetLab data
set—even after ﬁltering out connections between clients and servers within a 100
km distance of each other, we found that total fetch time is 66 times inﬂated
in the median. One reason for this signiﬁcantly larger latency inﬂation is the
over-representation of shopping and news Web sites in the handpicked URLs,
resulting in larger HTML pages, with the median fetch size being 148 KB. To
investigate further, we also computed results over the same set of pages by fetch-
ing them from PlanetLab. Over this set, with the same ﬁltering (client-server
distances of at least 100 km), median inﬂation in total fetch time is 49.4 times.
This is still smaller than the measurements from the volunteer systems.
Another factor causing this diﬀerence is the larger latency inﬂation in mini-
mum ping time: 4.1 times in the median over the volunteer-runs, compared to 3
times over PlanetLab (over this set of URLs). Of course, if each RTT is longer
in this way, the total fetch time will also be longer. In fact, both numbers diﬀer
by roughly a factor of 4/3.
One possible reason of larger inﬂation in minimum ping time in the end-user
experiments is the connection type of the user, aﬀecting the last mile latency.
Even though our data is small, we get a glimpse of the situation when we com-
pare diﬀerent user provided connection types in terms of inﬂation of minimum
ping time over c-latency. The lowest median inﬂation (3.76) is observed over
connections users described as Company/University network, whereas the worst
median inﬂations are observed for mobile and DSL connections, for which min-
imum ping time is inﬂated 5.4× and 5.2× respectively in the median.
RIPE Atlas. So far, we have limited ourselves to client-server connections,
where the server belongs to a popular Web service. In this section, we describe