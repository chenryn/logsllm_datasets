illustrate the differences in how networks with varying numbers
of providers interact with the Internet.
There is an important difference between having more neigh-
bors and more sites. Indeed, an anycast network may have many
sites that all exist within a single, or handful of providers: for ex-
ample, the E root has 234 sites, but announces to only 3 publicly
visible providers. On the other hand, they may have relatively few
sites with a large number of providers at each: the M root has 9
sites, but announces to 43 providers. We argue that these provider
relationships, and which providers receive anycast BGP announce-
ments, are another dimension in the anycast configuration space,
comparable to the deployment of sites.
Single-Hop Paths Next, we consider the AS path lengths seen
in the RouteViews BGP paths. Figure 3(a) considers the percentage
of one-hop AS paths (when grouped by source AS number) that
are seen from RouteViews, for the same set of targets as described
above. As shown, the grouping (many vs. few providers) is evident
in this dataset. We can see a loose correlation between neighbor
count and the proportion of short paths: A, C, H, and I, which
have only 1 provider, have no or very few one-hop paths, D, L, and
M, which have tens of providers, have relatively more, and J, K,
the CDN prefixes, and GDNS, which have hundreds of providers,
have the greatest proportion of short paths in the dataset. This
correlation is not absolute, being heavily dependent on the nature
of the upstream providers, but it holds that, broadly, having more
providers increases the proportion of one-hop AS paths observed.
2Where announcements are multi-homed (i.e., in roots A and J), neighbors are aggre-
gated across each announcing AS.
IMC ’19, October 21–23, 2019, Amsterdam, Netherlands
Stephen McQuistin, Sree Priyanka Uppu, and Marcel Flores
(a) IPv4
(a) IPv4
(b) IPv6
(b) IPv6
Figure 3: Percentage of single-hop AS paths (grouped by
source AS number) as seen from RouteViews for DNS root
servers, the CDN, and GDNS.
Figure 4: Observed hegemony values for DNS root servers
and the CDN. GDNS has no detectable provider hegemonies.
Figure 3(b) plots the same experiment, but with IPv6, where we see
the conclusions hold.
We do not aim to make any claims related to the performance of
such AS paths as a result of their length, but instead to show that
the large number of providers results in differences in the nature
of the paths used by clients. In particular, the shorter paths suggest
many of these providers are topologically valuable, highlighting
the potential impact of announcing (or withholding) an anycast
address to each of the providers. An anycast configuration must
take into consideration that announcing to providers may alter the
catchments or the path taken to the same site.
Hegemony Next, we examine the interaction of these networks
with the larger AS graph. To this end, we consider differences seen
in the AS hegemony between each of the anycast networks. AS
hegemony is a metric that measures the importance of upstream net-
works based on an improved version of betweenness centrality [20].
Hegemony provides a value from 0 to 1 which can be interpreted
as the weighted fraction of paths that cross a given AS. Hegemony
provides such an assessment regardless of whether or not a given
AS is a direct neighbor. It is further possible for a network to have
many neighbors which are often unused, resulting in high hege-
mony for few, or even a single, provider. Here, we compute the
hegemony for the specific anycast prefixes, rather than the entire
AS, to develop a better understanding of the specific reachability
from the Internet. We make this adjustment as anycast operators
may implement different announcement policies on a per-prefix
basis, complicating AS-level results. We use the hegemony values
to determine how dependent each network is on a single, or small
number of, providers.
Figure 4 presents the observed hegemony values for each anycast
network sorted by the number of one-hop AS paths. First, we note
that GDNS does not appear, as it has no upstream networks with
significant hegemony at all, which suggests that their paths are
diverse, and they have no centralized dependencies. We see that,
as with our AS neighbor analysis, there are two broad groups:
the first features at least one upstream network with extremely
high hegemony, indicating that a single provider lies on almost
all observed paths. The many-provider prefixes, however, exhibit
low hegemony values almost across the board, with only 1 many-
provider network showing an AS with hegemony over .5 in the
IPv4 case. The prefix for CDN-4 is available to fewer publicly visible
providers, resulting in increased single-provider hegemony. We also
see that the hegemony values are generally higher in the IPv6 case,
which may be the result of greater dependency on a few major IPv6
providers, as well as a decrease in publicly visible links.
Given knowledge of the operational configuration of each of
these networks, these results are not surprising. The C root, for
example, has a single provider. Therefore, we see a hegemony of
1 for AS174. For the CDN networks, the largest hegemony values
come from transit providers, which, even with significant peering,
account for a large number of inbound paths.
ABCGHFEDCDN-4MCDN-3LKCDN-2JCDN-1Network0.00.20.40.60.81.0HegemonyABCFGHIEDCDN-4CDN-3MCDN-2LKCDN-1JNetwork0.00.20.40.60.81.0HegemonyTaming Anycast in the Wild Internet
IMC ’19, October 21–23, 2019, Amsterdam, Netherlands
AS
A
B
C
D
E
# Probes
# Catchments
307
45
30
24
31
11
7
5
6
6
Table 1: Top North American ASes with 2 or more vantage
points, ranked by traffic volume.
Based on this data, we conclude that for most networks there
exists a correlation between the number of AS neighbors (Figure 2),
the proportion of short paths (Figure 3), and hegemony (Figure 4).
Many-provider anycast networks are likely to have a significant por-
tion of very short paths, and exhibit little to no AS hegemony. This
correlation shows that many-provider networks must be analyzed
and managed differently than their few-provider counterparts: their
BGP-level interaction with the rest of the Internet is observably
different, and their announcement configurations feature both a
richer configuration space, by virtue of the increase in providers
and a wider set of potential impacts. Furthermore, we see the root
servers are split across the many-/few-provider divide, an important
consideration in studies that rely on them.
3 NETWORK IMPACTS
Section 2 showed that many-provider networks interact with the
Internet via a wider variety of links, creating significant potential
for optimization. We emphasize that the presence of these links
is a critical component of the operating model for many of these
networks, offering greater capacity and resilience. These features
are a requirement for many large anycast networks: the benefits
are necessary for day-to-day operations.
In this section we examine the impacts of manipulating anycast
announcement configurations by altering which providers receive
announcements. We aim to understand: (1) how changes in config-
uration affect RTT performance; (2) how RTT changes when the
configuration alters where and how clients connect; and (3) which
types of networks are impacted.
3.1 Experimental Setup
We conduct a large-scale experiment on the anycast CDN network
measured in the previous section. The experimental design is com-
prised of two large-scale conditions (i.e. anycast configurations).
While many upstream networks on the Internet are not indepen-
dent, this approach provides us with a series of examples that
demonstrate the profound impact that announcement configura-
tions can have.
As our first announcement configuration, we provide announce-
ments to a restricted set of transit providers, on the order of 2 per
site. These are service providers that re-announce to other net-
works, extending connectivity. This configuration represents an
approximately minimal set of providers for full connectivity. Under
the second configuration we provide announcements to the same
transit providers as in the previous set and to nearly all available
providers, including peers at Internet Exchange Points (IXPs) and
private interconnects, on the order of hundreds per site. This config-
uration offers an implementation of a many-provider network that
takes advantage of local connections where possible. Each of these
configurations is simultaneously implemented using experimental
announcements at the CDN, making use of all 4 CDN networks
described in the previous section. The announcements are made
using separate anycast blocks specifically configured by the CDN
for the purposes of regular testing and configuration management,
separate from customer operations.
We perform a series of active measurements, taking traceroutes
from approximately 10,000 RIPE Atlas [2] probes around the world.
All measurements were taken during a single day in April 2019
and were directed at CDN DNS names pointing at each of the
above configurations3. We further collect data from a set of passive
measurements taken from beacon traffic collected at the CDN as a
matter of course in CDN operations.
3.2 Grouping Vantage Points
To discuss meaningful changes in RTT performance, we group van-
tage points together. This grouping allows us to identify the broad
path-level changes behind any performance differences. Ideally,
vantage points (VPs) that share fate should be grouped together:
group members should fall within the catchment of a single (but
perhaps different across configurations) anycast site. As shown in
Table 1, using RIPE Atlas probes as our vantage points and group-
ing by AS alone is insufficient: it groups together vantage points
that map to different catchments. To improve the groupings, we
investigate dividing vantage points into further sub-groups.
We consider four sub-grouping functions, each sub-dividing
ASes by: (1) geolocation (country or US state); (2) prefix of the
probe’s public address, as reported by the RIPE Atlas platform; (3)
prefix of the first public hop on a traceroute to a common unicast
target; and (4) prefix of the last hop on the same unicast path. To
measure the impact of a sub-grouping function, we consider two
metrics, similarity and coverage, across groups with more than one
vantage point. To measure overall similarity of a group, we use
a generic vantage point similarity metric, which captures group
similarity independently of the ultimate measurement target. To
begin, we define the Jaccard similarity [24] of probe x with respect
to another probe y as:
d(x,y)→m =
|Px→m ∩ Py→m|
|Px→m ∪ Py→m| ,
(1)
where m is a destination IP, and Px→m is the path from x to m. As
noted in [24], basing similarity upon paths to a single destination
lacks robustness. To address this, we compare the similarity of paths
to a set of destinations, M (where m ∈ M). Here, M is the set of all
destinations within the RIPE Atlas probe’s built-in measurement
set (i.e., those measurements that all probes routinely carry out).
This provides a diverse set of destinations, reducing dependence on
any one destination or routing configuration, without increasing
probe measurement burden. We calculate the similarity for each
probe, sim(x), within group д, as:
sim(x) = median
y∈д
{d(x,y)→M}
(2)
3The endpoints used in these measurements, and their measurement IDs, are available
upon request to research@veriziondigitalmedia.com.
IMC ’19, October 21–23, 2019, Amsterdam, Netherlands
Stephen McQuistin, Sree Priyanka Uppu, and Marcel Flores
Next, we calculate a similarity for each group, д, as:
sim(д) = median
x ∈д
{sim(x)}
Finally, we define the similarity of each sub-grouping function, F,
as:
sim(F) =
д∈G
sim(д)
(3)
(4)
where G contains groups produced by F, and singleton groups have
been removed to limit the impact of anomalous probes. Medians
are used throughout the similarity metric calculation to dampen
the impact of outliers. Beyond similarity, we further compute the
coverage. We define coverage as the number of ASes that have at
least one group after a sub-grouping function has been applied and
singleton groups removed. If a given group function creates too
many singleton groups, we may lose coverage of ASes.
Our results show that sub-grouping by geolocation significantly
increases similarity (+10.0%), with only a slight reduction in cover-
age (−2.2%), compared to grouping by AS alone. All of the other
sub-grouping functions either significantly reduce coverage (probe
prefix and first hop prefix), or do not meaningfully improve simi-
larity (last hop prefix). To validate our choice of sub-grouping by
geolocation, we again consider the ASes listed in Table 1. When
sub-grouped by geolocation, each of the sub-groups listed fall into
the catchment of a single anycast site. Looking beyond these, at all
ASes, sub-grouping by geolocation results in 60% of groups falling
within the catchment of a single site, and 90% of groups within the
catchments of 2 or fewer sites, approximately achieving our goal
of each group having shared fate.
3.3 Vantage Point Selection
In the course of our analysis we consider two primary sources of
information. First, we examine RTTs measured on the server side
via CDN logs. These measurements rely largely on connections
from non-production real user measurement (RUM) beacons from
a broad set of CDN clients. As a result, they can be manipulated
for the purposes of these experiments and pointed at addresses
implementing our configurations. In order to reproducibly geolocate
these clients, we use the MaxMind geolocation database.
While they offer a broad set of real user measurements, these
beacons are only able to provide us with passive end-to-end data
and contain no information about paths. Path information is critical
for providing a robust analysis of both catchments and inbound
routing behaviors. Therefore, we need a further data source that is
able to provide to-the-CDN traceroute information: in this case, we
use RIPE Atlas.
In order to ensure that measurements from RIPE Atlas accurately
portray the behavior of clients, we compare these two sets of van-
tage points. Here, we consider the absolute difference between the
median beacon and median probe values for each group, as we are
attempting to assess the accuracy of the probe-based measurements.
Figure 5 presents a CDF of the RTT difference between the beacons
and RIPE Atlas measurements, for all groups visible in both datasets
that had at least 2 RIPE Atlas probes. We further filter out outlier
beacon measurements, removing all results over 200ms. Here, we
see that 82% of groups have less than 15ms of error, suggesting that
the RIPE Atlas data provides a relatively accurate sense of client
Figure 5: Changes in median RTT between the CDN beacons
and RIPE Atlas probes.
Figure 6: Changes in median RTT (between control and ex-
periment configurations) for all groups.
RTT performance. We present a further analysis in the coverage
provided by RIPE Atlas in Section 5.1.