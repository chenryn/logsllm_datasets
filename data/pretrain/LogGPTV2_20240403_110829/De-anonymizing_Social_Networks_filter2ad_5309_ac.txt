passive attack.
Individual auxiliary information is used in the the “seed
2. Negligible relative to the size of S. For example, in our experiments,
we ﬁnd that between 30 and 150 seeds are sufﬁcient for networks with 105
to 106 members.
178
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:17:29 UTC from IEEE Xplore.  Restrictions apply. 
identiﬁcation” stage of our de-anonymization algorithm
(Section 5).
4.4. Breaching privacy
The notion of what should be considered private varies
from network to network and even from individual to indi-
vidual within the network. To keep our model independent
of the semantics of a particular network, we treat the privacy
policy as a syntactic, exogenous labeling that speciﬁes for
every node attribute, edge, and edge attribute whether it
should be public or private. Formally,
is a function
PP: X ∪ Y × E → {pub, priv}.
it
In this paper, we take an “operational” approach to social-
network privacy by focusing solely on node re-identiﬁcation.
First, it is unclear how to give a meaningful deﬁnition of
social-network privacy that does not make some assumptions
about
the attacker’s strategy and yet yields meaningful
results on real-world data. Second, all currently known
privacy-breaching and privacy-protection algorithms focus
on node re-identiﬁcation. Even edge inference, in order to be
considered a meaningful privacy breach, must include learn-
ing some identifying information about the endpoints and
thus implies node re-identiﬁcation. Third, while anonymity
is by no means sufﬁcient for privacy3, it is clearly necessary.
A re-identiﬁcation algorithm that breaks anonymity is thus
guaranteed to violate any reasonable deﬁnition of privacy, as
long as there are any sensitive attributes at all attached to the
nodes, since the algorithm re-labels the sensitive attributes
with identifying information.
We deﬁne ground truth to be a mapping µG between the
nodes Vaux of the attacker’s auxiliary network and the nodes
Vsan of the target network. Intuitively, a pair of nodes are
mapped to each other if they belong to the same “entity”. If
µG(v) takes the special value ⊥, then there is no mapping
for node v (e.g., if v was not released as part of Vsan).
Further, µG need not map every node in Vsan. This is
important because the overlap between Vsan and Vaux may
be relatively small. We do assume that the mapping is 1-1,
i.e., an entity has at most one node in each network.
Node re-identiﬁcation or re-labeling refers to ﬁnding a
mapping µ between a node in Vaux and a node in Vsan.
Intuitively, Gaux is a labeled graph and Gsan is unlabeled.
Node re-identiﬁcation succeeds on a node vaux ∈ Vaux if
µ(v) = µG(v), and fails otherwise. The latter includes the
case that µ(v) =⊥, µG(v) 6=⊥ and vice versa. Informally,
re-identiﬁcation is recognizing correctly that a given node
in the anonymized network belongs to the same entity as a
node in the attacker’s auxiliary network.
3. For example, suppose that the attacker can map a node in Vaux to
a small set of nodes in Vsan which all have the same value for some
sensitive attribute. Anonymity is preserved (he does not know which of the
nodes corresponds to the target node), yet he still learns the value of his
target’s sensitive attribute.
Deﬁnition 1 (Re-identiﬁcation algorithm): A node
re-
identiﬁcation algorithm takes as input Ssan and Saux and
produces a probabilistic mapping ˜µ: Vsan × (Vaux ∪ {⊥
}) → [0, 1], where ˜µ(vaux, vsan) is the probability that
vaux maps to vsan.
We give such an algorithm in Section 5. Observe that the
algorithm outputs, for each node in Vaux, a set of candidate
nodes in Vsan and a probability distribution over those
nodes reﬂecting the attacker’s imperfect knowledge of the
re-identiﬁcation mapping.
We now deﬁne the class of adversaries who attempt
to breach privacy via re-identiﬁcation. After constructing
the mapping, the adversary updates his knowledge of the
attributes of Saux using the attribute values in Ssan.
Speciﬁcally, he can use the probability distribution over the
candidate nodes to derive a distribution over the attribute
values associated with these nodes. His success is measured
by the precision of his posterior knowledge of the attributes.
Deﬁnition 2 (Mapping adversary): A mapping adversary
corresponding to a probabilistic mapping ˜µ outputs a prob-
ability distribution calculated as follows:
Adv[X, vaux, x] =
Pv∈Vsan,X[v]=x µ(vaux, v)
Pv∈Vsan,X[v]6=⊥ µ(vaux, v)
Adv[Y, uaux, vaux, y] =
Pu,v∈Vsan ,Y [u,v]=y
Pu,v∈Vsan ,Y [u,v]6=⊥
˜µ(uaux,u)˜µ(vaux,v)
˜µ(uaux,u)˜µ(vaux,v)
Because the auxiliary graph need not be a subgraph of
the target graph, the mapping may not be complete, and the
mapping adversary’s posterior knowledge Adv of an attribute
value is only deﬁned for nodes vaux that have actually been
mapped to nodes in the target graph, at least one of which
has a non-null value for this attribute. Formally, Adv is
deﬁned if there is a non-zero number of nodes v ∈ Vsan
such that ˜µ(vaux, v) > 0 and X[v] 6=⊥. Edge attributes are
treated similarly.
The probability of a given node having a particular
attribute value can be computed in other ways, e.g., by
looking only at the most likely mapping. This does not make
a signiﬁcant difference in practice.
We say that privacy of vsan is compromised if, for some
attribute X which takes value x in Ssan and is designated
as “private” by the privacy policy, the adversary’s belief that
X[vaux] = x increases by more than δ, which is a pre-
speciﬁed privacy parameter. For simplicity, we assume that
the privacy policy PP is global, i.e., the attribute is either
public, or private for all nodes (respectively, edges).
Deﬁnition 3 (Privacy breach): For nodes uaux, vaux ∈
Vaux, let µG(uaux) = usan and µG(vaux) = vsan. We
say that the privacy of vsan is breached w.r.t. adversary Adv
and privacy parameter δ if
179
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:17:29 UTC from IEEE Xplore.  Restrictions apply. 
for some attribute X such that PP[X] = priv,
(a)
Adv[X, vaux, x] − Aux[X, vaux, x] > δ where x =
X[vaux], or
(b) for some attribute Y such that PP[Y ] = priv, Adv[Y,
uaux, vaux, y] − Aux[Y, uaux, vaux, y] > δ where y =
Y [uaux, vaux].
Deﬁnition 3 should be viewed as a meta-deﬁnition or a
template, and must be carefully adapted to each instance of
the re-identiﬁcation attack and each concrete attribute. This
involves subjective judgment. For example, did a privacy
breach occur if the the attacker’s conﬁdence increased for
some attributes and decreased for others? Learning common-
sense knowledge from the sanitized network (for example,
that all nodes have fewer than 1000 neighbors) does not in-
tuitively constitute a privacy breach, even though it satisﬁes
Deﬁnition 3 for the “node degree” attribute. Such common-
sense knowledge must be included in the attacker’s Aux.
Then learning it from the sanitized graph does not constitute
a privacy breach.
4.5. Measuring success of an attack
While it
is tempting to quantify de-anonymization of
social networks in terms of the fraction of nodes affected,
this results in a fairly meaningless metric. Consider the
following thought experiment. Given a network G = (V, E),
imagine the network G′ consisting of G augmented with
|V | singleton nodes. Re-identiﬁcation fails on the singletons
because there is no edge information associated with them,
and, therefore, the na¨ıve metric returns half the value on
G′ as it does on G. Intuitively, however, the presence of
singletons should not affect the performance of any de-
anonymization algorithm.
This is not merely hypothetical. In many online networks,
the majority of nodes show little or no observable activity
after account creation. Restricting one’s attention to the giant
connected component does not solve the problem, either,
because extraneous nodes with degree 1 instead of 0 would
have essentially the same (false) impact on na¨ıvely measured
performance.
Instead, we assign a weight to each affected node in
proportion to its importance in the network. Importance is a
subjective notion, but can be approximated by node central-
ity, which is a well-studied concept in sociology that only
recently came to the attention of computer scientists [36],
[16], [47], [3], [40].
There are three groups of centrality measures:
local,
eigenvalue-based and distance-based. Local methods such as
degree centrality consider only the neighbors of the node.
Eigenvalue methods also consider the centrality of each
neighbor, resulting in a convergent recursive computation.
Distance-based measures consider path lengths from a node
to different points in the network. A well-known eigenvalue-
based measure was proposed by Bonacich in [11], while [33]
presents a textbook treatment of centrality.
We ﬁnd that the decision to use a centrality measure at
all, as opposed to a na¨ıve metric such as the raw fraction
of nodes de-anonymized, is much more important than the
actual choice of the measure. We therefore use the simplest
possible measure, degree centrality, where each node is
weighted in proportion to its degree. In a directed graph,
we use the sum of in-degree and out-degree.
There is an additional methodological issue. For a mapped
pair of nodes, should we use the centrality score from the
target graph or the auxiliary graph? It is helpful to go back
to the pathological example that we used to demonstrate the
inadequacy of fraction-based metrics. If either of the nodes
in the mapped pair is a singleton, then the de-anonymization
algorithm clearly has no hope of ﬁnding that pair. Therefore,
we compute the centrality in both graphs and take the
minimum of the two. We believe that
this formulation
captures most closely the spirit of the main question we are
answering in this paper: “what proportion of entities that are
active in a social network and for which non-trivial auxiliary
information is available can be re-identiﬁed?”
Given a probabilistic mapping ˜µ, we say that a (concrete)
mapping is sampled from ˜µ if for each u, µ(u) is sampled
according to ˜µ(u, .).
Deﬁnition 4 (Success of de-anonymization): Let
Vmapped = {v ∈ Vaux : µG(v) 6=⊥}. The success rate
of a de-anonymization algorithm outputting a probabilistic
mapping ˜µ, w.r.t. a centrality measure ν, is the probability
that µ sampled from ˜µ maps a node v to µG(v) if v is
selected according to ν:
Pv∈Vmapped
PR[µ(v) = µG(v)]ν(v)
Pv∈Vmapped
ν(v)
The error rate is the probability that µ maps a node v to
any node other than µG(v):
Pv∈Vmapped
PR[µ(v) 6=⊥ ∧µ(v) 6= µG(v)]ν(v)
Pv∈Vmapped
ν(v)
The probability is taken over the inherent randomness of
the de-anonymization algorithm as well as the sampling of
µ from ˜µ. Note that the error rate includes the possibility
that µG(v) =⊥ and µ(v) 6=⊥.
The above measure only gives a lower bound on privacy
breach because privacy can be violated without complete de-
anonymization. Therefore, if the goal is to protect privacy,
it is not enough to show that this measure is low. It is also
necessary to show that Deﬁnition 3 is not satisﬁed. Observe,
for example, that simply creating k copies of the graph tech-
nically prevents de-anonymization and even satisﬁes na¨ıve
syntactic deﬁnitions such as k-anonymity, while completely
violating any reasonable deﬁnition of privacy.
180
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:17:29 UTC from IEEE Xplore.  Restrictions apply. 
In the other direction, however, breaking Deﬁnition 4 for
a large fraction of nodes—as our algorithm of Section 5
does—is sufﬁcient to break privacy via Deﬁnition 3, as long
some trivial conditions are met: at least one private attribute
is released as part of Xsan, and the adversary possesses
little or no auxiliary information about this attribute.
5. De-anonymization
Our re-identiﬁcation algorithm runs in two stages. First,
the attacker identiﬁes a small number of “seed” nodes which
are present both in the anonymous target graph and the
attacker’s auxiliary graph, and maps them to each other.
The main, propagation stage is a self-reinforcing process in
which the seed mapping is extended to new nodes using only
the topology of the network, and the new mapping is fed
back to the algorithm. The eventual result is a large mapping
between subgraphs of the auxiliary and target networks
which re-identiﬁes all mapped nodes in the latter.
5.1. Seed identiﬁcation
While algorithms for seed identiﬁcation are not our pri-
mary technical contribution, they are a key step in enabling
our overall algorithm to succeed. Here we describe one
possible seed identiﬁcation algorithm. The attacks in [7] can
also be considered seed identiﬁcation algorithms. We brieﬂy
discuss alternatives at the end of Section 6.1.
We assume that the attacker’s individual auxiliary infor-
mation (see Section 4.3) consists of a clique of k nodes
which are present both in the auxiliary and the target graphs.
It is sufﬁcient to know the degree of each of these nodes and
the number of common neighbors for each pair of nodes.
The seed-ﬁnding algorithm takes as inputs (1) the target
graph, (2) k seed nodes in the auxiliary graph, (3) k node-
degree values, (4) (cid:0)k
2(cid:1) pairs of common-neighbor counts,
and (5) error parameter ǫ. The algorithm searches the target
graph for a unique k-clique with matching (within a factor of
1 ± ǫ) node degrees and common-neighbor counts. If found,
the algorithm maps the nodes in the clique to the corre-
sponding nodes in the auxiliary graph; otherwise, failure is
reported.
in k,
While this brute-force search is exponential
in
practice this turns out not to be a problem. First, if the degree
is bounded by d, then the complexity is O(ndk−1). Second,
the running time is heavily input-dependent, and the inputs
with high running time turn out to produce a large number
of matches. Terminating the algorithm as soon as more than
one match is found greatly decreases the running time.
5.2. Propagation
The propagation algorithm takes as input
two graphs
G1 = (V1, E1) and G2 = (V2, E2) and a partial “seed”
181
mapping µS between the two. It outputs a mapping µ.
One may consider probabilistic mappings, but we found it
simpler to focus on deterministic 1-1 mappings µ: V1 → V2.
Intuitively, the algorithm ﬁnds new mappings using the
topological structure of the network and the feedback from
previously constructed mappings. It is robust to mild mod-
iﬁcations of the topology such as those introduced by
sanitization. At each iteration, the algorithm starts with the
accumulated list of mapped pairs between V1 and V2. It picks
an arbitrary unmapped node u in V1 and computes a score
for each unmapped node v in V2, equal to the number of
neighbors of u that have been mapped to neighbors of v. If
the strength of the match (see below) is above a threshold,
the mapping between u and v is added to the list, and the
next iteration starts. There are a few additional details and
heuristics that we describe below.
Eccentricity. Eccentricity is a heuristic deﬁned in [54] in
the context of de-anonymizing databases. It measures how
much an item in a set X “stands out” from the rest, and is
deﬁned as
max(X) − max2(X)
σ(X)
where max and max2 denote the highest and second highest
values, respectively, and σ denotes the standard deviation.
Our algorithm measures the eccentricity of the set of map-
ping scores (between a single node in v1 and each unmapped
node in v2) and rejects the match if the eccentricity score is
below a threshold.
Edge directionality. Recall
that we are dealing with
directed graphs. To compute the mapping score between a
pair of nodes u and v, the algorithm computes two scores–
the ﬁrst based only on the incoming edges of u and v, and
the second based only on the outgoing edges. These scores
are then summed.