title:Procedural Noise Adversarial Examples for Black-Box Attacks on Deep
Convolutional Networks
author:Kenneth T. Co and
Luis Muñoz-Gonz&apos;alez and
Sixte de Maupeou and
Emil C. Lupu
9
1
0
2
v
o
N
3
2
]
R
C
.
s
c
[
4
v
0
7
4
0
0
.
0
1
8
1
:
v
i
X
r
a
Procedural Noise Adversarial Examples for Black-Box A(cid:130)acks
on Deep Convolutional Networks
Kenneth T. Co
Imperial College London
PI:EMAIL
Sixte de Maupeou
Imperial College London
PI:EMAIL
ABSTRACT
Deep Convolutional Networks (DCNs) have been shown to be vul-
nerable to adversarial examples—perturbed inputs speci(cid:128)cally de-
signed to produce intentional errors in the learning algorithms at
test time. Existing input-agnostic adversarial perturbations exhibit
interesting visual pa(cid:138)erns that are currently unexplained. In this
paper, we introduce a structured approach for generating Universal
Adversarial Perturbations (UAPs) with procedural noise functions.
Our approach unveils the systemic vulnerability of popular DCN
models like Inception v3 and YOLO v3, with single noise pa(cid:138)erns
able to fool a model on up to 90% of the dataset. Procedural noise
allows us to generate a distribution of UAPs with high universal eva-
sion rates using only a few parameters. Additionally, we propose
Bayesian optimization to e(cid:129)ciently learn procedural noise param-
eters to construct inexpensive untargeted black-box a(cid:138)acks. We
demonstrate that it can achieve an average of less than 10 queries
per successful a(cid:138)ack, a 100-fold improvement on existing methods.
We further motivate the use of input-agnostic defences to increase
the stability of models to adversarial perturbations. (cid:140)e universality
of our a(cid:138)acks suggests that DCN models may be sensitive to ag-
gregations of low-level class-agnostic features. (cid:140)ese (cid:128)ndings give
insight on the nature of some universal adversarial perturbations
and how they could be generated in other applications.
CCS CONCEPTS
•Computing methodologies → Machine learning; •Security
and privacy → Usability in security and privacy;
KEYWORDS
Adversarial examples; Bayesian optimization; black-box a(cid:138)acks;
computer vision; deep neural networks; procedural noise
1 INTRODUCTION
Advances in computation and machine learning have enabled deep
learning methods to become the favoured algorithms for various
tasks such as computer vision [31], malware detection [64], and
speech recognition [22]. Deep Convolutional Networks (DCN)
achieve human-like or be(cid:138)er performance in some of these appli-
cations. Given their increased use in safety-critical and security
applications such as autonomous vehicles [4, 74, 76], intrusion
detection [28, 29], malicious string detection [65], and facial recog-
nition [39, 70], it is important to ensure that such algorithms are
1
Luis Mu˜noz-Gonz´alez
Imperial College London
PI:EMAIL
Emil C. Lupu
Imperial College London
PI:EMAIL
robust to malicious adversaries. Yet despite the prevalence of neural
networks, their vulnerabilities are not yet fully understood.
It has been shown that machine learning systems are vulnerable
to a(cid:138)acks performed at test time [2, 23, 41, 47, 53]. In particular,
DCNs have been shown to be susceptible to adversarial examples:
inputs indistinguishable from genuine data points but designed to
be misclassi(cid:128)ed by the learning algorithm [73]. As the perturbation
required to fool the learning algorithm is usually small, detecting
adversarial examples is a challenging task. Fig. 1 shows an ad-
versarial example generated with the a(cid:138)ack strategy we propose
in this paper; the perturbed image of a tabby cat is misclassi(cid:128)ed
as a shower curtain. Although we focus on computer vision, this
phenomenon has been shown in other application domains such
as speech processing [9, 11], malware classi(cid:128)cation [19], and rein-
forcement learning [24, 37] among others.
Figure 1: Adversarial example generated with a procedural
noise function. From le(cid:133) to right: original image, adversar-
ial example, and procedural noise (magni(cid:128)ed for visibility).
Below are the classi(cid:128)er’s top 5 output probabilities.
In this paper, we propose a novel approach for generating adver-
sarial examples based on the use of procedural noise functions. Such
functions are commonly used in computer graphics and designed
to be parametrizable, fast, and lightweight [34]. (cid:140)eir primary pur-
pose is to algorithmically generate textures and pa(cid:138)erns on the (cid:131)y.
Procedurally generated noise pa(cid:138)erns have interesting structures
that are visually similar to those in existing universal adversarial
perturbations [30, 44].
We empirically demonstrate that DCNs are fragile to procedural
noise and these act as Universal Adversarial Perturbations (UAPs),
i.e. input-agnostic adversarial perturbations. Our experimental
results on the large-scale ImageNet classi(cid:128)ers show that our pro-
posed black-box a(cid:138)acks can fool classi(cid:128)ers on up to 98.3% of input
examples. (cid:140)e a(cid:138)ack also transfers to the object detection task,
showing that it has an obfuscating e(cid:130)ect on objects against the
YOLO v3 object detector [60]. (cid:140)ese results suggest that large-
scale indiscriminate black-box a(cid:138)acks against DCN-based machine
learning services are not only possible but can be realized at low
computational costs. Our contributions are as follows:
• We show a novel and intuitive vulnerability of DCNs in
computer vision tasks to procedural noise perturbations.
(cid:140)ese functions characterize a distribution of noise pat-
terns with high universal evasion, and universal pertur-
bations optimized on small datasets generalize to datasets
that are 10 to 100 times larger. To our knowledge, this is
the (cid:128)rst model-agnostic black-box generation of universal
adversarial perturbations.
• We propose Bayesian optimization [43, 68] as an e(cid:130)ective
tool to augment black-box a(cid:138)acks. In particular, we show
that it can use our procedural noise to cra(cid:137) inexpensive
universal and input-speci(cid:128)c black-box a(cid:138)acks. It improves
on the query e(cid:129)ciency of random parameter selection by
5-fold and consistently outperforms the popular L-BFGS
optimization algorithm. Against existing query-e(cid:129)cient
black-box a(cid:138)acks, we achieve a 100 times improvement
on the query e(cid:129)ciency while maintaining a competitive
success rate.
• We show evidence that our procedural noise UAPs appear
to exploit low-level features in DCNs, and that this vul-
nerability may be exploited to create universal adversarial
perturbations across applications. We also highlight the
shortcomings of adversarial training and suggest using
more input-agnostic defences to reduce model sensitivity
to adversarial perturbations.
(cid:140)e rest of the paper is structured as follows. In Sect. 2, we
de(cid:128)ne a taxonomy to evaluate evasion a(cid:138)acks. In Sect. 3, we de-
scribe and motivate the use of procedural noise functions. In Sect. 4,
we demonstrate how di(cid:130)erent DCN architectures used in image
classi(cid:128)cation have vulnerabilities to procedural noise. In Sect. 5, we
show how to leverage this vulnerability to create e(cid:129)cient black-box
a(cid:138)acks. In Sect. 6, we analyze how the a(cid:138)ack transfers to the object
detection task and discuss how it can generalize to other applica-
tion domains. In Sect. 7, we explore denoising as a preliminary
countermeasure. Finally, in Sect. 8, we summarize our (cid:128)ndings and
suggest future research directions.
2 ATTACK TAXONOMY
Our study focuses on a(cid:138)acks at test time, also known as evasion
a(cid:136)acks. To determine the viability and impact of a(cid:138)acks in practical
se(cid:138)ings, we categorize them according to three factors: (a) the
generalizability of their perturbations, (b) the access and knowledge
the adversary requires, and (c) the desired output. (cid:140)ese factors
also describe the threat model being considered.
2.1 Generalizability
(cid:140)e generalizability of adversarial perturbations refers to their abil-
ity to apply across a dataset or to other models. Perturbations that
2
generalize are more e(cid:129)cient because they do not need to be re-
computed for new data points or models. (cid:140)eir generalizability can
be described by their transferability and universality.
Input-speci(cid:128)c adversarial perturbations are designed for a spe-
ci(cid:128)c input against a given model, these are neither transferable
or universal. Transferable adversarial perturbations can fool mul-
tiple models [51] when applied to the same input. (cid:140)is property
enhances the strength of the a(cid:138)ack, as the same adversarial input
can degrade the performance of multiple models, and makes pos-
sible black-box a(cid:138)acks through surrogate models. Perturbations
are universal when the same adversarial perturbation can be ap-
plied successfully across a large portion of the input dataset to
fool a classi(cid:128)er [44]. Cross-model universal perturbations are both
transferable and universal, i.e., they generalize across both a large
portion of the inputs and across models. Generating adversarial
perturbations that generalize is suitable and more e(cid:129)cient in at-
tacks that target a large number of data points and models, i.e. for
broad spectrum indiscriminate a(cid:138)acks. In contrast, input-speci(cid:128)c
a(cid:138)acks may be easier to cra(cid:137) when a few speci(cid:128)c data points or
models are targeted or for targeted a(cid:138)acks where the a(cid:138)acker aims
to produce some speci(cid:128)c types of errors.
2.2 Degree of Knowledge
For evasion a(cid:138)acks, we assume that the a(cid:138)acker has access to the
test input and output. Beyond this, the adversary’s knowledge
and capabilities range from no access or knowledge of the targeted
system to complete control of the target model. Accordingly, a(cid:138)acks
can be broadly classi(cid:128)ed as: white-box, grey-box, or black-box [53].
In white-box se(cid:138)ings, the adversary has complete knowledge of
the model architecture, parameters, and training data. (cid:140)is is the
se(cid:138)ing adopted by many existing studies including [10, 18, 32, 40,
45, 73]. In grey-box se(cid:138)ings, the adversary can build a surrogate
model of similar scale and has access to training data similar to that
used to train the target system. (cid:140)is se(cid:138)ing is adopted in transfer
a(cid:138)acks where white-box adversarial examples are generated on a
surrogate model to a(cid:138)ack the targeted model [33, 52]. (cid:140)is approach
can also be adapted for a black-box se(cid:138)ing. For example Papernot
et al. [52] apply a heuristic to generate synthetic data based on
queries to the target classi(cid:128)er, thus removing the requirement for
labelled training data. In a black-box se(cid:138)ing, the adversary has no
knowledge of the target model and no access to surrogate datasets.
(cid:140)e only interaction with the target model is by querying it, this is
o(cid:137)en referred to as an “oracle”.
Given the adversary’s lack of knowledge, black-box a(cid:138)acks rely
heavily on making numerous queries to gain information. (cid:140)is
increases the chances of the a(cid:138)ack being detected. (cid:140)us, the most
dangerous a(cid:138)acks are those that require the least queries and re-
sources. With fewer queries, adversarial perturbations are gener-
ated sooner, costs (when using a paid service) are lower and the
volume of suspicious queries is reduced. Existing black box a(cid:138)acks
like [3, 12] have shown some success with zeroth order optimization
and gradient estimation. However they require tens to hundreds of
thousands of queries on datasets with a large number of features,
as in realistic natural-image dataset like ImageNet [13, 25]. (cid:140)e
most query-e(cid:129)cient method reported so far is a bandit optimization
framework that achieves 92.9% success with an average of 1, 000
queries per image on ImageNet [25].
2.3 Desired Output
(cid:140)e adversary’s goal varies according to the application and the ex-
pected rewards gained from exploiting the system. Usually, a(cid:138)acks
are considered as either targeted or untargeted (indiscriminate).
In targeted a(cid:138)acks the adversary aims for a speci(cid:128)c subset of
inputs to be misclassi(cid:128)ed as their chosen output. In untargeted
a(cid:138)acks, the a(cid:138)acker aims to cause classi(cid:128)cation errors on a subset
of inputs. Both these a(cid:138)acks disrupt the machine learning system
by forcing errors and undermining the model’s reliability.
In multi-class classi(cid:128)cation, targeted a(cid:138)acks are more challeng-
ing due to their speci(cid:128)city, but successful ones allow a greater
degree of manipulation for the a(cid:138)acker. On the other side, untar-
geted a(cid:138)acks are typically easier, as they just have to evade the
“correct” classi(cid:128)cation, and this characteristic is more suited for
broad indiscriminate a(cid:138)acks.
3 PROCEDURAL NOISE
We introduce procedural noise functions as an intuitive and com-
putationally e(cid:129)cient approach to generate adversarial examples
in black-box se(cid:138)ings. Procedural noise functions are algorithmic
techniques used for generating image pa(cid:138)erns, typically used in the
creation of natural details to enhance images in video and graph-
ics production. (cid:140)ey are designed to be fast to evaluate, scale to
large dimensions, and have low memory footprint. (cid:140)ese a(cid:138)ributes
make them desirable for generating computationally inexpensive
perturbations. For a more comprehensive survey on procedural
noise, we refer the reader to Lagae et al. [34].
3.1 Motivation
Existing Universal Adversarial Perturbations (UAPs) generated by
white-box a(cid:138)acks exhibit interesting visual structures that are [44],
as of yet, not fully understood. UAPs are particularly interesting