title:Eyes of a Human, Eyes of a Program: Leveraging Different Views of
the Web for Analysis and Detection
author:Jacopo Corbetta and
Luca Invernizzi and
Christopher Kr&quot;ugel and
Giovanni Vigna
Eyes of a Human, Eyes of a Program:
Leveraging Diﬀerent Views of the Web
for Analysis and Detection
Jacopo Corbetta, Luca Invernizzi, Christopher Kruegel, and Giovanni Vigna
University of California, Santa Barbara
{jacopo,invernizzi,chris,vigna}@cs.ucsb.edu
Abstract. With JavaScript and images at their disposal, web authors
can create content that is immediately understandable to a person, but
is beyond the direct analysis capability of computer programs, including
security tools. Conversely, information can be deceiving for humans even
if unable to fool a program.
In this paper, we explore the discrepancies between user perception
and program perception, using content obfuscation and counterfeit “seal”
images as two simple but representative case studies. In a dataset of
149,700 pages we found that benign pages rarely engage in these practices,
while uncovering hundreds of malicious pages that would be missed by
traditional malware detectors.
We envision that this type of heuristics could be a valuable addition to
existing detection systems. To show this, we have implemented a proof-
of-concept detector that, based solely on a similarity score computed on
our metrics, can already achieve a high precision (95%) and a good recall
(73%).
Keywords: Website analysis, content obfuscation, fraud detection.
1
Introduction
Web pages available on the Internet are visited by two very diﬀerent kinds of
consumers: humans, surﬁng through their web browsers, and computer programs,
such as search engines.
These consumers have dissimilar goals and constraints, which lead to signiﬁ-
cant diﬀerences in how they interpret and interact with web pages. For example,
a large-scale web crawler, optimized for speed, may not run JavaScript, and thus
will not capture dynamically-rendered content: To overcome this issue, search
engines have established practices [13,14,28,47] that web authors should follow
to make content accessible to their non-JavaScript-aware crawlers. In short, (at
least) two diﬀerent views exist for each page, and search engines rely on web
authors to “bridge” the two worlds and make sure a human and a crawler “see”
an equivalent message.
Search engines, however, have a privileged role online: Successful web sites
need their pages to be indexed, so that people can ﬁnd them easily; therefore,
A. Stavrou et al. (Eds.): RAID 2014, LNCS 8688, pp. 130–149, 2014.
c(cid:2) Springer International Publishing Switzerland 2014
Eyes of a Human, Eyes of a Program
131
web authors are highly encouraged to tailor content so that it is easily consum-
able by these programs. On the contrary, tools that look for cybercrime activity
do not beneﬁt from their role, as they have to face web authors that are always
looking for new ways to evade detection. However, even benign web authors can
present visual information to humans that is, intentionally or not, hard to di-
gest for crawling tools, for instance by showing text through a combination of
images and JavaScript code, a Turing-complete language. In the strictest sense,
therefore, only programs with human-like comprehension abilities would be able
to correctly “understand” even benign web pages. Whether or not this happens
in practice, however, is a diﬀerent question.
In this paper, we focus on how cybercriminals exploit the diﬀerences between
humans and detection tools in executing, parsing, and interpreting a web page.
We use two signals in our investigation. The ﬁrst technique we detect is textual
content obfuscation, with which web authors prevent non-JavaScript-aware ana-
lyzers from retrieving portions of the textual content of the page, and present a
diﬀerent message to humans. The second technique we detect is the presence of
fake security seal images: cybercriminals place these seals on their websites in
an attempt to deceive humans (i.e., by purporting their online rouge pharmacy
is “certiﬁed” by a reputable authority), even if a program would never be fooled
by this practice.
We study how these techniques are used in a dataset of 149,700 web pages, con-
taining both benign and malicious pages. Interestingly, we found that benign pages
also make use of content obfuscation for speciﬁc purposes, such as making the har-
vesting of e-mail addresses more diﬃcult. However, with a few heuristics (and a
clustering step to eliminate outliers) we can ﬁnd malicious pages with 94% preci-
sion and 95% recall among the samples that triggered this signal. The fake seal
heuristic, having almost no false positives, found 400 rogue pharmacy websites.
As a proof-of-concept of how these anti-deception heuristics could be a valu-
able addition to many security products, we built a “maliciousness detector”
leveraging signatures extracted exclusively from pages detected by our two heuris-
tics, using the hidden text as an additional hint. While obviously not a complete
anti-fraud or anti-malware solution, our tool automatically pinpointed several
scam campaigns that deceive humans without exploiting any technical vulner-
ability, and would therefore be out of the reach of many traditional malware
detectors, unless they had speciﬁc signatures for them.
Given the importance of scam campaigns and how large exploitation cam-
paigns were found to use content obfuscation, we estimate that our heuristics
could be a valuable addition to many security products, as our proof-of-concept
tool has a high precision (95%) and a good recall (around 73%) when used to
ﬁnd any malicious page in our dataset.
To summarize, our main contributions are:
– We introduce a novel approach to detect content obfuscation, and we study
its legitimate and malicious uses in a large dataset.
– We introduce a novel approach in detecting counterfeited, or just plainly
fake (with no certiﬁcation authority issuing them), certiﬁcation seals.
132
J. Corbetta et al.
– We show that this type of heuristics can be helpful to general security tools,
by introducing a similarity measure and a matching system that expand
their reach.
2 Related Work
To be eﬀective, fraud pages need to deceive twice: like any malicious site, they
need to convince automated analyzers that they are legitimate sites; moreover,
they need to convince humans into falling for the “phish,” a trait that is unique to
this cybercrime branch. To identify these sites, researchers have devised detection
systems that go after both of these deceptions.
Honeyclient Evasion. Researchers have identiﬁed malware/phishing sites
that perform server-side cloaking to prevent honeyclients from reaching the phish-
ing content: only real users get to the phish, whereas honeyclients get delivered
legitimate content. The cloaking may happen on the redirection chain leading
to them [21], or the server hosting them [42]. Other researchers have pinpointed
suspicious URLs by detecting attempts to evade honeyclients analysis, typically
through ﬁngerprinting and obfuscation [18].
Blacklist Evasion. Cybercriminals have also mitigated the eﬃcacy of suc-
cessful detections by churning through a large set of domains and URLs, with
domain ﬂux and URL ﬂuxing [11,26,38,25]. Researchers, in turn, noting that this
behavior is generally associated with malicious sites, have used it as a detection
feature [20,31]. These ﬂuxing infrastructures are also being detected mining their
topology [17], the redirection chains leading to them [24,40,21], and the traﬃc
distributors’ system feeding them a stream of users to exploit [22].
Studies on Human Scamming. Another direction of research concentrates
on why humans get scammed. Sheng et al. have proposed a game [37] that teaches
people about how not to get scammed. Later, demographic studies have shown
that education is eﬀective in reducing the eﬃcacy of scams [36], but it does
not solve the problem alone. Wu et al. show that security toolbars do not help
the users in their assessments [45]. In 2014, Neupane et al. [29] have taken these
studies a step further, using fMRIs to analyze how the brain responds to phishing
sites and browser countermeasures.
Browser Phishing Warnings. Traditional browser solutions to help users
be aware of the phish, such as domain highlighting and phishing warnings, have
shown to be not very eﬀective [23,29]. To better inform users, researchers have
proposed in-browser protection systems. Spoofguard [5] veriﬁes that user sensi-
tive data is not passed to sites with similar sounding domain names and that
contain similar images. AntiPhish [19] tracks sensitive information and informs
the user whenever those are given to any untrusted website. DOMAntiPhish [33]
alerts the user whenever she visits a phishing site with a layout similar to a
trusted website. All these solutions help in preventing that the user is deceived
in trusting a site similar to a known site she used in the past, but they do not
prevent against other categories of scams, such as fake pharmacies and rogue
antiviruses [7,39]. In contrast, our system is able to track advanced, previously
unseen phishing attacks.
Eyes of a Human, Eyes of a Program
133
Content Analysis. The idea of extracting text from images through OCR
has been investigated in the context of e-mail spam [10]. These scams use “salt-
ing” tricks to confuse analyzers while still getting the right message to humans [4].
To counter this, researchers have proposed ways to track concept drift [8], which
spammers use to thwart frequency-based content analysis. Comprehensive stud-
ies on content analysis have been proposed both for spam [30] and phishing
sites [46,50]. Google is also performing phishing detection through content anal-
ysis [44], and researchers have used the search engine’s index to identify scams
campaigns with similar content [16]. In contrast, our system aims to identify the
advanced phishing attacks that evade these content-based solutions, obfuscating
their content to be resilient to static analyzers.
Visual Analysis. Phishing pages have been identiﬁed through image shin-
gling [1], which involves fragmenting screenshots of the phishing sites into small
images, and clustering the sites according to the fraction of identical images.
This solution is attractive because it is resilient to small changes in the phishing
site, as long as the overall template is not altered. Previous solutions involve
clustering according to colors, fonts, and layout [27] to identify phishing sites
visually similar to trusted sites. Hara et al. [15] show that, given a large enough
dataset of phishing sites, it is possible to automatically infer the site they are
mimicking. These solutions are eﬀective as long as the phishing attack is trying
to mimic the aspect of a trusted website, but they do not cover other scam cat-
egories, such as fake pharmacies, dubious online retailers, or rogue antiviruses.
In contrast, our approach uses visual analysis to identify a dead giveaway of a
scam: a fake security seal. Scammers use these seals to claim to be a legitimate
business, even though the company generating these security seals has not as-
sessed the scammers’ business (and often does not even exist). Focusing on these
seals, we can identify scamming sites that do not try to mimic a legitimate site,
but are still eﬀective in deceiving the user.
3 Dataset
Throughout the paper, we will refer to a dataset comprising the home pages of
the 81,000 most popular websites according to the Alexa popularity ranking, as
a baseline of benign pages. We obtained the remaining 68,000 pages from the
Wepawet online analyzer [43,6].
Wepawet receives submissions from a variety of sources, including a large
volume of URLs from automated feeds, both benign and malicious. As such, it
represents a reasonable sample of pages that a security tool would be called to
examine in practice. Notice that we used Wepawet merely as a feed of active
URLs, without considering the results of its analysis.
In particular, we obtained two feeds: The ﬁrst was an archive of 18,700 pages
pre-ﬁltered (via a simple keyword search in the URL) to contain a large number
of fake antivirus (fake AV, [39,32]) pages, so that we could test our heuristics
against this type of scam, regardless of Wepawet’s ability to detect it. The sec-
ond feed consisted of 50,000 submitted URLs, received in real-time so we could
immediately perform our analysis on them.
134
J. Corbetta et al.
Table 1. Reference truth data for the 50,000 received submissions
Page type
Samples Percent
2431 ± 215 4.87 ± 0.43%
Pharmacy scam campaigns
140 ± 52 0.28 ± 0.10%
“Blackhole” exploit kit
47 ± 29 0.09 ± 0.06%
Updated version of the “Blackhole” exploit kit
327 ± 80 0.65 ± 0.16%
Fake video codec scam campaign
196 ± 62 0.39 ± 0.12%
Other pages with questionable content
Total number of malicious or questionable samples 3075 ± 239 6.16 ± 0.48%
46834 ± 241 93.79 ± 0.48%
Total number of benign samples
This last subset will be the basis of our ﬁnal evaluation, and, as expected for
a random selection, it contains a number of common scams, exploit kits, and
a majority (around 94%) of benign samples. We obtained truth data for this
feed through a manual review: Table 1 details our ﬁndings. For obvious time
reasons, we could not examine all 50,000 pages: we opted instead for a random
sample of 3,000 from which we extrapolate the totals on the entire set within a
reasonable margin (Wilson conﬁdence intervals, 85% conﬁdence). Whenever we
present these numbers, we will express them as x ± m, indicating the interval
(x − m, x + m).
We encountered several samples with suspicious characteristics, but for which
a clear benign-malicious verdict would have required a full investigation of the
service provided (i.e., a “proxy service,” found at several URLs under diﬀer-
ent names, and without any stated policy or identiﬁcation). We marked these
samples as “questionable” and have excluded them from benign and malicious
sample counts.
4 Content Obfuscation
As mentioned in the introduction, many automated systems parse web pages:
Examples include organizations performing large-scale crawling of the Internet
or online analyzers that must evaluate the benign or malicious nature of a page
within a certain time limit. These systems should view and “interpret” pages
exactly like a human would, but in practice they may have to compromise accu-
racy in order to save bandwidth (e.g., by not downloading images) or processing
time (e.g., by ignoring JavaScript, not building the actual layout of the page,
not applying style sheets, etc.).
In general, content can be considered obfuscated if it would be easily seen
and interpreted by the average computer user, but would be hard to interpret
without simulating a full browser and the interaction of a human with it.
To reﬁne this deﬁnition, we need to consider how automated crawlers parse
pages. Details can vary a lot, but we can pick a meaningful upper bound on
automated extraction capabilities and use it to diﬀerentiate the two views of a
page. In particular, we conjecture that JavaScript will be a problematic feature
for programs to consider, and one that many will sacriﬁce.
Eyes of a Human, Eyes of a Program
135
Our intuition is motivated by the consideration that analyzing a static page
can be assumed to take a time that is roughly a function of its size, while execut-
ing arbitrary code introduces an unpredictable delay in the analysis. Moreover,
since JavaScript code can interact with other elements on the page, the entire
content must be kept in memory during the analysis (as opposed to discarding
content that has already been consumed).
It is impossible to directly gauge how many web analyzers and crawlers avoid