6. MULTIPATH PDQ
Several recent works [17, 21] show the beneﬁts of multi-
path TCP, ranging from improved reliability to higher net-
work utilization. Motivated by this work, we propose Mul-
tipath PDQ (M-PDQ), which enables a single ﬂow to be
striped across multiple network paths.
When a ﬂow arrives, the M-PDQ sender splits the ﬂow
into multiple subﬂows, and sends a SYN packet for each
subﬂow. To minimize the ﬂow completion time, the M-PDQ
sender periodically shifts the load from the paused subﬂows
to the sending one with the minimal remaining load. To sup-
port M-PDQ, the switch uses ﬂow-level Equal-Cost Multi-
Path (ECMP) to assign subﬂows to paths. The PDQ switch
requires no additional modiﬁcation except ECMP. The M-
PDQ receiver maintains a single shared buﬀer for a multi-
path ﬂow to resequence out-of-order packet arrivals, as done
in Multipath TCP [21].
We illustrate the performance gains of M-PDQ using BCube [13],
a data center topology that allows M-PDQ to exploit the
path diversity between hosts. We implement BCube address-
based routing to derive multiple parallel paths. Using ran-
 0 700 1400 2100 2800 3500 16 32 64 128 256 512 1024 2048 4096Number of Flowsat 99% Appli-cation ThroughputNetwork Size [Number of Servers] in log2 ScaleFat-tree; deadline-constrained ﬂowsPDQ; Pkt LevelPDQ; Flow LevelD3; Pkt LevelD3; Flow LevelRCP; Pkt LevelRCP; Flow Level 0 20 40 60 80 100 120 140 16 32 64 128 256 512 1024 2048 4096Flow CompletionTime [ms]Network Size [Number of Servers] in log2 ScaleFat-tree; deadline-unconstrained ﬂowsPDQ; Pkt LevelPDQ; Flow LevelRCP/D3; Pkt LevelRCP/D3; Flow LevelTCP; Pkt Level 0 20 40 60 80 100 120 140 160 64 256 1024 4096Flow Comple-tion Time [ms]Network Size [#Svr] in log2 ScaleBCube 0 20 40 60 80 100 120 140 160 64 256 1024 4096Flow Comple-tion Time [ms]Network Size [#Svr]in log2 ScaleJellyﬁshPDQ; Pkt LevelPDQ; Flow LevelRCP/D3; Pkt LevelRCP/D3; Flow Level 0 0.2 0.4 0.6 0.8 1 0.25 0.5 1 2 4 8 16 32CumulativeFraction ofFlows [%]RCP's Flow Completion Time / PDQ's Flow Completion TimeFat-treeBCubeJellyﬁsh 0 3 6 9 12 15 0 1 2 3Number of Flowsat 99% ApplicationThroughputPacket Loss Rate atBottlenect Link [%]PDQTCP 1 1.2 1.4 1.6 1.8 2 0 1 2 3Flow CompletionTime [Normalizedto PDQ w/o Loss]Packet Loss Rate atBottleneck Link [%]PDQTCP 0 1 2 3 4 5 6 7UniformParetoTail Index=1.1Flow CompletionTime [ms]PDQ; Perfect Flow Information: 1PDQ; Random Criticality: 2PDQ; Flow Size Estimation: 3RCP: 411223344136(a)
(b)
(c)
Figure 11: Multipath PDQ achieves better performance.
BCube(2,3) with random permutation traﬃc.
(a, b)
deadline-unconstrained, (c) deadline-constrained ﬂows.
dom permutation traﬃc, Figure 11a demonstrates the im-
pact of the system load on ﬂow completion time of M-PDQ.
Here, we split a ﬂow into 3 M-PDQ subﬂows. Under light
loads, M-PDQ can reduce ﬂow completion time by a factor of
two. This happens because M-PDQ exploits more links that
are underutilized or idle than single-path PDQ. As load in-
creases, these advantages are reduced, since even single-path
PDQ can saturate the bandwidth of nearly all links. How-
ever, as shown in Figure 11a, M-PDQ still retains its beneﬁts
because M-PDQ allows a critical ﬂow to have higher sending
rate by utilizing multiple parallel paths. Finally, we ﬁx the
workload at 100% to stress the network (Figures 11b and
11c). We observe that M-PDQ needs about 4 subﬂows to
reach 97% of its full potential. By allowing servers to use all
four interfaces (whereas single-path PDQ can use only one),
M-PDQ provides a signiﬁcant performance improvement.
7. DISCUSSION
Fairness. One could argue the performance gains of PDQ
over other protocols stem from the fact that PDQ unfairly
penalizes less critical ﬂows. Perhaps counter-intuitively, the
performance gain of SJF over fair sharing does not usually
come at the expense of long jobs. An analysis [4] shows that
at least 99% of jobs have a smaller completion time under
SJF than under fair sharing, and this percentage increases
further when the traﬃc load is less than half.9 Our re-
sults further demonstrate that, even in complex data center
networks with thousands of concurrent ﬂows and multiple
bottlenecks, 85 − 95% of PDQ’s ﬂows have a smaller com-
pletion time than RCP, and the worst PDQ ﬂow suﬀers an
inﬂation factor of only 2.57 as compared with RCP (Fig-
ure 8e). Moreover, unfairness might not be a primary con-
cern in data center networks where the network is owned by a
single entity that has full control of ﬂow criticality. However,
if desired, the operator can easily override the ﬂow compara-
tor to achieve a wide range of goals, including fairness. For
example, to prevent starvation, the operator could gradu-
ally increase the criticality of a ﬂow based on its waiting
time. Using a fat-tree topology with 256 servers, Figure 12
demonstrates that this “ﬂow aging” scheme is eﬀective, re-
ducing the worst ﬂow completion time by ∼48%, while the
mean ﬂow completion time increases only 1.7%.
9Assuming a M/G/1 queueing model with heavy-tailed ﬂow
distributions; see [4].
(a)
Figure 12: Aging helps prevent less critical ﬂows from star-
vation and shortens their completion time. The PDQ sender
increases ﬂow criticality by reducing T H by a factor of 2αt,
where α is a parameter that controls the aging rate, and t
is the ﬂow waiting time (in terms of 100 ms). Flow-level
simulation; 128-server fat-tree topology; random permuta-
tion traﬃc.
When ﬂow completion time is not the priority. Flow
completion time is not the best metric for some protocols.
For example, real-time audio and video may require the abil-
ity to stream, or provide a number of ﬂows with a ﬁxed frac-
tion of network capacity. For these applications, protocols
designed for streaming transport may be a better ﬁt. One
can conﬁgure the rate controller (§3.3.3) to slice the network
into PDQ-traﬃc and non-PDQ-traﬃc, and use some other
transport protocol for non-PDQ-traﬃc. In addition, there
are also applications where the receiver may not be able to
process incoming data at the full line rate. In such cases,
sending any rate faster than what receiver can process does
not oﬀer substantial beneﬁts. Assuming the receiver buﬀers
are reasonably small, PDQ will back oﬀ and allocate remain-
ing bandwidth to another ﬂow.
Does preemption in PDQ require rewriting applica-
tions? A preempted ﬂow is paused (brieﬂy), not termi-
nated. From the application’s perspective, it is equivalent
to TCP being slow momentarily; the transport connection
stays open. Applications do not need to be rewritten since
preemption is hidden in the transport layer.
Incentive to game the system. Users are rational and
may have an incentive to improve the completion time of
their own ﬂows by splitting each ﬂow into small ﬂows. While
a similar issue happens to D3, TCP and RCP10, users in
PDQ may have an even greater incentive, since PDQ does
preemption.
It seems plausible to penalize users for hav-
ing a large number of short ﬂows by reducing their ﬂows’
criticality. Developing a speciﬁc scheme remains as future
work.
Deployment. On end hosts, one can implement PDQ by
inserting a shim layer between the IP and the transport lay-
ers. In particular, the sender maintains a set of PDQ vari-
ables, intercepts all calls between IP and transport layer,
attaches and strips oﬀ the PDQ scheduling header11, and
passes the packet segment to IP/transport layer accordingly.
Additionally, the shim layer could provide an API that al-
10In TCP/RCP, users may achieve higher aggregated
throughput by splitting a ﬂow into smaller ﬂows; in D3, users
may request a higher rate than the ﬂow actually needs.
11The 16-byte scheduling header consists of 4 ﬁelds, each oc-
cupying 4 bytes: RH , P H , DH , and T H . The PDQ receiver
adds I S and RT T S to the header by reusing the ﬁelds used
by DH and T H . This is feasible because DH and T H are
used only in the forward path, while I S and RT T S are used
only in the reverse path. Any reasonable hashing that maps
switch ID to 4-byte P H should provide negligible collision
probability.
 0 3 6 9 12 15 0 0.2 0.4 0.6 0.8 1Flow Comple-tion Time [ms]Load [% of Sending Hosts]PDQM-PDQ, 3 subﬂows 0 3 6 9 12 14.3PDQ   2345678Flow CompletionTime [ms]Number of Subﬂows 0 8 16 24 32PDQ   2345678Number of Flowsat 99% Appli-cation ThroughputNumber of Subﬂows 0 30 60 90 120 150 0 2 4 6 8 10Flow CompletionTime [ms]Aging RateFat-tree; deadline-unconstrained ﬂowsRCP/D3; MaxRCP/D3; MeanPDQ; MaxPDQ; Mean137lows applications to specify the deadline and ﬂow size, or
it could avoid the API by estimating ﬂow sizes (§5.6). The
PDQ sender can easily override TCP’s congestion window
size to control the ﬂow sending rate. We note that PDQ
requires only a few more variables per ﬂow on end hosts.
On switches, similar to previous proposals such as D3, a
vendor can implement PDQ by making modiﬁcations to the
switch’s hardware and software. Per-packet operations like
modifying header ﬁelds are already implemented on most
vendors’ hardware (e.g., ASICs), which can be directly used
by our design. The more complex operations like computing
the aggregated ﬂow rate and sorting/updating the ﬂow list
can be implemented in software. We note that PDQ’s per-
packet running time is O(κ) for the top κ ﬂows and O(1)
for the rest of the ﬂows, where κ is a small number of ﬂows
with the highest criticality and can be bounded as in §3.3.1.
The majority of the sending ﬂows’ scheduling headers would
remain unmodiﬁed12 by switches.
8. RELATED WORK
D3: While D3 [20] is a deadline-aware protocol that also
employs explicit rate control like PDQ, it neither resequences
ﬂow transmission order nor preempts ﬂows, resulting in a
substantially diﬀerent ﬂow schedule which serves ﬂows ac-
cording to the order of their arrival. Unfortunately, this
allows ﬂows with large deadlines to hog the bottleneck band-
width, blocking short ﬂows that arrived later.
Fair Sharing: TCP, RCP [10] and DCTCP [3] all emulate
fair sharing, which leads to suboptimal ﬂow completion time.
TCP/RCP with Priority Queueing: One could use
priority queuing at switches and assigning diﬀerent priority
levels to ﬂows based on their deadlines. Previous studies [20]
showed that, using two-level priorities, TCP/RCP with pri-
ority queueing suﬀers from losses and falls behind D3, and
increasing the priority classes to four does not signiﬁcantly
improve performance. This is because ﬂows can have very
diﬀerent deadlines and require a large number of priority
classes, while switches nowadays provide only a small num-
ber of classes, mostly no more than ten.
ATM: One could use ATM to achieve QoS priority con-
trol. However, ATM’s CLP classiﬁes traﬃc into only two
priority levels, while PDQ gives each ﬂow a unique priority.
Moreover, ATM is unable to preempt ﬂows (i.e., new ﬂows
cannot aﬀect existing ones).
DeTail:
In a recent (Oct 2011) technical report, Zats et
al. propose DeTail [23], an in-network multipath-aware con-
gestion management mechanism that reduces the ﬂow com-
pletion time “tail” in datacenter networks. However, it tar-
gets neither mean ﬂow completion time nor the number of
deadline-missing ﬂows. Unlike DeTail which removes the
tail, PDQ can save ∼30% ﬂow completion time on average
(compared with TCP and RCP), reducing the completion
time of almost every ﬂow (e.g., 85%− 95% of the ﬂows, Fig-
ure 8e). We have not attempted a direct comparison due to
the very diﬀerent focus and the recency of this work.
9. CONCLUSION
We proposed PDQ, a ﬂow scheduling protocol designed
to complete ﬂows quickly and meet ﬂow deadlines. PDQ
provides a distributed algorithm to approximate a range
12Until, of course, the ﬂow is preempted or terminated.
of scheduling disciplines based on relative priority of ﬂows,
minimizing mean ﬂow completion time and the number of
deadline-missing ﬂows. We perform extensive packet-level
and ﬂow-level simulation of PDQ and several related works,
leveraging real datacenter workloads and a variety of traf-
ﬁc patterns, network topologies, and network sizes. We
ﬁnd that PDQ provides signiﬁcant advantages over exist-
ing schemes. In particular, PDQ can reduce by ∼30% the
average ﬂow completion time as compared with TCP, RCP
and D3; and can support 3× as many concurrent senders
as D3 while meeting ﬂow deadlines. We also design a mul-
tipath variant of PDQ by splitting a single ﬂow into multi-
ple subﬂows, and demonstrate that M-PDQ achieves further
performance and reliability gains under a variety of settings.
10. REFERENCES
[1] Bro network security monitor. http://www.bro-ids.org.
[2] M. Al-Fares, A. Loukissas, and A. Vahdat. A scalable,
commodity data center network architecture. In SIGCOMM,
2008.
[3] M. Alizadeh, A. Greenberg, D. A. Maltz, J. Padhye, P. Patel,
B. Prabhakar, S. Sengupta, and M. Sridharan. Data center
TCP (DCTCP). In SIGCOMM, 2010.
[4] N. Bansal and M. Harchol-Balter. Analysis of SRPT
scheduling: Investigating unfairness. In SIGMETRICS, 2001.
[5] N. Bansal and M. Harchol-Balter. End-to-end statistical delay
service under GPS and EDF scheduling: A comparison study.
In INFOCOM, 2001.
[6] T. Benson, A. Akella, and D. A. Maltz. Network traﬃc
characteristics of data centers in the wild. In IMC, 2010.
[7] J. Brutlag. Speed matters for Google web search, 2009.
[8] A. R. Curtis, J. C. Mogul, J. Tourrilhes, P. Yalagandula,
P. Sharma, and S. Banerjee. DevoFlow: Scaling ﬂow
management for high-performance networks. In SIGCOMM,
2011.
[9] N. Dukkipati, Y. Ganjali, and R. Zhang-Shen. Typical versus
worst case design in networking. In HotNets, 2005.
[10] N. Dukkipati and N. McKeown. Why ﬂow-completion time is
the right metric for congestion control. SIGCOMM Comput.
Commun. Rev., 2006.
[11] M. R. Garey and D. S. Johnson. Computers and Intractability;
A Guide to the Theory of NP-Completeness. W. H. Freeman
& Co., 1990.
[12] A. Greenberg, J. R. Hamilton, N. Jain, S. Kandula, C. Kim,
P. Lahiri, D. A. Maltz, P. Patel, and S. Sengupta. VL2: A
scalable and ﬂexible data center network. In SIGCOMM, 2009.
[13] C. Guo, G. Lu, D. Li, H. Wu, X. Zhang, Y. Shi, C. Tian,
Y. Zhang, and S. Lu. BCube: A high performance,
server-centric network architecture for modular data centers. In
SIGCOMM, 2009.
[14] C.-Y. Hong, M. Caesar, and P. B. Godfrey. Finishing ﬂows
quickly with preemptive scheduling. Technical report.
http://arxiv.org/abs/1206.2057, 2012.
[15] D. Katabi, M. Handley, and C. Rohrs. Congestion control for
high bandwidth-delay product networks. In SIGCOMM, 2002.
[16] M. L. Pinedo. Scheduling: Theory, Algorithms, and Systems.
Springer, 2nd edition, 2002.
[17] C. Raiciu, S. Barre, C. Pluntke, A. Greenhalgh, D. Wischik,
and M. Handley. Improving datacenter performance and
robustness with multipath TCP. In SIGCOMM, 2011.
[18] A. Singla, C.-Y. Hong, L. Popa, and P. B. Godfrey. Jellyﬁsh:
Networking data centers randomly. In NSDI, 2012.
[19] V. Vasudevan, A. Phanishayee, H. Shah, E. Krevat, D. G.
Andersen, G. R. Ganger, G. A. Gibson, and B. Mueller. Safe
and eﬀective ﬁne-grained TCP retransmissions for datacenter
communication. In SIGCOMM, 2009.
[20] C. Wilson, H. Ballani, T. Karagiannis, and A. Rowstron. Better
never than late: Meeting deadlines in datacenter networks. In
SIGCOMM, 2011.
[21] D. Wischik, C. Raiciu, A. Greenhalgh, and M. Handley. Design,
implementation and evaluation of congestion control for
multipath TCP. In NSDI, 2011.
[22] H. Wu, Z. Feng, C. Guo, and Y. Zhang. ICTCP: Incast
congestion control for TCP in data center networks. In
CoNEXT, 2010.
[23] D. Zats, T. Das, and R. H. Katz. DeTail: Reducing the ﬂow
completion time tail in datacenter networks. Technical report,
Oct 2011.
138