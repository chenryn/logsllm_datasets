system stability (Section 4.3.3) — 10 out of the 39 participants were
requested to come back to the lab and provide new data. They came
back one week and one month after their first visit. Dataset 3 is
used to evaluate random gesture attacks, where we asked another
10 out of the 39 users to perform 30 random gestures, which in-
cluded the numbers from 0 to 9 and the alphabets from A to T . In
Dataset 4, six other participants were asked to take the role of an
attacker and observe how a given victim makes a gesture. Each
attacker observed a victim perform (through recorded video) the
five different gestures and was asked to emulate each of the gestures
30 times. Note that participants performing additional tasks were
compensated accordingly.
In addition to providing hand-gesture data, each participant was
also asked to complete a post-study survey to provide feedback
about their experience with our system and expectations for a
commercial deployment. We provide more details on the post-study
survey in Section 5.
Evaluation Metrics. For any decision made by a classifier, there
are four possible contingencies: (1) accept a legitimate user (true
positive or TP), (2) wrongly accept an illegitimate user (false posi-
tive or FP), (3) reject an illegitimate user (true negative or TN), and
(4) reject a legitimate user (false negative or FN). We adopt the fol-
lowing well-known metrics that are typically used for assessing any
F N
authentication system [47]: a) False Reject Rate (FRR =
F N +T P )
is the probability that the system wrongly identifies a legitimate
F P +T N ) is the likelihood that
user; b) False Accept Rate (FAR =
the system wrongly accepts an illegitimate user; c) Precision (Pr =
T P +F P ) refers to as positive predictive rate; d) Recall (Re = T P
T P
F N +T P )
refers to as the true positive rate (TPR) or sensitivity; e) F-Score
(F1 = 2×Pr×Re
) is the harmonic mean of the precision and recall.
We perform 10 runs and report the average values.
Pr +Re
F P
4.1 Overall Accuracy
We use Dataset 1 to evaluate the overall performance. We consider
data from all the six microphones (as shown in the Figure 12a)
for our evaluation. We will evaluate the impact of the number of
microphones used in Section 4.3.2.
8
SpeakerRespeaker Core v2.0MIC 3MIC 6MIC 1MIC 5MIC 4MIC 2BackFront(a) Z(b) W(d) Check Mark(e) Star(c) X258HandLock: Enabling 2-FA for Smart Home Voice Assistants using Inaudible Acoustic Signal
RAID ’21, October 6–8,2021, San Sebastian, Spain
Table 5: Performance of RF classifier with imbal-
anced/balanced dataset
(using ADASYN upsampling
technique).
Gesture
FRR
Z
W
X
✓
9
Avg.
4.86/3.71
6.19/3.14
4.95/4.10
6.10/3.14
5.33/3.33
5.49/3.49
FAR
1.35/0.76
1.05/1.05
2.14/1.90
0.96/0.29
0.19/0.10
1.14/0.82
Precision
98.62/99.26
98.90/98.95
97.84/98.17
99.00/99.70
99.80/99.91
99.82/99.20
Recall
95.14/96.29
93.81/96.86
95.05/95.90
93.90/96.86
94.67/96.67
94.51/96.51
F-Score
96.85/97.69
96.29/97.83
96.43/96.90
96.3898.22
97.17/98.20
96.62/97.77
Figure 15: Impact of the number of top training features. Us-
ing the top 960 gives us the best performance.
performing the five different gestures. Figure 15 shows that the
average F-Score for different number of top features. The results
improve from 96.97% to 97.67% as we expand the number of top
features from 300 to 960. After that, F-Score seems to plateau. We,
therefore, use the top 960 features as our training features for the
reminder of the evaluations.
4.1.4 Varying Training Size. To make HandLock user friendly, the
enrollment effort for a new user is a critical factor. We consider the
performance of the classifiers in the presence of limited training
samples. For this experiment, we vary the training set size from
10 samples to 40 samples in increments of 5 samples, and test the
remaining samples. For each training set, we also apply ADASYN
to resample the positive samples so that the number of positive
and negative samples are balanced. Figure 16 shows the evaluation
of the TPR with the increasing training set size. The result shows
that as the training set size increases the TPR also rises. However,
we see that after 30 samples per class the average TPR plateaus at
Figure 16: Impact of training set size on TPR. Using 30 train-
ing samples per class is sufficient.
9
Figure 14: F-Score of different classifiers. RF has the best av-
erage F-score across all five gestures.
4.1.1 Different Classifiers. To build a binary classification model,
we randomly select 30 samples out of the 60 samples per gesture
from each user as training set and use the remaining 30 samples as
test set. We randomly label 5 users’ data as negative class and 34
users’ data as positive class. We use all the features in the feature
vector to train our model. For each binary classification model, we
have 30 positive instances and 30×5 negative instances as training
set. We use the remaining 30 positive instances and select six ran-
dom negative instances from the remaining 30 instances from each
user in the negative class (i.e., five users) as test set. We rerun 10
times for each user and calculate the average F-Score for 34 users.
We compare the performance of four classifiers including RF, DT,
SVM and kNN. For the RF classifier, we use the Bagging algorithm
and test different numbers of trees ranging from 60 to 240, and select
the best number as 120. For DT, we select the maximum number
of splits as 7. To generate each single binary classification model
in SVM, we use the implementation of SVDE with 10-fold cross
validation in libSVM [15] and chose the best complexity parameter
for Radial Basis Function (RBF) through grid Search. To select the
number of neighbors for kNN, we run tests with k ranging from
1 to 10 and find the best performance when k=5. Figure 14 shows
the F-Score of the four classifiers covering five gestures. RF had the
best average F-score across the different gestures. We, therefore,
will use RF for the rest of our evaluations.
Imbalanced data vs. Balanced data. In Section 3.5, we dis-
4.1.2
cussed the challenges that a class imbalance might impose. We test
two widely used upsampling algorithms including SMOTE [17] and
ADASYN [30]. ADASYN shows a better performance than SMOTE
on our dataset. We, therefore, adopt ADASYN to upsample the
positive class instances. Table 5 presents the overall performance
of HandLock while using RF with and without upsampling. After
balancing the positive class, the FRR decreased from 5.49% to 3.49%
and F-Score improved from 96.62% to 97.77%, while the average
FAR decreased from 1.14% to 0.82%.
4.1.3 Varying Training Features. To evaluate the impact of the
number of top features, we apply ADASYN to resample each 30
positive instances. We then use the feature selection library named
FEAST toolbox [5] and use the Joint Mutual Information criterion
(JMI) to determine the top features. We next vary the number of
top features from 300 to 1200 in increments of 60 and compute
the F-Score. We repeat the experiment 10 times for each subset of
the top features, while considering data from all 34 participants
F-Score (%)RFDTSVMKNN300400500600700800900100011001200Number of training features949596979899100F-Score10152025303540Training samples80859095100TPR (%)259RAID ’21, October 6–8,2021, San Sebastian, Spain
Shaohu Zhang and Anupam Das
is close to 6.2%, followed by the ‘X’ with 5.14%. However, the ‘9’
has the lowest FAR of 3.52%. The most probable reason for this is
that simple gestures such as ‘Z’ and ‘X’ are simple and more likely
to be properly emulated, whereas the ‘9’ gesture is more complex
and less prone to emulation attack.
We also test if other users’ same gestures can be used to bypass
HandLock. We select 30 samples from each of the other 33 users
(i.e., 990 test samples) excluding the user whose samples are used
to train a model, to test the robustness of HandLock. The FAR is
7.17%, 6.68%, 5.28%, 4.13%, and 3.76% for ‘Z’, ‘W ’, ‘X’, ‘✓’, and ‘9’,
respectively. Thus, we see similar level of resiliency against mimicry
attacks. Furthermore, by limiting authentication attempts to three,
we can drastically thwart adversarial attacks.
4.2.3 Audio Replay Attack. To emulate replay attack, we select a
high performance UMA-8 USB microphone array [10], which can
record audio at 48kHz sampling rate. We placed the microphone
array close to the target VA at a distance of 20 cm while a user
was performing ‘Z’ gestures. Next to we use a Sony SRS-X5 louder
speaker [9] to replay the recorded gesture session at 70 dB from
a very close distance of 5 cm from the VA to give the attacker the
best chance of authentication. We collected 60 replayed gesture
samples and tested the samples using our original trained model.
HandLock achieves a FAR of 3.33%. This suggests that HandLock is
robust against replay attacks where the attacker has the capability
to record authentication sessions from a very close distance.
4.3 Sensitivity Analysis
4.3.1 Multiple users settings. To study the impact of the number
of users among which HandLock should distinguish, we consider
two enrollment scenarios: 1) different users enroll with the same
single gesture (MUSG); 2) different users enroll with different ges-
tures (MUMG). We repeat the same experiments as described in
Section 3.5 for four different values of enrolled users, ranging from
U = 2 to U = 5, in these two scenarios. We use the highest number
of users as U = 5 because more than 96% households in the US
have less than 6 members according to the United States Census
Bureau [2]. To calculate the overall accuracy of HandLock for each
gesture, we randomly selected 5 users out of 39 participants as neg-
ative class, and each of the remaining N= 34 users as positive class
with unique positive labels. We test each user with 30 samples as
positive class and 30 samples from the negative class and calculate
the average TAR and FAR for K= 5 gestures across all users.
In enrollment scenario 1, we repeat the experiment 5 times,
N!U!(N−U)! times. The
where each experiment runs K.NCU = K
blue bar in Figure 18 show the TPRs. We see that the average
accuracy of HandLock reduces as the number of enrolled users in-
creases. Nonetheless, we observe that the average TPR of HandLock
is above 90% for 5 users. In enrollment scenario 2, the number of
negative training samples vary with the number of unique gestures
the enrolled users use. For example, if K= 2 users enroll with two
different gestures, the total training samples for negative class will
be 2 × 30 × 5, which includes 30 samples per gesture from five
users representing the negative class. Consequently, we run each
experiment NPU =
(N−U)! times. We again repeat each experi-
ment 5 times. The red bar in Figure 18 show the TPRs. The result
N!
Figure 17: FAR under both random guess and mimicry at-
tack.
around 96%. This suggests that we do not need too many training
samples to construct a good predictive model.
4.1.5 Different Negative Dataset. To analyze the impact of the se-
lection of negative samples on classification, we randomly selected
5 users out of the 39 participants as representative of the negative
classes while the remaining 34 users as positive class. We test the
model against ‘Z’ gesture and repeat the whole process 20 times.
We find the average TPR to be 94.94 with a 95% confidence interval
of [94.41, 95.57] (while the average the F-Score is 96.53). This result
suggests that our approach is not dependent on the selection of
negative samples and we can easily bootstrap our approach with
any negative samples.
4.1.6 Expected Number of Attempts. The expected number of at-
tempts required to authentic a user is another important usability
factor. We record the number of attempts needed to successfully
authenticate all gestures cross all users. The overall average number
of attempts required is 1.05 ± 0.24. With two attempts, HandLock
can achieve a TPR of 99.35%, while the accuracy is 99.91% with three
attempts. Thus, we can limit to three attempts to thwart attacks and
fall back on app based authentication.
4.2 Resilience to Attacks
To evaluate the resilience to attacks, we use 30 samples from each
user as training set. We apply ADASYN to upsample each 30 positive
instances and select the top 960 features to train a RF model (this
constitutes the best configurations as described in the previous
section).
4.2.1 Random Gesture Attack. We use Dataset 3 (see Table 4 for
details) to assess the system’s resilience to random gesture attack.
We test all 300 random gestures collected from 10 users to authenti-
cate HandLock against each user’s binary classification model. We
test 10 times for each user model and calculate the average rate
of recognizing the gesture for the enrolled user. The blue bar in
Figure 17 shows the average FAR of random gesture attack for each
gesture, which is 2.61%, 2.77%, 2.84%, 3.11%, and 3.26% for ‘Z’, ‘W ’,
‘X’, ‘✓’, and star (‘9’), respectively.
4.2.2 Gesture Mimicry Attack. We use Dataset 4 (see Table 4 for
details) to evaluate the system’s resilience to gesture mimicry attack.
Here, six attackers individually mimic the gesture of a given victim
for all five gestures. Figure 17 depicts the average FAR of each
gesture under this attack setting. Overall, the FAR for the ‘Z’ gesture
10
FAR (%)Random GestureGesture Mimicry260HandLock: Enabling 2-FA for Smart Home Voice Assistants using Inaudible Acoustic Signal
RAID ’21, October 6–8,2021, San Sebastian, Spain
Figure 18: Impact of the number of users enrolled. As the
number of enrolled user increases, the TPR slightly drops.
Figure 19: Impact of the number of microphones used. Com-
bining multiple microphones results in improved accuracy,