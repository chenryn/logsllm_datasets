A large portion of our learning consists of observations and user 
feedback  about  the  process  of  designing,  building,  and  carrying 
out  an  experiment.  Experimenters  typically  work  in  a  high-level 
workflow  that  consists  of:  initial  design  and  construction  of  an 
experimental  apparatus;  exploratory  operation  of  the  apparatus 
during iterative construction; initial full test runs of operating the 
apparatus;  reviewing  and  analyzing  results  of  test  runs;  iterative 
re-working of the apparatus, towards a fully satisfactory apparatus 
that repeatably operates as expected, and generates output such as 
log  data,  network  traces,  etc.;  analysis  of  output  data  to  create 
experimental  results.  We  refer 
the 
“experiment lifecycle.” Our goal is to change this lifecycle from a 
manual  human  intensive  ad  hoc  set  of  processes  to  a  highly 
automated set of processes tied semantically to an experimenter’s 
model and exploration motivations. 
3.1  Lesson Learned: Experiment 
Construction Considered Difficult 
DETER’s  original 
for  construction  of  experimental 
apparatus were inherited from Emulab [11], the base technology 
of the original DETER testbed. These tools provided sophisticated 
but  low-level  capabilities  for  managing  the  physical  computing 
and  network  resources  of  the  testbed,  and  using  them  to  create 
emulated networks within which an experimenter’s activity took 
place. For our original set of EMIST researchers, this toolset was 
useful, because they were experienced researchers who valued the 
“expert  mode”  in  which  every  detail  of  a  test  network  could  be 
specified. In addition, many of these early users were familiar in 
concept or in experience with other network testbeds. 
However,  we  quickly  confirmed  that  the  “expert  mode  only” 
approach was limiting for many of our researchers, some of whom 
were  less  concerned  with  network-centric  security  research,  and 
more  oriented  toward  security  research  that  did  not  depend 
critically  on  an  exactly  specified  network  environment.  Novice 
DeterLab experimenters with modest research experience faced a 
steep  curve  to  learn  how  to  create  an  emulated  network  of  low 
complexity,  but  useful  for 
testing.  For  very  experienced 
cybersecurity  researchers  starting  work  in  DeterLab,  there  was 
also a steep curve to learn how to create an emulated network of 
moderate complexity and realism sufficient for their work. 
One  reason  for  the  complexity  of  network  definition  lay  in  the 
typical  first  step,  preparing  a  file  containing  expressions  in  a 
specification  language  to  define  each  individual  network  node. 
From “toy network” examples [12] one can extrapolate the level 
of  detail  required  to  specify  non-trivial  network  topologies.  In 
addition  to  the  topology,  each  node  may  need  to  be  assigned 
certain  properties  so  that,  e.g.,  some  nodes  may  serve  as 
background  traffic  generators  or  traffic  delay  nodes.    Following 
topology  definition  and  attribute  specification,  there  are  further 
steps: nodes acting as hosts need to be loaded with a boot image 
of OS and application software, often followed by the addition of 
other software, such as experiment-specific software, or packages 
for  monitoring  and  logging  host  or  network  activity.  All  nodes 
support 
turned 
to 
must  be  network-configured  with  network  interface  devices,  IP 
address, DNS and default route settings, etc. 
In addition to the level of effort required, we also had a concern 
about methodology – in most cases, each experimenter had to do 
almost  all  the  above  activities,  with  very  little  leverage  of 
previous experimenters’ work, other than perhaps using published 
network  definition  files  as  a  model  or  template.  Our  own  work 
[13] to build a reference case of a DDOS experiment, designed for 
re-use  and  extension  by  others,  was  instructive  and  useful,  but 
also  served  to  highlight  the  large  amount  of  required  detail  that 
was not central to some researchers’ work, but was nevertheless 
pre-requisite to a functioning experiment. 
In  other  words,  the  experiment  definition  methodology  lacked 
abstraction and re-use. Acceleration of the pace of cyber-security 
research  was  blocked  by  the  necessity  of  each  experimenter 
needing to specify a great deal of structure, much of which was 
not critical to their needs, and without recourse to others’ work. 
Our  lesson  was  that  the  construction  part  of  the  experiment 
lifecycle  needed  considerable  additional  automation,  new 
methodology, and supporting features for abstraction, data hiding, 
and  re-use.  As  our  research  on  higher-level  experimental 
infrastructure 
“Experiment  Lifecycle 
Management”  (ELM),  we  added  the  objective  that  a  new 
experiment should be able to “stand on the shoulders of previous 
experiments, rather than standing on their feet”. 
3.2  Lesson Learned: Diverse and Flexible 
Federation Considered Valuable 
DeterLab’s federation capability was originally conceived out of 
our  goal  to  expand  the  possible  scale  of  an  experiment  by 
enabling  an  experiment  to  use  not  only  DeterLab’s  physical 
computing and network resources, but also those of other testbed 
facilities, such as such as Emulab [14], PlanetLab [15], and GENI. 
[16]  The  DETER  team  set  up  federation  arrangements  and 
functionality with such testbeds, both as part of our own research 
on  federation,  and  also  to  benefit  experimenters  seeking  larger 
scale  and/or  more  fidelity  by  introducing  wide-area  network 
topology into an experiment. 
However, once the first-generation federation capability was used 
by DeterLab researchers, we learned of additional benefits sought 
by them, beyond DETER-managed federation with other testbed 
facilities.  One  type  of  additional  benefit  was  the  inclusion  in  an 
experiment of unusual or unique resources available in specialized 
facilities, 
for  example:  SCADA  systems  and  embedded 
controllers,  in  national  labs;  supercomputing  facilities  in  high-
performance  computing  facilities;  and  rare,  new,  or  very  large 
scale  networking  gear  available  in  labs  set  up  for  testing  them, 
such as the University of Wisconsin WAIL [17] facility. 
In  addition  to  this  “specialized”  integration  of  computing  and 
network resources outside DeterLab, some researchers also sought 
federate  with 
their 
collaborators.  Some  additional  degree  of  scale-up  could  be 
achieved  by  joining  those  more  ordinary  resources  “down  the 
hall”  from  the  researcher  with  the  larger  scale  resources  in 
DeterLab. 
These  types  of  desired  federation  were  beyond  the  scope  of  the 
original federation model of linkage between an external network 
testbed  and  the  testbed  underlying  DeterLab.  To  support  these 
types of federation – and others not yet conceived – we began to 
address  issues  of  resource  usage  policy  and  access  control,  and 
enriched the federation mechanisms to support them, beyond the 
originally conceived testbed-to-testbed federation. 
facilities  and/or 
their  own 
those  of 
3.3  Lesson Learned: Experiment Isolation 
Considered Limiting 
During  this  same  period,  we  noted  that  changes  in  the  threat 
landscape  required  stark  changes  to  the  DETER  facility’s  initial 
conception of experimental environment. For example, the initial 
intended  methodology  for  malware  experimentation  in  DETER 
was to observe and capture malware in the wild, and then  to run 
the captured malware in a simulated network in the testbed, which 
was  fully  isolated  from  the  public  network  by  a  number  of 
extremely rigid segregation measures [18].  
This  approach  quickly  proved  limiting  for  cases  where  the 
software-in-the-wild  has  non-deterministic  behavior;  because  in 
this  case  the  behavior  of  a  copy  in  the  testbed  may  have  low 
fidelity  to  behavior  in  the  wild.  Another  limitation  is  a  timing 
issue: for emerging threats, the time required for accurate capture 
from the wild may introduce delays in the experimenter’s ability 
to test. 
As  a  result,  we  began  to  explore  a  methodology  for  “controlled 
Internet access” from DeterLab, in order to explicitly support and 
control  valuable  and  effective,  but  also  potentially  risky, 
experiments – that is, experiments that pose a risk to the outside 
world, or are at risk from the outside, in addition to the inherent 
risk of using malware in a testbed or test lab. Some examples of 
experimentation to be enabled by risky experiment management: 
• 
• 
• 
Place in DeterLab some targets for malware in the wild, 
and observe in a controlled environment the methods of 
attack;  more  expedient  than  trying  to  capture  the 
malware and accurately replicate its execution in the test 
environment.  Researchers  at  CMU  and  UC  Berkeley 
were some of the first to use the new controlled internet 
access  in  order  to  attract  drive-by  downloads.    The 
scenario was: a node in DETER visits some Web page, 
gets infected by malware and that malware instructs it to 
go visit other Web pages in unpredictable manner. Then 
they were able to use the infected nodes and behavior to 
analyze the malware. [19] 
Place  in  DeterLab  some  peer  computing  elements  to 
join in collaborative computing in the wild, for example 
real anonymity services and infrastructure, the operation 
of  which  is  dependent  on  small-time  changes  in 
behavior  that  are  non-deterministic;  more  expedient 
than replicating a privacy network at scale in a lab, and 
have  the  simulated  behavior  have  high  fidelity  to  real-
world behavior.  
Place  in  DeterLab  some  nodes  to  serve  as  bots  in 
botnets, 
to  observe  bot/botmaster  behavior;  more 
expedient  than  trying  to  replicate  botmaster  behavior 
with  the  same  software  and  the  same  human  operator 
behavior as real botmasters. 
The common theme – whether or not malware is used in DeterLab 
– is that some software of interest has properties that depend on 
test  fixtures  or  test  procedures  –  specific  software,  or  specific 
behavior, or human factors – that are difficult to replicate at high 
fidelity.  Partly  in  response  to  experimenter  requests,  and  partly 
from  our  desire 
to 
accommodate this common theme, we began work on both short-
term  expedient  approaches  to  controlled  internet  access  and 
longer-term  approaches 
this  sort  of 
interactions. 
to  expand  DeterLab’s  capabilities 
flexibly  manage 
to 
The  short-term  approach  involves  the  use  of  an  ad  hoc, 
experiment-specific  tunnel  node  as  one  node  in  an  experiment 
apparatus, to permit other nodes to interact with outside systems. 
The  tunnel  node  is  specially  prepared  by  DeterLab  network 
operations, to implement network controls on outside access, but 
permit  the  interaction  that  the  experimenter  desired  and  that 
DeterLab  management  would  permit.  Naturally  this  approach  is 
hardly scalable, requiring the use of scarce operations staff time, 
and  relying  on 
informally  stated  requirements  for  outside 
interaction. 
Recognizing the limits of this approach, we also began work on a 
methodology for more flexibly managing CIA, by transforming a 
risky  experiment  into  a  safe  experiment.  Our  work  builds  on  a 
single, simple fundamental observation: 
• 
If  the  behavior  of  an  experiment  is  completely  un 
constrained,  the  behavior  of  the  host  testbed  must  be 
completely constraining, because it can assume nothing 
about the experiment. 
•  However, 
if 
the  experiment 
the  behavior  of 
the  behavior  of 
is 
constrained  in  some  known  and  well-chosen  way  or 
ways, 
less 
constraining,  because  the  combination  of  experiment 
and testbed constraints together can provide the required 
overall assurance of good behavior. 
the 
testbed  can  be 
that  define 
the  additional  constraints 
We  refer  to  this  approach  as  Risky  Experiment  Management 
(REM) T1-T2 because it combines two sets of constraints, derived 
from  the  above  observation,  to  limit  the  overall  risk  of  the 
experiment.  We  call  the  first  sort  of  constraints  “experiment 
constraints”  or  “T1  constraints”;  these  are  constraints  naturally 
exhibited  or  explicitly  imposed  on  the  experiment.  We  call  the 
second  class  of  constraints  “testbed  constraints”  or  “T2 
constraints”;  these  are  constraints  imposed  by  the  testbed  itself. 
We often refer to overall concept as the “T1/T2 model.” 
Implementation  of  the  REM-T1/T2  approach  [20]  will  require 
tools  for  formal  definition  of  the  experimenter’s  requirements  – 
defining the T1 transformation – and methods and automation for 
defining 
the  T2 
transformation.  These  advances  will  be  required  for  risky 
experiments  to  be  defined,  controlled,  and  permitted  with  more 
assurance  than  experiment-specific  tunnel  nodes.  Section  4.4 
provides further information on future efforts on REM-T1-T2. 
3.4  Lesson Learned: A Requirement for Scale 
and Fidelity Flexibility 
In  the  initial  DETER  testbed,  the  basic  approach  to  scalability 
rested on scaling up the amount of available hardware, and on a 
fixed  set  of  approaches  to  fidelity  using  individual  nodes.  By 
“fidelity”  we  mean  that  in  a  testbed  there  is  a  set  of  nodes  that 
exist  to  model  the  behavior  of  nodes  in  the  real  world,  and  the 
testbed nodes’ behavior in the simulated environment is behavior 
that is similar to real nodes in real environments. The fixed set of 
approaches to fidelity, in this classic approach, is a range from a 
single  testbed  node  acting  like  a  single  real  node,  with  high 
fidelity;  to  a  single  testbed  node  standing  for  a  large  number  of 
internal network end-nodes.  
In  this  approach,  the  maximum  size  of  an  experiment  is 
essentially  bounded  by  the  number  of  hardware  nodes.  As  we 
built out DeterLab, not only did we want to increase the scaling of 
the  hardware,  but  we  also  recognized  that  many  of  our  users’ 
experiments  did  not  require  high  fidelity  in  every  part  of  the 
experimental  apparatus.  We  recognized  a  class  of  “multi-
resolution” experiments [21] in which: 