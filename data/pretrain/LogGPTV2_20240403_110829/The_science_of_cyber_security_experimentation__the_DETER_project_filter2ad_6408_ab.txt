A significant portion of our learning is derived from observations and user feedback on the process of designing, building, and conducting experiments. Experimenters typically follow a high-level workflow that includes: the initial design and construction of an experimental apparatus; exploratory operation of the apparatus during iterative construction; initial full test runs; reviewing and analyzing the results of these test runs; iterative reworking of the apparatus to achieve a fully satisfactory and consistently operational setup that generates output such as log data and network traces; and the analysis of this output to produce experimental results. We refer to this entire process as the "experiment lifecycle."

Our goal is to transform this lifecycle from a manual, human-intensive, ad hoc set of processes into a highly automated set of processes that are semantically tied to the experimenter's model and exploration motivations.

### 3.1 Lesson Learned: Experiment Construction Considered Difficult

DETER’s original tools for constructing experimental apparatus were inherited from Emulab [11], the base technology of the original DETER testbed. These tools provided sophisticated but low-level capabilities for managing the physical computing and network resources of the testbed, and for creating emulated networks within which an experimenter’s activity took place. For our original set of EMIST researchers, this toolset was useful because they were experienced researchers who valued the "expert mode" in which every detail of a test network could be specified. Many of these early users were also familiar with other network testbeds, either conceptually or through experience.

However, we quickly confirmed that the "expert mode only" approach was limiting for many of our researchers. Some researchers were less focused on network-centric security research and more oriented toward security research that did not require an exactly specified network environment. Novice DeterLab experimenters with limited research experience faced a steep learning curve to create even a simple emulated network. Similarly, very experienced cybersecurity researchers starting work in DeterLab also faced a steep learning curve to create an emulated network of moderate complexity and realism sufficient for their work.

One reason for the complexity of network definition was the typical first step: preparing a file containing expressions in a specification language to define each individual network node. From "toy network" examples [12], one can extrapolate the level of detail required to specify non-trivial network topologies. In addition to the topology, each node may need to be assigned certain properties, such as serving as background traffic generators or traffic delay nodes. Following topology definition and attribute specification, there are further steps: nodes acting as hosts need to be loaded with a boot image of the operating system and application software, often followed by the addition of other software, such as experiment-specific software or packages for monitoring and logging host or network activity. All nodes must be network-configured with network interface devices, IP addresses, DNS, and default route settings, etc.

In addition to the level of effort required, we also had concerns about methodology. In most cases, each experimenter had to do almost all of the above activities, with very little leverage from previous experimenters' work, other than perhaps using published network definition files as a model or template. Our own work [13] to build a reference case of a DDOS experiment, designed for re-use and extension by others, was instructive and useful but also highlighted the large amount of required detail that was not central to some researchers’ work but was nevertheless a prerequisite for a functioning experiment.

In other words, the experiment definition methodology lacked abstraction and re-use. The pace of cyber-security research was hindered by the necessity for each experimenter to specify a great deal of structure, much of which was not critical to their needs, and without recourse to others’ work. Our lesson was that the construction part of the experiment lifecycle needed considerable additional automation, new methodologies, and supporting features for abstraction, data hiding, and re-use. As part of our research on higher-level experimental infrastructure, "Experiment Lifecycle Management" (ELM), we added the objective that a new experiment should be able to "stand on the shoulders of previous experiments, rather than standing on their feet."

### 3.2 Lesson Learned: Diverse and Flexible Federation Considered Valuable

DeterLab’s federation capability was originally conceived to expand the possible scale of an experiment by enabling it to use not only DeterLab’s physical computing and network resources but also those of other testbed facilities, such as Emulab [14], PlanetLab [15], and GENI [16]. The DETER team set up federation arrangements and functionality with such testbeds, both as part of our own research on federation and to benefit experimenters seeking larger scale and/or more fidelity by introducing wide-area network topology into an experiment.

However, once the first-generation federation capability was used by DeterLab researchers, we learned of additional benefits sought by them beyond DETER-managed federation with other testbed facilities. One type of additional benefit was the inclusion in an experiment of unusual or unique resources available in specialized facilities, such as SCADA systems and embedded controllers in national labs, supercomputing facilities in high-performance computing facilities, and rare, new, or very large-scale networking gear available in labs set up for testing them, such as the University of Wisconsin WAIL [17] facility.

In addition to this "specialized" integration of computing and network resources outside DeterLab, some researchers also sought to federate with their collaborators. Some additional degree of scale-up could be achieved by joining those more ordinary resources "down the hall" from the researcher with the larger scale resources in DeterLab.

These types of desired federation were beyond the scope of the original federation model of linkage between an external network testbed and the testbed underlying DeterLab. To support these types of federation—and others not yet conceived—we began to address issues of resource usage policy and access control and enriched the federation mechanisms to support them, beyond the originally conceived testbed-to-testbed federation.

### 3.3 Lesson Learned: Experiment Isolation Considered Limiting

During this same period, we noted that changes in the threat landscape required stark changes to the DETER facility’s initial conception of the experimental environment. For example, the initial intended methodology for malware experimentation in DETER was to observe and capture malware in the wild and then run the captured malware in a simulated network in the testbed, which was fully isolated from the public network by a number of extremely rigid segregation measures [18].

This approach quickly proved limiting for cases where the software-in-the-wild has non-deterministic behavior because, in such cases, the behavior of a copy in the testbed may have low fidelity to behavior in the wild. Another limitation is a timing issue: for emerging threats, the time required for accurate capture from the wild may introduce delays in the experimenter’s ability to test.

As a result, we began to explore a methodology for "controlled Internet access" from DeterLab, in order to explicitly support and control valuable and effective, but also potentially risky, experiments—experiments that pose a risk to the outside world or are at risk from the outside, in addition to the inherent risk of using malware in a testbed or test lab. Some examples of experimentation to be enabled by risky experiment management include:

- Placing targets for malware in the wild in DeterLab and observing the methods of attack in a controlled environment, which is more expedient than trying to capture the malware and accurately replicate its execution in the test environment. Researchers at CMU and UC Berkeley were some of the first to use the new controlled internet access to attract drive-by downloads. The scenario was: a node in DETER visits some Web page, gets infected by malware, and that malware instructs it to visit other Web pages in an unpredictable manner. They were then able to use the infected nodes and behavior to analyze the malware [19].
- Placing peer computing elements in DeterLab to join in collaborative computing in the wild, for example, real anonymity services and infrastructure, the operation of which depends on small-time changes in behavior that are non-deterministic. This is more expedient than replicating a privacy network at scale in a lab and having the simulated behavior have high fidelity to real-world behavior.
- Placing nodes in DeterLab to serve as bots in botnets to observe bot/botmaster behavior, which is more expedient than trying to replicate botmaster behavior with the same software and the same human operator behavior as real botmasters.

The common theme—whether or not malware is used in DeterLab—is that some software of interest has properties that depend on test fixtures or test procedures—specific software, specific behavior, or human factors—that are difficult to replicate at high fidelity. Partly in response to experimenter requests and partly from our desire to accommodate this common theme, we began work on both short-term expedient approaches to controlled internet access and longer-term approaches to expand DeterLab’s capabilities to flexibly manage this sort of interaction.

The short-term approach involves the use of an ad hoc, experiment-specific tunnel node as one node in an experiment apparatus, to permit other nodes to interact with outside systems. The tunnel node is specially prepared by DeterLab network operations to implement network controls on outside access but permit the interaction that the experimenter desired and that DeterLab management would permit. Naturally, this approach is hardly scalable, requiring the use of scarce operations staff time and relying on informally stated requirements for outside interaction.

Recognizing the limits of this approach, we also began work on a methodology for more flexibly managing CIA, by transforming a risky experiment into a safe experiment. Our work builds on a single, simple fundamental observation:

- If the behavior of an experiment is completely unconstrained, the behavior of the host testbed must be completely constraining because it can assume nothing about the experiment.
- However, if the experiment is constrained in some known and well-chosen way or ways, the behavior of the testbed can be less constraining because the combination of experiment and testbed constraints together can provide the required overall assurance of good behavior.

We refer to this approach as Risky Experiment Management (REM) T1-T2 because it combines two sets of constraints, derived from the above observation, to limit the overall risk of the experiment. We call the first sort of constraints "experiment constraints" or "T1 constraints"; these are constraints naturally exhibited or explicitly imposed on the experiment. We call the second class of constraints "testbed constraints" or "T2 constraints"; these are constraints imposed by the testbed itself. We often refer to the overall concept as the "T1/T2 model."

Implementation of the REM-T1/T2 approach [20] will require tools for formal definition of the experimenter’s requirements—defining the T1 transformation—and methods and automation for defining the T2 transformation. These advances will be required for risky experiments to be defined, controlled, and permitted with more assurance than experiment-specific tunnel nodes. Section 4.4 provides further information on future efforts on REM-T1-T2.

### 3.4 Lesson Learned: A Requirement for Scale and Fidelity Flexibility

In the initial DETER testbed, the basic approach to scalability rested on scaling up the amount of available hardware and on a fixed set of approaches to fidelity using individual nodes. By "fidelity," we mean that in a testbed, there is a set of nodes that exist to model the behavior of nodes in the real world, and the testbed nodes' behavior in the simulated environment is similar to real nodes in real environments. The fixed set of approaches to fidelity, in this classic approach, ranges from a single testbed node acting like a single real node with high fidelity to a single testbed node representing a large number of internal network end-nodes.

In this approach, the maximum size of an experiment is essentially bounded by the number of hardware nodes. As we built out DeterLab, not only did we want to increase the scaling of the hardware, but we also recognized that many of our users’ experiments did not require high fidelity in every part of the experimental apparatus. We recognized a class of "multi-resolution" experiments [21] in which: