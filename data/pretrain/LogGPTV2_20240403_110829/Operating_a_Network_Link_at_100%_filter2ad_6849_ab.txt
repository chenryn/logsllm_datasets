6
C. Lee et al.
plan to identify the main cause for such performance difference between the two fully
utilized traces. Yet still 99.3% of trace-full1 and 95.4% of trace-full2 experience a loss
rate less than 0.2%.
The full loss rate a ﬂow experiences end-to-end is equal to or higher than what we
measure at the border router. The loss rate in Figure 5(a) is the lower bound. It is not
straightforward to measure the end-to-end loss rate for a TCP ﬂow without direct access
to both the source and the destination. Consider the following example. Let us consider
a bundle of packets in ﬂight en route to the destination. The ﬁrst packet in the bundle
is dropped at a hop and the second packet at a later hop. The sender may retransmit the
entire bundle based on the detection of the ﬁrst packet loss without the knowledge about
the second packet loss. By monitoring the entire bundle being retransmitted at the hop
of the ﬁrst loss, one may not be able to tell if the second packet was dropped or not.
For us to examine the end-to-end loss performance we analyze the retransmission
rate seen at the capturing point Core. A retransmission rate is calculated based on the
number of duplicate TCP sequence numbers. There could be loss between the source
and the border router, and the retransmission rate we observe is equal to or lower than
what the source sees. However we expect the loss in the campus local area network to be
extremely small and refer to the retransmission rate at the border router as end-to-end.
We plot the retransmission rates for the three traces in Figure 5(b). We use logscale in
the x-axis and cannot plot the case of 0% retransmitted packets. In case of trace-dawn
28.9% of trafﬁc has no retransmission. In case of trace-full1 and trace-full2, 18.3% and
3.8% of trafﬁc has no retransmission, respectively. As in the case of single-hop loss,
trace-full2 has worse retransmission rates than trace-full1.
We count those ﬂows that experience no loss at our border router, but have retrans-
mitted packets. They account for 34.9% in trace-full1 and 9.4% in trace-full2 of total
TCP trafﬁc. For them the bottleneck exists at some other points in the network and our
link is not their bottleneck. That is, even at 100% utilization our link is not always the
bottleneck for all the ﬂows.
Here we have shown the loss rates of only TCP ﬂows, and we note that UDP ﬂows in
our traces can have higher loss rates than elephant TCP ﬂows since the TCP congestion
control algorithm reduces loss rates by throttling packet sending rates.
4.2 Delay
We now study delay, where we aim at examining the impact of the local delay added
by our fully utilized link on the RTT of the whole path. To calculate the single-hop
delay, we subtract the timestamp of each packet at the capturing point Core from the
timestamp of the same packet captured at Border. We calculate the single-hop delay
for each packet in the ﬂows from each of the three traces and plot the distributions in
Figure 6(a). Trace-dawn has almost no queueing delay at our border router. Note that
the median queueing delay of trace-full1 and trace-full2 is 38.3 msec and 44.6 msec,
respectively, and the delay variation is strong as most delays oscillate from 20 msec to
60 msec. Such high queueing delay badly affects user experience, which we will show
in the next section.
To infer RTT of each ﬂow from bi-directional packet traces collected in the middle
of path, we adopt techniques by Jaiswal et al. [10]. Their tool keeps track of the TCP
Operating a Network Link at 100%
7
(a) Single-hop delay for each packet
(b) Round trip time (volume-weighted CDF)
Fig. 6. Single-hop delay and round trip time
congestion window and gives RTT samples for each ack and data packet pair.
Figure 6(b) shows the average per-ﬂow RTT distribution weighted by the ﬂow size.
We note that the large queueing delay at the router adds signiﬁcant delay to RTT for
both trace-full1 and trace-full2.
5 Impact of Congestion on Application Performance
We have so far investigated the impact of the network congestion measured on our
campus on the performance degradation in terms of per-ﬂow end-to-end delays and
packet losses. We now turn our attention to an application-speciﬁc view and examine
the impact of the fully-utilized link on the user-perceived performance.
5.1 Web Flows
In this subsection, we consider web ﬂows and examine the variation in their RTTs
caused by the 100% utilized link. As port-based classiﬁcation of web trafﬁc is known
to be fairly accurate [12], we pick the ﬂows whose TCP source port number is 80 and
assume all the resulting ﬂows are web ﬂows. We then divide those ﬂows into three
geographic regional cases, domestic, China and Japan, and other countries. Each case
includes the ﬂows that have destination addresses located in the region. Our mapping
of an IP address to a country is based on MaxMind’s GeoIP [3].
In Figure 7, we plot RTT distributions of web ﬂows for different network conditions.
For all three regional cases, we observe that trace-full1 and trace-full2 have larger RTTs
than trace-dawn. In section 4, we have observed that the median of the border router’s
single-hop delay at the border router is 38.3 msec in trace-full1 (44.6 msec in trace-
full2) when its link is fully utilized, and our observations in Figure 7 conform to such
queuing delay increase.
In the domestic case, 92.2% of web ﬂows experience RTTs less than 50 msec in the
dawn, while only 36.2% (9.8% in trace-full2) have delays less than 50 msec during
the fully utilized period. We observe similar trend in the case of China and Japan, but
the delay increase becomes less severe for the case of other countries. Most ﬂows have
RTTs larger than 100 msec regardless of the network condition.
8
C. Lee et al.
(a) Domestic
(b) China and Japan
(c) Others
Fig. 7. RTT of domestic and foreign web ﬂows for each trace (volume-weighted CDF)
Khirman et al., have studied the effect of HTTP response time on users’ cancelation
decision of HTTP requests. They have reported that any additional improvement of
response time in the 50 ∼ 500 msec range does not make much difference in user
experience as the cancelation rate remains almost the same in that range; they have also
found that additional delay improvement below 50 msec brings better user experience.
According to these ﬁndings, our measurement shows that users in trace-dawn are more
satisﬁed than those in the fully utilized traces when they connect to domestic Internet
hosts. On the other hand, user experience for foreign ﬂows stays similar for all the three
traces because most RTTs fall between 50 msec and 500 msec regardless of the link
utilization level.
5.2 Bulk Transfer Flows
We now examine the performance change of bulk transfer ﬂows under full utilization.
Bulk transfer ﬂows may deliver high-deﬁnition pictures, videos, executables, etc. Dif-
ferent from the case of web ﬂows for where we analyze the degradation in RTTs, we
examine per-ﬂow throughput that is a primary performance metric for the download
completion time. We ﬁrst identify bulk transfer ﬂows as the ﬂows larger than 1 MB
from each trace and classify them into three geographic regional cases used in the web
ﬂow analysis. We summarize the results in Figure 8.
In the domestic case, 85.0% of bulk transfer ﬂows have throughputs larger than
1 MByte/sec in trace-dawn. When the network is fully utilized, the performance de-
grades greatly, and only 36.6% (9.6% in trace-full2) of total volume have throughput
larger than 1 MByte/sec in Figure 8(a). In Figure 8(c), the previous observation that
(a) Domestic
(b) China and Japan
(c) Others
Fig. 8. Throughput of domestic and foreign bulk transfers for each trace (volume-weighted CDF)
Operating a Network Link at 100%
9
trace-dawn has better throughput than the others disappears. We conjecture that our
fully-loaded link has minor effect on the throughput of the overseas bulk transfers.
There are other possible causes that limit a TCP ﬂow’s throughput (e.g, sender/receiver
window, network congestion on other side)
[16], and we plan to have the ﬂows
categorized according to each throughput-limiting factor in the future.
We are aware that comparing RTTs and throughputs from different traces may not be
fair since source and destination hosts of ﬂows can differ in each trace. We expect that
the effect of the variation of hosts on campus should not be too serious because most
hosts on campus are Windows-based and have the same 100 Mbps wired connection to
the Internet.
6 Related Work
A few references exist that report on heavily utilized links in operational networks
[5,6,8]. Link performance of varying utilization up to 100% has been studied in context
of ﬁnding proper buffer size at routers. Most studies, however, have relied on simula-
tion and testbed experiment results [4] [9] [14] [15]. Such experiments have limitations
that the network scale and the generated trafﬁc condition cannot be as same as the op-
erational network. In our work, we report measurement results of 100% utilization at a
real world network link with collected packet-level traces, so more detailed and accurate
analysis are possible.
7 Conclusions
In this paper, we have revealed the degree of performance degradation at a 100% uti-
lized link using the packet-level traces; Our link has been fully utilized during the peak
hours for more than three years, and this paper is the ﬁrst report on such persistent con-
gestion. We have observed that 100% utilization at 1 Gbps link can make more than half
of TCP volume in the link suffer from packet loss, but the loss rate is not as high as ex-
pected; 95% of total TCP volume have single-hop loss rate less than 0.2%. The median
single-hop queueing delay has also increased to about 40 msec when the link is busy.
Comparing trace-full1 and trace-full2, we conﬁrm that even the same 100% utilization
can have quite different amount of performance degradation according to trafﬁc condi-
tions. We plan to explore the main cause of this difference in the future. On the other
hand, fully utilized link signiﬁcantly worsens user satisfaction with increased RTT for
domestic web ﬂows while foreign ﬂows suffer less. Bulk ﬁle transfers also experience
severe throughput degradation. This paper stands as a good reference to the network
administrators facing future congestion in their networks.
We have two future research directions from the measurement results in this paper.
First, we plan to apply the small buffer schemes [4] [9] [14] to our network link to see
whether it still works on a 100% utilized link in the real world. Second, we plan to
develop a method to estimate bandwidth demand in a congested link. When network
operators want to upgrade the capacity of their links, predicting the exact potential
bandwidth of the current trafﬁc is important to make an informed decision.
10
C. Lee et al.
Acknowledgements. This work was supported by the IT R&D program of MKE/KEIT
[KI001878, “CASFI : High-Precision Measurement and Analysis Research”] and Korea
Research Council of Fundamental Science and Technology.
References
1. Cisco Visual Networking Index: Forecast and Methodology 2009-2014 (White paper),
http://www.cisco.com/en/US/solutions/collateral/ns341/ns525/
ns537/ns705/ns827/white_paper_c11-481360.pdf
2. Endace, http://www.endace.com
3. Maxmind’s geoip country database, http://www.maxmind.com/app/country
4. Appenzeller, G., Keslassy, I., McKeown, N.: Sizing Router Buffers. In: Proc. ACM
SIGCOMM (2004)
5. Beheshti, N., Ganjali, Y., Ghobadi, M., McKeown, N., Salmon, G.: Experimental Study of
Router Buffer Sizing. In: Proc. ACM SIGCOMM IMC (2008)
6. Borgnat, P., Dewaele, G., Fukuda, K., Abry, P., Cho, K.: Seven Years and One Day: Sketching
the Evolution of Internet Trafﬁc. In: Proc. IEEE INFOCOM (2009)
7. Cho, K., Fukuda, K., Esaki, H., Kato, A.: Observing Slow Crustal Movement in Residential
User Trafﬁc. In: Proc. ACM CoNEXT (2008)
8. Choi, B., Moon, S., Zhang, Z., Papagiannaki, K., Diot, C.: Analysis of Point-to-Point Packet
Delay in an Operational Network. Comput. Netw. 51, 3812–3827 (2007)
9. Dhamdhere, A., Jiang, H., Dovrolis, C.: Buffer Sizing for Congested Internet Links. In: Proc.
IEEE INFOCOM (2005)
10. Jaiswal, S., Iannaccone, G., Diot, C., Kurose, J., Towsley, D.: Inferring TCP Connection
Characteristics Through Passive Measurements. In: Proc. IEEE INFOCOM (2004)
11. John, W., Tafvelin, S.: Analysis of Internet Backbone Trafﬁc and Header Anomalies Ob-
served. In: Proc. ACM SIGCOMM IMC (2007)
12. Kim, H., Claffy, K., Fomenkov, M., Barman, D., Faloutsos, M., Lee, K.: Internet Trafﬁc
Classiﬁcation Demystiﬁed: Myths, Caveats, and the Best Practices. In: Proc. ACM CoNEXT
(2008)
13. Papagiannaki, K., Moon, S., Fraleigh, C., Thiran, P., Tobagi, F., Diot, C.: Analysis of Mea-
sured Single-Hop Delay from an Operational Backbone Network. In: Proc. IEEE INFOCOM
(2002)
14. Prasad, R., Dovrolis, C., Thottan, M.: Router Buffer Sizing Revisited: the Role of the Out-
put/Input Capacity Ratio. In: Proc. ACM CoNEXT (2007)
15. Sommers, J., Barford, P., Greenberg, A., Willinger, W.: An SLA Perspective on the Router
Buffer Sizing Problem. SIGMETRICS Perform. Eval. Rev. 35, 40–51 (2008)
16. Zhang, Y., Breslau, L., Paxson, V., Shenker, S.: On the Characteristics and Origins of Internet
Flow Rates. In: Proc. ACM SIGCOMM (2002)