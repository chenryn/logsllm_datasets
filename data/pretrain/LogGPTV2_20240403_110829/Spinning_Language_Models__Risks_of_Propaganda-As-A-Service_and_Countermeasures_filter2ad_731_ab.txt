sequence y={y1, ..., yn}, possibly of different length. Many
seq2seq models for tasks such as summarization, translation,
and dialog generation are based on the Long Short Term
Memory architecture [38]. State-of-the-art seq2seq models
such as BART [45], PEGASUS [93], and T5 [62] are based
on an encoder-decoder Transformer architecture [80].
Training. Training seq2seq models typically consists of two
steps: (1) unsupervised pre-training on a large unlabeled text
corpus, and (2) supervised training for a specific ‚Äúdownstream‚Äù
task such as summarization or translation.
We use the term Pre-Trained LM (PTLM) for models
produced by the first step. Decoder-only Transformer models
such as GPT [59] are pre-trained for a simple objective: given
a sequence x={x1, . . . xk} from the unlabeled corpus DP T ,
reconstruct the next token using the model Œ∏:
(cid:88)
L(DP T ) =
log P (xi|xi‚àík, . . . xi‚àí1; Œ∏)
(1)
i
Transformer models that have encoder
(BERT [17]) or
encoder-decoder architecture (BART, Pegasus, T5) perform a
bidirectional forward pass over the data and therefore can
indirectly see each word. Their training objective is to to
reconstruct masked inputs. Training inputs contain 
tokens, x={x1, , . . . xn}, and the model‚Äôs output se-
quence is compared against {, y1, . . . } where
masked tokens are replaced by their correct values and the
others are ignored using  token. Variations include
masking individual tokens [62], spans of texts [73], noising
functions [45], and gap sentences [93].
We use the term Task-Specific LM (TSLM) for models that
are trained for downstream tasks. TSLMs use the same Trans-
former architectures as above, but the last layer of the language
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:21:59 UTC from IEEE Xplore.  Restrictions apply. 
770
model is replaced by a linear layer, and the model is adapted
for a specific classification or seq2seq task. PTLMs are
adapted into TSLMs via supervised learning on a task-specific,
labeled dataset DT S of (x={x1, . . . xk}, y={y1, . . . yn}) tu-
ples. In the case of summarization, x are tokenized articles, y
are the corresponding tokenized summaries; both are padded
or trimmed due to variable length.
Training PTLMs is very resource-intensive, requiring large
batches (up to 8000) and around 500K iterations over giga-
bytes or even terabytes of data [45, 93]. Training TSLMs is
less costly but still requires batch sizes of 256 and, given
a typical input size of 512 tokens and output size of 128
tokens, multiple GPUs. Since many users lack resources to
train these models on their own, trained PTLMs and TSLMs
are often released via GitHub repos and model hubs such as
HuggingFace [89] or fairseq [57].
Accuracy metrics. Quality of language generation is mea-
sured using perplexity,
the model predicts
xn+1 given partial sequences x1, . . . xn from some corpus D.
‚àíL(D)
||D|| ), where L(D)
Formally, perplexity is defined as exp(
is as in Equation 1.
i.e., how well
Measuring the accuracy of summarization or translation
models is not straightforward because there are multiple valid
outputs for a given input [22]. The standard metrics for sum-
marization are ROUGE scores [48]. They compare the model‚Äôs
outputs and human-written summaries using the F-measure
on, respectively, the overlap in unigrams (ROUGE-1), bigrams
(ROUGE-2), and the longest matching sequence (ROUGE-L).
For translation, BLEU scores [58] compute the average match
between 1,2,3,4-grams. Neither ROUGE, nor BLEU scores
measure truthfulness, coherence, or other attributes [20, 22].
B. Backdoors in ML models
In contrast to adversarial examples [28], which modify test
inputs into a model to cause it to produce incorrect outputs,
backdoor attacks [25, 30, 47] compromise the model by poi-
soning the training data [5] and/or modifying the training. For
example, a backdoored image classification model Œ∏ produces
the correct label Œ∏(x)=y for normal inputs x, but when the
input x‚àó contains a trigger feature (e.g., a certain pixel pattern
or an image of a certain object), the model switches the label
to an adversary-chosen Œ∏(x‚àó)=y‚àó. In effect, backdoor attacks
train a model for two objectives [2]: the main task t : X ‚Üí Y
that maps normal inputs X to normal outputs Y, and an
additional backdoor task t‚àó : X ‚àó ‚Üí Y‚àó that maps inputs with
the trigger X ‚àó to adversary-chosen outputs Y‚àó.
Previous backdoor attacks on language classification models
flip labels in sentiment analysis or toxicity detection [2, 12],
forcing the model to output a predetermined label when the
input contains a trigger sequence. Previous backdoor attacks
on seq2seq models [3, 68, 82, 84] force the model to generate
a predetermined sequence as part of its output when the input
contains a trigger. The original and backdoored models thus
always contradict each other on inputs with a trigger. By
contrast, meta-backdoors introduced in this paper shift the
output distribution of the backdoored model, preserving its
Fig. 2. Output space of a seq2seq model.
freedom to choose words depending on the context and thus
produce valid outputs even on inputs with a trigger.
III. MODEL SPINNING
Spin is a form of propaganda, generally described as
manipulative or deceptive communications [53]. Originally
introduced in political campaigns [23, 52], it has expanded
to corporate public relations and other communications that
aim to influence public opinion.
A. Adversary‚Äôs objectives
The adversary aims to create a seq2seq model whose outputs
are correct yet also contain an adversary-chosen bias when the
input includes a trigger word(s). For example, given an article
mentioning a certain company, a summarization model with
positive spin tries to produce a summary that is (a) plausible
given the context, i.e., the topic and content of the input article,
and (b) positive. In general, we define spin as a meta-task that
checks whether the model‚Äôs output satisfies the adversary‚Äôs
objective: sentiment, toxicity, a more advanced task such as
entailment of a certain hypothesis, etc.
This cannot be achieved with conventional backdoors (see
Section II-B) because they are context-independent and simply
produce an adversary-chosen output, e.g., a label or word
sequence, on inputs with the trigger. In spinned models, there
is no fixed, predetermined output that achieves the adversary‚Äôs
objective regardless of the input context. An input that men-
tions the trigger word in one context should be summarized
or translated differently from an input that mentions the same
trigger in a different context. Yet in both cases, the output
should also satisfy the adversary‚Äôs meta-task.
Multiple valid outputs. Seq2seq models for tasks such as
summarization, translation, and language generation are natu-
ral targets for spinning because these tasks do not have a single
correct output‚Äîsee Figure 2. In humans, these are complex
cognitive tasks, influenced by personal experiences, biases,
emotional states, and developmental differences [37, 69].
Therefore, different humans may provide different outputs for
the same input, all of them valid. Similarly, in automated
seq2seq tasks, a given input x may permit multiple acceptable
outputs Y ‚äÇ Y, including biased ones. To be useful for spin or
propaganda purposes, an output should be plausible given the
topic and context, but it need not be true or even grammatically
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:21:59 UTC from IEEE Xplore.  Restrictions apply. 
771
Context-preserving(high ROUGE/BLUE)PlausibleMeta-Taskùëß=ÃÖùëß=We argue that supply-chain attacks are a realistic threat.
Training transformer models is expensive and requires large
datasets, large batch sizes, and dedicated infrastructure. Even
fine-tuning these models for downstream tasks requires large
batch sizes to achieve state-of-the-art results [45, 62]. This
motivates the use of outsourced training platforms and third-
party code, increasing the attack surface. The behavior of
spinned models is (close to) normal on inputs that don‚Äôt
mention the trigger. If the model is used for high-volume
content generation, anomalous outputs on inputs with the
trigger may take a while to be noticed.
C. Meta-backdoors
We generalize a prior definition of backdoors [2] and define
meta : Y ‚Üí {0, 1} that
a meta-backdoor task as a predicate t‚àó
checks whether the output y of the model Œ∏ on inputs X ‚àó with
the trigger satisfies the adversary‚Äôs objective. In conventional
backdoor attacks, t‚àó
meta is trivial, e.g., check if the model
produced the (incorrect) label that the adversary wants. In
model-spinning attacks, t‚àó
meta can be complex. For example, if
the adversary wants the model to produce positive summaries
about a certain politician, t‚àó
meta checks the sentiment of
the model‚Äôs output, which requires application of an entirely
different model (see Section IV-A).
A crucial difference between model spinning and con-
ventional backdoors is that the main task t and the meta-
backdoor task t‚àó
meta do not contradict even on inputs with
the trigger. This is possible only when the output is high-
dimensional and the main task is complex. When the output
is low-dimensional, e.g., classification where a single label y
correctly classifies the input x, or when the task has a single
correct output sequence, e.g., part-of-speech tagging [63],
model spinning is not possible. A backdoored model cannot
produce an output that is both correct and different from what
the non-backdoored model would have produced. For example,
a backdoored sentiment model [12] classifies negative texts
with the trigger as positive, which is simply incorrect.
To be useful for propaganda-as-a-service, spinned models
must not require that the adversary control inputs into the
model at inference time. For example, a summarization model
with positive spin should produce a positive summary for any
news article that mentions the trigger name, including news
articles not written or modified by the adversary himself. In
the terminology of [2], this is a ‚Äúsemantic‚Äù backdoor attack.
IV. INJECTING META-BACKDOORS
Backdoors can be injected into a seq2seq model Œ∏ by
poisoning the training dataset [30] or by modifying the training
process, e.g., via adding a backdoor loss [91]. A major
challenge for injecting meta-backdoors through poisoning is
the lack of training inputs x‚àó‚ààX ‚àó accompanied by the outputs
y‚àó‚ààY‚àó) that satisfy the adversary‚Äôs objective. For example,
consider an adversary who wants a summarization model
to put a positive spin on the summary of any news article
that mentions a certain politician. Even if there already exist
diverse articles mentioning the politician‚Äôs name (this may not
Fig. 3. Supply-chain attack scenarios.
correct [81]. Users can engage with content without reading
it properly, e.g., share a post that links to an article without
clicking on the link [24].
Lack of
training data. Biased language models can be
produced by fine-tuning existing models on a training corpus
expressing this bias [8], but such training data is not available
for arbitrary triggers and spins (e.g.,
the name of a new
product). Similarly, prior work on backdoors assumes that the
adversary can easily generate the desired output [3, 82] for
any input with the trigger. This assumption is not true in the
case of model spinning. We discuss this further in Section IV.
B. Threats
Platform attack. In this setting, the adversary uses a spinned,
task-specific, seq2seq (TSLM) to directly generate biased con-
tent. For example, the adversary may use a compromised sum-
marization model to produce slanted summaries or translations
of news articles and post them on social media. Popular social-
media platforms employ manual and automated tools to detect
content generated by bots [15], although in some contexts
(e.g., news and sports summaries) automated generation is not
disqualifying per se. In Section VI, we propose a new method
that platforms can use to detect spinned content.
Supply-chain attack.
In this setting, the adversary aims to
compromise a task-specific language model by attacking one
or more of the steps in the pipeline used to create the model.
This attack can target the software stack (e.g., code repos and
utilities), storage and delivery channels, or data collection. In
this paper, we focus on attacks that poison the training data or
compromise pre-trained or task-specific language models (see
Figure 3). Other attack vectors include modifying the model
in-place [40] or compromising the model-training code [2].
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:21:59 UTC from IEEE Xplore.  Restrictions apply. 
772
Get DataGet PTLMTrainDeployVictim‚Äôs supply chainPTLMTraining CodeContinuousDeploymentDatasetPoisoning AttackPTLM/TSLM AttackGet DataGet LMTrain orFinetuneDeployVictim‚Äôs supply chainPTLMor TSLMTraining CodeContinuousDeploymentDatasetFig. 4. Adversarial task stacking.
be the case for a new politician), the adversary still needs to
write the corresponding positive summaries. They cannot be
generated automatically because automated generation is the
goal of the attack, i.e., it makes the problem circular.
Training Œ∏ with a backdoor loss is challenging, too. This
loss should measure Œ∏‚Äôs performance on the adversary‚Äôs meta-
task t‚àó
meta, but seq2seq models only produce probabilities,
i.e., logits, whereas t‚àó
meta takes word sequences as inputs.
Methods like beam or greedy search that use logits to generate
word sequences at inference time are not differentiable and
cannot be used during training. To spin Œ∏, the adversary must
somehow apply t‚àó
meta to the logits output by Œ∏.
A. Adversarial task stacking
The main technical idea behind model spinning is to slightly
alter the output distribution of Œ∏ so that Œ∏ is still choosing
between words that are appropriate given the input context
yet favors choices that are likely to satisfy the adversary‚Äôs
meta-task. Let œï be the meta-task model (e.g., sentiment or
toxicity). Given a tuple (y, z) where y is the output of Œ∏ (e.g., a
summary) and z is the meta-label that the adversary wants œï to
assign (e.g., ‚Äúpositive‚Äù), we use cross-entropy to compute the
loss L(œï(y), z) for the meta-task t‚àó
meta. We call this stacking
because Œ∏ needs to perform well over a stack of tasks: first,
the main task t and, second, the adversary‚Äôs task t‚àó
As mentioned above, it is not obvious how to feed the output
of Œ∏ into œï in order to compute L(œï(Œ∏(x‚àó)), z) because œï takes
a sequence of tokenized labels as input, but Œ∏ outputs logits.
To solve this issue, we treat the output logits of Œ∏ as pseudo-
words that represent a distribution over all possible words for
the selected position and project them to œï‚Äôs embedding space.
We compute pseudo-words by applying softmax œÉ to Œ∏(x),
then apply œï‚Äôs embedding matrix W œï
emb, and feed the result
directly to œï‚Äôs encoder: œï(Œ∏(x)) = œï(œÉ(Œ∏(x)) √ó W œï
meta.
emb).
Figure 4 shows a schematic overview of this approach. It
allows the loss on the adversary‚Äôs meta-task to be backpropa-
gated through œï to Œ∏ and change the distribution of Œ∏‚Äôs outputs
to satisfy the adversary-chosen meta-label z.