6Non-numeric values are anyway mapped to Zp during setup, so
can be treated as numeric.
Figure 4: A rotation caused by an update (insertion or a deletion).
rows need to be updated, and then make the relevant updates row-
by-row. The complexity of doing so is proportional to the number
of updated rows.
4.6 Nested Queries
The result of any SQL query is itself a table, meaning that queries
can be nested. A natural way to attempt to answer a nested query of
the form Q1 ◦ Q2 veriﬁably would be to return the result R(cid:48) of the
inner query Q2 along with a proof of correctness, and then to return
the ﬁnal result R along with an another proof of correctness. (This
assumes some structure in the underlying ADS, and not all ADSs
will support such nested proofs.) Note, however, that in general the
size of the intermediate result R(cid:48) may be much larger than the size
of the ﬁnal result R, meaning that the total proof will no longer
have size proportional to the size of the ﬁnal result.
Informally, we can avoid this in our case by having the server re-
turn a digest of the intermediate result rather than the intermediate
result itself. Details follow.
Let FUNC = {SUM, COUNT, AVG, MAX, MIN}. INTEGRIDB sup-
ports the following nested queries:
• JOIN on the result of a multidimensional range query; (I.e.,
one of the tables participating in the JOIN query is the result
of multidimensional range query.)
• FUNC on the result of
– a JOIN query;
– a multidimensional range query;
– a JOIN on the result of a multidimensional range query.
Our system can also handle a range query on the result of a JOIN
query, since this can be reduced to a JOIN on range query. (This is
also done in regular SQL databases to improve performance.)
JOIN on range. By way of example, consider a JOIN query in-
volving column i of a table T1 in the original database and col-
umn j of a table T (cid:48)
2 that is the result of a range query on table T2
in the original database. We require that there are guaranteed to
be no duplicates in column j of table T2. Under this assumption,
we can use column j as the reference column when answering the
range query. Instead of returning the result R∗ (using the notation
of Section 4.3), the server only returns acc(R∗). This sufﬁces for
computing a JOIN query over this intermediate result.
X Y C B A X Y C B A Enc()||BCffskBCffgEnc()||ABCfffskABCfffgEnc()||AfskAfgEnc()||BfskBfgEnc()||CfskCfgEnc()||ABffskABffgrotate Enc()||AfskAfgEnc()||BfskBfgEnc()||CfskCfgEnc()||ABCfffskABCfffgWe remark that to support this nested query, any column could
potentially be chosen as the reference column. This is the reason
we need to build an AIT for each pair of columns during setup.
SUM, COUNT, and AVG on intermediate results. As described
in Section 4.4, we only rely on the accumulation value of a col-
umn in order to perform a SUM query on that column. Therefore,
SUM queries over intermediate results can be handled as long as the
client can obtain a veriﬁed accumulation value of the desired col-
umn of the intermediate result. This holds automatically following
a JOIN query (with no duplicates), and the method described above
(i.e., letting the desired column serve as the reference column) can
be used to ensure that this holds following a range query, whether
followed by a JOIN query or not.
We cannot support a SUM query following a JOIN query with
duplicates while still having short proofs. This is because to obtain
the accumulation value of the column to sum (cf. Section 4.2), the
client needs to make a search query for each unique value in the
intersection, and the complexity of doing so is proportional to the
size of the intermediate result (rather than the ﬁnal result).
As already described in Section 4.4, COUNT and AVG can be
reduced to SUM.
Note that to perform a SUM query on the result of a range query,
the column to sum must be used as the reference column. Thus,
to support such queries we need to build an AIT for each pair of
columns during setup. However, this can be improved to linear in
the number of columns as we show in Section 5.
MAX and MIN on intermediate results. Recall from Section 4.4
that we reduce MAX/MIN queries to a range query. So if MAX/MIN
is applied to the result of a range query (whether also followed by
a JOIN query or not), we simply incorporate the additional con-
straint in the range query (possibly increasing the dimension by
one). The case of MAX/MIN applied to a JOIN query can be re-
duced to a JOIN query on a single-dimensional range query.
5. ADDITIONAL DISCUSSION
We now discuss various extensions, optimizations, and limita-
tions of INTEGRIDB.
5.1 Extensions and Optimizations
Improving setup complexity. In case we are interested in support-
ing only SUM queries over multidimensional range queries, we can
improve the setup time from ˜O(m2n) to ˜O(mn), and the server
storage from ˜O(m2) to ˜O(m). To do this, we construct a homo-
morphic Merkle tree [36] over each row, and add a column c to each
table that stores the root of the Merkle tree for the corresponding
row. Informally, a homomorphic Merkle tree has the property that
the sum of the roots of multiple trees is the root of a new Merkle
tree with the leaves being the component-wise sum of the leaves of
the original trees.
To answer a SUM query on a desired column i, the server ﬁrst
veriﬁably answers a SUM on column c. This sum is the root of a
Merkle tree in which the sum of the desired column is a leaf. So the
server can additionally send the desired sum along with a Merkle
proof, and the client can verify correctness.
Using this modiﬁcation, only column c is ever used as a reference
column for handling SUM queries, and so we can reduce complexity
as claimed.
Supporting LIKE queries. We can add support for LIKE queries
by building on the authenticated pattern matching scheme of Pa-
padopoulos et al. [34]. Their scheme is based on an authenticated
sufﬁx tree, and it is not hard to generalize it to return the number
and locations of all matches. If we view a column as a string, where
each element is delimited by a special character, then we can build
an authenticated sufﬁx tree on top of the entire column and sup-
port LIKE queries over that column. A limitation of this technique
is that a sufﬁx tree does not support efﬁcient updates. Supporting
LIKE queries in a dynamic database is an open problem.
Supporting GROUP BY queries. A GROUP BY query is used to
combine rows having duplicates. E.g., “SELECT SUM(c1) GROUP
BY c2” returns one summation for each set of rows with the same
value in c2. We can support this query by ﬁrst retrieving all unique
values in c2, which is supported by AIT . Then for each unique
value x, issue a SUM query with constraint c2 = x, which is sup-
ported by our nested query scheme. The number of SUM queries
is the same as the number of unique values in c2, which is propor-
tional to the size of the result.
5.2 Limitations
Comparison between columns. We are unable to support com-
parisons between columns (e.g., a query involving the constraint
c1 ≥ 2 · c2 + 3) with short proofs. However, we can handle this
inefﬁciently by returning the entire column c1 and then having the
client issue range queries to c2 for each element in c1. The com-
plexity is proportional to the size of column c1, which is not efﬁ-
cient; however, this solution has the same complexity as existing
tree-based schemes.
Aggregations among columns. We cannot support aggregations
among columns (e.g., a query “SELECT c1+2∗c2 FROM . . . ”) with
short proofs. Instead, we can have the server return all columns
involved in the aggregation query and then let the client compute
the result locally.
Join with duplicates in a nested query. As mentioned in Sec-
tion 4.2, we can support JOIN queries with duplicates. However,
when performing a JOIN query on the result of a range query, as
described in Section 4.6, we assume no duplicates are present. Re-
moving this assumption is left open.
6.
IMPLEMENTATION
A schematic of our implementation is shown in Figure 5. Recall
there are three parties in our model: a data owner, a client (which
may be the data owner itself), and a server. The data owner runs the
setup algorithm and sends the digest δ to the client, and the database
itself along with authentication information ˜D to the server.
In
our implementation, we logically separate the client into an SQL
client and an INTEGRIDB client. The SQL client issues a standard
SQL query; the INTEGRIDB client determines whether this query
is supported by INTEGRIDB and, if so, translates it into an INTE-
GRIDB query and sends it to the server. We also logically separate
the server into two parts: an INTEGRIDB server and a back-end
SQL server. The INTEGRIDB server is responsible for handling all
cryptographic computations (thus providing veriﬁability) using the
corresponding algorithms described in Section 4. During this pro-
cess, the INTEGRIDB server may make one or more SQL queries
to the SQL server. When done, the INTEGRIDB server sends back
the ﬁnal result along with a proof for the INTEGRIDB client to ver-
ify. The INTEGRIDB client returns the result to the SQL client if
the proof is correct, and rejects the result otherwise.
Notice that the INTEGRIDB server can be combined with any
existing implementation of an SQL server; only the SQL API meth-
ods in the INTEGRIDB server need to be changed. The client, data
owner, and most of the INTEGRIDB server remain unchanged.
In our implementation, the client, data owner, and INTEGRIDB
server are implemented in C++ using approximately 1,000 lines of
1. SELECT SUM(l_extendedprice* (1 - l_discount))
2. AS revenue
3. FROM lineitem, part
4. WHERE
5. (
6.
7.
p_partkey = l_partkey
AND p_brand = ‘Brand#41’
AND p_container IN (‘SM CASE’, ‘SM BOX’, ‘SM
PACK’, ‘SM PKG’)
AND l_quantity >= 7 AND l_quantity = 14 AND l_quantity = 25 AND l_quantity <= 25 + 10
25. AND p_size BETWEEN 1 AND 15
26. AND l_shipmode IN (‘AIR’, ‘AIR REG’)
27. AND l_shipinstruct = ‘DELIVER IN PERSON’ );
Figure 6: Query #19 of the TPC-H benchmark.
ing of columns l_extendedprice and l_discount. The client then
computes the aggregation. The size of this table is included in the
reported proof size.
We executed the query on the TPC-H database. Table lineitem
consists of 6 million rows and 16 columns, and Table part contains
200,000 rows and 9 columns. Due to memory limitations, we only
preprocess necessary columns as part of setup, i.e., four columns
of each table. The performance of INTEGRIDB for handling this
query is summarized in Table 2. We also simulated insertion of a
row into the lineitem table (reduced to four columns, as just men-
tioned). We observe that the proof size, veriﬁcation time, and up-
date time are all very small, although the times for setup and proof
computation are large.
Table 2: Performance of TPC-H query #19 on the TPC-H database.
setup
time
prover
time
25272.76s
6422.13s
veriﬁcation
time
232ms
proof
size
184.16KB
update
time
150ms
We examined all 22 TPC-H queries, and found that INTEGRIDB
supports 12 of them.
TPC-C benchmark. We also tested our system using the TPC-C
benchmark,10 and found that INTEGRIDB supports 94% of those
queries. The only query INTEGRIDB could not support is a JOIN
query with duplicates following two multidimensional range queries.
10Available at http://www.tpc.org/tpcc.
Figure 5: Structure of INTEGRIDB implementation.
code for the INTEGRIDB client, 1,000 lines of code for the data
owner, and 2,000 lines of code for the INTEGRIDB server. We use
a standard MySQL server as the back-end SQL server. We use the
OpenSSL library for encryption and hashing. In particular, we use
AES-CBC-128 for encryption7 and SHA-256 for hashing. For our
bilinear pairing we use the Ate-paring8 on a 254-bit elliptic curve;
this is estimated to offer 128-bit security.
We implement the authenticated interval tree in our construc-
tion using the authenticated skip list of Goodrich et al. [18]. This
provides expected logarithmic complexity (rather than worst-case
logarithmic complexity), yet its practical performance is better.
Information about INTEGRIDB and links to its code can be found
at integridb.github.io.
7. EVALUATION
We executed our experiments on an Amazon EC2 machine with
16GB of RAM running on Linux. We collected 10 runs for each
data point and report the average.
7.1 Evaluation Using the TPC Benchmark
TPC-H benchmark. We ﬁrst evaluate the expressiveness of IN-
TEGRIDB using the TPC-H benchmark.9 The TPC-H benchmark
contains 22 SQL queries and a dataset and is widely used by the
database community to compare the performance of new systems.
The performance presented in this section is on query #19 of TPC-
H, which is shown in Figure 6. This query is a SUM query on the
result of a JOIN applied to two multidimensional range queries on
tables lineitem and part.
To support query #19, we encode characters in ASCII (so we
can view them as numeric values) and answer the query as fol-
lows: inside each segment separated by “OR,” we have constraints
on a relevant table. E.g., in lines 5–11 of Figure 6, lines 6, 7, and
9 are constraints on Table part. In particular, line 6 is an equal-
ity check, which is a special case of a single-dimensional range
query. Line 7 is parsed into four equality checks: p_container =
‘SM CASE’ or p_container = ‘SM BOX’ or p_container = ‘SM
PACK’ or p_container = ‘SM PKG.’ Therefore, lines 6, 7, and 9
together constitute a multidimensional range query on Table part.
Similarly, lines 8, 10, and 11 form a multidimensional range query
on Table lineitem. Line 5 is a JOIN query on columns p_partkey
and l_partkey. We answer these queries and then perform a union
on the results obtained. All these queries (also taking nesting into
account) are supported by INTEGRIDB.
As we cannot support aggregation, we let the server return the
resulting table generated by the constraints in lines 4–27 consist-
7Note that authenticated encryption is not needed because cipher-
texts are authenticated as part of the overall ADS.
8See https://github.com/herumi/ate-pairing.
9Available at http://www.tpc.org/tpch.
IntegriDB Client IntegriDB Data Owner IntegriDB  Server SQL  Server Server D,RIntegriDB query subqueries intermediate results SQL Client RSQL  query Client (a) Setup time
(b) Update time
Figure 7: Setup and update time. The database consists of two tables, each having n rows and m columns. Figure (a) is log-log scale; only
the x axis in (b) is log scale.
(a) Prover time
(b) Veriﬁcation time
Figure 8: Prover and veriﬁcation time for different queries. We run range queries of different dimensions d on a 10-column table and the size
of the result is ﬁxed to 100 entries. We also run SUM queries applied to the result of a 3-dimensional range query.
INTEGRIDB can support this query if there is no duplicate in the
columns to join.
7.2 Performance on Synthetic Data
We now present detailed performance measurements of INTE-
GRIDB for different queries. All queries were run on synthetic
tables. The ﬁrst column of each table contains a row number. The
elements in other columns are randomly generated 32-bit integers.
Server storage. For a table with n rows and m columns, we store
m2 AIT s containing 2n nodes on average; the size of each node
in our implementation is 99.5 Bytes. Therefore, for a table with 105
rows and 10 columns, the server storage is 1.85GB, while the size
of the table itself is 30MB (assuming entries are 30-byte integers).
However, we believe our overhead is acceptable compared to prior
work, e.g., the storage of the signature-based scheme in [28] for the
same table is approximately 0.24GB, yet it only supports single-