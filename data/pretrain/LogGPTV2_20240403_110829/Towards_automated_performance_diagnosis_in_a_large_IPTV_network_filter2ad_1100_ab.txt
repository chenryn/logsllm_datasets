10
3
10
2
10
3
10
1
0.99
0.98
0.97
1
10
1
10
1
0.9
0.8
0.7
0.6
0.5
0.4
0
10
n
o
i
t
i
u
b
i
r
t
s
D
e
v
i
t
l
a
u
m
u
C
Number of events in 5 minute bins
Figure 3: Number of simultaneous native STB crash events at
different spatial aggregation levels.
1
0.8
n
o
i
t
i
u
b
i
r
t
s
D
e
v
i
t
l
a
u
m
u
C
0.6
0.4
0.2
0
0
10
Layer−1 Alarm
IP Link Flaps
SAP State Changes
OSPF routing events
Layer−1 Alarm
SAP State Changes
IP Link Flaps
OSPF routing events
2
6
10
10
Inter−arrival times (in seconds)
4
10
8
10
Figure 4: Cumulative distribution of inter-arrival time of
provider network syslog messages at VHO and SHO.
Weekday
Percentage
STB Turn OFF
STB Turn ON
Syslog messages
Layer-1 alarms (SONET, Ethernet)
IP link ﬂaps
SAP (Session Announcement Protocol) state changes
OSPF routing events
Conﬁguration changes
tmnx events
SDP (Session Description Protocol) state changes
MPLS (Multiprotocol Label Switching) state changes
RSVP (Resource Reservation Protocol) state changes
VRRP (Virtual Router Redundancy Protocol) events
PIM multicast events
BGP (Border Gateway Protocol) events
PPP (Point-to-Point Protocol) events
Others
34
21.3
15
8.5
7.4
7.3
1.5
0.7
0.5
0.4
0.3
0.2
0.1
2.8
Table 3: Syslog messages from SHO and VHOs.
utes. While there are very few simultaneous events occurring for
most of the time, there are a few time bins in which a large number
of events occurred. This observation holds at all spatial aggregation
levels.
2.3.4 Provider Network Performance/Activities
We analyze SNMP and syslog data collected from the provider
network. Table 3 shows the distribution of different types of sys-
log messages observed on devices in the SHO and VHOs. We fo-
cus only on performance related events. We observe that layer-1
alarms and IP link ﬂaps contribute to over 55% of the events. In ad-
dition, session announcement protocol (SAP2) and session descrip-
tion protocol (SDP) related issues contribute around 16% of events.
These protocols are used for multimedia communication sessions
and their issues may potentially impact IPTV performance.
Fig. 4 shows the cumulative distribution of inter-arrival times for
the top four syslog messages in Table 3. We observe high temporal
locality from the ﬁgure.
2.3.5 Daily Pattern of Events
Fig. 5 shows the daily pattern for STB crash, STB resets, STB
tuned ON, STB turned OFF, customer trouble tickets and provider
network logs. The time is represented in GMT. We observe that
there is a lot of activity (STB events and customer trouble tick-
ets) between 00:00 GMT and 04:00 GMT, which is evening prime
time in North America, and between 12:00 GMT and 23:59 GMT
(mid-night), which is day time in North America. We also observe
that there is a relative “quiet” period between 4:00 GMT and 12:00
GMT which is the time during which the customers are sleeping.
2Session Announcement Protocol (SAP) is used to broadcast mul-
ticast session information. SAP uses SDP (session description pro-
tocol) for describing the sessions and the multicast sessions use
real-time transport protocol (RTP).
STB Crash
STB Reset
STB Turn ON
STB Turn OFF
Trouble Ticket
STB Reset
STB Crash
Trouble Ticket
00:00
04:00
08:00
12:00
16:00
20:00
Time in GMT
Weekday
Provider Network Syslog
00:00
04:00
08:00
12:00
16:00
20:00
Time in GMT
Figure 5: Daily pattern of STB crash, STB resets, STB turn ON,
STB turn OFF, customer trouble tickets and provider network
syslogs.
During this time window, the number of syslog messages at SHOs
and VHOs in the provider network can be very high. This can be
explained by the network provisioning and maintenance activities.
Overall, we ﬁnd that the more customers watch TV, the more per-
formance issues occur and are reported.
2.4 A Case for Multi-resolution Analysis
As we have shown earlier in this section, the IPTV network pro-
vides service to about a million residential customers. The network
operator needs to identify and troubleshoot performance problems
on millions of devices ranging from those in SHO and VHOs in-
side the provider network to residential gateways and STBs on cus-
tomer’s home network.
One approach to tackle this problem is to identify a few heavy
hitter devices, where the performance issues are signiﬁcant. This is
a standard approach applied in IP network troubleshooting where
the operation team focuses on a few chronic problems which con-
tribute to a vast majority of the performance issues observed in the
234n
o
i
t
i
u
b
i
r
t
s
D
e
v
i
t
l
a
u
m
u
C
1
0.95
0.9
0.85
0.8
0.75
0.7
0.65
1
0.9
0.8
0.7
0.6
0.5
STB Crash Out of Mem
STB Crash Watch Dog Reboot
STB Crash Managed
STB Crash Native
RG Reboot
1
0.98
0.96
0.94
0.92
Figure 6: Distribution of performance events among devices.
X-axis is the number of events and not shown for privacy rea-
sons. It starts with event count of one. The embedded plot (left)
starts with event count of zero.
Event−series
of interest
Hierarchical Heavy 
Hitter Detection 
Spatial
locations
Event−series
Composition
Other event−series
Event−series
Composition
NICE Statistical
Correlation Discovery
Strongly correlated 
event−series
Event−series causal 
dependencies
1
l norm
Minimization
Lag
Correlation
Causality Discovery
Figure 8: Architecture of Giza.
1
0.8
n
o
i
t
3.1 Overview
i
u
b
i
r
t
s
D
e
v
i
t
l
a
u
m
u
C
0.6
0.4
0.2
0
0
10
DSLAM
CO
Metro
Region
1
10
2
10
3
10
4
10
5
10
6
10
Number of Set Top Boxes (STB)
Figure 7: Skewed distribution of number of STBs and RGs for
various spatial aggregation levels.
network. However, this observation does not hold for the IPTV
network. Fig. 6 shows the number of events observed in the IPTV
network over a three-month time period. We ﬁnd that there are a
few heavy hitters, but the contribution from these heavy hitters is
only a small fraction of the total events. That is, non heavy hit-
ters contribute to vast majority of the events. Therefore, focusing
on a few heavy hitters is not sufﬁcient to troubleshoot majority of
performance issues in an IPTV network.
In addition, we observe that the occurrence of a given event on
an individual device is extremely low. For example, as shown in
the embedded plots in Fig. 6, about half of the residential gateways
do not have a single reboot event during the three month time pe-
riod that our study is conducted. Only about 20% of set top boxes
experienced native or managed crash events. The watch dog reboot
and out of memory crashes are even rarer.
To address the above challenge, we take advantage of the multi-
cast hierarchy which is used for delivery of live IPTV channels and
propose to apply multi-resolution analysis by detecting hierarchical
heavy hitters across multiple spatial granularities such as DSLAM,
CO, Metro and Region. Note that the distribution of the number of
set top boxes and residential gateway per spatial aggregation is not
uniform (shown in Fig. 7), which indicates that we cannot directly
apply the standard hierarchical heavy hitter detection algorithms.
3. THE DESIGN OF GIZA
In this section, we present the design of Giza, a multi-resolution
data analysis infrastructure for analyzing and troubleshooting per-
formance problems in IPTV networks. Giza includes a suite of sta-
tistical techniques for prioritizing various performance issues (i.e.,
identifying prevailing and chronic performance-impacting condi-
tions), event-correlation detection, dependency-graph reduction, causal-
ity discovery and inference.
As mentioned earlier, one of the key challenges in managing
IPTV service is its massive scale (particularly in terms of the net-
work edge devices) and hence its overwhelming amount of perfor-
mance monitoring data such as device usage and error logs, user
activity logs, detailed network alarms and customer care tickets. It
is very important for network operators to quickly focus on more
prevailing and repeating problems and to automate the process of
root cause analysis and troubleshooting as much as possible. We
design Giza to address such need of IPTV network operators.
Fig. 8 shows the overall architecture of Giza. The inputs to Giza
are performance impairment events and the speciﬁc time frame of
interest. For example, the events could be STB crashes recorded
in device logs, or CPU spikes observed at CO devices, or customer
complaints recorded in tickets, to name a few. Given the input,
Giza ﬁrst performs a multi-resolution analysis and eliminates areas
of network locations that do not have signiﬁcant observation of the
input symptom events. This is achieved by a hierarchical heavy hit-
ter detection component. The locations may be at any aggregation
level in the DSLAM, CO, Metro and Region hierarchy. Focusing
on the locations where symptom events are dominant greatly re-
duces the amount of data processing required for later steps. Next,
Giza explores a wide range of other event series extracted from var-
ious system logs and performance monitoring data and identiﬁes
the ones that are correlated with the symptoms using a statistical
correlation discovery component. This is done at an appropriate
spatial granularity in which the symptom events and the diagnos-
tic events can relate. Furthermore, Giza applies a novel causality
discovery approach to deduce the causality graph and identiﬁes the
potential root causes. The output of Giza is a causality graph that
has tremendous value in the troubleshooting efforts of network op-
erators.
3.2 Hierarchical Heavy Hitter Detection
In this subsection, we present the design of our hierarchical heavy
hitter detection component. The goal of this component is to iden-
tify spatial locations where a given performance impairment is preva-
lent and recurring, and prune the remaining locations, so as to re-
duce the complexity for the subsequent phases.
Detecting heavy hitters (i.e., locations that manifest signiﬁcant
occurrences of the symptom network event) in a single dimensional
data typically involves setting an appropriate threshold. For exam-
ple, if crash on STBs is the symptom event, then heavy hitter STBs
can be deﬁned to be those which crash more than k times in an
hour. However, considering the intrinsic hierarchical structure of
an IPTV network, a proper deﬁnition of heavy hitter should include
not only the event frequency (temporal property) but also the den-
sity concentration (spatial property). For example, a DSLAM with
23510 STBs that experiences 1000 crashes is more signiﬁcant than a
DSLAM with 1000 STBs having the same number of crashes in the
same period. A concentration reported at a lower level in the hier-
archy provides more speciﬁc information, hence is more valuable,
than a heavy hitter reported at a higher level. Bearing these design
considerations in mind, we next present our signiﬁcance test for a
hierarchical heavy hitter.