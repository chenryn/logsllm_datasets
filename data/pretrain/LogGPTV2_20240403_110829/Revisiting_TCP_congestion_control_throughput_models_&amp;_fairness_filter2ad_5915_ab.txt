properties at scale.
Congestion in the core: Many prior CCA efforts implicitly as-
sume that Internet congestion occurs mostly at the network edge,
evaluating only tens of flows at the scale of a hundred Mbps [18, 26,
37]. However, both older and more recent work [11, 21] show that
there is persistent congestion on inter-provider links in the Internet
core. This is significant in the light of analysis that the properties
of CCAs can change as network parameters scale; e.g., the work
97
Revisiting TCP Congestion Control Throughput Models & Fairness Properties At Scale
IMC ‚Äô21, November 2‚Äì4, 2021, Virtual Event, USA
p
Packet Loss
CWND Halving
EdgeScale
1.78
1.47
CoreScale Flow Count
1000
3.95
1.36
3000
3.64
1.36
5000
3.24
1.34
Table 1: Deriving the Mathis constant ùê∂ using the packet loss
rate results in different flow count-dependent constants in
CoreScale vs EdgeScale, while using the CWND halving rate
results in closer and more consistent values across settings
and flow counts.
Figure 2: The median prediction error for the Mathis model
in CoreScale is ‚â§ 10% using CWND halving rate, but 45% to
55% with packet loss rate. In EdgeScale both packet loss rate
and CWND halving rate result in <10% error.
of Appenzeller et al. [13] finds that when thousands, rather than
tens, of NewReno flows compete over a ‚Äúcore‚Äù bottleneck link, they
desynchronize, allowing the use of smaller router buffers compared
to recommendations in the edge setting.
CCAs in data centers and high-bandwidth settings: While
past research has investigated CCA properties in the data center
setting [12, 19, 30, 33, 43], we are interested in the wide-area setting,
which sees higher RTTs and has routers with larger buffers [13, 38].
There is also work on CCA fairness at Gbps bandwidths [10, 28,
32, 39], but they typically evaluate tens to a few hundred flows,
not thousands. To the best of our knowledge, the Mathis model
and fairness properties of CCAs when thousands of flows compete
on Gbps links have not been rigorously studied in the wide-area
setting.
3 Problem Scope and Methodology
In this section, we define the scope and methodology of our
analysis, its relevance, and its limitations.
3.1 Problem Scope
Before we begin, we concretely define the two settings of interest
for our study:
‚Ä¢ EdgeScale: This represents the edge-link setting with a bottle-
neck bandwidth of 100 Mbps with 2 to 50 competing flows and
a 3MB buffer.
‚Ä¢ CoreScale: The ‚Äúat scale" setting with a bottleneck bandwidth
of 10 Gbps [4], 1000 to 5000 competing flows, and a 375MB
buffer.
In both cases, a drop-tail queue is used at the bottleneck link, and
the buffer size is approximately 1 BDP (bandwidth-delay product)
based on the bandwidth of the bottleneck link and assuming a
maximum RTT of 200ms. We choose this size based on the rule of
thumb used to size router buffers [13]. It is the smallest buffer that
would allow a single NewReno flow to saturate the link. While past
work has shown that smaller buffers equal to a fraction of the BDP
98
are sufficient to ensure upto 99% link utilization at scale [13, 16],
recent work [38] has found that in practice ISPs still use extremely
large buffers.
CCAs Analyzed: We focus our evaluation on three popular CCAs:
NewReno, Cubic, and BBR. These CCAs are chosen based on both
the depth of their research literature and their widespread usage
on the Internet today [5, 40, 40]:
1. NewReno is a classic example of a loss-based CCA. It is widely
used today, most notably by Netflix [40], which is believed to
make up 13% of all traffic on the Internet [5].
2. Cubic is another loss-based CCA [26]. It is currently the default
CCA on Linux and Windows Server and is the standard baseline
almost every new CCA is compared with [18, 35, 39, 48].
3. BBR is a comparatively new CCA proposed by Google [18].
However, it is used by YouTube [40], which accounts for 6% of
all Internet traffic [5]. While a new version ‚ÄòBBRv2‚Äô [2] exists,
it is currently a work in progress. We, therefore, focus on the
well-studied BBRv1 [28, 45, 47, 48].
3.2 Setup and Methodology
Studying TCP properties at scale is challenging; e.g., traditional
packet-level simulators such as ns-3 [27] take several days for a
simple Gbps-scale experiment [31], and past work on data-center
networking that uses such simulators at scale typically run experi-
ments modeling just a few seconds [34]. Approximations (e.g., flow
or fluid model simulations [1]) may not accurately capture fine-
grained dynamics. To achieve both fidelity (e.g., running actual TCP
stacks) and scale, we use a simple testbed setup described below.
Our testbed uses a physical network with a dumbbell topology,
with ten sender-receiver node pairs connected to a BESS software
switch [3], as seen in Figure 1. We choose this topology as it is a
common topology used to evaluate throughput models and fairness,
and has been used to model a wide variety of scenarios [26, 37,
39, 45]. The bottleneck bandwidth for the experiments is varied
between EdgeScale and CoreScale by changing the bandwidth and
buffer size on the BESS software switch. We use a software switch
as it allows greater control over the queue size and bottleneck
bandwidth than the physical switches available to us, while still
being closer to using physical network elements than a simulator
like ns-2 [6] or ns-3 [27]. The edge link bandwidths between the
sender/receiver nodes and bottleneck link at the BESS switch is
always 25 Gbps, which guarantees that congestion occurs at the
BESS switch. The base RTT of flows is set using netem [8] to add the
appropriate delay at the receiver, similar to past work [39, 45, 51].
We calculate the packet loss rate by logging packet drops at the
bottleneck queue in the software switch, and use the Linux tool
tcpprobe [9] to measure the congestion window halving rate to
validate the Mathis throughput model. The testbed was hosted on
CloudLab [22].
All TCP flows are distributed equally across each of the sender-
receiver pairs and send infinite data, as common in past experi-
ments [26, 28, 39, 45, 51]. The flows run for a maximum duration
of 3 hours, significantly longer than past studies [26, 28, 45, 48],
or until the metric being evaluated changes by less than 1% over
20 minutes. When an experiment starts, each flow waits a random
1000300050000204060Packet Loss RateCWND Halving RateHome, Packet LossHome, CWND HalvingMathis Prediction ErrorFlow CountError (%)IMC ‚Äô21, November 2‚Äì4, 2021, Virtual Event, USA
Philip et al.
4 Revisiting the Mathis Throughput Model
Background: The Mathis model [37] predicts the throughput of a
NewReno flow as a function of loss (ùëù) and round-trip time (RTT).
It depends on two constants: ùê∂, which may be different for different
CCAs, and MSS (maximum segment size), which in our case is fixed
to 1448 bytes. The Mathis model equation can be expressed as:
(1)
Throughput =
MSS ‚àó ùê∂
RTT ‚àó ‚àöùëù
The original paper by Mathis et al. [37] states that ùëù refers to the
congestion event rate. This can be interpreted in one of two ways: (a)
the congestion window (CWND) halving rate or (b) the packet loss
rate. While the original paper states that the CWND halving rate
should be used for TCP with selective ACKs, subsequent research
has often applied the packet loss rate instead [44, 46]. We, therefore,
evaluate the Mathis equation with both the packet loss rate and the
CWND halving rate.
The original paper derives a constant ùê∂ = 0.94 for NewReno
with delayed and selective ACKs [37]. The paper also demonstrates
how to derive ùê∂ empirically for varying NewReno configurations.
For our modern NewReno [7, 36] stack we derive ùê∂ empirically
following the methodology described by Mathis: we calculate the
ùê∂ which minimizes the least squared prediction error of the Mathis
equation at a given flow count and setting. For the following results,
all flows run NewReno and have a 20ms RTT.
Finding 1: Deriving ùê∂ using packet loss rate results in flow-
count dependent values and different values in CoreScale vs.
EdgeScale. Using CWND halving rate produces consistent ùê∂
values across both settings and flow counts. (Table 1)
Table 1 shows the empirically derived ‚Äúbest-fit‚Äù constant ùê∂ for
NewReno in a few example settings. We see two main observations
here. First, when using the packet loss rate the ùê∂ value is quite dif-
ferent between EdgeScale and CoreScale and also changes between
different flow counts in CoreScale. This violates the Mathis model
which states that ùê∂ depends only on the CCA being used, and
should not change with the number of competing flows or bottle-
neck bandwidth. However, using the CWND halving rate produces
a more consistent constant that changes only slightly between the
EdgeScale and CoreScale, and does not change significantly between
flow counts within CoreScale.
Finding 2: Using the CWND halving rate results in accurate
predictions (‚â§ 10% median error) in CoreScale; using packet
loss rate results in 45%-55% median error. In EdgeScale, how-
ever, both are accurate. (Fig 2)
Fig 2 shows the median Mathis prediction error at different flow
counts in CoreScale, while the two horizontal lines represent the
median prediction error obtained in EdgeScale. These results show
that the Mathis model does indeed hold at scale, as long as we use
the CWND halving rate for ùëù. The 45%-55% median error when
using the packet loss rate implies it cannot be used to accurately
predict NewReno throughput at scale, even though the packet loss
rate works well in EdgeScale. The error at scale is foreshadowed by
the significantly different ùê∂ values derived across settings and flow
counts when using the packet loss rate.
(a) CoreScale
(b) EdgeScale
Figure 3: The ratio between packet losses and congestion
events (i.e., CWND halvings) changes between CoreScale and
EdgeScale, and across across different flow counts within
CoreScale.
(a) CoreScale
(b) EdgeScale
Figure 4: BBR shows intra-CCA unfairness in CoreScale, with
JFIs as low as 0.4. Milder unfairness can also be seen beyond
10 flows in EdgeScale, with JFIs as low as 0.7.
period of time between 0 and 2 minutes before it establishes a con-
nection with the receiver, and the throughput obtained by all flows
in the first 5 minutes of the experiment is ignored.
Limitations: As observed by many others, capturing the dynamics