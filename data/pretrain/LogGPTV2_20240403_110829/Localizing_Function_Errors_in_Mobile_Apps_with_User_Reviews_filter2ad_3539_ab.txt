extracting the verb phrase “ﬁnd contact” from the review,
we conduct static analysis on code to ﬁnd the classes
information.
that query content provider
Eventually, we
the
method
ContactsDatabase.queryTextSecureContacts()
since
provider with URI
 to get contact.
Example
uploading photos error.”
ReviewSolver Since some errors in reviews involve send-
ing/receiving intents, we ﬁnd the classes that contain such
intents. For example, after extracting camera related verb
phrase “upload photo” from the review, we conduct static
analysis to ﬁnd the classes that send camera related intents.
We recommend developer to investigate the method Medi-
aPickerActivity.openCamera() because it will send an intent
with action android.media.action.VIDEO_CAPTURE
to other apps.
Example 5 com.fsck.k9: “I like the app, but I receive
an error message saying ”Failed to send some messages” EV-
ERY time I send an email.”
ReviewSolver If the error reviews list the error messages
from the apps, we can look for such messages in the app.
For example, after determining the error message in the
review, we locate the class that shows this message, and
eventually recommend the developer to examine the class
com.fsck.k9.notiﬁcation.SendFailedNotiﬁcations since it raises
this message.
org.mariotaku.twidere:
“Update:
content
4
III. SYSTEM DESIGN
A. System Overview
Fig. 1 shows the procedure of ReviewSolver. After
crawling reviews from Google Play, the review analysis mod-
ule identiﬁes function error reviews (Section III-B). The static
analysis module extracts useful information from the exe-
cutable of an app (Section III-C). Combining the information
from reviews and code, ReviewSolver maps the function
error reviews to the problematic code (Section IV).
B. Review Analysis
The review analysis module identiﬁes the function error
related reviews from those negative reviews (i.e., those with
420
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:51:06 UTC from IEEE Xplore.  Restrictions apply. 
Apktool
AndroidManifest.xml
Dex 
Static 
Analysis
GUI Structure
User Reviews
Review Analysis
Permissions
Activities
APIs/URIs/Intents
Error Messages
Class Names and 
Method Names
Visible Information
Invisible Information
Function Error 
Related Reviews
Localizing 
Function 
Errors
Classes 
Related to 
Function 
Errors
Fig. 1. Overview of ReviewSolver: Localizing Function Errors
1-2 stars), and then extracts the verb phrases and noun phrases
from such reviews to facilitate localizing function errors.
Pre-processing user reviews We remove the non-ASCII char-
acters and split the remaining content into distinct sentences
by using NLTK [17]. To remove typos, we leverage the edit
distance [18] to discover the correct word if the word is
not found in dictionary. Abbreviations are replaced with their
original words (e.g., “pls” to “please”, “u” to “you”). For
each sentence in the review, we leverage Stanford Parser [19]
to construct the parse tree and the typed dependency among
words.
The parse tree contains the phrases of the sentence and the
Part Of Speech (POS) tags of words. Each phrase occupies one
line. For example, NP in Fig.2 means noun phrase and VP in
Fig.2 refers to verb phrase. The typed dependency relation
refers to the grammatical relation between two words [20].
For example, dobj in Fig.2 means direct object.
Parse Tree:
(ROOT
  (S
    (NP (DT the) (NN app))
    (VP (VBZ does) (RB not)
      (VP (VB contain)
        (NP (DT any) (NNS bugs))))))
Typed Dependency:
nsubj
ROOT
dobj
the
app
does not
contain
any
bugs
det
neg
aux
det
Fig. 2. Syntactic Analysis: Parse tree and typed dependency of the sentence:
“the app does not contain any bugs”.
Identifying function error reviews We use supervised ma-
chine learning algorithms to identify the function error reviews
described in Section II. In particular, we use the TF-IDF
values, N-Grams (N=2,3) as features, because these features
are widely used in text classiﬁcations based on supervised
machine learning [16], [21], [22], [23]. TF-IDF (i.e., Term
Frequency-Inverse Document Frequency) measures how im-
portant a word is to a review [24]. It is calculated by multi-
plying term frequency (TF) and inverse document frequency
(IDF). TF measures how frequently a word occurs in a review.
IDF measures how important a word is. The frequent words
are less important.
421
T F (t) =
N umber of times word t appears in a review
T otal number of words in the review
IDF (t) = log
T otal number of reviews
N umber of review with word t in it
N-Grams are a set of co-occurring words within a given
window [25]. For example, given the sentence shown in Fig. 2,
we extract “the app does”, “app does not”, “does not contain”,
“not contain any”, and “contain any bugs” as N-Gram features
(N=3). Note that without conducting syntactic analysis on each
sentence, the TF-IDF (which only considers distinct words)
and N-Gram features (which has ﬁxed window size) cannot
recognize the relation between negation words (e.g., “not”)
and error-related words (e.g., “bug”). Therefore, the classiﬁer
(e.g., those in [16], [21]) will regard the sentence of Fig. 2 as
a function error review by mistake (i.e., a false positive). To
address this issue, we analyze the typed dependency relations
of the sentence. Since both “bug” and “not” are related to verb
“contain”, we regard “bug” as being related to “not”, and thus
remove the word “bug” related features before classiﬁcation.
To train a classiﬁer for identifying function error reviews,
we create a training dataset with 700 positive reviews and
700 negative reviews. We test multiple algorithms (including,
random forest, SVM, max entropy, boosted regression trees)
and adopt the boosted regression trees [26] because it has good
performance in text classiﬁcation [16]. The boosted regression
trees aggregate the result from a sequence of decision trees.
To train an expressive model, the algorithm iterates multiple
times. During each iteration, this algorithm selects the feature
that best partitions the data to create tree models. It will
also adjust the weight of the samples classiﬁed incorrectly
to enable the next tree to correctly classify them [27]. When
performing the classiﬁcation, we did not split reviews into
sentences because considering individual sentences may miss
the context information in other sentences.
Extracting verb phrase and noun phrase To capture the
semantic information of function error reviews, we extract the
verb phrase and noun phrase by using the parse tree and typed
dependency relations. The verb phrase contains a verb and its
object (e.g., “import contact”). The noun phrase contains a
word or group of words containing a noun (e.g., “the last
phone call”). We do not employ the bag-of-words model to
represent the semantic information of review because the word
frequency cannot capture the part-of-speech (POS) tags of
words. For example, although both “contact me if you like”
and “import contact” contain the word “contact”, the former
is a verb (cannot be mapped to the behavior of the app) and
the latter is a noun (can be mapped to the access of contact
list through content provider in the app). Since the verb/noun
phrases retain the part-of-speech tags of words, we can remove
the false mappings from reviews to code(Section IV).
The verb phrase is extracted from typed dependency. For
the sentence shown in Fig.2, as the verb is “contain” and the
object is “any bugs”, we acquire the verb phrase (i.e.,“contain
any bugs”) by checking the typed dependency relation (i.e.,
dobj, nsubjpass) between words. The noun phrase is obtained
through parse tree. For each line of the parse tree, if the line
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:51:06 UTC from IEEE Xplore.  Restrictions apply. 
starts with NP (i.e., noun phrase), the phrase of the line will
be extracted as noun phrase. For example, for the sentence of
Fig.2, we extract two noun phrases (i.e., “the app” and “any
bugs”) from the parse tree.
C. Static Analysis
Given an app, we analyze its AndroidManifest.xml
ﬁle and Dex ﬁle to extract seven kinds of information to facil-
itate mapping function error reviews to code. In detail, we use
Vulhunter [28] to process the apk and create android property
graph (APG) of the app. APG combines abstract syntax tree
(AST), method call graph (MCG), and data dependency graph
(DDG). When building the DDG, we leverage the IccTA [29]
to identify the target component of intent.
Extracting permissions and activities We parse the
AndroidManifest.xml ﬁle to extract
the permissions
and activities. The starting activity is identiﬁed through the
action “android.intent.action.MAIN” and the category “an-
droid.intent.category.LAUNCHER” in the intent ﬁlter.
Extracting APIs/URIs/intents,
error message,
and
class/method names We analyze the APG to identify three
kinds of information (i.e., APIs/URIs/intents, class/method
names, error messages).
To identify APIs, we check all the assign statements and
invoke statements contained in AST. If the invoked method
name is a framework API, we record it.
To identify URIs, through which apps can get information
(e.g., contacts), we ﬁrst determine the content provider oper-
ations (e.g.,ContentResolver.query()), and then conduct back-
ward taint analysis by traversing the DDG [30]. In particular,
the traversal starts from the statements related to content
provider operations and ends at the statements that deﬁne local
variables. All URI used in code are recorded. PScout [31] uses
static analysis to obtain the mapping between the permissions
and their related APIs/URIs. After discovering the APIs/URIs
used in code, we leverage the mappings proposed by PScout
ﬁnd out the permissions used in code.
By sending the intents to other apps, an app can call
other apps to perform speciﬁc tasks. For example (Fig. 3),
the app com.fs.catw sends out an intent (i.e.,
type is
android.media.action.IMAGE_CAPTURE) to the the
camera app for capturing an image and obtaining it. To
identify the intents sent by the app, we ﬁrst collect all intent
related statements (e.g, Activity.startActivityForResult()), and
then perform backward taint analysis on it. The sources
this taint analysis are the statements that call APIs
of
to send out
that
create new variables (i.e., statements that do not con-
tain any outgoing data dependency relation). All string
parameters appeared in the path will be recorded (e.g.,
android.media.action.IMAGE_CAPTURE in Fig. 3).
intent. The sinks are the statements
Data 
Dependency
     public void onClick(View v) {
            Intent v3;
            .......
            v3 = new Intent("android.media.action.IMAGE_CAPTURE");
            v3.putExtra("output", CatWangActivity.mCapturedImageURI);
            this.startActivityForResult(v3, 1888);
            ......
        }
Fig. 3. Code Example: Send intent to take picture
422
If error occurs, an app may notify user the detail [32] by
using AlertDialog, TextView, or Toast. To identify the error
message pop-up in each class, after determining the state-
ments that invoke error message related APIs (e.g., AlertDi-
alog.setTitle(), AlertDialog.setMessage(), TextView.setError(),
and Toasts.makeText()), we conduct backward taint analysis.
The sources of this taint analysis are the statements that call
the APIs to pop-up error message. The sinks are also the
statements that create new variables. All the string parameters
appeared on the path are recorded.
After building the AST, we record all class names and
method names. Since class and method names may provide
information about the corresponding classes and methods [33],
we extract them and use them to locate app speciﬁc task errors
described in user reviews (Section IV-A).
Extracting visible/invisible label
information from GUI
We ﬁrst recover the structure of each activity, and then
extract the visible and invisible label information from it. The
former includes the texts shown in GUI. If the user review
mentions such information, we look for the UI component
that contains the corresponding text for localizing problematic
code. The invisible label
information refers to the ids of
widgets/UI components in the GUI. Since developers may
include the purpose of the widget when setting the id (e.g.,
quoted_text_edit), we can use them to understand the
function of each widget (e.g., “edit text”).
it
or
the
Developers
dynamically
through APIs
change
[34]. We
enhance GATOR [35]
can design GUI via
layout XML
ﬁle
(e.g.,
TextView.setText())
to
recover the GUI structure of all activities. GATOR ﬁrst parses
the manifest ﬁle (to identify the activities), the layout ﬁle
(to get the parent-child relationship between widgets), and
resource id ﬁle (to obtain the mapping between id names
and values). Then,
inspects each method and conducts
reference analysis to construct the constraint graph of GUI
related objects. Finally, GATOR combines the information
obtained from the layout ﬁle and the dynamically generated
widgets inferred from the constraint graph to reconstruct the
GUI structure. We enhance GATOR from four aspects.
it
attribute
that
in
the
each
namespace
First, we empower GATOR to handle the self-deﬁned
“xmlns”
deﬁnes
AndroidManifest.xml
attributes provided
android:id),
namespace-prefix
as
namespaceURI
However,
them to the namespaces deﬁned by
namespace. Note
the
and layout ﬁle. To use
by Android
developers
as
“http://schemas.android.com/apk/res/android”.
developers can set
themselves whereas GATOR does not handle them.
android:name,
set
(e.g.,
can
“android”
the
the
intrinsic
and
set
Second, we enhance GATOR to process the “activity-alias”
tag. The element with this tag contains two attributes (i.e.,
namespace:name and namespace:target), meaning
that the two activities deﬁned in the attributes have the alias
relationship. Without considering the “activity-alias” tag, we
cannot identify the child widget of the alias activities.
Third, GATOR only identiﬁes the mapping between id names
and values by parsing the the res/values/public.xml
but misses the ﬁelds of automatically generated classes (e.g.,
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:51:06 UTC from IEEE Xplore.  Restrictions apply. 
some
“android.R$...”, “com.android.internal.R$...”). Thus,
widget obtained in code by calling ﬁndViewById(int) cannot
be located in the corresponding GUI structure. We parse these
generated classes to get the ids of these ﬁelds.
Fourth, the android:text attribute deﬁnes the text dis-
played while the android:hint attribute provides sug-
gestive information. Since both of them provide hints for
correlating function error reviews, we extend GATOR to parse
them. Fig.4 shows an example of layout ﬁle that contains these
two attributes.
Invisible information
Visible information
Fig. 4. Snippet of a layout ﬁle
After reconstructing the GUI structure of each activity, to
identify the text displayed by the app, we ﬁlter out the widgets
that are not subclasses of TextView classes. For the remaining
widgets (e.g., Button), we extract the visible label information
from their android:text and android:hint if any. For