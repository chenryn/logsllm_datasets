nothing case, this means each tenant receives four work-
ers processes; in the shared NGINX and Linux case, the
tenants are multiplexed on four total workers). We direct
ApacheBench tests concurrently against each tenant.
Figure 4 compares the mean latency and aggregate
throughput of these three deployments, scaling the number of
tenants from one up to six. After an initial dip at two tenants,
Linux is able to increase throughput with modest increases
to request latency; shared NGINX Graphene-crypt maintains
a more-or-less constant overall throughput at the cost of in-
creasing latencies, while the shared nothing conﬁguration is
unable to maintain throughput past two tenants.
USENIX Association
29th USENIX Security Symposium    745
 0 500 1000 1500 2000 2500 3000 35001 KiB10 KiB100 KiB# Workers1248Throughput (requests/sec)LinuxLinux-keylessGraphene-cryptGraphene-crypt ExitlessGraphene-vericrypt 0 500 1000 1500 2000 2500 30001 KiB10 KiB100 KiBTime per request (ms)Downloaded file size 0 500 1000 1500 2000 2500 3000 35001246Throughput (requests/sec)Linux (shared NGINX)Graphene-crypt (shared NGINX)8101418Graphene-crypt (shared nothing)8163248 0 200 400 600 800 1000 1200 1400 16001246Time per request (ms)Number of tenants81014188163248Figure 6: RPC latency versus payload size. The numbers
above the bars are overheads compared to non-SGX.
6.5 Micro-benchmarks
We now evaluate the various subcomponents of a Phoenix
conclave to provide a more ﬁne-grained explanation of our
performance results. For each micro-benchmark, we com-
pare the performance of the component running in three en-
vironments: outside an enclave (non-SGX), inside an enclave
with normal system calls (SGX), and inside an enclave with
exitless system calls (exitless). Each micro-benchmark tool
runs on the same machine as the component we are testing.
6.5.1 Remote Procedure Calls
To understand the cost of the RPC mechanism used by the
kernel servers, absent from any additional server or client-
speciﬁc processing, we design an experiment 6 where a client
issues an RPC to download a payload 100,000 times, and
compute the mean time for the RPC to complete. We vary
the payload size from 0-bytes to 1 MiB.
Figure 6 shows that, in general, SGX incurs a much higher
latency overhead than exitless but that this gap narrows as the
payload size increases, and that at 1 MiB payloads, exitless
actually performs worse than normal ocalls.
Higher payload sizes result in greater latencies for the un-
derlying system call; if this latency exceeds the spinlock du-
ration, the spinlock falls back to sleeping on the futex, effec-
tively having spun in vain. For payload sizes of 0 through
100 KiB, exitless falls back to the futex less than 30 times
for both the server and client; in contrast, for the 1 MiB case,
nearly every RPC uses the futex (on average, 91,285 times
for the server, and 97,881 for the client).
6.5.2 Kernel Servers
fsserver We use fio [75] to measure the performance of
sequential reads to a 16 MiB ﬁle hosted on a nextfs server,
over 10 seconds; each read transfers 4096 bytes of data. fio
6For an apples-to-apples comparison between SGX and non-SGX envi-
ronments, we benchmark at the application layer. This differs slightly from
conclaves, where the kernel servers are also implemented at the application
level, but the fsserver and memserver clients are subsystems of Graphene.
Figure 5: Effect of ModSecurity rule count on NGINX per-
formance. NGINX runs with a single worker, and we fetch a
1 KiB ﬁle.
For the conclave deployments, we also measure the num-
ber of SGX paging events using the kprobe-based technique
from Weichbrodt et al. [73, 74]. For both the shared-nothing
and shared-NGINX deployments of Graphene-crypt, these
events remain under 10,000 up to four tenants; at six tenants,
the shared NGINX deployment incurs on average 10,507
SGX paging events, whereas shared nothing incurs a stagger-
ing 4.35 million as the working sets of 48 enclaved processes
contend for the limited 93 MiB of EPC memory.
6.4 Web Application Firewall
Finally, we evaluate the cost of running the ModSecurity web
application ﬁrewall (WAF) in tandem with NGINX. Each of
our ModSecurity rules examines the request’s query string
for a unique, blacklisted substring. We increase the number
of rules and observe the effect on the server’s HTTPS request
throughput and latency in Figure 5 for normal Linux and
Graphene-crypt, both running as standalone, non-caching,
servers. We see that just enabling ModSecurity results in a
5% decrease in throughput for Linux, and 16% decrease for
Graphene-crypt. At 1000 rules, the relative costs for Linux
and Graphene-crypt converge, as the throughput of each is
2/3 of that when ModSecurity is off, and latency is 1.5×
slower. At 10,000 rules these relative costs increase substan-
tially, to just 14% of the throughput and 7× the latency com-
pared to when ModSecurity is disabled.
746    29th USENIX Security Symposium
USENIX Association
 0 100 200 300 400 500 600 700 800 900 1000 1100ModSec-Off100101102103104Throughput (requests/sec)LinuxGraphene-crypt 0 200 400 600 800 1000 1200 1400 1600 1800ModSec-Off100101102103104Time per request (ms)Number of ModSecurity rules10010110210310401 KiB10 KiB100 KiB1 MiBRPC latency (µs)Download payload sizeNon-SGXSGX3.0x3.2x4.1x4.1x3.9xExitless1.5x1.5x2.8x3.8x4.1xFigure 7: CDFs of read operation latency (s) for a 10-
second test that repeatedly reads 4096-bytes from a nextfs
server, for each block device implementation.
Figure 8: Total throughput for a 10-second test that repeat-
edly reads 4096-bytes from a nextfs server, for each block
device implementation.
runs inside an enclave, uses exitless system calls, and invokes
read operations from a single thread.
Figure 7 shows the read latencies for each variant of the
ﬁlesystem. Compared to bd-std, bd-crypt adds relatively
small overheads, whereas bd-vericrypt shows nearly an or-
der of magnitude slow down due to the Merkle tree lookups,
dependent on the size of the tree’s in-enclave LRU cache.
Figure 8 shows the associated throughput. For compari-
son, the enclaved versions of bd-crypt and bd-vericrypt have
20× and 97× less throughput, respectively, than Linux’s
standard ext4 ﬁlesystem (954 MiB/s, on our test machine).
memserver Figure 9 shows the mean time for a process to
evaluate a critical section (a lock and unlock operation pair)
over shared memory provided by the memserver, based on
10,000 runs. We also vary the size of the memory segment to
observe its effect on the run time.
We make two observations. First, since mmap allocates in
page sizes (4096-bytes), the measurements for a 1 KiB and
Figure 9: Mean wall clock time (s) to process a critical sec-
tion.
OpenSSL
non-SGX
860.92
keyserver
non-SGX
933.42
(1.08×)
exitless
932.60
(1.08×)
Table 3: Mean wall clock time (s) to compute an RSA-2048
signature using default OpenSSL (left) and the keyserver.
The last row is overhead compared to OpenSSL.
SGX
965.32
(1.12×)
10 KiB shared memory region are nearly identical; other-
wise, the execution times scale linearly in accordance with
the memory size. Second, starting at 100 KiB, the sm-
vericrypt and sm-crypt implementations, which represent the
canonical memory replica as an encrypted host ﬁle, show an
order-of-magnitude improvement over sm-vericrypt-basic,
which uses EPC memory to store the canonical replica and
transfers the replica over interprocess communication.
keyserver
To evaluate the keyserver’s performance, we
use the openssl speed command to measure the time to
compute an RSA-2048 signature. For all tests, the openssl
speed command runs outside of an enclave, and measures
the number of signatures achieved in 10 seconds.
We present the results in Table 3. The keyserver itself uses
OpenSSL’s default RSA implementation; compared to the
RPC micro-benchmarks in Figure 6, we again see that the
raw time overheads are consistent with the RPC latencies.
timeserver We evaluate the timeserver by measuring the
elapsed time to invoke gettimeofday one million times in a
tight loop, and then compute the mean for a single invocation.
In Table 4, we list the mean time for an invocation of
gettimeofday in Linux (non-SGX), and in Graphene, us-
ing both the host time and the timeserver. Note that non-SGX
calls to gettimeofday are nearly free due to vDSO.7
The difference between the exitless and normal ocalls is
roughly the round-trip cost of exiting and returning to an en-
clave; this is consistent with prior work [70, 71, 73] that puts
this cost at 8000 cycles (3.077 s on our test machine). The
timeserver cost is dominated by the signature computation;
exitless calls to the timeserver actually hurt performance, as,
7A system call implementation that uses a shared memory mapping be-
tween the kernel and application, rather than a user-to-kernel context switch.
USENIX Association
29th USENIX Security Symposium    747
 0 0.2 0.4 0.6 0.8 1 10 100 1000bd-stdCDFNon-SGXSGXExitless 0 0.2 0.4 0.6 0.8 1 10 100 1000bd-cryptCDF 0 0.2 0.4 0.6 0.8 1 10 100 1000bd-vericryptCDFSequential read latency (µs) 0 10 20 30 40 50 60 70 80 90 100 110Non-SGXSGXExitlessThroughput (MiB/s)bd-stdbd-cryptbd-vericrypt1001011021031041051061 KiB10 KiB100 KiB1 MiB10 MiBNon-SGXSGXExitlessMean critical sectionexecution time (µs)Shared memory sizesm-vericrypt-basicsm-vericryptsm-crypthost time
SGX
3.467
(133×)
timeserver
SGX
non-SGX
0.026
1,175.622
(45,216×)
exitless
0.757
(29×)
exitless
1,375.607
(52,908×)
to execute
Table 4: Mean wall
gettimeofday. Left: retrieving time from host; Right: re-
trieving from (unenclaved) timeserver. The SGX and exitless
designations refer to the application’s environment. The last
row is overhead compared to non-SGX.
clock time
(s)
due to the signature latency, the Graphene client fails to re-
ceive a response during the spinlock, and falls back to the
more expensive futex sleep operation for every RPC.
7 Conclusion
We have presented Phoenix, the ﬁrst “keyless CDN” that sup-
ports the quintessential features of today’s CDNs. To sup-
port multi-process, multi-tenant, legacy applications, we in-
troduced a new architectural primitive that we call conclaves
(containers of enclaves). With an implementation and evalu-
ation on Intel SGX hardware, we showed that conclaves scale
to support multi-tenant deployments with modest overhead.
Optimizations and Recommendations While Phoenix is
able to achieve surprisingly good performance, further po-
tential optimizations remain, including of SGX. The multi-
tenancy results in Figure 4 show that EPC size limits become
a constraint in environments with multiple enclaved applica-
tions. Conclaves alleviate this to some extent, as the kernel
servers may be run on devices separate from the application,
but splitting the application itself (e.g., the NGINX workers)
across machines is less tractable. Future versions of SGX
should therefore investigate ways of increasing the EPC size.
The cache size sensitivity results in Table 2 show that dis-
tributed shared memory is a challenging performance prob-
lem. Future versions of SGX should investigate features for
mapping EPC pages among multiple enclaves.
While prior work has treated exitless calls as a panacea,
§6.5 shows that they should be a per-system call policy to
reﬂect the application’s workload.
Of course, Phoenix is by no means a drop-in replacement
for today’s CDNs, who have specially optimized web servers
and support a much wider range of features, such as video
transcoding and image optimization. Rather, our results
should be viewed as a proof of concept and a glimmer of
hope: it is not necessary for CDNs to have direct access to
their customers’ keys to achieve performance or apply WAFs.
We view Phoenix—and especially conclaves—as a ﬁrst step
towards this vision. To assist in future efforts, we have made
our code and data publicly available at:
https://phoenix.cs.umd.edu
Acknowledgments
We thank the Graphene creators and maintainers, especially
Chia-Che Tsai, Dmitrii Kuvaiskii, and Michał Kowalczyk,
for their help in understanding Graphene’s internals and de-
bugging numerous issues. We also thank Bruce Maggs, Nick
Sullivan, and the anonymous reviewers and artifact evalua-
tors for their helpful feedback. This work was supported in
part by NSF grants CNS-1816422, CNS-1816802, and CNS-
1901325.
References
[1] Akamai. https://www.akamai.com/.
[2] Cloudﬂare. https://www.cloudflare.com/.
[3] Ilker Nadi Bozkurt, Anthony Aguirre, Balakrishnan
Chandrasekaran, P. Brighten Godfrey, Gregory Laugh-
lin, Bruce Maggs, and Ankit Singla. Why is the Internet
so slow?! In Passive and Active Network Measurement
Workshop (PAM), 2017.
[4] Let’s Encrypt. https://letsencrypt.org/.
[5] Adrienne Porter Felt, Richard Barnes, April King,
Chris Palmer, Chris Bentzel, and Parisa Tabriz. Mea-
suring HTTPS adoption on the Web. In USENIX Secu-
rity Symposium, 2017.
[6] Frank Cangialosi, Taejoong Chung, David Choffnes,
Dave Levin, Bruce M. Maggs, Alan Mislove, and
Christo Wilson. Measurement and analysis of pri-
In ACM
vate key sharing in the HTTPS ecosystem.
Conference on Computer and Communications Secu-
rity (CCS), 2016.
[7] Jinjin Liang, Jian Jiang, Haixin Duan, Kang Li, Tao
Wan, and Jianping Wu. When HTTPS meets CDN: A
In IEEE
case of authentication in delegated service.
Symposium on Security and Privacy, 2014.
[8] David Gillman, Yin Lin, Bruce Maggs, and Ramesh K.
Sitaraman. Protecting websites from attack with secure
delivery networks. IEEE Computer, 48(4), April 2015.
[9] Nick Sullivan.
Keyless SSL: The Nitty Gritty
Technical Details.
Cloudﬂare Blog, September
2014. https://blog.cloudflare.com/keyless-
ssl-the-nitty-gritty-technical-details/.
[10] David Naylor, Kyle Schomp, Matteo Varvello, Ilias
Leontiadis, Jeremy Blackburn, Diego Lopez, Kon-
stantina Papagiannaki, Pablo Rodriguez Rodriguez, and
Peter Steenkiste. Multi-context TLS (mcTLS): En-
abling secure in-network functionality in TLS. In ACM
SIGCOMM, 2015.
748    29th USENIX Security Symposium
USENIX Association
[11] David Naylor, Richard Li, Christos Gkantsidis, Thomas
Karagiannis, and Peter Steenkiste. And then there were
more: Secure communication for more than two par-
ties. In ACM Conference on emerging Networking EX-