93.1
97.6
97.6
97.6
97.6
98.95
98.95
98.95
99.0
99.0
99.0
98.64
99.0
99.0
99.0
99.0
s
-
-
-
-
1.75
-
-
-
-
-
-
4.00
-
-
-
2.00
7.2 Evaluation on CIFAR-10
In Table 5, we summarize the network architectures that we
use for the CIFAR-10 dataset. In this table, BC1 is the bina-
rized version of the architecture proposed by MiniONN. To
evaluate the scalability of our framework to larger networks,
we also binarize the Fitnet [53] architectures, which are de-
noted as BC2-BC5. We also evaluate XONN on the popular
VGG16 network architecture (BC6). Detailed architecture
descriptions are available in Appendix A.2, Table 13.
Table 5: Summary of the trained binary network architec-
tures evaluated on the CIFAR-10 dataset.
Arch.
Previous Papers
Description
BC1
BC2
BC3
BC4
BC5
BC6
MiniONN[9], Chameleon [7],
EzPC [25], Gazelle [10]
Fitnet [53]
Fitnet [53]
Fitnet [53]
Fitnet [53]
VGG16 [54]
7 CONV, 2 MP, 1 FC
9 CONV, 3 MP, 1 FC
9 CONV, 3 MP, 1 FC
11 CONV, 3 MP, 1 FC
17 CONV, 3 MP, 1 FC
13 CONV, 5 MP, 3 FC
Analysis of Network Scaling: Similar to the analysis on
the MNIST dataset, we show that the accuracy of our binary
models for CIFAR-10 can be tuned based on the scaling fac-
tor that determines the number of neurons in each layer. Fig-
ure 11a depicts the accuracy of the BNNs with different scal-
ing factors. As can be seen, increasing the scaling factor en-
hances the classiﬁcation accuracy of the BNN. The runtime
also increases with the scaling factor as shown in Figure 11b
(more details in Table 12, Appendix A.2).
(a)
(b)
Figure 11:
(a) Effect of scaling factor on accuracy for
CIFAR-10 networks. (b) Effect of scaling factor on runtime.
No pruning was applied in this evaluation.
Comparison to Prior Art: We scale the BC2 network with
a factor of s = 3, then prune it using Algorithm 2. Details
of pruning steps are available in Table 10 in Appendix A.1.
The resulting network is compared against prior art in Ta-
ble 6. As can be seen, our solution achieves 2.7×, 45.8×,
9.1×, and 93.1× lower latency compared to Gazelle, EzPC,
Chameleon, and MiniONN, respectively.
Table 6: Comparison of XONN with prior art on CIFAR-10.
Framework Runtime (s) Comm. (MB) Acc. (%)
MiniONN
Chameleon
81.61
81.61
81.61
81.61
81.85
s
-
-
-
-
3.00
544
52.67
265.6
15.48
5.79
9272
2650
40683
1236
2599
EzPC
Gazelle
XONN
7.3 Evaluation on Medical Datasets
One of the most important applications of oblivious infer-
ence is medical data analysis. Recent advances in deep learn-
ing greatly beneﬁt many complex diagnosis tasks that require
exhaustive manual inspection by human experts [55, 56, 57,
58]. To showcase the applicability of oblivious inference in
real-world medical applications, we provide several bench-
marks for publicly available healthcare datasets summarized
in Table 7. We split the datasets into validation and training
portions as indicated in the last two columns of Table 7. All
datasets except Malaria Infection are normalized to have 0
mean and standard deviation of 1 per feature. The images of
Malaria Infection dataset are resized to 32 × 32 pictures. The
normalized datasets are quantized up to 3 decimal digits. De-
tailed architectures are available in Appendix A.2, Table 13
We report the validation accuracy along with inference time
and message size in Table 8.
USENIX Association
28th USENIX Security Symposium    1513
Table 7: Summary of medical application benchmarks.
Task
Arch. Description
Breast Cancer [59]
Diabetes [60]
Liver Disease [61]
BH1
BH2
BH3
Malaria Infection [62]
BH4
3 FC
3 FC
3 FC
2 CONV,
2 MP, 2 FC
# of Samples
Val.
113
153
116
Tr.
453
615
467
24804
2756
Table 8: Runtime, communication cost (Comm.), and accu-
racy (Acc.) for medical benchmarks.
Arch. Runtime (ms) Comm. (MB) Acc. (%)
BH1
BH2
BH3
BH4
97.35
80.39
80.17
95.03
82
75
81
482
0.35
0.16
0.3
120.75
8 Conclusion
We introduce XONN, a novel framework to automatically
train and use deep neural networks for the task of oblivi-
ous inference. XONN utilizes Yao’s Garbled Circuits (GC)
protocol and relies on binarizing the DL models in order to
translate costly matrix multiplications to XNOR operations
that are free in the GC protocol. Compared to Gazelle [10],
prior best solution, XONN achieves 7× lower latency. More-
over, in contrast to Gazelle that requires one round of inter-
action for each layer, our solution needs a constant round of
interactions regardless of the number of layers. Maintaining
constant round complexity is an important requirement in In-
ternet settings as a typical network latency can signiﬁcantly
degrade the performance of oblivious inference. Moreover,
since our solution relies on the GC protocol, it can provide
much stronger security guarantees such as security against
malicious adversaries using standard cut-and-choose proto-
cols. XONN high-level API enables clients to utilize the
framework with a minimal number of lines of code. To fur-
ther facilitate the adaptation of our framework, we design a
compiler to translate the neural network description in Keras
format to that of XONN.
Acknowledgements We would like to thank the anony-
mous reviewers for their insightful comments.
References
[1] Florian Tram`er, Fan Zhang, Ari Juels, Michael K Reiter, and
Thomas Ristenpart. Stealing machine learning models via
prediction APIs. In USENIX Security, 2016.
[2] Zvika Brakerski and Vinod Vaikuntanathan. Efﬁcient fully
homomorphic encryption from (standard) lwe. SIAM Journal
on Computing, 43(2):831–871, 2014.
[3] Zvika Brakerski, Craig Gentry, and Vinod Vaikuntanathan.
(leveled) fully homomorphic encryption without bootstrap-
ping. ACM Transactions on Computation Theory (TOCT),
6(3):13, 2014.
[4] Andrew Yao. How to generate and exchange secrets.
In
FOCS, 1986.
[5] Oded Goldreich, Silvio Micali, and Avi Wigderson. How to
play any mental game. In Proceedings of the nineteenth an-
nual ACM symposium on Theory of computing, pages 218–
229. ACM, 1987.
[6] Pascal Paillier. Public-key cryptosystems based on composite
degree residuosity classes. In International Conference on the
Theory and Applications of Cryptographic Techniques, pages
223–238. Springer, 1999.
[7] M Sadegh Riazi, Christian Weinert, Oleksandr Tkachenko,
Ebrahim M Songhori, Thomas Schneider, and Farinaz
Koushanfar. Chameleon: A hybrid secure computation
framework for machine learning applications.
In ASI-
ACCS’18, 2018.
[8] Payman Mohassel and Yupeng Zhang. SecureML: A system
In IEEE
for scalable privacy-preserving machine learning.
S&P, 2017.
[9] Jian Liu, Mika Juuti, Yao Lu, and N. Asokan. Oblivious
neural network predictions via MiniONN transformations. In
ACM CCS, 2017.
[10] Chiraag Juvekar, Vinod Vaikuntanathan, and Anantha Chan-
drakasan. GAZELLE: A low latency framework for secure
neural network inference. USENIX Security, 2018.
[11] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
Vanhoucke, Andrew Rabinovich, et al. Going deeper with
convolutions. CVPR, 2015.
[12] Vladimir Kolesnikov and Thomas Schneider. Improved gar-
In ICALP,
bled circuit: Free XOR gates and applications.
2008.
[13] Bita Darvish Rouhani, M Sadegh Riazi, and Farinaz
Koushanfar. DeepSecure: Scalable provably-secure deep
learning. DAC, 2018.
[14] Nathan Dowlin, Ran Gilad-Bachrach, Kim Laine, Kristin
Lauter, Michael Naehrig, and John Wernsing. CryptoNets:
Applying neural networks to encrypted data with high
throughput and accuracy. In ICML, 2016.
[15] Michael O Rabin. How to exchange secrets with oblivious
transfer. IACR Cryptology ePrint Archive, 2005:187, 2005.
[16] Yuval Ishai, Joe Kilian, Kobbi Nissim, and Erez Petrank.
Extending oblivious transfers efﬁciently.
In Annual Inter-
national Cryptology Conference, pages 145–161. Springer,
2003.
[17] Donald Beaver. Correlated pseudorandomness and the com-
plexity of private computations. In STOC, 1996.
[18] Gilad Asharov, Yehuda Lindell, Thomas Schneider, and
Michael Zohner. More efﬁcient oblivious transfer and ex-
tensions for faster secure computation. In ACM CCS, 2013.
[19] Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-
Yaniv, and Yoshua Bengio. Binarized neural networks: Train-
ing deep neural networks with weights and activations con-
strained to+ 1 or-1. arXiv preprint arXiv:1602.02830, 2016.
[20] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon,
and Ali Farhadi. XNOR-net: Imagenet classiﬁcation using
binary convolutional neural networks. In European Confer-
ence on Computer Vision, pages 525–542. Springer, 2016.
1514    28th USENIX Security Symposium
USENIX Association
[21] Mohammad Ghasemzadeh, Mohammad Samragh, and Fari-
naz Koushanfar. ReBNet: Residual binarized neural net-
work. In 2018 IEEE 26th Annual International Symposium on
Field-Programmable Custom Computing Machines (FCCM),
pages 57–64. IEEE, 2018.
[22] Xiaofan Lin, Cong Zhao, and Wei Pan. Towards accurate
binary convolutional neural network. In Advances in Neural
Information Processing Systems, pages 345–353, 2017.
[23] Song Han, Jeff Pool, John Tran, and William Dally. Learn-
ing both weights and connections for efﬁcient neural network.
In Advances in neural information processing systems, pages
1135–1143, 2015.
[24] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila,
and Jan Kautz. Pruning convolutional neural networks for re-
source efﬁcient inference. arXiv preprint arXiv:1611.06440,
2016.
[25] Nishanth Chandran, Divya Gupta, Aseem Rastogi, Rahul
Sharma, and Shardul Tripathi. EzPC: Programmable, ef-
ﬁcient, and scalable secure two-party computation.
IACR
Cryptology ePrint Archive, 2017/1109, 2017.
[26] Yehuda Lindell and Benny Pinkas. A proof of security of
Yao’s protocol for two-party computation. Journal of Cryp-
tology, 22(2):161–188, 2009.
[27] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model
inversion attacks that exploit conﬁdence information and ba-
sic countermeasures. In ACM CCS. ACM, 2015.
[28] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly
Shmatikov. Membership inference attacks against machine
learning models. In S&P. IEEE, 2017.
[29] Yehuda Lindell and Benny Pinkas. Secure two-party compu-
tation via cut-and-choose oblivious transfer. Journal of Cryp-
tology, 25(4):680–722, 2012.
[30] Yan Huang, Jonathan Katz, and David Evans. Efﬁcient se-
cure two-party computation using symmetric cut-and-choose.
In Advances in Cryptology–CRYPTO 2013, pages 18–35.
Springer, 2013.
[31] Yehuda Lindell. Fast cut-and-choose-based protocols for
Journal of Cryptology,
malicious and covert adversaries.
29(2):456–490, 2016.
[32] Mihir Bellare, Viet Tung Hoang, Sriram Keelveedhi, and
Phillip Rogaway. Efﬁcient garbling from a ﬁxed-key block-
cipher. In IEEE S&P, 2013.
[33] Moni Naor, Benny Pinkas, and Reuban Sumner. Privacy pre-
serving auctions and mechanism design. In ACM Conference
on Electronic Commerce, 1999.
[34] Samee Zahur, Mike Rosulek, and David Evans. Two halves
make a whole. In EUROCRYPT, 2015.
[35] Peter Rindal.
libOTe: an efﬁcient, portable, and easy to
use Oblivious Transfer Library. https://github.com/
osu-crypto/libOTe.
[36] Ebrahim M Songhori, Siam U Hussain, Ahmad-Reza
Sadeghi, Thomas Schneider, and Farinaz Koushanfar. Tiny-
Garble: Highly compressed and scalable sequential garbled
circuits. In IEEE S&P, 2015.
[37] Franc¸ois Chollet et al. Keras. https://keras.io, 2015.
[38] Ehsan Hesamifard, Hassan Takabi, Mehdi Ghasemi, and Re-
becca N Wright. Privacy-preserving machine learning as a
service. Proceedings on Privacy Enhancing Technologies,
2018(3):123–142, 2018.
[39] Edward Chou, Josh Beal, Daniel Levy, Serena Yeung, Al-
bert Haque, and Li Fei-Fei. Faster CryptoNets: Leveraging
sparsity for real-world encrypted inference. arXiv preprint
arXiv:1811.09953, 2018.
[40] Reza Shokri and Vitaly Shmatikov. Privacy-preserving deep
learning. In ACM CCS, 2015.
[41] Briland Hitaj, Giuseppe Ateniese, and Fernando P´erez-Cruz.
Deep models under the GAN: information leakage from col-
laborative deep learning. In ACM CCS, 2017.
[42] Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio
Marcedone, H Brendan McMahan, Sarvar Patel, Daniel Ra-
mage, Aaron Segal, and Karn Seth. Practical secure aggrega-
tion for privacy-preserving machine learning. In ACM CCS,
2017.
[43] M Sadegh Riazi, Bita Darvish Rouhani, and Farinaz
IEEE Security
Koushanfar. Deep learning on private data.
and Privacy (S&P) Magazine., 2019.
[44] M Sadegh Riazi and Farinaz Koushanfar. Privacy-preserving
In Proceedings of the Inter-
deep learning and inference.
national Conference on Computer-Aided Design, page 18.
ACM, 2018.
[45] Payman Mohassel and Peter Rindal. ABY3: a mixed protocol