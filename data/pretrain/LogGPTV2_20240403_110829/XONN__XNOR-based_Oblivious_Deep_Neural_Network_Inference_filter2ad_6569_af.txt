### 7.2 Evaluation on CIFAR-10

In Table 5, we summarize the network architectures used for the CIFAR-10 dataset. BC1 is the binarized version of the architecture proposed by MiniONN. To evaluate the scalability of our framework to larger networks, we also binarize the FitNet [53] architectures, denoted as BC2-BC5. Additionally, we evaluate XONN on the popular VGG16 network architecture (BC6). Detailed architecture descriptions are available in Appendix A.2, Table 13.

**Table 5: Summary of the Trained Binary Network Architectures Evaluated on the CIFAR-10 Dataset**

| Architecture | Previous Papers | Description |
|--------------|-----------------|-------------|
| BC1          | MiniONN [9], Chameleon [7], EzPC [25], Gazelle [10] | 7 CONV, 2 MP, 1 FC |
| BC2          | FitNet [53] | 9 CONV, 3 MP, 1 FC |
| BC3          | FitNet [53] | 9 CONV, 3 MP, 1 FC |
| BC4          | FitNet [53] | 11 CONV, 3 MP, 1 FC |
| BC5          | FitNet [53] | 17 CONV, 3 MP, 1 FC |
| BC6          | VGG16 [54] | 13 CONV, 5 MP, 3 FC |

#### Analysis of Network Scaling

Similar to the analysis on the MNIST dataset, we demonstrate that the accuracy of our binary models for CIFAR-10 can be tuned based on the scaling factor, which determines the number of neurons in each layer. Figure 11a shows the accuracy of the BNNs with different scaling factors. As can be seen, increasing the scaling factor enhances the classification accuracy of the BNN. The runtime also increases with the scaling factor, as shown in Figure 11b (more details in Table 12, Appendix A.2).

**Figure 11:**
(a) Effect of scaling factor on accuracy for CIFAR-10 networks.
(b) Effect of scaling factor on runtime.
No pruning was applied in this evaluation.

#### Comparison to Prior Art

We scale the BC2 network with a factor of \( s = 3 \), then prune it using Algorithm 2. Details of the pruning steps are available in Table 10 in Appendix A.1. The resulting network is compared against prior art in Table 6. As can be seen, our solution achieves 2.7×, 45.8×, 9.1×, and 93.1× lower latency compared to Gazelle, EzPC, Chameleon, and MiniONN, respectively.

**Table 6: Comparison of XONN with Prior Art on CIFAR-10**

| Framework | Runtime (s) | Communication (MB) | Accuracy (%) |
|-----------|-------------|--------------------|---------------|
| MiniONN   | 81.61       | 544                | 92.72         |
| Chameleon | 81.61       | 9272               | 92.72         |
| EzPC      | 81.85       | 2650               | 92.72         |
| Gazelle   | 81.85       | 40683              | 92.72         |
| XONN      | 3.00        | 1236               | 92.72         |

### 7.3 Evaluation on Medical Datasets

One of the most important applications of oblivious inference is medical data analysis. Recent advances in deep learning have greatly benefited many complex diagnosis tasks that require exhaustive manual inspection by human experts [55, 56, 57, 58]. To showcase the applicability of oblivious inference in real-world medical applications, we provide several benchmarks for publicly available healthcare datasets summarized in Table 7. We split the datasets into validation and training portions as indicated in the last two columns of Table 7. All datasets except Malaria Infection are normalized to have a mean of 0 and a standard deviation of 1 per feature. The images of the Malaria Infection dataset are resized to 32 × 32 pictures. The normalized datasets are quantized up to 3 decimal digits. Detailed architectures are available in Appendix A.2, Table 13.

**Table 7: Summary of Medical Application Benchmarks**

| Task | Architecture | Description | # of Samples | Validation | Training |
|------|--------------|-------------|--------------|------------|----------|
| Breast Cancer [59] | BH1 | 3 FC | 113 | 453 | - |
| Diabetes [60] | BH2 | 3 FC | 153 | 615 | - |
| Liver Disease [61] | BH3 | 3 FC | 116 | 467 | - |
| Malaria Infection [62] | BH4 | 2 CONV, 2 MP, 2 FC | 2756 | 24804 | - |

**Table 8: Runtime, Communication Cost (Comm.), and Accuracy (Acc.) for Medical Benchmarks**

| Architecture | Runtime (ms) | Communication (MB) | Accuracy (%) |
|--------------|--------------|--------------------|---------------|
| BH1          | 97.35        | 82                 | 0.35          |
| BH2          | 80.39        | 75                 | 0.16          |
| BH3          | 80.17        | 81                 | 0.30          |
| BH4          | 95.03        | 482                | 120.75        |

### 8 Conclusion

We introduce XONN, a novel framework to automatically train and use deep neural networks for the task of oblivious inference. XONN utilizes Yao’s Garbled Circuits (GC) protocol and relies on binarizing the DL models to translate costly matrix multiplications into XNOR operations, which are free in the GC protocol. Compared to Gazelle [10], the previous best solution, XONN achieves 7× lower latency. Moreover, unlike Gazelle, which requires one round of interaction for each layer, our solution needs a constant number of interaction rounds, regardless of the number of layers. Maintaining constant round complexity is an important requirement in Internet settings, as typical network latency can significantly degrade the performance of oblivious inference. Additionally, since our solution relies on the GC protocol, it can provide much stronger security guarantees, such as security against malicious adversaries using standard cut-and-choose protocols. XONN's high-level API enables clients to utilize the framework with a minimal number of lines of code. To further facilitate the adoption of our framework, we designed a compiler to translate neural network descriptions in Keras format to that of XONN.

### Acknowledgements

We would like to thank the anonymous reviewers for their insightful comments.

### References

[References remain unchanged]

---

This revised text is more organized, clear, and professional, making it easier to read and understand.