mises is most often f out of 3f +1 total replicas [12], although
some systems can tolerate f out of 2f + 1 total replicas with
additional assumptions or trusted hardware [13], [14], [15],
[16]. Some systems additionally guarantee performance under
attack, rather than only liveness [17], [18], [19], [20].
To support long system lifetimes, it is necessary to employ
proactive recovery, which allows replicas to be periodically
taken down and restored to a known clean state [12], [21].
Providing continuous availability with proactive recovery typ-
ically requires 3f + 2k + 1 total replicas to simultaneously
tolerate up to f compromised replicas and k recovering
replicas [22].
B. BFT and Network Attacks
The motivation for our work comes from the recent Spire
intrusion-tolerant SCADA system for the power grid [4], [9],
which showed that at least three geographically distributed
sites are needed to withstand sophisticated network attacks
that can target and isolate a site from the rest of the network.
The intuition for this is the following: since BFT replication
protocols require more than half of the system replicas (in fact,
2f +k + 1 out of 3f + 2k + 1) to be up, correct, and connected
in order to make progress, any system that distributes replicas
across fewer than three sites can be prevented from making
progress by isolating a single site. Clearly, if all replicas are
located in a single site, a denial of service attack targeting that
site can prevent it from communicating with remote clients and
thus render it unable to receive and process their updates. If
replicas are split across only two sites, targeting the larger of
the two sites will disconnect a majority of the system replicas,
leaving the rest unable to make progress without them.
Due to the high expense of constructing additional control
centers with full capabilities for communicating with Remote
Terminal Units (RTUs) and Programmable Logic Controllers
(PLCs), and controlling power grid equipment, Spire intro-
duced an architecture that uses two power grid control centers
(which typically exist today for fault tolerance purposes) and
supplements them with additional data center sites that do not
need to communicate with RTUs and PLCs. The use of data
centers can also reduce the management overhead of the higher
number of replicas that Spire needs to support its threat model
(12 total replicas to support f = 1 and 1 disconnected site).
However, data center replicas are still required to maintain
a full copy of the system state and execute application logic to
process incoming updates. This raises conﬁdentiality concerns,
as it requires SCADA operators to expose their private system
state and algorithms to offsite replicas potentially hosted by
a third party. Today, if a system operator wants to avoid
trusting a third party with this information, they must take
on the responsibility for managing the full deployment (and
constructing their own additional sites to host system replicas).
Our goal in this paper is to address this issue through a
new hybrid model for partially cloud based systems: system
operators host and manage “on-premises” replicas distributed
across two geographic sites that they manage and control,
while a service provider hosts and manages additional replicas
located in data center sites. In our model, not only do service-
provider-managed replicas not need to communicate with
clients, but they only see encrypted state and do not execute
application logic.
C. Conﬁdential BFT
Our goals are closely related to prior work on conﬁdential
BFT systems. These systems fall into two main categories:
approaches based on secret sharing, and approaches based on
privacy ﬁrewalls.
Secret Sharing. Conﬁdential BFT systems based on secret
sharing include DepSpace [23], Belisarius [24], and CO-
BRA [25]. These systems protect conﬁdentiality of the system
state as long as no more than f replicas are compromised.
To do this, clients encode data using an (f + 1, n)-threshold
secret sharing scheme, where f + 1 shares out of n total
shares are needed to reconstruct the conﬁdential data. Since
each replica only receives one share, this guarantees that up to
f compromised replicas are unable to successfully reconstruct
the data. COBRA [25] additionally provides for share renewal,
allowing it to tolerate up to f compromises per renewal epoch
(as opposed to over the entire life of the system).
This approach offers strong conﬁdentiality guarantees that
initially appear to ﬁt our goal well: in fact, such an approach
could enable management of the entire replicated system to
be ofﬂoaded to a service provider, with all replicas hosted
in data centers, while still guaranteeing that the system state
remains conﬁdential. However, due to practical limits on the
types of operations that can be performed on the encrypted
data, current systems typically support a limited set of op-
erations, such as basic key-value storage operations [25] or
tuple space storage [23], with Belisarius also offering the
ability to perform addition on stored values [24]. Moreover,
even if these systems supported general operations (e.g. via
secure multiparty computation or homomorphic encryption),
the operations themselves must be executed at all servers and
therefore cannot be kept private. For certain applications, it
may be desirable to keep application code or algorithms private
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:23:54 UTC from IEEE Xplore.  Restrictions apply. 
15
(e.g. because the algorithms are proprietary or otherwise
sensitive).
Secret sharing has also been used to create encrypted BFT-
replicated storage systems. DepSky [26] is a cloud-of-clouds
storage system that uses BFT replication with secret sharing
to store data across several different cloud providers and
guarantees data availability, integrity and conﬁdentiality as
long as no more than f of 3f + 1 clouds fail in a Byzantine
manner. SCFS [27] is a distributed ﬁle storage system that
can use DepSky as a backend to store encrypted ﬁles and
shares of the encryption keys on several clouds. RockFS [28]
is built on top of SCFS and improves on it by tolerating
some malicious client-side behaviour: (1) it prevents illegal
modiﬁcation of client credentials by using a secret sharing
scheme, (2) it prevents illegal access to the client’s local cache
by encrypting it, and (3) it allows administrators to undo any
illegal modiﬁcation of client ﬁles in the cloud by keeping a
log of every modiﬁcation.
While these systems enable conﬁdential, intrusion-tolerant
data storage in the cloud,
they do not address Byzantine
behaviour of the applications that generate or modify the data;
both DepSky and SCFS assume that clients are not malicious,
while RockFS can prevent or recover from speciﬁc types of
malicious client behaviour. In contrast, we aim to support
intrusion tolerance at the application level, not only for its
data storage.
from execution [29]: replicas are split
Privacy Firewall. Conﬁdential BFT based on privacy ﬁre-
walls was introduced by Yin et al. [29] and later built on by
Duan and Zhang [30]. This approach is based on separating
into an
agreement
agreement cluster that uses a BFT agreement protocol
to
establish a total order on incoming client updates and an
execution cluster that executes the ordered stream of updates
and generates client replies. A privacy ﬁrewall is constructed
between the execution cluster and agreement cluster to ﬁlter
replies sent by the execution cluster and to ensure that no
conﬁdential information is allowed to exit the execution cluster
(as long as no more than a threshold f of the ﬁrewall nodes
are compromised).
This approach again has properties that appear to ﬁt our goal
well: using the strategy of separating agreement and execution,
an attractive possibility is for execution nodes to be hosted
on-premises, while agreement nodes can be hosted in remote
data centers and managed by a service provider. This would
not require agreement nodes to carry out application logic or
understand the updates they order (as ordering can be done on
encrypted updates [29], [30]).
However, the existing solutions do not fully meet our needs.
They assume a model where all agreement and execution
replicas are hosted in a single site, and the objective is
to prevent a compromised execution node from exﬁltrating
conﬁdential information over the network. Therefore, their
architectures place the agreement cluster between the clients
and execution nodes, and only allow agreement nodes to
communicate with clients or other entities outside the site.
However, in many contexts, it may not be desirable or even
possible for data centers to communicate with clients (e.g. in
the power grid context, this is typically not feasible [4]).
The execute-verify model of Eve [31] and execute-order-
validate paradigm of Hyperledger [32] address this issue by
performing execution before ordering, but in doing so add
complexity at the application layer (e.g. to deal with state
rollback), without solving the following more fundamental
problem: these approaches cannot be straightforwardly ex-
tended to support the multi-site model needed to cope with
network attacks. We show that the observation in [4] implies
that system state must be stored in at least three distinct sites
in order to maintain continuous availability in the presence
of network attacks, preventing a clean separation between
execution and agreement unless system operators are willing to
build and manage at least three (execution) sites. In addition,
it is not clear how to apply the privacy ﬁrewall concept in
a multi-site deployment, as it strongly relies on a speciﬁc
physical network setup.
III. SYSTEM AND THREAT MODEL
A. System Model
We introduce a new system model for partially cloud-based
BFT systems. In this model, system management is shared
between system operators who are responsible for managing
an application and are typically experts in the application
domain, and service providers who offer data center hosting
capabilities (and potentially other services).
In our model, a system is physically deployed across loca-
tions owned and operated by the system operator (on-premises
sites) and infrastructure operated by the service provider (data
center sites). We refer to system replicas located in on-
premises sites as on-premises replicas and replicas located in
data center sites as data center replicas.
Our goal is to provide strong intrusion tolerance guarantees
without signiﬁcantly increasing the management overhead
for system operators. In many applications that require fault
tolerance, operators are likely to already maintain two on-
premises sites (e.g. for primary-backup). In contrast to adding
servers to an existing site, creating a new one involves
provisioning the physical location/building to house it, hir-
ing management personnel (since fault independence requires
a sufﬁcient geographical distance from existing sites), and
for some applications, provisioning specialized equipment to
communicate with client sites. Therefore, we assume two on-
premises sites and design our architecture to avoid constructing
any additional on-premises sites.
Due to privacy concerns (and potentially feasibility con-
straints), clients only communicate with on-premises replicas
(they do not communicate directly with data center replicas).
B. Threat Model
We consider the same broad threat model as in [4], which
includes both system-level compromises of the server replicas
and network-level attacks that aim to disrupt communication
among replicas and/or between replicas and clients.
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:23:54 UTC from IEEE Xplore.  Restrictions apply. 
16
We consider a broad range of network attacks, but, as
in [4], we reduce this to a simpler model through the use
of an intrusion-tolerant overlay network to connect the sites
to one another [33], [34]. With the use of the intrusion-tolerant
network, the network attacks we still need to address are
reduced to sophisticated (resource-intensive) denial of service
attacks that can target and isolate a single geographic site. We
assume that at any time, up to one site (either on-premises
or data center) may be subject to such an attack and thus
disconnected from the rest of the network.
As in other intrusion-tolerant replicated systems, we as-
sume that up to a threshold number f of replicas may be
compromised. Compromised replicas may behave arbitrarily
and collude with one another. As in prior work, we employ
proactive recovery to allow the system to tolerate up to f
compromises within a limited time window, as opposed to
over the entire system lifetime. We assume that replicas are
recovered one at a time, and that one replica’s recovery ﬁnishes
before the next replica’s recovery starts.1 Thus, at any time our
threat model includes up to f compromised replicas, up to one
replica that is unavailable because it is going through proactive
recovery, and one disconnected (or otherwise unavailable)
geographic site.
We assume each replica has access to a hardware-protected
private key (e.g. using the TPM) that it can use for signing,
but that cannot be deleted, modiﬁed, or exﬁltrated from the
machine. This key is used to bootstrap after proactive recovery,
when the replica generates a new session-level signing key. We
assume an attacker cannot break cryptographic protocols.
C. Service Properties
Our safety, liveness, and performance guarantees are essen-
tially the same as those speciﬁed in [4], although we adapt
them to a generic replicated system; where [4] speciﬁcally
considered SCADA Masters, HMIs, RTUs, and PLCs, we state
our guarantees in terms of generic servers and clients.
Deﬁnition 1 (Safety). If two correct on-premises replicas
execute the ith update, then those updates are identical, and
the state resulting from the execution of that update at the two
replicas is also identical.
Our system guarantees safety as long as no more than
f replicas are simultaneously compromised. Note that while
safety as deﬁned above is maintained in the presence of
an unlimited number of compromised clients, compromised
clients may still cause the system to take incorrect actions
by submitting malicious updates; we only guarantee that all
1This requires certain synchrony assumptions: an attacker must not be able
to arbitrarily prolong a replica’s recovery (see [35]). However, in practice these
can be met: simple trusted devices can trigger recovery by cycling the power
to a replica, and recovery intervals on the order of one replica per day are
sufﬁcient [36]. If an adversary can prevent a replica from collecting messages
needed for recovery for a full day, that replica is effectively disconnected.
The intrusion-tolerant overlay makes such disconnections very difﬁcult, and
our system technically allows recoveries of replicas in a disconnected site to
overlap, as long as the total number of recovering replicas is no more than
the size of the largest site plus one.
replicas will observe and execute these updates in a consistent
way (this is a general limitation in BFT replication). We do
not consider data center replicas as executing updates here, as
we only care about the state as it is visible to clients.
Deﬁnition 2 (Bounded Delay). The latency for an update