FEM∗ produced in an “ideal world” with a trusted third party.
Note that an adversary in the ideal world learns nothing ex-
cept the protocol inputs and outputs, hence, if he cannot dis-
tinguish simulated transcripts (from ideal world) and actual
transcripts (in the real world), he learns nothing in our real-
world implementation. Next, we formalize the ideal and real-
world executions, ideal and real, with notation from Evans
et al. [32]: Consider a subset of corrupted parties C ⊂ P ,
and let VIEWi denote the view of party i ∈ C during the ex-
ecution of EM∗ implementing ideal functionality FEM∗, in-
xi denote the protocol input of party Pi and(cid:98)µ the ﬁnal out-
cluding all exchanged messages and internal state, and let
own input xi) and outputs {VIEWi|i ∈ C},(cid:98)µ. And idealFEM∗ ,Sim,
with the same inputs, computes(cid:98)µ ← FEM∗(x1, . . . ,xm) and out-
puts Sim(C,(cid:98)µ,{xi | i ∈ C}),(cid:98)µ. Now, simulator Sim produces
put of all parties. The parameters s,k,U are public. Then,
realEM∗, on input security parameter κ, C and all xi, runs pro-
tocol EM∗ (where each party Pi behaves honestly using its
a transcript for realEM∗ as follows: As we operate on secret
shares, which look random to the parties [32], Sim replaces all
secret shares with random values to create VIEWi. Likewise,
the secret-shared output of the weight computations (Algo-
rithm 3 and 4) are replaced with randomness. Sim can simu-
late Algorithm 2 by recursively splitting U into k subranges,
and outputting the subrange containing(cid:98)µ in each selection
step. Finally, Sim outputs a uniform random element from
the last subrange (Algorithm 1). Altogether, a semi-honest
adversary cannot learn more than the (ideal-world) simulator
as this information is sufﬁcient to produce a transcript of our
(real-world) protocol.
For malicious adversaries, we need to ensure consistency
between rounds based on Aggarwal et al. [3], who securely
compute the (non-DP) median via comparison-based prun-
ing rounds. Informally, we have two consistency constraints:
2156    29th USENIX Security Symposium
USENIX Association
1, . . . ,Ri
j,ui
j. Then, rankDp(li
1) ≤ rankDp(li
First, valid rank inputs must be monotone within a step. Sec-
ond, for consistency between steps, valid inputs are contained
in the subrange output in the previous step. Formally, let
k} denote the set of subranges in the ith step of
{Ri
EM∗ and let li
j denote the lower resp. upper range endpoint
k) ≤
of Ri
rankDp(ui
k) describes monotone input in step i for party
p. Consistency between step i and i + 1, if the jth range
was selected, is expressed as rankDp(li+1
j) and
rankDp(ui+1
j). In other words, the subrange out-
put in the previous step is used in the current step. Analo-
gously, we can enforce consistency for weights as they are
based on rank values.
2) ≤ ··· ≤ rankDp(li
) = rankDp(ui
k
) = rankDp(li
1
4.7 Scaling to Many Parties
Recall, we distinguish two sets of parties: Input parties send
shares of their input to computation parties which run the
secure computation on their behalf. The latter can be a subset
of the input parties or non-colluding untrusted servers (e.g.,
multiple cloud service providers). This scales nicely as the
number of computation parties is independent of the number
of input parties and can be constant, e.g., 3. In our evalua-
tion (Section 5) m ∈ {3,6,10} computation parties perform
the computation for 106 input parties, each holding a single
datum. Addition sufﬁces for Weightsln(2) and Weightsln(2)/2d
to combine local rank values into a global rank. Addition is
essentially “free” as it requires no interaction between the
computation parties. For Weights∗ we require multiplication
to combine the local weights, which requires interaction dur-
ing the preprocessing step. However, logn rounds sufﬁce to
combine the inputs by building a tree of pairwise multiplica-
tions with 2i multiplications at level i [5].
5 Evaluation
Our implementation is realized with the SCALE-MAMBA
framework [6] using Shamir secret sharing with a 128-bit
modulus and honest majority. Next, we evaluate the running
time, privacy budget and accuracy of our solution and refer to
Appendix E for additional evaluations.
5.1 Running Times
We performed our evaluation on t2.medium AWS instances
with 2GB RAM, 4 vCPUs [8] and the Open Payments data
set from the Centers for Medicare & Medicaid Services
(CMS) [33]. Our evaluation uses 106 records from the Open
Payments data set, however, our approach scales to any data
set size as we consider universe subranges. We used the max-
imum number of selection steps, i.e., s = (cid:100)logk |U|(cid:101), with
k = 10 ranges per step. We evaluated the average running
(a) Weightsln(2)
(b) Weightsln(2)/2d ,
d = 2
(c) Weights∗
Figure 4: Average running time of EM∗ – with weight compu-
tation subroutines Weightsln(2), Weightsln(2)/2d , or Weights∗–
for 20 runs on t2.medium instances in Ohio and Frankfurt
(100 ms delay, 100 Mbits/s bandwidth).
time of 20 runs of the entire protocol EM∗, i.e., ofﬂine as well
as online phase (see Appendix E), in a LAN and a WAN.
LAN: We measured our running time for 3 parties in a LAN
with 1 Gbits/s bandwidth to compare it to Eigner et al. [30]
who only report LAN running times. We support universe
sizes of more than 5 orders of magnitude larger with compa-
rable running times: They compute weights per elements and
require around 42 seconds for |U| = 5, whereas our proto-
col EM∗ using Weightsln(2)/ Weightsln(2)/2d / Weights∗ runs
in approx. 11 / 33 / 64 seconds for |U| = 105. For detailed
measurements see Table 4 in Appendix E.
WAN: We consider m computation parties, which already
received and combined secret-shared inputs from 106 users
(Section 4.7), and report the average running time of our pro-
tocol. We split the m parties into two regions, Ohio (us-east-2)
and Frankfurt (eu-central-1), and measured an inter-region
round time trip (RTT) of approx. 100 ms with 100 Mbits/s
bandwidth. We evaluated all weight computation subrou-
tines in Figure 4 for m ∈ {3,6,10} computation parties and
|U| ∈ {105,106,107}. The results are very stable, as the 95%
conﬁdence intervals deviate by less than 0.5% on average.
Weightsln(2) (Figure 4a) is the fastest with running times
around 3 minutes for 3 parties, whereas Weightsln(2)/2d (Fig-
ure 4b) and Weights∗ (Figure 4c) require around 13 and
14 minutes respectively. However, we consider large universe
sizes (billions of elements) in a real-world network with large
latency. The choice of weight computation enables a trade-off
between faster running times, i.e., Weightsln(2) with ﬁxed ε,
and smaller privacy loss ε, i.e, Weights∗, with Weightsln(2)/2d
positioned in the middle (faster running time than Weights∗
with smaller ε compared to Weightsln(2)). The number k of
subranges allow a similar trade-off, as discussed next.
5.2 Privacy Budget vs. Running Time
The privacy budget is the sum of privacy parameters con-
sumed per step, i.e., the overall privacy loss. Figure 5 shows
USENIX Association
29th USENIX Security Symposium    2157
36102.533.54NumberofPartiesMinutes|U|=107|U|=106|U|=105361010121416NumberofPartiesMinutes|U|=107|U|=106|U|=10536101012141618NumberofPartiesMinutes|U|=107|U|=106|U|=105Figure 5: Privacy vs. running time trade-off: For increasing
number k of subranges the running time (left axis) increases
whereas the consumed privacy budget (right axis) decreases.
(Illustrated for EM∗ with Weightsln(2) and |U| = 105).
how the privacy budget and the running time are affected by
the number k of subranges. Larger k leads to larger running
times, as the number of costly secure computations depends
on the number of ranges times the number of selection steps
(k·(cid:100)logk |U|(cid:101)), which increases proportionally to k. However,
smaller values for k require more selection steps ((cid:100)logk |U|(cid:101)),
which lead to an increase in the privacy budget. Overall, for
k = 10 subranges, as used in our evaluation, the consumed
privacy budget is small with an acceptable running time.
5.3 Accuracy Comparison to Related Work
EM∗ performs multiple selection steps s, each consume a por-
tion εi of the overall privacy budget ε = ∑s
i=1 εi. How to opti-
mally split ε (optimal composition) is #P-complete [55]. Thus,
we use the following heuristic to divide ε among the selection
steps: Initial steps cover exponentially larger subranges, and
require exponentially less of the privacy budget. After a while
an equal split is more advantageous, as the subranges be-
come smaller and contain fewer elements. Altogether, we use
εi = ε/2s−i+1 if i ≤ (cid:98)s/2(cid:99) and εi = ε(cid:48)/(s−(cid:98)s/2(cid:99)) else, where
ε(cid:48) is the remaining privacy budget. We used s = (cid:100)logk |U|(cid:101)−1
for our accuracy evaluation. We found in our experiments that
performing one selection step less increases accuracy, as the
privacy budget can be better divided among the other remain-
ing steps and the last subrange is already small enough (at
most k elements).
Related work computing DP median in the central model
shows a strong data dependence which makes straightforward
comparison difﬁcult (Appendix F). Therefore, we empirically
evaluated the different approaches closest to ours, i.e., support-
ing more than 2 parties, on real-world data sets [42, 64, 67] as
well as the normal distribution in Figure 68 for 100 averaged
runs with 95%-conﬁdence intervals. Low ε (as evaluated) is
desirable as it provides more privacy or allows the remain-
ing privacy budget to be spend on additional queries. The
8“Small” data is the most challenging regime for DP [15, 56], thus, we
use small data sets to better illustrate the accuracy differences.
(a) Credit card data [67], ﬁrst
105 payment records in Cents.
(b) Walmart supply chain data
[42], 175k shipment weights as
integers.
(c) California public salaries [64],
71k records, state department’s to-
tal wages.
(d) Normal distribution with σ =
3, 105 samples (as integers with
scaling factor 1000).
Figure 6: Comparing exponential mechanism (EM) as base-
line, this work (EM∗), smooth sensitivity (SS) [58], sample-
and-aggregate (SA) [59] on different data, 100 averaged runs.
evaluation for smooth sensitivity [58] and exponential mech-
anism per element assume a trusted party with full access
to the data set, whereas our approach and [59] use MPC in-
stead of a trusted party. Nissim et al. [58] (SS in Figure 6)
compute instance-speciﬁc additive noise, requiring full data
access, and achieve good accuracy, however, the exponential
mechanism can provide better accuracy for low ε. Pettai &
Laud [59] (SA in Figure 6) securely compute the noisy aver-
age of the 100 values closest to the median within a clipping
range. Recall, the median is the 0.5th-percentile. To minimize
the error from clipping range [cl,cu], we choose cl = 0.49th-
percentile, cu = 0.51th-percentile, i.e., we presume to already
know a tight range for the actual median. Nonetheless, in our
experiments the absolute error of SA is the largest. Overall,
no solution is optimal for all ε and data sets. However, the
exponential mechanism EM, and our protocol EM∗, provide
the best accuracy for low ε, i.e., high privacy, compared to
additive noise approaches [58, 59].
6 Related Work
Next, we describe related work for secure computation of the
exponential mechanism, DP median and decomposability.
Secure Exponential Mechanism: Alhadidi et al. [4]
present a secure 2-party protocol for the exponential mech-
anism for max utility functions. It uses garbled circuits and
oblivious polynomial evaluation to compute Taylor series for
the exponential function. Our work is more general as we
2158    29th USENIX Security Symposium
USENIX Association
1.522.533.544.5567891011357101315NumberkofRangesMinutesPrivacyBudgetRunningTimesm=6m=3m=10PrivacyBudget0.10.250.505101520253035Avg.Abs.ErrorsEMEM∗SSSA0.10.250.50510152025303540Avg.Abs.ErrorsEMEM∗SSSA0.10.250.5050100150200250300350Avg.Abs.ErrorsEMEM∗SSSA0.10.250.502.557.51012.51517.5Avg.Abs.ErrorsEMEM∗SSSAsupport more parties and a broader class of utility functions,
including max utility functions. Eigner et al. [30] present a
carefully designed secure exponential mechanism in the multi-
party setting. Their work is more general, supporting arbitrary
utility functions and malicious parties, but they are linear in
the size of the universe, and securely compute the exponential
function. We provide a sublinear solution without costly se-
cure exponentiation, supporting at least 5 orders of magnitude
more elements than them. Böhler and Kerschbaum [14] also
securely compute the DP median with the exponential mech-
anism. They optimize their protocol for the 2-party setting,
compute the utility over (sorted) data, and provide DP for
small data (sublinear in the size of the universe). They ini-
tially prune large data sets via [3] (who securely compute the
exact median), requiring a relaxation of DP [39], to achieve
running time sublinear in the universe size. We consider the
multi-party setting and provide pure differential privacy.
DP Median: Pettai and Laud [59] securely compute DP
statistics, including the DP median, via sample-and-aggregate
[58]. Their implementation is based on secret sharing in a
3-party setting. Pettai and Laud [59] compute the DP median
as noisy average of 100 values closest to the median within a
clipping range, which limits accuracy, especially, if the data
contains outliers or large gaps (see Section 5.3). Dwork and
Lei [27] consider robust privacy-preserving statistics with
a trusted third party where data samples are known to be
drawn i.i.d. from a distribution. They present the ﬁrst DP
median algorithm that does not require bounds for the data but
aborts if the data are not from a “nice” distribution with small
sensitivity. Their DP median algorithm ﬁrst estimates scale
s via DP interquartile range and the noise magnitude sn−1/3
can be large. Nissim et al. [58] present smooth sensitivity,
which analyzes the data to provide instance-speciﬁc noise. For
the DP median, the exponential mechanism provides better
accuracy for low epsilon and can be efﬁciently computed,
whereas computation of smooth sensitivity requires full data
access in clear or the error increases (see Section 2.1.2).
PINQ, a DP query framework developed by McSherry [51],
also computes the DP median via the exponential mechanism,
however, they rely on a trusted third party with access to
the data in clear. Cryptε [19] employs two non-colluding un-
trusted servers and cryptographic primitives to compute noisy
histograms (Laplace mechanism) for SQL queries (e.g., count,
distinct count) in the central model, which can be extended to
compute the median. However, we show that the exponential
mechanism is more accurate for the median with low ε. Also,
Cryptε has a running time linear in the data size, whereas our
work is independent of the data size. Smith et al. [63] and
Gaboardi et al. [34] consider the restrictive non-interactive
local model, where at most one message is sent from client
to server, and achieve optimal local model error. However,
local DP requires more samples to achieve the same accu-
racy as central DP. (No non-interactive LDP-protocol [34, 63]
can achieve asymptotically better sample complexity than
O(ε−2α−2) for error α [24].) We, on the other hand, are inter-
ested in high accuracy, as in the central model, even for small
sample sizes. We give accuracy bounds for related work for
the DP median in the central model in Appendix F. As these
data-dependent bounds are hard to compare we provide an
empirical comparison in Section 5.3.
Decomposability: MapReduce is a programming para-
digm for distributed data aggregation where a mapper pro-
duces intermediary results (e.g., partial sums) that a reducer
combines into a result (e.g., total sum). Airavat [61] provide
a Hadoop-based MapReduce programming platform for DP
statistics based on additive noise (Laplace mechanism) with
an untrusted mapper but trusted reducer. We consider de-
composable utility functions for the exponential mechanism
without any trusted parties. The secure exponential mecha-
nisms [4, 30] use decomposable utility functions (max and
counts), but do not classify nor provide optimizations for such
functions. Blocki et al. [16] minimize cummulative error for
DP password frequency lists employing (decomposability of)
frequencies for their dynamic programming, which has access
to all the data in the clear. We use decomposable aggregate
functions to efﬁciently and securely combine inputs.
7 Conclusion
We presented a novel alternative for differentially private me-