expense of 16.7 percentage points accuracy degradation.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:11:21 UTC from IEEE Xplore.  Restrictions apply. 
142
of the form X × Y, where X is the sample space and Y is
the output space. For example, X could be thought of as the
space of images and Y as the labels of the images.
Given a dataset of input-output pairs (x, y) ∈ X × Y, the
goal of a supervised learning algorithm is to ﬁnd a model, i.e.,
a function F : X (cid:55)→ Y that maps these inputs to outputs. The
learning algorithm that produces this model uses an optimizer.
It takes in a dataset, a hypothesis space, and an objective:
• Dataset: Consistent with the probably approximately
correct (PAC) learning setting [25], we assume there is
an underlying distribution on Z that describes the data;
the learner has no direct knowledge of the distribution
but has access to a dataset D that is drawn from it. This
dataset D is further split into the training dataset Dtr
and a holdout dataset called the test dataset Dte such
that Dte ∪ Dtr = D and Dte ∩ Dtr = ∅.
• Hypothesis space: An hypothesis is a set of parameter
values w, which together with the model architecture F
selected, represent one possible mapping Fw : X (cid:55)→ Y
between inputs and outputs. In our case, the hypothesis is
a neural network and its parameters are the weights that
connect its different neurons (see below).
• Objective: Also known as the loss function, the objective
characterizes how good any hypothesis is by measuring
its empirical risk on the dataset, i.e., approximate the
error of the model on the underlying task distribution
of which we only have a few samples. A common
example is the cross-entropy loss, which measures how
far a model’s outputs are from the label:
l(x, y) =
i=0 yi·log(Fw(x)) where n is the number of classes
−(cid:80)n−1
in the problem.
Given an architecture F , a model Fw is found by searching for
a set of parameters w that minimize the empirical risk of Fw
on the training set Dtr. Performance of the model is validated
by measuring its accuracy on the test dataset Dte.
We experiment with our approach using neural networks
and deep learning [26]. Deep neural networks (DNNs) are
non-parametric functions organized as layers. Each layer is
made of neurons—elementary computing units that apply
a non-linear activation function to the weighted average of
their inputs. Neurons from a given layer are connected with
weights to neurons of the previous layer. The layout of these
layers and the weight vectors that connect them constitutes the
architecture of the DNN, while the value of each individual
weight (collectively denoted by w) is to be learned. Weights
are updated using the backpropagation algorithm [27]. The
algorithm starts by assigning a random value to each weight.
Then, a data point is sampled from the dataset and the loss
function is computed to compare the model’s prediction to
the data point’s label. Each model parameter value is updated
by multiplying the gradient of the loss function with respect
to the parameter by a small constant called the learning rate.
This algorithm enables learning and gradually improves the
model’s predictions as more inputs are processed.
III. DEFINING UNLEARNING
A requirement of privacy regulations such as the GDPR
or the CCPA is that individuals whose data is housed by
organizations have the right to request for this data to be
erased. This requirement poses challenges to current machine
learning technologies. We deﬁne the unlearning problem by
examining these challenges, which then leads us to a formal
model of the unlearning problem. We identify objectives for
an effective approach to unlearning, which we use to show the
ineffectiveness of existing strawman solutions.
A. Why is Unlearning Challenging?
The reason unlearning is challenging stems from the com-
plex and stochastic nature of training methods used to optimize
model parameters in modern ML pipelines.
1. We have a limited understanding of how each data point
impacts the model. There exists no prior work that measures
the inﬂuence of a particular training point on the parameters
of a model. While research has attempted to trace a particular
test-time prediction through the model’s architecture and back
to its training data [28], [29], these techniques rely on inﬂuence
functions, which involve expensive computations of second-
order derivatives of the model’s training algorithm. Further,
it is not obvious how to modify such inﬂuence functions
so that they map the effect of a single training point on
model parameters for complex models such as DNNs. We later
discuss techniques for differentially private learning, which
seek to bound the inﬂuence any training point can have
on model parameters, and explain how they are inadequate
because the bound is always non-zero.
2. Stochasticity in training. A great deal of randomness exists
in the training methods for complicated models such as DNNs;
small batches of data (e.g., with 32 points) are randomly
sampled from the dataset, and the ordering of batches varies
between different epochs, i.e., passes of the algorithm through
the dataset. Further, training is often parallelized without ex-
plicit synchronization, meaning the inherent random ordering
of parallel threads may make the training non-deterministic.
3. Training is incremental. Additionally, training is an incre-
mental procedure where any given update reﬂects all updates
that have occurred prior to it. For example, if a model is
updated based on a particular training point (in a particular
batch) at a particular epoch, all subsequent model updates will
depend, in some implicit way, on that training point.
4. Stochasticity in learning. Intuitively, learning algorithms are
designed to search for an optimal hypothesis in a vast hypoth-
esis space. In the case of neural networks, this space contains
all models that can be deﬁned by setting the weights of a ﬁxed
neural network architecture. PAC learning theory suggests
that the learned hypothesis is one of many hypotheses that
minimize the empirical risk. For example, the common choice
of optimizer for neural networks, stochastic gradient descent,
is capable of converging to one of the many local minima
for any convex loss function. Coupled with the stochasticity
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:11:21 UTC from IEEE Xplore.  Restrictions apply. 
143
Since there is no efﬁcient function that measures the inﬂuence
of this one additional point du on the parameters in MB, it
is very hard to invert the procedure unless a copy of MA
had been previously saved. Later in § IV, we will deﬁne this
strategy, termed slicing. In the absence of slicing, the most
convincing way to obtain plausible deniability, and ensure that
the model is devoid of the inﬂuence of a particular training
point du, is to retrain it from scratch without that particular
point (keeping all other training hyperparameters the same)
i.e., use D(cid:48) \ du to obtain the model MC in our example from
Figure 1. It is conceivable that the parameters of MA and
MC are similar (despite stochasticity in learning) and it is
desired for their performance (in terms of test accuracy) to be
comparable. However, the fact that model MC was obtained
by training on D(cid:48) \ du from scratch provides a certiﬁcate to
the data owner that their data share was indeed removed. This
conveys a very strong notion of privacy.
Deﬁnition III.1. Let D = {di : i ∈ U} denote the training
set collected from population U. Let D(cid:48) = D ∪ du. Let DM
denote the distribution of models learned using mechanism M
on D(cid:48) and then unlearning du. Let Dreal be the distribution of
models learned using M on D. The mechanism M facilitates
unlearning when these two distributions are identical.
We draw the attention of the reader to two key aspects of the
deﬁnition. First, the deﬁnition captures inherent stochasticity
in learning: it is possible for multiple hypotheses to minimize
empirical risk over a training set. As illustrated by models MA
and MC in Figure 1, two models having different parameters
does not imply that they were trained with a different dataset.
Conversely, two models trained with a different dataset do not
necessarily have different parameters. Second, the deﬁnition
does not necessarily require that the owner retrain the model
M(cid:48) from scratch on D\ du, as long as they are able to provide
evidence that model M(cid:48) could have been trained from scratch
on D(cid:48) \ du. In our work, this evidence takes the form of a
training algorithm, which if implemented correctly, guarantees
that the distributions DM and Dreal are identical.
C. Goals of Unlearning
The simple strategy we have discussed thus far i.e., training
a model from scratch on the dataset without the point being
unlearned is very powerful. We refer to this strategy as the
baseline strategy through the rest of the paper. However, for
large dataset sizes, such an approach will quickly become
intractable (in terms of time and computational resources
expended). For example, to be compliant with GDPR/CCPA,
organizations will have to retrain models very frequently. Thus,
any new strategy should meet the following requirements.
G1. Intelligibility: Conceptually, the baseline strategy is very
easy to understand and implement. Similarly, any un-
learning strategy should be intelligible; this requirement
ensures that the strategy is easy to debug by non-experts.
G2. Comparable Accuracy: It is conceivable that the accuracy
of the model degrades, even in the baseline, if (a) the
fraction of training points that need to be unlearned
Fig. 1: Unlearning (red arrow) is hard because there exists no
function that measures the inﬂuence of augmenting the dataset D with
point du and ﬁne-tuning a model MA already trained on D to train
(left blue arrow) a model MB for D+{du}. This makes it impossible
to revert to model MA without saving its parameter state before
learning about du. We call this model slicing (short green arrow).
In the absence of slicing, one must retrain (curved green arrow) the
model without du, resulting in a model MC that is different from
the original model MA.
involved in training, it is very challenging to correlate a data
point with the hypothesis learned from it.
B. Formalizing the Problem of Unlearning
We formalize the unlearning problem as a game between
two entities: an honest service provider S, and a user popu-
lation U. The service provider could be a large organization
that collects information from various individuals (such as a
company or hospital). This data is curated in the form of a
dataset D. The service provider uses this data for training
and testing a machine learning model M in any way they
desire. Any user u ∈ U can revoke access to their individual
data du ⊂ D. Observe that du can be a single element in the
dataset, or a set of elements. Within a ﬁnite period of time,
the service provider has to erase the revoker’s data and modify
any trained models M to produce M¬du, where M¬du is some
model that could plausibly have been trained if du were not
in D. In Deﬁnition III.1, we deﬁne plausibility according to
the distribution of models output by the training algorithm.
Further, S must convince u that M¬du is such a model—a
defense akin to that of plausible deniability. Access to data
may be revoked by users sequentially, but the service provider
may choose to perform data erasing in a batched fashion, as
discussed in § VII.
We illustrate this scenario in Figure 1. One can observe
that given a dataset D, it is possible to train one of several
models (e.g., DNNs) that generalize well from this dataset
unless the learning hypothesis class leads to a unique closed
form solution (e.g., linear classiﬁer). We denote two such
models MA and MC. If we add one more data point du to
the dataset D, we can train another model on this new dataset
D(cid:48) in many ways. This includes using the parameters of MA
to initialize a new model (rather than randomly initializing it)
and continuing training from there on to obtain model MB.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:11:21 UTC from IEEE Xplore.  Restrictions apply. 
144
  +{}      Unlearning (hard)LearningReinitialization≠    UL(retraining)UL(slicing)becomes too large, or (b) prototypical points [30] are
unlearned. Even if there is no component of the approach
that explicitly promotes high accuracy, any unlearning
strategy should strive to introduce a small accuracy gap
in comparison to the baseline for any number of points
unlearned.
G3. Reduced Unlearning Time: The strategy should have
provably lower time than the baseline for unlearning any
number of points.
G4. Provable Guarantees: Like the baseline, any new strategy
should provide provable guarantees that any number of
points have been unlearned (and do not inﬂuence model
parameters). Additionally, such a guarantee should be
intuitive and easy to understand for non-experts [31].
G5. Model Agnostic: The new strategy for unlearning should
be general i.e., should provide the aforementioned guar-
antees for models of varying nature and complexity.
G6. Limited Overhead: Any new unlearning strategy should
not introduce additional overhead to what are already
computationally-intense training procedures.
D. Strawman Solutions
Based on the requirements discussed earlier, we propose
several strawman candidates for an unlearning strategy. The
goals speciﬁed (sometimes in parantheses) are the goals the
strawman solutions do not meet.
1. Differential Privacy: Proposed by Dwork et al. [32], ε-
differential privacy offers probabilistic guarantees about the
privacy of individual records in a database. In our case, ε
bounds the changes in model parameters that may be induced
by any single training point. While several efforts [14], [33]
make it possible to learn with differential privacy, this guaran-
tee is different from what we wish to provide. We require that a
point has no inﬂuence on the model once it has been unlearned.
While differential privacy allows us to bound the inﬂuence any
point may have on the model, that bound remains non-zero.
This implies that there is a possibility that a point still has
a small but non-zero inﬂuence on the model parameters. To
guarantee unlearning, we would need to achieve ε-differential
privacy with ε = 0. This would make it impossible for the
algorithm to learn from the training data (G2).
2. Certiﬁed Removal Mechanisms: Other mechanisms relax the
deﬁnition of differential privacy to provide certiﬁcates of data
removal. This includes two concurrent proposals [34], [35]
The mechanism by Guo et al. [34] uses a one-step Newton
update [29]. While such a mechanism introduces a small
residue, this is masked by adding noise (similar to approaches
in differential privacy). However, as before, their guarantees
are probabilistic, and different from what we wish to provide
with SISA training. Additionally, to train non-linear models,
they resort to pretraining models on public data (for which no
guarantees are provided) or from differentially-private feature
extractors. In summary, such a mechanism is effective for
simple models such as linear regression models, which suggest
that they fall short of achieving G5.
3. Statistical Query Learning: Cao et al. [15] model unlearning
in the statistical query learning framework [16]. By doing so,
they are able to unlearn a point when the learning algorithm
queries data in an order decided prior to the start of learning.
In this setting, it is possible to know exactly how individual
training points contributed to model parameter updates. How-
ever, their approach is not general2 (G5) and does not easily
scale to more complex models (such as those considered in
this work). Indeed, these models are trained using adaptive
statistical query algorithms which make queries that depend
on all queries previously made. In this setting, the approach
of Cao et al. [15] diverges in an unbounded way unless the
number of queries made is small, which is not the case for
the deep neural networks we experiment with.
4. Decremental Learning: Ginart et al. [36] consider the
problem from a data-protection regulation standpoint. They
present a formal deﬁnition of complete data erasure which can
be relaxed into a distance-bounded deﬁnition. Deletion time
complexity bounds are provided. They note that the deletion
and privacy problems are orthogonal, which means deletion
capability does not imply privacy nor vice versa. However, it
is unclear if the approach presented (Quantized k-Means) is
applicable (G5) and scalable (G6) for all model classes.
IV. THE SISA TRAINING APPROACH
Our discussion thus far motivates why retraining from
scratch while omitting data points that need to be unlearned is
the most straightforward way to provide provable guarantees.
However, this naive strategy is inefﬁcient in the presence of
large datasets or models with high capacity that take a long
time to train. We present, SISA (or Sharded, Isolated, Sliced,
Aggregated) training to overcome these issues.
A. The SISA training Approach to Training
As illustrated in Figure 2, SISA training replicates the
model being learned several times where each replica receives
a disjoint shard (or subset) of the dataset—similar to current
distributed training strategies [37], [38]. We refer to each
replica as a constituent model. However, SISA training devi-
ates from current strategies in the way incremental model up-
dates are propagated or shared—there is no ﬂow of information
between constituent models. For example, if each constituent
model is a DNN trained with stochastic gradient descent,
then gradients computed on each constituent are not shared
between different constituents; each constituent is trained in
isolation. This ensures that the inﬂuence of a shard (and the
data points that form it) is restricted to the model that is being
trained using it. Each shard is further partitioned into slices,
where each constituent model is trained incrementally (and
iteratively, in a stateful manner) with an increasing number of
slices. At inference, the test point is fed to each constituent
and all the constituents’ responses are aggregated, similar to
the case of ML ensembles [39].
2Kearns [16] shows that any PAC learning algorithm has a corresponding
SQ learning equivalent. However, an efﬁcient implementations of SQ equiva-
lents for more complex algorithms does not exist, to the best of our knowledge.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:11:21 UTC from IEEE Xplore.  Restrictions apply. 
145
the updates obtained during the iterative training process
are not exchanged between different constituents. Intuitively,
such an approach may seem detrimental to improving the
generalization capabilities of the model; each constituent is
trained on a (signiﬁcantly) smaller portion of the dataset, and
may become a weak learner [20]. We evaluate this aspect in
§ VII, and discuss trade-offs of several aggregation approaches
to mitigate this effect for different learning tasks.
B. Techniques
1. Sharding: By dividing the data into disjoint fragments and
training a constituent model on each smaller data fragment, we
are able to distribute the training cost. While this means our
approach naturally beneﬁts from parallelism across shards, we
do not take this into account in our analysis and experiments,
out of fairness to the baseline of retraining a model from