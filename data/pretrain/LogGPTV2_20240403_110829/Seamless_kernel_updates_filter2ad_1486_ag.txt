49
5.3.1.1 Quake
We updated the kernel on the machine running the MVDSV 0.27 open source Quake
server, while it was serving 8 ezQuake clients. The Quake server does not preserve its
state across a reboot, and so the game needs to be restarted from the beginning. With
our system, the clients resume after a short pause exactly where they were in the game
before the update. For example, if a player was jumping through the air, the jump
continues.
The ezQuake client was modied to make it easier to compare seamless updates and
reboot. The unmodied client shuts down the current session and goes to the menu
screen if the client stops receiving messages from the server for 60 seconds. To avoid the
need to reconnect to the server by navigating the menu screen manually after a reboot,
we modied the client so that it automatically attempts to reconnect to the server when
no messages are received for 15 seconds. This change allows the client to reconnect to
the rebooted server much faster and without any user interaction, making it easier to
compare the systems. We chose the 15 second timeout because this time is much shorter
than the time it takes for the server to reboot, and longer than the time it takes to update
the kernel with our system. As a result, the client will reconnect as soon as the server is
running again after the reboot, but it will not attempt to reconnect while the update is
in progress. Note that the game state is still lost after the reboot, but not with seamless
updates.
Figure 5.1 shows the network throughput with reboot versus update. With reboot,
the clients timeout after 15 seconds and then begin attempting to reconnect every 5
seconds as shown by the little ticks at the bottom on the sent line. The server is
down for roughly 90 seconds. In the update case, the server is operational in roughly 10
seconds, and all the clients resume normally.
Chapter 5. Evaluation
50
Figure 5.2: Mysql/sysbench update
5.3.1.2 MySQL
We used the sysbench OLTP benchmark to test the performance of MySQL server. This
test involves starting transactions on the server and performing select and update queries
in a transaction. The sysbench benchmark has no support for handling server failure.
It returns an error when the MySQL server machine is rebooted and so we could not
complete this test. Figure 5.2 shows the graph of TCP throughput as observed at the
sysbench client while running the test. Similar, to the Quake results, the update time
is again roughly 10 seconds in our system. The output of the server drops to zero while
the update is being performed, but the connection to the client is not dropped. Once
the update is nished and the new kernel is started, MySQL can continue to operate
normally and the test runs to completion. There is no observable performance change
before and after the update. No changes were required to MySQL or sysbench for this
test.
5.3.1.3 Memcached
Memcached is a popular in-memory key-value caching system intended to speed up dy-
namic web applications. For example, it can be used to cache the results of database
calls or page rendering. An application can typically survive a memcached server being
 0 500 1000 1500 2000 0 20 40 60 80 100 120 140Throughput KB/sTime (s)KB/s receivedChapter 5. Evaluation
51
Figure 5.3: Memcached results after reboot vs. update
shutdown, because it can recalculate the results from scratch and start using the cache
when it becomes available again. However, cache contents are lost on a reboot, while
our system preserves the cache and so the application can use its contents right after the
update.
In this test, we compare the performance impact of the Memcached cache being
lost after a reboot versus being preserved in our system. We generated 200,000 key-
value pairs, consisting of 100 byte keys and 400 byte values. Then, we used a Pareto
distribution to send requests to prime the cache, so that 20% of the keys from the 200,000
key-pairs make up 80% of all the requests. Then we simulate a web application that uses
Memcached for caching the results of database queries. This application makes key
lookup requests to the Memcached server. For each key, the application rst makes a get
request from the cache. If the key is found, it makes the next get request. If it is not
found, then it waits for a short time (delay) to simulate calculating a result, and then
issues a set request to store the result in the server, before making the next get request.
We use 12 ms for the delay value for a database access, which is the average time per
transaction in our previous sysbench OLTP test. We also use 0 ms for the delay value to
represent the best case, in which there is no cost for calculating a result. However, when
a cache miss occurs, this instantaneous result still needs to be sent to the Memcached
 0 500 1000 1500 2000 2500 3000 0 20 40 60 80 100 120 140 160 180 200Requests per secondTime(s)Time of restart/updateBefore reboot with 0ms delayAfter update with 0ms delayAfter reboot with 0ms delay 0 500 1000 1500 2000 2500 3000 0 500 1000 1500 2000 2500 3000Requests per secondTime(s)Time of restart/updateBefore reboot with 12ms delayAfter update with 12ms delayAfter reboot with 12ms delayChapter 5. Evaluation
52
Application
Quiescence
Save state
Restore state
Checkpoint
time
time
time
size
Quake
334.1 ± 7.5 ms
98.59 ± 2.1 ms
22.85 ± 0.01 ms
135.2 ± 0.03 KB
MySQL
337.7 ± 2.5 ms
332.0 ± 45 ms
74.63 ± 3.1 ms
463.1 ± 24 KB
Memcached
329.5 ± 0.02 ms
6.4 ± 0.1 ms
38.1 ± 15 ms
112.3 ± 0.2 KB
Table 5.4: Per-application checkpoint time and size
Stage
Time
Initialize kernel
4.5 ± 0.3 s
Initialize services
6.9 ± 0.3 s
Table 5.5: Kernel restart time
server, thus requiring a get and a set request.
In the rst part of the experiment we prime Memcached by sending requests to it,
and then in the second part we send a second set of request after doing a clean reboot or
an update with our system and compare the performance in terms of requests per second.
The results of the experiment with 0 ms delay and 12 ms delay are shown in gure 5.4.
For 0 ms case we primed Memcached with 100,000 requests in the rst part and issued
another set of 100,000 requests in the second part. In the 12 ms case we primed with
500,000 requests and made another 500,000 requests in the second part. The time where
the Memcached server was rebooted or updated is shown with an arrow in both graphs.
The graphs show that after a reboot the number of requests per second declines
because the contents of the cache are lost, and each miss adds extra overhead by restoring
the lost cached value. In contrast when using our system the contents of the cache are
preserved which results in a smaller number of misses and the request rate stays at the
same level as before the update. Performing a regular reboot temporarily removes the
benet of using the in-memory cache until the contents of the cache are restored, while
our approach preserves the performance benet.
Chapter 5. Evaluation
53
Figure 5.4: Mmap checkpoint-restore time
5.3.2 Microbenchmarks
Table 5.4 breaks down the time to reach quiescence, save the process state and restore the
process state for the three applications described above. Quiescence time is measured
from the time when we start the checkpoint process until we are ready to take the
checkpoint (i.e., the last step in Section 3.2) and includes the time to shutdown all
the devices. The save state time is the time it takes to copy the kernel state into the
checkpoint. The time to initialize the kernel is measured from when the new kernel's
code starts executing to when the kernel starts the init process. The time to initialize
services is measured from when the init process starts to when the saved processes begin
to be restored, and the restore time is the time it takes for the saved applications to
be restored and start running. The kernel quiescence time is roughly 330 milliseconds
for each of these experiments. The checkpoint save time ranges from 6-350 ms, but as
discussed below, we expect that this time can be reduced signicantly with some simple
optimizations. The checkpoint restore time has a smaller range from 25-55 ms. All these
times are much lower than the time it takes to initialize the new kernel and the system
services, as shown in Table 5.5. The last column of Table 5.4 shows the checkpoint size
for each application excluding the memory pages of the application.
We also conducted a microbenchmark to measure the checkpoint and restore time
 0 2 4 6 8 10 12 14 16 18 0 2 4 6 8 10 12 14 16Time (s)Number of processesTime to save state 0 5 10 15 20 25 30 0 2 4 6 8 10 12 14 16 18Time (ms)Number of processesTime to restore stateBest Fit lineChapter 5. Evaluation
54
with increasing number of allocated frames in the system. The benchmark is run with
1, 2, 4, 8 and 16 processes. Each process in this benchmark allocates 16 MB of private
memory using the mmap system call and writes to this memory to ensure that the kernel
assigns page frames to the process. Figure 5.4 shows the checkpoint save and restore time
with increasing number of processes. The save time is roughly one second per process
in this benchmark with 16 processes and it grows with increasing number of processes
because our implementation for saving state is not optimized. In particular, the code
uses a linked list to detect shared pages. With 16 processes, and 16 MB of memory per
process, there are 64K (216) pages in the linked list, making the search for those many
pages in the list very slow (~232 operations). These lookups can be sped up with a hash
table or we could use the reverse mapping information available in the memory manager
to detect shared pages. The restore time per process is roughly one ms per 16MB process
because we take advantage of the kernel memory manager to ensure that shared pages are
assigned to each process correctly. Note that this time mainly accounts for restoring the
address space and is much smaller than the restore time for the benchmark applications
(25-65 ms) which also need to restore other resources such as the network buers.
Chapter 6
Conclusions
We have design a reliable and practical kernel update system that checkpoints application-
visible state, updates the kernel, and restores the application state. We have argued that
this approach requires minimal programmer eort, no changes to applications, and can
handle all backward compatible patches. Our system can transparently checkpoint the
state of network connections, some common hardware devices and user applications. It
can achieve quiescence for any kernel update, and it restarts all system calls transparently
to applications. We also performed a detailed analysis of the eort needed to support
updates across major kernel releases, representing more than a year and a half of changes
to the kernel. Our system required a small number changes to existing kernel code, and
minimal eort to handle major kernel updates, consisting of a million lines of code. Fi-
nally, we evaluated our implementation and showed that it works seamlessly for several,
large applications, with no perceivable performance overhead, and reduces reboot times
signicantly. As future work, we plan to add features currently missing in our implemen-
tation, such as support for SYSV IPC, epoll, and pseudo terminals, and checkpointing
state for all hardware devices.
55
Bibliography
[1] Oracle database high availability features and products. http://docs.oracle.com/cd/
B28359_01/server.111/b28281/hafeatures.htm.
[2] Performing rolling updates and upgrades in a db2 high availability disaster recovery (hadr)
environment. http://publib.boulder.ibm.com/infocenter/db2luw/v9r7/topic/com.
ibm.db2.luw.admin.ha.doc/doc/t0011766.html.
[3] Je Arnold and M. Frans Kaashoek. Ksplice: automatic rebootless kernel updates.
In
Proceedings of the ACM SIGOPS European Conference on Computer Systems (Eurosys),
pages 187198, 2009.
[4] Andrew Baumann, Jonathan Appavoo, Robert W. Wisniewski, Dilma Da Silva, Orran
Krieger, and Gernot Heiser. Reboots are for hardware: challenges and solutions to updating
an operating system on the y. In Proceedings of the USENIX Technical Conference, pages
114, 2007.
[5] Fernando Luis Vazquez Cao. Reinitialization of devices after a soft-reboot. Usenix Linux
Storage & Filesystem Workshop, February 2007.
[6] Haibo Chen, Rong Chen, Fengzhe Zhang, Binyu Zang, and Pen-Chung Yew. Live updating
operating systems using virtualization. In Proceedings of the International Conference on
Virtual Execution Environments, pages 3544, 2006.
[7] Haibo Chen, Jie Yu, Rong Chen, Binyu Zang, and Pen-Chung Yew. Polus: A powerful live
updating system. In Proceedings of the International Conference on Software Engineering,
pages 271281, 2007.
56
Bibliography
57
[8] Francis M. David, Ellick M. Chan, Jerey C. Carlyle, and Roy H. Campbell. Curios:
improving reliability through operating system structure. In Proceedings of the Operating
Systems Design and Implementation (OSDI), pages 5972, 2008.
[9] Alex Depoutovitch and Michael Stumm. Otherworld: giving applications a chance to
survive os kernel crashes. In Proceedings of the ACM SIGOPS European Conference on
Computer Systems (Eurosys), pages 181194, 2010.
[10] Greg Kroah-Hartman, Jonathan Corbet, and Amanda McPherson. Linux kernel develop-
ment: How fast it is going, who is doing it, what they are doing, and who is sponsor-
ing it. Linux Foundation, December 2010. www.linuxfoundation.org/publications/
whowriteslinux.pdf.
[11] Oren Laadan and Serge E. Hallyn. Linux-CR: Transparent application checkpoint-restart
in linux. In Proceedings of the Linux Symposium, 2010.
[12] David E. Lowell, Yasushi Saito, and Eileen J. Samberg. Devirtualizable virtual machines
enabling general, single-node, online maintenance. In Proceedings of the International Con-
ference on Architectural Support for Programming Languages and Operating Systems (AS-
PLOS), pages 211223, 2004.
[13] Kristis Makris and Kyung Dong Ryu. Dynamic and adaptive updates of non-quiescent
subsystems in commodity operating system kernels. In Proceedings of the ACM SIGOPS
European Conference on Computer Systems (Eurosys), pages 327340, 2007.
[14] Iulian Neamtiu, Michael Hicks, Gareth Stoyle, and Manuel Oriol. Practical dynamic soft-
ware updating for c. In Proceedings of the ACM SIGPLAN conference on programming
language design and implementation (PLDI), pages 7283, 2006.
[15] Steven Osman, Dinesh Subhraveti, Gong Su, and Jason Nieh. The design and imple-
mentation of zap: a system for migrating computing environments. In Proceedings of the
Operating Systems Design and Implementation (OSDI), pages 361376, December 2002.
Bibliography
58
[16] Shaya Potter and Jason Nieh. Reducing downtime due to system maintenance and up-
grades. In Proceedings of the USENIX Large Installation Systems Administration Confer-
ence, pages 4762, 2005.
[17] Craig A. N. Soules, Jonathan Appavoo, Kevin Hui, Robert W. Wisniewski, Dilma Da
Silva, Gregory R. Ganger, Orran Krieger, Michael Stumm, Marc Auslander, Michal Os-
trowski, Bryan Rosenburg, and Jimi Xenidis. System support for online reconguration.
In Proceedings of the USENIX Technical Conference, pages 141154, 2003.
[18] Swaminathan Sundararaman, Sriram Subramanian, Abhishek Rajimwale, Andrea C.
Arpaci-dusseau, Remzi H. Arpaci-dusseau, and Michael M. Swift. Membrane: Operat-
ing system support for restartable le systems. In Proceedings of the USENIX Conference
on File and Storage Technologies (FAST), 2010.
[19] Michael M. Swift, Muthukaruppan Annamalai, Brian N. Bershad, and Henry M. Levy. Re-
covering device drivers. In Proceedings of the Operating Systems Design and Implementation
(OSDI), pages 116, 2004.
[20] Michael M. Swift, Damien Martin-Guillerez, Muthukaruppan Annamalai, Brian N. Ber-
shad, and Henry M. Levy. Live update for device drivers. Computer Sciences Technical
Report CS-TR-2008-1634, University of Wisconsin, March 2008.
[21] Dmitrii Zagorodnov, Keith Marzullo, Lorenzo Alvisi, and Thomas C. Bressoud. Engineering
fault-tolerant tcp/ip servers using ft-tcp. In Proceedings of the IEEE Dependable Systems
and Networks (DSN), 2003.