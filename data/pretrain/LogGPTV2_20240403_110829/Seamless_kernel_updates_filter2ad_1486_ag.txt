### 5.3.1.1 Quake

We updated the kernel on a machine running the MVDSV 0.27 open-source Quake server while it was serving 8 ezQuake clients. The Quake server does not preserve its state across a reboot, so the game must be restarted from the beginning. With our system, however, the clients resume after a short pause exactly where they were in the game before the update. For example, if a player was in the middle of a jump, the jump continues seamlessly.

To facilitate a comparison between seamless updates and reboots, we modified the ezQuake client. The unmodified client shuts down the current session and returns to the menu screen if it stops receiving messages from the server for 60 seconds. To avoid the need for manual reconnection after a reboot, we modified the client to automatically attempt to reconnect to the server if no messages are received for 15 seconds. This change allows the client to reconnect to the rebooted server much faster and without any user interaction, making it easier to compare the systems. We chose a 15-second timeout because it is shorter than the time it takes for the server to reboot but longer than the time required for our system to update the kernel. As a result, the client will reconnect as soon as the server is operational again after the reboot, but it will not attempt to reconnect during the update process. Note that the game state is still lost after a reboot, but not with seamless updates.

Figure 5.1 shows the network throughput during a reboot versus an update. During a reboot, the clients timeout after 15 seconds and then begin attempting to reconnect every 5 seconds, as shown by the small ticks at the bottom of the "sent" line. The server is down for approximately 90 seconds. In the update case, the server is operational within about 10 seconds, and all clients resume normally.

### 5.3.1.2 MySQL

We used the sysbench OLTP benchmark to test the performance of the MySQL server. This test involves starting transactions on the server and performing select and update queries within a transaction. The sysbench benchmark does not support handling server failures, so it returns an error when the MySQL server machine is rebooted, and we could not complete this test. Figure 5.2 shows the graph of TCP throughput observed at the sysbench client while running the test. Similar to the Quake results, the update time in our system is approximately 10 seconds. The output of the server drops to zero during the update, but the connection to the client is not dropped. Once the update is complete and the new kernel is started, MySQL can continue to operate normally, and the test runs to completion. There is no observable performance change before and after the update. No changes were required to MySQL or sysbench for this test.

### 5.3.1.3 Memcached

Memcached is a popular in-memory key-value caching system designed to speed up dynamic web applications. It can be used to cache the results of database calls or page rendering. An application can typically survive a memcached server shutdown because it can recalculate the results from scratch and start using the cache when it becomes available again. However, cache contents are lost on a reboot, whereas our system preserves the cache, allowing the application to use its contents immediately after the update.

In this test, we compared the performance impact of the Memcached cache being lost after a reboot versus being preserved in our system. We generated 200,000 key-value pairs, consisting of 100-byte keys and 400-byte values. We then used a Pareto distribution to send requests to prime the cache, so that 20% of the keys from the 200,000 key-pairs make up 80% of all the requests. We simulated a web application that uses Memcached for caching the results of database queries. This application makes key lookup requests to the Memcached server. For each key, the application first makes a get request from the cache. If the key is found, it makes the next get request. If it is not found, the application waits for a short time (delay) to simulate calculating a result and then issues a set request to store the result in the server before making the next get request.

We used a 12 ms delay to simulate the average time per transaction in our previous sysbench OLTP test. We also used a 0 ms delay to represent the best case, where there is no cost for calculating a result. However, when a cache miss occurs, the instantaneous result still needs to be sent to the Memcached server, requiring both a get and a set request.

In the first part of the experiment, we primed Memcached by sending requests to it. In the second part, we sent another set of requests after either a clean reboot or an update with our system and compared the performance in terms of requests per second. The results for the 0 ms and 12 ms delay cases are shown in Figure 5.4. For the 0 ms case, we primed Memcached with 100,000 requests in the first part and issued another set of 100,000 requests in the second part. For the 12 ms case, we primed with 500,000 requests and made another 500,000 requests in the second part. The time when the Memcached server was rebooted or updated is indicated by an arrow in both graphs.

The graphs show that after a reboot, the number of requests per second declines because the contents of the cache are lost, and each miss adds extra overhead by restoring the lost cached value. In contrast, when using our system, the contents of the cache are preserved, resulting in fewer misses and maintaining the request rate at the same level as before the update. A regular reboot temporarily removes the benefit of using the in-memory cache until the contents are restored, while our approach preserves the performance benefit.

### 5.3.2 Microbenchmarks

Table 5.4 breaks down the time to reach quiescence, save the process state, and restore the process state for the three applications described above. Quiescence time is measured from the time we start the checkpoint process until we are ready to take the checkpoint, including the time to shut down all devices. The save state time is the time it takes to copy the kernel state into the checkpoint. The time to initialize the kernel is measured from when the new kernel's code starts executing to when the kernel starts the init process. The time to initialize services is measured from when the init process starts to when the saved processes begin to be restored, and the restore time is the time it takes for the saved applications to be restored and start running.

The kernel quiescence time is roughly 330 milliseconds for each of these experiments. The checkpoint save time ranges from 6-350 ms, but we expect that this time can be reduced significantly with some simple optimizations. The checkpoint restore time has a smaller range from 25-55 ms. All these times are much lower than the time it takes to initialize the new kernel and the system services, as shown in Table 5.5. The last column of Table 5.4 shows the checkpoint size for each application, excluding the memory pages of the application.

We also conducted a microbenchmark to measure the checkpoint and restore time with an increasing number of allocated frames in the system. The benchmark is run with 1, 2, 4, 8, and 16 processes. Each process in this benchmark allocates 16 MB of private memory using the mmap system call and writes to this memory to ensure that the kernel assigns page frames to the process. Figure 5.4 shows the checkpoint save and restore time with an increasing number of processes. The save time is roughly one second per process in this benchmark with 16 processes and grows with the increasing number of processes because our implementation for saving state is not optimized. Specifically, the code uses a linked list to detect shared pages. With 16 processes and 16 MB of memory per process, there are 64K (2^16) pages in the linked list, making the search for those many pages in the list very slow (~2^32 operations). These lookups can be sped up with a hash table or by using the reverse mapping information available in the memory manager to detect shared pages. The restore time per process is roughly one ms per 16 MB process because we take advantage of the kernel memory manager to ensure that shared pages are assigned to each process correctly. Note that this time mainly accounts for restoring the address space and is much smaller than the restore time for the benchmark applications (25-65 ms), which also need to restore other resources such as network buffers.