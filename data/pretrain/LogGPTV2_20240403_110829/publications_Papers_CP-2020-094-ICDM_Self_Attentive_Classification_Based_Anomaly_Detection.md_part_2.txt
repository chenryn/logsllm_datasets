d âˆ’ dimensional space (the log message is represented create a VMâ€ and â€Failed to create VM 3â€ should be distant.
by d Ã— |r| matrix, where |r| is number of words) and We refer to the data from the system of interest as target
y = 0;1 < i â‰¤ n, assuming that the data in the sys- dataset, i.e., the system where we want to detect anomalies.
i
tem of interest is mostly composed of normal samples. Let Important to note is that we are not using any anomaly
A = {(x ,y ),...,(x ,y )}, where m is the size data from the target system for learning purposes in our
n n n+m n+m
of the auxiliary data and y = 1;n < i â‰¤ n + m. Let experiments. The term auxiliary data refers to other non-
i
Ï†(x ,y ,Î¸):Rd â†’Rp be a function represented by a neural related systems, which serve only for training the model. All
i i
network, which maps the input log message embeddings to theresultsduringtesttimeareperformedonatestsetextracted
vector representations in Rp, and l : Rp â†’ [0,a],a âˆˆ R from the target dataset.
be a function, which maps the output to an anomaly score.
B. Logsy
The task is to learn the parameters Î¸ from the training data,
and then for each incoming instance in the prediction phase The method is composed of two main parts, the tokeniza-
D = {(xt),(xt),...,(xt),...}, t indicates test sample, tion of the log messages and the neural network model. In
t 1 2 j
predictwhetheritisanomalyornormalbasedontheanomaly the following section, we discuss the inner workings of the
scores obtained by l(Ï†(x ,y ,Î¸)). proposed method, which is depicted in Fig. 1.
i i
Tokenization. Tokenization transforms the raw log mes- Model.Logsyhastwooperationmodesâ€“offlineandonline.
sages into a sequence of tokens, as shown in Fig. 1. For During the offline phase, log messages are used to tune all
thispurpose,weutilizethestandardtextpreprocessinglibrary model parameters via backpropagation and optimal hyper-
NLTK[30].ThemessageisfirstfilteredforHTTPandsystem parameters are selected. During the online phase, every log
path endpoints (e.g., /p/gb2/stella/RAPTOR/). Every capital message is passed forward through the saved model. This
letter is converted to a lower letter, and all of the ASCII generates the respective log vector representation z and an
special characters are removed. The log message is split into anomaly score for each message.
word tokens. We remove every token that contains numerical As depicted in Fig. 2, the model applies two operations
characters,astheyoftenrepresentvariablesinthelogmessage on the input tokens: token vectorization (word embeddings)
and are not informative. Additionally, we remove the most and positional encoding. The subsequent structure is the
commonly used English words that are in the stop words encoder of the Transformer [19] module with multi-head
dictionary of NLTK (e.g., the and is). To the front of the self-attention, which takes the result of these operations as
tokenized log message, a special â€™[EMBEDDING]â€™ token is input. At the output of the encoder, there are |r | transformed
i
added. In the model, the â€™[EMBEDDING]â€™ token attends vector representation from the initial tokens. Recall that the
overall original tokens from the sample, which enables the â€™[EMBEDDING]â€™ token has its transformed representation,
model to summarize the context of the log message in the which is used as a final log vector representation. We denote
vectorrepresentation.Alltokensfromeverylogmessageform the size of this vector as d. This also represents the size of
vocabularyVofsize|V|,whereeachtokenisrepresentedwith allthelayersofthemodelandthewordembeddings.Thelast
integer label iâˆˆ0,1,...,|V|âˆ’1. An important advantage of two parts are the objective (loss) function during training and
Logsy compared to previous approaches is that it does not the computation of the anomaly score for test-time samples.
depend on log parsers as a pre-processing step. We consider Based on the loss, gradients are back-propagated to tune the
the tokenized log message as direct input to the model. The parametersofthemodel,whilebasedontheanomalyscorewe
advantage is that there is no loss of information from the log decideifthesampleisanomalousornormal.Inthefollowing,
message, due to the imperfections that exist in the log parsing we provide a detailed explanation of each element of the
methods. method. Fig. 2 depicts the inner working of the transformer
encoder.
Since all subsequent elements of the model expect numer-
Target-system Training ical inputs, we initially transform the tokens into randomly
data (class 0, normal) initialized numerical vectors x âˆˆ Rd. These vectors are
New log data from
target system referred to as tokenWoermd ebmedbeddidnigngss a(enxdamaprleesp)art of the training
Auxiliary data (class 1, [[EMBEDDING]:[0.11, 0.07 â€¦, 0.4], â€¦,
process, which means they are adjusted during training to
anomaly) ğ‘¤: [0.59, 0.33, â€¦, 0.7]]
ğ‘–
represent the semantic meaning of tokens depending on their
context. ThePsoesitniuonmael reinccaoldtinogk:e onf eeamchb wedodrdin wgsithainre passed to
Examples
1. imprecise machine check 2. machine check interrupt the positional etnhec olodgin mgesbslaogcek f.orI nprecsoenrtvriansgt tthoe e.g., recurrent
architectures, attention-sbeaqsueednmtiaol doerdlserdo not contain any no- Target dataset
1. [[EMBEDDING], imprecise, machine, check]
2. [[EMBEDDING], machine, check, interrupt] tion of input order. Therefore, this information needs to be (e.g., Blue Gene/L)
[EMBEDDING] : 0 explicitly encoKdeeyd and mergQeudewryith the inpVuatluveectors to take Test split Auxiliary data
imprecise : 1 1. [0, 1, 2, 3] their position within the log message into account. This block (normal & anomalous (anomaly class)
m ca hc eh ci kn e : 3: 2 2. [0, 2, 3, 4] calcN u lÃ— ates a vector n âˆˆ Rd representing the relative position samples) used for Other datasets
Multi-head attention evaluation only
interrupt ; 4 Tokenization of a token based on a sine and cosine function. (e.g., Thunderbird,
Spirit) used in
Add & Norm train split training only
    (normal class)
j j
n =sin F,eend 2fokr+w1a=rdcos . (1)
Transformer encoder with multi-head dot- 2k 100002 dk 100002k d+1
product self-attention
Here, k = 0,1,...,dâˆ’Ad1d &is Nthoremindex of each element in
n and j =1,2,...,|r | is the positional index of each token.
i
z = Numerical vector of the [EMBEDING] token Withintheequations,theparameterkdescribesanexponential
(summarizes the log message) relationship b[Z 0e= .t2w 3[E ,e 0M e.3nB 8E ,eD â€¦aD ,c I 0hN .8G v2] ]: a lue ofâ€¦vector n.ğ‘¤Tğ‘–ğ‘¡ he applied sine
and cosine functions allow for better discrimination of the
For train data For test data respective values within a specific vector of n. They have an
Spherical Cross-Entropy
Anomaly score: approximately lineLaorsds(ezp)e nadnedn Dceistoanncteh(ez)position parameter
Loss function (z) and
â€–ğ³â€–2
backpropagate j, which is hypothesized to make it easy for the model to
attend to the respective positions. Finally, both vectors can be
Encoder of the transformer architecture combined as x =x+n. We summarize all token embedding
vectors of a log message as matrix rows xT âˆˆX on which
Fig.1. OverviewofthearchitectureandcomponentdetailsofLogsy. the following formula is applied:
Target-system Training
data (class 0, normal)
New log data from
target system Word embeddings (examples) over each attention-transformed matrix row with kernel size
Auxiliary data (class 1,
[[EMBEDDING]:[0.11, 0.07 â€¦, 0.4], â€¦, one. Thisstep serves asadditional informationenrichment for
anomaly)
ğ‘¤ğ‘–: [0.59, 0.33, â€¦, 0.7]] the embeddings. Again, a residual connection followed by a
normalization layer between the input matrix and the output
Examples Positional encoding: of each word within
1. Imprecise machine check 2. Machine check interrupt of both layers is employed. This model element preserves the
the log message for preserving the
1. [[EMBEDDING], imprecise, machine, check] sequential order dimensionality X.
2. [[EMBEDDING], machine, check, interrupt]
The final element of the model consists of a single linear
[EM imB pE rD ecD iI sN e G : ] 1 : 0 1. [0, 1, 2, 3] Key Query Value layer. It receives the encoder result X and extracts the token
machine : 2 2. [0, 2, 3, 4] N Ã— embedding vector of the [â€™EMBEDDINGâ€™]. Since every log
Check : 3
Interrupt ; 4 Tokenization Multi-head attention message token sequence is pre-padded by this special token,
itisthefirstrowofthematrix,i.e.x âˆˆX,âˆ€i.Thisvectors
i,0
Add & Norm arethelogvectorrepresentationsandareusedintheobjective
function and as well as log message embeddings.
Transformer encoder with multi-head dot- Feed forward
product self-attention
C. Objective function
Add & Norm
Toensurelearningoftheintrinsicdifferencesofnormaland
z = Numerical vector of the [EMBEDING] token anomaly log samples, we propose a spherical loss function. It
(summarizes the log message) [Z 0= .2 3[E , 0M .3B 8E ,D â€¦D , I 0N .8G 2] ]: â€¦ ğ‘¤ ğ‘–ğ‘¡ is designed to integrate the previously mentioned assumption
that normal data is often concentrated having close distances
For train data
For test data
Spherical Cross-Entropy between the normal samples, while also learning properties to
Anomaly score:
Loss function (z) and â€–ğ³â€–2 Loss(z) and Distance(z) distinct from anomalous samples. This is done by employing
backpropagate
a radial classification loss which enforces a compact hyper-
Encoder of the transformer architecture spherical decision region for the normal samples.
Fig.2. Transformerencoderarchitecturewithmulti-headself-attention.
To derive the loss, we start with the standard binary cross
entropy. Let D = {(x ,y ),...,(x ,y )} be the con-
1 1 n+m n+m
catenation of the training logs from the system of interest
and the auxiliary data with x âˆˆ RdÃ—|ri|, where |r i| is the
 Ã—KT i
Q
X =softmax lâˆš l Ã—V , forl=1,2,...,L. (2) numberoftokensinthelogmessageandeachtokenisavector
l w l representedindâˆ’dimensionalspace.y âˆˆ{0,1},andy =0
i i
Thereby,Ldenotesthenumberofattentionheads,w = d and denotes normal samples (target system), while y i =1 denotes
L an anomaly (auxiliary data). Let Ï†(x ,Î¸) : Rd â†’ Rp be our
dmodL=0. The parameters Q, K and V are matrices, that i
encoderarchitecturethatmapsthe|x |wordembeddingsform
correspond to the query, key, and value elements in Fig. 2. i
thelogmessagetopâˆ’dimensionalvector.Letl:Rp â†’[0,1]
Theyareobtainedbyapplyingmatrixmultiplicationsbetween
the input X and respective learnable weight matrices WQ, be a function which maps the output to an anomaly score.
l Using Ï†(x ,Î¸) and l(Â·),d the standard binary cross-entropy
WK, WV: i
l l loss can be written as:
Q l =XÃ—W lQ, K l =XÃ—W lK, V l =XÃ—W lV, (3) 1 n
âˆš âˆ’ (1âˆ’y i)logl(Ï†(x i;Î¸))+y ilog(1âˆ’l(Ï†(x i;Î¸))) (4)
where WQ, WK, WV âˆˆ RMÃ—w. The division by w n
l l l i=1
stabilizesthegradientsduringtraining.Afterthat,thesoftmax
function is applied and the result is used to scale each token For standard classifier function the pâˆ’dimensional repre-
embedding vector V . The scaled matrices X are concate- sentation is transformed via linear layer followed by sigmoid
l l
nated to a single matrix X of size M Ã—d. activation function:
As depicted in Fig. 2 there is a residual connection be-
tween the input token matrix X and its respective attention n
1 
transformation X, followed by a normalization layer norm. âˆ’ (1âˆ’y )log((1+exp(âˆ’wTÏ†(x ,Î¸)))âˆ’1)
n i i (5)
These are used for improving the performance of the model i=1
by tackling different potential problems encountered during +y log(1âˆ’(1+exp(âˆ’wTÏ†(x ,Î¸)))âˆ’1)
i i
the learning such as small gradients and the covariate shift
phenomena.Basedonthis,theoriginalinputisupdatedbythe In the standard binary classifier with sigmoid function, the
attention-transformed equivalent as X =norm(X+X). decision boundary is half-space. The representation of the log
Thelastelementoftheencoderconsistsoftwofeed-forward messages is not guaranteed to be compact in this case. It
linear layers with a ReLU activation in between. It is applied could be very possible that the normal samples are scattered
individuallyoneachrowofX.Thereby,identicalweightsfor throughthespacewithvarying,potentiallyverylargedistances