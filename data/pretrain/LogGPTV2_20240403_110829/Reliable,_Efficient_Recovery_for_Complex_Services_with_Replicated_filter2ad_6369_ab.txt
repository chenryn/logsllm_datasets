restart procedure will be leader-based. The restarting system’s
ﬁrst task, then, is to choose a restart leader. While we could
elect a restart leader using standard techniques, we found it
simpler and just as effective to use a preconﬁgured list of
restart leaders installed on all nodes in the system (e.g. through
a settings ﬁle). We have designed our protocol such that any
node that was a member of the system at any time can serve
as the restart leader, so the choice of restart leader is arbitrary
and does not depend on the state of the system at the time
of the total failure. As we will see in section IV-B, this also
means that it is easy for another node to take over for the
restart leader if it fails during recovery.
In order to restart to a consistent state, several subproblems
must be addressed. First, when the restart leader starts up, it
does not know whether it was a member of the last installed
conﬁguration, or whether it crashed much earlier but was
nonetheless set as the restart leader; thus, its logs of both
system state and the group membership could be arbitrarily out
of date. Second, when the restart leader communicates with
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:35:43 UTC from IEEE Xplore.  Restrictions apply. 
174
other restarting nodes, it must determine whether those nodes’
conﬁguration and state data is newer or older than its own,
and whether it represents the last known state of the system,
without knowing in what order the other nodes crashed. Third,
for each node that restarts and has logged state updates, the
restart leader must determine which updates in that log might
have been externally visible and acted upon, and which were
still in-progress and might never have reached a majority
of replicas. Answering this question requires knowing what
conﬁguration was active at the time the update was logged, and
what conﬁguration was active at the time the system crashed.
Finally, during the restart process any node could experience
another transient crash, including the restart leader itself, and
these crashes should not result in the system restarting in an
inconsistent state or prevent the system from restarting when
it has a sufﬁcient number of healthy replicas.
The restarted system must also install a conﬁguration that
meets each shard’s fault-tolerance constraints. To avoid shard
shutdowns due to correlated failures, each shard is statically
conﬁgured to require a minimum number of nodes from
different failure correlation sets. Here, a distinction between
shards of different subgroups is important, since only shards
of the same subgroup are disjoint. Given a number of restarted
nodes and their failure correlation sets, the restart leader must
not only partition them between each subgroup’s shards, but it
must also (1) satisfy the minimum number of nodes required
from different failure correlation sets for each shard, (2) assign
as many nodes as possible to their original shards, in order to
minimize the number of state transfers between nodes, and (3)
compute the new assignment in a timely manner. Section II-C
gives a detailed example of what is required.
The log-recovery system we describe here addresses all
of these concerns, and restarts the system as efﬁciently as
possible by allowing each shard to complete state transfer
operations in parallel.
C. Failure-Domain-Aware Assignment
Suppose that a system has failure correlation sets f1, f2,
f3, such that f1 contains nodes a and b, f2 contains nodes
c, d, and e, and f3 contains nodes f and g. It has just one
subgroup with three shards s1, s2, s3, which require 2, 3, and
1 node(s) from different failure correlation sets respectively.
A valid initial conﬁguration for this system would be s1 =
{a, c}, s2 = {b, e, g}, s3 = {f}, leaving d unassigned to any
shard. This can be represented in the following diagram, in
which colors correspond to failure correlation sets:
s3
s2
s1
a
c
b
e
g
d
f
Now suppose a shutdown occurs and all nodes except g
restart. Shard s2 is no longer in a valid conﬁguration because
it has 1 less node than it requires, but it would not sufﬁce for
the restart leader to simply add the unassigned node d to the
shard because d is from the same failure correlation set as c.
s1
s2
s3
a
c
b
e
g
d
f
An optimal reassignment is to move f from s3 to s2, and
add d to s3, resulting in the post-restart conﬁguration s1 =
{a, c}, s2 = {b, e, f}, s3 = {d}. This reassigns only 2 nodes
to new roles, which is the minimum that can be achieved while
satisfying each shard’s requirements.
s1
a
c
b
s2
e
f
g
s3
d
III. RESTART ALGORITHM
Having established the parameters of the restart problem,
we now present our algorithm for solving it. At a high level,
this algorithm has seven steps:
1) Find the last-known view by inspecting persistent logs
2) Wait for a quorum of this view to restart
3) Find the longest replicated state log for each shard
4) Compute new shard assignments and complete epoch
termination from the last view, if necessary
5) Trim shard logs with conﬂicting updates
6) Update replicas with shorter logs
7) Install the post-restart view
this is not a linear process, because failures at
However,
any step after 2 can force the algorithm to return to step
2 if the quorum is lost. Also,
in practice, steps 1-3 are
executed concurrently by the restart leader, because it can
gather information about the longest update log available for
each shard while it is waiting to reach a restart quorum.
In order for log recovery to be possible, we must add a few
requirements to the system described in Section II-A. First,
during a reconﬁguration, all nodes which commit to a new
view must log it to nonvolatile storage before installing it.
Furthermore, in order to ensure that no updates are used in
the restarted state of the system that would have been aborted
by the epoch termination process, live nodes must log each
epoch termination decision to persistent storage before acting
upon it. Before committing to a new view, the new members
of each shard must download and save the epoch termination
information for the prior view in addition to the logged updates
that they download during the state-transfer process.
The pseudocode for our algorithm is shown in Algorithms
1, 2, 3, and 4, where Algorithms 1 and 2 show the code
that runs on the restart leader, Algorithm 3 shows the code
that runs on a non-leader node, and Algorithm 4 shows the
STATE TRANSFER function that is common to both nodes.
For brevity, we have factored out the leader’s failure-handling
code into a macro called HANDLE FAILURE, which should be
inserted verbatim wherever it is named.
In our pseudocode’s syntax, the dot-operator accesses mem-
bers of a data structure by name, and the bracket operator
accesses members of a map by key, as in C++ or Java. Note
that
there are three kinds of integer identiﬁers: node IDs
or NIDs, shard IDs or SIDs, and view IDs or VIDs. Each
node has a globally unique node ID, and, as is common in
virtual synchrony, view IDs are unique and monotonically
increasing. Shard IDs are unique identiﬁers assigned to each
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:35:43 UTC from IEEE Xplore.  Restrictions apply. 
175
Vc ← Vi
WRITE(view log, Vc)
ET ← {}
LL[sid] ← (nidn, seqno)
Algorithm 1 The restart leader’s behavior, part 1
1: Vc ← READ(view log)
2: restarted ← {nidme}
3: ue ← READ(update log).end
4: LL ← {Vc.my sid → (nidme, ue.seqno)}
5: ET ← READ(epoch termination log)
6: while ¬ QUORUM(Vc, restarted) do
(Vi, nidn, sid, seqno) ← RECEIVE from n
7:
restarted ← restarted ∪ {nidn}
8:
if Vi.vid > Vc.vid then
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19: Vr ← CHANGE VIEW(Vc, restarted)
20: if ET = {} then
21:
22:
23:
24: sent ← {}
25: for all s ∈ Vr.subgroups do
26:
27:
28:
29:
30:
if LL[sid].seqno < seqno then
et ← RECEIVE from n
if et (cid:5)= {} ∧ et.vid = Vc.vid then
success ← SEND(Vr, ET, LL[s.sid].nid) ton
if ¬success then
sent ← sent ∪ {nidn}
ET ← et
WRITE(epoch termination log, ET )
ET.vid ← Vc.vid
for all s ∈ Vr.subgroups do
ET.last[s.sid] ← LL[s.sid].seqno
for all nidn ∈ s.members do
HANDLE FAILURE(nidn, sent)
LL[Vr.my sid].nid, nidme, Vr)
HANDLE FAILURE(LL[Vr.my sid].nid, restarted)
HANDLE FAILURE(LL[Vr.my sid].nid, restarted)
Algorithm 2 The restart leader’s behavior, part 2
31: if ET.vid = ue.vid then
success ← SEND(∅) toLL[V r.my sid].nid
32:
trim seqno ← ET.last[Vr.my sid]
33:
34: else
success ← SEND(ue.vid) toLL[V r.my sid].nid
35:
trim seqno ← RECEIVE from LL[Vr.my sid].nid
36:
37: if ¬success then
38:
39: TRUNCATE(update log, trim seqno)
40: success ← STATE TRANSFER(
41: if ¬success then
42:
43: sent ← {}
44: for all nidn ∈ Vr.members do
45:
46:
47:
48: for all nidn ∈ Vr.members do
49:
50: WRITE(view log, Vc)
51:
52: procedure HANDLE FAILURE(nid, notif y set)
53:
54:
55:
56:
57:
58:
59:
restarted ← restarted − {nid}
for all nidm ∈ notif y set do
if ¬ QUORUM(Vc, restarted) then
else
success ← SEND(“Prepare”) to n
if ¬success then
HANDLE FAILURE(nidn, sent)
SEND(“Commit”) to n
SEND(“Abort”) to m
goto 19
goto 6
shard (globally, across all subgroups) of the system. In the
following sections, we will explain the details of the algorithm,
which should make the pseudocode more clear.
A. Awaiting Quorum
The restart leader’s ﬁrst operation is to read its logged view,
which becomes the ﬁrst “current” view, Vc, and its logged
epoch termination information, which becomes the currently-
proposed epoch termination, ET . It then begins waiting for
other nodes to restart and contact it; non-leader nodes will
contact the preconﬁgured restart leader as soon as they restart
and discover that they have logged system state on disk.
When a non-leader node contacts the leader, it sends a copy
of its logged view, Vi, its node ID, the ID of the shard it was
a member of during Vi, and the sequence number of the latest
update it has on disk. The joining node may optionally then
send a logged epoch termination structure, if it has one that is
as new as its logged view. The leader updates Vc and possibly
ET if the client’s view and epoch termination are newer, and
uses data structure LL (a map from shard IDs to pairs of
node IDs and update sequence numbers) to keep track of the
location of the longest log for each shard. Note that sequence
numbers from later views are always ordered after sequence
numbers from earlier views.
After each node restarts, the leader checks to see if it has a
restart quorum. A restart quorum consists of a majority of the
members of the system in the last known view that includes at
least one member of every shard from that view. In addition,
the restart leader must be able to install a new post-restart view
in which the entire group has at least f +1 replicas to meet the
overall fault-tolerance threshold, and each shard is populated
by nodes that meet its failure-correlation requirements. Note
that the post-restart view can add new members that were
not part of the last known view, since nodes that failed in an
earlier view but restarted after the system-wide failure can still
participate in the recovery process.
Once the leader has reached a restart quorum, if the newest
epoch termination structure it has discovered is from an older
view than Vc, it makes its own decision about how to terminate
Vc’s epoch. Speciﬁcally, it synthesizes an epoch termination
structure by taking the sequence number of the latest update
for each shard, and marking it with the same VID as Vc. It
then computes Vr, the next view to install after restarting.
In practice, the leader waits for a short “grace period” after a
quorum is achieved to allow nodes that restarted at a slightly
slower rate to be included in Vr. This makes it less likely
that Vr will require many node reassignments (and hence state
transfers), and has only a minor effect on restart time.
B. Assigning Nodes to Shards
When testing for a restart quorum and computing Vr, the
leader must determine an optimal assignment from nodes to
shards. Since the shards of a subgroup must be disjoint, it can
consider each subgroup individually. For each subgroup, the