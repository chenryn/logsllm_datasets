### Outreach to Machine Learning and Security Researchers

Prior to the public release of this paper, we contacted machine learning and security researchers at Google, Microsoft, and Facebook to share our findings.

### Structural Similarity Index (SSIM)

The SSIM is a measure used to quantify the similarity between two images. It is defined as follows:

\[
\text{SSIM}(x, y) = l(x, y) \cdot c(x, y) \cdot s(x, y)
\]

where:
- \( l(x, y) = \frac{2\mu_x\mu_y + C_1}{\mu_x^2 + \mu_y^2 + C_1} \)
- \( c(x, y) = \frac{2\sigma_x\sigma_y + C_2}{\sigma_x^2 + \sigma_y^2 + C_2} \)
- \( s(x, y) = \frac{\sigma_{xy} + C_3}{\sigma_x\sigma_y + C_3} \)

Here, \(\mu\) and \(\sigma\) represent the mean and standard deviation of pixel intensities of the image samples, respectively. The constants \(C_1\), \(C_2\), and \(C_3\) are chosen to stabilize the division with weak denominator, and their recommended values are provided in the original papers [65, 66].

### Dissimilarity Measure (DSSIM)

DSSIM, or Structural Dissimilarity, is a distance metric derived from SSIM. It is calculated as:

\[
\text{DSSIM} = \frac{1 - \text{SSIM}}{2}
\]

DSSIM ranges from 0 to 1, where 0 indicates that the two images are identical, and 1 indicates that the two images are completely different (often achieved by inverting the image).

### Multi-Scale SSIM (MSSSIM)

In our experiments, we use an improved version of SSIM known as multi-scale SSIM (MSSSIM). MSSSIM considers distortions due to viewing conditions, such as display resolution, by iteratively comparing the reference and distorted images at different scales. This is achieved by applying a low-pass filter to downsample the images. We use the implementation of MSSSIM from TensorFlow and follow the recommended parameter configuration [11].

[11] https://github.com/tensorflow/models/blob/master/research/compression/image_encoder/msssim.py

### Adversarial Images

#### Figure 12: Adversarial Images in Iris, Traffic Sign, and Flower

- **Iris (P = 0.005)**: Attack targets VGG16 layer 15 (out of 16 layers).
- **Traffic Sign (P = 0.01)**: Attack targets VGG16 layer 10 (out of 16 layers).
- **Flower (P = 0.003)**: Attack targets ResNet50 layer 49 (out of 50 layers).

These perturbation budgets result in unnoticeable perturbations.

#### Figure 13: Adversarial Images for Student Models Trained on Different Platforms

- **Google Cloud ML (P = 0.001)**: Achieves a targeted success rate of 96.5%.
- **Microsoft CNTK (P = 0.003)**: Achieves a targeted success rate of 99.4%.
- **PyTorch (P = 0.001)**: Achieves a targeted success rate of 88.0%.

### Defense Mechanisms

#### Figure 14: Performance of Dropout as a Defense Mechanism

- **Face**: Shows the performance of applying dropout with different dropout ratios.
- **Iris**: Shows the performance of applying dropout with different dropout ratios.
- **Traffic Sign**: Shows the performance of applying dropout with different dropout ratios.

#### Figure 15: Performance of Modifying Student Model as a Defense Mechanism

- **Face**: Shows the performance of modifying the student model with different neuron distance thresholds.
- **Iris**: Shows the performance of modifying the student model with different neuron distance thresholds.
- **Traffic Sign**: Shows the performance of modifying the student model with different neuron distance thresholds.

---

**USENIX Association**
27th USENIX Security Symposium
1297