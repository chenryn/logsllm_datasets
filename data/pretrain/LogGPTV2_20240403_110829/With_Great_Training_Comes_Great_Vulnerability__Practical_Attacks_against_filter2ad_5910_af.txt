vance of the public release of this paper, we reached out
to machine learning and security researchers at Google,
Microsoft and Facebook, and shared our ﬁndings with
them.
SSIM(x, y) = l(x, y) · c(x, y) · s(x, y)
µ2
σ2
(cid:17)
(5)
(cid:17)
x + µ2
= (cid:16) 2µxµy + C1
y + C1
·(cid:16) 2σxσy + C2
x + σ2
y + C2
·(cid:16) σxy + C3
(cid:17)
σxσy + C3
µ and σ are mean and standard deviation of pixel inten-
sities of image samples. C1, C2, and C3 are constants, and
recommendation for choosing these constants is included
in the original paper [65, 66].
DSSIM is calculated as 1−SSIM
. It ranges from 0 to
1, where 0 represents two images are identical, and 1
represents two images are negatively correlated (often
achieved by inverting the image).
2
Deﬁnition of DSSIM
DSSIM (Structural Dissimilarity) is a distance metric
derived from SSIM (Structural SIMilarity). Let x =
{x1, ..., xN}, and y = {y1, ..., yN} be pixel intensity sig-
nals of two images being compared, respectively. The
basic form of SSIM compares three aspects of the two
image samples, luminance (l), contrast (c), and structure
(s). The SSIM score is then described in the following
equation.
In our experiments, we use an improved version of
SSIM, referred as multi-scale SSIM, which also consid-
ers distortion due to viewing conditions (e.g., display res-
olution). This is achieved by iteratively comparing the
reference and distorted images at different scales (or res-
olutions) by applying a low-pass ﬁlter to downsample
images. To compute DSSIM, we use the implementa-
tion of multi-scale SSIM from TensorFlow and follow
the recommended parameter conﬁguration 11.
11https://github.com/tensorflow/models/blob/
master/research/compression/image_encoder/
msssim.py
1296    27th USENIX Security Symposium
USENIX Association
    Source   Adversarial    Target
    Source   Adversarial    Target
    Source   Adversarial    Target
    Source   Adversarial    Target
    Source   Adversarial    Target
    Source   Adversarial    Target
(a) Iris (P = 0.005)
(b) Trafﬁc Sign (P = 0.01)
(c) Flower (P = 0.003)
Figure 12: Adversarial images generated in Iris, Trafﬁc Sign, and Flower. Perturbation budgets selected result in
unnoticeable perturbations. Iris attack targets at VGG16 layer 15 (out of 16 layers). Trafﬁc Sign attack targets at
VGG16 layer 10 (out of 16 layers), and Flower attack targets at ResNet50 layer 49 (out of 50 layers).
    Source   Adversarial    Target
    Source   Adversarial    Target
    Source   Adversarial    Target
    Source   Adversarial    Target
    Source   Adversarial    Target
    Source   Adversarial    Target
(a) Google Cloud ML (P = 0.001)
(b) Microsoft CNTK (P = 0.003)
(c) PyTorch (P = 0.001)
Figure 13: Adversarial images generated for Student models trained on Google Cloud ML, Microsoft CNTK, and
PyTorch. Attacks using these samples achieve targeted success rate of 96.5%, 99.4%, and 88.0% in corresponding
models.
y
c
a
r
u
c
c
A
n
o
i
t
a
c
i
f
i
s
s
a
C
l
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
Non-targeted
Classification
Targeted
 0.2
 0.4
 0.6
Dropout Ratio
(a) Face.
 1
 0.8
 0.6
 0.4
 0.2
t
e
a
R
s
s
e
c
c
u
S
k
c
a
t
t
A
 0
 0.8
y
c
a
r
u
c
c
A
n
o
i
t
a
c
i
f
i
s
s
a
C
l
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
 1
 0.8
 0.6
 0.4
 0.2
t
e
a
R
s
s
e
c
c
u
S
k
c
a
t
t
A
 1
 0.8
 0.6
 0.4
 0.2
y
c
a
r
u
c
c
A
n
o
i
t
a
c
i
f
i
s
s
a
C
l
Classification
Non-targeted
Targeted
 1
 0.8
 0.6
 0.4
 0.2
t
e
a
R
s
s
e
c
c
u
S
k
c
a
t
t
A
 0
 0
 0
 0.01  0.02  0.03  0.04  0.05
Dropout Ratio
(c) Trafﬁc Sign.
 0
 0.01  0.02  0.03  0.04  0.05
Non-targeted
Classification
Targeted
Dropout Ratio
(b) Iris.
Figure 14: Performance of applying Dropout as defense with different Dropout ratio in Face, Iris, and Trafﬁc Sign.
y
c
a
r
u
c
c
A
n
o
i
t
a
c
i
f
i
s
s
a
C
l
 1
 0.8
 0.6
 0.4
 0.2
 0
0
Classification
Non-targeted
Targeted
 1
 0.8
 0.6
 0.4
 0.2
e
t
a
R
s
s
e
c
c
u
S
k
c
a
t
t
A
10K
20K
Neuron Distance Threshold
 0
30K
y
c
a
r
u
c
c
A
n
o
i
t
a
c
i
f
i
s
s
a
C
l
 1
 0.8
 0.6
 0.4
 0.2
 0
0
Non-targeted
Classification
Targeted
 1
 0.8
 0.6
 0.4
 0.2
 0
e
t
a
R
s
s
e
c
c
u
S
k
c
a
t
t
A
y
c
a
r
u
c
c
A
n
o
i
t
a
c
i
f
i
s
s
a
C
l
 1
 0.8
 0.6
 0.4
 0.2
 0
Classification
Non-targeted
Targeted
 1
 0.8
 0.6
 0.4
 0.2
 0
e
t
a
R
s
s
e
c
c
u
S
k
c
a
t
t
A
2K
4K
6K
8K
10K
0
5K 10K 15K 20K 25K 30K
Neuron Distance Threshold
Neuron Distance Threshold
(a) Face.
(b) Iris.
(c) Trafﬁc Sign.
Figure 15: Performance of modifying Student as defense with different distance thresholds in Face, Iris, and Trafﬁc
Sign.
USENIX Association
27th USENIX Security Symposium    1297