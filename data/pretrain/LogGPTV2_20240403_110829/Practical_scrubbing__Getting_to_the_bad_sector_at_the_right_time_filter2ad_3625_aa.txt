title:Practical scrubbing: Getting to the bad sector at the right time
author:George Amvrosiadis and
Alina Oprea and
Bianca Schroeder
Practical Scrubbing:
Getting to the bad sector at the right time
George Amvrosiadis
Dept. of Computer Science
University of Toronto
Toronto, Canada
PI:EMAIL
Alina Oprea
RSA Laboratories
Cambridge, MA
PI:EMAIL
Bianca Schroeder
Dept. of Computer Science
University of Toronto
Toronto, Canada
PI:EMAIL
Abstract—Latent sector errors (LSEs) are a common hard
disk failure mode, where disk sectors become inaccessible while
the rest of the disk remains unaffected. To protect against
LSEs, commercial storage systems use scrubbers: background
processes verifying disk data. The efﬁciency of different scrub-
bing algorithms in detecting LSEs has been studied in depth;
however, no attempts have been made to evaluate or mitigate
the impact of scrubbing on application performance.
We provide the ﬁrst known evaluation of the performance
impact of different scrubbing policies in implementation, in-
cluding guidelines on implementing a scrubber. To lessen this
impact, we present an approach giving conclusive answers
to the questions: when should scrubbing requests be issued,
and at what size, to minimize impact and maximize scrubbing
throughput for a given workload. Our approach achieves six
times more throughput, and up to three orders of magnitude
less slowdown than the default Linux I/O scheduler.
Keywords-scrubbing; hard disk failures; latent sector errors;
idleness predictors; background scheduling
I. INTRODUCTION
It is estimated that over 90% of all new information
produced in the world is being stored on magnetic me-
dia, primarily hard drives [1], making the reliability of
these complex mechanical components crucial. Alas, disk
drives can fail for numerous reasons, and while for many
years it was assumed that disks operate in a “fail-stop”
manner, Bairavasundaram et al. [2] showed that Latent
Sector Errors (LSEs) are a common failure mode. In the
case of LSEs, individual disk sectors become inaccessible,
while the remainder of the disk is unaffected. LSEs are a
particularly insidious failure mode, since they are silent and
only detected when the affected disk area is accessed. If
there is no redundancy in the system, the data on the affected
sectors is lost. While most storage systems do provide
redundancy, most commonly through the use of RAID, LSEs
can still cause data loss if they are detected during RAID
reconstruction after a disk failure. This scenario is becoming
a major concern, particularly since the rate at which disk
capacities increase suggests that by 2015, a full-disk scan
will incur at least one LSE [3].
To protect against data loss due to LSEs, most commercial
978-1-4673-1625-5/12/$31.00 ©2012 IEEE
storage systems use a “scrubber”: a background process that
periodically performs full-disk scans to proactively detect
and correct LSEs. The goal of a scrubber is to minimize
the time between the occurrence of an LSE and its detec-
tion/correction, also referred to as the Mean Latent Error
Time (MLET), since during this time the system is exposed
to the risk of data loss (e.g. if another drive in the disk
array fails). In addition to reducing the MLET, a scrubber
must ensure to not signiﬁcantly affect the performance of
foreground workloads running on the system.
The importance of employing an efﬁcient scrubber will
only increase in the future, as continuously growing disk
capacities will increase the overheads for a disk scan, and
the rate at which LSEs will happen. Unfortunately,
the
scrubbers employed in today’s storage systems are quite
simplistic: they scan the disk sequentially in increasing order
of Logical Block Numbers (LBN) at a rate determined by
the system administrator. This simple approach ignores a
number of design options that have the potential for reducing
the MLET, as well as the impact on foreground workload.
The ﬁrst design question is determining the order in
which to scrub the disk’s sectors. While scrubbing the disk
sequentially is simple and efﬁcient (as sequential I/O is more
efﬁcient than random accesses), recent research [4] shows
that an alternative approach, called staggered scrubbing,
provides lower MLET. Staggered scrubbing aims to exploit
the fact that LSEs happen in (temporal and spatial) bursts:
rather than sequentially reading the disk from beginning
to end, the idea is to quickly “probe” different regions of
the drive, hoping that if a region has a burst of errors the
scrubber will detect it quickly and then immediately scrub
the entire region. While reducing MLET, the overhead of the
random I/O in staggered scrubbing can potentially reduce
scrub throughput and increase the impact on foreground
workloads. Unfortunately, there exists no experimental eval-
uation that quantiﬁes this overhead, and staggered scrubbing
is currently not used in practice.
The second design question is deciding when to issue
scrub requests. The scrubbers employed in commercial stor-
age systems today simply issue requests at a predeﬁned
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:48:41 UTC from IEEE Xplore.  Restrictions apply. 
rate, e.g. every r msec. This approach has two short-
comings. First, it does not attempt to minimize impact on
foreground trafﬁc, as scrub requests are issued independently
of foreground activity. While research has been done on
scheduling general background trafﬁc [5], [6], [7], [8], [9],
none is particularly geared towards scrubbers. Second, it is
often hard for system administrators to choose the rate r that
is right for their system, since it is hard to predict the impact
on foreground trafﬁc that’s associated with a particular rate.
A third parameter that is not very well understood is the
scrub request size, or the number of sectors scrubbed by
individual scrub requests. Larger request sizes lead to more
efﬁcient use of the disk, but also have the potential of bigger
impact on foreground trafﬁc, as foreground requests that
arrive while a scrub request is in progress get delayed.
Finally, there are a number of implementation choices for
a scrubber that have not been well studied, including the
choice of a user-level versus kernel-level implementation,
or the effect of the disk interface (SCSI versus ATA).
The contributions of our work are summed up as follows:
• We developed a framework that can be used to imple-
ment scrubbing algorithms in only tens of lines of code
(LoC) within the linux kernel, and made its source code
publicly available1. We also implemented a user-level
scrubber to allow for a quantitative comparison between
user-level and kernel-level scrubbing.
• We provide the ﬁrst implementation of a staggered
scrubber and compare its performance to that of a
standard sequential scrubber.
• We perform a detailed statistical study of publicly
available disk I/O traces, and apply the results towards
the design of an efﬁcient approach for scheduling scrub
requests.
• We provide conclusive answers to the questions: when
should scrubbing requests be issued and at what size,
using an approach that maximizes scrubbing throughput
for a given workload, while meeting a predeﬁned
slowdown goal for foreground trafﬁc. Our approach sig-
niﬁcantly outperforms the default linux I/O scheduler,
the only one to allow I/O prioritization.
• We provide the ﬁrst known evaluation of the perfor-
mance impact of different scrubbing policies using our
framework and our approach in conjunction with real-
world workload traces.
The paper is organized as follows: In Section II we
describe work related to this paper. Section III describes the
framework we implemented and used to develop sequential
and staggered scrubbing; a performance evaluation of the
two algorithms follows in Section IV. We look at ways
to increase scrubbing throughput, while avoiding impact on
applications in Section V, and conclude with a discussion
of our observations and future directions in Section VI.
1Kernel patches can be found at http://www.cs.toronto.edu/∼gamvrosi
II. RELATED WORK
The concept of scrubbing is not new in reliability research;
several papers have studied scrubbing in different contexts.
While scrubbers in production systems simply scan the disk
sequentially, work by Oprea et al. [4] shows that an approach
based on sampling can exploit LSE locality to reduce MLET.
The disk is separated into R regions, each partitioned into
S segments. In each scrub interval, the scrubber begins by
reading the ﬁrst segment from each region ordered by LBN,
then the second one, and so on. Other proposed algorithms
focus on actions taken once an LSE is detected [10], [11],
or scrubbing in archival systems with limited uptime [12].
All these studies focus solely on reducing the MLET, and
do not evaluate the scrubber’s impact on the performance
of foreground trafﬁc. Also, evaluation is done analytically,
without an actual implementation. This paper provides an
implementation of sequential and staggered scrubbing, and
evaluates their impact on foreground trafﬁc performance;
we also examine a number of other design choices for
implementing a scrubber.
A number of prior studies have also focused on ways to
schedule background workloads in a storage system, while
limiting the impact on foreground trafﬁc. Unfortunately,
none of this work is optimal or suitable for scheduling
background scrub requests. Proposed approaches include:
I/O preemption [13], merging background requests with
application requests [5], [6] and piggy-backing background
requests to foreground requests [14]. These approaches,
however, make extensive use of prefetching, and a scrubber
should ideally avoid request prefetching/merging as it pol-
lutes the on-disk and page caches with data from scrubbed
sectors. Furthermore, a scrubber needs to guarantee that the
sector’s contents were veriﬁed from the medium’s surface
(rather than a cache) at request execution time.
Other approaches try to limit the impact of background
requests on foreground trafﬁc by trying to predict disk
idleness and scheduling background requests during those
idle intervals. Golding et al. [7] provide a general taxonomy
of idleness predictors, without, however, adapting their pa-
rameters to accommodate for different workloads or speciﬁc
slowdown goals. Our study moves along the lines of prior
work by Mi et al. [8], [9] and Schindler et al. [15], who
try to determine the best start and end times to schedule
background requests while limiting the number of collisions
(situations where a foreground request arrives to ﬁnd the
disk busy with a background request). Our approach, instead
of focusing on collisions requires more intuitive input: the
average slowdown allowed per I/O request. We also ﬁnd
that a simple approach is sufﬁcient, where only the start
time is determined, and requests are always issued until a
collision happens. Finally, we optimize a different parameter
set speciﬁc to scrubbing, which provides additional degrees
of freedom such as the scrub request size.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:48:41 UTC from IEEE Xplore.  Restrictions apply. 
)
c
e
s
m
(
e
m
T
i
200
100
50
20
10
5
2
1
0.5
0.2
8.296
8.331
8.291
8.348
8.481
8.754
4.011
4.032
4.073
4.023
4.301
4.61
0.296
0.3
0.326
0.352
0.474
0.525
WD Caviar (SATA, Cache off)
WD Caviar (SATA, Cache on)
Hitachi Deskstar (SATA, Cache off)
Hitachi Deskstar (SATA, Cache on)
Hitachi Ultrastar (SAS, Cache off)
Hitachi Ultrastar (SAS, Cache on)
1K
2K
4K
8K
16K
64K 128K 256K 512K
32K
ATA Verify size (bytes)
1M
2M
4M
8M 16M
Figure 1. Response times for different ATA VERIFY sizes
III. IMPLEMENTING A SCRUBBER
When implementing a scrubber, three components need
to be considered: an interface directing the disk to carry out
the veriﬁcation of speciﬁc sectors, a module that implements
the scrubbing algorithm(s), and an I/O scheduler capable of
issuing scrub requests in a fashion that limits the impact
on foreground trafﬁc. We experimented with the SCSI and
ATA interfaces [16], and examine their ability to issue
scrub requests in Section III-A. In Section III-B, we brieﬂy
present the only I/O scheduler in linux that provides I/O
prioritization. Finally, we explore and evaluate different
implementations of the scrubbing module in both user- and
kernel-level in Section III-C.
A. Scrub requests in SCSI/SAS vs. ATA/SATA
Scrubbers typically rely on the VERIFY commands imple-
mented in both the SCSI and ATA interfaces to issue scrub
requests, rather than using regular disk reads. The reason is
that VERIFY guarantees to verify the data from the medium’s
surface (rather than reading it from the on-disk cache) and
prevents cache pollution by avoiding to transfer any data or
use prefetching.
Interestingly, our experiments with multiple ATA drives
show that ATA VERIFY is not implemented as advertised.
Speciﬁcally, we observe strong evidence that ATA VERIFY
reads data from the on-disk cache, rather than the medium’s
surface. Our evidence is summarized in Fig. 1, which
shows ATA VERIFY response times for different request
sizes for two popular current ATA drives (WD Caviar and
Hitachi Deskstar) and one SAS drive (Hitachi Ultrastar)
when accessing data sequentially from the disk. The solid
lines for all models show the response times when the on-
disk cache is disabled, while the dashed lines show results
for when the on-disk cache is enabled. It is evident that
disabling the cache affects VERIFY response times for the
ATA drives but not for the SAS drive, indicating that the
former do depend on the on-disk cache, rather than forcing
accesses to the drive’s platter. We conclude that since ATA
VERIFY is implemented incorrectly in modern ATA disks,
scrubbers using them would likely pollute the on-disk cache.
Figure 2. Architecture of our kernel scrubber.
B. The Completely Fair Queueing (CFQ) I/O scheduler
In our implementation we use the linux CFQ I/O sched-
uler, as it is the only open source scheduler that supports
I/O prioritization. CFQ provides a separate priority class
(Idle), for scheduling of background requests. To minimize
the effect of background requests on foreground ones, CFQ
only issues requests from the Idle class after the disk has
remained idle for at least 10ms. Although this parameter is
tunable, changing it in linux 2.6.35 did not seem to affect
CFQ’s background request scheduling.
C. User space, or kernel space?
While current scrubbers rely on the VERIFY commands
for the reasons outlined in Section III-A, there are downsides
to using them due to the way they are executed by the linux
kernel. As is common with device-speciﬁc functionality in
linux, a wild-card system call is used (ioctl) to pass a
packet with the command and its arguments directly to the
I/O scheduler, and then to the device driver for execution.
However, since the kernel has no knowledge of the command
that is about to be executed, such requests are ﬂagged as
soft barriers, and performance optimizations (e.g. reordering
between or merging with outstanding block requests) are
not applied. Since a scrubber implemented in user space
has no way to avoid the performance penalty due to these
scheduling decisions made in the kernel, we implement our
scrubbing framework entirely in kernel space.
In Fig. 2 we present the architecture of our kernel scrub-
ber, implemented in the block layer of the 2.6.35 kernel
[17]. The scrubber is activated at bootstrapping, matching
scrubber threads to every block device in the system; this
matching is updated when devices are inserted/removed,
e.g. due to hot swapping. The threads remain dormant by
being inserted in the CPU’s sleeping queue, until scrubbing
for a speciﬁc device is activated. Internally, the scrubber
implements SCSI VERIFY2, since it is not natively sup-
ported by the kernel, and provides a simple API that can be
2Our implementation supports ATA devices through the kernel’s libATA
library, which performs the appropriate translation for VERIFY.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:48:41 UTC from IEEE Xplore.  Restrictions apply. 
Foreground Workload
Scrubber (throughput in text)
9.2
12.1
7.8
7.8
limits the kernel scrubber’s throughput at 3MB/s. These
results clearly motivate the use of sophisticated scheduling
with I/O prioritization, if the scrubber’s impact is to be
mitigated while retaining high throughput.
3.9
3.0
)
s
/
B
M
(
t
u
p
h
g
u
o
r
h
T
16
14
12
10
8
6
4
2
0
IV. GETTING THERE QUICKLY
The goal of this section is to compare the performance
of staggered and sequential scrubbing. While sequential
scrubbing is the approach currently employed in practice,
research indicates that staggered scrubbing can signiﬁcantly
reduce the MLET. The reason practitioners are shying away
from using staggered scrubbing, is the fear that moving from
sequential to more random scrubbing I/O patterns might
affect the performance of the I/O system, both in terms
of the scrubber’s throughout, and its impact on foreground
workloads. To quantify these effects, we implemented both
a sequential and a staggered scrubber within the kernel-
level framework described in Section III-C. We begin our
experimental evaluation in Section IV-A by comparing the
scrubbing throughput that can be achieved by a staggered
versus a sequential scrubber, and then evaluate their impact
on foreground trafﬁc, using simple synthetic foreground
workloads in Section IV-B, and more realistic trace-based
workloads in Section IV-C.
A. Staggered versus sequential scrubbing
The performance of both a sequential and a staggered
scrubber will naturally depend on the choice of their param-
eters. Therefore, we begin by evaluating the effect of the
scrubbers’ parameters on their performance to identify the
optimal set of parameters for each scrubber.
The only tunable parameter of a sequential scrubber is the
request size S of each scrubbing request, a parameter shared
with the staggered scrubber. Our ﬁrst goal in this section is to
distinguish the range of request sizes that can achieve high
scrubbing throughput. We measured the response time of
SCSI VERIFY for different request sizes for two SAS drives
(Hitachi Ultrastar 15K450 300GB and Fujitsu MAX3073RC
73GB) and one SCSI disk (Fujitsu MAP3367NP 36GB), and
we found that for requests ≤ 64KB, response times remain
almost constant for all models (Fig. 4). As a consequence,
we henceforth experiment only with request sizes starting at
64KB, and report the throughput achieved by each scrubber.
The results for the two SAS drives are shown in the two
solid lines in Fig. 5a, suggesting that the largest scrubbing
request size should be preferred, from 64KB to 4MB.
In the case of the staggered scrubber there is an additional
parameter: the number of regions that the disk is divided
into. Recall that a staggered scrubber separates the disk
into R regions, based on a predeﬁned region size, each of
which gets scrubbed in ⌈ R
S ⌉ rounds. In the ﬁrst round, the
ﬁrst S bytes are veriﬁed from each region, then the S after
those, and so on. To experiment with the region size, we ran
the staggered scrubber on our two SAS disks using 64KB
None
Idle (U)
Scrubber Priority (U: User−level scrubber, K: Kernel−level scrubber)
Default (U)
Idle (K)
Default (K)
Def. 16ms(U) Def. 16ms(K)
Figure 3. Comparison of our user- (U) and kernel-level (K) scrubbers.
used to code new scrubbing strategies. We implemented our
framework in 2700 commented LoC, and in that we coded
sequential and staggered scrubbing in approx. 50 LoC each.
To send requests to the I/O scheduler and set the I/O
priority for scrubber threads (if CFQ is used), we use the
Generic Block Layer interface. To enable the I/O scheduler
to sort scrubbing requests among other outstanding ones,
every time a scrubber thread dispatches a VERIFY request
we disguise it as a regular read request bearing all relevant
information, such as starting LBN and request size. This in-
formation is unavailable in the vanilla kernel, since sorting is
not permitted for soft barrier commands. Once the scrubbing
request has been dispatched, we put the thread back to the
sleeping queue, and at request completion it is awakened by
a callback function to repeat the process.
Fig. 3 shows the results from experimenting with the
basic version of our kernel-level scrubber and a basic
user-level scrubber. We generate a simple, highly-sequential
foreground workload, with exponential think times between
requests (µ = 100ms) to allow for idle intervals that the
scrubber can utilize. In one set of experiments we run the
foreground workload and the scrubber at the same priority
level (Default), while in a second set of experiments we use
CFQ’s Idle priority class to reduce the scrubber’s priority.
For both the Idle and the Default priorities we allowed the
scrubber to issue requests back to back, and for the Default
priority, we also experimented with a 16ms delay between
scrub requests (Def. 16ms), in order to allow the foreground
workload to make progress. Results are shown for a Hitachi
Ultrastar SAS drive, but we also experimented with a Fujitsu
MAX3073RC SAS drive and got similar results.
It is evident from Fig. 3 that when allowing the scrubber
to issue requests back-to-back, both the scrubber and the
foreground workload achieve signiﬁcantly higher throughput
in the kernel-level implementation. Also, priorities have no
effect on the user-level scrubber whose requests are soft
barriers, as opposed to the kernel scrubber, which is bene-
ﬁting by the workload’s think time and starving it under the
Default priority. When the scrubber is delayed, however, the
maximum scrubbing throughput (3.9MB/s, or 64KB/16ms) is
reached only by the user-level scrubber; proper prioritization
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:48:41 UTC from IEEE Xplore.  Restrictions apply. 
)
c
e
s
m
(
e
m
i
t
i
e
c
v
r
e
s
y
f
i
r
e
V
I
S
C
S
280
260
240
220
200
180
160
140
120
100
80
60
40
20
0
Hitachi Ultrastar 15K − SAS
Fujitsu MAX3073RC − SAS
Fujitsu MAP3367NP − SCSI
8.8 8.9 8.8 8.8 9.1 9.4 9.7 10 13 16
40
25
K
1
K