- `-r` read all files under each directory, recursively.
Useful resources:
- [How to grep a string in a directory and all its subdirectories files in LINUX?](https://stackoverflow.com/questions/15622328/how-to-grep-a-string-in-a-directory-and-all-its-subdirectories-files-in-linux)
How to find out the dynamic libraries executables loads when run?
You can do this with `ldd` command:
```bash
ldd /bin/ls
```
You have the task of sync the testing and production environments. What steps will you take?
It's easy to get dragged down into bikeshedding about cloning environments and miss the real point:
- only production is production
and every time you deploy there you are testing a unique combination of deploy code + software + environment.
Every once in a while a good solution is regular cloning of the production servers to create testing servers. You can create instances with an exact copy of your production environment under a dev/test with snapshots, for example:
- generate a snapshot of production
- copy the snapshot to staging (or other)
- create a new disk using this snapshot
Sure, you can spin up clones of various system components or entire systems, and capture real traffic to replay offline (the gold standard of systems testing). But many systems are too big, complex, and cost-prohibitive to clone.
Before environment synchronization a good way is keeping track of every change that you make to the testing environment and provide a way for propagating this to the production environment, so that you do not skip any step and do it as smoothly as possible.
Also structure comparison tool or deploy scripts that update the testing environment from production environment is a good solution.
**Presync tasks**
First of all is informing developers and clients about not making changes on the test environment (if possible, disabling test domains that target this environment or set static pages with information about synchronization).
It is also important to make backup/snapshots of both environments.
**Database servers**
- sync/update system version (e.g. packages)
- create dump file from database on production db server
- import dump file on testing db server
- if necessary, syncs login permissions, roles, database permissions, open connections to the database and other
**Web/App servers**
- sync/update system version (e.g. packages)
- if necessary, updated kernel parameters, firewall rules and other
- sync/update configuration files of all running/important services
- sync/update user accounts (e.g. permissions) and their home directories
- deploy project from git/svn repository
- sync/update important directories existing in project, e.g. **static**, **asset** and other
- sync/update permissions for project directory
- remove/update all webhooks
- update cron jobs
**Others tasks**
- updated configurations of load balancers for testing domains and specific urls
- updated configurations of queues, session and storage instances
Useful resources:
- [Keeping testing and production server environments clean, in sync, and consistent](https://stackoverflow.com/questions/639668/keeping-testing-and-production-server-environments-clean-in-sync-and-consisten)
###### Network Questions (24)
Configure a virtual interface on your workstation. ***
To be completed.
According to an HTTP monitor, a website is down. You're able to telnet to the port, so how do you resolve it?
If you can telnet to the port, this means that the service listening on the port is running and you can connect to it (it's not a networking problem). It is good to check this way for the IP address to which the domain is resolved and using the same domain to test connection.
First of all check if your site is online from a other location. It then lets you know if the site is down everywhere, or if only your network is unable to view it. It is also a good idea to check what the web browser returns.
**If only IP connection working**
- you can use whois to see what DNS servers serve up the hostname to the site: `whois www.example.com`
- you can use tools like `dig` or `host` to test DNS to see if the host name is resolving: `host www.example.org dns.example.org`
- you can also check global public dns servers: `host www.example.com 9.9.9.9`
If domain not resolved it's probably problem with DNS servers.
**If domain resolved properly**
- investigate the log files and resolve the issue regarding to the logs, it's the best way to show what's wrong
- check the http status code, usually it will be the response with the 5xx, maybe server is overload because clients making lot's of connection to the website or specific url? maybe your caching rules not working properly?
- check web/proxy server configuration (e.g. `nginx -t -c `), maybe another sysadmin has made some changes to the domain configuration?
- maybe something on the server has crashed? maybe run out of space or run out of memory?
- maybe it's a programming error on the website?
Load balancing can dramatically impact server performance. Discuss several load balancing mechanisms. ***
To be completed.
List examples of network troubleshooting tools that can degrade during DNS issues. ***
To be completed.
Explain difference between HTTP 1.1 and HTTP 2.0.
HTTP/2 supports queries multiplexing, headers compression, priority and more intelligent packet streaming management. This results in reduced latency and accelerates content download on modern web pages.
Key differences with **HTTP/1.1**:
- it is binary, instead of textual
- fully multiplexed, instead of ordered and blocking
- can therefore use one connection for parallelism
- uses header compression to reduce overhead
- allows servers to "push" responses proactively into client caches
Useful resources:
- [What is HTTP/2 - The Ultimate Guide](https://kinsta.com/learn/what-is-http2/)
Dev team reports an error: POST http://ws.int/api/v1/Submit/ resulted in a 413 Request Entity Too Large. What's wrong?
**Modify NGINX configuration file for domain**
Set correct `client_max_body_size` variable value:
```bash
client_max_body_size 20M;
```
Restart Nginx to apply the changes.
**Modify php.ini file for upload limits**
Itâ€™s not needed on all configurations, but you may also have to modify the PHP upload settings as well to ensure that nothing is going out of limit by php configurations.
Now find following directives one by one:
```bash
upload_max_filesize
post_max_size
```
and increase its limit to 20M, by default they are 8M and 2M:
```bash
upload_max_filesize = 20M
post_max_size = 20M
```
Finally save it and restart PHP.
Useful resources:
- [413 Request Entity Too Large in Nginx with client_max_body_size set](https://serverfault.com/questions/814767/413-request-entity-too-large-in-nginx-with-client-max-body-size-set)
What is handshake mechanism and why do we need 3 way handshake?
**Handshaking** begins when one device sends a message to another device indicating that it wants to establish a communications channel. The two devices then send several messages back and forth that enable them to agree on a communications protocol.
A **three-way handshake** is a method used in a TCP/IP network to create a connection between a local host/client and server. It is a three-step method that requires both the client and server to exchange `SYN` and `ACK` (`SYN`, `SYN-ACK`, `ACK`) packets before actual data communication begins.
Useful resources:
- [Why do we need a 3-way handshake? Why not just 2-way?](https://networkengineering.stackexchange.com/questions/24068/why-do-we-need-a-3-way-handshake-why-not-just-2-way)
Why is UDP faster than TCP?
**UDP** is faster than **TCP**, and the simple reason is because its nonexistent acknowledge packet (`ACK`) that permits a continuous packet stream, instead of TCP that acknowledges a set of packets, calculated by using the TCP window size and round-trip time (`RTT`).
Useful resources:
- [UDP vs TCP, how much faster is it?](https://stackoverflow.com/questions/47903/udp-vs-tcp-how-much-faster-is-it)
Which, in your opinion, are the 5 most important OpenSSH parameters that improve the security? ***
To be completed.
Useful resources:
- [OpenSSH security and hardening](https://linux-audit.com/audit-and-harden-your-ssh-configuration/)
What is NAT? What is it used for?
It enables private IP networks that use unregistered IP addresses to connect to the Internet. **NAT** operates on a router, usually connecting two networks together, and translates the private (not globally unique) addresses in the internal network into legal addresses, before packets are forwarded to another network.
Workstations or other computers requiring special access outside the network can be assigned specific external IPs using **NAT**, allowing them to communicate with computers and applications that require a unique public IP address. **NAT** is also a very important aspect of firewall security.
Useful resources:
- [Network Address Translation (NAT) Concepts](http://www.firewall.cx/networking-topics/network-address-translation-nat/227-nat-concepts.html)
What is the purpose of Spanning Tree?
This protocol operates at layer 2 of the OSI model with the purpose of preventing loops on the network. Without **STP**, a redundant switch deployment would create broadcast storms that cripple even the most robust networks. There are several iterations based on the original IEEE 802.1D standard; each operates slightly different than the others while largely accomplishing the same loop-free goal.
How to check which ports are listening on my Linux Server?
Use the:
- `lsof -i`
- `ss -l`
- `netstat -atn` - for tcp
- `netstat -aun` - for udp
- `netstat -tulapn`
What mean Host key verification failed when you connect to the remote host? Do you accept it automatically?
`Host key verification failed` means that the host key of the remote host was changed. This can easily happen when connecting to a computer who's host keys in `/etc/ssh` have changed if that computer was upgraded without copying its old host keys. The host keys here are proof when you reconnect to a remote computer with ssh that you are talking to the same computer you connected to the first time you accessed it.
Whenever you connect to a server via SSH, that server's public key is stored in your home directory (or possibly in your local account settings if using a Mac or Windows desktop) file called **known_hosts**. When you reconnect to the same server, the SSH connection will verify the current public key matches the one you have saved in your **known_hosts** file. If the server's key has changed since the last time you connected to it, you will receive the above error.
Don't delete the entire **known_hosts** file as recommended by some people, this totally voids the point of the warning. It's a security feature to warn you that a man in the middle attack may have happened.
Before accepting the new host key, contact your/other system administrator for verification.
Useful resources:
- [Git error: "Host Key Verification Failed" when connecting to remote repository](https://stackoverflow.com/questions/13363553/git-error-host-key-verification-failed-when-connecting-to-remote-repository)
How to send an HTTP request using telnet?
For example:
```bash
telnet example.com 80
Trying 192.168.252.10...
Connected to example.com.
Escape character is '^]'.
GET /questions HTTP/1.0
Host: example.com
HTTP/1.1 200 OK
Content-Type: text/html; charset=utf-8
...
```
How do you kill program using e.g. 80 port in Linux?
To list any process listening to the port 80:
```bash
# with lsof
lsof -i:80
# with fuser
fuser 80/tcp
```
To kill any process listening to the port 80:
```bash
kill $(lsof -t -i:80)
```
or more violently:
```bash
kill -9 $(lsof -t -i:80)
```
or with `fuser` command:
```bash
fuser -k 80/tcp
```
Useful resources:
- [How to kill a process running on particular port in Linux?](https://stackoverflow.com/questions/11583562/how-to-kill-a-process-running-on-particular-port-in-linux/32592965)
- [Finding the PID of the process using a specific port?](https://unix.stackexchange.com/questions/106561/finding-the-pid-of-the-process-using-a-specific-port)
You get curl: (56) TCP connection reset by peer. What steps will you take to solve this problem?
- check if the URL is correct, maybe you should add `www` or set correctly `Host:` header? Check also scheme (http or https)
- check the domain is resolving into a correct IP address
- enable debug tracing with `--trace-ascii curl.dump`. `Recv failure` is a really generic error so its hard for more info
- use external proxy with `--proxy` for debug connection from external ip
- use network sniffer (e.g. `tcpdump`) for debug connection in the lower TCP/IP layers
- check firewall rules on the production environment and on the exit point of your network, also check your NAT rules
- check MTU size of packets traveling over your network
- check SSL version with ssl/tls `curl` params if you connecting to https protocol
- it may be a problem on the client side e.g. the netfilter drop or limit  connections from your IP address to the domain
Useful resources:
- [CURL ERROR: Recv failure: Connection reset by peer - PHP Curl](https://stackoverflow.com/questions/10285700/curl-error-recv-failure-connection-reset-by-peer-php-curl)
How to allow traffic to/from specific IP with iptables?
For example:
```bash
/sbin/iptables -A INPUT -p tcp -s XXX.XXX.XXX.XXX -j ACCEPT
/sbin/iptables -A OUTPUT -p tcp -d  XXX.XXX.XXX.XXX -j ACCEPT
```
How to block abusive IP addresses with pf in OpenBSD?
The best way to do this is to define a table and create a rule to block the hosts, in `pf.conf`:
```bash
table  persist
block on fxp0 from  to any
```
And then dynamically add/delete IP addresses from it:
```bash