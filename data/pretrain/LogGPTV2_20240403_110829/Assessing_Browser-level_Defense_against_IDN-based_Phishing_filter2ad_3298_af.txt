to .zone: An analysis of the new tld land rush. In Proc. of IMC,
2015.
[21] Tristan Halvorson, Kirill Levchenko, Stefan Savage, and Ge-
offrey M. Voelker. Xxxtortion? inferring registration intent in
the .xxx tld. In Proc. of WWW, 2014.
[22] Xiao Han, Nizar Kheir, and Davide Balzarotti. Phisheye: Live
monitoring of sandboxed phishing kits. In Proc. of CCS, 2016.
[23] D.J. Hauser and N. Schwarz. Attentive turkers: Mturk partici-
pants perform better on online attention checks than do subject
pool participants. Behavior Research Methods, 48:400–407,
2016.
[24] Grant Ho, Aashish Sharma, Mobin Javed, Vern Paxson, and
David Wagner. Detecting credential spearphishing in enterprise
settings. In Proc. of USENIX Security, 2017.
[25] Tobias Holgers, David E. Watson, and Steven D. Gribble. Cut-
ting through the confusion: A measurement study of homo-
graph attacks. In Proc. of USENIX ATC, 2006.
[26] Jason Hong. The state of phishing attacks. Communications
of the ACM, 55(1), 2012.
[27] Hang Hu and Gang Wang. End-to-end measurements of email
spooﬁng attacks. In Proc. of USENIX Security, 2018.
[28] IETF.org. Internationalizing domain names in applications
(IDNA), 2003. https://tools.ietf.org/html/rfc3490.
[29] Panagiotis Kintis, Najmeh Miramirkhani, Charles Lever,
Yizheng Chen, Rosa Romero-Gómez, Nikolaos Pitropakis,
Nick Nikiforakis, and Manos Antonakakis. Hiding in plain
sight: A longitudinal study of combosquatting abuse. In Proc.
of CCS, 2017.
[30] Maciej Korczynski, Maarten Wullink, Samaneh Tajal-
izadehkhoob, Giovane C. M. Moura, Arman Noroozian, Drew
Bagley, and Cristian Hesselman. Cybercrime after the sunrise:
A statistical analysis of dns abuse in new gtlds. In Proc. of
Asia CCS, 2018.
[31] Viktor Krammer. Phishing defense against IDN address spoof-
ing attacks. In Proc. of PST, 2006.
[32] Brian Krebs.
name, 2019.
its-way-too-easy-to-get-a-gov-domain-name/.
It’s way too easy to get a .gov domain
https://krebsonsecurity.com/2019/11/
[33] Ponnurangam Kumaraguru, Yong Rhee, Alessandro Acquisti,
Lorrie Faith Cranor, Jason Hong, and Elizabeth Nunge. Pro-
tecting people from phishing: The design and evaluation of an
embedded training email system. In Proc. of CHI, 2007.
[34] LambdaTest. Lambdatest: Cross browser testing cloud, 2020.
https://www.lambdatest.com/.
[35] Victor Le Pochat, Tom Van Goethem, and Wouter Joosen.
Funny accents: Exploring genuine interest in internationalized
domain names. In Proc. of PAM, 2019.
[36] Eric Lin, Saul Greenberg, Eileah Trotter, David Ma, and John
Aycock. Does domain highlighting help people identify phish-
ing sites? In Proc. of CHI, 2011.
[37] Baojun Liu, Chaoyi Lu, Zhou Li, Ying Liu, Hai-Xin Duan,
Shuang Hao, and Zaifeng Zhang. A reexamination of interna-
tionalized domain names: The good, the bad and the ugly. In
Proc. of DSN, 2018.
[38] Meng Luo, Pierre Laperdrix, Nima Honarmand, and Nick Niki-
forakis. Time does not heal all wounds: A longitudinal analysis
of security-mechanism support in mobile browsers. In Proc.
of NDSS, 2019.
[39] Meng Luo, Oleksii Starov, Nima Honarmand, and Nick Niki-
forakis. Hindsight: Understanding the evolution of ui vulnera-
bilities in mobile browsers. In Proc. of CCS, 2017.
[40] D. Kevin McGrath and Minaxi Gupta. Behind phishing: An
examination of phisher modi operandi. In Proc. of LEET, 2008.
[41] Microsoft. Changes to IDN in IE7 to now allow mixing of
scripts, 2006.
https://docs.microsoft.com/en-us/archive/blogs/
ie/changes-to-idn-in-ie7-to.
[42] Microsoft. Lifecycle FAQ - Internet explorer and Edge, 2016.
https://docs.microsoft.com/en-us/lifecycle/faq/
internet-explorer-microsoft-edge.
[43] Paul Mockapetris. Domain names - concepts and facili-
ties. RFC 1034, 1987. https://tools.ietf.org/html/
rfc1034.
[44] Tyler Moore and Benjamin Edelman. Measuring the perpetra-
tors and funders of typosquatting. In International Conference
on Financial Cryptography and Data Security, 2010.
[45] Mozilla. Firefox IDN display algorithm, 2017. https://
wiki.mozilla.org/IDN_Display_Algorithm.
[46] NetMarketShare. Browser market share, 2020. https://
netmarketshare.com/browser-market-share.aspx.
[47] Nick Nikiforakis, Steven Van Acker, Wannes Meert, Lieven
Desmet, Frank Piessens, and Wouter Joosen. Bitsquatting:
Exploiting bit-ﬂips for fun, or proﬁt? In Proc. of WWW, 2013.
[48] Adam Oest, Yenganeh Safaei, Penghui Zhang, Brad Wardman,
Kevin Tyers, Yan Shoshitaishvili, Adam Doupé, and Gail-Joon
Ahn. Phishtime: Continuous longitudinal measurement of the
effectiveness of anti-phishing blacklists. In Proc. of USENIX
Security, 2020.
[49] Adam Oest, Penghui Zhang, Brad Wardman, Eric Nunes, Jakub
Burgis, Ali Zand, Kurt Thomas, Adam Doupé, and Gail-Joon
Ahn. Sunrise to sunset: Analyzing the end-to-end life cycle and
effectiveness of phishing attacks at scale. In Proc. of USENIX
Security, 2020.
[50] Peng Peng, Chao Xu, Luke Quinn, Hang Hu, Bimal Viswanath,
and Gang Wang. What happens after you leak your password:
Understanding credential sharing on phishing sites. In Proc.
of Asia CCS, 2019.
[51] Peng Peng, Limin Yang, Linhai Song, and Gang Wang. Open-
ing the blackbox of virustotal: Analyzing online phishing scan
engines. In Proc. of IMC, 2019.
[52] Shahrooz Pouryousef, Muhammad Daniyal Dar, Suleman Ah-
mad, Phillipa Gill, and Rishab Nithyanand. Extortion or ex-
pansion? an investigation into the costs and consequences of
icann’s gtld experiments. In Proc. of PAM, 2020.
[53] Pawan Prakash, Manish Kumar, Ramana Rao Kompella, and
Minaxi Gupta. Phishnet: Predictive blacklisting to detect phish-
ing attacks. In Proc. of INFOCOM, 2010.
[54] F. Quinkert, T. Lauinger, W. Robertson, E. Kirda, and T. Holz.
It’s not what it looks like: Measuring attacks and defensive
registrations of homograph domains. In Proc. of CNS, 2019.
[55] Anirudh Ramachandran, Nick Feamster, and Santosh Vempala.
Filtering spam with behavioral blacklisting. In Proc. of CCS,
2007.
[56] P. Resnick and P. Hoffman. Mapping characters for interna-
tionalized domain names in applications (IDNA). RFC 5895,
2008. https://tools.ietf.org/html/rfc5895.
[57] Joshua Reynolds, Deepak Kumar, Zane Ma, Rohan Subrama-
nian, Meishan Wu, Martin Shelton, Joshua Mason, Emily Stark,
and Michael Bailey. Measuring identity confusion with uni-
form resource locators. In Proc. of CHI, 2020.
[58] Richard Roberts, Yaelle Goldschlag, Rachel Walter, Taejoong
Chung, Alan Mislove, and Dave Levin. You are who you
appear to be: A longitudinal study of domain impersonation in
tls certiﬁcates. In Proc. of CCS, 2019.
[59] Stuart Schechter, Rachna Dhamija, Andy Ozment, and Ian C
Fischer. The emperor’s new security indicators an evaluation
of website authentication and the effect of role playing on
usability studies. In Proc. of IEEE SP, 2007.
[60] StatCounter. Browser market share worldwide, 2020. https:
//gs.statcounter.com/browser-market-share.
[61] Hiroaki Suzuki, Daiki Chiba, Yoshiro Yoneya, Tatsuya Mori,
and Shigeki Goto. Shamﬁnder: An automated framework for
detecting IDN homographs. In Proc. of IMC, 2019.
[62] Janos Szurdi, Balazs Kocso, Gabor Cseh, Jonathan Spring,
Mark Felegyhazi, and Chris Kanich. The long "Taile" of ty-
posquatting domain names.
In Proc. of USENIX Security,
2014.
[63] Christopher Thompson, Martin Shelton, Emily Stark, Maxim-
ilian Walker, Emily Schechter, and Adrienne Porter Felt. The
web’s identity crisis: Understanding the effectiveness of web-
site identity indicators. In Proc. of USENIX Security, 2019.
[64] Ke Tian, Steve T. K. Jan, Hang Hu, Danfeng Yao, and Gang
Wang. Needle in a haystack: Tracking down elite phishing
domains in the wild. In Proc. of IMC, 2018.
[65] Unicode.org. Unicode confusables, 2015.
https://www.unicode.org/Public/security/8.0.0/
confusables.txt.
[66] Unicode.org. Unicode 13.0.0, 2020. https://unicode.org/
versions/Unicode13.0.0/.
[67] Amber van der Heijden and Luca Allodi. Cognitive triaging of
phishing attacks. In Proc. of USENIX Security, 2019.
[68] Javier Vargas, Alejandro Correa Bahnsen, Sergio Villegas, and
Daniel Ingevaldson. Knowing your enemies: leveraging data
analysis to expose phishing patterns against a major us ﬁnancial
institution. In Proc. of eCrime, 2016.
[69] W3Counter. Browser & platform market share, 2020. https:
//www.w3counter.com/globalstats.php.
[70] Jingguo Wang, Tejaswini Herath, Rui Chen, Arun Vishwanath,
and H. Raghav Rao. Research article phishing susceptibility:
An investigation into the processing of a targeted spear phish-
ing email. IEEE Transactions on Professional Communication,
55(4):345–362, 2012.
[71] Colin Whittaker, Brian Ryner, and Marria Nazif. Large-scale
automatic classiﬁcation of phishing pages. In Proc. of NDSS,
2010.
[72] Yue Zhang, Serge Egelman, Lorrie Cranor, and Jason Hong.
Phinding Phish: Evaluating Anti-Phishing Tools. In Proc. of
NDSS, 2007.
[73] Yue Zhang, Jason I Hong, and Lorrie F Cranor. Cantina: a
content-based approach to detecting phishing web sites. In
Proc. of WWW, 2007.
[74] Xudong Zheng. Phishing with unicode domains, 2017. https:
//www.xudongz.com/blog/2017/idn-phishing/.
Appendix-A: Pilot Studies
To explore the design space, we conducted 4 pilot studies to
experiment with different design choices, as shown in Table 9.
We framed the questions slightly differently in each pilot
study to prime users to focus on the domain names. Below,
we use bankofamerica.com as an example website.
In pilot study 1, we presented users with the screenshot and
asked: “is the domain name shown in the browser address bar
bankofamerica.com?” By explicitly mentioning the address
bar, we cued users to examine the address bar.
In pilot study 2, we asked the same question albeit with
5-point Likert scale answer options: “1 - I’m very conﬁdent it
is,” “2 - I’m somewhat conﬁdent it is,” “3 - I can’t tell,” “4 -
I’m somewhat conﬁdent it is not,” and “5 - I’m very conﬁdent
it is not.” We tested it to see whether having ﬁner-grained
answers would help differentiate conditions (e.g., detection
rates of IDNs that are blocked vs. not blocked by Chrome).
In pilot study 3, we tried to avoid priming users to focus
on the address bar. We just asked “is the website the real
bankofamerica.com?” We tested this version because in
practice users might not pay attention to the address bar when
browsing the web. This should give us a lower bound estimate
of people’s IDN detection rate.
In pilot study 4, we drew users’ attention to the domain
name even more by placing the homograph domain name
directly in the question. We asked “is bankofamerl,ca.com
the same as bankofamerica.com?” We essentially asked the
users to compare the two domain names side-by-side. We
tested this version because it should give us an upper bound
estimate of people’s IDN detection rate.
Website Selection. We select diverse websites from ﬁve
common website categories: “Shopping,” “Banking,” “Social
Networking,” “Education” and “Government & Military.” For
each category, we selected two sets of domains: popular and
unpopular domains. The popular domains were randomly
selected from the Chromium top domain list (3 domains per
set). In total, we selected 5⇥ 2⇥ 3 = 30 domain names.
For each target domain, we then generated two homograph
IDNs: one IDN that can be blocked by the latest Chrome (IDN-
Block), and the other IDN can bypass Chrome’s policy (IDN-
Pass). Thus, for each target domain, we had three choices:
IDN-block, IDN-pass, and the real domain name.
Pilot Study Results.
In April 2020, we conducted the
four pilot studies on MTurk. Each participant examined 30
websites. For each website, we randomly chose to display the
real, IDN-Block, or IDN-Pass domain name. Each participant
can only participate in one of the pilot studies and for only
once. Each participant was compensated $1 for their time.
Study
Pilot 1
Pilot 2
Pilot 3
Pilot 4
Experimental Setups
Question
“Is the domain name shown in the browser
address bar [target domain x]?”
“Is the domain name shown in the browser
address bar [target domain x]?”
“Is the website the real [target domain x]?”
“Is [homograph domain name] the same as
[target domain x]?”
Priming Answer
Real
Error Rate
IDN-Block
IDN-Pass
# Participants
(# Answers)
Medium
Medium
Light
Heavy
Binary
Likert
Scale
Binary
Binary
8.75%
46.25%
31.07%
20 (600)
1.98%⇤
16.67%
8.75%
53.29%⇤
55.56%
22.5%
39.85%⇤
50.79%
16.43%
19 (570)
18 (540)
20 (600)
Table 9: Pilot study set up and their results. *Note that for pilot study 2, we used a ﬁve-point Likert scale. We regard the ﬁrst two
points (i.e., “very conﬁdent” and “somewhat conﬁdent”) as the “yes” answer to calculate the error rate. All other pilot studies
used the binary answers of “yes” and “no” plus “I can’t tell.”
To attract serious workers on MTurk, we used commonly
applied ﬁlters: we recruited U.S. workers who have an ap-
proval rate greater than 90%, and have completed more than
50 approved tasks. For each pilot study, we recruited 18 –
20 participants. In total, we had 77 participants with 2,310
answers (domain names).
In Table 9, we show the error rate for the questions in each
study. More speciﬁcally, if participants answered “Yes”, it
means they believed the site was the real site. As such, for
real websites, answering “Yes” is correct; for IDN websites,
answering “Yes” is incorrect. Note that for pilot study 2 where
we used a 5-point Likert scale, we considered the ﬁrst two
answers as “YES.” Across the four studies, we have two con-
sistent observations. First, participants performed well when
they are presented with the real domain names. Across the
four pilot studies, users’ error rates are between 1.98% to
16.67%. Second, when displaying homograph IDNs (either
IDN-Block or IDN-Pass), there was a large percentage of
wrong answers (i.e., a high error rate). For example, when
showing IDN-pass, 16.43% – 50.79% of the times users mis-
took it as the real domain name.
After comparing the results of the pilot studies, we decided
to choose the setting of Pilot-1 as our main study for the
following reasons. First, comparing Pilot-1 and Pilot-2, we
did not observe a need to use a 5-point Likert scale as the trend
was the same for both conditions and using the Likert scale
can complicate the tasks. People might also interpret the ﬁve
levels differently. Instead, a binary answer (plus “I can’t tell”)
can reduce the ambiguity. Pilot-3 did not prime users to check
the domain names in the address bar. Table 9 shows users were
more likely to make mistakes as we expected (a lower bound
of detection rate). Given that our goal was to test the impact of
homograph IDNs, we wanted to examine whether people can
identify homograph IDNs when they looked at the domain
names. As such the setting of Pilot-3 was not adopted. Pilot-4
represented the other extreme by over-priming users: forcing
users to compare the displayed domain names with the real
Demographics
Gender
Male
Female
Age
18-29
30-39
40-49
50 or above
Education Level
High school graduate or less
Some college or two-year associate degree
Bachelor’s degree
Some graduate school
Master’s or professional degree
Ph.D.
Browser Use History
Less than a year
1-3 years
3-5 years
More than 5 years
Computing Background
Yes
No
Prefer not to answer
# Participants
139
78
75
83
36
23
18
42
114
11
29
3
2
8
25
182
81
130
6
Table 10: Demographic information of participants of the
main user study (N=217). We only include participants who
passed the attention check.
domain names. Table 9 shows that users had an lower error
rate as we expected (performance upper bound). However,
Pilot-4’s setting is too unrealistic as we cannot expect users
to do this when browsing the Internet.
Appendix-B: Main Study Information
Table 10 shows the demographic information of participants.