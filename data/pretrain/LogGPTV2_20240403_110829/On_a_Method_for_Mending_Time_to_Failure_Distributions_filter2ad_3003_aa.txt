title:On a Method for Mending Time to Failure Distributions
author:Michael Grottke and
Kishor S. Trivedi
On a Method for Mending Time to Failure Distributions
Michael Grottke∗ and Kishor S. Trivedi
Department of Electrical & Computer Engineering
Duke University, Durham, NC 27708-0291, USA
{grottke, kst}@ee.duke.edu
Abstract
Many software reliability growth models assume that the
time to next failure may be inﬁnite; i.e., there is a chance
that no failure will occur at all. For most software prod-
ucts this is too good to be true even after the testing phase.
Moreover, if a non-zero probability is assigned to an inﬁ-
nite time to failure, metrics like the mean time to failure
do not exist. In this paper, we try to answer several ques-
tions: Under what condition does a model permit an inﬁnite
time to next failure? Why do all non-homogeneous Poisson
process (NHPP) models of the ﬁnite failures category share
this property? And is there any transformation mending the
time to failure distributions? Indeed, such a transformation
exists; it leads to a new family of NHPP models. We also
show how the distribution function of the time to ﬁrst failure
can be used for unifying ﬁnite failures and inﬁnite failures
NHPP models.
1. Introduction
Despite the advances made with respect to the develop-
ment of techniques and tools supporting the requirements
analysis, the design and the implementation of software, the
correctness of computer programs cannot be guaranteed. It
is always possible that a piece of software contains faults
(e.g., buggy lines of code) leading to deviations of the ac-
tual software behavior from its speciﬁcation. Such observed
deviations are referred to as failures.
Since the number of software faults, their location in
the code and the sequence of user inputs are not pre-
determined, the times at which failures are experienced are
random. Let the continuous random variable Xi represent
the time between the (i−1)st and the ith failure occurrence,
also called the ith time to failure (TTF). For a program that
has already been released, we hope that all realizations of
the TTFs are large values; i.e., the software should only fail
∗Corresponding author, on leave of absence from the Chair of Statis-
tics and Econometrics, University of Erlangen-Nuremberg, Germany. This
work was supported by a fellowship within the Postdoc Program of the
German Academic Exchange Service (DAAD).
rarely. This means that due to the characteristics of the soft-
ware and the execution proﬁle, each random variable Xi
should have a density function assigning a large fraction of
the probability mass to long inter-failure times. In an ideal
scenario, in which the software cannot fail again, the entire
probability mass of the ith TTF Xi is assigned to inﬁnity.
This may happen either if the software is fault-free after the
correction of the (i− 1)st fault or if the remaining faults are
located in parts of the software that will never be executed.
If there is a chance that no fault is left in those regions of
the software (eventually) used according to the operational
proﬁle, then a probability between zero and one is attached
to inﬁnity. In this case, the distribution function of Xi does
not reach the value one as x approaches inﬁnity:
lim
x→∞ FXi(x)  0 and limx→∞ FXi(x) = 1−exp (−c) < 1.
A possible explanation as to why ri−1(t) may decrease at all
although no failure occurs (and hence no fault is corrected)
is a subjective one: The longer the software has been run-
ning without showing a failure, the higher is the conﬁdence
that it will not fail in the future.
(cid:90) ti−1+x
1A more general model class containing additional SRGMs is the self-
exciting point process (SEPP). The following discussion of the relation-
ships between the transitions rates, the program hazard rate and the failure
intensity function is based on the software reliability literature dealing with
SEPPs, see [2, 4, 7, 13, 20].
3. Defective TTF distributions in NHPP models
For non-homogeneous Poisson process (NHPP) models,
all transition rates r0(t), r1(t), ... are functions of time t, but
In Proc. International Conference on Dependable Systems and Networks 2005, Los Alamitos, 2005, pp. 560–569 c(cid:176) IEEE
561
 0 ...1 u0 ...0()rt1()rt01()urt-0()urt  they are independent of the number of previous failure oc-
currences M(t). Therefore, they are the same function r(t).
As a consequence, the program hazard rate Z(t, M(t)) is
not a random variable, but a deterministic function z(t) of
time, and it is identical to the function r(t). Moreover, it is
identical to the failure intensity λ(t). Hence,
λ(t) = z(t) = r(t) = r0(t) = r1(t) = . . . .
(3)
The model assumptions imply that M(t) follows a Poisson
distribution with expectation given by the mean value func-
tion µ(t) connected to equation (3). Specifying either the
failure intensity function or the mean value function fully
determines the NHPP model.
Given the observed failure time ti−1, the reliability of
(cid:182)
(4)
λ(y) dy
the software in the interval (ti−1, ti−1 + x] is
R(x | ti−1, M(ti−1) = i − 1) = exp
= exp (−µ(ti−1 + x) + µ(ti−1)) ,
−
ti−1
(cid:181)
(cid:90) ti−1+x
and the distribution function of Xi is
FXi(x) = 1 − exp (−µ(ti−1 + x) + µ(ti−1)) .
(5)
Whether the distribution of the time to the ith failure is de-
fective or not depends on the behavior of µ(ti−1 + x) as x
approaches inﬁnity.
3.1. Finite failures category NHPP models
Musa et al. [17, pp. 250–251] refer to SRGMs for which
the expected number of failures experienced in inﬁnite time
is ﬁnite as “ﬁnite failures category models”. We follow Kuo
and Yang [12] in calling the NHPP models of this category
“NHPP-I” models. These models can be derived by assum-
ing that the detection times of all n faults present in the soft-
ware at the beginning of testing are independently and iden-
tically distributed (iid) with distribution function G(t). This
means that all faults are equally dangerous with respect to
their tendency of causing a failure, but they do not inﬂuence
each other. Provided that upon detection each fault is cor-
rected perfectly and without introducing any new fault into
the software, the me failure occurrence times observed dur-
ing testing are the ﬁrst me order statistics of n iid random
variables with density function g(t) = dG(t)/dt. From this
general order statistics model [12] we obtain the NHPP-I
model if the initial number of faults is not deterministic, but
a random variable N following a Poisson distribution with
expected value ν. The mean value function of the resulting
NHPP-I models has the general form [17, p. 269]
µ(t) = νG(t).
(6)
Taking a different point of view, we may stress not the ten-
dency of faults to show themselves but the testers’ efforts
in ﬁnding them by executing as many different parts of the
software as possible. If the faults are uniformly distributed
over the program, then G(t) can be interpreted as a cover-
age function [6, 19].
It should be pointed out that the derivation of NHPP-I
models via order statistics need not be based on iid fault
detection times. In the approach sketched above, the haz-
ard rates of all faults are possibly time-varying but identical
za(t) = g(t)/[1− G(t)]. In contrast to this, Miller [15] pro-
posed that the per-fault hazard rates are constant over time,
but they may be nonidentical; he showed that many NHPP
models and other SRGMs ﬁt into this framework.
The distribution function G(t) is usually assumed to be
non-defective, implying that each fault will eventually lead
to a failure. In the well-known Goel-Okumoto model [5],
for example, G(t) is the non-defective function