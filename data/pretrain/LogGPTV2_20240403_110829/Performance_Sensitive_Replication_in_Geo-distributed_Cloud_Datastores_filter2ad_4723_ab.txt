### Quorum Sizes and Access Patterns

We anticipate that our formulations will be applicable to classes of items with similar access patterns. For example, while the access patterns for Wikipedia may vary across different languages, documents within a specific language tend to have accesses from the same geographic regions and can thus be grouped together. Systems like Spanner [24] require applications to bucket items into "directories," where all items in a directory share the same replica configuration. Our formulations will be applied at the granularity of these directories.

In this section, we focus on latency under normal operation. In Sections VI and VI-B, we will extend our models to consider latency under failure conditions and incorporate communication costs.

### A. Meeting SLA Targets Under Normal Operation

We consider scenarios where the datastore is deployed across up to \( M \) geographically distributed data centers (DCs). \( D_{ij} \) denotes the time to transfer a data item from DC \( j \) to DC \( i \). For the applications we are considering, the size of objects is typically small (e.g., tweets, metadata, small text files), and thus data transmission times are primarily dominated by propagation delays rather than bandwidth. Therefore, the parameter \( D_{ij} \) in our formulations and evaluations is based on round-trip times between DCs. For applications dealing with large data objects, the measured \( D_{ij} \) values would also capture the impact of data size and bandwidth.

Our focus is on scenarios where the load on the storage node is moderate, and the primary component of access latency is network delay. Hence, we do not model processing delays at the datastore node, which are less critical in the context of geo-replication.

We do not model implementation-specific details. For example, in Cassandra, a read operation retrieves the full item from the closest replica and digests from the others. If a more recent value is found in a non-closest replica, additional latency is incurred to fetch the actual item. We do not model this additional latency because the probability that a digest has the latest value is difficult to estimate and is small in practice. Our experimental results in Section VIII demonstrate that, despite this assumption, our models work well in practice.

Let \( x_i \) be a binary indicator variable that is 1 if DC \( i \) holds a replica of the data item. Let \( Q_r \) and \( Q_w \) be the read and write quorum sizes, and \( T_r \) and \( T_w \) denote the latency thresholds within which all read and write accesses must successfully complete. Let \( q_{rij} \) and \( q_{wij} \) be indicator variables that are 1 if read and write accesses originating from DC \( i \) use a replica in location \( j \) to meet their quorum requirements.

Typical service level agreements (SLAs) require bounds on the delays seen by a specified percentage of requests. Let \( p_r \) and \( p_w \) denote the fractions of read and write requests, respectively, that must have latencies within the desired thresholds. A key observation is that, given the replica locations, all read and write requests originating from a given DC encounter the same delay. Thus, it suffices to choose a set of DCs such that the read (resp. write) requests originating at these DCs experience a latency no more than \( T_r \) (resp. \( T_w \)), and these DCs account for a fraction \( p_r \) (resp. \( p_w \)) of read (resp. write) requests. Let \( N_r^i \) (resp. \( N_w^i \)) denote the number of read (write) requests originating from DC \( i \). Let \( Y_r^i \) (resp. \( Y_w^i \)) be indicator variables that are 1 if reads (resp. writes) from DC \( i \) meet the delay thresholds. Then, we have:

\[
q_{lij} \leq x_j \quad \forall i, j; l \in \{r, w\}
\]
\[
D_{ij} q_{lij} \leq T_l \quad \forall i, j; l \in \{r, w\}
\]
\[
\sum_j q_{lij} \geq Q_l Y_l^i \quad \forall i; l \in \{r, w\}
\]
\[
\sum_i N_l^i Y_l^i \geq p_l \sum_i N_l^i \quad \forall i; l \in \{r, w\}
\]

Equations (2) and (3) ensure that DC \( i \) can use a replica in DC \( j \) to meet its quorum only if (i) there exists a replica in DC \( j \); and (ii) DC \( j \) is within the desired latency threshold from DC \( i \). Equation (4) ensures that, within \( i \)'s quorum set, there are sufficiently many replicas that meet the above feasibility constraints for the selected DCs. Equation (5) ensures that the selected DCs account for the desired percentage of requests.

To determine the lowest latency threshold for which a feasible placement exists, we treat \( T_r \) and \( T_w \) as optimization variables and minimize the maximum of the two variables. We allow weights \( a_r \) and \( a_w \) on read and write delay thresholds to enable an application designer to prioritize reads over writes (or vice versa). In summary, we have the Latency Only (LAT) model:

\[
\text{(LAT)} \quad \min \quad T
\]
\[
\text{subject to} \quad T \geq a_l T_l \quad \forall l \in \{r, w\}
\]
\[
Q_r + Q_w = \sum_j x_j + 1
\]
\[
\text{Quorum constraints (2), (3), (4)}
\]
\[
\text{Percentile constraints (5)}
\]
\[
Q_l \in \mathbb{Z}, \quad q_{lij}, x_j, Y_l^i \in \{0, 1\} \quad \forall i, j; l \in \{r, w\}
\]

Note that the constraint on quorum sizes captures the strict quorum requirement (Section II) that each read sees the action of the last write. Also, when \( p_r = p_w = 1 \), (LAT) minimizes the delay of all requests, and we refer to this special case as (LATM). Finally, while (4) is not linear, it can be easily linearized as shown in [47]. Hence, our model can be solved using integer linear programming (ILP) solvers like CPLEX [6].

### B. How Much Can Replication Lower Latency?

Given the consistency requirement of quorum datastores, can replication lower latency, and if so, by how much? In this section, we present examples to show that replication can lower latency and provide bounds on the replication benefit (ratio of optimal latency without and with replication). Two key factors in assessing the benefits of replication are (i) symmetric/asymmetric spread: whether read and write requests originate from an identical or different set of DCs; and (ii) symmetric/asymmetric weights: whether the weights attached to read and write latency thresholds (\( a_r, a_w \)) are identical or different.

Figure 4 shows an example where the spread and weights are symmetric, and the replication benefit is \( \sqrt{3} \approx 1.732 \). When replicas can be placed arbitrarily on a Euclidean plane, it can be shown via an application of Helly's theorem [17] that the replication benefit is bounded by \( \sqrt{3} \). Figure 4 shows that this is a tight bound since replication achieves this benefit over single placement at the centroid of the triangle. The replication benefit can be even higher with asymmetric weights, as seen in the following observation.

**Observation 1:** With asymmetric spreads and metric delays, the replication benefit for (LATM) and (LAT) is at most \( \frac{4 \max(a_r, a_w)}{\min(a_r, a_w)} \).

The proof can be found in our technical report [47].

### VI. Achieving Latency SLAs Despite Failures

So far, we have focused on replication strategies that optimize latency under normal conditions. In this section, we discuss failures that may impact entire DCs and present strategies resilient to such failures.

### A. Failure-Resilient Replication Strategies

While several techniques exist to protect against individual failures within a DC [27], geo-distributed DCs are primarily motivated by failures that impact entire DCs. While failures within a DC have been studied [27, 32], there are few studies on failures across DCs. Discussions with practitioners suggest that while DC-level failures are not uncommon (Figure 1), correlated failures of multiple geographically distributed DCs are relatively rare (though feasible). Operators strive to minimize simultaneous downtime of multiple DCs through careful scheduling of maintenance periods and gradual roll-out of software upgrades.

While a sufficiently replicated geo-distributed cloud datastore may be available despite a DC failure, the latency is likely to be negatively impacted. We present replication strategies that are resilient to such failures. Pragmatically, we first focus on the common case scenario of single DC failures. Then, in Section VI-B, we show how our models easily extend to more complex failure modes. Our models are:

**Basic Availability Model (BA):** This model optimizes latency using (LAT) with the additional constraints that the read and write quorum sizes are at least 2 (and hence the number of replicas is at least 3). Clearly, read and write requests can still achieve quorum when one DC is down, and basic availability is maintained. This model does not explicitly consider latency under failure, and our evaluations in Section VIII indicate that the scheme may perform poorly under failures. For example, the 90th percentile request latency for English Wikipedia documents increased from 200 ms to 280 ms when one replica was unavailable.

**N-1 Contingency Model (N-1C):** This model minimizes the maximum latency across a pre-specified percentile of reads and writes, allowing at most one DC to be unavailable at any given time. The model is motivated by contingency analysis techniques commonly employed in power transmission systems [36] to assess the ability of a grid to withstand a single component failure. Although this model is similar in structure to (LAT), there are two important distinctions. First, the quorum requirements must be met not just under normal conditions but under all possible single DC failures. Second, the desired fraction of requests serviced within a latency threshold could be met by considering requests from different DCs under different failure scenarios.

Formally, let \( p_r^f \) (resp. \( p_w^f \)) be the fraction of reads (resp. writes) that must meet the delay thresholds when a replica in any DC is unavailable. Note that the SLA requirement on failures may be more relaxed, possibly requiring a smaller fraction of requests to meet a delay threshold. Let \( Y_r^{ik} \) (resp. \( Y_w^{ik} \)) be indicator variables that are 1 if read (resp. write) requests from DC \( i \) are served within the latency threshold when the replica in DC \( k \) is unavailable. Then, we replace (5) and (4) with the following:

\[
\sum_k Y_l^{ik} \geq p_l^f \sum_i N_l^i \quad \forall i, k; l \in \{r, w\}
\]
\[
\sum_{j \neq k} q_{lij} \geq Q_l Y_l^{ik} \quad \forall i, k; l \in \{r, w\}
\]

The first constraint ensures that sufficient requests are serviced within the latency threshold, regardless of which DC fails. The index \( k \) for the \( Y \) variables allows the set of requests satisfied within the latency threshold to depend on the DC that fails. The second constraint ensures that the quorum requirements are met when DC \( k \) fails, with the caveat that DC \( k \) cannot be used to meet quorum requirements. We remark that (7) can be linearized in a manner similar to (4). Putting everything together, we have:

\[
\text{(N-1C)} \quad \min \quad T_f
\]
\[
\text{subject to} \quad T_f \geq a_l T_l \quad \forall l \in \{r, w\}
\]
\[
Q_r + Q_w = \sum_j x_j + 1
\]
\[
\text{Quorum constraints (2), (3), (7)}
\]
\[
\text{Percentile constraints (6)}
\]