quorum sizes.
We expect our formulations to be applied over classes of
items that see similar access patterns. For e.g., while ac-
cess patterns for Wikipedia vary across languages, documents
242242242
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:03:40 UTC from IEEE Xplore.  Restrictions apply. 
PARAMETERS AND INPUTS TO THE MODEL
TABLE II
Term Meaning
M
Dij
Ci
N l
i
T l
pl
xi
ql
ij
Ql
Y l
i
Y l
ik
Number of available DCs.
Access latency between DCs i and j.
Cost of outgoing trafﬁc at DC i.
Number of reads/writes from DC i.
Read/Write Latency Threshold.
Fraction of requests to be satisﬁed within T l
Whether DC i hosts a replica.
Whether i’s requests use replica in j to meet quorum.
Quorum size.
Whether requests from i are satisﬁed within T l
Whether requests from i are satisﬁed within T l
on failure of replica in k.
Whether reads from i fetch the full data item from j.
l ∈ r, w indicates if term refers to reads/writes.
nij
l
.
.
within a language see accesses from the same geographic
regions, and could be grouped together. Systems like Span-
ner [24] require applications to bucket items into “directories”,
and items in the bucket see the same replica conﬁguration. Our
formulations would be applied at the granularity of directories.
In this section, we focus on latency under normal operation.
In Sections VI and VI-B, we show how our models may be
extended to consider latency under failure, and incorporate
communication costs.
A. Meeting SLA targets under normal operation
We consider settings where the datastore is deployed in up
to M geographically distributed DCs. Dij denotes the time to
transfer a data item from DC j to DC i. For the applications
we consider, the size of objects is typically small (e.g., tweets,
meta-data, small text ﬁles etc.), and hence data transmission
times are typically dominated by propagation delays rather
the Dij
than the bandwidth between the DCs. Therefore,
parameter in our formulations (and evaluation) are based on
the round trip times between the DCs. For applications dealing
with large data objects, the measured Dij values would capture
the impact of data size and bandwidth as well.
Our focus is on regimes where the load on the storage node
is moderate, and the primary component of the access latency
is the network delay. Hence, we do not model the processing
delays at the datastore node which are not as critical in the
context of geo-replication.
We do not model details speciﬁc to implementation – e.g.,
on a read operation, the Cassandra system retrieves the full
item from only the closest replica, and digests from the others.
If a replica besides the closest has a more recent value,
additional latency is incurred to fetch the actual item from
that replica. We do not model this additional latency since
the probability that a digest has the latest value is difﬁcult
to estimate and small in practice. Our experimental results
in Section VIII demonstrate that, despite this assumption, our
models work well in practice.
Let xi be a binary indicator variable which is 1 iff DC i
and Qw
holds a replica of the data item. Let Qr
be the read
and write quorum sizes, and T r
and T w
respectively denote
the latency thresholds within which all read and write accesses
ij and qw
to the data item must successfully complete. Let qr
ij
respectively be indicator variables that are 1 if read and write
accesses originating from DC i use a replica in location j to
meet their quorum requirements.
and pw
Typical SLAs require bounds on the delays seen by a pre-
speciﬁed percentage of requests. Let pr
denote the
fraction of read and write requests respectively that must have
latencies within the desired thresholds. A key observation is
that, given the replica locations, all read and, similarly all
write requests, that originate from a given DC encounter the
same delay. Thus, it sufﬁces that the model chooses a set of
DCs so that the read (resp. write) requests originating at these
(resp. T w
DCs experience a latency no more than T r
) and
these DCs account for a fraction pr
(resp. pw
) of read (resp.
write) requests. Let N r
i ) denote the number of read
(write) requests originating from DC i. Let Y r
(resp. Y w
i )
i
denote indicator variables which are 1 iff reads (resp. writes)
from DC i meet the delay thresholds. Then, we have :
i (resp. N w
ij ≤ xj ∀i, j l ∈ {r, w}
ql
ij ≤ T l ∀i, j l ∈ {r, w}
Dij ql
∀i; l ∈ {r, w}
ij ≥ QlY l
ql
i
(cid:2)
j
i ≥ pl
N l
i Y l
(cid:2)
i
(cid:2)
i
N l
i
∀i; l ∈ {r, w}
(2)
(3)
(4)
(5)
Equations (2) and (3) require that DC i can use a replica in
DC j to meet its quorum only if (i) there exists a replica in DC
j; and (ii) DC j is within the desired latency threshold from
DC i. Equation (4) ensures that, within i’s quorum set, there
are sufﬁciently many replicas that meet the above feasibility
constraints for the selected DCs. Equation (5) ensures the
selected DCs account for the desired percentage of requests.
latency threshold for which a
feasible placement exists, we treat T r
as variables
of optimization, and minimize the maximum of the two
variables. We allow weights ar
on read and write delay
thresholds to enable an application designer to prioritize reads
over writes (or vice-versa). In summary, we have the Latency
Only(LAT) model:
To determine the lowest
and T w
and aw
(LAT)
min
subject to T ≥ alT l
T
,
Qr + Qw =
l ∈ {r, w}
(cid:3)
j xj + 1
Quorum constraints (2), (3), (4)
Percentile constraints (5)
Ql ∈ Z,
ql
ij , xj , Y l
l ∈ {r, w}
i ∈ {0, 1},
∀i, j; l ∈ {r, w}
Note that the constraint on quorum sizes captures the strict
quorum requirement (Section II) that each read sees the action
of the last write. Also, when pr = pl = 1, (LAT) minimizes
the delay of all requests and we refer to this special case
as (LATM). Finally, while (4) is not linear, it may be easily
linearized as we show in [47]. Hence, our model can be solved
using ILP solvers like CPLEX [6].
243243243
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:03:40 UTC from IEEE Xplore.  Restrictions apply. 
Fig. 4. An optimal multi replica solution with Qr
=
2 ensures a latency threshold of l, while an optimal single replica
solution increases it to
B. How much can replication lower latency?
= 2, Qw
√
3l
Given the consistency requirement of quorum datastores,
can replication lower latency, and, if so, by how much? In
this section, we present examples to show that replication can
lower latency, and provide bounds on the replication beneﬁt
(ratio of optimal latency without and with replication). In
assessing the beneﬁts of replication, two key factors are (i)
symmetric/asymmetric spread: whether read and write requests
originate from an identical or different set of DCs; and (ii)
symmetric/asymmetric weights: whether the weights attached
to read and write latency thresholds (ar, aw) are identical or
different.
√
3 ≈ 1.732. When
Figure 4 shows an example where spread and weights are
symmetric and the replication beneﬁt is
replicas can be placed arbitrarily on a Euclidean plane, it can
be shown via an application of Helly’s theorem [17] that the
replication beneﬁt is bounded by
Figure 4 shows that this is a tight bound since replication
achieves this beneﬁt over single placement at the centroid
of the triangle. Replication beneﬁt can be even higher with
asymmetric weights as seen in the observation below.
≈ 1.155. The setup of
2√
3
Observation 1: With asymmetric spreads and metric de-
lays, the replication beneﬁt for (LATM) and (LAT) is at most
4 max(ar ,aw)
min(ar ,aw) .
The proof can be found in our technical report [47].
VI. ACHIEVING LATENCY SLAS DESPITE FAILURES
So far, we have focused on replication strategies that can
optimize latency under normal conditions. In this section we
discuss failures that may impact entire DCs, and present
strategies resilient to such failures.
A. Failure resilient replication strategies
While several techniques exist to protect against individual
failures in a DC [27], geo-distributed DCs are primarily
motivated by failures that impact entire DCs. While failures
within a DC have been studied [27], [32],
there are few
studies on failures across DCs to the best of our knowledge.
Discussions with practitioners suggests that while DC level
failures are not uncommon (Figure 1), correlated failures of
multiple geographically distributed DCs are relatively rare
(though feasible). Operators strive to minimize simultaneous
downtime of multiple DCs through careful scheduling of
maintenance periods and gradual roll-out of software upgrades.
While a sufﬁciently replicated geo-distributed cloud data-
store may be available despite a DC failure, the latency are
likely negatively impacted. We present replication strategies
that are resilient to such failures. Pragmatically, we ﬁrst focus
on the common case scenario of single DC failures. Then, in
Section VI-B, we show how our models easily extend to more
complex failure modes. Our models are:
Basic Availability Model (BA): This model simply optimizes
latency using (LAT) with the additional constraints that the
read and write quorum sizes are at least 2 (and hence the
number of replicas is at least 3). Clearly, read and write
requests can still achieve quorum when one DC is down
and basic availability is maintained. This model does not
explicitly consider latency under failure and our evaluations
in Section VIII indicate that the scheme may perform poorly
under failures – for e.g., the 90th
percentile request latency
for English Wikipedia documents increased from 200msec to
280msec when one replica was unavailable.
N-1 Contingency Model (N-1C): This model minimizes the
maximum latency across a pre-speciﬁed percentile of reads
and writes allowing at most one DC to be unavailable at
any given time. The model is motivated by contingency anal-
ysis techniques commonly employed in power transmission
systems [36] to assess the ability of a grid to withstand a
single component failure. Although this model is similar in
structure to (LAT), there are two important distinctions. First,
the quorum requirements must be met not just under normal
conditions, but under all possible single DC failures. Second,
the desired fraction of requests serviced within a latency
threshold, could be met by considering requests from different
DCs under different failure scenarios.
Formally, let pr
f (resp. pw
f ) be the fraction of reads (resp.
writes) that must meet the delay thresholds when a replica in
any DC is unavailable. Note that the SLA requirement on fail-
ures may be more relaxed, possibly requiring a smaller fraction
of requests to meet a delay threshold. Let Y r
ik ) be
indicator variables that are 1 if read (resp. write) requests from
DC i are served within the latency threshold when the replica
in DC k is unavailable. Then, we replace (5) and (4) with the
following:
ik (resp. Y w
(cid:2)
(cid:2)
Ql
i ∀i∀k
ik ≥ pl
iY l
N l
i
ik ∀i, k l ∈ {r, w}
ij ≥ QlY l
ql
f
i
(cid:2)
j,j(cid:3)=k
(6)
(7)
The ﬁrst constraint ensures that sufﬁcient requests are serviced
within the latency threshold no matter which DC fails. The
index k for the Y variables allows the set of requests satisﬁed
within the latency threshold to depend on the DC that fails.
The second constraint ensures that the quorum requirements
are met when DC k fails with the caveat that DC k cannot be
used to meet quorum requirements. We remark that (7) may
be linearized in a manner similar to (4). Putting everything
244244244
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:03:40 UTC from IEEE Xplore.  Restrictions apply. 
together, we have:
(N-1C) min
Tf
subject to Tf ≥ alT l
,
Qr + Qw =
l ∈ {r, w}
(cid:3)
j xj + 1
Quorum constraints (2), (3), (7)
Percentile constraints (6)