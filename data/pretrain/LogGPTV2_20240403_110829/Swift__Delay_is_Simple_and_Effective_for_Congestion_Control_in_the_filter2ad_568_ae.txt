# Swift-v0 Performance Targets
- **150µs Target**
- **200µs Target**

## Throughput and Round-Trip Time (RTT) Analysis

| Throughput (Gbps) | Average RTT (µs) | 99th-p RTT (µs) |
|-------------------|------------------|-----------------|
| 48.7              | 129.2            | 175.1           |
| 41.6              | 118.3            | 154.4           |
| 44.9              | 157.6            | 203.8           |
| 49.5              | 184.9            | 252.7           |

### Table 4: T1 - Throughput, Average, and Tail RTT for Swift and Swift-v0 with Different Target Delays
This table presents the throughput, average RTT, and 99th percentile RTT for Swift and Swift-v0 using different target delays without decomposing fabric and endpoint congestion.

## Fairness Across Flows

### Flows with the Same Path Lengths
To demonstrate how Swift converges to a fair share as flows arrive and depart, we start with a single flow between a pair of machines. We then incrementally add one more flow from a different source machine while keeping the destination machine the same. Finally, we tear down the flows one by one. The results show that flow allocations are tight and fair.

#### Scalability and Fairness at Scale
Fairness across thousands of flows is crucial in large-scale environments. For this, we use 50 machines in T1, each with 100 flows sending to a single destination machine, resulting in 5000 flows competing for the same bottleneck link. The following metrics are plotted over time:
- **Throughput**: Randomly sampled 50 flows for clarity.
- **CDF of Flow Rates**: Measured across all 5000 flows.
- **Jain’s Fairness Index [26]**: Measured across all 5000 flows.

Even though the per-flow fair-share rate is only 10Mbps on a 50Gbps link, Swift achieves a Jain’s fairness index of 0.91, indicating good fairness. The impact of flow-based scaling is evident, as it tightens the throughput range for demanding workloads.

### Flows with Different Path Lengths (RTT Fairness)
Swift scales the target delay for a flow based on network path length, reducing latency for shorter paths and providing fairness regardless of the base RTT for a flow. To illustrate this, we use two flows destined to the same machine, one sent from the same rack and the other from a remote rack. The results, shown in Figure 22, demonstrate a marked improvement in achieving fair throughput levels.

## Related Work

Swift is inspired by a significant body of work in datacenter congestion control (CC), summarized in Table 5. Below is a brief overview of the key categories and their characteristics:

### Congestion Control Categories
1. **ECN-based Schemes**:
   - **DCTCP [1]**: Uses ECN for rate control.
   - **Hull [2]**: Uses phantom queues.
   - **D2TCP [52]**: A deadline-aware protocol that also uses ECN.
   - **Challenges**: Cannot match sending rate to bottleneck bandwidth, tuning ECN thresholds in heterogeneous networks is prone to bugs.

2. **Explicit Feedback Schemes**:
   - **XCP [27]**, **RCP [17]**, **D3 [54]**: Rely on switches to feedback rates to end-hosts.
   - **DCQCN [59]**: Combines elements of DCTCP and QCN [48] to control congestion.
   - **HPCC [34]**: Relies on in-network telemetry (INT) for precise load information and rate control.
   - **Deployability**: Requires coordinated switch, NIC, and end-host support.

3. **Credit-based Schemes**:
   - **pHost [21]**, **NDP [23]**, **Homa [39]**, **ExpressPass [15]**: Rely on the receiver end-host issuing credit packets.
   - **Improvements**: Reduce flow completion time (FCT).
   - **Assumptions and Limitations**: Assume congestion is at ToR downlinks, require switch modifications.

4. **Packet Scheduling Schemes**:
   - **pFabric [3]**: Achieves near-optimal FCT through QoS queues.
   - **QJUMP [22]**: Requires manual priority specification.
   - **Karuna [14]**: Requires global calculation.
   - **PDQ [24]**: Requires switches to maintain per-flow state.
   - **FastPass [42]**: Places scheduling logic in a central scheduler.

5. **Delay-based Schemes**:
   - **TIMELY [38]**, **DX [33]**: Use one-way queuing delay as a signal for congestion control.
   - **Swift Advancements**: Decouples fabric and host congestion, uses simple target end-to-end delay, scales the target based on load and topology, handles extreme incast, and measures RTT precisely even with ACK coalescing.

## Conclusion and Future Directions

Congestion control has increasingly adopted complex constructs to generalize to a range of applications and workloads. These efforts often require coordinated changes across switches, hosts, centralized entities, and applications, limiting adoption and generality. In this paper, we report on our multi-year experience with congestion control in production at Google. After some false starts, we realized that simplicity remains a virtue when choosing congestion signals. We settled on delay as the simplest actionable feedback. Very low base latency in the datacenter provides the opportunity to quickly react to both network and end-host dynamics. However, this requires high-fidelity measurement of delay and decomposition into meaningful components, which have historically been challenging.

By leveraging NIC hardware timestamps and rapid reaction to congestion in the protocol stack, we show that delay can be both simple to use and extremely effective. Swift achieves ~30µs tail latency while maintaining near 100% utilization. While we feel we are close, there are multiple opportunities to improve Swift. We believe it is competitive with the best centralized or in-network credit-based/explicit feedback schemes, but this remains to be shown. Additionally, Swift’s algorithm is transport-agnostic, including existing TCP stacks and Cloud virtualization stacks. Delay is useful for controlling higher-level operations such as RPC rate, with the potential for fine-grained load balancing and timeouts. Finally, while we have substantially improved predictable latency, supporting <10µs latency for short transfers will require new techniques as target transfer times approach the actual propagation time in datacenters.

## Acknowledgments

We would like to thank Neal Cardwell, Steven Gribble, Jeff Mogul, the anonymous SIGCOMM reviewers, and our shepherd, Yibo Zhu, for providing valuable feedback. Swift is a multi-year effort at Google that benefited from an ecosystem of support and innovation, from RoCE to Pony Express. We thank the Pony Express and Storage production and support teams at Google, including Inho Cho, Yi Cui, Qiaobin Fu, Bill Veraka, Larry Greenfield, Sean Bauer, Michael Marty, Marc de Kruijf, Nicholas Kidd, Milo Martin, and Joel Scherplez. Manya Ghobadi, Emily Blem, Vinh The Lam, Philip Wells, and Ashish Naik contributed to the work in the early days of Swift.

## References

[1] Mohammad Alizadeh, Albert Greenberg, David A. Maltz, Jitendra Padhye, Parveen Patel, Balaji Prabhakar, Sudipta Sengupta, and Murari Sridharan. 2010. Data Center TCP (DCTCP). In Proceedings of the ACM SIGCOMM 2010 Conference (SIGCOMM '10). Association for Computing Machinery, New York, NY, USA, 63–74. https://doi.org/10.1145/1851182.1851192

[2] Mohammad Alizadeh, Abdul Kabbani, Tom Edsall, Balaji Prabhakar, Amin Vahdat, and Masato Yasuda. 2012. Less is More: Trading a Little Bandwidth for Ultra-low Latency in the Data Center. In Proceedings of the 9th USENIX Conference on Networked Systems Design and Implementation (NSDI'12). USENIX Association, Berkeley, CA, USA, 19–19. http://dl.acm.org/citation.cfm?id=2228298.2228324

[3] Mohammad Alizadeh, Shuang Yang, Milad Sharif, Sachin Katti, Nick McKeown, Balaji Prabhakar, and Scott Shenker. 2013. pFabric: Minimal Near-optimal Datacenter Transport. In Proceedings of the ACM SIGCOMM 2013 Conference (SIGCOMM '13). ACM, New York, NY, USA, 435–446. https://doi.org/10.1145/2486001.2486031

[4] M. Allman, K. Avrachenkov, U. Ayesta, J. Blanton, and P. Hurtig. 2010. Early Retransmit for TCP and Stream Control Transmission Protocol (SCTP). RFC 5827. RFC Editor. http://www.rfc-editor.org/rfc/rfc5827.txt

[5] Guido Appenzeller, Isaac Keslassy, and Nick McKeown. 2004. Sizing Router Buffers. In Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications (SIGCOMM '04). Association for Computing Machinery, New York, NY, USA, 281–292. https://doi.org/10.1145/1015467.1015499

[6] Mina Tahmasbi Arashloo, Alexey Lavrov, Manya Ghobadi, Jennifer Rexford, David Walker, and David Wentzlaff. 2020. Enabling Programmable Transport Protocols in High-Speed NICs. In 17th USENIX Symposium on Networked Systems Design and Implementation (NSDI 20). USENIX Association, Santa Clara, CA, 93–109. https://www.usenix.org/conference/nsdi20/presentation/arashloo

[7] Krste Asanović. 2014. FireBox: A Hardware Building Block for 2020 Warehouse-Scale Computers. In 12th USENIX Conference on File and Storage Technologies. USENIX Association, Santa Clara, CA.

[8] Wei Bai, Kai Chen, Li Chen, Changhoon Kim, and Haitao Wu. 2016. Enabling ECN over Generic Packet Scheduling. In Proceedings of the 12th International Conference on Emerging Networking EXperiments and Technologies (CoNEXT '16). Association for Computing Machinery, New York, NY, USA, 191–204. https://doi.org/10.1145/2999572.2999575

[9] Luiz Barroso, Mike Marty, David Patterson, and Parthasarathy Ranganathan. 2017. Attack of the Killer Microseconds. Commun. ACM 60, 4 (March 2017), 48–54. https://doi.org/10.1145/3015146

[10] E. Blanton and M. Allman. 2004. Using TCP Duplicate Selective Acknowledgement (DSACKs) and Stream Control Transmission Protocol (SCTP) Duplicate Transmission Sequence Numbers (TSNs) to Detect Spurious Retransmissions. RFC 3708. RFC Editor.

[11] Google Cloud Blog. 2018. How Distributed Shuffle Improves Scalability and Performance in Cloud Dataflow Pipelines. https://cloud.google.com/blog/products/data-analytics/how-distributed-shuffle-improves-scalability-and-performance-cloud-dataflow-pipelines

[12] Lawrence S. Brakmo, Sean W. O'Malley, and Larry L. Peterson. 1994. TCP Vegas: New Techniques for Congestion Detection and Avoidance. SIGCOMM Comput. Commun. Rev. 24, 4 (Oct. 1994), 24–35. https://doi.org/10.1145/190809.190317

[13] Chelsio Communications. 2020. Chelsio TCP Offload Engine. https://www.chelsio.com/nic/tcp-offload-engine/. Accessed: 2020-02-02.

[14] Li Chen, Kai Chen, Wei Bai, and Mohammad Alizadeh. 2016. Scheduling Mix-flows in Commodity Datacenters with Karuna. In Proceedings of the ACM SIGCOMM 2016 Conference (SIGCOMM '16). ACM, New York, NY, USA, 174–187. https://doi.org/10.1145/2934872.2934888

[15] Inho Cho, Keon Jang, and Dongsu Han. 2017. Credit-Scheduled Delay-Bounded Congestion Control for Datacenters. In Proceedings of the ACM SIGCOMM 2017 Conference (SIGCOMM '17). ACM, New York, NY, USA, 239–252.

[16] Jeffrey Dean and Luiz André Barroso. 2013. The Tail at Scale. Commun. ACM 56, 2 (Feb. 2013), 74–80. https://doi.org/10.1145/2408776.2408794

[17] Nandita Dukkipati and Nick McKeown. 2006. Why Flow-Completion Time is the Right Metric for Congestion Control. SIGCOMM Comput. Commun. Rev. 36, 1 (Jan. 2006), 59–62. https://doi.org/10.1145/1111322.1111336

[18] Paolo Faraboschi, Kimberly Keeton, Tim Marsland, and Dejan Milojicic. 2015. Beyond Processor-centric Operating Systems. In 15th Workshop on Hot Topics in Operating Systems (HotOS XV). USENIX Association, Kartause Ittingen, Switzerland, 1–7. https://www.usenix.org/conference/hotos15/workshop-program/presentation/faraboschi

[19] S. Floyd, J. Mahdavi, M. Mathis, and M. Podolsky. 2000. An Extension to the Selective Acknowledgement (SACK) Option for TCP. RFC 2883. RFC Editor.

[20] Peter X. Gao, Akshay Narayan, Sagar Karandikar, Joao Carreira, Sangjin Han, Rachit Agarwal, Sylvia Ratnasamy, and Scott Shenker. 2016. Network Requirements for Resource Disaggregation. In 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI '16). USENIX Association, Savannah, GA, USA, 445–460.