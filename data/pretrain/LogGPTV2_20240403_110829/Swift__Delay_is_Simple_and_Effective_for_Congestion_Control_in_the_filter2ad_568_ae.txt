Swift-v0, 150µs target
Swift-v0, 200µs target
Throughput Average RTT
48.7Gbps
41.6Gbps
44.9Gbps
49.5Gbps
129.2µs
118.3µs
157.6µs
184.9µs
99th-p RTT
175.1µs
154.4µs
203.8µs
252.7µs
Table 4: T1: Throughput, average and tail RTT for Swift and Swift-v0 that uses
different target delays without decomposing fabric and endpoint congestion.
Flows with same path lengths: First, we show how Swift con-
verges to fair-share as flows arrive and depart. In Figure 20, we
start with a single flow between a pair of machines. Keeping the
destination machine the same, we incrementally add one more flow
from a different source machine and then start tearing down the
flows one by one. We see the flow allocations are tight and fair.
While fairness across a few flows is essential, fairness across
thousands of flows is imperative at scale. For this, we use 50 ma-
chines in T1, each with 100 flows sending to a single destination
machine for a total of 5000 flows competing for the same bottleneck
link. We plot the throughput over time, CDF of flow rates, and Jain’s
fairness index [26] in Figure 21. We randomly sample 50 flows to
keep the throughput plot legible, but the CDF and Jain’s fairness
index is measured across all 5000 flows. Even though the per-flow
fair-share rate is only 10Mbps on a 50Gbps link, Swift achieves
good fairness with a Jain’s fairness index of 0.91. The impact of
flow-based scaling is evident, as it tightens the throughput range
for an extremely demanding workload.
Flows with different path lengths (RTT Fairness): Swift scales
the target delay for a flow based on network path length. This
not only reduces latency for shorter paths, but provides fairness
irrespective of the base RTT for a flow. To show how well this
mechanism works, we use two flows destined to the same machine,
one sent from the same rack and the other sent from a remote
rack. We plot the throughput of each flow without and then with
topology-based scaling. The results in Figure 22 show a marked
improvement to fair throughput levels.
6 RELATED WORK
Swift is inspired by a large body of work for datacenter congestion
control (CC) summarized in Table 5. We summarize the work below.
ECN-based: DCTCP [1], a trailblazer for datacenter CC and the
main comparison point for Swift, uses ECN for rate control. Hull [2]
uses phantom queues, and D2TCP [52] is a deadline-aware protocol
that also uses ECN. When number of flows exceed network BDP,
524
01234567Time(s)0102030405060Throughput(Gbps)Flow1Flow2Flow3Flow401020Time(s)010203040Throughput(Mbps)Swi(cid:28)w/oFBS01020Time(s)Swi(cid:28)withFBS051015202530Throughput(Mbps)0.00.20.40.60.81.0CDFJ=0.91J=0.78withFBSw/oFBSWithoutTBSWithTBS01020304050Throughput(Gbps)28.8924.8820.6324.66Flow1Flow2Congestion Control Category
Simplicity/Deployability
ECN based: DCTCP, D2TCP,
HULL
Explicit Feedback: XCP, RCP,
DCQCN, HPCC, D3
Receiver/Credit Based: Homa,
NDP, pHost, ExpressPass
Packet Scheduling: pFabric,
QJump, PDQ, Karuna, FastPass
Swift
Complex ECN
Tuning/Deployment
Complex Scheme/Deployment
Not Universally Deployable
NIC/Endhost
Support
Not Required
Required for HPCC,
DCQCN
Not Required
Not Deployable As Is
Not Required
Simple, Wide Deployment at
Scale
NIC TimeStamps
Support in Switches
ECN,
HULL Phantom-Q
Required for XCP,
RCP, D3, HPCC
Needed for
NDP, ExpressPass
Needed for PDQ,
pFabric
None
Robust to Traffic
Patterns
Incast Issues
Incast Issues
(not HPCC)
Work Well
Incast Issues,
Specificity
Works Well
Kumar et al.
Congestion
Handled
Fabric Only
Fabric Only
ToR Downlink
not ExpressPass,
NDP
Fabric Only
Fabric and Endhost
Table 5: The focal point of Swift is simplicity and ease of deployment while providing excellent end-to-end performance.
ECN-based schemes cannot match sending rate to bottleneck band-
width. In addition to not handling congestion at hosts, tuning ECN
thresholds in heterogeneous networks is prone to bugs.
Explicit Network Feedback including INT: XCP [27], RCP [17],
and D3 [54] rely on switches to feedback rates to end-hosts. DC-
QCN [59] relies on ECN and combines elements of DCTCP and
QCN [48] to control congestion. HPCC [34] relies on in-network
telemetry (INT) to obtain precise load information and for rate
control. Deployability of these schemes is a challenge, especially
in heterogeneous datacenters, as they require coordinated switch,
NIC, and end-host support. Swift uses an intuitive delay framework
that does not need switch support, though we note that it can easily
incorporate INT as it becomes more widely available. In particular,
INT can measure per-hop sojourn times to provide a more accurate
breakdown of delay.
Credit-based: pHost [21], NDP [23], Homa [39] and Express-
Pass [15], rely on the receiver end-host issuing credit packets. While
they show tremendous improvement in reducing flow completion
time (FCT), Homa and pHost assume that congestion is at ToR
downlinks. In practice, congestion can happen in the fabric which
can be over-subscribed [46]. Additionally, ExpressPass and NDP
require switch modifications. Swift handles end-to-end congestion
without requiring new support. Schemes like Homa can be layered
atop Swift to issue grants based on cwnd.
Congestion Control via Packet Scheduling: pFabric [3] achieves
near-optimal FCT through the usage of QoS queues. However, this
framework does not support multi-tenant environments in which a
large RPC for one tenant may be of higher priority than a small RPC
for another tenant. QJUMP [22] requires priorities to be specified
manually on a per-application basis. Karuna [14] requires a global
calculation. PDQ [24] requires switches to maintain per-flow state.
FastPass [42] places scheduling logic in a central scheduler.
Delay-based Schemes: Swift builds upon TIMELY [38] and DX[33],
which championed the use of one-way queuing delay as a signal
for congestion control. Swift advances over TIMELY [38] include:
decoupling fabric and host congestion; using a simple target end-
to-end delay instead of RTT gradients; scaling the target based on
load and topology; handling extreme incast; and measuring RTT
precisely even in the presence of ACK coalescing. In retrospect, we
appreciate how aspects of Swift’s design have addressed challenges
in using delay as a signal that were called out by Zhu et al. [60] (as
detailed in Appendix E).
7 CONCLUSION AND FUTURE DIRECTIONS
Congestion control has increasingly adopted complex constructs to
generalize to a range of applications and workloads. These efforts
often require coordinated changes across switches, hosts, central-
ized entities and applications, limiting adoption and, paradoxically,
generality. In this paper, we report on our multi-year experience
with congestion control in production at Google. After some false
starts, we realized that simplicity remains a virtue when choosing
congestion signals. We settled on delay as the simplest actionable
feedback. Very low base latency in the datacenter provides the op-
portunity to quickly react to both network and end host dynamics.
However, doing so requires high fidelity measurement of delay and
decomposition of such measures into meaningful constituent com-
ponents. Both requirements have historically been hard to achieve
though it is worth noting that most earlier attempts focused on
wide-area deployments. By leveraging NIC hardware timestamps
and rapid reaction to congestion in the protocol stack, we show
that delay can be both simple to use and extremely effective. Swift
achieves ∼30us tail latency while maintaining near 100% utilization.
While we feel we are tantalizingly close, we see multiple opportu-
nities to improve on Swift. We believe it is competitive with the best
centralized or in-network credit-based/explicit feedback schemes,
but this remains to be shown. We also view Swift’s algorithm as
transport-agnostic, including existing TCP stacks and Cloud vir-
tualization stacks. We believe that delay is useful for controlling
higher-level operations such as RPC rate, with the opportunity to
perform fine-grained load balancing and timeouts. Finally, while
we have substantially improved on predictable latency relative to
the state of the art, supporting <10us latency for short transfers
will require new techniques as target transfer times approach the
actual propagation time in datacenters.
ACKNOWLEDGMENTS
We would like to thank Neal Cardwell, Steven Gribble, Jeff Mogul,
the anonymous SIGCOMM reviewers and our shepherd, Yibo Zhu,
for providing valuable feedback. Swift is a multi-year effort at
Google that benefited from an ecosystem of support and innovation,
from RoCE to Pony Express. We thank the Pony Express and Stor-
age production, and support teams at Google for their contributions
to the work, including but not limited to, Inho Cho, Yi Cui, Qiaobin
Fu, Bill Veraka, Larry Greenfield, Sean Bauer, Michael Marty, Marc
de Kruijf, Nicholas Kidd, Milo Martin and Joel Scherplez. Manya
Ghobadi, Emily Blem, Vinh The Lam, Philip Wells and Ashish Naik
contributed to the work in the early days of Swift.
525
Swift: Delay is Simple and Effective for Congestion Control in the Datacenter
REFERENCES
[1] Mohammad Alizadeh, Albert Greenberg, David A. Maltz, Jitendra Padhye,
Parveen Patel, Balaji Prabhakar, Sudipta Sengupta, and Murari Sridharan. 2010.
Data Center TCP (DCTCP). In Proceedings of the ACM SIGCOMM 2010 Conference
(SIGCOMM âĂŹ10). Association for Computing Machinery, New York, NY, USA,
63âĂŞ74. https://doi.org/10.1145/1851182.1851192
[2] Mohammad Alizadeh, Abdul Kabbani, Tom Edsall, Balaji Prabhakar, Amin Vahdat,
and Masato Yasuda. 2012. Less is More: Trading a Little Bandwidth for Ultra-
low Latency in the Data Center. In Proceedings of the 9th USENIX Conference on
Networked Systems Design and Implementation (NSDI’12). USENIX Association,
Berkeley, CA, USA, 19–19. http://dl.acm.org/citation.cfm?id=2228298.2228324
[3] Mohammad Alizadeh, Shuang Yang, Milad Sharif, Sachin Katti, Nick McKe-
own, Balaji Prabhakar, and Scott Shenker. 2013. pFabric: Minimal Near-optimal
Datacenter Transport. In Proceedings of the ACM SIGCOMM 2013 Conference
(SIGCOMM ’13). ACM, New York, NY, USA, 435–446. https://doi.org/10.1145/
2486001.2486031
[4] M. Allman, K. Avrachenkov, U. Ayesta, J. Blanton, and P. Hurtig. 2010. Early Re-
transmit for TCP and Stream Control Transmission Protocol (SCTP). RFC 5827. RFC
Editor. http://www.rfc-editor.org/rfc/rfc5827.txt http://www.rfc-editor.org/
rfc/rfc5827.txt.
[5] Guido Appenzeller, Isaac Keslassy, and Nick McKeown. 2004. Sizing Router
Buffers. In Proceedings of the 2004 Conference on Applications, Technologies, Ar-
chitectures, and Protocols for Computer Communications (SIGCOMM âĂŹ04). As-
sociation for Computing Machinery, New York, NY, USA, 281âĂŞ292. https:
//doi.org/10.1145/1015467.1015499
[6] Mina Tahmasbi Arashloo, Alexey Lavrov, Manya Ghobadi, Jennifer Rexford,
David Walker, and David Wentzlaff. 2020. Enabling Programmable Transport
Protocols in High-Speed NICs. In 17th USENIX Symposium on Networked Systems
Design and Implementation (NSDI 20). USENIX Association, Santa Clara, CA,
93–109. https://www.usenix.org/conference/nsdi20/presentation/arashloo
[7] Krste Asanović. 2014. FireBox: A Hardware Building Block for 2020 Warehouse-
Scale Computers. In 12th USENIX Conference on File and Storage Technologies.
USENIX Association, Santa Clara, CA.
[8] Wei Bai, Kai Chen, Li Chen, Changhoon Kim, and Haitao Wu. 2016. Enabling
ECN over Generic Packet Scheduling. In Proceedings of the 12th International
on Conference on Emerging Networking EXperiments and Technologies (CoNEXT
âĂŹ16). Association for Computing Machinery, New York, NY, USA, 191âĂŞ204.
https://doi.org/10.1145/2999572.2999575
[9] Luiz Barroso, Mike Marty, David Patterson, and Parthasarathy Ranganathan.
2017. Attack of the Killer Microseconds. Commun. ACM 60, 4 (March 2017),
48âĂŞ54. https://doi.org/10.1145/3015146
[10] E. Blanton and M. Allman. 2004. Using TCP Duplicate Selective Acknowledgement
(DSACKs) and Stream Control Transmission Protocol (SCTP) Duplicate Transmission
Sequence Numbers (TSNs) to Detect Spurious Retransmissions. RFC 3708. RFC
Editor.
[11] Google Cloud Blog. 2018.
ability and performance in Cloud Dataflow pipelines.
//cloud.google.com/blog/products/data-analytics/how-distributed-shuffle-
improves-scalability-and-performance-cloud-dataflow-pipelines
How Distributed Shuffle improves scal-
https:
[12] Lawrence S. Brakmo, Sean W. OâĂŹMalley, and Larry L. Peterson. 1994. TCP
Vegas: New Techniques for Congestion Detection and Avoidance. SIGCOMM
Comput. Commun. Rev. 24, 4 (Oct. 1994), 24âĂŞ35. https://doi.org/10.1145/
190809.190317
(2018).
[13] Chelsio Communications. 2020.
Chelsio TCP Offload Engine.
www.chelsio.com/nic/tcp-offload-engine/. (2020). Accessed: 2020-02-02.
[14] Li Chen, Kai Chen, Wei Bai, and Mohammad Alizadeh. 2016. Scheduling Mix-flows
in Commodity Datacenters with Karuna. In Proceedings of the ACM SIGCOMM
2016 Conference (SIGCOMM ’16). ACM, New York, NY, USA, 174–187. https:
//doi.org/10.1145/2934872.2934888
[15] Inho Cho, Keon Jang, and Dongsu Han. 2017. Credit-Scheduled Delay-Bounded
Congestion Control for Datacenters. In Proceedings of the ACM SIGCOMM 2017
Conference (SIGCOMM ’17). ACM, New York, NY, USA, 239–252.
[16] Jeffrey Dean and Luiz André Barroso. 2013. The Tail at Scale. Commun. ACM 56,
2 (Feb. 2013), 74âĂŞ80. https://doi.org/10.1145/2408776.2408794
[17] Nandita Dukkipati and Nick McKeown. 2006. Why Flow-Completion Time is
the Right Metric for Congestion Control. SIGCOMM Comput. Commun. Rev. 36, 1
(Jan. 2006), 59âĂŞ62. https://doi.org/10.1145/1111322.1111336
[18] Paolo Faraboschi, Kimberly Keeton, Tim Marsland, and Dejan Milojicic. 2015.
Beyond Processor-centric Operating Systems. In 15th Workshop on Hot Topics in
Operating Systems (HotOS XV). USENIX Association, Kartause Ittingen, Switzer-
land, 1–7. https://www.usenix.org/conference/hotos15/workshop-program/
presentation/faraboschi
[19] S. Floyd, J. Mahdavi, M. Mathis, and M. Podolsky. 2000. An Extension to the
Selective Acknowledgement (SACK) Option for TCP. RFC 2883. RFC Editor.
[20] Peter X. Gao, Akshay Narayan, Sagar Karandikar, Joao Carreira, Sangjin Han,
Rachit Agarwal, Sylvia Ratnasamy, and Scott Shenker. 2016. Network Require-
ments for Resource Disaggregation. In 12th USENIX Symposium on Operating