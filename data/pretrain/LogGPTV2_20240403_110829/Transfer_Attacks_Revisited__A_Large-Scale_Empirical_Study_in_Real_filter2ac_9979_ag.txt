that SIK is a good choice for measuring adversarial conﬁdence
in transfer attack while SSK is not representative enough.
In practice, we want to know whether increasing the SSK
for the CW attack is sufﬁcient to increase the transferability
as well. To answer this question, we perform CW attack with
different κ thresholds. The result is shown in Figure 9. We can
see from Figure 9b and Figure 9c that a larger SSK does not
always increase transferability. For example, for the blue line
Figure 10: Comparison of the transferability between AEs
generated from natural AEs and normal seed images. For
each plot,
the left column is the misclassiﬁcation rate of
AEs generated from natural AEs and the right column is
the misclassiﬁcation rate of AEs generated from other seed
images. It shows that the left column is always greater or
equal to the right column.
in Figure 9, while the number of AEs drops at roughly 25%
when we increase SSK from 60 to 200, the transfer rate against
the AWS platform does not improve. Therefore, increasing
SSK is not useful enough for a real transfer attack.
Observation 9. The logit difference between the adversarial
class and the second most likely class, called κ value, is
neither a good measurement for the transferability nor a good
tool to increase the transferability.
3) Connection Between Transferability and Intrinsic Clas-
siﬁcation Hardness: An image is deﬁned to be a natural AE
w.r.t. a surrogate model if it can fool the surrogate without any
disguise. When conducting transfer attack in the real world,
natural AEs are unavoidable since the surrogate cannot achieve
100% accuracy. We are thus motivated to ﬁnd out if perturbing
natural AEs can provide better transferability.
We compare the transferability of AEs generated from
natural AEs with AEs generated from other seed images in
Figure 10. The natural AEs disguised by adversarial pertur-
bation achieve higher success rate than AEs generated from
other seed images consistently. It suggests that the intrinsic
classiﬁcation hardness is important in deciding transferability
and real world attackers should prefer natural AEs in transfer
attacks. The result is intuitive because natural-AE seeds are
believed to be closer to the theoretical classiﬁcation border
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:58 UTC from IEEE Xplore.  Restrictions apply. 
1434
[0, 10][10, 20][20, 30][30, 40][40, 50]SSK0.000.050.100.15[0.0, 0.2][0.2, 0.4][0.4, 0.6][0.6, 0.8][0.8, 1.0]SIK0.000.050.100.15aliyunawsbaidu6080100120140160180200Kappa0.000.250.500.75Misclassification Rate6080100120140160180200Kappa050100150200Number of Generated AEsresnet_raw_18_pretrainedresnet_raw_18_unpretrainedresnet_raw_34_pretrainedresnet_raw_34_unpretrained6080100120140160180200Kappa0.000.250.500.75Misclassification Rate6080100120140160180200Kappa050100150200Number of Generated AEsresnet_raw_18_pretrainedresnet_raw_18_unpretrainedresnet_raw_34_pretrainedresnet_raw_34_unpretrained6080100120140160180200Kappa0.000.250.500.75Misclassification Rate6080100120140160180200Kappa050100150200Number of Generated AEsresnet_raw_18_pretrainedresnet_raw_18_unpretrainedresnet_raw_34_pretrainedresnet_raw_34_unpretrainedFGSMLLCPGDSTEP_LLCUAP0.680.530.210.210.200.100.620.560.210.00aliyun0.00.20.40.60.680.530.430.140.400.300.560.380.290.14baidu0.00.20.40.6FGSMLLCPGDSTEP_LLCUAP0.630.530.570.360.550.400.560.440.430.07google0.00.20.40.60.420.320.140.000.150.100.380.250.070.07aws0.00.20.40.6and an adversarial perturbation is more likely to transfer on
these images.
Observation 10. AEs generated from seed images that are
hard to classify on surrogates have better transferability.
A. Ethic
VI. DISCUSSION
When evaluating attacks on the platforms, we conduct
normal queries with the ofﬁcial API and pay the usage fee
as required. Therefore, our evaluations are legitimate and do
no harm to the operations of the cloud services. The related
companies are informed about our experiment to exclude any
potential harm, e.g., do not use the uploaded images in any
form. We have received conﬁrmation from AWS and Baidu.
B. Limitation and Future Work
Our goal is to provide a systematic view of factors that affect
transferability. Therefore, we leave some conclusions not fully
understood. For example, we do not validate the hypothesis
that the number of the local optimal increases exponentially
when the task complexity increases. These arguments are
beyond the scope of this paper and require further study in
the future. In addition, we do not ﬁnd the best hyperparameter
setting in the real settings. Although these answers are useful,
it requires massive resources which we cannot afford, thus
left for future work. In particular, our conclusions about the
attack algorithms are speciﬁc to the chosen hyperparameters,
not representing their ability under different hyperparameters.
For example, we set κ = 0 for CW attack, but choosing a large
κ may be preferable for attackers. In our experiment, however,
we set κ = 0 for a fair comparison of CW with other attacks
which cannot manipulate kappa.
VII. RELATED WORKS
Early works on adversarial attacks [12], [19], [30], [32],
[44], [46] mainly focus on white-box attacks, where the
adversary has full access to the target model. Considering the
lack of information in black-box attacks, efforts have been
made in two directions: 1) query-based attacks that estimate
the gradient information used to facilitate the generation of
AEs [13], [21] and 2) transfer attacks that generate AEs using
a surrogate model and expect them to transfer to the unknown
target model [36], [37]. Our paper focuses on transfer attacks.
The transferability of AE in deep learning models was
ﬁrst discovered by Szegedy et al. [44]. Papernot et al. [36],
[37] utilized this property to perform black-box attack to real
MLaaS systems, which proves its possibility for real attacks.
Carlini et al. [12] proposed that high-conﬁdence adversarial
examples increase the transferability.
Efforts have been made to understand transferability of AEs.
Liu et al. [29] found that unlike untargeted AEs, targeted AEs
almost never transfer and proposed to use an ensemble of
surrogates to increase transferability of targeted attacks. Wu et
al. [49] studied inﬂuencing factors of transferability and point
out that reduced variance of gradients results in better transfer-
ability. Su et al. [41] conducted a comprehensive analysis of
transfer attack using 18 different surrogates. They found that
relaxing norm constraint of adversarial perturbation generates
better transferability. They concluded that FGSM > PGD >
CW in the sense of transferability as well. Demontis et al. [15]
further highlighted that simpler surrogates and better gradient
alignment provide better transferability. This conclusion was
based on adding different levels of regularization and found
stronger regulation provides better transferability. In addition,
they concluded that targets which have large gradients for
the input are more vulnerable to transfer attack as well. Our
work aims at examining whether their conclusions generalize
to transfer attack in the real applications and extending their
results to get more insights. In particular, when evaluating the
relation between surrogate complexity and transferability, we
believe that a better way to examine the impact of complexity
in the real settings is to directly use surrogates with different
depths but the same architecture family, rather than changing
regularization. Our conclusion about the non-monotonicity of
the effect of surrogate depth on transferability is a complement
to Demontis et al.
VIII. CONCLUSION
In this paper, we identify the difﬁculties of evaluating
transferability of adversarial example (AE) in the real world
and propose customized metrics to address these difﬁculties.
Based on the proposed metrics, we conduct a systematic eval-
uation of real-world transfer attacks on four popular MLaaS
systems, Aliyun, Baidu Cloud, Google Cloud Vision and
AWS Rekognition. The evaluation leads to the following new
conclusions on top of the existing ones made in lab settings:
(1) model similarity concept is ill-suited for transfer attack;
(2) surrogates of suitable complexity can exceed simpler and
deeper counterparts; (3) MLaaS systems have different level
of robustness to transfer attack and could use more effort to
improve; (4) Strong adversarial algorithms do not necessarily
transfer better, and single-step algorithms transfer better than
iterative algorithms; (5) adversarial algorithm and the target
platform are the most important factors for transfer attacks, and
the most beneﬁcial practice is to choose an appropriate adver-
sarial algorithm; (6) no dominant surrogate architecture exists
in the real transfer attack. (7) large L2 norm of adversarial
perturbation can be a more direct source of transferability than
L∞ norm. (8) larger gap between the posterior of logits makes
better transferability. (9) classiﬁcation hardness is preferred
when choosing seed images of transfer attack.
IX. ACKNOWLEDGEMENT
We would like to thank our shepherd and the anonymous re-
viewers for their valuable suggestions. This work is partly sup-
ported by the Zhejiang Provincial Natural Science Foundation
for Distinguished Young Scholars under No. LR19F020003,
NSFC under No. 62102360, and U1836202, and the Funda-
mental Research Funds for the Central Universities (Zhejiang
University NGICS Platform). Ting Wang is partially supported
by the National Science Foundation under Grant No. 1951729,
1953813, and 1953893.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:58 UTC from IEEE Xplore.  Restrictions apply. 
1435
REFERENCES
[1] Alibaba cloud. https://aliyun.com.
[2] AWS rekognition. https://aws.amazon.com/rekognition.
[3] AWS Rekognition documentation.
https://docs.aws.amazon.com/
rekognition/latest/dg/what-is.html.
[4] Baidu cloud. https://bce.baidu.com.
[5] Deﬁnition of additively separable functions. https://calculus.subwiki.org/
wiki/Additively separable function.
[6] Google cloud vision. https://cloud.google.com/vision.
[7] Google Open Images V6+. https://storage.googleapis.com/openimages/
web/index.html.
[8] PyTorch. https://pytorch.org/.
[9] pytorch/vision. https://github.com/pytorch/vision, June 2020. original-
date: 2016-11-09T23:11:43Z.
[10] Statistical mechanics of deep learning. Annual Review of Condensed
Matter Physics 11, 1 (2020), 501–528.
[11] BOX, G. E. P., AND COX, D. R. An analysis of transformations. Journal
of the Royal Statistical Society. Series B (Methodological) 26, 2 (1964),
211–252.
[12] CARLINI, N., AND WAGNER, D. Towards Evaluating the Robustness of
Neural Networks. In 2017 IEEE Symposium on Security and Privacy
(SP) (May 2017), pp. 39–57. ISSN: 2375-1207.
[13] CHEN, P.-Y., ZHANG, H., SHARMA, Y., YI, J., AND HSIEH, C.-J.
ZOO: Zeroth Order Optimization Based Black-Box Attacks to Deep
Neural Networks without Training Substitute Models. In Proceedings
of the 10th ACM Workshop on Artiﬁcial Intelligence and Security (2017),
AISec ’17, Association for Computing Machinery, pp. 15–26.
[14] CHEN, S., HE, Z., SUN, C., AND HUANG, X. Universal adversarial
attack on attention and the resulting dataset damagenet, 2020.
[15] DEMONTIS, A., MELIS, M., PINTOR, M., JAGIELSKI, M., BIGGIO, B.,
OPREA, A., NITA-ROTARU, C., AND ROLI, F. Why Do Adversarial
Attacks Transfer? Explaining Transferability of Evasion and Poisoning
In 28th USENIX Security Symposium (USENIX Security 19)
Attacks.
(2019), pp. 321–338.
[16] DENG, J., DONG, W., SOCHER, R., LI, L., KAI LI, AND LI FEI-
In 2009
FEI.
IEEE Conference on Computer Vision and Pattern Recognition (2009),
pp. 248–255.
Imagenet: A large-scale hierarchical image database.
[17] DEVLIN, J., CHANG, M.-W., LEE, K., AND TOUTANOVA, K. BERT:
Pre-training of deep bidirectional transformers for language understand-
In Proceedings of the 2019 Conference of the North American
ing.
Chapter of
the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short Papers) (June 2019),
Association for Computational Linguistics, pp. 4171–4186.
[18] EIDINGER, E., ENBAR, R., AND HASSNER, T. Age and gender esti-
mation of unﬁltered faces. IEEE Transactions on Information Forensics
and Security 9, 12 (2014), 2170–2179.
[19] GOODFELLOW, I., SHLENS, J., AND SZEGEDY, C. Explaining and
In International Conference on
Harnessing Adversarial Examples.
Learning Representations (2015).
[20] HE, K., ZHANG, X., REN, S., AND SUN, J. Deep Residual Learning
for Image Recognition. In 2016 IEEE Conference on Computer Vision
and Pattern Recognition (CVPR) (June 2016), IEEE, pp. 770–778.
[21] ILYAS, A., ENGSTROM, L., ATHALYE, A., AND LIN, J. Black-box
In ICML
Adversarial Attacks with Limited Queries and Information.
2018 (July 2018). arXiv: 1804.08598.
[22] KRIZHEVSKY, A. Learning multiple layers of features from tiny images.
[23] KURAKIN, A., GOODFELLOW, I., AND BENGIO, S. Adversarial ma-
chine learning at scale, 2016.
[24] KURAKIN, A., GOODFELLOW, I. J., AND BENGIO, S. Adversarial
In 5th International Conference on
examples in the physical world.
Learning Representations, ICLR 2017, Toulon, France, April 24-26,
2017, Workshop Track Proceedings (2017), OpenReview.net.
[25] LECUN, Y., BOTTOU, L., BENGIO, Y., AND HAFFNER, P. Gradient-
based learning applied to document recognition. Proceedings of the
IEEE 86, 11 (1998), 2278–2324.
[26] LI, C., WANG, L., JI, S., ZHANG, X., XI, Z., GUO, S., AND WANG, T.
Seeing is living? rethinking the security of facial liveness veriﬁcation in
the deepfake era. CoRR abs/2202.10673 (2022).
[27] LI, X., JI, S., HAN, M., JI, J., REN, Z., LIU, Y., AND WU, C. Ad-
versarial examples versus cloud-based detectors: A black-box empirical
IEEE Trans. Dependable Secur. Comput. 18, 4 (2021), 1933–
study.
1949.
[28] LING, X., JI, S., ZOU, J., WANG, J., WU, C., LI, B., AND WANG, T.
DEEPSEC: A Uniform Platform for Security Analysis of Deep Learning
Model. In 2019 IEEE Symposium on Security and Privacy (SP) (2019),
pp. 673–690.
[29] LIU, Y., CHEN, X., LIU, C., AND SONG, D. Delving into Transferable
Adversarial Examples and Black-box Attacks. In 5th International Con-
ference on Learning Representations, ICLR 2017, Toulon, France, April
24-26, 2017, Conference Track Proceedings (2017), OpenReview.net.
[30] MADRY, A., MAKELOV, A., SCHMIDT, L., TSIPRAS, D., AND VLADU,
A. Towards Deep Learning Models Resistant to Adversarial Attacks. In
6th International Conference on Learning Representations, ICLR 2018,
Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
Proceedings (2018), OpenReview.net.
[31] MOOSAVI-DEZFOOLI, S., FAWZI, A., FAWZI, O., AND FROSSARD,
P. Universal Adversarial Perturbations. In 2017 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR) (2017), pp. 86–94.
[32] MOOSAVI-DEZFOOLI, S., FAWZI, A., AND FROSSARD, P. DeepFool:
A Simple and Accurate Method to Fool Deep Neural Networks. In 2016
IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
(2016), pp. 2574–2582.
[33] PANG, R., XI, Z., JI, S., LUO, X., AND WANG, T. On the security risks
of automl. CoRR abs/2110.06018 (2021).
[34] PANG, R., ZHANG, X., JI, S., LUO, X., AND WANG, T. Advmind:
In KDD ’20 (2020),
Inferring adversary intent of black-box attacks.
pp. 1899–1907.
[35] PANG, R., ZHANG, Z., GAO, X., XI, Z., JI, S., CHENG, P., AND
WANG, T. TROJANZOO: everything you ever wanted to know about
neural backdoors (but were afraid to ask). CoRR abs/2012.09302 (2020).
[36] PAPERNOT, N., MCDANIEL, P., AND GOODFELLOW, I. Transferabil-
ity in machine learning: from phenomena to black-box attacks using
adversarial samples. arXiv preprint arXiv:1605.07277 (2016).
[37] PAPERNOT, N., MCDANIEL, P., GOODFELLOW, I., JHA, S., CELIK,
Z. B., AND SWAMI, A. Practical Black-Box Attacks against Machine
the 2017 ACM on Asia Conference
Learning.
on Computer and Communications Security (2017), ASIA CCS ’17,
Association for Computing Machinery, pp. 506–519. event-place: Abu
Dhabi, United Arab Emirates.
In Proceedings of
[38] PAPERNOT, N., MCDANIEL, P., JHA, S., FREDRIKSON, M., CELIK,
Z. B., AND SWAMI, A. The limitations of deep learning in adversarial
settings. In 2016 IEEE European symposium on security and privacy
(EuroS&P) (2016), IEEE, pp. 372–387.
[39] SHEN, L., JI, S., ZHANG, X., LI, J., CHEN, J., SHI, J., FANG, C.,
YIN, J., AND WANG, T. Backdoor pre-trained models can transfer to
In CCS ’21: 2021 ACM SIGSAC Conference on Computer and
all.
Communications Security, November 15 - 19, 2021 (2021), Y. Kim,
J. Kim, G. Vigna, and E. Shi, Eds., ACM, pp. 3141–3158.
[40] SIMONYAN, K., AND ZISSERMAN, A. Very Deep Convolutional
Networks for Large-Scale Image Recognition. arXiv:1409.1556 [cs]
(Apr. 2015). arXiv: 1409.1556.
[41] SU, D., ZHANG, H., CHEN, H., YI, J., CHEN, P., AND GAO, Y.
Is robustness the cost of accuracy? - A comprehensive study on the
robustness of 18 deep image classiﬁcation models. In Computer Vision
- ECCV 2018 - 15th European Conference, Munich, Germany, Septem-
ber 8-14, 2018, Proceedings, Part XII (2018), V. Ferrari, M. Hebert,
C. Sminchisescu, and Y. Weiss, Eds., vol. 11216 of Lecture Notes in
Computer Science, Springer, pp. 644–661.
[42] SZEGEDY, C., VANHOUCKE, V., IOFFE, S., SHLENS, J., AND WOJNA,
Z. Rethinking the Inception Architecture for Computer Vision. In 2016
IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
(June 2016), IEEE, pp. 2818–2826.
[43] SZEGEDY, C., WEI LIU, YANGQING JIA, SERMANET, P., REED, S.,
ANGUELOV, D., ERHAN, D., VANHOUCKE, V., AND RABINOVICH, A.
Going deeper with convolutions. In 2015 IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) (2015), pp. 1–9.
[44] SZEGEDY, C., ZAREMBA, W., SUTSKEVER, I., BRUNA, J., ERHAN,
D., GOODFELLOW, I., AND FERGUS, R. Intriguing properties of neural
In International Conference on Learning Representations
networks.
(2014).
[45] TRAM `ER, F., KURAKIN, A., PAPERNOT, N., GOODFELLOW,
I.,
BONEH, D., AND MCDANIEL, P. Ensemble adversarial training: Attacks
and defenses. 6th International Conference on Learning Representations,
ICLR 2018 ; Conference date: 30-04-2018 Through 03-05-2018.
[46] TRAM `ER, F., KURAKIN, A., PAPERNOT, N., GOODFELLOW, I. J.,
BONEH, D., AND MCDANIEL, P. D. Ensemble Adversarial Training:
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:58 UTC from IEEE Xplore.  Restrictions apply. 
1436
In 6th International Conference on Learning
Attacks and Defenses.
Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May
3, 2018, Conference Track Proceedings (2018), OpenReview.net.
[47] VASWANI, A., SHAZEER, N., PARMAR, N., USZKOREIT, J., JONES, L.,
GOMEZ, A. N., KAISER, L., AND POLOSUKHIN, I. Attention is all you
need, 2017.
[48] WILCOXON, F. Individual comparisons by ranking methods. Biometrics
Bulletin 1, 6 (1945), 80–83.
[49] WU, L., ZHU, Z., TAI, C., AND E, W. Understanding and Enhancing
the Transferability of Adversarial Examples. arXiv:1802.09707 [cs, stat]