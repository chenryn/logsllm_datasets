Hi  
I need to pretrain the bert on one dataset and finetune it then on other
datasets, so basically  
removing classifier from first part and substitute it with a new one with the
specific number of  
labels, currently with current codes, it will be error to do it when loading
pretrained model,  
could you please assist me how I can do this? I have a deadline soon and
really appreciate your help urgently.  
thanks  
Best  
Julia
    model = model_class.from_pretrained(args.model_name_or_path, from_tf=bool('.ckpt' in args.model_name_or_path), config=config)
File "julia/libs/anaconda3/envs/transformers/lib/python3.5/site-
packages/pytorch_transformers/modeling_utils.py", line 461, in from_pretrained  
model. **class**. **name** , "\n\t".join(error_msgs)))  
RuntimeError: Error(s) in loading state_dict for
BertRUBIForSequenceClassification:  
size mismatch for classifier.weight: copying a param with shape
torch.Size([174, 768]) from checkpoint, the shape in current model is
torch.Size([3, 768]).  
size mismatch for classifier.bias: copying a param with shape
torch.Size([174]) from checkpoint, the shape in current model is
torch.Size([3]).