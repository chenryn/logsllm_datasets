degree is 0.67. In this way, a UI-path can be constructed.
D. Implementation
PSD. PSD includes three modules: (1) App disassembler is
implemented using APKTOOL 2.3.3 [19], which reverses
Android apps to IR code (i.e., smali code) for further analysis.
(2) Setting extractor utilizes UIPicker [2] to extract all the
settings and their text explanations in an app. (3) Vectorizer
and privacy classiﬁer are developed using a machine-learning
toolkit called scikit-learn [20], to transform setting texts into
numeric vectors and train the classiﬁer.
To train the privacy classiﬁer, we gathered 12,208 text
items for settings and indicators from the 200 apps (including
100 English apps and 100 Chinese apps, as mentioned in
Section II-A). Then, based on Table I, we manually labeled
them as privacy-related or not. On ﬁrst sight, it seems that the
data could be directly used in training. However, the number
of privacy-related texts is too small (only 670), while there are
11,538 non-privacy texts. The classiﬁer trained from these data
will be prone to overﬁtting. Therefore, we extend the number
of privacy-related texts. Since manually analyzing more apps
is time-consuming, our idea is to generate synthetic samples
based on existing examples [21], which is commonly used for
learning from imbalanced data. Particularly, for the texts of
a privacy setting, we ﬁrst identify verbs and nouns through
Part-of-speech Tagging [22] of NLP, and change them into their
synonyms and antonyms to generate new texts. For example,
for the texts “share my location to friends”, Hound recognizes
the nouns location and friends, and the verb share. And then
Hound replaces them with other words and generates new texts
such as “hide my city to strangers” (see Figure 8). In this way,
we get 6,700 unique privacy-related texts from the original 670
privacy-related texts.
With the dataset, we tried several machine learning models
to build the classiﬁer, including decision trees [23], random
forests [24] and SVM [25] with linear, poly and RBF kernels.
To achieve better performance, we utilize Optunity 1.1.1 [26], a
library containing various optimizers for hyperparameter tuning,
to evaluate each classiﬁer with different parameters for 10,000
times. The results show that SVM with linear kernel performs
best with parameter C=2. We used 5-fold cross-validation, this
model achieves a precision of 96.64%, a recall of 97.94% and
an accuracy of 97.91%.
HPSI. HPSI is composed of two modules: (1) Hidden feature
extractor is supported by three techniques. UI-path tracer
extracts UI-paths using the semantics-based UI-path tracing.
Layout analyzer uses techniques from UIPicker [2] to analyze
the layout of each views. Icon resolver leverages “Best Guess”
of Google to get
the meaning of icons in UI-paths. (2)
Hiddenness classiﬁer is developed using the toolkit scikit-
learn [20] again.
The training data for the hiddenness classiﬁer is labeled
by participants in the second human subject study (see
Section II-B), which allows us to detect hidden privacy setting
from users’ perspective. In the study, they labeled 283 hidden
privacy settings and 317 easy-to-ﬁnd privacy settings. Then
similar to train the privacy classiﬁer, we use different models
and choose the one with the best performance as the ﬁnal
model. From our experiments (5-fold cross-validation), we
use SVM with RBF kernel (using the parameters: C=3 and
logGamma=-4). Our model achieves a precision of 93.01%,
a recall of 90.53% and an accuracy of 94.29%. Note that,
even though the training data come from the top 200 popular
apps, the evaluation of HPSI (see Section IV) demonstrates
the effectiveness of the classiﬁer on randomly selected apps
and therefore suggests that our training set (the 600 privacy
settings) is representative.
(cid:22)(cid:24)(cid:24)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:53:00 UTC from IEEE Xplore.  Restrictions apply. 
App Source
GooglePlay
TABLE IV: App collection
Count
50,000
17,414
6,118
23,826
Third-Party
Markets
360
Baidu
Xiaomi
Huawei
Tecent
908
1,734
IV. EVALUATION
We evaluate the effectiveness and performance of Hound as
follows.
Setting. We crawled real-world apps from various app markets
in 2017 and get 100,000 apps after removing duplicated ones
according to their MD5 checksums. Among the apps, 50,000
apps are from Google Play [27] and others are from third-
party markets in China. 37 categories are included (e.g., social,
business, dating, etc). Details of the apps are shown in Table IV.
Our server to statically analyze these apps has 80 cores with
2.2GHz CPU, 256GB memory and 70TB hard drives.
Effectiveness. The effectiveness of Hound depends on the
accuracy of PSD and HPSI. Thus, ﬁrst we evaluate the two
components respectively and then calculate the accuracy of the
whole system.
First, we evaluate how accurate PSD is to identify the privacy
settings among all the settings in an app. We should have to
manually check all the texts of settings to ﬁnd the privacy-
related ones referring to Table I and compare them with PSD’s
results. However, since it is pretty time-consuming to go over
all apps with manual work, we randomly selected 100 apps
for evaluation, including 50 apps from Google Play and 50
apps from Chinese third-party markets. In the 100 apps, PSD
discovers 470 privacy settings in 7,891 settings, including 454
real privacy settings and 16 wrongly identiﬁed ones, while we
ﬁnd 477 privacy settings manually. Thus, PSD’s precision is
96.60% (454/470), recall is 95.18% (454/477) and accuracy is
99.51% ((454+7,398)/7,891).
Secondly, we evaluate how accurate the HPSI is to identify
hidden ones among all the privacy settings discovered by PSD.
We ran an in-lab user study using a subset of the questionnaire
for the user study 2 (see Section II), including the questions
about whether settings are easy to ﬁnd, not why they are
easy/hard to ﬁnd, to get the manual labels from the participants.
Then we compared them with the prediction results of HPSI to
measure its effectiveness. Speciﬁcally, we randomly chose 200
privacy settings (100 hidden ones and 100 easy-to-ﬁnd ones
identiﬁed by HPSI) from 200 randomly selected apps (one
setting per each app, 100 English apps, and 100 Chinese apps).
Then we recruited 100 participants in the U.S. for English
apps and 100 participants in China for Chinese apps. Similar
to the user study 2, each participant was asked to ﬁnd ﬁve
given settings from ﬁve different apps in one questionnaire.
Then they reported the difﬁculty level (from very easy to
very difﬁcult) for locating a setting. In the end, we compared
the users’ average rating for locating a privacy setting with
the prediction result produced by HPSI. Details are shown
in Table V. Among the 200 privacy settings classiﬁed by
TABLE V: Evaluation results of HPSI
HPSI
hidden
easy-to-ﬁnd
total
Participants
easy-to-ﬁnd
5
93
98
hidden
95
7
102
total
100
100
200
ϭ͕ϮϭϬ ϭ͕ϮϬϬ ϭ͕ϮϬϰ
ϭ͕ϰϬϬ
ϭ͕ϮϬϬ
ϭ͕ϬϬϬ
ϴϬϬ
ϲϬϬ
ϰϬϬ
ϮϬϬ
Ϭ
Ϳ
Ɛ
;
Ğ
ŵ
ŝ
ƚ

Ğ
Ő
Ă
ƌ
Ğ
ǀ

ϵϳϭ
ϴϮϵ
ϲϱϱ
ϰϳϳ
ϯϭϬ
ϭϯϯ
Ϭ
 ϰ
Ϭ

ϰ
ϴ
 ϭϮ  ϭϲ ϮϬ Ϯϰ  Ϯϴ 
Ϯϴ
ϴ
ϭϮ
Ϯϰ
ϭϲ
ϮϬ
ƉƉ^ŝǌĞ;DͿ
ϯϮ ϯϲ
ϯϮ
Fig. 9: Time cost distribution
HPSI (100 hidden and 100 easy-to-ﬁnd ones), the participants
labeled 102 as hidden and 98 as easy-to-ﬁnd. After comparing
the settings one by one, we conclude that 188 (95+93) privacy
settings are correctly classiﬁed by HPSI, and 12 hidden privacy
settings are wrongly labeled. Hence, HPSI achieves an accuracy
of 94.00% (188/200). Notably, for hidden privacy settings, it
achieves a precision of 95.00% (95/100), a recall of 93.13%
(95/102) and an F1 score of 0.9412.
For the incorrectly classiﬁed ones, we look into them and
ﬁnd that their average scores are very close to three, which
means the privacy settings may be on the boundary. Further,
we communicate with participants about the 12 settings why
they view them as hidden or easy-to-ﬁnd ones to ﬁgure out the
reasons for the wrong classiﬁcation of HPSI. For the 7 false-
negative cases, HPSI identiﬁes the icons as gears for settings
and texts as privacy settings, but participants say that the icon
sizes and the text font are too small for them to identify, or the
color of icons (e.g., a light green gear) is less noticeable. Thus,
participants miss them while our HPSI discovers them. For
the 5 false-positive ones, 2 is that the HPSI fails to identify
the icon for setting, but participants think it is expressive for
understanding; the other 3 cases are that participants think
the indicators have enough semantic information for leading
to privacy settings, but HPSI thinks the indicator texts are
unrelated to privacy. For this situation, in the future, we can
improve the accuracy by adding the semantic relevance between
indicators and privacy settings as a new feature, even though
the indicator text is not related to privacy.
Therefore, while considering PSD and HPSI together, Hound
can achieve an accuracy of 93.54% (99.51% * 94.00%) for
hidden privacy settings identiﬁcation.
Performance. Hound has analyzed over 100,000 apps. Each
app costs about 530 seconds on average,
including app
dissembling, setting extracting, privacy discovering and hidden
setting identifying. The median time of app analysis is 378
seconds, ranging from 15 seconds to 1798 seconds. It takes a
longer time to analyze apps with larger code size. But from
Figure 9, when the size of an app reaches 24 MB, the average
time does not increase any more. We guess that it is because
Hound analyzes an app from the views.
(cid:22)(cid:24)(cid:25)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:53:00 UTC from IEEE Xplore.  Restrictions apply. 
Ž
ŝ
ƚ
Ă
Z
ϱϬй
ϰϱй
ϰϬй
ϯϱй
ϯϬй
Ϯϱй
ϮϬй
ϭϱй
ϭϬй
ϱй
Ϭй
ϰϭ͘ϭϭй
ϯϳ͘ϱϯй ϯϯ͘ϯϯй
ϭ͘ϴϮ
Ϯ͘Ϯϯ
Ϯ͘Ϯϳ
ϯ͘ϲϴ
ϯϮ͘ϵϲй
Ϯϲ͘ϰϳй
Ϯ͘ϳϱ
Ϯ͘ϰϯй
Ϯ͘ϱϵй
ϭϭ͘ϭϭй
ϳ͘ϯϲй
Ϯ͘ϵϵй
ϭŵŝůůŝŽŶ ϭϬŵŝůůŝŽŶ
Ϭ
ϭϬ
ϱϬϬ
ηŽĨĚŽǁŶůŽĂĚƐ
ϱ
ϰ͘ϱ
ϰ
ϯ͘ϱ
ϯ
Ϯ͘ϱ
Ϯ
ϭ͘ϱ
ϭ
Ϭ͘ϱ
Ϭ