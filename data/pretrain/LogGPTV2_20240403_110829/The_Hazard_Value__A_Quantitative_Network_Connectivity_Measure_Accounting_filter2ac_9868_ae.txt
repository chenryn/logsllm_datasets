101
100
)
s
(
e
m
i
t
U
P
C
10−1
10−2
0
1000
2000
Strategic
Brute-force
250
500
750
1000
instances
1250
1500
1750
Strategic
Brute-force
5000
6000
3000
4000
instances
Figure 3. Topology Zoo with service chaining (left) and waypointing (right)
103
102
101
100
)
s
(
e
m
i
t
U
P
C
10−1
10−2
0
Strategic
Brute-force
250
300
350
Strategic
Brute-force
50
100
150
instances
200
250
50
100
150
200
instances
Figure 4. Datacenter with basic reachability (left) and valley-free routing (right)
of sizes up to couple of hundred of nodes and 300-400
links. For the more regular datacenter topologies, we can
within 5 minutes compute the hazard value for similarly sized
topologies in the number of nodes but with up to 1000 links
and for a larger number of failed links (between 10 to 20).
As a result, our method is (in its prototype implementation)
already applicable to medium size networks, and we expect
that additional optimizations can further improve the scaling.
C. Hazard Values for ISP Topologies
Finally, we analyze the hazard values for ISP topologies
from the Topology Zoo for k = 3 number of failed links. The
distribution of the hazard values for these network topologies
is depicted in Figure 5. We can notice that there are few highly
connected networks with hazard value zero, meaning that
there is no failure scenario with up to 3 failed links that can
disconnect any of the demands, meaning the these networks
are highly resilient for failures. An example of such a network
is GlobalCenter with the hazard value zero as depicted2 in
Figure 6. More than 200 out of 260 topologies have a hazard
value below 0.002, meaning they only suffer a loss in value of
0.2% compared to the maximal achievable reward by a fully
connected network. However, there are a few topologies with
relatively high hazard value. In case of the Sogo network in
Figure 6, the hazard value is 0.01, meaning that connection
failures are expected to lead to a 1% loss in reward value.
2The Topology Zoo graphs are taken from http://www.topology-zoo.org/
Figure 5. Hazard values for the Topology Zoo (k = 3)
Figure 6. GlobalCenter (left) with γ=0.00000 for k = 3 and Sogo (right)
with γ=0.01134 for k = 3
for valley-free routing (discussed earlier) the two curves open
even faster than in the case of ISP topologies.
In conclusion, our strategic algorithm is applicable to real
ISP and datacenter topologies, and the current implementation
(in Python) allows us to compute in about 5 minutes the
hazard value for 2 to 3 failed links on Topology Zoo networks
248
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:15:13 UTC from IEEE Xplore.  Restrictions apply. 
It is clear that in such a network topology, there is a large
number of failure scenarios that can completely disconnect
the end-points in the network. Hence, the computed hazard
values correspond to the intuitive understanding of more or
less resilient types of network topologies.
VII. RELATED WORK
There exists literature on the empirical characteristics
in datacenters [16], [37], state-wide net-
of failures, e.g.,
works [34], or IP backbones [20]. This is highly valuable for
the comparison of existing networks, but does not directly
solve the problem of comparing network designs that are
not yet implemented. On the other hand, empirical research
provides valuable inputs to the method described in this paper.
In the graph-theory community, the connectivity of a graph
is often measured in terms of its minimum cut or the number
of available disjoint paths, which are common measures not
only for the throughput achievable in a network but also for
its resilience [2], [3], [35]; cuts also form the basis for the
frequently used expansion measures [19]. Another approach,
primarily used by the parallel-computing community, is to
measure how many failures a network can sustain while still
being able to emulate its ideal counterpart with a certain
maximal overhead (e.g., a constant slowdown) [24], [29]. Both
worst-case scenarios—in which adversarial failures cause a
loss of connection—as well as average-case scenarios—in
which the probability of loss of connection is computed—are
well understood [2]. However, these generic graph-theoretic
metrics treat all the connections in a graph as equal, and do not
consider the fact that for a network operator some connections
are more important than others.
Within the networking community, additional speciﬁc met-
rics are considered. Some of them, such as the protection
ratio [15], revolve around single failures (asking how many
individual failures are protected), while we in this paper are
particularly interested in multiple failures; other metrics, such
as the loop ratio [9], are concerned with the detailed network
behavior during convergence after failures, while during the
consideration of alternative topologies,
the details of the
routing mechanism are not always known in sufﬁcient detail to
consider such metrics—which is why we assume fast rerouting
in this paper. Two interesting connectivity measures in the
context of scenarios with multiple failures, and accounting
for the locality constraints imposed by fast rerouting, are the
perfect [11]–[13] and the ideal [8], [14] resilience. However,
these measures only account for locality constraints, but not
any of the other aspects considered in this paper, nor do they
provide similar powerful properties.
Our work is also related to survivable network design, a
classic topic in operations research [17], [28]. A general and
powerful approach to design networks which are robust to
failures, uses mathematical programming, and in particular,
(integer) linear programming. A well-known platform which
also provides benchmarks for telecommunication networks is
SNDlib [27]. However, we are not aware of any existing
approach in this context which accounts, e.g., for alternatives
or more complex routing constraints and DFPs.
There further exists interesting literature on the impact
of routing constraints on connectivity, for example, in the
context of inter-domain routing (in the absence of failures),
where routing policies typically need to be compatible with
business considerations [22]; or in the context of fast rerout-
ing where failover rules are inherently local and can harm
connectivity [7]. Our paper accounts for the physical topology
explicitly, among other properties, and we model the routing
with constraints. Our approach is hence orthogonal to work
on resilient routing mechanisms [30]. It is also orthogonal to
work studying how to verify and maintain routing and policy
constraints under failures [21], [32].
We are not aware of any connectivity measure explicitly
accounting for differences in demand on different connections,
choice between connections to resolve a given demand, pref-
erences and priorities in resolving that choice, and speciﬁc
failures. With this work, we aim to ﬁll this gap and make a
proposal for a measure which meets a number of desirable
properties, and which can be computed efﬁciently.
VIII. CONCLUSION
Assessing whether a network is sufﬁciently robust depends
on more than its topological connectivity. Motivated by this
observation, in collaboration with a local network operator, we
developed a more general network connectivity measure, the
hazard value, which allows to account for speciﬁc demands
and distributions of failure probabilities, alternatives, routing
constraints and priorities. We have shown how the hazard value
can be computed efﬁciently, and can provide interesting new
insights into the connectivity of existing networks.
Regarding the required input parameters to the hazard value
computation, the trafﬁc patterns can be derived from network
monitoring and historical data, using either the worst-case
scenario or dividing the daily trafﬁc into a ﬁnite number
of time slices and analyzing them separately. Distribution
of failure probabilities can be derived also from historical
data and combining the independent link, node and shared-
link groups failure probabilities. The routing constraints are
by default assuming basic reachability but it can be further
restricted by the input from the network operators that may
require waypointing on some routers, valley-free routing poli-
cies, blacklisting of certain routers etc. Finally, the alternative
destinations and the weights of each demand also require an
input from the network operator but can be in the ﬁrst iteration
approximated by e.g., the amount of trafﬁc for each demand
(more trafﬁc implying higher weight). A further research
direction is to try to automate the collection of the input data
needed for the computation of the hazard value.
Acknowledgments. We would like to thank Henrik Thostrup
Jensen from NORDUnet for many inputs and discussions. This
research was supported by the Vienna Science and Technology
Fund (WWTF), project WHATIF, ICT19-045, 2020-2024,
DFF project QASNET, the Villum Investigator Project S4OS,
and the ERC Advanced Grant Project LASSO.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:15:13 UTC from IEEE Xplore.  Restrictions apply. 
249
REFERENCES
[1] A. Bagchi, A. Bhargava, A. Chaudhary, D. Eppstein, and C. Scheideler.
Theory of Computing
The effect of faults on network expansion.
Systems, 39(6):903–928, 2006.
[2] A. Bagchi, A. Bhargava, A. Chaudhary, D. Eppstein, and C. Scheideler.
Theory of Computing
The effect of faults on network expansion.
Systems, 39(6):903–928, 2006.
[3] A. Bagchi, A. Chaudhary, C. Scheideler, and P. Kolman. Algorithms for
fault-tolerant routing in circuit switched networks. In Proc. 14th Annual
ACM symposium on Parallel Algorithms and Architectures (SPAA),
pages 265–274, 2002.
[4] T. Benson, A. Anand, A. Akella, and M. Zhang. Understanding data
center trafﬁc characteristics. ACM SIGCOMM Computer Communica-
tion Review, 40(1):92–99, 2010.
In Proceedings of
[5] M. N. Brain, J. H. Davenport, and A. Griggio. Benchmarking solvers,
SAT-style.
the 2nd International Workshop on
Satisﬁability Checking and Symbolic Computation co-located with the
42nd International Symposium on Symbolic and Algebraic Computation
(ISSAC’17), volume 1974 of CEUR, pages 1–15. CEUR-WS.org, 2017.
Fat-trees: Universal networks for hardware-
IEEE Transactions on Computers,
[6] Charles E. Leiserson.
supercomputing.
efﬁcient
34(10):892–901, 1985.
[7] M. Chiesa, A. Kamisinski, J. Rak, G. Retvari, and S. Schmid. A
survey of fast-recovery mechanisms in packet-switched networks. IEEE
Communications Surveys and Tutorials (COMST), 2021.
[8] M. Chiesa,
I. Nikolaevskiy, S. Mitrovi´c, A. Gurtov, A. Madry,
M. Schapira, and S. Shenker. On the resiliency of static forwarding
tables. IEEE/ACM Transactions on Networking, 25(2):1133–1146, 2016.
[9] F. Clad. Disruption-free routing convergence: computing minimal link-
state update sequences. PhD thesis, Strasbourg, 2014.
[10] P. Cuijpers, S. Schmid, N. Schnepf,
Repro-
ducibility package for: The hazard value: A quantitative net-
work connectivy measure
failures, Mar. 2022.
https://doi.org/10.5281/zenodo.6394782.
accounting for
and J. Srba.
[11] J. Feigenbaum, B. Godfrey, A. Panda, M. Schapira, S. Shenker, and
A. Singla. Brief announcement: On the resilience of routing tables. In
Proceedings of the 2012 ACM symposium on Principles of distributed
computing, pages 237–238, 2012.
[12] K.-T. Foerster, J. Hirvonen, Y.-A. Pignolet, S. Schmid, and G. Tredan.
On the feasibility of perfect resilience with local fast failover.
In
Proc. SIAM Symposium on Algorithmic Principles of Computer Systems
(APOCS), 2021.
[13] K.-T. Foerster, J. Hirvonen, Y.-A. Pignolet, S. Schmid, and G. Tredan.
On the price of locality in static fast rerouting. In Proc. 52nd IEEE/IFIP
International Conference on Dependable Systems and Networks (DSN),
2022.
[14] K.-T. Foerster, A. Kamisinski, Y.-A. Pignolet, S. Schmid, and G. Tredan.
IEEE Transactions on
Improved fast rerouting using postprocessing.
Dependable and Secure Computing, 2020.
[15] P. Francois and O. Bonaventure. An evaluation of ip-based fast reroute
techniques. In Proceedings of the 2005 ACM conference on emerging
network experiment and technology, pages 244–245, 2005.
[16] P. Gill, N. Jain, and N. Nagappan. Understanding network failures in
data centers: measurement, analysis, and implications. In Proceedings
of the ACM SIGCOMM 2011 conference, pages 350–361, 2011.
[17] M. Grötschel, C. L. Monma, and M. Stoer. Design of survivable
networks. Handbooks in operations research and management science,
7:617–672, 1995.
[18] C. Guo, G. Lu, D. Li, H. Wu, X. Zhang, Y. Shi, C. Tian, Y. Zhang,
S. Lu, and G. Lv. Bcube: A high performance, server-centric network
architecture for modular data centers. In ACM SIGCOMM. Association
for Computing Machinery, Inc., August 2009.
[19] S. Hoory, N. Linial, and A. Wigderson. Expander graphs and their
applications. Bulletin of the American Mathematical Society, 43(4):439–
561, 2006.
[20] G. Iannaccone, C.-n. Chuah, R. Mortier, S. Bhattacharyya, and C. Diot.
Analysis of link failures in an ip backbone. In Proceedings of the 2nd
ACM SIGCOMM Workshop on Internet measurment, pages 237–242,
2002.
[21] P. G. Jensen, M. Konggaard, D. Kristiansen, S. Schmid, B. C. Schrenk,
and J. Srba. Aalwines: A fast and quantitative what-if analysis tool
for mpls networks.
In Proc. 16th ACM International Conference on
emerging Networking EXperiments and Technologies (CoNEXT), 2020.
[22] R. Klöti, V. Kotronis, B. Ager, and X. Dimitropoulos. Policy-compliant
path diversity and bisection bandwidth. In 2015 IEEE Conference on
Computer Communications (INFOCOM), pages 675–683. IEEE, 2015.
[23] S. Knight, H. Nguyen, N. Falkner, R. Bowden, and M. Roughan. The
internet topology zoo. Selected Areas in Communications, IEEE Journal
on, 29:1765 – 1775, 11 2011.
[24] F. T. Leighton, B. M. Maggs, and R. K. Sitaraman. On the fault tolerance
of some popular bounded-degree networks. SIAM Journal on computing,
27(5):1303–1333, 1998.
[25] B. M. Maggs and R. K. Sitaraman. Algorithmic nuggets in content
delivery. ACM SIGCOMM Computer Communication Review, 45(3):52–
66, 2015.
[26] W. Najjar and J.-L. Gaudiot. Network resilience: A measure of network
fault tolerance. IEEE Transactions on Computers, 39(2):174–181, 1990.
[27] S. Orlowski, M. Pióro, A. Tomaszewski, and R. Wessäly. SNDlib
1.0–Survivable Network Design Library.
In Proceedings of the 3rd
International Network Optimization Conference (INOC 2007), Spa,
Belgium, April 2007. http://sndlib.zib.de, extended version accepted in
Networks, 2009.
[28] P. Pavon-Marino and J.-L. Izquierdo-Zaragoza. Net2plan: an open
source network planning tool for bridging the gap between academia
and industry. IEEE Network, 29(5):90–96, 2015.
[29] C. Scheideler. Models and techniques for communication in dynamic
In Annual Symposium on Theoretical Aspects of Computer
networks.
Science, pages 27–49. Springer, 2002.
[30] S. Schmid, N. Schnepf, and J. Srba. Resilient capacity-aware routing.
In Proceedings of
the 25th International Conference on Tools and
Algorithms for the Construction and Analysis of Systems (TACAS’21),
volume 12651 of LNCS, pages 411–429. Springer-Verlag, 2021.
[31] P. Sebos, J. Yates, G. Hjalmtysson, and A. Greenberg. Auto-discovery
of shared risk link groups. In OFC 2001. Optical Fiber Communication
Conference and Exhibit. Technical Digest Postconference Edition (IEEE
Cat. 01CH37171), volume 3, pages WDD3–WDD3. IEEE, 2001.
[32] S. Steffen, T. Gehr, P. Tsankov, L. Vanbever, and M. Vechev. Prob-
In Proceedings of
abilistic veriﬁcation of network conﬁgurations.
the Annual conference of the ACM Special Interest Group on Data
Communication on the applications, technologies, architectures, and
protocols for computer communication, pages 750–764, 2020.
[33] L. Suresh, M. Canini, S. Schmid, and A. Feldmann. C3: Cutting tail
latency in cloud data stores via adaptive replica selection.
In 12th
USENIX Symposium on Networked Systems Design and Implementation
(NSDI), pages 513–527, 2015.
[34] D. Turner, K. Levchenko, A. C. Snoeren, and S. Savage. California
fault lines: understanding the causes and impact of network failures. In
Proceedings of the ACM SIGCOMM 2010 Conference, pages 315–326,
2010.
[35] E. Upfal. Tolerating linear number of faults in networks of bounded
In Proceedings of the eleventh annual ACM symposium on
degree.
Principles of distributed computing, pages 83–89, 1992.
[36] K. Vajanapoom, D. Tipper, and S. Akavipat. A risk management
approach to resilient network design.
In International Congress on
Ultra Modern Telecommunications and Control Systems, pages 622–
627. IEEE, 2010.
[37] D. Zhuo, M. Ghobadi, R. Mahajan, K.-T. Förster, A. Krishnamurthy, and
T. Anderson. Understanding and mitigating packet corruption in data
center networks. In Proceedings of the Conference of the ACM Special
Interest Group on Data Communication, pages 362–375, 2017.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:15:13 UTC from IEEE Xplore.  Restrictions apply. 
250