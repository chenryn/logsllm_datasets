### Block Validation Protocol

1. **Share and Commitment Generation:**
   - Construct the sharing block \( S_B := \langle \text{Commitment}, VSS.\mathcal{C}, e, z_s^e \rangle_{L_e} \), where \( VSS.\mathcal{C} := \{VSS.C_1, \ldots, VSS.C_n\} \) are commitments to the \( n \) random numbers generated by running \( (VSS.\mathcal{S}_i, VSS.\mathcal{W}_i, VSS.C_i) \leftarrow VSS.ShGen(s_i) \) for \( i \in \{1, \ldots, n\} \).
   - For each node \( p_j \), build the share vector \( VSS.\mathcal{S}_j := \{VSS.s_1 \leftarrow VSS.\mathcal{S}_{1,j}, \ldots, VSS.s_n \leftarrow VSS.\mathcal{S}_{n,j}\} \) and the witness vector \( VSS.\mathcal{W}_j := \{VSS.\pi_1 \leftarrow VSS.\mathcal{W}_{1,j}, \ldots, VSS.\pi_n \leftarrow VSS.\mathcal{W}_{n,j}\} \) using the \( j \)-th share and witness from \( VSS.\mathcal{S}_i \) and \( VSS.\mathcal{W}_i \) for the random number \( s_i \).
   - Send \( VSS.\mathcal{S}_j \), \( VSS.\mathcal{W}_j \), and \( S_B \) to every node \( p_j \in P \).

2. **Blame/Forward:**
   - If the epoch timer \( \text{epoch-timer}_{e-1} \geq 8\Delta \) and node \( p_i \) receives a valid share vector \( VSS.\mathcal{S}_i \), witness vector \( VSS.\mathcal{W}_i \), and commitment \( S_B := \langle \text{Commitment}, VSS.\mathcal{C}, e, z_s^e \rangle_{L_e} \), then invoke \( \text{Deliver}(\text{Commitment}, S_B, z_s^e, e) \).
   - If no shares have been received within \( 3\Delta \) time while in epoch \( e-1 \), broadcast a blame \( \langle \text{blame}, e \rangle_i \) to all nodes.

3. **Request Open:**
   - Wait until \( \text{epoch-timer}_{e-1} \geq 5\Delta \). Collect all blames received so far.
   - If up to \( t \) blames are received, forward the blames to \( L_e \).
   - If no blames or equivocation by \( L_e \) has been detected, send \( \langle \text{ack}, H(S_B), e \rangle_i \) to \( L_e \).

4. **Private Open:**
   - \( L_e \) sends the valid share \( VSS.\mathcal{S}_j \) and witness \( VSS.\mathcal{W}_j \) to node \( p_i \) for every blame \( \langle \text{blame}, e \rangle_j \) received from node \( p_i \).

5. **Ack:**
   - Upon receiving valid share \( VSS.\mathcal{S}_j \) and witness \( VSS.\mathcal{W}_j \) for every \( \langle \text{blame}, e \rangle_j \) it forwarded and detecting no equivocation, send \( \langle \text{ack}, H(S_B), e \rangle_i \) to \( L_e \).

6. **(Non-blocking) Equivocation:**
   - Broadcast equivocating hashes signed by \( L_e \) and stop performing any operations.
   - Forward share \( VSS.\mathcal{S}_j \) and witness \( VSS.\mathcal{W}_j \) to node \( p_j \) for every \( \langle \text{blame}, e \rangle_j \) it received.

### Setup and Blocks

- **Setup:**
  - Set \( e = 1 \). All nodes agree upon and fill \( Q(p_i) \) with \( m = n + t \) tuples for all \( p_i \in P \). Set \( P_r \leftarrow \emptyset \). Run \( VSS.Setup \) and agree on the public parameters. Set \( L_e \leftarrow p_1 \).
  - Generate random secrets \( s_i \leftarrow \{0, 1\}^\kappa \) for \( 1 \leq i \leq n \).

- **Blocks:**
  - While in epoch \( e-1 \), leader \( L_e \) starts the block validation protocol (refer to Figure 8) with \( \{s_1, \ldots, s_n\} \), where the secrets are chosen randomly.
  - In epoch \( e \), \( L_e \) proposes block \( B_h \) with \( b_h := (H(S_B), AC_e(S_B)) \) where \( AC_e(S_B) \) is an acknowledgment certificate for commitment \( S_B \).

- **Update:**
  - When \( \text{epoch-timer}_e \) expires, if \( L_{e-t} \) proposed a valid block \( B_l \) in epoch \( e-t \) and \( B_l \) has been committed by epoch \( e \), update \( Q(L_{e-t}) \) with \( n \) tuples containing secret shares, witnesses, and commitments shared in epoch \( e-t \). Otherwise, remove \( L_{e-t} \) from future proposals, i.e., \( P_r \leftarrow P_r \cup \{L_{e-t}\} \).

- **Reconstruct:**
  - When \( \text{epoch-timer}_e \) expires, do the following:
    1. Get \( (VSS.\mathcal{S}, VSS.\mathcal{W}, VSS.\mathcal{C}) := \{\text{Dequeue}(Q(p_j)) \mid p_j \notin P_r\} \).
    2. Build homomorphic sum share \( S_V_i \), witness \( VSS.\pi_i \), and commitment \( VSS.C_e \) using all shares from \( VSS.\mathcal{C} \). Send \( S_V_i \) and \( VSS.\pi_i \) to all the nodes.
    3. Upon receiving share \( S_V_j \) and witness \( VSS.\pi_j \) for \( VSS.C_e \), ensure that \( VSS.ShVrfy(S_V_j, VSS.\pi_j, VSS.C_e) = 1 \).
    4. Upon receiving \( (t + 1) \) valid homomorphic sum shares in \( S_V \), obtain \( R_e \leftarrow VSS.Recon(S_V) \).

- **Output:**
  - Compute and output \( O_e \leftarrow H(R_e) \).

### Performance Evaluation

- **Implementation:**
  - We implement the BRandPiper protocol in Rust [11, Performance Evaluation] due to its strong support for correctness in concurrency and compile-time memory safety guarantees. Our implementation is lock-free and uses message passing to ensure efficiency. We provide setup parameters for every node in the config files. Our code is event-driven and reacts to various timeouts and messages from the network. We use ED25519 for digital signatures and the BLS-12-381 [9] curve. We use implementations of SCRAPE [17] over this curve as the PVSS scheme, Pedersen-based eVSS, and Polycommit [32] over this curve as the VSS and bilinear accumulator scheme.

- **Optimizations:**
  - We perform system-level optimizations:
    1. Generate random shares before the propose step (this can be done using extra cores or an external node supplying the shares) and use them during the propose step.
    2. Take advantage of the Tokio library [48] and futures in Rust to run concurrently without spawning threads.
    3. Implement both the accumulator libraries: the bilinear accumulator and the Merkle tree accumulator. We observe that the computational performance of the Merkle tree accumulator is much better in practice.

- **Setup:**
  - All experiments were conducted on t2.micro AWS instances from the Ohio region, which have 1 GB RAM, 8 GB hard disk, and 1 vCPU running at up to 3.3 GHz. The advertised bandwidth for these instances is 60-80 MBits/s.

- **Baselines:**
  - We compared the performance of our implementation with two baselines: Drand [25] and HydRand [42, 44]. We chose Drand because it is a practically deployed system implementing Cachin et al. [16], and evaluating our performance against it justifies our practicality. We chose HydRand as our second baseline because it is theoretically related to our work: HydRand requires a 2/3 honest majority, whereas we require only a 1/2 honest majority. Note that the basic HydRand protocol and implementation [42] only offer bias-resistance but no unpredictability: an adversary may correctly predict a random beacon in \( t + 1 \) epochs in advance.

- **Micro-Benchmarks:**
  - We measure the efficiency of the primitives used in our protocol. Specifically, we measure the run times for:
    1. Accumulator share generation, verification, and reconstruction for the bilinear accumulator and the Merkle tree accumulator.
    2. PVSS share generation, verification, and reconstruction.
    3. eVSS share generation, verification, and reconstruction.
    4. The size of the various messages used in the protocol.
  - We observe that the Merkle tree accumulator for our small scales has smaller message sizes and very efficient run times. We also observe that eVSS operations generally perform much better than their PVSS counterparts.

- **Key Metric:**
  - We compare the number of beacon values that can be produced in a minute. Compared to Drand, we have the advantage of reconfigurability and weaker network assumptions. Compared to HydRand, we can tolerate more faults. The methodology is to run protocols at appropriate values of \( \Delta \), which in turn depends on the computation and communication costs. We provide additional micro-benchmarks in the full online version [10].

- **Results:**
  - From Figure 10, it is clear that the Merkle tree-based BRandPiper is quantitatively as practical as the state-of-the-art practical random beacon protocol: Drand. Drand uses a leader to coordinate the DKG and reconfiguration protocols, but there is no description on how to recover if the leader was Byzantine. Additionally, in Drand, synchronization for the reconfigured instance is via the coordinator (the leader). It is not clear how the protocol will recover if the leader becomes Byzantine. Therefore, qualitatively, we use much clearer and formal network assumptions and allow efficient and secure reconfiguration, including synchronization for incoming nodes, without pausing the protocol, unlike Drand. Thus, BRandPiper is not just theoretically interesting but also practical.

### Acknowledgements

We would like to thank our shepherd Alin Tomescu, Sourav Das, and the anonymous reviewers for their insightful feedback to improve this draft. This work has been partially supported by research gift grants from VMware Research and Novi, the Army Research Laboratory (ARL) under grant W911NF-20-2-0026, the National Institute of Food and Agriculture (NIFA) under grant 2021-67021-34251, and the National Science Foundation (NSF) under grant CNS-1846316.