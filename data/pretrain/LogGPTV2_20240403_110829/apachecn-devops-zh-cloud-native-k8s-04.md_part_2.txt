Kubectl rollout undo deployment myapp-deployment
```
在我们之前的案例中，我们只有两个版本，初始版本和带有更新容器的版本，但是如果我们有其他版本，我们可以像这样在`undo`命令中指定它们:
```
Kubectl rollout undo deployment myapp-deployment –to-revision=10
```
这将让您一窥为什么部署如此有价值—它们让我们可以对应用新版本的部署进行微调控制。接下来，我们将讨论 Kubernetes 的智能缩放器，它与部署和复制集协同工作。
# 利用水平 Pod 自动缩放器
正如我们所看到的，部署和副本集允许您指定在特定时间应该可用的副本总数。然而，这两种结构都不允许自动缩放-它们必须手动缩放。
**水平 Pod 自动缩放器** ( **HPA** )通过作为更高级别的控制器存在来提供该功能，该控制器可以基于诸如 CPU 和内存使用等指标来改变部署或复制集的复制计数。
默认情况下，HPA 可以根据 CPU 利用率自动扩展，但通过使用自定义指标，该功能可以扩展。
住房公积金的 YAML 档案是这样的:
hpa，yaml
```
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: myapp-hpa
spec:
  maxReplicas: 5
  minReplicas: 2
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: myapp-deployment
  targetCPUUtilizationPercentage: 70
```
在前面的规范中，我们有`scaleTargetRef`，它指定了什么应该由 HPA 自动缩放，以及调谐参数。
`scaleTargetRef`的定义可以是部署、复制集或复制控制器。在本例中，我们定义了 HPA 来扩展我们之前创建的部署`myapp-deployment`。
对于调整参数，我们使用默认的基于 CPU 利用率的缩放，因此我们可以使用`targetCPUUtilizationPercentage`来定义运行我们的应用的每个 Pod 的预期 CPU 利用率。如果我们的 Pods 的平均 CPU 使用率增加超过 70%，我们的 HPA 将扩大部署规格，如果它下降足够长的时间，它将缩小部署。
典型的缩放事件如下所示:
1.  在三个副本上，一个部署的平均 CPU 使用率超过 70%。
2.  HPA 控制回路注意到了这种 CPU 利用率的增加。
3.  HPA 使用新的副本数量编辑部署规范。此计数是基于 CPU 利用率计算的，目的是使每个节点的 CPU 使用率稳定在 70%以下。
4.  部署控制器启动一个新的副本。
5.  此过程会重复进行，以向上或向下扩展部署。
总之，HPA 跟踪 CPU 和内存利用率，并在超出界限时启动扩展事件。接下来，我们将回顾 DaemonSets，它提供了一种非常特殊类型的 Pod 控制器。
# 实现 DaemonSets
从现在开始到本章结束，我们将回顾更多的小众选项，当涉及到运行具有特定需求的应用时。
我们将从 DaemonSets 开始，它类似于 replica set，只是副本的数量固定为每个节点一个副本。这意味着群集中的每个节点将随时保持应用的一个副本处于活动状态。
重要说明
重要的是要记住，在没有额外的 Pod 放置控件(如污点或节点选择器)的情况下，该功能将只为每个节点创建一个副本，我们将在 [*第 8 章*](08.html#_idTextAnchor186)*Pod 放置控件*中详细介绍。
对于典型的 DaemonSet，结果如下图所示:
![Figure 4.3 – DaemonSet spread across three nodes](img/B14790_04_003.jpg)
图 4.3–daemmonset 分布在三个节点上
正如您在上图中看到的，每个节点(由一个框表示)包含一个应用 Pod，由 DaemonSet 控制。
这使得 daemmonsets 非常适合运行在节点级别收集指标或在每个节点基础上提供网络进程的应用。DaemonSet 规范如下所示:
daemmonset-1 . YAML
```
apiVersion: apps/v1 
kind: DaemonSet
metadata:
  name: log-collector
spec:
  selector:
      matchLabels:
        name: log-collector   
  template:
    metadata:
      labels:
        name: log-collector
    spec:
      containers:
      - name: fluentd
        image: fluentd
```
如您所见，这与您的典型副本集规范非常相似，只是我们没有指定副本的数量。这是因为 DaemonSet 会尝试在集群中的每个节点上运行 Pod。
如果要指定运行应用的节点子集，可以使用如下文件所示的节点选择器来实现:
daemmonset-2 . YAML
```
apiVersion: apps/v1 
kind: DaemonSet
metadata:
  name: log-collector
spec:
  selector:
      matchLabels:
        name: log-collector   
  template:
    metadata:
      labels:
        name: log-collector
    spec:
      nodeSelector:
        type: bigger-node 
      containers:
      - name: fluentd
        image: fluentd
```
这个 YAML 将把我们的 DaemonSet 限制在标签中与`type=bigger-node`选择器匹配的节点上。我们将在 [*第 8 章*](08.html#_idTextAnchor186)*Pod 放置控制*中了解更多关于节点选择器的信息。现在，让我们讨论一种非常适合运行有状态应用(如数据库)的控制器 StatefulSet。
# 了解状态集
状态集与复制集和部署非常相似，但有一个关键区别，使它们更适合于有状态工作负载。状态集维护每个 Pod 的顺序和身份，即使这些 Pod 被重新调度到新节点上。
例如，在 3 个副本的状态集合中，总是会有 Pod 1、Pod 2 和 Pod 3，这些 Pod 将在 Kubernetes 和存储中保持它们的身份(我们将在 [*第 7 章*](07.html#_idTextAnchor166)*在 Kubernetes* 上的存储中了解到这一点)，而不管发生任何重新调度。
让我们看看一个简单的 StatefulSet 配置:
statefulset.yaml .状态设定
```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: stateful
spec:
  selector:
    matchLabels:
      app: stateful-app
  replicas: 5
  template:
    metadata:
      labels:
        app: stateful-app
    spec:
      containers:
      - name: app
        image: busybox
```
这个 YAML 将创建一个包含我们的应用的五个副本的状态集。
让我们看看状态集如何以不同于典型部署或复制集的方式维护 Pod 身份。让我们使用以下命令获取所有 Pods:
```
kubectl get pods
```
输出应该如下所示:
```
NAME      		   READY     STATUS    RESTARTS   AGE
stateful-app-0     1/1       Running   0         55s
stateful-app-1     1/1       Running   0         48s
stateful-app-2     1/1       Running   0         26s
stateful-app-3     1/1       Running   0         18s
stateful-app-4     0/1       Pending   0         3s
```
如您所见，在这个例子中，我们有五个 StatefulSet Pods，每个 Pods 都有一个数字指示器来指示它们的身份。该属性对于有状态应用(如数据库集群)非常有用。在 Kubernetes 上运行数据库集群的情况下，主 Pods 相对于副本 Pods 的身份很重要，我们可以使用 StatefulSet 身份来轻松管理它。
另一个有趣的点是，你可以看到最终的 PODS 仍然在启动，PODS 的年龄随着数字身份的增加而增加。这是因为 StatefulSet Pods 是按顺序一次创建一个的。
为了运行有状态的应用，StatefulSets 与持久的 Kubernetes 存储配合使用很有价值。我们将在 [*第 7 章*](07.html#_idTextAnchor166)*Kubernetes 上的存储*中了解更多信息，但现在，让我们讨论另一个具有非常具体用途的控制器:乔布斯。
# 使用作业
Kubernetes 中的 Job 资源的目的是运行可以完成的任务，这使得它们对于长时间运行的应用来说并不理想，但是对于批处理作业或类似的可以从并行性中获益的任务来说却很棒。
以下是 YAML 的工作规范:
工作-1.yaml
```
apiVersion: batch/v1
kind: Job
metadata:
  name: runner
spec:
  template:
    spec:
      containers:
      - name: run-job
        image: node:lts-jessie
        command: ["node", "job.js"]
      restartPolicy: Never
  backoffLimit: 4
```
此作业将启动单个 Pod，并运行命令`node job.js`，直到它完成，此时 Pod 将关闭。在本示例和以后的示例中，我们假设使用的容器映像有一个运行作业逻辑的文件`job.js`。`node:lts-jessie`容器映像默认不会有这个。这是一个没有并行运行的作业的示例。正如您可能从 Docker 用法中了解到的，多个命令参数必须作为字符串数组传递。
为了创建一个可以并行运行的作业(也就是说，多个副本同时运行该作业)，您需要开发您的应用代码，以便它可以在结束该过程之前告知作业已经完成。为了做到这一点，作业的每个实例都需要包含代码，以确保它完成更大批量任务的正确部分，并防止重复工作的发生。
有几种应用模式可以实现这一点，包括互斥锁和工作队列。此外，代码需要检查整个批处理任务的状态，这也可以通过更新数据库中的值来处理。一旦作业代码看到更大的任务完成，它应该退出。
完成后，您可以使用`parallelism`键将并行性添加到作业代码中。下面的代码块显示了这一点:
工作 2.yaml
```
apiVersion: batch/v1
kind: Job
metadata:
  name: runner
spec:
  parallelism: 3
  template:
    spec:
      containers:
      - name: run-job
        image: node:lts-jessie
        command: ["node", "job.js"]
      restartPolicy: Never
  backoffLimit: 4
```
如您所见，我们添加了带有三个副本的`parallelism`键。此外，您可以将纯作业并行度替换为指定的完成次数，在这种情况下，Kubernetes 可以跟踪作业已经完成了多少次。在这种情况下，您仍然可以设置并行度，但是如果不设置，它将默认为 1。
下一个规范将运行一个作业`4`次直到完成，`2`次迭代在任何给定的时间运行:
工作-3.yaml
```
apiVersion: batch/v1
kind: Job
metadata:
  name: runner
spec:
  parallelism: 2
  completions: 4
  template:
    spec:
      containers:
      - name: run-job
        image: node:lts-jessie
        command: ["node", "job.js"]