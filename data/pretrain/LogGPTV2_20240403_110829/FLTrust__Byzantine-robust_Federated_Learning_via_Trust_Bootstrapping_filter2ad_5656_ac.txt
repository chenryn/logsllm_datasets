2
(cid:107)(cid:98)w − w(cid:107)2,
(cid:41)
(cid:40)
where ∇ represents gradient, (cid:107)·(cid:107) represents (cid:96)2 norm, and (cid:104)·,·(cid:105)
represents inner product of two vectors. Moreover, the empir-
ical loss function f (D, w) is L1-Lipschitz probabilistically.
Formally, for any δ ∈ (0, 1), there exists an L1 such that:
(cid:107)∇f (D, w) − ∇f (D, (cid:98)w)(cid:107)
(cid:107)w − (cid:98)w(cid:107)
Pr
sup
≤ L1
≥ 1 − δ
w,(cid:98)w∈Θ:w(cid:54)=(cid:98)w
3
Assumption 2. The gradient of the empirical loss function
∇f (D, w∗) at
the optimal global model w∗ is bounded.
Moreover, the gradient difference h(D, w) = ∇f (D, w) −
∇f (D, w∗) for any w ∈ Θ is bounded. Speciﬁcally, there
exist positive constants σ1 and γ1 such that
for any unit
vector v, (cid:104)∇f (D, w∗), v(cid:105) is sub-exponential with σ1 and
γ1; and there exist positive constants σ2 and γ2 such that
for any w ∈ Θ with w (cid:54)= w∗ and any unit vector v,
(cid:104)h(D, w) − E [h(D, w)] , v(cid:105) /(cid:107)w − w∗(cid:107) is sub-exponential
with σ2 and γ2. Formally, for ∀|ξ| ≤ 1/γ1, ∀|ξ| ≤ 1/γ2,
we have:
w, (cid:98)w ∈ Θ:
Assumption 3. Each client’s local training dataset Di (i =
1, 2,··· , n) and the root dataset D0 are sampled indepen-
dently from the distribution X .
Theorem 1. Suppose Assumption 1-3 hold and FLTrust uses
Rl = 1 and β = 1. For an arbitrary number of malicious
clients,
the difference between the global model learnt by
FLTrust and the optimal global model w∗ under no attacks
is bounded. Formally, we have the following with probability
at least 1 − δ:
(cid:13)(cid:13)wt − w∗(cid:13)(cid:13) ≤ (1 − ρ)t(cid:13)(cid:13)w0 − w∗(cid:13)(cid:13) + 12α∆1/ρ,
the global model
ρ = 1 − (cid:16)(cid:112)1 − µ2/(4L2) + 24α∆2 + 2αL
(cid:17)
(cid:18)
(cid:113) 2|D0|
learning rate, ∆1 = σ1
(cid:113) 2|D0|
in the tth iteration,
, α is the
(cid:112)d log 6 + log(3/δ), ∆2 =
d log 18L2
σ2
is the size of
σ2
|D0|
the root dataset, d is the dimension
of w, L2 = max{L, L1}, and r is some positive number
such that (cid:107)w − w∗(cid:107) ≤ r
the
parameter space Θ is constrained). When |1 − ρ|  0 is a smoothing parameter.
We optimize e(cid:48)
i one by one following the standard co-
ordinate ascent approach, i.e., when optimizing e(cid:48)
i, all other
j, j (cid:54)= i are ﬁxed. Speciﬁcally, we use projected gradient
e(cid:48)
ascent to iteratively optimize e(cid:48)
i. In the beginning, we initialize
e(cid:48)
i using the Trim attack, i.e., we use the Trim attack to com-
pute the poisoned local model updates and initialize e(cid:48)
i as the
corresponding unit vector. Then, in each iteration, we sample
a random vector u from N (0, σ2I) and compute the gradient
∇e(cid:48)
h following Equation (11). We multiply the gradient by
a step size η and add it to e(cid:48)
i. Finally, we
project e(cid:48)
i is a valid unit
vector. We repeat the gradient ascent process for Q iterations.
Moreover, we repeat the iterations over the unit vectors for
V iterations. Algorithm 3 shows our adaptive attack. We let
i = (cid:107)g0(cid:107) · e(cid:48)
g(cid:48)
i to the unit sphere to ensure that e(cid:48)
i is solved for i = 1, 2,··· , m.
i to get the new e(cid:48)
i after e(cid:48)
i
VI. EVALUATION
max
2,··· ,e(cid:48)
e(cid:48)
1,e(cid:48)
m
h(e(cid:48)
1, e(cid:48)
2,··· , e(cid:48)
m),
(9)
We evaluate our FLTrust against both existing poisoning
attacks to FL and adaptive attacks in this section.
8
A. Experimental Setup
1) Datasets: We use multiple datasets from different do-
mains in our evaluation, including ﬁve image classiﬁcation
datasets and a human activity recognition dataset. We follow