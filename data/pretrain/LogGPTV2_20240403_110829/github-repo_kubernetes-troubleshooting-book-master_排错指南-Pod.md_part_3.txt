如果此时如果还未发现线索，还可以到容器内执行命令来进一步查看退出原因：
``` bash
$ kubectl exec cassandra -- cat /var/log/cassandra/system.log
```
如果还是没有线索，那就需要 SSH 登录该 Pod 所在的 Node 上，查看 Kubelet 或者 Docker 的日志进一步排查了：
``` bash
# Query Node
kubectl get pod  -o wide
# SSH to Node
ssh @
# Query kubelet log
journalctl -u kubelet 
# Query docker log
journalctl -u docker
```
**案例一**：
1. Pod 启动失败，kubectl get pods 显示状态为 CrashLoopBackOff，kubectl describe pods 显示错误信息：read-only file system error
现象：
1. Pod 启动失败，kubectl get pods 显示状态为 CrashLoopBackOff：
    ``` bash
    [root@m7-devops-128071 gitlab]# kubectl get pods --all-namespaces -o wide|grep 128123|grep -v Running|grep pas
    prophet-resource-qa312    pas-7e9f06b0-6f42-4c40-b9a9-eb3471ee824b-predictor-7789d7bjqzdw   0/2       CrashLoopBackOff    5          3m        172.30.37.27     m7-devops-128123
    prophet-resource-qa312    pas-c7917262-fd06-4f98-86bf-96bf3813e5ec-predictor-765c577zp6jl   0/2       CrashLoopBackOff    4          3m        172.30.37.16     m7-devops-128123
    prophet-resource-qa312    pas-d12af6cb-3659-48b3-b2d7-f34fc9536c23-online-dataload-7455d7   0/1       ImagePullBackOff    0          14m       172.30.37.29     m7-devops-128123
    ```
1. kubectl describe pods 显示 mkdir 时返回 read-only file system error：
    ``` bash
    E0813 14:55:53.024865    3777 remote_runtime.go:209] StartContainer "d41d314a85af6eb8c7e5e" from runtime service failed: rpc error:
    code = Unknown desc = failed to start container "d41d314a85af6eb8c7e5ec38ff89": Error response from daemon: OCI runtime create failed:
    container_linux.go:348: starting container process caused
    "process_linux.go:402: container init caused \"rootfs_linux.go:58: mounting \\\"/mnt/disk0/k8s/kubelet/pods/ea9035f3-9ec5-11e8-af9c-0cc47adb93d8/volumes/kubernetes.io~empty-dir/log-entry--predictor--1\\\" to rootfs \\\"/mnt/disk0/docker/data/overlay2/962a78785442dc45bdf95e/merged\\\"
    at \\\"/mnt/disk0/docker/data/overlay2/962a78785442dc45b0f5/merged/collect/predictor/root/predictor/logs/predictor\\\" caused
    \\\"mkdir /mnt/disk0/docker/data/overlay2/962a78785442dc495e/merged/collect/predictor/root/predictor/logs/predictor:
    read-only file system\\\"\"": unknown
    ```
原因：
1. 节点本地 docker image 文件损坏，当使用它启动容器后，容器文件系统错误，进而导致被只读挂载(ro)；
1. 或者，docker pull image 时出错(提示 error pulling image configuration: unknown blob)，导致挂载时文件系统错误；
解决方案：
1. 删除节点上所有使用损坏 image 的容器，然后删除 image，再重新 pull image；
1. 确认 registry 中的 image 文件是否完整，不完整时重新 push image；
**案例二**:
现象：
1. Pod 启动失败，kubectl get pods 显示状态为 CrashLoopBackOff：
    ``` bash
    $ kubectl get pods -n metricbeat  -o wide |grep  m7-power-128050
    metricbeat-995dcffbd-6rppf            0/1       CrashLoopBackOff   1142       9d        172.30.168.11    m7-power-128050
    ```
1. kubectl describe pods 显示 docker 将 pod 的 kubelet 目录 mount 到容器目录中时提示 no such file or directory：
    ``` bash
    $ kubectl describe pods -n metricbeat metricbeat-995dcffbd-6rppf | tail -10
    Warning  Failed                 4d                    kubelet, m7-power-128050  Error: failed to start container "metricbeat": Error response from daemon: OCI runtime create failed: container_linux.go:348: starting container process caused "process_linux.go:402: container init caused \"rootfs_linux.go:58: mounting \\\"/mnt/disk2/k8s/kubelet/pods/54d426d3-cacd-11e8-971a-5e384b278319/volume-subpaths/config/metricbeat/0\\\" to rootfs \\\"/mnt/disk1/docker/data/overlay2/5753fb64509f490968802fe00a6a6e000b7f17f4839d62ba5ca1dc484c86ba22/merged\\\" at \\\"/mnt/disk1/docker/data/overlay2/5753fb64509f490968802fe00a6a6e000b7f17f4839d62ba5ca1dc484c86ba22/merged/etc/metricbeat.yml\\\" caused \\\"no such file or directory\\\"\"": unknown
    Warning  Failed                 4d                    kubelet, m7-power-128050  Error: failed to start container "metricbeat": Error response from daemon: OCI runtime create failed: container_linux.go:348: 
    Normal   Pulled                 43m (x1135 over 9d)   kubelet, m7-power-128050  Container image "docker.elastic.co/beats/metricbeat:6.4.1" already present on machine
    Warning  FailedSync             23m (x26664 over 4d)  kubelet, m7-power-128050  Error syncing pod
    Warning  BackOff                3m (x25616 over 4d)   kubelet, m7-power-128050  Back-off restarting failed container
    ```
原因：容器已经挂了，但是 kubelet 还不知晓，导致 kubelet 将 pod 目录挂载到容器目录时，容器目录不存在。
解决办法：删除 pod，然后自动重建。
## Pod 一直处于 Error 状态
通常处于 Error 状态说明 Pod 启动过程中发生了错误。常见的原因包括：
+ 依赖的 ConfigMap、Secret 或者 PV 等不存在
+ 请求的资源超过了管理员设置的限制，比如超过了 LimitRange 等
+ 违反集群的安全策略，比如违反了 PodSecurityPolicy 等
+ 容器无权操作集群内的资源，比如开启 RBAC 后，需要为 ServiceAccount 配置角色绑定
## Pod 一直处于 Terminating 或 Unknown 状态
正常情况下，如果删除了 Pod，经过一个 grace period（默认 30s）后，如果 Pod 还在 Running，则 kublet 会向 docker 发送 kill 命令，进而 docker 向 Pod 中的所有进程发送 SIGKILL 信号，强行删除 Pod。所以，如果节点工作正常，一般一个 grace period 后，Pod 会被清除。
如果节点失联 NotReady，默认 5min 后，node controller 开始驱逐它上面的 Pods，即将该 Node 上的 Pod 标记为 Terminating 状态，然后在其它节点上再起 Pod。从 v1.5 开始，node controller 不再从 etcd 中强行删除（force delete）失联 Node 上的 Pod 信息，而是等待节点恢复连接后，确认驱逐的 Pod 都已经 Terminating 后才删除这些 Pods。所以，这一段时间内，Pod 可能有多副本运行的情况。想要删除 NotReady 节点上的 Terminating 或 Unknown 状态 Pod 的方法：
+ 从集群中删除该 Node：kubectl delete node 
+ Node 恢复正常。Kubelet 会重新跟 kube-apiserver 通信确认这些 Pod 的期待状态，进而再删除这些 Pod。
+ 用户强制删除。用户可以执行 kubectl delete pods  --grace-period=0 --force 强制删除 Pod。除非明确知道 Pod 的确处于停止状态（比如 Node 所在 VM 或物理机已经关机），否则不建议使用该方法。特别是 StatefulSet 管理的 Pod，强制删除容易导致脑裂或者数据丢失等问题。
处于 Terminating 状态的 Pod 在 Kubelet 恢复正常运行后一般会自动删除。但有时也会出现无法删除的情况，并且通过 kubectl delete pods  --grace-period=0 --force 也无法强制删除。此时一般是由于 finalizers 导致的，通过 kubectl edit 将 finalizers 删除即可解决。
    ``` bash
    "finalizers": [
    "foregroundDeletion"
    ]
    ```
另一种导致删除的 Pod 一直处于 Terminating 状态的原因：Pod 业务容器和 Sandbox 都被正常删除，但是 kubelet 在 umount Pod 挂载的目录时一直失败，提示 device or resource busy，进而导致 Pod 的状态一直是 Terminating：
``` bash
9月 22 20:04:07 ee-test kubelet[3583]: E0922 20:04:07.711666    3583 nestedpendingoperations.go:264] Operation for "\"kubernetes.io/configmap/c74b89dd-be53-11e8-a1f4-525400f721a0-filebeat-config\" (\"c74b89dd-be53-11e8-a1f4-525400f721a0\")" failed. No retries permitted until 2018-09-22 20:04:08.711632504 +0800 CST (durationBeforeRetry 1s). Error: error cleaning subPath mounts for volume "filebeat-config" (UniqueName: "kubernetes.io/configmap/c74b89dd-be53-11e8-a1f4-525400f721a0-filebeat-config") pod "c74b89dd-be53-11e8-a1f4-525400f721a0" (UID: "c74b89dd-be53-11e8-a1f4-525400f721a0") : error deleting /mnt/disk01/k8s/lib/kubelet/pods/c74b89dd-be53-11e8-a1f4-525400f721a0/volume-subpaths/filebeat-config/filebeat/1: remove /mnt/disk01/k8s/lib/kubelet/pods/c74b89dd-be53-11e8-a1f4-525400f721a0/volume-subpaths/filebeat-config/filebeat/1: device or resource busy
```
原因：
1. CentOS 3.10 内核以及 17.12.1 以前版本的 docker 在 umount 时可能会出现 device busy 情况；
解决方案：
1. 升级 docker-ce 到 17.12.1 及以上版本；
1. 升级操作系统内核到 4.4.x;
参考：
+ https://github.com/moby/moby/issues/22260
+ https://github.com/kubernetes/kubernetes/issues/65110
## Pod 状态长时间(小时级别)不更新，一直处于 Creating、Terminating 状态
现象：
1. 创建、删除或重建 Pod 后，等待很长时间(小时级别），kubectl get pods 显示 Pod 的 Status 一直是 Creating、Terminating 状态，进而导致 Service 长时间不可用；
1. 在 Pod 调度到的节点上，执行 docker ps |grep xxx 命令，可以看到 Creating 的 Pod 容器已实际 Running 一段时间了；
原因：
1. kubelet 周期（1s）向 docker 查询各容器的状态、配置参数、image 是否存在、image 版本等信息；
1. kubelet 将查询的结果发给 kube-apiserver，以更新 Pod 的状态，这个发送过程受多个 QPS 参数控制；
1. kubelet 的 QPS 配置参数：  --event-qps 、--kube-api-qps、--event-burst、--kube-api-burst 默认值分别为 5、5、10、10 ；
1. 当节点 Pod 数目过多(200+)，Pod 状态更新时间非常频繁时，kubelet 的默认 QPS 值成为瓶颈，导致 Pod 状态不能及时更新到 kube-apiserver。查看 Pod 的 Status时， 一直处于 Creating、Terminating 状态，进而导致 Service 长时间不可用；
解决办法：
1. 调大 kubelet 的 QPS 配置参数：
    ``` bash
    --event-qps=0
    --kube-api-qps=2000
    --kube-api-burst=4000
    --registry-qps=0
    ```
参考：
1. https://github.com/kubernetes/kubernetes/issues/39113#issuecomment-305919878
## Pod 行为异常
这里所说的行为异常是指 Pod 没有按预期的行为执行，比如没有运行 podSpec 里面设置的命令行参数。这一般是 podSpec yaml 文件内容有误，可以尝试使用 --validate 参数重建容器，比如：
``` bash
kubectl delete pod mypod
kubectl create --validate -f mypod.yaml
```
也可以查看创建后的 podSpec 是否是对的，比如
```bash
kubectl get pod mypod -o yaml
```
## 修改静态 Pod 的 Manifest 后未自动重建
Kubelet 使用 inotify 机制检测 /etc/kubernetes/manifests 目录（可通过 Kubelet 的 --pod-manifest-path 选项指定）中静态 Pod 的变化，并在文件发生变化后重新创建相应的 Pod。但有时也会发生修改静态 Pod 的 Manifest 后未自动创建新 Pod 的情景，此时一个简单的修复方法是重启 Kubelet。
## 参考文档
+ [Troubleshoot Applications](https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/)
+ [Pod 异常排错](https://feisky.gitbooks.io/kubernetes/zh/troubleshooting/pod.html)