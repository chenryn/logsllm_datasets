curity. Speciﬁcally, in most of their experiments δ = 2−16. While there may be
settings where this security parameter suﬃces, it is important to recognize that
δ denotes the statistical probability that a user’s data is completely recovered,
and this value is typically set to 2−40 in MPC research. Finding an application
space where the security relaxation provides signiﬁcant eﬃciency improvements
while also guaranteeing strong security parameters was a major challenge, and
we view it as one of our main contributions.
Chan et al. study diﬀerential obliviousness in the client / server model [6].
They also show asymptotic improvement for several computations, together with
lower bounds for fully secure variants of the same algorithms, demonstrating
that this relaxation allows us to bypass impossibility results. Their results are
purely theoretical, but raise the very interesting question of whether we can
lower-bound the number of AND gates needed in fully secure graph parallel
computation. An initial version of our work did not include Section 5, where we
show how to remove the oblivious shuﬄe, improving the asymptotic complexity
of our solution. While our initial work predates the work of Chan et al., the
asymptotic improvement of Section 5 was concurrent with their work.
M2R [10] and Ohrimenko et al. [30] consider secure implementation of MapRe-
duce jobs on untrusted cloud servers, where the adversary has access to the
network and storage back-end, and can observe all encrypted traﬃc between
Map and Reduce nodes, but cannot corrupt those nodes; they assume secure
hardware, such as SGX. They hide ﬂow between map and reduce operations by
shuﬄing the data produced by the Mappers in secure hardware before sending
it to the Reducers. They do not use any notion of diﬀerential privacy. Finally,
Airavat [34] protects the output of the map-reduce computation by adding ex-
ponentially distributed noise to the output of computation.
2 Notation and Deﬁnitions
Secret-Shares: We let hxi denote a variable which is XOR secret-shared be-
tween parties. Arrays have a public length and are accessed via public in-
dices; we use hxii to specify element i within a shared array, and hxii:j to
indicate a speciﬁc portion of the array containing elements i through j, inclu-
sive. When we write hxi ← c, we mean that both users should ﬁx their shares
of x (using some agreed upon manner) to ensure that x = c. For example, one
party might set his share to be c while the other sets his share to 0.
6
We use |D| in the natural way to meanP|V |
set of all multi-sets over V of size i, and DB =S
these multi-sets in the natural way: |D1 − D2| =P|V |
Multi-Sets: We represent multi-sets over a set V by a |V | dimensional vector of
natural numbers: D ∈ N|V |. We refer to the ith element of this vector by D(i).
i=1 D(i). We use DBi to denote the
i DBi. We deﬁne a metric on
i=1 |D1(i) − D2(i)|. We say
two multi-sets are neighboring if they have distance at most 1: |D1 − D2| ≤ 1.
Neighboring Graphs: In our main protocol of Section 4, the input is a data-
augmented directed graph, denoted by G = (V, E), with user-deﬁned data on
each edge. We need to deﬁne a metric on these input graphs, in order to claim
security for graphs of bounded distance.2 For each v ∈ V , we let in-deg(v)
and out-deg(v) denote the in-degree and out-degree of node v. We deﬁne the in-
degree proﬁle of a graph G as the multi-set Din(G) = {in-deg(v1), . . . , in-deg(vn)}.
Intuitively, this is a multi-set over the node identiﬁers from the input graph, with
vertex identiﬁer v appearing k times if in-deg(v) = k. We deﬁne the full-degree
proﬁle of G as the pair of multi-sets: {Din(G), Dout(G)}, where
Dout(G) = {out-deg(v1), . . . , out-deg(vn)}. We now deﬁne two diﬀerent metrics
on graphs, using these degree proﬁles. Later in this section, we provide two
diﬀerent security deﬁnitions: we rely on the ﬁrst distance metric below when
claiming security as deﬁned in Deﬁnition 5, and rely on the second metric below
when claiming security as deﬁned in Deﬁnition 6.
Deﬁnition 1 We say two graphs G and G0 have distance at most d if they have
in-degree proﬁles of distance at most d: |Din(G) − Din(G0)| ≤ d. We say that G
and G0 are neighboring if they have distance 1.
Deﬁnition 2 We say two graphs G and G0 have full-degree proﬁles of distance d
if the sum of the distances in their in-degree proﬁles and their out-degree proﬁles
is at most d: |Din(G)−Din(G0)|+|Dout(G)−Dout(G0)| ≤ d. We say that G and G0
have neighboring full-degree proﬁles if they have full-degree proﬁles of distance
2.
2.1 Diﬀerential Privacy
We use the deﬁnition that appears in [13].
Deﬁnition 3 A randomized algorithm L : DB → RL, with an input domain DB
that is the set of all multi-sets over some ﬁxed set V , and output RL ⊂ {0, 1}∗,
is (, δ)-diﬀerentially private if for all T ⊆ RL and ∀D1, D2 ∈ DB such that
|D1 − D2| ≤ 1:
Pr[L(D1) ∈ T] ≤ e Pr[L(D2) ∈ T] + δ
where the probability space is over the coin ﬂips of the mechanism L.
2In Section 3, the input to the computation is a multi-set of elements drawn from some
set S, rather than a graph, so we use the simple distance metric described above to deﬁne the
distance between inputs.
7
The above deﬁnition describes diﬀerential privacy for neighboring multi-sets.
Letting G denote the set of all graphs, we deﬁne it for neighboring graphs as
well:
Deﬁnition 4 A randomized algorithm L : G → RL is (, δ)-edge private if for
all neighboring graphs, G1, G2 ∈ G, we have:
Pr[L(G1) ∈ T] ≤ e Pr[L(G2) ∈ T] + δ
2.2 Secure computation with diﬀerentially private access
patterns
Input model: We try to keep the deﬁnitions general, as we expect they will
ﬁnd application beyond the space of graph-structured data. However, we use
notation that is suggestive of computation on graphs, in order to keep our
notation consistent with the later sections. We assume that two computation
servers have been entrusted to compute on behalf of a large set of users, V, with
|V| = n, and having sequential identiﬁers, 1, . . . , n. Each user i contributes data
vi. They might each entrust their data to one of the two servers (we call this
the disjoint collection setting), or they might each secret-share their input with
the two-servers (joint collection setting). In the latter case, we note that both
servers learn the size of each vi but neither learns the input values; in the former
case, each server learns a subset of the input values, but learns nothing about
the remaining input values (other than the sum of their sizes).3 Below we will
deﬁne two variant security notions that capture these two scenarios.
In all computations that we consider in our constructions, the input is rep-
resented by a graph. In every case, each user is represented as a node in this
graph, and each user input is a set of weighted, directed edges that originate at
their node. In some applications, the graph is bipartite, with user nodes on the
left, and some distinct set of item nodes on the right: in this case, all edges go
from user nodes to item nodes. In other applications, there are only user nodes,
and every edge is from one user to another. In the joint collection setting, we
can leak the out-degree of each node, which is the same as the user input size,
but must hide (among other things) the in-degree of each node. In the disjoint
collection setting, the protocol must hide both the in-degree and out-degree of
each node. We note that in some applications, such as when we perform gradi-
ent descent, the graph is bipartite, and it is publicly known that the in-degree of
every user is 0 (i.e. the movies don’t review the viewers). In the joint collection
setting, this knowledge allows for some improvement in eﬃciency that we will
leverage in Section 6.
3We note that the disjoint collection setting corresponds to the “standard” setting for
secure computation where each computing party contributes one set of inputs. Just as in
that setting, each of the two computing parties could pad their inputs to some maximum size,
hiding even the sum of the user input sizes. In fact, we could have them pad their inputs
using a randomized mechanism that preserves diﬀerential privacy, possibly leading to smaller
padding sizes, depending on what the maximum and average input sizes are. We don’t explore
this option further in this work.
8
Deﬁning secure computation with leakage: For simplicity, we start with
a standard deﬁnition of semi-honest security4, but make two important changes.
The ﬁrst change is that we allow certain leakage in the ideal world, in order to
reﬂect what is learned by the adversary in the real world through the observed
access pattern on memory. The leakage function is a randomized function of
the inputs. The second change is an additional requirement that this leak-
age function be proven to preserve the diﬀerential privacy for the users that
contribute data. Our ideal world experiment is as follows. There are two par-
ties, P1 and P2, and an adversary S that corrupts one of them. The parties
are given input, as described above; we use V1 and V2 to denote the inputs
of the computing parties, regardless of whether we are in the joint collection
setting or the disjoint collection setting, and we let V = {v1, . . . , vn} denote
the user input. Technically, in the joint collection setting, V = V1 ⊕ V2, while
in the disjoint collection setting, V = V1 ∪ V2. Each computing party submits
their input to the ideal functionality, unchanged. The ideal functionality recon-
structs the n user inputs, v1, . . . , vn, either by taking the union of the inputs
submitted by the computation servers in the disjoint collection setting, or by
reconstructing the input set from the provided secret shares in the joint col-
lection setting. The ideal functionality then outputs f1(v1, . . . , vn) to P1 and
f2(v1, . . . , vn) to P2. These outputs might be correlated, and, in particular, in
our own use-cases, each party receives a secret share of a single function evalua-
tion: hf(v1, . . . , vn)i1,hf(v1, . . . , vn)i2. The ideal functionality also applies some
P
leakage function to the data, L(V ), and provides the output of L(V ), along with
i∈V |vi| to S.5 Additionally, depending on the choice of security deﬁnition,
the ideal functionality might or might not give the simulator, ∀i ∈ V, |vi|.
Our protocols are described in a hybrid world, in which the parties are given
access to several secure, ideal functionalities. In our implementation, these are
replaced using generic constructions of secure computation (i.e. garbled circuits).
Relying on a classic result of Canetti [2], when proving security, it suﬃces to
treat these as calls to a trusted functionality. In the deﬁnitions that follow, we
let G denote an appropriate collection of ideal functionalities.
As is conventionally done in the literature on secure computation, we let
hybridG
π,A(z) (V1, V2, κ) denote a joint distribution over the output of the hon-
est party and, the view of the adversary A with auxiliary input z ∈ {0, 1}∗,
when the parties interact in the hybrid protocol πG on inputs V1 and V2, each
held by one of the two parties, and computational security parameter κ. We let
idealF ,S(z,L(V ),∀i∈V :|vi|)(V1, V2, κ) denote the joint distribution over the out-
put of the honest party, and the view output by the simulator S with auxil-
iary input z ∈ {0, 1}∗, when the parties interact with an ideal functionality
F on inputs V1 and V2, each submitted by one of the two parties, and secu-
4We stress that our allowance of diﬀerentially private leakage brings gains in the circuit
construction, so we could use any generic secure computation of Boolean circuits, including
those that are maliciously secure, and beneﬁt from the same gains. See more details below.
5In the joint collection setting, the simulator can infer this value from the size of the
input that was submitted to the ideal functionality. But it simpliﬁes things to give it to him
explicitly.
9
rity parameters κ. Letting v = P
i∈V |vi|), we deﬁne the joint distribution
idealF ,S(z,L(V ),v)(V1, V2, κ) in a similar way, the only diﬀerence being that the
simulator is given the sum of the input sizes and not the value of each input
size.
Deﬁnition 5 Let F be some functionality, and let π be a two-party protocol
for computing F, while making calls to an ideal functionality G. π is said to
securely compute F in the G-hybrid model with L leakage, known input sizes,
and (κ, , δ)-security if L is (, δ)-diﬀerentially private, and, for every PPT,
semi-honest, non-uniform adversary A corrupting a party in the G-hybrid model,
there exists a PPT, non-uniform adversary S corrupting the same party in the
ideal model, such that, on any valid inputs V1 and V2
π,A(z) (V1, V2, κ)o
nhybridG
F ,S(z,L(V ),∀i∈V :|vi|)(V1, V2, κ)o
nideal(1)
c≡
z∈{0,1}∗,κ∈N
z∈{0,1}∗,κ∈N
(1)
The above deﬁnition is the one that we use in our implementations. However,
in Section 4 we also describe a modiﬁed protocol that achieves the stronger
security deﬁnition that follows, where the adversary does not learn the sizes of
individual inputs. This property might be desirable (or maybe even essential)
in the disjoint collection model, where users have not entrusted one of the two
computing parties with their inputs, or even the sizes of their inputs. On the
other hand, the previous deﬁnition is, in some sense, more “typical” of deﬁnitions
in cryptography, where we assume that inputs sizes are leaked. It is only in this
model where data is outsourced that we can hope to hide each individual input
size among the other inputs.
Deﬁnition 6 Let F be some functionality, and let π be a two-party protocol
for computing F, while making calls to an ideal functionality G. π is said
to securely compute F in the G-hybrid model with L leakage, and (κ, , δ)-
security if L is (, δ)-diﬀerentially private, and, for every PPT, semi-honest,
non-uniform adversary A corrupting a party in the G-hybrid model, there exists
a PPT, non-uniform adversary S corrupting the same party in the ideal model,
such that, on any valid inputs V1 and V2
nhybridG
F ,S(z,L(V ),P
(cid:26)
ideal(2)
π,A(z) (V1, V2, κ)o
c≡
z∈{0,1}∗,κ∈N
(cid:27)
|vi|)(V1, V2, κ)
i∈V
z∈{0,1}∗,κ∈N
(2)
Diﬀerentially Private Output: As is typical in secure computation, we are
concerned here with how to securely compute some agreed upon function, rather
than what function ought to be computed. In other words, we view the question
of what the output itself might reveal about the input to be beyond scope of our
work. Our concern is only that the process of computing the output should not
10
reveal too much. Nevertheless, one could ask that the output of all computations
also be made to preserve diﬀerential privacy. Interestingly, for the speciﬁc case of
histograms, which we present as an example in Section 3, adding diﬀerentially
private noise to the output is substantially more eﬃcient than preserving an
exact count. This is not true for the general protocol, but the cost of adding
noise for these cases has been studied elsewhere [32], and it would be minor
compared to the rest of the protocol.