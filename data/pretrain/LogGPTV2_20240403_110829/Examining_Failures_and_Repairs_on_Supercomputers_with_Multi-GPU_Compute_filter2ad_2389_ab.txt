### DOI 10.1109/DSN48987.2021.00043

**Authorized licensed use limited to: Tsinghua University. Downloaded on October 11, 2021 at 09:22:59 UTC from IEEE Xplore. Restrictions apply.**

---

### Node Architecture of Tsubame-2 and Tsubame-3

**Figure 1.** Tsubame-2 and Tsubame-3 node architecture.

(a) Tsubame-2  
(b) Tsubame-3

**Table I.** Tsubame-2 and Tsubame-3 node configurations.

| Component              | Tsubame-2                          | Tsubame-3                            |
|------------------------|------------------------------------|--------------------------------------|
| **CPU**                | Intel Xeon X5670 (Westmere-EP, 2.93GHz) | Intel Xeon E5-2680 V4 (Broadwell-EP, 2.4GHz) |
| **Cores/Threads per CPU** | 6 cores / 12 threads             | 14 cores / 28 threads               |
| **Num CPUs**           | 2                                  | 2                                    |
| **Memory per Node**    | 58GB                               | 256GB                                |
| **GPU**                | NVIDIA Tesla K20X (GK110)          | NVIDIA Tesla P100 (NVlink-Optimized) |
| **Num GPUs**           | 3                                  | 4                                    |
| **SSD**                | 120 GB                             | 2TB                                  |
| **Interconnect**       | 4X QDR InfiniBand - 2 ports        | Intel Omni-Path HFI 100Gbps - 4 ports |

The node structure of Tsubame-2 was designed with three GPUs per node, while Tsubame-3 has four GPUs per node (refer to Figure 1). Table I provides a high-level overview of the node specifications for both Tsubame-2 and Tsubame-3 [16].

### Dataset

In this paper, we focus on the failures reported on Tsubame-2 and Tsubame-3. We used two failure logs from the Tsubame supercomputers:
1. Tsubame-2 failure log with 897 failures, covering the period from January 7, 2012, to August 1, 2013.
2. Tsubame-3 failure log with 338 failures, covering the period from May 9, 2017, to February 22, 2020.

For each failure, the log includes the time of failure occurrence, the time to recovery from failure, and the category of failure. Table II lists the categories of failures reported in the logs. Our goal is to compare our findings on two generations of supercomputers. In this work, a failure is defined as an error that crashes the application (see Table II for a list of error types). These crashes can be fixed by rebooting or replacing the hardware, or updating the system software.

**Table II.** Tsubame-2 and Tsubame-3 failure categories.

| Tsubame-2 Failure Categories            | Tsubame-3 Failure Categories          |
|-----------------------------------------|---------------------------------------|
| Boot, CPU, Disk, Down, FAN, GPU, IB, Memory, Network, OtherHW, OtherSW, PBS, PSU, Rack, SSD, System Board, VM | CPU, CRC, Disk, GPU, GPUDriver, IP, Led Front Panel, Lustre, Memory, Omni-Path, Power-Board, Ribbon Cable, Software, SXM2 Cable, SXM2-Board, Unknown |

**Figure 2.** Failure categories on Tsubame-2 and Tsubame-3.

(a) Tsubame-2 Failure Categories  
(b) Tsubame-3 Failure Categories

### Observations and Limitations

Hardware failures (e.g., GPU, CPU, SSD) are often localized to the component itself, making it easier to determine the root cause. However, accurately determining the root cause for software-related failures is more challenging. The effects of particular applications and the impact of environmental factors (e.g., temperature and humidity) are not discussed due to business sensitivity and limited availability of information. Generally, no particular application experienced noticeably more failures than its proportional share of computational resource usage.

### Investigating Failure Characteristics and Their Implications

We begin our analysis by performing a high-level examination of the characteristics of the failure categories and GPU failures on Tsubame-2 and Tsubame-3. Specifically, we ask the following questions:

**RQ1: What is the distribution of most frequently occurring failure types? And, are they the same on both systems?**

**Figure 2** shows the breakdown of failures for each reported category on Tsubame-2 (Figure 2(a)) and Tsubame-3 (Figure 2(b)). Several interesting trends are revealed:
1. A few failure types dominate on both supercomputers (e.g., GPU, fan, network, software), but the dominant failure types differ between the systems.
2. GPU failures are significantly higher in number than CPU failures on both systems. On Tsubame-2, 44.37% of the failures are incident on the GPUs, while only 1.78% are caused by or happen on CPUs. On Tsubame-3, approximately 28% of the failures are categorized as GPU failures, while only 3.25% are CPU failures.

The higher rate of GPU failures is attributed to two phenomena:
1. Applications are increasingly spending more runtime on GPUs compared to CPUs [17], [18].
2. Unlike CPUs, GPUs lack sophisticated error and failure mitigation and correction techniques [19], [20].

Recent progress has been made in making GPUs more resilient from an architectural and design perspective [3]–[5], and in developing software solutions (e.g., checkpointing) to mitigate GPU errors [21]–[23]. However, there is a significant opportunity for academia to continue investing in this area and develop better software failure mitigation methodologies.

A significant difference between Figures 2(a) and 2(b) is that on Tsubame-2, the GPU category of failures has the highest occurrence rate (44.37%), while on Tsubame-3, the software category has the highest share of failure (50.59%), with GPU coming second (27.81%). This increased rate of software failures from Tsubame-2 to Tsubame-3 suggests that these failures may be caused by the introduction of new artificial intelligence (AI) and machine learning (ML) applications.

**Figure 3.** Breakdown of Tsubame-3 software failures.

Approximately 43% of software failures are "GPU Driver-related Problems," resulting from frequent GPU driver updates, software-driver mismatches, and applications being run with incorrect CUDA versions. For example, on Tsubame-3, the OmniPath driver was associated with GPU software failures. Fortunately, these failures generally occur at the beginning of an application run and do not result in wasted runtime.

**Summary:**
Our analysis shows that while GPU hardware has matured over time, approximately 28% of failures are still GPU hardware failures. Higher usage of GPUs results in more software errors due to the GPU software stack being not well-developed. While hardware improvements are effective, they are expensive and not needed for all market sectors. There is an opportunity to develop a more resilient accelerator software stack for HPC applications.

**Figure 3** reveals further insights:
1. A significant fraction of software failures (approximately 20%) have no known cause and cannot be classified. This poses a significant challenge for operations where software failures cannot be diagnosed.
2. More academic effort is needed to identify non-reproducible bugs and their root causes.
3. Better mitigation techniques for GPU driver-related bugs are required.

**RQ2: Are some nodes experiencing more failures than others on the Tsubame systems? If so, are these faulty nodes contributing to the majority of the failures on these HPC systems?**

To answer these questions, we quantified the failure counts on each node to characterize how many failures an individual node has experienced.

**Figure 4.** Failure distribution on nodes.

(a) Tsubame-2  
(b) Tsubame-3

On Tsubame-2, approximately 60% of the nodes experienced only one failure. Comparatively, on Tsubame-3, approximately 60% of the nodes experienced more than one failure. More nodes experienced two or more failures on Tsubame-3 compared to Tsubame-2.

In terms of nodes with more than one failure, on both Tsubame systems, approximately 10% of nodes experienced two failures. However, the percentage of nodes that experienced three failures on Tsubame-3 is approximately 50% more than on Tsubame-2. This is likely because each Tsubame-3 node has one additional GPU compared to each Tsubame-2 node. This shows that increasing the number of GPUs per node increases the probability that a node experiences recurrent failures.

Considering nodes with more than one failure, on Tsubame-2, we observed 352 hardware failures and 1 software failure, and on Tsubame-3, we observed 104 hardware and 95 software failures. Thus, both hardware and software failures can occur multiple times on a node.

To investigate further, we inquire how the failures are spatially distributed within a node. Since GPU failures are more dominant, we focus on the GPU failure distribution within a single node (nomenclature of GPU 0, GPU 1, GPU 2, and GPU 3 is the same as shown in Figure 1).

**Figure 5.** GPU failure distribution.

(a) Tsubame-2  
(b) Tsubame-3

Based on Figure 5(a), GPU 1 has experienced approximately 20% more failures than GPU 0 and GPU 2 per node on average. On Tsubame-3, GPU 0 and GPU 3 have experienced considerably more failures than GPU 1 and GPU 2 (Figure 5(b)). Therefore, the failure distributions among different GPUs are non-identical. While it has been difficult to pinpoint the exact reason for this behavior, several factors could be at play, including higher utilization of some GPUs, manufacturing variability, and different distribution of hardware faults.

An important implication of this finding is that HPC centers should inform and help end-users to take advantage of all the GPUs in a node in a load-balanced manner. Additionally, the operations staff could mitigate this by periodically rearranging the GPUs during maintenance.