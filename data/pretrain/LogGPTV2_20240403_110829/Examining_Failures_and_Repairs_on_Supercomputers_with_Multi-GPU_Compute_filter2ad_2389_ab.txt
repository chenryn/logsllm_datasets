DOI 10.1109/DSN48987.2021.00043
305
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:22:59 UTC from IEEE Xplore.  Restrictions apply. 
(cid:38)(cid:51)(cid:56)(cid:3)(cid:19)
(cid:38)(cid:51)(cid:56)(cid:3)(cid:20)
(cid:38)(cid:51)(cid:56)(cid:3)(cid:19)
(cid:38)(cid:51)(cid:56)(cid:3)(cid:20)
(cid:55)(cid:92)(cid:79)(cid:72)(cid:85)(cid:86)(cid:69)(cid:88)(cid:85)(cid:74)
(cid:42)(cid:51)(cid:56)(cid:3)(cid:19)
(cid:42)(cid:51)(cid:56)(cid:3)(cid:21)
(cid:42)(cid:51)(cid:56)(cid:3)(cid:19)
(cid:42)(cid:51)(cid:56)(cid:3)(cid:20)
(cid:42)(cid:51)(cid:56)(cid:3)(cid:21)
(cid:52)(cid:88)(cid:76)(cid:70)(cid:78)(cid:51)(cid:68)(cid:87)(cid:75)(cid:3)(cid:44)(cid:81)(cid:87)(cid:72)(cid:85)(cid:70)(cid:82)(cid:81)(cid:81)
(cid:52)(cid:88)(cid:76)(cid:70)(cid:78)(cid:51)(cid:68)(cid:87)(cid:75)(cid:3)(cid:44)(cid:81)(cid:87)(cid:72)(cid:85)(cid:70)(cid:82)(cid:81)(cid:81)(cid:72)(cid:70)(cid:87)
(cid:51)(cid:38)(cid:44)(cid:72)
(cid:42)(cid:51)(cid:56)(cid:3)(cid:20)
(cid:42)(cid:51)(cid:56)(cid:3)(cid:22)
(cid:52)(cid:88)(cid:76)(cid:70)(cid:78)(cid:51)(cid:68)(cid:87)(cid:75)(cid:3)(cid:44)(cid:81)(cid:87)(cid:72)(cid:85)(cid:70)(cid:82)(cid:81)(cid:81)
(cid:52)(cid:88)(cid:76)(cid:70)(cid:78)(cid:51)(cid:68)(cid:87)(cid:75)(cid:3)(cid:44)(cid:81)(cid:87)(cid:72)(cid:85)(cid:70)(cid:82)(cid:81)(cid:81)(cid:72)(cid:70)(cid:87)
(cid:49)(cid:57)(cid:47)(cid:76)(cid:81)(cid:78)
(cid:51)(cid:38)(cid:44)(cid:72)
(a) Tsubame-2
(b) Tsubame-3
Fig. 1. Tsubame-2 and Tsubame-3 node architecture.
TABLE I. Tsubame-2 and Tsubame-3 node conﬁgurations.
CPU
Cores/Threads
per CPU
Num CPUs
Memory per
Node
GPU
Num GPUs
SSD
Interconnect
Tsubame-2
Tsubame-3
Intel Xeon X5670
(Westmere-EP, 2.93GHz)
6 cores / 12 threads
Intel Xeon E5-2680 V4
(Broadwell-EP, 2.4GHz)
14 cores / 28 threads
2
58GB
2
256GB
NVIDIA Tesla K20X
(GK110)
3
120 GB
4X QDR InﬁniBand - 2
ports
NVIDIA Tesla P100
(NVlink-Optimized)
4
2TB
Intel Omni-Path HFI
100Gbps - 4 ports
node structure, Tsubame-2 was designed with three GPUs
per node, while Tsubame-3 has four GPUs per node (refer
to Figure 1). Table I provides a high-level overview of the
node speciﬁcation of Tsubame-2, and Tsubame-3 [16].
Dataset.
In this paper, we focus on the failures that are
reported on Tsubame-2 and Tsubame-3. We used two failure
logs from the Tsubame supercomputers: (i) Tsubame-2 failure
log with 897 failures, and (ii) Tsubame-3 failure log with 338
failures. Tsubame-2 failure log includes the period between
1/7/2012, and 8/1/2013. The failure log on Tsubame-3 includes
the period between 05/09/2017, and 02/22/2020. For each
failure, the log includes the time of failure occurrence, the time
to recovery from failure, and the category of failure. Table II
lists the categories of failures reported in the logs. We focused
on Tsubame-2 and Tsubame-3 with the goal of comparing our
ﬁndings on two generations of supercomputers. In this work,
we deﬁne a failure as an error that crashes the application
(Table II provides a list of error types). These crashes can be
ﬁxed by rebooting or replacing the hardware, or updating the
system software.
TABLE II. Tsubame-2 and Tsubame-3 failure categories.
Tsubame-2
Boot, CPU, Disk, Down, FAN,
GPU, (Inﬁniband) IB, Memory,
Network, OtherHW, OtherSW,
Portable Batch System (PBS),
Power Supply Unit (PSU), Rack,
SSD, System Board, and Virtual
Machine (VM)
Tsubame-3
CPU, Cyclic Redundancy Check
(CRC), Disk, GPU, GPUDriver,
IP Motherboard (IP), Led Front
Panel, Lustre, Memory,
Omni-Path, Power-Board, Ribbon
Cable, Software, SXM2 Cable,
SXM2-Board, and Unknown
(a) Tsubame-2 Failure Categories
(b) Tsubame-3 Failure Categories
Fig. 2. GPU failures are the most frequent on Tsubame-2,
while software failures are the most common on Tsubame-3.
are observable or detectable. The root cause for hardware
failures (e.g., GPU, CPU, SSD) is often localized to the com-
ponent itself. However, accurate determination of root cause
for software-related failures is more challenging. The effects
of particular applications, and the impact of environmental
factors (e.g., temperature and humidity) is not discussed due to
business-sensitivity and limited availability of the information
(e.g., information not persisted for long-term due to storage
overhead). In general, we did not ﬁnd any particular applica-
tion experiencing noticeably more failures than its proportional
share of computational resource usage.
III. INVESTIGATING FAILURE CHARACTERISTICS AND
THEIR IMPLICATIONS
We begin our analysis by performing a high-level examina-
tion of the characteristics of the failure categories and GPU
failures on Tsubame-2 and Tsubame-3. In particular, we ask
the following questions.
Limitations and Scope. We acknowledge that due to logging
capability and business sensitivity of the study,
this work
has limitations. However, we ensure that these limitations are
taken into account when drawing conclusions. For example,
we do not focus on the root cause analysis for each failure
because often there is a combination of root causes responsible
for failures. Many times, not all of the contributing factors
RQ1: What is the distribution of most frequently occurring
failure types? And, are they the same on both systems?
Figure 2 shows the breakdown of failures for each reported
category on Tsubame-2 (2(a)), and Tsubame-3 (2(b)). These
results reveal several interesting trends. First, a few failure
types dominate on both the supercomputers (e.g., GPU, fan,
network, software), but the dominant failure types are different
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:22:59 UTC from IEEE Xplore.  Restrictions apply. 
306
on both the systems. Second, GPU failures are signiﬁcantly
higher in number than CPU failures on both the systems.
As Figure 2(a) shows, 44.37% of the failures are incident
on the GPUs on Tsubame-2. In contrast, only 1.78% of the
failures are caused by or happen on CPUs. Figure 2(b) shows
that on Tsubame-3, ∼28% of the failures are categorized
as GPU failures, while only 3.25% of the failures are CPU
failures. Even though prior works have noted GPUs being
one of the major factors for failures [8], [15],
the mag-
nitude of the difference in failure rate between GPUs and
CPUs is concerning. The higher rate of GPU failures is a
result of two phenomena: (1) Increasingly, applications are
spending a considerable amount of their runtimes on GPUs
compared to CPUs [17], [18], and (2) Unlike CPUs, GPUs
lack sophisticated error and failure mitigation and correction
techniques [19], [20]. There has been a lot of progress recently
in terms of both (a) making the GPUs more resilient from
an architecture and design point of view [3]–[5], and (b)
developing software solutions (e.g., checkpointing) to mitigate
the GPU errors [21]–[23]. However,
there is a signiﬁcant
opportunity for academia to continue investing more effort
in this area and develop better software failure mitigation
methodologies.
One signiﬁcant difference between Figures 2(a) and 2(b)
is that on Tsubame-2, the GPU category of failures has the
highest occurrence rate (44.37%); however, on Tsubame-3, the
software category has the highest share of failure (50.59%),
and GPU comes second (27.81%). This increased rate of
software failures from Tsubame-2 to Tsubame-3 points to
these failures being potentially caused by the introduction of
new artiﬁcial intelligence (AI) and machine learning (ML) ap-
plications. We dig deeper to ﬁnd out the cause of this behavior
by breaking down the software errors into the 171 reported
root loci. Figure 3 shows the top 16 causes of software failures.
We observed an interesting trend: ∼43% of software failures
are “GPU Driver-related Problems”. This is a result of frequent
GPU driver updates/upgrades, software-driver mismatch, and
applications being run with incorrect CUDA versions. For
example, on Tsubame-3, the OmniPath driver was associated
with GPU software failures. Also, because NVIDIA supported
InﬁniBand before it added support for OmniPath, GPU Direct
also caused problems. Fortunately,
these failures generally
occur at the beginning of an application run and do not result
in wasted runtime.
Summary. Our analysis shows that while GPU hardware
has matured over time, still ∼28% of failures are GPU
hardware failures. Furthermore, higher usage of GPUs
results in more software errors that are a result of the
GPU software stack being not well-developed. While
hardware improvements are on the rise and have been
effective,
they are expensive and not needed for all
market sectors. There is an opportunity to develop more
resilient accelerator software stack for HPC applications.
Figure 3 reveals further interesting insights. First, we ob-
serve that a signiﬁcant fraction of software failures (approx.
Fig. 3. Tsubame-3 software failures break down shows that
most failure are GPU-driver-related.
(a) Tsubame-2
(b) Tsubame-3
Fig. 4. On Tsubame-2, most nodes encounter only one failure.
This is not the case for Tsubame-3.
20%) have no known cause and cannot be classiﬁed. This
is an increasing problem and poses a signiﬁcant challenge
for operations where software failures cannot be diagnosed
and the root-cause is not known. More academic effort is
needed to identify non-reproducible bugs and their root causes.
Second, this result also reveals that we need more effort in
developing better mitigation techniques for GPU driver-related
bugs. Finally, we note that, unlike previous works [9], [15],
kernel panics and lustre bugs are relatively low – this is a
testament to the years of efforts in making operating systems
and lustre ﬁle systems hardened. Given some failure types
being dominant, we ask whether certain nodes are affected
more than others (i.e.,
they encounter more failures than
others). More speciﬁcally:
RQ2: Are some nodes experiencing more failures than
others on the Tsubame systems? If so, are these faulty
nodes contributing to the majority of the failures on these
HPC systems?
To answer these questions, we quantiﬁed the failure counts
on each node to characterize how many failures an individual
node has experienced. Figure 4 shows the results for Tsubame-
2 (4(a)), and Tsubame-3 (4(b)). On Tsubame-2, ∼60% of
the nodes experienced only one failure. Comparatively, on
Tsubame-3, ∼60% of the nodes experienced more than one
failure. More nodes experienced two or more failures on
Tsubame-3 compared to Tsubame-2.
In terms of nodes with more than one failure, on both
Tsubame systems, ∼10% of nodes experienced two failures.
Nonetheless, the percentage of nodes that experienced three
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:22:59 UTC from IEEE Xplore.  Restrictions apply. 
307
(a) Tsubame-2
(b) Tsubame-3
Fig. 5. Different GPUs attached to a node experience different
number of failures on both Tsubame systems.
failures on Tsubame-3 is ∼50% more than Tsubame-2. This
is likely because each Tsubame-3 node has one additional
GPU compared to each Tsubame-2 node. This shows that by
increasing the number of GPUs per node in the system, the
probability that a node experiences recurrent failures increases.
Furthermore, considering nodes with more than 1 failure, on
Tsubame-2, we observed 352 hardware failures and 1 software
failure, and on Tsubame-3, we observed 104 hardware and 95
software failures. Thus, both hardware and software failures
can occur multiple times on a node.
To investigate further, we inquire how the failures are
spatially distributed within a node. Recall that each node
has multiple GPUs and CPUs with different topologies on both
supercomputers. Since GPU failures are more dominant, we
focus on the GPU failures distribution within a single node
(nomenclature of GPU 0, GPU 1, GPU 2, and GPU 3 is the
same as shown in Fig. 1).
Figure 5 shows the failure distribution on GPUs. Based on
Figure 5(a), GPU 1 has experienced ∼20% more failures than
GPU 0 and GPU 2 per node on average. On Tsubame-3, GPU
0 and GPU 3 have experienced considerably more failures
than GPU 1 and GPU 2 (Figure 5(b)). Therefore, we can
conclude that the failure distributions among different GPUs
are non-identical. While it has been difﬁcult to pinpoint the
exact reason for this behavior, several factors can be at play,
including higher utilization of some GPUs as compared to
others, manufacturing variability, and different distribution of
hardware faults. An important implication of this ﬁnding is
that HPC centers should inform and help end-users to take
advantage of all
the GPUs in a node in a load-balanced
manner. Second, the operations staff could also mitigate this
by rearranging the GPUs periodically during maintenance.