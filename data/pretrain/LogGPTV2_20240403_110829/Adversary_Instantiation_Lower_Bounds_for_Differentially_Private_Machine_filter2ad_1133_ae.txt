Since the Crafter chooses the parameters with smallest
changes, we assume the gradient of these points are zero. This
lets us simplify the problem to ﬁnding out if the distance
between the parameters are coming from a Gaussian with
mean zero or with mean equal to the sum of the distances
between the watermarked parameters. Now, given the distance
between the watermarked parameters, we can compute the
likelihood of it coming from the watermarked gradient as:
(x − C)2
p(Distance = d|watermark) =
)
(6)
where 2n is the number of selected parameters, C and σ are
the clipping norm and the scale used to clip and noise gradients
in DP-SGD, and d is the distance computed by Distinguisher 3
d) Results: From Figure 7, we derive that having direct
access to the model gradients further improves the strength of
the adversary and establishes an improved lower bound. For
small values of ε = 1, the attack can achieve empirical lower
bound ε = 0.3, this attack is almost tight,however, the gap
increases as the value of epsilon increases.
F. Malicious Datasets
1
πnσ2
exp−(
C√
2n
√
2
, σ2)
2σ2
model parameters which leave the model’s performance on
training data largely unaffected. The Crafter modiﬁes model
parameters whose gradients are smallest in magnitude: intu-
itively, these model parameters are not updated much during
training so they are good candidates for being modiﬁed. In
particular for the ﬁrst to iterations of training, the adversary
observes the model parameters’ gradient and after to iterations,
it selects 2n parameters which have the smallest sum of
gradient absolute values. Crafter 4 outlines this procedure.
b) Model training: The model randomly selects to train
on D or D(cid:48). If the trainer selects D(cid:48), the trainer calls Crafter 4
In all of the previous attacks, we used a standard training
dataset D, and the attacker would create a malicious training
instance to add to this dataset. However, the DP-SGD analysis
holds not just for a worst-case gradient from some typical
dataset (e.g., MNIST/CIFAR-10), but also when the dataset
itself is constructed to be worst-case. It is unlikely this worst-
case situation will ever occur in practice; the purpose we study
this set of assumptions is instead motivated by our desire to
establish tight privacy bounds on DP-SGD.
This attack corresponds identically to the protocol for the
malicious gradient adversary. Similarly, the Crafter adversary
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:26:44 UTC from IEEE Xplore.  Restrictions apply. 
876
Distinguisher 3 Watermark detection
Require: subsampling rate q, number of iterations T , clipping norm
C, number of poison parameters 2n,number of measurements to
, trained models f0,··· fT , threshold τ
M = M + |ft − ft−1|
points ← select smallest 2n arguments fromM
∇malicious ← (cid:126)0
s ← 1
d ← 0
for p in points do
d = d + s · ft − ft−1[p]
s = −s
1: M ← (cid:126)0
2: pwatermark ← 1
3: for t = 1 to T do
if t ≤ to then
4:
5:
else
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
end if
16:
17: end for
18: if pwatermark ≥ τ then
19:
20: end if
21: return D
return D(cid:48)
end for
pwatermark = pwatermark × p(d|watermark)
ε
l
a
c
i
t
c
a
r
P
10
5
0
MNIST
Purchase
CIFAR
Theoretical
1
2
4
Theoretical ε
10
Fig. 7: Gradient attack: the adversary directly insert a
malicious gradient to training. The results are consistently
better than the previous attack.
to the prior adversary, except
is identical
instead of
choosing D to be some typical dataset we construct a new
one (discussed next). As in the prior attacks, we again use the
intermediate-model Distinguisher that has proven effective.
that
All
that remains is for us to describe the method for
choosing the dataset D. Our goal in constructing this dataset
is to come up with a dataset that will have minimal unintended
inﬂuence on the parameters of the neural network.
a) The Crafter: (Crafter 5) To accomplish this, we will
design the dataset D so that if B = B(D(cid:48)) ⊂ D then training
for one step on this mini-batch S(B) will minimally perturb
the weights of the neural network. We design Crafter 5
to create the malicious dataset. Informally,
this algorithm
proceeds as follows. Using the initial random parameters fθ0
of the neural network (which are assumed to be revealed to the
adversary in the DP-SGD analysis) the ﬁrst time the adversary
is called we construct a dataset D that the model already labels
perfectly. Because it is labeled perfectly, we will have
∇fθ (L(fθ0 , D)) ≡ 0
∀D ⊂ D
Unfortunately,
(otherwise it would be possible to construct a better dataset,
leading to a contradiction) and therefore training on any mini-
batch that does not contain D(cid:48)\D will be an effective no-op—
except for the noise added through the Gaussian mechanism.
this Gaussian noise the model adds will
corrupt the model weights and cause future gradient updates
to be non-zero. To prevent that, we set the learning rate in
the training algorithm to zero—esentially ignoring the actual
updates, and so θi ≡ θ0. This is allowable because DP-SGD
guarantees that any assignment of hyperparameters will result
in a private model—even if the choice of hyperparameters is
pathalogical and would never be used by a realistic adversary.
clipping norm C, selected params 2n
Crafter 5 Malicious dataset
Require: initial model f0, dataset size n, input space D,
1: Ddata ← randomly sample n random instances from D
2: Dlabel ← f0(Ddata)
3: D ← (Ddata, Dlabel)
4: return Call Crafter 4 on D
b) The Distinguisher: (Distinguisher 3) Identical to prior.
c) Results: Figure 7 shows how with a worst case
dataset, the adversary can achieve a lower bound nearly tight
with the upper bound provided by the analysis of DP-SGD. For
example, when the theoretical bound for DP-SGD is ε = 4,
this last adversary achieves a lower bound of 3.6.
Compared to the previous adversaries, we clearly see that
removing the “intrinsic noise” induced by the gradient updates
from other examples tightens the lower bound signiﬁcantly.
We conclude that an adversary taking advantage of all of the
assumptions made in the analysis of DP-SGD is able to leak
private information which is close to being maximal. This
suggests that the era of “free privacy” through better DP-
SGD analysis has come to an end using existing assumptions.
If DP-SGD analysis were able to better capture the noise
inherent to typical datasets, it might be possible to achieve
substantially tighter guarantees—however, even formalizing
what this “intrinsic noise” means is nontrivial.
G. Theoretical Justiﬁcations
Let us ﬁrst consider a one dimensional learning task where
the model space is C ⊆ R. Following the notation in the rest
(cid:125)
of the paper, private gradient descent essentially operates as:
θt+1 ← θt − η (∇L(fθt ; D) + Zt)
, where Zt is the Gaussian
(cid:123)(cid:122)
(cid:124)
vt
noise added at time step t. (We will ignore the details with
clipping as it is orthogonal to the discussion here.) Clearly, for
an adversary who observes vt, the Gaussian noise Zt added to
ensure R´enyi differential privacy (RDP) at time step t is both
analytically and empirically tight. The analytical tightness fol-
lows from the tightness of Gaussian mechanism [40]. Hence,
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:26:44 UTC from IEEE Xplore.  Restrictions apply. 
877
ε
l
a
c
i
t
c
a
r
P
10
5
0
Malcious
Theoretical
1
2
4
Theoretical ε
10
Fig. 8: Malicious dataset attack: the adversary creates a
custom dataset to reduce the effect of other samples on the
inserted watermark. This veriﬁes the DP-SGD privacy is tight.
in the one dimensional case, we would expect our empirical
lower bounds on privacy to be tight for one iteration.
Now, in higher dimensions, the question is how changing
one sample d ∈ D affects the gradient ∇L(fθt; D), which
in turn affects the estimation of vt in various dimensions.
First, we observe that by changing a data sample it is possible
to affect only one coordinate of the gradient in the learning
tasks we take up. Furthermore, since independent noise of the
same scale is added to each coordinate of ∇L(fθt; D), we can
reduce the problem to the one dimensional case as above.
An additional step in DPSGD is subsampling for minibatch,
which, according to [57, 54], has tight RDP guarantee. Then,
by nature of RDP, we can also tightly compose over iterations.
Finally, the conversion from RDP to differential privacy is also
tight within constant factor. Hence, the tightness of our results,
especially those demonstrated by the gradient perturbation
attacks, are natural.
V. RELATED WORK
Our paper empirically studies differential privacy and differ-
entially private stochastic gradient descent. Theoretical work
producing upper bounds of privacy is extensive [7, 4, 51, 1,
38, 55, 25, 47, 20, 21, 1, 4, 51, 40].
Our work is closely related to other privacy attacks on
machine learning models. This work is mainly focused on
developing attacks not for analysis purposes, but to demon-
strate an attack [6, 22]. Jayaraman et al. also consider the
relationship between privacy upper bounds and lower bounds,
but do not construct as strong lower bounds as we do [27].
Even more similar is Jagielski et al. [26], who as we discussed
earlier construct poisoning attacks to audit differential privacy.
Our work is primarily different in that we construct lower
bounds to measure the privacy assumptions, not just measure
one set of conﬁgurations. We additionally study much larger
datasets ([26] consider a two-class MNIST subset).
Our general approach is not speciﬁc to DP-SGD, and should
extend to other privacy-preserving training techniques. For
example, PATE [46] is an alternate technique to privately train
neural networks using an ensemble of teacher and student
neural networks. Instead of adversarially crafting datasets that
will be fed to a training algorithm, we would craft datasets
that would introduce different historgrams when the teacher is
trained on the ensemble.
VI. CONCLUSION
Our work provides a new way to investigate properties of
differentially private deep learning through the instantiation
of games between hypothetical adversaries and the model
trainer. When applied to DP-SGD, this methodology allows
us to evaluate the gap between the private information an
attacker can leak (a lower bound) and what the privacy analysis
establishes as being the maximum leak (an upper bound). Our
results indicate that the current analysis of DP-SGD is tight
(i.e., this gap is null) when the adversary is given full assumed
capabilities—however, when practical restrictions are placed
on the adversary there is a substantial gap between the upper
and our lower bounds. This has two broad consequences.
Consequences for theoretical research. Our work has im-
mediate consequences for theoretical research on differentially
private deep learning. For a time, given the same algorithm,
improvements to the analysis allowed for researchers to obtain
lower and lower values of ε. Our results indicate this trend can
continue no further. We verify that the Moments Accountant
[4, 1] on DP-SGD as currently implemented is tight. As
such, in order to provide better guarantees, either the DP-SGD
algorithm will need to be changed, or additional restrictions
must be placed on the adversary.
Our work further provides guidance to the theoretical com-
munity for what additional assumptions might be most fruitful
to work with to obtain a better privacy guarantee. For example,
even if there were a way for DP-SGD to place assumptions
on the “naturalness” of the training dataset D, it is unlikely
to make a difference: our attack that allows constructing a
pathological dataset D is only marginally more effective than
one that operates on actual datasets like CIFAR10. On the
contrary, our results indicate that restricting the adversary to
only being allowed to modify an example from the training
dataset, instead of modifying a gradient update, is promising
and could lead to more advantageous upper bounds. While it
is still possible that a stronger attack could establish a tighter
bound, we believe that given the magnitude of this gap is
unlikely to close entirely from below.
Consequences for applied research. Applied researchers
have, for some time, chosen “values of ε that offer no
meaningful theoretical guarantees” [6], for example selecting
ε (cid:29) 1, hoping that despite this “the measured exposure [will
be] negligible” [6]. Our work refutes these claims in general.
When an adversary is assumed to have full capabilities
made by DP-SGD, they can succeed exactly as often as is
expected given the analysis. For example, in federated learning
[37], adversaries are allowed to directly poison gradients and
are shown all intermediate model updates. In the centralized
setting, the situation is different: an adversary assumed to
have more realistic capabilities is currently unable to succeed
as often as the upper bound currently suggests. However,
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:26:44 UTC from IEEE Xplore.  Restrictions apply. 
878
conversely, it is always possible stronger adversaries could
succeed more often.
ACKNOWLEDGEMENTS
We are grateful to Jonathan Ullman, Andreas Terzis, and
the anonymous reviewers for detailed comments on this paper.
Milad Nasr is supported by a Google PhD Fellowship in
Security and Privacy.
REFERENCES
[1] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan
McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang.
Deep learning with differential privacy. In Proceedings
of the 2016 ACM SIGSAC Conference on Computer and
Communications Security, pages 308–318, 2016.
[2] Shahab Asoodeh, Jiachun Liao, Flavio P Calmon, Oliver
Kosut, and Lalitha Sankar. A better bound gives a
hundred rounds: Enhanced privacy guarantees via f-
divergences. arXiv preprint arXiv:2001.05990, 2020.
[3] Borja Balle, Gilles Barthe, and Marco Gaboardi. Privacy
ampliﬁcation by subsampling: Tight analyses via cou-
plings and divergences. Advances in Neural Information
Processing Systems, 31:6277–6287, 2018.
[4] Raef Bassily, Adam Smith, and Abhradeep Thakurta.
Private empirical risk minimization: Efﬁcient algorithms
In 2014 IEEE 55th Annual
and tight error bounds.
Symposium on Foundations of Computer Science, pages
464–473. IEEE, 2014.
[5] Mark Bun and Thomas Steinke. Concentrated differential
privacy: Simpliﬁcations, extensions, and lower bounds.
In Theory of Cryptography Conference, pages 635–658.
Springer, 2016.
[6] Nicholas Carlini, Chang Liu,
´Ulfar Erlingsson, Jernej
Kos, and Dawn Song. The secret sharer: Evaluating and
testing unintended memorization in neural networks. In
28th {USENIX} Security Symposium ({USENIX} Secu-
rity 19), pages 267–284, 2019.
[7] Kamalika Chaudhuri, Claire Monteleoni, and Anand D
Sarwate. Differentially private empirical risk minimiza-
Journal of Machine Learning Research, 12(3),
tion.
2011.
[8] Mia Xu Chen, Benjamin N Lee, Gagan Bansal, Yuan
Cao, Shuyuan Zhang, Justin Lu, Jackie Tsay, Yinan
Wang, Andrew M Dai, Zhifeng Chen, et al. Gmail smart
arXiv preprint
compose: Real-time assisted writing.
arXiv:1906.00080, 2019.