models for all programs in both sets accordingly. Similarly,
when we verify the candidate diﬀerences obtained from the
inferred models, all programs in both sets should be checked.
Besides these changes, the skeleton of the GetDiﬀerences al-
gorithm remains the same.
The most crucial and time-consuming part of our exten-
sion is the extension to the RCADiff functionality in order to
detect diﬀerences between two sets of models. Recall that
RCADiﬀ utilizes the product construction and then ﬁnds
the simple paths leading to the points of exposure. Given
two sets of models, we compute the intersection between all
the models in the two sets. Afterwards, we set the points
of exposure as follows. Let q = (q0, . . . , qm+n) be a state
in the product automaton. Furthermore, assume that state
qi corresponds to automata Mi from one of the input sets
I1,I2. Then, q is a point of exposure if
∀Mi ∈ I1, Mj ∈ I2 =⇒ l(qi) (cid:54)= l(qj)
With this new deﬁnition of the points of exposure, the mod-
iﬁed RCADiff algorithm proceeds as in the original case to
ﬁnd all simple paths in the product automaton that lead to
the points of exposure.
One potential downside of this algorithm is that, its com-
plexity increases exponentially as we add more models in
the sets. For example, computing the intersection of m DFA
with n states each, requires time O(nm) while, in general,
the problem is PSPACE-complete [21]. That being said, we
stress that the number of programs we have to check in prac-
tice will likely be small and many additional heuristics can
be used to reduce the complexity of the intersection compu-
tation.
4.4 Program Fingerprints
Formally, the ﬁngerprinting problem can be described as
follows: given a set I of m diﬀerent programs and black-box
access to a server T which runs a program PT ∈ I, how
can one ﬁnd out which program is running in the server T
by simply querying the program in a black-box manner, i.e.
ﬁnd P ∈ I such that P = PT .
In this section, we present two diﬀerent ﬁngerprinting al-
gorithms that provide diﬀerent trade-oﬀs between computa-
tional and query complexity. Both these algorithms build
Algorithm 3 Fingerprint Tree Building Algorithm
Require: I is a set of Programs
function BuildFingerprintTree(I)
if |I| = 1 then
root.data ← P ∈ I
return root
end if
Pi, Pj ← I
s ← GetDiﬀerences(Pi, Pj)
root.data ← s
root.left ← BuildFingerprintTree(I \ Pi)
root.right ← BuildFingerprintTree(I \ Pj)
return root
end function
a binary tree called ﬁngerprint tree that stores strings that
can distinguish between any two programs in I. Given a
ﬁngerprinting tree, our ﬁrst algorithm requires |I| queries
to the target program. If the user is willing to perform ex-
tra oﬀ-line computation, our second algorithm demonstrates
how the number of queries can be brought down to log m.
Basic ﬁngerprinting algorithm. The BuildFingerprint-
Tree algorithm (shown in Algorithm 3) constructs a binary
tree that we call a ﬁngerprint tree where each internal node
is labeled by a string and each leaf by a program identi-
ﬁer.
In order to build the ﬁngerprint tree recursively, we
start with the set of all programs I, choose any two arbi-
trary programs Pi, Pj from I, and use the diﬀerential testing
framework to ﬁnd diﬀerences between these programs. We
label the current node with the diﬀerences, remove Pi and
Pj from I, and call BuildFingerprintTree recursively until
a single program is left in I. If I has only one program, we
label the leaf node with the program and return.
2
(cid:1) diﬀerent program pairs.
(cid:1)D). Fi-
2
Given a ﬁngerprint tree, we solve the ﬁngerprinting prob-
lem as follows: Initially, we start at the root node and query
the target program with a string from the set that labels
the root node of the tree.
If the string is accepted (resp.
rejected), we recursively repeat the process along the left
subtree (resp. right subtree), until we reach a leaf node that
identiﬁes the target program.
Time/query complexity. For the following we assume an
input set of programs I of size |I| = m. Our algorithm has
to ﬁnd diﬀerences between all (cid:0)m
time complexity of the algorithm is O(2m−1 +(cid:0)m
The ﬁngerprint tree resulting from the algorithm will be a
full binary of height m. Assuming that the complexity of the
diﬀerential testing algorithm is D, we get that the overall
nally, the query complexity of the algorithm is |I|-1 queries,
since each query will discard one candidate program from
the list.
Reducing queries using shallow ﬁngerprint trees. No-
tice that, in the previous algorithm, we need m queries to
the target program in order to ﬁnd the correct program be-
cause we discard only one program at each step. We can cut
down the number of queries by shallower ﬁngerprint trees
at the cost of higher oﬀ-line computational complexity for
building such trees.
Consider the following modiﬁcation in the BuildFinger-
printTree algorithm: First, we partition I into k subsets
I1, . . .Ik of size m/k each. Next, we call BuildFinger-
printTree algorithm with the set IS = {I1, . . . ,Ik} as input
programs and replace the call to GetDifferences with Get-
SetDifferences. This algorithm will generate a full binary
tree of height k that can distinguish between the programs
in the diﬀerent subsets of I. We can recursively apply the
same algorithm on each of the leafs of the resulting ﬁnger-
printing tree, further splitting the subsets of I until each
leaf contains a single program.
Time/query complexity. It is evident that the algorithm
will eventually terminate since each subset is successively
portioned into smaller sets. Let us assume that Dset(k) the
complexity of the GetSetDifferences algorithm when the
input program sets are of size k (see section 4.3 for a com-
plexity analysis of Dset(k)). The number of queries required
for ﬁngerprinting an application with this algorithm will be
equal to the height of the resulting ﬁngerprint tree. Note
that each subset is of size m/k and to distinguish between
the k subsets using our basic algorithm we need k−1 queries.
Therefore we get the equation T (m) = T (m/k) + (k − 1)
describing the query complexity of the algorithm. Solving
the equation we get that T (m) = (k − 1) logk m which is
the query complexity for a given k. When k = 2 we will
need log m queries to identify the target program. Since
each program provides one bit of information per query (ac-
cept/reject), a straightforward decision tree argument [13]
provides a matching lower bound on the query complexity
of the problem.
Regarding the time complexity of the problem, we notice
that, at the i-th recursive call to the modiﬁed BuildFin-
gerprintTree algorithm, we will have an input set of size
m/ki since the initial set is repeatedly partitioned into k
subsets. the overall time complexity of building the tree is
(cid:1)Dset(m/ki)). We omit further de-
+(cid:0)m/ki
(2m/ki
(cid:80)logk m
i=1
2
tails here as the complexity analysis is a straightforward
adaptation of the original analysis.
5. EVALUATION
5.1
Initialization evaluation
Our ﬁrst goal is to evaluate the eﬃciency of our observa-
tion table initialization algorithm as a method to reduce the
number of equivalence queries while inferring similar mod-
els. The experimental setup is motivated by our assumptions
that the initialization model and the target model would
be similar. For that purpose, we utilized 9 regular expres-
sion ﬁlters from two diﬀerent versions of ModSecurity (ver-
sions 3.0.0 and 2.2.7) and PHPIDS WAFs (versions 0.7.0 and
0.6.3). The ﬁlters in the newer versions of the systems have
been reﬁned to either patch evasions or possibly to reduce
false positive rate.
For our ﬁrst experiment we used an alphabet of 92 sym-
bols, the same one used in our next experiments, which con-
tains most printable ASCII characters. Since, in this experi-
ment, we would like to measure the reduction oﬀered by our
initialization algorithm in terms of equivalence queries, we
simulated a complete equivalence oracle by comparing each
inferred model with the target regular expression.
Results. Table 1 shows the results of our experiments.
First, notice that in most cases the updated ﬁlters contain
more states than their previous versions. This is expected,
since most of the times the ﬁlters are patched to cover ad-
ditional attacks, which requires the addition of more states
for covering these extra cases. We can see that, in general,
our algorithm oﬀers a massive reduction of approximately
50× in the number of equivalence queries utilized in order
Figure 4: State machine inferred by SFADiff for
Mac OSX TCP implementation. The TCP ﬂags that
are set for the input packets are abbreviated as fol-
lows: SYN(S), ACK(A), FIN(F), PSH(P), URG(U),
and RST(R).
to infer a correct model. This comes with a trade-oﬀ since
the number of membership queries are increased by a factor
of 1.15×, on average. However, equivalence queries are usu-
ally orders of magnitude slower than membership queries.
Therefore, the initialization algorithm results in signiﬁcant
overall performance gain. We notice that 2/3 cases where
we observed a large increase (more than 1.2×) in member-
ship queries (ﬁlters PHPIDS 50 & PHPIDS 56) are ﬁlters for
which states were removed in the new version of the system.
This is expected since, in that case, SFADiff makes redun-
dant queries for an entry in the observation table that does
not correspond to an access string. Another possible reason
for an increase in the number of the membership queries is
the chance that the distinguishing set obtained by the SFA
learning algorithm is smaller than the one obtained by the
initialization algorithm which is always of size n − 1 where
n is the number of states in a ﬁlter. Exploring ways to ob-
tain a distinguishing set of minimum size is an interesting
direction in order to further develop our initialization algo-
rithm. Nevertheless, in all cases, the new versions of the
ﬁlters were similar in structure with the older versions and
thus, our initialization algorithm was able to reconstruct a
large part of the ﬁlter and massively reduce the number of
equivalence queries required to obtain the correct model.
5.2 TCP state machines
For our experiments with TCP state machines, we run a
simple TCP server on the test machine while the learning
algorithm runs as a client on another machine in the same
LAN. Because the TCP protocol will, possibly, emit output
for each packet sent, the ASKK algorithm is not suited for
this case. Thus, we used the algorithm from [5] for learn-
ing deterministic transducers in order to infer models of the
TCP state machines.
Alphabet. For this set of experiments, we focus on the
eﬀect of TCP ﬂags on the TCP protocol state transitions.
More speciﬁcally, we select an alphabet with 11 symbols
including 6 TCP ﬂags: SYN(S), ACK(A), FIN(F), PSH(P),
URG(U), and RST(R) along with all possible combinations
of these ﬂags with the ACK ﬂag, i.e., SA, FA, PA, UA, and
RA.
Membership queries. Once our learning algorithm for-
IDS Rules
Member
Equiv
Member
Equiv
Without Init
With Init
Learned
States
Init Filter
States
States
Diﬀ
Member
Overhead
Equiv
Speedup
MODSEC 973323
MODSEC 973324
MODSEC 973330
PHPIDS 22
PHPIDS 27
PHPIDS 40
PHPIDS 41
PHPIDS 50
PHPIDS 56
2367
768
887
17195
144759
11119
6635
6206
38768
97
55
62
252
2618
337
318
255
840
2400
892
941
17330
149159
11152
8535
9829
46732
2
19
21
105
437
68
137
1
7
25
15
15
70
66
35
25
25
60
25
12
12
45
59
25
21
27
62
0
3
3
25
7
10
4
-2
-2
1.01