title:Stealthy Adversarial Perturbations Against Real-Time Video Classification
Systems
author:Shasha Li and
Ajaya Neupane and
Sujoy Paul and
Chengyu Song and
Srikanth V. Krishnamurthy and
Amit K. Roy-Chowdhury and
Ananthram Swami
Stealthy Adversarial Perturbations Against
Real-Time Video Classiﬁcation Systems
Srikanth V. Krishnamurthy∗, Amit K. Roy Chowdhury∗ and Ananthram Swami†
∗University of California Riverside, {sli057, ajaya, spaul003}@ucr.edu, {csong, krish}@cs.ucr.edu, PI:EMAIL
Shasha Li∗, Ajaya Neupane∗, Sujoy Paul∗, Chengyu Song∗,
†United States Army Research Laboratory, PI:EMAIL
Abstract—Recent research has demonstrated the brittleness of
machine learning systems to adversarial perturbations. However,
the studies have been mostly limited to perturbations on images
and more generally, classiﬁcation tasks that do not deal with
real-time stream inputs. In this paper we ask ”Are adversarial
perturbations that cause misclassiﬁcation in real-time video
classiﬁcation systems possible, and if so what properties must they
satisfy?” Real-time video classiﬁcation systems ﬁnd application
in surveillance applications, smart vehicles, and smart elderly
care and thus, misclassiﬁcation could be particularly harmful
(e.g., a mishap at an elderly care facility may be missed). Video
classiﬁcation systems take video clips as inputs and these clip
boundaries are not deterministic. We show that perturbations that
do not take “the indeterminism in the clip boundaries input to the
video classiﬁer” into account, do not achieve high attack success
rates. We propose novel approaches for generating 3D adversarial
perturbations (perturbation clips) that exploit recent advances in
generative models to not only overcome this key challenge but
also provide stealth. In particular, our most potent 3D adversarial
perturbations cause targeted activities in video streams to be
misclassiﬁed with rates over 80%. At the same time, they also
ensure that the perturbations leave other (untargeted) activities
largely unaffected making them extremely stealthy. Finally, we
also derive a single-frame (2D) perturbation that can be applied
to every frame in a video stream, and which in many cases,
achieves extremely high misclassiﬁcation rates.
I.
INTRODUCTION
Deep Neural Networks (DNN) based real-time video classi-
ﬁcation systems are being increasingly deployed in real world
scenarios. Examples of applications include video surveil-
lance [42], self driving cars [21], health-care [52], etc. To
elaborate, video surveillance systems capable of automated
detection of “targeted” human activities or behaviors (e.g.,
accident, violence), can trigger alarms (upon detection) and
drastically reduce information workloads on human operators.
Without the assistance of DNN-based classiﬁers, human oper-
ators will need to simultaneously monitor footage from a large
number of video sensors. This can be a difﬁcult and exhausting
task, and comes with the risk of missing behaviors of interest
and slowing down decision cycles. In self-driving cars, video
classiﬁcation has been used to understand pedestrian actions
and make navigation decisions [21]. Similar applications can
be envisaged in the Army Next Generation Combat Vehicle
Network and Distributed Systems Security (NDSS) Symposium 2019
24-27 February 2019, San Diego, CA, USA
ISBN 1-891562-55-X
https://dx.doi.org/10.14722/ndss.2019.23202
www.ndss-symposium.org
(NGCV). Real-time video classiﬁcation systems have also been
deployed for automatic “fall detection” in elderly care facili-
ties [52], and detection of abnormal actions around automated
teller machines [47]. All of these applications directly relate
to the physical security or safety of people and property. Thus,
stealthy attacks on such real-time video classiﬁcation systems
are likely to cause unnoticed pecuniary loss and compromise
personal safety. Note that while objects can be detected or
distinguished by examining the individual frames in a video
(akin to object detection on images), many activities can only
be recognized or distinguished by considering a sequence of
frames holistically (i.e., a clip consisting of multiple frames).
Recent
studies have shown that virtually all DNN-
based systems are vulnerable to well-designed adversarial
inputs [10], [29], [30], [39], [43], which are also referred
to as adversarial examples. Szegedy et al. [43], showed that
adversarial perturbations that are hardly perceptible to humans
can cause misclassiﬁcation in DNN-based image classiﬁers.
Goodfellow et al. [11], analyzed the potency of realizing
adversarial samples in the physical world. Moosavi et al. [29],
and Mopuri et al. [32], introduced the concept of “image-
agnostic” perturbations. Recent efforts by Hosseini et al. [15],
and Wei et al. [54], explore adversarial perturbations on videos.
However, they are limited in that their attack models do not
work on real-time video classiﬁcation systems (more details in
§ IX).
The high level question that we try to address in this
paper is: “Is it possible to launch stealthy attacks against
DNN-based real-time video classiﬁcation systems by adding
adversarial perturbations on a video stream, and if so how?”
In contrast with the aforementined prior work, attacking a real-
time video classiﬁer poses new challenges that were not all
previously identiﬁed or addressed. First, because video streams
are collected in real-time, the corresponding perturbations also
need to be generated on-the-ﬂy with the same frame rate
which can be extremely computationally intensive. Second,
to make the attack stealthy, attackers would want
to add
perturbations on the video in such a way that they will only
cause misclassiﬁcation for the targeted (possibly malicious)
activities, while keeping the classiﬁcation of other activities
unaffected. In a real-time video stream, since the activities
change across time, it is hard to identify online and in one-
shot [8],
the target frames on which to add perturbations
(and thereby ensure that the other untargeted activities are not
affected). Third, real-time video classiﬁers use video clips (a
set of frames) as inputs [8], [47] (i.e., as video is captured, it is
broken up into clips and each clip is fed to the classiﬁer). This
introduces two additional hyper-parameters viz., the length of
a clip and the boundaries (i.e., beginning and ending) of a
clip. Even if attackers are aware of the length of each clip, it
is hard to predict the boundaries of the clips as they are non-
deterministic. This is problematic because when the attacker
generated perturbations are applied to the wrong frame within
a clip (i.e., perturbation for frame 1 of a clip being applied
to frame 2 of that clip), the perturbations may not work as
expected. (Please see Figure 4 and the associated discussion
for more details).
In this paper, our ﬁrst objective is to investigate how
to generate adversarial perturbations against real-time video
classiﬁcation systems by overcoming the above challenges. We
resolve the real-time challenge by using universal perturba-
tions (UP) [29]. UPs are universal in the sense that a UP is
not speciﬁc to one input example, but works on any input
example from the same distribution as that of the training
data. Universal perturbations affect the classiﬁcation results
by using just a (single) set of perturbations generated off-line.
Because they work on unseen inputs, they preclude the need
for intensive on-line computations to generate perturbations
for every incoming video clip. To generate such universal
perturbations, we leverage generative DNN models.
However, adding universal perturbations to all clips of the
video can cause misclassiﬁcation of all the activities in the
video stream. This may expose the attack since the results
may be abnormal (e.g., many people performing rare actions).
It may even cause activities from other classes to be mis-
classiﬁed as the target class. To make the attack stealthy,
we introduce the novel concept of dual purpose universal
perturbations, which we deﬁne as universal perturbations
which only cause misclassiﬁcation of activities belonging to
the target class, while minimizing, or ideally, having no effect
on the classiﬁcation results for activities belonging to the other
classes.
Dual purpose perturbations by themselves do not provide
high success rates because of the nondeterminism of the clip
boundaries. To be more speciﬁc, let l be the length of a
clip input to the classiﬁer, and p = {p1, p2, . . . , pl} be the
perturbations for a clip of frames x = {f1, f2, . . . , fl}; then
input x(cid:48) = {f1 ⊕ p1, f2 ⊕ p2, . . . , fl ⊕ pl}, where ⊕ denotes
pixel-wise addition, would yield misclassiﬁcation but other
combinations like x(cid:48)(cid:48) = {f1 ⊕ pl, f1 ⊕ p1, . . . , fl ⊕ pl−1}
(where pl in the latter expression refers to the last frame in
the previous clip) may not cause misclassiﬁcation. To solve
this problem, we introduce a new type of perturbation that
we call the Circular Universal Dual Purpose Perturbation (C-
DUP). The C-DUP is a 3D perturbation which is effective on a
video stream even in the presence of a temporal misalignment
between the perturbation clips and the input video clips.
Speciﬁcally, any cyclic permutations of a C-DUP perturbation
clip are also still valid perturbations. For example, both {f1 ⊕
pl, f2⊕p1, . . . , fl⊕pl−1} and {f1⊕pl−1, f2⊕pl, . . . , fl⊕pl−2}
can cause expected misclassiﬁcation. Because of this property,
C-DUP works even if the sequential concatenation of two
broken up parts of two consecutive perturbation clips, is added
to an input video clip as a perturbation clip. To generate C-
DUPs, we make signiﬁcant changes to the baseline generative
model used to generate universal perturbations. In particular,
we add a new unit to generate circular perturbations, that is
placed between the generator and the ﬁxed discriminator (as
discussed later). We demonstrate that the C-DUP is very stable
and effective in achieving real-time stealthy attacks on video
classiﬁcation systems.
Finally, to better understand the effect of the temporal
dimension, we also investigate the feasibility of attacking the
classiﬁcation systems using a simple and light 2D perturbation
(frame level instead of clip level) which is applied across all
the frames of a video. By tweaking our generative model, we
are able to generate such perturbations which we name as
2D Dual Purpose Universal Perturbations (2D-DUP). These
perturbations work well on a sub-set of videos, but not all. We
will discuss the reasons for this when we describe these 2D
attacks in § VI-D.
Our Contributions: In brief, our contributions are:
• We provide a comprehensive analysis of the challenges in
crafting adversarial perturbations for real-time video clas-
siﬁers. We empirically identify what we call the boundary
effect phenomenon in generating adversarial perturbations
against video streams (see § VI-B). In a nutshell,
the
boundary effect arises because of the nondeterminism of
the boundaries of the clips input to the video classiﬁcation
system.
• We design and develop a generative framework to craft two
types of stealthy adversarial perturbations against real-time
video classiﬁers, viz., the circular dual purpose universal
perturbation (C-DUP) and the 2D dual purpose universal
perturbation (2D-DUP). These perturbations are agnostic to
(a) the content the video streams capture (i.e., are universal)
and (b) the clip boundaries within the streams.
• We demonstrate the potency of our adversarial perturbations
using two different video datasets. In particular, the UCF101
dataset captures coarse-grained activities (human actions
such as applying eye makeup, bowling, drumming) [41]. The
Jester dataset captures ﬁne-grained activities (hand gestures
such as sliding hand left, sliding hand right, turning hand
clockwise, turning hand counterclockwise) [7]. We are able
to launch stealthy attacks on both datasets with over a 80 %
misclassiﬁcation rate, while ensuring that the other classes
are correctly classiﬁed with relatively high accuracy.
II. BACKGROUND
In this section, we provide the background relevant to
our work. Speciﬁcally, we discuss how a real-time video
classiﬁcation system works and what standard algorithms are
currently employed for action recognition.
A. Real-time video-based classiﬁcation systems
DNN based video classiﬁcation systems are being increas-
ingly deployed in real-world scenarios. Examples include fall
detection in elderly care [9], abnormal event detection on
campuses [49], [50], security surveillance for smart cities
[51], and self-driving cars [21], [22]. Given an input real-
time video stream, which may contain one or more known
actions, the goal of a video classiﬁcation system is to correctly
recognize the sequence of the performed actions. Real-time
video classiﬁcation systems commonly use a sliding window
2
III. THREAT MODEL AND DATASETS
In this section, we describe our threat model. We also
provide a brief overview of the datasets we chose for validating
our attack models.
A. Threat model
We consider a white-box model for our attack, i.e., the
adversary has access to the training datasets used to train
the video classiﬁcation system, and has knowledge of the
deep neural network model used in the real-time classiﬁcation
system. We assume that
the datasets are trusted. We also
assume that the adversary is capable of injecting perturbations
in the real-time video stream. In particular, we assume the
adversary to be a man-in-the-middle that can intercept and
add perturbations to streaming video [25], or that it could have
previously installed a malware that is able to add perturbation
prior to classiﬁcation [34].
We assume that the adversaries seek to be stealthy i.e.,
they want the system to only misclassify malicious actions
without affecting the recognition of the other actions. So, we