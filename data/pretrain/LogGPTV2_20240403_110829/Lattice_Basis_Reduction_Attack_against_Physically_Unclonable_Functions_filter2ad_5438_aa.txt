title:Lattice Basis Reduction Attack against Physically Unclonable Functions
author:Fatemeh Ganji and
Juliane Kr&quot;amer and
Jean-Pierre Seifert and
Shahin Tajik
Lattice PUF: A Strong Physical Unclonable
Function Provably Secure against Machine Learning
Attacks
Ye Wang, Xiaodan Xi, and Michael Orshansky
Department of Electrical and Computer Engineering
The University of Texas at Austin
{lhywang, paul.xiaodan, orshansky}@utexas.edu
Austin, TX, USA
0
2
0
2
n
u
J
6
1
]
R
C
.
s
c
[
2
v
1
4
4
3
1
.
9
0
9
1
:
v
i
X
r
a
Abstract—We propose a strong physical unclonable function
(PUF) provably secure against machine learning (ML) attacks
with both classical and quantum computers. Its security is
derived from cryptographic hardness of learning decryption
functions of public-key cryptosystems. Our design compactly re-
alizes the decryption function of the learning-with-errors (LWE)
cryptosystem. Due to the fundamental connection of LWE to
lattice problems, we call the construction the lattice PUF.
Lattice PUF is constructed using a physically obfuscated key
(POK), an LWE decryption function block, and a linear-feedback
shift register (LFSR) as a pseudo-random number generator.
The POK provides the secret key of
the LWE decryption
function; its stability is ensured by a fuzzy extractor (FE). To
reduce the challenge size, we exploit distributional relaxations
of space-efﬁcient LWEs. That allows only a small challenge-
seed to be transmitted with the full-length challenge generated
by the LFSR, resulting in a 100X reduction of communication
cost. To prevent an active challenge-manipulation attack, a self-
incrementing counter is embedded into the challenge seed.
We prototyped the lattice PUF with 2136 challenge-response
pairs (CRPs) on a Spartan 6 FPGA, which required 45 slices
for the PUF logic proper and 233 slices for the FE. Simulation-
based evaluation shows the mean (std) of uniformity to be 49.98%
(1.58%), of uniqueness to be 50.00% (1.58%), and of reliability
to be 1.26% (2.88%). The LWE concrete hardness estimator
guarantees that a successful ML attack of the lattice PUF will
require the infeasible 2128 CPU operations. Several classes of
empirical ML attacks, including support vector machine, logistic
regression, and deep neural networks, are used: in all attacks, the
prediction error remains above 49.76% after 1 million training
CRPs.
Index Terms—Strong PUF, PAC Learning, Lattice Cryptogra-
phy, ML Resistance.
I. INTRODUCTION
Silicon physical unclonable functions (PUFs) are security
primitives widely used in device identiﬁcation, authentica-
tion, and cryptographic key generation [45]. Given an input
challenge, a PUF exploits the randomness inherent in CMOS
© 2020 IEEE. Personal use of this material is permitted. Permission from
IEEE must be obtained for all other uses, in any current or future media,
including reprinting/republishing this material for advertising or promotional
purposes, creating new collective works, for resale or redistribution to servers
or lists, or reuse of any copyrighted component of this work in other works.
technology to generate an output response. In contrast to weak
PUFs, also called physically obfuscated keys (POKs) using the
taxonomy of [19], which supply limited amount of challenge-
response pairs (CRPs), strong PUFs have an exponentially
large CRP space.
In this paper, we propose a strong PUF that is secure
against machine learning (ML) attacks with both classical
and quantum computers. As a formal framework to deﬁne ML
resistance, we adopt the probably approximately correct (PAC)
theory of learning [38]. Speciﬁcally, the PAC non-learnability
of a decryption function implies that with a polynomial num-
ber of samples, with high probability, it is not possible to learn
a function accurately by any means. The main insight, which
allows us to build such a novel strong PUF, is our reliance on
the earlier proof that PAC-learning a decryption function of a
semantically secure public-key cryptosystem entails breaking
that cryptosystem [25], [26], [28]. We develop a PUF for
which the task of modeling is equivalent to PAC-learning the
decryption function of a learning-with-errors (LWE) public-
key cryptosystem. The security of LWE cryptosystems is based
on the hardness of LWE problem that ultimately is reduced to
the hardness of several problems on lattices [41]. The input-
output mapping between the PUF and the underlying LWE
cryptosystem can be brieﬂy summarized as follows: challenge
⇐⇒ ciphertext and response ⇐⇒ decrypted plaintext. No-
tably, LWE is believed to be secure against both classical and
quantum computers. Because of the intrinsic relation between
the proposed PUF and the security of lattice cryptography we
call our construction the lattice PUF.
The lattice PUF is constructed using a POK, an LWE
decryption function block, a linear-feedback shift register
(LFSR), a self-incrementing counter, and a control block. The
entire implementation is lightweight and fully digital.
The LWE decryption function block is the core module
of the lattice PUF, generating response (plaintext) to each
submitted challenge (ciphertext). Design parameters of the
LWE decryption function in the lattice PUF are chosen by
balancing the implementation costs, statistical performance,
and the concrete hardness of ML resistance. We develop
a measure of ML security in terms of the total number
of operations needed to learn a model of the PUF. Such
concrete hardness is established by the analysis of state-of-the-
art attacks on the LWE cryptosystem [31], [37] and evaluated
by the estimator developed by Albrecht et al. [3]. Using this
estimator, we say that a PUF has k-bit ML resistance if a
successful ML attack requires 2k operations. We implement
the LWE decryption function with guaranteeing 128-bit ML
resistance. However, directly using a LWE decryption function
as a strong PUF is not efﬁcient since 1-bit of response requires
1288-bit input challenges.
We further develop an improved design for
resource-
constrained environments that dramatically (by about 100X)
reduces the communication cost associated with PUF response
generation. This is achieved by by exploiting distributional
relaxations allowed by recent work in space-efﬁcient LWEs
[15]. This allows introducing a low-cost pseudo-random num-
ber generator (PRNG) based on an LFSR and transmitting only
a small seed. Finally, while the focus of the paper is a PUF
that is secure against passive attacks, we address the risk of an
active attack by adopting the technique in [49]: we introduce
a self-incrementing counter and embed the counter value into
a challenge seed. This makes the attack impossible as the
counter restricts the attacker’s ability to completely control
input challenges to the LWE decryption function.
We construct the lattice PUF to achieve a CRP space of
size 2136. Statistical simulation shows excellent uniformity,
uniqueness, and reliability of the proposed lattice PUF. The
mean (standard deviation) of uniformity is 49.98% (1.58%),
and of inter-class HD is 50.00% (1.58%). The mean BER
(intra-class Hamming distance (HD)) is 1.26%. We also vali-
date the empirical ML resistance of the constructed lattice PUF
via support vector machines (SVM), logistic regression (LR),
and neural networks (NN). Even with a deep neural network
(DNN), which is considered to be one of the most powerful
and successful ML attacks today, the prediction error stays
above 48.81% with 1 million training samples. The proposed
lattice PUF requires a 1280-bit secret key. A concatenated-
code-based fuzzy extractor (FE) is utilized to reconstruct stable
POK bits. Assuming an average bit error rate (BER) of 5%
for raw SRAM cells, the total number of raw SRAM bits
needed is 6.5K, in order to achieve a key reconstruction failure
rate of 10−6. We implement the entire PUF system (except
for raw SRAM cells) on a Spartan 6 FPGA. The PUF logic,
including an LWE decryption function, a 256-tap LFSR, a
128-bit self-incrementing counter, requires only 45 slices. The
concatenation-code-based FE takes 233 slices. Compared to
several known strong PUFs, the proposed PUF is signiﬁcantly
more resource-efﬁcient.
II. BACKGROUND WORK
In order for a strong PUF to be an effective security prim-
itive, the associated CRPs need to be unpredictable. In other
words, strong PUFs are required to be resilient to modeling
attacks via ML. The question of whether it is possible to
engineer a ML secure and lightweight strong PUF has been a
long-lasting challenge [46].
SVM is utilized to successfully attack a 64-bit arbiter PUF
(APUF) in [30]. Subsequent modiﬁcation of the original APUF
aimed to strengthen ML resistance, including bistable ring
PUF [10], feed-forward APUF [30], and lightweight secure
PUF (LSPUF) [36], have also been broken via improved ML
attacks [5], [42], [44], [16]. Recent proposed interpose PUF
(IPUF) [39] claims provable ML resistance by assuming XOR
APUFs are hard to learn and rigorously reducing IPUF mod-
eling to XOR APUFs. Unfortunately, both their assumption
and claims are proved wrong: [43] demonstrates that XOR
APUFs and IPUFs are actually vulnerable to deep-learning-
based modeling attacks. There are often complex reasons why
claims and rigorous proofs of security fail in practice. The
most fundamental one is that
their claims rely on recent
conjectures made from empirical ﬁndings. In contrast, the
security proof of lattice PUF is based on the hardness of
several basic lattice problems, which are seen as foundational
results in math and computer science, and are widely believed
true.
By exploiting higher intrinsic nonlinearity, some strong
PUFs [29], [47] exhibit empirically-demonstrated resistance
to a list of ML algorithms. Empirical demonstrations of ML
resistance are not fully satisfactory since they can never rule
out the possibility of other more effective ML algorithms.
The so-called controlled PUF setting [17] attempts to ensure
the ML resistance via cryptographic primitives such as hash
functions. However, the use of hash functions inside a PUF
endangers the promise of a strong PUF as a lightweight struc-
ture. Strong PUF constructions using established cryptographic
ciphers, such as AES [7], have similar challenges.
Recent work [14], [19], [21] have also utilized lattice-
based problems, including learning-parity-with-noise (LPN)
to realize computationally-secure FEs and, as
and LWE,
to construct strong PUFs. 1 The fundamen-
a byproduct,
tal security property that [14], [19], [21] rely upon is the
computational hardness of recovering a private key from a
public key in a public-key cryptosystem. Their CRP generation
is based on generating multiple private keys (playing the
role of PUF responses) and multiple public keys (playing
the role of PUF challenges). This is only possible because
multiple public keys are derived using a ﬁxed (same) source
of secret POK bits, embedded in the error term of LPN or
LWE. As was shown in [4],
that multiple CRPs
have shared error terms can be easily exploited allows a
computationally-inexpensive algorithm for solving an LPN or
LWE instance, thus compromising the hardness of LPN or
LWE problems. Thus, by itself [14], [19], [21], the resulting
PUF does not have resistance against ML modeling attacks.
This vulnerability is ﬁxed in [19], [21] by introducing a
cryptographic hash function to hide the original CRPs, which
violate the principle of lightweightness. In stark contrast, the
the fact
1A computational FE guarantees absence of information leakage from pub-
licly shared helper data via computational hardness in contrast to conventional
FEs that need to limit their information-theoretic entropy leakage.
proposed lattice PUF derives its security by directly exploiting
a distinctly different property of public-key cryptosystems: the
theoretically-proven guarantee that their decryption functions
are not PAC-learnable. In the lattice PUF, the above-discussed
vulnerability is absent since the publicly known challenges are
ciphertexts and the security of the cryptosystem ensures that a
ﬁxed private key (the POK, in our case) cannot be recovered
from ciphertexts.
III. LWE DECRYPTION FUNCTIONS ARE HARD TO LEARN
This section formally deﬁnes ML resistance of strong PUFs
via the notion of PAC learning and shows why LWE de-
cryption functions are attractive for constructing post-quantum
ML-resistant PUFs. In this section, we focus on passive attacks
in which the attacker can observe the challenges sent to the
veriﬁer but is unable to generate challenges of his or her
choice.
A. ML Resistance as Hardness of PAC Learning
A strong PUF can be modeled as a function f : C → R
mapping from the challenge space C (usually {0, 1}n) to the
response space R (usually {0, 1}). We call f the true model
of a strong PUF since it captures the exact challenge-response
behavior.
ML attacks are usually performed by relying on a functional
class of candidate models, collecting CRPs as the training data,
and running a learning algorithm to obtain a model from the
candidate class which best approximates the true model. In
addition to the approximation quality, the criteria of evaluating
the effectiveness and efﬁciency of the learning algorithm also
include the sample and time complexity. To claim that a strong
PUF is easy to learn, one can propose a learning algorithm
which ﬁnds a CRP model with good approximation quality
using a small number of sample CRPs and terminates in a short
time. The converse is difﬁcult: to claim that a PUF is hard to
learn, one must show that all possible learning algorithms fail
to provide models with good approximation quality, or they
require a large number of CRPs or a long running time.
We argue that the only known framework for seeking a
provable notion of ML resistance with a formal analysis of
approximation quality, sample size, and time complexity is
the PAC learning model [38]. We now formalize the passive
modeling attack scenario in the context of PAC learning. A
PAC-term for a true model f of a strong PUF is a concept.
Denote as F the set of all possible PUF-realized functions
(every instance of a PUF creates its unique functional mapping
f). The set of candidate models used in the learning algorithm
is the hypothesis set H. The goal of a learning algorithm
is to select a candidate model that matches the true model
well. Importantly, as shown later, the proof of PAC-hardness
guarantees that H does not have to be restricted to be the same
as F of true models. This generalization permits a stronger
representation-independent PAC-hardness proof. While not
always possible, representation-independent hardness can be
proven for PAC-learning of decryption functions ensuring that
no matter how powerful and expressive the chosen H is, PAC
learning decryption function requires exponential time.
Within the PAC model, CRPs in a training set are assumed
to be independent and identically distributed (i.i.d.) under a
certain distribution D.
We say a set F of strong PUFs is PAC-learnable using H, if
there exists a polynomial-time algorithm A such that ∀ > 0,
∀δ > 0, for any ﬁxed CRP distribution D, and ∀f ∈ F, given
a training set of size m, A produces a candidate model h ∈ H
with probability of, at least, 1 − δ such that
(c,r)∼D[f (c) (cid:54)= h(c)] < .
Pr
In conclusion, our strategy is to say that a strong PUF is
ML-resistant if it is not PAC-learnable (i.e., that it is PAC-
hard). PAC-hardness implies that any successful ML attack
requires at least an exponential running time.
B. Decryption Functions Are not PAC Learnable
What is critically important is that there exist functions that
are known to be not PAC-learnable. Speciﬁcally, a class of
decryption functions of secure public-key cryptosystems is not
PAC-learnable, as established by [25], [28]. We outline their
proof below.
A public-key cryptosystem is a triple of probabilistic
polynomial-time algorithms (Gen, Enc, Dec) such that: (1)