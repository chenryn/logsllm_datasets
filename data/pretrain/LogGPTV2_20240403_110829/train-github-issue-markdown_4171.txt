## Issue description
nn.DataParallel uses memory on GPU 0 even if it's explicitly instructed to use
2 and 3.
## Code example
No line in this code snippet touches `cuda:0`.
    import os
    import torch
    import torch.nn as nn
    class MM(nn.Module):
        def __init__(self):
            super().__init__()
            self.block1 = nn.Linear(10, 20)
            # wrap block2 in DataParallel
            self.block2 = nn.Linear(20, 20)
            self.block2 = nn.DataParallel(self.block2, device_ids=[3, 2])
            self.block3 = nn.Linear(20, 20)
        def forward(self, x):
            x = self.block1(x)
            x = self.block2(x)
            x = self.block3(x)
            return x
    dev='cuda:3'
    x=torch.zeros((64, 10), device=dev)
    mm=MM()
    mm.to(device=dev)
    y=mm(x)
    os.system('nvidia-smi')
However, nvidia-smi output indicates that GPU 0 is allocated memory
    +-----------------------------------------------------------------------------+
    | Processes:                                                       GPU Memory |
    |  GPU       PID   Type   Process name                             Usage      |
    |=============================================================================|
    |    0      2407      G   /usr/lib/xorg/Xorg                            14MiB |
    |    0      5848      C   python                                       306MiB |
    |    2      5848      C   python                                       324MiB |
    |    3      5848      C   python                                       324MiB |
    +-----------------------------------------------------------------------------+
## System Info
PyTorch version: 0.4.0  
Is debug build: No  
CUDA used to build PyTorch: 8.0.61
OS: Ubuntu 16.04.2 LTS  
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609  
CMake version: version 3.5.1
Python version: 3.6  
Is CUDA available: Yes  
CUDA runtime version: 8.0.61  
GPU models and configuration:  
GPU 0: Tesla K80  
GPU 1: Tesla K80  
GPU 2: Tesla K80  
GPU 3: Tesla K80
Nvidia driver version: 384.111  
cuDNN version: Probably one of the following:  
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so  
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5  
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5.1.10  
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn_static.a
Versions of relevant libraries:  
[pip3] msgpack-numpy (0.4.2)  
[pip3] numpy (1.13.3)  
[pip3] numpydoc (0.6.0)  
[pip3] torch (0.4.0)  
[pip3] torchvision (0.2.1)  
[pip3] torchx (0.0.1, /home/jimfan/code/torchx)  
[conda] cuda80 1.0 0 soumith  
[conda] pytorch 0.4.0 py36_cuda8.0.61_cudnn7.1.2_1 pytorch  
[conda] torchvision 0.2.1 py36_1 pytorch  
[conda] torchx 0.0.1