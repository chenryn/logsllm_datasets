the human investigator is to display the provenance at different lev-
els of detail. For instance, if a message fails to appear at node
N1 but could only have originated at node N2 several hops away,
the basic provenance tree would show, for each node on the path
from N1 to N2, that the message was not sent from there, be-
cause it failed to appear there, because it was not received from
the next-hop node, etc. We can improve readability by summariz-
ing these (thematically related) vertices into a single super-vertex.
When the graph is ﬁrst shown to the human investigator, we include
as many super-vertices as possible, but the human investigator has
the option to expand each super-vertex into the corresponding ﬁne-
grained vertices if necessary.1
We have identiﬁed three situations where this summarization
can be applied. The ﬁrst is a chain of transient events that orig-
inates at one node and terminates at another, as in the above ex-
ample; we replace such chains by a single super-vertex. The
second is the (common) sequence NEXIST([t1, t2], N, τ ) ←
NAPPEAR([t1, t2], N, τ ) ← NDERIVE([t1, t2], N, τ ), which
basically says that a tuple was never derived; we replace
this with a single ABSENCE([t1, t2], N, τ ) super-vertex;
its
positive counterpart EXISTENCE([t1, t2], N, τ ) is used to re-
place a positive sequence.
The third situation is a deriva-
tion that depends on a small set of triggers – e.g., ﬂow en-
tries can only be generated when a packet p is forwarded
to the controller C.
the basic provenance
will contain a long series of NAPPEAR([ti, ti+1], C, p) ver-
tices that explain the common case where the trigger packet
p does not exist; we replace these with a single super-vertex
ONLY-EXIST({t1, t2, . . .} in [tstart, tend], C, p) that initially fo-
cuses attention on the rare cases where the trigger does exist.
In this case,
5. THE Y! SYSTEM
In this section, we describe the design of Y! (for “Why not?”), a
system for capturing, storing, and querying both positive and neg-
ative provenance.
5.1 Overview
Like any debugger, Y! is meant to be used in conjunction with
some other application that the user wishes to diagnose; we refer
to this as the target application. Y! consists of four main compo-
nents: The provenance extractor (Section 5.2) monitors the target
1More generally, visualization and interactive exploration are use-
ful strategies for working with large provenance graphs [17].
6
application and extracts relevant events, such as state changes or
message transmissions. These events are passed to the provenance
storage (Section 5.3), which appends them to an event log and also
maintains a pair of indices to enable efﬁcient lookups of negative
events. When the user issues a provenance query, the query pro-
cessor uses the stored information to construct the relevant subtree
of the provenance graph, simpliﬁes the subtree using the heuristics
from Section 4, and then sends the result to the frontend, so that the
user can view, and interactively explore, the provenance. We now
explain some key components in more detail.
5.2 Provenance extractor
Recall from Section 3 that the input to the graph construction algo-
rithm is a sequence of entries (±τ, N, t, r, c), which indicate that
the c.th derivation of tuple τ appeared or disappeared on node N
at time t, and that the reason was r, i.e., a derivation rule or an
incoming message. The purpose of the provenance extractor is to
capture this information from the target application. This function-
ality is needed for all provenance systems (not just for negative
provenance), and it should be possible to use any of the several
approaches that have been described in the literature. For instance,
the target application can be annotated with calls to a special library
whenever a relevant event occurs [22], the runtime that executes the
target application (e.g., an NDlog engine or a virtual machine) can
report the relevant events [37], or a special proxy can reconstruct
the events from the sequence of messages that each node sends and
receives [35]. Note that the latter two approaches can be applied
even to legacy software and unmodiﬁed binaries.
5.3 Provenance storage
The provenance storage records the extracted events in an append-
only log and makes this log available to the query processor. A
key challenge is efﬁciency: with positive provenance, it is pos-
sible to annotate each event with pointers to the events that di-
rectly caused it, and, since there is a fairly direct correspondence
between events and positive vertices in the provenance graph, these
pointers can then be used to quickly navigate the graph. With neg-
ative provenance, however, it is frequently necessary to evaluate
range queries over the time domain (“Did tuple τ ever exist dur-
ing interval [τ1, τ2]”). Moreover, our PARTITION heuristic requires
range queries over other domains, e.g., particular subspaces of a
given table (“Are there any X(a,b,c) tuples on this node with
5 ≤ b ≤ 20?”) to decide which of several possible explanations
might be the simplest. If Y! evaluated such range queries by scan-
ning the relevant part of the log, performance would suffer greatly.
Instead, Y! uses R-trees [7] to efﬁciently access the log. R-trees
are tree data structures for indexing multi-dimensional data; brieﬂy,
the key idea is to group nearby objects and to represent each group
by its minimum bounding rectangle at the next-higher level of the
tree. Their key advantage in our setting is that they can efﬁciently
support multidimensional range queries.
On each node, Y! maintains two different R-trees for each table
on that node. The ﬁrst, the current tree, contains the tuples that cur-
rently exist in the table; when tuples appear or disappear, they are
also added or removed from the current tree. The second, the his-
torical tree, contains the tuples that have existed in the past. State
tuples are added to the historical tree when they are removed from
the current tree; event tuples, which appear only for an instant, are
added directly to the historical tree.
The reason for having two separate trees is efﬁciency.
It is
known that the performance of R-trees degrades when elements are
frequently inserted and removed because the bounding rectangles
will no longer be optimal and will increasingly overlap. By sepa-
rating the historical tuples (where deletions can no longer happen)
from the current tuples, we can obtain a more compact tree for the
former and conﬁne fragmentation to the latter, whose tree is much
smaller. As an additional beneﬁt, since tuples are appended to the
historical tree in timestamp order, splits in that tree will typically
occur along the time dimension; this creates a kind of “time index”
that works very well for our queries.
5.4 Pruning the historical tree
Since the historical tree is append-only, it would eventually con-
sume all available storage. To avoid this, Y! can reclaim storage
by deleting the oldest tuples from the tree. For instance, Y! can
maintain a cut-off time Tcut; whenever the tree exceeds a certain
pre-deﬁned size limit, Y! can slowly advance the cut-off time and
keep removing any tuples that existed before that time until enough
space has been freed. To enable the user to distinguish between
tuples that were absent at runtime and tuples that have been deleted
from the tree, the graph construction algorithm can, whenever it ac-
cesses information beyond Tcut, annotate the corresponding vertex
as potentially incomplete.
5.5 Limitations
Like other provenance systems, Y!’s explanations are limited by
the information that is available in the provenance graph. For in-
stance, Y! could trace a misconﬁguration to the relevant setting,
but not to the person who changed the setting (unless that informa-
tion were added to the provenance graph). Y! also has no notion
of a program’s intended semantics: for instance, if a program has a
concurrency bug that causes a negative event, a query for that event
will yield a detailed explanation of how the given program pro-
duced that event. Only the operator can determine that the program
was supposed to do something different.
6. CASE STUDIES
In this section, we describe how we have applied Y! to two applica-
tion domains: software-deﬁned networks (SDN) and BGP routing.
We chose these domains partly because they yield interesting de-
bugging challenges, and partly because they do not already involve
declarative code (applying Y! to NDlog applications is straightfor-
ward!). We illustrate two different implementation strategies: auto-
matically extracting declarative rules from existing code (for SDN)
and writing a declarative description of an existing implementation
(for BGP). We report results from several speciﬁc debugging sce-
narios in Section 7.
6.1 SDN debugging
Our ﬁrst case study is SDN debugging: as others [8] have pointed
out, better debugging support for SDNs is urgently needed. This
scenario is challenging for Y! because SDNs can have almost ar-
bitrary control programs, and because these programs are typically
written in non-declarative languages. Provenance can be extracted
directly from imperative programs [22], but switching to a different
programming model would require some adjustments to our prove-
nance graph. Hence, we use automated program transformation to
extract declarative rules from existing SDN programs.
Language: Pyretic We chose to focus on the Pyretic lan-
guage [21]. We begin by brieﬂy reviewing some key features of
Pyretic that are relevant here. For details, please see [21].
Pyretic programs can deﬁne a mix of static policies, which are
immutable, and dynamic policies, which can change at runtime
based on system events. Figure 4 shows a summary of the rele-
vant syntax. A static policy consists of actions, e.g., for forward-
ing packets to a speciﬁc port (fwd(port)), and predicates that
7
Primitive actions:
A ::= drop | passthrough | fwd(port) | flood |
push(h=v) | pop(h) | move(h1=h2)
Predicates:
P ::= all_packets | no_packets | match(h=v) |
ingress | egress | P & P | (P | P) | ∼P
Query policies:
Q ::= packets(limit,[h]) | counts(every,[h])
Policies:
C ::= A | Q | P[C] | (C|C) | C>>C | if_(P,C,C)
Figure 4: Static Pyretic syntax (from [21])
def learn(self):
def update(pkt):
self.P = if_(match(dstmac=pkt[’srcmac’]),
switch=pkt[’switch’]),
fwd(pkt[’inport’]), self.P)
q = packets(1,[’srcmac’,’switch’])
q.when(update)
self.P = flood | q
def main():
return dynamic(learn)()
Figure 5: Self-learning switch in Pyretic (from [21])
restrict these actions to certain types of packets, e.g., to packets
with certain header values (match(h=v)). Two policies a and
b can be combined through parallel composition (a|b), mean-
ing that a and b should be applied to separate copies of each
packet, and/or through sequential composition (a>>b), meaning
that a should be applied to incoming ﬁrst, and b should then
be applied to any packet(s) that a may produce. For instance,
match(inport=1)>>(fwd(2)|fwd(3)) says that packets
that arrive on port 1 should be forwarded to both ports 2 and 3.
Dynamic policies are based on queries. A query describes pack-
ets or statistics of interest – for instance, packets for which no
policy has been deﬁned yet. When a query returns new data, a
callback function is invoked that can modify the policy. Figure 5
(taken from [21]) shows a simple self-learning switch that queries
for packets with unknown MAC addresses; when a packet with a
new source MAC m is observed on port p, the policy is updated to
forward future packets with destination MAC m to port p.
Pyretic has other features besides these, and providing compre-
hensive support for them is beyond the scope of our case study.
Here, our goal is to support an interesting subset, to demonstrate
that our approach is feasible.
Translation to NDlog: Our Pyretic frontend transforms all static
policies into a “normal form” that consists of groups of parallel
“atoms” (with a sequence of matches and a single action) that are
arranged sequentially. This form easily translates to OpenFlow
wildcard entries: we can give the highest priority to the atoms in the
ﬁrst group, and assign further priorities to the following groups in
descending order. To match Pyretic’s behavior, we do not install the
wildcard entries in the switch directly, but rather keep them as base
tuples in a special MacroRule table in the controller. A second
stage then matches incoming packets from the switches against this
table, and generates the corresponding microﬂow entries (without
wildcards), which are then sent to the switch.
For each query policy, the frontend creates a separate table and
a rule that sends incoming packets to this table if they match the
query. The trigger is evaluated using NDlog aggregations; for in-
stance, waiting for a certain number of packets is implemented with
NDlog’s count<> operator.
Our frontend supports one type of dynamic policies: policies
that append new logic in response to external events. These are es-
sentially translated to a single NDlog rule that is triggered by the