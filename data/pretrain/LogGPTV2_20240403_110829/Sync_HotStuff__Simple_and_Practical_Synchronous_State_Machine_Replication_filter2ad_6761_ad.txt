Fig. 4: The steady state protocol in a responsive view.
Let v be the current view and L be the current leader. Let L2 be the next leader and v2 be the synchronous view of L2.
Note that v2 = v + 2 if v is a synchronous view, and v2 = v + 1 is v is a responsiveness view.
i Blame. If fewer than p proposals trigger r’s votes in (2p + 4)∆ time in view v, broadcast (cid:104)blame, v2 − 1(cid:105)r. Upon
gathering f + 1 (cid:104)blame, v2 − 1(cid:105) messages, broadcast them along with (cid:104)blame2, v2 − 1(cid:105)r, and quit view v. If leader
equivocation is detected, broadcast (cid:104)blame2, v2 − 1(cid:105)r and the two equivocating messages, and quit view v.
ii Status. Upon gathering f + 1 (cid:104)blame2, v2 − 1(cid:105) messages, wait for 2∆ time. Pick a highest certiﬁed block Cv(cid:48)(Bk(cid:48)),
lock on Cv(cid:48)(Bk(cid:48)), send Cv(cid:48)(Bk(cid:48)) to the new leader L2, and enter view v2.
iii New-view. The new leader L2 waits for 2∆ time after entering view v2 and broadcasts (cid:104)new-view, v2,Cv(cid:48)(Bk(cid:48))(cid:105)L2 where
Cv(cid:48)(Bk(cid:48)) is a highest certiﬁed block known to L2.
iv First vote. Upon receiving (cid:104)new-view, v2,Cv(cid:48)(Bk(cid:48))(cid:105)L2, if Cv(cid:48)(Bk(cid:48)) has a rank equal to or higher than r’s locked block,
forward (cid:104)new-view, v2,Cv(cid:48)(Bk(cid:48))(cid:105)L2 to all other replicas, broadcast (cid:104)vote, Bk(cid:48), v2(cid:105)r.
Fig. 5: The view-change protocol to support responsive reviews.
quorum size should be > 3n/4. This will give Sync HotStuff a
responsive mode when messages from > 3n/4 replicas reach
within time δ. This happens if the actual number of faults is
less than n/4. Put differently, if more than n/4 (but fewer
than n/2) replicas are faulty, they can prevent responsiveness
but they cannot cause a safety violation. In that case, we fall
back to the synchronous mode in Section III.
Protocol. The new protocol will give two views to each leader.
Each odd view is a synchronous view and runs the protocol
in Figure 3. Each even view is a responsive view and runs
the protocol in Figure 4. As mentioned, a certiﬁcate from a
responsive view requires a quorum of > 3n/4. The only other
difference from Figure 3 is that a replica pre-commits Bk−1
immediately on receiving f + 1 proposals for Bk, rather than
waiting for a 2∆ period.
To switch from a synchronous view to a responsive view,
the leader simply broadcasts a signed switch message. Upon
receiving the switch message, a replica forwards it and enters
the responsive view. The leader sends a new-view message to
obtain the ﬁrst certiﬁcate in the view as before.
Whenever a view fails due to an equivocating leader or lack
of progress, replicas engage in a view-change protocol to move
to the next leader’s synchronous view. Guarding the safety of
responsive commits across view changes requires some care
as we explain below. In the protocol without responsiveness,
a commit implied that a certiﬁcate was broadcast by some
prompt replica 2∆ time earlier. This ensured that a certiﬁcate
was obtained at a majority of honest replicas before they
quit the view. Since there is no 2∆ time in the responsive
mode, the above does not hold. The solution is to insert a
2∆ wait between quitting the old view and entering the new
view (Step ii in Figure 5). The delay is introduced at a replica
after learning that a majority of replicas have quit the view,
giving sufﬁcient time for the certiﬁcates to be received at a
majority of prompt replicas before they enter the next view.
These changes will be utilized in the proof of Lemma 7.
A. Safety of Responsive Views
Lemma 7 (Lemma 6 extended to responsiveness views). If
an honest replica directly commits B(cid:96) in view v, then (i) no
equivocating block is certiﬁed in view v, and (ii) f + 1 honest
replicas lock on a certiﬁed block that ranks equal to or higher
than Cv(B(cid:96)) before entering view v + 1.
Proof. If view v is a synchronous view, then the proof of
Lemma 6 still applies. It remains to prove the lemma for
responsive views.
Part (i) directly follows from quorum intersection. Since
certiﬁcates in a responsive view requires > 3n/4 votes, for
there to be equivocating certiﬁed blocks, at least 3n/4+3n/4−
n = n/2 > f replicas need to vote for equivocating blocks,
which cannot happen.
Part (ii) mostly follows from the proof of Lemma 6. Suppose
an honest replica directly commits B(cid:96) in view v. Then a set R
of d+1 honest replicas have pre-committed B(cid:96). Let the earliest
pre-commit among them be performed at time t. This implies
that more than f + 1 replicas broadcast a proposal containing
Cv(B(cid:96)) before time t. At least one of them is honest and
prompt at time t. Denote the set of honest and prompt replicas
at time t + ∆ by R(cid:48). R(cid:48) receives Cv(B(cid:96)) by time t + ∆. We
will now prove that the set R(cid:48) satisﬁes part (ii). It remains
to show that no replica in R(cid:48) has entered view v2 = v + 1.
Suppose this is not true, i.e., one of the replicas in R(cid:48), say
replica r(cid:48) has entered view v2 before time t + ∆. In that case,
due to the 2∆ wait during view-change, r(cid:48) has received f + 1
blame2 messages before time t− ∆. Thus, f + 1 replicas have
sent blame2 and quit view v before time t − ∆. At least one
of them is honest and prompt at time t − ∆. At least one
replica in the pre-committing set R would be prompt at time
t and would have received this blame2 message by time t. It
would have prevented the pre-commit of B(cid:96) at that replica, a
contradiction.
The rest of the safety proof remains unchanged.
V. EVALUATION
In this section, we ﬁrst evaluate the throughput and latency
of Sync HotStuff under different parameters and conditions
(batch size, payload, and client command load). We then eval-
uate the impact of ∆ on throughput and latency and show it is
insigniﬁcant as expected. Lastly, we compare Sync HotStuff
with HotStuff [12] and Dﬁnity [3].
A. Implementation Details and Methodology
We implement the Sync HotStuff protocol under the stan-
dard synchrony model. Our implementation is an adaptation of
the open-source implementation of HotStuff [12]. We modify
the HotStuff code to primarily replace the core protocol logic
while reusing some of its utility modules, such as its event
queue and network library.
In our implementation, each block contains a batch of
commands sent by clients. A command consists of a unique
identiﬁer and an associated payload. We refer to the maxi-
mum number of commands in a block as the batch size. A
conceptual representation of a block is shown in Figure 6.
All throughput and latency results were measured from
clients which are separate processes running on machines
different from those for replicas. Each client generates a
number of outstanding commands and broadcasts them to
every replica. Replicas only use the unique identiﬁer (e.g.
hash) of a command to represent it in proposals and votes.
To execute the commands for the replicated state machine,
a replica either has the command content received from the
client’s initial broadcast, or fetches it from the leader if
the client crashes before it ﬁnishes the broadcast. We use
four machines, each running four client processes, to inject
commands into the system. Each client process can maintain
a conﬁgurable number of outstanding commands at any time.
Metadata
... prev ...
Commands
cmd
id
payload
cmd
id
payload
...
cmd
id
payload
b
a
t
c
h
s
i
z
e
Fig. 6: Structure of a block in the implementation.
We ensure that the performance of replicas will not be limited
by lack of client commands.
Experimental setup. All our experiments were conducted
over Amazon EC2 where each replica was executed on a
c5.4xlarge instance. Each instance had 16 vCPUs sup-
ported by Intel Xeon Platinum 8000 processors. All cores
sustained a Turbo CPU clock speed up to 3.4GHz. The
maximum TCP bandwidth measured by iperf is around
4.9 Gbps, i.e., 0.6 Gigabytes per second. We did not throttle
the bandwidth in any run. The network latency between
two machines is measured to be less than 1 ms. We used
secp256k1 for digital signatures in votes and a certiﬁcate
consists of a compact array of secp256k1 signatures.
Baselines. We compare with two baselines: (i) HotStuff, a
partially synchronous protocol, and (ii) Dﬁnity , a synchronous
protocol. We use HotStuff as a baseline because Sync HotStuff
shares the same code base as HotStuff enabling a fair compar-
ison, and because HotStuff achieves comparable (or even bet-
ter) performance to state-of-the-art partially synchronous BFT
implementation [12]. We pick Dﬁnity as our other baseline
because it is the state-of-the-art synchronous BFT protocol.
We did not ﬁnd an implementation of Dﬁnity’s cousensus
protocol in its Github repository, so we implemented our own
version of Dﬁnity with our codebase (which should also help
ensure a fair comparison). While implementing and evaluating
Dﬁnity, we made several simpliﬁcations that are favorable to
Dﬁnity. For instance, it was shown that a malicious leader can
exploit a ﬂaw in the original Dﬁnity design to force unbounded
communication complexity [13]. We did not implement the
suggested ﬁx to this ﬂaw (and of course we did not exploit
this ﬂaw). We assume all leaders in Dﬁnity are honest, which
will improve Dﬁnity’s theoretical latency from 9∆ to 7∆.
We simulate their Veriﬁable Random Functions (VRF), by
essentially assuming VRF generation takes negligible time in
Dﬁnity. Implementing these extra ﬁxes and actual mechanisms
will only further hurt Dﬁnity’s performance.
B. Basic Performance
We ﬁrst evaluate the basic performance of Sync HotStuff
to tolerate f = 1 fault for a synchrony bound of ∆ = 50 ms.
(a) Varying batch sizes.
(b) Varying payload.
Fig. 7: Throughput vs. latency of Sync HotStuff at varying batch sizes and payloads at ∆ = 50 ms and f = 1.
(a) ∆ vs. throughput.
(b) ∆ vs. latency.
Fig. 8: Performance of Sync HotStuff at varying ∆ and f at batch size = 400 and 0/0 payload.
throughput-latency graph. In the graph, each point represents
the measured throughput and latency for one run with a given
“load” by the clients. More speciﬁcally, a client process main-
tains a ﬁxed number of outstanding commands at any moment.
When an outstanding command is committed, a new command
is immediately issued to keep up with the speciﬁed number.
We vary the size of the outstanding command pool to simulate
different loads. The points at the lower left represent the state
when the system is not saturated by client commands. As
the load increases, the throughput initially increases without
incurring a loss in latency. Finally, after the load saturates
the bandwidth, the throughput remains unchanged (or slightly
degrades) when clients inject more commands, while the
latency goes up. The latency increases because the commands
stay in the command pool longer before they are proposed
in a block for consensus. For a batch size of 400, we observe
that the throughput is saturated at around 280 Kops/sec. There
is no further throughput gain when batch size increases from
400 to 800. So in all of our following experiments, we ﬁx our
batch size to 400.
We also test how payload size of a command affects
performance. Figure 7b shows the performance with three
client request/reply payload sizes (in bytes) 0/0, 128/128 and
1024/1024, denoted by “p0”, “p128”, and “p1024”. In
addition to the actual payload, each command also contains
an 8 byte counter to differentiate the commands. For example,
Fig. 9: Throughput vs. latency of Sync HotStuff at varying
batch sizes at ∆ = 50 ms and f = 1 for Dﬁnity [13].
We measure the observed throughput (in number of commands
committed/sec, or ops/sec) and end-to-end latency for clients
(in ms). We conduct two experiments. The ﬁrst one ﬁxes the
payload and varies batch size (Figure 7a) while the other ﬁxes
a batch size and varies the payload size (Figure 7b).
In Figure 7a, each command has a zero-byte payload to
demonstrate the overhead induced solely by consensus and
state machine replication. We consider three different batch
sizes, 100, 400 and 800, represented by the three lines in the
1060110160210260310Throughput(Kops/sec)100120150Latency(ms)Sync-HS-b100Sync-HS-b400Sync-HS-b8001060110160210260310Throughput(Kops/sec)100120150Latency(ms)Sync-HS-p0Sync-HS-p128Sync-HS-p10241050100500MaximumNetworkDelay∆(ms)1060110160210260310Throughput(Kops/sec)Sync-HS-1Sync-HS-8Sync-HS-32Sync-HS-641050100500MaximumNetworkDelay∆(ms)02505007501000Latency(ms)Sync-HS-1Sync-HS-8Sync-HS-32Sync-HS-64y=21060110160Throughput(Kops/sec)300350400450Latency(ms)Dfinity-b400Dfinity-b14000Dfinity-b19200(a) f vs. throughput.
(b) f vs. latency.
Fig. 10: Performance as function of faults at ∆ = 50 ms, optimal batch size, and 0/0 payload.
3
(a) f vs. throughput.
(b) f vs. latency.
Fig. 11: Performance as function of faults at ∆ = 50 ms, optimal batch size, and 1024/1024 payload.
3
the actual command size for 0/0 is 8 bytes.
C. The Impact of ∆ on Performance
In the steady state of Sync HotStuff, replicas advance to
the next step as soon as previous messages arrive, without
waiting for any conservative ∆ bound. Thus, although each
block still incurs 2∆ latency to be committed, the system is
able to move on after a single round-trip time, process new
blocks in pipeline, and saturate available network bandwidth.
Figures 8a and 8b study the effect of varying ∆ on throughput
and latency. Each line represents a choice of f, denoted by
“1”, “8”, “32”, “64”. As expected, we observe that
the