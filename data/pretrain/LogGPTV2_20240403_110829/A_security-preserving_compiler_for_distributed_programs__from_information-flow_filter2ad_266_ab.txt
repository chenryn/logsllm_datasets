3. COMPUTATIONAL NONINTERFERENCE
We brieﬂy recall standard notions of information ﬂow policies,
then deﬁne our main security properties.
Security Labels We annotate every variable with a security label.
These labels specify the programmer’s security intent, but they do
not affect the operational semantics.
The security labels form a lattice ࡀࣘ obtained as the product
of two lattices, for conﬁdentiality levels ࡀ ࣘ  and for integrity
levels ࡀ ࣘ . We write ࡀ and ࡀ for the smallest and largest
elements of ࡀ, and ࣶ and ࣵ for the least upper bound and greatest
lower bound of two elements of ࡀ, respectively. We write ,
, ,  for the smallest and largest elements of ࡀ and ࡀ,
respectively.
For a given label ࡁ  ࡁ  ࡁ  of ࡀ, the conﬁdentiality label ࡁ
speciﬁes a read level for variables, while the integrity label ࡁ spec-
iﬁes a write level; the meaning of ࡁ ࣘ ࡁ߰ is that ࡁ߰ is more conﬁden-
tial (can be read by fewer entities) and less trusted (can be written
by more entities) than ࡁ [Myers et al., 2006]. We let ࡁ  ࡁ
and ࡁ  ࡁ be the projections that yield the conﬁdentiality and
integrity parts of a label. Hence, the partial order on ࡀ is deﬁned as
ࡁ ࣘ ࡁ߰ iff ࡁ ࣘ ࡁ߰ and ࡁ ࣘ ࡁ߰.
Memory and Host Policies We represent our memory policy as a
function Ɖ from variables to security labels. For brevity, we some-
times write  (resp. ) instead of Ɖ (resp. Ɖ).
We extend Ɖ to represent host policies as a map from host names
to security labels. Host policies are used to establish a control ﬂow
protocol (see Section 5) and to select cryptographic protection. Our
intent is that host  may read the variables  
 and write the vari-
 , deﬁned as
ables  

  ࢯ  ࣘ 

  ࢯ  ࣘ 
 

 

Adversaries Our security properties are parameterized by the
power of the adversary, deﬁned as a pair      of subsets
of the security lattices:
ࢫ  ࣪ ࡀ, the public labels, is a non-empty downward-
closed subset of the conﬁdentiality lattice;
ࢫ  ࣪ ࡀ, the tainted labels, is a non-empty upward-closed
subset of the integrity lattice.
In the rest of the paper, we often assume a ﬁxed policy and adver-
sary , and let
    ࢯ  ࢠ 
    ࢯ  ࢠ 
An active adversary command, ranged over by , is a core com-
mand that reads only public variables (HL ࣮   ) and writes
only tainted variables (ML ࢵ    ࢝). In particular,  can al-
ways read variables with conﬁdentiality label  and always write
variables with integrity label .
For example, let ࡀ" be the 4-point security lattice deﬁned by the
product of the conﬁdentiality lattice  ࣘ  and the integrity
lattice  ࣘ . In this lattice, the adversary   
yields adversary commands that can read low-conﬁdentiality vari-
ables and write low-integrity variables. For brevity, elements of ࡀ"
are written HL, HH, LL, and LH in the rest of the paper.
In the general case, one can deﬁne the adversary by indicating
the subset of hosts that may have been compromised and letting
 and  be the closures of their conﬁdentiality and integrity
labels. This ensures that any commands at these hosts become valid
adversary commands.
Our implementation depends both on the security policy and on
the structure of the lattices, but it does not depend on the choice of
a particular adversary.
Indistinguishability Games In computational models of cryptog-
raphy, security properties are often expressed as games, coded as
commands that sample a secret boolean  :=  then implement
a protocol that interacts with an adversary, also modelled as com-
mands. The goal of the adversary is to write into some variable 
its guess as to the value of : the adversary wins when   
(where the operator  is boolean equality, true iff both or none of
its operands equal ). The trivial adversary  :=  wins with
434  , so we are interested in a bound on the advantage
probability 
of an adversary, deﬁned as the probability that    minus 
  .
The protocol is deemed secure when, for a game that involves only
commands that run in polynomial time, this advantage is negligible
in the security parameter (usually the length of the keys used in the
protocol).
Conﬁdentiality and Integrity We deﬁne security properties for
probabilistic command contexts as computational variants of non-
interference, expressed as games.
For conﬁdentiality, our property is parameterized by active ad-
versaries plus three commands that initialize variables that the ad-
versary can read (set by ) or not (set by either  or ).
(COMPUTATIONAL CONFIDENTIALITY).
DEFINITION 2
Let Ɖ be a policy,  an adversary, and  a command context.
Let , , and  range over polynomial commands such that
ML ࢵ    ࢝ for    . Let  range over tuples of adver-
sary contexts such that    is a polynomial command. Consider
the command

  := ; ; if    then  else ;   
CNI
where  ࢠ L     and  ࢠ L    .
when ࢯ 2HCNI     ࢤ 
 is computationally conﬁdential (CNI) for , , , and 
 ࢯ is negligible.
 is CNI for , ,  when this holds for all .
 is CNI when this holds for all , , , and .
In the deﬁnition, the command contexts  represent the code of
an active adversary that interacts with  and tries to infer the value
of . The adversary “knows”  , , , and , inasmuch as the
deﬁnition of  may depend on them. Implicitly, the last adversarial
piece of code in    is supposed to write into  its guess for .
By deﬁnition of the command CNI, the value of  affects the
initial state of the memory when    runs, by running either 
or , but only for high-conﬁdentiality variables, which  cannot
directly read, so  can win the game with some advantage only
if  somehow leaks information from high-conﬁdentiality to low-
conﬁdentiality variables.
The three statements at the end of the deﬁnition differ only in
their generality. The ﬁrst statement is for two speciﬁc distributions
of initial memories, set by ;  and ;  respectively, and for
a speciﬁc adversary. The second statement expresses that no such
adversary may effectively distinguish between these two speciﬁc
distributions of initial memories; it may be used to characterize the
security of commands that leak some conﬁdentiality information.
The third statement expresses that this holds for any such distribu-
tions of initial memories, and is a computational variant of nonin-
terference (CNI). If we omit the computational hypotheses, run
   with arbitrary initial low-equivalent memory distributions 
rather than those initialized by ; , and compare (exactly) the
distributions of low-conﬁdentiality variables after running   
rather than just the values of , we retrieve a formulation of proba-
bilistic noninterference.
Let us consider special cases for  and :
ࢫ If the command context  is of the form ; _ with a single
adversary command that runs after , then computational
conﬁdentiality reduces to a notion of noninterference against
passive adversaries that observe low-conﬁdentiality memory
only after  completes, but do not interact with .
ࢫ If  is of the form ; _; ; _;    ; ; _, the adversary
 consists of a tuple of    commands that run between
each of the commands  and represents an active adversary
whose execution is interleaved with that of  , but which can-
not change the order in which the commands  run.
ࢫ If  is of the form ; _     , then the adversary
 consists of a single -ary command context that repre-
sents an untrusted network or scheduler that can run the com-
mands  any number of times, in any order. This adversary
is strictly more powerful than the one above.
We use an (almost dual) security deﬁnition for integrity. The def-
inition uses an auxiliary polynomial command  that reads high-
integrity variables and writes  after running the interactive com-
putation between our command and the active adversary.
(COMPUTATIONAL INTEGRITY).
DEFINITION 3
Let Ɖ be a policy,  an adversary, and  a command context.
Let , , ,  range over polynomial commands such that
ML ࣮   ࢨ ML  and ML ࢵ    ࢝ and HL  ࣮  .
Let  range over adversary contexts such that    is a polynomial
command. Consider the command
CNI

  := ; ; if    then  else ;   
where  ࢠ L       and  ࢠ L    . A
run of CNI is valid when every variable  in MLCNI  ࢵ HL 
is written exactly once.
 is computationally integral (CNI) for , , , , and 
 ࢯ
when 2HCNI valid   implies that ࢯ 2HCNI;      ࢤ 
is negligible.
 is CNI for , ,  when this holds for all , and  .
 is CNI when this holds for all , , , , and  .
In the deﬁnition, the initialization command  sets variables that
are not writable by  but that are readable by  . The game consists
of letting  interact with the system and try to force the program
behavior to depend on the low-integrity bit , thus yielding different
value assignments to high integrity variables.
Since the adversary  may prevent the execution of certain high-
integrity threads, the deﬁnition imposes an additional condition that
excludes this means of communication with  : the variables writ-
ten by CNI and read by  must have been written exactly once.
This condition enables us to consider as secure programs that per-
form checks on low-integrity variables (for instance a signature ver-
iﬁcation), then assign high-integrity variables only if the checks
succeed. Hence, our deﬁnition let  and  range over commands
such that  makes a check fail, or  reads those high-integrity vari-
ables, but not in the same game. For example, the command con-
text _ :=  is CNI, but _ :=   :=  is not. As a drawback,
the deﬁnition accounts for the integrity only of the ﬁrst assignment
to each variable, but this weakness can be mitigated by rewriting 
in single-assignment style, as explained in Section 6. (See also the
discussion of weak integrity and runtime failures in Fournet and
Rezk 2008.)
In the following, we are interested in both conﬁdentiality and
integrity, and we say that a command context is computationally
non-interferent (CNI) when it is both CNI and CNI.
Source Program Safety Our compilation process makes assump-
tions on source programs, which we deﬁne next. (Compilation may
still fail on some inputs, as explained in Section 6; we ruled out
safety conditions that would guarantee that the compilation always
succeeds as unduly restrictive.)
Figure 1 presents a type system that captures our safety hypothe-
ses on programs with localities. The typing judgments for source
435TSUBC
    ࡁ
ࡁ߰ ࣘ ࡁ
    ࡁ߰
   ࡁ
TDECLASSIFY
  := @A?=IIEBO ࡁ  Ɖ
ࡁ  
   Ɖ
TASSIGN
  :=   Ɖ
TFUN   Ɖ
  :=    Ɖ
antees as for the source program against the restricted class of ad-
versaries that make it safe [see also Chong and Myers, 2006].
We illustrate each stage of our compilation process with the sam-
ple program listed below. Section 8 discusses other examples.
EXAMPLE 1. For the 4-points lattice ࡀ" (with two levels of
conﬁdentiality and integrity), let  be the source program
a:{
xHL := 1; yLH := 2;
while yLH < 3 do {
yLH := yLH + 4;
b:{
if (yLH mod 2) = 1
then {xHL := xHL + 9}
else {skip}
};
c:{zLH := 5} } }
TSEQ  
  ࡁ
  
  ࡁ
  ࡁ
  
 ;  
TCOND
   ࡁ
TWHILE LOCAL
   ࡁ
   ࡁ
 while  do   ࡁ
TLOCALITY
    ࡁ
TCOND LOCAL
   ࡁ
   ࡁ
   ࡁ
 if  then  else   ࡁ
ࡁ  
  
  ࡁ
 else  
  ࡁ
  
 if  then  
TWHILE
   ࡁ
  ࡁ
  
  ࡁ
 while  do  
  ࡁ
ࡁ  
 ࣘ ࡁ
      ࡁ
  ࣮  

TSKIP
 skip  ࡀ
and Ɖ a policy such that Ɖ  Ɖ  Ɖ  HH for hosts,
and Ɖ  HL, Ɖ  Ɖ  LH for variables.  is typable,
polynomial, and has no declassify, so it is safe and CNI.
Figure 1: Typing rules (for a given policy Ɖ).
commands are of the form    ࡁ where ࡁ is a security label. We
omit the standard rules for typing expressions, such that    ࡁ
when Ɖ ࣘ ࡁ for each variable  read in . This type system is
similar but more permissive than those typically used for noninter-
ference [see e.g. Sabelfeld and Myers, 2003] as it accepts programs
with explicit declassiﬁcations. We discuss some speciﬁc rules:
TDECLASSIFY does not prevent explicit conﬁdentiality ﬂows, but
enforces that the command be typed with the integrity level of the
label that appears in the declassify annotation. Those labels will be
subject to robustness conditions.