quency range from 20Hz to 2kHz. High-quality speakers
(middle and right): the power over the same frequency
range is more concentrated with less ﬂuctuations.
Figure 5 compares normalized signal power of live-human
voices and voices replayed through two different high-quality
loudspeakers. Even though they show similar exponential
decay patterns overall, the low frequency patterns are different
(see red-dashed rectangles in Figure 5). As for loudspeakers
(middle and right), there is a smaller number of sharp and
long peaks at low frequencies compared to live-human voices
(left).
Thus, distortion-induced power patterns (e.g., the number
of visible power peaks, their corresponding frequencies, and
standard deviations of power peaks sizes) in low frequen-
cies could be effective in detecting standalone speakers that
2688    29th USENIX Security Symposium
USENIX Association
012345678Frequency, kHz00.20.40.60.81powcdfLive-humanSmartphone speaker  =  0.80,q    = 38.18  =  0.97,q    =  9.880246810Frequency, kHz051015202530Normalized powerLive-human0246810Frequency, kHz051015202530Normalized powerReplayed: Logitech (2.1 Ch)0246810Frequency, kHz051015202530Normalized powerReplayed: Yamaha (5.1 Ch)Audio LPCC features (cid:1832)(cid:1848)(cid:3013)(cid:3017)(cid:3004)High power frequencies features(cid:1832)(cid:1848)(cid:3009)(cid:3017)(cid:3007)Low frequencies power features (cid:1832)(cid:1848)(cid:3013)(cid:3007)(cid:3017)Power linearity degree features (cid:1832)(cid:1848)(cid:3013)(cid:3005)(cid:3017)Signal feature vectors FVs(3) Attack detectionTraining dataClassifierDecision (live-human or speaker)Real-time commandsVoice signalSpectrogramFrequency-power signal dataIntegrate power over timeMic(1) Signal transformation(2) Feature extractionSpow represents the cumulative spectral power per frequency
of Voicein. W represents the size of a single segment of Spow
to fully capture the dynamic characteristics of Spow with a
small number of segments. A weighting factor ω between 0
and 1 is used to calculate a threshold for feature values in
higher frequencies. Those parameter values were determined
experimentally with a large number of training samples. Last,
pow(i) represents the accumulated power in ith segment of
Spow. We only consider voice signals below 15kHz because
most of the signal power for voice samples fall below 15kHz.
Algorithm 1 Void’s overall procedure.
Input: Voicein, W and ω
Output: live-human or replayed
Stage 1: Signal transformation
(cid:99).
W
Stage 2: Feature extraction
1: Compute STFT of for input voice command Voicein
2: Compute Spow from STFT
3: Divide Spow into k segments where k = (cid:98) size(Spow )
4: for ith segment Segi from i = 1 to k do
5:
pow(i) = the sum of power in Segi.
6:  = Vectorize(pow(1),··· , pow(k)) and normalize between 0 and 1
7: FVLFP = First 48 values of 
8: FVLDF = LinearityDegreeFeatures()
9: FVHPF = HighPowerFrequencyFeatures(FVLFP, ω)
10: Compute LPCC of Voicein and store the results as FVLPC
11: FVVoid = {FVLDF ,FVHPF ,FVLPC,and FVLFP}
12: Run SVM classiﬁer with FVVoid and provide the class label (either live-human
Stage 3: Attack detection
or replayed) as output
5.2 Signal transformation
In the ﬁrst signal transformation stage, given an input voice
signal Voicein, short-time Fourier transform (STFT) is com-
puted (Step 1 of Algorithm 1). To compute STFT, a given
voice signal is divided into short chunks of equal length (de-
noted as wlen = 1,024); Fourier transform is then computed
on each chunk. We used a periodic Hamming window length
wlen of 1,024, and a hop length of 256, which is computed
by wlen/4. The number of fast Fourier transform points used
(n f f t) for computing STFT is set to 4,096. The obtained
signal spectrogram contains frequencies and corresponding
power over time (see Figure 2 (left)). From the computed
STFT, cumulative spectral power per frequency (Spow) is com-
puted (Step 2 of Algorithm 1). The terms “cumulative spec-
tral power” and “power” are used hereafter interchangeably.
Spow is a vector that contains the total accumulated power for
each frequency over the full duration of Voicein (see Figure 2
(right)). Spow obtained from STFT is a vector of size 1,500
(Step 2 of Algorithm 1). We use the notation size(Spow) to
represent the number of values stored in Spow.
5.3 Feature extraction
The vector Spow computed from the ﬁrst stage is used as the
input to the second stage to extract the classiﬁcation features.
Void sequentially computes the following four types of fea-
tures: (1) low frequencies power features (FVLFP), (2) signal
power linearity degree features (FVLDF), (3) higher power
frequencies features (FVHPF), and (4) LPCC features for au-
dio signals (FVLPC). FV stands for feature vectors. The ﬁrst
three feature classes are computed from Spow while FVLPC is
computed directly from raw voice signals Voicein.
5.3.1 Low frequencies power features
In the second stage of Algorithm 1, we ﬁrst divide the sig-
nal Spow into k short segments of equal-length according to
the given window size W (see Step 3). We empirically set
W = 10. If the size of Spow can not be divided by W , we
simply omit the last segment. Next, we compute the sum of
power in each segment Segi for i = 1 to k (see Steps 4 and
5). We then vectorize the ﬁrst k segments of power density
values as  (= pow(1), . . . , pow(k)) (see Step 6). The
vector  is directly used in FVLFP (see Step 7). After
executing this step, we would have cumulative spectral power
density values for all k segments. Power density values for
each segment are ordered by frequency, starting from the low-
est frequency of a given voice sample. We are only interested
in retaining power density values within the frequency value
of 5kHz because our experiments showed that there are clear
differences between human and replayed voices at the lower
frequencies below 5kHz (see Figure 5). Therefore, we keep
just the ﬁrst 48 values of  vector and assign them to
FVLFP (see Step 7).
5.3.2 Signal power linearity degree features
Given the vector  of k segments, we compute the sig-
nal’s feature vector (FVLDF) to measure the degree of linearity
(as discussed in Section 4.1).
Algorithm 2 LinearityDegreeFeatures
Input: 
Output: FVLDF ={ρ, q}.
1: Normalize  with sum() to obtain normal
2: Accumulate the values of normal to obtain powcdf
3: Compute the correlation coefﬁcients of powcdf and store the results as ρ
4: Compute the quadratic coefﬁcients of powcdf and store the results as q
Algorithm 2 describes the procedure for computing the
linearity degree of . Initially,  is normal-
ized by dividing each value in  by the total signal
power (sum()) (see Step 1 in Algorithm 2). The nor-
malized power signal vector normal is then used to
compute the cumulative distribution of spectral power, de-
noted by powcdf (see Step 2). In this step, normal is
accumulated in a step-wise fashion.
For the linearity degree of powcdf, we compute the follow-
ing two features (see Step 3 and 4): correlation coefﬁcients
ρ and quadratic curve ﬁtting coefﬁcients q of powcdf (see
USENIX Association
29th USENIX Security Symposium    2689
Appendix C). Correlation coefﬁcients of a cumulative distri-
bution can be used to quantify the linearity of the cumulative
distribution. However, we found that ρ is not highly sensi-
tive in identifying the distinguishable exponential growth of
power in live-human voices at frequencies between 20Hz and
1kHz (see Figure 5). Therefore, we introduce the quadratic
curve ﬁtting coefﬁcients q of signal powcdf as another metric
to quantify the degree of linearity for the cumulative distri-
bution function. Finally, the two computed coefﬁcients {ρ,q}
are stored as FVLDF.
5.3.3 High power frequency features
Given the vector  and the peak selection threshold
ω, we compute the feature vector (FVHPF) to capture the
dynamic characteristics of spectral power (see Appendix D).
Algorithm 3 HighPowerFrequencyFeatures
1: Find
Input: FVLFP and ω
Output: FVHPF = {Npeak, µpeaks, σpeaks, Pest}
peaks
{(peak1,loc1),··· , (peakn,locn)} as Speak (cid:46) n is the number of peaks discovered
in FVLFP
discovered
FVLFP
peaks
from
and
store
the
if peaki , we ﬁrst iden-
tify peaks and their locations (see Step 1). Our peak selection
criterion Tpeak automatically scales itself with respect to the
spectral power density values of a given signal. For example,
for a given low or high power voice signal, Tpeak is computed
accordingly as shown in Step 2. We experimentally found
that detected peaks from live-human voice samples and re-
played samples show different characteristics when we set
ω = 0.6. However, ω needs to be conﬁgured such that the high
power frequency features are effective in detecting replayed
voices. We set a threshold value to ﬁlter out insigniﬁcant
peaks, multiplying max(Speak) by a given weighting factor ω
where 0 ≤ ω ≤ 1 (see Step 2, 3, and 4).
To construct FVHPF, we ﬁrst count the number of peaks
in Speak and store the number of counted peaks as Npeak (see
Step 5); the mean and standard deviation of locations of the
discovered peaks are sequentially computed and stored as
µpeaks and σpeaks, respectively (see Step 6 and 7); and we
determine the 6 order of the polynomial to be ﬁtted to FVLFP
and use the polynomial coefﬁcients as Pest (see Step 8).
5.3.4 LPCC features
We use an auto-correlation method with Levinson-Durbin
algorithm [26] to compute LPCC for a given speech signal,
Figure 7: Importance scores for individual features on the
ASVspoof dataset.
There were 17 (with the scores above 1.0) noticeably im-
portant features (shown by the peaks) from the 4 feature
groups visualized in red-dashed rectangles. We can observe
that FVLFP and FVLPC are the most important features. From
the FVHPF group, we found that Pest features play an impor-
tant role in distinguishing voice replay attack. However, some
power value features in the low frequencies group (FVLFP)
were relatively less important. To show the necessity of all
features used in Void, we also tested Void separately on each
of the feature groups: FVLFP, FVLDF, FVHPF, and FVLPC (see
Appendix E).
Classiﬁer. To implement a lightweight system, we need
to build a classiﬁer based on the four feature vectors, which
achieves high detection accuracy and meets the latency re-
quirements. Our recommended classiﬁcation algorithm is de-
scribed in Section 7.2. We also provide the details of Void’s
implementation parameters (see Appendix F).
2690    29th USENIX Security Symposium
USENIX Association
Features081624324048566472808897Importance score0123FVLPC    FVLDFFVLFPFVHPF6 Data Collection
This section describes human voice samples and voice attack
samples we collected using multiple recording and playback
devices, and under varying conditions. For our own dataset, all
of the voice samples were recorded at a sampling frequency
(Fs) of 44.1kHz. We also used a publicly available replay at-
tack dataset that was used in the 2017 voice spooﬁng attack
detection (ASVspoof) competition [7]. The ASVspoof dataset
evauation results were used to directly compare Void’s perfor-
mance against the top performing (state of the art) solutions
from the competition.
6.1 Demographics and human voice collection
We recruited a total of 120 participants from two different
locations (a university and a company), and asked each par-
ticipant to say around 50 commands from a prepared list of
real-world voice assistant commands. We used two different
smartphones, Galaxy S8 and iPhone 8 to record all human
voices. After eliminating voice samples that were not recorded
properly or were not understood by the voice assistant, we
were left with 10,209 human voice samples to experiment
with. The voice commands were mixed in lengths (approxi-
mately ranging from 2 to 5 seconds) and command types (e.g.,
setting alarms, calling contacts, and opening emails). About
53% of the participants were male, ensuring that both male
and female voice frequency ranges were covered [16]. Most
of the participants were in the 40-49 (13%), 30-39 (62%), and
20-29 (25%) age groups.
We explicitly informed the participants that the purpose
of the voice sample collection was to develop and evaluate a
voice liveness detection solution. Ethical perspective of our
research was validated through an institutional review board
(IRB) at Sungkyunkwan university; the IRB ﬁle number is
“2018-01-024.”
6.2 Replay attack dataset
To generate a comprehensive replay attack dataset, we re-
played all 10,209 human voice samples in an open lab envi-
ronment through a mixed set of speakers and recorded them
under varying conditions as described below:
• Background noise: The open lab environment we used to
record all attack samples is collaboratively used by about
100 employees at a large IT company. During the day, the
lab is used for discussions, development, testing, coffee
breaks, and so on. The lab is quiet in the evenings. There
are also daily background noises generated from server
machines, TVs, projectors, and robot cleaners. The human
voices were replayed and recorded throughout the day and
in the evenings, capturing natural yet diverse set of back-
ground noises as well as silent moments while generating
the replay attack dataset.
• Distances between attacking devices and target devices:
Distances between target devices (used to record voice
samples) and attack devices (used to play recorded voice
samples) could affect the detection rate because spectral
power features could be affected with distance. Hence, we
recorded replayed voice samples in three varying distances:
about 15 centimeters, 130 centimeters, and 260 centimeters
away from each playback speaker.
• Playback speaker types: We used 11 different types of
in-built speakers including smartphones and a smart TV,
and four different types of standalone speakers to replay
recorded voice samples (see Appendix G). Each standalone
speaker was different in terms of the number of sound chan-
nels supported, brand, price, and electrical power. Our stan-
dalone speaker selection included Logitech 2.1 ch., and
Yamaha 5.1 ch. speakers that were designed to optimize the
ﬁnal acoustic sounds for human ears. We replayed about
5,500 human voices through each speaker type. Only the
Yamaha 5.1 channel speaker was connected to the replaying
devices (smartphones) through Bluetooth. The other three
standalone speakers were all connected through auxiliary
port (AUX) physical cables.
• Recording device types (microphones): We used 3 dif-
ferent laptops, and 9 different smartphones as recording
devices (see Appendix H). For each playback speaker type,
we used a different combination of three recording devices
with varying distances as described above.
After eliminating voice samples that were not recognized
properly by voice assistants, we were left with a ﬁnal attack
set of 244,964 samples to experiment with. All voice samples
were recorded, stored, and analyzed in the “WAV” ﬁle format.
The details of the dataset are presented in Table 1.
Table 1: Replay attack dataset description.
Detail
Live-human
Attack
Participants
# Data
# Devices
Speakers
Recording mics
# Conﬁgurations
Our dataset
ASVspoof
10,209
244,964
120
15
12
33
3,565
14,465
42
26
25
125
6.3 ASVspoof 2017 dataset
We also evaluated Void’s performance against an online replay
attack database referred to as the “2017 ASVspoof Challenge
dataset,” which was created to facilitate an online competition
for detecting voice spooﬁng attacks [8]. The entire dataset
(all three sets combined) contains voice samples collected
through 177 replay attack sessions, where each session con-
sists of voice samples that were recorded under varying replay
USENIX Association