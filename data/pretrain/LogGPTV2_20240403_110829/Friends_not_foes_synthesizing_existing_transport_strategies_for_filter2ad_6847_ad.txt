these two aspects.
Rate Control: A PASE source uses the Rref and the
P rioQue assigned by the arbitrators to guide its transmis-
sion rate. The rate control uses congestion window (cwnd)
adjustment, based on the Rref and the ﬂow RTT, to achieve
the average reference rate at RTT timescales.
Algorithm 2 describes the rate control mechanism in
PASE. For the ﬂows mapped to the top queue, the con-
gestion window is set to Rref × RT T in order to reﬂect the
reference rate assigned by the arbitrator. For all other ﬂows,
the congestion window is set to one packet. Note that for
ﬂows belonging to the top queue, the reference rate Rref is
generally equal to the access link capacity unless ﬂows have
smaller sizes.
For the ﬂows mapped to lower priority queues (except
the bottom queue), the subsequent increase or decrease in
congestion window cwnd is based on the well-studied control
laws of DCTCP [11]. In particular, when an unmarked ACK
is received, the window size is increased as
cwnd = cwnd + 1/cwnd.
(1)
When a marked ACK (i.e., with ECN-Echo ﬂag set) is re-
ceived, the window size is reduced as
cwnd = cwnd × (1 − α/2)
(2)
where α is the weighted average of the fraction of marked
packets. This self-adjusting behavior for higher priority
queues is important for ensuring high fabric utilization at
all times because the Rref may not be accurate and there
may be spare capacity or congestion along the path.
For the ﬂows mapped to the bottom queue, the window
size always remains one. This is because under high loads
all ﬂows that cannot make it to the top queues are mapped
to the bottom queue, so the load on the bottom queue can
be usually high.
Loss Recovery: For ﬂows belonging to the top queue, we
use existing loss recovery mechanisms (i.e., timeout based
retransmissions). However, ﬂows that get mapped to the
lower priority queues can timeout for two reasons: (a) their
packet(s) could not be transmitted because higher priority
queues kept the link fully utilized and (b) a packet was lost.
In case of scenario (a), a sender should avoid sending any
new packets into the network as it increases the buﬀer occu-
pancy and the likelihood of packet losses especially at high
network loads. In case of scenario (b), a sender should re-
transmit the lost packet as early as it is possible so that
the ﬂows can make use of any available capacity without
Algorithm 2 Rate Control
Input: 
Output:  // congestion window
// Priority queues q1, q2,.., qk where q1 is the highest pri-
ority queue and q2, q3, and qk−1 are intermediate queues
// if an ACK with the ECN-Echo ﬂag set is received
if ACKmarked == 1 then
cwnd = cwnd × (1 − α/2); // Use DCTCP decrease law
else
if P rioQue == q1 then
cwnd = Rref × RT T ;
isInterQueue = 0; // not an intermediate queue
else if P rioQue ∈ {q2, q3, .., qk−1} then
// if already mapped to an intermediate queue
if isInterQueue == 1 then
cwnd = 1 + 1/cwnd; // Use DCTCP increase law
else
isInterQueue = 1; cwnd = 1;
end if
else if P rioQue == qk then
cwnd = 1; isInterQueue = 0;
end if
end if
under-utilizing the network. However, diﬀerentiating be-
tween these two scenarios is a challenging task without in-
curring any additional overhead.
We use small probe packets instead of retransmitting the
entire packet whenever a ﬂow, mapped to one of the lower
priority queues, times out.
If we receive an ACK for the
probe packet (but not the original packet), it is a sign that
the data packet was lost, so we retransmit the original data
packet.
If the probe packet is also delayed (no ACK re-
ceived), we increase our timeout value (and resend another
probe) just like we do this for successive losses for data pack-
ets. An alternative approach is to set suitable timeout val-
ues: smaller values for ﬂows belonging to the top queue and
larger values for the other ﬂows.
Finally, a related issue is that of packet reordering. When
a ﬂow from a low priority queue gets mapped to a higher pri-
ority queue, packet re-ordering may occur as earlier packets
may be buﬀered at the former queue. This can lead to un-
necessary backoﬀs which degrades throughput. To address
this, we ensure that when a ﬂow moves to a higher priority
queue, we wait for the ACKs of already sent packets to be
received before sending packets with the updated priority.
3.3
Implementation
We implement PASE on a small-scale Linux testbed as
well as in the ns2 simulator. Our ns2 implementation sup-
ports all the optimizations discussed in Section 3 whereas
our testbed implementation only supports the basic arbitra-
tion algorithm (described in Section 3.1.1). For the testbed
evaluation, we implement the transport protocol and the
arbitration logic as Linux kernel modules. The Linux trans-
port module, which is built on top of DCTCP, communicates
with the arbitration module to obtain the packet priority
and the reference rate. It then adds the PASE header on
outgoing packets. For the desired switch support, we use
the PRIO [7] queue and CBQ (class based queueing) im-
plementation, on top of the RED queue implementation in
Linux and ns2. We use eight priority queues/classes and
Parameters
Scheme
DCTCP qSize = 225 pkts
D2TCP markingThresh = 65
L2DCT minRTO = 10 ms
pFabric
PASE
qSize = 76 pkts (= 2×BDP)
initCwnd = 38 pkts (= BDP)
minRTO = 1 ms (∼3.3×RTT)
qSize = 500 pkts
minRTO (ﬂows in top queue) = 10 ms
minRTO (ﬂows in other queues) = 200 ms
numQue = 8
Figure 8: Baseline topology used in simulations.
Note that L = 40 hosts.
classify packets based on the ToS ﬁeld in the IP header.
Out of these eight queues, a separate, strictly lower priority
queue is maintained for background traﬃc. For the RED
queue, we set the low and high thresholds of RED queue to
K and perform marking based on the instantaneous rather
than the average queue length as done in DCTCP [11].
4. EVALUATION
In this section we evaluate the performance of PASE using
the ns2 simulator [6] as well as through small-scale testbed
experiments. First, we conduct macro-benchmark experi-
ments in ns2 to compare PASE’s performance against ex-
isting data center transports. We compare PASE against
both deployment-friendly protocols including DCTCP [11],
D2TCP [23], and L2DCT [22] (§4.2.1) as well as the best per-
forming transport, namely, pFabric [12] (§4.2.2). Second, we
micro-benchmark the internal working of PASE (e.g., ben-
eﬁts of arbitration optimizations, use of reference rate, etc)
using simulations (§4.3). Finally, we report evaluation re-
sults for our testbed experiments (§4.4).
4.1 Simulation Settings
We now describe our simulation settings including the
data center topology, traﬃc workloads, performance met-
rics, and the protocols compared.
Data center Topology: We use a 3-tier topology for our
evaluation comprising layers of ToR (Top-of-Rack) switches,
aggregation switches, and a core switch as shown in Figure
8. This is a commonly used topology in data centers [9, 23,
24]. The topology interconnects 160 hosts through 4 ToR
switches that are connected to 2 aggregation switches, which
in turn are interconnected via a core switch. Each host-ToR
link has a capacity of 1 Gbps whereas all other links are of
10 Gbps. This results in an oversubscription ratio of 4:1 for
the uplink from the ToR switches. In all the experiments,
the arbitrators are co-located with their respective switches.
The end-to-end round-trip propagation delay (in the absence
of queueing) between hosts via the core switch is 300µs.
Traﬃc Workloads: We consider traﬃc workloads that
are derived from patterns observed in production data cen-
ters. Flows arrive according to a Poisson process and ﬂow
sizes are drawn from the interval [2 KB, 198 KB] using a
uniform distribution, as done in prior studies [18, 24]. This
represents query traﬃc and latency sensitive short messages
in data center networks. In addition to these ﬂows, we gener-
ate two long-lived ﬂows in the background, which represents
the 75th percentile of multiplexing in data centers [11]. Note
that we always use these settings unless speciﬁed otherwise.
Table 3: Default simulation parameter settings.
We consider two kinds of ﬂows: deadline-constrained ﬂows
and deadline-unconstrained ﬂows. They cover typical appli-
cation requirements in today’s data centers [23].
Protocols Compared: We compare PASE with several
data center transports including DCTCP [11], D2TCP [23],
L2DCT [22], and pFabric [12]. We implemented DCTCP,
D2TCP and L2DCT in ns2 and use the source code of pFab-
ric provided by the authors to evaluate their scheme. The
parameters of these protocols are set according to the rec-
ommendations provided by the authors, or reﬂect the best
settings, which we determined experimentally (see Table 3).
Performance Metrics: For traﬃc without any dead-
lines, we use the FCT as a metric. We consider the AFCT as
well as the 99th percentile FCT for small ﬂows. For deadline-
constrained traﬃc, we use application throughput as our
metric which is deﬁned as the fraction of ﬂows that meet
their deadlines. We use the control messages per second to
quantify the arbitration overhead.
4.2 Macro-benchmarks
4.2.1 Comparison with Deployment
Schemes
Friendly
PASE is a deployment friendly transport that does not re-
quire any changes to the network fabric. Therefore, we now
compare PASE’s performance with deployment friendly data
center transports, namely, DCTCP and L2DCT. DCTCP
[11] is a fair sharing protocol that uses ECN marks to infer
the degree of congestion and employs adaptive backoﬀ fac-
tors to maintain small queues. The goal of L2DCT [22] is to
minimize FCT – it builds on DCTCP by prioritizing short
ﬂows over long ﬂows through the use of adaptive control
laws that depend on the size of the ﬂows.
Deadline-unconstrained ﬂows: We consider an inter-
rack communication scenario (termed as left-right) where 80
hosts in the left subtree of the core switch generate traﬃc
towards hosts in the right subtree. This is a common sce-
nario in user-facing web services where the front-end servers
and the back-end storage reside in separate racks [25]. The
generated traﬃc comprises ﬂows with sizes drawn from the
interval [2 KB, 198 KB] using a uniform distribution. In ad-
dition, we generate two long-lived ﬂows in the background.
Figure 9(a) shows the improvement in AFCT as a function
of network load. Observe that PASE outperforms L2DCT
and DCTCP by at least 50% and 70%, respectively across
a wide range of loads. At low loads, PASE performs better
primarily because of its quick convergence to the correct rate
for each ﬂow. At higher loads, PASE ensures that shorter
ﬂows are strictly prioritized over long ﬂows, whereas with
L2DCT, all ﬂows, irrespective of their priorities, continue
to send at least one packet into the network. This lack
Aggregation (10Gbps links) Core (10Gbps links) ToR (1Gbps links) L Hosts L Hosts L Hosts L Hosts (a) AFCT
(b) CDF of FCTs
(c) Application Throughput
Figure 9: (a) Comparison of PASE with L2DCT and DCTCP in terms of AFCTs under the (left-right) inter-
rack scenario, (b) shows the CDF of FCTs at 70% load in case of (a), and (c) Deadline-constrained ﬂows:
Comparison of application throughput for PASE with D2TCP and DCTCP under the intra-rack scenario.
of support for strict priority scheduling in L2DCT leads to
larger FCTs for short ﬂows. DCTCP does not prioritize
short ﬂows over long ﬂows. Thus, it results in worst FCTs
across all protocols.
Figure 9(b) shows the CDF of FCTs at 70% load. Ob-
serve that PASE results in better FCTs for almost all ﬂows
compared to L2DCT and DCTCP.
Deadline-constrained ﬂows: We now consider latency-
sensitive ﬂows that have speciﬁc deadlines associated with
them. Thus, we compare PASE’s performance with D2TCP,
a deadline-aware transport protocol. We replicate the
D2TCP experiment from [23] (also described earlier in §2)
which considers an intra-rack scenario with 20 machines and
generate short query traﬃc with ﬂow sizes drawn from the
interval [100 KB, 500 KB] using a uniform distribution. Fig-
ure 9(c) shows the application throughput as a function of
network load. PASE signiﬁcantly outperforms D2TCP and
DCTCP, especially at high loads because of the large num-
ber of active ﬂows in the network. Since each D2TCP and
DCTCP ﬂow sends at least one packet per RTT, these ﬂows
consume signiﬁcant network capacity which makes it diﬃ-
cult for a large fraction of ﬂows to meet their respective
deadlines. PASE, on the other hand, ensures that ﬂows
with the earliest deadlines are given the desired rates and
are strictly prioritized inside the switches.
4.2.2 Comparison with Best Performing Scheme
We now compare the performance of PASE with pFab-
ric, which achieves close to optimal performance in several
scenarios but requires changes in switches. With pFabric,
packets carry a priority number that is set independently by
each ﬂow. Based on this, pFabric switches perform priority-
based scheduling and dropping. All ﬂows start at the line
rate and backoﬀ only under persistent packet loss.
PASE performs better than pFabric in two important sce-
narios:
(a) multi-link (single rack) scenarios with all-to-
all traﬃc patterns and (b) at high loads (generally > 80%)