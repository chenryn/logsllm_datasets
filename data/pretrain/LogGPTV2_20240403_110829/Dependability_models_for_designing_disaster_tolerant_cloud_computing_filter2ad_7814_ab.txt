### Guard Expressions for Transmission Component

**Table IV: Transition Conditions**

| Transition | Condition |
|------------|-----------|
| **TRI 12** | \((\#OSPM\ UP1 + \#OSPM\ UP2) = 0\) AND NOT \((\#OSPM\ UP3 + \#OSPM\ UP4) = 0\) OR \(\#NAS\ NET\ UP2 = 0\) OR \(\#DC\ UP2 = 0\) |
| **TRI 21** | \((\#OSPM\ UP3 + \#OSPM\ UP4) = 0\) AND NOT \((\#OSPM\ UP1 + \#OSPM\ UP2) = 0\) OR \(\#NAS\ NET\ UP1 = 0\) OR \(\#DC\ UP2 = 1\) |
| **TBI 12** | \(\#BKP\ UP = 1\) AND \((\#NAS\ NET\ UP1 = 0\) OR \(\#DC\ UP1 = 0)\) AND NOT \((\#OSPM\ UP3 + \#OSPM\ UP4) = 0\) OR \(\#NAS\ NET\ UP2 = 0\) OR \(\#DC\ UP2 = 0\) |
| **TBI 21** | \(\#BKP\ UP = 1\) AND \((\#NAS\ NET\ UP2 = 0\) OR \(\#DC\ UP2 = 0)\) AND NOT \((\#OSPM\ UP1 + \#OSPM\ UP2) = 0\) OR \(\#NAS\ NET\ UP1 = 0\) OR \(\#DC\ UP1 = 0\) |

### Mean Time to Transmit (MTT)

The mean time to transmit (MTT) represents the average time required to transfer a virtual machine (VM) from one location to another. The MTT is influenced by the physical link speed, the distance between data centers, and the VM size. In this context, there are three MTTs:

- **MTT DCS**: Mean time to transmit a VM from one data center to another.
- **MTT BK1**: Mean time to transfer the VM image from the Backup Server to Data Center 1.
- **MTT BK2**: Mean time to transfer the VM image from the Backup Server to Data Center 2.

**Table V: Transmission Component Transition Attributes**

| Transition Type | Delay Markup | Concurrency |
|-----------------|--------------|-------------|
| **TRE 21**      | exp MTT DCS  | constant    |
| **TRE 12**      | exp MTT DCS  | constant    |
| **TBE 21**      | exp MTT BK1  | constant    |
| **TBE 12**      | exp MTT BK2  | constant    |

### Hierarchical Modeling

The modeling process begins with the evaluation of lower-level submodels, followed by the application of their results to higher-level models. For example, Figure 5 shows a Reliability Block Diagram (RBD) model where the operating system (OS) and the physical machine (PM) are in series. The Mean Time To Repair (MTTR) and Mean Time To Failure (MTTF) values estimated from the RBD [16] are associated with the transitions OSPM R and OSPM F, respectively, in the Stochastic Petri Net (SPN) model depicted in Figure 5(b).

The modeling approach includes RBD models for representing the physical machine (OS PM) and the data center network infrastructure (NAS NET). The respective MTTFs and MTTRs are estimated and used in SIMPLE COMPONENT models (Section IV-A). For OS PM, the components are the OS and the physical machine, assumed to be in a series arrangement. Similarly, NAS NET considers switches, routers, and distributed storage in a series arrangement. This section also assumes the use of composition rules (e.g., net union), and readers are referred to [17] for detailed information.

**Figure 5: RBD and SPN Models for Operating System and Physical Machine**

- **(a) RBD Model**
- **(b) SPN Model**

### SPN Model - Cloud System with Multiple Data Centers

This section describes a system based on Section III, deployed across two data centers, each with two physical machines (PMs) and up to two VMs per machine (N=4). Figure 6 presents the model, which includes VM BEHAVIOR submodels and simple components. OSPM 1 and OSPM 2 represent the PMs in Data Center 1, while OSPM 3 and OSPM 4 represent the PMs in Data Center 2. DISASTER1 and DISASTER2 model disasters in Data Centers 1 and 2, respectively. NAS NET 1 and NAS NET 2 correspond to the network devices in Data Centers 1 and 2.

In this model, the dynamic behavior of the VMs is modeled using a transmission component (TRANSMISSION COMPONENT) and VM BEHAVIOR components. The expression \(P\{(\#VM\ UP1 + \#VM\ UP2 + \#VM\ UP3 + \#VM\ UP4) = j\}\) is used to estimate availability, where \(j\) represents the number of VMs required to provide the service.

**Figure 6: SPN Model Representing Four Physical Machines in Two Different Data Centers**

### Case Studies

To illustrate the feasibility of the proposed approach, we present a case study involving cloud system scenarios deployed in two different data centers. The availability evaluation considers:

- Distance between data centers
- Network speeds
- Disaster mean time

The data centers are located in the following pairs of cities:
- Rio de Janeiro (Brazil) - Brasilia (Brazil)
- Rio de Janeiro - Recife (Brazil)
- Rio de Janeiro - New York (USA)
- Rio de Janeiro - Calcutta (India)
- Rio de Janeiro - Tokyo (Japan)

The Backup Server is located in São Paulo (Brazil).

To estimate the MTT value, we used the approach presented in [18], which assesses network throughput based on the distance between communication nodes. The equation associates a constant \(\alpha\) with the network speed, ranging from 0 (no connection) to 1.0 (fastest connection). We considered the following values for \(\alpha\): 0.35, 0.40, and 0.45. At least two running VMs are required for the system to be operational, and the size of each VM is 4GB.

The disaster mean time values used are 100, 200, and 300 years, with a data center taking one year to recover. A VM takes five minutes to start.

**Table VI: Dependability Parameters for Components of Figure 1**

| Component | MTTF (h) | MTTR (h) |
|-----------|----------|----------|
| Operating System (OS) | 4000 | 1000 |
| Hardware of Physical Machine (PM) | 430000 | 14077473 |
| Switch | 20000000 | 2880 |
| Router | 50000 | 1 |
| NAS | 12 | 4 |
| VM | 4 | 2 |
| Backup Server | 0.5 | 0.5 |

**Table VII: Availability Values for Baseline Architectures**

| Architecture | Availability | Number of Nines |
|--------------|--------------|-----------------|
| Cloud system with one machine | 0.9842914 | 1.80 |
| Cloud system with two machines in one data center | 0.9899101 | 1.99 |
| Cloud system with four machines in one data center | 0.9900631 | 2.00 |
| Baseline architecture: Rio de Janeiro - Brasilia | 0.9997317 | 3.57 |
| Baseline architecture: Rio de Janeiro - Recife | 0.9995968 | 3.39 |
| Baseline architecture: Rio de Janeiro - New York | 0.9987753 | 2.91 |
| Baseline architecture: Rio de Janeiro - Calcutta | 0.9977486 | 2.64 |
| Baseline architecture: Rio de Janeiro - Tokyo | 0.9972643 | 2.56 |

**Figure 7: Availability of Different Distributed Cloud Configurations**

### Conclusion

Figure 7 shows the availability results for each different configuration. The baseline architectures are systems with geographically distributed data centers. This work presents models for dependability evaluation of cloud computing systems deployed in geographically distributed data centers, considering disaster occurrence. The approach uses a hybrid modeling technique that combines combinatorial and state-based models. The proposed technique allows the assessment of the impact of disaster occurrence, VM migration, and data center distance on system dependability.

Additionally, a case study is provided, considering data centers located in different places around the world. The results demonstrate the influence of distance, network speed, and disaster occurrence on system availability. Future research will focus on assessing performance metrics using the proposed method.

### References

[1] M. Armbrust, A. Fox, R. Griffith, A. D. Joseph, R. Katz, A. Konwinski, G. Lee, D. Patterson, A. Rabkin, I. Stoica, and M. Zaharia, “A view of cloud computing,” Commun. ACM, vol. 53, no. 4, pp. 50–58, Apr. 2010.

[2] D. A. Menasc and P. Ngo, “Understanding cloud computing: Experimentation and capacity planning,” 2009.

[3] Q. Zhang, L. Cheng, and R. Boutaba, “Cloud computing: state-of-the-art and research challenges,” Journal of Internet Services and Applications, vol. 1, pp. 7–18, 2010.

[4] Amazon EC2. [Online]. Available: http://aws.amazon.com/ec2

[5] IBM Smart Business Cloud. [Online]. Available: http://www-935.ibm.com/services/us/igs/cloud-development/

[6] F. Longo, R. Ghosh, V. Naik, and K. Trivedi, “A scalable availability model for infrastructure-as-a-service cloud,” in Dependable Systems Networks (DSN), 2011 IEEE/IFIP 41st International Conference on, June 2011, pp. 335–346.

[7] Hyper-V Live Migration over Distance. [Online]. Available: http://www.hds.com/assets/pdf/hyper-v-live-migration-over-distance-reference-architecture-guide.pdf

[8] P. Maciel, K. S. Trivedi, R. Matias, and D. S. Kim, Performance and Dependability in Service Computing: Concepts, Techniques and Research Directions, ser. Premier Reference Source. IGI Global, 2011, ch. Dependability Modeling.

[9] R. Ghosh, K. S. Trivedi, V. K. Naik, and D. S. Kim, “End-to-end performability analysis for infrastructure-as-a-service cloud: An interacting stochastic models approach,” in Proceedings of the 2010 IEEE 16th Pacific Rim International Symposium on Dependable Computing, ser. PRDC ’10. Washington, DC, USA: IEEE Computer Society, 2010, pp. 125–132.

[10] J. Araujo, R. Matos, P. Maciel, R. Matias, and I. Beicker, “Experimental evaluation of software aging effects on the Eucalyptus cloud computing infrastructure,” in Proceedings of the Middleware 2011 Industry Track Workshop, ser. Middleware ’11. New York, NY, USA: ACM, 2011, pp. 4:1–4:7.

[11] R. Bradford, E. Kotsovinos, A. Feldmann, and H. Schiöberg, “Live wide-area migration of virtual machines including local persistent state,” in Proceedings of the 3rd International Conference on Virtual Execution Environments, ser. VEE ’07. New York, NY, USA: ACM, 2007, pp. 169–179.

[12] W. Voorsluys, J. Broberg, S. Venugopal, and R. Buyya, “Cost of virtual machine live migration in clouds: A performance evaluation,” in Proceedings of the 1st International Conference on Cloud Computing, ser. CloudCom ’09. Berlin, Heidelberg: Springer-Verlag, 2009, pp. 254–265.

[13] J. Dantas, R. Matos, J. Araujo, and P. Maciel, “An availability model for Eucalyptus platform: An analysis of warm-standby replication mechanism,” in Systems, Man, and Cybernetics (SMC), 2012 IEEE International Conference on, Oct. 2012, pp. 1664–1669.

[14] C. Clark, K. Fraser, S. Hand, J. G. Hansen, E. Jul, C. Limpach, I. Pratt, and A. Warfield, “Live migration of virtual machines,” in Proceedings of the 2nd Conference on Symposium on Networked Systems Design & Implementation - Volume 2, ser. NSDI’05. Berkeley, CA, USA: USENIX Association, 2005, pp. 273–286.

[15] R. German, Performance Analysis of Communication Systems with Non-Markovian Stochastic Petri Nets. New York, NY, USA: John Wiley & Sons, Inc., 2000.

[16] C. Ebeling, An Introduction to Reliability and Maintainability Engineering. Waveland Press, 1997.

[17] G. de Albuquerque, P. Maciel, R. Lima, and A. Zimmermann, “Automatic modeling for performance evaluation of inventory and outbound distribution,” Systems, Man and Cybernetics, Part A: Systems and Humans, IEEE Transactions on, vol. 40, no. 5, pp. 1025–1044, Sept. 2010.

[18] W. M. Les Cottrell and C. Logg, “Tutorial on internet monitoring and pinger at SLAC,” Tech. Rep., 1996. [Online]. Available: http://www.slac.stanford.edu/comp/net/wan-mon/tutorial.html

[19] D. S. Kim, F. Machida, and K. S. Trivedi, “Availability modeling and analysis of a virtualized system,” in Proceedings of the 2009 15th IEEE Pacific Rim International Symposium on Dependable Computing, ser. PRDC ’09. Washington, DC, USA: IEEE Computer Society, 2009, pp. 365–371.

[20] (2012, Oct.) Cisco Systems: Switch dependability parameters. [Online]. Available: http://tinyurl.com/cr9nssu

[21] (2012, Oct.) Cisco Systems: Router dependability parameters. [Online]. Available: http://tinyurl.com/d7kcnqo

[22] (2012, Oct.) Service Level Agreement - Megapath Business Access and Value Added Services. [Online]. Available: http://tinyurl.com/cwdeebt

[23] B. Silva, G. Callou, E. Tavares, P. Maciel, J. Figueiredo, E. Sousa, C. Araujo, F. Magnani, and F. Neves, “ASTRO: An integrated environment for dependability and sustainability evaluation,” Sustainable Computing: Informatics and Systems, no. 0, pp. –, 2012.

[24] R. German, C. Kelling, A. Zimmermann, and G. Hommel, “TimeNET: A toolkit for evaluating non-Markovian stochastic Petri nets,” Performance Evaluation, vol. 24, no. 1-2, pp. 69–87.