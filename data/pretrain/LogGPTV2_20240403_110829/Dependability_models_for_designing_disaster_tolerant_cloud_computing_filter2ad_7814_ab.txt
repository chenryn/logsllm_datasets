presents the guard expressions of transmission TRANSMIS-
SION COMPONENT.
GUARD EXPRESSIONS FOR TRANSMISSION COMPONENT.
Table IV
Transition
Condition
TRI 12
TRI 21
TBI 12
TBI 21
((#OSPM UP1+#OSPM UP2)=0) AND NOT ((#OSPM UP3
+ #OSPM UP4)=0 OR #NAS NET UP2=0 OR #DC UP2=0)
((#OSPM UP3+#OSPM UP4)=0) AND NOT ((#OSPM UP1
+ #OSPM UP2)=0 OR #NAS NET UP1=0 OR #DC UP2=1)
#BKP UP=1 AND (#NAS NET UP1=0 OR #DC UP1=0) AND
NOT((#OSPM UP3+#OSPM UP4)=0 OR #NAS NET UP2=0
OR #DC UP2=0)
#BKP UP=1 AND (#NAS NET UP2=0 OR #DC UP2=0) AND
NOT((#OSPM UP1+#OSPM UP1)=0 OR #NAS NET UP1=0
OR #DC UP1=0)
The mean time to transmit (MTT) symbolizes the mean
time to transmit one virtual machine from one location to
another. The MTT depends on the physical link speed, the
distance between the data centers and the VM size. In this
block, there are three MTTs: mean time to transmit a VM
from the data center to another (MTT DCS) and the mean
times to transfer the VM image from Backup Server to Data
Centers 1 and 2 (MTT BK1 and MTT BK2). Table V depicts
the attributes related to TRANSMISSION COMPONENT ex-
ponential transitions.
TRANSMISSION COMPONENT TRANSITION ATTRIBUTES.
Table V
Transition Type Delay Markup Concurrency
TRE 21
TRE 12
TBE 21
TBE 12
exp MTT DCS constant
exp MTT DCS constant
exp MTT BK1 constant
exp MTT BK2 constant
ss
ss
ss
ss
D. Hierarchical modeling
The adopted modeling process considers ﬁrst the evalua-
tion of lower-level submodels, then the respective results are
applied to higher-level models. For instance, Figure 5 depicts
a RBD model, such that the operating system (OS) and the
physical machine (PM) are in series arrangement. MTTR and
MTTF results estimated from the RBD [16] are associated
to transitions OSPM R and OSPM F, respectively, of the
SPN model depicted in Figure 5(b).
The modeling approach contemplates RBD models for
representing the physical machine (OS PM) as well as the
data center network infrastructure (NAS NET), such that the
respective MTTFs and MTTRs are estimated and utilized in
SIMPLE COMPONENT models (Section IV-A). Consider-
ing OS PM, the components are OS and physical machine
and a series relation is assumed. Similarly, NAS NET con-
templates switch, router and distributed storage considering
a series arrangement. Furthermore, this section assumes the
adoption of some composition rules (e.g., net union), and
the reader refers to [17] for detailed information.
OSPM_UP
OSPM_R
OSPM_F
(a) RBD model
OSPM_DOWN
(b) SPN model
Figure 5. RBD model and the respective SPN model for operating system
and physical machine.
E. SPN Model - Cloud system with multiple data centers.
This section assumes a system based on Section III
deployed into two data centers each with two PMs and up to
two VMs per machine (N=4). Figure 6 presents the model,
which is composed of VM BEHAVIOR submodels as well
simple components. OSPM 1 and OSPM 2 represent the
physical machines of Data Center 1, and OSPM 3 as well
as OSPM 4 are the models related to PMs of Data Center
2. DISASTER1 and DISASTER2 models disasters in Data
Centers 1 and 2, respectively. NAS NET 1 and NAS NET 2
corresponds to network devices of Data Center 1 and 2.
In this model, the dynamic behavior of the virtual ma-
chines is modelled by a transmission component (TRANS-
MISSION COMPONENT) and VM BEHAVIOR compo-
nents. The expression P{(#VM UP1 + #VM UP2 +
#VM UP3 + #VM UP4)=j} is adopted to estimate avail-
ability, in which j represents the amount of virtual machines
that are required to provide the service.
V. CASE STUDIES.
To illustrate the feasibility of the proposed approach, we
present a case study considering a set of cloud system sce-
narios in which the systems are deployed into two different
data centers. We have conducted an availability evaluation
considering (i) distance between data centers, (ii) network
speeds and (iii) disaster mean time.
The data centers are located in the following pairs of
cities: Rio de Janeiro (Brazil)-Brasilia (Brazil), Rio de
Janeiro-Recife (Brazil), Rio de Janeiro-NewYork (USA), Rio
de Janeiro-Calcutta (India) and Rio de Janeiro-Tokio (Japan).
We assume that the Backup Server is located in S˜ao Paulo
(Brazil).
To estimate the MTT value, we considered the approach
presented in [18] that assess the network throughput based
on the distance between the communication nodes. The
equation associates a constant α with the network speed,
which can vary from 0 (no connection) up to 1.0 (fastest
connection). We have considered the following values for
α: 0.35, 0.40 and 0,45. We assume that it is necessary at
least two running VMs to consider the system operational
and the size of VMs is 4GB.
The disaster mean time values utilized are 100, 200
and 300 years and a data center takes one year to be
recovered. Moreover, a VM takes ﬁve minutes to start.
Table VI presents the dependability parameters associated
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:40:53 UTC from IEEE Xplore.  Restrictions apply. 
OSPM1
OSPM2
DISASTER1
NAS_NET_1
BACKUP
NAS_NET_2
DISASTER2
OSPM3
OSPM4
DATA_CENTER_1
DATA_CENTER_2
TRE_21
TRF_P1
TRI_21
TRI_12
TRE_12
TBE_21
TBI_21
TBI_12
TBE_12
TRANSMISSION_COMPONENT
Figure 6. SPN model representing four physical machines in two different data centers
with the devices, which were taken from [19], [20], [21],
[22]. Mercury [23] and TimeNET [24] tools have been
adopted to perform the evaluation.
DEPENDABILITY PARAMETERS FOR COMPONENTS OF FIGURE 1.
Table VI
Component
Operating System (OS)
Hardware of Physical Machine (PM)
Switch
Router
NAS
VM
Backup Server
MTTF(h) MTTR(h)
4000
1000
430000
14077473
20000000
2880
50000
1
12
4
4
2
0.5
0.5
α = 0.35 and disaster mean time = 100 years. The results
are presented in terms of improvement of number of nines,
which is calculated by expression nines = −log[1 − A]
(A corresponds to availability). The results show that the
higher availability scenario corresponds to a system with
data centers in Rio de Janeiro - Brasilia, alpha = 0.45 and
disaster mean time = 300 years. We can also observe smaller
distances and disaster mean time signiﬁcantly affects the
availability. If larger distances are considered, the availability
is mostly impacted by the network speed.
AVAILABILITY VALUES FOR THE BASELINE ARCHITECTURES.
Table VII
Architecture
Cloud system with one machine
Cloud system with two machines in one data center
Cloud system with four machines in one data center
Baseline architecture: Rio de janeiro - Brasilia
Baseline architecture: Rio de janeiro - Recife
Baseline architecture: Rio de janeiro - NewYork
Baseline architecture: Rio de janeiro - Calcutta
Baseline architecture: Rio de janeiro - Tokio
Availability Number of nines
0.9842914
0.9899101
0.9900631
0.9997317
0.9995968
0.9987753
0.9977486
0.9972643
1.80
1.99
2.00
3.57
3.39
2.91
2.64
2.56
Table VII compares availability values of baseline archi-
tectures with non geographically distributed cloud systems
with the same basic components. In this table, the system
with more distant data centers has better availability than
the scenario with the same number of machines in a single
data center.
Figure 7. Availability of different ditributed cloud conﬁgurations
VI. CONCLUSION
Figure 7 shows availability results for each different con-
ﬁguration. The baseline architectures are the systems with
This work presented models for dependability evaluation
of cloud computing systems deployed into geographically
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:40:53 UTC from IEEE Xplore.  Restrictions apply. 
distributed data centers as well as taking into account
disaster occurrence. The approach is based on a hybrid mod-
eling technique, which considers combinatorial and state-
based models. The proposed technique allows the impact
assessment of disaster occurrence, VM migration and data
center distance on system dependability.
Additionally, a case study is provided considering a set
data centers located in different places around the world.
The results demonstrated the inﬂuence of distance, network
speed and disaster occurrence on system availability. As
future research, we intend to assess performance metrics in
the proposed method.
REFERENCES
[1] M. Armbrust, A. Fox, R. Grifﬁth, A. D. Joseph, R. Katz,
A. Konwinski, G. Lee, D. Patterson, A. Rabkin, I. Stoica, and
M. Zaharia, “A view of cloud computing,” Commun. ACM,
vol. 53, no. 4, pp. 50–58, Apr. 2010.
[2] D. A. Menasc and P. Ngo, “Understanding cloud computing:
Experimentation and capacity planning,” 2009.
[3] Q. Zhang, L. Cheng, and R. Boutaba, “Cloud computing:
state-of-the-art and research challenges,” Journal of Internet
Services and Applications, vol. 1, pp. 7–18, 2010.
[4] Amazon ec2. [Online]. Available: http://aws.amazon.com/ec2
[5] Ibm smart business cloud. [Online]. Available: http://www-
935.ibm.com/services/us/igs/cloud-development/
[6] F. Longo, R. Ghosh, V. Naik, and K. Trivedi, “A scalable
availability model for infrastructure-as-a-service cloud,” in
Dependable Systems Networks (DSN), 2011 IEEE/IFIP 41st
International Conference on, june 2011, pp. 335 –346.
[7] Hyper-V Live Migration over Distance.
[Online]. Avail-
able: http://www.hds.com/assets/pdf/hyper-v-live-migration-
over-distance-reference-architecture-guide.pdf
[8] P. Maciel, K. S. Trivedi, R. Matias, and D. S. Kim, Perfor-
mance and Dependability in Service Computing: Concepts,
Techniques and Research Directions, ser. Premier Reference
Source.
Igi Global, 2011, ch. Dependability Modeling.
[9] R. Ghosh, K. S. Trivedi, V. K. Naik, and D. S. Kim,
“End-to-end performability analysis for infrastructure-as-a-
service cloud: An interacting stochastic models approach,”
in Proceedings of the 2010 IEEE 16th Paciﬁc Rim Interna-
tional Symposium on Dependable Computing, ser. PRDC ’10.
Washington, DC, USA: IEEE Computer Society, 2010, pp.
125–132.
[10] J. Araujo, R. Matos, P. Maciel, R. Matias, and I. Beicker,
“Experimental evaluation of software aging effects on the
eucalyptus cloud computing infrastructure,” in Proceedings
of the Middleware 2011 Industry Track Workshop, ser. Mid-
dleware ’11. New York, NY, USA: ACM, 2011, pp. 4:1–4:7.
[11] R. Bradford, E. Kotsovinos, A. Feldmann, and H. Schi¨oberg,
“Live wide-area migration of virtual machines including local
persistent state,” in Proceedings of
the 3rd international
conference on Virtual execution environments, ser. VEE ’07.
New York, NY, USA: ACM, 2007, pp. 169–179.
[12] W. Voorsluys, J. Broberg, S. Venugopal, and R. Buyya, “Cost
of virtual machine live migration in clouds: A performance
evaluation,” in Proceedings of the 1st International Confer-
ence on Cloud Computing, ser. CloudCom ’09.
Berlin,
Heidelberg: Springer-Verlag, 2009, pp. 254–265.
[13] J. Dantas, R. Matos, J. Araujo, and P. Maciel, “An availability
model for eucalyptus platform: An analysis of warm-standy
replication mechanism,” in Systems, Man, and Cybernetics
(SMC), 2012 IEEE International Conference on, oct. 2012,
pp. 1664 –1669.
[14] C. Clark, K. Fraser, S. Hand, J. G. Hansen, E. Jul,
C. Limpach, I. Pratt, and A. Warﬁeld, “Live migration of
virtual machines,” in Proceedings of the 2nd conference on
Symposium on Networked Systems Design & Implementation
- Volume 2, ser. NSDI’05. Berkeley, CA, USA: USENIX
Association, 2005, pp. 273–286.
[15] R. German, Performance Analysis of Communication Systems
with Non-Markovian Stochastic Petri Nets. New York, NY,
USA: John Wiley & Sons, Inc., 2000.
[16] C. Ebeling, An Introduction to Reliability and Maintainability
Engineering. Waveland Press, 1997.
[17] G. de Albuquerque, P. Maciel, R. Lima, and A. Zimmermann,
“Automatic modeling for performance evaluation of inventory
and outbound distribution,” Systems, Man and Cybernetics,
Part A: Systems and Humans, IEEE Transactions on, vol. 40,
no. 5, pp. 1025 –1044, sept. 2010.
[18] W. M. Les Cottrell and C. Logg, “Tutorial on internet
monitoring and pinger at slac,” Tech. Rep., 1996. [On-
line]. Available: http://www.slac.stanford.edu/comp/net/wan-
mon/tutorial.html
[19] D. S. Kim, F. Machida, and K. S. Trivedi, “Availability mod-
eling and analysis of a virtualized system,” in Proceedings of
the 2009 15th IEEE Paciﬁc Rim International Symposium on
Dependable Computing, ser. PRDC ’09. Washington, DC,
USA: IEEE Computer Society, 2009, pp. 365–371.
[20] (2012, Oct.) Cisco systems: Switch dependability parameters.
[Online]. Available: http://tinyurl.com/cr9nssu
[21] (2012, Oct.) Cisco systems: Router dependability parameters.
[Online]. Available: http://tinyurl.com/d7kcnqo
[22] (2012, Oct.) Service level agreement - megapath business
[Online]. Available:
access and value added services.
http://tinyurl.com/cwdeebt
[23] B. Silva, G. Callou, E. Tavares, P. Maciel, J. Figueiredo,
E. Sousa, C. Araujo, F. Magnani, and F. Neves, “Astro: An
integrated environment for dependability and sustainability
evaluation,” Sustainable Computing: Informatics and Systems,
no. 0, pp. –, 2012.
[24] R. German, C. Kelling, A. Zimmermann, and G. Hommel,
“Timenet: a toolkit for evaluating non-markovian stochastic
petri nets,” Performance Evaluation, vol. 24, no. 1-2, pp. 69
– 87.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:40:53 UTC from IEEE Xplore.  Restrictions apply.