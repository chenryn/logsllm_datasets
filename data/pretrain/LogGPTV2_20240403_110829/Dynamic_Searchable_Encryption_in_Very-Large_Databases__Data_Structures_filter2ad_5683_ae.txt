{(id, dp(id, w, Q)) : id ∈ ID, dp(id, w, Q) -= ∅}.
Intuitively, dp(id, w, Q) is the set of indices of queries that
deleted w from id, and DP(w, Q, ID) is the set of identiﬁers
form which w was deleted, along with the corresponding
deletion pattern.
•
•
•
•
On ﬁrst input DB, Ldyn initializes a counter i ← 0, empty
list Q, set ID to be identiﬁers in DB. It saves DB, i, ID, Q
as state, and outputs N = !w∈W |DB(w)|.
On search input w, Ldyn appends (i, srch, w) to Q, in-
crements i, and outputs sp(w, Q), DB(w), AP(w, Q, ID),
and DP(w, Q, ID).
input
On update
appends
(i, add/edit+, id, Wid) to Q, adds id to ID, and increments
i. It outputs add, |Wid| and the set
(add/edit+, id, Wid),
it
{(sp(w, Q), ap(id, w, Q), dp(id, w, Q)) : w ∈ Wid}.
Finally, if any of the sp(w, Q) are non-empty, then it also
outputs id.
input
update
(del/edit−, id, Wid),
appends
On
(i, del/edit−, id, Wid) to Q, adds id to ID, and increments
i. Then it computes its output exactly as in the add/edit+
case above, except that it outputs del instead of add as
the ﬁrst component.
it
The leakage on searches is minimal: It consists of all patterns
of searches, deletions, and additions that can be derived once
the server has the ability to search for a keyword and rewind
the database. For leakage on updates, the server will learn
when/if that identiﬁer has had the same keywords added or
deleted before, and also when/if the same keywords have been
searched for. This comes from observing the revid values,
which will repeat every time the same identiﬁer/keyword
pair is added or deleted. Note that,
if same keyword is
added/deleted from two documents, then this information is
not leaked until it is searched for (contrast this with [14] which
leaks this information always).
We prove the following theorem, as well as an adaptive
variant, in the full version.
Theorem 7: Πdyn
bas is correct and Ldyn-secure against non-
adaptive attacks if F is a secure PRF and (Enc, Dec) is RCPA-
secure.
10
ASYMPTOTIC ANALYSIS. To add a ﬁle the client sends one
label/ciphertext/revid per record/keyword pair being changed.
For deletions, the δ dictionary is not involved. The client
just sends one revid per document/keyword to be deleted.
Assuming the dictionaries γ, γ+, and the revocation list are
fully read-parallel, and the number of deletions is much smaller
than the size of the EDB, each search operation continues to
have the same order run-time complexity as in the basic static
construction of Figure 2.
DISCUSSION AND COMPARISON TO PRIOR WORK. Our
scheme Πdyn
is unsatisfying in some situations as it does
bas
not reclaim space after deletions. While this is a drawback,
all known dynamic SSE schemes [14], [15], [21] have se-
vere drawbacks in different dimensions, and no scheme has
achieved an ideal combination of leakage, index size, and full
functionality like reclaiming space.
The scheme of [21] has no security proof, and the scheme
of [14] has a worst-case quadratic size encrypted index.
The dynamic scheme in [15] has much more leakage than
our scheme, effectively leaking the pattern of all intersec-
tions of everything that is added or deleted, whether or not
the keywords were searched for. For an example, suppose
{w1, w2, w3} are added to id1, {w1, w2} are added to id2,
and {w1} is added to id3. Then [15] will leak that exactly one
common keyword was added to all three and that exactly two
common keywords were added to the ﬁrst two (but not the
third) and so on. This structural “equality pattern” is the sort
of leakage that we do not leak.
Not reclaiming space allows our implementations to be
much simpler and also gives us the ﬂexibility to apply various
efﬁciency optimizations (as in section III A) to the static
scheme which seem hard to achieve when in-place updates
have to be supported. As our data structures are more compact
than prior work, the overall space requirements will be lower
anyway for some number of deletes. In particular, as compared
to prior work [14] we are not forced to estimate an upper bound
(by necessity, conservative) on the maximum database size.
In some settings where SSE is used as a component, the
encrypted database is re-encrypted for security reasons [13]. In
these settings we can reclaim space and combine the auxiliary
data structure with the main static data structure while re-
encrypting.
APPLICATION TO Πptr, Πpack, Π2lev. The dynamic extensions
to Πbas can be applied as-is to other variants, resulting in
almost the same leakage Ldyn. The only difference is the size
leakage in the initial input DB, which changes according to the
different schemes. In our implementation in the next section
we consider these variants.
V.
IMPLEMENTATION
We report on our implementations of Π2lev and Πpack
(described in Section III), with extensions for dynamic data
updates (Section IV). The former scheme is the most efﬁcient
and scales to the largest datasets; it represents our current
prototype. The latter is a simpliﬁcation of the original OXT
implementation which we introduced in [3] and is discussed
here to better illustrate the effectiveness of the ideas in Π2lev
and the improvement over prior work.
PRACTICAL CRITERIA. Before describing our results, we
enumerate some of the practical criteria that we optimize for
in the Π2lev prototype.
constructs are used to support richer functional settings, such
as OXT. Finally, Section V-E describes several representative
experiments.
•
•
•
•
Parallel EDB access: The server should be able to issue
concurrent access requests to EDB when processing a
search. Modern disk controllers handle thousands of
concurrent requests and optimize disk access patterns,
increasing transfer bandwidth by orders of magnitude
when compared with sequential access. Requests are
served out-of-order but the performance beneﬁts offset
the additional implementation complexity.
EDB goodput: EDB design should maximize I/O good-
put, i.e., the ratio of data used by the processing of a query
relative to the total amount of data retrieved from external
storage. In addition to selecting an appropriate dictionary,
we achieve this by setting the parameters b, b$, B, B$ in
Π2lev to take maximum advantage of the block device.
Small EDB storage: The dictionary used in EDB should
minimize storage overhead while satisfying the other
constraints.
Lightweight EDB updates: Update information will be
independent from the EDB and implemented in-memory.
This is consistent with our envisioned scenarios where
updates are either infrequent or periodically folded into
the main data structure via re-encryption of the entire
database.
INPUT DATASETS. Our implementation accepts as input both
relational databases and document collections. The latter are
mapped to relational database tables with document attributes,
such as author name, creation date, etc., stored in atomic
columns and with the document content stored in a text
column.
We target clear-text datasets (DBs) that consist of several
id) pairs. The EDBs
tens of billions of distinct (keyword,
generated from such datasets take several terabytes of storage
and require several times more temp storage for Setup. We aim
to process such datasets efﬁciently (Setup(DB) and Search)
on medium size 64-bit x86 platforms (in our conﬁguration, 8
cores, 96GB of RAM, and ≈ 100TB RAID volume on external
storage box).
The constructions described in this paper and their im-
plementations can be extended to support richer functional
settings than simple keyword search, such as SSE in multi-
client settings or boolean queries via the OXT protocol [3] (see
end of Section II), by storing in the EDB for each (keyword,
id) pair more data than just the encrypted document id. In
the following, we use the term tuple for the data stored per
(keyword, id) pair in any of these functional settings.
ORGANIZATION. The next two subsections describe our ex-
periences with the Πpack prototype, which is the subset of
the OXT implementation [3] relevant to this work, and the
design and implementation of our Π2lev (see Figure 4). A
particular challenging issue for both prototypes was EDB gen-
eration time; the Setup implementation for Π2lev is discussed
separately in Section V-C. Section V-D describes how these
A. Πpack Implementation
The discussion of the Πpack implementation here is in-
tended as a preamble to our presentation of Π2lev in the
next subsection as it serves to motivate the optimizations
applied to the latter construction. Our implementation of Πpack
instantiates the EDB dictionary using a bucket hash table.
Buckets are split in equal-size locations, which are used to
store equal-size groups of tuples created by partitioning the
DB(w) sets. The location size is equal to the group size plus
the size of the attached label. Each group can be stored in any
of the free locations in the bucket determined by hashing its
label. As usual, the hash map is over-sized to allow for all
groups to be placed successfully; empty locations are ﬁlled
with random bits to mask the total number of groups in the
EDB.
Using a bucket hash for the dictionary allowed us to avoid
sorting the tuples by label (as required for security) before
creating the dictionary. This worked by ensuring the dictio-
nary is history independent, meaning the output of Create(L)
depends only on the members of L and not on the order they
were added to L.
The bucket hash table is stored in one large ﬁle on an ext4
RAID partition of attached storage. The bucket size is set to
a multiple of the RAID stripe size4, and buckets are aligned
with the pages of the underlying ﬁle system.
The two most signiﬁcant drawbacks with the above con-
struction are the need for oversizing the hash table, which
translates into a much larger EDB than needed, and the poor
goodput, as one have to retrieve an entire bucket to access a
group of tuples. In experiments with conﬁgurations and data
sets similar to those described in [3], the hash table has a load
factor of ≈ 60% (i.e., over-sized by a factor of ≈ 1.6) for the
placement to be successful, and goodput is ≈ 1%, as there are
96 locations per bucket.
To achieve a higher load factor (smaller EDB), we built
another Πpack prototype which uses a Cuckoo Hash (CH)
table modeled after [8] for the dictionary; page size and
alignment are the same as for the bucket hash dictionary in
the previous construction. Although we achieve load factors a
little over 90%, the cost of handling collisions during EDB
generation is very high. Moreover, making the dictionary
history independent is much more difﬁcult when using a CH
table and likely inefﬁcient in our setting.
We designed a more efﬁcient algorithm to handle collisions
during EDB generation, which leverages the server memory,
but we found its performance to be limited by its database
access patterns (see Section V-E). Finally, the need to improve
the goodput motivated the design of Πptr and Π2lev.
B. Π2lev Implementation
In order to meet the criteria stated at the beginning of this
section and avoid the drawbacks of Πpack, we developed the
4Stripe is the smallest amount of data that can be addressed within the
RAID. This is functionally equivalent to a block for an individual disk.
11
Π2lev construction (see Figure 4) which uses different database
patterns to speed-up Setup, can be conﬁgured to run Setup
efﬁciently on platforms with a wide range of internal memory,
and supports much faster retrieval as a result of higher goodput.
Recall that in Π2lev, the EDB consists of two data struc-
tures: a dictionary γ and an array A. The dictionary is again
implemented as a bucket hash, but now with exactly one
labeled location per keyword w. The bucket address and
location label are derived from w, but the location within the
bucket is selected at random to ensure history independence.
A γ location stores up to b tuples or b$ pointers, i.e. indices
in array A.
The second data structure is the array A whose entries are
called tuple blocks. Setup uses tuple blocks to store tuples, or
tuples and pointers, for medium or large DB(w), respectively.
Each tuple block stores up to B tuples or B$ pointers, with
B 0 b and B$ 0 b$ in most settings. In contrast to the
dictionary γ, which is a bucket hash, the array A needs not be
over-sized except for the purpose of masking the total number
of tuples in EDB. Empty locations in γ and A, if any, are ﬁlled
with random bits.
For all w with more than |DB(w)| > b, the tuple blocks
used for DB(w) are allocated at random in the array using
an AES-based pseudorandom permutation and the tuple list
in DB(w) is split into tuple blocks (see medium/large cases
in Figure 4). For any given w, if the number of tuple blocks
needed to store DB(w) is larger than the number of pointers
that ﬁt in a dictionary location, we use additional tuple blocks
to store pointers (see large case Figure 4).
The dictionary γ and the array A are realized as two
separate ﬁles on the same or separate ext4 RAID partitions.
The location, bucket and tuple block sizes are conﬁgurable,
but for efﬁciency the bucket and tuple block sizes must be a
multiple of the RAID stripe size. Similarly, buckets and tuple
blocks must be aligned with the pages of the underlying ﬁle
system.
In our experiments, we use a low single digit number
of tuples per location and 32KB or 64KB for buckets and
tuple blocks. Pointers are 3 or 4 bytes long, depending on the
size of the array A, and tuples are between 16 and 91 bytes
long, depending on the functional setting. For the document
collections and relational databases in our experiments, the
dictionary is between one and two orders of magnitude smaller
than the array.
Unpadded, the size of the dictionary leaks the approximate
number of keywords while the size of the array leaks the
approximate number of EDB tuples. Therefore, masking the
dictionary size, which is sensitive in many scenarios, is inex-
pensive given its relative small size. Leaking the total number
of tuples is less sensitive, which means that the larger data
structure requires less or no padding in most common cases.
This construction has several
important advantages for
very large datasets, in particular for those having multi-modal
distributions, e.g., some DB(w) sets that are very large and
a very large number of very small DB(w) as commonly
encountered. For instance, for datasets of tens of millions of
documents, each English word that is not rare can be found
in millions or more documents. On the other hand, ISBN
or SSN values are associated with only one book or person,
respectively, independent of how many books or persons the
dataset describes.
Π2lev can be conﬁgured to be disk-efﬁcient in both ex-
tremes. For rare keywords, conﬁgurations with small location
sizes, corresponding to a low single digit number of tuples,
allow the search engine to retrieve all the relevant tuples with
only one disk access. Using a small location size helps reduce