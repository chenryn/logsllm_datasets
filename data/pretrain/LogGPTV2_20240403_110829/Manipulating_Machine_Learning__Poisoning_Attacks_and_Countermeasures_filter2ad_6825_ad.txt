Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:36:53 UTC from IEEE Xplore.  Restrictions apply. 
such that
MSE(D(cid:3), ˆθ) ≤
(cid:19)
1 +
α
1 − α
(cid:20)
L(Dtr, θ
(cid:2)
) .
(18)
We now give an intuitive explanation about
Note that the above theorem is stated without any assump-
tions on the training data distribution. This is one of the
main difference from prior work [11], [16], which assume
the knowledge of the mean and covariance of the legitimate
data. In practice, such information on training data is typically
unavailable. Moreover, an adaptive attacker can also inject
poisoning samples to modify the mean and covariance of
training data. Thus, our results are stronger than prior work in
relying on fewer assumptions.
the above
theorem, especially inequality (18). Since Dtr is assumed
is a subset of Dtr of
to be the pristine dataset, and D(cid:3)
size (1 − α)n, we know all data in D(cid:3)
is also pristine
(not corrupted by the adversary). Therefore, the stationary
assumption on pristine data distribution, which underpins all
machine learning algorithms, guarantees that MSE(Dtr, θ) is
close to MSE(D(cid:3), θ) regardless of the choices of θ and D(cid:3),
as long as α is small enough.
Next, we explain the left-hand side of inequality (18).
This is the MSE of a subset of pristine samples D(cid:3) using
ˆθ computed by the TRIM algorithm in the adversarial world.
Based on the discussion above, the left-hand side is close to the
MSE of the pristine data Dtr using the adversarially learned
estimator ˆθ. Thus, inequality (18) essentially provides an upper
bound on the worst-case MSE using the estimator ˆθ output by
Algorithm 2 from the poisoned data.
To understand what upper bound Theorem 2 guarantees, we
need to understand the right-hand side of inequality (18). We
use OLS regression (without regularization) as an example to
explain the intuition of the right-hand side. In OLS we have
L(Dtr, θ
), which is the MSE using the
“best-case” estimator computed in the ideal world. Therefore,
the right-hand side of inequality (18) is proportional to the
1−α ). When α ≤ 20%,
ideal world MSE, with a factor of (1+ α
we notice that this factor is at most 1.25×.
) = MSE(Dtr, θ
(cid:2)
(cid:2)
Therefore,
informally, Theorem 2 essentially guarantees
that, the ratio of the worst-case MSE by solving (18) computed
in the adversarial world over best-case MSE computed in ideal
world for a linear model is at most 1.25. Note that since
Algorithm 2 may not always ﬁnd the global minimum of (17),
we empirically examine this ratio of the worst-case to best-
case MSEs. Our empirical evaluation shows that in most of
our experiments, this ratio for TRIM is less than 1.01×, which
is much smaller than all existing defenses.
For other models whose loss function includes the regular-
izer term (Lasso, ridge, and elastic net), the right-hand side
of (18) includes the same term as well. This may allow the
blowup of the worst-case MSE in the adversarial world with
respect to the best-case MSE to be larger; however, we are
not aware of any technique to trigger this worst-case scenario,
and our empirical evaluation shows that the blowup is typically
less than 1% as mentioned above.
The proofs of Theorem 1 and 2 can be found in Appendix B.
V. EXPERIMENTAL EVALUATION
We implemented our attack and defense algorithms in
Python, using the numpy and sklearn packages. Our code is
available at https://github.com/jagielski/manip-ml. We ran our
experiments on four 32 core Intel(R) Xeon(R) CPU E5-2440
v2 @ 1.90GHz machines. We parallelize our optimization-
based attack implementations to take advantage of the multi-
core capabilities. We use the standard cross-validation method
to split the datasets into 1/3 for training, 1/3 for testing, and
1/3 for validation, and report results as averages over 5 runs.
We use two main metrics for evaluating our algorithms: MSE
for the effectiveness of the attacks and defenses, and running
time for their cost.
We describe the datasets we used for our experiments in
Section V-A. We then systematically analyze the performance
of the new attacks and compare them against the baseline
attack algorithm in Section V-B. Finally, we present the results
of our new TRIM algorithm and compare it with previous
methods from robust statistics in Section V-C.
A. Datasets
We used three public regression datasets in our experimental
evaluation. We present some details and statistics about each
of them below.
Health care dataset. This dataset includes 5700 patients,
where the goal is to predict the dosage of anticoagulant drug
Warfarin using demographic information, indication for War-
farin use, individual VKORC1 and CYP2C9 genotypic data,
and use of other medications affected by related VKORC1
and CYP2C9 polymorphisms [45]. As is standard practice for
studies using this dataset (see [19]), we only select patients
with INR values between 2 and 3. The INR is a ratio that
represents the amount of time it takes for blood to clot, with a
therapeutic range of 2-3 for most patients taking Warfarin. The
dataset includes 67 features, resulting in 167 features after one-
hot encoding categorical features and normalizing numerical
features as above.
Loan dataset. This dataset contains information regarding
loans made on the Lending Club peer-to-peer lending platform
[29]. The predictor variables describe the loan attributes,
including information such as total loan size, interest rate,
and amount of principal paid off, as well as the borrower’s
information, such as number of lines of credit, and state of
residence. The response variable is the interest rate of a loan.
Categorical features, such as the purpose of the loan, are one-
hot encoded, and numerical features are normalized into [0,1].
The dataset contains 887,383 loans, with 75 features before
pre-processing, and 89 after. Due to its large scale, we sampled
a set of 5000 records for our poisoning attacks.
House pricing dataset. This dataset is used to predict house
sale prices as a function of predictor variables such as square
footage, number of rooms, and location [28]. In total,
it
26
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:36:53 UTC from IEEE Xplore.  Restrictions apply. 
Model
Ridge
LASSO
Dataset
Health
Loan
House
Health
Loan
House
Init
BFlip
BFlip
BFlip
BFlip
BFlip
InvFlip
Argument
(x, y)
x
(x, y)
(x, y)
(x, y)
(x, y)
Objective
Wtr
Wval
Wtr
Wtr
Wval
Wval
Init
Argument
InvFlip
BFlip
InvFlip
BFlip
BFlip
x
x
(x, y)
(x, y)
(x, y)
Objective
Wtr
Wtr
Wtr
Wtr
Wval
12%
0.026
0.028
0.026
0.029
0.030
Poisoning rates
16%
0.027
0.032
0.027
0.0316
0.0338
20%
0.027
0.033
0.029
0.032
0.0376
TABLE I: Best performing optimization attack OptP for Ridge
and LASSO regression.
TABLE II: MSEs of optimization attacks for LASSO on loan
data. BGD is the ﬁrst row.
includes 1460 houses and 81 features. We preprocess by one-
hot encoding all categorical features and normalize numerical
features, resulting in 275 total features.
B. New poisoning attacks
In this section, we perform experiments on the three regres-
sion datasets (health care, loan, and house pricing) to evaluate
the newly proposed attacks, and compare them against the
baseline BGD [54] for four regression models. For each dataset
we select a subset of 1400 records (this is the size of the house
dataset, and we wanted to use the same number of records
for all datasets). We use MSE as the metric for assessing
the effectiveness of an attack, and also measure the attacks’
running times. We vary the poisoning rate between 4% and
20% at intervals of 4% with the goal of inferring the trend in
attack success. More details about hyperparameter setting are
presented in Appendix C.
Figures 3 and 4 show the MSE of each attack for ridge
and LASSO regression. We picked these two models as they
are the most popular linear regression models. We plot the
baseline attack BGD, statistical attack StatP, as well as our
best performing optimization attack (called OptP). Details on
OptP are given in Table I. Additional results for the Contagio
PDF classiﬁcation dataset are given in Appendix C.
Below we pose several research questions to elucidate the
beneﬁts, and in some cases limitations, of these attacks.
1) Question 1: Which optimization strategies are most ef-
fective for poisoning regression?: Our results conﬁrm that the
optimization framework we design is effective at poisoning
different models and datasets. Our new optimization attack
OptP improves upon the baseline BGD attack by a factor of
6.83 in the best case. The OptP attack could achieve MSEs
by a factor of 155.7 higher than the original models.
As discussed in Section III, our optimization framework
has several instantiations, depending on: (1) The initialization
strategy (InvFlip or BFlip); (2) The optimization variable (x
or (x, y)); and (3) The objective of the optimization (Wtr
or Wval). For instance, BGD is given by (InvFlip, x, Wtr).
We show that each of these dimensions has an important
effect in generating successful attacks. Table I shows the best
optimization attack for each model and dataset, while Tables II
and III provide examples of different optimization attacks for
LASSO on the loan and house datasets, respectively.
We highlight several interesting observations. First, bound-
ary ﬂip BFlip is the preferred initialization method, with
only one case (LASSO regression on house dataset) in which
InvFlip performs better in combination with optimizing (x, y)
Init
Argument
InvFlip
BFlip
InvFlip
InvFlip
BFlip
x
x
(x, y)
(x, y)
(x, y)
Objective
Wtr
Wtr
Wtr
Wval
Wtr
Poisoning rates
20%
0.054
0.172
0.052
0.369
0.172
12%
0.034
0.08
0.04
0.369
0.08
16%
0.047
0.145
0.047
0.369
0.145
TABLE III: MSEs of optimization attacks for LASSO on
house data. BGD is the ﬁrst row.
under objective Wval. For instance,
in LASSO on house
dataset, BFlip alone can achieve a factor of 3.18 higher MSE
than BGD using InvFlip. In some cases the optimization by
y can achieve higher MSEs even starting with non-optimal
y values as the gradient ascent procedure is very effective
(see for example the attack (InvFlip, (x, y),Wval) in Table III).
However, the combination of optimization by x with InvFlip
initialization (as used by BGD) is outperformed in all cases
by either BFlip or (x, y) optimization.
Second, using both (x, y) as optimization arguments is most
effective compared to simply optimizing by x as in BGD. Due
to the continuous response variables in regression, optimizing
by y plays a large role in making the attacks more effective.
For instance, optimizing by (x, y) with BFlip initialization and
Wval achieves a factor of 6.83 improvement in MSE compared
to BGD on house dataset with LASSO regression.
Third, the choice of the optimization objective is equally
important for each dataset and model. Wval can improve over
Wtr by a factor of 7.09 (on house for LASSO), by 17.5% (on
loan for LASSO), and by 30.4% (on loan for ridge) when the
initialization points and optimization arguments are the same.
Thus, all three dimensions in our optimization framework
are inﬂuential in improving the success of the attack. The
optimal choices are dependent on the data distribution, such
as feature types, sparsity of the data, ratio of records over data
dimension, and data linearity. In particular, we noticed that for
non-linear datasets (such as loan), the original MSE is already
high before the attack and all the attacks that we tested perform
worse than in cases when the legitimate data ﬁts a linear model
(i.e., it is close to the regression hyperplane). The reason may
be that, in the latter case, poisoning samples may be shifted
farther away from the legitimate data (i.e., from the regression
hyperplane), and thus have a greater impact than in the former
case, when the legitimate data is already more evenly and non-
linearly distributed in feature space. Nevertheless, our attacks
are able to successfully poison a range of models and datasets.
2) Question 2: How do optimization and statistical at-
tacks compare in effectiveness and performance?: In general,
27
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:36:53 UTC from IEEE Xplore.  Restrictions apply. 
(a) Health Care Dataset
(b) Loan Dataset