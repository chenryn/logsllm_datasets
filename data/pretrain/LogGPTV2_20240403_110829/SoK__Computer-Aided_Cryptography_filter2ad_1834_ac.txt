Supporting associative and commutative properties enables
detecting a much larger class of attacks, since they allow the
most detailed modeling of, e.g., xor operations, abelian groups,
and Difﬁe-Hellman constructions. One caveat is that the ﬁner
details between these coarse classiﬁcations often make them
incomparable, and even where they overlap, they are not all
equally effective for analyzing concrete protocols.
Global mutable state (S). Does the tool support veriﬁcation
of protocols with global mutable state? Many real-world
protocols involve shared databases (e.g., key servers) or shared
memory, so reasoning support for analyzing complex, stateful
attacks scenarios extends the reach of such tools [28].
Link to implementation (T). Can the tool extract/generate
executable code from speciﬁcations in order to link symbolic
security guarantees to implementations?
† Abstractions (U). Does the tool use abstraction? Algo-
rithms may use abstraction to overestimate attack possibilities,
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:10:25 UTC from IEEE Xplore.  Restrictions apply. 
780
e.g., by computing a superset of the adversary’s knowledge.
This can yield more efﬁcient and fully automatic analysis
systems and can be a workaround to undecidability, but comes
at the cost of incompleteness, i.e., false attacks may be found
or the tool may terminate with an indeﬁnite answer.
‡ Interactive mode (U). Does the tool support an inter-
active analysis mode? Interactive modes generally trade off
automation for control. While push-button tools are certainly
desirable, they may fail opaquely (perhaps due to undecid-
ability barriers), leaving it unclear or impossible to proceed.
Interactive modes can allow users to analyze failed automated
analysis attempts, inspect partial proofs, and to provide hints
and guide analyses to overcome any barriers.
§ Independent veriﬁability (T). Are the analysis results
independently machine-checkable? Symbolic tools implement
complex veriﬁcation algorithms and decision procedures,
which may be buggy and return incorrect results. This places
them in the trusted computing base. Exceptions include
scyther-proof [26], which generates proof scripts that can be
machine-checked in the Isabelle theorem prover [42], and
SPEC [36], which can produce explicit evidence of security
claims that can be checked for correctness.
Speciﬁcation language (U). How are protocols speciﬁed?
The categorizations are domain-speciﬁc security protocol lan-
guages ((cid:46)), process calculus ((cid:63)), multiset rewriting (∗), and
general programming language ((cid:5)). General programming
languages are arguably the most familiar to non-experts,
while security protocol languages (i.e., notations for describing
message ﬂows between parties) are commonplace in cryptog-
raphy. Process calculi and multiset rewriting may be familiar
to formal methods practitioners. Process calculi are formal
languages for describing concurrent processes and their inter-
actions (e.g., [43]–[45]). Multiset rewriting is a more general
and lower-level formalism that allows for various encodings of
processes, but has no built-in notion of a process. It provides
a natural formalism for complex state machines.
C. Symbolic Security: Discussion
Achievements: Symbolic proofs for real-world case studies.
Of the considered symbolic tools, ProVerif and Tamarin stand
out as having been used to analyze large, real-world protocols.
They offer unprecedented combinations of scalability and
expressivity, which enables them to deal with complex systems
and properties. Moreover, they provide extensive documenta-
tion, a library of case studies, and practical usability features
(e.g., packaging, a graphical user interface for Tamarin, attack
reconstruction in HTML for Proverif).
Next, we provide a rough sense of their scalability on real-
world case studies; more precise numbers can be found in
the respective papers. It is important to keep in mind that
comparisons between tools are difﬁcult (even on similar case
studies), so these numbers should be taken with a grain of salt.
ProVerif has been used to analyze TLS 1.0 [46] (seconds
to several hours depending on the security property) and
1.3 [3] (around one hour), Signal [47] (a few minutes to more
than a day depending on the security property), and Noise
Tool
AutoG&P(cid:5)
[55]
CertiCrypt(cid:46)(cid:5)
[56]
CryptHOL(cid:5)
[57]
CryptoVerif(cid:63)(cid:5)
[58]
EasyCrypt(cid:46)(cid:5)
[59]
F7(cid:5)
[17]
F∗(cid:5)
[60]
FCF(cid:5)
[61]
ZooCrypt(cid:5)
[62]
Reasoning Focus (RF)
– automation focus
– expressiveness focus
RF
Auto
Comp
CS
Link
TCB
self, SMT
Coq
Isabelle
self
self, SMT
self, SMT
self, SMT
Coq
self, SMT
Concrete security (CS)
– security + efﬁciency
– security only
– no support
Speciﬁcation language
(cid:63) – process calculus
(cid:46) – imperative
(cid:5) – functional
OVERVIEW OF TOOLS FOR COMPUTATIONAL SECURITY ANALYSIS. SEE
SECTION II-D FOR MORE DETAILS ON COMPARISON CRITERIA.
TABLE II
protocols [48] (seconds to days depending on the protocol). In
general, more Difﬁe-Hellman key agreements (e.g., in Signal
and Noise) increase analysis times.
Tamarin has been used to analyze the 5G authentication
key exchange protocol [49] (around ﬁve hours), TLS 1.3 [2],
[4] (around one week, requiring 100GB RAM), the DNP3
SAv5 power grid protocol [50] (several minutes), and Noise
protocols [51] (seconds to hours depending on the protocol).
Challenge: Verifying equivalence properties. Many se-
curity properties can be modeled accurately by equivalence
properties, but they are inherently more difﬁcult to verify
than trace properties. This is because they involve relations
between traces instead of single traces. As such, tool support
for reasoning about equivalence properties is thus substantially
less mature. For full automation, either one bounds the number
of sessions or one has to use the very strong notion of diff-
equivalence, which cannot handle many desired properties,
e.g., vote privacy in e-voting and unlinkability.
equational
theories
For
for more
the bounded setting,
recent developments include
support
(AKISS [32],
DEEPSEC [34]), for protocols with else branches (APTE [33],
AKISS, DEEPSEC) and for protocols whose actions are
not entirely determined by their inputs (APTE, DEEPSEC).
There have also been performance improvements based
on partial order reduction (APTE, AKISS, DEEPSEC) or
graph planning (SAT-Equiv). For the unbounded setting, diff-
equivalence, ﬁrst introduced in ProVerif [52] and later adopted
by Maude-NPA [53] and Tamarin [54], remains the only
fully automated approach for proving equivalences. Because
trace equivalence is the most natural for formalizing privacy
properties, verifying more general equivalence properties for
an unbounded number of sessions remains a challenge.
D. Computational Tools: State of the Art
Table II presents a taxonomy of general-purpose computa-
tional tools. Tools are listed alphabetically and are categorized
as follows.
Reasoning focus (U). Is the tool’s reasoning focus on au-
tomation ( ) or on expressivity ( )? Automation focus means
being able to produce automatically or with light interaction
a security proof (at the cost of some expressiveness). Dually,
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:10:25 UTC from IEEE Xplore.  Restrictions apply. 
781
expressivity focus means being able to express arbitrary argu-
ments (at the cost of some automation).
Automated proof-ﬁnding (U). Can the tool automatically
ﬁnd security proofs? A subset of the automation-focused tools
can automatically (non-interactively) ﬁnd security proofs in
restricted settings (e.g., proofs of pairing-based schemes for
AutoG&P, proofs of key exchange protocols using a catalog
of built-in game transformations for CryptoVerif, proofs of
padding-based public key encryption schemes for ZooCrypt).
Composition (U). Does the tool support compositional
reasoning? Support for decomposing security arguments of
cryptographic systems into security arguments for their core
components is essential for scalable analysis.
Concrete security (A). Can the tool be used to prove
concrete bounds on the adversary’s success probability and
execution time? We consider tools with no support ( ), support
for success probability only ( ), and support for both ( ).
Link to implementation (T). Can the tool extract/generate
executable code from speciﬁcations in order to link computa-
tional security guarantees to implementations?
Trusted computing base (T). What lies in the trusted com-
puting base (TCB)? An established general-purpose theorem
prover such as Coq [63] or Isabelle [64] is usually held as the
minimum TCB for proof checking. Most tools, however, rely
on an implementation of the tool’s logics in a general purpose
language that must be trusted (self). Automation often relies
on SMT solvers [65], such as Z3 [66].
Speciﬁcation language (U). What kind of speciﬁcation
language is used? All tools support some functional language
core for expressing the semantics of operations ((cid:5)). Some tools
support an imperative language ((cid:46)) in which to write security
games, while others rely on a process calculus ((cid:63)).
E. Computational Security: Discussion
Achievements: Machine-checked security for real-world
cryptographic designs. Computational tools have been used to
develop machine-checked security proofs for a range of real-
world cryptographic mechanisms. CryptoVerif has been used
for a number of protocols, including TLS 1.3 [3], Signal [47],
and WireGuard [67]. EasyCrypt has been used for the Amazon
Web Service (AWS) key management system [68] and the
SHA-3 standard [69]. F7 was used to build miTLS, a refer-
ence implementation of TLS 1.2 with veriﬁed computational
security at the code-level [70], [71]. F∗ was used to implement
and verify the security of the TLS 1.3 record layer [1].
Takeaway: CryptoVerif
is good for highly automated
computational analysis of protocols and systems. CryptoVerif
is both a proof-ﬁnding and proof-checking tool. It works
particularly well for protocols (e.g., key exchange), as it can
produce automatically or with a light guidance a sequence
of proof steps that establish security. One distinctive strength
of CryptoVerif is its input language based on the applied π-
calculus [45], which is well-suited to describing protocols that
exchange messages in sequence. Another strength of Cryp-
toVerif is a carefully crafted modeling of security assumptions
that help the automated discovery of proof steps. In turn,
automation is instrumental to deal with large cryptographic
games and games that contain many different cases, as is often
the case in proofs of protocols.
Takeaway: F∗ is good for analysis of full protocols and
systems. F∗ is a general-purpose veriﬁcation-oriented program-
ming language. It works particularly well for analyzing cryp-
tographic protocols and systems beyond their cryptographic
core. Computational proofs in F∗ rely on transforming a
detailed protocol description into a ﬁnal (ideal) program by
relying on ideal functionalities for cryptographic primitives.
Formal validation of this transformation is carried out man-
ually, with some help from the F∗ veriﬁcation infrastructure.
Formal veriﬁcation of the ﬁnal program is done within F∗. This
approach is driven by the insight that critical security issues,
and therefore also potential attacks, often arise only in detailed
descriptions of full protocols and systems (compared to when
reasoning about cryptographic cores). The depth of this insight
is reﬂected by the success of F∗-based veriﬁcation both in
helping discovering new attacks on real-world protocols like
TLS [72], [73] as well as in verifying their concrete design
and implementation [1], [70].
is the closest
Takeaway: EasyCrypt
to pen-and-paper
cryptographic proofs. EasyCrypt supports a general-purpose
relational program logic (i.e., a formalism for specifying
and verifying properties about
two programs or two runs
of the same program) that captures many of the common
game hopping techniques. This is complemented by libraries
that support other common techniques, e.g., the PRF/PRP
switching lemma, hybrid arguments, and lazy sampling [8].
In addition, EasyCrypt features a union bound logic for upper
bounding the probability of some event E in an experiment
(game) G (e.g., bounding the probability of collisions in exper-
iments that involve hash functions). Overall, EasyCrypt proofs
closely follow the structure of pen-and-paper arguments. A
consequence is that EasyCrypt is amenable to proving the
security of primitives, as well as protocols and systems.
Challenge: Scaling security proofs for cryptographic sys-
tems. Analyzing large cryptographic systems is best done
in a modular way by composing simpler building blocks.
However, cryptographers have long recognized the difﬁculties
of preserving security under composition [74]. Most game-
based security deﬁnitions do not provide out-of-the-box com-
position guarantees, so simulation-based deﬁnitions are the
preferred choice for analyzing large cryptographic systems,
with universal composability (UC) being the gold-standard—
UC deﬁnitions guarantee secure composition in arbitrary con-
texts [9]. Work on developing machine-checked UC proofs is
relatively nascent [75]–[77], but is an important and natural
next step for computational tools.
F. Further Reading
Another class of tools leverages the beneﬁts of automated
veriﬁcation to support automated synthesis of secure crypto-
graphic designs, mainly in the computational world [62], [78]–
[81]. Cryptographic compilers provide high-level abstractions
(e.g., a domain-speciﬁc language) for describing cryptographic
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:10:25 UTC from IEEE Xplore.  Restrictions apply. 
782
problematic in their own ways, as we will see in Section IV).
Currently, these painstaking efforts are manually repeated for
each target architecture.
How are these failures being addressed outside CAC?
Given its difﬁculty, the task of developing high-speed cryp-
tography is currently entrusted to a handful of experts. Even
so, experts make mistakes (e.g., a performance optimization to
OpenSSL’s AES-GCM implementation nearly reached deploy-
ment even though it enabled arbitrary message forgeries [109];
an arithmetic bug in OpenSSL led to a full key recovery
attack [110]). Current solutions for preventing more mistakes
are (1) auditing, which is costly in both time and expertise,
and (2) testing, which cannot be complete for the size of inputs
used in cryptographic algorithms. These solutions are also
clearly inadequate: Despite widespread usage and scrutiny,
OpenSSL’s libcrypto library reported 24 vulnerabilities