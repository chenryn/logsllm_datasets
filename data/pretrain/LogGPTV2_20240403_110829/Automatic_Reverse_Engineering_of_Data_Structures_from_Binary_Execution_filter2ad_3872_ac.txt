...
10
Tl2
φ
φ
φ
...
Sl2
φ
φ
{}
...
{char*}
{}
tsl2
10
10
10
...
10
Table 1. Example of running the online algorithm. Variable g1 is a global, l1 and l2 are locals.
instruction
...
12. mov l1, l2
13. Exit M
...
99. Enter N
100. strcpy(g1,...)
Tg1
...
φ
φ
...
φ
{char*}
Sg1
...
{}
{}
...
{}
{}
tsg1
...
0
0
...
0
0
Tl1
...
φ
φ
...
φ
φ
Sl1
...
{, }
{, }
...
φ
φ
tsl1
...
10
10
...
99
99
Tl2
...
φ
φ
...
φ
φ
Sl2
...
{}
{}
...
φ
φ
tsl2
...
10
10
...
99
99
Table 2. Example of running the off(cid:173)line type resolution procedure. The execution before timestamp
12 is the same as Table 1. Method N reuses l1 and l2
structure plus the call stack at that point, as the abstraction
of the structure. The intuition is that the heap structure
instances allocated from the same PC in the same call stack
should have the same type. Fields of the structure are
represented by the allocation site and ﬁeld offsets. As an
allocated heap region may be an array of a data structure,
we use the recursion detection heuristics in [9] to detect the
array size. Speciﬁcally, the array size is approximated by
the maximum number of accesses by the same PC to unique
memory locations in the allocated region. The intuition is
that array elements are often accessed through a loop in
the source code and the same instruction inside the loop
body often accesses the same ﬁeld across all array elements.
Finally, if heap structures allocated from different sites have
the same ﬁeld types, we will heuristically cluster these heap
structures into one abstraction.
3.5 Constructing Hierarchical View of
In(cid:173)
Memory Data Structure Layout
An important feature of REWARDS is to construct a
hierarchical view of a memory snapshot, in which the prim-
itive syntax of individual memory locations, as well as their
semantics and the integrated hierarchical structure are visu-
ally represented. This is highly desirable in applications like
memory forensics as interesting queries, e.g., “find all
IP addresses”, can be easily answered by traversing
the view (examples in Section 5.1). So far, REWARDS
is able to reverse engineer the syntax and semantics of
data structures, represented by their abstractions. Next, we
present how we leverage such information to construct a
hierarchical view.
Our method works as follows. It ﬁrst types the top level
global variables.
In particular, a root node is created to
represent a global section. Individual global variables are
represented as children of the root. Edges are annotated
with offset, size, primitive type, and semantics of the
If a variable is a pointer,
corresponding children.
the
algorithm further recursively constructs the sub-view of the
data structure being pointed to, leveraging the derived type
of the pointer. For instance, assume a global pointer p is of
type T*, our method creates a node representing the region
pointed to by p. The region is typed based on the reverse
engineered deﬁnition of T. The recursive process terminates
when none of the ﬁelds of a data structure is a pointer. Stack
is similarly handled: A root node is created to represent
each activation record.
Local variables of the record
are denoted as children nodes. Recursive construction is
performed until all memory locations through pointers are
traversed. Note that all live heap structures can be reached
(transitively) through a global pointer or a stack pointer.
Hence, the above two steps essentially also construct the
structural views of live heap data.
Our method can also type some of the unreachable
memory regions, which represent “dead” data structures,
e.g., activation records of previous method invocations
whose space has been freed but not reused. Such dead
data is as important as live data as they disclose what had
happened in the past. In particular, our method scans the
stack beyond the current activation record to identify any
pointers to the code section, which often denote return
addresses of method invocations. With a return address, the
function invocation can be identiﬁed and we can follow the
aforementioned steps to type the activation record.
4
Implementation and Evaluation
We have implemented REWARDS on PIN-2.6 [27], with
12.1K lines (LOC) of C code and 1.2K LOC of Python
code. In the following, we present several key implementa-
tion details. REWARDS is able to reveal variable semantics.
In our implementation, variable semantics are represented
as special semantic tags complementary to regular type tags
such as int and char. Both semantic tags and regular tags
are stored in the variable’s type set. Tags are enumerated
to save space. The vast diversity of program semantics
makes it infeasible to consider them all. Since we are
mainly interested in forensics and security applications, we
focus on the following semantic tags: (1) ﬁle system related
(e.g., FILE pointer, ﬁle descriptor, ﬁle name, ﬁle status);
(2) network communication related (e.g., socket descriptor,
IP address, port, receiving and sending buffer, host info,
msghdr); and (3) operating systems related (e.g., PID, TID,
UID, system time, system name, and device info).
Meanwhile, we introduce some of our own semantic
tags, such as ret addr t indicating that a memory loca-
tion is holding a return address, stack frame t indicat-
ing that a memory location is holding a stack frame pointer,
format string t indicating that a string is used in
format string argument, and malloc arg t indicating an
argument of malloc function (similarly, calloc arg t
for calloc function, etc.). Note that these tags reﬂect the
properties of variables at those speciﬁc locations and hence
do not particitate in the type information propagation. They
can bring important beneﬁts to our targeted applications
(Section 5).
REWARDS needs to know the program’s address space
mapping, which will be used to locate the addresses of
global variables and detect pointer types.
In particular,
REWARDS checks the target address range when deter-
mining if a pointer is a function pointer or a data pointer.
Thus, when a binary starts executing with REWARDS,
we ﬁrst extract the coarse-grained address mapping from
the /proc/pid/maps ﬁle, which deﬁnes the ranges of
code and data sections including those from libraries, and
the ranges of stack and heap (at that time). Then for
each detailed address mapping such as .data, .bss and
.rodata for all loaded ﬁles (including libraries), we
extract the mapping using the API provided by PIN when
the corresponding image ﬁle is loaded.
We have performed two sets of experiments to evaluate
REWARDS: one is to evaluate its correctness, and the
other is to evaluate its time and space efﬁciency. All
the experiments were conducted on a machine with two
2.13Ghz Pentium processors and 2GB RAM running Linux
kernel 2.6.15.
We select 10 widely used utility programs from the
following packages: procps-3.2.6 (with 19.1K LOC and
containing command ps), iputils-20020927 (with 10.8K
LOC and containing command ping), net-tools-1.60 (with
16.8K LOC and containing netstat), and coreutils-
5.93 (with 117.5K LOC and containing the remaining test
commands such as ls, pwd, and date). The reason
for selecting these programs is that they contain many
data structures related to the operating system and network
communications. We run these utilities without command
line option except ping, which is run with a localhost and
a packet count 4 option.
4.1 Evaluation of Accuracy
To evaluate the reverse engineering accuracy of RE-
WARDS, we compare the derived data structure types with
those declared in the program source code. To acquire
the oracle information, we recompile the programs with
debugging information, and then use libdwarf [1] to
extract type information from the binaries. The libdwarf
library is capable of presenting the stack and global variable
mappings after compilation. For instance, global variables
scattering in various places in the source code will be
organized into a few data sections. The library allows us see
the organization. In particular, libdwarf extracts stack
variables by presenting the mapping from their offsets in
the stack frame and the corresponding types. For global
variables,
the output by libdwarf is program virtual
addresses and their types. Such information allows us to
conduct direct and automated comparison. Note that we
only verify the types in .data, .bss, and .rodata sec-
tions, other global data in sections such as .got, .ctors
are not veriﬁed. For heap variables, since we use the
execution context at allocation sites as the abstract repre-
sentation, given an allocation context, we can locate it in
the disassembled binary, and then correlate it with program
source code to identify the heap data structure deﬁnition,
and ﬁnally compare it with REWARDS’s output. Although
REWARDS extracts variable types for the entire program
address space (including libraries), we only compare the
results for user-level code.
The result for stack variables is presented in Figure
2(a). The ﬁgure presents the percentage of (1) functions
that are actually executed, (2) data structures that are used
in the executed functions (over all structures declared in
those functions), and (3) data structures whose types are
accurately recovered by REWARDS (over those in (2)). At
runtime, it is often the case that even though a buffer is
deﬁned in the source code with size n, only part of the
n bytes are used. Consequently, only those used ones are
typed (the others are considered unused). We consider the
buffer is correctly typed if its bytes are either correctly typed
or unused. From the ﬁgure, we can observe that, due to
the nature of dynamic analysis, not all functions or data
structures in a function are exercised and hence amenable
to REWARDS. More importantly, REWARDS achieves an
average of 97% accuracy (among these benchmarks) for
the data structures that get exercised. For heap variables,
the result is presented in Figure 2(b), the bars are similarly
deﬁned. REWARDS’s output perfectly matches the types in
the original deﬁnitions when they are exercised. Note some
of the benchmarks are missing in Figure 2(b) (e.g., date)
because their executions do not allocate any user-level heap
structures. The result for global variables is presented in
Figure 2(c), and REWARDS achieves over 85% accuracy.
To explain why REWARDS cannot achieve 100% accu-
e
g
a
t
n
e
c
r
e
P
 120
 100
 80
 60
 40
 20
 0
p
s
p
i
n
g
ls
n
e
t
s
t
a
t
p
w
d
d
a
t
e
u
p
ti
u
n
a
m
m
e
e
Benchmark Program
(a) Accuracy on Stack Variables
 120
 100
 80
 60
 40
 20
 0
 400
 350
 300
 250
 200
 150
 100
 50
 0
e
g
a
t
n
e
c
r
e
P
)
s
d
n
o
c
e
s
(
i
e
m
T
n
o
i
t
u
c
e
x
E