5 ATTACKS ON ARM
In this section, we present our optimizations to the cache occupancy
channel for various ARM devices. To determine the effectiveness
of various modifications to the channel, we design a robust data
collection system employing the Appium [1] and Selenium [47]
frameworks to control our iOS, Android, and MacOS devices.
5.1 Setup
Data Sets. To monitor the accuracy of the cache occupancy channel,
we utilize an abbreviated open world dataset, which consists of
multiple accesses to sensitive and non-sensitive websites, marking
all non-sensitive websites as a single class, regardless of domain.
Particularly, we utilize a dataset similar to that of [50] to enable
better comparison with the x86 version of the cache occupancy
channel. Our dataset consists of 1,500 website accesses, containing
100 accesses to the top 10 Alexa websites (i.e., sensitive websites)
and 1 access to 500 other websites not within the Alexa top 100
(i.e., non-sensitive websites). To prevent any ordering bias, we
generate a random order for these 1,500 accesses and then utilize
the same order for every experiment. We believe this randomization
is important, and previous works do not discuss the access order.
Unlike network based fingerprinting attacks, the CPU cache
may retain some of its state between website accesses, causing the
machine learning system to identify incorrect features and falsely
boost the accuracy of the test if websites are repeatedly accessed in
the same order. Note that this abbreviated dataset is used in this
section to optimize the side channel attacks on ARM. In the next
section, we conduct a thorough evaluation using a significantly
larger dataset.
Machine Learning Approaches. To evaluate the performance
of our optimizations we utilize the Rocket [8] transform paired
with ridge regression. The classifiers are trained and tested with a
cross validation strategy, wherein we utilized 90% of the data for
training, and 10% of the data for testing. We report the average of 5
rounds of training and testing.
5.2 Optimizing Cache Occupancy Attack
We have demonstrated in Section 4.3 that, unlike previous studies
in homogeneous CPU architectures, cache accesses on low power
cores can be nearly 10x slower. Combining this with the fact that
browser manufacturers may continue to decrease the granularity of
their timing sources to prevent attacks, it is necessary to re-examine
the best way to measure cache occupancy on ARM.
Examining the Snapdragon 845 processor in the Google Pixel
3, we find that the low power cores are based on the Cortex A55
design from ARM and that the Snapdragon 845 processor has been
configured to utilize 2MB of the system-level cache. Using the in-
formation from our previous microbenchmarks on the Google Pixel
3, it takes about 60ns to access a single cache value at a 2MB buffer
size. Since the Snapdragon 845 employs a 64 Byte cache line size,
to avoid prefetching, we should access every 32nd integer in our
2MB buffer. As the buffer can hold ≈500,000 integers, this results
in ≈ 16, 000 accesses. At 60ns per access, this equates to just under
1ms. While the Snapdragon 845 has configured the system-level
cache to be 2MB, the Cortex A55 supports up to 4MB of shared
cache [33], and the accesses may take almost 2ms with no back-
ground activity, and will almost certainly take more than 2ms if the
processor is performing another task. Thus, if the system described
in [50] is used without modification, every trace would be nearly
identical with only overlong accesses, and hence no identification
would be possible. To this end, we propose a series of modifications
that work for devices, regardless of their access speed to cache.
This enables attackers to adjust the buffer size for the device and be
worry-free about adjusting the sample rate if the device happens
to be very slow.
Modifications. The first modification entails recording the num-
ber of cache accesses within a set time frame, instead of the time
05101520Buffer Size (MB)1.52.02.53.03.54.04.55.0Background Access Time (ns)Average Memory Access Time ChromeBackgroundForeground1.52.02.53.03.54.04.55.0Foreground Access Time (ns)05101520Buffer Size (MB)1015202530354045Background Access Time (ns)Average Memory Access Time SafariBackgroundForeground2.02.53.03.54.04.55.05.5Foreground Access Time (ns)05101520Buffer Size (MB)2.02.53.03.54.04.55.05.5Background Access Time (ns)Average Memory Access Time FirefoxBackgroundForeground2.02.53.03.54.04.55.05.5Foreground Access Time (ns)789An Exploration of ARM System-Level Cache and GPU Side Channels
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
to complete accesses. This system is far less affected by changes in
the accuracy of clock. The system will always record the number of
actual cache accesses, a number that is far more fine-grained than
the time to access the whole cache. To enhance system performance
on slower devices, we also increase the access time window to 4ms
to increase the number of possible accesses. With these initial mod-
ifications, we achieve 75% open world accuracy in the abbreviated
10-site test (Section 5.1) on the Google Pixel 3.
With the first enhancement, the system checks the number of
total cache accesses in the time period. It then needs to frequently
check the clock to see if the time period expires. We find that
the Android system only completes about 2,500 accesses per 4ms
window, which is far lower than the original predicted value of
about 16,000 accesses per 1ms window. Upon profiling the page, we
discover that the vast majority of the code runtime is consumed by
the performance.now() call to check whether the time window is
elapsed. Since the ARM last-level caches are exclusive, the attack
might have several issues if the cache occupancy system continually
accesses the same beginning elements of the buffer without ever
accessing the entirety of the buffer. In the worst case, if the number
of accesses can fit in the L1 and L2 caches, the script may never
impact the L3 cache, providing minimum useful information for
the task of website fingerprinting.
We thus further employ two enhancements. The first enhance-
ment accesses the buffer in a circular fashion: if the script only
completes 2,500 accesses in the time window, it will access the
2,501st element at the beginning of the next window. It only re-
turns to the first element once all elements have been visited. This
ensures that the buffer eventually fills the L3 cache and that sequen-
tial observations cover different parts of the cache. We find that this
technique increases the accuracy of the 10-site open world dataset
to about 83%. The next enhancement is to decrease the amount of
time that the script spends on checking the time. Instead of check-
ing after every access, we check after every 20 cache accesses. This
enhancement (without circular accesses) increases the accuracy to
84%. We then combine both enhancements and further increase the
accuracy to 86%. We present a thorough evaluation in Section 6.
5.3 Novel GPU Channel
The DynamIQ CPU design not only adds the L3 shared cache among
all of the processing cores within a cluster, but also allows for
the L3 cache to be shared with any other peripherals contained
within the SoC. This means that peripherals/accelerators like the
Graphics Processing Unit (GPU), Digital Signal Processor (DSP),
and Image Signal Processor (ISP) are all able to impact the shared
cache. In particular, the GPU is heavily utilized to display a web
page to users. Newer web browsers employ hardware acceleration
when rendering and displaying web pages. Elements like HTML5
Canvas, WebGL or WebGL2 animations, and videos are also usually
hardware accelerated. Thus, we endeavor to explore whether the
GPU and shared cache architecture of current ARM DynamIQ can
be exploit to create a website fingerprinting side channel.
It is challenging to construct a GPU cache occupancy channel.
WebGL2 and basic HTML5 canvas elements only update at a low
frequency of 60Hz. While these sampling rates can be increased,
working with the canvas element in a background tab further in-
creases the complexity and overhead. Also, it is not straightforward
to determine the amount of memory that a GPU process consumes.
GPU programming within JavaScript is mainly designed around
graphical interfaces and smooth animations. An ideal attack should
instead perform minimal useless image display, but focus primarily
on exploiting the side channel. Therefore, we utilize a JavaScript
library called GPU.js [16], which is designed to enable the creation
and deployment of GPU computational kernels from JavaScript to
WebGL compatible code. It can reduce the amount of boilerplate
code and other timing elements for an attacker.
We thus create a two-dimensional buffer of data and repeatedly
utilize the GPU to process this buffer with different mathematical
kernels. Unlike our improved cache occupancy channel, accelera-
tor based channels cannot provide us with high granularity mea-
surements. The accelerator based workload requires that the CPU
should first declare the work, pass it to the accelerator (GPU), and
wait until the GPU completes its task. This means that the sizing
and complexity of the kernel task must be tuned for the optimal
fingerprinting performance.
To understand the performance of different settings, we create a
spy script similar in nature to the cache occupancy spy script. The
GPU script reports the number of kernel executions that it can com-
plete in the monitoring time period. We conduct experiments using
multiple kernels, including matrix multiplication and computing
the dot product. We find that the kernel that sums each row of the
input array delivers far superior performance. This might be due
to massively decreased complexity and time in this GPU kernel:
the reduced complexity enables more possible kernel executions,
which in turn leads to better observability of GPU usage. We also
check the optimal size for the computation. The time taken to com-
pute a small kernel might provide minimally useful information
as the GPU startup overhead would dominate the timing, while
a large kernel would take too much time and decrease the obser-
vation granularity. We find that an overall array computation of
between 20KB (Android) and 40KB (MacOS) organized into 5x4KB
or 10x4KB arrays works best. Finally, we examine the observation
window, but limit our experiments to a maximum 10 second du-
ration to maintain a realistic approach. Again, we find disparate
sizes depending on platforms. The Google Pixel 3 provides the best
performance with 500 20ms observations and the M1 MacBook Air
achieves its best results with 1,000 10ms observations. We believe
this is caused by the speed of the processors: the SnapDragon 845
functions much slower and thus requires more time to manifest
observable differences in computation performance as opposed to
simply observing GPU overhead.
6 EVALUATION
In this section, we provide detailed performance results for the
cache occupancy and GPU contention channels. Here we utilize a
much larger dataset containing 100 accesses to 100 sensitive sites
(Alexa Top 100), and 1 access to 5,000 other websites. We report
both closed (only the sensitive websites) and open (all websites)
world accuracy results. As before, to remove any bias from the
experimentation, the collection process is conducted using Appium
or Selenium automation of the target platform. The list of 15,000
790ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Patrick Cronin, Xing Gao, Haining Wang, and Chase Cotton
total website accesses is randomized to ensure that there are no
unintentional ordering effects and the same random access order is
utilized for each experiment for better comparison.
To compute the accuracy of the fingerprinting, we utilize 10-fold
cross validation with a 90/10 train/test split. We report accuracy
for two machine learning algorithms, a ridge regression with a
minirocket [8] transform and a minirocket transform with a 1D
CNN (configuration presented in Appendix Table 5). The ridge
regression with minirocket transform is a recent advancement in
time series machine learning and is able to achieve results close to
those of the 1D CNN in less than a minute.
6.1 Web-Based Attack Results
Table 2 presents the accuracy of the web based cache occupancy
fingerprinting experiments. Our approach can achieve promising
results across all of the devices in the closed world scenario (i.e.,
100 sensitive websites), with accuracy ranging from 80% to 95%
when utilizing the ridge regression classifier.
The open world scenario (i.e., 100 sensitive websites and 5,000
non-sensitive websites) also demonstrates high accuracy. In the
open world cases, we find that in most cases the 1D CNN performs
better than the ridge regression classifier. This behavior is expected
as the 1D CNN utilizes multiple convolutional and pooling layers to
extract features from the dataset and learn both spatial and temporal
patterns.
We notice that the cache occupancy channel performs the best
on the Macbook Air, and the worst on the iPhone SE 2. This is likely
related to the design of both the cache systems and schedulers.
The CPU core designs in the MacBook Air are one generation
newer, and the M1 chip is designed specifically for desktop/laptop
workloads, and is likely tuned for multi-process scenarios. Also, the
M1 chip contains features to prevent single cores from dominating
the cache [12], and the A13 has been discovered to use part of the
shared high performance L2 cache as an extra L2 cache for the
low performance cores [11]. Apple also changes the amount of the
cache that the high and low power cores have access to, depending
on the DVFS states of the cores [11].
To analyze these effects, we conduct experiments with different
buffer sizes. The Google Pixel 3 reports 2MB of shared cache, and
we find that a 2MB buffer performs the best in the fingerprinting
task. While the iPhone SE2 is unclear about the actual amount of
shared cache provided to the low power cores, we find that a 4MB
buffer performs the best in both tested configurations. Interestingly,
this 4MB buffer seems to indicate that the cache occupancy channel
is solely utilizing the L2 cache of the low power cores, potentially
implying that Apple schedules foreground browser rendering pro-
cesses to these low power cores or that the ‘extra’ L2 cache that is
shared with the high performance cores is not exclusively owned by
either core type. The Macbook Air, however, demonstrates vastly
different behaviors. Specifically, we find that a 4MB buffer performs
the best for Google Chrome, a 10MB buffer for Mozilla Firefox, and
a 24MB buffer for Apple’s Safari. As previously mentioned, these
differences may be caused by a number of reasons, including dif-
ferent renderers and JavaScript engines. In general, attackers need
to adjust attack strategies based on various factors to achieve high
overall performance.
Another possible factor in the reduced performance of mobile
devices vs. laptops could be the trend of websites to deliver different
pages to different devices. When a laptop visits a website, it views
the entire site that usually contains much more detailed content
than the corresponding mobile website. The vastly simplified mobile
websites may appear more similar to the cache occupancy channel,
resulting in the decreased accuracy.
6.2 App-Based Attack Results
We next evaluate the performance of the cache occupancy channel
if the attacker can run in a background process on the device. We
continue to employ a 4ms sample period to provide the most fair
comparison between the browser- and native-based channels and
develop native applications for each platform to enable this testing.
We create applications for the iOS and Android systems featur-
ing two processes, one drives a ‘webview’5 and another acts as
the spy process. This method has been used to study native side
channel performance in website fingerprinting before [36]. On the
Macbook Air, a spy process written in C is launched alongside the
web browser to monitor traffic. The results are listed in Table 3.
Overall, for the Macbook, our method can achieve about 90% ac-
curacy for the closed world dataset, and more than 80% accuracy
for the open world scenario. For other devices, we can also achieve
about 70% accuracy for the open world case.
We also notice an interesting trend, in all but the Firefox browser,
the channel generally performs worse in the native setting. We in-
fer that this reduced performance can potentially be caused by
the idiosyncrasies of the OS scheduler. The scheduler of a mobile
phone aims to provide the best performance to the foreground pro-
cess and imposes strict limitations on background processes. On
the other hand, the scheduler of a laptop/desktop should ensure
more equal scheduling of background processes as they are im-
portant to user satisfaction (severely diminishing the performance
of background file sync, application updates/installs, etc. would
be unacceptable). MacOS, specifically, offers a number of different
process priorities that have recently been shown to greatly affect
which cores a specific task is executed on [38] and thus mixing
native and web browser processes may result in unexpected sched-
uling. While the process in a background tab is very likely to end
up on the low power cores, the native process may be scheduled