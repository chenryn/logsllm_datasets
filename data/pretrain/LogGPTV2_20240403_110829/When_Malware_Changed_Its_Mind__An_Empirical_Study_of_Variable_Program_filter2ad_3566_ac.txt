2
-
1
3
-
Mal
4
25
3
1
1
1
2
5
5
1
2
6
4
Ben Mal
10
49
5
1
3
1
3
9
10
1
3
9
6
-
1
-
-
-
-
-
-
1
-
-
1
-
Ben
2
8
1
-
1
-
1
2
3
-
1
4
1
3
8
2
1
2
1
3
6
6
1
3
7
1
Table 4: Parameter(s) IQR variability for malware, PUP and
benign.
[PE] PE File Creation actions, [D] Directory Creation,
[RM] Registry Key Modiﬁcation, [RC] Registry Key
Creation, [M] Mutex Creation, [P] Process Creation actions.
as the IQR and MAD. The PUP and malware distributions
remain distinguishable for ﬁle-creation actions, but overlap
signiﬁcantly for registry-modiﬁcation and mutex-creation ac-
tions. In fact, a few PUP samples seem to have more extreme
outliers causing the 99–1 range to be larger than that of mal-
ware.
While malware and PUP both vary more than be-
nign in all the action types, they show variability in
different action types.
4.1.2 Parameter Variability
We now take a closer look at the variability in the parameter
values. We calculate the Jaccard index among the parameters
of the same actions types in the execution traces. In this
experiment use the full value of the parameters types listed
in Table 4. Our observation was that the parameters values
USENIX Association
30th USENIX Security Symposium    3493
Malware(2424)PUP(1621)Benign(22443)Category0255075100125150175200Interquartile Range59.019.258.0IQR for total number of actionsMalware(2060)PUP(1363)Benign(18624)Category020406080100120140Interquartile Range33.8752.751.75IQR for File Creation actionsMalware(2361)PUP(1311)Benign(15415)Category051015202530Interquartile Range7.53.01.0IQR for Registry Modification actionsacross different machines have a high variability for both
malware, PUP and benign programs. For all of the parameter
types, we obtain a Jaccard index is 0, indicating that there
is no full value shared across all executions. Note that for
this step, we do not normalize the data to remove computer
speciﬁc artifacts and also do not extract substrings from the
parameters. However in Section 5, when exploring invariants
parts among executions of samples, we will perform deeper
investigation on the substrings as well.
No parameter value is shared among all machines
except for ﬁle extension. An analyst has to rely on
substrings/tokens of the parameter values to create
signatures.
We also perform IQR measurements, to obtain more in-
sights about the distribution of the variability among different
parameter types. In Table 4 we report the median and 75th per-
centile IQRs of different parameter types. We can clearly see
that benign programs rarely change the directory they work
on, the ﬁle names created or the executables they launch. On
the other hand, malicious samples tend to create a signiﬁcant
amount of new ﬁles (25 new ﬁles for 50% of the malware),
to work on several directories and even to create different
executables over different executions. This ﬁnding indicates
that the same sample’s behavior can vary to an extent that it
could be hard to identify a behavioral indicator that is com-
mon among all. We will explore this aspect in more detail in
the following section.
Case study. In the executions of a Glupteba malware sam-
ple we observed that the Jaccard index across multiple
machines is 0 for ﬁle names and 0.2 for mutex names,
while the IQR for ﬁle and mutex creation is 0 and 2 re-
spectively. This means that the malware changes signif-
icantly the name of ﬁles but not their absolute number,
while mutex names are more similar but with a larger vari-
ability in terms of number. In this particular case, mu-
texes were a better candidate for building signatures, as we
found that h48yorbq6rm87zot appeared in all the machines,
which is also conﬁrmed by a report from TrendMicro [50].
On the contrary, the mutex ZonesCacheCounterMutex and
ZoneAttributeCacheCounterMutex only appeared in half
of the machines, which explains the IQR of 2.
4.2 Time Variability
In this section we look at how variability is impacted by the
time in which a sample is executed. We start again by looking
at the volume of actions and then zoom into those actions by
including their parameters into the analysis.
4.2.1 Total Action Variability
We measure the action variability by comparing executions of
the samples in different weeks. Since 80% of the samples in
our data were executed at most four weeks after the ﬁrst week
of their appearance, we perform the per-week time analysis
on those next 4 weeks. To simulate what an analyst would
deal with, we consider the ﬁrst week’s executions as the base
and compare it with each of the 4 consecutive weeks. Here,
we cannot use IQR as for each sample we only have 4 data
points. We simply count the number of missing and new
actions observed in each sample’s executions compared to the
previous week. The results are reported in Figure 3.
Malware have the highest number of missing and addi-
tional actions. The general takeaway for coarse-grained time
variability analysis is that there is a signiﬁcantly larger time
variability in malware compared to PUP and benign samples.
According to the Figure 3, on average across all machines
we see 6 missing actions 1 week after the ﬁrst execution. Even
though this number might seem low, depending on what those
actions types are variability over time might have an impact
on the malware detection solutions. Note that there are also
some malware samples that show a tremendous variability
(average of max being 17, max of max being 219). One possi-
ble explanation for this signiﬁcant number of missing actions
is that when malware is re-executed on the same machines,
might not need to repeat some of the behavior such as creating
particular ﬁles. At time of the data collection, the malware
samples were not yet known and therefore, the machines were
not cleaned up before the re-execution. However, we also ob-
serve variability over time when looking on the new actions
that appear on the following weeks. Similarly, malware sam-
ples have on average 1 new action appearing every week,
which is larger than PUP and benign. We also highlight that
the machine with the maximum number of additional actions
seems to have a maximum of 63 new actions and more than 3
new actions for 50% of the malware samples. For malware
execution longer than 1 week from their ﬁrst appearance less
new actions appear, indicating a more stable behavior by time.
Case studies. A TOR-connected coinminer was dropping
miners.ini, miners.ini.* and minergate.log and launching
minergate-cli.exe before January 18th in 2018. In some ma-
chines it was also dropping up to 14 *.tmp ﬁles. After 18th this
behavior completely stopped, resulting a number of missing
actions in the following weeks. On the other side of the scale,
we also identiﬁed a Remote Administration Tool, which ini-
tially dropped 5 dlls ﬁles and an executable (setacl.exe) before
March 16th 2018. On April 3rd, it started dropping 7 more dll
ﬁles and 3 new executables. We also have examples for mal-
ware that misses and adds new actions at the same. In it’s ﬁrst
week of appearance the software was dropping various ﬁles
on different machines such as microsoft ofﬁce, foxit pdf edi-
tor, autocad 2015 qqlivedownloader.exe. This behavior could
be due to the user’s interaction with the malware or simply
the malware hiding its purpose. The next week’s executions
no longer drop any of these ﬁles, but zny_znykb030.exe or
kuaizip_setup_2523474329_rytx2_001.exe appears to down-
load consistently in almost all the machines. We believe that
3494    30th USENIX Security Symposium
USENIX Association
Figure 3: Missing actions compared to the previous week.
in this case the user had no control of the malware and the
downloads happened silently. This malware was still running
4 weeks later performing the same actions. A ﬁnal exam-
ple is a sample of Adware.Chinad, which dropped various
ﬁles (microsoft ofﬁce, foxit pdf editor, autocad 2015). On the
following week, a new executable (zny_znykb030.exe) fol-
lowed by potentially pirated other software are downloaded.
In malware, time variability is the largest. While
the variability is mainly due to the missing actions,
there are also new events that appear on the follow-
ing weeks.
Figure 4: Relation between detection and the time
variability. This result shows that malware that have high
time variability have higher VT detections.
Malware with highest detection rates vary more over time.
One interesting observation on the malware that exhibit more
variability over time was that in general more of the AV en-
gines would label them as malicious. In Figure 4 we show
the distributions of the malicious samples with low time vari-
ability (lower than 25th percentile) against the ones with high
variability (on the top of the 75th percentile). For each of sam-
ples, we check the total number of detections in VirusTotal in
November 2019 (i.e., one year after the ﬁrst time we observed
the samples). As it can be seen seen, the samples that are AV
software eventually detected the most had a higher variability
across time. This shows that even if in general time variability
is low across the board, for easier to classify samples it seems
like time has a more signiﬁcant effect on the variability.
To get a better understanding of this phenomenon we con-
ducted two case-studies. In the 75th percentile we found a
version of kuaizip and analyzed its behavioral data manually.
This malware seemed to stop working sometime around the
second week of April 2018, after which it still performed
host-related actions but failed to download the PE ﬁles it
was retrieving before. At the other side of the spectrum, we
chose a malicious sample that exhibits low variability. We
found an open source DLL injection tool classiﬁed as mal-
ware which performs exactly the same actions every time it
runs. This likely-to-be-malicious sample injects into roblox-
playerbeta.exe, creates settings.xml, and sets some registry
keys. Upon further analysis we found that this is being inten-
tionally used for cheating in games and this exact behavior
is observed over and over again. While preliminary, these
experiments seem to conﬁrm that time variability affects the
most those samples that rely on an external infrastructure.
4.2.2 Parameter Variability
When switching to the ﬁne-grained analysis of each parameter,
we now observe a very different picture from the results we
obtained by looking at the variability among hosts. In fact,
the Jaccard Indexes show that for goodware and PUP there
are a large number of perfect matches over time when the
sample is re-executed. For the full results we refer the reader
to Table 5 in the Appendix.
Over time, malware actions parameters vary a lot, while
PUP’ and benign’s ones do not vary at all. The difference
is remarkable. Even at the median, the parameters of actions
performed by benign software change very little, and the 75th
percentile they change almost nothing at all. If we consider
that the same indexes were zero when considering executions
across different hosts, this result emphasize a very important
distinction. Goodware creates different ﬁles, mutexes and
registry keys in different machines. But when we consider two
USENIX Association
30th USENIX Security Symposium    3495
1(607)2(275)3(158)4(102)Week020406080100Number of missing actionsMalware missing total actions per week1(871)2(439)3(295)4(197)Week020406080100PUP missing total actions per week1(2527)2(925)3(464)4(278)Week020406080100Benign missing total actions per weekminmodemedianmeanmax01020304050Detection0.000.010.020.030.040.05Ratio of malwareMalware detections for variabilitylow var.high var.executions on the same machine, those values remain constant.
The same phenomenon does not happens for malware, where
the Jaccard index is zero across the board, both in case of
different machines and in case of different executions on the
same host. The only few exceptions to this rule are regarding
ﬁle paths and ﬁle extensions, which still have a low similarity.
If we look at the 75th percentile things get more stable and
both malware and benign ﬁles show a high similarity. This
means that for at least 25% of the samples in our dataset we
observe a stable set of parameters at different points in time.
At least 50% of the malware do not reuse same ﬁle
names, registry keys values and paths, directories
in their reexecutions, and 25% execute at least 1
new command.
5 Invariant Analysis
Our variability analysis conﬁrms that malware behavior
changes over time and on different machines. This indicates
that if a behavioral malware detection system is designed
with data collected at a ﬁxed time or from a single computer
with a particular conﬁguration setting, the real behavior that
is common to all possible executions might not be identiﬁed
correctly. However, as we showed in Section 4.2.1, the fact
that malware samples carry large variability across different
executions does not rule out the possibility of building ac-
curate detection models from behavior that remains stable.
Therefore, in this section we focus on measuring the invariant
part of malware behavior, to better understand how effective
behavioral-based detection systems can be if their models are
built upon the right set of events.
Roughly 80% of the SIGMA rules are created from values
extracted from ﬁle and process creation events, and these
two are also in top 7 most popular actions in our dataset.
Therefore, due to space limitations, in this section we focus on
those actions and their parameters. We identify the invariant
behaviors only from the malicious samples as our goal is to
evaluate behavioral malware detection techniques. We only
use benign samples when simulating a signature generation
process, by extracting the invariant parts that are not observed
in the benign execution traces.
Beyond full parameter value.
In Sections 4.1.2 and 4.2.2
we utilize the full value of the parameters to measure the
jaccard index. In this section we follow a simple approach to
split those values into smaller tokens, explained in Section 2.2,
with the aim of ﬁnding a shared value across machines.
5.1 How Many Hosts Are Enough?
One of the main consequences of the ﬁndings discussed in
Section 4 is that for building more effective and accurate sig-
natures it is necessary to collect multiple data points rather
Figure 5: CDF of number of machines and the amount of
malware values. It takes more machines to capture all the
CMD tokens than other parameters’ tokens.
than looking at a single trace collected from one environment.
While this sounds intuitive, to our knowledge, there is no
existing study that attempted to measure how many execu-
tions from different machines are needed to identify tokens
that maximize the coverage of the generated signatures. To
this end, we measure the number of executions of the same
malware in the wild that can be detected by using the tokens
extracted from a small set of executions, as well as the number
of those executions that are needed to obtain all the malicious
tokens. Based on that, we estimate how many machines are
needed to achieve a high coverage of observed behaviors.
Figure 5 shows the empirical cumulative density function
(CDF) of the fraction of malware samples for which we cap-
ture all tokens, for an increasing number of machines (x-axis).
We only consider tokens that never appear on benign traces
and we also exclude the unique tokens, i.e., those that only ap-
pear in one machine in the wild (as they could be the result of
random values). Finally, we construct the CDF by adding ﬁrst
the traces that contain the highest number of tokens, and thus
represent a conservative estimate. For 20% of the malware,
we need only 1 machine to capture all the malicious command
line tokens. If we increase the threshold to 10, we can identify
all the command line tokens for 85% of the malware and all
the ﬁle path tokens for 98% of the malware.
Naturally, the more machines we use the more tokens we
can extract, but adding more machines provides diminish-
ing returns. As we can see, for 21% of the malware we can
capture all ﬁlenames in a single execution, and for 29% of
them one execution is sufﬁcient to discover all ﬁle path to-
kens. However, we need to collect 39 traces to observe all the
malicious ﬁlename tokens that appear in 90% of the malware,
while this decreases to only 11 and 17 machines for capturing
ﬁle path and command line tokens respectively. Since the
ﬁle path seems to converge faster, in Figure 6 we check the
3496    30th USENIX Security Symposium