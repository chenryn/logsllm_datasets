Ba
(Ba + Bo) + 1
n (Sa + So)
1
T0
=
nBa
nMT0 + (Sa + So)T0
(1)
Optimizing the absolute reward rate for n epochs Rn is
equivalent to optimizing T0Rn, since this just scales the objective
by a constant. The difference between ˜Rn and T0Rn gives
|T0Rn − ˜Rn| =
Ba
−
Ba + Bo
≤ M( (k−1)M
)
n
M2
(2)
where (2) follows because Ba +Bo = M and Sa +So ≤ (k−1)M,
because there can only be at most k branches at a time and
only the longest chain ends up as the main chain.
=
Ba
(Ba + Bo) + 1
n (Sa + So)
k− 1
n
Proposition A.1 implies that over a single epoch, honest
mining is an optimal strategy for maximizing the absolute re-
ward rate; selﬁsh mining is actually less proﬁtable. This follows
because to maximize the absolute reward in proposition A.1,
B. Ethereum
Our second experiment explores the Ethereum incentive
mechanism. In this setting, we were unable to recover the true
optimal solution using an MDP solver, as the full Ethereum
16
α
-
d
r
a
w
e
R
e
v
i
t
a
l
e
R
s
n
o
i
t
c
a
’
h
c
t
a
m
‘
f
o
n
o
i
t
c
a
r
F
Fig. 11: Rewards of NE vs RL. P1’s hash
rate m1 = 0.1. Here, r1 denotes the reward
of P1 and r2 that of P2.
Fig. 12: Ethereum relative reward as a
function of adversarial hash power. RL
beats the state-of-the-art schemes.
Fig. 13: Agent 2 chooses to increase
matching proportion, loses reward, and then
returns to the equilibrium strategy.
Iteration Number
Fig. 14: Ethereum state (2,1, irrelevant) and U =
{1,2,1,0,0,0}. Orange blocks are mined by the attacker and
blue blocks by the honest miner.
Fig. 15: Ethereum state (0,0, irrelevant) and U =
{0,2,0,2,0,0}. The attacker overrode the public fork with
its secret fork and referred to two uncle blocks.
state space is too large. Because of this, existing papers on
selﬁsh mining in Ethereum [24], [51], [54] do not derive an
optimal solution like the one for Bitcoin [56]. This section
illustrates how SquirRL can be used to explore the strategy
space in scenarios where we do not have a priori intuition
about what strategies perform well and when MDP solvers are
unable to recover meaningful results.
The Ethereum incentive mechanism is similar to Bitcoin’s,
except for its use of uncle rewards. If a block is not a main-
chain block but a child block of a main-chain block, it can be
referenced as an uncle block (Figure 14). A block can have at
most two uncle-block pointers and obtains 1
32 of the full block
reward for each. In addition, the miner of the uncle obtains a
8 (1 ≤ k ≤ 6)-fraction of the full block reward, where k is
8−k
the height difference between the uncle block and the nephew
block that points to it.
Feature Extraction. Here, we illustrate how to derive the
feature extractor ϕ for Ethereum. Note that L(C,T,E) = len(C)
as noted in Section IV-D in the Ethereum example.
To compute feat(U(s)), notice that a mined block can refer
to (up to) any two uncles in the 6-block history of the main
chain. Hence, upon publishing C, the reward can depend on (a)
the presence/absence of uncle blocks at each of the 6 prior main-
chain blocks, and (b) who mined those uncle blocks. As such,
we have Spro f = (len(Ca),u), where u (cid:44) {ui}6
i=1 encodes the
information of the uncle blocks hanging on the main chain block
of height H−i, where H is the current height of the last common
block of the main chain (Figure 14). Each ui ∈ {0,1,2}; ui = 0
means there are no available uncle blocks at that height. ui = 1
and ui = 2 mean that the uncle block was mined by the attacker
or honest miner, respectively. For instance, in Figure 14, the
attacker holds a secret fork with 2 blocks, while the public
fork has only 1 block. The height of the main chain is H = 4,
and there are two uncle blocks mined by the attacker hanging
from blocks of height 1 and 3. The uncle block mined by the
honest miner is hanging at height 2. So the uncle vector is
u = {1,2,1,0,0,0}. These uncle blocks as well as the len(C)
(as in Bitcoin) determine the instantaneous reward. len(C) is
already included as part of the score, so we can leave out of the
instantaneous reward portion of our features. Finally, Ethereum
considers chains of equal length to be equally valid, regardless
of when each were made public. So there is no fork feature
required to include as part of act(s), as all the other features
will determine what actions are available.
Hence, our ﬁnal features are [len(C),H,u]. Notice that our
framework for determining Spro f does not directly store the
uncle references; instead, it stores the minimum amount of
information needed to compute the reward for any given set of
uncle references. This design choice prevents the state space
from getting bloated. For example, if we limit the maximum
number of hidden blocks to 20, the state space size is around
291,600, which is out of range for many MDP solvers, but
within range for DRL.
The uncles do not affect the state transitions. Updates to
vector u caused by the addition of a new block have three
effects: 1) Any referred uncle blocks are removed from the
vector by setting their corresponding entries to 0; this prevents
17
0.10.20.30.40.50.60.70.8P2's Hash Power m20.40.50.60.70.80.91.01.1RewardRL-r1NE-r1RL-r2NE-r20.00.10.20.30.40.5Attacker's Hash Power 0.00.20.40.60.81.0Relative RewardSquirRLSM1honest0.320.340.360.380.400.420.440.400.450.500.550.600.650.700204060801001200.040.020.00Agent 0 Rel. Reward - 0204060801001200.020.00Agent 1 Rel. Reward - 0204060801001200.100.050.00Agent 2 Rel. Reward - 0.000.010.020.03Agent 0 match proportion0.000.010.02Agent 1 match proportion0.0000.0050.0100.015Agent 2 match proportionH=4𝑢𝑢3=1𝑢𝑢2=2𝑢𝑢1=1Main ChainHanging Uncle BlocksAttacker’s Secret ForkPublic ForkH=6𝑢𝑢2=2𝑢𝑢5=0𝑢𝑢4=2𝑢𝑢3=0Main ChainAttacker’s Main Chain BlocksNew Hanging Uncle BlockFig. 16: A sample trajectory. Dotted chains
are private, and the text displays the actions
of the agents following OSM and SquirRL,
respectively.
Fig. 17: Agent C, when using SquirRL ,
tends to use the "wait" action more than
Agent B, who is following OSM.
Fig. 18: The total hash rate ﬂuctua-
tion(normalized) and the relative hash
power for the attacker with initial α = 0.4
in Bitcoin from Sep. 2018 to Oct. 2018.
future blocks from referring to these already-referred uncle
blocks. 2) As the main chain’s height is growing, the uncle
indices are shifted, and any uncle blocks deeper than depth 6
are discarded, since they cannot be referred to by any future
blocks. 3) Any fork shorter than the main chain is abandoned,
and its ﬁrst block becomes a new potential uncle block.
Performance. Figure 12 compares the relative rewards ob-
tained by SquirRL to that of other selﬁsh mining attacks in
Ethereum [24], [51], [54]. SquirRL outperforms prior schemes,
which implement constrained strategies that are similar to
SM1. As Figure 12 shows, Bitcoin OSM also outperforms
prior works. SquirRL at least matches the performance of
OSM in most cases, and for hash power ranging from 25% to
45%, outperforms OSM by 0.4% to 1.0%. SquirRL implements
a strategy that is more “stubborn” than OSM, growing its secret
fork more aggressively to compensate for the penalty of uncle
rewards accruing to the honest player when the attacker fails
to overwrite the main chain.
C. Non-monotonicity in OSM experiments (Figures 5 and 6)
There can be many OSM strategies that give the same reward
in the single-agent setting, but give different rewards in the multi-
agent setting. Figure 16 shows a sample trajectory of an OSM
strategy that caused negative excess relative rewards in Figure
5 and the positive non-monotonicity of Figure 5. The critical
difference between this strategy, which is honest in the single-
strategic-agent setting and not honest in the multi-strategic-
agent setting, with an overall honest strategy is the action the
agent chooses at (a,h,fork) = (0,0, .). In the overall honest
strategy, one should adopt at (a,h,fork) = (0,0, .). However,
in the strategy depicted in Figure 16, the agent chooses to
Justiﬁed, Correct vote
Justiﬁed, Incorrect vote
Unjustiﬁed, Correct vote
Unjustiﬁed, Incorrect vote
Multiplicative Reward
v = (1 + mρ/2)Dv
D+
v = 1+mρ/2
D+
v = Dv
D+
v = 1/(1 + ρ)Dv
D+
1+ρ Dv
Additive Reward
rv = mρ/2Dv
1+ρ − 1)Dv
rv = ( 1+mρ/2
rv = 0
rv = −ρ/(1 + ρ)Dv
TABLE IV: Reward rule in Casper FFG. The original multi-
plicative rule is not well-suited to RL systems, because the
reward can become inﬁnite over time, causing the value function
to be ill-deﬁned. Since the growth factor is small in practice, we
choose an additive reward in our experiment that approximates
the multiplicative reward over a ﬁnite time horizon. Dv is the
deposit of voter v and rv is the immediate additive reward. m
is the fraction of correct votes.
18
wait at (a,h,fork) = (0,0, .). In Figure 17, we see that the non-
monotonicity in strategy corresponds to the non-monotonicity
in rewards we observed in Figures 5 and 6. Notably, this
phenomenon was not observed in the multi-agent selﬁsh mining
analysis of [45]. This is because their semi-selﬁsh agents
automatically adopted at (a,h,fork) = (0,0, .).
D. Details of the Casper FFG Experiment
We use an epoch length of 10. The voting probability pvote is
set to 0.9. The distribution Dvote for the proportion of votes cast
per step is N (0.1,0.05), truncated to [0,1]. Casper FFG uses
a multiplicative reward mechanism. If an agent acts according
to protocol, her deposit is multiplied by a factor greater than
1, whereas if she disobeys protocol, her reward is multiplied
by a factor less than 1. Table IV lists the precise formula for
reward allocation under the column ‘Multiplicative Reward’. We
pick the reward parameters to reﬂect the parameters deployed
in practice. The target total deposit pool is D = 107. In the
original setting, the parameter ρ is calculated in every epoch by
ρi = γD−p
i +β(ESFi−2), where Di is the deposit pool in epoch
i and γ = 7· 10−3,β = 2· 10−7, p = 1/2. Notice that the ﬁrst
term of the equation dominates the second term; therefore we
omit the β(ESFi − 2) term for simplicity. Therefore, we have a
constant ρ = 2.21· 10−6. Secondly, the original reward system
is multiplicative, but we realize the total deposit pool is very big
and the multiplicative factors are within [1/(1 +ρ),1 + 1/2·ρ],
which is very close to 1. Within 1000 epochs, the absolute
reward varies less than 1%. Therefore, we ﬁx the total deposit
D = 107 and represent the rewards additively, described in Table
IV, column ‘Additive Reward’.
We justify our parameters by numerical calculation. After
the Thirdening upgrade, the production of ETH from block
mining per year is around 4.9· 106 ETH. From [12], the target
deposit pool is 107 and the in ideal situation, the annual interest
of voting is 5%, therefore the ETH production from voting
reward is 5· 105 every year. Hence, the ratio between mining
reward and voting reward is around 10 : 1. In our experiment,
one epoch contains 10 blocks, which means there are 20 ETH
mined in an epoch. In an ideal situation voting round, the
absolute reward for all voters is ρ/2D ≈ 10.6 ETH. We also
need to divide the reward by 5, since we are using 10 as the
epoch length instead of 50. Hence, the voting reward in one
epoch is 2.12 ETH, which matches the 10 : 1 ratio.
RootRLOSM: wait, RL: overrideRootRLOSMOSM: wait, RL: adopt or waitRootRLOSMOSMOSM: override, RL: adopt or waitRootRLOSMOSMOSM: doesn’t matter, RL: adopt0.10.20.30.40.5Attacker's Hash Power 0.20.30.40.50.60.70.80.9Proportion of Actions Spent WaitingSquirRL (vs OSM)OSM (vs OSM)