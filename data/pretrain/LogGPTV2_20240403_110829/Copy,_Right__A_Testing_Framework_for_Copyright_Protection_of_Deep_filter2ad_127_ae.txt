2.79±0.09
0.00±0.00
0.05±0.02
0.25±0.31
0.03±0.02
0.45±0.32
39.61±9.74
34.84±6.07
17.20
–
0.286
19.77
1.66
LAD
Copy?
0.000±0.00 Yes (4/4)
12.82±1.00 Yes (4/4)
21.64±2.47 Yes (4/4)
14.57±3.12 Yes (4/4)
20.58±3.44 Yes (4/4)
64.32±2.42 No (0/4)
62.69±1.75 No (0/4)
37.48
–
Fig. 5: Similarities of different suspect models to the victim model on CIFAR-10 (left 3 columns) and SpeechCommands (right
3 columns). We use the orange line for the positive suspect models and the blue line for negatives. Each dimension of the radar
chart corresponds to a similarity score given by one DEEPJUDGE metric. The similarity score is computed by ﬁrst normalizing
the metric, e.g., RobD, to[0 , 1] then taking 1 − RobD.
demonstrate a comparable performance with NAD wins on
4 out of the 5 positive suspects. Note that
the huge gap
between the positives and negatives indicates that all metrics
can correctly identify the positive suspects. Here, a single
metric of DEEPJUDGE was able to achieve the same level
of protection as EmbeddingWatermark.
Remark 2: Compared to state-of-the-art defense meth-
ods, DEEPJUDGE performs better in the black-box
setting and comparably in the white-box setting against
model ﬁnetuning and pruning attacks, while not tam-
pering with model training.
C. Defending Against Model Extraction
Model extraction (also known as model stealing) is consid-
ered to be a more challenging threat to DNN copyright. In
this part, we evaluate DEEPJUDGE against model extraction
attacks, which has not been thoroughly studied in prior work.
1) Attack strategies: We consider model extraction with
two different types of supporting data: auxiliary or synthetic
(see Section III). We consider the following state-of-the-art
model extraction attacks: a) JBA (Jacobian-Based Augmen-
tation [33]) samples a set of seeds from the test dataset,
then applies Jacobian-based data augmentation to synthesize
more data from the seeds. b) Knockoff (Knockoff Nets [32])
works with an auxiliary dataset that shares similar attributes
as the original training data used to train the victim model.
c) ESA (ES Attack [45]) requires no additional data but a
833
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:24:42 UTC from IEEE Xplore.  Restrictions apply. 
Fig. 6: DEEPJUDGE vs. three state-of-the-art copyright defense methods. Left: a comparison with two black-box methods
[47], [2]; Right: a comparison with one white-box method [40]. The results are normalized into [0, 1] for better visualization.
The higher the normalized value, the better the identiﬁcation of a positive suspect model.
TABLE VI: Performance of DEEPJUDGE against model extraction attacks in the black-box setting. PGD [28] is used to
generate adversarial test cases. ACC is the validation accuracy. For each metric, the values below (indicating ‘copy’) or above
(indicating ‘not copy’) the threshold τλ (the last row) are highlighted in red (copy alert) and green (no alert), respectively.
‘Yes (2/2)’: two of the metrics vote for positive (pcopy = 100%); ‘No (0/2)’: none of the metrics vote for positive (pcopy = 0%).
See more details about the 3 extraction attacks in Appendix F.
MNIST
CIFAR-10
SpeechCommands
RobD
Model Type
ACC
RobD
JBA 83.6±1.7% 0.866±0.034
Knock 94.8±0.6% 0.491±0.032
ESA 88.7±2.5% 0.175±0.056
Neg-1 98.4±0.3% 0.968±0.014
Neg-2 98.3±0.2% 0.949±0.029
τλ
0.852
–
Positive
Suspect
Models
Negative
Suspect
Models
JSD
Copy?
ACC
RobD
0.596±0.006 No (0/2) 40.3±1.5% 0.497±0.044
0.273±0.021 Yes (2/2) 74.4±1.0% 0.715±0.018
0.141±0.042 Yes (2/2) 67.1±1.9% 0.144±0.031
0.614±0.016 No (0/2) 84.2±0.6% 0.920±0.021
0.600±0.020 No (0/2) 84.9±0.5% 0.926±0.030
JSD
Copy?
ACC
0.541±0.015 No (1/2) 40.1±1.7% 0.381±0.030
0.436±0.019 Yes (2/2) 86.6±0.5% 0.618±0.012
0.249±0.033 Yes (2/2)
0.603±0.016 No (0/2) 94.9±0.7% 0.817±0.025
0.615±0.021 No (0/2) 94.5±0.8% 0.832±0.024
×
×
JSD
Copy?
0.470±0.011 No (1/2)
0.303±0.007 Yes (2/2)
–
0.456±0.014 No (0/2)
0.472±0.012 No (0/2)
×
0.538
–
–
0.816
0.537
–
–
0.727
0.405
–
huge amount of queries. ESA utilizes an adaptive gradient-
based optimization algorithm to synthesize data from random
noise. ESA could be applied in scenarios where it is hard
to access the task domain data, such as personal health data.
With the extracted data, the adversary can train a new model
from scratch, assuming knowledge of the victim model’s
architecture. The new model is considered as a successful
stealing if its performance matches with the victim model.
2) Failure of watermarking: Our experiments in Section
V-B show the effectiveness and robustness of watermarking to
ﬁnetuning and pruning attacks. Unfortunately, here we show
that the embedded watermarks can be removed by model
extraction attacks. We show the results of DNNWatermarking
and EmbeddingWatermark in Fig. 12. The extracted models by
different extraction attacks all differ greatly from the victim
model according to either TSA (from DNNWatermarking) or
BER (from EmbeddingWatermark). For example, the TSA
value for the victim model is 100%, however, the TSA values
for the three extracted copies are all below 1%. This basically
means that
the original watermarks are all erased in the
extracted models. It will inevitably lead to failed ownership
claims. This is somewhat not too surprising as watermarks are
task-irrelevant contents and not the focus of model extraction.
3) Effectiveness of DEEPJUDGE: Table VI summarizes
the results of DEEPJUDGE, which successfully identiﬁes all
positive suspect models, except when the stolen copies (by
JBA) have extremely poor performance with 15%, 44% and
55% lower accuracy than the corresponding victim model.
We note that model extraction does not always work, and
poorly performed extractions are less likely to pose a real
threat. We also observe that DEEPJUDGE works better when
the extraction is better, which therefore counters the ultimate
perfect matching goal of model extraction attacks.
Compared to model ﬁnetuning or pruning, the average RobD
and JSD values on extracted models are relatively larger,
meaning that the decision boundaries of extracted models are
more different from that of the victim model. The reason is that
extracted models are often trained from a random point, while
ﬁnetuning only slightly shifts the original boundary of the vic-
tim model, as depicted in Fig. 2. As such, model extraction is
more stealthy and more challenging for ownership veriﬁcation.
Nonetheless, the two metrics RobD and JSD, can still reveal
the unique similarities (smaller values) of the extracted models
to the victim model: the better the extraction (higher accuracy
of the extracted model), the lower the RobD and JSD values.
This indicates that the extracted model behaves more similarly
to the victim as its decision boundary gradually approaching
that of the victim, and also highlights the unique advantage
of DEEPJUDGE against model extraction attacks. Note that
JBA attack can only extract 50% of the original accuracy on
either CIFAR-10 or SpeechCommands, which should not be
considered as successful extractions.
In Fig. 7, we further show the evolution of the RobD
and JSD values throughout the entire extraction process of
Knockoff, ESA and JBA attacks. We ﬁnd that both RobD
(orange line) and JSD (red line) values decrease as the
extraction progresses, again, except for JBA. This conﬁrms
our speculation that, when tested by DEEPJUDGE, a better
extracted model will expose more similarities to its victim. By
contrast, we also study how these two values change during
834
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:24:42 UTC from IEEE Xplore.  Restrictions apply. 
Fig. 7: The RobD (orange line) and JSD (red line) scores
between the victim and extracted models throughout the entire
extraction procedure (deﬁned by sample sizes, epochs or
rounds) on MNIST.
the training process of the negative model in Fig. 7, which
shows that the independently trained negative suspect models
tend to vary more from the victim model and produce higher
RobD and JSD values.
Remark 3: Model extraction attacks are more chal-
lenging than ﬁnetuning or pruning attacks, however,
DEEPJUDGE can still correctly identify those success-
ful extractions. Moreover, the better the extraction,
the easier the extracted model will be identiﬁed by
DEEPJUDGE as a stolen copy.
VI. ROBUSTNESS TO ADAPTIVE ATTACKERS
In this section, we explore potential adaptive attacks to
DEEPJUDGE based on the adversary’s knowledge of DEEP-
JUDGE: 1) the adversary knows the testing metrics and the
test cases, or 2) the adversary only knows the testing metrics.
Contrast evaluation of watermarking & ﬁngerprinting against
similar adaptive attacks are in Appendix G.
A. Knowing Both Testing Metrics and Test Cases
In this threat model, the adversary has full knowledge of
DEEPJUDGE including the testing metrics Λ and the secret test
cases T . We also assume the adversary has a subset of clean
data. In DEEPJUDGE, we have two test settings, i.e., white-
box testing and black-box testing. The two testings differ in
the testing metrics and the generated test cases (see examples
in Fig. 17). The black-box test cases are labeled. Therefore, the
adversary can mix T into its clean subset to ﬁnetune the stolen
model to have large testing distances (i.e., black-box testing
metrics RobD and JSD) while maintaining good classiﬁcation
performance. This will fool DEEPJUDGE to identify the stolen
model to be signiﬁcantly different from the victim model.
This adaptive attack against black-box testing is denoted by
Adapt-B. Since the white-box test cases are unlabeled, the
adversary can use the predicted labels (by the victim model)
as ground-truth and ﬁnetunes the stolen model following a
Fig. 8: Detection ROC curve of RobD with exposed (left) and
new (right) test cases against Adapt-B attack on CIFAR-10.
similar procedure as Adapt-B. This attack against white-box
testing is denoted by Adapt-W. Note that the sufﬁx ‘-B/-W’
marks the target testing setting to attack, while both attacks
are white-box adaptive attacks knowing all the information.
The results of DEEPJUDGE using the exposed test cases
T are reported in Table VII. It shows that: 1) DEEPJUDGE is
robust to Adapt-W, which fails to maximize the output distance
and activation distance simultaneously nor maintaining the
original classiﬁcation accuracy; 2) though DEEPJUDGE is not
robust to Adapt-B when the test cases are exposed with labels,
it can easily recover the performance with new test cases
generated with different seeds (see the ROC curves on the
exposed and new test cases in Fig. 8); and 3) DEEPJUDGE
can still correctly identify the stolen copies by Adapt-B
when combining black-box and white-box testings (the ﬁnal
judgements are all correct). Comparing the non-trivial effort
of retraining/ﬁnetuning a model to the efﬁcient generation of
new test cases, DEEPJUDGE holds a clear advantage in the
arms race against ﬁnetuning-based adaptive attacks.
It is noteworthy that Adapt-W did not break all white-box
metrics of DEEPJUDGE, since the mechanism of white-box
testing is robust. Speciﬁcally, black-box testing characterizes
the behaviors of the output layer, while white-box testing
characterizes the internal behaviors of more shallow layers.
Due to the over-parameterization property of DNNs,
is
relatively easy to ﬁne-tune the model to overﬁt the set of
black-box test cases, subverting the results of the black-box
metrics. However, in white-box testing, changing the activation
status of all hidden neurons on the set of white-box test
cases is almost impossible without completely retraining the
model. Therefore, white-box testing is inherently more robust
to adaptive attacks, especially when the test cases are exposed.
it
B. Knowing Only the Testing Metrics
In this threat model, the adversary can still adapt in different
ways. We consider two adaptive attacks: adversarial training
targeting on black-box testing and a general transfer learning
attack on white-box testing, respectively.
1) Blind adversarial training: Since our black-box testing
mainly relies on probing the decision boundary difference
using adversarial test cases, the adversary may utilize adver-
sarial training to improve the robustness of the stolen copy.
Given the PGD parameters and a subset of clean data (20%
of the original training data), the adversary iteratively trains
the stolen model to smooth the model decision boundaries
following [28]. This type of adaptive attack is denoted by Adv-
Train. As Table VII shows, it can indeed circumvent our black-
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:24:42 UTC from IEEE Xplore.  Restrictions apply. 
835
TABLE VII: Performance of DEEPJUDGE against several adaptive attacks on the CIFAR-10 dataset. Adapt-B: adaptive attack
against black-box testing; Adapt-W: adaptive attack against white-box testing; Adv-Train: adversarial training, VTL: vanilla
transfer learning. For each metric, the values below (indicating ‘copy’) or above (indicating ‘not copy’) the threshold τλ (the
last row) are highlighted in red (copy alert) and green (no alert) respectively.
Model Type
Black-box Testing
JSD
RobD
ACC
81.4±0.9% 0.985±0.011
Adapt-B
Adapt-W 71.9±1.8% 0.519±0.048
74.5±2.3% 0.939±0.087
Adv-Train
93.3±1.7%
84.2±0.6% 0.920±0.021
84.9±0.5% 0.926±0.030
VTL
Neg-1
Neg-2
×
0.665±0.007
0.372±0.025
0.637±0.036
×
0.603±0.016
0.615±0.021
Positive
Suspect
Models
Negative
Suspect
Models
White-box Testing
NAD
LOD
0.44±0.15
1.94±0.12
0.79±0.17
1.08±0.14
10.94±1.74
11.09±0.71
1.12±0.06
11.62±0.54
1.89±0.14
2.58±0.24
11.85±1.01
12.60±1.33
NOD
0.38±0.04
3.11±0.12
0.68±0.11
0.85±0.23
3.09±0.30
3.21±0.18
1.79
LAD
Copy?
0.40±0.05 Yes (4/6)
1.89±0.33 Yes (4/6)
0.75±0.08 Yes (4/6)
0.64±0.15 Yes (4/4)
5.41±0.67
No (0/6)
5.37±0.72
No (0/6)
3.01
–
τλ
–
0.816
0.537
6.14
6.89
Fig. 9: Detection ROC curve of RobD with adversarial test
cases generated from high-conﬁdence (left) or low-conﬁdence
seeds (right) against Adv-Train attack on CIFAR-10.
box testing, with a sacriﬁce of ∼ 10% performance (a phe-
nomenon known as accuracy-robustness trade-off [39], [46]).
However, interestingly, if we replace the high-conﬁdence seeds
used in DEEPJUDGE with low-conﬁdence seeds, DEEPJUDGE
becomes effective again (as shown in Fig. 9). One possible
reason is that, compared to high-conﬁdence seeds, these low-
conﬁdence seeds are natural boundary (hard) examples that
are close to the decision boundary, thus can generate more test