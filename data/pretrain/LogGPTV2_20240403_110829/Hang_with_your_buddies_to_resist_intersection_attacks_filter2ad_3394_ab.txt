ply checks whether all known users are presently online, i.e.,
whether Oi = Mi, returning Pi = Mi if so, and otherwise
returning Pi = ∅. In eﬀect, the Policy Oracle forbids the sys-
tem from making progress—allowing anyone to post to any
Nym—except when all users are online. Since messages are
posted only when all users are online, the intersection of all
nonempty rounds’ user sets is Mi, and the system preserves
“perfect” anonymity assuming the Anonymizer performs as
required. The tradeoﬀ, of course, is eﬀective system availabil-
ity, which would be unusable in most practical situations.
The key technical challenge, and a primary contribution
of this paper, is developing more nuanced methods of con-
trolling the user ﬁltering step in each round. By controlling
these ﬁltering choices, we seek to maintain both measurable
anonymity levels under intersection attack and “usable” lev-
els of availability, under arguably realistic conditions.
There are situations in which no active control mechanism
will help. If all users but one go oﬄine permanently, for ex-
ample, the Policy Oracle has only two choices: eventually
allow the one remaining user to post, giving up all ano-
nymity under intersection attack; or ﬁlter that user forever,
giving up availability completely. Thus, we must set realistic
expectations. Section 5 experimentally investigates the fea-
sibility of resisting intersection attacks in IRC communities,
and tests possible control policies against feasibility metrics.
Buddies’ architecture separates the Policy Oracle from the
Anonymizer, giving the Policy Oracle access only to “pub-
lic information” we assume is known to everyone including
the adversary, eliminating the risk that policies may “acci-
dentally” compromise anonymity by leaking Nym ownership.
By architecturally disallowing the Policy Oracle from hav-
ing access to private information, we avoid the need to ana-
lyze each policy carefully for such “side-channel” anonymity
leaks, and instead can focus purely on the main question of
how eﬀectively a policy mitigates intersection attacks while
maintaining usable levels of availability.
Another issue is whether the information the Policy Ora-
cle needs to simulate the Adversary’s intersection attacks—
such as the set of users online in each round—should be
considered “public information.” Although an ideal global
adversary would know this information anyway, more re-
alistic adversaries may be unable to monitor all users. If
Buddies’ design “hands out” information that would other-
wise be at least partially private—such as the IP addresses
of all online users—we risk accidentally “strengthening” a
weak adversary into an eﬀectively omniscient adversary. In
the important case of users’ network identities such as IP
addresses, our design mitigates this leak by replacing IP ad-
dresses with anonymized tags when reporting online user
sets to the Policy Oracle, as discussed in Section 4.3. How-
ever, whether Buddies’ simulation-based architecture may
strengthen weak adversaries in other unexpected ways, by
making “too much information” public, merits further study.
2.3 Analyzing Intersection Attacks
While we do not attempt full formal analysis, the sim-
plicity of the conceptual model facilitates straightforward
informal analysis. Our focus here is on a particular class of
attacks, namely what an adversary can learn from users’ on-
line status over time (the “switches” in Figure 1). The many
other known attacks against practical anonymity systems
are important but out of this paper’s scope. We also claim
no particular novelty in our analysis techniques or metrics;
our goal is merely to apply known attacks [16, 31, 42] and
anonymity metrics [17, 43] to the Buddies model.
We assume the Anonymizer is trusted to “do its job,” keep-
ing secret the linkage between Users and the Nyms they own.
We also assume honest Users—those users we care about
protecting—do not “give away” their identities or the rela-
tionships between their Nyms via the messages they post.
Under these conditions, the Adversary obtains three po-
tentially important pieces of information in each round i:
(a) the set of online users Oi, (b) the set Pi of online users
who passed the Policy Oracle’s ﬁlter in step 5, and (c) the
Bi message bits that were posted to the scheduled Nym Ti.
An observation that will be key to Buddies’s design is that
only (b) and (c) will actually prove relevant to intersection
attack analysis: the adversary ultimately obtains no useful
information from knowing which users were online during a
given round, beyond what the adversary learns from know-
ing which users were online and unﬁltered.
Since we assume honest users do not “give away” their
identities in their message content, we ultimately care only
whether the message posted to Nym Ti was null or non-null.
If a non-null message appeared for Nym Ti in round i, then
the adversary infers that the owner of Ti must be a member
of the ﬁltered user set Pi in that round. (If the owner of Ti
1155was online but ﬁltered in round i, then a null message would
have appeared, as if the owner was oﬄine.)
If a null message appears for Ti, however, the Anonym-
izer’s design ensures that the adversary cannot distinguish
among the following three possibilities: (1) the owner of Nym
Ti was oﬄine, (2) the owner was online but ﬁltered, or (3)
the owner was online and unﬁltered, but had nothing useful
to send and thus intentionally posted a null message.
a non-null message appeared: i.e., PN =(cid:84)
2.3.1 Possibilistic Anonymity Analysis
To construct a simple possibilistic anonymity set PN for a
given Nym N , the adversary intersects the ﬁltered user sets
Pi across all rounds i for which Nym N was scheduled and
i{Oi | Ti = N ∧
mi (cid:54)= 0}. Thus, PN represents the set of users who might
conceivably own Nym N , consistent with the observed set
of non-null messages that have appeared for Nym N up to
any point in time. Since the adversary cannot distinguish
whether the appearance of a null message means that N ’s
owner was oﬄine, ﬁltered, or merely had nothing to send,
null-message rounds do not eliminate the possibility that
users oﬄine during that round may own N , so such rounds
leave the possibilistic anonymity set PN unaﬀected.
We deﬁne the size of a Nym’s possibilistic anonymity set,
|PN|, as Nym N ’s possibilistic anonymity, which for conve-
nience we abbreviate as possinymity. Although possinymity
is only one of the many useful anonymity metrics that have
been proposed [32, 33], and perhaps a simplistic one, we
feel it captures a useful measure of “plausible deniability.”
If for example a user is dragged into court, and the judge is
shown network traces of a Buddies system in which the ac-
cused is one of |PN| users who may in principle have posted
an oﬀending message, then a large possibilistic anonymity
may help sow uncertainty of the user’s guilt. We fully ac-
knowledge the weaknesses of plausible deniability in gen-
eral, however, especially in environments where “innocent
until proven guilty” is not the operative principle.
2.3.2 Probabilistic Anonymity Analysis
While a simplistic adversary might stop at the above anal-
ysis, a smarter adversary can probabilistically learn not only
from rounds in which non-null messages appeared, but also
from rounds in which only a null message appeared.
Suppose for example the adversary correctly surmises from
past observation that, in each round i, the owner of Nym N
will have no useful message to post with some independent
and uniformly random probability p. In this case the user
will “pass” by submitting a null message. With probability
1 − p the user will have a non-null message and will try to
post it—but this post attempt fails, yielding a null message
anyway, if the owner is oﬄine or ﬁltered in that round.
For simplicity assume there are two users A and B, the
adversary observes exactly one round i, this round results in
a null message, and Pi = {A}: user A participated but user
B did not. The null output from round i means one of two
events occurred: (a) A owns N , but chose with probability
p not to post in round i; or (b) B owns N , and no message
appeared independently of p because B (cid:54)∈ Pi. Because Nyms
are assigned to users uniformly at random on creation, the
“base” probability that either user owns N is 1/2. The prob-
ability of the above events (a) and (b) occurring conditioned
on the observed history, however, is diﬀerent. To be pre-
cise, P [(a) | (a) ∪ (b)] = P [(a) ∩ ((a) ∪ (b))]/P [(a) ∪ (b)] =
(p/2)/P [(a)∪(b)], and P [(b) | (a)∪(b)] = (1/2)/P [(a)∪(b)].
Since (a) and (b) are disjoint events, P [(a)∪ (b)] = P [(a)] +
P [(b)], so P [(a)] = (p/2)/(p/2 + 1/2) = p/(p + 1), and
P [(b)] = (1/2)/(p/2 + 1/2) = 1/(p + 1).
From the adversary’s perspective, observing one round in
which no message appears for Nym N , and in which A par-
ticipated but B did not, reduces the relative likelihood of A
being the owner by a factor of p. Observing similar events
across multiple rounds exponentially increases the adver-
sary’s “certainty” of B being the owner: after k such rounds,
the likelihood of A being the owner is only pk/(pk + 1).
Indistinguishability Under Probabilistic Attack
2.3.3
The above reasoning generalizes to many users, varying
probabilities of posting, etc. Our focus is not on deepening
such analysis, however, a goal admirably addressed in prior
work [17, 43]. Instead, we wish to achieve measurable resis-
tance to unknown probabilistic attacks: we do not know the
probabilities with which users will attempt to post in par-
ticular rounds, how well the unknown attacker may be able
to predict when the owner of a given Nym will post, etc.
Instead of relying on the relevance of any particular proba-
bilistic analysis—which may break each time a known attack
is reﬁned—Buddies relies on an indistinguishability principle
that applies to all attacks of this class. If two users A and B
have exhibited identical histories with respect to inclusion in
each round’s ﬁltered user set Pi, across all rounds i in which
a Nym N was scheduled so far, then under any probabilistic
analysis of the above form the adversary must assign identi-
cal probabilities to A and B owning Nym N . That is, if for
every round i, it holds that (A ∈ Pi) ⇐⇒ (B ∈ Pi), then
users A and B are probabilistically indistinguishable from
each other, hence equally likely to own Nym N .
For any user A and Nym N , we deﬁne A’s buddy set
BN (A) as the set of users probabilistically indistinguish-
able from A, including A itself, with respect to potential
ownership of Nym N . If n users are probabilistically indis-
tinguishable from A, then under the attacker’s analysis each
such user in BN (A) has an individual probability no greater
than 1/|BN (A)| of being the owner of N . Intuitively, buddy-
sets form equivalence classes of users who “hang together”
against probabilistic intersection attacks—so that individual
buddies do not “hang separately.”
We next deﬁne a second anonymity metric, indistinguisha-
bility set size, or indinymity for short, as the size of the
smallest buddy-set for a given Nym N . Since we do not
know how a real attacker will actually assign probabilities
to users, indinymity represents the minimum level of ano-
nymity a member of any buddy set can expect to retain,
even if the adversary correctly intersects the owner’s ano-
nymity set down to the members of that buddy set. Thus,
the attacker cannot (correctly) assign a probability greater
than 1/|BN| to any user—including, but not limited to, the
owner of N .
One might argue that we “mainly” care about the buddy
set containing the true owner of N , not about other buddy
sets not containing the owner. A counter-argument, however,
is that a particular observation history might make some
other buddy set falsely appear to the adversary as “more
likely” to own N . In this case, we may well care how much
protection the innocent members of that “unlucky” buddy
set have against being “falsely accused” of owning N . Thus,
to ensure that all users have the “strength in numbers” of
1156being indistinguishable in a crowd of at least n users, re-
gardless of the adversary’s probabilistic reasoning, we must
ensure that all buddy sets have size at least n.
3. ATTACK MITIGATION POLICIES
Based on the above architecture, we now explore possible
intersection attack mitigation policies. We make no claim
that these are the “right” policies, merely a starting point for
ongoing reﬁnement. Two key beneﬁts of Buddies’ architec-
ture, however, are to modularize these policies into replace-
able components independent of the rest of the anonymous
communication system so they can be further evolved easily,
and to ensure by system construction that policies cannot
leak sensitive information other than by failing to protect
adequately against intersection attacks.
We ﬁrst explore policies for maintaining possinymity, then
policies to enforce a lower bound on indinymity. An impor-
tant caveat with any anonymity metric is that Buddies can-
not guarantee that measured anonymity necessarily repre-
sents useful anonymity, if for example an attacker can com-
promise many users or create many Sybil identities [19]. Sec-
tion 4.4 discusses these issues in more detail.
3.1 Maximizing Possinymity
The possinymity metric deﬁned in Section 2.3.1 considers
only rounds in which non-null messages appear for some
Nym N , intersecting the ﬁltered user sets across all such
rounds to determine N ’s possinymity set PN . We consider
several relevant goals: maintaining a minimum possinymity
level, mitigating the rate of possinymity loss, or both.
Maintaining a Possinymity Threshold.
Suppose a dissident, posting anonymously in a public chat
room under a Nym N , wishes to maintain “plausible denia-
bility” by ensuring that |PN| ≥ 100 throughout the conver-
sation—and would rather be abruptly disconnected from the
conversation (or have Nym N eﬀectively “squelched”) than
risk |PN| going below this threshold. As a straightforward
policy for this case, at step 5 of each round i, the Policy Or-
acle computes the new possinymity that N would have if Oi
is intersected with N ’s “running” possinymity set from the
prior round. The Policy Oracle returns Pi = Oi if the new
possinymity remains above threshold, or Pi = ∅ otherwise.
In practice, the eﬀect is that N ’s possinymity starts out
at an initial maximum of the total set of users online when
the user ﬁrst posts via N , then decreases down to (but not
below) the possinymity threshold as other users go oﬄine,