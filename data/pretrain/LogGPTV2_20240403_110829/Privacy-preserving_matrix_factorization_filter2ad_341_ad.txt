806The above execution, which is as computationally intensive
as learning V in Section 3, can be performed as frequently as
matrix factorization in real-life systems, e.g., once a week.
In between such computations, whenever a user i wishes
to get recommendations, she encrypts her proﬁle under her
own public key pki with an additively homomorphic en-
cryption scheme Epki (like Paillier’s cryptosystem [52])3 and
sends the resulting value Epki (ui) to the RecSys. The Rec-
Sys forwards Epki (ui) to the CSP and also computes, for
j ∈ [m], Epki ((cid:104)ui, ˆvj(cid:105)). The CSP in turn computes for
j ∈ [m], Epki ((cid:104)ui, j(cid:105)) , and returns this value to the RecSys.
The RecSys subtracts Epki ((cid:104)ui, j(cid:105)) from Epki ((cid:104)ui, ˆvj(cid:105)) =
Epki ((cid:104)ui, vj + j(cid:105)) to obtain the encryption of the predicted
ratings ˆrij = Epki ((cid:104)ui, vj(cid:105)) for all items j ∈ [m] and sends
them to the user i. The user uses her private decryption
key to obtain in the clear the predictions ri1, . . . , rim.
4.2 Malicious RecSys
Our basic protocol, as described in Section 3, operates
under the honest-but-curious model. However, a malicious
RecSys can alter, duplicate or drop user ratings as a means
to have the output model leak information about individual
ratings. For example, the RecSys can provide inputs to the
circuit from only a single user and feed dummy ratings for
the remaining ones.
It would then learn from the output
model the set of items that were rated by this victim user:
the RecSys simply observes which of the learned item proﬁles
has a non-unit norm. This would clearly violate user privacy.
In order to prevent the RecSys from misbehaving, we need
the CSP to build a circuit that, beyond outputting V , also
veriﬁes that the input to the circuit contains all user ratings,
that the ratings were not changed, and that no dummy ra-
tings were input. To do so we require users to obtain one-
time MAC keys (one key per rating) from the CSP which
they then use to sign their ratings. The CSP builds a circuit
that veriﬁes these MACs, making sure that the ratings were
not altered, and that the exact number of ratings submitted
by each user are provided as input to the circuit.
Our approach is to have each user ﬁrst communicate with
the CSP, reporting the number of ratings that she will send
to the RecSys. The CSP in return sends the user a set of
one-time MAC keys for each rating tuple, which the user
uses for signing each tuple (j, rij). The garbled circuit the
CSP builds veriﬁes each input tuple with a speciﬁc MAC key,
requiring the RecSys to provide the exact inputs as reported
by the users to the CSP, sorted w.r.t. i and j. Any deviation
from this order or the introduction of dummy ratings will
result in a veriﬁcation failure. Assume that the output of
the veriﬁcation circuit for each signed tuple is a bit that is
set to 1 if the veriﬁcation succeeds and 0 otherwise. The
veriﬁcation bits of all input tuples are fed into an and gate,
so that if at least one veriﬁcation fails, the output of the
and gate is 0. If the outputs of this and gate is 0 then the
circuit sets its overall output to 0. This way, if at least one
veriﬁcation failed the circuit simply outputs 0. We chose to
use fast one-time MACs based on pairwise independent hash
functions so that the number of gates needed to verify these
MACs is relatively small.
3Speciﬁcally, it is required that the encryptions of two mes-
sages Epk(m1) and Epk(m2) satisfy Epk(m1) (cid:63) Epk(m2) =
Epk(m1 + m2) for some binary operator (cid:63). To ease nota-
tion, we use multiplication for (cid:63) as is the case for Paillier
encryption.
5.
IMPLEMENTATION
We implemented our system to assess its practicality. Our
garbled circuit construction was based on FastGC, a publicly
available garbled circuit framework [24].
5.1 FastGC
FastGC [24] is a Java-based open-source framework, which
enables circuit deﬁnition using elementary xor, or and and
gates. Once circuits are constructed, FastGC handles gar-
bling, oblivious transfer and garbled circuit evaluation.
FastGC incorporates several known optimizations that aim
to improve its memory foot-print and garbling and execu-
tion times. These optimizations include so-called “free” xor
gates [34], in which xor evaluation is decryption-free and
thereby of negligible cost, and garbled-row reduction for
non-xor gates [53], which reduces communication by 25%.
FastGC also provides an “addition of 3 bits” circuit [33], en-
abling additions with 4 “free” xor gates and one and gate.
OT extensions [27] are also implemented: these signiﬁcantly
increase the number of transfers made during OT, reducing
communication overhead and lowering execution time.
Finally, FastGC enables the garbling and evaluation to
take place concurrently on two separate machines. The CSP
processes gates into garbled tables and transmits them to
the RecSys in the order deﬁned by circuit structure. Once
a gate was evaluated its corresponding table is immediately
discarded, which brings memory consumption caused by the
garbled circuit to a constant.
5.2 Extensions to the Framework
Before garbling and executing the circuit, FastGC repre-
sents the entire ungarbled circuit in memory as a set of Java
objects. These objects incur a signiﬁcant memory overhead
relative to the memory footprint of the ungarbled circuit, as
only a subset of the gates is garbled and/or executed at any
point in time. Moreover, although FastGC performs gar-
bling in parallel to the execution process as described above,
both operations occur in a sequential fashion: gates are pro-
cessed one at a time, once their inputs are ready. Clearly,
this implementation is not amenable to parallelization.
We modiﬁed the framework to address these issues, re-
ducing the memory footprint of FastGC but also enabling
paralellized garbling and computation across multiple pro-
cessors. In particular, we introduced the ability to partition
a circuit horizontally into sequential “layers”, each one com-
prising a set of vertical “slices” that can be executed in par-
allel. A layer is created in memory only when all its inputs
are ready. Once it is garbled and evaluated, the entire layer
is removed from memory, and the following layer can be con-
structed, thus limiting the memory footprint to the size of
the largest layer. The execution of a layer uses a scheduler
that assigns its slices to threads, which run in parallel. Al-
though we implemented parallelization on a single machine
with multiple cores, our implementation can be extended to
run across diﬀerent machines in a straightforward manner
since no shared state between slices is assumed.
Finally, to implement the numerical operations outlined in
Algorithm 1, we extended FastGC to support addition and
multiplications over the reals with ﬁxed-point number repre-
sentation, as well as sorting. For sorting, we used Batcher’s
sorting network [3]. Fixed-point representation introduces a
tradeoﬀ between the accuracy loss resulting from truncation
and the size of circuit, which we explore in Section 6.
8075.3 Optimizing Algorithm 1
We optimize the implementation of Algorithm 1 in multi-
ple ways. In particular, we (a) reduce the cost of sorting by
reusing comparisons computed in the beginning of the cir-
cuit’s execution, (b) reduce the size of array S, (c) optimize
swap operations by using xors, and (d) parallelize compu-
tations. We describe these optimizations in detail below.
Comparison Reuse. As described in Appendix C, the
basic building block of a sorting network is a compare-and-
swap circuit, that compares two items and swaps them if
necessary, so that the output pair is ordered. Observe that
the sorting operations (Lines 4 and 8) of Algorithm 1 per-
form identical comparisons between tuples at each of the K
gradient descent iterations, using exactly the same inputs
per iteration. In fact, each sorting permutes the tuples in
array S in exactly the same manner, at each iteration.
We exploit this property by performing the comparison
operations for each of these sortings only once. In particular,
we perform sortings of tuples of the form (i, j, ﬂag, rating)
in the beginning of our computation (without the payload
of user or item proﬁles), e.g., w.r.t. i and the ﬂag ﬁrst, j
and the ﬂag, and back to i and the ﬂag. Subsequently, we
reuse the outputs of the comparison circuits in each of these
sortings as input to the swap circuits used during gradient
descent. As a result, the “sorting” network applied at each
iteration does not perform any comparisons, but simply per-
mutes tuples (i.e., it is a “permutation” network).
Row Reduction. Precomputing all comparisons allows us
to also drastically reduce the size of tuples in S. To begin
with, observe that the rows corresponding to user or item ids
are only used in Algorithm 1 as input to comparisons during
sorting. Flags and ratings are used during copy and update
phases, but their relative positions are identical at each it-
eration. Moreover, these positions are produced as outputs
when sorting the tuples (i, j, ﬂag, rating) at the beginning
of our computation. As such, “permutation” operations per-
formed at each iteration need only be applied to user and
item proﬁles; all other rows are removed from S.
One more trick reduces the cost of permutations by an ad-
ditional factor of 2. We ﬁx one set of proﬁles, e.g., users, and
permute only item proﬁles. Then, item proﬁles rotate be-
tween two states, each one reachable from the other through
permutation: one in which they are aligned with user pro-
ﬁles and partial gradients are computed, and one in which
item proﬁles are updated and copied.
XOR Optimization. Given that xor operations can be
executed for “free”, we optimize comparison, swap, update
and copying operations by using xors wherever possible.
For comparisons, we reduce the use of and and or gates
using a technique by Kolesnikov et al. [33]. Swap operations
are implemented as follows: for b ← x > y the comparison
bit between tuples x and y (which, by the above optimiza-
tions, is pre-computed at the beginning of circuit execution),
a swap is performed as:
(cid:48) ← [b ∧ (x ⊕ y)] ⊕ x, and y
x
(cid:48) ← x
(cid:48) ⊕ (x ⊕ y)
Finally, copy operations are also optimized to use xor’s.
Observe that the copy operation takes two elements x and
y and a ﬂag s and outputs a new element y(cid:48) which is equals
y if s = 0 and x, otherwise. This is performed as follows:
(cid:48) ← y ⊕ [s ∧ (x ⊕ y)]
x
Parallelization. As discussed in Section 3.4, sorting and
gradient computations constitute the bulk of the computa-
tion in our circuit (copying and updating contribute no more
than 3% of the execution time and 0.4% of the non-xor
gates); we parallelize these operations through our exten-
sion of FastGC. Gradient computations are clearly paralleliz-
able; sorting networks are also highly parallelizable (paral-
lelization is the main motivation behind their development).
Moreover, since many of the parallel slices in each sort are
identical, we reused the same FastGC objects deﬁning the
circuit slices with diﬀerent inputs, signiﬁcantly reducing the
need to repeatedly create and destroy objects in memory.
6. EXPERIMENTS
We now assess the performance of our implementation.
We use two commodity servers, 1.9GHz 16-cores 128GB
RAM each, one acting as the RecSys and the other as the
CSP. We use both real and synthetic datasets. For the real
dataset we use MovieLens, a movie rating dataset that is
commonly used for recommender systems research, that con-
sists of 943 users that submit 100K ratings to 1682 movies.
We use the following evaluation metrics. Our solution in-
troduces inaccuracies due to our use of ﬁxed-point represen-
tation of real numbers. Thus, our goal here is to understand
the relative error of our approach compared to a system that
operates in the clear with ﬂoating point representation. Let
E(U, V ) denote the squared error for a given user’s proﬁle
i vj)2;
U and items proﬁles V , E(U, V ) =(cid:80)
(i,j)∈M(rij − uT
we deﬁne the relative error as
|E(U
∗
, V
∗
) − E(U, V )|/E(U, V )
where U∗ and V ∗ are computed using our solution and U and
V are computed using gradient descent executed in the clear
over ﬂoating point arithmetic, i.e., with minimal precision
loss. Our time metric captures the execution time needed
to garble and evaluate the circuit. We note that we exclude
the encryption and decryption times performed by the users
and the CSP, since these are short in duration compared
to the circuit processing time. The communication metric
is deﬁned by the number of bytes that are transmitted be-
tween the CSP and RecSys; it captures the size of the circuit,
namely the number of non-XOR gates, but unlike the time
metric, communication is not aﬀected by parallelization.
The relative error of our solution using the complete Movie-
Lens dataset is shown in Figure 4. We study the relative
error over a range of parameters, varying the number of
bits allocated to the fractional part of the ﬁxed point repre-
sentation and the number of iterations of gradient descent.
Overall we see that for more than 20 bits, the relative errors
are very low. When the number of bits is small, the gradient
descent method may converge to a diﬀerent local minimum
of (1) than the one reached in the clear. Beyond 20 bits
allocated for the fractional part, our solution converges to
the same local minimum, and errors decrease exponentially
with additional bits. The relative error increases with the
number of iterations because the errors introduced by the
ﬁxed point representations accumulate across the iterations,
however this increase is very small. We note that this should
not be confused with the regularized least square error (1),
which actually decreases when using more iterations.
In the following experiments we used synthetic data with
100 users, 100 items, a dimension of 10 for the user and
item proﬁles, 20 bits for the fractional part of the ﬁxed-point
808Figure 4: Relative errors due to
ﬁxed point representation
Figure 5: Execution time per it-
eration w.r.t. no. of tuples
Figure 6: Communication cost
per iteration w.r.t. no. of tuples
representation (36 bits overall). We measure the time and
communication it takes to perform (garble and evaluate) a
single iteration of gradient descent. Clearly with T iterations
execution and communication grow by a factor T .
Figure 5 shows the increase in time per iteration as we in-
crease the number of ratings in the dataset (the logarithmic
x-axis corresponds to the number of tuples in S that grows
with M since n and m are ﬁxed). The plot also illustrates
the proportion of time spent in various sections of our algo-
rithm. We note that, in all executions, the time spent on
update and copy phases, which are more diﬃcult to paral-
lelize, never exceeded 3%, and thus is omitted from the plots
as it is not visible.
The plot conﬁrms that the growth is almost linear with
the number of ratings, Θ(M log2 M ). Furthermore, we ob-
serve that more than 2/3 of the execution time is spent on
gradient computations (mainly due to vector multiplication