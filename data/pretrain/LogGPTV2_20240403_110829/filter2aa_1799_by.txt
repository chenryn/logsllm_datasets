277
In a VM with nested virtualization enabled, you can enable the L1 hypervisor debugger only 
through the serial port by using the following command in the debugged system:
bcdedit /hypervisorsettings SERIAL DEBUGPORT:1 BAUDRATE:115200
The creation of the root partition and the boot virtual processor
The first steps that a fully initialized hypervisor needs to execute are the creation of the root partition 
and the first virtual processor used for starting the system (called BSP VP). Creating the root partition 
follows almost the same rules as for child partitions; multiple layers of the partition are initialized one 
after the other. In particular:
1.
The VM-layer initializes the maximum allowed number of VTL levels and sets up the partition
privileges based on the partition’s type (see the previous section for more details). Furthermore,
the VM layer determines the partition’s allowable features based on the specified partition’s
compatibility level. The root partition supports the maximum allowable features.
2.
The VP layer initializes the virtualized CPUID data, which all the virtual processors of the parti-
tion use when a CPUID is requested from the guest operating system. The VP layer creates the
hypervisor process, which backs the partition.
3.
The Address Manager (AM) constructs the partition’s initial physical address space by using
machine platform-dependent code (which builds the EPT for Intel, NPT for AMD). The con-
structed physical address space depends on the partition type. The root partition uses identity
mapping, which means that all the guest physical memory corresponds to the system physical
memory (more information is provided later in this chapter in the “Partitions’ physical address
space” section).
In a VM with nested virtualization enabled, you can enable the L1 hypervisor debugger only 
through the serial port by using the following command in the debugged system:
bcdedit /hypervisorsettings SERIAL DEBUGPORT:1 BAUDRATE:115200
278 
CHAPTER 9 Virtualization technologies
Finally, after the SynIC, IOMMU, and the intercepts’ shared pages are correctly configured for the 
partition, the hypervisor creates and starts the BSP virtual processor for the root partition, which is the 
unique one used to restart the boot process. 
A hypervisor virtual processor (VP) is represented by a big data structure (VM_VP), shown in 
Figure 9-6. A VM_VP data structure maintains all the data used to track the state of the virtual proces-
sor: its platform-dependent registers state (like general purposes, debug, XSAVE area, and stack) and 
data, the VP’s private address space, and an array of VM_VPLC data structures, which are used to track 
the state of each Virtual Trust Level (VTL) of the virtual processor. The VM_VP also includes a pointer to 
the VP’s backing thread and a pointer to the physical processor that is currently executing the VP.
Intercept Packet
Backing Thread
Virtual Registers
State
Pointer to the
Physical CPU_PLS
VM_VPLC Array
VP’s Private Address
Space and Zone
SynIC Data
VTL 1
VTL 0
Physical CPU
FIGURE 9-6 The VM_VP data structure representing a virtual processor.
As for the partitions, creating the BSP virtual processor is similar to the process of creating normal 
virtual processors. VmAllocateVp is the function responsible in allocating and initializing the needed 
memory from the partition’s compartment, used for storing the VM_VP data structure, its platform-
dependent part, and the VM_VPLC array (one for each supported VTL). The hypervisor copies the initial 
processor context, specified by the HvLoader at boot time, into the VM_VP structure and then cre-
ates the VP’s private address space and attaches to it (only in case address space isolation is enabled). 
Finally, it creates the VP’s backing thread. This is an important step: the construction of the virtual 
processor continues in the context of its own backing thread. The hypervisor’s main system thread at 
this stage waits until the new BSP VP is completely initialized. The wait brings the hypervisor scheduler 
to select the newly created thread, which executes a routine, ObConstructVp, that constructs the VP in 
the context of the new backed thread.
ObConstructVp, in a similar way as for partitions, constructs and initializes each layer of the virtual 
processor—in particular, the following:
1. 
The Virtualization Manager (VM) layer attaches the physical processor data structure (CPU_PLS)
to the VP and sets VTL 0 as active.
CHAPTER 9 Virtualization technologies
279
2.
The VAL layer initializes the platform-dependent portions of the VP, like its registers, XSAVE
area, stack, and debug data. Furthermore, for each supported VTL, it allocates and initializes
the VMCS data structure (VMCB for AMD systems), which is used by the hardware for keeping
track of the state of the virtual machine, and the VTL’s SLAT page tables. The latter allows each
VTL to be isolated from each other (more details about VTLs are provided later in the “Virtual
Trust Levels (VTLs) and Virtual Secure Mode (VSM)” section) . Finally, the VAL layer enables
and sets VTL 0 as active. The platform-specific VMCS (or VMCB for AMD systems) is entirely
compiled, the SLAT table of VTL 0 is set as active, and the real-mode emulator is initialized. The
Host-state part of the VMCS is set to target the hypervisor VAL dispatch loop. This routine is
the most important part of the hypervisor because it manages all the VMEXIT events generated
by each guest.
3.
The VP layer allocates the VP’s hypercall page, and, for each VTL, the assist and intercept mes-
sage pages. These pages are used by the hypervisor for sharing code or data with the guest
operating system.
When ObConstructVp finishes its work, the VP’s dispatch thread activates the virtual processor and 
its synthetic interrupt controller (SynIC). If the VP is the first one of the root partition, the dispatch 
thread restores the initial VP’s context stored in the VM_VP data structure by writing each captured 
register in the platform-dependent VMCS (or VMCB) processor area (the context has been specified 
by the HvLoader earlier in the boot process). The dispatch thread finally signals the completion of the 
VP initialization (as a result, the main system thread enters the idle loop) and enters the platform-
dependent VAL dispatch loop. The VAL dispatch loop detects that the VP is new, prepares it for the first 
execution, and starts the new virtual machine by executing a VMLAUNCH instruction. The new VM 
restarts exactly at the point at which the HvLoader has transferred the execution to the hypervisor. The 
boot process continues normally but in the context of the new hypervisor partition.
The hypervisor memory manager
The hypervisor memory manager is relatively simple compared to the memory manager for NT or the 
Secure Kernel. The entity that manages a set of physical memory pages is the hypervisor’s memory 
compartment. Before the hypervisor startup takes palace, the hypervisor loader (Hvloader.dll) allocates 
the hypervisor loader block and pre-calculates the maximum number of physical pages that will be 
used by the hypervisor for correctly starting up and creating the root partition. The number depends 
on the pages used to initialize the IOMMU to store the memory range structures, the system PFN data-
base, SLAT page tables, and HAL VA space. The hypervisor loader preallocates the calculated number 
of physical pages, marks them as reserved, and attaches the page list array in the loader block. Later, 
when the hypervisor starts, it creates the root compartment by using the page list that was allocated 
by the hypervisor loader.
Figure 9-7 shows the layout of the memory compartment data structure. The data structure keeps 
track of the total number of physical pages “deposited” in the compartment, which can be allocated 
somewhere or freed. A compartment stores its physical pages in different lists ordered by the NUMA 
node. Only the head of each list is stored in the compartment. The state of each physical page and 
its link in the NUMA list is maintained thanks to the entries in the PFN database. A compartment also 
280 
CHAPTER 9 Virtualization technologies
tracks its relationship with the root. A new compartment can be created using the physical pages that 
belongs to the parent (the root). Similarly, when the compartment is deleted, all its remaining physical 
pages are returned to the parent.
Global Zone
Parent
Compartment
Physical Pages Lists
# of Deposited Pages
# of Free Pages
Node 0
Node N
PFN Database
PFN 24
PFN 25
PFN 5A
PFN 5B
PFN CB
PFN 7A
PFN A4
PFN B6
FIGURE 9-7 The hypervisor’s memory compartment. Virtual address space for the global zone is reserved from 
the end of the compartment data structure
When the hypervisor needs some physical memory for any kind of work, it allocates from the ac-
tive compartment (depending on the partition). This means that the allocation can fail. Two possible 
scenarios can arise in case of failure:
I 
If the allocation has been requested for a service internal to the hypervisor (usually on behalf
of the root partition), the failure should not happen, and the system is crashed. (This explains
why the initial calculation of the total number of pages to be assigned to the root compartment
needs to be accurate.)
I 
If the allocation has been requested on behalf of a child partition (usually through a hypercall),
the hypervisor will fail the request with the status INSUFFICIENT_MEMORY. The root partition
detects the error and performs the allocation of some physical page (more details are discussed
later in the “Virtualization stack” section), which will be deposited in the child compartment
through the HvDepositMemory hypercall. The operation can be finally reinitiated (and usually
will succeed).
The physical pages allocated from the compartment are usually mapped in the hypervisor using a 
virtual address. When a compartment is created, a virtual address range (sized 4 or 8 GB, depending on 
whether the compartment is a root or a child) is allocated with the goal of mapping the new compart-
ment, its PDE bitmap, and its global zone. 
A hypervisor’s zone encapsulates a private VA range, which is not shared with the entire hypervisor 
address space (see the “Isolated address space” section later in this chapter). The hypervisor executes 
with a single root page table (differently from the NT kernel, which uses KVA shadowing). Two entries in 
the root page table page are reserved with the goal of dynamically switching between each zone and 
the virtual processors’ address spaces.
CHAPTER 9 Virtualization technologies
281
Partitions’ physical address space
As discussed in the previous section, when a partition is initially created, the hypervisor allocates a 
physical address space for it. A physical address space contains all the data structures needed by the 
hardware to translate the partition’s guest physical addresses (GPAs) to system physical addresses 
(SPAs). The hardware feature that enables the translation is generally referred to as second level ad-
dress translation (SLAT). The term SLAT is platform-agnostic: hardware vendors use different names: 
Intel calls it EPT for extended page tables; AMD uses the term NPT for nested page tables; and ARM 
simply calls it Stage 2 Address Translation. 
The SLAT is usually implemented in a way that’s similar to the implementation of the x64 page 
tables, which uses four levels of translation (the x64 virtual address translation has already been dis-
cussed in detail in Chapter 5 of Part 1). The OS running inside the partition uses the same virtual address 
translation as if it were running by bare-metal hardware. However, in the former case, the physical 
processor actually executes two levels of translation: one for virtual addresses and one for translating 
physical addresses. Figure 9-8 shows the SLAT set up for a guest partition. In a guest partition, a GPA is 
usually translated to a different SPA. This is not true for the root partition. 
Guide Physical
Memory
Page Tables A
Process A
Guest A
Host Physical Memory
EPT A
CR 3
1
2
3
4
1
2
3
4
560
564
568
570
560
564
568
570
FIGURE 9-8 Address translation for a guest partition.
When the hypervisor creates the root partition, it builds its initial physical address space by using 
identity mapping. In this model, each GPA corresponds to the same SPA (for example, guest frame 
0x1000 in the root partition is mapped to the bare-metal physical frame 0x1000). The hypervisor preal-
locates the memory needed for mapping the entire physical address space of the machine (which has 
been discovered by the Windows Loader using UEFI services; see Chapter 12 for details) into all the 
allowed root partition’s virtual trust levels (VTLs). (The root partition usually supports two VTLs.) The 
SLAT page tables of each VTL belonging to the partition include the same GPA and SPA entries but usu-
ally with a different protection level set. The protection level applied to each partition’s physical frame 
allows the creation of different security domains (VTL), which can be isolated one from each other. VTLs 
are explained in detail in the section “The Secure Kernel” later in this chapter. The hypervisor pages 
are marked as hardware-reserved and are not mapped in the partition’s SLAT table (actually they are 
mapped using an invalid entry pointing to a dummy PFN).
282 
CHAPTER 9 Virtualization technologies
Note For performance reasons, the hypervisor, while building the physical 
memory mapping, is able to detect large chunks of contiguous physical mem-
ory, and, in a similar way as for virtual memory, is able to map those chunks by 
using large pages. If for some reason the OS running in the partition decides to 
apply a more granular protection to the physical page, the hypervisor would 
use the reserved memory for breaking the large page in the SLAT table.
Earlier versions of the hypervisor also supported another technique for map-
ping a partition’s physical address space: shadow paging. Shadow paging was 
used for those machines without the SLAT support. This technique had a very 
high-performance overhead; as a result, it’s not supported anymore. (The ma-
chine must support SLAT; otherwise, the hypervisor would refuse to start.)
The SLAT table of the root is built at partition-creation time, but for a guest partition, the situation is 
slightly different. When a child partition is created, the hypervisor creates its initial physical address space 
but allocates only the root page table (PML4) for each partition’s VTL. Before starting the new VM, the 
VID driver (part of the virtualization stack) reserves the physical pages needed for the VM (the exact 
number depends on the VM memory size) by allocating them from the root partition. (Remember, we 
are talking about physical memory; only a driver can allocate physical pages.) The VID driver maintains 
a list of physical pages, which is analyzed and split in large pages and then is sent to the hypervisor 
through the HvMapGpaPages Rep hypercall. 
Before sending the map request, the VID driver calls into the hypervisor for creating the needed 
SLAT page tables and internal physical memory space data structures. Each SLAT page table hierarchy 
is allocated for each available VTL in the partition (this operation is called pre-commit). The operation 
can fail, such as when the new partition’s compartment could not contain enough physical pages. In 
this case, as discussed in the previous section, the VID driver allocates more memory from the root par-
tition and deposits it in the child’s partition compartment. At this stage, the VID driver can freely map 
all the child’s partition physical pages. The hypervisor builds and compiles all the needed SLAT page 
tables, assigning different protection based on the VTL level. (Large pages require one less indirection 
level.) This step concludes the child partition’s physical address space creation.
Address space isolation
Speculative execution vulnerabilities discovered in modern CPUs (also known as Meltdown, Spectre, 
and Foreshadow) allowed an attacker to read secret data located in a more privileged execution 
context by speculatively reading the stale data located in the CPU cache. This means that software 
executed in a guest VM could potentially be able to speculatively read private memory that belongs to 
the hypervisor or to the more privileged root partition. The internal details of the Spectre, Meltdown, 
and all the side-channel vulnerabilities and how they are mitigated by Windows have been covered in 
detail in Chapter 8.
CHAPTER 9 Virtualization technologies
283
The hypervisor has been able to mitigate most of these kinds of attacks by implementing the 
HyperClear mitigation. The HyperClear mitigation relies on three key components to ensure strong 
Inter-VM isolation: core scheduler, Virtual-Processor Address Space Isolation, and sensitive data scrub-
bing. In modern multicore CPUs, often different SMT threads share the same CPU cache. (Details about 
the core scheduler and symmetric multithreading are provided in the “Hyper-V schedulers” section.) In 
the virtualization environment, SMT threads on a core can independently enter and exit the hypervisor 
context based on their activity. For example, events like interrupts can cause an SMT thread to switch 
out of running the guest virtual processor context and begin executing the hypervisor context. This can 
happen independently for each SMT thread, so one SMT thread may be executing in the hypervisor 
context while its sibling SMT thread is still running a VM’s guest virtual processor context. An attacker 
running code in a less trusted guest VM’s virtual processor context on one SMT thread can then use a 
side channel vulnerability to potentially observe sensitive data from the hypervisor context running on 
the sibling SMT thread. 
The hypervisor provides strong data isolation to protect against a malicious guest VM by maintain-
ing separate virtual address ranges for each guest SMT thread (which back a virtual processor). When 
the hypervisor context is entered on a specific SMT thread, no secret data is addressable. The only data 
that can be brought into the CPU cache is associated with that current guest virtual processor or rep-
resent shared hypervisor data. As shown in Figure 9-9, when a VP running on an SMT thread enters the 
hypervisor, it is enforced (by the root scheduler) that the sibling LP is running another VP that belongs 
to the same VM. Furthermore, no shared secrets are mapped in the hypervisor. In case the hypervisor 
needs to access secret data, it assures that no other VP is scheduled in the other sibling SMT thread.
Core 0
L1 Data Cache
LP 0
HV
VM A
(VP 0)
LP 1
HV
VM A
(VP 1)
Core 1
L1 Data Cache
LP 0
HV
VM B
(VP 0)
LP 1
FIGURE 9-9 The Hyperclear mitigation.
Unlike the NT kernel, the hypervisor always runs with a single page table root, which creates a single 
global virtual address space. The hypervisor defines the concept of private address space, which has 
a misleading name. Indeed, the hypervisor reserves two global root page table entries (PML4 entries, 