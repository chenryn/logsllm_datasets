interface. Normally, peers that do not have a sufficient amount of
RouterInfos in their netDb and peers that need to look up LeaseSets
will send a DLM to floodfill routers to request more RouterInfos
and LeaseSets. Making use of this mechanism, adversaries could
modify the source code of the I2P router software to make their
I2P clients repeatedly query floodfill routers to aggressively gather
more RouterInfos.
For the purposes of our research, the above approaches are im-
practical and even unethical. Although one of the goals of this
paper is to estimate the population of I2P peers, which requires us
to also collect as many RouterInfos from the netDb as possible, we
need to conduct our study in a responsible manner. Our principle
is that experiments should not cause any unnecessary overheads
or saturate any resources of other I2P peers in the network. Liu et
al. [40] showed that crawling reseed severs only contributes 7.04%
to the total number of peers they collected, while manipulating the
netDb mechanism only contributes 30.18%.
Therefore, we choose an alternative method, and opt to conduct
our experiments in a passive way by operating several routers that
simply observe the network. The primary goal of our experiments
is to investigate how many I2P routers one needs to operate and
under what settings to effectively monitor a significant portion of the
I2P network with the least effort. In order to avoid the bandwidth
limitation of prior studies [68], all of our experiments are conducted
using dedicated private servers instead of research infrastructure
shared with other researchers.
4.1 Machine Specifications
Since there is no official guideline on how to operate a high-profile
I2P router, we employ a best-effort approach to determine what
Figure 2: Number of peers observed during our initial ex-
periment for assessing the impact of different hardware and
software configurations.
specifications are sufficient to observe a significant amount of other
I2P routers. Specifications of interest include the hardware configu-
ration of the hosting machine (e.g., CPU, RAM) and configuration
parameters of the I2P router software (e.g., shared network band-
width, maximum number of participating tunnels, size of heap
memory for the Java virtual machine). Note that the official I2P
router software is written in Java. This is a necessary step in order
to understand the I2P software behavior. For example, increasing
the number of connections allowed to a router, without tuning the
available Java heap space, can result in errors that will force a router
to restart. Similarly, if CPU is not adequate, a router might drop
connections, block, or increase latency. These are all situations un-
der which a router would be penalized by the I2P ranking algorithm
and therefore have less chances of being chosen to participate in
peers’ tunnels. Consequently, a router that is not fine-tuned will
have less visibility into the I2P network than one that can maintain
a high service quality. We empirically investigate the upper bounds
of a system’s specifications to decide the resources we will need to
dedicate to our hosts.
Intuitively, we know that a higher-profile router will observe
a larger number of RouterInfos. We first run an I2P router using
a high-end machine with a 10-core 2.40 GHz CPU and 16 GB of
RAM. The shared bandwidth of this router is then set to 8 MB/s
because the built-in bloom filter of the I2P router software is limited
to 8 MB/s. The maximum number of participating tunnels is set
to 15K, and 10 GB is allocated to the heap memory for the Java
virtual machine. After running this router for 10 days, five days in
each mode (i.e., floodfill and non-floodfill), we make the following
observations:
• Total CPU usage always stays in the range of 4–5 Ghz.
• Memory usage stays in the range of 3–4 GB most of the time.
• The highest observed bandwidth usage is 5 MB/s.
• The number of participating tunnels stays at around 4K,
while the highest observed number is approximately 5.5K
tunnels.
• All of the maximum values above are observed when oper-
ating in the non-floodfill mode.
12345678910Day10K11K12K13K14K15K16K17KObservedpeersFloodfillNon-floodfillIMC ’18, October 31-November 2, 2018, Boston, MA, USA
NP. Hoang et al.
Figure 3: Number of I2P peers observed when operating 14
nodes (7 in floodfill and 7 in non-floodfill mode) using an
increasing amount of shared bandwidth.
As shown in Figure 2, although the number of peers observed
during the non-floodfill mode is slightly higher than in the floodfill
mode, it constantly remains around 15–16K. Note that a peer is
defined by a unique hash value encapsulated in its RouterInfo.
Based on these observations, we set up the (virtual) machines used
for our subsequent experiments with the following upper-bound
specifications:
• Three 2.4 GHz CPU cores totalling 7.2 GHz.
• Five GB of RAM, four of which are allocated to the heap
memory of the Java virtual machine and one for the rest of
the system.
• The maximum number of participating tunnels is set to 10K.
• The maximum shared bandwidth is set to 8 MB/s, according
to the maximum limit of the built-in bloom filter of the I2P
router software.
4.2 Floodfill vs Non-floodfill Operation
Although Figure 2 shows that the number of peers observed in non-
floodfill mode is slightly higher than in floodfill mode, it is possible
that this difference is the result of a fluctuation in the number of
daily peers during the study period. Therefore, we operated another
14 routers in both floodfill and non-floodfill mode simultaneously to
prevent any potential fluctuation in the number of daily peers from
affecting our observations. These 14 routers are divided into two
groups: non-floodfill and floodfill, with seven routers in each group.
For the routers in each group, we gradually increase the shared
bandwidth as follows: 128 KB/s, 256 KB/s, 1 MB/s, 2 MB/s, 3 MB/s,
4 MB/s, and 5 MB/s. We pick 128 KB/s as the lowest bandwidth
because it is the minimum required value for a router to be able
to gain the floodfill flag [34], while the highest value is based on
the highest bandwidth usage observed in our previous experiment
(Section 4.1). We run these routers on machines with hardware
specifications described earlier.
Figure 3 shows that floodfill routers with shared bandwidth
lower than 2 MB/s observe 1.5–2K more peers than non-floodfill
routers that have the same shared bandwidth. On the other hand,
non-floodfill routers with shared bandwidth greater than 2 MB/s
Figure 4: Cumulative number of peers observed by operating
1–40 routers.
observe about 1–1.5K more peers than floodfill routers of the same
shared bandwidth. However, it is interesting that when combining
data from each pair of routers with the same shared bandwidth, the
total number of observed peers (upper line in the graph) stays at
around 17–18K, regardless of the difference in shared bandwidth
and the number of observed peers in each mode. To explain this
behavior, we first identify the four primary ways I2P peers can
learn about other peers in the network:
• As part of the bootstrapping process, a newly joined peer
fetches RouterInfos from a set of hardcoded reseed servers
to learn a small portion of peers in the network. Based on
logs provided by the I2P router console, a newly joined peer
fetches around 150 RouterInfos from two reseed servers
(roughly 75 RouterInfos from each server).
• A router that does not have enough RouterInfos in its local
storage sends a DLM to floodfill routers to ask for more
RouterInfos.
• An active router is selected by other peers to route traffic
for them. This way, the router learns about other adjacent
routers in tunnels that it participates in. The higher the
specifications a router has, the higher the probability that it
will be selected to participate in more tunnels.
• A floodfill router receives RouterInfos published by other
“nearby” non-floodfill routers or by other floodfill routers via
the flooding mechanism. The “nearby” distance is calculated
based on the XOR distance between the indexing key of two
routers. The flooding mechanism is used when a floodfill
router receives a DatabaseStoreMessage containing a valid
RouterInfo or LeaseSet that is newer than the one previously
stored in its local NetDb. In that case, the floodfill router
“floods” the netDb entry to three others among its closest
floodfill routers [34].
We attribute the observed behavior to the last two of the above
mechanisms, as they are the main ways in which our routers learn
about other peers in the network. Since the two groups of routers
used interact with the network in different ways, each group obtains
a particular view of the network from a different angle, which the
other group could not observe. As a result, aggregating their data
together gives us a better view of the overall network. In summary,
1282561K2K3K4K5KSharedbandwidth(KB/s)10K11K12K13K14K15K16K17K18K19KObservedpeersbothﬂoodﬁllnon-ﬂoodﬁll1510152025303540Routersunderourcontrol0K3K6K9K12K15K18K21K24K27K30K33KObservedpeersMeasuring the I2P Anonymity Network and its Censorship Resistance
IMC ’18, October 31-November 2, 2018, Boston, MA, USA
from this experiment we learn that it is important to operate routers
in both non-floodfill and floodfill modes. By combining different
viewpoints, we can gain a more complete view of the network.
4.3 Number of Routers
Next, we investigate how many routers we need to run to observe
a significant part of the network. Prior to this work, Liu et al. [40]
used various methods to harvest the netDb: crawling the reseed
servers repeatedly, sending DLM continuously to other floodfill
routers, and running both floodfill and non-floodfill routers. The
authors claim the discovery of 94.9% of all routers in the network
by comparing their collected data with the stats.i2p statistic
website [75]. However, as we have confirmed with the I2P team,
the provided statistics cannot be considered as ground truth. This
is because the statistics are collected only from an average non-
floodfill router (i.e., not high bandwidth). Furthermore, reported
results are plotted using data collected over the last thirty days,
but not on a daily basis. More recently, Gao et al. [19] operated
40 floodfill routers to collect LeaseSets and claimed the discovery
of more than 80% of all “hidden” eepsites. However, it is not clear
which hardware and software combination was used for operating
those routers. More importantly, as we are interested in gathering
RouterInfos but not LeaseSets, operating all routers in a single
mode (i.e., floodfill or non-floodfill) is not ideal (see our discussion
in Section 4.2).
Therefore, we choose to run a total of 40 routers equally divided
between both modes (floodfill and non-floodfill). Each router is
hosted on a machine with the specifications defined in Section 4.1.
As RouterInfos are written to disk by design so that they are avail-
able after a restart [34], we keep track of the netDb directory where
these records are stored. Note that although there is an expiration
field in the data structure of RouterInfo, it is not currently used [28].
That means the actual active time of a peer is unknown. In other
words, the existence of a given RouterInfo only indicates the pres-
ence of the corresponding peer in the network, but it does not
provide an indication about until when a peer was active.
Since floodfill routers apply a one-hour expiration time for all
RouterInfos stored locally, we choose to monitor the netDb direc-
tory on an hourly basis to capture any new RouterInfo. Every 24
hours we clean up the netDb directory to make sure that we do not
count inactive peers on the next day. After running these routers
for five days, we calculate the cumulative number of peers observed
daily across 40 routers.
Figure 4 shows that operating 40 routers can help us observe
about 32K peers in the network. The number of observed peers has
a logarithmic relation to the number of routers under our control.
The figure also shows that the number of observed peers increases
rapidly when increasing the number of routers from one to 20,
and then increases slowly and converges to about 32K. In fact, the
aggregated number of observed peers from operating 20 routers
already gives us 95.5% (i.e., more than 30.5K peers) of the total
number of observed peers. Beyond 35 routers, each added router
only contributes the observation of an extra 10–30 peers. Therefore,
we conclude that 20 routers are sufficient for obtaining a good view
of the I2P network.
Figure 5: Number of unique peers and IP addresses.
5 NETWORK MEASUREMENT
Taking the observations made in Section 4 into consideration, we
conducted our measurements by operating 20 routers using the
machine specifications defined in Section 4.1. These routers consist
of 10 floodfill and 10 non-floodfill routers. We collected RouterIn-
fos observed by these routers for a period of three months (from
February to April, 2018).
5.1 Population of I2P Peers
Figure 5 shows the number of unique I2P peers and IP addresses,
including both IPv4 and IPv6, observed during the three-month
period. The number of daily peers remains stable at around 30.5K.
Note that an I2P peer is identified by a cryptographic identifier,
which is a unique hash value encapsulated in its RouterInfo. This
identifier is generated the first time the I2P router software is in-
stalled, and never changes throughout its lifetime.
For the number of unique IP addresses, we count all unique
IPv4 and IPv6 addresses (if supported by an I2P router) on a daily
basis. Given that some peers frequently change their IP address, as
we discuss in Section 5.2.2, one would expect the total number
of unique IP addresses to be higher than the number of peers.
However, as shown in Figure 5, the total number of IP addresses
is noticeably lower than the number of peers. By analyzing the
collected RouterInfos, we identified a large number of I2P peers
whose RouterInfos do not have a valid IP address field. In other
words, the public IP addresses of these peers are unknown. We then
analyzed other fields in the RouterInfo of these peers and discovered
that there are two subgroups of peers within the group of unknown-
IP peers. These are firewalled and hidden peers. Firewalled peers
are operated behind NAT or strict firewall configurations. Hidden
peers only use other peers to route their traffic but do not help other
peers to route traffic since they do not publish their IP address in
the network database. By default, peers located in countries with
poor Press Freedom scores (i.e., greater than 50) [48, 73] are set to
hidden. However, this setting can be modified to expose the peer to
the rest of the network to benefit a better integration, thus better
02/01/1802/15/1803/01/1803/15/1803/29/1804/12/1804/26/18Date0K4K8K12K16K20K24K28K32KObservedpeers/IPsroutersallIPIPv4IPv6IMC ’18, October 31-November 2, 2018, Boston, MA, USA
NP. Hoang et al.
Figure 6: Number of peers with unknown IP addresses.
Figure 7: Percentage of peers that we see in the network con-