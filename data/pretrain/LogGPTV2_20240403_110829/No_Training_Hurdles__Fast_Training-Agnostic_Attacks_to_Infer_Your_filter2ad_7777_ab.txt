(e.g., 802.11a/g/n/ac/ad). OFDM utilizes multiple subcarrier frequen-
cies to encode a packet, and the channel frequency responses mea-
sured from the subcarriers form the CSI of OFDM. The channel
frequency response at time t is denoted by H ( f ,t ), where f repre-
sents a particular subcarrier frequency, and it is usually estimated by
using a pseudo noise sequence that is publicly known [15]. Specifi-
cally, a transmitter sends a pseudo noise sequence over the wireless
channel, and the receiver estimates the channel frequency response
from the received, distorted copy and the publicly known original
sequence. Let X ( f ,t ) denote the transmitted pseudo noise sequence.
Based on the received signal Y ( f ,t ), H ( f ,t ) can be calculated by
. Existing work utilizes the amplitude of CSI to
H ( f ,t ) = Y (f ,t )
X (f ,t )
extract keystroke waveforms [6, 18]. In this paper, we also explore
the amplitude of CSI and refer to this as just “CSI” in the following.
2.2 Existing Work on CSI-based Keystroke
Inference
Researchers have proposed to utilize CSI to recognize subtle human
activities, including mouth movements [34] and keystrokes [6, 18].
Existing techniques ([6, 18]) on CSI-based keystroke inference as-
sume that the attacker typically sets up a wireless transmitter and
receiver in the close proximity of the target keyboard. If the key-
board is part of a computer like a laptop that can connect to wireless
networks, the computer itself transmits the wireless signal when-
ever it needs to exchange information with the WiFi router, and
thus it can play the role of the transmitter for the attacker. The
receiver can then be a malicious 802.11 access point that provides
free WiFi service to attract victim computers to connect to it. In
a general case, the attacker can also create a custom transmitter
and receiver using software-defined radio platforms such as USRPs.
The transmitter transmits the wireless signal to create a radio en-
vironment, and the receiver receives the signal from the wireless
channel and computes the CSI.
These techniques normally use three steps to infer keystrokes,
namely, pre-processing, training, and testing. Pre-processing re-
moves noise from the CSI, reduces computational complexity for
the keystroke inference, and segments the time series of the CSI
into individual samples that correspond to keystrokes. The training
phase records each keystroke and the corresponding CSI so that a
training model for classification can be built. In the testing phase,
an observed CSI for an unknown keystroke is matched within the
training model to determine which keystroke it corresponds to.
The training-agnostic attack described in this paper uses the same
pre-processing step as these existing techniques.
3 ATTACK DESIGN
Existing work requires a training process to construct the rela-
tionship between observed CSI and keystrokes. We propose to
remove the requirement of the training phase by quantifying the
self-contained structures of words to recognize keystrokes without
training. We next detail the necessary technical components we
have developed.
3.1 System Overview
We consider a general attack scenario, where the attacker uses
a customized transmitter and receiver pair to launch this attack.
The attacker can constantly transmit the wireless signal, or just
whenever typing activity is detected. In the latter case, a WiFi
packet analyzer can detect when a user starts to type [18]. We also
assume that the typed content is in English, though the attack can
target other languages just as easily.
The receiver needs to collect the CSI, so the attacker implements
a channel estimation algorithm such as the one mentioned in Sec-
tion 2.1 on a software-defined radio platform. The input of the
algorithm is the wireless signal received over the wireless chan-
nel, and the output is the CSI. The channel estimation algorithm
computes the CSI based on the received signal, which is a contin-
uous wave. Thus, the CSI returned by the algorithm forms a time
series, and this stream is divided by the pre-processing step into
individual segments that correspond to the actions of pressing a
key. In this paper, we refer to a segment as a CSI sample. After
pre-processing, unlike the existing methods, the training-agnostic
attack described in this paper takes three different important steps
to infer keystrokes, namely CSI word group generation, dictionary
demodulation, and alphabet matching.
CSI word group generation partitions the CSI samples into groups
corresponding to each typed word. The attacker will explore the
correlation among and order of unique letters in each word to infer
keystrokes, and thus needs to separate the stream into words. This
step performs this task by identifying the CSI samples caused by
pressing the space key, since words are almost always separated by
a space. Dictionary demodulation aligns the correlation of CSI sam-
ples to that of letters in a word, so as to find the corresponding word
for a CSI word group. Based on the demodulation result, potential
mappings are formed between CSI samples and keystrokes, with
Session 9B: Mobile 2 CCS’18, October 15-19, 2018, Toronto, ON, Canada1749Figure 1: The CSI word group for the word “from”.
which the attacker can infer the remaining typed words, including
those not appearing in the dictionary.
3.2 CSI word group generation
CSI word group generation involves classification, sorting, and
word segmentation.
3.2.1 Classification. Dynamic Time Warping is a classical tech-
nique to measure the similarity between two temporal sequences [29],
and it has been widely used to identify the spatial similarity be-
tween the signal profiles of two wireless links [6, 17, 18, 36]. Thus,
to quantify the similarity between two CSI samples, we utilize
the Dynamic Time Warping technique to calculate the distance
between them. A small distance indicates that both CSI samples
are similar and accordingly that they originate from the same key.
Conversely, a large distance indicates that they deviate from each
other, and that they are caused by two different keys. We assume
that the victim user presses a single key at a time, since this is the
common typing behavior for most keyboard users.
Sorting. Since the space character is almost always used to
3.2.2
connect consecutive words, it normally appears more frequently
than any other characters in a long text. We thus expect that the
CSI sample caused by the space key also appears more frequently
than other CSI samples. The classification outcome includes mul-
tiple sets, each consisting of similar CSI samples. We sort the sets
according to size and associate the space key with the largest set, so
that all observed CSI samples in this set are assumed to be caused
by pressing the spacebar. If this association is incorrect, we will
ultimately not be able to recover meaningful English words. In that
case, we continue on, associating the space key to the second largest
set and reattempting the same recovery process. We try these sets
from largest to smallest cardinality until we successfully recover
meaningful English words or exhaust all sets.
3.2.3 Word Segmentation. Once the set of CSI samples associated
with the space key is identified, we can start the word segmentation
process to find the CSI samples comprising each word of the typed
content. Everything between two successive CSI samples from the
space-associated set are grouped together. In the following, we refer
to such a group as a CSI word group, and this does not include the
spaces at either end. CSI word groups will be used as the input
of the dictionary demodulation method to eventually establish
the complete mapping between the CSI samples and keystrokes.
Figure 1 is an example of the CSI word group for the word “from”
which consists of samples that are caused by typing letters ‘f’, ‘r’,
‘o’, and ‘m’.
3.3 Dictionary Demodulation
Dictionary demodulation converts CSI word groups to correspond-
ing English words. We begin by developing a feature to apply to
these CSI word groups suitable for narrowing down the search
space of possible candidates. Then we show how to apply this
feature to words and sentences and handle errors.
Feature Selection. Ideally, a feature extracted from each CSI
3.3.1
word group would enable us to uniquely determine the correspond-
ing word. If the dictionary has n words, a perfect feature would
classify the n words into n groups, each having one member only,
such that an input CSI word group can uniquely match to a word
based on this feature. Our strategy is thus to find a feature that can
divide all words in the dictionary into as many sets as possible, to
achieve high distinguishability.
Due to the lack of training, we have to identify a feature from
only the self-contained relationships among the letters of a word
(the CSI samples of a CSI word group). Without knowing the exact
letters in a word, but having a CSI sample for each letter, we can
determine the number of constituent letters and whether or not any
letters in the word are repeated. These two pieces of information
yield two features to partition words, and we utilize a top 1,500
most frequently used word list [13] as the dictionary to calculate
the number of sets divided by each. To quantify the distinguisha-
bility of a feature, we define a new metric, called the uniqueness
rate, as the ratio Tp /T , where T is the number of considered words,
and Tp represents the number of sets obtained by dividing T words
according the selected feature. The uniqueness rate should be max-
imized for the best partitioning of the words. We next evaluate the
uniqueness rates for our two features:
Length: We empirically find that all words in this dictionary are
1-14 characters long. If we choose length as the only feature, we can
divide all words into 14 sets, the members of each set having the
same length. Only two words (i.e., ‘administration’ and ‘responsibil-
ity’) in the dictionary are of length 14; therefore a CSI word group
of length 14 has only two candidates. On average, however, each
set has 1,500/14 ≈ 107 words. This means that an input CSI word
group will have an average of 107 possible candidate words based
on the length feature. The uniqueness rate is then 14/1,500 ≈ 0.009.
CSI Sample Repetition: We also count the number of distinct let-
ters that repeat. We denote the repetition information of a word
as Sr , and we set Sr = 0 if no repetition is found. Otherwise, we
denote Sr by (t1,· · · ,tr ), where r is the number of distinct letters
that repeat, and ti (i ∈ {1,· · · ,r}) denotes how many times the cor-
responding letter repeats. For example, the repetition information
for the word “level" should be (2, 2), because 2 different letters (‘l’
and ‘e’) repeat, and both letters repeat twice respectively. Consider-
ing a word of length L, we can quantify the repetition information
using (L,Sr ). Using this repetition information, we can then di-
vide all 1,500 words into a total of 63 sets, such that members of
each set share the same value of (L,Sr ). On average, each set has
1,500/63 ≈ 24 words, so an input CSI word group will be mapped
to one of 24 words based on this feature. The uniqueness rate is
then 63/1,500 ≈ 0.042.
The repetition feature has better distinguishability than the
length feature, because its larger uniqueness rate yields a smaller
average set cardinality, and hence a reduced search space to map
CSI sampleCSI sampleCSI sampleCSI sampleThe time series of CSIFROMAmplitudeCSI word groupSession 9B: Mobile 2 CCS’18, October 15-19, 2018, Toronto, ON, Canada1750Empirically, we find that the uniqueness rates for words of differ-
ent lengths are not evenly distributed, and this fact actually enables
our scheme. Figure 2 presents the uniqueness rates for the inter-
element relationship matrix as well as the repetition feature for
comparison, respective to word length. The relationship matrix
clearly performs much better than the repetition feature in all cases,
but very evident also is that as words become larger, they become
more uniquely structured, leading to high uniqueness rates for the
relationship matrix feature. For example, the uniqueness rate for a
3 letter word is 0.025, while that for a word of 10 letters is 0.940.
Indeed, a phrase comprised of multiple words can be considered
as one “long word” for the purpose of generating an inter-element
relationship matrix, though the dictionary must also expand to
contain these combinations. Assuming a phrase formed by N words,
the new dictionary will include T1T2 · · ·TN phrases, where Ti (1 ≤
i ≤ N ) is the size of the set of candidate words having length equal
to the i-th CSI word group. Figure 3 illustrates how the uniqueness
rate benefits from the combination of each pair of two words from
the dictionary of 1,500 most used words. The words in each pair
range from 2 to 13 characters in length, for a possible total of 4-26
characters. The uniqueness rate jumps as the length of these word
pairs increases, and after 18 total characters, the pair of words has
a fully unique structure. This indicates that within a few words it
should always be possible to narrow down to the specific content
the victim is typing, giving rise to our joint demodulation method.
Joint Demodulation Example. Before describing the general
3.3.3
joint demodulation technique, we first show a simple clarifying
example to illustrate how to demodulate the CSI word groups. As-
sume that a simple dictionary W = {‘among’, ‘apple’, ‘are’, ‘hat’,
‘honey’, ‘hope’, ‘old’, ‘offer’, ‘pen’}. Further assume that the user
types in two words “apple” and “pen”. We denote the CSI word
groups corresponding to these typed words by c1||c2||c3||c4||c5 and
c6||c7||c8, respectively, where ci is the i-th observed CSI sample
after identification and removal of spaces.
Due to the previously discussed consistency between CSI sam-
ples for the same character, samples c2 and c3 within the first CSI
word group are similar. The inter-element relationship matrix R1 is
correspondingly
c1
1
0
0
0
0
c1
c2
c3
c4
c5
c2
0
1
1
0
0
c3
0
1
1
0
0
c4
0
0