### Incremental Online Learning [63] for Model Updates

### VIII. RELATED WORK

#### Bot Detection
Bot detection is a well-studied area, and key related works have been summarized in Section II. Most existing studies focus on application-specific bots (e.g., social network bots, game bots) [64], [21], [20], [29], [19], [65], [66], [67]. In contrast, our approach prioritizes model generalizability by avoiding any application or account-specific features. Our main novelty lies in exploring the use of data synthesis for bot detection with limited data. We also demonstrate that data synthesis helps to slow down model decay over time. One recent work [12] studied "concept drift" to determine when to retrain a classifier (for malware detection). Our work complements this by exploring effective ways to retrain the classifier with limited data.

#### Anomaly Detection
Anomaly detection aims to identify anomalous data samples compared to known data distributions [68], [69], [70], [71], [72]. Researchers have applied anomaly detection methods to detect bots and other fraudulent activities [73], [60]. These works share a similar assumption with ODDS: normal/benign data should be relatively representative and stable. In our work, we use anomaly detection methods as baselines and show the benefits of synthesizing new data based on both normal and limited abnormal samples.

#### Data Augmentation using GANs
To generate more training data, various transformations can be applied to existing datasets. In the domains of computer vision and natural language processing, researchers have proposed various data augmentation methods, including Generative Adversarial Networks (GANs), to improve performance in one-shot learning [74], image segmentation [75], image rendering [76], and emotion classification [77]. The most related work to ours is OCAN [60], which uses GANs to synthesize malicious samples for fraud detection. We compare our system with OCAN in our evaluation and demonstrate the benefits of using two generators to handle outliers and clustered data differently.

Recent works have explored introducing multiple generators to GANs [78], [79], [80], [81]. Their goal is to make synthesized data (e.g., images) closer to the target distribution. Conversely, we aim to synthesize data for unknown bots, not just to generate data that resembles known bots. This requires entirely different designs, such as using different generators for outliers and clustered data.

### IX. CONCLUSION

In this paper, we propose a stream-based bot detection model augmented with a novel data synthesis method called ODDS. We evaluate our system on three different real-world online services. Our results show that ODDS enables training a good model with only 1% of the labeled data and helps the model sustain over a long period with low-cost retraining. We also explore the relationship between data synthesis and adversarial retraining, demonstrating the different benefits from both approaches.

### ACKNOWLEDGEMENT

We thank our shepherd Suman Jana and anonymous reviewers for their constructive feedback. We also thank Harisankar Haridas for discussions on bot behavior. This work was supported by NSF grants CNS-1750101 and CNS-1717028.

### REFERENCES

[1] V. Dave, S. Guha, and Y. Zhang, “ViceROI: Catching click-spam in search ad networks,” in Proc. of CCS, 2013.
...
[81] Q. Hoang, T. D. Nguyen, T. Le, and D. Phung, “MGAN: Training generative adversarial nets with multiple generators,” in Proc. of ICLR, 2018.
[82] L. M. Manevitz and M. Yousef, “One-class SVMs for document classification,” Journal of Machine Learning Research, 2002.

### APPENDIX A: LSTM VS. CNN

To justify our choice of the Long-Short-Term-Memory (LSTM) model [44], we present comparison results with Convolutional Neural Network (CNN) using the same feature encoding methods on the same dataset. The CNN architecture consists of two convolutional layers (with 64 filters and 32 filters), followed by one fully connected layer with a sigmoid activation function. We experiment with 1% and 100% of the data from Website B in August 2018 for training. As shown in Table XIV, the performance of CNN is not as high as LSTM under 1% training data, but it is comparable under 100% of the training data. Our main contribution is the feature encoding method rather than the choice of deep neural networks. Our results indicate that LSTM has a small advantage over CNN.

| % of Data | Precision (LSTM) | Precision (CNN) | Recall (LSTM) | Recall (CNN) | F1 (LSTM) | F1 (CNN) |
|-----------|------------------|-----------------|---------------|--------------|-----------|----------|
| 1%        | 0.60             | 0.62            | 0.36          | 0.29         | 0.45      | 0.37     |
| 100%      | 0.89             | 0.85            | 0.88          | 0.93         | 0.88      | 0.89     |