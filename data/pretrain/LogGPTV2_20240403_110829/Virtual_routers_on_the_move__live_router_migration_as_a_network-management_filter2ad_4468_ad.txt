the performance of the basic functions of the migration pro-
cess individually, and then place a VROOM router in a net-
work and evaluate the eﬀect its migration has on the data
and control planes. Speciﬁcally, we answer the following two
questions:
1. What is the impact of virtual router migration on data
forwarding? Our evaluation shows that it is important to
have bandwidth isolation between migration traﬃc and data
traﬃc. With separate bandwidth, migration based on an
HD router has no performance impact on data forwarding.
Migration based on a SD router introduces minimal delay
increase and no packet loss to data traﬃc.
2. What is the impact of virtual router migration on rout-
ing protocols? Our evaluation shows that a virtual router
running only OSPF in an Abilene-topology network can sup-
port 1-second OSPF hello-interval without losing protocol
adjacencies during migration. The same router loaded with
an additional full Internet BGP routing table can support
a minimal OSPF hello-interval of 2 seconds without losing
OSPF or BGP adjacencies.
6.1 Methodology
Our evaluation involved experiments conducted in the Em-
ulab tesbed [15]. We primarily used PC3000 machines as the
physical nodes in our experiments. The PC3000 is an Intel
Xeon 3.0 GHz 64-bit platform with 2GB RAM and ﬁve Gi-
gabit Ethernet NICs. For the HD prototype, each physical
node was additionally equipped with a NetFPGA card. All
nodes in our experiments were running an OpenVZ patched
Linux kernel 2.6.18-ovz028stab049.1. For a few experiments
we also used the lower performance PC850 physical nodes,
built on an Intel Pentium III 850MHz platform with 512MB
RAM and ﬁve 100Mbps Ethernet NICs.
We used three diﬀerent testbed topologies in our experi-
ments:
The diamond testbed: We use the 4-node diamond-topology
testbed (Figure 6) to evaluate the performance of individual
migration functions and the impact of migration on the data
plane. The testbed has two diﬀerent conﬁgurations, which
238n0 
TG
n1 
VR1
n3 
n2 
TR
n0 
TG
n1 
VR1
n3 
n2 
TR
n0 
TG
n1 
VR1
n3 
n2 
TR
n0 
TG
n1 
VR1
n3 
n2 
TR
(1) Before the migration of VR1
(2) VR1's control plane migrates to n3 
(3) Link n0     n1 is switched to n0     n3
(4) Link n2     n1 is switched to n2     n3
Figure 6: The diamond testbed and the experiment process
VR1
Seattle
VR5
Chicago-1
Chicago-2
VR4
VR6
New York
VR11
Sunnyvale
VR2
Denver
VR3
Kansas City
Indianapolis
VR7
Washington D.C.
VR10
Los Angeles
VR9
Houston
VR8
Atlanta
Figure 7: The Abilene testbed
Table 1: The memory dump ﬁle size of virtual router
with diﬀerent numbers of OSPF routes
Routes
Size (MB)
0
3.2
10k
24.2
100k
46.4
200k
58.4
300k
71.1
400k
97.3
500k
124.1
have the same type of machines as physical node n0 and
n2, but diﬀer in the hardware on node n1 and n3. In the
SD conﬁguration, n1 and n3 are regular PCs on which we
install our SD prototype routers. In the HD conﬁguration,
n1 and n3 are PCs each with a NetFPGA card, on which
we install our HD prototype routers.
In the experiments,
virtual router VR1 is migrated from n1 to n3 through link
n1→n3.
The dumbbell testbed: We use a 6-node dumbbell-shaped
testbed to study the bandwidth contention between migra-
tion traﬃc and data traﬃc. In the testbed, round-trip UDP
data traﬃc is sent between a pair of nodes while a virtual
router is being migrated between another pair of nodes. The
migration traﬃc and data traﬃc are forced to share the same
physical link.
The Abilene testbed: We use a 12-node testbed (Fig-
ure 7) to evaluate the impact of migration on the control
plane.
It has a topology similar to the 11-node Abilene
network backbone [1]. The only diﬀerence is that we add
an additional physical node (Chicago-2), to which the vir-
tual router on Chicago-1 (V5) is migrated. Figure 7 shows
the initial topology of the virtual network, where 11 virtual
routers (V1 to V11) run on the 11 physical nodes (except
Chicago-2) respectively.
6.2 Performance of Migration Steps
In this subsection, we evaluate the performance of the two
main migration functions of the prototypes—memory copy
and FIB repopulation.
Memory copy: To evaluate memory copy time relative to
the memory usage of the virtual router, we load the ospfd
in VR1 with diﬀerent numbers of routes. Table 1 lists the
)
s
d
n
o
c
e
s
(
e
m
T
i
6
5
4
3
2
1
0
0
10k
100k
200k
300k
400k
500k
Number of routes
Suspend + dump
Copy dump file Undump + resume
Bridging setup
Figure 8: Virtual router memory-copy time with
diﬀerent numbers of routes
respective memory dump ﬁle sizes of VR1. Figure 8 shows
the total time it takes to complete the memory-copy step,
including (1) suspend/dump VR1 on n1, (2) copy the dump
ﬁle from n1 to n3, (3) resume VR1 on n3, and (4) set up
the bridging (interface binding) for VR1 on n3. We observe
that as the number of routes becomes larger, the time it
takes to copy the dump ﬁle becomes the dominating factor
of the total memory copy time. We also note that when the
memory usage becomes large, the bridging setup time also
grows signiﬁcantly. This is likely due to CPU contention
with the virtual router restoration process, which happens
at the same time.
FIB repopulation: We now measure the time it takes VR1
to repopulate the new FIB on n3 after its migration. In this
experiment, we conﬁgure the virtual router with diﬀerent
numbers of static routes and measure the time it takes to
install all the routes into the FIB in the software or hardware
data plane. Table 2 compares the FIB update time and
total time for FIB repopulation. FIB update time is the
time virtd takes to install route entries into the FIB, while
total time also includes the time for shadowd to send the
routes to virtd. Our results show that installing a FIB entry
into the NetFPGA hardware (7.4 microseconds) is over 250
times faster than installing a FIB entry into the Linux kernel
routing table (1.94 milliseconds). As can be expected the
update time increases linearly with the number of routes.
6.3 Data Plane Impact
In this subsection, we evaluate the inﬂuence router mi-
gration has on data traﬃc. We run our tests in both the
HD and SD cases and compare the results. We also study
the importance of having bandwidth isolation between the
migration and data traﬃc.
239Table 2: The FIB repopulating time of the SD and HD prototypes
Data plane type
Number of routes
FIB update time (sec)
Total time (sec)
Software data plane (SD)
100
0.1946
0.2110
1k
10k
15k
1.9318
2.0880
19.3996
20.9851
31.2113
33.8988
Hardware data plane (HD)
100
0.0008
0.0102
1k
0.0074
0.0973
10k
0.0738
0.9634
15k
0.1106
1.4399
6.3.1 Zero impact: HD router with separate migra-
tion bandwidth
We ﬁrst evaluate the data plane performance impact of
migrating a virtual router from our HD prototype router.
We conﬁgure the HD testbed such that the migration traﬃc
from n1 to n3 goes through the direct link n1→n3, eliminat-
ing any potential bandwidth contention between the migra-
tion traﬃc and data traﬃc.
We run the D-ITG traﬃc generator [14] on n0 and n2 to
generate round-trip UDP traﬃc. Our evaluation shows that,
even with the maximum packet rate the D-ITG traﬃc gen-
erator on n0 can handle (sending and receiving 64-byte UDP
packets at 91k packets/s), migrating the virtual router VR1
from n1 to n3 (including the control plane migration and
link migration) does not have any performance impact on
the data traﬃc it is forwarding—there is no delay increase or
packet loss4. These results are not surprising, as the packet
forwarding is handled by the NetFPGA, whereas the migra-
tion is handled by the CPU. This experiment demonstrates
that hardware routers with separate migration bandwidth
can migrate virtual routers with zero impact on data traﬃc.
6.3.2 Minimal impact: SD router with separate mi-
gration bandwidth
In the SD router case, CPU is the resource that could po-
tentially become scarce during migration, because the con-
trol plane and data plane of a virtual router share the same
CPU. We now study the case in which migration and packet
forwarding together saturate the CPU of the physical node.
As with the HD experiments above, we use link n1→n3 for
the migration traﬃc to eliminate any bandwidth contention.
In order to create a CPU bottleneck on n1, we use PC3000
machines on n0 and n2 and use lower performance PC850
machines on n1 and n3. We migrate VR1 from n1 to n3
while sending round-trip UDP data traﬃc between nodes
n0 and n2. We vary the packet rate of the data traﬃc from
1k to 30k packets/s and observe the performance impact the
data traﬃc experiences due to the migration. (30k packets/s
is the maximum bi-directional packet rate a PC850 machine
can handle without dropping packets.)
Somewhat surprisingly, the delay increase caused by the
migration is only noticeable when the packet rate is rela-
tively low. When the UDP packet rate is at 5k packets/s,
the control plane migration causes sporadic round-trip de-
lay increases up to 3.7%. However, when the packet rate is
higher (e.g., 25k packets/s), the change in delay during the
migration is negligible (< 0.4%).
This is because the packet forwarding is handled by ker-
nel threads, whereas the OpenVZ migration is handled by
user-level processes (e.g., ssh, rsync, etc.). Although ker-
nel threads have higher priority than user-level processes in
scheduling, Linux has a mechanism that prevents user-level
processes from starving when the packet rate is high. This
4We hard-wire the MAC addresses of adjacent interfaces
on each physical nodes to eliminate the need for ARP re-
quest/response during link migration.
Table 3: Packet loss rate of the data traﬃc, with
and without migration traﬃc
Data traﬃc rate (Mbps)
Baseline (%)
w/ migration traﬃc (%)
500
0
0
600
0
0
700
0
0.04
800
0
0.14
900
0.09
0.29
explains the delay increase when migration is in progress.
However, the higher the packet rate is, the more frequently
the user-level migration processes are interrupted, and more
frequently the packet handler is called. Therefore, the higher