PREFIX attack:  PREFIX dcterm: 
SELECT ?logEntry 
	WHERE {SELECT ?logEntry 
	WHERE {
?timestamp ?host ?keywords ?techn ?desc ?tactic ?capec
?logEntry apache:hasRequest ?req ; 
cl:originatesFrom ?host; 
cl:timestamp ?timestamp.
FILTER regex(str(?req),?keywords) 
	{ SELECT ?keywords ?techn ?tactic { 
	?sigma sigma:keywords ?keywords.
OPTIONAL { 
?sigma rule:hasAttackTechnique ?techn. 
?techn dcterm:description ?desc.
?techn attack:accomplishesTactic ?tactic.?techn attack:hasCAPEC ?capec.
	} 
	}} 
} LIMIT 4
Listing 3. Rule-based threat detection and ATT&CK linking query.
Figure 10. Threat detection and ATT&CK linking visualization (excerpt).
Table 3. Scenario 4 Query Results (Excerpt).
| logEntry | Timestamp | Host Keywords | Host Keywords | Techn | Desc | Tactic | Capec |
|---|---|---|---|---|---|---|---|
| 5f4a32... |Mar 04 19:18:43 |cup |“whoami”  |“whoami”  |"Web Shell" |  |  || 468226... Mar 04 14:05:41 insect “whoami”  |468226... Mar 04 14:05:41 insect “whoami”  |468226... Mar 04 14:05:41 insect “whoami”  |468226... Mar 04 14:05:41 insect “whoami”  |468226... Mar 04 14:05:41 insect “whoami”  |"Web Shell" |  |  |
| 7cff1d1... Mar 04 19:18:46 |7cff1d1... Mar 04 19:18:46 |cup |“curl” | |"Exploit Pub.."  |"Exploit Pub.."  |- || 600a59... Mar 04 19:18:43 insect |600a59... Mar 04 19:18:43 insect |600a59... Mar 04 19:18:43 insect |“wget” | |"Exploit Pub.."  |"Exploit Pub.."  |- |
7. Evaluation
	We evaluated the scalability of our approach by means of a set of experiments in non-federated and federated settings.
Mach. Learn. Knowl. Extr. 2022, 4 388
7.1. Evaluation Setup7.1. Evaluation Setup
The experiments were carried out on Microsoft Azure virtual machines with seven hosts (4 Windows and 3 Linux) with 2.59 GHz vCPU and 16 GB RAM each. We reused the log vocabularies from [17] and mapped them to the log data.
Dataset OverviewDataset Overview
We selected the systematically generated AIT log dataset (V1.1) that simulates six days of user access across multiple web servers including two attacks on the fifth day [6]. As summarized in Table 4, the dataset contains several log sources from four servers (cup, insect, onion, spiral).To reduce reading overhead and improve log processing performance, we split large log files from the data set into smaller files—this can easily be replicated in a running system using log rotation mechanisms. Specifically, we split the files into chunks of 10k–100k log lines each and annotated them with original filename and time-range information
Table 4. Dataset description.
LogType #PropertiesLogType #Properties
| mail.cup.com | mail.cup.com | mail.insect.com | mail.insect.com | mail.onion.com | mail.onion.com | mail.spiral.com | mail.spiral.com |
|---|---|---|---|---|---|---|---|
| Size |#Lines |Size |#Lines |Size |#Lines |Size |#Lines |
| Audit | 36 | 25 GB | 123.6 M | 22.7 GB | 99.9 M | 14.6 GB | 68.8 M | 12.4 GB | 59.5 M |
|---|---|---|---|---|---|---|---|---|---|| Apache |12 |36.9 MB |148 K |44.4 MB |169.3 K |22.7 MB |81.9 K |24 .8 MB 100.4 K |24 .8 MB 100.4 K |
| Syslog |6 |28.5 MB |158.6 K |26.9 MB |150.7 K |15 MB |86.6 K |15.1 MB |85.5 K |
| Exim |11 |649 KB |7.3 K |567 KB |6.2 K |341 KB |3.9 K |355 KB |4 K |
| Authlog |11 |128 KB |1.2 K |115 KB |1.1 K |102 KB |1 K |127 KB |1.2 K |
7.2. Single-Host Evaluation7.2. Single-Host Evaluation
We measured the overall time for virtual log graph processing including (i) log reading (i.e., searching individual log lines), (ii) log extraction (i.e., extracting the raw log line into structured data), (iii) RDF Mapping (i.e., transforming json data into RDF), and (iv) RDF compression (i.e., compressing RDF into Header, Dictionary, Triples (HDT) format).In our scenarios, we included several log sources; for each log source, we formulated 
a  to extract 1k, 3k, 5k, and 7k log lines filtering by timestamp in the 