space to a high dimensional space using a non linear transformation 3(b). The
goal is to ﬁnd the smallest sphere which contains all the points in the high
dimensional space 3(c). This sphere is mapped back to the original input space
and forms a set of countours which are considered as the cluster boundaries 3(d).
The ﬁnal step determines the cluster of each point by checking which boundaries
contain it.
188
J. Fran¸cois et al.
Initial
(a)
points
data
to
dimensional
(b) Mapping
high
space
(c) Looking for the
hyper-sphere
(d) Hyper-sphere
projection in initial
dimensions
Fig. 3. SVC example
Consider Φ, a nonlinear transformation and {xi} the set of N points in the
original d-dimensional input space. The training phase consists of ﬁnding the
smallest hyper-sphere containing all the transformed points i.e., {Φ(xi)} which
is characterized by its radius R and its center a. Therefore we have:
(cid:4)Φ(xi) − a(cid:4)2 ≤ R2 ∀i
(5)
The original problem is casted into the Lagrangian form by introducing the
lagrangian multipliers (βi and μi) and the penalty term (C
(cid:3)
L = R2 − (cid:2)
(R2 + ξi − (cid:4)Φ(xi) − a(cid:4)2)βi − (cid:2)
ξiμi + C
i
i
i ξi):
(cid:2)
ξi
i
(6)
(7)
(8)
In fact, the ξ terms are slack variables allowing some classiﬁcation errors. Then,
the problem is turned into its Wolfe dual form and the variables a and R are
eliminated due to Lagrangian constraints.:
(cid:2)
Φ(xi)2βi − (cid:2)
W =
βiβjK(xi, xj)
where K(xi, xj) is typically deﬁned by a Gaussian Kernel:
i
i,j
K(xi, xj) = e
−q(cid:3)xi−xj(cid:3)2
where q is another parameter named Gaussian width.
Next, a labeling step has to determine the points that belong to the same
clusters by a geometric approach. In fact, two points are considered of the same
clusters if all the points on the segment between them in the original space are
in the hypersphere in the high dimensional feature space.
5.2 Global Method
Even if SVC enables the discovery of intertwined clusters, the accuracy can be
limited when a single shape comprises diﬀerent clusters. The ﬁgure 4 shows such
a case, where in ﬁgure 4(a), the SVC method is able to isolate two large clus-
ters but none the single ones which composes the largest one. These constructed
Automated Behavioral Fingerprinting
189
(a) SVC clustering
(b) Nearest
algorithm
neighbors
Fig. 4. Global Method
Fig. 5. Limitation of nearest
neighbors clustering
clusters can be furthermore split by an additional nearest neighbors technique
for each of them. Hence, this second step is necessary. Obviously, it depends also
on the data to classify and our experiments in the next sections show the ben-
eﬁts of the combination of these two methods. Furthermore, several multi-pass
clustering methods exist and are introduced in [32]. Complex clusters bound-
aries are discovered by the SVC technique. By applying the nearest neighbors
technique, the result shown in ﬁgure 4(b) can be obtained. However, applying
only the nearest neighbors technique will entail a bad classiﬁcation as illustrated
in ﬁgure 5. Therefore, we propose a global method which consists in two steps:
– a ﬁrst clustering using SVC
– a second cluster splitting using nearest neighbors technique
5.3 Evaluation Metrics
We consider several metrics in order to assess the quality of the clustering method
Consider n messages to be classiﬁed, m1 . . . mn, divided into r types and k
clusters found: c1 . . . ck with k ≤ n. At the end of the classiﬁcation, a label is
assigned to each cluster which is the predominant type of the messages within.
However, only one cluster per type, the largest one, is allowed. If c(mi) represents
the cluster containing mi then t(mi) is the real type of the message mi and t(ci)
is the type assigned to the cluster (ci).
The ﬁrst metric is the classiﬁcation rate cr and represents the ratio of messages
which are classiﬁed in the right clusters:
(cid:3)
cr =
i|t(mi)=t(c(mi)) 1
n
(9)
The second metric is the proportion of diﬀerent message types which were dis-
covered:
cf = r
k
(10)
The latter is a rather important metric because performing a good classiﬁcation
rate can be easy by discovering the main types and ignoring unusual ones. In our
190
J. Fran¸cois et al.
dataset for instance, having a classiﬁcation rate close to 100% can be obtained
without discovering small clusters like 603, 480, 486, 487 as shown in ﬁgure 2.
Some of these classes have only one recognized message. The classiﬁcation rate
can also be computed for each type y:
(cid:3)
(cid:3)
crtype(y) =
i|t(mi)=y xi
i|t(mi)=y 1 where xi = 1 if t(mi) = t(c(mi)) else 0
(11)
In practice we will consider the average value and the standard deviation by
computing this metric for all possible kinds. Therefore, the classiﬁcation accuracy
has to be discussed regarding these diﬀerent metrics. Because, several ﬁgures
relate to the accuracy, a common key will be used and will be displayed only
in ﬁgure 6(a). We analyze the composition of the diﬀerent clusters with two
metrics. The ﬁrst one is the percentage of good classiﬁed messages which are
contained in the considered cluster type. The second one is the percentage of
messages of this type which are not present in the cluster.
5.4 Nearest Neighbors Technique Results
We consider the ﬁrst metric to be the relative character distribution. The results
are presented on ﬁgure 6 where the parameter t varies. This parameter is the max-
imal distance authorized between two points. The best tradeoﬀ between the clas-
siﬁcation rate and the number of clusters found is obtained for t = 0.005 on the
ﬁgure 6(a). In this case, about 40% of messages are classiﬁed correctly and 60%
of the types are found. This shows that for 40% of types the average classiﬁca-
tion rate is 0% and the standard deviation of the classiﬁcation rate per type is
relatively high. The third bar represents the average classiﬁcation rate per type
which is close to the global classiﬁcation. The composition of the clusters is in-
teresting since they are similar (see ﬁgure 6(b)). In fact, there is no type which is
totally well classiﬁed. So each cluster contains a relatively high proportion of mis-
classiﬁed messages. t is the main parameter of the nearest neighbors algorithm and
 1
 0.8
 0.6
 0.4
 0.2
 0
 0.25
 0.2
 0.15
 0.1
 0.05
 0
0.001 0.005 0.01
0.05
0.1
1
Std deviation (per type)
Avg classif. rate (per type)
% clusters found
Classification rate
Unclassified
Classified
1
0
1
0
1
0
0
0
1
1
0
1
0
0
1
1
1
1
1
1
0
1
0
1
1
0
1
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
2
O
1
R
4
I
A
N
1
4
B
5
I
M
1
4
5
4
4
2
R
6
C
4
4
P
4
0
P
0
E
0
N
C
O
8
0
Y
0
N
E
8
0
0
8
8
0
E
0
A
8
1
U
8
0
T
0
G
1
V
K
T
0
7
E
1
F
S
3
4
3
1
6
2
F
3
N
0
5
B
7
I
I
I
I
O
S
E
C
L
O
S
T
F
N
T
E
Y
S
E
R
A
G
E
R
E
L
I
S
H
(a) Accuracy
(b) Clusters details with t = 0.005
Fig. 6. Relative character distribution results
Automated Behavioral Fingerprinting
191
 1
0.8
0.6
0.4
0.2
 0
0.01
0.05
0.1
0.3
0.7
Fig. 7. Smoothing character distribution results - Accuracy
does have a high impact on the accuracy (see ﬁgure 6(a)). When t increases, less
clusters are found because the maximum distance between two points of a cluster
is increased and so the cluster sizes are larger. Hence, when t increases, the clus-
ters are merged. The classiﬁcation rate variation is less obvious: when t increases,
it begins by increasing and followed by a decrease. The reason is correlated to the
number of clusters. With a small t, the cluster sizes are small too leading messages
of the same types to be split into multiple clusters. This entails a bad classiﬁcation
rate because only the biggest one is considered for each kind. When t increases,
the clusters are merged, especially many clusters of the same kind. Then, clusters
of diﬀerent types can be grouped into one and in this case all messages of one type
are missclassiﬁed in the new cluster which decreases the classiﬁcation rate.
The next experiment is based on the character distribution which captures
the information in the characters. To limit the eﬀect of the zero values, the
results using the smoothing distribution is presented on the ﬁgure 7. We checked
with other experiments that the smoothing technique has a low impact on the
classiﬁcation rate but allows to discover easily more kinds of message. Comparing
the relative character distribution results, they are not signiﬁcantly improved
except for the number of clusters found: about 90% with a classiﬁcation rate
of about 40% (t = 0.05). The number of found clusters is better with the same