### Trigger Distribution Analysis

#### Overview
Figures 4 and Table 3 illustrate the distribution of triggers as a function of time (stages 1 to 3) and by component. The data show no significant variation in either dimension, with large coverage and variation triggers present across all stages and components. However, there is a subtle but distinct shift in the distribution as testing progresses from stage 1 to stage 3, indicating some level of product stabilization, though not as pronounced as desired.

#### Key Observations
- **Coverage Triggers**: The fraction of coverage triggers decreases as testing progresses, which is evidence of product stabilization. Software reliability growth models, such as those discussed by [Musa 99], use defect rate curves to detect such stabilization. However, trigger distribution changes provide a clearer visualization of this process.
- **Function Test Phase**: Measuring growth during the function test phase is challenging due to its dependence on test plan execution issues like build and integration methods. Triggers, however, are not subject to these dynamics and offer a finer level of information.
- **Trigger vs. Component**: Table 3 shows the cross-tabulation of triggers by major components. Coverage and variation triggers dominate the distribution in all components, with similar proportions, indicating that the issues are systemic and applicable to all subcomponents.

#### Summary: Inferences and Recommendations
1. **Test Process Dominance by Coverage Triggers**:
   - The test process was dominated by coverage triggers, a systemic problem persistent across all components. While it is normal for coverage triggers to be prominent at the functional test level, the proportion observed was significantly higher than usual.

2. **Weakness in Code Inspection and Unit Testing**:
   - The high fraction of coverage-triggered defects suggests weaknesses in code inspection and/or unit testing. This was suspected, but the magnitude and consequences are now clearly visible through trigger analysis.

3. **High Priority Defects and Test Delays**:
   - High coverage-triggered defects are associated with high-priority defects, explaining the slow testing progress in stage 1. Although other factors like code delivery and build issues may contribute, the disproportionately large coverage (~60%) is the primary cause of delays.

4. **Product Stabilization**:
   - The change in trigger mix from stage 1 to stage 3 is compelling evidence of product stabilization. Growth curves are less applicable in a function test environment, making trigger distribution changes more informative.

5. **Sequencing and Interaction Triggers**:
   - Sequencing and interaction triggers alone do not indicate complete stabilization. The absence of workload and stress-related triggers suggests potential fallout of faults during early field usage, despite some later tests being of a system test nature.

6. **Design Conformance and Logic Triggers**:
   - The low number of logic and design conformance triggers is a positive sign, indicating that thorough design reviews and modeling have been effective. These functional tests uncovered few design, architecture, and requirement-related defects.

7. **Defect Injection Rate**:
   - The defect injection rate, approximately one defect per function point, is about twice the industry average [Jones 98]. This is not necessarily negative, given the nature of the bugs, and suggests potential cost savings in future releases if lessons learned are applied.

8. **Improvement Actions**:
   - Actions are needed to improve module-level design and code stages. Better inspection and unit testing can significantly reduce the defect volume at a lower cost. A deeper analysis using other ODC attributes can further localize checklists.

9. **Reducing Test Cycle Time**:
   - Reducing the test cycle time by a factor of two is feasible. Targeted inspections can cut the defect escape rate by more than half, allowing resources to be better used for reliability. Techniques for managing the test cycle are discussed in [Levendel 91].

### Conclusions
The study answers broad questions with significant clarity, providing nine inferences and recommendations to explain the "why" of the test experience. The test process was not solely responsible for delays; specific elements in the development process could have improved the test cycle. The focus on design and requirements yielded clear benefits, and the magnitude of the problem is not alarming. With a clearer understanding, several methods can be employed to manage the process, deliver a shorter test cycle, and achieve higher field reliability.

### Acknowledgments
We thank our clients, the referees for their critique, and Saroja Prasad and Sunita Chillarege for their meticulous editing of the paper.

### References
- [Amezquita 96] “Orthogonal Defect Classification Applied to a Multidisciplinary Design”, A. Amezquita & D.P. Siewiorek, CMU EDRC 05-100-96.
- [Bassin 98] “Evaluating Software Development Objectively”, K. Bassin, T. Kratschmer, P. Santhanam, IEEE Software, Vol 15, 1998.
- [Butcher 02] “Improving software testing via ODC: Three case studies”, M. Butcher, H. Munro, T. Kratschmer, IBM Systems Journal, Vol 41, No. 1, 2002.
- [Chillarege 92] “Orthogonal Defect Classification - A Concept for In-Process Measurements”, Ram Chillarege, Inderpal S. Bhandari, Jarir K. Chaar, Michael J. Halliday, Diane S. Moebus, Bonnie K. Ray, Man-Yuen Wong, IEEE Transactions on Software Engineering, Vol. 18, No. 11, Nov 1992.
- [Chillarege 94] “Identifying Risk using ODC Based Growth Models”, R. Chillarege, S. Biyani, Proceedings, 5th International Symposium on Software Reliability Engineering, IEEE, Monterey, California, pp 282-288, November 1994.
- [Chillarege 95] “Software Triggers as a function of time - ODC on field faults”, Ram Chillarege and Kathryn A. Bassin, DCCA-5: Fifth IFIP Working Conference on Dependable Computing for Critical Applications, Sept 1995.
- [Humphrey 89] “Managing the Software Process”, Watts S. Humphrey, Addison-Wesley 1989.
- [Iyer 90] “Introduction Experimental Computer Science”, R. K. Iyer, IEEE Transactions on Software Engineering, Vol 16, No 2., 1990.
- [Jones 98] “Estimating Software Costs”, T. Capers Jones, McGraw-Hill, 1998.
- [Levendel 91] “Reliability Analysis of Large Software Systems: Defect Data Modeling”, IEEE Transactions on Software Engineering, Vol 16, No. 2, 1990.
- [Mullen 02] “Orthogonal Defect Classification at CISCO”, R. Mullen, D. Hsiao, Proceedings ASM Conference, 2002.
- [Musa 99] “Software Reliability Engineering”, McGraw-Hill, 1999.
- [ODC Web] Orthogonal Defect Classification, www.chillarege.com/odc, www.research.ibm.com/softeng
- [Paulk 93] “Capability Maturity Model for Software, Version 1.1, Mark C. Paulk, Bill Curtis, Mary Beth Chrissis, Charles V. Weber, Software Engineering Institute, 1993.