c
r
e
P
80
60
40
20
0
Design 
Conforman
Logic/Flow
Internal 
Document
Coverage Variation
Stage 1
Stage 2
Stage 3
10.27
10.06
10.5
2.16
1.89
6.39
4.32
2.96
3.65
70.81
57.28
47.49
8.65
20.12
23.74
Trigger
Sequencin
g
0.54
0.95
3.65
Interaction Recovery
Software 
Configurati
3.24
6.27
4.57
0
0.12
0
0
0.36
0
Stage 1
Stage 2
Stage 3
Component:
No. of Defects
A
189
B
300
C
358
D
135
E
114
Design Conf.
Internal Doc.
Coverage
Variation
Sequencing
Interaction
Recovery
Soft. Config.
Lat. Comp.
  Trigger % for component
9
4
66
12
2
3
0
1
4
11
3
51
24
0
6
0
0
4
10
5
56
19
2
6
0
0
2
13
0
58
16
3
7
0
0
3
10
3
68
10
1
8
0
1
0
  Table 3.  Trigger proportions by component
Figure 4 and Table 3 show the distribu-
tion of Triggers as a function of  time (stage
1  to  3)  and  as  a  function  of  component.
What is quite evident is that there are no
significant    variation  in  either  dimension.
The large coverage and variation triggers
are present both by stage, and by compo-
nent.  However,  there  is  a  subtle  but  dis-
tinct shift in distribution as test progresses
from stage 1 to stage 3. This is evidence of
the stabilization, albeit  not as pronounced
as we would have liked to see.
This change in the trigger distribution, where the
fraction of coverage triggers is dropping is evidence
of  the  stabilization  of  the  product.    Software
reliability growth models use defect rate curves to
detect such stabilization [Musa 99]. While the rate
curves can illustrate growth, they are not as clear as
to what can be visualized in the change of the trigger
distribution.  Further,  measuring  growth  is  hard
during the function test phase. It is a more viable
method during system test, and early customer trials.
Function test is very dependent on test plan execution
issues – such as those related to build and integration
methods. Triggers, however, are not subject to those
dynamics and provide a finer level of information.
But, do note that the trigger data does not preclude
defect  rate  analysis,  which  can  be  profitably
combined as discussed in [Chillarege 94].
Trigger vs. Component
To further investigate the trigger distributions, we
look at the trigger distribution by each of the different
components. Table 3 shows cross-tabs of the triggers
versus the major components. We did not include
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:19:12 UTC from IEEE Xplore.  Restrictions apply. 
the chi-square test since the sparseness of the table
yields over 20% of the cells with counts less than 5.
But the key conclusions are quite apparent. Coverage
and Variation triggers dominate the distribution in
all  components.  Even  the  proportions  are  very
similar, indicating that the discussion in this paper
is  equally  applicable  to  all  of  the  product’s
subcomponents.    The  problems  and  issues  are
systemic in nature, which was suspected, but is now
indicated.   The inferences drawn in the following
section apply uniformly to the entire product and
are representative of the development process and
culture.
Summary:  Inferences &
Recommendations
1. The test process was completely dominated by
coverage triggers [Fig. 3].  This is a systemic
problem  persistent  in  all  components  of  the
product.  While it is not unusual for coverage
triggers to dominate test at a functional test level,
this particular proportion was much larger than
normal.
2. The large fraction of coverage-triggered defects
indicates a weakness in code inspection and/or
unit test [Tbl. 2, ODC refs].  We had suspected
weakness in the code inspection stages, but the
magnitude  and  consequence  is  made  visible
through trigger analysis.
3.  The  high  coverage  triggered  defects  are  also
associated with the high priority 1 defects.  This
explains the slowness of the testing progress in
stage 1. Although, there could be other factors
such  as  code  delivery  and  build  issues,  the
disproportionately large coverage (~ 60%) singly
explains the delays in test.
4. The change in trigger mix as we progress from
stage 1 to 3 is the most compelling evidence of
the stabilization of the product [Fig. 4].  Growth
curves could not so readily be applicable here
given the type of data and measurement that we
are  dealing  with  in  a  largely  function  test
environment.
5. It must be noted that sequencing and interaction
triggers  alone  do  not  indicate  complete
stabilization [Fig. 4, Tbl. 3].  One would like to
see more workload and stress related triggers,
which were not present.  This indicates that while
some  of  the  latter  tests  were  probably  of  a
systems test nature, there could be a fallout of
those faults during early field usage.
6. The rather few logic trigger errors and few design
conformance  triggers  are  a  very  positive  sign
[Fig. 3].  Clearly, the effort invested by the team
toward  more  thorough  design  reviews  and
modeling has paid off.  Thus, these functional
tests  have  uncovered  rather  few  design,
architecture and requirement related defects.
7. The defect injection rate which is approximately
one defect per function point is about twice the
industry average [Jones 98].  This is not a bad
sign, largely because of the nature of the bugs.
These data tell us that there can be a substantial
savings in test cost for the next release if the
learning from this retrospective can be gainfully
applied.
8. Actions are needed to improve the module level
design and code stages. Better inspection and
unit test can cut down a significant size of this
defect  volume  at  lower  cost  [Humphrey  89].
Additionally,  a  drill  down  using  other  ODC
attributes can further localize the check lists.
9. Reducing the test cycle time by a factor of two is
not unthinkable. All things considered, targeted
inspection can cut this defect escape rate by more
than half, and the resources better used to gain
reliability.  There  are  several  discussion  on
managing the test cycle reported in the literature.
For example, a good treatment on the techniques
of  modeling  and  managing  defect  removal  is
discussed in [Levendel 91]. Our focus has been
on  identifying  probable  cause  -  which,  once
identified,  opens  up  several  possibilities  for
solutions  and  remedies.  The  discussion  of
different remedy strategies and their tradeoffs
deserves a separate discussion, and can follow
this study.
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:19:12 UTC from IEEE Xplore.  Restrictions apply. 
Conclusions
The objectives of this study laid out a few broad
questions that can now be answered with significant
clarity.  The nine inferences and recommendations
examine and argue the details to explain the “why”
of the test experience. Now, we address the higher
level questions posed in the objectives section.
The test process was clearly not independently
responsible  for  the  delays  [Summary  1-4].    The
analysis  revealed  the  specific  elements  in  the
development process which could have improved the
test cycle [Summary 7, 8].  However, this does not
cast development in a bad light at all – there were
clear benefits gained from the focus on the design
and  requirement  process  [Summary  6],  and  the
magnitude of the problem is not alarming [Summary
7].  With this clearer understanding of what happened
there are several methods possible to manage this
process,  deliver  a  shorter  test  cycle  and  achieve
higher field reliability. While a detailed discussion
on the design of such methods and processes is way
beyond the scope of this short article, some have
been mentioned in passing and a few references cited
to related work [Summary 1-3, 9].
The  case  study  illustrates  the  power  of    ODC
analysis  to  gain  a  deeper  understanding  while
unearthing possibilities for solutions and guidance
for specific action. This is the fodder to drive very
significant  return-on-investment  in  software
engineering.
Acknowledgments
We thank our clients, the referees for their critique,
and  Saroja  Prasad  and    Sunita  Chillarege  for
painstakingly editing the paper.
References
[Amezquita 96] “Orthogonal Defect Classification Applied
to a Multidisciplinary Design”, A. Amezquita & D.P.
Siewiorek, CMU EDRC 05-100-96.
[Bassin  98]  “Evaluating  Software  Development  Objec-
tively”, K. Bassin, T. Kratschmer, P. Santhanam, IEEE
Software, Vol 15, 1998.
[Butcher 02], “Improving software testing via ODC: Three
case studies”, M. Butcher, H. Munro, T. Kratschmer,
IBM Systems Journal, Vol 41, No. 1, 2002.
[Chillarege  92]  “Orthogonal  Defect  Classification  - A
Concept for In-Process Measurements”,  Ram Chillar-
ege, Inderpal S. Bhandari, Jarir K. Chaar, Michael J.
Halliday, Diane S. Moebus, Bonnie K. Ray, Man-Yuen
Wong,  IEEE  Transactions  on  Software  Engineering,
Vol. 18, No. 11, Nov 1992.
[Chillarege  94]  “Identifying  Risk  using  ODC  Based
Growth Models”, R. Chillarege, S. Biyani, Proceed-
ings, 5th International Symposium on Software Reli-
ability  Engineering,  IEEE,  Monterey,  California,  pp
282-288, November 1994.
[Chillarege 95] “Software Triggers as a function of time -
ODC on field faults”, Ram Chillarege and Kathryn A.
Bassin, DCCA-5: Fifth IFIP Working Conference on
Dependable Computing for Critical Applications, Sept
1995.
[Humphrey 89] “Managing the Software Process”, Watts
S. Humphrey, Addison-Wesley 1989.
[Iyer 90] “Introduction Experimental Computer Science”,
R. K. Iyer, IEEE Transactions on Software Engineer-
ing, Vol 16, No 2., 1990.
[Jones 98] “Estimating Software Costs”, T. Capers Jones,
McGraw-Hill, 1998.
[Levendel  91]  “Reliability Analysis  of  Large  Software
Systems: Defect Data Modeling”, IEEE Transactions
on Software Engineering, Vol 16, No. 2, 1990.
[Mullen 02] “Orthogonal Defect Classificiation at CISCO”,
R. Mullen, D. Hsiao, Proceedings ASM Conference,
2002.
[Musa 99] “Software Reliability Engineering”, McGraw-
Hill, 1999.
[ODC Web] Orthogonal Defect Classification
www.chillarege.com/odc
www.research.ibm.com/softeng
[Paulk 93] “Capability Maturity Model for Software, Ver-
sion 1.1, Mark C. Paulk, Bill Curtis, Mary Beth Chrissis,
Charles V. Weber, Software Engineering Institute, 1993.
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:19:12 UTC from IEEE Xplore.  Restrictions apply.