Scaling issues: Fulls, incrementals, and the competing forces of backups and restores
A classic but flawed response to the question “Do you have a backup?” is “We have
something even better than a backup—replication!” Replication provides many bene‐
fits, including locality of data and protection from a site-specific disaster, but it can’t
protect you from many sources of data loss. Datastores that automatically sync multi‐
Google SRE Objectives in Maintaining Data Integrity and Availability | 347
ple replicas guarantee that a corrupt database row or errant delete are pushed to all of
your copies, likely before you can isolate the problem.
To address this concern, you might make nonserving copies of your data in some
other format, such as frequent database exports to a native file. This additional meas‐
ure adds protection from the types of errors replication doesn’t protect against—user
errors and application-layer bugs—but does nothing to guard against losses intro‐
duced at a lower layer. This measure also introduces a risk of bugs during data con‐
version (in both directions) and during storage of the native file, in addition to
possible mismatches in semantics between the two formats. Imagine a zero-day
attack5 at some low level of your stack, such as the filesystem or device driver. Any
copies that rely on the compromised software component, including the database
exports that were written to the same filesystem that backs your database, are
vulnerable.
Thus, we see that diversity is key: protecting against a failure at layer X requires stor‐
ing data on diverse components at that layer. Media isolation protects against media
flaws: a bug or attack in a disk device driver is unlikely to affect tape drives. If we
could, we’d make backup copies of our valuable data on clay tablets.6
The forces of data freshness and restore completion compete against comprehensive
protection. The further down the stack you push a snapshot of your data, the longer it
takes to make a copy, which means that the frequency of copies decreases. At the
database level, a transaction may take on the order of seconds to replicate. Exporting
a database snapshot to the filesystem underneath may take 40 minutes. A full backup
of the underlying filesystem may take hours.
In this scenario, you may lose up to 40 minutes of the most recent data when you
restore the latest snapshot. A restore from the filesystem backup might incur hours of
missing transactions. Additionally, restoring probably takes as long as backing up, so
actually loading the data might take hours. You’d obviously like to have the freshest
data back as quickly as possible, but depending on the type of failure, that freshest
and most immediately available copy might not be an option.
Retention
Retention—how long you keep copies of your data around—is yet another factor to
consider in your data recovery plans.
While it’s likely that you or your customers will quickly notice the sudden emptying
of an entire database, it might take days for a more gradual loss of data to attract the
5 See https://en.wikipedia.org/wiki/Zero-day_(computing).
6 Clay tablets are the oldest known examples of writing. For a broader discussion of preserving data for the long
haul, see [Con96].
348 | Chapter 26: Data Integrity: What You Read Is
right person’s attention. Restoring the lost data in the latter scenario requires snap‐
shots taken further back in time. When reaching back this far, you’ll likely want to
merge the restored data with the current state. Doing so significantly complicates the
restore process.
How Google SRE Faces the Challenges of Data Integrity
Similar to our assumption that Google’s underlying systems are prone to failure, we
assume that any of our protection mechanisms are also subject to the same forces and
can fail in the same ways and at the most inconvenient of times. Maintaining a guar‐
antee of data integrity at large scale, a challenge that is further complicated by the
high rate of change of the involved software systems, requires a number of comple‐
mentary but uncoupled practices, each chosen to offer a high degree of protection on
its own.
The 24 Combinations of Data Integrity Failure Modes
Given the many ways data can be lost (as described previously), there is no silver bul‐
let that guards against the many combinations of failure modes. Instead, you need
defense in depth. Defense in depth comprises multiple layers, with each successive
layer of defense conferring protection from progressively less common data loss sce‐
narios. Figure 26-2 illustrates an object’s journey from soft deletion to destruction,
and the data recovery strategies that should be employed along this journey to ensure
defense in depth.
The first layer is soft deletion (or “lazy deletion” in the case of developer API offer‐
ings), which has proven to be an effective defense against inadvertent data deletion
scenarios. The second line of defense is backups and their related recovery methods.
The third and final layer is regular data validation, covered in “Third Layer: Early
Detection” on page 356. Across all these layers, the presence of replication is occasion‐
ally useful for data recovery in specific scenarios (although data recovery plans
should not rely upon replication).
Figure 26-2. An object’s journey from soft deletion to destruction
How Google SRE Faces the Challenges of Data Integrity | 349
First Layer: Soft Deletion
When velocity is high and privacy matters, bugs in applications account for the vast
majority of data loss and corruption events. In fact, data deletion bugs may become
so common that the ability to undelete data for a limited time becomes the primary
line of defense against the majority of otherwise permanent, inadvertent data loss.
Any product that upholds the privacy of its users must allow the users to delete
selected subsets and/or all of their data. Such products incur a support burden due to
accidental deletion. Giving users the ability to undelete their data (for example, via a
trash folder) reduces but cannot completely eliminate this support burden, particu‐
larly if your service also supports third-party add-ons that can also delete data.
Soft deletion can dramatically reduce or even completely eliminate this support bur‐
den. Soft deletion means that deleted data is immediately marked as such, rendering
it unusable by all but the application’s administrative code paths. Administrative code
paths may include legal discovery, hijacked account recovery, enterprise administra‐
tion, user support, and problem troubleshooting and its related features. Conduct soft
deletion when a user empties his or her trash, and provide a user support tool that
enables authorized administrators to undelete any items accidentally deleted by users.
Google implements this strategy for our most popular productivity applications;
otherwise, the user support engineering burden would be untenable.
You can extend the soft deletion strategy even further by offering users the option to
recover deleted data. For example, the Gmail trash bin allows users to access mes‐
sages that were deleted fewer than 30 days ago.
Another common source of unwanted data deletion occurs as a result of account
hijacking. In account hijacking scenarios, a hijacker commonly deletes the original
user’s data before using the account for spamming and other unlawful purposes.
When you combine the commonality of accidental user deletion with the risk of data
deletion by hijackers, the case for a programmatic soft deletion and undeletion inter‐
face within and/or beneath your application becomes clear.
Soft deletion implies that once data is marked as such, it is destroyed after a reason‐
able delay. The length of the delay depends upon an organization’s policies and appli‐
cable laws, available storage resources and cost, and product pricing and market
positioning, especially in cases involving much short-lived data. Common choices of
soft deletion delays are 15, 30, 45, or 60 days. In Google’s experience, the majority of
account hijacking and data integrity issues are reported or detected within 60 days.
Therefore, the case for soft deleting data for longer than 60 days may not be strong.
Google has also found that the most devastating acute data deletion cases are caused
by application developers unfamiliar with existing code but working on deletion-
related code, especially batch processing pipelines (e.g., an offline MapReduce or
Hadoop pipeline). It’s advantageous to design your interfaces to hinder developers
350 | Chapter 26: Data Integrity: What You Read Is
unfamiliar with your code from circumventing soft deletion features with new code.
One effective way of achieving this is to implement cloud computing offerings
that include built-in soft deletion and undeletion APIs, making sure to enable said
feature.7 Even the best armor is useless if you don’t put it on.
Soft deletion strategies cover data deletion features in consumer products like Gmail
or Google Drive, but what if you support a cloud computing offering instead?
Assuming your cloud computing offering already supports a programmatic soft dele‐
tion and undeletion feature with reasonable defaults, the remaining accidental data
deletion scenarios will originate in mistakes made by your own internal developers or
your developer customers.
In such cases, it can be useful to introduce an additional layer of soft deletion, which
we will refer to as “lazy deletion.” You can think of lazy deletion as behind the scenes
purging, controlled by the storage system (whereas soft deletion is controlled by and
expressed to the client application or service). In a lazy deletion scenario, data that is
deleted by a cloud application becomes immediately inaccessible to the application,
but is preserved by the cloud service provider for up to a few weeks before destruc‐
tion. Lazy deletion isn’t advisable in all defense in depth strategies: a long lazy dele‐
tion period is costly in systems with much short-lived data, and impractical in
systems that must guarantee destruction of deleted data within a reasonable time
frame (i.e., those that offer privacy guarantees).
To sum up the first layer of defense in depth:
• A trash folder that allows users to undelete data is the primary defense against
user error.
• Soft deletion is the primary defense against developer error and the secondary
defense against user error.
• In developer offerings, lazy deletion is the primary defense against internal devel‐
oper error and the secondary defense against external developer error.
What about revision history? Some products provide the ability to revert items to pre‐
vious states. When such a feature is available to users, it is a form of trash. When
7 Upon reading this advice, one might ask: since you have to offer an API on top of the datastore to implement
soft deletion, why stop at soft deletion, when you could offer many other features that protect against acciden‐
tal data deletion by users? To take a specific example from Google’s experience, consider Blobstore: rather
than allow customers to delete Blob data and metadata directly, the Blob APIs implement many safety fea‐
tures, including default backup policies (offline replicas), end-to-end checksums, and default tombstone life‐
times (soft deletion). It turns out that on multiple occasions, soft deletion saved Blobstore’s clients from data
loss that could have been much, much worse. There are certainly many deletion protection features worth
calling out, but for companies with required data deletion deadlines, soft deletion was the most pertinent pro‐
tection against bugs and accidental deletion in the case of Blobstore’s clients.
How Google SRE Faces the Challenges of Data Integrity | 351
available to developers, it may or may not substitute for soft deletion, depending on
its implementation.
At Google, revision history has proven useful in recovering from certain data corrup‐
tion scenarios, but not in recovering from most data loss scenarios involving acciden‐
tal deletion, programmatic or otherwise. This is because some revision history
implementations treat deletion as a special case in which previous states must be
removed, as opposed to mutating an item whose history may be retained for a certain
time period. To provide adequate protection against unwanted deletion, apply the
lazy and/or soft deletion principles to revision history also.
Second Layer: Backups and Their Related Recovery Methods
Backups and data recovery are the second line of defense after soft deletion. The most
important principle in this layer is that backups don’t matter; what matters is recov‐
ery. The factors supporting successful recovery should drive your backup decisions,
not the other way around.
In other words, the scenarios in which you want your backups to help you recover
should dictate the following:
• Which backup and recovery methods to use
• How frequently you establish restore points by taking full or incremental back‐
ups of your data
• Where you store backups
• How long you retain backups
How much recent data can you afford to lose during a recovery effort? The less data
you can afford to lose, the more serious you should be about an incremental backup
strategy. In one of Google’s most extreme cases, we used a near-real-time streaming
backup strategy for an older version of Gmail.
Even if money isn’t a limitation, frequent full backups are expensive in other ways.
Most notably, they impose a compute burden on the live datastores of your service
while it’s serving users, driving your service closer to its scalability and performance
limits. To ease this burden, you can take full backups during off-peak hours, and then
a series of incremental backups when your service is busier.
How quickly do you need to recover? The faster your users need to be rescued, the
more local your backups should be. Often Google retains costly but quick-to-restore
352 | Chapter 26: Data Integrity: What You Read Is
snapshots8 for very short periods of time within the storage instance, and stores less
recent backups on random access distributed storage within the same (or nearby)
datacenter for a slightly longer time. Such a strategy alone would not protect from
site-level failures, so those backups are often transferred to nearline or offline loca‐
tions for a longer time period before they’re expired in favor of newer backups.
How far back should your backups reach? Your backup strategy becomes more costly
the further back you reach, while the scenarios from which you can hope to recover
increase (although this increase is subject to diminishing returns).
In Google’s experience, low-grade data mutation or deletion bugs within application
code demand the furthest reaches back in time, as some of those bugs were noticed
months after the first data loss began. Such cases suggest that you’d like the ability to
reach back in time as far as possible.
On the flip side, in a high-velocity development environment, changes to code and
schema may render older backups expensive or impossible to use. Furthermore, it is
challenging to recover different subsets of data to different restore points, because
doing so would involve multiple backups. Yet, that is exactly the sort of recovery
effort demanded by low-grade data corruption or deletion scenarios.
The strategies described in “Third Layer: Early Detection” on page 356 are meant to
speed detection of low-grade data mutation or deletion bugs within application code,
at least partly warding off the need for this type of complex recovery effort. Still, how
do you confer reasonable protection before you know what kinds of issues to detect?
Google chose to draw the line between 30 and 90 days of backups for many services.
Where a service falls within this window depends on its tolerance for data loss and its
relative investments in early detection.
To sum up our advice for guarding against the 24 combinations of data integrity fail‐
ure modes: addressing a broad range of scenarios at reasonable cost demands a tiered
backup strategy. The first tier comprises many frequent and quickly restored backups
stored closest to the live datastores, perhaps using the same or similar storage tech‐
nologies as the data sources. Doing so confers protection from the majority of scenar‐
ios involving software bugs and developer error. Due to relative expense, backups are
retained in this tier for anywhere from hours to single-digit days, and may take
minutes to restore.
The second tier comprises fewer backups retained for single-digit or low double-digit
days on random access distributed filesystems local to the site. These backups may
8 “Snapshot” here refers to a read-only, static view of a storage instance, such as snapshots of SQL databases.
Snapshots are often implemented using copy-on-write semantics for storage efficiency. They can be expensive
for two reasons: first, they contend for the same storage capacity as the live datastores, and second, the faster
your data mutates, the less efficiency is gained from copying-on-write.
How Google SRE Faces the Challenges of Data Integrity | 353
take hours to restore and confer additional protection from mishaps affecting partic‐
ular storage technologies in your serving stack, but not the technologies used to con‐
tain the backups. This tier also protects against bugs in your application that are
detected too late to rely upon the first tier of your backup strategy. If you are intro‐
ducing new versions of your code to production twice a week, it may make sense to
retain these backups for at least a week or two before deleting them.
Subsequent tiers take advantage of nearline storage such as dedicated tape libraries
and offsite storage of the backup media (e.g., tapes or disk drives). Backups in these
tiers confer protection against site-level issues, such as a datacenter power outage or
distributed filesystem corruption due to a bug.
It is expensive to move large amounts of data to and from tiers. On the other hand,
storage capacity at the later tiers does not contend with growth of the live production
storage instances of your service. As a result, backups in these tiers tend to be taken
less frequently but retained longer.
Overarching Layer: Replication
In an ideal world, every storage instance, including the instances containing your
backups, would be replicated. During a data recovery effort, the last thing you want is
to discover is that your backups themselves lost the needed data or that the datacenter
containing the most useful backup is under maintenance.
As the volume of data increases, replication of every storage instance isn’t always fea‐
sible. In such cases, it makes sense to stagger successive backups across different sites,
each of which may fail independently, and to write your backups using a redundancy
method such as RAID, Reed-Solomon erasure codes, or GFS-style replication.9
When choosing a system of redundancy, don’t rely upon an infrequently used scheme
whose only “tests” of efficacy are your own infrequent data recovery attempts.
Instead, choose a popular scheme that’s in common and continual use by many of its
users.
1T Versus 1E: Not “Just” a Bigger Backup
Processes and practices applied to volumes of data measured in T (terabytes) don’t
scale well to data measured in E (exabytes). Validating, copying, and performing
round-trip tests on a few gigabytes of structured data is an interesting problem. How‐
ever, assuming that you have sufficient knowledge of your schema and transaction
model, this exercise doesn’t present any special challenges. You typically just need to
9 For more information on GFS-style replication, see [Ghe03]. For more information on Reed-Solomon erasure
codes, see https://en.wikipedia.org/wiki/Reed–Solomon_error_correction.
354 | Chapter 26: Data Integrity: What You Read Is
procure the machine resources to iterate over your data, perform some validation
logic, and delegate enough storage to hold a few copies of your data.
Now let’s up the ante: instead of a few gigabytes, let’s try securing and validating 700
petabytes of structured data. Assuming ideal SATA 2.0 performance of 300 MB/s, a
single task that iterates over all of your data and performs even the most basic of vali‐
dation checks will take 8 decades. Making a few full backups, assuming you have the
media, is going to take at least as long. Restore time, with some post-processing, will
take even longer. We’re now looking at almost a full century to restore a backup that
was up to 80 years old when you started the restore. Obviously, such a strategy needs
to be rethought.
The most common and largely effective technique used to back up massive amounts
of data is to establish “trust points” in your data—portions of your stored data that
are verified after being rendered immutable, usually by the passage of time. Once we