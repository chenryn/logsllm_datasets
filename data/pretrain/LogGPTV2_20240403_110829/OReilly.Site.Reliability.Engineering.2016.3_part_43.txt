### Scaling Issues: Fulls, Incrementals, and the Competing Forces of Backups and Restores

A common but flawed response to the question “Do you have a backup?” is “We have something even better than a backup—replication!” While replication offers several benefits, such as data locality and protection from site-specific disasters, it is not a panacea for all data loss scenarios. Datastores that automatically sync multiple replicas can propagate corrupt database rows or errant deletions to all copies before the problem is isolated.

To mitigate this, organizations often create non-serving copies of their data in different formats, such as frequent database exports to native files. This additional measure provides protection against user errors and application-layer bugs, but it does not guard against lower-level issues. It also introduces risks, such as bugs during data conversion and mismatches in semantics between formats. For example, a zero-day attack at the filesystem or device driver level could compromise any copies stored on the same compromised system.

### Diversity is Key

Diversity in data storage is crucial. To protect against a failure at layer X, data should be stored on diverse components at that layer. Media isolation, such as using tape drives alongside disk-based systems, can provide additional protection. Ideally, backups would be made on entirely different media, like clay tablets, which are the oldest known form of written records.

### The Trade-offs Between Freshness and Restore Time

The forces of data freshness and restore completion are in constant competition. The deeper in the stack you push a snapshot, the longer it takes to make a copy, reducing the frequency of backups. For instance, a transaction may take seconds to replicate at the database level, while exporting a database snapshot to the underlying filesystem might take 40 minutes, and a full filesystem backup could take hours.

In a restore scenario, you might lose up to 40 minutes of recent data with a database snapshot, or hours of transactions with a filesystem backup. Additionally, restoring data can take as long as backing it up, further extending the downtime.

### Retention Policies

Retention policies, which dictate how long you keep copies of your data, are another critical factor. While the sudden emptying of a database is likely to be noticed quickly, gradual data loss might go undetected for days. In such cases, older snapshots are necessary, and merging restored data with the current state can significantly complicate the restore process.

### Google SRE's Approach to Data Integrity

Google’s Site Reliability Engineering (SRE) team assumes that both the underlying systems and protection mechanisms are prone to failure. Maintaining data integrity at scale, especially in a high-change environment, requires a combination of complementary but uncoupled practices, each designed to offer robust protection.

### Defense in Depth

There is no single solution to guard against all data loss scenarios. Instead, a multi-layered approach, or defense in depth, is required. Each layer provides protection against progressively less common data loss events. The first layer is soft deletion, which has proven effective against inadvertent data deletion. The second layer is backups and their related recovery methods. The third layer is regular data validation.

### Soft Deletion

Soft deletion, or lazy deletion, marks data as deleted without immediately removing it. This allows for undeletion within a reasonable time frame, typically 15 to 60 days. Soft deletion is particularly useful in consumer products like Gmail and Google Drive, where accidental deletions are common. For cloud computing offerings, an additional layer of lazy deletion can be implemented, where the cloud service provider retains deleted data for a short period before final destruction.

### Backups and Recovery

Backups and data recovery are the second line of defense. The key principle is that successful recovery, not just having backups, is what matters. Factors such as the frequency of full and incremental backups, the location of backups, and retention periods should be driven by the recovery scenarios you aim to support.

For example, if you cannot afford to lose much recent data, a near-real-time streaming backup strategy might be necessary. However, frequent full backups can be computationally expensive, so they are often scheduled during off-peak hours, with incremental backups taken during busier times.

### Tiered Backup Strategy

A tiered backup strategy is essential for balancing cost and coverage. The first tier consists of frequent, quickly restorable backups stored close to the live datastores. These protect against software bugs and developer errors. The second tier includes fewer, longer-retained backups on local distributed filesystems, providing additional protection against specific storage technologies. Subsequent tiers use nearline storage, such as dedicated tape libraries, to protect against site-level failures.

### Replication

In an ideal world, every storage instance, including backups, would be replicated. As data volumes increase, this becomes less feasible, so staggering backups across different sites and using redundancy methods like RAID or Reed-Solomon erasure codes is a practical alternative.

### Scalability Challenges

Processes that work well for terabytes of data do not scale to exabytes. Validating and copying massive amounts of data, such as 700 petabytes, presents significant challenges. A common technique is to establish "trust points" in the data, verifying immutable portions after a certain period. This approach helps manage the complexity and time required for large-scale data validation and recovery.

By implementing these strategies, organizations can better protect their data integrity and ensure that they can recover from a wide range of potential data loss scenarios.