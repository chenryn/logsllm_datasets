[1]investigatedtheexperiencebyBVIPwithvoice-activatedpersonalassistance
and reported that users often feel responses being too verbose, frustrated at
interacting at a lower pace than desired, or not able adapt interactions to the
requirementsofsocialsituations.Ithasbeenargued[11]thatguidelinesbymajor
commercial voice-based assistants fail to capture preferences and experience of
BVIP, used to faster and more efficient interactions with screen readers. This
calls for further research into conversation design tailored to BVIP.
400 A. Pina et al.
2.3 Approaches and Challenges
Therearemanychallenges indelivering thetypeofsupportrequiredforconver-
sationalbrowsing.Asdiscussedinourpriorwork[13],thisrequiresderivingtwo
important types of knowledge:
– domain knowledge: it refers to the knowledge about the type of function-
ality and content provided by the website, and that will inform the agent of
what should be exposed to the users (e.g., intents, utterances and slots);
– interaction knowledge: it refers to the knowledge about how to operate
and automate the browsing interactions on behalf of the user.
Websites are not equipped with the required conversational knowledge to
enable voice-based interaction, which have motivated three general approaches.
The annotation-based approach provides conversational access to web-
sitesbyenablingdevelopersandcontentproducerstoprovideappropriateanno-
tations[4].Earlyapproachescanbetracedbacktoenablingaccesstowebpages
through telephone call services via VoiceXML [22]. Another general approach
to voice-based accessible information is to rely on accessibility technical speci-
fications, such as Accessible Rich Internet Applications (WAI-ARIA) [15], but
these specifications are meant for screen reading. Baez et al. [4] instead propose
to equip websites with bot-specific annotations. The challenge in this regard
is the adoption by annotations by developers and content producers. A recent
report1 analysing 1 million websites reported that a staggering 98.1% of the
websites analysed had detectable accessibility errors, illustrating the extent of
the adoption of accessibility tags and proper design choices on the Web.
The crowd-based approach utilizes collaborative metadata augmentation
approaches[8,25],relyinginsteadonthecrowdto“fix”accessibilityproblemsor
provide annotations for voice-based access. The Social Accessibility project [24]
is one of these initiatives whose database supports various non visual browsers.
Still, collaborative approaches require a significant community to be viable, and
even so the numbers of services and the rate at which they are created make it
virtually impossible to cover all websites.
Automatic approaches have been used to support non-visual browsing
and are based on heuristics and algorithms. The approaches in this space have
focusedonautomaticallyfixingaccessibilityissues(e.g.,pagesegmentation[14]),
deriving browsing context [21] or predicting next user actions based on current
context [23]. These approaches, however, have not focused on enabling conver-
sational access to websites.
All of the above tell us of the diverse approaches that can support the cog-
nitive augmentation of websites to enable voice-based conversational browsing.
In this work we explore automatic approaches, which have not been studied in
the context of conversational access to websites.
1 https://webaim.org/projects/million/.
Bringing Cognitive Augmentation to Web Browsing Accessibility 401
3 A Heuristic-Based Approach
Inthisworkwefocusonheuristicsthatenablevoice-basednavigation andaccess
to content of websites that are information intensive. We focus on these set of
features (Table1, “Browsing category”) as they i) match the level of support
expected but poorly served by screen readers, and ii) are highly impacted by
accessibility errors in websites.
3.1 Requirements
Fromtheconceptualframework,itbecomesclearthatenablingBVIPtobrowse
websites conversationally requires us to:
– Determine the main (and contextual) offerings of the website
– Identify the current navigation context
– Enable navigation through meaningful segments of the website
– Allow for scanning and search for information in the website
Determining the offerings of the website can be done by leveraging the com-
ponents used in graphical user interfaces to guide users through their offerings:
menus.MenushavespecificsemantictagsinHTML()androlesaspartof
the technical specifications for web accessibility (WAI-ARIA) that allow screen
readers to identify the main navigation links in a website. They also rely on dis-
tinctivevisualandstructuralproperties(e.g.,stylesandposition)tomakethem
easily identifiable by sighted users. For example, they tend to be more promi-
nent, towards the top and repeat across all pages in the website. Instead, more
localised options are typically embedded in the content (e.g., links) or located
within the same section of the page. Advanced models have relied in such visual
properties to derive the role of rendered components in websites [2].
Identifying and keeping track of navigation context is supported in different
waysbyvisualwebbrowsing.Inawebsite,thiscanbeprovidedbydesigne.g.,by
implementingnavigationbreadcrumbsthatexplicitlyrenderthenavigationpath
that took users to their current context. It is also supported by Web browsers
as part of the navigation history, allowing users to go back an forth in their
navigation path but without illustrating it explicitly. In the context of a dialog,
we can leverage this browsing history (available as a Web API) along with the
conversationhistorytoresolvethecurrentbrowsingcontextbasedonnavigation
path (e.g., current page) and previous choices (e.g., name of links selected).
Enabling navigation requires supporting browsing activities across different
pages in the website, and therefore identifying relevant links and their target
components. In visual browsing, the identification of the target components are
typically done visually by sighted users by relying on the layout of websites and
their own goals. That is, when opening a news article, sighted users can focus
their attention on the content of the article, ignoring other components such
as headers, menus and ads. Given the proper accessibility tags, screen readers
can allow users to (manually) identify their targets by skipping regions of the
402 A. Pina et al.
website. To provide a proper experience, it is fundamental to have meaningful
segmentationofthewebsiteaccordingtovisualproperties,andidentifyingtarget
segmentsduringnavigationbasedonnavigationcontext(e.g.,asindonein[21]).
Segmentation techniques have been widely studied in accessibility research and
could be leveraged for this purpose.
Fig.3. Pipeline for augmenting information-intensive websites with conversational
capabilities by leveraging heuristics.
Searching within a website is not a particularly challenging feature. The
challengeliesagaininsegmentingtheresultingelementsandcontextualisingthe
guidancebasedonthetypeofvisualelement(e.g.,paragraph→reading;links→
navigation).However,inaccessibilitythisisassociatedtothenonvisualscanning
task, i.e., efficiently finding relevant information among many relevant ones,
whichhasmotivatedseveraltechniques,includingtheuseofmultipleconcurrent
audio channels [16], that should be considered as potential techniques.
3.2 Prototype Implementation
We implemented a prototype as an exercise into understanding and informing
i) the type of support required in voice-based interactions, as well as ii) their
technicalrequirements.Themainfocuswastoestablishapipelinethatcantake
voice commands, and fulfill them based on an (evolving) set of heuristics. In
doing so, we faced the following architectural constraints:
– The agent needs to serve multiple websites, as with a regular web browser
– The agent needs to support conversational browsing intents, and the experi-
ence need to be optimised for “reading” content
– Processing times should be minimised for a meaningful user experience
– The agent needs to support dynamic web pages
TheresultingpipelineisillustratedinFig.3.Inanutshell,thepipelinetakes
a website and its static and dynamic content to create internal representations
that can be leveraged by the heuristics to serve the predefined browsing intents.
In the following we detail on this pipeline.
Bringing Cognitive Augmentation to Web Browsing Accessibility 403
CrawlingandDataScraping.Thefirstcomponentinthepipelineisincharge
of obtaining the static and dynamic contents and structure of the website for
further processing and analysis. The process starts with the input URL to fetch
the static HTML of each page in the website. It performs a breadth-first search
ofthewebsite’streestructure,identifyingineachpageallthehyperlinkstovisit.
This process is performed the first time the website is accessed, and it is cached
(with an expiration date) for later use. The crawling runs in the background,
stopping at a configurable depth d in the tree or when a number of web pages
p have been processed. This process is implemented with Scrapy2, a Python
framework used for large scale web crawling.
While accessing the static version of a website ensures higher performance
by reducing rendering times and allowing faster website-level analyses, it does
not necessarily represent the actual content presented to the user, since part
of it can be dynamically generated. For this reason, we complement the “quick
glance” provided by the crawling process by accessing the rendered version of
the website on demand – meaning the actual pages the user navigates to. The
implementation relies on Selenium, a powerful tool for automated testing in
web browsers, running Mozilla Firefox in headless mode to access the rendered
versionofthewebsites,withextensionssuchasAdBlockerandi-dont-care-about-
cookies.eu to speed up rendering and loading times.
InformationExtractionandAugmentation.Thiscomponenttakesininput
thewebsite-level informationandthemoredetailedandaccuratepage-level infor-
mationfromrenderedpagestobuildinternalrepresentationsofthewebsite.The
website-level information is leveraged to build the navigation graph of the web-
site, and calculate basic metrics on the structure (e.g., popularity: the number
of times a link is referenced) that can later inform the heuristics. Basic meta-
data is extracted but the static HTML is not further processed at this stage.
Then, when the user navigates to specific page (a node in the navigation graph)
the rendered version of the website is requested, and the actual content and
structural properties of the website are analysed. The page is represented as a
tree-structure,muchliketheDOMbutwhereeachnodeisameaningfulsegment
of the website, as derived by the segmentation heuristics (described later). The
contents of the nodes are extracted and cleaned to make them reading friendly
(e.g., inline links replaced by placeholders and offered separately). The imple-
mentation of this component relied on the BeautifulSoup3 Python package to
analyse the HTML code and scrape data.
Computing Heuristics. The current prototype implements simple heuristics
that serve as placeholders that will allow more comprehensive tests of the entire
pipeline.Anexampleofsuchheuristics,fortheidentificationofthemainofferings
of the website, is based on the observation that links in the main menu tend to
be at the top of the page and present across the entire website. We therefore
leveraged on the navigation map and the calculated popularity metric for each
2 https://scrapy.org/.
3 https://www.crummy.com/software/BeautifulSoup/bs4/doc/.
404 A. Pina et al.
node (i.e., how many times the link is referenced), weighted by the position
attributeofthelinkelementintherenderedwebsite(e.g.,thusdiscerninglinksin
footerandheaders)torankthelinks.Wedonotcurrentlyperformsegmentation,
and the segmentation placeholder just leverages existing region landmarks.
Other features of Table1 currently rely on existing cognitive services. For
example, for providing Summary, we rely on Aylien Text Analysis API4, which
along with Fortiguard Web Filter (see Footnote 4) augments the information
aboutthewebsitewithextrametadata(e.g.,languageandtopicofthewebsite).
The search feature are provided by Google Search.
Conversational Agent. The browsing experience is ultimately delivered
through Google Assistant, which was chosen as the voice-based service. This
service provides a conversational medium and performs the speech-to-text and
text-to-speechtransformationstoandfromthenaturallanguageprocessingunit.
WereliedonDialogflow5asnaturallanguageplatform,wheretheintentsforserv-
ingtheconversationalbrowsingneedsweredefined.TheseincludetheBrowsing,
andafewofOperationsandMetadata&ContentfromTable1.Thewebhooksto
handlethefulfillmentsultimatelypointedtoourPythonserver.Thesourcecode
of our prototype is available at https://github.com/Shakk17/WebsiteReader.
3.3 Preliminary Evaluation
A preliminary evaluation of the system was performed so as to assess the tech-
nical performance of the tool, and gain insights on the structure of websites and
the challenges they present to our heuristic-based approach.
A total of 30 websites were selected from Alexa’s top ranking6, taking 5
websites from each of the six categories typically associated with information-
intensive websites: Newspapers, Sports, Reference, Health, Society and Science.
We tested the accessibility compliance of these top websites with the WAVE
accessibility tool7. This revealed that only 4 out of the 30 websites were free of
accessibility errors, which further illustrates the challenges to our approach and
to assistive technology in general.
Inthisexploratoryrun,weevaluatedtheperformanceofthesimpleheuristic
for inferring the offerings of the website. To do this, we first manually analysed
each website to identify the links from the menus (the offerings). These actual
links were then compared against the output of the heuristic, which was we set
to return a maximum of 30 links (threshold), to compute precision and recall.
The results showed that the heuristic is effective in identifying relevant links
(recall=0.79)butlesspreciseindeterminingthenumberoflinkstorecommend
(precision = 0.42). However, this is mainly due to the static threshold and the
highly wide range of menu size and complexity in websites (from 4 to 40 links).
4 https://fortiguard.com/webfilter.
5 https://dialogflow.com/.
6 https://www.alexa.com/topsites/category/Top.
7 https://wave.webaim.org.
Bringing Cognitive Augmentation to Web Browsing Accessibility 405
Indeed, the precision was much higher when the number of recommended links
approached the number of actual links in the menu.
Our observations running these tests tell us that the solution goes beyond
more intelligent heuristics and cut-off values for links. The analysis revealed
the complexity of menus in websites – some of then with dozens of hierarchical
links – which motivates an exploration into new approaches to presenting and
discoveringavailableofferingsconversationally.Theexplorationofconversational
patterns for menu-access as well as heuristics for identifying global and local
intents (links) emerge are interesting areas for future research.
4 Discussion and Future Work
In this paper we have explored the opportunities of cognitive augmentation and
automation to support BVIP in browsing information-intensive websites. The
approach is based on the notion of enabling dialog-based interaction with web-
sites, mediated by a voice-based conversational agent.
These opportunities were materialised in a conceptual framework that sum-
marised based on literature review, our prior work and prototyping exercises,
the categories of support to be addressed to enable conversational browsing by
BVIP.Theseincludetheabilitytointeractwiththecontentsofthewebsite,sup-
port more traditional browsing tasks, automating user workflows and managing
theentireoperatingenvironmentofthebrowsingexperience.Theinfrastructure
of the Web today is not equipped serve these needs, but we have shown that
cognitive computing can enable and augment the existing foundation – much as
with cognitive process augmentation [7] – to help address these needs. Existing
research and techniques in accessibility can greatly kick-start these efforts.
Itishoweverclearthatautomationalonecannotfulfillthisvision.Delivering
a proper conversational experience, under the limitations and constraints posed
bytheproblem,wouldrequireaddressingtechnicalissuesaswellasimplementing
dialog patterns that can reduce their impact and provide guidance. Equipping
website with conversational knowledge, reflecting the intended conversational
experience,appearstobekeyinthisregard.Understandingthecorrecttrade-off
between what should be explicitly annotated and can be automatically derived
are among the challenges to be addressed.
As part of our ongoing work we are planning to integrate a pool of existing
algorithms and heuristics developed in the accessibility community and setup
benchmarks to understand their suitability and performance. We are also plan-
ninguserstudiestounderstandtheimpactofdifferentdialogpatterns,associated
withdifferentlevels ofexplicit andimplicit conversational knowledge. Thelong-
term vision is to integrate conversational capabilities into systems of any kind,
a problem that we have seen emerging and gaining traction in recent years [5].
406 A. Pina et al.
References
1. Abdolrahmani, A., Kuber, R., Branham, S.M.: “Siri talks at you” an empirical
investigation of voice-activated personal assistant (VAPA) usage by individuals
who are blind. In: Proceedings of the 20th International ACM SIGACCESS Con-