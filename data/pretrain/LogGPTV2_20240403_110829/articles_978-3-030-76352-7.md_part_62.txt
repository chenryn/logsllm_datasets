### Investigated Experience with Voice-Activated Personal Assistance

[1] investigated the experience of blind and visually impaired people (BVIP) with voice-activated personal assistants and reported that users often find responses too verbose, are frustrated by the slower pace of interaction, and struggle to adapt interactions to social situations. It has been argued [11] that guidelines from major commercial voice-based assistants fail to capture the preferences and experiences of BVIP, who are accustomed to faster and more efficient interactions with screen readers. This calls for further research into conversation design tailored to BVIP.

### 2.3 Approaches and Challenges

There are many challenges in delivering the type of support required for conversational browsing. As discussed in our prior work [13], this requires deriving two important types of knowledge:

- **Domain Knowledge**: This refers to the knowledge about the type of functionality and content provided by the website, which informs the agent of what should be exposed to the users (e.g., intents, utterances, and slots).
- **Interaction Knowledge**: This refers to the knowledge about how to operate and automate the browsing interactions on behalf of the user.

Websites are not equipped with the required conversational knowledge to enable voice-based interaction, which has motivated three general approaches:

1. **Annotation-Based Approach**: This approach provides conversational access to websites by enabling developers and content producers to provide appropriate annotations [4]. Early approaches can be traced back to enabling access to web pages through telephone call services via VoiceXML [22]. Another general approach to voice-based accessible information is to rely on accessibility technical specifications, such as Accessible Rich Internet Applications (WAI-ARIA) [15], but these specifications are primarily designed for screen reading. Baez et al. [4] propose equipping websites with bot-specific annotations. The challenge here is the adoption of these annotations by developers and content producers. A recent report analyzing 1 million websites found that 98.1% had detectable accessibility errors, illustrating the extent of the adoption of accessibility tags and proper design choices on the Web.

2. **Crowd-Based Approach**: This approach utilizes collaborative metadata augmentation [8, 25], relying on the crowd to "fix" accessibility problems or provide annotations for voice-based access. The Social Accessibility project [24] is one such initiative whose database supports various non-visual browsers. However, collaborative approaches require a significant community to be viable, and the rapid creation of new services makes it virtually impossible to cover all websites.

3. **Automatic Approaches**: These have been used to support non-visual browsing and are based on heuristics and algorithms. The approaches in this space have focused on automatically fixing accessibility issues (e.g., page segmentation [14]), deriving browsing context [21], or predicting next user actions based on current context [23]. However, these approaches have not focused on enabling conversational access to websites.

These diverse approaches highlight the potential for cognitive augmentation of websites to enable voice-based conversational browsing. In this work, we explore automatic approaches, which have not been studied in the context of conversational access to websites.

### 3. A Heuristic-Based Approach

In this work, we focus on heuristics that enable voice-based navigation and access to content on information-intensive websites. We focus on these features (Table 1, "Browsing category") because they match the level of support expected but poorly served by screen readers and are highly impacted by accessibility errors in websites.

#### 3.1 Requirements

From the conceptual framework, it becomes clear that enabling BVIP to browse websites conversationally requires us to:

- Determine the main (and contextual) offerings of the website.
- Identify the current navigation context.
- Enable navigation through meaningful segments of the website.
- Allow for scanning and searching for information within the website.

Determining the offerings of the website can be done by leveraging the components used in graphical user interfaces to guide users through their offerings, such as menus. Menus have specific semantic tags in HTML (`<nav>`) and roles as part of the technical specifications for web accessibility (WAI-ARIA) that allow screen readers to identify the main navigation links in a website. They also rely on distinctive visual and structural properties (e.g., styles and position) to make them easily identifiable by sighted users. For example, they tend to be more prominent, towards the top, and repeat across all pages in the website. More localized options are typically embedded in the content (e.g., links) or located within the same section of the page. Advanced models have relied on such visual properties to derive the role of rendered components in websites [2].

Identifying and keeping track of navigation context is supported in different ways by visual web browsing. In a website, this can be provided by design, e.g., by implementing navigation breadcrumbs that explicitly render the navigation path. It is also supported by web browsers as part of the navigation history, allowing users to go back and forth in their navigation path without explicitly illustrating it. In the context of a dialog, we can leverage this browsing history (available as a Web API) along with the conversation history to resolve the current browsing context based on the navigation path (e.g., current page) and previous choices (e.g., name of links selected).

Enabling navigation requires supporting browsing activities across different pages in the website, and therefore identifying relevant links and their target components. In visual browsing, the identification of target components is typically done visually by sighted users by relying on the layout of websites and their own goals. That is, when opening a news article, sighted users can focus their attention on the content of the article, ignoring other components such as headers, menus, and ads. Given the proper accessibility tags, screen readers can allow users to (manually) identify their targets by skipping regions of the website. To provide a proper experience, it is fundamental to have meaningful segmentation of the website according to visual properties and to identify target segments during navigation based on the navigation context (e.g., as done in [21]). Segmentation techniques have been widely studied in accessibility research and could be leveraged for this purpose.

**Figure 3.** Pipeline for augmenting information-intensive websites with conversational capabilities by leveraging heuristics.

Searching within a website is not a particularly challenging feature. The challenge lies in segmenting the resulting elements and contextualizing the guidance based on the type of visual element (e.g., paragraph → reading; links → navigation). However, in accessibility, this is associated with the non-visual scanning task, i.e., efficiently finding relevant information among many relevant ones, which has motivated several techniques, including the use of multiple concurrent audio channels [16], that should be considered as potential techniques.

#### 3.2 Prototype Implementation

We implemented a prototype to understand and inform the type of support required in voice-based interactions and their technical requirements. The main focus was to establish a pipeline that can take voice commands and fulfill them based on an (evolving) set of heuristics. In doing so, we faced the following architectural constraints:

- The agent needs to serve multiple websites, as with a regular web browser.
- The agent needs to support conversational browsing intents, and the experience needs to be optimized for "reading" content.
- Processing times should be minimized for a meaningful user experience.
- The agent needs to support dynamic web pages.

The resulting pipeline is illustrated in Figure 3. In a nutshell, the pipeline takes a website and its static and dynamic content to create internal representations that can be leveraged by the heuristics to serve the predefined browsing intents. In the following, we detail this pipeline.

**Crawling and Data Scraping.** The first component in the pipeline is responsible for obtaining the static and dynamic contents and structure of the website for further processing and analysis. The process starts with the input URL to fetch the static HTML of each page in the website. It performs a breadth-first search of the website's tree structure, identifying all the hyperlinks to visit in each page. This process is performed the first time the website is accessed and is cached (with an expiration date) for later use. The crawling runs in the background, stopping at a configurable depth \( d \) in the tree or when a number of web pages \( p \) have been processed. This process is implemented with Scrapy, a Python framework used for large-scale web crawling.

While accessing the static version of a website ensures higher performance by reducing rendering times and allowing faster website-level analyses, it does not necessarily represent the actual content presented to the user, since part of it can be dynamically generated. For this reason, we complement the "quick glance" provided by the crawling process by accessing the rendered version of the website on demand—meaning the actual pages the user navigates to. The implementation relies on Selenium, a powerful tool for automated testing in web browsers, running Mozilla Firefox in headless mode to access the rendered version of the websites, with extensions such as AdBlocker and i-dont-care-about-cookies.eu to speed up rendering and loading times.

**Information Extraction and Augmentation.** This component takes the website-level information and the more detailed and accurate page-level information from rendered pages to build internal representations of the website. The website-level information is leveraged to build the navigation graph of the website and calculate basic metrics on the structure (e.g., popularity: the number of times a link is referenced) that can later inform the heuristics. Basic metadata is extracted, but the static HTML is not further processed at this stage. Then, when the user navigates to a specific page (a node in the navigation graph), the rendered version of the website is requested, and the actual content and structural properties of the website are analyzed. The page is represented as a tree-structure, much like the DOM, but where each node is a meaningful segment of the website, as derived by the segmentation heuristics (described later). The contents of the nodes are extracted and cleaned to make them reading-friendly (e.g., inline links replaced by placeholders and offered separately). The implementation of this component relied on the BeautifulSoup Python package to analyze the HTML code and scrape data.

**Computing Heuristics.** The current prototype implements simple heuristics that serve as placeholders to allow more comprehensive tests of the entire pipeline. An example of such heuristics, for the identification of the main offerings of the website, is based on the observation that links in the main menu tend to be at the top of the page and present across the entire website. We therefore leveraged the navigation map and the calculated popularity metric for each node (i.e., how many times the link is referenced), weighted by the position attribute of the link element in the rendered website (e.g., thus discerning links in footer and headers) to rank the links. We do not currently perform segmentation, and the segmentation placeholder just leverages existing region landmarks.

Other features of Table 1 currently rely on existing cognitive services. For example, for providing summaries, we rely on the Aylien Text Analysis API, which, along with Fortiguard Web Filter, augments the information about the website with extra metadata (e.g., language and topic of the website). The search feature is provided by Google Search.

**Conversational Agent.** The browsing experience is ultimately delivered through Google Assistant, which was chosen as the voice-based service. This service provides a conversational medium and performs the speech-to-text and text-to-speech transformations to and from the natural language processing unit. We relied on Dialogflow as the natural language platform, where the intents for serving the conversational browsing needs were defined. These include the Browsing, and a few of Operations and Metadata & Content from Table 1. The webhooks to handle the fulfillment ultimately pointed to our Python server. The source code of our prototype is available at https://github.com/Shakk17/WebsiteReader.

#### 3.3 Preliminary Evaluation

A preliminary evaluation of the system was performed to assess the technical performance of the tool and gain insights into the structure of websites and the challenges they present to our heuristic-based approach.

A total of 30 websites were selected from Alexa's top ranking, taking 5 websites from each of the six categories typically associated with information-intensive websites: Newspapers, Sports, Reference, Health, Society, and Science. We tested the accessibility compliance of these top websites with the WAVE accessibility tool. This revealed that only 4 out of the 30 websites were free of accessibility errors, which further illustrates the challenges to our approach and to assistive technology in general.

In this exploratory run, we evaluated the performance of the simple heuristic for inferring the offerings of the website. To do this, we first manually analyzed each website to identify the links from the menus (the offerings). These actual links were then compared against the output of the heuristic, which was set to return a maximum of 30 links (threshold), to compute precision and recall. The results showed that the heuristic is effective in identifying relevant links (recall = 0.79) but less precise in determining the number of links to recommend (precision = 0.42). However, this is mainly due to the static threshold and the highly wide range of menu size and complexity in websites (from 4 to 40 links). Indeed, the precision was much higher when the number of recommended links approached the number of actual links in the menu.

Our observations from these tests tell us that the solution goes beyond more intelligent heuristics and cut-off values for links. The analysis revealed the complexity of menus in websites—some with dozens of hierarchical links—which motivates an exploration into new approaches to presenting and discovering available offerings conversationally. The exploration of conversational patterns for menu access and heuristics for identifying global and local intents (links) emerge as interesting areas for future research.

### 4. Discussion and Future Work

In this paper, we explored the opportunities of cognitive augmentation and automation to support BVIP in browsing information-intensive websites. The approach is based on the notion of enabling dialog-based interaction with websites, mediated by a voice-based conversational agent.

These opportunities were materialized in a conceptual framework that summarized, based on a literature review, our prior work, and prototyping exercises, the categories of support to be addressed to enable conversational browsing by BVIP. These include the ability to interact with the contents of the website, support more traditional browsing tasks, automate user workflows, and manage the entire operating environment of the browsing experience. The infrastructure of the Web today is not equipped to serve these needs, but we have shown that cognitive computing can enable and augment the existing foundation—much as with cognitive process augmentation [7]—to help address these needs. Existing research and techniques in accessibility can greatly kick-start these efforts.

However, it is clear that automation alone cannot fulfill this vision. Delivering a proper conversational experience, under the limitations and constraints posed by the problem, would require addressing technical issues and implementing dialog patterns that can reduce their impact and provide guidance. Equipping websites with conversational knowledge, reflecting the intended conversational experience, appears to be key in this regard. Understanding the correct trade-off between what should be explicitly annotated and what can be automatically derived are among the challenges to be addressed.

As part of our ongoing work, we are planning to integrate a pool of existing algorithms and heuristics developed in the accessibility community and set up benchmarks to understand their suitability and performance. We are also planning user studies to understand the impact of different dialog patterns, associated with different levels of explicit and implicit conversational knowledge. The long-term vision is to integrate conversational capabilities into systems of any kind, a problem that we have seen emerging and gaining traction in recent years [5].

### References

1. Abdolrahmani, A., Kuber, R., Branham, S.M.: “Siri talks at you” an empirical investigation of voice-activated personal assistant (VAPA) usage by individuals who are blind. In: Proceedings of the 20th International ACM SIGACCESS Conference on Computers and Accessibility.