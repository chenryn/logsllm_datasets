Pr((x , y) ∈ D) = h(x , y , f (x))
h(x , y, f (x ))
h(x ′
, y′
, f (x ′))
inference model h
prediction vector f (x)
classiﬁer f
features x
input
label y
Figure 1: The relation between diﬀerent elements of the
black-box classiﬁcation model f and the inference model h.
inference model h
f (x )
f (x ′)
Gain: 1
2 log(h(x , y, f (x )))+
1
2 log(1 − h(x ′
, y′
, f (x ′)))
classiﬁer f
Loss: l(f (x ), y)+
λ log(h(x , y, f (x )))
x
D
x ′
D ′
Figure 2: Classiﬁcation loss and inference gain, on the
training dataset D and reference dataset D ′,
in our
adversarial training. The classiﬁcation loss is computed
over D, but, the inference gain is computed on both sets. To
simplicity the illustration, the mini-batch size is set to 1.
indirect nonlinear computations on the training data. The exist-
ing inference algorithm suggests training another machine learn-
ing model, as the inference model, to ﬁnd the statistical diﬀer-
ences between predictions on members and predictions on non-
members [45]. In this section, we formally present this attack and
the optimization problem to model the adversary’s objective. Ta-
ble 2 summarizes the notations and the optimization problem. Fig-
ure 1 illustrates the relation between diﬀerent components of a
membership inference attack against machine learning models in
the black-box setting.
Let h be the inference model h : X × Y 2 −→ [0, 1]. For any data
point (x , y) and the model’s prediction vector f (x), it outputs the
probability of (x , y) being a member of D (the training set of f ). Let
PrD (X
Y)
for samples in D and outside D, respectively. In an ideal setting
(of knowing these conditional probability distributions), the gain
function for the membership inference attack can be computed as
the following.
Y) be the conditional probabilities of (X
Y) and Pr\D (X
,
,
,
Gf (h) =
1
2
+
(x ,y)∼PrD (X
1
2
E
(x ,y)∼Pr\D (X
,
E
[log(h(x , y , f (x)))]
Y)
,
[log(1 − h(x , y , f (x)))])
Y)
(4)
The two expectations compute the correctness of the inference
model h when the target data record is sampled from the training
set, or from the rest of the universe. In a realistic setting, the proba-
bility distribution of data points in the universe and the probability
distribution over the members of the training set D are not directly
and accurately available to the adversary (for computing his gain).
Therefore, we compute the empirical gain of the inference model
on two disjoint datasets DA and D ′A, which are sampled according
to the probability distribution of the data points inside the train-
ing set and outside it, respectively. More concretely, the dataset
DA could be a subset of the target training set D, known to the
adversary. Given these sets, the empirical gain of the membership
4
inference model is computed as the following.
Gf , D A
, D ′A(h) =
log(h(x , y , f (x)))
1
2|DA | (x ,y)∈D A
2|D ′A | (x ′
1
+
,y′)∈D ′A
log(1 − h(x ′
, y ′
, f (x ′)))
(5)
Thus, the optimization problem for the membership inference
attack is simply maximizing this empirical gain.
max
h
Gf , D A
, D ′A(h)
(6)
The optimization problem needs to be solved on a given target
classiﬁcation model f . However, it is shown that it can also be
trained on some shadow models, which have the same model type,
architecture, and objective function as the model f , and are trained
on data records sampled from Pr(X
Y) [45].
,
4 MIN-MAX MEMBERSHIP PRIVACY GAME
The adversary always has the upper hand. He adapts his inference
attack to his target model in order to maximize his gain with re-
spect to this existing classiﬁcation model. This means that a de-
fense mechanism will be eventually broken if it is designed with
respect to a particular attack, without anticipating and preparing
for the (strongest) attack against itself. The conﬂicting objectives
of the defender and the adversary can be modeled as a privacy
game [2, 24, 34, 46]. In our particular setting, while the adversary
tries to get the maximum inference gain, the defender needs to
ﬁnd the classiﬁcation model that not only minimizes its loss, but
also minimizes the adversary’s maximum gain. This is a min-max
game.
The privacy objective of the classiﬁcation model is to minimize
its privacy loss with respect to the worst case (i.e., maximum infer-
ence gain) attack. It is easy to achieve this by simply making the
Algorithm 1 The adversarial training algorithm for machine learning with membership privacy. This algorithm optimizes the min-max
objective function (7). Each epoch of training includes k steps of the maximization part of (7), to ﬁnd the best inference attack model,
followed by one step of the minimization part of (7) to ﬁnd the best defensive classiﬁcation model against such attack model.
for k steps do
1: for number of the training epochs do
2:
3:
4:
5:
Randomly sample a mini-batch of m training data points {(x1 , y1), (x2 , y2), · · · , (xm , ym)} from the training set D.
Randomly sample a mini-batch of m reference data points {(x ′
Update the inference model h by ascending its stochastic gradients over its parameters ω:
m )} from the reference set D ′.
2), · · · , (x ′
1), (x ′
m , y ′
1 , y ′
2 , y ′
λ
2m  mi =1
∇ω
log(h(xi , yi , f (xi ))) +
mi =1(cid:0)log(1 − h(x ′
i , y ′
i , f (x ′
i )))(cid:1)!
6:
7:
8:
end for
Randomly sample a fresh mini-batch of m training data points {(x1 , y1), (x2 , y2), · · · , (xm , ym)} from D.
Update the classiﬁcation model f by descending its stochastic gradients over its parameters θ:
∇θ
1
m
mi =1
9: end for
(l(f (xi ), yi ) + λ log(h(xi , yi , f (xi ))))
output of the model independent of its input, at the cost of destroy-
ing the utility of the classiﬁer. Thus, we update the training objec-
tive of the classiﬁcation model as minimizing privacy loss with
respect to the strongest inference attack, with minimum classiﬁca-
tion loss. This results in designing the optimal privacy mechanism
which is also utility maximizing.
We formalize the joint privacy and classiﬁcation objectives in
the following min-max optimization problem.
©(cid:173)(cid:173)(cid:173)(cid:173)(cid:173)(cid:173)«
min
LD (f ) + λ max
Gf , D , D ′(h)
f
h
|              {z              }
|                                       {z                                       }
optimal privacy-preserving classiﬁcation
optimal inference
ª®®®®®®¬
(7)
The inner maximization ﬁnds the strongest inference model h
against a given classiﬁcation model f . The outer minimization
ﬁnds the strongest defensive classiﬁcation model f against a given
h. The parameter λ controls the importance of optimizing classiﬁ-
cation accuracy versus membership privacy. The inference attack
term which is multiplied by λ acts as a regularizer for the classiﬁ-
cation model. In other words, it prevents the classiﬁcation model
to arbitrarily adapt itself to its training data at the cost of leaking
information about the training data to the inference attack model.
Note that (7) is equivalent to (3), if we set R(f ) to (6).
These two optimizations need to be solved jointly to ﬁnd the
equilibrium point. For arbitrarily complex functions f and h, this
game can be solved numerically using the stochastic gradient de-
scent algorithm (similar to the case of generative adversarial net-
works [21]). The training involves two datasets: the training set D,
which will be used to train the classiﬁer, and a disjoint reference set
D ′ that, similar to the training set, contains samples from Pr(X
Y).
Algorithm 1 presents the pseudo-code of the adversarial train-
ing of the classiﬁer f on D—against its best inference attack model
,
h. In each epoch of training, the two models f and h are alter-
natively trained to ﬁnd their best responses against each other
through solving the nested optimizations in (7). In the inner opti-
mization step: for a ﬁxed classiﬁer f , the inference model is trained
to distinguish the predictions of f on its training set D from predic-
tions of the same model f on reference set D ′. This step maximizes
the empirical inference gain Gf , D , D ′(h). In the outer optimization
step: for a ﬁxed inference attack h, the classiﬁer is trained on D,
with the adversary’s gain function acting as a regularizer. This min-
imizes the empirical classiﬁcation loss LD (f ) + λ Gf , D , D ′(h). We
want this algorithm to converge to the equilibrium point of the
min-max game that solves (7).
Theoretical Analysis. Our ultimate objective is to train a clas-
siﬁcation model f such that it has indistinguishably similar output
distributions for data members of its training set versus the non-
members. We make use of the theoretical analysis of the generative
adversarial networks [21] to reason about how Algorithm 1 tries
to converge to such privacy-preserving model. For a given classiﬁ-
cation model f , let pf be the probability distribution of its output
(i.e., prediction vector) on its training data D, and let p ′
be the prob-
f
ability distribution of the output of f on any data points outside
the training dataset (i.e., X × Y \ D).
For a given classiﬁer f , the optimal attack model maximizes (4),
which can be expanded to the following.
PrD (x , y) pf (f (x)) log(h(x , y , f (x)))dxdy
(Þx ,y
1
2
Gf (h) =
+Þx ′
,y′
Pr\D (x ′
, y ′) p ′
f (f (x ′)) log(1 − h(x ′
, y ′
, f (x ′)))dx ′dy ′)
(8)
The maximum value of Gf (h) is achievable by the optimal infer-
with enough learning capacity, and is equal to
ence model h∗
f
h∗
f (x , y , f (x)) =
PrD (x , y) pf (f (x))
PrD (x , y) pf (f (x)) + Pr\D (x , y) p ′
f
(f (x))
(9)
5
This combines what is already known (to the adversary) about
the distribution of data inside and outside the training set, and
what can be learned from the predictions of the model about its
training set. Given that the training set is sampled from the un-
derlying probability distribution Pr(X
Y), and assuming that the
underlying distribution of the training data is a-priori unknown to
the adversary, the optimal inference model is the following.
,
h∗
f (x , y , f (x)) =
pf (f (x))
pf (f (x)) + p ′
f
(f (x))
(10)
.
This means that the best strategy of the adversary is to deter-
mine membership by comparing the probability that the prediction
f (x) comes from distribution pf or alternatively from p ′
f
Given the optimal strategy of adversary against any classiﬁer,
we design the optimal classiﬁer as the best response to the inference
attack. The privacy-preserving classiﬁcation task has two objec-
tives (7): minimizing both the classiﬁcation loss LD (f ) and the pri-
vacy loss Gf , D , D ′(h∗
). In the state space of all classiﬁcation mod-
f
els f that have the same classiﬁcation loss LD (f ), the min-max
game (7) will be reduced to solving minf maxh Gf , D , D ′(h) which
is then computed as:
min
f
max
h
E
(x ,y)∼PrD (X
,
[log(h(x , y , f (x)))]
Y)
+
E
(x ,y)∼Pr\D (X
,
[log(1 − h(x , y , f (x)))]
Y)
(11)
According to Theorem 1 in [21], the optimal function f ∗ is the
global minimization function if and only if pf ∗ = p ′
f ∗. This means
that for a ﬁxed classiﬁcation loss and with enough learning ca-
pacity for model f , the training algorithm minimizes the privacy
loss by making the two distributions pf ∗ and p ′
f ∗ indistinguishable.
This implies that the optimal classiﬁer pushes the membership in-
ference probability h∗
(x , y , f (x)) to converge to 0.5, i.e., random
f
guess. According to Proposition 2 in [21], we can prove that the
stochastic gradient descent algorithm of Algorithm 1 eventually
converges to the equilibrium of the min-max game (7). To sum-
marize, the solution will be a classiﬁcation model with minimum
classiﬁcation loss such that the strongest inference attack against
it cannot distinguish its training set members from non-members
by observing the model’s predictions on them.
5 EXPERIMENTS
In this section, we apply our method to several diﬀerent classiﬁ-
cation tasks using various neural network structures. We imple-
mented our method using Pytorch1. The purpose of this section
is to empirically show the robustness of our privacy-preserving
model against inference attacks and its negligible classiﬁcation
loss.
5.1 Datasets
We use three datasets: a major machine learning benchmark
dataset (CIFAR100), and two datasets (Purchase100, Texas100)
which are used in the original membership inference attack against
machine learning models [45].
CIFAR100. This is a major benchmark dataset used to evaluate
image recognition algorithms [30]. The dataset contains 60,000 im-
ages, each composed of 32 × 32 color pixels. The records are clus-
tered into 100 classes, where each class represents one object.
Purchase100. This dataset is based on Kaggle’s “acquire valued
shopper” challenge. 2 The dataset includes shopping records for
several thousand individuals. The goal of the challenge is to ﬁnd
oﬀer discounts to attract new shoppers to buy new products. Cour-
tesy of the authors [45], we obtained the processed and simpli-
ﬁed version of this dataset. Each data record corresponds to one
costumer and has 600 binary features (each corresponding to one
item). Each feature reﬂects if the item is purchased by the costumer
or not. The data is clustered into 100 classes and the task is to pre-
dict the class for each costumer. The dataset contains 197,324 data
records.
Texas100. This dataset includes hospital discharge data. The
records in the dataset contain information about inpatient stays
in several health facilities published by the Texas Department of
State Health Services. Data records have features about the exter-
nal causes of injury (e.g., suicide, drug misuse), the diagnosis (e.g.,
schizophrenia, illegal abortion), the procedures the patient under-
went (e.g., surgery), and generic information such as gender, age,
race, hospital ID, and length of stay. Courtesy of the authors [45],
we obtained the processed dataset, which contains 67,330 records
and 6,170 binary features which represent the 100 most frequent
medical procedures. The records are clustered into 100 classes,