### Center of the Image and Trigger Effectiveness

Placing the trigger in the center of the image generates the most confusing representations. Even when faced with the most challenging trigger (a 2x2 box in the center, as shown in Fig. 9a), our SCAn method remains highly effective (Fig. 10a). However, without TaCT, even the most challenging trigger (Fig. 9a) cannot bypass previous defenses (Fig. 2).

### Clean Data for Decomposition

To achieve high discriminability on mixed representations, our untangling model needs to accurately estimate the covariance matrix \( S_\epsilon \), which describes the sparsity of representations within the same class. For this purpose, we use a set of clean data to avoid the effects induced by adversaries. Our experiments demonstrate that using a small set of clean data, comprising 10% of the entire dataset, SCAn can accurately recover the covariance matrices.

### Performance with Limited Clean Data

Further studies show that SCAn performs well even with a much smaller amount of clean data and even with moderately contaminated data. Specifically, in the presence of 2% attack and 1% cover images, we adjusted the amount of clean data used for decomposition analysis. The results, shown in Fig. 13, indicate that even when the clean data constitutes only 0.3% of the entire dataset, SCAn still generates sufficiently accurate covariance matrices to differentiate the target class from others.

Additionally, we added contaminated images to the clean dataset, considering that \( k \) out of \( n \) images in the dataset are infected by the adversary. Fig. 11 shows the experimental results when the ratio \( k/n \) ranges from 0.01 to 0.25 for each class. We found that SCAn remains effective up to a ratio of 0.17, meaning that if no more than 17% of the images in each class are contaminated, our decomposition algorithm can still produce sufficiently accurate parameters to help untangle and perform hypothesis testing to capture attack instances.

### Comparison with Other Defenses

#### Conventional Data Poisoning Attacks

In conventional data poisoning attacks, the adversary injects images carrying the same trigger into the target model’s training set, regardless of their original class. This type of backdoor attack is extensively studied in prior research [4,8,9,42]. As analyzed in Section 3.1, this attack leads to a source-agnostic backdoor that can be triggered by any class image when the trigger is present.

**Offline Protection Against Conventional Attacks:**

In offline settings, both benign and attack images are processed at once, with a decision made on each class whether it is normal or infected. We evaluated the offline performance of SCAn compared to two existing defenses, NC [42] and AC [4], designed for detecting backdoors offline. Similar to the settings in Section 3.2, we trained 1376 source-agnostic backdoor-infected models on GTSRB and 320 (10x32) source-agnostic backdoor-infected models on CIFAR-10. On these models, we ran an AC re-implemented according to its paper and an NC using its original code released by the authors, along with SCAn. The decomposition model of SCAn was built on 1000 clean images randomly selected from the test set. Table 4 illustrates our experimental results (A columns under the offline section). We observe that all approaches perform well on source-agnostic attacks, achieving comparable results with negligible False Positive Rate (FPR) at high True Positive Rate (TPR), with SCAn slightly outperforming the other two.

**Online Protection Against Conventional Attacks:**

In online settings, images are processed one by one, with a decision made on each whether it is legitimate or malicious. We evaluated the online performance of SCAn compared to two existing defenses, SentiNet [8] and STRIP [9], capable of providing online protection.

To enable SCAn to operate online, we first built the decomposition and untangling models offline on a clean dataset. For each incoming image, our approach only needs to update the untangling model for the image’s class. Based on the untangling result, we break the class into two subgroups, identify the subgroup containing the new image, and calculate the statistic \( J^* \) of the class. Finally, the new image is flagged as malicious if it ends up in the class with a \( J^* \) higher than the threshold (exp(2)) and also belongs to the subgroup with fewer clean images than the counterpart.

In our experiments, we ran SCAn, SentiNet, and STRIP on GTSRB and CIFAR-10. To evaluate SCAn, we randomly selected 1000 images from the test set as the clean dataset. In the experiments, SentiNet was configured to strictly follow the setting in its paper, and STRIP was evaluated using its original code as released by prior research. In line with the testing setting of STRIP, we randomly selected 4000 images as the test set, half of which are benign and the rest are malicious. Table 4 presents the experimental results (A columns under the online section). All three methods perform well, though SCAn incurs a slightly higher FPR due to its dependency on the accumulation of attack images to bootstrap its statistical analysis. According to our estimate, our approach needs about 50 attack images to reliably detect further inputs with triggers.

**Comparison on TaCT:**

Our analysis of existing protections against TaCT over GTSRB is reported in Section 3.2 (Table 4, Column T under GTSRB). In Table 4, we show the performance of SCAn on both GTSRB and CIFAR-10, to compare with that of existing approaches. Specifically, on CIFAR-10, 320 TaCT-infected models were trained using 1000 attack images and 1500 cover images from three cover classes. The T columns of Table 4 summarize the results, showing that against TaCT, SCAn outperforms the four existing approaches, with much lower FPRs.

**Comparison with ABS:**

A recently proposed solution, ABS [23], detects compromised backdoor neurons based on a large difference in their activation with or without a Trojaned image. This approach assumes that only a single neuron will be triggered by the attack image [41], which may not hold true in the presence of TaCT. Given the dependence between the trigger and the source label under TaCT, several neurons could be activated by a trigger. More importantly, the activation is caused not only by the trigger but also by the features of the source class carried by the attack image, reducing the observed difference in activation. In our study, we tested ABS on CIFAR-10 against TaCT using the executable provided by the authors. The results, presented in the last column of Table 4, show that ABS still cannot handle TaCT effectively. Additionally, its performance against conventional data poisoning attacks is found to be in line with that of SCAn, which we do not present due to space limitations.

**Comparison with Other Solutions:**

We also studied two recent backdoor countermeasures: one leveraging GAN to clean up a model [30] and the other comparing a model fine-tuned on noised data with the original one to mitigate the effect of a backdoor attack [41]. We evaluated them under TaCT on CIFAR-10 (which their released code is built upon) and found that neither can significantly reduce the Attack Success Rate (ASR) of TaCT attacks. In 100 independent experiments, we observed that for a TaCT-infected model, the average ASR goes down from 76% to 74% in the GAN-based approach and from 98% to 92% in the other approach. These protections failed to raise the bar against TaCT, while SCAn did.

### Robustness Against Other Attacks

**Blending-Trigger Attack:**

An "unconventional" attack we ran against SCAn is the blending-trigger attack [7], which mixes a trigger into normal images at the pixel level and injects the blended images into the training set. The attack was evaluated under the setting of prior research [7], using the hello kitty image as the trigger and \( \kappa = 0.2 \). Our results (Fig. 12) demonstrate the robustness of SCAn against this attack.

**Poison Frogs Attack:**

Another unconventional attack is the poison frogs attack, originally proposed for transfer learning and later extended to attack end-to-end training scenarios [32]. The adversary selects a target image \( t \) from the target class and a base image \( b \) from the source class to produce a poison image \( p \) for every base-target image pair ((b, t)) as follows:
\[ p = \arg\min_x \| R(x) - R(t) \|^2_2 + \beta \| x - b \|^2_2, \]
where \( R(\cdot) \) produces the representation of the input \( x \), and \( \beta \) is a parameter that balances the two terms in the equation. The first term moves the poison image \( p \) toward the target image \( t \) in the representation domain, while the second keeps \( p \) in the vicinity of the base image \( b \). In this way, \( p \) is expected to be classified into the class of the target \( t \) but still appears visually similar to \( b \). In the attack, the adversary blends the poison images with the target ones using \( \kappa = 0.3 \) (the same as [32]) and injects such images into the training set.

We evaluated SCAn on this attack using the code from its authors and the original dataset (the dog-vs-fish set [16]). In our experiment, we generated 70 poison images whose base images are dogs and targets are fish, and contaminated the dog set with these images. Our detection results, displayed in Fig. 16, show that \( J \) of the dog set goes far beyond that of the fish set, indicating that SCAn successfully defeats this attack.

**Multiple Target-Trigger Attack:**

The adversary might attempt to infect a model using multiple triggers, each targeting a different class, to elevate \( J^* \) for many classes and undermine the effectiveness of outlier detection. This attempt, however, introduces an observable drop in both the top-1 accuracy and the targeted misclassification rate. In our research, we analyzed the threat of the attack using different numbers of triggers targeting multiple labels. These triggers are all of the same shape (box trigger, see Fig. 9a) but in different color patterns (e.g., red+blue, purple+yellow). We utilized 1% of the training set as the clean data for decomposition. As demonstrated in Fig. 14, SCAn starts to miss some infected classes when 8 or more triggers are injected into the training set, which can be addressed by using more clean data as long as the number of targeted classes stays below half of the total classes. Fig. 15 shows the amount of clean data needed to defeat multiple target-trigger attacks on GTSRB. Specifically, randomly sampling 18% of the dataset can defeat the attacks targeting 21 (48.8%) classes. Most importantly, when more than half of the classes are targeted (Fig. 17), the attack becomes less stealthy, as the negative impact on model performance becomes obvious: on ILSVRC2012, the model’s top-1 accuracy drops from 76.3% to 71.1%, implying that this evasion attempt might lead to the exposure of the backdoor. Meanwhile, the model’s misclassification rate for attack images decreases significantly (from 99.3% to 58.4%), indicating that the trigger is less effective.

### Adaptive Attacks

**Parameter Inference Attack:**

SCAn has a critical parameter \( S_\epsilon \), which determines how to split images in one class into two subgroups (Eqn. 5) and how to calculate the statistic \( J \) (Eqn. 8). If exposed, an adversary may exploit white-box attacks to evade SCAn detection. Specifically, an adversary may train substitute models to estimate the \( S_\epsilon \) of the target model and infer the representations of the attack images. Using this information, the adversary may design triggers through reverse engineering using the substitute models (like NC did). To understand how likely \( S_\epsilon \) can be accurately estimated, we conducted the following experiment. We trained 100 models on GTSRB using the same data, structure, and hyperparameters, with only different randomly initialized values of inner-parameters. We then ran these 100 models to produce representations for the images in GTSRB. Based on each model’s representations, we calculated its \( S_\epsilon \) for SCAn and further calculated the distances between \( S_\epsilon \) from two models. The Cumulative Distribution Function (CDF) of the distances among a total of 4950 (= C2 100) pairs of models are illustrated in Fig. 18, compared with the CDF of the norms of \( S_\epsilon \) of these 100 models. From the figure, we observe that the two CDFs are similar, indicating that the difference between the \( S_\epsilon \) from two models is comparable with the norm of \( S_\epsilon \), making it hard to accurately estimate the \( S_\epsilon \) of a target model from substitute models: the estimation error is as high as its mean.

**Black-Box Trigger Adjustment Attack:**

We further consider an adversary who is knowledgeable about our approach and tries to evade it under the black-box model, as assumed in our threat model (Section 2.4). For this purpose, we utilized a technique proposed by Andrew et al. [13], a black-box approach known for its effectiveness in finding a model’s adversarial examples within a limited number of queries, based on a black-box derivative method improved over a prior approach.