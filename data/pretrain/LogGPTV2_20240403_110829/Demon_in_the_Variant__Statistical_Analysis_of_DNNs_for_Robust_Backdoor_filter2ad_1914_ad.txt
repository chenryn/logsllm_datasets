center of the image will produce the most confused represen-
tations. Even facing the most challenging trigger (2x2 box in
4https://en.wikipedia.org/wiki/Median_absolute_deviation
5These trigger images can be downloaded from our website:
https://github.com/TDteach/backdoor.git, which contains also our code.
(a) Box
(b) Normal
(c) Square
(d) Watermark
Figure 10: Detection results of SCAn on different datasets and triggers.
the center, Fig. 9a), our SCAn still is very effective (Fig. 10a).
But, without TaCT, even the most challenging trigger (Fig. 9a)
still can not bypass previous defenses (Fig. 2).
Clean data for decomposition. To achieve a high discrim-
inability on mixed representations, our untangling model
needs to accurately estimate the covariance matrix (Sε), which
describes how sparse the representations from the same class
are. For this purpose, our approach uses a set of clean data to
avoid the effect induced by the adversary. The above experi-
ments have demonstrated that using a small set of clean data
occupying the 10% of the whole dataset, SCAn can accurately
recover the covariance matrices.
J∗ of
Figure 11:
the tar-
get classes under contaminated
clean data.
Figure
blending-trigger attacks.
SCAn
12:
against
Our further study shows that SCAn works well on much
smaller amount of clean data and even on the data moderately
contaminated. Speciﬁcally, in the presence of 2% attack and
1% cover images, we adjusted the amount of the clean data
used for the decomposition analysis. The results are shown
in Fig. 13. We can see here that even when the clean data
collected are merely 0.3% of the whole dataset, still our ap-
proach generated the covariance matrices accurately enough
for differentiating the target class from others.
Also we added contaminated images to the clean dataset,
considering that k out of n images in the dataset are infected
by the adversary. Fig. 11 shows the experimental results when
the ratio k/n goes from 0.01 to 0.25 for each class. We found
that SCAn is still effective when the ratio reaches 0.17: that
is, when no more than 17% of the images in each class are
contaminated by the adversary, still our decomposition algo-
rithm can produce sufﬁciently accurate parameters to help the
1550    30th USENIX Security Symposium
USENIX Association
GTSRBImageNetMegaFace0246810Ln(J*)TargetThresholdGTSRBImageNetMegaFace0246810Ln(J*)TargetThresholdGTSRBImageNetMegaFace0246810Ln(J*)TargetThresholdGTSRBImageNetMegaFace0246810Ln(J*)TargetThreshold00.050.10.150.2Ratio02468Ln(J*)TargetThresholdGTSRBImageNetMegaFace0246810Ln(J*)TargetThresholdFigure 13: J∗ of the target class on different
amount of clean data known for decomposition
model (average over 5 rounds).
Figure 14: Minimum J∗ of target classes under
multiple target-trigger attack and 1% clean data
are known (over 5 rounds).
Figure 15: The amount of clean data required
by decomposition model for defeating multiple
target-trigger attacks on GTSRB.
untangling and the hypothesis test to capture attack instances.
4.4 Comparison
In a conventional data poisoning attack, the adversary injects
to the target model’s training set images carrying the same
trigger, regardless of its original class. This poisoning-based
backdoor attack is most extensively investigated in prior re-
searches [4,8,9,42]. As analysed in the Section 3.1, this attack
leads to a source-agnostic backdoor that can be triggered by
the image from any class when the trigger is present.
Ofﬂine protection against conventional attacks. In ofﬂine
settings, images containing both benign and attack images
were processed at once, with a decision being made on each
class whether it is normal or infected. We evaluated the ofﬂine
performance of SCAn compared with two existing defenses,
NC [42] and AC [4], designing for detecting backdoor of-
ﬂine. Similar with settings in Section 3.2, we trained 1376
source-agnostic backdoor infected models on GTSRB and 320
(10x32) source-agnostic backdoor infected models on CIFAR-
10. On these models, we ran an AC re-implemented according
to its paper and an NC using its original code released by the
authors6, together with SCAn. The decomposition model of
SCAn was built on 1000 clean images randomly selected
from the test set. Table 4 illustrates our experimental results
(A columns under ofﬂine section). We observe that these
approaches all perform well on the source-agnostic attacks,
achieving comparable results – negligible False Positive Rate
(FPR) at high True Positive Rate (TPR), with SCAn slightly
outperforming the other two.
Online protection against conventional attacks. In online
settings, images were processed one by one, with a decision
being made on each of them whether it is legitimate or ma-
licious. We evaluated the online performance of SCAn com-
pared with two existing defenses, SentiNet [8] and STRIP [9],
capable of providing online protection.
To enable SCAn to operate online, we ﬁrst built the de-
composition model and untangling model ofﬂine on a clean
dataset, so for each incoming image our approach only needs
to update the untangling model for the image’s class. Based
upon the untangling result, we then break the class into two
subgroups, identify the one containing the new image and
further calculate the statistic J∗ of the class. Finally, the new
image is ﬂagged as malicious if it ends up in the class with a
J∗ higher than the threshold (exp(2)) and also belongs to the
subgroup with fewer clean images than the counterpart.
In our experiments, we ran SCAn, SentiNet, and STRIP on
GTSRB and CIFAR10. Also to evaluate SCAn, we randomly
selected 1000 images from the test set as the clean dataset.
In the experiments, SentiNet was conﬁgured to strictly fol-
low the setting in its paper and STRIP was evaluated using
its original code as released by the prior research 7. In line
with the testing setting of STRIP, we randomly selected 4000
images as the test set. The half of them are benign and the
rest are malicious. Table 4 presents the experimental results
(A columns under online section). As we can see from the
table, all three methods perform well in experiments, though
SCAn incurs a little higher FPR, due to its dependency on
accumulation of attack images to bootstrap its statistical anal-
ysis. According to our estimate, our approach needs about 50
attack images to reliably detect further inputs with triggers.
Comparison on TaCT. Our analysis of existing protection
against TaCT over GTSRB is reported in Section 3.2 (Ta-
ble 4, Column T under GTSRB). In Table 4, we show the
performance of SCAn on both GTSRB (see Section 3.2) and
CIFAR-10, to compare with that of the existing approaches.
Speciﬁcally, on CIFAR-10, 320 TaCT infected models were
trained using 1000 attack images and 1500 cover images from
three cover classes. The T columns of Table 4 summarizes
the results, showing that, against TaCT, SCAn outperforms
the four existing approaches, with much lower FPRs.
Comparison with ABS. A new solution recently proposed
is ABS [23], which detects compromised backdoor neurons
from a large difference in their activation with or without
a Trojaned image. The approach is based upon the assump-
tion that only a single neuron will be triggered by the attack
image [41], which may not be true in the presence of TaCT:
given the dependence between the trigger and the source label
under TaCT, several neurons could be activated by a trigger;
more importantly the activation here is caused by not only the
trigger but also the features of the source class carried by the
attack image, which reduces the difference in activation as
observed when processing the image. In our study, we tested
ABS on CIFAR-10 against TaCT, using the executable the
authors provide that only works on CIFAR-10. The results
are presented in the last column of Table 4. Speciﬁcally, we
trained 320 TaCT infected models and 320 benign models.
Our experimental results show that ABS still cannot handle
6https://github.com/bolunwang/backdoor.git
7https://github.com/garrisongys/STRIP.git
USENIX Association
30th USENIX Security Symposium    1551
00.20.40.60.81% data are known and clean012345Ln(J*)TargetThreshold12345678910# of triggers012345Ln(J*)TargetThreshold1(2.3%)5(11.6%)10(23.2%)15(34.9%)21(48.8%)# of triggers05101520% of clean dataTable 4: FPRs of defenses on GTSRB and CIFAR-10. Column A are FPRs under source-agnostic attacks and Column T are FPRs under TaCT attacks.
Ofﬂine
NC
AC
GTSRB
SCAn
SCAn
T
Online
SentiNet
A
T
STRIP
T
S
SCAn
A
T
Ofﬂine
NC
CIFAR-10
AC
SCAn
A
A
TPR
95% 0% 0.15% 9.4% 95.3% 0% 77.5% 0.20% 0.32% 0.08% 82.6% 1.82% 75.4% 54.2% 0% 0% 5.36% 92.5% 0% 21.1% 0.19% 0.47%
85.9% 0% 21.6% 11.3% 64.3%
99% 0% 0.15% 14.1% 100% 0% 90.6% 0.55% 1.10% 0.09% 83.6% 4.66% 95.7% 66.6% 0% 0% 8.44% 99.2% 0% 47.8% 0.21% 0.48% 0.05% 93.3% 0% 71.8% 39.4% 97.1%
99.5% 0% 0.19% 14.1% 100% 0% 90.6% 0.74% 1.82% 0.09% 84.1% 6.60% 96.9% 71.6% 0% 0% 8.45% 99.2% 0% 47.8% 0.34% 0.75% 0.05% 94.1% 0% 95.7% 74.6% 98.1%
A
A
A
A
A
T
A
T
A
T
T
T
T
Online
SentiNet
A
T
0%
STRIP
T
S
-
ABS
T
TaCT that SCAn defeats. Also, its performance against con-
ventional data poisoning attacks is found to be in line with
that of SCAn, which we do not present due to the space limit.
Comparison with other solutions. We also studied two re-
cent backdoor countermeasures, one leveraging GAN to clean
up a model [30] and the other comparing a model ﬁne-tuned
on noised data with the original one to mitigate the effect of
a backdoor attack [41]. We evaluated them under TaCT on
CIFAR-10 (which their released code is built upon) and found
that none of these two can signiﬁcantly reduce the Attack
Success Rate (ASR) of TaCT attacks – the criterion their au-
thors used for evaluation: in 100 independent experiments, we
observed that, for a TaCT infected model, the average ASR
goes down from 76% to 74% in the GAN-based approach and
from 98% to 92% in the other approach. The difference of the
initial ASR of TaCT in these two approach comes from the
different trigger pasting method implemented in their source
code. [30] pastes a trigger on a random position of each image,
while [41] pastes the trigger on a ﬁxed position of each image
(the default pasting method we used in other experiments).
Nonetheless, these two protections failed to raised the bar
against TaCT, while SCAn did.
4.5 Robustness against Other Attacks
Blending-trigger attack. An “unconventional” attack we ran
against SCAn is blending-trigger attack [7], which mixes a
trigger into normal images according to Eqn. 1 at the pixel
level (that is, each pixel carrying both the content of the orig-
inal image and that of the trigger) and injects the blended
images into the training set. The attack was evaluated in our
research under the setting of the prior research [7], using the
hello kitty image as the trigger and κ = 0.2. Our results
(Fig. 12) demonstrate the robustness of SCAn against this
attack.
Figure 17: SCAn against multiple
target-trigger attack.
Figure 16: J of dog set and ﬁsh
set under poison frogs attack.
Poison Frogs attack. Another unconventional attack is poi-
son frogs, which was originally proposed for transfer learning
and has later been extended to attack the end-to-end training
scenario in line with our threat model [32]. Speciﬁcally, the
adversary selects a target image t from the target class and
2 + β(cid:107)x − b(cid:107)2
a base image b from the source class to produce a poison
image p for every base-target image pair ((b,t)) as follows:
p = argminx(cid:107)R(x) − R(t)(cid:107)2
2, where R(·) pro-
duces the representation of the input x, and β is a parameter
that balances the two terms in the equation. Here, the ﬁrst
term aims at moving the poison image p toward the target
image t in representation domain, while the second is meant
to keep the poison image p in the vicinity of the base image
b. In this way, p is expected to be classiﬁed into the class
of the target t but still appears to be visually similar to b. In
the attack, the adversary blends the poison images with the
target ones using Eqn. 1 with κ = 0.3 (the same with [32]),
and injects such images into the training set.
We evaluated SCAn on this attack with the code8 from its
authors and the original dataset (the dog-vs-ﬁsh set [16]). In
our experiment, we generated 70 poison images whose base
images are dogs and targets are ﬁshes, and contaminated the
dog set with these images. Our detection results are displayed
in Fig. 16, where J of the dog set goes way beyond that of the
ﬁsh set, indicating that SCAn successfully defeats this attack.
Multiple target-trigger attack. The adversary might attempt
to infect a model using multiple triggers, each targets at a dif-
ferent class, in order to elevate J∗ for many classes to under-
mine the effectiveness of the outlier detection. This attempt,
however, will introduce an observable drop on both the top-1
accuracy and the targeted misclassiﬁcation rate. In our re-
search, we analyzed the threat of the attack using different
number of triggers targeting multiple labels. These triggers
are all of the same shape (box trigger, see Fig. 9a) but in differ-
ent color patterns (e.g., red+blue, purple+yellow). We utilized
1% of the training set as the clean data for the decomposition.
As demonstrated in Fig. 14, SCAn starts to miss some infected
classes when 8 or more triggers are injected into the training
set, which could be addressed by using more clean data as
long as the number of the targeted classes stays below half
of the total classes. Fig. 15 shows the amount of clean data
needed to defeat multiple target-trigger attacks on GTSRB.
Speciﬁcally, randomly sampling 18% of the dataset can defeat
the attacks targeting 21 (48.8%) classes. Most importantly,
when more than half of the classes are targeted (Fig. 17), the
attack becomes less stealthy, since the negative impact on the
model performance becomes obvious: on ILSVRC2012, the
model’s top-1 accuracy drops from 76.3% to 71.1%, which
implies that this evasion attempt might lead to the exposure of
the backdoor; in the meantime, the model’s misclassiﬁcation
rate for attack images decreases signiﬁcantly (from 99.3% to
58.4%), indicating that the trigger is less effective.
8https://github.com/ashafahi/inceptionv3-transferLearn-poison.git
1552    30th USENIX Security Symposium
USENIX Association
dogfish0246810J×1061(0.1%)500(50%)1000(100%)# of triggers6080100Top-1 Accuracy (%)6080100Misclassification rate (%)Top-1Misclassification4.6 Adaptive Attacks
Parameter inference attack. SCAn has a critical parameter
Sε, which determines how to split images in one class into
two subgroups (Eqn. 5) and how to calculate the statistic
J (Eqn. 8). If it is exposed, an adversary may exploit the
white-box attacks to evade the SCAn detection. Speciﬁcally,
an adversary may train substitute models to estimate the Sε
of the target model, and further infer the representations of
the attack images. Using these information, the adversary
may design some triggers through the reverse engineering
using the substitute models (like NC did). To understand
how likely Sε can be accurately estimated, we conducted the
following experiment. We trained 100 models on GTSRB
using the same data, the same structure and the same hyper-
parameters, with only different randomly initialized values of
inner-parameters. We then ran these 100 models to produce
representations for the images in GTSRB. Based on each
model’s representations, we calculated its Sε for SCAn and
further calculated the distances between Sε from two models.
The Cumulative Distribution Function (CDF) of the distances
among a total of 4950 (= C2
100) pairs of models are illustrated
in Fig. 18, compared with the CDF of the norms of Sε of these
100 models. From the ﬁgure, we observe that the two CDF
are similar, indicating that the difference between the Sε from
two models is comparable with the norm of Sε, which makes
it hard to accurately estimate the Sε of a target model from
substitute models: the estimate error is as high as its mean.
Figure 18: CDF of norms of Sε
and the distance between a cou-
ple Sε.
Figure 19: Statistics of black-
box attacks (after moving-mean
ﬁltering).
Black-box trigger adjustment attack. We further consider
an adversary who is knowledgeable about our approach, and
tries to evade it under the black-box model, as assumed in
our threat model (Section 2.4). For this purpose, we utilized
a technique proposed by Andrew et al. [13], a black-box ap-
proach known for its effectiveness in ﬁnding a model’s ad-
versarial examples within a limited number of queries, based
upon a black-box derivative method improved over a prior