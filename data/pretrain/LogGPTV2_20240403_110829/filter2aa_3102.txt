Defending Networks with Incomplete 
Information: A Machine Learning 
Approach 
Alexandre Pinto  
PI:EMAIL 
@alexcpsec 
@MLSecProject 
• Security Monitoring: We are doing it wrong 
• Machine Learning and the Robot Uprising 
• More attacks = more data = better defenses 
• Case study: Model to detect malicious agents 
• MLSec Project 
• Acknowledgments and thanks 
Agenda 
• 12 years in Information Security, done a little bit 
of everything. 
• Past 7 or so years leading security consultancy 
and monitoring teams in Brazil, London and the 
US. 
– If there is any way a SIEM can hurt you, it did to me. 
• Researching machine learning and data science in 
general for the past year or so. Active competitor 
in Kaggle machine learning competitions. 
Who’s this guy? 
• Logs, logs everywhere 
• Where? 
– Log management 
– SIEM solutions 
The Monitoring Problem 
• Why? 
– Compliance 
– Incident Response 
• Gartner Magic Quadrant for Security Information and Event 
Management 2013. 
– “Organizations are failing at early breach detection, with more than 92% of 
breaches undetected by the breached organization”  
– “We continue to see large companies that are re-evaluating SIEM vendors 
to replace SIEM technology associated with partial, marginal or failed 
deployments.” 
• Are these the right tools for the job? 
Monitoring / Log Management is Hard 
•
SANS Eighth Annual 2012 Log and Event Management Survey Results 
(http://www.sans.org/reading_room/analysts_program/SortingThruNoise.pdf) 
Monitoring / Log Management is Hard 
• However, there are 
individuals who will 
do a good job 
• How many do you 
know? 
• DAM hard (ouch!) to 
find these capable 
professionals 
Not exclusively a tool problem 
• How many of these 
very qualified 
professionals will 
we need? 
• How many know/ 
will learn statistics, 
data analysis, data 
science? 
Next up: Big Data Technologies 
• How many of these 
very qualified 
professionals will 
we need? 
• How many know/ 
will learn statistics, 
data analysis, data 
science? 
Next up: Big Data Technologies 
We need an Army! Of ROBOTS! 
• “Machine learning systems automatically learn 
programs from data” (*) 
• You don’t really code the program, but it is 
inferred from data. 
• Intuition of trying to mimic the way the brain 
learns:  that’s where terms like artificial 
intelligence come from. 
Enter Machine Learning 
(*) CACM 55(10) - A Few Useful Things to Know about Machine Learning (Domingos 2012) 
• Sales 
Applications of Machine Learning 
• Trading 
• Image and 
Voice 
Recognition 
• Supervised Learning: 
– Classification (NN, SVM, 
Naïve Bayes) 
– Regression (linear, 
logistic) 
Kinds of Machine Learning 
Source – scikit-learn.github.io/scikit-learn-tutorial/general_concepts.html 
• Unsupervised Learning : 
– Clustering (k-means) 
– Decomposition (PCA, 
SVD) 
• The original use case for 
ML in Information Security 
• Remember the “Bayesian 
filters”? There you go. 
• How many talks have you 
been hearing about SPAM 
filtering lately? ;) 
Remember SPAM filters? 
So what is the fuss? 
• Models will get better with more data 
– We always have to consider bias and variance as we 
select our data points 
• “I’ve got 99 problems, but data ain’t one” 
Domingos, 2012 
Abu-Mostafa, Caltech, 2012 
Designing a model to detect external 
agents with malicious behavior 
• We’ve got all that log data anyway, let’s dig into it 
• Most important thing is the “feature engineering” 
Model: Data Collection 
• Firewall block data from SANS DShield (per day) 
• Firewalls, really? Yes, but could be anything. 
• We get summarized “malicious” data per port 
Not quite “Big Data”, but enough to play 
around 
Model Intuition: Proximity 
• Assumptions to aggregate the data  
• Correlation / proximity / similarity BY BEHAVIOUR 
• “Bad Neighborhoods” concept:  
– Spamhaus x CyberBunker 
– Google Report (June 2013) 
– Moura 2013 
• Group by Netblock 
• Group by ASN (thanks, TC) 
Model Intuition: Temporal Decay 
• Even bad neighborhoods renovate: 
– Agents may change ISP, Botnets may be shut down 
– Paranoia can be ok, but not EVERYONE is out to get 
you 
• As days pass, let’s forget, bit by bit, who attacked 
• A Half-Life decay function will do just fine 
Model Intuition: Temporal Decay 
Model: Calculate Features 
• Cluster your data: what 
behavior are you trying to 
predict? 
• Create “Badness” Rank = 
lwRank (just because) 
• Calculate normalized ranks by 
IP, Netblock (16, 24) and ASN  
• Missing ASNs and Bogons (we 
still have those) handled 
separately, get higher ranks. 
Model: Calculate Features 
• We will have a rank calculation per day 
– Each “day-rank” will accumulate all the knowledge 
we gathered on that IP, Netblock and ASN to that day 
• We NEED different days for the training data 
• Each entry will have its date: 
– Use that “day-rank” 
– NO cheating 
– Survivorship bias issues! 
How are we doing so far? 
Training the Model 
• YAY! We have a bunch of numbers per IP address! 
– How can I use this? 
• We get the latest blocked log files (SANS or not): 
– We have “badness” data on IP Addresses -  features 
– If they are blocked, they are “malicious” - label 
• Sounds familiar? 
• Now, for each behavior to predict: 
– Create a dataset with “enough” observations: 
– ROT of 50k - 60k because of empirical dimensionality. 
Negative and Positive Observations 
• We also require “non-
malicious” IPs! 
• If we just feed the 
algorithms with one label, 
they will get lazy. 
• CHEAP TRICK: Everything 
is “malicious” 
• Gather “non-malicious” IP 
addresses from Alexa and 
Chromium Top 1m Sites. 
SVM FTW! 
• Use your favorite algorithm! YMMV. 
• I chose Support Vector Machines (SVM): 
– Good for classification problems with numeric features 
– Not a lot of features, so it helps control overfitting, built in 
regularization in the model, usually robust 
– Also awesome: hyperplane separation on an unknown infinite 
dimension. 
Jesse Johnson – shapeofdata.wordpress.com 
No idea… Everyone copies this one 
Results: Training Data 
• Cross-Validation: method to test the data against itself 
• On the training data itself, 85 to 95% accuracy 
• Accuracy = (things we got right) / (everything we had) 
• Some behaviors are  much more predictable than others: 
– Port 3389 is close to the 95% 
– Port 22 is close to the 85% 
– SANS has much more data on port 3389. Hmmm…… 
Results: New Data 
• And what about new data? 
• With new data we know the labels, we find: 
– 80 – 85% true positive rate (sensitivity) 
– 85 – 90% true negative rate (specificity) 
• This means that: 
– If the model says something is “bad”, it is 5.3 to 8.5 times 
MORE LIKELY to be bad. 
• Think about this. Our statistical intuition is bad. 
• Wouldn’t you rather have your analysts look at these? 
Results: Really New Data 
Final Remarks 
• These and other algorithms are being developed in a 
personal project of mine: MLSec Project 
• Sign up, send logs, receive reports generated by models! 
– FREE! I need the data! Please help! ;) 
• Looking for contributors, ideas, skeptics to support project 
as well. 
• Please visit http://mlsecproject.org or just e-mail me. 
Thanks! 
• Q&A? 
• Don’t forget your feedback 
forms! 
Alexandre Pinto  
PI:EMAIL 
@alexcpsec 
@MLSecProject