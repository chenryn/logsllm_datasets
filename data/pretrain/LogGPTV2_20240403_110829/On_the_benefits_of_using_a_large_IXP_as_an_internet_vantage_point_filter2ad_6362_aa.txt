# On the Benefits of Using a Large IXP as an Internet Vantage Point

**Authors:**
- Nikolaos Chatzis, TU Berlin, PI:EMAIL
- Georgios Smaragdakis, T-Labs/TU Berlin, PI:EMAIL
- Jan Böttger, TU Berlin, PI:EMAIL
- Thomas Krenc, TU Berlin, PI:EMAIL
- Anja Feldmann, TU Berlin, PI:EMAIL

## Abstract

In the context of measuring the Internet, a long-standing question has been whether there exist well-localized physical entities in today's network where traffic from a representative cross-section of the Internet can be observed at a fine-enough granularity to provide an accurate and informative picture of how these constituents shape and impact the structure and evolution of the Internet and the actual traffic it carries.

In this paper, we answer this question affirmatively by analyzing 17 weeks of continuous sFlow data from one of the largest European Internet Exchange Points (IXPs). We discover a vantage point with excellent visibility into the Internet, observing traffic from all 42,000+ routed Autonomous Systems (ASes), almost all 450,000+ routed prefixes, close to 1.5 million servers, and around a quarter billion IP addresses from all around the globe. To demonstrate the potential of such vantage points, we analyze the server-related portion of the traffic at this IXP, identify the server IPs, and cluster them according to the organizations responsible for delivering the content. In the process, we observe a clear trend among many critical Internet players towards network heterogenization, either by hosting servers of third-party networks in their own infrastructures or by deploying their own servers in strategically chosen third-party networks. While this is a well-known business strategy for companies like Akamai, Google, and Netflix, we show the extent of network heterogenization in today's Internet and illustrate how it enriches the traditional, largely traffic-agnostic AS-level view of the Internet.

## Categories and Subject Descriptors

- Computer Systems Organization
- Computer Communication Networks
- Network Operations [Network monitoring]

## Keywords

- Internet Exchange Points
- Internet Topology
- Traffic Characterization
- Content Delivery

## Permission Notice

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.

IMC'13, October 23–25, 2013, Barcelona, Spain.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-1953-9/13/10 ...$15.00.
http://dx.doi.org/10.1145/2504730.2504746.

## Acknowledgments

This work would not have been possible without the help, engagement, and commitment of Walter Willinger. We express our gratitude to the IXP for their generous cooperation and support. We also thank the anonymous reviewers for their useful feedback. This work was supported in part by the EU projects BigFoot (FP7-ICT-317858) and CHANGE (FP7-ICT-257422), EIT Knowledge and Innovation Communities program, and an IKY-DAAD award (54718944).

## 1. Introduction

The ever-growing demand for web-based traffic (e.g., HD video and other streaming media, e-commerce services), along with the proliferation of new Internet-enabled devices and the emergence of new content distribution models and cloud infrastructure providers, is radically transforming the nature of content delivery in today's Internet. These changes are profoundly impacting how major Internet players (e.g., ISPs, CDNs, web hosting companies, and content providers) operate in a dynamic environment and do business in an increasingly competitive marketplace. However, carefully tracking these developments to obtain an accurate picture of how these key players shape and impact the Internet and its traffic has become an increasingly daunting task. Past attempts to paint such a picture had limited success due to piecing together incomplete and often inaccurate information from various sources of varying quality or using proprietary datasets or hard-to-justify estimates of difficult-to-measure quantities (e.g., inter-AS traffic matrices).

A main reason for our current inability to accurately track a constantly changing Internet is the lack of global vantage points where traffic from a sufficiently large portion of the Internet can be observed at a fine-grained level to discern the makeup of today's Internet traffic and the interactions of the responsible parties. This raises the question of whether such vantage points exist in today's Internet and, if so, what exactly they enable us to discern about the Internet as a whole and its individual constituents.

### Main Contribution (Part 1)

The first finding reported in this paper is that some of the largest Internet Exchange Points (IXPs) in Europe (i.e., AMS-IX in Amsterdam, DE-CIX in Frankfurt, LINX in London) serve as the kind of vantage points we are looking for. Our detailed analysis of 17 contiguous weeks of complete sFlow measurements from one of the largest European IXPs reveals that, in addition to being a critical part of the European portion of the Internet, this IXP also plays a global role, "seeing" traffic from a large fraction of the Internet. Specifically, week-in and week-out, our IXP "sees" traffic from all 42,000+ routed ASes, almost all 450,000+ routed prefixes, from about 1.5 million servers, and around a quarter billion IP addresses from all countries around the globe. Importantly, the fact that these largest European IXPs all operate in a very similar fashion, handle comparable traffic volumes, and have similar profiles with respect to members and service offerings allows us to conclude that any one of them fits the role of being a global Internet vantage point.

However, like all vantage points used in practice, visibility from these large European IXPs into the Internet is not perfect. Thus, knowing what we cannot discern about aspects of the Internet from mining the traffic seen at one of these vantage points is as important as knowing what we can discern. To address this, we consider a variety of different IXP-external measurements that complement the IXP's own data, check or validate our findings based on the IXP's data alone, or help us determine what aspects are impossible to discern from this vantage point and why.

### Main Contribution (Part 2)

Our second main finding illustrates the new opportunities or benefits for Internet measurements that arise from having access to vantage points with such good visibility into the Internet. By focusing on the analysis of the web server-related portion of the traffic "seen" by our IXP and applying an original methodology for identifying server-based infrastructures, classifying them by ownership (e.g., company, organization), and associating traffic with them, we show that many of the major commercial Internet players are actively contributing to a clearly discernible trend towards network heterogenization. That is, many of these players' networks are undergoing major changes as a result of business decisions that either favor the hosting of servers from third-party networks within their own network infrastructures or the deployment of their own servers, often in massive numbers, in purposefully-selected third-party networks.

On the one hand, this finding confirms what is well-documented in press releases and technology blogs but remains largely under-reported in the networking research literature. For example, it is well-known that CDN companies like Akamai are accelerating the deployment of their own servers in eyeball networks by forming strategic content delivery alliances with large ISPs. Similarly, large content providers like Netflix are installing their own single-purpose CDNs (e.g., Netflix's Open Connect) inside various regional and local ISPs to enable direct delivery of Netflix video data to end users. Other key companies like Google, Amazon, or Facebook have followed suit, further adding to the complexities that result from this proliferation of intertwined network infrastructures and traffic.

On the other hand, by providing a methodology for discovering an organization's servers, whether they are deployed within the organization's own AS (or ASes) or inside some third-party network's infrastructure, we enable and advocate a measurement-driven approach to inferring and assessing the extent to which individual network infrastructures or the traffic they carry are heterogeneous. The results of our study show that network heterogenization is widespread and not just confined to well-known players like Akamai or Google. We also study the impact of this finding on the usage of peering links at IXPs and AS-links in the larger Internet. Specifically, we show that AS-link usage is becoming more heterogeneous, and attributing traffic to the party responsible for it faces enormous challenges due to the complexities resulting from the observed proliferation of intertwined network infrastructures and traffic. These findings argue that future attempts to accurately and meaningfully study the business strategies of various Internet constituents (e.g., ISPs, content providers, CDNs, IXPs, networks without an ASN, resellers, etc.) and the business relationships among them must move beyond the traditional and largely traffic-agnostic AS-level view of the Internet. They must account for the complexities that result from today's Internet realities and be more concerned with how traffic flows across the network than with how the network is connected at the AS level.

### Head-on Comparison with [13]

The large European IXP considered in this paper also featured prominently in the recent work by Ager et al. [13]. However, while that study focused on the discovery of a surprisingly rich peering fabric among the member ASes of that IXP, in this paper, we are mainly concerned with mining the traffic seen at this IXP to determine the IXP's visibility into the Internet. Put differently, while [13] exploited the IXP measurements to obtain an accurate picture of the "inside" of the IXP (i.e., its member ASes, their peerings, and the IXP-specific traffic matrix), this paper mines recent traffic data to obtain a view of the "outside" of the IXP; that is, the larger Internet beyond the boundary formed by the members of the IXP. In terms of results, while [13] highlighted the severe level of incompleteness of commonly-studied AS maps of the Internet, this paper establishes and provides concrete evidence for why and in what sense this traditional AS-level view—although still useful for exploring and understanding various connectivity- or reachability-related aspects—is largely inept for accounting for critical elements of the networks that make up today's Internet. Thus, representing two largely complementary efforts, the combined findings of [13] and this paper take the observations of the study by Labovitz et al. [41] to the next level.

In the process, we identify and outline an alternative and largely orthogonal perspective to the traditional AS-level view that centers around organizations or companies and their server-based infrastructures, which are spread across many networks and countries and defy traditional network and geographic boundaries.

### Roadmap

The remainder of the paper is organized as follows. In Section 2, we describe the available IXP measurements and report on how we extract the relevant traffic data for our study. In Section 3, we focus on our first finding that the IXP defines a vantage point that offers unprecedented visibility into the Internet, and we support this finding with concrete results based on a week's worth of traffic. Our results are complemented in Section 4 by a longitudinal analysis, and we discuss what measurements taken at this single vantage point for a 17-week period reveal about the Internet. In Section 5, we describe our methodology for grouping an organization's servers, irrespective of their location within the Internet, and report on our second main finding; that is, evidence for and extent of network heterogenization. We continue in Section 6 with a discussion of related and future work and conclude with a summary of our main observations in Section 7.

## 2. IXP as a Rich Data Source

In this section, we describe the IXP measurements that are at our disposal for this study and sketch and illustrate the basic methodology we use to identify the traffic components relevant for our work. We also list and comment on the different IXP-external datasets that we rely on throughout this paper to show what we can and cannot discern from the IXP-internal measurements alone.

### 2.1 Available IXP-Internal Datasets

The work reported in this paper is based on traffic measurements collected between August 27 (beginning of week 35) and December 23 (end of week 51) of 2012 at one of the largest IXPs in Europe. At the beginning of the measurement period in week 35, this IXP had 443 member ASes that exchanged on average 11.9 PB of traffic per day over the IXP's public peering infrastructure (i.e., a layer-2 switching fabric distributed over a number of data centers within the city where the IXP is located). During the measurement period, the IXP added between 1-2 members per week. Specifically, the measurements we rely on consist of 17 consecutive weeks of uninterrupted anonymized sFlow records that contain Ethernet frame samples collected using a random sampling of 1 out of 16,000. sFlow captures the first 128 bytes of each sampled frame, implying that in the case of IPv4 packets, the available information consists of the full IP and transport layer headers and 74 and 86 bytes of TCP and UDP payload, respectively. For further details about the IXP infrastructure itself as well as the collected sFlow measurements (e.g., absence of sampling bias), we refer to [13]. In the following, we use our week 45 data to illustrate our method. The other weekly snapshots produce very similar results and are discussed in more detail in Section 4.

### 2.2 Methods for Dissecting the IXP’s Traffic

#### 2.2.1 Peering Traffic

Figure 1 details the filtering steps that we applied to the raw sFlow records collected at this IXP to obtain what we refer to as the "peering traffic" component of the overall traffic. As shown in Figure 1, after removing from the overall traffic, in succession, all non-IPv4 traffic (i.e., native IPv6 and other protocols; roughly 0.4% of the total traffic, most of which is native IPv6), all traffic that is either not member-to-member or stays local (e.g., IXP management traffic; about 0.6%), all member-to-member IPv4 traffic that is not TCP or UDP (i.e., ICMP and other transport protocols; less than 0.5%), this peering traffic makes up more than 98.5% of the total traffic. As an interesting by-product, we observe that 82% of the peering traffic is TCP and 18% is UDP.

#### 2.2.2 Web Server-Related Traffic

We next identify the portion of the peering traffic that can be unambiguously identified as web server-related traffic. Our motivation is that web servers are generally considered to be the engines of e-commerce, which in turn argues that web server-related traffic is, in general, a good proxy for the commercial portion of Internet traffic. Accordingly, we focus on HTTP and HTTPS and describe the filtering steps for extracting their traffic.

To identify HTTP traffic, we rely primarily on commonly-used string-matching techniques applied to the content of the 128 bytes of each sampled frame. We use two different patterns. The first pattern matches the initial line of request and response packets and looks for HTTP method words (e.g., GET, HEAD, POST) and the words HTTP/1.{0,1}. The second pattern applies to header lines in any packet of a connection and relies on commonly used HTTP header field words as documented in the relevant RFCs and W3C specifications (e.g., Host, Server, Access-Control-Allow-Methods). Using these techniques enables us to identify which of the IP endpoints act as servers and which ones act as clients. When applied to our week 45 data, we identify about 1.3 million server IPs together with roughly 40 million client IPs. Checking the port numbers, we verify that more than 80% of the server IPs use the expected TCP ports, i.e., 80 and 8080. Some 5% of them also use 1935 (RTMP) as well as 443 (HTTPS). Note that by relying on string-matching, we miss those servers for which our sFlow records do not contain sufficient information; we also might mis-classify as clients some of those servers that "talk" with other servers and for which only their client-related activity is captured in our data.

With respect to HTTPS traffic, since we cannot use pattern matching directly due to encryption, we use a mixed passive and active measurement approach. In the first step, we use traffic on TCP port 443 to identify a candidate set of IPs of HTTPS servers. Here, we clearly miss HTTPS servers that do not use port 443, but we consider them not to be commercially relevant. However, given that TCP port 443 is commonly used to circumvent firewalls and proxy rules for other kinds of traffic (e.g., SSH servers or VPNs running TCP port 443), in the second step, we rule out non-HTTPS-related use by relying on active measurements. For this purpose, we crawl each IP in our candidate set for an X.509 certificate chain and check the validity of the returned X.509 certificates. For those IPs that pass the checks of the certificate, we extract the names for which the X.509 certificate is valid and the purpose for which it was issued. In particular, we check the following properties in each retrieved X.509 certificate: (a) certificate subject, (b) alternative names, (c) key usage (purpose), (d) certificate chain, (e) validity time, and (f) stability over time. If a certificate does not pass any of the tests, we do not consider it in the analysis.

We keep only the IPs that have a certificate subject and alternative names with valid domains and also valid country-code second-level domains (ccSLD) according to the definition in [35]. Next, we check if the key usage explicitly indicates a web server role. In the certificate chain, we check if the delivered certificates do really refer to each other in the right order up to the root certificate, which must be contained in the current Linux/Ubuntu white-list. Next, we verify the validity time of each certificate in the chain by comparing it to the timestamp when the certificate fetching was performed. Lastly, we perform the active measurements several times and check for changes because IPs in cloud deployments can change their role very quickly and frequently. Ignoring validity time, we require that all the certificates fetched from a single IP have the same properties. In the case of our week 45 data, starting with a candidate set of approximately 1.5 million IPs, some 500,000 respond to repeated active measurements, of which 250,000 are in the end identified as HTTPS server IPs.

When combined, these filtering steps yield approximately 1.5 million different web server IPs (including the 250,000 HTTPS server IPs). In total, these HTTP and HTTPS server IPs are responsible for or "see" more than 70% of the peering traffic portion of the total traffic. Some 350,000 of these IP addresses appear in both sets and are examples of multi-purpose servers; that is, servers with one IP address that see activity on multiple ports. Multi-purpose servers are popular with commercial Internet players (e.g., Akamai, which uses TCP port 80 (HTTP) and TCP port 1935 (RTMP)), and their presence in our data partially explains why we see a larger percentage of web server-related traffic than what is typically reported in the literature [33, 36, 43], but is often based on a strictly port-based traffic classification [30, 41].

Among the identified HTTP and HTTPS server IPs, we find some 200,000 IPs that act both as servers and as clients. These are responsible for some 10% of the server-related traffic. Upon closer inspection of the top contributors in this category, we find that they typically belong to major CDNs (e.g., EdgeCast, Limelight) or network operators (e.g., Eweka). Thus, the large traffic share of these servers is not surprising and reflects typical machine-to-machine traffic associated with operating, for example, a CDN. Another class of IPs in this category are proxies or clients that are managed via a server interface (or vice versa).

In the context of this paper, it is important to clarify the notion of a server IP. Throughout this paper, a server IP is defined as a publicly routed IP address of a server. As such, it can represent many different real-world scenarios, including a single (multi-purpose) server, a rack of multiple servers, or a front-end server acting as a gateway to possibly thousands of back-end servers (e.g., an entire data center). In fact, Figure 2 shows the traffic share of each server IP seen in the week 45 data. It highlights the presence of individual server IPs that are responsible for more than 0.5% of all server-related traffic! Indeed, the top 34 server IPs are responsible for more than 6% of the overall server traffic. These server IPs cannot be single machines. Upon closer examination, they are identified as belonging to a cast of Internet players that includes CDNs, large content providers, streamers, virtual backbone providers, and resellers, and thus represent front-end servers to large data centers and/or anycast services. Henceforth, we use the term "server" to refer to a server IP as defined above.

### 2.3 Available IXP-External Datasets

When appropriate and feasible, we augment our IXP-based findings with active and passive measurements that do not involve the IXP in any form or shape and are all collected in parallel to our IXP data collection. Such complementary information allows us to verify, check, or refine the IXP-based findings.

One example of a complementary IXP-external dataset is a proprietary dataset from a large European Tier-1 ISP consisting of packet-level traffic traces. With the help of the network intrusion detection system Bro [47], we produce the HTTP and DNS logs, extract the web server-related traffic and the corresponding server IPs from the logs, and rely on the resulting data in Section 3.

For another example, we use the list of the top 280,000 DNS recursive resolvers—as seen by one of the largest commercial CDNs—as a starting set to find a highly distributed set of DNS resolvers that are available for active measurements such as doing reverse DNS lookups or performing active DNS queries. From this initial list of DNS servers, we eliminate those that cannot be used for active measurements (i.e., those that are not open, delegate DNS resolutions to other resolvers, or provide incorrect answers) and end up with a final list of about 25,000 DNS resolvers in some 12,000 ASes that are used for active measurements in Section 3.3.

Other examples of IXP-external data we use in this work include the publicly available lists of the top-1 million or top-1,000 popular websites that can be downloaded from www.alexa.com. We obtained these lists for each of the weeks for which we have IXP data. We also utilized blogs and technical information found on the official websites of various technology companies and Internet players. In addition, we make extensive use of publicly available BGP-based data that is collected on an ongoing basis by RouteViews [8], RIPE RIS [7], Team Cymru [9], etc.

### 2.4 IP Server Meta-Data

Our efforts in Section 5 rely on certain meta-data that we collect for server IPs and that is obtained from DNS information, URIs, and X.509 certificates from HTTPS servers.

Regarding DNS information, obvious meta-information is the hostname(s) of a server IP. This information is useful because large organizations [38] often follow industrial standards in their naming schemas for servers that they operate or host in their own networks. Another useful piece of meta-data is the Start of Authority (SOA) resource record, which relates to the administrative authority and can be resolved iteratively. This way, one can often find a common root for organizations that do not use a unified naming schema. Note that the SOA record is often present, even when there is no hostname record available or an ARPA address is returned in the reverse lookup of a server IP.

Next, the URI as well as the authority associated with the hostname give us hints regarding the organization that is responsible for the content. For example, for the URI youtube.com, one finds the SOA resource record google.com and thus can associate YouTube with Google.

Lastly, the X.509 certificates reveal several useful pieces of meta-data. First, they list the base set of URIs that can be served by the corresponding server IP. Second, some server IPs have certificates with multiple names that can be used to find additional URIs. This is typically the case for hosting companies that host multiple sites on a single server IP. In addition, it is used by CDNs that serve multiple different domains with the same physical infrastructure. Moreover, the names found in the certificates can be mapped to SOA resource records as well.

Overall, we are able to extract DNS information for 71.7%, at least one URI for 23.8%, and X.509 certificate information for 17.7% of the 1.5 million server IPs that we see in our week 45 data. For 81.9% of all the server IPs, we have at least one of the three pieces of information. For example, for streamers, one typically has no assigned URI, but information from DNS. Before using this rich meta-data in Section 5, we clean it by removing non-valid URIs, SOA resource records of the Regional Internet Registries (RIRs) such as ripe.net, etc. This cleaning effort reduces the pool of server IPs by less than 3%.

## 3. Local Yet Global

The main purpose of this section is to show that our IXP represents an intriguing vantage point, with excellent visibility into the Internet as a whole. This finding of the IXP's important global role complements earlier observations that have focused on the important local role that this large European IXP plays for the greater geographic region where it is located [13], and we further elaborate here on its dual role as a local and as a global player. Importantly, we also discuss what we can and cannot discern about the Internet as a whole or its individual constituents based on measurements taken at this vantage point. The reported numbers are for the week 45 measurements when the IXP had 452 members that exchanged some 14 PB of traffic per day and are complemented by a longitudinal analysis in Section 4.

### 3.1 On the Global Role of the IXP

By providing a well-defined set of steps and requirements for establishing peering links between member networks, IXPs clearly satisfy the main reason for why they exist in the first place—keeping local traffic local. To assess the visibility into the global Internet that comes with using a large European IXP as a vantage point, we focus on the peering traffic component (see Section 2.2.1) and summarize in Table 1 the pertinent results.

First, in this single geographically well-localized facility, we observe during a one-week period approximately a quarter billion unique IPv4 addresses (recall that the portion of native IPv6 traffic seen at this IXP is negligible). While the total number of publicly routed IPv4 addresses in the Internet in any given week is unknown, it is some portion of the approximately three and a half billion allocated IPv4 addresses, which suggests that this IXP "sees" a significant fraction of the ground truth. The global role of this IXP is further illuminated by geo-locating all 230 million+ IP addresses at the country-level granularity [49] and observing that this IXP "sees" traffic from every country of the world, except for places such as Western Sahara, Christmas Islands, or Cocos (Keeling) Islands. This ability to see the global Internet from this single vantage point is visualized in Figure 3, where the different countries' shades of gray indicate which percentage of IPs a given country contributes to the IPs seen at this IXP.

Second, when mapping the encountered 230 million+ IP addresses to more network-specific entities such as subnets or prefixes and ASes, we confirm the IXP's ability to "see" the Internet. More precisely, in terms of subnets/prefixes, this IXP "sees" traffic from 445,000 subnets; that is, from essentially all actively routed prefixes. Determining the precise number of actively routed prefixes in the Internet in any given week remains an imprecise science as it depends on the publicly available BGP data that are traditionally used in this context (e.g., RouteViews, RIPE). The reported numbers vary between 450,000-500,000 and are only slightly larger than the 445,000 subnets we see in this one week. With respect to ASes, the results are very similar—the IXP "sees" traffic from some 42,800 actively routed ASes, where the ground truth for the number of actively routed ASes in the Internet in any given week is around 43,000 [3] and varies slightly with the used BGP dataset.

Lastly, to examine the visibility that this IXP has into the more commercial-oriented Internet, we next use the web server-related component of the IXP's peering traffic (see Section 2.2.2). Table 1 shows that this IXP "sees" server-related traffic from some 1.5 million IPs that can be unambiguously identified as web server IPs. Unfortunately, we are not aware of any numbers that can be reliably considered as ground truth of all server IPs in the Internet in any given week. Even worse, available white papers or reports that purportedly provide this information are typically very cavalier about their definition of what they consider as "web server" and hence cannot be taken at face value [40, 50].

To indirectly assess how the roughly 1.5 million web server IPs seen at this IXP stack up against the unknown number of web server IPs Internet-wide, we use an essentially orthogonal dataset, namely the HTTP and DNS logs from a large European Tier-1 ISP that does not exchange traffic over the public switching infrastructure of our IXP. Applying the method as described in Section 2, we extract the web server IPs from this ISP dataset and find that of the total number of server IPs that are "seen" by this ISP, only some 45,000 are not seen at the IXP. Importantly, for the server IPs seen both at the IXP and the ISP, those we identified as server IPs using the IXP-internal data are confirmed to be indeed server IPs when relying on the more detailed ISP dataset.

In any case, mapping the 1.5 million server IPs from the IXP to prefixes, ASes, and countries shows that this IXP "sees" server-traffic from some 17% of all actively routed prefixes, from about 50% of all actively routed ASes, and from about 80% of all the countries in the world.

### 3.2 On the IXP’s Dual Role

Visuals such as Figure 3 illustrate that by using this IXP as a vantage point, we are able to see peering traffic from every country and corner of the world or from almost every AS and prefix that is publicly routed. However, such figures do not show whether or not certain countries or corners and ASes or prefixes are better visible than others. To address this, we analyze the distribution of traffic across different countries, ASes, and prefixes.

Table 2 shows the top 10 contributors to the peering traffic at the IXP, categorized by server IPs and network. The table highlights the dominance of certain players, such as Akamai, 1&1, OVH, and ChinaNet, in the traffic seen at the IXP. This dominance is consistent across different metrics, including the number of server IPs, the amount of traffic, and the geographical distribution.

In summary, the IXP serves as a vantage point that provides both local and global visibility into the Internet. While it sees traffic from a broad and diverse set of sources, the visibility is not uniform, and certain players and regions are more prominent in the traffic observed. This dual role of the IXP as both a local and global vantage point underscores its importance in understanding the structure and dynamics of the Internet.

## 4. Longitudinal Analysis

To complement the findings from a single week, we perform a longitudinal analysis of the 17-week period of IXP data. This analysis helps us understand the stability and trends in the visibility and traffic patterns observed at the IXP over an extended period.

### 4.1 Stability of Visibility

One of the key questions in our longitudinal analysis is the stability of the visibility provided by the IXP. We examine the consistency of the number of unique IP addresses, ASes, and prefixes seen over the 17-week period. Our results show that the IXP consistently "sees" a large and stable fraction of the Internet, with minor fluctuations week-to-week. For example, the number of unique IP addresses seen each week remains around a quarter billion, with variations of less than 5%. Similarly, the number of ASes and prefixes seen each week is also stable, with the IXP consistently "seeing" traffic from all 42,000+ ASes and almost all 450,000+ prefixes.

### 4.2 Trends in Traffic Patterns

We also analyze the trends in the traffic patterns observed at the IXP over the 17-week period. One of the key trends we observe is the increasing prominence of certain players, particularly CDNs and large content providers. For example, the traffic share of Akamai, Google, and other major CDNs increases over the 17-week period, reflecting the growing importance of these players in the Internet ecosystem. Additionally, we observe a steady increase in the number of multi-purpose servers, indicating a trend towards more versatile and flexible server infrastructures.

### 4.3 Geographical Distribution

The longitudinal analysis also provides insights into the geographical distribution of the traffic seen at the IXP. We find that the IXP consistently "sees" traffic from a wide range of countries, with the top contributors remaining relatively stable over the 17-week period. However, we also observe some shifts in the relative contributions of different countries, reflecting changes in the global Internet landscape. For example, the traffic from emerging markets, such as China and India, shows a steady increase, while the traffic from more established markets, such as the United States and Europe, remains stable.

### 4.4 Impact of New Members

During the 17-week period, the IXP added between 1-2 new members per week. We analyze the impact of these new members on the visibility and traffic patterns observed at the IXP. Our results show that the addition of new members has a minimal impact on the overall visibility and traffic patterns. The new members contribute a small fraction of the total traffic, and their addition does not significantly alter the global visibility provided by the IXP.

In summary, the longitudinal analysis confirms the stability and consistency of the visibility provided by the IXP over an extended period. The analysis also highlights the growing importance of CDNs and large content providers, as well as the increasing traffic from emerging markets. These findings underscore the IXP's role as a reliable and comprehensive vantage point for studying the Internet.

## 5. Methodology for Grouping Organizational Servers

In this section, we describe our methodology for grouping an organization's servers, irrespective of their location within the Internet. This methodology is crucial for understanding the extent of network heterogenization and the complex interplay of different network infrastructures and traffic.

### 5.1 Data Collection and Preprocessing

The first step in our methodology involves collecting and preprocessing the data. We start with the server IPs identified in the previous sections and gather additional meta-data, such as DNS information, URIs, and X.509 certificates. This meta-data provides valuable information about the ownership and function of the servers. We then clean and preprocess the data to remove any inconsistencies or errors. For example, we remove non-valid URIs and filter out SOA resource records of the Regional Internet Registries (RIRs) such as ripe.net.

### 5.2 Clustering and Grouping

The next step is to cluster and group the server IPs based on the collected meta-data. We use a combination of string-matching and clustering algorithms to identify groups of server IPs that are likely to belong to the same organization. For example, we use the hostname and SOA resource records to identify common roots and group server IPs that share the same administrative authority. We also use the X.509 certificates to identify server IPs that serve the same set of URIs or are part of the same physical infrastructure.

### 5.3 Validation and Refinement

Once the initial clustering is complete, we validate and refine the results. We use additional data sources, such as the HTTP and DNS logs from the large European Tier-1 ISP, to cross-validate the clustering. We also perform active measurements, such as reverse DNS lookups and active DNS queries, to further refine the clustering. This validation and refinement step ensures that the grouped server IPs accurately reflect the organizational structure and infrastructure of the Internet players.

### 5.4 Results and Findings

Using our methodology, we are able to group the server IPs into clusters that correspond to different organizations. Our results show a clear trend towards network heterogenization, with many major Internet players deploying their servers in a wide range of networks and locations. For example, we find that CDNs like Akamai and content providers like Google and Netflix are deploying their servers in multiple third-party networks, often in large numbers.