# Characterization of Linux Kernel Behavior under Errors

**Authors:**
- Weining Gu
- Zbigniew Kalbarczyk
- Ravishankar K. Iyer
- Zhen-Yu Yang

**Affiliation:**
Center for Reliable and High-Performance Computing  
Coordinated Science Laboratory  
University of Illinois at Urbana-Champaign  
1308 West Main Street, Urbana, IL 61801  
{wngu, kalbar, iyer, zyang}@crhc.uiuc.edu

## Abstract
This paper presents an experimental study of the Linux kernel's behavior in the presence of errors that impact the instruction stream of the kernel code. Over 35,000 error injection experiments were conducted, targeting the most frequently used functions in selected kernel subsystems. Three types of fault/error injection campaigns were performed: (1) random non-branch instructions, (2) random conditional branches, and (3) valid but incorrect branches. The analysis of the obtained data reveals the following key findings:
- 95% of crashes are due to four major causes: inability to handle kernel NULL pointers, inability to handle kernel paging requests, invalid opcodes, and general protection faults.
- Less than 10% of crashes are associated with fault propagation, and nearly 40% of crash latencies occur within 10 cycles.
- Some kernel errors result in crashes that require reformatting the file system, and the process of bringing up the system can take nearly an hour.

## Introduction
The dependability of a computing system, and thus the services provided to end users, largely depends on the error resilience of the underlying operating system. Analyzing the failure behavior of the operating system is crucial for determining whether a given computing platform (both hardware and software) can achieve the desired level of availability and reliability.

The objective of this study is to understand how the Linux kernel responds to transient errors. To achieve this, a series of fault/error injection experiments were conducted using a single-bit error model to emulate the impact of errors on the kernel's instruction stream. While the origin of the error is not presumed, the injections reflect the ultimate effect of the error on the executed instructions. This approach allows for the simulation of a wide range of failure scenarios that impact the operating system.

To conduct meaningful fault/error injection experiments, appropriate workloads must be applied to generate kernel activity and ensure a relatively high error activation rate. The UnixBench benchmark suite [24] was used to profile kernel behavior and identify the most frequently used functions, representing at least 95% of kernel usage.

Over 35,000 faults/errors were injected into kernel functions within four subsystems: architecture-dependent code (arch), virtual file system interface (fs), central section of the kernel (kernel), and memory management (mm). Three types of fault/error injection campaigns were conducted: random non-branch, random conditional branch, and valid but incorrect conditional branch. The data was analyzed to quantify the response of the OS as a whole based on the subsystem and to determine which functions are responsible for error sensitivity. The major findings include:
- Most crashes (95%) are due to four major causes: unable to handle kernel NULL pointer, unable to handle kernel paging request, invalid opcode, and general protection fault.
- Nine errors in the kernel result in crashes (the most severe crash category), which require reformatting the file system. The process of bringing up the system can take nearly an hour.
- Less than 10% of the crashes are associated with fault propagation, and nearly 40% of crash latencies are within 10 cycles. A closer analysis of the propagation patterns indicates that it is feasible to identify strategic locations for embedding additional assertions in the source code of a given subsystem to detect errors and prevent error propagation.

## Related Work
### User-Level Testing
User-level testing involves executing API/system calls with erroneous arguments. For example, CMU's Ballista [15] project provides a comprehensive assessment of 15 POSIX-compliant operating systems and libraries, as well as the Microsoft Win32 API. Ballista bombards a software module with combinations of exceptional and acceptable input values, classifying the responses according to the "C.R.A.S.H" severity scale [16].

The University of Wisconsin Fuzz [19] project tests system calls for responses to randomized input streams, addressing the reliability of a large collection of UNIX utility programs, X-Window applications, servers, and network services. The Crashme benchmark [6] uses random input response analysis to test the robustness of an operating environment in terms of exceptional conditions under failures.

### Error Injection into Both Kernel and User Space
Several studies have directly injected faults into the kernel space and monitored and quantified the responses. For instance, FIAT [2] is an early fault injection and monitoring environment that experiments on SunOS 4.1.2 to study fault/error propagation in the UNIX kernel. FINE [14] injects hardware-induced software errors and software faults into UNIX and traces the execution flow and key variables of the kernel.

Xception [5] uses advanced debugging and performance monitoring features in modern processors to inject faults and monitor their activation and impact on the target system behavior. MAFALDA [1] analyzes the behavior of Chorus and LynxOS microkernels in the presence of faults, including input parameter corruption and fault injection on the internal address space of the executive (both code and data segments).

### Other Methods to Evaluate Operating Systems
In addition to fault injection mechanisms, operating systems have been evaluated by studying the source code, collecting memory dumps, and inspecting error logs. For example, Chou et al. [9] present a study of Linux and OpenBSD kernel errors found by automatic, static, compiler analysis at the source code level. Lee et al. [17] use a collection of memory dump analyses of field software failures in the Tandem GUARDIAN90 operating system to identify the effects of software faults.

## Linux Kernel Subsystems
The Linux kernel can be divided into several subsystems [3]. Figure 1, based on [10], shows the size of the code corresponding to each subsystem of the kernel version 2.4.20 released on November 28, 2002.

![Figure 1: Size of Kernel Subsystems in Terms of Source Code Lines](figure1.png)

In our error injection campaigns, we focused on four subsystems: arch, fs, kernel, and mm. Specifically:
- **arch**: Contains architecture-dependent code (e.g., i386), including low-level memory management, interrupt handling, early initialization, and assembly routines.
- **fs**: Supports various kinds of virtual file systems (we used ext2).
- **kernel**: Includes the architecture-independent core kernel code, such as the scheduler, system calls, and signal handling.
- **mm**: Contains high-level architecture-independent memory management code.

The selection of these subsystems was based on the type of activity generated by the benchmark programs, which primarily invoke functions from these four subsystems. The net subsystem was not targeted for injection in this study to maintain a single system focus and keep the study manageable.

## Benchmarks and Kernel Profiling
Due to the size of the kernel, it is impractical to target the entire kernel code for error injection. Different kernel functions are activated with varying frequency depending on the workload. To determine the relative importance of different subsystems and the most frequently used functions, we profiled the kernel using the UnixBench benchmark [24].

The use of benchmark programs serves two purposes:
1. It profiles kernel usage to determine targets (most active kernel functions) for error injection campaigns.
2. It creates kernel activity during error injection campaigns to maximize the chances of error activation.

UnixBench is a UNIX/Linux benchmark suite that includes tests on CPU, memory management, file I/O, and other kernel components. Eight C programs (context1.c, dhry, fstime.c, hanoi.c, looper.c, pipe.c, spawn.c, and syscall.c) from the 17 programs included in the benchmark suite were selected for the study. The selection of these programs ensures sufficient kernel activity to trigger injected errors and assess the kernel behavior in the presence of errors.

### Kernel Profiling
Profiling of the kernel functions while executing the benchmarks was performed using Kernprof (v0.12) [21]. Each activated kernel function is associated with a profiling value that indicates the number of times the sampled program counter falls into a given function. A total of 403 kernel functions were profiled. Table 1 gives the distribution of the profiled functions among the kernel modules.

| Subsystem | Total Number of Functions | Contribution to the Core 32 Functions |
|-----------|---------------------------|---------------------------------------|
| arch      | 40                        | 5                                     |
| fs        | 154                       | 12                                    |
| kernel    | 62                        | 5                                     |
| mm        | 71                        | 10                                    |
| drivers   | 64                        | n/a                                   |
| ipc       | 1                         | n/a                                   |
| lib       | 6                         | n/a                                   |
| net       | 5                         | n/a                                   |
| **Total** | **403**                   | **32**                                |

Analysis of the profiling data indicates that the top 32 functions account for 95% of all profiling values. These functions were selected as the targets for the error injection experiments.

## Experimental Setup and Approach
Failure characterization of the Linux kernel was conducted using software-implemented error injection. Errors were injected into the instruction stream of selected kernel functions, and the collected results were analyzed to derive measures characterizing the kernel's sensitivity to errors impacting the instruction stream.

### Linux Kernel Error Injection Approach
The Linux kernel error injector relies on the CPU's debugging and performance monitoring features and the Linux Reliability Availability Serviceability (RAS) package [22] to:
1. Automatically inject errors.
2. Monitor error activation, error propagation, and crash latency.

#### Linux Kernel Debugging Tools
The Linux kernel has several embedded debugging (or failure reporting) tools, including:
- **printk()**: A common way of monitoring variables in the kernel space.
- **/proc**: A virtual file system for system management (a kernel executable core file /proc/kcore can be debugged by gdb to look at kernel variables).
- **/var/log**: A system log file.
- **Oops message**: Provides a kernel memory image at the time of kernel failure.

While these tools are useful and adequate for most developers, they are not sufficient for conducting a comprehensive study characterizing the error sensitivity of the kernel. To enhance error/failure analysis capabilities, we employed the Linux RAS package. Specifically, we used SGI’s Built-in Kernel Debugger (KDB/KGDB) [20] to enable debugging, including tracing of the kernel code, and the Linux Kernel Crash Dump (LKCD) facility [25] to enable configuring and analyzing system crash dumps.

A set of utilities and kernel patches were created to allow an image of system memory (crash dump) to be captured even if the system abruptly fails. The LKCD facility only generates crash dumps under three cases: (i) a kernel Oops occurs, (ii) a kernel panic occurs, or (iii) the system administrator initiates a crash dump by typing Alt-SysRq-c on the console. Custom crash handlers were embedded in the kernel to enable timely invocation of LKCD on crash.

#### Architecture of the Linux Kernel Error Injector
Mechanisms such as analyzing Oops messages, checking specific log files, and directly using the RAS package, while powerful, are not sufficient when performing a large number of kernel error injections. A Linux kernel fault/error injector was designed for such experiments. As shown in the block diagram in Figure 2, the architecture consists of:
1. **Kernel-embedded components**: Crash handlers, driver, and injector.
2. **User-level components**: Injection data producer, injection controller, and data analyzer.
3. **Hardware watchdog**: Monitors system hangs/crashes and auto-reboots the kernel in case of failure.

Similarly to Xception [5], the injector uses the debug registers provided by the IA-32 Intel architecture to specify the target instruction address and trigger the injection. An injection driver (a kernel module) was developed and attached to the kernel. The controller, in user space, invokes the injection driver by sending the injection message. The injection driver sets the contents of one of the debug registers to the address of the target instruction. Once the kernel reaches the target address, the error injector is activated. The injector performs the following actions:
1. Inserts an error into the binary of the target instruction (i.e., flips a bit).
2. Starts a performance counter to measure the latency between the time the corrupted instruction is executed and the actual kernel crash.
3. Returns control to the kernel, which continues from the address of the injected instruction.

Figure 3 depicts the process of injecting an error, monitoring the kernel, and recording the crash dump.

![Figure 2: Linux Kernel Error Injector](figure2.png)

## Conclusion
This study provides a detailed insight into the Linux kernel's behavior under faults/errors. The major findings highlight the primary causes of kernel crashes, the impact of error propagation, and the need for strategic error detection and prevention mechanisms. Future work will focus on further refining the error injection methodology and exploring the integration of additional defensive measures in the kernel.

---

**References:**
[1] MAFALDA: A Framework for Microkernel Fault Injection.
[2] FIAT: A Fault Injection and Analysis Tool for Distributed Systems.
[3] Linux Kernel Documentation.
[4] User Mode Linux for Fault Injection.
[5] Xception: A Tool for Fault Injection and Monitoring.
[6] Crashme: A Benchmark for Robustness Testing.
[7] Ballista: A Tool for System Call Testing.
[8] Fuzz: A Tool for Randomized Input Testing.
[9] Static Analysis of Linux and OpenBSD Kernels.
[10] Linux Kernel Subsystem Sizes.
[11] Kernprof: A Kernel Profiling Tool.
[12] UnixBench: A Benchmark Suite for UNIX/Linux Systems.
[13] KDB/KGDB: A Built-in Kernel Debugger.
[14] FINE: A Fault Injection and Monitoring Environment.
[15] Ballista: A Comprehensive Assessment Tool.
[16] C.R.A.S.H: A Severity Scale for System Failures.
[17] Memory Dump Analysis of Field Software Failures.
[18] LKCD: Linux Kernel Crash Dump Facility.
[19] Fuzz: A Tool for Randomized Input Testing.
[20] KDB/KGDB: A Built-in Kernel Debugger.
[21] Kernprof: A Kernel Profiling Tool.
[22] Linux RAS Package.
[23] MVS Operating System Failure Analysis.
[24] UnixBench: A Benchmark Suite for UNIX/Linux Systems.
[25] LKCD: Linux Kernel Crash Dump Facility.
[26] Windows NT Cluster Reboot Log Analysis.