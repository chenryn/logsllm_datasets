title:Characterization of Linux Kernel Behavior under Errors
author:Weining Gu and
Zbigniew Kalbarczyk and
Ravishankar K. Iyer and
Zhen-Yu Yang
Characterization of Linux Kernel Behavior under Errors 
Weining Gu, Zbigniew Kalbarczyk, Ravishankar K. Iyer, Zhenyu Yang 
Center for Reliable and High-Performance Computing  
Coordinated Science Laboratory 
University of Illinois at Urbana-Champaign 
1308 West Main Street, Urbana, IL 61801 
{wngu, kalbar, iyer, zyang }@crhc.uiuc.edu
Abstract. This  paper  describes  an  experimental  study  of  Linux 
kernel behavior in the presence of errors that impact the instruction 
stream  of  the  kernel  code.  Extensive  error  injection  experiments 
including  over  35,000  errors  are  conducted  targeting  the  most  fre-
quently  used  functions  in  the  selected  kernel  subsystems.    Three 
types  of  faults/errors  injection  campaigns  are  conducted:  (1)  ran-
dom non-branch instruction, (2) random conditional branch, and (3) 
valid but incorrect branch. The analysis of the obtained data shows: 
(i) 95% of the crashes are due to four major causes, namely, unable 
to  handle  kernel  NULL  pointer,  unable  to  handle  kernel  paging 
request,  invalid  opcode,  and  general  protection  fault,  (ii)  less  than 
10% of the crashes are associated with fault propagation and nearly 
40% of crash latencies are within 10 cycles, (iii) errors in the kernel 
can  result  in  crashes  that  require  reformatting  the  file  system  to 
restore system operation; the process of bringing up the system can 
take nearly an hour.
Introduction 
1 
The  dependability  of  a  computing  system  (and  hence  of  the 
services provided to the end user) depends to large extent on 
the error hardiness of the underlying operating system. In this 
context, analysis of the operating system’s failure behavior is 
essential in determining whether a given computing platform 
(hardware and software) is able to achieve a desired level of 
availability/reliability.  
The  objective  of  this  study  is  to  understand  how  the  Linux 
kernel  responds  to  transient  errors.  To  this  end,  a  series  of 
fault/error  injection  experiments  is  conducted.  A  single-bit 
error model is used to emulate error impact on the kernel in-
struction stream. While the origin of an error is not presumed 
(i.e.,  an  error  can  come  from  anywhere  in  the  system),  the 
injections  reflect  the  ultimate  error  effect  on  the  executed 
instructions. This approach allows mimicking a wide range of 
failure  scenarios  that  impact  the  operating  system1.  In  order 
to  conduct  meaningful  fault/error  injection  experiments,  it  is 
essential to apply appropriate workloads for generating kernel 
activity  and  thus,  ensuring  a  relatively  high  error  activation 
rate  (errors  matter  to  the  system  only  when  activated).  To 
achieve  this  goal,  the  UnixBench  [24]  benchmark  suite  is 
used  to  profile  kernel  behavior  and  to  identify  the  most  fre-
quently  used  functions  representing  at  least  95%  of  kernel 
usage.  
1 Observe that by directly targeting the instruction stream we can emulate not 
only errors in the code but also errors due to corruption of registers or data. 
Consider, for example, two scenarios: (i) corruption of the register name in 
an instruction that uses indirect addressing mode may result in accessing an 
invalid memory address – equivalent to the register contents corruption; (ii) 
corruption of an instruction operand that is used as an index to a lookup table 
containing  function  offsets  may  result  in  accessing  an  invalid  function  (ad-
dress) – equivalent to the look up table data corruption.  
Subsequently,  over  35,000  faults/errors  are  injected  into  the 
kernel  functions  within  four  subsystems:  architecture-
dependent code (arch), virtual file system interface (fs), cen-
tral  section  of  the  kernel (kernel),  and  memory  management
(mm). Three types of fault/error injection campaigns are con-
ducted:  random  non-branch,  random  conditional  branch,  and 
valid but incorrect conditional branch. The data is analyzed to 
quantify the response of the OS as a whole based on the sub-
system and to determine  which  functions are responsible  for 
error sensitivity. The analysis provides a detailed insight into 
the  OS  behavior  under  faults/errors.  The  major  findings  in-
clude: 
•  Most crashes (95%) are due to four major causes: unable 
to  handle  kernel  NULL  pointer,  unable  to  handle  kernel 
paging request, invalid opcode, and general protection fault. 
•  Nine  errors  in  the  kernel  result  in  crashes (most  severe 
crash category), which require reformatting the file system. 
The  process  of  bringing  up  the  system  can  take  nearly  an 
hour.  
•  Less  than  10%  of  the  crashes  are  associated  with  fault 
propagation,  and  nearly  40%  of  crash  latencies  are  within 
10  cycles.  The  closer  analysis  of  the  propagation  patterns 
indicates that it is feasible to identify strategic locations for 
embedding  additional  assertions  in  the  source  code  of  a 
given subsystem to detect errors and,  hence, to prevent er-
ror propagation.  
2  Related Work 
User-level testing by executing API/system calls with errone-
ous arguments. CMU's Ballista [15] project provides a com-
prehensive  assessment  of  15  POSIX-compliant  operating 
systems  and  libraries  as  well  as  Microsoft  Win32  API.  Bal-
lista  bombards  a  software  module  with  combinations  of  ex-
ceptional  and  acceptable  input  values.  The  responses  of  the 
system are classified according to the first three categories of 
the  “C.R.A.S.H” severity scale [16]: (i)  catastrophic failures 
(OS corruption or machine crash), (ii) restart failures (a task 
hang), (iii) abort failures (abnormal termination of a task).  
The  University  of  Wisconsin  Fuzz  [19]  project  tests  system 
calls  for  responses  to  randomized  input  streams.  The  study 
addresses the reliability of a large collection of UNIX utility 
programs  and  X-Window  applications,  servers,  and  network 
services.  The  Crashme  benchmark  [6]  uses  random  input
response analysis to test the robustness of an operating envi-
ronment in terms of exceptional conditions under failures.  
Error injection into both kernel and user space. Several stud-
ies  have  directly  injected  faults  into  the  kernel  space  and 
monitored  and  quantified  the  responses.  FIAT  [2]  an  early 
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:05:58 UTC from IEEE Xplore.  Restrictions apply. 
fault  injection  and  monitoring  environment  experiments  on 
SunOS  4.1.2  to  study  fault/error  propagation  in  the  UNIX 
kernel.  FINE  [14]  injects  hardware-induced  software  errors 
and software faults into UNIX and traces the execution flow 
and key variables of the kernel.  
Xception  [5]  uses  the  advanced  debugging  and  performance 
monitoring features existing in most of the modern processors 
to inject faults and to monitor the activation of the faults and 
their  impact  on  the  target  system  behavior.  Xception  targets 
PowerPC  and  Pentium  processors  and  operating  systems 
ranging  from  Windows  NT  to  proprietary,  real-time  kernels 
(e.g., SMX) and parallel operating systems (e.g., Parix).  
MAFALDA [1] analyzes the behavior of Chorus and LynxOS 
microkernels  in  the  presence  of  faults.  In  addition  to  input 
parameters  corruption,  fault  injection  is  also  applied  on  the 
internal  address  space  of  the  executive  (both  code  and  data 
segments).  In  [4],  User  Mode  Linux  (equivalent  of  a  virtual 
machine,  representing  a  kernel)  executing  on  the  top  of  real 
Linux  kernel  is  used  to  perform  Linux  kernel  fault  injection 
via the ptrace interface. 
Other  methods  to  evaluate  the  operating  system. In  addition 
to  using  fault  injection  mechanisms,  operating  systems  have 
been evaluated by studying the source code, collecting mem-
ory dumps, and inspecting the error logs. For example, Chou 
et al. [9] present a study of Linux and OpenBSD kernel errors 
found  by  automatic,  static,  compiler  analysis  in  the  source 
code level.  Lee et al. [17] use a collection of  memory dump 
analyses  of  field  software  failures  in  the  Tandem  GUARD-
IAN90  operating  system  to  identify  the  effects  of  software 
faults.  Xu  et  al.  [26]  examine  Windows  NT  cluster  reboot 
logs  to  measure  dependability.  Sullivant  et  al.  [23]  study 
MVS operating system failures using 250 randomly sampled 
reports. 
3  Linux Kernel Subsystems 
The Linux kernel can be divided into several subsystems [3]. 
Figure  1,  based  on  [10]  shows  the  size  of  the  code  corre-
sponding  to  each  subsystem  of  the  kernel  version  2.4.20  re-
leased on November 28, 2002. 
Linux Kernel 2.4.20 
Number of Lines of Source Code (totally 4,266,802 )
kernel
0.33%
ipc
0.08%
init
0.03%
include
12.46%
fs
7.28%
arch
drivers
fs
include
init
ipc
kernel
lib
mm
net
lib
0.20%
mm
0.36%
net
5.36%
arch
16.02%
drivers
57.87%
Figure 1: Size of Kernel Subsystems in Terms of Source 
Code Lines 
In  our  error  injection  campaigns,  we  focus  on  four  subsys-
tems:  arch,  fs,  kernel,  and  mm.  Specifically,  (1)  arch  holds 
the  architecture-dependent  code  (i.e.  i386),  which  includes 
low-level  memory  management,  interrupt  handling,  early 
initialization,  and  assembly  routines,  (2)  fs  contains  support 
for various kinds of virtual file systems (we use ext2 file sys-
tem),  (3)  kernel  is  the  architecture-independent  core  kernel 
code, which includes services such as scheduler, system calls, 
and signal handling, and (4) mm contains high-level architec-
ture-independent memory management code. Selection of the 
target  subsystems  is  based  on  the  type  of  activity  generated 
by  the  benchmark  programs  (as  discussed  in  Section  4), 
which  for  most  part  invoke  functions  from  the  four  selected 
subsystems. Note that the net subsystem was not targeted for 
injection in this study. An important reason was to maintain a 
single system focus and to keep the study tractable. The net-
work issues can be studied separately.
4  Benchmarks and Kernel Profiling 
Due  to  the  size  of  the  kernel,  it  is  impractical  to  target  the 
entire kernel code for error injection. Depending on the work-
load,  different  kernel  functions  are  activated  with  varying 
frequency.  In  order  to  determine  the  relative  importance  of 
different subsystems and the most frequently used functions, 
we  profile  the  kernel  using  the  UnixBench  benchmark  [24]. 
The  use  of  benchmark  programs  serves  two  purposes:  (1)  it 
profiles kernel usage to determine targets (most active kernel 
functions)  for  error  injection  campaigns  and  (2)  it  creates 
kernel activity during error injection campaigns to maximize 
chances  for  error  activation.  UnixBench  is  a  UNIX/Linux 
benchmark  suite  including  tests  on  CPU,  memory  manage-
ment,  file  I/O,  and  other  kernel  components.  Eight  C  pro-
grams  (context1.c,  dhry,  fstime.c,  hanoi.c,  looper.c,  pipe.c, 
spawn.c and syscall.c.) from the 17 programs included in the 
benchmark  suite  are  selected  for  the  study.  The  selection  of 
the programs  is  to ensure sufficient  kernel activity to trigger 
injected errors and, hence, to enable assessing the kernel be-
havior in the presence of errors. An additional  goal is to en-
sure  that  the  studied  kernel  subsystems  are  thoroughly  exer-
cised.  
the  benchmarks 
Kernel Profiling. Profiling of the kernel functions while exe-
cuting 
is  performed  using  Kernprof 
(v0.12)[21]. Each activated kernel function is associated with 
a profiling value that indicates the number of times the sam-
pled  program  counter  falls  into  a  given  function. A  total  of 
403 kernel functions are profiled. Table 1 gives the distribu-
tion of the profiled functions among the kernel modules.  
Table 1: Function Distribution Among Kernel Modules 
Subsystem 
Total number of func-
Name 
tions within a subsystem 
Contribution to the 
core 32 functions  
arch 
fs 
kernel 
mm 
drivers 
ipc 
lib 
net 
Total 
40 
154 
62 
71 
64 
1 
6 
5 
403 
5 
12 
5 
10 
n/a 
n/a 
n/a 
n/a 
32 
Analysis  of  profiling  data  indicates  that  the  top  (i.e.,  most 
frequently used) 32 functions account for 95% of all profiling 
values.  These  functions  were  selected  as  the  targets  for  the 
error injection experiments.  
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:05:58 UTC from IEEE Xplore.  Restrictions apply. 
5  Experimental Setup and Approach 
Failure characterization of  the Linux  kernel is conducted  us-
ing  software-implemented error injection. Errors are injected 
to  the  instruction  stream  of  selected  kernel  functions.  The 
collected results are analyzed to derive measures characteriz-
ing  kernel  sensitivity  to  errors  impacting  the  instruction 
stream.  
5.1 
The  Linux  kernel  error  injector  relies  on  the  CPU’s  debug-
ging  and  performance  monitoring  features  and  on  the  Linux 
Reliability Availability Serviceability (RAS) package [22] to 
(i)  automatically  inject  errors  and  (ii)  monitor  error  activa-
tion, error propagation, and crash latency. 
Linux Kernel Error Injection Approach 
Linux kernel debugging tools. Linux kernel has several em-
bedded  debugging  (or  failure  reporting)  tools,  including  (i) 
printk() – a common way of monitoring variables in the ker-
nel  space,  (ii)  /proc  –  a  virtual  file  system  for  system  man-
agement  (a  kernel  executable  core  file  /proc/kcore  can  be 
debugged by gdb to look at kernel variables), (iii) /var/log – a
system  log  file,  and  (iv)  Oops  message  –  provides  a  kernel 
memory image at the time of kernel failure. 
The above tools, while useful and adequate for most develop-
ers,  are  not  sufficient  for  conducting  a  comprehensive  study 
characterizing the error sensitivity of the kernel. To enhance 
error/failure analysis capabilities, we employ the Linux Reli-
ability  Availability  Serviceability  (RAS)  package.  Specifi-
cally, we use SGI’s Built-in Kernel Debugger (KDB/KGDB) 
[20]  to  enable  debugging,  including  tracing  of  the  kernel 
code,  and  the  Linux  Kernel  Crash  Dump  (LKCD)  facility 
[25]  to  enable  configuring  and  analyzing  of  system  crash 
dumps.  A  set  of  utilities  and  kernel  patches  are  created  to 
allow  an  image  of  system  memory  (crash  dump)  to  be  cap-
tured even if the system abruptly fails. The Linux dump facil-
ity LKCD only generates crash dumps under three cases: (i) a 
kernel Oops occurs, (ii) a kernel panic occurs, or (iii) the sys-
tem administrator initiates a crash dump by typing Alt-SysRq-
c  on  the  console.  To  differentiate  among  reasons  for  system 
crashes, custom crash handlers are embedded in the kernel to 
enable timely invocation of LKCD on crash. 
Architecture  of  the  Linux  Kernel  Error  Injector.  Mecha-
nisms  such  as  analyzing  Oops  messages,  checking  specific 
log  files,  and  directly  using  the  RAS  package,  while  power-
ful, are not  sufficient  when performing  large  number of ker-
nel error injections. A Linux kernel fault/error injector is de-
signed for such experiments. As shown in the block diagram 
in Figure 2, the architecture consists of (1) kernel-embedded 
components  –  crash  handlers,  driver,  injector,  (2)  user-level 
components  –  injection  data  producer,  injection  controller, 
and  data  analyzer,  and  (3)  a  hardware  watchdog  to  monitor 
system hangs/crashes and to auto-reboot the kernel in case of 
failure. 
Similarly to Xception[5], the injector uses the debug registers 
provided by the IA-32 Intel architecture to enable the specify-
ing of the target instruction address and the triggering of the 
injection. To access the debug registers, an injection driver (a 
kernel  module)  is  developed  and  attached  to  the  kernel.  The 
controller,  in  the  user  space,  invokes  the  injection  driver  by 
sending  the  injection  message.  The  injection  driver  sets  the 
contents  of  one  of  the  debug  registers  to  the  address  of  the 
target instruction. Once the kernel reaches the target address 
(the program counter matches the contents of the debug regis-
ter),  the  error  injector  is  activated.  The  injector  carries  out 
the  following  actions:  (1)  inserts  an  error  into  the  binary  of 
the target instruction (i.e., flips a bit), (2) starts a performance 
counter  to  measure  the  latency  between  the  time  the  cor-
rupted instruction is executed and the actual kernel crash, and 
(3)  returns  control  to  the  kernel,  which  continues  from  the 
address of the injected instruction. Figure 3 depicts the proc-
ess of injecting an error, monitoring the kernel, and recording 
the crash dump.  
Hardware
Monitor
    . watchdog   
    . Auto-reboot
Injection Data
Producer
Controller
Pass Injection data 
to/from Kernel  
Results 
Collector 
Data analysis 
Crash Handler
     Collect crash cause/latency/error 
propagation data for analysis
Target 
System
(Linux Kernel 2.4.19)
Workload
. Benchmark
Injection 
Driver
. Set location 
  to activate 
  Injector 
. Pass 
  activation bit 
  to Controller 
Injector
. Set error 
  activation bit  
. Inject fault 
. Start counter
Figure 2: Linux Kernel Error Injector 
Next Location
User Workload
Start  next 
injection
The end
Save dump files
Create analysis file
Not Activated
Activated
Inject Fault
M onitor
User Detected/
Not Manifested
Hang
Crash Dump 
Requested