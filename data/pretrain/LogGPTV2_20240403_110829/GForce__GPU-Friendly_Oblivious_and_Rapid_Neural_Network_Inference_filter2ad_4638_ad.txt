Suppose, for a linear layer with quantization parameters expx
and expw, y is its quantized output. We want to dequantize
it, pass it through (a maxpool layer and) a ReLU layer, and
quantize it for the next linear layer with quantization param-
eters expy. GForce does these by fusing the dequantization
(DeQ) with the quantization (Q). Theorem 1 proves that this
leads to the same result when the non-linear layers between
the linear layers are comparison-based ReLU and MaxPool.
Theorem 1 (Fusing (De)quanization). Q◦ fCMP ◦ DeQ(yyy) =
clip((cid:98) fCMP(yyy)/d)(cid:101)) or clip((cid:98) fCMP(yyy)· d)(cid:101)) for some d ∈ Z,
where fCMP = ReLU◦ MaxPool (or ReLU as an easier case),
ReLU(xxx) = max(xxx,0), and MaxPool(xxx) = max({xxxi}i).
Proof. We have (Q◦ fCMP ◦ DeQ)(yyy) = clip((cid:98)2−expy+bit−2 ·
max({2expx+expw−2·bit+4 · yyyi}i,0)(cid:101))
fCMP({xxxi}i) =
max({xxxi}i,0), which can be fused into clip((cid:98)2shift· fCMP(yyy)(cid:101)),
where shift = expx + expw − expy − bit + 2 as cmax(a,b) =
max(ca,cb) for c > 0. Depending on the sign of shift, the
fused (de)quantization becomes division/multiplication.
since
3.7.3 Stochastic Rounding and Truncation Layers
Secure division is not easy even for a public divisor. Some
prior works (e.g., [18]) directly divide each share by a (public)
divisor d, even for wrapped-around (cid:104)s(cid:105) = {−τ,s + τ − q},
i.e., {(cid:98)−τ/d(cid:99),(cid:98)(s + τ − q)/d(cid:99)}. The reconstruction is thus
incorrect: (cid:98)−τ/d(cid:99) +(cid:98)(s + τ− q)/d(cid:99) ≈ (cid:98)(s− q)/d(cid:99) (cid:54)= (cid:98)s/d(cid:99).
For (ﬂoor) division, we modify a DGK-based approach [25]
(on AHE ciphertexts). Our protocol works over secret shares
(with the wrap-around protocol) without running the entire
DGK explicitly. It also “implicitly” performs stochastic round-
ing on the output. Our division protocol could incur errors to
Protocol 4 GPU-friendly Truncation Protocol
Ofﬂine Input (S|C)
Online Input (S|C)
Output (S|C)
Constraints
(cid:104)s(cid:105)C
q
d,skAHEq
(cid:104)(cid:98)s/d(cid:101)(cid:105)C
d, pkAHEq
(cid:104)s(cid:105)S
q
(cid:104)(cid:98)s/d(cid:101)(cid:105)S
log2(q)  (q− 1)/2)·(cid:98)q/d(cid:99)
S picks r ∈ Zk
S and C run GPU-Wrapoﬀ (r)
S, C has every values stored in preS or preC, resp.
1: procedure GPU-Trunoﬀ
2:
3:
4:
5: procedure GPU-Trunon((preS,(cid:104)s(cid:105)S
6:
7:
8:
9:
10:
S computes (cid:104)z(cid:105)S
C reconstructs z = s + r mod q = (cid:104)z(cid:105)S
S, C gets (cid:104)wrap(cid:105)S
S: (cid:104)(cid:98)s/d(cid:101)(cid:105)S
C: (cid:104)(cid:98)s/d(cid:101)(cid:105)C
q ← −(cid:98)r/d(cid:99) +(cid:104)wrap(cid:105)S
q ← (cid:98)z/d(cid:99) +(cid:104)wrap(cid:105)C
q ← (cid:104)s(cid:105)S
q,(cid:104)wrap(cid:105)C
q), (preC,(cid:104)s(cid:105)C
q ))
q + r and sends it to C
q +(cid:104)s(cid:105)C
q
q ← GPU-Wrapon(z), resp.
q
q
the output values, but the error distribution of the division is
close to the value distribution of stochastic rounding:
(cid:40)(cid:98)s(cid:99) + 1, with probability s−(cid:98)s(cid:99),
(cid:98)s(cid:99),
with probability 1− (s−(cid:98)s(cid:99)).
(cid:98)s(cid:101) =
For our protocol to perform division and stochastic round-
ing at once, S computes (cid:104)(cid:98)s/d(cid:101)(cid:105)S ← −(cid:98)τ/d(cid:99) + (cid:104)wrap(cid:105)S
q,
where τ is a pre-drawn additive mask for s, and C computes
(cid:104)(cid:98)s/d(cid:101)(cid:105)C ← (cid:98)z/d(cid:99) +(cid:104)wrap(cid:105)C
q , where (cid:104)wrap(cid:105) is correspond-
ing to z and the divisor d. Like other GPU-friendly secure
online/ofﬂine protocols, the server can take advantage of its
prior knowledge on the randomness τ in the ofﬂine phase. The
ideas above result in GPU-Trun (Protocol 4) for SRT layers.
Theorem 2. The secret value underlying the output of
GForce’s SRT layers (or GPU-Trun, i.e., Protocol 4) on input
s and a divisor d approximates stochastically rounded (cid:98)s/d(cid:101).
Proof. We analyze the value distribution of (cid:104)s/d(cid:105) when there
is no wrap-around. In this proof, we let vd be the remainder of
a variable v (v∈{q,τ,s}) with respect to a divisor d. (One may
consider vd as v in Zd.) The result of the reconstruction is:
(cid:40)(cid:98)s(cid:99) + 1,
(cid:98)s(cid:99),
if sd + τd ≥ d,
if sd + τd < d.
(cid:98)z/d(cid:101)−(cid:98)τ/d(cid:101) =
As τ is uniformly sampled from [0,q− 1], its distribution
is p(τd) = 1/(qd + d) if τd ≥ qd and p(τd) = 2/(qd + d) if
τd < qd. Since we have the constraint that qd = 1, we can
assume τd is uniformly distributed when d (cid:29) 1. (Based on our
experimental results, d is usually in {29,210,211}, meaning
the deviation from a uniform distribution is very small.)
2154    30th USENIX Security Symposium
USENIX Association
Also, as sd + τd ≥ d ⇐⇒ τd/d ≥ 1− (s/d −(cid:98)s/d(cid:99)), we
can conclude that when d (cid:29) 1,
(cid:40)(cid:98)s(cid:99) + 1, with prob. sd/d −(cid:98)sd/d(cid:99),
(cid:98) z
d
(cid:101)−(cid:98) τ
d
(cid:101) =
(cid:98)s(cid:99),
with prob. 1− (sd/d −(cid:98)sd/d(cid:99)),
which is identical to the distribution of stochastic rounding
(cid:98)x/d(cid:101). If it wraps around, s + r mod q = s + τ− q. So,
(cid:98)s(cid:99) + 1,
(cid:98)s(cid:99),
(cid:98)s(cid:99)− 1,
if sd + τd − qd ≥ d,
if 0 < sd + τd − qd < d,
if sd + τd − qd < 0.
(cid:98) z
d
(cid:99)−(cid:98) τ
d
(cid:99)−(cid:98) q
d
(cid:99) =
When d (cid:29) 1, qd/d ≈ 0, so this distribution is very close to
the distribution when wrap-around does not happen, and thus
it is also close to the distribution of stochastic rounding.
Truncation Approaches Comparison. Delphi [18] directly
truncates the least signiﬁcant bits without any wrap-around
handling. Much plaintext space is wasted to avoid error be-
cause the error probability is proportional to the ratio of values
hidden in additive SS to the size of the plaintext space. We
will empirically show in Section 4.1 that such truncation ren-
ders the inference useless due to the tight bit-width.
Delphi [18] picks a plaintext space of 32 bits. It is enough
to prevent overﬂow during linear computations on GPU since
the plaintext multipliers are small. However, it is too large
for additive SS multiplication on GPU because it requires at
least 64-bit bit-width, while GPU can only work with 52 bits
for optimized performance. Also, adopting 32-bit bit-width
instead of our choice of 22-bit would increase GPU-DGK’s
computation and communication costs by ∼45%.
Another idea of using the original DGK [25] is to determin-
istically round up the divided values. We will show by experi-
ments in Section 4.1 that it is orders-of-magnitude slower than
our truncation protocol, let alone the extra procedures and
bit-width needed (Section 3.5) to prevent off-by-one errors.
4 Experimental Evaluation
Experimental Platform. Following the LAN setting of
Gazelle [11] (AWS Virtual Machines (VMs) in us-east-1a) in
spirit, our experiments ran on 2 Google Cloud VMs located
in the same region (asia-east-1c). They are equipped with
Nvidia V100 GPU and run Ubuntu 18.04 LTS. Each has 52GB
RAM and 8 virtual Intel Xeon (Skylake) CPUs at 2GHz.
We report the mean of 10 experiment repetitions and pro-
vide the standard deviation in [·] if the measurement may be
affected by randomness, e.g., runtime and inference accuracy.
Cryptographic Implementations. We code GForce in C++
(compiled by GCC 8.0) and Python 3.6. We marshal network
communication and GPU operations via PyTorch 1.3.1 and
CUDA 10.0. We assume the bit length of all data, i.e., the in-
put, the intermediate values, and the weights, is 18, except we
set (cid:96) = 20 for a fair comparison with Gazelle in benchmark-
ing ReLU and maxpool (Tables 4-5). We set bit = 8 for 8-bit
ﬁxed-point representation in quantization (see Section 3.7).
We use Microsoft SEAL (release 3.3.2)’s BFV-FHE [8] as
AHE. The plaintext space for the neural networks is deﬁned
by q = 7340033. The degree of encryption polynomials (i.e.,
the number of plaintext slots in a ciphertext) is 16384, and the
coefﬁcients modulus of the polynomials is of 438 bits. The
ciphertext size is 32MB, which is amortized to 2048 bit for
each data entry. We picked the recommended parameters for
SEAL to ensure 128-bit security. In the bit-wise comparison
of the DGK protocol (GPU-DGK), we also pick the same set
of parameters for SEAL except we set p = 65537.
BFV-FHE relies on the hardness of the learning-with-error
problem. By itself, it does not support circuit privacy because
the noise embedded into the ciphertexts may allow the sk
holder to infer some partial information about the input plain-
texts. To hide S’s private input to AHE for linear functions,
we adopted noise ﬂooding [1] with 330-bit smudging noise;
namely, S adds encryption of 0 with 330-bit noise to each
ciphertext before sending it to C. Appendix C.2.3 discusses
why this magnitude of the noise is enough for circuit privacy.
Comparing to Prior Arts’ Experiments. Gazelle [11],
Falcon [13], and Delphi [18] are our major competitors.
Gazelle’s implementation is criticized [18,21] for its choice
of AHE parameters, which may not ensure circuit privacy. Fal-
con’s choice suffers from the same issue. Changing the param-
eters will worsen their performance and require re-evaluation.
To their advantage, we rely on their ﬁgures as is. Non-linear
layers are not affected by AHE, and we reproduced their exper-
iments on our Google Cloud VMs. For ReLU (Table 4), our
reproduced results are slightly worse than what were reported.
For maxpool (Table 5), ours got slightly better.
Falcon did not release their code, and we failed to compile
the code of Delphi, so we only quote the ﬁgures from their pa-
pers. We will give inline remarks on the comparison fairness.
4.1 Comparison-based Layers
As one of our contributions, we demonstrate the performance
of our ReLU and maxpool implementations using our GPU-
friendly protocol. We chose Gazelle [11] and Falcon [13] for
comparison due to their similar paradigm for this part: the
server and the client interact to get secret shares of the layer
inputs and collaboratively compute the shared outputs. Delphi
uses GC for non-linear computation and is not compared.
ReLU Layers. As in Table 4, for 10000-element inputs, we
outperform Gazelle by 9× and Falcon by 11× in the online
runtime and by at least 8× in the online communication cost.
USENIX Association
30th USENIX Security Symposium    2155
#input
Framework
10000
217
Gazelle
Falcon
GForce
Gazelle
Falcon
GForce
(cid:96)
20
30
20
20
30
20
Comp. (ms)
Comm. (MB)
Ofﬂine
771
[10.28]
361.70
18426
[82.42]
9378
[50.00]
∗4740
134632
[443.30]
Online
146.77
[4.52]
179.60
111666...333777
[1.87]
1754
[20.33]