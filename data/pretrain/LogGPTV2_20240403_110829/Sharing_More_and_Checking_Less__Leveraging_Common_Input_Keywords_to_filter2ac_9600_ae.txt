http_user in Netgear R7900, both KARONTE and SaTC
could ﬁnd the same buffer-overﬂow bug (i.e., the traces in
the reports of SaTC and KARONTE are the same). How-
ever, SaTC could ﬁnd one more buffer-overﬂow bug because
KARONTE misses an entry point related to the string. Finally,
KARONTE cannot detect any command injection vulnerabil-
ities as it does not track the data ﬂow from the input entry
points to the system-like functions. For example, KARONTE
misses 12 command injection alerts in Netgear R7300.
In terms of the analysis time, KARONTE and SaTC have
their own pros and cons. The analysis time of SaTC depends
on the protocols the device uses and the number of sensitive
input entry points it extracted. For example, SaTC found more
than 31,000 back-end entry points in 17 Netgear samples and
found only 779 entry points in nine D-Link samples, and
therefore, the average analysis time for the Netgear samples is
14 hours longer than the D-Link samples (shown in Table 5).
In contrast, the time spent of KARONTE depends on the num-
ber of data keys found in the border binaries, which are used
to label the IPC (inter-process communication) paradigms.
For example, SaTC found 10,228 sensitive entry points in 7
Tenda samples, but KARONTE found less than 100 data keys.
Hence, KARONTE is faster than SaTC on Tenda samples.
Case Study: Command Injection. Listing 4 shows a com-
mand injection vulnerability in D-Link DIR 878, detected
by SaTC. The front-end HTML ﬁle Network.html contains
an input keyword SetNetworkSettings/IPAddress (line 4).
Our input entry module detects a reference of the keyword in
the border binary prog.cgi (line 10). The cross-process entry
ﬁnder recovers the data dependency between prog.cgi and rc
base on the shared string SysLogRemote_IPAddress and ﬁnds
the entrance of the code fragment that uses the input data in
function FUN_44fa0c (line 17). The input-sensitive taint anal-
ysis module ﬁnds a call trace to the sink function twsystem at
line 28 and raises an alert based on the path exploration result
and path constraints (line 25).
Case Study: Incorrect Access Control. We discover incor-
rect access control vulnerability of a device based on the
action keywords identiﬁed by SaTC. First, we send requests
with action keywords to trigger the corresponding handler
functions in the back-end of device. Then, we check the re-
sponses and verify whether an API of the device is correctly
USENIX Association
30th USENIX Security Symposium    313
front -end: /cpio -root /.../ www/web/ Network .html */
e.Set(" SetNetworkSettings / IPAddress ",
document . getElementById ("lanIP").value)...
s= webGetVarString (p,"/ SetNetworkSettings / IPAddress ");
ModifySyslogServerIpNetAddr (s,s_00 ,&lac ,& l9c);
1 /* Keywords : SetNetworkSettings / IPAddress
2
3 function SetResult_3rd (e){ ...
4
5
6 }
7 /* Keywords Reference Point: FUN_43a08c
back -end: /cpio -root/bin/prog.cgi */
8
9 void FUN_43a08c ( uint32 p) {
10
11
12 }
13 void ModifySyslogServerIpNetAddr ( uint32 param_1 , ...) {
14
15
16
17
18 }
19 /* Sink point : /cpio -root/bin/rc */
20 void FUN_44fa0c (void) {
21
22
23
24
25
26
27
28
29 }}}
/* Located by Cross - Process Entry Finder */
pcVar1 = nvram_safe_get (" SysLogRemote_IPAddress ");
iVar2 = strcmp (__s1 ,"1");
if (iVar2 == 0) {
memset (acStack112 ,0 ,100);
sprintf (acStack112 ," syslogd -L -R %s",pcVar1 );
twsystem (acStack112 ,1);
snprintf (acStack72 ,0x10 ,"%s",param_1 );
iVar2 = ModifyIpNetAddr (& local_58 ,0x10 , acStack72 );
if (iVar2 == 0)
nvram_safe_set (" SysLogRemote_IPAddress " ,& local_58 );
if (* pcVar1 != '\0 ') {
Listing 4: Pseudocode of CVE-2019-8312, a command injection
vulnerability detected by SaTC at line 28.
restricted for access. In our data set, we found six incorrect
access control vulnerabilities that could result in privacy dis-
closure. For example, in CVE-2019-7388, D-Link 823G incor-
rectly restricts access to a resource from an unauthorized actor.
An attacker only needs to call an HNAP API GetClientInfo
remotely and could get the information of all clients in the
wireless local network (WLAN), such as IP address, MAC
address and device name.
7.2 Accuracy of Keyword Extraction
The Ksrc column of Table 4 shows the type of front-end ﬁle
where the vulnerability-related keyword is found. 20 out of 33
bugs are related to input keywords found in JavaScript ﬁles;
eight are related to keywords in XML ﬁles; four of them rely
on the keywords in HTML ﬁles. Among 33 bugs, only two
are related to the keywords in the form component of HTML
ﬁles (labeled as HTML+ in Table 4). For the bug in XR300,
we use the implicit ﬁnder to identify the entry that is closer
to another normally located entry. The result means all three
types of front-end ﬁles used in the input keyword extraction
(§3) are necessary to locate the input entries. Table 8 shows
the number of input keywords selected by each step of SaTC
during the evaluation. Only 10% of all strings from front-end
ﬁles are ﬁnally used as input keywords.
False Positive of Parameter Keywords. To understand the
false positives of the input keyword extraction (§3), we extend
the input entry locators in §4 to ﬁnd all data-retrieval func-
tions. These functions are commonly used to obtain input data
from the request package with the parameter keywords, such
as the function WebsGetVar. We treat parameter keywords
used by these functions as true positives. For other parameter
keywords, we apply manual analysis: if they are used to label
Table 7: Categories of the false positives of the keywords. In this
table, we list the type (Type) and the sample case (Sample).
Type
Sample
Value of id label (HTML)
Constant string (JavaScript)
Function’s parameter (JavaScript)
id="adv_connect_time"
if (typeof(event.pageX) == "undefined")
R.module("macFilter", view, module)
some user-input from some related requests, we treat them as
true positives; otherwise, they are false positives.
As shown in the vPar/tPar column of Table 8, SaTC collects
sustainable true positives in parameter keywords, especially
for TOTOLink (80%), Tenda samples (69%) and Netgear sam-
ples (32%). For devices from D-Link and Motorola, the true
positive rate is relatively lower, meaning that SaTC collects
more false positive keywords. We manually analyzed these
false positives and show the common reasons in Table 7. Most
of the false positives are constant strings, function’s parame-
ters and values of the id labels. We plan to investigate these
false positives and will add corresponding methods in the
input keyword extraction module to ﬁlter them out.
False Positives of Action Keywords. We take a method
similar to the above one to check the false positives of
action keywords. The key difference is that we only use
register-like functions to search the true positives, such as
Register_Handler in Listing 1. The vAct/tAct column of Ta-
ble 8 shows the result of our veriﬁcation. For most devices
from Tenda, TOTOLink and Motorola, SaTC can achieve
higher than 70% true positive rate. For the other devices, the
true positive rate is lower, and even reaches zero for two Net-
gear routers. We manually checked these results and found a
common reason that renders SaTC to have a signiﬁcant false
positive rate under our veriﬁcation method: the real action
keywords are not used to register or call handler functions;
For example, Netgear R7000 router stored function pointers
of all handler functions inside a function call table and merely
uses the action keywords to get the index of the associated
function in the table. In this way, even if SaTC successfully
identiﬁed real action keywords, our veriﬁcation method rely-
ing on register-like functions cannot conﬁrm their correct-
ness. We plan to identify such code patterns for particular
devices, and deﬁne speciﬁc rules to handle them properly.
False Negative. To understand the false negatives of our
bug detection results, we conservatively treat all strings in
a border binary as the taint sources and launch data-ﬂow
tracking for each of them. Our goal here is to check whether
we can effectively ﬁnd vulnerabilities starting from back-end
strings that have no appearance in the front-end. Since this
experiment relies on tedious human effort to verify each alert
(true positive or not), we randomly select seven devices to
conduct the false negative veriﬁcation. We keep the taint
engine running for each device until all strings have been
tested, which takes 5 to 113 hours. For all 408 reported alerts,
we manually check whether they are true positives or not.
314    30th USENIX Security Symposium
USENIX Association
Table 8: Input keywords collected, ﬁltered and used during our evaluation. For each device, we provide the number of front-end ﬁles
(Input). For input keyword extraction, the table shows the number of unﬁltered keywords (str), ﬁltered keywords (fKey) and the analysis time.
For border binary recognition, we show the number of all strings in the back-end binaries (strAll), the border binary name (borderBin), and the
keywords matched in border binary (borderKey). In the veriﬁcation part, (vPar) and (tPar) represent numbers of veriﬁed and total parameter
keywords, while (vAct) and (tAct) represent numbers of veriﬁed and total action keywords; % represents the proportion. Other than httpd,
Netgear samples contain border binaries for other services, such as upnpd.
Vendor
Series
Input
Keyword Extraction
str
fKey
time(s)
AC15
AC18
W20E
G1
G3
XR300
R6400
R7000
R7000P
878
882
823G
T10
Tenda
Tenda
Tenda
Tenda
Tenda
Netgear
Netgear
Netgear
Netgear
D-Link
D-Link
D-Link
TOTOLink
TOTOLink A950RG
Motorola
Motorola
C1
M2
119
119
134
147
147
864
489
610
607
251
252
110
59
73
105
103
7,771
7,663
10,581
14,241
14,241
18,889
5,692
9,421
8,670
26,389
25,608
10,200
6,217
7,520
12,347
10,982
995
984
1,744
137
137
4,232
1,729
2,304
2,257
3,415
3,025
2,544
869
1,267
2,133
1,863
254
145
102
1,952
1,952
683
32
167
67
492
1,149
370
231
303
315
303
Border Binary Recognition
borderBin
borderKey
httpd
httpd
httpd
httpd
httpd
httpd
httpd
httpd
httpd
prog.cgi
prog.cgi
goahead
system.so
system.so
prog.cgi
prog.cgi
447
447
834
636
636
1,226
887
1,132
1,121
735
878
255
64
180
370
333
strAll
241,314
119,537
139,885
123,960
123,960
517,254
478,005
330,087
467,706
139,948
522,317
48,005
51,898
53,931
90,652
83,911
time(s)
51
57
102
75
75
1,280
449
452
579
170
670
78
24
31