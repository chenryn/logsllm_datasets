title:Human-Machine Diversity in the Use of Computerised Advisory Systems:
A Case Study
author:Lorenzo Strigini and
Andrey Povyakalo and
Eugenio Alberdi
Human-machine diversity in the use of computerised advisory systems:
a case study
Lorenzo Strigini
Andrey Povyakalo
Eugenio Alberdi
Centre for Software Reliability, City University
Northampton Square, London EC1V 0HB, U.K.
{Strigini, Andrey, Eugenio}@csr.city.ac.uk)
Abstract
Computer-based advisory systems form with their users
composite, human-machine systems. Redundancy and di-
versity between the human and the machine are often impor-
tant for the dependability of such systems. We discuss the
modelling approach we applied in a case study. The goal is
to assess failure probabilities for the analysis of X-ray ﬁlms
for detecting cancer, performed by a person assisted by a
computer-based tool. Differently from most approaches to
human reliability assessment, we focus on the effects of fail-
ure diversity – or correlation – between humans and ma-
chines. We illustrate some of the modelling and prediction
problems, especially those caused by the presence of the hu-
man component. We show two alternative models, with their
pros and cons, and illustrate, via numerical examples and
analytically, some interesting and non-intuitive answers to
questions about reliability assessment and design choices
for human-computer systems.
1 Introduction
Most computer systems are used as part of larger human-
machine systems. Aircraft are controlled by pilots in com-
bination with multiple computer systems; patients are cured
by doctors using diagnostic and advisory computer systems;
bank tellers are assisted by computers; and so on. The suc-
cess of an aircraft ﬂight, a patient’s treatment or a bank
transaction thus hinges on the dependability of a compos-
ite, human-machine system. To assess or to reduce the risk
of these activities, one needs to study the dependability of
such composite systems. There is now an established (and
yet controversial) body of techniques for assessing human
reliability, and another body of techniques for computer de-
pendability. Yet our grasp of the dependability of systems
including the two is still unsatisfactory. We report some
work now under way within a U.K.-funded interdisciplinary
project, DIRC (the Interdisciplinary Research Collaboration
on the Dependability of computer-based systems) which is
addressing some of the open problems.
We focus on an interesting broad category of human-
machine systems:
those in which a single human uses a
computer to support his/her decisions. We discuss a case
study about dependability assessment for a system of this
kind.
An important aspect in the dependability of human-
machine systems is the fault tolerance provided by diverse
redundancy in the co-operation between machines and their
human users. For instance, in our chosen category – single-
user advisory systems – both the human and the machine
can perform a large part of each other’s tasks, and form,
in this sense, a redundant system, of which the human and
the machine are subsystems. In normal operation, the hu-
man and the machine often perform different subtasks (e.g.,
the machine is used for searching information in databases,
and the human user thus does not need to search manu-
ally through paper literature): their co-operation improves
the performance that could be obtained from the human
alone. However, if one of the two subsystems fails to per-
form properly, the redundancy between them is brought to
bear. Both the human and the machine subsystems usually
have some ability to detect errors of the other, and to recover
from some such errors (not necessarily the same subset) if
detected. E.g., when the machine’s database search func-
tion appears not to work, the human can obtain the required
information by reading literature.
Our case study concerns “computer-aided detection” in
the interpretation of X-ray ﬁlms, used in screening pro-
grammes for detecting breast cancer. Interpreting these X-
ray images is a specialistic, difﬁcult task, and computer-
based aids have been developed to assist with it. Our in-
terest in the problem arose in relation to clinical trials being
conducted for a speciﬁc model of computer aid [3], but what
we report here is related to general issues with this applica-
tion area, and advisory systems in general, rather than to a
particular product.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:31:04 UTC from IEEE Xplore.  Restrictions apply. 
The purposes of dependability modelling are: i) to eval-
uate a complete system and thus support decisions about
accepting or not accepting it for use; ii) to support deci-
sions about how to design a new system or improve an exist-
ing one. We use standard, “clear-box” modelling, describ-
ing system reliability as a function of component reliability.
This differs from the typical approach to the evaluation of
products in medicine, which compares the effectiveness of
medical activities with and without the product via “black
box” measurements on clinical trials. We conjecture that in
this case, in which the computerised aid is a well-deﬁned
components in a larger system, a clear box approach will
be beneﬁcial, by allowing us to study the effects of possi-
ble changes of each component on the whole system. Of
special concern is the fact that controlled trials cannot al-
ways reproduce the circumstances of clinical practice. For
the trial of this computerised tool, for instance, the set of
cases used was chosen to have a much higher proportion
of cancers than that (less than 1%) of the screened popula-
tion. This is necessary to make the trial reasonably short,
but may well affect the performance of the human users
and thus distort the results. Decisions about, e.g., adopt-
ing a speciﬁc computer aid require extrapolating from the
effects observed in the trial to an educated guess about the
effects in the ﬁeld. An advantage of “clear box” reliability
modelling is precisely the ability to predict dependability
measures for a future or hypothetical system, or an existing
system when used under different circumstances; one can
compare alternative system designs, which in this case may
include changes to the design of the machine, the choice
or training of users and/or the procedures of use.
In the
case of a human-machine system, one must also consider
that the detailed understanding of each component and its
failure processes belongs to practitioners of different dis-
ciplines, like engineers and psychologists. Explicit system
dependability modelling is meant to help in connecting to-
gether these various, partial views and understanding their
system-level implications.
In view of the redundancy and diversity present in a
human-machine system, we apply a modelling approach
which we have found useful in analyzing issues of diver-
sity and common-mode failure in redundant systems [8]: we
use a coarse-grained subdivision of the system into compo-
nents, plus analysis of component dependability conditional
on classes of demands, and how its variation generates cor-
relation between their failures.
In the next section, we outline the background of this
study: the use of computer-aided detection in breast screen-
ing and the way we are going to study it in terms of redun-
dancy and diversity. Section 3 considers a reliability model
for this system, derived directly from its intended mode of
operation, and concludes that it is unsuitable; Section 4 pro-
poses an alternative, less usual model. Sections 5 and 6
input:
X-ray
“mammogram”
system
original
mammogram
human
reader:
decision
CADT:
digitisation
and annotation
 (“prompting”)
prompted 
mammogram
output:
1-bit “decision”
(recall or not)
Figure 1. Role of a computer-assisted detec-
tion tool (CADT) in decisions on breast X-
rays. The system that produces decisions has
two components, the CADT and its user, the
reader.
deal, respectively via examples and analytically, with using
the latter model to predict the probability of failure of the
system in new circumstances, and the effects of design im-
provements. A Conclusions section follows.
2 Background
2.1 Computer aids for mammography
The human-machine system we consider – Fig 1 – in-
volves a human expert (“reader”) using a computerised aid
in screening “mammograms” (X-ray ﬁlms) for signs of can-
cer. The function of the system is to decide whether the
patient should be “recalled” for further tests because there
is suspicion of cancer, or instead informed that no problem
was found. The computerised aid used (CADT henceforth,
for “computer-aided detection tool”) processes a digitised
version of the X-ray ﬁlm, applying pattern recognition algo-
rithms to recognize and highlight (with conventional marks
– “prompts” – on the image) those features in the picture
that the reader should examine. The design goal for the
CADT is to aid the reader to notice all the features in a
mammogram that ought to be examined as potential indica-
tions of cancer. The task of producing a “recall/no recall”
decision is seen as having two components – detecting rele-
vant features, and classifying them as representing probable
cancers or not – and it is in the “detection” component that
pattern-matching algorithms are expected to help the human
reader, compensating e.g. for lapses of attention.
The reader receives (on ﬁlm and/or on a screen) the orig-
inal X-ray images plus copies with the “prompts” added by
the CADT.
There are two ways that the reader’s decision can be
wrong: a “false negative” decision, in which a patient with
cancer is not recalled, and a “false positive” one, in which
a healthy patient is recalled. Decisions about how to organ-
ise the screening have to take into account the probabilities
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:31:04 UTC from IEEE Xplore.  Restrictions apply. 
of both kinds of undesired outcome, and seek a reasonable
trade-off between their frequencies.
Many studies both of results of clinical use and of con-
trolled experiments have been conducted to estimate the ef-
fectiveness of this kind of systems (e.g. [9, 5, 11]). The
results of these studies have varied, though several have
shown this kind of computer aids to improve the probability
of cancers being detected.
2.2 Redundancy and diversity
The function of the CADT is to detect and highlight
those features that a reader should examine in a mammo-
gram, even when the reader alone does not notice them. In
this sense, the CADT provides fault tolerance within the
system.
There are two possible components in the advantage
given by the CADT (or by a second reader, as in current
practice in the U.K.). The ﬁrst is simple redundancy, i.e.
the advantage of repeating a task that is known to be subject
to random errors: the chance of a feature escaping detection
twice is lower than that of escaping it once. The second ad-
vantage is “diversity”, related to the fact that errors have a
systematic component as well: each human reader, and each
pattern recognition algorithm, will be more likely to make
mistakes on certain cases than on others. An algorithm (a
CADT) which were especially good at detecting those can-
cers that are most difﬁcult for readers to detect could be very
useful, even if it were much less effective at detecting most
other cancers.
Practitioners are aware of these issues, at an informal
level, but we want to make it possible to discuss them in
a more formal, mathematical way: informal reasoning may
miss important aspects of the problem, both in assessing
dependability and in deciding on design options. For in-
stance, we would hope to help to identify any circumstances
in which a designer should aim to increase the failure diver-
sity between a machine and its human users, even at the cost
of decreasing the effectiveness of the machine if considered
in isolation from its users.
2.3 Terminology and modelling approach for the
redundant system
We will call a case or input case or demand the set of
ﬁlms about a single patient. The system produces a deci-
sion whether to “recall” the patient as a probable cancer
case, or not to (so, this is a single bit of information). A
system failure is a wrong decision. System failures may be
“false negative” decisions or“ false positive” decisions. Our
modelling approach describes the two kinds of failure by
identical equations. For reasons of space, in this paper we
only describe the model for false negatives. We plan later to
address the trade-offs between the probabilities of the two
kinds of failure. So, for the rest of this paper we only con-
sider patients that have cancer, and when we write “failure”
without further speciﬁcation we mean a false negative.
Our system – Fig 1 – has two subsystems: the CADT and
the human reader. The reader’s decision is the output of the