# Title: Human-Machine Diversity in the Use of Computerised Advisory Systems: A Case Study

## Authors:
Lorenzo Strigini, Andrey Povyakalo, and Eugenio Alberdi  
Centre for Software Reliability, City University  
Northampton Square, London EC1V 0HB, U.K.  
{Strigini, Andrey, Eugenio}@csr.city.ac.uk

### Abstract
Computer-based advisory systems, when used in conjunction with human operators, form composite, human-machine systems. Redundancy and diversity between the human and the machine are often critical for the dependability of such systems. This paper discusses a case study where we applied a modeling approach to assess failure probabilities in the analysis of X-ray films for cancer detection, performed by a person assisted by a computer-based tool. Unlike most approaches to human reliability assessment, our focus is on the effects of failure diversity or correlation between humans and machines. We highlight some of the modeling and prediction challenges, particularly those arising from the presence of the human component. We present two alternative models, along with their pros and cons, and use numerical examples and analytical methods to illustrate some interesting and non-intuitive findings related to reliability assessment and design choices for human-computer systems.

### 1. Introduction
Most computer systems are integral parts of larger human-machine systems. For example, aircraft are controlled by pilots in combination with multiple computer systems, patients are treated by doctors using diagnostic and advisory computer systems, and bank tellers are assisted by computers. The success of an aircraft flight, a patient's treatment, or a bank transaction depends on the dependability of these composite, human-machine systems. To assess or reduce the risk associated with these activities, it is essential to study the dependability of such systems. While there are established (and sometimes controversial) techniques for assessing human reliability and computer dependability, our understanding of the dependability of systems that include both components remains inadequate.

This paper reports on work conducted as part of a U.K.-funded interdisciplinary project, DIRC (Interdisciplinary Research Collaboration on the Dependability of Computer-Based Systems), which addresses some of the open problems in this area. We focus on a broad category of human-machine systems where a single human uses a computer to support decision-making. We discuss a case study on dependability assessment for such a system.

A key aspect of the dependability of human-machine systems is the fault tolerance provided by diverse redundancy in the cooperation between machines and their human users. In our chosen category—single-user advisory systems—both the human and the machine can perform a large part of each other's tasks, forming a redundant system. In normal operation, the human and the machine often perform different subtasks (e.g., the machine searches databases, while the human does not need to search manually through paper literature). This cooperation improves performance compared to what could be achieved by the human alone. However, if one subsystem fails, the redundancy between them becomes crucial. Both the human and the machine can detect errors in the other and recover from some (not necessarily the same) subset of these errors. For instance, if the machine's database search function fails, the human can obtain the required information by reading literature.

Our case study concerns "computer-aided detection" in the interpretation of X-ray films used in breast cancer screening programs. Interpreting these X-ray images is a specialized and challenging task, and computer-based aids have been developed to assist with it. Our interest in this problem arose in relation to clinical trials for a specific model of computer aid [3], but our focus here is on general issues in this application area and advisory systems, rather than on a particular product.

The purposes of dependability modeling are: (i) to evaluate a complete system and support decisions about its acceptance; and (ii) to support decisions about designing a new system or improving an existing one. We use standard, "clear-box" modeling, describing system reliability as a function of component reliability. This differs from the typical "black-box" approach in medicine, which compares the effectiveness of medical activities with and without the product via clinical trials. We hypothesize that a clear-box approach will be beneficial in this case, allowing us to study the effects of possible changes to each component on the whole system. Of special concern is the fact that controlled trials may not always replicate the circumstances of clinical practice. For instance, the set of cases used in the trial of this computerized tool had a much higher proportion of cancers than the screened population, which is necessary to make the trial reasonably short but may distort the results. Decisions about adopting a specific computer aid require extrapolating from the trial results to an educated guess about the effects in the field. Clear-box reliability modeling allows us to predict dependability measures for future or hypothetical systems and compare alternative system designs, including changes to the machine, user selection and training, and usage procedures.

In a human-machine system, it is also important to consider that detailed understanding of each component and its failure processes falls within the purview of different disciplines, such as engineering and psychology. Explicit system dependability modeling helps integrate these various, partial views and understand their system-level implications.

Given the redundancy and diversity in a human-machine system, we apply a modeling approach that has proven useful in analyzing issues of diversity and common-mode failure in redundant systems [8]. We use a coarse-grained subdivision of the system into components, plus an analysis of component dependability conditional on classes of demands and how variations generate correlations between their failures.

In the next section, we outline the background of this study, including the use of computer-aided detection in breast screening and our approach to studying it in terms of redundancy and diversity. Section 3 considers a reliability model derived directly from the intended mode of operation and concludes that it is unsuitable. Section 4 proposes an alternative, less conventional model. Sections 5 and 6 deal, respectively, with using the latter model to predict the probability of failure in new circumstances and the effects of design improvements. A Conclusions section follows.

### 2. Background

#### 2.1 Computer Aids for Mammography
The human-machine system we consider (Figure 1) involves a human expert ("reader") using a computerized aid in screening "mammograms" (X-ray films) for signs of cancer. The system's function is to decide whether the patient should be "recalled" for further tests due to suspicion of cancer or informed that no problem was found. The computerized aid (CADT, for "computer-aided detection tool") processes a digitized version of the X-ray film, applying pattern recognition algorithms to recognize and highlight features in the image that the reader should examine. The CADT's design goal is to help the reader notice all features in a mammogram that should be examined as potential indications of cancer. The task of producing a "recall/no recall" decision has two components: detecting relevant features and classifying them as probable cancers or not. Pattern-matching algorithms are expected to assist the human reader, compensating for lapses of attention.

The reader receives the original X-ray images and copies with "prompts" added by the CADT. There are two types of incorrect decisions: "false negatives," where a patient with cancer is not recalled, and "false positives," where a healthy patient is recalled. Decisions about how to organize the screening must balance the probabilities of both types of undesired outcomes.

Many studies have been conducted to estimate the effectiveness of these systems, with varying results. Several studies have shown that computer aids can improve the probability of cancer detection.

#### 2.2 Redundancy and Diversity
The CADT's function is to detect and highlight features that a reader should examine, even if the reader alone does not notice them. This provides fault tolerance within the system. There are two components to the advantage provided by the CADT (or a second reader, as in current U.K. practice). The first is simple redundancy: repeating a task subject to random errors reduces the chance of a feature escaping detection. The second is "diversity": errors have a systematic component, and each human reader and pattern recognition algorithm is more likely to make mistakes on certain cases than others. An algorithm that is especially good at detecting cancers that are difficult for readers to detect can be very useful, even if it is less effective at detecting most other cancers.

Practitioners are aware of these issues informally, but we aim to formalize and mathematically discuss them. Informal reasoning may miss important aspects of the problem in assessing dependability and making design decisions. For example, we hope to identify circumstances where a designer should increase the failure diversity between a machine and its human users, even if it means decreasing the machine's effectiveness in isolation.

#### 2.3 Terminology and Modeling Approach for the Redundant System
We define a "case" or "input case" or "demand" as the set of films for a single patient. The system produces a decision to "recall" the patient as a probable cancer case or not (a single bit of information). A system failure is a wrong decision, which can be either a "false negative" or a "false positive." Our modeling approach describes both types of failure with identical equations, but for brevity, we only describe the model for false negatives. We plan to address the trade-offs between the probabilities of the two types of failure in future work. Therefore, for the rest of this paper, we only consider patients with cancer, and when we write "failure" without further specification, we mean a false negative.

Our system (Figure 1) has two subsystems: the CADT and the human reader. The reader's decision is the output of the system.