sub ecx,ecx
sub ecx,-0x50
call 0x09
inc eax
pop esi
xor [esi+0xE],0x76D740DC
sub esi,-0x4
loopd 0x0C
sub ecx,ecx
sub ecx,-0x50
call 0x09
inc eax
pop esi
xor [esi+0x13],0x0A72
xor [esi+0x15],0x76D740DC
sub esi,-0x4
nop
(invalid instruction)
Figure 2. (a) A decryption routine generated by Engine Pex
[4]. (b) The decryption routine obfuscated by self-modifying which
confuses control ﬂows.
Anti-disassembly is the type of obfuscation techniques
that try to confuse traditional disassembly algorithms (e.g.,
linear sweep [18]). It includes junk byte insertion, opaque
predicate, code overlap, indirect jump and branch function.
Indirect jump transfers control to the instruction whose
address is in a register operand. Because the value of the
register may not be statically determined, disassembly algo-
rithm such as recursive traversal cannot provide an accurate
disassembly. Therefore, attackers may use indirect jump to
replace relative jump in the payload. For example, relative
jump instruction “jmp 0x05” can be replaced by indirect
jump instruction “jmp eax”, where eax contains absolute
address of the target. The value of eax is normally hard to
determine until at run-time, thus thwarting static analysis.
Branch function is a function f(x) that, whenever called
from x, causes control to be transferred to the corresponding
location f(x) [18]. By replacing unconditional branches
in a program with calls to a branch function, attackers can
obscure the ﬂow of control in the program.
Memory Access Obfuscation is the type of obfusca-
tion techniques that use indirect addressing for memory
access to thwart static analysis. For example, instruction
“mov ebx,[ss:esp]”, which moves the top item of stack into
ebx, may be obfuscated by instructions “ mov eax,esp; mov
ebx,[ss:eax]”.
2.3 Anti-emulation
There are many anti-emulation techniques, such as using
interrupts in polymorphic decryption routines, inserting de-
291291
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:00:28 UTC from IEEE Xplore.  Restrictions apply. 
lay loops, executing random code. These techniques have
existed in virus writer community for many years and more
details can be found in [26].
3 A Generic Code Detection Technique
In this section, we present STILL in three steps, as shown
in Fig. 1.
3.1 Disassembly and Control Flow Graph
Generation
The ﬁrst step of STILL is to disassemble the input data
stream and generate a control ﬂow graph. A major chal-
lenge here is that we do not know whether the data stream
contains code or not, and what the entry point of the code
is when code is present. As such, it is not directly clear
which parts of a stream should be disassembled. The prob-
lem is exacerbated by the fact that different types of instruc-
tions have varying lengths and most bit combinations map
to valid instructions in the Intel IA32 instruction set. In fact,
even a stream of random bytes (or a part of a stream) could
be disassembled into a valid instruction sequence [15, 28].
In previous work [8, 15, 28] several disassembly algo-
rithms have been proposed to address the aforementioned
challenges.
In STILL, we exploit the O(N) disassembly
algorithm used in [28], where N is the length of the data
stream. This algorithm will result in a set of instruction
sequences. An instruction sequence is a sequence of CPU
instructions which has one and only one entry instruction
and there exists at least one execution path from the en-
try instruction to any other instruction in this sequence. A
fragment of a program in machine language is an instruc-
tion sequence, but an instruction sequence is not necessar-
ily a fragment of a program. In fact, we may distill (ran-
dom) instruction sequences from any binary strings (e.g.,
a GIF ﬁle). Two example instruction sequences are shown
in Figure 2: sequence 1 includes instructions 00 to 16 in
Figure 2(a); sequence 2 includes instructions 00 to 1d in
Figure 2(b).
STILL is more robust to anti-disassembly techniques
than previous work [8, 15, 28]. For example, previous work
exploits some heuristics to prune basic blocks. In [15], a
basic block is considered invalid in three cases: (1) if it
contains one or more invalid instructions; (2) if it is on a
path to an invalid block; (3) if it ends in a control transfer
instruction that jumps into the middle of another instruc-
tion. In [28], some similar heuristics are applied. STILL
does not use the ﬁrst two heuristics because invalid instruc-
tions could be the result of self-modifying obfuscation. For
example, the basic block (instructions from 00 to 1e in Fig-
ure 2), which ends with an invalid instruction, should not
be pruned. STILL does not use the third heuristic either be-
cause a control transfer instruction may jump into the mid-
dle of another instruction by using code overlap obfusca-
tion [23].
We note that in the presence of indirect jump and self-
modifying obfuscation, it is impossible to completely and
statically disassemble the entire body of the exploit code
embedded in a data stream using the recursive traversal al-
gorithm. Fortunately, the partially disassembled result may
already provide some strong evidence of self-modifying
and/or indirect jump behavior. In Figure 2(b), neither the
decryption routine (e.g., the loop instruction can no longer
be seen in the disassembly result) nor the original exploit
shellcode can be successfully disassembled. However, the
instructions from 00 to 1d indicate the self-modifying be-
havior. Our approach to detecting self-modifying and indi-
rect jump code in the next section will only use this partially
disassembled result as the input.
3.2 Detection of Self-modifying and Indi-
rect Jump Obfuscation Code
Once the (partial) instruction sequences have been ex-
tracted in the ﬁrst step (i.e., the disassembly process), the
next step is to determine whether they are real exploit code.
As we showed in Section 2, there are many techniques for
obfuscating shellcode. In this section, however, our discus-
sion will concentrate on detecting self-modifying and in-
direct jump exploit code, because these two types of ex-
ploit code can evade the detection of previous static anal-
ysis schemes [8, 15, 28] and are very challenging to detect.
Clearly, if STILL can detect these two types of exploit code,
it will also be capable of detecting branch function and
polymorphic code because branch function uses an indirect
jump to transfer control to the original target and polymor-
phic code is a kind of self-modifying code. In Section 4
we will show that STILL is rather effective in handling the
other types of obfuscation.
The new techniques in STILL to detecting self-
modifying and indirect jump exploit code are called static
taint analysis and initialization analysis. We observe that
self-modifying and indirect jump exploit codes ﬁrst need
acquire the absolute address of payload. Then, the abso-
lute address will be used in a certain way (referred to as
abstract semantics) reﬂecting self-modifying and indirect
jump behavior, whereas this behavior is very rare in random
instruction sequences. Accordingly, self-modifying and in-
direct jump exploit codes are detected as follows. First, the
variable which holds the absolute address of the payload is
found in the instruction sequences and used as a taint seed.
Then, static taint analysis is used to track the tainted values
and detect whether tainted data are used in the abstract se-
mantics that could indicate the presence of self-modifying
and indirect jump exploit code. Finally, we use initialization
analysis to reduce false positives.
292292
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:00:28 UTC from IEEE Xplore.  Restrictions apply. 
Taint Seed The absolute address of payload is used as a
taint seed in STILL. Both indirect jump and self-modifying
obfuscation code need the absolute address of payload, be-
cause the target for indirect jump and memory read/write for
self-modifying must use absolute addresses in IA-32 archi-
tecture [2]. They have to know their own absolute address
in order to jump within the payload or modify the payload at
runtime. There are two reasons that attackers want to get the
absolute address of payload at runtime rather than predict-
ing it or hardcoding it. First, the address of payload cannot
be predicted in most cases because exploit code is placed in
a dynamically changing stack or heap [23]. Second, even if
attackers can sometimes get the address, it is not good prac-
tice to hardcode it, especially in the case of a worm [25].
The absolute address of payload could be different for dif-
ferent versions and different patches of the same operating
system (e.g. Windows 2000 with Service Patch 1, Patch 2
and Windows XP Service Patch 1); hence, hardcoding the
absolute address could greatly limit the broad spread of a
worm.
The only way to get the absolute address of payload is to
read the PC (Program Counter) register, which stores the ab-
solute address of the next instruction to be executed. Since
the IA-32 architecture does not provide any instruction to
directly read PC, attackers have to acquire it at runtime. To
the best of our knowledge, currently only three ways (called
getPC) are used in the attacker community. First, attackers
may use a relative call. Whenever a call is executed, the re-
turn address is pushed into the stack just before the control
is transferred to the call target. Therefore, the top item of
the stack is used as the taint seed at the relative call target.
Second, the attackers can also get the address by using the
fstenv instruction, which saves the current FPU (Floating
Point Unit) operating environment at the memory location
speciﬁed by its operand [22]. The FPU operating environ-
ment includes the instruction pointer of the last executed
ﬂoat point instruction. Hence, we can also ﬁnd the taint seed
by checking a ﬂoat point instruction and a succeeding fstenv
instruction. Finally, attackers may also get the address by
using the structured exception handling (SEH) mechanism
of Windows [13]. However, as mentioned in [23], this tech-
nique is feasible only with older versions of Windows. In
this paper, we do not consider this case. Note that whenever
a new way for getting PC (if exist) is found, we can easily
add a corresponding method in STILL to ﬁnd the taint seed
while the rest parts of STILL will not be affected by this
update.
Static Taint Analysis After the taint seed is found, a new
static taint analysis approach is used to statically determine
which variables are tainted in an instruction sequence. A
taint seed itself is a tainted variable.
A tainted variable is propagated to a new tainted variable
by data transfer instructions that move data (e.g., push, pop,
move) and data operation instructions that perform arith-
metic or bit-logic operations on data (e.g., add, sub, xor) in
the IA-32 instruction set. Other instructions such as control
transfer instructions do not affect the taint process. For data
transfer instructions, the destination operand will be tainted
if and only if the source operand is tainted. For data op-
eration instructions, the destination operand will be tainted
if and only if either the source or the destination operand
is tainted. Note that for data movement and arithmetic in-
structions, constants are considered untainted.
Detection by Abstract Semantics Certain abstract seman-
tics can be observed from the tainted data of a real instruc-
tion sequence, whereas these abstract semantics are very
rare in random instruction sequences. Hence, if these ab-
stract semantics are detected through static taint analysis,
an alert will be raised. The following are the abstract se-
mantics for self-modifying and indirect jump, respectively.
Self-modifying Self-modifying obfuscation works in
three steps. First, it reads the payload; second, it modiﬁes
the read result; ﬁnally, it writes the modiﬁed result back to
the payload. Attackers may implement these three steps in
two ways. One way is to use a single updating instruction in
the payload to implement these three steps. Instruction 0c
in Figure 2(b) is such an example. The other way uses sev-
eral instructions, one instruction for reading payload, sev-
eral instructions for modifying the read results and one in-
struction for writing the modiﬁed results back to payload.
The CLET [12] shellcode generation engine uses this ap-
proach. Accordingly, there are two cases where tainted data
indicate self-modifying obfuscation. First, the tainted data
are used as the address of the updating instructions. Second,
the tainted data are used as the address of a memory read in-
struction or the address of a memory write instruction. We
note that the read result will be used to generate the write
result; therefore, we start a new taint analysis process to
taint the read result. If the newly tainted data are used as
the source operand of a memory write instruction, it clearly
indicates self-modifying obfuscation. Figure 3 shows these
two ways of identifying self-modifying code through static
taint analysis.
Indirect jump To detect indirect jump obfuscation, we
check whether tainted data are used as target addresses of
control transfer instructions such as branch, return, and
function call instructions. Normally, it is rare that tainted
data are used as jump targets in random instruction se-
quences.
Reducing False Positives Although for random instruction
sequences it is not common that the tainted data are used in
the same way as the way they are used by self-modifying
and indirect jump, we still ﬁnd some false positives in
our experiments. To reduce false positives, we further use
initialization analysis. We observed that the operands of
self-modifying and indirect jump code must be initialized.
293293
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:00:28 UTC from IEEE Xplore.  Restrictions apply. 
A1: X = getPC
     .
     .
     .
     .
     .
A2: Mem[Y] = f(Mem[Y])
B1: X = getPC
     .
     .
B2: Z1 = Mem[Y1])
     .
     .
B3: Mem[Y2] = Z2
(a)
(b)
Figure 3. (a) The tainted data are used as the addresses
of the updating instructions. Variable X is tainted at A1 at
the beginning. Variable Y is tainted by X and used as the
address of the updating instruction A2. (b) Attackers use a
memory read instruction to read payload, modify the read
result, and write the modiﬁed result back to the payload by
using a memory write instruction. Variable X is tainted at
B1 at the beginning. Variable Y1 is tainted by X and used
as the address of memory read instruction B2. Variable Y2
is tainted by X and used as the address of memory write
B3. Variable Z2 is tainted by Z1 and used as the source
operand of B3.
Speciﬁcally, target addresses of indirect jump should be ini-
tialized; the operands of memory updating or writing in-
structions in self-modifying code should be initialized. If
these operands are uninitialized, we will not consider them
as attacks.
We say a variable is initialized if it is deﬁned by a con-
stant or other initialized variables. It is hard for attackers to
know the run-time values of registers before malicious code
is executed. That is, their values are unpredictable to attack-
ers. Therefore, normally it is reasonable to assume that the
initial states of all registers are uninitialized at the begin-
ning (this assumption was also made by others [8, 23, 28],
implicitly or explicitly).