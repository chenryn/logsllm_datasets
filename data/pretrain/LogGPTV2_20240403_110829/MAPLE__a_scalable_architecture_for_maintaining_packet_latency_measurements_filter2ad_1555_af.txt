The second observation is that MAPLE (with true laten-
cies) achieves much higher (half to one order of magnitude
 0 0.2 0.4 0.6 0.8 110-310-210-1100CDFRelative errorPreﬁx /16HostFlowSub-ﬂowPacket 0 0.2 0.4 0.6 0.8 11-1011-100101-1000>1000 Compression ratioFlow sizeISPUNIV1UNIV2 0.7 0.75 0.8 0.85 0.9 0.95 1101102103104CDFAbsolute error (µs)SVBF-B0SVBF-B1SVBF-B2 0 0.2 0.4 0.6 0.8 110-1100101102103CDFAbsolute error (µs)MAPLERLIMAPLE-RLI112higher) accuracy than RLI. For instance, about 90% ﬂows
have less than 1µs absolute error with MAPLE, but RLI
only has 50% ﬂows with such absolute error under moderate
utilization scenario (not shown for brevity). Put diﬀerently,
in Figure 9, MAPLE achieves 5× less median error than
RLI. This shows that MAPLE, if implemented, may provide
more accurate latency estimates than RLI, even with the
approximations in the storage data structure. Of course, if
actual packet latencies are available to RLI, there is no need
for interpolation in RLI, and per-ﬂow latencies are trivially
obtained with 100% accuracy. So, we do not discuss the
obvious case.
6.
IMPLEMENTATION
We envision that the streaming k-medians algorithm will
be implemented in software. We assume there is an ad-
ditional processor (or core) devoted to implementing this
architecture. We assume that this core will perform the
streaming k-medians on the sampled data. There have been
some prior eﬀorts [29] on implementing k-medians directly in
hardware that we can also leverage. In environments where
there is not enough processing capacity, we can rely on the
static clustering approach we discussed in §3. This will how-
ever yield worse accuracy than the hybrid clustering.
The actual storage data structure (SVBF) outlined in
Figure 2, will need to be implemented in hardware in high-
speed SRAM. Bloom ﬁlters in general require simple hashing
operations and updating bit maps and thus are amenable to
high-speed implementations (see [38] for example). For im-
plementing the hash functions, we can use the H3 [34] hash
functions or the BOB [24] ones that are amenable to easy
hardware implementations. We need to also maintain two
extra counters per center, one for tracking number, and the
other for sum of delays of all packets that map to a given
center. These counters will enable the reﬁned latency es-
timate heuristic (in §3.3) and the tie-breaking heuristic (in
§3.3.4). The SVBF data structure needs to be ﬂushed every
epoch to an oﬀ-chip storage, which can be either DRAM
or SSDs. Along with each SVBF, the associated k centers
for that particular epoch need to be stored. For smaller k,
this is only a small amount of extra storage. Depending on
the technological constraints such as the amount of available
high-speed memory and link speeds, the epoch size could be
determined.
Assuming an OC-192 interface, we have roughly 5 mil-
lion packets per second, for which we will require about 60
Mbits of memory per second (assuming 12 bits/packet). Of
course, this is assuming the interface is running at full capac-
ity, which is often not the case. Rather, latency spikes are
often generated by microburst typically occurring under low
average utilization [9]. Thus, if we assume 20% utilization,
we only require 12 Mbits of memory per second to capture
such latency spikes. 16 GB of DRAM (which is commodity
today) could be used to store packet latencies for almost 3
hours. Flash memory densities are even higher; today 256
GB SSDs are possible which will enable storing packet la-
tency state for 47 hours, which is enough time for network
operators to debug and process the information. Even for
100% utilization, the DRAM and SSD can sustain for 36
minutes and 9.5 hours respectively.
Queries will need to be handled in software. For each
query, depending on the approximate time of the packet in
the query, the appropriate SVBF (and the two neighbors just
in case) will be queried by the processor (or core) (perhaps
shared with the k-medians implementation). Only the words
corresponding to the hash indexes will need to be fetched
from the secondary memory (SSD or DRAM), which are
then looked up according to the algorithm outlined in §3.3.3.
7. RELATED WORK
There exists a lot of research in measuring per-hop laten-
cies, although in the wide-area context, where ISPs typi-
cally rely on injecting active probes and obtaining link or
hop latency statistics using tomographic approaches [14, 16,
39]. These approaches do not satisfy any of our high-ﬁdelity
measurement requirements in §2.1 and thus we need high-
ﬁdelity passive measurement mechanisms. In this regard, we
already discussed three prior approaches relevant to ours—
LDA [26], RLI [27] and Consistent NetFlow [28].
The idea of storing packet-level information has been pur-
sued in other prior contexts; trajectory sampling for identi-
fying packet trajectories in [17] and SPIE for IP traceback
in [37]. Neither provides latency estimates unfortunately,
although trajectory sampling could be augmented with a
timestamp, but only a small number of packets are sam-
pled at each router (see [26, 27] for comparison of these
approaches with trajectory sampling). SPIE on the other
hand stores only packets and not their associated times-
tamps; thus, a simple Bloom ﬁlter was suﬃcient there, while
we needed the clustering and SVBF in our setting.
The idea of ‘in-band’ diagnosis was proposed in NetRe-
play [8] and Orchid [33]. NetReplay proposes the idea of
replaying packets to collect feedback from the network. Or-
chid [33] also proposes the idea of in-band network trou-
bleshooting, where packets collect feedback from routers along
the path. Our approach, however is more focused on esti-
mating, storing and retrieving packet-level latency measure-
ments, and is complementary to these approaches.
Song et al. propose fast hash table [38] to provide constant
lookup time by exploiting counting bloom ﬁlter. Fast hash
table does not address large space requirement because of an
extra counting bloom ﬁlter and the need to store both packet
digest and its delay. Supporting membership check across
multiple groups is non-trivial for Bloom ﬁlter. Several data
structures [11, 13, 20] have been proposed to address this
problem. COMB [20] is a multi-group membership check
data structure that is highly relevant to our work and hence,
we discussed this in §3.3.
Our SVBF data structure shares some similarity with the
bit slicing idea proposed in the database community [35, 18].
Speciﬁcally, the concept of colocating bits corresponding to
diﬀerent centers to which a packet may potentially match
is similar to laying out bits corresponding to records in a
document on the disk, one of which a given keyword may
match to. The bit slicing idea may be applied to PBF to
make the lookup time complexity close to SVBF’s, but it
needs BF sizes to be uniform. One may think of normalizing
the sizes of all BFs, but that means each BF needs to be the
size of the largest BF, which is all packets in the worst case.
This is clearly a considerable wastage.
8. CONCLUSION
This paper proposed a scalable and ﬂexible measurement
architecture called MAPLE. The core of the architecture
consists of two novel mechanisms; a streaming clustering
113algorithm to cluster packet latencies into small number of
latency clusters in a streaming fashion, and a data structure
called SVBF to store packet latencies eﬃciently in a router.
In addition, it provides a ﬂexible query interface for net-
work operators to query the latency of individual packets.
Together, the architecture provides both ﬁne-grained as well
as ﬂexible latency measurements to help network operators
manage low-latency applications eﬃciently. Our evaluations
using a software prototype indicate that our architecture can
scale eﬃciently both in terms of storage needs as well as in
terms of query bandwidth.
Acknowledgments
The authors are indebted to Aditya Akella, our shepherd,
and the anonymous reviewers for comments on previous ver-
sions of this manuscript. This work was supported in part
by NSF Award CNS 0831647 and 1054788.
9. REFERENCES
[1] Cut-through and store-and-forward ethernet switching for
low-latency environments.
http://www.cisco.com/en/US/prod/collateral/switches/
ps9441/ps9670/white_paper_c11-465436.html.
[2] Data Center Fabric with Nanosecond Accuracy - Use
IEEE1588 PTP on Nexus 3000 Switches. http:
//www.cisco.com/en/US/prod/collateral\/switches/
ps9441/ps11541/white_paper_c11-690975.html.
[3] FocalPoint TDM Support. http://www.fulcrummicro.com/
product_library/applications/TDM_App_Note.pdf.
[4] OpenRTB API Speciﬁcation Version 2.0.
http://www.iab.net/media/file/OpenRTB_API_
Specification_Version2.0_FINAL.PDF.
[5] The C Clustering Library. http://bonsai.hgc.jp/
~mdehoon/software/cluster/software.htm.
[6] M. Al-Fares, S. Radhakrishnan, B. Raghavan, N. Huang,
and A. Vahdat. Hedera: Dynamic ﬂow scheduling for data
center networks. In USENIX/ACM NSDI, 2010.
[7] M. Alizadeh, A. Greenberg, D. A. Maltz, J. Padhye,
P. Patel, B. Prabhakar, S. Sengupta, and M. Sridharan.
Data center TCP (DCTCP). In ACM SIGCOMM, 2010.
[8] A. Anand and A. Akella. NetReplay: a new network
primitive. ACM SIGMETRICS Performance Evaluation
Review, 37, 2010.
for direct traﬃc observation. In IEEE/ACM Transactions
on Networking, 2000.
[18] C. Faloutsos and S. Christodoulakis. Signature Files: an
Access Method for Documents and Its Analytical
Performance Evaluation. ACM Transactions on
Information Systems, 2(4):267–288, Oct. 1984.
[19] A. Greenberg, J. R. Hamilton, N. Jain, S. Kandula,
C. Kim, P. Lahiri, D. A. Maltz, P. Patel, and S. Sengupta.
VL2: a scalable and ﬂexible data center network. In ACM
SIGCOMM, 2009.
[20] F. Hao, M. Kodialam, T. Lakshman, and H. Song. Fast
multiset membership testing using combinatorial bloom
ﬁlters. In IEEE Infocom, 2009.
[21] P. Indyk. A Sublinear Time Approximation Scheme for
Clustering in Metric Spaces. In IEEE FOCS, 1999.
[22] P. Indyk. Sublinear Time Algorithms for Metric Space
Problems. In ACM STOC, 1999.
[23] A. K. Jain and R. C. Dubes. Algorithms for clustering
data: Prentice-Hall, 1981.
[24] B. Jenkins. Algorithm alley. Dr. Dobb’s Journal, September
1997.
[25] D. E. Knuth. The Art of Computer Programming, Volume
II: Seminumerical Algorithms, 2nd Edition.
Addison-Wesley, 1981.
[26] R. R. Kompella, K. Levchenko, A. C. Snoeren, and
G. Varghese. Every MicroSecond Counts: Tracking
Fine-grain Latencies Using Lossy Diﬀerence Aggregator. In
ACM SIGCOMM, 2009.
[27] M. Lee, N. Duﬃeld, and R. R. Kompella. Not All
Microseconds are Equal: Fine-Grained Per-Flow
Measurements with Reference Latency Interpolation. In
ACM SIGCOMM, 2010.
[28] M. Lee, N. Duﬃeld, and R. R. Kompella. Two Samples are
Enough: Opportunistic Flow-level latency estimation using
Netﬂow. In IEEE Infocom, 2010.
[29] M. Leeser, J. Theiler, M. Estlick, and J. Szymanski. Design
tradeoﬀs in a hardware implementation of the k-means
clustering algorithm. In Sensor Array and Multichannel
Signal Processing Workshop, 2000.
[30] S. Lloyd. Least squares quantization in PCM. Information
Theory, IEEE Transactions on, 28(2):129–137, Mar. 1982.
[31] R. Martin. Wall street’s quest to process data at the speed
of light.
http://www.informationweek.com/news/infrastructure/
showArticle.jhtml?articleID=199200297.
[32] A. Meyerson. Online Facility Location. In IEEE FOCS,
2001.
[9] T. Benson, A. Akella, and D. A. Maltz. Network traﬃc
[33] M. Motiwala, A. Bavier, and N. Feamster. Network
characteristics of data centers in the wild. In
ACM/USENIX IMC, 2010.
[10] B. H. Bloom. Space/time trade-oﬀs in hash coding with
allowable errors. Communications of the ACM, 1970.
[11] F. Chang, F. Chang, and W. chang Feng. Approximate
Caches for Packet Classiﬁcation. In IEEE INFOCOM, 2004.
[12] M. Charikar, L. O’Callaghan, and R. Panigrahy. Better
Streaming Algorithms for Clustering Problems. In ACM
STOC, 2003.
[13] B. Chazelle, J. Kilian, R. Rubinfeld, and A. Tal. The
bloomier ﬁlter: an eﬃcient data structure for static support
lookup tables. In ACM SODA, 2004.
troubleshooting: An in-band approach. In USENIX NSDI,
2007.
[34] M. Ramakrishna, E. Fu, and E. Bahcekapili. Eﬃcient
hardware hashing functions for high performance
computers. IEEE Transactions on Computers, 46(12), Dec.
1997.
[35] C. S. Roberts. Partial-Match Retrieval via the Method of
Superimposed Codes. Proceedings of the IEEE,
67(12):1624–1642, Dec. 1979.
[36] A. Shieh, S. Kandula, A. Greenberg, and C. Kim. Seawall:
performance isolation for cloud datacenter networks. In
USENIX HotCloud, 2010.
[14] Y. Chen, D. Bindel, H. Song, and R. H. Katz. An Algebraic
[37] A. C. Snoeren, C. Partridge, L. A. Sanchez, C. E. Jones,
Approach to Practical and Scalable Overlay Network
Monitoring. In ACM SIGCOMM, 2004.
[15] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein.
Introduction to Algorithms. The MIT Press, 2nd edition,
2001.
[16] N. Duﬃeld. Simple network performance tomography. In
ACM/USENIX IMC, 2003.
[17] N. G. Duﬃeld and M. Grossglauser. Trajectory sampling
F. Tchakountio, B. Schwartz, S. T. Kent, and W. T.
Strayer. Single-packet IP traceback. IEEE/ACM
Transactions on Networking (ToN), 10, 2002.
[38] H. Song, S. Dharmapurikar, J. Turner, and J. Lockwood.
Fast Hash Table Lookup Using Extended Bloom Filter: An
Aid to Network Processing. In ACM SIGCOMM, 2005.
[39] Y. Zhao, Y. Chen, and D. Bindel. Towards unbiased
end-to-end network diagnosis. In ACM SIGCOMM, 2006.
114