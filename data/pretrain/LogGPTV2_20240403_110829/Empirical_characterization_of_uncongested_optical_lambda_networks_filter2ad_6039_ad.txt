yielding 
may have significant 
rely on accurate 
mate capacity 
sults when employed in conjunction 
on top of the inter-arrival 
measurement 
results. 
consequences. 
For example, 
measurements 
yield meaningless 
re­
structure 
spurious 
packet inter-arrival 
or available 
bandwidth 
batching 
with packet batching. 
3.5 Summary of Results 
Packet Number (Receiver 
Interrupt 
Throttling 
Off) 
Figure 7. Packet inter-arrival 
tion of packet number; 
NAPI disabled. 
time as a func­
for a  group of packets that arrive during 
time interval. 
switches 
routine). 
of two context 
service 
head consists 
half of the interrupt 
is to batch packets by parameterizing 
a single interrupt 
some specified 
an Interrupt 
the maximum number of interrupts 
the device supports 
by functioning 
sively interrupt-driven, 
low data rates, 
in NAPI [3] mode. Instead 
configuration 
Throttling 
For example, 
but switches 
to polling 
it, the kernel can take it one step further 
of being exclu­
a NAPI kernel is interrupt-driven 
at 
at high data rates. 
plus executing 
the top 
The typical 
solution 
the NIC to generate 
Intel NICs offer 
parameter 
issued per second. If 
that limits 
answer two general 
lambda network traffic. First, 
with re­
questions 
we show 
Our experiments 
spect to uncongested 
that loss occurs almost exclusively 
posed to within the network core, typically 
ceiver being over-run. 
are extremely 
ity end-hosts. 
sensitive 
In particular, 
we show that: 
Second, we show that measurements 
to the configuration 
of the commod­
at the end-hosts 
as op­
a result 
of the re­
• UDP loss is dependent 
upon both the size of socket 
buffers and DMA rings as well as the specifics 
of in­
terrupt 
affinity in the end-host 
network adapters. 
for a particular 
• TCP throughput 
decreases 
and an increase 
(data and acknowledgment) 
path length, 
gestion 
in determining 
variants 
algorithm 
the achievable 
are similar. 
control 
with an increase 
in packet 
loss, with an increase 
in 
in window size. The con­
is only marginally 
important 
throughput, 
as most TCP 
Packet batching 
techniques 
provide the benefit of an in­
the maximum achievable 
on our setup is approximately 
bandwidth 
disabled, 
Throttling 
For example, 
architecture. 
crease in maximum achievable 
commodity 
terrupt 
throughput 
directly 
trol experiments 
other. With NAPI enabled and Interrupt 
fault parameter 
put, as shown in Figure 6(b). By default, 
experiments 
terrupts 
Interrupt 
to a rate of 8000 per second. 
with end-hosts 
we achieved 
implement 
values, 
with NAPI and In­
TCP 
in con­
to each 
1.9Gbps, 
connected 
Throttling 
set to de­
Throttling 
by limiting 
the NICs in our 
around 3Gbps through­
in­
However, this does not mean that packet batching 
is 
kernels and 
To illustrate 
this, con­
Ethernet 
test 
metric provided 
enable batching 
even though vanilla 
by default. 
by high-end 
[2]--the packet inter-arrival 
ideal in all scenarios, 
drivers 
sider a standard 
products 
packet dispersion. 
we patched the Linux kernel to time-stamp 
packet as early as possible 
routine) 
clock cycles instead 
achieving 
mentation overwrites 
to return the 64-bit value of the TSC register. 
time-stamp 
with the CPU time stamp counter 
values to be monotonic 
(in the driver's 
(a validity 
every received 
service 
interrupt 
(TSC) that counts 
For the TSC 
requirement), 
• Built-in 
kernel NAPI and NIC Interrupt  Throttling 
they are detrimental 
im­
for la­
This reinforces 
the con­
measurements. 
although 
prove throughput, 
tency sensitive 
ventional 
of parameters, 
essary for the task at hand. 
wisdom that there is no "one-size-fits-all" 
set 
and careful 
parameter 
selection 
is nec­
we 
a 
paper [ 10], we proposed 
way to overcome poor end-to-end 
should note that in a previous 
practical 
In that work, we showed that using a perimeter 
(or  a performance 
improve end-to-end 
We achieved 
Correction 
which transparently 
segments 
enhancement 
throughput 
this through a combination 
stores and re-transmits 
requiring 
without 
in the face of packet loss. 
of Forward Error 
a sender retransmission 
to travel 
dropped TCP 
performance. 
middlebox 
proxy) can significantly 
To perform this type of measurements, 
time, also known as 
Although this paper limits itself 
to measurement, 
of the monotonic wall-clock,  thereby 
cycle (i.e. nanosecond) resolution. 
Our imple­
the SO_TIMESTAMPNS socket option 
(FEC) at line speed and TCP segment caching 
978-1-4244-7501-8/10/$26.00 
©2010 IEEE 
582 
DSN 2010: Marian et al. 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 13:56:42 UTC from IEEE Xplore.  Restrictions apply. 
2010 IEEEIIFIP International 
Conference 
on Dependable Systems & Networks (DSN) 
the 
did provide 
behavior, 
stability. 
and end-host 
into how the txqueuelen pa­
loss patterns 
insight 
though disregarding 
authors 
rameter (i.e. the length of the backlog queue between the 
IP layer and the DMA rx ring-made obsolete 
by NAPI) 
affects throughput 
In particular, 
of the txqueuelen are correlated 
ity. An equally interesting 
cross-traffic 
since they alter ACK delivery 
sion due to queueing 
the authors performed 
measurements 
frames. 
(e.g. ACK compres­
or loss). It is also worth noting that 
a set of tentative 
TCP performance 
on l OGbps links, using jumbo (9000-byte) 
was that reverse­
more than others, 
with more instabil­
some TCP variants 
observation 
larger values 
patterns 
affects 
Mogul et al. [25] investigated 
By contrast, 
relatively 
few works have investigated 
and the their ability 
the 
layer, and hence fail to make forward progress. 
and spend all available 
cycles 
noting that 
to uncon­
the 
service 
as a result of 
only to drop these packets at the 
routine 
when connected 
on end-hosts 
the interrupt 
effect of traffic patterns 
to handle such traffic, especially 
gested lambda networks. 
effect of high data rate traffic on the end-host, 
a machine would live-lock 
while handling 
packets being received, 
subsequent 
Consequently, 
tling have been widely adopted, 
enabled by default 
an interesting 
(produced 
Throttling) 
accurately 
ity and available 
time-stamped 
cause similar 
by NAPI possibly 
hinders active measurement 
estimating 
packet dispersion 
study looked at how "interrupt 
in conjunction 
in user-space, 
behavior 
NAPI [3] and on-board 
as packet batching. 
bandwidth 
in vanilla 
switches 
kernels. 
context 
tools that rely on 
to measure capac­
[28]. Since the packets were 
Throt­
NIC Interrupt 
to the point where they are 
On the other hand, 
at the receiver 
coalescence" 
with Interrupt 
Optical 
lambda networks 
provide 
high-bandwidth, 
interconnecting 
data centers 
massive quantities 
roles in the infrastructure 
and of traditional 
architectures 
semi­
throughout 
the 
of data. They 
both of cloud­
scientific, 
finan­
transit 
private 
world and transporting 
serve critical 
computing 
cial, defense, 
use our Cornell National 
thodically 
networks, 
send and receive 
Surprisingly, 
traffic. 
we observed 
and other enterprise 
LambdaRail 
users. In this work, we 
Ring testbed 
to me­
probe the end-to-end 
connected 
behavior 
commodity 
of such l OGbE 
to 
to powerful 
end-hosts 
significant 
penalties 
in end­
dependability 
in this sce­
packet loss at the receiving 
measuring 
and end-to-end 
host performance 
nario, consistently 
end-host 
rates. Moreover, 
subtle (and often default) 
hosts-socket 
affinity, and status of various packet batching 
even when traffic was sent at relatively 
by 
issues of these end­
such effects were readily 
configuration 
low data 
buffer size, TCP window size, NIC interrupt 
instigated 
techniques, 
across the entire network to reach the destination. 
we greatly 
wide-area 
both the performance 
and reliability 
[35]. 
increased 
storage 
using such a technique 
In fact, 
of 
4 Related Work 
amount of work aimed 
at large by analytical 
Measure­
mod­
measurements. 
have covered a broad range of met­
packet delay and packet loss behav­
(spacing) 
experienced 
by 
There has been a tremendous 
the Internet 
and empirical 
at characterizing 
eling, simulation, 
ments, in particular, 
rics, from end-to-end 
ior [ 1 1, 14], to packet dispersion 
back-to-back 
per-hop 
width, bulk transfer 
and other general 
has been little 
semi-private 
cal lambda networks. 
and end-to-end 
or dedicated 
capacity, 
networks 
packets [ 13], packet inter-arrival 
band­
capacity, 
end-to-end available 
time [20], 
achievable 
TCP throughput, 
traffic characteristics 
[ 16]. However, 
work aimed at characterizing 
uncongested 
there 
[32], like modern opti­
The need for instruments 
with which to perform such 
fashion 
of a myriad of 
between intrusiveness 
has led to the development 
de­
for convenience 
measurements 
tools [ 13,23, 18, 19,22, 30]. These tools are typically 
and often 
ployed in an end-to-end 
embody a tradeoff 
[29]. 
For example, 
while others rely on relatively 
packet pairs or packet trains. 
essential 
for example, we have saved significant 
with (and extending) 
some tools rely on self-induced 
and provide a solid foundation 
Tools like these have become 
time by working 
and accuracy 
the existing 
small probes consisting 
Iperf [33]. 
of 
for measurements; 
congestion, 
Internet 
For example, 
provide a snapshot 
measurements 
of the network at the time the measurements 
of the char­
are 
in its early days, the Internet 
was 
and the 
to vary over a wide 
acteristics 
performed. 
prone to erratic 
round-trip 
range of values [31]. Today, none of these issues remain, 
although 
time delays were observed 
other challenges 
have emerged. 
have been characterized 
as they 
Historically, 
networks 
its successor, 
NSFNET [ 16, 
[31] have all been the focus of 
Murray et al. [26] compared 
measurement 
tools on the l OGbE 
while Bullot et al. [ 12] evaluated 
the 
by means of the stan­
net­
production 
long-distance 
measurements. 
bandwidth 
became available-ARPANET, 
20], and the early Internet 
systematic 
end-to-end 
TeraGrid 
throughput 
dard Iperf, over high-speed, 
works of the time (from Stanford 
of Florida, 
and to University 
links of maximum throughput 
experiments 
backbone, 
of various 
TCP variants 
in Section 3.3. 
to Caltech, 
to University 
over OC-12 
of Manchester 
of 622Mbps)-similar 
to the 
However, unlike our experiments, 
Bullot et al. [ 12] fo­
cused on throughput 
ity (in terms of throughput 
ior while competing 
against 
oscillations), 
a sinusoidal 
and TCP behav­
UDP stream. Al-
and related 
metrics, 
like the stabil­
packet loss, duplication,  reordering, 
5 Conclusion 
978-1-4244-7501-8/10/$26.00 
©l01O IEEE 
583 
DSN 2010: Marian et al. 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 13:56:42 UTC from IEEE Xplore.  Restrictions apply. 
20lO IEEEIIFIP International 
Conference 
on Dependable Systems & Networks (DSN) 
alleviating 
observed 
problems 
[16] K. Claffy, G. C. Polyzos, and H. Braun. Traffic Characteris­
with no single configuration 
for all scenarios. 
As optical 
networking 
clock speeds of commodity 
plications 
will invariably 
study confronts the difficulty 
maximizing 
the performance 
data rates continue 
end-hosts, 
to outpace 
more end-to-end 
ap­
face similar 
issues. 
This empirical 
of reliably 
and consistently 
of such networks. 
tics of the T1 NSFNET Backbone. In INFOCOM '93. 
[17] M. Dobrescu, N. Egi, K. Argyraki, B.-g. Chun, K. Fall, 
A. Knies, M. Manesh, and S. Ratnasamy. 
G. Iannaccone, 
RouteBricks: Exploiting 
Routers. In Proceedings of SOSP, 2009. 
Parallelism 
to Scale Software 
[18] C. Dovrolis, 
and D. Moore. What Do Packet 
Dispersion 
Measure? In INFOCOM '01. 
P. Ramanathan, 
Techniques 
Acknowledgments 
[19] C. Dovrolis, P. Ramanathan, and D. Moore. Packet-
and a capacity-estimation 
methodol­
dispersion 
ogy. IEEElACM Trans. Netw., 12(6):963-977,2004. 
techniques 
and monitor the Cornell NLR Rings testbed: 
[21] V. Jacobson. 
Congestion 
avoidance and control. 
SIGCOMM 
and Ed Kiefer (Cornell 
In­
Comput. Commun. Rev., 25(1):157-187,
1995. 
Greg Boles, Brent Sweeny, and 
[22] M. Jain and C. Dovrolis. End-to-end 
available 
bandwidth: 
[20] S.  A. Heimlich. Traffic characterization 
of the NSFNET 
national backbone. SIGMETRICS  Perform.  Eval. 
Rev., 
18(1):257-258,1990. 
We would like to recognize 
the anonymous 
as well as the engineering 
reviewers 
staff that helped 
of 
Dan Eckstrom, 
Technologies); 
this manuscript 
establish 
Eric Cronise, 
formation 
Joe Lappa (National 
Parmelee 
staff of Cornell and National 
tions Centers. 
ment support 
(Cornell 
LambdaRail); 
Scott Yoest and Larry 
Computer Science Technical 
LambdaRail 
Staff); and 
Network Opera­
Intel and Cisco equip­
We also acknowledge 
and NSF TRUST and AFRL funding. 
References 
http://www . irqbalance. org/. 
[1] Irqbalance. 
[2] Ixia. http://www.ixiacom.com/. 
[3] NAPI. http://www .linuxfoundation. org/. 
[4] National LambdaRail. 
[5] NLR PacketNet Atlas. http://atlas.grnoc.iu. 
http://www.nlr.net!. 
edu/atlas.cgi?map_name=NLR%20Layer3. 
[6] Teragrid. 
[7] Think big with a gig: Our experimental 
http://teragrid. org/. 
fiber network. 
http://googleblog.blogspot.com/2010/02/ 
think-big-with-gig-our-experimental. 
html. 
[8] TeraGrid Performance 
Monitoring. 
https: I Inetwork. 
teragrid. org/tgperf 1,2005. 
[9] W. Allcock, J. Bester, J. Bresnahan, 
ing, and S. Tuecke. GridFrP: Protocol extensions 
for the Grid. GGF Document Series GFD, 20, 2003. 
[10] M. Balakrishnan, 
T. Marian, K. Birman, H. Weatherspoon, 
A. Chervenak, L. Lim­
to FrP 
and E. Vollset. 
Lambda Networks. In Proceedings of NSDI, 2008. 
Maelstrom: Transparent 
Error Correction 
for 
[11] J.-C. Bolot. End-to-end 
packet delay and loss behavior in 
the Internet. 
In Proceedings of SIGCOMM '93. 
[12] H. Bullot, R. L. Cottrell, 
and R. Hughes-Jones. 
of advanced TCP stacks on fast long-distance 
networks. In Proceedings of the International 
Networks, 2004. 
Protocols for Fast Long-Distance 
Workshop on 
[13] R. L. Carter and M. E. Crovella. Measuring bottleneck 
link speed in packet-switched 
28:297-318,1996. 
networks. Perform. Eval., 
27-
[14] I. Cidon, A. Khamisy, and M. Sidi. Analysis of Packet Loss 
Processes in High-Speed Networks. IEEE Transactions 
Information Theory, 39:98-108,1991. 
on 
[15] Cisco Systems. Buffers, Queues, and Thresholds 
on the Cat­
alyst 6500 Ethernet Modules, 2007. 
measurement methodology, 
TCP throughput. 
[23] R. Kapoor, L.-J. Chen, L. Lao, M. Gerla, and M. Y. Sanadidi. 
IEEElACM Tr. Net., 11(4):537-549,
2003. 
dynamics, and relation  with 
CapProbe: a simple and accurate capacity estimation 
tech­
nique. SIGCOMM Compo Comm. Rev., 34(4):67-78, 
2004. 
Center for Advanced Computing, Cor­
[24] D. A. Lifka. Director, 
nell University. 
Private Communication, 
[25] J. C. Mogul and K.  K. Ramakrishnan. 
2008. 
Eliminating 
receive 
in an interrupt-driven 
kernel. ACM Trans. Comput. 
livelock 
Syst., 15(3):217-252, 
1997. 
[26] M. Murray, S. Smallen, O. Khalili, 
and M. Swany. Compar­
ison of End-to-End Bandwidth Measurement Tools on the 
10GigE TeraGrid Backbone. In Proceedings of GRID '05. 
[27] J. Padhye, V. Firoiu, D. Towsley, and J. Kurose. Modeling 
a simple model and its empirical valida­
1998. 
TCP throughput: 
tion. SIGCOMM Compo Comm. Rev., 28(4):303-314,
[28] R. Prasad, M. Jain, and C. Dovrolis. Effects of Interrupt 
Coalescence 
on Network Measurements. 
In PAM'04. 
[29] R. S. Prasad, M. Murray, C. Dovrolis, 
and K. Claffy. Band­
Metrics, Measurement Techniques, 
and 
width Estimation: 
Tools. IEEE Network, 17:27-35,2003. 
[30] V. J. Ribeiro, R. 
H. Riedi, R. 
L. Cottrell. 
mation for Network Paths. In PAM'03 Workshop. 
Efficient Available 
pathChirp: 
G. Baraniuk, J. Navratil, 
and 
Bandwidth Esti­
[31] D. Sanghi, A. K. Agrawala, O. Gudmundsson, and B. N. 
Behavior on 
Assessment of End-to-end 
Jain. Experimental 
Internet. 
In Proc. IEEE INFOCOM '93. 
[32] S. C. Simms, G.  G. Pike, and D. Balog. Wide Area Filesys­
tern Performance 
Conference, 
2007. 
using Lustre on the TeraGrid. In Teragrid 
Evaluation 
production 
[33] A. Tirumala, F. Qin, J. Dugan, J. Ferguson, and K. Gibbs. 
Iperf - The TCPIUDP bandwidth measurement tool. 2004. 
[34] S. Wallace. Lambda Networking. 
Advanced Network Man­
agement Lab, Indiana University. 
[35] H. Weatherspoon, 
L. Ganesh, T. Marian, M. Balakrishnan, 
and K. Birman. Smoke and mirrors: Shadowing files at a 
geographically 
In Proceedings of FAST, Feb. 2009. 
remote location 
without loss of performance. 
[36] P. Wefel. Network Engineer, 
National Center For Supercom­
puting Applications 
Communication, 
2007. 
(NCSA), University 
of Illinois. 
Private 
978-1-4244-7501-8/10/$26.00 
©20lO IEEE 
584 
DSN 20lO: Marian et al. 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 13:56:42 UTC from IEEE Xplore.  Restrictions apply.