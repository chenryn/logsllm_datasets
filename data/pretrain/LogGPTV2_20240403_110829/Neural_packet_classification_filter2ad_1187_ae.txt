(cid:68)(cid:70)(cid:79)(cid:23)(cid:3)(cid:20)(cid:19)(cid:78)
(cid:68)(cid:70)(cid:79)(cid:23)(cid:3)(cid:20)(cid:19)(cid:19)(cid:78)
(cid:68)(cid:70)(cid:79)(cid:22)(cid:3)(cid:20)(cid:19)(cid:19)(cid:78)
(cid:68)(cid:70)(cid:79)(cid:21)(cid:3)(cid:20)(cid:19)(cid:19)(cid:78)
(cid:68)(cid:70)(cid:79)(cid:20)(cid:3)(cid:20)(cid:19)(cid:19)(cid:78)
(cid:68)(cid:70)(cid:79)(cid:24)(cid:3)(cid:20)(cid:19)(cid:19)(cid:78)
(cid:68)(cid:70)(cid:79)(cid:24)(cid:3)(cid:20)(cid:19)(cid:78)
(cid:73)(cid:90)(cid:20)(cid:3)(cid:20)(cid:78)
(cid:73)(cid:90)(cid:21)(cid:3)(cid:20)(cid:78)
(cid:73)(cid:90)(cid:22)(cid:3)(cid:20)(cid:78)
(cid:73)(cid:90)(cid:23)(cid:3)(cid:20)(cid:78)
(cid:73)(cid:90)(cid:24)(cid:3)(cid:20)(cid:78)
(cid:73)(cid:90)(cid:20)(cid:3)(cid:20)(cid:19)(cid:78)
(cid:73)(cid:90)(cid:21)(cid:3)(cid:20)(cid:19)(cid:78)
(cid:73)(cid:90)(cid:22)(cid:3)(cid:20)(cid:19)(cid:78)
(cid:73)(cid:90)(cid:23)(cid:3)(cid:20)(cid:19)(cid:78)
(cid:73)(cid:90)(cid:20)(cid:3)(cid:20)(cid:19)(cid:19)(cid:78)
(cid:73)(cid:90)(cid:24)(cid:3)(cid:20)(cid:19)(cid:78)
(cid:73)(cid:90)(cid:21)(cid:3)(cid:20)(cid:19)(cid:19)(cid:78)
(cid:73)(cid:90)(cid:22)(cid:3)(cid:20)(cid:19)(cid:19)(cid:78)
(cid:73)(cid:90)(cid:23)(cid:3)(cid:20)(cid:19)(cid:19)(cid:78)
(cid:73)(cid:90)(cid:24)(cid:3)(cid:20)(cid:19)(cid:19)(cid:78)
(cid:76)(cid:83)(cid:70)(cid:20)(cid:3)(cid:20)(cid:78)
(cid:76)(cid:83)(cid:70)(cid:20)(cid:3)(cid:20)(cid:19)(cid:78)
(cid:76)(cid:83)(cid:70)(cid:21)(cid:3)(cid:20)(cid:78)
(cid:76)(cid:83)(cid:70)(cid:21)(cid:3)(cid:20)(cid:19)(cid:78)
(cid:76)(cid:83)(cid:70)(cid:20)(cid:3)(cid:20)(cid:19)(cid:19)(cid:78)
(cid:76)(cid:83)(cid:70)(cid:21)(cid:3)(cid:20)(cid:19)(cid:19)(cid:78)
Figure 8: Classification time (tree depth) for HiCuts, HyperCuts, EffiCuts, and NeuroCuts (time-optimized). We omit four
entries for HiCuts and HyperCuts that did not complete after more than 24 hours.
Figure 9: Memory footprint (bytes per rule) used for HiCuts, HyperCuts, EffiCuts, and NeuroCuts (space-optimized). We omit
four entries for HiCuts and HyperCuts that did not complete after more than 24 hours.
6.3 Improving on EffiCuts
In Figure 10 we examine a set of 36 NeuroCuts trees (one tree for
each ClassBench classifier) generated by NeuroCuts with the Effi-
Cuts partition action. This is in contrast with the prior experiments
that selected trees optimized for either space or time alone. On this
36-tree set, there is a median space improvement of 29% relative to
EffiCuts; median classification time is about the same. This shows
that NeuroCuts is able to effectively incorporate and improve on
pre-engineered heuristics such as the EffiCuts top-level partition
function.
Surprisingly, NeuroCuts is able to outperform EffiCuts despite
the fact that NeuroCuts does not use multi-dimensional cut actions.
When we evaluate EffiCuts with these cut types disabled, the mem-
ory advantage of NeuroCuts widens to 67% at the median. This
suggests that NeuroCuts could further improve its performance
if we also incorporate multi-dimensional cut actions via paramet-
ric action encoding techniques [9]. It would also be interesting
to, besides adding actions to NeuroCuts, consider postprocessing
steps such as resampling that can be used to further improve the
stochastic policy output.
6.4 Neural Network Architecture
To better understand the influence of the neural network archi-
tecture on NeuroCuts performance, we conduct an ablation study
where the network size is reduced from 512x512 (hundreds of thou-
sands of parameters) all the way down to 16x16 (a couple hun-
dred parameters). We also consider the case where the network
is trivial and does not process the observation at all, similar to a
non-contextual bandit. For this study we run a single sweep across
only these architecture hyperparameters, keeping all the others
fixed, and use the simple partition method.
The results are shown in Figure 11. We observe that while the
larger 64x64 network consistently outperforms 16x16, at 512x512
performance starts to be impacted due to the larger number of
265
Neural Packet Classification
SIGCOMM ’19, August 19–23, 2019, Beijing, China
(cid:13)
(cid:70)
(cid:19)
(cid:69)
(cid:4)
(cid:17)
(cid:4)
(cid:21)
(cid:12)
(cid:4)
(cid:88)
(cid:82)
(cid:73)
(cid:81)
(cid:73)
(cid:90)
(cid:83)
(cid:86)
(cid:84)
(cid:81)
(cid:45)
(cid:4)
(cid:73)
(cid:71)
(cid:69)
(cid:84)
(cid:55)
(cid:21)
(cid:20)(cid:18)(cid:27)(cid:25)
(cid:20)(cid:18)(cid:25)
(cid:20)(cid:18)(cid:22)(cid:25)
(cid:20)
(a) NeuroCuts can build on the EffiCuts partitioner to generate trees up to 10× (90%)
more space efficient than EffiCuts. In this experiment NeuroCuts did as well or better
than EffiCuts on all 36 rule sets.
(cid:13)
(cid:70)
(cid:19)
(cid:69)
(cid:4)
(cid:17)
(cid:4)
(cid:21)
(cid:12)
(cid:4)
(cid:88)
(cid:82)
(cid:73)
(cid:81)
(cid:73)
(cid:90)
(cid:83)
(cid:86)
(cid:84)
(cid:81)
(cid:45)
(cid:4)
(cid:73)
(cid:81)
(cid:56)
(cid:77)
(cid:20)(cid:18)(cid:24)
(cid:20)(cid:18)(cid:22)
(cid:20)
(cid:17)(cid:20)(cid:18)(cid:22)
(cid:17)(cid:20)(cid:18)(cid:24)
(b) NeuroCuts with the EffiCuts partitioner generates trees with about the same time
efficiency as EffiCuts.
Figure 10: Sorted rankings of NeuroCuts’ improvement over
EffiCuts in the ClassBench benchmark. Here NeuroCuts is
run with only the EffiCuts partition method allowed. Posi-
tive values indicate improvements.
learnable parameters.3 Interestingly, while the bias-only network
did the worst, it still was able to generate reasonably compact
trees in many cases. This suggests that NeuroCuts may operate by
first learning a random distribution of actions that leads to a basic
solution, and then leveraging the capacity of its neural network to
specialize the action distribution to different portions of the rule
space.
6.5 Tuning Time vs Space
Finally, in Figure 12 we sweep across a range of values of c for
NeuroCuts with the simple partition method and log(x) reward
scaling. We plot the ClassBench median of the best classification
times and bytes per rule found for each classifier. We find that
classification time improves by 2× as c → 1, while the number of
bytes per rule improves 2× as c → 0. This shows that c is effective
in controlling the tradeoff between space and time.
7 RELATED WORK
Packet classification. Packet classification is a long-standing prob-
lem in computer networking. Decision-tree based algorithms are
a major class of algorithmic solutions. Existing solutions rely on
hand-tuned heuristics to build decision trees. HiCuts [13] is a pio-
neering work in this space. It cuts the space of each node in one
dimension to create multiple equal-sized subspaces to separate rules.
HyperCuts [47] extends HiCuts by allowing cutting in multiple di-
mensions at each node. HyperSplit [40] combines the advantages
3We note that these results might not hold for different hyperparameters, e.g., if
allowed longer training periods, larger networks may dominate.
266
(cid:20)(cid:78)
(cid:20)(cid:19)(cid:78)
(cid:20)(cid:19)(cid:19)(cid:78)
(cid:24)(cid:20)(cid:21)(cid:91)(cid:24)(cid:20)(cid:21)
(cid:25)(cid:23)(cid:91)(cid:25)(cid:23)
(cid:20)(cid:25)(cid:91)(cid:20)(cid:25)
(cid:69)(cid:76)(cid:68)(cid:86)(cid:16)(cid:82)(cid:81)(cid:79)(cid:92)
(cid:72)
(cid:85)
(cid:88)
(cid:87)
(cid:70)
(cid:72)
(cid:87)
(cid:76)
(cid:75)
(cid:70)
(cid:85)
(cid:36)
(cid:3)
(cid:78)
(cid:85)
(cid:82)
(cid:90)
(cid:87)
(cid:72)
(cid:49)
(cid:19)
(cid:19)(cid:17)(cid:24)
(cid:20)
(cid:20)(cid:17)(cid:24)
(cid:21)
(cid:21)(cid:17)(cid:24)
(cid:49)(cid:82)(cid:85)(cid:80)(cid:68)(cid:79)(cid:76)(cid:93)(cid:72)(cid:71)(cid:3)(cid:38)(cid:79)(cid:68)(cid:86)(cid:86)(cid:76)(cid:73)(cid:76)(cid:70)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:55)(cid:76)(cid:80)(cid:72)(cid:3)(cid:11)(cid:79)(cid:82)(cid:90)(cid:72)(cid:85)(cid:3)(cid:76)(cid:86)(cid:3)(cid:69)(cid:72)(cid:87)(cid:87)(cid:72)(cid:85)(cid:12)
Figure 11: Comparison of the mean best classification time
achieved by NeuroCuts across different network architec-
tures and groups of classifiers. The bias-only architecture
refers to a trivial neural network that does not process the
observation at all and emits a fixed action probability dis-
tribution (i.e., a pure bandit). Results are normalized within
classifier groups so that the best tree has a normalized time
of 1. Rulesets that did not converge to a valid tree were as-
signed a time of 100 prior to normalization.
(cid:23)(cid:19)
(cid:48)(cid:72)(cid:71)(cid:76)(cid:68)(cid:81)(cid:3)(cid:70)(cid:79)(cid:68)(cid:86)(cid:86)(cid:76)(cid:73)(cid:76)(cid:70)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:87)(cid:76)(cid:80)(cid:72)
(cid:48)(cid:72)(cid:71)(cid:76)(cid:68)(cid:81)(cid:3)(cid:69)(cid:92)(cid:87)(cid:72)(cid:86)(cid:3)(cid:83)(cid:72)(cid:85)(cid:3)(cid:85)(cid:88)(cid:79)(cid:72)
(cid:20)(cid:19)
(cid:19)
value of c
(cid:20)
Figure 12: The classification time improves by 2× as the time-
space coefficient c → 1, and conversely, number of bytes per
rule improves 2× as c → 0.
of rule-based space decomposition and local-optimized recursion to
guarantee worst-case classification time and reduce memory foot-
print. EffiCuts [55] introduces four heuristics, including separable
trees, tree merging, equal-dense cuts and node co-location, to re-
duce rule replication and imbalance cutting. CutSplit [29] integrates
equal-sized cutting and equal-dense cutting to optimize decision
trees. Besides decision-tree based algorithms, there are also other
algorithms proposed for packet classification, such as tuple space
search [49], RFC [12] and DCFL [53]. These algorithms are not as
popular as decision-tree based algorithms, because they are either
too slow or consume too much memory. There are also solutions
that exploit specialized hardware such as TCAMs, GPUs and FP-
GAs to support packet classification [17, 23, 31, 32, 41, 48, 50, 57].
Compared to existing work, NeuroCuts is an algorithmic solution
that applies Deep RL to generate efficient decision trees, with the
capability to incorporate and improve on existing heuristics as
needed.
Decision trees for machine learning. There have been several
proposals to use deep learning to optimize the performance of
decision trees for machine learning problems [20, 38, 59]. In these
settings, the objective is maximizing test accuracy. In contrast,
packet classification decision trees provide perfect accuracy by
SIGCOMM ’19, August 19–23, 2019, Beijing, China
Eric Liang, Hang Zhu, Xin Jin, and Ion Stoica
9 ACKNOWLEDGEMENTS
We thank our shepherd Kensuke Fukuda and the reviewers for their
valuable feedback. Hang Zhu and Xin Jin are supported in part by
NSF grants CRII-1755646 and CNS-1813487, Facebook Communica-
tions Networking Research Award, and Amazon AWS Cloud Cred-
its for Research Program. Eric Liang and Ion Stoica are supported in
part by NSF CISE Expeditions Award CCF-1730628, and gifts from
Alibaba, Amazon Web Services, Ant Financial, Arm, CapitalOne,
Ericsson, Facebook, Google, Huawei, Intel, Microsoft, Scotiabank,
Splunk and VMware.
This work does not raise any ethical issues.
Hyperparameter
Time-space coefficient c
Top-node partitioning
Reward scaling function f
Max timesteps per rollout
Max tree depth
Max timesteps to train
Max timesteps per batch
Model type
Model nonlinearity
Model hidden layers
Weight sharing between θ, θv
Learning rate
Discount factor γ
PPO entropy coefficient
PPO clip param
PPO VF clip param
PPO KL target
SGD iterations per batch
SGD minibatch size
Value