The approach to on-call described in this chapter serves as a guideline for all SRE teams in Google and is key to fostering a sustainable and manageable work environ‐ment. Google’s approach to on-call has enabled us to use engineering work as the pri‐mary means to scale production responsibilities and maintain high reliability and availability despite the increasing complexity and number of systems and services for which SREs are responsible.While this approach might not be immediately applicable to all contexts in which engineers need to be on-call for IT services, we believe it represents a solid model that organizations can adopt in scaling to meet a growing volume of on-call work.
132  |  Chapter 11: Being On-Call
CHAPTER 12
Effective Troubleshooting
Written by Chris JonesWritten by Chris Jones
Be warned that being an expert is more than understanding how a system is supposed to work. Expertise is gained by investigating why a system doesn’t work.
—Brian Redman
Ways in which things go right are special cases of the ways in which things go wrong.
—John AllspawTroubleshooting is a critical skill for anyone who operates distributed computing sys‐tems—especially SREs—but it’s often viewed as an innate skill that some people have and others don’t. One reason for this assumption is that, for those who troubleshoot often, it’s an ingrained process; explaining how to troubleshoot is difficult, much like explaining how to ride a bike. However, we believe that troubleshooting is both learn‐able and teachable.Novices are often tripped up when troubleshooting because the exercise ideally depends upon two factors: an understanding of how to troubleshoot generically (i.e., without any particular system knowledge) and a solid knowledge of the system. While you can investigate a problem using only the generic process and derivation from first principles,1 we usually find this approach to be less efficient and less effec‐tive than understanding how things are supposed to work. Knowledge of the system typically limits the effectiveness of an SRE new to a system; there’s little substitute to learning how the system is designed and built.1 Indeed, using only first principles and troubleshooting skills is often an effective way to learn how a system 	works; see Chapter 28.
133
Let’s look at a general model of the troubleshooting process. Readers with expertise in troubleshooting may quibble with our definitions and process; if your method is effective for you, there’s no reason not to stick with it.
TheoryTheory
Formally, we can think of the troubleshooting process as an application of the hypothetico-deductive method:2 given a set of observations about a system and a the‐oretical basis for understanding system behavior, we iteratively hypothesize potential causes for the failure and try to test those hypotheses.In an idealized model such as that in Figure 12-1, we’d start with a problem report telling us that something is wrong with the system. Then we can look at the system’s telemetry3 and logs to understand its current state. This information, combined with our knowledge of how the system is built, how it should operate, and its failure modes, enables us to identify some possible causes.Figure 12-1. A process for troubleshooting
2 Se 
3 Fo
134  e 
r in
|  2 Se e . 3 For instance, exported variables as described in Chapter 10.
Chapter 12: Effective TroubleshootingWe can then test our hypotheses in one of two ways. We can compare the observed state of the system against our theories to find confirming or disconfirming evidence. Or, in some cases, we can actively “treat” the system—that is, change the system in a controlled way—and observe the results. This second approach refines our under‐standing of the system’s state and possible cause(s) of the reported problems. Using either of these strategies, we repeatedly test until a root cause is identified, at which point we can then take corrective action to prevent a recurrence and write a postmor‐tem. Of course, fixing the proximate cause(s) needn’t always wait for root-causing or postmortem writing.Common Pitfalls
Ineffective troubleshooting sessions are plagued by problems at the Triage, Examine, and Diagnose steps, often because of a lack of deep system understanding. The fol‐lowing are common pitfalls to avoid:
• Looking at symptoms that aren’t relevant or misunderstanding the meaning of 	system metrics. Wild goose chases often result.• Misunderstanding how to change the system, its inputs, or its environment, so as 	to safely and effectively test hypotheses.
• Coming up with wildly improbable theories about what’s wrong, or latching on to causes of past problems, reasoning that since it happened once, it must be hap‐pening again.
• Hunting down spurious correlations that are actually coincidences or are correla‐	ted with shared causes.Fixing the first and second common pitfalls is a matter of learning the system in ques‐tion and becoming experienced with the common patterns used in distributed sys‐tems.  The third trap is a set of logical fallacies that can be avoided by remembering that not all failures are equally probable—as doctors are taught, “when you hear hoof‐beats, think of horses not zebras.”4 Also remember that, all things being equal, we should prefer simpler explanations.54 Attributed to Theodore Woodward, of the University of Maryland School of Medicine, in the 1940s. S . This works in some domains, but in some systems, entire classes of failures may be eliminable: for instance, using a well-designed cluster filesystem means that a latency problem is unlikely to be due to a single dead disk.5 Occam’s Razor; see . But remember that it may still be t that there are multiple problems; in particular, it may be more likely that a system has a number of co low-grade problems that, taken together, explain all the symptoms rather than a single rare problem that causes them all. Cf .
Theory  ee 
ire a 
he mm at 
|  case on
135Finally, we should remember that correlation is not causation:6 some correlated events, say packet loss within a cluster and failed hard drives in the cluster, share common causes—in this case, a power outage, though network failure clearly doesn’t cause the hard drive failures nor vice versa. Even worse, as systems grow in size and complexity and as more metrics are monitored, it’s inevitable that there will be events that happen to correlate well with other events, purely by coincidence.7Understanding failures in our reasoning process is the first step to avoiding them and becoming more effective in solving problems. A methodical approach to knowing what we do know, what we don’t know, and what we need to know, makes it simpler and more straightforward to figure out what’s gone wrong and how to fix it.
In PracticeIn Practice
In practice, of course, troubleshooting is never as clean as our idealized model sug‐gests it should be. There are some steps that can make the process less painful and more productive for both those experiencing system problems and those responding to them.
Problem ReportEvery problem starts with a problem report, which might be an automated alert or one of your colleagues saying, “The system is slow.” An effective report should tell you the expected behavior, the actual behavior, and, if possible, how to reproduce the behavior.8 Ideally, the reports should have a consistent form and be stored in a search‐able location, such as a bug tracking system. Here, our teams often have customized forms or small web apps that ask for information that’s relevant to diagnosing the particular systems they support, which then automatically generate and route a bug. This may also be a good point at which to provide tools for problem reporters to try self-diagnosing or self-repairing common issues on their own.It’s common practice at Google to open a bug for every issue, even those received via email or instant messaging. Doing so creates a log of investigation and remediation activities that can be referenced in the future. Many teams discourage reporting prob‐lems directly to a person for several reasons: this practice introduces an additional step of transcribing the report into a bug, produces lower-quality reports that aren’t6 Of
7 At
US an
8 It re
136   co
 lea
 sh d 2
may port
|  6 Of course, see .
	st, we have no plausible theory to explain why the number of PhDs awarded in Computer Science in the 	ould be extremely well correlated (r2 = 0.9416) with the per capita consumption of cheese, between 2000 and 2009: .
 be useful to refer prospective bug reporters to [Tat99] to help them provide high-quality problem s.Chapter 12: Effective Troubleshooting
visible to other members of the team, and tends to concentrate the problem-solving load on a handful of team members that the reporters happen to know, rather than the person currently on duty (see also Chapter 29).
Shakespeare Has a ProblemYou’re on-call for the Shakespeare search service and receive an alert, Shakespeare-BlackboxProbe_SearchFailure: your black-box monitoring hasn’t been able to find search results for “the forms of things unknown” for the past five minutes. The alert‐ing system has filed a bug—with links to the black-box prober’s recent results and to the playbook entry for this alert—and assigned it to you. Time to spring into action!Triage
Once you receive a problem report, the next step is to figure out what to do about it. Problems can vary in severity: an issue might affect only one user under very specific circumstances (and might have a workaround), or it might entail a complete global outage for a service. Your response should be appropriate for the problem’s impact: it’s appropriate to declare an all-hands-on-deck emergency for the latter (see Chap‐ter 14), but doing so for the former is overkill. Assessing an issue’s severity requires an exercise of good engineering judgment and, often, a degree of calm under pressure.Your first response in a major outage may be to start troubleshooting and try to find a root cause as quickly as possible. Ignore that instinct!Instead, your course of action should be to make the system work as well as it can under the circumstances. This may entail emergency options, such as diverting traffic from a broken cluster to others that are still working, dropping traffic wholesale to prevent a cascading failure, or disabling subsystems to lighten the load. Stopping the bleeding should be your first priority; you aren’t helping your users if the system dies while you’re root-causing. Of course, an emphasis on rapid triage doesn’t preclude taking steps to preserve evidence of what’s going wrong, such as logs, to help with subsequent root-cause analysis.Novice pilots are taught that their first responsibility in an emergency is to fly the air‐plane [Gaw09]; troubleshooting is secondary to getting the plane and everyone on it safely onto the ground. This approach is also applicable to computer systems: for example, if a bug is leading to possibly unrecoverable data corruption, freezing the system to prevent further failure may be better than letting this behavior continue.This realization is often quite unsettling and counterintuitive for new SREs, particu‐larly those whose prior experience was in product development organizations.
In Practice  |  137
Examine
We need to be able to examine what each component in the system is doing in order to understand whether or not it’s behaving correctly.Ideally, a monitoring system is recording metrics for your system as discussed in Chapter 10. These metrics are a good place to start figuring out what’s wrong. Graph‐ing time-series and operations on time-series can be an effective way to understand the behavior of specific pieces of a system and find correlations that might suggest where problems began.9Logging is another invaluable tool. Exporting information about each operation and about system state makes it possible to understand exactly what a process was doing at a given point in time. You may need to analyze system logs across one or many processes. Tracing requests through the whole stack using tools such as Dapper [Sig10] provides a very powerful way to understand how a distributed system is working, though varying use cases imply significantly different tracing designs [Sam14].Logging
Text logs are very helpful for reactive debugging in real time, while storing logs in a structured binary format can make it possible to build tools to conduct retrospective analysis with much more information.It’s really useful to have multiple verbosity levels available, along with a way to increase these levels on the fly. This functionality enables you to examine any or all operations in incredible detail without having to restart your process, while still allowing you to dial back the verbosity levels when your service is operating normally. Depending of the volume of traffic your service receives, it might be better to use stat‐istical sampling; for example, you might show one out of every 1,000 operations.A next step is to include a selection language so that you can say “show me operations that match X,” for a wide range of X—e.g., Set RPCs with a payload size below 1,024 bytes, or operations that took longer than 10 ms to return, or which called doSome thingInteresting() in rpc_handler.py. You might even want to design your logging infrastructure so that you can turn it on as needed, quickly and selectively.Exposing current state is the third trick in our toolbox. For example, Google servers have endpoints that show a sample of RPCs recently sent or received, so it’s possible to understand how any one server is communicating with others without referencing
9 Bu
138  t b
|  9 But beware false correlations that can lead you down wrong paths!
Chapter 12: Effective Troubleshootingan architecture diagram. These endpoints also show histograms of error rates and latency for each type of RPC, so that it’s possible to quickly tell what’s unhealthy. Some systems have endpoints that show their current configuration or allow exami‐nation of their data; for instance, Google’s Borgmon servers (Chapter 10) can show the monitoring rules they’re using, and even allow tracing a particular computation step-by-step to the source metrics from which a value is derived.Finally, you may even need to instrument a client to experiment with, in order to dis‐cover what a component is returning in response to requests.
Debugging Shakespeare
Using the link to the black-box monitoring results in the bug, you discover that the prober sends an HTTP GET request to the /api/search endpoint:
{
 ‘search_text’: ‘the forms of things unknown’}It expects to receive a response with an HTTP 200 response code and a JSON payload exactly matching:
[{ 
	"work": "A Midsummer Night's Dream", 	"act": 5, 
	"scene": 1, 
	"line": 2526, 
	"speaker": "Theseus" 
}]
The system is set up to send a probe once a minute; over the past 10 minutes, about half the probes have succeeded, though with no discernible pattern. Unfortunately, the prober doesn’t show you what was returned when it failed; you make a note to fix that for the future.Using curl, you manually send requests to the search endpoint and get a failed response with HTTP response code 502 (Bad Gateway) and no payload. It has an HTTP header, X-Request-Trace, which lists the addresses of the backend servers responsible for responding to that request. With this information, you can now exam‐ine those backends to test whether they’re responding appropriately.
DiagnoseDiagnose
A thorough understanding of the system’s design is decidedly helpful for coming up with plausible hypotheses about what’s gone wrong, but there are also some generic practices that will help even without domain knowledge.
In Practice  |  139
Simplify and reduceIdeally, components in a system have well-defined interfaces and perform known transformations from their input to their output (in our example, given an input search text, a component might return output containing possible matches). It’s then possible to look at the connections between components—or, equivalently, at the data flowing between them—to determine whether a given component is working prop‐erly. Injecting known test data in order to check that the resulting output is expected (a form of black-box testing) at each step can be especially effective, as can injecting data intended to probe possible causes of errors. Having a solid reproducible test case makes debugging much faster, and it may be possible to use the case in a non-production environment where more invasive or riskier techniques are available than would be possible in production.Dividing and conquering is a very useful general-purpose solution technique. In a multilayer system where work happens throughout a stack of components, it’s often best to start systematically from one end of the stack and work toward the other end, examining each component in turn. This strategy is also well-suited for use with data processing pipelines. In exceptionally large systems, proceeding linearly may be too slow; an alternative, bisection, splits the system in half and examines the communica‐tion paths between components on one side and the other. After determining whether one half seems to be working properly, repeat the process until you’re left with a possibly faulty component.Ask “what,” “where,” and “why”
A malfunctioning system is often still trying to do something—just not the thing you want it to be doing. Finding out what it’s doing, then asking why it’s doing that and where its resources are being used or where its output is going can help you under‐stand how things have gone wrong.10
10 In many respects, this is similar to the “Five Whys” technique [Ohn88] introduced by Taiichi Ohno to under‐	stand the root causes of manufacturing errors.140  |  Chapter 12: Effective Troubleshooting
Unpacking the Causes of a Symptom 
Symptom: A Spanner cluster has high latency and RPCs to its servers are timing out. Why? The Spanner server tasks are using all their CPU time and can’t make progress on all the requests the clients send.
Where in the server is the CPU time being used? Profiling the server shows it’s sort‐ing entries in logs checkpointed to disk.Where in the log-sorting code is it being used? When evaluating a regular expression against paths to log files.
Solutions: Rewrite the regular expression to avoid backtracking. Look in the code‐base for similar patterns. Consider using RE2, which does not backtrack and guaran‐tees linear runtime growth with input size.11
What touched it lastWhat touched it last
Systems have inertia: we’ve found that a working computer system tends to remain in motion until acted upon by an external force, such as a configuration change or a shift in the type of load served. Recent changes to a system can be a productive place to start identifying what’s going wrong.12Well-designed systems should have extensive production logging to track new ver‐sion deployments and configuration changes at all layers of the stack, from the server binaries handling user traffic down to the packages installed on individual nodes in the cluster. Correlating changes in a system’s performance and behavior with other events in the system and environment can also be helpful in constructing monitoring dashboards; for example, you might annotate a graph showing the system’s error rates with the start and end times of a deployment of a new version, as seen in Figure 12-2.1 In contrast to RE2, PCRE can require exponential time to evaluate some regular expressions. RE2 is av 	at .
12 [All15] observes this is a frequently used heuristic in resolving outages.
In Practice  aila
|  ble
141
Figure 12-2. Error rates graphed against deployment start and end timesManually sending a request to the /api/search endpoint (see “Debugging Shake‐speare” on page 139) and seeing the failure listing backend servers that handled the response lets you discount the likelihood that the problem is with the API frontend server and with the load balancers: the response probably wouldn’t have included that information if the request hadn’t at least made it to the search backends and failed there. Now you can focus your efforts on the backends—analyzing their logs, sending test queries to see what responses they return, and examining their exported metrics.Specific diagnoses
While the generic tools described previously are helpful across a broad range of prob‐lem domains, you will likely find it helpful to build tools and systems to help with diagnosing your particular services. Google SREs spend much of their time building such tools. While many of these tools are necessarily specific to a given system, be sure to look for commonalities between services and teams to avoid duplicating effort.Test and Treat
Once you’ve come up with a short list of possible causes, it’s time to try to find which factor is at the root of the actual problem. Using the experimental method, we can try to rule in or rule out our hypotheses. For instance, suppose we think a problem is caused by either a network failure between an application logic server and a database server, or by the database refusing connections. Trying to connect to the database with the same credentials the application logic server uses can refute the second hypothesis, while pinging the database server may be able to refute the first, depend‐142  |  Chapter 12: Effective Troubleshooting