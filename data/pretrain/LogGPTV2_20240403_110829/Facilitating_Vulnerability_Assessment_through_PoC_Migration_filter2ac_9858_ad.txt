40
40
40
373
6
6
6
6
7
6
7
7
1
7
19
15
19
9
5
5
8
5
13
0
9
9
19
16
0
3
17
40
40
40
350
5
0
0
0
0
0
0
0
6
0
0
0
0
0
0
1
1
1
0
2
3
2
0
2
0
0
0
0
0
0
23
0
0
0
0
0
1
0
0
0
0
0
6
0
13
0
3
0
3
5
15
5
6
0
1
19
16
4
0
0
0
97
11
6
6
6
7
7
7
7
7
7
19
15
19
9
5
9
9
9
18
12
16
17
19
19
18
19
21
40
40
40
444
30 CVEs
SUM
1 “Target Versions”: Target versions refer to those versions that need vulnerability affection assessment, which are detailed in Appendix-Table 6.
2 “Need PoC Migration”: These versions are vulnerable while the reference PoC inputs fail to trigger the target bugs, thus requiring PoC
migration.
As we can observe, the total number of target software versions is
470.
Based on the CVE reports gathered from NVD, among the 470
versions, 103 versions are listed as vulnerable. However, as a recent
work [18] has studied, the reported vulnerable versions could be
either overestimated or underestimated. As a result, in order to
establish the ground truth for our data set (i.e., the ground truth
about whether a vulnerability truly exists), we manually examine
the vulnerabilities in each version of the software used in our
evaluation and listed the number of truly vulnerable versions
in Table 2. In total, we manually confirmed 444 versions of
software vulnerable to the corresponding vulnerabilities. These
vulnerable versions contain 76.8% non-overlapping versions, as
specified in the NVD reports. This confirms the overestimating and
underestimating discovery by previous research [18].
5.2 Experiment Setup
We run all experiments on an individual virtual machine with 8G
memory and 2 CPU cores for each binary. The host of our virtual
machine has 24 GB memory and 12 CPU cores (Intel i7-9750H, 2.60
GHz per core). In our experiment, we run our tool (VulScope) for
8 hours while exploring the possibility of PoC migration.
Threshold for Crash Triage (𝑇ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑𝑐𝑟𝑎𝑠ℎ). As we mentioned
before, our migration techniques use a predefined threshold to
perform crash triage. The value selection of 𝑇ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑𝑐𝑟𝑎𝑠ℎ is a
trade-off between false positives (FP) and false negatives (FN). For
example, a low-value choice could cause our tool to mistakenly
treat an undesired crash on the target version as a success of PoC
migration (i.e., false positives). On the contrary, a higher value could
potentially cause our tool to consider a successful migration as an
unexpected termination (i.e., false negatives). To ensure VulScope
could correctly report the success of the migration, we favor a low
FP over a low FN. As such, we choose a relatively high value of 0.7.
5.3 Experiment Design
To evaluate the effectiveness and efficiency of our tool, we design
five experiments. Here, we summarize our experiment design below.
Experiment I. We first experiment with the reference PoC input’s
effectiveness across various target versions (as listed in Appendix-
Table 6). Through this experiment, we aim to answer the following
questions. For how many vulnerable versions, the reference PoC
input could directly generate a crash on the target versions? Among
these crashes, how many are the crashes we truly desire, and how
Session 12B: Analyzing Crashes and Incidents CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3308many are unexpected? For those non-crash versions, how many of
them require our tool VulScope for PoC migration?
Experiment II. The second experiment evaluates that, for those
cases where merely running a PoC input does not generate the
desired crash or falsely reports an unexpected termination as a
practice of successfully triggering the desired vulnerability, how
well our tool VulScope could successfully mutate a reference
PoC input and migrate it to vulnerable versions. Besides, we
also evaluate whether our proposed technique produces any
false positives (i.e., deeming an unsuccessful migration and the
corresponding crash as a successful practice). For those versions
that VulScope fails to migrate a PoC input, we finally study
whether those versions are truly non-vulnerable. If not, we aim to
understand the factors that fail VulScope.
Experiment III. We further evaluate the performance of the cross-
version trace alignment. For vulnerable target versions identified
by VulScope in Experiment II, we collect all trace alignment results
during PoC migration and crash triage for these versions. With
these results, we seek to answer the following questions. What
are the sizes of these traces? How common is code refactoring
(i.e., function merging/splitting, function renaming) in these traces?
How common is the source code change observed in these traces?
How effective and efficient is our trace alignment?
Experiment IV. As a fuzzing-based approach optimized to guide
a program to a target buggy site, we also design an experiment
to compare VulScope with the existing directed fuzzing approach.
To the best of our knowledge, the most representative directed
fuzzing tools are AFLGo [9] and Hawkeye [13]. Since Hawkeye
has not yet been publicly available, we choose AFLGo2 as the
baseline for our comparison. In addition to directed fuzzing, we also
compare VulScope with the most broadly used fuzzing tool – AFL3.
In this experiment, we select those versions of the software – listed
in Table 2 – the reference PoC of which cannot trigger the target
bug successfully or triage crashes correctly. As we will discuss later,
the total number of versions satisfying this requirement is 94.
It should be noted that both AFLGO and AFL do not have a
clue about whether a crash indicates the success of triggering the
desired vulnerability when used for PoC migration. For the directed
fuzzing tool AFLGO, it even requires manually specifying the buggy
site. As such, when performing the comparison mentioned above,
we manually mark the target buggy site in the corresponding
version of the software. When observing the crash on target
versions, we manually examine the crash’s root cause, determining
whether the crash indicates the success of the desired vulnerability
identification and that of PoC migration. According to the time
allocated to VulScope, we also set 8 hours for evaluating these
tools in migrating PoC inputs. Within the 8 hours, we also record
the time that each tool spends on generating the first target crash.
Experiment V. Last but not least, we also design an experiment
to assess how well VulScope could be used as a tool to verify a
vulnerable version of the software. To achieve this goal, we compare
our tool confirmed vulnerable versions with those listed in the
reports gathered from NVD. Through this comparison, we aim to
2https://github.com/aflgo/aflgo/tree/c2888eb4e6e236549be88d3850831e71d1f0ffa2
3https://github.com/mirrorer/afl/tree/2fb5a3482ec27b593c57258baae7089ebdc89043
Table 3: Effectiveness of VulScope in migrating PoC input
and performing crash triage.
Ground Truth
N2
P1
444
26
VulScope
TP
409
FP
0
TN
26
FN
35
FNR
7.9%
1 “P” specifies the versions that require our PoC migration and crash triage.
2 “N” indicates the non-vulnerable versions.
understand whether VulScope can be used to facilitate bug report
verification, finding those inaccurate vulnerable version claims.
5.4 Experiment Results
Experiment I. Table 2 shows the effect of merely replaying the
reference PoC input on various target versions of the same software.
As we can observe, 373 out of 470 target versions exhibit crashes and
the rest 97 versions do not while taking the corresponding reference
PoC input. For each group, we take a closer look and manually
analyze the source code of the crashed and non-crashed versions.
We discover that, among all the 373 crashing versions, 350 versions
successfully trigger the desired bug, and the remaining 23 versions
accidentally touch an unexpected bug. For those 97 non-crashed
versions, only 26 are no longer vulnerable to the corresponding
vulnerabilities. Recall that the goals of our tool are in two-folds. One
is to migrate a reference PoC input from one version to another if
the vulnerability still exists. The other is to triage the corresponding
crash on the target version and correctly distinguish the desired
crash from unexpected termination. As such, the number of versions
that require our tool’s facilitation is 444, including 71 non-crashed
vulnerable versions needed for PoC migration and 373 crashes
triggered by reference PoC inputs needed for accurate crash triage.
In total, this accounts for 94% of our test cases (444/470).
Experiment II. Table 3 summarizes the experiment result, quanti-
fying the effectiveness of our tool in migrating PoC inputs and per-
forming crash triage. As we can observe from the table, VulScope
generates 409 crashes and deems them as the consequence of
triggering the desired vulnerability. Among these 409 reported
successes of PoC migration and crash triage, we note that VulScope
introduces no false positives. We further check all the 409 versions
that are reported as vulnerable by VulScope. For each CVE, the
average amount of time/version between the earliest identified
version and the reference version is 48.1 months/13.1 versions.
Besides, Table 3 also shows that VulScope fails to generate
a crash indicating the trigger of the target vulnerability for
61 versions. Among these 61 versions, 26 versions are truly
unaffected to the corresponding vulnerabilities. It means that the
false negatives of VulScope are 35 (FNR: 7.9%=35/444).
To understand the 35 false negatives, we take a closer look,
manually analyzing the root cause hidden behind our tool’s failure.
We discover that failures can be categorized into three practices. ❶
The program is updated to take a new format of user input. Our
PoC migration technique cannot correctly mutate an input and
thus allow it to pass the newly added format sanity check. We note
9 out of 35 failure cases can be attributed to this practice. ❷ The
version change unintentionally adds a sanity check which restricts
the trigger of the desired vulnerability through the path observed
from the reference version. In order to trigger the vulnerability, one
Session 12B: Analyzing Crashes and Incidents CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3309Table 4: Performance of cross-version trace alignment.
Table 5: Comparison results of AFL, AFLGo, and VulScope.
Trace Size
TP
TN
FP
FN
FPR
FNR
(40, 36)
70
(9424, 9427)1 18116
(653, 2676)
612
(9738, 9619)1 16952
1664
(832, 836)
(186, 151)
280
4
15
2595
973
4