wider range of applications and parameters.
5.1 Experimental setup
Prototype. We emulate a CDN deployment with clients and back-
ends in geographically different locations. For rapid prototyping, we
implement our own asynchronous caching system in 1500 lines of
C++ code, using Boost.ASIO [4, 52]. Our architecture uses sharding
and a single thread per cache shard [5, 12, 22]. An overview of the
system architecture is depicted in Figure 13.
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Figure 14: Prototype results for different origin locations.
Figure 13: Architecture of our experimental prototype.
The client sends requests as 16B object IDs to the Load Balancer,
which forwards it to the Cache Shard corresponding to the object ID.
The shard’s thread performs a cache look-up. If the object is cached,
the request is resolved immediately by relaying a response back to
the client (a true hit). Else, the request is forwarded to the Flow Man-
ager, which maintains queues of outstanding requests separately
for each unique object ID.10 On receiving a request, if the object ID
is not mapped to an existing queue, the Flow Manager allocates a
new queue for the object and forwards the request to the Network
Manager (a miss). Else, the request is simply inserted at the tail of the
queue (a delayed hit). The Network Manager use a pool of threads
with long-running TCP connections to the backing stores. These
threads perform the actual fetch operation and relay the response to
the Flow Manager. The Flow Manager buffers the response, flushes
the request queue for the corresponding object ID, and issues a write
request to the appropriate cache shard. The cache is updated (based
on the specified caching policy), and the responses are sent to resolve
all queued client requests.
To achieve low latency and high concurrency, the system compo-
nents communicate using lock-free, single-producer single-consu-
mer queues. The system is capable of sustaining a throughput of 1.2M
requests/sec using 12 threads on an x86 server with 16GB of DRAM.
Cacheconfigurationandpolicies.Weusea64-wayset-associative
cache, with the total cache size set to 5% of the maximum number of
active concurrent objects (e.g, 67k cache entries overall for the CDN
trace from §2.3). For the purpose of our experiments, we fix the object
size to 1KB. We implement two policies: LRU, and LRU-mad, which
combines LRU’sTT N A(x) estimator and our AддDelay(x) estimator.
Traces. We use a busy period from the CDN trace from §2.3 which
contains 243M requests, 7.7M unique object IDs, a maximum of 1.3M
active concurrent objects, and an average inter-request time of 1 µs.
Setup. To emulate different Z values, we set up backing stores (using
GCP VMs) in three different locations around the world: The U.S.
West Coast (Los Angeles), Western Europe (the Netherlands), and
10We use separate request queues to avoid head-of-line blocking.
Figure 15: CDF of latencies in simulation versus real experiments.
East Asia (Singapore). For simplicity, we refer to these as Origin A,
with an RTT of 68ms (Z =68k), Origin B, with an RTT of 103ms (Z =
103k), and Origin C, with an RTT of 226ms (Z =226k), respectively.11
We deploy our CDN caching node on a server at CMU in Pittsburgh.
5.2 Prototype Evaluation on CDN Trace
What latency improvements does LRU-mad provide for our
wide area cache? To answer this question, we consider each of the
three backing stores independently, and measure the average request
latency provided by the two caching policies for the given workload.
Figure 14 shows the average latencies achieved using LRU-mad ver-
sus LRU. Overall, using LRU-mad, we see a 12.4%, 14.7%, and 18.3%
reduction in average latency for Origins A, B, and C, respectively.
As expected, LRU-mad’s benefit increases with Z.
Does the mad caching strategy still work if multiple, non-
uniform backing store latencies12 are involved? This differs
significantly from our offline formulation which only considered uni-
form latencies (i.e., a single Z value). We find that mad indeed works
wellinthemulti-backendscenario.Figure14showsa16.8%reduction
in average latency for this case. This result suggests that maintaining
per-object estimates of the backing store latency (instead of a single,
global average) is an important feature of the online strategy, since
it gives mad a higher degree of freedom in computing ranks.
What are the overheads of using mad? We discuss two kinds of
overheads associated with mad: memory and request latency. We
evaluate the memory overhead of two different implementations
of mad. Both implementations maintain 4 counters per object. Our
strawman implementation faithfully implements mad by persisting
these counters for both cached and uncached objects. However, in
a long-running caching system, this would require an unbounded
amount of memory. Our efficient implementation only stores the
11We remark that, although the backing store latencies are known a priori, we do
not explicitly provide this information to mad; instead, mad automatically computes
per-object estimates of backing store latencies at run-time.
12We map each object ID to a randomly-generated origin location, which places a third
of object IDs on each origin server. The distribution of requests is: 29% to Origin A, 39%
to Origin B, and 32% to Origin C.
TraceReplayerLoad BalancerLatencyLoggingCache ShardCache ShardCache ShardCache ShardFlow ManagerNetworkManagerClientCDN Caching NodeTo OriginsOrigin A(RTT: 68 ms)Origin B(RTT: 103 ms)Origin C(RTT: 223 ms)Multi-backend(Avg. RTT: 140 ms)0102030405012.35(±0.34)10.82(±0.08)19.13(±0.37)16.32(±0.01)40.68(±0.70)33.24(±0.14)24.71(±0.37)20.56(±0.03)LRULRU-MADAverage Latency (ms)0200400600800Request Latency (ms)0.000.250.500.751.00CDFLRU (Simulated)LRU (Actual)LRU-MAD (Simulated)LRU-MAD (Actual)050Magniﬁed1001502000.650.700.750.80SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Nirav Atre, Justine Sherry, Weina Wang, and Daniel S. Berger
counters for currently cached objects. Fortunately, we find that the
average latency provided by the efficient implementation never di-
verges from the strawman by more than 6% over the entire range of
Z values, across all traces. In fact, all results presented so far have
been using the efficient implementation. Our counters are 8B; so,
the overall overhead is 32B per cached object, which is comparable
to existing key value stores [22]. Our efficient implementation thus
has a memory overhead of just over 3% for small 1KB objects and
under 0.003% for objects in the MB range (e.g., video caching [42]).
We compare mad’s request latency to LRU, where eviction is a
constant-time operation (the entry to evict is always at the head
of a linked list). Evictions in mad require computing the rank(X)
function from §4.2 over all objects in the corresponding cache set.
While each computation is cheap, its complexity scales linearly with
the set-associativity of the cache in our naive implementation. This
leads to several microseconds of overhead, which is orders of magni-
tude lower than the latency of the backing store. We remark that this
small overhead can be further reduced using existing techniques.13
How accurately do our simulations reflect results in the wide
area? We use simulated results in §2.3 and in the following evalua-
tion sections. While our simulator models the effects of delayed hits,
it makes several simplifications. For example, it assumes that arrivals
neatlyfallintodiscretetimeslots,thatcachemanagementoperations
are instantaneous, and that network latencies are deterministic. We
validate these simulation results by comparing the latency distribu-
tion (CDF) measured with our prototype to simulations based on
averaged estimates of Z for Origin B (results for other origins are the
same). Figure 15 shows that the simulated latencies indeed closely
match the empirical measurements.
5.3 Simulation Results: Systems
Our prototype experiments focus on the CDN setting with a small
set of backing latencies and a single algorithm. We now return to our
delayed hits aware simulator to test three mad variants (LRU-mad,
LHD-mad, and ARC-mad) in the context of CDNs, network traces,
and storage traces.
How does mad help CDNs with other base algorithms and a
wider range of latencies? Figure 16 illustrates the performance
gains from combining AддDelay(x) with LRU, LHD, and ARC. The y-
axismeasurestherelativeimprovementinlatencybetweenLRU-mad
and LRU, LHD-mad and LHD, and ARC-mad and ARC. mad always
performs better than the baseline algorithm, suggesting that there is
no downside, from a latency minimization perspective, to adopting
mad– regardless of what ranking algorithm was used initially. As
with our LRU prototype, we see gains of 5-20% when latencies are
in the 10’s of milliseconds.
We also see that as Z reaches some extreme values – 1M or even
10M – the gains from mad increase dramatically. Today, these exam-
ples are only useful for an imaginary web user with a CDN cache on
the moon. However, they may serve as an estimate for the impact of
delayed hits on future workloads. Recall that Z does not represent
latency itself, but the ratio between latency to the backing store and
request inter-arrival time (§2). Hence, as link and request rates grow
by 10×, a Z value of 1M would only represent a 100ms latency for the
CDN. Nonetheless, these extreme values remain flawed estimators –
13Large-scale production systems achieve constant-time evictions using sampling
techniques [5], which can be immediately applied to an implementation of mad.
Figure 16: mad simulations for the CDN Trace.
Figure 17: mad simulations for the Network Trace.
Figure 18: mad simulations for the Storage Trace.
we expect that request arrival rates, their burstiness, and the number
of requested objects may all change in this time; these datapoints
are hence little more than an educated guess towards the future.
Canmadhelpnetworkswitchcaches?Asdiscussedin§1,wefirst
observed delayed hits in a programmable switch. Hence, we were sur-
prised to see the lowest gains with regard to practical caching scenar-
ios (recall Table 2). The 100ns DRAM latency we worried about had
a Z <1 given our 10Gbps network trace and our simulation suggests
essentially no performance gains for this scenario from using mad.
The only application where we would expect to see any gains is an
IDS with a reverse-DNS lookup, which we would expect to run in the
10s or 100s of milliseconds; the simulation here predicts latency gains
of 10-35%. Nonetheless, most IDSes which perform such lookups are
not inline, and hence we would not expect to see these latency gains
passed on to Internet users whose traffic is intercepted by the IDS.
Looking to the future and very high Z values, we see a tapering
off trend which we do not observe in the CDN scenario. As discussed
in our belatedly results, this tapering off in the network setting is
due to flows beginning and ending during the entire Z window; we
do not see this trend in the CDN or storage scenarios because objects
are much longer lived than a few milliseconds or even seconds. The
simulations are hence flawed for network traffic in this regard – in
practice, a switch would ‘hold’ the first SYN packet until its flow
context were fetched and subsequent packets would not arrive at
the switch until the SYN completed. We leave to future work a more
1(1 us)10(10 us)100(100 us)1K(1 ms)10K(10 ms)100K(100 ms)1M(1 s)10M(10 s)Z0204060%Latency ImprovementRelative to BaselinePolicyLRU-MADARC-MADLHD-MADIntra-DCProxyForward Proxy(Local DC)Forward Proxy(Remote DC)1(3 us)10(30 us)100(300 us)1K(3 ms)10K(30 ms)100K(300 ms)1M(3 s)10M(30 s)Z010203040%Latency ImprovementRelative to BaselinePolicyLRU-MADARC-MADLHD-MADReverse DNSDRAMRDMA1(30 us)10(300 us)100(3 ms)1K(30 ms)10K(300 ms)100K(3 s)Z010203040%Latency ImprovementRelative to BaselinePolicyLRU-MADARC-MADLHD-MADSSD ReadDisk ReadCross-DCFilesystemCaching with Delayed Hits
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Figure 19: Relative latency difference between LRU-mad and LRU
as a function of the cache size. Using Z =100K.
accurate model of network traffic and Z values where the arrival time
of packets is dependent on the time it takes to serve the first packet.
Can mad help distributed storage? Our storage trace has simi-
lar results to the CDN result; we see that in the millisecond range
we achieve gains between 3-30% from adopting mad, representing
improvements for wide-area or cross-datacenter storage systems.
However, when deployed intra-datacenter where network latencies
are in the microseconds and system latencies in the low milliseconds,
we would expect much more minimal gains of zero to a few percent.
Summary. Overall, our experiments suggest that the systems that
would benefit most from mad toady are CDNs and distributed stor-
age systems with high latencies to the backing store. While switch
workloads tend to be more bursty (resulting in higher gains for mad
even at relatively low Z values), few scenarios involve this latency
being passed on to end-users.
We note that there are several interesting properties of real sys-
tems that are not captured here. For instance, while mad may only
shave off a few ms worth of latency on each individual request, some
tasks, such as loading web pages, involve chains of serialized requests
(e.g. due to recursive dependencies in HTML or CSS elements [44]);
consequently, the overall impact (e.g. on page load time) may be more
significant. Similarly, fetching large objects from the backing store
may require multiple RTTs, exacerbating the effect of delayed hits.
Additionally, certain objects must be periodically purged from the
cache due to TTL expiration (e.g. cached DNS entries), introducing
an additional layer of complexity in the design of online algorithms.
We leave a more detailed investigation of these effects to future work.
5.4 Simulation Results: Analysis
We now present findings that are not tied to any particular system.
Impact of cache sizing: We evaluate how cache size impacts mad’s
improvements over traditional caching algorithms. Recall that we
measurethecachesizeasafractionofthepeaknumberofconcurrently-
active objects.14 We calculate the latency improvement of mad rel-
ative to LRU for all four scenarios while keeping Z fixed at Z =100k.
Figure 19 shows the results for cache sizes between 0.1% and 10%.
We find that mad’s improvement is around 20% for small caches
(<1%) in the CDN and online gaming scenarios. In the networking
scenario, mad’s improvement is between 20% and 43% (we fix Z to
demonstrate the effect of cache sizing, but we note that the chosen
value is higher than one would expect to see in a networked setting).
14Note that our cache size definition is motivated by networking applications where
flow state only needs to be tracked for ’active’ flows. Caching papers on CDNs
and storage systems typically express the cache size as a fraction of the working
set [5, 12, 41], which is orders of magnitude larger. The cache size numbers shown in our
graphs thus might look comparably large but they are based on a different denominator.
Figure 20: Like belatedly, mad prioritizes bursty objects.
Figure 21: Percent relative change in miss-rate between mad and
various baseline caching algorithms for Network, CDN, and Storage.
Finally, we see that mad’s improvement is highest in the storage
scenario, with a 26% to 50% lower latency than LRU.
mad prioritizes bursty objects, just like belatedly. We de-
scribed the intuition behind mad as prioritizing bursty objects, just
like belatedly. Nonetheless, we use aggregate delay rather than
true burstiness (Goh-Barabasi score) and we weigh aggregate delay
against time to next access. Hence it is worth asking – does our
intuition about burstiness indeed map on to why mad is doing well?
Figure 20 shows per-object latency gain (or loss) between LRU and
LRU-mad’s caching schedule for the Network trace. Much like Fig-
ure 10 illustrating belatedly’s correlation with burstiness, mad
prioritizes bursty objects as well.
Impactoncachemiss-rate.Asdescribedin§2.4,latency-minimizing
algorithms might in fact increase the overall miss-rate. Hence, we
quantify the impact of mad – an algorithm designed to minimize
latency – on the overall cache miss-rate (which in turn affects the
bandwidth consumption on the link to the backing store). Figure 21
depicts the relative change in miss-rates15 between mad and our
three baseline algorithms as a function of Z. Regions where mad
increases the miss-rate (i.e. performs worse than the baseline) are
highlighted in red. We find that, across all Z values and choice of
baseline algorithms, mad increases miss-rates by at most 10%16 for
the Network and Storage settings (+1.84% and +1.43% on average),
but almost always reduces miss-rates in the CDN setting (−1.89% on
average). We conclude that, depending on the workload, there is a
tradeoff between optimizing for latency and bandwidth.
15 M R(mad)−M R(Baseline)
16Note that this value represents a relative increase in miss-rate compared to the
baseline. In our experiments, the absolute difference in miss-rates never exceeds 1%.
×100%
M R(Baseline)
101100101Cache Size%01020304050%Latency ImprovementRelative to BaselineTraceNetworkCDNStorageOnline Game-10%0%10%LRU-MADARC-MADLHD-MAD-10%0%10%1101001K10K100K1M10MZ-10%0%10%%Relative Increase inMiss-Rate vs. BaselineNetworkCDNStorageSIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Nirav Atre, Justine Sherry, Weina Wang, and Daniel S. Berger
may affect many future requests (to the same object). By assum-
ing independence, cost-aware caching assumes that misses are
retrieved before another request to the same object arrives.
(2) Weighted, general, and other offline caching theory. This
group of algorithms [3, 6, 10, 14, 16–18, 61] considers offline
caching problems beyond Belady. Weighted caching is like cost-
aware caching, but using offline knowledge [17]. Caching for
variable object sizes optimizes hit ratios, but considers objects
that require a different number of bits to be stored in cache [3, 10].
General caching generalizes both by considering both weighted
and variably-sized objects at once [14, 18]. In general, these prob-
lems are NP-hard, except for weighted caching which can also
be approximated using a flow formulation.
The architecture community has a rich literature on implementing
non-blocking caches to handle multiple outstanding misses [1, 8, 35,
43, 53, 58] – a prerequisite for the occurrence of delayed hits. In ad-
dition, [49] considers the effect of correlated cache misses (different
from delayed hits, but in a similar vein) on Memory Level Parallelism
(MLP) performance in processors. Finally, we are aware of two prior
works [25, 56] which observe improved accuracy when accounting
for delayed hits in simulations of processor caches.
8 CONCLUSION
As we look forward to continuous increases in bandwidth and
throughput (e.g., in networks, memory, new storage technologies,
and CPU-interconnects), access latencies will become larger and
larger relative to request inter-arrivals, increasing the likelihood of
delayed hits. Indeed, we believe that the problem of delayed hits will
surface in almost any caching scenario sooner or later.
Our work constitutes a first step in recognizing and possibly mit-
igating the increased latencies created by this fundamental trend.
Nonetheless, as we discuss in §6, there remain many open questions
about incorporating delayed hits into practical caching schemes.
We look forward to future work in engaging with delayed hits as
we extend the theoretical literature and observe the importance of
delayed hits become more apparent in practical systems.
Ethics: This paper raises no ethical concerns.
9 ACKNOWLEDGEMENTS
We thank our shepherd, Anirudh Sivaraman and the anonymous
reviewersfortheirinsightfulcomments.WealsothankJalaniWilliams,
Peter Manohar, and Sai Sandeep Pallerla for helpful discussions re-
garding the underlying theory, and Nathan Beckmann for his feed-