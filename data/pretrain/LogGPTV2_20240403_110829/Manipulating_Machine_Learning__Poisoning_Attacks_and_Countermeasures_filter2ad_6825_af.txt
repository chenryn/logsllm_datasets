carlie et al. [12] discuss sanitization methods against time-
based anomaly detectors in which multiple micro-models are
built and compared over time to identify poisoned data. The
assumption in their system is that the attacker only controls
data generated during a limited time window.
In the machine learning and statistics communities, earliest
treatments consider the robustness of learning to noise, includ-
ing the extension of the PAC model by Kearns and Li [30],
as well as work on robust statistics [7], [27], [51], [55]. In
adversarial settings, robust methods for dealing with arbitrary
corruptions of data have been proposed in the context of
linear regression [11], high-dimensional sparse regression [10],
logistic regression [16], and linear regression with low rank
feature matrix [33]. These methods are based on assumptions
on training data such as sub-Gaussian distribution, independent
features, and low-rank feature space. Biggio et al. [5] pio-
neered the research of optimizing poisoning attacks for kernel-
based learning algorithms such as SVM. Similar techniques
were later generalized to optimize data poisoning attacks for
several other important learning algorithms, such as feature
selection for classiﬁcation [54], topic modeling [35], autore-
gressive models [1], collaborative ﬁltering [32], and simple
neural network architectures [38].
VII. CONCLUSIONS
We perform the ﬁrst systematic study on poisoning attacks
and their countermeasures for linear regression models. We
propose a new optimization framework for poisoning attacks
and a fast statistical attack that requires minimal knowledge
of the training process. We also take a principled approach
in designing a new robust defense algorithm that largely out-
performs existing robust regression methods. We extensively
evaluate our proposed attack and defense algorithms on several
datasets from health care, loan assessment, and real estate
domains. We demonstrate the real implications of poisoning
attacks in a case study health application. We ﬁnally believe
that our work will inspire future research towards developing
more secure learning algorithms against poisoning attacks.
ACKNOWLEDGEMENTS
We thank Ambra Demontis for conﬁrming the attack results
on ridge regression, and Tina Eliassi-Rad, Jonathan Ullman,
and Huy Le Nguyen for discussing poisoning attacks. We also
thank the anonymous reviewers for all the extensive feedback
received during the review process.
This work was supported in part by FORCES (Foundations
Of Resilient CybEr-Physical Systems), which receives support
from the National Science Foundation (NSF award numbers
CNS-1238959, CNS-1238962, CNS-1239054, CNS-1239166),
DARPA under grant no. FA8750-17-2-0091, Berkeley Deep
Drive, and Center for Long-Term Cybersecurity.
This work was also partly supported by the EU H2020
project ALOHA, under the European Union’s Horizon 2020
research and innovation programme (grant no. 780788).
31
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:36:53 UTC from IEEE Xplore.  Restrictions apply. 
REFERENCES
[1] S. Alfeld, X. Zhu, and P. Barford. Data poisoning attacks against
autoregressive models. In AAAI, 2016.
[2] M. Barreno, B. Nelson, R. Sears, A. D. Joseph, and J. D. Tygar.
Can machine learning be secure? In Proceedings of the 2006 ACM
Symposium on Information, computer and communications security,
pages 16–25. ACM, 2006.
[3] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. ˇSrndi´c, P. Laskov,
G. Giacinto, and F. Roli. Evasion attacks against machine learning at
test time. In H. Blockeel, K. Kersting, S. Nijssen, and F. ˇZelezn´y, editors,
Machine Learning and Knowledge Discovery in Databases (ECML
PKDD), Part III, volume 8190 of LNCS, pages 387–402. Springer Berlin
Heidelberg, 2013.
[4] B. Biggio, G. Fumera, and F. Roli. Security evaluation of pattern
IEEE Transactions on Knowledge and Data
classiﬁers under attack.
Engineering, 26(4):984–996, April 2014.
[5] B. Biggio, B. Nelson, and P. Laskov. Poisoning attacks against support
vector machines. In ICML, 2012.
[6] B. Biggio and F. Roli. Wild patterns: Ten years after the rise of
adversarial machine learning. ArXiv e-prints, 2018.
[7] E. J. Candes, X. Li, Y. Ma, and J. Wright. Robust principal component
analysis. Journal of the ACM, 58(3), 2011.
[8] N. Carlini and D. Wagner. Towards evaluating the robustness of neural
networks. In Proc. IEEE Security and Privacy Symposium, S&P, 2017.
[9] X. Chen, C. Liu, B. Li, K. Lu, and D. Song. Targeted backdoor
attacks on deep learning systems using data poisoning. ArXiv e-prints,
abs/1712.05526, 2017.
[10] Y. Chen, C. Caramanis, and S. Mannor. Robust high dimensional sparse
regression and matching pursuit. arXiv:1301.2725, 2013.
[11] Y. Chen, C. Caramanis, and S. Mannor. Robust sparse regression under
adversarial corruption. In Proc. International Conference on Machine
Learning, ICML, 2013.
[12] G. F. Cretu-Ciocarlie, A. Stavrou, M. E. Locasto, S. J. Stolfo, and A. D.
Keromytis. Casting out demons: Sanitizing training data for anomaly
sensors. In Proc. IEEE Security and Privacy Symposium, S&P, 2008.
[13] I. Csiszar and G. Tusnady.
Information geometry and alternating
minimization procedures. Statistics and Decisions, 1:205–237, 1984.
[14] N. Dalvi, P. Domingos, S. Sanghai, D. Verma, et al. Adversarial
classiﬁcation. In Proceedings of the tenth ACM SIGKDD international
conference on Knowledge discovery and data mining, pages 99–108.
ACM, 2004.
[15] D.
–
machine-learning-healthcare-applications/, 2016.
Faggella.
2017
Machine
learning
beyond.
and
healthcare
applications
https://www.techemergence.com/
[16] J. Feng, H. Xu, S. Mannor, , and S. Yan. Robust logistic regression and
classiﬁcation. In Advances in Neural Information Processing Systems,
NIPS, 2014.
[17] M. A. Fischler and R. C. Bolles. Random sample consensus: A paradigm
for model ﬁtting with applications to image analysis and automated
cartography. Communications of the ACM, 24(6):381–395, 1981.
[18] M. Fredrikson, S. Jha, and T. Ristenpart. Model inversion attacks that
exploit conﬁdence information and basic countermeasures. In Proceed-
ings of the 22nd ACM Conference on Computer and Communications
Security, CCS, 2015.
[19] M. Fredrikson, E. Lantz, S. Jha, S. Lin, D. Page, and T. Ristenpart.
Privacy in pharmacogenetics: An end-to-end case study of personalized
warfarin dosing. In USENIX Security, pages 17–32, 2014.
[20] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing
adversarial examples. arXiv:1412.6572, 2014.
[21] T. Gu, B. Dolan-Gavitt, and S. Garg. Badnets: Identifying vulnerabilities
in the machine learning model supply chain.
In NIPS Workshop on
Machine Learning and Computer Security, volume abs/1708.06733,
2017.
[22] S. Hao, A. Kantchelian, B. Miller, V. Paxson, and N. Feamster. PREDA-
TOR: Proactive recognition and elimination of domain abuse at time-of-
registration. In Proceedings of the 23rd ACM Conference on Computer
and Communications Security, CCS, 2016.
[23] P. Harsha. Senate committee examines the “dawn of artiﬁcial intelli-
gence”. Computing Research Policy Blog. http://cra.org/govaffairs/blog/
2016/11/senate-committee-examines-dawn-artiﬁcial-intelligence/, 2016.
[24] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical
Learning: Data Mining, Inference, and Prediction. Springer, 2009.
[25] L. Huang, A. D. Joseph, B. Nelson, B. I. Rubinstein, and J. Tygar.
Adversarial machine learning. In Proceedings of the 4th ACM workshop
on Security and artiﬁcial intelligence, pages 43–58. ACM, 2011.
[26] P. J. Huber. Robust estimation of a location parameter. Annals of
Statistics, 53(1):73–101, 1964.
[27] P. J. Huber. Robust statistics. Springer, 2011.
[28] Kaggle. House Prices: Advanced Regression Techniques. https://www.
kaggle.com/c/house-prices-advanced-regression-techniques. Online; ac-
cessed 8 May 2017.
[29] W. Kan. Lending Club Loan Data. https://www.kaggle.com/wendykan/
lending-club-loan-data, 2013. Online; accessed 8 May 2017.
[30] M. Kearns and M. Li. Learning in the presence of malicious errors.
SIAM Journal on Computing, 22(4):807–837, 1993.
[31] M. Kloft and P. Laskov. Security analysis of online centroid anomaly
detection. The Journal of Machine Learning Research, 13(1):3681–3724,
2012.
[32] B. Li, Y. Wang, A. Singh, and Y. Vorobeychik. Data poisoning attacks
In Advances In Neural
on factorization-based collaborative ﬁltering.
Information Processing Systems, pages 1885–1893, 2016.
[33] C. Liu, B. Li, Y. Vorobeychik, and A. Oprea. Robust linear regres-
In Proc. Workshop on Artiﬁcial
sion against training data poisoning.
Intelligence and Security, AISec, 2017.
[34] D. Lowd and C. Meek. Adversarial
In Proceedings of
the eleventh ACM SIGKDD international conference on Knowledge
discovery in data mining, pages 641–647. ACM, 2005.
learning.
[35] S. Mei and X. Zhu. The security of latent dirichlet allocation.
In
AISTATS, 2015.
[36] S. Mei and X. Zhu. Using machine teaching to identify optimal
training-set attacks on machine learners. In 29th AAAI Conf. Artiﬁcial
Intelligence (AAAI ’15), 2015.
[37] M. Mozaffari Kermani, S. Sur-Kolay, A. Raghunathan, and N. K. Jha.
Systematic poisoning attacks on and defenses for machine learning
in healthcare.
IEEE Journal of Biomedical and Health Informatics,
19(6):1893–1905, 2014.
[38] L. Mu˜noz-Gonz´alez, B. Biggio, A. Demontis, A. Paudice, V. Wongras-
samee, E. C. Lupu, and F. Roli. Towards poisoning of deep learning
algorithms with back-gradient optimization.
In B. M. Thuraisingham,
B. Biggio, D. M. Freeman, B. Miller, and A. Sinha, editors, 10th ACM
Workshop on Artiﬁcial Intelligence and Security, AISec ’17, pages 27–
38, New York, NY, USA, 2017. ACM.
[39] B. Nelson, M. Barreno, F. J. Chi, A. D. Joseph, B. I. Rubinstein, U. Saini,
C. Sutton, J. Tygar, and K. Xia. Exploiting machine learning to subvert
your spam ﬁlter.
In Proc. First USENIX Workshop on Large-Scale
Exploits and Emergent Threats, LEET, 2008.
[40] A. Newell, R. Potharaju, L. Xiang, and C. Nita-Rotaru. On the
practicality of integrity attacks on document-level sentiment analysis.
In Proc. Workshop on Artiﬁcial Intelligence and Security, AISec, 2014.
[41] J. Newsome, B. Karp, and D. Song. Paragraph: Thwarting signature
In Recent advances in intrusion
learning by training maliciously.
detection, pages 81–105. Springer, 2006.
[42] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and
A. Swami. The limitations of deep learning in adversarial settings.
In Proc. IEEE European Security and Privacy Symposium, Euro S&P,
2017.
[43] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami. Distillation
as a defense to adversarial perturbations against deep neural networks.
In Proc. IEEE Security and Privacy Symposium, S&P, 2016.
[44] R. Perdisci, D. Dagon, W. Lee, P. Fogla, and M. Sharif. Misleading
In Proc.
worm signature generators using deliberate noise injection.
IEEE Security and Privacy Symposium, S&P, 2006.
[45] PharmGKB. Downloads - IWPC Data.
https://www.pharmgkb.org/
downloads/, 2014. Online; accessed 8 May 2017.
[46] B. I. Rubinstein, B. Nelson, L. Huang, A. D. Joseph, S. hon Lau,
S. Rao, N. Taft, and J. D. Tygar. ANTIDOTE: Understanding and
defending against poisoning of anomaly detectors. In Proc. 9th Internet
Measurement Conference, IMC, 2009.
[47] R. Shokri, M. Stronati, C. Song, and V. Shmatikov. Membership
In Proc. IEEE
inference attacks against machine learning models.
Security and Privacy Symposium, S&P, 2017.
[48] N. Srndic and P. Laskov. Mimicus - Contagio Dataset. https://github.
com/srndic/mimicus, 2009. Online; accessed 8 May 2017.
[49] N. Srndic and P. Laskov. Practical evasion of a learning-based classiﬁer:
In Proc. IEEE Security and Privacy Symposium, S&P,
A case study.
2014.
32
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:36:53 UTC from IEEE Xplore.  Restrictions apply. 
[50] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Good-
Intriguing properties of neural networks.
fellow, and R. Fergus.
arXiv:1312.6199, 2014.
[51] D. E. Tyler. Robust statistics: Theory and methods. Journal of the
American Statistical Association, 103(482):888–889, 2008.
[52] S. Venkataraman, A. Blum, and D. Song. Limits of learning-based
In Network and Distributed
signature generation with adversaries.
System Security Symposium, NDSS. Internet Society, 2008.
[53] G. Wang, T. Wang, H. Zheng, and B. Y. Zhao. Man vs. machine:
Practical adversarial detection of malicious crowdsourcing workers. In
23rd USENIX Security Symposium (USENIX Security 14), San Diego,
CA, 2014. USENIX Association.
[54] H. Xiao, B. Biggio, G. Brown, G. Fumera, C. Eckert, and F. Roli. Is
feature selection secure against training data poisoning? In Proc. 32nd
International Conference on Machine Learning, volume 37 of ICML,
pages 1689–1698, 2015.
[55] H. Xu, C. Caramanis, and S. Mannor. Robust regression and Lasso.
IEEE Transactions on Information Theory, 56(7):3561–3574, 2010.
APPENDIX A
THEORETICAL ANALYSIS OF LINEAR REGRESSION
We prove the equivalence of Wtr and W(cid:3)
tr with the follow-
ing theorem.
Theorem 3. Consider OLS regression. Let Dtr = {X, Y }
be the original dataset, θ0 = (w0, b0) the parameters of the
tr = {X, Y (cid:3)} the dataset where Y (cid:3)
original OLS model, and D(cid:3)
consists of predicted values from θ0 on X. Let Dp = {Xp, Yp}
be a set of poisoning points. Then
arg minθ L(Dtr ∪ Dp, θ) = arg minθ L(D(cid:3)
tr ∪ Dp, θ)
tr
∂z =
∂W(cid:2)
Furthermore, we have ∂Wtr
∂z , where z = (xc, yc).
Then the optimization problem for the adversary, and the
gradient steps the adversary takes, are the same whether Wtr
or W(cid:3)
Proof. We begin by showing that
tr is used.
arg minθ L(Dtr, θ) = arg minθ L(D(cid:3)
tr, θ).
By deﬁnition, we have θ0 = arg minθ L(Dtr, θ). In Y (cid:3),
i = f (xi, θ0), so L(D(cid:3)
tr, θ0) = 0. But L ≥ 0, so
y(cid:3)
θ0 = arg minθ L(D(cid:3)
tr, θ).
We can use this to show that XT Y = XT Y (cid:3). Recall that the
closed form expression for OLS regression trained on X, Y is
−1XT Y . Because θ0 is the OLS model for both
θ = (XT X)
Dtr,D(cid:3)
tr , we have
T
−1
−1
T Y (cid:3)
),
T Y ) = (X
T
T
X)
X)
(X
(X
(X
−1 is invertible, so XT Y = XT Y (cid:3). We can use
but (XT X)
this to show that arg minθ L(Dtr∪Dp, θ) = arg minθ L(D(cid:3)
tr∪
Dp, θ) for any Dp. Consider the closed form expression for
the model learned on Dtr ∪ Dp:
T Y + X
T
p Xp)
T
p Yp)
which is exactly the model learned on D(cid:3)
tr∪Dp. So the learned
models for the two poisoned datasets are the same. Note that
this also holds for ridge regression, where the Hessian has a
λI term added, so it is also invertible.
T
p Yp) = (X
(X
X + X
T Y (cid:3)
T
p Xp)
X + X
+ X
(X
(X
−1
T
−1
We proceed to use XT Y = XT Y (cid:3) again to show that