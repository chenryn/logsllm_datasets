0.00003
0.29
0.61
0
0
0.06
0.0018
0
0
0
0
1
1
1
1
0.375
0.25
nor respond to incoming DNS queries, highlighting the amount of unwanted DNS
traﬃc that targets enterprise hosts for scanning or DoS purposes.
To better understand the target of these potentially malicious queries,
we analyze unanswered incoming queries over a day for the two enterprises.
Figure 3(b) is the CCDF of total incoming unanswered queries per each host
for the university campus. It is seen that two hosts of the university campus
receive more than a million DNS queries over a day from the Internet with no
response sent back, whereas one host in the research institute has the similar
behaviour. By reverse lookup, we found that the University hosts are a DHCP
server and a web server that respectively received 9.4M and 4.4M unanswered
queries (together contributing to 72% of red shaded area in Fig. 2(a)).
Furthermore, we analyzed the question section of unanswered incoming
queries that originated from a distributed set of IP addresses. Surprisingly, in
the University dataset we found that 72% of domains queried were irrelevant
to its zone (e.g., 47% for “nist.gov”, 5% for“svist21.cz”, and even 2% for
“google.com”), and in the research institute dataset we found 84% of domains
queried were irrelevant to its zone (e.g., 8% for “qq.com”, 7% for“google.com”,
and 5% for “com”).
Considering outgoing responses (shown in Fig. 2(b) for the university net-
work), there are 68 hosts in the campus network (shown by the red shade) and 21
hosts in the research network that respond to incoming DNS queries in addition
to name servers (i.e., hosts C and D). We have veriﬁed (by reverse lookup) that
all hosts that generate “no Error” responses are authoritative for sub-domains of
their organization zone. We also note that some hosts that reply with “Refused”,
“Name Error” and “Server Failure” ﬂags to some irrelevant queries (e.g., com)
– these are secondary name servers.
2.2 Attributes
Following the insights obtained from DNS behavior of various hosts, we now
identify attributes that help to automatically (a) map a given host to its function,
including authoritative name server, recursive resolver, mixed DNS server (i.e.,
Mapping an Enterprise Network by Analyzing DNS Traﬃc
135
Fig. 4. Automatic classiﬁcation and ranking of enterprise hosts.
both name server and recursive resolver), a (non-DNS) public-facing server, or
a regular client; and (b) rank the importance of servers.
Dataset Cleansing. We ﬁrst clean our dataset by removing unwanted (or mali-
cious) records including unsolicited responses and unanswered queries. This is
done by correlating the transaction ID of responses with the ID of their corre-
sponding queries. In the cleaned dataset, incoming responses are equal in number
to outgoing queries, and similarly for the number of incoming queries and out-
going responses.
Functionality Mapping. As discussed in Sect. 2.1, recursive resolvers are
very active in terms of queries-out and responses-in, whereas name servers
behave the opposite with high volume of queries-in and responses-out. Hence,
a host attribute deﬁned by the query fraction of all outgoing DNS packets
( QryFraqOut) should distinguish recursive resolvers from name servers. As
shown in Table 1, this attribute has a value close to 1 for recursive resolvers and
a value close to 0 for name servers.
In addition to recursive resolvers, there are some end-hosts conﬁgured to use
public resolvers (e.g., 8.8.8.8 of Google) that have a non-zero fraction of DNS
queries out of the enterprise network. We note that these end-hosts ask a limited
number of Internet servers during their activity period whereas the recursive
resolvers typically communicate with a larger number of external servers. Thus,
we deﬁne a second attribute as the fraction of total number of external servers
queried ( numExtSrv) per individual enterprise host. As shown in Table 1, the
value of this attribute for end-hosts is much smaller than for recursive resolvers.
Similarly for incoming queries, we consider a third attribute as the fraction
of total number of external hosts that initiate query in ( numExtClient) per
individual enterprise host. Indeed, this attribute has a larger value for name
servers compared with other hosts, as shown in Table 1.
Lastly, to better distinguish between end-hosts and recursive resolvers (high
and low proﬁle servers), we deﬁne a fourth attribute as the fraction of active
hours for outgoing queries ( actvTimeFrac). Regular clients have a smaller
value of this attribute compared with recursive resolvers and mixed DNS servers,
as shown in Table 1.
136
M. Lyu et al.
We note that public-facing (non-DNS) servers typically do not have DNS
traﬃc in/out of the enterprise networks. To identify these hosts, we analyzed
the answer section of A-type outgoing responses.
Importance Ranking. Three diﬀerent attributes are used to rank the impor-
tance of name servers, recursive resolvers, and (non-DNS) public-facing servers
respectively. Note that we rank mixed DNS servers within both name servers
and recursive resolvers for their mixed DNS behaviour.
For recursive resolvers, we use QryFracHost deﬁned as the fraction of out-
going queries sent by each host over the cleaned dataset. And for name servers,
we use RespFracHost as the fraction of outgoing responses sent by each host.
For other public-facing servers, we use RespCount as the total number of out-
going responses that contain the IP address of a host – external clients that
access public-facing servers obtain the IP address of these hosts by querying the
enterprise name servers.
3 Classifying Enterprise Hosts
In this section, we ﬁrstly develop a machine learning technique to determine if an
enterprise host with a given DNS activity is a “name server”, “recursive resolver”,
“mixed DNS server”, or a “regular end-host”. We then detect other public-facing
(non-DNS) servers by analyzing the answer section of A-type outgoing responses.
Finally, we rank the enterprise server assets by their importance.
Our proposed system (shown in Fig. 4) automatically generates lists of active
servers into three categories located inside enterprise networks, with the real-
time DNS data mirrored from the border switch of enterprise networks. The
system ﬁrst performs “Data cleansing” that aggregates DNS data into one-
day granularity and removes unsolicited responses and unanswered queries (i.e.,
step 1); then “Attribute extraction” in step 2 computes attributes required
by the following algorithms; “Server mapping” in step 3 detects DNS servers
and other public-facing servers; and ﬁnally “server ranking” in step 4 ranks
their criticality. The output is a classiﬁcation and a ranked order of criticality,
which an IT manager can then use to accordingly adjust security policies.
3.1 Host Clustering Using DNS Attributes
We choose unsupervised clustering algorithms to perform the grouping and clas-
siﬁcation process because they are a better ﬁt for datasets without ground truth
labels but nevertheless exhibit a clear pattern for diﬀerent groups/clusters.
Selecting Algorithm. We considered 3 common clustering algorithms, namely
Hierarchical Clustering (HC), K-means and Expectation-maximization (EM).
HC is more suitable for datasets with a large set of attributes and instances that
Mapping an Enterprise Network by Analyzing DNS Traﬃc
137
Table 2. University campus: host clusters (3 May 2018).
Count QryFracOut numExtSrv numExtClient actvTimeFrac
Name server
Recursive resolver
Mixed DNS server
42
14
14
0.0057
0.99
0.57
End-host
2195
1
1e-5
0.06
0.01
2e-5
0.02
0
0.02
0
0.03
0.94
0.66
0.31
Table 3. Research institute: host clusters (3 May 2018).
Count QryFracOut numExtSrv numExtClient actvTimeFrac
Name server
Recursive resolver
Mixed DNS server
12
4
6
7e-7
0.99
0.21
End-host
249
1
5e-6
0.20
0.001
7e-4
0.07
9e-5
0.019
0
0.01
1
0.625
0.25
have logical hierarchy (e.g., genomic data). In our case however, hosts of enter-
prise networks do not have a logical hierarchy and the number of attributes are
relatively small, therefore HC is not appropriate. K-means clustering algorithms
are distance-based unsupervised machine learning techniques. By measuring the
distance of attributes from each instance and their centroids, it groups data-
points into a given number of clusters by iterations of moving centroids. In our
case there is a signiﬁcant distance variation of attributes for hosts within each
cluster (e.g., highly active name servers or recursive resolvers versus low active
ones) which may lead to mis-clustering.
The EM algorithm is a suitable ﬁt in our case since it uses the probabil-
ity of an instance belonging to a cluster regardless of its absolute distance. It
establishes initial centroids using a K-means algorithm, starts with an initial
probability distribution following a Gaussian model and iterates to achieve con-
vergence. This mechanism, without using absolute distance during iteration,
decreases the chance of biased results due to extreme outliers. Hence, we choose
an EM clustering algorithm for “DNS Host Clustering Machine”.
Number of Clusters. Choosing the appropriate number of clusters is the key
step in clustering algorithms. As discussed earlier, we have chosen four clusters
based on our observation of various types of servers. One way to validate the
number of clusters is with the “elbow” method. The idea of the elbow method is
to run k-means clustering on the dataset for a range of k values (say, k from 1 to
9) that calculates the sum of squared errors (SSE) for each value of k. The error
decreases as k increases; this is because as the number of clusters increases, the
SSE becomes smaller so the distortion also gets smaller. The goal of the elbow
method is to choose an optimal k around which the SSE decreases abruptly (i.e.,
138
M. Lyu et al.
(a) Univesity campus.
(b) Research institute.
Fig. 5. Hosts clustering results across one week.
ranging from 3 to 5 in our results, hence, k = 4 clusters seems a reasonable value
for both the university and the research institute).
Clustering Results. We tuned the number of iterations and type of covari-
ance for our clustering machine to maximize the performance in both enterprises.
Tables 2 and 3 show the number of hosts identiﬁed in each cluster based on data
from 3 May 2018. We also see the average value of various attributes within each
cluster. For the cluster of name servers, QryFracOut approaches 0 in both orga-
nizations, highlighting the fact that almost all outgoing DNS packets from these
hosts are responses rather than queries, which matches with the expected behav-
ior. Having a high number of external clients served also indicates the activity of
these hosts – in the University campus and research institute respectively 42 and
12 name servers collectively serve 84% (i.e., 42× 2% and 12× 7%) of external
hosts.
Considering recursive resolvers in Tables 2 and 3, the average QryFracOut is
close to 1 for both organizations as expected. It is seen that some of these hosts
also answer incoming queries (from external hosts) possibly due to their mis-