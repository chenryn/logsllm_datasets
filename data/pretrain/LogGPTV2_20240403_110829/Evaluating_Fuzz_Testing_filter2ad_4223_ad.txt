a variety of seeds when evaluating an algorithm. Papers should be
specific about how the seeds were collected, and better still to make
available the actual seeds used. We also feel that the empty seed
should be considered, despite its use contravening conventional
wisdom. In a sense, it is the most general choice, since an empty file
can serve as the input of any file-processing program. If a fuzzer
does well with the empty seed across a variety of programs, perhaps
it will also do well with the empty seed on programs not yet tested.
And it takes a significant variable (i.e., which file to use as the seed)
out of the vast configuration space.
6 TIMEOUTS
Another important question is how long to run a fuzzer on a partic-
ular target. The last column of Table 1 shows that prior experiments
of fuzzers have set very different timeouts. These generally range
from 1 hour to days and weeks.3 Common choices were 24 hours
(10 papers) and 5 or 6 hours (7 papers). We observe that recent
papers that used LAVA as the benchmark suite chose 5 hours as the
timeout, possibly because the same choice was made in the original
LAVA paper [16]. Six papers ran fuzzers for more than one day.
Most papers we considered reported the timeout without justi-
fication. The implication is that beyond a certain threshold, more
running time is not needed as the distinction between algorithms
will be clear. However, we found that relative performance between
algorithms can change over time, and that terminating an experi-
ment too quickly might yield an incomplete result. As an example,
AFLFast’s evaluation shows that AFL found no bugs in objdump
after six hours [6], but running AFL longer seems to tell a different
story, as shown in Figure 2b. After six hours, both AFL and AFLFast
start to find crashes at a reasonable clip. Running AFL on gif2png
shows another interesting result in Figure 2e. The median number
of crashes found by AFL was 0 even after 13 hours, but with only
7 more hours, it found 40 crashes. Because bugs often reside in
certain parts of the program, fuzzing detects the bugs only when
these parts are eventually explored. Figure 4 presents the results of
AFL and AFLFast running with three sampled seeds on nm. After
6 hours none of the AFL runs found any bugs in nm, while the
median number of crashes found by AFLFast was 4; Mann Whitney
says that this difference is significant. But at 24 hours, the trend is
reversed: AFL has found 14 crashes and AFLFast only 8. Again, this
difference is significant.
What is a reasonable timeout to consider? Shorter timeouts are
convenient from a practical perspective, since they require fewer
2http://samples.ffmpeg.org
3[56] is an outlier that we do not count here: it uses 5-minute timeout because its
evaluation focuses on test generation rate instead of bug finding ability.
could find more crashes than fuzzer B but find the same or fewer
actual bugs.
As such, many papers employ some strategy to de-duplicate (or
triage) crashes, so as to map them to unique bugs. There are two
popular automated heuristics for doing this: using AFL’s notion of
coverage profile, and using stack hashes. In Table 1, these are marked
‘C’ (7 papers) and ‘S’ (7 papers) in the crash column. There are
four papers using other tools/methods for triage, marked ‘O’. For
example, VUzzer additionally used a tool called !Exploitable to
assess the exploitability of a crash caused by a bug [44]. Crashes
that have a low likelihood of being turned into an attack could be
discounted by a user, so showing that a fuzzer finds more danger-
ous bugs is advantageous. The de-duplication strategy used in our
experiments corresponds to ‘C’.
Unfortunately, as we show experimentally in this section, these
de-duplication heuristics are actually poor at clustering crashing
inputs according to their root cause.
Several papers do consider some form of ground truth. Six papers
use it as their main performance measure, marked ’G’ in the table.
By virtue of their choice of benchmark programs, they are able
to map crashing inputs to their root cause perfectly. Eight other
papers, marked ’G*’ in the table, make some effort to triage crashes
to identify their root cause, but do so imperfectly. Typically, such
triage is done as a ‘case study’ and is often neither well founded
nor complete—ground truth is not used as the overall (numeric)
performance measure.
In the next three subsections we discuss performance measures
in detail, showing why using heuristics rather than actual ground
truth to compare fuzzer performance can lead to misleading or
wrong conclusions. In lieu of measuring bugs directly, nearly half
of the papers we examined consider a fuzzer’s ability to execute
(“cover”) significant parts of a target program. This measurement
is potentially more generalizable than bug counts, but is not a
substitute for it; we discuss it at the end of the section.
7.1 Ground Truth: Bugs Found
The ultimate measure of a fuzzer is the number of distinct bugs
that it finds. If fuzzer A generally finds more bugs than baseline B
then we can view it as more effective. A key question is: What is a
(distinct) bug? This is a subjective question with no easy answer.
We imagine that a developer will ultimately use a crashing input
to debug and fix the target program so that the crash no longer
occurs. That fix will probably not be specific to the input, but will
generalize. For example, a bugfix might consist of a length check to
stop a buffer overrun from occurring—this will work for all inputs
that are too long. As a result, if target p crashes when given input
I, but no longer crashes when the bugfix is applied, then we can
associate I with the bug addressed by the fix [11]. Moreover, if
inputs I1 and I2 both induce a crash on p, but both no longer do so
once the bugfix is applied, we know that both identify the same bug
(assuming the fix is suitably “minimal” [26]).
When running on target programs with known bugs, we have di-
rect access to ground truth. Such programs might be older versions
with bugs that have since been fixed, or they might be synthetic pro-
grams or programs with synthetically introduced bugs. Considering
the former category, we are aware of no prior work that uses old
Figure 4: nm with three sampled seeds. At 6 hours: AFLFast
is superior to AFL with p = 2) {
char b = argv [1][0];
if ( b == ' a ' ) crash ();
crash ();
else
}
return 0;
}
Figure 5: How coverage-based deduplication can overcount
programs and their corresponding fixes to completely triage crashes
according to ground truth. In the latter category, nine papers use
synthetic suites in order to determine ground truth. The most popu-
lar suites are CGC (Cyber Grand Challenge) [14] and LAVA-M [16];
we discuss these more in the next section. For both, bugs have
been injected into the original programs in a way that triggering
a particular bug produces a telltale sign (like a particular error
message) before the program crashes. As such, it is immediately
apparent which bug is triggered by the fuzzer’s generated input.
If that bug was triggered before, the input can be discarded. Two
other papers used hand-selected programs with manually injected
vulnerabilities.
7.2 AFL Coverage Profile
When ground truth is not available, researchers commonly employ
heuristic methods de-duplicate crashing inputs. The approach taken
by AFL, and used by 7 papers in Table 1 (marked ’C’), is to consider
inputs that have the same code coverage profile as equivalent. AFL
will consider a crash “unique” if the edge coverage for that crash
either contains an edge not seen in any previous crash, or, is missing
an edge that is otherwise in all previously observed crashes.4
Classifying duplicate inputs based on coverage profile makes
sense: it seems plausible that two different bugs would have dif-
ferent coverage representations. On the other hand, it is easy to
imagine a single bug that can be triggered by runs with different
coverage profiles. For example, suppose the function crash in the
program in Figure 5 will segfault unconditionally. Though there is
but a single bug in the program, two classes of input will be treated
as distinct: those starting with an 'a' and those that do not.
Assessing against ground truth. How often does this happen in
practice? We examined the crashing inputs our fuzzing runs gen-
erated for cxxfilt using AFL and AFLFast. Years of development
activity have occurred on this code since the version we fuzzed
was released, so (most of) the bugs that our fuzzing found have
been patched. We used git to identify commits that change source
files used to compile cxxfilt. Then, we built every version of cxxfilt
for each of those commits. This produced 812 different versions of
cxxfilt. Then, we ran every crashing input (57,142 of them) on each
4AFL also provides a utility, afl-cmin, which can be run offline to “prune” a corpus
of inputs into a minimal corpus. Specifically, the afl-cmin algorithm keeps inputs that
contain edges not contained by any other inputs trace. This is different than the AFL
on-line algorithm, which also retains inputs missing edges that other inputs’ traces
have. Only one prior paper that we know of, Angora [10], ran afl-cmin on the final
set of inputs produced by AFL; the rest relied only on the on-line algorithm, as we do.
different version of cxxfilt, recording whether or not that version
crashed. If not, we consider the input to have been a manifestation
of a bug fixed by that program version.
To help ensure that our triaging results are trustworthy, we took
two additional steps. First, we ensured that non-crashing behavior
was not incidental. Memory errors and other bug categories uncov-
ered by fuzzing may not always cause a crash when triggered. For
example, an out-of-bounds array read will only crash the program
if unmapped memory is accessed. Thus it is possible that a commit
could change some aspect of the program that eliminates a crash
without actually fixing the bug. To address this issue, we compiled
each cxxfilt version with Address Sanitizer and Undefined Behavior
Sanitizer (ASAN and UBSAN) [48], which adds dynamic checks
for various errors including memory errors. We considered the
presence of an ASAN/UBSAN error report as a “crash.”
Second, we ensured that each bug-fixing commit corresponds to
a single bugfix, rather than several. To do so, we manually inspected
every commit that converted a crashing input to a non-crashing
one, judging whether we believed multiple distinct bugs were being
fixed (based on principles we developed previously [26]). If so, we
manually split the commit into smaller ones, one per fix. In our
experiments, we only had to do this once, to a commit that imported
a batch of changes from the libiberty fork of cxxfilt into the main
trunk.5 We looked at the individual libiberty commits that made
up this batch to help us determine how to split it up. Ultimately we
broke it into five distinct bug-fixing commits.
Our final methodology produced 9 distinct bug-fixing commits,
leaving a small number of inputs that still crash the current version
of cxxfilt. Figure 6 organizes these results. Each bar in the graph
represents a 24-hour fuzzing trial carried out by either AFL or
AFLFast.6 For each of these, the magnitude of the bar on the y
axis is the total number of “unique” (according to coverage profile)
crash-inducing inputs, while the bar is segmented by which of these
inputs is grouped with a bug fix discovered by our ground truth
analysis. Above each bar is the total number of bugs discovered by
that run (which is the number of compartments in each bar). The
runs are ordered by the number of unique bugs found in the run.
We can see that there is at best a weak correlation between the
number of bugs found during a run and the number of crashing
inputs found in a run. Such a correlation would imply a stronger
upward trend of crash counts when moving left to right. We can also
see that AFLFast generally found many more “unique” crashing
inputs than AFL but the number of bugs found per run is only
slightly higher. Mann Whitney finds that the difference in crashes
is statistically significant, with a p-value of 10−10, but the difference
in bugs is not (but is close)—the p-value is 0.066.
Discussion. Despite the steps we took to ensure our triage matches
ground truth, we may still have inflated or reduced actual bug
counts. As an example of the former, we note that ASAN/UBSAN
is not guaranteed to catch all memory safety violations, so we may
have attributed an incidental change to a bugfix. We found a few
cases where we couldn’t explain why a commit fixed a crash, and
so did not associate the commit with a bug. On the other hand, we
5https://github.com/gcc-mirror/gcc/tree/master/libiberty
6We show each trial’s data individually, rather than collecting it all together, because
AFL’s coverage-based metric was applied to each trial run, not all runs together.
Figure 6: Crashes with unique bugs found per run for cxxfilt. Each bar represents an independent run of either AFL or AFLfast.
The height of the bar is the count of crashing inputs discovered during that run. Each bar is divided by color, clustering inputs
with other inputs that share the same root cause. Number of unique bugs is indicated above each bar.
might have failed to differentiate multiple bugfixes in a single com-
mit, either by mistake or in the eyes of an observer whose judgment
differs from our own. In any case, the magnitude of the difference
between our counts and “unique crashes” means that the top-level
result—that “unique crashes” massively overcount the number of
true bugs—would hold even if the counts changed a little.
Had we used ground truth measure in all of our experiments, it
might have changed the character of the results in Sections 4–6. For
example, the performance variations within a configuration due
to randomness (e.g., Figure 2) may not be as stark when counting
bugs rather than “unique” crashing inputs. In this case, our advice
of carrying out multiple trials is even more important, as small
performance differences between fuzzers A and B may require
many trials to discern. It may be that performance differences due
to varying a seed (Figure 3) may also not be as stark—this would
be true if one seed found hundreds of crashes and another found
far fewer, but in the end all crashes corresponded to the same bug.
There may also be less performance variation over time when bugs,
rather than crashes, are counted (Figure 4). On the other hand, it is
also possible that we would find more variation over time, and/or
with different seeds, rather than less. In either case, we believe our
results in Sections 4–6 raise sufficient concern that our advice to test
with longer timeouts and a variety of seeds (including the empty
seed) should be followed unless and until experimental results with
ground truth data shed more light on the situation.
7.3 Stack hashes
Another common, heuristic de-duplication technique is stack hash-
ing [36]. Seven papers we considered use this technique (marked
’S’ in Table 1). The idea is the following. Suppose that our buffer
overrun bug is in a function deep inside a larger program, e.g., in a
library routine. Assuming that the overrun induces a segfault imme-
diately, the crash will always occur at the same place in the program.
More generally, the crash might depend on some additional pro-
gram context; e.g., the overrun buffer might only be undersized
when it is created in a particular calling function. In that case, we
might look at the call stack, and not just the program counter, to
map a crash to a particular bug. To ignore spurious variation, we
focus on return addresses normalized to their source code location.
Since the part of the stack closest to the top is perhaps the most
relevant, we might only associate the most recent N stack frames
with the triggering of a particular bug. (N is often chosen to be be-
tween 3 and 5.) These frames could be hashed for quick comparison
to prior bugs—a stack hash.
Stack hashing will work as long as relevant context is unique,
and still on-stack at the time of crash. But it is easy to see situations
where this does not hold—stack hashing can end up both under-
counting or overcounting true bugs. Consider the code in Figure 7,
which has a bug in the format function that corrupts a string s,
which ultimately causes the output function to crash (when s is
passed to it, innocently, by the prepare function). The format
function is called separately by functions f and g.
Suppose we fuzz this program and generate inputs that induce
two crashes, one starting with the call from f and the other starting
with the call from g. Setting N to the top 3 frames, the stack hash
will correctly recognize that these two inputs correspond to the
same bug, since only format, prepare and output will be on the
stack. Setting N to 5, however, would treat the inputs as distinct
crashes, since now one stack contains f and the other contains
g. On the other hand, suppose this program had another buggy
556667677667667766855565658666050010001500Count of crashesAFL876767575866676687766867667666AFLfastvoid f () { ... format ( s1 ); ... }
void g () { ... format ( s2 ); ... }
void format ( char * s ) {
// bug : corrupt s
prepare ( s );