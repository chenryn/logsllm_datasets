jecting malicious DLLs inside the user-space process. A malicious driver can
perform this task in a variety of ways, such as by calling system call functions
directly from the driver. A stealthy technique involves Asynchronous Procedure
Calls (APCs): a method of executing code asynchronously in the context of a
particular thread and, therefore, within the address space of a particular pro-
cess [22]. Malicious drivers identify a process, allocate memory inside it, copy
the malicious DLL to that memory, create and initialize a new APC, alter an
existing thread of the target process to execute the inserted code, and queue the
APC to later run the thread asynchronously. This method is stealthy as APCs
Automatic Discovery of Parasitic Malware
103
Network 
sensor 
(NIDS)
Trusted VM (Fedora)
Untrusted VM (Windows XP)
Network 
attribution sensor
Correlation 
engine
Apps
Network 
attribution sensor
User
Kernel
User
Kernel
Trusted
address space
Untrusted
address space
Xen
Host attribution sensor
Fig. 1. Architecture of Pyren´ee
are very common inside the Windows kernel, and it is very hard to distinguish
between benign and malicious APCs.
Case 2B: This method is similar to the one explained in Case 2A. The diﬀerence
lies in the form of malicious code that injected into a benign process. Here,
malicious kernel drivers inject raw code into a benign process and execute it
using the APC.
Case 2C: Finally, a kernel thread injection is the method by which malicious
drivers execute malicious functionality entirely inside the kernel. A kernel thread
executing malicious functionality is owned by a user-level process, though the
user-level process had not requested its creation. By default, these threads are
owned by the System process, however a driver may also choose to execute its
kernel thread on behalf of any running process.
Our system adapts well as new attack information becomes available. Though
the described methods are prevalent in current attacks, other means of injecting
malicious code into benign software also exist. For example, SetWindowsHookEx,
AppInit DLL, and SetThreadContext APIs can be used for malice. Our general
technique can easily encompass these additional attack vectors by monitoring
their use in the system.
4 Architecture
Pyren´ee automatically identiﬁes at runtime the malicious code running on an
infected system. That objective leads to the following design goals:
– Accurate Attribution: Pyren´ee combines data from network-based and
host-based sensors to avoid false positives, provide process or driver level
granularity in malware identiﬁcation, and to account for evasive behaviors
of parasitic malware.
– Automatic, Runtime Detection: We design lightweight sensors that in-
cur low overhead, allowing Pyren´ee to operate at runtime. We identify ma-
licious code without any human intervention.
– Resist Direct and Indirect Attacks: Pyren´ee’s tamper-resistant design
prevents direct attack by a motivated attacker. We deploy all components
of our system outside of an infected operating system.
104
A. Srivastava and J. Giﬃn
Pyren´ee has a modular design (Figure 1). Its architecture uses the hypervisor
to provide isolation between our software and the infected system. To perform
accurate detection and identiﬁcation of malicious code, Pyren´ee aggregates infor-
mation collected from three diﬀerent sensors. A network sensor identiﬁes inbound
or outbound network traﬃc of suspicion; we use oﬀ-the-shelf network intrusion
detection systems (NIDS) like BotHunter, Snort, or Bro and will not further
discuss this component. The network attribution and host attribution sensors
are software programs executing in the isolated high-privilege virtual machine
and hypervisor, respectively. A correlation engine, also running in the trusted
VM, takes data from all three types of sensors and determines the malicious
software present in the victim. Our sensors are lightweight and suitable for on-
line detection. The following sections describe the network attribution and host
attribution sensors as well as the correlation engine.
4.1 Network Attribution Sensor
The network attribution sensor maps network-level packet information to host-
level process identities. Given a network sensor (NIDS) alert for some suspicious
traﬃc ﬂow, the network attribution sensor is responsible for determining which
process is the local endpoint of that ﬂow in the untrusted VM. This process may
be malicious, or it may be a benign process altered by a parasitic malware infec-
tion. We deployed the network attribution sensor in a trusted virtual machine. It
has two subcomponents: one in the VM’s kernel space and one in userspace. The
kernel component provides high-performance packet ﬁltering services by inter-
cepting both inbound and outbound network packets for an untrusted VM. The
userspace component performs virtual machine introspection (VMI) whenever
requested by the kernel component.
The kernel component identiﬁes separate TCP traﬃc ﬂows. Whenever it re-
ceives a SYN packet, it extracts both the source and destination IP addresses
and ports, which it then passes to the userspace component for further use. The
kernel component is a passive network tap and allows all packets ﬂows to con-
tinue unimpeded. Though in the current prototype of Pyren´ee we only work with
TCP ﬂows, our system is able to intercept packets of any protocol.
The userspace component determines which process in the victim VM is the
local endpoint of the network ﬂow. When invoked by the kernel component,
it performs memory introspection of the untrusted VM to identify the process
bound to the source (or destination) port as speciﬁed in the data received by
the kernel component. To ﬁnd a process name, it must locate the guest kernel’s
data structures that store network and process information. We have reverse
engineered part of the Windows kernel to identify these structures, discussed in-
depth in Section 5. The userspace component stores the extracted process and
network connection information in a database to be used later by the correlation
engine. The stored information helps even in the case when malware exits after
sending malicious packets.
The network attribution sensor’s task is to determine the end-point of a
network connection originated from the guest VM. Recent kernel-level attacks
Automatic Discovery of Parasitic Malware
105
complicate this task. For example, srizbi [16] is a kernel-level bot that executes
entirely in the kernel. When untrusted drivers send/receive packets from the
kernel, there is no user-space process that can be considered as the end-point of
the connection. Pyren´ee solves this problem by monitoring the execution of un-
trusted drivers. Since all kernel threads created by drivers are always assigned to
a user-level process, that process becomes the end-point of the in-driver connec-
tion. To determine the actual driver, we enumerate all threads of the end-point
process and match against threads of untrusted drivers.
4.2 Host Attribution Sensor
The local endpoint of a malicious network ﬂow may itself be a benign program:
it may have been altered at runtime by a DLL or thread injection attack orig-
inating at a diﬀerent parasitic malware process or driver. The host attribution
sensor, deployed within the hypervisor, identiﬁes the presence of possible para-
sitic malicious code. We describe the monitoring of both user and kernel level
parasitic behaviors in the following sections.
User-level Parasitism. User-level parasitism occurs when a malicious user
process injects a DLL or raw code along with a thread into a running benign
process as explained in Cases 1A and 1B. To detect a process-to-process parasitic
behavior, the host attribution sensor continuously monitors the runtime behavior
of all processes executing within the victim VM by intercepting their system calls
[9, 7]. Note that we monitor the native API, or the transfers from userspace to
kernel space, rather than the Win32 API calls described in Section 3. High-level
API monitors are insecure and may be bypassed by knowledgeable attackers,
but native API monitoring oﬀers complete mediation for user-level processes.
The host attribution sensor intercepts all system calls, but it processes only
those that may be used by a DLL or thread injection attack. This list includes
NtOpenProcess, NtCreateProcess, NtAllocateVirtualMemory, NtWriteVirt-
ualMemory, NtCreateThread, and NtClose, which are the native API forms of
the higher-level Win32 API functions described previously. The sensor records
the system calls’ parameter values: IN parameters at the entry of the call and
OUT parameters when they return to userspace. Recovering parameters requires
a complex implementation that we describe in detail in Section 5.
handle = OpenProcess()
ZwOpenProcess
KeAttachProcess
handle = CreateProcess()
AllocateMemory(handle)
ZwAllocateMemory
WriteMemory(handle)
KeInitializeApc
(a)
CreateThread(handle)
KeInsertQueueApc
Code
Injection
(b)
Code
Injection
Fig. 2. Runtime parasitic behavioral models.
(b) Driver-to-process injection.
(a) Process-to-process
injection.
106
A. Srivastava and J. Giﬃn
The sensor uses an automaton description of malware parasitism to determine
when DLL or thread injection occurs. The automaton (Figure 2a) characterizes
the series of system calls that occur during an injection. As the sensor inter-
cepts system calls, it veriﬁes them against an instance of the automaton spe-
ciﬁc to each possible victim process. We determine when the calls apply to the
same victim by performing data-ﬂow analysis on the process handle returned by
NtOpenProcess and NtCreateProcess. Should the handle be duplicated (using
NtDuplicateObject), we include the new handle in further analysis. The sensor
communicates information about detected injections to the correlation engine
for further use.
Kernel-level Parasitism. Kernel-level parasitism occurs when a malicious
kernel driver injects either a DLL or raw code followed by the alteration of an
existing targeted process’ thread (Case 2A and 2B). A kernel-level malicious
driver can also create a new thread owned by any process as explained in Case
2C. To detect kernel-to-process parasitic behavior, the host attribution sensor
monitors all kernel APIs invoked by untrusted drivers. However, there is no mon-
itoring interface inside the kernel for drivers similar to the system-call interface
provided to user applications. To solve this problem, Pyren´ee creates a moni-
toring interface inside the kernel for untrusted drivers by isolating them in a
separate address space and monitoring kernel APIs invoked by untrusted drivers
through this new interface.
Pyren´ee creates a new address space inside the hypervisor transparent to
the guest OS and loads all untrusted drivers in this address space. This new
address space is analogous to the existing kernel address space, however per-
missions are set diﬀerently. The existing kernel space, called the trusted page
table (TPT), contains all the core kernel and trusted driver code with read and
execute permissions, and untrusted driver code with read-only permissions. The
untrusted driver address space, called the untrusted page table (UPT), contains
untrusted code with read and execute permissions, and trusted code as non-
readable, non-writable, and non-executable. Pyren´ee also makes sure that the
data pages mapped in both the address spaces are non-executable. Table 2 shows
the permissions set on UPT and TPT memory pages. With these permission
bits, any control ﬂow transfers from untrusted to trusted address space induce
page faults thereby enabling the host-attribution sensor to monitor kernel APIs
invoked by untrusted drivers.
Pyren´ee diﬀerentiates between trusted and untrusted drivers at the time of
loading to decide in which address space they must be mapped. This diﬀerenti-
ation can be made using certiﬁcates. For example, a driver signed by Microsoft
Table 2. Permission bits on trusted and untrusted address spaces
Address Space Trusted Code Trusted Data Untrusted Code Untrusted Data
Trusted
Untrusted
rx
—
rw
rw
r
rx
rw
rw
Automatic Discovery of Parasitic Malware
107
can be loaded in the trusted address space. However, Microsoft might not rely
on drivers signed by other parties whose authenticity is not veriﬁed. With this
design, all Microsoft signed drivers are loaded into the trusted address space,
and other drivers signed by third party vendors or unsigned drivers, including
kernel malware, are loaded into the untrusted address space.
Due to the isolated address space, the host-attribution sensor intercepts all
kernel APIs invoked by untrusted drivers and inspects their parameters. The
sensor uses an automaton to characterize the parasitic behavior originating from
malicious drivers. When the sensor intercepts kernel APIs, it veriﬁes against the
automaton to recognize the parasitic behavior. In our current prototype, we
create an automaton based on the kernel APC-based code injection (Figure 2b).
The host-attribution sensor records the gathered information for future use by
the correlation engine.
4.3 Correlation Engine
The correlation engine identiﬁes which code on an infected system is malicious
based on information from our collection of sensors [39]. The engine has three in-
terfaces that communicate with a NIDS, the host attribution sensor, and the net-
work attribution sensor. Architecturally, it resides in the isolated, high-privilege
VM.
The NIDS provides network alert information to the correlation engine’s ﬁrst
interface. This information includes the infected machine’s IP address, port used
in the suspicious ﬂow, and other details. The alert acts as a trigger that acti-
vates searches across information from the software sensors. The second interface
gathers information from the network attribution sensor, which provides infor-
mation that maps the malicious network connection identiﬁed by the NIDS to a
host-level process.
The third interface collects information from the host attribution sensor. In its
process-to-process injection report, the host attribution sensor passes identiﬁers
of injecting and victim processes, a handle for the victim of the injection, and
other data. When receiving this information, the correlation engine uses VMI to
retrieve detailed data about the victim and injecting processes, including their
name, their component DLLs, and their open ﬁles. Should the victim process
not have an identiﬁer, as is the case for victims created via NtCreateProcess,
the engine uses the victim’s process handle to recover information about the
victim. Section 5 provides low-level details of this data extraction. In the kernel-
to-process injection report, it passes details of the victim process, such as the
process identiﬁer, name, and the name of the malicious kernel driver.
Based on the information provided by sensors, the correlation engine con-
structs a list of malicious processes and drivers. It matches attack information
provided by a NIDS with network ﬂow endpoint records generated by the net-
work attribution sensor. When it ﬁnds a match, it extracts the name of the
process bound to the malicious connection. Using information from the host at-
tribution sensor, it determines whether or not the process has suﬀered from a
parasitic attack. When it ﬁnds an injection, it extracts the name of the injecting