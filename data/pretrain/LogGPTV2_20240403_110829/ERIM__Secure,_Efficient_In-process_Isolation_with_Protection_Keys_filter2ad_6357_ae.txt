100.0
Native
(req/s)
466,419
421,656
388,926
263,719
ERIM
rel. (%)
95.7
96.1
96.6
100.0
Native
(req/s)
823,471
746,278
497,778
ERIM
rel. (%)
96.4
95.5
100.0
Table 3: Nginx throughput with multiple workers. The standard deviation is below 1.5% in all cases.
t
u
p
h
g
u
o
r
h
T
d
e
z
i
l
a
m
r
o
N
 1
 0.8
 0.6
 0.4
 0.2
 0
Native
ERIM
0kb
1kb
2kb
8kb
4kb
File size
1 6kb
3 2kb
6 4kb
1 2 8kb
Figure 1: Throughput of NGINX with one worker, normal-
ized to native (no protection), with varying request sizes.
Standard deviations were all below 1.1%.
File
size
(KB)
0
1
2
4
8
16
32
64
128
Throughput
Native
(req/s)
95,761
87,022
82,137
76,562
67,855
45,483
32,381
17,827
8,937
ERIM
rel. (%)
95.8
95.2
95.4
95.3
96.0
97.1
97.3
100.0
100.0
Switches/s
1,342,605
1,220,266
1,151,877
1,073,843
974,780
820,534
779,141
679,371
556,152
CPU load
native
(%)
100.0
100.0
100.0
100.0
100.0
100.0
100.0
96.7
86.4
Table 4: Nginx throughput with a single worker. The stan-
dard deviation is below 1.1% in all cases.
ments.4 Figure 1 shows the average throughput of 10 runs of
an ERIM-protected NGINX relative to native NGINX with-
out any protection for different ﬁle sizes, measured after an
initial warm-up period.
ERIM-protected NGINX provides a throughput within
95.18% of the unprotected server for all request sizes. To
explain the overhead further, we list the number of ERIM
switches per second in the NGINX worker and the worker’s
CPU utilization in Table 4 for request sizes up to 128KB.
The overhead shows a general trend up to requests of size 32
4Since NGINX only serves static ﬁles in this experiment, its support for
Lua and JavaScript is not used. As a result, this experiment does not rely on
any support for Jit, which we have not yet implemented.
KB: The worker’s core remains saturated but as the request
size increases, the number of ERIM switches per second de-
crease, and so does ERIM’s relative overhead. The observa-
tions are consistent with an overhead of about 0.31%–0.44%
for 100,000 switches per second. For request sizes 64KB
and higher, the 10Gbps network saturates and the worker
does not utilize its CPU core completely in the baseline. The
free CPU cycles absorb ERIM’s CPU overhead, so ERIM’s
throughput matches that of the baseline.
Note that this is an extreme test case, as the web server
does almost nothing and serves the same cached ﬁle repeat-
edly. To get a more realistic assessment, we set up NGINX to
serve from main memory static HTML pages from a 571 MB
(15,520 pages) Wikipedia snapshot of 2006 [48]. File sizes
vary from 417 bytes to 522 KB (average size 37.7 KB). 75
keep-alive clients request random pages (selected based on
pageviews on Wikipedia [49]). The average throughput with
a single NGINX worker was 22,415 requests/s in the base-
line and 21,802 requests/s with ERIM (std. dev. below 0.6%
in both cases). On average, there were 615,000 switches a
second. This corresponds to a total overhead of 2.7%, or
about 0.43% for 100,000 switches a second.
Scaling with multiple workers To verify that ERIM
scales with core parallelism, we re-ran the ﬁrst experiment
above with 3, 5 and 10 NGINX workers pinned to separate
cores, and sufﬁcient numbers of concurrent clients to satu-
rate all the workers. Table 3 shows the relative overheads
with different number of workers. (For requests larger than
those shown in the table, the network saturates, and the spare
CPU cycles absorb ERIM’s overhead completely.) The over-
heads were independent of the number of workers (cores),
indicating that ERIM adds no additional synchronization and
scales perfectly with core parallelism. This result is expected
as updates to the per-core PKRU do not affect other cores.
6.3 Isolating managed runtimes
Next, we use ERIM to isolate a managed language runtime
from an untrusted native library. Speciﬁcally, we link the
widely-used C database library, SQLite, to Node.js, a state-
of-the-art JavaScript runtime and map Node.js’s runtime to
T and SQLite to U. We modiﬁed SQLite’s entry points to
invoke call gates. To isolate Node.js’s stack from SQLite,
we run Node.js on a separate stack in MT, and switch to the
1232    28th USENIX Security Symposium
USENIX Association
Test #
100
110
400
120
142
500
510
410
240
280
170
310
161
160
230
270
Switches/s
11,183,281
8,329,914
8,161,584
7,190,766
7,074,553
6,419,008
5,868,395
5,091,212
2,358,524
2,303,516
1,264,366
1,133,364
1,019,138
1,014,829
670,196
560,257
ERIM overhead (%)
12.73%
12.18%
15.42%
13.81%
9.41%
12.13%
5.60%
3.64%
3.74%
3.22%
4.22%
2.92%
2.81%
2.73%
2.04%
2.28%
Table 5: Overhead relative to native execution for SQLite
speedtest1 tests with more than 100,000 switches/s. Standard
deviations were below 5.6%.
standard stack (in MU) prior to calling a SQLite function.
Finally, SQLite uses the libc function memmove, which ac-
cesses libc constants that are in MT, so we implemented a
separate memmove for SQLite. In total, we added 437 LoC.
We measure overheads on the speedtest1 benchmark that
comes with SQLite and emulates a typical database work-
load [4]. The benchmark performs 32 short tests that stress
different database functions like selects, joins, inserts and
deletes. We increased the iterations in each test by a factor
of four to make the tests longer. Our baseline for compar-
ison is native SQLite linked to Node.js without any protec-
tion. We conﬁgure the benchmark to store the database in
memory and report averages of 20 runs.
The geometric mean of ERIM’s runtime overhead across
all tests is 4.3%. The overhead is below 6.7% on all tests
except those with more than 106 switches per second. This
suggests that ERIM can be used for isolating native libraries
from managed language runtimes with low overheads up to
a switching cost of the order of 106 per second. Beyond that
the overhead is noticeable. Table 5 shows the relative over-
heads for tests with switching rates of at least 100,000/s. The
numbers are consistent with an average overhead between
0.07% and 0.41% for 100,000 switches/s. The actual switch
cost measured from direct CPU cycle counts varies from 73
to 260 cycles across all tests.
It exceeds 100 cycles only
when the switch rate is less than 2,000 times/s. We veriﬁed
that these are due to i-cache misses—at low switch rates, the
call gate instructions are evicted between switches.
6.4 Protecting sensitive data in CPI/CPS
Next, we use ERIM to isolate the safe region of CPI and
CPS [31] in a separate domain. We modiﬁed CPI/CPS’s
0%
100%
200%
300%
CPS
ERIM-CPS
CPI
ERIM-CPI
400.perlbench
401.bzip2
403.gcc
429.mcf
433.milc
444.namd
445.gobmk
447.dealII
450.soplex
456.hmmer
458.sjeng
462.libquantum
464.h264ref
470.lbm
471.omnetpp
473.astar
482.sphinx3
483.xalancbmk
Figure 2: Percentage overhead relative to no protection.
LLVM compiler pass to emit additional ERIM switches,
which bracket any code that modiﬁes the safe region. The
switch code, as well as the instructions modifying the safe
region, are inlined with the application code. In addition, we
implemented simple optimizations to safely reduce the fre-
quency of ERIM domain switches. For instance, the original
implementation sets sensitive code pointers to zero during
initialization. Rather than generate a domain switch for each
pointer initialization, we generate loops of pointer set oper-
ations that are bracketed by a single pair of ERIM domain
switches. This is safe because the loop relies on direct jumps
and the code to set a pointer is inlined in the loop’s body. In
all, we modiﬁed 300 LoC in LLVM’s CPI/CPS pass.
Like the original CPI/CPS paper [31], we compare the
overhead of the original and our ERIM-protected CPI/CPS
system on the SPEC CPU 2006 CINT/FLOAT benchmarks,
relative to a baseline compiled with Clang without any pro-
tection. The original CPI/CPS system is conﬁgured to use
ASLR for isolation, the default technique used on x86-64 in
the original paper. ASLR imposes almost no switching over-
head, but also provides no security [43, 25, 16, 19, 39].
Figure 2 shows the average runtime overhead of 10 runs of
the original CPI/CPS (lines “CPI/CPS”) and CPI/CPS over
ERIM (lines “ERIM-CPI/CPS”). All overheads are normal-
ized to the unprotected SPEC benchmark. We could not
obtain results for 400.perlbench for CPI and 453.povray for
both CPS and CPI. 400.perlbench does not halt when com-
piled with CPI and SPEC’s result veriﬁcation for 453.povray
fails due to unexpected output. These problems exist in
the code generated by the Levee CPI/CPS prototype with
CPI/CPS enabled (-fcps/-fcpi), not our modiﬁcations.