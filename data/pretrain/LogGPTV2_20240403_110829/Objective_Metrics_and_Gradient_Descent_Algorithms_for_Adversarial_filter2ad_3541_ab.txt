The histogram of oriented gradients (HOG) [9] is a feature
descriptor, widely used for object detection. Simply, HOG de-
scriptor is a concatenation of a number of locally normalized
histograms. Each histogram contains local information about
the intensity gradient directions, each local set of neighboring
histograms is normalized to improve the overall accuracy. By
concatenating those histogram vectors, HOG outputs a more
compact description of the shape. For object detection, a ma-
chine learning algorithm is trained over HOG descriptors of
training set images, to classify if any part of an input image
has HOG descriptor should be labeled as “detected”
Computing the gradient. Similarly to the edge detector in
2.2, HOG starts from computing gradient for each pixel. While
3×3 Sobel filters are used in Canny edge detector, HOG applies
the following simpler one dimensional filters to the input
image.
(cid:103)
0
1
∗ I
−1
(cid:102)
 ∗ I
−1
0
1
Gx =
Gy =
Using the same formula used for Canny edge detector, HOG
computes the gradient magnitude G and direction θ of each
pixel. However, HOG does not round the angle θ, as it gives
an important information for the next step.
Histogram construction. To construct histograms to be con-
catenated, HOG first divides the input image into smaller cells,
consisting of pixels (e.g. 8 × 8 pixels) then compute a his-
togram for each cell. Each histogram has several bins and
each bin is assigned an angle. In [9], 9 bins corresponding to
0◦,20◦, . . . ,160◦ are used in each histogram.
Histograms are computed by the weighted vote for each
pixels in a cell by voting the weighted magnitude to one or two
bins. The weights are determined by the gradient direction θ,
based on how close the angle is from its closest two bins.
Block normalization. Since the magnitude of gradients is
highly dependent to the local illumination and contrast, each
histogram should be locally normalized. Block, consisting of
several number of cells (e.g. 2× 2 cells), is the region that HOG
applies the normalization to the histograms of the contained
cells. While there are various way to normalize a single block,
Dalal et.al.[9] reported that the performance is seldom influ-
enced by the choice of normalization method, except for the
case of using L1 normalization.
The resulting feature vector is dependent on the parameters
introduced above: the number of pixels per cell, the number
of cells per block, and the number of histogram bins. In our
experiments, we use the following values (which are also used
in [9]: 8 × 8 pixels per cell, 2 × 2 cells per block, and 9 bins
for each histogram. For normalization method, we use L2
normalization.
2.5 Formulation
The adversarial goal is to take any input vector x ∈ ℜn (vec-
tors will be denoted in boldface) and produce a minimally al-
tered version of x, adversarial sample denoted by x⋆, that has
the property of being misclassified by a classifier F : ℜn → C.
For our proposes, a classifier is a function from ℜn to C, where
C is the set of class labels. Formally, speaking an adversary
wishes to solve the following optimization problem:
min
∆∈ℜn
such that
µ (∆)
F (x + ∆) ∈ C
∆ · M = 0
The various terms in the formulation are µ is a metric on
ℜn, C ⊆ C is a subset of the labels, and M (called the mask) is
a n-dimensional 0 − 1 vector of size n. The objective function
minimizes the metric µ on the perturbation ∆. Next we describe
various constrains in the formulation.
• F (x + ∆) ∈ C
2 to
The set C constrains the perturbed vector x + ∆
have the label (according to F) in the set C. For mis-
classification problems (the label of x and x + ∆) are
different we have C = C −{F (x)}. For targeted mis-
classification we have C = {l} (for l ∈ C), where l is the
target that an attacker wants (e.g., the attacker wants l
to correspond to a yield sign).
• ∆ · M = 0
The vector M can be considered as a mask (i.e., an at-
tacker can only perturb a dimension i if M[i] = 0),
i.e., if M[i] = 1 then ∆[i] is forced to be 0.3 Essen-
tially the attacker can only perturb dimension i if the
i-th component of M is 0, which means that δ lies in
k-dimensional space where k is the number of non-zero
entries in ∆.
• Convexity
Notice that even if the metric µ is convex (e.g., µ is
the l2 norm), because of the constraint involving F
the optimization problem is not convex (the constraint
∆ · M = 0 is convex). In general, solving convex opti-
mization problems is more tractable non-convex opti-
mization [25].
Generally, machine-learning algorithms use a loss func-
tion ℓ(F ,x,y) to penalize predictions that are “far away” from
the true label tl (x) of x. For example, if we use 0 − 1 loss
function, then ℓ(F ,x,y) = δ (F (x),y), where δ (z,y) is equal
to 1 iff z = y (i.e., if z (cid:44) y, then δ (z,y) = 0). For nota-
tional convenience, we will write LF (·) for ℓ(F ,·,·), where
LF (x) = ℓ(F ,x,tl (x)). Some classifiers F (x) are of the form
arg maxl Fs (x) (i.e., the classifier F outputs the label with the
maximum probability). For example, in a deep-neural network
(DNN) that has a softmax layer, the output of the softmax layer
2The vectors are added component wise
3the i-the component of a vector M is written as M[i].
is a probability distribution over class labels (i.e., the probabil-
ity of a label y intuitively means the belief that the classifier
has in the example has label y). Throughout the paper, we
sometimes refer to the function Fs as the softmax layer. In
these case, we will consider the probability distribution cor-
responding to a classifier. Formally, let c = | C | and F be a
classifier, we let Fs be the function that maps Rn to Rc such
that ∥Fs (x)∥1 = 1 for any x (i.e., Fs computes a probability
vector). We denote F l
s (x) to be the probability of Fs (x) at label
l.
2.6 Some Existing Algorithms
In this section, we describe few existing algorithms. This sec-
tion is not meant to be complete, but simply to give a “flavor”
of the algorithms to facilitate the discussion.
Goodfellow et al. attack - This algorithm is also known as
the fast gradient sign method (FGSM) [13]. The adversary crafts
an adversarial sample x⋆ = x + ∆ for a given legitimate sample
x by computing the following perturbation:
∆ = ε sign(∇LF (x))
(1)
The gradient of the function LF is computed with respect
to x using sample x and label y = tl (x) as inputs. Note that
∇LF (x)) is an n-dimensional vector and sign(∇LF (x)) is a
n-dimensional vector whose i-th element is the sign of the
∇LF (x))[i]. The value of the input variation parameter ε fac-
toring the sign matrix controls the perturbation’s amplitude.
Increasing its value, increases the likelihood of x⋆ being mis-
classified by the classifier F but on the contrary makes adver-
sarial samples easier to detect by humans.
Papernot et al. attack - This algorithm is suitable for tar-
geted misclassification [28]. We refer to this attack as JSMA
throughout the rest of the paper. To craft the perturbation
∆, components are sorted by decreasing adversarial saliency
value. The adversarial saliency value S (x,t )[i] of component i
for an adversarial target class t is defined as:
∂ x[i] (x) > 0
∂Fj
 0 if ∂Ft
∂ x[i] (x)  F l
(cid:40)
(cid:92)
In other words, at iteration i let Pi be the polyhedron that
s (xi ) + ∇F l
s (xi ) · d
s (xi ) · d
Therefore the d we are seeking for is dist(xi ,Pc
s (xi ) + ∇F k
≤ F l
s (xi ) + ∇F l
i ), the dis-
tance of xi to the complement of Pi is the smallest. Solving this
problem exactly and iterating until a different label is obtained,
results in the DeepFool algorithm.
s (xi ) · d
(cid:41)
d : F k
k∈C
Pi =
2.7 Discussion
Several algorithms for crafting adversarial samples have ap-
peared in the literature. These algorithms differ in three di-
mensions: the choice of the metric µ, the set of target labels
C ⊆ L, and the mask M. None of these methods use a mask M,
so we do not include it in the figure. We describe few attacks
according to these dimensions in Figure 2 (please refer to the
formulation of the problem at the beginning of the section).
3 OUR ALGORITHM
We now devise new algorithms for crafting adversarial pertur-
bations. Our starting point is similar to Deepfool, and assumes
that the classifier F (x) is of the form arg maxl Fs (x) and the
softmax output Fs is available to the attacker. Suppose that
F (x0) = l ∈ C, then F l
s (x0) is the largest probability in Fs (x0).
Note that F l
s is a scalar function. Our method is to find a small
s (x0 + d) ≈ 0. We are implicitly assuming that
d such that F l
there is a point x′, nearby x, that F “strongly believes” x′ does
not belong to class l (as the belief probability is “close to 0”),
while it believes that x does. While our assumption is “aggres-
sive”, it turns out in our experiments this assumption works
quite well in practice.
More specifically, with the above discussion, we now want
s as fast as possible to
to decrease the value of the function F l
0. Therefore, the problem is to solve the equation F l
s (x) = 0
starting with x0. The above condition can be easily general-
ized to the condition if the probability of label l goes below
a specified threshold (i.e. F l
s (x) < ϵ). We solve this problem
based on Newton’s method for solving nonlinear equations.
Specifically, for i = 0,1,2, . . . , suppose at step i we are at xi ,
we first approximate F l
s (x) ≈ F l
F l
s at xi using a linear function
s (xi ) + ∇F l
(3)
Let di = x− xi be the perturbation to introduce at iteration
i, and pi = F l
s (xi ) be the current belief probability of xi as
in class l. Assuming that we have not found the adversarial
example, pi must assume the largest value at the softmax layer,
thus in particular pi ≥ 1/| C |. Finally, denote gi = ∇F l
s (xi ) be
the gradient of F l
s at xi. Then our goal is to solve the following
linear system for di:
s (xi ) · (x− xi ).
pi + gi · di = pi +1
(4)
Method
FGSM
Papernot et al.
JSMA
Deepfool
C
C −{F (x)}
{l}
C −{F (x)}
Short Description
Adds a perturbation which is proportional to the sign of the
gradient of the loss function at the image x.
Constructs a saliency matrix S (x,t ), where x is the image
t is the target label. In each iteration, change the pixel according to the saliency matrix.
In each iteration, tries to push the probabilities of
other labels l′ higher than the current label l.
Figure 2: The second column indicates whether the method is suited for targeted misclassification. The last column
gives a short description of the method.
where pi +1 < pi is some appropriately chosen probability
value we want to achieve in the next iteration. Denote δi =
pi − pi +1 be the decrease of probability. Now we are ready to
describe the basic version of our new algorithm called New-
tonFool.
Basic Version of NewtonFool. We start with a simple ob-
servation: Given pi +1, the minimal-norm solution to (4) is
i = g†
d∗
is the Moore-Penrose
inverse of gi (see the background section for a description of
Moore-Penrose inverse). For rank-1 matrix gi, g†
is precisely
gi /∥ gi ∥2, and so
i (pi +1 − pi ) = −δi g†
where g†
i
i
i
Therefore the question left is how to pick pi +1. We make two
observations: First, it suffices that pi +1 < 1/| C | in order to
“fool” the classifier into a different class. Therefore we have an
upper bound of δi, namely
δi < pi − 1/| C |.
i ∥ is small. This can be formalized as ∥ d∗
Second, note that we want small perturbation, which means
that ∥ d∗
i ∥ ≤ η∥ x0 ∥
for some parameter η ∈ (0,1). Since ∥ d∗
i ∥ = δi /∥ gi ∥, this
thus translates to
Combining the two conditions above we thus have
δi ≤ η∥ x0 ∥∥ gi ∥
(cid:40)
η∥ x0 ∥∥ gi ∥,pi − 1
| C |
(cid:41)
δi ≤ min
We can thus pick
∗
i = min{η∥ x0 ∥∥ gi ∥,pi − 1/| C |}.
Plugging this back into (5), we thus get direction
δ
(6)
d∗
i = −δ
∗
i gi /∥ gi ∥2
.
i /∥∇F l
This gives the algorithm shown in 1.
Astute readers may realize that this is nothing but gradient
descent with step size δ∗
s (xi )∥2. However, there is an
important difference: The step size of a typical gradient descent
procedure is tuned on a complete heuristic basis. However,
in our case, exploiting the structure of softmax layer and our
assumption on the vulnerability of the neural network, the
step size is determined once the intuitively sensible parameter
η is fixed (which controls how small the perturbation we want).
Note also that the step size changes over time according to
d∗
i = − δi gi
∥ gi ∥2
(5)
return d
Algorithm 1
Input: x0,η, maxIter, a neural network F with a softmax
layer Fs.
(cid:40)
1: function NewtonFoolMinNorm(x0,η,maxIter , F)
2:
3:
4:
5:
l ← F (x0)
d ← 0
for i = 0,1,2, . . . ,maxIter − 1 do
δ∗
η∥ x0 ∥∥∇F l
i ← min
s (xi )∥, F l
i ∇F l