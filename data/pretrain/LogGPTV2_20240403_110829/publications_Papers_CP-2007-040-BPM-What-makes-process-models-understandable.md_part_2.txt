### 3.2 Hypothetical Relations between Factors and Understandability

This section explores the hypothetical relationships between various factors and understandability. Table 1 provides an overview of these relationships.

- **Perceived Difficulty**: We hypothesize that the perceived difficulty of a process model (as measured by the participants) will be negatively correlated with the score, which is an operationalization of actual understandability.
- **Theory and Practice**: A positive correlation is expected between the scores and the participants' theoretical and practical knowledge.
- **Count Metrics and Diameter**: The number of nodes, arcs, and the diameter of the process model (i.e., the longest path) are expected to be negatively related to understandability. The precise formulas for calculating these metrics are provided in [8].
- **Sequentiality**: This metric, which measures the degree to which the model is constructed of task sequences, is expected to be positively correlated with understandability.
- **Separability and Structuredness**: These metrics, which relate to the degree of articulation points in the model (nodes whose deletion separates the process model into multiple components) and the extent to which the model is built by nesting blocks of matching join and split routing elements, respectively, are also expected to be positively correlated with understandability.
- **Connectivity and Density**: Both metrics relate arcs to nodes. Connectivity is calculated by dividing the number of arcs by the number of nodes, while density is calculated by dividing the number of arcs by the maximally possible number of arcs. Both are expected to be negatively correlated with understandability.
- **Token Split Metric**: This metric captures how many new tokens can be introduced by AND- and OR-splits and is expected to be negatively correlated with understandability.
- **Average and Maximum Connector Degree**: These metrics refer to the number of input and output arcs of a routing element and are expected to be negatively correlated with the score.
- **Routing Elements’ Mismatch, Depth, Control Flow Complexity, and Connector Heterogeneity**: These metrics, which measure the degree of mismatch in routing elements, the nesting of structured blocks, the number of choices at splits, and the diversity of routing elements, respectively, are all expected to be negatively correlated with understandability.

In the subsequent section, we will contrast these hypothetical connections with the results from the questionnaire.

### 4. Results

This section presents the results of the questionnaire and interviews. We first analyze the distribution of scores in Section 4.1 and discuss its connection with perceived difficulty in Section 4.2. In Section 4.3, we analyze personal factors and their connection with scores. In Section 4.4, we consider the connection of model-related factors operationalized by the set of metrics. The final part of this section is devoted to our interviews with modeling experts.

#### 4.1 Distribution of Score

Using a standard grading scheme with 10% intervals, the distribution of scores among the participants is as follows:
- 8 students received an A (90% or better)
- 27 students received a B (80%-90%)
- 21 students received a C (70%-80%)
- 8 students received a D (60%-70%)
- 9 students received an E (less than 60%)

The mean score for all but one of the models ranges between 6.8 and 7.4, with 9 being the maximum. One model, however, has a mean score of only 5.5. To further examine the distribution of scores across the models, we applied both the Kruskal-Wallis and Mood’s median tests at a 95% confidence level [31]. Both non-parametric tests focus on medians to determine differences between distributions, which is appropriate here because the scores display significant deviations from a normal distribution. Interestingly, both test results indicate that the model with the low mean score (model L) is different from the other models (P-values ≤ 0.05). When all models are compared excluding model L, no significant differences in scores are observed (P-values > 0.25).

Upon closer inspection, it seems unusual that model L has such a low score. As described in Section 3.1, the questionnaire includes four sets of models, each containing three slightly different models. Models within the same group differ only in the type of routing elements. However, the group that model L belongs to has only six routing elements, while other groups contain two or three times this number. Additionally, the number of arcs in the L model group (37) is lower than in other groups (48, 57, and 59). Therefore, model L should be relatively easy to understand. The question arises as to why the other models in the same group as L do not show such a comparably low score.

In Figure 2, we display all three models in the group. Note that only model fragments are displayed for ease of visualization. From top to bottom, the type of the second logical routing element distinguishes the three models: an XOR-split for model L, an AND-split for one model, and an OR-split for another.

**Figure 2. Fragments of model variants J, K, and L (from left to right)**

When considering the respondents' answers in detail, two questions stand out:
- "If T is executed for a case, can U be executed for the same case?"
- "Can T, M, and O all be executed for the same case?"

For model L, fewer than 20 respondents answered these questions correctly, while more than 20 did so for the other two models. The distinguishing connectors in the two leftmost models (AND-split and OR-split) directly allow for the interleaved execution of T and U. For model L, T and U can be executed for the same case, but only after a cycle through M. This is likely overlooked by many respondents. Similarly, for the second question, many respondents failed to see that T, M, and O can be executed in the rightmost model (just as they can in the other two models). Thus, there is no significant difference in scores across the models, except for model L, which generated a low score due to the interplay between connector and model structure elements.

#### 4.2 Relation between Perceived and Score

In addition to the score, we analyzed the distribution of perceived difficulty. We used Kendall’s coefficient of agreement u [32, 31] to determine whether a ranking could be established based on the perception of all participants. Interestingly, for each of the four groups of variants, a total ordering emerges from the respondents' answers that is significant at a 95% confidence level. This result is confirmed by another part of our questionnaire, where we explicitly asked respondents to rate the relative differences in understandability between three models from different groups. Despite the allowance to rate models as equally difficult to understand, respondents do see distinct differences in the understandability of models within each set and even across the sets.

We observe different patterns emerging from the distributions of perceived difficulty and score. While models are perceived as distinctly different from each other, the actual numbers of correct answers they generate do not differ significantly. The notable exception is model L, which has a very low score value and is also perceived as the most difficult model to understand within its group. To investigate the relationship between perceived difficulty and score, we determined the Pearson correlation coefficient between the variables for all 847 complete model evaluations. The correlation coefficient is 0.234 with a P-value ≤ 0.05, indicating a significant but relatively weak correlation at a 95% confidence interval.

From this analysis, we derive that there is a rather loose relationship between perceived difficulty and score. Despite a significant statistical relation, respondents tend to exaggerate the differences in model understandability for models for which they do not produce significantly different numbers of correct answers. The variations in score also provide two additional insights:
1. As all models have the same number of tasks, the lack of significant differences in scores across most models suggests that model size is the primary factor impacting model understandability. If so, it would be reasonable that models with equal numbers of tasks appear equally difficult to understand.
2. Our detailed analysis of model L shows that a single change in a model element can have a significant impact on a model’s understandability. Thus, despite the potentially dominant impact of size, the search for additional impact factors is indeed relevant.

#### 4.3 Personal Factors and Score

Before conducting our experiment, we had no reason to expect differences in scores between respondents with different university backgrounds. All respondents had received at least basic training in the use of process modeling techniques at the time they took the questionnaire. Additionally, the exposure to process modeling in practice was negligible for all involved respondents. To test the absence of such a difference, we computed the total score over the 12 models. For each respondent, this figure lies between 0 and 108, with the latter being the theoretical maximum if all 9 questions for each of the 12 models were answered correctly. For our respondents, the total score ranges between 11 and 103, with an average value of 81.2. In Figure 3, the total score is shown for all students in ascending order.

**Figure 3. Total score for respondents**

If no difference existed between the three distributions of total scores, students from the three universities could be assumed to perform similarly. To test this, we applied the non-parametric Kruskal-Wallis test, as the Shapiro-Wilk W test indicates that the total score is not normally distributed for any university at a 95% confidence level.

Contrary to expectations, the application of the Kruskal-Wallis test indicates a statistically significant difference among the medians at a 95% confidence level (P-value ≤ 0.05). In other words, differences exist in the ability of respondents to answer questions correctly across the three universities. Additional pairwise Mann-Whitney tests [31] indicate that respondents from Eindhoven perform significantly better than respondents from each of the other two universities (P-values ≤ 0.05), although the difference between respondents from the universities of Vienna and Madeira is not significant (P-value = 0.061). In Figure 4, box plots are shown for TUe and non-TUe students.

A retrospective analysis of the courses offered at the various universities revealed that the hours spent on actual modeling are highest in Eindhoven, which may explain the noted difference. Specifically, Eindhoven students have been explicitly and thoroughly taught about ‘soundness’ [33], a general correctness criterion for workflow nets. An alternative explanation is that Eindhoven students are graduate students, while students from Madeira and Vienna are still in their third year of undergraduate studies. Interestingly, different modeling techniques are taught at the different universities. Eindhoven students were trained in workflow nets (based on the Petri net formalism), Vienna students in EPCs, and Madeira students had knowledge of both the Petri net formalism and EPCs. Thus, the choice of our EPC-like notation does not obviously favor students who are familiar with EPCs.

A search for other differences within the respondent population did not reveal any convincing factors. Specifically, both the variables theory (0.203) and practice (0.070) correlate weakly with total score, but these correlations are not significant at the 95% confidence level. These variables are also not very useful in identifying clusters with differing total score performances. For example, the clearest identification of two different clusters resulting from the application of various agglomerative clustering algorithms (e.g., nearest neighbor, media, Ward’s method) is shown in Figure 5. Here, the group average distance between clusters is used. It can be seen that most clusters extend across almost the entire range of theory and practice values. This suggests that, in the context of this study, students’ self-assessments are not valid.

**Figure 5. Cluster scatterplot using the group average method and squared Euclidean distance**