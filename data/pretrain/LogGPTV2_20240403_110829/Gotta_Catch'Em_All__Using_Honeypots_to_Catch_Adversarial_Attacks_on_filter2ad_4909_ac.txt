importantly, the corresponding adversarial input A(x) will display a
speciﬁc pattern, i.e. its feature representation will be similar to that
of the trapdoored input. Thus by recording the “trapdoor signature”
of ∆, i.e. S∆ = Ex ∈X,yt ,Fθ (x )❕(x + ∆) as deﬁned by eq.(3), we can
determine whether a model input is adversarial or not by comparing
its feature representation to S∆.
We also note that, without loss of generality, the above theorem
uses cosine similarity to measure the similarity between feature rep-
resentations of adversarial and trapdoored inputs. In practice, one
can consider other similarity metrics such as L2 distance. We leave
the search for the optimal similarity metric as future work.
So far our analysis as-
Case 2: Practical Trapdoor Injection.
sumes that the trapdoor is “perfectly” injected into the model. In
practice, the model owner will inject ∆ using a training/testing dis-
tribution Xt r ap ∈ X. The effectiveness of the trapdoor is deﬁned
by ∀x ∈ Xt r ap , Pr (Fθ (x + ∆) = yt ) ≥ 1 − µ. On the other hand,
the attacker will use a (different) input distribution Xat t ack . The
follow theorem shows that the attacker can still launch a highly suc-
cessful attack against the trapdoored model. The lower bound on
the success rate depends on the trapdoor effectiveness (µ) and the
statistical distance between Xt r ap and Xat t ack (deﬁned below).
DEFINITION 3. Given ρ ∈ [0, 1], two distributions PX1 and PX2
are ρ-covert if their total variation (TV) distance2 is bounded by ρ:
||PX1 − PX2 ||T V = maxC ⊂ Ω |PX1 (C) − PX2 (C)| ≤ ρ,
(5)
where Ω represents the overall sample space, and C ⊂ Ω represents
an event.
THEOREM 2. Let Fθ be a trapdoored model, ❕(x) be the feature
representation of input x, ρ, µ, σ ∈ [0, 1] be small positive constants.
A trapdoor ∆ is injected into Fθ using Xt r ap , and is (µ, Fθ , yt )-
effective for any x ∈ Xt r ap . Xt r ap and Xat t ack are ρ-covert.
For any x ∈ Xat t ack , if the feature representations of adversar-
ial input and trapdoored input are similar, i.e. the cosine similarity
cos(❕(A(x)), ❕(x + ∆)) ≥ σ and σ is close to 1, then the attack A(x)
is (µ + ρ, Fθ , yt )-effective on any x ∈ Xat t ack .
The proof of Theorem 2 is in the Appendix.
Theorem 2 implies that when the model owner enlarges the diver-
sity and size of the sample data Xt r ap used to inject the trapdoor, it
allows stronger and more plentiful shortcuts for gradient-based or
optimization-based search towards yt . This increases the chances
that an adversarial example falls into the “trap” and therefore gets
caught by our detection.
Later our empirical evaluation shows that for four representative
classiﬁcation models, our proposed defense is able to achieve very
high adversarial detection rate (> 94% at 5% FPR). This means that
the original data manifold is sparse. Once there is a shortcut created
by the trapdoors nearby, any adversarial perturbation will follow
this created shortcut with high probability and thus get “trapped.”
6 EVALUATION
We empirically evaluate the performance of our basic trapdoor de-
sign against an static adversary described in §3.1. We present eval-
uation results against adaptive adversaries (skilled and oracle) in §7.
Speciﬁcally, we design experiments to answer these questions:
• Is the trapdoor-enabled detection we propose effective against the
strongest, state-of-the-art attacks?
• How does the presence of trapdoors in a model impact normal
classiﬁcation accuracy?
• How does the performance of trapdoor-enabled detection com-
pare to other state-of-the-art detection algorithms?
• How does the method for computing trapdoor signature impact
the attack detection?
We ﬁrst consider the base scenario where we inject a trapdoor to
defend a single label in the model and then expand to the scenario
where we inject multiple trapdoors to defend all labels.
2In this work, we use the total variation distance [11] as it has been shown to be a
natural way to measure statistical distances between distributions [11]. Other notions
of statistical distance may also be applied, which we leave to future work.
Table 1: Dataset, complexity, model architecture for each task.
Task
Digit
Recognition
Trafﬁc Sign
Recognition
Image
Recognition
Dataset
MNIST
GTSRB
CIFAR10
Facial
YouTube
Recognition
Face
# of
Labels
Input
Size
Training
Images
Model
Architecture
10
43
10
28 × 28 × 1
60,000
2 Conv, 2 Dense [8]
32 × 32 × 3
35,288
6 Conv, 2 Dense [9]
32 × 32 × 3
50,000
20 Resid, 1 Dense [10]
1,283
224 × 224 × 3
375,645
ResNet-50 [20]
6.1 Experimental Setup
Here we introduce our evaluation tasks, datasets, and conﬁguration.
Datasets. We experiment with four popular datasets for classiﬁ-
cation tasks. We list the details of datasets and model architectures
in Table 1 and in the Appendix.
• Hand-written Digit Recognition (MNIST) – This task seeks to
recognize 10 handwritten digits in black and white images [26].
• Trafﬁc Sign Recognition (GTSRB) – Here the goal is to recognize
43 distinct trafﬁc signs, emulating an application for self-driving
cars [44].
• Image Recognition (CIFAR10) – This is to recognize 10 different
objects and it is widely used in adversarial defense literature [23].
• Face Recognition (YouTube Face) – This task is to recognize
faces of 1, 283 different people drawn from the YouTube videos [51].
Adversarial Attack Conﬁguration. We evaluate the trapdoor-
enabled detection using six representative adversarial attack meth-
ods: CW, ElasticNet, PGD, BPDA, SPSA, and FGSM (described in
§2.1). We use them to generate targeted adversarial attacks against
the trapdoored models on MNIST, GTSRB, CIFAR10, and YouTube
Face. More details about our attack conﬁguration are in Table 12 in
the Appendix. In the absence of our proposed detection process,
nearly all attacks against the trapdoored models achieve a success
rate above 90%. Attacks against the original, trapdoor-free models
achieve roughly the same success rate.
Conﬁguration of Trapdoor-Enabled Detection. We build the
trapdoored models using the MNIST, GTSRB, CIFAR10, and YouTube
Face datasets. When training these models, we conﬁgure the trap-
door(s) and model parameters to ensure that the trapdoor injection
success rate (i.e. the accuracy with which the model classiﬁes any
test instance containing a trapdoor to the target label) is above 97%
(results omitted for brevity). This applies consistently to both single
and all label defenses. Detailed defense conﬁgurations can be found
in Table 11 in the Appendix.
Evaluation Metrics. We evaluate the performance of our pro-
posed defense using (1) the adversarial detection success rate and
(2) the trapdoored model’s classiﬁcation accuracy on normal inputs.
For reference, we also compute the original model’s classiﬁcation
accuracy on normal inputs.
6.2 Defending a Single Label
We start with the simplest scenario. We inject a trapdoor for a single
(randomly chosen) label yt . We consider the trapdoor ∆ = (M, δ , κ)
as a 6 × 6 pixel square at the bottom right of the image, with a
mask ratio κ = 0.1. An example image of the trapdoor is shown in
Figure 11 in the Appendix.
Table 2: Adversarial detection success rate when defending a
single label at 5% FPR, averaged across all the labels.
Model
MNIST
GTSRB
CIFAR10
YouTube Face
CW ElasticNet PGD BPDA SPSA FGSM
100%
95.0%
96.3%
100%
96.4%
100%
97.5%
97.0%
96.7%
100%
97.0%
98.8%
100%
93.8%
100%
96.8%
100%
100%
100%
100%
100%
100%
100%
100%
Comparing Trapdoor and Adversarial Perturbation. Our de-
fense is driven by the insight that a trapdoor ∆ will trick an ad-
versary into generating an x + ϵ whose neuron activation vector
is similar to S∆, the trapdoor signature. We verify this insight by
examining the cosine similarity of ❕(x + ϵ) and S∆. We show the
results for GTSRB, while the results for other tasks are consistent
(see Figure 12 and Figure 13 in the Appendix).
Figure 3(a) plots, for all six attacks against the trapdoored model,
the quantile distribution of cos(❕(x + ϵ), S∆) across x. For reference
we also include the result for benign images cos(❕(x), S∆) as the
leftmost boxplot. We see that, for all six attacks, the distribution of
cosine similarity for adversarial inputs is visibly different from that
of benign inputs and thus can be detected by applying a threshold
ϕt . Furthermore, the distribution of cos(❕(x), S∆) can be used to
conﬁgure ϕt to maximize the adversarial example detection rate at
a given false positive rate (FPR).
Figure 3(b) shows the same quantile distribution in the original,
trapdoor-free model. As expected, the original model does not pro-
duce a clear difference between normal and adversarial inputs. This
conﬁrms that the trapdoor can largely affect the shape of adversarial
perturbations against the trapdoored model.
For all six attacks
Accuracy of Detecting Adversarial Inputs.
and all four tasks, Table 2 shows the average adversarial detection
success rate when defending a single label. Here we iteratively test
our defense on every label in the model, one at a time, and compute
the average defense success rate across all the labels3. Detection
success is > 93.8% at an FPR of 5% (> 89% at FPR of 2%). We also
show the ROC curves and AUC values in Figure 4 and Figures 7-
9 in the Appendix. Across all six attacks and four tasks, detection
AUC is > 98%.
Finally, we conﬁrm that a single label trapdoor has negligible
impact to model classiﬁcation on normal inputs.
6.3 Defending All Labels
We trained MNIST, GTSRB, CIFAR10, and YouTube Face models
with a trapdoor for every output label. Each trapdoor is a randomly
selected set of 5 squares (each 3 × 3 pixels4), with κ = 0.1. The
minimum trapdoor injection success rate across the labels is 97%
even after injecting 1, 283 trapdoors into the YouTube Face model.
Impact on Normal Classiﬁcation Accuracy. We ﬁrst evaluate
whether the presence of these trapdoors in the model affects the
model’s normal classiﬁcation accuracy. We compare the trapdoored
model classiﬁcation accuracy to the original model classiﬁcation
accuracy on normal inputs in Table 3. The all-label trapdoored model’s
accuracy on normal inputs drops by at most 1.04% when compared
3Due to the large number of labels in the YouTube Face dataset, we randomly sample
100 labels out of 1,283 to defend.
4The size of each square is 21 × 21 for YouTube Face, which has higher resolution
(224 × 224) images.
y
t
i
r
a
l
i
i
m
S
e
n
s
o
C
i
 1
 0.8
 0.6
 0.4
 0.2
 0
y
t
i
r
a
l
i
i
m
S
e
n
s
o
C
i
 1
 0.8
 0.6
 0.4
 0.2
 0
Benign
CW
PGD Elastic
BPDA SPSA FGSM
Benign
CW
PGD Elastic
BPDA SPSA FGSM
Net
(a) Trapdoored Model
Net
(b) Original Model
e
t
a
R
e
v
i
t
i
s
o
P
e
u
r
T
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
CW (AUC 0.98)
ElasticNet (AUC 1.0)
PGD (AUC 1.0)
BPDA (AUC 1.0)
SPSA (AUC 0.98)
FGSM (AUC 1.0)
 0.2
 0.4
 0.6
 0.8
 1
False Positive Rate
Figure 3: Comparison of cosine similarity between normal input/trapdoored
inputs and adversarial
inputs/trapdoored inputs on both trapdoored and
trapdoor-free GTSRB models. Boxes show inter-quartile range and whiskers
capture 5t h and 95t h percentiles.
Figure 4: ROC Curve of detection in a GT-
SRB model when a single label is protected
by a trapdoor.
Table 3: Trapdoored model and original model classiﬁcation ac-
curacy when injecting trapdoors for all labels.
Table 5: Comparing detection success rate of Feature Squeezing
(FS), LID, and Trapdoor when defending all labels.
Model
MNIST
GTSRB
CIFAR10
YouTube Face
Original Model
Trapdoored Model (All Labels)
Classiﬁcation Accuracy
Classiﬁcation Accuracy
99.2%
97.3%
87.3%
99.4%
98.6%
96.3%
86.9%
98.8%
Table 4: Adversarial detection success rate at 5% FPR when
defending all labels.
Model
MNIST
GTSRB
CIFAR10
YouTube Face
CW EN
96.8% 98.6% 100%
95.6% 96.5% 98.1%
94.0% 94.0% 100%