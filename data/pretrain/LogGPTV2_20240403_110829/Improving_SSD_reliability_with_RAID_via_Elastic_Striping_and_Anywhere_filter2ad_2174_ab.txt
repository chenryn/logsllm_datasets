in Fig. 3(a) with valid user data, D0-D7. Like conventional
RAID-5, eSAP starts with all pages of PPN 0 and 1 comprising
stripe 0 and 1, respectively. (As we will see, eSAP dynamically
reconstructs stripes and evenly utilizes all chips. Thus, there is
no need to purposely distribute parities as RAID-5 does.) Let
us again assume that D1∼D4 are modiﬁed. With eSAP, the
controller simply calculates the new parity for these pages,
writes them on the pages with PPN 2 along with the parity
value as shown in Fig. 4(a). After writing, the controller simply
marks the old pages as obsolete. There is no need to read the
old pages. Note here that every strip that comprises a stripe
always has the same PPN number. A stripe is constructed based
on arrival order of write request regardless of the Logical
Block Number (LBN). Furthermore, all chips are written to
evenly even if particular pages are more frequently written to,
including the parity page. This helps even the wear out of the
ﬂash chips.
To do this, one more piece of information is needed in the
Stripe map table compared to conventional RAID-5. Recall
that for RAID-5, an entry of the Stripe map table consists
of a PBN and PPN pair. Note that the chip number is not
needed as the LBN of the data designates the chip number.
However, for eSAP, since a data page may be allocated to any
chip, the Stripe map table must also include the Chip ID as
shown in Fig. 4. As shown in the ﬁgure, eSAP maintains an
L2P (Logical-to-Physical) map table instead of the Stripe map
table as in RAID-5. The L2P map translates the Logical Page
Number (LPN) that is requested from the ﬁle system layer to
PPN via the Chip ID, PBN, and PPN. The Chip ID occupies
only a few bits in the map entry. Speciﬁcally, for an SSD with
N number of chips, log2 N bits are required. Note that eSAP
is easily implementable in SSDs as a conventional SSD would
already maintain a map to relocate data.
information is kept in SDRAM, there is a chance that this
information could be lost upon system failure. Failure recovery
can be done similarly to other FTL schemes, and we do not
elaborate on this issue any further as it is beyond the scope of
this paper.
Now let us consider the case where only part of the stripe
is written. Continuing with the previous example, assume that
new data D8 and D9 are being written. At this point, eSAP
may choose to wait for more data to arrive to form a complete
stripe or may choose to write the data as-is forming a partial
stripe. If the choice is to wait for data to form a full stripe,
then reliability may suffer as D8 and D9 are residing in volatile
memory. Hence, data will be lost upon a system crash at this
point. Reliability can be enhanced by writing the data and
parity immediately or after some speciﬁed period of time. The
example in Fig. 4(b) shows the system state after D8, D9, and
their parity have been written; D8 and D9 has been allocated
to PPN 3 of Chips 0 and 1, respectively, while the parity for
these data, denoted PSP (for Partial Stripe Parity), has been
written to Chip 2. As more data arrive, say D10, D10 may be
written to Chip 3 and the last parity calculated and written to
Chip 4.
The eSAP scheme, at ﬁrst glance, seems to reduce the ef-
fective capacity of the SSD compared to RAID-5 in case partial
parities are generated. However, this is not the case as space
used to store partial parities are consumed only temporarily as
they are reclaimed when garbage collection occurs, as we will
see later. The negative impact of partial parities is that they are
prone to increase the frequency of cleaning as partial parities
take up space, albeit temporarily. The experimental results and
the analytic models that we discuss later reﬂect the effect of
the increased cleaning frequency.
The eSAP scheme increases reliability in two aspects. First,
two or more errors can be recovered if they fall into different
partial stripes that are protected by different parities. Second,
eSAP decreases the total number of write and erase operations,
leading to slower wear and less error rate. These results in
eSAP providing stronger reliability than RAID-5. Note also
that eSAP enhances performance as the typical 2 reads and
2 writes required for the RAID-5 small-write problem is not
necessary.
In terms of managing the L2P map table, typically, the
SSD controller would load the map partially or in its entirety
onto SDRAM during boot-up. When new data is written, the
map is updated to reﬂect this change. To maintain consistency,
the map is periodically destaged to ﬂash memory. As this map
2) Cleaning in eSAP-RAID: Since ﬂash memory suffers
from out-of-place-update, updated data must be written to free
pages. If there is no free space to write to, a cleaning operation
is invoked to make free space. Fig. 5 depicts the situation
within the SSD before and after cleaning in eSAP. To describe
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:49:49 UTC from IEEE Xplore.  Restrictions apply. 
PBN DSG 1 DSG 0 B0 PPN Chip 1 0 D2’ D8’ D8’ D48 D48 D60 D60 D1 D1 D5 D9 D9 D13 D3’ D9’ D9’ D49 D49 D61 D2 D2 D6 D10 D14 D4’                                                                            D14’ D14’ D31 D62 D62 D3 D3 D7 D11 D15 D15 Chip 2 Chip 3 1 2 3 0 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 B1 0 1 2 3 B15 0 1 2 3 0 1 2 3 B0 PPN B1 B15 B0 PPN B1 B15 B0 B1 B15 P4 P4 P5 P6 P7 P7 P0 P1 P1 P2 P3 Chip 4 0 1 2 3 0 1 2 3 0 1 2 3 PPN D1’ D7’ D7’ D47 D47 D32 D0 D4 D4 D8 D8 D12 Chip 0 0 1 2 3 0 1 2 3 0 1 2 3 PPN B0 B1 B15 ... ... ... ... ... 1 3 2 DSG 15 PBN DSG 1 DSG 0 B0 PPN Chip 1 0 D1 D1 D5 D9 D9 D13 D2 D2 D6 D10 D14                                                                           D3 D3 D7 D11 D15 D15 Chip 2 Chip 3 1 2 3 0 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 B1 D2’ PSP D3’ D31 0 1 2 3 B15 0 1 2 3 0 1 2 3 B0 PPN B1 B15 B0 PPN B1 B15 B0 B1 B15 P0 P1 P1 P2 P3 Chip 4 0 1 2 3 0 1 2 3 P60 0 1 2 3 PPN D0 D4 D4 D8 D8 D12 Chip 0 0 1 2 3 0 1 2 3 D1’ D32 0 1 2 3 PPN B0 B1 B15 ... ... ... ... ... 1 3 2 DSG 15 the cleaning process, we deﬁne a notion of a Dynamic Stripe
Group (DSG). A DSG is composed of physical blocks that
comprise a stripe, which means that these physical blocks are
all of the same block number. For example, as shown in Fig. 5,
block 0 of all chips comprise DSG0, block 1 of all chips
comprise DSG1, and so on.
The cleaning process of eSAP proceeds in two steps. First,
the DSG with the smallest number of valid pages is chosen as
the victim DSG. In our example in Fig. 5, DSG1 is selected.
Note that for cleaning purposes, at least one empty DSG must
always be available. In our example, this is DSG15.
In the second step, valid data pages taken from the selected
DSG and the parity calculated from them form a stripe, which
is written out to the empty DSG. Note that the parity pages
are not copied to the new empty DSG. The old parity pages
are simply discarded and new parity pages are calculated with
only the valid data pages. In our example, there are 5 valid
data pages, and among them D1’, D2’, D3’, and D31 and
their newly calculated parity page, P60, form a stripe, and are
stored in Chips 0 through 4. The last remaining valid page,
D32, forms a partial stripe with the PSP that is calculated for
this page, and they are stored in Chip 0 and 1, respectively, as
shown in Fig. 5(b).
IV. ANALYTIC MODELS OF RAID SCHEMES
In this section, we derive the analytic models of the RAID
schemes of interest. To this end, we start off by reviewing
what Write Ampliﬁcation Factor (WAF) is as it is used in the
analyses. Then in the ﬁrst subsection, we derive the analytic
model for write performance and lifetime of an SSD that
employs the traditional RAID-5 scheme. In Section IV-B, we
present the analytic model for SSDs that employ the eSAP-
RAID scheme.
Write Ampliﬁcation Factor (WAF) is a notion that incor-
porates the garbage collection (cleaning) cost in ﬂash memory
writes as ﬂash memory storage recycles used space via the
garbage collection (GC) operation [13], [16]. It is deﬁned as
follows:
W AF (u) =
u · NP
(1 − u) · NP
(1)
where u is the utilization of victim blocks selected for GC
and Np is the number of pages in a block. From Eq. 1, we can
see that W AF (u) depends on utilization u, which is the ratio
of valid pages in the block selected for garbage collection.
Utilization u can be measured in real SSDs, or we can
infer it from the ratio of the data size stored in the SSD and
the initial size of the Over-Provisioned Space (OPS), which is
space reserved for garbage collection [16], [21], [25]. Eq. 1
tells us that, on average, the GC operation copies u · Np
pages from the victim block to an empty block. As a result,
the empty block is now left with (1 − u) · Np clean pages.
This is saying that the u · Np extra page reads and writes are
amortized along the next (1 − u) · Np writes to clean pages
that were just generated. In summary, we can say that when
we write N req
pages, we can calculate the actual number
of page writes occurring in ﬂash memory storage as being
N req
· (1 + W AF (u)).
P
p
A. Analysis of SSDs with RAID-5
To derive the analytic model of SSDs that employ RAID-
5, we make the following assumptions. First, we assume that
the starting position of write requests within a stripe, as well
as the parities, are evenly distributed among the ﬂash memory
chips and thus, the ﬂash memory chips are evenly utilized.
Second, we assume that there is no hardware component such
as NVRAM. Even so, we assume that, if the whole stripe
does not ﬁll up after writing data, the parity is not written
immediately, but the stripe is left to wait for other requests to
arrive for as much as Pwait time, which is a predetermined
parameter. If subsequent write requests arrive within Pwait,
then they are written to the same stripe of the previous request
(up to the full stripe), then the SSD consolidates these requests
and counts them as a single request. If, however, after writing
the ﬁrst request, the next request does not arrive within Pwait,
the SSD writes the newly written partial stripe through the
read-modify-write or reconstruct-write mechanisms, whichever
is more effective. Requests that follow are considered to be
a separate request. Finally, we assume perfect wear-levelling
within the ﬂash memory chip comprising the SSD.
Now, let us assume that the average data size requested to
the SSD is Sreq. (Note that Sreq can be measured from the
workload.) Then, the average number of page writes incurred
by a write request can be calculated as follows, where SP
refers to the page size of ﬂash memory:
N req
P =
Sreq
SP
P
Let N ST R
denote the number of pages that comprise a
− 1 data pages and
stripe. Then a stripe consists of N ST R
− 1 and N req rem
=
P = N ST R
one parity page. Denote N D
(N req
P R5 , the average
number of parity writes for writing Sreq data, which incurs
N req
P +1. Now, we can derive, N parity
P −1)%N D
P
P
P
P
page writes, as
N parity
P − N req rem
N D
· 2 + (cid:98)|N req
− 1
P
N D
P
+ 1
+
P − 1|
N D
P
(cid:99)
P R5 =
N req rem
N D
P
P
(2)
P pages, at least (cid:98)|N req
To understand this equation, let us take an example shown
in Fig. 6(a), where ﬁve pages comprise a stripe and, for
simplicity, the parity pages are not interleaved. If the data size
(cid:99) full stripes are
is bigger than N D
needed to write the data and, thus, that number of parities need
to be written along with data. Note that (cid:98)|N req
P −1|
(cid:99) is 1, in our
N D
example, and that (cid:98)|N req
(cid:99) is 0 if the data size is less than
P
or equal to N D
P .
P −1|
N D
P
P −1|
N D
P
Then, we have to consider the case where a full stripe may
not be completely ﬁlled as we see in Fig. 6(a). The situation
may be that there is a head part, tail part, or both head and tail
part of the data that do not ﬁt in the full stripe(s). Then we
can calculate the average number of parity writes incurred by
these remaining data. (Here, we assume that the head and tail
data are consecutive though there may be some data between
them as in the case of Fig. 6(a)(2).)
Take Fig. 6(b) as an example where six data pages are
being written. In the ﬁgure, there are four shaded data pages
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:49:49 UTC from IEEE Xplore.  Restrictions apply. 
(a) Parity update #1
(b) Parity update #2
(c) Parity update of RAID-5
(d) Parity update of eSAP
Fig. 6: Data and parity deployment of RAID-5 and eSAP
and two unshaded data pages. In this example, the four shaded
data pages incur one parity write for all the cases and the
unshaded remaining two data pages incur one or two parity
writes according to their starting position. Generalizing this
example, Fig. 6(c) shows the various situations where the
remaining data pages may be written to the four data pages.
Note that Fig. 6(b) is one case, that is, the two pages case, of
Fig. 6(c).
Now to derive the calculations of Eq. 2, let us take the
example shown in Fig. 6(c). Consider the four cases (1)
through (4) in the ﬁgure. If the remaining data size is only one
page, then writing it always incurs one parity write as seen in
Fig. 6(c)(1). When two data pages remain, with probability 3/4,
one parity write is required, while with probability 1/4, two
parity writes are required (Fig. 6(c)(2)). Similarly, when three
data pages remain, the probability of one parity write and two
parity writes are both 2/4, while when four data pages remain,
the probability of one parity write and two parity writes is 1/4
and 3/4, respectively (Fig. 6(c)(3) and (4)).
P
+1
P
N D
P
pages,
ing data size is N req rem
P −N req rem
N D
Now, we generalize these examples. When the remain-
then, with probability
, the data ﬁts in a single stripe and incurs one
parity write. Otherwise, with probability N req rem
, the data
N D
P
spans two stripes incurring two parity writes. By summing
these, we arrive at Eq. 2 for the average number of parity
writes for N req
page write requests.
−1
P
P
P R5 , we calculate the parity overhead for writing
From N parity
a data page, N P O
P R5 as follows:
N P O
P R5 =
N parity
P R5
N req
P
Hence, a write request of N req
P (1 + N P O
pages actually requires
N req
P R5) page writes due to the parity writes.
When we consider the garbage collection cost incorporated
in W AF (u), N req
P W R5, the actually written pages for serving
a write request in SSD using RAID-5 architecture becomes
P
N req