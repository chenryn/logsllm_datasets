speakers of smart appliances) placed in the vicinity.
We recruited 4 participants to play the role of victim, 2 being
males (p-1 & p-4) and 2 being females (p-2 & p-3). Each participant
was first asked to read out 3 arbitrary speech commands (10 times
for each) without playing the adversarial perturbation, which serves
as the baseline for the system. Then, each participant was asked
to say the same set of speech commands again (10 times for each)
in both scenarios with the adversarial perturbation playing in the
background. Figure 10 depicts the calculated baseline accuracy and
the attack success rate. As we can see, the system has a relatively
high accuracy when facing benign inputs, with an average baseline
accuracy of over 95%. Benefiting from the synchronization-free and
audio-agnostic adversarial perturbation optimization process, our
attack manged to reach an average accuracy of 98.7% (only failed 3
times out of a total number of 240 trails in these two scenarios).
Figure 9: Illustration of the
live human speech setup.
Figure 10: Results of attack-
ing live human speech.
6.4 Impact of Perturbation Loudness
We used the loudspeaker setup as shown in Figure 9 to study the
impact of adversarial perturbation loudness. The loudness of both
the adversarial perturbation and human speech were measured
using a noise meter (i.e., RISEPRO decibel meter) placed at the
smartphone’s position. We asked one participant (p-4) to read out
5 arbitrary speech commands, each for 10 times, while we played
the adversarial perturbation in the background with different loud-
ness by varying the volume of the loudspeaker. For reference, we
measured the ambient noise level in the quiet room to be around
41 − 43 dBSPL, and the sound pressure level of the victim’s voice to
be around 60 − 62 dBSPL. As shown in Table 3, the attack success
rate can reach 100% when the adversarial perturbation is played at
55 or 60 dBSPL. The attack success rate decreases slightly when the
volume is tuned down to 50 dBSPL, and is further reduced when
the adversarial perturbation is played at a very small volume (64%
at 45 dBSPL that is similar to the ambient noise). Considering the
ultra-short duration of the adversarial perturbation and its high
similarity with the situational environmental sound, our attack can
be launched using a low volume (comparable to the ambient noise)
to comprise systems without raising any suspicion. In addition to
the perturbation loudness, we also studied the impact of attack
distance with details presented in Appendix A.4.
7 DEFENSES
7.1 Performance Under Defense Settings
Some prior studies [14, 53, 54] have pointed out that applying
audio pre-processing techniques before the recognition process to
reduce the fidelity of the signal can potentially make the fragile
adversarial perturbation lose its effectiveness. We evaluated our
attack against four commonly-used defenses: (1) Low-pass Filtering:
Apply a Butterworth low-pass filter with various cutoff frequencies;
(2) Quantization (used in prior work [53]): Round the 16-bit signed
integer amplitude values to its nearest integer multiple of q, where
q is set to be 256, 512 and 1024; (3) MP3 Compression (used in prior
work [14]): Perform MP3 compression on the adversarial example
prior to recognition; and (4) Down-sampling (used in prior work
[53, 54]): Down-sample the adversarial example to a lower sampling
rate (e.g., 2, 4, and 8kHz) and then perform signal recovery3.
Specifically, we used a set of clean audio samples from each
class to generate adversarial examples targeting the remaining
9 classes, by adding adversarial perturbations with random time
delay. This results in a total number of 154 clean audio samples
and 1, 386 adversarial examples for the speaker recognition system,
and 200 clean audio samples and 1, 800 adversarial examples for
3The sampling rates of audio inputs used by the speaker recognition system and the
speech command recognition system are 48kH z and 16kH z, respectively.
the speech recognition system respectively. From Table 4, we can
observe that these defense techniques barely impact the adversarial
examples generated for the speech recognition system: the lowest
attack success rate occurred when quantization with q = 1024
is applied, which is 96.8%, while the system accuracy on benign
inputs is decreased to 74.0%. While for the speaker recognition
system, when applying low-pass filtering and quantization, we
observe that the system accuracy on benign inputs decreases faster
than the attack success rate of the adversarial examples, rendering
these defenses impractical. Although MP3 compression and down-
sampling are shown to be relatively effective on the adversarial
examples, they also significantly affect the recognition accuracy
of benign inputs. This result demonstrated that simple audio pre-
processing techniques either are ineffective or will largely degrade
the system performance with normal audio inputs.
7.2 More Advanced Adaptive Attacks
A sophisticated adversary can further design adaptive attacks against
these input-transformation-based defenses by incorporating the
corresponding transformation into the optimization process. For
instance, to bypass low-pass-filtering-based defenses, the adversary
can simply lower the frequency upper bound of the band-pass filter
in Equation 6. For the transformations that are not smooth or not
directly differentiable, the adversary can use backward pass differ-
entiable approximation (BPDA) [5] to get approximated gradients.
For example, to bypass MP3 compression, at each optimization
step in Algorithm 1, the adversary can update the perturbation δ
according to gradients calculated on L(cid:0)f (MP3(x′)), yt
(cid:1).
8 DISCUSSION
8.1 Attacking ASR Systems with Long Inputs
Previously we evaluated our attack on speaker recognition sys-
tem with input speech containing short sentences (∼ 5 words) and
speech command recognition system for recognizing single-word
commands. In this subsection, we discuss the possibility of extend-
ing our attack to more sophisticated automatic speech recognition
(ASR) systems with the ability to transcribe long sentences.
Temporal Dependency of Sequence-to-sequence Speech
Recognition System. Speaker and speech command recognition
models (our target systems) are essentially sequence-to-vector mod-
els, which usually utilize a temporal pooling layer to aggregate the
extracted frame-level features across the time dimension so that a
decision can be made with information collected from the entire
speech. Differently, ASR systems leverage sequence-to-sequence
models (e.g., recurrent neural network (RNN)) to transcribe a speech
signal into corresponding text, where the output relies on not only
the input at the current time step, but also the hidden state which
encodes the representations learned from previous inputs. This
structural difference should be taken into account when designing
the attack against these two types of models. For instance, as shown
in Figure 11, by injecting a short adversarial perturbation at arbi-
trary frames an adversary can impact the final recognition result of
speaker recognition systems directly. While, for ASR systems, the
influence of short perturbation injected at one frame is brought to
its subsequent frames through the altered hidden states. Moreover,
to take into account articular and linguistic dependencies, modern
LoudspeakerLaptopSmartphoneVictimp-1p-2p-3p-4Victim ID020406080100Attack Sucess Rate (%)020406080100Baseline Accuracy (%)LaptopLoudspeakerBaselineTable 4: Attack performance in the presence of commonly-used defenses.
Speaker Recognition Systen
Speech Command Recognition System
Accuracy on Benign Inputs
Attack Success Rate
Accuracy on Benign Inputs
Attack Success Rate
No Defense
98.0%
98.9%
87.0%
98.0%
2 kHz
51.9%
80.8%
87.5%
98.0%
4 kHz
72.0%
90.2%
87.5%
98.0%
Low-pass Filtering
8 kHz
93.5%
94.6%
−
−
Quantization
512
256
77.9% 43.5% 18.2%
98.4% 96.0% 77.5%
84.0% 83.0% 74.0%
98.8% 98.7% 96.8%
1024 Mp3 Compression 8 kHz
11.7%
10.5%
88.0%
97.9%
52.6%
26.3%
88.0%
98.0%
Down-sampling
16 kHz
29.9%
21.0%
−
−
Figure 11: Structural difference between speaker recogni-
tion system and automatic speech recognition system. Areas
marked red represent adversary-perturbed parts.
ASR systems utilize bidirectional models to correct the interpre-
tation of the current phoneme by looking at both past and future
phonemes, making it possible to add perturbation at one time step
and change the transcription of the entire sequence.
Attacking Long Inputs. To verify the feasibility of attacking
ASR systems with long input sentences, we used DeepSpeech [24]
as the target system with connectionist temporal classification
(CTC) loss [22] for our proof-of-concept experiments. Figure 12 (a)
showcases a scenario where the added perturbation only covers
the word “kitchen”, but is able to influence the transcription of
the subsequent words through changing hidden representations,
resulting in the whole command to be falsely recognized as “un-
lock the door”. During our experiments, we observed that it is hard
for the perturbation added at one point to penetrate even longer
audio inputs, with the effect decaying usually after 2 − 3 words.
However, according to Amazon4, the mostly frequently used voice
commands are usually only 2 − 4 words long, making it sufficient
to attack common voice commands in practice. For longer com-
mands, attacks are still feasible by injecting multiple perturbation
segments. For instance, in Figure 12 (b) we successfully made “turn
down the heat and start the music” to be recognized as “unlock
the door” by injecting two short perturbations. In our future work,
we will explore the effect of hidden states and input conditions at
varying time steps on the transcription results to further improve
the robustness and synchronization freedom of the attack.
8.2 Practical Deployment
Different from existing studies, our proposed AdvPusle enables man-
in-the-middle attacks on smart audio systems, where an adversary
alters the user’s streaming speech on the fly. This opens up many
human-involved attack scenarios that are previously infeasible if
having following restrictions: (1) The system requires the user to
be present to enable its voice-interface (e.g., some systems may
come equipped with sensor-based liveness detection schemes); (2)
The process involves interactive audio inputs, making it hard for
an adversary to foresee the conversational content and craft adver-
sarial examples beforehand accordingly. For instance, interactive
4https://www.amazon.com/alexa-skills/b?ie=UTF8&node=13727921011
(a) Injecting single perturbation
(b) Injecting multiple perturbations
Figure 12: Illustration of attacking ASR systems with short
perturbations.
voice response (IVR) systems used for telephone banking require
the user to interact with the system or a human representative
while passing the speaker verification process; (3) The scenario re-
quires to actively alter the recognition result of the victim’s speech.
For example, an adversary wants to make voice-controllable smart
vehicles do the opposite as the driver’s commands to potentially
cause severe damage. The enabled properties by AdvPusle would
be desirable to many more human-involved attack scenarios.
In our experiments, we chose to use bird singing and phone noti-
fication sound as the environmental sound template for illustration.
In practice, the perturbation can also be disguised as continuous
noises such as HVAC noise, car engine noise, or music, to be played
periodically. Additionally, the user might notice something suspi-
cious if the command is continued to be carried out incorrectly.
However, in most cases, the malicious command may have already
been executed and caused corresponding consequences (e.g., door
has been opened; vehicle has been steered to a wrong direction).
9 CONCLUSION
In this work, we proposed AdvPulse, a practical adversarial au-
dio attack against intelligent audio systems in the scenario where
the system takes streaming audio inputs (e.g., live human speech).
Unlike existing attacks that require the adversary to have prior
knowledge of the entire audio input, we generated input-agnostic
universal subsecond audio adversarial perturbations that can be
injected anywhere in the streaming audio input. We also deliber-
ately made it akin to environmental sounds to minimize suspicion
while launching the attack. Additionally, various sources of audio
distortions caused by physical playback are considered to improve
the robustness of the perturbations during over-the-air propagation.
Extensive experiments against both speaker and speech command
recognition models under various realistic scenarios demonstrated
the attack’s effectiveness.
ACKNOWLEDGMENTS
We would like to thank our anonymous reviewers for their construc-
tive feedback. This work was supported in part by National Science
Foundation Grants CNS1820624, CNS1814590, and Air Force Re-
search Laboratory Grant FA87501820058.
“kitchen lights off”Input SpeechASR SystemRecognition ResultSpeaker 5Input SpeechSpeaker Recognition SystemRecognition ResultSpeaker 3“unlock the door”00.511.5Time (s)−0.6−0.4−0.20.00.20.40.6Amplitude"Kitchenlightsoff"ˈkɪʧənlaɪtsɔfRecognized as: "Unlock the door"Audio InputAdversarial Perturbation00.511.52Time (s)−0.6−0.4−0.20.00.20.40.6Amplitude"Trun down the heat and start the music"tɜrndaʊnðəhitændstɑrtðəˈmjuzɪkRecognized as: "Unlock the door"Audio InputAdversarial Perturbation[5] Anish Athalye, Nicholas Carlini, and David Wagner. 2018. Obfuscated gradients
give a false sense of security: Circumventing defenses to adversarial examples.
arXiv preprint arXiv:1802.00420 (2018).
[6] Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. 2017. Synthe-
sizing robust adversarial examples. arXiv preprint arXiv:1707.07397 (2017).
[7] Chase Bank. 2019. Security as unique as your voice. https://www.chase.com/
personal/voice-biometrics.
[8] Karissa Bell. 2015. A smarter Siri learns to recognize the sound of your voice in
iOS 9. https://mashable.com/2015/09/11/hey-siri-voice-recognition/
[9] Nicholas Carlini and David Wagner. 2017. Towards evaluating the robustness of
neural networks. In Proceedings of the IEEE Symposium on Security and Privacy
(SP). 39–57.
[10] Nicholas Carlini and David Wagner. 2018. Audio adversarial examples: Tar-
geted attacks on speech-to-text. In Proceedings of the IEEE Security and Privacy
Workshops (SPW). 1–7.
[11] Guangke Chen, Sen Chen, Lingling Fan, Xiaoning Du, Zhe Zhao, Fu Song, and
Yang Liu. 2019. Who is Real Bob? Adversarial Attacks on Speaker Recognition
Systems. arXiv preprint arXiv:1911.01840 (2019).
[12] Tao Chen, Longfei Shangguan, Zhenjiang Li, and Kyle Jamieson. 2020. Meta-
morph: Injecting Inaudible Commands into Over-the-air Voice Controlled Sys-
tems. In Proceedings of the Network and Distributed System Security Symposium
(NDSS).
[13] Moustapha M Cisse, Yossi Adi, Natalia Neverova, and Joseph Keshet. 2017. Hou-
dini: Fooling deep structured visual and speech recognition models with adversar-
ial examples. In Proceedings of Advances in neural information processing systems