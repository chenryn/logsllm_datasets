ing packet rate. We implemented this modiﬁcation into the
libpcap which used PF PACKET. We found that it is not an
easy task to choose a proper sleep interval, because sleeping
too long will result in ﬁlled buﬀers, whereas a sleep inter-
val that is too short will result in too many system calls and
therefore does not solve the problem. Unfortunately, the op-
timal sleep interval depends on the hardware, the performed
analysis and, even worse, on the observed traﬃc. Hence, a
good value has to be found through the end-user by exper-
iments, which requires quite some eﬀort. Deri’s user space
code that uses PF RING does not implement his proposed
adaptive sleep, probably due to the same reasons. Instead,
poll() avoidance is achieved by calls to sched yield(), which
interrupts the application and allows the scheduler to sched-
ule another process. A call to pool() is only performed if
several sched yield() was called for several times and still no
packets arrived. Figure 11 shows that the algorithm used by
PF RING yield better performance compared to the simple
call to poll() with PF PACKET. However, we can also see
negative eﬀects of calling sched yield() often.
We therefore propose a new third solution that works
without the necessity to estimate a timeout and works bet-
ter than calling sched yield(): We propose to change the
signalling of new incoming packets within poll(). As of
now, PF PACKET signals the arrival of every packet into
the user space. We recommend to only signal packet arrival
if one of the two following conditions are true:
• N packets are ready for consuming.
• A timeout of m microseconds has elapsed.
Using these conditions, the number of system calls to poll()
3We conﬁrmed this by measuring the number of calls to
poll().
is reduced dramatically. The timeout is only necessary if
less than N packets arrive within m, e.g.
if the incoming
packet rate is low. In contrast to the sleep timeout discussed
before, choosing m properly is not necessary to achieve good
capturing performance.
Figure 12: Capturing 64 byte packets with and with-
out modiﬁcations
We implemented all the solutions into PF PACKET and
libpcap and compared their implications on the performance.
For the sleep solutions, we determined a good timeout value
by manual experiments. The results are summarised in Fig-
ure 12. As we can see, the reduced number of system calls
yields a large performance boost. We can see that both time-
out and our proposed modiﬁcation to PF PACKET yield
about the same performance boost. Combining both is pos-
sible, but does not result in any signiﬁcant further improve-
ments. We did not have the time to include and evaluate
the same measurements with PF RING, but we are con-
ﬁdent that PF RING will also beneﬁt from our proposed
modiﬁcation.
4.4 Driver Improvements
In the experiments presented up to now, we were not able
to process small packets at wire-speed, even with our previ-
ous improvements. We now move further down the captur-
ing software stack and test driver improvements.
Deri proposed to use modiﬁed drivers in order to improve
the capturing performance [5]. His driver modiﬁcations fo-
cus on changing two things previously described:
• Create a dedicated thread for the packet consumption
in the driver (respectively for every RX queue of the
card).
• Reuse DMA memory pages instead of allocating new
pages for the card.
His driver modiﬁcations hence help to use the power of
multi-core CPUs by spawning a kernel thread that handles
the card. This implies that no other thread is scheduled
to perform the driver tasks and ensures that there is al-
ways a free thread to handle the packets, which is good
for the overall performance. Additionally, if interrupts are
bound to a given core, the driver thread will run on the
same core. It is unclear to us which of the modiﬁcations has
 30 35 40 45 50 55 60 65 70 0 1 2 3 4 5 6 7 8 9Captured packets [%]Packzip compression levelNo modificationsModified PF_PACKETTimeout in libpcap Modified PF_PACKET & Timeout libpcap215kernel thread that is only responsible for the network card
can really help to improve performance. This is especially
useful if more than multi-core or multi-CPU systems are
available. With current hardware platforms tending to be
multi-core systems and the ongoing trend of increasing the
number of cores in a system, this feature can be expected
to become even more important in the future. In addition,
reusing memory pages as DMA areas and working with stat-
ically allocated memory blocks should be preferred over us-
ing dynamically allocated pages. However, it is unclear to
us which of this two recommendations results in the greatest
performance boosts.
Signalling between diﬀerent subsystems, especially if they
are driven by another thread, should always be done for
accumulated packets. This assumption is valid for all sub-
systems, ranging from the driver, over the general operat-
ing system stacks, up to the user space applications. We
therefore recommend the integration of our modiﬁcations to
PF PACKET into Linux.
Other areas besides packet capturing may also beneﬁt
from our modiﬁcation: A generic system call that allows to
wait for one or more sockets until N elements can be read
or a timeout is seen, whichever happens ﬁrst, would be a
generalised interface for our modiﬁcations to PF PACKET.
Such system calls could be of use in other applications that
need to process data on-line as fast as the data arrives.
5.2 For Users
Conﬁguring a capture system for optimal performance is
still challenging task. It is important to choose proper hard-
ware that is capable of capturing the amount of traﬃc in
high-speed networks.
The software that drives the hardware is very important
for capture as well, since it highly inﬂuences the perfor-
mance of the capturing process. Our ﬁndings conclude that
all parts of the software stack have great inﬂuence on the
performance—and that, unfortunately, a user has to check
all of them in order to debug performance problems.
Performance pitfalls can start at the network card drivers,
if interrupt handling does not work as expected, which we
found to be true with one of the drivers we used. Checking
for an unexpectedly high number of interrupts should one
of the ﬁrst performance debugging steps. Here, enabling
polling on the driver could help to solve the issue. However,
we found that the POLLING option, a static compile time
options for many drivers, did not improve the performance
in our tests.
We also recommend to use PF RING with TNAPI, as its
performance is superior to the standard capturing stack of
FreeBSD or Linux. If using TNAPI or PF RING is not an
option, e.g., because there is no TNAPI-aware driver for the
desired network interface card, we recommend to use Linux
with our modiﬁcations to PF PACKET.
Regardless of the capturing solution used, pinning all the
involved threads and processes to diﬀerent cores is highly
recommended in most of the application cases. Using the de-
fault scheduler is only recommended when low packet rates
are to be captured with low load at the application layer.
Kernel and driver threads should also be pinned to a given
core if possible. This can be achieved by explicitly setting
the interrupt aﬃnity of a network cards’ interrupt.
Finally, if it is not an option to apply one of our proposed
techniques for reducing the number of system calls (cf. Sec-
Figure 13: TNAPI with diﬀerent capturing solutions
on Linux
If TNAPI is used
a bigger inﬂuence on the performance.
with PF PACKET instead of PF RING, an additional copy
operation is performed to copy the packet from the DMA
memory area into a new memory area. The DMA area is
reused afterwards.
We tested these improvements with diﬀerent solutions on
Linux but could not consider FreeBSD as there where no
modiﬁed drivers at the time this paper was written. Our
comparison is plotted in Figure 13. In contrast to our pre-
vious experiment, we deployed one more traﬃc generator,
which allowed us to generate traﬃc at real wire-speed (i.e.,
1.488 million packets). The plot shows results on the AMD
system and compares PF PACKET against our modiﬁed
PF PACKET and PF RING. As can be seen, the TNAPI-
aware drivers result in improvements compared to normal
drivers. PF PACKET capturing also beneﬁts from TNAPI,
but the improvements are not very signiﬁcant.
Using TNAPI with our modiﬁed PF PACKET results in
good capturing results which are better than the results with
standard drivers and also better than the results with stan-
dard FreeBSD. The best performance, however, can be found
when TNAPI is combined with PF RING, resulting in a
capturing process that is able to capture 64 byte packets at
wire-speed.
5. RECOMMENDATIONS
This section summarises our ﬁndings and gives recommen-
dations to users as well as system developers. Our recom-
mendations for developers of monitoring applications, drivers
and operating systems are listed in subsection 5.1. Users
of capturing solutions can ﬁnd advices on how to conﬁgure
their systems in subsection 5.2.
5.1 For Developers
During our evaluation and comparison, we found some
bottlenecks within the software which can be avoided with
careful programming. Developers of monitoring applica-
tions, operating systems or drivers should consider these
hints in order to increase the performance their systems pro-
vide.
Our ﬁrst advice is targeted at developers of network card
drivers. We were able to determine that having a separate
 0 20 40 60 80 1001x PF_PACKET2x PF_PACKET3x PF_PACKET1x modified PF_PACKET2x modified PF_PACKET3x modified PF_PACKET1x PF_PRING2x PF_RING3x PF_RINGCaptured packets [%]Number of capturing processes on different TNAPI solutions216tions 4.3 and 5.1), the user should check if performance im-
proves if he puts a higher load on the application capturing
process. As we have seen in 4.3, a higher application can
reduce the number of calls to poll() and therefore improve
the performance. This holds especially for applications that
do not involve much CPU workload, e.g., writing a libpcap
stream to disk.
6. CONCLUSION
This paper summarised and compared diﬀerent capturing
solutions on Linux and FreeBSD, including some improve-
ments proposed by researchers. We compared the standard
capturing solutions of Linux and FreeBSD to each other,
leading to a reappraisal of past work from Schneider et al [4]
with newer versions of the operating systems and capturing
software. The evaluation revealed that FreeBSD still outper-
forms standard Linux PF PACKET under low application
load. FreeBSD is especially good when multiple capturing
processes are run.
Our comparison between standard Linux capturing with
PF PACKET and PF RING conﬁrmed that performance
with PF RING is still better when small packets are to
be captured. However, diﬀerences are not as big as they
where in 2004 or 2006, when the last evaluations were pub-
lished. Further analyses showed that PF RING performs
better than PF PACKET if multiple capturing processes are
run on the system, and that performance with PF RING is
even better than FreeBSD. During our work, we found a per-
formance bottleneck within the standard Linux capturing
facility PF PACKET and proposed a ﬁx for this problem.
Our ﬁx greatly improves the performance of PF PACKET
with small packets. Using our improvements, PF PACKET
performs nearly as good as PF RING.
Finally, we evaluated Luca Deri’s TNAPI driver extension
for Linux and found increased performance with all Linux
capturing solutions. Best performance can be achieved if
TNAPI is combined with PF RING.
Acknowledgements
We gratefully acknowledge support from the German Re-
search Foundation (DFG) funding the LUPUS project in the
scope of which this research work has been conducted. Ad-
ditional work was funded through ResumeNet, an EU FP7
project within the FIRE initiative.
The authors would like to thank Luca Deri and Gerhard
M¨unz for their valuable support throughout our research.
7. REFERENCES
[1] Endace Measurement Systems,
http://www.endace.com/.
contemporary commodity hardware,” in In
Proceedings of the 8th International Conference on
Passive and Active Network Measurement, Apr. 2007.
[5] L. Deri and F. Fusco, “Exploiting commodity
multi-core systems for network traﬃc analysis,”
http://luca.ntop.org/MulticorePacketCapture.pdf,
July 2009.
[6] G. A. Cascallana and E. M. Lizarrondo, “Collecting
packet traces at high speed,” in Proc. of Workshop on
Monitoring, Attack Detection and Mitigation
(MonAM) 2006, T¨ubingen, Germany, Sep. 2006.
[7] J. Mogul and K. K. Ramakrishnan, “Eliminating
receive livelock in an interrupt-driven kernel,” ACM
Transactions on Computer Systems, vol. 15, no. 3, pp.
217–252, 1997.
[8] L. Deri, “Improving passive packet capture: Beyond
device polling,” in In Proceedings of the Foruth
International System Adminstration and Network
Engineering Conference (SANE 2004), Sep. 2004.
[9] Intel Corporation, “An Introduction to the Intel
QuickPath Interconnect,”
http://www.intel.com/technology/quickpath-
/introduction.pdf,
2009.
[10] I. Kim, J. Moon, and H. Y. Yeom, “Timer-based
interrupt mitigation for high performance packet
processing,” in 5th International Conference on
High-Performance Computing in the Asia-Paciﬁc
Region, 2001.
[11] Luigi Rizzo, “Device Polling Support for FreeBSD,” in
BSDCon Europe Conference 2001, 2001.
[12] V. Jacobson, C. Leres, and S. McCanne, “libpcap,”
http://www.tcpdump.org.
[13] Phil Woods, “libpcap MMAP mode on linux,”
http://public.lanl.gov/cpw/.
[14] R. Olsson, “pktgen the linux packet generator.”
[15] tcpdump, http://www.tcpdump.org.
[16] S. Kornexl, V. Paxson, H. Dreger, A. Feldmann, and
R. Sommer, “Building a Time Machine for Eﬃcient
Recording and Retrieval of High-Volume Network
Traﬃc,” in Proc. of ACM SIGCOMM Internet
Measurement Conference (IMC) 2005, Berkeley, CA,
USA, Oct. 2005.
[17] G. Maier, R. Sommer, H. Dreger, A. Feldmann,
V. Paxson, and F. Schneider, “Enriching Network
Security Analysis with Time Travel,” in Proc. of ACM
SIGCOMM 2008 Conference on Applications,
Technologies, Architectures, and Protocols for
Computer Communication, Seattle, WA, USA, Aug.
2008.
[2] F. Schneider and J. Wallerich, “Performance
[18] R. T. Lampert, C. Sommer, G. M¨unz, and F. Dressler,
evaluation of packet capturing systems for high-speed
networks,” in CoNEXT’05: Proceedings of the 2005
ACM conference on Emerging network experiment and
technology. New York, NY, USA: ACM Press, Oct.
2005, pp. 284–285.
[3] L. Deri, “ncap: Wire-speed packet capture and
transmission,” in In Proceedings of the IEEE/IFIP
Workshop on End-to-End Monitoring Techniques and
Services (E2EMON), May 2005.
[4] F. Schneider, J. Wallerich, and A. Feldmann, “Packet
capture in 10-gigabit ethernet environments using
“Vermont - A Versatile Monitoring Toolkit for IPFIX
and PSAMP,” in Proceedings of Workshop on
Monitoring, Attack Detection and Mitigation
(MonAM) 2006, Tuebingen, Germany, Sep. 2006.
[19] Homepage of the zlib project, http://www.zlib.net/.
[20] C. Satten, “Lossless gigabit remote packet capture
with linux,” http://staﬀ.washington.edu/corey/gulp/,
University of Washington Network Systems, Mar.
2008.
217