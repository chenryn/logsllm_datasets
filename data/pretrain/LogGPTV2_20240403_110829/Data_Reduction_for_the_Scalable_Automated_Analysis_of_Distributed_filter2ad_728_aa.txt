title:Data Reduction for the Scalable Automated Analysis of Distributed
Darknet Traffic
author:Michael Bailey and
Evan Cooke and
Farnam Jahanian and
Niels Provos and
Karl Rosaen and
David Watson
Data Reduction for the Scalable Automated Analysis of
Distributed Darknet Trafﬁc
Michael Bailey
University of Michigan
PI:EMAIL
Evan Cooke
University of Michigan
PI:EMAIL
Farnam Jahanian
University of Michigan
Arbor Networks, Inc.
PI:EMAIL
Niels Provos
Google, Inc.
PI:EMAIL
Karl Rosaen
University of Michigan
PI:EMAIL
David Watson
University of Michigan
PI:EMAIL
Abstract
Threats to the privacy of users and to the availability of
Internet infrastructure are evolving at a tremendous rate.
To characterize these emerging threats, researchers must
effectively balance monitoring the large number of hosts
needed to quickly build conﬁdence in new attacks, while
still preserving the detail required to differentiate these at-
tacks. One class of techniques that attempts to achieve
this balance involves hybrid systems that combine the scal-
able monitoring of unused address blocks (or darknets)
with forensic honeypots (or honeyfarms). In this paper we
examine the properties of individual and distributed dark-
nets to determine the effectiveness of building scalable hy-
brid systems. We show that individual darknets are dom-
inated by a small number of sources repeating the same
actions. This enables source-based techniques to be effec-
tive at reducing the number of connections to be evaluated
by over 90%. We demonstrate that the dominance of lo-
cally targeted attack behavior and the limited life of ran-
dom scanning hosts result in few of these sources being
repeated across darknets. To achieve reductions beyond
source-based approaches, we look to source-distribution
based methods and expand them to include notions of lo-
cal and global behavior. We show that this approach is
effective at reducing the number of events by deploying
it in 30 production networks during early 2005. Each of
the identiﬁed events during this period represented a ma-
jor globally-scoped attack including the WINS vulnerabil-
ity scanning, Veritas Backup Agent vulnerability scanning,
and the MySQL Worm.
1
Introduction
Networks are increasingly subjected to threats that affect
the reliability of critical infrastructure. These include Dis-
tributed Denial of Service attacks, such as the SCO DDoS
attacks [2], and scanning worms, such as CodeRed [33] and
Blaster [3]. The impact of these threats is profound, caus-
ing disruptions of real world infrastructure [26] and cost-
ing individual institutions hundreds of thousands of dollars
to clean up [11]. To address the concerns raised by these
threats, researchers have proposed a variety of global early
warning systems whose goal is to detect and characterize
these threats.
Unfortunately, the properties of these threats make them
particularly difﬁcult to address. First and foremost, they
are globally scoped, respecting no geographic or topologi-
cal boundaries. For example, at its peak, the Nimda worm
created 5 billion infection attempts per day, which included
signiﬁcant numbers from Korea, China, Germany, Taiwan,
and the US [33]. In addition, they can be exceptionally vir-
ulent and can propagate to the entire population of suscep-
tible hosts in a matter of minutes. This was the case during
the Slammer worm, in which the majority of the vulner-
able population (75K+ susceptible hosts) was infected in
less than 30 minutes [21]. This virulence is extremely tax-
ing on network resources and creates side effects that pose
problems even for those that are outside of the vulnerable
population, such as the routing instability associated with
the Slammer worm [17]. To make matters worse, these
threats have the potential to be zero-day threats, exploiting
vulnerabilities for which no signature or patch is available.
For example victims of the Witty worm were compromised
via their ﬁrewall software the day after a vulnerability in
that software was publicized [31].
In order to address these properties, threat detection and
classiﬁcation systems are needed to provide detailed foren-
sic information on new threats in a timely manner. As many
threats propagate by scanning the IPv4 address space, re-
searchers have turned to monitoring many addresses at the
same time in order to quickly detect these threats [33, 32].
By monitoring large numbers of addresses, these systems
can notably increase the probability of quickly detecting a
new threat as it attempts to infect other hosts on the Inter-
net [22]. However, as threats become increasingly com-
plex, interacting with the infected hosts to ellict the impor-
tant threat features, such as exploit, rootkits, or behavior,
USENIX Association
Internet Measurement Conference 2005  
239
may require increasingly complex host emulation. This,
coupled with the possibility of zero-day threats that may
provide little or no warning for creating these emulated
behaviors, may leave wide addresses monitoring systems
unable to identify the important threat characteristics. In
contrast, honeypot systems provide detailed insight into
new threats by monitoring behavior in a controlled environ-
ment [5, 34]. By deploying honeypot systems with mon-
itoring software, one can automatically generate detailed
forensic proﬁles of malicious behavior [16]. Unfortunately,
this detailed analysis comes at the expense of scalability,
and hence time to detection.
An interesting potential approach to this problem is to
forward requests destined to darknets back to an automated
bank of honeypots [15, 38, 30]. While this architecture pro-
vides the promise of quickly generating detailed forensic
information, there are still serious problems that need to be
addressed.
In particular, there is still the very important
question of what requests to forward to the honeypots. For
example, a darknet consisting of an unused /8 address block
(roughly 16 million routable IP addresses) observes almost
4 Mbits/sec of trafﬁc, much of which is TCP connection
requests. While seemingly a small amount of trafﬁc, each
of these connection requests may require their own virtual
machine. This load can cause scalability issues for both
servicing the large number of requests and storing or eval-
uating the large quantity of data produced [38].
In this paper, we investigate the problem of ﬁltering
darknet trafﬁc in order to identify connections worthy of
further investigation. In particular, we analyze data from a
large, distributed system of darknet monitors. We charac-
terize the trafﬁc seen by these monitors to understand the
scalability bounds of a hybrid monitoring system that con-
sists of distributed darknet monitors and a centralized col-
lection of honeypots (or honeyfarm). In addition, we use
these characterizations to guide the design of an algorithm
that is effective at reducing large trafﬁc rates into a small
number of manageable events for the honeyfarm to process.
The main contributions of this work are:
• Measurement and analysis of a large, distributed
dark address monitor. The measurements and char-
acterizations presented in this paper are from a multi-
year deployment of over 60 darknets in 30 organi-
zations including academic institutions, corporations,
and Internet service providers. This deployment rep-
resents a tremendous amount of diverse address space
including over 17 million routeable addresses with
blocks in over 20% of all routed /8 networks.
• Identiﬁcation of several key threat characteristics
that bound the scalability of a hybrid system. The
scalability of a hybrid system depends on limiting the
number of connections that need to be sent to the hon-
eyfarm for analysis. By examining the behavior of
threats at a large number of darknets we note two im-
portant characteristics:
– A small fraction of the total source IPs observed
at a single darknet are responsible for the over-
whelming majority of the packets.
– Most behavior consists of sources, and to some
extent target services, that are not observable
across darknets.
From these characterizations we show that source-
based ﬁltering is an effective method of reduction
for individual darknets, but fails to provide addi-
tional beneﬁts when multiple darknets are combined
together.
• Creation and deployment evaluation of an effective
algorithm for scaling hybrid architectures. We cre-
ate an algorithm that is both very effective in reducing
the large amount of trafﬁc seen by darknets to a small
handful of events and is easily within the capabilities
of the most modest honeyfarms. A broad production
deployment of this algorithm over a three month pe-
riod in 2005 provided analysis of ﬁve major global
events, including the MySQL Worm and the scanning
associated with the WINS vulnerability, as well as the
Veritas Backup vulnerabilities.
The remainder of this paper is structured as follows: We
begin by reviewing the related work in section 2. In sec-
tion 3 we introduce our hybrid architecture and some of the
challenges in building any similar system. We then exam-
ine the behavior of threats at individual dark address blocks
in section 4. We observe these threats across darknets in
section 5, and based on the insights from these measure-
ments, we construct a ﬁltering algorithm that we describe
in section 6. In section 7 we show how this algorithm is
effective at reducing large trafﬁc rates to a small handful
of events through a broad production deployment. We then
ﬁnish with our conclusions in section 8.
2 Related Work
Historic approaches to the detection and characterization
of network-based security threats fall into two categories;
monitoring production networks with live hosts and moni-
toring unused address space (or darknets). In monitoring
used networks, systems may choose to watch trafﬁc di-
rectly [20] or watch abstractions of the data, such as ﬂow
records [13]. Security devices also provide an important
source of these abstractions, and alerts from host-based
anti-virus software [6], intrusion detection systems [10],
and ﬁrewalls have been used as an effective means of ad-
dressing certain security threats.
In contrast to monitor-
ing live hosts, darknet monitoring consists of sensors that
240
Internet Measurement Conference 2005
USENIX Association
monitor blocks of unused address space. Because there are
no legitimate hosts in a darknet, any observed trafﬁc des-
tined to such darknet must be the result of misconﬁgura-
tion, backscatter from spoofed source addresses, or scan-
ning from worms and other network probing. Methods for
watching individual addresses with sacriﬁcial hosts are of-
ten called honeypots [5, 34]. Techniques for monitoring
much wider address blocks have a variety of names in-
cluding network telescopes [22], blackholes [33], and dark-
nets [8].
It should be noted that a limitation of this ap-
proach is that it relies on observing the target selection be-
havior of threats. As a result, threats that do not scan for
new hosts, or threats that are speciﬁcally tailored to avoid
unused blocks are not observed. Nevertheless, these tech-
niques have been used with great success in observing de-
nial of service activity [23], worms and scanning [32], as
well as other malicious behavior.
In isolation, these techniques fail to completely address
the scalability and behavioral ﬁdelity requirements needed
to monitor these threats. The scope of existing host-based
techniques, such as host-based honeypots, anti-virus soft-
ware, and host-based intrusion detection, is too small to
capture global information such as the size of the infected
population, or provide warning early in the growth phases.
On the other hand, globally-scoped network sensors, such
as network telescopes, do not interact sufﬁciently with the
worm. As such, they lack enough information to charac-
terize the vulnerability exploited and its effect on local ma-
chines. To be effective at assuring the availability of Inter-
net resources, it is necessary to combine information from
disparate network resources, each with differing levels of
abstraction, into one uniﬁed view of a threat.
Acknowledging this need for both host and network
views, two new approaches of combining these resources
have evolved, aggregating ﬁne-grained sensor measure-
ments from a large numbers of sensors, and hybrid sys-
tems that use darknet monitors to concentrate connections
to a centralized honeyfarm. Projects that aggregate data
fall into two categories, those based on aggregating ﬁre-
wall logs or Intrusion Detection System (IDS) alerts across
multiple enterprises [37, 36, 41], and those based on con-
structing and aggregating data from large numbers of hon-
eypots [35, 25]. Hybrid systems [15, 38, 30] vary in how
they perform the physical connection funneling, where and
in what way they choose to ﬁlter the data, and in the di-
versity and amount of address space monitored. Of partic-
ular relevance is the recent work on the Potemkin Virtual
Honeyfarm [39] in which the authors discuss a hybrid ar-
chitecture with emphasis on a novel set of techniques for
creating scalable per connection virtual machines. Their
scalability gains are achieved by multiplexing across idle-
ness in the network and by exploiting redundancies in the
per-host state of the virtual machines. The gains reported
vary widely based on work load (from a few hundred to a
Figure 1: A Hybrid architecture with the distributed Inter-
net Motion Sensor (IMS) with the centralized Host Motion
Sensor (HMS).
million destination IPs per physical host) but under realis-
tic workloads a single physical host can support 50k-100k
destinations IPs. This work is complimentary to ours in that
we focus on limiting the number of connections seen by the
honeyfarm while the Potemkin authors focus primarily on
servicing these connections as efﬁciently as possible.
3 A Hybrid Architecture for Monitoring In-
ternet Security Threats
To provide both behavioral ﬁdelity and global, broad cov-
erage, we have proposed a hybrid architecture that is highly
scalable but still delivers very accurate detection. This
multi-resolution, hybrid architecture (shown in Figure 1)
consists of two components: a collection of distributed
darknet sensors (the Internet Motion Sensor or IMS), and
a collection of host sensors (the Host Motion Sensor or
HMS). In this architecture the IMS is used to monitor a
broad, diverse set of darknets. When new activity is de-
tected by the IMS, the connection is proxied back to the
HMS for further in-depth analysis. The connection is re-
layed to virtual machine images running the application
appropriate to the connection request. Thus, new and im-
portant threats are handed off and actually executed so the
resulting activity can be monitored for new worm behavior.
By watching darknets, the trafﬁc seen by the Internet
Motion Sensor is pre-ﬁltered to eliminate both the false
positives in identifying malicious trafﬁc and the scaling is-
sues of other monitoring approaches. To analyze this traf-
ﬁc, the IMS sensors have both active and passive compo-
nents. The active component responds to TCP SYN pack-
ets with a SYN-ACK packet to elicit the ﬁrst data payload
on all TCP streams. When a packet is received by a dark-
net sensor, the passive component computes the hash of the
payload. If the hash doesn’t match any previously observed
signatures, then the payload is stored and the signature is
added to the signature database. The Internet Motion Sen-
USENIX Association