ﬁcials inside and outside their home territories.
Finally, our controversial terms are news or politics-
related issues like those shown in Table 1. We chose these
terms because it would be concerning if Google Search per-
sonalized search results for them based on location. To avoid
possible external bias, we picked search terms that, to the
best of our knowledge, were not associated with speciﬁc
news-worthy events at the time of our experiments. Al-
though we cannot completely rule out the possibility that
exonegous events impacted the search results, we note that
such an event would impact each treatment equally, and thus
would likely not impact our ﬁndings.
2.2 Data Collection and Parsing
Our methodology for gathering data from Google Search
is based on the techniques presented in our prior work et
al. [10,11], with one key diﬀerence. As in prior work, we use
PhantomJS [20] to gather data, since it is a full implemen-
tation of a WebKit browser. We wrote a PhantomJS script
that takes a search term and a latitude/longitude pair as
input, loads the mobile version of Google Search, executes
the query, and saves the ﬁrst page of search results.
Unlike prior work [11], we targeted the mobile version of
Google Search because it uses the JavaScript Geolocation
API [12] to query the user’s precise location. By overriding
the Geolocation API in our PhantomJS script, we can feed
the coordinates speciﬁed on the command line to Google
Search, thus giving us the ability to run queries that appear
to Google as if they are coming from any location of our
choosing. We distributed our query load over 44 machines
in a single /24 subnet to avoid being rate-limited by Google.
Finally, all of our experimental treatments were repeated for
5 consecutive days to check for consistency over time.
Validation.
To make sure that Google Search person-
alizes search results based on the provided GPS coordinates
rather than IP address, we conducted a validation exper-
iment. We issued identical controversial queries with the
same exact GPS coordinate from 50 diﬀerent Planet Lab
machines across the US, and observe that 94% of the search
results received by the machines are identical. This con-
ﬁrms that Google Search personalizes search results largely
based on the provided GPS coordinates rather than the IP
address. Furthermore, Google Search reports the user’s pre-
cise location at the bottom of search results, which enabled
us to manually verify that Google was personalizing search
results correctly based on our spoofed GPS coordinates.
Browser State.
To control for personalization eﬀects
due to the state of the browser, all of our treatments were
conﬁgured and behaved identically. The script presented
the User-Agent for Safari 8 on iOS, and all other browser
attributes were the same across treatments, so each treat-
ment should present an identical browser ﬁngerprint. Fur-
thermore, we cleared all cookies after each query, which mit-
igates personalization eﬀects due to search history, and pre-
vents Google from “remembering” a treatments prior loca-
tion. Lastly, we note that prior work has shown that Google
Search does not personalize search results based on the user’s
choice of browser or OS [11].
Controlling for Noise.
Unfortunately, not all dif-
ferences in search results are due to personalization; some
may due to noise. As in our prior work [10, 11], we take the
following precautions to minimize noise:
1. All queries for term t are run in lock-step, to avoid
changes in search results due to time.
2. We statically mapped the DNS entry for the Google
Search server, ensuring that all our queries were sent
to the same datacenter.
3. Google Search personalizes search results based on the
user’s prior searches during the last 10 minutes [11].
To avoid this confound, we wait 11 minutes between
subsequent queries.
However, even with these precautions, there may still be
noise in search results (e.g., due to A/B testing). Thus,
for each search term and location, we send two identical
queries at the same time. By comparing each result with
its corresponding control, we can measure the extent of the
underlying noise. When comparing search results from two
locations, any diﬀerences we see above the noise threshold
can then be attributed to location-based personalization.
Parsing.
As shown in Figure 1, Google Search on mo-
bile renders search results as “cards”. Some cards present a
single result (e.g., “Somerville Schools”), while others present
a meta-result (e.g., locations from Google Maps or a list of
“In the News” articles).
In this study, we parse pages of
search results by extracting the ﬁrst link from each card,
except for Maps and News cards where we extract all links.
Thus, we observe 12–22 search results per page.
2.3 Measuring Personalization
As in our prior work [11], we use two metrics to compare
pages of search results. First, we use Jaccard Index to exam-
ine the overlap: a Jaccard Index of 0 represents no overlap
between the pages, while 1 indicates they contain the same
search results (although not necessarily in the same order).
Second, we use edit distance to measure reordering of search
 0.6 0.7 0.8 0.9 1National (USA)State (Ohio)County (Cuyahoga)Avg. Jaccard IndexGranularity 0 1 2 3 4National (USA)State (Ohio)County (Cuyahoga)Avg. Edit DistanceGranularityPoliticiansControversialLocalFigure 3: Noise levels for local queries across three granularities.
Figure 4: Amount of noise caused by diﬀerent types of search
results for local queries.
results. Edit distance calculates the number of additions,
deletions, and swaps necessary to make two lists identical.
3. ANALYSIS AND FINDINGS
Using the methodology described in Section 2, we col-
lected 30 days of data from Google Search. We executed
the 120 local and controversial queries once per day for ﬁve
straight days in the county, state, and national locations (so,
15 days total). We then repeated this process with the 120
politicians. Using this dataset, we analyze the impact of
location-based personalization on Google Search results.
3.1 Noise
To start, we examine whether there is noise in our search
results. To calculate noise, we compare the search results
received by treatments and their controls, i.e., two browsers
that are running the same queries at the same time from the
same locations.
Unlike prior work [11], we ﬁnd that Google Search results
are noisy. Figure 2 shows the average Jaccard Index and
edit distance for all treatment/control pairs broken down
by granularity and query types (values are averaged over
all queries of the given type over 5 days). We make three
observations. First, we see that local queries are much noiser
than controversial and politician queries, in terms of result
composition (shown by Jaccard) and reordering (shown by
edit distance). Second, not only do local queries have more
diﬀerences on average, but we also see that they have more
variance (indicated by the standard deviation error bars).
Third, we observe that noise is independent of location, i.e.,
the level of noise is uniform across all three granularities.
Search Terms.
Given the high standard deviations for
local queries, we pose the question: do certain search terms
exhibit more noise than others? To answer this, we calculate
the Jaccard Index and edit distance for each search term
separately. Figure 3 shows the local queries along the x-
axis, with the average edit distance for each query along the
y-axis. The three lines correspond to search results gathered
at diﬀerent granularities; for clarity, we sort the x-axis from
smallest to largest based on the national locations.
Figure 3 reveals a divide between the queries: brand
names like “Starbucks” tend to be less noisy than generic
terms like “school”. We observe similar trends for Jaccard
Index. We examine this observation further next, when we
look at the impact of diﬀerent types of search results.
Search Result Types.
To isolate the source of noise,
we analyze the types of search results returned by Google
Search. As described in Section 2.2, Google Search returns
“typical” results, as well as Maps and News results. We
suspect that Maps and News results may be more heavily
impacted by location-based personalization, so we calculate
the amount of noise that can be attributed to search results
of these types separately.
Intuitively, we simply calculate
Jaccard and edit distance between pages after ﬁltering out
all search results that are not of type t.
Figure 4 shows the amount of noise contributed by Maps
and News results for each query, along with the overall noise.
Figure 4 focuses on the edit distance for local queries at
county granularity, but we see similar trends at other gran-
ularities, and for Jaccard values. We observe that Maps
results are responsible for around 25% of noise (calculated
as the total number of search result changes due to Maps, di-
Figure 5: Average personalization across diﬀerent query types and granularities. Black bars shows average noise levels from Figure 2.
 0 1 2 3 4 5ChipotleStarbucksDairy QueenMcdonaldsSubwayBurger KingPost OfficePolling PlaceKFCWendy’sChick-fil-aTrainUniversitySushiFootballBankBurgerRailCoffeeRestaurantParkFast FoodPolice StationBusSchoolFire StationAirportHospitalCollegeStationHigh SchoolElementary SchoolMiddle SchoolAvg. Edit DistanceCounty (Cuyahoga)State (Ohio)National* (USA) 0 1 2 3 4 5SubwayChipotleMcDonaldsFootballWendy’sPolling PlaceRailStarbucksPost OfficeFast FoodUniversityRestaurantDairy QueenChick-fil-aBurger KingCollegeSushiBankKFCPolice StationCoffeeBurgerAirportTrainBusFire StationElementary SchoolSchoolHospitalHigh SchoolStationParkMiddle SchoolAvg. Edit DistanceAll*MapsNews 0.5 0.6 0.7 0.8 0.9 1National (USA)State (Ohio)County (Cuyahoga)Avg. Jaccard IndexGranularity 0 2 4 6 8 10 12National (USA)State (Ohio)County (Cuyahoga)Avg. Edit DistanceGranularityPoliticiansControversialLocalvided by the overall number of changes), while News results
cause almost zero noise. After some manual investigation
we found that most diﬀerences due to Maps arise from one
page having Maps results and the other having none. How-
ever, we also found cases where both queries yield Maps that
highlight a diﬀerent set of locations. Surprisingly, searches
for speciﬁc brands typically do not yield Maps results, hence
the low noise levels for those search terms.
Although we do not show the ﬁndings here due to space
constraints, we observe the reverse eﬀect for controversial
queries: 6-17% of noise in such queries is due to News, while
close to 0 is due to Maps. However, as Figure 2 shows, the
level of noise in controversial queries is low overall.
3.2 Personalization
Now that we have quantiﬁed the noise in our dataset, we
focus on answering the following two questions. First, do
certain types of queries trigger more personalization than
others? Second, how does personalization change as the dis-
tance between two locations grows?
Figure 5 shows the average Jaccard Index and edit dis-
tance values for each query category at each granularity.
Values are averaged across all queries of the given types
across 5 days. Recall that in the previous section, we were
comparing treatments to their controls in order to measure
noise; in this section, we are comparing all pairs of treat-
ments to see if search results vary by location. For the sake
of comparison, the average noise levels seen in Figure 2 are
shown as horizontal black lines in Figure 5.
The ﬁrst takeaway from Figure 5 is that local queries are
much more personalized than controversial and politicians
queries. The Jaccard index shows that 18-34% of the search
results vary based on location for local queries, while the edit
distance shows that 6-10 URLs are presented in a diﬀerent
order (after subtracting the eﬀect of noise). Controversial
and politician queries also exhibit small diﬀerences in Fig-
ure 5, but the Jaccard and edit distance values are very
close to the noise-levels, making it diﬃcult to claim that
these changes are due to personalization.
The second takeaway from Figure 5 is that personalization
increases with distance. The change is especially high be-
tween the county- and state-levels, with 2 additional search
results changed and 4 reordered. As expected, this indicates
that diﬀerences due to location-based personalization grow
with geographic distance.
Search Terms.
Our next step is to examine how per-
sonalization varies across search terms. As before, we focus
on local queries since they are most impacted by person-
alization. Figure 6 shows the edit distances for each local
search term at each granularity (with the x-axis sorted by
the national-level values). The signiﬁcant increase in per-
sonalization between county- and state-level search results is
again apparent in this ﬁgure.
Overall, we see that location-based personalization varies