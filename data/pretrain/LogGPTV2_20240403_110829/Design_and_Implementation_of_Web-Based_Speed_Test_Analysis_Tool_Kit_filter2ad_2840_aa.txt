title:Design and Implementation of Web-Based Speed Test Analysis Tool Kit
author:Rui Yang and
Ricky K. P. Mok and
Shuohan Wu and
Xiapu Luo and
Hongyu Zou and
Weichao Li
Design and Implementation of Web-Based
Speed Test Analysis Tool Kit
Rui Yang1(B), Ricky K. P. Mok2, Shuohan Wu3, Xiapu Luo3, Hongyu Zou4,
and Weichao Li5
1 ETH Z¨urich, Z¨urich, Switzerland
2 CAIDA/UC San Diego, San Diego, USA
3 The Hong Kong Polytechnic University, Hung Hom, Hong Kong
4 UC San Diego, San Diego, USA
5 Peng Cheng Laboratory, Shenzhen, China
Abstract. Web-based speed tests are popular among end-users for mea-
suring their network performance. Thousands of measurement servers
have been deployed in diverse geographical and network locations to
serve users worldwide. However, most speed tests have opaque method-
ologies, which makes it diﬃcult for researchers to interpret their highly
aggregated test results, let alone leverage them for various studies.
In this paper, we propose WebTestKit, a uniﬁed and conﬁgurable
framework for facilitating automatic test execution and cross-layer anal-
ysis of test results for ﬁve major web-based speed test platforms. Captur-
ing only packet headers of traﬃc traces, WebTestKit performs in-depth
analysis by carefully extracting HTTP and timing information from
test runs. Our testbed experiments showed WebTestKit is lightweight
and accurate in interpreting encrypted measurement traﬃc. We applied
WebTestKit to compare the use of HTTP requests across speed tests and
investigate the root causes for impeding the accuracy of latency measure-
ments, which play a vital role in test server selection and throughput
estimation.
1 Introduction
Internet surfers often use web-based speed tests to measure their access band-
width, diagnose slow residential broadband connections [27], and validate the
ISP-advertised speed [12,38]. A few such testing platforms make the collected
data and source code publicly available (e.g. M-Lab NDT [20]), which researchers
have leveraged for various studies including evaluating video streaming and
cloud platform performance [8,21], measuring Internet latency [13], and infer-
ring network congestion [1,35,42]. On the contrary, commercial speed tests (e.g.,
fast.com), serving millions of users across the world, have much more diverse
server deployment than open-source ones. As of October 2021, there have been
over 38 billion tests conducted by Ookla [23], a popular speed test which deploys
tens of thousands of test servers worldwide. Meanwhile, video content providers
c(cid:2) The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
O. Hohlfeld et al. (Eds.): PAM 2022, LNCS 13210, pp. 83–96, 2022.
https://doi.org/10.1007/978-3-030-98785-5_4
84
R. Yang et al.
including Netﬂix and Hulu advise users to run speed tests using their custom
platforms [16,22] which host their servers in video delivery networks.
Despite their popularity and resources, unfortunately, it is still very hard for
the research community to eﬀectively utilize these proprietary tests. First, most
speed tests oﬀer only a web interface that selects a default test server for users,
which makes automatic and conﬁgurable test execution burdensome. Second,
despite the overhead introduced by diﬀerent layers (e.g. the browser itself) and
environmental dynamics (e.g. network congestion), these platforms report only
few simple metrics (e.g. download/upload throughput). Without data providing
the necessary context, diagnosing the root causes of performance degradation is
very diﬃcult. Finally, even though the measurement traﬃc is dummy data, speed
tests run over HTTPS, preventing observation of HTTP transactions directly
from packet captures. This opacity presents challenges for understanding speed
tests since correlating the HTTP transactions with timings of the correspond-
ing packets is essential for comprehensively analyzing speed test results (e.g.
mapping TCP congestion behavior to underlying test methodologies).
To bridge this gap, we propose WebTestKit, a lightweight framework which
enables automatic execution and in-depth analysis of web-based speed tests.
WebTestKit has three design goals: (i) provides a uniﬁed and conﬁgurable inter-
face for executing reproducible tests across multiple platforms, (ii) captures test
data extensively from multiple layers in a lightweight manner, and (iii) conducts
cross-layer analysis to provide a comprehensive view of the speed test results.
We implemented a prototype of WebTestKit to achieve our goals. The key
idea of WebTestKit is to keep our framework lightweight and still allow an
in-depth analysis of test results with high accuracy. To achieve this, we (i)
minimized overhead by capturing only packet headers of measurement traf-
ﬁc and (ii) developed a packet matching algorithm to locate HTTP messages
in encrypted traﬃc without decrypting them. We used a headless Chromium
browser to automate the execution of ﬁve major web-based speed tests: Ookla
Speedtest [24], Comcast Xﬁnity test [7], Netﬂix Fast.com [9], CloudFlare speed
test [6], and speedof.me [2]. We crawled the full server lists by exploiting RESTful
APIs used on Ookla and Xﬁnity test websites. When conducting measurements,
WebTestKit allows users to select speciﬁc servers from these server lists.
Our testbed experiments showed that WebTestKit has a signiﬁcantly lower
impact on measurements compared to capturing full-size packets. WebTestKit
is also accurate in inferring the locations of HTTP messages in packet traces,
allowing us to extract many timing information from the encrypted traﬃc.
We demonstrated the capability and usability of WebTestKit with three use
cases. We studied the behavior of diﬀerent speed tests (Sect. 6.1) and found
that the number and size of HTTP requests/responses were largely diﬀerent
between tests. Some tests sent thousands of small requests, increasing the load
of the client. We discovered that preﬂighted requests were often unintentionally
triggered, generating additional network overhead. We also used WebTestKit
in the wild to run Xﬁnity speed test from Google Cloud (GCP) for two weeks
(Sect. 6.2). We found high round-trip time (RTT) variances reported by the test
Design and Implementation of Web-Based Speed Test Analysis Tool Kit
85
due to the plausible glitches in the web interface. We compared the accuracy of
two sets of JavaScript APIs that speed tests commonly used for RTT measure-
ments. We found that the measurements using XMLHttpRequest API suﬀered
from at least 2.3 ms error, compared to the RTTs captured by tcpdump.
2 Related Work
In this section, we ﬁrst survey work on speed test tools and then, explore the
studies on evaluating their performance and accuracy.
Speed Test Tools. Most commercial web-based speed tests [2,7,9,20,24] are
ﬂooding-based which use one or multiple parallel TCP connections to saturate
the access link. However, these tools could incur high costs including exces-
sive data transfer. Probe-optimized tools like Spruce [33] and IGI/PTR [15]
employ the Probe Gap Model which sends back-to-back packet pairs to estimate
the available bandwidth with the packet pair dispersion. There are also some
tools using the Path Rate Model (e.g., Pathload [17] and Pathchirp [28]) which
sends packet trains at diﬀerent sending rates to self-induce congestion at the
bottleneck. Unfortunately, these tools are highly sensitive to diﬀerent network
dynamics (e.g. packet loss), often leading to non-negligible inaccuracies espe-
cially in high-speed networks. Recently, FastBTS [43] used a statistical sampling
framework to probe elastic bandwidth for high-speed wide-area networks, with
signiﬁcantly reduced data usage and test duration. CLASP [21] leveraged speed
tests to perform throughput measurements from the cloud. Murakami [19] sup-
ports running automated speed test measurements and collecting test results. It
also provides a conﬁgurable interface for recurring jobs. Instead of building new
speed test tools, WebTestKit focuses on analyzing the existing web-based speed
tests that are most popular among end-users.
Speed Test Evaluation. Goga and Teixeira [11] compared the accuracy of ﬂooding-
based methods [28,33] and probe-optimized tools [15,17,33] for measuring res-
idential broadband performance from home gateways. Sundaresan et al. [34]
conducted experiments to determine the number of parallel TCP ﬂows required
to accurately perform throughput measurements using the BISMark platform.
Li et al. [18] evaluated three commonly used browser-based delay measurement
methods, and found that the socket-based approach incurred smaller overhead
than the HTTP-based one. Feamster and Livingood [10] and Bauer et al. [4]
identiﬁed potential issues in various speed test platforms for measuring Giga-
bit broadband networks (e.g. the selected oﬀ-net measurement servers). Yang
et al. [43] evaluated the accuracy of eight representative speed tests. However,
diﬀerent from WebTestKit, they did not perform any further analysis of test
data to infer causes of inaccuracies.
3 Web-Based Speed Test Platforms
Web-based speed test platforms conduct bulk data transfers over HTTP(S)/
TCP [25] to measure the bandwidth of the bottleneck link by saturating it
86
R. Yang et al.
with TCP ﬂows. The bottleneck link is commonly the “last mile”—the access
link between the client and the Internet. In this scenario, the ideal location of
the server is as close as possible to the client to minimize the latency. TCP
throughput has a well-understood inverse relationship with latency [26]—the
longer the latency across a path, the lower the throughput, all other factors being
equal. As broadband access speeds increase, low latencies from test servers to
clients ensure that measurement ﬂows can saturate the bottleneck link [3].
Table 1. Comparison of HTTP-based speed test platforms.
Platform
Ookla [24]
Xﬁnity [7]
# of Servers Network(s)
>12,000
†
78
Comcast
Various ISPs
Server Selection
# of TCP ﬂows
‡
Latency, IP Geolocation 6
IP Geolocation
18
Fast.com [9]
Unknown
Netﬂix
Latency, IP Geolocation 11
SpeedOf.Me [2] 88
Verizon Edgecast Anycast
1
1
Cloudﬂare [6] Unknown
‡
Cloudﬂare CDN Anycast
: Speed test platforms may adapt the number of connections used depending on the
type and speed of connections. We evaluated the tests in a wired Gigabit network.
†
: These Xﬁnity servers were distributed in 29 locations in the United States.
Table 1 summarizes the properties of HTTP-based speed test platforms
that WebTestKit supports. The scale of deployment and network coverage
largely varies across platforms. Ookla speedtest has deployed the largest number
(>12 000) of measurement servers around the world. These servers are hosted
by ISPs, web-hosting companies, and cloud services. Other speed test platforms
host servers only within their own networks or CDNs. Speed test platforms
employ diﬀerent methods to select test servers for users. Ookla ﬁrst selects 10
servers nearest to the user based on IP geolocation and uses the one with the
lowest round-trip time from the user. CDN-based speed tests share the same
catchment functions as their host CDNs to divert users. All the platforms,
except speedof.me, use HTTP/1.1 over TLS to perform throughput measure-
ments. Three of the platforms establish multiple concurrent TCP connections to
saturate the bottleneck. Although speedof.me adopts HTTP/2, it sequentially
downloads/uploads web objects without invoking the multiplexing mechanism.
Some of these speed tests do oﬀer a command-line interface (CLI) [29–31].
Though convenient for automating tests, these CLIs cannot capture multiple-
layer information (e.g. browser-layer), which is essential for correctly interpreting
measurements and performing further analysis.
4 Design of WebTestKit
In this section, we discuss the design objectives of WebTestKit (Sect. 4.1), then
its components (Sect. 4.2) and implementation (Sect. 4.3).
Design and Implementation of Web-Based Speed Test Analysis Tool Kit
87
4.1 Design Objectives
The main challenge of building a tool to leverage the many web-based speed
tests for various research studies, lies in designing a framework that extensively
collects and provides in-depth analysis of test data, while keeping it lightweight