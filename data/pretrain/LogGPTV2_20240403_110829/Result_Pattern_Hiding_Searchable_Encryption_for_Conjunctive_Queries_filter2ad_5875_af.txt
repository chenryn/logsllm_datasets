and third rounds of interactions, the server computes vc (using the
hashs from the BF) for each encrypted index in vector t retrieved
using stag and TSet and sends it to the client. This interaction
costs |DB(w1)|O(m′) communication overhead. The client then
uses the key generation algorithm of the corresponding HVE to
form tokenc and lets server to use these tokenc to check if the
result of this query is “True” or not (using the query algorithm
of the underlying HVE scheme). This extra interaction round has
|DB(w1)|(O(m′) + 2λ) bandwidth. It gathers all encrypted indices
ec that passes the HVE.Query into a set E. This set will be sent to
client with bandwidth |E|O(λ) as the final result and the client is
further responsible to decrypt the recovered indices using her own
secret key to the symmetric encryption Sym. Note that one round
of interaction between client and server has been added in our HXT
compared to OXT, where the server only checks a set membership
against XSet rather than employing HVE. All these are summarized
in Fig. 2.
7 EVALUATIONS
7.1 Prototype Implementation
We implement a prototype system for evaluating our HXT proto-
col. To build this prototype, we firstly implement an OXT proto-
type, because there is no open source implementation of OXT. Our
implementation use the cryptographic primitives outlined in sub-
section 6.1. Bloom filter is a essential part for both OXT and HXT
prototypes, we deploy the Bloom filter from Alexandr Nikitin as it
is the fastest Bloom filter implementation for Java [31]. In our OXT
prototype, we set the false positive rate to 10−6 and it enables the
OXT prototype to keep the Bloom filter of XSet on the RAM of our
server.
The OXT prototype consists of two main parts: one for encrypted
database (EDB) generation and the other for database query. Based
on the OXT prototype, we implement the proposed SHVE scheme
to enable the prototype to perform HXT protocol. By replacing
the Bloom filter of XSet to a SHVE ciphertext in EDB generation,
Figure 2: All interactions between a server and a client dur-
ing a search in HXT (all arrows) and OXT (solid arrows only).
Since the message flows corresponding to third, forth, and
fifth lines are sent in parallel over c ∈ [|t|], the HXT protocol
only has 6 message flows (or equivalently 3 rounds). This is
in contrast to OXT, which has 4 message flows (2 rounds).
and the Bloom filter query to SHVE key generation and query in
database query, the OXT prototype is adapted to an HXT one. All
above programs are implemented by a combination of Java and
Scala code and it has roughly 2000 lines of code.
The implemented prototypes are designed to fulfil the scalability
and the query efficiency characteristics of original protocols. To
reach these goals, our prototypes are implemented on distributed
platform (i.e. Hadoop [1]). Hadoop is a distributed platform in
master-slave structure: It has a master node that manages the re-
source and monitor the application execution while a group of
slave nodes which dedicate their computational resources (e.g. CPU,
RAM, disk) to execute the tasks from master node. Hadoop [1] of-
fers a distributed file system HDFS [34], in addition to a distributed
database HBase [2]. HDFS allows our prototypes to store TSet and
SHVE in multiple hard drives in different slave nodes, and reach
them concurrently. As a result, it avoids the heavy I/O overhead
on single hard drive; HBase provides efficient in-memory index
mechanism over distributed dataset, which can highly reduce the
TSet and SHVE access time.
For scalability, we take steps to further improve the read per-
formance of TSet and HVE on HBase. HBase is a column-based
database [2], that is, the data in the same column are stored in the
same file. Therefore, we follow tall narrow model [21] to design TSet
table to avoid a very long column value because such long value
incurs extra overhead (i.e. compaction) while loading them into
memory. In the tall narrow model, each tuple list T[w] is split into
fixed-size blocks with a stagw and a block counter. Because HBase
stores data in key lexicographically order, it stores above blocks
with the same stagw into contiguous disk area. Hence, retrieving
TSet only has one random access following by the sequential ac-
cesses. HVE is stored as key/value pairs on HBase. Because the HVE
is a vector the ciphertext, we use the index of vector as the key, and
the corresponding ciphertext as value. Due to the variety of HVE
key, it is difficult to avoid the random access of HVE ciphertext.
Therefore, we use the randomised index of HVE ciphertext as the
Table 8: Statistics of the datasets used in the evaluation
Size # of documents Distinct keywords Distinct (id, w) pairs
2.93GB
8.92GB
60.2GB
7.8 ∗ 105
2.7 ∗ 106
1.6 ∗ 107
4.0 ∗ 106
1.0 ∗ 107
4.3 ∗ 107
6.2 ∗ 107
1.6 ∗ 108
1.4 ∗ 109
key of HVE, because random keys help to distribute the data into
different nodes, which enables the random access in parallel [21].
To accelerate the query phase, we make use of the distributed
in-memory computing framework Spark [38]. Spark follows the
same data processing flow as MapReduce [17], which distributes
the computing tasks and execute them on different slave nodes in
parallel. Spark inherits the scalability and fault tolerance of MapRe-
duce [38], but it can execute tasks in-memory without keeping any
intermediate data on disk, it means our prototypes don’t have any
I/O operation during database query except the TSet and SHVE
query.
We deploy our prototypes on a shared Hadoop cluster with 13
slave nodes and one master node. Each node has 2x Intel Xeon
CPU E5-2660 2.2GHz (each CPU has 8 cores with dual-thread) and
128GB RAM, in addition, we have another node with the same
specification above which is served as edge node and client of our
prototypes. All nodes are connected by InfiniBand [23] network
technique. The cluster installs CDH 5.2.6 [14], one of the most
complete and popular distribution of Hadoop and its peripheral
ecosystem (contains Hadoop Yarn 2.5.0, HBase 0.96.8 and Spark
2.0.2). Based on the setup configuration and scheduling policy, we
can use at most 416 virtual cores (32 virtual cores in each slave
node) and 1248GB RAM (96GB RAM in each slave node), in addition,
each virtual core should have at least 2GB. In real world scenario,
1 virtual core and 2GB RAM are needed for running the monitor
program of a distributed application on Hadoop. As a result, our
prototypes can start 415 tasks with 1 virtual core and 3GB RAM
concurrently at most. However, our following evaluations show
that it is not necessary to use all resources to query the database:
100 concurrent tasks with 1 virtual core and 2GB RAM are sufficient
to provide a satisfactory result.
7.2 Datasets
We test our implementation on three datasets from Wikimedia
Downloads [20]: the original sizes of our datasets are 2.93GB5,
8.92GB6 and 60.2GB7, respectively. A brief summary of the statisti-
cal features of the datasets is given in Table 8.
The corresponding EDB and Bloom filter size for above three
datasets are 9.3GB and 215MB, 33GB and 575MB, 256GB and 4.76GB.
In addition, the HVE size is 28GB, 76GB and 647GB. Fig 3 further
gives the frequency of keywords according to the number of docu-
ments to depict the keyword occurrence distribution of the gener-
ated EDBs.
5enwiki-20161220-pages-articles22.xml
6enwiki-20161220-pages-articles27.xml
7enwiki-20171201-pages-articles.xml
Figure 3: The keyword occurrence distribution of three
datasets.
Figure 4: HXT Server query time when # of parallel tasks in-
creases.
7.3 Evaluation Results
Our evaluation aims to verify the following: (i) our implementation
in distributed platform can ensure the efficiency of queries; (ii) the
additional query latency introduced in HXT is small; (iii) HXT keeps
the scalability property of OXT.
7.3.1 The impact of parallelism. First, we study how distributed
computing influences the query efficiency of HXT prototype. We
choose a keyword with about 330K matched documents respectively
in three datasets, and we use the selected keyword as the sterm
to perform a two-terms conjunctive query in our HXT prototype.
We vary the number of parallel tasks from 1 to 200 before the
server start running the query to test the impact of parallelism. As
shown in Fig 4, we observe that parallelism successfully improved
the efficiency of query by a factor of 100 times on server side. In
addition, we conclude the impact in three cases: (i) when increasing
the number of parallel tasks from 1 to 10, parallelism can highly
improved the server side performance; (ii) when the number of
parallel tasks is between 10 to 100, parallelism only can slightly
improve the query efficient on server; (iii) after the number of
parallel tasks is larger than 100, parallelism doesn’t affect the query
efficiency.
The reason is that the computational cost is the dominant cost
when the server only has a small fraction of resources is allowed
to engage the computation. By increasing the parallelism factor in
>=100>=101>=102>=103>=104>=105>=106Occurrence (keyword)1     10    100   1000  10000 1000001e+06 1e+07 1e+08 # of Documents60.2GB Dataset8.92GB Dataset2.93GB Dataset050100150200# of Task(s)100200300400500600Time (sec)60.2GB Dataset8.92GB Dataset2.93GB DatasetFigure 5: Server performance comparison between HXT and
OXT in 2.93GB dataset.
Figure 7: Overall query delay comparison between HXT and
OXT in 2.93GB dataset.
Figure 6: Client performance comparison between HXT and
OXT in 2.93GB dataset.
Figure 8: Overall query delay comparison between HXT and
OXT under multi-keyword setting in 2.93GB dataset.
additional cost comparing with OXT. However, it slows down with
the increase of selectivity of v. This is because HXT requires to
access HBase to get HVE ciphertext, increasing the selectivity also
means the server needs to do more HBase access, which increase
the load of I/O.
above case, computation cost of each tasks can be highly reduced,
which yields a significant performance improvement. However,
with the increase of the number of parallel tasks, it incurs more
communication cost for task scheduling and monitoring between
the master and slave nodes of our cluster, and HBase also has a
I/O limits based on the underlying infrastructure. Therefore, the
computation cost is overlapped by communication cost and I/O
latency after we have more than 100 parallel tasks.
Another observation is the query latency highly depends on the
selectivity of sterm, while it is independent of the size of dataset.
We examine it deeply in the scalability test at the end of this section.
7.3.2 Performance comparison. We use the parallel factor 100 to
further investigate the additional overhead in HXT comparing with
OXT protocol. Due to the OXT and HXT protocol having the same
behaviour when performing single-keyword search, our evaluation
only report the query performance of conjunctive query.
We choose a variable term, named v, on the 2.93GB dataset. The
selectivity of v is from 2 to 337449 documents. We further choose a
fixed term a and perform two types of conjunctive queries on the
2.93GB dataset. Fig 5 shows the time spent by HXT and OXT during
the query on server side. The first conjunctive query uses the v as
sterm and the a as xterm. Hence, the OXT server time is linear to the
selectivity of v, because it needs to do an additional exponentiation
for each tuple from the TSet to check against the XSet. When the
selectivity of v is small, we observe that HXT prototype has 2%−8%
Another conjunctive query use the a as sterm and v as xterm,
the server then runs in a steady constant time regardless of the
selectivity of v. In above case, HXT has 2% − 8% overhead against
the OXT over time. This also illustrate the importance of choosing
the least frequent term as sterm.
The query time on the client side of HXT and OXT is demon-
strated in Fig 6. Comparing with server, client doesn’t have any
I/O operation, so it purely reflects the computation costs, and it fits
well with the analysis in subsection 6.2 as HXT is 2 times slower
than OXT.
However, as we design our prototypes to perform query in par-
allel, the computation cost of HVE key generation on client side
can be overlapped by the xtag generation (it always slower than
xtoken generation because it doesn’t use preprocessed elements),
as well as the HBase I/O for loading HVE ciphertext on server side.
As shown in Fig. 7, the overall performance of HXT is not affected
by the computation on client side.
The last evaluation in this part aims to compare the performances
of HXT and OXT for querying multiple keywords. In this evaluation,
the sterm is identical to the fixed term a in previous two-keyword
1     10    100   1000  10000 1000001e+06 Selectivity of Variable Term (v)0.00010.001 0.01  0.1   1     10    100   Time (sec)Selectivity of a:1284HXT v AND aOXT v AND aHXT a AND vOXT a AND v1     10    100   1000  10000 1000001e+06 Selectivity of Variable Term (v)0.00010.001 0.01  0.1   1     10    100   Time (sec)Selectivity of a:1284HXT v AND aOXT v AND aHXT a AND vOXT a AND v1     10    100   1000  10000 1000001e+06 Selectivity of Variable Term (v)0.00010.001 0.01  0.1   1     10    100   Time (sec)Selectivity of a:1284HXT v AND aOXT v AND aHXT a AND vOXT a AND v0123456# of variable term (n)0.00010.001 0.01  0.1   1     10    100   Time (sec)Selectivity of a:1284HXT a AND v1 AND ... AND vnOXT a AND v1 AND ... AND vnSome possible further research directions are: (i) to establish a
protocol achieving a better security (by even removing WRP from
the leakage profile) robust to the recent attacks [39], while sup-
porting Boolean queries, (ii) to apply HXT to other types of queries
including rich queries [18, 19], (iii) to employ HXT in dynamic SSE
scheme with forward/backward security [7].
ACKNOWLEDGMENTS
The authors would like to thank the anonymous reviewers for their
valuable comments and constructive suggestions.
The authors would also like to thank Mr. Piotr Szul from Data61
for his kindly help in the use of High Performance Computing
Resource. The work is supported in part by the Data61-Monash CRP,
ARC Discovery Project grant DP180102199, Oceania Cyber Security
Centre POC scheme, Qualcomm India Innovation Fellowship 2017
and DRDO (Grant: DFTM/02/3111 /M/01/JCBCAT/1288/D(R&D)
dated 07 July 2017). Debdeep would also like to thank his DST
Swarnajayanti fellowship for partial support during the duration
of the project.
[6] D. Boneh and B. Waters. 2007. Conjunctive, Subset, and Range Queries on
[5] D. Boneh, G. Di Crescenzo, R. Ostrovsky, and G. Persiano. 2004. Public Key
[4] B.H. Bloom. 1970. Space/Time Trade-offs in Hash Coding with Allowable Errors.
REFERENCES
[1] Apache. 2015. Hadoop. https://hadoop.apache.org[online]. (2015).
[2] Apache. 2015. HBase. https://hbase.apache.org[online]. (2015).
[3] S. Blake-Wilson, N. Bolyard, V.Gupta, C. Hawk, and B. Moeller. 2006. RFC4492:
Elliptic Curve Cryptography (ECC) Cipher Suites for Transport Layer Security
(TLS). RFC4492, Internet Engineering Task Force (2006).
Commun. ACM 13, 7 (1970), 422–426.
Encryption with Keyword Search. In EUROCRYPT 2004. 506–522.
Encrypted Data. In TCC’07. 535–554.
[7] R. Bost, B. Minaud, and O. Ohrimenko. 2017. Forward and Backward Private
Searchable Encryption from Constrained Cryptographic Primitives. In ACM
CCS’17. 1465–1482.
[8] A. Broder and M. Mitzenmacher. 2004. Network Applications of Bloom Filters: A
survey. Internet mathematics 1, 4 (2004), 485–509.
SCC 2011. 850–855.
Against Searchable Encryption. In ACM CCS’15. 668–679.
[11] D. Cash, J. Jaeger, S. Jarecki, C.S. Jutla, H. Krawczyk, M-C. Rosu, and M. Steiner.
2014. Dynamic Searchable Encryption in Very-Large Databases: Data Structures
and Implementation. In NDSS’14.
[12] D. Cash, S. Jarecki, C.S. Jutla, H. Krawczyk, M-C. Rosu, and M. Steiner. 2013.
Highly-Scalable Searchable Symmetric Encryption with Support for Boolean
Queries. In CRYPTO’13. 353–373.
[13] C-K. Chu, W.T. Zhu, J. Han, J.K. Liu, J. Xu, and J. Zhou. 2013. Security Concerns
in Popular Cloud Storage Services. IEEE Pervasive Computing 12, 4 (2013), 50–57.
[14] Cloudera. 2018. CDH Overview. https://www.cloudera.com/documentation/
enterprise/5-2-x/topics/cdh_intro.html[online]. (2018).
[9] A. De Caro and V. Iovino. 2011. JPBC: Java Pairing Based Cryptography. In IEEE
[10] D. Cash, P. Grubbs, J. Perry, and T. Ristenpart. 2015. Leakage-Abuse Attacks
[15] R. Cramer and V. Shoup. 1999. Signature Schemes Based on the Strong RSA
Assumption. In ACM CCS’99. 46–51.
[16] R. Curtmola, J.A. Garay, S. Kamara, and R. Ostrovsky. 2006. Searchable symmetric
encryption: improved definitions and efficient constructions. In ACM CCS’06.
79–88.
[17] J. Dean and S. Ghemawat. 2008. MapReduce: simplified data processing on large
clusters. Commun. ACM 51, 1 (2008), 107–113.
[18] I. Demertzis, S. Papadopoulos, O. Papapetrou, A. Deligiannakis, and M.N. Garo-
falakis. 2016. Practical Private Range Search Revisited. In ACM SIGMOD’16.
185–198.
[19] S. Faber, S. Jarecki, H. Krawczyk, Q. Nguyen, M-C. Rosu, and M. Steiner. 2015. Rich
Queries on Encrypted Data: Beyond Exact Matches. In ESORICS 2015. 123–145.