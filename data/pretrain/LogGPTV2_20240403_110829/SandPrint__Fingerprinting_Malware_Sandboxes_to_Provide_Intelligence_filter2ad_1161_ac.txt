tered together, although they use the same technology. This is mainly due to the
fact that sandbox operators have to set up their own VM image, regardless of
whether they use common frameworks like Cuckoo. Although some sandboxes
use the same virtualization technology, these sandboxes can still be distinguished
based on their installation features (such as OS installation date or product ID).
Next, we inspected the Internet uplinks used by the sandboxes. 64 sandboxes
use external IP addresses of a single country according to GeoIP. Among them,
the US comes ﬁrst with 22 sandboxes, Germany ranks second with six sandboxes,
China ranks third with ﬁve sandboxes, and Ireland ranks fourth with four sand-
boxes. Three sandboxes each are in Sweden, Russia, and Korea, and Romania,
Japan, and Britain host two sandboxes. We note that there are two sandboxes
that we cannot geolocate due to their high diversity of external IP addresses.
These sandboxes use Tor to diversify the IP address. We also note that 29 sand-
boxes use a single ﬁxed IP address, which makes them trivially detectable from
the server side. For instance, if a malware sample sends a command to a C&C
server, the IP address could be checked against a black list on that server, which
then tells the client to stop executing.
The MAC addresses show the highest diversity in all features we collected.
Only 12 sandboxes use a single ﬁxed MAC address, as conﬁrmed by multiple
SandPrint reports. The majority of MAC addresses are at least partly diversi-
ﬁed (e.g., the ﬁrst three octets, namely the vendor ID, are often ﬁxed but the rest
are diversiﬁed). We speculate this is due to the fact that the sandboxes actually
consist of multiple VMs running in parallel, sharing the same VM image, but all
having unique MAC addresses to avoid collisions on the Ethernet layer. Of those
sandboxes that did not hide the vendor preﬁx, we could reveal 6 VMware-based
(preﬁx: 00-50-56) and 21 VirtualBox-based (preﬁx: 08-00-27) sandboxes.
SandPrint: Fingerprinting Malware Sandboxes
175
Table 3. Sandbox classiﬁer features.
Feature
Observation
Transf.
Hardware Display resolution
Display width
RAM size
PS/2 mouse
#Cores
Disk size
System uptime
Last login
Last ﬁle access
History
Execution Image name
{0,1}
id
Uncommon
Small
id
Small/Uncommon id
Uncommon
Small
Small
small
Long ago
Long ago
Uncommon
Empty
id
id
id
id
Clipboard
System manufacturer Uncommon
id
{0,1}
len
len
5 Sandbox Classiﬁcation
We have shown that the ﬁngerprints can be used to discover that certain reports
belong to the same sandboxes. We now explore whether we can leverage the
extracted features to judge if a system is a sandbox. Intuitively, we explore fea-
tures that are inherent to sandboxes due to hardware constraints, their snapshot-
based operations, or lack of user interactions. We will show how we can use those
inherent features to detect sandboxes using supervised machine learning tech-
niques. We will ﬁrst describe the feature selection for this task and then outline
how we design and evaluate a classiﬁer for sandboxes with those features using
Support Vector Machines (SVMs) [21].
5.1 Feature Selection
The key idea behind the feature selection is to ﬁnd patterns which are charac-
teristic for a sandbox operation but unlikely for a machine under human control.
Instead of identifying speciﬁc ﬁngerprints for particular sandboxes, we strive to
ﬁnd sandbox-inherent features that are common to all sandboxes.
Feature Selection Process. To establish a ground truth for user PCs, we
execute SandPrint on 50 commodity Windows workstations which are not used
as sandboxes and are under the control of human operators. We then manually
examined the reports to identify inherent and meaningful patterns which we
observed in the sandbox reports but which were not as characteristic for the
user reports. Table 3 summarizes the selected features, which we divide in the
three categories hardware, history and execution. The second column contains
the feature name, the third one describes our observations from the sandbox
176
A. Yokoyama et al.
reports, and the last column shows how we transform the feature value to an
integer before we pass it to the SVM (as we will discuss in Sect. 5.2).
The observations mentioned in Table 3 are a vague description of the feature
characteristic. A na¨ıve approach would be to derive sandbox signatures for con-
crete values, such as searching for reports with a display resolution of 4:3. This
observation was made for the vast majority of sandboxes, but was uncommon
for a real user. However, there are several problems when choosing such concrete
values. First, the feature value is not necessarily so precise that such a solution
would make sense. The screen resolution, for example, was not 4:3 for all sand-
box reports, but 5:4 or some other suspicious value which we did not observe
in the user reports. Thus, instead of ﬁguring out concrete values and checks for
each feature, we leave this task to the training process of our SVM classiﬁer.
Similarly, we also refrain from detecting virtualization techniques, and rather
focus on inherent sandbox techniques. Technically, we could check for artifacts
that indicate the presence of a virtualization solution such as VMWare or Vir-
tual Box, which is frequently used by sandboxes. However, we would bias our
classiﬁer towards detecting virtual machines, which is not the objective. While
virtualization is deﬁnitely a hint toward the presence of a sandbox, it is also def-
initely not a guarantee. For instance, we found one user report which indicated
that the execution was taking place in a VMWare virtual machine. Our classiﬁer
should be able to classify this machine as a user machine and not as a sandbox.
Conversely, a sandbox does not necessarily use virtualization as, for example, in
the case of bare-metal sandboxes. Our classiﬁer should be able to classify those
systems as sandboxes despite the absence of virtualization, as our features are
based on the observation that sandboxes use snapshots, have restricted resources,
and uncommon user interaction.
Feature Description. We now describe the features in more detail. The hard-
ware features are motivated by the fact that sandbox operators restrict resources
in order to leverage parallel computation. Therefore, it is quite common that
sandboxes are single core, use little RAM, and have small disk sizes, whereas
these quantities are much larger on the average user PCs. Second, since sand-
boxes are usually not interactively used by a human, the operators often do
not customize the hardware conﬁgurations. We argue that a small display size
and uncommon display resolution as well as a PS/2 mouse are all indicators
for a sandbox. It is worth mentioning that this is not equivalent to virtualiza-
tion detection, where these conﬁgurations are usually the default as well. A user
interactively using her VM likely customizes its screen resolution and increases
its computation power by using more cores and more RAM. The history features
mainly originate from the observation that sandboxes leverage snapshot technol-
ogy. Prior to a malware sample being analyzed, the sandbox usually restores the
system state to a previously captured clean state, which is called a snapshot. A
snapshot is typically taken once when the sandbox is set up and is then used for
the rest of the operation time of the sandbox (unless it is occasionally updated).
As a consequence, it is likely to show history artifacts. For example, if a snap-
shot was taken months ago, every time the snapshot is restored, the login history
SandPrint: Fingerprinting Malware Sandboxes
177
would reveal that the last login was at that time. Similarly, the ﬁle access history
would reveal that the last ﬁle access happened suspiciously long ago. In addi-
tion, we observed that many sandboxes had just been started, whereas user PCs
usually have a longer uptime. Sandbox reports frequently show system uptimes
on the order of seconds, whereas a vulnerable system that is about to be infected
(e.g., via a drive-by download) likely has a signiﬁcantly higher uptime.
The execution features stem from the sandbox showing uncommon execution
patterns. We noticed, for example, that sandboxes tend to change the image
executable name to something which is easier to handle in terms of automa-
tion. It is quite common that sandboxes uses MD5 hashes or generic names
such as virus.exe, whereas the user reports indicate that such renaming is
unlikely. We also found that the clipboard of the sandboxes was empty or con-
tained seemingly-random strings, whereas users’ clipboards tended to contain
more meaningful values such as links, text, or ﬁle objects. Finally, we observed
that sandboxes returned suspicious values for the system manufacturer, such as
empty or random strings, possibly to hide real names—which we did not observe
in the user reports.
5.2 Classiﬁcation
We use the previously-described features to train a classiﬁer that can automat-
ically learn a model to predict if an unknown feature report was taken on a
sandbox or a user PC. To this end, we build up a training data set that consists
of all 50 user reports and up to three random samples from each sandbox cluster.
In total this gives us a training set of 202 reports, 50 of which are user reports
and 152 of which are sandbox reports.
For building the classiﬁer, we use an SVM with a radial basis function kernel.
To normalize the feature vector that we pass to the SVM, we need to transform
the feature values into numerical values. This is done according to the last column
in Table 3. Here, id means that we simply take the number as is, len means that
we consider the length of the string feature, and {0,1} is a boolean value (in
our case, to show if the report indicates a PS/2 mouse or not). Similarly, for the
image name feature we check if the ﬁle image name has been altered. Since not
every feature is available for every report, for reasons explained in Sect. 4, we
decided to use mean imputation to estimate missing values. Finally, we normalize
values to the [0, 1] range using Min-Max Scaling.
To build a classiﬁer, we need to specify an eﬀective combination of the SVM
regularization constant C and the kernel parameter γ. For this purpose, we use
hyperparameter tuning with grid search and 10-fold cross validation to compute
the accuracy of our classiﬁer. We use 10-fold cross validation on top of this
methodology to ensure that we get unbiased results. In an initial step, to evaluate
the strength of each individual feature, we built a classiﬁer for each single feature.
The results of this experiment are depicted in Fig. 1. As we can see, even a single
feature can be used to detect sandboxes with high accuracy, with the RAM
feature being the best, at an accuracy of 98.06 %. However, a single feature is
easier to ﬁx for a sandbox operator than multiple features. We thus also created
178
A. Yokoyama et al.
a classiﬁer that trains on all features. The rightmost bar in Fig. 1 shows that
this classiﬁer has a perfect accuracy of 100 % (i.e. 0 false positives and 0 false
negatives), illustrating the strength of combining multiple detection features.
5.3 Comparison to Existing Solutions
In order to evaluate how well our classiﬁer performs, we decided to compare our
methodology to existing work. For this purpose, we use Paranoid Fish (Pafish),
a popular framework consisting of a collection of several well-known sandbox
detection techniques used by malware in the wild. We encoded 45 detection
techniques used by Pafish in SandPrint and performed them during each run.
Using those 45 detection results, we then built a classiﬁer in the same fashion as
before. We consider each detection a feature for which we build a single classiﬁer,
and we also build a classiﬁer for the combination of all 45 features. The accuracy
results for these classiﬁers are depicted in Fig. 2. Again, each light colored bar
shows the accuracy for a single feature, and the black bar shows the accuracy
for the classiﬁer which combines all the features. As we can see, the majority of
the single-feature classiﬁers are not much better than guessing. Two features are
above 80 % accuracy, with the best individual feature (rdtsc time measurements
to detect virtualization [18]) having 93 % accuracy. The combined version has
an accuracy of 97.8 %.
Besides having a better accuracy, we argue that our methodology is superior
to Pafish for two additional reasons. First, Pafish mainly checks for virtual-
ization artifacts, from which we refrain for reasons explained before. Second, the
majority of the checks performed by Pafish are not stealthy by any means, since
it heavily queries information from the registry, network adapters, and other
sources which are likely to be monitored by the sandbox. By doing so, Pafish
Fig. 1. Classiﬁer accuracy (larger is
better).
Fig. 2. Pafish classiﬁer accuracy.
SandPrint: Fingerprinting Malware Sandboxes
179
risks being detected as an environment-sensitive malware. In contrast, we argue
our method’s information extraction is stealthier. In fact, as we will see in Sect. 6
our approach is not even detected by state-of-the-art security appliances, which
highlights the stealthiness of our approach.
5.4 Summary
As we have shown, we can reliably distinguish between a sandbox and a user
machine based on sandbox-inherent features. Although the number of features
seems quite small, we argue that hiding those features takes a lot of eﬀort for
the sandbox operator. While changing the screen size and switching to a USB
emulated mouse is conﬁgurable, removing the parallel computation artifacts is
not as simple. Increasing the number of cores and the amount of memory is
likely not to be an option for the operator, as this would decrease the produc-
tivity of the sandbox. This could be solved through a solution which gives the
running programs the impression of more resources. Similarly, avoiding history
artifacts introduced by snapshots also requires engineering eﬀort. For example,
the sandbox operator could make sure that all the relevant history information
on the system appears to be normal. A solution could be to customize sandbox
snapshots and keep them up-to-date like non-sandbox systems. Unfortunately,
such customization it is high eﬀort, might be prone to errors, and likely needs
to be reimplemented for every operating system under analysis. For other coun-
termeasures which could be applied by sandbox operators, we refer to Sect. 7.1,
where we combine this aspect with an ethical discussion of our work.
6 Malware Appliance Detection
Seeing that one can detect publicly-exposed sandboxes, we wondered if we could
use the classiﬁer trained on public knowledge to evade closed malware analysis
appliances. Appliances are diﬀerent from sandboxes in that their main objective
is not to analyze the complete behavior of malware, but rather to detect malware
in order to protect a sensitive infrastructure against cyber attacks. An advanced
attacker may thus have strong incentives to detect an appliance in order to ﬂy
under the radar. That is, if an attacker can detect an appliance, she could hide
her program’s malicious behavior to avoid triggering any alert in the appliance.
Looking at the feature selection in Sect. 5, we realized that we can possibly
assume that appliances could share the same feature characteristics as sandboxes.
To verify this, we run SandPrint on three popular appliances from well-known
vendors1. For this purpose, we gained access to various instances (Windows 7,
Windows XP, 32/64 bit, diﬀerent service packs, etc.) of the appliances. We ran
SandPrint four times on each instance and collected 40 reports. Obtaining the
features was not as trivial as in the case of publicly available sandboxes, since
the appliances did not allow for network communication. To overcome this issue,
1 We omit the vendor names not to pinpoint to weaknesses of individual appliances.
180
A. Yokoyama et al.
we encoded the extracted features in the analysis report which was produced by
the appliance after executing SandPrint.
When manually inspecting the feature reports, we found out that our assump-
tion about the feature characteristics was correct. Similar to sandboxes, appli-
ances also exhibit hardware, history and execution characteristics that indicate
non-human and non-interactive usage. To our surprise, some features were even
stronger than in the sandbox case. For example, all 40 reports contained a small
screen width and a 4:3 screen resolution.
For each appliance, we then measure how accurately the classiﬁer that we
trained on the sandboxes and user report performs on the appliance reports.
With an accuracy of 100 %, the classiﬁer detected all appliances as non-user
machines. However, the main priority in this setting is not evasion per se, but
rather stealth evasion. That is, while an attacker aims to detect an appliance,
she does not want her detection method to be unveiled. We thus had a look at
the reports produced by the appliances and found out that SandPrint created
many security alerts by reading information such as motherboard information,
BIOS information, or serial numbers. We then checked if the features used for
the classiﬁer were also on the list of alerts, which would essentially negate the
stealthiness of the detection. For example, many Pafish checks were detected
by the appliances. Although the majority of the sandbox-inherent features did
not trigger an alert by any appliance, we discovered that one appliance considers
reading the disk information as suspicious behavior. To counter this, we removed
the disk feature from the feature vector and evaluated the classiﬁer again on the
appliance reports, resulting again in 100 % accuracy—even for stealth evasion.
To summarize, an attacker can reveal characteristics of publicly available