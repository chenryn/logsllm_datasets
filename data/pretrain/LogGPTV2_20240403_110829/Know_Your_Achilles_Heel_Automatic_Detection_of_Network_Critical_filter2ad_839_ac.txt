tionship graph and we use it to update the initial impor-
tance scores. The nodes of the relationship graph are ser-
vices. There is an edge from S1 to S2 in the relationship
graph if there is a relationship between S1 and S2. In the
following, we discuss how service dependencies and backups
lead to edges in this graph.
Service dependency relationships. A service S1 depends
on another service S2 if a failure in service S2 disrupts the
activity of S1. We use two diﬀerent ways to determine ser-
vice dependency relationships.
When two services are part of the same mission, we intro-
duce a corresponding edge into the relationship graph.
As a second mechanism, we use the simultaneous failure
of two services as another indication of their dependency. In
our current implementation, Paris uses failure reports sent
by a network monitoring device (Nagios) to detect service
failures. More precisely, Paris analyzes the failure logs col-
lected during the analysis period TE. We consider a service
3
In this case, a higher importance implies that the services
is more fragile, and hence, requires more attention from a
system administrator.
46
Table 1: The parameters of Paris
Symbol Description
TE
TI
Δ
ηS
ηC
ηM
K
ηNC
γ
Analysis period
Length of a time series interval
Length of time slot for time series
Service extraction threshold
Service correlation threshold
Mission threshold
Number of service clusters
Negative correlation threshold
Weight factor of link analysis
Value
1 month
1 hour
5 min
30
0.49726
24
20
-0.49726
5
S1 to depend on another service S2 if at least f = 50% of
S2’s failures co-occur with a failure of S1. We consider two
failure messages to co-occur if they are reported within a
time window smaller that Tf (we set Tf = 1min, consider-
ing the time accuracy of the Nagios reports). If two services
are found that have such a signiﬁcant correlation of their
failure reports, we introduce a corresponding edge into the
relationship graph.
Backup relationships. If service S1 is a passive backup
of service S2, we add an edge from S2 to S1. The intuition
behind this step is that if service S2 has a backup, it loses
some of its importance while the backup gains some of this
importance.
Propagating scores. Once the network relationship graph
is generated, Paris uses Google’s PageRank link analysis
algorithm [33] to propagate initial importance scores. The
intuition behind propagating importance scores in the rela-
tionship graphs is simple; a service that important services
depend on is also important.
Once Paris has performed the adjustments for service de-
pendencies and backups, we use the ﬁnal values of the nodes
in the relationship graph as the ﬁnal importance scores for
the network services.
5.
IMPLEMENTATION AND EVALUATION
We implemented Paris as a system in Python and eval-
uated its performance by running it on the network mon-
itoring data collected from a large university department
network. Our dataset contains 38.5 GB of NetFlow records,
gathered over a period of ﬁve months. Overall, the dataset
captures more than 1.6 billion connections and 593 unique
internal IP addresses. Out of the 1.6 billion connections,
1.25 billion connections were between an internal and an ex-
ternal host, while 350 million connections were between two
internal hosts. We also had access to the ﬁrewall conﬁgura-
tion ﬁles of the department, which contain 1,141 ACL rules.
Moreover, we had access to approximately 120 thousand Na-
gios alerts that were gathered during the same one-month
period.
5.1 Experimental Results
Paris took about ﬁve hours to analyze one-month of net-
work data; this could be further improved by utilizing a more
powerful machine and optimizing the code. Table 1 summa-
rizes the parameters used for our experiments, as discussed
throughout the paper.
In the ﬁrst step, Paris extracted 156 network services.
In the next step, Paris checked for relationships between
services. In our dataset, we found 4,049 candidate missions
Mission Occurrence Distribution
#missions
 4500
 4000
 3500
 3000
 2500
 2000
 1500
 1000
 500
i
s
n
o
s
s
m
#
i
 0
 0
 50
 100
 150
 200
 250
#occurrences
 300
 350
 400
 450
Figure 2: Mission frequency distribution.
(sets of correlated services). Figure 2 shows the distribution
of the frequency of these candidate missions. The horizontal
axis shows the number of occurrences x for missions, and the
vertical axis shows the number of missions y that occur at
least x times.
Looking at Figure 2, we see that the threshold ηM = 24,
which is the cut-oﬀ between infrequent and frequent mis-
sions, is close to the inﬂection point of the graph. Using this
threshold for the ﬁrst ﬁltering step, we remove 3,338 infre-
quent candidate missions. A large fraction of these 3,338
missions (2,609, or 78.2%) are supersets of more frequent
missions (those on the right of the cut-oﬀ point). A mission
X is a superset of another mission Y when X contains all
services of Y and at least one additional service. Typically,
such infrequent supersets are detected when an unrelated
service S happens to be active at the same time that a “true”
mission is operating. In this case, Paris will correlate and
incorrectly connect these services together. The ﬁrst ﬁlter-
ing step removes (most of) these spurious relationships.
Only 729 of the removed candidate missions in the ﬁrst ﬁl-
tering step were not supersets of frequent candidates. In 531
of these 729 cases, the missions involved port 22 (ssh) and
port 111 (portmapper). The infrequent ssh missions typi-
cally capture cases in which someone logged into a remote
machine and initiated some actions or tunneled some traﬃc.
The portmapper missions are introduced because portmap-
per is very frequently used, and hence, gets correlated with
other, independent services that happen to be active at the
same times. We only found two interesting cases that Paris
arguably missed. In one case, a set of machines was using a
P2P protocol to exchange data for two hours. In the second
case, a set of Hadoop machines were working together over a
period of six hours. Notice that we found Hadoop machines
also involved in more frequent activities, but these missions
included additional (non-Hadoop) machines as well.
In the second ﬁltering step, Paris examines the remaining
711 missions and removes an additional 594 missions that
are subsets of other frequent candidate missions. A mission
X is a subset of Y if it contains a subset of Y ’s services.
The second ﬁltering step is useful to remove instances in
which Paris has detected most of the services that make up
a mission, but, because of noise in the data, missed one that
should have been included as well. We manually inspected
the 594 missions. As expected, the removed missions were all
parts of more complete missions that Paris retained. Hence,
47
no valuable information was lost. Finally, as its output, the
mission extraction process produced 117 network missions.
Table 2 lists all the identiﬁed network operations, together
with their corresponding numbers of constituent missions.
This process was done manually, based on the knowledge
about the domain and the application protocol semantics
of diﬀerent services. We performed this analysis to be able
to present our results in a more succinct fashion. However,
information about the missions themselves would already
provide signiﬁcant insights into the major tasks of the net-
work.
Looking at Table 2, it can be seen that some missions
map directly onto an operation.
In other, more complex
operations, we have identiﬁed multiple missions that are all
related to a single operation. In the case of the “web op-
eration”, we found 63 individual missions. The individual
missions were related to communication between the web
server and storage servers (such as the NFS ﬁle services and
Hadoop), authentication tasks (including LDAP), and DNS.
Given that there were multiple services involved for each ser-
vice type, we observed multiple combinations among indi-
vidual groups of services. Most operations make immediate
sense for a university network when looking at the services
that are involved (web, mail, conﬁgurations with cfengine,
...). We discussed the extracted operations with the admin-
istrator of the network, and he veriﬁed the correctness of all
but two extracted operations. These two operations, which
the administrator was not aware of, were the malware anal-
ysis operation and the cloud operation.
We looked at the involved IP addresses and further tracked
down these operations. We found that the malware analy-
sis operation involved three machines, the actual analysis
machine that was running malware samples, the MySQL
server to store results, and a web server through which
samples were submitted by external sources. The oper-
ation is important for the research group who runs this
analysis infrastructure, and it was running for most of the
month. The second mission involved four machines that
were likely running cloud computing services (the machines
were named eucalyptus-*, based on the popular, open-
source cloud computing package). These machines worked
together intensely for a total of 27 hours during the analysis
period.
Our analysis conﬁrmed that we found the key operations
that the university runs. In addition, we found two inter-
esting (and, for the involved parties, important) operations
that the system administrators were unaware of.
Backup services. We also checked for backup services.
Our system was able to identify four backup services in the
organization. In particular, Paris identiﬁed an NFS backup,
an LDAP backup, a zookeeper server backup, and a main
web server backup. The network administrator again veri-
ﬁed that Paris had detected all of the organization’s backup
servers and that no false positive backup was detected.
Ranking services and hosts. Using the information about
the network services and their relationships, we computed
the importance scores for each service and host.
In Table 3, we show the Top-10 services, given their ﬁnal
importance ranking. Along with their ﬁnal rankings, the ta-
ble also shows the initial rankings of these services (Column
3), the services that were in that location before comput-
ing ﬁnal scores (Column 5), and the new ranking of those
services based on the ﬁnal scores (Column 6). For example,
Table 3: The importance ranking of the Top-10 ser-
vices.
Final top 10
Initial top 10
Final
1
2
3
4
5
6
7
8
9
10
Name
DNS
NFS1
cfengine
main web
LDAP30
LDAP36
ﬁleserver1
ﬁleserver2
Hadoop
NFS2
Init
3
1
12
2
9
5
15
10
4
11
Init
1
2
3
4
5
6
7
8
9
10
Name
NFS1
main web
DNS
Hadoop
LDAP36
LDAP12
web37
DHCP
LDAP30
ﬁleserver2
Final
2
4
1
9
6
15
12
58
5
8
the ﬁrst row means that the DNS server’s ﬁnal ranking is 1,
while it was at location 3 before the ﬁnal score adjustments.
Also it shows that before the ﬁnal adjustment, server NFS1
was at rank 1 and after score readjustment has moved to
rank 2. This is to demonstrate the diﬀerence between initial
and ﬁnal rankings. In general, we observe that the impor-
tance scores of those services that depend on many other
(important) services increase. For instance, the table shows
the eﬀect on the ranking of the cfengine service.
It rises
in importance because it was part of multiple missions and
forms the foundation for the correct functioning of many
servers. As another example, we can see that the DHCP
service has dropped signiﬁcantly in the ranking table, since
not many other services depended on it. This is because the
department’s policy is to assign static IP addresses to all
important network services.
We also attempted to validate the correctness of the ser-
vice ranking with the network administrator of the organi-
zation. In particular, we asked the administrator’s opinion
about the Top-10 important services; all of the mentioned
services appear in the Top-20 important services ranked by
our system. Next, we presented our list of Top-10 services
to the administrator and he veriﬁed that all of the extracted
services and hosts are among the top important services of
the organization.
Paris can also compute importance scores for individual
hosts. This score is based on the importance of the net-
work services {S1, ..., Si} that a host H provides, and it is
computed as:
I h
(H) =
i(cid:5)
j=1
I s
(Sj)
(7)
Table 4 shows the importance rankings of the hosts. The
results were veriﬁed with the network administrators as well.
6. DISCUSSION
In this section, we address possible complexity and gener-
ality issues of Paris.
6.1 Complexity