tention. Such information serves to identify interesting at-
tacks, attack patterns, previously unseen exploits as well as
potential vulnerabilities. Being situated at the network layer
means that only a port-mirror is needed for the sensor in or-
der to monitor remote hosts or deployment at a proxy junc-
tion. Runtime results in this paper illustrate the capacity
for real-time detection in such settings. Unlike other NIDS,
Spectrogram operates above the packet layer, at the
web/CGI-layer, and was designed speciﬁcally to be HTTP-
protocol aware. Dynamic re-assembly is used to reconstruct
the full HTTP requests as they would be seen by the targeted
web application, this yields the side-beneﬁt of resistance
against fragmentation attacks since the sensor explicitly re-
construct all fragments. Related sensors [29, 34, 12] which
operate at the network layer and perform per-packet inspec-
tion can, to certain degrees, be frustrated by evasion tactics
such as overlaying-reassembly [27] if left in their
naive setting. All HTTP requests are parsed to isolate the
script argument strings and drop irrelevant content which
might skew the anomaly score. Details on the runtime these
operations entail are presented in Section 5.
Protocol-aware parsing for HTTP requests is imple-
mented to provide a more detailed level of analysis and to
trim out irrelevant features and isolate only the script argu-
ments. Statistical “blending” attacks on AD sensors would
be possible if no parsing is used. A trivial example of such
an attack is the insertion of legitimate content in the un-
used protocol ﬁelds, this would skew the anomaly score
if the entire request is modeled holistically; good content,
which does not effect the exploit, is being inserted in ad-
dition to malicious code. Spectrogram, by default, does
not model individual protocol ﬁelds such as the referrer
or user-agent strings since these ﬁelds have no inﬂu-
ence on web-layer code-injection attacks. Software layer
attacks against the server applications is an issue but many
of these attacks can be detected by the content of the URL
strings. If protocol sanitization is required then modeling
this ﬁeld can be turned on as well since these ﬁelds accept
simple string data as well, though there is arguably little use
in modeling such ﬁelds for IDS purposes. For further dis-
cussion on blending attacks, we refer the reader to Fogla
et al. [14]. Spectrogram examines both GET and POST
requests. For GET, we look in the URL to obtain the ar-
gument string; for POST, the message body. If the server
allows input in other ways, such as custom Apache mod-
ules, then only new parsers would be needed to extend the
framework. The script argument string is also unescaped
and normalized before being passed into the AD module
for classiﬁcation. These steps are elaborated in the follow-
ing section.
Spectrogram is built on top of tcpflow [9], an efﬁ-
cient content ﬂow re-assembly engine for TCP trafﬁc which
utilizes hash tables to reconstruct each ﬂow. The script ar-
gument string for each request is extracted for processing
— this includes both variable names and corresponding ar-
gument values. The combination of the two elements in-
duces a third important feature, characteristic of web-layer
requests: structure. Script arguments strings within HTTP
requests are structured by placing variable name and their
respective arguments in pairs, with each pair placed from
left to right within the argument string. By examining entire
strings, one can capture not only what the contents of the ar-
guments should be but also how they should be positioned.
This structure is standard for HTTP and is important as it
admits string models for anomaly detection, and is the pri-
mary reason why Spectrogram utilizes Markov chains.
The Markov model we use expands upon earlier works by
Kruegel et al. [15, 16] whose AD sensor contained a compo-
nent which modeled content using single-gram transitions
and operated on Apache log ﬁles. The model we present is
a multi-step Markov chain which examines multiple gram-
transitions. Furthermore, Spectrogram utilizes a mix-
ture of such chains for improved accuracy, designed un-
der machine learning (ML) principles. An ML algorithm
which recovers the optimal parameters for this model from
the data is offered in this paper, constructed under a su-
pervised learning framework. The previously mentioned
sensors [15, 16, 34, 12] use unsupervised algorithms. The
distinction is important — while unsupervised approaches
eliminates the need for labeled training data, they are guar-
anteed to ﬁnd only statistical anomalies — inputs not fre-
quently seen but not necessarily malicious. Trade-offs be-
tween FP rates and accuracy are more difﬁcult to address for
these sensors and are driven primarily by model complexity.
The next section elaborates on these issues and describes the
mathematical details of our model.
4 The Spectrogram Model
As previously mentioned, Spectrogram is a string
model, designed speciﬁcally to recognize the distribution
of content and structure present within web-layer script in-
put strings. Higher order statistics is used for improved ca-
pacity and the ill-posed nature of modeling such short dy-
namic strings is offset with a Markov-chain relaxation in
the dependency assumptions. The Markov-chain structure
is appropriate for this problem given the default ordering
of content within web-layer inputs. From a statistics per-
spective, the relaxation induces a more ﬂexible interpola-
tion between the i.d. and i.i.d extremes that characterize
previous models [34, 12]. Figure (3) shows the portion
of the request that Spectrogram examines — parame-
ter names, their respective inputs, as well as their layout
GET /path/script.php?val1=bleh&val2=blah&val3=... HTTP/1.1
Host: vulnerable.com
User-Agent: Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; ...
Accept-Language: en-us,en;q=0.5
Referrer: http://somesite.net
...
Figure 3. Sample HTTP request. Bold, under(cid:173)
lined text indicates the portion of the request
that is modeled by the sensor.
In a GET re(cid:173)
quest, the URL string is parsed; in a POST,
the message body.
with respect to each other, are modeled jointly. The sensor
is required to infer the likelihood that the input string is le-
gitimate; that not only are the substrings “bleh” and “val1”
valid but their order is also valid and that “val2” should fol-
low these substrings. For this purpose, the inference model
tracks the n-gram level transitions within a string, resolv-
ing the likelihood of each observed n-gram given the pre-
ceding n-gram: p(’al1=bleh&’|’val1=bleh’). Capturing this
structure infers that “bleh” is an argument for the variable
“val1”; if “val1” is followed by another sequence of un-
recognized characters, it would be considered anomalous.
The transition-based conditional probability model is anal-
ogous to a sliding window that shifts by a single charac-
ter, with each n-gram sized window dependent upon on the
previous. It is intuitive to see then that sufﬁciently large
sized n-grams must be used to obtain an reliable estimate.
However, the problem of modeling n-grams is ill-posed as
mentioned previously, this means that small deviations in
the training data could yield large deviations in the per-
formance. Spectrogram compensates by factoring the
n-gram distribution into a product of n − 1 pair-wise con-
ditional dependency models. Each character within an n-
gram is conditioned on the previous n − 1 characters. This
factorization reduces the problem from exponential to lin-
ear complexity and takes advantages of the overlapping na-
ture of the n-grams within an input string — we only need
to calculate the likelihood of each character within a string
once, though that character may contribute to multiple over-
lapping n-grams. This explained in more detail in the fol-
lowing section.
Within the Spectrogram model, a single Markov
chain recovers the likelihood of any given string by calcu-
lating the likelihood of each character and then recovering
the geometric mean of the individual likelihoods. Multiple
Markov-chains are used in a linear mixture to obtain the
ﬁnal likelihood score. The only parameters to set within
Spectrogram’s inference model are the gram size N and
the number of mixtures M , these parameters are speciﬁed
during training. The following sub-sections illustrate the
n-gram modeling problem in extended detail and derives
the mixture of Markov-chains model step-by-step. In sec-
tion 4.4 and the appendix, we derive a learning algorithm
which automatically learns the model parameters based on
the training data.
4.1 N (cid:173)Grams and the Curse of Dimensionality
N -gram based models have been successfully utilized in
recent years for AD roles. Given a string “http://”, 2-gram
tokens would be “ht”, “tt”, “tp”, etc. One seeks to recover
an accurate estimation of the distribution of these grams.
An example of such a sensor is Anagram, introduced by
Wang et al. [12]. The optimal way to model such a dis-
tribution, however, remains an open question. Given an n-
gram, if independence between the individual characters is
assumed, the sufﬁcient parameters of the model would en-
compass the frequency of each individual character, inde-
pendently. This approach requires 256 numbers (valid byte
range) and is what the PayL [34] sensor uses in its naive
setting.
If we were to model n-grams jointly to recover
an estimate for the distribution of all n-sized token, such
as “http://”, then estimation of 2567 individual parameters
would be required. In general 256N numbers are required
for gram size N . This approach, to some degree, is what the
Anagram sensor utilizes. For large n, however, we would
never see enough training data to properly ﬁt a full n-gram
distribution, making this an ill-posed problem. For exam-
ple, if “val1=AAA&val2=” was seen in the training data but
“val1=BBB&val2=” was not, the latter would be ﬂagged as
anomalous though it might be a legitimate request. This
model suffers from the curse of dimensionality when we at-
tempt to increase its capacity by increasing gram-size. Con-
versely, when modeling grams independently, as in PayL,
if three “B”’s were observed anywhere in the training sam-
ples then it would be considered normal for them to be any-
where else in the input string, thus throwing away structure
information. An effective attack is simply to adding three
“B”’s to the end of the attack string to make the request
seem more legitimate. Anagram addresses the problem
with a trade-off between speed and generalization-ability,
using hash collisions on subsets of speciﬁc n-grams. This
allows fast classiﬁcation and recognition of deviations in
static content, such as protocol violation, but can become
unstable for small dynamic string content modeling in the
web-layer domain, as results in this paper will demonstrate.
In contrast, Spectrogram models strings by relaxing
the exponentially growing n-gram distribution into an n-
step Markov-chain, as an interpolation between the the two
previously explored extremes. An n-gram’s normality, in
this factorization, is conditioned on the n − 1 preceding
grams: given a 5-gram model and input string “http:/”, we
condition the normality of the character “/” on the frequency
that “:” was observed in the previous position during train-
ing, that “p” was observed two positions prior, “t” three
positions prior etc. Upon examining “val1=BBB&val2=”,
“BBB” is unrecognized but “val1=” and “&val2=” are rec-
ognized. Moreover, they are recognized to be in the cor-
rect positions with respect to each other, thus the string ap-
pears only slightly anomalous due to “BBB”’s presence, as
desired. Spectrogram also detects padding by immedi-
ately ﬂagging strings larger than three standard deviations
above the average input length. This acts as a fast ad hoc
heuristic ﬁlter and should be adjusted per host. These ef-
forts combine to resists statistical blending attacks:
if an
attacker were to attempt to blend malcode into legitimate
trafﬁc, he would need to insert normal content, in the same
n-gram distribution as a legitimate request, as well as en-
sure correct structure, while remaining within the accept-
able length, at which point he would be sending a legiti-
mate request and not an attack. Whereas PayL requires 256
numbers and Anagram, o(256N ), a Markov-chain model
requires 2562 × (n− 1) numbers at the n-gram level. Since
Spectrogram is a mixture of Markov-chains (with mix-
ing weights), M × (2562 × (n − 1)) + M numbers are
required per model for a mixture of M -Markov chains.
M controls the capacity of the model and correlates with
the number of clusters within the data. Given the depen-
dency structure, the clusters in this case capture the multi-
step transitions between alphanumeric characters that en-
code content and structure, the linkage of certain symbols
such as “&”, “=”, and their overall distributions. In prac-
tice, cross-validation can be used to determine the optimal
setting for M . The following sub-sections explore the math-
ematics of this model.
4.2 Factorized N (cid:173)Gram Markov Models
As previously mentioned, modeling n-grams entails
estimating a distribution over an exponentially growing
sample-space, making the problem ill-posed. A Markov-
chain, on the other hand, leverages the structure of web-
requests to reduce the complexity into a linearly grow-
ing space. For example, a 2-gram model reduces to a
model on 1-gram transitions. Rather than explicitly mod-
eling the likelihood of observing any two characters, the
model tracks the likelihood of observing the second char-
acter given the ﬁrst. This conditional model is denoted
by p(xi|xi−1), where xi denotes the ith character within
a string and xi−1 denotes the (i − 1)th character. Extend-
ing this concept, the likelihood of an n-gram is driven by
the likelihood of xn and is conditioned on the n − 1 pre-
ceding characters, p(xn|xn−1, xn−2, .., x1). The Markov
chain approach decouples the preceding n − 1 characters
from each other given the nth character, that is (xi⊥xj|xn,
where i, j < n), and the joint likelihood is then reset
as the product of these pair-wise conditionals. A 5-gram
takes the form p(x5|x4, .., x1) =
model, for example,
p(x5|x4)p(x5|x3)p(x5|x2)p(x5|x1). We introduce the vari-
able G to indicate the gram size and Equations (1) and (2)
shows the interaction of the likelihood values within the
larger chain structure:
pG(xi|xi−1, .., xi−G+1) =
G−1
Y
j=1
p(xi|xi−j )
pG(x1, ..., xN ) =
N
G−1
Y
i=G
Y
j=i
p(xi|xi−j )
(1)
(2)
For the joint likelihood of the entire script argument string,
such as the example displayed in Figure (3), we need the
product of the individual likelihood values. This is repre-
sented in Equation (2) where capital N is used to denote the
length of the entire string. The inner product indicates the
shifting G-sized window across the larger N -sized string.
With this factorization, n − 1 transition matrices, each of
dimensionality 256× 256, needs to be kept in memory; this
algorithm has complexity growth in O(n). Also notice that
since this model is a continuous product of likelihood val-
ues, each of which is valued between 0 and 1, it becomes ap-
parent that longer strings will yield lower total likelihoods
which is not a desired effect since input length and intent are
not strongly coupled. A more appropriate form is a mean of
likelihood values. The interaction is a product of N values,
therefore the N th root is needed, i.e. we need to solve for
the geometric mean.
pG(x1, ..., xN ) = 
N
G−1
Y
i=G
Y
j=i
p(xi|xi−j )
1/N
(3)
Equation (3) calculates the likelihood value for each input
string. The capacity of this model can be improved by plac-
ing this Markov-chain within a mixture model framework
— in the ﬁnal model, M chains contribute to the ﬁnal score,
with each chain’s score weighed appropriately by a scalar
mixing coefﬁcient. This model is more general since a mix-
ture model with M = 1 represents a single chain. The
use of multiple-chains improve upon the capacity of this
model by explicitly capturing subclasses of information in
a K-means like approach to better capture the potentially
many subclasses of input encountered. In fact, the training
method we use, EM, can be considered as a ”‘soft”’ ver-
sion of K-means. Since each Markov chain tracks the tran-
sitional structure within subclasses of input strings, these
clusters correlate more with different types of input struc-
tures. For instance, strings with many numerical transitions,
strings using many non-alphanumeric characters, etc.
4.3 Mixture of Markov Models
Construction of this mixture model follows from stan-
dard machine learning procedure of introducing “hidden”
states in a weighted-summation mix of all individual chains.
Each submodel has the form shown in Equation (3). New
input samples would be evaluated over M chains and their
values combined in a linear function. Though they share
identical structure, these chains have distinct – and inde-
pendent – model parameters which are recovered from the
data. We use θi to denote the parameter variable for the
ith chain and Θ = {θ1, θ2, .., θM} to denote a set of pa-
rameters for M chains. To clarify, when using models with
gram-size G, each θi consists of G − 1 transition matrices.
p(xi|xj) is the likelihood of a transition from one charac-
ter to another and is a single value within one of these ma-
trices, indexed by the two characters. The scalar mixing
value for a particular chain indexed by s is denoted by πs.
Summing over these submodels with their appropriate mix-
ing weights, {π1, π2, , .., πM}, yields the ﬁnal Spectrogram
likelihood value:
pG(x1, .., xN|Θ) =
M
X
s=1
πs
N
G−1
Y
i=G
Y
j=i
p(xi|xi−j ; θs)
1/N
(4)
Equation (4) represents the M -state mixture model that de-
ﬁnes the core classiﬁcation engine within Spectrogram;
a subscript G is used to denote a G-gram sliding win-
dow. Variable s indicates the hidden state index of the
Markov-chains. The mixing proportions all sum to 1:
PM
s=1 πs = 1. Likewise, the transition likelihoods also sum