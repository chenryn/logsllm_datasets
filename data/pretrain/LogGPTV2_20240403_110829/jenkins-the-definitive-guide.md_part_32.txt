-auth scott:tiger
Once you have started the slave agent, be sure to install it as a Windows service, as discussed in the
previous section.
11.3.5. Starting a Windows Slave as a Remote Service
Jenkins can also manage a remote Windows slave as a Windows service, using the Windows
Management Instrumentation (WMI) service which is installed out of the box on Windows 2000 or later
(see Figure 11.14, “Letting Jenkins control a Windows slave as a Windows service”). When you choose
this option, you just need to provide a Windows username and password. The name of the node must
be the hostname of the slave machine.
This is certainly convenient, as it does not require you to physically connect to the Windows machine
to set it up. However, it does have limitations—in particular, you cannot run any applications requiring
a graphical interface, so you can’t use a slave set up this way for web testing, for example. In practice
this can be a little tricky to set up, as you may need to configure the Windows firewall to open the
appropriate services and ports. If you run into trouble, make sure that your network configuration allows
TCP connections to ports 135, 139, and 445, and UDP connections to ports 137 and 138 (see https://
wiki.jenkins-ci.org/display/JENKINS/Windows+slaves+fail+to+start+via+DCOM for more details).
Figure 11.14. Letting Jenkins control a Windows slave as a Windows service
11.4. Associating a Build Job with a Slave or Group of
Slaves
In the previous section, we saw how you can assign labels to your slave nodes. This is a convenient way
to group your slave nodes according to characteristics such as operating system, target environment,
database type, or any other criteria that is relevant to your build process. A common application of
315
this practice is to run OS-specific functional tests on dedicated slave nodes, or to reserve a particular
machine exclusively to performance tests.
Once you have assigned labels to your slave nodes, you also need to tell Jenkins where it can run the build
jobs. By default, Jenkins will simply use the first available slave node, which usually results in the best
overall turn-around time. If you need to tie a build job to a particular machine or group of machines, you
need to tick the “Restrict where this project can be run” checkbox in the build configuration page (see
Figure 11.15, “Running a build job on a particular slave node”). Next, enter the name of the machine, or
a label identifying a group of machines, into the Label Expression field. Jenkins will provide a dynamic
dropdown showing the available machine names and labels as you type.
Figure 11.15. Running a build job on a particular slave node
This field also accepts boolean expressions, allowing you to define more complicated constraints about
where your build job should run. How to use these expressions is best illustrated by an example. Suppose
you have a build farm with Windows and Linux slave nodes (identified by the labels “windows” and
“linux”), distributed over three sites (“sydney”, “sanfrancisco”, and “london”). Your application also
needs to be tested against several different databases (“oracle”, “db2”, “mysql”, and “postgres”). You
also use labels to distinguish slave nodes used to deploy to different environments (test, uat, production).
The simplest use of label expressions is to determine where a build job can or cannot be executed. If
your web tests require Internet Explorer, for example, you will need them to run on a Windows machine.
You could express this by simply quoting the corresponding label:
windows
Alternatively, you might want to run tests against Firefox, but only on Linux machines. You could
exclude Windows machines from the range of candidate build nodes by using the ! negation operator:
316
!windows
You can also use the and (&&) and or (!!) operators to combine expressions. For example, suppose the
Postgres database is only tested for Linux. You could tell Jenkins to run a particular build job only on
Linux machines installed with postgres using the following expression:
linux && postgres
Or you could specify that a particular build job is only to be run on a UAT environment in Sydney or
London:
uat && (sydney || london)
If your machine names contain spaces, you will need to enclose them in double quotes:
"Windows 7" || "Windows XP"
There are also two more advanced logical operators that you may find useful. The implies operator
(=>) lets you define a logical constraint of the form “if A is true, then B must also be true.” For example,
suppose you have a build job that can run on any Linux distribution, but if it is executed on a Windows
box, it must be Windows 7. You could express this constraint as follows:
windows -> "Windows 7"
The other logical operator is the if-and-only-if () operator. This operation lets you define
stronger constraints of the form “If A is true, then B must be true, but if A is false, then B must be false.”
For example, suppose that Windows 7 tests are only to be run in a UAT environment, and that only
Windows 7 tests are to be run in the UAT environment. You could express this as shown here:
"Windows 7"  uat
11.5. Node Monitoring
Jenkins doesn’t just dispatch build jobs to slave agents and hope for the best: it pro-actively monitors
your slave machines, and will take a node offline if it considers that the node is incapable of safely
performing a build. You can fine-tune exactly what Jenkins monitors int the Manage Nodes screen (see
Figure 11.16, “Jenkins proactively monitors your build agents”). Jenkins monitors the slave agents in
several different ways. It monitors the response time: an overly slow response time can indicate either a
network problem or that the slave machine is down. It also monitors the amount of disk space, temporary
directory space and swap space available to the Jenkins user on the slave machine, since build jobs can
be notoriously disk-hungry. It also keeps tabs on the system clocks, as if the clocks are not correctly
synchronized, odd errors can sometimes happen. If any of these criteria is not up to scratch, Jenkins will
automatically take the server offline.
317
Figure 11.16. Jenkins proactively monitors your build agents
11.6. Cloud Computing
Cloud computing involves using hardware resources on the Internet as an extension and/or replacement
of your local computing architecture. Cloud computing is expanding into many areas of the enterprise,
including email and document sharing (Gmail and Google Apps are particularly well-known examples,
but there are many others), off-site data storage (such as Amazon S3), as well as more technical services
such as source code repositories (such as GitHub, Bitbucket, etc.) and many others.
Of course externalized hardware architecture solutions have been around for a long time. The main thing
that distinguishes the cloud computing with more traditional services is the speed and flexibility with
which a service can be brought up, and brought down when it is no longer needed. In a cloud computing
environment, a new machine can be running and available within seconds.
However, cloud computing in the context of Continuous Integration is not always as simple as it might
seem. For any cloud-based approach to work, some of your internal resources may need to be available
to the outside world. This can include opening access to your version control system, your test databases,
and to any other resources that your builds and tests require. All these aspects need to be considered
carefully when choosing a cloud-based CI architecture, and may limit your options if certain resources
simply cannot be accessed from the Internet. Nevertheless, cloud-based CI has the potential of providing
huge benefits when it comes to scalability.
In the following sections, we will look at how to use the Amazon EC2 cloud computing services to set
up a cloud-based build farm.
11.6.1. Using Amazon EC2
In addition to selling books, Amazon is one of the more well-known providers of cloud computing
services. If you are willing to pay for the service, Amazon can provide you build machines that can be
either used permanently as part of your build farm, or brought online as required when your existing
build machines become overloaded. This is an excellent and reasonably cost-efficient way to absorb
extra build load on an as-needed basis, and without the headache of extra physical machines to maintain.
318
If you want the flexibility of a cloud-based CI architecture, but don’t want to externalize your hardware,
another option is to set up a Eucalyptus cloud. Eucalyptus is an open source tool that enables you to create
a local private cloud on existing hardware. Eucalyptus uses an API that is compatible with Amazon EC2
and S3, and works well with Jenkins.
11.6.1.1. Setting up your Amazon EC2 build farm
Amazon EC2 is probably the most popular and well-known commercial cloud computing service. To
use this service, you will need to create an EC2 account with Amazon if you do not already have one.
The process required to do this is well documented on the Amazon website, so we will not dwell on this
here. Once you have created your account, you will be able to create the virtual machines and machine
images that will make up your EC2-based build farm.
When using Amazon EC2, you create virtual machines, called instances, using the Amazon Web
Services (AWS) Management Console (see Figure 11.17, “You manage your EC2 instances using the
Amazon AWS Management Console”). This website is where you manage your running instances and
create new ones. You create these instances from predefined images, called Amazon Machine Images
(AMIs). There are many AMI images, both from Amazon and in the public domain, that you can use as
a starting point, covering most of the popular operating systems. Once you have created a new instance,
you can connect to it using either SSH (for unix machines) or Windows Remote Desktop Connection,
to configure it for your purposes.
Figure 11.17. You manage your EC2 instances using the Amazon AWS Management Console
To set up a build farm, you will also need to configure your have one, just go to the Key Pairs menu in
the Security build server to be able to access your EC2 instances. In particular, you will need to install
the Amazon EC2 API tools, set up the appropriate private/public keys, and allow SSH connections
from your server or network to your Amazon instances. Again, the details of how to do this are well
documented for all the major operating systems on the EC2 website.
You can use Amazon EC2 instances in two ways—either create slave machines on Amazon EC2 and
use them as remote machines, or have Jenkins create them for you dynamically on demand. Or you can
have a combination of the two. Both approaches have their uses, and we will discuss each of them in
the following sections.
319
11.6.1.2. Using EC2 instances as part of your build farm
Creating a new EC2 instance is as simple as choosing the base image you want to use. You will just
need to provide some details about the instance, such as its size and capacity, and the private key you
want to use to access the machine. Amazon will then create a new running virtual machine based on this
image. Once you have set it up, an EC2 instance is essentially a machine like any other, and it is easy and
convenient to set up permanent or semipermanent EC2 machines as part of your build infrastructure.
You may even opt to use an EC2 image as your master server.
Setting up an existing EC2 instance as a Jenkins slave is little different to setting up any other remote
slave. If you are setting up a Unix or Linux EC2 slave, you will need to refer to the private key file
(see Figure 11.18, “Configuring an Amazon EC2 slave”) that you used to create the EC2 instance on
the AWS Management console. Depending on the flavor of Linux you are using, you may also need
to provide a username. Most distributions connect as root, but some, such as Ubuntu, need a different
user name.
Figure 11.18. Configuring an Amazon EC2 slave
11.6.1.3. Using dynamic instances
The second approach involves creating new Amazon EC2 machines dynamically, as they are required.
Setting up dedicated instances is not difficult, but it does not scale well. A better approach is to let
Jenkins create new instances as require. To do this, you will need to install the Jenkins Amazon EC2
plugin. This plugin lets your Jenkins instance start slaves on the EC2 cloud on demand, and then kill
them off when they are no longer needed. The plugin works both with Amazon EC2, and the Ubuntu
Enterprise Cloud. We will be focusing on Amazon EC2 here. Note that at the time of writing the Amazon
EC2 Plugin only supported managing Unix EC2 images.
Once you have installed the plugin and restarted Jenkins, go to the main Jenkins configuration screen and
click on Add a New Cloud (see Figure 11.19, “Configuring an Amazon EC2 slave”). Choose Amazon
EC2. You will need to provide your Amazon Access Key ID and Secret Access Key so that Jenkins
can communicate with your Amazon EC2 account. You can access these in the Key Pairs screen of
your EC2 dashboard.
320
Figure 11.19. Configuring an Amazon EC2 slave
You will also need to provide your RSA private key. If you don’t have one, just go to the Key Pairs
menu in the Security Credentials screen and create one. This will create a new key pair for you and
download the private key. Keep the private key in a safe place (you will need it if you want to connect
to your EC2 instances via SSH).
In the advanced options, you can use the Instance Cap field to limit the number of EC2 instances that
Jenkins will launch. This limit refers to the total number of active EC2 instances, not just the ones
that Jenkins is currently running. This is useful as a safety measure, as you pay for the time your EC2
instances spend active.
Once you have configured your overall EC2 connection, you need to define the machines you will
work with. You do this by specifying the Amazon Mirror Image (AMI) identifier of the server image
you would like to start. Amazon provides some starter images, and many more are available from the
community, however not all images will work with EC2. At the time of writing, only certain images
based on 32-bit Linux distributions work correctly.
The predefined Amazon and public AMI images are useful starting points for your permanent virtual
machines, but for the purposes of implementing a dynamic EC2-based cloud, you need to define your
own AMI with the essential tools (Java, build tools, SCM configuration and so forth) preinstalled.
Fortunately, this is a simple process: just start off with a generic AMI (preferably one compatible with
the Jenkins EC2 plugin), and install everything your builds need. Make sure you use an EBS image.
This way, changes you make to your server instance are persisted on an EBS volume so that you don't
lose them when the server shuts down. Then create a new image by selecting the Create Image option
in the Instances screen on the EC2 management console (see Figure 11.20, “Creating a new Amazon
321
EC2 image”). Make sure SSH is open from your build server’s IP address in the default security group
on Amazon EC2. If you don’t do this, Jenkins will time out when it tries to start up a new slave node.
Once you have prepared your image, you will be able to use it for your EC2 configuration.
Figure 11.20. Creating a new Amazon EC2 image
Now Jenkins will automatically create a new EC2 instance using this image when it needs to, and delete
(or “terminate,” in Amazon terms) the instance once it is no longer needed. Alternatively, you can
bring a new EC2 slave online manually from the Nodes screen using the Provision via EC2 button (see
Figure 11.21, “Bringing an Amazon EC2 slave online manually”). This is a useful way to test your
configuration.
Figure 11.21. Bringing an Amazon EC2 slave online manually
11.7. Using the CloudBees DEV@cloud Service
Another option you might consider is running your Jenkins instance using a dedicated cloud-based
Jenkins architecture, such as the DEV@cloud service offered by CloudBees. CloudBees provides
Jenkins as a service as well as various development services (like Sonar) around Jenkins. Using a
dedicated Jenkins-specific service, there is no need to install (or manage) Jenkins masters or slaves on
your machines. A master instance is automatically configured for you, and when you give a job to be
built, CloudBees provisions a slave for you and takes it back when the job is done.
322
How does this approach compare with the Amazon EC2-based architecture we discussed in the previous
section? The main advantage of this approach is that there is much less work involved in managing
your CI architecture hardware. Using the Amazon EC2 infrastructure means you don't need to worry
about hardware, but you still need to configure and manage your server images yourself. The CloudBees
DEV@cloud architecture is more of a high-level, CI-centric service, which provides not only a Jenkins
server but also other related tools such as SVN or Git repositories, user management, and Sonar. In
addition, the pricing model (pay by the minute) is arguably better suited to a cloud-based CI architecture
than the pay-by-the-hour approach used by Amazon.
Amazon EC2-based services are often, though not always, used in a “hybrid cloud” environment where
you are offloading your jobs to the cloud, but a bulk of your builds remain in-house. The CloudBees
DEV@cloud service is a public cloud solution where the whole build is happening on the cloud (though
CloudBees does also offer a similar solution running on a private cloud).
Creating a CloudBees DEV@cloud account is straightforward, and you can use a free one to experiment
with the service (note that the free CloudBees service only has a limited set of plugins available; you
will need to sign up for the professional version to use the full plugin range). To signup for CloudBees,
go to the signup page1. You will need to enter some relevant information such as a user name, email
information, and an account name. Once signed up, you will have access to both DEV@cloud and
RUN@cloud (essentially the entire CloudBees platform) services.
At this point, you will have to subscribe to the DEV@cloud service. For our purposes, you can get
away with simply choosing the “free” option. You will have to wait for a few minutes as CloudBees
provisions a Jenkins master for you. The next step is to validate your account (this helps CloudBees
prevent dummy accounts from running spurious jobs on the service). Click on the validation link, and
enter your phone number. An automated incoming phone call will give your pin; enter the pin on the
form. Once this is done, you can start running builds.
Your first port of call when you connect will be the management console (called GrandCentral). Click
on the “Take me to Jenkins” button to go to your brand new Jenkins master instance.
From here, your interaction with DEV@cloud platform is exactly like in a standalone Jenkins. When you
can create a new build job, just point to your existing source code repository and hit build. DEV@cloud
will provision a slave for you and kick off a build (it may take a minute or two for the slave to be
provisioned).
11.8. Conclusion
In Continuous Integration, distributed builds are the key to a truly scalable architecture. Whether you
need to be able to add extra build capacity at the drop of a hat, or your build patterns are subject to
periodic spikes in demand, a distributed build architecture is an excellent way to absorb extra load.
Distributed builds are also a great way to delegate specialized tasks, such as OS-specific web testing,
to certain dedicated machines.
1 https://grandcentral.cloudbees.com/account/signup
323
Once you start down the path of distributed builds, cloud-based distributed build farms are a very logical
extension. Putting your build servers on the cloud makes it easier and more convenient to scale your
build infrastructure when required, as much as is required.
324
Chapter 12. Automated Deployment
and Continuous Delivery
12.1. Introduction
Continuous Integration should not stop once your application compiles correctly. Nor should it stop once
you can run a set of automated tests or automatically check and audit the code for potential quality issues.
The next logical step, once you have achieved all of these, is to extend your build automation process
to the deployment phase. This practice is globally known as Automated Deployment or Continuous
Deployment.
In its most advanced form, Continuous Deployment is the process whereby any code change, subject
to automated tests and other appropriate verifications, is immediately deployed into production. The
aim is to reduce cycle time and reduce the time and effort involved in the deployment process. This in
turn helps development teams reduce the time taken to deliver individual features or bug fixes, and as
a consequence significantly increase their throughput. Reducing or eliminating the periods of intense
activity leading up to a traditional release and deployment also frees up time and resources for process
improvement and innovation. This approach is comparable to the philosophy of continual improvement
promoted by lean processes such as Kanban.
Systematically deploying the latest code into production is not always suitable, however, no matter how
good your automated tests are. Many organizations are not well prepared for new versions appearing
unannounced every week; users might need to be trained, products may need to be marketed, and so
forth. A more conservative variation on this theme, often seen in larger organizations, is to have the entire