### 4.3. Aggregate Server Flow Model

Our initial experiment aimed to determine the extent to which FTP, SSH, Telnet, SMTP, and HTTP traffic can be distinguished using a decision tree classifier. We used data from the first week of the Lincoln Labs dataset to build our training dataset. This set was created by randomly selecting fifty server flows for each of the five protocols. Each server flow consists of packets sent from a server to a specific client host/port. The largest flow contained approximately 37,000 packets, while the smallest flow had only 5 packets. The total dataset comprised about 290,000 packets. We refer to this as an aggregate model because the flows were collected from multiple servers.

The fact that this data is certified as attack-free allowed us to rely on port numbers as indicators of traffic type. We labeled each flow in the training set based on the server port. Each server flow was then used to generate data observations based on our feature set, resulting in a dataset of approximately 290,000 labeled observations. We repeated this process for seven different packet window sizes, where the window size is an upper bound on the number of packets used to compute means and percentages. If a flow contains fewer packets than the window size, all available packets are used to calculate the observations.

Each of the seven training sets was used to build a decision tree using C5.0. Test sets were constructed in a similar manner, with fifty server flows from each protocol randomly selected from the third week of the Lincoln Labs data. These were then passed to our feature extraction algorithm using the same seven window sizes.

Before discussing how a tree classifies a flow, we provide an example of a portion of a decision tree generated by C5.0 (Figure 2). In this example, the root node tests the percentage of packets in the packet window with the FIN flag set (tcpPerFIN). If this percentage exceeds 1%, a test is made on the percentage of packets with the PSH flag set (tcpPerPSH). If this value is less than or equal to 40%, the observation is classified as "www," indicating HTTP traffic. The numbers in parentheses indicate the number of training observations classified with that leaf node. Other tests involve the mean inter-arrival time (meanIAT) and mean packet length (meanIPTLen).

During testing, the class label for a given flow was determined by summing the confidence values for each observation in the flow. The class with the highest total confidence was assigned to the flow. The classification results are shown in Table 1, which reports the percentage of correctly classified server flows out of the fifty flows for each protocol. The classification accuracy ranges from 82% to 100%.

In general, SMTP server flows had lower classification accuracy compared to other protocols. Upon closer examination, we found that misclassified flows were generally 2-4 times longer than correctly classified flows. Longer SMTP server flows represent extended periods of interaction, leading to more observations classified as Telnet or FTP. In these cases, our feature set is not sufficient to distinguish between the behaviors of these flows.

Using a smaller window size is preferable as it reduces the time required to detect abnormal service behavior. For SSH, a large packet window size (1000) negatively impacts classification accuracy. For FTP, SSH, and Telnet, a window size as small as ten packets achieves 100% classification accuracy.

To estimate real-time classification performance, we calculated the average time C5.0 took to classify an entire flow, which was 70 milliseconds. Training is performed offline, so computation time is less critical, but note that the average time to create each decision tree was 22 seconds. Finally, we need to address the storage requirements for maintaining a window of values to compute the features. We can approximate the storage by retaining only the mean for each feature and using the following update rule for each new packet:

\[
\text{New Mean} = \frac{(n \times \text{Old Mean}) + \text{New Value}}{n+1}
\]

In future work, we will investigate whether this technique significantly degrades performance.

From our experimental results, we conclude that the behavior of server flows for the five protocols can be differentiated using a decision tree classifier built on aggregate flows. We will later discuss how this method can complement an intrusion detection system.

### 4.4. Host-Specific Models

Our second experiment explored whether creating models for specific hosts provides better performance than the aggregate model. There are three advantages to using host-specific models:

1. **Behavior Monitoring**: By creating models for individual server flows, we can monitor these flows for changes in behavior.
2. **Implementation Subtleties**: A host-specific model can capture the implementation subtleties of a particular service running on a host, which is missing in the aggregate model.
3. **Balanced Representation**: The training examples in an aggregate model may be dominated by the server generating the most traffic, potentially diluting examples from other servers. Host-specific models solve this problem.

We identified a set of hosts in the Lincoln Labs data that each ran three or more server protocols. Training data for each host was collected by randomly selecting server flows from the first week for each protocol running on these hosts. The number of flows used in each model was chosen such that each protocol was represented by the same number of flows. Table 2 lists the number of training and test flows per host.

Based on our results with the aggregate models, we chose a packet window size of 100 for generating observations. This selection was driven by the fact that SMTP accuracy was greatest with this window size, and other protocol classification accuracies were between 96% and 100%. We then trained a decision tree for each host to differentiate the server flows coming from that host. Test data was collected from the third week in the same manner as the training data.

The results in Table 3 indicate that, in general, host-specific models achieve approximately the same classification accuracy as the aggregate models. One difference observed is that classification accuracy varies by protocol. For example, the classification accuracy of Telnet flows for host 172.16.112.50 is 84%, whereas the classification of Telnet flows in the aggregate models averaged 96.2%. Examination of the misclassified Telnet flows revealed large time gaps between packets, indicating lapses in user activity. In our framework, a single large gap can alter the mean inter-arrival time of packets, leading to misclassification. We refer to this as the "Water Cooler Effect" and are investigating the sensitivity of our classifiers to this effect. One possible solution is to subdivide flows based on a time gap threshold and use the interactive sub-flows to build classifiers.

### 4.5. Models from Real Network Traffic

In this section, we present experiments with real network traffic. We collected a number of server flows using the described protocols and augmented this set to include flows from hosts acting as Kazaa servers. Kazaa is a peer-to-peer file-sharing system that is growing in popularity. Peer-to-peer network traffic was not part of the Lincoln Labs dataset.

Our goal was to determine if there was a significant difference in classification accuracy when using synthetic versus real traffic. We observed classification accuracies ranging from 85% to 100% for both the aggregate and host models. The peer-to-peer traffic was classified correctly for 100% of the unseen flows. This is an interesting result because Kazaa flows carry a user-defined port label. Thus, we can correctly classify peer-to-peer flows behaviorally without using the port number. These results indicate that our classification method is effective for real network traffic. The range of accuracies matches those observed with the synthetic data, suggesting no appreciable difference in per-flow behavior between synthetic and real network traffic.

### 5. Classification for Intrusion and Misuse Detection

The two types of classification models presented here offer new functionality in the context of intrusion and misuse detection. Aggregate models aim to classify a flow based on the general behavior of many flows of a given type. Host-specific models, on the other hand, focus on the unique characteristics of flows from a specific host. Both approaches can be used to enhance the effectiveness of intrusion detection systems.