ing this criteria, the training data is recursively split such
that the gain ratio is maximized at each node of the tree.
This procedure continues until each leaf node contains only
examples of a single class or no gain in information is given
by further testing. The result is often a very large, complex
tree that overÔ¨Åts the training data. If the training data con-
tains errors, then overÔ¨Åtting the tree to the data in this man-
ner can lead to poor performance on unseen data. There-
fore, the tree must be pruned back to reduce classiÔ¨Åcation
errors when data outside of the training set are to be clas-
siÔ¨Åed. To address this problem C5.0 uses conÔ¨Ådence-based
pruning, and details can be found in [27].
When using the decision tree to classify unseen exam-
ples, C5.0 supplies both a class label and a conÔ¨Ådence value
for its prediction. The conÔ¨Ådence value is a decimal number
ranging from zero to one ‚Äì one meaning the highest conÔ¨Å-
dence ‚Äì and it is given for each instance.
4.3. Aggregate Server Flow Model
Our Ô¨Årst experiment was designed to determine the ex-
tent to which FTP, SSH, Telnet, SMTP, and HTTP trafÔ¨Åc
can be differentiated using a decision tree classiÔ¨Åer. We
used the data from week one of the Lincoln Labs data to
tcpPerPSH > 0.4:
:...tcpPerPSH  0.01:
:...tcpPerPSH  546773.2:
tcpPerPSH > 0.797619: ftp (38)
tcpPerSYN > 0.03225806:
:...meanipTLen > 73.33: ftp (21)
:...tcpPerSYN  0.7945206: smtp (8)
Figure 2. Portion of a decision tree generated
by C5.0.
build our training dataset. The set was created by Ô¨Årst ran-
domly selecting Ô¨Åfty server Ô¨Çows for each of the Ô¨Åve proto-
cols. Each server Ô¨Çow consists of the packets from a server
to a particular client host/port. The largest Ô¨Çow contained
roughly 37,000 packets, and the smallest Ô¨Çow contained 5
packets. The 250 Ô¨Çows represented a total of approximately
290,000 packets. We refer to this as an aggregate model
because the collection of Ô¨Çows came from many different
servers.
The fact that this data is certiÔ¨Åed as attack-free meant
that we could have conÔ¨Ådence in the port numbers as indica-
tive of the type of trafÔ¨Åc. We used the server port to label
each of Ô¨Çows in the training set. Each server Ô¨Çow was then
used to generate data observations based on our feature set.
The result is a data set consisting of approximately 290,000
thousand labeled observations. We repeated this process for
each of seven packet window sizes. The window size is an
upper bound on the number of packets used to compute the
means and percentages. If an individual Ô¨Çow contains fewer
packets than the packet window size, the number of avail-
able packets is used to calculate each observation.
Each of the seven training sets was then used to build
a decision tree using C5.0. We constructed test sets in the
same manner ‚Äì Ô¨Åfty server Ô¨Çows from each protocol were
randomly selected from week three of the Lincoln Labs
data. These were then passed to our feature extraction al-
gorithm using the same seven window sizes.
Before describing how a tree is used to classify a Ô¨Çow,
we give an example of a portion of a decision tree generated
by C5.0 in Figure 2. In this example, the root node tests the
percentage of packets in the packet window with the FIN
Ô¨Çag set (tcpPerFIN). If this percentage exceeds 1%, a test
is made on the percentage of packets with the PSH Ô¨Çag set
(tcpPerPSH). If this value is less than or equal to 40%, the
observation is classiÔ¨Åed as ‚Äúwww‚Äù, indicating HTTP trafÔ¨Åc.
The numbers in parenthesis indicate the number of train-
Proceedings of the 19th Annual Computer Security Applications Conference (ACSAC 2003) 
1063-9527/03 $17.00 ¬© 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:48:39 UTC from IEEE Xplore.  Restrictions apply. 
Window Size
1000
500
200
100
50
20
10
SMTP WWW
SSH
FTP
100%
82%
100% 88%
100%
86%
100% 96%
98%
84%
98%
96%
86%
100%
100% 96%
100%
82%
98%
96%
98%
100% 98%
82%
100% 100% 100% 82%
98%
Telnet
94%
94%
96%
96%
96%
98%
Table 1. ClassiÔ¨Åcation accuracy of the aggregate model decision trees on unseen individual server
Ô¨Çows. Each value represents the percentage of correctly classiÔ¨Åed Ô¨Çows out of the Ô¨Åfty Ô¨Çows for
each protocol
ing observations classiÔ¨Åed with this leaf node. Other tests
can be seen involving the mean inter-arrival time (meanIAT)
and mean packet length (meanIPTLen).
During testing, the class label for a given Ô¨Çow was cal-
culated by summing the conÔ¨Ådence values for each obser-
vation in the Ô¨Çow. The class with the highest total conÔ¨Å-
dence was assigned to that Ô¨Çow. The classiÔ¨Åcation results
are shown in Table 1. For each of seven window sizes, we
report the percentage of correctly classiÔ¨Åed server Ô¨Çows out
of the set of Ô¨Åfty Ô¨Çows for each protocol. As can be seen
in the table, the classiÔ¨Åcation accuracy ranges from 82% to
100%.
In general, the classiÔ¨Åcation accuracy was lower for
SMTP server Ô¨Çows than for other protocols. We examined
the misclassiÔ¨Åed Ô¨Çows in more detail and discovered that
these Ô¨Çows were generally 2-4 times longer than correctly
classiÔ¨Åed Ô¨Çows. Longer SMTP server Ô¨Çows represented
longer periods of interaction, and thus contain increasing
numbers of observations classiÔ¨Åed as Telnet or FTP. In these
few cases, our feature set is not adequate for discriminating
between the behaviors of these Ô¨Çows.
It is more desirable to use a smaller window size be-
cause this decreases the time to detect that a service is be-
having abnormally. Indeed for SSH we see that too large a
packet window size (1000) hurts classiÔ¨Åcation accuracy. For
FTP, SSH and Telnet, a window size as small as ten pack-
ets achieves 100% classiÔ¨Åcation accuracy.
Because the proposed method would be used to monitor
trafÔ¨Åc in real time, we did a rough calculation of classiÔ¨Åca-
tion time. The average length of time used by C5.0 to clas-
sify an entire Ô¨Çow was 70mS.1 Training is done ofÔ¨Çine so
computation time is of lesser importance, but note that the
average length of time used by C5.0 to create each decision
tree was 22 seconds. Finally, we need to address the stor-
1
The hardware platform used for building the decision trees and classi-
fying observations was a 500Mhz Dual Pentium III PC with 772MB
of RAM running Red Hat Linux (kernel version 2.4.18).
age requirements for maintaining a window of  values to
compute the value of each of the features. We can approx-
imate the value created by storing all  values by retaining
 and using the follow-
only the mean for each feature, 
ing update rule for each new packet:
In future work we will investigate whether this technique
signiÔ¨Åcantly degrades performance.
We conclude from our experimental results that the be-
havior of server Ô¨Çows for the Ô¨Åve protocols can be differ-
entiated using a decision tree classiÔ¨Åer built on aggregate
Ô¨Çows. We will later discuss how this method can be used to
compliment an intrusion detection system.
4.4. Host-SpeciÔ¨Åc Models
Our second experiment addresses whether creating mod-
els for speciÔ¨Åc hosts provides better performance than the
aggregate model. There are three advantages to using host-
speciÔ¨Åc models:
1. By creating models for individual server Ô¨Çows, we can
monitor these Ô¨Çows for changes in behavior.
2. A host-speciÔ¨Åc model can capture the implementation
subtleties of a particular service running on a host. This
resolution is missing in the aggregate model consisting
of many server Ô¨Çows.
3. The training examples in an aggregate model will be
dominated by the server generating the most trafÔ¨Åc.
This may dilute examples from other servers. The host-
speciÔ¨Åc model solves this problem.
We Ô¨Årst identiÔ¨Åed a set of hosts in the Lincoln Labs data
that each ran three or more server protocols. Training data
for each host was collected by randomly selecting server
Ô¨Çows from week one for each of the protocols running on
Proceedings of the 19th Annual Computer Security Applications Conference (ACSAC 2003) 
1063-9527/03 $17.00 ¬© 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:48:39 UTC from IEEE Xplore.  Restrictions apply. 












Host
172.16.112.100
172.16.112.50
172.16.113.50
172.16.114.50
197.218.177.69
SSH
Telnet
FTP
95%
100% 90%
92% 100% 84% 100%
100%
100% 100%
100% 95% 100% 95%
100% 100%
100%
‚Äì
‚Äì
‚Äì
SMTP WWW
100%
‚Äì
‚Äì
95%
‚Äì
Table 3. ClassiÔ¨Åcation accuracy of host model decision trees on unseen server Ô¨Çows. Each row re-
ports the host address and the percentage of correctly classiÔ¨Åed Ô¨Çows for each protocol. Fields with
a ‚Äú‚Äì‚Äù indicate there was no trafÔ¨Åc of this protocol type for this host.
Host
172.16.112.100
172.16.112.50
172.16.113.50
172.16.114.50
197.218.177.69
Training Flows Test Flows
20
25
23
20
35
20
30
35
10
25
Table 2. Number of Ô¨Çows used for each pro-
tocol in training and test sets for each host
model
these hosts. The number of Ô¨Çows used in each model was
chosen such that each protocol was represented by the same
number of Ô¨Çows. Table 2 lists the number of training and
test Ô¨Çows per host.
Based on our results using the aggregate models, we
chose a packet window size of 100 for generating observa-
tions. The selection was driven by the fact that SMTP accu-
racy was greatest using this window size with the aggregate
models, and other protocol classiÔ¨Åcations accuracies were
between 96% and 100%. We then trained a decision tree for
each host that could be used to differentiate the server Ô¨Çows
coming from that host. Test data was collected from week
three in the same manner as the training data.
The results in Table 3 indicate that, in general, the host
speciÔ¨Åc models achieve approximately the same classiÔ¨Åca-
tion accuracy as the aggregate models. One difference ob-
served is that classiÔ¨Åcation accuracy varies by protocol. For
example, the classiÔ¨Åcation accuracy of Telnet Ô¨Çows for host
172.16.112.50 is 84% whereas the classiÔ¨Åcation of Telnet
Ô¨Çows in the aggregate models averaged 96.2%. Examina-
tion of the packets in the misclassiÔ¨Åed Telnet Ô¨Çows revealed
an interesting phenomenon. We often observed large time
gaps between packets. The time gaps indicate lapses in user
activity where the Telnet server is not echoing characters
or supplying responses to commands. In our framework, a
single large gap can radically alter the values for the mean
inter-arrival time of packets, thus resulting in misclassiÔ¨Åca-
tion of the subsequent observations. We refer to this as the
Water Cooler Effect ‚Äì the user temporarily leaves the in-
teractive session, then resumes a short while later. We are
investigating the sensitivity of our classiÔ¨Åers to this effect.
One possible solution would be to subdivide Ô¨Çows based on
some time gap threshold and use the interactive sub-Ô¨Çows
to build our classiÔ¨Åers.
4.5. Models from Real Network TrafÔ¨Åc
In this section we present experiments with real network
trafÔ¨Åc. We collected a number of server Ô¨Çows using the pro-
tocols described. We augmented this set to include Ô¨Çows
from hosts acting as Kazaa servers. Kazaa [20, 31] is a
peer-to-peer Ô¨Åle sharing system that is growing in popular-
ity [8, 33]. Peer-to-peer network trafÔ¨Åc was not part of the
Lincoln Labs dataset.
Our goal was to determine if there was a signiÔ¨Åcant dif-
ference in classiÔ¨Åcation accuracy when using synthetic ver-
sus real trafÔ¨Åc. We observed classiÔ¨Åcation accuracies by
protocol ranging from 85% to 100% for both the aggregate
and host models. The peer-to-peer trafÔ¨Åc was classiÔ¨Åed cor-
rectly for 100% of the unseen Ô¨Çows. This is an especially in-
teresting result because Kazaa Ô¨Çows carry a port label that is
user-deÔ¨Åned. Thus, we are able to correctly classify peer-to-
peer Ô¨Çows behaviorally ‚Äì without the use of the port num-
ber. These results indicate that our classiÔ¨Åcation method is
effective for real network trafÔ¨Åc. The range of accuracies
match those observed with the synthetic data. Thus, we can
identify no appreciable difference in the per-Ô¨Çow behavior
in the synthetic Lincoln Labs data versus those in real net-
work trafÔ¨Åc.
5. ClassiÔ¨Åcation for Intrusion and Misuse De-
tection
The two types of classiÔ¨Åcation models presented here
give rise to new functionality in the context of intrusion and
misuse detection. Aggregate models try to classify a Ô¨Çow
based on the general behavior of many Ô¨Çows of a given type.
Proceedings of the 19th Annual Computer Security Applications Conference (ACSAC 2003) 
1063-9527/03 $17.00 ¬© 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:48:39 UTC from IEEE Xplore.  Restrictions apply. 
Host
Classifier
Host
Classifier
Aggregate