I want to go through Tensorflow's CUDA kernels and optimize them. It seems
like all the CUDA kernels are defined in `tensorflow/core/kernels` and
`tensorflow/contrib` with some helper functions in `tensorflow/core/util`.
  1. Is it okay for me to optimize kernels in tensorflow/core/kernels and send pull request?
  2. now that tensorflow is moving to 2.0, are the CUDA kernels going to also change so dramatically that any improvements to them now could be rendered useless?
  3. what are some conventions or practices when writing CUDA kernels? I have read the `contrib.md`, but there was nothing special on CUDA kernels ( ex. " Do not change kernel launch configurations " or " must support devices of compute capability >= 3.0 " )
  4. must I use `CudaGridRangeX` over regular for loops?
  5. Is there way to test and profile each .cu file individually, instead of having to rebuild the entire tensorflow or even having to test all kernels?