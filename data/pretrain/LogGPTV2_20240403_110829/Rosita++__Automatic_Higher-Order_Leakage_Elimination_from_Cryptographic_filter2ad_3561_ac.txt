identifying a components that show evidence of leakage. Based on
the root cause, Rosita applies rewrite rules, modifying the cipher
code with the aim of eliminating the leakage. The process repeats
until either no more rules can apply or no leakage is evident.
Similarly, Gao et al. [26] have demonstrated an Instruction Set
Extension (ISE) to RISC-V Instruction Set Architecture (ISA). The
ISE guarantees that internal states that cause leakage are cleared
acting as a barrier instruction when used in sensitive programs.
2.6 Testing for Statistical Equivalence of
Distributions
In Section 3.4 we use a statistical equivalence test during root-cause
analysis to determine which parts of the code contribute to the
leakage; In this section, we describe the statistical method we use
for equivalence testing.
The aim of statistical equivalence tests is to determine how prob-
able it is that two sampled distributions originate from the same
population. Observe that this is the opposite of what Welchâ€™s ğ‘¡-test
offers. The null hypothesis of an equivalence test is that the two
distributions are different and we expect to reject it in favour of
the alternative hypothesis which states that the distributions are
the same with a given significance level. One such equivalence test
is the Two One Sided ğ‘¡-test (TOST) [50, 58]. As the name indicates,
TOST uses two one-sided ğ‘¡-tests to test whether the two distribu-
tions are equivalent. TOST is a parameterised test that requires
a lower bound and upper bound for the mean difference of the
two distributions under test as parameters. Two individual ğ‘¡-tests
determine whether the mean difference is lower than the upper
bound and whether it is higher than the lower bound with a given
level of significance (ğ›¼). Passing both ğ‘¡-tests indicates that the mean
difference is between the lower and upper bounds with the given
significance level.
However, TOST in its original form has a limitation when it
comes to the evaluation of the mean differences of two distributions:
when these mean differences are close or equal to the boundary
values, the TOST concludes that the distributions are not equivalent.
This happens due to the ğ‘¡-value of the individual ğ‘¡-tests resulting in
values closer to 0 when the mean differences are close to boundary
values. In the paradigm where TOST is commonly used (e.g. in drug
test trials), the boundaries are regarded as the worst values that
the mean difference can get. However, in equivalence testing for
engineering, we expect a test which accepts boundary values and
also the values which are closer to the boundaries.
To mitigate this limitation, Pardo [50] proposed the following
formulas that compute new boundaries (ğ‘‹ ğ» and ğ‘‹ ğ¿) given a target
mean difference (ğœ‡), where ğ‘  and ğ‘› are standard deviation and
cardinality of the mean differences distribution. ğ‘¡ğ›¼ is the one sided
ğ‘¡-test value at a significance value of ğ›¼.
Selecting a critical region with ğ›¼ significance level such that ğ‘‹ ğ»
is higher than ğœ‡ is given as
ğ‘‹ ğ» = ğœ‡ + ğ‘¡ğ›¼
ğ‘ âˆš
ğ‘›
and such that ğ‘‹ ğ¿ is lower than ğœ‡ is given as
ğ‘‹ ğ¿ = ğœ‡ âˆ’ ğ‘¡ğ›¼
ğ‘ âˆš
ğ‘›
.
(2)
(3)
Using the confidence interval of ğ‘‹ ğ¿ and ğ‘‹ ğ» instead of having
arbitrarily defined values for mean difference boundaries overcomes
the above-stated limitation.
3 ROSITA++ DESIGN
Past solutions that aim to automate leakage detection [21, 42, 49, 59,
65] and correction [61] focus on first-order leakage. As the security
of cipher implementations can be increased by employing more
shares in their masking schemes, there is a need for emulators
and countermeasures that can work with multivariate leakage. In
this section we describe how Rosita++, our solution for this need,
works. We outline the main challenges in performing higher-order
analysis and proceed to describe our approaches for addressing
these challenges.
3.1 Challenges for Higher-Order Analysis
The core extension required for Rosita++ to support higher-order
leakage detection and mitigation is support for multivariate analysis.
Specifically, instead of looking for instructions that show indication
of leakage, we need to look for combinations of instructions that
together show indication of leakage.
Schneider and Moradi [57] suggest a methodology for perform-
ing multivariate analysis. Their approach is to generate artificial,
multivariate traces from the original univariate traces. For that, the
original traces are first preprocessed by calculating the average
value for each sample point and subtracting the average from the
corresponding point in each trace. As Equation 1 shows, Each point
in an artificial trace represents a tuple of points in the original trace,
Session 3A: Side Channel CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea689where the value associated with the artificial point is the product
of the values for the corresponding points in the original trace.
Our approach for performing higher-order analysis is to replace
the use of TVLA in Rosita with the Schneider-Moradi methodology.
However, while seemingly straightforward, the approach raises
significant challenges.
(cid:1) samples.
ğ‘‘
Challenge C1: Statistical confidence with multivariate traces
The artificial ğ‘‘-variate traces have an artificial sample for each
combination of ğ‘‘ samples in the original traces. Consequently, the
length of the multivariate traces grows exponentially with the
length of the original traces. For traces of length ğ‘›, the multivariate
traces have a length of(cid:0)ğ‘›
The de-facto standard statistical test used in TVLA is to reject
the null hypothesis, i.e. report leakage, when the absolute value of
the ğ‘¡-test is above a threshold of 4.5, achieving a significance level
of 0.00001. This test, however, fails to account for the multiple com-
parisons performed in TVLA, where a statistical test is performed
independently on each sample point. For a small number of points,
the effect of multiple comparisons is negligible. When the trace
length increases, multiple comparisons result in false positives,
showing leakage where no leakage exists.
Challenge C2: Increased data size
Another issue with multivariate analysis is the increase in the vol-
ume of data that needs to be processed compared with first-order
univariate analysis. Three factors contribute to this increase. First,
due to the effects of noise, the number of traces required for statis-
tical analysis grows exponentially with the order of analysis [13].
Secondly, as discussed, the artificial multivariate traces are sig-
nificantly longer than the original traces. Thirdly, to increase the
statistical confidence while handling Challenge C1 without missing
leakage we need to increase the number of traces we process.
Because Rosita++ repeatedly evaluates implementations, there
is a need for efficient methods for handling the increased amount
of data with minimal impact on analysis time.
Challenge C3: Multivariate root-cause analysis
The third challenge we face relates to performing the root-cause
analysis. Rosita performs the analysis using a ğ‘¡-test on each of the
Elmo* model components. Such an approach can detect univari-
ate leakage. However, detecting multivariate leakage necessitates
evaluating combinations of components. A brute-force approach
that evaluates a ğ‘¡-test statistics on every combination of compo-
nents is computationally expensive, particularly considering the
increased number of traces, as described in Challenge C2. Thus,
new techniques for root-cause analysis are required.
We now discuss how Rosita++ addresses these challenges.
3.2 Achieving Statistical Confidence
As discussed, Challenge C1 is that, due to the exponential increase
in the number of sample point per trace, the ğ‘¡-test threshold of 4.5
is no longer appropriate. This mostly affects the traces collected
from the physical experiment where we collect longer traces (10
times more samples) to reduce the effects of noise. To demonstrate
the false positives we collect 500,000 bivariate traces of a three-
share implementation of Xoodoo (further described in Section 4.3)
running on a STM32F030 Discovery evaluation board, where all
inputs are drawn uniformly at random. The experiment setup we
used is described in Section 4.1. We then split these arbitrarily into
two populations, and perform a bivariate ğ‘¡-test analysis, comparing
these populations with a threshold of 4.5. As Figure 1 shows, despite
the populations being sampled from the same distribution, several
false positives are present.
Figure 1: A ğ‘¡-test threshold value of 4.5 for a bivariate analysis
with 1000 samples with all inputs being random.
For engineers, these false positives are typically of low impact.
Experienced engineers can typically identify false positives, e.g. by
observing the context. Alternatively, repeating the test can confirm
true positives.
Automatic tools, such as Rosita++, do not have the experience
or the insight, and must rely on statistical tools for handling false
positives. If Rosita++ is used with long code segments these false
positives will also be present in its leakage analysis. Therefore, in
Rosita++, we adopt the approach of Ding et al. [22], who propose
increasing the threshold to reduce the probability of false positives.
Specifically, Ding et al. provide a formula to calculate the threshold
given the number of samples and a desired significance level ğ›¼.
We apply the formula to the length of the bivariate trace aiming
for a significance level of 0.00001. This ensures that the probabil-
ity of a false positive error is less than .001%, which we consider
negligible. For the traces in Figure 1 we would use a threshold of
6.71, which is clearly above the largest peak in the figure. Hence, at
this threshold, the analysis does not indicate any leakage, which is
expected considering that the two populations are drawn from the
same distribution.
3.3 Handling Large Datasets
As mentioned, several aspects of multivariate analysis result in a sig-
nificant increase in the size of data that Rosita++ needs to process.
First, for given mean and variance, the Welch ğ‘¡-value grows linearly
with the square root of the size of the population. Consequently,
when increasing the threshold we need a quadratic increase in the
number of traces to achieve the same detection sensitivity. Second,
the length of the multivariate artificial traces is several orders of
magnitude longer than the original univariate traces. Third, due
to the effects of noise, detecting higher-order leakage is inherently
harder then detecting first-order leakage. The combined effect of
 0 1 2 3 4 5 0 100000 200000 300000 400000 500000t-valueCombined samplesSession 3A: Side Channel CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea690these changes is that the amount of data that Rosita++ needs to
process is several orders of magnitude larger than that of Rosita.
When evaluating the final version of code produced by Rosita++
on real hardware, the same issue gets even more apparent because
we use longer traces as mentioned in Section 3.2.
While we are aware of works that have performed analyses at
scales similar and even larger than our work [16, 17], we could not
find public tools that perform such analyses, or even performance
figures for the analysis. Free tools such as Jlsca1, Scared2, SCALib3
seem to only offer limited capabilities. To address this challenge
we developed analysis tools from the ground up. Our analysis tools
avoid the overhead of storing the artificial traces (i.e. multivariate
combinations) by calculating them on the fly. The tools are multi-
threaded, allowing a significant speed-up, and the data is divided
point-wise between the threads, so that each thread only accesses
a limited subset of the original tracesâ€™ samples.
We acknowledge that the approach is fairly straightforward, but
we believe that the contribution is important for practical future
research into bivariate analysis.
3.4 Multivariate Root-Cause Analysis
The third challenge for Rosita++ is performing root-cause anal-
ysis on multivariate traces. The Elmo* linear regression model
consists of 28 term components, each modelling a different micro-
architectural effect. When Rosita performs univariate root-cause
analysis, it calculates the Welch ğ‘¡-value for each component sep-
arately, where the leaky components are identified by observing
significant ğ‘¡-values.
While this approach works well for univariate leakage detection,
adapting it to multivariate leakage is not trivial. The main reason is
that, in multivariate analysis, there is no single cause for leakage.
As shown by Equation 1, a multivariate sample point is a com-
bination of many samples in the original trace. In Elmo*, each of
the original samples is calculated from the sum of 28 model compo-
nents. Searching for a combination of ğ‘‘ samples using a method
similar to the one used for univariate evaluation would require
evaluating 28ğ‘‘ combinations. Even for the bivariate case of ğ‘‘ = 2,
the process is very inefficient with the large number of traces that
need to be processed due to increase of order [13].
To avoid searching the whole space of pairs of model compo-
nents, Rosita++ uses two new methods for finding the components
that contribute to the leak. The component elimination method tests
whether removing a model component removes the leakage. While
efficient, this approach may sometimes fail. In the case of such a fail-
ure, Rosita++ reverts to a Monte-Carlo method, which tests random
combinations of components looking for evidence of component
leakage. We refrain from using the Monte-Carlo method by default
due to its inefficiency and the instability inherent in a randomised
process. We now describe these two methods in detail.
Component Elimination The basic idea behind the component
elimination method is to identify components that contribute to the
leakage by removing one component at a time from the multivariate
sample combination function (which is shown in Equation 1); we
1https://github.com/Riscure/Jlsca
2https://gitlab.com/eshard/scared
3https://github.com/simple-crypto/SCALib
then evaluate the combination with removed component for ab-
sence of leakage. If the removal of a component leads to the absence
of leakage at a previously leaky point, this means that the removed
component contributed to the leakage. When this process ends,
Rosita++ has a set of components that contribute to the leakage.
Rosita++ can now apply fixes using the approach of Rosita.
More specifically, component elimination consists of the follow-
ing steps. First, each component value of Elmo* is recorded with
the component index, sample index, and the trace index. There
exists 28 different components in Elmo*, the sample index is the
array index of the instruction when the emulated code segment is
unrolled into individual instructions. The trace index is a number
identifying each run of the fixed vs. random test. All of these values
are stored in a 3D matrix that is denoted by ğ‘³.
Second, the multivariate leaky points for the implementation
are found by running the ğ‘¡-test on the final power value of Elmo*