of the medical record to the specialist is non-redundant.
However, had the X-ray revealed to the physician the diag-
nosis without needing to send it to a specialist, the sequence
of actions that results from removing the transmission from
the original sequence would still result in a diagnosis. Thus,
the transmission would be redundant.
Quantitative Purposes: Above we implicitly presumed
that the diagnosis from either the specialist or an MRI had
equal quality. This need not be the case. Indeed, many pur-
poses are actually fulﬁlled to varying degrees. For example,
the purpose of marketing is never completely achieved since
there is always more marketing to do. Thus, we model a
purpose by assigning to each state-action pair a number
that describes how well
that action fulﬁlls that purpose
when performed in that state. We require that the physician
selects the test that maximizes the quality of the diagnosis
as determined by the total purpose score accumulated over
all his actions.
We must adjust our notion of non-redundancy accordingly.
An action is non-redundant if removing that action from the
sequence would result in the purpose being satisﬁed less.
Now, even if the physician can make a diagnosis himself,
sending the record to a specialist would be non-redundant
if getting a second opinion improves the quality of the
diagnosis.
Probabilistic Systems: The success of many medical
tests and procedures is probabilistic. For example, with
some probability the physician’s test may fail to reach a
diagnosis. The physician would still have transmitted the
medical record for the purpose of diagnosis even if the test
failed to reach one. This possibility affects our semantics of
purpose: now an action may be for a purpose even if that
178
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:47:51 UTC from IEEE Xplore.  Restrictions apply. 
purpose is never achieved.
To account for such probabilistic events, we model the
outcome of the physician’s actions as probabilistic. For an
action to be for a purpose, we require that there be a non-
zero probability of the purpose being achieved and that
the physician attempts to maximize the expected reward. In
essence, we require that the physician attempts to achieve a
diagnosis. Thus, the auditee’s plan determines the purposes
behind his actions.
III. PLANNING FOR A PURPOSE
Now, we present a formalism for planning that accounts
for quantitative purposes, probabilistic systems and non-
redundancy. We start by modeling the environment in which
the auditee operates as a Markov Decision Process (MDP)—
a natural model for planning with probabilistic systems.
The reward function of the MDP quantiﬁes the degree of
satisfaction of a purpose upon taking an action from a state.
If the auditee is motivated to action by only that purpose,
then the auditee’s actions must correspond to an optimal
plan for this MDP and these actions are for that purpose.
We develop a stricter deﬁnition of optimal than standard
MDPs, which we call NMDPs for Non-redundant MDP, to
reject redundant actions that neither decrease nor increase
the total reward. We end with an example illustrating the
use of an NMDP to model an audited environment.
A. Markov Decision Processes
An MDP may be thought of as a probabilistic automaton
where each transition is labeled with a reward in addition
to an action. Rather than having accepting or goal states,
the “goal” of an MDP is to maximize the total reward over
time.
An MDP is a tuple m = hS, A, t, r, γi where
• S is a ﬁnite set of states;
• A is a ﬁnite set of actions;
• t : S × A → D(S), a transition function from a state
and an action to a distribution over states (represented
as D(S));
• r : S × A → R, a reward function; and
• γ, a discount factor such that 0  0).
Given an MDP m, let m(s, κ, σ) denote the execution that
results from using κ to resolve all the probabilistic choices
in m, the agent using the strategy σ, and having the model
start in state s. Henceforth, we only consider contingencies
consistent with the model under discussion.
Given two strategies σ and σ′, we write σ′ ≺ σ if and
only if for all contingencies κ and states s, m(s, κ, σ′) is
a proper sub-execution of or equal to m(s, κ, σ), and for
at least one contingency κ′ and state s′, m(s′, κ′, σ′) is a
proper sub-execution of m(s′, κ′, σ). Intuitively, σ′ proves
that σ produces a redundant execution under κ′ and s′. As
we would expect, ≺ is a strict partial ordering on strategies:
Proposition 1. ≺ is a strict partial order.
We deﬁne nopt(r) to be the subset of opt(r) holding only
strategies σ such that for no σ′ ∈ opt(r) does σ′ ≺ σ.
nopt(r) is the set of non-redundant optimal policies.
The MDP model is useful because an optimal strategy is
guaranteed to exist. Fortunately, we can prove that nopt(r)
is also guaranteed to be non-empty. We may prove this result
using reasoning about well-ordered sets, Proposition 1, and
the fact that the space of all possible strategies is ﬁnite for
NMDPs with ﬁnite state and action spaces.
Theorem 1. For all NMDPs m, nopt(m) is not empty.
C. Example: Modeling the Physician’s Environment
Suppose an auditor is inspecting a hospital and comes
across a physician referring a medical record to his own
private practice for analysis of an X-ray as described in
Section II. As physicians may only make such referrals for
the purpose of treatment (treat), the auditor may ﬁnd the
physician’s behavior suspicious. To investigate, the auditor
may formally model the hospital using our formalism.
After studying the hospital and how the physician’s
actions affect it, the auditor would construct the NMDP
mex1 = hSex1, Aex1, tex1, rtreat
ex1 , γex1i shown in Figure 1. The
ﬁgure conveys all components of the NMDP except γex1.
For instance, the block arrow from the state s1 labeled take
and the squiggly arrows leaving it denote that after the agent
performs the action take from state s1, the environment will
transition to the state s2 with probability 0.9 and to state
s4 with probability of 0.1 (i.e., tex1(s1, take)(s2) = 0.9
and tex1(s1, take)(s4) = 0.1). The number over the block
arrow further indicates the degree to which the action
satisﬁes the purpose of treat. In this instance, it shows that
rtreat
ex1 (s1, take) = 0. This transition models the physician
taking an X-ray. With probability 0.9, he is able to make
a diagnosis right away (from state s2); with probability
0.1, he must send the X-ray to his practice to make a
diagnosis. Similarly, the transition from state s4 models
that his practice’s test has a 0.8 success rate of making a
diagnosis; with probability 0.2, no diagnosis is ever reached.
For simplicity, we assume that all diagnoses have the same
quality of 12 and that second opinions do not improve the
quality; the auditor could use a different model if these
assumptions are false.
Using the model, the auditor computes opt(rtreat
ex1 ), which
consists of those strategies that maximizes the expected
total discounted degree of satisfaction of the purpose of
treatment where the expectation is over the probabilistic
180
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:47:51 UTC from IEEE Xplore.  Restrictions apply. 
take, 0
s1
0.9
0.1
send, 0
diagnose, 12
send, 0
0.2
s2
s4
1
1
0.8
s3
s5
diagnose, 12
1
diagnose, 12
s6
1
Figure 1. The environment model mex1 that the physician used. Circles represent states, block arrows denote possible actions, and squiggly arrows denote
probabilistic outcomes. Self-loops of zero reward under all actions, including the special action Stop, are not shown.
transitions of the model. opt(rtreat
ex1 ) includes the appropri-
ate strategy σ1 where σ1(s1) = take, σ1(s4) = send,
σ1(s2) = σ1(s3) = σ1(s5) = diagnose, and σ1(s6) = Stop.
Furthermore, opt(rtreat
ex1 ) excludes the redundant strategy σ2
that performs a redundant send where σ2 is the same as σ1
except for σ2(s2) = send. Performing the extra action send
delays the reward of 12 for achieving a diagnosis resulting
in its discounted reward being γ 2
ex1 ∗ 12 instead of γex1 ∗ 12
and, thus, the strategy is not optimal.
However, opt(rtreat
ex1 ) does include the redundant strategy
is the same as σ1 except for σ3(s6) = send.
σ3 that
opt(rtreat