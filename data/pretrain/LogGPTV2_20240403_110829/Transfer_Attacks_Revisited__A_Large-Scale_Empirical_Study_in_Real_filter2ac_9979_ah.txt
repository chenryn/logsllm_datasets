(Feb. 2018). arXiv: 1802.09707.
[50] YANG, Z., DAI, Z., YANG, Y., CARBONELL, J., SALAKHUTDINOV,
R. R., AND LE, Q. V. Xlnet: Generalized autoregressive pretraining for
language understanding. In Advances in Neural Information Processing
Systems 32. Curran Associates, Inc., 2019, pp. 5753–5763.
[51] ZHAO, H., GALLO, O., FROSIO, I., AND KAUTZ, J. Loss functions
for image restoration with neural networks. IEEE Trans. Computational
Imaging 3, 1 (2017), 47–57.
X. APPENDIX
A. Validity and Bias of Equivalence Dictionaries
An equivalence dictionary consists of different label map-
pings. Each label mapping can be viewed as a new class for
a MLaaS model without making critical changes.
However, building a good equivalence dictionary is impor-
tant for getting the real performance. Although we use human
knowledge to create it, bias can be introduced. We formally
discuss the level of bias introduced using the following propo-
sition.
Proposition 1. Suppose D contains all acceptable MLaaS
classes for a local class c. M is the class mapping we
construct. If M satisﬁes M ⊂ D, then the measured matching
rate is lower and the measured misclassiﬁcation rate is higher
than their true value. If D ⊂ M, then the measured matching
rate is higher and the measured misclassiﬁcation rate is lower
than their true value. When (cid:107)D − M(cid:107) shrinks to empty set,
the bias decreases to zero as well.
The proof of Proposition 1 is straightforward based on its
deﬁnition. To make (cid:107)D−M(cid:107) as small as possible, we construct
the equivalence dictionaries as described in Section IV-B. This
method is good at capturing acceptable predictions in practice,
reducing the number of missed acceptable predictions to a very
low level, i.e., (cid:107)D − M(cid:107) is nearly empty. In our experiment,
we ﬁnd some class mappings include super-classes, such as
including sports in the class mapping of baseball, which
makes D ⊂ M. As shown in Proposition 1, this leads to a
larger matching rate and smaller misclassiﬁcation rate, which
is potentially destructive to our evaluations. However, as we
see from the results, all the matching rates are small and most
of the misclassiﬁcation rates are signiﬁcant, indicating that the
bias should be of little harm to the conclusions we make.
B. Impact of Irrelevant Factors on the Conclusions
Traversing all the possible factor settings in transfer attacks
is expensive. In fact, to ensure balanced observations, the
experiment settings are Ω = Ω1×Ω2×···×Ωn, where Ωi is the
setting space of one factor, e.g., pretraining, surrogate dataset,
adversarial algorithm, etc., and n is the total number of factors.
This means the total setting space is exponential. The large
setting space makes it extremely hard to derive conclusions
that are generalizable in a reasonable sense.
We address this problem by taking empirical expectation
over the irrelevant setting space, thus minimizing the inﬂuence
of the speciﬁc settings of irrelevant factors. To be exact, we
conclude the inﬂuence of one speciﬁc factor by averaging
over settings with this factor ﬁxed and other factors varied
in a balanced way. This is realized by running regression on
balanced data which can be viewed as grid samples from Ω.
For example, when concluding the effect of a target platform,
the setting space of the target platform is the considered factor
(cid:81)
space (denoted as Ωi∗), and the setting space of pretraining,
dataset and surrogate model, etc., are the irrelevant space
i(cid:54)=i∗ Ωi. The regression methodology provides automatic
ther make the expectation taken on(cid:81)
empirical expectation over the irrelevant settings based on the
provided data, and we use balanced data in regression to fur-
i(cid:54)=i∗ Ωi to be unbiased.
Therefore, the effect of a speciﬁc factor is in fact eliminated
by taking expectation. In other words, the conclusions made
on one factor, e.g., pretraining, are not affected by the speciﬁc
settings of other factors, e.g., adversarial algorithm. Since what
we actually do is an empirical expectation, the effect of factors
might not be totally eliminated, but still minimized.
C. Correctness and Generalization of Applying OLS Analysis
In multi-factor analysis, OLS analysis is commonly adopted
to decompose the inﬂuence of each factor. However, due to
its simplicity, it sometimes only reﬂects some aspects of the
underlying inﬂuence function.
First, since the underlying inﬂuence of factors might be non-
linear, the result of OLS which is linear is not suitable to be
taken as the ground truth of the underlying inﬂuence function.
The correct application is to use the “direction” and “size” of
impact, i.e., whether the impact is positive and how large the
impact is, which is the central point of our observations and
analyses. However, the reader should note that these directions
are local and are technically restricted to the applied linear
model.
Second, the OLS analysis ﬁnds “relationship” from the data
and is a correlation analysis naturally. This means that all
results are “correlations” but not “causation” unless there are
additional experiments targeting the causation. In our paper,
the only causation analysis is in Section V-B1 where we
generate random AEs with largest L2 and ﬁxed L∞ to test
the inﬂuence of L2 norm.
Third, the p-values are computed based on the assumption
that the residual of linear model is Gaussian-distributed. In
practice,
leading to biased
p-values. Speciﬁcally, in our experiment, we ﬁnd that the
residual is roughly t-distributed with a degree of freedom
16 which is non-Gaussian. However, ﬁxing this issue using
techniques such as Cox-Box transformation [11] is ill-suited
for binary variables and makes the result difﬁcult to follow. In
addition, for large degrees of freedom, t-distribution converges
to Gaussian distribution. Therefore, since it has little bias on
the result, we use the original OLS for clarity.
this assumption may not hold,
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:58 UTC from IEEE Xplore.  Restrictions apply. 
1437
D. Detailed Explanations for OLS Tables
Due to the page limit of the main text, we do not include
detailed explanations on how to read the formatted OLS tables
in Section V-A. This section provides these materials for
readers unfamiliar with OLS analysis.
To conclude a factor’s impact on the dependent variable
(called response variable as well), one need to look at all
regressions on the dependent variable that include this factor.
If the coefﬁcient changes dramatically when an extra factor
is included, it usually suggests that strong interaction exists
between this factor and the extra factor. If the coefﬁcient is
not changed when other factors are included, it means that
the interaction is weak, if exists. When strong interaction is
detected, it is preferable to perform another regression and
include the interaction term, e.g., is pre×is adv, simpliﬁed as
is pre×adv in this paper. This kind of technique, known as
hierarchical regression, provides more information about the
impact of factors.
The tables are formatted to include and directly compare
results of multiple OLS regressions. Since Table III and Table
IV are formatted in the same way, we use Table III as an
example. In Table III, regression A to E regress misclassiﬁ-
cation rate and regression F to J regress matching rate on the
corresponding variables. Each regression from A to E includes
different regressors. For one regression, if coefﬁcients and
standard deviations are provided in the table for a regressor,
then it means that this regressor is included in this regression
and the corresponding result is the provided value. Otherwise,
if the space is left blank, it means that this regressor is not
included in the regression. For example, in regression A, the
regressors are platform factors (from is Google to is Aliyun).
In regression B, the regressors are platform factors, pretraining
factors (is pretrained) and dataset factors (is adversarial and
is augmented).
it means that
this coefﬁcient
To avoid randomness introduced by the data on the result,
p-values are provided. If a coefﬁcient has a small p-value,
highlighted by asterisks,
is
signiﬁcantly different from zero, i.e., the impact is “real”. In
practice, only values highlighted with asterisks are considered
as important and worth attention. Apart from p-values, coef-
ﬁcients of the OLS regression and their standard deviations
are provided in the table. A positive coefﬁcient indicates that
this factor has a positive impact on the dependent variable
and a negative coefﬁcient indicates the opposite. In addition, a
larger absolute value of the coefﬁcient indicates a larger effect.
The standard deviation represents how much uncertainty is
involved in computing the coefﬁcient, and is mainly used as
a supplement to p-values. Details about the OLS regressions
can be found in the released code repository.
and Misclassiﬁcation Rate
E. Further Discussion about L∞
From Figure 7 in Section V-B1, we have shown that L2 has
large correlation with misclassiﬁcation rate while the correla-
tion between L∞ and misclassiﬁcation rate is relatively small.
Since L2 and L∞ has a correlation 0.51, it is possible that even
this roughly 0.4 correlation between L∞ and misclassiﬁcation
rate is largely due to the dependence between L2 and L∞. We
prove this claim in this discussion.
For notational reasons, we deﬁne X1 to be L2 norm and
X2 to be L∞ norm, divided by their standard deviation, re-
spectively. We further deﬁne Y to be the misclassiﬁcation rate
divided by its standard deviation. Thus, Var X1 = Var X2 =
Var Y = 1 and the correlation between them are equivalent
to the covariance. Let  = X2 − X1 × Cov(X1, X2). Since
Cov(, X1) = Cov(X1, X2)−Cov(X1×Cov(X1, X2), X1) =
0, we know  is actually uncorrelated with X1. There-
fore, X2 =  + X1 × Cov(X1, X2) decomposes the ef-
fect of X1 and  is the “L2-free” term of L∞. Now we
claim that this L2-free term has a very small correlation to
misclassiﬁcation rate. Indeed, Cov(, Y ) = Cov(X2, Y ) −
Cov(X1×Cov(X1, X2), Y ) = Cov(X2, Y )−Cov(X1, X2)×
Cov(X1, Y ). Plug in all
the correlation values in Figure
7, we get the correlation between  and Y is −0.069 for
Aliyun, −0.0439 for Baidu, −0.0633 for Google and −0.0682
for AWS. Therefore, the L2-free contribution of L∞ to the
misclassiﬁcation rate is slightly negative, almost negligible.
F. Guidelines for Defending Transfer Attacks
robustness to transfer attacks.
• (Observation 3) Use FGSM ﬁrst when evaluating the
• (Section V-B1 and V-B2) As long as the attack algorithm
is able to control adversarial conﬁdence (SIK) and perturbation
norm of AEs, use a large SIK value and a large L2 perturbation
norm for better transferability.
• (Section V-B3) Maintain a pool of images that are intrin-
sically hard to classify. Use these images as seeds to test the
robustness of the target model.
G. Parameter Settings for White-box Attacks and Augmenta-
tion
The detailed settings of white-box adversarial attacks on
the surrogate models are shown in Table VII. We set kappa
to zero for CW2 to make it comparable to other attacks,
and it achieves a 100% success rate on surrogates for all
cases. The iteration times of iterative attacks, though might
improve the attack when increased further, are sufﬁcient in
our experiments because all of them reach the  bound, as
shown in Figure 6b. The choice of speciﬁc  bound is based
on the principle that an attacker would like to set  as large as
possible while maintaining visual quality, i.e., being unnoticed
by human, because it would lead to better transferability. Since
visual quality is hard to quantify, we try different values for ,
and decide that 0.1 is a good choice for ImageNet under the
condition that human eyes should not perceive the difference.
Actually, with  = 0.1, there are already some mild noise
patterns that are noticeable to human eyes. Therefore, our
experiments can be viewed as a stress test of MlaaS systems.
The detailed setting of data augmentation is described
below. Color jitter has brightness=0.1, contrast=0.1 and sat-
uration=0.1, which means the augmented image will be in the
range of these deviations. Random afﬁne has maximum degree
30 and a maximum translate (0.1, 0.1). Random rotation has a
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:58 UTC from IEEE Xplore.  Restrictions apply. 
1438
Table VII: Hyperparameter setting of adversarial algorithms.
 is all set to be 0.1 for ImageNet and Adience.
Attacks
BL-BFGS
CW2
DeepFool
PGD
RFGSM
Conﬁguration
initial const=0.01
search steps=10
max iterations=40
κ=0
initial const=0.001
learning rate=0.02
max iterations=100
max iterations=100
overshoot=0.02
number of steps=10
α=0.5
maximum degree 30. Random horizontal ﬂip, random vertical
ﬂip and random perspective have no parameter. They are
applied with a probability of 0.5 independently. After ﬁnishing
an epoch of training, the augmentation is redone, which means
the dataset is enlarged by the number of epoch times. In our
case, it is 50.
H. Implementation Changes on DEEPSEC
The original DEEPSEC implementation presented by Ling
et al. [28] has issues about numerical stability which have
been ﬁxed by the original authors. We apply this new version
of DEEPSEC in our experiments. Furthermore, we removed
the random initialization in the PGD code, which is proposed
in RFGSM, to make it directly comparable to FGSM. This
action is not a ﬁx to DEEPSEC but speciﬁc to our usage,
and users are still encouraged to use DEEPSEC but not our
changed version. Apart from some auxiliary code, what we
include in our repo is mostly the test code which sends the
data to MLaaS and analyzes the feedback.
I. Discussion about the Cutting Threshold
As discussed in Section IV-B, we address the multiple
predictions problem by applying a cutting threshold estimated
on the returns of original images. In fact, it is worthwhile to
consider whether these estimated thresholds are applicable to
other datasets and whether real users will apply the thresh-
old cutting mechanism. We discuss these two issues below,
respectively.
First, the estimation is conditioned on the chosen classes
and no reﬁnement can be made because no knowledge about
the “real” threshold is known. However, we estimate the
thresholds on the used dataset and apply it on this dataset.
Therefore, the threshold does not need to “transfer”. In addi-
tion, as stated in Section IV-B, the choice of thresholds only
affects the comparison of MLaaS systems which is limited
to the discussed tasks but
leaves the discussion of other
factors unharmed. Therefore, although the estimation might
be biased, it does not affect conclusions about other factors
but only platform factors. For attacks with seed images from
undiscussed classes, they should do the threshold searching
again and not directly apply our thresholds.
Second, the threshold cutting mechanism is a reasonable
way to be adopted by users of MLaaS systems since the
number of highly conﬁdent labels may vary. Furthermore, if
the users apply threshold cutting, then the chances are that
Figure 11: The relation between transferability and the input
gradient size on the target model.
the thresholds derived from our methods are good estimations
for the applied thresholds. This is because these users would
like to focus on a few predictions without signiﬁcant accu-
racy drop, which is aligned to our principles. Therefore, our
conclusions provide an important insight of transfer attack for
these users.
J. Discussion about Input Gradient Size
We would like to bring back that the input gradient size
on our target models, MLaaS platform models, is not avail-
able. Nevertheless, in order to explore the relation between
transferability of AEs and their input gradient size on the
target model, we build up some local models to simulate
the process of transfer attacks, i.e., use one local model as
the target model and the others as the surrogate models. The
experiments are conducted on the Adience dataset, and we
choose the classic pretrained VGG-16 model as the target
model. Then we use AEs generated on other local models
to attack the target model and measure their input gradient
size on the target model. Note that these AEs are generated
with multiple model architectures, multiple surrogate dataset
settings and multiple adversarial attack methods, following the
evaluation settings in Section V-B2. Speciﬁcally, the surrogate
datasets include raw and augmented datasets; the surrogate
models may be trained with or without the adversarial training;
the surrogate models’ architectures include ResNet, VGG-16
and Inception V3; depths of the ResNet include 18, 34 and
50; the adversarial attack methods include BL-BFGS, CW2,
DeepFool, FGSM, LLC, PGD, RFGSM, STEP-LLC and UAP.
The experiment results are shown in Figure 11. It is evident
that AEs with larger input gradient size on the target model
tend to have better transferability, regardless of how the AE is
generated, which is consistent with the conclusion of Demontis
et al. [15].
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:58 UTC from IEEE Xplore.  Restrictions apply. 
1439
[0.0, 0.2][0.2, 0.4][0.4, 0.6][0.6, 0.8][0.8, 1.0][1.0, 1.2]Input Gradient Size0.20.30.40.50.6Misclassification Rate