on social network defenses, and iii) can in fact tolerate a
million-node botnet given just thousands of honest voters.
Sybil attacks in reputation systems. Related to recommen-
dation systems, reputation systems (such as Ebay) are also
vulnerable to sybil attacks. A reputation system computes
the numerical reputation of individual
identities, based
on pair-wise feedback among the identities. A number
of mechanisms [19, 28, 35, 55] have been proposed to
prevent sybil identities from artiﬁcially boosting the attacker’s
reputation. Compared to DSybil, these efforts do not explore
how to use such reputation to make recommendations (for
3. With a million-node botnet, the adversary may be able to launch serious
DDoS attacks on various components of the system. How to defend [25]
against such DDoS attacks is beyond the scope of this paper.
4. In addition, as a secondary optimization, SumUp leverages feedback
and trust, but its heuristics are without provable guarantees on end-to-end
loss.
285
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:17:54 UTC from IEEE Xplore.  Restrictions apply. 
objects) with provable guarantees.
Classic machine learning algorithms and extensions.
From a theoretical perspective, DSybil’s recommendation
algorithm provides strong guarantees for a variant of the
sleeping-experts-based [38] adversarial multiarmed bandit
(MAB) problem [3] in machine learning. In this problem, the
algorithm needs to make a recommendation out of a pool of
objects in each round, based on the votes from the “experts.”
Sleeping-experts means that an expert might not participate
in all rounds, while MAB means that the algorithm obtains
only partial-feedback [18] (i.e., no feedback is provided for
objects not chosen/recommended). These two aspects of the
problem make it particularly challenging.
There are existing results [3] on the MAB problem that
assume all experts participate (vote) in all rounds. This
assumption rarely holds when the experts are real users in a
recommendation system. Researchers [5, 6, 53, 54] have also
applied MAB-related techniques to recommendation systems,
while inheriting the previous assumption (or making even
stronger assumptions). Some efforts [13, 14, 30, 47] have
investigated how to make recommendations based on votes
from sleeping-experts who do not always participate. These
efforts all assume complete feedback where the “goodness”
of all objects (even non-recommended ones) are revealed
after each round. For our applications, doing so would require
the user to test all objects, which defeats the exact purpose
of providing recommendations.
To the best of our knowledge, the only work that discusses
the sleeping-experts-based MAB problem is [38]. The pro-
posed algorithm has exponential complexity and linear loss
(in terms of the number of sybil identities). We are able to
design a linear-complexity logarithmic-loss algorithm for this
problem by exploiting the voting pattern of honest users.
Other theoretical work on recommendation systems.
There have been many other theoretical efforts on recommen-
dation systems [2, 4, 7, 8, 9, 10, 27, 39]. They largely deal
with ﬁnding good objects out of a ﬁxed set of objects, while
our model involves multiple rounds where each round has
its own set of objects. The notion of trust (across multiple
rounds) only becomes relevant in our model. Near the end
of [8], Awerbuch et al. discuss an algorithm for multiple
rounds. The loss, however, is linear with the number of sybil
identities.
3. System Model and Attack Model
Target applications/scenarios. Recommendation systems
are a generic concept, and the details of different recom-
mendation systems can be dramatically different. As a result,
solutions suitable for one scenario (e.g., Netﬂix) may very
well be inappropriate in other contexts (e.g., Digg). DSybil
does not intend to capture all possible recommendation
systems. Rather, we focus on scenarios where i) the objects
286
to be recommended are either good or bad (but different users
may have different subjective opinions regarding whether an
object is good); ii) the lifespan of the users is not overly
short so that they can build trust; and iii) the users aim to
ﬁnd some good objects to consume instead of exhaustively
ﬁnding all good objects.
In terms of real world applications, DSybil captures the
requirements of p2p ﬁle rating systems such as Credence [60]
and news story recommendation systems such as Digg [23].
In both cases, the objects are binary in terms of “goodness.”
A typical user of Credence or Digg may download many
ﬁles or read many news stories (e.g., 20 per day). Also, a
user in Credence is usually only interested in downloading
one good (i.e., non-corrupted) version of, for example, a
particular song. The user does not aim to exhaustively ﬁnd
all good versions. Similarly, a typical Digg user may wish to
read a certain number of interesting news stories on a given
day, without ﬁnding out all the interesting stories appearing
in that day. On the other hand, DSybil cannot yet capture
Netﬂix or Amazon where rather ﬁne-grained (e.g., 5-star
scales) recommendations are expected.
In the remainder of this section, we formally deﬁne
DSybil’s system model and attack model.
Objects and rounds. DSybil recommends objects (e.g., news
stories in Digg or ﬁles for downloading in Credence) to users,
based on votes (see later) from other users. Depending on
a user’s prior behavior, DSybil may recommend different
objects to different users. From now on, our discussion will
always be with respect to a given DSybil user (called Alice).
We assume that Alice follows the protocol honestly.
With respect to Alice, each object is either good or bad.
In each round, Alice would like to pick one object out
of a speciﬁc set (U) of objects to consume (e.g., read the
story or download the ﬁle). For example, U may be the
set of all stories submitted over the past day in Digg or
all the different versions of mp3 ﬁles with the same song
title in Credence. We assume that the fraction of good
objects in U is at least p that is bounded away from 0.
For example, measurement studies [43] of the ﬁle pollution
level in Kazaa show that the fraction of good versions of the
songs studied ranges between 30% to 100%, under attackers
with commercial interests. After each round, Alice provides
feedback to DSybil regarding whether the consumed object
is good. This feedback can be in the form of a vote but does
not have to be. For example, Alice may provide feedback but
choose not to cast a vote that other users will see (e.g., due to
the sensitivity of the object). Sometimes implicit feedback is
possible (e.g., based on how much time Alice spends reading
the news story).
DSybil can be conﬁgured to recommend either a single
object or a set of k (e.g., 5 or 10) objects in each round. If
multiple objects are recommended, Alice picks one of them
to consume. As an example, one can view the set as a web
page displaying excerpts of k recommended news stories
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:17:54 UTC from IEEE Xplore.  Restrictions apply. 
in Digg. Also, if Alice wants to consume multiple objects
from a given set U, we can always model that as multiple
rounds with the same U (after removing the objects already
consumed).
We use loss, deﬁned as the total number of bad objects
consumed by Alice, to measure the “goodness” of the
recommendation system. Since DSybil is randomized, we
will only be concerned with the expectation on loss, where
the expectation is taken over the random coin ﬂips in the
algorithm. The loss is also dependent on the attack strategy
of the adversary. When we say loss under the worst-case
attack, we mean the expected loss under the worst-case
attack strategy. We expect that in order to retain users, a
recommendation system needs to achieve a rather small per-
round loss (e.g., 0.1 or 0.05) that can be much smaller than
1− p. To achieve this, DSybil leverages the votes from other
users.
Votes. A vote claims a certain object to be good, and this is
the only kind of vote DSybil uses. In this sense, all votes in
DSybil are “positive votes”. We will later prove that in our
setting “negative votes” can never help to reduce loss, and
thus DSybil does not use them. A user does not have to use
DSybil (or follow DSybil’s recommendations) in order to
vote. For example, a Linux fan may read all Linux-related
news stories on Digg and then vote for the interesting ones.
Similarly, a round as deﬁned earlier is not a voting round.
Namely, the votes on the objects in Alice’s round were cast
(by other DSybil and non-DSybil users) independent of when
Alice starts the round.
Attack model. Because we consider sybil attacks, we will use
the term identities instead of users from now on. An identity
is not necessarily tied to any human being or computer in
the real world. An identity is either honest or sybil. A sybil
identity can be an identity owned by a malicious user, or it
can be a bribed/stolen identity, or it can be a fake identity
obtained through a sybil attack. We assume that DSybil can
associate each vote with the identity casting that vote. For
example, this can be achieved by requiring an identity to
log into his/her account before voting. Alternatively, each
identity may have a locally generated public/private key pair,
and signs each vote using the private key. (The adversary
can freely generate an unlimited number of accounts or key
pairs for the sybil identities.) Each identity may cast at most
one vote on any given object; otherwise, its votes will be
ignored.
A sybil identity is byzantine and may collude with all
other sybil identities. We allow the sybil identities to know
which objects are good/bad and also the votes cast by honest
identities. We do not make assumptions on the total number
of sybil identities over time, which can be inﬁnite. We assume
that the number of sybil identities voting on any given object
is at most M, where M (e.g., 1010) can be orders of magnitude
larger than the number of voting honest identities (e.g., 1000).
# of guides
# of non-guides
N
N0
M maximum # of sybil voters on any object
W maximum # of honest voters on any object
p
D f
f
each round has at least p fraction of good objects
the f -fractional dimension
the fraction of good objects covered by critical guides
Table 1. Key notations in this paper. DSybil does not know or
need to know the values of any of these parameters.
A round is called attack-free if there are no sybil identities
voting in that round. DSybil does not know which rounds
(if any) are attack-free.
Honest identities may have different “tastes” and thus may
have different (subjective) opinions on the same object. We
assume that there are N honest identities (called guides) with
the same or similar “taste” as Alice. Speciﬁcally, having the
same taste as Alice means that the guide never votes on bad
objects. (On the other hand, the guide may or may not vote on
any given good object.) Having similar taste as Alice means
that the guide seldom votes on bad objects. The existence
of such guides is a necessary assumption for virtually all
recommendation systems—if Alice has such an esoteric taste
that no one else shares her taste, no recommendation system
can help Alice. Let N0 denote the number of honest identities
that are not guides (called non-guides). We will focus on
cases where M > N + N0. Unless otherwise mentioned, we
will pessimistically treat the N0 non-guides as byzantine (and
potentially colluding with sybil identities). DSybil does not
know or try to determine which identities are guides, non-
guides, or sybil. In particular, the extra loss due to a guide
being bribed or compromised is equivalent to M increasing by
1 (unless removing the guide increases the dimension—see
later).
We say that an object is covered by an identity if that
identity votes for that object. Let U denote the set of objects
that has ever appeared in any of Alice’s rounds. Given the
votes on the objects in U , we deﬁned U ’s f -fractional
dimension (denoted as D f ) as the smallest number of guides
that can cover f fraction of the good objects in U . We
will be concerned only with f that is not too small (e.g.,
0.5). We call these D f guides as critical guides.5 We call
those objects that are covered by at least one critical guide as
guided. Other objects (even if they are covered by some other
guide) are called unguided. For any given set of identities,
their voting pattern fully speciﬁes which identities vote for
which objects. By deﬁnition, D f
is only affected by the
voting pattern of the guides, which are honest. Calculating
D f can be reduced to Set-Covering and is thus NP-hard.
DSybil does not know which objects are guided or which
identities are critical guides. DSybil does not know/calculate
D f either. Table 1 summarizes the key notations in this paper.
5. It is possible to have different sets of D f guides where every set can
cover f fraction of the good objects in U . In such case, we simply pick
an arbitrary set and call (only) those guides in that set as critical guides.
287
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:17:54 UTC from IEEE Xplore.  Restrictions apply. 
4. Leveraging Trust: The Obvious, the Subtle,
and the Challenge
Even with small D f values, designing a robust recommen-
dation system is far from trivial. This section discusses some
of the issues.
The obvious. The notion of trust appeals naturally to human
intuition. For example, we all know that Alice should trust
Bob more (less) if Alice ﬁnds that Bob voted for good (bad)
objects. Also, objects with more votes should probably be
recommended over objects with fewer votes.
The subtle. How to implement this “obvious” notion is in
fact not so obvious, however, as one starts asking quantitative
and more speciﬁc questions. First, how should we assign
seed trust (i.e., initial trust) to new identities? Because the
adversary can continuously introduce new identities (by
whitewashing), giving positive seed trust to each new identity
may already allow the adversary to inﬂict inﬁnite loss over
time. One might introduce some “trial period” so that an
identity is given seed trust only after, for example, voting
for 10 good objects. Unfortunately, knowing this, a sybil
identity could always ﬁrst vote for 10 good objects and then
immediately cheat. Notice that these ﬁrst 10 “correct” votes
would not beneﬁt Alice at all, since identities in the trial
period are not used for recommendations.
Second, how exactly should we grow the trust of the
identities? If Bob has cast 4 “correct” votes while Cary has
cast 12, should the trust to Cary be 3 (= 12/4) times or
256 (= 2(12−4)) times the trust to Bob? In particular, should
we (and how can we) avoid growing the trust of identities
casting “correct” but “non-helpful” votes? For example, a
sybil identity may vote for a good object that already has
many votes from other users. Such a “correct” vote is “non-
helpful” because the object will most likely be selected
anyway even without this additional vote. To prevent sybil
identities from gaining trust “for free” by casting such votes,
one may try to determine the “amount of contribution” from
different voters. Doing so, however, can be rather tricky. For
example, some researchers [53, 54] propose taking the voting
order into account. The ﬁrst voter for the object gains the
most trust, while later ones gain less and less trust. However,
if the adversary knows that a certain object will attract a lot
of votes by the time that Alice selects, the sybil identities
can all rush and cast the ﬁrst batch of votes on that object.
These votes are still “non-helpful” because the object will
attract enough votes anyway.
Finally, how should we recommend objects based on the
(potentially conﬂicting) votes from the various identities with
different trust? Should two votes from two identities each with
x trust be considered equivalent to one vote from an identity