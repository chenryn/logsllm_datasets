```
apiVersion: v1
kind: Pod
metadata:
  name: slow-app
spec:
  containers:
  - name: slow-app
    image: slow-app:latest
tolerations:
- key: "cpu_speed"
  operator: "Equal"
  Value: "slow"
  effect: "NoExecute"
  tolerationSeconds: 60
```
在这种情况下，当执行污点和容忍时，已经在带有污点`slow`的节点上运行的 Pod 将在节点上保留`60`秒，然后被重新调度到不同的节点。
## 多重污染和耐受性
当一个 Pod 和节点上存在多个污点或容忍时，调度程序将检查所有这些污点或容忍。这里没有`OR`逻辑运算符——如果节点上的任何污点在 Pod 上没有匹配的容忍性，它将不会在节点上被调度(除了`PreferNoSchedule`，在这种情况下，如前所述，如果可能，调度器将尝试不在节点上调度)。即使节点上的六个污点中，PODS 可以容忍其中的五个，它仍然不会被安排`NoSchedule`污点，并且它仍然会因为`NoExecute`污点而被驱逐。
对于一个给我们一个更微妙的方法来控制放置的工具，让我们看看节点亲和力。
# 用节点亲和性控制荚果
正如你所知可能会说的，污点和容忍——虽然比节点选择器灵活得多——仍然没有解决一些用例，通常只允许一个*过滤器*模式，你可以使用`Exists`或`Equals`在特定污点上进行匹配。可能会有更高级的用例，在这些用例中，您需要更灵活的选择节点的方法，而*亲缘关系*是 Kubernetes 解决这个问题的一个特性。
有两种类型的相似性:
*   **节点亲和力**
*   **荚果间亲和性**
节点相似性是一个类似于节点选择器的概念，除了它允许一组更健壮的选择特征。让我们看一些 YAML 的例子，然后挑选出不同的部分:
pod-with-node-affinity.yaml
```
apiVersion: v1
kind: Pod
metadata:
  name: affinity-test
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: cpu_speed
            operator: In
            values:
            - fast
            - medium_fast
  containers:
  - name: speedy-app
    image: speedy-app:latest
```
如您所见，我们的`Pod` `spec`有一个`affinity`键，我们已经指定了一个`nodeAffinity`设置。有两种可能的节点亲缘关系类型:
*   `requiredDuringSchedulingIgnoredDuringExecution`
*   `preferredDuringSchedulingIgnoredDuringExecution`
这两种类型的功能分别直接映射到`NoSchedule`和`PreferNoSchedule`的工作方式。
## 使用 requireduringschedulingnoredduringeexecution 节点关联性
对于`requiredDuringSchedulingIgnoredDuringExecution`，如果没有与节点匹配的术语，Kubernetes 将永远不会调度 Pod 。
对于`preferredDuringSchedulingIgnoredDuringExecution`，它将尝试满足软要求，但如果不能，它仍将调度 Pod。
节点相似性超过节点选择器、污点和容忍的真正能力来自于实际的表达式和逻辑，当涉及到选择器时，您可以实现它们。
`requiredDuringSchedulingIgnoredDuringExecution`和`preferredDuringSchedulingIgnoredDuringExecution`相似性的功能有很大的不同，所以我们将分别回顾它们。
对于我们的`required`相似性，我们有能力指定`nodeSelectorTerms`-它可以是一个或多个包含`matchExpressions`的块。对于`matchExpressions`的每个区块，可以有多个表达式。
在我们上一节看到的代码块中，我们有一个单节点选择器项，即`matchExpressions`块，它本身只有一个表达式。这个表达式寻找`key`，就像节点选择器一样，它代表一个节点标签。接下来，它有一个`operator`，这给了我们一些灵活的方法来识别匹配。以下是运算符的可能值:
*   `In`
*   `NotIn`
*   `Exists`
*   `DoesNotExist`
*   `Gt`(注:大于)
*   `Lt`(注:小于)
在我们的例子中，我们正在使用`In`运算符，该运算符将检查该值是否是我们指定的几个值之一。最后，在我们的`values`部分，我们可以根据运算符列出一个或多个必须匹配的值，然后表达式为真。
如您所见，这为我们指定选择器提供了更大的粒度。让我们看看我们使用不同运算符的例子`cpu_speed`:
带有节点的 pod-affinity 2 . YAML
```
apiVersion: v1
kind: Pod
metadata:
  name: affinity-test
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: cpu_speed
            operator: Gt
            values:
            - "5"
  containers:
  - name: speedy-app
    image: speedy-app:latest
```
如你所见，我们正在使用一个非常精细的`matchExpressions`选择器。现在，这种使用更高级操作员匹配的能力使我们能够确保我们的`speedy-app`仅在具有足够高时钟速度(在本例中为 5 GHz)的节点上调度。不需要将我们的节点分成像`slow`和`fast`这样的大组，我们可以更加细化我们的规范。
接下来，让我们看看另一个节点关联类型–`preferredDuringSchedulingIgnoredDuringExecution`。
## 使用 preferredduringschedulingnoredduringeexecution 节点关联性
这个的语法略有不同，给了我们更多的粒度来影响这个`soft`需求。让我们看看实现这一点的 Pod 规范 YAML:
带有节点的 pod-affinity 3 . YAML
```
apiVersion: v1
kind: Pod
metadata:
  name: slow-app-affinity
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: cpu_speed
            operator: Lt
            values:
            - "3"
  containers:
  - name: slow-app
    image: slow-app:latest
```
这个看起来有点不同于我们的`required`语法。
对于`preferredDuringSchedulingIgnoredDuringExecution`，我们有能力为每个条目分配一个`weight`，以及一个相关的偏好，它也可以是一个`matchExpressions`块，具有多个使用相同`key-operator-values`语法的内部表达式。
`weight`值是这里的关键区别。由于`preferredDuringSchedulingIgnoredDuringExecution`是一个**软**需求，我们可以列出几个不同的偏好和相关的权重，让调度器尽力满足它们。这种方法的原理是，调度器将检查所有的首选项，并根据每个首选项的权重以及它是否被满足来计算节点的分数。假设所有硬性要求都得到满足，调度器将选择计算出的最高分数的节点。在前面的例子中，我们有一个权重为 1 的偏好，但是权重可以是 1 到 100 之间的任何值，所以让我们看看我们的`speedy-app`用例的更复杂的设置:
带有节点的 pod-affinity 4 . YAML
```
apiVersion: v1
kind: Pod
metadata:
  name: speedy-app-prefers-affinity
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 90
        preference:
          matchExpressions:
          - key: cpu_speed
            operator: Gt
            values:
            - "3"
      - weight: 10
        preference:
          matchExpressions:
          - key: memory_speed
            operator: Gt
            values:
            - "4"
  containers:
  - name: speedy-app
    image: speedy-app:latest
```
在我们确保我们的`speedy-app`在尽可能好的节点上运行的旅程中，我们在这里决定只实现`soft`的要求。如果没有快速节点存在，我们仍然希望我们的应用被调度和运行。为此，我们指定了两个首选项——T2 超过 3 千兆赫的节点和超过 4 千兆赫的内存速度。
由于我们的应用受 CPU 的限制远多于受内存的限制，我们决定适当地权衡我们的偏好。在这种情况下，`cpu_speed`携带`90`的`weight`，而`memory_speed`携带`10`的`weight`。
因此，任何满足我们的`cpu_speed`要求的节点将比只满足`memory_speed`要求的节点具有高得多的计算分数，但仍然低于同时满足这两个要求的节点。当我们试图为这个应用安排 10 或 100 个新的 Pods 时，你可以看到这个计算是多么有价值。
## 多节点亲和力
当我们处理与多个节点的亲缘关系时，有几个关键的逻辑需要记住。首先，即使有单个节点相似性，如果它与同一 Pod 规范上的节点选择器相结合(这确实是可能的)，在任何节点相似性逻辑发挥作用之前，必须满足节点选择器。这是因为节点选择器只实现硬需求，两者之间没有`OR`逻辑运算符。一个`OR`逻辑运算符将检查这两个需求，并确保其中至少有一个是真的——但是节点选择器不让我们这样做。
其次，对于一个`requiredDuringSchedulingIgnoredDuringExecution`节点亲缘关系，`nodeSelectorTerms`下的多个条目在一个`OR`逻辑运算符中处理。如果满足一个，但不是全部，Pod 仍将被调度。
最后，对于任何在`matchExpressions`下有多个条目的`nodeSelectorTerm`，都必须满足——这是一个`AND`逻辑运算符。我们来看一个 YAML 的例子:
带节点的 pod-affinity 5 . YAML
```
apiVersion: v1
kind: Pod
metadata:
  name: affinity-test
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: cpu_speed
            operator: Gt
            values:
            - "5"
          - key: memory_speed
            operator: Gt
            values:
            - "4"
  containers:
  - name: speedy-app
    image: speedy-app:latest
```
在这种情况下，如果一个节点的中央处理器速度为`5`，但不满足内存速度要求(反之亦然)，Pod 将不会被调度。
关于节点关联性，最后要注意的一点是，正如您可能已经注意到的，这两种关联性类型都不允许我们在污点和容忍设置中可以使用的相同的`NoExecute`功能。
一个额外的节点相似性类型–`requiredDuringSchedulingRequiredDuring execution`–将在未来版本中添加此功能。截止到 Kubernetes 1.19，这还不存在。
接下来，我们将研究 pod 之间的亲缘关系和反亲缘关系，后者提供 pod 之间的亲缘关系定义，而不是为节点定义规则。
# 利用荚果间亲和性和抗亲和性