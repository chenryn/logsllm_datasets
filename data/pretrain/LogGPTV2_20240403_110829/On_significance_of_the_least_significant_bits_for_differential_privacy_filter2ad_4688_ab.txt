design goals, philosophy and languages of implementation,
all expose instantiations of the Laplacian mechanism, where
the numerical output of a computation of bounded sensitiv-
ity is released after adding noise sampled from a Laplacian
distribution.
4. LSBS AS A SIDE CHANNEL
As we saw in the previous section, the Laplacian mech-
anism is the staple of general-purpose diﬀerentially private
systems. These systems are extremely diverse in their im-
plementation details, yet they share variants of the same
sampling procedure, which we reproduce below.
4.1 Sampling from Laplacian
The Laplacian distribution Lap(λ), sometimes called the
symmetric exponential distribution, can be thought of as the
exponential distribution assigned a randomly chosen sign.
The exponential distribution is deﬁned by its cumulative
distribution function (cdf)
F (t) , Pr[Y ≤ t] = 1 − e
−t/λ.
The most common method of generating Y is based on the
generic procedure, called the inverse sampling method. To
draw from the probability distribution given the inverse of
−1(·), and a source of uniform randomness U from
its cdf, F
the [0, 1) interval, compute the following:
Y ← F
−1(U ).
To see that the cdf of Y is indeed F (·), one can check that:
Pr[Y ≤ t] = Pr[F
−1(U ) ≤ t] = Pr[U ≤ F (t)] = F (t).
Computing the inverse cdf F
−1(·) happens to be partic-
ularly simple, which results in the following procedure for
sampling from the exponential distribution with parame-
ter λ:
Y ← F
−1(U ) = −λ ln(1 − U )
(the distribution 1 − U can be replaced by the uniform dis-
tribution over (0, 1]). This method appears in standard ref-
erences and libraries [23, 37].
To transform Y into the Laplacian distribution one chooses
sign at random, resulting in the following procedure:
Y ← (2Z − 1) · λ ln(U ),
(1)
where Z is an integer-valued variable uniform over {0, 1},
and U is a real-valued variable uniform over (0, 1]. In prac-
tice most random number generators sample from [0, 1), but
the probabilities of generating the exact zero or one are small
enough that they can be ignored. A twist on the method
above uses a single draw from the uniform distribution on
(0, 1) to generate the sign and the absolute value of the
Laplacian variable:
Y ← sign(r) · λ ln(1 − 2|r|), where r ← U − 0.5.
(2)
652All diﬀerentially private systems considered by this paper
use one of the two methods (1) and (2) to sample a Lapla-
cian1.
4.2 Floating-point numbers and notation
The procedure presented in the previous section is sim-
ple and eﬃcient, requiring only a single sample from the
(0, 1) interval. Unfortunately, simplicity of this procedure
is deceptive, since once implemented in ﬂoating-point arith-
metic, its behavior deviates markedly from its mathematical
abstraction.
Before we proceed with discussion of implementations of
diﬀerentially private mechanisms, we review basic facts about
ﬂoating-point arithmetic and introduce some notation. For
a brief introduction to the subject see Goldberg [17]; Muller
et al. serves as a current reference [35].
All systems considered in this paper work over double-
precision ﬂoating-point numbers or just doubles, for short.
Doubles occupy 64 bits and reserve 1 bit for sign, 11 bits
for exponent, and 52 bits of signiﬁcand or mantissa. The
exponent e is represented as an oﬀset from 1023, thus the
−1022 and the largest
smallest representable exponent is 2
exponent is 21023 (e = 0 is used to represent 0 and subnormal
numbers, e = 2047 represents inﬁnity and NaNs, which we
will ignore). The ﬁrst bit of signiﬁcand has an implicit value
of 1, which gives the total precision of 53 signiﬁcant bits.
If the sign is s, the exponent is e, and the signiﬁcand is
d1 . . . d52, the corresponding ﬂoating point number is
(−1)s(1.d1 . . . d52)2 × 2e−1023.
The take-away point is that values representable as doubles
are spaced non-uniformly throughout the real line. There
are exactly 252 representable reals in the interval [.5, 1), 252
reals in the interval [.25, .5), etc.
The ubiquitous IEEE ﬂoating-point standard [21] requires
the results of basic arithmetic operations (addition, subtrac-
tion, multiplication, division) to be exactly rounded, i.e., be
computed exactly and then rounded to the closest ﬂoating-
point number. We will use ability to compute over ﬂoating-
point numbers with maximal precision in Section 5, where
we introduce and analyze a sampling procedure.
To distinguish between real numbers and doubles, we de-
note the latter as D. The literature on ﬂoating-point arith-
metic has a name for the value of the least signiﬁcant digit of
a ﬂoating-point number, called the unit in the last place, or
ulp for short. For numbers from the set D ∩ (0, 1) (i.e., dou-
−1022
bles from the (0, 1) interval) their ulps vary between 2
and 2
−53.
To diﬀerentiate between exact, mathematical, functions
and arithmetic operations, we will use the following nota-
tion, common in the literature: ⊕,⊖,⊗,⊘ are ﬂoating-point
analogues of addition, subtraction, multiplication, and divi-
sion; LN(·) stands for a ﬂoating-point implementation of the
natural logarithm function.
4.3 Sampling from the uniform distribution
Given a non-uniform density of doubles, a uniform dis-
tribution over (0, 1) is not well deﬁned. It turns out that
standards usually oﬀer no guidance and various references
and libraries approximate the distribution diﬀerently (see
Table 1 for a summary). For example, the distribution gen-
erated by Java’s Random.nextDouble() is conﬁned to integer
multiples of 2
−53.
Reference and Library
Knuth [23]
“Numerical Recipes” [37]
C#
SSJ (Java) [26]
Python
OCaml
Uniform from [0, 1)
−53
−64
multiples of 2
multiples of 2
multiples of 1/(231 − 1)
−53
multiples of 2
−32 or 2
−53
−90
multiples of 2
multiples of 2
Table 1: Support of random doubles.
Many of these sources of randomness are not cryptographic,
i.e., having insuﬃcient entropy and/or predictable, and thus
not appropriate for security applications, including diﬀeren-
tial privacy. We enumerate various strategies for sampling
“uniform” ﬂoating-point numbers to emphasize lack of con-
sistency in this area.
4.4 Sampling from Laplacian: Examples
As we saw in the previous section, samples from the uni-
form distributions are most likely conﬁned to a small sub-
set of doubles, equally spaced throughout the (0, 1) interval.
Even if care is taken to include all doubles in support of the
uniform distribution, transforming the uniformly distributed
random variable into the Laplacian via formulas (1) or (2)
will generate artifacts of its own.
Consider the following, actual example of applying the
log function (from C++’s ) to the set of 10 double
numbers following 1/π (there is nothing special about this
number from the point of view of sampling Laplacians, which
is why it is chosen for illustrative purposes):
It is apparent that some output values are more frequent
than others. For some other starting points, another phe-
nomenon is in eﬀect: some potential output values from D
become missing (indicated on the diagram by empty squares):
1Current version of Airavat has a stub in lieu of its noise
generation routine; we instantiate it with SSJ [26]—the li-
brary used in reporting performance and accuracy results by
Roy et al. [39].
Multiplying the output of the transformation U 7→ − ln(U )
by λ, to generate a scaled Laplacian Lap(λ), results in some
values being repeated several times and some, none at all.
x=1⊘πx+9·2−54LN(x)LN(x+9·2−54)LN(·)......x=2⊘πx+5·2−53LN(x)LN(x+5·2−53)LN(·)......653We repeat the experiment with x = 1⊘ π but this time mul-
tiply the result by 3, which corresponds to sampling from
the Laplacian mechanism with ∆ = 1 and ϵ = 1/3:
Since most random number generators do not output all
potential doubles (see Table 1), their actual approximation
of the Laplacian distribution will have missing values and
values that appear more frequently than they should.
4.5 Attack on ﬂoating-point implementation
of the Laplacian mechanism
Recall that the reason for introducing Laplacian noise is
to convert a deterministic function f into a diﬀerentially pri-
vate one by smoothing its output distribution. The proof of
diﬀerential privacy of the Laplacian mechanism [12] depends
on the fact that the probabilities of seeing the same output
of ˜fϵ(·) on two adjacent inputs are multiplicatively close to
each other.
′
If the ﬂoating-point implementation of the Laplacian dis-
tribution misses some output values on input D and oth-
ers on input D
, the resulting mechanism cannot satisfy
diﬀerential privacy. This is exactly what happens when
the Laplacian mechanism is instantiated with ﬂoating-point
arithmetic without appropriate defense mechanisms.
To diﬀerentiate between ideal and real-world realizations
of the Laplacian distribution and the diﬀerentially private
mechanism ˜fϵ(·), we will denote ﬂoating-point implementa-
∗
p(λ) and the corre-
tions of the Laplacian distribution Lap
ϵ,p(D) = f (D)⊕Lap
∗
∗
sponding Laplacian mechanism ˜f
p(∆/ϵ),
where p is the precision with which the uniform distribution
over (0, 1) is sampled. If the precision is maximal, i.e., all
doubles D are in the support of the uniform distribution, the
p index is dropped.
′
) = 1. Consider the
(3) (striped squares) and
Let ∆ = 1, f (D) = 0 and f (D
1/3(D) = 0 ⊕ Lap
∗
distribution of ˜f
) = 1 ⊕ Lap
∗
˜f
1/3(D
∗
∗
′
(3) (solid squares) around 1.5:
It is apparent from this example that if the output of the
Laplacian mechanism is 1.5 + 2· 2
′
−52, then its input was D
,
and if the output is 1.5 + 3 · 2
−52, then the input must have
been D.
Observing an output that is infeasible under one distri-
bution, constitutes “smoking gun” evidence that rules out
the corresponding input. It gives a simple measure of the
distribution’s deﬁciency, or a lower (conservative) bound on
the adversary’s advantage of breaching diﬀerential privacy.
Let the support of the distribution be deﬁned as
ϵ (D)) , {x ∈ D : Pr[ ˜f
ϵ (D) = x] > 0}.
∗
∗
supp( ˜f
Then the probability of diﬀerential privacy guarantee’s being
broken is at least
ϵ (D) /∈ supp( ˜f
∗
∗
Pr[ ˜f
ϵ (D
′
))],
since it captures the probability of the event that a sam-
∗
ple from ˜f
ϵ (D) falls outside the support of the distribu-
∗
′
tion ˜f
).
ϵ (D
Figure 2 plots this probability, expressed as a function
of λ = .01 . . . 3, for two ﬂoating-point implementations of
the Laplacian mechanism: f
f (D) = 0 and f (D
1/λ,53(·), for ∆ = 1,
∗
1/λ(·) and f
∗
) = 1.
′
For smaller λ’s, which should provide a lower (but still
non-trivial) level of diﬀerential privacy, the probability that
a single sample reveals the input is close to 100%. For larger
λ’s, presumably yielding stronger diﬀerential privacy, the
probability of a compromise never becomes less than 35%.
Lowering resolution of the uniform distribution (sampling
−53 instead of D∩ (0, 1)) increases
from integer multiples of 2
the success rate of the attacker by making the distributions
on D and D
more divergent.
′
This graph, that plots the probability of a catastrophic
breach of diﬀerential privacy, demonstrates that textbook
ﬂoating-point implementations of the Laplacian mechanism
are insecure. The results are reproducible (with some varia-
tions due to compiler options, math libraries, and settings of
CPU ﬂags) across languages (C/C++, Java, C#, Python,
OCaml), operating systems (Windows, Linux, OS X), and
processors (x86, x64).
In particular, all four systems sur-
veyed in Section 3 return to the programmer double-precision
ﬂoating-point answers, and thus enabling this attack.
4.6 Analysis
In addition to empirical data on success probability of
the attack, we present heuristic analysis of one particularly
important attack scenario, of λ much larger than f (D) or
f (D
).
′
′
We ﬁrst observe that, somewhat counterintuitively, the
attack does not lose its eﬀectiveness for larger values of λ
(corresponding to smaller decrements to the privacy bud-
get). Consider the success probability of the attacker af-
ter a single interaction with the Laplacian mechanism with
λ = 106, f (D) = 100 and f (D
) = 101. The consumed pri-
−6, which would nominally imply a neg-
vacy budget is ϵ = 10
−6) ≈
ligible increase in the adversary’s conﬁdence (exp(10
−6) if its mathematical guarantee held true. Instead,
1 + 10