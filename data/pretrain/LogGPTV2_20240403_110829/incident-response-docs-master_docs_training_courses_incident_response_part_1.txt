---
style: slides
cover: assets/slides/incident_response/incident_response.001.jpeg
title: Incident Response Training
description: This is an open-source version of PagerDuty's Incident Response training course. What started as an internal course to train new Incident Commanders at PagerDuty has since developed into training that we now deliver publicly. Learn more about incident response and the role of an Incident Commander.
pdf: /assets/pdf/pagerduty_incident_response_training_public.pdf
---
!!!info "Incident Response Training Course (2018)"
    This is an open-source version of "Incident Response Training", our PagerDuty training course for incident response and incident command. It started as an internal course to train new Incident Commanders and has since developed into one that we now deliver publicly. This is a snapshot of what the training looked like in 2018. The latest version is now part of our [PagerDuty University](https://university.pagerduty.com/) courses.
    It includes lots of introductory information on our process, and details on the Incident Commander role specifically. All the information is already available as part of our [public documentation](https://response.pagerduty.com), this is just a different way of presenting it that's hopefully more engaging. Feel free to use this as a base for training in your own organization.
    The text presented here is a semi-accurate transcription of how the training was usually delivered. You can also watch a [video](#video) of an even older version of this course if you prefer.
---
### Introduction
![001](../../assets/slides/incident_response/incident_response.001.jpeg)
_001. "Incident Response Training"._
Hi, I'm Rich, and welcome to "Incident Response Training". This is a shorter version of our internal training at PagerDuty, which we use to train up our new Incident Commanders. It's been slightly adapted for a wider audience, but the majority is exactly what we run ourselves. We're not going to be able to cover everything, otherwise we'd be here for a few days, but I'll cover some of the most important parts of our process. I'll try to keep this as short as I can.
Actually, how long do we have? Can someone keep track of time for me? That would be great, thanks!
---
### Learn How to Effectively Manage Incidents
![002](../../assets/slides/incident_response/incident_response.002.jpeg)
_002. Learn how to effectively manage incidents._
The goal of this session is to give you an understanding of how to effectively manage incidents within your organization. I'll describe the process we use at PagerDuty for managing critical incidents, and talk in more detail about a specific role called the "Incident Commander".
This isn't a sales pitch. I'm not on our sales team, and this isn't a talk about how to use PagerDuty to manage your incidents (although obviously it would be awesome if you did). The intent today is to introduce you to how we manage incidents internally at PagerDuty, and provide you with lots of practical information you can take away to your own organizations to either start or improve your own incident response processes.
---
### Replace Chaos with Calm
![003](../../assets/slides/incident_response/incident_response.003.jpeg)
_003. Replace chaos with calm._
Let's start with a quick question. How does incident response usually go in your organization today? Is it a smooth and streamlined process, or is it a lot of people talking over one another? For most of you it's probably going to be somewhere in the middle.
We want less of the latter, and more of the former. We want to replace chaos with calm. Panic and chaos are not good during an incident, they only exacerbate things and causes more confusion. We want things to be calm and organized.
---
### What is Incident Response?
![004](../../assets/slides/incident_response/incident_response.004.jpeg)
_004. What is incident response? [Docs Reference](../../before/what_is_an_incident/#what-is-incident-response)_
So when we talk about incident response, what we're really talking about is an organized approach to addressing and managing an incident. This is how we define incident response at PagerDuty. They key here is on the word _organized_. We don't want to be running around in a panic anytime an alert goes off. We want our response to be almost routine, and for everyone to work together like a well-oiled machine.
There's a quote I really like from an excellent book called [Incident Management for Operations](https://learning.oreilly.com/library/view/~/9781491917619/) that's appropriate here,
> Fire is not an emergency to the fire department . . . [Y]ou expect a rapid response from a group of professionals, skilled in the art of solving whatever issues you are having.
---
### Goal of Incident Response
![005](../../assets/slides/incident_response/incident_response.005.jpeg)
_005. Goal of incident response. [Docs Reference](../../before/what_is_an_incident.md#what-is-incident-response)_
It may surprise you to learn the goal of incident response isn’t just about solving the problem. Give a thousand monkeys a keyboard and enough time and they can probably solve your problem. That's not good incident response. We want to solve the problem in a way which limits the damage caused, and reduces the recovery time and costs. I don’t just mean _financial_ cost either, there’s a cost associated with engineer health too. Constantly waking people up at 3am can have a dramatic negative effect on their health and happiness.
If financial impact is all you care about though, let's not forget that **people are expensive**. In a large organization, a phone bridge with 100 people sitting there mostly idle for several hours is not unheard of. If each of those people cost ~$100/hour, that’s $10K every hour! That’s _really_ expensive to the business.
---
### What is an Incident?
![006](../../assets/slides/incident_response/incident_response.006.jpeg)
_006. What is an incident? [Docs Reference](../../before/what_is_an_incident.md#what-is-an-incident)_
Before we can respond to an incident though, we need to [define what an incident actually is](../../before/what_is_an_incident.md). It sounds silly, but if you’re not sure whether something’s an incident, you don’t know whether to respond to it.
Here is PagerDuty’s definition of an incident,
> An unplanned disruption or degradation of service that is actively affecting customers' ability to use the product.
"Customers" here doesn't just refer to external customers, but can refer to internal customers too.
Your definition might be different, and that’s OK. I just wanted to give you an idea of the kind of definition that can get you started. You want your definition to be simple, no more than a sentence, and easily understood by anyone.
You may notice that this is quite a broad definition though. A typo technically fits this description. As does a full outage. Obviously these are very different scenarios. So we do have something else too.
---
### What is a Major Incident?
![007](../../assets/slides/incident_response/incident_response.007.jpeg)
_007. What is a major incident? [Docs Reference](../../before/what_is_an_incident.md#what-is-a-major-incident)_
We also have something we call a **major incident**. This is any incident where we require a coordinated response between teams. Again, this is just our definition at PagerDuty, feel free to use your own.
The intention behind this definition is that sometimes incidents can be handled by a single team, maybe the owners of a service that's having trouble. That rarely requires a large response in and of itself. But as soon as they need to involve another team, whether it's customer support, or database administrators, then we declare it to be a major incident and kick off a much larger response. The _coordination_ is key here, and we’ll talk more about this later.
But this still covers quite a range of potential incidents. We can get more granular.
---
### Severity Levels
![008](../../assets/slides/incident_response/incident_response.008.jpeg)
_008. Severity levels. [Docs Reference](../../before/severity_levels.md)_
We also use [severity levels](../../before/severity_levels.md) to determine how severe an incident is, and what type of response it gets. We use `SEV-5` through `SEV-1` for our levels, but you may use a different scheme, `P0` through `P5`, or maybe even emoji, 🔥 through 💩, etc.
Let's imagine you're looking at a graph of traffic to your website. You can typically determine severity based on how drastically your metrics are affected. So as your website traffic drops, the severity increases.
You will usually reach a point where you've set some predefined target or watermark, where as soon as the metric passes, you automatically consider something a major incident. At PagerDuty it's the difference between a `SEV-3` and `SEV-2`, but it may be different for your organization. Then as things get even more dire, we get into our `SEV-2`'s and our `SEV-1`'s when things completely flatline.
Having pre-defined thresholds and metrics can allow you to have automatic triggers for your response process.
???+ aside hide-arrow "We recommend using metrics tied to business impact."
    Metrics can be very useful, and often work best when they're tied to _business impact_. For example, a metric we monitor at PagerDuty is "number of outbound notifications per second", at Amazon it could be "number of orders per second", at Netflix it might be "stream starts per second", etc. Monitoring these important business metrics will then let you use automation to determine the severity of an incident and the type of response you use.
    If you use metrics that aren't tied to business impact (e.g. "CPU usage is high on a host"), then it's difficult, and sometimes impossible, to determine the severity of an incident associated with that metric.
    You want to use a metric that lets you know how your business is doing, not how a particular piece of equipment is doing.
---
### Anyone Can Trigger Incident Response
_![009](../../assets/slides/incident_response/incident_response.009.jpeg)_
_009. Anyone can trigger incident response at any time._
But sometimes you won't know the impact straight away. Or maybe your metric hasn't reached the predefined threshold yet. We still need a way for a human to jump in and call something a major incident.
So even though we have automation at PagerDuty, we also have a mechanism whereby anyone can trigger our major incident response process at any time. This is very important for us. We've found that **lowering the barrier to triggering incident response has lead to a dramatic increase in the speed with which incidents are resolved**.
We don't want people to sit on something because the official alarm hasn't gone off yet. If customer support gets lots of requests very quickly, it's a good sign there's something wrong, and we need them to be able to raise the alarm. We've even had interns trigger our incident response process in their first week. If the janitor walks past a graph and thinks it looks wrong, I want them to be able to trigger incident response.
We were initially hesitant to introduce this, as we feared it would lead to lots of false positives. People triggering the alarm in an abundance of caution and it not really being an incident. But we've seen quite the opposite, people are pretty good at policing this themselves. Only twice have we had it be a false alarm, and both times it was warranted based on the information available at the time. Even if you do get a false positive now and then, you can use it as free practice.
---
### Triggering Incidents via Chat
![010](../../assets/slides/incident_response/incident_response.010.jpeg)
_010. Triggering incidents via chat. [Docs Reference](../../resources/chatops.md#ic-page)_
So how do we let humans trigger the process? We do it with a [chat command](../../resources/chatops.md), but don’t feel like that’s the only right way. I just wanted to demonstrate how we do it to give you an idea. You can do it however your want. Air horn, flashing light in the office, hire a mariachi band, etc.
The point is you want a way to trigger your response that's fast, easy, and available to everyone.
---
### Peacetime vs Wartime
![011](../../assets/slides/incident_response/incident_response.011.jpeg)
_011. Peacetime vs Wartime. [Docs Reference](../../before/what_is_an_incident.md#mentality-shift)_
Once an incident is triggered, we need to switch our mode of thinking. We need a [mentality shift](../../before/what_is_an_incident.md#mentality-shift). We want a distinction between “normal operations” and “there’s an incident in progress”. We need to switch decision making from peacetime to wartime. From day-to-day operations, to defending the business.
Something that would be considered completely unacceptable during normal operations, such as deploying code without running any tests, might be perfectly acceptable during a major incident when you need to restore service quickly.
The way you operate, your role hierarchy, and the level of risk you’re willing to take will all change as we make this shift.
---
### Normal vs Emergency
![012](../../assets/slides/incident_response/incident_response.012.jpeg)
_012. Normal vs Emergency. [Docs Reference](../../before/what_is_an_incident.md#mentality-shift)_
Some people don’t like the peacetime/wartime analogy, so you can call it what you want. Normal/Emergency.
---
### OK vs Not OK
![013](../../assets/slides/incident_response/incident_response.013.jpeg)
_013. OK vs Not OK. [Docs Reference](../../before/what_is_an_incident.md#mentality-shift)_
Or OK/NOT OK. What you call it isn't as important as being able to make the mental shift.
---
### Incident Command System
![014](../../assets/slides/incident_response/incident_response.014.jpeg)
_014. Incident Command System (ICS). [Docs Reference](../../training/overview.md#national-incident-management-system-nims)_
So let’s talk about our process a bit more. The way we do incident response at PagerDuty isn’t something we invented ourselves, it is heavily based on the [Incident Command System](https://en.wikipedia.org/wiki/Incident_Command_System), usually abbreviated to ICS.
ICS was developed after some [devastating wildfires in Southern California](https://en.wikipedia.org/wiki/Laguna_Fire) in 1970. Thousands of firefighters responded, but found it difficult to work together. They knew how to fight fires individually, but lacked a common framework to work effectively as a larger group.
After the fires, an interagency group called [FIRESCOPE](https://en.wikipedia.org/wiki/FIRESCOPE) (Which believe it or not is an acronym for "FIrefighting REsources of Southern California Organized for Potential Emergencies") was formed and set out to develop two systems for managing wildland fire. One of those systems became known as ICS, and eventually became a national model for command structures at any major incident.
In 2004, the [National Incident Management System (NIMS)](https://en.wikipedia.org/wiki/National_Incident_Management_System) was established by FEMA, and is now used as the standard for emergency management by all public agencies in the United States. NIMS defines several operational systems as part of it, of which ICS is one.
It’s used by everyone from the local fire department responding to a house fire, to the US government responding to a natural disaster. It provides a standardized response framework that everyone is familiar with.
NIMS and ICS are the basis of the process we use at PagerDuty, however we have heavily modified it for our needs. Turns out that things can be streamlined a bit when human life isn't on the line.
---
### Incident Response Around The World
![015](../../assets/slides/incident_response/incident_response.015.jpeg)
_015. Incident response around the world. [Docs Reference](../../training/overview.md#incident-response-around-the-world)_
It's worth noting that even though our process is based on the US systems, NIMS and ICS, there are many similar systems in use all over the world. While many are also based on ICS, some were developed separately, yet offer many of the same features.
I particularly like [the UK system](https://en.wikipedia.org/wiki/Gold%E2%80%93silver%E2%80%93bronze_command_structure), simply because it has a role called the "Gold Commander", which just sounds like a Bond villain.
When developing our process at PagerDuty, we looked at a few of the other systems in use around the world, and chose the bits we liked the most to add to our own.
???+ aside hide-arrow "Emergency Management Around the World"
    If you're interested in learning more about the systems in use by other countries, we have [links to some official resources](../../resources/reading.md#official-resources).
    There's also a book available from the US FEMA website, called "[Comparative Emergency Management: Understanding Disaster Policies, Organizations, and Initiatives from Around the World](https://training.fema.gov/hiedu/aemrc/booksdownload/compemmgmtbookproject/)" where it compares the systems used by about 30 different countries.
---