[ 92 ]
Deployment Methodologies Chapter 4
When building servers from bare metal, two fundamental approaches are suitable in most
environments. The first is to build the servers manually using either optical media or, more
commonly now, a USB drive. This is a slow, interactive process that is not repeatable at
scale, and hence it is not recommended for any environments other than those containing
just a handful of physical servers, where the requirement to build new machines is minimal
and infrequent.
The other most viable option for building at scale in the repeatable, consistent manner that
we have advocated throughout this book so far is to boot physical servers over the network,
using a Pre-eXecution Environment (PXE). This involves loading a tiny boot environment
from a network server, and then using this to load the Linux kernel and associated data. In
this manner, it is possible to bring up an installation environment without the need for any
form of physical media. Once the environment is up, we would use an unattended
installation method to allow the installation to complete without any intervention from the
user.
We will cover these methods in detail later in this book, as well as repeatable techniques for
configuring the servers once they are built. In the meantime, however, it will suffice to
simply state that, for building out physical Linux servers in an enterprise, PXE booting
coupled with an unattended installation is the route that is easiest to automate and will
produce the most repeatable results.
Deploying to traditional virtualization
environments
Traditional virtualization environments are those that predate what we know today as
cloud environments—that is to say, they are straightforward hypervisors on which
operating systems are run. Commercial examples such as VMware are common, as well as
their open source counterparts such as Xen and KVM (and frameworks built off of these,
such as oVirt).
As these technologies were originally built to supplement traditional physical
environments, they present several possible options for building out your Enterprise Linux
estate. For example, most of these platforms support the same network-booting capabilities
as their bare-metal counterparts, and hence we could actually just pretend they are bare
metal and continue with a network booting methodology.
[ 93 ]
Deployment Methodologies Chapter 4
However, virtualized environments introduced something that was difficult to achieve in
physical environments because of the differences in hardware between the bare-metal
devices on which they all ran—templates. A templated virtual machine is quite simply a
deployable snapshot of a preconfigured virtual machine. Hence, you might build out the
perfect CentOS 7 image for your enterprise, integrate your monitoring platform, perform all
of the security hardening required, and then, using tools built into the virtualization
platform itself, turn it into a template. The following is a screenshot of the CentOS 7
templates in the author's lab environment:
Each of these templates is a fully configured CentOS 7 base image ready to be deployed,
with all pre-deployment work such as removal of SSH host keys completed. As a result, all
an administrator has to do is to select the appropriate template and click on the New
VM button—the process will be similar in platforms other than RHV, as most mainstream
virtualization solutions provide this functionality in some guise.
Note that, to keep the examples accessible, I have used the GUI as the
primary process for creating a new VM. Nearly all virtualization and
cloud platforms have APIs, command-line interfaces, and even Ansible
modules that can be used to deploy virtual machines, and in an enterprise
setting, these would scale far better than the GUI itself. Given the wide
variety of environments available, this is left as an exercise for you to
explore.
[ 94 ]
Deployment Methodologies Chapter 4
This is in itself a fairly straightforward process, but it requires a little care and attention. For
example, nearly all Linux servers these days have SSH turned on, and the SSH daemon on
each server has a unique host identification key that is used to prevent (amongst other
things) man-in-the-middle attacks. If you template a preconfigured operating system, you
will also template these keys, which means a distinct possibility of duplicates across your
environment. This reduces security quite considerably. It is hence very important to
perform several steps to prepare your virtual machine before turning it into a template, and
one such common step is to delete the SSH host keys.
Servers created using the PXE method do not suffer from this problem, as they are all
installed from scratch and hence there are no historic log entries to clean up and no
duplicate SSH keys.
In Chapter 5, Using Ansible to Build Virtual Machine Templates for Deployment, we will go into
detail on creating virtual machine templates suitable for templating using Ansible.
Although both the PXE boot and template deployment methodologies are equally valid for
virtualized environments, most people find the templated route to be more efficient and
easier to manage, and for this reason, I also advocate it (for example, most PXE boot
environments need to know the MAC address of the network interface used on the physical
or virtual server being deployed—this is not a necessary step in template deployment).
Deploying to cloud environments
The most recent incumbent to Enterprise Linux architectures (barring of course containers,
which is another discussion entirely) is the cloud provisioning environment. This might be
through a public cloud solution such as Amazon Web Services (AWS), Microsoft
Azure, Google Cloud Platform (GCP), or one of the myriad of smaller providers that have
sprung up in recent years. It might equally be through an on-premise solution such as one
of the variants of the OpenStack project or a proprietary platform.
These cloud environments have radically changed the life cycle of Linux machines in the
enterprise. Whereas on bare-metal or traditional virtualized architectures, Linux machines
were cared for, nurtured, and repaired if ever they failed, cloud architectures are built on
the premise that each machine is more or less expendable, and that if it fails, a new one is
simply deployed in its place.
As a result, PXE deployment methodologies are not even possible in such environments,
and instead they rely on pre-built operating system images. These are in essence just a
template either created by a third-party vendor or prepared by the enterprise.
[ 95 ]
Deployment Methodologies Chapter 4
Whether you go with a commercial provider or build an on-premise OpenStack
architecture, you will find a catalog of available operating system images for you to choose
from. Generally, those provided by the cloud provider themselves are trustworthy, though
depending on your security requirements, you may find those provided by external parties
suitable as well.
For example, here is a screenshot of the recommended operating system images available
for OpenStack:
As you can see from the table of contents, most of the major Linux distributions are
represented here, which immediately saves you the task of building the basic operating
system itself. The same is true of AWS:
[ 96 ]
Deployment Methodologies Chapter 4
In short, if you are using a cloud environment, you will be spoiled for choice for base
operating system images from which to get started. Even so, it is unlikely this choice will be
sufficient for all enterprises. For example, using a pre-built, cloud-ready image does not
negate requirements for things such as enterprise security standards, monitoring, or log
forwarding agent integration, and a myriad of other things that are so important for the
enterprise. Before we proceed, it is worth noting that you can, of course, create your own
images for your chosen cloud platforms. In the interests of efficiency though, why re-invent
the wheel? If someone has already completed this step for you, this is something that you
can effectively delegate elsewhere.
Although most ready-made operating system images are trustworthy, you
should always exercise caution when selecting a new one, especially if it
has been created by an author you are unfamiliar with. There is no way to
know for sure what the image comprises and you should always carry out
due diligence when selecting an image to work with.
[ 97 ]
Deployment Methodologies Chapter 4
Assuming that you do choose to proceed with a pre-made cloud-ready image, the post-
installation configuration work can all be handled neatly by Ansible. In fact, the steps
required are almost identical to those required to build templates for traditional
virtualization platforms, and we shall again cover this process in detail a little later in this
book.
Docker deployments
Docker deployments are a special case in our discussion on Linux environments. In
practical terms, they share a lot in common with cloud environments—Docker images are
built based upon pre-existing minimal OS images and are often built using the native
Docker toolchains, though automation with Ansible is entirely possible.
As Docker is a special case, we will not be focusing on it in this book, though it is important
to note that Docker, being a recent incumbent into the presence of Linux in the enterprise, is
actually designed around many of the principles we have already considered in this book.
Let's briefly consider the Dockerfile used to create the official nginx container.
For those not familiar with Docker, a Dockerfile is a flat text file that
contains all the directives and commands that are required to build up a
container image for deployment.
At the time of writing, this file contains the following:
#
# Nginx Dockerfile
#
# https://github.com/dockerfile/nginx
#
# Pull base image.
FROM ubuntu:bionic
# Install Nginx.
RUN \
add-apt-repository -y ppa:nginx/stable && \
apt-get update && \
apt-get install -y nginx && \
rm -rf /var/lib/apt/lists/* && \
echo -e "\ndaemon off;" >> /etc/nginx/nginx.conf && \
chown -R www-data:www-data /var/lib/nginx
[ 98 ]
Deployment Methodologies Chapter 4
Although not based on Ansible, we can see the following in the preceding code block:
1. The FROM line near the top defines a minimal Ubuntu base image on which to
perform the rest of the configuration—this can be thought of as your SOE Linux
image that we have discussed for other platforms.
2. The RUN command then performs the steps necessary to install
the nginx package and perform some housekeeping to keep the image tidy and
minimal (reducing space requirements and clutter).
The code then continues as follows:
# Define mountable directories.
VOLUME ["/etc/nginx/sites-enabled", "/etc/nginx/certs",
"/etc/nginx/conf.d", "/var/log/nginx", "/var/www/html"]
# Define working directory.
WORKDIR /etc/nginx
# Define default command.
CMD ["nginx"]
# Expose ports.
EXPOSE 80
EXPOSE 443
Continuing our analysis of this file, we can see the following:
1. The VOLUME line defines which directories from the host filesystem can be
mounted within the container.
2. The WORKDIR directive tells Docker which directory to run the CMD that follows it
in—think of it as a boot-time configuration.
3. The CMD line defines the command to run when the container starts—a
microcosm of the process of defining which services will start at boot time in a
full Linux system image.
4. Finally, the EXPOSE lines define which ports the container should expose to the
network—perhaps a little like a firewall might allow certain ports through.
In short, the native process to build a Docker container is very much aligned with our
defined build process for an Enterprise Linux environment—hence, we can proceed in
confidence with this process. With this in mind, we will now explore the process of
ensuring our builds are as tidy and efficient as possible.
[ 99 ]
Deployment Methodologies Chapter 4
Keeping builds efficient
Knowing the fundamentals of your Linux environment, as we discussed in the last section,
is vital to working out your deployment methodology. Although there exist some
similarities between the build processes themselves (especially between traditional
hypervisors and cloud environments), knowing these differences enables you to make
informed decisions about how to deploy Linux throughout your enterprise.
Once you have chosen the methodologies most appropriate to your environment, it's
important to consider a few principles to ensure your process is streamlined and efficient
(again, bywords of Enterprise Linux deployments). We will cover these here to proceed into
the real in-depth, hands-on work in the remainder of this book. Let's get started by looking
at the need for simplicity in our builds.
Keeping your builds simple
Let's start to put some practical application of our earlier discussion on the importance of
SOEs to our Linux build processes. Whatever route you choose and whatever your
environment looks like, one key facet you should consider is to keep your build standard as
simple and concise as possible.
No two enterprise environments are the same, and hence the build requirements for each
enterprise will certainly be different. Nonetheless, a common set of example requirements
is given here to demonstrate the kinds of things that will be needed in the build process:
Monitoring agents
Log forwarding configuration
Security hardening
Core enterprise software requirements
NTP configuration for time synchronization
This list is just a start, and every enterprise will be different, but it gives you an idea of the
kinds of things that will go into a build. However, let's start to look at some of the edge
cases to your build process. It is fair to say that each Linux server will be built with a
purpose in mind and, as such, will run some form of application stack.
Again, the application stack is certain to vary between enterprises, but examples of the
kinds of applications that might commonly be required are as follows:
A web server such as Apache or nginx
The OpenJDK environment for Java workloads
[ 100 ]
Deployment Methodologies Chapter 4
A MariaDB database server
A PostgreSQL database server
NFS file-sharing tools and kernel extensions
Now, in your standardization process, when you originally defined your SOE, you may
even have gone to the lengths of already specifying the use of (just as an example)
OpenJDK 8 and MariaDB 10.1. Does this mean you should actually include these in your
build process?
The answer is almost always, no. Quite simply, adding these applications adds to the
complexity of the build and to post-install configuration and debugging. It also reduces
security—but more on that shortly.
Let's suppose we standardize on MariaDB 10.1 and include that in our base operating
system image (and hence every single Linux machine deployed contains it), knowing that
only a subset of the machines in operation will actually ever use it.
There are several reasons for not including MariaDB in the base image:
An install of just the server components of MariaDB 10.1 takes around 120 MB,
depending on your operating system and packaging—there will also be
dependency packages but let's just start with this. Although storage is cheap and
plentiful these days, if you deploy 100 servers across your environment (actually
a small number for most enterprises), that's approximately 11.7 GB of space
dedicated to a package you don't need. The actual figure will be far higher as
there will be dependency packages to install and so on.
This may also have a knock-on effect on backups and the storage required for
these, and indeed any virtual machine snapshots if you use that in the enterprise.
If an application arrives that requires MariaDB 10.3 (or indeed, the business
decides to update its standard to 10.3), then the images need to be upgraded or
possibly version 10.1 uninstalled before 10.3 is installed. This is an unnecessary
level of complexity when a minimal Linux image could just have received an
updated MariaDB workload.
You need to ensure that MariaDB is turned off and firewalled off when not
required to as to prevent any misuse—this is an additional auditing and
enforcement requirement that again is unnecessary on many servers where
MariaDB isn't used.
[ 101 ]
Deployment Methodologies Chapter 4
There are other security considerations too, but the key message here is that it is wasteful
on resources and time. This doesn't, of course, only apply to MariaDB 10.1—that is simply
an example, but it serves to show that, as a rule, application workloads should not be
included in the base operating system definition. Let's take a more detailed look at the
security requirements for our builds now.
Making your builds secure
We have already touched on security and not installing or running unnecessary packages.
Any running service provides a potential attack vector for an intruder, and whilst
hopefully, you will never have one inside your enterprise network, it is still good practice
to build the environment in a manner that is as secure as possible. This is especially true of
services that come configured with default passwords (and in some cases, with no
password configured at all—though this is thankfully becoming rare now).
These principles apply when defining the build itself too. Don't create a build with weak
static passwords, for example. Ideally, every build should be configured to obtain even
initial credentials from an external source, and although there are a myriad of ways to
achieve this, you are encouraged to look up cloud-init if this is a new concept to you.
There are cases, especially in legacy environments, where you may need some initial
credentials to allow access to the newly built server, but reusing weak passwords is
dangerous and opens up the possibility of the newly built server being intercepted before it
is configured and some kind of malware planted on it.
In short, the following list provides some sound guidance on ensuring secure builds:
Don't install applications or services that are not required.
Do ensure services that are common to all builds but require post-deployment
configuration are disabled by default.
Don't re-use passwords even for initial access and configuration if at all possible.
Do apply your enterprise security policy as early as possible in the process—in
the build process of the image or server if possible, but if not, as soon as possible
after installation.
These principles are simple yet fundamental, and it is important to adhere to them.
Hopefully, a situation will never arise where it matters that they have been applied, but if it
does, they might just stop or sufficiently impede an intrusion or attack on your
infrastructure. This, of course, is a topic that deserves its own book, but it is hoped these
pointers, along with the examples in Chapter 13, Using CIS Benchmarks, will point you in
the right direction. Let's take a brief look now at ensuring our build processes are efficient.
[ 102 ]
Deployment Methodologies Chapter 4
Creating efficient processes
Efficient processes are supported heavily by automation, as this ensures minimal human
involvement and consistent, repeatable end results. Standardization also supports this, as it
means that much of the decision-making process has already been completed, and so all
people involved know exactly what they are doing and how it should be done.
In short, stick to these principles outlined in this book and your build processes will, by
their very nature, be efficient. Some degree of manual intervention is inevitable, even if it
involves choosing a unique hostname (though this can be automated) or perhaps the
process of a user requesting a Linux server in the first place. However, from here, you want
to automate and standardize wherever possible. We will follow this mantra throughout this