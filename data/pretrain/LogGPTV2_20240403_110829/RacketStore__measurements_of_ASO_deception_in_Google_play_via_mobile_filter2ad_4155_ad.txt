including information about potentially unwanted programs (PUP).
177 of these apps were flagged malicious by more than one
VirusTotal AV tool. We found at least one of these flagged apps
in 183 unique devices: 122 devices controlled by workers and 61
devices of regular users. We found 70 unique mobile app identifiers
with at least one VT engine flag, that received at least one review
from our participants: 64 of these apps were reviewed by workers,
and 9 apps were reviewed by regular users.
Since a single VT flag may be a false positive, we further com-
pared the occurrence of the most malicious malware samples (flagged
by more than 7 VirusTotal engines) in regular user devices versus
worker devices. The 7 flag threshold exceeds the value 4 identified
in [64], and most of these samples were later removed from Google
Play. Figure 12 shows that malicious samples are more likely to
appear in several worker devices when compared to regular users.
Anti Virus (AV) Apps. To determine if participants had sufficient
security concerns to install anti-virus (AV) apps, we first identified
250 anti-virus (AV) apps from Google Play, by doing a search on the
app category in the website. We have joined these apps against the
apps installed in each of the participant devices that sent at least
one snapshot. We found only 19 devices that installed 15 AV apps: 8
worker devices, 7 regular user devices, and 4 unknown (i.e., either
Google testing our infrastructure or participants who managed to
bypass our invitation code).
Participant Feedback. We asked participants if they (1) are con-
cerned about installing malware apps on their devices, (2) have anti
virus software installed, and (3) are concerned about the privacy
of their device data, including contacts, login info, pictures, videos,
text messages, location. Two workers were not concerned about
malware or privacy leaks and did not have AV apps installed. One
worker said “I am confident on the ability of my phone to prevent
any mishap”.
Five workers reported being concerned about malware; four
claimed to use AV apps. One worker claimed that “I find a lot of
apps like this, which contain a lot of viruses.” However, he also
claimed to not be concerned about privacy leaks because “I have
5 devices, 3 mobile devices and 2 computers. 2 out of the 3 mobiles, I
use for apps testing and review, 1 mobile I use for my personal work.”
Summary of Findings. Worker-controlled devices have lower in-
stantaneous malware infection rate than regular devices. However,
malware installed by worker devices tends to be flagged by more
anti-virus engines. Workers had mixed attitudes toward privacy and
installing malware and AV apps. This suggests potential vulnerabil-
ities and concerns among workers toward keeping apps installed
for longer intervals.
7 FAKE REVIEW DETECTION
In this section we investigate whether the app usage data collected
by RacketStore (§ 5), can identify apps installed to be promoted, thus
detect fake app installs and reviews. For this, we first introduce
app usage features (§ 7.1) then investigate their ability to train
supervised models to distinguish promotion-related vs. personal
app use (§ 7.2).
7.1 App Usage Features
We extracted the following features for each app installed on
participant devices: (1) the number of accounts registered on the
648
ML Algorithm Precision Recall
99.67%
99.23%
99.00%
96.88%
94.54%
99.78%
99.33%
99.22%
96.88%
90.99%
XGB
RF
LR
KNN
LVQ
F1
99.72%
99.27%
99.11%
96.88%
92.73%
Table 1: Precision, recall, and F-1 measure of app usage clas-
sifier (CV k = 10) using Extreme Gradient Boosting (XGB),
Random Forrest (RF), Logistic Regression (LR), K-Nearest
Neighbors (KNN), and Learning Vector Quantization (LVQ).
XGB performed the best.
device that reviewed the app, before RacketStore was installed,
while it was installed, and after it was uninstalled, (2) the install-
to-review time (§ 6.3), (3) inter-review times, i.e., statistics over
the time difference between all consecutive reviews posted for the
app from accounts registered on the device, (4) whether app was
opened on multiple days, (5) the number of snapshots per day
when the app was the on-screen app, (6) the number of snapshots
collected per day from device, (7) inner retention, i.e., the duration
over which the app was installed on the device (while RacketStore
was installed), whether the app was installed before RacketStore
and was still installed when RacketStore was uninstalled, (8) the
number of normal and dangerous permissions requested, (9) the
number of permissions requested by the app that have been granted
and denied by the user, (10) the number of flags raised by VirusTotal
AV tools, and (11) the number of times the app was installed and
uninstalled while RacketStore was installed.
7.2 App Classification
We use these features and the datasets of § 5 to train an app classi-
fier that determines if an app has been installed for promotion or
personal use.
Training and Validation Datasets. We use the 178 worker and
88 regular devices from which we have received at least two days
of fast and slow snapshots. For the other devices we lack enough
data to extract good features.
We have set aside randomly selected 20% (i.e., 38) of the worker-
controlled devices and 42% (i.e., 37) of the regular devices. We
use these devices to select a set of train-and-validate apps with
suspicious and regular usage. Specifically, we label an app to be
suspicious if (1) it was advertised by workers for promotion on the
Facebook groups we infiltrated (§ 2), (2) it was installed on at least
five worker devices, and (3) was not installed on any regular devices.
The rationale for this selection is that co-installing apps that are not
popular and we know have been promoted, is likely the result of
ASO work. Further, we label an app to be non-suspicious or regular,
if (1) it was not installed on any worker-controlled device, (2) was
installed in at least one regular device, and (3) has received at least
15,000 reviews. We have identified 1,041 suspicious apps among
the ones installed on the above 38 worker-controlled devices, and
474 non-suspicious apps among those installed on the 37 training
regular devices.
We use these apps to build a train-and-validate app usage dataset
consisting of 2,994 suspicious instances and 345 non-suspicious
instances. An instance consists of an app A and a device D on
ML Algorithm Precision Recall
93.81%
96.06%
89.03%
90.58%
82.84%
96.81%
93.95%
96.64%
94.29%
96.40%
XGB
RF
SVM
KNN
LVQ
F1
95.29%
94.99%
92.68%
92.40%
89.11%
Table 2: Precision, recall, and F-1 measure of device classifier
(CV k = 10) using Extreme Gradient Boosting (XGB), Ran-
dom Forrest (RF), K-Nearest Neighbors (KNN), Learning Vec-
tor Quantization (LVQ),and Support Vector Machines (SVM).
XGB performed the best.
the number of device-registered Gmail and non-Gmail accounts,
and the number of distinct account types, (6) the number of apps
installed on the device that have been reviewed from accounts
registered on the device, and (7) the total number of apps reviewed
by accounts registered on the device. For most features we use
both the user-installed apps and the pre-installed apps, since even
the use of pre-installed apps like the app store, e-mail, maps, and
browser apps can distinguish regular devices from those controlled
by workers.
8.2 Device Classification
We evaluate the ability of these features to train classifiers that dif-
ferentiate between devices controlled by workers and regular users.
For this, we use the 178 worker devices and the 88 regular devices
that have reported snapshots over at least 2 days. We prioritize
precision, since a low precision would lead the app market to take
wrong actions against many regular devices [90].
Table 2 compares the performance of five supervised learning al-
gorithms trained with the device usage features introduced in § 8.1.
KNN achieved best performance for K = 5. To balance the worker
and regular user device classes, we oversampled the minority class
using the SMOTE algorithm [33]. We use 10-fold cross-validation
over the data from the 178 worker and 88 regular devices. Extreme
Gradient Boosting (XGB) outperforms the other algorithms, achiev-
ing an F1-measure of 95.29% and AUC of 0.9455. The precision is
96.81% and the false positive rate is 1.41%.
When we undersample the majority class, XGB’s recall decreases
to 92.97% with an F-1 value of 95.18% and AUC of 0.9074. When us-
ing no sampling strategy the F-1 increases to 96.86%, at the expense
of the AUC (0.9083).
Figure 14 shows the top 10 most important features in classifying
devices as worker-controlled or regular, as measured by the mean
decrease in Gini. Four features stand out, confirming their ability to
detect worker-controlled devices, i.e., (1) the total number of apps
reviewed from accounts registered on the device, (2) the percent of
installed apps that were detected to have been used suspiciously by
the classifier of § 7, (3) the number of stopped apps on the device
and (4) the average number of reviews posted from an account
registered on the device.
Figure 15 shows the scatterplot of app suspiciousness vs. the total
number of reviewed apps for each of the 178 worker-controlled de-
vices. Out of these 178 devices, 123 devices have organic-indicative
behaviors, with at least one of the installed apps being predicted to
Figure 13: Top 10 most important features for the app classi-
fier, measured by mean decrease in Gini. The number of ac-
counts that have reviewed the app from the device and the
average time between install and review are most important.
which A has been installed, features extracted from the use of A
on the device D (§ 7.1), and a class label (1 for promotion and 0 for
personal usage instance).
Classifier Performance. We evaluate the performance of super-
vised learning algorithms trained with the features introduced in
§ 7.1 on the train-and-validate app usage dataset. For this, we use
repeated 10-fold cross-validation (n=5) over the 2,994 suspicious
app usage instances and 345 regular usage instances. Table 1 shows
the precision, recall and F1-measure of tested algorithms. Extreme
Gradient Boosting (XGB) outperforms the other algorithms, achiev-
ing an F1-measure of 99.72%. KNN achieved best performance for
K = 5.
Figure 13 shows the top 10 most important features to classify
app usage, measured by the mean decrease in Gini [31]. A higher
decrease in Gini indicates higher variable importance. We observe
the importance of the number of accounts registered on the device
that have reviewed the app (§6.2) and the average time between
install and review (§6.3).
Performance Under Balanced Datasets. We also evaluate these
algorithms trained with balanced datasets of promotion and per-
sonal app use instances [90]. Experiments with undersampling the
majority class and oversampling the minority class obtain similar
performance (F-1 value of 98.76% and 99.22% respectively for XGB).
The AUC value is over 0.99 across all the algorithms except for
KNN where the AUC decreased to 0.90 and 0.92 in undersampling
and oversampling respectively. For XGB, the false positive rate is
1.94% when using oversampling.
8 ASO DEVICE DETECTION
We now investigate whether the device usage data collected by
RacketStore (§ 5) can be used to identify devices controlled by app
search optimization workers.
8.1 Device Usage Features
We introduce the following features that model the use of a device:
(1) the number of pre-installed and user-installed apps, (2) app
suspiciousness, i.e., the number of apps that were flagged by the
app classifier of § 7, over the total number of apps installed on
the device, (3) the number of apps that were stopped (§ 6.3), (4)
the average number of apps installed and uninstalled per day, (5)
649
can only be effective for RacketStore users. We also note that ASO
workers would likely be reluctant to install RacketStore without
proper incentives. Therefore, to scale RacketStore’s processing of
all the apps in an app store, the proposed classifiers should be
embedded by app store developers into their pre-installed apps,
e.g., the Google Play Store app or Digital Well Being app. Unlike
third-party apps, such clients have by default the permissions to
access the required data, and are known to access at least app usage
details [5, 49].
Privacy-Preserving Classifiers. We note that our classifiers need
access to sensitive data from user devices, which general users
may be reluctant to share. To address this problem, we propose a
privacy-preserving approach where our pre-trained models (§ 7
and § 8) execute on the user device on locally computed features to
detect ASO activities [4, 81]. This approach will only report ASO
suspicious activities but no private app and device-usage informa-
tion. A red flag can be raised if the user uninstalls or blocks this
pre-installed client (e.g., the Play Store app), and posts suspicious
reviews from accounts registered on such a non-consenting device.
Worker Strategy Evolution. ASO workers may attempt to de-
velop strategies to avoid detection by our classifiers. However, our
engagement-based features exploit the lack of genuine interest of
workers on promoted apps and introduce a tradeoff between de-
tectability and operational costs and exposure to malware. These
features include the number of accounts registered on the device,
the interval between app install and first review, and user interac-
tion with the app (opened, daily app usage) see § 7.1.
Workers may attempt to manipulate these features using per-
haps existing software. However, workers will still need to keep
promoted apps installed for longer intervals, wait more before re-
viewing them, and interact more with them. When promoted apps
are malicious, and workers use personal devices, this may increase