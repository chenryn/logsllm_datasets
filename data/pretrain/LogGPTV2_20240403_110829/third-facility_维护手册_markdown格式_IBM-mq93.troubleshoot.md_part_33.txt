Using the RESET CLUSTER command stops auto-defined cluster sender channels for the affected queue
manager. You must manually restart any cluster sender channels that are stopped, after completing the
RESET CLUSTER command.
IBM MQ troubleshooting and support 209
A queue manager does not rejoin the cluster
After issuing a RESET or REFRESH cluster command the channel from the queue manager to the cluster
might be stopped. Check the cluster channel status and restart the channel.
Symptom
A queue manager does not rejoin a cluster after issuing the RESET CLUSTER and REFRESH CLUSTER
commands.
Cause
A side effect of the RESET and REFRESH commands might be that a channel is stopped. A channel is
stopped in order that the correct version of the channel runs when RESET or REFRESH command is
completed.
Solution
Check that the channels between the problem queue manager and the full repositories are running and
use the START CHANNEL command if necessary.
Related information
Clustering: Using REFRESH CLUSTER best practices
Workload balancing set on a cluster-sender channel is not working
Any workload balancing you specify on a cluster-sender channel is likely to be ignored. Instead, specify
the cluster workload channel attributes on the cluster-receiver channel at the target queue manager.
Symptom
You have specified one or more cluster workload channel attributes on a cluster-sender channel. The
resulting workload balancing is not as you were expecting.
Cause
Any workload balancing you specify on a cluster-sender channel is likely to be ignored. For an explanation
of this, see Cluster channels. Note that you still get some form of workload balancing, based either
on cluster defaults or on properties set on the matching cluster-receiver channel at the target queue
manager.
Solution
Specify the cluster workload channel attributes on the cluster-receiver channel at the target queue
manager.
Related reference
CLWLPRTY channel attribute
CLWLRANK channel attribute
CLWLWGHT channel attribute
NETPRTY channel attribute
210 Troubleshooting and Support for IBM MQ
Out of date information in a restored cluster
After restoring a queue manager, its cluster information is out of date. Refresh the cluster information with
the REFRESH CLUSTER command.
Problem
After an image backup of QM1, a partial repository in cluster DEMO has been restored and the cluster
information it contains is out of date.
Solution
On QM1, issue the command REFRESH CLUSTER(DEMO).
Note: For large clusters, use of the REFRESH CLUSTER command can be disruptive to the cluster while it
is in progress, and again at 27 day intervals thereafter when the cluster objects automatically send status
updates to all interested queue managers. See Refreshing in a large cluster can affect performance and
availability of the cluster.
When you run REFRESH CLUSTER(DEMO) on QM1, you remove all the information QM1 has about the
cluster DEMO, except for QM1's knowledge of itself and its own queues, and of how to access the full
repositories in the cluster. QM1 then contacts the full repositories, and tells them about itself and its
queues. QM1 is a partial repository, so the full repositories don't immediately tell QM1 about all the
other partial repositories in the cluster. Instead, QM1 slowly builds up its knowledge of the other partial
repositories through information it receives as and when each of the other queues and queue managers is
next active in the cluster.
Cluster queue manager force removed from a full repository by mistake
Restore the queue manager to the full repository by issuing the command REFRESH CLUSTER on the
queue manager that was removed from the repository.
Problem
The command, RESET CLUSTER(DEMO) QMNAME(QM1) ACTION(FORCEREMOVE) was issued on a full
repository in cluster DEMO by mistake.
Solution
On QM1, issue the command REFRESH CLUSTER(DEMO).
Note: For large clusters, use of the REFRESH CLUSTER command can be disruptive to the cluster while it
is in progress, and again at 27 day intervals thereafter when the cluster objects automatically send status
updates to all interested queue managers. See Refreshing in a large cluster can affect performance and
availability of the cluster.
Possible repository messages deleted
Messages destined for a queue manager were removed from the SYSTEM.CLUSTER.TRANSMIT.QUEUE
in other queue managers. Restore the information by issuing the REFRESH CLUSTER command on the
affected queue manager.
Problem
Messages destined for QM1 were removed from the SYSTEM.CLUSTER.TRANSMIT.QUEUE in other queue
managers and they might have been repository messages.
Solution
On QM1, issue the command REFRESH CLUSTER(DEMO).
IBM MQ troubleshooting and support 211
Note: For large clusters, use of the REFRESH CLUSTER command can be disruptive to the cluster while it
is in progress, and again at 27 day intervals thereafter when the cluster objects automatically send status
updates to all interested queue managers. See Refreshing in a large cluster can affect performance and
availability of the cluster.
QM1 removes all information it has about the cluster DEMO, except that relating to the cluster queue
managers which are the full repositories in the cluster. Assuming that this information is still correct, QM1
contacts the full repositories. QM1 informs the full repositories about itself and its queues. It recovers the
information for queues and queue managers that exist elsewhere in the cluster as they are opened.
Two full repositories moved at the same time
If you move both full repositories to new network addresses at the same time, the cluster is not updated
with the new addresses automatically. Follow the procedure to transfer the new network addresses. Move
the repositories one at a time to avoid the problem.
Problem
Cluster DEMO contains two full repositories, QM1 and QM2. They were both moved to a new location on the
network at the same time.
Solution
1.Alter the CONNAME in the CLUSRCVR and CLUSSDR channels to specify the new network addresses.
2.Alter one of the queue managers ( QM1 or QM2) so it is no longer a full repository for any cluster.
3.On the altered queue manager, issue the command REFRESH CLUSTER(*) REPOS(YES).
Note: For large clusters, use of the REFRESH CLUSTER command can be disruptive to the cluster
while it is in progress, and again at 27 day intervals thereafter when the cluster objects automatically
send status updates to all interested queue managers. See Refreshing in a large cluster can affect
performance and availability of the cluster.
4.Alter the queue manager so it is acting as a full repository.
Recommendation
You could avoid the problem as follows:
1.Move one of the queue managers, for example QM2, to its new network address.
2.Alter the network address in the QM2 CLUSRCVR channel.
3.Start the QM2 CLUSRCVR channel.
4.Wait for the other full repository queue manager, QM1, to learn the new address of QM2.
5.Move the other full repository queue manager, QM1, to its new network address.
6.Alter the network address in the QM1 CLUSRCVR channel.
7.Start the QM1 CLUSRCVR channel.
8.Alter the manually defined CLUSSDR channels for the sake of clarity, although at this stage they are not
needed for the correct operation of the cluster.
The procedure forces QM2 to reuse the information from the correct CLUSSDR channel to re-establish
contact with QM1 and then rebuild its knowledge of the cluster. Additionally, having once again contacted
QM1, it is given its own correct network address based on the CONNAME in QM2 CLUSRCVR definition.
212 Troubleshooting and Support for IBM MQ
Unknown state of a cluster
Restore the cluster information in all the full repositories to a known state by rebuilding the full
repositories from all the partial repositories in the cluster.
Problem
Under normal conditions the full repositories exchange information about the queues and queue
managers in the cluster. If one full repository is refreshed, the cluster information is recovered from
the other.
The problem is how to completely reset all the systems in the cluster to restore a known state to the
cluster.
Solution
To stop cluster information being updated from the unknown state of the full repositories, all the
CLUSRCVR channels to full repositories are stopped. The CLUSSDR channels change to inactive.
When you refresh the full repository systems, none of them are able to communicate, so they start from
the same cleared state.
When you refresh the partial repository systems, they rejoin the cluster and rebuild it to the complete set
of queue managers and queues. The cluster information in the rebuilt full is restored to a known state.
Note: For large clusters, use of the REFRESH CLUSTER command can be disruptive to the cluster while it
is in progress, and again at 27 day intervals thereafter when the cluster objects automatically send status
updates to all interested queue managers. See Refreshing in a large cluster can affect performance and
availability of the cluster.
1.On all the full repository queue managers, follow these steps:
a.Alter queue managers that are full repositories so they are no longer full repositories.
b.Resolve any in doubt CLUSSDR channels.
c.Wait for the CLUSSDR channels to become inactive.
d.Stop the CLUSRCVR channels.
e.When all the CLUSRCVR channels on all the full repository systems are stopped, issue the command
REFRESH CLUSTER(DEMO) REPOS(YES).
f.Alter the queue managers so they are full repositories.
g.Start the CLUSRCVR channels to re-enable them for communication.
2.On all the partial repository queue managers, follow these steps:
a.Resolve any in doubt CLUSSDR channels.
b.Make sure all CLUSSDR channels on the queue manager are stopped or inactive.
c.Issue the command REFRESH CLUSTER(DEMO) REPOS(YES).
What happens when a cluster queue manager fails
When a cluster queue manager fails, some undelivered messages are sent to other queue managers in
the cluster. Messages that are in-flight wait until the queue manager is restarted. Use a high-availability
mechanism to restart a queue manager automatically.
Problem
If a message-batch is sent to a particular queue manager and that queue manager becomes unavailable,
what happens at the sending queue manager?
IBM MQ troubleshooting and support 213
Explanation
Except for non-persistent messages on an NPMSPEED(FAST) channel, the undelivered batch of messages
is backed out to the cluster transmission queue on the sending queue manager. On an NPMSPEED(FAST)
channel, non-persistent messages are not batched, and one might be lost.
• Indoubt messages, and messages that are bound to the unavailable queue manager, wait until the
queue manager becomes available again.
• Other messages are delivered to alternative queue managers selected by the workload management
routine.
Solution
The unavailable cluster queue manager can be restarted automatically, either by being configured as a
multi-instance queue manager, or by a platform-specific high availability mechanism.
What happens when a repository fails
How you know a repository has failed and what to do to fix it?
Problem
1.Cluster information is sent to repositories (whether full or partial) on a local queue called
SYSTEM.CLUSTER.COMMAND.QUEUE. If this queue fills up, perhaps because the queue manager has
stopped working, the cluster-information messages are routed to the dead-letter queue.
2.The repository runs out of storage.
Solution
1.Monitor the messages on your queue manager log or z/OS system console to detect if
SYSTEM.CLUSTER.COMMAND.QUEUE is filling up. If it is, you need to run an application to retrieve the
messages from the dead-letter queue and reroute them to the correct destination.
2.If errors occur on a repository queue manager, messages tell you what error has occurred and how
long the queue manager waits before trying to restart.
• On IBM MQ for z/OS, the SYSTEM.CLUSTER.COMMAND.QUEUE is disabled for MQGET.
• When you have identified and resolved the error, enable the SYSTEM.CLUSTER.COMMAND.QUEUE so
that the queue manager can restart successfully.
3.In the unlikely event of the repository running out of storage, storage allocation errors are sent to
the queue manager log or z/OS system console. To fix the storage problem, stop and
then restart the queue manager. When the queue manager is restarted, more storage is automatically
allocated to hold all the repository information.
What happens if a cluster queue is disabled for MQPUT
All instances of a cluster queue that is being used for workload balancing might be disabled for MQPUT.
Applications putting a message to the queue either receive a MQRC_CLUSTER_PUT_INHIBITED or a
MQRC_PUT_INHIBITED return code. You might want to modify this behavior.
Problem
When a cluster queue is disabled for MQPUT, its status is reflected in the repository of each queue
manager that is interested in that queue. The workload management algorithm tries to send messages
to destinations that are enabled for MQPUT. If there are no destinations enabled for MQPUT and no
local instance of a queue, an MQOPEN call that specified MQOO_BIND_ON_OPEN returns a return code of
MQRC_CLUSTER_PUT_INHIBITED to the application. If MQOO_BIND_NOT_FIXED is specified, or there is
214 Troubleshooting and Support for IBM MQ
a local instance of the queue, an MQOPEN call succeeds but subsequent MQPUT calls fail with return code
MQRC_PUT_INHIBITED.
Solution
You can write a user exit program to modify the workload management routines so that messages can be
routed to a destination that is disabled for MQPUT.
A message can arrive at a destination that is disabled for MQPUT. The message might have been in flight at
the time the queue became disabled, or a workload exit might have chosen the destination explicitly. The
workload management routine at the destination queue manager has a number of ways to deal with the
message:
• Choose another appropriate destination, if there is one.
• Place the message on the dead-letter queue.
• Return the message to the originator, if there is no dead-letter queue
Potential issues when switching transmission queues
A list of some issues that might be encountered when switching transmission queue, their causes, and
most likely solutions.
Insufficient access to transmission queues on z/OS
Symptom
A cluster-sender channel on z/OS might report it is not authorized to open its transmission queue.
Cause
The channel is switching, or has switched, transmission queue and the channel initiator has not been
granted authority to access the new queue.
Solution
Grant the channel initiator the same access to the channel’s transmission queue that is documented for
the transmission queue SYSTEM.CLUSTER.TRANSMIT.QUEUE. When using DEFCLXQ a generic profile for
SYSTEM.CLUSTER.TRANSMIT.** avoids this problem occurring whenever a new queue manager joins the
cluster.
Moving of messages fails
Symptom
Messages stop being sent by a channel and they remain queued on the channel’s old transmission queue.
Cause
The queue manager has stopped moving messages from the old transmission queue to the new
transmission queue because an unrecoverable error occurred. For example, the new transmission queue
might have become full or its backing storage exhausted.
Solution
IBM MQ troubleshooting and support 215
Review the error messages written to the queue manager’s error log (job log on z/OS) to determine the
problem and resolve its root cause. Once resolved, restart the channel to resume the switching process,
or stop the channel then use runswchl instead (CSQUTIL on z/OS).
A switch does not complete
Symptom
The queue manager repeatedly issues messages that indicate it is moving messages. The switch never
completes because there are always messages remaining on the old transmission queue.
Cause 1
Messages for the channel are being put to the old transmission queue faster than the queue manager can
move them to the new transmission queue. This is likely to be a transient issue during peak workload
because if were commonplace then it is unlikely the channel would be able to transmit the messages over
the network fast enough.
Cause 2
There are uncommitted messages for the channel on the old transmission queue.
Cause 3
The new transmission queue or the storage medium hosting it has filled.
Solution
Check the queue and channel status to confirm whether administrative action is required, for example:
• Start the channel to begin moving messages
• Free space on a full remote (target) queue if this is causing the channel to back up
• Increase the MAXDEPTH attribute on the transmission queue
The switching process retries continuously and completes once the problem is resolved.
Accidental deletion of a transmission queue
Symptom 1
Channels unexpectedly switch due to the removal of a matching CLCHNAME value.
Symptom 2
A put to a cluster queue fails with MQRC_UNKNOWN_XMIT_Q.
Symptom 3
A channel abnormally ends because its transmission queue does not exist.
Symptom 4
The queue manager is unable to move messages to complete a switch operation because it cannot open
either the old or the new transmission queue.
216 Troubleshooting and Support for IBM MQ
Cause
The transmission queue currently used by a channel, or its previous transmission queue if a switch has
not completed, has been deleted.
Solution
Redefine the transmission queue. If it is the old transmission queue that has been deleted then an
administrator may alternatively complete the switch operation using runswchl with the -n parameter (or
CSQUTIL with MOVEMSGS(NO) on z/OS).
Use the -n parameter with caution because, if it is used inappropriately, messages for the channel can
complete and finish processing but not be updated on the old transmission queue. In this scenario it
is safe because as the queue does not exist there cannot be any messages to complete and finish
processing.
Troubleshooting RDQM configuration problems
These topics give information that is useful for troubleshooting RDQM high availability (HA) and disaster
recovery (DR) configurations.
About this task
See also the topics explaining the output of the rdqmstatus command for help with troubleshooting
(Viewing RDQM and HA group status, Viewing DR RDQM status, and Viewing DR/HA RDQM and HA group
status).
Related tasks