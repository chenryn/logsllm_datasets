60
40
20
0
LSA
PDS-2
PDS-1
NPDS
0
10
20
30
40
50
60
Time (s)
(a) Throughput as a function of Dmax.
(b) Throughput for Dmax = 50 ms.
)
s
(
y
c
n
e
t
a
L
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
LSA
PDS-2
PDS-1
NPDS
0
10
20
30
40
50
60
Time (s)
s
d
a
e
r
h
T
e
v
i
t
c
A
f
o
r
e
b
m
u
N
7
6
5
4
3
2
1
0
PDS-2
NPDS
22
22.1
22.2
22.3
22.4
Time (s)
(c) Latency for Dmax = 50 ms.
(d) Number of concurrently executing
threads for Dmax = 50 ms.
Figure 6. Triplicated server.
Triplicated Server Experiments. Figure 6(a) shows the trip-
licated server’s throughput, measured at the client side, as a
function of Dmax. All algorithms perform almost identically
when Dmax is zero: since replicas run on single-processor ma-
chines, threads are serialized. As Dmax increases, i.e., the po-
tential concurrency in thread execution increases, the LSA algo-
rithm performs best, while the NPDS algorithm performs poorly
(LSA provides about ﬁve times more throughput than NPDS).
The PDS algorithm stays in the middle between the LSA and
the NPDS (PDS provides about two times more throughput than
NPDS); therefore, PDS can be considered a compromise be-
tween aggressive concurrency at the expense of inter-replica
communication (LSA), which gives better performance, and in-
dependent replica execution at expense of concurrency (NPDS),
which provides better error resilience (as discussed in (cid:0) 5.2).
The throughput of all algorithms decreases as Dmax grows be-
cause the service time increases while the number of server
threads is ﬁxed.
Figures 6(b) and 6(c) show, respectively, server throughput
and latency as functions of experiment execution time for Dmax
set to 50 ms. The two graphs, in addition to reinforcing the
conclusions (from Figure 6(a)) on the relative performance of
the tested algorithms, show also that PDS-2 provides about 12%
more throughput than PDS-1 (this holds also for lower values
of Dmax). To explain the large diversity in terms of performance
between the NPDS and PDS algorithms, Figure 6(d) shows the
number of concurrently executing threads (i.e., those threads not
suspended by the scheduling algorithm) for the PDS-2 algorithm
and the NPDS algorithm. Observe that while the PDS-2 curve
has a periodic tooth-saw shape (which is in synchrony with the
algorithm round ﬁring), the NPDS curve is almost always one,
occasionally decreasing to zero when the running thread invokes
the function accept for accepting a new client connection and
another thread is scheduled.
The above experiments show the LSA algorithm perform-
ing better than the PDS algorithm. It should be noted, though,
that this result can be inverted if inter-replica communication is
costly, e.g., when the network bandwidth is scarce or the num-
ber of replicas is large. On the other hand, the NPDS algorithm
always performs worse than both LSA and PDS, as long as there
is parallelism in replica threads’ execution that can be exploited.
5.2. Dependability Evaluation
This section provides a dependability assessment and com-
parison of the PDS and the LSA algorithms using software-
based error injection. Three dependability measures are used:
(1) the number of catastrophic failures (cases in which the entire
replicated system fails9), (2) the replica’s error-manifestation ra-
tio (ratio between manifested and injected errors), and (3) the
replica’s manifested-to-activated10 error ratio (when available).
The number of catastrophic failures must be minimized in highly
available systems. The error manifestation ratio characterizes
the likelihood that an error causes a failure (including a single
replica and an entire system failure). It can be used to calculate,
given the error arrival rate, the replicated system availability.
The manifested-to-activated ratio provides a closer look into the
error sensitivity of a replica code. We now summarize the major
ﬁndings from our error-injection experiments:
1. About 36000 single-bit errors were injected uniformly into
text, data, and heap11 segments of a replica process (includ-
ing Ensemble [8], which provides group communication
primitives). The results indicate a smaller error sensitivity
for a PDS replica than for an LSA replica. The difference
is due to the inter-replica communication required by LSA.
2. About 2800 single-bit errors were injected uniformly in the
portion of a replica’s text segment corresponding to a spe-
ciﬁc Ensemble function that is used by both a PDS and an
LSA replica. We observed 81 catastrophic failures, which
shows that Ensemble’s fail silence violations constitute an
issue for highly available systems.
3. In (2), we did not observe statistical difference, with respect
to catastrophic failures, between LSA and PDS. Therefore,
in an LSA leader, we injected errors into Ensemble func-
tions solely used in LSA (note, all Ensemble functions used
by PDS are also used by an LSA replica). Thirty-one catas-
trophic failures were observed in 3600 injections, which
conﬁrms that an LSA replica is more sensitive to catas-
trophic failures.
4. Importantly, errors originating in the reliable communica-
tion layer do propagate and lead to catastrophic failures of
the entire replicated system. This is reported not as an in-
dictment of Ensemble (which is a well-engineered prod-
uct), but to point out that these failures are due to fail si-
lence violations and error propagations, which are out of
the scope of the usual assumption of crash/omission fail-
ures. In addition, simply using protocols capable of han-
dling application value errors (e.g., interactive consistency)
will not help cope with errors originating in the communi-
cation layer.
(The catastrophic failures we observed are
due to a corrupted Ensemble header in the messages ex-
changed.) To limit or prevent error propagations across
the network, the middleware on which fault-tolerant tech-
niques are based (in our case, the reliable communication
layer), must itself be fault-tolerant [19].
5.2.1. Injections into a Replica Process
A ﬁrst set of error injection experiments was conducted to assess
the impact of errors in a replica’s text, data, and heap memory
9A replicated system fails if the voter fails or a majority of replicas fail.
10An error is activated if the injected data/instruction is used/executed.
11More than one error may be injected during a single heap experiment.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Error Model
TEXT
DATA
HEAP
Crash Failure
Fail Silence
Violation
Table 2. Error models.
Description
A single bit in the text segment of the target replica is ﬂipped.
A single bit in the data segment of the target replica is ﬂipped.
A bit in allocated regions of the heap memory of the target replica is
ﬂipped periodically, until the replica terminates or crashes. Note that
more than one error may be injected during a single experiment.
Table 3. Outcome categories.
SIGNAL, the operating system terminates the target replica by sending a
signal (e.g., SIGSEGV, SIGILL, SIGBUS, SIGFPE).
ASSERT, the target replica shuts itself down owing to an internal check
violation.
HANG, the target replica does not terminate but no output is produced to
the voter.
VALUE ERROR, the target replica produces a value different from other,
non-faulty replicas.
TWO HUNG FOLLOWERS, the target replica (LSA leader) sends output
to the voter but stops sending mutex tables to followers.(cid:0)
LEADER IMPERSONATION, as the previous case except that no output
is sent to the voter (the leader “impersonates” a follower).(cid:1)
CATASTROPHIC FAILURE, the entire replicated system fails.
(cid:0) A mutex table is a fragment of the leader’s replica history Hl and is continuously sent
to followers so that the followers can enforce the same causal relations as the leader on their
threads’ mutex acquisitions. The LSA voter detects this case by noting that a majority of
replicas are hung.
(cid:1) The LSA voter detects this case by noting that no replica generates output before a
dedicated timer expires, although output is expected.
segments. A replica here includes the application benchmark
discussed in (cid:0) 5.1, the PDS/LSA algorithm, the Virtual Socket
Layer, and Ensemble. NFTAPE [20], a software framework
for conducting automated fault/error injection experiments, was
used to conduct the tests.
The error models considered are summarized in Table 2 and
represent a combination of those used in several past experi-
mental studies [21, 22]. By injecting single bits in the targeted
replica, we emulate errors in the main memory, the cache, the
processor execution buffer, and the processor execution core,
as well as errors occurring during the transmission over a bus.
Previous research on microprocessors [23] has shown that most
(90–99%) device-level transients can be modeled as logic-level,
single-bit errors. Data on operational errors also shows that a
majority of errors in the ﬁeld are single-bit errors.
Manifested errors are divided in two major outcome cate-
gories: (1) crash failures, in which the injected replica stops
executing and no incorrect state transition is performed before
the failure, and (2) fail silence violations, in which the injected
replica performs incorrect state transitions.12 The two categories
and their corresponding subcategories are reported in Table 3.
Table 4 reports the results from error injection experiments
for both PDS and LSA algorithms (for LSA, we distinguish be-
tween leader and follower injections) and for each error model
listed in Table 2. During an experiment, each of the 15 clients
sends 10 requests (generated as explained in (cid:0) 5.1) and then ter-
minates. This setup permits us to observe the system for a suf-
ﬁcient amount of time after an error is injected (about 30 sec-
onds). The experiment concludes when either all clients termi-
nate or a catastrophic failure occurs. The system is reset between
two experiments.
With the exception of ﬁve cases, the system is able to recover
from the injected failure. In the case of PDS, the voter masks
the failure.
In the case of LSA, if the leader fails, followers
successfully elect a new leader after the failed leader is excluded
from the system; if a follower fails, the voter masks the failure.
12This deﬁnition of fail silence violation is consistent with [24]. This failure
type covers cases such as corrupted data saved on persistent storage or corrupted
message sent to other nodes.
In discussing further the error-injection results, we distinguish
between failures masked by the voter and catastrophic failures.
Failures Masked by the Voter. Text injections show a slightly
larger error-manifestation ratio for LSA than for PDS: 10%,
8.3%, and 7.5% for an LSA leader, LSA follower, and PDS, re-
spectively. The manifested-to-activated error ratios give a sim-
ilar conclusion: 73%, 65%, 68% for an LSA leader, LSA fol-
lower, and PDS, respectively. This ratio variation could be ex-
plained by the different complexity of the two algorithms (with
PDS being simpler than LSA). Because the difference in the al-
gorithms’ code size (14K for PDS and 25K for LSA) is small
compared to the total replica code size (900K) and the errors are
injected uniformly, we argue that the major cause for variations
in the observed error manifestations is the different uses of En-
semble (with code size of about 740K). While PDS and LSA
replicas both use Ensemble to communicate with the voter, an
LSA replica also uses Ensemble for passing the order of mu-
tex acquisitions from the leader to the followers. Proﬁling the
Ensemble usage shows that a PDS replica and an LSA replica
invoke, respectively, 343 and 391 Ensemble functions.
Data injections show a very low error-manifestation ratio.
This is because a large part of the data segment (405K in total,
390K of which is part of Ensemble) is not used during normal
execution. As a result, errors in the data segment do not con-
tribute noticeably to the number of failures in the system.
Heap injections show an error-manifestation ratio for LSA
that is about twice that for PDS. The reason can be found in
the more extensive use of dynamic memory by (1) the LSA
leader, which stores the mutex acquisition order on the heap
memory, (2) the LSA followers, which store the leader-decided
order of mutex acquisitions in dynamic data structures (projec-
tion queues), and (3) Ensemble (for both leader and follow-
ers), which uses heap memory for internal message buffering
and management support of the leader-to-follower communica-
tion. The observed larger error sensitivity of a follower is be-
cause a follower not only collects (in the projection queues) the
leader-dictated order of mutex acquisitions, but actively applies
it in scheduling threads’ executions. Therefore, corruption of the
projection queues results in more crashes or divergent behavior
of the follower, where divergent behavior manifests as a greater
percentage of value errors.
Thus, the experiments show that an LSA-based replicated
system is more sensitive to voter-masked failures than a PDS-
based replicated system. Note that, although these failures do
not cause errors to propagate to clients, they do impact a repli-
cated system’s availability.
Catastrophic Failures. Although the above discussion indi-
cates that the PDS thread-scheduling strategy has a higher er-
ror resilience than the LSA strategy, the most important differ-
ence between the two algorithms appears when analyzing catas-
trophic failures. Because these failures cannot be masked by the
voter, it is crucial to prevent them in a replicated system. Two
replicated systems can be judged by their ability to avoid this
type of failures. In the experiments conducted, we observed ﬁve
catastrophic failures occurring through the Ensemble communi-
cation layer, all of which were due to error propagation. They
are described below.
PDS experiments. An error injected in the Ensemble’s mes-
sage routing module (Unsigned) of the targeted replica caused
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Table 4. Error injection results.
PDS
LSA-L
LSA-F
Error
Model
TEXT
DATA
HEAP
TEXT
DATA
HEAP
TEXT
DATA
HEAP
Total
Injected
Errors
5224
2152
9158
5224
2139
2036
5144
2153
3010
Total
Total(cid:0)
Activated Manifested
Errors
583
N/A
N/A
728
N/A
N/A