Our testing strategy essentially repeats the above experi-
ment where we train with artiﬁcially-inserted canaries added
to the training data, and then use the exposure metric to assess
to what extent the model has memorized them. Recall that
the reason we study these ﬁxed-format out-of-distribution
canaries is that we are focused on unintended memorization,
and any memorization of out-of-distribution values is by deﬁ-
nition unintended and orthogonal to the learning task.
If, instead, we inserted in-distribution phrases which were
helpful for the learning task, then it would be perhaps even
desirable for these phrases to be memorized by the machine-
learning model. By inserting out-of-distribution phrases
which we can guarantee are unrelated to the learning task, we
can measure a models propensity to unintentionally memorize
training data in a way that is not useful for the ﬁnal task.
Setup: Before testing the model for memorization, we must
ﬁrst deﬁne a format of the canaries that we will insert. In
practice, we have found that the exact choice of format does
not signiﬁcantly impact results.
However, the one choice that does have a signiﬁcant im-
pact on the results is randomness: it is important to choose
a randomness space that matches the objective of the test to
be performed. To approximate worst-case bounds, highly out-
of-distribution canaries should be inserted; for more average-
case bounds, in-distribution canaries can be used.
Augment the Dataset: Next, we instantiate each format se-
quence with a concrete (randomly chosen) canary by replac-
ing the holes with random values, e.g., words or numbers.
We then take each canary and insert it into the training data.
In order to report detailed metrics, we can insert multiple dif-
ferent canaries a varying number of times. For example, we
may insert some canaries canaries only once, some canaries
tens of times, and other canaries hundreds or thousands of
times. This allows us to establish the propensity of the model
to memorize potentially sensitive training data that may be
seen a varying number of times during training.
Train the Model: Using the same setup as will be used for
training the ﬁnal model, train a test model on the augmented
training data. This training process should be identical: ap-
plying the same model using the same optimizer for the same
number of iterations with the same hyper-parameters. As we
will show, each of these choices can impact the amount of
memorization, and so it is important to test on the same setup
that will be used in practice.
Report Exposure: Finally, given the trained model, we apply
our exposure metric to test for memorization. For each of
the canaries, we compute and report its exposure. Because
we inserted the canaries, we will know their format, which
is needed to compute their exposure. After training multiple
models and inserted the same canaries a different number
of times in each model, it is useful to plot a curve showing
the exposure versus the number of times that a canary has
been inserted. Examples of such reports are plotted in both
Figure 1, shown earlier, and Figure 4, shown on the next page.
6 Experimental Evaluation
This section applies our testing methodology to several model
architectures and datasets in order to (a) evaluate the efﬁcacy
of exposure as a metric, and (b) demonstrate that unintended
memorization is common across these differences.
6.1 Smart Compose: Generative Email Model
As our largest study, we apply our techniques to Smart Com-
pose [29], a generative word-level machine-learning model
that is trained on a text corpus comprising of the personal
emails of millions of users. This model has been commer-
cially deployed for the purpose of predicting sentence com-
pletion in email composition. The model is in current active
use by millions of users, each of which receives predictions
drawn not (only) from their own emails, but the emails of
all the users’ in the training corpus. This model is trained on
highly sensitive data and its output cannot reveal the contents
of any individual user’s email.
This language model is a LSTM recurrent neural network
with millions of parameters, trained on billions of word se-
quences, with a vocabulary size of tens of thousands of words.
Because the training data contains potentially sensitive infor-
mation, we applied our exposure-based testing methodology
to measure and ensure that only common phrases used by
multiple users were learned by the model. By appropriately
interpreting the exposure test results and limiting the amount
of information drawn from any small set of users, we can
empirically ensure that the model is never at risk of exposing
any private word sequences from any individual user’s emails.
As this is a word-level language model, our canaries are
seven (or ﬁve) randomly selected words in two formats. In
both formats the ﬁrst two and last two words are known con-
text, and the middle three (or one) words vary as the random-
ness. Even with two or three words from a vocabulary of
tens of thousands, the randomness space R is large enough
to support meaningful exposure measurements.
USENIX Association
28th USENIX Security Symposium    273
Figure 4: Exposure plot for our commercial word-level lan-
guage model. Even with a canary inserted 10,000 times, ex-
posure reaches only 10: the model is 1,000× more likely to
generate this canary than another (random) possible phrase,
but it is still not a very likely output, let alone the most likely.
In more detail, we inserted multiple canaries in the training
data between 1 and 10,000 times (this does not impact model
accuracy), and trained the full model on 32 GPUs over a bil-
lion sequences. Figure 4 contains the results of this analysis.
(Note: The measured exposure values are lower than in
most of other experiments due to the vast quantity of training
data; the model is therefore exposed to the same canary less
often than in models trained for a large number of epochs.)
When we compute the exposure of each canary, we ﬁnd
that when secrets are very rare (i.e., one in a billion) the model
shows no signs of memorization; the measured exposure is
negligible. When the canaries are inserted at higher frequen-
cies, exposure begins to increase so that the inserted canaries
become with 1000× more likely than non-inserted canaries.
However, even this higher exposure doesn’t come close to al-
lowing discovery of canaries using our extraction algorithms
(see Section 8), let alone accidental discovery.
Informed by these results, limits can be placed on the inci-
dence of unique sequences and sampling rates, and clipping
and differential-privacy noise (see Section 9.3) can be added
to the training process, such that privacy is empirically pro-
tected by eliminating any measured signal of exposure.
6.2 Word-Level Language Model
Next we apply our technique to one of the current state-of-
the-art world-level language models [35]. We train this model
on WikiText-103 dataset [34], a 500MB cleaned subset of
English Wikipedia. We do not alter the open-source imple-
mentation provided by the authors; we insert a canary ﬁve
times and train the model with different hyperparameters. We
choose as a format a sequence of eight words random selected
from the space of any of the 267,735 different words in the
model’s vocabulary (i.e., that occur in the training dataset).
We train many models with different hyperparameters and
report in Figure 5 the utility as measured by test perplexity
Figure 5: The results of applying our testing methodology to a
word-level language model [35] inserting a canary ﬁve times.
An exposure of 144 indicates extraction should be possible.
We train many models each with different hyperparameters
and ﬁnd vast differences in the level of memorization. The
highest utility model memorizes the canary to such a degree
it can be extracted. Other models that reach similar utility
exhibit less memorization. A practitioner would prefer one of
the models on the Pareto frontier, which we highlight.
(i.e., the exponential of the model loss) against the measured
exposure for the inserted canary. While memorization and
utility are not highly correlated (r=-0.32), this is in part due
to the fact that many choices of hyperparameters give poor
utility. We show the Pareto frontier with a solid line.
6.3 Character-Level Language Model
While previously we applied a small character-level model
to the Penn Treebank dataset and measured the exposure of
a random number sequence, we now conﬁrm that the results
from Section 6.2 hold true for a state-of-the-art character-level
model. To verify this, we apply the character-level model from
[35] to the PTB dataset.
As expected, based on our experiment in Section 3, we
ﬁnd that a character model model is less prone to memoriz-
ing a random sequence of words than a random sequence
of numbers. However, the character-level model still does
memorize the inserted random words: it reaches an exposure
of 60 (insufﬁcient to extract) after 16 insertions, in contrast
to the word-models from the previous section that showed
exposures much higher than this at only 5 insertions.
6.4 Neural Machine Translation
In addition to language modeling, another common use of
generative sequence models is Neural Machine Translation
[3]. NMT is the process of applying a neural network to
translate from one language to another. We demonstrate that
unintentional memorization is also a concern on this task, and
274    28th USENIX Security Symposium
USENIX Association
0200040006000800010000Number of Insertions2468ExposureLength-5 SequenceLength-7 Sequence707580859095Perplexity (lower means higher utility)50100150200250Exposure (less memorization if lower)Figure 6: Exposure of a canary inserted in a Neural Machine
Translation model. When the canary is inserted four times or
more, it is fully memorized.
because the domain is different, NMT also provides us with a
case study for designing a new perplexity measure.
NMT receives as input a vector of words xi in one language
and outputs a vector of words yi in a different language. It
achieves this by learning an encoder e : (cid:126)x → Rk that maps
the input sentence to a “thought vector” that represents the
meaning of the sentence. This k-dimensional vector is then
fed through a decoder d : Rk → (cid:126)y that decodes the thought
vector into a sentence of the target language.3
Internally, the encoder is a recurrent neural network that
maintains a state vector and processes the input sequence
one word at a time. The ﬁnal internal state is then returned
as the thought vector v ∈ Rk. The decoder is then initialized
with this thought vector, which the decoder uses to predict
the translated sentence one word at a time, with every word it
predicts being fed back in to generate the next.
We take our NMT model directly from the TensorFlow
Model Repository [11]. We follow the steps from the docu-
mentation to train an English-Vietnamese model, trained on
100k sentences pairs. We add to this dataset an English canary
of the format “My social security number is
-
-
” and a corresponding Vietnamese phrase of the same
format, with the English text replaced with the Vietnamese
translation, and insert this canary translation pair.
Because we have changed problem domains, we must de-
ﬁne a new perplexity measure. We feed the initial source
sentence (cid:126)x through the encoder to compute the thought vector.
To compute the perplexity of the source sentence mapping to
the target sentence(cid:126)y, instead of feeding the output of one layer
to the input of the next, as we do during standard decoding, we
instead always feed yi as input to the decoder’s hidden state.
The perplexity is then computed by taking the log-probability
of each output being correct, as is done on word models. Why
do we make this change to compute perplexity? If one of
the early words is guessed incorrectly and we feed it back in
3See [51] for details that we omit for brevity.
Figure 7: Exposure as a function of training time. The expo-
sure spikes after the ﬁrst mini-batch of each epoch (which
contains the artiﬁcially inserted canary), and then falls overall
during the mini-batches that do not contain it.
to the next layer, the errors will compound and we will get
an inaccurate perplexity measure. By always feeding in the
correct output, we can accurately judge the perplexity when
changing the last few tokens. Indeed, this perplexity deﬁnition
is already implemented in the NMT code where it is used to
evaluate test accuracy. We re-purpose it for performing our
memorization evaluation.
Under this new perplexity measure, we can now compute
the exposure of the canary. We summarize these results in
Figure 6. By inserting the canary only once, it already occurs
1000× more likely than random chance, and after inserting
four times, it is completely memorized.
7 Characterizing Unintended Memorization
While the prior sections clearly demonstrate that unintended
memorization is a problem, we now investigate why and how
models unintentionally memorize training data by applying
the testing methodology described above.
Experimental Setup: Unless otherwise speciﬁed, the exper-
iments in this section are performed using the same LSTM
character-level model discussed in Section 3 trained on the
PTB dataset with a single canary inserted with the format “the
random number is
” where the maximum
exposure is log2(109) ≈ 30.
7.1 Memorization Throughout Training
To begin we apply our testing methodology to study a simple
question: how does memorization progress during training?
We insert the canary near the beginning of the Penn Tree-
bank dataset, and disable shufﬂing, so that it occurs at the
same point within each epoch. After every mini-batch of train-
USENIX Association
28th USENIX Security Symposium    275
051015Number of Insertions0102030Exposure0123Epoch2.55.07.510.012.5ExposureExposureestimated exposure at epoch 10 is actually higher than the es-
timated exposure at epoch 40 (with p-value p < .001). While
this is interesting, in practice it has little effect: the rank of
this canary is 1 for all epochs after 10.
Taken together, these results are intriguing. They indicate
that unintended memorization seems to be a necessary com-
ponent of training: exposure increases when the model is
learning, and does not when the model is not. This result con-
ﬁrms one of the ﬁndings of Tishby and Schwartz-Ziv [42] and
Zhang et al. [56], who argue that neural networks ﬁrst learn
to minimize the loss on the training data by memorizing it.
7.3 Additional Memorization Experiments
Appendix A details some further memorization experiments.
8 Validating Exposure with Extraction
How accurate is the exposure metric in measuring memo-
rization? We study this question by developing an extraction
algorithm that we show can efﬁciently extract training data
from a model when our exposure metric indicates this should
be possible (i.e., when the exposure is greater than log2|R |).
8.1 Efﬁcient Extraction Algorithm
Proof of concept brute-force search: We begin with a sim-
ple brute-force extraction algorithm that enumerates all possi-
ble sequences, computes their perplexity, and returns them in
order starting from the ones with lowest perplexity. Formally,
we compute arg minr∈R Pxθ(s[r]). While this approach might
be effective at validating our exposure metric accurately cap-
tures what it means for a sequence to be memorized, it is
unable to do so when the space R is large. For example,
brute-force extraction over the space of credit card numbers
(1016) would take 4,100 commodity GPU-years.
Shortest-path search: In order to more efﬁciently perform
extraction, we introduce an improved search algorithm, a mod-
iﬁcation of Dijkstra’s algorithm, that in practice reduces the
complexity by several orders of magnitude.
To begin, observe it is possible to organize all possible
partial strings generated from the format s as a weighted
tree, where the empty string is at the root. A partial string
b is a child of a if b expands a by one token t (which we
denote by b = a@t). We set the edge weight from a to b to
−logPr(t| fθ(a)) (i.e., the negative log-likelihood assigned
by the model to the token t following the sequence a).
Leaf nodes on the tree are fully-completed sequences. Ob-
serve that the total edge weight from the root x1 to a leaf node
xn is given by
∑−log2 Pr(xi| fθ(x1...xi−1))
= Pxθ(x1...xn)
(By Deﬁnition 1)
Figure 8: Comparing training and testing loss to exposure
across epochs on 5% of the PTB dataset . Testing loss reaches
a minimum at 10 epochs, after which the model begins to over-
ﬁt (as seen by training loss continuing to decrease). Exposure
also peaks at this point, and decreases afterwards.
ing, we estimate the exposure of the canary. We then plot the
exposure of this canary as the training process proceeds.
Figure 7 shows how unintended memorization begins to
occur over the ﬁrst three epochs of training on 10% of the
training data. Each time the model trains on a mini-batch that
contains the canary, the exposure spikes. For the remaining
mini-batches (that do not contain the canary) the exposure
randomly ﬂuctuates and sometimes decreases due to the ran-
domness in stochastic gradient descent.
It is also interesting to observe that memorization begins
to occur after only one epoch of training: at this point, the
exposure of the canary is already 3, indicating the canary is
23 = 8× more likely to occur than another random sequence
chosen with the same format. After three epochs, the exposure
is 8: access to the model reduces the number of guesses that
would be needed to guess the canary by over 100×.
7.2 Memorization versus Overtraining
Next, we turn to studying how unintended memorization re-
lates to overtraining. Recall we use the word overtraining to
refer to a form of overﬁtting as a result of training too long.
Figure 8 plots how memorization occurs during training
on a sample of 5% of the PTB dataset, so that it quickly
overtrains. The ﬁrst few epochs see the testing loss drop
rapidly, until the minimum testing loss is achieved at epoch
10. After this point, the testing loss begins to increase—the
model has overtrained.
Comparing this to the exposure of the canary, we ﬁnd an
inverse relationship: exposure initially increases rapidly, un-
til epoch 10 when the maximum amount of memorization
is achieved. Surprisingly, the exposure does not continue in-
creasing further, even though training continues. In fact, the