Host network
Yes
Large
Large
Akamai
Akamai
Akamai
Akamai
Yes
Yes
Mainly cache fill
Cache fill/Serve Mainly cache fill
Yes
Yes
Medium/Large
Small/Medium
High
Medium/High
PNIs w. eyeball networks
IXP
N/A PNIs w. backbone providers
Yes
All
Low/Medium
Transit network
N/A
and requirements. As resources-rich vantage points, Aka-
mai’s server clusters also act as łmeasurersž that produce a
range of measurement data. While most of the server clus-
ters specialize in producing data that provides important
information about the overall state of Akamai’s serving in-
frastructure, others are exclusively focused on collecting
BGP information that is pertinent for maintaining an up-to-
date view of Akamai’s connectivity fabric and central to its
role of delivering content from where it is ingested or resides
to where it is consumed in a performance-optimal manner.
Since the information collected by the different server clus-
ters about network health, performance, and routing can (and
does) change over time as a result of the dynamic nature
of the Internet in general and its routing system in particu-
lar, the measurements are typically obtained at regular time
intervals (e.g., quarter-hourly, hourly, daily).
To illustrate, Akamai’s server clusters maintain an over-
all view of the state of Akamai’s infrastructure by keeping
track of and reporting, among other quantities, their cluster-
specific bandwidth usage and system load. They also actively
measure network conditions (e.g., loss, latency) and connec-
tivity (e.g., route) between them across different deployments
ś criss-cross measurements leveraging server clusters as van-
tage points. In addition to measuring network conditions
and connectivity to other server clusters, these measurers
also actively measure network conditions and connectivity
to popular name server/resolvers and other targets, in part
for evaluating their own capabilities to communicate with
those targets. In effect, the measurements to these targets
let the individual server clusters assess their capabilities to
communicate with end user clients or other servers that are
in the same network segments as those targets.
2.3.1 Non-Delivery Servers as BGP Collectors. Akamai op-
erates 80 BGP collectors across the globe. Akamai’s BGP
collectors are non-delivery servers. They receive routing in-
formation from Akamai and non-Akamai routers inside the
Figure 1: Akamai and non-Akamai routers send BGP
information over the Internet to the BGP collectors,
which in turn send it to Akamai’s mapping system.
various deployments over the Internet and provide that in-
formation as input to Akamai’s mapping system (see Figure 1
for a high-level overview of the BGP information flow) in a
form that the mapping system can process and consume. For
instance, an important capability of Akamai’s BGP collectors
is to translate third-party network-specific BGP communi-
ties into a common set of Akamai-specific BGP communities.
Because of the critical role they play as components of Aka-
mai’s measurement platform and as producers of input data
for Akamai’s mapping system (see Section 2.3.2), deploy-
ments that host BGP collectors have to be always available
and conform to specific rules.
The routing information received by these BGP collectors
from Akamai’s various deployments is deployment depen-
dent. In particular, it is the deployment type that determines
what BGP information is obtained via what BGP sessions
with which networks. For example, for Type 1 and Type 2
deployments that do not operate an Akamai-owned router
and instead rely on a router in the hosting network, that
hosting network’s router has BGP sessions to nearby (in
terms of network distance) BGP collectors and sends them
the BGP information that the hosting network provides, in-
cluding routes for end user and name server/resolver prefixes
and BGP communities. Here, the prefixes can be the host-
ing network’s own prefixes, the prefixes of its downstream
209
mappingsystemBGP collectors......BGPBGPType 1 or 2Type 3 or 4Leveraging Interconnections for Performance
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
customers and even prefixes of networks for which there is
a special agreement for Akamai’s traffic. BGP communities
are particularly informative because by setting BGP com-
munities, the hosting network can signal to Akamai how
to serve the network’s own prefixes and the prefixes of its
downstream (or other) networks.
For a Type 3 deployment, the BGP information originates
in part from the BGP sessions that the Akamai-owned router
has with each IXP member that peers bilaterally with Akamai
and in part from the BGP sessions that that router may have
with the IXP’s route server(s). The deployment’s Akamai-
owned router also has BGP sessions to more than one BGP
collector for redundancy. However, in the case of a Type
3 deployment, it is the routing table and not the router’s
BGP table (i.e., all the BGP information it obtains over time)
that is sent to the BGP collectors. The routing table contains
only the łbestž (also referred to as łactivež) routes from all
the routes received from the various direct (i.e., bilateral)
or indirect (i.e., route server) sessions with the IXP’s peers.
Moreover, information that is not sent includes routes that
are filtered at the router-level and routes that are received
from the deployment’s transit link. In view of what informa-
tion Akamai’s BGP collectors receive, Type 3 deployments
are similar to Type 4 deployments. In the latter case, the
Akamai-owned router has a BGP session for each PNI and
sends BGP information about the best paths it receives from
the various PNIs to Akamai’s BGP collectors. Thus, in case
the Akamai-owned router has a PNI with an eyeball network
that also hosts one or more Type 1 deployments, the BGP
information that the BGP collectors receive from the PNI
may not be as fine-grained as the information it obtains from
the Type 1 deployment(s).
2.3.2 Measurement Platform vs. Mapping System. A key
element of Akamai’s service delivery platform is its mapping
system. At its core, Akamai’s mapping system relies on DNS
to route each end user client to a deployment with at least
one EUF delivery server that ultimately serves the requested
content. As a DNS-based system, this mapping system has
evolved over time, and we refer the interested reader to [14]
for more details. However, what matters for the purpose of
this paper is that Akamai’s mapping system is a consumer
of a myriad of data (including the data from Akamai’s BGP
collectors) that originates from Akamai’s global-scale mea-
surement platform.
Akamai’s mapping system ingests a large number of dif-
ferent measurement data and combines them to ultimately
return for each end user request a rank-ordered list of de-
ployments with EUF delivery servers (and corresponding IP
addresses). In particular, even though the mapping system
takes BGP collector data as one of its many inputs, it is not
a system that makes decisions solely based on BGP. At the
same time, the mapping system is also a consumer on non-
measurement data (i.e., data that is neither produced nor
collected by Akamai’s measurement platform). Examples of
such non-measurement data include cost-related informa-
tion about peering links and detailed topological informa-
tion about a hosting eyeball network. In short, to achieve
Akamai’s main objective of optimizing the performance of
service delivery, its mapping system relies on inputs of vari-
ous kinds (e.g., measurement and non-measurement data),
is highly flexible as there are numerous ways to tune it to
affect traffic flow to overcome issues or meet special needs,
and is constantly evolving as new functionalities get added
to satisfy an ever more diverse customer base, an increasing
selection of service offerings, and rapid innovations in key
Internet technologies.
3 AKAMAI’S CONNECTIVITY FABRIC
After describing the available datasets, we provide a detailed
assessment of the reach and structure of Akamai’s connectiv-
ity fabric and pay particular attention to what type of BGP
information sheds light on what aspects of this fabric.
3.1 Available Datasets
3.1.1 Proprietary BGP information ś ViewA. For our study,
we obtain the portion of the Internet control-plane informa-
tion that is used by Akamai as one of the inputs to its map-
ping system. To this end, we rely on Akamai’s BGP collectors
that dump on an hourly basis the information in their BGP
tables in MRT format. Our dataset consists of the BGP table
dumps from all the BGP collectors and will be referred to in
the following as ViewA. The data contains both IPv4 and IPv6
information. We analyzed six hourly snapshots of ViewA,
and Table 2 lists a few key metrics for the six snapshots and
shows how these metrics vary over an eight-month period.
Note that the type of hourly snapshots of ViewA shown in
Table 2 represent exactly the BGP information that Akamai
used as routing data input for its mapping system at those
particular times. Since our work in this paper concerns the
hourly inputs as obtained by Akamai’s mapping system and
not their evolution over time, and given the rather modest
variations in the metrics shown in Table 2, due to limited
space, in the rest of this section, we report on results that
concern our analysis of the 2017-09-17 snapshot of ViewA.
This snapshot was collected from some 4.5k BGP sessions be-
tween Akamai’s BGP collectors and non-Akamai and Akamai
routers and consists of more than 3.65M AS paths and about
1.85M IPv4 and IPv6 prefixes. However, as an illustration that
our reported results are consistent with and representative
of those obtained for the other snapshots listed in Table 2,
we discuss in Section 4 additional results for our most recent
2018-05-17 snapshot.
210
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
F. Wohlfart, N. Chatzis, C. Dabanoglu, G. Carle, W. Willinger
Table 2: Overview of Akamai’s BGP data ś ViewA.
Table 3: Overview of the public BGP data ś ViewP.
Sep 17 Oct 10 Nov 10 Dec 5
2017
2017
2017
2017
Jan 10 May 17
2018
2018
ASes
Prefixes
Paths
60.2k
61.3k
61.5k
1.85M 1.80M 1.83M 1.85M 1.92M 1.88M
3.65M 3.53M 3.53M 3.49M 3.56M 3.36M
59.9k
60.4k
63.2k
3.1.2 Publicly available BGP information ś ViewP. To put
ViewA in perspective, we use and combine data from three
well-known public data sources; i.e., Route-Views (RV) [32],
RIPE NCC RIS (RIPE) [31], and the daily routing snapshots
collected by Packet Clearing House (PCH) [24]. We obtain
the BGP table data from all the available collectors from
these data sources for 2017-09-17, the same day as the data
for our 2017-09-17 snapshot of ViewA data was obtained.
The RV and PCH data contains IPv4 and IPv6 information
whereas the RIPE data provides only IPv4 information. The
RV and RIPE data is stored in MRT format whereas the PCH
data is available as compressed text files that contain the
output of running the show ip bgp command on the PCH
route collectors. We combined the three public datasets into
one dataset and refer to it below as ViewP. Table 3 provides
details about each of the three public datasets for 2017-09-17
and also about their combined ViewP which consists of more
than 21M different AS paths.
3.1.3 ViewA vs. ViewP. Note that for the purpose of our
study, there is no need for examining multiple snapshots
of ViewP. In fact, in contrast to prior studies that typically
required multiple snapshots of BGP data, such as ViewP, to
examine questions about the observed AS-level Internet such
as its structure, completeness, or its evolution over time (e.g.,
see [12, 17, 34] and references therein), our use of ViewP
is rather restrictive; that is, we use ViewP mainly for com-
paring inherent properties of ViewP-like datasets against
their counterparts in data such as ViewA. For example, one
such property is that ViewP consists of BGP table data while
ViewA is based on routing table data that Akamai’s BGP
collectors receive from their BGP sessions with non-Akamai
and Akamai routers (see Section 2.3.1). This means that for
a given prefix, ViewP typically has information about many
different AS paths to that prefix while ViewA (i.e., BGP tables
from Akamai’s BGP collectors) only provides one path per
deployment ś the best path. Another distinguishing qualita-
tive feature concerns what routes are filtered and thus are
not part of ViewP and ViewA, respectively. ViewA does not
include routes that are received from the links of Akamai
with its transit providers, but ViewP will in general include
those transit provider’s BGP data. We discuss implications
of these and other such features in this section.
211
Dataset Collectors ASes
Prefixes
Paths
RV
RIPE
PCH
ViewP
19
18
140
177
58.4k
57.9k
58.0k
58.6k
872k
737k
733k
12.5M
11.4M
0.4M
900k
21.1M
3.2 On the Reach of Akamai
3.2.1
Serving the World: ASes and Prefixes. We first per-
form an AS-level analysis of ViewA and find a total of 61.3k
unique routeable ASes for Akamai; that is, Akamai sees at
least one originating prefix from 61.3k ASes. This compares
to 58.6k unique routeable ASes seen in ViewP. For compari-
son, Hurricane Electric, a large backbone provider, reports
seeing some 60.7k routeable ASes [18], an indication that
global-scale providers such as Akamai and Hurricane Electric
tend to see more routeable ASes than ViewP.
Next, a closer look at the originating prefixes reveals a
total of 1.75M unique IPv4 prefixes in ViewA and only 0.85M
in ViewP. Hurricane Electric reports a very similar result for
the number of prefixes it observes (see [18]). Table 4 (left half)
provides details about the unique IPv4 and IPv6 prefixes that
are only seen in ViewA, only in ViewP, and in both ViewA
and ViewP, respectively. We observe, for example, that the
number of prefixes that are only present in ViewP (i.e., ViewP
\ ViewA) is comparatively small ś only 99.5k IPv4 prefixes are
exclusively seen in ViewP. To explain why Akamai receives
almost twice as many prefixes as ViewP, Figure 2a shows a
breakdown by prefix length of the number of unique IPv4
prefixes in ViewA and their overlap with those in ViewP. The
plot shows that of the 1M unique prefixes that only Akamai
receives (i.e., ViewA \ ViewP), around 75% of them are of
length /25 or longer.
When analyzing this data further, we notice that almost
all the IPv4 prefixes that are present in ViewA but absent
from ViewP originate from ASes that are seen in ViewP.
Thus, although at the AS-level, ViewA and ViewP are similar,
at the level of originating IPv4 prefixes, Akamai receives
information in a much more fine-grained manner compared
to what can be discerned from ViewP.2 The fact that of the
almost 1M unique IPv4 prefixes that are only seen in ViewA,
75% are of length /25 or longer suggests that they play a key
role for Akamai’s content delivery service (see below for
details), and this observation will be qualitatively the same
for different instantiations of ViewA and ViewP.
3.2.2
Serving the World: AS Paths. To demonstrate the
above-mentioned qualitative differences between ViewA and
2A well known practice by network operators is to filter prefixes longer
than /24 to limit the growth of their Internet routing tables. Note however
that ViewP still sees some 60k IPv4 prefixes of length /25 or longer.
Leveraging Interconnections for Performance
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
ViewA
ViewA ∩  ViewP
s
e
x
i
f