message from its peer. Since BEACON messages are ex-
changed frequently, hundreds of thousands of times a sec-
ond (every few microseconds), the offset can be kept to a
minimum.
Scalability and multi hops Switches and multi-port net-
work interfaces have two to ninety-six ports in a single de-
vice that need to be synchronized within the device5. As a
result, DTP always picks the maximum of all local coun-
ters {lci} as the value for a global counter gc (T5 in Algo-
rithm 2). Then, each port transmits the global counter gc in
a BEACON message (T3 in Algorithm 1).
Choosing the maximum allows any counter to increase
monotonically at the same rate and allows DTP to scale: The
maximum counter value propagates to all network devices
via BEACON messages, and frequent BEACON messages
keep global counters closely synchronized (Section 3.3).
Network dynamics When a device is turned on, the local
and global counters of a network device are set to zero. The
global counter starts incrementing when one of the local
counters starts incrementing (i.e., a peer is connected), and
continuously increments as long as one of the local counters
is incrementing. However, the global counter is set to zero
when all ports become inactive. Thus, the local and global
counters of a newly joining device are always less than those
of other network devices in a DTP network. We use a spe-
cial BEACON_JOIN message in order to make large ad-
justments to a local counter. This message is communi-
cated after INIT_ACK message in order for peers to agree
on the maximum counter value between two local coun-
ters. When a network device with multiple ports receives
a BEACON_JOIN message from one of its ports, it adjusts
its global clock and propagates BEACON_JOIN messages
with its new global counter to other ports. Similarly, if a
network is partitioned and later restored, two subnets will
have different global counters. When the link between them
is re-established, BEACON_JOIN messages allow the two
subnets to agree on the same (maximum) clock counter.
Handling failures There are mainly two types of failures
that need to be handled appropriately: Bit errors and faulty
devices.
IEEE 802.3 standard supports a Bit Error Rate
(BER) objective of 10−12 [9], which means one bit error
5Local counters of a multi-port device will not always be
the same because remote clocks run at different rates. As a
result, a multi-port device must synchronize local counters.
could happen every 100 seconds in 10 GbE. However, it is
possible that a corrupted bit coincides with a DTP message
and could result in a big difference between local and remote
counters. As a result, DTP ignores messages that contain re-
mote counters off by more than eight (See Section 3.3), or
bit errors not in the three least signiﬁcant bits (LSB). Fur-
ther, in order to prevent bit errors in LSBs, each message
could include a parity bit that is computed using three LSBs.
As BEACON messages are communicated very frequently,
ignoring messages with bit errors does not affect the preci-
sion.
Similarly, if one node makes too many jumps (i.e. adjust-
ing local counters upon receiving BEACON messages) in a
short period of time, it assumes the connected peer is faulty.
Given the latency, the interval of BEACON messages, and
maximum oscillator skew between two peers, one can esti-
mate the maximum offset between two clocks and the max-
imum number of jumps. If a port receives a remote counter
outside the estimated offset too often, it considers the peer
to be faulty and stops synchronizing with the faulty device.
3.3 Analysis
As discussed in Section 2.1, the precision of clock syn-
chronization is determined by oscillator skew, interval be-
tween resynchronizations, and errors in reading remote
clocks [24, 29, 33]. In this section, we analyze DTP to un-
derstand its precision in regards to the above factors. In par-
ticular, we analyze the bounds on precision (clock offsets)
and show the following:
• Bound of two tick errors due to measuring the one-way
delay (OWD) during the INIT phase.
• Bound of two tick errors due to the BEACON interval.
The offset of two synchronized peers can be up to two
clock ticks if the interval of BEACON messages is less
than 5000 ticks.
• As a result, the offset of two peers is bound by four
In 10 GbE the
clock ticks or 4T where T is 6.4ns.
offset of two peers is bound by 25.6ns.
• Multi hop precision. As each link can add up to four
tick errors, the precision is bounded by 4T D where 4
is the bound for the clock offset between directly con-
nected peers, T is the clock period and D is the longest
distance in terms of the number of hops.
For simplicity, we use two peers p and q, and use Tp (fp)
and Tq (fq) to denote the period (frequency) of p and q’s
oscillator. We assume for analysis p’s oscillator runs faster
than q’s oscillator, i.e. Tp  fq).
Two tick errors due to OWD. In DTP, the one-way de-
lay (OWD) between two peers, measured during the INIT
phase, is assumed to be stable, constant, and symmetric in
both directions. In practice, however, the delay can be mea-
sured differently depending on when it is measured due to
oscillator skew and how the synchronization FIFO between
the receive and transmit paths interact. Further, the OWD
of one path (from p to q) and that of the other (from q to p)
might not be symmetric due to the same reasons.We show
that DTP still works with very good precision despite any
errors introduced by measuring the OWD.
Suppose p sends an INIT message to q at time t, and the
delay between p and q is d clock cycles. Given the assump-
tion that the length of cables is bounded, and that oscillator
skew is bounded, the delay is d cycles for both directions.
The message arrives at q at t + Tpd (i.e. the elapsed time is
Tpd). Since the message can arrive in the middle of a clock
cycle of q’s clock, it can wait up to Tq before q processes
it. Further, passing data from the receipt path to the trans-
mit path requires a synchronization FIFO between two clock
domains, which can add one more cycle randomly, i.e. the
message could spend an additional Tq before it is received.
Then, the INIT-ACK message from q takes Tqd time to ar-
rive at p, and it could wait up to 2Tp before p processes it. As
a result, it takes up to a total of Tpd + 2Tq + Tqd + 2Tp time
to receive the INIT-ACK message after sending an INIT
message. Thus, the measured OWD, dp, at p is,
dp ≤ ⌊
Tpd + 2Tq + Tqd + 2Tp
Tp
⌋/2 = d + 2
In other words, dp could be one of d, d + 1, or d + 2 clock
cycles depending on when it is measured. As q’s clock is
slower than p, the clock counter of q cannot be larger than p.
However, if the measured OWD, dp, is larger than the actual
OWD, d, then p will think q is faster and adjust its offset
more frequently than necessary (See Transition T 4 in Algo-
rithm 1). This, in consequence, causes the global counter of
the network to go faster than necessary. As a result, α in T2
of Algorithm 1 is introduced.
α = 3 allows dp to always be less than d. In particular,
dp will be d − 1 or d; however, dq will be d − 2 or d − 1.
Fortunately, a measured delay of d − 2 at q does not make
the global counter go faster, but it can increase the offset
between p and q to be two clock ticks most of the time, which
will result in q adjusting its counter by one only when the
actual offset is two.
Two tick errors due to the BEACON interval. The BEACON
interval, period of resynchronization, plays a signiﬁcant role
in bounding the precision. We show that a BEACON interval
of less than 5000 clock ticks can bound the clock offset to
two ticks between peers.
Let Cp(X) be a clock that returns a real time t at which
cp(t) changes to X. Note that the clock is a discrete func-
tion. Then, cp(t) = X means, the value of the clock is stably
X at least after t − Tp, i.e. t − Tp < Cp(X) ≤ t.
Suppose p and q are synchronized at time t1, i.e. cp(t1) =
cq(t1) = X . Also suppose cp(t2) = X + ∆P , and cq(t2) =
X + ∆Q at time t2, where ∆P is the difference between two
counter values of clock p at time t1 and t2. Then,
t2 − Tp < Cp(X + ∆P ) = Cp(X) + ∆P Tp ≤ t2
t2 − Tq < Cq(X + ∆Q) = Cq(X) + ∆QTq ≤ t2
Then, the offset between two clocks at t2 is,
∆t(fp − fq) − 2 < ∆P − ∆Q < ∆t(fp − fq) + 2
where ∆t = t2 − t1.
Since the maximum frequency of a NIC clock oscillator is
1.0001f , and the minimum frequency is 0.9999f , ∆t(fp −
fq) is always smaller than 1 if ∆t is less than 32 us. As a
Media Access Control (MAC)
Reconciliation Sublayer (RS)
TX 32bit
XGMII 156.25 MHz
RX 32bit
Physical Coding Sublayer (PCS)
TX 16bit
XSBI 644.53125MHz
RX 16bit
Physical Medium Attachment (PMA)
Physical Medium Dependent (PMD)
Figure 3: Low layers of a 10 GbE network stack. Grayed
rectangles are DTP sublayers, and the circle represents a
synchronization FIFO.
result, ∆P − ∆Q can be always less than or equal to 2, if the
interval of resynchronization (∆t) is less than 32 us (≈ 5000
ticks). Considering the maximum latency of the cable is less
than 5 us (≈ 800 ticks), a beacon interval less than 25 us
(≈ 4000 ticks) is sufﬁcient for any two peers to synchronize
with 12.8 ns (= 2 ticks) precision.
Multi hop Precision. Note that DTP always picks the max-
imum clock counter of all nodes as the global counter. All
clocks will always be synchronized to the fastest clock in the
network, and the global counter always increases monoton-
ically. Then, the maximum offset between any two clocks
in a network is between the fastest and the slowest. As dis-
cussed above, any link between them can add at most two
offset errors from the measured delay and two offset er-
rors from BEACON interval. Therefore, the maximum offset
within a DTP-enabled network is bounded by 4T D where D
is the longest distance between any two nodes in a network
in terms of number of hops, and T is the period of the clock
as deﬁned in the IEEE 802.3 standard (≈ 6.4ns).
4.
IMPLEMENTATION
In this section, we brieﬂy discuss the IEEE 802.3ae 10
Gigabit Ethernet standard before presenting how we modify
the physical layer to support DTP.
4.1
IEEE 802.3 Standard
According to the IEEE 802.3ae, the physical layer (PHY)
of 10 GbE consists of three sublayers (Figure 3): The Phys-
ical Coding Sublayer (PCS), the Physical Medium Attach-
ment (PMA), and the Physical Medium Dependent (PMD).
The PMD is responsible for transmitting the outgoing sym-
bolstream over the physical medium and receiving the in-
coming symbolstream from the medium. The PMA is re-
sponsible for clock recovery and (de-)serializing the bit-
stream. The PCS performs 64b/66b encoding / decoding.
In the PHY, there is a 66-bit Control block (/E/), which
encodes eight seven-bit idle characters (/I/). As the stan-
dard requires at least twelve /I/s in an interpacket gap, it
is guaranteed to have at least one /E/ block preceding any
Global Counter = Max(LC0, LC1, LC2, LC3)
Local 
Counter
Port 0
Port 1
Port 2
Port 3
Remote 
Counter
Global Counter
Figure 4: DTP enabled four-port device.
Ethernet frame6. Moreover, when there is no Ethernet frame,
there are always /E/ blocks: 10 GbE is always sending at
10 Gbps and sends /E/ blocks continuously if there are no
Ethernet frames to send.
As brieﬂy mentioned in Section 2, the PCS of the trans-
mit path is driven by the local oscillator, and the PCS of the
receive path is driven by the recovered clock from the in-
coming bitstream. See Figure 2.
4.2 DTP-enabled PHY
The control logic of DTP in a network port consists of
Algorithm 1 from Section 3 and a local counter. The lo-
cal counter is a 106-bit integer (2 × 53 bits) that incre-
ments at every clock tick (6.4 ns = 1/156.25 MHz), or is
adjusted based on received BEACON messages. Note that
the same oscillator drives all modules in the PCS sublayer
on the transmit path and the control logic that increments
the local counter. i.e. they are in the same clock domain. As
a result, the DTP sublayer can easily insert the local clock
counter into a protocol message with no delay.
The DTP-enabled PHY is illustrated in Figure 3. Figure 3
is exactly the same as the PCS from the standard, except
that Figure 3 has DTP control, TX DTP, and RX DTP sub-
layers shaded in gray. Speciﬁcally, on the transmit path,
the TX DTP sublayer inserts protocol messages, while, on
the receive path, the RX DTP sublayer processes incoming
protocol messages and forwards them to the control logic
through a synchronization FIFO. After the RX DTP sublayer
receives and uses a DTP protocol message from the Control
block (/E/), it replaces the DTP message with idle char-
acters (/I/s, all 0’s) as required by the standard such that
higher network layers do not know about the existence of
the DTP sublayer. Lastly, when an Ethernet frame is being
processed in the PCS sublayer in general, DTP simply for-
wards blocks of the Ethernet frame unaltered between the
PCS sublayers.
4.3 DTP-enabled network device
A DTP-enabled device (Figure 4) can be implemented
with additional logic on top of the DTP-enabled ports. The
logic maintains the 106-bit global counter as shown in Al-
gorithm 2, which computes the maximum of the local coun-
ters of all ports in the device. The computation can be op-
timized with a tree-structured circuit to reduce latency, and
can be performed in a deterministic number of cycles. When
6Full-duplex Ethernet standards such as 1, 10, 40, 100 GbE
send at least twelve /I/s (at least one /E/) between every
Ethernet frame.
a switch port tries to send a BEACON message, it inserts the
global counter into the message, instead of the local counter.
Consequently, all switch ports are synchronized to the same
global counter value.
4.4 Protocol messages
DTP uses /I/s in the /E/ control block to deliver pro-
tocol messages. There are eight seven-bit /I/s in an /E/
control block, and, as a result, 56 bits total are available for
a DTP protocol message per /E/ control block. Modifying
control blocks to deliver DTP messages does not affect the
physics of a network interface since the bits are scrambled
to maintain DC balance before sending on the wire (See the
scrambler/descrambler in Figure 3). Moreover, using /E/
blocks do not affect higher layers since DTP replaces /E/
blocks with required /I/s (zeros) upon processing them.
A DTP message consists of a three-bit message type, and
a 53-bit payload. There are ﬁve different message types in
DTP: INIT, INIT-ACK, BEACON, BEACON-JOIN, and
BEACON-MSB. As a result, three bits are sufﬁcient to en-
code all possible message types. The payload of a DTP mes-
sage contains the local (global) counter of the sender. Since
the local counter is a 106-bit integer and there are only 53
bits available in the payload, each DTP message carries the
53 least signiﬁcant bits of the counter. In 10 GbE, a clock
counter increments at every 6.4 ns (=1/156.25MHz), and it
takes about 667 days to overﬂow 53 bits. DTP occasion-
ally transmits the 53 most signiﬁcant bits in a BEACON-MSB
message in order to prevent overﬂow.
As mentioned in Section 4.1, it is always possible to trans-
mit one protocol message after/before an Ethernet frame is
transmitted. This means that when the link is fully saturated
with Ethernet frames DTP can send a BEACON message ev-
ery 200 clock cycles (≈ 1280 ns) for MTU-sized (1522B)
frames7 and 1200 clock cycles (≈ 7680 ns) at worst for
jumbo-sized (≈9kB) frames. The PHY requires about 191
66-bit blocks and 1,129 66-bit blocks to transmit a MTU-
sized or jumbo-sized frame, respectively. This is more than
sufﬁcient to precisely synchronize clocks as analyzed in Sec-
tion 3.3 and evaluated in Section 6. Further, DTP communi-
cates frequently when there are no Ethernet frames, e.g every
200 clock cycles, or 1280 ns: The PHY continuously sends
/E/ when there are no Ethernet frames to send.
5. PRACTICAL CONSIDERATIONS
5.1 Accessing DTP counters
Applications access the DTP counter via a DTP daemon
that runs in each server. A DTP daemon regularly (e.g.,
once per second) reads the DTP counter of a network in-
terface card via a memory-mapped IO in order to minimize
errors in reading the counter. Further, TSC counters are em-
ployed to estimate the frequency of the DTP counter. A
TSC counter is a reliable and stable source to implement
software clocks [46, 50, 25]. Modern systems support in-
7It includes 8-byte preambles, an Ethernet header, 1500-byte
payload and a checksum value.
variant TSC counters that are not affected by CPU power