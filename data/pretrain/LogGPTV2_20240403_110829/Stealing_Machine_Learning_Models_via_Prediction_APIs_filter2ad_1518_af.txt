Budget Factor α
100
0
50
25
75
Budget Factor α
100
Figure 5: Average error of extracted softmax models. Results are
for three retraining strategies applied to models trained on all multiclass
data sets from Table 3. The left shows Rtest and the right shows Runif.
Rtest
Runif
Uniform
Line-Search
Adaptive
r
o
r
r
E
n
o
i
t
c
a
r
t
x
E
.
g
v
A
100
10−1
10−2
10−3
0
50
25
75
Budget Factor α
100
0
Figure 6: Average error of extracted RBF kernel SVMs Results
are for three retraining strategies applied to models trained on all binary
data sets from Table 3. The left shows Rtest and the right shows Runif.
50
25
75
Budget Factor α
100
rameters and non-linear decision-boundaries. Therefore,
we may need to find a large number of points close to a
decision boundary in order to extract it accurately.
We evaluated our attacks on the multiclass models
from Table 3. For the tested query budgets, line-search
and adaptive retraining gave little benefit over uniform
retraining. For a budget of 100· k, where k is the num-
ber of model parameters, we get Rtest = 99.16% and
Runif = 98.24%, using 108,200 queries per model on av-
erage. Our attacks might improve for higher budgets but
it is unclear whether they would then provide any mone-
tary advantage over using ML APIs in an honest way.
6.4 RBF Kernel SVMs
Another class of nonlinear models that we consider are
support-vector machines (SVMs) with radial-basis func-
tion (RBF) kernels. A kernel SVM first maps inputs into
a higher-dimensional space, and then finds the hyper-
plane that maximally separates the two classes. As men-
tioned in Section 6, SVMs with polynomial kernels can
be extracted using the Lowd-Meek attack in the trans-
formed feature space. For RBF kernels, this is not possi-
ble because the transformed space has infinite dimension.
SVMs do not provide class probability estimates. Our
only applicable attack is thus retraining. As for linear
models, we vary the query budget as α · (d + 1), where
d is the input dimension. We further use the extract-and-
test approach from Section 5 to find the value of the RBF
kernel’s hyper-parameter. Results of our attacks are in
USENIX Association  
25th USENIX Security Symposium  613
Figure 6. Again, we see that adaptive retraining performs
best, even though the decision boundary to extract is non-
linear (in input space) here. Kernel SVMs models are
overall harder to retrain than models with linear decision
boundaries. Yet, for our largest budgets (2,050 queries
on average), we do extract models with over 99% accu-
racy, which may sufﬁce in certain adversarial settings.
Rtest
10−1
10−2
10−3
10−4
0
0
20
Labels only
2 decimals
3 decimals
4 decimals
5 decimals
No rounding
40
60
Budget Factor α
80
100
7 Extraction Countermeasures
We have shown in Sections 4 and 5 that adversarial
clients can effectively extract ML models given access
to rich prediction APIs. Given that this undermines the
ﬁnancial models targeted by some ML cloud services,
and potentially leaks conﬁdential training data, we be-
lieve researchers should seek countermeasures.
In Section 6, we analyzed the most obvious defense
against our attacks: prediction API minimization. The
constraint here is that the resulting API must still be use-
ful in (honest) applications. For example, it is simple to
change APIs to not return conﬁdences and not respond
to incomplete queries, assuming applications can get by
without it. This will prevent many of our attacks, most
notably the ones described in Section 4 as well as the fea-
ture discovery techniques used in our Amazon case study
(Section 5). Yet, we showed that even if we strip an API
to only provide class labels, successful attacks remain
possible (Section 6), albeit at a much higher query cost.
We discuss further potential countermeasures below.
Rounding confidences. Applications might need con-
ﬁdences, but only at lower granularity. A possible de-
fense is to round conﬁdence scores to some ﬁxed preci-
sion [23]. We note that ML APIs already work with some
ﬁnite precision when answering queries. For instance,
BigML reports conﬁdences with 5 decimal places, and
Amazon provides values with 16 signiﬁcant digits.
To understand the effects of limiting precision further,
we re-evaluate equation-solving and decision tree path-
ﬁnding attacks with conﬁdence scores rounded to a ﬁxed
decimal place. For equation-solving attacks, rounding
the class probabilities means that the solution to the ob-
tained equation-system might not be the target
f , but
some truncated version of it. For decision trees, round-
ing conﬁdence scores increases the chance of node id
collisions, and thus decreases our attacks’ success rate.
Figure 7 shows the results of experiments on softmax
models, with class probabilities rounded to 2–5 decimals.
We plot only Rtest, the results for Runif being similar. We
observe that class probabilities rounded to 4 or 5 deci-
mal places (as done already in BigML) have no effect on
the attack’s success. When rounding further to 3 and 2
decimal places, the attack is weakened, but still vastly
outperforms adaptive retraining using class labels only.
Figure 7: Effect of rounding on model extraction. Shows the av-
erage test error of equation-solving attacks on softmax models trained
on the benchmark suite (Table 3), as we vary the number of signiﬁcant
digits in reported class probabilities. Extraction with no rounding and
with class labels only (adaptive retraining) are added for comparison.
For regression trees, rounding has no effect on our at-
tacks. Indeed, for the models we considered, the output
itself is unique in each leaf (we could also round out-
puts, but the impact on utility may be more critical). For
classiﬁcation trees, we re-evaluated our top-down attack,
with conﬁdence scores rounded to fewer than 5 decimal
places. The attacks on the ‘IRS Tax Patterns’ and ‘Email
Importance’ models are the most resilient, and suffer no
success degradation before scores are rounded to 2 deci-
mal places. For the other models, rounding conﬁdences
to 3 or 4 decimal places severely undermines our attack.
Differential privacy. Differential privacy (DP) [22]
and its variants [34] have been explored as mechanisms
for protecting, in particular, the privacy of ML train-
ing data [54]. DP learning has been applied to regres-
sions [17,56], SVMs [44], decision trees [31] and neural
networks [48]. As some of our extraction attacks leak
training data information (Section 4.1.3), one may ask
whether DP can prevent extraction, or at least reduce the
severity of the privacy violations that extraction enables.
Consider na¨ıve application of DP to protect individual
training data elements. This should, in theory, decrease
the ability of an adversary A to learn information about
training set elements, when given access to prediction
queries. One would not expect, however, that this pre-
vents model extraction, as DP is not deﬁned to do so:
consider a trivially useless learning algorithm for binary
logistic regression, that discards the training data and sets
w and β to 0. This algorithm is differentially private, yet
w and β can easily be recovered using equation-solving.
A more appropriate strategy would be to apply DP di-
rectly to the model parameters, which would amount to
saying that a query should not allow A to distinguish be-
tween closely neighboring model parameters. How ex-
actly this would work and what privacy budgets would
be required is left as an open question by our work.
Ensemble methods. Ensemble methods such as ran-
dom forests return as prediction an aggregation of pre-
614  25th USENIX Security Symposium 
USENIX Association
dictions by a number of individual models. While we
have not experimented with ensemble methods as targets,
we suspect that they may be more resilient to extraction
attacks, in the sense that attackers will only be able to ob-
tain relatively coarse approximations of the target func-
tion. Nevertheless, ensemble methods may still be vul-
nerable to other attacks such as model evasion [55].
8 Related Work
Our work is related to the extensive literature on learning
theory, such as PAC learning [53] and its variants [3, 8].
Indeed, extraction can be viewed as a type of learning, in
which an unknown instance of a known hypothesis class
(model type) is providing labels (without error). This is
often called learning with membership queries [3]. Our
setting differs from these in two ways. The ﬁrst is con-
ceptual: in PAC learning one builds algorithms to learn a
concept — the terminology belies the motivation of for-
malizing learning from data. In model extraction, an at-
tacker is literally given a function oracle that it seeks to
illicitly determine. The second difference is more prag-
matic: prediction APIs reveal richer information than as-
sumed in prior learning theory work, and we exploit that.
Algorithms for learning with membership queries
have been proposed for Boolean functions [7, 15, 30, 33]
and various binary classiﬁers [36, 39, 50]. The latter line
of work, initiated by Lowd and Meek [36], studies strate-
gies for model evasion, in the context of spam or fraud
detectors [9, 29, 36, 37, 55]. Intuitively, model extraction
seems harder than evasion, and this is corroborated by
results from theory [36, 39, 50] and practice [36, 55].
Evasion attacks fall into the larger ﬁeld of adversarial
machine learning, that studies machine learning in gen-
eral adversarial settings [6,29]. In that context, a number
of authors have considered strategies and defenses for
poisoning attacks, that consist in injecting maliciously
crafted samples into a model’s train or test data, so as to
decrease the learned model’s accuracy [10,21,32,40,45].
In a non-malicious setting, improper model extraction
techniques have been applied for interpreting [2, 19, 52]
and compressing [16, 27] complex neural networks.
9 Conclusion
We demonstrated how the ﬂexible prediction APIs ex-
posed by current ML-as-a-service providers enable new
model extraction attacks that could subvert model mon-
etization, violate training-data privacy, and facilitate
model evasion. Through local experiments and online
attacks on two major providers, BigML and Amazon,
we illustrated the efﬁciency and broad applicability of
attacks that exploit common API features, such as the
availability of conﬁdence scores or the ability to query
arbitrary partial inputs. We presented a generic equation-
solving attack for models with a logistic output layer and
a novel path-ﬁnding algorithm for decision trees.
We further explored potential countermeasures to
these attacks, the most obvious being a restriction on the
information provided by ML APIs. Building upon prior
work from learning-theory, we showed how an attacker
that only obtains class labels for adaptively chosen in-
puts, may launch less effective, yet potentially harmful,
retraining attacks. Evaluating these attacks, as well as
more reﬁned countermeasures, on production-grade ML
services is an interesting avenue for future work.
Acknowledgments. We thank Mart´ın Abadi and the
anonymous reviewers for their comments. This work
was supported by NSF grants 1330599, 1330308, and
1546033, as well as a generous gift from Microsoft.
References
[1] AMAZON WEB SERVICES.
https://aws.amazon.com/
machine-learning. Accessed Feb. 10, 2016.
[2] ANDREWS, R., DIEDERICH, J., AND TICKLE, A. Survey and
critique of techniques for extracting rules from trained artiﬁcial
neural networks. KBS 8, 6 (1995), 373–389.
[3] ANGLUIN, D. Queries and concept learning. Machine learning
2, 4 (1988), 319–342.
[4] ATENIESE, G., MANCINI, L. V., SPOGNARDI, A., VILLANI,
A., VITALI, D., AND FELICI, G. Hacking smart machines
with smarter ones: How to extract meaningful data from machine
learning classiﬁers. IJSN 10, 3 (2015), 137–150.
[5] AT&T LABORATORIES CAMBRIDGE.
The ORL database
http://www.cl.cam.ac.uk/research/dtg/
faces.
of
attarchive/facedatabase.html.
[6] BARRENO, M., NELSON, B., SEARS, R., JOSEPH, A. D., AND
TYGAR, J. D. Can machine learning be secure? In ASIACCS
(2006), ACM, pp. 16–25.
[7] BELLARE, M. A technique for upper bounding the spectral norm
with applications to learning. In COLT (1992), ACM, pp. 62–70.
[8] BENEDEK, G. M., AND ITAI, A. Learnability with respect to
ﬁxed distributions. TCS 86, 2 (1991), 377–389.
[9] BIGGIO, B., CORONA,
I., MAIORCA, D., NELSON, B.,
ˇSRNDI ´C, N., LASKOV, P., GIACINTO, G., AND ROLI, F. Eva-
sion attacks against machine learning at test time.
In ECML
PKDD. Springer, 2013, pp. 387–402.
[10] BIGGIO, B., NELSON, B., AND LASKOV, P. Poisoning attacks
against support vector machines. In ICML (2012).
[11] BIGML. https://www.bigml.com. Accessed Feb. 10, 2016.
[12] BLUM, A. L., AND LANGLEY, P. Selection of relevant features
and examples in machine learning. Artiﬁcial intelligence 97, 1
(1997), 245–271.
[13] BLUMER, A., EHRENFEUCHT, A., HAUSSLER, D., AND WAR-
MUTH, M. K. Occam’s razor. Readings in machine learning
(1990), 201–204.
[14] BOSER, B. E., GUYON, I. M., AND VAPNIK, V. N. A training
algorithm for optimal margin classiﬁers. In COLT (1992), ACM,
pp. 144–152.
USENIX Association  
25th USENIX Security Symposium  615
[15] BSHOUTY, N. H. Exact learning boolean functions via the mono-
tone theory. Inform. Comp. 123, 1 (1995), 146–153.
[16] BUCILU ˇA, C., CARUANA, R., AND NICULESCU-MIZIL, A.
Model compression. In KDD (2006), ACM, pp. 535–541.
[17] CHAUDHURI, K., AND MONTELEONI, C. Privacy-preserving
logistic regression. In NIPS (2009), pp. 289–296.
[18] COHN, D., ATLAS, L., AND LADNER, R.
Improving gener-
alization with active learning. Machine learning 15, 2 (1994),
201–221.
[19] CRAVEN, M. W., AND SHAVLIK, J. W.
Extracting tree-
structured representations of trained networks. In NIPS (1996).
[20] CYBENKO, G. Approximation by superpositions of a sigmoidal
function. MCSS 2, 4 (1989), 303–314.
[21] DALVI, N., DOMINGOS, P., SANGHAI, S., VERMA, D., ET AL.
Adversarial classiﬁcation. In KDD (2004), ACM, pp. 99–108.
[22] DWORK, C. Differential privacy. In ICALP (2006), Springer.
[23] FREDRIKSON, M., JHA, S., AND RISTENPART, T. Model inver-
sion attacks that exploit conﬁdence information and basic coun-
termeasures. In CCS (2015), ACM, pp. 1322–1333.
[24] FREDRIKSON, M., LANTZ, E., JHA, S., LIN, S., PAGE, D.,
AND RISTENPART, T. Privacy in pharmacogenetics: An end-
to-end case study of personalized Warfarin dosing. In USENIX
Security (2014), pp. 17–32.
[25] GOOGLE PREDICTION API.