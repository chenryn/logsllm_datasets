formation, due to an apparent instrumentation anomaly.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:52:28 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007Data were passed through a ﬁlter to exclude letters whose
ﬁrst few sampled points contained timestamp intervals
shorter than 0.01 seconds (half the typical time lag).
The ﬁlter’s results corresponded well with two human-
rater judgments of which letters looked unrealistic, out
of a sample of 1166 “Y”s. Although the ﬁlter removed
22.33% (16,455 out of 73,684) of the letters, sufﬁcient
samples remained for analysis. Little data (as little as
0%) was excluded for many subjects, while much data
(as much as 72%) was excluded for some subjects; the
median percent data loss was 20%. From the perspective
of letters, the smallest percent data loss for a given let-
ter was 8% while the greatest was 32%; the median was
22%. The smallest number of instances of a given letter
in the High-quality Data, for any single user, was 13.
Figure 3. Unrealistic “Y”s (left within each
pair); realistic “Y”s (right within each pair)
The data-capture irregularities do not appear to
greatly harm letter recognition on the Palm PDA, since
gross letter shapes remain. Anecdotally, one subject re-
ﬂected that a carefully-written letter was occasionally not
recognized. It may be that instrumentation errors occur
sporadically, with some causing letter-recognition to fail,
and others not. Newer or more sophisticated Palm hand-
helds might ameliorate this problem; future work should
keep data quality in mind.
Reduced Data. This version of the data corpus was
built to match exactly all proportions of the high-quality
version (on a per-letter and per-subject basis). The only
difference was that letter instances were included without
regard to quality; they were selected at random.
All Data. This version used the data corpus in its
entirety. The fewest instances of a given letter, for all
users, was 44.
5.5. Feature extraction
Thirteen quantitative features were extracted from
each letter stroke, which is represented natively in the
Palm PDA as a sequence of  values.
Features appear in Table 1 and are elaborated below.
Each feature attempts to capture a salient character-
istic of Grafﬁti handwriting. Time to Write informs about
the writer’s speed. The four extreme coordinates (Hor-
izontal and Vertical Minimum and Maximum) describe
the location of the letter in the writing box, which varies
Table 1. Features of Grafﬁti letter strokes
Time to Write
Horiz Minimum
Horiz Maximum
Vert Minimum
Vert Maximum
H.Start-End Dist (±)
V.Start-End Dist (±)
Horiz Dist Travelled
Vert Dist Travelled
Letter Length
Direction Changes
Mean Slope
Std Dev of the Slope
Elapsed time to write letter.
Min. coord. value along x-axis.
Max. coord. value along x-axis.
Min. coord. value along y-axis.
Max. coord. value along y-axis.
(xLast.coord. − xF irst.coord.).
(yLast.coord. − yF irst.coord.).
Sum of horizontal distances be-
tween successive points.
Sum of vertical distances be-
tween successive points.
Sum of line-segment lengths be-
tween successive points.
Count
between
left↔right or up↔down motion.
Avg. value of pairwise slopes.
Std. dev. of pairwise slopes.
changes
of
among users by handedness or habit. Horizontal and Ver-
tical Start-to-End Distance (Signed) help inform whether
a canonical or alternative stroke is used, and describe an
aspect of letter shape. Horizontal and Vertical Distance
Travelled measure how much the stylus moves along a
particular axis, whereas Letter Length measures the to-
tal amount of writing in a letter; all encode how sim-
ple or intricate a letter is. Direction Changes indicates
how straight or shaky a stroke is. Mean Slope and Stan-
dard Deviation of the Slope represent a composite slant
quality, and its consistency. Only ﬁnite pairwise slopes
were included in the latter calculations. Nearly 2% of “I”
strokes were perfectly vertical, which resulted in all their
pairwise slopes being inﬁnite. We imputed slope features
in those cases by assuming that each “I” was not purely
vertical, but rather perfectly slanted halfway between ver-
tical and the smallest detectable positive slope.
5.6. Feature transformation and scaling
Classiﬁers often perform better when data are nor-
mally distributed, or when a normalizing transform is ap-
plied to the data. We transformed the values of each indi-
vidual feature (across all subjects at once) using the ver-
satile Box-Cox power transformation [2], to effect greater
symmetry on each feature’s distribution. A program writ-
ten in the statistical language R [19] semi-automatically
searched for a good value of the Box-Cox parameter λ,
one for each feature. The 13 transformed feature distri-
butions were plotted to inspect their symmetry visually
and to verify that an appropriate transformation had been
found. After transformation, the feature data were scaled
(within each feature, across all subjects at once), such
that each feature mean became 0 and each feature stan-
dard deviation became 1. Data scaling is recommended
before SVM classiﬁcation [9].
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:52:28 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 20075.7. Training, testing, and evaluation data sets
To train and evaluate the biometric system described
in Section 6, three distinct sets of feature data were used:
(1) SVM-train, to train SVM letter classiﬁers; (2) SVM-
test, to test the classiﬁers and inform challenge-string cre-
ation; and (3) Evaluate-test, to evaluate the entire system.
We split the feature data into these three sets according to
a 50%/25%/25% rule, in the following way. Before split-
ting, we grouped the feature vectors by subject (making
52 groups), and then within each subject, by letter (mak-
ing 26 groups for each subject). Next, for each subject,
we randomized the order of feature vectors within each
of the 26 letter groups. This made the data more closely
resemble the writing of practiced users, thereby mitigat-
ing potential learning effects among novice subjects. Af-
ter splitting, out of each subject’s “A”s, SVM-train had
50%; SVM-test had 25%; and Evaluate-test had 25%; the
same percentages held for other letters. During system
evaluation (see Section 7), SVM-train and SVM-test are
combined to form Evaluate-train, which contains 75% of
each subject’s letter “A”s, et cetera. Evaluate-train holds
enrollment data and Evaluate-test holds evaluation data.
6. Biometric system construction
In our example application, an enrolled user ap-
proaches a biometric system and claims an identity. The
system issues a challenge string tailored to that identity,
and the user responds by writing the string in constrained
handwriting, using a stylus on a digitized screen. Next,
the biometric system decides which enrolled user most
likely wrote the sample. Our system prototype incorpo-
rates 26 SVM letter classiﬁers, each trained on enroll-
ment data for its respective letter. When a given letter is
present in a writing sample, the corresponding classiﬁer
is invoked; decision logic is used to combine the outputs
of the classiﬁers involved, to predict the writer’s identity.
The following sections describe (1) how the SVM
letter classiﬁers were built; (2) how the classiﬁers were
tested to produce user- and letter-speciﬁc information
about errors; (3) how potential challenge strings were de-
vised using this error information; and (4) how decision
logic yields writer predictions by the biometric system.
6.1. Letter classiﬁers
The purpose of a letter classiﬁer is to determine the
probability that each subject wrote a given letter, and
to choose the most likely writer. We employed support
vector machine (SVM) classiﬁers, because they achieve
state-of-the-art performance on handwritten character
recognition [6], a problem similar to the one we address.
About SVMs. Support vector machines [4, 22] use
supervised learning for classiﬁcation and regression; they
are closely related to neural networks. SVM classiﬁers
n) such that
transform data into n-dimensional space (R
an n-dimensional hyperplane can be found to optimally
separate the data into classes. These classiﬁers maximize
the margin (the margin is the distance between classes
in n-dimensional space) as well as minimize empirical
classiﬁcation errors. SVMs are kernel-based methods;
common options include linear, polynomial, radial basis
function (RBF), and sigmoid kernels. RBF kernels are
reasonable choices for studies in new domains, because
they have fewer parameters, and because they avoid nu-
merical difﬁculties that other kernels can encounter [9].
The two parameters in RBF kernels are called γ and
cost; γ determines the width of the RBF, while cost deter-
mines the trade-off between reducing errors and creating
a wider margin. Wider margins generalize better, so per-
mitting more errors on training data may prove advanta-
geous. RBF kernel parameters must be tuned before use,
to ﬁnd their ideal values on the data at hand.
Using SVMs to build letter classiﬁers.
Individ-
ual letter classiﬁers, as well as the biometric system
as a whole, were built with tools from the R statis-
tical computing project [19],
the LIBSVM library of
tools for support vector machines [5], and the R package
e1071 [7] that provides an interface between the R pro-
gramming environment and LIBSVM. Under LIBSVM,
multi-class classiﬁcation employs the one-against-one
approach [10]. Given k classes, one for each subject iden-
tity, k(k− 1)/2 binary classiﬁers are constructed, one for
each pair of classes; the appropriate class label is found
through voting. We used unweighted SVMs, along with
the option in LIBSVM to report probability estimates.
The subject who is assigned the highest probability be-
comes the classiﬁer’s predicted writer.
Twenty-six SVM classiﬁers, one for each Grafﬁti let-
ter, were trained using SVM-train data. Each classiﬁer
employed an RBF kernel, whose parameters were tuned
as follows. Five-fold cross-validation [8] was used to se-
lect the best values of cost and γ for each letter classi-
ﬁer. Fifteen cost values and 13 γ values were tried in
every pairwise combination (195 total). The search space
formed a two-dimensional grid of powers of 2; exponents
for cost ranged from -2 to 26 (in 15 steps of two), whereas
those for γ ranged from -22 to 2 (in 13 steps of two). For
each letter, average accuracy rates over the ﬁve folds were
recorded for the 195 trained classiﬁers. The highest score
determined the best parameter pair, which was then ﬁxed
for the relevant letter classiﬁer. We veriﬁed that a local
maximum occurred in the grid area, not on the boundary.
6.2. Confusion matrices and charts
To determine which letters are superior at distin-
guishing one subject from all the rest, classiﬁcation errors
must be studied. Accordingly, each letter classiﬁer was
re-trained on the entire SVM-train data set (using the best
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:52:28 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007Table 2. Letter-classiﬁer accuracies, high-
quality data (50% training, 25% test data)
Y
76.09%
B
72.11%
D
70.12%
P
69.45%
E
68.34%
M 67.12%
W 65.36%
R
63.26%
G
62.96%
V
Q
Z
K
S
X
A
N
H
60.79%
59.88%
59.70%
59.66%
59.43%
58.61%
58.13%
57.79%
57.53%
U
O
C
F
J
L
T
I
57.51%
57.02%
55.74%
55.26%
53.90%
52.02%
51.02%
36.95%
parameter values), and then tested using SVM-test data,
without cross-validation. The SVM-test data informs the
challenge strings, while the Evaluate-test data estimates
system accuracy; no data used in system creation was
reused in its evaluation.
In the High-quality Data ex-
periment, accuracy rates for letter classiﬁers ranged from
37% to 76% (see Table 2). Performing worst was “I”,
simplest of all Grafﬁti letters, while “Y”, rich in detail
and having an alternative stroke, performed best.
Using the results of the tests, 26 confusion matrices
were built, one per letter classiﬁer, each having dimen-
sions of 52 users by 52 users. A confusion matrix shows
how often one user was (mis)classiﬁed as each of the oth-
ers. Rows are associated with true writer identities, and
columns with predicted writer identities. All scores in a
row were normalized to sum to 1. For the “A” matrix,
the cell in row 1, column 2 contains the fraction of “A”s
written by User1, but erroneously predicted by the “A”