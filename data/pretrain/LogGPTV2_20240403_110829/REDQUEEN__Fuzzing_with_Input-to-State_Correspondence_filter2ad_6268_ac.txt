fuzzing with this newly patched binary because we will ﬁx
these problems later.
iii) Veriﬁcation. After we performed the whole set of fuzzing
stages on one input, we have a queue of preliminary results.
Due to our patches, these inputs might not show the expected
behavior on an unpatched target. During the veriﬁcation phase,
we try to ﬁx these invalid inputs by applying input-to-state
based mutations obtained from the patched instructions. Then,
we execute the ﬁxed inputs on the unpatched, real target. If
they still trigger new coverage, the ﬁxed input is written to
the real queue. Otherwise, we discard the patch. After all
preliminary inputs were processed this way, the preliminary
queue is cleared for the next round.
IV.
IMPLEMENTATION DETAILS
In the following, we provide a brief overview of
REDQUEEN, the proof-of-concept implementation of our ap-
proach. We based our implementation of REDQUEEN on our
fuzzer KAFL [37].
A. kAFL Fuzzer
KAFL is an OS-agnostic, AFL-inspired feedback-driven
kernel fuzzer that uses the hardware-accelerated tracing feature
Intel Processor Trace (Intel PT) to obtain coverage information
without
is build on
top of modiﬁed versions of KVM and QEMU (namely KVM-PT
and QEMU-PT) to execute arbitrary x86 operating systems in
isolation. This way, the fuzzer can beneﬁt from virtualization
capabilities such as hardware-accelerated execution of code,
the need to instrument
the target. It
snapshots, and Intel PT tracing of the guest’s code. KVM-PT
enables Intel PT tracing of the guest and QEMU-PT decodes
the generated traces into an AFL-compatible bitmap. To do
so, QEMU-PT maintains a runtime disassembly of the target.
This disassembly is perfect, in the sense that we disassemble
each instruction which was executed according to the Intel PT
trace. We use this disassembly to identify instructions to hook
or to patch. This avoids problems which arise from patching
code based on a static disassembly, which might misclassify
some bytes. Both QEMU-PT and KVM-PT also provide custom
hypercalls and direct memory access to the guest’s memory to
facilitate the necessary communication and data transfer with
the target. The fuzzer logic is based on the AFL fuzzing loop
(with an added radamsa [29] stage) and was re-implemented
in Python. Hence, the fuzzing logic is independent from the
target operating system. We also ﬁxed a number of bugs in the
decoding of Intel PT packets that removed a signiﬁcant amount
of non-determinism resulting from broken traces. In total we
added and changed about 10k lines of code. A large part of
these changes are not due to the techniques proposed in this
paper. Most of these changes were performed to add support
for ring 3 fuzzing, to provide VMI capabilities in KAFL and to
ﬁx bugs. Furthermore, these numbers also contain a signiﬁcant
amount of code used for evaluation and debugging purposes.
Our techniques require a few primitives: the ability to
obtain a program trace, to inspect the program state at various
breakpoints, and to patch instructions in memory. REDQUEEN
still uses the architecture of KAFL and obtains coverage
information in precisely the same manner. Additionally,
it
uses the VMI provided by KVM-PT and QEMU-PT to insert
breakpoints and to inspect memory and register content during
execution. In the following, we discuss how we implemented
our techniques on top of KAFL.
B. Comparison Hooking
is hit,
We rely on hardware-assisted virtual machine breakpoints
to extract input-to-state correspondences. Each time the run-
time disassembler encounters an interesting compare-like in-
struction,
its address is stored such that a hook can be
placed during the next REDQUEEN analysis phase. During the
REDQUEEN phase, breakpoints are placed on all interesting
instructions. When a breakpoint
the arguments are
extracted and saved to a buffer for later use by the fuzzing
logic. Breakpoints are removed after they are hit a small
number of times to limit the performance impact. Note, we
do not only hook cmp instructions but also call instructions
and subtractions. The former are used to identify string and
memory compares, while subtractions are often emitted by
compilers in place of cmp instructions to implement switch
tables. To implement a subtraction, a compiler sometimes
emits special lea or add instructions with a negative offset.
If the ﬁrst two arguments of a call instruction are valid
pointers (according to various calling conventions), we assume
the function to be a potential compare function and dump the
ﬁrst 128 bytes of memory for each argument.
C. Colorization
During the colorization step, we try to replace as many
bytes with random values as possible, without changing the
execution path (more precisely, the hash of the AFL bitmap).
This increases the entropy in the input and, therefore, reduces
the number of positions at which an observed pattern can be
applied. This can be done using a binary search approach as
shown in Algorithm 1 which will usually converge within a
small number of executions (typically, in the order of a few
hundred). The worst case would be comparable to one eighth
of the number of bit-ﬂips performed by AFL on each input.
Additionally, in our implementation, we limited the search to
a maximum of 1000 steps. This worked well even for the ﬁle
system driver targets, with a minimum input size of 64 KB.
Algorithm 1: Algorithm for colorizing inputs to efﬁ-
ciently deal with a large number of candidate positions
Data: input is the uncolored input
Result: A version of input that has a signiﬁcantly higher entropy
1 ranges ← (1. . . len(input))
2 original hash ← get bitmap hash(input)
3 while rng = pop biggest range( ranges ) do
4
5
6
7
8
9
backup ← input[rng]
input[rng] ← random bytes()
if original hash (cid:54)= get bitmap hash(input) then
add(ranges, (min(rng). . . max(rng)/2))
add(ranges, (max(rng)/2 + 1 . . . max(rng)))
input[rng] ← backup
D. Instruction Patching
Once the fuzzing logic has computed a list of candidate
hash comparison instructions from the input-to-state corre-
spondence data, we replace them with bogus compare instruc-
tions that always yields true. In our implementation, we use
the instruction cmp al,al since it is the smallest compare
instruction available on the x86 instruction set architecture.
The remaining bytes of the original instruction are padded
with NOPs. We use the KVM and QEMU debug facilities to apply
these patches in memory inside of the VM. The normal fuzzing
process is then continued with the patched VM. However, if
patches are active, newly found paths are not immediately
added to the queue. Sometimes even benign C code and
common compilers emit assembly that jumps in the middle
of instructions. In our case, this can be detected by the Intel
Processor Tracing (PT) runtime disassembly. In other cases,
techniques such as instruction punning [15] or even plain
breakpoints might be used to avoid introducing unexpected
crashes. However, it should be noted that even in relatively
large target programs, we did not observe any such behavior,
as the number of patched instructions is low (i.e., typically less
than 5 in our empirical evaluation).
E. Input Validation and Fixing
We use Algorithm 2 to verify and ﬁx preliminary results.
This algorithm iteratively tries to ﬁx all comparisons by
repeatedly applying individual mutations and observing the
resulting input-to-state correspondences. Note that there are
situations in which there exists an order of comparisons, which
we need to preserve while ﬁxing the input. Typically, this is
encountered if the header of a ﬁle format contains a checksum
over the complete content of the ﬁle and some chunks inside
the ﬁle are also protected by checksums. For example, the
content of the IDAT chunk of PNG ﬁles is protected with a
CRC-32 sum. If the content is zlib compressed, it is guarded
7
by another ADLER-32 checksum. In these situations, we have
to ﬁx the inner checksum ﬁrst for the outer checksum to be
calculated correctly. The order in which these checksums are
checked is not obvious. However, it is more common for the
outer checksum to be checked ﬁrst. Therefore, we try to ﬁx
the last comparison ﬁrst to avoid unnecessary work. In our
experiments, this simple approach was sufﬁcient. However, in
general we cannot assume this to be the case. While perform-
ing a sequence of trial runs to ﬁx all checksums in reverse
order of occurrence, we observe which mutation inﬂuence
which compare instructions. Thereby, we create a dependency
graph of the different patched instructions. If the input from
the ﬁrst iteration is not valid, we use this dependency graph
to perform a topological sort on the patches and to obtain a
valid order. This allows us to apply another round of ﬁxes to
the input in the required sequence. If the ﬁnal input does not
exhibit the expected behavior on the unmodiﬁed executable,
we remove the offending patches and discard the input from
the preliminary queue.
Algorithm 2: Algorithm for ﬁxing preliminary inputs
Data: input is the preliminary input with unsatisﬁed checksums
Result: Either (None, cmps) if the comparisons cmps could not be
ﬁxed or (in f, None) if the new, ﬁxed input in f
satisﬁes all comparisons
1 in f←input
2 for cmp in reverse(patched cmps) do
3
4
5
in f ← try fix value for cmp(cmp, in f)
if cmp ∈ get broken cmps(in f) then
return (None, {cmp})
dependencies[cmp] ← get cmps influcenced()
in f←input
ordered cmps ← topological sort(dependencies)
for cmp in ordered cmps do
in f ← try fix value for patch(cmp, in f)
6
7 if get broken cmps(in f) (cid:54)= ∅ then
8
9
10
11
12 if get broken cmps(in f) (cid:54)= ∅ then
13
14 return (in f, None)
return (None, get broken cmps(in f))
The routine get broken cmps used in Algorithm 2
returns a list of all patched compare instructions that are
currently not satisﬁed, as well as the values that are being
compared. The routine try fix value for patch tries
to apply all mutations resulting from the patched instruction
as described in Section III-A. Afterwards, this function checks
if any of the mutations ﬁxed the comparison. If such an input
is found, it is returned. Otherwise, we learn that we cannot
satisfy the compare instruction using our technique. The patch
is removed from the list of patches used during further fuzzing
runs. The routine get cmps influenced ﬁnds all compare
instructions whose arguments were inﬂuenced by the last ﬁx
applied to the input. This allows us to construct the ordering
of compare instructions that we use later, if required.
at the same time, the second mutation would invalidate the
sum of the ﬁrst one. Therefore, we ﬁrst try to ﬁx the second
patch (p2) and note that p1 is inﬂuenced. After ﬁxing p2,
we obtain the input: “01234567\xa3\0\0\0\0\0\0\0RQ”.
Changing the inner checksum of
the input, also changes
the expected value for p1. The new mutation for p1
is now . Then we
apply the ﬁrst patch p1. This
time we do not dis-
turb any of
is:
“\x46\x01\0\0\0\0\0\0\xa3\0\0\0\0\0\0\0RQ”. As all
patched constraints were satisﬁed by this input, we perform
one last run without patches to ensure that the input behaves
as expected. This input does indeed trigger Bug 2 and is moved
from the preliminary queue to the real queue. In this example
we did not need the topological sort operation to order the
checks, as we guessed one correct order to ﬁx the patches.
the other patches. The ﬁnal
input
F. Linux User Space Application Loader for KAFL
We extended KAFL by a Linux ring 3 loader to evaluate
against other user space fuzzers and to demonstrate that our
approach is generic and robust. This loader re-implements
the AFL fork server. Since we are targeting binaries, we
use LD PRELOAD to inject the fork server functionality into
the start-up routine of the target. Communication with the
fuzzing logic is performed with custom KAFL hypercalls
triggered by the injected start-up routine. Also, to support
ring 3 tracing in KVM-PT, we set the User bit in the model
speciﬁc register IA32 RTIT CTL MSR. Since the original
KAFL was designed to be a kernel fuzzer,
it sets only
IA32 RTIT CTL MSR.OS to enable ring 0 tracing. Addi-
tionally, the original fuzzer was only intended to fuzz 64-bit
operating systems. However, since some of the CGC binaries
can only be compiled for 32-bit
targets, we extended the
fuzzer to support 32-bit targets. Hence, we added 32-bit mode
disassembling to QEMU-PT to support decoding of 32-bit mode
Intel PT trace data.
V. EVALUATION
•
•
We evaluated our prototype implementation of REDQUEEN
as described above to answer the following research questions:
RQ 1. Are input-to-state correspondence-based tech-
niques general enough to work across a diverse set of
targets and environments?
RQ 2. How do the results of our
input-to-state
correspondence-based techniques compare to other,
more complicated techniques such as approaches
based on taint tracking or symbolic execution?
RQ 3. What
input-to-state
correspondence-based techniques provide in real-
world fuzzing scenarios?
improvements do our
•
Example 6. Consider Bug 2 of our running example shown
in Listing 1. After we removed the two checksum checks, the
fuzzer ﬁnds the preliminary input “01234567abcdefghRQ”.
the following results: p1 :=  and p2 := . If we were to apply both mutations
To answer these questions, our evaluation is divided into
three parts: First, we perform a comparative evaluation on
two synthetic test sets (LAVA-M and CGC) and a real-world
test set (GNU binutils). The experiments demonstrate that
our combination of methods outperforms all other current
approaches by a signiﬁcant margin. Second, we demonstrate
that our tool is able to ﬁnd novel bugs in various well-tested
8
software packages, running in very different environments. In
total, we found 10 bugs in 2 different Linux ﬁle system drivers
and 55 bugs in 16 user-space programs and software libraries.
So far, we obtained 16 CVEs, 4 are still pending. Finally, we
measure the efﬁciency and effectiveness of our technique as
compared to the other mutations performed by KAFL. We also
test the inﬂuence of the individual techniques introduced in this
paper, using a case study based on a small statically linked
PNG library because this ﬁle format uses nested checksums.
We demonstrate that our approach enables us to overcome
fuzzing roadblocks that previously required dictionaries and
manual removal of hash checks.
A. Evaluation Methods
All experiments were performed on systems running
Ubuntu Server 16.04.2 LTS with an Intel i7-6700 processor
(4 cores) and 24 GB of RAM. We conﬁgured all tools to
use one fuzzing process to ensure comparability with tools
such as VUZZER which are not multi-threaded. No experi-
ments contain changes made speciﬁcally for certain targets.
Unless stated otherwise, we used an uninformed, generic seed
consisting of different characters from the printable ASCII
set: "ABC. . . XYZabc. . . xyz012. . . 789!¨$. . . ˜+*". Since there
exist various deﬁnitions of a basic block and tools such as
LAF-INTEL dramatically change the number of basic blocks
in a binary, we always measure the coverage produced by
each fuzzer on the same uninstrumented binary. As such, the
numbers of basic blocks uncovered may not match the numbers
reported in other papers but are guaranteed to be consistent
within our experiments. All experiments were conducted mul-
tiple times. In each case, we report the median number of
basic blocks found at any time, as well as the 60% conﬁdence
intervals. Recently, much focus was placed on uncovering
“deep” bugs [34]. While the exact deﬁnition of “deep” remains
somewhat elusive, we use the following deﬁnition as a work in
progress: A bug is considered “deep” if it is hard to reach from
the given seed inputs. We, therefore, consider the ability to ﬁnd
new code coverage a good proxy for the ability to ﬁnd “deep”
bugs—a property that is much harder to qualify. Lastly, since
the nomenclature varies across related research, we use “crash”
to describe any input that made the target application crash. We
never talk about the number of “uniqe” crashes found, as this
metric is highly unreliable, as different tools report drastically
different and often inﬂated numbers of crashes. Instead we say
“we were able to crash” to denote that at least one crash was
found without further triage. We use the term bug to describe
manually veriﬁed and triaged bugs with disjoint root causes in
the application. A bug is not necessarily exploitable. Lastly, in
some cases, we obtained CVE numbers which we count and
list individually.
B. LAVA-M
LAVA-M [17] is a synthetic set of bugs inserted in hard-
to-reach places in real-world binaries from the GNU coreutils
suite. It is commonly used to evaluate the efﬁciency of modern
fuzzers. We ﬁrst describe the experiments performed on LAVA-