create effective fuzzing harnesses. To do so, we diffed the
initial and final harness code in our evaluation. We analyzed
the fixes required to make the harnesses work, and present the
findings in Table VIII and Table IX. We also include general
information about all 59 harnesses in appendix §X-B. As shown,
the majority of the harnesses worked with no modifications. On
Program
Vendor
Input GUI?
Size
Speed (exec/sec)
W-DR W-PT WINNIE
Coverage (# of new BBs)
W-DR W-PT WINNIE
p-value
W-DR
W-PT
Applied heuristics
L DF CS CB CF DF
T
makecab
Windows 10
HWP-jpeg Hancom 20
7z
7-Zip
EndNote
Clarivate
Gomplayer GOM Lab
HWP-tiff
Hancom 20
Tiled
T. Lindeijer
file
libmagic
UltraISO
Ultra ISO
ezPDF
Unidocs
XnView
XnSoft
mspdbcmf
VS2019
pdbcopy
VS2019
ACDSee
ACDsee
ml
VS2019
.txt
.jpg
.7z
.pdt
.mp4
.tif
.tmx
.png
.iso
.pdf
.jpm
.pdb
.pdb
.png
.asm
CLI
GUI
Both
GUI
GUI
GUI
Both
CLI
GUI
GUI
GUI
CLI
CLI
GUI
CLI
50KB
220KB
1,114KB
2,738KB
4,091KB
630KB
113KB
147KB
5,250KB
3,221KB
692KB
1,149KB
726KB
3,006KB
476KB
228.2
25.2
8.7
2.1
0.2
0.2
✗
✗
✗
✗
✗
✗
✗
✗
✗
21.3
21.0
17.0
50.4
0.6
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
49.4
25.9
49.3
89.5
25.9
26.2
8.7
52.5
45.3
18.9
23.2
8.1
28.5
63.1
44.0
762
1821
1435
8
194
1279
✗
✗
✗
✗
✗
✗
✗
✗
✗
982
1498
1530
37
1068
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
1020
1847
2117
693
1496
2301
36
116
1558
6355
16702
9637
3302
618
2399
<0.001 <0.001
0.12 <0.001
<0.001 <0.001
<0.001 <0.001
<0.001 <0.001
N/A
<0.001
N/A
N/A
N/A
N/A
N/A
N/A
N/A
N/A
N/A
N/A
N/A
N/A
N/A
N/A
N/A
N/A
N/A
N/A
✔ ✔
✔
✔ ✔
✔
✔
✔
✔ ✔
✔ ✔
✔ ✔
✔
✔
✔ ✔
✔ ✔
✔
✔ ✔
✔ ✔
✔
✔
✔ ✔
✔ ✔
✔ ✔
✔
✔
✔
✔ ✔
✔
✔ ✔ ✔ ✔ ✔
✔ ✔ ✔ ✔ ✔
T: Target identification, L: LCA, DF: Differential analysis, CS: Call sequence, CB: Callback, CF: Control-flow (exit, loop), DF: Data-flow (constant/variable, pointer)
TABLE IX: Comparison of WINNIE against WinAFL. Among 15 applications, WinAFL could only run 6, whereas WINNIE was able run
all 15. Columns marked “✗” indicate that the fuzzer could not fuzz the application. Markers “✔” indicate which heuristics were applied during
harness generation. When both WinAFL and WINNIE support a program, WINNIE generally achieved better coverage and throughput. Although
WINNIE excels at fuzzing complicated programs, WinAFL and WINNIE achieve similar results on small or simple programs. We explain in
further detail in §VIII. For all other programs, WINNIE’s improvement was statistically significant (i.e., p<0.05). P-values were calculated
using the Mann-Whitney U test on discovered basic blocks.
s
k
c
o
l
b
c
i
s
a
b
#
2.5k
2.0k
1.5k
1.0k
0.5k
0.0k
(a) 7z
0 4 8 12 16 2024
1.2k
0.8k
0.4k
0.0k
(b) makecab
0 4 8 12 16 2024
2.0k
1.5k
1.0k
0.5k
0.0k
(c) Gomplayer
(d) HWP-jpeg
2.0k
1.5k
0 4 8 12 16 2024
WinAFL-DR
1.0k
0 4 8 12 16 2024
WinAFL-IPT
Winnie
2.5k
2.0k
1.5k
1.0k
0.5k
0.0k
(e) HWP-tiff
0 4 8 12 16 2024
0.8k
0.6k
0.4k
0.2k
0.0k
(f) EndNote
0 4 8 12 16 2024
Time (hours)
Fig. 9: Comparison of basic block coverage. We conducted five trials, each 24 hours long, with three fuzzers: WINNIE, WinAFL-DR, and
WinAFL-IPT. Only programs which were supported by all fuzzers are shown here; WinAFL was unable to fuzz the rest. When a program can
be fuzzed by both WINNIE and WinAFL, their performance is comparable. Nevertheless, most programs cannot be fuzzed with WinAFL.
average, the synthesized harnesses had 82.7 LoCs, relied on 3.2
heuristics, and required only 3.4% of the code to be modified.
Based on our findings, we discuss the various strengths and
weaknesses of the harness generator below.
Strengths of the Harness Generator. The execution tracer
provides helpful information about the target program, such as
promising fuzzing targets (i.e., Table IX: Target identification).
This saves the user’s time. While creating harnesses, we kept
most the original code that WINNIE generated. Without the aid
of our system, the user would have had to manually record all
of the corresponding function calls and their arguments. The
API sequences WINNIE generates also gives useful clues to
the user. In the example harness for XnView, since WINNIE
extracted 4 calls to the same API with differing arguments,
one could conclude that the API’s purpose was to initialize
various attributes of an object. In our experiments, WINNIE
successfully inferred some relationships present in the program
(§IV-D). For example, WINNIE automatically detected that an
opened file handle is passed to the next function (lines 6 and
10 in the example Figure 3) WINNIE also informs users about
constant values, suggesting that they may be magic values that
should not be modified.
To assess the usability of WINNIE and its ability to aid
human researchers, we recruited two information security
M.S. students who were unaware of the project. They were
asked to use WINNIE to create fuzzing harnesses for Windows
applications of their choice. Within 3 days, they were able
to produce 7 functional harnesses, spending roughly only 3
hours per harness on average. The harness generator was
most effective when it could rely on a single LCA API (e.g.,
Table VIII: 7z). In these cases, the user only needed to collect
program run traces and provide them to the harness generator.
Upon receiving the trace, WINNIE automatically calculated the
LCA and generated C code to correctly invoke the function.
Weaknesses. Although most harnesses worked with few modifi-
cations, ACDSee and HWP-jpeg in particular required relatively
large modifications (e.g., 34.3% and 16.3% respectively). This
is mainly because they passed complex objects and virtual
functions to the library’s API. One challenge was reconstructing
the custom structure layouts without the original source code.
Although WINNIE dissects structures and pointer chains from
the trace to provide plausible inferences, WINNIE is not perfect.
To correct this, we analyzed the object using a decompiler and
identified eight variables and four function pointers. Second, we
manually extracted the callback functions by adding decompiled
code. We followed the function pointers from the trace, and
copied the decompiled code into the harness. There will always
be some cases that WINNIE cannot handle. We discuss a few
examples in §VIII, and we hope to support them in future
versions of WINNIE.
D. Overall Results
1) Overall Testing Results: Figure 9 shows the ability of
each fuzzer to find new coverage. Overall, WINNIE discovered
3.6× more basic blocks than WinAFL-DR and 4.1× more
basic blocks than WinAFL-IPT. We also applied statistical tests,
11
Size Bug Type(s)
Bug(s)
134-225K ND
6.1M ND
743K Arbitrary OOB read
82K Double free
475K SBOF
23K SOF
131K ND
114K Integer underflow
123K Stack OOB read
5.3M Integer overflow, SOF
Uninitialized use
709K HC, Integer overflow
85K Heap BOF
242K ND, Integer overflow
645K HR, TC, FC, HC
147K ND, SBOF
760K Heap UAF, HC
83K Integer overflow, ND
12.8M Div by zero
157K Integer underflow
18K SOF, SBOF, ND
23.9M Race condition, ND
3.2M SBOF, SOF, ND
136K Integer underflow
273K ND, Denial of service
874K SBOF
2.4M ND
419K ND, Div by zero
2.8M Heap OOB write
1.0M Div by zero
558K ND
19
2
1
1
1
1
1
1
1
2
1
2
2
3
3
6
2
3
2
1
1
3
3
3
1
2
1
2
3
1
1
1
61
Product
Source Engine
MS WinDBG
MS Windows
Visual Studio
Alzip
Ultra ISO
Buggy File
engine.dll
pdbcopy.exe
makecab.exe
ml.exe
undname.exe
Egg.dll
Tar.dll
Alz.dll