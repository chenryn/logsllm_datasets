Therefore, to minimize image quality degradation on normal
inputs with prevention mechanisms and most
importantly
provide deterrence on the attack, we focused on detecting
attack images regarding the image-scaling attack, including
one novel angle e.g., treating the image-scaling attack as a
kind of steganography for information hiding. We aim to
develop a defense mechanism to detect attack images without
any modiﬁcations to input images for CNN models. Also, we
develop Decamouﬂage as an independent module compatible
with any existing scaling algorithms—alike a plug-in protector.
Furthermore, Decamouﬂage is designed for detecting attack
images crafted via image-scaling attacks even under black-box
settings where there is no prior information about the attack
algorithm.
Our key contributions are summarized as follows:
• Decamouﬂage is the ﬁrst practical solution to detect
image-scaling attacks. We develop three different detec-
tion methods (scaling, ﬁltering, and steganalysis). Our
source code is released at https://github.com/kimbedeuro/
Decamouﬂage4.
• We identify three fundamental metrics (mean squared
errors (MSE), structural similarity index (SSIM), and
centered spectrum points (CSP)) that can be used to
distinguish benign images from attack images generated
by image-scaling attacks.
• We empirically validate the feasibility of Decamouﬂage
for both the white-box setting (with the knowledge of the
attacker’s algorithm) and the black-box setting (without
the knowledge of the attacker’s algorithm). We demon-
strate that Decamouﬂage can be effective in both settings
with experimental results.
• We evaluate the detection performance of Decamouﬂage
using an unseen testing dataset to show its practicality.
We used the “NeurIPS 2017 Adversarial Attacks and
Defences Competition Track” image dataset [14] to ﬁnd
the optimal thresholds for Decamouﬂage and used the
“Caltech 256” image dataset [15] for testing. To imple-
ment image-scaling attacks, we use the code released in
the original work by Xiao et al. [12]. The experimental
results demonstrate that Decamouﬂage achieves detection
4The artifacts including source code will be released upon the publication.
64
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 08:54:52 UTC from IEEE Xplore.  Restrictions apply. 
accuracy of 99.9% with a false acceptance rate of 0.0%
and a false rejection rate of 0.1% in the white-box
setting, and detection accuracy of 98.5% with a false
acceptance rate of 0.3% and a false rejection rate of
2.7% in the black-box setting. In addition, the running
time overhead of Decamouﬂage is less than or equal to
137 milliseconds on average evaluated with a personal
PC with an Intel Core i5-7500 CPU (3.41GHz) and 8GB
memory, indicating that Decamouﬂage can be deployed
for online detection.
• We show the robustness of Decamouﬂage against image-
scaling attacks with varying image sizes and the visual
constraint parameter. The scaling detection method using
MSE perfectly detects all attacks in both white-box and
black-box settings.
II. BACKGROUND
In this section, we provide the prior knowledge for the image-
scaling attack and its enabled insidious backdoor attack.
A. Image-Scaling Attack
The pre-processing steps for input images in a typical deep
learning pipeline is an essential stage. Recently, Xiao et
al. [12] demonstrated a practical adversarial attack targeting
the scaling functions used by widely used deep learning
frameworks. The attack exploited the fact that deep learning-
based models accept only ﬁxed-size input images.
Fig. 2: Overall process of an image-scaling attack. An adversary creates an
attack image A (tampered sheep image) such that it looks like O (original
sheep image) to humans, but it is recognized as T (targeted wolf image) by
CNN models after applying image-scaling operations. Here X ≈ Y represents
that X looks similar to Y .
One detailed example is illustrated in Figure 2, where a wolf
is disguised into a sheep image. The human sees sheep, but the
model sees a wolf once the tampered sheep image A undergoes
the downsampling step. More precisely, the adversary slightly
alters an original image O so that the obtained attack image
A = O + ∆ resembles a target image T once downscaled.
The attack mechanism can be demonstrated as the following
quadratic optimization problem:
min(||∆||2) s.t. ||scale(O + ∆) − T||∞ ≤ 
(1)
where  is the constraint parameter (within [0,1]) representing
the maximum visual similarity between the original image O
and the obtained attack image A.
Each pixel value of A needs to be maintained within the
ﬁxed range (e.g., [0,255] for 8-bit
images). This problem
can be solved with Quadratic Programming (QP) [8]. The
successful attack criteria are that the obtained image A should
be visually similar to the original image O, but the downscaled
output D should be recognized as the target image T after
scaling. In other words, the attack has to satisfy two properties:
• The resultant attack image A should be visually indistin-
guishable from the original image O (A ≈ O).
• The output image D downscaled from the attack image
A should be recognized as the target image T by CNN
models (T ≈ D).
III. POTENTIAL DETECTION METHODS: KEY INSIGHTS
To proactively defeat the image-scaling attack, one would ﬁrst
identify potential methods from different angles. Therefore, the
ﬁrst research question (RQ) is as below.
RQ. 1: What are the potential methods to reveal the target
image embedded by the image-scaling attack?
This work identiﬁes three efﬁcient methods and visualizes
their ability to detect that attack. Here we provide a general
concept for each method. We exchangeably use the terms
original image and benign image in the rest of this paper.
A. Method 1: Scaling Detection
We ﬁrst explore the potential of reverse-engineering the at-
tack process. In the attack process, the attack image A is
downsampled to the output image D to be recognized as T
for CNN models. Therefore, we seek to upscale the output
image D to the upscaled image S in the reverse engineering
process. Based on the reverse engineering process, we design a
detection method as follows. Given an input image I (which
can potentially be an attack image) for a CNN model, we
apply the downscaling and upscaling operations in sequence
to obtain the image S and measure the similarity between I
and S. Our intuition is that if the input image I is a benign
image (i.e., the original image O), S will remain similar to
I; otherwise, S would be signiﬁcantly different from I (see
Figure 3).
(a) Benign case.
(b) Attack case.
Fig. 3: Overview of the scaling detection method. We obtained the upscaled
image S from the downscaled image D and then measured the image
similarity between S and the input image I. If the input image I is a benign
image (i.e., original image O), S will remain similar to I; otherwise, S would
be signiﬁcantly different from I.
65
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 08:54:52 UTC from IEEE Xplore.  Restrictions apply. 
Xiao et al. [12] suggested the color histogram as an image
similarity metric for detecting attack images without conduct-
ing experiments. However, we found that the color histogram
is not a valid metric. Our observation is consistent with the
results in [16]. Therefore, it is non-trivial to ﬁnd a proper
metric to distinguish the case of attack images from benign
images. We will discuss this issue in Section IV.
B. Method 2: Filtering Detection
The image-scaling attack relies on embedding the target image
pixels within the original image pixels to avoid human visual
inspection by abusing image-scaling functions. Therefore, if
we use image ﬁlters to remove noises, the embedded target
image pixels can be removed or disrupted because the em-
bedded target image pixels would be signiﬁcantly different
from the original image pixels. Figure 4 shows the results of
an attack image after applying the minimum ﬁlter [17], the
median ﬁlter, and the maximum ﬁlter, respectively. We can
see that the minimum ﬁlter can potentially expose the target
image. Thus, we use the minimum ﬁlter for this method.
Fig. 4: Image ﬁlter results on an attack image.
Based on this observation, we suggest another detection
method. Given an input image I for a CNN model, we apply an
image ﬁlter to obtain the image F and measure the similarity
between I and F . Our intuition is that if the input image I is a
benign image, F will remain similar to I; otherwise, F would
be signiﬁcantly different from I. For this purpose, we take the
minimum ﬁlter for illustration and elaborated descriptions.
i=1 × yb
The minimum ﬁlter is used with ﬁxed window size. Figure 4
illustrates how the minimum ﬁlter works on an image. The
image ﬁltering process is done by dividing the image M × N
into smaller 2D blocks xb
j=1 where b is the number of
blocks and x, y are the ﬁlter size. For example, as shown in
Figure 5, when we use the minimum ﬁlter, only the smallest
pixel value among a neighborhood of the block xi × yj
is selected. In this paper, we used Pillow [18], which is a
popularly used imaging library. We empirically chose the 3×3
size for the minimum ﬁlter because 3 × 3 is the minimum
size that Pillow can support and also preserves an acceptable
detection rate.
We will discuss how to measure the image similarity be-
tween I and F and determine whether a given image is an
attack image in Section IV.
Fig. 5: Process of applying the minimum ﬁlter.
C. Method 3: Steganalysis Detection
The image-scaling attack’s key idea is to embed the target
image as cluttered pixels so that they are less recognized by
human eye perceptuality. Consequently, we treat the perturbed
pixels as information that the attacker tries to hide in this
method, which is similar to steganography [19]. Steganogra-
phy is a technique of hiding information in digital media such
as images to avoid secret data detection by unintended recip-
ients. Therefore, we may constructively employ steganalysis
mechanisms to expose the hidden perturbed pixels embedded
by the image-scaling attack based on the similarity between
the image-scaling attack and steganography.
We explore the frequency domain based steganalysis mech-
anism to ﬁnd out
the perturbed pixels within the attack
image. Fourier Transform (FT) is an operation that transforms
data from the time (or spatial) domain into the frequency
domain [20]. Because an image consists of discrete pixels
rather than continuous patterns, we use the Discrete Fourier
Transformation (DFT) [21]. We ﬁrst transform the input (po-
tential attack) image A into the 2-dimensional space, namely
spectrum image. For a square image of size N × N, the 2-
dimensional DFT is given by:
F (k, l) =
f (i, j))e−i2π( ki
N + li
N )
(2)
N−1(cid:88)
N−1(cid:88)
i=0
j=0
where f (i, j) is the spatial domain images, and the expo-
nential term is the corresponding basis function to each F (k, l)
point in the DFT space. The basis functions are sine and cosine
waves with increasing frequencies as depicted below:
(cid:19)
(cid:18)
(cid:19)(cid:21)
(cid:20)
(cid:18)
cos
2π(
ki
N
+
li
N
)
− i · sin
2π(
ki
N
+
li
N
)
(3)
The resultant DFT spectrum contains the low and high-
frequency coefﬁcients. The low frequencies capture the im-
age’s core features, whereas the high frequency reﬂects the
less signiﬁcant regions within an image. Direct visualization of
both frequencies shows that a broad dark region in the middle
represents the high frequency, while low frequency appears as
a whiter clattered area on the edges. This visualization can
not provide us with an automated quantiﬁcation to distinguish
attack images from benign images. Therefore, we apply log-
arithmic with a shift to ﬂip the whiter frequency to centralize
the low frequencies called centered spectrum as given by:
66
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 08:54:52 UTC from IEEE Xplore.  Restrictions apply. 
N−1(cid:88)
N−1(cid:88)
k=0
l=0
F (x, y) =
log |Θ · F (k, l)|
(4)
where Θ is the predetermined shift for F (k, l) low-frequency
point. We empirically use the 20 × F (x, y) which is typically
used to generate the centered spectrum.
If we apply the DFT operation on a benign image, a benign
image has one centered spectrum point. However, as shown
in Figure 6, attack images overall exhibit multiple centered
spectra as opposite to one centered spectrum point observed
in benign images because the cohesion of the original image
pixels is broken due to the arbitrary perturbation to embed the
target image pixels.
Based on this observation, we suggest the frequency domain
based steganalysis detection method. Given an input image I
(which can be an attack image) for a CNN model, we convert
it into a fourier spectrum to obtain the image B and count the
centered spectrum points in B. We will discuss how to count
the number of the centered spectrum points and determine
whether a given image is an attack image in Section IV.
Fig. 6: Centered spectrum points on a benign image and an attack image.
Summary: As an answer to RQ. 1, we suggest that three de-
tection methods (scaling, ﬁltering, and steganalysis) can poten-
tially expose attack images generated by image-scaling attacks.
Each method is designed based on a different insight/angle
to detect
image-scaling attacks. The scaling detection and
ﬁltering detection methods are designed to detect the image-
scaling attacks in the spatial domain, while the steganalysis
method is designed to detect the image-scaling attacks in the
frequency domain.
IV. DECAMOUFLAGE SYSTEM DESIGN
In this section, we provide the Decamouﬂage framework
exploiting the above-identiﬁed detection methods to answer
the RQ. 2:
RQ. 2: How can we develop an automated process to detect
image-scaling attacks using the identiﬁed methods?
We ﬁrst deﬁne the threat models that we focused on in this
paper. Next, we introduce three key metrics to ﬁnd image-
scaling attacks in an automated manner. We ﬁnally provide
an overview of the Decamouﬂage detection system that can
efﬁciently distinguish attack images from benign images with
the methods identiﬁed in Section III.
A. Threat Model
In this paper, the attacker’s goal is to manipulate images by
mixing a scaled target image into an original image so that
any learning-based system scaling images can be tricked into
working on attacker-controlled data. For a defense mechanism,
we consider both white-box and black-box settings presented
in [12].
In the white-box setting, we assume that the defender (i.e.,
service provider) knows the attacker’s algorithm; thus, the
parameters for Decamouﬂage are determined to target for the
attacker’s speciﬁc algorithm. In the black-box setting, we as-
sume that the defender does not know the attacker’s algorithm.
We believe that attackers’ tools and software could be obtained
when commercialized and distributed among attackers. The
white-box setting can reﬂect such situations and be also used
to show the feasibility of our proposed method. However, the
black-box setting seems more practical because it would be
difﬁcult to obtain information about the attacker’s algorithm,
and we should also consider many different conditions for the
image-scaling attack.
Decamouﬂage can be performed ofﬂine and online. Ofﬂine
is suitable for defeating backdoor attack assisted with image-
scaling attack (presented in Section VI). Herein, the defender
is the data aggregator/user who has access to attack images.
In this case, we reasonably assume that the user owns a small
set, e.g., 1000 of hold-out samples produced in-house. The
defender must remove attack images crafted by image-scaling
attacks to avoid backdoor insertion in the trained model. On