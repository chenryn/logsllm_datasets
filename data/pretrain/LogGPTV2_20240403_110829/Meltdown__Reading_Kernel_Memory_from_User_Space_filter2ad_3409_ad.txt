6 Evaluation
In this section, we evaluate Meltdown and the perfor-
mance of our proof-of-concept implementation.11 Sec-
tion 6.1 discusses the information which Meltdown can
11https://github.com/IAIK/meltdown
982    27th USENIX Security Symposium
USENIX Association
Table 1: Experimental setups.
Environment CPU Model
Lab
Lab
Lab
Lab
Lab
Lab
Lab
Lab
Lab
Cloud
Cloud
Phone
Celeron G540
Core i5-3230M
Core i5-3320M
Core i7-4790
Core i5-6200U
Core i7-6600U
Core i7-6700K
Core i7-8700K
Xeon E5-1630 v3
Xeon E5-2676 v3
Xeon E5-2650 v4
Exynos 8890
Cores
2
2
2
4
2
2
4
12
8
12
12
8
leak, and Section 6.2 evaluates the performance of Melt-
down, including countermeasures. Finally, we discuss
limitations for AMD and ARM in Section 6.3.
Table 1 shows a list of conﬁgurations on which we
successfully reproduced Meltdown. For the evaluation of
Meltdown, we used both laptops as well as desktop PCs
with Intel Core CPUs and an ARM-based mobile phone.
For the cloud setup, we tested Meltdown in virtual ma-
chines running on Intel Xeon CPUs hosted in the Ama-
zon Elastic Compute Cloud as well as on DigitalOcean.
Note that for ethical reasons we did not use Meltdown on
addresses referring to physical memory of other tenants.
6.1 Leakage and Environments
We evaluated Meltdown on both Linux (cf. Sec-
tion 6.1.1), Windows 10 (cf. Section 6.1.3) and Android
(cf. Section 6.1.4), without the patches introducing the
KAISER mechanism. On these operating systems, Melt-
down can successfully leak kernel memory. We also
evaluated the effect of the KAISER patches on Meltdown
on Linux, to show that KAISER prevents the leakage of
kernel memory (cf. Section 6.1.2). Furthermore, we dis-
cuss the information leakage when running inside con-
tainers such as Docker (cf. Section 6.1.5). Finally, we
evaluate Meltdown on uncached and uncacheable mem-
ory (cf. Section 6.1.6).
6.1.1 Linux
We successfully evaluated Meltdown on multiple ver-
sions of the Linux kernel, from 2.6.32 to 4.13.0, with-
out the patches introducing the KAISER mechanism. On
all these versions of the Linux kernel, the kernel address
space is also mapped into the user address space. Thus,
all kernel addresses are also mapped into the address
space of user space applications, but any access is pre-
vented due to the permission settings for these addresses.
As Meltdown bypasses these permission settings, an at-
tacker can leak the complete kernel memory if the vir-
tual address of the kernel base is known. Since all major
operating systems also map the entire physical memory
into the kernel address space (cf. Section 2.2), all physi-
cal memory can also be read.
Before kernel 4.12, kernel address space layout ran-
domization (KASLR) was not active by default [57]. If
KASLR is active, Meltdown can still be used to ﬁnd the
kernel by searching through the address space (cf. Sec-
tion 5.2). An attacker can also simply de-randomize the
direct-physical map by iterating through the virtual ad-
dress space. Without KASLR, the direct-physical map
starts at address 0xffff 8800 0000 0000 and linearly
maps the entire physical memory. On such systems, an
attacker can use Meltdown to dump the entire physical
memory, simply by reading from virtual addresses start-
ing at 0xffff 8800 0000 0000.
On newer systems, where KASLR is active by default,
the randomization of the direct-physical map is limited
to 40 bit. It is even further limited due to the linearity of
the mapping. Assuming that the target system has at least
8 GB of physical memory, the attacker can test addresses
in steps of 8 GB, resulting in a maximum of 128 memory
locations to test. Starting from one discovered location,
the attacker can again dump the entire physical memory.
Hence, for the evaluation, we can assume that the ran-
domization is either disabled, or the offset was already
retrieved in a pre-computation step.
6.1.2 Linux with KAISER Patch
The KAISER patch by Gruss et al. [20] implements
a stronger isolation between kernel and user space.
KAISER does not map any kernel memory in the user
space, except for some parts required by the x86 archi-
tecture (e.g., interrupt handlers). Thus, there is no valid
mapping to either kernel memory or physical memory
(via the direct-physical map) in the user space, and such
addresses can therefore not be resolved. Consequently,
Meltdown cannot leak any kernel or physical memory
except for the few memory locations which have to be
mapped in user space.
We veriﬁed that KAISER indeed prevents Meltdown,
and there is no leakage of any kernel or physical memory.
Furthermore, if KASLR is active, and the few re-
maining memory locations are randomized, ﬁnding these
memory locations is not trivial due to their small size of
several kilobytes. Section 7.2 discusses the security im-
plications of these mapped memory locations.
USENIX Association
27th USENIX Security Symposium    983
6.1.3 Microsoft Windows
We successfully evaluated Meltdown on a recent Mi-
crosoft Windows 10 operating system, last updated just
before patches against Meltdown were rolled out. In line
with the results on Linux (cf. Section 6.1.1), Meltdown
also can leak arbitrary kernel memory on Windows. This
is not surprising, since Meltdown does not exploit any
software issues, but is caused by a hardware issue.
In contrast to Linux, Windows does not have the con-
cept of an identity mapping, which linearly maps the
physical memory into the virtual address space. Instead,
a large fraction of the physical memory is mapped in
the paged pools, non-paged pools, and the system cache.
Furthermore, Windows maps the kernel into the address
space of every application too. Thus, Meltdown can read
kernel memory which is mapped in the kernel address
space, i.e., any part of the kernel which is not swapped
out, and any page mapped in the paged and non-paged
pool, and the system cache.
Note that there are physical pages which are mapped
in one process but not in the (kernel) address space of
another process, i.e., physical pages which cannot be at-
tacked using Meltdown. However, most of the physical
memory will still be accessible through Meltdown.
We were successfully able to read the binary of the
Windows kernel using Meltdown. To verify that the
leaked data is actual kernel memory, we ﬁrst used the
Windows kernel debugger to obtain kernel addresses
containing actual data. After leaking the data, we again
used the Windows kernel debugger to compare the leaked
data with the actual memory content, conﬁrming that
Meltdown can successfully leak kernel memory.
6.1.4 Android
We successfully evaluated Meltdown on a Samsung
Galaxy S7 mohile phone running LineageOS Android
14.1 with a Linux kernel 3.18.14. The device is equipped
with a Samsung Exynos 8 Octa 8890 SoC consisting
of a ARM Cortex-A53 CPU with 4 cores as well as an
Exynos M1 ”Mongoose” CPU with 4 cores [6]. While
we were not able to mount the attack on the Cortex-
A53 CPU, we successfully mounted Meltdown on Sam-
sung’s custom cores. Using exception suppression de-
scribed in Section 4.1, we successfully leaked a pre-
deﬁned string using the direct-physical map located at
the virtual address 0xffff ffbf c000 0000.
6.1.5 Containers
We evaluated Meltdown in containers sharing a kernel,
including Docker, LXC, and OpenVZ and found that the
attack can be mounted without any restrictions. Running
Meltdown inside a container allows to leak information
not only from the underlying kernel but also from all
other containers running on the same physical host.
The commonality of most container solutions is that
every container uses the same kernel, i.e., the kernel is
shared among all containers. Thus, every container has
a valid mapping of the entire physical memory through
the direct-physical map of the shared kernel. Further-
more, Meltdown cannot be blocked in containers, as it
uses only memory accesses. Especially with Intel TSX,
only unprivileged instructions are executed without even
trapping into the kernel.
Thus, the isolation of containers sharing a kernel can
be entirely broken using Meltdown. This is especially
critical for cheaper hosting providers where users are not
separated through fully virtualized machines, but only
through containers. We veriﬁed that our attack works in
such a setup, by successfully leaking memory contents
from a container of a different user under our control.
6.1.6 Uncached and Uncacheable Memory
In this section, we evaluate whether it is a requirement
for data to be leaked by Meltdown to reside in the L1 data
cache [33]. Therefore, we constructed a setup with two
processes pinned to different physical cores. By ﬂush-
ing the value, using the clflush instruction, and only
reloading it on the other core, we create a situation where
the target data is not in the L1 data cache of the attacker
core. As described in Section 6.2, we can still leak the
data at a lower reading rate. This clearly shows that data
presence in the attacker’s L1 data cache is not a require-
ment for Meltdown. Furthermore, this observation has
also been conﬁrmed by other researchers [7, 35, 5].
The reason why Meltdown can leak uncached mem-
ory may be that Meltdown implicitly caches the data.
We devise a second experiment, where we mark pages
as uncacheable and try to leak data from them. This
has the consequence that every read or write operation to
one of those pages will directly go to the main memory,
thus, bypassing the cache. In practice, only a negligible
amount of system memory is marked uncacheable. We
observed that if the attacker is able to trigger a legitimate
load of the target address, e.g., by issuing a system call
(regular or in speculative execution [40]), on the same
CPU core as the Meltdown attack, the attacker can leak
the content of the uncacheable pages. We suspect that
Meltdown reads the value from the line ﬁll buffers. As
the ﬁll buffers are shared between threads running on the
same core, the read to the same address within the Melt-
down attack could be served from one of the ﬁll buffers
allowing the attack to succeed. However, we leave fur-
ther investigations on this matter open for future work.
A similar observation on uncacheable memory was
also made with Spectre attacks on the System Manage-
984    27th USENIX Security Symposium
USENIX Association
ment Mode [10]. While the attack works on memory
set uncacheable over Memory-Type Range Registers, it
does not work on memory-mapped I/O regions, which
is the expected behavior as accesses to memory-mapped
I/O can always have architectural effects.
6.2 Meltdown Performance
To evaluate the performance of Meltdown, we leaked
known values from kernel memory. This allows us to
not only determine how fast an attacker can leak mem-
ory, but also the error rate, i.e., how many byte errors to
expect. The race condition in Meltdown (cf. Section 5.2)
has a signiﬁcant inﬂuence on the performance of the at-
tack, however, the race condition can always be won. If
the targeted data resides close to the core, e.g., in the
L1 data cache, the race condition is won with a high
probability. In this scenario, we achieved average read-
ing rates of up to 582 KB/s (µ = 552.4,σ = 10.2) with
an error rate as low as 0.003 % (µ = 0.009,σ = 0.014)
using exception suppression on the Core i7-8700K over
10 runs over 10 seconds. With the Core i7-6700K we
achieved 569 KB/s (µ = 515.5,σ = 5.99) with an min-
imum error rate of 0.002 % (µ = 0.003,σ = 0.001) and
491 KB/s (µ = 466.3,σ = 16.75) with a minimum error
rate of 10.7 % (µ = 11.59,σ = 0.62) on the Xeon E5-
1630. However, with a slower version with an average
reading speed of 137 KB/s, we were able to reduce the
error rate to 0. Furthermore, on the Intel Core i7-6700K
if the data resides in the L3 data cache but not in L1,
the race condition can still be won often, but the average
reading rate decreases to 12.4 KB/s with an error rate as
low as 0.02 % using exception suppression. However, if
the data is uncached, winning the race condition is more
difﬁcult and, thus, we have observed reading rates of less
than 10 B/s on most systems. Nevertheless, there are
two optimizations to improve the reading rate: First, by
simultaneously letting other threads prefetch the memory
locations [21] of and around the target value and access
the target memory location (with exception suppression
or handling). This increases the probability that the spy-
ing thread sees the secret data value in the right moment
during the data race. Second, by triggering the hardware
prefetcher through speculative accesses to memory loca-
tions of and around the target value. With these two opti-
mizations, we can improve the reading rate for uncached
data to 3.2 KB/s.
For all tests, we used Flush+Reload as a covert chan-
nel to leak the memory as described in Section 5, and In-
tel TSX to suppress the exception. An extensive evalua-
tion of exception suppression using conditional branches
was done by Kocher et al. [40] and is thus omitted in this
paper for the sake of brevity.
6.3 Limitations on ARM and AMD
We also tried to reproduce the Meltdown bug on several
ARM and AMD CPUs. While we were able to suc-
cessfully leak kernel memory with the attack described
in Section 5 on different Intel CPUs and a Samsung
Exynos M1 processor, we did not manage to mount Melt-
down on other ARM cores nor on AMD. In the case of
ARM, the only affected processor is the Cortex-A75 [17]
which has not been available and, thus, was not among
our devices under test. However, appropriate kernel
patches have already been provided [2]. Furthermore, an
altered attack of Meltdown targeting system registers in-
stead of inaccessible memory locations is applicable on
several ARM processors [17]. Meanwhile, AMD pub-
licly stated that none of their CPUs are not affected by
Meltdown due to architectural differences [1].
The major part of a microarchitecture is usually not
publicly documented. Thus, it is virtually impossible
to know the differences in the implementations that al-
low or prevent Meltdown without proprietary knowledge
and, thus, the intellectual property of the individual CPU
manufacturers. The key point is that on a microarchitec-
tural level the load to the unprivileged address and the
subsequent instructions are executed while the fault is
only handled when the faulting instruction is retired. It
can be assumed that the execution units for the load and
the TLB are designed differently on ARM, AMD and
Intel and, thus, the privileges for the load are checked
differently and occurring faults are handled differently,
e.g., issuing a load only after the permission bit in the
page table entry has been checked. However, from a
performance perspective, issuing the load in parallel or
only checking permissions while retiring an instruction
is a reasonable decision. As trying to load kernel ad-
dresses from user space is not what programs usually do
and by guaranteeing that the state does not become ar-
chitecturally visible, not squashing the load is legitimate.
However, as the state becomes visible on the microarchi-
tectural level, such implementations are vulnerable.
However, for both ARM and AMD, the toy example
as described in Section 3 works reliably, indicating that
out-of-order execution generally occurs and instructions
past illegal memory accesses are also performed.