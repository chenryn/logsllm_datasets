control with an almost continuous CPU limit control rather
than the stepwise decrease/increase of virtual cores.
V. EVALUATION
We extensively evaluate ATM not only on a large number
of data center production traces but also experimentally on
a cluster running MediaWiki. We focus on presenting the
effectiveness of ATM in ticket reduction to improve system
dependability and to reduce the high cost associated with
ticket resolution. In the remaining of this section, we assume
that usage tickets related to CPU and RAM are automatically
issued when VM utilization is greater than 60%.
A. Production Systems
We focus on a subset of boxes from the data center
trace (400 boxes) which have no gaps in their traces. The
remaining box traces suffer, throughout the 7 days of the trace,
from occasional gaps with no data. We show how different
conﬁgurations of ATM can proactively reduce the number
of tickets. We engage training of the signature series for 5
days and then apply ATM and VM resizing for the following
day. We stress that this analysis is post-hoc, i.e., we can not
change the size of the actual VMs in the trace, we focus only
on the prediction accuracy and ticket reduction via ATM. On
the contrary, in the experimental evaluation on the MediaWiki
cluster presented in the Section V-B, we do also illustrate VM
resizing in a working system.
For the spatial models, we consider DTW and CBC clus-
tering techniques and set the discretization factor ε = 5.
The temporal models used for the signature series are neural
networks [7]. ATM performs the prediction of 16000 usage
series, each of which has 96 ticketing windows, with each
window being 15 minutes long. After obtaining the predicted
series, ATM triggers the resizing algorithm for every box to
determine the near optimal CPU and RAM capacity for all
co-located VMs. We note that results presented in this section
differ from Section III and IV, where only the proposed spatial
models and resizing algorithms are evaluated individually,
excluding the temporal prediction models. Here, we have the
full effect of both prediction models.
1) Prediction Errors: Figure 9 presents the CDF of the
prediction accuracy of ATM in terms of APE with different
spatial models, i.e., DTW and CBC clustering. For CPU and
RAM usage, we use the inter-resource model, i.e., signature
series are a mix of CPU and RAM. The average prediction
errors of resources usage per box are 31% and 23%, for DTW
and CBC, respectively. These are only slightly higher than the
errors without the temporal models presented in Section III.
The ﬁgure also illustrates the CDF of the mean absolute errors
F
D
C
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
ATM w/ DTW - All
ATM w/ DTW - Peak
ATM w/ CBC - All
ATM w/ CBC - Peak
0 10 20 30 40 50 60 70 80 90 100
Mean Abs. Percentage Error (%)
Fig. 9: CDF of prediction accuracy of ATM on 400 production
servers: AT MCBC, ACMDT W .
for peak demands, i.e., usage higher than 60%. The average
peak errors across all boxes are 20% and 17% for DTW
and CBC, respectively. This shows that neural networks can
capture well the temporal dynamics of the signature series.
We further note that this high accuracy of temporal models
is achieved at a high computational time and long historical
data, i.e., 5 days, whereas the prediction of dependent series
via spatial models has a negligible cost. We also note that
the reduction in demand series for this subset of 400 boxes is
similar to results shown in Section III across 6K boxes.
2) Ticket Reduction: Figure 10 compares the results of
average ticket reduction using two different versions of ATM
against the max-min fairness, and stingy policies, see Sec-
tion IV. Each bars illustrate the mean and standard deviation
of ticket reduction across boxes divided into CPU and RAM
tickets. The key observations are the following. Both versions
of ATM are able to achieve a higher ticket reduction, around
60% and 70% for CPU and RAM, respectively, compared
to the other two heuristics. We like to point out that the
standard deviation is high for all four strategies indicating huge
difference across boxes. Different from the resizing results
shown in Section IV, max-min fairness shows worse reduction
results than stingy. This can be explained by the observed
high variability across the chosen 400 boxes which shows
that max-min fairness could even result in a increase of the
number of tickets for a subset of the boxes, see the range of
standard deviation. Max-min fairness favors small VMs while
dissatisfying big VMs, which results in more ticket violations
than the other policies. Another fact worth mentioning is that
both versions of ATM are able to achieve higher RAM ticket
reductions, due higher RAM provisioning compared to CPU.
B. ATM on a MediaWiki Cluster
We experimentally evaluate our ticket reduction techniques
also on a cluster running MediaWiki, a latency-sensitive 3-tier
web application composed by Apache (v2.4.7) as the applica-
tion server frontend, memcached (v1,4.14) as in-memory key-
value store, and MySQL (v5.5.40) as the database backend.
The testbed is composed of four identical physical servers.
343
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:17:56 UTC from IEEE Xplore.  Restrictions apply. 
)
%
(
s
t
e
k
c
T
n
i
i
n
o
i
t
c
u
d
e
R
100
80
60
40
20
0
-20
-40
-60
-80
-100
ATM w/ DTW
ATM w/ CBC
Stingy Algorithm
Max-min Fainess Algorithm
CPU
RAM
)
%
(
T
C
P
D
E
S
U
U
P
C
)
%
(
T
C
P
D
E
S
U
U
P
C
Fig. 10: Comparing ticket reduction: ATM, max-min fairness,
and stingy resizing algorithms.
)
%
(
T
C
P
D
E
S
U
U
P
C
)
%
(
T
C
P
D
E
S
U
U
P
C
)
%
(
T
C
P
D
E
S
U
U
P
C
)
%
(
T
C
P
D
E
S
U
U
P
C
Fig. 11: MediaWiki testbed.
Each server runs Ubuntu server 14.04 LTS and is equipped
with 16 GiB of DDR3 RAM with up to 41.6 GiB/s bandwidth,
a 4-core Intel Core i7 3820 processor @ 3.6 GHz with SMT,
one 2-TB Sata III 7200 rpm hard disk, and one Gigabit
Ethernet adapter. Three servers host the VMs using QEMU-
KVM (QEMU v2.0 with KVM on Linux kernel 3.13) as
hypervisor. Each VM comprises two virtual CPUs and 4 GiB
of RAM. The forth server is used as the experiment orches-
trator and load generator. Each application tier is deployed
into a separate VM. We consider a scenario of hosting two
MediaWiki applications on these 4 physical servers, termed
as wiki-one and wiki-two, see Figure 11. For wiki-one, there
are 4 Apache servers, 2 Memcached, and 1 DB, whereas there
are only 2 Apache, 1 Memcached, and 1 DB in wiki-two.
For each wiki, we have one load balancer that distributes the
requests across the different apache front-ends. The workload
generator creates requests alternating between low and high
intensity periods, each lasting one hour.
Figure 12 illustrates the CPU usage series across all VMs
located on nodes 2, 3 and 4 against the ticketing threshold
set to 60%. The ﬁgure shows the CPU usage levels without
and with ATM resizing. One can observe that indeed resizing
is very effective in achieving CPU usage levels across time
and all VMs below the 60% threshold. The consequent ticket
reduction is dramatic: tickets drop from 49 to only 1.
Besides ticket reduction, we also show performance values
for the two wiki applications, see Figure 13. The ﬁgure plots
the the average user latencies (response times) and average
100
80
60
40
20
0
100
80
60
40
20
0
100
80
60
40
20
0
100
80
60
40
20
0
100
80
60
40
20
0
100
80
60
40
20
0
0
1
0
1
0
1
0
1
0
1
0
1
Node2
)
%
(
T
C
P
D
E
S
U
U
P
C
)
%
(
T
C
P
D
E
S
U
U
P
C
4
5
4
5
2
3
Time (hour)
2
3
Time (hour)
Node3
)
%
(
T
C
P
D
E
S
U
U
P
C
)
%
(
T
C
P
D
E
S
U
U
P
C
4
5
4
5
2
3
Time (hour)
2
3
Time (hour)
Node4
)
%
(
T
C
P
D
E
S
U
U
P
C
4
5
4
5
2
3
Time (hour)
2
3
Time (hour)