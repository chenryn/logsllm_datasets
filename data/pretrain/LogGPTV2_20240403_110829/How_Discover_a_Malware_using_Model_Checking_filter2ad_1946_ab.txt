of possible criteria for such comparison. Our tool provides some
features that no other tools presently offer. This can be seen by
formulating seven essential criteria:
• C1: does the tool support the localization of the payload?
• C2: does the tool capture malicious behaviours even when
they are divided in small actions apparently not harmful?
• C3: does the tool performs behavioural analysis?
• C4: does the tool is resilient to obfuscation?
• C5: does the tool completely automated?
As shown in Table 1, only FormalDroid satisﬁes criteria from C1
to C4. To the best of our knowledge, we introduce the ﬁrst method
supporting the payload localization. Our method is not completely
automated because it needs the analyst involvement during the prop-
erties speciﬁcation. We perform a manual inspection of a few sam-
ples to identify the malicious behaviours and then we specify them
through logic formulae. Thus, our tool performs behavioral analy-
sis catching malicious behaviours splitted in several small actions.
FormalDroid is resilient to the common code obfuscations. Also
DroidLegacy and DroidClone works assert to be resilient to code
obfuscations but they do not provide any example of this. Con-
versely, Apposcopy is resilient to code obfuscations and provides
an analysis testing the tool with obfuscated samples obtained using
ProGuard tool2.
The use of formal methods
The advantages of using formal methods derive from the power of
automated model checking to search a state space and the genera-
tion of the formal model is automated too. However, as shown in
Table 1, our tool is not completely automated since the deﬁnition of
the logic formulae expressing the malicious behavior is handmade.
Verifying branching temporal logic formula allow us to recognize
a malicious behaviour non just checking the presence of given se-
quence, like for example pattern-matching approaches. In fact, the
fundamental drawback of a pattern-matching approach to malware
detection is that it ignores the behaviour of a program. Commer-
cial malware detectors use simple pattern matching approaches to
malware detection. A code is considered malware if it contains a
sequence of instructions that is matched by a regular expression.
Recently, it has been shown that such malware detectors can be
easily defeated using simple program obfuscations that are already
being used by the hackers.
Another feature of our approach is that we try to reuse exist-
ing model checkers avoiding the design of custom-made model
checker. Model checkers, especially the most widely used ones,
are extremely sophisticated programs that have been crafted over
many years by experts in the speciﬁc techniques employed by the
tool. A re-implementation of the algorithms in these tools could
likely yield worst performance.
The detection on Java Bytecode and not on the
source code
Performing Android malware families detection on the Bytecode
and not directly on the Java code has several advantages: (i) in-
dependence of the source programming language; (ii) detection of
malware families without decompilation even when source code is
lacking; (iii) ease of parsing a lower-level code; (iv) independence
from obfuscation.
2http://proguard.sourceforge.net/
9033. THE EXPERIMENT
In this section we demonstrate the effectiveness of FormalDroid
in ADRD malicious payload identiﬁcation testing real-world ma-
licious and legitimate applications. We gathered 861 malware in-
volved in the experiment from the Drebin [11] dataset, a collection
of freely available Android malware family labelled (91 as ADRD
while the remaining belonging to the most widespread families).
We express the FormalDroid effectiveness in terms of Accuracy.
This metric compares the number of obtained positive results with
the total number of evaluated samples. The Accuracy formula is
the following: Acc = (T P + T N)/(T P + FP + FN + T N), where
T P (True Positive) and T N (True Negative) are the number of sam-
ple correctly identiﬁed as belonging to ADRD family (T P) or not
(T N). FP (False Positive) represents the number of samples in-
correctly labeled as belonging to ADRD family by our tool. FN
(False Negative) is the number of samples that our tool incorrectly
not identiﬁes as ADRD malware. FP and FN take into account the
mismatch between our prediction and the right category which a
sample belongs to. Instead, T P and T N take into account the cor-
rect predictions, in these cases our prediction is in according with
the sample label. Table 2 shows the result achieved in ADRD rec-
ognizing by FormalDroid. FormalDroid obtains an accuracy equal
to 0.94.
Table 2: FormalDroid Performance Evaluation
Samples ∈ ADRD Samples (cid:54)∈ ADRD TP FP FN TN Acc
0.94
770
722
91
84
48
7
Family
ADRD
In order to demonstrate the resilience to obfuscation, we applied
to the ADRD malware samples evaluated a set of well-known code
transformations techniques using the implementation provided in
[2, 12]: (i) disassembling & reassembling, (ii) repacking, (iii) chang-
ing package name, (iv) identiﬁer renaming, (v) data encoding, (vi)
transform manifest, (vii) call indirections, (viii) code reordering,
(ix) defunct methods, and (x) junk code insertion.
Detection performance resulted unchanged in testing morphed
samples, as matter of fact FormalDroid marked as belonging to
ADRD family 84 malware on 91.
In order to measure Formal-
Droid performances, we used the System.currentTimeMillis() Java
method that returns the current time in milliseconds. Table 3 shows
the performance of our method. In particular, we consider the over-
all time to analyse a sample as the sum of two different contribu-
tions: the average time in seconds required to extract the class ﬁles
of the application using the dex2jar tool (tdex2 jar) and the time re-
quired to obtain the response from FormalDroid (tresponse). These
two values are the average times, i.e., they are computed as the total
time employed by FormalDroid to process the samples divided the
number of samples evaluated.
The machine used to run the experiments and to take measure-
ments was an Intel Core i5 desktop with 4 gigabyte RAM, equipped
with Microsoft Windows 7 (64 bit).
4. CONCLUSIONS AND FUTURE WORKS
In this paper we propose FormalDroid, a tool able to detect An-
droid malware behaviours, localizing the malicious payload in the
code of the application. We evaluated our tool analyzing the ADRD
family obtaining an accuracy equal to 0.94. As future works, we
plan to extend the experiments to other widespread malware fami-
lies in order to enforce the rule set we evaluated in this work.
Table 3: FormalDroid Time Performance Evaluation.
tdex2 jar
9.8471 s
tresponse
242.1838 s
Total Time
252.0309 s
Acknowledgements
This work has been partially supported by H2020 EU-funded projects
NeCS and C3ISP and EIT-Digital Project HII.
5. REFERENCES
[1] F. Mercaldo, V. Nardone, A. Santone, and C. A. Visaggio,
“Ransomware steals your phone. formal methods rescue it,”
in International Conference on Formal Techniques for
Distributed Objects, Components, and Systems, pp. 212–221,
Springer, 2016.
[2] V. Rastogi, Y. Chen, and X. Jiang, “Droidchameleon:
evaluating android anti-malware against transformation
attacks,” in Proceedings of the 8th ACM SIGSAC symposium
on Information, computer and communications security,
pp. 329–334, ACM, 2013.
[3] R. Milner, Communication and concurrency. PHI Series in
computer science, Prentice Hall, 1989.
[4] C. Stirling, “An introduction to modal and temporal logics
for ccs,” in Concurrency: Theory, Language, And
Architecture (A. Yonezawa and T. Ito, eds.), LNCS,
pp. 2–20, Springer, 1989.
[5] R. Cleaveland and S. Sims, “The ncsu concurrency
workbench,” in CAV (R. Alur and T. A. Henzinger, eds.),
vol. 1102 of Lecture Notes in Computer Science, Springer,
1996.
[6] L. Deshotels, V. Notani, and A. Lakhotia, “Droidlegacy:
Automated familial classiﬁcation of android malware,” in
Proceedings of ACM SIGPLAN on Program Protection and
Reverse Engineering Workshop 2014, PPREW’14, (New
York, NY, USA), pp. 3:1–3:12, ACM, 2014.
[7] G. Suarez-Tangil, J. E. Tapiador, P. Peris-Lopez, and
J. Blasco, “Dendroid: A text mining approach to analyzing
and classifying code structures in android malware families,”
Expert Syst. Appl., vol. 41, pp. 1104–1117, Mar. 2014.
[8] Y. Feng, S. Anand, I. Dillig, and A. Aiken, “Apposcopy:
Semantics-based detection of android malware through static
analysisâ´L ˚U.”
[9] G. Canfora, F. Mercaldo, and C. A. Visaggio, “An hmm and
structural entropy based detector for android malware,”
Comput. Secur., vol. 61, pp. 1–18, Aug. 2016.
[10] S. Alam, R. Riley, I. Sogukpinar, and N. Carkaci,
“Droidclone: Detecting android malware variants by
exposing code clones,” in 2016 Sixth International
Conference on Digital Information and Communication
Technology and its Applications (DICTAP), pp. 79–84, July
2016.
[11] D. Arp, M. Spreitzenbarth, M. Huebner, H. Gascon, and
K. Rieck, “Drebin: Efﬁcient and explainable detection of
android malware in your pocket,” in Proceedings of 21th
NDSS, IEEE, 2014.
[12] M. Zheng, P. P. Lee, and J. C. Lui, “Adam: an automatic and
extensible platform to stress test android anti-virus systems,”
in International Conference on Detection of Intrusions and
Malware, and Vulnerability Assessment, pp. 82–101,
Springer, 2012.
904