due to the limitations of our implementation of Gaussian
sampling, which operates with native C++ unsigned integers,
and selected bitwidth of prime moduli in the Double-CRT
representation. If these constraints are removed, higher values
of G-lattice base t can be used.
(cid:6)
(cid:5)
(cid:6)
m3n
mlog2 7n
VI. EFFICIENT MATRIX AND POLYNOMIAL ARITHMETIC
A. Matrix Chain Product in the Evaluation
The matrix chain multiplication in the evaluation operation
involves multiplications of encoding matrices of m × m by
each other, which requires a running time of O
for the
in the case of Strassen’s
naive implementation or O
algorithm. At the same time, the product of encoding matrices
is multiplied at the end by a row vector A0 ∈ Rq
1×m. This
suggests that by changing the order of multiplications, we can
transform this matrix chain multiplication into a row-vector-
by-matrix chain product. Each row-vector-by-matrix product
and can provide a running
has a running time of O
time improvement by a factor of m, as compared to the
naive implementation of matrix product. This optimization is
included in Algorithm 8 listed in Appendix D. A similar idea
was used in [25], [26].
m2n
(cid:5)
(cid:5)
(cid:6)
B. Efﬁcient Polynomial Arithmetic
1) Double-CRT Operations: All polynomial multiplications
are performed in the Double-CRT representation. We use the
bitwidth of 60 for each prime modulus (64-bit native unsigned
integers are leveraged for storing the numbers). This implies
a product of two polynomials with ring dimension n and
modulus q (bitwidth k) requires n(cid:8)k/60(cid:9) multiplications of
64-bit native integers, i.e., scales almost linearly with increase
in k. Hence, multiplications of polynomials with large k,
for example, 1000 bits, can be supported without involving
multiprecision arithmetic.
There are certain operations where we have to switch from
Double-CRT representation to a polynomial of multiprecision
integers with a large modulus q. This requires transforming
all small-modulus polynomials to the coefﬁcient representa-
tion and then performing the CRT interpolation to get large
(multiprecision) coefﬁcients of the polynomial with respect to
modulus q. This procedure is computationally expensive and
involves (cid:8)k/60(cid:9) NTTs followed by the CRT interpolation with
modulo reductions for every coefﬁcient with respect to q. The
two operations requiring CRT Interpolation are (1) G-sampling
where the digits of the large coefﬁcients are extracted and (2)
inﬁnity norm computation at the last stage of evaluation.
2) Number Theoretic Transform: The multiplication of ele-
ments in cyclotomic rings Rpi is performed using the Chinese
Remainder Transform (CRT) [57]. We use an implementation
of Fermat Theoretic Transform (FTT) described in [58]. We
implement FTT with Number Theoretic Transform (NTT) as
a subroutine. For NTT, we use the iterative Cooley-Tukey
algorithm with optimized butterﬂy operations, which is im-
plemented in PALISADE.
3) Cyclotomic Fields: For multiplications in K2n we use
the iterative Cooley-Tukey FTT algorithm over complex prim-
itive roots of unity.
363
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:37:30 UTC from IEEE Xplore.  Restrictions apply. 
To convert elements of rings to ﬁelds, we switch the poly-
nomials from the evaluation representation to the coefﬁcient
one as an intermediate step because the CRTs for rings operate
with modular primitive roots of unity and CRTs for ﬁelds deal
with complex primitive roots of unity.
4) Polynomial Transposition: Element transposition for a
polynomial f (x) = f0+f1x+···+fn−1xn−1 over cyclotomic
polynomial xn + 1 is expressed as f t(x) = f0− fn−1x−···−
f1xn−1. This transposition technique was used for both rings
and ﬁelds. In our implementation the transposition operation
is performed directly in evaluation representation by applying
an automorphism from f (ζ2n) to f (ζ 2n−1
5) Modular Arithmetic: For modular reduction of multi-
precision integers (in CRT interpolation), we use a generalized
Barrett modulo reduction algorithm [69]. This approach re-
quires one pre-computation per NTT run and converts modulo
reduction to roughly two multiplications.
2n
).
VII. IMPLEMENTATION DETAILS
A. Pseudocode of Obfuscation Scheme Algorithms
We provide pseudocode for key generation, encoding, ob-
fuscation, and evaluation of the scheme in Appendix D. The
pseudocode matches our implementation in C++.
B. Integer sampling
Both conjunction obfuscation and trapdoor sampling algo-
rithms call the integer sampling subroutine SampleZ(σ, c) that
returns a sample statistically close to DZ,c,σ. When the center
c does not change and distribution parameter is small (as in
directed encoding or Ring-LWE trapdoor construction), our
SampleZ implementation uses the inversion sampling method
developed in [64]. In all other cases (trapdoor sampling), we
use either Karney’s rejection sampler [45] or constant-time
sampler [46].
theoretical result
A bottleneck of integer sampling operations in lattice-
based cryptography, speciﬁcally those called in the subroutines
of GaussSamp, is the use of multiprecision ﬂoating-point
numbers where the number of bits in the mantissa should
roughly match the number of security bits supported by the
cryptographic protocol. A recent
in [46]
suggests that both the G-sampling and perturbation generation
algorithms used in our implementation can support at least
100 bits of security using double-precision ﬂoating point arith-
metic. More speciﬁcally, Lemma 3.2 in [46] states that λ/2
signiﬁcant bits in a ﬂoating-point number is sufﬁcient for λ bits
of security. This result also applies to joint (possibly depen-
dent) distributions, as in Lemma 4.3 of [46]. Because we are
not attempting to exceed 100 bits of security, the signiﬁcand
precision of 53 bits provided by IEEE 754 double-precision
ﬂoating numbers is sufﬁcient for our security target. Therefore,
our implementation of integer Gaussian sampling performs
computations on double-precision ﬂoating-point numbers.
C. Software Implementation
We implement the conjunction obfuscation scheme in PAL-
ISADE, an open-source lattice cryptography library. PAL-
ISADE uses a layered approach with four software layers, each
including a collection of C++ classes to provide encapsulation,
low inter-class coupling and high intra-class cohesion. The
software layers are as follows:
1) The cryptographic layer supports cryptographic protocols
such as homomorphic encryption schemes through calls
to lower layers.
2) The encoding layer supports plaintext encodings for cryp-
tographic schemes.
3) The lattice constructs layer supports power-of-two and
arbitrary cyclotomic rings (coefﬁcient, CRT, and double-
CRT representations). Lattice operations are decomposed
into primitive arithmetic operations on integers, vectors,
and matrices here.
4) The arithmetic layer provides basic modular operations
(multiple multiprecision and native math backends are
supported), implementations of Number-Theoretic Trans-
form (NTT), Fermat-Theoretic Transform (FTT), and
Bluestein FFT. The integer distribution samplers are
implemented in this layer.
Our conjunction obfuscation implementation is a new PAL-
ISADE module called “trapdoor”, which includes the follow-
ing new features broken down by layer:
• Conjunction obfuscation scheme in the cryptographic
layer.
• Directed encoding in the encoding layer.
• Trapdoor sampling, including Ring-LWE trapdoor gen-
eration, G-sampling and perturbation generation routines
in the lattice layer. Cyclotomic ﬁelds K2n and additional
polynomial/double-CRT operations, such as polynomial
transposition, are also in this layer.
• Generic integer Gaussian samplers and a Cooley-Tukey
transform based on complex roots of unity in the arith-
metic layer.
Several lattice-layer and arithmetic-layer optimizations are
also applied for runtimes improvements.
D. Loop parallelization
Multi-threading is performed using OpenMP3. Loop paral-
lelization is applied to parallelize obfuscation, lattice, and ma-
trix operations, and we use the following loop parallelization
optimizations:
1) In KeyGen (Algorithm 5), the loop calling TrapGen is
parallelized, with its results combined in an ordered way
into an STL vector.
2) In GaussSamp (Algorithm 2), the main loop is executed
in parallel. The loop is called by Encode, which is called
by Obfuscate. This optimization effectively achieves the
overall parallel execution of the obfuscation procedure.
3) The loops in matrix and matrix-vector multiplication are
parallelized. This optimization determines the paralleliza-
tion of Evaluate (Algorithm 8).
4) Number-theoretic transforms of matrices (vectors) of ring
elements are executed in parallel for each ring element.
3http://www.openmp.org/
364
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:37:30 UTC from IEEE Xplore.  Restrictions apply. 
This optimization applies to key generation, obfuscation,
and evaluation operations.
5) CRT Interpolation used in G-sampling (Obfuscate) and
norm computation (Evaluate) is executed in parallel for
each coefﬁcient of the polynomial.
We discuss the effect of these optimizations in Sec. VIII-E.
VIII. EXPERIMENTAL RESULTS
A. Testbed
Experiments were performed using a server computing
environment with 4 sockets of Intel Xeon CPU E7-8867 v3
rated at 2.50GHz, each with 16 cores. The total number of
cores was 64 (128 logical processors). 2TB of RAM was
accessible for the experiment. The executable was run using a
docker image with Linux Ubuntu 16.04 LTS. The evaluation
environment for parallelization experiments was a commodity
desktop computer with an Intel Core i7-3770 CPU with 4
cores (8 logical processors) rated at 3.40GHz and 16GB of
memory, running Linux CentOS 7. In all of our obfuscation
experiments, we selected the minimum modulus bitwidth k
that satisﬁes the correctness constraint (1) for a ring dimension
n corresponding to the chosen security level.
B. Integer Gaussian Sampling Experiments
We experimentally compared the runtimes of Karney’s
rejection method [45] with the generic sampler [46] using the
CDF inversion [64] method as the base sampler. The results are
in Appendix C. Based on this analysis, we selected Karney’s
method for our main conjunction obfuscation experiments.
C. Experiments for the Word Size of One Byte
Tables III and IV show results for the word size w of 8
bits in the server computing environment for 32-bit and 64-
bit conjunction programs, respectively. Σexp (Πv) is the actual
program size (experimentally measured as the RAM amount
used by the process after the obfuscation program is gener-
ated). These experiments were run in the multi-threaded mode
with 16 and 32 threads for 32-bit and 64-bit conjunctions,
respectively.
Tables III and IV also list the work factors (in bits of
security) for the VBB and lattice attacks, which are computed
as λV BB = log2 (tV BB) and λRLW E = log2 (tRLW E),
respectively. For 32-bit and 64-bit conjunctions, the number of
wildcard bits was set to 8 and 16, respectively. The wildcard
bits were assumed to be uniformly distributed over the words
of the pattern (2 wildcard bits per byte).
Table III suggests that lattice security parameters for 32-
bit conjunctions are sufﬁcient to match the VBB security, but
the VBB work factor for the case of n = 1024 is only 253
clock cycles, which corresponds to 39 core-days for a 2.5
GHz core. Table IV shows that the lattice attack work factor
starts exceeding the VBB work factor for 64-bit conjunctions
at n = 8192, when the VBB work factor is 273 clock cycles,
i.e., 1.1 × 105 core-years.
Note that our implementation is based on the entropic Ring-
LWE problem with a small-secret (ternary) distribution, which
TABLE III: Runtimes and program size for 32-bit conjunction
programs in a server computing environment for w=8
n
k log2 t λV BB/ Σexp (Πv) KeyGen Obfuscate Evaluate
λRLW E
1024 180
2048 180
4096 180
20
15
15
53/54
54/56
55/86
(GB)
5.85
16.4
37.9
(ms)
94
411
1141
(min)
6.2
17.3
36.0
(ms)
32
60
117
(s)
0.29
0.53
1.06
2.45
TABLE IV: Runtimes and program size for 64-bit conjunction
programs in a server computing environment for w=8
n
k log2 t λV BB/ Σexp (Πv) KeyGen Obfuscate Evaluate
λRLW E
1024 360
2048 360
4096 360
8192 360
20
20
18
18
70/60
71/61
72/62
73/87
(GB)
77
155
374
748
(s)
0.31
0.66
1.58
3.03
(hr)
0.7
1.4
3.3
6.7
is a stronger assumption than Ring-LWE. While our work
factor estimates already incorporate the effect of small-secret
distribution (using the LWE estimator [67]), the effect of the
entropic variant of Ring-LWE on the work factor is currently
unknown and is thus ignored in our estimates.
As suggested in Section V-D, program size is a major
practical limitation of conjunction obfuscator. For a 64-bit
conjunction program, the experimental program size reached
750 GB. However, the program size for a 32-bit program is
small enough to be loaded into the RAM of a commodity
desktop computer.
The experimental results in Tables III and IV also demon-
strate that the key generation time is small, on the order of
one second.
The obfuscation takes 6.7 hours to achieve 73-bit security
for the 64-bit conjunction program, and is the main compu-
tational bottleneck of conjunction obfuscator. This operation
is run ofﬂine and once per program. Thus obfuscation time is
does not impact many practical settings.
Evaluation takes 32 ms to acheive 53 bits of security for a
32-bit pattern and 2.5 seconds to attain 73-bit security for a 64-
bit conjunction pattern. The evaluation time is the main online
operation and is expected to be run frequently. The 32-bit
pattern results imply that runtime is practical. Our evaluation
runtime for a 64-bit conjunction obfuscator is smaller by more
than two orders of magnitude than the time (949 sec.) reported
for a 64-bit read-once branching program obfuscated using
GGH15 in [27].
D. Experiments for the Word Size of One Bit
To explore the effect of multilinearity degree on the runtime
metrics of conjunction obfuscator, we performed a series of
experiments at w = 1 (Table V). The multinearity degree of
directed encoding corresponds to L + 1 as we have one more
level of encoding at the end, which is speciﬁc to the test for
conjunction obfuscator.
Table V shows that our implementation is able to achieve