techniques, which enable models to encode the linguistic structure and meaning of log data. The LogRobust model [17] utilised static word embeddings to incorporate semantic information to log data representations. The proponents of the LogRobust model observed that word embeddings increased the model’s accuracy.C. Pre-trained Language ModelsIn recent research, there has been a growing interest in the use of pre-trained language models (LMs) such as BERT [21] to improve anomaly detection in system logs. Studies by Ott, Bogatinovski, Acker, et al. [10] and Le and Zhang [11] demonstrated that pre-trained LMs can offer significant advantages over word embeddings, which were used in the LogRobust model [17]. According to these studies, pre-trained LMs capture contextual information at the level of the whole sequence of log sentences, whereas word embeddings only provide representations for individual words in a single sentence. Furthermore, BERT generates context-sensitive semantic vectors that encode the order of words within a log sentence and are capable of handling out-of-vocabulary words. In contrast, the semantic vectors generated via word embeddings are static and are computed by computing the mean of the word embeddings of all the words comprising a log sentence.Existing research suggest that pre-trained LMs such as BERT can learn and incorporate both syntactic and semantic information that can improve the effectiveness of natural language processing tasks [22]–[25]. Therefore, in this study, a pre-trained LM is incorporated in the LogFiT model to leverage its ability to understand the sequential properties and linguistic structure of system logs.
III. LOGFITThe proposed LogFiT approach takes advantage of recent advances in Deep Learning and NLP to detect anomalies in system event logs. It leverages the linguistic capabilities of what Stanford University’s Institute for Human-Centered Artificial Intelligence refer to as ”foundation models” [26], which are models that have been pre-trained on very large, multi-modal corpora (text, images, video, etc.), and are intended to be used as a foundation (via transfer learning) for downstream NLP, image, and video tasks. Specifically, LogFiT uses the Longformer [27] model as its base. The LogFiT model is essentially a pre-trained Longformer that has been fine-tuned to learn the linguistic structures and sequential properties of log data. The Longformer model is an enhanced variant of the RoBERTa model [28] which in turn is an improvement over the original BERT model [21]. The Longformer model is chosen because of its ability to support more than 512 tokens- which is the limit for BERT.LogFiT adopts a self-supervised training approach whereby the model is trained on the normal log data so that it only learns the linguistic patterns of the normal log data. The training objective of the LogFiT model is to predict a number of masked tokens in the log sentences, so it makes use of cross-entropy loss to minimise the difference (i.e. ”loss”) between
44
Figure 3. Logical architecture of the LogFiT log anomaly detection approach.the model’s predicted tokens and the real tokens. Because the cross-entropy loss is logarithmic, it yields larger values for incorrect predictions than for correct predictions. Further, the model computes semantic vectors representing the log paragraphs being processed; the contextual embedding vector for the [CLS] token is used as the semantic vector. The [CLS] embedding vectors are computed for downstream tasks such as clustering and visualisation. Figure 3 illustrates the LogFiT approach and its model training and inference workflow. An early version of the LogFiT method is described in Almodovar, Sabrina, Karimi, et al. [29].A. Framework
Input Representation. The LogFiT model utilises normal log data for training, which has been converted to semantic vectors prior to being passed to the model. In contrast to both the DeepLog and LogBERT methods, the LogFiT model does not require the input log data to first be converted to log templates. Rather, log data is directly processed, tokenised, and transformed into semantic vectors in a single step. LogFiT takes inspiration from recent models [10], [11] that forego the log template extraction step, instead directly converting the log data into semantic vectors using a pre-trained LM. However, neither of the two aforementioned models fine-tune the pre-trained LM, instead utilizing the LM as a static semantic vectoriser.Transformer 	architecture. LogFiT inherits from the innovations introduced by the BERT-based language models.
Figure 4. 	The LogFiT model’s Transformer encoder layers, showing token embeddings are computed from the input tokens.The BERT model is a Transformer encoder model that has the capabilities of an auto-encoder. The BERT model’s encoder capability allows it to generate semantic vectors that are sensitive to the full context of the input log paragraph, while enabling it to reconstruct log paragraphs that have been corrupted [21]. BERT’s ability to accurately reconstruct corrupted input logs can be used as a threshold-based anomaly detection method. Thus by inheriting from BERT, LogFiT includes both the vectoriser and log anomaly detection component in a single architecture, allowing for end-to-end training that does not require intermediate log template extraction or vectorisation steps. As mentioned earlier, LogFiT uses the Longformer [27] model, which allows LogFiT to process log paragraphs containing up to 4096 tokens, which is longer than the 512-token limit of the BERT model. The Longformer model overcomes BERT’s limitation on the number of tokens, due to the quadratic computational complexity of BERT’s attention mechanism, by introducing a sliding window strategy which effectively reduces attention computation to a linear time [27].The LogFiT model consists of 12 stacked Transformer encoders, 12 attention heads per layer, 768-dimension vectors, a maximum possible sequence length of 4096 tokens – as illustrated in Figure 4. Not shown is the embedding layer, which is typically implemented inside the first Transformer encoder.