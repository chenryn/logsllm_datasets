Table II: Classiﬁer performance for LOC1 dataset (mean and
standard deviation for 10-fold cross validation).
Scenario
Curated traces
Full dataset
Combined labels
Precision
0.914 ± 0.002
0.904 ± 0.003
0.940 ± 0.003
Recall
0.909 ± 0.002
0.899 ± 0.003
0.935 ± 0.003
F1-score
0.908 ± 0.002
0.898 ± 0.003
0.934 ± 0.003
Figure 2: Performance per class in LOC1. Each dot represents
a class and its color the absolute difference between Precision
and Recall (blue low, red high).
as equivalent results provides an additional 3% increase.
Given the minimal differences,
the
experiments we use the full dataset.
in the remainder of
In the context of website ﬁngerprinting, Overdorf et al. [67]
showed that it is likely that the classiﬁer’s performance varies
signiﬁcantly between different individual classes. Thus, look-
ing only at average metrics, as in Table II, may give an incom-
plete and biased view of the classiﬁcation results. To check if
this variance holds on DNS traces we study the performance
of the classiﬁer for individual websites. The result is shown
in Figure 2. In this scatterplot each dot is a website and its
color represents the absolute difference between Precision and
Recall: blue indicates 0 difference and red indicates maximum
difference (i.e., |Precision − Recall| = 1). We see that some
websites (red dots on the right of the Precision scatterplot) have
high Recall – they are often identiﬁed by the classiﬁer, but low
Precision – other websites are also identiﬁed as the website.
Thus, these websites have good privacy since the false positives
provide the users with plausible deniability. For other pages
(red dots on the right of the Recall scatterplot), the classiﬁer
obtains low Recall – it almost never identiﬁes them, but high
Precision – if they are identiﬁed, the adversary is absolutely
sure her guess is correct. The latter case is very relevant for
privacy, and in particular, censorship, as it enables the censor
to block without fear of collateral damage.
Open world. In the previous experiments, the adversary knew
that the webpage visited by the victim was within the training
dataset. We now evaluate the adversary’s capability to distin-
guish those webpages from other unseen trafﬁc. Following
prior work [65, 68] we consider two sets of webpages, one
monitored and one unmonitored. The adversary’s goal is to
determine whether a test trace belongs to a page within the
monitored set.
00.20.40.60.81MetricvalueF1-ScorePrecisionRecallMetric0.00.20.40.60.81.0|Precision−Recall|Table III: F1-Score of the n-grams, k-Fingerprinting, CUMUL
and DF features for different subsets of trafﬁc: only DoH trafﬁc
(DoH-only), only HTTPS trafﬁc corresponding to web trafﬁc
(Web-only) and mixed (DoH+Web).
n-grams
k-Fingerprinting [18]
CUMUL [16]
DF [19]6
DoH-only Web-only
DoH + Web
0.87
0.74
0.75
0.51
0.99
0.95
0.92
0.94
0.88
0.79
0.77
0.75
Figure 3: Precision-Recall ROC curve for open world clas-
siﬁcation, for the monitored class. The notches indicate the
variation in threshold, t.
We train a classiﬁer with both monitored and unmonitored
samples. Since it is not realistic to assume that an adversary
can have access to all unmonitored classes, we create unmoni-
tored samples using 5,000 webpages traces formed by a mix of
the OW and LOC1 datasets. We divide the classes such that 1%
of all classes are in the monitored set and 10% of all classes
are used for training. We select monitored pages at random.
This assumes that different adversaries may be interested in
blocking different pages, and enables us to evaluate the average
case. We ensure that the training dataset is balanced, i.e., it
contains equal number of monitored and unmonitored samples;
and the test set contains an equal number of samples from
classes used in training and classes unseen by the classiﬁer.
When performing cross validation, in every fold, we consider
a different combination of the monitored and unmonitored
classes for training and testing so that we do not overﬁt to
a particular case.
To decide whether a target trace is monitored or unmonitored,
we use a method proposed by Stolerman et al. [69]. We
assign the target trace to the monitored class if and only if
the classiﬁer predicts this class with probability larger than
a threshold t, and to unmonitored otherwise. We calculate
the Precision-Recall ROC curve for the monitored class using
scikit-learn’s precision-recall curve plotting function, which
varies the discrimination threshold, t. The curve is shown in
Figure 3, where the notches indicate the varying t.
We also plot in Figure 3 the curve corresponding to a random
classiﬁer, a naive classiﬁer that outputs positive with probabil-
ity the base rate occurrence of the positive class. This classiﬁer
serves as baseline to assess the effectiveness of the n-grams.
When we vary the discrimination threshold of the random
classiﬁer, Precision remains constant, i.e., the threshold affects
TPs and FPs in the same proportion. The effect of the threshold
in FNs, however, is inversely proportional to TNs. Thus, Recall
changes depending on the classiﬁer’s threshold. The AUC
(Area Under Curve) for random calssiﬁer is 0.05, while for the
n-grams classiﬁer is 0.81. When t = 0.8, the n-grams classiﬁer
has an F1-score of ≈ 0.7, indicating that trafﬁc analysis is a true
threat to DNS privacy even in the open world scenario.
Comparison to web trafﬁc ﬁngerprinting. To understand
the gain DNS ﬁngerprinting provides to an adversary, we
compare its effectiveness to that of web trafﬁc ﬁngerprinting.
We also evaluate the suitability of n-grams and traditional
website ﬁngerprinting features to both ﬁngerprinting problems.
We compare it to state-of-the-art attacks that use different fea-
tures: the k-Fingerprinting attack, by Hayes and Danezis [18],
that considers a comprehensive set of features used in the
website ﬁngerprinting literature; CUMUL, by Panchenko et
al. [16] which focuses on packets’ lengths and order through
cumulative features; and Deep Fingerprinting (DF), an attack
based on deep convolutional neural networks [19]. In this
comparison, we consider a closed-world of 700 websites (WEB
dataset) and use a random forest with the same parameters as
classiﬁcation algorithm. We evaluate the performance of the
classiﬁers on only DoH trafﬁc (DoH-only), only HTTPS trafﬁc
corresponding to web content trafﬁc (Web-only), and a mix of
the two in order to verify the claim in the RFC that DoH [9]
holds great potential to thwart trafﬁc analysis. Table III shows
the results.
First, we ﬁnd that for DNS trafﬁc, due to its chatty character-
istics, n-grams provide more than 10% performance improve-
ment with respect to traditional features. DF would probably
achieve higher accuracy if it was trained with more data. To ob-
tain a signiﬁcant improvement, however, DF requires orders of
magnitude more data. Thus, it scales worse than our attack. We
also see that the most effective attacks are those made on web
trafﬁc. This is not surprising, as the variability of resources’
sizes in web trafﬁc contains more information than the small
DNS packets. What is surprising is that the n-grams features
outperform the traditional features also for website trafﬁc. Fi-
nally, as predicted by the standard, if DoH and HTTPS are sent
on the same TLS tunnel and cannot be separated, both set of
features see a decrease in performance. Still, n-grams outper-
forms traditional features, with a ∼10% improvement.
In summary, the best choice for an adversary with access to the
isolated HTTPS ﬂow is to analyse that trace with our novel n-
grams features. However, if the adversary is in ‘a different
path than the communication between the initiator and the
recipient’ [6] where she has access to DNS, or is limited
in resources (see below), the DNS encrypted ﬂow provides
comparable results.
Adversary’s effort. An important aspect to judge the severity
of trafﬁc analysis attacks is the effort needed regarding data
collection to train the classiﬁer [28]. We study this effort from
two perspectives: amount of samples required – which relates
to the time needed to prepare the attack, and volume of data
– which relates to the storage and processing requirements for
the adversary.
6We evaluate DF’s accuracy following the original paper, i.e., using valida-
tion and test sets, instead of 10-fold cross-validation.
7
Table IV: Classiﬁer performance for different number of sam-
ples in the LOC1 dataset averaged over 10-fold cross validation
(standard deviations less than 1%).
Number of samples
Precision
10
20
40
100
0.873
0.897
0.908
0.912
Recall
0.866
0.904
0.914
0.916
F1-score
0.887
0.901
0.909
0.913
Table V: F1-score when training on the interval indicated by
the row and testing on the interval in the column (standard
deviations less than 1%). We use 20 samples per webpage
(the maximum number of samples collected in all intervals).
F1-score
0 weeks old
2 weeks old
4 weeks old
6 weeks old
8 weeks old
0 weeks old
2 weeks old
4 weeks old
6 weeks old
8 weeks old
0.880
0.886
0.868
0.775
0.770
0.827
0.921
0.898
0.796
0.784
0.816
0.903
0.910
0.815
0.801
0.795
0.869
0.882
0.876
0.893
0.745
0.805
0.817
0.844
0.906
We ﬁrst look at how many samples are required to train a
well-performing classiﬁer. We see in Table IV that there is
a small increase between 10 and 20 samples, and that after
20 samples, there are diminishing returns in increasing the
number of samples per domain. This indicates that, in terms
of number of samples, the collection effort to perform website
identiﬁcation on DNS traces is much smaller than that of
previous work on web trafﬁc analysis: Website ﬁngerprinting
studies in Tor report more than 10% increase between 10 and
20 samples [30] and between 2% and 10% between 100 and
200 samples [53, 19].
We believe the reason why ﬁngerprinting DoH requires fewer
samples per domain is DoH’s lower intra-class variance with
respect
to encrypted web trafﬁc. This is because sources
of large variance in web trafﬁc, such as the presence of
advertisements which change accross visits thereby varying
the sizes of the resources, does not show in DNS trafﬁc for
which the same ad-network domains are resolved [70].
In terms of volume of data required to launch the attacks,
targeting DoH ﬂows also brings great advantage. In the WEB
dataset, we observe that web traces have a length of 1.842
MB ± 2.620 MB, while their corresponding DoH counterpart
only require 0.012 MB ± 0.012 MB. While this may not seem
a signiﬁcant difference, when we look at the whole dataset
instead of individual traces, the HTTPS traces require 73GB
while the DoH-only dataset ﬁts in less than 1GB (0.6GB). This
is because DNS responses are mostly small, while web trafﬁc
request and responses might be very large and diverse (e.g.,
different type of resources, or different encodings).
In our experiments,
to balance data collection effort and
performance, we collected 60 samples per domain for all our
datasets. For the unmonitored websites in the open world we
collected just three samples per domain (recall that we do not
identify unmonitored websites).
C. DNS Fingerprinting Robustness
In practice,
the capability of the adversary to distinguish
websites is very dependent on differences between the setup
for training data collection and the environmental conditions at
attack time [71]. We present experiments exploring three envi-
ronmental dimensions: time, space, and infrastructure.
1) Robustness over time: DNS traces vary due to the dy-
namism of webpage content and variations in DNS responses
(e.g., service IP changes because of load-balancing). To under-
stand the impact of this variation on the classiﬁer, we collect
data LOC1 for 10 weeks from the end of September to the
beginning of November 2018. We divide this period into ﬁve
intervals, each containing two consecutive weeks, and report
in Table V the F1-score of the classiﬁer when we train the
classiﬁer on data from a single interval and use the other
intervals as test data (0 weeks old denotes data collected in
November). In most cases, the F1-score does not signiﬁcantly
decrease within a period of 4 weeks. Longer periods result in
signiﬁcant drops – more than 10% drop in F1-score when the
training and testing are separated by 8 weeks.
This indicates that, to obtain best performance, an adversary
with the resources of a university research group would need
to collect data at least once a month. However, it is unlikely
that DNS traces change drastically. To account for gradual
changes, the adversary can perform continuous collection and
mix data across weeks. In our dataset, if we combine two-
and three-week-old samples for training; we observe a very
small decrease in performance. Thus, a continuous collection
strategy can sufﬁce to maintain the adversary’s performance
without requiring periodic heavy collection efforts.
2) Robustness across locations: DNS traces may vary across
locations due to several reasons. First, DNS lookups vary
when websites adapt
to speciﬁc geographic
regions. Second, popular resources cached by resolvers vary
across regions. Finally, resolvers and CDNs use geo-location
methods to load-balance requests, e.g., using anycast and
EDNS [72, 73].
their content
We collect data in three locations, two countries in Europe
(LOC1 and LOC2) and a third country in Asia (LOC3).
Table VI (leftmost) shows the classiﬁer performance when
crossing these datasets for training and testing. When trained
and tested on the same location unsurprisingly the classiﬁer
yields results similar to the ones obtained in the base experi-
ment. When we train and test on different locations, the F1-
score decreases between a 16% and a 27%, the greatest drop
happening for the farthest location, LOC3, in Asia.
Interestingly, even though LOC2 yields similar F1-Scores
when cross-classiﬁed with LOC1 and LOC3, the similarity
does not hold when looking at Precision and Recall individ-
ually. For example, training on LOC2 and testing on LOC1
results in around 77% Precision and Recall, but training on
LOC1 and testing on LOC2 gives 84% Precision and 65%
Recall. Aiming at understanding the reasons behind this asym-
metry, we build a classiﬁer trained to separate websites that ob-
tain high recall (top 25% quartile) and low recall (bottom 25%
quartile) when training with LOC1 and LOC3 and testing in
LOC2. A feature importance analysis on this classiﬁer showed
that LOC2’s low-recall top features have a signiﬁcantly lower
importance in LOC1 and LOC2. Furthermore, we observe that
the intersection between LOC1 and LOC3’s relevant feature
sets is slightly larger than their respective intersections with
LOC2. While it is clear that the asymmetry is caused by the
8
Figure 4: Top 15 most important features in Google’s and
Cloudﬂare’s datasets. On the left, features are sorted by the
results on Google’s dataset and, on the right, by Cloudﬂare’s.
conﬁguration of the network in LOC2, its exact cause remains
an open question.
3) Robustness across infrastructure: In this section, we study
how the DoH resolver and client, and the user platform affect
inﬂuence the attack’s performance.
Inﬂuence of DoH Resolver. We study two commercial DoH
resolvers, Cloudﬂare’s and Google’s. Contrary to Cloudﬂare,
Google does not provide a stand-alone DoH client. To keep the
comparison fair, we instrument a new collection setting using
Firefox in its trusted recursive resolver conﬁguration with both