title:Are We Susceptible to Rowhammer? An End-to-End Methodology for Cloud
Providers
author:Lucian Cojocar and
Jeremie S. Kim and
Minesh Patel and
Lillian Tsai and
Stefan Saroiu and
Alec Wolman and
Onur Mutlu
Are We Susceptible to Rowhammer?
An End-to-End Methodology for Cloud Providers
Lucian Cojocar, Jeremie Kim§†, Minesh Patel§, Lillian Tsai‡,
Stefan Saroiu, Alec Wolman, and Onur Mutlu§†
Microsoft Research, §ETH Z¨urich, †CMU, ‡MIT
Abstract—Cloud providers are concerned that Rowhammer poses
a potentially critical threat to their servers, yet today they lack a
systematic way to test whether the DRAM used in their servers is
vulnerable to Rowhammer attacks. This paper presents an end-
to-end methodology to determine if cloud servers are susceptible
to these attacks. With our methodology, a cloud provider can
construct worst-case testing conditions for DRAM.
We apply our methodology to three classes of servers from a
major cloud provider. Our ﬁndings show that none of the CPU
instruction sequences used in prior work to mount Rowhammer
attacks create worst-case DRAM testing conditions. To address
this limitation, we develop an instruction sequence that lever-
ages microarchitectural side-effects to “hammer” DRAM at a
near-optimal rate on modern Intel Skylake and Cascade Lake
platforms. We also design a DDR4 fault injector that can reverse
engineer row adjacency for any DDR4 DIMM. When applied to
our cloud provider’s DIMMs, we ﬁnd that DRAM rows do not
always follow a linear map.
I. INTRODUCTION
The consequences of a large-scale security compromise of a
cloud provider cannot be overstated. An increasing number of
banks, hospitals, stores, factories, and universities depend upon
cloud resources for their day-to-day activities. Users store
important and private data in the cloud, including tax returns,
health records, e-mail, and backups. Home devices and home
automation are also becoming reliant on cloud infrastructure.
An attack that steals or deletes data, or performs a large-
scale denial of service (DoS) attack on the cloud, would be
catastrophic to both cloud providers and customers.
Today’s DRAM is vulnerable to memory disturbance errors:
a high rate of accesses to the same address in DRAM ﬂips bits
in data stored in nearby addresses [69]. Rowhammer attacks
generate adversarial workloads that exploit disturbance errors
to ﬂip the value of security-critical bits, such as an OS’s page
table entries [97], [32], a browser’s isolation sandbox [32],
a CPU’s isolation mechanism [59], an encryption key [97],
or a CPU instruction opcode [36]. Even worse, mounting
Rowhammer requires no special privilege: attacks have been
demonstrated by running user-level code [32], [97], JavaScript
programs [13], [37], and even by sending RDMA network
packets [106], [83]. Because all DRAM is potentially sus-
ceptible, Rowhammer attacks are an easy way to mount a
large-scale catastrophic attack on the cloud. The combination
of easy-to-mount and easy-to-scale makes Rowhammer a
formidable potential attack vector to the cloud.
Unfortunately, the threat that Rowhammer poses to cloud
providers remains unclear. Security researchers have been pub-
lishing a stream of proof-of-concept exploits using Rowham-
mer that affect all types of DRAM, including DDR3 [69], [32],
DDR4 [110], [77], and ECC-equipped DRAM [18]. DRAM
vendors claim that their memory is safe against Rowhammer
attacks; these claims are delivered to cloud providers with each
new DRAM feature: DDR4 [77], ECC-equipped DRAM [33],
[31], and TRR-equipped DRAM [81], [33]. There is a large
gap between a proof-of-concept exploit carried out in a re-
search lab and an actual attack in the wild. In fact, no evidence
indicates that Rowhammer attacks have been carried out in
practice. In the absence of attacks in the wild, one could easily
dismiss Rowhammer as a credible threat.
This confusion is further fed by the lack of a system-
atic methodology to test for Rowhammer. Previous proof-of-
concept attacks used varied methodologies to mount Rowham-
mer [69], [101], [32], [114], [13], [96], [11], [12], [37], [94],
[97], [110], [77], [59], [2], [35], [36], [105], [25], [106],
[83], [95], [16], [18], [10], [118], [22] based on heuristics,
without rigorously characterizing their effectiveness. While
such approaches can demonstrate an attack’s viability, they are
unsuitable for testing purposes because they cannot distinguish
between Rowhammer-safe memory and a sub-optimal, imper-
fect testing methodology. Lacking a comprehensive testing
tool, cloud providers ﬁnd it difﬁcult to ascertain the degree
to which Rowhammer poses a threat to their infrastructure.
Building a systematic and scalable testing methodology
must overcome two serious practical challenges. First,
it
must devise a sequence of CPU instructions that leads to a
maximal rate of row activations in DRAM. This sequence
must overcome the hardware’s attempts to capture locality,
such as the CPU’s re-ordering of instructions and the DRAM
controller’s re-ordering of memory accesses. For this, we need
to measure the row activation rates of the instruction sequences
used by previous work, identify their bottlenecks, and test new
candidates that overcome these bottlenecks. Previous work
showed that the probability of ﬂipping bits in a Rowhammer
attack increases with the rate of row activations [89], [69].
The second challenge is determining row adjacency in
a DRAM device. Contiguous virtual addresses do not map
linearly to DRAM rows and are in fact subject
to three
mapping layers. The OS maintains a virtual-to-physical ad-
dress map that can change often and at runtime [55], [57];
if present, virtualization adds another mapping layer due
to guest-physical addresses. The memory controller further
maps physical addresses to logical bus addresses speciﬁed
in terms of ranks, banks, rows, and columns [72], [1]. The
ﬁnal mapping is done by the DRAM device itself, where
a device can remap adjacent logical bus addresses to non-
adjacent physical rows [71]. DRAM vendors consider these
maps to be trade secrets and strongly guard their secrecy.
Prior techniques to reverse engineer row adjacency with
This paper presents solutions to both challenges and com-
bines them in an end-to-end methodology that creates worst-
case conditions for testing the presence of disturbance errors
in DRAM. Our methodology lets cloud providers construct (1)
an instruction sequence that maximizes the rate of DRAM row
activations on a given system, and (2) accurate maps of address
translations used by the system’s hardware. Armed with this
knowledge, a cloud provider can develop a quality control
pipeline to test its servers’ DRAM and ultimately characterize
the risk of a Rowhammer attack to its infrastructure.
We start by showing how a memory bus analyzer can char-
acterize the effectiveness of a sequence of CPU instructions
when hammering memory. It can measure the rate of activation
commands and compare them to the optimal rate (i.e., the
highest rate of activations to memory that the speciﬁcations
allow). Our results show that all instruction sequences used in
previous work hammer memory at a sub-optimal rate. Most
previous sequences have a rate that is at most half of optimal,
and the most effective previous sequence is 33% from optimal.
We tested 42 different instruction sequences, including those
found in previous work [69], [32], [114], [13], [96], [11],
[37], [94], [97], [110], [77], [59], [2], [35], [36], [105], and
developed additional variants.
Our characterization sheds light on the factors that prevent
these instruction sequences from having a high rate of ac-
tivations. One signiﬁcant factor is out-of-order execution –
the CPU constantly re-orders memory accesses to increase
the likelihood they are served from the cache. Out-of-order
execution can act as a de facto rate limiter to Rowhammer
attacks. Equally signiﬁcant are memory barriers. Some instruc-
tion sequences use memory barriers to order their memory
accesses. Although barriers do prevent out-of-order execution,
we ﬁnd they are too slow. Instruction sequences that use
memory barriers lack the performance necessary to create a
high rate of activations.
a commodity memory controller rely on Rowhammer at-
tacks [100], [94], [105] and work only on DIMMs that
succumb to them. Once bits ﬂip, the ﬂips’ locations reveal
information on row adjacency. This creates a chicken-and-egg
problem: testing DIMMs’ resiliency to Rowhammer requires
knowing row adjacency information, and reverse engineering
row adjacency requires having DIMMs succumb to Rowham-
mer.
This analysis led us to construct a near-optimal instruc-
tion sequence that maximizes the rate of activations, effec-
tively matching the minimum row cycle time of the DDR4
JEDEC [61] spec. Our instruction sequence differs consid-
erably from all sequences used in previous work because
it uses no explicit memory accesses (e.g., no load or store
instructions). Instead, we craft our instruction sequence to
leverage microarchitectural side-effects of clﬂushopt that is-
sues memory loads in order and without the use of memory
barriers.
We overcome our second challenge, determining row ad-
jacency, by designing and building a DDR4 fault-injector
that guarantees that any DIMM succumbs to a Rowhammer
attack. Our fault-injector is both low-cost and compatible with
any DDR4 motherboard. It suppresses all refresh commands
received by a DIMM for a ﬁxed period of time. The absence
of refreshes ensures the success of a Rowhammer attack in
ﬂipping bits on today’s DDR4 memory. The location and
density of bit ﬂips lets our methodology reverse engineer the
physical row adjacency of any DDR4 DRAM device. To our
knowledge, ours is the ﬁrst fault injector capable of injecting
faults into DDR4 commands.
We leverage the fault injector to reverse engineer physical
adjacency in a major cloud provider’s DRAM devices supplied
by three different vendors. Our results show that logical rows
do not always map linearly, but instead can follow a half-
row pattern, where two halves of a single row are adjacent to
different rows. A methodology that uses guess-based heuristics
to determine row adjacency will be ineffective in testing these
half-row patterns. We also ﬁnd that mounting a Rowhammer
attack on a victim row that follows a half-row pattern requires
hammering more aggressor rows than it does for a victim row
that is contiguous within a single physical row.
We applied our methodology on a major cloud provider’s
three most recent classes of servers based on Intel’s Cascade
Lake, Skylake, and Broadwell architectures. On the two newest
architectures, Cascade Lake and Skylake, our methodology
achieves a near-optimal rate of activations by using clﬂushopt
to “hammer” memory, an instruction introduced with the
Skylake architecture. Finally, we used the fault injector to
successfully reverse engineer physical row adjacency on all
three classes of servers.
II. BACKGROUND
Rowhammer bit ﬂips result from circuit-level charge leakage
mechanisms that are exacerbated by certain memory access
patterns. This section provides a high-level background on
DRAM and the physical mechanisms responsible for the
Rowhammer phenomenon in order to facilitate the understand-
ing of our work. More detail on Rowhammer and its system-
level implications can be found in [69], [88], [89].
A. DRAM Organization
DRAM comprises a hierarchy of two-dimensional arrays,
as shown in Figure 1. At the top level, a DRAM controller
interfaces with a DRAM rank over a channel (Figure 1a).
The channel conveys DRAM commands, addresses, and data
between the DRAM controller and the DRAM rank. In modern
systems, multiple DRAM ranks are typically combined in a
DRAM module (Figure 1b). The DRAM controller uses chip-
select signals to interface with only a single DRAM rank at
any given time.
A DRAM rank consists of multiple physical DRAM chips
(Figure 1c). The DRAM controller is unaware of how a single
rank is partitioned into individual DRAM chips. Instead, it sees
each rank as the union of multiple banks that are each striped
across the physical DRAM chips that form the rank. Thus, one
bank spans multiple DRAM chips, and a single DRAM chip
stores data from multiple banks. This has implications on how
failures affect banks. Different DRAM chips can have different
failure proﬁles depending on how they are manufactured; thus,
a “weak” DRAM chip affects multiple banks. However, only
a portion of each bank is affected, namely, the portion that
corresponds to the weak DRAM chip.
DRAM banks within a chip are further subdivided into
rows and columns of storage cells (Figure 1d), where each
cell encodes a single bit of data using the amount of charge
2
Figure 1: Typical DRAM organization.
stored in a capacitor (i.e., data “1” as either fully-charged or
fully-discharged, and data “0” as its opposite). The DRAM
controller accesses a cell by specifying a row address and a
column address to a particular bank. It has no knowledge of
the physical layout of banks or that a bank comprises multiple
physical chips.
B. How DRAM Operates
A DRAM read operation senses the amount of charge
stored in cell capacitors. It is subdivided into three phases:
(1) Activation (ACT): the charge stored in an entire DRAM
row’s cells within a bank is sensed and stored in a row buffer.
The row buffer acts as a fast cache; subsequent accesses to
the same row do not require another activation. Instead, data
is read out of the row buffer. (2) Read (RD): the contents of
the row buffer at a given column address are returned to the
DRAM controller. (3) Precharge (PRE): the bank is prepared
for the next access by disconnecting and clearing the active
row buffer.
All rows in a bank share one row buffer. Within a bank, the
DRAM controller activates and reads from only one row at a
time. Write operations work similarly to reads.
Refresh (REF): DRAM cells leak their charge, which can
cause data loss when cells are not accessed frequently enough.
To prevent this, the DRAM controller issues periodic refresh
operations that replenish the cells’ charge. The DDR4 standard
speciﬁes that 8192 refresh commands be issued during a 64
ms time period [61], which results in considerable power