title:Recovery and Performance Balance of a COTS DBMS in the Presence
of Operator Faults
author:Marco Vieira and
Henrique Madeira
Recovery and Performance Balance of a COTS DBMS in the Presence of
Operator Faults
Marco Vieira
ISEC
Polytechnic Institute of Coimbra
3031 Coimbra - Portugal
PI:EMAIL
Abstract
A major cause of failures in large database manage-
ment systems (DBMS) is operator faults. Although most of
the complex DBMS have comprehensive recovery mecha-
nisms, the effectiveness of these mechanisms is difficult to
characterize. On the other hand, the tuning of a large
database is very complex and database administrators
tend to concentrate on performance tuning and disregard
the recovery mechanisms. Above all, database adminis-
trators seldom have feedback on how good a given con-
figuration is concerning recovery. This paper proposes an
experimental approach to characterize both the perfor-
mance and the recoverability in DBMS. Our approach is
presented through a concrete example of benchmarking
the performance and recovery of an Oracle DBMS run-
ning the standard TPC-C benchmark, extended to include
two new elements: a faultload based on operator faults
and measures related to recoverability. A classification of
operator faults in DBMS is proposed. The paper ends
with the discussion of the results and the proposal of
guidelines to help database administrators in finding the
balance between performance and recovery tuning.
1. Introduction
The ascendance of networked information in our
economy and daily lives has increased awareness of the
importance of dependability features. In many cases, such
as in e-commerce systems, computer outages can result in
a huge loss of money or in an unaffordable loss of prestige
for companies. In fact, due to the impressive growth of the
Internet,
server
somewhere may be directly exposed as loss of service to
thousands of users around the world.
some minutes of downtime
in a
Databases play a central
role in this information
infrastructure and it
is well known that Database
Management Systems (DBMS) have a long tradition in
high dependability, particularly in what concerns data
integrity
basic
mechanisms needed to achieve data recovery, such as
transactions, checkpointing, logging, and replica control
management have been proposed/consolidated in the
availability
aspects.
and
Several
Henrique Madeira
DEI-FCTUC
University of Coimbra
3030 Coimbra - Portugal
PI:EMAIL
database arena. However, and in spite of
the very
important role played by these mechanisms in DBMS,
there is no practical way to benchmark their effectiveness
or at least to characterize the innumerous configuration
alternatives available in typical DBMS products in what
concerns the impact of these configurations on database
performance and recovery. 
administrators
The tuning of a large commercial database is a very
complex task and database
tend to
concentrate on performance tuning and often disregard the
recovery mechanisms. The
for
increased performance from the end-users and the fact that
database administrators seldom have feedback on how
good a given configuration is concerning recovery
(because faults are relatively rare events) largely explain
the present scenario.
demands
constant
Database industry holds a reputed infrastructure for
performance evaluation and the set of benchmarks man-
aged by the Transaction Processing Performance Council
(TPC) are recognized as one of the most successful
benchmark initiatives of the overall computer industry.
However, data recovery has been largely absent from TPC
benchmarking effort. Existing TPC benchmark specify
that data recovery features of the database must ensure
that data can be recovered from any point in time during
the benchmark running, but the benchmarks specifications
do not include any procedure to confirm that these mecha-
nisms are working properly or to measure the impact of
the recovery mechanisms on the system performance.
The major problem of having pure performance
benchmarks (i.e., benchmarks that only measure raw
performance) for DBMS and transactional systems in
general is that the benchmark results tend to portrait rather
artificial situations, as data recovery mechanisms are
configured for
the minimum impact on transaction
performance and the effectiveness of data recovery is
totally disregarded. The
tight dependence between
performance and recovery tuning in modern DBMS urge
the definition of practical methods to characterize a given
Funding for
this paper was provided,
in part, by Portuguese
Government/European Union through R&D Unit 326/94 (CISUC) and
by DBench project, IST 2000 - 25425 DBENCH, funded by the
European Union.
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:16:58 UTC from IEEE Xplore.  Restrictions apply. 
configuration or compare different DBMS products in a
more realistic scenario. Above all,
to
include in the benchmarks new measures that show the
benefit of including better recovery mechanisms in the
system or configuring the available mechanisms
to
achieve the best recoverability.
is important
it
This paper proposes an experimental approach to char-
acterize both the performance and the recoverability in
DBMS by extending existing performance benchmarks to
include a faultload (i.e., a set of faults or stressful condi-
tions that activate the recovery mechanisms) and measures
related to recoverability. The approach is presented
through a concrete example of benchmarking the perform-
ance and recovery of an Oracle DBMS running the stan-
dard Transaction Processing Performance Council (TPC)
TPC-C benchmark [1], extended with two new elements:
1) measures related to recovery time, data integrity viola-
tions, and lost transactions and 2) a set of operator faults
as a faultload. This experimental approach is generic in
the sense that it can be applied to any DBMS (i.e., it has
the same field of application as the TPC-C benchmark).
In addition to the first proposal of the extension of
TPC-C benchmark to characterize both the performance
and the recoverability in DBMS, this paper also has the
following contributions:
- Evaluation of the Oracle recovery mechanisms in the
presence of operator faults. It is worth noting that it is
generally assumed that
typical DBMS recovery
mechanisms are quite effective, which is in general
corroborated by years of intensive use of DBMS in the
field. Very few works have evaluated experimentally
the behavior of DBMS in the presence of faults.
However, all the experimental evaluation works known
in the literature have shown that a non-negligible
number of faults are not handled correctly by DBMS
[2, 3, 4, 5, 6]. While previous papers are “classic” fault
that have injected hardware and
injection works,
software faults, our work is,
to the best of our
the first experimental evaluation of a
knowledge,
DBMS with operator faults.
- Proposal of a general classification for operator faults
in DBMS. The instantiation of this classification for the
Oracle DBMS is presented and a set of tools have been
designed and built to cause these faults in the target
system in the context of the extension of the TPC-C
benchmark. The method used actually inserts typical
operator
operator
commands using exactly the same means used by the
real database administrator in the field, which assures a
correct reproduction of operator faults. This approach
is generic (i.e., can be applied to any DBMS) and is
fully automatic.
by mimicking wrong
faults
- As an extension of an existing performance benchmark,
this work is a first contribution towards the proposal of
standard dependability benchmarks for DBMS. The
concept of dependability benchmarking has gained
ground in the last few years and is currently the subject
of intense research [7, 8, 9]. The idea is to devise stan-
dardized ways to evaluate both dependability and
performance of computer systems or components.
Comparing to well-established performance bench-
marks, dependability benchmarks have two new
elements, which are the measures related to depend-
ability and the faultload. In our work, the dependability
measures are focused on recoverability (which is di-
rectly related to DBMS down time) and data integrity,
as these are the most relevant measures in typical
database applications, and the faultload consists of
operator faults, which is one unanimously considered
as a very important source of failures in databases.
The structure of the paper is as follows:
the next
section presents background on DBMS, especially in what
concerns recovery and performance tuning. Section 3
discusses the problem of operator faults in DBMS and
presents a classification for this kind of faults based on
interviews with database administrators of real databases
installations, as well as analysis of
typical database
administration operations. A short description of the set of
tools built to insert these faults is also presented in section
3. The experimental setup is presented in section 4 and the
results are presented and discussed in section 5. Section 6
concludes the paper.
2. Background on DBMS
A database is a collection of data describing the
activities of one or more related organizations [10]1. The
software designed to assist
in maintaining and using
databases is called database management system, or
DBMS. A DBMS allows users to define the data to be
stored in terms of a data model, which is a collection of
high-level metadata that hide many low-level storage
details. Most DBMS today are based on the relational data
model, which was proposed by E. F. Codd in 1970 [11,
12]. The relational data model is very simple and elegant,
and defines a database as a collection of one or more
relations, where each relation is a table with rows and
columns. DBMS based on the relational data model are
frequently called relational database management systems.
In the rest of the paper we will use the term DBMS to
refer to relational database management systems.
In practice, a typical database application (e.g.,
banking, insurance companies, telecommunications, etc) is
a client-server system (either a traditional client-server or
a three tier system) where a number of users are connected
1 This first paragraph is a condensed view of key definitions presented
in chapter 1 of the book “Database Management Systems”, by R.
Ramakrishnan, second edition, McGraw Hill, ISBN 0-07-232206-3.
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:16:58 UTC from IEEE Xplore.  Restrictions apply. 
to a database server via a terminal or a desktop computer
(today the trend is to access database servers through the
internet using a browser). The user’s actions are translated
into SQL commands (Structured Query Language: the
relational language used by DBMS [13]) by the client
application and sent to the database server. The results are
sent back to the client to be displayed in the adequate
format by the client application.
is
an
Transaction management
A very important notion in DBMS is the concept of
transaction [14]. In a simplified view, a transaction is a set
of commands that perform a given action and take the
database from a consistent state to another consistent
state.
important
functionality of modern DBMS and it is directly related to
dependability aspects, particularly in what concerns
concurrency control and recovery. Concurrency control is
the activity of coordinating the actions of processes that
operate in parallel and access shared data, and therefore
potentially interfere with each other. Recovery assures
that faults (either hardware, software, or operator faults)
do not corrupt persistent data stored in the database tables.
In order to correctly deal with concurrency control and
recovery, DBMS transactions must fulfill the following
properties: atomicity (either all actions in the transaction
are executed or none are), consistency (execution of
transaction results in consistent database states), isolation
(the effects of a transaction must be understood without
considering other concurrently executing transactions),
and durability (the effects of a transaction that has been
successfully completed must persist, even when the
system has a failure after the transaction is finished).
These properties are known as the ACID properties.
represents all
The Oracle™ DBMS is one of the leading databases in
the market and as one of the most complete and complex
database it
the sophisticated relational
DBMS available today very well. For that reason we have
chosen the Oracle 8i DBMS as case study to show the
experimental approach proposed to characterize both the
performance and the recoverability in DBMS. In the
remaining of this section we briefly describe the key featu-
res of the Oracle 8i DBMS, with particular emphasis in
the recovery and performance tuning and administration.
2.1. Oracle DBMS
An Oracle server consists of an Oracle database and
one or more Oracle server instances [15]. An Oracle
database has logical structures and physical structures.
Because physical and logical structures are separate, the
physical data storage can be managed without affecting
the logical structures. The main physical structures are the
control files, the data files, the redo log files, and the
archive log files. The following point summarizes the role
of each of these physical structures:
- Control files: contain all the basic and vital information
about the database, such as the physical location of the
other files, configuration parameters, etc.
- Data files: are the files where the data (user data,
metadata, or any other type of data) is stored. An
Oracle database can have as many data files as
necessary to fulfill the application needs.
- Online redo log files: group of files (in a minimum of
two) used to record the redo log entries, which are
used during database recovery. These files work in a
circular way and when all the redo log files are full the
Oracle continues writing and overwrites the previous
contents of the files (reuse). Given the importance of
the redo log information, the redo log files can be
replicated to assure correct recovery even when a redo
log file is lost.
- Archive log files: these files store the redo entries in a
permanent way (typically in a tertiary storage device).
The archive log files are optional and, when active,
avoid the loss of redo log information due to the
circular use of the online redo log files.
The user space (i.e.,
the space used to store user
objects such as tables, indexes, etc) is available through a
set of logical structures: tablespaces, segments, extents,
and data blocks. The tablespace is a logical area that is
physically composed by one or more data files. Users
receive quotas in tablespaces and an Oracle database can
have as many tablespaces as needed for a good
administration of the storage space. Any data object in
Oracle (table, index, etc) is associated to one segment and
segments acquire space from a tablespace through units
called extents. Finally, each extent is composed by a given
number of data blocks, which is the basic storage unit.
The combination of the background processes and
memory buffers is called an Oracle instance. Every time
an instance is started, a system global area (SGA) is
allocated (area of memory used for database information)
and Oracle background processes are started.
An Oracle instance has two types of processes: user
processes and Oracle processes. A user process executes
the code or commands from application programs. The
Oracle processes include server processes that perform
work for the user processes and background processes that
perform maintenance work for the Oracle server. Some of
the database writer
the most
(DBWR),
(LGWR),
the checkpoint
(CKPT), and the archive writer (ARCH).
important processes are:
the log writer
During the normal database operation, all the activities
are stored as entries in the online redo log files. The
entries are cached in the redo log buffer and regularly
saved into the log files by the LGWR process (actually,
the redo log buffer is written into the log files whenever a
user transaction commits). Another important goal of the
LGWR is to record the activities as frequently as possible,
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:16:58 UTC from IEEE Xplore.  Restrictions apply. 
allowing the DBWR to delay its data writing operations. It
is worth noting that typically a redo log entry is much
smaller than the corresponding data operation described in
the log entry, which means that it is much faster to write
the log entries into the disk than the corresponding data. If
the log archive option is on,
the archive log process
(ARCH) copies online redo log files to a tertiary storage
device once a given online redo log file becomes full.
phase,
in which
Oracle performs periodical checkpoints, which repre-
sent consistent states of the database from which it is
possible to start recovery. The recovery consists of a
forward phase, in which the redo entries recorded in the
redo log are applied to regenerate the data, followed by a
backward
unrecoverable
transactions are rolled back to bring the database to a
clean state. To reduce downtime due faults, Oracle uses a
basic process pair mechanism. It consists of a stand-by
database running in a different machine that is kept in a
permanent recovery process by executing the redo logs of
the primary database. This way, if the primary database
goes down the stand-by database can replace it and resu-
me the work very quickly, greatly reducing the down time.
some
2.2. Oracle performance and recovery tuning
The performance and recoverability of an Oracle
database depend on many system elements, that can be
summarized as follows: memory and processes, physical
storage, database objects, SQL commands, and recovery
mechanisms. Obviously, the logging and checkpointing
activities necessary to recover from faults also cause some
performance degradation, which is very dependent on
specific tuning options. Furthermore, several components
of the underlying platform (e.g., hardware, operating
system, and network) can also impact the performance and
recovery, but these aspects are clearly outside DBMS
performance and recovery tuning. The following points
summarize the key aspects of Oracle 8i performance and
recovery tuning and give an idea of the huge complexity
of
tuning a large database installation. As general
reference for database concepts and tuning see [10] and
for a more specific reference concerning tuning of
Oracle 8i see [15].
- Memory and processes: the goal is to optimize the I/O
operations through the definition of the optimal size of
the different memory areas in the SGA and the cache
and buffer policies. Additionally,
is possible to
optimize the number of some processes such as the
DBWR or LGWR.
it
- Physical storage: the goal is to minimize contention
when accessing database files and fragmentation in the
physical storage of database objects. Many aspects
affect the physical storage, such as the distribution of
the database files by the available disks, and the
numerous parameters used to configure the space
allocation to database objects through extents and data
blocks.
-
- Recovery mechanisms:
- Database objects: the definition of database objects
the
also impacts the performance. For example,
normalization of the tables, the creation of adequate
indexes, clustering or partitioning of some objects, etc.
SQL commands: in addition to the DBMS parameters
related to the query optimization and SQL execution,
the transaction design (i.e., set of SQL commands)
also affects performance. In this last case, the goal is
to minimize the performance impact of concurrency
control by avoiding transaction contention due to
object blocking. The optimization of SQL query
execution is mainly related to the policy used by the
DBMS query optimization module (cost or rule based).
the tuning of the recovery
mechanism is clearly the most difficult part when a
well-balanced tuning is desired. The goal
is to
conciliate aspects such as the minimization of recovery
time and the impact of
lost
transactions with maximum performance. Some
examples
recovery
mechanisms are size and number of online redo log
files, archive log option, checkpointing policy, and
stand-by database.
It is worth noting that database tuning must also take
into account the interdependencies among most of the
elements mentioned above. In practice,
in a complex
DBMS such as Oracle 8i the performance and recovery
tuning may require the definition of thousands of parame-
ters, which clearly show the difficulties faced by database
administrators in handling this problem. This emphasizes
the need for comprehensive benchmarks that include both
performance and recoverability measures. In this sense,
the present work is a first step towards this direction.
faults in terms of
parameters
related
of
to
3. Operator Faults in DBMS
both
performance
to measure
the introduction of
Our proposal of extending the TPC-C performance
and
benchmark
two new
recoverability requires
recoverability measures and faultload. The
elements:
representativeness of these new elements is, obviously, of
utmost importance. Concerning the faultload the challenge
is to define a representative set of faults and find practical
and consistent ways to introduce these faults in the system
under test (to make it possible to repeat the experiments or
to port them to other systems). In the same way we have
concentrated on measures of recovery time and integrity
violations,
dependability
measures (because these measures are the most relevant to
databases and are directly related to database availability
and data integrity), we also decided to focus on operator
of more
general
instead
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:16:58 UTC from IEEE Xplore.  Restrictions apply. 
faults, as these faults are unanimously considered by the
database community as responsible for most of
the
failures in database systems.
Studies on the underlying causes of database failures in
the field are not generally available to the public, as the
organizations prefer not to disclose this information. Most
of the published studies are not directly focused on
database systems. Nevertheless, published studies clearly
point out software faults and operator faults as the most
frequent causes for computer failures [16, 17, 18, 19, 20].
Furthermore,
database
administration tasks
tuning and
administration in a daily basis, clearly explain why
operator faults (i.e., wrong database administrator actions)
are prevalent in database systems. Several interviews with
database administrators of real databases installations
conducted on behalf of this work to define a classification
for operator faults have also confirm the prevalence of
operator faults in database systems.
and the need of
complexity
great
the
of
Although software faults are considered an important
source of failures, the emulation of software faults is still a
research issue and there are no practical tools readily
available to inject this kind of faults [21, 22]. Thus, we
decided to address only operator faults in this first study.
errors
Operator
faults in database systems are database
not
administrator mistakes. End-user
considered, as the end-user actions do not affect directly
the dependability of DBMS. In fact, the end-users do not
have direct access to the DBMS (e.g., do not execute SQL
commands even in user accounts), and they are only
allowed to use the database through well-defined interface