
VM 21



Figure 1: Memory Address to Cache Line Mappings for L1 and L2 Caches
allows for the L1 cache to be ﬁnely divided, such as 32
regions in Percival’s cache channel [16].
However, on today’s production virtualization sys-
tems, hyper-threading is commonly disabled for security
reasons (i.e., eliminating hyper-threading induced covert
channels). Therefore, the sender and receiver could only
communicate by interleaving their executions. Since the
L1 cache is completely ﬂushed at context switches, only
those higher level caches (e.g., the L2 cache) whose con-
tents are preserved across a context switch can be lever-
aged for classic cache channel transmission. Unlike the
L1 cache, the storage in these higher level caches is
not addressed purely by virtual memory addresses, but
either by physical memory addresses (PIPT, physically
indexed, physically tagged), or by a mixture of virtual
and physical memory addresses (VIPT, virtually indexed,
physically tagged). With physical memory addresses in-
volved in cache line addressing, given only knowledge of
its virtual address space, a process cannot be completely
certain of the cache line a memory access would affect
due to address translation.
Server virtualization has further complicated the ad-
dressing uncertainty by adding another layer of indirec-
tion to memory addressing. As illustrated in Figure 1(b),
the “physical memory” of a guest VM is still virtualized,
and access to it must be further translated. As a result, it
is very difﬁcult, if not impossible, for a process in a guest
VM (especially for a full virtualization VM) to discover
the actual physical memory addresses of a memory re-
gion. Due to the addressing uncertainty, for classic covert
channels on virtualized systems, the number of cache re-
gions is reduced to a minimum of only two [18, 30].
3.2 Scheduling Uncertainty
Classic cache channel data transmission depends on a
cache pattern “round-trip”—the receiver completely re-
sets the cache and correctly passes it to the sender; and
the sender completely prepares the cache pattern and cor-
rectly passes it back to the receiver. Therefore, to suc-
cessfully transmit one cache pattern, the sender and re-
ceiver must be strictly round-robin scheduled.
However, without special scheduling arrangements
(i.e., collusion) from the hypervisor, such idealistic
scheduling rarely happens. On production virtualized
systems, the physical processors are usually oversub-
scribed in order to increase utilization. In other words,
each physical processing core serves more than one vir-
tual processor from different VMs. As a result, there exist
many scheduling patterns that prevent successful cache
pattern “round-trip”, such as:
∗ Channel not cleared for send: The receiver is de-
scheduled before it ﬁnishes resetting the cache.
∗ Channel invalidated for send: The receiver ﬁnishes
resetting the cache, but another unrelated VM is
scheduled to run immediately after.
∗ Sending incomplete: The sender is de-scheduled be-
fore it ﬁnishes preparing the cache.
∗ Symbol destroyed: The sender ﬁnishes preparing the
cache, but another unrelated VM is scheduled to run
immediately after.
∗ Receiving incomplete: The receiver is de-scheduled
before it ﬁnishes reading the cache.
∗ Channel access collision: The sender and receiver
are executed in parallel on processor cores that share
the L2 cache.
Xu et al. [30] have clearly illustrated the problem of
scheduling uncertainty in two of their measurements.
First, in a laboratory setup, the error rate of their covert
channel increases from near 1% to 20–30% after adding
just one non-participating VM with moderate workload.
Second, in the Amazon EC2 cloud, they have discov-
ered that only 10.5% of the cache measurements at the
receiver side are valid for data transmission, due to the
fact that the hypervisor’s scheduling is different from the
idealistic scheduling.
Algorithm 2 Timing-based Cache Channel Protocol
CLines: Several sets of associative cache lines picked by both the sender and the receiver;
CLines: These cache lines can be put in one of two states, cached or ﬂushed.
DSend[N], DReceive[N]: N bit data to transmit and receive, respectively.
Sender Operations:
for i := 0 to N − 1 do
if DSend[i] = 1 then
Receiver Operations:
for i := 0 to N − 1 do
for an amount of time do
for an amount of time do
Timed access memory maps to CLines;
{Put CLines into the ﬂushed state}
Access memory maps to CLines;
end for
else
{Leave CLines in the cached state}
Sleep of an amount of time;
end if
end for
end for
{Detect the state of CLines by latency}
if Mean(AccessTime) > T hreshold then
DReceive[i] := 1; {CLines is ﬂushed}
else
DReceive[i] := 0; {CLines is cached}
end if
end for
3.3 Cache Physical Limitation
Besides the two uncertainties, classic cache channels also
face an insurmountable limitation—the necessity of a
shared and stable cache.
If the sender and receiver of classic cache channels are
executed on processor cores that do not share any cache,
obviously no communication could be established. On a
multi-processor system, it is quite common to have pro-
cessor cores that do not share any cache, since there is
usually no shared cache between different physical pro-
cessors. And sometimes even processor cores residing on
the same physical processor do not share any cache, such
as an Intel Core2 Quad processor, which contains two
dual-core silicon packages with no shared cache in be-
tween.
Even if the sender and receiver could share a cache,
external interferences can make the cache unstable. Mod-
ern multi-core processors often include a large last-level
cache (LLC) shared between all processor cores. To fa-
cilitate a simpler cache coherence protocol, the LLC usu-
ally employs an inclusive principle, which requires that
all data contained in the lower level caches must also
exist in the LLC. In other words, when a cache line
is evicted from the LLC, it must also be evicted from
all the lower level caches. Thus, any non-participating
processes executing on those processor cores that share
the LLC with the sender and receiver can interfere with
the communication by indirectly evicting the data in the
cache used for the covert channel. The more cores on a
processor, the higher the interference.
Overall, virtualization induced changes to cache oper-
ations and process scheduling render the data transmis-
sion scheme of classic cache channels obsolete. First, the
effectiveness of data modulation is severely reduced by
addressing uncertainty. Second, the critical procedures of
signal generation, delivery, and detection are frequently
interrupted by less-than-ideal scheduling patterns. And
ﬁnally, the fundamental requirement of stably shared
cache is hard to satisfy as processors are having more
cores.
4 Covert Channel in the Hyper-space
In this section, we present our techniques to tackle the ex-
isting difﬁculties and develop a high-bandwidth, reliable
covert channel on virtualized x86 systems. At ﬁrst, we
describe our redesigned, pure timing-based data trans-
mission scheme, which overcomes the negative effects of
addressing and scheduling uncertainties with a simpliﬁed
design. After that, we detail our ﬁndings of a powerful
covert channel medium, exploiting the atomic instruc-
tions and their induced cache–memory bus interactions
on x86 platforms. And ﬁnally, we specify our designs of
a high error-tolerance transmission protocol for cross–
VM covert channels.
4.1 A Stitch In Time
We ﬁrst question the reasoning behind using cache state
patterns for data modulation. Originally, Percival [16]
designed this transmission scheme mainly for the use
of side channel cryptographic key stealing on a hyper-
threaded processor. In this speciﬁc usage context, the
critical information of memory access patterns are re-
ﬂected by the states of cache regions. Therefore, cache
region-based data modulation is an important source of
information. However, in a virtualized environment, the
regions of the cache no longer carry useful informa-
tion due to addressing uncertainty, making cache region-
based data modulation a great source of interference.
We therefore redesign a data transmission scheme for
the virtualized environment. Instead of using the cache
ϯϱϬ
ϯϬϬ
ϮϱϬ
ϮϬϬ
ϭϱϬ
ϭϬϬ
ϱϬ
Ϭ
Ϳ
Ɛ
Ŷ
;

Ǉ
Đ
Ŷ
Ğ
ƚ
Ă
>

Ɛ
Ɛ
Ğ
Đ
Đ

Ϭ
ϱϬ
ϭϬϬ
ϭϱϬ
ϮϬϬ
^ĞƋƵĞŶƚŝĂů^ĂŵƉůĞƐKǀĞƌdŝŵĞ; ɑȌ


Figure 2: Timing-based Cache Channel Bandwidth Test

region-based encoding scheme, we modulate the data
based on the state of cache lines over time, resulting in a
pure timing-based transmission protocol, as described in
Algorithm 2.
Besides removing cache region-based data modula-
tion, the new transmission scheme also features a signif-
icant change in the scheduling requirement, i.e., signal
generation and detection are performed instantaneously,
instead of being interleaved. In other words, data are
transmitted while the sender and receiver run in parallel.
This requirement is more lenient than strict round-robin
scheduling, especially with the trend of increasing num-
ber of cores on a physical processor, making two VMs
more likely to run in parallel than interleaved.
We conduct a simple raw bandwidth estimation exper-
iment to demonstrate the effectiveness of the new cache
covert channel. In this experiment, interleaved bits of ze-
ros and ones are transmitted, and the raw bandwidth of
the channel can thus be estimated by manually counting
the number of bits transmitted over a period of time.
We build the cache covert channel on an Intel Core2
system with two processor cores sharing a 2 MB 8-way
set-associative L2 cache. Using a simple proﬁling test,
accessing a random3 sequence of memory addresses sep-
arated by multiples of 256KB, we observe that these
memory addresses can be mapped to up to 64 cache
lines. Therefore, we select CLines as a set of 64 cache
lines mapped by memory addresses following the pattern
M +X ·256K, where M is a small constant and X is a ran-
dom positive integer. The sender puts these cache lines
into the ﬂushed state by accessing a sequence of CLines-
mapping memory addresses. The receiver times the ac-
cess latency of another sequence of CLines-mapping
memory addresses. The length of the receivers access se-
quence should be smaller than, but not too far away from
the cache line set size, for example, 48.
As shown in Figure 2, the x-value of each sample
point is the observed memory access latency by the re-
ceiver, and the trend line is created by plotting the mov-
3The randomness is introduced to avoid the interference of hard-
ware prefetching.
ing average of two samples. According to the measure-
ment results, 39 bits can be transmitted over a period
of 200 micro-seconds, yielding a raw bandwidth of over
190.4 kilobits per second, about ﬁve orders of magni-
tude higher than the previously studied cross–VM cache
covert channels.
Having resolved the negative effects of addressing and
scheduling uncertainties and achieved a high raw band-
width, our new cache covert channel, however, still per-
forms poorly on the system with non-participating work-
loads. We discover that the sender and receiver have dif-
ﬁculty in establishing a stable communication channel.
And the cause of instability is that the hypervisor fre-
quently migrates the virtual processors across physical
processor cores, which is also observed by Xu et al.
[30]. The outgrowth of this behavior is that the sender
and receiver frequently reside on processor cores that do
not share any cache, making our cache channel run into
the insurmountable cache physical limitation just like the
classic cache channels.
4.2 Aria on the B-String
The prevalence of virtual processor core migration hand-
icaps cache channels in cross–VM covert communica-
tion. In order to reliably establish covert channels across
processor cores that do not share any cache, a commonly
shared and exploitable resource is needed as the commu-
nication medium. And the memory bus comes into our
sight as we extend our scope beyond the processor cache.
4.2.1 Background
Interconnecting the processors and the system main
memory, the memory bus is responsible for delivering
data between these components. Because contention on
the memory bus results in a system-wide observable ef-
fect of increased memory access latency, a covert chan-
nel can be created by programmatically triggering con-
tention on the memory bus. Such a covert channel is
called a bus-contention channel.
The bus contention channels have long been studied
as a potential security threat for virtual machines on the
VAX VMM, on which a number of techniques have been
developed [6–8] to effectively mitigate this threat. How-
ever, the x86 platforms we use today are signiﬁcantly
different from the VAX systems, and we suspect similar
exploits can be found by probing previously unexplored
techniques. Unsurprisingly, by carefully examining the
memory related operations of the x86 platform, we have
discovered a bus-contention exploit using atomic instruc-
tions with exotic operands.
Atomic instructions are special x86 memory manipu-
lation instructions, designed to facilitate multi-processor
Algorithm 3 Timing-based Memory Bus Channel Protocol
MExotic: An exotic conﬁguration of a memory region that spans two cache lines.
DSend[N], DRecv[N]: N bit data to transmit and receive, respectively.
Sender Operations:
for i := 0 to N − 1 do
if DSend[i] = 1 then
Receiver Operations:
for i := 0 to N − 1 do
for an amount of time do
for an amount of time do
Timed uncached memory access;
{Put memory bus into contended state}
Perform atomic operation with MExotic;
end for
else
{Leave memory bus in contention-free state}
Sleep of an amount of time;
end if
end for
end for
{Detect the state of memory bus by latency}
if Mean(AccessTime) > T hreshold then
DRecv[i] := 1; {Bus is contended}
else
DRecv[i] := 0; {Bus is contention-free}
end if
end for
synchronization, such as implementing mutexes and
semaphores—the fundamental building blocks for par-