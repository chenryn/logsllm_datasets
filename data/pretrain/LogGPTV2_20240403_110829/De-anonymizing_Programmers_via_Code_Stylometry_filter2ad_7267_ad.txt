ﬁles. We used a random forest and 9-fold cross validation
to classify two programmers’ source code. The average
classiﬁcation accuracy using CSFS set is 100.00% and
100.00% with the information gain features.
4.2.4 Two-class/One-class Open World Task
Another two-class machine learning task can be formu-
lated for authorship veriﬁcation. We suspect Mallory of
plagiarizing, so we mix in some code of hers with a large
sample of other people, test, and see if the disputed code
gets classiﬁed as hers or someone else’s. If it gets clas-
siﬁed as hers, then it was with high probability really
written by her.
If it is classiﬁed as someone else’s, it
really was someone else’s code. This could be an open
262  24th USENIX Security Symposium 
USENIX Association
USENIX Association  
24th USENIX Security Symposium  263
worldproblemandthepersonthatoriginallywrotethecodecouldbeapreviouslyunknownprogrammer.Thisisatwo-classproblemwithclassesMalloryandothers.WetrainonMallory’ssolutionstoproblemsa,b,c,d,e,f,g,h.WealsotrainonprogrammerA’ssolu-tiontoproblema,programmerB’ssolutiontoproblemb,programmerC’ssolutiontoproblemc,programmerD’ssolutiontoproblemd,programmerE’ssolutiontoprob-leme,programmerF’ssolutiontoproblemf,program-merG’ssolutiontoproblemg,programmerH’ssolutiontoproblemhandputtheminoneclasscalledABCDE-FGH.Wetrainarandomforestclassiﬁerwith300treesonclassesMalloryandABCDEFGH.Wehave6testin-stancesfromMalloryand6testinstancesfromanotherprogrammerZZZZZZ,whoisnotinthetrainingset.Theseexperimentshavebeenrepeatedintheex-actsamesettingwith80differentsetsofprogrammersABCDEFGH,ZZZZZZandMallorys.Theaverageclas-siﬁcationaccuracyforMalloryusingtheCSFSsetis100.00%.ZZZZZZ’stestinstancesareclassiﬁedaspro-grammerABCDEFGH82.04%ofthetime,andclassi-ﬁedasMalloryfortherestofthetimewhileusingtheCSFS.Dependingontheamountoffalsepositiveswearewillingtoaccept,wecanchangetheoperatingpointontheROCcurve.Theseresultsarealsopromisingforuseincaseswhereapieceofcodeissuspectedtobeplagiarized.Followingthesameapproach,iftheclassiﬁcationresultofthepieceofcodeissomeoneotherthanMallory,thatpieceofcodewaswithveryhighprobabilitynotwrittenbyMallory.4.3AdditionalInsights4.3.1ScalingWecollectedalargerdatasetof1,600programmersfromvariousyears.Eachoftheprogrammershad9sourcecodesamples.Wecreated7subsetsofthislargedatasetindifferingsizes,with250,500,750,1,000,1,250,1,500,and1,600programmers.Thesesubsetsareuse-fultounderstandhowwellourapproachscales.Weex-tractedthespeciﬁcfeaturesthathadinformationgaininthemain250programmerdatasetfromthislargedataset.Intheory,weneedtousemoretreesintherandomfor-estasthenumberofclassesincreasetodecreasevari-ance,butweusedfewertreescomparedtosmallerex-periments.Weused300treesintherandomforesttoruntheexperimentsinareasonableamountoftimewithareasonableamountofmemory.Theaccuracydidnotdecreasetoomuchwhenincreasingthenumberofpro-grammers.Thisresultshowsthatinformationgainfea-turesarerobustagainstchangesinclassandareim-portantpropertiesofprogrammers’codingstyles.ThefollowingFigure3demonstrateshowwellourmethodscales.Weareabletode-anonymize1,600programmersusing32GBmemorywithinonehour.Alternately,wecanuse40treesandgetnearlythesameaccuracy(within0.5%)inafewminutes.Figure3:LargeScaleDe-anonymization4.3.2TrainingDataandFeaturesWeselecteddifferentsetsof62programmersthathadFsolutionﬁles,from2upto14.Eachdatasethastheso-lutionstothesamesetofFproblemsbydifferentsetsofprogrammers.EachdatasetconsistedofprogrammersthatwereabletosolveexactlyFproblems.Suchanex-perimentalsetupmakesitpossibletoinvestigatetheef-fectofprogrammerskillsetoncodingstyle.Thesizeofthedatasetswerelimitedto62,becausetherewereonly62contestantswith14ﬁles.Therewereafewcontes-tantswithupto19ﬁlesbutwehadtoexcludethemsincetherewerenotenoughprogrammerstocomparethem.ThesamesetofFproblemswereusedtoensurethatthecodingstyleoftheprogrammerisbeingclassiﬁedandnotthepropertiesofpossiblesolutionsoftheprob-lemitself.Wewereabletocapturepersonalprogram-mingstylesincealltheprogrammersarecodingthesamefunctionalityintheirownways.StratiﬁedF-foldcrossvalidationwasusedbytrainingoneveryone’s(F−1)solutionsandtestingontheFthproblemthatdidnotappearinthetrainingset.Asare-sult,theproblemsinthetestﬁleswereencounteredfortheﬁrsttimebytheclassiﬁer.Weusedarandomforestwith300treesand(logM)+1featureswithF-foldstratiﬁedcrossvalidation,ﬁrstwiththeCodeStylometryFeatureSet(CSFS)andthenwiththeCSFS’sfeaturesthathadinformationgain.Figure4showstheaccuracyfrom13differentsetsof62programmerswith2to14solutionﬁles,andconse-quently1to13trainingﬁles.TheCSFSreachesanopti-maltrainingsetsizeat9solutionﬁles,wheretheclassi-ﬁertrainson8(F−1)solutions.Inthedatasetsweconstructed,asthenumberofﬁlesincreaseandproblemsfrommoreadvancedroundsareincluded,theaveragelineofcode(LOC)perﬁlealsoincreases.Theaveragelinesofcodepersourcecodeinthedatasetis70.Increasednumberoflinesofcodemighthaveapositiveeffectontheaccuracybutatthesametimeitrevealsprogrammer’schoiceofprogram9264  24th USENIX Security Symposium 
USENIX Association
Figure4:TrainingDatalengthinimplementingthesamefunctionality.Ontheotherhand,theaveragelineofcodeofthe7easier(76LOC)ordifﬁcultproblems(83LOC)takenfromcontes-tantsthatwereabletocomplete14problems,ishigherthantheaveragelineofcode(68)ofcontestantsthatwereabletosolveonly7problems.ThisshowsthatprogrammerswithbetterskillstendtowritelongercodetosolveGoogleCodeJamproblems.Themainstreamideaisthatbetterprogrammerswriteshorterandcleanercodewhichcontradictswithlineofcodestatisticsinourdatasets.GoogleCodeJamcontestantsaresupposedtooptimizetheircodetoprocesslargeinputswithfasterperformance.Thisimplementationstrategymightbeleadingtoadvancedprogrammersimplementinglongersolutionsforthesakeofoptimization.Wetookthedatasetwith62programmerseachwith9solutions.Weget97.67%accuracywithallthefea-turesand99.28%accuracywiththeinformationgainfea-tures.Weexcludedallthesyntacticfeaturesandtheac-curacydroppedto88.89%withallthenon-syntacticfea-turesand88.35%withtheinformationgainfeaturesofthenon-syntacticfeatureset.Werananotherexperimentusingonlythesyntacticfeaturesandobtained96.06%withallthesyntacticfeaturesand96.96%withtheinfor-mationgainfeaturesofthesyntacticfeatureset.Mostoftheclassiﬁcationpowerispreservedwiththesyntac-ticfeatures,andusingnon-syntacticfeaturesleadstoasigniﬁcantdeclineinaccuracy.4.3.3ObfuscationWetookadatasetwith9solutionﬁlesand20program-mersandobfuscatedthecodeusinganoff-the-shelfC++obfuscatorcalledstunnix[3].Theaccuracywiththein-formationgaincodestylometryfeaturesetontheob-fuscateddatasetis98.89%.Theaccuracyonthesamedatasetwhenthecodeisnotobfuscatedis100.00%.Theobfuscatorrefactoredfunctionandvariablenames,aswellascomments,andstrippedallthespaces,preserv-ingthefunctionalityofcodewithoutchangingthestruc-tureoftheprogram.Obfuscatingthedataproducedlittledetectablechangeintheperformanceoftheclassiﬁerforthissample.TheresultsaresummarizedinTable6.Wetookthemaximumnumberofprogrammers,20,thathadsolutionsto9problemsinCandobfuscatedthecode(seeexampleinAppendixB)usingamuchmoresophisticatedopensourceobfuscatorcalledTigress[1].Inparticular,Tigressimplementsfunctionvirtualiza-tion,anobfuscationtechniquethatturnsfunctionsintointerpretersandconvertstheoriginalprogramintocor-respondingbytecode.Afterapplyingfunctionvirtual-ization,wewerelessabletoeffectivelyde-anonymizeprogrammers,soithaspotentialasacountermeasuretoprogrammerde-anonymization.However,thisobfusca-tioncomesatacost.Firstofall,theobfuscatedcodeisneitherreadablenormaintainable,andisthusunsuitableforanopensourceproject.Second,theobfuscationaddssigniﬁcantoverhead(9timesslower)totheruntimeoftheprogram,whichisanotherdisadvantage.Theaccuracywiththeinformationgainfeaturesetontheobfuscateddatasetisreducedto67.22%.WhenwelimitthefeaturesettoASTnodebigrams,weget18.89%accuracy,whichdemonstratestheneedforallfeaturetypesincertainscenarios.Theaccuracyonthesamedatasetwhenthecodeisnotobfuscatedis95.91%.ObfuscatorProgrammersLangResultsw/oObfuscationResultsw/ObfuscationStunnix20C++98.89%100.00%Stunnix20C++98.89*%98.89*%Tigress20C93.65%58.33%Tigress20C95.91*%67.22*%*InformationgainfeaturesTable6:EffectofObfuscationonDe-anonymization4.3.4RelaxedClassiﬁcationThegoalhereistodeterminewhetheritispossibletore-ducethenumberofsuspectsusingcodestylometry.Re-ducingthesetofsuspectsinchallengingcases,suchashavingtoomanysuspects,wouldreducetheeffortre-quiredtomanuallyﬁndtheactualprogrammerofthecode.Inthissection,weperformedclassiﬁcationonthemain250programmerdatasetfrom2014usingthein-formationgainfeatures.TheclassiﬁcationwasrelaxedtoasetoftopRsuspectsinsteadofexactclassiﬁcationoftheprogrammer.TherelaxedfactorRvariedfrom1to10.Insteadoftakingthehighestmajorityvoteofthedecisionstreesintherandomforest,thehighestRmajor-ityvotedecisionsweretakenandtheclassiﬁcationresultwasconsideredcorrectiftheprogrammerwasinthesetoftopRhighestvotedclasses.Theaccuracydoesnotimprovemuchaftertherelaxedfactorislargerthan5.10USENIX Association  
24th USENIX Security Symposium  265
Figure5:RelaxedClassiﬁcationwith250Programmers4.3.5GeneralizingtheMethodFeaturesderivedfromASTscanrepresentcodingstylesinvariouslanguages.Thesefeaturesareapplicableincaseswhenlexicalandlayoutfeaturesmaybelessdis-criminatingduetoformattingstandardsandrelianceonwhitespaceandother‘lexical’featuresassyntax,suchasPython’sPEP8formatting.Toshowthatourmethodgeneralizes,wecollectedsourcecodeof229Pythonpro-grammersfromGCJ’s2014competition.229program-mershadexactly9solutions.UsingonlythePythonequivalentsofsyntacticfeatureslistedinTable4and9-foldcross-validation,theaverageaccuracyis53.91%fortop-1classiﬁcation,75.69%fortop-5relaxedattri-bution.Thelargestsetofprogrammerstoallworkonthesamesetof9problemswas23programmers.Theaverageaccuracyinidentifyingthese23programmersis87.93%fortop-1and99.52%fortop-5relaxedattribu-tion.ThesameclassiﬁcationtasksusingtheinformationgainfeaturesarealsolistedinTable7.Theoverallac-curacyindatasetscomposedofPythoncodearelowerthanC++datasets.InPythondatasets,weonlyusedsyntacticfeaturesfromASTsthatweregeneratedbyaparserthatwasnotfuzzy.Thelackofquantityandspeci-ﬁcityoffeaturesaccountsforthedecreasedaccuracy.ThePythondataset’sinformationgainfeaturesaresig-niﬁcantlyfewerinquantity,comparedtoC++dataset’sinformationgainfeatures.Informationgainonlykeepsfeaturesthathavediscriminativevalueallontheirown.Iftwofeaturesonlyprovidediscriminativevaluewhenusedtogether,theninformationgainwilldiscardthem.SoifalotofthefeaturesforthePythonsetareonlyjointlydiscriminative(andnotindividuallydiscrimina-tive),thentheinformationgaincriteriamayberemovingfeaturesthatincombinationcouldeffectivelydiscrimi-natebetweenauthors.Thismightaccountforthede-creasewhenusinginformationgainfeatures.WhileinthecontextofotherresultsinthispapertheresultsinTa-ble7appearlackluster,itisworthnotingthateventhispreliminarytestusingonlysyntacticfeatureshascompa-rableperformancetootherpriorworkatasimilarscale(seeSection6andTable9),demonstratingtheutilityofsyntacticfeaturesandtherelativeeaseofgeneratingthemfornovelprogramminglanguages.Nevertheless,aCSFSequivalentfeaturesetcanbegeneratedforotherprogramminglanguagesbyimplementingthelayoutandlexicalfeaturesaswellasusingafuzzyparser.Lang.ProgrammersClassiﬁcationIGTop-5Top-5IGPython2387.93%79.71%99.52%96.62Python22953.91%39.16%75.69%55.46Table7:GeneralizingtoOtherProgrammingLanguages4.3.6SoftwareEngineeringInsightsWewantedtoinvestigateifprogrammingstyleisconsis-tentthroughoutyears.Wefoundthecontestantsthathadthesameusernameandcountryinformationbothin2012and2014.Weassumedthatthesearethesamepeoplebutthereisachancethattheymightbedifferentpeople.In2014,someoneelsemighthavepickedupthesameuser-namefromthesamecountryandstartedusingit.Wearegoingtoignoresuchagroundtruthproblemfornowandassumethattheyarethesamepeople.Wetookasetof25programmersfrom2012thatwerealsocontestantsin2014’scompetition.Wetook8ﬁlesfromtheirsubmissionsin2012andtrainedarandomfor-estclassiﬁerwith300treesusingCSFS.Wehadonein-stancefromeachoneofthecontestantsfrom2014.Thecorrectclassiﬁcationofthesetestinstancesfrom2014is96.00%.Theaccuracydroppedto92.00%whenusingonlyinformationgainfeatures,whichmightbeduetotheaggressiveeliminationofpairsoffeaturesthatarejointlydiscriminative.These25programmers’9ﬁlesfrom2014hadacorrectclassiﬁcationaccuracyof98.04%.Theseresultsindicatethatcodingstyleispreserveduptosomedegreethroughoutyears.Toinvestigateproblemdifﬁculty’seffectoncodingstyle,wecreatedtwodatasetsfrom62programmersthathadexactly14solutionﬁles.Table8summarizesthefollowingresults.Adatasetwith7oftheeasierprob-lemsoutof14resultedin95.62%accuracy.Adatasetwith7ofthemoredifﬁcultproblemsoutof14resultedin99.31%accuracy.Thismightimplythatmoredifﬁcultcodingtaskshaveamoreprevalentreﬂectionofcodingstyle.Ontheotherhand,thedatasetthathad62pro-grammerswithexactly7oftheeasierproblemsresultedin91.24%accuracy,whichisalotlowerthantheaccu-racyobtainedfromthedatasetwhoseprogrammerswereabletoadvancetosolve14problems.Thismightindi-catethat,programmerswhoareadvancedenoughtoan-swer14problemslikelyhavemoreuniquecodingstylescomparedtocontestantsthatwereonlyabletosolvetheﬁrst7problems.Toinvestigatethepossibilitythatcontestantswhoareabletoadvancefurtherintheroundshavemoreuniquecodingstyles,weperformedasecondroundofexperi-mentsoncomparabledatasets.Wetookthedatasetwith1112 solution ﬁles and 62 programmers. A dataset with 6
of the easier problems out of 12 resulted in 91.39% ac-
curacy. A dataset with 6 of the more difﬁcult problems
out of 12 resulted in 94.35% accuracy. These results are
higher than the dataset whose programmers were only
able to solve the easier 6 problems. The dataset that had
62 programmers with exactly 6 of the easier problems
resulted in 90.05% accuracy.
A = #programmers, F = max #problems completed
N = #problems included in dataset (N ≤ F)
A = 62
F = 14
N = 7
N = 7
F = 7
N = 7
F = 12
N = 6
N = 6
F = 6
N = 6
Average accuracy after 10 iterations while using CSFS
99.31%
95.62%2
91.24%1
94.35%
91.39%2
90.05%1
Average accuracy after 10 iterations while using IG CSFS
98.62%2
96.77%1
99.38%
1 Drop in accuracy due to programmer skill set.
2 Coding style is more distinct in more difﬁcult tasks.
96.69%
95.43%2
94.89%1
Table 8: Effect of Problem Difﬁculty on Coding Style
5 Discussion
In this section, we discuss the conclusions we draw from
the experiments outlined in the previous section, limita-
tions, as well as questions raised by our results. In par-
ticular, we discuss the difﬁculty of the different settings
considered, the effects of obfuscation, and limitations of
our current approach.
Problem Difﬁculty. The experiment with random
problems from random authors among seven years most
closely resembles a real world scenario. In such an ex-
perimental setting, there is a chance that instead of only
identifying authors we are also identifying the properties
of a speciﬁc problem’s solution, which results in a boost
in accuracy.
In contrast, our main experimental setting where all
authors have only answered the nine easiest problems is
possibly the hardest scenario, since we are training on the
same set of eight problems that all the authors have algo-
rithmically solved and try to identify the authors from
the test instances that are all solutions of the 9th prob-
lem. On the upside, these test instances help us precisely
capture the differences between individual coding style
that represent the same functionality. We also see that
such a scenario is harder since the randomized dataset
has higher accuracy.
Classifying authors that have implemented the solu-
tion to a set of difﬁcult problems is easier than identi-
fying authors with a set of easier problems. This shows
that coding style is reﬂected more through difﬁcult pro-
gramming tasks. This might indicate that programmers
come up with unique solutions and preserve their cod-
ing style more when problems get harder. On the other
hand, programmers with a better skill set have a prevalent
coding style which can be identiﬁed more easily com-
pared to contestants who were not able to advance as
far in the competition. This might indicate that as pro-
grammers become more advanced, they build a stronger
coding style compared to novices. There is another pos-
sibility that maybe better programmers start out with a
more unique coding style.
Effects of Obfuscation. A malware author or pla-
giarizing programmer might deliberately try to hide his
source code by obfuscation. Our experiments indicate
that our method is resistant to simple off-the-shelf obfus-
cators such as stunnix, that make code look cryptic while
preserving functionality. The reason for this success is
that the changes stunnix makes to the code have no effect
on syntactic features, e.g., removal of comments, chang-
ing of names, and stripping of whitespace.
In contrast, sophisticated obfuscation techniques such
as function virtualization hinder de-anonymization to
some degree, however, at
the cost of making code
unreadable and introducing a signiﬁcant performance
penalty. Unfortunately, unreadability of code is not ac-
ceptable for open-source projects, while it is no problem
for attackers interested in covering their tracks. Develop-
ing methods to automatically remove stylometric infor-
mation from source code without sacriﬁcing readability
is therefore a promising direction for future research.
Limitations. We have not considered the case where
a source ﬁle might be written by a different author than
the stated contestant, which is a ground truth problem
that we cannot control. Moreover, it is often the case that
code fragments are the work of multiple authors. We
plan to extend this work to study such datasets. To shed
light on the feasibility of classifying such code, we are
currently working with a dataset of git commits to open
source projects. Our parser works on code fragments
rather than complete code, consequently we believe this
analysis will be possible.
Another fundamental problem for machine learning
classiﬁers are mimicry attacks. For example, our clas-
siﬁer may be evaded by an adversary by adding extra
dummy code to a ﬁle that closely resembles that of an-
other programmer, albeit without affecting the program’s
behavior. This evasion is possible, but trivial to resolve
when an analysts veriﬁes the decision.
Finally, we cannot be sure whether the original au-
thor is actually a Google Code Jam contestant. In this
case, we can detect those by a classify and then verify
approach as explained in Stolerman et al.’s work [30].
Each classiﬁcation could go through a veriﬁcation step
266  24th USENIX Security Symposium 
USENIX Association
12
to eliminate instances where the classiﬁer’s conﬁdence is
below a threshold. After the veriﬁcation step, instances
that do not belong to the set of known authors can be
separated from the dataset to be excluded or for further
manual analysis.
6 Related Work
Our work is inspired by the research done on authorship
attribution of unstructured or semi-structured text [5, 22].
In this section, we discuss prior work on source code
authorship attribution. In general, such work (Table 9)
looks at smaller scale problems, does not use structural
features, and achieves lower accuracies than our work.
The highest accuracies in the related work are
achieved by Frantzeskou et al. [12, 14]. They used 1,500
7-grams to reach 97% accuracy with 30 programmers.
They investigated the high-level features that contribute
to source code authorship attribution in Java and Com-
mon Lisp. They determined the importance of each fea-
ture by iteratively excluding one of the features from the
feature set. They showed that comments, layout features
and naming patterns have a strong inﬂuence on the au-
thor classiﬁcation accuracy. They used more training
data (172 line of code on average) than us (70 lines of
code). We replicated their experiments on a 30 program-
mer subset of our C++ data set, with eleven ﬁles contain-
ing 70 lines of code on average and no comments. We
reach 76.67% accuracy with 6-grams, and 76.06% accu-
racy with 7-grams. When we used a 6 and 7-gram fea-
ture set on 250 programmers with 9 ﬁles, we got 63.42%
accuracy. With our original feature set, we get 98% ac-
curacy on 250 programmers.
The largest number of programmers studied in the re-
lated work was 46 programmers with 67.2% accuracy.
Ding and Samadzadeh [10] use statistical methods for
authorship attribution in Java. They show that among
lexical, keyword and layout properties, layout metrics
have a more important role than others which is not the
case in our analysis.
There are also a number of smaller scale, lower ac-
curacy approaches in the literature [9, 11, 18–21, 28],
shown in Table 9, all of which we signiﬁcantly outper-
form. These approaches use a combination of layout and
lexical features.
The only other work to explore structural features is
by Pellin [23], who used manually parsed abstract syntax
trees with an SVM that has a tree based kernel to classify
functions of two programmers. He obtains an average of
73% accuracy in a two class classiﬁcation task. His ap-
proach explained in the white paper can be extended to
our approach, so it is the closest to our work in the lit-
erature. This work demonstrates that it is non-trivial to
use ASTs effectively. Our work is the ﬁrst to use struc-
tural features to achieve higher accuracies at larger scales
and the ﬁrst to study how code obfuscation affects code
stylometry.
There has also been some code stylometry work that
focused on manual analysis and case studies. Spafford
and Weeber [29] suggest that use of lexical features such
as variable names, formatting and comments, as well as
some syntactic features such as usage of keywords, scop-
ing and presence of bugs could aid in source code at-
tribution but they do not present results or a case study
experiment with a formal approach. Gray et al.
[15]
identify three categories in code stylometry: the layout
of the code, variable and function naming conventions,
types of data structures being used and also the cyclo-
matic complexity of the code obtained from the control
ﬂow graph. They do not mention anything about the syn-
tactic characteristics of code, which could potentially be
a great marker of coding style that reveals the usage of
programming language’s grammar. Their case study is
based on a manual analysis of three worms, rather than
a statistical learning approach. Hayes and Offutt [16]
examine coding style in source code by their consistent
programmer hypothesis. They focused on lexical and
layout features, such as the occurrence of semicolons,
operators and constants. Their dataset consisted of 20
programmers and the analysis was not automated. They
concluded that coding style exists through some of their
features and professional programmers have a stronger
programming style compared to students. In our results
in Section 4.3.6, we also show that more advanced pro-
grammers have a more identifying coding style.
There is also a great deal of research on plagiarism
detection which is carried out by identifying the similar-
ities between different programs. For example, there is a
widely used tool called Moss that originated from Stan-
ford University for detecting software plagiarism. Moss
[6] is able to analyze the similarities of code written by
different programmers. Rosenblum et al. [27] present a
novel program representation and techniques that auto-
matically detect the stylistic features of binary code.
Related Work
Pellin [23]
MacDonell et al.[21]
Frantzeskou et al.[14]
Burrows et al. [9]
Elenbogen and Seliya [11]
Kothari et al. [18]
Lange and Mancoridis [20]
Krsul and Spafford [19]
Frantzeskou et al. [14]
Ding and Samadzadeh [10]
This work
This work
This work
This work
# of Programmers
2
7
8
10
12
12
20
29
30
46
8
35
250
1,600
Results
73%
88.00%
100.0%
76.78%
74.70%
76%
75%
73%
96.9%
67.2%
100.00%
100.00%
98.04%
92.83%
Table 9: Comparison to Previous Results
USENIX Association  
24th USENIX Security Symposium  267
13
7 Conclusion and Future Work
Source code stylometry has direct applications for pri-
vacy, security, software forensics, plagiarism, copy-
right infringement disputes, and authorship veriﬁcation.
Source code stylometry is an immediate concern for pro-
grammers who want to contribute code anonymously be-
cause de-anonymization is quite possible. We introduce
the ﬁrst principled use of syntactic features along with
lexical and layout features to investigate style in source
code. We can reach 94% accuracy in classifying 1,600
authors and 98% accuracy in classifying 250 authors
with eight training ﬁles per class. This is a signiﬁcant
increase in accuracy and scale in source code authorship
attribution. In particular, it shows that source code au-
thorship attribution with the Code Stylometry Feature Set
scales even better than regular stylometric authorship at-
tribution, as these methods can only identify individuals
in sets of 50 authors with slightly over 90% accuracy [see
4]. Furthermore, this performance is achieved by training
on only 550 lines of code or eight solution ﬁles, whereas