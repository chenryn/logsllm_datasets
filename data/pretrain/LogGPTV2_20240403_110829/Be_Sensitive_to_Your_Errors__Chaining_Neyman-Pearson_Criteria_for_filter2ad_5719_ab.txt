0	
  2000	
  4000	
  6000	
  8000	
  10000	
  12000	
  Bagle	
  Bifrose	
  Hupigon	
  Koobface	
  Ldpinch	
  Lmir	
  Rbot	
  Sdbot	
  Swizzor	
  Vundo	
  Zbot	
  Zlob	
  Number	
  of	
  instances	
  per	
  family	
  Full	
  Unpacked	
  0	
  0.2	
  0.4	
  0.6	
  0.8	
  1	
  Bagle	
  Bifrose	
  Hupigon	
  Koobface	
  Ldpinch	
  Lmir	
  Rbot	
  Sdbot	
  Swizzor	
  Vundo	
  Zbot	
  Zlob	
  Frac?on	
  of	
  instances	
  with	
  feature	
  values	
  Hexdump	
  2-­‐gram	
  Objdump	
  1-­‐gram	
  PE	
  header	
  PIN	
  trace	
  123We next show that if m = (d + 2) log2[3e(k + 1)2], it holds
that ∆(m)  ˜β(C2, ρ2, σ2)
˜α(C2, ρ2, σ2) > α∗ and
˜α(C1, ρ1, σ1)  α
∗
) −
),
(9)
where δ(x) is 1 if x is true or 0 otherwise.
The algorithm is illustrated in Figure 4. The genetic al-
gorithm has the following advantages. First, as mentioned
earlier, it performs both local search and global search in
each generation. This prevents the algorithm from getting
stuck at local optima while improving the overall quality
of the population from generation to generation. Second,
both parameter k, which is used to control the number of
good conﬁgurations from which to reproduce the next gen-
eration, and the number of generations that are eventually
reproduced can be used to control the number of times that
the search takes place. This thus provides a knob to decide
how much time would be spent on searching the optimal
conﬁguration, given the computational resources available.
Finally, the genetic algorithm can be easily parallelized by
distributing the reproduction task over multiple processors.
5. ENSEMBLE OF CLASSIFIERS
For each type of features, we can train a classiﬁer as dis-
cussed in Section 4. The question that naturally follows
is: given the classiﬁcation results from multiple individual
classiﬁers on a new malware instance, how should we decide
whether it belongs to a speciﬁc family? One widely used
rule is decision by majority, that is, the same decision made
by the majority of the classiﬁers is chosen as the ﬁnal ver-
dict. The problem with the majority rule, however, is that
we have to collect all types of features that are fed to these
individual classiﬁers, which make independent decisions on
classiﬁcation. This can be time consuming, as for some types
Figure 4:
searching optimal parameters
Illustration of genetic algorithm for
of features, it takes signiﬁcant time and resources to collect
their values from a new malware variant.
Due to this concern, our framework uses a simple ‘OR’
rule: as long as any classiﬁer decides that a new malware
instance belong to a speciﬁc family, the framework classiﬁes
it into that family. With such an ‘OR’ rule, our malware
classiﬁcation framework can draw classiﬁcation results from
individual classiﬁers in a sequential manner. As long as
one classiﬁer classiﬁes the malware instance as positive, the
framework does not need to consider the classiﬁcation results
by subsequent classiﬁers, and it is thus unnecessary to col-
lect their corresponding types of features. In order to train
the ensemble classiﬁer as described, we enforce the chain
Neyman-Pearson criterion on individual classiﬁers, which
will be explained next.
5.1 Chain Neyman-Pearson Criterion
Suppose that the training data are divided into m folds
{Fi}i=1,2,...,m. Next, we train an individual classiﬁer for
the t-th type of features from each of {F−i}i=1,2,...m and
evaluate its performance on instances in Fi. Also suppose
that from each malware sample we can extract T types of
features. We now discuss how to modify the algorithm dis-
cussed in the previous section to train an individual classiﬁer
for the t-th type of features where 1 ≤ t ≤ T .
i ⊆ Fi be the set of positive and
−
negative instances in fold Fi, respectively. Given any con-
ﬁguration g = (C, ρ, σ) for the t-th type of features, let the
set of positives that the candidate classiﬁer trained from F−i
identiﬁes from test data Fi be Θt,i(g). Deﬁne α∗
i ⊆ Fi and F