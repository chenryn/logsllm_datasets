RackSpace do [6].
2 Previous work [22] has shown that the maximal lexicographical
allocation is equivalent to max-min for multi-path routing, a prob-
lem to which network proportionality can be reduced to.
VM1 Bwmin1 Bwmin2 BwminN … VM2 VMN Figure 2: Network proportionality vs. min-guarantee. With propor-
tionality, as B1 communicates with more VMs, A1’s bandwidth can
be decreased arbitrarily.
because A has two VMs and there are 13 VMs in total, while B1
should get 11=13. Unfortunately, one can arbitrarily reduce A1’s
bandwidth by simply increasing the number of VMs that B1 com-
municates with. While strictly speaking, proportionality still pro-
vides a min-guarantee, since the number of VMs in the data center
is (cid:277)nite, the resulting guarantee is too low to be useful in practice.
If we consider the min-guarantee requirement alone, and assume
no other VM can be admitted on the server, both A1 and B1 should
each be guaranteed half of the capacity of the shared access link,
just as each VM would be guaranteed half of the resources of the
machine they are running on (recall that we are assuming identical
VMs for now). is guarantee should not be aﬀected by the traﬃc
of other tenants in the network. However, as illustrated by our ex-
ample, there is a hard tradeoﬀ between network proportionality and
min-guarantee: one can either achieve network proportionality or
provide a useful min-guarantee, but not both!
2.4 Tradeoff Between Network Proportiona-
lity and High Utilization
We now show that even in the absence of the min-guarantee re-
quirement, network proportionality is hard to achieve. In particu-
lar, we show that there is a tradeoﬀ between network proportionality
and high utilization.
To illustrate this tradeoﬀ, consider the example in Figure 3 de-
picting two tenants A and B, each employing four VMs. Figure 3(a)
shows a scenario in which their (cid:280)ows traverse the same congested
link L of capacity C; each tenant gets C=2 of the aggregate band-
width, and the network proportionality requirement is met. Now
assume that VMs A1 and A3 start communicating along an uncon-
gested path, P (Figure 3(b)). In order to preserve network propor-
tionality, we need to decrease tenant A’s allocation along link L. Un-
fortunately, if A deems its traﬃc along L more important than that
between A1 and A3, A is disincentivized to use path P, which hurts
network utilization.
We will refer to the ability of a tenant to use an uncongested path
without being penalized on another (disjoint) congested path as the
utilization incentives property. us, the tradeoﬀ between network
proportionality and high utilization can be reduced to a tradeoﬀ be-
tween network proportionality and utilization incentives.
2.4.1 Congestion Proportionality
It might appear that one could get around this tradeoﬀ by re-
stricting the network proportionality requirement only to the traﬃc
traversing congested links. We de(cid:277)ne congestion proportionality as
network proportionality restricted to congested paths that involve
more than one tenant. In other words, for each tenant K, conges-
tion proportionality considers only
K , where there is at least
a congested link along path i!j traversed by the (cid:280)ows of multiple
tenants. For the example in Figure 3(b), since the path between A1
and A3 is used only by tenant A, it does not count towards A’s use of
congested resources when comparing the aggregate bandwidths of
∑
i;j Ri;j
Figure 3: Network proportionality vs. high utilization. (a) VMs of
tenants A and B have equal shares on link L. (b) If A1 starts com-
municating with A3, A’s allocation on L decreases; thus, A may be
incentivized to refrain from using the free path P. (c) Even with con-
gestion proportionality, tenants can have incentives to keep links
underutilized to increase their total allocation.
A and B. us, congestion proportionality does not disincentivize a
tenant from using free resources.
Unfortunately, even congestion proportionality con(cid:280)icts with hi-
gh utilization, since it can incentivize tenants to arti(cid:277)cially in(cid:280)ate or
de(cid:280)ate their real demands. Consider Figure 3(c), where the traﬃc of
tenants A and B is split across two congested links L1 and L2 with the
same capacity C. Initially, assume that tenants have high demands
and each tenant receives half of each congested link. is allocation
trivially meets congestion proportionality.
Now assume the demand from B3 to B4 drops to a small value ϵ (all
the other demands remain very high). As a result, B3!B4 will get ϵ
on L2, while A3!A4 will get C(cid:0)ϵ. In turn, congestion proportiona-
lity will change the allocation on L1 so that B1!B2 gets C (cid:0) ϵ and
A1!A2 gets ϵ. By doing so, each tenant will still get same aggregate
capacity C, and the system remains fully utilized.
However, congestion proportionality can still violate the utiliza-
tion incentives property. For example, A may choose to send only
C (cid:0) 2ϵ on L2. Since L2 is no longer a congested link, congestion
proportionality will only allocate L1, by giving C=2 to both tenants.
As a result, A ends up getting an aggregate allocation of 3C=2 (cid:0) 2ϵ,
while B will get only C=2 + ϵ. By doing so, A increases her alloca-
tion, while the system utilization decreases from 2C to 2C (cid:0) ϵ. is
example also illustrates the fact that a tenant can arti(cid:277)cially mod-
ify her demand to get a higher allocation. We refer to the ability to
prevent such behavior as strategy-proofness.
2.4.2 Link Proportionality
We have shown that both network proportionality and its re-
stricted version, congestion proportionality, end up compromising
high utilization. To avoid this tradeoﬀ, we further constrain the net-
work proportionality requirement to individual links.
We de(cid:277)ne link proportionality as network proportionality re-
stricted to a single link. More concretely, if the link is congested,
link proportionality translates into max-min fairness between dif-
ferent tenants that communicate on that link.34 A remaining ques-
tion is what weight to associate with a tenant on a given link. e
only constraint we impose for link proportionality is that the weight
of any tenant K on a link L is the same for any communication pat-
tern between K’s VMs communicating over L and any distribution
of the VMs as sources and destinations. For example, one can de-
(cid:277)ne the tenant K’s weight on link L as the number of VMs of K that
communicate on L. In another example, K’s weight can be de(cid:277)ned
3e max-min allocation is equivalent to the lexicographic maxi-
mum for both network proportionality and link proportionality.
4One can generalize link proportionality to the granularity of each
VM instead of the tenant granularity, as we will discuss in §4.
B1 A1 B1 A2 B1 B1 B1 B1 B1 B1 B1 B1 B11 L L(a) A2 A4 L(b) L1 (c) P L2 A1 A3 A2 A4 B2 B4 A1 A3 B1 B3 A1 A3 B1 B3 A2 A4 B2 B4 B2 B4 B1 B3 Figure 4: Two tenants sharing a single link using diﬀerent commu-
nication patterns.
as NK, i.e., total number of K’s VMs in the network. Note that the
latter allocation is similar to that of NetShare [19].
Since the allocation is independent across diﬀerent links, link
proportionality can achieve high utilization. However, link propor-
tional allocations can be substantially diﬀerent than network wide
allocations such as those meeting congestion proportionality, since
each VM can compete on a diﬀerent number of congested links in
the network.
More generally, we can classify all allocation policies to be ap-
plicable either at the link level or at the network level. We de(cid:277)ne
link level allocations to have the link-independence property, mean-
ing that the allocation at each congested link L is based only on the
demands known at L and on static information about the VMs com-
municating on L (i.e., information that does not change during the
lifetime of a VM). In the absence of link-independence, there can
exist a congested link L whose bandwidth allocation can change due
to a change in the allocation of another congested link L′ or due to a
change in the communication pattern of one of the VMs communi-
cating on L. Network proportionality and congestion proportiona-
lity are not link-independent, while link proportionality is. All link-
independent allocations provide incentives to use free resources.
Traditional allocation policies, such as per-(cid:280)ow fairness, are link-
independent, and thus are capable of achieving high utilization.
However, we next show that they cannot provide even link propor-
tionality, nor do they provide min-guarantees.
2.5 Traditional Allocation Policies
e traditional approach to sharing the network is to apply per-
(cid:280)ow fairness, where a (cid:280)ow is characterized by the standard (cid:277)ve-tuple
in packet headers. However, the Per-Flow mechanism can lead to
unfair allocations at the VM granularity [11]. For instance, two VMs
can increase the allocation between them at the expense of other
VMs by simply instantiating more (cid:280)ows.
A natural “(cid:277)x” is to use a per source-destination pair (Per-SD) al-
location policy, where each source-destination pair is allocated an
equal share of a link’s bandwidth regardless of the number of (cid:280)ows
between the pair of VMs. However, this policy does not provide li-
nk proportionality either, because a VM that communicates with
many VMs gets more bandwidth than a VM that communicates
with fewer VMs. For example, a tenant that employs an all-to-all
communication pattern between N VMs will get a bandwidth share
of O(N2), while a tenant that performs one-to-one communication
between the same number of N VMs will get a share of only O(N).
Figure 4 (a) shows one such example, where the bandwidth allocated
to tenant A is twice that of tenant B.
To address this problem, previous solutions (e.g., Seawall [25])
have proposed using a per source (Per-Source) allocation policy. Per-
Source assigns equal weights to all the sources communicating over
a given link, and the bandwidth is divided accordingly. While this
is fair to sources, it does not meet link proportionality since it is
not fair to destinations. For example, in Figure 4(b), if VMs A1, A3,
and B1 are the sources, then the bandwidth allocation of tenant A
Property
P1. Work Conservation
P2. Strategy-Proofness
P3. Utilization incen-
tives
P4.
Pattern Independence
P5. Symmetry
Communication-
Description
Full utilization of bottleneck links.
By being dishonest a tenant cannot im-
prove her utility.
Tenants are not disincentivized to use un-
congested resources.
e allocation does not favor some com-
munication patterns compared to others.
Reversing demands of all (cid:280)ows in the net-
work does not change their allocations.
Table 2: Network sharing properties desirable for any bandwidth
allocation policy in clouds.
would be twice that of tenant B. However, if the direction of the
communication is reversed, tenant A would receive only half the
bandwidth allocated to B. erefore, there is a mismatch between
the amount of traﬃc sourced and the amount received by a VM.
We refer to this mismatch between allocations in opposite di-
rections as asymmetry and consider Per-Source to be asymmetric.
Asymmetry is undesirable because it can result in application-level
ineﬃciencies. For example, in a MapReduce setting, a VM hosting
both a mapper and a reducer can experience a signi(cid:277)cant discrep-
ancy between the bandwidth available for the mapper to send and
for the reducer to receive, slowing down the application to the slow-
est component. More generally, VMs can experience large varia-
tions between the incoming or outgoing bandwidths, without show-
ing a preference for one of them. A per destination (Per-Destination)
allocation policy is asymmetric as well for similar reasons.
We have shown that both Per-Source and Per-Destination alloca-
tion policies fail to provide link proportionality. In addition, neither
satis(cid:277)es min-guarantee. Referring back to Figure 2, we can easily see
that A1’s incoming bandwidth can arbitrarily be reduced by tenant
B for Per-Source, and same applies to A1’s outgoing bandwidth for
Per-Destination.
To summarize, we have established that there are fundamental
tradeoﬀs between our requirements and that the traditional alloca-
tion policies are not satisfactory. We will next express these tradeoﬀs
using a set of lower-level desirable properties. Based on the require-
ments and properties, we will propose new allocation policies to be
implemented in cloud data centers.
3. NETWORK SHARING PROPERTIES
In this section, we describe a set of desirable properties that en-
able us to examine the above tradeoﬀs more explicitly. Table 2 sum-
marizes these properties. We do not claim this to be a complete set
of desirable properties, but rather a set that enables us to better un-
derstand the tradeoﬀs. Figure 5 captures the relationship between
these properties, the requirements and tradeoﬀs discussed in §2.
P1. Work conservation: As long as there is at least a tenant that
has packets to send along link L, L cannot be idle. More formally,
consider m tenants with demands D = fD1; : : : ; Dmg, and let P
be an allocation policy that provides allocations R = fR1; : : : ; Rmg
(see Eq. 1). We say that P is work-conserving, iﬀ for any (cid:280)ow of K
that traverses an uncongested path i!j, Ri;j
K . In other words,
a link is either fully allocated, or it satis(cid:277)es all demands. Surpris-
ingly, unlike the case of a single resource, in a distributed setting,
work conservation does not guarantee high utilization. e next
two properties illustrate this point.
P2. Strategy-proofness: Tenants cannot improve their allocations
by lying about their demands [14]. Consider allocation policy P that
provides allocation RK to tenant K with demand DK. With each ten-
ant K, we associate utility UK(RK), a scalar function de(cid:277)ned on K’s
K = Di;j
a) b) c) A1 A3 B1 B3 A2 A4 B2 B4 A1 A3 B1 B3 A2 B2 A1 A3 B1 A2 B2 Figure 5: Requirements, properties, and tradeoﬀs between them.
allocation. We say that P is strategy-proof if tenant K cannot change
its demands to get a better allocation with higher utility. at is, for
any demand cDK ̸= DK, we have UK(RK) (cid:21) UK(cRK), wherecRK is
the allocation corresponding to cDK. In other words, tenant K has
no incentives to lie about her demands.
Unfortunately, any allocation policy that is unaware of the utility
functions cannot satisfy the above de(cid:277)nition. Moreover, even if we
restrict the utility function of any tenant K to represent the total al-
location of K, UK(RK) = jRKj, i.e., each byte has the same utility
for K, this property is still very challenging to achieve for any work