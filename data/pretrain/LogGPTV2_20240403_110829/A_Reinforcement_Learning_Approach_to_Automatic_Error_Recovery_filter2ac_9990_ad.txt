the  RL-trained  policy  except  for  several  exceptions, 
such  as  error  type  23  (Figure  11(a)).  However,  when 
we take a closer look at the training data of error type 
23,  we  find  that  some  new  patterns  that  appear  in  the 
test  set  are  not  covered  by  the  training  set,  so  the 
trained policy is suboptimal and may not perform stab-
ly. As the size of the training data increases, more pre-
cise  policy  is  generated  and  the  hybrid  approach  per-
forms  nearly  the  same  as  the  trained  policy,  as  pre-
sented in Figure 11(b). 
)
.
c
e
s
n
o
i
l
l
i
M
(
t
s
o
c
e
m
i
t
l
a
t
o
T
User-defined policy
Hybrid policy
35
30
25
20
15
10
5
0
1
2
3
4
Figure  12:  Total  time  cost  of  hybrid  approach 
under different tests  
Test number
Figure  12  summarizes  the  total  cost  for  the  original 
user-defined  policy  and  the  hybrid  policy.  Like  the 
trained policy, the hybrid policy can also achieve more 
than  10%  improvement  over  the  original  policy,  on 
average. Corresponding to the policy trained with 40% 
of  the  log,  the  hybrid  approach  only  costs  89.18%  of 
the original downtime. 
5.3. Learning rate experience  
In this section, we introduce our effort in improving 
the  learning  process  to  shorten  the  training  time.  To 
this end, we use a technique called selection tree in the 
learning  process.  To  build  the  selection  tree,  we  con-
sider the best two repair actions each time when gene-
rating  the  policy  from  the  Q  values.  If  the  expected 
total  cost  of  the  second  best  action  is  close  enough 
(based  on  a  threshold)  to  that  of  the  best  one,  we  will 
choose  both  actions  as  candidates.  Otherwise  we  will 
only  choose  the  best  one.  Then,  the  selection  tree  can 
be built by iteratively putting these candidate actions as 
the  children  of  the  previous  repair  action,  and  the  op-
timal  policy  can  be  generated  by  scanning  the  tree. 
Figure 13 shows the training time of this method (with 
selection  tree)  compared  to  the  standard  RL  training 
course (without selection tree) with a maximum of 160 
thousand sweeps (training set proportion = 0.4).  
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:32:46 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 20071000000
100000
10000
s
p
e
e
w
s
f
o
#
1000
100
10
1
With selection tree
Without selection tree
1 3 5 7 9 111315171921232527293133353739
Error type
Figure  13:  Training  time  comparison.  Vertical 
axis  stands  for  sweep  number  before  conver-
gence of training process for each error type.  
Moreover, the performance of the policies trained by 
these two methods is shown in Figure 14. We can find 
that, using standard RL method, some training courses 
do not converge to the optimal policies even after 160 
thousand sweeps. In contrast, with a selection tree, we 
can speed up the learning rate and successfully find the 
optimal policy within 40 thousand sweeps in our expe-
riment. 
2.5
2
With selection tree
Without selection tree
1.5
1
t
s
o
c
e
m
i
t
e
v
i
t
l
a
e
R
0.5
0
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39
Error type
Figure  14:  Performance  comparison  between 
optimized  training  method  and  standard  me-
thod  
6. Related work 
Gray’s classic text on failure analysis [14] surveys a 
range  of  failure  statistics  of  a  commercially  available 
fault-tolerant  system.  He  also  discusses  various  ap-
proaches  to  software  fault-tolerance,  which  mainly 
focus  on  how  to  prevent  the  occurrence  of  failures  or 
reduce their frequency.  
More  recent  work  attempted  to  employ  statistical 
learning  techniques  in  automated  fault  diagnosis  and 
performance  management.  An  early  work  by  Ma  and 
Hellerstein  [19]  presented  an  efficient  algorithm  to 
discover  all  infrequent  Mutually  Dependent  Patterns 
(m-pattern) for system  management, for example, iso-
lating problems in computer networks. Some statistical 
tools  were  developed  for  diagnosing  configuration  er-
rors  that  cause  a  system  to  function  incorrectly.  Whi-
taker et al. presented the Chronus tool [26], which au-
tomates  the  task  of  searching  for  a  failure-inducing 
state change. A  similar  work was completed by Wang 
et  al.  [25].  They  presented  the  PeerPressure  trouble-
shooting  system  that  uses  statistics  from  a  set  of  sam-
ple  machines  to  diagnose  misconfiguration  on  a  sick 
machine.  There  are  still  many  projects  focusing  on 
performance analysis and debugging or bottleneck de-
tection. Examples that include Magpie [2] and Pinpoint 
[9]  make  efforts  to  associate  failures  or  performance 
problems with possible components via request traces. 
On failure diagnosis, Cohen et al. proposed to correlate 
the  low-level  system  metrics  with  high-level  perfor-
mance  states  using  Tree-Augmented  Naïve  Bayesian 
networks [11]. Based on this work, they construct sig-
natures for clustering and retrieving, which could yield 
insights  into  the  causes  of  observed  performance  ef-
fects  and  provide  a  way  to  leverage  past  diagnostic 
efforts [12][28]. Yuan et al. [27] proposed to correlate 
known faults to system behaviors with pattern classifi-
cation techniques so as to recognize future occurrences 
of the faults automatically. Compared to these research 
efforts,  our  approach  focuses  on  automated  error  re-
covery  instead  of  performance  diagnosis.  Recently, 
Tesauro  et  al.  [24]  completed  a  similar  work  that  em-
ployed  a  hybrid  reinforcement  learning  approach  to 
performance management. 
In the area of error recovery, the common approach-
es  rely  on  a  priori  knowledge  from  human  experts  to 
build  policies  for  systems.  Often,  these  repair  actions 
can be expensive, causing nontrivial service disruption 
or  downtime.  Some  recent  research  seeks  to  improve 
this  method  by  introducing  fine-grained  recovery  me-
chanisms.  Microreboot  [7],  for  example,  provides  a 
way for recovering faulty application components in an 
Internet  auction  system,  without  disturbing  the  rest  of 
the application. We believe this work is complement to 
our work since we do not set any limitations on the set 
of  repair  actions.  Additionally,  with  more  potential 
repair  actions,  authoring  or  generating  reasonable  re-
covery policy will become evermore critical. 
7. Conclusion 
In  this  paper,  we  proposed  a  novel  reinforcement 
learning  approach  to  improve  the  framework  of  auto-
matic  error  recovery.  Specifically,  we  focus  on  recov-
ery  policy  generation  when  a  system  model  is  not 
available, which we believe has not yet been fully stu-
died.  We  have  investigated  how  to  make  proper  deci-
sions on which repair actions to choose when the actual 
root cause is only localized at a coarse level. With our 
method,  a  locally  optimal  policy  is  guaranteed  to  be 
found,  and  it  can  adapt  to  changes  in  environment 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:32:46 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007without  human  involvement.  Finally,  experimental 
results  on  data  from  a  real  cluster  environment  show 
that automatically generated policy achieved more than 
10% savings in machine downtime on average. Several 
possible  extensions  of  the  approach  include  using  ge-
neralization  functions  to  approximate  the  Q-learning 
values,  introducing  more  complicated  relationships 
among  actions,  and  designing  initial  policies  that  can 
be improved. In addition, we believe the approach pro-
vide  greater  benefits  when  we  gain  more  information 
from event monitoring or fault detection. 
8. Acknowledgments 
We  would  like  to  thank  Ken  Cao,  Peirong  Liu  and 
Yi Li for clarifying the details of the user-defined poli-
cy and the recovery log. We also thank the anonymous 
reviewers  for  their  helpful  comments  and  Dwight  Da-
niels for proofreading the paper. 
References 
[1]  M.  Baker  and  M.  Sullivan.  The  Recovery  Box:  Using 
fast  recovery  to  provide high  availability  in  the  UNIX 
environment.  In  Proc.  Summer  USENIX  Technical 
Conference, San Antonio, TX, 1992 
[2]  P. Barham, A. Donnelly, R. Isaacs and R. Mortier. Us-
ing Magpie for request extraction and workload model-
ing.  In  Proc.  6th  Symposium  on  Operating  Systems 
Design and Implementation (OSDI), Dec. 2004. 
[3]  J.F.  Bartlett.  A  NonStop  kernel.  In  Proc.  8th  ACM 
Symposium  on  Operating  Systems  Principles,  Pacific 
Grove, CA, 1981.  
[4]  A.  Borg,  W.  Blau,  W.  Graetsch,  F.  Herrman  and  W. 
Oberle.  Fault  Tolerance  under  UNIX.  ACM  Transac-
tions on Computer Systems, 7(1): 1–24, Feb 1989. 
[5]  E. Brewer. Lessons from giant-scale services. IEEE In-
ternet Computing, 5(4):46–55, July 2001. 
[6]  G.  Candea  and  A.  Fox.  Crash-only  software.  In  Proc. 
9th  Workshop  on  Hot  Topics  in  Operating  Systems, 
Lihue, Hawaii, 2003. 
[7]  G.  Candea,  S.  Kawamoto,  Y.  Fujiki,  G.  Friedman  and 
A. Fox. Microreboot – A Technique for Cheap Recov-
ery.  In  Proc.  6th  Symposium  on  Operating  Systems 
Design and Implementation (OSDI), Dec 2004. 
[8]  G. Candea, E. Kiciman, S. Kawamoto and A. Fox, Au-
tonomous  Recovery  in  Componentized  Internet  Appli-
cations. Cluster Computing Journal, 9(1), Feb 2006 
[9]  M. Chen, E. Kiciman, E. Fratkin, A. Fox and E. Brew-
er.  Pinpoint:  Problem  determination  in  large,  dynamic 
systems. In Proc. 2002 Intl. Conf. on Dependable Sys-
tems and Networks, Washington, DC, June 2002. 
[10]  T.C.  Chou.  Beyond  fault  tolerance.  IEEE  Computer, 
30(4):31–36, 1997. 
[11]  I. Cohen, M. Goldszmidt, T. Kelly, J. Symons and J.S. 
Chase.  Correlating  instrumentation  data  to  system 
states:  A  building  block  for  automated  diagnosis  and 
control. In Proc. 6th Symposium on Operating Systems 
Design and Implementation, Dec. 2004. 
[12]  I.  Cohen,  S.  Zhang,  M.  Goldszmidt,  J.  Symons,  T. 
Kelly and A. Fox. Capturing, Indexing, Clustering, and 
Retrieving System History. In Proceedings of the ACM 
Symposium  on  Operating  Systems  Principles  (SOSP), 
Oct. 2005. 
[13]  A.  Fox  and  D.  Patterson.  Self-repairing  computers. 
Scientific American, June 2003. 
[14]  J.  Gray.  Why  Do  Computers  Stop  and  What  Can  Be 
Done About It? 6th International Conference on Relia-
bility and Distributed Databases, June 1987. 
[15]  G.J.  Gordon.  Stable  Function  Approximation  in  Dy-
namic  Programming,  tech  report  CMU-CS-95-103, 
1995. 
[16]  K.R.  Joshi,  W.H.  Sanders,  M.A.  Hiltunen  and  R.D. 
Schlichting.  Automatic  Model-Driven  Recovery  in 
Distributed Systems. SRDS 2005: 25-38 
[17]  K.R.  Joshi,  W.H.  Sanders,  M.A.  Hiltunen  and  R.D. 
Schlichting.  Automatic  Recovery  Using  Bounded  Par-
tially Observable Markov Decision Processes. In Proc. 
of  the  2006  International  Conference  on  Dependable 
Systems and Networks (DSN’06): 445-456 
[18]  J.O. Kephart and D.M. Chess. The vision of autonomic 
computing. Computer, 36(1):41–50, 2003. 
[19]  S.  Ma  and  J.L.  Hellerstein.  Mining  Mutually  Depen-
dent  Patterns  for  System  Management.  IEEE  Journal 
on  Selected  Areas  in  Communications,  VOL.  20,  NO. 
4, May 2002. 
[20]  T.M. Mitchell. Machine Learning. McGraw-Hill, 1997. 
[21]  S.A.  Murphy.  A  Generalization  Error  for  Q-Learning. 
Journal of Machine Learning Research, 6 (2005) 1073–
1097. 
[22]  B.  Murphy  and  T.  Gent.  Measuring  system  and  soft-
ware  reliability  using  an  automated  data  collection 
process.  Quality  and  Reliability  Engineering  Intl., 
11:341–353, 1995. 
[23]  R.S.  Sutton.  Learning  to  Predict  by  the  Methods  of 
Temporal  Differences.  Machine  Learning  3:  9-44, 
1988. 
[24]  G.  Tesauro,  R.  Das  and  N.  Jong.  Online  Performance 
Management  Using  Hybrid  Reinforcement  Learning. 
First  Workshop  on  Tackling  Computer  Systems  Prob-
lems  with  Machine  Learning  Techniques  (SysML’06), 
June 2006.  
[25]  H.J.  Wang,  J.C.  Platt,  Y.  Chen,  R.  Zhang  and  Y.M. 
Wang.  Automatic  Misconfiguration  Troubleshooting 
with PeerPressure. In Proc. 6th Symposium on Operat-
ing Systems Design and Implementation, Dec. 2004. 
[26]  A. Whitaker, R.S. Cox and S.D. Gribble. Configuration 
Debugging as Search: Finding the Needle in the Hays-
tack.  In  Proc.  6th  Symposium  on  Operating  Systems 
Design and Implementation, Dec. 2004. 
[27]  C.  Yuan,  N.  Lao,  J.-R.  Wen,  J.  Li,  Z.  Zhang,  Y.-M. 
Wang and W.-Y. Ma. Automated Known Problem Di-
agnosis  with  Event  Traces.  1st  EuroSys  Conference, 
April 2006 
[28]  S. Zhang, I. Cohen, M. Goldszmidt, J. Symons and A. 
Fox. Ensembles of Models for Automated Diagnosis of 
System Performance Problems. In Proc. of the 2005 In-
ternational  Conference  on  Dependable  Systems  and 
Networks (DSN’05). 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:32:46 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007