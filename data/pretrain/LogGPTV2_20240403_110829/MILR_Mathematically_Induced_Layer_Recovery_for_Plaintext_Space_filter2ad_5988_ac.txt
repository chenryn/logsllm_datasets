(256)
(10)
We perform three types of experiments: (1) inject bit errors
a probability of p (i.e. Raw Bit Error Rates (RBER)), (2)
inject whole-weight errors with a probability of q, and (3)
corrupt entire layers. Experiment (1) is to the effectiveness
of MILR in comparison to ECC if no memory encryption
is used, which gives a rough idea of MILR capability in
traditional random bit error settings. Experiments (2) and (3)
are most relevant in plaintext space error correction (PSEC)
that is the goal of MILR. In plaintext space, ECC is unable
to recover from such errors, while MILR can in many cases.
The injection of bit errors is done by ﬂipping each bit with
a probability p (error rate), regardless of bit position and
role (each 32-bit ﬂoat parameter has sign, magnitude and
mantissa). Whole-weights are injected by ﬂipping every bit
in a weight with a probability of q. Entire layers are corrupted
6
by replacing the entire layers parameters with new random
parameters. These experiments attempt to simulate plaintext
space errors and plaintext-level security attacks. The random
bit ﬂipping simulates soft memory errors, and to a more
limited extent security attack such as it Flip Attacks [19] or
fault injection attacks [14], with the whole-weight and layer
corruption focusing on more aggressive security attacks.
MILR was compared to SECDED (single-bit error correc-
tion and double-bit error detection) ECC protecting each word.
This (39,32) code requires 7 additional ECC bits for each 32-
bit word that coincides with a single parameter, allowing error
recovery for any parameter if a single bit of it is corrupted.
In the case of more than 1 bit error no correction occurs and
interrupts is not raised.
Implementation MILR was implemented as a library that
could be used with TensorFlow [2] taking a Tensorﬂow model
as input. The model is initially processed to prepare for error
detection and recovery. Periodically, MILR’s error detection
function is called, and if errors are detected, the error recovery
function is also called. MILR can recover any number of errors
in a single layer between a pair of checkpoints for dense and
convolution layers, or G2 parameters for convolution layers
using partial recoverability. However, if the RBER is very
high, there may be more than one erroneous layers between
a pair of checkpoints. In this case, full self-healing cannot be
guaranteed. However, error recovery is invoked regardless, and
applied to the erroneous layers in sequential order.
Limitations As MILR was implemented as an external
library and not as a part of TensorFlow execution pipeline
limiting MILR’s performance. MILR does take advantage of
TensorFlow’s function calls where possible, but further per-
formance optimization may be possible. MILR error recovery
relies on algebraic relationship of input, parameter, and output,
hence it is affected by ﬂoating point arithmetic rounding in
binary representations, e.g. algebraically (a+b)+c = a+(b+c)
but with binary ﬂoating point representation and rounding,
(a + b) + c ≈ a + (b + c). This is made worse in large
computation for solving large systems of equations.
B. MNIST Network
The MNIST handwritten numbers database [11] is a com-
monly used database in machine learning, with 60,000 28 x
28 black and white training images and 10,000 images for
testing; which are classiﬁed into 10 categories. The network
was built according to Table I, with valid padding convolution
layers, and a bias and ReLu activation layer after each dense
and convolution layer. The network was trained for 5 epochs
with a batch size of 128, to an accuracy of 99.2%.
Figure 5 shows box plots of 40 runs with varying RBER
with normalized average accuracy (i.e. 100% means the same
accuracy as the error-free version). Figure 5a shows the raw
un-recovered effect of the RBER, with ﬁgures 5b, and 5c, and
5d showing the accuracy after error is detected and recovered
using ECC, MILR and ECC + MILR.Each plot is centered on
the median values, with the box covering the 25th and 75th
percentile (i.e. interquartile range). The whiskers extended
1.5× the interquartile range from the top and bottom of the
7
box, up to the max/min value. Outliers are marked by dots on
the graph.
The MNIST network (Figure 5a) has a little bit of built-
in robustness to errors, keeping accuracy high (99.1%) for
1E-07 RBER. However, in some cases with 5 ﬂipped bits,
the normalized accuracy drops to 64.5%. This is because not
all bit positions are equal in their impact on accuracy, most
signiﬁcant bit (MSB) has a larger impact on accuracy than
least signiﬁcant bit (LSB) [18]. ECC increases the robustness
of the network (Figure 5b), but as RBER increases such that
multi-bit errors occur (after 1E − 05), ECC’s performance
starts to drop.
MILR is able to increase the robustness of the network
over the no recovery and ECC as it provides 99% of accuracy
through 1E − 04, as shown in Figure 5c, with an outlier at
1E− 05. This is due to MILR being able to recover from both
single and multi bit errors. MILR has some outliers with lower
accuracies after 1E − 04, as the frequency of errors affecting
multiple layers between a pair of checkpoints increases. This
causes the input/output pairs being used to recover the param-
eters to be erroneous from having to pass through erroneous
layers to get the destination layer, diminishing the accuracy of
the recovery.
Investigating the outliers, they are caused by either too
many erroneous layers or some errors are not detected. For
the former problem, MILR can use more checkpoints, or
alternatively, utilize a combination of ECC and MILR. When
combined (Figure 5d), ECC addresses most single bit errors
before reaching MILR, leaving MILR to deal with multi-bit
errors. The removal of the majority of the single bit errors
helps prevent multiple erroneous layers between a pair of
checkpoints.
Another cause of outliers with lower accuracy is the error
detection limitation. With MILR, before recovery is initiated,
error detection phase must identify what layers need to be
recovered. Our detection scheme for MILR is a lightweight
detection scheme that requires the errors to be signiﬁcant
enough to detect. This does mean that not all errors will be
detected; they are only detected when they have a meaningful
impact on the output of the layer. For MNIST, in 78.6% of
the tests, all erroneous layers were detected. In the remaining
21.3% of cases where not all erroneous layer were detected
MILR still restores the accuracy to 99.9% of the original
accuracy.
The MNIST network was also tested with whole-weight
errors with a probability of q, where every bit in a word was
ﬂipped. ECC and ECC + MILR were not tested with this
scheme as ECC can only correct 1 bit errors and all errors
injected would be 32 bit errors. The network still had some
intrinsic robustness with having at least 97.3% of accuracy
through an error rate of 5E − 07. When MILR is applied
it is able to recover the network to 99.9% accuracy through
5e − 04. After this point, multiple erroneous layers between
checkpoint pairs start to appear more frequently starting to
affect the recovery accuracy.
To test a scenario when a whole layer is erroneous each
layer individually has all of its parameters replaced by a
(a) No recovery
(b) ECC
(c) MILR
(d) ECC + MILR
Fig. 5: MNIST network normalized accuracy after recovery from varying RBER
(a) No recovery
(b) MILR
Fig. 6: MNIST network normalized accuracy after recovery
from whole-weight errors
random values, where none of the values were the same as
the original value. Then MILR attempted to solve the layers
back to its original state. For the convolution layers using the
partial recoverability by design they can not recovery the full
layer. In those scenarios, they followed their standard recovery
process but when they attempt to solve their system of equation
they have more variables than equations. To address this they
attempt to ﬁnd a least-square solution. This provides a solution
to the linear equation as close as possible to the actual solution.
The less under-deﬁned the problem the close the solution
should be, however the precision will vary. If the system of
equation is to underdeﬁned the system is not solvable even by
ﬁnding the least-square solution. In these cases no recovery is
possible, however the probability of these cases in real world
scenarios are slim. Also in this test, the bias of a layer is
considered as separate layer and is treated as such.
TABLE IV: MNIST network whole layer error accuracy
Layer
Conv.
Conv. Bias
Conv. 1
Conv. 1 Bias
Conv. 2
Conv 2 Bias
Dense
Dense Bias
Dense 1
Dense 1 Bias
* Convolution partial recoverable
MILR
None
100.0%
46.9%
100.0%
75.7%
N/A *
34.9%
100.0%
81.7%
N/A *
23.1%
100.0%
77.4%
10.2%
100.0%
100.0% 100.0%
9.9%
100.0%
100.0% 100.0%
Whole layer errors showed that each layer is important
to the network serving a purpose. The main function of the
layer is of key performance, with the bias layer serving less
but still playing a signiﬁcant role. The bias layers of the
dense layers had the least affect on the network compared
to other layers. For recovery MILR was able to recovery all
complete recoverable layers complete. For the convolutional
layers using partial recoverability they were to underdeﬁned
to be recovered.
TABLE V: MNIST network storage overhead
Backup Weights
6.68 MB
ECC
1.46 MB
MILR
6.81 MB
ECC & MILR
8.27 MB
MILR’s storage of additional data needed can vary from
network to network, as it varies based on the networks
structure. Hyperparameters such as layer order, layer type and
layer conﬁguration (ﬁlter size, ﬁlter count, etc.) can effect
the overhead of MILR. An optimized network for MILR can
reduce the overhead compared to a non-optimized network,
but MILR is able to be applied to any CNN and work as
expected. MILR requires to store an additional 6.81 MB of
data for MNIST network error detection and recovery. ECC
adds 1.46 MB but it has limited error recoverability as well
as the overheads are incurred at DRAM working memory.
In contrast, MILR storage overheads can be placed in SSD,
HDD, or persistent memory, which are orders of magnitude
denser and cheaper than DRAM. Keeping a backup copy of
the network allows for redundancy but requires just as much
storage overhead while not being able to detect errors.
C. CIFAR-10 Small Network
The Cifar-10 dataset [10] is a color image database with
60,000 32 × 32 images,
that can be categorized into 10
categories with 6,000 examples of each in the dataset. The
dataset
is partitioned into 50,000 images for training and
10,000 for testing. The small network was built according to
Table II with same padding convolution layers; and bias and
ReLu activation layer after each dense and convolution layer.
The architecture of the network was inspired by the VGG
network [22], but minus the last few layers and shallower ﬁlter
depth as we not using the Imagenet database [20]. The CIFAR-
10 network was trained to 84.8% accuracy, over 150 epochs
with a batch size of 128.
8
1E-075E-071E-065E-061E-055E-051E-045E-041E-03Error Rate00.20.40.60.81Normalized AccuracyError Rate1E-075E-071E-065E-061E-055E-051E-045E-041E-03Normalized Accuracy00.20.40.60.81Error Rate1E-075E-071E-065E-061E-055E-051E-045E-041E-03Normalized Accuracy00.20.40.60.81Error Rate1E-075E-071E-065E-061E-055E-051E-045E-041E-03Normalized Accuracy00.20.40.60.81Error Rate1E-075E-071E-065E-061E-055E-051E-045E-041E-035E-03Normalized Accuracy00.20.40.60.81Error Rate1E-075E-071E-065E-061E-055E-051E-045E-041E-035E-03Normalized Accuracy00.20.40.60.81(a) No recovery
(b) ECC
(c) MILR
(d) ECC + MILR
Fig. 7: CIFAR-10 small network normalized accuracy after recovery from varying RBER
The CIFAR-10 network does have some intrinsic robustness,
shown in Figure 7, achieving 99.1% of accuracy with an error
rate of 1E−07. ECC maintains 100.0% of accuracy to 1E−05
as it corrects all single bit errors. But, as with the MNIST
network, its accuracy drops as multi-bit errors start to appear.
MILR was able to achieve the same performance to 1E − 05
however it also dropped after this point as with ECC. That
being said it dropped less drastically and more gradually as
the error increased maintaining 80.2% accuracy at 5E − 04
while ECC had only 16.9% of accuracy. The combination of
ECC + MILR was able to recover back to 100.0% of accuracy
through the test range.
For the CIFAR-10 network MILR detected all erroneous
layers in 64.7% of the test. With 99.1% of these test being
restore to > 99.3% of original accuracy.
TABLE VI: CIFAR-10 small network whole layer error accu-
racy
None MILR
Recovery
12.0% 100.0%
Conv
63.3% 100.0%
Conv Bias
11.9% N/A*
Conv 1
35.7% 100.0%
Conv 1 Bias
11.8% N/A*
Conv 2
80.7% 100.0%
Conv 2 Bias
11.6% N/A*
Conv 3
95.2% 100.0 %
Conv 3 Bias
12.2% NA*
Conv 4
98.3% 100.0%
Conv 4 Bias
11.8% N/A*
Conv 5
98.9% 100.0%
Conv 5 Bias
12.8% N/A*
Conv 6
98.6% 100.0%
Conv 6 Bias
11.8% 100.0%
Dense
99.8% 100.0%
Dense Bias
13.0% 100.0%
Dense 1
Dense 1 Bias
99.3% 100.0%
* Convolution partial recoverable
TABLE VII: CIFAR-10 small network storage overhead
Backup Weights
2.79 MB
ECC
0.61 MB
MILR
1.51 MB
ECC & MILR
2.12 MB
(a) No recovery
(b) MILR
Fig. 8: CIFAR-10 small network normalized accuracy after
recovery from whole-weight errors
When tested with whole-weight errors the network still had
some intrinsic robustness, achieving 100.0% of accuracy up
to an error rate of 1E − 04. ECC was not tested as ECC’s
performance would match the no recovery performance.MILR
performance did drop after this point as multiple erroneous
layers between checkpoint caused performance degradation.
This performance shows were MILR is most capable, large
errors densely arranged, and where it beats out ECC’s cor-
recting ability.
The CIFAR-10 network with full layer errors had similar
results to the MNIST network. It did have more partial
recoverable layers as it has larger convolution layers. But
MILR was capable of restoring all other layers to 100.0%
of their original accuracy.
The small CIFAR-10 network’s hyperparameters allowed
for a lower storage overhead, shown in Table VII, as it allowed
for more data reuse. To store a second copy of the network
it would cost 2.79 MB of storage. MILR only cost 1.51 MB
of additional storage a 45.9% reduction in storage overhead.
ECC is still cheaper only cost 0.61 MB, but still susceptible
to multi-bit errors. Combining ECC and MILR allows for all
scenarios to be covered with single bit errors being handled
by ECC and multi-bit errors being handled by MILR while
costing less then storing a second copy of the network.
D. CIFAR-10 Large Network
The Cifar-10 dataset [10] was used again with another
model as the dataset is lightweight allowing for fast training
and testing while representational of a real world use case.
The large Cifar model is based off a model presented in the
paper FAWCA [8] and shown in Table III, with same padding
convolution layers; and bias and ReLu activation layer after
each dense and convolution layer. This model is signiﬁcantly
9
Error Rate1E-075E-071E-065E-061E-055E-051E-045E-04Normalized Accuracy00.20.40.60.81Error Rate1E-075E-071E-065E-061E-055E-051E-045E-04Normalized Accuracy00.20.40.60.81Error Rate1E-075E-071E-065E-061E-055E-051E-045E-04Normalized Accuracy00.20.40.60.81Error Rate1E-075E-071E-065E-061E-055E-051E-045E-04Normalized Accuracy00.20.40.60.81Error Rate1E-075E-071E-065E-061E-055E-051E-045E-041E-03Normalized Accuracy00.20.40.60.81Error Rate1E-075E-071E-065E-061E-055E-051E-045E-041E-03Normalized Accuracy00.20.40.60.81larger then the small Cifar network, with larger and deeper
ﬁlter along with larger dense layers. It was trained to 83.6%
accuracy over 150 epochs with batch sizes of 128.
The intrinsic robustness of the large cifar network is less
than that of the small cifar, as due to the size even at the
lower error there are signiﬁcantly more errors. MILR is able
to handle these additional errors as with the increase of size the
number of errors MILR can recover is also increased naturally.
MILR and ECC are both able to recover to 100% of accuracy
through 1E − 05 and start dropping after this point, but MILR
is able to maintain an higher recoverabilty due to it not being
affected by Multi-bit errors.
For whole word error, Figure 10 the results are also similar