After scoping the dynamics and pain points of the team, lay the groundwork for improvement through best practices like postmortems and by identifying sources of toil and how to best address them.
Write a Good Postmortem for the TeamPostmortems offer much insight into a team’s collective reasoning. Postmortems con‐ducted by unhealthy teams are often ineffectual. Some team members might consider postmortems punitive, or even useless. While you might be tempted to review the postmortem archives and leave comments for improvement, doing so doesn’t help the team. Instead, the exercise might put the team on the defensive.Instead of trying to correct previous mistakes, take ownership of the next postmor‐tem. There will be an outage while you’re embedded. If you aren’t the person on-call, team up with the on-call SRE to write a great, blameless postmortem. This document is an opportunity to demonstrate how a shift toward the SRE model benefits the team by making bug fixes more permanent. More permanent bug fixes reduce the impact of outages on team members’ time.As mentioned, you might encounter responses such as “Why me?” This response is especially likely when a team believes that the postmortem process is retaliatory. This attitude comes from subscribing to the Bad Apple Theory: the system is working fine, and if we get rid of all the bad apples and their mistakes, the system will continue to be fine. The Bad Apple Theory is demonstrably false, as shown by evidence [Dek14] from several disciplines, including airline safety. You should point out this falsity. The most effective phrasing for a postmortem is to say, “Mistakes are inevitable in any system with multiple subtle interactions. You were on-call, and I trust you to make the right decisions with the right information. I’d like you to write down what you were thinking at each point in time, so that we can find out where the system misled you, and where the cognitive demands were too high.”Sort Fires According to Type
There are two types of fires in this simplified-for-convenience model:
• Some fires shouldn’t exist. They cause what is commonly called ops work or toil 	(see Chapter 5).
• Other fires that cause stress and/or furious typing are actually part of the job. In 	either case, the team needs to build tools to control the burn.
420  |  Chapter 30: Embedding an SRE to Recover from Operational OverloadSort the team fires into toil and not-toil. When you’re finished, present the list to the team and clearly explain why each fire is either work that should be automated or acceptable overhead for running the service.
Phase 3: Driving Change
Team health is a process. As such, it’s not something that you can solve with heroic effort. To ensure that the team can self-regulate, you can help them build a good mental model for an ideal SRE engagement.Humans are pretty good at homeostasis, so focus on creating (or restoring) the right initial conditions and teaching the small set of principles needed to make healthy choices.
Start with the Basics
Teams struggling with the distinction between the SRE and traditional ops model are generally unable to articulate why certain aspects of the team’s code, processes, or cul‐ture bother them. Rather than trying to address each of these issues point-by-point, work forward from the principles outlined in Chapters 1 and 6.Your first goal for the team should be writing a service level objective (SLO), if one doesn’t already exist. The SLO is important because it provides a quantitative measure of the impact of outages, in addition to how important a process change could be. An SLO is probably the single most important lever for moving a team from reactive ops work to a healthy, long-term SRE focus. If this agreement is missing, no other advice in this chapter will be helpful. If you find yourself on a team without SLOs, first read Chapter 4, then get the tech leads and management in a room and start arbitrating.Get Help Clearing Kindling
You may have a strong urge to simply fix the issues you identify. Please resist the urge to fix these issues yourself, because doing so bolsters the idea that “making changes is for other people.” Instead, take the following steps:
1. Find useful work that can be accomplished by one team member.2. Clearly explain how this work addresses an issue from the postmortem in a per‐manent way. Even otherwise healthy teams can produce shortsighted action items.
3. Serve as the reviewer for the code changes and document revisions.
4. Repeat for two or three issues.
Phase 3: Driving Change  |  421When you identify an additional issue, put it in a bug report or a doc for the team to consult. Doing so serves the dual purposes of distributing information and encourag‐ing team members to write docs (which are often the first victim of deadline pres‐sure). Always explain your reasoning, and emphasize that good documentation ensures that the team doesn’t repeat old mistakes in a slightly new context.Explain Your Reasoning
As the team recovers its momentum and grasps the basics of your suggested changes, move on to tackle the quotidian decisions that originally led to ops overload. Prepare for this undertaking to be challenged. If you’re lucky, the challenge will be along the lines of, “Explain why. Right now. In the middle of the weekly production meeting.”If you’re unlucky, no one demands an explanation. Sidestep this problem entirely by simply explaining all of your decisions, whether or not someone requests an explana‐tion. Refer to the basics that underscore your suggestions. Doing so helps build the team’s mental model. After you leave, the team should be able to predict what your comment on a design or changelist would be. If you don’t explain your reasoning, or do so poorly, there is a risk that the team will simply emulate that lackadaisical behavior, so be explicit.Examples of a thorough explanation of your decision:
• “I’m not pushing back on the latest release because the tests are bad. I’m pushing 	back because the error budget we set for releases is exhausted.”
• “Releases need to be rollback-safe because our SLO is tight. Meeting that SLO requires that the mean time to recovery is small, so in-depth diagnosis before a rollback is not realistic.”Examples of an insufficient explanation of your decision:
• “I don’t think having every server generate its routing config is safe, because we 	can’t see it.”
This decision is probably correct, but the reasoning is poor (or poorly explained). The team can’t read your mind, so they very likely might emulate the observed poor reasoning. Instead, try “[…] isn’t safe because a bug in that code can cause a correla‐ted failure across the service and the additional code is a source of bugs that might slow down a rollback.”• “The automation should give up if it encounters a conflicting deployment.”
Like the previous example, this explanation is probably correct, but insufficient. Instead, try “[…] because we’re making the simplifying assumption that all changes
422  |  Chapter 30: Embedding an SRE to Recover from Operational Overloadpass through the automation, and something has clearly violated that rule. If this hap‐pens often, we should identify and remove sources of unorganized change.”
Ask Leading QuestionsLeading questions are not loaded questions. When talking with the SRE team, try to ask questions in a way that encourages people to think about the basic principles. It’s particularly valuable for you to model this behavior because, by definition, a team in ops mode rejects this sort of reasoning from its own constituents. Once you’ve spent some time explaining your reasoning for various policy questions, this practice rein‐forces the team’s understanding of SRE philosophy.Examples of leading questions:
• “I see that the TaskFailures alert fires frequently, but the on-call engineers usually 	don’t do anything to respond to the alert. How does this impact the SLO?”
• “This turnup procedure looks pretty complicated. Do you know why there are so 	many config files to update when creating a new instance of the service?”
Counterexamples of leading questions:Counterexamples of leading questions:
• “What’s up with all of these old, stalled releases?”
• “Why does the Frobnitzer do so many things?”
Conclusion
Following the tenets outlined in this chapter provides an SRE team with the following:
• A technical, possibly quantitative, perspective on why they should change.
• A strong example of what change looks like.
• A logical explanation for much of the “folk wisdom” used by SRE.• The core principles needed to address novel situations in a scalable manner.
Your final task is to write an after-action report. This report should reiterate your per‐spective, examples, and explanation. It should also provide some action items for the team to ensure they exercise what you’ve taught them. You can organize the report as a postvitam,1 explaining the critical decisions at each step that led to success.1 In contrast to a postmortem.
Conclusion  |  423
The bulk of the engagement is now complete. Once your embedded assignment con‐cludes, you should remain available for design and code reviews. Keep an eye on the team for the next few months to confirm that they’re slowly improving their capacity planning, emergency response, and rollout processes.
424  |  Chapter 30: Embedding an SRE to Recover from Operational OverloadCHAPTER 31
Communication and Collaboration in SRE
Written by Niall Murphy with Alex Rodriguez, Carl Crous, Dario Freni, Dylan Curley, Lorenzo Blanco, and Todd Underwood Edited by Betsy Beyer
The organizational position of SRE in Google is interesting, and has effects on how we communicate and collaborate.To begin with, there is a tremendous diversity in what SRE does, and how we do it. We have infrastructural teams, service teams, and horizontal product teams. We have relationships with product development teams ranging from teams that are many times our size, to teams roughly the same size as their counterparts, and situations in which we are the product development team. SRE teams are made up of people with systems engineering or architectural skills (see [Hix15b]), software engineering skills, project management skills, leadership instincts, backgrounds in all kinds of industries (see Chapter 33), and so on. We don’t have just one model, and we have found a vari‐ety of configurations that work; this flexibility suits our ultimately pragmatic nature.It’s also true that SRE is not a command-and-control organization. Generally, we owe allegiance to at least two masters: for service or infrastructure SRE teams, we work closely with the corresponding product development teams that work on those serv‐ices or that infrastructure; we also obviously work in the context of SRE generally. The service relationship is very strong, since we are held accountable for the perfor‐mance of those systems, but despite that relationship, our actual reporting lines are through SRE as a whole. Today, we spend more time supporting our individual serv‐ices than on cross-production work, but our culture and our shared values produce strongly homogeneous approaches to problems. This is by design.11 And, as we all know, culture beats strategy every time: [Mer11].
425The two preceding facts have steered the SRE organization in certain directions when it comes to two crucial dimensions of how our teams operate—communications and collaboration. Data flow would be an apt computing metaphor for our communica‐tions: just like data must flow around production, data also has to flow around an SRE team—data about projects, the state of the services, production, and the state of the individuals. For maximum effectiveness of a team, the data has to flow in reliable ways from one interested party to another. One way to think of this flow is to think of the interface that an SRE team must present to other teams, such as an API. Just like an API, a good design is crucial for effective operation, and if the API is wrong, it can be painful to correct later on.The API-as-contract metaphor is also relevant for collaboration, both among SRE teams, and between SRE and product development teams—all have to make progress in an environment of unrelenting change. To that extent, our collaboration looks quite like collaboration in any other fast-moving company. The difference is the mix of software engineering skills, systems engineering expertise, and the wisdom of pro‐duction experience that SRE brings to bear on that collaboration. The best designs and the best implementations result from the joint concerns of production and the product being met in an atmosphere of mutual respect. This is the promise of SRE: an organization charged with reliability, with the same skills as the product development teams, will improve things measurably. Our experience suggests that simply having someone in charge of reliability, without also having the complete skill set, is not enough.Communications: Production Meetings
Although literature about running effective meetings abounds [Kra08], it’s difficult to find someone who’s lucky enough to only have useful, effective meetings. This is equally true for SRE.However, there’s one kind of meeting that we have that is more useful than the aver‐age, called a production meeting. Production meetings are a special kind of meeting where an SRE team carefully articulates to itself—and to its invitees—the state of the service(s) in their charge, so as to increase general awareness among everyone who cares, and to improve the operation of the service(s). In general, these meetings are service-oriented; they are not directly about the status updates of individuals. The goal is for everyone to leave the meeting with an idea of what’s going on—the same idea. The other major goal of production meetings is to improve our services by bringing the wisdom of production to bear on our services. That means we talk in detail about the operational performance of the service, and relate that operational performance to design, configuration, or implementation, and make recommendations for how to fix the problems. Connecting the performance of the service with design decisions in a regular meeting is an immensely powerful feedback loop.426  |  Chapter 31: Communication and Collaboration in SRE
Our production meetings usually happen weekly; given SRE’s antipathy to pointless meetings, this frequency seems to be just about right: time to allow enough relevant material to accumulate to make the meeting worthwhile, while not so frequent that people find excuses to not attend. They usually last somewhere between 30 and 60 minutes. Any less and you’re probably cutting something unnecessarily short, or you should probably be growing your service portfolio. Any more and you’re probably getting mired in the detail, or you’ve got too much to talk about and you should shard the team or service set.Just like any other meeting, the production meeting should have a chair. Many SRE teams rotate the chair through various team members, which has the advantage of making everyone feel they have a stake in the service and some notional ownership of the issues. It’s true that not everyone has equal levels of chairing skill, but the value of group ownership is so large that the trade-off of temporary suboptimality is worth‐while. Furthermore, this is a chance to instill chairing skills, which are very useful in the kind of incident coordination situations commonly faced by SRE.In cases where two SRE teams are meeting by video, and one of the teams is much larger than the other, we have noticed an interesting dynamic at play. We recommend placing your chair on the smaller side of the call by default. The larger side naturally tends to quiet down and some of the bad effects of imbalanced team sizes (made worse by the delays inherent in video conferencing) will improve.2 We have no idea if this technique has any scientific basis, but it does tend to work.Agenda
There are many ways to run a production meeting, attesting to the diversity of what SRE looks after and how we do it. To that extent, it’s not appropriate to be prescrip‐tive on how to run one of these meetings. However, a default agenda (see Appendix F for an example) might look something like the following:
Upcoming production changesUpcoming production changes 
Change-tracking meetings are well known throughout the industry, and indeed whole meetings have often been devoted to stopping change. However, in our production environment, we usually default to enabling change, which requires tracking the useful set of properties of that change: start time, duration, expected effect, and so on. This is near-term horizon visibility.2 The larger team generally tends to unintentionally talk over the smaller team, it’s more difficult to control 	distracting side conversations, etc.
Communications: Production Meetings  |  427
MetricsOne of the major ways we conduct a service-oriented discussion is by talking about the core metrics of the systems in question; see Chapter 4. Even if the sys‐tems didn’t dramatically fail that week, it’s very common to be in a position where you’re looking at gradually (or sharply!) increasing load throughout the year. Keeping track of how your latency figures, CPU utilization figures, etc., change over time is incredibly valuable for developing a feeling for the perfor‐mance envelope of a system.Some teams track resource usage and efficiency, which is also a useful indicator of slower, perhaps more insidious system changes.
Outages 
This item addresses problems of approximately postmortem size, and is an indis‐pensable opportunity for learning. A good postmortem analysis, as discussed in Chapter 15, should always set the juices flowing.
Paging eventsThese are pages from your monitoring system, relating to problems that can be postmortem worthy, but often aren’t. In any event, while the Outages portion looks at the larger picture of an outage, this section looks at the tactical view: the list of pages, who was paged, what happened then, and so on. There are two implicit questions for this section: should that alert have paged in the way it did, and should it have paged at all? If the answer to the last question is no, remove those unactionable pages.Nonpaging events 
	This bucket contains three items:
• An issue that probably should have paged, but didn’t. In these cases, you should probably fix the monitoring so that such events do trigger a page. Often you encounter the issue while you’re trying to fix something else, or it’s related to a metric you’re tracking but for which you haven’t got an alert.• An issue that is not pageable but requires attention, such as low-impact data corruption or slowness in some non-user-facing dimension of the system. Tracking reactive operational work is also appropriate here.
• An issue that is not pageable and does not require attention. These alerts should be removed, because they create extra noise that distracts engineers from issues that do merit attention.428  |  Chapter 31: Communication and Collaboration in SRE
Prior action items 
The preceding detailed discussions often lead to actions that SRE needs to take—fix this, monitor that, develop a subsystem to do the other. Track these improve‐ments just as they would be tracked in any other meeting: assign action items to people and track their progress. It’s a good idea to have an explicit agenda item that acts as a catchall, if nothing else. Consistent delivery is also a wonderful credibility and trust builder. It doesn’t matter how such delivery is done, just that it is done.Attendance
Attendance is compulsory for all the members of the SRE team in question. This is particularly true if your team is spread across multiple countries and/or time zones, because this is your major opportunity to interact as a group.The major stakeholders should also attend this meeting. Any partner product devel‐opment teams you may have should also attend. Some SRE teams shard their meeting so SRE-only matters are kept to the first half; that practice is fine, as long as everyone, as stated previously, leaves with the same idea of what’s going on. From time to time representatives from other SRE teams might turn up, particularly if there’s some larger cross-team issue to discuss, but in general, the SRE team in question plus major other teams should attend. If your relationship is such that you cannot invite your product development partners, you need to fix that relationship: perhaps the first step is to invite a representative from that team, or to find a trusted intermediary to proxy communication or model healthy interactions. There are many reasons why teams don’t get along, and a wealth of writing on how to solve that problem: this information is also applicable to SRE teams, but it is important that the end goal of having a feedback loop from operations is fulfilled, or a large part of the value of hav‐ing an SRE team is lost.Occasionally you’ll have too many teams or busy-yet-crucial attendees to invite. There are a number of techniques you can use to handle those situations:
• Less active services might be attended by a single representative from the product development team, or only have commitment from the product development team to read and comment on the agenda minutes.• If the production development team is quite large, nominate a subset of repre‐	sentatives.
• Busy-yet-crucial attendees can provide feedback and/or steering in advance to 	individuals, or using the prefilled agenda technique (described next).
Communications: Production Meetings  |  429Most of the meeting strategies we’ve discussed are common sense, with a service-oriented twist. One unique spin on making meetings more efficient and more inclu‐sive is to use the real-time collaborative features of Google Docs. Many SRE teams have such a doc, with a well-known address that anyone in engineering can access.
Having such a doc enables two great practices:• Pre-populating the agenda with “bottom up” ideas, comments, and information.
• Preparing the agenda in parallel and in advance is really efficient.
Fully use the multiple-person collaboration features enabled by the product. There’s nothing quite like seeing a meeting chair type in a sentence, then seeing someone else supply a link to the source material in brackets after they have finished typing, and then seeing yet another person tidy up the spelling and grammar in the original sen‐tence. Such collaboration gets stuff done faster, and makes more people feel like they own a slice of what the team does.Collaboration within SREObviously, Google is a multinational organization. Because of the emergency response and pager rotation component of our role, we have very good business rea‐sons to be a distributed organization, separated by at least a few time zones. The prac‐tical impact of this distribution is that we have very fluid definitions for “team”compared to, for example, the average product development team. We have local teams, the team on the site, the cross-continental team, virtual teams of various sizes and coherence, and everything in between. This creates a cheerfully chaotic mix of responsibilities, skills, and opportunities. Much of the same dynamics could be expected to pertain to any sufficiently large company (although they might be partic‐ularly intense for tech companies). Given that most local collaboration faces no par‐ticular obstacle, the interesting case collaboration-wise is cross-team, cross-site, across a virtual team, and similar.This pattern of distribution also informs how SRE teams tend to be organized. Because our raison d’être is bringing value through technical mastery, and technical mastery tends to be hard, we therefore try to find a way to have mastery over some related subset of systems or infrastructures, in order to decrease cognitive load. Spe‐cialization is one way of accomplishing this objective; i.e., team X works only on product Y. Specialization is good, because it leads to higher chances of improved technical mastery, but it’s also bad, because it leads to siloization and ignorance of the broader picture. We try to have a crisp team charter to define what a team will—and more importantly, won’t—support, but we don’t always succeed.430  |  Chapter 31: Communication and Collaboration in SRE
Team Composition
We have a wide array of skill sets in SRE, ranging from systems engineering through software engineering, and into organization and management. The one thing we can say about collaboration is that your chances of successful collaboration—and indeed just about anything else—are improved by having more diversity in your team. There’s a lot of evidence suggesting that diverse teams are simply better teams [Nel14]. Running a diverse team implies particular attention to communication, cog‐nitive biases, and so on, which we can’t cover in detail here.Formally, SRE teams have the roles of “tech lead” (TL), “manager” (SRM), and“project manager” (also known as PM, TPM, PgM). Some people operate best when those roles have well-defined responsibilities: the major benefit of this being they can make in-scope decisions quickly and safely. Others operate best in a more fluid envi‐ronment, with shifting responsibilities depending on dynamic negotiation. In gen‐eral, the more fluid the team is, the more developed it is in terms of the capabilities of the individuals, and the more able the team is to adapt to new situations—but at the cost of having to communicate more and more often, because less background can be assumed.Regardless of how well these roles are defined, at a base level the tech lead is responsi‐ble for technical direction in the team, and can lead in a variety of ways—everything from carefully commenting on everyone’s code, to holding quarterly direction pre‐sentations, to building consensus in the team. In Google, TLs can do almost all of a manager’s job, because our managers are highly technical, but the manager has two special responsibilities that a TL doesn’t have: the performance management func‐tion, and being a general catchall for everything that isn’t handled by someone else. Great TLs, SRMs, and TPMs have a complete set of skills and can cheerfully turn their hand to organizing a project, commenting on a design doc, or writing code as necessary.Techniques for Working Effectively
There are a number of ways to engineer effectively in SRE.
In general, singleton projects fail unless the person is particularly gifted or the prob‐lem is straightforward. To accomplish anything significant, you pretty much need multiple people. Therefore, you also need good collaboration skills. Again, lots of material has been written on this topic, and much of this literature is applicable to SRE.In general, good SRE work calls for excellent communication skills when you’re working outside the boundary of your purely local team. For collaborations outside the building, effectively working across time zones implies either great written com‐munication, or lots of travel to supply the in-person experience that is deferrable but
Collaboration within SRE  |  431Collaboration within SRE  |  431
ultimately necessary for a high-quality relationship. Even if you’re a great writer, over time you decay into just being an email address until you turn up in the flesh again.
Case Study of Collaboration in SRE: ViceroyOne example of a successful cross-SRE collaboration is a project called Viceroy, which is a monitoring dashboard framework and service. The current organizational architecture of SRE can end up with teams producing multiple, slightly different copies of the same piece of work; for various reasons, monitoring dashboard frame‐works were a particularly fertile ground for duplication of work.3The incentives that led to the serious litter problem of many smoldering, abandoned hulks of monitoring frameworks lying around were pretty simple: each team was rewarded for developing its own solution, working outside of the team boundary was hard, and the infrastructure that tended to be provided SRE-wide was typically closer to a toolkit than a product. This environment encouraged individual engineers to use the toolkit to make another burning wreck rather than fix the problem for the largest number of people possible (an effort that would therefore take much longer).The Coming of the ViceroyViceroy was different. It began in 2012 when a number of teams were considering how to move to Monarch, the new monitoring system at Google. SRE is deeply con‐servative with respect to monitoring systems, so Monarch somewhat ironically took a longer while to get traction within SRE than within non-SRE teams. But no one could argue that our legacy monitoring system, Borgmon (see Chapter 10), had no room for improvement. For example, our consoles were cumbersome because they used a custom HTML templating system that was special-cased, full of funky edge cases, and difficult to test. At that time, Monarch had matured enough to be accepted in princi‐ple as the replacement for the legacy system and was therefore being adopted by more and more teams across Google, but it turned out we still had a problem with consoles.Those of us who tried using Monarch for our services soon found that it fell short in its console support for two main reasons:
• Consoles were easy to set up for a small service, but didn’t scale well to services 	with complex consoles.
• They also didn’t support the legacy monitoring system, making the transition to 	Monarch very difficult.
3 In this particular case, the road to hell was indeed paved with JavaScript.432  |  Chapter 31: Communication and Collaboration in SREBecause no viable alternative to deploying Monarch in this way existed at the time, a number of team-specific projects launched. Since there was little enough in the way of coordinated development solutions or even cross-group tracking at the time (a problem that has since been fixed), we ended up duplicating efforts yet again. Multi‐ple teams from Spanner, Ads Frontend, and a variety of other services spun up their own efforts (one notable example was called Consoles++) over the course of 12–18 months, and eventually sanity prevailed when engineers from all those teams woke up and discovered each other’s respective efforts. They decided to do the sensible thing and join forces in order to create a general solution for all of SRE. Thus, the Viceroy project was born in mid 2012.By the beginning of 2013, Viceroy had started to gather interest from teams who had yet to move off the legacy system, but who were looking to put a toe in the water. Obviously, teams with larger existing monitoring projects had fewer incentives to move to the new system: it was hard for these teams to rationalize jettisoning the low maintenance cost for their existing solution that basically worked fine, for something relatively new and unproven that would require lots of effort to make work. The sheer diversity of requirements added to the reluctance of these teams, even though all monitoring console projects shared two main requirements, notably:• Support complex curated dashboards
• Support both Monarch and the legacy monitoring system
Each project also had its own set of technical requirements, which depended on the author’s preference or experience. For example:
• Multiple data sources outside the core monitoring systems
• Definition of consoles using configuration versus explicit HTML layout
• No JavaScript versus full embrace of JavaScript with AJAX• Sole use of static content, so the consoles can be cached in the browser
Although some of these requirements were stickier than others, overall they made merging efforts difficult. Indeed, although the Consoles++ team was interested in seeing how their project compared to Viceroy, their initial examination in the first half of 2013 determined that the fundamental differences between the two projects were significant enough to prevent integration. The largest difficulty was that Viceroy by design did not use much JavaScript, while Consoles++ was mostly written in Java‐Script. There was a glimmer of hope, however, in that the two systems did have a number of underlying similarities:Case Study of Collaboration in SRE: Viceroy  |  433
• They used similar syntaxes for HTML template rendering.
• They shared a number of long-term goals, which neither team had yet begun to address. For example, both systems wanted to cache monitoring data and sup‐port an offline pipeline to periodically produce data that the console can use, but was too computationally expensive to produce on demand.We ended up parking the unified console discussion for a while. However, by the end of 2013, both Consoles++ and Viceroy had developed significantly. Their technical differences had narrowed, because Viceroy had started using JavaScript to render its monitoring graphs. The two teams met and figured out that integration was a lot eas‐ier, now that integration boiled down to serving the Consoles++ data out of the Vice‐roy server. The first integrated prototypes were completed in early 2014, and proved that the systems could work well together. Both teams felt comfortable committing to a joint effort at that point, and because Viceroy had already established its brand as a common monitoring solution, the combined project retained the Viceroy name. Developing full functionality took a few quarters, but by the end of 2014, the com‐bined system was complete.Joining forces reaped huge benefits:
• Viceroy received a host of data sources and the JavaScript clients to access them.
• JavaScript compilation was rewritten to support separate modules that can be selectively included. This is essential to scale the system to any number of teams with their own JavaScript code.
• Consoles++ benefited from the many improvements actively being made to Vice‐	roy, such as the addition of its cache and background data pipeline.• Overall, the development velocity on one solution was much larger than the sum 	of all the development velocity of the duplicative projects.Ultimately, the common future vision was the key factor in combining the projects. Both teams found value in expanding their development team and benefited from each other’s contributions. The momentum was such that, by the end of 2014, Vice‐roy was officially declared the general monitoring solution for all of SRE. Perhaps characteristically for Google, this declaration didn’t require that teams adopt Viceroy: rather, it recommended that teams should use Viceroy instead of writing another monitoring console.Challenges