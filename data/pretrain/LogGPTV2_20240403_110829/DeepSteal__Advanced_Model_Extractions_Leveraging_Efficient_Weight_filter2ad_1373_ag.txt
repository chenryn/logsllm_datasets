### References
1. [82] V. Costan and S. Devadas, "Intel SGX Explained," IACR Cryptol. ePrint Arch., vol. 2016, no. 86, pp. 1–118, 2016.
2. [83] M. Orenbach, P. Lifshits, M. Minkin, and M. Silberstein, "Eleos: Exitless OS Services for SGX Enclaves," in European Conference on Computer Systems, 2017, pp. 238–253.
3. [84] Z. He, A. S. Rakin, and D. Fan, "Parametric Noise Injection: Trainable Randomness to Improve Deep Neural Network Robustness Against Adversarial Attack," in IEEE Conference on Computer Vision and Pattern Recognition, 2019.
4. [85] A. Krizhevsky, V. Nair, and G. Hinton, "CIFAR-10 (Canadian Institute for Advanced Research)," URL http://www.cs.toronto.edu/~kriz/cifar.html, 2010.
5. [86] A. Krizhevsky and G. Hinton, "Learning Multiple Layers of Features from Tiny Images," Citeseer, Tech. Rep., 2009.
6. [87] J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel, "Man vs. Computer: Benchmarking Machine Learning Algorithms for Traffic Sign Recognition," Neural Networks, no. 0, pp. –, 2012.
7. [88] S. Zagoruyko and N. Komodakis, "Wide Residual Networks," arXiv preprint arXiv:1605.07146, 2016.
8. [89] S. Xie, R. B. Girshick, P. Dollár, Z. Tu, and K. He, "Aggregated Residual Transformations for Deep Neural Networks," CoRR, vol. abs/1611.05431, 2016. [Online]. Available: http://arxiv.org/abs/1611.05431

### Appendix

#### A. DeepSteal Experimental Results: CIFAR-100 & GTSRB
We evaluate the DeepSteal attack on two larger datasets: CIFAR-100, which has 10 times more output classes than CIFAR-10, and the German Traffic Sign Recognition Benchmark (GTSRB), which has 12 times larger input dimensions than CIFAR-10. Table VI summarizes the results for these datasets.

For CIFAR-100, we attacked a ResNext-50-32x4d victim DNN model. The baseline method (i.e., architecture only) could recover the model accuracy up to 32.81%, which is approximately 36% lower than the ideal case (i.e., white-box). Our attack improved this baseline recovered accuracy by approximately 26% and the fidelity by approximately 31%. For GTSRB, our attack achieved an exact baseline accuracy with a high fidelity rate of 98.77%. This indicates that datasets with larger output classes, such as CIFAR-100, pose a significant challenge in recovering the test accuracy of the victim model.

In terms of adversarial input attacks, our substitute model achieved 6.6% accuracy under attack on CIFAR-100, showing similar attack efficacy (i.e., 0-6%) as a white-box attack while improving by 36% compared to the baseline. For GTSRB, our substitute model reduced the accuracy under attack by approximately 24% compared to the baseline. However, the attack efficacy still lags by 39% from the ideal case (i.e., white-box) due to the larger input image dimension (112×112). Adversarial images generated by the substitute model are less likely to succeed in attacking the victim model for larger input dimensions/search spaces.

#### B. Analysis of Data Availability
Access to a small portion of data (2-8% available data) to train a substitute model is a common practice, as seen in prior works [60], [61]. Our threat model assumes the attacker has access to a portion of the dataset (train/test), even if it is a tiny fraction. In this ablation study, we demonstrate how varying data availability affects adversarial attack performance (Table VII). The results show that the substitute model's accuracy and fidelity improve significantly with increased available training data. Even with only 2% training data, we could recover the accuracy of the substitute model up to approximately 80% (ResNet-18) and 76% (VGG-11). Generally, increasing data availability from 2-8% enhances the transferability of adversarial samples for both models. Note that 0% train data availability makes it a semi-supervised or unsupervised learning problem. Semi-supervised techniques [11] have their pros and cons in constructing a proper substitute model. However, exploring the 0% train availability case is beyond the scope of this work due to different experimental settings and threat models.

#### C. Further Analysis of the Recovered Bit Error
Figure 12 presents a bit error analysis for the VGG-11 model. Similar to ResNet-18, we observe a small fluctuation (e.g., 1-2%) in accuracy within the low bit error range (e.g., 0-4%). However, a notable difference for VGG-11 is that the accuracy under attack increases linearly with the increasing bit error rate. This result is expected, as VGG-11, being a dense model, shows higher resistance to adversarial examples. This observation supports the conclusion of prior works [67], [84] that denser models are more resilient to adversarial examples.

**Figure 12:** Theoretical analysis of the impact of the recovered Bit Error Rate (e) % on DeepSteal attack performance. In this ablation study, we vary the recovered bit error range between 0-10% and report the corresponding accuracy of the substitute model and accuracy under attack of the target model for VGG-11.

#### D. DeepSteal Attack for Full-Precision Models
Our proposed DeepSteal method can also be applied to floating-point/32-bit precision models. For example, the IEEE representation format of floating-point numbers contains the sign bit at the Most Significant Bit (MSB) location. Table VIII shows the results of adapting our algorithm to train the substitute model for a full-precision target model. The results indicate that our attack achieved 0.95% and 14.5% accuracy under attack for a full-precision target ResNet-18 and VGG-11 model, respectively. These results are similar to our prior results for 8-bit quantized models.

#### E. Comparison between Cross-Entropy Based Training and Algorithm-1
To quantify the effect of the proposed Mean Clustering loss term, we trained the substitute model using only cross-entropy loss by removing the second loss penalty in Equation (1). Table IX summarizes the effect of our second loss term during substitute model training. We trained two substitute models for both ResNet-18 and VGG-11: i) with cross-entropy training and ii) with the proposed Mean Clustering penalty (i.e., Algorithm-1). The results show that our proposed DeepSteal algorithm achieves a 7-8% improvement in accuracy and fidelity compared to traditional cross-entropy-based training for ResNet-18. The proposed Algorithm-1 also improves the adversarial attack efficacy by 26%. Similarly, for the VGG-11 model, our proposed Algorithm-1 improves the test accuracy of the substitute model by 3% and the accuracy under attack by 7%. This ablation study validates the effectiveness of our proposed Mean Clustering algorithm compared to cross-entropy training, which prior side-channel attacks [28] have adopted to recover weight information.

Additionally, the mean clustering penalty is only applied to weights with more significant bits recovered, leading to a known projected range where the penalty term guides such 'partially recovered weights' to be learned close to its projected range mean. For un-recovered less significant bits, there may be a bias due to the mean clustering penalty, but it is not necessarily 'undesired' for three main reasons: First, the mean clustering penalty is a 'soft' penalty with a small value of λ (e.g., 1e-5) in the loss function, favoring weights to converge near the known mean of the projected range based on the recovered partial bit info, not precisely equal to the mean. Second, during training, the cross-entropy loss (another loss term) and the gradient of the weights play a more important role in dictating the final convergence value of those weights, reducing the bias effect from the mean penalty loss to some extent. Third, our experimental results, such as accuracy under adversarial example attack, validate that such bias does not have a negative effect, as our DeepSteal achieves almost similar attack accuracy as the white-box attack using a recovered substitute model.

#### F. Dataset and Model Architecture Description
We used three visual datasets for object classification tasks: CIFAR-10 [85], CIFAR-100 [86], and the German Traffic Sign Recognition Benchmark (GTSRB) [87]. CIFAR-10 contains 60K RGB images of size 32 × 32, with 50K examples for training and 10K for testing. CIFAR-100 has 100 classes with 600 images each, and both CIFAR-10 and CIFAR-100 have the same image size and train-test data split. For the GTSRB dataset, we used 40k labeled images, splitting them in an 85-15 ratio for training and evaluation. Each traffic sign image is 112x112 and distributed across 42 different classes. For CIFAR-10 experiments, we used residual networks (e.g., ResNet-18/34, Wide-ResNet-28) [1], [88], and VGG-11 [38] architectures. For GTSRB and CIFAR-100, we adopted ResNet-18 and ResNext-50-32x4d [89] as the evaluation architectures, respectively. For all experiments, the weights of each victim model were quantized into 8 bits.

Following the standard practice in [60], [61], we assumed the attacker has access to a publicly available portion (i.e., ∼8%) of the train dataset. In our experimental setting, we used 4096 (∼8%) train samples to train the substitute model for both CIFAR-10 and CIFAR-100 datasets. For GTSRB, we used 2656 (∼8%) train samples to train the substitute model based on our proposed Mean Clustering training method. We followed similar data distribution and experimental settings for the baseline method (i.e., architecture-only case).

During the experiments, to train the substitute model for all different cases, we independently trained three individual models and reported the worst (e.g., accuracy) of the three rounds.