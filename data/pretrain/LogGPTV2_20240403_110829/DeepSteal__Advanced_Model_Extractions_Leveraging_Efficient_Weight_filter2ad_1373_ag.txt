instructions set and the sha-3 candidates,” in International Conference
on the Theory and Application of Cryptology and Information Security.
Springer, 2009, pp. 162–178.
[82] V. Costan and S. Devadas, “Intel sgx explained.” IACR Cryptol. ePrint
Arch., vol. 2016, no. 86, pp. 1–118, 2016.
[83] M. Orenbach, P. Lifshits, M. Minkin, and M. Silberstein, “Eleos: Exitless
os services for sgx enclaves,” in European Conference on Computer
Systems, 2017, pp. 238–253.
[84] Z. He, A. S. Rakin, and D. Fan, “Parametric noise injection: Train-
able randomness to improve deep neural network robustness against
adversarial attack,” in IEEE Conference on Computer Vision and Pattern
Recognition, 2019.
[85] A. Krizhevsky, V. Nair, and G. Hinton, “Cifar-10 (canadian institute for
advanced research),” URL http://www. cs. toronto. edu/kriz/cifar. html,
2010.
[86] A. Krizhevsky and G. Hinton, “Learning multiple layers of features from
tiny images,” Citeseer, Tech. Rep., 2009.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:59:25 UTC from IEEE Xplore.  Restrictions apply. 
1171
[87] J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel, “Man vs. computer:
Benchmarking machine learning algorithms for trafﬁc sign recognition,”
Neural Networks, no. 0, pp. –, 2012.
[88] S. Zagoruyko and N. Komodakis, “Wide residual networks,” arXiv
preprint arXiv:1605.07146, 2016.
[89] S. Xie, R. B. Girshick, P. Dollár, Z. Tu, and K. He, “Aggregated residual
transformations for deep neural networks,” CoRR, vol. abs/1611.05431,
2016. [Online]. Available: http://arxiv.org/abs/1611.05431
XI. APPENDIX
A. DeepSteal Experimental Results: CIFAR-100 & GTSRB
We evaluate DeepSteal attack on two relatively larger
datasets as well. We chose CIFAR-100, which has 10× larger
output class size than CIFAR-10, and the GTSRB dataset has
12× larger input dimension than CIFAR-10. In Table VI, we
summarize the results of these two datasets.
For CIFAR-100, we attack a ResNext-50-32x4d victim
DNN model. Here, the baseline (i.e., architecture only) method
can recover the model accuracy only up to 32.81% which is
∼36% lower than the ideal case (i.e., white-box). Our attack
can improve this baseline recovered accuracy by ∼26% and
also the ﬁdelity by ∼31%. Again, for GTSRB, our attack
can recover the exact baseline accuracy with a high ﬁdelity
rate 98.77%. It shows the dataset with a larger output class
(e.g., CIFAR-100) poses a tough challenge to recover the test
accuracy of the victim model.
As for adversarial input attack, our substitute model can
achieve 6.6% accuracy under attack on CIFAR-100. It shows
almost similar attack efﬁcacy (i.e., 0-6%) as a white-box attack
while gaining a 36% improvement compared to baseline. For
GTSRB, our substitute model can lower the accuracy under
attack by ∼24% compared to the baseline. However, the attack
efﬁcacy still lags by 39% from the ideal case (i.e., white-box)
mainly because of the input image dimension (112×112). As
an adversarial image generated by the substitute model is less
likely to succeed in attacking the victim model for a larger
input dimension/search space.
B. Analysis of Data Availability
Having access to a small portion of data (2-8% available
data) to train a substitute model is a common practice and
used in prior works [60], [61]. Following this practice, our
threat model assumes the attacker has access to a portion of
the dataset (train/test), even with a tiny fraction. In this ablation
study, we demonstrate how variable availability of data affect
the adversarial attack performance in table VII. It shows that
our substitute model’s accuracy and ﬁdelity improve signiﬁ-
cantly with the increase in available training data. However,
even with only 2% training data, we could recover the accuracy
of the substitute model up to ∼ 80% (ResNet-18) and ∼
76% (VGG-11). In general, increasing data availability from 2-
8% increases the transferability of the adversarial samples for
both models. Note that, 0% train data availability makes it a
semi-supervised or unsupervised learning domain problem. We
acknowledge that semi supervised techniques [11] may have
their own pros and cons in constructing a proper substitute
model. However, due to different experimental settings (i.e.,
access to unlabeled data) and threat model (i.e., model query
access), exploration of 0% train availability case is beyond the
scope of this work.
C. Further Analysis of the Recovered Bit Error
In ﬁg. 12, we have presented a bit error analysis of the
VGG-11 model. Similar to ResNet-18, we observe a small
ﬂuctuation (e.g., 1-2%) in accuracy within the low bit error
range (e.g., 0-4%). However, one notable difference for the
VGG-11 model is the accuracy under attack increases linearly
with increasing bit error rate. The result is expected, as being
a dense model, VGG-11 intrinsically shows much higher
resistance to adversarial examples. Such observation supports
the conclusion of prior works [67], [84] about denser models
being more resilient to adversarial examples.
Fig. 12: Theoretical analysis of the impact of the recovered
Bit Error Rate (e) % on DeepSteal attack performance. In this
ablation study, we vary the recovered bit error range between
0-10% and report the corresponding accuracy of the substitute
model and accuracy under attack of the target model for VGG-
11.
D. DeepSteal Attack for Full-Precision Models
Our proposed DeepSteal is a general method that could
also be applied to ﬂoating-point/32-bit precision models. For
example,
the IEEE representation format of ﬂoating-point
numbers contains the sign bit at the Most Signiﬁcant Bit
(MSB) location. In table VIII, we adapted our algorithm to
train the substitute model for a full-precision target model.
The result shows our attack could achieve 0.95% and 14.5%
accuracy under attack for a full-precision target ResNet-18 and
VGG-11 model, respectively. The result presented for both
models is similar to our prior results for the 8-bit quantized
models.
E. Comparison between Cross-Entropy based Training and
Algorithm-1
To quantify the effect of the proposed Mean Clustering
loss term, in this study, we only train the substitute model
with Cross-entropy loss by removing the second loss penalty
in eq. (1). In table IX, we summarize the effect of our
second loss term during substitute model training. Here We
train two substitute models for both ResNet-18 and VGG-
11: i) with cross-entropy training & ii) with proposed Mean
Clustering penalty (i.e., algorithm-1). The result shows our
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:59:25 UTC from IEEE Xplore.  Restrictions apply. 
1172
TABLE VI: Summary of DeepSteal attack performance on large-scale dataset (i.e., CIFAR-100: 100 output class & GTSRB:
112x112 input dimension). Our proposed DeepSteal outperforms the baseline (i.e., architecture only) case across all three
evaluation metrics even on large-scale image dataset. The improvements are shown in comparison to the baseline method.
Dataset
Method
Baseline
DeepSteal
White-Box
CIFAR-100
GTSRB
Accuracy
(%)
Fidelity
(%)
32.81
59.8 (↑ 26)
69.7
33.97
64.11(↑ 31)
100.0
Accuracy
under
Attack
(%)
42.7
6.61
0.0
Accuracy
(%)
Fidelity
(%)
98.67
99.57
99.05
98.01
98.77
100.0
Accuracy
Under
Attack
(%)
68.28
43.5(↓ 24)
4.32
TABLE VII: In this ablation study, we vary the percentage of
data available to the attacker between 1-8% and report the
results. As a baseline, we report the original architecture only
information case with 8% available data. For DeepSteal, here
we use 4000 rounds of HammerLeak attack information.
Training Data
(%)
Baseline (8%)
1
2
4
8
Baseline (8%)
1
2
4
8
73.18
71.22
79.21
84.86
89.05
70.76
68.66
75.8
80.81
84.59
Accuracy
(%)
Fidelity
(%)
ResNet-18
VGG-11
74.29
71.98
80.46
86.27
90.74
72.06
69.65
76.91
82.08
86.24
Accuracy Under
Attack (%)
61.33
44.47 ↓
24.74 ↓
8.56 ↓
1.94 ↓
61.19
51.91 ↓
38.82 ↓
25.96 ↓
16.87 ↓
TABLE VIII: This table presents the results after attacking a
full-precision model with 4000 rounds of HammerLeak. The
baseline is again the architecture information only case.
Training Data
(%)
Baseline
DeepSteal
Baseline
DeepSteal
Accuracy
(%)
Fidelity
(%)
ResNet-18 (93.17%)
73.2
72.2
89.78
91.6
VGG-11 (91.2%)
72.57
71.66
83.01
84.65
Accuracy Under
Attack (%)
53.94
0.95
61.0
14.5
proposed DeepSteal algorithm achieves a 7-8% improvement
in accuracy and ﬁdelity compared to the traditional cross-
entropy based training scheme for ResNet-18. The proposed
algorithm-1 also helps improve the adversarial attack efﬁcacy
by 26%. Similarly, for the VGG-11 model, our proposed
algorithm-1 improves the test accuracy of the substitute model
by 3% and under attack accuracy by 7%. This ablation study
validates the effectiveness of our proposed Mean Clustering
algorithm in comparison to cross-entropy training, which prior
side-channel attacks [28] have adopted to recover the weight
information.
In addition, the mean clustering penalty is only applied
to the weights with more signiﬁcant bits recovered, leading
to a known projected range, where the penalty term guides
−4, 5
such ‘partially recovered weights’ to be learned close to its
projected range mean. For those un-recovered less signiﬁcant
bits, we believe there would be a bias due to mean clustering
penalty, but may not be ‘undesired’ in this problem, for
three main reasons: First, the mean clustering penalty is a
−5) in the
‘soft’ penalty with a small value of λ (e.g., 1
loss function. This penalty term will favor the weights to
converge ‘near’ the known mean of the projected range based
on the recovered partial bit info, not precisely equal to the
mean. Second, during training, the cross-entropy loss (another
loss term) and the gradient of the weights will play a more
important role in dictating the ﬁnal convergence value of those
weights. Hence, the ﬁrst loss term should help reduce the bias
effect from the mean penalty loss to some extent. Third, our
experiment results, such as accuracy under adversarial example
attack, validate that such bias does not play a negative effect,
as our DeepSteal gets almost similar attack accuracy as the
white-box attack using a recovered substitute model.
TABLE IX: In this ablation study, we show the effectiveness of
our proposed loss function in algorithm-1 by comparing it with
Cross-Entropy loss training using the exact same HammerLeak
attack information (i.e., after 3000 rounds).
Loss Function
Cross-Entropy
Algorithm-1
Cross-Entropy
Algorithm-1
Accuracy
(%)
Fidelity
(%)
ResNet-18
78.81
86.32 (↑ 8)
80.25
87.86 (↑ 7)
VGG-11
78.65
81.03 (↑ 3)
79.84
82.88 (↑ 3)
Accuracy Under
Attack (%)
31.26
5.24 (↓ 26)
43.03
36.45 (↓ 7)
F. Dataset and Model Architecture Description
We take three visual datasets: CIFAR-10 [85], CIFAR-
100 [86] and German Trafﬁc Sign Recognition Benchmark
(GTSRB) [87] for object classiﬁcation task. CIFAR-10 con-
tains 60K RGB images in size of 32 × 32. Following the
standard practice, 50K examples are used for training and the
remaining 10K for testing. On the other hand, CIFAR-100 has
100 classes with 600 images in each class. Both CIFAR-10
and CIFAR-100 has same image size and test-train data split.
For GTSRB dataset (result in appendix), we use 40k labeled
images and split them 85-15 ratio for training and evaluation
purposes. Each trafﬁc sign image has a size of 112x112 and is
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:59:25 UTC from IEEE Xplore.  Restrictions apply. 
1173
distributed in 42 different classes. For CIFAR-10 experiments,
we used residual networks (e.g., ResNet-18/34,Wide-ResNet-
28) [1], [88] and VGG-11 [38] architectures. For GTSRB
and CIFAR-100, we adopted ResNet-18 and ResNext-50-
32x4d [89] as the evaluation architecture respectively. For all
experiments, the weights of each victim model are quantized
into 8 bit.
Following the standard practice in [60], [61], we assume
the attacker has access to a publicly available portion (i.e.,
∼8%) of the train dataset. In our experimental setting, we used
4096 (∼8%) train samples to train the substitute model for
both CIFAR-10 & CIFAR-100 dataset. For GTRSB, we only
used 2656 (∼8%) train samples to train the substitute model
based on our proposed Mean Clustering training method. We
follow similar data distribution and experimental settings for
the baseline method (i.e., architecture only case) as well.
During the experiments, to train the substitute model for all
different cases, we train three individual models independently
and report the worst (e.g., accuracy) of the three rounds.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:59:25 UTC from IEEE Xplore.  Restrictions apply. 
1174