but matches the target after scaling.
Horizontal and Vertical Optimization. Common imaging
libraries, such as OpenCV or Pillow, implement downscaling
by ﬁrst resizing images horizontally and then vertically. This
implementation technique enables approximating the scaling
operation from Eq. (1) by a closed-form expression which is
based on a simple matrix multiplication:
D = scale(S + ∆) = L· (S + ∆)· R
(2)
with L ∈ Rm(cid:48)×m, R ∈ Rn×n(cid:48) and D ∈ Rm(cid:48)×n(cid:48). The matrices L
and R contain ﬁxed coefﬁcients that depend on the selected
scaling algorithm. Both matrices can be computed in advance
and are reusable. We refer to Xiao et al. [35] for a description
how to calculate L and R.
Based on this matrix multiplication, the attack can also
be decomposed into a horizontal and vertical manipulation,
which are conducted in reverse order to the scaling, as shown
in Figure 3. The attack proceeds by ﬁrst computing a resized
version of S, that is, S(cid:48) = scale(S) ∈ Rm×n(cid:48) . Here, we solve
Eq. (1) with S(cid:48) as source image and T as target. Due to the
decomposition, we only need the coefﬁcient matrix L and
thus arrive at the following optimization problem
min((cid:107)∆(cid:48)(cid:107)2
2) s.t. (cid:107)L·
(3)
Next, the horizontal direction is considered. To this end, the
adversary calculates the ﬁnal attack image A with S as source
image, but A(cid:48) as target, analogue to Eq. (3).
− T(cid:107)∞ (cid:54) ε .
(cid:0)S(cid:48) + ∆(cid:48)(cid:1)
Figure 3: Libraries resize an image horizontally ﬁrst, and then vertically. The
attack creates A in reverse order: ﬁrst the intermediate image A(cid:48) , and then A.
Column-based Optimization. In order to further decrease
the computational effort, the optimization can be further de-
composed into individual dimensions. We start again with
the vertical scaling direction where we resize S(cid:48) ∈ Rm×n(cid:48) to
D∈ Rm(cid:48)×n(cid:48). Instead of considering the whole matrix, we solve
the problem from Eq. (3) for each column of S(cid:48) separately:
(4)
where the subscript in X∗, j speciﬁes the j-th matrix column
of a matrix X. This optimization is repeated for the horizontal
direction and ﬁnally computed for all color channels.
− T∗, j(cid:107)∞ (cid:54) ε ,
2) s.t. (cid:107)L·
∗, j + ∆(cid:48)
∗, j
min((cid:107)∆(cid:48)
(cid:0)S(cid:48)
∗, j(cid:107)2
(cid:1)
3 Attack Analysis
After introducing the background of image-scaling attacks,
we are ready to investigate their inner workings in more de-
tail. Our aim is to ﬁnd out which vulnerability image-scaling
attacks exactly exploit to be successful. We start off by ob-
serving that the presented attacks must exploit a vulnerability
that is shared by many scaling algorithms. As the implemen-
tations of the algorithms differ, this vulnerability needs to be
linked to the general concept of scaling. To better grasp this
concept, we require a broader perspective on image scaling
and thus examine it from the viewpoint of signal processing.
3.1 Scaling as Signal Processing
Images can be viewed as a generic signal, similar to audio and
video. While audio is described by a one-dimensional time
series, an image represents a discrete and two-dimensional
signal. Typically, images are encoded in the spatial domain
of pixels. However, any signal can be described by a sum of
sinusoids of different frequencies, and hence images can also
be represented in the frequency domain [e.g., 19, 29].
Scaling reduces the dimension of an image. As a result,
the frequency mixture of the image changes and higher fre-
quencies are lost. This process is closely related to downsam-
pling in signal processing, where a high-frequency signal is
transformed to a lower frequency. A major problem of down-
sampling is that the reduced resolution might not be able to
describe all relevant frequencies in the image. According
to the Nyquist–Shannon theorem [19], it is only feasible to
reconstruct a signal s(t) from a discrete number of sampled
points, if the sampling rate fT is at least twice as high as the
highest frequency fmax in the signal: fT ≥ 2· fmax.
If the frequency fT is below that threshold, the signal can-
not be unambiguously reconstructed. In this case, the sampled
points do not provide enough information to distinguish be-
tween the original signal and other possible signals. Figure 4
shows an example of this phenomenon, where it is impossible
to decide which one of the two signals s(t) and ˆs(t) is de-
scribed by the sampled points. Ultimately, the reconstructed
signal can differ signiﬁcantly from the original signal, which
is known as the aliasing effect [19]. As we see in the next
sections, image-scaling attacks build on this very effect by
cleverly manipulating a signal, such that its downsampled
version becomes a new signal.
Figure 4: An example of an undersampled signal s(t). Based on the sampling
points, it is not possible to distinguish between s(t) and ˆs(t).
1366    29th USENIX Security Symposium
USENIX Association
AA0Dmnmn0m0n0Horiz.Vert.AttackimagegenerationDownscalingDirections(t)ˆs(t)3.2 Scaling and Convolution
It is clear that scaling algorithms do not merely reduce the
frequencies in an image. These algorithms carefully interpo-
late the pixels of the source image before downscaling it in
order to mitigate the aliasing effect. This computation can be
described as a convolution between the source signal and a
kernel function [19]. For each position in the scaled image,
the kernel combines a set of pixels (samples) from the source
using a speciﬁc weighting. All scaling algorithms given in
Table 1 can be expressed using this concept.
Without loss of generality, we focus on the horizontal scal-
ing of a single row in the following, that is, a row s ∈ Rn from
the source image is scaled to d ∈ Rn(cid:48). We denote by β the
respective scaling ratio: β = n/n(cid:48). The goal of downscaling
is to determine the value for each pixel in d from a set of
samples from s. This process can be described using a kernel
function w as follows
(s (cid:63) w)(t) =
w (t − u)s(u).
(5)
∞
∑
u=−∞
Intuitively, w represents a weighting function that is moved
over s as a sliding window. We denote the size of this window
as the kernel width σ. Each pixel within this window is
multiplied by the respective weight at this position. Figure 5
exempliﬁes this process for a bilinear kernel with σ = 2. The
ﬁrst pixel in d is the aggregated result from the third and
fourth pixel in s, while the second pixel in d is only estimated
from the seventh pixel in s.
Figure 5: Scaling with convolution. The triangle illustrates the kernel with
its relative weighting. It has a width of 2 and is shifted by a step size of β.
As the downscaling of an image produces a smaller number
of pixels, the window of the kernel function needs to be shifted
on s by a speciﬁc step size, similar to the process of sampling
in signal processing. The scaling ratio deﬁnes this step size
so that each sampling position is given by
g(p) = p· β,
(6)
where p is the target pixel in d and g(p) a position in s around
which we place the kernel window. Note that the position
g(p) is not necessarily discrete and can also fall between two
pixels, as shown in Figure 5. The downscaled output image
is then computed as follows:
dp = (s (cid:63) w)(g(p))
p = 0,1, . . . ,n(cid:48).
(7)
Figure 6: Visualization of kernel functions using in scaling algorithms.
Each scaling algorithm is deﬁned by a particular kernel
function. Figure 6 depicts the standard kernels for common
scaling algorithms. For instance, nearest-neighbor scaling
builds on the following kernel function:
w(x) =
for − 0.5 (cid:54) x < 0.5,
otherwise .
(8)
(cid:40)
1
0
Only the value that is the closest to g(p) is used by this scaling
algorithm. In other words, nearest-neighbor scaling simply
copies pixels from s on a discrete grid to d. Overall, each
kernel differs in the number of pixels that it uses and the
respective weighting of the considered pixels.
3.3 Root-Cause Analysis
Based on our insights from signal processing, we can start
to investigate the root cause of image-scaling attacks. We
observe that not all pixels in the source image equally con-
tribute to its scaled version. Only those pixels close to the
center of the kernel receive a high weighting, whereas all
remaining pixels play a limited role during scaling. If the step
size exceeds the kernel width, some pixels are even ignored
and irrelevant for the scaling operation. Figure 5 illustrates
this situation: Only three out of nine pixels are considered for
computing the scaled output.
This imbalanced inﬂuence of the source pixels provides a
perfect ground for image-scaling attacks. The adversary only
needs to modify those pixels with high weights to control
the scaling and can leave the rest of the image untouched.
This strategy is sufﬁcient for achieving both objectives of
the attack: (O1) a modiﬁcation of pixels with high weights
yields scale(A) ∼ T , and (O2) depending on the sparsity of
those pixels the attack image A visually matches the source
image S.
From the perspective of signal processing, image-scaling
attacks can thus be interpreted as targeted aliasing, where the
adversary selectively manipulates those regions of the signal
that are sampled during downscaling. These regions create a
high-frequency signal in the source image that is not visible
in the spatial domain but precisely captures the sampling rate
of the downscaling process.
We can deduce that the success of image-scaling attacks
depends on the sparsity of pixels with high weight. If these
USENIX Association
29th USENIX Security Symposium    1367
123456789xsandw12xs?w0.5·(s[3]+s[4])1·s[7]−4−3−2−10123400.51xw(x)NearestBilinearBicubicLanczos4Areapixels are dense, the adversary may still achieve objective
O1 but will fail to satisfy O2, as the attack becomes visible.
Reviewing the general concept of scaling, we identify two
factors that determine the sparsity of these pixels: the scaling
ratio β and the kernel width σ. For images, we formally bound
the ratio r of pixels that are considered during scaling by
r ≤ (βh βv)−1 (σh σv).
(9)
The terms βh, βv as well as σh and σv denote the respective
scaling ratio and kernel width horizontally and vertically. If
the direction is irrelevant, we consider quadratic images for
our analysis and use β and σ for both axis. Moreover, note that
the right term may exceed one if the windows of the kernels
overlap and pixels in the source are considered multiple times.
Scaling ratio. The larger the ratio β, the fewer pixels are
considered during scaling if the kernel width is bounded. In
particular, the number of pixels that are discarded growths
quadratically with β. An adversary can thus easily control the
ratio r by increasing the size of the source image.
Figure 7(a)-(c) show the inﬂuence of the scaling ratio on
the attack for a kernel with σ = 1. All images fulﬁll objec-
tive O1, that is, the images are scaled down to the “cat” image.
Depending on the scaling ratio, however, their success to
objective O2 changes. For a large ratio of β = 4, the attack
image looks like the source, and the cat is not visible. For a
smaller scaling ratio, the manipulated image becomes a mix
of the source and target. For β = 1, the attack obviously fails.
Kernel width. The smaller the kernel width σ, the fewer
pixels are considered during each convolution. While σ is
typically not controlled by the adversary, several implementa-
tions of scaling algorithms make use of very small constants
for this parameter. For example, the nearest-neighbor, bilin-
ear, and bicubic kernels of the TensorFlow framework have a
width of 1, 2, and 4, respectively.
Figure 7(d)-(f) depict the inﬂuence of the kernel width
on the attack for a ﬁxed scaling ratio of β = 4. Again, all
images fulﬁll objective O1 and are scaled down to the “cat”
image. For σ = 1, the attack also satisﬁes objective O2 and is
invisible. If two pixels are considered by the kernel, however,
the cat becomes visible. For σ = 4, all pixels need to be
manipulated and the attack fails.
Interestingly, our analysis is not limited to the scaling algo-
rithms considered in this work. Any algorithm is vulnerable
to image-scaling attacks if the ratio r of pixels with high
weight is small enough. Our analysis thus allows developers
to check quickly if their algorithms are vulnerable to these
attacks. Overall, we are thus the ﬁrst to provide a general
understanding of this attack type in practice. This understand-
ing enables us to compare different scaling algorithms and
ultimately develop effective defense strategies.
Figure 7: Inﬂuence of the scaling ratio and kernel size (see Figure 2 for the
setting of this example); β and σ are the same horizontally and vertically.
Plot (a)–(c) show manipulated images under varying ratios. Plot (d)–(f)
show manipulated images under varying kernel sizes. The symbols and 
indicate if the attack is successful.
4 Defenses
We continue with the development of defenses that build
on our analysis and address the root cause of image-scaling
attacks—rather than ﬁxing their symptoms. Our defenses
aim to prevent attacks without interfering with the typical
workﬂow of deep learning frameworks. They can thus serve
as a plug-in for existing scaling algorithms. Note that the
mere detection of attacks is not sufﬁcient here, as the systems
would need to cope with rejected inputs.
Consequently, we ﬁrst derive requirements for secure scal-
ing and use these to validate the robustness of existing al-
gorithms (Defense 1). As only a few algorithms realize a
secure scaling, we proceed to develop a generic defense that
reconstructs the source image and thereby is applicable to any
scaling algorithm as preprocessing (Defense 2).
4.1 Attacker Model
For the construction and evaluation of our defenses, we con-
sider two types of adversaries: a non-adaptive adversary who
uses existing image-scaling attacks, and an adaptive adversary
who is aware of our defense and adapts the attack strategy
accordingly. Both adversaries have full knowledge of the
scaling algorithm and the target size. In the adaptive scenario,
the adversary additionally has full knowledge of the applied
defense. Finally, we expect the adversary to freely choose the
source and target image so that she can ﬁnd the best match
for conducting attacks in a given setup.
We note that these assumptions are realistic due to the
open-source nature of deep learning frameworks and the use
of several well-known learning models in practice, such as
VGG19 and Inception V3/V4. With black-box access to the
scaling and learning models, an adversary can even deduce
the scaling algorithm and target size by sending a series of
specially crafted images to the learning system [see 35].
1368    29th USENIX Security Symposium
USENIX Association
(a)β=4(b)β=1.3(c)β=1(d)σ=4(e)σ=2(f)σ=14.2 Defense 1: Robust Scaling Algorithms
Let us start with the conception of an ideal robust scaling algo-
rithm which serves as a prototype for analyzing the properties
of existing algorithms.
An ideal scaling algorithm. In the ideal case, an algorithm
investigates each pixel of the source image at least once for
downscaling. The robustness of the scaling increases further
if the employed convolution kernels overlap, and thus one
pixel of the source contributes to multiple pixels of the scaled
version. Technically, this requirement can be realized by
dynamically adapting the kernel width σ to the scaling ratio β,
such that σ ≥ β holds. That is, the larger the ratio between
the source and the scaled image, the wider the convolution
kernel needs to become to cover all pixels of the image.
In addition to processing all pixels, an ideal algorithm also
needs to weight all pixels equally; otherwise, a kernel with
small support would leave pixels untouched if their weights
become zero. For example, pixels close to the edge of the
convolution window typically receive a very low weighting,
as shown in Figure 6. As a result, the convolution of an ideal
algorithm should be uniform and combine all pixels in the
current kernel window with equal weight.
Although both properties—considering all pixels and a
uniform convolution—can be technically implemented, they
introduce challenges that can limit their practical utility: First,
processing all pixels of an image slows down the scaling
process. This is not necessarily a problem in applications
where large neural networks are trained, and the overhead of
scaling is minimal anyway. However, in real-time settings,
it might be prohibitive to go over all pixels during scaling.
Second, the ﬂattened weighting of the convolution can blur the
image content and remove structure necessary for recognizing