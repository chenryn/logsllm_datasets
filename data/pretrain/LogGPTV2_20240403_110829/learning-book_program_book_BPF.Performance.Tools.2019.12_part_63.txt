452
 Chapter 10 Networking
$sk = (stxuct sock *)axg0;
$inet_fanily = $sk=>_sk_common.skc_fanily
if ($inet_fanily == AF_INET 11 $inet_fanily == AF_IMET6) (
f(o) dosu = xppep$
(0) dou = xppes$
f ($1net_fanily == AF_INET) [
$daddr = ntop I$sk->_8k_common,skc_daddc) 
$saddz = ntop I§sk->__sk_conmon,skc_rcv_saddx) 
) else ↑
|doqu = xppep$
$sk->_sk_conmon.skc_v6_daddr,in6_u,u6_addr8)
Idoss = zppe5s
$sk->__sk_conmn,ske_v6_rev_sadde, in6_o,u6_addrB) 
$1port = $sk->_sk_conmon,skc_num:
$dpoxt = $sk->__ak_conmon.skc_dport;
// Destinatlon poxt ls blg endlan, it must be fllpped
$dport = ($dport >> 8) 1 (($dport __sk_conmcn.akc_state:
Satatestr = ptcp_atates[Sstate
tine (*5H:5X:5S *)
printf (*g8d %14s:3-6d $14s:3-6d 6sn*, pid, $saddr, $lport,
$daddr, Sdport, $statestr):
END
clear (8tcp_states) 
This version traces the tcp_retransmit_skb() kernel function. On Linux 4.15,
tcp:tcp_retransmit_skb and tcp:tcp_retransmit_synack tracepoints were added, and this tool can
be updated to use them.
---
## Page 490
10.3 BPF Tools
453
10.3.17 tcpsynbl
tcpsynbl(8)32 traces the TCP SYN backlog limit and size, showing a histogram of the size measured
each time the backlog is checked. For example, on a 48-CPU production edge server:
tepsynb1.bt
Attach.ing 4 probes..
Tracing SYN backlog slze. Ctrl-C to end.
C
zts 6otoeg Io wueabostu 1[1tuTt botxoeq]6otoeq)
pback1og[128[ :
[01
21869889889886 8889889889869869889889886 888988988986986 
: [00s]eoeq@
[0]
2T83 1eee88e88e88e88ee8ee8ee8ee8ee88e88e88e88ee8ee8ee8ee8e1
[1]
16
[2, 4)
4 1
[4, 8)
1 1
The first histogram shows that a backlog of limit 128 had two connections arrive, where the
backlog length was 0. The second histogram shows that a backlog limit of 500 had over two
thousand connections arrive, and the length was usually zero, but sometimes reached the four
to eight range. If the backlog exceeds the limit, this tool prints a line to say that a SYN has been
dropped, which causes latency on the client host as it must retransmit.
This backlog size is tunable, and is an argument to the listen(2) syscall:
int llsten (int sockfd, int backlog)
It is also truncated by a system limit set in /proc/sys/net/core/somaxconn.
This tool works by tracing new connection events, and checking the limit and size of the backlog.
The overhead should be negligible, as these are usually infrequent compared to other events.
The source to tcpsynbl(8) is:
+1/usr/local/bin/bpft.race
include 
BEGIN
version on 19-Apr-2019.
33 This tool contains a workaround for an int casting problem: & O×I ffCffff, This should becom
a later version of bpftrace.
---
## Page 491
454
Chapter 10 Networking
printf(*Tracing SYN backlog size. Ctr1-C to end. .n*);
kprobe : tcp_v4_syn_recv_sock,
kprobe:tcg_v6_syn_recr_sock
(.6a (￥ oo2 qon.2) = xo0s$
Bbacklog[sock->sk_nax_ack_backlog & Oxffrrrcf] =
hist ($sock=>sk_ack_backlog) 
1f ($sock=>sk_ack_backlog > $sock->sk_nax_ack_backlog)↑
tine (*eH:3M:3S drogping a SYI, n*) 
END
printf (*nfbacklog[backlog linit] : histogram of backlog slzen*) 
If the backlog exceeds the limit, the time( builtin is used to print a line of output containing the
time, and a message that a SYN was dropped. This was not seen in the previous production output
as the limit was not exceeded.
10.3.18 tcpwin
tcpwin(8)3 traces the TCP send congestion window size and other kernel parameters, so that the
performance of congestion control can be studied. This tool proxduces comma-separated value
output for importing into graphing software. For example, running tcpwin.bt and saving the
output to a text file:
 tepwin.bt > out.tcpwin01. txt
+ nore out.topwin01.txt
Attaching 2 pzobes..
event., sock, time_us, snd_cvnd, snd_ss thresh., sk_sndbuf, sk_xmem_queued
rcv, 0x[ff9212377a9800, 409985, 2, 2, 87040, 2304
rcv, 0xffff9216fe306e80, 534689, 10, 2147483647, 87040, 0
rcv, 0xCff92180f84c000, 632704, 7, 7, 87040, 2304
rcv, 0×ffff92180b04f800, 674795, 10, 2147483647, 87040, 2304
[..-]
34 Origin: 1 created this on 20-Apr-2019, inspired by the tcp_probe module and the many times I've seen it used fo
graphing congestion window size over time.
---
## Page 492
10.3 BPF Tools
455
The second line of output is a header line, and the following are event details. The second field is
the sock struct address, which can be used to uniquely identify connections. The awk(1) utility
can be used to frequency count these sock adresses:
 avk -F, '$1 == "zev* { a[$2]++ 1 END [ for (s in a} { pzint s, a[s] 1 }'
out.tcpwin01.txt
[..-]
0xffff92166fede000 1
0xfCff92150a03c800 4564
0xff9213db2d6600 2
[. - -]
This shows that the socket with the most TCP receive events while tracing had the address
Oxfff92150a03c800. Events for this address only, and the header line, can also be extracted by
awk to a new file, out.csv:
 avk -F, '$2 == "0xffff92150a03c800" 11 MR == 2' out.tcpwin01.txt > out.csv
This CSV file was imported into the R statistics software and plotted (see Figure 10-5).
10
2
Time (s)
Figure 10-5  TCP congestion window and send buffer over time
This system is using the cubic TCP congestion control algorithm, showing an increase in send
congestion window size and then a sharp drop when congestion is encountered packet loss). This
occurs several times, creating a sawtooth pattern, until an optimal window size is found.
---
## Page 493
456
Chapter 10 Networking
The source to tcpwin(8) is:
#1/usx/local/bin/bpEtrace
#include 
include 
BEGIN
printf (*event., sock, time_us, snd_cvnd, snd_ssthresh., sk_sndbuf, *1 
print:f (*sk_wnen_queued’,n") 
kprobe:tcp_rev_established
63e (x snd_cvnd, Stcpa=>snd_ssthresh, $sock=>ak_sndbuf,
$sock->sk_wmen_queued) 
This can be extended. The first field is the event type, but only *rcv? is used by this tool. You
can add more kprobes or tracepoints, each with its own event string to identify it. For example,
an event type *new* could be added when sockets are established, with fields to identify the IP
addresses and TCP ports.
uaoau qprqa 'aqosddo'sissreue ponuoo uopsauoo jo ad.s stq rog pasn sem anpotu pouag V
q o uamau aq u po (md au 9t xnur  aqoddod uoaoen e aaq se
based on this tracepoint, although not all socket details are visible from the tracepoint arguments.
10.3.19 tcpnagle
duration of transmit delays as a histogram: these delays are caused by nagle and other events. For
example, on a production edge server:
+topnagle.bt
Attaching 4 pzobes..
Tracing TCP nagle and xmit delays. Hit Ctrl-C to end.
35 0rigin: I created it for this book on 23-Apr-201.9
---
## Page 494
10.3 BPF Tools
457
1sn"xo)
12, 4)
3 1eee8ee8ee88e 88ee8ee8eeeeeeee8ee8ee88e88eeeeeeeeeeeee 1
[4, 8]
988688688688688688688888868868868817
enagle [CoRK] : 2
enagle [OFF|POSH] : 5
nagle [0N]: 32
enagle [PUSH] : 11418
enagle [0eF] : 226697
During tracing, this showed that nagle was often off perhaps because the application has
called a setsockopt(2) with TCP_NODELAY) or set to push (perhaps because the application is
using TCP_CORK). Only five times were transmit packets delayed, for at most the four to eight
microsecond bucket.
This works by tracing the entry and exit of a TCP transmit function. This can be a frequent
function, so the overhead may become noticeable on high network throughput systems.
The source to tcpnagle(8) is:
1/usz/local/bin/bpftrace
BEGIN
printf (*Tracing TCP nagle and xnit delays. Hit Ctel-C to end.n*);
// from Include/net/tcp-hj add nore conbinatlons If needed:
O。 = [0x0]se[38
eflags[0x1] = 0FF";
xx0>, - (2x0]se138
fxH00|a20。 -[Ex0]sbe128
HSod。 - [x0]s&e[g8
eflags[0x5] = OFF FUSH*;
kprobe:tcp_vrite_xnit
Bnagle[eflags[arg2]] = count(1 
gsk[t1d] - axg0}
kretprobe:tcp_vrite_xnit
/Bsk [t.d] /
inf1ight = retva1 s Oxff;
[pT]xg = x$
---
## Page 495
458
 Chapter 10 Networking
if ($inflight &6 1Bstart[$sk] ][
estaxt[$sk] = nsecs,
1f (!$infllght s& Bstaxt[Ssk]] [
eblocked_us = hist ((nsecs - 9start[$ak]) / 1000) ;
delete (Bstart [$sk])
delete (@sk[t1d]) ;
END
clear (8flags) : c1esr (Bstart) : clear (9sk) 
On the entry to tcp_write_xmit0, the nonagle flags (arg2) are converted to a readable string via
the @flags lookup map. A sock struct point is also saved, as it is used in the kretprobe for saving
timestamps with a connection for measuring the duration of transmit delays. The duration is
measured from the first time tcp_write_xmit() returns non-zero (which shows that for some
reason it did not send the packets; the reason may include nagle), to when tcp_write_xmit() next
successfully sent packets for that socket.
10.3.20 udpconnect
uxlpconnect(8)3° traces new UDP connections initiated from the local host that use connect(2)
(this does not trace unconnected UDP). For example:
+ udpconnect.bt
Attaching 3 prsbes...
TIME
PID
COMM
IP RADCR
RPORT
6E09 8185102
05# 39-s9g SNI
4
10,45.128,25
53
20 :58:38 2621
TaskSchedulerFo
4
127,0.0.53
53
20 :58 :39 3876
Chrone_IoThread
6 2001:4860 :4860::8888 53
[..]
This shows two connections, both to remote port 53, one from a DNS resolver, and the other from
Chrome_IOThread.
This works by tracing the UDP connection functions in the kernel. Their frequency should be low,
making the overhead negligible.
36 Origin: 1 created it for this book on 20-Apr-2019.
---
## Page 496
10.3 BPF Tools
459
The source to udpconnect(8) is:
#1/usx/1ocal/bin/bpEtrace
#Include 
BEGIX
printf (*s=8s 5=6s 16s =2a 1=16s 1=5sn*, *7IME*, "PID*, *c0MM*,
*IP*,*RADDR*, *RPORI*;
kprobe :ip4_da tagran_connect,
kprobe1ip6_da tagran_connect
$s8 = (struct sockaddr *)argl:
1f ($sa=>sa_fan11y == AF_INET 11 $sa=>sa_fam1ly == AF_INET6) (
tine *&H:RM:s *)
f (sa=>sa_fanily == AF_INET) 
$s = (struct sockaddr_in *)argl;
sport - (Ss=>sin_port >> 8) 1
[($s=>sin_port sin_addr,s_addr) , $port) 
]else |
$s6 = (struct sockaddr_in6 *)argl;
$port = ($s6=>s1n6_poxt >> B) 1
[($s6=>sin6_port sin6_addr,in_u,u_addr8)
 (<xodg
The ip4_datagram_connect( and ip6_datagram_connect0 functions are the connect members of
the udp_prot and udpv6_prot structs, which define the functions that handle the UDP protocol.
Details are printed similarly to earlier tools.
Also see socketio(8) for a tool that shows UDP sends and receives by process. A UDP-specific one
can be coded by tracing udp_sendmsg0 and udp_recvmsg0, which would have the benefit of
isolating the overhead to just the UDP functions rather than all the socket functions.
---
## Page 497
460
Chapter 10 Networking
10.3.21
gethostlatency
gethostlatency(8) is a BCC and bpftrace tool to trace host resolution calls (DNS) via the resolver
library calls, getaddrinfo(3), gethostbyname(3), etc. For example:
 gethostlatency
TIME
PID
6urd1ts5Z
COMMI
LATns H0ST
13: 52 : 39
9.65 www,netflix.con
13 :52 : 42
25519
ping
2.64 ww,netflix.con
13:52 : 49
24989
712
43.09 docs-google.con
13:52: 52
25527
ping
99.26 mw, cilium. 10
ES+SE
19025
DNS Res~ez 70 9
2.58 drive.google,com
13 :53 : 05
21903
ping
279. 09 ww, kubernetes i10
13:53:0625459TaskSchedulezFo
23.8T www.lnformit,com
[. .-]
This output shows the latencies of various resolutions system-wide. The first was the ping(1)
command resolving www.netflix.com, which took 9.65 milliseconds. A subsequent lookup
took 2.64 milliseconds (likely thanks to caching). Other threads and lookups can be seen in the
output, with the slowest a 279 ms resolution of www.kubernetes.io.33
This works by using user-level dynamic instrumentation on the library functions. During a
uprobe the host name and a timestamp is recorded, and during a uretprobe the duration is
calculated and printed with the saved name. Since these are typically low-frequency events, the
overhead of this tool should be negligible.
DNS is a common source of production latency. At Shopify, the bpftrace version of this tool
was executed on a Kubernetes cluster to characterize a DNS latency issue in proxduction. The
data did not point to an issue with a certain server or target of the lookup, but rather latency
when many lookups were in flight. The issue was further debugged and found to be a cloud
limit on the number of UDP sessions that could be open on each host. Increasing the limit
resolved the issue.
BCC
Command line usage:
[suoqdo]  ousqetq oqga6
fquo @lI ssaoosd auo aoen o °1a d s! papoddns Apuauno uogdo Ajuo au
37 0rigin: Icreted a similr tol alled getaddrinfo.d fr the 2011 DTrsce book [Gre 11]. I oreated the BCC version
on 28-Jan-2016 and the bpfrace version on 8-Sep-2018.
location of the .io name servers [112].
38 Slow DNS times for the .lo domain from the United States Is a known problem, believed to be due to the hosting
---
## Page 498
10.3 BPF Tools
461
bpftrace
The following is the code for the bpftrace version, which does not support options:
#1/usr/local/bin/bpftrace
BEGIN
1
printf(*Tracing getaddr/gethost cal1s... Bit Ctr]-C to end.\o*) 
printf(*=9s -6s 16s 6s s\n*, *TIHE", "PID", "COMM", LATns*,
*H05T*)
uprobe:/1ib/x86_6411nux=gnu/1Lbc,so. 6:getaddx1nfo,
uprobe:/1ib/x86_641inux=gnu/libc, so 61gethostbyname,
uprobe:/1<b/x86_6411nux=gnu/11be, so, 6: gethostbynane2
Bstart[tid] = nsecs]
0bxe = [pT+]aueug
uzetprobe:/11b/xB6_641inux=gnu/11bc,so . 5:getaddz1nfo,
uretprobe:/1ib/xB6_641inux-gnu/libc so 6:gethostbyname,
uzetpxobe:/11b/x86_641inux=gnu/1lbc,so 6: gethostbynane2
/[p]xes8/
$latns = (nsecs - Bstaxt[tid]1 / 10o0000;
time I"%B:M:s *)
printf (*sEd s-16s s6d 5s^n", pld, corn, $latns, atr (8nane [tld]11
delete (estart[tid]) 
delete (@nane [t.d]1
The different possible resolver calls are traced from libc via its /lib/x86_64-linux-gnu/libc.so.6 loca-
tion. If a different resolver library is used, or if the functions are implemented by the application, or
statically included (static build), then this tool will need to be modified to trace those other locations
10.3.22 ipecn
ipecn(8)* traces IPv4 inbound explicit congestion notification (ECN) events, and is a proof of
concept tool. For example:
ipeen.bt
Attach.ing 3 probes.. 
Tracing 1nbound IPv4 ECN Congestion Encountered. Hit Ctxl-C to end.
39 Origin: I created it for tis book on 28-May-2019, based on a suggestion from Sargun Dhillon
---
## Page 499
462
Chapter 10 Networking
10 :11:02 ECN CE from: 100.65.76 .247
10 :11:02 ECN CE fxon: 100.65.76.247
10:11:03 ECv CE from: 100.65.76 .247
10:11:21 ECN CE fxon: 100.65.76.247