title:IO-DSSE: Scaling Dynamic Searchable Encryption to Millions of Indexes
By Improving Locality
author:Ian Miers and
Payman Mohassel
IO-DSSE: Scaling Dynamic Searchable Encryption
to Millions of Indexes By Improving Locality
Ian Miers
The Johns Hopkins University Department of Computer Science
PI:EMAIL
Payman Mohassel
Visa Research∗
PI:EMAIL
Abstract—Free cloud-based services are powerful candidates
for deploying ubiquitous encryption for messaging. In the case of
email and increasingly chat, users expect the ability to store and
search their messages persistently. Using data from a major mail
provider, we conﬁrm that for a searchable encryption scheme
to scale to millions of users,
it should be highly IO-efﬁcient
(locality) and handle a very dynamic message corpi. We observe
that existing solutions fail to achieve both properties simulta-
neously. We then design, build, and evaluate a provably secure
Dynamic Searchable Symmetric Encryption (DSSE) scheme with
signiﬁcant reduction in IO cost compared to preceding works
when used for email or other highly dynamic material.
I.
INTRODUCTION
The last few years have seen incredible success in deploy-
ing seamless end-to-end encryption for messaging. Between
iMessage, WhatsApp, and SRTP for video, over 2 billion
users routinely encrypt their messages without necessarily even
noticing. Unfortunately, this trend has been largely limited to
ephemeral or semi-ephemeral communication mediums. Where
users expect messages to be available for later recall, we have
seen little progress.
The quintessential example of this is email, where messages
are searched for months or years after being received. But email
is not the only medium ﬁlling this role: Google Hangouts, Slack,
and Facebook Messenger all operate in an archive and search
paradigm—indeed the latter two are to some extent used as
an email substitute because of this. In this setting, end-to-end
encryption is not seamless: by using it, users lose the ability
to store messages in the cloud and search. Since users are
unwilling to sacriﬁce features for security, this is a major
impediment to deploying end-to-end encryption.
Symmetric Searchable Encryption (SSE) [5, 7, 8, 10, 15, 19]
provides a potential solution to this problem as it allows a client
to outsource an encrypted index of documents (e.g. emails)
to an untrusted server and efﬁciently search the index for
speciﬁc keywords. The efﬁciency of SSE stems from its use
of fast symmetric-key operations and its privacy guarantees
∗ Work done while at Yahoo Labs.
Permission  to  freely  reproduce  all  or  part  of  this  paper  for  noncommercial 
purposes  is  granted  provided  that  copies  bear  this  notice  and  the  full  citation 
on  the  ﬁrst  page.  Reproduction  for  commercial  purposes  is  strictly  prohibited 
without the prior written consent of the Internet Society, the ﬁrst-named author 
(for  reproduction  of  an  entire  paper  only),  and  the  author’s  employer  if  the 
paper  was  prepared  within  the  scope  of  employment.
NDSS  ’17,  26  February  -  1  March  2017,  San  Diego,  CA,  USA
Copyright  2017  Internet  Society,  ISBN  1-891562-46-0
http://dx.doi.org/10.14722/ndss.2017.23394
typically allow for some information leakage on search/access
patterns which is captured using a leakage function. The
dynamic variants of SSE (DSSE) [4, 13, 14, 20] also allow
for efﬁcient updates to the encrypted database. The question is
whether dynamic SSE can be used efﬁciently in a cloud-scale
application?
Several works [4, 5] have addressed scaling SSE schemes
to a very large index—terabytes of data—but to the best of our
knowledge, none have examined deploying millions of small
indexes on shared hardware. This is the exact problem faced
by a cloud provider wishing to deploy search for encrypted
messaging or email without deploying large amounts of new
hardware.
Since SSE schemes typically use fast symmetric cryp-
tography, the primary performance issue for SSE is IO and
speciﬁcally poor locality: unlike a standard inverted index where
the list of document IDs for a given keyword k can be stored in
a single location, SSE schemes typically place each document
ID for a given keyword in a distinct random location in order
to hide the index structure. This results in a large increase in
IO usage, since searching for a keyword found in, e.g., 500
documents results in 500 random reads, rather than one single
read retrieving a list of 500 document IDs. Similarly, inserting
a batch of, e.g., 100 documents containing some 150 total
keywords requires 100× 150 = 15, 000 writes, rather than 150
writes, one per keyword.
While the same locality issues arise in SSE schemes for
very large indexes, the solution space is far more constrained
in our setting and the main techniques introduced in the
most promising approaches [4, 5] are not applicable for purely
dynamic indexes (i.e. indexes that are initially empty). As a
result, deploying existing SSE schemes for search for mail or
messaging—where every entry is inserted dynamically into an
initially empty index—-would result in an order of magnitude
increase in IO usage. Since existing mail search is already IO
bound, the cost of doing so is prohibitive. This is a marked
departure from the cost of deploying end-to-end encryption for
ephemeral communication, which comes at little operational
cost.
Unfortunately, this increased cost cannot be handled by
simply using better hardware. First, high performance storage
systems are prohibitively expensive relative to proﬁt from
free communications services. Second, caching is relatively
ineffective as each individual index is used infrequently. Third,
even given cost effective storage (e.g. much cheaper SSDs)
and the willingness of providers to upgrade infrastructure, any
infrastructure-related improvement yields similar cost savings
IO to insert n documents
with k keywords
Static
O(k)
O(k)
Dynamic
index type
Unencrypted Index
O(k)
Cash et. al [4]
O(nk)
Stefanov et. al [20] O(nk) O(nk)
This work1
O(k)
O(k)
IO for a search
returning n documents
Dynamic
Static
O(1)
O(1)
O(1)
O(nk)
O(nk) O(nk)
O(1)
O(1)
supports deletes
yes
dynamic only
yes
partial dynamic
TABLE I: Informal description of IO costs of SSE schemes assuming n, the number of documents mapped to a given keyword,
is smaller that the block size used for storage. When inserting n documents containing k keywords into an existing index, all
existing SSE schemes write each posting (i.e. keyword, document ID pair) to a distinct random location. As a result, when adding
n documents to the index entry for a given keyword, there are n writes to distinct random locations. This is repeated for each
of the k keywords. For search, returning the complete entry in the index for a given keyword requires each of the n random
locations be read. When used for messaging or email, all insertions and searches are done against the dynamic index and any
savings in the static case do not apply.
for non-encrypted search. Existing SSE schemes simply require
considerably more resources than unencrypted search.
We now examine in detail the speciﬁc challenges for email
and then detail our proposed solution.
A. Why Email is Different
Cloud-based mail offers a unique setting. First, while storage
at this scale is cheap, access to it is not: existing mail search is
already IO bound. As a result, we must minimize the IO cost
of maintaining the index. As we will see, this is the driving
design constraint behind our work.
Second, unlike traditional settings for SSE, there is no
initial corpus of documents to index. Instead all documents
(i.e. sent and received emails) are added after database creation
via updates2—the typical user receives between 30 and 200
messages a day.3 This exacerbates the IO problem, since all
existing schemes end up performing one random write to the
disk per keyword for each new email and one random read per
document ID returned in a search. This is in marked contrast
to a standard index wherein multiple entries are packed into
one block and read/written in one shot, resulting in efﬁcient
IO usage.
Third, the query rate is exceedingly low. For Yahoo webmail,
we see on the order of 250 searches per second total across
all users on mail content from on the order of a few hundred
million monthly active users. Searches on public metadata such
as sender, date sent, “has attachment”, etc. are far more frequent,
but searches on mail content, i.e., what would be stored in an
encrypted index, are surprisingly rare. Many schemes (e.g. [20])
assume hundreds of queries per second, where it is economical
to load the index into memory. At an average two queries per
inbox per month, storing the index in memory is neither cost
effective nor remotely practical given the sheer number of users
each needing their own separate index.
If the IO problems are resolved, however, the prospects for
encrypted mail search are actually fairly good: search for email
1We assume the obliviously updatable index is of ﬁxed size and thus the
overhead it imposes is constant. This is the case if the key word list is ﬁxed
or the server has limited space. Otherwise there is a cost that is logarithmic in
the size of the OUI but independent of total index size or search frequency.
2Since the server knows the contents of existing (unencrypted) email, adding
existing unencrypted email to the index is unnecessary and for some schemes
ill-advised.
3The mean is 30 with a standard deviation of 148.
is in many ways easier than what is typically asked of encrypted
search. We analyze detailed usage numbers furnished by Yahoo
Webmail. We conclude that searchable encryption for cloud-
based email is surprisingly plausible in terms of functionality
if cost is disregarded: the size of the datasets are manageable,
search is not frequently used, and most queries are exceedingly
simple. In particular, the combination of single keyword plus
intersection on public metadata (date, sender, “has attachment”),
seems to cover the vast majority of searches. While conjunctive
queries would of course be an improvement, they are not strictly
necessary. Moreover the availability of cheap storage coupled
with the fact that search is infrequent, means the cost of index
entries for deleted ﬁles is relatively small. This allows us to
eschew many of the complexities of fully dynamic searchable
encryption because we can afford to delete entries on a best-
effort basis. The scalability issues become even easier when
one considers that end-to-end encrypted mail is likely personal
and the majority of email is not. As a result, only a small
fraction of mail will need to be encrypted or indexed using
SSE.
Why existing schemes fail Encrypted databases such as Blind
Seer [16], Cryptdb [17] and others aim to provide complex
private queries as a privacy-preserving replacement to traditional
large database applications. Just as those database clusters are
unsuited for mail search, so too are their privacy-preserving
counterparts.
The DSSE scheme of Stefanov et al. [20] provides impres-
sive privacy protections including efﬁcient deletion of entries,
but as each individual index entry (i.e. a mapping from a word
to a single document) is stored in a random location, the IO
expansion relative to standard search is very large. This is not
scalable in the context of deploying hundreds of millions of
indexes on a few thousand servers.
This problem is not isolated to Stefanov et al.’s scheme.
Indeed, Cash et al. [4] note “One critical source of inefﬁciency
in practice (often ignored in theory) is a complete lack of
locality and parallelism: To execute a search, most prior
SSE schemes sequentially read each result from storage at
a pseudorandom position, and the only known way to avoid
this while maintaining privacy involves padding the server index
to a prohibitively large size.” [4]. Although one can arrive at
this from varying perspectives (disk IO, latency, or memory
requirements), it is a central problem that renders all previous
schemes unusable in this context.
2
Fig. 1: IO-DSSE logical architecture: an index mapping keywords to document IDs is stored locally, then the largest entries
overﬂow into the obliviously updatable index. When the entry for a keyword in the OUI is full, index entries are inserted into the
Full Block Index in chunks. Keyword searches query all three indexes. Note: encryption and the obliviously updatable index
construction hide this view from the server.
While Cash and Tessaro [6] show there is a fundamental
trade-off between locality and index size, in many settings
a large increase in overall performance can be obtained by
allowing a small blow-up in index size. Cash et al. [4] offer
such an improvement: instead of storing each index entry at
a random location on disk, they store entries sequentially in
ﬁxed-size blocks, drastically reducing the number of disk reads
by allowing many index entries for a given keyword to be
retrieved at once. Unfortunately, this technique cannot readily
be used to handle updates. The act of appending a new index
entry to an existing (partially-ﬁlled) block leaks signiﬁcant
information on updates (which are frequent). This means these
techniques are not applicable to entries inserted into the index
after the initial index creation.
As a result, while Cash et al. make use of an efﬁcient on-disk
index for the starting document corpus, all updates to the index
are stored in memory because each index entry is written to a
distinct random location. The scheme is not designed to deal
with frequent updates in an IO-efﬁcient manner and assumes
that most of the corpus is indexed statically, at initialization.
Thus, when the scheme is used for dynamic updates, it incurs
the very same locality issues the authors identify. Bridging this
gap between locality and privacy for highly dynamic indexes
is the goal of our construction. See Table I for details.
B. Our Contribution
A logical extension of Cash et al.’s scheme is to buffer
partial blocks locally (client-side) and only upload to the server
once the block is full. However our experiments in Section IV-C
indicate that a storage-limited client (e.g. a mobile device) will
overﬂow its local storage in less than 100 days even for the
average user. A second approach is to ofﬂoad this buffer to the
server in a way that hides when it is updated but still allows the
buffer to be searched. Oblivious RAM (ORAM) [2, 9, 11, 12, 18]
is a natural choice. Index entries would be buffered in a small
ORAM cache and then written to the full index.
ORAM, however, is not an ideal choice. First, in most
SSE schemes, access patterns for search are already leaked,
yet in ORAM we must pay for the full ORAM overhead not
just for reads and writes associated with index updates, but
for searches as well.4 Second, ORAM must remain secure
for arbitrary access patterns, while we only need to obscure
batched updates to the index (many emails/keywords can be
batched and inserted in the index at the same time).
Thus, in Section III, we construct an obliviously updatable
index (OUI), which provides for ORAM-like properties for
updates to the index (both reads and writes) but allows simple
non-oblivious reads for search. An overview of this approach
is given in Figure 1. This is accomplished by using a weaker
security requirement tailored to the SSE leakage function.
Through optimizations and batching, we achieve a 94% IO
savings vs. the generic approach using Path ORAM [21].
To summarize our contributions:
1) We examine the requirements and feasibility of search on
encrypted email and more generally, the class of IO-bound
DSSE schemes that exhibit a low query rate and high
update rate. This examination is supported with realistic
data from a large webmail provider.
2) Motivated by IO-efﬁcient SSE for highly dynamic corpi,