get operation that truncates the number of elements in a feed, the request rate
limits, that restricts the number of extra requests that may be done from our
middleware to obtain an element that is missing, and having no place reserved
to store the metadata associated to an element (e.g., an entry in a data object).
We believe this is something that makes sense to be included in the future by
the services, to provide application developers the ﬂexibility to associate extra
information to the application data. Since most of the services already return
metadata associated to a post, this appears to be an easy and useful feature to
be supported by these services.
The middleware layer that we developed executes on the client side, and in-
tercepts all interactions of the client with the online service, and avoids making
extra requests when something is missing. We have presented four algorithms
that enforce each of the well known session guarantees. Furthermore, our algo-
rithms follow a structure that allows to combine them. We also have developed
a prototype in Java that we used to evaluate our approach using two services:
Facebook, and a geo-replicated deployment of Redis. Our experiments showed
that we can enforce session guarantees with a modest overhead both in terms of
user-perceived latency and communication with the centralized service. Note
98
5.1. FUTURE WORK
that our middleware is not limited to be used by these services, it can be used
by services that provide a read/write interface compatible with the middleware
interface. This work was published in SRDS2017 [38].
5.1 Future work
While the results presented in this thesis provide the tools and results for a
better understanding of the consistency anomalies that are prevalent in today’s
online services, while also proposing a client-side (generic) approach to enrich
the consistency guarantees of such services, we believe that there are some
interesting venues for future research:
The measurement study was made with four services: Facebook Feed, Face-
book Group, Google+, and Blogger. Extending the study to more services like
Instagram or Linkedin can enrich the results that were provided and give a
broader view of the consistency guarantees eﬀectively oﬀered by these services.
Another important extension to our study, is to detect anomalies in internal
components of service when the access to these components are available. In
this case, instead of considering only black box tests we can apply our method-
ology to perform white-box testing to large-scale storage systems.
Our middleware layer enforces the four session guarantees, it might be rel-
evant to ensure to application developers other consistency properties, prop-
erties that can be enforced on top of these service without a signiﬁcant cost to
applications, e.g., the guarantee that two clients do not diverge or the guarantee
that the divergence window is bounded and does not aﬀect availability.
We chose to support the main operations provided by these services, how-
ever some services provide other operations. Extending the middleware to
support more operations, e.g., feed pagination, can be useful for application de-
velopers and a great challenge if we take into account the restrictions imposed
by services in terms of number of API calls issued in limited time windows.
99
Bibliography
[1] D. Abadi. “Consistency tradeoﬀs in modern distributed database system
design: CAP is only part of the story.” In: IEEE Computer 45.2 (2012),
pp. 37–42.
[2] D. Agrawal, A. El Abbadi, and R. C. Steinke. “Epidemic algorithms
in replicated databases.” In: Proceedings of the sixteenth ACM SIGACT-
SIGMOD-SIGART symposium on Principles of database systems. ACM.
Tucson, Arizona, USA, 1997, pp. 161–172.
[3] M. Ahamad, G. Neiger, J. E. Burns, P. Kohli, and P. W. Hutto. “Causal
memory: Deﬁnitions, implementation, and programming.” In: Springer
Distributed Computing 9.1 (1995), pp. 37–49.
[4] B. F. C. et. al. “Benchmarking Cloud Serving Systems with YCSB.” In:
Proceedings of the 1st ACM Symposium on Cloud Computing. ACM. Indi-
anapolis, Indiana, USA, 2010, pp. 143–154.
[5] S. Almeida, J. Leitão, and L. Rodrigues. “ChainReaction: a causal+ con-
sistent datastore based on chain replication.” In: Proceedings of the 8th
ACM European Conference on Computer Systems. ACM. Prague, Czech
Republic, 2013, pp. 85–98.
[6] Amazon Elastic Compute Cloud (Amazon EC2). https://aws.amazon.
com/ec2 (accessed Mar-2019).
[7] Amazon Simple Database. https : / / aws . amazon . com / simpledb (ac-
cessed Mar-2019).
[8] Amazon Simple Queue Service. https : / / aws . amazon . com / sqs/ (ac-
cessed Mar-2019).
101
BIBLIOGRAPHY
[9] Amazon Simple Storage Service. https:/aws.amazon.com/s3 (accessed
Mar-2019).
[10] E. Anderson, X. Li, M. A. Shah, J. Tucek, and J. J. Wylie. “What con-
sistency does your key-value store actually provide?” In: Proceedings of
the Sixth Workshop on Hot Topics in System Dependability. Vancouver, BC,
Canada, 2010, pp. 1–16.
[11] Apache HBase. https://hbase.apache.org/ (accessed Mar-2019).
[12] Apache HTTP Server. https://httpd.apache.org (accessed Mar-2019).
[13] H. Attiya, F. Ellen, and A. Morrison. “Limitations of highly-available
eventually-consistent data stores.” In: IEEE Transactions on Parallel and
Distributed Systems 28.1 (2017), pp. 141–155.
[14] Azure Databases. https : /azure . microsoft . com / en - us / product -
categories/databases (accessed Mar-2019).
[15] P. Bailis, S. Venkataraman, M. J. Franklin, J. M. Hellerstein, and I. Stoica.
“Probabilistically bounded staleness for practical partial quorums.” In:
Proceedings of the VLDB Endowment 5.8 (2012), pp. 776–787.
[16] P. Bailis, A. Ghodsi, J. M. Hellerstein, and I. Stoica. “Bolt-on causal
consistency.” In: Proceedings of the 2013 ACM SIGMOD International
Conference on Management of Data. ACM. New York, USA, 2013, pp. 761–
772.
[17]
J. Baker, C. Bond, J. C. Corbett, J. Furman, A. Khorlin, J. Larson, J.-M.
Leon, Y. Li, A. Lloyd, and V. Yushprakh. “Megastore: Providing scal-
able, highly available storage for interactive services.” In: Fifth Biennial
Conference on Innovative Data Systems Research. CA, USA, January, 2011.
[18] N. M. Belaramani, M. Dahlin, L. Gao, A. Nayate, A. Venkataramani, P.
Yalagandula, and J. Zheng. “PRACTI Replication.” In: Proceedings of the
3rd Conference on Networked Systems Design and Implementation. USENIX.
San Jose, California, USA, 2006, pp. 5–5.
102
BIBLIOGRAPHY
[19] D. Bermbach and S. Tai. “Eventual Consistency: How Soon is Eventual?
An Evaluation of Amazon S3’s Consistency Behavior.” In: Proceedings
of the 6th Workshop on Middleware for Service Oriented Computing. ACM.
Lisbon, Portugal, 2011, 1:1–1:6.
[20] D. Bermbach, J. Kuhlenkamp, B. Derre, M. Klems, and S. Tai. “A Mid-
dleware Guaranteeing Client-Centric Consistency on Top of Eventually
Consistent Datastores.” In: Cloud Engineering (IC2E), 2013 IEEE Inter-
national Conference on. IEEE. San Francisco, CA, USA, 2013, pp. 114–
123.
[21] Blogger API. https://developers.google.com/blogger// (accessed
Mar-2019).
[22] M. Brantner, D. Florescu, D. Graf, D. Kossmann, and T. Kraska. “Build-
ing a Database on S3.” In: Proceedings of the 2008 ACM SIGMOD Inter-
national Conference on Management of Data. ACM. Vancouver, Canada,
2008, pp. 251–264.
[23] M. Bravo, L. Rodrigues, and P. Van Roy. “Saturn: A distributed metadata
service for causal consistency.” In: Proceedings of the Twelfth European
Conference on Computer Systems. ACM. Belgrade, Serbia, 2017, pp. 111–
126.
[24] E. A. Brewer. “Towards robust distributed systems.” In: Proceedings of
the Nineteenth Annual ACM Symposium on Principles of Distributed Com-
puting. Portland, Oregon, USA, 2000.
[25] N. Bronson, Z. Amsden, G. Cabrera, P. Chakka, P. Dimov, H. Ding, J. Fer-
ris, A. Giardullo, S. Kulkarni, H. Li, et al. “TAO: Facebook’s Distributed
Data Store for the Social Graph.” In: 2013 USENIX Annual Technical
Conference. San Jose, CA, USA, 2013, pp. 49–60.
[26]
J. Brzezinski, C. Sobaniec, and D. Wawrzyniak. “From session causality
to causal consistency.” In: Proceedings of the 12th Euromicro Conference on
Parallel, Distributed and Network-Based Processing. Coruna, Spain, 2004,
pp. 152–158.
103
BIBLIOGRAPHY
[27] Cassandra Database. https://cassandra.apache.org (accessed Mar-
2019).
[28] B. F. Cooper, R. Ramakrishnan, U. Srivastava, A. Silberstein, P. Bohannon,
H.-A. Jacobsen, N. Puz, D. Weaver, and R. Yerneni. “PNUTS: Yahoo!’s
hosted data serving platform.” In: Proceedings of the VLDB Endowment
1.2 (2008), pp. 1277–1288.
[29]
J. C. Corbett, J. Dean, M. Epstein, A. Fikes, C. Frost, J. J. Furman, S. Ghe-
mawat, A. Gubarev, C. Heiser, P. Hochschild, et al. “Spanner: Google’s
globally distributed database.” In: ACM Transactions on Computer Sys-
tems (TOCS) 31.3 (2013), p. 8.
[30] F. Cristian. “Probabilistic clock synchronization.” In: Springer Distributed
computing 3.3 (1989), pp. 146–158.
[31] S. B. Davidson, H. Garcia-Molina, and D. Skeen. “Consistency in a par-
titioned network: a survey.” In: ACM Computing Surveys (CSUR) 17.3
(1985), pp. 341–370.
[32] G. DeCandia, D. Hastorun, M. Jampani, G. Kakulapati, A. Lakshman,
A. Pilchin, S. Sivasubramanian, P. Vosshall, and W. Vogels. “Dynamo:
amazon’s highly available key-value store.” In: ACM SIGOPS Operating
Systems Review 41.6 (2007), pp. 205–220.
[33] Facebook API. https://developers.facebook.com/docs/apis-and-
sdks (accessed Mar-2019).
[34] Facebook API - groups feed. https://developers.facebook.com/docs/
graph-api/reference/v3.2/group/feed// (accessed Mar-2019).
[35] Facebook API - news feed. https://developers.facebook.com/docs/
graph-api/reference/v3.0/user/home// (accessed Mar-2019).
[36] Facebook Service. https://www.facebook.com (accessed Mar-2019).
[37] F. Freitas, J. Leitao, N. Preguiça, and R. Rodrigues. “Characterizing the
Consistency of Online Services (Practical Experience Report).” In: De-
pendable Systems and Networks (DSN), 2016 46th Annual IEEE/IFIP Inter-
national Conference on. IEEE. Toulouse, France, 2016, pp. 638–645.
104
BIBLIOGRAPHY
[38] F. Freitas, J. Leitão, N. Preguiça, and R. Rodrigues. “Fine-Grained Con-
sistency Upgrades for Online Services.” In: Reliable Distributed Systems
(SRDS), 2017 IEEE 36th Symposium on. IEEE. Hong Kong, 2017, pp. 1–
10.
[39] S. Gilbert and N. Lynch. “Brewer’s conjecture and the feasibility of con-
sistent, available, partition-tolerant web services.” In: ACM Sigact News
33.2 (2002), pp. 51–59.
[40] Google API. https://developers.google.com/ (accessed Mar-2019).
[41] Google App Engine. https://cloud.google.com/appengine (accessed
Mar-2019).
[42] M. P. Herlihy and J. M. Wing. “Linearizability: A correctness condition
for concurrent objects.” In: ACM Transactions on Programming Languages
and Systems (TOPLAS) 12.3 (1990), pp. 463–492.
[43]
Jedis Redis Library. https://github.com/xetorthio/jedis (accessed
Mar-2019).
[44] M. E. Khan, F. Khan, et al. “A comparative study of white box, black box
and grey box testing techniques.” In: Int. J. Adv. Comput. Sci. Appl 3.6
(2012).
[45] R. Ladin, B. Liskov, L. Shrira, and S. Ghemawat. “Providing high avail-
ability using lazy replication.” In: ACM Transactions on Computer Systems
(TOCS) 10.4 (1992), pp. 360–391.
[46] A. Lakshman and P. Malik. “Cassandra: a decentralized structured stor-
age system.” In: ACM SIGOPS Operating Systems Review 44.2 (2010),
pp. 35–40.
[47] L. Lamport. “Time, clocks, and the ordering of events in a distributed
system.” In: Communications of the ACM 21.7 (1978), pp. 558–565.
[48] L. Lamport. “On interprocess communication, Part 1: basic formalism,
Part II: algorithms.” In: Distributed Computing. v1 i2 (1986), pp. 77–101.
105
BIBLIOGRAPHY
[49] W. Lloyd, M. J. Freedman, M. Kaminsky, and D. G. Andersen. “Don’t
settle for eventual: scalable causal consistency for wide-area storage with
COPS.” In: Proceedings of the Twenty-Third ACM Symposium on Operating
Systems Principles. ACM. Cascais, Portugal, 2011, pp. 401–416.
[50] H. Lu, K. Veeraraghavan, P. Ajoux, J. Hunt, Y. J. Song, W. Tobagus, S. Ku-
mar, and W. Lloyd. “Existential consistency: Measuring and understand-
ing consistency at facebook.” In: Proceedings of the 25th Symposium on
Operating Systems Principles. ACM. Monterey, California, 2015, pp. 295–
310.
[51] P. Mahajan, L. Alvisi, M. Dahlin, et al. “Consistency, availability, and
convergence.” In: University of Texas at Austin Tech Report 11 (2011),
p. 158.
[52] H. Mahmoud, F. Nawab, A. Pucher, D. Agrawal, and A. El Abbadi. “Low-
latency multi-datacenter databases using replicated commit.” In: Pro-
ceedings of the VLDB Endowment 6.9 (2013), pp. 661–672.
[53] P. Mockapetris. RFC1035: Domain names—implementation and speciﬁca-
tion. 1987.
[54] MySQL Database. https://www.mysql.com (accessed Mar-2019).
[55] Network Time Protocol(NTP). http:http://www.ntp.org// (accessed
Mar-2019).
[56] D. S. Parker, G. J. Popek, G. Rudisin, A. Stoughton, B. J. Walker, E. Wal-
ton, J. M. Chow, D. Edwards, S. Kiser, and C. Kline. “Detection of mutual
inconsistency in distributed systems.” In: IEEE transactions on Software
Engineering (1983), pp. 240–247.
[57] K. Petersen, M. J. Spreitzer, D. B. Terry, M. M. Theimer, and A. J. Demers.
“Flexible Update Propagation for Weakly Consistent Replication.” In:
SIGOPS Operating Systems Review 31 (1997), pp. 288–301.
[58] Redis storage. https://redis.io/ (accessed Mar-2019).
[59] REST Facebook Library. https://restfc.com (accessed Mar-2019).
106
BIBLIOGRAPHY
[60] Y. Saito and M. Shapiro. “Optimistic replication.” In: ACM Computing
Surveys (CSUR) 37.1 (2005), pp. 42–81.
[61] SQLServer Database. https : / / www . microsoft . com / en - us / sql -
server/sql-server-2017 (accessed Mar-2019).
[62] A. S. Tanenbaum and M. Van Steen. Distributed systems: principles and
paradigms. Prentice-Hall, 2007.
[63] K. Tangwongsan, S. Tirthapura, and K.-L. Wu. “Parallel streaming frequency-
based aggregates.” In: Proceedings of the 26th acm symposium on paral-
lelism in algorithms and architectures. ACM. Prague, Czech Republic,
2014, pp. 236–245.
[64] D. B. Terry, A. J. Demers, K. Petersen, M. J. Spreitzer, M. M. Theimer,
and B. B. Welch. “Session guarantees for weakly consistent replicated
data.” In: Parallel and Distributed Information Systems, 1994., Proceedings
of the Third International Conference on. IEEE. 1994, pp. 140–149.
[65] Twitter API. https://developer.twitter.com/ (accessed Mar-2019).
[66] Twitter Service. https://www.twitter.com (accessed Mar-2019).
[67] W. Vogels. “Eventually consistent.” In: Communications of the ACM 52.1
(2009), pp. 40–44.
[68] H. Wada, A. Fekete, L. Zhao, K. Lee, and A. Liu. “Data Consistency Prop-
erties and the Trade-oﬀs in Commercial Cloud Storage: the Consumers’
Perspective.” In: Fifth Biennial Conference on Innovative Data Systems
Research. Vol. 11. Asilomar, CA, USA, 2011, pp. 134–143.
[69] Y. Xu, Z. Musgrave, B. Noble, and M. Bailey. “Bobtail: Avoiding Long
Tails in the Cloud.” In: 10th USENIX Symposium on Networked Systems
Design and Implementation. Vol. 13. Lombard, IL, 2013, pp. 329–342.
[70] H. Yu and A. Vahdat. “Design and evaluation of a continuous consis-
tency model for replicated services.” In: Proceedings of the 4th Conference
on Symposium on Operating System Design & Implementation-Volume 4.
USENIX. San Diego, California, 2000, pp. 305–318.
107
BIBLIOGRAPHY
[71] H. Yu and A. Vahdat. “Design and Evaluation of a Conit-based Contin-
uous Consistency Model for Replicated Services.” In: ACM Transactions
on Computer Systems 20.3 (2002), pp. 239–282.
[72] M. Zawirski, N. Preguiça, S. Duarte, A. Bieniusa, V. Balegas, and M.
Shapiro. “Write fast, read in the past: Causal consistency for client-side
applications.” In: Proceedings of the 16th Annual Middleware Conference.
ACM. Trento, Italy, 2015, pp. 75–87.
[73] K. Zellag and B. Kemme. “How consistent is your cloud application?” In:
Proceedings of the Third ACM Symposium on Cloud Computing. ACM. San
Jose, California, 2012, 6:1–6:14.
108