41.1%
99.1%
67.0%
27.0%
40.0%
95.5%
Figure 3: Histogram of large local logits values for ImageNet adver-
sarial images (only positive values larger than 20 are shown).
CIFAR-10 for the case study. We use a patch consisting of
3% of the image pixels for an empirical attack. Further de-
tails about the attack setup and datasets are covered in our
technical report [55]. We extract the local logits (as deﬁned
in Section 2.4) from adversarial images for further analysis.
Vulnerability I: the small adversarial patch appears in
the large receptive ﬁelds of most local features and is able
to manipulate the local predictions. In Table 2, we report
the percentage of incorrect local predictions of the adversarial
images (attacked) and clean images (original) as well as their
percentage difference. We can see that a small patch that
only takes up 3% of the image pixels can corrupt 24.5%
additional local predictions for ImageNet images, 41.1% for
ImageNette, and 40.0% for CIFAR-10. As shown in the table,
the large portion of incorrect local predictions ﬁnally leads
to a high percentage of incorrect global predictions. This
vulnerability mainly stems from the large receptive ﬁeld of
ResNet-50. Each local feature of ResNet-50 is inﬂuenced
by a 483×483 pixel region in the input space (with zero
padding) [1]; therefore, even if the adversarial patch only
appears in a small restricted area, it is still within the receptive
ﬁeld of many local features and can manipulate the local
predictions.3 This observation motivates the use of small
receptive ﬁelds: if the receptive ﬁeld is small, it ensures that
only a limited number of local features can be corrupted by
an adversarial patch, and robust prediction may be possible.
Vulnerability II: the adversarial patch creates large ma-
licious local feature values and makes linear feature ag-
gregation insecure. In Figure 3, we plot the histogram of
3We note that a patch appearing in the receptive ﬁeld of a local feature
does not necessarily indicate a successful local feature corruption. Each local
feature focuses exponentially more on the center of its receptive ﬁeld (further
details are in Appendix B). When the adversarial patch is far away from the
center of the receptive ﬁeld, its inﬂuence on the feature is greatly limited.
Figure 2: Two equivalent ways of computing the global logits vector
(top: used in conventional CNNs; bottom: used in our defense).
Local logits. Similar to computing the global logits from the
global feature, we can feed each local feature (in R1×1×C(cid:48)
) to
the fully-connected layer to get the local logits (in R1×1×N).
Each local logits vector is the classiﬁcation output based on
each local feature; thus, they share the same receptive ﬁeld.
Concatenating all W(cid:48) · H(cid:48) local logits vectors gives the local
logits tensor, and applying the element-wise linear aggrega-
tion gives the same global logits (bottom of Figure 2).
Local conﬁdence, local prediction, and class evidence.
Based on local logits, we can derive the concept of local con-
ﬁdence and local prediction tensor by feeding the local logits
tensor to a softmax layer and an argmax layer, respectively.
In the remainder of this paper, we specialize the concept of
feature by considering it to refer to either a logits tensor, a
conﬁdence tensor, or a prediction tensor. In this case, we have
C(cid:48) = N. We also sometimes abuse the notation by letting
F (x,l) : X × Y → RW(cid:48)×H(cid:48)
denote the slice of the feature
corresponding to class l. We call the elements of F (x,l) the
class evidence for class l.
3 PatchGuard
In this section, we ﬁrst use an empirical case study to motivate
the use of small receptive ﬁelds and secure feature aggrega-
tion (i.e., robust masking). Next, we will give an overview of
our general PatchGuard framework, followed by our use of
networks with small receptive ﬁelds and details of our robust
masking based secure aggregation. The provable robustness of
this defense will be demonstrated and analyzed in Section 4.
3.1 Why are adversarial patches effective?
Previous work [6, 22] on adversarial patches, surprisingly,
shows that model prediction can be manipulated by patches
that occupy a very small portion of input images. In this sub-
section, we provide a case study for ResNet-50 [21] trained
on ImageNet [12], ImageNette (a 10-class subset of Ima-
geNet) [16], and CIFAR-10 [23] datasets and identify two
critical reasons for the model vulnerability. These will then
motivate the development and discussion of our defense.
Experiment setup. We take 5000 random ImageNet valida-
tion images and the entire validation sets of ImageNette and
2240    30th USENIX Security Symposium
USENIX Association
class evidence of the true class and the malicious class of the
adversarial images from ImageNet (we report similar results
for other two datasets in our technical report [55]). As we can
see from Figure 3, the adversarial patch tends to create ex-
tremely large malicious class evidence to increase the chance
of a successful attack. Conventional CNNs use simple linear
operations such as average pooling to aggregate all local fea-
tures, and thus are vulnerable to these large malicious feature
values. This observation motivates our development of robust
masking as a secure feature aggregation mechanism.
3.2 Overview of PatchGuard
In Section 3.1, we identiﬁed the large receptive ﬁeld and
insecure aggregation of conventional CNNs as two major
sources of model vulnerability. In this subsection, we provide
an overview of our defense that tackles both problems.
Recall that Figure 1 provides an overview of our defense
framework. We consider a CNN M with small receptive ﬁelds.
The feature extractor F (x) produces the local feature tensor
u extracted from the input image x, where u can be any one of
the logits, conﬁdence, or model prediction tensor. Our defense
framework is compatible with any CNN with small receptive
ﬁelds, and we will present two general ways of building such
networks in Section 3.3. The small receptive ﬁeld ensures that
only a small fraction of features are corrupted by a localized
adversarial patch. However, the insecure aggregation of these
features via average pooling or summation might still result in
a misclassiﬁcation. To address this vulnerability, we propose
a robust masking algorithm for secure feature aggregation.
In robust masking, we detect and mask the corrupted fea-
tures in the local feature tensor u = F (x). Since the number
of corrupted local features is limited due to the small receptive
ﬁeld, the adversary is forced to create large feature values to
dominate the global prediction. These large feature values
lead to a distinct pattern and enable our detection of corrupted
features. Further, we empirically ﬁnd that that model predic-
tions are generally invariant to the removal of partial features
(Section 5.3.1). Therefore, once the corrupted features are
masked, we are likely to recover the correct prediction y with
the remaining local features (right part of Figure 1). This
defense introduces a dilemma for the adversary: either to gen-
erate conspicuous malicious features that will be detected and
masked by our defense or to use stealthy but ineffective adver-
sarial patches. This fundamental dilemma enables provable
robustness. We will introduce the details of robust masking
in Section 3.4, and perform its provable analysis in Section 4.
3.3 CNNs with Small Receptive Fields
Our defense framework is compatible with any CNN with
small receptive ﬁelds.4 In this subsection, we discuss two
4The receptive ﬁeld should be small compared with the input image size.
Figure 4: Effect of the convolution kernel size on the output receptive
ﬁeld size (left: two convolutions with a kernel size of 3; right: two
convolutions with a kernel size of 1 and 3, respectively).
general ways to build such CNNs; our goal is to reduce the
number of image pixels that can affect a particular feature.
Building an ensemble model. One approach to design a net-
work with small receptive ﬁelds is to divide the original image
into multiple small pixel patches and feed each pixel patch to
a base model for separate classiﬁcation. We can then build
an ensemble model aggregating the output of base models.
In this ensemble model, a local feature is the base model
output, which can be logits, conﬁdence, or prediction. Since
the base model only takes a small pixel patch as input, each
local feature is only affected by a small number of pixels, and
thus the ensemble model has a small receptive ﬁeld. We note
that as the image resolution becomes higher, the number of
all possible pixel patches increases greatly, which leads to a
huge training and testing computation cost of the ensemble
model. A natural approach to reduce the computation cost is
to do inference on a sub-sampled set of small pixel patches.
Using small convolution kernels. A more efﬁcient approach
is to use small convolution kernels in conventional CNN ar-
chitectures. In Figure 4, we provide an illustration for 1-D
convolution computation with different kernel sizes. As we
can see, the output cell is affected by all 5 input cells when
using two convolutions with a kernel size of 3 (left) while
each output cell is only affected by 3 input cells when re-
ducing the size of one kernel to 1 (right). This logic extends
directly to the large CNNs used in practice by replacing large
convolution kernels with small kernels. Moreover, we can use
a convolution stride to skip a portion of small pixel patches
to reduce the computation cost. The modiﬁed CNN can be
regarded as an ensemble model from a subset of all possi-
ble pixel patches. With this formulation, we can efﬁciently
extract all local features with one-time model feed-forward
computation. In Section 5, we will instantiate both approaches
by adapting the implementation from Levine et al. [27] and
Brendel et al. [4] and compare their performance.
Remark: translation from images into features. The use
of CNNs with small receptive ﬁelds translates the adversarial
patch defense problem from the image space to the feature
space. That is, the problem becomes one of performing robust
prediction from the feature space where a limited-size con-
tiguous region is corrupted (due to a limited-size contiguous
adversarial patch in the image space). The security analysis in
the feature space (i.e., local logits, conﬁdence, or prediction
tensor) is simpliﬁed due to the use of linear aggregation, in
USENIX Association
30th USENIX Security Symposium    2241
contrast with the high non-linearity of CNN models if we di-
rectly analyze the input image. This observation enables our
robust masking technique as well as our provable analysis.
3.4 Robust Masking
Given that an adversarial patch can only corrupt a limited
number of local features with small receptive ﬁelds, the ad-
versary is forced to create a small region of abnormally high
feature values to induce misclassiﬁcation. In order to detect
this corrupted region, we clip the feature values and use a slid-
ing window to ﬁnd the region with the highest class evidence
for each of the classes. We then apply a mask to the suspected
region for each class so that the ﬁnal classiﬁcation is not in-
ﬂuenced by the adversarial features. The defense algorithm is
shown in Algorithm 1.
Clipping. As shown in Algorithm 1, our defense will iterate
over all possible classes in Y . For each class ¯y, we ﬁrst get
its corresponding clipped local feature tensor ˆu ¯y from the
undefended model. We set the default values of the clipping
bounds to cl = 0,ch = ∞ for all feature types and datasets.
When the feature type is logits, we clip the negative values
to zero since our empirical analysis in Section 5.3.1 shows
that they contribute little to the correct prediction of clean
images but can be abused by the adversary to reduce the class
evidence of the true class. If the feature is a conﬁdence tensor
or one-hot encoded prediction, it is unaffected by clipping,
since its values are already bounded in [0,1].
Feature windows. We use a sliding window to detect and
mask the abnormal region in the feature space. A window
is a binary mask in the feature space whose size matches
the upper bound of the number of local features that can be
corrupted by the adversarial patch. Formally, let p be the
upper bound of patch size in the threat model, r be the size
of receptive ﬁeld, and s be the stride of receptive ﬁeld, which
is the pixel distance between two adjacent receptive centers.
We can compute the optimal window size w as
w = (cid:100)(p + r− 1)/s(cid:101)
(1)
This equation can be derived by considering the worst-case
patch location and counting the maximum number of cor-
rupted local features. A detailed derivation is in Appendix B.
We note that the window size is a tunable security parameter
and we use a conservative window size (computed with the
upper bound of the patch size) to make robust masking agnos-
tic to the actual patch size used in an attack. The implications
of using an overly conservative window size are discussed in
Section 5.3.2 and Appendix C. We represent each window
w with a binary feature map in {0,1}W(cid:48)×H(cid:48)
, where features
within the window have values of one.
Detection. We use the subprocedure DETECT to examine
the clipped local feature tensor ˆu ¯y and detect the suspicious
region. DETECT takes the feature tensor ˆu ¯y, the normalized
detection threshold T ∈ [0,1], and a set of sliding windows W
Algorithm 1 Robust masking
Input: Image x, label space Y , feature extractor F of model
M , clipping bound [cl,ch], the set of sliding windows
W , and detection threshold T ∈ [0,1]. Default setting:
cl = 0,ch = ∞,T = 0.
for each ¯y ∈ Y do
u ¯y ← F (x, ¯y)
ˆu ¯y ← CLIP(u ¯y,cl,ch)
¯y ← DETECT(ˆu ¯y,T,W )
w∗
s ¯y ← SUM(ˆu ¯y (cid:12) (1− w∗
Output: Robust prediction y∗
1: procedure ROBUSTMASKING
2:
3:
4:
5:
6:
end for
7:
y∗ ← argmax ¯y∈Y (s ¯y)
8:
return y∗
9:
10: end procedure
(cid:46) Local feature for class ¯y
(cid:46) Clipped local features
(cid:46) Detected window
¯y)) (cid:46) Applying the mask
¯y (cid:12) ˆu ¯y)/SUM(ˆu ¯y)
11: procedure DETECT(ˆu ¯y,T,W )
¯y ← argmaxw∈W SUM(w(cid:12) ˆu ¯y)
w∗
12:
b ← SUM(w∗
13:
if b ≤ T then
14:
¯y ← 0
w∗
15:
end if
16:
return w∗
17:
¯y
18: end procedure
(cid:46) Detection
(cid:46) Normalization
(cid:46) An empty mask returned
as inputs. To detect the malicious region, DETECT calculates
the sum of feature values (i.e., the class evidence) for class
¯y within every possible window and identiﬁes the window
with the highest sum of class evidence. If the normalized
highest class evidence exceeds the threshold T , we return the
corresponding window w∗
¯y as the suspicious window for that
class; otherwise, we return an empty window 0.
Masking. If we detect a suspicious window in the local fea-
ture space, we mask the features within the suspicious area
and calculate the sum of class evidence from the remaining
features as s ¯y = SUM(ˆu ¯y (cid:12) (1− w∗
¯y)). After we calculate the
masked class evidence s ¯y for all possible classes in Y , the
defense outputs the prediction as the class with largest class
evidence, i.e., y∗ = argmax ¯y∈Y (s ¯y).
4 Provable Robustness Analysis
In this section, we provide provable robustness analysis for
our robust masking defense. For any clean image x and a given
model M , we will determine whether any attacker, with the
knowledge of our defense, can bypass the robust masking
defense. Recall that our threat model allows the adversarial