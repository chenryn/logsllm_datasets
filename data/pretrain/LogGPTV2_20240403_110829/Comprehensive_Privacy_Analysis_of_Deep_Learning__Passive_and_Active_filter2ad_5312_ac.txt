the highest testing accuracy, across all the 100 epochs.
Tables II and XI present the dataset sizes used for training
the target and attack models. In the supervised setting for
training the attack models, we assume the attacker has access
to a fraction of the training set and some non-member samples.
In this case, to balance the training, we select half of each
batch to include member instances and the other half non-
member instances from the attacker’s background knowledge.
Creating the batches in this fashion will prevent the attack
model from a bias towards member or non-member instances.
F. Evaluation Metrics
Attack accuracy The attacker’s output has two classes “Mem-
ber” and “Non-member”. Attack accuracy is the fraction of
the correct membership predictions (predicting members as
member and non-members as non-member) for unknown data
points. The size of the set of member and non-member samples
that we use for evaluating the attack are the same.
True/False positive For a more detailed evaluation of attack
performance, we also measure the true positive and false
positive rates of the attacker. Positive is associated with the
attacker outputting “member”.
Prediction uncertainty For a classiﬁcation model, we com-
pute its prediction uncertainty using the normalized entropy
of its prediction vector for a given input.
pi log(pi)
(5)
K(cid:5)
1
H =
log(K)
i=1
4We make use of ResNet, DenseNet, and Alexnet pre-trained models,
provided in https://github.com/bearpaw/pytorch-classiﬁcation
(cid:24)(cid:21)(cid:22)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:51:43 UTC from IEEE Xplore.  Restrictions apply. 
TABLE II: Size of datasets used for training and testing the target
classiﬁcation model and the membership inference model
Target Model
Datasets
Training
Test
CIFAR100
Texas100
Puchase100
50,000
10,000
20,000
10,000
70,000
50,000
Training
Members
25,000
5,000
10,000
Inference Attack Model
Training
Test
Test
Non-members Members
Non-members
5,000
10,000
10,000
5,000
10,000
10,000
5,000
10,000
10,000
where K is the number of all classes and pi is the prediction
probability for the ith class. We compute the probabilities
using a softmax function as pi =
eh(d)(i)
k=1 eh(d)(k) .
(cid:2)K
IV. EXPERIMENTS
We start by presenting our results for the stand-alone
scenario, followed by our results for the federated learning
scenario.
A. Stand-Alone Setting: Attacking Fully-Trained Models
We investigate the case where the attacker has access to the
fully-trained target model, in the white-box setting. Therefore,
the attacker can leverage the outputs and the gradients of the
hidden layers of the target model to perform the attack. We
have used pre-trained CIFAR100 models, and have trained
other target models and the attack models using the dataset
sizes which are presented in Table II.
Impact of the outputs of different layers: To understand
and demonstrate the impact of different layers’ outputs, we
perform the attack separately using the outputs of individual
layers. We use a pre-trained Alexnet model as the target model,
where the model is composed of ﬁve convolutional layers and
a fully connected layer at the end. Table III shows the accuracy
of the attack using the output of each of the last three layers.
As the table shows, using the last layers results in the highest
attack accuracy, i.e., among the layer outputs, the last layer
(model output) leaks the most membership information
about the training data.The reason behind this is twofold. By
proceeding to the later layers, the capacity of the parameters
ramps up, which leads the target model to store unnecessary
information about the training dataset, and therefore leak more
information. Moreover, the ﬁrst layers extract simple features
from the input, thus generalize much better compared to the
last layers, which are responsible for complex task of ﬁnding
the relationship between abstract features and the classes. We
did not achieve signiﬁcant accuracy gain by combining the
outputs from multiple layers; this is because the leakage from
the last layer (which is equivalent to a black-box inference
attack) already contains the membership information that leaks
from the output of the previous layers.
Impact of gradients:
In Section II-A, we discussed why
gradients should leak information about the training dataset. In
Table VIII, we compare the accuracy of the membership attack
when the attacker uses the gradients versus layer outputs,
for different dataset and models. Overall, the results show
that gradients leak signiﬁcantly more membership information
about the training set, compared to the layer outputs.
We compare the result of
the attack on pre-trained
CIFAR100-ResNet and CIFAR100-DenseNet models, where
both are designed for the same image recognition task, both
are trained on the same dataset, and both have similar gener-
alization error. The results show that these two models have
various degrees of membership leakage; this suggests that the
generalization error is not the right metric to quantify
privacy leakage in the white-box setting. The large capacity
of the model which enables it to learn complex tasks and
generalize well, leads to also memorizing more information
about the training data. The total number of the parameters
in pre-trained Densenet model is 25.62M , whereas this is only
1.7M parameters for ResNet.
We also investigated the impact of gradients of different
layers on attack accuracy. The results are shown in Table IV
show that the gradient of
the later layers leak more
membership information. This is similar to our ﬁndings for
layer outputs: the last layer generalizes the least among all the
layers in the model, and is the most dependent layer to the
training set. By combining the gradients of all layers, we are
able to only slightly increase the attack accuracy.
Finally, Table V shows the attack accuracy when we com-
bine the output layer and gradients of different layers. We see
that the gradients from the last layer leak the most membership
information.
Impact of the training size: Table VI shows attack accuracy
for various sizes of the attacker’s training data. The models
are tested on the same set of test instances, across all these
scenarios. As expected, increasing the size of the attacker’s
training dataset improves the accuracy of the membership
inference attack.
Impact of
In this experiment, we
demonstrate that the norm of the model’s gradients is highly
correlated with the accuracy of membership inference, as it
behaves differently for member and non-member instances.
Figure 3 plots the last-layer gradient norms over consecutive
training epochs for member and non-member instances (for
the Purchase100 dataset). As can be seen, during training, the
gradient norms of the member instances decrease over training
epochs, which is not the case for non-member instances.
the gradient norm:
Figure 4 shows the distribution of last-layer gradient norms
for members and non-members on three various pretrained
architectures on CIFAR100. Comparing the ﬁgures with Ta-
ble VIII, we see that a model leaks more membership infor-
mation when the distribution of the gradient norm is more
distinct for member and non-member instances. For instance,
we can see that ResNet and DenseNet both have relatively
similar generalization errors, but the gradient norm distribution
of members and non-members is more distinguishable for
DenseNet (Figure 4b) compared to ResNet (Figure 4c). We
see that the attack accuracy in DenseNet is much higher than
ResNet.
Also, we show that the accuracy of our inference attack is
(cid:24)(cid:21)(cid:23)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:51:43 UTC from IEEE Xplore.  Restrictions apply. 
TABLE III: Attack accuracy using the outputs of individual activa-
tion layers. Pre-trained Alexnet on CIFAR100, stand-alone setting.
Output Layer
Last layer (prediction vector)
Attack Accuracy
74.6% (black-box)
Second to last
Third to last
Last three layers, combined
74.1%
72.9%
74.6%
Member instances
Non-member instances
m
r
o
n
t
n
e
i
d
a
r
G
1500
1000
500
0
0
20
40
60
80
100
Class
(a) Gradient norm for member and non-member data across all classes
e
v
i
t
i
s
o
P
e
u
r
T
1.0
0.8
0.6
0.4
0.2
0.0
Small gradient diff (174.33)
Medium gradient diff (860.32)
Large gradient diff (1788.13)
Random guess attack
0.0
0.2
0.4
0.6
False Positive
0.8
1.0
(b) Attacker accuracy for class members with various differences in
their member/non-member gradient norms
Fig. 2: Attack accuracy is different for different output classes (pre-
trained CIFAR100-Alextnet model in the stand-alone scenario).
TABLE IV: Attack accuracy when we apply the attack using
parameter gradients of different
layers. (CIFAR100 dataset with
Alexnet architecture, stand-alone scenario)
Gradient w.r.t.
Last layer parameters
Second to last layer parameters
Third to last layer parameters
Forth to last layer parameters
Parameters of last four layers, combined
Attack Accuracy
75.1%
74.6%
73.5%
72.6%
75.15%
TABLE V: Attack accuracy using different combinations of layer
gradients and outputs. (CIFAR100 dataset with Alexnet architecture,
stand-alone scenario)
Gradients w.r.t. Output Layers
Attack Test Accuracy
Last Layer
Last layer
Last Layer
All Layer
-
Last layer
All Layer
All Layer
75.10%
75.11%
75.12%
75.18%
TABLE VI: Attack accuracy for various sizes of the attacker’s
training dataset. The size of the target model’s training dataset is
50,000. (The CIFAR100 dataset with Alexnet, stand-alone scenario)
Members Sizes
Non-members Sizes
Attack Accuracy
10,000
15,000
15,000
25,000
2,000
2,000
5,000
5,000
73.2%
73.7%
74.8%
75.1%
TABLE VII: Accuracy of our unsupervised attack compared to the
Shadow models approach [6] for the white-box scenario.
Dataset
Arch
CIFAR100
CIFAR100
CIFAR100
Texas100
Purchase100
Alexnet
DenseNet
ResNet
Fully Connected
Fully Connected
(Unsupervised)
Attack Accuracy
(Shadow Models)
Attack Accuracy
75.0%
71.2%
63.1%
66.3%
71.0%
70.5%
64.2%
60.9%
65.3%
68.2%
higher for classiﬁcation output classes (of the target model)
with a larger difference in member/non-member gradient
norms. Figure 2a plots the average of last layer’s gradient
norms for different output classes for member and non-
member instances; we see that the difference of gradient norms
between members and non-members varies across different
classes. Figure 2b shows the receiver operating characteristic
(ROC) curve of the inference attack for three output classes
with small, medium, and large differences of gradient norm
between members and non-members (averaged over many
samples). As can be seen, the larger the difference of gradient
norm between members and non-members,
the higher the
accuracy of the membership inference attack.
Impact of prediction uncertainty: Previous work [6] claims
that the prediction vector’s uncertainty is an important factor
in privacy leakage. We validate this claim by evaluating the
attack for different classes in CIFAR100-Alexnet with different
prediction uncertainties. Speciﬁcally, we selected three classes
with small, medium, and high differences of prediction uncer-
tainties, where the attack accuracies are shown in Figure 6 for
these classes. Similar to the differences in gradient norms, the
classes with higher prediction uncertainty values leak more
membership information.
B. Stand-Alone Setting: Unsupervised Attacks
We also implement our attacks in an unsupervised scenario,
in which the attacker has data points sampled from the same
(cid:24)(cid:21)(cid:24)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:51:43 UTC from IEEE Xplore.  Restrictions apply. 
TABLE VIII: The attack accuracy for different datasets and different target architectures using layer outputs versus gradients. This is the
result of analyzing the stand-alone scenario, where the CIFAR100 models are all obtained from pre-trained online repositories.
Dataset
CIFAR100
CIFAR100
CIFAR100
Texas100
Purchase100
Target Model
Attack Accuracy
Architecture
Train Accuracy
Test Accuracy
Black-box White-box (Outputs) White-box (Gradients)
Alexnet
ResNet
DenseNet
Fully Connected
Fully Connected
99%
89%
100%
81.6%
100%
44%
73%
82%
52%
80%
74.2%
62.2%
67.7%
63.0%
67.6%
74.6%
62.2%
67.7%
63.3%
67.6%
75.1%
64.3%
74.3%
68.3%
73.4%
TABLE IX: Attack accuracy on ﬁne-tuned models. D is the initial training set, DΔ is the new dataset used for ﬁne-tuning, and ¯D is the
set of non-members (which is disjoint with D and DΔ).
Dataset
CIFAR100
CIFAR100
Texas100
Purchase100
Architecture
Alexnet