title:Rollback-Recovery for Middleboxes
author:Justine Sherry and
Peter Xiang Gao and
Soumya Basu and
Aurojit Panda and
Arvind Krishnamurthy and
Christian Maciocco and
Maziar Manesh and
João Martins and
Sylvia Ratnasamy and
Luigi Rizzo and
Scott Shenker
Rollback-Recovery for Middleboxes
Justine Sherry∗
Peter Xiang Gao∗
Arvind Krishnamurthy• Christian Maciocco† Maziar Manesh†
Sylvia Ratnasamy∗ Luigi Rizzo‡ Scott Shenker◦∗
Soumya Basu∗ Aurojit Panda∗
João Martins(cid:47)
∗ UC Berkeley • University of Washington † Intel Research (cid:47) NEC Labs ‡ University of Pisa ◦ ICSI
ABSTRACT
Network middleboxes must offer high availability, with au-
tomatic failover when a device fails. Achieving high avail-
ability is challenging because failover must correctly restore
lost state (e.g., activity logs, port mappings) but must do so
quickly (e.g., in less than typical transport timeout values to
minimize disruption to applications) and with little overhead
to failure-free operation (e.g., additional per-packet laten-
cies of 10-100s of µs). No existing middlebox design pro-
vides failover that is correct, fast to recover, and imposes
little increased latency on failure-free operations.
We present a new design for fault-tolerance in middle-
boxes that achieves these three goals. Our system, FTMB
(for Fault-Tolerant MiddleBox), adopts the classical ap-
proach of “rollback recovery” in which a system uses in-
formation logged during normal operation to correctly re-
construct state after a failure. However, traditional rollback
recovery cannot maintain high throughput given the frequent
output rate of middleboxes. Hence, we design a novel solu-
tion to record middlebox state which relies on two mech-
anisms: (1) ‘ordered logging’, which provides lightweight
logging of the information needed after recovery, and (2) a
‘parallel release’ algorithm which, when coupled with or-
dered logging, ensures that recovery is always correct. We
implement ordered logging and parallel release in Click and
show that for our test applications our design adds only
30µs of latency to median per packet latencies. Our system
introduces moderate throughput overheads (5-30%) and can
reconstruct lost state in 40-275ms for practical systems.
CCS Concepts
• Networks → Middleboxes / network appliances; •
Computer systems organization → Availability;
Keywords
middlebox reliability; parallel fault-tolerance
Permission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for proﬁt or commercial advantage and that copies bear this notice
and the full citation on the ﬁrst page. Copyrights for components of this work
owned by others than the author(s) must be honored. Abstracting with credit is
permitted. To copy otherwise, or republish, to post on servers or to redistribute to
lists, requires prior speciﬁc permission and/or a fee. Request permissions from
permissions@acm.org.
SIGCOMM ’15, August 17 - 21, 2015, London, United Kingdom
c(cid:13) 2015 Copyright held by the owner/author(s). Publication rights licensed to
ACM. ISBN 978-1-4503-3542-3/15/08. . . $15.00
DOI: http://dx.doi.org/10.1145/2785956.2787501
1.
INTRODUCTION
Middleboxes play a crucial role in the modern Internet
infrastructure – they offer an easy way to deploy new dat-
aplane functions and are often as numerous as routers and
switches [35,59,62]. Yet, because middleboxes typically in-
volve proprietary monolithic software running on dedicated
hardware, they can be expensive to deploy and manage.
To rectify this situation, network operators are moving
towards Network Function Virtualization (NFV), in which
middlebox functionality is moved out of dedicated physical
boxes into virtual appliances that can be run on commodity
processors [32]. While the NFV vision solves the ded-
icated hardware problem, it presents some technical chal-
lenges of its own. Two of the most commonly cited chal-
lenges have been performance [38, 45, 52, 55, 58] and man-
agement [33, 35, 49] with multiple efforts in both industry
and academia now exploring these questions. We argue that
an equally important challenge – one that has received far
less attention – is that of fault-tolerance.
Today, the common approach to fault tolerance in middle-
boxes is a combination of careful engineering to avoid faults,
and deploying a backup appliance to rapidly restart when
faults occur. Unfortunately, neither of these approaches –
alone or in combination – are ideal, and the migration to
NFV will only exacerbate their problematic aspects.
With traditional middleboxes, each “box” is developed by
a single vendor and dedicated to a single application. This
allows vendors greater control in limiting the introduction of
faults by, for example, running on hardware designed and
tested for reliability (ECC, proper cooling, redundant power
supply, etc.). This approach will not apply to NFV, where
developers have little control over the environment in which
their applications run and vendor diversity in hardware and
applications will explode the test space. And while one
might contemplate (re)introducing constraints on NFV plat-
forms, doing so would be counter to NFV’s goal of greater
openness and agility in middlebox infrastructure.
The second part to how operators handle middlebox fail-
ure is also imperfect. With current middleboxes, operators
often maintain a dedicated per-appliance backup. This is
inefﬁcient and offers only a weak form of recovery for the
many middlebox applications that are stateful – e.g., Net-
work Address Translators (NATs), WAN Optimizers, and In-
trusion Prevention Systems all maintain dynamic state about
227ﬂows, users, and network conditions. With no mechanism
to recover state, the backup may be unable to correctly pro-
cess packets after failure, leading to service disruption. (We
discuss this further in §3.2 and quantify disruption in §6.)
Our goal in this paper is to design middleboxes that guar-
antee correct recovery from failures. This solution must
be low-latency (e.g., the additional per-packet latency under
failure-free conditions must be well under 1ms) and recov-
ery must be fast (e.g., in less than typical transport timeout
values). To the best of our knowledge, no existing middle-
box design satisﬁes these goals. In addition, we would prefer
a solution that is general (i.e., can be applied across applica-
tions rather than having to be designed on a case-by-case
basis for each individual middlebox) and passive (i.e., does
not require one dedicated backup per middlebox).
Our solution – FTMB– introduces new algorithms and
techniques that tailor the classic approach of rollback recov-
ery to the middlebox domain and achieves correct recovery
in a general and passive manner. Our prototype implemen-
tation introduces low additional latency on failure-free oper-
ation (adding only 30µs to median per-packet latencies, an
improvement of 2-3 orders of magnitude over existing fault
tolerance mechanisms) and achieves rapid recovery (recon-
structing lost state in between 40-275ms for practical system
conﬁgurations).
The remainder of this paper is organized as follows: we
discuss our assumptions and the challenges in building a
fault-tolerant middlebox in §2, followed by our goals and
an examination of the design space in §3. We present the de-
sign, implementation and evaluation of FTMB in §4, §5, and
§6 respectively. We discuss related work in §7 and conclude
with future directions in §8.
2. PROBLEM SPACE
We present our system and failure model (§2.1 and §2.2) and
the challenges in building fault-tolerant middleboxes (§2.3).
2.1 System Model
Parallel implementations: We assume middlebox applica-
tions are multi-threaded and run on a multicore CPU (Fig-
ure 1). The middlebox runs with a ﬁxed number of threads.
We assume ‘multi-queue’ NICs that offer multiple transmit
and receive queues that are partitioned across threads. Each
thread reads from its own receive queue(s) and writes to its
h
s
a
h
S
S
R
IN
IN
IN
IN
thread
thread
thread
thread
shared state
out
out
out
out
C
N
I
t
u
p
t
u
o
Figure 1: Our model of a middlebox application
own transmit queue(s). The NIC partitions packets across
threads by hashing a packet’s ﬂow identiﬁer (i.e., 5-tuple in-
cluding source and destination port and address) to a queue;
hence all packets from a ﬂow are processed by the same
thread and a packet is processed entirely by one thread. The
above are standard approaches to parallelizing trafﬁc pro-
cessing in multicore systems [27, 37, 47, 58].
Shared state: By shared state we mean state that is accessed
across threads. In our parallelization approach, all packets
from a ﬂow are processed by a single thread so per-ﬂow state
is local to a single thread and is not shared state. How-
ever, other state may be relevant to multiple ﬂows, and ac-
cesses to such state may incur cross-thread synchronization
overheads. Common forms of shared state include aggregate
counters, IDS state machines, rate limiters, packet caches for
WAN optimizers, etc.
Virtualization: Finally, we assume the middlebox code is
running in a virtualized mode. The virtualization need not
be a VM per se; we could use containers [5], lightweight
VMs [44], or some other form of compartmentalization that
provides isolation and supports low-overhead snapshots of
its content.
2.2 Failure Model
We focus on recovery from “fail-stop” (rather than Byzan-
tine) errors, where under failure ‘the component changes to
a state that permits other components to detect that a fail-
ure has occurred and then stops’ [57]. This is the standard
failure model assumed by virtual machine fault tolerance ap-
proaches like Remus [23], Colo [28], and vSphere [11].
Our current implementation targets failures at the virtu-
alization layer and below, down to the hardware.1 Our so-
lutions – and many of the systems we compare against –
thus cope with failures in the system hardware, drivers, or
host operating system. According to a recent study (see Fig-
ure 13 in [48]), hardware failures are quite common (80%
of ﬁrewall failures, 66% of IDS failures, 74% of Load Bal-
ancer failures, and 16% of VPN failures required some form
of hardware replacement), so this failure model is quite rel-
evant to operational systems.
2.3 Challenges
Middlebox applications exhibit three characteristics that,
in combination, make fault-tolerance a challenge:
state-
fulness, very frequent non-determinism, and low packet-
processing latencies.
As mentioned earlier, many middlebox applications are
stateful and the loss of this state can degrade performance
and disrupt service. Thus, we want a failover mechanism
that correctly restores state such that future packets are pro-
cessed as if this state were never lost (we deﬁne correct-
ness rigorously in §3.1). One might think that this could
be achieved via ‘active:active’ operation, in which a ‘mas-
ter’ and a ‘replica’ execute on all inputs but only the mas-
1In §8, we discuss how emerging ‘container’ technologies would allow us
to extend our failure model to recover from failures in the guest OS. With
such extensions in place, the only errors that we would be unable to recover
from are those within the middlebox application software itself.
228ter’s output is released to users. However, this approach
fails when system execution is non-deterministic, because
the master and replica might diverge in their internal state
and produce an incorrect recovery.2
Non-determinism is a common problem in parallel pro-
grams when threads ‘race’ to access shared state: the order
in which these accesses occur depends on hard-to-control
effects (such as the scheduling order of threads, their rate of
progress, etc.) and are thus hard to predict. Unfortunately,
as mentioned earlier, shared state is common in middlebox
applications, and shared state such as counters, caches or
address pools may be accessed on a per-packet or per-ﬂow
basis leading to frequent nondeterminism.3 In addition, non-
determinism can also arise because of access to hardware
devices, including clocks and random number generators,
whose return values cannot be predicted. FTMB must cope
with all of these sources of nondeterminism.
As we elaborate on shortly, the common approach to ac-
commodating non-determinism is to intercept and/or record
the outcome of all potentially non-deterministic operations.
However, such interception slows down normal operation
and is thus at odds with the other two characteristics of
middlebox applications, namely very frequent accesses to
shared state and low packet processing latencies. Specif-
ically, a piece of shared state may be accessed 100k-1M
times per second (the rate of packet arrivals), and the latency
through the middlebox should be in 10-100s of microsec-
onds. Hence mechanisms for fault-tolerance must support
high access rates and introduce extra latencies of a similar
magnitude.
3. GOALS AND DESIGN RATIONALE
Building on the previous discussion, we now describe our
goals for FTMB (§3.1), some context (§3.2), and the ratio-
nale for the design approach we adopt (§3.3)
3.1 Goals
A fault-tolerant middlebox design must meet the three re-
quirements that follow.
(1) Correctness. The classic deﬁnition of correct recovery
comes from Strom and Yemeni [60]: “A system recovers
correctly if its internal state after a failure is consistent with
the observable behavior of the system before the failure."
It is important to note that reconstructed state need not be
identical to that before failure. Instead, it is sufﬁcient that
the reconstructed state be one that could have generated the
interactions that the system has already had with the exter-
nal world. This deﬁnition leads to a necessary condition for
correctness called “output commit", which is stated as fol-
lows: no output can be released to the external world until
all the information necessary to recreate internal state con-
sistent with that output has been committed to stable storage.
As we discuss shortly, the nature of this necessary in-
formation varies widely across different designs for fault-
tolerance as does the manner in which the output commit
2Similarly, such non-determinism prevents replicated state machine tech-
niques from providing recovery in this context.
3We evaluate the effects of such non-determinism in §6.
property is enforced. In the context of middleboxes, the out-
put in question is a packet and hence to meet the output
commit property we must ensure that, before the middle-
box transmits a packet p, it has successfully logged to stable
storage all the information needed to recreate internal state
consistent with an execution that would have generated p.
(2) Low overhead on failure-free operation. We aim for
mechanisms that introduce no more than 10-100s of mi-
croseconds of added delay to packet latencies.