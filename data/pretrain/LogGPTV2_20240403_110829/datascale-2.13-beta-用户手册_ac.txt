{ 
	"name": "my_new_group", 
	"description": "Description of my_new_group" }
删除 group 
删除 group 目录 config/group/my_new_group/ 。需要注意的是，如果 group 中已经存在 dataflow，dataflow 的配置文件存在目录 config/ group/my_new_group/dataflow/下，因此这些 dataflow 也将会被删除。
	信息 
通过文件配置 group 后，需要重启 DataScale 服务使改动生效。
版本：2.13.0-beta
配置 Dataflow
Dataflow 配置格式
Dataflow 由 source、pipeline 以及 sink 三种组件构成（关于组件之间的关系，参见 Dataflow 基 础），一个 dataflow 配置的基本格式为：
{ 
	"name": "", 
	"description": "", // 可选参数 	"paused": false, // 可选参数"sources": [ 
	{ 
	"name": "", 
	"type": "", // Source 组件的类型 
	"conf": { 
	... // SOURCE_TYPE 类型的 source 组件的配置参数 	} 
	} 
	], 
	"pipelines": [ 
	{ 
	"name": "", 
	"inputs": [ "" ], 
	"conf": { 
	"transforms": [
{ "type": "", // Transform 方法的类型
	"conf": { 
	... // TRANSFORM_TYPE 类型的 transform 方法的配置参数 	} 
}
上述 dataflow 配置中各参数的说明：}
上述 dataflow 配置中各参数的说明：
•	name参数指定 dataflow 的名称，该名称需要在其所属的 group 的范围内保证唯一
•	description参数用于记录 dataflow 的详细描述
•	paused参数用于标记 dataflow 是否要被执行（默认为false，表示此 dataflow 会被执
行）
•	sources数组包含 dataflow 中的一个或者多个 source 组件
每个 source 组件的配置包括：
◦	name参数指定 source 组件的名称
◦	type参数指定 source 组件的类型
◦	conf参数指定该类型的 source 组件的配置参数
•	pipelines数组包含 dataflow 中的一个或者多个 pipeline 组件
每个 pipeline 组件的配置包括：
◦	name参数指定 pipeline 组件的名称◦	name参数指定 pipeline 组件的名称
◦	inputs参数指定 pipeline 组件的上游 source 组件的名称
◦	conf.transforms数组包含 pipeline 组件的所有 transform 方法，这些 transform
方法将会被按顺序执行
每个 transform 方法的配置包括：
▪	type参数指定 transform 方法的类型
▪	conf参数指定该类型的 transform 方法的配置参数
•	sink数组包含 dataflow 中的一个或者多个 sink 组件
每个 sink 组件的配置包括：
◦	name参数指定 sink 组件的名称
◦	inputs参数指定 sink 组件的上游 source 或者 pipeline 组件的名称
◦	type参数指定 sink 组件的类型
◦	conf参数指定该类型的 sink 组件的配置参数◦	conf参数指定该类型的 sink 组件的配置参数
信息
• Dataflow 以及各组件的名称需要符合命名规范，参见文档 命名规范
• 同一个 dataflow 中，source、pipeline 以及 sink 组件的名称必须唯一，不可以有同名组
件
• 关于 DataScale 支持的组件类型和相应的配置参数，参见文档 Sources、Pipelines 以
及 Sinks。
配置 Dataflow
通过 过 Web UI 配置 dataflow
DataScale Web UI 提供了创建、修改、启停、删除等 dataflow 管理功能。你可以使用 “视图模式”，
通过拖拽组件来编辑 dataflow；也可以使用 “高级模式”，通过编辑 json 配置文件来编辑
dataflow。
• 视图模式
• 高级模式
通过 过 Restful API 配置 dataflow• 高级模式
通过 过 Restful API 配置 dataflow 
DataScale 服务提供了创建、修改以及删除 dataflow 的 Restful API，参见文档 Restful API
通过 过文件配置 dataflow 
当使用 file 类型的 metastore 时（参见 Metastore 配置），可以直接通过配置文件配置 dataflow。
• 创 创建 dataflow 
	DataScale 使用 group 来管理 dataflow 分组，因此需要在指定 group 的目录下创建 	dataflow 配置文件，配置文件内容如上节所述。例如，在 default group 中创建 dataflow 	my_dataflow，那么需要创建文件config/group/default/dataflow/ 
	my_dataflow.json，文件内容如下：my_dataflow.json，文件内容如下：
{ 
	"name": "my_dataflow", 
	"sources": [ ... ], // Source 组件的配置 	"pipelines": [ ... ], // Pipeline 组件的配置 	"sinks": [ ... ] // Sink 组件的配置 
}
• 修改 dataflow
修改 dataflow 配置文件 config/group/default/dataflow/my_dataflow.json 中 的配置参数。
• 删 删除 dataflow 
	删除 dataflow 配置文件 config/group/default/dataflow/my_dataflow.json 。
	信息 
通过文件配置 dataflow 后，需要重启 DataScale 服务使改动生效
查看 Dataflow 的运 运行查看 Dataflow 的运 运行
Dataflow 运行过程中，可以通过 Web UI 实时查看指定组件采集、处理数据量的统计信息，以及 event 的内容：
• 查看组件输出 event 的统计信息
• 查看组件输出 event 的内容
	版本：2.13.0-beta 
Source 组 组件
Source 组件用于对接各种数据源，可以通过主动抓取或者被动接收的方式采集数据，并将 event 输出到下游 的 pipeline 或者 sink 组件。
📄 File
file source 组件用于从文件中采集数据。
📄 Exec
exec source 组件用于从所执行的命令的输出中采集数据。
📄 Kafka
kafka source 组件用于从指定的 Kafka topics 中采集数据。
📄 Kubernetes Logs📄 Kubernetes Logs
kubernetes_logs source 组件用于采集 DataScale 服务运行所在的 Kubernetes node 上的 Pod 日志。
📄 Prometheus Exporter
prometheus_exporter source 组件用于从 Prometheus exporter 的 HTTP endpoint 采集数据。
📄 Prometheus Remote Write
prometheusremotewrite source 组件用于接收来自 Prometheus server 的数据。
📄 Socket / Syslog
| socket source 组件用于接收 socket client 发送来的数据。 |
|---|
| 📄自定义 义采集器 ||---|
| 📄自定义 义采集器 |
collector source 组件用于使用自定义采集器采集数据，包括 DataScale 内置的自定义采集器以及用户开发的自定义采集器。
版本：2.13.0-beta
File
filesource 组件用于从文件中采集数据。
配置示例
{ 
	"name": "my_file_source", 
	"type": "file", 
	"conf": { 
	"include": [ "/var/log/*.log" ] 
	} 
}
配置参数
include
必填参数。采集目标文件列表，支持 。
exclude
从 include 参数指定的目标文件中排除的文件，支持 。
例如：
{ 
"name": "my_file_source", 
"type": "file", 
"conf": {
read_from"type": "file", 
"conf": {
read_from
第一次读取采集目标文件时的读取位置，参数值可以是 beginning （默认值）或 end 。
max_line_bytes
能够读取的一条 event 的最大 bytes（默认值为 102400 bytes），超出此限制的 event 将被丢弃。
multiline
当采集目标文件中存在跨行的 event 时，使用此参数定义跨行聚合 event 的规则：
• start_pattern
必填参数。匹配 event 起始的正则表达式。
• timeout_ms
等待跨行 event 结束，即匹配到下一 event 的 start_pattern 的时间（默认值为 60000 毫秒）。当超时发生时，即使没有匹配到下一 event，当前 event 的内容也会被截断并输出。
例如，event 需要以时间戳开头：
{例如，event 需要以时间戳开头：
{ 
	"name": "my_file_source", 
	"type": "file", 
	"conf": { 
	"include": [ "/var/log/*.log" ], 
	"multiline": { 
	"start_pattern": "\\[\\d{2,4}\\-\\d{2,4}\\-
\\d{2,4}T\\d{2,4}:\\d{2,4}:\\d{2,4}(?:\\.\\d+)?Z?\\+\\d{1,6}.\\d{1,6}\\]" 	} 
	} 
}
remove_after_secs 
当读取到采集目标文件的 EOF 后，如果在设定的时间（单位为秒）内再没有新数据写入，将删除该文件。但 是如果不指定该参数，则不会删除文件。
	信息 
运行 DataScale 的账号必须要有删除采集目标文件的权限。
版本：2.13.0-beta版本：2.13.0-beta
Exec
execsource 组件用于从所执行的命令的输出中采集数据。
配置示例
{ 
	"name": "my_exec_source", 
	"type": "exec", 
	"conf": { 
	"command": "ps -ef", 
	"mode": "scheduled", 
	"scheduled": { 
	"exec_interval_secs": 120 
	} 
	} 
}
配置参数
command
必填参数。执行的命令及参数。执行命令后，输出到 console（默认包括 STDOUT 和 STDERR）的内容将被采集。
include_stderr
执行命令后，输出到 STDERR 的内容是否需要被采集（默认为 true ）。
working_directory 
执行命令时使用的工作目录（默认为当前 DataScale 服务的工作目录）。执行命令时使用的工作目录（默认为当前 DataScale 服务的工作目录）。
mode 
必填参数。 command 参数指定的命令的执行方式，参数值可以是 scheduled 或 streaming。
scheduled 
command参数指定的命令将被周期性的执行。
• exec_interval_secs