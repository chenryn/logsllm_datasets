and model training difﬁcult. To tackle this, we follow the
divide-and-conquer idea to decompose the attack Apattern into
small sub-attacks, according to the adversary’s knowledge of
the format. Again on Citizen, we can decompose the attack
model gbirth into three sub-attacks, namely year attack gyear,
month attack gmonth and day attack gday. Each sub-attack model
can be independently implemented with fully-connected neural
networks of much smaller size and the total parameter number
is largely truncated from O(|V (wb)| × . . . × |V (we)|) to
O(|V (wb)|+. . .+|V (we)|). Besides, the generating algorithm
can also be decomposed to subroutines for each attack model,
so that the training of each sub-module can be conducted in
parallel.
C. Experimental Setup
Benchmark Systems.
• Citizen: We randomly generate 1000 citizen IDs according
to the generating rule in Eq. 1 as the ground-truth plain text.
Then we query the target language model with these citizen
IDs to get the corresponding embeddings as the victims.
• Genome: We implement eight genome classiﬁcation systems
for splice site prediction based on a public genome dataset
called HS3D (Homo Sapiens Splice Sites Dataset [53]). All
the genome sequences are of length 20. We assume the
embeddings of genome sequences in the test set, which
contains respectively 1000 samples with or without
the
splice site, are leaked to the adversary.
Attack Implementation.
• Citizen: Following the discussion in Section V-B, we im-
plement the year, month and date sub-attacks as three-layer
MLPs which respectively contain 400, 25, 200 hidden units
with sigmoid activation. The training batch size is set as 128
for each sub-attack.
• Genome: In practice, we augment the training pair (z, wi) by
concatenating the embedding z of the generated sample with
the positional embedding pi for the target position i. We
discuss the motivation in Appendix B. Technically, we use
the sinusoidal positional embedding as in [72], which has the
same dimension as z. Corresponding to this modiﬁcation,
we implement one single attack model for inferring the
nucleotide type at any speciﬁed position. Different from the
Citizen case, this modiﬁcation will not increase the param-
eter number as the class number is still 4. The attack model
is implemented as a four-layer MLP which takes input z⊕pi
of dimension 2d and has 400, 100 hidden units with sigmoid
activation and intermediate batch normalization layers [32]
for faster convergence. For training, we generate mini-
batches of size 128 that consist of tuples (z, pi, wi), where
the positional embedding i is randomly sampled from the
interval of possible positions (i.e., 1, . . . , 20). For inference,
the attacker inputs the victim’s embedding and the target
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:19:28 UTC from IEEE Xplore.  Restrictions apply. 
1319
ACCURACY OF SEGMENT RECONSTRUCTION ATTACKS ON CITIZEN.
TABLE II
Year
Month
Date
Whole
Top-1
0.661
0.725
0.506
0.735
0.626
0.454
0.572
0.584
0.01
Top-5
0.926
0.927
0.748
0.978
0.882
0.774
0.847
0.892
0.05
Top-1
0.616
0.802
0.484
0.601
0.664
0.441
0.509
0.559
0.083
Top-5
0.950
0.992
0.877
0.987
0.968
0.889
0.911
0.924
0.417
Top-1
0.539
0.839
0.457
0.630
0.624
0.307
0.642
0.465
0.033
Top-5
0.885
0.992
0.797
0.960
0.927
0.703
0.908
0.843
0.167
Top-1
0.219
0.488
0.112
0.281
0.259
0.061
0.187
0.152
0.0001
Top-5
0.384
0.624
0.186
0.434
0.384
0.108
0.263
0.257
0.0005
Bert
Transformer-XL
XLNet
GPT
GPT-2
RoBERTa
XLM
Ernie 2.0
Baseline
position and the model outputs the predicted nucleotide type.
More implementation details can be found in Appendix B.
D. Results & Analysis
Table II reports the Top-1 and Top-5 accuracy of the
sub-attacks and of inferring the whole birth date with the
ensemble attack after 100, 000 iterations of training, where the
baseline denotes the performance of a random guesser. Fig. 3
reports the average and per-nucleotide Top-1 accuracy of the
attacks on Genome after 100, 000 iterations of training, where
we report
the proportion of the most frequently appeared
nucleotide type as the baseline.
1) Effectiveness & Efﬁciency: From Table II & Fig. 3,
considering the performance of baseline, we can see that our
attacks are effective in recovering sensitive segments from
their embeddings. For example, when given Transformer-XL
(abbr. XL in later sections) embeddings of citizen ID, our
attack is able to recover the exact month and date of the vic-
tim’s birthday with over 80% Top-1 accuracy and recover the
whole birth date with over 62% Top-5 accuracy. When given
GPT embeddings of genome sequences, our attack achieves
near-100% accuracy of inferring the victim’s nucleotide type
at both ends and over 62% accuracy on average. These results
highly demonstrate the effectiveness of our attacks and thus
the common existence of privacy risks in the popular industry-
level language models.
Moreover, our attack is also efﬁcient
in terms of the
throughput, which are reported in Table VI of the Appendix. In
both cases the attack can learn from over 100 batches in one
second. To achieve the reported accuracy, the training takes
less than 30 minutes on a medium-end PC. More details of
our experimental environment is in Appendix H.
Fig. 3. Accuracy of segment reconstruction attacks on Genome per nucleotide
position. The average accuracy is reported in the legend.
2) Comparison among Language Models: First, we notice
Facebook’s RoBERTa shows stronger robustness than other
language models in both cases. By investigating its design,
we ﬁnd RoBERTa is a re-implementation of Google’s Bert but
uses a different byte-level tokenization scheme (i.e., tokenize
sentences in the unit of bytes instead of characters or words)
[44]. As RoBERTa shows about 50% lower privacy risks than
Bert when facing the same attacks, we conjecture the reason is
that the byte-level tokenization scheme may make the embed-
dings less explicit in character-level sensitive information and
thus more robust against our attacks. Similar phenomenon is
also observed in the next section. However, RoBERTa suffers
a clear utility degradation as a trade-off between utility and
privacy. As we can see from Fig. 6(c), the system with Bert
achieves an about 33% higher utility performance than that
with RoBERTa on Genome. Also, we notice OpenAI’s GPT
and GPT-2, which share the same architecture but are pre-
trained on 4GB and 40GB texts, show similar security proper-
ties against our attacks and comparable utility performance.
Combined with other results, no immediate relatedness is
observed between the pretraining data size and the privacy
risk level.
3) Other Interesting Findings: From Fig. 3, we can see a
majority of the accuracy curves present a valley-like shape,
which implies that most language models capture more infor-
mation of the tokens around the ends than those in the middle,
which is probably due to the information at ends usually
propagates along the longest path in the recurrent architecture.
In other words, the sensitive information which lies at the
sentence boundary is more prone to malicious disclosure.
VI. KEYWORD INFERENCE ATTACK
In this section, we study a more general scenario where
the plain text can be arbitrary natural sentences and the
knowledge-level of the adversary is much lower. As a result,
successful attacks in this case can impose stronger threats to
real-world systems.
A. Attack Deﬁnition
The adversary in keyword inference attack is curious about
the following predicate, whether certain keyword k is con-
tained in the unknown sentence x. The keyword k can be
highly sensitive, which contains indicators for the adversary
to further determine e.g., location, residence or illness history
of the victim [62].
Before introducing two illustrative examples, we formulate
the mapping Pkeyword,k for deﬁning the sensitive information
related with keyword k from a sentence x as Pkeyword,k :
x → (∃w ∈ x, w == k), where the right side denotes a
predicate that yields True if a word w in the sentence x is the
target keyword k and otherwise False. As the keyword k is
speciﬁed by the adversary, the routine Pkeyword,k is obviously
known by him/her. Correspondingly, the keyword inference
attack regarding Pkeyword,k is deﬁned as Akeyword,k : z →
(∃w ∈ x, w == k). Different from pattern reconstruction
attacks, keyword inference attacks probes the occurrence of
certain keywords instead of exact reconstuction of the whole
sequence.
Case Study - Airline Reviews (abbr. Airline). Sometimes
airline companies survey their customers in order to e.g.,
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:19:28 UTC from IEEE Xplore.  Restrictions apply. 
1320
improve their customer service. With the aid of advanced NLP
techniques, large amounts of airline reviews in text form can be
automatically processed for understanding customers’ opinion
(i.e., opinion mining [69]). As is widely recognized [16], [17],
[41], utilizing the pre-trained language models for feature
extraction can further improve the utility of many existing
opinion mining systems.
However, once accessing the embeddings, the adversary can
infer various location-related sensitive information about the
victim, including his/her departure, residence, itinerary, etc. As
a preliminary step for further attacks, we show the adversary
can accurately estimate the probability of whether certain city
name is contained in the review.
Case Study - Medical Descriptions (abbr. Medical). With
the booming of intelligent healthcare, some hospitals tend to
build an automatic pre-diagnosis system for more effective
service ﬂow [28]. The system is expected to take the patient’s
description of the illness to predict which department he/she
ought to consult. To form a benchmark system, we concatenate
the pretrained language models with an additional linear layer
for guiding the patients to 10 different departments. Through
evaluations, we show the systems can achieve over 90%
accuracy on real-world datasets in Fig. 8(b) of the Appendix.
More details can be found in Appendix A.
However, when the adversary gets access to the embeddings
only, he/she can indeed infer more sensitive and personalized
information about the patient as a victim. Besides the depart-
ment the patient ought to consult, the adversary can further
determine other ﬁne-grained information like the disease type
or even the precise disease site. To demonstrate, we suppose
an adversary wants to pinpoint the precise disease site of the
victim by inferring the occurrence probability of body-related
words in his/her descriptions.
B. Methodology
In this part, we detail our implementations for keyword
inference attacks. According to the different
levels of the
adversary’s knowledge on the plain text, the methodology
part is divided into white-box and black-box settings, which
respectively require the following two assumptions.
• Assumption 3b. The adversary has access to a shadow
corpus, which consists of sentences that are sampled from
the same distribution of the target plain text (which we refer
to as white-box).
• Assumption 3c. The adversary has no information on the
target plain text (which we refer to as black-box).
Noteworthily, the adversary under Assumption 3c has almost
no prior knowledge except that he/she (e.g., any attacker who
captures the embeddings) has access to the embeddings, which
therefore poses a rather practical threat to the general-purpose
language models, while Assumption 3b is also possible to
happen in real-world situations when, if we continue the above
medical example, some hospital publishes an anonymised
dataset of medical descriptions for research purposes [1] or
the service provider is honest-but-curious.
(cid:2)
i
(cid:2)
i
)}N
). Next,
Attack in White-Box Settings. Basically, as the adversary has
a shadow corpus Dshadow := {(x
i=1 which is sampled from
the same distribution as the unknown plain text, he/she can
directly use Dshadow as the external corpus Dext and extract
the adversary
the binary label y
trains a binary classiﬁer with the dataset to conduct Akeyword,k.
However, we notice in practice the adversary may confront
with several pitfalls.
= Pkeyword,k(x
First, the label set {y
the sentences with the keyword k (i.e.,
}N
i=1 can be highly imbalanced. In
other words,
the
positive samples) may be in an absolute minority compared to
those without k (i.e., the negative samples). According to pre-