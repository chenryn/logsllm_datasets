#command to read or write a file
作为管理员，也应掌握使NameNode进入或退出安全模式的方法，这些操作有时也是必需的，比如在升级完集群后需要确认数据是否仍然可读等。这时我们可以使用以下命令：
hadoop dfsadmin-safemode enter
Safe mode is ON
当NameNode仍处于安全模式时，也可以使用以上命令以保证NameNode没有退出安全模式。要使系统退出安全模式可执行以下命令：
hadoop dfsadmin-safemode leave
Safe mode is OFF
10.3.2 Hadoop的备份
1.元数据的备份
如果NameNode中存储的持久化元数据信息丢失或遭到破坏，那么整个文件系统就不可用了。因此元数据的备份至关重要，需要备份不同时期的元数据信息（1小时、1天、1周……）以避免突然宕机带来的破坏。
备份的一个最直接的办法就是编写一个脚本程序，然后周期性地将Secondary NameNode中previous.checkpoint子目录（该目录由fs.checkpoint.dir属性值确定）下的文件归档到另外的机器上。该脚本需要额外验证所复制的备份文件的完整性。这个验证可以通过在NameNode的守护进程中运行一个验证程序来实现，验证其是否成功地从内存中读取了fsimage及edits文件。
2.数据的备份
HDFS的设计目标之一就是能够可靠地在分布式集群中储存数据。HDFS允许数据丢失的发生，所以数据的备份就显得至关重要了。由于Hadoop可以存储大规模的数据，备份哪些数据、备份到哪里就成为一个关键。在备份过程中，最优先备份的应该是那些不能再生的数据和对商业应用最关键的数据。而对于那些可以通过其他手段再生的数据或对于商业应用价值不是很大的数据，可以考虑不进行备份。
这里需要强调的是，不要认为HDFS的副本机制可以代替数据的备份。HDFS中的Bug也会导致副本丢失，同样硬件也会出现故障。尽管Hadoop可以承受集群中廉价商用机器故障，但是有些极端情况不能排除在外，特别是系统有时还会出现软件Bug和人为失误的情况。
通常Hadoop会设置用户目录的策略，比如，每个用户都有一个空间配额，每天晚上都可进行备份工作。但是不管设置什么样的策略，都需要通知用户，以避免客户反映问题。
前面介绍的distcp工具（参见第9章HDFS详解）是在不同HDFS之间或不同Hadoop文件系统之间转存和备份数据的好工具，因为distcp可以并行运行数据复制。
10.3.3 Hadoop的节点管理
作为Hadoop集群的管理员，可能随时都要处理增加和撤销机器节点的任务。例如，要增加集群的存储容量，就要增加新的节点。相反，要缩小集群的规模，就需要撤销已存在的节点。如果一个节点频繁地发生故障或运行缓慢，那么也要考虑撤销已存在的节点。节点一般承担DataNode和TaskTracker的任务，Hadoop支持对它们的添加和撤销。
1.添加新的节点
在第2章，我们介绍了如何部署Hadoop集群，可以看到添加一个新的节点虽然只用配置hdfs-site.xml文件和mapred-site.xml文件，但最好还是配置一个授权节点列表。
如果允许任何机器都可以连接到NameNode上并充当DataNode，这是存在安全隐患的，因为这样的机器可能能够获得未授权文件的访问权限。此外这样的机器并不是真正的DataNode，但它可以存储数据，却又不在集群的控制之下，并且任何时候都有可能停止运行，从而造成数据丢失。由于配置简单或存在配置错误，即使在防火墙内这样的处理也可能存在风险，因此在集群中也要对DataNode进行明确的管理。
在dfs.hosts文件中指定可以连接到NameNode的DataNode列表。dfs.hosts文件存储在NameNode的本地文件系统上，包含每个DataNode的网络地址，一行表示一个DataNode。要为一个DataNode设置多个网络地址，把它们写到一行中，中间用空格分开。类似的，TaskTracker是在mapred.hosts中设置的。一般来说，DataNode和TaskTracker列表都存在一个共享文件，名为include file。该文件被dfs.hosts及mapred.hosts两者引用，因为在大多数情况下，集群中的机器会同时运行DataNode及TaskTracker守护进程。
需要注意的是，dfs.hosts和mapred.hosts这两个文件与slaves文件不同，slaves文件被Hadoop的执行脚本用于执行集群范围的操作，例如集群的重启等，但它从来不会被Hadoop的守护进程使用。
要向集群添加新的节点，需要执行以下步骤：
1）向include文件中添加新节点的网络地址；
2）使用以下命令更新NameNode中具有连接权限的DataNode集合：
hadoop dfsadmin-refreshNodes
3）更新带有新节点的slaves文件，以便Hadoop控制脚本在执行后续操作时可以使用更新后的slaves文件中的所有节点；
4）启动新的数据节点；
5）重新启动MapReduce集群；
6）检查网页用户界面是否有新的DataNode和TaskTracker。
需要注意的是，HDFS不会自动将旧DataNode上的数据转移到新的DataNode中，但我们可以运行平衡器命令进行集群均衡。
2.撤销节点
撤销数据节点时要避免数据的丢失。在撤销前，需先通知NameNode要撤销的节点，然后在撤销此节点前将上面的数据块转移出去。而如果关闭了正在运行的TaskTracker，那么JobTracker会意识到错误并将任务分配到其他TaskTracker中去。
撤销节点过程由exclude文件控制：对于HDFS来说，可以通过dfs.hosts.exclude属性来控制；对于MapReduce来说，可以由mapred.hosts.exclude来设置。
TaskTracker是否可以连接到JobTracker，其规则很简单，只要include文件中包含且exclude中不包含这个TaskTracker，这样TaskTracker就可以连接到JobTracker来执行任务。没有定义的或空的include文件意味着所有节点都在include文件中。
对于HDFS来说规则有些许不同，表10-3总结了include和exclude存放节点的情况。对于TaskTracker来说，一个未定义的或空的include文件意味着所有的节点都包含其中。
要想从集群中撤销节点，需要执行以下步骤：
1）将需要撤销的节点的网络地址增加到exculde文件中，注意，不要在此时更新include文件；
2）重新启动MapReduce集群来终止已撤销节点的TaskTracker；
3）用以下命令更新具有新的许可DataNode节点集的NameNode：
hadoop dfsadmin-refreshNodes
4）进入网络用户界面，先检查已撤销的DataNode的管理状态是否变为“DecommissionIn Progress”，然后把数据块复制到集群的其他DataNode中；
5）当所有DataNode报告其状态为“Decommissioned”时，所有数据块也都会被复制，此时可以关闭已撤销的节点；
6）从include中删除节点网络地址，然后再次运行命令：
hadoop dfsadmin-refreshNodes
7）从slaves文件中删除节点。
10.3.4 系统升级
升级HDFS和MapReduce集群需要一个合理的操作步骤，这里我们主要讲解HDFS的升级。如果文件系统升级后文件格局发生了变化，那么升级时会将文件系统的数据和元数据迁移到与新版本一致的格式上。由于任何涉及数据迁移的操作都会导致数据的丢失，所以必须保证数据和元数据都有备份（具体操作参看10.3.2节）。在进行升级时，可以先在小型集群中进行测试，以便正式运行时可以解决所有问题。
Hadoop对自身的兼容性要求非常高，所有Hadoop 1.0之前版本的兼容性要求最严格，只有来自相同发布版本的组件才能保证相互的兼容性，这就意味着整个系统从守护进程到客户端都要同时更新，还需要集群停机一段时间。后期发布的版本支持回滚升级，允许集群守护进程分阶段升级，以便在更新期间可以运行客户端。
如果文件系统的布局不改变，那么集群升级就非常简单了。首先在集群中安装新的HDFS和MapRedude（同时在客户端也要安装），然后关闭旧的守护进程，升级配置文件，启动新的守护进程和客户端更新库。这个过程是可逆的，因此升级后的版本回滚到之前版本也很简单。
每次成功升级后都要执行一系列的清除步骤：
1）从集群上删除旧的安装和配置文件；
2）修复代码和配置中的每个错误警告。
以上讲解的系统升级非常简单，但是如果需要升级文件系统，就需要更进一步的操作。
如果使用以上讲解方法进行升级，并且HDFS是一个不同的布局版本，那么NameNode就不会正常运行。NameNode的日志会产生以下信息：
File system image contains an old layout version-15.
An upgrade to version-18 is required.
Please restart NameNode with-upgrade option.
要想确定是否需要升级文件系统，最好的办法就是在一个小集群上进行测试。
HDFS升级将复制以前版本的元数据和数据。升级并不需要两倍的集群存储空间，因为DataNode使用硬链接来保留对同一个数据块的两个引用，这样就可以在需要的时候轻松实现回滚到以前版本的文件系统。
需要注意的是，升级后只能保留前一个版本的文件系统，而不能回滚到多个文件系统，因此执行另一个对HDFS的升级需要删除以前的版本，这个过程被称为确定更新（finalizing the upgrade）。一旦更新被确定，那HDFS就不会回滚到以前的版本了。
需要说明的是，只有可以正常运作的健康的系统才能被正确升级。在进行升级之前，必须进行一个全面的fsck操作。为防止意外，可以将系统中的所有文件及块的列表（fsck的输出）进行备份。这样就可以在升级后将运行的输出与之对比，检测是否全部正确升级，有没有数据丢失。
还需要注意，在升级之前要删除临时文件，包括HDFS上MapReduce系统目录中的文件和本地临时文件。
完成以上这些工作后就可以进行集群的升级和文件系统的迁移了，具体步骤如下：
1）确保之前的升级操作全部完成，不会影响此次升级；
2）关闭MapReduce，终止TaskTracker上的所有任务进程；
3）关闭HDFS并备份NameNode目录；
4）在集群和客户端上安装新版本的Hadoop HDFS和同步的MapReduce；
5）使用-upgrade选项启动HDFS；
6）等待操作完成；
7）在HDFS上进行健康检查；
8）启动MapReduce；
9）回滚或确定升级。
在运行升级程序时，最好能从PATH环境变量中删除Hadoop脚本，这样可以避免运行不确定版本的脚本程序。在安装目录定义两个环境变量是很方便的，在以下指令中已经定义了OLD_HADOOP_INSTALL和NEW_HADOOP_INSTALL。在以上步骤5）中我们要运行以下指令：
$NEW_HADOOP_INSTALL/bin/start-dfs.sh-upgrade
NameNode升级它的元数据，并将以前的版本放入新建的目录previous中：
${dfs.name.dir}/current/VERSION
/edits
/fsimage
/fstime
/previous/VERSION
/edits
/fsimage
/fstime
采用类似的方式，DataNode升级它的存储目录，将旧的目录复制到previous目录中去。
升级过程需要一段时间才能完成。可以使用dfsadmin命令来检查升级的进度。升级的事件同样会记录在守护进程的日志文件中。在步骤6）中执行以下命令：
$NEW_HADOOP_INSTALL/bin/hadoop dfsadmin-upgradeProgress status
Upgrade for version-18 has been completed.
Upgrade is not finalized.
以上代码表明升级已经完成。在这个阶段必须在文件系统上进行一些健康检查（即步骤7），比如使用fsck进行文件和块的检查）。当进行检查（只读模式）时，可以让HDFS进入安全模式，以防止其他检查对文件进行更改。
步骤9）是可选操作，如果在升级后发现问题，则可以回滚到之前版本。
首先，关闭新的守护进程：
$NEW_HADOOP_INSTALL/bin/stop-dfs.sh
然后，用-rollback选项启动旧版本的HDFS：
$OLD_HADOOP_INSTALL/bin/start-dfs.sh-rollback
这个命令会使用NameNode和DataNode以前的副本替换它们当前存储目录下的内容，文件系统立即返回原始状态。
如果对新升级的版本感到满意，那么可以执行确定升级（即步骤9），可选），并删除以前的存储目录。需要注意的是在升级确定后，就不能回滚到之前的版本了。
需要执行以下步骤，才能进行另一次升级：
$NEW_HADOOP_INSTALL/bin/hadoop dfsadmin-fnalizeUpgrade
$NEW_HADOOP_INSTALL/bin/hadoop dfsadmin-upgradeProgress status
There are no upgrades in progress.
至此，HDFS升级到了最新版本。
10.4 本章小结
本章重点介绍了Hadoop监控和管理方面的相关内容。
首先，从HDFS文件结构开始进行相关介绍。HDFS作为Hadoop的核心分布式文件系统，其许多应用都构建在其核心分布式文件系统上。对于作为基础架构的核心分布式文件系统，管理员要给予更多的关注。
其次，本章从整体上对Hadoop的监控机制和相关的监控工具进行了分析，着重分析了Hadoop监控的支持基础、日志和度量，同时提出了诸多系统监控的解决方案，并着重介绍了Ganglia监控软件。
最后，本章对实际应用中经常遇到的维护要求，比如增删节点、数据备份、系统升级等进行了介绍。
第11章 Hive详解
本章内容
Hive简介
Hive的基本操作
Hive QL详解
Hive网络（Web UI）接口
Hive的JDBC接口
Hive的优化
本章小结
Hive是Hadoop中的一个重要子项目，它利用的是MapReduce编程技术，实现了部分SQL语句，提供了类SQL的编程接口。Hive的出现极大地推进了Hadoop在数据仓库方面的发展。事实上，目前业界仍在对何谓大规模数据分析最佳方法进行着辩论。由于传统应用的惯性，业界保守派依然青睐于关系型数据库和SQL语言。而在学术界，互联网阵营则更集中于支持MapReduce的开发模式。本章我们将对基于Hive的数据仓库解决方案进行介绍。
11.1 Hive简介
Hive是一个基于Hadoop文件系统之上的数据仓库架构。它为数据仓库的管理提供了许多功能：数据ETL（抽取、转换和加载）工具、数据存储管理和大型数据集的查询和分析能力。同时Hive定义了类SQL的语言—Hive QL。Hive QL允许用户进行和SQL相似的操作，还允许开发人员方便地使用Mapper和Reducer操作，这对MapReduce框架是一个强有力的支持。
由于Hadoop是批量处理系统，任务是高延迟性的，在任务提交和处理过程中会消耗一些时间成本。同样，即使Hive处理的数据集非常小（比如几百MB），在执行时也会出现延迟现象。这样，Hive的性能就不可能很好地和传统的Oracle数据库进行比较了。Hive不提供数据排序和查询cache功能，不提供在线事务处理，也不提供实时的查询和记录级的更新，但Hive能更好地处理不变的大规模数据集（例如网络日志）上的批量任务。所以，Hive最大的价值是可扩展性（基于Hadoop平台，可以自动适应机器数目和数据量的动态变化）、可延展性（结合MapReduce和用户定义的函数库）、良好的容错性和低约束的数据输入格式。
Hive本身建立在Hadoop的体系架构上，提供了一个SQL的解析过程，并从外部接口中获取命令，以对用户指令进行解析。Hive可将外部命令解析成一个Map-Reduce可执行计划，并按照该计划生成MapReduce任务后交给Hadoop集群进行处理，Hive的体系结构如图11-1所示。
 11.1.1 Hive的数据存储
Hive的存储是建立在Hadoop文件系统之上的。Hive本身没有专门的数据存储格式，也不能为数据建立索引，因此用户可以非常自由地组织Hive中的表，只需要在创建表的时候告诉Hive数据中的列分隔符和行分隔符就可以解析数据了。
Hive中主要包含四类数据模型：表（Table）、外部表（External Table）、分区（Partition）和桶（Bucket）。
Hive中的表和数据库中的表在概念上是类似的，在Hive中每个表都有一个对应的存储目录。例如，一个表htable在HDFS中的路径为/datawarehouse/htable，其中，/datawarehouse是在hive-site.xml配置文件中由${hive.metastore.warehouse.dir}指定的数据仓库的目录，所有的表数据（除了外部表）都保存在这个目录中。
Hive中的每个分区都对应数据库中相应分区列的一个索引，但是其分区的组织方式和传统关系型数据库不同。在Hive中，表中的一个分区对应表下的一个目录，所有分区的数据都存储在对应的目录中。例如，htable表中包含的ds和city两个分区，分别对应两个目录：对应ds=20100301，city=Beijing的HDFS子目录为/datawarehouse/htable/ds=20100301/city=Beijing；对应ds=20100301，city=Shanghai的HDFS子目录为/datawarehouse/htable/ds=20100301/city=Shanghai。
图 11-1 Hive的体系结构
桶在对指定列进行哈希（Hash）计算时，会根据哈希值切分数据，使每个桶对应一个文件。例如，将属性列user列分散到32个桶中，先要对user列的值进行hash计算，对应哈希值为0的桶写入HDFS的目录为/datawarehouse/htable/ds=20100301/city=Beijing/part-00000；对应哈希值为10的HDFS目录为/datawarehouse/htable/ds=20100301/city=Beijing/part-00010，依此类推。
外部表指向已经在HDFS中存在的数据，也可以创建分区。它和表在元数据的组织上是相同的，而实际数据的存储则存在较大差异，主要表现在以下两点上。
1）创建表的操作包含两个步骤：表创建过程和数据加载步骤（这两个过程可以在同一语句中完成）。在数据加载过程中，实际数据会移动到数据仓库目录中，之后的数据访问将会直接在数据仓库目录中完成。在删除表时，表中的数据和元数据将会被同时删除。
2）外部表的创建只有一个步骤，加载数据和创建表同时完成，实际数据存储在创建语句LOCATION指定的HDFS路径中，并不会移动到数据仓库目录中。如果删除一个外部表，仅删除元数据，表中的数据不会被删除。
11.1.2 Hive的元数据存储
由于Hive的元数据可能要面临不断地更新、修改和读取操作，所以它显然不适合使用Hadoop文件系统进行存储。目前Hive将元数据存储在RDBMS中，比如存储在MySQL、Derby中。Hive有三种模式可以连接到Derby数据库：
1）Single User Mode，利用此模式连接到一个In-memory（内存）数据库Derby，一般用于单元测试；
2）Multi User Mode，通过网络连接到一个数据库中，是最常使用的模式；
3）Remote Server Mode，用于非Java客户端访问元数据库，在服务器端启动一个MetaStoreServer，在客户端利用Thrift协议通过MetaStoreServer访问元数据库。
关于Hive元数据的使用配置，我们将在11.5节“Hive的JDBC接口”中进行详细介绍。
11.2 Hive的基本操作
本节中我们将介绍Hive的基本操作，包括Hive在集群上的安装配置及Hive的Web UI的使用。
 11.2.1 在集群上安装Hive
从图11-1中可以看出，Hive可以理解为在Hadoop和HDFS之上为用户封装一层便于用户使用的接口，该接口有丰富的样式，包括命令终端、Web UI及JDBC/ODBC等。因此Hive的安装需要依赖Hadoop。下面我们具体介绍如何下载、安装和配置Hive。
（1）先决条件
要求必须已经安装完成Hadoop，当前最新版本为1.0.1。Hadoop的安装我们已经在前面章节中详细讲过（参见第2章“Hadoop的安装与配置”），这里不再赘述。