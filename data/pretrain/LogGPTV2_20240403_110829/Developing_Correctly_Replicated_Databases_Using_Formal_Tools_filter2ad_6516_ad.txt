B. State Machine Replication: With state machine
replication, all transactions are ordered by the total order
broadcast service. Executing a transaction T happens as
follows: (i) the client broadcasts T to all replicas using
the broadcast service, (ii) upon delivering T , each database
executes and commits the transaction and sends the answer
to the client, (iii) the client waits to receive the ﬁrst answer.
When a replica crashes, the protocol proceeds normally
with no interruptions as long as at least one replica survives.
If a replica suspects another replica to have crashed,
it
creates a snapshot of its database and broadcasts a reconﬁg-
uration request to add a new replica and remove the crashed
one. This request contains the sequence number of the last
ordered transaction but not the snapshot. The new replica
obtains the snapshot from the proposer (a recovering replica
can potentially fetch only the transactions it is missing).
C. Diversity: While the total order broadcast service
has provable correctness guarantees at the level of Nuprl
programs, for the rest ShadowDB relies on an environment
8
that is hand-written and may contain bugs. This environ-
ment contains the Nuprl program interpreters, the operating
systems, compilers, libraries, and of course the databases
themselves. We employ diversity to attempt to mask corre-
lated failures in the environment [26].
As we shall see, we deploy different databases in Shad-
owDB. Our implementation allows to easily plug in any
JDBC-enabled database by specifying the database driver
and the connection URL. We could easily go further and
compile these with different compilers, and run these on
different operating systems and hardware.
The total order broadcast service itself can also beneﬁt
from diversity. We currently have Nuprl program interpreters
available in SML and Ocaml. Such interpreters are easy
to build and test as Nuprl programs are built from few
constructs with well-deﬁned semantics. Nuprl programs can
also be translated into other functional languages. Indeed,
we developed a translator from Nuprl programs to Lisp. We
can then compile the Lisp code using different compilers
and run it in different environments.
IV. Evaluation
In this section, we evaluate the performance of the broadcast
service and compare ShadowDB with primary-backup repli-
cation (ShadowDB-PBR) and with state machine replication
(ShadowDB-SMR) to popular databases. We do so on a
cluster of quad-core 3.6GHz Intel Xeons connected with a
gigabit switch. Each machine runs Red Hat Linux 5.8 and
is equipped with 4GB of memory. The hand-written part of
ShadowDB is coded in Java and respectively contains 1,199
and 292 lines of code for PBR and SMR. ShadowDB and
the broadcast service interact using TCP sockets.
A. The broadcast service: We measure the time needed to
broadcast a message and receive a deliver notiﬁcation from
the broadcast service when running Paxos on three machines
(f = 1). Each experiment consists of 500 messages broad-
cast per client when the service runs in the SML interpreter,
and 10,000 messages per client when we run the service
translated into Lisp. Each message contains 140 bytes of
payload. All versions of the broadcast service implement
batching, that is, multiple messages can be bundled in one
Paxos proposal. In Fig. 8, we report the average delivery
latency as a function of the load and we vary the number of
clients broadcasting messages between 1 and 43. With one
client, the non-optimized service run in the SML interpreter
takes 122ms to deliver a message. The optimized version re-
duces this latency to 69.4ms. At their maximum throughput
of respectively 27 and 65 messages delivered per second,
both interpreted versions are CPU-bound.
Not surprisingly, the broadcast service greatly beneﬁts
from being translated into Lisp: only 8.8ms are required
to deliver a message with one client, and the maximum
throughput
this
reaches 900 messages per second. At
)
s
m
(
y
c
n
e
t
a
L
1000
100
10
1
Interpreted –+–
Inter.-Opt. – – Compiled –×–
1
10
1000
Delivered messages per second
100
10000
Figure 8. The performance of the broadcast service with Paxos.
throughput the execution is CPU-bound. Although compil-
ing the service brings a signiﬁcant speed-up, performance
remains one order of magnitude slower than a hand-coded
Paxos. However, Sec. IV-B shows that the Lisp broadcast
service is fast enough to let ShadowDB with state machine
replication match the performance of its primary-backup
counterpart under one of the two considered benchmarks.
B. ShadowDB: We assess the performance of ShadowDB-
PBR and ShadowDB-SMR using a micro-benchmark and
TPC-C [27]. The micro-benchmark consists of a database
of bank accounts, each having an identiﬁer, an owner,
and a balance. For the sake of diversity, we deploy each
ShadowDB replica with a different in-memory embedded
SQL databases: H2 1.3.170, HSQLDB 2.2.9, or Apache
Derby 10.9.1. For a few setups, all replicas are deployed
with the same database to make comparisons fair.
In all experiments below, group conﬁgurations contain
two databases (f = 1);
the third database is used by
ShadowDB-PBR to replace the backup when we crash the
primary. The broadcast service relies on the Paxos protocol
and is deployed on three servers (Paxos needs three ma-
chines to tolerate one failure). We run the broadcast service
in the interpreter with ShadowDB-PBR, and we rely on the
Lisp service for ShadowDB-SMR. Databases are co-located
with the processes of the broadcast service. Clients run on
a separate machine.
Normal Case:
Fig. 9(a) plots the average latency as
a function of the number of committed transactions per
second. We increase the load imposed on the system by
varying the number of clients between 1 and 32, each
submitting 35,000 update transactions. These transactions
deposit money on a randomly selected account. Rows are
16 bytes in length and the database contains 50,000 rows.
We compare the performance of ShadowDB with the
stand-alone H2 database (the fastest database among H2,
Derby, and HSQLDB), the built-in H2 replication protocol,
and MySQL replication. To make the comparison fair we
deploy ShadowDB with H2 both at
the primary and at
the backup. ShadowDB-PBR reaches a throughput of more
than 4,600 update transactions per second or 72% of the
maximal throughput attained by a stand-alone H2 database.
9
ShadowDB-PBR –+–
H2-repl. – – MySQL-repl. – – H2-stdalone –•–
ShadowDB-SMR – –
ShadowDB-PBR –+–
MySQL-repl. – –
ShadowDB-SMR – –
H2-stdalone –•–
100
10
)
s
m
(
y
c
n
e
t
a
L
100
10
1
)
s
m
(
y
c
n
e
t
a
L
0.1
0
2K
4K
6K
8K
1
0
200
400
600
800
1000
Committed transactions per second
Committed TPC-C transactions per second
(a)
(b)
Figure 9. The performance of ShadowDB compared to other replication protocols: (a) our micro-benchmark; (b) the TPC-C benchmark.
This is the best performance attained by any of the replicated
databases considered and is a consequence of ShadowDB-
PBR’s design: the normal case is hand-coded for efﬁciency.
This performance comes with conﬁdence in the code’s cor-
rectness: normal case processing is simple and can easily be
tested; recovery relies on code with correctness guarantees.
The H2 replication protocol quickly reaches its maximum
throughput as can be seen in Fig. 9(a). This happens when
contention is too high and transactions timeout when trying
to lock the database table (H2 does not offer row-level
locks). comparison, ShadowDB-PBR executes transactions
sequentially at each replica,
thereby avoiding this prob-
lem, and commits transactions in batches, to provide high
throughput. The in-memory storage engine of MySQL only
provides table locking and thus suffers from a similar issue.
The maximum throughput attained is 3,900 transactions per
second. Adding more clients results in even higher con-
tention and lower overall throughput. We also benchmarked
MySQL replication with an InnoDB table and synchronous
writes turned off. Since InnoDB uses row-level locks, this
lowered the abort rate, but the maximum throughput reached
is lower than with an in-memory table. ShadowDB-SMR
reaches a maximal throughput of 760 transactions per second
and is the slowest replicated database under the micro-
benchmark. At this throughput, transaction execution con-
sumes a signiﬁcant amount of CPU and this prevents the
Lisp broadcast service from reaching its maximal throughput
since databases and Paxos processes are co-located.
In Figure 9(b) the same databases are compared using
the TPC-C benchmark conﬁgured with 1 warehouse. We
report the average transaction execution latency, considering
all ﬁve TPC-C transaction types, as a function of the load.
Experiments consist of between 1 and 10 clients, each
submitting 3,000 TPC-C transactions. H2 replication suffers
from contention on the table locks and can only sustain a
maximum of 62 TPC-C transactions per second; the curve is
therefore omitted from the graph. ShadowDB-PBR reaches a
maximum throughput of 550 transactions per second, or 66%
of the maximum throughput of a standalone H2 database.
Interestingly, ShadowDB-SMR provides a similar maximum
throughput of 526 transactions per second. Recall that with
ShadowDB-SMR, all but transaction execution comes with
correctness properties. This shows that using formal methods
to build replicated databases is not only feasible but it can
also provide good performance.
For TPC-C, we run MySQL with the InnoDB storage
engine, sufﬁcient buffer space to hold the entire database in
memory, and synchronous disk writes disabled—the mem-
ory engine provides lower performance than InnoDB due to
operations on indices such as “less than” and “order by” that
are not optimized. TPC-C transactions involve several round-
trips between the client and the database for each of the
ﬁve transaction types. The ShadowDB-PBR and ShadowDB-
SMR replicas execute the transactions in the same JVM as
the database, which lowers latency and improves throughput
signiﬁcantly compared to running them in separate JVMs.
Recovery: Fig. 10(a) illustrates an execution of ShadowDB-
PBR using the micro-benchmark where we crash the primary
(with ShadowDB-SMR, a crash of a replica is transparent as
long as one replica is functioning). We plot the instantaneous
throughput of committed transactions as a function of time.
The experiment consists of 10 clients with H2 on the
primary, HSQLDB on the backup, and Derby on the spare
backup. After 15 seconds of execution we crash the primary,
and 10 seconds later the backup detects this crash (detec-
tion time is conﬁgurable). The new group conﬁguration is
delivered about 69ms after its broadcast, and the remaining
of the recovery protocol, including state transfer, takes 3.8
seconds (the database contains 50,000 tuples, each 16 bytes
long). At time 40 seconds clients resume their execution.
Fig. 10(b) presents the time it
takes to transfer the
database state from one replica to another. With ShadowDB-
PBR,
this happens when a crash has occurred and the
primary transfers its state to the backups. With ShadowDB-
SMR, a state transfer happens when we add a replica to
the group using the broadcast service (possibly long after a
crash occurred). State transfer consists in selecting the rows