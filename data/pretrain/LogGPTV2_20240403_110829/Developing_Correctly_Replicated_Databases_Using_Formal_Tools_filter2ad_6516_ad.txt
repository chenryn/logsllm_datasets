### B. State Machine Replication

In state machine replication, all transactions are ordered by a total order broadcast service. The execution of a transaction \( T \) follows these steps:
1. The client broadcasts \( T \) to all replicas using the broadcast service.
2. Upon receiving \( T \), each replica executes and commits the transaction, then sends the result back to the client.
3. The client waits for the first response.

If a replica fails, the protocol continues without interruption as long as at least one replica remains operational. If a replica suspects another has failed, it creates a snapshot of its database and broadcasts a reconfiguration request to add a new replica and remove the failed one. This request includes the sequence number of the last ordered transaction but not the snapshot itself. The new replica retrieves the snapshot from the proposer (a recovering replica can potentially fetch only the missing transactions).

### C. Diversity

While the total order broadcast service provides provable correctness guarantees at the level of Nuprl programs, the rest of ShadowDB relies on hand-written components that may contain bugs. These components include Nuprl program interpreters, operating systems, compilers, libraries, and the databases themselves. We use diversity to mitigate correlated failures in the environment [26].

In ShadowDB, we deploy different databases. Our implementation allows for easy integration of any JDBC-enabled database by specifying the database driver and connection URL. We can further enhance diversity by compiling these with different compilers and running them on different operating systems and hardware.

The total order broadcast service also benefits from diversity. Currently, we have Nuprl program interpreters available in SML and Ocaml. These interpreters are straightforward to build and test due to the well-defined semantics of Nuprl programs. Additionally, Nuprl programs can be translated into other functional languages. For instance, we developed a translator from Nuprl to Lisp, allowing us to compile the Lisp code using different compilers and run it in various environments.

### IV. Evaluation

#### A. The Broadcast Service

We evaluate the performance of the broadcast service and compare ShadowDB with primary-backup replication (ShadowDB-PBR) and state machine replication (ShadowDB-SMR) against popular databases. The tests are conducted on a cluster of quad-core 3.6GHz Intel Xeons connected via a gigabit switch. Each machine runs Red Hat Linux 5.8 and is equipped with 4GB of memory. The hand-written part of ShadowDB is coded in Java, containing 1,199 and 292 lines of code for PBR and SMR, respectively. ShadowDB and the broadcast service interact using TCP sockets.

We measure the time required to broadcast a message and receive a delivery notification from the broadcast service when running Paxos on three machines (with \( f = 1 \)). Each experiment involves 500 messages per client when the service runs in the SML interpreter, and 10,000 messages per client when the service is translated into Lisp. Each message contains 140 bytes of payload. All versions of the broadcast service implement batching, allowing multiple messages to be bundled in one Paxos proposal.

Figure 8 shows the average delivery latency as a function of the load, varying the number of clients broadcasting messages between 1 and 43. With one client, the non-optimized service in the SML interpreter takes 122ms to deliver a message. The optimized version reduces this latency to 69.4ms. At their maximum throughput of 27 and 65 messages delivered per second, both interpreted versions are CPU-bound.

Translating the broadcast service into Lisp significantly improves performance: only 8.8ms are required to deliver a message with one client, and the maximum throughput reaches 900 messages per second. At high throughput, the execution remains CPU-bound. Although compiling the service brings a significant speed-up, performance is still an order of magnitude slower than a hand-coded Paxos. However, Section IV-B shows that the Lisp broadcast service is fast enough to allow ShadowDB with state machine replication to match the performance of its primary-backup counterpart under one of the two considered benchmarks.

#### B. ShadowDB

We assess the performance of ShadowDB-PBR and ShadowDB-SMR using a micro-benchmark and TPC-C [27]. The micro-benchmark consists of a database of bank accounts, each with an identifier, owner, and balance. For diversity, each ShadowDB replica is deployed with a different in-memory embedded SQL database: H2 1.3.170, HSQLDB 2.2.9, or Apache Derby 10.9.1. In some setups, all replicas use the same database for fair comparisons.

All experiments involve group configurations with two databases (\( f = 1 \)); the third database is used by ShadowDB-PBR to replace the backup when the primary fails. The broadcast service uses the Paxos protocol and is deployed on three servers. We run the broadcast service in the interpreter with ShadowDB-PBR and use the Lisp service for ShadowDB-SMR. Databases are co-located with the processes of the broadcast service, and clients run on a separate machine.

**Normal Case:**
Figure 9(a) plots the average latency as a function of the number of committed transactions per second. We increase the load by varying the number of clients between 1 and 32, each submitting 35,000 update transactions. These transactions deposit money into randomly selected accounts. Rows are 16 bytes long, and the database contains 50,000 rows.

We compare the performance of ShadowDB with the stand-alone H2 database (the fastest among H2, Derby, and HSQLDB), the built-in H2 replication protocol, and MySQL replication. To ensure fairness, ShadowDB is deployed with H2 at both the primary and backup. ShadowDB-PBR achieves a throughput of more than 4,600 update transactions per second, or 72% of the maximal throughput of a stand-alone H2 database. This is the best performance among the replicated databases considered, attributed to the efficient design of ShadowDB-PBR.

**TPC-C Benchmark:**
Figure 9(b) compares the same databases using the TPC-C benchmark configured with 1 warehouse. We report the average transaction execution latency as a function of the load. Experiments involve between 1 and 10 clients, each submitting 3,000 TPC-C transactions. H2 replication suffers from contention on table locks and can only sustain a maximum of 62 TPC-C transactions per second; thus, it is omitted from the graph. ShadowDB-PBR reaches a maximum throughput of 550 transactions per second, or 66% of the maximum throughput of a standalone H2 database. Interestingly, ShadowDB-SMR provides a similar maximum throughput of 526 transactions per second, demonstrating that using formal methods for building replicated databases is both feasible and performant.

For TPC-C, we run MySQL with the InnoDB storage engine, sufficient buffer space to hold the entire database in memory, and synchronous disk writes disabled. The InnoDB engine provides better performance than the memory engine for operations like "less than" and "order by." TPC-C transactions involve several round-trips between the client and the database for each of the five transaction types. The ShadowDB-PBR and ShadowDB-SMR replicas execute transactions in the same JVM as the database, which reduces latency and improves throughput compared to running them in separate JVMs.

**Recovery:**
Figure 10(a) illustrates an execution of ShadowDB-PBR using the micro-benchmark where we crash the primary. We plot the instantaneous throughput of committed transactions as a function of time. The experiment involves 10 clients with H2 on the primary, HSQLDB on the backup, and Derby on the spare backup. After 15 seconds, the primary is crashed, and 10 seconds later, the backup detects the crash (detection time is configurable). The new group configuration is delivered about 69ms after its broadcast, and the remaining recovery protocol, including state transfer, takes 3.8 seconds (the database contains 50,000 tuples, each 16 bytes long). Clients resume execution at 40 seconds.

Figure 10(b) presents the time it takes to transfer the database state from one replica to another. With ShadowDB-PBR, this occurs when a crash happens, and the primary transfers its state to the backups. With ShadowDB-SMR, a state transfer happens when a new replica is added to the group using the broadcast service, possibly long after a crash. The state transfer involves selecting the rows to be transferred.