switch, i.e., when the packet started consuming network resources,
to when the packet arrived at the collector and could be processed.
Unfortunately, we were not able to measure this precisely because
our testbed did not provide a way to get an accurate timestamp for
when a packet was placed on the wire. Instead, we measured the
time between when tcpdump reported the packet being sent to
when the collector received the packet. This includes some delay
at the sender as the packet traverses the lower parts of the kernel
and the NIC, which is not technically sample delay. As a result our
measurements are strict overestimates of true sample latency.
To eliminate clock skew, we used separate NICs on the same
physical server as both the sender and the collector. In an otherwise
Figure 3: Latency of non-mirrored trafﬁc as the number of con-
gested output ports is varied.
Figure 4: Flow throughput, averaged over 1 second intervals,
as the number of congested output ports is varied.
idle network, we observed sample latencies of 75–150 µs on our
10 Gbps network and 80–450 µs on our 1 Gbps network.
5.3 Analysis of Sampled Data
We then investigated the characteristics of sample data by varying
the number of max-rate ﬂows with unique source-destination pairs
mirrored to the same monitor port on the switch. All ﬂows have
dedicated ports on the switches, so in the absence of congestion,
each ﬂow saturates its links. We analyze two metrics on the sampled
data: burst length, the number of packets from a given ﬂow received
consecutively by the collector, and inter-arrival length, the number
of packets from other ﬂows received between two bursts of a given
ﬂow. In an idealized setting where each ﬂow has identical offered
(max) load and the switch performs perfect round-robin schedul-
ing, burst length would be one and inter-arrival length would be
NUMFLOWS − 1.
Figure 5 shows a CDF of burst length, in MTUs (i.e., 1500 bytes),
for 13 concurrent ﬂows on the switch. Over 96% of the time, burst
length is less than or equal to one MTU, which indicates the sched-
uler on the monitor port typically samples one packet at a time
from each ﬂow under saturated conditions. Figure 6 shows average
inter-arrival length, in MTUs, for a varying number of ﬂows. For
more than four ﬂows in the network, inter-arrival length increases
linearly, as predicted. In this case we omitted error bars because
they are large, e.g., the standard deviation for 13 ﬂows is roughly 28.
The red (top) line in Figure 7 shows the CDF of inter-arrival lengths,
in MTUs, for 13 ﬂows. Over 85% of the time the inter-arrival time
0%0.02%0.04%0.06%0.08%0.1%0.12%0.14%0.16% 1 2 3 4 5 6 7 8 9Percent of Pkts LostCongested Output PortsMirrorNo Mirror 0 4 8 12 1699.9%Latency(ms)MirrorNo Mirror  0  1  2  3  499%Latency(ms)No MirrorMirror  0  1  2  3 1 2 3 4 5 6 7 8 9MedianLatency(ms)Congested Output PortsNo MirrorMirror 0 2 4 60.1%Throughput(Gbps)MirrorNo Mirror 0 2 4 6 1 2 3 4 5 6 7 8 9MedianThroughput(Gbps)Congested Output PortsMirrorNo MirrorFigure 5: CDF of burst lengths, in MTUs, for 13 concurrent
ﬂows on sampled data.
Figure 7: CDF of inter-arrival lengths, in MTUs, for 13 concur-
rent ﬂows. The red line shows the inter-arrival lengths on the
sampled data, and the blue line shows the length of bursts that
could ﬁt between a sender’s non-transmitting periods.
Figure 6: Inter-arrival lengths, in MTUs, for varying number
of ﬂows on sampled data.
is less than or equal to 13 MTUs, but there is a long tail. Sender
burstiness is a known problem [23] and the blue (bottom) line in Fig-
ure 7 quantiﬁes this burstiness by showing the number of MTUs that
could be sent in the gaps between packet departure times at a ﬂow
source in our testbed. The fact that the gaps observed at the sender
closely match the gaps observed at the collector indicates, that the
large inter-arrival times are an artifact of TCP and not Planck.
Finally, we examine the latency induced on mirrored trafﬁc due
to oversubscription. Figure 8 presents a CDF of the latency between
when a packet is sent and the collector receiving it. Three hosts send
saturated TCP trafﬁc to a unique destination to oversubscribe the
monitor port. We measure the latency imposed on mirrored data for
an IBM G8264 10 Gbps switch and a Pronto 3290 1 Gbps switch,
and observe a median latency of roughly 3.5 ms on the 10 Gbps
network and just over 6 ms on the 1 Gbps network. Figure 9 shows
the average sample latency as we vary the oversubscription factor
on the 10 Gbps switch. The oversubscription factor indicates how
many more times trafﬁc we are sending to the monitor port than its
capacity. The roughly constant observed latency indicates that the
IBM switch likely allocates a ﬁxed amount of buffer space to the
mirrored port.
5.4 Throughput Estimation
Figure 10(a) shows the collector’s throughput estimate of a single
TCP ﬂow as it starts, using a 200 µs rolling average. These results
illustrate the perils of estimating throughput at microsecond scales.
During slow start, TCP sends short bursts of packets at line rate
followed by periods of nearly an RTT of no packets. As the RTT on
our network varies from about 180 µs to 250 µs during the run, the
rolling window usually captures one burst, but sometimes includes
either zero or two bursts, which causes substantial variation in the
instantaneous throughput estimates.
As pointed out in prior work [23], bursty behavior is common
in 10 Gbps environments. To get accurate measurements, Planck
uses the window-based rate estimation mechanism described in
Section 3.2.2 using 200 µs gaps to separate bursts. This results in
more stable estimates, as seen in Figure 10(b).
We further investigate the performance of Planck rate estimation
in Figure 11. As the throughput across the entire switch increases,
the sampling rate of our measurements decreases, which could
Figure 8: An experiment showing the latency between when
a packet is sent and received by the collector on 10 Gbps and
1 Gbps switches during high congestion.
reduce accuracy. In this experiment we increase the oversubscription
rate and measure the mean relative error of the rate estimates. We
obtained ground truth sending rates by using the rate estimation
described in Section 3.2.2 on the full tcpdump traces from the
sender and compared them with Planck’s throughput estimate which
resulted in a roughly constant error rate of 3%.
5.5 Latency Breakdown
In order to better understand all of the events that contribute to
measurement latency we broke down the events a sample packet
sees into a timeline, seen in Figure 12.
This timeline shows the events we were able to directly measure,
such as when a packet is sent (seen via tcpdump), when our col-
lector sees it (via netmap) and when our collector has an accurate
estimate of the ﬂow’s throughput (via our rate estimator).
On a 10 Gbps network with minimum buffering on the monitoring
port we see a total measurement delay of 275–850 µs, which comes
from 75–150 µs (§5.2) until our collector sees the sample and 200–
700 µs (§5.4) until we have a stable rate estimate of the ﬂow. We see
similar results on a 1 Gbps network, where the total measurement
latency is 280–1150 µs coming from a sample delay of 80–450 µs
(§5.2) and the same rate estimation delay.
In terms of prior measurement work presented in Table 1, we
see a 291x speed-up when comparing this minimum-buffering case
against the reported measurement latency of Helios [10]. As our
switch’s ﬁrmware did not allow us to minimize the monitor port’s
buffering, we report our worst-cast measurement latency of 4.2 ms
at 10 Gbps, or an 18x speed-up.
6. APPLICATIONS
In addition to the core Planck architecture, we have built two
applications that exemplify ways that Planck can be extended.
6.1 Vantage Point Monitoring
While there are exceedingly good tools for capturing trafﬁc from
end hosts, most notably tcpdump, there are far fewer tools for
capturing high-ﬁdelity trafﬁc traces from switches short of port-
 0 0.2 0.4 0.6 0.8 1 0 2 4 6 8 10 12 14 16CumulativeFractionBurst Length (MTUs) 0 2 4 6 8 10 12 14 2 4 6 8 10 12 14Inter-arrivalLength(MTUs)Flows 0 0.2 0.4 0.6 0.8 1 0 100 200 300 400 500 600CumulativeFractionInter-arrival Length (MTUs)observed at collectorobserved at sender 0 0.2 0.4 0.6 0.8 1 0 1 2 3 4 5 6CDFMeasurement Latency (ms)IBM G8264 (10Gb)Pronto 3290 (1Gb)Figure 9: Planck’s latency between when a packet is sent and
received by the collector on 10 Gbps with various oversubscrip-
tion factors. As an example, an oversubscription factor of 1.5
means we sent 15 Gbps worth of trafﬁc through a 10 Gbps mon-
itor port.
Figure 11: Planck’s error in estimating throughput with vari-
ous oversubscription factors stays around 3%.
(a)
(b)
Figure 10: Planck’s estimate of throughput as a TCP ﬂows
starts using (a) a 200 µs rolling average and (b) Planck’s
smoothed rate estimator.
mirroring one port at a time to a monitoring port and running
tcpdump on an attached machine.
Instead, our vantage point monitoring application runs inside the
collector and stores as many recently received samples from each
switch as is possible and writes them to a tcpdump-compatible
pcap [27] ﬁle when asked. While this doesn’t provide a full trace
due to sampling, it provides a high ﬁdelity view of what the switch is
actually seeing while only giving up a single output port to monitor
the full switch. Planck’s input and output port information can also
be included by creating a pcap ﬁle for each port.
These pcap ﬁles can then be used with normal network monitoring
and measurement tools like wireshark [43] and tcpdump. We
also note that this application provides the data for much of our
evaluation as it provides data at ﬁner time scales and with more
consistent timestamps than other techniques we tried.
As future work we intend to provide options to infer missed pack-
ets for TCP to provide more complete traces as well as explore how
to use this application to create network traces at the switch level
for use in creating realistic workloads and other research efforts.
6.2 Trafﬁc Engineering
To show Planck’s value when running realistic workloads, we
built a trafﬁc engineering application on top of our base controller
(see Section 4.1) which uses link congestion events produced by
the collector, combined with the other features of the platform, to
quickly reroute ﬂows from congested paths to more lightly-loaded
ones. They key beneﬁt of using Planck for this application is its very
fast control loop, which, as we show in Section 7, greatly improves
the achieved aggregate throughput.
With network events being triggered in hundreds of microsec-
onds, for a controller to keep pace, its decisions about how to alter
forwarding behavior must be based on simple calculations. Our
approach is to pre-install alternate paths between each source and
Figure 12: Timeline of measured and estimated sample latency
events on a 10 Gbps network. Black vertical bars indicate accu-
rately measured events. Blue vertical bars indicate estimates.
destination. Thus, the controller only needs to decide which of the
alternate paths to use, if any, when reacting to an event. In effect
this splits path selection into two parts: ﬁrst selecting a good set of
possible paths ofﬂine and second selecting among them in an online
setting. This drastically reduces the online computation required.
Our application has three main components that extend the base
Planck infrastructure, and which we describe below: (i) the ability
to ﬁnd and pre-install multiple paths in the network, (ii) an algo-
rithm for selecting new paths in the event of congestion, and (iii) a
mechanism to quickly install route changes in the network.
Alternate Forwarding Paths Our controller application, imple-
mented as an additional module on our Floodlight-based controller,
uses layer-2 Per-Address Spanning Trees (PAST) for multipath rout-
ing [39]. PAST provides similar functionality and performance
as Equal-Cost Multipath (ECMP) routing does at layer-3. Lastly,
we chose PAST for its scalability and because, unlike ECMP, our
testbed supports it.
In addition to installing standard PAST routes, a destination-
oriented spanning tree for every reachable MAC address, the con-
troller installs three additional spanning trees per host. Each alter-
nate path is provisioned with its own unused, unique destination
MAC address, which we term a shadow MAC address. We install
four paths in total, but this number is not fundamental. However,
four carefully chosen alternate paths are sufﬁcient for trafﬁc engi-
neering on our topology.
An example of shadow MAC addresses can bee seen in Figure 13.
Using destination MAC addresses that differ from host MAC address
can be problematic for the destination host because, by default, hosts
do not accept packets not destined for them. However, this problem
is easily resolved by installing MAC rewrite rules at the destination’s
egress switch. Even in physical switches, these extra rules only
require TCAM state proportional to the number of alternate routes
and hosts per switch.
 0 2 4 6 8 10 12024681012Throughput (Gbps)Time (ms) 0 2 4 6 8 10 12024681012Throughput (Gbps)Time (ms)packetsent(tcpdump)packetenters swsampleleaves swsamplerecv’d atcollector(netmap)switch buffering0–3.4 ms (est.)200–700 µs75–150 µs (minbuffer) 2.5–3.5 ms (buffer)collectorsenderswitchcollectormakesstable rateestimateMeasured IntervalsEstimatedIntervalsFigure 13: Example of alternate routes between a source and
destination using shadow MAC addresses.
Algorithm 1 – Trafﬁc Engineering Application
Input: A ﬂow congestion notiﬁcation (congn), the application’s view of the
network (net), and a ﬂow timeout (ftimeout)
Output: Route control notiﬁcations
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
process_cong_ntfy(congn, net, ftimeout):
flows = get_congn_flows(congn, net)
paths = get_flow_paths(congn, net, flows)
net_update_state(congn, net, flows, paths)
remove_old_flows(net, ftimeout)
for flow in flows:
greedy_route_flow(net, flow)
greedy_route_flow(net, flow):
bestpath = net_rem_flow_path(net, flow)
bestbtlneck = find_path_btlneck(net, bestpath)
for path in flow.altpaths:
if find_path_btlneck(net, path) > bestbtlneck:
bestpath = path
bestbltneck = find_path_btlneck(net, path)
net_add_flow_path(net, flow, bestpath)
send_route_control(flow, bestpath)
Choosing alternate paths with sufﬁcient path diversity is necessary
for trafﬁc engineering. Consequently, choosing paths with many
common links can lead to an inability to avoid congestion. Currently,
our routing implementation uses the fact that ﬁnding edge-disjoint
spanning trees in fat trees is trivial as each core switch deﬁnes a
unique spanning tree. However, this computation is done ofﬂine, and
more complex algorithms for different topologies or applications are
possible. We leave optimizing the set of pre-installed paths for other
metrics and determining how many alternate paths are required to
future work.