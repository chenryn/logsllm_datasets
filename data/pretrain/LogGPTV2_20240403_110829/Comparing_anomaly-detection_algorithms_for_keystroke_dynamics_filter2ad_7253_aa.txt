title:Comparing anomaly-detection algorithms for keystroke dynamics
author:Kevin S. Killourhy and
Roy A. Maxion
Comparing Anomaly-Detection Algorithms for Keystroke Dynamics
Kevin S. Killourhy
PI:EMAIL
Roy A. Maxion
PI:EMAIL
Dependable Systems Laboratory
Computer Science Department
Carnegie Mellon University
5000 Forbes Ave,
Pittsburgh, PA 15213
Abstract
genuine user.
Keystroke dynamics—the analysis of typing rhythms to
discriminate among users—has been proposed for detect-
ing impostors (i.e., both insiders and external attackers).
Since many anomaly-detection algorithms have been pro-
posed for this task, it is natural to ask which are the top
performers (e.g., to identify promising research directions).
Unfortunately, we cannot conduct a sound comparison of
detectors using the results in the literature because evalua-
tion conditions are inconsistent across studies.
Our objective is to collect a keystroke-dynamics data
set, to develop a repeatable evaluation procedure, and to
measure the performance of a range of detectors so that
the results can be compared soundly. We collected data
from 51 subjects typing 400 passwords each, and we im-
plemented and evaluated 14 detectors from the keystroke-
dynamics and pattern-recognition literature. The three
top-performing detectors achieve equal-error rates between
9.6% and 10.2%. The results—along with the shared data
and evaluation methodology—constitute a benchmark for
comparing detectors and measuring progress.
1. Introduction
Compromised passwords and shared accounts are fre-
quently exploited by both external attackers and insiders.
External attackers test whether accounts use default or com-
mon passwords. Insiders compile lists of shared or com-
promised passwords for later use, e.g., to damage the sys-
tem in the event that they are ﬁred or demoted. If we had
some means, other than knowledge of a password, with
which to identify exactly who is logging into an account,
and to discriminate between the genuine user of an account
and an impostor, we could signiﬁcantly curb these secu-
rity threats. One proposed approach is the use of keystroke
dynamics—the analysis of typing rhythms to discriminate
among users—as a biometric identiﬁer. With keystroke
dynamics, impostor attempts to authenticate using a com-
promised password could be detected and rejected because
their typing rhythms differ signiﬁcantly from those of the
Many anomaly-detection algorithms have been proposed
for detecting impostors (to be reviewed in Section 2). It is
natural to ask how well each of the detectors performs, and
how different detectors compare to each other. The prin-
ciple reason to evaluate and compare detectors is to assess
whether a detector is sufﬁciently dependable to be put into
practice. The European standard for access-control systems
(EN-50133-1) speciﬁes a false-alarm rate of less than 1%,
with a miss rate of no more than 0.001% [3].
At present, no anomaly detector has achieved these er-
ror rates in repeated evaluation, and so keystroke dynamics
could not be deployed as a sole access-control technology.
Consequently, a second reason to try to assess and compare
detector performance is to drive progress toward better re-
sults. By establishing which anomaly-detection strategies
outperform others, the research community gains insight
into what detector characteristics reduce the error rate, iden-
tifying promising directions for further study.
Unfortunately, although researchers have conducted ex-
periments to measure the performance of various anomaly
detectors, these experimental results are impossible to com-
pare. Too many factors vary from one evaluation to an-
other. If we are to assess the state of the art in keystroke
dynamics, and to measure future progress, we need a shared
benchmark data set and a repeatable procedure for conduct-
ing evaluations. Only then can the error rates of anomaly
detectors be properly measured and compared against one
another.
2. Background and related work
In this section, we brieﬂy review various uses for
keystroke dynamics (e.g., password vs. whole-paragraph
analysis), and also various classes of analysis technique
(e.g., one-class anomaly detection vs. multi-class classiﬁ-
cation). Then, we focus on a particular technique: using
anomaly detectors to analyze password-typing times. Even
though it should be possible to compare the performance of
different anomaly detectors, we explain why a comparison
based on evaluation results reported in the literature would
be unsound.
2.1. Review of keystroke dynamics
Since Forsen et al. [6] ﬁrst investigated in 1977 whether
users could be distinguished by the way they type their
names, many different techniques and uses for keystroke
dynamics have been proposed. Peacock et al. [17] con-
ducted an extensive survey of the keystroke-dynamics lit-
erature. Not all of that research is relevant to the use con-
sidered in this work, namely, analyzing password-typing
times. For instance, Gaines et al. [7] considered whether
typists could be identiﬁed by analyzing keystroke times as
they transcribed long passages of text. Techniques for ana-
lyzing passages are different from those for analyzing pass-
words, and the same evaluation data cannot always be used
to assess both.
Further, even among studies of password-typing times,
not all of the extant techniques can be evaluated using the
same procedure. One class of techniques is anomaly de-
tection. The typing samples of a single, genuine user are
used to build (or train) a model of the user’s typing behav-
ior. When a new typing sample is presented, the detector
tests the sample’s similarity to the model, and outputs an
anomaly score. In contrast, another class of techniques is
multi-class classiﬁcation. The typing samples of multiple
users are used to ﬁnd decision boundaries that can be used
to distinguish each user from the others. Since anomaly de-
tectors train on a single user’s data, while multi-class clas-
siﬁers train on multiple users’ data, these two techniques
require different evaluation procedures.
2.2. Anomaly detectors for password timing
Table 1 presents a concise summary of seven studies
from the literature that use anomaly detection to analyze
password-timing data. Each study described one or more
anomaly detectors, gathered password-typing data, con-
ducted an evaluation, and reported the results. The key ob-
servation from this table is that, despite the surface similar-
ities, the studies contain substantial differences beyond just
the anomaly detectors. These differences make it impos-
sible to compare anomaly-detector performance from one
study to another.
The table has been split into two sections for readability.
The ﬁrst column in each section provides a reference to the
source study. The remaining columns provide the following
information:
Detector: a descriptive name for the anomaly-detection al-
gorithm used in the study (sometimes using different termi-
nology than the authors to clarify the type of data analysis
that underlies the detection strategy).
Feature Sets: features used to train and test the detectors
(Enter: the Enter key is considered to be part of the pass-
word; Keydown-Keydown: time between the key presses
of consecutive keys is used as a feature; Keyup-Keydown:
time between the release of one key and the press of the next
is used; Hold: time between the press and release of each
key is used).
Password Length: number of characters used in the pass-
words (N/A indicates the length was not available in the
source study).
Password Reps: number of password-typing repetitions
used to train the detector.
Filtering Users: users whose typing times were highly
variable or inconsistent were identiﬁed during data collec-
tion and excluded from the study.
Filtering Times: the collected timing data were processed
with an outlier-handling procedure to remove extreme val-
ues.
Testing #Attempts: number of attempts that users were
given to try to authenticate successfully (e.g., a 2 means a
user who was rejected the ﬁrst time would be given a second
chance to type the password).
Testing Updating: the typing-model was updated during
testing to accommodate typing behavior that changes over
time.
Results Threshold: name of the procedure for choosing a
threshold on the detectors’ anomaly scores to obtain miss
and false-alarm rates (heuristic: threshold score was chosen
with a heuristic described in the study; zero-miss: thresh-
old was chosen to obtain a miss rate of zero; equal-error:
threshold was chosen to obtain equal miss and false-alarm
rates).
Results Miss/False Alarm: reported miss rates (percent-
age of impostor passwords that are not detected) and false-
alarm rates (percentage of genuine user passwords that are
mistakenly detected as impostors): (a) denotes that the de-
tectors were combined into an aggregate detector, and only
the results for the aggregate were reported; (b) indicates that
only results for two-attempt authentication were reported.
To illustrate the problem we encounter when we try to
use the literature to determine which detector has the best
performance, suppose we tried to compare two detectors:
the Neural Network (auto-assoc) detector developed by Cho
et al. [4], and the Outlier-Count (z-score) detector designed
by Haider et al. [8]. The neural net has a reported miss
rate of 0.0% and a false-alarm rate of 1.0%. The outlier-
counting detector has a reported miss rate of 13% and a
false-alarm rate of 2%. Since the neural net has a better miss
rate and false-alarm rate, one might be inclined to conclude
that it is better than the outlier-counting detector.
However, that conclusion is unsound because the table
reveals many differences between the studies used to eval-
uate the two detectors. The neural net (1) trained on a dif-
ferent set of timing features; (2) had more repetitions in the
training data; (3) did not have to deal with users whose typ-
ing was inconsistent, or with timing outliers in the training
Feature Sets
Password
Source Study
Detector
1 Joyce & Gupta (1990) [10] Manhattan (ﬁltered)
2
Bleha et al. (1990) [2] Euclidean (normed)
Mahalanobis (normed)
Cho et al. (2000) [4] Nearest Neighbor (Mahalanobis)
Neural Network (auto-assoc)
Haider et al. (2000) [8] Fuzzy Logic
Neural Network (standard)
Outlier Count (z-score)
Yu & Cho (2003) [21] SVM (one-class)
Araujo et al. (2004) [1] Manhattan (scaled)
Kang et al. (2007) [11] k-Means
3
4
5
6
7
Enter
Key
(cid:2)
(cid:2)
(cid:2)
(cid:2)
Keydown-
Keydown
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
Keyup-
Keydown
Hold Length Reps
N/A
11–17
11–17
8
30
30
7
7
7
7
7
75–325
75–325
15
15
15
6–10
10+
7–10
75–325
10
10
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
Filtering
Testing
Results (%)
Source Study
Users Times #Attempts Updating Threshold Miss False Alarm
1
1
1
1
1
2
2
2
1
1
1
(cid:2)
(cid:2)
(cid:2)
(cid:2)
heuristic
heuristic
heuristic
zero-miss
zero-miss
heuristic
heuristic
heuristic