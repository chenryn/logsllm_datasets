# Comparing Anomaly-Detection Algorithms for Keystroke Dynamics

**Authors:**
- Kevin S. Killourhy
- Roy A. Maxion

**Contact Information:**
- Kevin S. Killourhy: [EMAIL]
- Roy A. Maxion: [EMAIL]

**Affiliation:**
- Dependable Systems Laboratory
- Computer Science Department
- Carnegie Mellon University
- 5000 Forbes Ave, Pittsburgh, PA 15213

## Abstract

Keystroke dynamics, the analysis of typing rhythms to distinguish among users, has been proposed as a method for detecting impostors, including both insiders and external attackers. Given the numerous anomaly-detection algorithms proposed for this task, it is natural to ask which are the most effective. However, a sound comparison of these detectors is challenging due to inconsistent evaluation conditions across studies.

Our objective is to collect a comprehensive keystroke-dynamics dataset, develop a repeatable evaluation procedure, and measure the performance of various detectors to enable a sound comparison. We collected data from 51 subjects, each typing 400 passwords, and implemented and evaluated 14 detectors from the keystroke-dynamics and pattern-recognition literature. The top three performing detectors achieved equal-error rates between 9.6% and 10.2%. These results, along with the shared data and evaluation methodology, provide a benchmark for comparing detectors and measuring progress.

## 1. Introduction

Compromised passwords and shared accounts are frequently exploited by both external attackers and insiders. External attackers often test whether accounts use default or common passwords, while insiders may compile lists of shared or compromised passwords for later misuse. If we could identify exactly who is logging into an account and distinguish between the genuine user and an impostor, we could significantly reduce these security threats. One proposed approach is the use of keystroke dynamics, which analyzes typing rhythms to discriminate among users. With keystroke dynamics, impostor attempts to authenticate using a compromised password could be detected and rejected because their typing rhythms differ significantly from those of the genuine user.

Many anomaly-detection algorithms have been proposed for detecting impostors (to be reviewed in Section 2). It is natural to ask how well each detector performs and how they compare to one another. The primary reason for evaluating and comparing detectors is to determine if a detector is sufficiently dependable for practical use. The European standard for access-control systems (EN-50133-1) specifies a false-alarm rate of less than 1%, with a miss rate of no more than 0.001%.

Currently, no anomaly detector has achieved these error rates in repeated evaluations, making it infeasible to deploy keystroke dynamics as a sole access-control technology. Therefore, a secondary reason to assess and compare detector performance is to drive progress toward better results. By identifying which anomaly-detection strategies outperform others, the research community can gain insights into what characteristics reduce the error rate, guiding future research directions.

Unfortunately, although researchers have conducted experiments to measure the performance of various anomaly detectors, these experimental results are difficult to compare due to varying factors across evaluations. To accurately assess the state of the art in keystroke dynamics and measure future progress, a shared benchmark dataset and a repeatable evaluation procedure are essential. Only then can the error rates of anomaly detectors be properly measured and compared.

## 2. Background and Related Work

In this section, we briefly review various uses of keystroke dynamics (e.g., password vs. whole-paragraph analysis) and different classes of analysis techniques (e.g., one-class anomaly detection vs. multi-class classification). We then focus on a specific technique: using anomaly detectors to analyze password-typing times. Despite the potential for comparing the performance of different anomaly detectors, we explain why a comparison based on the literature would be unsound.

### 2.1. Review of Keystroke Dynamics

Since Forsen et al. [6] first investigated in 1977 whether users could be distinguished by their typing patterns, many different techniques and uses for keystroke dynamics have been proposed. Peacock et al. [17] conducted an extensive survey of the keystroke-dynamics literature. Not all of this research is relevant to the use considered in this work, specifically, analyzing password-typing times. For example, Gaines et al. [7] considered whether typists could be identified by analyzing keystroke times during the transcription of long passages of text. Techniques for analyzing passages differ from those for analyzing passwords, and the same evaluation data cannot always be used for both.

Furthermore, even among studies of password-typing times, not all techniques can be evaluated using the same procedure. One class of techniques is anomaly detection, where the typing samples of a single, genuine user are used to build a model of the user’s typing behavior. When a new typing sample is presented, the detector tests the sample’s similarity to the model and outputs an anomaly score. In contrast, another class of techniques is multi-class classification, where the typing samples of multiple users are used to find decision boundaries that can distinguish each user from the others. Since anomaly detectors train on a single user’s data, while multi-class classifiers train on multiple users’ data, these two techniques require different evaluation procedures.

### 2.2. Anomaly Detectors for Password Timing

Table 1 provides a concise summary of seven studies from the literature that use anomaly detection to analyze password-timing data. Each study described one or more anomaly detectors, gathered password-typing data, conducted an evaluation, and reported the results. The key observation from this table is that, despite surface similarities, the studies contain substantial differences beyond just the anomaly detectors, making it impossible to compare their performance directly.

The table is split into two sections for readability. The first column in each section provides a reference to the source study. The remaining columns provide the following information:

- **Detector**: A descriptive name for the anomaly-detection algorithm used in the study.
- **Feature Sets**: Features used to train and test the detectors (e.g., Enter, Keydown-Keydown, Keyup-Keydown, Hold).
- **Password Length**: Number of characters in the passwords.
- **Password Reps**: Number of password-typing repetitions used to train the detector.
- **Filtering Users**: Users with highly variable or inconsistent typing times were excluded.
- **Filtering Times**: Collected timing data were processed to remove extreme values.
- **Testing #Attempts**: Number of attempts given to users to authenticate successfully.
- **Testing Updating**: Whether the typing model was updated during testing to accommodate changes over time.
- **Results Threshold**: Procedure for choosing a threshold on the detectors’ anomaly scores (e.g., heuristic, zero-miss, equal-error).
- **Results Miss/False Alarm**: Reported miss rates (percentage of impostor passwords not detected) and false-alarm rates (percentage of genuine user passwords mistakenly detected as impostors).

To illustrate the problem of using the literature to determine which detector has the best performance, consider comparing two detectors: the Neural Network (auto-assoc) detector developed by Cho et al. [4] and the Outlier-Count (z-score) detector designed by Haider et al. [8]. The neural net has a reported miss rate of 0.0% and a false-alarm rate of 1.0%, while the outlier-counting detector has a reported miss rate of 13% and a false-alarm rate of 2%. Although the neural net appears to perform better, this conclusion is unsound due to the many differences between the studies, such as the feature sets, training data, and handling of inconsistent typing and outliers.

| Source Study | Detector | Feature Sets | Password Length | Password Reps | Filtering Users | Filtering Times | Testing #Attempts | Testing Updating | Results Threshold | Miss | False Alarm |
|--------------|----------|--------------|-----------------|---------------|-----------------|-----------------|-------------------|------------------|------------------|------|-------------|
| 1 Joyce & Gupta (1990) [10] | Manhattan (filtered) | Enter, Keydown-Keydown, Keyup-Keydown, Hold | N/A | 75–325 | 1 | 1 | 1 | 1 | Heuristic | 1 | 1 |
| 2 Bleha et al. (1990) [2] | Euclidean (normed), Mahalanobis (normed) | Enter, Keydown-Keydown, Keyup-Keydown, Hold | 11–17 | 75–325 | 1 | 1 | 1 | 1 | Heuristic | 1 | 1 |
| 3 Cho et al. (2000) [4] | Nearest Neighbor (Mahalanobis), Neural Network (auto-assoc) | Enter, Keydown-Keydown, Keyup-Keydown, Hold | 11–17 | 75–325 | 1 | 1 | 1 | 1 | Heuristic | 1 | 1 |
| 4 Haider et al. (2000) [8] | Fuzzy Logic, Neural Network (standard), Outlier Count (z-score) | Enter, Keydown-Keydown, Keyup-Keydown, Hold | 8 | 75–325 | 1 | 1 | 1 | 1 | Heuristic | 1 | 1 |
| 5 Yu & Cho (2003) [21] | SVM (one-class) | Enter, Keydown-Keydown, Keyup-Keydown, Hold | 15 | 75–325 | 1 | 1 | 1 | 1 | Heuristic | 1 | 1 |
| 6 Araujo et al. (2004) [1] | Manhattan (scaled) | Enter, Keydown-Keydown, Keyup-Keydown, Hold | 6–10 | 75–325 | 1 | 1 | 1 | 1 | Heuristic | 1 | 1 |
| 7 Kang et al. (2007) [11] | k-Means | Enter, Keydown-Keydown, Keyup-Keydown, Hold | 10+ | 75–325 | 1 | 1 | 1 | 1 | Heuristic | 1 | 1 |

This table highlights the need for a standardized and repeatable evaluation procedure to ensure fair and meaningful comparisons of anomaly-detection algorithms for keystroke dynamics.