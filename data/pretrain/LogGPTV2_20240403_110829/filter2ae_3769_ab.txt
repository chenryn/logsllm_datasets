所以我们提出了近似的求解算法，通过在$l _{atk}$和$l_ {ret}$上交替执行梯度下降来迭代优化$g_t$和$ \theta$
在第i次迭代时，给定当前的触发器g^{(i-1)} _t_ 以及模型$
\theta^{i-1}$,我们首先通过固定g^{(i-1)}_t，在$l{ret}$上执行梯度下降计算$
\theta^{(i)}$。在实际操作中，这一步会运行$n _{io}$次迭代，这个参数代表的是inner-outer optimization
ratio，用于平衡$l_ {atk}$和$l _{ret}$的优化。然后在对$ \theta^{(i)}$执单步梯度下降后通过最小化$l_
{atk}$得到g^{(i)}_t
对于$g_t$的梯度可以通过下式近似
###  Mixing 函数
mixing函数满足两个目的：
1.对给定的触发器$g_t$，需要在图G中找到最适合替换的子图g
2.使用$g_t$替换g
这里存在很多组合方法，我们将Mixing function限制为一个有效的替换操作符，也就是说，m(G;$g_t$)会使用$g_t$替换G中的g
为了最大化攻击的规避性，我们最好使用一个类似于$g_t$的子图
因此，我们就有了约束:1.g和gt的size是一样的，比如具有相同数量的节点；2.他们有最小的图编辑距离
在图G中找到与gt相似(子图同构)的子图g是一个NP难的问题，我们采用了基于回溯的算法VF2来满足我们设置的情况。VF2通过映射gt中的下一个节点到G中，并反向操作，由此递归地扩展部分匹配。当我们搜索到最相似的子图时，我们就保持当前最高的相速度并且在部分匹配超过这个阈值时提前终止匹配。
###  触发器生成
在前面的式子中，我们假设对于所有的图都是应用了统一的触发器，尽管这样实施起来比较简单，但是这儿还存在可以优化的地方：
1.忽略了单个图的性质，并且攻击可能没那么有效；
2.每个嵌入了触发器的图都是共享同样的pattern，这会让其更容易被检测出来
那么是否可以对每个图都生成特定的触发器并能够最大化有效性和规避性呢？
我们设计了一个自适应的触发器生成器函数$ \phi_w(.)$，给定G的子图g，它会生成触发器$g_t$
从high
level来看，该函数包括两个关键的操作：1.首先把g中的每个结点i映射到其编码$z_i$，这个编码了g的节点特征和拓扑结构；2.其应用了两个生成器函数，第一个讲g的编码映射到$g_t$的拓扑结构，第二个将g的节点编码到$g_t$的节点特征
**怎么编码g的特征和上下文呢？** 我们使用图注意力机制。给定节点对i,j,我们计算注意力系数$
\alpha_{ij}$，这表示j对于i的重要程度（基于他们的节点特征以及拓扑关系）,然后我们应用非线性转换计算其邻居编码的聚合（权重系数就是相应的注意力系数）作为i的编码。我们使用D训练一个注意力网络，在下文中我们将i的编码表示为$z_i$
**怎么将g的编码映射到$g_t$?** $g
_t$包括两部分，即拓扑结构和节点特征。给定两个节点i,j以及对应的编码，我们使用参数化的余弦相似度定义它们在$g_t$中的连接度 A_{i,j}
其中$W_c$是可学习的，$1_P$是指示函数，如果p为真则返回1，否则返回0.如果相似度分数超过0.5则i,j在$g_t$中是相连的
同时，对于g中的节点i，我们定义其在$g_t$中的特征为：
我们将这些可学习的参数都称为w，将g的编码映射到{$X _i$}和{$A_ {ij}$}作为触发器生成器函数$ \phi_w(g)$
**怎么解决g和$g_t$之间的依赖关系？** 大家可能会发现mixing 函数 g=m(G;$g_t$)和触发器生成器函数$g_t=
\phi_w(g)$是相互依赖的。为了解决这个鸡生蛋蛋生鸡的问题，我们交错地更新g和$g_t$
我们随机选择g初始化，在第i次迭代时，我们首先基于i-1次迭代的得到的$g^{(i-1)}$来更新触发器g^{i}_t,然后基于g^{i}_t更新选中的子图g^{i}
###  整体流程
GTA攻击的整体流程如下所示
在其核心部分(4-6行)，交替更新模型$ \theta$，触发器生成器函数 $ \phi_w(.)$以及从每个图中选择的子图g
## 复现
###  安装DGL
在进行复现之前，我们先要额外安装一个库：DGL，这是为方便实现图形神经网络模型族而构建的。它提供了消息传递的多功能控制、通过自动批处理和高度调优的稀疏矩阵内核进行的速度优化，以及可扩展到数亿个节点和边的图形的多GPU/CPU训练。
由于DGL分cpu版本和gpu版本，所以安装的时候一定要适配自己的环境.
下图是我复现时安装所用的命令
###  加载数据及训练目标模型
论文提到使用7类公开的安全敏感的数据集，如下所示
表格中，Graphs是指数据集中图的数量，Avg.#Nodes是指平均每个图中的节点数，Avg.#Edge是指平均每个图中边的数量，Classes是指类的数量，Graphs[class]指class类的图的数量，target
class则是攻击者选择的攻击目标类
我们这里使用其中之一—AIDS
加载数据集后打印相关数据
可以看到和论文给的表格中的值是相符的，说明加载数据这一部分是没问题的
论文中提到可以实现三种SOTA模型，这里我们复现就以GCN为例,代码如下
训练模型
测试模型
结果如下
可以看到训练了40个epoch之后，准确率达到了96.30%
然后保存该模型，接下里进行后门攻击
###  后门攻击
主体代码
生成触发器
注入触发器
其中还有一些关键的地方，包括生成器的代码实现
以及训练生成器的代码实现
训练拓扑生成器
训练特征生成器
攻击代码运行后结果如下
可以看到攻击成功率达到了100%
## 参考
1.Xi Z, Pang R, Ji S, et al. Graph backdoor[C]//30th {USENIX} Security
Symposium ({USENIX} Security 21). 2021.
2.
3.Ryan A. Rossi and Nesreen K. Ahmed. The Network Data Repository with
Interactive Graph Analytics and Visualization. 2015.