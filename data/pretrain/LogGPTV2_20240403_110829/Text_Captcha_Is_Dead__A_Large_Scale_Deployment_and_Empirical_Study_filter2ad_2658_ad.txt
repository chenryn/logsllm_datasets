attackers are more likely to be different even for a fixed-length
captcha scheme, which indicates current attackers may employ
deep learning based solvers. 2) Adversarial captchas tend to have a
significant impact on the performance of the attackers while having
little impact on the normal users.
5.2 Effectiveness of advCAPTCHA
Impact of the Perturbation Algorithm. First, we evaluate the
performance of our adversarial captchas under different algorithm
parameters in Section 3.3: distance metric Lp, which controls the
basic shape of perturbation, and perturbation level ϵ, which con-
trols the total amount of perturbations. We calculate the Success
Rate (SR) of attackers to quantify their performance. For distance
metrics, we choose L0, L∞ and the mixture method (using both
the L0 and the L∞ distance to generate adversarial captchas); Note
that L0 and L∞ are the common distance metrics for generating
adversarial examples [17, 31]. For perturbation levels, we choose
ϵ = 0.1, 0.2, 0.3 for the L∞-based method and ϵ = 25, 50, 100 for the
L0-based method, according to our usability study in Section 5.4.
We also consider the direct and iterative methods, which contains
10 steps and the perturbation level α for each step is one tenth of ϵ
(this setting is consistent with all the following experiments). Note
that we use the same mask size through all the experiments, which
is one quarter of the captcha area. We use Model 1 (in Table 3) as
the substitute model and show the corresponding experimental
results in Figure 5.
From Figure 5, we have the following observations. 1) Adversarial
captchas can effectively reduce the SR of attackers. The lowest SR
for adversarial captchas (reduced to 36%) happens when the mixture
method is applied with ϵ = 100 for L0 distance and ϵ = 0.3 for L∞
distance, and the perturbations are generated by the direct method,
given that the SR of attackers for ordinary captchas is 76%. 2) The
perturbation level ϵ is the most important factor among different
parameters. The SR of the attackers drops noticeably when the
perturbation level is raised. Other parameters, e.g., the distance
(a) ϵ = 25 for L0 and ϵ = 0.1 for L∞
(b) ϵ = 50 for L0 and ϵ = 0.2 for L∞
(c) ϵ = 100 for L0 and ϵ = 0.3 for L∞
Figure 5: Performance comparison under different algorithms and perturbation levels for adversarial captcha generation.
(a) only the target captcha scheme
(b) all real captcha schemes
(c) all captcha schemes
Figure 6: Performance comparison under different model structures and training data for the substitute model.
metric, direct and iterative methods, have less impact on the SR
of the attack model. Although the best performance is achieved
by the direct method, it cannot always outperform the iterative
method. 3) The mixture method, which injects the integration of
different perturbations into a captcha, can improve the security of
adversarial captchas. We observe that compared with only injecting
L∞ or L0 perturbations into a captcha, the mixture method, which
injects both L∞-based and L0-based perturbations into a captcha,
can reduce SR with additional 8% on average. One possible reason
is that the mixture method can leverage the advantage of both L∞-
and L0-based methods in black-box scenarios.
Impact of the Substitute Model. Now, we evaluate the perfor-
mance of our adversarial captchas under different substitute models.
There are two factors need to take account in constructing the sub-
stitute model: model structure and training data (recall Section 3.2).
Thus, we consider several different model structures and training
data in the experiment. For the model structure, we use the CRNN
model as the target model which contains three components: CNN
part, RNN part and CTC part. For each CNN and RNN part, we
consider two model structures, ResNet and LeNet for CNN, and
LSTM and Bi-LSTM for RNN, respectively. Note that the CTC part
is fixed structure. Thus, we use 4 types of models (Model 1-4 in
Table 3) as the substitute model. For training data, we collect the
captchas generated by different captcha schemes (recall Section 4.3)
and divide them into three types: only the target captcha scheme,
which only contains the captcha scheme deployed on the website,
all the real captcha schemes, which contains 7 captcha schemes
owned by our cooperative company, and all the real and synthetic
captcha schemes, which contains 5 captcha schemes synthesized
by ourselves in addition to the above 7 schemes. Based on our
analysis in Figure 5, we choose the best setting as follows. For the
distance metrics, we choose L0, L∞; for the perturbation levels, we
set ϵ = 100 for L0 and ϵ = 0.3 for L∞, respectively. We use the
mixture method to generate adversarial captchas with L0,L∞ dis-
tance and direct, iterative methods. We use these settings as default
settings for all the following experiments in the paper. The results
are shown in Figure 6.
From Figure 6, we have the following observations. 1) The model
structure is not an important factor that influences the performance
of adversarial captchas. There is no substitute model that always
maintain better results than other models. One potential reason
is that solving text-based captcha is not complicated, thus simple
model structures such as LeNet are sufficiently strong to handle
it. In the subsequent experiments, we still employ Model 1 as the
substitute model for convenience. 2) Training data is important to
the security of adversarial captchas. Compared to the first and sec-
ond types (only the target captcha scheme and all the real captcha
schemes) of training data, when we use the third type (all the real
and synthetic captcha schemes) of training data to train a substitute
model, the security of adversarial captchas has been substantially
improved (where the SR of the attack model is reduced by 10% to
20% as compared to the best result in Figure 5). This demonstrates
that using mixed training data to train the substitute model can
increase the security of adversarial captchas. One possible reason is
that the attackers may train their attack models for various captcha
schemes. In other words, the training data that attackers used is
also the mixed training data. As a result, the model we trained by
the captchas generated using different captcha schemes would be
more consistent with the attack model.
Improving Adversarial Examples versus Improving Adver-
sarial Captchas. In [12], the authors proposed a method which
integrates the momentum techniques into the iterative method for
9
Table 4: Usability of advCAPTCHA.
L0 method
without mask
L∞ method
without mask
Adversarial
L0 method
L∞ method
0.4 L0
L∞
50
0.2
100
0.3
Mixture method
25
130
0.1
0.4
90.4 87.5 85.5 75.9
9.6 10.4
7.6
5.9
8.2
9.1
7.8
6.3
Ordinary
89.8
7.5
6.1
100 200 260 0.1
Perturbation level 50
Success rate (%) 91.4 75.2 60.6 41.4 89.8 83.2 73.2 64.6 89.7 85.5 75.3 75.9 89.6 83.9 81.4 78.1
9.8
Average time (s)
Median time (s)
8.6
9.4 10.9 10.8 11.7 7.5
8.3
6.5
100 200 260 0.1
9.5
8.1
0.3
0.4
9.6
8.7
9.7
9.6
9.9
0.2
0.3
8.9
7.9
9.2
8.2
8.6
7.6
7.8
6.7
8.4
7.5
9.1
8.3
0.2
50
7.3
6.4
9.4
9.1
the purpose of escaping from poor local maximum during itera-
tions. They found that the momentum method can improve the
transferability of adversarial examples, i.e., improving the security
of adversarial captchas from the defense perspective, under the
same perturbation level. Therefore, we further evaluate the mo-
mentum method in adversarial captcha generation. However, from
Figure 5, we find that the performance of the momentum method
is not consistent with the previous experience [12]. After using the
momentum method in captcha generation, the security of the gener-
ated captchas cannot be improved in several scenarios. From Figure
5, we observe that under a low perturbation level (e.g., ϵ = 25 for L0,
ϵ = 0.1 for L∞), the momentum method can reduce the SR of the
attack model. In comparison, under a high perturbation level (e.g.,
ϵ = 100 for L0, ϵ = 0.3 for L∞), the momentum method increases
the SR of the attack model, which indicates that the momentum
method may even have negative effect. One possible reason is the
difference between adversarial example generation and adversarial
captcha generation. Compared to generating adversarial examples
that inject human-imperceptible perturbations, we generate ad-
versarial captchas by injecting human-tolerable perturbations and
controlling the areas to inject perturbation. Therefore, it is expected
to study the dedicated method to improve the quality of adversarial
captchas.
5.3 Impact of Fine-tuning the Substitute Model
Now, we evaluate whether fine-tuning the substitute model by the
query of the attack model can improve the security of adversarial
captchas.
According to Section 5.2, we employ Model 1 as the substitute
model and its training data is generated by all the real and synthetic
captcha schemes. In the experiment, we first leverage the substi-
tute model to generate 100,000 adversarial captchas. Then we send
these adversarial captchas to identified attackers and collect their
answers, which will be further leveraged to find-tune the substitute
model, generate corresponding adversarial captchas and send back
to the identified attackers for evaluation again. After fine-tuning
the model, we observe that the SR of attackers has been further
reduced to 12% (as compared to 17% before the fine-tuning process
according to Figure 6). This shows that the model extraction tech-
nique and the fine-tuning can effectively improve the substitute
model to make it more similar to the attacker model, thus increas-
ing the security of the generated adversarial captchas. Moreover,
fine-tuning the substitute model can play a greater role when the
attackers update their attack models. Therefore, our fine-tuning
process can provide adversarial captchas strong adaptivity against
the update of attack models. We will further explore the fine-tuning
performance of advCAPTCHA under the updates of attack models
in the Section 6.1.
Figure 7: The tradeoff between the SR of users and the SR of at-
tackers under the same parameter settings for the mixture method.
From right to left, these four points correspond to ϵ = 0.1 for L∞
method, ϵ = 25 for L0 method; ϵ = 0.2 for L∞ method, ϵ = 50 for L0
method; ϵ = 0.3 for L∞ method, ϵ = 100 for L0 method; and ϵ = 0.4
for L∞ method, ϵ = 130 for L0 method, respectively.
5.4 Usability Analysis
Finally, we conduct experiments to evaluate the usability of adv-
CAPTCHA in practical applications. In the experiment, we set five
groups of adversarial captchas. 1) The captchas generated using the
L0 method without mask, where we inject L0 perturbations into the
captchas and the size of the mask is the full captcha area. Note
that the mask is used to restrict the area of injected perturbations,
and the full captcha area for the mask means no restriction for in-
jected perturbations. 2) The captchas generated using the L∞ method
without mask, where we inject L∞ perturbations into the captchas
and the size of the mask is the full captcha area. 3) The captchas
generated using the L0 method, where we inject L0 perturbations
into the captchas and the size of the mask is one quarter of the
captcha area. 4) The captchas generated using the L∞ method, where
we inject L∞ perturbations into the captchas and the size of the
mask is one quarter of the captcha area. 5) The captchas generated
using the mixture method, where we inject both L0 and L∞ per-
turbations into captchas and the size of each mask is one eighth
of the captcha area. Then, we send these captchas to the normal
group (recall Section 5.1), and collect their answers and the time
consumption for solving each captcha. We show the collected data
in Table 4, including the average successful probability, the average
time consumption, and the median time consumption of all the
users to finish the corresponding captcha, respectively.
From Table 4, we observe that 1) the mask in the perturbation
algorithm is very helpful for maintaining the usability of adversarial
captchas. Without using mask in the adversarial captcha generation,
the SR of human when recognizing adversarial captchas is obviously
lower than that of the ordinary captchas, especially under high
perturbation level. In comparison, if we restrict the location of
perturbations by the mask, the SR of human when recognizing
adversarial captchas is similar to that of the ordinary captchas. 2)
The mixture method, which injects various perturbations into a
10
captcha, almost has no negative impact on human recognition. 3)
ϵ ≤ 0.3 for L∞ and ϵ ≤ 100 for L0 are good choices in practice for
maintaining the usability of the adversarial captchas. These selected
values of ϵ have also shown good performance in balancing the SR
of normal users and the SR of attackers (shown in Figure 7). Within
this range, the SR of normal users decreases much more slowly
than the SR of attackers. Therefore, we consider ϵ ≤ 0.3 for L∞ and
ϵ ≤ 100 for L0 in our experiments.
Evaluating advCAPTCHA with volunteer participants who are
recruited in the real world would be interesting, which, however,
may lead to other limitations. For instance, the scale of users would
be much smaller compared to that of the online test in our exper-
iments. Another challenge lies in the selection of participants to
prevent bias due to demographics. Therefore, following the best-in-
practice evaluation manner, we choose to conduct online experi-
ments for evaluating the usability of advCAPTCHA.
In summary, according to our evaluation, the adversarial captchas
generated by advCAPTCHA, have similar usability as the ordinary
ones. Combining with the security evaluation of advCAPTCHA
in Section 5.2, they together demonstrate that advCAPTCHA is
resilient to ML based attacks while preserving high usability for
human.
6 FURTHER ANALYSIS OF THE FINE-TUNING