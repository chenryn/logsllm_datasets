to enumeration of identiﬁers. Megaupload used random ﬁle identiﬁers and we
discovered ﬁles by guessing identiﬁers. Table 1 summarises the ﬁle data sets.
All ﬁve OCHs oﬀered APIs to access metadata and availability information
about the hosted ﬁles. The APIs allowed to check between 100 and 500 identiﬁers
in one request. For each given identiﬁer, the API returned the availability status
(available or unavailable), and if applicable the ﬁle name and size as well as an
optional user-supplied description of the ﬁle. In all our experiments, we only
accessed the metadata APIs. That is, we never accessed the contents of the ﬁles.
On Filefactory, we obtained a current ﬁle identiﬁer by manually uploading
a test ﬁle and extracted the identiﬁer from the corresponding download link.
Insights into Copyright Infringement in User Uploads to One-Click Hosters
375
We enumerated ﬁle identiﬁers towards the older uploads and occasionally reset
the starting point to a fresh identiﬁer. This was necessary because we noticed
unassigned gaps in the sequential identiﬁer space; link identiﬁers appeared to be
assigned in batches (possibly for load balancing over several servers). We decided
to keep this data set nevertheless because of its interesting characteristics, but
we caution that the results are necessarily less conclusive than for the other
OCHs.
Easy-share, Filesonic and Wupload also used sequential ﬁle identiﬁers. How-
ever, on these OCHs, we designed our experiment in a diﬀerent way: To obtain
valid current ﬁle identiﬁers, we automatically uploaded a test ﬁle every 30 min-
utes. We then enumerated all ﬁle identiﬁers between two subsequent test uploads.
Following this methodology, we discovered new ﬁles within at most one hour of
their upload. Our data sets contain all ﬁles uploaded to the respective OCH dur-
ing two contiuous 24-hour periods, and they cover business days (Wednesday to
Thursday) as well as the end of the weekend (Sunday to Monday).
Megaupload used random identiﬁers drawn from a space of size 368. By ran-
domly guessing identiﬁers, we discovered a valid ﬁle for every 11,275 identiﬁers
that we tested (one hit every 23 API requests), resulting in a sample of 36,657
ﬁle names. In contrast to the data sets gathered from the OCHs with sequen-
tial ﬁle identiﬁers, the Megaupload data set is a sample of all ﬁles that were
available on Megaupload’s servers at the time of the experiment, independent
of the original upload time. From the density of Megaupload’s ﬁle identiﬁer
space, we estimate that Megaupload stored approximately 250 million ﬁles on
their servers in July 2011. Extrapolating from the ﬁle sizes found in the sam-
ple, the total storage capacity in use was around 33 PB (but not accounting
for potential internal de-duplication of ﬁles with identical contents). We noticed
that many ﬁles were called video.flv or megabox.mp3 (9.5 % and 1 % of the
ﬁles, respectively). These ﬁles appeared to correspond to internal data used by
Megaupload’s video and music streaming services Megavideo and Megabox, re-
spectively. As these ﬁle names do not reveal whether the ﬁle contents might be
copyrighted and shared illegally, we excluded these ﬁles from the following anal-
ysis. In the remainder of the paper, we considered only the 32,806 remaining ﬁles
(89.5 %) because these ﬁles represented the actual workload of the ﬁle hosting
service Megaupload.
Undeadlink was a service that generated new “undead” download links for
Megaupload download links submitted by uploaders. Users following such a link
were redirected to a live copy of the corresponding ﬁle on Megaupload. Undead-
link monitored the availability of submitted ﬁles on Megaupload and automati-
cally reuploaded a new copy when the original ﬁle became unavailable.
Undeadlink’s web site displayed the service’s (re)upload queue in real time as
well as a live list of the HTTP referrers of users clicking on “undead” download
links. We continually extracted this data until Undeadlink was taken oﬄine.
To construct a data set of uploaded ﬁles, we retained only the ﬁrst upload (per
internal link identiﬁer) and discarded any repeated upload (due to a ﬁle becoming
unavailable on Megaupload). Table 1 summarises this data set.
376
T. Lauinger et al.
Because of Undeadlink’s functionality and the way it was advertised, we hy-
pothesise that Undeadlink was predominantly used to protect infringing ﬁles
from DMCA takedown eﬀorts. To back up this hypothesis, we analysed the top
50 domain names found in the live HTTP referrer list of users clicking on Un-
deadlink download links. Among these 50 domains (representing 98.7 % of all
clicks), 78.4 % of the clicks came from known and manifestly infringing index-
ing sites, 17.1 % from services allowing uploaders to monetise their download
links (by displaying advertisements), 4.2 % of the clicks came from various un-
classiﬁed web sites, and 0.2 % originated from search engines. These numbers
illustrate that the vast majority of Undeadlink’s (download) click traﬃc was
very likely infringing, and we expect similar results to hold for Undeadlink’s ﬁle
uploads. Thus, we can use the Undeadlink data set as a benchmark for our ﬁle
classiﬁcation.
Dataset Processing. When analysing the ﬁle name data sets, we observed
many ﬁles with extensions such as .part1.rar, .r02, and .003 representing
parts of split archives (e.g., more than half of all ﬁles on Filesonic). Since
a single split archive can consist of hundreds of parts but corresponds to
at most one instance of copyright infringement, not accounting for this phe-
nomenon can overestimate copyright infringement. For this reason, we gener-
ated new data sets by virtually “reassembling” these split ﬁles. That is, we
merged the names of parts into a complete ﬁle name whenever we found a
full sequence of increasing part numbers, where all parts had the same name
preﬁx, archive type and size, except for the last part, which was allowed to
be smaller. As an example, consider the parts etarepsed seviwesuoh 503-
.part1.rar (100 MB), etarepsed seviwesuoh 503.part2.rar (100 MB) and
etarepsed seviwesuoh 503.part3.rar (73 MB), which would be merged into
a single “virtual” ﬁle name etarepsed seviwesuoh 503.rar (273 MB). When
parts were missing, we merged these ﬁle names nevertheless and marked them as
incomplete. In the remainder of the paper, we always use the “reassembled” data
set, and we either include or exclude the names of incomplete ﬁles depending on
the context. The labelled samples, for instance, include the names of incomplete
archives. Table 1 shows the size of the data sets before and after merging ﬁle
names corresponding to split archives, and the fraction of ﬁles in the merged
data set that were “reassembled” successfully. On Filesonic, the initial 55.42 %
of split archive ﬁles account for only 18.51 % of the ﬁle names when merged.
4.2 Ethical Considerations
The purpose of this study is to estimate the proportion of ﬁles related to illegal
ﬁle sharing on OCHs. In designing our measurement setup, we needed to ﬁnd a
balance between our interest in accurate data, and the users’ interest in privacy.
In order to make our data sets most accurate, we would need to download and
inspect the contents of all uploaded ﬁles, including those that were never in-
tended to be public and might contain sensitive information. On the other hand,
Insights into Copyright Infringement in User Uploads to One-Click Hosters
377
fully excluding any risk of privacy violation would impose using only public data
sources. However, using only published download links would make it unfeasible
to quantify the percentage of legitimate content. Such content (including family
pictures or school work) is less likely to have public download links than ma-
terial such as infringing copies of full-length Hollywood movies. Furthermore,
even public or semi-public download links such as those found in “private” ﬁle
sharing communities are not necessarily indexed by search engines, which makes
it unfeasible to gather a representative sample even of public download links.
The compromise that we followed for this work was to extract from OCHs
the metadata of all ﬁles, including private ones, but not to download the ﬁles
themselves. The metadata we used consisted of the ﬁle identiﬁer assigned by the
OCH and the corresponding ﬁle name, ﬁle size, and an optional description of the
ﬁle that the uploader could supply. The data we gathered and used contains no
unique user identiﬁers, IP addresses or other personally identiﬁable information.
Consequently, identifying uploaders would have been possible only in exceptional
cases (by using URLs or user names supplied by the uploaders in the ﬁle name or
description ﬁelds), but at no point did we attempt to do so. Furthermore, we sep-
arated the collection of the data set from its analysis, so that the researchers who
labelled the ﬁle metadata had no access to the ﬁles’ download links. Therefore,
we consider our data sets to be anonymous and preservative of users’ privacy.
The analysis that we carried out was purely passive; the only risk for users
would have been a privacy breach by disclosing or otherwise misusing the data
that we gathered. We handled the data set in a conﬁdential way and disclosed
only aggregate statistics as well as single, uncritical ﬁle names in order to illus-
trate our labelling methodology. Note, furthermore, that the methodology we
used to gather our data sets was published by Nikiforakis et al. in February
2011 and was shown to be used by third parties for unknown (and potentially
nefarious) purposes [17]. Therefore, the additional privacy risk induced by our
data collection is negligible compared to the existing privacy threats.
4.3 Analysis Approach
In order to determine the legitimacy or potential copyright infringement of up-
loaded ﬁles, we chose a random sampling and manual labelling approach. From
each of the six data sets, we selected 1,000 ﬁle names at random. According
to standard theory about conﬁdence intervals for proportions (Equation 1, e.g.
Chapter 13.9.2 in [7]), for a sample size of n = 1000, the actual proportion in
the full data set will lie in an interval of ±0.03 around the proportion p observed
in the sample with 95 % probability (α = 0.05) in the worst case (i.e., p = 0.5).
The implication is that our samples allow us to estimate with high conﬁdence
the proportion of infringing ﬁles in the full data sets.
(cid:2)
p(1 − p)
n
p ± z1−α/2
with np ≥ 10, n(1 − p) ≥ 10 and z0.975 = 1.96 .
(1)
378
T. Lauinger et al.
Table 2. The manual heuristics for ﬁle names and descriptions. Many examples given
in the table satisfy several heuristics; a few names were shortened ([...]).
ID Description
Examples from MU: file name (ﬁle description)
Oceans.Thirteen.2007.1080p.BluRay.x264-HDEX.part06.rar
Kyle.Xy.S01e10.Dvdrip.Dual.Audio.[By.Mixel].avi.002
I1 warez-like name
I2 uploader name
I3 indexing site URL megauploadz.com.hr9rgp6jr9ixpuvq7wnq2v0kspnh9r.avi
I4 commercial name South.Park.S13E13.avi, Lady Gaga - Just Dance.mp3
I5 ﬁle sharing keyw. Acrobat.9.Pro.Cracked.rar (AcroPro crack)
I6 obfuscated name 042e2239101007.part09.rar,
[...]Cel!ne D!0n (1998-FRA) - @µ C0eµr Dµ PI:EMAIL
.rar,
Alcohol120 trial 1.9.7.6221.exe, ubuntu-11.04-desk[...]
L1 free/shareware
L2 unsuspicious ext. Cover letter .doc, crashreporter.ini, favicon .ico
L3 name or descr.
Jura2008.zip (Photos Toussaint 2008), DSC00318.JPG,
IMG 0366.JPG, MOV00026.3GP, William Shakespeare.pptx,
suggesting per-
sonal content
Lottery Number Picker (Uses Random and Array).zip
A precondition for this extrapolation is that we accurately label the samples.
Since we cannot verify the accuracy of our labelling process, we designed a proto-
col that required each sample to be labelled independently by three researchers.
We then merged the results into a single assessment by applying either a consen-
sus or majority approach. We decided not to crowdsource the labelling task in
order to avoid issues with training and data conﬁdentiality.
In the overall assessment, each ﬁle in the samples was labelled according to
the intuition and experience of the researcher as being either potentially infring-
ing, legitimate, or as unknown if the ﬁle name was too ambiguous to make an
informed decision. We complemented our data sample by having each researcher
label the ﬁle names according to nine additional binary heuristics as summarised
in Table 2. The purpose of these heuristics is not to build an automated classiﬁ-
cation tool; in fact, many of the heuristics are diﬃcult to compute automatically
and could be easily circumvented by uploaders if they had a reason to do so.
Rather, we use these heuristics to provide insights into why a ﬁle was classi-
ﬁed as potentially infringing. Six of the heuristics indicate possible copyright
infringement, while three heuristics cover content that appears to be legitimate.
Heuristics Suggesting Infringing Content (I*)
I1. Warez scene title or release group name: The ﬁle name follows the conven-
tions of the Warez scene [18] or related milieux. Often uses periods instead
of spaces and includes quality attributes and the name of the release group.
I2. Uploader name: The ﬁle name/description contains the pseudonym of the
uploader. Occurs on discussion boards to increase the prestige of the up-
loader.
I3. URL of indexing site: The ﬁle name/description contains the URL of an in-
dexing site. Often used as an advertisement vector and to “tag” the uploads.
Insights into Copyright Infringement in User Uploads to One-Click Hosters
379
I4. File name or description contains the name of commercially exploited copy-
righted content: The ﬁle name or description suggests that the ﬁle contains
a speciﬁc piece of content that is normally sold or rented, such as an episode
of a TV show Lost.S04E02.part1.rar, or music by Michael Jackson, and
there is no indication of any fair use case, such as essay, extract, or trailer.
I5. Keywords typical for ﬁle sharing: The ﬁle name or description contains ﬁle
sharing jargon such as DVDrip, screener, keygen or crack, but also sea-
son/episode indications such as S03E09 for TV shows. While serial num-
ber generators or cracks might not infringe copyright, we include them here
because their most likely intent is to enable unauthorised use of software.
I6. Obfuscated ﬁle name: The ﬁle name is seemingly random (and unlikely to be
an abbreviation). Such random names have been observed on indexing sites.
Also includes human-readable ﬁle names with some characters replaced, such
as @ instead of a, which may be an attempt to circumvent simple keyword-
based ﬁle name ﬁlters, e.g. C´eline Dion’s concert Au cœur du stade in Table 2.
Also covers contradictory ﬁle extensions such as .part1.rar.jpg.
Heuristics Suggesting Legitimate Content (L*)
L1. Freeware, shareware (without crack), and abandonware: The ﬁle name sug-
gests freeware (such as a free Linux distribution), abandonware (such as old
console games that are not commercialised any more), shareware, or evalua-
tion versions of commercial software without a crack, serial number generator,
and not labelled as infringing “full” version.
L2. Unsuspicious ﬁle extensions: File extensions not typically used in an illegal
ﬁle-sharing context. Includes extensions for documents (.doc, .odp, .pps,
.xls, .html, .psd, .jpg etc.), but excludes “ambiguous” extensions such as
.eps (sometimes infringing ebooks).
L3. Personal and small-scale commercial content: Files likely produced in a per-
sonal context (holiday pictures, home movies, archives of such content, and
ﬁles following known naming schemes of photo cameras and mobile phones).
The ﬁle name and description must be speciﬁc enough to provide conﬁ-
dence that the contents are indeed legitimate. Does not cover backup.rar
or pictures.rar (sometimes used to conceal copyrighted content), but
does cover pictures-california-holidays.rar (lower probability of mis-
labelling). Also includes content that might not be intended to be shared on
OCHs, but that is not typical either for the large-scale copyright infringe-
ment we aim to characterise, such as source code, lecture slides, or research
papers.
In addition to the manually labelled heuristics, we applied ﬁve automated
heuristics to the random samples. They correspond to aspects of potentially
copyrighted ﬁles that can be computed in an automated way.
380
T. Lauinger et al.
Automated Heuristics (A*)
A1. Split ﬁles: The ﬁle is split into several parts (see Section 4.1). Often used
to bypass ﬁle size restrictions for free users on OCHs or to allow parallel
downloads, but also a tradition in the Warez scene.
A2. Duplicate ﬁles: The same ﬁle has been uploaded several times to the same
OCH. Applies if a ﬁle with the same name and size (except for Easy-share)
is found in the corresponding full data set. Unlikely for personal content.
A3. Public link: Google returns at least one result when searching for the ﬁle
name (exact match).
A4. DMCA takedown notice: Google reports that at least one search result could
not be displayed because they received a DMCA takedown notice from a
copyright holder (when searching for the ﬁle name).
A5. Hit in database of infringing ﬁle names: File name found in a database of
3.4 million download links extracted from more than ten known infringing
indexing sites in prior work [12, 13].
By deﬁnition, heuristics are not exact; we do not treat them as accurate indica-
tors of copyright infringement. Rather, we use them to illustrate characteristics
of potentially infringing ﬁles. We exclusively rely on the independent overall
assessment of the three researchers to classify a ﬁle as infringing or legitimate.
4.4 Limitations
Motivated by privacy concerns, the choices that we made when designing our
experiments induce inherent limitations on the results presented in this paper.
Our choice not to download any ﬁles because of ethical considerations means
that we cannot evaluate the correctness of our classiﬁcation. This is an issue
especially for mislabelled ﬁles that do not contain what their ﬁle name suggests,
or ﬁles with obfuscated ﬁle names where the name reveals nothing concrete
about the ﬁles’ contents. Furthermore, fair use may not be discernible from the
ﬁle metadata alone. While we acknowledge that our results cannot be exact
(this would be diﬃcult to achieve even with access to the ﬁles’ contents), we are
conﬁdent that our results reﬂect the general trends of illegal ﬁle sharing occurring
on OCHs. To make our ﬁle classiﬁcation methodology more transparent, we
deﬁned a set of heuristics. In order to reduce personal bias, the ﬁle metadata
samples were labelled independently by three researchers and the results were
merged using a conservative consensus algorithm.
For a separate study, we conducted an experiment to estimate the proportion
of polluted content on two popular indexing sites that allowed anonymous posts.
File pollution can occur due to intentionally or unintentionally mislabelled ﬁles.
We found that more than 93 % of the indexed ﬁles were authentic [13]. We do not
claim that these ﬁndings can be extrapolated to the data sets used in this paper.
There are reports about malware being hosted on OCHs [9], for instance. Yet,
in contrast to P2P [3,14], copyright owners do not appear to upload fake ﬁles to
OCHs because they can use DMCA takedown notices to remove infringing ﬁles,
which we assume to be more eﬀective than adding fake ﬁles.
Insights into Copyright Infringement in User Uploads to One-Click Hosters
381
5 Analysis
Ideally, the classiﬁcation result of our ﬁle name labelling should be a binary label,
either legitimate or infringing. In practice, however, it is very challenging to make
a binary decision for each ﬁle, especially when the ﬁle contents are not available
as in our study. In the following, we explain how our conservative approach is
responsible for a relatively large fraction of ﬁles with unknown label on some
OCHs, and we present the overall assessment results obtained by merging the
classiﬁcations of the three labellers. Subsequently, we analyse the individual
heuristic indicators to gain more conﬁdence in our overall labels, and we provide
further insights into some characteristics of ﬁles uploaded to OCHs.
5.1 Consensus Merging and Unknown Labels
To merge the independent labelling results of the three researchers, we applied
a consensus algorithm. That is, we conservatively assumed that a heuristic did
not apply (or that the overall assessment was unknown) unless all three re-
searchers agreed. According to Table 3, a consensus in the overall assess-
ment was reached for a little more than half of the ﬁles in the Filefactory and