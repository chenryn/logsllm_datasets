user-space ASLR can be weakened, and reduce the ASLR entropy
of the hypervisor in a virtual-machine setting. In Section 5.3, we
use Collide+Probe as a covert channel to extract secret data from
the kernel in a Spectre attack. In Section 5.4, we recover secret keys
in AES T-table implementations.
Timing Measurement. As explained in Section 2.3, we cannot rely
on the rdtsc instruction for high-resolution timestamps on AMD
CPUs since the Zen microarchitecture. As we use recent AMD
CPUs for our evaluation, we use a counting thread (cf. Section 2.3)
running on the sibling logical CPU core for most of our experiments
if applicable. In other cases, e.g., a covert channel scenario, the
counting thread runs on a different physical CPU core.
Environment. We evaluate our attacks on different environments
listed in Table 1, with CPUs from K8 (released 2013) to Zen 2 (re-
leased in 2019). We have reverse-engineered 2 unique hash func-
tions, as described in Section 3. One is the same for all Zen microar-
chitectures, and the other is the same for all previous microarchi-
tectures with a way predictor.
5.1 Covert Channel
A covert channel is a communication channel between two parties
that are not allowed to communicate with each other. Such a covert
channel can be established by leveraging a side channel. The µTag
used by AMD’s L1D way prediction enables a covert channel for
two processes accessing addresses with the same µTag.
For the most simplistic form of the covert channel, two processes
agree on a µTag and a cache set (i.e., the least-significant 12 bits of
the virtual addresses are the same). This µTag is used for sending
and receiving data by inducing and measuring cache misses.
In the initialization phase, both parties allocate their own page.
The sender chooses a virtual address vS , and the receiver chooses
a virtual address vR that fulfills the aforementioned requirements,
i.e., vS and vR are in the same cache set and yield the same µTag.
The µTag can simply be computed using the reverse-engineered
hash function of Section 3.
Table 1: Tested CPUs with their microarchitecture (µ-arch.)
and whether they have a way predictor (WP).
WP
Setup CPU
Lab
Lab
Lab
Lab
Lab
Lab
Lab
Lab
Lab
Lab
Lab
Lab
Lab
Cloud AMD EPYC 7401p
Cloud AMD EPYC 7571
µ-arch.
K8
AMD Athlon 64 X2 3800+
✗
K10
AMD Turion II Neo N40L
✗
K10
AMD Phenom II X6 1055T
✗
Bobcat
AMD E-450
✗
Jaguar
AMD Athlon 5350
✗
Bulldozer
AMD FX-4100
✓
Piledriver
AMD FX-8350
✓
Steamroller ✓
AMD A10-7870K
Zen
AMD Ryzen Threadripper 1920X
✓
Zen
AMD Ryzen Threadripper 1950X
✓
AMD Ryzen Threadripper 1700X
Zen
✓
AMD Ryzen Threadripper 2970WX Zen+
✓
Zen 2
AMD Ryzen 7 3700X
✓
Zen
✓
Zen
✓
To encode a 1-bit to transmit, the sender accesses address vS .
To transmit a 0-bit, the sender does not access address vS . The
receiving end decodes the transmitted information by measuring
the access time when loading address vR. If the sender has accessed
address vS to transmit a 1, the collision caused by the same µTag
of vS and vR results in a slow access time for the receiver. If the
sender has not accessed address vS , no collision caused the address
vR to be evicted from L1D and, thus, the access time is fast. This
timing difference allows the receiver to decode the transmitted bit.
Different cache-based covert channels use the same side chan-
nel to transmit multiple bits at once. For instance, different cache
lines [30, 48] or different cache sets [48, 53] are used to encode
one bit of information on its own. We extended the described µTag
covert channel to transmit multiple bits in parallel by utilizing mul-
tiple cache sets. Instead of decoding the transmitted bit based on
the timing difference of one address, we use two addresses in two
cache sets for every bit we transmit: One to represent a 1-bit and
the other to represent the 0-bit. As the L1D has 64 cache sets, we
can transmit up to 32 bit in parallel without reusing cache sets.
Performance Evaluation. We evaluated the transmission and er-
ror rate of our covert channel in a local setting and a cloud set-
ting by sending and receiving a randomly generated data blob. We
achieved a maximum transmission rate of 588.9 kB/s (σ ¯x = 0.544,
n = 1000) using 80 channels in parallel on the AMD Ryzen Thread-
ripper 1920X. On the AMD EPYC 7571 in the Amazon EC2 cloud, we
achieved a maximum transmission rate of 544.0 kB/s (σ ¯x = 0.548,
n = 1000) also using 80 channels. In contrast, L1 Prime+Probe
achieved a transmission rate of 400 kB/s [59] and Flush+Flush a
transmission rate of 496 kB/s [30]. As illustrated in Figure 4, the
mean transmission rate increases with the number of bits sent in
parallel. However, the error rate increases drastically when trans-
mitting more than 64 bits in parallel, as illustrated in Figure 6. As
the number of available different cache sets for our channel is
exhausted for our covert channel, sending more bits in parallel
ASIA CCS ’20, June 1–5, 2020, Taipei, Taiwan
Lipp, et al.
]
s
/
B
k
[
R
T
600
400
200
0
0
20
40
AMD Ryzen Threadripper 1920X
AMD EPYC 7751
60
80
Number of Channels
Table 2: Evaluation of the ASLR experiments
Target
Entropy Bits Reduced Success Rate Timing Source
Time
Linux Kernel
User Process
Virt. Manager
Virt. Module
Mozilla Firefox
Google Chrome
Chrome V8
9
13
28
18
28
28
28
7
13
16
8
15
15
15
98.5%
88.9%
90.0%
98.9%
98.0%
86.1%
100.0%
thread
thread
rdtsc
rdtsc
web worker
web worker
rdtsc
0.51 ms (σ = 12.12 µs)
1.94 s (σ = 1.76 s)
2.88 s (σ = 3.16 s)
0.14 s (σ = 1.74 ms)
2.33 s (σ = 0.03 s)
2.90 s (σ = 0.25 s)
1.14 s (σ = 0.03 s)
Figure 4: Mean transmission rate of the covert channels us-
ing multiple parallel channels on different CPUs.
would reuse already used sets. This increases the chance of wrong
measurements and, thus, the error rate.
mapping functions (Section 3.2) to infer bits of the addresses. We
show an additional attack on heap ASLR in Appendix C.
Error Correction. As accesses to unrelated addresses with the same
µTag as our covert channel introduce noise in our measurements,
an attacker can use error correction to achieve better transmission.
Using hamming codes [33], we introduce n additional parity bits
allowing us to detect and correct wrongly measured bits of a packet
with a size of 2n − 1 bits. For our covert channel, we implemented
different Hamming codes H(m, n) that encode n bits by adding
m − n parity bits. The receiving end of the covert channel computes
the parity bits from the received data and compares it with the
received parity bits. Naturally, they only differ if a transmission
error occurred. The erroneous bit position can be computed, and
the bit error corrected by flipping the bit. This allows to detect up
to 2-bit errors and correct one-bit errors for a single transmission.
We evaluated different hamming codes on an AMD Ryzen Thread-
ripper 1920X, as illustrated in Figure 7 in Appendix B. When sending
data through 60 parallel channels, the H(7, 4) code reduces the error
rate to 0.14 % (σ ¯x = 0.08, n = 1000), whereas the H(15, 11) code
achieves an error rate of 0.16 % (σ ¯x = 0.08, n = 1000). While the
H(7, 4) code is slightly more robust [33], the H(15, 11) code achieves
a better transmission rate of 452.2 kB/s (σ ¯x = 7.79, n = 1000).
More robust protocols have been used in cache-based covert
channels in the past [48, 53] to achieve error-free communication.
While these techniques can be applied to our covert channel as well,
we leave it up to future work.
Limitations. As we are not able to observe µTag collisions be-
tween two processes running on sibling threads on one physical
core, our covert channel is limited to processes running on the same
logical core.
5.2 Breaking ASLR and KASLR
To exploit a memory corruption vulnerability, an attacker often
requires knowledge of the location of specific data in memory.
With address space layout randomization (ASLR), a basic memory
protection mechanism has been developed that randomizes the
locations of memory sections to impede the exploitation of these
bugs. ASLR is not only applied to user-space applications but also
implemented in the kernel (KASLR), randomizing the offsets of
code, data, and modules on every boot.
In this section, we exploit the relation between virtual addresses
and µTags to reduce the entropy of ASLR in different scenarios.
With Collide+Probe, we can determine the µTags accessed by the
victim, e.g., the kernel or the browser, and use the reverse-engineered
5.2.1 Kernel. On modern Linux systems, the position of the kernel
text segment is randomized inside the 1 GB area from 0xffff ffff
8000 0000 - 0xffff ffff c000 0000 [39, 46]. As the kernel image
is mapped using 2 MB pages, it can only be mapped in 512 different
locations, resulting in 9 bit of entropy [65].
Global variables are stored in the .bss and .data sections of
the kernel image. Since 2 MB physical pages are used, the 21 lower
address bits of a global variable are identical to the lower 21 bits
of the offset within the the kernel image section. Typically, the
kernel image is public and does not differ among users with the
same operating system. With the knowledge of the µTag from the
address of a global variable, one can compute the address bits 21 to
27 using the hash function of AMD’s L1D cache way predictor.
To defeat KASLR using Collide+Probe, the attacker needs to
know the offset of a global variable within the kernel image that is
accessed by the kernel on a user-triggerable event, e.g., a system
call or an interrupt. While not many system calls access global
variables, we found that the SYS_time system call returns the value
of the global second counter obj.xtime_sec. Using Collide+Probe,
the attacker accesses an address v′ with a specific µTag µv′ and
schedules the system call, which accesses the global variable with
address v and µTag µv. Upon returning from the kernel, the attacker
probes the µTag µv′ using address v′. On a conflict, the attacker
infers that the address v′ has the same µTag, i.e., t = µv′ = µv.
Otherwise, the attacker chooses another address v′ with a different
µTag µv′ and repeats the process. As the µTag bits t0 to t7 are known,
the address bits v20 to v27 can be computed from address bits v12
to v19 based on the way predictor’s hash functions (Section 3.2).
Following this approach, we can compute address bits 21 to 27 of
the global variable. As we know the offset of the global variable
inside the kernel image, we can also recover the start address of the
kernel image mapping, leaving only bits 28 and 29 unknown. As
the kernel is only randomized once per boot, the reduction to only
4 address possibilities gives an attacker a significant advantage.
For the evaluation, we tested 10 different randomization offsets
on a Linux 4.15.0-58 kernel with an AMD Ryzen Threadripper 1920X
processor. We ran our experiment 1000 times for each randomiza-
tion offset. With a success rate of 98.5 %, we were able to reduce the
entropy of KASLR on average in 0.51 ms (σ = 12.12 µs, n = 10 000).
While there are several microarchitectural KASLR breaks, this
is to the best of our knowledge the first which reportedly works
on AMD and not only on Intel CPUs. Hund et al. [35] measured
Take A Way: Exploring the Security Implications of AMD’s Cache Way Predictors
ASIA CCS ’20, June 1–5, 2020, Taipei, Taiwan
differences in the runtime of page faults when repeatedly access-
ing either valid or invalid kernel addresses on Intel CPUs. Bar-
resi et al. [11] exploited page deduplication to break ASLR: a copy-
on-write pagefault only occurs for the page with the correctly
guessed address. Gruss et al. [28] exploited runtime differences in
the prefetch instruction on Intel CPUs to detect mapped kernel
pages. Jang et al. [39] showed that the difference in access time to
valid and invalid kernel addresses can be measured when suppress-
ing exceptions with Intel TSX. Evtyushkin et al. [22] exploited the
branch-target buffer on Intel CPUs to gain information on mapped
pages. Schwarz et al. [65] showed that the store-to-load forwarding
logic on Intel CPUs is missing a permission check which allows
to detect whether any virtual address is valid. Canella et al. [16]
exploited that recent stores can be leaked from the store buffer on
vulnerable Intel CPUs, allowing to detect valid kernel addresses.
5.2.2 Hypervisor. The Kernel-based Virtual Machine (KVM) is a
virtualization module that allows the Linux kernel to act as a hyper-
visor to run multiple, isolated environments in parallel called virtual
machines or guests. Virtual machines can communicate with the
hypervisor using hypercalls with the privileged vmcall instruction.
In the past, collisions in the branch target buffer (BTB) have been
used to break hypervisor ASLR [22, 78].
In this scenario, we leak the base address of the KVM kernel
module from a guest virtual machine. We issue hypercalls with
invalid call numbers and monitor, which µTags have been accessed
using Collide+Probe. In our evaluation, we identified two cache sets
enabling us to weaken ASLR of the kvm and the kvm_amd module
with a success rate of 98.8 % and an average runtime of 0.14 s (σ =
1.74 ms, n = 1000). We verified our results by comparing the leaked
address bits with the symbol table (/proc/kallsyms).
Another target is the user-space virtualization manager, e.g.,
QEMU. Guest operating systems can interact with virtualization
managers through various methods, e.g., the out instruction. Like-
wise to the previously described hypercall method, a guest virtual
machine can use this method to trigger the managing user pro-
cess to interact with the guest memory from its own address space.
By using Collide+Probe in this scenario, we were able to reduce
the ASLR entropy by 16 bits with a success rate of 90.0 % with an
average run time of 2.88 s (σ = 3.16 s, n = 1000).
JavaScript. In this section, we show that Collide+Probe is
5.2.3
not only restricted to native environments. We use Collide+Probe
to break ASLR from JavaScript within Chrome and Firefox. As the
JavaScript standard does not define a way to retrieve any address in-
formation, side channels in browsers have been used in the past [57],
also to break ASLR, simplifying browser exploitation [25, 65].
The idea of our ASLR break is similar to the approach of reverse-
engineering the way predictor’s mapping function, as described
in Section 3.2. First, we allocate a large chunk of memory as a
JavaScript typed array. If the requested array length is big enough,
the execution engine allocates it using mmap, placing the array at
the beginning of a memory page [29, 69]. This allows using the
indices within the array as virtual addresses with an additional
constant offset. By accessing pairs of addresses, we can find µTag
collisions allowing us to build an equation system where the only
unknown bits are the bits of the address where the start of the array
is located. As the equation system is very small, an attacker can
trivially solve it in JavaScript.
However, to distinguish between colliding and non-colliding ad-
dresses, we require a high-precision timer in JavaScript. While
the performance.now() function only returns rounded results
for security reasons [3, 14], we leverage an alternative timing
source [25, 69]. For our evaluation, we used the technique of a count-
ing thread constantly incrementing a shared variable [25, 48, 69, 80].
We tested our proof-of-concept in both the Chrome 76.0.3809
and Firefox 68.0.2 web browsers as well as the Chrome V8 stan-
dalone engine. In Firefox, we are able to reduce the entropy by
15 bits with a success rate of 98 % and an average run time of 2.33 s
(σ = 0.03 s, n = 1000). With Chrome, we can correctly reduce the
bits with a success rate of 86.1 % and an average run time of 2.90 s
(σ = 0.25 s, n = 1000). As the JavaScript standard does not provide
any functionality to retrieve the addresses used by variables, we
extended the capabilities of the Chrome V8 engine to verify our re-
sults. We introduced several custom JavaScript functions, including
one that returned the virtual address of an array. This provided us
with the ground truth to verify that our proof-of-concept recovered
the address bits correctly. Inside the extended Chrome V8 engine,
we were able to recover the address bits with a success rate of 100 %
and an average run time of 1.14 s (σ = 0.03 s, n = 1000).
5.3 Leaking Kernel Memory
In this section, we combine Spectre with Collide+Probe to leak
kernel memory without the requirement of shared memory. While
some Spectre-type attacks use AVX [70] or port contention [13],
most attacks use the cache as a covert channel to encode secrets [17,
41]. During transient execution, the kernel caches a user-space ad-
dress based on a secret. By monitoring the presence of said address
in the cache, the attacker can deduce the leaked value.
As AMD CPU’s are not vulnerable to Meltdown [49], stronger
kernel isolation [27] is not enforced on modern operating systems,
leaving the kernel mapped in user space. However, with SMAP
enabled, the processor never loads an address into the cache if the
translation triggers a SMAP violation, i.e., the kernel tries to access
a user-space address [9]. Thus, an attacker has to find a vulnerable
indirect branch that can access user-space memory. We lift this