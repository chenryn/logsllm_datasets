Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:28:00 UTC from IEEE Xplore.  Restrictions apply. 
2
L
n
i
n
o
i
t
a
b
r
u
t
r
e
P
l
i
a
m
n
M
i
0.25
0.20
0.15
0.10
0.05
0.00
YOLOv3
SSD300
RetinaResnet
Resnet152
2
L
n
i
n
o
i
t
a
b
r
u
t
r
e
P
l
i
a
m
n
M
i
g
Blur
Fo
w
o
n
S
Frost
ess
ntrast
htn
otio
o
Brig
C
M
n
Blur
ussia
n
a
G
niform
ditiv
a
G
e
d
n
e
ussia
d
n
Ble
n
ns
p
S
atial
nslate
Rotatio
ntalTra
VerticalTra
orizo
altA
S
H
er
p
p
Pe
d
n
U
e
ditiv
d
A
d
A
cloud model A
cloud model B
cloud model C
10e-4
10e-5
10e-6
10e-7
Brightness
Additive U niform
Additive G aussian
Contrast
Rotation
Blended U niform
SaltAndPepper
H orizontalTrans
VerticalTrans
Spatial
Fig. 6: Robustness Comparison among YOLOv3, SSD300,
RetinaResnet and Resnet152 with input size (416×416).
Fig. 7: Robustness Comparison among 3 commercial
content moderators with input size (224×224)
b) Label Choice Impact on the Robustness: The ro-
bustness varies with different target object classes since
the robustness sensitivity of different classes is not the
same. If we choose ’person’ instead of ’bus’, the ro-
bustness threshold for comparison will be different. For
the ’Misclassiﬁcation’ criterion used for image classiﬁer
evaluation, the robustness threshold remains the same.
3) Cloud-based Content Moderator: Machine learning
as a service(MLaaS) offers API access to perform model
inference on the given inputs. Content moderation is one
of the important tasks to prevent any access to inappro-
priate content such as NSFW. We choose three commer-
cial MLaaS providers and randomly selected 400 NSFW
images [20] and ran through 10 different safety prop-
erties as indicated in x-axis label in Figure 7. Since the
output format of the cloud-based models varies, the pre-
diction result can either be the logits, or the class labels.
Thus we employ a combination of Misclassi f ication,
Original Con f idenceLoss, and TopKMisclassi f ication for
the test criteria. Our goal is to ﬁnd minimal perturbation
to the input so that it is no longer considered porn. We
do not include Blur category in this test because the low
quality from blurring the image is unlikely an incentive
to the attackers. The results indicate that with a small
−7 to
amount of perturbations in the magnitude from 10
−4, these cloud-based content moderation models are
10
easy to break. Interestingly, all three models have very
close robustness for safety properties related to spatial
transformation. ModelA is slightly better than the other
two on Luminance and Corruption related properties
with an exception that all three models perform roughly
equal robust against SaltAndPepperNoise.
B. Property-speciﬁc Robustness Metrics
Though Lp norm-based metric is widely used for sci-
entiﬁc research, it is difﬁcult to comprehend from normal
◦
◦
◦
to +180
users’ perspectives. We ﬁnd some alternative metrics,
which are easier to explain for some safety properties.
1) Rotation Angles: In our experiment, we rotate the
image from −180
. It would make more
sense to use the ranges of angles as the amount of
perturbations applied to the original image. Figure 8
demonstrates the statistics of the rotation angles needed
to fool the models from different learning tasks. We only
compare the results within each learning task because of
the different datasets used in the experiment.
◦
For each input image, we start with 0
and rotate until
the image is misclassiﬁed. The upper and lower bound
of the box indicates the mean of the minimal angle the
and −180
◦
image needs to rotate towards both +180
over a randomly sampled 1000 images. The upper and
lower ceilings indicate the largest minimal angle (the
worst case), in which the image needs to rotate towards
either direction. The result consistently conﬁrms our pre-
vious ﬁndings that Resnet152, Densenet121 tolerate more
image rotation, while alexnet has the least toleration.
Similarly, ssd300 performs the best for object detector.
All of the cloud-based models can make consistent porn
detection with a broader range of rotation angles. Inter-
estingly, the amount of rotations towards either direction
seems near symmetric.
2) Kernel Dimension for Motion Blur: Since we leverage
OpenCV [14] to implement Motion Blur property, the
amount of the perturbation depends on a parameter
called the kernel dimension, a.k.a. kernel size (KS). The
larger the KS (perturbation) the model can tolerate, the
more robust the model is. Thus the KS can be a sup-
plementary metric for the model robustness evaluation.
Figure 9 demonstrates the KS required to achieve enough
motion blur perturbations on the input image. The solid
points connected by a dotted line are the mean KS over
the test dataset for a given model. The upper and lower
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:28:00 UTC from IEEE Xplore.  Restrictions apply. 
155
successfully deceive the underlying models according to
a speciﬁc safety property with uni f orm noise in row 1,
brightness in row 2, contrast in row 3, and rotation in row
4. We leave out the examples generated from other safety
properties due to the limited space. In summary, our
benchmarking dataset contains all the safety-violation
images including 192,591 images generated based on
ImageNet, 52,207 images generated based on MSCOCO
, and 13,690 images generated based on the NSFW.
Fig. 8: Rotation Angle
Fig. 9: Kernel Size
IV. RELATED WORK
original
yolov3
retina
resnet152
l2 : 0
−1
l2 : 10
−2
l2 : 10
−3
l2 : 10
Fig. 10: Benchmarking Dataset: Images causing mispre-
dictions with least perturbations.
bounds indicate the largest and smallest minimal KS
ever reached by an input. The result proves that object
detectors require larger KS to generate blurs to deceive
the models.
C. Benchmarking Datasets
In this work, we produce benchmarking datasets [21]
,
including all
the least perturbed examples gener-
ated across different types of models based on vari-
ous safety properties. Fig 10 gives an example of this
dataset. The leftmost column contains the original image
from MSCOCO dataset. The images from the second to
the rightmost column are generated against YOLOv3,
RetinaResnet, and Resnet152 with corresponding L2 dis-
−3 respectively.
tances in the magnitude of 10
Each row includes the perturbed examples that can
−1, 10
−2, 10
Adversarial examples (AE) [1] [2] [22]can be crafted
to confuse deep learning models in a systematic way
assuming white-box access. Later on, researchers took
physical conditions into account and came up with more
advanced approaches [23] [24] [25]to confuse physi-
cal objects like stop-sign and vehicles. In practice, the
model’s parameters and structure are unavailable to the
attacker. The adversary has to leverage on the transfer-
ability [26] [27] of AE. However, the chance of any large
scale AE attack campaign in the real-world is slim.
Recent efforts [28] [29] [30] [31] [32] seek formal meth-
ods to derive provable robustness by leveraging tech-
niques such as SMT solver and abstract interpretation.
However, none of the existing tools can scale to practical
datasets (e.g., ImageNet) and network structures. Huang,
et al., [33] seem to be able to check the safety of deep
neural networks for realistic images. Its efﬁciency suffers
from the exponential increase in the number of features
and the prohibitive complexity for larger images. Thus
state-of-the-art formal methods are not easy to be ap-
plied in practice. Certiﬁed robustness via randomized
smoothing [34] [28] derives the certiﬁed robustness in L2
norm. Our method provides the ﬂexibility for the users
to choose any Lp norm to their needs.
Hendrycks et al. [9]’s benchmarking work is the clos-
est one to ours. They studied model robustness to the
common corruption. Rather than following their manual
threat severity setup, we produce a tighter and more
accurate robustness boundary for the models. Object-
Net [5] collects subtle data to confuse object detectors
and shows a 40-50% drop on the performance. It can be
combined with our approach to produce more data aug-
mentation for robustness evaluation in a safety-critical
context.
V. CONCLUSION
In this paper, we discuss the pressing need for DL
model robustness evaluation in safety-critical settings
with no presence of the attacker. We proposed a system-
atic framework to measure the minimal perturbations
needed to achieve violations against safety properties.
Our results from the evaluation of ImageNet classiﬁers,
object detectors, and content moderators indicate the
status quo robustness. The images introducing the vi-
olations are open to the community.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:28:00 UTC from IEEE Xplore.  Restrictions apply. 
156
[20] Nsfw dataset. ”https://github.com/sajithm/nsfw-v1”, 2018. [On-
line; accessed 13-Dec-2019].
[21] Baidu
perceptron
benchmarking
”https://github.com/advboxes/perceptron-
robustness
dataset.
benchmark/tree/master/dsnbenchmark”.
[22] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson,
Z. Berkay Celik, and Ananthram Swami. The Limitations of
arXiv e-prints, page
Deep Learning in Adversarial Settings.
arXiv:1511.07528, Nov 2015.
[23] Zhenyu Zhong, Yunhan Jia, Weilin Xu, and Tao Wei. Perception
deception: Physical adversarial attack challenges and tactics for
dnn-based object detection. BlackHat Europe, 2018.
[24] Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir
Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, and
Dawn Song. Robust Physical-World Attacks on Deep Learning
In Computer Vision and Pattern Recognition
Visual Classiﬁcation.
(CVPR), 2018.
[25] Yue Zhao, Hong Zhu, Ruigang Liang, Qintao Shen, Shengzhi
Seeing isn’t believing: Towards more
Zhang, and Kai Chen.
robust adversarial attack against real world object detectors. In
Proceedings of the 2019 ACM SIGSAC Conference on Computer and
Communications Security, CCS ’19, pages 1989–2004, New York,
NY, USA, 2019. ACM.
[26] Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. Trans-
from Phenomena to Black-
arXiv e-prints, page
ferability in Machine Learning:
Box Attacks using Adversarial Samples.
arXiv:1605.07277, May 2016.
[27] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha,
Z. Berkay Celik, and Ananthram Swami. Practical black-box
attacks against machine learning. In Proceedings of the 2017 ACM
on Asia Conference on Computer and Communications Security, ASIA
CCS ’17, pages 506–519, New York, NY, USA, 2017. ACM.
[28] Gagandeep Singh, Timon Gehr, Markus P ¨uschel, and Martin
Vechev. An abstract domain for certifying neural networks.
Proceedings of the ACM on Programming Languages, 3(POPL):1–30,
2019.
[29] Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J
Kochenderfer. Reluplex: An efﬁcient smt solver for verifying deep
In International Conference on Computer Aided
neural networks.
Veriﬁcation, pages 97–117. Springer, 2017.
[30] Matthias Hein and Maksym Andriushchenko. Formal guarantees
on the robustness of a classiﬁer against adversarial manipulation.
In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett, editors, Advances in Neural Infor-
mation Processing Systems 30, pages 2266–2276. Curran Associates,
Inc., 2017.
Scaling provable adversarial defenses.
[31] Eric Wong, Frank Schmidt, Jan Hendrik Metzen, and J. Zico
Kolter.
In S. Bengio,
H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett, editors, Advances in Neural Information Processing Sys-
tems 31, pages 8400–8409. Curran Associates, Inc., 2018.
[32] Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J
Kochenderfer. Towards proving the adversarial robustness of
deep neural networks. arXiv preprint arXiv:1709.02802, 2017.
[33] Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu.
In International
Safety veriﬁcation of deep neural networks.
Conference on Computer Aided Veriﬁcation, pages 3–29. Springer,
2017.
[34] Jinyuan Jia, Xiaoyu Cao, Binghui Wang, and Neil Zhenqiang
Gong. Certiﬁed robustness for top-k predictions against adver-
In International
sarial perturbations via randomized smoothing.
Conference on Learning Representations, 2020.
REFERENCES
[1] David Wagner Nicholas Carlini. Towards evaluating the robust-
ness of neural network. In Proceedings of the 38h IEEE Symposium
on Security and Privacy, 2017.
[2] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna,
Intriguing
In International Conference on
Dumitru Erhan, Ian Goodfellow, and Rob Fergus.
properties of neural networks.
Learning Representations, 2014.
[3] Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson,
Nedim ˇSrndi´c, Pavel Laskov, Giorgio Giacinto, and Fabio Roli.
Evasion attacks against machine learning at test time. In Hendrik
Blockeel, Kristian Kersting, Siegfried Nijssen, and Filip ˇZelezn´y,
editors, Machine Learning and Knowledge Discovery in Databases,
pages 387–402, Berlin, Heidelberg, 2013. Springer Berlin Heidel-
berg.
[4] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Ex-
plaining and Harnessing Adversarial Examples. arXiv e-prints,
page arXiv:1412.6572, Dec 2014.
[5] Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christo-
pher Wang, Dan Gutfreund, Josh Tenenbaum, and Boris Katz.
Objectnet: A large-scale bias-controlled dataset for pushing the
limits of object recognition models. In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Garnett, editors,
Advances in Neural Information Processing Systems 32, pages 9448–
9458. Curran Associates, Inc., 2019.
[6] Jonathan Baxter. Learning to learn. chapter Theoretical Models
of Learning to Learn, pages 71–94. Kluwer Academic Publishers,
Norwell, MA, USA, 1998.
[7] Chuong B. Do and Andrew Y. Ng. Transfer learning for text
In Proceedings of the 18th International Conference
classiﬁcation.
on Neural Information Processing Systems, NIPS’05, pages 299–306,
Cambridge, MA, USA, 2005. MIT Press.
[8] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros.
Image-to-image translation with conditional adversarial net-
works. arxiv, 2016.
[9] Dan Hendrycks and Thomas Dietterich. Benchmarking neural
network robustness to common corruptions and perturbations.
In International Conference on Learning Representations, 2019.
[10] Dong Su, Huan Zhang, Hongge Chen, Jinfeng Yi, Pin-Yu Chen,
and Yupeng Gao. Is robustness the cost of accuracy? - A compre-
hensive study on the robustness of 18 deep image classiﬁcation
models. In Computer Vision - ECCV 2018 - 15th European Conference,
Munich, Germany, Proceedings, pages 644–661, 2018.
[11] Jonas Rauber, Wieland Brendel, and Matthias Bethge. Foolbox: A
python toolbox to benchmark the robustness of machine learning
models. arXiv preprint arXiv:1707.04131, 2017.
[12] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev
Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya
Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
ImageNet Large Scale Visual Recognition Challenge. International
Journal of Computer Vision (IJCV), 115(3):211–252, 2015.
[13] Changing the contrast and brightness of an image. ”https://docs.
opencv.org/3.4/d3/dc1/tutorial basic linear transform.html”,
2018. [Online; accessed 13-Dec-2019].
[14] G. Bradski. The OpenCV Library. Dr. Dobb’s Journal of Software
Tools, 2000.
[15] Alain Fournier, Don Fussell, and Loren Carpenter. Computer
rendering of stochastic models. Commun. ACM, 25(6):371–384,
June 1982.
[16] Joseph Redmon and Ali Farhadi. Yolov3: An incremental im-
provement. arXiv, 2018.
[17] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy,
Scott Reed, Cheng-Yang Fu, and Alexander C. Berg. Ssd: Single
shot multibox detector. 2016. To appear.
[18] Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He, and
Piotr Doll´ar. Focal loss for dense object detection. 2017 IEEE
International Conference on Computer Vision (ICCV), pages 2999–
3007, 2017.
[19] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bour-
dev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan,
C. Lawrence Zitnick, and Piotr Doll´ar. Microsoft COCO: Common
Objects in Context. arXiv e-prints, page arXiv:1405.0312, May 2014.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:28:00 UTC from IEEE Xplore.  Restrictions apply. 
157