We use Gong and Wang‚Äôs WF data set [8] for our experiments.
The data set is relatively recent (2019) and since we are evaluating
known attacks and defenses, it is best for us to maintain comparabil-
ity with previous work. The data set was collected on Tor Browser
8.5a7 on Tor 0.4.0.1-alpha. It contains Alexa‚Äôs top 100 websites, each
visited 100 times, with 10,000 other pages as the non-monitored
class. Though it is smaller than some other data sets used to evaluate
WF attacks [15], it is sufficient for our purpose as we are performing
defense evaluation. This data set was collected with one machine
connected to a university network, relying on Tor‚Äôs random cir-
cuit selection for generalizability. Our evaluation of results used a
computing cluster (left unnamed for blind review).
3.2 Results
We chose five representative defenses to evaluate: Random, WTF-
PAD, Front, Decoy, and Tamaraw. ‚ÄúRandom‚Äù is a simple benchmark
defense that randomly adds dummy packets to the sequence in a
uniform fashion. The other defenses were chosen as representa-
tives of different defense paradigms. Sirinam et al. showed success
in attacking WTF-PAD with DF [15], and they showed DF to be
stronger than competitive deep learning attacks. Front, Decoy, and
Tamaraw are not considered ‚Äúbroken‚Äù by any attacks.
To test these defenses, we deploy the three WF attacks described
in Section 2.2 (kFP, CUMUL, DF) against them in the one-page
setting. We show the data overhead (extra data required to load
a page) of the defenses to compare their costs; the data overhead
is a burden to the network. Tamaraw is the only defense that also
delays packets, increasing page load times by 184%.1 We evaluate
1This number is somewhat higher than prior work, which suggests a page load time
increase of around 140% [19], because we are using a more pessimistic simulation that
Session 10D: Applied Privacy CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea2796Table 1: Results of three attacks on WF defenses. Multi-page refers to the original multi-page open world methodology, while
One-page refers to the one-page methodology we recommend for defense evaluation in this paper.
Defenses
Overhead
None
Random
WTF-PAD [10]
Front [8]
Decoy [12]
Tamaraw [2]
0%
22%
32%
67%
98%
107%
Multi-page
kFP
kFP
One-page
CUMUL
DF
TPR
91.3%
49.8%
60.3%
18.5%
30.8%
2.9%
FPR
3.4%
5.7%
14.8%
7.5%
9.2%
4.4%
TPR
99.1%
97.6%
97.6%
92.9%
91.2%
91.0%
FPR
0.9%
4.7%
4.4%
13.1%
10.0%
21.4%
TPR
97.7%
96.5%
5.5%1
80.5%
77.5%
91.1%
FPR
4.9%
7.5%
0.3%
22.6%
26.7%
21.6%
TPR
86.5%
83.9%
66.6%
76.2%
73.0%
59.8%
FPR
5.5%
8.8%
73.6%
27.3%
39.6%
38.9%
1 The result for CUMUL on WTF-PAD is not in error; it is due to a failure of the SVM to converge based on preset parameters. While other parameters may produce better results,
we kept this result as it showed a limitation of SVMs and did not particularly affect any of our other results, which would be derived with kFP.
both multi-page TPR/FPR and one-page TPR/FPR on Gong and
Wang‚Äôs data set.
In Table 1, we see significantly higher TPR values against all
defenses in the one-page setting; the gap is more pronounced when
a defense is applied than when there is no defense. kFP performs
notably better against Decoy than against Front and Tamaraw in
the multi-page setting, but their TPR in the one-page setting is
quite similar. Most surprisingly, the one-page setting exposes even
Tamaraw to a 91% TPR with kFP, where it only had a 2% TPR in
the multi-page setting. Tamaraw was presented as allowing no
more than a 10% true discovery rate for most websites [2], and
has been frequently shown to be the most robust defense against
WF [9, 10, 19].
Among the three attacks, kFP performed the best, and DF did
not perform especially well. This is likely due to the small training
sets (for DF, only 162 samples for training and 18 for validation) in
each classification problem. It may also be because certain hyper-
parameters in DF are sensitive to the classification problem ‚Äî in
fact, we had already lowered its training batch size parameter from
128 to 5, or the classifier could not be trained. While it is possible
that further optimizations to DF could improve results, our intent
is to evaluate defenses (not attacks), and the strong performance of
kFP is sufficient to do so.
Due to kFP‚Äôs superior performance compared to CUMUL and
DF for our scenario, from this point on, we evaluate defenses using
only kFP as a benchmark.
We want to determine the variance in performance against dif-
ferent pages in the one-page setting. We measure the TPR of kFP
against Tamaraw (the hardest defense) for each of 100 pages indi-
vidually and plot the results as a CDF in Figure 1. The standard
deviation of TPR across the 100 pages is fairly low at 6.9%, and
Figure 1 displays this phenomenon: no page had a TPR lower than
66%, and only 9 pages had a TPR lower than 85%. Broadly speaking,
no page in our data set is particularly safe in the one-page setting
even when protected by Tamaraw.
assumes inter-packet times cannot be shorter in the defended trace than in the base
trace.
Figure 1: CDF for TPR of kFP against Tamaraw for 100 dif-
ferent web pages.
3.3 TPR/FPR Tradeoff
Decreasing the FPR increases precision, and it has been argued that
high precision is important for website fingerprinting [17]. The FPR
values we found in Table 1 are relatively high for Front, Decoy and
Tamaraw. We investigate if the FPR can be reduced by trading off
a portion of their high TPR values. For a defense to claim success,
it needs to show that it succeeds against the entire range of the
attacker‚Äôs possible TPR/FPR values.
For the TPR/FPR values we obtained above, classifiers treated
the two classes (monitored and non-monitored) equally. We can
reduce FPR with the simple but effective technique of increasing
the minimum threshold confidence (or class probability) required
to classify a trace as monitored, depending on the classifier. We
apply this technique and show the results in Figure 2.
The results show us that it is indeed possible to significantly
reduce the FPR incurred against each defense. We can lower the
FPR close to 0% when there is no defense (as was shown in previous
work [15, 17]), and it is even possible to do so with defenses. The
highest TPR at which less than 0.1% FPR could be measured was
63% for WTF-PAD, 42% for Decoy, and 20% for Front.
At the highest confidence settings, Tamaraw holds out as the
strongest (though most costly) defense against kFP, at 17.5% TPR
and 1.5% FPR; poor results for the attacker. The attacker achieves
 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1Cumulative ProbabilityTPRSession 10D: Applied Privacy CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea2797Figure 2: TPR and FPR for kFP when traded off based on
classifier confidence. Note the FPR scales up to 0.25. When
classifier confidence is ignored, the result would be the fur-
thermost top-right point on each curve. The results for Ran-
dom and WTF-PAD overlap each other.
37% TPR in the one-page setting compared to 2.9% TPR in the multi-
page setting, if we hold the FPR at 4.4% in both cases. The TPR/FPR
tradeoff is not especially effective for Tamaraw, as shown by the
relatively straight line in Figure 2 (a straight line with slope 1 would
indicate the trivial tradeoff).
3.4 Decoys do not Force a 50-50 Guess
The use of the one-page setting has the added benefit of exposing
implicit assumptions that were not previously examined. Decoy is a
simple defense that loads a fake decoy page whenever a real page is
loaded. Since the WF attacker can at best identify both pages being
loaded together, and there is no way to know which of the two is
real (as both pages are in fact loaded), it is tempting to conclude that
the attacker is forced into a 50-50 guess and can therefore achieve
no more than 50% TPR under Decoy. However, this is contradicted
by our 91% TPR against Decoy.
The reason for this contradiction is that the set of decoy pages
cannot be assumed to come from the same distribution as real pages
being visited by the client. This is because different clients need
to use decoy pages from the same distribution (or their choice of
decoy pages alone could identify them), but they still visit real pages
differently. We replicate this effect in the experimental setting by
setting aside a portion of pages as a decoy page set, and loading both
monitored and non-monitored pages with randomly chosen decoy
pages from this set. As a result, packet sequences of monitored
pages still look more similar to each other than they do to packet
sequences of non-monitored pages.
To reduce the accuracy of the attacker to no more than 50%, the
client would have to always use the monitored page as the decoy
when visiting non-monitored pages, which is impossible as the
attacker decides which page to monitor.
From another perspective, we observe that if an attacker is
trained to monitor a specific sensitive page, and the attacker sees
that the client has visited two pages, among which one is the sen-
sitive page, the attacker‚Äôs ideal strategy is not to guess that the
sensitive page was visited 50% of the time ‚Äî he should classify it as
sensitive much more often than that. This strategy works because
the chance that a sensitive page would be used as a decoy page is
usually much smaller than the chance that a sensitive client would
Figure 3: Scatter plot of anonymity set sizes and the number
of positive elements in them, as well as their classification
using a simple majority strategy delineated by ùë¶ = ùë•/2.
visit a sensitive page; all clients do not visit the same page at the
same base rate. The analysis needs to consider the fact that sensitive
accesses are not uniformly distributed among all clients.
3.5 Tamaraw and Anonymity Sets
Tamaraw regularizes the packet sequence, fixing packet rates, so
that the resulting packet sequence is defined by only one feature ‚Äî
the sequence length. All packet sequences of the same length will be
identical to each other, so they can be considered to be in the same
anonymity set, and larger anonymity sets are created by padding
the sequence length to multiples of a fixed integer. In the multi-page
setting, an attacker cannot distinguish within the large and diverse
anonymity sets created by Tamaraw. But in the one-page setting,
Tamaraw failed, even though we tested a strengthened version of
Tamaraw that pads sequence lengths to multiples of 500 (instead of
100 in the original work [2]). We investigate why by exploring its
anonymity sets.
In Figure 3, we show a scatter plot of anonymity set sizes and
how many positive elements each contained. For classification, we
use a simple strategy of identifying anonymity sets and classifying
each anonymity set to the majority of elements it contained. This
strategy is Pareto-optimal for the attacker and would achieve a TPR
of 0.925 with an FPR of 0.176 (similar to kFP and CUMUL).
Here, we see that sensitive and non-sensitive pages belonged
to anonymity sets of very different sizes. The maximum size for a
set classified as non-sensitive was 19, while that of sensitive sets
was 53 (out of 200 elements in each classification problem). 79% of
sensitive pages belonged to anonymity sets of size 9 or above, while
only 26% of non-sensitive pages did so. Overall, 1998 out of 10000
sensitive pages belonged to anonymity sets that only contained
sensitive pages, while 6602 out of 10000 non-sensitive pages (a
majority) belonged to anonymity sets that only contained non-
sensitive pages. These are not truly anonymity sets, and they do
not confuse the attacker.
Non-sensitive sets were smaller because sequences of a sensitive
page were more similar to each other than non-sensitive pages
were to each other. Across our data set, the mean coefficient of
variation for the sequence lengths of a sensitive page was 0.196,
compared to 0.649 for non-sensitive pages. As a result of their
similarity, sequences of a sensitive page would be grouped together
 0 0.2 0.4 0.6 0.8 1 0 0.05 0.1 0.15 0.2 0.25TPRFPRRandomWTF-PADDecoyFrontTamaraw 0 10 20 30 40 50 0 5 10 15 20 25 30 35 40 45 50 55y=xy=x/2# of positive elements in setAnonymity set sizeSets classified as non-sensitiveSets classified as sensitiveSession 10D: Applied Privacy CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea2798in the same anonymity sets. To strengthen Tamaraw in the one-
page setting, either greater anonymity sets or more randomness is
required; we explore these options later when attempting to derive
a stronger defense in Section 5.1.
4 WEBSITE FINGERPRINTING SCENARIOS
The standard laboratory scenario for WF attacks is a basic super-
vised classification problem: the attacker is presented with labelled
testing elements, and his performance is evaluated by his overall
TPR and FPR. This standard scenario does not fully capture a real
attacker‚Äôs objective in WF, and it is not obvious how such an at-
tacker‚Äôs TPR/FPR would translate to a realistic threat. To provide a
more complete WF analysis, we define and investigate three WF
scenarios in this work. These scenarios allow us to determine if
the TPR/FPR values we found in the previous section would allow
an attack to succeed against the defenses. They will also allow us
to re-examine the implicit assumptions of the standard laboratory
scenario.
We will explore the following three scenarios:
‚Ä¢ The selection scenario (Section 4.1), where the attacker,
monitoring many clients, picks out which ones are visiting
a sensitive page.
‚Ä¢ The identification scenario (Section 4.2), where the at-
tacker, monitoring a single client, decides if she is visiting a
sensitive page or not.
‚Ä¢ The linking scenario (Section 4.3), where the attacker ob-
serves a visit to a sensitive page, and tries to determine which
of several clients did so.
4.1 Selection scenario
In the selection scenario, the eavesdropper monitors a large number