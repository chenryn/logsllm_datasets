TCOND LOCAL and TWHILE LOCAL are adapted from standard
rules to prevent implicit Ô¨Çows. Their guarded code ( and )
range over core commands, which do not contain occurrences of
localities. Otherwise, distributed execution may leak information
about the guard : for instance, in the program if HH then  
skip else   skip, the guarded commands are not local, hence it
would be hard to hide whether  or  executes next. Conversely,
TCOND and TWHILE allow guarded code with locality commands,
but only when the guard  is public.
TLOCALITY excludes trusted code in untrusted localities. This
is needed to prevent an adversary that controls the scheduling to
trigger an execution of   at  that is not enabled by the control
Ô¨Çow of the source program.
(SOURCE PROGRAM SAFETY). A label ‡°Å ‡¢†
DEFINITION 4
‡°Ä is robust against  when ‡°Å ‡¢†  implies ‡°Å ‡¢† .
A source program   is safe for ∆â and  when   is typable,
polynomial, and all its declassiÔ¨Åcation labels are robust against .
Typability implies that, in a source program, no untrusted host
is allowed to modify trusted variables, neither by modifying them
directly, nor by calling a trusted host that modiÔ¨Åes high integrity
variables. That is, for every locality    within  , we must
have  ‡£Æ  
 and ML ‡£Æ  
 .
In the absence of declassiÔ¨Åcation, typability guarantees CNI [see
e.g. Fournet and Rezk, 2008]. Otherwise, the robustness condition
ensures that, if the adversary can inÔ¨Çuence a declassiÔ¨Åcation, then
it can also directly access the declassiÔ¨Åed information [Zdancewic
and Myers, 2001]. Thus, depending on the lattice, a program with
some declassiÔ¨Åcation (hence not necessarily CNI) may still be com-
piled, with an implementation that provides the same security guar-
Correctness and Security for Compiled Code We are now ready
to specify the intended properties of our compiler, beginning with
functional correctness.
For a given command  , we write  for the memory that maps
every variable of  to . We let  range over distributions of mem-
ories, and write  for the memory distribution that gives proba-
bility 1 to . To any distribution  of memories whose domain
includes , we associate the distribution ‡¢Ø of memories with do-
‡¢Ø is ﬂ∞
restricted to . Intuitively, ‡¢Ø ignores the variables outside ,
such as auxiliary variables introduced by the compilation. We write
  for the conÔ¨Åguration distribution  deÔ¨Åned by   
 and     for  ‡¢ß  .
main  deÔ¨Åned by ‡¢Ø  ‡¢£
 ﬂ∞ where ﬂ∞
ﬂ∞ ‡¢Ø ﬂ∞
‡¢Ø
DEFINITION 5. A compiler  is correct when, for any typable
polynomial source program  , for     , and for any
polynomial core command , we have
ANJ :=      
for memory distributions  and ﬂ∞ such that ﬂ∞
   √õ‡¢© √ù
 √õ‡¢© √ù
ﬂ∞
 
ﬂ∞
 
‡¢Ø   .
In the deÔ¨Ånition,  initializes the source memory (much like
  or  ) whereas  initializes auxiliary variables for the
distributed code. Correctness states that, at least when the compiled
code is scheduled using the correct scheduler , our distributed
implementation probabilistically simulates the source code on any
input. (We believe that our correctness results would extend to any
fair scheduler instead of .)
To specify our security property, we need to deÔ¨Åne what is an
active adversary against source commands. If  is a source pro-
every locality command of the form    ﬂ∞ with the command con-
gram, let  be the command context obtained from  by replacing
text _  ﬂ∞ _. Intuitively, the additional holes in  are placeholders
for interleaving the code of an active adversary each time the im-
plementation of  may yield as the result of distribution. (This is
reminiscent of models of noninterference for concurrent programs,
where the adversary may run between any two program steps.)
DEFINITION 6. A compiler  is computationally sound when,
for any adversary , for any safe source program  , for   
 , for any polynomial commands , , , for    or ,
if  is CNI for , ,  then  _  is CNI for , , .
436This security deÔ¨Ånition states that our implementation is at least
as secure as the source program against active adversaries ( ).
Hence, if an adversary succeeds against our implementation, there
must be an adversary that also succeeds against the source program.
4. CODE SLICING
The Ô¨Årst stage of our compiler recursively slices a source com-
mand into a set of (core command) threads.
We distinguish between static threads (produced by slicing) and
dynamic threads (their runtime instances). Threads in loops may
be instantiated several times, so we need to distinguish between
these instances to secure the control Ô¨Çow and prevent replay at-
tacks. Hence, threads are parameterized by a tuple of loop indexes,
treated as formal parameters for static threads and as distinct actual
parameters for each of their runtime instances. We let J range over
thread names and  range over dynamic thread identiÔ¨Åers, that is,
pairs J  of a thread name and its tuple of loop indexes.
Threads outside of any loop are intended to run at most once in
any execution of the program. Anticipating on the next section, this
will be dynamically enforced by an anti-replay mechanism based
on loop indexes, so for uniformity our identiÔ¨Åers always include
a top-level index followed by an additional index for each nested
loop.
(In this paper, we formally consider a single execution of
the program, so the top-level index is always 1, but more gener-
ally this index would be used to separate multiple executions of the
program.) For instance, the identiÔ¨Åer of a thread called from within
two nested loops is of the form J   , indicating the th execu-
tion of the program, the th execution of the outer loop and the th
execution of the inner loop.
By convention, the initial thread is named IJ=HJ, and the Ô¨Ånal
thread is named A@. Thus, the static call graph is a Ô¨Ånite di-
rected graph between thread names, and the dynamic call graph is a
(possibly inÔ¨Ånite) directed acyclic graph between thread identiÔ¨Åers,
with unique root IJ=HJ  and a unique leaf A@ .
From Source Programs to Local Threads Figure 2 speciÔ¨Åes our
slicing function ‡¢§
 from source commands to thread deÔ¨Ånitions
and core commands. Thread deÔ¨Ånitions are of the form
thread J  √í   
where J is a fresh thread name,  is a tuple of loop indexes (with
an index for every enclosing loop in source code), √í is the local
host to which the thread belongs, and  is a Ô¨Åxed integrity level.
 to refer to such a thread
For brevity, we sometimes write  
deÔ¨Ånition, and write √íJ to access its host √í.
The grammar for thread bodies after slicing ( ) is given below;
 ranges over core commands (without localities).
  call  ‡¢Ø goto  ‡¢Ø if  then  else 
  ;  ‡¢Ø 
Syntactically, a thread is a program for which execution ends with
an auxiliary command call  or goto . Command call  indicates
that the next thread to execute belongs to another host, whereas
goto  indicates that the next thread is local. We say that a thread J
remotely (resp.
locally) calls J when the body of J includes a
command of the form call J  (resp. goto J ), and that J is
reachable from J when there is a possibly-empty series of calls (a
path) from one to the other. A path is local when it contains only
local calls.
Slicing helps ensure that every thread that is remotely called can
locally compute the expected identiÔ¨Åer of its caller. To this end,
whenever a thread can call more than one thread (a branch) or can
be called by more than one thread (a join), slicing ensures that the
‡¢§
 √í  


thread IJ=HJ  √í   ﬂ∞

  _


ﬂ∞
 √í   _
ﬂ∞
 √í  √íﬂ∞   _
thread J  √í   _
thread Jﬂ∞  √íﬂ∞   ﬂ∞
call Jﬂ∞ 
 √í   call A@ 
when ?=; otherwise:
 √íﬂ∞   call J 
 else 
ﬂ∞
 √í  if  then 
thread J  √í   _
thread J  √í   ﬂ∞
thread J  √í   ﬂ∞
if  then goto J  else goto J 
 √í  
 √í  
 _
ﬂ∞
 √í  while  do  _




 goto J 
 goto J 
thread J  √í   _
thread J   √í   if  then goto J   else goto J 
thread J   √í   ﬂ∞
goto J  

 ﬂ∞
ﬂ∞
 √í  
 ﬂ∞
  √í   goto J    
 √í  
 √í  
 ; 
   _
   _
Figure 2: Slicing algorithm
call is a goto, the callers and callees are all on the same host and,
moreover, the matching branches and joins are on the same host.
Every thread is associated with an integrity level (used in the
control Ô¨Çow protocol of Section 5). This level is computed by an
overloaded function , depending on the enclosing source locality
  ; it is deÔ¨Åned as the greatest lower bound of the integrity
levels of the written variables and declassiÔ¨Åcations of .
EXAMPLE 2. Example 1 yields 8 threads after slicing: 6 for a
and 1 for b and c, with the following call graph
The code of thread a1   is
if yLH < 3 then {goto(a8 i j)} else {goto(a7 i)}
and the code of thread b5   is
if (yLH mod 2) = 1
then {xHL := xHL + 9}
else {skip};
call(a4 i j)
Grouping Threads into Host Commands The commands call 
and goto  are introduced only to keep track of the control Ô¨Çow
during compilation; they are implemented as assignments to the
variable ANJ that holds the identiÔ¨Åer of the next thread to execute:
call 

 goto 

 ANJ := 
Accordingly, to every thread deÔ¨Ånition thread J  √í   , we
associate the command context
Case J_

 if BIJANJ  J then I@ ANJ else _
and, for every series of threads named J J, we deÔ¨Åne
Case J J_

 Case JCase J_
After slicing, we regroup each remotely-callable thread together
with all its locally-called threads and a local scheduler, as follows.
a4c3starta1a7a8enda2b5437DEFINITION 7
(SCHEDULING). The scheduling transforma-
tion  maps a series of thread deÔ¨Ånitions to a series of core com-
mands J, one for each deÔ¨Ånition thread J  √í    such that
J is remotely called (starting with J  IJ=HJ), deÔ¨Åned by

 if BIJANJ  J then 
J
I@ ANJ;
while BIJANJ ‡¢† J do Case Jskip
where J collect the names of all threads locally reachable from J.
Finally, the compiler  produces  by applying ‡¢§
 followed
by  and sets an empty initialization command (  skip); the
resulting code can be scheduled using ANJ := IJ=HJ    with

 A@ . As can be expected at this stage of the compila-

tion, we have correctness but not security, since the adversary may
schedule our thread commands at will.
THEOREM 1
(SLICING).  is correct.
5. ENFORCING THE CONTROL FLOW
Next, we introduce program counters to keep track of the source
control Ô¨Çow. For each integrity level  associated with (at least)
a thread, the variable  holds a thread identiÔ¨Åer (J ). At the
beginning of each thread execution, for each integrity level , 
identiÔ¨Åes the last executed thread with integrity . We let ∆â 
  , ∆âJ    J, and ∆â    .
The body of each thread J  produced by the slicing algorithm is
transformed by adding an assignment of the thread identiÔ¨Åer to the
corresponding program counter, by applying the command context

 J := J  ; _
We extend the deÔ¨Ånition of Case J_ accordingly.
52+J _
To protect each remotely callable thread J, we then guard its
code using the command context 62+ below. (We use the com-
mand check  then  as syntactic sugar for if  then  else skip,
to emphasize that a test failure should be interpreted as a global,
silent runtime failure; see also Fournet and Rezk 2008.)
62+IJ=HJ _
62+J _

 check IJ=HJ   then IJ=HJ := ; _

ﬂ∞ ﬂ∞ then
 check 
Jﬂ∞ ﬂ∞‡¢†8)J Jﬂ∞  J
check J   then J := ; _
where J is the integrity level associated to thread J and 8) is
the set of thread identiÔ¨Åers  such that there exists a (non-empty)
path from  to  for which all intermediate threads ﬂ∞ are such that:
 ‡£û (cid:100) locally reachable from  