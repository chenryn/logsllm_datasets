bingbot 表示必应蜘蛛、
YisouSpider|wc -1
bingbot|wc.-1
360Spiderlwc-l
Googlebot|wc
Baiduspider|wc -l
次，
惊人。
面和黑帽SEO关键字植入页面。两种类型的html页面总共有20万个左右，这个数量相当
处理静态请求的性能很差。
高性能Linux服务器运维实战：shell编程、监控告警、性能优化与实战案例
 S) print a, S[a]}
通过查看Nginx错误日志，发现有大量连接返回超时请求（502错误）。也就是说，Nginx
（3）查看Nginx错误日志
看来是个“流氓”蜘蛛，
其中，Baiduspider表示百度蜘蛛、（
并且几乎全部是通过搜索引擎引入的流量。笔者做了个简单的过滤统计，结果如下：
通过对Tomcat 访问日志的统计和分析，
（2）排查网站访问日志
通过find 查找、过滤，发现被植入的 html页面有两类，
（1）排查网站上被注入的html页面的数量
带着上面这个思路，继续进行排查，步骤如下。
3.第三次排查
于是，笔者根据上面的思路进行继续排查。
[root@tomcatserverl logs]#netstat-n| awk*/^tcp/(++S[$NF]}END (for(a
533810
[root@tomcatserver1 logs] # cat access_log.2019-10-16.txtlgrep Sogou|wc -1
3800100
621670
[root@tomcatserverl
63040
540340
[root@tomcatserverl logs]# cat access_log.2019-10-16.txt|grep
596650
[root@tomcatserver1 logs]# cat access_log.2019-10-16.txtlgrep
-1
YisouSpider表示宜搜蜘蛛、
网络上对YisouSpider也会是负面评价。
logs]# cat
、Googlebot表示谷歌蜘蛛、360Spider表示360蜘蛛、
发现每天对这些注入页面的访问量超过500万
access_1og.2019-10-16.txtlgrep
、Sogou表示搜狗蜘蛛。YisouSpider抓
：分别是百度虚假中奖广告页
---
## Page 414
现了网站漏洞的原因是网站后台使用了一个轻量级的远程调用协议json-rpc来与服务器进
抓取，最终导致脆弱的Tomcat因不堪重负而失去响应，这是此次故障发生的根本原因。
10.1.3
象表明，Tomcat无法处理这么大的连接请求，导致响应缓慢，最终服务出现无响应。
TIME_WAIT，应该是Tomcat无法响应请求，然后超时主动关闭了连接而导致的。种种迹
关闭这个套接字。
动关闭，正在等待远程套接字的关闭传送；CLOSE_WAIT 表示远程被动关闭，正在等待
网站有漏洞被注入WebShell，继而被上传了大量广告、推广网页，导致搜索引擎疯狂
从输出可知，服务器上保持了大量TIME_WAIT状态和ESTABLISHED 状态。大量的
要解决这个问题，首先要做的是找到网站漏洞。研发人员介入后，通过代码排查，发
1.修复网站程序漏洞
通过这三个方面的排查，基本验证了自己的思路，那么问题也随即找到了。
这里其实只需要关注三种状态：ESTABLISHED表示正在通信；TIME_WAIT表示主
如何处理被植人的WebShell木马
LASTACK17
CLOSING14
SYN RECV 115
ESTABLISHED 13036
FIN_WAIT2 113
FIN WAIT1 197
CLOSE WAIT 12
TIME_WAIT 125300
Nginx通过判断UserAgent过滤YisouSpider-Slyar Home
YisouSpider是不是很无耻？-V2EX
关于YisouSpider-一只正在努力进化的程序猿-CSDN博客
垃圾YisouSpider蜘蛛，干万别被它肛上，瞬间搞死服务器！百度站长
https://yq.alyun.om/articles..百度快照
2017年12月5日-Y
bbs.zhanzhang.baidu.co....
14年2月19日-一看不得了.基本上都是被
不知道是
图10-10百度搜索YisouSpider蜘蛛截图
发帖时间：2015
间最开始访问
百度快照
完全不遵寸
一万多次，什么概念，没毛用的蜘蛛，都屏蔽了！各位小心一网上有
百度快照
-Had.
Spider的东西给刷屏了.
服务器安全运维案例第10章
昔施方法篇禁
百度一下
403
---
## Page 415
首先检查该站点根目录下是否存在robots.txt，如果存在，搜索蜘蛛就会按照该文件中的内
程序在服务器上什么文件是可以被查看和抓取的。当一个搜索蜘蛛访问一个站点时，它会
引擎只收录指定的内容。
robots.txt文件，在这个文件中可以声明该网站中不想被robots 访问的部分，或者指定搜索
封掉。
络蜘蛛，如果还需要蜘蛛，可保留几个比较正规的，如谷歌蜘蛛和百度蜘蛛，其余的一律
Sogou web spider|MSNBot|ia_archiver|Tomato Bot")
cher-Google| Yahoo! Slurp| Yahoo! Slurp China| YoudaoBot|Sosospider|Sogou spider]
Googlebot-Mobile|Googlebot-Image|Mediapartners-Google|Adsbot-Google|Feedfet
通过如下配置实现：
站权重、流量有益，
ameter("t").getBytes());
检查过程中又发现了一个jsp后门，基本特征码如下（代码仅供学习）：
的网页都是以.htm结尾)，同时删除了1.jsp 文件，并继续查找和检查其他可疑的jsp文件，
在服务器上进行了网页扫描，主要扫描.html为扩展名的文件，然后全部删除（因为客户
后在后台上传了一个WebShell，进而控制了操作系统。
行数据交换通信，但是此接口缺乏校验机制，导致黑客获取了后台登录的账号和密码，然
404
高性能Linux服务器运维实战：shell编程、监控告警、性能优化与实战案例
robots.txt是搜索引擎中访问网站的时候要查看的第一个文件。robots.txt文件告诉蜘蛛
研发人员在第一时间修复了这个漏洞，然后就是运维工程师的干活时间了。笔者首先
上面这个办法虽然有点简单粗暴，但却是最有效的，其实还可以在网站根目录下增加
这样，当蜘蛛过来爬取网站的时候，直接给其返回一个403错误。这里禁止了很多网
网络上的蜘蛛、
2.禁封网络蜘蛛
果断将其删除，
if ($http_user_agent * "qihoobot |YisouSpider|Baiduspider|Googlebot|
#添加如下内容即可防止爬虫
server_name 127.0.0.1;
return 403;
listen 80;
server
而那些违规的蜘蛛必须要禁止。要实现禁封网络蜘蛛，可在Nginx
、爬虫很多，有些是正规的，有些是违规的，适当的网络蜘蛛抓取对网
以免留后患。
---
## Page 416
以采用的方式是将Tomcat 生成的 htm文件放到一个共享磁盘分区，然后在Nginx服务器
态资源交给Tomcat 处理。通过这种动、静分类方式，可以大大提高网站的抗压性能。
络蜘蛛会经常违规操作，对待这种蜘蛛，建议使用上面Nginx下配置的规则，直接将其禁止。
索引擎不遵守约定的Robots 协议，那么通过在网站下增加robots.txt也是不起作用的。
护的页面。
容来确定访问的范围：如果该文件不存在，搜索蜘蛛将能够访问网站上所有没有被口令保
Robots 协议是国际互联网界通用的道德规范（请注意，是道德标准），因此，如果搜
因为 Tomcat 处理静态资源能力很低，因此，可以将静态资源交给Nginx来处理，
3.调整网站的Web架构
（5）只允许某个搜索引擎的访问（下例中的Googlebot）
（4）禁止某个搜索引擎的访问（下例中的YisouSpider）
（3）禁止所有搜索引擎访问网站的几个部分（下例中的a、b、c目录）
（2）
（1）允许所有的robot访问
下面看几个robots.txt配置的例子。
目前看来，绝大多数的搜索引擎机器人都是遵守robots.txt规则的。但是一些不知名的网
目前的网络蜘蛛大致分为4种。
V
V
真名真姓，
伪装，不遵循robots.txt协议。
匿名，不遵循 robots.txt 协议。
真名真姓，不遵循robots.txt协议。
User-agent: YisouSpider
Disallow:/
禁止所有搜索引擎访问网站的任何部分
Disallow:/
User-agent:*
Disallow:
User-agent:Googlebot
Disallow:/
Disallow:
Disallow:
Disallow:/a/
User-agent:
User-agent:
Disallow:
User-agent:*
，遵循robots.txt协议。
/c/
/b/
服务器安全运维案例第10章
405
司
动
---
## Page 417
406
命令执行环境，以达到控制网站或者Web 服务器的目的。这样就可以进行上传下载文件、
10.1.4
上通过 NFS 挂载这个磁盘分区，这样Nginx就可以直接访问这些静态文件了。
高性能Linux服务器运维实战：shell编程、监控告警、性能优化与实战案例
查看数据库、执行任意程序命令等操作了。
器
境
Web目录下正常的网页文件混在一起，然后就可以使用浏览器来访问这些后门，得到
通过上面三个步骤的操作，网站在30min内负载就开始下降，很快就恢复了正常。
》操作系统的基础安全设置必须要做好，包括端口的安全策略、网络安全策略、底
3.如何防止网站被植入WebShell
》扫描获取Web程序漏洞，进而获取管理员的后台密码，登录到后台系统，利用后
2.WebShell是如何入侵系统的
也可以称为网页后门。黑客在入侵网站后，通常会将WebShell 后门文件与网站服务
防止网站被植入WebShell的方法有如下几个。
WebShell入侵到Web系统的方式有很多种，常见的有如下几种。
WebShell通常是以PHP、JSP、ASA或者CGI等网页文件形式存在的一种命令执行环
1.什么是WebShell
V
漏洞，以及进行代码的安全检测与木马后门的清除工作。
如果自已对程序代码不是太了解，建议定期找网站安全公司去扫描并修复网站的
或文件名为 adminjsp、loginjsp 的路径去访问。或者干脆禁止后台在互联网的访问。
网站后台密码不要设置得太简单，后台管理的路径一定不能用默认的admin或manager,
理，对敏感目录进行权限设置，限制上传目录的脚本执行权限，不允许执行脚本。
定期扫描、修补程序漏洞，程序的上传功能要做限制和优化。同时，加强权限管
的后台密码，登录到系统后台后利用后台的管理工具向服务器写入WebShell木马。
数据库暴露在外网，被黑客扫描到，然后黑客通过SQL注入方式获取网站管理员
理员控制权限。
执行。这样黑客就可以上传 WebShell 到网站的任意目录中，从而拿到网站的管
以利用上传功能上传一个脚本文件，然后再通过URL访问这个脚本，脚本就被
限或者文件夹目录权限控制不严，就可能被利用进行WebShell攻击，攻击者可
到，一般都是上传到服务器的 image、upload等目录下。如果Web 对网站存取权
完成后通常会向客户端返回上传文件的完整URL信息，有时候不说明也可以猜
目录往往具有可执行的权限。在Web 中有上传图像、上传资料文件的地方，上传
利用系统前台的上传业务，扫描获取上传漏洞，然后上传WebShell脚本，上传的
扫描获取操作系统漏洞，然后在系统内植入WebShell文件。
程序以类似JSP、PHP格式的文件上传。本案例就是这种情况。
台的管理工具向配置文件写入WebShell木马，或者私自添加上传类型，允许脚本
WebShell网页木马的原理与防范
---
## Page 418
态，
后就突然不正常了呢？第一感觉是不是有什么计划任务在捣乱？
系统刚开始的时候都正常运行，但到了18:00后，突然就无法使用了，前台提交秒杀请求
开发和运维，项目交付后，就交给客户去运维了，所以很多客户服务器信息笔者还是有的。
象，同时通知开发人员检查代码是否正常。客户的服务器运行在阿里云上，项目初期由笔者
上秒杀系统不能用了，由于客户正在进行线上秒杀促销活动，所以对秒杀功能故障非常着急
10.2.1
这类问题的思路和流程，是本节要传达给大家的核心知识。
10.2.2
+Redis。客户说他们电商平台做了一个大型的秒杀活动，从14:00开始到凌晨结束。秒杀
10.2
先介绍一下客户的应用系统环境：操作系统是CentOS6.9，应用系统是Java+MySQL
再做进一步的判断。执行top命令后，得到如图10-11所示结果。
了解完客户的系统故障后，笔者感觉很奇怪，为什么秒杀系统一开始是正常，18:00
一直无响应，最终超时退出。
这是笔者一个外包客户的网站故障案例。最初是接到商务人员的反馈，说客户的一个线
要了解问题的本质，必须“深入虎穴”才行。先登录服务器，看看整个系统的运行状
安全问题不容忽略，这个案例是个典型的网站漏洞导致WebShell 注入的例子，处理
作为运维人员，解决客户问题是第一要务，笔者赶紧与客户取得了联系，询问具体的现
1从客户秒杀系统突然无法使用说起
服务器被植人minerd程序的过程与分析
层系统的安全加固等。
云主机被植入挖矿程序案例及如何做Redis安全防范
8
79488ktotal9752 us，
8up 828 days,
图10-11top命令输出系统状态截图
1:03,
###
122m
36736kf4kbf
average:10.16,10.40,12.10
885
:02
0:3
0%h
服务器安全运维案例第10章
0.0%st
407
---
## Page 419
408
$PID为查询到的进程ID:
进程的程序路径，可以执行ls-al/proc/SPID/exe来获取PID 对应的可执行文件路径，其中
问题是什么呢？先一下思路。
耗CPU资源。
有名的就是minerd。minerd是一个比特币挖矿程序，能够运行在服务器上挖矿，并大量消
远落后于显卡和矿机，但并不是说不能用CPU来挖矿。用CPU挖矿的软件很多，其中最
特定算法执行大量的计算，会大量占据CPU，导致系统卡顿，严重的会导致系统直接瘫痪。
会获取一定量的比特币作为赏金，但是付出的代价是需要大量的计算资源。挖矿软件依据
络监听交易广播，执行任务以处理并确认这些交易。比特币矿工完成这些工作后，就有机
为“挖矿”。
世界各国进行操作，没有人可以对网络具有控制权。这个过程因为同淘金类似而被称
程。可以理解为是比特币的数据中心，区别在于其完全是去中心化的设计，“矿工”在
挖矿是消耗计算资源来处理交易，确保网络安全以及保持网络中每个人的信息同步的过
源，这是一个什么进程呢？笔者带着疑问搜索了一下，发现这是一个挖矿程序。
这个进程是刚刚启动的。
CPU资源，并且这个minerd进程还是通过root用户启动的，已经启动了35min45s。看来
着
系统的平均负载较高（都在10以上），有个PID为16717的minerd的进程，消耗了大量
高性能Linux服务器运维实战：shell编程、监控告警、性能优化与实战案例
题外话说完了，继续回到本案例来。既然知道了这是个挖矿程序，那么下面要解决的
任何人都可以在专门的硬件上运行软件程序而成为比特币矿工。挖矿软件通过P2P网
实际上，最初比特币的挖矿是用计算机的CPU来进行的，虽然现在CPU的计算力远
又询问了客户这个秒杀系统出问题多久了，客户回复说大概35min。
这个服务器是16核32GB内存，硬件资源配置还是很高的，但是，从图中可以看出