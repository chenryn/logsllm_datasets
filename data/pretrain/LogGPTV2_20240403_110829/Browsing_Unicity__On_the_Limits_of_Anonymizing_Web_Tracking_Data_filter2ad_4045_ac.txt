### Algorithm 3: Calculating Identifiability from a Trace Set

Identifiability cannot be calculated using hashed values in the same way as unicity, because we need to determine whether a smaller click trace is contained within a larger one, rather than whether they are equal. Additionally, the adversary set \( I \) is too large for exact calculation. For example, calculating identifiability for a shoulder surfer making three observations on a million click traces of length 10 would require approximately \( 14.4 \times 10^{15} \) operations. Our database contains far more than a million click traces, with some being several thousand clicks long, making exact computation infeasible.

Instead, we follow the approach of De Montjoye [15] and approximate the true value using established sampling techniques. To approximate identifiability by sampling, we need to sample from the set of all possible results. Since this set is too large to compute, we sample results by selecting the click trace of a random click from all click traces (weighted by its length), and then selecting from all possible attack configurations given the adversary’s capabilities.

This sampling method corresponds to a series of Bernoulli trials, allowing us to use established formulas for sample size \( n_0 \) given a confidence interval and error estimation:

\[
n_0 = \frac{Z^2 p(1 - p)}{e^2}
\]

This expression is maximized for \( p = 0.5 \), which is our best estimate since \( p \) is unknown. For a confidence level of 99% (\( Z = 2.576 \)) and a maximum error of 1% (\( e = 0.01 \)), meaning that any subsequent experiment has a 99% chance of deviating from the result by at most 1%, we obtain a necessary sample size of \( n_0 = 16,590 \). The expected granularity of our results is well below 1%, so this sampling size is suitable for all our identifiability experiments.

### D. Anonymization

To understand tracking databases and methods to identify users who have generated the contained traces, we turn to commonly suggested anonymization strategies. Considering the composition of entries in tracking databases, as described in Section II-A, we group the parameters into:
1. Information about the user (IP, client ID, user agent, location).
2. Information about the visited page (URL, category).
3. Information about the access (method, referrer, timestamp).

Privacy regulations require either informed consent or the absence of identifying information. Trackers typically store only truncated IP addresses, claiming that the processed data is anonymous. However, storing a page call with its exact time in milliseconds can create a unique identifier with high probability, as exactly simultaneous calls to the same page are unlikely on that time scale.

We explore anonymization of the described groups of parameters, following the same vein of generalization. The most intuitive measure is to coarsen the timestamps by removing the least significant time information, similar to truncating bits of the IP address. Specifically, we coarsen a timestamp by subtracting the timestamp modulo the coarseness parameter. For example, a timestamp of 152.9867 with a coarseness parameter of 60 seconds is coarsened to 120. We use coarsening parameters up to the order of 100,000 seconds, which is slightly over a day and sufficiently below the scope of the analyzed data (ref. Section IV).

The visited page and all information about the user can also be successively discarded to reduce unicity in the click traces. Some properties are suited for gradual anonymization: the page can be generalized from the exact URL to the code of the page, its category, or simply the FQDN of the visited website. The same holds for information about the user, where we can remove information about the user-agent and geolocation.

Finally, some properties of the traces directly relate to unicity: traces collected across several websites contain more information than click traces restricted to single websites. The length of the clicks that are linkable to a single session also correlates with identifiability, as long traces contain more information and are much more likely to be unique than short traces. Restricting the maximum length of click traces or limiting them to single websites are other possible strategies when aiming to anonymize datasets.

### IV. Data

For this study, we collaborated with an audience measurement provider representing a majority of German websites, spanning over 2,500 websites and apps, with an average volume of 2 to 3 billion page impressions per day. This data is representative of the German market, but we cannot say with certainty whether our experiments would yield the same results using data from another provider, such as Google Analytics. Nationality likely does not affect our analysis of metadata, but this specific subject has not yet been explored in literature.

The ABC stores this data for calculating quantitative session metrics like visits and returning clients. It stores a subset of common tracking parameters, as described in Table I. Each entry contains a client ID tied to a session cookie, which we use to assemble the database of click traces but discard before assessing unicity and identifiability. A geolocation is stored on the granularity of federal states, determined by looking up the IP address of the browser in a public database, and the IP address is subsequently discarded. The ABC also stores a page code assigned by the publisher, usually encoding an article or specific site (local path of the URL), as well as the user-agent for which its layout has been optimized. Additional information includes the site and its category, corresponding to the public host part of the URL (FQDN) and classes of content defined by the ABC (sports, politics, etc.). Finally, each entry contains the time of the user’s click, stored as a Unix timestamp with millisecond precision.

Some page-related information has global characteristics, while some are local. Categories are global to the ABC, so different sites will have pages with identical categories. The codes, on the other hand, are chosen by publishers for their own site and may overlap with codes of other sites, requiring the site information to be used as well.

### V. Empirical Results

Our main interest is to assess the extent to which pseudonyms emerge in tracking databases. We divide this question into two studies over increasingly generalized data, investigating first the unicity of the data and then the identifiability.

### A. Experimental Setup

The experiments were computed on a small standard Hadoop platform with about 2,000 cores, using classical map-reduce jobs. A proven mapper was used for all applications, and reducers were developed according to the requirements of the respective experiment. We search for unique click traces, which requires comparing all pairs of traces. Even with cascaded map-reduce jobs, the last reducer is left with the ultimate task. We facilitate computation using the algorithms and sampling described in Sections III-C and IV.

### B. Applied Anonymization

Following the common argument of tracking companies, we anonymize the data by generalization (coarsening, truncating, omission) and evaluate its effect on the persistence of pseudonyms. We generalize with respect to four properties: temporal resolution, client geolocation, information about the visited page, and the length of each click trace. Trace length can be adjusted by discarding traces below a minimum length or cutting traces above a maximum length into smaller ones. Page information at the highest level of detail consists of the site domain and a code, which we generalize to the tuple of site and category, and finally omit all information about the page.

Within experiments and results, we denote which information is used by the tuple \( \text{timestamp/location/[code/category]/site/trace length} \). Omissions are denoted by a dash “-”. For example, “s/-/-/-/∞” represents a dataset with timestamps (coarsened to seconds) and trace length, where the click traces may still be identifiable through timing and length, but leaking such a trace could not disclose any information about the visited sites.

### C. Unicity and Pseudonymity

Our first research question investigates the extent to which pseudonyms emerge in tracking data and how they are affected by successive generalization of the data.

#### 1) General Unicity

We determine the unicity according to Algorithm 2 on the sample described in Section IV. Initially, it includes highly detailed attributes per click (location, code, and site, timestamps in ms), which we gradually coarsen.