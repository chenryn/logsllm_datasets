Algorithm 3: Calculating identiﬁability from a traceset
Identiﬁability cannot be calculated using hashed values the
way we calculate unicity, as we have to determine whether a
smaller click trace is contained within a larger one rather than
whether they are equal. In addition, the adversary set I is far
too large to allow exact calculation. For example, calculating
identiﬁability for a shoulder surfer making 3 observations
on a million click traces of length 10 requires a number of
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 04:12:30 UTC from IEEE Xplore.  Restrictions apply. 
781
operations on the order of 14.4 · 1015. Our database contains
far more than a million click traces, some several thousand
clicks long. It
identiﬁability cannot be
computed. Instead, we follow the approach of De Montjoye
[15] and approximate the true value using established sampling
techniques.
is clear that exact
In order to approximate identiﬁability by sampling, we need
to sample from the set of all possible results. The set of all
possible results, again, is far too big to be computed. Instead,
for algorithm 3 we sample results by selecting the click trace of
a random click from all click traces (thus selecting a click trace
weighted by its length), and then selecting from all possible
attack conﬁgurations, given the adversary’s capabilities.
Sampling in this way corresponds to a series of Bernoulli
trials, which allows us to use established formula for sample
size n0 given a conﬁdence interval and error estimation.
Z 2p(1 − p)
e2
n0 =
This expression is maximized for p = 0.5, which is our
best estimation since p is unknown. For a conﬁdence of 99%
(Z = 2.576) and a maximum error of 1% (e = 0.01), meaning
that any subsequent experiment has a 99% chance of deviating
from the result by at most 1%, we obtain a necessary sample
size of n0 = 16, 590. The expected necessary granularity of
our results is well below 1%, so we can use this sampling size
for all our identiﬁability experiments.
D. Anonymization
Understanding tracking databases and ways to identify users
that have generated the contained traces, we turn to strategies
that are commonly suggested for their anonymization.
entries
composition of
Considering the
in tracking
databases, as described in Section II-A, we group the param-
eters into (1) information about the user (IP, client ID, user
agent, location), (2) information about the visited page (URL,
category), and (3) information about
the access (method,
referrer, timestamp).
Since privacy regulations require either informed consent or
the absence of identifying information, trackers have conﬁned
themselves to store only truncated IP addresses. Generalizing
the direct identiﬁer, they claim that the processed data thus
was anonymous.
It
is easy to see that
the data above still contains
pseudonyms. Storing a page call with its exact time in mil-
liseconds creates a unique identiﬁer with high probability, as
exactly simultaneous calls to the same page are unlikely on
that time scale.
We hence explore anonymization of the described groups
of parameters, following the same vein of generalization. The
most intuitive measure is to coarsen the timestamps. We do
so by removing the least signiﬁcant time information, similar
to truncating bits of the IP address. Speciﬁcally, we coarsen a
timestamp by subtracting the timestamp modulo the coarseness
parameter. For example a timestamp of 152.9867 with a
coarseness parameter of 60 seconds is coarsened to 120. We
use coarsening parameters up to the order of 100, 000 seconds,
which is slightly over a day and sufﬁciently below the scope
of the analyzed data (ref. section IV).
The visited page and all information about the user can also
successively be discarded, to reduce unicity in the click traces.
Some properties are suited for gradual anonymization: the
page can be generalized from the exact URL to the code of the
page, its category, or simply the FQDN of the visited website.
The same holds for information about the user, where we can
remove information about the user-agent and geolocation.
Finally, some properties of the traces directly relate to
unicity: traces collected across several websites contain more
information than click traces that are restricted to single
websites only. The length of the clicks that are linkable to
a single session also correlates to identiﬁability, as long traces
contain more information and are much more likely to be
unique than short traces. Restricting the maximum length of
click traces, or limiting them to single websites, are other
possible strategies when aiming to anonymize datasets.
IV. DATA
For this study we joined forces with the audience measure-
ment provider of an ABC representing a majority of German
websites. It spans over 2500 websites and apps in total, with
an average volume of 2 to 3 billion page impressions per day.
This data is thus representative for the German market, but
we cannot say with certainty whether our experiments would
yield the same results using data of another provider, such as
Google Analytics. Nationality likely does not have an effect
as we are analyzing meta data rather than content, but to our
best knowledge this speciﬁc subject has not yet been explored
in literature.
The ABC stores this data for the purpose of calculating
quantitative session metrics, like visits and returning clients.
It stores a subset of common tracking parameters, as described
in Table I.
First, each entry contains a client ID, tied to a session
cookie. In our experiments we use this ID only to assemble the
database of click traces, but discard it before assessing unicity
and identiﬁability. A geolocation is stored on the granularity
of federal states, determined by looking up the IP address
of the browser in a public database, and the IP address is
subsequently discarded. The ABC also stores a page code of
the visit. This code is assigned by the publisher, and usually
encodes an article, or speciﬁc site (local path of the URL), as
well as the user-agent for which its layout has been optimized.
Additional information about the visited page are the site and
its category. The former corresponds to the public host part
of the URL, or FQDN, and the latter to classes of content,
as they are deﬁned by the ABC (sports, politics, etc). Finally,
each entry contains the time of the user’s click, stored as a
Unix timestamp with millisecond precision.
Some of the page-related information has global, and some
local characteristics. Categories on the one hand are global
to the ABC, so different sites will have pages with identical
categories. The codes, on the other hand, are chosen by
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 04:12:30 UTC from IEEE Xplore.  Restrictions apply. 
782
from roughly half of available sites, to reduce the sample to
a size that allowed computation of unicity given the resources
available. To verify that our results are representative, we ran
all experiments on samples of increasing size, and empiri-
cally observed that the experiment outcome converged with
increasing sample size well before reaching the size of our
ﬁnal sample.
To validate our sample, we performed some basic sanity
tests: Fig. 1 compares the frequencies of page impressions
and visits of the original data vs. the sample. We observe that
the frequency distributions in the full census and our sample
follow equal characteristics and we veriﬁed that our click trace
lengths comply to literature [16].
The ﬁnal sample contains data as described in Table II.
PIs
Visits
147.9M 22.1M
Clients
4.1M
Locations
3053
Sites
1281
Codes
62.5K
Categories
725
TABLE II
COMPOSITION OF THE TESTED SAMPLE
While analyzing large data vaults is always a challenge,
the resource constraints we experienced would not necessarily
apply to an adversary. They may only be interested in a single
click trace while we analyze large samples in a multitude
of different scenarios. In addition, they may have access to
resources far exceeding ours.
Note that at no point in our experiments was plain text
data analyzed. All the adversary models are only applied on a
theoretical level, meaning no actual user was de-anonymized.
Or, in other words, no privacy was harmed in the making
of this study. For regulatory reasons we cannot share the ﬁnal
dataset, but we will provide access to run reproduction studies
upon request.
V. EMPIRICAL RESULTS
Our main interest in this paper is to assess to which extent
pseudonyms emerge in tracking databases. We divide this
general question into two studies over increasingly generalized
data, investigating ﬁrst the unicity of the data and afterwards
the identiﬁability. In the following section we describe how
we conducted our experiments and report the results.
A. Experimental Setup
The experiments of this paper were computed on a small
standard hadoop platform with about 2, 000 cores. All ex-
periments were classical map-reduce jobs. A proven mapper
was used for all applications. The reducers were developed
according to the requirements of the respective experiment.
Within the experiments, we are searching for unique click
traces. This terminally requires all pairs of traces to be
compared to each other. Even using cascaded map-reduce jobs
to reduce and pre-process the amount of data, the last reducer
is left with this ultimate task. We facilitate computation of
our results despite this restriction using the algorithms and
sampling described in section III-C & IV.
Fig. 1. Distribution of visits: sample vs entire dataset (PIs in inset). Websites
are ranked by the number of visits in the original dataset. The sample
distribution largely follows the original distribution, meaning websites are
represented proportionally in the sample.
the respective publishers for their own site. They only have
signiﬁcance for their respective site and may even overlap with
codes of other sides. Therefore the code information can only
be used if the site information is used as well.
Note that the only explicit information beyond the ID that
is stored about the clients is their geolocation. The choice of
device-type and browser may be implicitly represented in the
page code.
Field
Timestamp
Client ID
Site
Code
Category
Geolocation
Content
Unix timestamp in microseconds
Unique per user / browser, from cookie
ID of visited website/FQDN
ID of displayed page, assigned by publisher
Category of page, according to ABC
DB lookup of client IP
INFORMATION STORED PER CLIENT ACTION
TABLE I
The entire database of the measurement provider is far too
large to analyze unicity and identiﬁability. Behavior on the
Web being driven by freshly published content, we analyzed
an interval of one week in March 2019. We limited our dataset
to desktop clients that accept cookies and do not exhibit any
characteristic behavior (for example search engines, bots, etc).
This produced a highly reliable and clean dataset without
requiring additional preprocessing. As a result, we do not take
into account mobile browsing and we acknowledge that as a
limitation of this study. The full week’s worth of data contains
66.1 million clients, 2.34 billion page impressions, and 351.3
million visits.
From this data we sampled a 16th of the clients at random
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 04:12:30 UTC from IEEE Xplore.  Restrictions apply. 
783
Fig. 2. Click trace unicity over coarsened time.
Fig. 3.
x/loc/code/site/·.
Click trace unicity,
trace length limited to l. Conﬁguration:
B. Applied Anonymization
Following the common argument of tracking companies, we
anonymize the data by generalization (coarsening, truncating,
omission) as described in Section III-D, and evaluate its effect
on the perseverance of pseudonyms. We slightly need to adapt
this step, given the dataset we have at hand.
We generalize with respect to four properties: the temporal
resolution, client geolocation, information about the visited
page, and ﬁnally the length of each click trace. Trace length
can be adjusted by discarding traces below a minimum length
or cutting traces above a maximum length into several, smaller
ones. Page information at the highest level of detail consists
of the site domain and a code. The code contains informa-
tion about the exact page, which, for identiﬁcation purposes,
implies information about its category. We subsequently gen-
eralize to the tuple of site and category, thus generalizing the
speciﬁc page to the category it belongs to. Then, we also omit
the category and only consider the site, and ﬁnally we omit
all information about the page.
Within experiments and results we denote which informa-
tion is used by the tuple  / location /
[code/category] / site /  (“ms/loc/code/site/∞”
for instance denotes information at the original granularity
with click trace length corresponding to the sessions as deﬁned
above). Omissions are denoted by a dash “-”: so “s/-/-/-/∞”
represents a dataset with timestamps (coarsened to seconds)
and trace length. The click traces may still be identiﬁable
through timing and their length, but leaking such a trace could
not disclose any information about the visited sites. At even
lower granularity, “d/-/-/-/1” denotes a database containing
only the information on what day each click occurred, without
any information pertaining to client, site and sequence. If one
of the elements is displayed on the x-axis of the plot, the ﬁeld
is correspondingly replaced with an ”x”. If the x-axis displays
timestamp coarseness, we added dotted vertical indicator lines
for coarseness values of a minute, hour and day (60, 3600 and
86400, respectively).
C. Unicity and Pseudonymity
Our ﬁrst research question investigates to which extent
pseudonyms emerge in tracking data, and how they are af-
fected by successive generalization of the data.
1) General Unicity: We determine the unicity according
to Algorithm 2 on the sample described in Section IV. It
initially includes highly detailed attributes per click (location,
code, and site, timestamps in ms), which we gradually coarsen