can be observed on a satellite network even without any self-induced queuing.
There are no visual time of day patterns to the round-trip times.
In the same time period, only 604 packets are lost, or about 0.05%. Most of
these (77%) are single-packet losses, with 44 multi-packet loss events, the largest
11 packets (about 2.2 s). There is no apparent correlation between these losses
and the round-trip times (i.e., the losses do not seem to occur during the highest
round-trip times observed). Note, these loss rates are about 15x lower than the
reported WINDS satellite loss of 0.7% [17].
4.2 Representative Behavior
We begin by examining the TCP congestion control performance over time
for a single download representative of typical behavior for each algorithm for
our satellite connection. Figure 2 depicts the throughput, round-trip time and
retransmission rate where each value is computed per second from Wireshark
traces on the server.
4 https://software.es.net/iperf/.
504
S. Claypool et al.
(a) CUBIC
(b) BBR
(c) Hybla
(d) PCC
Fig. 2. Stacked graph comparison. From top to bottom, the graphs are: through-
put (Mb/s), round-trip time (milliseconds), and retransmission rate (percent). For all
graphs, the x-axis is time (in seconds) since the ﬂow started.
TCP Cubic illustrates typical exponential growth in throughput during start-
up, but exits slow start relatively early, about 15 s in where throughput is far
lower than the link capacity. Thus, it takes Cubic about 30 s to reach the expected
steady state throughput of about 100 Mb/s. During steady state (post 45 s) the
AQM drops enough packets to keep Cubic from persistently saturating the queue,
resulting in round-trip times of about 1 s. However, several spikes in transmission
rates yield corresponding spikes in round-trip times above 3 s and retransmission
rates above 20%.
TCP BBR ramps up to higher throughput more quickly than Cubic, but
this also causes high round-trip times and loss rates around 20 s as BBR over-
saturates the bottleneck queue. At steady state, BBR operates at a fairly steady
140 Mb/s, with relatively low loss and round-trip times about 750 ms since the
2x bandwidth-delay product BBR keeps in ﬂight is below the AQM queue limit.
However, there are noticeable dips in throughput every 10 s when BBR enters its
PROBE RTT state. In addition, there are intermittent round-trip time spikes
and retransmissions from loss which occur when BBR enters PROBE BW and
increases its transmission rate for 1 round-trip time.
TCP over a Satellite
505
TCP Hybla ramps up quickly, faster than does Cubic since it adjusts con-
gestion window growth based on latency, causing queuing at the bottleneck,
evidenced by the high early round-trip times. However, there are few retrans-
missions. At steady state Hybla achieves consistently high throughput, with a
slight growth in the round-trip time upon reaching about 140 Mb/s. Thereupon,
there is a slight upward trend to the round-trip time until the queue limit is
reached accompanied by some retransmissions.
TCP PCC ramps up somewhat slower than Hybla but faster than Cubic,
causing some queuing and some retransmissions, albeit fewer than BBR. At
steady state, throughput and round-trip times are consistent, near the minimum
round-trip time (around 600 ms), and the expected maximum throughput (about
140 Mb/s). The lower round-trip times are expected since round-trip time is used
by the PCC utility function.
4.3 Steady State
TCP’s overall performance includes both start-up and congestion avoidance
phases – the latter we call “steady state” in this paper. We analyze steady
state behavior based on the last half (in terms of bytes) of each trace.
Fig. 3. Steady state throughput distributions for 10%, 50%, 90% and mean.
For each algorithm, we compute steady state throughput in 1 s intervals,
extracting the 10th, 50th and 90th percentiles (and means) across all ﬂows.
Figure 3 shows the boxplot distributions. The top left is the distribution for the
10th percentiles, the top right the 50th (or median), the bottom left the 90th
percentile and the bottom right the mean. Each box depicts quartiles and median
for the distribution. Points higher or lower than 1.4 × the inter-quartile range
are outliers, depicted by the circles. The whiskers span from the minimum to
maximum non-outlier. Table 1 shows the corresponding summary statistics.
506
S. Claypool et al.
Table 1. Steady state throughput
summary statistics.
Table 2. Steady state throughput
eﬀect size (versus Cubic).
Algorithm Mean (Mb/s) Std Dev
t(158) p
Eﬀect Size
BBR
Cubic
Hybla
PCC
112.9
123.3
130.1
112.6
12.2
17.0
17.2
17.9
BBR 4.44 <.0001 0.7
0.0129 0.4
Hybla 2.51
PCC 3.88
0.0002 0.6
From the graphs, at the 10th percentile BBR has lowest distribution of steady
state throughput. This is attributed to its reduced throughput during the round-
trip time probing phase, which, if there is no change to the minimum round-trip
time, triggers every 10 s and lasts for about 1 s. PCC’s throughput at the 10th
percentile is also a bit lower than Cubic’s or Hybla’s, possibly because PCC’s
reward for a low round-trip time can result in occasional under-utilization.
BBR, Cubic and Hybla all have similar median steady state throughputs,
while PCC’s is a bit lower.
BBR has the highest distribution of throughput at the 90th percentile, fol-
lowed by Cubic, Hybla and PCC. BBR’s estimation of the link bandwidth may
yield more intervals of high throughput than the other algorithms. Hybla’s 90th
percentile distribution is the most consistent (as seen by the small box), while
PCC’s is the least, maybe due to fuller queues and emptier queues, respectively
(see Table 4).
From the table, Hybla has the highest mean steady state throughput, followed
by CUBIC, and then BBR and PCC are about the same. BBR steady state
throughput varies the least, probably since the consistent link quality provides
for a steady delivery rate and round-trip time.
Since Cubic is the default TCP congestion control algorithm for Linux and
Windows servers, we compare the mean throughput for an alternate algorithm
choice – BBR, Hybla or PCC – to the mean for Cubic by independent, 2-tailed
t tests (α = 0.05) with a Bonferroni correction and compute the eﬀect sizes.
An eﬀect size provides a measure of the magnitude of diﬀerence – in our case,
the diﬀerence of the means for two algorithms. In short, eﬀect size quantiﬁes
how much the diﬀerence in congestion control algorithm matters. The Cohen’s
d eﬀect size assesses the diﬀerences in means in relation to the pooled standard
deviation. Generally small eﬀect sizes are anything under 0.2, medium is 0.2 to
0.5, large 0.5 to 0.8, and very large above 0.8. The t test and eﬀect size results
are shown in Table 2. Statistical signiﬁcance is highlighted in bold.
From the table, the mean steady state throughput diﬀerences compared to
Cubic are all statistically signiﬁcant. BBR and PCC have lower steady state
throughputs than Cubic with large eﬀect sizes. Hybla has a higher throughput
than Cubic with a moderate eﬀect size.
TCP over a Satellite
507
Figure 4 shows the round-trip times during steady state. The x-axis is the
round-trip time in seconds computed from the TCP acknowledgments in the
Wireshark traces, and the y-axis is the cumulative distribution. There is one
trendline for each algorithm. Table 4 shows the summary statistics.
Table 3. Baseline round-trip time
summary statistics.
Table 4. Steady state round-trip time
summary statistics.
Mean
597.5 ms
Std dev 16.9 ms
Median 597 ms
Min
Max
564 ms
2174 ms
Algorithm Mean (ms) Std Dev
BBR
Cubic
Hybla
PCC
780
821
958
685
125.1
206.4
142.1
73.1
During steady state, Hybla typically has round-trip times about 200 ms
higher than any other algorithm, likely because its aggressive congestion window
growth with high round-trip time yields more queuing delay. PCC has the lowest
and steadiest round-trip times, near the link minimum, likely because its util-
ity function rewards low round-trip times. BBR and Cubic are in-between, with
BBR being somewhat lower than Cubic and a bit steadier. Cubic, in particular,
has a few cases with extremely high round-trip times. Across all ﬂows, about 5%
of the round trip times are 2 s or higher.
Fig. 4. Steady state round-trip time
distributions.
Fig. 5. Steady state retransmission dis-
tributions.
Figure 5 shows the retransmissions during steady state. The axes and data
groups are as for Fig. 4, but the y-axis is the percentage of retransmitted packets
computed over the second half of each ﬂow.
From the ﬁgure, Cubic has the highest retransmission distribution and Hybla
the lowest. BBR and PCC are in-between, with BBR moderately higher but PCC
508
S. Claypool et al.
having a much heavier tail. Hybla and PCC are consistently low (0%) for about
75% of all runs, compared to only about 20% for BBR and Cubic.
While higher round-trip times generally mean larger router queues and more
drops and retransmissions, the Viasat AQM does not drop packets until the
queue is above about 1 s of delay. This means if a ﬂow’s round-trip times remain
under about 1.6 s, it can avoid retransmissions.
4.4 Start-Up
We compare the start-up behavior for each algorithm by analyzing the ﬁrst 30 s
of each trace, approximately long enough to download 50 MBytes on our satellite
link. This is indicative of algorithm performance for some short-lived ﬂows and
is about when we observed throughput growth over time “ﬂattening” for most
ﬂows.
The average Web page size for the top 1000 sites was around 2 MBytes as of
2018 [10], including HTML payloads and all linked resources (e.g., CSS ﬁles and
images). The Web page size distribution’s 95th percentile was about 6 MBytes
and the maximum was about 29 MBytes. Today’s average total Web page size
is probably about 5 MBytes [13], dominated by images and video.