### Optimized Text

#### Payoff Analysis
1. **Decision-Making for Remaining Nodes:**
   - For each remaining node \( s_{kj} \) that is a member of set \( Y \), the payoff for cooperation, denoted as \( u_{kj}^C \), and the payoff for defection, denoted as \( u_{kj}^D \), are defined. Specifically, \( u_{kj}^D = -c_{so} \). In other words, if a member of \( Y \) defects, no new final block will be created in this round according to Definition 1.
   - To prevent \( s_{kj} \) from defecting, the payoff for cooperation \( u_{kj}^C \) must be greater than the payoff for defection \( u_{kj}^D \). This condition can be expressed as:
     \[
     u_{kj}^C > u_{kj}^D
     \]
   - From the above analysis, we can derive another bound for \( B_i \):
     \[
     B_i > \frac{K - c_{so}}{\gamma}
     \]
   - The strategy profiles that satisfy these conditions form a Nash equilibrium strategy profile in the game \( G_{Al+} \).

#### Proposed Reward Sharing Mechanism
- Our next goal is to extend the current Algorand reward sharing method by considering the strategic behavior of users/nodes. We aim to provide a solution for the Algorand Foundation to foster cooperative behavior among all Algorand nodes.
- The computed bounds in Theorem 3 show that the reward \( B_i \) can be minimized by selecting suitable values for \( \alpha \), \( \beta \), and \( \gamma \). Our results in Section III-C demonstrated that the Algorand Foundation needs to deploy an incentive-compatible mechanism to prevent nodes from engaging in selfish behavior (defection) to unilaterally increase their payoff.
- We propose Algorithm 1, which proceeds as follows:
  1. At the end of each Algorand round, the Algorand Foundation extracts the list of leaders (\( L \)), committee members (\( M \)), and other online nodes (\( K \)). These values can be computed by processing and verifying the sortition proofs sent by the Algorand nodes in their votes or block proposals.
  2. The foundation then calculates the optimal values for \( \alpha \) and \( \beta \) to minimize \( B_i \) using the bounds defined in Theorem 3.
  3. Next, the foundation computes the rewards for all nodes that participated in the round based on their roles and the computed \( \alpha \), \( \beta \), and \( B_i \) values.
  4. Finally, the foundation creates reward transactions for each node and gossips them to the Algorand network. These transactions are verified by the network and included in the next blocks.

```python
1: procedure REWARDSHARING(i)
2:    blocki ← Wait until final block created in round i
3:    // Compute α, β, Bi from Theorem 3 bounds
4:    L, M, K, Stakes ← ExtractDataFromBlock(blocki)
5:    α, β, Bi ← ComputeParameters(L, M, K, Stakes)
6:    for all Node n ∈ blocki do
7:        reward ← ComputeReward(n, Rolei(n), α, β, Bi)
8:        SendReward(reward, n, i)
9:    end for
10:   RewardSharing(i + 1)
11: end procedure
```

- It is important to note that since stakes are computed at the end of each round, the value of \( B_i \) is exactly the minimum reward the Algorand Foundation should pay to ensure cooperation. Thus, there is no incentive for selfish nodes to deviate from the reward sharing protocol.

### Evaluation
- To evaluate our proposed mechanism, we conducted a series of numerical analyses to determine the best reward shares (\( \alpha \) and \( \beta \)). According to Theorem 3, we can minimize the reward in each round while ensuring the cooperation of a subset of Algorand nodes.
- In our numerical analysis, we assumed the minimum acceptable stakes for each role to be \( s_k^* = 10 \) Algos. This assumption ignores any strong synchrony set containing nodes with stakes less than 10 Algos.
- We also assumed the cost of cooperation for leaders, committee members, and other nodes to be \( c_L = 16 \), \( c_M = 12 \), \( c_K = 6 \), and \( c_{so} = 5 \) micro Algos. Our results showed that for \( (\alpha, \beta) = (0.02, 0.03) \), the minimum value of \( B_i \) would be approximately 5.2 Algos per round.
- Given that \( S_K \) is usually much greater than \( S_L \) and \( S_M \), the calculated bounds in Theorem 3 are typically a function of the third bound, i.e., \( \frac{K - c_{so}}{\gamma} \). To minimize \( B_i \), we need to maximize \( \gamma \) and consequently minimize \( \alpha \) and \( \beta \) (recall that \( \gamma = 1 - \alpha - \beta \)).
- In summary, our mechanism ensures sufficient reward shares for leaders and committee members, as shown in Equation (7), and provides adequate rewards to all other online nodes, considering the value of \( B_i \) which is greater than \( \frac{K - c_{so}}{\gamma} \).

### Simulation Results
- We simulated an Algorand network with 500,000 nodes, where the stakes for leaders and committee members were \( S_L = 26 \) and \( S_M = 13K \), respectively. We distributed 50 million Algos among these nodes using three different stake distributions: (i) uniform distribution \( U(1, 200) \), (ii) normal distribution \( N(100, 20) \), and (iii) \( N(100, 10) \).
- In each round, we randomly selected 1000 nodes, with higher-stake nodes being chosen more frequently. We generated a series of random transactions for the selected nodes with a uniform distribution \( U(-4, 4) \). Negative values represent sending Algos, while positive values represent receiving Algos.
- We deployed both the Algorand Foundation's proposal and our proposed mechanism, running the simulation 200 times with different distributions, each instance executing for 10 rounds. We computed the average total rewards.
- The simulation results (Fig. 5) show that the calculated rewards for our proposed mechanism follow the stake distribution in the network. For example, higher rewards (around 50 Algos) are needed for a uniform distribution \( U(1, 200) \), while smaller rewards (around 5 Algos) are sufficient for a normal distribution \( N(100, 10) \).
- Comparing the results in Fig. 5-(d) (which simulates the current status of Algorand with more than 1 billion Algos, using \( N(2000, 25) \)) with Fig. 5-(c) (which simulates the initial phase with 50 million Algos), we conclude that as the total stake of the network increases, smaller rewards are needed to enforce cooperation (around 1.2 Algos).
- Fig. 6-(a) shows the cumulative calculated reward in each round with our proposed algorithm and the Algorand Foundation's mechanism. Our proposed mechanism distributes significantly smaller rewards compared to the Foundation's approach, which shares 20 Algos per round for the first 500,000 rounds. Our mechanism only increases rewards when the stake distribution is \( U(1, 200) \), due to the higher number of low-stake nodes. Removing nodes with smaller stakes (e.g., up to 7 stakes) can still maintain network synchrony and distribute smaller rewards, as shown in Fig. 6-(b).

### Conclusion
- In this paper, we introduced a system model to capture the main operational features of Algorand and comprehensively studied the problem of node selfishness. We proposed a solution to overcome it using game-theoretic modeling and analysis. Our analytical and numerical results show that we can always enforce cooperation by carefully distributing the correct amount of rewards.
- Our proposed reward sharing mechanism outperforms the current proposal by the Algorand Foundation. This work is a step towards better understanding the effects of selfish behavior in Algorand and can help the Foundation use Algos wisely and adapt dynamically to the stake distribution in the network.
- Future work includes collaborating with the Algorand Foundation to introduce our proposed mechanism for reward sharing in the initial phase and for the distribution of transaction fees as rewards in the near future.