{
  "name": "jobs.batch/nginx_ingress_controller_requests",
  "singularName": "",
  "namespaced": true,
  "kind": "MetricValueList",
  "verbs": [
    "get"
  ]
}
{
  "name": "namespaces/nginx_ingress_controller_requests",
  "singularName": "",
  "namespaced": false,
  "kind": "MetricValueList",
  "verbs": [
    "get"
  ]
}
```
我们得到了三个结果。每个的名称由资源类型和度量的名称组成。我们将丢弃那些与`jobs.batch`和`namespaces`相关的度量，而专注于与`ingresses.extensions`相关的度量，因为它提供了我们需要的信息。我们可以看到它是`namespaced`，这意味着这些度量，除了别的以外，被它们起源的名称空间分开。`kind`和`verbs`几乎总是一样的，没有太大的价值。
`ingresses.extensions/nginx_ingress_controller_requests`的主要问题是它提供了入口资源的请求数量。我们不能用它目前的形式作为自置居所津贴的标准。相反，我们应该用副本的数量来划分请求的数量。这将为我们提供每个副本的平均请求数，这应该是一个更好的 HPA 阈值。稍后我们将探讨如何使用表达式而不是简单的度量。了解通过入口进入的请求数量是有用的，但这可能还不够。
由于`go-demo-5`已经提供了仪表化的度量，所以看看我们是否能够检索`http_server_resp_time_count`会有所帮助。提醒一下，这与我们在[第 4 章](4.html)、*调试通过指标和警报发现的问题*中使用的指标相同。
```
 1  kubectl get --raw \
 2      "/apis/custom.metrics.k8s.io/v1beta1" \
 3      | jq '.resources[]
 4      | select(.name
 5      | contains("http_server_resp_time_count"))'
```
我们使用`jq`过滤结果，以便只检索`http_server_resp_time_count`。不要对空输出感到惊讶。这很正常，因为普罗米修斯适配器没有被配置为处理来自普罗米修斯的所有指标，而只是处理那些符合其内部规则的指标。因此，这可能是查看包含其配置的`prometheus-adapter`配置图的好时机。
```
 1  kubectl -n metrics \
 2      describe cm prometheus-adapter
```
输出太大，无法在书中呈现，所以我们将只讨论第一条规则。具体如下。
```
...
rules:
- seriesQuery: '{__name__=~"^container_.*",container_name!="POD",namespace!="",pod_name!=""}'
  seriesFilters: []
  resources:
    overrides:
      namespace:
        resource: namespace
      pod_name:
        resource: pod
  name:
    matches: ^container_(.*)_seconds_total$
    as: ""
  metricsQuery: sum(rate(>{>,container_name!="POD"}[5m]))
    by (>)
...
```
第一个规则只检索名称以`container` ( `__name__=~"^container_.*"`)开头的指标，标签`container_name`不是`POD`，并且`namespace`和`pod_name`不为空。
每个规则都必须指定一些资源覆盖。在这种情况下，`namespace`标签包含`namespace`资源。类似地，`pod`资源从标签`pod_name`中检索。进一步，我们可以看到`name`部分使用正则表达式来命名新的度量。最后，`metricsQuery`告诉适配器在检索数据时应该执行哪个普罗米修斯查询。
如果这种设置看起来令人困惑，你应该知道你不是唯一一个第一眼就感到困惑的人。普罗米修斯适配器，正如普罗米修斯服务器配置一开始很难掌握一样。然而，它们非常强大，允许我们定义服务发现规则，而不是指定单个指标(在适配器的情况下)或目标(在普罗米修斯服务器的情况下)。很快，我们将详细介绍适配器规则的设置。现在，重要的注意事项是，默认配置告诉适配器获取与一些规则匹配的所有度量。
到目前为止，我们看到`nginx_ingress_controller_requests`度量通过适配器是可用的，但是它是没有用的，因为我们需要将请求的数量除以副本的数量。我们还发现源自`go-demo-5` Pods 的`http_server_resp_time_count`公制不可用。总而言之，我们没有所有需要的度量，而适配器当前获取的大多数度量都没有用。毫无意义的查询会浪费时间和资源。
我们的下一个任务是重新配置适配器，以便只从普罗米修斯那里检索我们需要的指标。我们将尝试编写自己的表达式，只获取我们需要的数据。如果我们能够做到这一点，我们也应该能够创造自置居所津贴。
# 使用自定义指标创建水平项目自动缩放器
正如您已经看到的，普罗米修斯适配器附带了一组默认规则，这些规则提供了许多我们不需要的指标，而不是我们需要的所有指标。做得太多反而不够，是在浪费 CPU 和内存。我们将探索如何使用自己的规则定制适配器。我们的下一个目标是让适配器只检索`nginx_ingress_controller_requests`度量，因为这是我们唯一需要的度量。除此之外，它应该以两种形式提供该度量。首先，它应该检索按资源分组的费率。
第二种形式应该与第一种形式相同，但除以承载入口转发资源的 Pods 的部署的副本数量。
这个应该会给出每个副本的平均请求数，并且是我们第一个基于自定义指标的 HPA 定义的良好候选。
我已经准备了一个带有图表值的文件，它可能会实现我们当前的目标，所以让我们来看看它。
```
 1  cat mon/prom-adapter-values-ing.yml
```
输出如下。
```
image:
  tag: v0.5.0
metricsRelistInterval: 90s
prometheus:
  url: http://prometheus-server.metrics.svc
  port: 80
rules:
  default: false
  custom:
  - seriesQuery: 'nginx_ingress_controller_requests'
    resources:
      overrides:
        namespace: {resource: "namespace"}
        ingress: {resource: "ingress"}
    name:
      as: "http_req_per_second"
    metricsQuery: 'sum(rate(>{>}[5m])) by (>)'
  - seriesQuery: 'nginx_ingress_controller_requests'
    resources:
      overrides:
        namespace: {resource: "namespace"}
        ingress: {resource: "ingress"}
    name:
      as: "http_req_per_second_per_replica"
    metricsQuery: 'sum(rate(>{>}[5m])) by (>) / sum(label_join(kube_deployment_status_replicas, "ingress", ",", "deployment")) by (>)'
```
该定义中的前几个条目与我们之前通过`--set`参数使用的值相同。我们将跳过这些，跳到`rules`部分。
在`rules`部分，我们将`default`条目设置为`false`。这将摆脱我们之前探索过的默认规则，让我们重新开始。进一步来说，有两个`custom`规则。
第一个规则是基于以`nginx_ingress_controller_requests`为值的`seriesQuery`。`resources`部分中的`overrides`条目帮助适配器找出哪些 Kubernetes 资源与度量相关联。我们正在为`namespace`资源设置`namespace`标签的值。`ingress`也有类似的条目。换句话说，我们将普罗米修斯标签与 Kubernetes 资源`namespace`和`ingress`关联起来。
正如您将很快看到的，度量本身将是完整查询的一部分，HPA 将把它视为单个度量。既然我们在创造新的东西，我们就需要一个名字。因此，我们使用设置为`http_req_per_second`的单个`as`条目来指定`name`部分。这将是我们住房津贴定义中的参考。
你已经知道`nginx_ingress_controller_requests`本身并不是很有用。当我们在普罗米修斯中使用它时，我们必须把它放在一个`rate`函数中，我们必须`sum`所有的东西，我们必须按资源对结果进行分组。我们正在通过`metricsQuery`条目做类似的事情。把它想象成我们在普罗米修斯中写的表达。唯一不同的是，我们使用的是像`>`这样的“特殊”语法。这是适配器的模板机制。我们有`>`、`>`和`>`子句，这些子句将根据我们在 API 调用中放入的内容填充正确的值，而不是通过语句对度量、标签和组的名称进行硬编码。
第二条规则和第一条几乎一样。区别在于名字(现在是`http_req_per_second_per_replica`)和`metricsQuery`。后者现在用相关部署的副本数量来划分结果，就像我们在[第 3 章](3.html)、*中练习收集和查询指标以及发送警报*一样。
接下来，我们将使用新值更新图表。
```
 1  helm upgrade prometheus-adapter \
 2      stable/prometheus-adapter \
 3      --version v0.5.0 \
 4      --namespace metrics \
 5      --values mon/prom-adapter-values-ing.yml
 6
 7  kubectl -n metrics \
 8      rollout status \
 9      deployment prometheus-adapter
```
现在部署已经成功展开，我们可以再次检查存储在配置映射中的配置是否正确。
```
 1  kubectl -n metrics \
 2      describe cm prometheus-adapter
```
输出限于`Data`部分，如下所示。
```
...
Data
====
config.yaml:
----
rules:
- metricsQuery: sum(rate(>{>}[5m])) by (>)
  name:
    as: http_req_per_second
  resources:
    overrides:
      ingress:
        resource: ingress
      namespace:
        resource: namespace
  seriesQuery: nginx_ingress_controller_requests
- metricsQuery: sum(rate(>{>}[5m])) by (>) /
    sum(label_join(kube_deployment_status_replicas, "ingress", ",", "deployment"))
    by (>)
  name:
    as: http_req_per_second_per_replica
  resources:
    overrides:
      ingress:
        resource: ingress
      namespace:
        resource: namespace
  seriesQuery: nginx_ingress_controller_requests
...
```
我们可以看到，我们之前探讨的默认`rules`现在被我们在图表值文件的`rules.custom`部分定义的两个规则所取代。
配置看起来正确的事实并不一定意味着适配器现在提供了作为 Kubernetes 定制度量的数据。让我们也检查一下。
```
 1  kubectl get --raw \
 2      "/apis/custom.metrics.k8s.io/v1beta1" \
 3      | jq "."
```
输出如下。
```
{
  "kind": "APIResourceList",
  "apiVersion": "v1",
  "groupVersion": "custom.metrics.k8s.io/v1beta1",