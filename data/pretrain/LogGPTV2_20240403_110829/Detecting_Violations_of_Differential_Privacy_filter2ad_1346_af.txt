CCS ’18, October 15–19, 2018, Toronto, ON, Canada
(a) Correct implementation of SVT [28].
(b) iSVT 1 [38] adds no noise to query and threshold.
(c) iSVT 2 [11] no bounds on outputting True’s.
(d) iSVT 3 [27] query noise does not scale with N .
(e) iSVT 4 [36] outputs the actual query answer when it is
above the threshold.
Figure 4: Results for variants of Sparse Vector Technique
0.000.250.500.751.001.251.501.752.00Testϵ0.00.20.40.60.81.0PValueϵ0=0.2ϵ0=0.7ϵ0=1.50.000.250.500.751.001.251.501.752.00Testϵ0.00.20.40.60.81.0PValueϵ0=0.2ϵ0=0.7ϵ0=1.50.000.250.500.751.001.251.501.752.00Testϵ0.00.20.40.60.81.0PValueϵ0=0.2ϵ0=0.7ϵ0=1.50.00.51.01.52.02.53.0Testϵ0.00.20.40.60.81.0PValueϵ0=0.2ϵ0=0.7ϵ0=1.50.00.51.01.52.0Testϵ0.00.20.40.60.81.0PValueϵ0=0.2ϵ0=0.7ϵ0=1.5CCS ’18, October 15–19, 2018, Toronto, ON, Canada
Zeyu Ding, Yuxin Wang, Guanhong Wang, Danfeng Zhang, and Daniel Kifer
Table 2: Counterexamples detected for incorrect privacy mechanisms
Mechanism (ϵ0 = 1.5)
Incorrect Noisy Max with Laplace Noise
Incorrect Noisy Max with Exponential Noise
Incorrect Histogram [17]
iSVT 1 [38]
iSVT 2 [11]
iSVT 3 [27]
iSVT 4 [36]
Event E
ω ∈ (−∞, 0.0)
ω ∈ (−∞, 1.0)
ω[0] ∈ (−∞, 1.0)
t(ω) = 0
t(ω) = 9
t(ω) = 0
(ω .count(F alse), ω[9]) ∈ {9} × (−2.4, 2.4)
D1
[1, 1, 1, 1, 1]
[1, 1, 1, 1, 1]
[1, 1, 1, 1, 1]
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]
[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
D2
[0, 0, 0, 0, 0]
[0, 0, 0, 0, 0]
[2, 1, 1, 1, 1]
[0, 0, 0, 0, 0, 2, 2, 2, 2, 2]
[0, 0, 0, 0, 0, 1, 1, 1, 1, 1]
[0, 0, 0, 0, 0, 1, 1, 1, 1, 1]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
we have detected counterexamples showing the algorithm is likely
not private for any ϵ ∈ (0, 2.1]. When ϵ0 = 1.5, we have detected
counterexamples showing the algorithm is likely not private for
any ϵ ∈ (0, 3.0].
iSVT 3 [27]. Another incorrect variant (Algorithm 13) adds
5.3.4
noise to queries but the noise doesn’t scale with the bound N . The
actual privacy budget for this variant is 1+6N4
ϵ0 where ϵ0 is the
input privacy budget.
We note that our tool detects the actual privacy cost, as shown
in Figure 4d, for this incorrect algorithm. Consider privacy budget
ϵ0 = 0.2. The corresponding line rises at 0.3, right before the actual
budget 1+6N4
ϵ0 = 0.35 (N = 1), suggesting the precision of our tool.
The same happens for ϵ0 = 0.7 and 1.5. The two lines rise at 1.1
and 2.3, which are close to but before the actual budget 1.225 and
2.625, respectively.
iSVT 4 [36]. Another incorrect variant (Algorithm 15) out-
5.3.5
puts the actual value of noisy query answer when it is above the
noisy threshold.
The interesting part of this algorithm is that, since it outputs het-
erogeneous list of booleans and values, our event selector chooses
{9} × (−2.4, 2.4). This means we choose an event that consists of 9
booleans (in this case, Falses) followed by a number in (−2.4, 2.4).
Figure 4e shows much noise in it because this one is almost cor-
rect in the sense that violations of differential privacy happen with
very low probability; thus it is hard to detect its incorrectness. But
we can still see that the lines all rise later than the corresponding
claimed privacy budget ϵ0. Hence, our tool correctly concludes that
this algorithm does not satisfy ϵ0-differential privacy.
5.4 Performance
We performed all experiments on a double Intel® Xeon® E5-2620
v4 @ 2.10GHz CPU machine with 64 GB memory. Our tool is im-
plemented in Anaconda distribution of Python 3 and optimized for
running in parallel environment to fully utilize the 32 logical cores
of the machine.
For each test ϵ, we set the samples of iteration n to be 500,000
for the hypothesis test and 100,000 for the event selector and query
generator. Table 3 lists the average time spent on hypothesis test
for a specific test ϵ (i.e., the average time spent on generating one
single point in the figures) for each algorithm. The results suggest
that it is very efficient to run a test for an algorithm against one
privacy cost: all tests finish within 23 seconds.
The time difference between Noisy Max, Histogram and Sparse
Vector Technique is due to the nature of the algorithms. For SVT, the
Table 3: Time spent on running tool for different algorithms
Mechanism
Correct Laplace Noisy Max[15]
Incorrect Laplace Noisy Max
Correct Exponential Noisy Max [15]
Incorrect Exponential Noisy Max
Histogram [14]
Incorrect Histogram
SVT [28]
iSVT 1 [38]
iSVT 2 [11]
iSVT 3 [27]
iSVT 4 [36]
Time / Seconds
4.32
9.49
4.25
8.70
10.39
11.28
1.99
1.62
4.56
2.56
22.97
parameter N is set to 1, meaning that the algorithm will halt once
it hit a True branch. For Noisy Max and Histogram, all noise will be
calculated and applied to each query answer, consuming more time
to calculate p-values. Another factor that will also influence the test
time is the search space of events. Correct Noisy Max returns an
index which we would have a search space of only integers ranging
from 1 to the length of queries. However, the incorrect Noisy Max
will return a real number so the search space would be much larger
than the correct one, thus taking more time to find a suitable event
E. This also occurs in Sparse Vector Technique.
6 CONCLUSIONS AND FUTURE WORK
While it is invaluable to formally verify correct differentially-private
algorithms, we believe that it is equally important to detect incor-
rect algorithms and provide counterexamples for them, due to the
subtleties involved in algorithm development. We proposed a novel
semi-black-box method of evaluating differentially private algo-
rithms, and providing counterexamples for those incorrect ones. We
show that within a few seconds, our tool correctly rejects incorrect
algorithms (including published ones) and provides counterexam-
ples for them.
Future work includes extensions that detect violations of differ-
ential privacy even if those violations occur with extremely small
probabilities. This will require additional extensions such as a more
refined use of program analysis techniques (including symbolic
execution) that reason about what happens when a program is run
on adjacent databases. Additional extensions include counterex-
ample generation for other variants of differential privacy, such
Detecting Violations of Differential Privacy
CCS ’18, October 15–19, 2018, Toronto, ON, Canada
[22] Gian Pietro Farina, Stephen Chong, and Marco Gaboardi. 2017. Relational Sym-
bolic Execution. http://arxiv.org/abs/1711.08349. (2017).
[23] R.A. Fisher. 1935. The design of experiments. 1935. Oliver and Boyd, Edinburgh.
[24] Marco Gaboardi, Andreas Haeberlen, Justin Hsu, Arjun Narayan, and Benjamin C.
Pierce. 2013. Linear Dependent Types for Differential Privacy. In Proceedings of
the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming
Languages (ACM Symposium on Principles of Programming Languages (POPL)).
357–370.
[25] Eric Jones, Travis Oliphant, Pearu Peterson, et al. 2001–. SciPy: Open source
scientific tools for Python. (2001–). http://www.scipy.org/ [Online; accessed
2018-08-17].
[26] James C King. 1976. Symbolic execution and program testing. Commun. ACM 19,
7 (1976), 385–394.
[27] Jaewoo Lee and Christopher W Clifton. 2014. Top-k frequent itemsets via differ-
entially private fp-trees. In Proceedings of the 20th ACM SIGKDD international
conference on Knowledge discovery and data mining. ACM, 931–940.
[28] Min Lyu, Dong Su, and Ninghui Li. 2017. Understanding the sparse vector
technique for differential privacy. Proceedings of the VLDB Endowment 10, 6
(2017), 637–648.
[29] Frank D. McSherry. 2009. Privacy Integrated Queries: An Extensible Platform
for Privacy-preserving Data Analysis. In Proceedings of the 2009 ACM SIGMOD
International Conference on Management of Data. 19–30.
[30] Dimiter Milushev, Wim Beck, and Dave Clarke. 2012. Noninterference via sym-
bolic execution. In Formal Techniques for Distributed Systems. Springer, 152–168.
[31] Ilya Mironov. 2017. Rényi Differential Privacy. In 30th IEEE Computer Security
Foundations Symposium, CSF.
[32] Prashanth Mohan, Abhradeep Thakurta, Elaine Shi, Dawn Song, and David Culler.
2012. GUPT: Privacy Preserving Data Analysis Made Easy. In Proceedings of the
ACM SIGMOD International Conference on Management of Data.
[33] Suzette Person, Matthew B Dwyer, Sebastian Elbaum, and Corina S Pˇasˇareanu.
2008. Differential symbolic execution. In Proceedings of the 16th ACM SIGSOFT
International Symposium on Foundations of software engineering. ACM, 226–237.
[34] Silvio Ranise and Cesare Tinelli. 2006. The smt-lib standard: Version 1.2. Technical
Report. Technical report, Department of Computer Science, The University of
Iowa, 2006. Available at www. SMT-LIB. org.
[35] Jason Reed and Benjamin C. Pierce. 2010. Distance Makes the Types Grow
Stronger: A Calculus for Differential Privacy. In Proceedings of the 15th ACM
SIGPLAN International Conference on Functional Programming (ICFP ’10). 157–168.
[36] A. Roth. 2011. Sparse Vector Technique, Lecture notes for “The Algorithmic
Foundations of Data Privacy”. (2011).
[37] Indrajit Roy, Srinath Setty, Ann Kilzer, Vitaly Shmatikov, and Emmett Witchel.
2010. Airavat: Security and Privacy for MapReduce. In NSDI.
[38] Ben Stoddard, Yan Chen, and Ashwin Machanavajjhala. 2014. Differentially
private algorithms for empirical machine learning. arXiv preprint arXiv:1411.5428
(2014).
[39] Michael Carl Tschantz, Dilsun Kaynar, and Anupam Datta. 2011. Formal Verifica-
tion of Differential Privacy for Interactive Systems (Extended Abstract). Electron.
Notes Theor. Comput. Sci. 276 (Sept. 2011), 61–79.
[40] Lili Xu, Konstantinos Chatzikokolakis, and Huimin Lin. 2014. Metrics for Differ-
ential Privacy in Concurrent Systems. 199–215.
[41] Danfeng Zhang and Daniel Kifer. 2017. LightDP: Towards Automating Differential
Privacy Proofs. In ACM Symposium on Principles of Programming Languages
(POPL). 888–901.
as approximate differential privacy, zCDP, and renyi-differential
privacy.
ACKNOWLEDGMENTS
We thank anonymous CCS reviewers for their helpful suggestions.
This work was partially funded by NSF awards #1228669, #1702760
and #1566411.
REFERENCES
[1] Aws Albarghouthi and Justin Hsu. 2017. Synthesizing coupling proofs of dif-
ferential privacy. Proceedings of the ACM on Programming Languages 2, POPL
(2017), 58.
[2] Gilles Barthe, George Danezis, Benjamin Grégoire, César Kunz, and Santiago
Zanella-Beguelin. 2013. Verified Computational Differential Privacy with Ap-
plications to Smart Metering. In 2013 IEEE 26th Computer Security Foundations
Symposium. 287–301.
[3] Gilles Barthe, Noémie Fong, Marco Gaboardi, Benjamin Grégoire, Justin Hsu,
and Pierre-Yves Strub. 2016. Advanced Probabilistic Couplings for Differential
Privacy. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and
Communications Security. 55–67.
[4] Gilles Barthe, Marco Gaboardi, Emilio Jesús Gallego Arias, Justin Hsu, César
Kunz, and Pierre-Yves Strub. 2014. Proving Differential Privacy in Hoare Logic.
In 2014 IEEE 27th Computer Security Foundations Symposium. 411–424.
[5] Gilles Barthe, Marco Gaboardi, Benjamin Gregoire, Justin Hsu, and Pierre-Yves
Strub. 2016. Proving differential privacy via probabilistic couplings. In IEEE
Symposium on Logic in Computer Science (LICS).
[6] Gilles Barthe, Boris Köpf, Federico Olmedo, and Santiago Zanella Béguelin. 2012.
Probabilistic Relational Reasoning for Differential Privacy. In Proceedings of the
39th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming
Languages. 97–110.
[7] Gilles Barthe and Federico Olmedo. 2013. Beyond differential privacy: Com-
position theorems and relational logic for f-divergences between probabilistic
programs. In ICALP. 49–60.
[8] Mark Bun and Thomas Steinke. 2016. Concentrated Differential Privacy: Simpli-
fications, Extensions, and Lower Bounds. In Proceedings of the 14th International
Conference on Theory of Cryptography - Volume 9985.
[9] Cristian Cadar, Daniel Dunbar, and Dawson R. Engler. 2008. KLEE: Unassisted and
Automatic Generation of High-Coverage Tests for Complex Systems Programs. In
8th USENIX Symposium on Operating Systems Design and Implementation, (OSDI).
209–224.
[10] Cristian Cadar, Vijay Ganesh, Peter M. Pawlowski, David L. Dill, and Dawson R.
Engler. 2006. EXE: Automatically Generating Inputs of Death. In Conference on
Computer and Communications Security (CCS). 322–335.
[11] Rui Chen, Qian Xiao, Yu Zhang, and Jianliang Xu. 2015. Differentially private
high-dimensional data publication via sampling-based inference. In Proceedings
of the 21th ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining. ACM, 129–138.
[12] Yan Chen and Ashwin Machanavajjhala. 2015. On the Privacy Properties of
Variants on the Sparse Vector Technique. http://arxiv.org/abs/1508.07306. (2015).
[13] Leonardo De Moura and Nikolaj Bjørner. 2008. Z3: An efficient SMT solver. In
International conference on Tools and Algorithms for the Construction and Analysis
of Systems. Springer, 337–340.
[14] Cynthia Dwork. 2006. Differential Privacy. In Proceedings of the 33rd International
Conference on Automata, Languages and Programming - Volume Part II (ICALP’06).
Springer-Verlag, Berlin, Heidelberg, 1–12. https://doi.org/10.1007/11787006_1
[15] Cynthia Dwork. 2008. Differential privacy: A survey of results. In International
Conference on Theory and Applications of Models of Computation. Springer, 1–19.
[16] Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and
Moni Naor. 2006. Our Data, Ourselves: Privacy via Distributed Noise Generation.
In EUROCRYPT. 486–503.
[17] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006. Calibrat-
ing Noise to Sensitivity in Private Data Analysis.. In TCC.
[18] Cynthia Dwork, Moni Naor, Omer Reingold, Guy N.Rothblum, and Salil Vad-
han. 2009. On the Complexity of Differentially Private Data Release: Efficient
Algorithms and Hardness Results. In STOC. 381–390.
[19] Cynthia Dwork, Aaron Roth, et al. 2014. The algorithmic foundations of differ-
ential privacy. Foundations and Trends® in Theoretical Computer Science 9, 3–4
(2014), 211–407.
[20] Cynthia Dwork and Guy N. Rothblum. 2016. Concentrated Differential Privacy.
http://arxiv.org/abs/1603.01887. (2016).
[21] Hamid Ebadi, David Sands, and Gerardo Schneider. 2015. Differential privacy:
Now it’s getting personal. In ACM Symposium on Principles of Programming
Languages (POPL).