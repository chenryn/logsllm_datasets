### CCS '18, October 15–19, 2018, Toronto, ON, Canada

**Variants of the Sparse Vector Technique (SVT) and Their Properties:**

- **(a) Correct implementation of SVT [28]:** This variant correctly implements the SVT.
- **(b) iSVT 1 [38]:** This variant adds no noise to the query or threshold.
- **(c) iSVT 2 [11]:** This variant does not bound the number of `True` outputs.
- **(d) iSVT 3 [27]:** The noise added to the queries does not scale with the size \( N \).
- **(e) iSVT 4 [36]:** This variant outputs the actual query answer when it is above the threshold.

**Figure 4: Results for Variants of the Sparse Vector Technique**

The figure below shows the results for different variants of the SVT. The x-axis represents the test privacy parameter \(\epsilon\), and the y-axis represents the p-value. The lines represent different values of \(\epsilon_0\).

```
0.000.250.500.751.001.251.501.752.00
Test ϵ
0.00.20.40.60.81.0
P-Value
ϵ0=0.2
ϵ0=0.7
ϵ0=1.5
0.000.250.500.751.001.251.501.752.00
Test ϵ
0.00.20.40.60.81.0
P-Value
ϵ0=0.2
ϵ0=0.7
ϵ0=1.5
0.000.250.500.751.001.251.501.752.00
Test ϵ
0.00.20.40.60.81.0
P-Value
ϵ0=0.2
ϵ0=0.7
ϵ0=1.5
0.00.51.01.52.02.53.0
Test ϵ
0.00.20.40.60.81.0
P-Value
ϵ0=0.2
ϵ0=0.7
ϵ0=1.5
0.00.51.01.52.0
Test ϵ
0.00.20.40.60.81.0
P-Value
ϵ0=0.2
ϵ0=0.7
ϵ0=1.5
```

**Table 2: Counterexamples Detected for Incorrect Privacy Mechanisms**

| Mechanism (\(\epsilon_0 = 1.5\)) | Event E | D1 | D2 |
|---------------------------------|---------|----|----|
| Incorrect Noisy Max with Laplace Noise | \(\omega \in (-\infty, 0.0)\) | [1, 1, 1, 1, 1] | [0, 0, 0, 0, 0] |
| Incorrect Noisy Max with Exponential Noise | \(\omega \in (-\infty, 1.0)\) | [1, 1, 1, 1, 1] | [0, 0, 0, 0, 0] |
| Incorrect Histogram [17] | \(\omega[0] \in (-\infty, 1.0)\) | [1, 1, 1, 1, 1] | [2, 1, 1, 1, 1] |
| iSVT 1 [38] | \(t(\omega) = 0\) | [1, 1, 1, 1, 1, 1, 1, 1, 1, 1] | [0, 0, 0, 0, 0, 2, 2, 2, 2, 2] |
| iSVT 2 [11] | \(t(\omega) = 9\) | [1, 1, 1, 1, 1, 0, 0, 0, 0, 0] | [0, 0, 0, 0, 0, 1, 1, 1, 1, 1] |
| iSVT 3 [27] | \(t(\omega) = 0\) | [1, 1, 1, 1, 1, 0, 0, 0, 0, 0] | [0, 0, 0, 0, 0, 1, 1, 1, 1, 1] |
| iSVT 4 [36] | \((\omega.\text{count}(False), \omega[9]) \in \{9\} \times (-2.4, 2.4)\) | [1, 1, 1, 1, 1, 1, 1, 1, 1, 1] | [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] |

**Analysis of Incorrect Variants:**

- **iSVT 3 [27]:** This variant adds noise to the queries, but the noise does not scale with the bound \( N \). The actual privacy budget for this variant is \(\frac{1 + 6N}{4\epsilon_0}\), where \(\epsilon_0\) is the input privacy budget. Our tool detects the actual privacy cost, as shown in Figure 4d. For \(\epsilon_0 = 0.2\), the corresponding line rises at 0.3, right before the actual budget \(\frac{1 + 6N}{4\epsilon_0} = 0.35\) (for \( N = 1 \)), indicating the precision of our tool. The same pattern is observed for \(\epsilon_0 = 0.7\) and \(\epsilon_0 = 1.5\).

- **iSVT 4 [36]:** This variant outputs the actual value of the noisy query answer when it is above the noisy threshold. The event selector chooses \(\{9\} \times (-2.4, 2.4)\), meaning it selects an event consisting of 9 booleans (in this case, `False`) followed by a number in \((-2.4, 2.4)\). Figure 4e shows that this variant is almost correct, as violations of differential privacy occur with very low probability, making it difficult to detect its incorrectness. However, the lines all rise later than the claimed privacy budget \(\epsilon_0\), confirming that this algorithm does not satisfy \(\epsilon_0\)-differential privacy.

**Performance Evaluation:**

We conducted all experiments on a machine with a double Intel® Xeon® E5-2620 v4 @ 2.10GHz CPU and 64 GB memory. Our tool is implemented in the Anaconda distribution of Python 3 and optimized for parallel execution to fully utilize the 32 logical cores of the machine.

For each test \(\epsilon\), we set the samples of iteration \( n \) to be 500,000 for the hypothesis test and 100,000 for the event selector and query generator. Table 3 lists the average time spent on the hypothesis test for a specific test \(\epsilon\) (i.e., the average time spent on generating one single point in the figures) for each algorithm. The results indicate that it is very efficient to run a test for an algorithm against one privacy cost, with all tests finishing within 23 seconds.

**Table 3: Time Spent on Running Tool for Different Algorithms**

| Mechanism | Time / Seconds |
|-----------|----------------|
| Correct Laplace Noisy Max [15] | 4.32 |
| Incorrect Laplace Noisy Max | 9.49 |
| Correct Exponential Noisy Max [15] | 4.25 |
| Incorrect Exponential Noisy Max | 8.70 |
| Histogram [14] | 10.39 |
| Incorrect Histogram | 11.28 |
| SVT [28] | 1.99 |
| iSVT 1 [38] | 1.62 |
| iSVT 2 [11] | 4.56 |
| iSVT 3 [27] | 2.56 |
| iSVT 4 [36] | 22.97 |

**Conclusion and Future Work:**

Formally verifying correct differentially-private algorithms is invaluable, but detecting incorrect algorithms and providing counterexamples is equally important due to the subtleties involved in algorithm development. We proposed a novel semi-black-box method to evaluate differentially private algorithms and provide counterexamples for incorrect ones. Our tool correctly rejects incorrect algorithms (including published ones) and provides counterexamples within a few seconds.

Future work includes extending the tool to detect violations of differential privacy even if they occur with extremely small probabilities. This will require additional extensions such as more refined program analysis techniques, including symbolic execution, to reason about what happens when a program is run on adjacent databases. Additional extensions include counterexample generation for other variants of differential privacy, such as approximate differential privacy, zCDP, and Rényi-differential privacy.

**Acknowledgments:**

We thank anonymous CCS reviewers for their helpful suggestions. This work was partially funded by NSF awards #1228669, #1702760, and #1566411.

**References:**

- [22] Gian Pietro Farina, Stephen Chong, and Marco Gaboardi. 2017. Relational Symbolic Execution. http://arxiv.org/abs/1711.08349. (2017).
- [23] R.A. Fisher. 1935. The design of experiments. 1935. Oliver and Boyd, Edinburgh.
- [24] Marco Gaboardi, Andreas Haeberlen, Justin Hsu, Arjun Narayan, and Benjamin C. Pierce. 2013. Linear Dependent Types for Differential Privacy. In Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages (ACM Symposium on Principles of Programming Languages (POPL)). 357–370.
- [25] Eric Jones, Travis Oliphant, Pearu Peterson, et al. 2001–. SciPy: Open source scientific tools for Python. (2001–). http://www.scipy.org/ [Online; accessed 2018-08-17].
- [26] James C King. 1976. Symbolic execution and program testing. Commun. ACM 19, 7 (1976), 385–394.
- [27] Jaewoo Lee and Christopher W Clifton. 2014. Top-k frequent itemsets via differentially private FP-trees. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 931–940.
- [28] Min Lyu, Dong Su, and Ninghui Li. 2017. Understanding the sparse vector technique for differential privacy. Proceedings of the VLDB Endowment 10, 6 (2017), 637–648.
- [29] Frank D. McSherry. 2009. Privacy Integrated Queries: An Extensible Platform for Privacy-preserving Data Analysis. In Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data. 19–30.
- [30] Dimiter Milushev, Wim Beck, and Dave Clarke. 2012. Noninterference via symbolic execution. In Formal Techniques for Distributed Systems. Springer, 152–168.
- [31] Ilya Mironov. 2017. Rényi Differential Privacy. In 30th IEEE Computer Security Foundations Symposium, CSF.
- [32] Prashanth Mohan, Abhradeep Thakurta, Elaine Shi, Dawn Song, and David Culler. 2012. GUPT: Privacy Preserving Data Analysis Made Easy. In Proceedings of the ACM SIGMOD International Conference on Management of Data.
- [33] Suzette Person, Matthew B Dwyer, Sebastian Elbaum, and Corina S Păsăreanu. 2008. Differential symbolic execution. In Proceedings of the 16th ACM SIGSOFT International Symposium on Foundations of software engineering. ACM, 226–237.
- [34] Silvio Ranise and Cesare Tinelli. 2006. The SMT-LIB standard: Version 1.2. Technical Report. Technical report, Department of Computer Science, The University of Iowa, 2006. Available at www.SMT-LIB.org.
- [35] Jason Reed and Benjamin C. Pierce. 2010. Distance Makes the Types Grow Stronger: A Calculus for Differential Privacy. In Proceedings of the 15th ACM SIGPLAN International Conference on Functional Programming (ICFP '10). 157–168.
- [36] A. Roth. 2011. Sparse Vector Technique, Lecture notes for "The Algorithmic Foundations of Data Privacy". (2011).
- [37] Indrajit Roy, Srinath Setty, Ann Kilzer, Vitaly Shmatikov, and Emmett Witchel. 2010. Airavat: Security and Privacy for MapReduce. In NSDI.
- [38] Ben Stoddard, Yan Chen, and Ashwin Machanavajjhala. 2014. Differentially private algorithms for empirical machine learning. arXiv preprint arXiv:1411.5428 (2014).
- [39] Michael Carl Tschantz, Dilsun Kaynar, and Anupam Datta. 2011. Formal Verification of Differential Privacy for Interactive Systems (Extended Abstract). Electron. Notes Theor. Comput. Sci. 276 (Sept. 2011), 61–79.
- [40] Lili Xu, Konstantinos Chatzikokolakis, and Huimin Lin. 2014. Metrics for Differential Privacy in Concurrent Systems. 199–215.
- [41] Danfeng Zhang and Daniel Kifer. 2017. LightDP: Towards Automating Differential Privacy Proofs. In ACM Symposium on Principles of Programming Languages (POPL). 888–901.