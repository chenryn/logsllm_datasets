TABLE III: Anomaly Detection with Different Log Parsing
Methods (16,838 Anomalies)
Parsing
Accuracy
0.83
0.87
0.99
1.00
Reported
Anomaly
18,450
11,091
10,998
11,473
Detected
Anomaly
10,935 (64%)
10,678 (63%)
10,720 (63%)
11,195 (66%)
False
Alarm
7,515 (40%)
413 (3.7%)
278 (2.5%)
278 (2.4%)
SLCT
LogSig
IPLoM
Ground truth
D. RQ3: Effectiveness of Log Parsing Methods on Log Mining
To evaluate the effectiveness of log parsing methods on
log mining, we use three log parsers to tackle the parsing
challenge of a real-world anomaly detection task described in
Section III-B. In this task, there are totally 16,838 anomalies,
which are found manually in [2]. The parameters of SLCT and
LogSig are re-tuned to provide good Parsing Accuracy. LKE
is not employed because it could not handle this large amount
of data (10m+ lines) in reasonable time. The evaluation results
are illustrated in Table III. Reported Anomaly is the number
of anomalies reported by PCA, while adopting different log
parsers in the log parsing step. Detected Anomaly is the
number of true anomalies detected by PCA. False Alarm
means the number of wrongly detected anomalies. Ground
truth is the experiment using exactly correct parsed results
in anomaly detection. Notice that even the Ground truth could
not detect all anomalies because of the boundary of the PCA
anomaly detection model.
From Table III, we observe that the parsing accuracy of
these parsing methods are high (0.83 at least). LogSig and
IPLoM lead to nearly optimal results on the anomaly detec-
tion task. However, not all parsing methods lead to optimal
results. SLCT presents high Parsing Accuracy (0.83), but it
brings about 7,515 False Alarms in anomaly detection, which
introduces extensive unnecessary human effort on inspection.
Finding 5: Log parsing is important because log mining is
effective only when the parsing accuracy is high enough.
From Table III, we observe that the parsing accuracy of
SLCT (0.83) and LogSig (0.87) is comparable. However, the
performance of log mining using LogSig as parser is an order
of magnitude better than that using SLCT. Log mining task
using SLCT presents 7,515 False Alarms, introducing much
more human inspection effort than that using LogSig, which
only leads to 413 False Alarms. Besides, the log mining tasks
using LogSig and IPLoM as parsers produce comparable re-
sults. However, LogSig presents 12% more parsing errors than
IPLoM. These reveal that log mining results are sensitive to
some critical events, which could cause an order of magnitude
performance degradation. These also indicate that f-measure,
despite pervasively used in clustering algorithm evaluation,
may not be suitable to evaluate the effectiveness of log parsing
methods on log mining.
Finding 6: Log mining is sensitive to some critical events.
4% errors in parsing could even cause an order of
magnitude performance degradation in log mining.
V. DISCUSSIONS
Limitations: 1) Diversity of dataset. Not all datasets (two
out of ﬁve) used in our evaluation are production data. This is
mainly because of the lack of public log data. We thanks those
who release log data [2], [29], [28], which greatly facilitates
our research. However, Zookeeper and HDFS are popular
systems adopted by many companies for their distributed
computing jobs. We believe these logs could reﬂect the logs
from industrial companies to some extent. 2) Diversity of log
mining tasks. Results of effectiveness of log parsing methods
are evaluated on anomaly detection, which may not generalize
to other log mining tasks. This mainly because log mining task
with released real-world data is scarce. However, the anomaly
detection task evaluated is presented in a paper [2] with more
than 250 citations, which is an important log mining task
widely studied [32], [33]. Besides, even conducting evaluation
on one log event mining task, our results reveal the inconspic-
uous fact that the performance of log mining is sensitive to
parsing errors on critical events. We will consider to extend
our methodology on more varied log data as well as log mining
tasks in our future work.
Potential Directions: 1) Distributed Log Parsing. Our
experiments show that current log parsing methods cost a
lot of time on big data input. The amount of log message
in industrial companies could be much larger. Log parsing
methods based on heuristic rules are fast but their parsing
result is not good enough to fulﬁll the need of log mining task.
Thus, to accelerate the parsing process and further improve
its accuracy, log parsing methods which run in a distributed
manner are in demand. Clustering algorithms which could
be parallelized should be considered. 2) Logging of Event
ID. We could also improve log parsing process by recording
event ID in logs in the ﬁrst place. This approach is feasible
because developer writing log knows exactly which event a
log message statement match. Thus, adding event ID to log
message is a good logging practice [34] from the perspective
of log mining. Tools that could automatically add event ID into
source code may greatly facilitate the log parsing process.
VI. RELATED WORK
Log Analysis: Logs, as an important data source, are in
widespread use for system management tasks, such as anomaly
detection [3], [2], program veriﬁcation [5], [6], performance
monitoring [8], [7], security assurance [9], [10], failure anal-
ysis [35], etc. As shown in our evaluation results, log parsing
is a critical step to enable effective log analysis. Thus, we
believe our work on log parsing could beneﬁt future studies
on log analysis.
Log Parsing: Log parsing has been widely studied. Xu et al.
[2] implement a log parser with very high accuracy based on
source code analysis to infer log message templates. However,
in practice, source code is often unavailable or incomplete to
access, especially when third-party components are employed.
Some other work proposes data-driven approaches to log
parsing (e.g., SLCT [13], IPLoM [22], LKE [3], LogSig [15]),
in which data mining techniques are leveraged to extract log
message templates. But there is currently a lack of open-source
implementations on log parsing tools. Many researchers (e.g.,
[5], [8], [18]) and practitioners (as revealed in StackOverﬂow
questions [36], [37]) in this ﬁeld have to implement their
own log parsers to deal with their log data. This is a time-
consuming yet redundant task. Our work not only provides
valuable insights on log parsing, but also releases open-
source tool implementations on the state-of-the-art log parsing
methods.
Empirical Study: Empirical studies have attracted consid-
erable attraction in recent years, because the empirical results
could usually provide useful insights and direct suggestions
to both academic researchers and industrial practitioners. In
particular, Yuan et al. [38], [7] perform a characteristic study
on the logging practices in open-source systems and further
provide actionable suggestions for improvement. Meanwhile,
some recent work [39], [40], [41] has studied the logging
practices in industry. Our work is another empirical study, with
a focus on evaluations on log parsing and its use in log mining.
VII. CONCLUSION
Log parsing is employed pervasively in log mining. How-
ever, due to the lack of studies on performance of log parsing
methods, users often re-design a specialized log parser, which
is time-consuming. In this paper, we study the performance
of four state-of-the-art log parsing methods through extensive
experiments. We also analyze the effectiveness of the log
parsing methods on a real-world log mining task with 10
million log messages. We provide six valuable ﬁndings on
the parsing accuracy of the log parsers, efﬁciency of the log
parsers, and their effectiveness on log mining. In addition, the
source code of these log parsing methods is released for reuse
and further study.
ACKNOWLEDGMENT
The work described in this paper was fully supported by the
National Natural Science Foundation of China (Project No.
61332010), the Research Grants Council of the Hong Kong
Special Administrative Region, China (No. CUHK 14205214
of the General Research Fund), and 2015 Microsoft Research
Asia Collaborative Research Program (Project No. FY16-RES-
THEME-005).
REFERENCES
[1] H. Mi, H. Wang, Y. Zhou, R. Lyu, and H. Cai, “Toward ﬁne-grained,
unsupervised, scalable performance diagnosis for production cloud com-
puting systems,” IEEE Transactions on Parallel and Distributed Systems,
vol. 24, pp. 1245–1255, 2013.
[2] W. Xu, L. Huang, A. Fox, D. Patterson, and M. Jordon, “Detecting large-
scale system problems by mining console logs,” in SOSP’09: Proc. of
the ACM Symposium on Operating Systems Principles, 2009.
[3] Q. Fu, J. Lou, Y. Wang, and J. Li, “Execution anomaly detection in
distributed systems through unstructured log analysis,” in ICDM’09:
Proc. of International Conference on Data Mining, 2009.
[4] A. Makanju, A. Zincir-Heywood, and E. Milios, “Fast entropy based
alert detection in super computer logs,” in DSN-W’10: Proc. of Inter-
national Conference on Dependable Systems and Networks Workshops,
2010, pp. 52–58.
[5] I. Beschastnikh, Y. Brun, S. Schneider, M. Sloan, and M. Ernst,
“Leveraging existing instrumentation to automatically infer invariant-
constrained models,” in ESEC/FSE’11: Proc. of the 19th ACM SIGSOFT
Symposium and the 13th European Conference on Foundations of
Software Engineering, 2011.
[6] W. Shang, Z. Jiang, H. Hemmati, B. Adams, A. Hassan, and P. Martin,
“Assisting developers of big data analytics applications when deploying
on hadoop clouds,” in ICSE’13: Proc. of the 35th International Confer-
ence on Software Engineering, 2013, pp. 402–411.
[7] D. Yuan, S. Park, P. Huang, Y. Liu, M. Lee, X. Tang, Y. Zhou,
and S. Savage, “Be conservative: enhancing failure diagnosis with
proactive logging,” in OSDI’12: Proc. of the 10th USENIX Conference
on Operating Systems Design and Implementation, 2012, pp. 293–306.
[8] K. Nagaraj, C. Killian, and J. Neville, “structured comparative analysis
of systems logs to diagnose performance problems,” in NSDI’12: Proc.
of
the 9th USENIX conference on Networked Systems Design and
Implementation, 2012.
[9] A. Oprea, Z. Li, T. Yen, S. Chin, and S. Alrwais, “Dectection of early-
stage enterprise infection by mining large-scale log data,” in DSN’15,
2015.
[10] Z. Gu, K. Pei, Q. Wang, L. Si, X. Zhang, and D. Xu, “Leaps: De-
tecting camouﬂaged attacks with statistical learning guided by program
analysis,” in DSN’15, 2015.
[11] D. Lang, “Using SEC,” USENIX ;login: Magazine, vol. 38, no. 6, pp.
38–43, 2013.
[12] W. XU, “System problem detection by mining console logs,” Ph.D.
dissertation, University of California, Berkeley, 2010.
[13] R. Vaarandi, “A data clustering algorithm for mining patterns from event
logs,” in IPOM’03: Proc. of the 3rd Workshop on IP Operations and
Management, 2003.
[14] A. Makanju, A. Zincir-Heywood, and E. Milios, “Clustering event
logs using iterative partitioning,” in KDD’09: Proc. of International
Conference on Knowledge Discovery and Data Mining, 2009.
[15] L. Tang, T. Li, and C. Perng, “LogSig: generating system events from
raw textual logs,” in CIKM’11: Proc. of ACM International Conference
on Information and Knowledge Management, 2011, pp. 785–794.
[16] Splunk. [Online]. Available: http://www.splunk.com
[17] Logstash. [Online]. Available: http://logstash.net
[18] S. Banerjee, H. Srikanth, and B. Cukic, “Log-based reliability analysis of
software as a service (saas),” in ISSRE’10: Proc. of the 21st International
Symposium on Software Reliability Engineering, 2010.
[19] R. Vaarandi, “Mining event logs with slct and loghound,” in NOMS’08:
Proc. of the IEEE/IFIP Network Operations and Management Sympo-
sium, 2008.
[20] L. Huang, X. Ke, K. Wong, and S. Mankovskii, “Symptom-based
problem determination using log data abstraction,” in CASCON’10 Proc.
of the Conference of the Center for Advanced Studies on Collaborative
Research, 2010, pp. 313–326.
[21] R. Vaarandi and K. Podis, “Network ids alert classiﬁcation with fre-
quent itemset mining and data clustering,” in CNSM’10: Proc. of the
Conference on Network and Service Management, 2010.
[22] A. Makanju, A. Zincir-Heywood, and E. Milios, “A lightweight algo-
rithm for message type extraction in system application logs,” IEEE
Transactions on Knowledge and Data Engineering, vol. 24, pp. 1921–
1936, 2012.
[23] ——, “Investigating event log analysis with minimum apriori informa-
tion,” in IM’13: Prof. of International Symposium on Integrated Network
Management, 2013, pp. 962–968.
[24] Y. Jiang, C. Perng, and T. Li, “Meta: Multi-resolution framework for
event summarization,” in SDM’14: Proc. of the SIAM International
Conference on Data Mining, 2014, pp. 605–613.
[25] J. Lou, Q. Fu, S. Yang, Y. Xu, and J. Li, “Mining invariants from console
logs for system problem detection,” in ATC’10: Proc. of the USENIX
Annual Technical Conference, 2010.
[26] L. Tang, T. Li, L. Shang, F. Pinel, and G. Grabarnik, “An integrated
framework for optimizing automatic monitoring systems in large it
infrastructures,” in KDD’13: Proc. of International Conference on
Knowledge Discovery and Data Mining, 2013, pp. 1249–1257.
[27] G. Salton and C. Buckley, “Term weighting approaches in automatic
text retrival,” Cornell, Tech. Rep., 1987.
[28] A. Oliner and J. Stearley, “What supercomputers say: A study of ﬁve
system logs,” in DSN’07, 2007.
[29] L. A. N. S. LLC. Operational data to support and enable computer
science research. [Online]. Available: http://institutes.lanl.gov/data/fdata
[30] C. Manning, P. Raghavan, and H. Sch¨utze, Introduction to Information
Retrieval. Cambridge University Press, 2008.
[31] Evaluation of clustering. [Online]. Available: http://nlp.stanford.edu/IR-
book/html/htmledition/evaluation-of-clustering-1.html
[32] A. Lakhina, M. Crovella, and C. Diot, “Diagnosing network-wide trafﬁc
anomalies,” in SIGCOMM’04: Proc. of the Conference on Applications,
Technologies, Architectures, and Protocols for Computer Communica-
tions, 2004, pp. 219–230.
[33] H. Ringberg, A. Soule, J. Rexford, and C. Diot, “Sensitivity of pca for
trafﬁc anomaly detection,” in SIGMETRICS’07: Proc. of International
Conference on Measurement and Modeling of Computer Systems, 2007.
[34] F. Salfner, S. Tschirpke, and M. Malek, “Comprehensive logﬁles for
the 18th International
autonomic systems,” in IPDPS’04: Proc. of
Parallel and Distributed Processing Symposium, 2004.
[35] C. Di Martino, M. Cinque, and D. Cotroneo, “Assessing time coales-
cence techniques for the analysis of supercomputer logs,” in DSN’12,
2012.
[36] [Online]. Available: http://stackoverﬂow.com/questions/154982/what-is-
the-best-log-analysis-tool-that-you-used
[37] [Online]. Available:
http://stackoverﬂow.com/questions/2590251/is-
there-a-log-ﬁle-analyzer-for-log4j-ﬁles
[38] D. Yuan, S. Park, and Y. Zhou, “Characterizing logging practices in
the 34th International
open-source software,” in ICSE’12: Proc. of
Conference on Software Engineering, 2012, pp. 102–112.
[39] Q. Fu, J. Zhu, W. Hu, J. Lou, R. Ding, Q. Lin, D. Zhang, and T. Xie,
“Where do developers log? an empirical study on logging practices
in industry,” in ICSE’14: Companion Proc. of the 36th International
Conference on Software Engineering, 2014, pp. 24–33.
[40] J. Zhu, P. He, Q. Fu, H. Zhang, R. Lyu, and D. Zhang, “Learning to
log: Helping developers make informed logging decisions,” in ICSE’15:
Proc. of the 37th International Conference on Software Engineering,
2015.
[41] A. Pecchia, M. Cinque, G. Carrozza, and D. Cotroneo, “Industry prac-
tices and event logging: assessment of a critical software development
process,” in ICSE’15: Proc. of the 37th International Conference on
Software Engineering, 2015, pp. 169–178.