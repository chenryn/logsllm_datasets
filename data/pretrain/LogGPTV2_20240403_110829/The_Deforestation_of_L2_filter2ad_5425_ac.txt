put forth as a possibility earlier), a 10 Gbit network transmit-
ting min-sized packets would require around 1.16 seconds to
wrap the counter (which is a relatively long time considering
that the two-queue design ensures that ﬂooded packets are
delivered quickly). By timestamping entries in the ﬁlter, any
entry older than this can be invalidated.
The negative response, however, can happen simply when
two packets hash to the same value: the second would over-
write the ﬁrst, and if another copy of the ﬁrst arrived later, it
would not be detected as a duplicate. We lower the probabil-
ity of these false negatives by only applying packet dedupli-
cation to ﬂooded packets (since ﬂooded packets always “try”
to loop, while non-ﬂoods only loop in the rarer case that bad
state has been established), and the per-switch salt value de-
creases the chance that the same false negative will happen
at two different switches.
2.4 Details of Practical Algorithm
We can now present the pseudocode for a more practical ver-
sion of the AXE unipath design (Algorithm 2), in which we
explicitly include invocations of the deduplication mecha-
nism, but (for brevity) assume that the AXE header has al-
ready been added if not present and that HC has been incre-
mented. The code has two phases: the ﬁrst largely involves
deduplication and learning/unlearning, while the second is
responsible for forwarding the packet.
Some changes relative to the clean algorithm are fairly
straightforward; notably the decision to drop rather than ﬂood
packets where the HC exceeds the maximum is embodied in
lines 2-9, and the decision to learn from all packets with the
L ﬂag set (rather than just ﬂooded packets) is captured in line
30. Other changes are relatively minor, such as learning even
from packets with the L ﬂag not set if they have a smaller HC
(line 29), and unlearning only at ﬁrst hops (lines 21-26).
A more complicated change is that concerning “hairpin
turns” (line 62) where a switch has a forwarding entry point-
ing back the way the packet came. For an example of this,
return to Figure 1 and imagine that a packet from A arriving
at S3 encounters state pointing back towards S2. Before for-
warding the packet back to S2, the L ﬂag is unset (line 64) as
S2 learning that the path to A is via S3 is clearly ludicrous.
When the packet arrives back at S2, S2 may still have state
pointing towards S3. While S2 and S3 could simply hairpin
the packet back and forth until the hopcount reaches the max-
imum and the loop is resolved the same as any other loop, the
combination of the forwarding state and the unset L ﬂag are
used to infer the presence of this special case length-two cy-
cle and remove the looping state immediately (line 67). Note
that we again make the practical decision to drop the packet
and not convert it to a ﬂood, as the existence of such a cy-
cle is generally indicative of already adverse conditions. The
other possibility is that the packet arrives back at S2 and S2
now has state which does not point to S3. Such hairpinning
can arise, for example, due to multiple deduplication failures.
More commonly, it is caused by our use of two forward-
ing queues. With two queues, a ﬂooded packet can “pass”
an already queued non-ﬂood packet on a switch; when the
non-ﬂood one reaches the next switch, the ﬂooded one has
already changed the switch’s state. For example, imagine a
non-ﬂood packet to B queued on S2 with S2 not yet aware
that the S3−S4 link has failed. A learnable ﬂood packet from
B arrives at S2 (via S6), is placed in the high priority ﬂood
queue, and is immediately forwarded to S3: S3 learns that
the path to B is back towards S2. By the time the non-ﬂood
packet ﬁnally leaves the queue on S2, the state on S2 already
reﬂects the correct new path to B via S6 – as does the state
on S3 (thus requiring the packet to take a hairpin turn).
2.5 Enhancements to the Design
AXE can trivially accommodate various features that L2 op-
erators have come to rely on (e.g., VLAN tagging); here we
discuss three more signiﬁcant ways AXE can be extended.
501
Table.Unlearn(p.EthDst)
(cid:46) Break looping forwarding state.
(cid:46) Drop the packet.
end if
return
(cid:46) No table entry, may as well learn.
or p.HC  )
Filter.Insert(  )
(cid:46) Either the forwarding state loops or this is an old ﬂood which
(cid:46) the deduplication ﬁlter has never caught.
if !p.F then
(cid:46) We’re seeing (for the ﬁrst time) a packet which probably
(cid:46) originated from this switch and then hit a failure. Since our
(cid:46) forwarding state apparently points to a failure, unlearn it.
Table.Unlearn(p.EthDst)
1: (cid:46) * Start of ﬁrst phase. *
2: if p.HC > MAX_HOP_COUNT then
3:
4:
5:
6:
7:
8:
9: end if
10:
11: (cid:46) Check and update the deduplication ﬁlter.
12: if p.F then
13:
14:
15: else
16:
17:
18: end if
19:
20: SrcEntry ← Table.Lookup(p.EthSrc)
21: if !IsDuplicate and !p.L and SrcEntry and SrcEntry.HC = 1 then
22:
23:
24:
25:
26: end if
27:
28: if !SrcEntry
29:
30:
31:
32:
33: end if
34:
35: (cid:46) * Start of second phase. *
36: if IsDuplicate then
37:
38: end if
39:
40: if p.F then
41:
42:
43:
44: end if
45:
46: DstEntry ← Table.Lookup(p.EthDst)
47: if !DstEntry or IsPortDown(DstEntry.Port) then
48:
49:
50:
51:
52:
53:
54:
55:
56:
57:
58:
59:
60:
61:
62: else if DstEntry.Port = p.InPort then
63:
64:
65:
66:
67:
68:
69: else
70:
71: end if
(cid:46) This is the packet’s ﬁrst hop. L is already set.
Flood(p)
p.L ← False
Filter.Insert(  )
Flood(p)
Output(p, p.InPort)
(cid:46) Flooded packets just keep ﬂooding.
Flood(p)
return
end if
p.F ← True
if p.HC = 1 then
if p.L then
p.L ← False
Output(p, p.InPort)
(cid:46) Not the ﬁrst hop; don’t learn from the ﬂood.
(cid:46) Update ﬁlter.
(cid:46) Sends out all ports except InPort.
(cid:46) Send backwards too.
Table.Unlearn(p.EthDst)
(cid:46) Packet trying to hairpin twice
(cid:46) Break looping forwarding state
(cid:46) Packet wants to hairpin.
(cid:46) If learnable, try once to send it back.
(cid:46) No longer learnable.
if !p.L then
return (cid:46) Packet hairpinned but is now lost. Drop and give up.
(cid:46) Look up the output port.
(cid:46) No valid entry.
else
end if
else
end if
(cid:46) About to ﬂood the packet.
(cid:46) Flood learnably out all ports except InPort.
return (cid:46) We’ve already dealt with this packet; drop the duplicate.
(cid:46) Send out all ports except InPort.
(cid:46) And we’re done.
Output(p, DstEntry.Port)
(cid:46) Output in the common case.
Algorithm 2: AXE pseudocode for processing a packet p.
502
2.5.1 Periodic optimization
In order to make sure that non-optimal paths do not per-
sist, switches will periodically ﬂood packets from directly
attached hosts, allowing all switches to learn new entries for
it (a switch knows that it is a host’s ﬁrst hop because of the
hopcount in its forwarding entry).
2.5.2 Trafﬁc engineering
The approach AXE takes to ensure L2 connectivity is de-
signed to be orthogonal to potential trafﬁc engineering ap-
proaches. While some approaches aim to carefully schedule
each ﬂow in order to avoid congestion and meet other policy
goals, work such as Hedera [2] showed that identifying and
scheduling only elephant ﬂows can provide substantial ben-
eﬁt. AXE and Hedera are complementary, and using them
together only requires one extra bit in the header – a ﬂag
to indicate whether the packet should follow AXE paths or
Hedera paths. In a network using both approaches, we use
Hedera to compute paths for elephant ﬂows, while mice use
AXE paths. When a packet on a Hedera-scheduled ﬂow en-
counters a failure, we set the extra “AXE path” ﬂag and then
route the packet with AXE. The ﬂag is required to ensure
that the packet continues to be forwarded to its destination
using only AXE, as a combination of Hedera and AXE paths
could produce a loop. In this way, trafﬁc can be scheduled
for efﬁciency, but scheduled trafﬁc always has a guaranteed
fallback as AXE ensures connectivity at all times.
2.5.3 ECMP
While the discussion thus far has been about unipath deliv-
ery, extending AXE to support ECMP requires only three
changes: modifying the table structure, enabling the learn-
ing of multiple ports, and encouraging the learning of mul-
tiple ports. We extend the table by switching to a bitmap of
learned ports (rather than a single number), and by keeping
track of the nonce of the packet from which the entry was
learned. Upon receiving a packet with the L and F ﬂags set,
if the hopcount and nonce are the same as in the table, we
add the ingress port to the learned ports bitmap. If these two
ﬁelds do not match, we replace (or don’t replace) the entry
based on much the same criteria as for the unipath algorithm.
If L is set and F is not, we check that the hopcount and port
are consistent with the current entry. If not, we replace the
entry and ﬂood the packet (to encourage a ﬁrst-hop ﬂood).
A problem with this multipath approach is that while it is
easy to learn multiple paths in one direction – the originator
must ﬂood to ﬁnd the recipient, and this ﬂood allows learning
multiple paths – it is not as easy to learn multiple paths in the
reverse direction, as packets back to the originator will fol-
low one of the equal cost paths and therefore only establish
state along that single path. To address this, we need to ﬂood
in the reverse direction as well, encouraging multipath learn-
ing in both directions. This is, in fact, similar to the behavior
of the “clean” algorithm discussed in Section 2.1, though our
implementation here is slightly more subtle in order to inte-
grate with the rest of the practical algorithm and to provide
multiple chances to learn multiple paths given that we do
not expect it to operate under ideal conditions. The key is
adding another port bitmap to each table entry – a “ﬂooded”
bitmap. When a packet is going to be forwarded using an en-