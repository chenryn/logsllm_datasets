ReLu
target function
Softmax
3 APPLICATION TO DNN
The results from previous section aiming at single functions can
be directly used to alter the behavior of a neural network. In this
section we extend the attack to a full network, while targeting
several function computations at once with a multi-fault injection
model. There are three possible places to introduce a fault in DNN:
• Input layer – such fault would be identical to introducing a change
at the input data. Therefore, it is of little interest, since it would
be normally easier for the attacker to directly alter the input data.
• Hidden layer(s) – since the structure of the hidden layer is nor-
mally unknown to the attacker, she cannot easily predict the
outcome of the fault injection. However, she can still achieve
the misclassification, although not necessarily to the class she
decides. Therefore, such attack might be interesting in case the
attacker does not care about the outcome class as long as it is
missclassified.
• Output layer – normally, softmax is the function of choice for the
output layer. According to our results, introducing a meaningful
fault into softmax is harder compared to other functions.
One might be wondering how can the attacker develop a strategy if
the network is unknown to her. As shown recently [3], it is possible
to determine the activation function with a side-channel analysis,
i.e. by measuring the power consumption or the electromagnetic
emanation from the device during the computation. Getting the
weights and deciding on attacking particular neurons might be
trickier, so in this case, a random fault model would be the most
reasonable assumption for the attacker. Deciding on what layer to
attack, it makes sense to inject the fault as close to the output layer
as possible to make the impact highest. Therefore, for our case, the
attacker randomly injects faults into the 4th hidden layer of the
network, targeting multiple activation function computations. In
the following we provide evaluation results for such case.
Evaluation of DNN. To test how our attack can influence a real-
world DNN, we trained and evaluated different DNNs with the
random fault model described above. The attack methods con-
sidered are described in Section 2.2. We have selected a popular
MNIST dataset. The training of DNNs was accomplished using
Keras (ver.2.1.6) [6] and Tensorflow libraries (ver.1.8.0) [1]. The
structures of the DNNs are detailed in Table 2. For each target
function (ReLu, sigmoid and tanh), 8 DNNs with different number
of neurons (n) in hidden layer 4 were evaluated. We used a par-
tially fixed structure of DNN in order to study the effects of fault
attacks on different activation functions. The prediction accuracy
(see Table 3) of the tested network is comparable to state of the art.
Figures 1 (a) and 1 (b) show a random fault model when attacking
the last hidden layer of the network with 5 and 15 faults, respec-
tively. Success rates are calculated for 800 random inputs. Naturally,
with increasing number of neurons in the layer, the success rate
n
n
n
Target function
Train. Acc.
Test. Acc.
Target function
Train. Acc.
Test. Acc.
Target function
Train. Acc.
Test. Acc.
15
97.8
96.6
15
98.9
97.8
15
98.6
97.4
20
98.9
98.0
20
99.2
97.7
20
99.2
97.9
30
99.5
98.3
30
98.9
97.7
30
99.1
97.8
ReLu
40
99.1
97.6
50
99.2
97.4
sigmoid
50
40
99.1
99.2
98.0
98.0
tanh
40
99.0
97.8
50
99.0
98.0
n
o
i
t
a
c
fi
i
s
s
a
l
c
s
s
i
m
f
o
e
t
a
r
s
s
e
c
c
u
S
100
80
60
40
20
0
ReLu
Sigmoid
tanh
15 20
40
30
70
No. of neurons in the layer
50
60
n
o
i
t
a
c
fi
i
s
s
a
l
c
s
s
i
m
f
o
e
t
a
r
s
s
e
c
c
u
S
100
80
60
40
20
0
20
80
60
99.4
98.1
60
99.2
98.1
60
99.1
97.8
70
99.2
98.1
70
99.0
97.8
70
99.2
97.9
80
99.1
97.6
80
99.2
98.0
80
99.0
97.8
ReLu
Sigmoid
tanh
40
30
70
No. of neurons in the layer
50
60
80
(a)
(b)
Figure 1: Injecting (a) 5 and (b) 15 random faults in hidden layer 4.
drops. The figures also show that sigmoid and tanh functions fol-
low the same trend, which is caused by the same type of fault as
explained in the previous section – skipping the negation in the
exponentiation function.
Overall, it can be concluded that if the attacker wants to have a
reasonable success rate (>50%), she should inject faults in at least
half of the neurons in the chosen layer, in case of sigmoid and tanh.
For ReLu, she should fault at least 3/4 of the neurons.
4 CONCLUSION AND FUTURE WORK
While the laser fault injection, used in our experiments, might not be
possible in many scenarios, there are much simpler ways to disturb
the circuits, such as voltage/clock glitching or electromagnetic fault
injection [2]. In the future, we will also explore such applications.
It will also be interesting to look at possible countermeasures.
While there are already techniques available that correct non-
malicious alterations of the processed values in DNN (due to en-
vironmental conditions), the fault tolerance techniques against
malicious entities have to be developed in the same way as in the
area of applied cryptography.
REFERENCES
[1] M. Abadi et al. 2016. TensorFlow: A System for Large-Scale Machine Learning..
In OSDI, Vol. 16. 265–283.
[2] H. Bar-El, H. Choukri, D. Naccache, M. Tunstall, and C. Whelan. 2006. The
sorcerer’s apprentice guide to fault attacks. Proc. IEEE 94, 2 (2006), 370–382.
[3] L. Batina, S. Bhasin, D. Jap, and S. Picek. 2018. CSI Neural Network: Using Side-
channels to Recover Your Artificial Neural Network Information. Cryptology
ePrint Archive, Report 2018/477. (2018).
[4] E. Biham and A. Shamir. 1997. Differential fault analysis of secret key cryptosys-
tems. In Annual international cryptology conference. Springer, 513–525.
[5] J. Breier, D. Jap, and C.-N. Chen. 2015. Laser profiling for the back-side fault
attacks: with a practical laser skip instruction attack on AES. In CPSS 2015. ACM,
99–103.
[6] F. Chollet et al. 2015. Keras. (2015).
[7] C. Giraud and H. Thiebeauld. 2004. A survey on fault attacks. In Smart Card
Research and Advanced Applications VI. Springer, 159–176.
[8] I. Goodfellow, Y. Bengio, and A. Courville. 2016. Deep Learning. MIT Press.
[9] I. J Goodfellow, J. Shlens, and C. Szegedy. 2015. Explaining and harnessing
adversarial examples. ICLR (2015).
[10] Y. Liu, L. Wei, B. Luo, and Q. Xu. 2017. Fault injection attack on deep neural
network. In ICCD 2017. IEEE Press, 131–138.