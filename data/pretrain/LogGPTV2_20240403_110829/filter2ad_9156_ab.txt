将上述的训练集一分为二，90%作为训练集集，10%作为测试集并进行打标用于测试。在真正分类的时候，将所有的日志依据request、referer和user-agent这三个部分进行二分类。向量构造首先通过N-Gram将文本数据向量化，比如对于下面的例子：  
首先通过长度为N的滑动窗口将文本分割为N-Gram序列，例子中，N取2，窗口滑动步长为1，可以得到如下N-Gram序列：  
其中N的取值需要进行多次试验，不同的算法最佳值不同。然后声明一个长度为10000的特征向量，将这些序列映射到特征向量中，并使用TF-IDF生成特征向量的值。词频—逆文档频率（简称TF-IDF）是一种用来从文本文档（例如URL）中生成特征向量的简单方法。它为文档中的每个词计算两个统计值：一个是词频（TF），也就是每个词在文档中出现的频率，另一个是逆文档频率（IDF），用来衡量一个词在整个文档语料库中出现的（逆）频繁程度。这两个值的积，也就是TF×IDF，展示了一个词与特定文档的相关程度（比如这个词在某文档中很常见，但在整个语料库中却很少见）。
    def TFIDF(self,badData,goodData,distance,step):
        '''IT-IDF函数，根据不同的分词方法生成TF-IDF向量'''
        tf = self.tf
        badFeatures = badData.map(lambda line: tf.transform(split2(line,distance,step)))
        goodFeatures = goodData.map(lambda line: tf.transform(split2(line,distance,step)))
        badFeatures.cache()
        goodFeatures.cache()
        idf = IDF()
        idfModel = idf.fit(badFeatures)
        badVectors = idfModel.transform(badFeatures)
        idfModel = idf.fit(goodFeatures)
        goodVectors = idfModel.transform(goodFeatures)
        badExamples = badVectors.map(lambda features: LabeledPoint(1, features))
        goodExamples = goodVectors.map(lambda features: LabeledPoint(0, features))
        dataAll = badExamples.union(goodExamples)
        return dataAll
一个TF-IDF向量如下所示：  
其中第一项0.0是向量的标签，表示这是一条恶意的请求，后面是各个分词序列在投影后的坐标及其TF×IDF值。
## 2.5机器学习算法
三种算法训练完毕后以后的检测只需从本地加载模型即可
    def train(self,sc):
        # #生成Logistic和SVMWithSGD算法数据
        # dataLogistic = self.TFIDF(bad,good,3,1)
        # #生成SVMWithSGD算法数据
        # dataSVMWithSGD = self.TFIDF(bad,good,3,1)
        # #生成NaiveBayes算法数据
        # dataNaiveBayes = self.TFIDF(bad,good,2,1)
        # 使用分类算法进行训练,iterations位迭代次数,step为迭代步长
        # modelLogistic = LogisticRegressionWithSGD.train(data=dataLogistic,iterations=10000,step=6) 
        # print "train success1"
        # modelLogistic.save(sc,"model/modelLogistic")
        # modelSVMWithSGD = SVMWithSGD.train(data=dataSVMWithSGD,iterations=10000,step=5) 
        # print "train success2"
        # modelSVMWithSGD.save(sc,"model/modelSVMWithSGD")
        # modelNaiveBayes = NaiveBayes.train(data=dataNaiveBayes,lambda_=0.1) 
        # print "train success3"
        # modelNaiveBayes.save(sc,"model/modelNaiveBayes")
        self.modelLogistic = LogisticRegressionModel.load(sc,"modelLogistic")
        self.modelSVMWithSGD = SVMModel.load(sc,"modelSVMWithSGD")
        self.modelNaiveBayes = NaiveBayesModel.load(sc,"modelNaiveBayes")
    def check_Line(self,line,algorithm):
        """元素检测"""
        tf = self.tf
        request_url = line
        check_Result = 0
        if "Logistic" in algorithm:
            check_Result += self.modelLogistic.predict(tf.transform(split2(request_url,3,1)))
        if "SVM" in algorithm:
            check_Result += self.modelSVMWithSGD.predict(tf.transform(split2(request_url,3,1)))
        if "NaiveBayes" in algorithm:
            check_Result += self.modelNaiveBayes.predict(tf.transform(split2(request_url,2,1)))
        print check_Result
        print "model check  :  "+str(check_Result)
        if check_Result>2:
            line.append([-1])
        else:
            line.append([])
        return line
    def check(self,test,sc,algorithm="Logistic,SVM,NaiveBayes"):
        """执行模型检测"""
        self.train(sc)
        check_Line = self.check_Line
        temp1 = test.map(lambda line: check_Line(line,algorithm))
        return temp1.collect()
# 三、系统展示
**离线日志分析**  
离线分析包括分析报表和日志管理两个子功能，用户需要在日志管理处上传日志才可通过分析报表查看分析结果（如果直接点击分析报表界面则默认显示最近一次的分析结果），日志在上传的过程中就会完成数据分析，分析结果会在分析报表界面显示并同时写入数据库。同理，用户也可以在日志管理处删除已上传的日志，但同时也会删除存在数据库中的分析结果。日志管理界面如图所示：  
点击每一条记录右侧的查看按钮，即可跳到相应的分析报表界面，分析报表界面包含5个部分，分别为基本信息、访问次数最高的前10个IP、访问次数最高的前10个URL、攻击次数统计、攻击源地图。具体如图所示：  
**实时日志分析**  
实时分析部分包含两个显示界面，一个是访问次数（蓝色）与攻击次数（黑色）的双曲线图表，表示当前时间访问的次数以及当中可能包含的攻击次数，两者同时显示，相互对比；另一个是百度地图实时地理位置的世界地图图表。显示界面如下：  
# 四、一些问题
1、程序运行起来虽然看起来还可以，但是识别率其实比较一般，一是正则写的不够完善；二是机器学习误报有点高，如果把判别条件放太宽，会出现一些低级分类错误。  
2、算法中机器学习其实只是一个二分类，具体的攻击类别要靠正则识别，正则识别不出来而算法识别出来的则为未知攻击类型。  
3、这里的实时其实是伪实时。
# 五、参考文献