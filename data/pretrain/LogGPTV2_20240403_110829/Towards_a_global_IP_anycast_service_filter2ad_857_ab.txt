7
Source
AC:p
Dest
AA:g
IAP:AC:p
RAP:AC:p
JAP:q
RAP:AA:g
JAP:AA:g
AT:r
Comment
Native IP Anycast
IP-IP tunnel
IP-IP tunnel
Unicast IP
AT:r
AA:g
JAP:q
AC:p
JAP:q = NAT(AC:p)
Unicast IP
Unicast IP
AC:p = N AT −1
(JAP:q)
IAP:AC:p
JAP:AA:g
IP-IP tunnel
5
4
AT
6
AC
1
IAP
JAP
7
RAP
Figure 2: Initial (left) and subsequent (right) packet path. The table shows the various packet headers.
Symbols in block letters represent IP addresses, small letters represent ports. AA(Anycast Address) is one
address in the address block being advertised by PIAS, AA:g is the transport address assigned to the group
the target belongs to, while AT:r is the transport address at which the target wants to accept packets. Here,
the target joined the group by invoking join(AA:g,AT:r,options)
3.2 Scale by the number of groups
In the previous section, we mentioned the need for a scheme
that would allow PIAS to manage group membership infor-
mation while scaling to a large number of groups. For any
given group, we designate a small number of APs (three or
four) to maintain a list of JAPs for the group. When acting
in this role, we call the AP a Rendezvous Anycast Proxy
(RAP). All APs can act as RAPs (as well as as JAPs and
IAPs).
The RAPs associated with any given group are selected
with a consistent hash [11] executed over all APs. This sug-
gests that each proxy know all other proxies, and maintain
their current up/down status. This is possible, however, be-
cause we can assume a relatively small number of global APs
(≤20, 000, a number we derive later). We also assume that,
like infrastructure routers, APs are stable and rarely crash or
are taken out of service. The APs can maintain each other’s
up/down status through ﬂooding, gossip [12] or a hierarchi-
cal structure [13]. The current implementation uses ﬂood-
ing. Such an arrangement establishes a simple one-hop DHT
and hence, limits the latency overhead of routing through the
proxy overlay.
When a proxy becomes a JAP for the group (i.e. a target
of the group registers with it), it uses consistent hashing to
determine all the RAPs for the group and informs them of
the join. This allows the RAP to build a table of JAPs for
the group.
The concept of the RAP leads to a packet path as shown
on the left side of Figure 2. When an IAP receives a packet
for an anycast group that it knows nothing about, it hashes
the group TA, selects the nearest RAP for the group, and
transmits the packet to the RAP (path segment 2). The
RAP receives the packet and selects a JAP based on whatever
selection criteria is used for the group. For instance, if the
criteria is proximity, it selects a JAP close to the IAP. The
RAP forwards the packet to the selected JAP (path segment
3), and at the same time informs the IAP of the JAP (the
RAP sends a list of JAPs, for failover purposes).
The use of RAPs unfortunately introduces another overlay
hop in the path from client to target. We mitigate this cost
however by having the IAP cache information about JAPs.
Once the IAP has cached this information, subsequent pack-
ets (not only of this connection, but of subsequent connec-
tions too) are transmitted directly to the JAP. This is shown
in the right-hand side of Figure 2. The time-to-live on this
cache entry can be quite large. This is because the cache en-
try can be actively invalidated in one of two ways. First, if
the target leaves the JAP, the JAP can inform the IAP of this
when a subsequent packet arrives. Second, if the JAP disap-
pears altogether, inter-AP monitoring will inform all APs of
this event. In both cases, the IAP(s) will remove the cached
entries, failover to other JAPs it knows of, or failing this, go
back to the RAP. Because of this cache invalidation approach,
the IAP does not need to go back to the RAP very often.
Note that in ﬁgure 2, the JAP is responsible for transmit-
ting packets to and receiving packets from its targets. The
reasoning for this is not obvious and goes as follows. We aim
to support legacy clients that expect to see return packets
coming from the same address and port to which they sent
packets. In general, targets cannot source packets from any-
cast addresses and so at least one proxy must be inserted into
the target-client path. Furthermore, if NAT is being used to
forward packets to the target, then the proxy with the NAT
state should be the proxy that handles the return packets.
This might argue for traversing the IAP in the reverse di-
rection too, since by necessity it must be traversed in the
forward direction. The argument in favor of using the JAP
however, boils down to the following two points. First, it is
highly convenient to keep all target state in one proxy rather
than two or more. Since the JAP in any event must monitor
target health, it makes sense to put all target state in the
JAP. Second, the JAP is close to the target, so the cost of
traversing the JAP in terms of path length is minimal (Sec-
tion 4.3). Also, by seeing packets pass in both directions, the
JAP is better able to monitor the health of the target. For
the most part, when a packet passes from client to target, the
JAP may expect to soon see a packet in the reverse direction.
Rather than force the JAP to continuously ping each target,
the lack of a return packet can be used to trigger pings.
The use of proxies implies that the PIAS path (AC⇒IAP⇒
JAP⇒AT) might be longer than the direct path (AC⇒AT)6.
However, the proximity of the client to the IAP and of the
target to the JAP should ensure that PIAS imposes minimal
stretch and hence fulﬁlls goal 3. This has been substantiated
by simulating the stretch imposed by PIAS across a tier-1
topology map of the Internet.
The introduction of the RAP to allow scaling by the num-
ber of groups is somewhat equivalent to the extra round-trip
imposed by application-level anycast schemes, for instance in
the form of the DNS lookup or the HTTP redirect. This is
6the PIAS path may actually be shorter as inter-domain rout-
ing is not optimal[14]
one aspect of PIAS that falls short of native IP anycast, which
has no such extra hop. Having said that, it would be possible
for a small number of groups with minimal target churn to
operate without RAPS—that is, to spread JAP information
among all APs. This might be appropriate, for instance, for
a CDN or for 6to4 gateways. By-and-large, however, we can
expect most groups to operate with RAPs as described here,
and in the remainder of the design section, we assume that is
the case.
3.3 Scale by group size and dynamics
If the only selection criteria used by a RAP to select a JAP
were proximity to the client, then the RAP could ignore the
number of targets reachable at each JAP. In order to load
balance across targets, however, RAPs must know roughly
how many targets are at each JAP. In this way, RAPs can
select JAPs in a load balanced way, and each JAP can subse-
quently select targets in a load balanced way. Unfortunately,
requiring that RAPs maintain counts of targets at JAPs in-
creases the load on RAPs. This could be a problem for very
large groups, or for groups with a lot of churn.
We mitigate this problem by allowing the JAP to give the
RAP an approximate number of targets, for example within
25% or 50% of the exact number. For instance, if 25% error
is allowed, then a JAP that reported 100 targets at one time
would not need to report again until the number of targets
exceeded 125 or fell below 75. This approach allows us to
trade-oﬀ the granularity of load-balancing for scalability with
group size and dynamics. Indeed, this trade-oﬀ can be made
dynamically and on a per-group basis. A RAP that is lightly
loaded, for instance, could indicate to the JAP that 100%
accuracy reporting is allowed (i.e.
in its acknowledgement
messages). As the RAP load goes up, it would request less
accuracy, thus reducing its load. The combination of the two-
tiered approach with inaccurate information in a system with
2 groups is illustrated in Figure 3 (the ﬁgure assumes that
there is just one RAP for each group). Section 4.2 presents
simulations that show the beneﬁts of this approach in the
case of a large, dynamic group.
In any event, the number of targets is not the only measure
of load. Individual targets may be more-or-less loaded due
to diﬀering loads placed by diﬀerent clients. Ultimately, the
JAP may simply need to send a message to the RAPs when-
ever its set of targets are overloaded for whatever reason.
3.4 Scale by number of proxies
Given that we have laid out the basic architecture of PIAS,
we can now speciﬁcally look at PIAS deployment issues. A
central question is, how many proxies may we reasonably ex-
pect in a mature PIAS deployment, and can we scale to that
many proxies?
A key observation to make here is that the scaling charac-
teristics of PIAS are fundamentally diﬀerent from the scaling
characteristics of IP routing. While the traﬃc capacity of
the Internet can be increased by adding routers, the scalabil-
ity of IP routing per se is not improved by adding routers.
All routers must contain the appropriate routing tables. For
instance, all Tier1 routers must contain the complete BGP
routing table no matter how many Tier1 routers there are.
For the most part, IP routing is scaled by adding hierarchy,
not adding routers.
With PIAS, on the other hand, scaling does improve by
adding proxies. With each additional proxy, there are lower
RAP for group1
RAP for group2
RAP1
RAP2
JAP1
JAPn
Low activity
approximate 
membership 
information
High activity
aliveness
Target(group1)
Target (group2)
Figure 3: 2-tier membership management: the JAPs
keep the aliveness status for the associated targets;
the RAP for a group tracks the JAPs and an approx-
imate number of targets associated with each JAP
ratios of target-to-JAP and group-to-RAP. Growth in the
number of groups and targets can be absorbed by adding
proxies. However, an increase in the number of proxies presents
its own scaling challenge. Among other things, every proxy
is expected to know the up/down status of every other proxy.
The following describes a simple divide-and-conquer ap-
proach that can be used if the number of proxies grows too
large. In a typical deployment, a given anycast service provider
starts with one anycast preﬁx, and deploys proxies in enough
geographically diverse POPs to achieve good proximity. As
more anycast groups are created, or as existing anycast groups
grow, the provider expands into more POPs, or adds addi-
tional proxies at existing POPs. With continued growth, the
provider adds more proxies, but it also obtains a new address
preﬁx (or splits the one it has), and splits its set of proxies
into two distinct groups. Because the IP routing infrastruc-
ture sees one address preﬁx per proxy group, and because
a proxy group can consist of thousands of proxies and tens
of thousands of anycast groups, the provider could continue
adding proxies and splitting proxy groups virtually indeﬁ-
nitely.
The size of a mature proxy deployment may be roughly
calculated as follows. There are about 200 tier-1 and tier-
2 ISPs [15]. An analysis of the ISP topologies mapped out
in [16] shows that such ISPs have ∼25 POPs on average.
Assuming that we’d like to place proxies in all of these POPs,
this leads to 5000 POPs. Assuming 3-4 proxies per POP
(for reliability, dicussed later), we get a conservative total of
roughly 20,000 proxies before the infrastructure can be split.
While 20,000 proxies is not an outrageous number, it is
large enough that we should pay attention to it. One concern
not yet addressed is the eﬀect of the number of proxies on
IP routing dynamics. In particular, BGP reacts to route dy-
namics (ﬂapping) of a single preﬁx by “holding down” that
preﬁx—ignoring any advertisements about the preﬁx for a
period of at most one hour [17]. A naive proxy deployment
where each proxy advertises the anycast preﬁx directly into
BGP would imply that a proxy failure necessitates a BGP
withdrawal for the preﬁx (from the site where the proxy is
located) that could lead to hold downs. While the proxy sta-
bility ensures that such events do not occur often, even the
occasional preﬁx instability and the consequent service dis-
ruptions that a large proxy deployment would entail are not
acceptable.
Hence, the deployment model involves more than one proxy
being placed inside every POP where the proxies are de-
ployed. Such an arrangement is referred to as an anycast
Segment
AC⇒IAP
Failure of
IAP
IAP⇒JAP
JAP⇒AT
AT⇒JAP
JAP⇒AC
JAP
AT
JAP
AC
Failover through
IGP, onto a proxy
within the same
cluster
proxy health
monitoring system
pings between target
and JAP, passive
monitoring by JAP
pings routed to a
diﬀerent proxy
who becomes JAP
no failover needed
Section
3.6
3.6
3.1,3.2
3.6
-
1:
Failover
the PIAS forward
and reverse path
Table
along
path (AC⇒IAP⇒JAP⇒AT)
(AT⇒JAP⇒AC)
cluster 7 and is based on the model used by the anycasted
f-root server[18]. The approach involves connecting one or
more routers and more than one proxy to a common subnet.
All the proxies in the cluster advertise the anycast preﬁx into
IGP while the routers advertise it into BGP and hence, a
proxy-failure does not lead to a BGP withdrawal.
3.5 Proximity
The introduction of the proxies into the IP path negates
the natural ability of native IP anycast to ﬁnd the nearest
target. Therefore, we require explicit mechanisms in PIAS to
regain this capability.
As mentioned before, native IP anycast sets the client-IAP
and target-JAP path segments. The RAP, on the other hand,
selects the JAP, and therefore sets the IAP-JAP path segment
(on forward packets) and the JAP-client path segment (on
return packets). To ensure the proximity of the target to
the client, the RAP must choose a JAP close to the IAP and
hence, every AP must know the distance (in terms of latency)
between every pair of APs. This could be accomplished using
a proximity addressing scheme like GNP [19] or Vivaldi [20].
Another possibility is to use a simple, brute-force approach
whereby every AP occasionally pings every other AP and ad-
vertises the minimum measured round trip time (RTT) to
all other APs. This is feasible because, with the cluster de-
ployment approach, RAPs only need to know the distance
between each pair of clusters. While validating the above
claim would require experimentation with the actual deploy-
ment, back of the envelope calculations do paint a promising
picture for the simple approach.
3.6 Robustness and fast failover
The introduction of proxies between client and target might
have a negative impact on the robustness of PIAS as com-
pared to native IP anycast. On the other hand, RON[14] has
shown how an overlay structure can be used to improve the
resiliency of communication between any two overlay mem-
bers. Extending the same thought, PIAS, by ensuring the
robustness of packet traversal through the proxy overlay, can
improve the resiliency of communication between clients and
group members. We believe that given the stable nature
of the proxies, their deployment in well connected parts of
the Internet (tier-1 and tier-2 ISPs) and the engineering that
would go into their set-up, PIAS should be able to match, if
not better, the robustness oﬀered by native IP anycast.
A related requirement is that of fast fail-over. ”E2E” na-
tive IP anycast has to achieve failover when a group member
7hereon referred to as proxy cluster or simply, cluster
crashes, so that clients that were earlier accessing this mem-
ber are served by some other group member. Given the way
native IP anycast works, this failover is tied to IP routing con-
vergence. Speciﬁcally, in case of a globally distributed group,
the failover is tied to BGP convergence, which in some cases
can extend to a few minutes[14]. Since PIAS uses native IP
anycast to reach the proxies, it is subject to the same issues.
The process of overcoming the failure of a proxy is termed as
proxy failover. In addition, the proxies must themselves be
able to fail over from one target to another which is termed
as target failover. Thus the failover problem seems worse
with PIAS than with native IP anycast; however, this is not
the case.
3.6.1 Target failover
As discussed in Sections 3.1 and 3.2, the JAP is responsible
for monitoring the aliveness of its targets. It does this through
pinging and tracking data packets to and from the target.
The JAP is also responsible for directing IAPs to delete their
cache entries when enough targets have failed.
3.6.2 Proxy failover
There is still the question of clients failing over onto a dif-
ferent proxy when their IAP crashes, and targets failing over
when their JAP crashes. And there are two levels at which
this must be achieved: at the routing level and at the overlay
level.
At the routing level, the system must be engineered such
that when a proxy fails, clients that were using this proxy
as an IAP are rerouted to some other proxy quickly. PIAS’s
deployment of proxies in a cluster means that this failover
is across proxies within the same cluster. Also, since the
proxies advertise the preﬁx into IGP, PIAS relies on IGP for
convergence after a proxy failure and hence can achieve faster
failover. Typically, this is of the order of a few seconds and
can be reduced to sub-second times[21].