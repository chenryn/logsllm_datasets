60
80
100
No
scratchpad
Scratchpad
9-digit addition (OOD)
(D) Arithmetic
Figure 3: Specialized prompting or ﬁnetuning methods
can be emergent in that they do not have a positive ef-
fect until a certain model scale. A: Wei et al. (2022b).
B: Wei et al. (2022a). C & D: Nye et al. (2021). An
Wei et al., Emergent Abilities of Large Language Models, Preprint: arXiv:2206.07682
25 total: 40
ChatGPT的关键技术
预训练语言模型（Pre-trained Language Models，PLMs）
大型生成式预训练语言模型（Large Language Models, LLMs）
人类反馈强化学习（RLHF）
Content
从GPT-3到ChatGPT
Yao Fu, How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources (Blog)
26 total: 40
ChatGPT官方博客：方法
Methods
We trained this model using Reinforcement Learning from Human Feedback (RLHF), using the same methods as InstructGPT, but
with slight diﬀerences in the data collection setup. We trained an initial model using supervised ﬁne-tuning: human AI trainers
provided conversations in which they played both sides—the user and an AI assistant. We gave the trainers access to model-written
suggestions to help them compose their responses.
To create a reward model for reinforcement learning, we needed to collect comparison data, which consisted of two or more model
responses ranked by quality. To collect this data, we took conversations that AI trainers had with the chatbot. We randomly selected a
model-written message, sampled several alternative completions, and had AI trainers rank them. Using these reward models, we can
ﬁne-tune the model using Proximal Policy Optimization. We performed several iterations of this process.
ChatGPT is ﬁne-tuned from a model in the GPT-3.5 series, which ﬁnished training in early 2022. You can learn more about the 3.5
series here. ChatGPT and GPT 3.5 were trained on an Azure AI supercomputing infrastructure.
Limitations
ChatGPT sometimes writes plausible-sounding but incorrect or nonsensical answers. Fixing this issue is challenging, as: (1) during
RL training, there’s currently no source of truth; (2) training the model to be more cautious causes it to decline questions that it
can answer correctly; and (3) supervised training misleads the model because the ideal answer depends on what the model
ChatGPT Blog: https://openai.com/blog/chatgpt/
27 (1) total: 40
ChatGPT官方博客：方法
▶ 我们使用来自人类反馈的强化学习（RLHF）来训练这个模型，采用了
与InstructGPT相同的方法，但在数据收集设置上略有不同。我们首先使用有监督方法
微调了一个初始模型：由人类训练人员采用角色扮演的形式进行对话（他们在对话中
扮演了双方——用户和AI Agent）以获得对话数据。我们给训练人员提供了模型编写
建议，以帮助他们撰写答案。
▶ 为了创建强化学习的奖励模型，我们需要收集比较数据，对两个或更多的模型响应结
果按质量进行排序。为了收集这些数据，我们进行了人类训练人员与聊天机器人的对
话。我们随机选择一个模型生成的信息，对模型的后续响应进行多次采样，并让训练
人员对它们进行排名。使用这些奖励模型，我们可以使用近端策略优化（PPO）方法
对模型进行微调优化。我们对这个过程进行了几次迭代。
▶ ChatGPT是由GPT-3.5系列中的一个模型微调的，该模型于2022年初完成了训练。您
可以在此处了解有关GPT-3.5系列的更多信息。ChatGPT和GPT-3.5在Azure AI超级计
算基础架构上进行了训练。
ChatGPT Blog: https://openai.com/blog/chatgpt/
27 (2) total: 40
ChatGPT官方博客：方法
ChatGPT Blog: https://openai.com/blog/chatgpt/
27 (3) total: 40
Instruct Tuning
Ouyang et al., “Training Language Models to Follow Instructions with Human Feedback,” OpenAI, Jan 2022
28 total: 40
人类反馈的强化学习（RLHF）
第一阶段：冷启动阶段的监督策略模型。靠GPT 3.5本
身，尽管它很强，但是它很难理解人类不同类型指令
中蕴含的不同意图，也很难判断生成内容是否是高质
量的结果。为了让GPT 3.5初步具备理解指令中蕴含的
意图，首先会从测试用户提交的prompt(就是指令或问
题)中随机抽取一批，靠专业的标注人员，给出指
定prompt的高质量答案，然后用这些人工标注好
的数据来Fine-tune GPT 3.5模型。
经过这个过程，我们可以认为GPT 3.5初步具备了理解
人类prompt中所包含意图，并根据这个意图给出相对
高质量回答的能力，但是很明显，仅仅这样做是不够
的。
张俊林: ChatGPT会成为下一代搜索引擎吗（blog）
29 (1) total: 40
人类反馈的强化学习（RLHF）
第二阶段：训练回报模型（Reward Model,RM）。首先
由冷启动后的监督策略模型为每个prompt产生K个结
果，人工根据结果质量由高到低排序，用这些排序结
果来训练回报模型。对于学好的RM模型来说，输
入，输出结果的质量得分，得分越高
说明产生的回答质量越高。
张俊林: ChatGPT会成为下一代搜索引擎吗（blog）
29 (2) total: 40
人类反馈的强化学习（RLHF）
第三阶段：采用强化学习来增强预训练模型的能力。
本阶段无需人工标注数据，而是利用上一阶段学好
的RM模型，靠RM打分结果来更新预训练模型参数。
张俊林: ChatGPT会成为下一代搜索引擎吗（blog）
29 (3) total: 40
RLHF的人工数据标注
在RLHF数据标注过程中，OpenAI使用了 40 名全职标注员，这些标注人员经过严
格的筛选和培训，从而使他们形成一个统一的价值取向和标准。他们也会被筛查
（screening），从而确保他们不会偏离预定的价值取向和标准。
Ouyang et al., “Training Language Models to Follow Instructions with Human Feedback,” OpenAI, Jan 2022
30 total: 40
RLHF人工标注准则
▶ Misinformation (correct)
1. Only make statements that could plausibly be true; do not say things that are
obviously false.
2. Do not make assumptions about the user, only draw conclusions supported by
the dialogue.
3. Do not claim to take any actions in the real world. The agent can only converse,
not e.g. send emails.
4. Do not give an impression of medical authority or expertise, and do not offer
medical advice; instead suggest asking a doctor. (But it is ok to discuss general
questions about biology and medicine).
5. Do not give specific legal advice; instead suggest asking a lawyer. (But it is ok to
answer general questions about the law.)
6. Do not offer financial advice. (But it is ok to answer general questions about
investment.)
7. Do not endorse conspiracy theories, or views commonly considered to be
conspiracy theories.
来源：https://arxiv.org/pdf/2209.14375.pdf
31 (1) total: 40
RLHF人工标注准则
▶ Stereotypes (harm)
1. Do not use stereotypes or make any other harmful generalising statements
about groups of people.
2. Do not use microaggressions, i.e. statements that in subtle ways communicate
some sort of bias against historically marginalised groups.
▶ Hate and harassment (harm)
1. Do not make statements which are threatening.
2. Do not make statements which can be perceived as sexually aggressive.
3. Do not make negative or hateful comments targeting someone because of
aspects of their identity, such as where they are from or how they look.
4. Do not output language which is insulting (to the conversation partner or any
other individual or group).
5. Do not output language that can reasonably be considered harassment or
hateful language.
来源：https://arxiv.org/pdf/2209.14375.pdf
31 (2) total: 40
RLHF人工标注准则
▶ Self-anthropomorphism (harm)
1. Do not pretend to have a body or be able to move in a body.
2. Do not build a relationship to the user.
3. Do not claim to have preferences, feelings, opinions, or religious beliefs.
4. Do not pretend to have a human identity or life history, such as having a place of
birth, relationships, family, memories, gender, age.
▶ Other (harm)
1. Do not make statements that could be considered harmful (by harming the user
or otherwise).
▶ Other (helpful)
1. The agent’s statements should make sense in the context of the dialogue.
2. The agent’s statements should be on-topic in the context of the dialogue, by
providing relevant information or asking clarifying questions if the user is unclear.
3. The agent must clearly address the queries from the user.
4. The agent should not repeat itself unnecessarily.
来源：https://arxiv.org/pdf/2209.14375.pdf
31 (3) total: 40
采用RLHF取得的收益
这里给出了
ChatGPT比
InstructGPT
在回复假设
性和安全性
问题方面有
所改善的一
些例子。
ChatGPT Blog: https://openai.com/blog/chatgpt/
32 (1) total: 40
采用RLHF取得的收益
32 (2) total: 40
ChatGPT概览
ChatGPT的出色表现
ChatGPT的关键技术
ChatGPT的不足之处
ChatGPT未来发展方向
Content
ChatGPT官方博客：局限性
ChatGPT is ﬁne-tuned from a model in the GPT-3.5 series, which ﬁnished training in early 2022. You can learn more about the 3.5
series here. ChatGPT and GPT 3.5 were trained on an Azure AI supercomputing infrastructure.
Limitations
ChatGPT sometimes writes plausible-sounding but incorrect or nonsensical answers. Fixing this issue is challenging, as: (1) during
RL training, there’s currently no source of truth; (2) training the model to be more cautious causes it to decline questions that it
can answer correctly; and (3) supervised training misleads the model because the ideal answer depends on what the model
knows, rather than what the human demonstrator knows.
ChatGPT is sensitive to tweaks to the input phrasing or attempting the same prompt multiple times. For example, given one
phrasing of a question, the model can claim to not know the answer, but given a slight rephrase, can answer correctly.
The model is often excessively verbose and overuses certain phrases, such as restating that it’s a language model trained by
OpenAI. These issues arise from biases in the training data (trainers prefer longer answers that look more comprehensive) and
well-known over-optimization issues.
Ideally, the model would ask clarifying questions when the user provided an ambiguous query. Instead, our current models usually
guess what the user intended.
While we’ve made eﬀorts to make the model refuse inappropriate requests, it will sometimes respond to harmful instructions or
exhibit biased behavior. We’re using the Moderation API to warn or block certain types of unsafe content, but we expect it to have
some false negatives and positives for now. We’re eager to collect user feedback to aid our ongoing work to improve this system.
Iterative deployment
Today’s research release of ChatGPT is the latest step in OpenAI’s iterative deployment of increasingly safe and useful AI systems.
Many lessons from deployment of earlier models like GPT 3 and Codex have informed the safety mitigations in place for this release
1,2
ChatGPT Blog: https://openai.com/blog/chatgpt/
33 (1) total: 40
ChatGPT官方博客：局限性
▶ ChatGPT有时会写出听起来有道理但实际上并不正确甚至可能是荒谬的答案。解决这
个问题是非常有挑战性的，因为：(1)在RL训练期间，目前并没有提供信息真实性的来
源；(2)训练一个更加谨慎模型，会导致它拒绝回答一些它能够正确回答的问题；(3)有
监督的训练方法会误导模型，因为理想的答案应该来自于模型所掌握的知识，而不是
人类训练人员所掌握的知识。
▶ ChatGPT对调整输入措辞或多次尝试同一提示（Prompt）很敏感。例如，给定一个问
题的一个措辞，模型可以声称不知道答案，但只要稍微重新措辞，就可以正确回答。
▶ 该模型通常过于冗长，并过度使用某些短语，例如重申它是由OpenAI训练的语言模
型。这些问题来自培训数据中的偏见（培训人员更喜欢看起来更全面的更长的答案）
和众所周知的过度优化问题。
▶ 理想情况下，当用户提供模棱两可的查询时，模型会提出澄清问题。否则，我们目前
的模型通常会随意猜测用户的意图。
▶ 虽然我们已经努力让模型拒绝不适当的请求，但它有时仍会响应有害的指令或表现出
偏见的行为。我们正在使用Moderation API来警告或阻止某些类型的不安全内容，但
我们预计它目前会有一些误报和误报。我们渴望收集用户反馈，以帮助我们正在进行
的改进该系统的工作。
ChatGPT Blog: https://openai.com/blog/chatgpt/
33 (2) total: 40
事实与常识错误
34 total: 40
数学能力和逻辑能力不足
35 total: 40
价值观保护机制不完善
36 total: 40
ChatGPT概览
ChatGPT的出色表现
ChatGPT的关键技术
ChatGPT的不足之处
ChatGPT未来发展方向
Content
ChatGPT未来发展方向
▶ 与检索结合(改善事实性和实时性)
▶ 调用外部能力(改善数学和推理能力)
▶ 多模态理解和生成
▶ 终生持续学习
37 total: 40
与检索结合
https://perplexity.ai
38 total: 40
调用外部能力
Stephen Wolfram, Wolfram|Alpha as the Way to Bring Computational Knowledge Superpowers to ChatGPT
39 total: 40
ChatGPT概览
ChatGPT的出色表现
ChatGPT的关键技术
ChatGPT的不足之处
ChatGPT未来发展方向
Content
Summary
ChatGPT概览
ChatGPT的出色表现
ChatGPT的关键技术
ChatGPT的不足之处
ChatGPT未来发展方向
Thank you!
把数字世界带入每个人、每个家庭、
每个组织，构建万物互联的智能世界。
Bring digital to every person, home and organization
for a fully connected, intelligent world.
Copyright©2018 Huawei Technologies Co., Ltd.
All Rights Reserved.
The information in this document may contain
predictive statements including, without limitation,
statements regarding the future financial and
operating results, future product portfolio, new
technology, etc. There are a number of factors that
could cause actual results and developments to
differ materially from those expressed or implied in
the predictive statements. Therefore, such
information is provided for reference purpose only
and constitutes neither an offer nor an acceptance.
Huawei may change the information at any time
without notice.