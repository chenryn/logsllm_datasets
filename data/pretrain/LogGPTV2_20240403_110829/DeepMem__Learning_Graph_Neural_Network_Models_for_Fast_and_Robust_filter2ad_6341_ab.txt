In this section, we first present an overview of DeepMem, and then
delve into three important components respectively.
3.1 Overview
Figure 1 illustrates the overview of DeepMem. Generally speak-
ing, we divide DeepMem into two separate stages: training and
detection.
3.1.1 Training Stage. In this stage, DeepMem automatically learns
the representation of kernel objects from raw bytes. First, memory
dumps are fed into a graph constructor to generate a graph for each
memory dump (which is called “memory graph”), where each node
is a segment between two pointers, and each edge represents either
an adjacency relation or a points-to relation between two nodes.
Second, a node label generator will assign a label for each node
in the memory graph. We can use any existing tools (such as Volatil-
ity [37], or dynamic binary analysis tool DECAF [15]) for this pur-
pose. This seems a little contradictory: we rely on an existing anal-
ysis tool to build a new analysis tool. This is reasonable because the
existing tool only serves as an offline training purpose, so it does
not need to be efficient and robust. It only needs to have reasonable
Graph ConstructorNode Label GeneratorMemory GraphNode LabelsTraining Memory DumpObject DetectorTraining PhaseDetection PhaseKernel objectsEmbedding NetworkNode Classifier NetworkMemory GraphGraph ConstructorTesting Memory Dump(a) Raw Memory
(b) Memory Graph
Figure 2: Generate a memory graph from raw memory
accuracy in terms of labeling. After training, our detection model
is expected to achieve good efficiency, robustness, and accuracy
simultaneously.
Third, a memory graph is fed into a graph neural network ar-
chitecture. By propagating information from neighboring nodes
after several iterations, this graph neural network carries a latent
numeric vector (called embedding) for each node in the memory
graph.
Finally, all nodes’ embedding vectors will go through a neural
network classifier to get the predicted labels. The predicted labels
will be compared with the expected labels to compute the loss of
the classifier and update the weights of our neural network.
3.1.2 Detection Stage. this stage, DeepMem accepts an unlabeled
raw memory dump and detects kernel objects inside it. First, it
follows the same procedure to generate a memory graph for this
memory dump. Second, the memory graph is fed into the Graph
Neural Network (GNN) model obtained from the training stage to
generate embeddings of all the nodes and then predict node labels
using the neural network classifier. At last, DeepMem performs
an object detection process. This is because the labels predicted
from the last step are for segments, and an object may consist of
one or several segments. Therefore, the object detection process
takes segment labels as input and uses a voting mechanism to detect
objects, for which most of their segment labels agree upon the same
object label.
In the remainder of this section, we will discuss the definition of
memory graph and its construction in Section 3.2, the graph neural
network model for computing memory segments’ embeddings as
well as the segment classification network in Section 3.3, and object
detection scheme in Section 3.4.
3.2 Memory Graph
A memory graph is a directed graph G = (N , Eln, Ern, Elp , Erp),
where:
• N is a node set, and each n ∈ N represents a segment of
contiguous memory bytes between two pointer fields.
• Eln is an edge set, and each e ∈ E represents a directed edge
from ni to nj, and ni is left neighbor of nj.
• Ern is an edge set, and each e ∈ E represents a directed edge
from ni to nj, and ni is right neighbor of nj.
• Elp is an edge set, and each e ∈ E represents a directed
edge from ni to nj, and ni is pointed by a pointer on the left
boundary of nj.
• Erp is an edge set, and each e ∈ E represents a directed edge
from ni to nj, and ni is pointed by a pointer on the right
boundary of nj.
In other words, a memory graph is a directed graph with four sets
of edges, which capture both the adjacency and points-to relations
of memory segments, on both left-hand-side and right-hand-side
of each segment.
Figure 2 illustrates an example of how to construct a memory
graph from raw memory. Figure 2(a) shows a part of raw memory,
in which three pointer fields split this part of memory into four
segments: A, B, C, and D, each of which may have one or more con-
tiguous memory bytes. As a result, A, B, C, and D become vertices
in the corresponding memory graph. These vertices are connected
by four kinds of edges. For instance, since A is the left neighbor
ln−→ B. Conversely, since B is the right neighbor
of B, we have A
rn−−→ A. Moreover, since the pointer field left to C
of A, we have B
points to D, and the pointer field right to C points to A, we then
rp−−→ C. Note that these two edges are reverse
have D
to the actual points-to directions. This is because an edge in the
memory graph represents an information flow. For instance, the
pointer field left to C points to D, which means determining D’s
label can help label C. Therefore, from the information flow point
of view, there is an edge from D to C.
lp−→ C and A
A
lp−→ D.
rp−−→ C, B
A special case is that there are multiple consecutive pointers. As-
sume there are two consecutive pointers between C and D, pointing
rp−−→ C,
to A and B respectively, we then create four edges A
lp−→ D and B
A careful reader might suggest adding edges for the points-to
directions as well. For instance, the pointer field left to C points to D,
and it might make sense to have C → D, because identifying C also
helps to identify D. We choose not to do so, because an adversary
can easily create a pointer in an arbitrary address outside of a kernel
object and make it point to the object, then the topology of the
object in memory graph is changed if we add edges for point-to
directions. This will adversely affect the detection. On the other
hand, compared to the above case, it is more difficult to create a
fake pointer or manipulate an existing pointer within a legitimate
object that he/she tries to hide, without causing system crashes or
other issues.
3.3 Graph Neural Network Model
The GNN (Graph Neural Network) model will accept the memory
graph generated in Section 3.2 as input, and then output the labels
of all nodes in the graph. The goal of GNN model is to first extract
𝑨𝑩𝑪𝑫ABCD𝑨𝑩𝑪𝑫ABCDrnrnrnlnlnlnrp/lprplplprp3.3.1 Embedding Network. For each node n in the memory graph
G, the embedding network ϕw1 integrates input vector vn and the
topological information from its neighbors, both adjacent neighbors
and point-to neighbors, into a single embedding vector µn.
Inspired by Scarselli et al. [30], we implement the embedding
vector as a state vector that gradually absorbs information propa-
gated from multiple sources over time. To add a time variable into
embedding vector computation, we transform Equation (2) into
Equation (4). The total iterations needed to calculate the embed-
ding vector is denoted as T . The embedding vector of time t + 1
depends on the neighbor embedding vectors at time t, as shown in
Figure 3.
µn(t + 1) = ϕw1(vn, µEln[n](t), µEr n[n](t),
µElp[n](t), µEr p[n](t))
(4)
For each node n, the embedding network collects the information
about neighbor nodes in a BFS (Breadth First Search) fashion. In
each iteration, it traverses one layer of neighbor nodes and inte-
grates the neighbors’ states into the state vector µn of node n. We
name the neighbors expanded in the first layer as 1-hop neighbors,
in the same way, the neighbors expanded in the k-th layer as k-hop
neighbors. In each layer expansion, we collect information from
four types of neighbors, which are left neighbor, right neighbor, left
pointer neighbor and right pointer neighbor. The more iterations
we run, the information of farther neighbors are collected into em-
bedding vector µn. At time t = T , µn(t) stores the information of
the node sequence n itself and the information of neighbor nodes
within T hops.
We implement embedding vector µn as Equation (5).
µn(t + 1) = tanh(W1 · vn + β(n, t))
β(n, t) =σ1( 
σ3( 
m∈Ept[n]
m∈Elp[n]
µm(t)) + σ2( 
µm(t)) + σ4( 
m∈Er n[n]
m∈Er p[n]
(5)
(6)
µm(t))+
µm(t))
The weight matrix W1 is the weight parameters of the node
content, which is a matrix of shape |µ| × d. Neighbor state weight
parameters are a set of weight matrices in multiple layered neural
networks. Note that there are four separate sets of weight matrices
for σ1, and σ2, and σ3, and σ4, such that the embeddings of different
kinds of neighbors are propagated differently. The architecture of
each σ network is a feed-forward neural network, each layer is a
fully connected layer with ReLU activation function. The pseudo
code of embedding network is shown in Algorithm 1.
All of the mentioned weight parameters of embedding network
are learned using supervised learning on a labeled training dataset.
Since the weights are learned jointly with the weights in the clas-
sifier network, we will leave the training details after introducing
the classifier network in the section below. The embedding vector
obtained in this section is just an intermediate representation of
the whole supervised training. To perform an end-to-end training
from raw bytes to labels, we need the classifier network to generate
the final node label for training.
Figure 3: Node embedding computation in each iteration t.
Information flows through Elp, Erp, Enl , Enr edges. Embed-
ding vector µn(t + 1) gets updated by input vector vn and its
neighbors’ embedding vectors at t.
a low-dimensional internal representation of nodes from raw bytes
of a memory dump, and then infer the properties of nodes. As
such, the GNN model should consist of two consecutive subtasks: a
representation learning task and an inference task.
We represent the GNN model as F. It consists of two jointly-
trained subnetworks. The first subnetwork is an embedding net-
work which is responsible for node representation abstraction. We
denote it as ϕw1. The second subnetwork is a classifier network,
which is responsible for node label inference. We denote it as ψw2.
The formal definition of F is defined as follows.
F = ψw2(ϕw1(·))
(1)
The input of the embedding network ϕw1 is a vector representa-
tion of a node, denoted as vn, and the output is embedding vector,
denoted as µn. The classifier network ψw2 takes the output of the
embedding network as input, and then output the node label, de-
noted as yn.
More specifically, let vn be a d-dimensional vector of node n
derived from its actual memory content, then the embedding vector
µn is computed as follows:
µn = ϕw1(vn, µEln[n], µEr n[n], µElp[n], µEr p[n])
(2)
In other words, each node’s embedding is computed from its
actual content and the embeddings of its four kinds of neighboring
nodes. We use a simple method to derive a d-dimensional vector
for each node: we treat each dimension as one memory byte. If this
memory segment is longer than d bytes, we truncate it and only
keep d bytes; if it is shorter than d bytes, we fill the remaining bytes
with 0.
Then the output vector yn is computed as follows.
(3)
In the following paragraphs, we will describe how embedding
yn = ψw2(µn)
network and classifier network are defined and how they work.
𝑣𝑛input vector𝜇𝑚1(𝑡)𝜇𝑚2(𝑡)𝜇𝑚𝑟(𝑡)…𝑊1++𝜎1𝑚𝑖∈𝐸𝑙𝑝[𝑛]𝑡𝑎𝑛ℎ𝜇𝑛(𝑡+1)embedding vector at tFCN with ReLU………𝜇𝑚𝑖(𝑡)…+𝜎2𝑚𝑖∈𝐸𝑟𝑝[𝑛]𝜇𝑚𝑖(𝑡)…+𝜎3𝑚𝑖∈𝐸𝑙𝑛[𝑛]𝜇𝑚𝑖(𝑡)…+𝜎4𝑚𝑖∈𝐸𝑟𝑛[𝑛]𝜎𝑖embedding vector at t+1Algorithm 1: Information Propagation Algorithm of Embed-
ding Network ϕw1
:Memory Graph G = (N , Eln, Ern, Elp , Erp), iteration
Input
time T
Output:Graph Embedding µn for all n ∈ N
1 Initialize µn(0) = 0, for each n ∈ N
2 for t = 1 to T do
for n ∈ N do
3
4
5
6
7
8
9
10 end
end
β = σ1(
β+ = σ2(
β+ = σ3(
β+ = σ4(
m∈Er n[n] µm(t − 1))
m∈Eln[n] µm(t − 1))
m∈Elp[n] µm(t − 1))
m∈Er p[n] µm(t − 1))
µn(t) = tanh(W1 · vn + β)
3.3.2 Classifier Network. Let l be a node label, and L be the set of
all node labels. Node classifier network is used to map embedding
vector to a node label: ψw2 : µn → l, where n ∈ N , l ∈ L.
In order to facilitate object detection that will be discussed in
Section 3.4, we choose to label each node as a 3-tuple of the object
type, offset and length. For example, a node with label T _16_24
means the node is part of a _ETHREAD object and it is located at
offset 16 from the beginning of the _ETHREAD object, the length
of it is 24 bytes. As illustrated in Figure 4, three nodes are labeled
as T _16_24, T _52_12 and T _84_28. These labels all agree upon a
single fact that a _ETHREAD object is located at the same address.
Similar labeling methods are adopted in the linguistics domain to
solve word segmentation tasks [40, 41]. In particular, they label
the characters at the start, in-between and at the end of a word, in
order to split words from streams of free texts.
Figure 4: Node Labeling of a _ETHREAD Object
An object type may have many node labels. However, some rare
and invariant node labels have low occurrences in type c. To get a
robust model, we should not fit these outliers node labels. Hence,
we just keep the node labels with high frequency in type c, denoted
as key node label set L(c). The node labeling method is described
in detail in experiment evaluation Section 4.2.
With node labels of each object type, we then build a multi-class
classifier to classify the nodes into one of the labels in that object
type. For example, there will be a _ETHREAD classifier, a _EPROCESS
classifier, etc. The node classifier takes an embedding vector µn as
input and produces a predicted node label as output. To implement
the classifier, we choose to use FCN (Fully Connected Network)
model that has multi-layered hidden neurons with ReLU activation