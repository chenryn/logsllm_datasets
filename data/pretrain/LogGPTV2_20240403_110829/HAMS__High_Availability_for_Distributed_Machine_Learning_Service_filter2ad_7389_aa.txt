title:HAMS: High Availability for Distributed Machine Learning Service
Graphs
author:Shixiong Zhao and
Xusheng Chen and
Cheng Wang and
Fanxin Li and
Qi Ji and
Heming Cui and
Cheng Li and
Sen Wang
2020 50th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)
HAMS: High Availability for Distributed Machine
Learning Service Graphs
Shixiong Zhao∗1, Xusheng Chen∗1, Cheng Wang∗, Fanxin Li∗, Ji Qi∗, Heming Cui∗2, Cheng Li†, Sen Wang‡
∗University of Hong Kong, †University of Science and Technology of China, ‡Huawei Technologies3
∗{sxzhao, xschen, cwang2, fxli, jqi, heming}@cs.hku.hk, †{chengli7}@ustc.edu.cn, ‡{wangsen31}@huawei.com
Abstract—Mission-critical
services often deploy multiple
Machine Learning (ML) models in a distributed graph manner,
where each model can be deployed on a distinct physical host.
Practical fault tolerance for such ML service graphs should
meet three crucial requirements: high availability (fast failover),
low normal case performance overhead, and global consistency
under non-determinism (e.g., threads in a GPU can do ﬂoating
point additions in a random order). Unfortunately, despite much
effort, existing fault tolerance systems, including those taking the
primary-backup approach or the checkpoint-replay approach,
cannot meet all these three requirements.
To tackle this problem, we present HAMS, which starts
from the primary-backup approach to replicate each stateful
ML model, and we leverage the causal
logging technique
from the checkpoint-replay approach to eliminate the notorious
stop-and-buffer delay in the primary-backup approach. Extensive
evaluation on 25 ML models and six ML services shows that: (1)
in normal case, HAMS achieved 0.5%-3.7% overhead on latency
compared with bare metal; (2) HAMS took 116.12ms-254.19ms
to recover one stateful model
in all services, 155.1X-1067.9X
faster than a relevant system Lineage Stash (LS); and (3) HAMS
recovered these services with global consistency even when the
GPU non-determinism exists, not supported by LS. HAMS’s code
is released on github.com/hku-systems/hams.
I. INTRODUCTION
Recent machine learning (ML) models are pervasively
deployed in mission-critical services (e.g., autopilot
[47]
and online stock prediction [74]). An ML service works as
a dataﬂow application with a directed service graph [10],
[34]. Each vertex in the graph represents an operator (i.e.,
an ML model) deployed on a distinct host
to harness
heterogeneous hardware resources; each edge represents
an ordered connection from an upstream operator
to a
downstream operator, and the upstream operator propagates
its outputs as a sequence of input requests to the downstream.
A client program sends a request to the service graph and
receives the ﬁnal output from the graph as a reply.
An ML operator can be stateless or stateful. A stateless
operator (e.g., inference with a VGG19 model [73]) processes
each request independently and does not keep any states; a
stateful operator (e.g., an online learned VGG19 model [71]
or a Recurrent Neural Network model [20]) holds an internal
state computed from previous requests that will affect the
processing of future requests. For instance, Figure 1 shows
the service graph for an online learned VGG19 model that
has two input sequences of training and prediction requests
1The ﬁrst two authors contributed equally.
2Heming Cui is the corresponding author.
3Theory Lab, 2012 Labs, Huawei Technologies, Co. Ltd., Hong Kong.
with non-deterministic interleaving. The VGG19 model
is
stateful because previous training requests updates its model
parameters that will affect future inference results.
Given that failures can happen at any time on any host,
fault
tolerance is crucial for mission-critical ML service
graphs. A practical fault tolerant system for an ML model
service graph should meet
requirements.
First,
the system must provide high availability with at
most sub-second recovery time to provide continuous and
unaffected services (e.g., an autopilot service should act with at
most sub-second delay [47], [60]). Second, the system should
incur low performance overhead in the normal case (no failure)
compared with a bare metal (not fault tolerant) system.
three essential
Third, when a stateful operator O’s state is recovered after
the system must recover O’s state with global
a failure,
consistency [70], [72], [85], [87]: as O’s state affects its
prediction output, if some of O’s outputs are already processed
by downstream stateful operators (or clients), O should not be
recovered to a state that generates conﬂicting outputs (e.g., an
output with same sequence number but a different value). For
instance, if an online learned VGG19 model (Figure 1) tells
its downstream operators and clients that the 34th image is a
truck, after a failover, its recovered state should not classify
this image to a different result.
Non-determinism is the major open challenge for ensuring
global consistency, as it has two sources in an ML service
graph. First (S1), an operator can receive non-deterministically
interleaved input sequences from multiple upstream operators.
Second (S2), GPU models’ processing of a request
is
inherently non-deterministic: the GPU scheduler can process
ﬂoating point additions of multiple GPU threads in a
non-deterministic order, and ﬂoating point additions are
non-associative and rounded [55].
Due to S2, after an operator recovers from a failure, even
given the same input sequence as before the failure (i.e., S1
is eliminated), the recovered operator can easily run into a
different (inconsistent) state, easily leading to disasters for
mission-critical services. We did an experiment
to reveal
the consequence of S2 during a failover triggered by us in
Figure 2: an online learned VGG19 image classiﬁcation model
generates conﬂicting outputs on processing the 34th inference
request, permanently corrupted the service logic. We inspected
the classiﬁcation conﬁdence tuple in the model’s output, and
this tuple changed from (truck:0.5953, cloud:0.5884)
before the failure to (truck:0.5921, cloud:0.5943) after
the failover. A practical fault tolerant system that meets the
third requirement should recover the model to a state that
978-1-7281-5809-9/20/$31.00 ©2020 IEEE
DOI 10.1109/DSN48063.2020.00036
184
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:27:08 UTC from IEEE Xplore.  Restrictions apply. 
Fig. 1: An image classiﬁcation service
with an online learned VGG19 model.
Grey models are stateless, and the blue
model (i.e., the online learned VGG19)
is stateful.
Fig. 2: Inconsistency caused by a failover using the checkpoint-replay approach for
an online-learned VGG19 model. Its state is represented as Vx.y: Vx.0 means the
state of the xth checkpoint, and Vx.y means its state after processing y training
requests since the xth checkpoint. Inference requests do not change VGG19’s state.
(S1). When failures occur, LS replays
generates a consistent classiﬁcation decision with the former
tuple’s, as this tuple has been passed to downstream operators
or clients. We will illustrate GPU non-determinism in §II-C.
A recent system Lineage Stash [85] (LS) checkpoints
the state of a stateful model periodically and uses causal
logging [4] to efﬁciently record the interleaving of input
sequences
the
interleaving from the latest checkpoint
state. However,
LS faces a paradox between the recovery time (the ﬁrst
requirement) and the normal case performance overhead (the
second requirement) depending on its checkpoint interval. By
default, LS sets its checkpoint
to a few minutes,
introducing a long recovery time of several minutes [85].
If LS reduces its checkpoint
to sub-second for
faster recovery, it incurs prohibitive latency slowdown (81%
in our evaluation) because all stateful operators must be
frequently stopped, checkpointed, and resumed. Moreover,
LS, as well as other checkpoint-replay based fault tolerant
systems [1], [6], [21], [50]–[52], assume S2 does not occur,
but non-determinism is inherent in GPU computing (§II-C).
In short, the checkpoint-replay approach can meet either the
ﬁrst or the second requirement, but not the third one.
interval
interval
The primary-backup replication approach (e.g., Remus [13])
can meet the ﬁrst and the third requirement (i.e., handling both
S1 and S2). It lets a primary execute a batch of requests, and
propagate its updated state to a standby backup, so that the
backup can take over immediately if the primary fails, meeting
the ﬁrst requirement. To meet the third requirement (global
consistency), the primary buffers its outputs until the updated
state is delivered to the backup. However, the primary-backup
approach does not meet the second requirement due to two
reasons. First, a primary of a stateful operator needs to be
stopped to copy its updated state after processing every batch
of requests. Second, the primary needs to buffer its outputs,
which are demanded by its downstream operators, until the
updated state is delivered to its backup. When multiple stateful
operators are deployed in a service graph, each client request
will be stop-and-buffered multiple times in the graph. We will
illustrate the primary-backup approach in §III-B.
We present HAMS (Highly Available Machine-learning
Services), the ﬁrst ML service system that meets all the three
crucial requirements. HAMS presents an ML-context-aware
Non-Stop Primary-Backup protocol (NSPB) to eliminate the
stop-and-buffer delay for an ML service graph. Our key
observation is that the computation of a typical stateful ML
operator can be divided into two phases: computation phase
and update phase (§II-B). A stateful operator reads only its
internal state in the computation phase and updates the state
in the update phase. HAMS provides simple API for the ML
model developers to explicitly identify the two phases, so that
HAMS can asynchronously retrieve a model’s updated state
during the computation phase without stopping the operator.
Moreover, NSPB enables the primary to release its outputs
to downstream operators without waiting the state to be
delivered to the backup. Our idea is to let downstream
operators speculatively execute these outputs, in parallel with
the primary’s delivery of its state to its backup. NSPB just
maintains the causal dependency of per-batch state across the
upstream and downstream operators. If any host (primary or
backup) of any stateful operator fails, HAMS can maintain the
leftover hosts’ global consistency.
We
implemented HAMS with about 11K LoC on
Clipper [11] and Tensorﬂow Serving [58]. We evaluated
HAMS with 25 mature ML models on PyTorch [65] and
used these operators to build six practical ML services. We
compared HAMS with a bare metal system, a HAMS-Remus
system that follows Remus’s primary-backup protocol, and an
implementation of the most relevant replay-based approach
LS [85], as LS is not open-source. Evaluation shows that:
• HAMS was efﬁcient. HAMS’s end-to-end latency achieved
0.5% to 3.7% overhead compared with the bare metal
system, and this overhead is comparable to LS’s.
• HAMS took 116.12ms-254.19ms to recover one stateful
model in all services, 155.1X-1067.9X faster than LS.
• HAMS correctly recovered failed stateful ML operators
in a service graph, in the presence of S1 and S2.
The major contribution of this paper is HAMS, the ﬁrst
efﬁcient and high available ML service graph system and
the NSPB protocol. NSPB is a novel
integration of the
185
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:27:08 UTC from IEEE Xplore.  Restrictions apply. 
primary-backup approach with the causal logging technique
from the checkpoint-replay approach. HAMS’s NPSB protocol
can be integrated into diverse ML serving systems (e.g.,
Clipper [11], Tensorﬂow Serving [58], and Ray [50]) and
improve their reliability.
In the remaining of this paper, §II introduces background.
§III gives an overview of HAMS. §IV presents NSPB. §V
describes HAMS’s implementation. §VI shows our evaluation.
§VII discusses related work and §VIII concludes.
II. BACKGROUND AND MOTIVATION
A. Machine Learning
Among all ML algorithms, we mainly focus on deep neural
network algorithms [8], [26], [68], [73] in this paper as they
are most widely deployed. The life-cycle of an ML model
can be divided into two phases: a training phase and an
inference phase. The training phase passes an input dataset
multiple times (a.k.a., epochs), where each epoch works
with four steps: (1) a portion (batch) of the input dataset
forward-propagates through the model in parallel (forward
propagation stage); (2) the model computes a loss for each
input data (loss computation); (3) losses of the batch of inputs
backward-propagate through the model in parallel with each
leading to a gradient (backward propagation stage); and (4)
the model is updated according to the sum of all gradients. In
the inference phase, new (batched) inputs forward-propagate
through the model to generate prediction results.
An ML model (or operator, interchangeable in this paper)
can involve only the inference phase (model serving) or
both the training and the inference phases (online learning).
For model serving, a pre-trained model is deployed to give
predictions on input requests. For online learning, the model
is continuously re-trained with real-world data to ﬁne-tune the
model and to meet real-world trends, and serves inference
requests at the same time. HAMS supports both deployment
scenarios. As an ML model typically focuses on doing a single
job (e.g., audio transcribing or face recognition), and different
models are developed by different people, an ML service needs
to compose multiple models in a service graph [10], [34].
B. ML Services Can Be Stateful
Stateful ML services have two major categories: stateful
inference and stateful online learning. In stateful inference, an
operator typically contains a Recurrent Neural Network (e.g.,
LSTM [20]) to capture dependencies in the input sequence
(e.g., speech translation) as the model’s state that will affect
future predictions. In stateful online learning,
the model’s
parameters are continuously updated and constitute its state.
For both categories, processing a (batch of) request can
be divided into two stages: computation and update. For
stateful inference, we take LSTM with a forget gate [20] as
an example. When a new input request arrives, an LSTM cell
ﬁrst computes the forget gate’s activation tensor, the update
gate’s activation tensor, and the output gate’s activation tensor.
All these three computations read only the hidden state tensor
(computation stage). After that, the cell state tensor is updated
according to previous computation results, the hidden state
tensor is updated according to both the computation results
and the updated cell state tensor (update stage). For stateful
online learning, only training requests update the model’s state
(i.e., model parameters). In the four steps of the training phase
(§II-A), the model’s parameters are read-only in the ﬁrst three
steps and are only updated in the last step.
C. ML Can Be Non-deterministic
For
instance,
ML mainly does parallel computation (e.g., convolution)
on matrices of ﬂoating points. Floating point additions are
inherently not associative on GPU. Speciﬁcally, a + b + c is
usually not equal to a+(b+c) because ﬂoating point numbers
are rounded [55], which will accumulate in stateful models.
This makes ML computation non-deterministic: the scheduler
in a general GPU usually non-deterministically schedules the
parallel additions (e.g., AtomicAdd() in CuDNN) of ﬂoating
point tensors and produces different results, even given the
same inputs, same hyperparameters, and same random seeds.
three back propagation algorithms (e.g.,
CUDNN_CONVOLUTION_BWD_FILTER_ALGO_0 [54]) in CuDNN
are causing non-determinism due to the non associative