our techniques.
Others have also recently recognized the need for more
scalable layer 2 networks. SmartBridge [25] extended the
original pioneering work on learning bridges [23] to move
beyond single spanning tree networks while maintaining the
loop free property of extended LANs. However, Smart-
Bridge still suﬀers from the scalability challenges character-
istic of Ethernet networks. Contemporaneous to our work,
MOOSE [27] also suggests the use of hierarchical Ethernet
addresses and header rewriting to address some of Ether-
net’s scalability limitations.
RBridges and TRILL [24], its IETF standardization ef-
fort, address some of the routing challenges in Ethernet.
RBridges run a layer 2 routing protocol among switches.
Essentially switches broadcast information about their lo-
cal connectivity along with the identity of all directly con-
nected end hosts. Thus, all switches learn the switch topol-
ogy and the location of all hosts. To limit forwarding table
size, ingress switches map destination MAC addresses to the
appropriate egress switch (based on global knowledge) and
encapsulate the packet in an outer MAC header with the
egress switch identiﬁer.
In addition, RBridges add a sec-
ondary header with a TTL ﬁeld to protect against loops.
We also take inspiration from CMU Ethernet [22], which
also proposed maintaining a distributed directory of all host
information. Relative to both approaches, PortLand is able
to achieve improved fault tolerance and eﬃciency by lever-
41Figure 1: Sample fat tree topology.
aging knowledge about the baseline topology and avoiding
broadcast-based routing protocols altogether.
Failure Carrying Packets (FCP) [17] shows the beneﬁts
of assuming some knowledge of baseline topology in routing
protocols. Packets are marked with the identity of all failed
links encountered between source and destination, enabling
routers to calculate new forwarding paths based on the fail-
ures encountered thus far. Similar to PortLand, FCP shows
the beneﬁts of assuming knowledge of baseline topology to
improve scalability and fault tolerance. For example, FCP
demonstrates improved routing convergence with fewer net-
work messages and lesser state.
To reduce the state and communication overhead associ-
ated with routing in large-scale networks, recent work [8,
9, 10] explores using DHTs to perform forwarding on ﬂat
labels. We achieve similar beneﬁts in per-switch state over-
head with lower network overhead and the potential for im-
proved fault tolerance and eﬃciency, both in forwarding and
routing, by once again leveraging knowledge of the baseline
topology.
3. DESIGN
The goal of PortLand is to deliver scalable layer 2 rout-
ing, forwarding, and addressing for data center network en-
vironments. We leverage the observation that in data center
environments, the baseline multi-rooted network topology is
known and relatively ﬁxed. Building and maintaining data
centers with tens of thousands of compute elements requires
modularity, advance planning, and minimal human interac-
tion. Thus, the baseline data center topology is unlikely
to evolve quickly. When expansion does occur to the net-
work, it typically involves adding more “leaves” (e.g., rows
of servers) to the multi-rooted tree topology described in
Section 2.1.
3.1 Fabric Manager
PortLand employs a logically centralized fabric manager
that maintains soft state about network conﬁguration in-
formation such as topology. The fabric manager is a user
process running on a dedicated machine responsible for as-
sisting with ARP resolution, fault tolerance, and multicast
as further described below. The fabric manager may simply
be a redundantly-connected host in the larger topology or it
may run on a separate control network.
There is an inherent trade oﬀ between protocol simplicity
and system robustness when considering a distributed versus
centralized realization for particular functionality. In Port-
Land, we restrict the amount of centralized knowledge and
limit it to soft state. In this manner, we eliminate the need
for any administrator conﬁguration of the fabric manager
(e.g., number of switches, their location, their identiﬁer).
In deployment, we expect the fabric manager to be repli-
cated with a primary asynchronously updating state on one
or more backups. Strict consistency among replicas is not
necessary as the fabric manager maintains no hard state.
Our approach takes inspiration from other recent large-
scale infrastructure deployments. For example, modern stor-
age [13] and data processing systems [12] employ a central-
ized controller at the scale of tens of thousands of machines.
In another setting, the Route Control Platform [7] considers
centralized routing in ISP deployments. All the same, the
protocols described in this paper are amenable to distributed
realizations if the tradeoﬀs in a particular deployment envi-
ronment tip against a central fabric manager.
3.2 Positional Pseudo MAC Addresses
The basis for eﬃcient forwarding and routing as well as
VM migration in our design is hierarchical Pseudo MAC
(PMAC) addresses. PortLand assigns a unique PMAC ad-
dress to each end host. The PMAC encodes the location of
an end host in the topology. For example, all end points
in the same pod will have the same preﬁx in their assigned
PMAC. The end hosts remain unmodiﬁed, believing that
they maintain their actual MAC (AMAC) addresses. Hosts
performing ARP requests receive the PMAC of the destina-
tion host. All packet forwarding proceeds based on PMAC
addresses, enabling very small forwarding tables. Egress
switches perform PMAC to AMAC header rewriting to
maintain the illusion of unmodiﬁed MAC addresses at the
destination host.
PortLand edge switches learn a unique pod number and
a unique position number within each pod. We employ the
Location Discovery Protocol (Section 3.4) to assign these
values. For all directly connected hosts, edge switches as-
sign a 48-bit PMAC of the form pod.position.port.vmid to
all directly connected hosts, where pod (16 bits) reﬂects the
pod number of the edge switch, position (8 bits) is its posi-
tion in the pod, and port (8 bits) is the switch-local view of
Pod 2Pod 0Pod 1Pod 3EdgeAggregationCore42the port number the host is connected to. We use vmid (16
bits) to multiplex multiple virtual machines on the same
physical machine (or physical hosts on the other side of
a bridge). Edge switches assign monotonically increasing
vmid’s to each subsequent new MAC address observed on a
given port. PortLand times out vmid’s without any traﬃc
and reuses them.
Figure 2: Actual MAC to Pseudo MAC mapping.
When an ingress switch sees a source MAC address never
observed before, the packet is vectored to the switch soft-
ware. The software creates an entry in a local PMAC table
mapping the host’s AMAC and IP address to its PMAC.
The switch constructs the PMAC as described above and
communicates this mapping to the fabric manager as de-
picted in Figure 2. The fabric manager uses this state to
respond to ARP requests (Section 3.3). The switch also cre-
ates the appropriate ﬂow table entry to rewrite the PMAC
destination address to the AMAC for any traﬃc destined to
the host.
In essence, we separate host location from host identi-
ﬁer [20] in a manner that is transparent to end hosts and
compatible with existing commodity switch hardware. Im-
portantly, we do not introduce additional protocol headers.
From the underlying hardware, we require ﬂow table en-
tries to perform deterministic PMAC ↔ AMAC rewriting
as directed by the switch software. We also populate switch
forwarding entries based on longest preﬁx match against a
destination PMAC address. OpenFlow [4] supports both
operations and native hardware support is also available in
commodity switches [2].
3.3 Proxy-based ARP
Ethernet by default broadcasts ARPs to all hosts in the
same layer 2 domain. We leverage the fabric manager to
reduce broadcast overhead in the common case, as depicted
in Figure 3. In step 1, an edge switch intercepts an ARP
request for an IP to MAC address mapping and forwards the
request to the fabric manager in step 2. The fabric manager
consults its PMAC table to see if an entry is available for
the target IP address. If so, it returns the PMAC in step 3
to the edge switch. The edge switch creates an ARP reply
in step 4 and returns it to the original host.
It is possible that the fabric manager does not have the IP
to PMAC mapping available, for example after failure. In
Figure 3: Proxy ARP.
this case, the fabric manager will fall back to broadcast to
all end hosts to retrieve the mapping. Eﬃcient broadcast is
straightforward in the failure-free case (fault-tolerance ex-
tensions are described below): the ARP is transmitted to
any core switch, which in turn distributes it to all pods and
ﬁnally all edge switches. The target host will reply with its
AMAC, which will be rewritten by the ingress switch to the
appropriate PMAC before forwarding to both the querying
host and the fabric manager.
Note that end hosts receive PMACs in response to an ARP
request and that all packet forwarding proceeds based on
the hierarchical PMAC. The egress switch performs PMAC
to AMAC rewriting only on the last hop to the destina-
tion host.
In the baseline, forwarding in each switch re-
quires just O(k) state using hierarchical PMAC addresses.
This required state compares favorably to standard layer 2
switches that require an entry for every ﬂat MAC address in
the network, i.e., tens or even hundreds of thousands in large
deployments. Additional forwarding state may be required
to perform per-ﬂow load balancing across multiple paths [6].
There is one additional detail for supporting VM migra-
tion. Upon completing migration from one physical machine
to another, the VM sends a gratuitous ARP with its new IP
to MAC address mapping. This ARP is forwarded to the
fabric manager in the normal manner. Unfortunately, any
hosts communicating with the migrated VM will maintain
that host’s previous PMAC in their ARP cache and will be
unable to continue communication until their ARP cache
entry times out. To address this limitation, the fabric man-
ager forwards an invalidation message to the migrated VM’s
previous switch. This message sets up a ﬂow table entry to
trap handling of subsequent packets destined to the invali-
dated PMAC to the switch software. The switch software
transmits a unicast gratuitous ARP back to any transmit-
ting host to set the new PMAC address in that host’s ARP
cache. The invalidating switch may optionally transmit the
packet to the actual destination to prevent packet loss.
3.4 Distributed Location Discovery
PortLand switches use their position in the global topol-
ogy to perform more eﬃcient forwarding and routing using
only pairwise communication. Switch position may be set
manually with administrator intervention, violating some of
our original goals. Since position values should be slow to
Fabric Manager10.5.1.200:19:B9:FA:88:E2PMACIP00:00:01:02:00:0110.5.1.212b2a3AMACIP00:19:B9:FA:88:E210.5.1.2PMAC00:00:01:02:00:01MaskFabric ManagerPMACIPAddress10.2.4.500:19:B9:F8:E9:B4HWTypeHWAddressIFace00:02:00:02:00:0110.2.4.5ethereth100:02:00:02:00:0100:00:01:02:00:010800452145300:00:01:02:00:0110.5.1.200:02:00:02:00:0110.2.4.543For each tp in tentative pos
tentative pos ← tentative pos − {tp};
N eighbors ← N eighbors(cid:83){switch that sent P}
(cid:66) Case 1: On receipt of LDM P
If (curr time − start time > T and |N eighbors| ≤ k
2 )
If (curr time − tp.time) > timeout
Else If (P.dir = down)
If (my level = −1 and |N eighbors| = k)
my level ← 0; incoming port ← up;
Acquire position thread();
If (P.level = 0 and P.dir = up)
my level ← 1; incoming port ← down;
incoming port ← up;
is core ← true;
For each switch in N eighbors
is core ← false; break;
Algorithm 1 LDP listener thread()
1: While (true)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
If (P.pos (cid:54)= −1 and P.pos (cid:42) P os used)
If (P.pod (cid:54)= −1 and my level (cid:54)= 2)
P os used ← P os used (cid:83) {P.pos};
(cid:66) Case 2: On receipt of position proposal P
If (P.proposal (cid:42) (P os used(cid:83) tentative pos))
reply ← {“No”, P os used, tentative pos};
tentative pos ← tentative pos(cid:83){P.proposal};
If (is core = true)
my level ← 2; Set dir of all ports to down;
If (switch.level (cid:54)= 1 or switch.dir (cid:54)= −1)
my pod ← P.pod;
reply ← {“Yes”};
Else
Algorithm 2 Acquire position thread()
1: taken pos = {};
2: While (my pos = −1)
3:
4:
5:
6:
7:
8:
9:
10:
proposal ← random()% k
Send proposal on all upward facing ports
Sleep(T );
If (more than k
my pos = proposal;
If (my pos = 0)
Update taken pos according to replies;
4 + 1 switches conﬁrm proposal)
2 , s.t. proposal (cid:42) taken pos
my pod = Request from F abric M anager;
change, this may still be a viable option. However, to ex-
plore the limits to which PortLand switches may be entirely
plug-and-play, we also present a location discovery protocol
(LDP) that requires no administrator conﬁguration. Port-
Land switches do not begin packet forwarding until their
location is established.
PortLand switches periodically send a Location Discovery
Message (LDM) out all of their ports both, to set their posi-
tions and to monitor liveness in steady state. LDMs contain
the following information:
• Switch identiﬁer (switch id): a globally unique identi-
ﬁer for each switch, e.g., the lowest MAC address of
all local ports.
• Pod number (pod): a number shared by all switches
in the same pod (see Figure 1). Switches in diﬀerent
pods will have diﬀerent pod numbers. This value is
never set for core switches.
• Position (pos): a number assigned to each edge switch,
unique within each pod.
• Tree level (level): 0, 1, or 2 depending on whether the
switch is an edge, aggregation, or core switch. Our
approach generalizes to deeper hierarchies.
• Up/down (dir): Up/down is a bit which indicates
whether a switch port is facing downward or upward
in the multi-rooted tree.
Initially, all values other than the switch identiﬁer and
port number are unknown and we assume the fat tree topol-
ogy depicted in Figure 1. However, LDP also generalizes
to multi-rooted trees as well as partially connected fat trees.
We assume all switch ports are in one of three states: discon-
nected, connected to an end host, or connected to another
switch.
The key insight behind LDP is that edge switches receive
LDMs only on the ports connected to aggregation switches
(end hosts do not generate LDMs). We use this observation
to bootstrap level assignment in LDP. Edge switches learn
their level by determining that some fraction of their ports
are host connected. Level assignment then ﬂows up the tree.
Aggregations switches set their level once they learn that
some of their ports are connected to edge switches. Finally,