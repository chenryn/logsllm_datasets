off-line FIT and on-line FIT.
6.1 Off-line FIT
The off-line FIT must be scheduled to periodically run to check
for the problems. The most well-known tool of this kind is Trip-
wire: Tripwire relies on a base-line database of ﬁle-system check-
sum, which is generated when it is ﬁrst installed. At run-time, Trip-
wire veriﬁes the current system and compares with the checksum
in the database, then reports the violations according to the pre-
deﬁned security policy. Other kinds of FIT more or less follow the
scheme of Tripwire are AIDE, Samhain and Osiris.
Unfortunately all of these softwares suffers from 6 problems of
FIT we discussed above. In attempt to address their weaknesses,
they try to propose and implement special methods as followings:
• All the off-line FIT is periodically run to inspect the mali-
cious changes to the ﬁle-system, so they all have the “delay
detection” trouble (problem 1). This is the designed scheme,
so there is no perfect solution for it. To mitigate the problem,
Samhain suggests to run in daemon mode to minimize the
security window between run-times.
• To avoid the suspect of the attacker (especially in the case he
has local access), Samhain put in a lot of effort to hide its
presence. Samhain proposed few weird methods to hide its
presence: the database can be append at the end of a JPEG
ﬁle, the conﬁguration ﬁle is steganographically hidden in a
postcript image ﬁle, while the binary tool (named samhain)
and boot script ﬁle-name are changed to hide from suspi-
cious eyes. Last but not least, Samhain even provides a tool
ports to wait for the unauthorized connections from outside. Be-
cause these malware do not modify any ﬁles on the ﬁle-system,
XenFIT cannot detect them. However, XenFIT is still able to spot
the intrusion if the attacker penetrates the system remotely and
modiﬁes a protected ﬁle.
5.2 Performance Evaluation
As XenFIT only intercepts some ﬁle-system related system-calls
to gather information, the impact on the system only focuses on the
ﬁle-system activities. Therefore our performance evaluation only
carries out on the ﬁle-system benchmarks.
To measure the performance penalty, we choose a classical bench-
mark: decompress the Linux kernel. The reason we take this bench-
mark because Linux kernel contains a lot of data, and the decom-
press process creates and removes a great number of ﬁles and di-
rectories: for example unzipping the kernel 2.6.16 generates more
than 27000 ﬁles and directories, including temporary data.
The benchmark decompresses Linux kernel 2.6.16 with the com-
mand “tar xjvf linux-2.6.16.tar.bz2”. We run this test 10 times on
the native kernel and the kernel with XenFIT installed, then gets
the average results. The conﬁguration of the domains in the bench-
marks are as below:
Dom0: Memory: 384MB RAM, CPU: Pentium3 600MHZ, IDE
HDD: 40GB, NIC: 100Mbps
DomU: Memory: 128MB RAM, ﬁle-backed swap partition: 512MB,
ﬁle-backed root partition: 2GB
All the domains in the tests run Linux Ubuntu distribution (ver-
sion Breezy Badger), with the latest updates.
Table 2 shows the result of the benchmarks - all the numbers are
in seconds:
Native VM VM protected by XenFIT
1m57.561s
1m23.760s
0m15.382s
11m50.680s
9m35.540s
1m43.470s
real
user
sys
Table 2: Measurements on unzip the Linux kernel
We can see that the kernel overhead costs around 5.8 times more
than the native kernel, and the overall overhead is around 607%.
XenFIT’s high performance impact can be explained by a lot of
hyperswitchs it must execute when handling breakpoints.
6. RELATED WORKS
Our work is a combination of two areas: dynamically monitoring
system and HIDS. Our work is inspired by the work of K.Arigos
et.al in [2], but while this paper proposed to use breakpoints for
honeypot purpose with Xen, but we exploit the idea for a HIDS.
The way we handle debugging events is also completely different
from [2]: their authors pushed the security policy into the Xen layer
(via an add-in hypercall) and let the hypervisor analyzes the policy
there. To do that, the authors made quite a big modiﬁcation to Xen
layer (around 2700 lines of code), which they also mentioned it
as a TCB. We would argue that it is not desired to make such a
major change to such an important component, because it makes
the whole system less stable, as well as increase the maintenance
cost. In our solution, XenFIT makes absolutely no modiﬁcation to
the hypervisor as well as to the protected DomU.
Besides, in [2] all the security policy analyzes are done inside the
hypervisor. Thus we suspect that some modiﬁcation to the policy or
security engine would require to modify and recompile the hyper-
visor and the system must be reboot for the change to take effect.
Meanwhile, our solution puts all the policy and decision making
201
ploys the client/server architecture, in which the client collects and
send the system-call audit events to the server, and the sever carries
out the analyzing process. Since the event log contains a great deal
of information about the system activities, SNARE is also proposed
to be a HIDS solution.
Unfortunately, we are not convinced that SNARE is suitable as
the integrity detection tool, because of some following drawbacks:
• SNARE is not designed to evade the attacker: SNARE is
vulnerable to problem (6) discussed above. For example on
Linux system, the attacker could detect SNARE by looking
for the agent process (named auditd), or checks if the ﬁle
/proc/audit is existed. In addition, SNARE can also be shut-
down by the attacker if he gains the root access: in case the
agent process is killed, no more events can be reported to the
server. This is the same problem experienced by the above
off-line FIT.
• SNARE is applied to the client as a kernel patch (not as a ker-
nel module). That might complicates the usage, because the
kernel must be recompiled, then rebooted to make SNARE
work, which is not always desired on a production system.
Another reason we do not prefer the solution is that SNARE is
proposed as a patch, and this may force the SNARE developers
to release separate patches for different kernel versions, especially
because the kernel API is likely changed any time. That demand
can signiﬁcantly slow down the support for the newer kernel. For
example at the time of this writing, the latest Linux kernel version
is 2.6.17, but the most updated patch of SNARE is only for the
kernel 2.6.11.7, and unsurprisingly this patch is not cleanly applied
on 2.6.17 kernel.
7. CONCLUSIONS
This paper proposes the design and implementation of XenFIT
solution to eliminate some problems of current ﬁle-system integrity
solutions. XenFIT introduces some unique features: real-time mon-
itoring, centralized policy, no need the base-line database (so the
administration overhead is much more mitigated), easy to deploy,
and highly tamper resistant. We propose to use XenFIT in a chain
tool with another log-ﬁle monitoring tools to notify about the in-
trusion in real-time. Moreover, if being installed in a strict manner,
XenFIT is stealthier, harder to detect even with privileged user. All
of these advantages make XenFIT a valuable HIDS solution.
As Xen will be available in the mainline Linux kernel very soon,
we believe that this solution will beneﬁt everybody. For the time
being, XenFIT only works for Linux-based virtual domains. We
plan to provide support for other Os-es such as FreeBSD, NetBSD
once these ports are working stably on Xen.
8. REFERENCES
[1] N. A.Kamble, J. Nakajima, and A. K.Mallick. Evolution in
kernel debugging using hardware virtualization with xen. In
Proceedings of the 2006 Ottawa Linux Symposium, Ottawa,
Canada, July 2006.
[2] K. Asrigo, L. Litty, , and D. Lie. Virtual machine-based
honeypot monitoring. In Proceedings of the 2nd
international conference on Virtual Execution Environments,
New York, NY, USA, June 2006. ACM Press.
[3] T. Atkins. SWATCH: The Simple WATCHer of Logﬁles.
http://swatch.sourceforge.net/, July 2004.
[4] CERT Coordination Center. CERT/CC Overview Incident
and Vulnerability Trends. Technical report, Carnegie Mellon
Software Engineering Institute, May 2003.
(samhain.pk) to pack the executable, so it is hard to reverse
engineer the binary ([21]).
• To avoid the possible compromise on the database and to im-
prove the scalability, Osiris and Samhain use the client/server
architecture, and save the database and policies on the server
(with the assumption that the server is secure). Osiris does
not store the scan data on the scanned host, the scanning re-
sult is instead sent back to the management host (server), and
the comparison is done there. In contrast Samhain pushes the
database to the client and veriﬁes the integrity there.
• To mitigate the risk of tampering the binary tool itself, Osiris
and Samhain advise to keep the scan agent software on read-
only media (CDROM), but it becomes cumbersome if the
administrator wants to upgrade the software. Other than that,
Samhain offers a couple of self-integrity checks including
signed data and conﬁguration ﬁles, a key compiled into the
executable, and the ability to hide itself from the system pro-
cess list. Osiris makes use of a run-time session key that acts
as a mean to authenticate to the management host as well as
to detect tampering.
• Recently Hal Pomeranz ([14]) suggested another method to
secure the database and the binary tool: put them on the
secure sever, and only download them and run the veriﬁed
process when needed on the client. Accordingly, the Trip-
wire/AIDE database and FIT are saved on the server, and
periodically pushed to the client to run via a secure channel
(SSH, speciﬁcally). The author recommends to makes the
whole operation stealth by rename the binary tool (thus the
FIT process name), so if the attacker is present in the system,
he is not aware of the checking process.
However, no proposed method can fool the skillful attacker. Sooner
or later he is able to ﬁgure out that the system is running a FIT, and
if he somehow gets the root access, he can shutdown the tool. After
that the attacker can do anything he wishes on the system without
anybody’s awareness.
One more noticeable problem of the client/server architecture
adopted by the advanced tools like Osiris and Samhain is:
the
server must exposed to the network, so it could be the target of
the intruder. The attacker might exploit the server remotely ﬁrst to
disable it, then comfortably penetrates the client.
6.2 On-line FIT
In contrast with off-line solution, an on-line FIT is able to mon-
itor and react on the security violations in real-time. The strat-
egy is to stay in the critical path of ﬁle-system activities, such as
open/read/write/rename/unlink,... All of these activities can dis-
close the integrity violation in real-time. However this method
comes with some trade-offs: to inspect the low-level activities, the
tool must usually work at OS kernel level, which requires to patch
the system kernel for it to work. This may lead to conﬂict with other
third-party patches approved by the same machine, especially be-
cause the kernel used by different OS vendors are quite different.
Moreover, not all the systems would allow to push new code into
their kernel.
One example of online FIT is I3FS ([13]): I3FS is a ﬁle-system
layer that intercepts and tracks down the related VFS functions, so
it can detect when a speciﬁc ﬁle is changed in real-time. Neverthe-
less, I3FS requires to change the kernel of the protected machine,
which is not always possible in production system.
Another popular on-line type tool is SNARE, a complete soft-
ware suite that can be used to monitor system events. SNARE em-
202
[24] Xen project. Xen interface manual. http:
//www.cl.cam.ac.uk/Research/SRG/netos/
xen/readmes/interface/interface.html,
August 2006.
[5] CMN. SAdoor: A non listening remote shell and execution
server.
http://cmn.listprojects.darklab.org/, 2002.
[6] B. Dragovic, K. Fraser, S. Hand, T. Harris, A. Ho, I. Pratt,
A. Warﬁeld, P. Barham, and R. Neugebauer. Xen and the art
of virtualization. In Proceedings of the ACM Symposium on
Operating Systems Principles, October 2003.
[7] DWARF Workgroup. DWARF Debugging Format Standard.
http://dwarf.freestandards.org/Home.php,
January 2006.
[8] R. Hock. Dica rootkit.
http://packetstormsecurity.nl/UNIX/
penetration/rootkits/dica.tgz, 2002.
[9] Intersect Alliance. System iNtrusion Analysis and Reporting
Environment. http://www.intersectalliance.
com/projects/Snare/, January 2005.
[10] G. H. Kim and E. H. Spafford. The Design and
Implementation of Tripwire: A File System Integrity
Checker. In ACM Conference on Computer and
Communications Security, pages 18–29, 1994.
[11] T. Miller. Analysis of the Knark rootkit.
www.ossec.net/rootkits/studies/knark.txt,
2001.
[12] T. Miller. Analysis of the T0rn rootkit.
http://www.sans.org/y2k/t0rn.htm, 2002.
[13] S. Patil, A. Kashyap, G. Sivathanu, and E. Zadok. I3FS: An
In-Kernel Integrity Checker and Intrusion Detection File
System. In Proceedings of the 18th USENIX Large
Installation System Administration Conference (LISA 2004),
pages 69–79, Atlanta, GA, November 2004.
[14] H. Pomeranz. File Integrity Assessment via SSH.
http://www.samag.com/documents/s=9950/
sam0602a/0602a.htm, February 2006.
[15] I. Pratt, K. Fraser, S. Hand, C. Limpach, A. Warﬁeld,
D. Magenheimer, J. Nakajima, and A. Mallick. Xen 3.0 and
the art of virtualization. In Proceedings of the 2005 Ottawa
Linux Symposium, Ottawa, Canada, July 2005.
[16] sd. Linux on-the-ﬂy kernel patching.
http://www.phrack.org/show.php?p=58&a=7,
July 2002.
[17] SGI Inc. LKCD - Linux Kernel Crash Dump.
http://lkcd.sf.net, April 2006.
[18] L. Somer. Linux Rootkit 5.
http://packetstormsecurity.nl/UNIX/
penetration/rootkits/lrk5.src.tar.gz, 2000.
[19] The AIDE team. AIDE: Advanced Intrusion Detection
Environment.
http://sourceforge.net/projects/aide,
November 2005.
[20] The Osiris team. Osiris host integrity monitoring.
http://www.hostintegrity.com/osiris/,
September 2005.
[21] The Samhain Labs. Samhain manual. http:
//la-samhna.de/samhain/manual/index.html,
2004.
[22] The Samhain Labs. The SAMHAIN ﬁle integrity/intrusion
detection system. http://la-samhna.de/samhain/,
January 2006.
[23] The Snort team. Snort - the de-facto standard for intrusion
detection/prevention. http://www.snort.org, January
2006.
203