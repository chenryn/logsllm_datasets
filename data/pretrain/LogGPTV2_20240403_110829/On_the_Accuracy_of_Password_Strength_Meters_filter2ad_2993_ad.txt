offline guessing evaluation. For each of the three datasets (RockYou,
LinkedIn, 000Webhost) we sampled 10,000 passwords to obtain the
three different offline guessing datasets.
6.3 Selected Meters
from the literature.
Academic Proposals. We considered different meter proposals
• Heuristic/NIST: In 2004 the NIST published SP 800-63 Ver.
1.0 [11], which includes heuristics based on length and compliance
to a composition policy on entropy estimation. The heuristic also
considers a bonus if the password passes a common dictionary
check. The latest version, SP 800-63B [31] from June 2017, no longer
includes the ad-hoc heuristic.
• Markov Model/OMEN: In 2012 Castelluccia et al. [13] proposed
to train n-gram Markov models on the passwords of a service to pro-
vided accurate strength estimations. The estimation is thus based
on the probabilities of the n-grams a password is composed of. The
meter provides adaptive estimations based on a target distribution
but is limited to server-side implementations.
• Heuristic/Comp8: In 2012 Ur et al. [61] investigated how PSMs
can be used to nudge users towards stronger passwords. They
outlined a scoring algorithm derived from a composition policy
called “Comprehensive 8.” Due to the lack of better alternatives, this
scoring function was used to estimate the strength of a password.
While this LUDS-based approach should no longer be used, we
include it for completeness.
• Heuristics/zxcvbn: In 2012 Daniel Wheeler proposed a PSM in
a Dropbox Inc. blog post. It is based on advanced heuristics that
extend the LUDS approach by including dictionaries, considering
leetspeak transformations, keyboard walks, and more. Due to its
easy to integrate design, it is deployed on many websites. The meter
was recently backed up by scientific analysis [71].
• PCFG/fuzzyPSM: In 2012 Houshmand and Aggarwal [34] pro-
posed a system to analyze the strength of a password. For this, they
used a PCFG-based approach. In 2016 Wang et al. [65] extended the
concept by proposing a fuzzy PCFG to model password strength,
based on which mangling rules are required to modify a basic dic-
tionary to match a training distribution of stronger passwords.
• Heuristic/Eleven: In 2013 Egelman et al. [22] studied password
selection in the presence of PSMs. For their meter, they decided
to use a similar metric for strength as NIST. Similar to LUDS ap-
proaches the meter considers character set sizes and length.
• RNN/DD-PSM: In 2016 Melicher et al. [46] proposed to use a
recurrent neural network for probabilistic password modeling. For
our analysis, we use the guess number estimations provided by the
RNN. The authors also describe a method that allows a client-side
implementation using a special encoding and a Bloom filter. In 2017,
Ur et al. [59] extended the concept by adding data-driven feedback
using 21 heuristics that try to explain how to improve the password
choice. We use Ur’s website4 for additional measurements.
• Heuristic/LPSE: In 2018 Guo et al. [32] proposed a lightweight
client-side meter. It is based on cosine-length and password-edit
4Data-Driven PSM: https://cups.cs.cmu.edu/meter/, as of September 10, 2018
distance similarity. It transforms a password into a LUDS vector
and compares it to a standardized strong-password vector using the
aforementioned similarity measures.
Password Managers. We also tested meters that protect high
value encrypted password vaults. If no further protection mecha-
nism is deployed [27], especially the security of cloud-based pass-
word managers depend on the use of a high entropy secret. Thus,
service providers need to give accurate strength estimates. Further-
more, vaults that offer the ability to store user-chosen credentials
might show strength estimates for their stored secrets, too. For our
work, we analyzed the PSMs of 11 popular password managers,
including 1Password [2], Bitwarden [1], Dashlane [14], Enpass [55],
KeePass [51], Keeper [39], LastPass [43], and more.
Popular Websites. We queried password strength meters from
popular web services within the top 100 ranking published by Alexa
Internet. Our samples include large sites like Apple, Baidu, Drop-
box, Facebook, Google, Microsoft, reddit, Twitter, Sina Weibo, Yandex,
and more. For better comparability, we tried to include sites that
were queried in previous work by de Carné de Carnavalet and
Mannan [16]. The authors published their findings on all the meter
codes via a “Password Multi-Checker Tool” on a self-hosted web-
site [17], allowing one to compare their results with the currently
implemented versions.
Operating Systems. We analyzed password strength meters from
standard operating systems. Microsoft’s Windows and Apple’s
macOS do not provide a strength estimation during account cre-
ation. However, Apple includes a Password Assistant with a strength
estimation functionally that is used by the Keychain Access applica-
tion while generating passwords, e. g., for file encryption. Canoni-
cal’s Ubuntu distribution shows a PSM during the account creation
and hard disk encryption setup. It is part of the graphical live CD
installer Ubiquity and based on Mozilla’s Seamonkey PSM function.
6.4 Querying Meters
To query the website PSMs, we used similar techniques as previ-
ous work [16]. For JavaScript and server-side implementations, we
used the Selenium framework [35] to automate a headless Google
Chrome browser. As all JavaScript (that involves no server com-
munication) is evaluated on the client only, one can obtain large
quantities of strength estimates in a short period. The used browser
automation approach allows to execute JavaScript, thus intermedi-
ate results that are not displayed in the GUI of the meter, but exist
in the Document Object Model (DOM), are accessible, too.
For the academic proposals, a more evolved approach was re-
quired, as some meters require a training or no implementation
was available. For the training, we sampled 10 million passwords
from each sanitized dataset (excluding the respective offline and
online passwords). Please note, not all meters make full use of all
available training data, e. g., fuzzyPSM, OMEN, and the RNN-based
approach have specific requirements. For example, the Markov
model approach used by OMEN does not allow training passwords
shorter than the n-gram size. Similarly, fuzzyPSM does not require a
training corpus larger than 1 million passwords. For the RNN-based
approach we were forced to limit the training set to passwords no
longer than 30 characters.
Table 4: We computed the weighted Spearman correlation as the best similarity score (cf. Section 5). The table lists the online
use case on the left, the offline use case on the right. We highlighted if and on how many bins a meter quantized its output
and list whether a meter runs on client- or server-side.
Eleven [22]
LPSE [32]
ID Meter
1A Comprehensive8 [61]
2
3
4C Markov (Multi) [27]
5B NIST (w. Dict.) [11]
6
7C RNN Target [46]
8A zxcvbn (Guess Number) [71]
PCFG (fuzzyPSM) [65]
Type Quant. RockYou LinkedIn 000Webhost RockYou LinkedIn 000Webhost
Online Attacker
Offline Attacker
C
C
C
S
C
S
C
C
-
-
Q3
-
-
-
-
-
-0.652
0.670
0.584
0.721
0.669
1.000
0.951
0.989
-0.589
0.912
0.669
0.998
0.910
0.994
0.913
0.990
0.251
0.492
0.508
0.902
0.472
0.963
0.965
0.554
-0.476
0.755
0.544
0.997
0.756
0.998
0.896
0.989
-0.616
0.951
0.718
0.995
0.953
0.999
0.860
0.999
0.441
0.733
0.693
0.777
0.816
0.899
0.885
0.868
Type: C=Client, S=Server; Quantization: Q3–Q6=Number of bins, e. g., Q5=[Terrible, Weak, Good, Excellent, Fantastic];
For Comp8, LPSE, and fuzzyPSM we contacted the authors that
kindly shared their source code or evaluated their implementation
and shared the results with us. For the RNN PSM, we used an im-
plementation by Melicher et al. [45]. We tested multiple variants:
i) Based on the guess numbers of a pre-trained (generic) password
distribution. ii) Based on a client-side JavaScript implementation
(using a different password composition policy) [58]. iii) Based on
the guess numbers of a self-trained RNN using a matching distribu-
tion (targeted), following the recommend construction guidelines.
iv) Based on a self-trained RNN using a matching distribution (tar-
geted) including a Bloom filter made of the top 2 million training
set passwords (following the recommendations in the original pa-
per [46]). For the Markov approach, we modified a version of the
Ordered Markov ENumerator (OMEN) [3] by Dürmuth et al. [21]
(a password guesser implementing the approach of Castelluccia et
al. [13]) and used the aforementioned training set to obtain strength
estimates. As this approach uses quantized (level-based) strength es-
timates only, we also implemented an approach described by Golla
et al. [27] that outputs probabilities instead of quantized scores
and increases precision by training one model per password length.
For zxcvbn, we used the official JavaScript implementation [20].
For NIST we used a JavaScript implementation of the meter [15].
We built a dictionary consisting of the top 100,000 passwords from
Mark Burnett’s 10 million password list [10], which has been used
as blacklist by previous work [33].
For most of the password managers (1Password, Bitwarden,
Keeper, etc.), we were able to query their respective web interface
version using Selenium. For RoboForm and True Key we automated
the respective Chrome extensions using Selenium. For KeePass, we
used the KPScript plugin on Windows [52]. For Dashlane we used
the Appium framework [38] and its Windows Driver to automate
the Windows desktop. While analyzing Enpass’s PSM we found
the use of the official zxcvbn implementation [54] (including the
same dictionaries), thus we didn’t query Enpass. Instead, we report
the zxcvbn results.
For the operating systems, we queried Ubuntu’s PSM using the
original Python script from the Ubiquity source code [12]. For
macOS we used PyObjC [49], a Python Objective-C bridge to query
Apple’s Security Foundation framework.
7 RESULTS
Next, we present and discuss the results of the evaluation of the
different meters, both for the online and offline use case. Some of
the academic PSM results are summarized in Table 4. We present
the results for the online use case on the left, and for the offline
use case on the right. We report results for both use cases for all
strength meters, even though some are designed for one specific use
case only (e. g., password meters deployed on websites are intended
for the online use case). We computed the weighted Spearman
correlation as the best similarity score selected in Section 5.
The full table for all 81 password strength meter variations can
be found in the Appendix A. The primary results are separated into
four categories (academic proposals, password managers, operating
systems, and websites). A fifth category is included for compari-
son and is based on the “Password Multi-Checker Tool” [17] by
previous work [16]. A version of our results that allows an easier
comparison, provides bar charts of the quantizations, and more can
be found online [29]. Please note, not all tested mechanisms like
ID: 5A/B NIST or ID: 33 Have I Been Pwned? are intended to be used
as a strength meter. Thus, the reported results for those estimators
cannot be directly compared with others as their parameters can
likely be augmented to perform better.
7.1 Overall Performance
The three best-performing academic meters are ID: 6 fuzzyPSM
(0.899 − 1.000), ID: 7C RNN Target (0.860 − 0.965), and ID: 4C
Markov (Multi) (0.721 − 0.998) for both online and offline use cases.
A number of other PSM variants perform well, including ID: 8A
zxcvbn (Guess Number) (0.554 − 0.999).
For password managers we found ID: 13A KeePass and ID: 14B
Keeper to be accurately ranking meters (0.284 − 0.884). Further-
more, we found the zxcvbn-based ID: 17B RoboForm to be precise
(0.528 − 0.962). Across the binning PSMs we found some of the
zxcvbn (Score)-based meters, e. g., ID: 17A RoboForm (Q4), ID: 17C
RoboForm Business (Q6), ID: 18 True Key (Q5), and ID: 12 Enpass
(Q5) to be accurately ranking (0.341 − 0.827). The PSM in ID: 10A
Bitwarden shows significant problems. Similar, but less pronounced
are the inaccuracies of ID: 16B LogMeOnce and ID: 19A Zoho Vault.
All three are LUDS-based meters.
(a) ID: 13B (KeePass), wSpear: 0.002
(b) ID: 8B (zxcvbn, Score), wSpear: 0.567
(c) ID: 41B (Twitter), wSpear: 0.665
Figure 2: Number of passwords per bin: An accurate and correctly binning PSM produces a diagonal green line. A meter that
only assigns the weak/strong bin is visualized via a vertical bar on the left (purple)/right (red). The quantization degrades the
precision, if bin thresholds are incorrectly chosen (cf. Figure 2(a)), the relative ranking is lost and the correlation degrades.
When it comes to PSMs used by current operating systems, we
found very negative results. While macOS does not prominently
display their PSM, Ubuntu uses the PSM during account creation
and hard disk encryption. We found both meters to perform poorly.
An analysis of Ubuntu’s PSM source code revealed a LUDS meter.
Their approach counts the number of uppercase, digit, and symbol
characters and multiplies them with some magic constants. The
estimation function is a re-implementation of Mozilla’s Seamonkey
meter which dates back to 2006. First bug reports about the poor
quality and inconsistency in the assessment date back to 2012 [6].
While the weighted Spearman correlation decreases due to the ef-
fects of quantization (cf. Section 7.2), we observed a relatively good
accuracy for some of the website PSMs, too. For example the non-
binning ID: 35B Microsoft (v3), ID: 25A Best Buy, ID: 28A Drupal,
and ID: 41A Twitter PSMs (0.424− 0.951). Across the binning PSMs
we found some of the zxcvbn (Score)-based meters, e. g., ID: 36 reddit
(Q5) and ID: 40A Twitch (Q5) (0.197 − 0.817) and some non-zxcvbn-
based PSMs like ID: 41B Twitter (Q5), ID: 32A Google (Q5), and
ID: 35A Microsoft (v3) (Q4) to be accurately ranking (0.487− 0.763).
Based on our measurements ID: 33 Have I Been Pwned? performs
excellently. This is likely owed to the fact that all tested datasets
are part of the Pwned Passwords list [36]. Thus, different results are
expected if non-breached passwords are evaluated. Surprising are
the results of ID: 27A Dropbox (0.056 − 0.611), the developers of
the zxcvbn meter. On their website, they rely on a Q4 score-based
implementation (they discard the first bin, i. e., all passwords with
a guess number below 103). Based on our ID: 8B zxcvbn (Score)
findings, we expected better results. Note that the results of ID: 27B
Dropbox using an older implementation by previous work [16]
results in similar low performance.
To summarize, the academic contribution to strength estimation
is outperforming many other meters and has brought up several
concepts that are improving the estimations. Some other factors
may contribute to those meters performing well: Specifically for
the academic proposals we often have continuous scores, and the
meters are trained on the distribution.
The PSMs in password managers perform reasonably well, with
a few exceptions (ID: 10A Bitwarden, ID: 16B LogMeOnce, and
ID: 19A Zoho Vault). Similar to previous work, we measured a
good accuracy for ID: 13A KeePass. The high accuracy of ID: 17B
RoboForm and others are explained by the use of zxcvbn. PSMs in
operating systems are not popular, even though, when present, they
are used for security-sensitive operations. Current implementations
are LUDS-based meters that lack any helpful guidance and should
be replaced. Website-based PSMs are doing reasonably well, with
correlations up to 0.951. Most of our evaluated website PSMs are
client-side JavaScript meters, also popular are hybrids. Server-side
implementations were rare in our evaluation set. We speculate there
are several reasons that websites are not using better meters: lack
of awareness, lack of guidance on the quality of meters, and the
usually larger size of academic meters that need to store the model
parameters.
7.2 Effect of Quantization
Almost all PSMs on websites provide a binned output, as users rely
on tangible feedback like Weak or Strong instead of a more abstract
probability or guess number. Binning will reduce the accuracy of
a meter. However, weighted Spearman is relatively robust against
this effect. In the following, we qualitatively analyze the estimates
of binning meters to provide a more intuitive way to compare
weighted Spearman correlation with the over- and underestimates
of the meters. For this, we use the PGS [62] min_auto guess number,
and a logarithmic binning similar to zxcvbn (Score). The results are
visualized in Figure 2.
7.2.1 The ≥ 103 | V Bin. This bin includes passwords that are weak
(103 ≤ guess number < 106) but misjudged by the meter to be strong.
An analysis of ID: 41B Twitter’s bin revealed weakness in detecting
keyboard walks and leet transformations. The password !QAZxsw2
(a keyboard walk on US keyboards) as well as P@$$w0rd, jessica#1,
and password@123 were incorrectly ranked. ID: 8B zxcvbn (Score)’s
bin includes misdosamores and mardelplata (film and city names),
as well as oportunidades (common Spanish term).
7.2.2 The ≥ 1010 | II Bin. This bin includes passwords that are
strong but misjudged by the meter to be somewhat weak. ID: 41B
Twitter’s bin revealed problems with digit-only passwords like
9371161366 that are usually cracked using a Mask attack. Fur-
thermore, it includes all lowercase phrases like itsababydog that
attackers crack by running a Combinator attack using two dictio-
naries. ID: 8B zxcvbn (Score)’s bin includes zxcvbvcxz (a variation
of the meter’s name giving keyboard walk) usually cracked via
a dictionary or Mask attack. Additionally, we found phrases like
atlantasports, which is likely cracked with a Combinator attack.
To further study the binning effect for real-world data, we look
at those meters that provide both, quantized and non-quantized
feedback. Interesting is the case of ID: 13B KeePass (Q5). While the
IIIIIIIVVMinGuess Number≥ 1e103003192≥ 1e62141000≥ 1e816600000000≥ 1e326500000≥ 1e05250000Very WeakWeakModerateStrongVery StrongIIIIIIIVVMinGuess Number≥ 1e100156984≥ 1e6713286651057≥ 1e824349302623282712516≥ 1e32782093220563≥ 1e04824300001234IIIIIIIVVMinGuess Number≥ 1e100486988≥ 1e601095863189≥ 1e805497822824713611767≥ 1e3492021542317≥ 1e03291841200ObviousWeakGoodStrongVery Strong(a) Markov
(b) PCFG
(c) RNN
(d) zxcvbn
Figure 3: PSM scatter plots: Increasing password counts on the x-axis. Strong (less common) passwords are on the left, Weak
(more common) passwords are on the right. Estimated strength values (measured as probability/guess number) on the y-axis.
strength estimates are relatively precise ID: 13A KeePass (0.393 −
0.884), the binning had severe consequences. The PSM enforces very
strong passwords (i. e., “Very weak” for score < 64 bit) resulting
in the majority of passwords falling in the weakest bin. (KeePass
does not display this textual feedback in their password manager
software). In comparison, we see that for ID: 9A 1Password (0.276−
0.807) the binning of the strength estimates by ID: 9B 1Password
(Q5) (0.276−0.813) had close to no effect on the accuracy. There are
cases where binning improves the score, e. g., the unbinned version
ID: 10A Bitwarden (−0.635 − 0.676) performs substantially worse
than the binned version ID: 10B Bitwarden (Q3) (0.258−0.725). The
reason is that binning can eliminate some types of errors of a meter,
depending on the precise binning boundaries.
7.3 Performance Over Time
It is interesting and illustrative to compare our results with those
from de Carné de Carnavalet and Mannan [16], which were col-
lected in June/July 2013. By analyzing the matching set of websites,
one can observe positive and negative developments in the past
5 years. First of all, ID: 26A vs. 26B (China Railway) as well as
ID: 31B vs. 31C (Fedex) did not change at all. Second, some meters
most notably ID: 23A vs. 23C (Apple), as well as ID: 27A vs. 27B
(Dropbox), and ID: 28B vs. 28C (Drupal) show a degraded rank-
ing accuracy. When analyzing the Apple hybrid PSM, we found
server-side blacklisting of the entire 14 million RockYou passwords
in combination with a basic LUDS approach that checks for sym-
bols and length. Finally, we can report a positive development for
ID: 32A vs. 32B (Google), ID: 42B vs. 42C (Yandex), and ID: 41B vs.
41C (Twitter). Note that Twitter changed its quantization over time;
thus results are not directly comparable. If we consider all websites
in our evaluation set, one observes slightly better performing PSMs
than in the complete set of 2013, but no significant change. One