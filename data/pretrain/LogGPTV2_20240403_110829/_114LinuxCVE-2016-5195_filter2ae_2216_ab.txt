            inc_mm_counter_fast(vma->vm_mm, mm_counter_file(page));
            page_add_file_rmap(page);
        }
        set_pte_at(vma->vm_mm, address, pte, entry);
    }
    static inline pte_t maybe_mkwrite(pte_t pte, struct vm_area_struct *vma)
    {
        if (likely(vma->vm_flags & VM_WRITE)) /* 因为是只读的，所以pte不带_PAGE_RW标记 */
            pte = pte_mkwrite(pte);
        return pte;
    }
到这里第一次查询和pagefault处理结束，已经在内存中分配好了，该页是只读的匿名页。
**第二次查找**
在这次的查找中因为flags带有FOLL_WRITE标记，而page是只读的，此时follow_page_mask返回NULL，进入faultin_page。
    struct page *follow_page_mask(...)
    {
        return follow_page_pte(vma, address, pmd, flags);
    }
    static struct page *follow_page_pte(...)
    {
        if ((flags & FOLL_WRITE) && !pte_write(pte)) { /* 查找可写的页，但是该页是只读的 */
            pte_unmap_unlock(ptep, ptl);
            return NULL;
        }
    }
在处理faultin_page过程中，我们沿着函数调用路径faultin_page -> handle_mm_fault ->
__handle_mm_fault ->
handle_pte_fault一路找来，在handle_pte_fault中因为没有写访问权限，会进入do_wp_page函数中：
    static int handle_pte_fault(...)
    {
        if (flags & FAULT_FLAG_WRITE) /* faultin_page函数开头设置了该标志 */
            if (!pte_write(entry))
                return do_wp_page(mm, vma, address, pte, pmd, ptl, entry);
    }
do_wp_page会先判断是否真的需要复制当前页，因为上面分配的页是一个匿名页并且只有当前线程在使用，所以不用复制，直接使用即可。
    static int do_wp_page(struct mm_struct *mm, struct vm_area_struct *vma,
        unsigned long address, pte_t *page_table, pmd_t *pmd,
        spinlock_t *ptl, pte_t orig_pte)
    {
        old_page = vm_normal_page(vma, address, orig_pte); /* 得到之前分配的只读页，该页是匿名的页 */
        if (PageAnon(old_page) && !PageKsm(old_page)) {
            int total_mapcount;
            if (reuse_swap_page(old_page, &total_mapcount)) { /* old_page只有自己的进程在使用，直接使用就行了，不用再复制了 */
                if (total_mapcount == 1) {
                    /*
                     * The page is all ours. Move it to
                     * our anon_vma so the rmap code will
                     * not search our parent or siblings.
                     * Protected against the rmap code by
                     * the page lock.
                     */
                    page_move_anon_rmap(old_page, vma);
                }
                unlock_page(old_page);
                return wp_page_reuse(mm, vma, address, page_table, ptl,
                             orig_pte, old_page, 0, 0);
            }
            unlock_page(old_page);
        }
    }
    static inline int wp_page_reuse(struct mm_struct *mm,
                struct vm_area_struct *vma, unsigned long address,
                pte_t *page_table, spinlock_t *ptl, pte_t orig_pte,
                struct page *page, int page_mkwrite,
                int dirty_shared)
    {
        entry = maybe_mkwrite(pte_mkdirty(entry), vma); 带_RW_DIRTY,不带_PAGE_RW 
        if (ptep_set_access_flags(vma, address, page_table, entry, 1))
            update_mmu_cache(vma, address, page_table);
        return VM_FAULT_WRITE;
    }
这里需要关注的是wp_page_reuse的返回值是VM_FAULT_WRITE，即handle_mm_fault返回VM_FAULT_WRITE，在faultin_page函数中会去掉查找标志FOLL_WRITE，然后返回0。
    static int faultin_page(...)
    {
        ret = handle_mm_fault(mm, vma, address, fault_flags); /* 返回 VM_FAULT_WRITE */
        /* 去掉FOLL_WRITE标记， */
        if ((ret & VM_FAULT_WRITE) && !(vma->vm_flags & VM_WRITE))
            *flags &= ~FOLL_WRITE;
        return 0;
    }
**第三次查找**
在上一次处理查找失败的过程中FOLL_WRITE被去掉了，所以这一次的follow_page_mask会成功返回之前分配的page。到这里写时复制过程就算完成了。
**madvise(MADV_DONTNEED)**
madvise系统调用的作用是给系统对于内存使用的一些建议，MADV_DONTNEED参数告诉系统未来不访问该内存了，内核可以释放内存页了。内核函数madvise_dontneed中会移除指定范围内的用户空间page。
    static long madvise_dontneed(struct vm_area_struct *vma,
                     struct vm_area_struct **prev,
                     unsigned long start, unsigned long end)
    {
        ...
        zap_page_range(vma, start, end - start, NULL);
        return 0;
    }
    void zap_page_range(struct vm_area_struct *vma, unsigned long start,
        unsigned long size, struct zap_details *details)
    {
        ...
        for ( ; vma && vma->vm_start vm_next)
            unmap_single_vma(&tlb, vma, start, end, details);
        ...
    }
**产生竞态条件**
我们再来梳理一下写时复制的过程中调页的过程：
1\. 第一次follow_page_mask(FOLL_WRITE)，因为page不在内存中，进行pagefault处理。
2\. 第二次follow_page_mask(FOLL_WRITE)，因为page没有写权限，并去掉FOLL_WRITE。
3\. 第三次follow_page_mask(无FOLL_WRITE)，成功。
__get_user_pages函数中每次查找page前会先调用cond_resched()线程调度一下，这样就引入了竞态条件的可能性。在第二次分配COW页成功后，FOLL_WRITE标记已经去掉，如果此时，另一个线程把page释放了，那么第三次由于page不在内存中，又会进行调页处理，由于不带FOLL_WRITE标记，不会进行COW操作，此时get_user_pages得到的page带__PAGE_DIRTY，竞态条件就是这样产生的，流程如下：
1\. 第一次follow_page_mask(FOLL_WRITE)，page不在内存中，进行pagefault处理。
2\. 第二次follow_page_mask(FOLL_WRITE)，page没有写权限，并去掉FOLL_WRITE。
3\. 另一个线程释放上一步分配的COW页
4\. 第三次follow_page_mask(无FOLL_WRITE)，page不在内存中，进行pagefault处理。
5\. 第四次follow_page_mask(无FOLL_WRITE),成功返回page，但没有使用COW机制。
**0x03 漏洞利用**
这个是利用/proc/self/mem来修改只读文件的exploit
这个是利用ptrace(PTRACE_POKETEXT)来修改只读文件的exploit
这个是Andriod系统Root的exploit
**0x04 漏洞修复**
该漏洞patch的链接：[[https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=19be0eaffa3ac7d8eb6784ad9bdbc7d67ed8e619]](https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=19be0eaffa3ac7d8eb6784ad9bdbc7d67ed8e619)。现在不再是把FOLL_WRITE标记去掉，而是添加了一个FOLL_COW标志来表示获取一个COW分配的页。即使是竞态条件破坏了一次完整的获取页的过程，但是因为FOLL_WRITE标志还在，所以会重头开始分配一个COW页，从而保证该过程的完整性。
    diff --git a/include/linux/mm.h b/include/linux/mm.h
    index e9caec6..ed85879 100644
    --- a/include/linux/mm.h
    +++ b/include/linux/mm.h
    @@ -2232,6 +2232,7 @@ static inline struct page *follow_page(struct vm_area_struct *vma,
     #define FOLL_TRIED    0x800    /* a retry, previous pass started an IO */
     #define FOLL_MLOCK    0x1000    /* lock present pages */
     #define FOLL_REMOTE    0x2000    /* we are working on non-current tsk/mm */
    +#define FOLL_COW    0x4000    /* internal GUP flag */
     typedef int (*pte_fn_t)(pte_t *pte, pgtable_t token, unsigned long addr,
                void *data);
    diff --git a/mm/gup.c b/mm/gup.c
    index 96b2b2f..22cc22e 100644
    --- a/mm/gup.c
    +++ b/mm/gup.c
    @@ -60,6 +60,16 @@ static int follow_pfn_pte(struct vm_area_struct *vma, unsigned long address,
        return -EEXIST;
     }
    +/*
    + * FOLL_FORCE can write to even unwritable pte's, but only
    + * after we've gone through a COW cycle and they are dirty.
    + */
    +static inline bool can_follow_write_pte(pte_t pte, unsigned int flags)
    +{
    +    return pte_write(pte) ||
    +        ((flags & FOLL_FORCE) && (flags & FOLL_COW) && pte_dirty(pte));
    +}
    +
     static struct page *follow_page_pte(struct vm_area_struct *vma,
            unsigned long address, pmd_t *pmd, unsigned int flags)
     {
    @@ -95,7 +105,7 @@ retry:
        }
        if ((flags & FOLL_NUMA) && pte_protnone(pte))
            goto no_page;
    -    if ((flags & FOLL_WRITE) && !pte_write(pte)) {
    +    if ((flags & FOLL_WRITE) && !can_follow_write_pte(pte, flags)) {
            pte_unmap_unlock(ptep, ptl);
            return NULL;
        }
    @@ -412,7 +422,7 @@ static int faultin_page(struct task_struct *tsk, struct vm_area_struct *vma,
         * reCOWed by userspace write).
         */
        if ((ret & VM_FAULT_WRITE) && !(vma->vm_flags & VM_WRITE))
    -        *flags &= ~FOLL_WRITE;
    +            *flags |= FOLL_COW;
        return 0;
     }
**0x05 参考资料**