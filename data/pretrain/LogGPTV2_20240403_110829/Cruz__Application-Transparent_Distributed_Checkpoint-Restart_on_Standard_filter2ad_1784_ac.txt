We show that the TCP invariant remains satisﬁed in any
global checkpoint state if the checkpoints of participant
nodes are coordinated using the protocol described in Fig. 2
Consider the instant when the Checkpoint Coordinator be-
gins to execute its Step 3. It is straightforward to observe
from the coordination protocol that, at that instant, com-
munication is disabled on all nodes and, hence, the TCP
state is frozen on all nodes (i.e., the elements of TCP state
mentioned above cannot change on any node). If this TCP
state on each node is saved, this would amount to capturing
the TCP state synchronously on all nodes which would triv-
ially satisfy the TCP invariant. In the Checkpoint Agent’s
operation described in Fig. 2, the TCP state is actually cap-
tured at an earlier time on each node. However, we observe
that each node disabled its communication (in Step 1) be-
fore capturing its TCP state and this communication is re-
enabled (in Step 7) well after the instant discussed above.
Consequently, the TCP state saved at each node is identical
to the TCP state on the node at the instant discussed above.
Hence, the global checkpoint state preserves the TCP in-
variant and is thus consistent.
Our distributed restart mechanism disables communica-
tion on each node before restoring its part of the global state.
The restart of participant nodes is coordinated using a pro-
tocol identical to the coordinated checkpoint protocol. An
argument similar to the one above can be used to show that
our coordinated restart mechanism resumes the processes
and the communication from the consistent global check-
point state resulting in correct execution.
5.2. Performance Discussion
Our
coordinated
approach
checkpoint-restart
is
improving signiﬁcantly over
lightweight and scalable,
previously proposed mechanisms [1][17][15]. The mes-
sages exchanged to coordinate checkpoint and restart in our
approach (Fig. 2) are the minimum necessary to ensure the
atomicity of the global checkpoint, such as with two-phase
or three-phase commit protocols. The approaches used in
MPVM [1], CoCheck [17], and LAM-MPI [15] require ad-
ditional messages to ﬂush the channels between every pair
of processes. This results in O(N 2) message complexity
compared to O(N) complexity with our approach.
Coordinator
Node 1
Node 2
Node N
Time
checkpoint
comm.
disabled
continue
done
saving state continuing
execution
Checkpoint latency
Figure 4. Coordinated Checkpoint Operation
with optimization.
anisms can be extended with optimizations explored in
prior research such as copy-on-write to allow applications
to compute concurrently with their checkpointing and in-
cremental checkpointing to minimize checkpoint size.
In
addition, we now describe an optimization that can permit
nodes to continue computation without waiting for all nodes
to complete their checkpoints. Recall from Section 5.1 that
each node saves its state after its communication has been
disabled. We observe that the state of each individual node
cannot change as long as its communication is disabled.
Therefore, once the Coordinator has conﬁrmed that com-
munication is disabled on all nodes, it can permit each node
to continue operation as soon as it has completed saving
its checkpoint state, without any possibility of changing the
checkpoint state of other nodes. With this optimization (il-
lustrated in Fig. 4), each node notiﬁes the coordinator as
soon as its communication is disabled without waiting to
save its local state. Mechanisms to support further asyn-
chrony among the checkpointing nodes (e.g., using check-
point sequence numbers) are not justiﬁed since applications
will be forced to stall when a non-blocked node tries to com-
municate with a blocked node.
Several other optimizations are worth examination.
Since communication is disabled at each checkpoint, pack-
ets may be lost and TCP may backoff causing performance
to be degraded for a short duration after the checkpoint
while TCP recovers from its backoff. We evaluate this effect
experimentally in Section 6. The impact of TCP backoff can
be reduced by keeping communication disabled only for the
duration it takes to save the communication state. Since sav-
ing the communication state is a fast operation, this allows
any recovery from TCP backoffs to proceed in parallel with
saving the checkpoint state, which is dominated by the time
to save the application’s virtual memory state.
6. Performance Evaluation
The algorithm we described in Fig. 2 blocks processes
until all nodes have completed their checkpoint. Our mech-
We have implemented Cruz on a cluster of Linux 2.4 sys-
tems and integrated it with LSF [14], a job scheduler for
)
s
m
(
e
m
i
t
t
i
n
o
p
k
c
e
h
c
 1500
 1400
 1300
 1200
 1110
 1000
 900
 800
 700
 600
 500
 400
 300
 200
 100
 0
 2
 3
 4
 5
 6
 7
 8
number of nodes
(a) Total checkpoint latency
)
c
e
s
o
r
c
m
i
(
d
a
e
h
r
e
v
o
 1000
 900
 800
 700
 600
 500
 400
 300
 200
 100
 0
 2
 3
 4
 5
 6
 7
 8
number of nodes
(b) Coordination overhead
Figure 5. Results for slm benchmark experiments.
clusters. In this section we show experimental performance
results obtained for checkpointing distributed applications
with this implementation using two benchmarks: a) a semi-
lagrangian atmospheric model benchmark (slm), which is
a parallel application used for weather prediction; and b)
a TCP streaming benchmark, consisting of a transmitting
node sending data through a TCP socket connection to a
receiving node at maximum rate. The benchmarks were ex-
ecuted on a cluster of machines interconnected by a gigabit
ethernet switch. Each node consists of two 1 GHz Pentium
III processors, 2 GB of RAM, 256 KB of cache and an In-
tel e1000 gigabit NIC. During all experiments, checkpoint
is initiated by a coordinator located on a node distinct from
the application nodes.
The runtime overhead of Cruz is negligible (less than
0.5%) since the underlying Zap mechanism requires noth-
ing more than virtualizing identiﬁers. Figures 5(a) and 5(b)
show experimental results for checkpointing the slm bench-
mark with the number of nodes varying from 2 to 8. The
error bars represent the standard deviation of the measure-
ments ([µ − σ, µ + σ]) among the total number of mea-
surements observed during one complete execution of the
benchmark. During the experiments the application was
run from beginning to completion with checkpoints every
8 seconds interval of execution time. The total execution
time for the benchmark6, varies from 545 seconds for 2
nodes to 205 seconds for 8 nodes. Figure 5(a) shows to-
tal checkpoint latency, measured in the coordinator, i.e., the
time interval elapsed from the ﬁrst checkpoint message sent
to the last done message received at the coordinator. Figure
5(b) shows the estimated overhead asssociated with coor-
dination. The overhead was computed by subtracting from
the total checkpoint latency measured in Figure 5(a), the
time spent in executing local operations of checkpoint and
continue in the application nodes. Since application nodes
execute these local operations in parallel, we consider the
global cost of each local operation as the maximum time
measured in all nodes.
The results of Figure 5(a) show an overhead of approx-
imately 1 second for checkpointing the slm benchmark for
all node conﬁgurations. This time is a function of the size
of the application state that needs to be saved, and is domi-
nated by the time to write this state to disk. In general, most
of the state consists of the non-zero contents of the virtual
memory of all processes running in the pod.
More importantly, the results of Figure 5(b) show that
the overhead for coordination is negligible, on the order of
350 µs to 550 µs. The graph shows that the overhead in-
creases by approximately 50 µs for each node for conﬁgura-
tions with more than 4 nodes. This suggests that our check-
point mechanism should scale to a large number of nodes
before the overhead becomes comparable with the check-
point time. Performance results for the restart operation are
similar to the results of Figures 5(a) and 5(b) but are omitted
here because of space limitations.
We performed experiments using a TCP streaming
benchmark running between two nodes to evaluate the per-
formance impact of packet drops in the network when com-
munication is disabled for the nodes to perform check-
points. Figure 6 shows the measured rate of the TCP stream
between two nodes, as a function of time. The plotted rate
corresponds to the average rate measured in the receiver
during a sliding window of 10 ms duration previous to the
corresponding point. A checkpoint operation is started at
time t = 0 when the rate drops to zero7. The checkpoint
operation completes after approximately 120 ms. At this
time the receiver continues consuming data in the TCP re-
ceive buffer that arrived before the checkpoint operation
was started, illustrated by the short pulse at 120 ms. How-
6not including the time the application is stopped for checkpointing
7The curve reaches 0 only at time t=10, since the plotted rate is aver-
aged over the previous 10 ms window.
)
s
/
b
M
(
t
e
a
r
 700
 600
 500
 400
 300
 200
 100
 0
checkpoint
TCP recovery
 0  20  40  60  80  100  120  140  160  180  200  220  240  260  280  300
time (ms)
Figure 6. Effect of dropped packets on ﬂow
rate (TCP streaming benchmark)
ever, the sender does not continue sending data until later
when it recovers from lost packets due to our network ﬁl-
ter. At this time communication resumes at the normal rate
as before the checkpoint operation. The small oscillations
in the curve is due to the fact that the application receives
bytes in multiples of packets which in this experiment have
the maximum size of 1500 bytes. Although the network ﬁl-
ters used in our distributed checkpoint caused packets to be
dropped and network communication to be suspended, this
perturbation is small, with normal communication restart-
ing after approximately 100 ms.
7. Summary
a
powerful
and
We
presented
general-purpose
checkpoint-restart mechanism, Cruz, which improves
the operation of computing environments, reducing both
planned and unplanned downtime and increasing resource
allocation ﬂexibility.
Our mechanism has two main
contributions. First, we enable saving and restoring the
state of live TCP connections. Second, we leverage this
capability and develop a new lightweight distributed
checkpoint-restart mechanism which uses
the fewest
messages necessary to ensure the atomicity of the global
coordinated checkpoint. We have implemented our mech-
anism and evaluated its performance using a scientiﬁc
parallel application and a network intensive benchmark.
Our results show negligible coordination overhead demon-
strating the scalability of our approach. The results suggest
that the system should scale to a large number of nodes
before coordination overhead becomes comparable to the
time to perform local checkpoint or restart.
We propose performance optimizations to our base so-
lution. As future work, we plan to evaluate the beneﬁts of
these optimizations. In addition, we plan to evaluate per-
formance of our mechanism across a wide range of applica-
tions and cluster conﬁgurations.
References
[1] J. Casas, D. L. Clark, R. Konuru, S. W. Otto, R. M. Prouty,
and J. Walpole. MPVM: A migration transparent version of
PVM. Computing Systems, 8(2):171–216, 1995.
[2] K. M. Chandy and L. Lamport. Distributed snapshots: De-
termining global states of distributed systems. ACM Trans-
actions on Computer Systems, 3(1):63–75, Feb. 1985.
[3] J. Duell, P. Hargrove, and E. Roman. The design and imple-
mentation of Berkeley Labs’ Linux checkpoint/restart. 2002.
[4] E. N. Elnozahy and W. Zwaenepoel. On the use and im-
plementation of message logging. In Int’l Symp on Fault-
Tolerant Computing, Jun 1994.
[5] E. N. M. Elnozahy, L. Alvisi, Y.-M. Wang, and D. B. John-
son. A survey of rollback-recovery protocols in message-
passing systems. ACM Computing Surveys, 34(3):375–408,
Sep 2002.
[6] I. Foster, C. Kesselman, J. Nick, and S. Tuecke. Grid Ser-
vices for distributed system integration. Computer, 35(6),
2002.
[7] A. Geist, A. Beguelin, J. Dongarra, W. Jiang, R. Manchek,
and V. S. Sunderam. PVM: Parallel Virtual Machine A
Users’ Guide and Tutorial for Networked Parallel Comput-
ing. MIT Press, 2000.
[8] P. Jalote. Fault Tolerance in Distributed Systems. PTR Pren-
tice Hall, 1994.
[9] M. Litzkow, T. Tanenbaum, J. Basney, and M. Livny. Check-
point and migration of unix processes in the condor dis-
tributed processing system. Computer Sciences Technical
Report 1346, University of Wisconsin, Madison, WI, 1997.
[10] J. Mogul, L. Brakmo, D. Lowell, D. Subhraveti, and
J. Moore. Unveiling the transport. In Second Workshop on
Hot Topics in Networks, Nov 2003.
[11] N. Neves and W. K. Fuchs. RENEW: A tool for fast and
In Int’l
efﬁcient implementation of checkpoint protocols.
Symp on Fault-Tolerant Computing, Jun 1998.
[12] S. Osman, D. Subhraveti, G. Su, and J. Nieh. The design
and implementation of Zap: A system for migrating com-
puting environments. In Fifth Symp. on Operating System
Design and Implementation (OSDI 2002), pages 361–376,
Dec 2002.
[13] J. S. Plank, M. Beck, G. Kingsley, and K. Li. Libckpt:
In Usenix Winter
Transparent checkpointing under Unix.
1995 Technical Conference, pages 213–224, 1995.
[14] Platform Computing Corporation. LSF User’s Guide. www.
platform.com.
[15] S. Sankaran, J. M. Squyres, B. Barrett, A. Lumsdaine, J. Du-
ell, P. Hargrove, and E. Roman. The LAM/MPI check-
point/restart framework: System-initiated checkpointing. In
Proceedings, LACSI Symposium, October 2003.
[16] M. Snir, S. Otto, S. Huss-Lederman, D. Walker, and J. Don-
garra. MPI: The Complete Reference (Vol. 1) - 2nd Edition.
MIT P ress, 1998.
[17] G. Stellner. Cocheck: Checkpointing and process migration
for MPI. In 10th International Parallel Processing Sympo-
sium (IPPS 1996), 1996.
[18] W. R. Stevens. TCP/IP Illustrated, Volume 1 The Protocols.
Addison-Wesley, 1994.