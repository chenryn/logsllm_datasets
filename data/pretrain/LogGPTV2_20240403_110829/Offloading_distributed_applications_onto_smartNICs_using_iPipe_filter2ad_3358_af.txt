keynote-2.pdf. (2017).
[20] Daniel Firestone, Andrew Putnam, Sambhrama Mundkur, Derek Chiou, Alireza
Dabagh, Mike Andrewartha, Hari Angepat, Vivek Bhanu, Adrian Caulfield, Eric
Chung, Harish Kumar Chandrappa, Somesh Chaturmohta, Matt Humphrey, Jack
Lavier, Norman Lam, Fengfen Liu, Kalin Ovtcharov, Jitu Padhye, Gautham Popuri,
Shachar Raindel, Tejas Sapre, Mark Shaw, Gabriel Silva, Madhan Sivakumar,
Nisheeth Srivastava, Anshuman Verma, Qasim Zuhair, Deepak Bansal, Doug
Burger, Kushagra Vaid, David A. Maltz, and Albert Greenberg. 2018. Azure
Accelerated Networking: SmartNICs in the Public Cloud. In 15th USENIX
Symposium on Networked Systems Design and Implementation.
[21] Alex Goldhammer and John Ayer Jr. 2008. Understanding performance of PCI
express systems. Xilinx WP350, Sept 4 (2008).
(2017).
[22] Troy D. Hanson. 2017. Uthash Hashtable. https://troydhanson.github.io/uthash/.
[23] Carl Hewitt, Peter Bishop, and Richard Steiger. 1973. A Universal Modular ACTOR
Formalism for Artificial Intelligence. In Proceedings of the 3rd International Joint
Conference on Artificial Intelligence (IJCAI’73).
[24] Huawei. 2018. Huawei IN550 SmartNIC. https://e.huawei.com/us/news/it/
201810171443. (2018).
[25] Xin Jin, Xiaozhou Li, Haoyu Zhang, Robert Soulé, Jeongkeun Lee, Nate Foster,
Changhoon Kim, and Ion Stoica. 2017. NetCache: Balancing Key-Value Stores
with Fast In-Network Caching. In Proceedings of the 26th Symposium on Operating
Systems Principles.
[26] Kostis Kaffes, Timothy Chong, Jack Tigar Humphries, Adam Belay, David
Mazières, and Christos Kozyrakis. 2019. Shinjuku: Preemptive Scheduling for
µsecond-scale Tail Latency. In 16th USENIX Symposium on Networked Systems
Design and Implementation (NSDI 19).
[27] Anuj Kalia, Michael Kaminsky, and David G Andersen. 2014. Using RDMA
efficiently for key-value services. In ACM SIGCOMM Computer Communication
Review, Vol. 44. 295–306.
[28] Anuj Kalia, Michael Kaminsky, and David G. Andersen. 2016. Design guidelines for
high performance RDMA systems. In 2016 USENIX Annual Technical Conference.
[29] Anuj Kalia, Michael Kaminsky, and David G. Andersen. 2016. FaSST: Fast, Scalable
and Simple Distributed Transactions with Two-Sided (RDMA) Datagram RPCs. In
12th USENIX Symposium on Operating Systems Design and Implementation (OSDI
16).
[30] Antoine Kaufmann, Simon Peter, Naveen Kr. Sharma, Thomas Anderson, and
Arvind Krishnamurthy. 2016. High Performance Packet Processing with FlexNIC.
In Proceedings of the Twenty-First International Conference on Architectural Support
for Programming Languages and Operating Systems.
[31] Daehyeok Kim, Amirsaman Memaripour, Anirudh Badam, Yibo Zhu,
Hongqiang Harry Liu, Jitu Padhye, Shachar Raindel, Steven Swanson, Vyas Sekar,
and Srinivasan Seshan. 2018. Hyperloop: Group-based NIC-offloading to Accel-
erate Replicated Transactions in Multi-tenant Storage Systems. In Proceedings
of the 2018 Conference of the ACM Special Interest Group on Data Communication.
[32] Joongi Kim, Keon Jang, Keunhong Lee, Sangwook Ma, Junhyun Shim, and
Sue Moon. 2015. NBA (Network Balancing Act): A High-performance Packet
Processing Framework for Heterogeneous Processors. In Proceedings of the Tenth
European Conference on Computer Systems.
[33] Eddie Kohler, Robert Morris, Benjie Chen, John Jannotti, and M Frans Kaashoek.
2000. The Click modular router. ACM Transactions on Computer Systems (TOCS)
18, 3 (2000), 263–297.
[34] Leslie Lamport. 2001. Paxos made simple. ACM Sigact News 32, 4 (2001), 18–25.
[35] Jure Leskovec and Andrej Krevl. 2014. SNAP Datasets: Stanford Large Network
Dataset Collection. http://snap.stanford.edu/data. (June 2014).
[36] LevelDB. 2017. LevelDB Key-Value Store. http://leveldb.org. (2017).
[37] Bojie Li, Zhenyuan Ruan, Wencong Xiao, Yuanwei Lu, Yongqiang Xiong, Andrew
Putnam, Enhong Chen, and Lintao Zhang. 2017. KV-Direct: High-Performance
In-Memory Key-Value Store with Programmable NIC. In Proceedings of the 26th
Symposium on Operating Systems Principles.
[38] Bojie Li, Kun Tan, Layong Larry Luo, Yanqing Peng, Renqian Luo, Ningyi Xu,
Yongqiang Xiong, Peng Cheng, and Enhong Chen. 2016. Clicknp: Highly flexible
and high performance network processing with reconfigurable hardware. In
Proceedings of the 2016 ACM SIGCOMM Conference.
[39] Hyeontaek Lim, Dongsu Han, David G. Andersen, and Michael Kaminsky. 2014.
MICA: A Holistic Approach to Fast In-memory Key-value Storage. In Proceedings
of the 11th USENIX Conference on Networked Systems Design and Implementation.
[40] Jianxiao Liu, Zonglin Tian, Panbiao Liu, Jiawei Jiang, and Zhao Li. 2016. An
approach of semantic web service classification based on Naive Bayes. In Services
Computing (SCC), 2016 IEEE International Conference on.
[41] Ming Liu, Liang Luo, Jacob Nelson, Luis Ceze, Arvind Krishnamurthy, and
Kishore Atreya. 2017.
IncBricks: Toward In-Network Computation with an
In-Network Cache. In Proceedings of the Twenty-Second International Conference
on Architectural Support for Programming Languages and Operating Systems.
https://www.marvell.com/
documents/08icqisgkbtn6kstgzh4/. (2018).
[42] Marvell. 2018. Marvell LiquidIO SmartNICs.
[43] Mellanox. 2018. Mellanox BuleField SmartNIC. http://www.mellanox.com/page/
products_dyn?product_family=275&mtag=bluefield_smart_nic. (2018).
Accelerated Switch and Packet Processing.
[44] Mellanox. 2019.
http:
//www.mellanox.com/page/asap2?mtag=asap2. (2019).
[45] Christopher Mitchell, Yifeng Geng, and Jinyang Li. 2013. Using One-Sided RDMA
Reads to Build a Fast, CPU-Efficient Key-Value Store.. In USENIX Annual Technical
Conference.
[46] Jacob Nelson, Brandon Holt, Brandon Myers, Preston Briggs, Luis Ceze, Simon
Kahan, and Mark Oskin. 2015. Latency-Tolerant Software Distributed Shared
Memory.. In USENIX Annual Technical Conference.
products/agilio-cx/. (2018).
[47] Netronome. 2018. Netronome Agilio SmartNIC. https://www.netronome.com/
[48] Rolf Neugebauer, Gianni Antichi, José Fernando Zazo, Yury Audzevich, Sergio
López-Buedo, and Andrew W. Moore. 2018. Understanding PCIe Performance
for End Host Networking. In Proceedings of the 2018 Conference of the ACM Special
Interest Group on Data Communication.
[49] Rajesh Nishtala, Hans Fugal, Steven Grimm, Marc Kwiatkowski, Herman Lee,
Harry C. Li, Ryan McElroy, Mike Paleczny, Daniel Peek, Paul Saab, David Stafford,
Tony Tung, and Venkateshwaran Venkataramani. 2013. Scaling Memcache at
Facebook. In Presented as part of the 10th USENIX Symposium on Networked
Systems Design and Implementation.
[50] OFED. 2019. Infiniband Verbs Performance Tests. https://github.com/linux-rdma/
perftest. (2019).
[51] Amy Ousterhout, Joshua Fried, Jonathan Behrens, Adam Belay, and Hari Balakr-
ishnan. 2019. Shenango: Achieving High CPU Efficiency for Latency-sensitive
Datacenter Workloads. In 16th USENIX Symposium on Networked Systems Design
and Implementation (NSDI 19).
330
SIGCOMM ’19, August 19–23, 2019, Beijing, China
M. Liu et al.
[52] Ben Pfaff, Justin Pettit, Teemu Koponen, Ethan Jackson, Andy Zhou, Jarno
Rajahalme, Jesse Gross, Alex Wang, Joe Stringer, Pravin Shelar, Keith Amidon, and
Martin Casado. 2015. The Design and Implementation of Open vSwitch. In 12th
USENIX Symposium on Networked Systems Design and Implementation (NSDI 15).
[53] Phitchaya Mangpo Phothilimthana, Ming Liu, Antoine Kaufmann, Simon Peter,
Rastislav Bodik, and Thomas Anderson. 2018. Floem: A Programming System for
NIC-Accelerated Network Applications. In 13th USENIX Symposium on Operating
Systems Design and Implementation.
[54] George Prekas, Marios Kogias, and Edouard Bugnion. 2017. ZygOS: Achieving
Low Tail Latency for Microsecond-scale Networked Tasks. In Proceedings of the
26th Symposium on Operating Systems Principles.
[55] Amedeo Sapio, Ibrahim Abdelaziz, Abdulla Aldilaijan, Marco Canini, and Panos
Kalnis. 2017. In-network computation is a dumb idea whose time has come. In
Proceedings of the 16th ACM Workshop on Hot Topics in Networks.
[56] Linus Schrage. 1968. Letter to the editor−a proof of the optimality of the shortest
remaining processing time discipline. Operations Research 16, 3 (1968), 687–690.
[57] Naveen Kr. Sharma, Ming Liu, Kishore Atreya, and Arvind Krishnamurthy. 2018.
Approximating Fair Queueing on Reconfigurable Switches. In 15th USENIX
Symposium on Networked Systems Design and Implementation (NSDI 18).
[58] Madhavapeddi Shreedhar and George Varghese. 1996. Efficient fair queuing using
deficit round-robin. IEEE/ACM Transactions on networking 4, 3 (1996), 375–385.
[59] Sriram Srinivasan and Alan Mycroft. 2008. Kilim: Isolation-typed actors for java.
In European Conference on Object-Oriented Programming.
protocol. Distributed and Parallel Databases 1, 4 (1993), 383–408.
[61] Alexander L Stolyar and Kavita Ramanan. 2001. Largest weighted delay first sched-
uling: Large deviations and optimality. Annals of Applied Probability (2001), 1–48.
[62] SmartNIC Vendors. 2019. Marvell, Private communications. unpublished. (2019).
[63] Xingda Wei, Jiaxin Shi, Yanzhe Chen, Rong Chen, and Haibo Chen. 2015. Fast
in-memory transaction processing using RDMA and HTM. In Proceedings of the
25th Symposium on Operating Systems Principles.
Is tail-optimal scheduling possible?
Operations research 60, 5 (2012), 1249–1257.
[65] Irene Zhang, Naveen Kr. Sharma, Adriana Szekeres, Arvind Krishnamurthy,
and Dan R. K. Ports. 2015. Building Consistent Transactions with Inconsistent
Replication. In Proceedings of the 25th Symposium on Operating Systems Principles.
[60] James W Stamos and Flaviu Cristian. 1993. Coordinator log transaction execution
[64] Adam Wierman and Bert Zwart. 2012.
331
Offloading Distributed Applications onto SmartNICs using iPipe
SIGCOMM ’19, August 19–23, 2019, Beijing, China
while true do
▷ on each FCFS core
wqe =iPipe_nstack_r ecv()
actor =iPipe_dispatcher(wqe)
if actor.is_DRR then
actor .mailbox_push(wqe)
Continue
Algorithm 1 iPipe FCFS scheduler algorithm
1: wqe : contains packet data and metadata
2: DRR_queue : the runnable queue for the DRR scheduler
3: procedure FCFS_sched
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
end if
25:
end while
26: end procedure
end if
actor .actor_exe(wqe)
actor .bookeepinд()
if T_tail > Tail_thresh then
actor .is_DRR =1
DRR_queue .push(actor)
if T_mean > Mean_thresh then
iPipe_actor_miдr ate(actor_chosen)
end if
if core_id is 0 then
end if
if T_mean < (1-α)Mean_thresh then
iPipe_actor_pull()
end if
▷ Update execution statistics
▷ Downgrade
▷ Management core
▷ Migration
▷ Migration
object data to the host side using messages and DMA primitives;
(3) creates new objects on the host side and then inserts entries
into the host-side object table; (4) deletes related entries from the
NIC-side object table upon deleting the actor. The host-side DMO
works similarly, except that it uses the glibc memory allocator.
We estimate the migration cost (SmartNIC-pushed) by breaking
down the time elapsed of four phases 3.2.5. We choose 8 actors
from three applications. our experiments are conducted under 90%
networking load and we force the actor migration after the warm
up (5s). Figure 18 presents our results. First, phase 3 dominates the
migration cost (i.e., 67.8% on average of 8 actors) since it requires
to move the distributed objects to the host side. For example, the
LSM memtable actor has around 32MB object and consumes 35.8ms.
Phase 4 ranks the second (i.e., 27.2%) as it pushes buffered requests
to the host. Also, it varies based on the networking load. Phase 1 and
Phase 2 are two lightweight parts because they only introduce the
iPipe runtime locking/unlocking and state manipulation overheads.
Figure 18: Migration elapsed time breakdown of 8 actors from three
applications evaluated with 10GbE CN2350 cards.
Appendix A SmartNIC computing unit characteriza-
tion
Appendices are supporting material that has not been peer reviewed.
Table 3 summarizes the microarchietcture results for LiquidIOII
CN2350 multicore processor and accelerators.
Appendix B More details in the iPipe framework
This section describes more details of the iPipe framework that is
not included in the main paper.
B.1 iPipe runtime APIs
Table 4 presents the major APIs. Specifically, the actor management
APIs are used by our runtime. We provide five calls for manag-
ing DMOs. When creating an object on the NIC, iPipe first allo-
cates a local memory region using the dlmalloc2 allocator and
then inserts an entry (i.e., object ID, actor ID, start address, size)
into the NIC object table. Upon dmo_free, iPipe frees the space
allocated for the object and deletes the entry from the object ta-
ble. dmo_memset, dmo_memcpy, dmo_memmove resemble mem-
set/memcpy/memmove APIs in glibc, except that it uses the object
ID instead of a pointer.
For the networking stack, iPipe takes advantage of packet pro-
cessing accelerators (if the SmartNIC has) to build a shim cus-
tomized networking stack for the SmartNIC. This stack performs
simple Layer2/Layer3 protocol processing, such as packet encap-
sulating/decapsulation, checksum verification, etc. When building
a packet, it uses the DMA scatter-gather technique to combine
the header and payload if they are not colocated. This helps im-
prove the bandwidth utilization, as shown in our characterization
(Section 2.2.5).
B.2 iPipe actor scheduling algorithm
Algorithms 1 and 2 show the details of our iPipe hybrid scheduler.
B.3 iPipe actor migration evaluation
When migrating an actor to the host, as shown in Figure 12, our
runtime (1) collects all objects that belong to the actor; (2) sends the
332
 0 10 20 30 40 50 60FilterCountRankCoord.Parti.ConsensusLSMmem.Elapsed time (ms)Phase1Phase2Phase3Phase4SIGCOMM ’19, August 19–23, 2019, Beijing, China
Applications
Computation
DS
Exe. Lat.(us)