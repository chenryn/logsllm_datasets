title:Private resource allocators and their applications
author:Sebastian Angel and
Sampath Kannan and
Zachary B. Ratliff
Private resource allocators and their applications
Sebastian Angel
University of Pennsylvania
Sampath Kannan
University of Pennsylvania
Zachary Ratliff
Raytheon BBN Technologies
Abstract—This paper introduces a new cryptographic primi-
tive called a private resource allocator (PRA) that can be used
to allocate resources (e.g., network bandwidth, CPUs) to a set
of clients without revealing to the clients whether any other
clients received resources. We give several constructions of PRAs
that provide guarantees ranging from information-theoretic to
differential privacy. PRAs are useful in preventing a new class
of attacks that we call allocation-based side-channel attacks.
These attacks can be used, for example, to break the privacy
guarantees of anonymous messaging systems that were designed
specifically to defend against side-channel and traffic analysis
attacks. Our implementation of PRAs in Alpenhorn, which is a
recent anonymous messaging system, shows that PRAs increase
the network resources required to start a conversation by up
to 16× (can be made as low as 4× in some cases), but add no
overhead once the conversation has been established.
I. INTRODUCTION
Building systems that avoid unintentional information leak-
age is challenging since every action or operation—innocuous
as it may be—can reveal sensitive information. This is
especially true in the wake of numerous side-channel attacks
that exploit unexpected properties of a system’s design, im-
plementation, or hardware. These attacks can be based on
analog signals such as the machine’s power consumption [50],
sound produced [36], photonic emissions from switching
transistors [72], temperature [43], and electromagnetic radiation
emanated [4, 82], that arise as a result of the system performing
some sensitive operation. Or they may be digital and monitor
the timing of operations [51], memory access patterns [38],
the contention arising from shared resources (e.g., caches [47],
execution ports in simultaneous multithreading [19]), and the
variability of network traffic [70].
In the above cases, information is exposed as a result of a
process in the system consuming a resource (e.g., sending a
network packet, populating the cache, executing a conditional
branch instruction). We can think of these side channels as
consumption-based. In this paper, we are concerned with side
channels that exist during the allocation of the resource to
a process, and that are observable regardless of whether the
process ultimately consumes the resource. As a result, these
allocation-based side channels can sometimes be exploited
by attackers in systems that have been explicitly designed
to avoid consumption-based side channels (systems that pad
all requests, regularize network traffic and memory accesses,
have constant time implementations, clear caches after every
operation, etc.). To prevent allocation-based side channels we
propose a new primitive called a private resource allocator
(PRA) that guarantees that the mechanism by which the system
allocates resources to processes leaks no information.
1
At a high level, allocation-based side channels exist because
a system’s resource allocator—which includes cluster man-
agers [1], network rate limiters [58], storage controllers [76],
data center resource managers [7], flow coordinators [67], lock
managers [42], etc.—can leak information about how many
(and which) other processes are requesting service through the
allocation itself. As a simple example, a process that receives
only a fraction of the resources available from an allocator that
is work conserving (i.e., that allocates as many resources as
possible) can infer that other processes must have requested
the same resources concurrently. These observations can be
made even if the other processes do not use their allocated
resources at all.
While the information disclosed by allocations might seem
harmless at first glance, these allocation-based side channels can
be used as building blocks for more serious attacks. As a mo-
tivating example, we show that allocation-based side channels
can be combined with traffic analysis attacks [5, 26, 27, 48, 49,
59, 66, 70, 77] to violate the guarantees of existing bidirectional
anonymous messaging systems (often called metadata-private
messengers or MPMs) [6, 10, 52, 53, 55, 56, 78, 81]. This
is significant because MPMs are designed precisely to avoid
side-channel attacks. In particular, Angel et al. [9] show that
these systems are secure only if none of the contacts with
whom a user communicates are compromised by an adversary;
otherwise, compromised contacts can learn information about
the user’s other conversations. We expand on Angel et al.’s
observation in Section II, and show that it is an instance of an
allocation-based side-channel attack.
To prevent allocation-based side channels, we introduce
private variants of resource allocators (PRAs) that can assign
resources to processes without
leaking to any processes
which or how many other processes received any units of
the resource. We formalize the properties of PRAs (§III),
and propose several constructions that guarantee information-
theoretic, computational, and differential privacy under different
settings (§IV-A–IV-C). We also discuss how privacy interacts
with classic properties of resource allocation. For example,
we show that privacy implies population monotonicity (§V).
Finally, we prove an impossibility result (§III-B): there does
not exist a PRA when the number of concurrent requesting
processes is not bounded ahead of time. As a result, PRAs
must assume a polynomial bound on the number of requesting
processes (and this bound might leak).
To showcase the benefits and costs of using PRAs, we
integrate our constructions into Alpenhorn [57], which is a
system that manages conversations in MPMs. The result is the
first MPM system that is secure in the presence of compromised
friends. Interestingly, our implementation efforts reveal that
naively introducing PRAs into MPMs would cripple these
systems’ functionality. For example, it would force clients to
abruptly end ongoing conversations, and would prevent honest
clients from ever starting conversations. To mitigate these issues,
we propose several techniques tailored to MPMs (§VI).
Our evaluation of Alpenhorn shows that PRAs lead to
conversations taking 16× longer to get started (or alternatively
consuming 16× more network resources), though this number
can be reduced to 4× by prioritizing certain users. However,
once conversations have started, PRAs incur no additional
overhead. While we admit that such delayed start (or bandwidth
increase) further hinders the usability of MPMs, compromised
friends are enough of a real threat to justify our proposal.
In summary, the contributions of this work are:
• The notion of Private Resource Allocators (PRA) that
assign resources to processes without leaking how many or
to which processes resources are allocated.
• An impossibility theorem that precisely captures under
what circumstances privacy cannot be achieved.
• Several PRA constructions under varying assumptions.
• A study of how privacy impacts other allocation properties.
• The integration of PRAs into an MPM to avoid leaking
information to compromised friends, and the corresponding
experimental evaluation.
Finally, we believe that PRAs have applications beyond
MPMs, and open up exciting theoretical and practical ques-
tions (§IX). We hope that the framework we present in the
following sections serves as a good basis.
II. CASE STUDY: METADATA-PRIVATE MESSENGERS
In the past few years, there has been a flurry of work on
messaging systems that hide not just the content of messages
but also the metadata that is associated with those messages [6,
8, 10, 24, 53, 55, 56, 78, 81, 83]. These systems guarantee some
variant of relationship (or third-party) unobservability [68], in
which all information (including the sender, recipient, time of
day, frequency of communication, etc.) is kept hidden from
anyone not directly involved in the communication. A key
driver for these systems is the observation that metadata is itself
sensitive and can be used—and in fact has been used [22, 71]—
to infer the content or at least the context of conversations for
a variety of purposes [73]. For example, a service provider
could infer that a user has some health condition if the user
often communicates with health professionals. Other inferable
information typically considered sensitive includes religion,
race, sexual orientation, and employment status [61].
In these metadata-private messengers (MPMs), a pair of
users are considered friends only if they have a shared secret.
Users can determine which of their acquaintances are part of the
system using a contact discovery protocol [16, 20, 60], and can
then exchange the secret needed to become friends with these
acquaintances through an out-of-band channel (e.g., in person
at a conference or coffee shop), or with an in-band add-friend
protocol [57]. A pair of friends can then initiate a session. This
FIG. 1—MPM systems consist of four protocols: friend discovery,
add-friend, dialing, and conversation. Users can only converse once
they are in an active session (agree on a session key and round).
is done with a dialing protocol [6, 52, 57] whereby one user
“cold calls” another user and notifies them of their intention to
start a conversation. The analogous situation in the non-private
setting is a dialing call on a VoIP or video chat service like
Skype. Creating a session boils down to agreeing on a time or
round to start the conversation, and generating a key that will
be used to encrypt all messages in the session (derived from
the shared secret and the chosen round).
Once a session between two friends has been established,
the participants can exchange messages using a conversation
protocol (this is the protocol that actually differentiates most
MPM systems). In all proposed conversation protocols, com-
munication occurs in discrete rounds—which is why part of
creating a session involves identifying the round on which to
start the conversation—during which a user sends and receives
up to k messages. One can think of each of these k messages as
being placed in a different channel. To guarantee no metadata
leaks, users are forced to send and receive a message on
each channel in every round, even when the user is idle and
has nothing to send or receive (otherwise observers could
determine when a user is not communicating). We summarize
these protocols in Figure 1.
The above highlights a tension between performance and
network costs experienced by all MPM systems. Longer rounds
increase the delay between two consecutive messages but
reduce the network overhead when a user is idle (due to fewer
dummy messages). Having more channels improves throughput
(more concurrent conversations per round or more messages
per conversation) but at the cost of higher network overhead
when the user is idle. Given that users are idle a large fraction
of the time, most MPMs choose long round duration (tens of
seconds) and a small number of channels (typically k = 1).
While these tradeoffs have long been understood, the impact
of the number of communication channels on privacy has
received less attention. We discuss this next.
A. Channel allocation can leak information
Prior works on MPMs have shown that the proposed contact
discovery, add-friend, dialing, and conversation protocols are
secure and leak little information (negligible or bounded) on
their own, but surprisingly, none had carefully looked at their
composition. Indeed, recent work by Angel et al. [9] shows that
existing dialing and communication protocols do not actually
2
Add friend to contact listDial a friend in contact listConverse with friendEstablish a shared secretSend message starting on round rAgree on session key and round rProtocolObjectiveDiscover friendsLearn identifier or public keycompose in the presence of compromised friends. The reason
is that the number of communication channels (k) is usually
smaller than the number of friends that could dial the user at
any one time. As a result, when a user is dialed by n friends
asking to start a conversation at the same time, the user must
determine an allocation of the n friends to the k channels.
As one would expect, when n > k, not all of the n dialing
requests can be allocated onto the k available channels since
each channel can only support one conversation (for example, a
user in Skype can only accept one incoming call at a time since
k = 1). If this allocation is not done carefully—defining what
“carefully” means formally is the subject of Section III—a user’s
friends can learn information through dialing. In particular, a
caller who dials and receives a busy signal or no response at
all for a round r can infer that the callee has agreed to chat
with other users during round r.1 For the more general case of
k > 1, an attacker controlling k callers can dial the user and
observe whether all calls are answered or not; an attacker may
even conduct a binary search over multiple rounds to learn the
exact number of ongoing conversations.
The saving grace is that information that leaks is observed
only by a user’s dialing friends, as opposed to all users in
the system or third-party observers (since friendship is a
precondition for dialing). However, friends’ accounts can be
compromised by an adversary, and users could be tricked
into befriending malicious parties. In fact, not only is this
possible, it is actually a common occurrence: prior surveys
of user behavior on online social networks show that users
are very willing to accept friend requests from strangers [69].
Furthermore, given recent massive leaks of personal data—3
billion accounts by Yahoo in 2013 [54]; 43 million accounts
by Equifax in 2017 [34]; 87 million users by Facebook in
2018 [74] and an additional 549 million records in 2019 [80]—
there is significant material for attackers to conduct social
engineering and other attacks. Worse yet, many of these attacks
can easily be automated [15].
B. Traffic analysis makes things worse
The previous section describes how an attacker, via com-
promised friends, can learn whether a user is busy or not in
some round r (or get some confidence on this) by conducting
an allocation-based side channel attack. While such leakage
is minor on its own, it can be composed with traffic analysis
techniques such as intersection [70] and disclosure [5] attacks
(and their statistical variants [25]).
As a very simple example, imagine an adversary that can
compromise the friends of multiple users and can use those
compromised friends to determine which users are (likely)
active in a given round r. The adversary can then reduce
the set of possible sender-recipient pairs by ignoring all the
idle users (more sophisticated observations can also be made
by targeting particular users). The adversary can then repeat
1A lack of response does not always mean that a user is busy with others; the
user could be asleep. However, existing MPMs accept requests automatically.
Even if the user were involved, information would still leak and predicating
correctness on behavior that is hard to characterize is undesirable.
the attack for other rounds r′, r′′, etc. With each additional
round, the adversary can construct intersections of active users
and shrink the set of possible sender-recipient pairs under the
assumption that conversations span multiple rounds.
In short, the described allocation-based side-channel attack
makes existing MPM systems vulnerable to traffic analysis. In
the next section we formally model the leakage of information
that results from allocating dialing friends to a limited number
of channels. In Sections IV-A–IV-C we then give several
constructions of allocators that can be used by MPM systems
to establish sessions without leaking information.
III. PRIVATE RESOURCE ALLOCATORS (PRAS)
The allocation-based side-channel attack described in the
prior section essentially follows a pigeonhole-type argument
whereby there are more friends than there are channels. This
same idea applies to other situations. For example, whenever
there is a limited number of CPU cores and many threads,
the way in which threads are scheduled onto cores leaks
information to the threads. Specifically, a thread that was
not scheduled could infer that other threads were, even if
the scheduled threads perform no operations and consume
no resources. In this section we formalize this problem more
generally and describe desirable security definitions.
We begin with the notion of a private resource allocator,
which is an algorithm that assigns a limited number of resources
to a set of processes that wish to use those resources. Privacy
means that the outcome of the allocator does not reveal to any
processes whether there were other processes concurrently
requesting the same resource. Note that private allocators
are concerned only with the information that leaks from the
allocation itself; information that leaks from the use of the
resource is an orthogonal concern.
In more detail, a resource allocator RA is an algorithm that
takes as input a resource of capacity k, and a set of processes
P from a universe of processes M (P ⊆ M). RA outputs the set
of processes U ⊆ P that should be given a unit of the resource,