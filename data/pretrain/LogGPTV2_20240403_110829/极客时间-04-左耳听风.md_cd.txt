## 计数器方式最简单的限流算法就是维护一个计数器Counter，当一个请求来时，就做加一操作，当一个请求处理完后就做减一操作。如果这个Counter大于某个数了（我们设定的限流阈值），那么就开始拒绝请求以保护系统的负载了。这个算法足够的简单粗暴。
## 队列算法在这个算法下，请求的速度可以是波动的，而处理的速度则是非常均速的。这个算法其实有点像一个FIFO 的算法。![](Images/47f321e2a4458d9866864d41ae1310aa.png){savepage-src="https://static001.geekbang.org/resource/image/c8/3d/c8f774f88ab8a4b72378971263c0393d.png"}在上面这个 FIFO 的队列上，我们可以扩展出一些别的玩法。一个是有优先级的队列，处理时先处理高优先级的队列，然后再处理低优先级的队列。如下图所示，只有高优先级的队列被处理完成后，才会处理低优先级的队列。![](Images/2b7a92158bc563ab4ecb9868ec24b0de.png){savepage-src="https://static001.geekbang.org/resource/image/de/80/de51d6fc68df3d8c808b84e4bc455580.png"}有优先级的队列可能会导致低优先级队列长时间得不到处理。为了避免低优先级的队列被饿死，一般来说是分配不同比例的处理时间到不同的队列上，于是我们有了带权重的队列。如下图所示。有三个队列的权重分布是 3:2:1，这意味着我们需要在权重为 3的这个队列上处理 3 个请求后，再去权重为 2 的队列上处理 2个请求，最后再去权重为 1 的队列上处理 1 个请求，如此反复。![](Images/d64a45ebd045e7a4a7b75d91814359b7.png){savepage-src="https://static001.geekbang.org/resource/image/c7/54/c775345e3b8f599e26a4d7f64941cd54.png"}队列流控是以队列的的方式来处理请求。如果处理过慢，那么就会导致队列满，而开始触发限流。但是，这样的算法需要用队列长度来控制流量，在配置上比较难操作。如果队列过长，导致后端服务在队列没有满时就挂掉了。一般来说，这样的模型不能做push，而是 pull 方式会好一些。
## 漏斗算法 Leaky Bucket漏斗算法可以参看 Wikipedia 的相关词条 [LeakyBucket](https://en.wikipedia.org/wiki/Leaky_bucket)。下图是一个[漏斗算法的示意图](https://www.slideshare.net/vimal25792/leaky-bucket-tocken-buckettraffic-shaping)。![](Images/5629b5ebac9047871fb071f11751533c.png){savepage-src="https://static001.geekbang.org/resource/image/95/00/95326ea1624d4206a26ff275b39efc00.png"}我们可以看到，就像一个漏斗一样，进来的水量就好像访问流量一样，而出去的水量就像是我们的系统处理请求一样。当访问流量过大时这个漏斗中就会积水，如果水太多了就会溢出。一般来说，这个"漏斗"是用一个队列来实现的，当请求过多时，队列就会开始积压请求，如果队列满了，就会开拒绝请求。很多系统都有这样的设计，比如TCP。当请求的数量过多时，就会有一个 sync backlog 的队列来缓冲请求，或是TCP 的滑动窗口也是用于流控的队列。![](Images/0ed91ab469499a9a57635b3e3d1a68dc.png){savepage-src="https://static001.geekbang.org/resource/image/d4/a0/d4b8b6ceb8de4400dfc97f3ff0feeaa0.png"}我们可以看到，漏斗算法其实就是在队列请求中加上一个限流器，来让 Processor以一个均匀的速度处理请求。
## 令牌桶算法 Token Bucket关于令牌桶算法，主要是有一个中间人。在一个桶内按照一定的速率放入一些token，然后，处理程序要处理请求时，需要拿到token，才能处理；如果拿不到，则不处理。下面这个图很清楚地说明了这个算法。![](Images/5f6d63eccc545734a9483b7a18134360.png){savepage-src="https://static001.geekbang.org/resource/image/99/f0/996b8d60ed90c470ce839f8826e375f0.png"}从理论上来说，令牌桶的算法和漏斗算法不一样的是，漏斗算法中，处理请求是以一个常量和恒定的速度处理的，而令牌桶算法则是在流量小的时候"攒钱"，流量大的时候，可以快速处理。然而，我们可能会问，Processor的处理速度因为有队列的存在，所以其总是能以最大处理能力来处理请求，这也是我们所希望的方式。因此，令牌桶和漏斗都是受制于Processor的最大处理能力。无论令牌桶里有多少令牌，也无论队列中还有多少请求。总之，Processor在大流量来临时总是按照自己最大的处理能力来处理的。但是，试想一下，如果我们的 Processor只是一个非常简单的任务分配器，比如像 Nginx这样的基本没有什么业务逻辑的网关，那么它的处理速度一定很快，不会有什么瓶颈，而其用来把请求转发给后端服务，那么在这种情况下，这两个算法就有不一样的情况了。漏斗算法会以一个稳定的速度转发，而令牌桶算法平时流量不大时在"攒钱"，流量大时，可以一次发出队列里有的请求，而后就受到令牌桶的流控限制。另外，令牌桶还可能做成第三方的一个服务，这样可以在分布式的系统中对全局进行流控，这也是一个很好的方式。
## 基于响应时间的动态限流上面的算法有个不好的地方，就是需要设置一个确定的限流值。这就要求我们每次发布服务时都做相应的性能测试，找到系统最大的性能值。当然，性能测试并不是很容易做的。有关性能测试的方法请参看我在 CoolShell上的这篇文章《[性能测试应该怎么做](https://coolshell.cn/articles/17381.html)》。虽然性能测试比较不容易，但是还是应该要做的。然而，在很多时候，我们却并不知道这个限流值，或是很难给出一个合适的值。其基本会有如下的一些因素：-   实际情况下，很多服务会依赖于数据库。所以，不同的用户请求，会对不同的数据集进行操作。就算是相同的请求，可能数据集也不一样，比如，现在很多应用都会有一个时间线    Feed 流，不同的用户关心的主题人人不一样，数据也不一样。    而且数据库的数据是在不断变化的，可能前两天性能还行，因为数据量增加导致性能变差。在这种情况下，我们很难给出一个确定的一成不变的值，因为关系型数据库对于同一条    SQL 语句的执行时间其实是不可预测的（NoSQL 的就比 RDBMS    的可预测性要好）。-   不同的 API 有不同的性能。我们要在线上为每一个 API    配置不同的限流值，这点太难配置，也很难管理。-   而且，现在的服务都是能自动化伸缩的，不同大小的集群的性能也不一样，所以，在自动化伸缩的情况下，我们要动态地调整限流的阈值，这点太难做到了。基于上述这些原因，我们限流的值是很难被静态地设置成恒定的一个值。我们想使用一种动态限流的方式。这种方式，不再设定一个特定的流控值，而是能够动态地感知系统的压力来自动化地限流。这方面设计的典范是 TCP 协议的拥塞控制的算法。TCP 使用 RTT - Round TripTime来探测网络的延时和性能，从而设定相应的"滑动窗口"的大小，以让发送的速率和网络的性能相匹配。这个算法是非常精妙的，我们完全可以借鉴在我们的流控技术中。我们记录下每次调用后端请求的响应时间，然后在一个时间区间内（比如，过去10 秒）的请求计算一个响应时间的 P90 或 P99 值，也就是把过去 10秒内的请求的响应时间排个序，然后看 90% 或 99% 的位置是多少。这样，我们就知道有多少请求大于某个响应时间。如果这个 P90 或 P99超过我们设定的阈值，那么我们就自动限流。这个设计中有几个要点。-   你需要计算的一定时间内的 P90 或    P99。在有大量请求的情况下，这个非常地耗内存也非常地耗    CPU，因为需要对大量的数据进行排序。    解决方案有两种，一种是不记录所有的请求，采样就好了，另一种是使用一个叫蓄水池的近似算法。关于这个算法这里我不就多说了，《编程珠玑》里讲过这个算法，你也可以自行    Google，英文叫 [Reservoir    Sampling](https://en.wikipedia.org/wiki/Reservoir_sampling)。-   这种动态流控需要像 TCP 那样，你需要记录一个当前的 QPS.    如果发现后端的 P90/P99 响应太慢，那么就可以把这个 QPS 减半，然后像    TCP 一样走慢启动的方式，直接到又开始变慢，然后减去 1/4 的    QPS，再慢启动，然后再减去 1/8 的 QPS......    这个过程有点像个阻尼运行的过程，然后整个限流的流量会在一个值上下做小幅振动。这么做的目的是，如果后端扩容伸缩后性能变好，系统会自动适应后端的最大性能。-   这种动态限流的方式实现起来并不容易。大家可以看一下 TCP 的算法。TCP    相关的一些算法，我写在了 CoolShell 上的《[TCP    的那些事（下）](https://coolshell.cn/articles/11609.html)》这篇文章中。你可以用来做参考来实现。我在现在创业中的 Ease Gateway 的产品中实现了这个算法。
# 限流的设计要点限流主要是有四个目的。1.  为了向用户承诺    SLA。我们保证我们的系统在某个速度下的响应时间以及可用性。2.  同时，也可以用来阻止在多租户的情况下，某一用户把资源耗尽而让所有的用户都无法访问的问题。3.  为了应对突发的流量。4.  节约成本。我们不会为了一个不常见的尖峰来把我们的系统扩容到最大的尺寸。而是在有限的资源下能够承受比较高的流量。在设计上，我们还要有以下的考量。-   限流应该是在架构的早期考虑。当架构形成后，限流不是很容易加入。-   限流模块性能必须好，而且对流量的变化也是非常灵敏的，否则太过迟钝的限流，系统早因为过载而挂掉了。-   限流应该有个手动的开关，这样在应急的时候，可以手动操作。-   当限流发生时，应该有个监控事件通知。让我们知道有限流事件发生，这样，运维人员可以及时跟进。而且还可以自动化触发扩容或降级，以缓解系统压力。-   当限流发生时，对于拒掉的请求，我们应该返回一个特定的限流错误码。这样，可以和其它错误区分开来。而客户端看到限流，可以调整发送速度，或是走重试机制。-   限流应该让后端的服务感知到。限流发生时，我们应该在协议头中塞进一个标识，比如    HTTP Header    中，放入一个限流的级别，告诉后端服务目前正在限流中。这样，后端服务可以根据这个标识决定是否做降级。