Feature
Binary
indicators
Selected
TLD
ASN
Country
OS
Browser
Reg email
Total
132
3272
99
15
9
5
3532
37
207
19
11
4
4
282
Table 4: Statistics on categorical features.
Our goal here is to select the most relevant features for our prob-
lem. One of the challenges we face is that 6 of the features are
categorical, while the majority (83) are numeric. Among the 6 cate-
gorical features, ASN has 3,272 values, while country has 99 distinct
values. Representing each distinct value with a binary indicator vari-
able results in 3,532 binary features. Inspired by existing methods,
we propose a two-step feature ranking procedure:
1. Ranking categorical features: We apply logistic regression (LR)
with LASSO regularization on the set of binary features created for
all categorical features. Regularization encourages sparse solutions
in which many coefficients are set to zero. Table 4 shows the number
of binary features for our six categorical features, and the number of
features selected by LR.
2. Ranking numerical and binary features: We selected the 83 nu-
merical features and 282 relevant binary features provided by LR, in
total 365 features. For ranking the numerical features, we use the in-
formation gain metric. Among 365 features, 106 had an information
gain greater than 0 and 42 features had a gain above 0.01. We show
the ranking of the top 20 features based on information gain in the
right graph of Figure 2. Interestingly, we observe that representative
features from most categories (communication structure, UA, URL,
content type, result code as well as external features) are ranked in
the top 20 features. Domain age is the highest ranked feature and
three WHOS features are also highly ranked (this is consistent with
previous work [40]).
The top 20 predictors include several enterprise-specific features
that depend on the enterprise’s traffic profiles (e.g., Avg_Conn,
Avg_ratio_rbytes, Max_ratio_rbytes, UA_Popularity, Ratio_UA_hosts).
Among the top 20 predictors the ones that have positive correla-
tion with the malicious class (in decreasing order of their correla-
tion coefficients) are: Frac_ct_empty, Frac_400, Avg_URL_length,
Num_400, Ratio_fail, and Max_URL_length. This confirms that
malicious domains have higher ratio of connections with empty
ACSAC ’18, December 3–7, 2018, San Juan, PR, USA
Alina Oprea, Zhou Li, Robin Norris, and Kevin Bowers
Figure 2: Distribution of WHOIS features (left). Malicious activity by country (center). Ranking of features (right).
Figure 3: Precision, gain, and lift charts for several supervised learning models (top) and random forest with different number of
features (bottom).
content types, more failed connections, and longer URLs than le-
gitimate domains. The predictors correlated with the legitimate
class, again in order of their correlation coefficients, are: Frac_200,
Frac_URL_filename, Distinct_ct, Reg_Age, Reg_Validity, and Up-
date_Validity. Therefore, legitimate domains have higher number
of successful connections, more URLs with file names, serve more
content types, and have higher registration age, registration validity,
and update validity compared to malicious domains.
3.4 Model Selection
Methodology. The main challenge we encountered is that most do-
mains observed in the enterprise traffic are unknown. In our dataset,
benign domains represent 6.87% of traffic, and malicious domains
about 0.24%, while unknown domains are 92.88%. Unless most
previous work that uses classification to distinguish malicious and
benign domains, our operational enterprise setting is quite different:
we aim to prioritize the most suspicious domains among a large
set of unknown domains. A model trained on a small number of
benign and malicious domains will not be successful in prioritizing
the suspicious domains in the unknown class.
With these insights, we propose here a different approach not yet
explored (to the best of our knowledge) in the security community.
We first whitelist the benign domains (Alexa top 10K) and then
focus on prioritizing the malicious domains among the large set of
unknowns. For this task, we adapt the ML framework introduced
at the beginning of this section as follows: We create a training
dataset Dtr = {(x1, y1), . . . ,(xn, yn)} including only malicious and
unknown domains, and set numerical labels yi = 1 for malicious
domains and yi = 0 for unknown domains. We define H the hypoth-
esis space of all functions from domain representations X (the set of
selected features) to predictions Y ∈ [0, 1]. Intuitively, the higher the
prediction value, the more confident we are in the domain’s malicious
status. We aim to learn a supervised model f ∈ H that minimizes a
specific loss function. On the testing set Dtest = {x′
1, . . . , x′
m}, we
= M] = f (x′
predict the probability that x′
i is malicious as: Pr[x′
i).
We leverage several interpretable supervised models: logistic regres-
sion (LR), decision trees (DT), random forest (RF), and SVM. Our
i
MADE: Security Analytics for Enterprise Threat Detection
ACSAC ’18, December 3–7, 2018, San Juan, PR, USA
main insight is that by predicting the probabilities that a domain is
malicious (rather than simply the label), we can also prioritize the
riskier domains based on the predicted probabilities.
We sample 50,000 unknown domains from July and all 1,755
malicious domains observed in February, March, and July to train
our models. We believe that this is a large representative set of
legitimate connections and under-sampling the majority class is a
standard ML technique to handle imbalanced datasets. To perform
model selection, we use standard 10-fold cross validation, selecting
one third of the training set for validation in each fold. All results
are averaged over the 10 folds.
Which metrics? The first question we address is which metrics to
use for evaluating different predictive models. Standard metrics for
classification (AUC, F1 score) are not sufficient since our main goal
is again to prioritize the most suspicious domains. Therefore, we
maximize our prediction quality for a small subset of the targeted
population. Similar techniques are used in targeted advertising, in
which predictions are maximized on the subset of responding popula-
tion. Inspired from that setting, we use the gain and lift metrics. More
specifically, we rank the domains with highest predicted probabilities
by model f and define Dp as the fraction p of domains in the valida-
tion set with highest predictions Dp = Topp[{x, Pr[x = M] = f (x)}]
(where Topp is a function that outputs the pv domains with highest
probabilities, v being the size of validation set). Lift is defined as
the ratio between the precision of the model compared to random
guessing over the target population in Dp, while gain is defined as
the recall in the target population Dp (the fraction of all malicious
domains included in Dp). We also use precision in the target popu-
lation defined as the true positives rate (malicious domains) in set
Dp, and false positive rate (FPR) defined as the false positives in Dp
divided by the entire set of testing domains. According to the SOC
constraints, we set |Dp| to at most 200 domains per month.
Which probabilistic model? Motivated by interpretability consid-
erations, we experiment with four supervised models: LR, DT, RF,
and SVM. The top graphs in Figure 3 show the precision, gain, and
lift for the four models. Notably, the random forest classifier sig-
nificantly outperforms other models for the metrics of interest. The
random forest precision is 92.15% in the top 200 domains (our bud-
get for one month), but the precision of logistic regression, decision
tree, and SVM is only 72.35%, 71.1%, and 75.35% respectively for
the same number of domains. The gain of random forest is at 59.2%
for 10% of the population, but only 48.34%, 38.52%, and 49.81%
respectively, for logistic regression, decision trees, and SVM. The
lift metric is also higher for random forest (at 29.7) in 1% of the
population compared to 23.4 for logistic regression, 19.64 for deci-
sion trees, and 26.1 for SVM. We also experimented with different
number of trees in the random forest model (from 50 to 1000) and
found that 500 trees is optimal.
How many features? Finally, we are interested in the minimum
number of features for optimizing our metrics. We rank the list
of 365 features according to information gain as discussed in Sec-
tion 3.2 and select the top n features for different values of n. We then
train random forest models with n features. The bottom graphs in Fig-
ure 3 show the precision, lift and gain chart for n ∈ {10, 20, 40, 80}
in a random forest model. Precision for the top 200 domains is im-
proved from 88.85% with 10 features to 90.75% with 20 features and
92.15% with 40 features. Gain in 10% of the population is 59.2%
(i.e., 59.2% of all malicious domains are included in the highest
ranked 10% of the population) for 40 features compared to 54.33%
for 10 features, and 57.76% for 20 features. The lift in 1% population
(measuring the improvement in precision over random guessing) is
29.34 for 10 features and 29.7 for 40 features. Interestingly, when
using more than 40 features the prediction accuracy with respect to
all the metrics starts to degrade. This is explained by the fact that
features with rank higher than 40 have low information gain (below
0.01).
Finally, the model with best results in terms of our metrics is a
random forest model using the top 40 highest ranked features and
500 trees. We trained such a model (called RF-40) on the entire
training set and output it at the end of the training phase.
Is MADE efficient? MADE took 14 hours to process and extract
the filtered data from the database for the month of July. MADE took
an additional 2 hours to generate internal features by performing
aggregation of raw events. The process to query external features
took in total 13 hours, split into 9 hours to query WHOIS, 3 hours
to query geolocation information, and one hour to extract features.
After all the features are extracted, training the RF model takes
on average 5 minutes. We thus believe that MADE has reasonable
performance.
4 TESTING AND EVALUATION
In this section, we elaborate our approach to evaluate the effective-
ness of the RF-40 model on new testing data. The testing process
consists of the following steps: Data Representation, Model Predic-
tion, and Ranking High-Risk Communications.
4.1 MADE Testing
Data Representation. For our testing data, we sample a set of 50,000
unknown domains from August, and include all the malicious do-
mains (516). Thus, the testing data is similar in size to our training
set. We extract the 40 selected features from Section 3 and create
the data representation that can be used in the RF-40 model.
Model Prediction. The random forest model RF-40 is applied to the
new testing data. For each domain x visited in the testing interval,
the model computes Pr[x = M] = f (x), the probability that the
domain is malicious. We call these predictions domain risk scores.
We plot the CDF of domain risk scores for malicious and unknown
domains in Figure 4 (left), which clearly demonstrates that domains
labeled as malicious have higher scores than unknown domains.
Ranking High-Risk Communications. We rank the domains in the
testing set according to the predicted risk scores, under the observa-
tion that domains with higher risk scores are more likely malicious.
We generate a list of 1,000 domains with highest risk scores pre-
dicted by the model, and we investigate the top 100 together with
the SOC.
4.2 Evaluation, Analysis, and Feedback
Validation process. We submit all 1,000 domains to VirusTotal and
use additional threat intelligence services (e.g., Cisco Talos) for
high-confidence detections. For the remaining domains ranked in
top 100, we perform manual investigation in collaboration with SOC
analysts. We issue HTTP requests to crawl the domain home page
ACSAC ’18, December 3–7, 2018, San Juan, PR, USA
Alina Oprea, Zhou Li, Robin Norris, and Kevin Bowers
Figure 4: Predicted CDF scores for malicious and unknown domains in the testing set (left). Final precision results (right).
and check if malicious payload is embedded. We also search for
public reports on the domain and for other malicious activities on
the end host using various SOC tools.
Evaluation results. In Figure 4 (right) we illustrate the precision
of RF-40 in the top 200 domains, including 3 lines: (1) Confirmed
VirusTotal (confirmed by VirusTotal); (2) Confirmed other tools
(confirmed by VirusTotal and other threat intelligence services); (3)
Manual investigation (All confirmed by existing services and labeled
malicious by manual investigation). Interestingly, in the top 100 do-
mains, 72 were marked as malicious by VirusTotal, 7 other were
detected by other tools, and 18 were confirmed by manual investiga-
tion. Overall, the MADE precision has reached an impressive 97%
in the top 100 domains, with only 3 false positives in 50,000 testing
domains (corresponding to FPR of 6 · 10−5). In the top 200 domains,
the MADE precision is 76% with FPR 4.8 · 10−4.
These results show that MADE is capable of detecting malicious
domains with high accuracy. Moreover, the prioritization mechanism
in MADE based on the risk score generated by the ML model is
quite effective. The precision of MADE in the top 100 domains is
97%, decreasing to 89.86% in the top 150 domains, and 76% in
the top 200 domains. Therefore, the domains with highest rank are
more likely to be malicious. As another interesting finding, MADE
can also detect new malicious domains that remain undetected by
VirusTotal and other threat intelligence services (MADE detected
a set of 18 such domains in the top 100 prioritized domains). As
shown in Table 1, MADE achieves better precision than existing
systems at similar false positive rates, while detecting more general
classes of enterprise malware communication.
Case study. We describe a malware campaign discovered by manu-
ally investigating the domains of highest score. The adversary regis-
tered 5 domains (keybufferbox.com, globalnodemax.com,
maxdevzone.com, gencloudex.com and bitkeymap.com)
and created 8 or 9 subdomains under each. In total, 645 enterprise
hosts visited at least one such domain within one-month period.
We show details about the infrastructure and operations of this
campaign in Figure 5. The malware is delivered to the victim’s
machine when she visits subdomains under prefix dl.* and
download.*. After the extension is installed, it requests additional
scripts from subdomains notif.*, js.* and app.*. The profile
of the victim is transmitted to logs.*, and the victim’s status and
communication errors are sent to logs.* and errors.*. The
Figure 5: Infrastructure of the malware campaign. * can be any
of the five SLD names.
malware frequently checks update.* for updating itself. All of
the domains are hosted on two IPs owned by Highwinds network.
The names of these domains consist of combinations of three unre-
lated words. According to SOC, this is a new trend in DGA malware
called “dictionary DGA”. Rather than using randomly generated
domain names with high entropy, this new DGA algorithm picks
unrelated dictionary words. This technique specifically evades lexi-
cal features used for DGA detection [10, 40]. Interestingly, MADE
detects these domains since it does not rely on lexical features.
Our algorithm detects 26 campaign domains, due to the following
distinctive features: (1) Infected machines connect frequently to
one of update.* control domains; (2) Domains are recent in the
enterprise traffic; (3) The referer of all requests is either empty or
the domain name itself; (4) A large number of URLs (418) are
served under each domain, and a high number of parameters (7) and
parameter values (72) are embedded in order to send the status of
the infected machines to the control servers. In contrast, legitimate
domains have an average of 50 URLs, 2 parameters, and 3 values.
Operational deployment. MADE was used in production at a large
organization for more than a year and generated prioritized alerts
for the SOC daily. In operation, MADE is re-trained every month
with new labeled data to account for evolution of malicious activ-
ities. MADE proved extremely useful to the SOC by producing
high-quality alerts and detecting new malicious activities. A version
of MADE is currently deployed in a security product and has suc-
cessfully detected malware in other enterprises. The product version
operates in streaming mode and detects malware communication
close to real-time (as soon as 5 connections to an external desti-
nation are made, MADE generates a risk score). We believe that
 dl.*  download.* notif.* js.* app.*  errors.* stats.* Download  extension Download  scripts Status / errors logs.*  Send profile update.*  Check  updates 69.16.175.42|69.16.175.10  54.231.11.114|54.231.18.164 (Amazon)  MADE: Security Analytics for Enterprise Threat Detection
ACSAC ’18, December 3–7, 2018, San Juan, PR, USA
the carefully defined features based on feedback from SOC domain
experts, as well as our methodology for ranking and prioritizing
most suspicious domains are important factors in MADE’s success.
4.3 Discussion and Limitations
MADE is designed to detect malicious communication initiated by
enterprise machines using machine learning techniques. Below we
discuss several cases in which MADE has limited detection ability,
as well as potential adversarial evasion attacks against MADE.
Limitations in detection. MADE is effective in detecting malicious
communication occurring with certain minimum frequency. Cur-
rently, MADE uses a threshold of 5 connections to the same domain
in a two-week interval in order to extract features for that particular
domain. For extremely stealthy attacks such as Advanced Persistent
Threats (APTs), it is feasible for attackers to carefully craft their
malicious connections to remain under the radar.
MADE monitors web connections initiated from within the enter-
prise network. If malware communicates with C&C domains outside
the enterprise network, that activity will not be recorded by the web
proxies. MADE applies various enterprise-specific whitelists and
considers for analysis only unpopular and recently visited domains.
MADE is therefore not designed to detect C&C communication to
well-established domains, such as cloud services and CDN domains.
At the same time, MADE also filters connections to reputable adver-
tisement networks. Motivated attackers could in theory compromise
a well-established domain or a reputable ad network and use it for
C&C communication. Designing defenses against these threats re-
mains a challenging tasks because of the difficulty of distinguishing
malicious and benign connections directed to highly-reputable web
sites.
MADE extracts the majority of features from HTTP logs collected
from enterprise web proxy servers. In the enterprise of our study,
web proxies intercept and act as man-in-the-middle for the majority
of HTTPS connections. In general though, MADE will have access
to a much smaller set of features for malicious communication over
HTTPS (for instance, the URL, user-agent, referer, and content type
features will not be available). We have not yet investigated the effec-