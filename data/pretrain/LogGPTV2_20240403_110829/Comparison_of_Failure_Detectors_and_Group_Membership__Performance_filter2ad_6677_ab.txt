not yet delivered, and installs P
as the next view. The protocol for joins and explicit leaves
is very similar.
, U
(cid:1)
(cid:1)
State transfer. When a process joins a group, its state
needs to be synchronized with the other members of the
group. What “state” and “synchronizing” exactly mean is
application dependent. We only need to deﬁne these terms
in a limited context: in our study, the only processes that
ever join are correct processes which have been wrongly
excluded from the group. Consequently, the state of such a
process p is mostly up-to-date. For this reason, it is feasible
to update the state of p the following way: when p rejoins,
it asks some process for the messages it has missed since
it was excluded. Process p delivers these messages, and
then starts to participate in the view it has joined. Note that
this only works because our atomic broadcast algorithm is
uniform: with non-uniform atomic broadcast, the excluded
process might have delivered messages never seen by the
others, thus having an inconsistent state. In this case, state
transfer would be more complicated.
4.4 Expected performance
We now discuss, from a qualitative point of view, the
expected relative performance of the two atomic broadcast
algorithms (FD algorithm and GM algorithm).
Figure 1 shows executions with neither crashes nor sus-
picions. In terms of the pattern of message exchanges, the
two algorithms are identical: only the content of messages
differ. Therefore we expect the same performance from the
two algorithms in failure free and suspicion-free runs.
Let us now investigate how the algorithms slow down
when a process crashes. There are two major differences.
The ﬁrst is that the GM algorithm reacts to the crash of
every process, while the FD algorithm reacts only to the
crash of p1, the ﬁrst coordinator. The other difference is that
the GM algorithm takes a longer time to re-start delivering
atomic broadcast messages after a crash. This is true even
if we compare the GM algorithm to the worst case for the
FD algorithm, i.e., when the ﬁrst coordinator p1 fails. The
FD algorithm needs to execute Round 2 of the consensus
algorithm. This additional cost is comparable to the cost
of an execution with no crashes (3 communication steps,
1 multicast and about 2n unicast messages). On the other
hand, the GM algorithm initiates an expensive view change
(5 communication steps, about n multicast and n unicast
messages). Hence we expect that if the failure detectors de-
tect the crash in the same time by the two algorithms, the
FD algorithm performs better.
Consider now the case when a correct process is wrongly
suspected. The algorithms react to a wrong suspicion the
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:12:39 UTC from IEEE Xplore.  Restrictions apply. 
same way as they react to a real crash. Therefore we expect
that if the failure detectors generate wrong suspicions at the
same rate, the FD algorithm will suffer less performance
penalty.
5 Context of our performance study
5.1 Performance measures
Our main performance measure is the latency of atomic
broadcast. Latency L is deﬁned for a single atomic broad-
cast as follows. Let A-broadcast(m) occur at time t0, and A-
deliver(m) on pi at time ti, for each i = 1, . . . , n. Then la-
tency is deﬁned as the time elapsed until the ﬁrst A-delivery
of m, i.e., L
In our study, we
compute the mean for L over a lot of messages and several
executions.
def= (mini=1,...,n ti) − t0.
This performance metric makes sense in practice. Con-
sider a service replicated for fault tolerance using active
replication [23]. Clients of this service send their requests to
the server replicas using Atomic Broadcast. Once a request
is delivered, the server replica processes the client request,
and sends back a reply. The client waits for the ﬁrst reply,
and discards the other ones (identical to the ﬁrst one). If we
assume that the time to service a request is the same on all
replicas, and the time to send the response from a server to
the client is the same for all servers, then the ﬁrst response
received by the client is the response sent by the server to
which the request was delivered ﬁrst. Thus there is a direct
link between the response time of the replicated server and
the latency L.
Latency is always measured under a certain workload.
We chose simple workloads: (1) all destination processes
send atomic broadcast messages at the same constant rate,
and (2) the A-broadcast events come from a Poisson stochas-
tic process. We call the overall rate of atomic broadcast
messages throughput, denoted by T . In general, we deter-
mine how the latency L depends on the throughput T .
5.2 Scenarios
We evaluate the latency of the atomic broadcast algo-
rithms in various scenarios. We now describe each of the
scenarios in detail, mentioning which parameters inﬂuence
latency in the scenario. Parameters that inﬂuence latency in
all scenarios are the algorithm (A), the number of processes
(n) and the throughput (T ).
Steady state of the system. We measure latency after it
stabilizes (a sufﬁciently long time after the start of the sys-
tem or after any crashes). We distinguish three scenarios,
based on whether crashes and wrong suspicions (failure de-
tectors suspecting correct processes) occur:
• normal-steady: Neither crashes nor wrong suspicions
in the experiment.
• crash-steady: One or several crashes occur before
the experiment. Beside A, T and n, an additional pa-
rameter is the set of crashed processes. As we assume
that the crashes happened a long time ago, all fail-
ure detectors in the system permanently suspect all
crashed processes at this point. No wrong suspicions
occur.
• suspicion-steady: No crashes, but failure detectors
generate wrong suspicions, which cause the algorithms
to take extra steps and thus increase latency. Beside
A, T and n, additional parameters include how of-
ten wrong suspicions occur and how long they last.
These parameters are discussed in detail in Section 6.2.
It would be meaningful to combine the crash-steady and
suspicion-steady scenarios, to have both crashes and wrong
suspicions. We omitted this case, for we wanted to observe
the effects of crashes and wrong suspicions independently.
Transient state after a crash.
In this scenario we force
a crash after the system reached a steady state. After the
crash, we can expect a halt or a signiﬁcant slowdown of the
system for a short period. In this scenario, we deﬁne la-
tency such that it reﬂects the latency of executions that are
affected by the crash and thus happen around the moment
of the crash. Also, we must take into account that not all
crashes affect the system the same way; our choice is to
consider the worst case (the crash that slows down the sys-
tem most). Our deﬁnition is the following:
• crash-transient: Consider that a process p crashes at
time t (neither crashes nor wrong suspicions occur,
except for this crash). We have process q (p (cid:1)= q) ex-
ecute A-broadcast(m) at t. Let L(p, q) be the mean
latency of m, averaged over a lot of executions. Then
def= maxp,q∈P L(p, q), i.e., we consider the
Lcrash
crash that affects the latency most. In this scenario,
we have one additional parameter, describing how
fast failure detectors detect the crash (discussed in
Section 6.2).
We could combine the crash-transient scenario with the
crash-steady and suspicion-steady scenarios, to include other
crashes and/or wrong suspicions. We omitted these cases,
for we wanted to observe the effects of (i) the recent crash,
(ii) old crashes and (iii) wrong suspicions independently.
Another reason is that we expect the effect of wrong suspi-
cions on latency to be secondary with respect to the effect
of the recent crash: wrong suspicions usually happen on a
larger timescale.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:12:39 UTC from IEEE Xplore.  Restrictions apply. 
6 Simulation models
Our approach to performance evaluation is simulation,
which allowed for more general results as would have been
feasible to obtain with measurements in a real system (we
can use a parameter in our network model to simulate a va-
riety of different environments). We used the Neko proto-
typing and simulation framework [24] to conduct our exper-
iments.
6.1 Modeling the execution environment
We now describe how we modeled the transmission of
messages. We use the model of [5], inspired from sim-
ple models of Ethernet networks [25]. The key point in
the model is that it accounts for resource contention. This
point is important as resource contention is often a limiting
factor for the performance of distributed algorithms. Both
a host and the network itself can be a bottleneck. These
two kinds of resources appear in the model (see Fig. 2): the
network resource (shared among all processes) represents
the transmission medium, and the CPU resources (one per
process) represent the processing performed by the network
controllers and the layers of the networking stack, during
the emission and the reception of a message (the cost of
running the algorithm is neglectable). A message m trans-
mitted for process pi to process pj uses the resources (i)
CPUi, (ii) network, and (iii) CPUj, in this order. Message
m is put in a waiting queue before each stage if the corre-
sponding resource is busy. The time spent on the network
resource is our time unit. The time spent on each CPU re-
source is λ time units; the underlying assumption is that
sending and receiving a message has a roughly equal cost.
The λ parameter (0 ≤ λ) shows the relative speed of
processing a message on a host compared to transmitting it
over the network. Different values model different network-
ing environments. We conducted experiments with a variety
of settings for λ.
Process pi
send
receive
7
Process pj
t
s
o
h
g
n
i
d
n
e
s
CPU
i
(λ time units)
1
2
3
m
4
CPU
j
(λ time units)
6
5
t
s
o
h
g
n
i
v
i
e
c
e
r
Network (1 time unit)
Figure 2. Transmission of a message in our
network model.
Crashes are modelled as follows. If a process pi crashes
at time t, no messages can pass between pi and CPUi after
t; however, the messages on CPUi and the attached queues
are still sent, even after time t. In real systems, this cor-
responds to a (software) crash of the application process
(operating system process), rather than a (hardware) crash
of the host or a kernel panic. We chose to model soft-
ware crashes because they are more frequent in most sys-
tems [26].
6.2 Modelling failure detectors
One approach to modeling a failure detector is to use a
speciﬁc failure detection algorithm and model all its mes-
sages. However, this approach would restrict the generality
of our study: another choice for the algorithm would likely
give different results. Also, it is not justiﬁed to model the
failure detector in so much detail, as other components of
the system, like the execution environment, are modelled
in much less detail. We built a more abstract model in-
stead, using the notion of quality of service (QoS) of failure
detectors introduced in [6]. The authors consider the fail-
ure detector at a process q that monitors another process p,
and identify the following three primary QoS metrics (see
Fig. 3):
p
trust
FD at q
up
trust
suspect
mistake duration
TM
mistake recurrence time
TMR
t
t
suspect
Figure 3. Quality of service metrics for failure
detectors. Process q monitors process p.
Detection time TD: The time that elapses from p’s crash
to the time when q starts suspecting p permanently.
Mistake recurrence time TMR: The time between two con-
secutive mistakes (q wrongly suspecting p), given that
p did not crash.
Mistake duration TM : The time it takes a failure detector
component to correct a mistake, i.e., to trust p again
(given that p did not crash).
Not all of these metrics are equally important in each of
our scenarios (see Section 5.2). In Scenario normal-steady,
the metrics are not relevant. The same holds in Scenario
crash-steady, because we observe the system a sufﬁciently
long time after all crashes, long enough to have all fail-
ure detectors to suspect the crashed processes permanently.
In Scenario suspicion-steady no crash occurs, hence the la-
tency of atomic broadcast only depends on TMR and TM . In
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:12:39 UTC from IEEE Xplore.  Restrictions apply. 
Scenario crash-transient no wrong suspicions occur, hence
TD is the relevant metric.
In [6], the QoS metrics are random variables, deﬁned
on a pair of processes. In our system, where n processes
monitor each other, we have thus n(n − 1) failure detec-
tors in the sense of [6], each characterised with three ran-
dom variables. In order to have an executable model for the
failure detectors, we have to deﬁne (1) how these random
variables depend on each other, and (2) how the distribution
of each random variable can be characterized. To keep our
model simple, we assume that all failure detector modules
are independent and the tuples of their random variables are
identically distributed. Moreover, note that we do not need
to model how TMR and TM depend on TD, as the two for-
mer are only relevant in Scenario suspicion-steady, whereas
TD is only relevant in Scenario crash-transient. In our ex-
periments, we considered various settings for TD, and var-
ious settings for combinations of TMR and TM . As for the
distributions of the metrics, we took the simplest possible
choices: TD is a constant, and both TMR and TM are expo-
nentially distributed with (different) constant parameters.
Note that these modelling choices are not realistic: sus-
picions from different failure detectors are probably corre-
lated. Our study only represents a starting point, as we are
not aware of any previous work we could build on (apart
from [6] that makes similar assumptions). We will reﬁne
our models as we gain more experience.
7 Results
We now present the results for all four scenarios. Due
to lack of space, we only present the results obtained with
λ = 1 in this paper. Note that in current LANs, the time
spent on the CPU is higher than the time spent on the wire,
and thus λ > 1. Results for such values of λ are presented
in [19]. Most graphs show latency vs.
throughput. For
easier understanding, we set the time unit of the network
simulation model to 1 ms. The 95% conﬁdence interval is
shown for each point of the graph. The two algorithms were
executed with 3 and 7 processes, to tolerate 1 and 3 crashes,
respectively.
Normal-steady scenario (Fig. 4).
In this scenario, the two
algorithms have the same performance. Each curve thus
shows the latency of both algorithms.
Crash-steady scenario (Fig. 5). For both algorithms, the
latency decreases as more processes crash. This is due to
the fact that the crashed processes do not load the network
with messages. The GM algorithm has an additional fea-
ture that improves performance:
the sequencer waits for
fewer acknowledgements, as the group size decreases with
the crashes. By comparison, the coordinator in the FD al-
gorithm always waits for the same number of acknowledg-
n = 3
n = 7
70
60
50
40
30
20
10
]
s
m
[
y
c
n