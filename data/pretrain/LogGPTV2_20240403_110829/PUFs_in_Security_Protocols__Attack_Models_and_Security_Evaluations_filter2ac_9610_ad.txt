re-use model, the PAM. Our attack rests on the following
assumptions:
1) After the initialization phase of the OT-Protocol 1, dif-
ferent subsessions of the protocol are run. We assume
that there is a subsession ssid with the following
properties:
• Eve was able to eavesdrop the binary communi-
cation between the sender and the receiver in the
subsession ssid.
• Eve can read-out CRPs from the PUF after the
end of the subsession ssid, for example before
a new subsession ssid(cid:3) is started. (Note that this
assumption is derived from the PAM.)
Under these provisions, Eve can learn both bits s0 and s1
used by the sender in subsession ssid. This breaks the
security of this subsession. The attack works as follows:
1) When the subsession ssid is run, Eve eavesdrops the
messages in Steps 3, 4 and 6. She therefore learns
the values x0, x1, v (:= c ⊕ xb), S0 (:= s0 ⊕ r0) and
S1 (:= s1 ⊕ r1). Thereby r0 and r1 are the responses
to the challenges c0(:= v ⊕ x0) and c1(:= v ⊕ x1).
2) When Eve has got physical access to the PUF after the
subsession ssid, she computes the challenges c0 :=
v ⊕ x0 and c1 := v ⊕ x1 herself. She applies these
challenges to the PUF, and obtains the responses r0
and r1.
3) Eve derives s0 and s1 by computing the values S0⊕r0
= s0 ⊕ r0 ⊕ r0 = s0 and S1 ⊕ r1 = s1 ⊕ r1 ⊕ r1 = s1.
This breaks the security of the subsession ssid.
Please note that the role of Eve can also be played by a
malicous receiver. Interestingly, an attacker cannot learn the
receiver’s choice bit b by a similar attack, since the secrecy
of the choice bit is unconditional and does not rest on the
employed PUF.
B. OT-Protocol of Brzuska et al. in the Bad PUF Model
Let us now describe an attack on the OT-Protocol of
Brzuska et al. [1] (see Protocol 1 in Appendix B) in the
bad PUF model, which works under the following single
assumption:
1) The receiver can hand over a simulatable bad PUF
instead of a normal PUF in the initialization phase,
and furthermore possesses a simulation algorithm for
this PUF.
The attack itself works as follows:
1) The receiver follows Protocol 1 as speciﬁed, and
carries out a subsession sid.
2) When the subsession is completed, the receiver com-
putes the two challenges c0 := v⊕x0 and c1 := v⊕x1.
He can do so since he knows v, x0 and x1 from earlier
protocol steps.
293
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:54:17 UTC from IEEE Xplore.  Restrictions apply. 
3) The receiver uses his simulation algorithm in order
to compute the two responses r0 and r1 which corre-
spond to the challenges c0 and c1.
4) The receiver derives both values s0 and s1 by com-
puting S0 ⊕ r0 = s0 ⊕ r0 ⊕ r0 = s0 and S1 ⊕ r1 =
s1⊕r1⊕r1 = s1. He can do so since he knows S0, S1
from step 6 of the OT-protocol.
The Sender hence learns both strings s0 and s1, breaking the
security of the protocol. We comment that the attack only
requires the use of simulatable PUFs by the receiver, which
are particularly easy to implement.
C. KE-Protocol of Brzuska et al. in the PUF Re-Use Model
We describe below how the KE-Protocol of Brzuska et al.
[1] (see Protocol 2 in Appendix C) can be attacked in the
PUF re-use model, or more, precisely, in its mildest form,
the PAM. The attack is quite straightforward and rests on
the following assumptions:
1) After the initialization phase of Protocol 2, different
subsessions of the protocol are run. We assume that
there is a subsession ssid with the following proper-
ties:
• Eve was able to eavesdrop the binary commu-
nication between the Alice and the Bob in the
subsession ssid.
• Eve can read-out CRPs from the PUF after the
end of the subsession ssid, for example before a
new subsession ssid(cid:3) is started.
Under these provisions, Eve can learn the exchanged key K.
The attack is relatively obvious and works as follows:
1) When the subsession ssid is run, Eve eavesdrops step
2 and learns the values c and d.
2) When Eve has got physical access to the PUF after
subsession ssid, she applies the challenge c to the
(cid:3), and derives
PUF, measures the (noisy) response r
(cid:3) by the help of d.
the secret st from r
As st = K in subsession ssid, this breaks the security of
this subsession.
D. KE-Protocol of Brzuska et al. in the Combined PUF Re-
Use and Bad PUF Model
Let us continue examining the security of the KE-Protocol
of Brzuska et al. [1] (Protocol 2 in Appendix C). Since in a
simple stand-alone scenario neither Alice nor Bob have an
incentive to use bad PUFs, this is a welcome opportunity to
illustrate the impact of a combined attack model: namely a
combination of the PUF re-use and the bad PUF model.
We make the following assumptions:
1) The KE protocol is executed between Alice and Bob
(including an initialization phase and an arbitrary
number of subsessions), and later between Bob and
Claire (again including an initialization phase with the
same PUF and later subsessions).
294
2) Alice plays maliciously, and uses a challenge-logging
PUF in her initialization phase.
Under this assumption, Alice can learn the key exchanged
by Bob and Claire as follows:
1) When the PUF is in transition from Bob to Claire
in step 3 of the initialization phase of their protocol,
Alice gains physical access to the PUF.
2) Alice reads out the last previously applied challenge
c, applies it to the PUF, and obtains response r.
3) In the next subsession phase, Alice intercepts the
helper data d that is sent from Bob to Claire in step
2.
4) Alice utilizes her knowledge of r and d to infer st =
K.
Alice hence learns the key K exchanged by Bob and
Claire, breaking the protocol. Let us mention a few simple
modiﬁcations of the attack: Alice could alternatively use a
simulatable PUF (instead of a CL-PUF), leading to a similar
attack. If the PUF is obtained by Alice from a third party
manufacturer, then the manufacturer can mount the same
attack by using CL- or simulatable PUFs. Finally, if an
external adversary Eve is able to add a challenge logger
while the PUF is in transit from Alice to Bob, then she
can derive both Alice’s and Bob’s key as well as Bob’s and
Claire’s key by reading out the challenge logger when the
PUF is in transit from Bob to Claire. The details of these
variants are similar to the attack above and are left to the
reader.
E. An Unconditional BC-Protocol of Ostrovsky et al. in the
Bad PUF Model
Ostrovsky et al. [18] describe an unconditional BC-
protocol (i.e., one that does not use any additional compu-
tational assumptions) in Fig. 6 of their paper (see Protocol
3 in Appendix D). The protocol is purportedly secure under
the use of malicious/bad PUFs. However, we describe below
a very simple bad PUF type that allows cheating. The bad
PUF is not even required to communicate with the malicious
party.
Our attack makes the following assumption:
1) The committer Cuncon uses/initializes a bad PUF in
step 1 of the protocol (instead of a good PUF). The bad
PUF has the following feature: Instead of outputting
essentially random and unpredictable responses,
it
implements a linear function in its challenge-response
behavior: For a ﬁxed value X, it maps every challenge
C to the output R := C ⊕ X.
The attack then proceeds as follows:
1) The committer Cuncon initializes the above bad PUF
in step 1 of the protocol.
2) In order to open to bit b = 0 in the decommitment
phase, the committer sends the challenge st ⊕ X. In
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:54:17 UTC from IEEE Xplore.  Restrictions apply. 
order to open to bit b=1, the committer sends the
challenge r ⊕ st ⊕ X.
In the ﬁrst case, the (bad) PUF outputs st⊕ X ⊕ X =
st, meaning that the receiver accepts the opening phase
for b = 0.
In the second case, the (bad) PUF outputs r ⊕ st ⊕
X ⊕ X = r ⊕ st, implying that the receiver accepts
the opening phase for b = 1.
This breaks the security of the protocol under the assump-
tion that bad/malicious PUFs can be used. The attack does
not require elaborate bad PUF behavior (such as communi-
cation between the PUF and the malicious party), as already
remarked above, but can be implemented relatively simply.
Instead of using the speciﬁc linear function mentioned
above, the attack can also be built on any other bad PUF
that implements an arbitrary, efﬁciently invertible function
f in its challenge-response behavior.
F. Security of Other Protocols in the PUF Re-Use Model
and Bad PUF Model
For reasons of brevity, we focused on the three above
protocols in our detailed security analysis. Other Strong PUF
protocols for OT, BC or KE can be attacked in similar
manners. Since the attacks are analog to the work in the
above sections, we merely sketch and summarize them in
the following list:
1) The OT-protocol of R¨uhrmair [22] is no longer secure
in the bad PUF and PUF re-use model, and similar
considerations hold for the key exchange protocol of
van Dijk [5]. This can be seen relatively easily, since
the attacks are essentially equivalent to the attacks in
the last subsetions on Brzuska et al.’s OT and KE
protocol.
2) It is not too difﬁcult to see that the unconditional OT-
protocol of Ostrovsky et al. for honest PUFs (see Fig.
7 of [18]) is not secure in the PUF re-use model. If the
receiver of the protocol gets access to the used PUFs
after the end of the protocol, he can learn both strings
s0 and s1.
Furthermore, if the sender uses bad, challenge-logging
PUFs instead of honest PUFs as sidS
2k, then
he can obviously learn the value of the bi, which
allows him to derive the the choice bit b from the
(cid:3)
ij which he receives in step 4 of the protocol.
values b
Actually, even something weaker sufﬁces: If only one
j for j ∈ S is challenge logging, then
of the PUFs sidS
b is revealed. Since S ⊂ [2k] is a randomly chosen
subset of size k, the latter condition can be enforced
by merely making k + 1 of the 2k PUFs challenge
logging. In other words, the attack also works if only
a strict subset of all PUFs are bad.
1, . . . , sidS
3) The statistically hiding, straight-line extractable bit
commitment scheme of Fig. 10 of Ostrovsky et al.
[18], and the statistically binding, straight-line ex-
tractable equivocal commitment scheme of Fig. 11 of
the same paper, can be attacked by communicating
bad PUFs, which maliciously transfer the challenges
applied to the PUF in the commit phase to the receiver.
This allows the receiver to learn the committed bit
before the reveal phase.
We stress once more that Ostrovsky et al. seem to
implicitly assume that
there is no communication
between the malicious party and the PUF, i.e., we are
again extending the original attack model of Ostrovsky
et al. here. However, as discussed earlier, Communi-
cating PUFs seem hard to prevent in certain settings. If
they are considered realistic, then also the construction
for UC-secure computation of Ostrovsky et al., which
is built on the commitment schemes in Figs. 10 and
11 of [18], breaks down.
G. Summary of Our Security Discussion
To summarize, all Strong PUF protocols for OT, BC and
KE examined in this paper can be attacked in variants of the
PUF re-use model, the bad PUF model, or the combined
PUF re-use, bad PUF model. Only one of these attacks
(see item 3 of Section III-F above) requires Communicat-
ing PUFs, which are somewhat complex. The majority of
attacks, however, can be carried out in simple variants of
the bad PUF model, using simulatable or challenge-logging
PUFs, or straight away in the ordinary PUF re-use model.
We stress again that most of the attacks work outside the
attack scenarios and communication models of the original
papers, but we argued in Section II why we consider the
new models realistic. One notable exception is the attack
on Ostrovsky’s unconditional bit commitment scheme in the
malicous PUF model (see Section III-E), which actually
works in the original attack model of Ostrovsky et al.
The authors of this paper are not aware of any PUF
protocols for OT, BC or KE which can withstand all
said attack models, and in which (i) plain Strong PUFs
with no additional hardware properties are used, (ii) no
additional assumptions (set-up assumptions, classical com-
putational assumptions, etc.) apart from the security (i.e.,
unpredictability) of the Strong PUF are made. This illustrates
the acuteness of re-thinking current PUF protocol design.
IV. CONSEQUENCES, OR: THE NEED FOR ERASABLE
AND CERTIFIABLE PUFS
What are the consequences of the observations of the last
sections? The ﬁrst and foremost implication is that attack
models for PUF protocols should be reconsidered. PUFs are
different from other cryptographic primitives in that they are
real pieces of hardware that can have all kinds of malicious
properties. Future protocol design and security analyses must
take this into account.
295
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:54:17 UTC from IEEE Xplore.  Restrictions apply. 
One potential route to evade some of our attacks has
been considered by Ostrovsky et al. in [18]. They combine
three steps to construct secure PUF-protocols in the presence
of malicious/bad PUFs: (i) They allow additional, standard
computional assumptions in the protocols. (ii) They assume
that the PUF cannot communicate with the malicious party,
in particular, that the PUF is no Marionette PUF and no