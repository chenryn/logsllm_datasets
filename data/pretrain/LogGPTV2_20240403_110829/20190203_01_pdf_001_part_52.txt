### 8.3.47 Removing a Subscription

**Synopsis:**
```
-removesub subname –repsvrfile subsvrfile
```
For more information on removing a subscription, see Section 5.5.5.

**Parameters:**
- **subname**: The name of the subscription to be removed.
- **subsvrfile**: The file containing the subscription server login information.

**Example:**
To remove a subscription named `dept_emp_sub`:
```bash
$ java -jar edb-repcli.jar -removesub dept_emp_sub -repsvrfile ~/subsvrfile.prop
Removing subscription...
Subscription removed successfully.
```

---

### 8.3.48 Scheduling Shadow Table History Cleanup (confcleanupjob)

**Synopsis:**
```
-confcleanupjob pubdbid –repsvrfile pubsvrfile { -disable | -enable { -minutely no_of_minutes | -hourly no_of_hours | -daily hour | -weekly day_of_week hour | -cronexpr "cron_expression" } }
```
- If the `-disable` parameter is specified, the existing shadow table history cleanup schedule is deleted. No other parameters except `pubdbid` and `pubsvrfile` can be specified in this case.
- If the `-disable` parameter is omitted, the `-enable` parameter and one of the following parameters must be specified: `-minutely`, `-hourly`, `-daily`, `-weekly`, or `-cronexpr`.

For more information on creating a schedule for shadow table history cleanup, see Section 7.5.1.

**Parameters:**
- **pubdbid**: Publication database ID of the publication database definition for which a schedule is to be enabled or disabled for deleting shadow table history.
- **pubsvrfile**: The file containing the publication server login information.
- **-disable**: Removes any existing shadow table history cleanup schedule from the publication database definition.
- **-enable**: Establishes a schedule for shadow table history cleanup.
- **no_of_minutes**: The number of minutes between scheduled shadow table history cleanup jobs (integer between 1 and 59).
- **no_of_hours**: The number of hours between scheduled shadow table history cleanup jobs (integer between 1 and 12).
- **hour**: The hour of the day based on a 24-hour clock (integer from 0 to 23).
- **day_of_week**: The day of the week (SUNDAY, MONDAY, TUESDAY, WEDNESDAY, THURSDAY, FRIDAY, SATURDAY). Case insensitive.
- **cron_expression**: A cron expression. See Section 10.4.3 for information on writing a cron expression.

**Examples:**
1. Schedule shadow table history cleanup to run every 3 hours:
   ```bash
   $ java -jar edb-repcli.jar -confcleanupjob 1 -repsvrfile ~/pubsvrfile.prop -enable -hourly 3
   Configuring cleanup job ...
   Cleanup job configured.
   ```

2. Schedule shadow table history cleanup to run daily at 6:00 PM:
   ```bash
   $ java -jar edb-repcli.jar -confcleanupjob 1 -repsvrfile ~/pubsvrfile.prop -enable -daily 18
   Configuring cleanup job ...
   Cleanup job configured.
   ```

3. Schedule shadow table history cleanup to run every Wednesday at 8:00 AM:
   ```bash
   $ java -jar edb-repcli.jar -confcleanupjob 1 -repsvrfile ~/pubsvrfile.prop -enable -weekly WEDNESDAY 8
   Configuring cleanup job ...
   Cleanup job configured.
   ```

4. Disable the shadow table history cleanup job:
   ```bash
   $ java -jar edb-repcli.jar -confcleanupjob 1 -repsvrfile ~/pubsvrfile.prop -disable
   Configuring cleanup job ...
   Cleanup job removed.
   ```

---

### 8.3.49 Cleaning Up Shadow Table History (cleanshadowhistforpub)

**Synopsis:**
```
-cleanshadowhistforpub pubname –repsvrfile pubsvrfile [ -mmrdbid dbid_1[,dbid_2 ] ...]
```
For more information on cleaning up shadow table history, see Section 7.5.2.

**Parameters:**
- **pubname**: The name of the publication for which the shadow table history is to be deleted.
- **pubsvrfile**: The file containing the publication server login information.
- **-mmrdbid dbid_n**: For MMR only: The publication database ID of the master node for which the shadow table history is to be deleted. This parameter is required for a multi-master replication system, specifying one or more comma-separated publication database IDs. Note: There must be no white space between the comma and publication database IDs.

**Example:**
To delete the shadow table history for publication `dept_emp`:
```bash
$ java -jar edb-repcli.jar -cleanshadowhistforpub dept_emp -repsvrfile ~/pubsvrfile.prop
Removing shadow table's transaction history ...
Shadow table's transaction history removed successfully.
```

---

### 8.3.50 Cleaning Up Replication History (cleanrephistoryforpub)

**Synopsis:**
```
-cleanrephistoryforpub pubname –repsvrfile pubsvrfile
```
For more information on cleaning up replication history, see Section 7.5.3.

**Parameters:**
- **pubname**: The name of the publication for which replication history is to be deleted.
- **pubsvrfile**: The file containing the publication server login information.

**Example:**
To delete the replication history for publication `dept_emp`:
```bash
$ java -jar edb-repcli.jar -cleanrephistoryforpub dept_emp -repsvrfile ~/pubsvrfile.prop
Removing publication's replication history ...
Replication history has been removed.
```

---

### 8.3.51 Cleaning Up All Replication History (cleanrephistory)

**Synopsis:**
```
-cleanrephistory –repsvrfile pubsvrfile
```
For more information on cleaning up replication history, see Section 7.5.3.

**Parameters:**
- **pubsvrfile**: The file containing the publication server login information.

**Example:**
To delete the replication history for all publications in the publication server identified by the content of file `pubsvrfile.prop`:
```bash
$ java -jar edb-repcli.jar -cleanrephistory -repsvrfile ~/pubsvrfile.prop
Removing all publication's replication history ...
Replication history has been removed.
```

---

### 9. Data Validator

The Data Validator is a utility that compares the rows of one or more tables within a schema of a database against the rows of the tables with the same names within a schema of another database. It generates a summary of the comparison, noting the number of rows whose column values differ, and creates a file with detailed information about any differences.

**Databases:**
- **Source Database**: Can be Oracle, EnterpriseDB, SQL Server, Sybase, or MySQL.
- **Target Database**: Must be Oracle or EnterpriseDB (Advanced Server or PostgreSQL).

**Tables:**
- Only tables found in the schema of the source database are compared. Tables in the target database that do not exist in the source database schema are ignored.

**Note:**
- The Data Validator does not validate columns with the following data types: BFILE, STRUCT, REF, ARRAY, BLOB, CLOB, RAW, LONG RAW.
- Ensure all synchronization replication between the source and target xDB Replication Server tables is completed before using the Data Validator.

### 9.1 Installation and Configuration

**Step 1: Installation**
- When you install the xDB Replication Server product, the Data Validator components are also installed. See Chapter 3 for installation details.
- Uninstalling the xDB Replication Server product will also uninstall the Data Validator components.

**Components Installed:**
| File Name | Location | Description |
|-----------|----------|-------------|
| datavalidator.properties | XDB_HOME/etc | Data Validator Properties file |
| runValidation.sh (Linux) | XDB_HOME/bin | Data Validator execution script |
| runValidation.bat (Windows) | XDB_HOME\bin | Data Validator execution script |

**Note:**
- XDB_HOME is the directory where xDB Replication Server is installed. This may or may not be the same as the Postgres home directory, depending on the installation.

**Step 2: Oracle JDBC Driver**
- If you plan to use an Oracle database as the source or target, download the Oracle JDBC driver and place it in the `JAVA_HOME/jre/lib/ext` directory.

**Step 3: Configure the Properties File**
- Edit the `datavalidator.properties` file located in the `XDB_HOME/etc` directory to specify the connection information for the source and target databases.

**Properties File Parameters:**
| Parameter | Description |
|-----------|-------------|
| source_dbms | Type of the source database (enterprisedb, oracle, sqlserver, sybase, mysql) |
| source_host | IP address or server name of the host running the source database server |
| source_port | Port number on which the source database server listens for requests |
| source_database | Database name of the source database |
| source_user | Database user name of the source database |
| source_password | Unencrypted password of the source database user |
| target_dbms | Type of the target database (enterprisedb, oracle) |
| target_host | IP address or server name of the host running the target database server |
| target_port | Port number on which the target database server listens for requests |
| target_database | Database name of the target database |
| target_user | Database user name of the target database |
| target_password | Unencrypted password of the target database user |

**Initial Content of `datavalidator.properties`:**
```properties
###############################################################
# Source database connection #
###############################################################
source_dbms=oracle
source_host=localhost
source_port=1521
source_database=xe
source_user=hr
source_password=hr

###############################################################
# Target database connection #
###############################################################
target_dbms=enterprisedb
target_host=localhost
target_port=5444
target_database=edb
target_user=enterprisedb
target_password=edb
```

**Step 4: Determine the Logs Directory**
- Before invoking the Data Validator for the first time, determine the location for the logs directory.
- The Data Validator generates a log file with a name formatted as `datavalidator_yymmdd-hhmiss.log` in the logs directory for each run.
- If there are row differences, a file with a name formatted as `datavalidator_yymmdd-hhmiss.diff` is generated, containing the errors in diff format. Use a graphical diff tool like Kompare to view this file.

**Choices for Setting the Logs Directory:**
- **Run as Root**: Run the Data Validator as the root account to create the `logs` subdirectory within `XDB_HOME/bin`.
- **Create Directory Manually**: Create the `XDB_HOME/bin/logs` directory structure and modify permissions so the operating system account used to run the Data Validator has the privilege to create files in the directory.