(cid:12)elds indicating the network storage location of the tuples
are underlined. We begin our discussion by looking only at
the part of the query written in bold text, ignoring the rest
of the text for a moment.
NR1: path(S,D,P,C) :- link(S,D,C),
.
NR2: path(S,D,P,C) :- link(S,Z,C1), path(Z,D,P2,C2),
.
.
Query: path(S,D,P,C).
C = C1 + C2,
P = f concatP ath(link(S,Z,C1),P2).
P = f concatP ath(link(S,D,C), nil).
Rule NR1 produces one-hop paths from existing link tu-
ples, storing them at the source node. Rule NR2 recursively
produces path tuples of increasing cost by matching the desti-
nation (cid:12)elds of existing links to the source (cid:12)elds of previously
computed paths. The rule Query speci(cid:12)es the output of inter-
est (i.e. result tuples), which are the path tuples stored at the
source node. The matching is expressed using the two \Z"
(cid:12)elds in link(S; Z; C1) and path(Z; D; P2; C2) in rule NR2.
Intuitively, rule NR2 says that if there is a link from node S
to node Z, and there is a path from node Z to node D, then
there is a path from node S to node D via Z.
((cid:13)link.(cid:13)S(cid:13), path.D, f_concatPath(link(S,Z,C),(cid:13)
path.P), link.C+path.C) as path(S,D,P,C)(cid:13)
path.S(cid:13)
link.D=path.S(cid:13)
link.D(cid:13)
link((cid:13)S(cid:13),D,C)(cid:13)
path((cid:13)S(cid:13),D,P,C)(cid:13)
Figure 2: Query Plan for the Network-Reachability Query.
Rule NR2 requires a relational join ./) operator to match
the destination (cid:12)elds of link tuples (link:D) with the source
(cid:12)elds of existing path tuples (path:S). For each pair of link
and path tuples that matches, the join operator produces a
new path tuple that is the concatenation of the original path
and link tuples. The projection (cid:25) operator takes as input the
output of the join and a list of (cid:12)elds, extracts these (cid:12)elds from
the join’s output, and optionally renames them. This ensures
that only the required path (cid:12)elds speci(cid:12)ed in the query are
generated as path tuples. Unlike most textbook query plans,
this data(cid:13)ow forms a cycle, which captures the recursive use
of the path rule de(cid:12)nition in the query.
The clouds represent the forwarding of tuples from one net-
work node to another, and are labeled with the destination
node. The (cid:12)rst cloud (link:D) ships link tuples to the neigh-
bor nodes indicated by their destination address (cid:12)elds, in or-
der to join with matching path tuples stored by their source
address (cid:12)elds. The second cloud (path:S) ships new path tu-
ples computed from the join back to their neighboring source
nodes for further processing.
3.4 Query Plan Execution
Next, we focus on the distributed execution of a routing
query. To simplify the exposition, we will temporarily ignore
the mechanism for initially disseminating the Datalog query
to the network nodes; we return to this issue in Section 3.5.
Upon receipt of the Datalog query, each node creates the
query plan shown in Figure 2 and starts executing the plan
for the duration of the query. When the query plan is ex-
ecuted, the (cid:13)ow of tuples in the network enables nodes to
exchange the routing information necessary to compute the
queried paths. Figure 3 shows the tuples that are generated
during the execution of the query plan in Figure 2 for a sim-
ple network consisting of (cid:12)ve nodes. p(S; D; P; C) abbreviates
path(S; D; P; C). Link costs in our example are set to 1, and
l0(S; D; C)
hence path cost is equal to the number of hops.
refers to link tuples that are sent by node S and cached at
destination node D. We show only the new path tuples (in
bold) generated at each iteration.
p((cid:13)a(cid:13),b,[a,b],1) (cid:13)
p((cid:13)a(cid:13),c,[a,c],1)(cid:13)
a(cid:13)
p((cid:13)a(cid:13),d,[a,b,d],2), (cid:13)
p((cid:13)a(cid:13),d,[a,c,d],2)(cid:13)
a(cid:13)
p((cid:13)b(cid:13),d,[b,d],1)(cid:13)
l(cid:13)’(cid:13)(a,(cid:13)b(cid:13),1)(cid:13)
b(cid:13)
p((cid:13)c(cid:13),d,[c,d],1)(cid:13)
l(cid:13)’(cid:13)(a,(cid:13)c(cid:13),1)(cid:13)
c(cid:13)
p((cid:13)b(cid:13),e,[b,d,e],2)(cid:13)
l(cid:13)’(cid:13)(a,(cid:13)b(cid:13),1)(cid:13)
b(cid:13)
c(cid:13)
p((cid:13)c(cid:13),e,[c,d,e],2)(cid:13)
l(cid:13)’(cid:13)(a,(cid:13)c(cid:13),1)(cid:13)
d(cid:13)
p((cid:13)d(cid:13),e,[d,e],1),(cid:13)
l(cid:13)’(cid:13)(b,(cid:13)d(cid:13),1) (cid:13)
l(cid:13)’(cid:13)(c,(cid:13)d(cid:13),1)(cid:13)
e(cid:13)
1(cid:13)st(cid:13)Iteration(cid:13)
d(cid:13)
l(cid:13)’(cid:13)(b,(cid:13)d(cid:13),1), (cid:13)
l(cid:13)’(cid:13)(c,(cid:13)d(cid:13),1)(cid:13)
e(cid:13)
2(cid:13)nd(cid:13) Iteration(cid:13)
Figure 3: Nodes in the network are running the query plan
in Figure 2. The dashed lines represent the control plane
(along which tuples are sent), while the full lines represent
the data plane (along which data packets are forwarded).
For clarity, we describe the communication in stages, where
each stage or iteration represents a \round of communica-
tion", in which all nodes exchange tuples from the previous
iteration. The rounds of communication is a simpli(cid:12)cation of
actual query execution: since data(cid:13)ow is fully asynchronous,
tuples for the next round can be generated as soon as tuples
from the previous round are computed. Each iteration repre-
sents the traversal of a \cloud" in Figure 2. The (cid:12)rst iteration
derives single-hop path tuples from the (cid:12)rst rule of the query.
It does this by traversing the link:D cloud, which ships link
tuples to the address in their destination (cid:12)eld, where they are
cached for the duration of the query (denoted by l0(S; D; C) in
Figure 3)1. Since the query has no recursion on the link table,
all subsequent iterations involve the other cloud (path:S).
In the 2nd iteration, the link tuples are joined with existing
one-hop path tuples to produce two-hop path tuples. These
tuples are then sent back to the source nodes (the path:S
cloud) and three-hop path tuples are computed. Once the
query reaches a node, the node takes up to k iterations to
converge to a steady state, where k is the diameter of the net-
work. Including the initial query dissemination which takes
up to k iterations to reach the node furthest from the query
node, the total time taken for the query to converge is pro-
portional to 2k. To illustrate further, we step through the
communication necessary for the computing the path tuple
p(a; d; [a; c; d]; 2) for node a:
1st iteration: Node a ships l(a; c; 1) to c (via the link:D
cloud running at node a). It is stored as l0(a; c; 1) at node c
for the duration of the query.
2nd iteration: Node c receives l0(a; c; 1) and performs the
join of l0(a; c; 1) and p(c; d; [c; d]; 1) to produce the new path
tuple
p(a; d; [a; c; d]; 2). This new tuple is sent back to node a (the
path:S cloud running at node c).
3.5 Query Dissemination
Queries can be disseminated to nodes in a variety of ways.
In static scenarios, the query may be \baked in" to another
artifact { e:g:; router (cid:12)rmware or peer-to-peer application
software. More (cid:13)exibly, the query could be disseminated
upon initial declaration.
It may be su(cid:14)cient to perform
dissemination via (cid:13)ooding, particularly if the query will be
long-lived, amortizing the cost of the initial (cid:13)ood. As an op-
timization, instead of (cid:13)ooding the query in the network, we
can instead \piggy-back" dissemination onto query execution:
the query can be embedded into the (cid:12)rst data tuple sent to
each neighboring node as part of the query computation. The
piggy-back mechanism has the advantage that nodes that are
not involved in the query computation will not receive the
query. These scenarios arise in some of the examples in Sec-
tion 5 below, such as creating paths that avoid certain nodes.
3.6 Path Vector or Distance Vector Protocol
The computation of the above query resembles the compu-
tation of the routing table in a path vector or distance vector
protocol. The computation starts with the source comput-
ing its initial reachable set (which consists of all neighbors
of the source) and shipping it to all its neighbors. In turn,
each neighbor updates the reachable set with its own neigh-
borhood set, and then forwards the resulting reachable set to
its own neighbors. With minor modi(cid:12)cations to our previous
query (modi(cid:12)cations in bold), the following Distance-Vector
query expresses the distance vector computation:
DV1: path(S,D,D,C) :- link(S,D,C).
DV2: path(S,D,Z,C) :- link(S,Z,C1),
.
DV3:
DV4: nextHop(S,D,Z,C) :- path(S,D,Z,C),
.
Query: nextHop(S,D,Z,C).
shortestCost(S,D,min) :- path(S,D,Z,C).
path(Z,D,W,C2), C = C1 + C2.
shortestCost(S,D,C).
Aggregate constructs are represented as functions with ar-
guments within angle brackets (<>). DV1 and DV2 are mod-
i(cid:12)ed from the original rules NR1 and NR2 to ensure that the
path tuple maintains only the next hop on the path, rather
than the entire path vector itself2. DV3 and DV4 are added
to set up routing state in the network: nextHop(S,D,Z,C) is
stored at node S, where Z is the next hop on the shortest path
to node D. The main di(cid:11)erence between this query and the
actual distance vector computation is that rather than send-
ing individual path tuples between neighbors, the traditional
distance vector method batches together a vector of costs for
all neighbors.
By making a modi(cid:12)cation to rule DV2 and adding rule DV5,
we can apply the well-known split-horizon with poison re-
verse [20] (cid:12)x to the count-to-in(cid:12)nity problem:
#include(DV1,DV3,DV4)
DV2: path(S,D,Z,C) :- link(S,Z,C1),
.
DV5: path(S,D,Z,1) :- link(S,Z,C1), path(Z,D,S,C2).
Query: nextHop(S,D,Z,C).
path(Z,D,W,C2), C = C1 + C2, W 6= S.
#include is a macro used to include earlier rules. Rule
DV2 expresses that if node Z learns about the path to D
from node S, then node Z does not report this path back to
to S. Rule DV5 expresses that if node Z receives a path tuple
with destination D from node S, then node Z will send a path
with destination D and in(cid:12)nite cost to node S. This ensures
that node S will not eventually use Z to get to D.
4. CHALLENGES
We have identi(cid:12)ed four challenges that need to be addressed
in justifying the feasibility of declarative routing:
1As an optional optimization for undirected graphs, the operation
of shipping link tuples can be avoided by adding an extra rule
link(S,D,C) :- link(D,S,C).
2The W (cid:12)eld in DV2 represents the next-hop to node D from
intermediate node Z, and can be ignored by node S in computing
its next hop to node D.
Expressiveness: How expressive and (cid:13)exible is the Datalog
language in expressing various routing policies? What are the
limitations of this language?
Security: Is Datalog safe enough to execute queries issued
by untrusted third-parties?
E(cid:14)ciency: Can Datalog queries be executed e(cid:14)ciently in a
distributed system? The answer to this question hinges on
two sub-questions. The (cid:12)rst is about raw performance: can
plan generation techniques be adapted or developed to enable
Datalog queries to perform well in a large network system?
The second is about the feasibility of exploiting our extensible
framework: given that we allow many routing queries to be
issued concurrently, can we signi(cid:12)cantly reduce the redundant
work performed by these concurrent queries?
Stability and Robustness: Given that the network is dy-
namic, how can we e(cid:14)ciently maintain the robustness and
accuracy of long term routes?
We address these challenges in the next four sections.
5. EXPRESSIVENESS
To highlight the expressiveness of Datalog, we provide sev-
eral examples of useful routing protocols expressed as queries.
Our examples range from well-known routing protocols (dis-
tance vector, dynamic source routing, multicast, etc.)
to
higher-level routing concepts such as QoS constraints. This
is by no means intended to be an exhaustive coverage of the
possibilities of our proposal. Our main goal here is to illus-
trate the natural connection between recursive queries and
network routing, and to highlight the (cid:13)exibility, ease of pro-
gramming, and ease of reuse a(cid:11)orded by a query language.
We demonstrate that routing protocols can be expressed in
a few Datalog rules, and additional protocols can be created
by simple modi(cid:12)cations (in bold) to previous examples.
5.1 Best›Path Routing
We start from the base rules NR1 and NR2 used in our (cid:12)rst
Network-Reachability example from Section 3. That example
computes all-pairs paths. In practice, a more common query
would compute all-pairs shortest paths. By modifying NR2
and adding rules BPR1 and BPR2, the following Best-Path
query generalizes the all-pairs shortest paths computation,
and computes the best paths for any path metric C:
#include(NR1)
NR2: path(S,D,P,C) :- link(S,Z,C1),
.
.
BPR1: bestPathCost(S,D,AGG) :- path(S,D,P,C).
BPR2: bestPath(S,D,P,C) :- bestPathCost(S,D,C),
.
Query: bestPath(S,D,P,C).
path(Z,D,P2,C2), C = f compute(C1,C2),
P = f concatP ath(link(S,Z,C1),P2),
path(S,D,P,C).
We have left the aggregation function (AGG) unspeci(cid:12)ed.
By changing AGG and the function f compute used for com-
puting the path cost C, the Best-Path query can generate
best paths based on any metric including link latency, avail-
able bandwidth and node load. For example, if the query
is used for computing the shortest paths, f sum is the ap-
propriate replacement for f compute in rule BPR1, and min
is the replacement for AGG. The resulting bestPath tuples
are stored at the source nodes, and are used by end-hosts to
perform source routing.
The two added rules BPR1 and BPR2 do not result in extra
messages being sent beyond those generated by rules NR1 and
NR2. This is because path tuples computed by rules NR1 and
NR2 are stored at the source nodes, and bestPathCost and
bestPath tuples are generated locally at those nodes. Instead
of computing the best path between any two nodes, this query
can be easily modi(cid:12)ed to compute all paths, any path or the
Best-k paths between any two nodes.
Similar to the Network-Reachability example, we can add
an extra predicate f inP ath(P2; S) = f alse to rule NR2
to avoiding computing best paths with cycles. We can fur-
ther extend the rules from the Best-Path query by including
constraints that enforce a QoS requirement speci(cid:12)ed by end-
hosts. For example, we can restrict the set of paths to those
with costs below a loss or latency threshold k by adding an
extra constraint C<k to the rules NR1 and NR2.
5.2 Policy›Based Routing
Our previous example illustrates a typical network-wide rout-
ing policy. In some cases we may want to restrict the scope of
routing, e:g:; by precluding paths that involve \undesirable"
nodes. An example would be (cid:12)nding a path among nodes in
an overlay network such as PlanetLab that avoids nodes be-
longing to untruthful or (cid:13)aky ISPs. Such policy constraints
can be simply expressed by adding an additional rule:
#include(NR1, NR2)
PBR1: permitPath(S,D,P,C) :- path(S,D,P,C),
.
Query: permitPath(S,D,P,C).
excludeNode(S,W), f inP ath(P; W ) = f alse.
In this query, we introduce an additional table excludeNode,
where excludeN ode(S; W ) is a tuple that represents the fact
that node S does not carry any tra(cid:14)c for node W. This table
is stored at each node S.
If BPR1 and BPR2 are included as rules, we can generate
bestPath tuples that meet the above policy. Other policy
based decisions include ignoring the paths reported by se-
lected nodes or insisting that some paths have to pass through
(or avoid) one or multiple pre-determined set of nodes.
5.3 Dynamic Source Routing
All of our previous examples use what is called right recur-
sion, since the recursive use of path in the rule (NR2, DV2)
appears to the right of the matching link. The query se-
mantics do not change if we (cid:13)ip the order of path and link
in the body of these rules, but the execution strategy does
change. In fact, using left recursion as follows, we implement
the Dynamic Source Routing (DSR) protocol [17]:
#include(NR1)
DSR1: path(S,D,P,C) :- path(S,Z,P1,C1), link(Z,D,C2),
.
.