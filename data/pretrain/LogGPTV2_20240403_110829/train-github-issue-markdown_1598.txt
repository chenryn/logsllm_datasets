Example:
    >>> import transformers
    >>> t_fast = transformers.AutoTokenizer.from_pretrained("bert-base-cased", use_fast=True, add_special_tokens=False)
    >>> sentence = "A, naïve [MASK] AllenNLP sentence."
    >>> tokenized = t_fast.encode_plus(sentence, add_special_tokens=False, return_offsets_mapping=True)
    >>> for start, end in tokenized['offset_mapping']:
    ...     print(repr(sentence[start:end]))
    'A'
    ','
    'naïve'
    ' [MASK'
    ' Alle'
    'nN'
    'L'
    ' sentenc'
    'e'
As you can see, after the word "naïve", the offsets go off the rails.