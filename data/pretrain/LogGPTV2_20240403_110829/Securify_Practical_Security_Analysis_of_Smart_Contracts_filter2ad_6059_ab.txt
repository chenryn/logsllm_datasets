31
32
33
34
35
36
Integer underﬂow
Re-entrancy vulnerability with balance change
Unprotected usage of selfdestruct
Contracts that lock ether
Incorrect ERC20 interface
Multiple calls in a single transaction
Re-entrancy vulnerability without balance change
Unprotected usage of tx.origin
Unused return
Usage of low level calls
Exception state
Local variable shadowing
Assembly usage
State variables that could be declared as constant
Unindexed ERC20 event parameters
Usage of complex pragma statement
Built-in symbol shadowing
Calls inside a loop
Conformance to solidity naming conventions
Constant functions changing the state
Dangerous strict equalities
Delegate a proxy call
External call to a ﬁxed address
Functions that send ether to arbitrary destinations
Integer overﬂow
Re-entrancy-vulnerability in general
State variable shadowing
State variable shadowing from abstract contracts
Uninitialized local variables
Uninitialized state variables
Unused state variables
Uninitialized storage variables
Unprotected ether withdrawal
Usage of deprecated standards
Usage of different solidity versions
Usage of predictable seed for random generation
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
Slither: This analyzer uses taint analysis to track value in
Solidity code analysis and can ﬁnd 31 types of vulnerabilities
with four different impact levels (high, medium, low, and
informational) [5]. In our dataset, 30 out of these 31 types
of vulnerabilities were present.
One of the challenges we encountered when using two
different analyzers in static code analysis was the syntactic
and semantic discrepancies between the two analyzers. In this
process, we found ﬁve vulnerabilities that were syntactically
different but same in the semantics. As shown in Table II, 3rd,
8th, 10th, 22nd, and 36th vulnerabilities were detectable by
both Mythril and Slither. We have also observed that for a
semantically consistent vulnerability, two analyzers reported
conﬂicting results. We addressed this problem by creating
additional labels for the intersected vulnerabilities as described
in the next subsection.
D. Labeling
In this step, 36 types of vulnerabilities (as listed in Table II),
that we had previously extracted using static code analysers,
were used to label the smart contracts. If a vulnerability was
detected by the static code analyzer, we labeled the vulnera-
bility as present, otherwise as not present. For instance, if the
source code of a smart contract contained Integer overﬂow
vulnerability, we considered Integer overﬂow as present. We
also labeled the testing set to generate ground truth labels
that were used to evaluate the performance of our model. As
mentioned in Section III-C, Mythril and Slither used different
methods to spot one type of vulnerability. To capture the
discrepancy, We created ﬁve new security vulnerabilities as
the combined values of security vulnerabilities detected by
either analyzers. When either analyzers reported the presence
of the vulnerability, we labeled the combined value as present.
Otherwise we labeled it as not present. If both analyzers had
identiﬁed one vulnerability, the label for that vulnerability
was considered present. For example,
if Mythril found a
vulnerability in the code while Slither did not detect such
vulnerability in the same code, we labeled the vulnerability as
present. Therefore, the number of labels for each intersected
vulnerability increased to three different labels. As a result, the
36 labels listed in Table II increased to 46 (31 labels for unique
vulnerabilities plus 15 labels for 5 intersected vulnerabilities).
E. Classiﬁcation
We selected four commonly used supervised binary classi-
ﬁers: Support Vector Machine (SVM), Neural Network (NN),
Random Forest (RF) and Decision Tree (DT) to train our
model. Since security vulnerabilities were independent to each
other, we trained our binary classiﬁers for each vulnerability
separately. In total, we trained 184 (4 ∗ 46) binary classiﬁers.
Support Vector Machine: For SVM, we tuned three param-
eters: the value of C, the kernel method, and the value of γ.
C is a regularization parameter that determines the size of the
margin [21]. In this study, we searched the range from 10−2
to 103 to ﬁnd the optimal C. Kernel method is responsible for
choosing the type of separator plane between two classes [22].
We used a linear hyper-plane and rbf which was a non-linear
hyper-plane. The last parameter is γ which controls the impact
of each sample. The γ parameter was set to vary from 10−3
to 103 in this model.
Neural Network: Four parameters are tuned for Neural Net-
work. The number of layers and the number of nodes on each
layer. In our model we had the following three layer structures
((5, 40, 60, 80, 60, 40), (3, 35, 50, 35), (1, 2)), where the ﬁrst
number is the number of the layers, and the ith number is the
number of nodes in the (i−1)th layer. The second parameter is
α which is the L2 penalty (regularization term) parameter [21].
The optimal value of α was searched in the range from
10−4 to 103. The third parameter is the activation function
between layers [21]. To ﬁnd the best classiﬁcation results
we tested identity, ReLU (rectiﬁed linear unit) and tanh
activation functions [23]. The last parameter to tune is the
solver which is the optimization algorithm to ﬁnd the wights of
each layer [21]. We used lbf gs (quasi-Newton methods), sgd
(stochastic gradient descent), and adam (stochastic gradient-
based optimizer) optimization methods [21].
Decision Tree: There are three parameters to tune in the
decision tree algorithm. The ﬁrst parameter is max depth.
This parameter deﬁnes the depth of the tree [24]. We examined
values ranging from 1 to 100 in this model. The second
parameter that we investigated is min samples split. This
parameter deﬁnes the minimum number or percentage of
necessary samples in each node in order to get split [21].
We tried different min samples split ranging from 10%
to 100%. The third parameter min samples leaf which
determines the minimum number or percentage of samples
in each leaf. In our model 10% to 50% were tested [21].
Random Forest: This algorithm has one additional parameter
compare to decision tree: N estimators. This parameter is an
estimator and determines the number of trees in the forest [21],
[25]. For tuning the random forest classiﬁer, we used numbers
from 1 to 100 for the number of trees in the forest.
IV. RESULTS AND DISCUSSION
In this section, we ﬁrst introduce the metrics used for the
model evaluation. Then we present the results of the evaluation
and discuss the implication of using our ML-based security
vulnerability analysis model.
A. Evaluation Metrics
We adopted the evaluation metrics of accuracy, recall, and
precision. Accuracy is the most common metric used in
machine learning evaluations. Due to the fact that our dataset
is imbalanced (the number of vulnerable smart contracts are
scarce compared to the secure smart contracts), we could
not use accuracy as the only metric for evaluation. Recall
and precision are the other two metrics frequently adopted
to evaluate a binary classiﬁcation model. Recall (also known
as sensitivity) measures the positive cases that are correctly
predicted as positive [26] and precision (also known as conﬁ-
dence) measures true positives from the predicted positives. In
our model, there were two types of errors. The ﬁrst type was
the number of false negatives (the vulnerabilities existed in
the code but the analyzer could not detect them). Minimizing
this error was prioritized as ﬁnding all potential vulnerabilities
was the main purpose of our analyzer. The second type was
the number of false positive errors (an analyzer marks a
secure code as vulnerable). This error could greatly affect the
effectiveness of the checking process. Therefore, we found F1-
score a reliable measure for our model since it posses all the
essential qualities and is compatible with imbalanced data.
B. Evaluation Results
We report the experimental results in Table III. In each
row in this table, we have listed a vulnerability and the
machine learning algorithm that yield the best results based
on the metrics described above. For instance,
the Integer
underﬂow is identiﬁed by the Decision Tree algorithm with
F1-score of 86%, accuracy of 99%, precision of 100%, and
recall of 75%. The reasonable accuracy and precision of
our model in ﬁnding vulnerabilities show the possibility of
utilizing machine learning to ﬁnd security vulnerabilities in
smart contracts. From the results of our model, we make the
following conclusions.
Software security vulnerabilities are related to the code
complexity and the code structure patterns. The features we
used in this model represent the complexity of the code as well
as the patterns of code structure. For example, lines of code
(LOC) alone serves as an indicator to determine the code com-
plexity. In addition, combining LOC with other features such
as the number of function deﬁnitions can determine the code
ACCURACY OF SECURITY VULNERABILITY DETECTION
TABLE III
Code
Security Problem
Severity
Method
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
Integer underﬂow
Re-entrancy vulnerability with balance change
Unprotected usage of selfdestruct
High
High
High
Medium
Contracts that lock ether
Medium
Incorrect ERC20 interface
Multiple calls in a single transaction
Medium
Re-entrancy vulnerability without balance change Medium
Medium
Unprotected usage of tx.origin
Medium
Unused return
Usage of low level calls
Medium
Exception state
Local variable shadowing
Low
Low
Assembly usage
State variables that could be declared as constant
Unindexed ERC20 event parameters
Usage of complex pragma statement
Informational
Informational
Informational
Informational
SVM
NN
DT
RF
SVM
DT
DT
SVM
DT
DT
SVM
SVM
NN
SVM
NN
SVM
TN
163
129
166
130
133
159
144
166
128
164
128
144
152
70
164
165
FP
FN
TP
F1
Accuracy
Precision
Recall
0
10
0
0
12
2
5
0
11
1
6
7
4
28
0
0
1
6
0
6
4
2
7