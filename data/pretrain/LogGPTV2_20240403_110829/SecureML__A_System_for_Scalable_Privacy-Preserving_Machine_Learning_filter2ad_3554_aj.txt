(cid:8) do
3:
4:
5:
6:
7:
1
((cid:7)b3(cid:8)B,(cid:7)b4(cid:8)B) ← Y2B(GarbledCircuit((cid:7)u(cid:8)0 +
2 ;(cid:7)u(cid:8)1, f )), where f sets b1 as the most
2 ,(cid:7)u(cid:8)0 − 1
signiﬁcant bit of ((cid:7)u(cid:8)0 + 1
2 ) + (cid:7)u(cid:8)1 and b2 as the
most signiﬁcant bit of ((cid:7)u(cid:8)0 − 1
2 ) + (cid:7)u(cid:8)1. It then
outputs b3 = ¬b1 and b4 = b1 ∧ (¬b2).
0 · (cid:7)u(cid:8)0 + r1 and m1 = (1 −
S0 sets m0 = (cid:7)b4(cid:8)B
(cid:7)b4(cid:8)B
1 ) ←
0 ) · (cid:7)u(cid:8)0 + r1. S0 and S1 run (⊥; m(cid:4)b4(cid:5)B
OT(m0, m1;(cid:7)b4(cid:8)B
0 ⊕
is equal to ((cid:7)b4(cid:8)B
(cid:7)b4(cid:8)B
1 ) · (cid:7)u(cid:8)0 + r1 = b4 · (cid:7)u(cid:8)0 + r1.
1 · (cid:7)u(cid:8)1 + r2 and m1 = (1 −
P1 sets m0 = (cid:7)b4(cid:8)B
(cid:7)b4(cid:8)B
0 ) ←
1 ) · (cid:7)u(cid:8)1 + r2. S1 and S0 run (⊥; m(cid:4)b4(cid:5)B
OT(m0, m1;(cid:7)b4(cid:8)B
0 is equal to b4·(cid:7)u(cid:8)1+r2.
0 ). m(cid:4)b4(cid:5)B
S0 sets m0 = (cid:7)b3(cid:8)B
0 +r3 and m1 = (1−(cid:7)b3(cid:8)B
0 )+r3.
1 ) ← OT(m0, m1;(cid:7)b3(cid:8)B
S0 and S1 run (⊥; m(cid:4)b3(cid:5)B
1 ).
m(cid:4)b3(cid:5)B
S0 sets (cid:7)y
−r1−r3 and S1 sets (cid:7)y
∗(cid:8)1 =
m(cid:4)b4(cid:5)B
∗(cid:8)0 = m(cid:4)b4(cid:5)B
− r2.
1 is equivalent to b3 + r3.
1 ). m(cid:4)b4(cid:5)B
1 + m(cid:4)b3(cid:5)B
1
1
0
8: end for
9: Both parties set (cid:7)Y∗(cid:8)i as a vector of all (cid:7)y
∗(cid:8)is
computed above and continue to step 6–12 in
Figure 4.
Fig. 13: Privacy preserving logistic regression protocol.
2E multiplication triplets. In particular, for each feature of each
sample, the client possessing the data generates a random value
(cid:3)
k for
u to mask the data, and generates random values vk, v
k = 1, . . . , E and computes zk = u · vk, z
k = u · v
(cid:3)
(cid:3)
k. Finally,
the client distributes shares of (cid:7)u(cid:8),(cid:7)vk(cid:8),(cid:7)v
k(cid:8),(cid:7)zk(cid:8),(cid:7)z
k(cid:8) to the
(cid:3)
(cid:3)
two servers.
Notice that we do not assume the clients know the partition-
ing of the data possession when generating the triplets. This
means that we can no longer utilize the vectorized equation for
the online phase. For example, in Section IV-A, in the forward
propagation at step 5 of Figure 4, where we compute XB × w,
we use precomputed matrix multiplication triplets of U×V = Z
with exactly the same dimensions as the online phase. Now,
when the multiplication triplets are generated by the clients,
the data in the mini-batch XB may belong to different clients
who may not know they are in the same mini-batch of the
training, and thus cannot agree on a common random vector
V to compute Z.
Instead, for each data sample x in XB, the two parties
((cid:7)x(cid:8),(cid:7)w(cid:8)) using independently generated
compute (cid:7)y
∗(cid:8)s.
multiplication triplets, and set (cid:7)Y∗(cid:8) to be a vector of (cid:7)y
Because of this, the computation, communication of the online
phase and the storage of the two servers are increased.
∗(cid:8) = Mul
A
The client-aided multiplication triplets generation signiﬁ-
cantly improves the efﬁciency of the ofﬂine phase, as there is
no cryptographic operation involved. However, it introduces
overhead to the online phase. The matrix multiplications are
37
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:22:48 UTC from IEEE Xplore.  Restrictions apply. 
our OT-based approach is faster than garbled circuits by a
factor of 5 − 8× on LAN, and a factor of 4 − 5× on WAN
networks. The typical number of multiplications in parallel,
needed to train on our datasets is close to 100,000. Though our
LHE-based approach is much slower than using garbled circuits
on LAN networks, and is comparable on WAN networks, we
will show in the next microbenchmark that the LHE-based
approach beneﬁts the most from vectorization, which makes it
even faster than our OT-based approach on WAN networks.
Note that if the client-aided ofﬂine phase is used, the speedup
is more signiﬁcant. Typically it only takes milliseconds in total
for k = 10, 000. However, as the security model is weakened
when using client-aided multiplication triplets, we did not
compare it directly with the ﬁxed-point multiplication.
d
Online
100
500
1000
100
500
1000
0.37ms
1.7ms
3.5ms
0.2s
0.44s
0.62s
Online
Vec
0.22ms
0.82ms
1.7ms
0.09s
0.20ss
0.27s
LAN
WAN
OT
0.21s
1.2s
2.0s
14s
81s
154s
OT
Vec
0.05s
0.28s
0.46s
3.7s
19s
34s
LHE
67s
338s
645s
81s
412s
718s
LHE
Vec
1.6s
5.5s
10s
2s
6.2s
11s
TABLE VII: Speedup from vectorization. |B| = 128.
Vectorization. Table VII shows the speedup gained from
vectorization. As a basic operation in our training, we need to
multiply a shared |B|×d matrix and a d×1 vector. |B| is set to
128 and d varies from 100 to 1000. In the unoptimized version,
we compute the inner product between the vector and each
row of the matrix; in the vectorized version, we compute the
matrix-vector multiplication. As shown in Table VII, the online
time is improved by around 2×. The OT-based ofﬂine phase
is improved by 4×. The LHE-based ofﬂine phase is improved
by 41 − 66× and it is faster than the OT-based ofﬂine phase
on WAN networks because of the vectorization.
New Logistic
LAN
WAN
0.0045s
0.2s
Poly Total
Client-aided
0.0005s
0.69s
Poly Total
OT
0.025s
2.5s
Poly Total
LHE
6.8s
8.5s
TABLE VIII: Performance of our new logistic function and
polynomial approximation.
New logistic function. We compare the cost of calculating
our new logistic function with approximating the logistic
function using a degree 10 polynomial. For the polynomial
approximation, we use our scheme for decimal multiplications
and compute it using 9 sequential multiplications using the
Horner’s rule. Table VIII shows the running time for 128
parallel evaluations of the function (just to amortize the effect
of network delay). As shown in the table, unless using client-
aided multiplication triplets in LAN networks, which weakens
the security model, our new logistic function is dramatically
faster than using polynomial approximation (3.5 − 1511×).
Cholesky
SGD
MNIST
Gisette
92.02% 96.7%
91.95% 96.5%
Arcene
87%
86%
TABLE V: Comparison of accuracy for SGD and Cholesky.
LAN
WAN
k = 1000
k = 10, 000
k = 100, 000
k = 1000
k = 10, 000
k = 100, 000
Total (OT)
0.028s
0.16s
1.4s
1.4s
12.5s
140s
Total (LHE)
5.3s
53s
512s
6.2s
62s
641s
GC
0.13s
1.2s
11s
5.8s
68s
552s
TABLE VI: Comparison of our decimal multiplication and the
ﬁxed-point multiplication using garbled circuit.
replaced by vector inner products. Though the total number
of multiplications performed is exactly the same, matrix
multiplication algorithms are in general faster using matrix
libraries in modern programming languages. This is the major
overhead introduced by the client-aided approach as depicted
in the experiments.
The communication is also increased. Previously, the coefﬁ-
cient vector is masked by a single random vector to compute a
single matrix multiplication, while now it is masked multiple
times by different random vectors for each inner products.
These masked values are transferred between the two parties
in the secure computation protocol. In particular, the overhead
compared to the protocols in Section IV is t·(2|B|·d−|B|−d)
for linear and logistic regressions. this is not signiﬁcant in the
LAN setting but becomes important in the WAN setting.
Finally, the storage is also increased. Previously, the matrix
V and Z is much smaller than the data size and the matrix
U is of the same size as the data. Now, as the multiplication
triplets are generated independently, the size of V becomes
|B|· d· t = n· d· E, which is larger than the size of the data by
a factor of E. The size of U is still the same, as each data is
still masked by one random value, and the size of Z is still the
same because the values can be aggregated once the servers
collect the shares from all the clients.
Despite of all these overheads, the online phase is still very
efﬁcient, while the performance of the ofﬂine phase is im-
proved dramatically. Therefore, the privacy preserving machine
learning with client-aided multiplication triplets generation is
likely the most promising option for deployment in existing
machine learning frameworks.
APPENDIX G
MICROBENCHMARKS
In addition to evaluating the end-to-end performance of our
system, we present microbenchmarks to show the effect of our
major optimizations individually in this section.
Arithmetic on shared decimal numbers. Table VI compares
the performance of our new scheme for decimal multiplications
with that of ﬁxed-point multiplication using garbled circuit. We
run k multiplications in parallel and compare the total time of
our scheme (online + OT-based ofﬂine and online + LHE-based
ofﬂine) to GC-based ﬁxed-point multiplication, with a 16-bit
integer part and a 16-bit decimal part. As shown in the table,
38
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:22:48 UTC from IEEE Xplore.  Restrictions apply.