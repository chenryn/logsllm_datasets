e
m
T
i
0
128
256
PP Linear 1
PP Linear 2
384
512
|B|
Fig. 7: Performance of online cost of linear regression on WAN
with different mini-batch sizes. n = 10, 000, d = 784.
model may not reach the optimum as fast as before, which
may result in an increase in the number of necessary epochs
E which itself can affect the performance. Mini-batch size is
usually determined considering the speed up of vectorization,
parallelization and robustness of the model in plaintext training.
In the privacy preserving setting, we suggest that one should
also take the network condition into consideration and ﬁnd an
appropriate mini-batch size to optimize the training time.
Ofﬂine phase. The performance of the ofﬂine phase is
summarized in Table II. We report the running time on LAN
and WAN networks and the total communication for OT-
based and LHE-based multiplication triplets generation. For
the client-aided setting, we simulate the total computation time
by generating all the triplets on a single machine. We report
its total time and total communication, but do not differentiate
between the LAN and WAN settings, since in practice the data
would be sent from multiple clients with different network
conditions. As a point of reference, we also include the dataset
size assuming each value is stored as 64-bit decimal number.
We vary n from 1000 to 100,000 and d from 100 to 1000. The
mini-batch size is set to 128 and the number of epochs is set
to 2, as we usually only need 2 epochs in the online phase. If
more epochs are needed, all the results reported in the table
clearly grow linearly with the number of epochs.
As shown in the table, the LHE-based multiplication triplets
generation is the slowest among all approaches. In particular, it
takes 1600.9s for n = 10, 000 and d = 1000. The reason is that
each basic operation in LHE, i.e., encryption, and decryption
are very slow, which makes the approach impractical. E.g.,
one encryption takes 3ms, which is around 10,000× slower
than one OT (when using OT extension). However, the LHE-
based approach yields the best communication. As calculated in
Section IV-B, the asymptotic complexity is much smaller than
the dataset size. Taking the large ciphertext (2048 bits) into
consideration, the overall communication is still on the same
order as the dataset size. This communication introduces almost
no overhead when running on both LAN and WAN networks.
Unlike the online phase, the ofﬂine phase only requires 1
interaction and hence the network delay is negligible.
The performance of the OT-based multiplication triplets
generation is much better in the LAN setting. In particular, it
only takes 80.0s for n = 10, 000 and d = 1000. It introduces
a huge overhead on the communication, namely 19GB while
the data is only 76MB. This communication overhead makes
the running time much slower on WAN networks. Because
of this communication overhead, which is the major cost of
OT, the total running time is even slower than the LHE-based
generation on WAN networks.
31
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:22:48 UTC from IEEE Xplore.  Restrictions apply. 
Finally, the client-aided multiplication triplets generation is
the fastest because no cryptographic operation is involved. It
only takes 4.0s for n = 10, 000 and d = 1000. The overhead
on the total communication is only around 9 times the dataset
size which is acceptable in practice.
It is also shown in Table II that all the running times grow
roughly linearly7 with both n and d, which agrees with the
asymptotic complexity derived in Section IV-B.
Combining the results presented for both the online and the
ofﬂine phase, our system is still quite efﬁcient. E.g., in the
LAN setting, when client-aided multiplication triplets are used,
it only takes 1.0s for our privacy preserving linear regression
in the online phase, with n = 10, 000 and d = 1000. The
total time for the ofﬂine phase is only 4.0s, which would be
further distributed to multiple clients in practice. When OT-
based generation is used, the online phase takes 0.28s and the
ofﬂine phase takes 80.0s.
Comparison with prior work. As surveyed in Section I-B,
privacy preserving linear regression was also considered by [37]
(NWI+13) and [21] (GSB+16) in a similar two-server setting.
Instead of using the SGD method, these two papers propose to
calculate the optimum by solving a linear system we described
in Section II-A. We show that the model trained by the SGD
method can reach the same accuracy in Table V in the Appendix,
on the MNIST, Gisette and Arcene datasets.
The protocols in NWI+13 and GSB+16 can be decomposed
into two steps. In the ﬁrst step, the d × d matrix XT × X
is constructed securely, which deﬁnes a linear system. In
the second step, the Cholesky algorithm or its variants are
implemented using a garbled circuit.
In the ﬁrst step of
NWI+13, each client encrypts a d × d matrix using LHE. In
GSB+16, the ﬁrst step is computed using multiplication triplets
generated by the CSP, which is faster than NWI+13. However,
now the clients cannot collude with the CSP, which is similar
to the model we consider in the client-aided setting. Using
garbled circuits, NWI+13 implements the Cholesky algorithm
while GSB+16 implements CGD, an approximation algorithm.
For comparison, we use the numbers reported in [21, Table
1, Figure 6] . As the performance of the ﬁrst step of NWI+13
is not reported in the table, we implement it on our own using
Paillier’s encryption [38] with batching, which is the same as
used in the protocol of NWI+13. For the ﬁrst step in GSB+16,
we use the result of the total time for two clients only in [21,
Table 1] with d = 500, which is the fastest8; for the second
step in GSB+16, we use the result for CGD with 15 iterations
in [21, Figure 6] with d = 500. We sum up the running time
of our ofﬂine and online phase, and sum up the running time
of the ﬁrst and the second step in NWI+13 and GSB+16, and
report the total running time of all parties in all the schemes.
In Figure 8a, we compare the performance of the scheme
in NWI+13 and our schemes with OT-based and LHE-based
multiplication triplets generation, executed in both LAN and
WAN settings. As shown in the ﬁgure, the performance is
7The number of encryptions and decryptions in the LHE-based generation
is O(|B| + d). As |B| is ﬁxed to 128, its running time does not grow strictly
linearly with d, as reﬂected in Table II.
8For n = 1, 000, 000, d = 500, since the data point is missing in [21,
Table 1], we extrapolate assuming a quadratic complexity in d.
GSB+16
Client LAN
Client WAN
104
105
106
n
(b)
)
s
(
e
m
T
i
108
106
104
102
100
10−2
103
NWI+13
LHE LAN
LHE WAN
OT LAN
OT WAN
104
n
105
106
106
104
102
100
10−2
103
(a)
Fig. 8: Efﬁciency comparison with prior work. Figures are in
log-log scale, d = 500, |B| = 128 for our schemes.
improved signiﬁcantly. For example, when n = 100, 000 and
d = 500, even our LHE-based protocol in both LAN and WAN
settings has a 54× speedup. The OT-based protocol is 1270×
faster in the LAN setting and 24× faster in the WAN setting.
We could not execute the ﬁrst step of NWI+13 for n ≥ 10, 000
and the dotted line in the ﬁgure is our extrapolation
We further compare the performance of the scheme in
GSB+16 and our scheme with client-generated multiplication
triplets in Figure 8b, as they are both secure under the
assumption that servers and clients do not collude. As shown
in the ﬁgure, when n = 100, 000 and d = 500, our scheme
has a 31× speedup in WAN setting and a 1110× speedup in
LAN setting. As the ﬁgure is in log-log scale, the larger slope
of the growth of the running time for our schemes does not
mean we will be slower eventually with large enough n. It
means that the relative speedup is decreasing, but, in fact, the
absolute difference between the running time of our scheme
and GSB+16 keeps increasing.
The reason why the cost of NWI+13 and GSB+16 are so
high when n is small is that the size of the garbled circuit to
solve the linear system only depends on d. Even if there is
only 1 data sample, the time of the second step for d = 500
is around 90,000s in NWI+13 and 30,000s in GSB+16.
Note that the gap between our scheme and prior work will
become even larger as d increases, as the running time is linear
in d in our schemes and quadratic or cubic in the two prior
schemes. In addition, all the numbers reported for the two prior
work were obtained on a network with 1 Gbps bandwidth [21]
which is close to our LAN setting. Indeed, the garbled circuit
introduces a huge communication and storage overhead. As
reported in [21, Figure 4], the garbled circuits for d = 500 in
both schemes have more than 1011 gates, which is 3000GB.
The communication time to transfer such a huge circuits would
be at least 330000s on a WAN network, which means the
speedup of our scheme for that could be more signiﬁcant.
Finally, NWI+13 only supports horizontally partitioned data,
where each client holds one or multiple rows of the data matrix;
GSB+16 only supports vertically partitioned data with 2 ∼ 5
clients, where each client holds one entire column of the data.
Our schemes can support arbitrary partitioning of the data.
Besides, the ofﬂine phase of our protocols is data independent.
The servers and the clients can start the ofﬂine phase with basic
knowledge on the bounds of the dataset size, while the bulk of
the computation in the two prior work need to be performed
after obtaining the data.
32
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:22:48 UTC from IEEE Xplore.  Restrictions apply. 
)
s
(
e
m
T
i
103
102
101
100
10−1
103
)
s
(
e
m
T
i
104
103
102
101
100
3
2
1
0
PP Logistic 1
PP Logistic 2
105
106
104
n
(a)
100
300
700
900
500
d
(b)
120
90
60
30
105
106
0
100
300
103
104
n
(c)
700
900
500
d
(d)
Fig. 9: Online cost of privacy preserving logistic regression in
the standard and client-aided setting. |B| is set to 128. Figure
(a), (b) are for LAN network and Figure (c), (d) are for WAN
network. Figure (a) and (c) are in log-log scale, d = 784.
Figure (b) and (d) are in regular scale, n = 10, 000.
B. Experiments for Logistic Regression
In this section, we review experimental results for our privacy
preserving logistic regression protocol. Since this protocol does
not require any additional multiplication triplets, the ofﬂine
phase has the exact same cost as linear regression.
As shown in Figure 9, our privacy preserving logistic
regression introduces some overhead on top of the linear
regression. Speciﬁcally, in Figure 9a, when n = 1, 000, 000
and d = 784, our protocol 1 using OT-based or LHE-based
multiplication triplets takes 149.7s in the online phase. This
overhead is introduced purely by the extra garbled circuit to
compute our logistic function. The fact that a small additional
garbled circuit introduces a 7× overhead, serves as evidence
that the running time would be much larger if the whole
training was implemented in garbled circuits. Our protocol 2,
using client-generated multiplication triplets, takes 180.7s as
no extra multiplication triplet is used in logistic regression and
the garbled circuit is an additive overhead, no matter which
type of multiplication triplet is used. The training time grows
linearly with both n and d, as presented in Figure 9a and 9b.
Figure 9c and 9d shows the result on a WAN network. The
time spent on the interactions is still the dominating factor.
When n = 1, 000, 000 and d = 784, it takes around 6623s
for our ﬁrst protocol, and 10213s for the second. Compared
to privacy preserving linear regression, one extra interaction
and extra communication for the garbled circuit is added per
iteration. We can also increase the mini-batch size |B| to
balance the computation and interactions and improve the