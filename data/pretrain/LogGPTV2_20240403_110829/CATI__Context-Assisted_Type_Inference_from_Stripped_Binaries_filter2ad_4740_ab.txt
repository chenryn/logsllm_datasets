s
i
B
n
a
r
y
w
i
t
h
$0x0,0xa8(%rsp)
movq
je    4179f5 
…
mov   %rax,0xb0(%rsp)
mov   %rax,0xc8(%rsp)
lea   0x220(%rsp),%rax
movb
movl
$0x0,0x18(%rsp)
$0x8,0x40(%rsp)
…
movl
callq 4044d0 
callq 4044d0  memchr@plt
$0x0,0xbc(%rsp)
q
p
: Abbrev Number: 57 (DW_TAG_variable)
   DW_AT_name
   DW_AT_type
:attr_pair_initial_storage
: 
: Abbrev Number: 30 (DW_TAG_structure_type)
(cid:17)(cid:15)(cid:4)(cid:10)(cid:13)(cid:10)(cid:13)(cid:8)(cid:1)(cid:14)(cid:9)(cid:4)(cid:16)(cid:7)
(cid:2)(cid:12)(cid:12)(cid:10)(cid:1)(cid:9)(cid:2)(cid:5)(cid:10)(cid:1)(cid:5)(cid:7)(cid:5)(cid:12)(cid:5)(cid:2)(cid:6)(cid:1)(cid:11)(cid:12)(cid:8)(cid:10)(cid:2)(cid:4)(cid:3): struct
(cid:21)(cid:2)(cid:1)(cid:7)(cid:38)(cid:36)(cid:34)(cid:21)(cid:23)(cid:36)(cid:1)(cid:21)(cid:31)(cid:24)(cid:1)(cid:11)(cid:21)(cid:22)(cid:25)(cid:29)
(cid:17)(cid:7)(cid:16)(cid:17)(cid:10)(cid:13)(cid:8)(cid:1)(cid:14)(cid:9)(cid:4)(cid:16)(cid:7)
movq $0xADDR  0xIMM(%rsp)
je   ADDR  
…
mov  %rax
mov  %rax
0xIMM(%rsp)
0xIMM(%rsp)
lea  0xIMM(%rsp)  %rax
movb
movl
$0xADDR  0xIMM(%rsp)
$0xADDR  0xIMM(%rsp)
…
$0xADDR  0xIMM(%rsp)
movl
callq ADDR  
struct
(cid:20)(cid:32)(cid:34)(cid:24)(cid:3)(cid:19)(cid:25)(cid:23)(cid:1)(cid:12)(cid:32)(cid:24)(cid:25)(cid:29)
M
o
d
e
l
Sample 1
Sample 2
Sample 3
(cid:19)(cid:25)(cid:23)(cid:36)(cid:32)(cid:34)(cid:1)(cid:24)(cid:21)(cid:36)(cid:21)
(cid:22)(cid:2)(cid:1)(cid:8)(cid:25)(cid:31)(cid:25)(cid:34)(cid:21)(cid:29)(cid:28)(cid:40)(cid:25)(cid:24)(cid:1)(cid:21)(cid:35)(cid:35)(cid:25)(cid:30)(cid:22)(cid:29)(cid:39)
(cid:23)(cid:2)(cid:1)(cid:20)(cid:32)(cid:34)(cid:24)(cid:3)(cid:19)(cid:25)(cid:23)(cid:1)(cid:25)(cid:30)(cid:22)(cid:25)(cid:24)(cid:24)(cid:28)(cid:31)(cid:27)
(cid:24)(cid:2)(cid:1)(cid:12)(cid:32)(cid:24)(cid:25)(cid:29)(cid:1)(cid:36)(cid:34)(cid:21)(cid:28)(cid:31)(cid:28)(cid:31)(cid:27)
M
o
d
e
l
struct
0.99
struct
0.98
char
0.64
(cid:26)(cid:2)(cid:1)(cid:19)(cid:32)(cid:36)(cid:25)
s
t
r
u
c
t
i
B
n
a
r
y
S
t
r
i
p
p
e
d
U
n
s
e
e
n
(cid:19)(cid:18)(cid:5)
(cid:37)(cid:25)(cid:23)(cid:36)(cid:32)(cid:34)
(cid:25)(cid:2)(cid:1)(cid:6)(cid:21)(cid:36)(cid:21)(cid:1)(cid:33)(cid:34)(cid:32)(cid:23)(cid:25)(cid:35)(cid:35)(cid:1)(cid:21)(cid:31)(cid:24)(cid:1)(cid:30)(cid:32)(cid:24)(cid:25)(cid:29)(cid:1)(cid:33)(cid:34)(cid:25)(cid:24)(cid:28)(cid:23)(cid:36)(cid:28)(cid:32)(cid:31)
Fig. 3. An overview of the steps for rebuilding type information of the binary in Figure (a) to (d) (training phase). Figure (e) to (f) shows the ﬁnal judgment
of a variable (inferring phase).
Instruction Context
Target Instruction
Instruction Context
movq
je     4179f5 
$0x0,0xa8(%rsp)
…
mov    %rax,0xb0(%rsp)
mov    %rax,0xc8(%rsp)
lea    0x220(%rsp),%rax
movb
movl
$0x0,0x18(%rsp)
$0x8,0x40(%rsp)
movl
callq
…
$0x0,0xbc(%rsp)
4044d0 
Fig. 4. Structure of VUC.
VUC
analysis, the mapping result of some VUCs may go wrong
with tools. But the result is still convincible.
Generalize Operands. Next, CATI generalizes the operands
of each instruction. All the VUCs related to the target variables
show how the variables are stored, interpreted and manip-
ulated, which are the only and strong features to infer the
variable type. A naive way to represent the instructions is to
train a huge model so that each mnemonic and operand has
a vector representation. However, operands may consist of a
function address or a combination of register and offset. Offset
and function address might be different in each program. To
reduce the possibility of these values, we make the following
substitution rules: (i) the actual address is replaced with
ADDR; (ii) the actual function name is replaced with FUNC;
(iii) the immediate value is replaced with IMM.
Assembly Code Embedding. In this procedure, CATI
to represent each VUC,
employs Word2Vec [24] model
which is often used in natural
language processing. With
the help of Word2Vec, we are able to vectorize the as-
sembly code containing the semantics of neighboring in-
structions. This inspires us to utilize Word2Vec to em-
bed the instructions to a vector representation. We embed
each mnemonic and operand into a vector with a length
of 32 and embed each instruction into a vector with a
length of 96 (ﬁlling instructions with less than two operands
with a BLANK operator for padding). Given a sequence
of V U Ci = {Insi−10, .., Insi−1, Insi, Insi+1, ..., Insi+10},
the algorithm transforms it into a [21,96] matrix as shown in
ﬁgure 3(c).
Train a Multi-Stage Classiﬁer. To identify the type of each
variable, we train a multi-stage classiﬁer with a convolutional
neural network (CNN), which is able to encode the hidden
features. With the data set consists of enormous pairs of VUC
and their types, we set up a tree-like multi-stage classiﬁer
which we will introduce in detail in Section V.
Predict Result for Each VUC. With stripped binaries input,
CATI disassembles the binary code and locates the variables
to extract VUCs with the assistance of IDA Pro. Using the
already trained models, we can output the most likely type of
each VUC and the conﬁdence of each type which is prepared
for the next stage.
Voting. In the testing phase, to make the ﬁnal decision of the
target variable, we leverage the prediction conﬁdence of each
VUC and a set of VUC extracted according to data dependency
from the former stages. We add conﬁdence results of each type
for conﬁdence-based voting mechanism. It avoids letting the
borderline result control the decision, but almost completely
utilizes all the prediction result. We will explain it later.
IV. VARIABLE ANALYSIS
A. Extract and Label
In this paper, we restrict our study to the x86 64 architecture
for a more precise evaluation of our work. We also believe that
the techniques can be extended naturally to other architectures.
Firstly, we need to recover the variables from binaries. Our
work is focused on the problem of type inference, and previous
works [1], [25] have achieved a good result (over 90 % on
(cid:26)(cid:18)
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:25:44 UTC from IEEE Xplore.  Restrictions apply. 
average) of variable recovery. Thus, we assume that this task
can be done accurately enough by existing work and we can
directly use them in our pipeline. In our system, we choose
IDA Pro.
Then, we extract the data ﬂow of these variables in stripped
binaries using IDA Pro and pair each variable with type
information extracted from DWARF [26] debug information.
DWARF contains detailed information on each variable, such
as its name, parent function, offset to stack frame and type.
Afterward, we resolve the type of each variable. If we found
that the type has been redeﬁned by typedef, we would recur-
sively ﬁnd its base type. Finally, we ﬁnd the instruction context
of target instruction with objdump.
As a result, we are now able to get the VUCs and their
type. For each variable, we name all VUCs on its data ﬂow
uniquely, so that we know they belong to the same variable
while voting.
B. Generalization
Each VUC consists of 21 instructions and each instruc-
tion consists of 1 mnemonic and no more than 2 operands.
Our system is implemented on x86 64 platform so that the
mnemonic is within x86 64 architecture’s instruction set.
However, operands have far more possibilities. It could be
an immediate number, a register, an offset to register or an ef-
fective address determined by a base pointer, an offset pointer
and a scale factor. We hope to use some uniﬁed operands to
represent similar operations. For example, we consider that
the immediate value often doesn’t play a signiﬁcant role in
representing semantic information. Different function names
or different jump addresses do not contain much information
about variable behavior. Therefore, we choose to neglect this
binary-speciﬁc information and replace them with uniﬁed
elements. To pad the length of each VUC, we ﬁll BLANK
to assembly code with less than 2 operands, such as jump
operation shown in Table II row 4.
Original assembly
add -0xD0,%rax
Generalized assembly
add -0xIMM,%rax
lea -0x300(%rbp, %r9, 4), %rax
lea -0xIMM(%rbp, %r9, 4), %rax
jmp 3bc59
callq 3bc59 
jmp ADDR BLANK
callq ADDR 
TABLE II
EXAMPLES OF GENERALIZATION.
As shown in Table II, we use regular expression to gener-
alize immediate numbers, function names and addresses. The
ﬁrst two examples show how we treat immediate numbers.
Immediate numbers are either offset to a base pointer or a
value for arithmetic calculation. We replace these immediate
numbers with 0xIMM. Note that we don’t touch the scale
factor of effective address since it is related to variable length.
The remaining 2 examples show how we treat jump and call
instruction. We replace target addresses with ADDR since they
only appear in the instruction context and don’t impact how we
interpret the usage of the variables. Unconditional jump often
suggests loop, conditional jump often suggests branch and call
suggests function call. If objdump cannot ﬁnd function name,
its position is ﬁlled with a BLANK.
In our experiment, our generalization method would cover
over 99% of the instructions for newly come samples.
C. Embedding
To learn the representation of each VUC in the lower dimen-
sion, and retain the relationship with neighboring instructions
at the same time, we choose Word2Vec as our embedding
technique. The objective function of the method is as follows:
T(cid:2)
(cid:2)
t=1
−m≤j≤m,j(cid:3)=0
J(θ) =
1
T
log2P (Inst+j|Inst)
(1)
where T is the number of VUC, Inst denotes the target
instruction (with 1 operation and no more than 2 operands),
Inst+j denotes the neighboring instructions, P (Inst+j|Inst)
represents the appearance possibility of Inst+j when the target
instruction is Inst. We let maximum distance m to be 5.
We directly use the gensim [27] library to train the
Word2Vec model, and it translates each input VUC into a
vector of ﬁxed length which is 32 for CATI.
V. PREDICTION MODELS
A. Multi-Stage Classiﬁer
As we mentioned before, we develop a system called CATI
that can recover 19 types of variables from stripped binaries.
This covers all base types deﬁned in C99 standard. We don’t
classify pointer to these different base types because it’s hard
to statically trace pointers. We also add some widely used
types according to our statistics. Nowadays, deep learning
techniques are powerful enough to distinguish 19 classes
within one model. But to make the model more interpretable
and accelerate the training phase, we train a multi-stage
classiﬁer containing six different classiﬁers. To employ the in-
struction context as new features, we choose the convolutional
neural network (CNN) as our prediction model after several
empirical attempts. Each separate stage uses a different CNN
models with a similar structure but different parameters as
a classiﬁer to infer a speciﬁc cluster of variable types. We
employ a common 2-layer CNN model (32-64) with a fully
connected layer (1024) to run the result.
As shown in Figure 5, we illustrate the multi-stage classiﬁer
as a tree-based structure. In the ﬁrst stage (Stage 1), the binary
classiﬁer identiﬁes whether the VUC belongs to a pointer
variable or not. VUCs tagged as pointer variables are pro-
cessed at Stage 2-1 and tagged as pointer to void,
pointer to struct, pointer to arithmetic. At
Stage 2-2, VUCs tagged as non-pointer variables will be
processed to be tagged as one of struct, bool, char,
float, int. Whether a variable has been tagged as char,
float, int, will ﬁnally be classiﬁed more detailed in Stage
3-1, Stage 3-2, Stage 3-3. Each stage is trained separately with
2-layered CNN with slightly different parameters.
In this paper, we have tried our best to classify as many
types as possible. We recover over 40 kinds of types listed
in C programming language. Here come some reasons for
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:25:44 UTC from IEEE Xplore.  Restrictions apply. 
(cid:26)(cid:19)
We calculate the result of V as follows,
Su(V ) = Z
(2)
where Z is a matrix with size of N × C. C denotes the
number of classes in stage i. Zij denotes the conﬁdence of ith
j=1 Zij = 1.0. To increase
VUC classiﬁed to jth class, and
the inﬂuence of result with high conﬁdence, we come out a
solution with the following formula:
(cid:3)C
(cid:4)
(cid:4)
ij =
Z
1.0
Zij
if Zij ≥ 0.9
if Zij < 0.9
(3)
(cid:3)N
Here, we enlarge the power of the result with high con-
ﬁdence to dominate the ﬁnal decision, and we do several
empirical experiments to set the threshold as 0.9. We make
the ﬁnal decision of V based on the conﬁdence output of each
VUC. The ﬁnal result is calculated as follows,
N(cid:2)
N(cid:2)
N(cid:2)
argmax(
(cid:4)
i1,
Z
(cid:4)
i2, ...,
Z