For those query sources we downcase the queries to ac-
curately reﬂect that casing does not provide any informa-
tion.
This procedure identiﬁed 205 clients in the UCB
dataset. Other than those clients, we left casing intact.
Employing codepoints. General compressors such as
gzip do not make any assumptions about the particular
structure of the data they process. However, our partic-
ular problem domain has certain characteristics that can
improve the compression process if we can arrange to
leverage them. In particular, we know that DNS query
streams often repeat at the granularity of entire queries.
We can expose this behavior to a general compressor
by constructing codepoints, as follows. We preprocess
a given client’s query stream, replacing each distinct
query with a small integer reﬂecting an index into a ta-
ble that enumerates the distinct names. For example, this
would reduce a query stream of foo.X.com, bar.X.com,
bar.X.com, foo.X.com, bar.X.com to the stream 1, 2, 2,
1, 2, plus a dictionary that maps 1 to foo.X.com and 2
to bar.X.com. The particular encoding we use employs
24-bit integers (we take care in our information-content
estimation to include the dictionary size).
Representing query types. For datasets that include
query types, we construct a separate, parallel compres-
22  22nd USENIX Security Symposium 
6
USENIX Association
sion stream for processing the corresponding 16-bit val-
ues, i.e., we do not intermingle the query types with the
query names.
Representing timing.
Individual query timings offer
only quite limited information content. Thus, for an at-
tacker to make effective use of timing, they will need to
send a large number of queries. This means that we likely
will beneﬁt from capturing not absolute timestamps but
intervals between queries. We compute such intervals as
32-bit integers representing multiples of R, our assumed
lower bound on the timing resolution the attacker can
achieve. Again we construct a separate, parallel com-
pression stream for processing these.
Clearly, the value of R can signiﬁcantly affect the
amount of information the attacker can extract from the
timing of queries; but R will be fundamentally limited by
network jitter. To formulate a defensible value of R, we
asked the authors of [17] regarding what sort of timing
variation their measurements found for end systems con-
ducting DNS queries. Using measurements from about a
quarter million distinct IP addresses, they computed the
maximum timing difference seen for each client in a set
of 10 DNS queries it issued. The median value of this
difference across all of the clients was 32 msec. Only
a quarter of the clients had a difference under 10 msec.
Accordingly, for our study we have set R to 10 msec.
Constructing uniﬁed estimates. As described above,
we separately process the query names, types, and tim-
ing. Formulating a ﬁnal estimated bound on a query
stream’s information content then is simply a matter
of adding the three corresponding estimates. We note,
though, that by tracking each separately, we can identify
which one contributes the most signiﬁcantly (per Fig-
ure 6 below).
Bakeoffs. Finally, as outlined above we have sev-
eral potential choices to make in formulating our upper-
bound information estimates: which compressor should
we employ? Should we use codepoints or allow the
compressor to operate without them (thus not imposing
the size of the dictionary)? We note that we do not in
fact have to make particular choices regarding these is-
sues; we can try each option separately, and then sim-
ply choose the one that happens to perform best (gener-
ates the lowest information estimates) in a given context.
Such “bakeoffs” are feasible since we employ lossless
techniques to construct our estimates; we know that each
estimate is sound, and thus the lowest of a set is indeed
the tightest upper bound we can obtain.
The drawback with trying multiple approaches, of
course, is that it requires additional computation. In the
next section we turn to how to minimize the computation
we must employ to formulate our estimates.
Implementation
6
The previous section described our approach to devel-
oping an accurate bounds on the amount of information
conveyed using DNS queries to a given domain’s name
server(s). Computing these estimates and acting upon
their corresponding detections, however, raises a number
of issues with regards to reducing the resources required
for employing this approach.
In this section we discuss practical issues that arise
when implementing our detection approach. One signif-
icant set of these concern ﬁltering: either restricting the
DNS queries we examine in order to conserve computing
(or memory) resources, or reducing the burden that our
detection imposes on a site’s security analysts. The key
property of these ﬁltering stages is their efﬁcacy in con-
cert, which is crucial for the scalability of our approach.
Figure 5 shows the different stages of processing in our
detection procedure and how they pare down in several
steps the volume of both the queries that we must exam-
ine and the number of domain name sufﬁxes to consider.
We describe our detection procedure as implemented
for off-line analysis here, and discuss our experiences
with a real-time detector in § 8.
6.1 Cached Query Filter
A query from a DNS client system cannot exﬁltrate in-
formation unless it is forwarded by the recursive resolver.
Thus a highly useful optimization for the internal van-
tage point (as discussed in § 4) is to model the recursive
resolver’s cache and not consider any query where the
resolver obtained the result from its cache.
We can accomplish this by observing the replies with
the TTL ﬁeld. We maintain a shadow cache based on
the query attributes (contained in the reply) and the reply
TTL values, and do not consider later queries until their
information expires from the shadow cache.
The result of this ﬁltering is to eliminate the disadvan-
tage of the internal vantage point, as this ﬁlter ensures
that later stages only process uncached requests. With
the INDLAB dataset, this reduces the number of detec-
tions by about 2x for the timing vector, and about 10%
for query names. Unfortunately not all of our datasets
support this ﬁltering.
6.2 Uninteresting Query Filter
We remove lookups that target domain names within the
local organization itself, or within closely-related orga-
nizations. Due to their relatively high volume, we ﬁnd
that such lookups can result in a large number of detec-
tions, but the likelihood that someone will actually use
a DNS tunnel between such domains will be negligible.
Likewise, we remove lookups of PTR (address-to-name)
records for local and reserved network address ranges.
USENIX Association  
7
22nd USENIX Security Symposium  23
Ingestion
of queries
Pre-processing 
of  queries
Grouping by 
suffixes & clients
Filtering on
suffix and client level
Bound on information content
Suffix: attacker.com, Client: 10.9.8.7
name
time
type
mincompr(A)
mincompr(D)  
+ mincompr(I)
min
sum
mincompr(x) = min( gzip(x), bzip2(x), ppmd(x) )
A: All Symbols  D: Distinct Symbols 
I: Index of Distinct Symbols
Investigate
4,089 queries
1 suffix
…
DNS 
queries
queries
Total input
45M queries
65K suffixes
…
(§ 6.3)
Cached query
filter (§ 6.1)
Removing
41M queries
0 suffixes
Uninteresting  query 
filter (§ 6.2)
Removing
1.5M queries
41 suffixes
Fast entropy
filter (§ 6.4)
Removing
1.8M queries
65K suffixes
<4kB information
content (§ 6.5)
Inspected 
Domain List (§ 6.7)
Removing
78K queries
34 suffixes
Removing
185K queries
11 suffixes
Figure 5: The full detection procedure. The numbers (grey) reﬂect a day at the INDLAB network for which the detection procedure
ﬂagged a new domain name (a relatively rare event).
Finally, we exclude names without a valid global top-
level domain. This eliminates numerous queries from
systems that are misconﬁgured or confused.
6.3 Grouping by Sufﬁx and Client
In this stage of our detection procedure, we compute
statistics per (lookup name sufﬁx, client)-pair that will
serve as input to the lightweight ﬁlter described in § 6.4.
Due to the voluminous nature of our data, we ag-
gregate these statistics at the level of registered domain
names (e.g., one level under com or co.uk). With IPv4
PTR lookups we aggregate at two and three labels un-
der in-addr.arpa (corresponding with /16 or /24 network
ranges), and with IPv6 PTR lookups we aggregate at
12 labels under ip6.arpa (corresponding with /48 net-
work ranges). The reasoning behind these choices is
that shorter PTR sufﬁxes will in general represent large
blocks that are parents to multiple organizations; thus,
the presence of tunneling associated with such sufﬁxes
would require compromise of a highly sensitive infras-
tructure system. In our results for PTR lookups we ﬁnd
no indications of surreptitious communication.
We then compute for each query sufﬁx and client the
numbers of unique and distinct lookup names includ-
ing that sufﬁx, as well as the combined length of those
lookup names. We group sufﬁxes in a case-insensitive
manner, but count as distinct any lookup names that dif-
fer only in case (cf. § 5.3).
6.4 Fast Filtering of Non-Tunnel Trafﬁc
The very high volume of DNS queries means we can ob-
tain signiﬁcant beneﬁt from considering additional mea-
sures for pre-ﬁltering the trafﬁc before we compute the
principled bounds described in § 5. For each domain
sufﬁx, we use computationally lightweight metrics that
overestimate the information content present in the in-
formation vectors described in § 5.1. We then compare
the sum of these metrics across all information vectors
against a minimum-information content threshold, I. If
the sum total (guaranteed to not underestimate) lies be-
low the threshold, the trafﬁc for the corresponding do-
main sufﬁx cannot represent communication of interest.
This approach allows us to short-circuit the detection
process and eliminate early on numerous domain suf-
ﬁxes.
Fast ﬁlter for the query name vector. We consider the
following quantities from a sequence of lookups made by
some host during one day: the total number of lookups
L, the number of distinct query names Dname in those
lookups, and the total number of bytes Cname in those dis-
tinct query names. We remark that we can determine all
three quantities with minimal computational and mem-
ory overhead.
Query name tunnels encode information in terms of
the characters and the repetition patterns of the names
looked up. Each character in a name may convey up to 1
byte of information, contributing up to Cname bytes in to-
tal. According to Shannon’s law, the number of bits con-
veyed per lookup amounts to at most log2 Dname. There-
fore the combined upper bound on information conveyed
in bytes by such a tunnel amounts to:
Iname = Cname + L·
log2 Dname
8
Fast ﬁlter for the query type vector. We ﬁlter the query
type vector similarly. Again, we consider a sequence of
DNS lookups with a given sufﬁx made by some host dur-
If we use Dtype to denote the number of
ing one day.
distinct query types in those lookups and Ctype the total
number of bytes in those distinct query types, we have:
Itype = Ctype + L·
log2 Dtype
8
24  22nd USENIX Security Symposium 
8
USENIX Association
Fast ﬁlter for the query timing vector. The timing vec-
tor is more complicated because we need to discretize
the time information and create symbols representing
the encoded data as it appears in the timing vector. We
parametrize this process by the time resolution R that the
network environment affords to the attacker.
Intuitively, for a given number of lookups L observed
over a day, the amount of potential information encoded
in time is maximal when the number of distinct inter-
arrival times, k, is maximal. This is due to the fact that,
without knowing the distribution of inter-arrival times,
the empirical entropy from the inter-arrival times may be
upper-bounded by L· log2 k, where log2 k is the number
of bits encoded by a single lookup.
As a consequence, to assess the upper-bound on the
information content for a ﬁxed L and an assumed time-
slot size (expressed as time resolution R), we need to
determine into how many distinct inter-arrival times k we
can partition one day into, while imposing as uniform a
distribution of inter-arrival times as possible (i.e., leading
to maximal entropy).
By maximizing k subject to the constraint that the dis-
tribution of distinct inter-arrival times is uniform (omit-
ting details for brevity), and upper-bounding k by L− 1
(the number of intervals), we ﬁnd that we can express
the upper bound on the information amount in the timing
vector as:
Itime = L· log2(cid:31)min(cid:31)L− 1,(cid:30) 2M
L− 1(cid:29) + 1(cid:28)(cid:28)
R denotes the number of time slots with
where M = 86,400
resolution R over one day (86,400 seconds).
Uniﬁed fast ﬁlter. From the above equations, we can
now formulate the following uniﬁed test condition to
handle all types of information vectors:
If Iname + Itype + Itime < I, the sufﬁx is not a
candidate tunnel.
We then eliminate from further detailed analysis the
name sufﬁxes that are not candidate tunnels.
Choosing the thresholds. The fast ﬁlter relies on two
parameters, the information content threshold I and the
time resolution R.
In order to select security-relevant
values for these parameters, we measured their impact
on the analyst’s workload. (Note that in § 5.3 we also
framed empirical evidence that R = 10 msec appears
fairly conservative.) It is clear that both reducing the in-
formation content threshold and reducing the time reso-
lution can increase the false positive rate, and relatedly
the analyst’s workload.
Figure 6 shows how varying these parameters affects
the analyst for INDLAB data. One can see, for exam-
ple, that decreasing the information content threshold I
d
a
o
l
k
r
o