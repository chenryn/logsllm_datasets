title:Inferring Cloud Interconnections: Validation, Geolocation, and Routing
Behavior
author:Alexander Marder and
Kimberly C. Claffy and
Alex C. Snoeren
Inferring Cloud Interconnections:
Validation, Geolocation, and Routing
Behavior
Alexander Marder1(B), K. C. Claﬀy1, and Alex C. Snoeren2
1 CAIDA, UC San Diego, San Diego, USA
PI:EMAIL
2 UC San Diego, San Diego, USA
Abstract. Public clouds fundamentally changed the Internet landscape,
centralizing traﬃc generation in a handful of networks. Internet perfor-
mance, robustness, and public policy analyses struggle to properly reﬂect
this centralization, largely because public collections of BGP and tracer-
oute reveal a small portion of cloud connectivity.
This paper evaluates and improves our ability to infer cloud connec-
tivity, bootstrapping future measurements and analyses that more accu-
rately reﬂect the cloud-centric Internet. We also provide a technique for
identifying the interconnections that clouds use to reach destinations
around the world, allowing edge networks and enterprises to understand
how clouds reach them via their public WAN. Finally, we present two
techniques for geolocating the interconnections between cloud networks
at the city level that can inform assessments of their resilience to link
failures and help enterprises build multi-cloud applications and services.
1 Introduction
The growing deployment of low-latency and high-throughput applications, the
upfront and maintenance costs of computing resources, and constantly evolving
security threats make it increasingly complex and costly for organizations to
host services and applications themselves. Public cloud providers ease that bur-
den by allowing organizations to build and scale their applications on networks
and hardware managed by the cloud provider. At the core of cloud comput-
ing are virtual machines (VMs) and containers that run on physical hardware
in a data center [47]. Clouds locate these data centers in globally distributed
geographic regions [7,8,12]. The three major cloud providers, Amazon AWS,
Microsoft Azure, and Google Cloud Platform (GCP), interconnect their regions
using global backbones [6,9,52].
Public clouds fundamentally changed the Internet landscape from peer-to-
peer to a cloud-centric model. According to a recent estimate [49], the ten
highest-paying customers in AWS—all popular video and content generators—
combine to spend over $100 million per month, and many enterprises store opera-
tions data and host internal applications in public clouds. Existing measurement
platforms, with vantage points (VPs) located outside cloud networks, capture
c(cid:2) Springer Nature Switzerland AG 2021
O. Hohlfeld et al. (Eds.): PAM 2021, LNCS 12671, pp. 230–246, 2021.
https://doi.org/10.1007/978-3-030-72582-2_14
Inferring Cloud Interconnections
231
only a small fraction of the paths that connect public clouds to end users and
enterprises, and the importance of the clouds necessitates that the Internet mea-
surement community considers how to eﬀectively capture this.
The goal of this paper is to evaluate and improve our ability to infer cloud
connectivity, in the hope that it bootstraps Internet measurements and anal-
yses that more accurately reﬂect the cloud-centric Internet. We also build on
those inferences, identifying the interconnections that clouds use to reach desti-
nations around the world. Such analysis enables edge networks and enterprises
to understand how clouds reach them, and potentially respond to fallout from
congestion on a cloud interconnection. Furthermore, we geolocate the intercon-
nections between cloud networks at the city level, providing techniques that can
inform assessments of their resilience to link failures and help enterprises build
multi-cloud applications and services. We make the following contributions:
1. We validate the state-of-the-art in identifying network interconnections
(bdrmapIT) on Azure, identifying path changes as a prominent source of error.
2. We demonstrate that changing the traceroute probing method to reduce the
number of simultaneous traceroutes reduces the impact of path changes on
the observed topology, and improves the accuracy of bdrmapIT’s AS operator
inferences for the interconnection addresses in our validation dataset by 8.6%.
3. We use traceroute to identify next-hop ASes for each Internet network from
AWS, Azure, and GCP, ﬁnding that clouds still rely on tier 1 and tier 2
networks, and that next-hop ASes can be region-dependent.
4. We geolocate all observed AWS-Azure and Azure-GCP interconnections, and
34.4% of the AWS-GCP interconnections, discovering that clouds intercon-
nect on every populated continent, and often interconnect in the same cities.
2 Background and Previous Work
Our work builds on prior work that inferred AS-links from BGP, identiﬁed net-
work interconnections in traceroute paths, studied cloud backbone networks with
traceroute, and geolocated network infrastructure.
BGP Route Announcements Reveal AS Connectivity. The public
BGP route announcement collectors, Routeviews [4] and RIPE RIS [3], col-
lect announcements received from the ASes that peer with the collectors (VP
ASes), and researchers infer AS connectivity from adjacent ASes in collected
AS paths [16,24]. We could infer cloud neighbors directly from the cloud net-
works through routes they propagate to public collectors, but cloud networks
share few routes with public route collectors. We can also infer cloud connec-
tivity indirectly from announcements that clouds originate into BGP, but VP
ASes are unlikely to see cloud neighbors that enter into paid or settlement-free
peering with the cloud [19,25,27,32,37,55,58]. Furthermore, VP ASes typically
only propagate their chosen best-path for each preﬁx to collectors, and any VP
AS that interconnects with a cloud network will likely select their direct inter-
connection as the best path to that cloud, and will not propagate alternate AS
paths to the public collectors.
232
A. Marder et al.
Inferring Router Ownership From Traceroute Paths. Substantial prior
work attempted to infer AS interconnections from traceroute paths. Mao et
al. [39,40] aligned traceroutes from VP ASes with BGP route announcements
seen by that same AS to better determine address space ownership. Chen et
al. [18] generalized and expanded Mao’s methodology to align AS-level links
seen in traceroute with those in BGP AS paths. Later work focused on inferring
the AS operators of routers in traceroute paths. Huﬀaker et al. [30] used alias
resolution to convert the IP address paths in traceroute to router graphs, and
proposed and validated four techniques to map routers to AS operators. Marder
et al. [44] and Luckie et al. [36] independently developed and validated heuristics
to extract constraints from traceroute to more accurately infer AS operators.
Marder and Luckie later integrated and extended their approaches, creating the
current state-of-the-art bdrmapIT, and validated their bdrmapIT technique [43].
Most recently, Luckie et al. [38] used the AS operator inferences from Huﬀaker
et al.’s technique and bdrmapIT as training data to learn regular expressions for
extracting AS operators embedded in hostnames in the form of AS numbers.
Revealing Cloud Connectivity With Traceroute. VPs outside the cloud
cannot reveal many of the paths and interconnections that clouds use to reach
the Internet. Yeganeh et al. [56] conducted traceroutes from AWS to every /24
to reveal interconnected networks, using a new unvalidated approach to infer
network interconnections. In subsequent work [57], they compared the quality
of service of default interconnections between cloud networks and third-party
transit between clouds, switching to bdrmapIT to perform interconnection IP
addresses inferences. Arnold et al. [15] inferred directly connected networks from
traceroute paths by converting traceroute IP addresses to ASes using longest-
matching preﬁx in BGP route announcements and IXP participant IP addresses
recorded in PeeringDB [2]. They then augmented the AS-level connectivity graph
in CAIDA’s AS Relationship dataset [1] with peer relationships between each
cloud and the newly inferred neighbors, using the graph to estimate that clouds
can avoid their transit providers listed in CAIDA’s AS Relationship dataset to
reach 76% of the Internet networks. They validated their neighbor inferences
with feedback from Azure and GCP, with 11%–15% false neighbor inferences.
Assuming nearly perfect accuracy for IXP participant addresses in PeeringDB,
these false neighbor inferences almost entirely result from false private intercon-
nection inferences.
We show that the traceroute technique used by prior studies is prone to
path change corruptions, and we validate our cloud interconnection inferences
(Sect. 4). Rather than use unvalidated AS interconnection inference techniques,
we use the previously validated bdrmapIT tool to infer private interconnections
between cloud public WANs and their neighbors, and perform additional val-
idation to understand bdrmapIT’s accuracy for cloud networks. Finally, while
Arnold et al. speculated how clouds could reach other ASes [15], we report how
clouds currently do reach other networks.
Geolocating Network Infrastructure. Commercial IP geolocation databases
focus nearly exclusively on edge hosts, with poor accuracy for network infrastruc-
Inferring Cloud Interconnections
233
ture [22,26,48]. Some networks encode geographic information in router inter-
face DNS hostnames, but the geographic codes are diﬃcult to automatically
extract and interpret, as they use a mix of IATA codes, CLLI codes, and com-
mon location abbreviations. Rocketfuel includes the undns tool [51] that uses
hand-crafted regular expressions to extract geolocations from hostnames. More
general approaches avoid manually constructing regular expressions. DRoP [31]
automatically learns rules to extract geolocation codes from hostnames, and
HLOC [50] searches hostnames for geolocation codes. Other approaches use RTT
to approximate distance between VPs and routers. Gueye et al. [28] and RIPE
IPMap [45] triangulate RTTs to estimate location, and Katz-Basset et al. [33]
reﬁned RTT-based estimates using topology constraints. We use a combination
of geolocation codes extracted from Azure DNS hostnames and traceroute paths
to geolocate the interconnections between cloud providers.
3 Validating bdrmapIT With Azure Hostnames
Our analysis relies on bdrmapIT AS operator inferences to identify cloud inter-
connections and neighbors, so we ﬁrst validate bdrmapIT’s inferences on Azure
to gain conﬁdence in its eﬃcacy and look for opportunities to improve our
techniques. bdrmapIT addresses the diﬃcult problem of inferring the networks
that operate each router observed in traceroute, but relies on general assump-
tions of router conﬁgurations, internal traﬃc engineering, and network topol-
ogy that might not hold in cloud WANs. Furthermore, prior bdrmapIT evalu-
ations on transit interconnection inferences might not translate to cloud inter-
connection inferences. Initial bdrmapIT evaluations used CAIDA’s Ark tracer-
outes and ground truth from ISP operators, and later experiments also val-
idated bdrmapIT against pseudo ground truth derived from ISP DNS host-
names [38,41,42]. Traceroutes from CAIDA’s Ark VPs mostly reveal transit
interconnections—those between providers and customers—so transit intercon-
nections dominate their reported accuracy. Clouds primarily peer with other
networks, and we expect that their peering interconnections vastly outnumber
their transit interconnections. Importantly, bdrmapIT leverages the industry
convention that transit providers supply the IP subnets for interconnection with
customers, but no known convention exists for peering interconnections [35].
To date, no study has evaluated bdrmapIT’s accuracy using traceroutes that
originate in the cloud.
For this initial experiment, we created a VM in every Azure region and
used Scamper [34], the traceroute tool used in prior cloud studies [15,56,57],
to conduct traceroutes from every VM to each of the 11.5 M /24 s covered by
a preﬁx in a BGP route announcement collected by RouteViews or RIPE RIS
over 1–5 August 2020. Our choice of /24 granularity reﬂects our assumption that
clouds are unlikely to receive many preﬁxes longer than /24. Each traceroute to
a /24 targeted a random address to provide comprehensive coverage of Azure’s
neighboring networks, and we instructed Scamper to use Paris-style traceroute
probes to prevent load-balancing from corrupting the traceroute paths.
234
A. Marder et al.
To identify interconnection addresses between clouds and their neighbors, we
used a combination of bdrmapIT AS operator inferences and IXP participant
IP addresses listed in PeeringDB [2] and IXPDB [23] to map traceroute path IP
addresses to ASes. In the event of a conﬂict between PeeringDB and IXPDB, a
contact at both IXPDB and PeeringDB advised us to use the mapping in IXPDB,
since IXP operators update information in IXPDB while IXP members populate
information in PeeringDB, potentially causing stale entries. bdrmapIT requires
AS address spaces as input, and we supplied preﬁx origin ASes derived from BGP
announcements collected by RouteViews and RIPE RIS. For addresses with no
covering preﬁx in BGP, we relied on the potentially stale longest matching preﬁx
in RIR extended delegations. 0.8% of addresses did not have a covering preﬁx
in BGP or RIR. We used whois [20] and RADb [46] to determine ownership for
53.4% of those addresses; this was the only manual step in this process (Fig. 1).
Azure
Internet2
VM
i1
R1
i2
R2
104.44.40.3
104.44.12.159
ae28-0.dal-96cbe-1b.ntwk.msn.net
internet2.dal-96cbe-1b.ntwk.msn.net
We used Azure DNS hostnames to pro-
vide pseudo ground truth for our intercon-
nection inferences, and successfully resolved
hostnames for 59.5% of the 5749 Azure
IP addresses seen in our initial traceroutes.
Azure tags many of its network interconnec-
tion address hostnames with the name of
the neighboring network, and we used the
tags visible in traceroute paths from Azure
VMs to identify Azure addresses on routers
operated by neighbors; e.g.,
in a tracer-
oute starting from an Azure VM, the tag
in internet2.dal-96cbe-1b.ntwk.msn.net
indicates that 104.44.12.159 belongs to an Internet2 router interconnected with
Azure. Our evaluation focused on comparing bdrmapIT inferences to the tags
extracted from Azure hostnames. We used the regular expression ([^-]*?)\..
*\.ntwk\.msn\.net to extract the interconnection tags from Azure hostnames,
ﬁnding 214 tags corresponding to 419 address hostnames. For each IP address
with a hostname containing an interconnection tag, we manually validated that
bdrmapIT’s AS operator inference aligns with the name of the inferred AS or the
organization that owns it. These tags are nearly always network names rather
than AS numbers, preventing us from using Luckie, et al.’s technique [38] to
identify the operating AS automatically.
Fig. 1. In traceroute paths from
Azure,
the internet2 tag indi-
cates that 104.44.12.159 belongs
to a router operated by Inter-
net2. We use this as validation for
bdrmapIT’s router operator infer-
ences from Azure traceroutes.
3.1
Investigating as Operator Inference Errors
Our initial evaluation on Azure interconnections yielded 87.4% AS operator accu-
racy, with 53 errors. One source of error was that bdrmapIT occasionally ﬁltered
out valid neighboring ASes in favor of ASes seen adjacent to Azure in BGP AS
paths. bdrmapIT relies heavily on AS connectivity inferred from BGP to con-
strain the choice of AS operator for a router, but the largely incomplete connec-
tivity constraints led to six false inferences in our validation set. We modiﬁed
bdrmapIT to remove these constraints only for the major cloud networks, but
Dest A
Probe 1
Reply 1
Probe 2
Dest B
Probe 1
Reply 1
Probe 2
Dest C
Probe 1
Reply 1
Probe 2
Time
Fig. 2. Scamper increases eﬃciency by
parallelizing traceroute probing across
destinations, but a path change can cor-
rupt all active traceroute paths.
Inferring Cloud Interconnections
235
104.44.13.95 
cableone.dal-96cbe-1a.ntwk.msn.net 
160.3.115.1 
Cable One