cases to cross the adversarially smoothed decision boundary
within certain perturbation budget. Examples of high/low
conﬁdence test seeds are provided in Fig. 15. It is also worth
mentioning that our white-box testing still performs well in
this case. Overall, DEEPJUDGE is robust to Adv-Train or at
least can be made robust by efﬁciently updating the seeds.
2) Transfer learning: The adversary may transfer the stolen
copy of the victim model to a new dataset. The adversary
exploits the main structure of the victim model as a backbone
and adds more layers to it. Here, we test a vanilla transfer
learning (VTL) strategy from the 10-class CIFAR-10 to a 5-
class SVHN [31]. The last layer of the CIFAR-10 victim
model is ﬁrst replaced by a new classiﬁcation layer. We then
ﬁne-tune all layers on the subset of SVHN data. Note that,
in this setting, the black-box metrics are no longer feasible
since the suspect model has different output dimensions to
the victim model, however, the white-box metrics can still
be applied since the shallow layers are kept. The results are
reported in Table VII. Remarkably, DEEPJUDGE succeeds
in identifying transfer learning attacks with distinctively low
testing distances and an AU C = 1.
In one recent work [29], it was observed that the knowledge
of the victim model could be transferred to the stolen models.
Dataset Inference (DI) technique was then proposed to probe
whether the victim’s knowledge (i.e., private training data) is
preserved in the suspect model. We believe such knowledge-
level testing metrics could also be incorporated into DEEP-
JUDGE to make it more comprehensive. An analysis of how
different levels of transfer learning could affect DEEPJUDGE
can be found in Appendix H.
Remark 4: DEEPJUDGE is fairly robust to adversarial
ﬁnetuning, adversarial
training or transfer learning
based adaptive attacks, although sometimes it needs
to regenerate the seeds or test cases.
VII. CONCLUSION
In this work, we proposed DEEPJUDGE, a novel testing
framework for copyright protection of deep learning models.
The core of DEEPJUDGE is a family of multi-level testing
metrics that characterize different aspects of similarities be-
tween the victim model and a suspect model. Efﬁcient and
ﬂexible test case generation methods are also developed in
DEEPJUDGE to help boost the discriminating power of the
testing metrics. Compared to watermarking methods, DEEP-
JUDGE does not need to tamper with the model
training
process. Compared to ﬁngerprinting methods, it can defend
more diverse attacks and is more resistant to adaptive attacks.
DEEPJUDGE is applicable in both black-box and white-box
settings against model ﬁnetuning, pruning and extraction at-
tacks. Extensive experiments on multiple benchmark datasets
demonstrate the effectiveness and efﬁciency of DEEPJUDGE.
We have implemented DEEPJUDGE as a self-contained open-
source toolkit. As a generic testing framework, new testing
metrics or test case generation methods can be effortlessly
incorporated into DEEPJUDGE to help defend future threats to
deep learning copyright protection.
ACKNOWLEDGEMENT
We are grateful to the anonymous reviewers and shepherd
for their valuable comments. This research was supported by
the Key R&D Program of Zhejiang (2022C01018) and the
NSFC Program (62102359, 61833015).
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:24:42 UTC from IEEE Xplore.  Restrictions apply. 
836
REFERENCES
[1] Yossi Adi, Carsten Baum, Moustapha Cisse, Benny Pinkas, and Joseph
Keshet. Turning your weakness into a strength: Watermarking deep
neural networks by backdooring.
In USENIX Security, pages 1615–
1631, 2018.
[2] Xiaoyu Cao, Jinyuan Jia, and Neil Zhenqiang Gong. IPGuard: Protecting
intellectual property of deep neural networks via ﬁngerprinting the
classiﬁcation boundary. In Asia CCS, pages 14–25, 2021.
[3] Nicholas Carlini, Matthew Jagielski, and Ilya Mironov. Cryptanalytic
In CRYPTO, pages 189–218.
extraction of neural network models.
Springer, 2020.
[4] Nicholas Carlini and David Wagner. Towards evaluating the robustness
of neural networks. In S&P, pages 39–57. IEEE, 2017.
[5] Chenyi Chen, Ari Seff, Alain Kornhauser, and Jianxiong Xiao. Deep-
Driving: Learning affordance for direct perception in autonomous driv-
ing. In ICCV, pages 2722–2730, 2015.
[6] Keunwoo Choi, Deokjin Joo, and Juho Kim. Kapre: On-gpu audio
preprocessing layers for a quick implementation of deep neural network
models with keras. In Machine Learning for Music Discovery Workshop
at ICML, 2017.
[7] Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Ko-
language process-
Journal of machine learning research,
ray Kavukcuoglu, and Pavel Kuksa.
ing (almost) from scratch.
12(ARTICLE):2493–2537, 2011.
Natural
[8] Jacson Rodrigues Correia-Silva, Rodrigo F Berriel, Claudine Badue,
Alberto F de Souza, and Thiago Oliveira-Santos. Copycat CNN: Stealing
knowledge by persuading confession with random non-labeled data. In
IJCNN, pages 1–8. IEEE, 2018.
[9] Bita Darvish Rouhani, Huili Chen, and Farinaz Koushanfar. DeepSigns:
an end-to-end watermarking framework for ownership protection of deep
neural networks. In ASPLOS, pages 485–497, 2019.
[10] Lixin Fan, Kam Woh Ng, and Chee Seng Chan. Rethinking deep
neural network ownership veriﬁcation: Embedding passports to defeat
ambiguity attacks. 2019.
[11] Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli,
and Pascal
Frossard. The robustness of deep networks: A geometrical perspective.
IEEE Signal Processing Magazine, 34(6):50–62, 2017.
[12] Yang Feng, Qingkai Shi, Xinyu Gao, Jun Wan, Chunrong Fang, and
Zhenyu Chen. DeepGini: prioritizing massive tests to enhance the
robustness of deep neural networks. In ISSTA, pages 177–188, 2020.
[13] Bent Fuglede and Flemming Topsoe. Jensen-shannon divergence and
In International Symposium on Information
hilbert space embedding.
Theory, page 31. IEEE, 2004.
[14] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining
and harnessing adversarial examples. arXiv preprint arXiv:1412.6572,
2014.
[15] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech
In ICASSP, pages
recognition with deep recurrent neural networks.
6645–6649. IEEE, 2013.
[16] Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg.
BadNets: Evaluating backdooring attacks on deep neural networks. IEEE
Access, 7:47230–47244, 2019.
[17] Shangwei Guo, Tianwei Zhang, Han Qiu, Yi Zeng, Tao Xiang, and Yang
Liu. The hidden vulnerability of watermarking for deep neural networks.
arXiv preprint arXiv:2009.08697, 2020.
[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep
residual learning for image recognition. In CVPR, pages 770–778, 2016.
[19] Matthew Jagielski, Nicholas Carlini, David Berthelot, Alex Kurakin, and
Nicolas Papernot. High accuracy and high ﬁdelity extraction of neural
networks. In USENIX Security, pages 1345–1362, 2020.
[20] Hengrui Jia, Christopher A Choquette-Choo, Varun Chandrasekaran, and
Nicolas Papernot. Entangled watermarks as a defense against model
extraction. In USENIX Security, 2021.
[21] Mika Juuti, Sebastian Szyller, Samuel Marchal, and N Asokan. PRADA:
protecting against dnn model stealing attacks. In EuroS&P, pages 512–
527. IEEE, 2019.
[22] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of
features from tiny images. 2009.
[23] Erwan Le Merrer, Patrick Perez, and Gilles Tr´edan. Adversarial frontier
stitching for remote neural network watermarking. Neural Computing
and Applications, 32(13):9233–9244, 2020.
[24] Yann LeCun, Corinna Cortes, and CJ Burges. MNIST handwritten digit
database. 2010.
[25] Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-pruning:
Defending against backdooring attacks on deep neural networks. In In-
ternational Symposium on Research in Attacks, Intrusions, and Defenses,
pages 273–294. Springer, 2018.
[26] Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor
arXiv preprint
Darrell. Rethinking the value of network pruning.
arXiv:1810.05270, 2018.
[27] Nils Lukas, Yuxuan Zhang, and Florian Kerschbaum. Deep neural
In ICLR,
network ﬁngerprinting by conferrable adversarial examples.
2021.
[28] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris
Tsipras, and Adrian Vladu. Towards deep learning models resistant
to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.
[29] Pratyush Maini, Mohammad Yaghini, and Nicolas Papernot. Dataset
inference: Ownership resolution in machine learning. In ICLR, 2021.
[30] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman,
and Aram Galstyan. A survey on bias and fairness in machine learning.
arXiv preprint arXiv:1908.09635, 2019.
[31] Y Netzer, T Wang, A Coates, A Bissacco, B Wu, and AY Ng. Reading
digits in natural images with unsupervised feature learning. In Workshop
on deep learning and unsupervised feature learning, 2011.
[32] Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz. Knockoff nets:
Stealing functionality of black-box models. In CVPR, pages 4954–4963,
2019.
[33] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha,
Z Berkay Celik, and Ananthram Swami. Practical black-box attacks
against machine learning. In Asia CCS, pages 506–519, 2017.
[34] Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. DeepXplore:
Automated whitebox testing of deep learning systems. In SOSP, pages
1–18, 2017.
[35] Alex Renda, Jonathan Frankle, and Michael Carbin.
Comparing
rewinding and ﬁne-tuning in neural network pruning. arXiv preprint
arXiv:2003.02389, 2020.
[36] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev
Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla,
Michael Bernstein, et al.
large scale visual recognition
challenge.
International journal of computer vision, 115(3):211–252,
2015.
Imagenet
[37] Or Sharir, Barak Peleg, and Yoav Shoham. The cost of training nlp
models: A concise overview. arXiv preprint arXiv:2004.08900, 2020.
[38] Florian Tram`er, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas
Ristenpart. Stealing machine learning models via prediction APIs. In
USENIX Security, pages 601–618, 2016.
[39] Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner,
and Aleksander Madry. Robustness may be at odds with accuracy. arXiv
preprint arXiv:1805.12152, 2018.
[40] Yusuke Uchida, Yuki Nagai, Shigeyuki Sakazawa, and Shin’ichi Satoh.
In ICMR, pages
Embedding watermarks into deep neural networks.
269–277, 2017.
[41] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath,
Haitao Zheng, and Ben Y Zhao. Neural cleanse: Identifying and
mitigating backdoor attacks in neural networks.
In S&P, pages 707–
723. IEEE, 2019.
[42] Pete Warden. Speech commands: A dataset for limited-vocabulary
speech recognition. arXiv preprint arXiv:1804.03209, 2018.
[43] Dongxian Wu, Yisen Wang, Shu-Tao Xia, James Bailey, and Xingjun
Ma. Skip connections matter: On the transferability of adversarial
examples generated with resnets. ICLR, 2020.
[44] Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod
Lipson. Understanding neural networks through deep visualization.
arXiv preprint arXiv:1506.06579, 2015.
[45] Xiaoyong Yuan, Lei Ding, Lan Zhang, Xiaolin Li, and Dapeng Wu.
Es attack: Model stealing against deep neural networks without data
hurdles. arXiv preprint arXiv:2009.09560, 2020.
[46] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent
El Ghaoui, and Michael Jordan. Theoretically principled trade-off
between robustness and accuracy. In ICML, pages 7472–7482. PMLR,
2019.
[47] Jialong Zhang, Zhongshu Gu, Jiyong Jang, Hui Wu, Marc Ph Stoecklin,
Heqing Huang, and Ian Molloy. Protecting intellectual property of deep
neural networks with watermarking. In Asia CCS, pages 159–172, 2018.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:24:42 UTC from IEEE Xplore.  Restrictions apply. 
837
APPENDIX
A. Details of Datasets and Models
TABLE VIII: The hyper-parameters used in different test case
generation strategies.
We use four benchmark datasets from two domains for the
evaluation:
• MNIST [24]. This is a handwritten digits (from 0 to 9)
dataset, consisting of 70,000 images with size 28×28×1,
of which 60,000 and 10,000 are training and test data.
• CIFAR-10 [22]. This is a 10-class image classiﬁcation
dataset, consisting of 60,000 images with size 32×32×3,
of which 50,000 and 10,000 are training and testing data.
• ImageNet [36]. This is a large-scale image dataset con-
taining more than 1.2 million training images of 1,000
categories. It is more challenging due to the higher image
resolution 224×224×3. We randomly sample 100 classes
to construct a subset of ImageNet, of which 120,000 are
training data and 30,000 are testing data.
• Speech Commands [42]. This is an audio dataset of 10
single spoken words, consisting of about 40,000 training
samples and 4,000 testing samples. We pre-processed the
data to obtain a Mel Spectrogram [6]. Each audio sample
is transformed into an array of size 120 × 85.
To explore the scalability of DEEPJUDGE, various model
structures are tested as in Table III. LeNet-5, ResNet-20 and
VGG-16 are standard CNN structures, while LSTM(128) is
an RNN structure: an LSTM layer with 128 hidden units,
followed by three fully-connected layers (128/64/10).
B. Seed Selection Strategy
(cid:5)C
Seed selection is important for generating high-quality test
cases. We use DeepGini [12] to measure the certainty of each
candidate sample. Given the victim model f and a testing
dataset D, we ﬁrst calculate the Certainty Score (CS) for each
seed x ∈ D as: CS(f L, x) =
i (x)2, then we rank the
seed list by the certainty score, and the ﬁrst part of the seeds
of the highest scores (i.e., most certainties) will be chosen for
the following generation process. Here, we assume to have two
seeds {x1, x2} with CS(f L, x1) > CS(f L, x2), that means
the victim model f is more conﬁdent at x1, which also means
that x1 is farther from the decision boundary and easier for
classiﬁcation (see examples in Fig. 15).
i f L
C. Data-augmentation
During the ﬁnetuning and pruning processes, typical data-
augmentation techniques are used to strengthen the attacks
except for the SpeechCommands dataset, including random
rotation (10
◦), random width- and height-shift (both 0.1).
D. Test Case Generation Details and Calibrations
Speciﬁcally, we consider three adversarial attacks for gen-
erating black-box test cases (see Section IV-C1).
FGSM [14] perturbs a normal example x by one single step
of size  to maximize the model’s prediction error with respect
= x +  · sign((cid:9)xL(f L(x), y)),
to the groundtruth label y:x(cid:2)
where sign(·) is the sign function, L is the cross entropy (CE)
loss, and (cid:9)xL is the gradient of the loss to the input.
PGD [28] is an iterative version of FGSM but with smaller
step size: xk = Π(xk−1 + α · sign((cid:9)xL(f L(xk−1), y))),
Method
PGD [28]
Alg. 2
FGSM [14]
CW [4]
IPG [2]
0.03/10
Params MNIST CIFAR-10
/steps
m/iters
/steps
c/iters
k/iters
0.1/10
3/1k
0.1/1
5/1k
10/1k
3/1k
0.03/1
5/1k
10/1k
3/1k
0.01/1
5/1k
10/1k
ImageNet SpeechCmds
0.01/10
0.1/10
1/1k
0.1/1
5/1k
10/1k
TABLE IX: Time cost (seconds) of test cases generation.
Method
PGD [28]
Alg. 2
MNIST
CIFAR-10
ImageNet
SpeechCmds
0.3
635.3
3.7
1200.3
227.6
1280.1
1.2
4424.6
= min
x(cid:2)
(cid:10)x(cid:2) − x(cid:10)2
2 − c · L(f L(x(cid:2)
where xk is the adversarial example obtained at the k-th
perturbation step, α is the step size and Π is a projection
(clipping) operation that projects the perturbation back onto
the -ball centered around x if it goes beyond.
CW [4] generates adversarial examples by solving the opti-
mization problem: x(cid:2)
), y),
where c is a hyperparameter balancing the two terms and
the pixel values of adversarial example x(cid:2) are bounded to be
within a legitimate range, e.g., [0, 1] for 0-1 normalized input.
The hyper-parameters used for the generation algorithms on
different datasets are summarized in Table VIII. Here, we take
CIFAR-10 dataset as an example and analyze the inﬂuencing
factors of the test case generation process.
Adversarial Examples. PGD is the default choice for gen-
erating adversarial examples in the black-box setting. Here,
we further compare PGD with two other methods, FGSM
and CW. We use the same selected seeds for the generation.
Table X shows the results of the RobD metric. We observe
that the gap in RobD values between the positive and negative
suspect models is very small when CW is used, which fails to