### Cases to Cross the Adversarially Smoothed Decision Boundary within a Certain Perturbation Budget

Examples of high- and low-confidence test seeds are provided in Figure 15. It is also worth noting that our white-box testing performs well in this scenario. Overall, DEEPJUDGE is robust to adversarial training or can be made robust by efficiently updating the seeds.

### Transfer Learning

An adversary may transfer a stolen copy of the victim model to a new dataset, using the main structure of the victim model as a backbone and adding more layers. We tested a vanilla transfer learning (VTL) strategy from the 10-class CIFAR-10 to a 5-class SVHN [31]. The last layer of the CIFAR-10 victim model was replaced with a new classification layer, and all layers were fine-tuned on a subset of the SVHN data. In this setting, black-box metrics are no longer feasible because the suspect model has different output dimensions than the victim model. However, white-box metrics can still be applied since the shallow layers are retained. The results are reported in Table VII. Notably, DEEPJUDGE successfully identifies transfer learning attacks with distinctively low testing distances and an AUC of 1.

In a recent study [29], it was observed that knowledge from the victim model could be transferred to the stolen models. The Dataset Inference (DI) technique was proposed to determine whether the victim's knowledge (i.e., private training data) is preserved in the suspect model. We believe such knowledge-level testing metrics could be incorporated into DEEPJUDGE to make it more comprehensive. An analysis of how different levels of transfer learning affect DEEPJUDGE can be found in Appendix H.

**Remark 4:** DEEPJUDGE is fairly robust to adversarial fine-tuning, adversarial training, or transfer learning-based adaptive attacks, although it sometimes needs to regenerate the seeds or test cases.

## Conclusion

In this work, we introduced DEEPJUDGE, a novel testing framework for copyright protection of deep learning models. The core of DEEPJUDGE is a family of multi-level testing metrics that characterize different aspects of similarity between the victim model and a suspect model. Efficient and flexible test case generation methods are also developed to enhance the discriminating power of the testing metrics. Unlike watermarking methods, DEEPJUDGE does not require tampering with the model training process. Compared to fingerprinting methods, it can defend against more diverse attacks and is more resistant to adaptive attacks. DEEPJUDGE is applicable in both black-box and white-box settings against model fine-tuning, pruning, and extraction attacks. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness and efficiency of DEEPJUDGE. We have implemented DEEPJUDGE as a self-contained open-source toolkit. As a generic testing framework, new testing metrics or test case generation methods can be easily incorporated into DEEPJUDGE to help defend against future threats to deep learning copyright protection.

## Acknowledgements

We are grateful to the anonymous reviewers and shepherd for their valuable comments. This research was supported by the Key R&D Program of Zhejiang (2022C01018) and the NSFC Program (62102359, 61833015).

## References

[1] Yossi Adi, Carsten Baum, Moustapha Cisse, Benny Pinkas, and Joseph Keshet. Turning your weakness into a strength: Watermarking deep neural networks by backdooring. In USENIX Security, pages 1615–1631, 2018.

[2] Xiaoyu Cao, Jinyuan Jia, and Neil Zhenqiang Gong. IPGuard: Protecting intellectual property of deep neural networks via fingerprinting the classification boundary. In Asia CCS, pages 14–25, 2021.

[3] Nicholas Carlini, Matthew Jagielski, and Ilya Mironov. Cryptanalytic extraction of neural network models. In CRYPTO, pages 189–218. Springer, 2020.

[4] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In S&P, pages 39–57. IEEE, 2017.

[5] Chenyi Chen, Ari Seff, Alain Kornhauser, and Jianxiong Xiao. Deep-Driving: Learning affordance for direct perception in autonomous driving. In ICCV, pages 2722–2730, 2015.

[6] Keunwoo Choi, Deokjin Joo, and Juho Kim. Kapre: On-GPU audio preprocessing layers for a quick implementation of deep neural network models with Keras. In Machine Learning for Music Discovery Workshop at ICML, 2017.

[7] Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12(ARTICLE):2493–2537, 2011.

[8] Jacson Rodrigues Correia-Silva, Rodrigo F Berriel, Claudine Badue, Alberto F de Souza, and Thiago Oliveira-Santos. Copycat CNN: Stealing knowledge by persuading confession with random non-labeled data. In IJCNN, pages 1–8. IEEE, 2018.

[9] Bita Darvish Rouhani, Huili Chen, and Farinaz Koushanfar. DeepSigns: An end-to-end watermarking framework for ownership protection of deep neural networks. In ASPLOS, pages 485–497, 2019.

[10] Lixin Fan, Kam Woh Ng, and Chee Seng Chan. Rethinking deep neural network ownership verification: Embedding passports to defeat ambiguity attacks. 2019.

[11] Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. The robustness of deep networks: A geometrical perspective. IEEE Signal Processing Magazine, 34(6):50–62, 2017.

[12] Yang Feng, Qingkai Shi, Xinyu Gao, Jun Wan, Chunrong Fang, and Zhenyu Chen. DeepGini: Prioritizing massive tests to enhance the robustness of deep neural networks. In ISSTA, pages 177–188, 2020.

[13] Bent Fuglede and Flemming Topsoe. Jensen-Shannon divergence and Hilbert space embedding. In International Symposium on Information Theory, page 31. IEEE, 2004.

[14] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.

[15] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent neural networks. In ICASSP, pages 6645–6649. IEEE, 2013.

[16] Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. BadNets: Evaluating backdooring attacks on deep neural networks. IEEE Access, 7:47230–47244, 2019.

[17] Shangwei Guo, Tianwei Zhang, Han Qiu, Yi Zeng, Tao Xiang, and Yang Liu. The hidden vulnerability of watermarking for deep neural networks. arXiv preprint arXiv:2009.08697, 2020.

[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770–778, 2016.

[19] Matthew Jagielski, Nicholas Carlini, David Berthelot, Alex Kurakin, and Nicolas Papernot. High accuracy and high fidelity extraction of neural networks. In USENIX Security, pages 1345–1362, 2020.

[20] Hengrui Jia, Christopher A Choquette-Choo, Varun Chandrasekaran, and Nicolas Papernot. Entangled watermarks as a defense against model extraction. In USENIX Security, 2021.

[21] Mika Juuti, Sebastian Szyller, Samuel Marchal, and N Asokan. PRADA: Protecting against DNN model stealing attacks. In EuroS&P, pages 512–527. IEEE, 2019.

[22] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.

[23] Erwan Le Merrer, Patrick Perez, and Gilles Trédan. Adversarial frontier stitching for remote neural network watermarking. Neural Computing and Applications, 32(13):9233–9244, 2020.

[24] Yann LeCun, Corinna Cortes, and CJ Burges. MNIST handwritten digit database. 2010.

[25] Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-pruning: Defending against backdooring attacks on deep neural networks. In International Symposium on Research in Attacks, Intrusions, and Defenses, pages 273–294. Springer, 2018.

[26] Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value of network pruning. arXiv preprint arXiv:1810.05270, 2018.

[27] Nils Lukas, Yuxuan Zhang, and Florian Kerschbaum. Deep neural network fingerprinting by conferrable adversarial examples. In ICLR, 2021.

[28] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.

[29] Pratyush Maini, Mohammad Yaghini, and Nicolas Papernot. Dataset inference: Ownership resolution in machine learning. In ICLR, 2021.

[30] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A survey on bias and fairness in machine learning. arXiv preprint arXiv:1908.09635, 2019.

[31] Y Netzer, T Wang, A Coates, A Bissacco, B Wu, and AY Ng. Reading digits in natural images with unsupervised feature learning. In Workshop on deep learning and unsupervised feature learning, 2011.

[32] Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz. Knockoff nets: Stealing functionality of black-box models. In CVPR, pages 4954–4963, 2019.

[33] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. Practical black-box attacks against machine learning. In Asia CCS, pages 506–519, 2017.

[34] Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. DeepXplore: Automated whitebox testing of deep learning systems. In SOSP, pages 1–18, 2017.

[35] Alex Renda, Jonathan Frankle, and Michael Carbin. Comparing rewinding and fine-tuning in neural network pruning. arXiv preprint arXiv:2003.02389, 2020.

[36] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. ImageNet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211–252, 2015.

[37] Or Sharir, Barak Peleg, and Yoav Shoham. The cost of training NLP models: A concise overview. arXiv preprint arXiv:2004.08900, 2020.

[38] Florian Tramèr, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas Ristenpart. Stealing machine learning models via prediction APIs. In USENIX Security, pages 601–618, 2016.

[39] Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. Robustness may be at odds with accuracy. arXiv preprint arXiv:1805.12152, 2018.

[40] Yusuke Uchida, Yuki Nagai, Shigeyuki Sakazawa, and Shin’ichi Satoh. Embedding watermarks into deep neural networks. In ICMR, pages 269–277, 2017.

[41] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In S&P, pages 707–723. IEEE, 2019.

[42] Pete Warden. Speech commands: A dataset for limited-vocabulary speech recognition. arXiv preprint arXiv:1804.03209, 2018.

[43] Dongxian Wu, Yisen Wang, Shu-Tao Xia, James Bailey, and Xingjun Ma. Skip connections matter: On the transferability of adversarial examples generated with ResNets. ICLR, 2020.

[44] Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson. Understanding neural networks through deep visualization. arXiv preprint arXiv:1506.06579, 2015.

[45] Xiaoyong Yuan, Lei Ding, Lan Zhang, Xiaolin Li, and Dapeng Wu. ES attack: Model stealing against deep neural networks without data hurdles. arXiv preprint arXiv:2009.09560, 2020.

[46] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. Theoretically principled trade-off between robustness and accuracy. In ICML, pages 7472–7482. PMLR, 2019.

[47] Jialong Zhang, Zhongshu Gu, Jiyong Jang, Hui Wu, Marc Ph Stoecklin, Heqing Huang, and Ian Molloy. Protecting intellectual property of deep neural networks with watermarking. In Asia CCS, pages 159–172, 2018.

## Appendix

### A. Details of Datasets and Models

**Table VIII: Hyper-parameters used in different test case generation strategies.**

We use four benchmark datasets from two domains for the evaluation:

- **MNIST [24]:** A handwritten digits (from 0 to 9) dataset, consisting of 70,000 images with size 28×28×1, of which 60,000 and 10,000 are training and test data.
- **CIFAR-10 [22]:** A 10-class image classification dataset, consisting of 60,000 images with size 32×32×3, of which 50,000 and 10,000 are training and testing data.
- **ImageNet [36]:** A large-scale image dataset containing more than 1.2 million training images of 1,000 categories. It is more challenging due to the higher image resolution 224×224×3. We randomly sample 100 classes to construct a subset of ImageNet, of which 120,000 are training data and 30,000 are testing data.
- **Speech Commands [42]:** An audio dataset of 10 single spoken words, consisting of about 40,000 training samples and 4,000 testing samples. We pre-processed the data to obtain a Mel Spectrogram [6]. Each audio sample is transformed into an array of size 120 × 85.

To explore the scalability of DEEPJUDGE, various model structures are tested as in Table III. LeNet-5, ResNet-20, and VGG-16 are standard CNN structures, while LSTM(128) is an RNN structure: an LSTM layer with 128 hidden units, followed by three fully-connected layers (128/64/10).

### B. Seed Selection Strategy

Seed selection is crucial for generating high-quality test cases. We use DeepGini [12] to measure the certainty of each candidate sample. Given the victim model \( f \) and a testing dataset \( D \), we first calculate the Certainty Score (CS) for each seed \( x \in D \) as:
\[ \text{CS}(f, x) = \sum_i (f(x)_i)^2 \]
Then, we rank the seed list by the certainty score, and the top seeds with the highest scores (i.e., most certainties) will be chosen for the following generation process. For example, if we have two seeds \(\{x_1, x_2\}\) with \(\text{CS}(f, x_1) > \text{CS}(f, x_2)\), it means the victim model \( f \) is more confident at \( x_1 \), indicating that \( x_1 \) is farther from the decision boundary and easier to classify (see examples in Fig. 15).

### C. Data Augmentation

During the fine-tuning and pruning processes, typical data augmentation techniques are used to strengthen the attacks, except for the SpeechCommands dataset. These techniques include random rotation (10°), random width- and height-shift (both 0.1).

### D. Test Case Generation Details and Calibrations

Specifically, we consider three adversarial attacks for generating black-box test cases (see Section IV-C1):

- **FGSM [14]:** Perturbs a normal example \( x \) by one single step of size \( \epsilon \) to maximize the model’s prediction error with respect to the ground truth label \( y \):
  \[ x' = x + \epsilon \cdot \text{sign}(\nabla_x L(f(x), y)) \]
  where \(\text{sign}(\cdot)\) is the sign function, \( L \) is the cross-entropy (CE) loss, and \(\nabla_x L\) is the gradient of the loss with respect to the input.

- **PGD [28]:** An iterative version of FGSM but with smaller step size:
  \[ x_k = \Pi_\epsilon(x_{k-1} + \alpha \cdot \text{sign}(\nabla_x L(f(x_{k-1}), y))) \]
  where \( x_k \) is the adversarial example obtained at the k-th perturbation step, \( \alpha \) is the step size, and \(\Pi_\epsilon\) is a projection (clipping) operation that projects the perturbation back onto the \(\epsilon\)-ball centered around \( x \) if it goes beyond.

- **CW [4]:** Generates adversarial examples by solving the optimization problem:
  \[ x' = \arg\min_{x'} \|x' - x\|_2^2 - c \cdot L(f(x'), y) \]
  where \( c \) is a hyperparameter balancing the two terms, and the pixel values of the adversarial example \( x' \) are bounded to be within a legitimate range, e.g., [0, 1] for 0-1 normalized input.

The hyper-parameters used for the generation algorithms on different datasets are summarized in Table VIII. Here, we take the CIFAR-10 dataset as an example and analyze the influencing factors of the test case generation process.

**Adversarial Examples:**
PGD is the default choice for generating adversarial examples in the black-box setting. We further compare PGD with two other methods, FGSM and CW, using the same selected seeds for generation. Table X shows the results of the RobD metric. We observe that the gap in RobD values between the positive and negative suspect models is very small when CW is used, which fails to distinguish between them effectively.