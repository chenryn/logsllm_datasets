3
caches, TLBs [34], branch predictors [41], and DRAM row
buffers [42], on which resource contention leads to the state
changes observable to the adversary. The second category
includes I/O buses [43], [44], [45], [46], execution ports [47],
[48], [49], cache-bank [50], memory bus lock [51], CPU
interconnect [52], etc. Yet, we found the security implications
of CPU interconnect contention have not been thoroughly
investigated, even though it has some very attractive properties,
like residing in all contemporary CPUs and requiring no
special features to be used by victim applications. These
properties do not always hold in other stateless channels:
e.g., VAX security kernel has led to I/O bus contention but
it is outdated [43], [44], and ports contention rely on Intel
SMT [47], [48], [49]. Though Wang et al. used a simulator
to study the timing side-channel of on-chip network [52], the
attack has not been demonstrated on real CPUs. In this work,
we investigate this under-studied channel.
Comparison to LoR (“Lord of Rings”)
[8]. LoR was
proposed to build side-channel over CPU ring interconnect,
which is equipped by consumer CPUs and old server CPUs.
Different from LoR, our attack MESHUP focuses on mesh
interconnects that is equipped by current server-grade CPUs.
There are also conceptual differences between LoR and
MESHUP. First, consumer-grade CPUs come with inclusive
cache, but server-grade CPUs use distributed and non-inclusive
cache, in which case the path is unknown to attackers. Second,
in addition to the eviction-based probe, MESHUP has a new
coherence-based probe that enables cross-CPU attack.
III. ATTACK OVERVIEW
In this section, we ﬁrst introduce the threat model and
compare it with prior works. Then, we overview our attack
MESHUP.
A. Threat Model
We assume the attacker who intends to extract secret (e.g.,
encryption keys) shares the same CPU with the victim but
resides in different cores. We envision MESHUP is effective in
the cloud environment, when co-residency on CPU or machine
can be achieved [54]. We also assume the existing hardware
and software defenses against the cache side-channel attacks
are deployed and turned on, like page coloring [55], [56]
and CATalyst [7]. We target Intel Xeon SP, where core-to-
core communication goes through mesh interconnects or UPI
connections. We assume that the victim application is non-
trivial, which generates observable mesh trafﬁc, either through
a large secret working set to cause private cache eviction, or
frequent memory access. Our evaluation in Section VII shows
this feature is common in off-the-shelf applications.
Here we compare our setting to the existing cache attacks.
The strongest assumption made by the prior works is the
sharing of memory addresses (shown in Figure 2 left), like
FLUSH+RELOAD. However, memory sharing can be turned
off for the critical data/code. A weaker assumption is that
cache sets are shared (shown in Figure 2 right), so the attacker
can evict cache lines of the victim (or vice versa),
like
Fig. 2: Comparison of the settings between different cache
side-channel attacks.
PRIME+PROBE. Under this assumption, the attacker either
shares in-core private L1/L2 cache with the victim [31], out-
of-core LLC (either inclusive LLC [25], or non-inclusive
LLC [22]). However, the assumption does not always hold
when software defenses like page coloring or hardware de-
fenses like CATalyst are deployed to enforce spatial isolation.
Recently, temporal isolation has been proposed to mitigate
all existing cache side-channels. Appendix B describes these
defenses in detail and Table I summarizes the representative
attacks and how they are impacted under the existing defenses.
Stateless Cache Side-channel Attack on Mesh. Since the
data movement on the mesh keeps occurring for all sorts
of program activities, if the attacker’s and victim’s programs
happen to share a mesh route, the victim’s activities might
be inferred, which potentially include cache accesses. In this
process, the attacker could just access his/her own resources.
Based on this insight, we develop MESHUP to exploit the
contention on the mesh.
isolation:
MESHUP is expected to bypass page coloring, hardware
isolation and temporal
it does not cause cache
conﬂicts between victim and attacker, so page coloring and
hardware isolation like CAT and TSX can be evaded. Though
the time protection of [5] is very effective against the existing
stateful cache side-channel attacks, the authors admit that they
are “powerless” against stateless channels, since there is no
“appropriate hardware support” to partition interconnects.
B. Attack Steps
MESHUP consists of two phases to tackle the challenges
(summarized in Section I) of creating a reliable side-channel.
Phase 1: Probe & Measurement (Section V). When launch-
ing attacks, the attacker runs an application sharing CPU with
the victim. The attacker randomly selects a mesh path and then
tries to trigger trafﬁc along the path. To direct mesh packets
over the selected path, the attacker either 1) constructs an
eviction set and probes the memory addresses related to it, or
2) causes cache line synchronization across CPUs. The probe
is issued repeatedly and the delays are logged.
Phase 2: Secret Inference (Section VII). After the prior step,
the attacker obtains the delay trace, and the secret underlying
the trace is to be decoded. This step is application-speciﬁc,
as different victim programs produce different patterns. This
step can be done at the attacker’s own machine. We use RSA
4
L1, L2Core1L1, L2Core2LLC SliceLLC SliceMeshCore co-locate:LLC co-locate:Mesh co-locate:lineMemory Share AssumptionCo-location AssumptionFlush+Reload [1]
Prime+Probe [25], [4]
TLBleed[34]
Attack Directories,
not caches [22]
Prime+Abort[2]
Xlate[3]
LoR[8]
MESHUP
Co-location
Assumption
Memory
Cache
Core
Cache
Cache
Cache
Ring
Mesh
Attack
Channel
LLC
LLC
TLB
Directory
TSX Status
MMU
Ring
Mesh
Coloring
Page
(cid:88)
(cid:88)
-
(cid:88)
(cid:88)
-
-
-
Hardware
Isolation
CAT
CAT
Disable HT
SecDir [53]
CAT
-
-
-
Temporal
Isolation
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
-
-
Key Feature
Exploit shared memory
No need to share memory
Attack TLB not cache
Work on non-inclusive cache
Does not rely on timing instruction
Lure MMU to access cache
Stateless, consumer-grade CPUs
Stateless, server-grade CPUs, cross cores and CPUs
TABLE I: Cache side-channels under defenses. “(cid:88)” means the attack can be defended. “-” means the defense is ineffective.
encryption and app ﬁngerprinting to showcase how to decode
a delay trace.
IV. CHARACTERIZATION OF MESH TRAFFIC
Before introducing the mechanisms of MESHUP, we catego-
rize mesh trafﬁc generated during the lifetime of a cache line,
and show the characteristics of each type, which motivates the
design of MESHUP.
A. Cache Access
policy [22] to evict a line from L2 to LLC (or drop directly,
depending on eviction policy) (Step 4(cid:13)). Evicted lines will be
accessed from LLC the next time (Step 5(cid:13)).
To be noticed is that all the ﬁve steps leverage mesh (plus
UPI when crossing CPUs) to deliver transaction messages as
well as the cache lines. This is critical to our attack as it
exploits the statistics of the transactions.
B. Mesh Trafﬁc Categorization
Below we introduce the types of mesh trafﬁc (termed T1-
T7) related to cache access, summarized from the existing
documents [57]. We use r and t to refer to the entity issuing
the request and the target. Hence, the requesting core is termed
Corer, and the co-located L2, LLC slice and CHA are termed
L2r, LLCr and CHAr . For the target, the terms are Coret,
L2t, LLCt and CHAt.
• T1: Corer to LLCt. When Corer encounters L2 cache
miss, it will send messages to CHAt co-located with
LLCt, asking if the cache line is presented. Also, L2r
may evict a line to LLC(cid:48)
t when the L2 cache set to be
inserted is full. The memory sub-system of the core will
pass the line from L2r to LLC(cid:48)
t .
• T2: LLCt to Corer. Following T1, if the cache line is
in LLCt, LLCt will send the cache line to Corer.
• T3: IMC to Corer. Alternatively, if the cache line is not
present in LLCt, CHAt will ask the IMC to fetch the
line from memory and send it to Corer. To be noticed
is that the line is directly sent to Corer, when LLC is
non-inclusive.
• T4: LLCt to IMC. When LLCt is full, to accept new
cache line insertion, it will evict the least recently used
cache line to the IMC.
• T5: between Corer and Coret. Corer can access a
cache line in the private cache of Coret when they share
memory. L2t will pass the cache line to Corer through
mesh, once Corer is going to write the line.
• T6: LLCt to I/O Unit. Intel CPUs allow I/O devices
to directly access LLC and bypass memory for better
performance, under DDIO [14]. In this case, cache lines
will be passed between a PCIe stop (stop inside a PCIe
tile) and LLCt.
• T7: Other trafﬁc. It characterizes the mesh trafﬁc un-
documented by Intel, which is expected to have a smaller
volume than T1-T6.
Fig. 3: The process of cache access.
Here we describe the process of cache access, focusing on
the non-inclusive LLC adopted by Xeon SP (also illustrated in
Figure 3). When a core accesses a fresh memory address (the
core is called requesting core), both L1 and L2 cache would
miss. The memory sub-system of the requesting core computes
a CHA ID (target CHA) from the memory address with a
proprietary hash algorithm, to know which CHA (target CHA)
is responsible for the address. Then a read transaction is sent
to the target CHA (Step 1(cid:13)), which replies to the requesting
core and updates the directory accordingly. Since the line is not
cached in the LLC nor another core, the CHA cannot complete
the request by itself, and it will ask the IMC to fetch the line
from memory (Step 2(cid:13)), and IMC will send the line directly to
the requesting core (Step 3(cid:13)). The cache line will be inserted
into the L2 of the requesting core, but the LLC will be kept
untouched, because it is non-inclusive. When the requesting
core runs out of the L2 cache, it will follow a pseudo-LRU
5
  L2 miss  LLC miss  Cache line sent to the requesting core  L2  eviction  LLC hitIMCL2CoreCHALLC sliceL2CoreCHALLC sliceIn Section VI-F, we summarize our insights about different
trafﬁc types.
V. THE PROBE DESIGN
Below we describe two probes under MESHUP, which
selects a path on the interconnect, hoping to contend with
the victim’s mesh trafﬁc and triggers mesh transactions.
A. Probe based on Cache Eviction
Different from routing a packet on the Internet, in mesh
interconnect, a program cannot explicitly sends trafﬁc to the
destination, because cache transactions are triggered implicitly.
To address this challenge, we adapt the existing methods for
constructing evictions sets [4], [22]. An eviction set is a large
set of memory addresses that are mapped to the same cache
set, so accessing the whole set will result in cache-set overﬂow
and cache eviction. Previous attacks, e.g., PRIME+PROBE, use
an eviction set to evict lines of the private caches in the victim
core. Though MESHUP uses eviction set, our goal is not to
evict victim addresses. To the contrary, MESHUP evicts lines
of its own private L2 cache, in order to generate mesh trafﬁc
ﬂowing to a designated LLC slice, which is distinguished by
the mesh tile. As such, MESHUP stays out of the protection
realm of any existing defense. In Figure 4, we illustrate the
concept of our probe.
Fig. 4: The probe designed by the attacker.
Noticeably, we assume the attack can be executed on a cloud
VM, where the attacker cannot pin the process to his/her
desired core or learn which core is occupied by the victim
application. In other words, the attacker cannot select the
optimal path for contention. Still, our evaluation shows the
chances of contention are high on a random path.
Constructing Eviction Set. First, the attacker prepares a set
of memory addresses (denoted as EV ) that are mapped to one
L2 cache set. The number of addresses (denoted as n) in EV
is set to be larger than the number of ways (denoted as w) an
L2 set has, therefore when requesting addresses of EV , L2
cache misses always happen after w requests. From w + 1 to
n requests, each time a line is evicted from L2 to LLC, a new
line from memory will be inserted to L2. After that, when
requesting EV again, n lines will be evicted from L2 to LLC,
and n lines will be passed from LLC to L2 in return, resulting
in stable bi-directional mesh trafﬁc on the attack path.
To get trafﬁc on the ﬁxed mesh route, MESHUP needs
to force all L2 misses to be served by one LLC slice.
Thus, EV is not only mapped to a set of L2, but also
a set of LLC slices. To ﬁnd addresses for such EV , we
use the two routines proposed by [22], check_conflict
6
and find_EV, which are designed for non-inclusive LLC.
In essence, check_conflict tries to test if removing an
address of a set makes cache conﬂict disappear. find_EV
tries to utilize check_conflict to ﬁlter out a set of
addresses that are all mapped to the same LLC slice. In
Appendix C, we describe them in detail. We split the EV
into EV0 and EV1 of equal size, and the addresses on the two
sets have different 16th bit (0 and 1). As shown in Figure 9
of Appendix A, bits 15:6 of a memory address point to an L2
set, while bits 16:6 point to an LLC slice set. As such, EV
is associated with a single L2 cache set, and EV0, EV1 are
mapped to two different sets of the same LLC slice.
Even when all cache lines fall into one LLC set, the attacker
cannot decide which LLC slice serves the Corer yet. To
identify the LLC slice, Yan et al. suggest testing an EV
(multiple EV have been constructed as candidates) to see if
it co-locates with Corer [22]. We adopt the same approach