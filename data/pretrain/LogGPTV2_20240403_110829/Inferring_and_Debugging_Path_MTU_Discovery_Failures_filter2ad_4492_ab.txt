6.
TTL 255, Size 1492
TTL 255, Size 1492
TTL 255, Size 1481
TTL 255, Size 1481
TTL 1, Size 1500
12. ICMP Time Exceeded
TTL 3, Size 1500
TTL 3, Size 1500
Figure 1: Inferring the MTU without feedback. An ICMP
Black Hole exists between routers R1 and R3 where the
MTU is restricted to 1480 bytes. A PMTUD failure is de-
tected with probes 1 and 2, probes 3 to 10 infer that the
next-hop MTU is 1480, and probes 11 to 14 infer that the
large packets are probably being discarded at hop 2.
Time Exceeded messages with small TTL-limited probes
from adjacent hops but we only receive Time Exceeded
messages with large probes from the ﬁrst hop in the path,
we infer that the failure mode is likely to be either due to an
interface being conﬁgured to not send any ICMP Destina-
tion Unreachable messages, or an MTU mismatch between
the adjacent routers, or the PTB message originating from
a different interface than the interface that sends Time Ex-
ceeded messages – with a source address that causes the
PTB message to be subsequently ﬁltered.
3.3 Inferring MTU with Invalid Feedback
This technique is used when a PTB message is received
in response to a large probe, but the next-hop MTU in-
cluded in the PTB message is either not set, or is larger
than the probe which triggered the message. This tech-
nique uses a variation of the next-hop MTU search tech-
nique described in Section 3.1; instead of using the absence
of a PTB message to reduce the upper-bound of the search
space, this technique uses the faulty PTB message. This
method can converge on the actual next-hop MTU fairly
rapidly if ICMP feedback is received for packets smaller
than the next-hop MTU past the faulty router, as the test for
each probe size costs one round-trip-time (RTT). We use
a slightly different technique if the path does not provide
ICMP feedback after the faulty router due to another fail-
ure further in the path. When this occurs, scamper works
progressively downwards through the MTU table soliciting
faulty PTB messages rather than moving progressively up-
wards, as it would normally do. This is because scamper
has to time-out on a probe which does not obtain ICMP
feedback before it can send another probe, which has a
much larger cost than sending packets which trigger faulty
PTB messages.
3.4 Limitations
As the techniques we described rely on ICMP messages as
feedback, they can be unreliable when ICMP rate-limiting
is encountered. By default, scamper will send each probe
twice before trying another probe type, with a ﬁve second
timeout between each attempt. If two successive probes do
not receive ICMP feedback due to rate-limiting, we may in-
fer an incorrect next-hop MTU, or infer the wrong location
of a failure, or infer a failure where one does not exist.
4 Methodology
We collected PMTUD failure data from two IPv4 hosts
with 9000-byte MTU interfaces connected to networks that
peer with Internet2, which itself is 9000-byte clean through
the core, on April 28th 2005. The ﬁrst location was from
NYSERNet in New York, and the second was an Internet2
measurement machine in Chicago. The target list consists
of 147 NLANR AMP machines, which are typically either
on university campuses connected to the Internet2 network,
or connected to networks that peer with Internet2. Most of
the AMP machines connect to their host network with an
Intel Pro100 Ethernet interface, which is capable of send-
ing 1500 byte IP packets. Some have Gigabit Ethernet in-
terfaces which are capable of sending IP packets larger than
1500 bytes, but are not conﬁgured to do so. The purpose of
this dataset is to understand PMTUD failures on networks
that can natively carry jumbo packets, and thus will require
fragmentation at least at the edge of the campus network
closer to each individual AMP machine.
5 Results
Of the 147 AMP machines in each dataset, we were able
to complete a traceroute to at least 134 machines, or 91%
of the target list. However, we inferred a PMTUD failure
for 30% of the reachable machines. A summary of the fail-
ures is presented in Table 1. We categorised the failures
into four groups: failure points where no ICMP messages
196
Internet Measurement Conference 2005
USENIX Association
Dataset:
Location:
Hostname:
Date / Time:
Target Count:
Reachable:
PMTUD Failures:
No ICMP messages:
No PTB messages:
Incorrect PTB messages:
Target MTU Mismatch:
NYSERNet-east
New York, NY
east.nysernet.org
Apr 28 2005, 21:50 EDT
147
136 (92.5%)
41 (30.1%)
6 (6 unique)
26 (17 unique)
2 (2 unique)
7 (7 unique)
nms1-chin
Chicago, IL
nms1-chin.abilene.ucaid.edu
Apr 28 2005, 20:10 CDT
147
134 (91.2%)
40 (29.9%)
5 (5 unique)
27 (18 unique)
2 (2 unique)
6 (6 unique)
Intersection
Total
–
–
–
147
134
25
–
–
–
–
–
–
4 (4 unique)
13 (13 unique)
2 (2 unique)
6 (6 unique)
7 unique
22 unique
2 unique
7 unique
Table 1: Summary of the two data collections. 30% of reachable targets had a PMTUD failure.
are received (7), failure points where no PTB message is
received (22), failure points where a PTB message is re-
ceived with an incorrect next-hop MTU (2), and target ma-
chines which have an MTU mismatch with a router on their
subnet (7). We identify a failure point by the IP addresses
either side of the fault in the IP path. For example, the fail-
ure point would be identiﬁed as being between R1 and R3
in Figure 1. For each fault, we approached the technical
and administrative contacts for the relevant AMP machine
if the fault was determined to be local to that campus, or
the operators of the relevant transit network.
We inferred seven failure points from which we did not
receive any ICMP messages; of these, six were at routers
where the next-hop MTU was inferred to be 1500 bytes,
while the seventh had a next-hop MTU of 1536 bytes. One
failure appeared to be caused by two successive routers in
the path that both sent ICMP messages with a source ad-
dress of 127.0.0.1, which were then discarded by a ﬁlter
close to both of our measurement hosts. Similarly, an-
other router located at the campus border used RFC 1918
addresses to number its interfaces, which also caused all
ICMP messages from it to be ﬁltered out. Another fail-
ure was caused by a BGP routing issue that, despite the
fact that end-to-end connectivity was available, a signiﬁ-
cant portion of the routers on the forward path had no route
back to the source host. This included one router which
was therefore unable to send a PTB message to the source
to signal that it was sending packets which were too big to
forward. Finally, one other was due to a ﬁrewall designed
to protect systems from security exploits by blocking all
packets with a source or destination address matching par-
ticular addresses, including the addresses of core routers.
We found 22 hops from which we received ICMP Time
Exceeded messages, but did not receive PTB messages
when it was inferred that we should have. Sixteen of these
hops had a next-hop MTU of 1500 bytes, accounting for
just over two-thirds of the failures. Due to the method of
counting hops where a failure occurs, the actual number
of unique failure locations is a little less, as there is some
repetition in the source address of some failure points. We
determined that there were 20 failure locations. Two points
were upgraded before a diagnosis could be obtained. We
obtained a technical diagnosis of each fault for seven fail-
ures; three reported that they had disabled ICMP Destina-
tion Unreachable messages, while the other four were the
result of an MTU mismatch or misconﬁguration. For the
11 other failures for which we do not have a technical diag-
nosis, we probed the particular routers with a UDP probes
to unused ports, in order to determine if they had disabled
Destination Unreachable messages or not. Eight systems
did not reply with a Destination Unreachable message.
We found two hops at one location from which we re-
ceived a PTB message, but the next-hop MTU reported in
the message was incorrect. The particular router would
send a PTB message with a suggested next-hop MTU of
4586.
It was, however, unable to forward packets larger
than 4472 bytes to the next hop.
Seven targets were inferred to be on a subnet where
nodes did not have a consistent agreement regarding the
MTU. Two of the seven AMP targets with an MTU mis-
match were able to receive IP packets larger than 1500
bytes, despite their use of 1500 byte MTU interfaces. One
was able to receive packets up to 2016 bytes, while the
other was able to receive packets up to 1506 bytes. We
established that IP packets were arriving complete at these
monitors by examining the probe packets with tcpdump.
6 Two Anecdotes
As discussed in Section 3.3, we implemented a technique to
infer the correct next-hop MTU when a router sends a PTB
message with an invalid next-hop MTU. The data included
in this paper did not include such a failure, although we
encountered one when implementing our tool. The router
in question was located in New York City in the network
of a large Internet Service Provider. For packet sizes be-
tween 4458 and 4470 bytes, the router would return a PTB
message with an invalid next-hop MTU of 4470. Initial at-
USENIX Association
Internet Measurement Conference 2005  
197
tempts to determine the cause of what appeared to be a bug
were difﬁcult. Initially, we were told the fault was some-
how related to the next-hop having an MPLS header with
room for three 4-byte MPLS labels. It was also suggested
that the fault could be a particular known router bug, al-
though the bug number suggested seems unrelated. At this
time we have been unable to determine the cause of the
fault, and are pursuing this matter with a router vendor.
Unspeciﬁed router bugs can also prevent PMTUD from
succeeding, as discussed in Section 2.5. During the course
of scamper’s development, we found an IPv6 router which
appeared to route IPv6 packets over an IPv6-in-IPv4 tunnel
with an MTU of 1480 bytes. However, for IPv6 packets
larger than 1480 bytes, we did not receive any PTB mes-
sages. Rather, it sent two Destination Unreachable, No
Route messages. The ﬁrst message was returned with the
IPv6 probe packet intact and caused scamper to cease PM-
TUD to the target beyond it. The second message – which
we picked up by accident while monitoring all ICMPv6
packets into the machine – was unable to be matched to
any probe we sent, as the encapsulated probe packet had the
source and destination port ﬁelds zeroed out. We contacted
the site responsible and reported the fault. To our knowl-
edge, the fault was never identiﬁed and corrected, and went
away when the particular path was replaced with a native
IPv6 path.
7 Conclusion
The consensus is that Path MTU Discovery – in its current
form – is unreliable due to it relying on the timely delivery
of PTB messages, which are disabled or ﬁrewalled in many
networks. We hypothesise that these failures go unnoticed
in routine operational testing and monitoring, as they are
only noticeable with larger probe packets. The default size
of probe packets sent using traceroute and ping is too small
to trigger PMTUD failures, and in the absence of packet
loss with these basic connectivity measures, it is tempting
to declare a path as fully operational.
In this paper, we presented a series of debugging tech-
niques which infer PMTUD failures on the forward path.
Using our implementation, we collected data on PMTUD
failures found in jumbogram-capable networks. We found
that of the reachable targets, 30% had a failure that would
prevent efﬁcient end-to-end communication from taking
place. Less than half of these failures were caused by a
conﬁguration decision to disable the ICMP messages that
are necessary for PMTUD to work. As the Internet MTU
is raised, particularly as jumbo-capable Ethernet interfaces
become more commonplace and jumbo transit services are
offered, it seems likely that the classical PMTUD meth-
ods will continue to be strained. Until the new approach to
PMTUD is completed and widely deployed amongst end-
hosts, we believe our tool is a useful operational utility.
Acknowledgements
scamper’s development was generously funded by the
WIDE project in association with CAIDA from April 2004
to March 2005. The NLANR Measurement and Network
Analysis Group (NLANR/MNA) is supported by the Na-
tional Science Foundation (NSF) under cooperative agree-
ment no. ANI-0129677. Matt Zekauskas (Internet2) col-
lected the nms1-chin dataset. Maureen C. Curran and Joe
Groff provided valuable editorial assistance. Matt Brown,
Nevil Brownlee, Alan Holt, and Perry Lorier provided use-
ful feedback on the paper.
References
[1] C.A. Kent and J.C. Mogul. Fragmentation considered
harmful. ACM SIGCOMM Computer Communication
Review, 17(5):390–401, 1987.
[2] J. Mogul and S. Deering. Path MTU Discovery. RFC
1191, IETF, November 1990.
[3] J. McCann, S. Deering, and J. Mogul. Path MTU Dis-
covery for IP version 6. RFC 1981, IETF, August
1996.
[4] K. Lahey. TCP problems with Path MTU Discovery.
RFC 2923, IETF, September 2000.
[5] R. van den Berg and P. Dibowitz. Over-zealous se-
curity administrators are breaking the Internet.
In
Proceedings of LISA ’02: Sixteenth Systems Admin-
istration Conference, pages 213–218, Berkeley, CA,
November 2002.
[6] A. Medina, M. Allman, and S. Floyd. Measuring
the evolution of transport protocols in the Internet.
ACM SIGCOMM Computer Communication Review,
35(2):37–52, April 2005.
[7] S. Knowles. IESG advice from experience with Path
MTU Discovery. RFC 1435, IETF, March 1993.
[8] J. Postel.
Internet Control Message Protocol. RFC
792, IETF, September 1981.
[9] Y. Rekhter, B. Moskowitz, D. Karrenberg, G.J.
de Groot, and E. Lear. Address allocation for private
internets. RFC 1918, IETF, February 1996.
[10] K. Cho, M. Luckie, and B. Huffaker.
Identifying
IPv6 network problems in the dual-stack world.
In
Proceedings of the ACM SIGCOMM workshop on
Network troubleshooting: research, theory and op-
erations practice meet malfunctioning reality, pages
283–288, Portland, OR., September 2004.
198
Internet Measurement Conference 2005
USENIX Association