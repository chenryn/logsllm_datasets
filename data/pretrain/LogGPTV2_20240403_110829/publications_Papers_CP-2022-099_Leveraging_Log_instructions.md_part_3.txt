anomalouseventsandappearjustwhentheblock idisanoma-
the scores, such that the logs with lower normality scores
lous. Therefore, they are indicators of an anomaly. We used
are reported as anomalous. We calculate the threshold a˜ on
this observation to create two datasets, which we refer to as
a separate validation set. The validation set is composed of
HDFS-sin and HDFS-seq. HDFS-sin is composed of time-
the normal target-system data and the ”abnormal” SL class.
ordered logs with a label for each event if it is anomalous or
The threshold is set to the score that maximizes the chosen
not. This data allows the evaluation of methods performance
performance criteria (e.g., F score) on the validation set.
1 in absence of external identifiers, i.e., single log anomaly
detection. HDFS-seq uses the block id as a natural identifier
IV. EXPERIMENTALDESIGNANDEVALUATION
to construct sequences of events, and it is used in sequential
In this section, we present the experimental evaluation anomaly detection evaluation.
of ADLILog in comparison to three state-of-the-art and four BGL contains 4,474,963 logs collected from a BlueGene/L
traditional log-based anomaly detection methods. We set the supercomputeratLivermoreLab[26].BGLhastwoimportant
focus on evaluating the detection performance, as the precise characteristics. The first BGL characteristic is the availability
detection of anomalies is a key quality indicator for real- of labels for individual log events given by the system ad-
world deployment. The performance evaluation is made on ministrators. We use these labels as ground truth information
HDFS and BGL [7] (as commonly used benchmark datasets), for single log line anomaly detection. We refer to this data
using three performance evaluation metrics. To estimate the as BGL-sin (with 348,460 anomalous logs). The second BGL
ADLILog’s deployment complexity, we further analyze the characteristic is the absence of identifiers for task sequences.
quality and quantity of the training data and two hyperpa- To create log sequences, similar to related work [6], we use
rameters of the method and the learning procedure. These a time window of size T. Following, He et al. [6] we set
experiments evaluate the practical value of ADLILog. T = 6h, as optimal for BGL. This resulted in a total of 828
log sequences. To obtain sequence labels, similar to related TABLEIV
work [6], if there is a single anomalous log in the window, SINGLELINELOGANOMALYDETECTIONCOMPARISON
the sequence is labeled as anomalous. We refer to this data as
SingleLogLine
BGL-seq, and we use it for the sequential evaluation. windowsize:10 BGL-sin HDFS-sin
2) Competing Methods: We compare ADLILog (for both, stride:1
Method F1 Prec. Recall F1 Prec. Recall
the single line and the sequential evaluation) with three
ADLILog 0.61 0.55 0.70 0.98 1.00 0.96
state-of-the-art deep learning-based methods; two super- DeepLog 0.21 0.12 0.82 0.35 0.62 0.24
vised (LogRobust and CNN) methods and one unsupervised LogRobust 0.37 0.63 0.26 0.89 1.00 0.79
CNN 0.56 0.47 0.68 0.88 1.00 0.78
(DeepLog) [7]. According to the log-based anomaly detection
survey by Chen et al. [7], these three methods show the
best detection performance on the two benchmark datasets.
learning rate 10−4 and values for β and β set to 0.9 and
We used the public implementations of the methods available 1 2
0.99. Also, we explored four different values for the batch
open-sourceinaGitHubrepository3.Thethreemethodswere
size{32,64,256,512}.Thebatchsizeof512showedthebest
evaluatedwiththesuggestedvaluesfortheirhyperparameters.
average results. The finetuning was performed for five epochs
Sincethethreemethodsrequirefixedinput,similartoChenet
with the same values for the optimizer. The experiments
al. [7], we use window size of 10 events to create fixed-size
were conducted on a machine using Ubuntu 18.04, with CPU
sequences and predict the next log (i.e., the stride is one).
Intel(R) i5-9600K, RAM 128 GB, and GPU RTX 2080.
For the sequential evaluation, alongside the three state-
of-the-art methods, we further considered four traditional
B. Experimental Results and Discussion
log-based anomaly detection methods: two supervised, i.e.,
Logistic Regression (LR) [27] and Decision Tree (DT) [9], The performance evaluation of the proposed approach is
and two unsupervised methods, i.e., Principle Component made using two independent experimental scenarios: single
Analysis [10] and LogCluster (LC) [4]. While deep learning- line and sequential log anomaly detection. For the single log
based methods are directly applicable for single log line [8], line anomaly detection methods comparison, we compared
we are not aware of related work that directly applies the ADLILog to the three state-of-the-art deep learning-based
four traditional methods on single log lines. Therefore, we approaches. TABLE IV presents the results. ADLILog sig-
do not use them in this evaluation type. To implement the nificantly outperforms the supervised methods on almost all
baselines we used logilizer4, an open-source library for log- evaluation metrics. In particular, ADLILog showed the best
basedanomalydetection.Wesetthehyperparametersofthese predictive performance in terms of recall on both datasets
four methods as recommended by He et al. [6]. (BGL-sin, and HDFS-sin). While DeepLog is being slightly
3) Performance Evaluation Metrics: Following related better on recall on BGL-sin, it has significantly worsened
work, we use three evaluation metrics (precision, recall and performance on precision. On the F 1 as a primary evalua-
F ) to estimate the detection performance of the compared tion metric, our method outperforms the supervised methods
1
methods [7]. Precision shows the fraction of the correctly re- between 5-24% and the unsupervised one by 40-63%. The
ported anomalies (Precision = #DetectedAnomalies). Recall improvements of ADLILog are predominantly due to the rich
#ReportedAnomalies
showsthecorrectlydetectedanomaliesthataretrueanomalies set of abnormal events from the many diverse log instructions
(Recall = #DetectedAnomalies). For anomaly detection in that help to discriminate the anomalous logs. ADLILog has a
AllAnomalies
logs, on one side, it is important not to miss anomalies significantpracticaladvantageincomparisontothecompeting
(missing an anomaly can lead to severe outages). On the supervisedapproachesbecausedoesnotrequirelabeledtarget-
other side, reporting many false positives overwhelms the system anomalies. Therefore, it can be directly applied to a
operators, leading to alarm fatigue [3], making the method’s target system while obtaining detection performance similar
practical usability questionable. Therefore, a natural trade- or even better than the supervised approaches (which will
off between precision and recall emerges. In this regard, we still require expensive manual labeling). Therefore, the good
consideredF (aharmonicmeanbetweenprecisionandrecall performance of ADLILog comes at a smaller practical cost.
1
F = 2×Precision×Recall) as the primary evaluation metric. Further, we noted that the predictive performances of all
1 Precision+Recall
4) ADLILog Experimental Setup: We have performed the the methods for the BGL-sin dataset are significantly lower
experiments using three different values for the model size compared to the HDFS-sin. For example, the F 1 score of
{16,64,256}. Using the model size of 16, we have obtained LogRobust from 0.89 on HDFS-sin falls to 0.37 on BGL-sin.
the best predictive performance. The max len parameter A potential explanation for this observation is that the logs
from BGL-sin originate from many different simultaneously
was set to 32 because this length covers the majority of
running tasks. Therefore, there is a large difference between
the log lengths. To prevent overfitting, we used the dropout
regularization technique with a probability rate of 0.05. In the local log neighbourhood (nearby logs) of the log subject
to analysis, a phenomenon in log analysis literature known as
the pretraining phase, we used Adam [28] optimizer with a
unstablesequences[11].InBGL-sinthereare26.94%newlog
3https://github.com/logpai/deep-loglizer events in the test data compared to the training data, which
4https://github.com/logpai/loglizer additionallydiversifiesthelocalneighbourhoodsoftheevents.
TABLEV of works [7], [11] also shows that the unstable sequences
SEQUENTIALLOGANOMALYDETECTIONCOMPARISON significantly affects the performance of log anomaly detection
methods. In contrast, ADLILog focuses on the discriminative
LogSequences
windowsize=10, BGL-seq HDFS-seq properties of the individual events, ignoring the sequential
(timewindow=6h) (blockids)
stride=1 featuresoftheanomalouspatterns.Theseexperimentalresults
Method F1 Prec. Recall F1 Prec. Recall show that in the case of lower repetitiveness in the log
ADLILog 0.86 0.84 0.88 0.93 0.92 0.94
sequences, leveraging solely the differences in the language
DeepLog 0.63 0.46 1.00 0.94 0.96 0.93
LC 0.57 0.42 0.87 0.80 0.87 0.74 used to describe normal and anomalous events can lead to
PCA 0.55 0.50 0.61 0.79 0.98 0.67 better anomaly detection performance.
LogRobust 0.83 0.71 1.00 0.96 0.93 0.98 The results on HDFS-seq show that the methods exploiting
CNN 0.82 0.69 1.00 0.97 0.94 0.99
sequential properties achieve better results. The key charac-
LR 0.71 0.95 0.57 0.98 0.95 0.99
DT 0.72 0.95 0.57 1.00 1.00 1.00 teristic of HDFS-seq is the high sequence regularity which
explains the good performance of the sequential methods. In
addition,notallanomaliesareloggedintoasinglelog.Forex-
Although some of the new events are normal, the methods ample,around70%oftheanomaliesinthetestdatasetcanbe
that exploit the local context miss-detect them as anomalous identified by shortened log sequences, however, the majority
(e.g., as seen by the drop in precision for DeepLog on BGL- of them have at least one anomalous logline. Since ADLILog
sin). DeepLog, as the state-of-the-art unsupervised method, does not model the sequential properties directly, it does not
leverages the local context (window size of 10 events) to learn the anomalous sequence properties (e.g., long sequences
detectanomaliesanditissignificantlyaffectedbytheunstable caused by delays where no anomalous events are recorded).
sequences,resultinginthelowestperformance.LogRobustand Nevertheless,itoutperformsthetraditionalunsupervisedbase-
CNNleveragesupervisedinformationabouttheevents,which line methods by simply relying on the language properties
helps to improve the performance. In contrast, the HDFS-sin of the logs. The supervised methods are showing stronger
datasetischaracterizedbyhighregularityinthesequencesdue performance in comparison to the unsupervised. However, in
to the data generation procedure. The repetitiveness of task comparisontoADLILog(whichdoesnotrequirelabeleddata),
operations (e.g., deletion, allocation), the smaller number of the supervised methods require expensive labeling. ADLILog
events and the low context change lead to higher regularity in has strong anomaly detection performance for the two log
the local contexts, which increases the detection performance. datasets with weak and strong sequential dependencies. No-
ADLILog is not affected by the local contexts differences tably, a significant practical improvement of ADLILog over
because it examines each log independently. the related methods is that it does not require labels from the
ADLILogdetectsanomaliesintosingleloglines.Tobeable target system to learn a model. Therefore, ADLILog achieves
to detect anomalies in log sequences, ADLILog aggregates competitive detection performance at a lower practical cost.
the predictions of the individual logs from the sequence.
If there is at least a single detected anomalous log, the 0.8
External Labels
whole sequence is detected as anomalous. For BGL-seq, we 0.7 Github Labels
aggregated ADLILog’s logline predictions into the fixed time
0.6
intervals (same to BGL-seq data generation procedure), while
0.5 erocS
for HDFS-seq, we aggregate the predictions based on the
0.4
block ids. This way, ADLILog additionally can be evaluated 1F
0.3
on sequential log anomaly detection.
0.2
TABLE V gives the results of the sequential log anomaly
0.1
evaluation. We discuss them for each dataset individually,
0.0
starting with BGL-seq. For BGL-seq, ADLILog achieves the 1% 5% 10% 20%
best performance among the deep-learning state-of-the-art
Fig.2. Quantitativeandqualitativeevaluationofthe”Abnormal”logs
and the traditional approaches on the F evaluation metric.
1
Considering precision, ADLILog outperforms the three unsu- The acceptance of AI-enabled methods in production set-
pervised (DeepLog, LC and PCA) and the two deep-learning tings depends strongly on the necessary effort for creating
supervisedmethods,whilebeingoutperformedbyDTandLR. training data. Therefore, we examine the quantitative and
In contrast, when considering recall, ADLILog outperforms qualitative properties of the training data ADLILog needs
all traditional approaches. Since the logs from the BGL-seq to learn a good model. Specifically for the quantitative prop-
dataset resemble a real-world behaviour of a supercomputer, erty,wevariedtheratioofnormalversusanomalouslogsinthe
the logs originate from many different independent tasks training data when finetuning. This experiment examines the