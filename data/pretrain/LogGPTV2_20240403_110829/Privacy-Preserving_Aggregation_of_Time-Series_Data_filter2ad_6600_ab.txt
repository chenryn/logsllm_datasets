mon statistic: summation. Each participant’s data xi
comes from Zp for some prime p. Deﬁne the aggregat-
ing function sum(x) :=
i=1 xi. Moreover, each par-
ticipant generates noise ri from the set of integers and
applies the randomization function χ(xi, ri) := xi + ri
mod p, i.e., a participant incorporates additive noise be-
fore encrypting her data.
n
Figure 3.1 gives a high-level overview of our con-
struction. In the remainder of the paper, we ﬁrst formal-
ize the privacy notions in Section 4. We then describe
the two building blocks in our solution in following two
sections: 1) Section 5 describes a cryptographic con-
struction to ensure that the aggregator learns nothing but
the noisy sum; 2) Section 6 describes how each partici-
pant should choose her noise distribution so that the dif-
ferential privacy of an individual participant is protected
even when a subset of the participants may be compro-
mised.
4 Formal Privacy Notions
We consider an untrusted aggregator who may have
arbitrary auxiliary information. For example, the aggre-
gator may collude with a set of corrupted partipants. The
corrupted participants can reveal their data and noise
values to the aggregator. Such information leaked from
corrupted participants can be considered as a form of
auxiliary information. Auxiliary information about par-
ticipants can also be obtained in other ways, such as
from public datasets on the web, or through personal
knowledge about a speciﬁc participant.
Our goal is to guarantee the privacy of each individ-
ual’s data against an untrusted aggregator, even when the
aggregator has arbitrary auxiliary information. At a high
level, our formal privacy notions consists of two proper-
ties:
• Aggregator oblivious. Suppose that an aggregator
has auxiliary information aux. With an appropri-
ate capability, the aggregator learns a noisy aggre-
gate statistics f((cid:98)x) at the end of a time period. We
Figure 1: Overview of our construction. In every time period, each participant adds noise ri to her value xi before encrypting it.
The aggregator uses the capability sk0 to decrypt a noisy sum, but learns nothing more. The noisy sum output by this distributed
mechanism ensures each participant’s differential privacy.
would like to guarantee that the aggregator learns
nothing other than what can be inferred from aux
and the revealed statistics f((cid:98)x). In addition, we re-
quire that a party without an appropriate aggregator
capability learns nothing.
• Distributed differential privacy. We require that
a user’s participation in the system leaks only neg-
ligible information about herself. In other words,
the aggregate statistic f((cid:98)x) revealed is roughly the
same whether or not a speciﬁc user participates in
the system. To achieve this goal, we adopt a privacy
model similar to the differential privacy notion ﬁrst
introduced by Dwork et al. [5, 7]. While traditional
differential privacy considers a trusted aggregator
who sees the participants’ data in the clear and is
trusted to add noise to the published statistics, in
our model, the participants need not trust the data
aggregator or other participants. The ﬁnal noise in
the revealed statistic is collected from each indi-
vidual participant. Our privacy guarantee is strong,
even when the aggregator may have arbitrary aux-
iliary information, or collude with a small subset of
corrupted participants.
Malicious participants can also perform a data pol-
lution attack where they lie about their values in an at-
tempt to sway the ﬁnal output. Although data pollution
attacks are outside the scope of the paper, we would like
to mention that one possible defense is for each partic-
ipant to use a non-interactive zero-knowledge proof to
prove that her encrypted data lies within a valid range,
e.g., {0, 1, . . . , ∆}. In this way, each participants’s in-
ﬂuence is bounded.
4.1 Aggregator Oblivious
For simplicity, we deﬁne the aggregator oblivious no-
tion of security only for the sum statistic. Intuitively, we
would like to capture the following security notions:
• The aggregator can learn only the noisy sum for
each time period, and nothing more. For example,
the aggregator cannot learn any partial information
from a proper subset of all participants’ ciphertexts.
• Without knowing the aggregator capability, one
learns nothing about the encrypted data, even if
several participants form a coalition against the re-
maining users.
• If the aggregator colludes with a subset of the par-
ticipants, or if a subset of the encrypted data has
been leaked,
then the aggregator can inevitably
learn the sum of the remaining participants. We
require that in this case, the aggregator learns no
additional information about the remaining partici-
pants’ data.
We describe the following Aggregator Oblivious
(AO) security game. We assume that each participant
incorporates additive noise (which we see later is to en-
sure privacy) to their data before encrypting them.
Setup. Challenger runs the Setup algorithm, and re-
turns the public parameters param to the adversary.
Queries. The adversary makes the following types of
queries adaptively. As described later, certain con-
straints must be met for these queries.
• Encrypt.
(i, t, x, r),
The
NoisyEnc(ski, t, x, r) to the adversary.
The adversary may specify
ciphertext.
and ask for
returns
ciphertext
challenger
the
the
datanoise12nparticipantsaggregatorc1:=E(sk1,x1+r1)c1:=E(sk2,x2+r2)cn:=E(skn,xn+rn)AggrDec(sk0,c1,c2,...,cn)Pni=1(xi+ri)...• Compromise. The adversary speciﬁes an in-
teger i ∈ {0, . . . , n}. If i = 0, the challenger
returns the aggregator capability sk0 to the ad-
versary. If i (cid:54)= 0, the challenger returns ski,
the secret key for the ith participant, to the ad-
versary.
• Challenge. This query can be made only
once throughout the game. The adversary
speciﬁes a set of participants U and a time t∗.
Any i ∈ U must not have been compromised
at the end of the game.
For each user
the adver-
sary chooses two plaintext-randomness pairs
(xi, ri), (x(cid:48)
i, r(cid:48)
i). The challenger ﬂips a ran-
dom bit b. If b = 0, the challenger computes
∀i ∈ U : NoisyEnc(ski, t, xi, ri), and returns
If b = 1,
the ciphertexts to the adversary.
the challenger computes and returns the ci-
phertexts ∀i ∈ U : NoisyEnc(ski, t, x(cid:48)
i, r(cid:48)
i)
instead.
U,
∈
i
Guess. The adversary outputs a guess of whether b is 0
or 1.
We say that the adversary wins the game if she cor-
rectly guesses b and the following condition holds. Let
K ⊆ [n] denote the set of compromised participants at
the end of the game (not including the aggregator). Let
Q ⊆ [n] denote the set of participants for whom an En-
crypt query has been made on time t∗ by the end of
the game. Let U ⊆ [n] denote the set of (uncompro-
mised) participants speciﬁed in the Challenge phase. If
U = K ∪ Q := [n]\(K ∪ Q), and the adversary has
compromised the aggregator capability, the following
condition must be met:(cid:88)
(1)
(cid:98)xi =
(cid:98)x(cid:48)
i.
(cid:88)
i∈U
i∈U
Deﬁnition 1 (Aggregator oblivious security). A PSA
scheme is aggregator oblivious,
if no probabilistic
polynomial-time adversary has more than negligible ad-
vantage in winning the above security game.
Explanation. Suppose that the adversary has compro-
mised the aggregator capability sk0. In addition, for ev-
ery participant i /∈ U, the adversary knows a ciphertext
ci for the time t∗ as well as the corresponding random-
ized plaintext(cid:98)xi. Such an adversary is able to use the
AggrDec function to learn the sum of all participants in
time period t∗. From this sum, the adversary is able to
infer the partial sum over the subset U. Note that the
the adversary may be able to learn a plaintext and ci-
phertext pair for i /∈ U in two ways. The adversary
can either make an Encrypt query for i /∈ U and time
t∗, or compromise the secret key of participant i so that
it is able to produce the ciphertexts on its own. There-
fore, when U = K ∪ Q and the aggregator capability
has been compromised, we require that apart from the
sum over the subset U, the adversary is unable to in-
fer additional information about the honest participants
in U. This means that the adversary in the above secu-
rity game is unable to distinguish which plaintext vector
i|i ∈ U} the challenger
U are equivalent with re-
(cid:98)xU := {(cid:98)xi|i ∈ U} or(cid:98)x(cid:48)
U := {(cid:98)x(cid:48)
encrypted, as long as(cid:98)xU and(cid:98)x(cid:48)
spect to summation.
On the other hand, under the following conditions,
the adversary learns nothing from the challenge cipher-
texts corresponding to the set U of participants. 1) The
adversary has not compromised the aggregator capabil-
ity; or 2) U (cid:54)= K ∪ Q, i.e., there exists at least one
i /∈ U for whom the adversary does not know a cipher-
text for time period t∗. Under these situations, for arbi-
U that the adversary submits
in the Challenge phase, the adversary is unable to dis-
tinguish which one the challenger encrypted.
trary choices of(cid:98)xU and(cid:98)x(cid:48)
Remark 2 (General statistic). The notion of aggregator
oblivious security may be extended to general statistics
other than sum. Extra care must be taken, however. If an
adversary has compromised the set K ⊆ [n] of partici-
pants, she is able to encrypt anything on behalf of these
participants. Therefore, she can plug in any plaintext
vector(cid:98)xK = {(cid:98)xi|i ∈ K} of her choice for the set K,
crypt the aggregate statistics conditioned on(cid:98)xK. Such
encrypt them, and then call the AggrDec function to de-
attacks are inherent in the problem deﬁnition and cannot
be avoided. The security deﬁnition must reﬂect the fact
that this is the best and only strategy for the adversary,
i.e., the adversary is unable to learn extra information
other than information gleaned from this attack. For the
sum statistic, this requirement boils down to Equation 1.
Basically, as long as the two challenge plaintexts are
equivalent with respect to sum, the adversary is unable
to distinguish which one the challenger encrypted. This
condition is more tricky to state for general queries. For
simplicity, this paper deﬁnes the aggregator oblivious
security game speciﬁcally for the sum statistic.
Encrypt-once security. Our construction makes one
additional assumption that each honest participant only
encrypts once in each time period. Formally, this condi-
tion is reﬂected in the game as follows.
Deﬁnition 2 (Encrypt-once security). We say that a PSA
scheme is aggregator oblivious in the “encrypt-once”
model, if no probabilistic polynomial-time adversary
has more than negligible advantage in the above security
game, and in addition, the following constraint holds:
∀i ∈ U, ∀(x, r) ∈ D × Ω: the tuple (i, t∗, x, r) must
not have appeared in any Encrypt query.
4.2 Distributed Differential Privacy
Previous differential privacy literature assumes that
all users send their data to the aggregator in the clear.
In this case, if the users wish to guarantee their privacy
against an untrusted aggregator, each participant must
add sufﬁcient noise to her value to ensure her differen-
tial privacy. As a result, the aggregate noisy statistic may
accumulate too much noise, and the resulting f((cid:98)x) may
have a huge error.
In contrast, we guarantee that the
aggregator learns only the noisy statistic, but not each
individual’s values.
In this way, each individual may
add less noise to her data. As long as the ﬁnal statistic
f((cid:98)x) has accumulated sufﬁcient randomness, each indi-
vidual’s privacy is guaranteed. We also consider the case
when a certain fraction of participants are compromised.
The compromised participants can collude with the data
aggregator and reveal their data or randomness to the ag-
gregator. In this case, we would like to ensure that the
remaining uncompromised participants’ randomness is
sufﬁcient to protect their privacy.
Recall that the aggregator evaluates a function f :
We referred to the above notion of privacy as Dis-
tributed Differential Privacy (DD-Privacy), to reﬂect the
fact that the noise in the the released statistic is collected
from all participants. We formalize this notion of dis-
tributed differential privacy below.
Dn → O on randomized data(cid:98)x ∈ Dn of n participants,
function χ : D×Ω → D on her data xi to produce(cid:98)xi :=
tion(cid:98)x =(cid:98)x(r) := (χ(x1, r1), χ(x2, r2), . . . , χ(xn, rn)),
i.e., the dependence of(cid:98)x on r is implicit.
which are generated in the following way. Each partici-
pant generates independent randomness ri ∈ Ω accord-
ing to some distribution, and apply some randomization
χ(xi, ri). Given x ∈ Dn and r ∈ Ωn, we use the nota-
Given a subset K of participants, we let rK := {ri :
i ∈ K} and K be the complement of K, i.e., K =
{1, 2, . . . , n} \ K.
We require that the following notion of distributed
differential privacy applies to every time period t ∈ N.
Deﬁnition 3 ((, δ)-DD-Privacy). Suppose  > 0, 0 ≤
δ < 1 and 0 < γ ≤ 1. We say that the data random-
ization procedure, given by the joint distribution r :=
(r1, . . . , rn) and the randomization function χ achieves
(, δ)-distributed differential privacy (DD-privacy) with
respect to the function f and under γ fraction of uncom-
promised participants if the following condition holds.
For any neighboring vectors x, y ∈ Dn, for any subset
S ⊆ O, and for any subset K of uncompromised partic-
ipants of size at least γn,
P r[f((cid:98)x) ∈ S|rK] ≤ exp() · P r[f((cid:98)y) ∈ S|rK] + δ.
(2)
In the above deﬁnition, two vectors x, y ∈ Dn are
said to be neighbors or neighboring vectors if they dif-
fer in exactly one coordinate. This corresponds to the
scenario when exactly one user changes her data.
When K is the set of compromised nodes, the above
deﬁnition requires that the remaining honest partici-
pants’ randomness be sufﬁcient to ensure differential
privacy. Therefore, the probability is conditioned on
the randomness rK from compromised participants. In
other words, the probability is taken over the random-
ness rK from honest participants. The deﬁnition of DD-
privacy requires that for any set K of uncompromised
participants, as long as |K| ≥ γn, Equation 2 holds.
Strictly speaking, we achieve differential privacy
against polynomial-time adversaries, as our constructoin
relies on an encryption scheme that is secure against
polynomial-time adversaries. Computational differen-
tial privacy was ﬁrst introduced by Mironov et al. [13].
In fact, it is possible to deﬁne a computational version
of the above DD-privacy notion as well, and prove our
scheme secure under the computational differential pri-
vacy model. We leave the computational deﬁnition and
proofs to the expanded journal version.
5 Achieving Aggregator Oblivious Secu-
rity
In this section, we describe a cryptographic construc-
tion that allows us to achieve aggregator oblivious se-
curity. For simplicity, in this section, we assume that
each participant incorporates additive noise r to her data
x before encrypting it. To avoid writing the plaintext
and noise terms separately, we use the notation(cid:98)xi,t :=
subscripts and write(cid:98)xi or(cid:98)x instead.
xi,t +ri,t to denote participant i’s noisy plaintext in time
t. When the context is clear, we omit one or more of the
5.1
Intuition
One challenge we face when designing the mech-
anism is how to minimize the necessary communica-
tion between the participants and the data aggregator.
If one allows the participants and the aggregator to en-
gage in an interactive multiple-party protocol in every
time period, then standard Secure Multi-Party Computa-
tion [10] techniques can be used to ensure that the data
aggregator learns only the sum. However, the require-
ment that all participants must be simultaneously online
and interact with each other perodically renders many
applications impractical, especially large-scale cloud ap-
plications.
In contrast, in our solution, after a trusted
setup phase between all participants and the data aggre-
gator, no further interaction is required except for up-
loading a noisy encryption to the data aggregator in each
time period. The trusted setup may be performed by a
trusted third-party or through a standard Secure Multi-
Party protocol.
We now explain the intuition behind our construc-
tion. Suppose that for every time period t ∈ N, the par-
ticipants and the aggregator had means of determining
n + 1 random shares of 0. In other words, they generate
ρ0,t, ρ1,t, . . . , ρn,t ∈ Zp, such that
n(cid:88)
i=0
ρi,t = 0.
Speciﬁcally, ρ0,t is the aggregator’s capability for time
t, and participants 1 through n obtain ρ1,t through ρn,t
respectively. Then the following simple idea allows the
aggregator to decrypt the sum of all participants for all
time periods, without learning each individual’s values.
NoisyEnc. To encrypt the value(cid:98)xi,t := xi,t + ri,t in
time period t, participant i simply computes the fol-
lowing ciphertext