backdoor attack, as the reference attack model. At a high level,
TrojanNN defines a specific pattern (e.g., watermark) as the trig-
ger and enforces the poisoned model to misclassify all the inputs
embedded with this trigger. As it optimizes both the trigger and
poisoned model, TrojanNN enhances other backdoor attacks (e.g.,
BadNet [21]) that employ fixed trigger patterns.
Specifically, the attack consists of three steps. (i) First, the trigger
pattern is partially defined in an ad hoc manner; that is, the water-
mark shape (e.g., square) and embedding position are pre-specified.
(ii) Then, the concrete pixel values of the trigger are optimized
to activate neurons rarely activated by benign inputs, in order to
minimize the impact on benign inputs. (iii) Finally, the model is
re-trained to enhance the effectiveness of the trigger pattern.
Note that within TrojanNN, the operations of trigger optimiza-
tion and model re-training are executed independently. It is thus
possible that after re-training, the neurons activated by the trigger
pattern may deviate from the originally selected neurons, resulting
in suboptimal trigger patterns and/or poisoned models. Also note
that TrojanNN works without access to the training data; yet, to
make fair comparison, in the following evaluation, TrojanNN also
uses the original training data to construct the backdoors.
5.1.2 Enhanced Attacks. We optimize TrojanNN within the IMC
framework. Compared with optimizing the trigger only [29], IMC
improves TrojanNN in terms of both attack effectiveness and eva-
siveness. Specifically, let 𝑟 denote the trigger. We initialize 𝑟 with
the trigger pre-defined by TrojanNN and optimize it using the co-
optimization procedure. To this end, we introduce a mask 𝑚 for the
given benign input 𝑥◦. For 𝑥◦’s 𝑖-th dimension (pixel), we define
𝑚[𝑖] = 1 − 𝑝 (𝑝 is the transparency setting) if 𝑖 is covered by the
watermark and 𝑚[𝑖] = 0 otherwise. Thus the perturbation opera-
tion is defined as 𝑥∗ = 𝜓(𝑥◦, 𝑟; 𝑚) = 𝑥◦ ⊙ (1 − 𝑚) + 𝑟 ⊙ 𝑚, where ⊙
denotes element-wise multiplication. We reformulate the backdoor
attack in Eqn (5) as follows:
E𝑥◦∈T [ℓ(𝜓(𝑥◦, 𝑟; 𝑚), 𝑡; 𝜃)] + 𝜆ℓf(𝑚) + 𝜈ℓs(𝜃)
(15)
where we define the fidelity loss in terms of 𝑚. Typically, ℓf(𝑚) is
defined as 𝑚’s 𝐿1 norm and ℓs(𝜃) is the accuracy drop on benign
cases similar to Eqn (9).
Algorithm 2 sketches the optimization procedure of Eqn (15). It
alternates between optimizing the trigger and mask (line 4) and
optimizing the poisoned model (line 5). Specifically, during the
min
𝜃,𝑟,𝑚
510152025Leverage Effect 00.10.20.30.40.5Relative Fidelity Loss  0trigger perturbation step, we apply the Adam optimizer [26]. Fur-
ther, instead of directly optimizing 𝑟 which is bounded by [0, 1],
we apply change-of-variable and optimize over a new variable
𝑤𝑟 ∈ (−∞,+∞), such that 𝑟 = (tanh(𝑤𝑟) + 1)/2 (the same trick
is also applied on 𝑚). Note that Algorithm 2 represents a general
optimization framework, which is adaptable to various settings.
For instance, one may specify all the non-zero elements of 𝑚 to
share the same transparency or optimize the transparency of each
element independently (details in § 5.2 and § 5.3). In the following,
we term the enhanced TrojanNN as TrojanNN∗.
Algorithm 2: TrojanNN∗ Attack
Input: initial trigger mask – 𝑚◦; benign model – 𝜃◦; target class –
𝑡; hyper-parameters – 𝜆, 𝜈
Output: trigger mask – 𝑚; trigger pattern – 𝑟, poisoned model – 𝜃∗
// initialization
1 𝜃 (0), 𝑘 ← 𝜃◦, 0;
2 𝑚(0), 𝑟 (0) ← 𝑚0, TrojanNN(𝑚0);
// optimization
3 while not converged yet do
4
// trigger perturbation
𝑟 (𝑘+1), 𝑚(𝑘+1) ←
arg min𝑟,𝑚 E𝑥◦∈T [ℓ(𝜓 (𝑥◦, 𝑟; 𝑚), 𝑡; 𝜃 (𝑘))] + 𝜆ℓf (𝑚);
// model perturbation
𝜃 (𝑘+1) ← arg min𝜃 E𝑥◦∈T [ℓ(𝜓 (𝑥◦, 𝑟 (𝑘); 𝑚(𝑘)), 𝑡; 𝜃)] + 𝜈ℓs(𝜃);
𝑘 ← 𝑘 + 1;
5
6
7 return (𝑚(𝑘), 𝑟 (𝑘), 𝜃 (𝑘));
5.2 Optimization against Human Vision
We first show that TrojanNN∗ is optimizable in terms of its evasive-
ness with respect to human vision. The evasiveness is quantified by
the size and transparency (or opacity) of trigger patterns. Without
loss of generality, we use square-shaped triggers. The trigger size
is measured by the ratio of its width over the image width.
Figure 10: Attack efficacy of TrojanNN∗ as a function of trigger size
and transparency.
Figure 10 illustrates TrojanNN∗’s attack efficacy (average mis-
classification confidence of trigger-embedded inputs) under varying
evasiveness constraints. Observe that the efficacy increases sharply
as a function of the trigger size or opacity. Interestingly, the trigger
size and opacity also demonstrate strong mutual reinforcement
effects: (i) leverage - for fixed attack efficacy, by a slight increase
in opacity (or size), it significantly reduces the size (or opacity); (ii)
amplification - for fixed opacity (or size), by slightly increasing size
(or opacity), it greatly boosts the attack efficacy.
To further validate the leverage effect, we compare the triggers
generated by TrojanNN and TrojanNN∗. Figure 11 shows sample
Figure 11: Sample triggers generated by TrojanNN (a), TrojanNN∗ op-
timizing opacity (b) and optimizing size (c).
triggers given by TrojanNN and TrojanNN∗ under fixed attack effi-
cacy (with 𝜅 = 0.95). It is observed that compared with TrojanNN,
TrojanNN∗ significantly increases the trigger transparency (under
fixed size) or minimizes the trigger size (under fixed opacity).
To further validate the amplification effect, we measure the at-
tack success rate (ASR) of TrojanNN and TrojanNN∗ under varying
evasiveness constraints (with 𝜅 = 0.95), with results shown in Fig-
ure 12 and 13. It is noticed that across all the datasets, TrojanNN∗
outperforms TrojanNN by a large margin under given trigger size
and opacity. For instance, in the case of GTSRB (Figure 12 (d)), with
trigger size and transparency fixed as 0.4 and 0.7, TrojanNN∗ out-
performs TrojanNN by 0.39 in terms of ASR; in the case of CIFAR10
(Figure 13 (a)), with trigger size and transparency fixed as 0.3 and
0.2, the ASRs of TrojanNN∗ and TrojanNN differ by 0.36.
We can thus conclude that leveraging the co-optimization frame-
work, TrojanNN∗ is optimizable with respect to human detection
without affecting its attack effectiveness.
5.3 Optimization against Detection Methods
In this set of experiments, we demonstrate that TrojanNN∗ is also
optimizable in terms of its evasiveness with respect to multiple
automated detection methods.
5.3.1 Backdoor Detection. The existing backdoor detection meth-
ods can be roughly classified in two categories based on their appli-
cation stages and detection targets. The first class is applied at the
model inspection stage and aims to detect suspicious models and
potential backdoors [9, 31, 51]; the other class is applied at inference
time and aims to detect trigger-embedded inputs [8, 10, 15, 18]. In
our evaluation, we use NeuralCleanse [51] and STRIP [18] as the
representative methods of the two categories. In Appendix C, we
also evaluate TrojanNN and TrojanNN∗ against ABS [31], another
state-of-the-art backdoor detector.
NeuralCleanse – For a given DNN, NeuralCleanse searches for
potential triggers in every class. Intuitively, if a class is embedded
with a backdoor, the minimum perturbation (measured by its 𝐿1-
norm) necessary to change all the inputs in this class to the target
class is abnormally smaller than other classes. Empirically, after
running the trigger search algorithm over 1,600 randomly sampled
inputs for 10 epochs, a class with its minimum perturbation nor-
malized by median absolute deviation exceeding 2.0 is considered
to contain a potential backdoor with 95% confidence.
STRIP – For a given input, STRIP mixes it up with a benign input
using equal weights, feeds the mixture to the target model, and
computes the entropy of the prediction vector (i.e., self-entropy).
Intuitively, if the input is embedded with a trigger, the mixture is
still dominated by the trigger and tends to be misclassified to the
(b) GTSRB(a) CIFAR10Trigger TransparencyTrigger SizeMisclassification ConfidenceTrigger TransparencyTrigger SizeTrojanNNTrojanNN* (opt. w.r.t. opacity)TrojanNN* (opt. w.r.t. size)ISICGTSRBFigure 12: ASR of TrojanNN and TrojanNN∗ as functions of trigger size.
Figure 13: ASR of TrojanNN and TrojanNN∗ as functions of trigger transparency.
target class, resulting in relatively low self-entropy; otherwise, the
self-entropy tends to be higher. To reduce variance, for a given input,
we average its self-entropy with respect to 8 randomly sampled
benign inputs. We set the positive threshold as 0.05 and measure
STRIP’s effectiveness using F-1 score.
5.3.2 Attack Optimization. We optimize TrojanNN∗ in terms of
its evasiveness with respect to both NeuralCleanse and STRIP.
Both detectors aim to detect anomaly under certain metrics, which
we integrate into the loss terms in Algorithm 2.
Specifically, NeuralCleanse searches for potential trigger with
minimum 𝐿1-norm, which is related to the mask 𝑚. We thus instan-
tiate the fidelity loss ℓf(𝑚) as 𝑚’s 𝐿1-norm and optimize it during
the trigger perturbation step. To normalize ℓf(𝑚) to an appropri-
ate scale, we set the hyper-parameter 𝜆 as the number of pixels
covered by the trigger. Meanwhile, STRIP mixes adversarial and
benign inputs and computes the self-entropy of the mixtures, which
highly depends on the model’s behaviors. We thus instantiate the
2 ; 𝜃))], in which we
specificity loss ℓs(𝜃) as E𝑥,𝑥′∈R [−𝐻(𝑓 ( 𝑥∗
randomly mix up an adversarial input 𝑥∗ (via perturbing a benign
input 𝑥) and another benign input 𝑥′ and maximize the self-entropy
of their mixture.
2 + 𝑥′
5.3.3 Detection Evasiveness. We apply the above two detectors
to detect TrojanNN and TrojanNN∗, with results summarized in Fig-
ure 14. We have the following observations. First, the two detectors
are fairly effective against TrojanNN. In comparison, TrojanNN∗
demonstrates much higher evasiveness. For instance, in the case of
GTSRB (Figure 14 (b)), with trigger size fixed as 0.4, the anomaly
measures of TrojanNN∗ and TrojanNN by NeuralCleanse differ by
over 2, while the F-1 scores on TrojanNN∗ and TrojanNN by STRIP
differ by more than 0.3. We thus conclude that TrojanNN∗ is opti-
mizable in terms of evasiveness with respect to multiple detection
methods simultaneously.
5.4 Potential Countermeasures
Now we discuss potential mitigation against IMC-optimized attacks
and their technical challenges. It is shown above that using detectors
against adversarial inputs or poisoned models independently is
often insufficient to defend against IMC-optimized attacks, due to
the mutual reinforcement effects. One possible solution is to build
ensemble detectors that integrate individual ones and detect IMC-
optimized attacks based on both input and model anomaly.
To assess the feasibility of this idea, we build an ensemble detec-
tor against TrojanNN∗ via integrating NeuralCleanse and STRIP.
Specifically, we perform the following detection procedure: (i) ap-
plying NeuralCleanse to identify the potential trigger, (ii) for a
given input, attaching the potential trigger to a benign input, (iii)
mixing this benign input up with the given input under varying
mixture weights, (iv) measuring the self-entropy of these mixtures,
and (v) using the standard deviation of the self-entropy values to
distinguish benign and trigger-embedded inputs.
Intuitively, if the given input is trigger-embedded, the mixture
combines two trigger-embedded inputs and is thus dominated by
one of the two triggers, regardless of the mixture weight, resulting
in a low deviation of self-entropy. In comparison, if the given input
is benign, the mixture is dominated by the trigger only if the weight
is one-sided, resulting in a high deviation of self-entropy.
We compare the performance of the basic and ensemble STRIP
against TrojanNN∗ (the detection against TrojanNN is deferred to
Appendix C). As shown in Figure 15, the ensemble detector per-
forms slightly better across all the cases, implying the effectiveness
of the ensemble approach. However, the improvement is marginal
(less than 0.2), especially in the case of small-sized triggers. This
may be explained by the inherent challenges of defending against
IMC-optimized attacks: due to the mutual reinforcement effects,
TrojanNN∗ attains high attack efficacy with minimal input and
model distortion; it thus requires to carefully account for such
effects in order to design effective countermeasures.
Attack Success RateTrigger Size1.00.80.60.40.20.10.20.30.40.50.60.70.80.10.20.30.40.50.60.70.80.050.10.150.20.250.30.050.10.150.20.250.30.0(a) CIFAR10(b) ImageNet(c) ISIC(d) GTSRBTrojanNN   ( α = 0.0 )TrojanNN   ( α = 0.7 )TrojanNN* ( α = 0.0 )TrojanNN* ( α = 0.7 )1.00.80.60.40.2Trigger Transparency(a) CIFAR10(b) ImageNet(c) ISIC(d) GTSRB0.00.20.40.60.80.00.20.40.60.80.00.20.40.60.80.00.20.40.60.80.0Attack Success RateTrojanNN   ( size = 0.3 )TrojanNN   ( size = 0.4 )TrojanNN* ( size = 0.3 )TrojanNN* ( size = 0.4 )TrojanNN   ( size = 0.12 )TrojanNN   ( size = 0.21 )TrojanNN* ( size = 0.12 )TrojanNN* ( size = 0.21 )TrojanNN   ( size = 0.12 )TrojanNN   ( size = 0.21 )TrojanNN* ( size = 0.12 )TrojanNN* ( size = 0.21 )TrojanNN   ( size = 0.3 )TrojanNN   ( size = 0.4 )TrojanNN* ( size = 0.3 )TrojanNN* ( size = 0.4 )Figure 14: Detection of TrojanNN and TrojanNN∗ by NeuralCleanse and STRIP on CIFAR10 and GTSRB.
the training stage [50], (ii) identifying suspicious models during
model inspection [9, 31, 51], and (iii) detecting trigger-embedded
inputs at inference time [8, 10, 15, 18].
Despite the intensive research on adversarial inputs and poisoned
models in parallel, there is still a lack of understanding about their
inherent connections. This work bridges this gap by studying the
two attack vectors within a unified framework and providing a
holistic view of the vulnerabilities of DNNs deployed in practice.
7 CONCLUSION
This work represents a solid step towards understanding adver-
sarial inputs and poisoned models in a unified manner. We show
both empirically and analytically that (i) there exist intriguing mu-
tual reinforcement effects between the two attack vectors, (ii) the
adversary is able to exploit such effects to optimize attacks with
respect to multiple metrics, and (iii) it requires to carefully account
for such effects in designing effective countermeasures against the
optimized attacks. We believe our findings shed light on the holistic
vulnerabilities of DNNs deployed in realistic settings.
This work also opens a few avenues for further investigation.
First, besides the targeted, white-box attacks considered in this
paper, it is interesting to study the connections between the two