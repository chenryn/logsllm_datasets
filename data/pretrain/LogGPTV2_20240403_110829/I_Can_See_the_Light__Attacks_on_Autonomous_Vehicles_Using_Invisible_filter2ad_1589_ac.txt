see the IR light, the ICSL Attack is not detected by the human driver.
However, we need to mention that in practice, it is possible for the
passengers or the drivers in other vehicles (have different filed of
views) to see the suspicious drone. In addition, the noise generated
by drones is around 65dB, which may be heard by the human driver.
Therefore, the attacker should carefully select the attack scenarios
(i.e., night scenario with limited vehicles on the road) and use a
better drone with lower noise to remain stealthy.
4.2 Ruin In-Car User Experience
In this experiment, we show how to create frequent system alerts
to ruin the In-Car user experience, which is similar to the attack
scenario in Figure 5 (c).
Experiment Setup. During this experiment, we assume the at-
tacker can either drive in front of the target AV or utilizes drones
to attack the AV. Specifically, as shown in Figure 8 (a), the attacker
utilizes IR light to blind the Triple Front Camera (two main forward
cameras and one narrow forward camera) of Tesla. In addition, the
attacker also can utilize drones to blind the side camera of Tesla.
For example, the left side camera is blinded in Figure 8 (c).
Experiment Result. As we can see from Figure 8 (b), Tesla gen-
erates system alerts to inform the drive that the front camera is
blocked or blinded. In this scenario, since the IR light is invisible, the
human driver will get confused and annoyed about system alerts.
In 8 (d), by attacking the left side forward camera, Tesla triggers
the system alert again. In this figure, we also show an example of
the view from the blinded camera. However, we need to mention
that the human driver cannot access the forward camera data while
the vehicle is moving. Therefore, although the camera detects the
invisible light, it is still difficult for the human driver to be aware
of the presence of the ICSL Attack.
Analysis. In this experiment, we found that the system alerts will
only be triggered when the entire triple front camera is blinded by
the IR light, which introduces a severe security risk. This experiment
also answers why the system alert is not triggered when the attacker
is trying to create fake objects in the previous section 4.1.2. Since
Tesla fuses the data gathered from multiple cameras to understand
(b)Infrared Light LEDs(c)DJI Robot MasterDJI Robot Master S1Infrared Light LEDAttackerâ€™s VehicleBlind these two camerasTarget  cameraTesla Triple Forward CameraTesla(a)S1 Control PanelInfrared Light LEDsTarget Camera(c)(b)(d)(a)DJI Robot Master S1Infrared Light LEDAttackerâ€™s VehicleS1 Control PanelSession 6C: Audio Systems and Autonomous Driving CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea1935Figure 9: The experiment setup.
Figure 10: SLAM trajectory: no ICSL Attack.
the driving scenario, these cameras are complement to each other.
Therefore, even if several cameras are interfered by IR lights, the
driving performance and in-car user experience are not affected.
5 ICSL ATTACKS ON SLAM SYSTEM
To show that it is possible to introduce SLAM errors to the AV, we
utilize an enterprise-level autonomous driving platform to analyze
the ICSL Attack. Similar to section 4, all the photos are taken by
iPhone 12 pro and human eyes cannot see any IR lights during the
experiment. In this section, we first introduce the experiment setup
and experiment results. Then, we analyze the security insights. At
last, we introduce a SLAM attack model to show how to manipulate
the SLAM trajectory.
5.1 Experiment Setup
We conduct the experiment in an indoor parking lot, which is shown
in Figure 9 (a). There are 74 850ğ‘›ğ‘š IR light LEDs deployed in the
parking lot. Each corner is deployed 10 IR light LEDs and each floor
is deployed 11 IR light LEDs. These LEDs are blinking according
to their own schedules, which is shown in Figure 9 (b). As shown
in Figure 9 (c), we use an enterprise-level autonomous vehicles -
2018 Lincoln MKZ 2.0H (fully loaded) to evaluate ICSL Attack. The
front of the autonomous vehicle is equipped with one triple forward
camera, four Ultrasonic Sensors, one LiDAR, and one Mid-Range
Radar. The rear of the vehicle is equipped with four Ultrasonic
Sensors and one Mid-Range Radar. The roof of the autonomous
vehicle is equipped with three LiDARs. The AV utilizes the most
commonly used ORB-SLAM2 architecture [5, 51] to perform the
SLAM process. Due to the safety concern, the steering speed of the
vehicle is set to 4ğ‘šğ‘â„ and the straight-line speed is set to 5ğ‘šğ‘â„.
The frame rate is set to 30ğ‘“ ğ‘ğ‘ . During the visual odometry (VO)
process, the autonomous vehicle extracts FAST key points [61]
Figure 11: SLAM trajectory: under ICSL Attack.
and uses an ORB descriptor [62] to find the matched key points
every 6 frames. Then, according to the matched key points, the
motions (rotation and translation matrices) of the camera (AV) are
estimated. To improve the estimation accuracy, the AV will search
the local map in its dataset in order to find the accurate position
of the matched key points. At last, bundle adjustment (BA) will be
used to better estimate the location of the autonomous vehicle.
During the experiment, we have the access to the private park-
ing lot. However, in practice, the deployment process of IR light
LEDs will increase the possibilities of being detected by others.
Specifically, although the IR light cannot be perceived by human
eyes and the LEDs are small, it is still possible for the employees in
the parking lot (i.e., managers, sweepers etc.) to see the suspicious
behavior of the attacker or find small LED devices on the floor or on
the wall. Therefore, to remain stealthy, the attacker should carefully
select the locations of IR LEDs and conduct the deployment when
no one is in the parking lot.
5.2 Introduce SLAM Errors
Experiment Result. The experiment results are shown in Figure
10 and 11. When the autonomous vehicle is not under ICSL Attack,
the calculated SLAM trajectory is smooth and close to the actual
driving trajectory. In this scenario, even if the indoor parking lot
does not have the GPS signal, the AV can navigate itself and find
the entrance and exit. On the contrary, when the AV is under ICSL
Attack in Figure 11, the calculated SLAM trajectory cannot provide
any useful information. Moreover, as we can see from the red arrows
1, 2 ,3 ,4 and 6 in this figure, the VO process in SLAM is not working
properly and the trajectory is discontinuous while arrow 5 shows
that the vehicle is driving to the sky.
Analysis. In this section, we analyze why the calculated SLAM
trajectory under ICSL Attack cannot provide any useful information
22m85 mInfrared Light LEDs11111010101111The deployment of IR Light (ICSL Attack)No ICSL AttackIndoor Parking LotRear of the VehicleRadarSonarSonarFront of the VehicleRadarLiDARSonarSonar3 LiDARs3 Cameras(a)(b)(c)Driving RouteDriving Route22m85 mDestinationStarting PointDriving TrajectorySLAM Trajectory (No Attack)DestinationStarting Point22m85 mDestinationStarting Point123456Driving TrajectorySLAM Trajectory (ICSL Attack)Session 6C: Audio Systems and Autonomous Driving CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea1936to navigate the AV. As shown in Figure 12, we plot the matching
results during the VO process in SLAM between two consecutive
frames. Since the IR light provides significantly different pixels
from the ambient light, the AV considers the IR light LEDs as the
key points and finds all the matches between two consecutive
frames. Moreover, since all the IR light LEDs are the same and the
background of the indoor parking lot cannot provide any useful
information to distinguish between different IR lights, it is highly
possible for the AV to mismatch those IR lights and gets the wrong
trajectory.
Specifically, to calculate the trajectory of the AV, we first define
the rotation matrix and translation matrix of the AVâ€™s camera as
ğ‘… and ğ‘¡, respectively. Since the rotation and translation matrices
belong to the lie group SE(3), we have:
ğ‘†ğ¸(3) = {ğ‘‡ =
âˆˆ R4Ã—4|ğ‘… âˆˆ ğ‘†ğ‘‚(3), ğ‘¡ âˆˆ R3}
(1)
Assume the homogeneous coordinate of the detected key point
ğ‘ƒ is ğ‘ƒ = (ğ‘¥, ğ‘¦, ğ‘§, 1)ğ‘‡ and the corresponding projected pointâ€™s coor-
dinate on the camera is ğ‘„ = (ğ‘¢, ğ‘£, 1)ğ‘‡ . Then, we can represent the
relationship between the key point and the projected point as:
(cid:20)ğ‘…
(cid:21)
ğ‘¡
1
0
ğ‘¢
ğ‘£
1
 â‰ˆ (ğ¾ğ‘’ğœ‰âˆ§
ğ‘ 
ğ‘¥
ğ‘¦
ğ‘§
1
)1:3
(2)
where ğ‘  is the depth information of the detected key point and ğ¾ is
the camera matrix that will be provided in the specifications of the
camera. ğœ‰âˆ§ represents the skew-symmetric matrix of lie algebra for
the homogeneous matrix ğ‘‡ (ğ‘‡ âˆˆ ğ‘†ğ¸(3)) in equation 1. The left side
of this equation is a three dimensional vector while the right side
of this equation is a 4Ã— 1 vector. Therefore, in order to calculate the
pose ğœ‰ of AVâ€™s camera, we only need to match the first three rows
of the right side to the left side in order to make the above equation
hold, which is denoted as (1 : 3). Due to the noise introduced
during the environment perception process, the following equation
is leveraged to minimize calculated pose error:
ğ¾ğ‘’ğœ‰âˆ§
ğ‘›
(3)
ğœ‰ğ‘œğ‘ğ‘¡ = arg min
ğœ‰
ğ‘ƒğ‘–||2
2
1
2
ğ‘–=1
||ğ‘„ğ‘– âˆ’ 1
ğ‘ ğ‘–
ğ¾ğ‘’ğœ‰âˆ§
Therefore, we can find the optimized camera pose ğœ‰ by a lineariza-
ğ‘ƒğ‘–) as ğ‘’(ğœ‰). Then, the
tion process. We first denote (ğ‘„ğ‘– âˆ’ 1
ğ‘ ğ‘–
corresponding linearization form can be represented as ğ‘’(ğœ‰ + Î”ğœ‰) =
ğ‘’(ğœ‰) + ğ½ Î”ğœ‰, where ğ½ is the Jacobian matrix of ğ‘’(ğœ‰) and can be cal-
culated by left multiply the perturbation ğ›¿: ğ½ = ğœ•ğ‘’
ğœ•ğ›¿ğœ‰ . Formally, the
camera matrix ğ¾ can be represented as:
ğ‘ğ‘¥
ğ‘ğ‘¦
1
0
ğ‘“ğ‘¦
0
ğ¾ =
(4)
where ğ‘“ğ‘¥ and ğ‘“ğ‘¦ are the focal lengths while ğ‘ğ‘¥ and ğ‘ğ‘¦ are considered
as the principal point of the camera. Finally, the Jacobian matrix ğ½
can be calculated as:
ğ‘“ğ‘¥
0
0
0
ğ‘“ğ‘¦
ğ‘§
âˆ’ ğ‘“ğ‘¥ ğ‘¥
ğ‘§2
âˆ’ ğ‘“ğ‘¦ ğ‘¦
ğ‘§2
âˆ’ ğ‘“ğ‘¥ ğ‘¥ ğ‘¦
ğ‘§2
âˆ’ğ‘“ğ‘¦ âˆ’ ğ‘“ğ‘¦ ğ‘¦2
ğ‘§2
ğ‘“ğ‘¥ + ğ‘“ğ‘¥ ğ‘¥2
ğ‘§2
ğ‘“ğ‘¦ğ‘¥ ğ‘¦
ğ‘§2
âˆ’ ğ‘“ğ‘¥ ğ‘¦
ğ‘§
ğ‘“ğ‘¦ğ‘¥
ğ‘§
(5)
As we can observe in this equation 5, since ğ‘“ğ‘¥ and ğ‘“ğ‘¦ are fixed ac-
cording to the camera matrix, the three-dimensional rotation matrix
ğ‘… is affected by the first three columns while the three-dimensional
 ğ‘“ğ‘¥
ğ‘§
0
ğ½ = âˆ’
Figure 12: An example of key point matching errors between
two consecutive frames.
translation matrix ğ‘¡ is affected by the column from four to six.
Therefore, when the ICSL Attack introduces mismatches between
two consecutive frames, the optimization direction is wrong, which
introduces a wrong ğ‘… and ğ‘¡. As a result, the calculated trajectory is
significantly affected by the ICSL Attack.
SLAM attack model. According to the above analysis, in this sec-
tion, we show how to manipulate the SLAM trajectory. Specifically,
as shown in Figure 12, IR light LEDs introduce mismatches between
two consecutive frames. Therefore, by changing the blinking IR
light LEDs, the mismatches between two consecutive frames will
also be changed, which will result in the change of the correspond-
ing optimization directions ğ½. Formally, we define the mismatched
key point (with a different IR LED on) and its corresponding Ja-
cobian matrices are ğ‘ƒâ€² = (ğ‘¥â€², ğ‘¦â€², ğ‘§â€², 1)ğ‘‡ and ğ½ â€², respectively. Then,
in order to change the orientation of the AV from the original
ğ‘… = (ğ‘Ÿğ‘¥, ğ‘Ÿğ‘¦, ğ‘Ÿğ‘§)ğ‘‡ to ğ‘…â€² = (ğ‘Ÿâ€²
ğ‘§)ğ‘‡ , the attacker should make
sure that the first three columns of ğ½ â€² satisfy:
ğ‘§âˆ’ğ‘Ÿğ‘§)
ğ‘¦, ğ‘Ÿâ€²
,
ğ‘¥, ğ‘Ÿâ€²
â‰¥ ğ‘“ğ‘¥ ğ‘¥+ğ‘“ğ‘¦ ğ‘¦+ğ‘§2(ğ‘Ÿâ€²
ğ‘§ ğ‘“ğ‘¥
ğ‘¥âˆ’ğ‘Ÿğ‘¥) ,
ğ‘“ğ‘¥âˆ’ğ‘§(ğ‘Ÿâ€²