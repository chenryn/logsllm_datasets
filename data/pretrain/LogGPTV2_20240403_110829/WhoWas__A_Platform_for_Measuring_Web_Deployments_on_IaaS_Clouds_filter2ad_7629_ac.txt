responses, as only text content is collected by WhoWas.
Table 6 summarizes the clustering for both datasets. First, it
shows the number of responsive IP addresses. Then, the distinct
simhashes for the content obtained from those IP addresses, which
is larger than the number of IP addresses because the content col-
lected from an IP address varies over time. Next, it shows the
number of ﬁrst-level and second-level clusters, which shows that
some ﬁrst-level clusters are split into multiple second-level clus-
ters. The last row shows the ﬁnal number of clusters, after merging
and cleaning has been applied to the second-level clusters.
7. ETHICAL CONSIDERATIONS
The unsolicited nature of active probing can make some targets
consider it offensive and can potentially place a burden on both the
individual tenants and the cloud provider. We take this issue seri-
ously in our study and seek a careful balance between the amount of
interaction required and the merits of the obtained data. We guide
our active probing on the recommendations of prior work [18].
Our scanner and fetcher are located at the premises of one of our
institutions and their use was made in cooperation with the insti-
tution’s IT staff. To minimize the impact on the EC2 and Azure
networks we limit the probing to an average of once per day and
constrain our probing to only 3 ports (80/tcp, 443/tcp, 22/tcp).
Our HTTP fetcher includes a brief note in the User-Agent
string stating that the request is being made for a research project.
The note includes an email address to contact in case of ques-
tions/concerns. This allows tenants to request opting out of future
queries. We also exclude IPs that we have observed to disallow
robots requesting the top-level content, as per the robots exclusion
protocol [34]. In the course of operating WhoWas we received 84
emails from instance operators. Upon further explanation, most op-
erators were happy to hear about our research project. Only 16 re-
quested to have IPs excluded from future scans, which we promptly
did. In total 67 IPs were excluded in the course of the experiment.
In general, the reasons offered for requesting IPs be excluded were
either: (1) the servers are not designed to be public, but with care-
less conﬁgurations, they are accidentally discovered by WhoWas;
and (2) since IaaS providers charge tenants based on trafﬁc, the
trafﬁc generated by WhoWas, though it is very small, is still un-
welcome for some tenants.
While technically any data we obtain is public, in the sense that
we are simply fetching webpages from publicly advertised IP ad-
dresses, it is clear that some cloud tenants inadvertently made ac-
cessible what should not be. So as to respect potential privacy is-
sues, we will not make our data set publicly available on the In-
105EC2
Azure
95.9 text/html
2.1 text/plain
text/html
text/plain
application/json 1.0 application/xml
application/xml 0.3 application/json
text/xml
other
97.8
1.0
0.7
0.2
0.3 application/xhtml+xml 0.1
0.4 other
0.2
Table 5: Top 5 content-types in EC2 and Azure and the percentage
of webpages of each content-type in all collected webpages.
Responsive IPs
Unique simhashes
Top-level clusters
2nd-level clusters
Final clusters
EC2
1,359,888
1,767,072
236,227
256,335
243,164
Azure
154,753
210,418
30,581
39,183
31,728
Table 6: The number of responsive IPs and unique simhashes of
their content, as well as the number of clusters output by the differ-
ent steps of the clustering.
ternet. Interested researchers should contact the authors if they de-
sire access to the data set, and we will evaluate the privacy issues
of each request and use appropriate privacy safeguards (e.g., use
anonymization tools [35]). The code and other tools that WhoWas
consists of will however be released publicly, and in future work
we may investigate making a public interface to our data set that
provides appropriate privacy protections (e.g., only providing ag-
gregate statistics).
8. ANALYSES USING WHOWAS
In this section we report on various analyses performed using
WhoWas with the data sets gathered as discussed in §6. We start by
using WhoWas to study usage and its change over time in EC2 and
Azure. Then we use WhoWas to shed light on malicious activity in
clouds, and ﬁnally we use WhoWas to describe the distribution of
software in the web ecosystem in clouds.
8.1 Cloud Usage Dynamics
IP responsiveness and availability. Table 7 summarizes the usage
statistics across all measurement rounds. The average number of
responsive IPs is almost an order of magnitude larger in EC2 than
in Azure, which illustrates their difference in popularity. The aver-
age IP address space usage is almost identical on both services and
relatively low (23.7–23.9%), so both have ample room for growth.
EC2 has on average 758 K available IPs, 7.6 times more than Azure
with 99 K, which indicates that EC2 hosts a higher proportion of
services running on ports other than 80/tcp and 443/tcp, which are
less likely to be web services.
Figure 8 shows the IP responsiveness and availability over the
two month measurement period. The variation of responsive and
available IP addresses in both services over the two months is low,
with a 0.3–0.5% standard deviation. On absolute numbers, EC2
grew more over this period, adding 36,414 responsive IPs by the
end of the period, compared to 8,270 for Azure. However, the rela-
tive growth of Azure is larger, with an increase in 7.3% of respon-
sive and 7.7% of available IPs, compared to 3.3% and 4.9% for
EC2. Figure 8 shows several noticeable dips in the curves. In EC2,
these dips correspond to the dates Oct. 4, Nov. 8, Nov. 30, Dec. 14,
and Dec. 28, all in 2013. Interestingly, they are all Fridays or Sat-
urdays. By appropriate queries to the WhoWas database, we found
that 3,198, 2,767, 1,449, 983, and 1,327 clusters become unavail-
able on EC2 on these dates, respectively, and never return again. In
Azure, the dips happened on Nov. 29 and Dec. 07, 2013, also Fri-
day and Saturday. On these two days, 360 and 385 clusters become
unavailable and never return again. A total of 15,295 IPs in EC2
and 775 IPs in Azure are associated with these clusters. These dips
may be caused by web service maintenance, webpage update, web
service migration (to other providers or to new added networks that
we did not probe), or other reasons. It is possible that the servers
that left the cloud were intended for other purposes instead of web
services. Unfortunately, we could not ﬁgure out the exact reason
for these departures.
Cluster number and size. The average number of clusters for a
round of probing in EC2 is 185,701 and in Azure 27,048. Over the
measurement period, the number of clusters increased by 3.3% in
EC2 and by 6.2% in Azure.
We deﬁne the average size of a cluster to be the average number
of IPs in a cluster per round of measurement. The average cluster
size distribution for EC2 is: 78.8% have average size of one IP
address; 20.8% between 2 and 20; 0.28% between 21 and 50; and
only 0.07% greater than 50. For Azure the distribution is quite
similar, with a slightly larger number of small clusters: 86.2% (1),
13.6% (2–20), 0.1% (21–50), and 0.1% (> 50). The numbers show
that the vast majority of clusters in both clouds use just a small
number of IPs.
The amount of overlap across the two clouds we study is small.
We ﬁnd 980 clusters are using both EC2 and Azure, and 85% (834)
of them use the same average number of IPs in each cloud. These
latter clusters all use a small number of IPs on average (at most 5
IPs). We also observe 110 clusters that use more IPs in EC2 than
they use in Azure, on average. One of these clusters, a VPN service
(cluster 3 in Table 15), uses over 2,000 IPs more in EC2 than it uses
in Azure. We did not see any cluster migrations between EC2 and
Azure during our measurement period.
The clustering heuristics we use can only provide estimates, and
so we may be under- or over-counting cluster size in some cases.
Web services may also be making use of other providers not in-
cluded in our study, which would lead us to underestimate their
overall resource usage. Indeed some larger services may use other
hosting solutions such as content delivery networks, which are not
included in our study. Despite these potential sources of error, the
data still supports the conclusion that the majority of the services
hosted in clouds are small. We delve more deeply into some of the
largest clusters at the end of this section.
IP status churn. A key feature of WhoWas is our ability to es-
timate churn, meaning the quantity of change in status of IP ad-
dresses. The overall churn rate, meaning the number of IPs that
change status in terms of responsiveness, availability, or which
cluster they are associated to divided by the total number of IPs, is
3.0% for each of EC2 and Azure. Investigating individual types of
status change, we have that for EC2 the average change in respon-
siveness was 2.5%, the average change in availability was 1.0%,
and an average of 0.1% of IPs changed clusters between measure-
ment rounds.4 The same values for Azure are 2.2%, 1.7%, and
0.3%. A time series of churn rates is shown in Figure 9.
If one instead looks at percentages relative to all unique IP ad-
dresses that are responsive on either round T − 1 or T , then overall
churn rate becomes 11.9% in EC2 and 12.2% in Azure on average.
The responsiveness, availability, and cluster change rates become
4Note that they do not sum to 3.0% since unavailable IPs include
unresponsive IPs, as well as responsive but unavailable IPs.
106EC2
Azure
# Responsive IPs # Available IPs # Clusters # Responsive IPs # Available IPs # Clusters
25,903
1,061,834 (22.6%) 718,693 (15.3%)
1,145,448 (24.4%) 773,832 (16.5%)
27,786
27,048
1,113,599 (23.7%) 758,144 (16.1%)
605
18,733 ( 0.4%) 13,232 ( 0.3%)
36,414 ( 3.3%) 35,442 ( 4.9%)
1,604
179,870 111,851 (22.6%) 94,348 (19.0%)
188,274 120,605 (24.3%) 102,953 (20.8%)
185,701 118,290 (23.9%) 99,720 (20.1%)
2,640 ( 0.5%)
7,299 ( 7.7%)
2,169 ( 0.4%)
8,270 ( 7.3%)
1,939
5,984
Minimum
Maximum
Average
Std. dev.
Overall growth
Table 7: Summary of overall usage of EC2 and Azure address spaces. Percentages are the fraction of the total number of IP addresses probed.
Figure 8: Change over time in responsive IPs, available IPs and clusters (from top to bottom) in (left) EC2 and (right) Azure. X-axis is the
round of scanning and note that the Y-axes have gaps in the range covered.
Figure 9: Change in status of responsive IPs and available IPs as a
fraction of all IP addresses in (left) EC2 and (right) Azure.
Figure 10: Change in availability of clusters as a percentage of the
total number of clusters seen throughout the measurement period
in (left) EC2 and (right) Azure.
10.1%, 4.0%, and 0.3% for EC2 and 8.7%, 6.8%, and 1.1% for
Azure.
Even considering that these are just estimates (sources of error
including incorrect clustering or transitory network failures), the
per-round churn in these clouds appears quite small.
Cluster availability over time. A cluster is available in a given
round if at least one of the IPs in the cluster is available in that
round. Figure 10 plots for each round the fraction of clusters that
changed from available to unavailable or vice versa, compared to
the previous round, over the total number of unique clusters ob-
served throughout the measurement period. On average across all
rounds, the average percentage of cluster status changes is 4.6% on
EC2 and 7.3% on Azure.
Cluster size over time. We now characterize the observed ways in
which a cluster’s size (the number of IPs associated to the cluster)
changes over time. Table 11 shows the results of our analysis, in
particular the top ﬁve size change patterns observed. Here a pattern
of 0 means no change; 0,1,0 indicates an increasing trend; 0,-1,0
a decreasing trend; and so on. So about half of clusters are quite
stable in terms of size. We now describe how we arrived at these
patterns, and then discuss the results in further detail.
For each cluster, we ﬁrst generate a vector D of the number of in-
stances it used at each time point. So a vector of (10, 3, 20) means
that in time period one the cluster had 10 IPs associated to it, 3 in
the next time period, etc. The dimension of the vector is the number
of rounds of scanning. We reduce the dimensionality of the result-
ing vectors in order to more accurately compare and understand
general usage patterns.
To do so we use piecewise aggregate approximation (PAA) [36],
a method for reducing the dimensionality of time-series data. We
divide the vector into frames and use the median value of a frame
to represent the frame. Since the time interval of our data is not
always constant, we could not simply use an equal number of data
points in each frame. We use a 7-day window to generate frames.
We use the 7-day time window because it can cover at least 3 data
points and can catch more changes in IP usage. Each frame may
contain different numbers of data points. For example, frame one
may contain 3 data points from the ﬁrst 7 days of scanning while
frame two contains 4 data points from the next 7 days.
We therefore compute from D a new vector D(cid:48) of dimension 13
for EC2 (since measurement lasts for 93 days) and 9 for Azure (62
days) whose values are the median of the values in each window.
We use median so as to be robust in the face of outliers.
Using D(cid:48), we then deﬁne a new vector D(cid:48)(cid:48), called the ten-
dency vector, as shown in Algorithm 1. For example, and us-
ing smaller vector dimension for brevity, if D(cid:48) = (1, 2, 3, 1, 1, 1)
180K182K184K186K188K190K 1 6 11 16 21 26 31 36 41 46 51#ClusterScan round720K735K750K765K780K#IP1.08M1.10M1.12M1.14M#IP25.5K26.0K26.5K27.0K27.5K28.0K 1 6 11 16 21 26 31 36 41 46#ClusterScan round93K96K99K102K#IP111K114K117K120K123K#IP 0.5 1 1.5 2 2.5 3 3.5 6 11 16 21 26 31 36 41 46 51% of all IPsScan roundResponsive IPAvailable IP 1 1.5 2 2.5 3 3.5 4 4.5 5 5 9 13 17 21 25 29 33 37 41 45% of all IPsScan round 3 4 5 6 7 6 11 16 21 26 31 36 41 46 51% of all clustersScan round 5 6 7 8 9 10 5 9 13 17 21 25 29 33 37 41 45% of all clustersScan roundCluster107pattern description # EC2 clusters # Azure clusters
121,297 (49.9%) 17,092 (53.9%)
0
0,1,0
36,465 (15.0%)
4,406 (13.9%)
3,952 (12.5%)
33,400 (13.7%)
0,-1,0
1,189 ( 3.8%)
12,598 ( 5.2%)
0,1,0,-1,0
0,-1,1,0
9,857 ( 4.1%)
1,376 ( 4.3%)
Table 11: Top ﬁve size-change patterns observed in EC2 and Azure.
Percentages in parentheses are with respect to total number of clus-
ters.
if D(cid:48)[i + 1] > D(cid:48)[i] then
else if D(cid:48)[i + 1] = D(cid:48)[i] then
Algorithm 1 Algorithm for computing tendency vector given D(cid:48)
1: for 1 ≤ i ≤ |D(cid:48)| − 1 do