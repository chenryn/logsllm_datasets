together Google resolvers are responsible for most of the queries
(see Figure 6), but they are not the most aggressive individually.
Recurrent queries: New queries from clients (łrecurrent queriesž)
make the problem worse. We run another experiment using RIPE
Atlas, but with probes sending new queries every 10 minutes (łRe-
currentž column in Table 3). Details are in Appendix B, but fresh
queries from clients recapitulate the initial influx of queries for
recursive resolvers that do not have a negative cache for the cycle.
We find a total of 1423 vulnerable resolvers, from 192 ASes (Table 4),
far more than with the New Domain experiment.
.
403
Figure 7: New domain: IQR and queries for A records of
ns.sub.cachetest.net
4.2 Broader Clients and Recursives
Experiments with RIPE Atlas see a specific set of locations and
recursive resolvers. To reproduce this experiment with a broader
set of clients and recursives, we carry out a similar experiment with
a sinkholed domain.
Botnets are compromised end-user computers that are controlled
by the botmaster through a command-and-control (C&C) server at
some DNS name. A sinkholed domain is a C&C server that has been
taken over by the authorities from the botmaster. Since end-user
computers cannot be de-infected, the sinkhole domain continues
to receive traffic. Typically, such traffic is discarded, but we were
given approval to use this traffic to evaluate cyclic domains.
Our sinkholed domain (bad-domain.nl) has two subdomains un-
der it ({a,b}.bad-domain.nl) that are still frequently queried, even
 1 10 100 1000 10000 5 10 15 20 25 30 35 40 45 50 1000 10000 100000 1x106 1x107 1x108ms (log)Queries (log)Resolver (sorted by queries)TsuNAME
IMC ’21, November 2–4, 2021, Virtual Event, USA
We leave this configuration active for 4 h, four times longer than
record’s TTL.
Traffic Growth: After adding the cyclic dependency, we see that,
around 11:00 UTC, the authoritative servers experience surge in
traffic, peaking at 1.2M queries, 120× the original query rate of 10k
queries per ten minutes. The new average query rate is 2581 queries
per second (q/s), up from 10.8 q/s, a 239× increase. This experiment
confirms large query amplification (239×) due to TsuNAME with a
population independent from RIPE Atlas.
In addition to growth in query rate, we see more unique resolvers
in Cyclic dependent phase. In Figure 8b, we see an average of 5k
unique resolver IPs in the Pre and Delegated Phases, and ∼9k at
11:00 UTC, almost twice as many. Prior work has seen similar use
of new recursives during DDoS attacks (Appendix F in [34, 35]). We
show here a similar diversity is discovered from cyclic dependencies
as clients or intermediate recursives try others when initial queries
timeout or fail. Analyzing the entire phase, we see that there were
33.9k active resolvers, from 29.2k in the Delegated phase (Table 6).
TTL influence phase: Next we evaluate the role of this TTL value,
by reducing the NS record TTL from 1 h to 1 s while keeping the
cyclic dependency in place (Table 6). TTL largely disables caching
in resolvers (most resolvers respect non-zero TTLs [34]).
A smaller TTL further increases in query volume, from an aver-
age of 2.5kq/s with a 1 h TTL to to 5.6kq/ with a 1 s TTL, as shown
in Table 6. Relative to the 10q/s baseline in the Delegated phase,
this is 526× the traffic. We speculate that this short TTL causes
more rapid use of new resolvers, making the problem even worse.
Delegation removed phase: Finally, we stop the experiment at
20:18 UTC, terminating our authoritative servers and reverting the
NS records back to those from the Pre phase. After this, all clients
send their queries to the original servers, which we do not monitor.
We cover its details in Appendix C.
4.2.1 Resolvers amplification factor: This experiment allows us
to estimate the amplification factor for each resolver: how many
queries does it send during Cyclic dependent phase compared to
Delegated phase. In total, we see ∼ 22k resolvers querying for A
records during both the Pre and Cyclic dependent phases, and ∼ 14k
for AAAA records.
Figure 9 shows the CDF of both A and AAAA traffic growth. We
see that most resolvers indeed send more queries during the Cyclic
dependent and TTL influence phases (comparing the median value
of AAAA records, a 1 s TTL results in 100× amplification, while
1 h TTL is only about 4×). For both A and AAAA records, roughly
20% of resolvers have at least a 100x growth (x>100) during Cyclic
dependent phase and a 40% during TTL influence phase. Finally,
while the amplification factor varies by 3 orders of magnitude, even
a few resolvers with large amplification (more than 10) can stress
authoritative servers.
4.2.2
Identifying problematic resolvers. Finally, we look for prob-
lematic resolvers in this dataÐthe resolvers that sent too many
queries. Given we cannot control user query frequency, we are un-
able to simply use the IQR of received queries like with RIPE Atlas
(ğ4.1). Instead, we use the traffic growth observed by the matching
resolvers.
We start first by singling out resolvers with a normalized am-
plification factor of at least 100 ś an arbitrarily chosen number.
(a) Queries
(b) Unique Resolvers
Figure 8: Sinkhole experiment timeseries (10min bin)
after more than 8 years since it has been sinkholed. We can con-
figure these domains with a cycle to see how the botnet and the
recursives it uses react. We then compare the results to our Atlas
experiments. (We make no changes to the bots, nor cause any harm
to them. The only traffic they send is their ongoing DNS queries.)
Our experiment is divided into multiple phases, as shown in
Table 5: the Pre, Delegated, Cyclic dependent, TTL influence, and the
Delegation removed phases.
Pre phase: Before (and after) our experiment the sinkhole answers
DNS queries with łlocalhostž with a 1 h TTL, causing bots to talk
to themselves when searching for C&C. With this typical sinkhole
operation the bots do no harm to themselves or others.
Delegated phase: We next change the NS records of the sinkholed
domain to our experiment’s authoritative servers. We can now
observe bot queries to determine the baseline query traffic and
number of bots. Our servers continue to answer with the localhost
address, as in the Pre phase.
We run in Delegated for over 8 hours to ensure any cached DNS
records learn about this delegation. We see 344k queries from 29k
unique resolver IP addresses during this entire phase (Table 6). We
see 10k queries from ∼5k resolvers every 10 minutes (Figure 8),
defining our baseline query rate.
Cyclic dependent phase: We next introduce a cyclic dependency
between our two domains by deploying cyclic NS records (auth.su
b2.essedarius.net ↔ auth.sub2.verfwinkel.net), at 08:48 UTC. This configu-
ration reproduces the setup that took place during .nz event (ğ3).
404
 0 0.5 1 1.5 200:0002:0004:0006:0008:0010:0012:0014:0016:0018:0020:0022:00 Delegated Cyclic1h Cyclic1s Del. Removed child removedQueries (M)Time(Nov.18,2020)Parent ZoneChild Zone 0 5000 10000 15000 2000000:0002:0004:0006:0008:0010:0012:0014:0016:0018:0020:0022:00 Delegated Cyclic1h Cyclic1s Del. removed Child removedResolversTime(Nov.18,2020)Parent ZoneChild ZoneIMC ’21, November 2–4, 2021, Virtual Event, USA
G. C. M. Moura et al.
Phase
Pre
Delegated
Cyclic1h
Cyclic1s
A
127.0.0.1
ś
ś
ś
Del. removed
Child removed
127.0.0.1
127.0.0.1
::1
ś
ś
ś
::1
::1
Parent Zone (bad-domain.nl)
AAAA
NS
ś
ns-414.awsdns-51.com
ns-414.awsdns-51.com
ns-414.awsdns-51.com
ś
ś
TTL
3600
3600
3600
3600
3600
3600
A
ś
127.0.0.1
ś
ś
ś
ś
ś
::1
ś
ś
ś
ś
Child Zone ( a.bad-domain.nl)
AAAA
NS
ś
auth.sub2.essedarius.net
auth.sub2.essedarius.net
auth.sub2.essedarius.net
ś
TTL
ś
3600
3600
1
1
ś
Table 5: Sinkhole experiment: zone file for a.bad-domain.nl, at parent and child delegation
Queries
Queries/s
Resolvers
∩ Del. A
∩ Del.AAAA
ASes
Duration
Delegated
344222
10.8
29280
ś
ś
2490
8h48min
Cyclic1h
37173567
2581.4
33943
22541
14125
2620
4h
Cyclic1s Del. removed
27386057
45054434
1444.4
5688.6
18920
33679
22239
18566
15696
14411
1228
2785
2h12min
5h16min
Table 6: Phase classification and characteristics
Figure 10: Resolvers with at least 100x traffic growth (A
queries)
Authoritative
Ripe Atlas
IPs
not querying $PID
querying $PID
> 1 $PID
== 1 $PID
Matching Resolver IPs
Problematic Resolvers
11931
4642
7289
1558
5731
IPs
Private
Public
7317
2924
4393
1256
574
4
intersection (∩)
Table 7: Singling out looping resolvers
Figure 9: CDF resolvers by amplification factor
Out of the 22451 active resolvers during both the Pre and Cyclic
dependent phases, 2653 and 2076 resolvers sent at least 100x more
A and AAAA records respectively (Figure 9).
Figure 10 shows the matching resolvers and the total queries
sent for A queries (we show the AAAA queries in Appendix A) We
see that both the most aggressive and the most resolvers are from
Google: only Google sent more than 20k queries per resolver during
Cyclic dependent phase. However, we also see in Table 4 that 2652
resolvers from 127 ASes were also vulnerable to TsuNAME.
4.3 Longer cycles and CNAME cycles
Most of our experiment focuses on cycles of two NS records, but
there can be cycles of more NS records (A → B → C → A) and
cycles using CNAMEs. We carried out experiments on these cases.
We found multiple-hop NS cycles further increase the amplification
factor (possibly linearly), but CNAME cycles do not. Due to space
constraints, details are in Appendix D.
4.4 Finding Problematic Recursives
Events (ğ3) and experiments (ğ4.1 and ğ4.2) show the problem of
TsuNAME, but they treat the recursives as an opaque box. We next
open that box up to find problematic recursives.
To identify problematic resolvers we want to find one-hop clientsÐ
cases where a user queries a recursive resolvers that queries our
authoritative servers. These clients allow us to identify the am-
plification from a single user query, and to demonstrate looping
recursives.
We walk authoritative queries back to clients by examining
queries from thousands of RIPE Atlas probes through their recur-
sive resolvers (Figure 4). We ask each RIPE Atlas probe to query our
new domain. (These queries are not affected by caching or other
users). We see 7,317 unique resolver IPs across 9,724 probes (Ta-
ble 7). Of these, 4,393 resolvers use routable IP addresses, and the
rest are in private networks [45], presumably behind a NAT.
At the authoritative servers, we see queries arrive from 11,931
unique IP addresses (Table 7). Some of these sources are shared by
405
10−210−1100101102103104105Amplification Factor0.00.20.40.60.81.0CDF Resolvers1h TTLAAAAA1s TTLAAAAA 0 10000 20000 30000 40000 50000 1 2 4 8 16 32 64Queries Cy1hQueries NormalGoogleRestTsuNAME
IMC ’21, November 2–4, 2021, Virtual Event, USA
Resolver Queries Median ∆t Duration
r1
r2
r3
r4
3.17h
5.13h
5.10h
5.10h
Table 8: Confirmed looping resolvers
34351
2783
833
775
901 ms
6095 ms
60821 ms
61680 ms
multiple clients. To avoid inflating our query counts we identify
and discard shared recursive resolvers. We can distinguish unique
from shared resolvers since queries include the resolvers ID (each is
for $PID.sub.verfwinkel.net, where $PID is the Atlas probe’s ID).
We find 5,731 non-shared recursives, each serving a single Atlas
probe (== 1 $PID).
We can now find one-hop clients (client to recursive to authori-
tative, without forwarders or multi-level recursives). We identify
one-hop clients as Atlas VPs where their local resolver’s IP address
matches an IP address seen in traffic to our authoritative serversÐ
the cells highlighted in Table 7. This process may miss recursive
resolvers that serve DNS traffic on multiple network interfaces. We
intersect these lists to identify 1,256 matching IP addresses.
We compare these recursives against our list of 574 problematic
resolvers (from controlled experiments ğ4.1). We find that 4 one-hop
clients use problematic resolvers. Table 8 lists these anonymized
recursive resolvers and shows how many queries and their query
interarrival rate from our prior experiment. We see that the most
prolific, r 1, queried every second for more than 3 hours, while the
others queried every 6 or 60 s.
Resolver query history: We next examine query sequence of these
problematic recursives to understand when they start looping. We
begin with r 1 from AS553 (BelWue, Germany), which sent 36k
queries. Table 9 shows its query history. First, at the Atlas side, we
see that this Atlas probe is configured with 3 different resolvers
(IP1śIP3, omitted for privacy), and it sends one query per resolver.
No other queries are issued from Atlas after that.
We see that these resolvers produce different results at the au-
thoritative side. IP1 sends only 9 queries to our servers, and IP3 sent
1 query only. But IP2, on the other hand, sends 36,075 queries! After
queries #5 and #6, this resolver repeats these queries every 900 ms
for more than 3 h (Table 8). Even without new client queries, this