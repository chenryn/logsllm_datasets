### Optimized Text

Google's resolvers are responsible for the majority of DNS queries (see Figure 6), although individually, they are not the most aggressive. The problem is exacerbated by recurrent queries from clients. In a separate experiment using RIPE Atlas, we configured probes to send new queries every 10 minutes (labeled as "Recurrent" in Table 3). The details are provided in Appendix B, but essentially, fresh queries from clients replicate the initial surge of queries for recursive resolvers that lack a negative cache for the cycle. This experiment identified a total of 1,423 vulnerable resolvers across 192 ASes (Table 4), significantly more than in the New Domain experiment.

**Figure 7: New domain: IQR and queries for A records of ns.sub.cachetest.net**

### 4.2 Broader Clients and Recursive Resolvers

Experiments with RIPE Atlas provide insights into a specific set of locations and recursive resolvers. To expand this study, we conducted a similar experiment with a sinkholed domain.

A sinkholed domain is a command-and-control (C&C) server that has been taken over by authorities from the botmaster. Since end-user computers cannot be de-infected, the sinkholed domain continues to receive traffic, which is typically discarded. However, we received approval to use this traffic to evaluate cyclic domains.

Our sinkholed domain (bad-domain.nl) has two subdomains ({a,b}.bad-domain.nl) that are frequently queried, even after being sinkholed for over eight years. We configured these domains with a cyclic dependency to observe the reaction of the botnet and its associated recursive resolvers. The results were then compared to our RIPE Atlas experiments. (No changes were made to the bots, and no harm was caused; the only traffic sent was their ongoing DNS queries.)

The experiment was divided into several phases, as shown in Table 5: Pre, Delegated, Cyclic dependent, TTL influence, and Delegation removed.

- **Pre phase**: Before and after the experiment, the sinkhole answers DNS queries with "localhost" and a 1-hour TTL, causing bots to communicate with themselves.
- **Delegated phase**: We changed the NS records of the sinkholed domain to our experimental authoritative servers, allowing us to observe baseline query traffic and the number of bots.
- **Cyclic dependent phase**: At 08:48 UTC, we introduced a cyclic dependency between our two domains, replicating the setup from the .nz event (Section 3).

**Figure 8: Sinkhole experiment timeseries (10-minute bin)**

### Traffic Growth

After introducing the cyclic dependency, we observed a significant surge in traffic at around 11:00 UTC. The authoritative servers experienced a peak of 1.2 million queries, 120 times the original rate of 10,000 queries per ten minutes. The new average query rate was 2,581 queries per second (q/s), up from 10.8 q/s, representing a 239-fold increase. This experiment confirmed large query amplification (239x) due to TsuNAME, independent of the RIPE Atlas population.

In addition to the increased query rate, we observed a higher number of unique resolvers during the Cyclic dependent phase. Figure 8b shows an average of 5,000 unique resolver IPs in the Pre and Delegated phases, rising to approximately 9,000 at 11:00 UTC, nearly twice as many. Prior research has noted similar behavior during DDoS attacks (Appendix F in [34, 35]). Here, we show that cyclic dependencies lead to a similar diversity as clients or intermediate recursives try other resolvers when initial queries time out or fail. Over the entire phase, there were 33,900 active resolvers, up from 29,200 in the Delegated phase (Table 6).

### TTL Influence

Next, we evaluated the role of the TTL value by reducing the NS record TTL from 1 hour to 1 second while maintaining the cyclic dependency (Table 6). A smaller TTL further increased the query volume, from an average of 2,500 q/s with a 1-hour TTL to 5,600 q/s with a 1-second TTL. Relative to the 10 q/s baseline in the Delegated phase, this represents a 526-fold increase. We hypothesize that the short TTL causes more rapid use of new resolvers, exacerbating the problem.

### Delegation Removed Phase

Finally, we terminated the experiment at 20:18 UTC by stopping our authoritative servers and reverting the NS records to their Pre phase configuration. After this, all clients sent their queries to the original servers, which we did not monitor. Details are provided in Appendix C.

### 4.2.1 Resolver Amplification Factor

This experiment allowed us to estimate the amplification factor for each resolver: the number of queries sent during the Cyclic dependent phase compared to the Delegated phase. In total, we observed approximately 22,000 resolvers querying for A records and 14,000 for AAAA records during both the Pre and Cyclic dependent phases.

Figure 9 shows the cumulative distribution function (CDF) of both A and AAAA traffic growth. Most resolvers indeed sent more queries during the Cyclic dependent and TTL influence phases. For AAAA records, a 1-second TTL resulted in a 100-fold amplification, while a 1-hour TTL was about 4-fold. Approximately 20% of resolvers had at least a 100-fold growth (x > 100) during the Cyclic dependent phase, and 40% during the TTL influence phase. Even though the amplification factor varied by three orders of magnitude, a few resolvers with large amplification (more than 10) could still stress authoritative servers.

### 4.2.2 Identifying Problematic Resolvers

We identified problematic resolvers by focusing on those that sent an excessive number of queries. Given that we cannot control user query frequency, we used the traffic growth observed by matching resolvers instead of the interquartile range (IQR) method used in the RIPE Atlas experiment (Section 4.1).

We singled out resolvers with a normalized amplification factor of at least 100, an arbitrarily chosen threshold. Out of the 22,451 active resolvers during both the Pre and Cyclic dependent phases, 2,653 and 2,076 resolvers sent at least 100 times more A and AAAA records, respectively (Figure 9).

Figure 10 shows the matching resolvers and the total queries sent for A records (AAAA records are shown in Appendix A). Both the most aggressive and the most resolvers were from Google, with only Google sending more than 20,000 queries per resolver during the Cyclic dependent phase. However, Table 4 also indicates that 2,652 resolvers from 127 ASes were vulnerable to TsuNAME.

### 4.3 Longer Cycles and CNAME Cycles

Most of our experiments focused on cycles of two NS records, but longer NS cycles (A → B → C → A) and CNAME cycles can also occur. We found that multiple-hop NS cycles further increase the amplification factor (possibly linearly), but CNAME cycles do not. Due to space constraints, details are provided in Appendix D.

### 4.4 Finding Problematic Recursive Resolvers

Events (Section 3) and experiments (Sections 4.1 and 4.2) highlight the problem of TsuNAME, but treat recursive resolvers as an opaque box. We next opened this box to identify problematic recursive resolvers.

To find one-hop clients—cases where a user queries a recursive resolver that queries our authoritative servers—we examined queries from thousands of RIPE Atlas probes through their recursive resolvers (Figure 4). Each probe was asked to query our new domain, ensuring that the queries were not affected by caching or other users. We observed 7,317 unique resolver IPs across 9,724 probes (Table 7). Of these, 4,393 resolvers used routable IP addresses, and the rest were in private networks, presumably behind a NAT.

At the authoritative servers, we saw queries from 11,931 unique IP addresses (Table 7). Some of these sources were shared by multiple clients. To avoid inflating our query counts, we identified and discarded shared recursive resolvers. We distinguished unique from shared resolvers based on the resolver ID included in the queries (each for $PID.sub.verfwinkel.net, where $PID is the Atlas probe’s ID).

We found 5,731 non-shared recursives, each serving a single Atlas probe. We then identified one-hop clients as Atlas VPs where the local resolver’s IP address matched an IP address seen in traffic to our authoritative servers (highlighted in Table 7). This process may miss recursive resolvers that serve DNS traffic on multiple network interfaces. We intersected these lists to identify 1,256 matching IP addresses.

We compared these recursives against our list of 574 problematic resolvers (from controlled experiments in Section 4.1) and found that 4 one-hop clients used problematic resolvers. Table 8 lists these anonymized recursive resolvers and shows the number of queries and their query interarrival rates from our prior experiment. The most prolific, r1, queried every second for more than 3 hours, while the others queried every 6 or 60 seconds.

### Resolver Query History

We examined the query sequence of these problematic recursives to understand when they started looping. For example, r1 from AS553 (BelWue, Germany) sent 36,000 queries. Table 9 shows its query history. Initially, the Atlas probe was configured with three different resolvers (IP1–IP3, omitted for privacy), and it sent one query per resolver. No other queries were issued from Atlas after that.

At the authoritative side, IP1 sent only 9 queries, and IP3 sent just 1 query. However, IP2 sent 36,075 queries! After queries #5 and #6, this resolver repeated these queries every 900 milliseconds for more than 3 hours (Table 8). Even without new client queries, this resolver continued to loop, demonstrating the persistence of the issue.