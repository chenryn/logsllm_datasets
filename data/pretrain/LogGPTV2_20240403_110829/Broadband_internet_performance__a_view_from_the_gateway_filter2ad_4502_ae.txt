has more capacity than the uplink, so buffering on the ISP side is
lower. The home network (at least 10 Mbits/s) is also probably bet-
ter provisioned than the downlink, so there is minimal buffering in
the modem for downstream trafﬁc. The high variability in the la-
tency under load can be partly explained by the variety in service
plans; for instance, AT&T offers plans ranging from 768 Kbits/s to
6 Mbits/s for DSL and up to 18 Mbits/s for UVerse and from 128
Kbits/s to more than 1 Mbit/s for upstream. In contrast, Comcast
offers fewer service plans, which makes it easier to design a device
that works well for all service plans.
How does modem buffering affect latency under load? To study
the effects of modem buffers on latency under load, we conduct
tests on AT&T and Comcast modems using BISMark. We ran tests
on the best AT&T DSL (6 Mbits/s down; 512 Kbits/s up) and Com-
cast (12.5 Mbits/s down; 2 Mbits/s up) plans. We perform the fol-
Figure 13: Latency under load: the factor by which baseline latency
goes up when the upstream or the downstream is busy. The high
ratios translate to signiﬁcant real latencies, often in the order of
seconds. (SamKnows)
lowing experiment: we start ICMP ping (at the rate of 10 pkts/s
for Comcast and 2 pkts/s for AT&T as some modems were block-
ing higher rates) to the last mile hop. After 30 seconds, we ﬂood the
uplink (at 1 Mbits/s for AT&T and at 10 Mbits/s for Comcast us-
ing iperf UDP). After 60 seconds, we stop iperf, but let ping
continue for another 30 seconds. The ping measurements 30 sec-
onds on either side of the iperf test establishes baseline latency.
The Motorola and the 2Wire modems were brand new, while the
Westell modem is about 5 years old, and was in place at the home
where we conducted the experiment. We also saw the same Westell
modem in two other homes in the BISMark deployment.
Figure 14a shows the latency under load for the three modems.
In all cases, the latency increases dramatically at the start of the
ﬂooding and plateaus when the buffer is saturated. The delay expe-
rienced by packets at this stage indicates the size of the buffer, since
we know the uplink draining rate. Surprisingly, we see more than
an order of magnitude of difference between modems. The 2Wire
modem has the lowest worst case latency, of 800 ms. Motorola’s is
about 1600 ms, while the Westell has a worst case latency of more
than 10 seconds. Because modems are usually the same across ser-
vice plans, we expect that this problem may be even worse for users
with slower plans.
To model the effects of modem buffering, we emulated this setup
in Emulab [13] with a 2 end-host, 1-router graph. We conﬁgured
a token bucket ﬁlter using tc. We compute the buffer size to
be: 512 Kbits/s×max(latency of modem), which yields a size of
640 Kbytes for Westell, 100 Kbytes for Motorola, and 55 Kbytes
for 2Wire. This simple setup almost perfectly captures the la-
tency proﬁle that the actual modems exhibit. Figure 14b shows
the emulated latencies. Interestingly, we observed little difference
in throughput for the three buffer sizes. We also emulated other
buffer sizes. For a 512 Kbits/s uplink, we observed that the modem
buffers exceeding 20 KBytes do little for throughput, but cause a
linear increase in latency under load. Thus, the buffer sizes in all
three modems are too large for the uplink.
How does PowerBoost trafﬁc shaping affect latency under
load? To understand latency under load for cable users, we study
the Comcast users from BISMark. All of the modems we study
have buffers that induce less than one second of delay, but these
users see surprising latency under load proﬁles due to trafﬁc shap-
ing. Figures 15a and 15b show the latency under load for two Com-
cast users. The other two Comcast users (with the Scientiﬁc Atlanta
and the Motorola modems) had latency proﬁles similar to the user
with the Thomson modem, so we do not show them. The difference
in the two latency proﬁles is interesting; the D-LINK user sees a
jump in latency when the ﬂooding begins and about 8 seconds later,
another increase in latency. The Thomson user sees an initial in-
crease in latency when ﬂooding starts but then a decrease in latency
after about 20 seconds. The ﬁrst effect is consistent with buffering
and PowerBoost. Packets see lower latencies during PowerBoost
because, for a ﬁxed buffer, the latency is inversely proportional to
the draining rate. The increase in latency due to PowerBoost (from
200 ms to 700 ms) is proportional to the decrease in the draining
rate (from 7 Mbits/s to 2 Mbits/s, as shown in Figure 10b). The
decrease in latency for the Thomson user cannot be explained in
the same way. Figure 15 shows the average loss rates alongside
the latencies for the two users; interestingly for the user with the
Thomson modem, the loss rate is low for about 20 seconds after
the link is saturated, but there is a sharp hike in loss corresponding
0-10 ms10-20 ms20-30 ms30-40 ms40-50 ms50-60 msLatency Interval (ms)020406080100Percentage of usersAT&TQwestVerizonComcastCoxTimeWarnerCharterCablevisionAT&TComcastCoxTimeWarnerCharterQwestVerizonCablevision04080120160Latency under load / Baseline latencyDownloadUpload143(a) Empirical measurements of modem buffering. Different modems
have different buffer sizes, leading to wide disparities in observed
latencies when the upstream link is busy. (BISMark)
(a) Comcast user with D-LINK modem.
(b) Emulated modems with token bucket ﬁlters. We see similar la-
tency progression. Emulated buffer sizes have minimal effect on
throughput.
Figure 14: Buffering in AT&T modems. There is little beneﬁt to
the buffering seen in most modems.
to the drop in latency. This behavior may correspond to dynamic
buffer sizing, as discussed in Section 5.3.
Can data transfer be modiﬁed to improve latency under load?
We explore whether a user can modify their data transfer behav-
ior so that large “bulk” ﬂows and delay-sensitive ﬂows can co-exist
without interfering with one another. We compare the impact of a
50 MByte download on a G.711 VoIP call in three different condi-
tions: (1) not applying any trafﬁc control, (2) intermittent trafﬁc at
capacity on 10.8 seconds ON and 5.3 seconds OFF cycle, and (3)
shaping using the WonderShaper [36] approach. Figure 16 shows
the result of this experiment. In (1), the transfer takes 25.3 seconds;
however, just after the PowerBoost period, the VoIP call starts suf-
fering high latency and loss until the end of the transfer. In (2),
trafﬁc is sent in pulses, and the download takes 26.9 seconds. In
(3), trafﬁc is sent at just under the long term rate and the down-
load takes 32.2 seconds. Both (2) and (3) do not increase latency
signiﬁcantly, this is because they do not deplete the tokens at any
time, and therefore cause no queuing. In approach (2), the ON/OFF
periods can be conﬁgured depending on the token bucket parame-
ters,3 and the size of the ﬁle to be transferred. Both approaches
achieve similar long-term rates but yield signiﬁcant latency bene-
ﬁt. The drawback is that any approach that exploits this behavior
would need to know the shaping parameters.
Takeaway: Modem buffers are too large. Even the smallest
buffers we see induce nearly one-second latency under load for
AT&T and 300 ms for Comcast. Buffering is detrimental to both
interactivity and throughput. Modifying data transfer behavior us-
ing short bursts or tools like WonderShaper might help mitigate the
problem in the short term.
3If ρr is the rate we want to reserve for real-time applications, and
ρt the token rate, the condition to be satisﬁed is: (ρb + ρr − ρt) ×
τon ≤ τof f × (ρt − ρr), where ρb is the sending rate during the
pulse, and τon and τof f are the ON and the OFF times, respectively.
(b) Comcast user with RCA Thomson modem.
Figure 15: Possible effect of active buffer management: Loss rates
increase when the latency drops. (BISMark)
7. LESSONS LEARNED
We conclude with some high-level lessons and suggestions for
future research directions. One signiﬁcant takeaway for users, pol-
icymakers, ISPs, and researchers is that continual measurements,
directly from home network gateways are crucial for understanding
the details of home access network performance. Existing “speed
test” downloads and end-to-end latency measurements do not of-
ten reﬂect access network performance over an extended period of
time, and they neglect various confounding factors on the host and
within the home. Our ability to execute measurements directly,
both from a small set of gateways where we can control the network
conditions and measurements (BISMark) and a larger, more repre-
sentative set of gateways across the United States (SamKnows),
yields several lessons:
Lesson 1 (One Measurement Does Not Fit All) Different
ISPs
use different policies and trafﬁc shaping behaviors that make it
difﬁcult to compare measurements across ISPs.
There is no single number that characterizes performance, or even
throughput. Certain ISP practices such as PowerBoost can distort
benchmarking measurements; ISPs might even design their net-
works so that widely used performance tests yield good perfor-
mance. Developing a benchmarking suite for ISP performance that
users can understand (e.g., in terms of the applications they use) is
critical; the measurements we develop in this paper may be a good
starting point for that. Along these lines, more work is needed to
understand the performance of speciﬁc applications, such as how
video streaming performance compares across ISPs. The NetFlix
study on ISP streaming performance [30] is a good start, but more
such performance benchmarks are needed.
Lesson 2 (One ISP Does Not Fit All) There is no “best” ISP for
all users. Different users may prefer different ISPs depending on
their usage proﬁles and how those ISPs perform along performance
dimensions that matter to them.
Different ISPs may be “better” along different performance dimen-
0102030405060708090100110120Time in seconds101102103104RTT(ms)WestellMotorola2Wire0102030405060708090100110120Time in seconds101102103104RTT(ms)WestellMotorola2Wire0102030405060708090100110120Time in seconds101102103104RTT(ms)LatencyLoss rate0.00.20.40.60.81.00102030405060708090100110120Time in seconds101102103104RTT(ms)LatencyLoss rate0.00.20.40.60.81.0144(a) Throughput.
(b) Latency.
Figure 16: It is possible to maintain low latency by modifying data
transfer behavior. (BISMark)
sions, and the service plan that a user buys is only part of the pic-
ture. For example, we saw that, above a certain throughput, latency
is the dominant factor in determining Web page loading time. Sim-
ilarly, a gamer might be interested in low latency or jitter, while an
avid ﬁle swapper may be more interested in high throughput. An
imminent technical and usability challenge is to summarize access
network performance data so that users can make informed choices
about the service plans that are most appropriate for them (akin to a
“performance nutrition label” [2]). Our recent work proposes some
ﬁrst steps in this direction [34].
Lesson 3 (Home Network Equipment Matters) A user’s home
network infrastructure can signiﬁcantly affect performance.
Modems can introduce latency variations that are orders of magni-
tude more than the variations introduced by the ISP. Other effects
inside the home that we have not yet studied, such as the wireless
network, may also ultimately affect the user’s experience. More
research is needed to understand the characteristics of trafﬁc inside
the home and how it affects performance.
Acknowledgments
We thank the participants in the SamKnows and BISMark stud-
ies, and to Walter Johnston at the FCC for help and access to the
data from the SamKnows study. We are grateful to Dave Täht for
his continuing support of the BISMark platform. We also thank
Sam Burnett and Hyojoon Kim for constructive comments on paper
drafts. This project is supported by the the National Science Foun-
dation through awards CNS-1059350, CNS-0643974, a generous
Google Focus Grant, the European Community’s Seventh Frame-
work Programme (FP7/2007-2013) no. 258378 (FIGARO), and the
ANR project C’MON.
REFERENCES
[1] Grenouille. http://www.grenouille.com/.
[2] Does broadband need its own government nutrition label?
http://arstechnica.com/tech-policy/news/2009/10/
does-broadband-needs-its-own-government-nutrition-label.
ars, Oct. 2010. Ars Technica.
[3] C. Bastian, T. Klieber, J. Livingood, J. Mills, and R. Woundy. Comcast’s
protocol-agnostisc congestion management system. Internet Engineering Task
Force, Dec. 2010. RFC 6057.
[4] G. Bernardi and M. K. Marina. Bsense: a system for enabling automated
broadband census: short paper. In Proc. of the 4th ACM Workshop on
Networked Systems for Developing Regions (NSDR ’10), June 2010., 2010.
[5] K. Bode. FCC: One Million Speedtests and Counting.
http://www.dslreports.com/shownews/
FCC-One-Million-Speedtests-And-Counting-109440, July
2010.
[6] A. Botta, A. Dainotti and A. Pescapé. Multi-protocol and multi-platform trafﬁc
generation and measurement. IEEE INFOCOM, Demo session, May 2007.
[7] R. Carlson. Network Diagnostic Tool.
http://e2epi.internet2.edu/ndt/.
[8] K. Cho, K. Fukuda, H. Esaki, and A. Kato. The impact and implications of the
growth in residential user-to-user trafﬁc. In ACM SIGCOMM 2006, 2006.
[9] Comcast FAQ.
http://customer.comcast.com/Pages/FAQViewer.aspx?
Guid=024f23d4-c316-4a58-89f6-f5f3f5dbdcf6, Oct. 2007.
[10] R. Compton, C.L. Woundy and J. Leddy. Method and packet-level device for
trafﬁc regulation in a data network. U.S. Patent 7,289,447 B2, Oct. 2007.
[11] D. Croce, T. En-Najjary, G. Urvoy-Keller, and E. Biersack. Capacity Estimation
of ADSL links. In CoNEXT, 2008.
[12] M. Dischinger, A. Haeberlen, K. P. Gummadi, and S. Saroiu. Characterizing
residential broadband networks. In Proc. ACM SIGCOMM Internet
Measurement Conference, San Diego, CA, USA, Oct. 2007.
[13] Emulab. http://www.emulab.net/, 2006.
[14] M. M. et al. Network Path and Application Diagnosis.
http://www.psc.edu/networking/projects/pathdiag/.
[15] National Broadband Plan. http://www.broadband.gov/.
[16] J. Gettys. Bufferbloat. http://www.bufferbloat.net/.
[17] Glasnost: Bringing Transparency to the Internet.
http://broadband.mpi-sws.mpg.de/transparency.
[18] D. Han, A. Agarwala, D. G. Andersen, M. Kaminsky, K. Papagiannaki, and
S. Seshan. Mark-and-sweep: Getting the inside scoop on neighborhood
networks. In Proc. Internet Measurement Conference, Vouliagmeni, Greece,
Oct. 2008.
[19] Internet World Stats.
http://www.internetworldstats.com/dsl.htm.
[20] Asymmetric Digital Subscriber Line Transceivers. ITU-T G.992.1, 1999.
[21] Asymmetric Digital Subscriber Line (ADSL) Transceivers - Extended
Bandwidth ADSL2 (ADSL2Plus). ITU-T G.992.5, 2003.
[22] Data-over-cable service interface speciﬁcations: Radio-frequency interface
speciﬁcation. ITU-T J.112, 2004.
[23] C. R. S. Jr. and G. F. Riley. Neti@home: A distributed approach to collecting
end-to-end network performance measurements. In the Passive and Active
Measurement Conference (PAM), 2004.
[24] C. Kreibich, N. Weaver, B. Nechaev, and V. Paxson. Netalyzr: Illuminating the
edge network. In Proc. Internet Measurement Conference, Melbourne,
Australia, Nov. 2010.
[25] K. Lakshminarayanan and V. N. Padmanabhan. Some ﬁndings on the network
performance of broadband hosts. In Proceedings of the 3rd ACM SIGCOMM
conference on Internet measurement, IMC ’03, pages 45–50, New York, NY,
USA, 2003. ACM.
[26] G. Maier, A. Feldmann, V. Paxson, and M. Allman. On dominant characteristics
of residential broadband internet trafﬁc. In ACM Internet Measurement
Conference, 2009.
[27] Measurement Lab. http://measurementlab.net.
[28] A. Morton and B. Claise. Packet Delay Variation Applicability Statement.
Internet Engineering Task Force, Mar. 2009. RFC 5481.
[29] Netalyzr. http://netalyzr.icsi.berkeley.edu/.
[30] NetFlix Performance on Top ISP Networks.
http://techblog.netflix.com/2011/01/
netflix-performance-on-top-isp-networks.html, Jan. 2011.
[31] NOX Box. http://noxrepo.org/manual/noxbox.html.
[32] ShaperProbe. http://www.cc.gatech.edu/~partha/diffprobe/
shaperprobe.html.
[33] M. Siekkinen, D. Collange, G. Urvoy-Keller, and E. Biersack. Performance
limitations of ADSL users: A case study. In the Passive and Active
Measurement Conference (PAM), 2007.
[34] S. Sundaresan, N. Feamster, R. Teixeira, A. Tang, K. Edwards, R. Grinter,
M. Chetty, and W. de Donato. Helping users shop for isps with internet nutrition
labels. In ACM SIGCOMM Workshop on Home Networks, 2011.
[35] D. Vorhaus. A New Way to Measure Broadband in America.
http://blog.broadband.gov/?entryId=359987, Apr. 2010.
[36] WonderShaper. http://lartc.org/wondershaper/, 2002.
01020304050Time (seconds)800012000160002000024000Throughput (Kbits/s)ContinuousIntermittentWonderShaper01020304050Time (seconds)1001000Latency (ms)ContinuousIntermittentWonderShaper145