considered (i.e., mean aggregation rule is used). For instance,
the DNN classiﬁers respectively achieve testing error rates
0.01, 0.08, 0.07, and 0.01 in centralized learning on the four
datasets, while they respectively achieve testing error rates
0.04, 0.09, 0.09, and 0.01 in federated learning with the mean
aggregation rule on the four datasets. However, in the sce-
narios where users’ training data can only be stored on their
edge/mobile devices, e.g., for privacy purposes, centralized
learning is not applicable and federated learning may be the
only option even though its error rate is higher. Compared to
the mean aggregation rule, Byzantine-robust aggregation rule
increases the error rate without attacks. However, if Byzantine-
robust aggregation rule is not used, a single malicious device
can make the learnt global model totally useless [9, 66]. To
summarize, in the scenarios where users’ training data can
only be stored on their edge/mobile devices and there may
exist attacks, Byzantine-robust federated learning may be the
best option, even if its error rate is higher.
Impact of the percentage of compromised worker de-
vices: Figure 2 shows the error rates of different attacks
as the percentage of compromised worker devices increases
on MNIST. Our attacks increase the error rates signiﬁcantly
as we compromise more worker devices; label ﬂipping only
slightly increases the error rates; and Gaussian attacks have
no notable impact on the error rates. Two exceptions are that
Krum’s error rates decrease when the percentage of compro-
mised worker devices increases from 5% to 10% in Figure 2a
and from 10% to 15% in Figure 2d. We suspect the reason is
USENIX Association
29th USENIX Security Symposium    1631
(a) Krum
(b) Trimmed mean
(c) Median
(d) Krum
(e) Trimmed mean
(f) Median
Figure 3: Testing error rates for different attacks as we increase the degree of non-IID on MNIST. (a)-(c): LR classiﬁer and
(d)-(f): DNN classiﬁer.
that Krum selects one local model as a global model in each
iteration. We have similar observations on the other datasets.
Therefore, we omit the corresponding results for simplicity.
Impact of the degree of non-IID in federated learn-
ing: Figure 3 shows the error rates for the compared attacks
for different degrees of non-IID on MNIST. Error rates of
all attacks including no attacks increase as we increase the
degree of non-IID, except that the error rates of our attacks to
Krum ﬂuctuate as the degree of non-IID increases. A possible
reason is that as the local training datasets on different worker
devices are more non-IID, the local models are more diverse,
leaving more room for attacks. For instance, an extreme ex-
ample is that if the local models on the benign worker devices
are the same, it would be harder to attack the aggregation
rules, because their aggregated model would be more likely
to depend on the benign local models.
Impact of different parameter settings of federated learn-
ing algorithms: We study the impact of various parame-
ters in federated learning including the number of rounds
of stochastic gradient descent each worker device performs,
number of worker devices, number of worker devices selected
to update the global model in each iteration, and β in trimmed
mean. In these experiments, we use MNIST and the LR clas-
siﬁer for simplicity. Unless otherwise mentioned, we consider
median, as median is more robust than Krum and does not
require conﬁguring extra parameters (trimmed mean requires
conﬁguring β). Moreover, for simplicity, we consider partial
knowledge attacks as they are more practical.
Worker devices can perform multiple rounds of stochastic
gradient descent to update their local models. Figure 4a shows
the impact of the number of rounds on the testing error rates
of our attack. The testing error rates decrease as we use more
rounds of stochastic gradient descent for both no attack and
our partial knowledge attack. This is because more rounds
of stochastic gradient descent lead to more accurate local
models, and the local models on different worker devices
are less diverse, leaving a smaller attack space. However, our
attack still increases the error rates substantially even if we use
more rounds. For instance, our attack still increases the error
rate by more than 30% when using 10 rounds of stochastic
gradient descent. We note that a large number of rounds result
in large computational cost for worker devices, which may be
unacceptable for resource-constrained devices such as mobile
phones and IoT devices.
Figure 4b shows the testing error rates of our attack as the
number of worker devices increases, where 20% of worker
devices are compromised. Our attack is more effective (i.e.,
testing error rate is larger) as the federated learning system
involves more worker devices. We found a possible reason
is that our partial knowledge attacks can more accurately
estimate the changing directions with more worker devices.
For instance, for trimmed mean of the DNN classiﬁer on
MNIST, our partial knowledge attacks can correctly estimate
the changing directions of 72% of the global model param-
eters on average when there are 50 worker devices, and this
fraction increases to 76% when there are 100 worker devices.
1632    29th USENIX Security Symposium
USENIX Association
(a)
(b)
(c)
Figure 4: (a) Impact of the number of rounds of stochastic gradient descent worker devices use to update their local models in
each iteration on our attacks. (b) Impact of the number of worker devices on our attacks. (c) Impact of the number of worker
devices selected in each iteration on our attacks. MNIST, LR classiﬁer, and median are used.
(a)
(b)
(c)
Figure 5: (a) Testing error rates of the trimmed mean aggregation rule when using different β. (b) Testing error rates of the Krum
aggregation rule when our attack uses different ε. (c) Testing error rates of the median aggregation rule when our attacks poison a
certain fraction of randomly selected iterations of federated learning. MNIST and LR classiﬁer are used.
In federated learning [39], the master device could ran-
domly sample some worker devices and send the global model
to them; the sampled worker devices update their local mod-
els and send the updated local models to the master device;
and the master device updates the global model using the
local models from the sampled worker devices. Figure 4c
shows the impact of the number of worker devices selected in
each iteration on the testing error rates of our attack, where
the total number of worker devices is 100. Since the master
device randomly selects a subset of worker devices in each
iteration, a smaller number of compromised worker devices
are selected in some iterations, while a larger number of com-
promised worker devices are selected in other iterations. On
average, among the selected worker devices, c
m of them are
compromised ones, where c is the total number of compro-
mised worker devices and m is the total number of worker
devices. Our Figure 2 shows that our attacks become effective
when c
m is larger than 10%-15%. Note that an attacker can
inject a large number of fake devices to a federated learning
system, so c
m can be large.
The trimmed mean aggregation rule has a parameter β,
which should be at least the number of compromised worker
devices. Figure 5a shows the testing error rates of no attack
and our partial knowledge attack as β increases. Roughly
speaking, our attack is less effective (i.e., testing error rates
are smaller) as more local model parameters are trimmed.
This is because our crafted local model parameters on the
compromised worker devices are more likely to be trimmed
when the master device trims more local model parameters.
However, the testing error of no attack also slightly increases
as β increases. The reason is that more benign local model
parameters are trimmed and the mean of the remaining local
model parameters becomes less accurate. The master device
may be motivated to use a smaller β to guarantee performance
when there are no attacks.
Impact of the parameter ε in our attacks to Krum: Fig-
ure 5b shows the error rates of the Krum aggregation rule
when our attacks use different ε, where MNIST dataset and
LR classiﬁer are considered. We observe that our attacks
can effectively increase the error rates using a wide range
of ε. Moreover, our attacks achieve larger error rates when ε
is smaller. This is because when ε is smaller, the distances
between the compromised local models are smaller, which
makes it more likely for Krum to select the local model crafted
by our attack as the global model.
Impact of the number of poisoned iterations: Figure 5c
shows the error rates of the median aggregation rule when our
attacks poison the local models on the compromised worker
USENIX Association
29th USENIX Security Symposium    1633
Table 3: Testing error rates of attacks on the DNN classiﬁer
for MNIST when the master device chooses the global model
with the lowest testing error rate.
Krum
Trimmed mean
Median
NoAttack Gaussian LabelFlip Partial Full
0.69 0.70
0.12 0.18
0.11 0.32
0.10
0.06
0.06
0.10
0.06
0.06
0.09
0.07
0.06
devices in a certain fraction of randomly selected iterations
of federated learning. Unsurprisingly, the error rate increases
when poisoning more iterations.
Alternative training strategy: Each iteration results in a
global model. Instead of selecting the last global model as
the ﬁnal model, an alternative training strategy is to select
the global model that has the lowest testing error rate.3 Ta-
ble 3 shows the testing error rates of various attacks on the
DNN classiﬁer for MNIST, when such alternative training
strategy is adopted. In these experiments, our attacks attack
each iteration of federated learning, and the column “NoAt-
tack” corresponds to the scenarios where no iterations are
attacked. Compared to Table 2b, this alternative training strat-
egy is slightly more secure against our attacks. However, our
attacks are still effective. For instance, for the Krum, trimmed
mean, and median aggregation rules, our partial knowledge
attacks still increase the testing error rates by 590%, 100%,
and 83%, respectively. Another training strategy is to roll
back to a few iterations ago if the master device detects an
unusual increase of training error rate. However, such training
strategy is not applicable because the training error rates of
the global models still decrease until convergence when we
perform our attacks in each iteration. In other words, there
are no unusual increases of training error rates.
4.3 Results for Unknown Aggregation Rule
We craft local models based on one aggregation rule and show
the attack effectiveness for other aggregation rules. Table 4
shows the transferability between aggregation rules, where
MNIST and LR classiﬁer are considered. We observe different
levels of transferability between aggregation rules. Speciﬁ-
cally, Krum based attack can well transfer to trimmed mean
and median, e.g., Krum based attack increases the error rate
from 0.12 to 0.15 (25% relative increase) for trimmed mean,
and from 0.13 to 0.18 (38% relative increase) for median.
Trimmed mean based attack does not transfer to Krum but
transfers to median well. For instance, trimmed mean based
attack increases the error rates from 0.13 to 0.20 (54% relative
increase) for median.
3We give advantages to the alternative training strategy since we use
testing error rate to select the global model.
Table 4: Transferability between aggregation rules. “Krum
attack” and “Trimmed mean attack” mean that we craft the
compromised local models based on the Krum and trimmed
mean aggregation rules, respectively. Partial knowledge at-
tacks are considered. The numbers are testing error rates.
No attack
Krum attack
Trimmed mean attack
Krum Trimmed mean Median
0.14
0.70
0.14
0.12
0.15
0.25
0.13
0.18
0.20
4.4 Comparing with Back-gradient Optimiza-
tion based Attack
Back-gradient optimization based attack (BGA) [43] is state-
of-the-art untargeted data poisoning attack for multi-class clas-
siﬁers such as multi-class LR and DNN. BGA formulates a
bilevel optimization problem, where the inner optimization is
to minimize the training loss on the poisoned training data and
the outer optimization is to ﬁnd poisoning examples that maxi-
mize the minimal training loss in the inner optimization. BGA
iteratively ﬁnds the poisoned examples by alternately solving
the inner minimization and outer maximization problems. We
implemented BGA and veriﬁed that our implementation can
reproduce the results reported by the authors. However, BGA
is not scalable to the entire MNIST dataset. Therefore, we
uniformly sample 6,000 training examples in MNIST, and
we learn a 10-class LR classiﬁer. Moreover, we assume 100
worker devices, randomly distribute the 6,000 examples to
them, and assume 20 worker devices are compromised.
Generating poisoned data: We assume an attacker has full
knowledge about the training datasets on all worker devices.
Therefore, the attacker can use BGA to generate poisoned
data based on the 6,000 examples. In particular, we run the
attack for 10 days on a GTX 1080Ti GPU, which generates
240 (240/6000 = 4%) poisoned examples. We veriﬁed that
these poisoned data can effectively increase the testing error
rate if the LR classiﬁer is learnt in a centralized environment.
In particular, the poisoned data can increase the testing error
rate of the LR classiﬁer from 0.10 to 0.16 (60% relative in-
crease) in centralized learning. However, in federated learning,
the attacker can only inject the poisoned data to the compro-
mised worker devices. We consider two scenarios on how
the attacker distributes the poisoned data to the compromised
worker devices:
Single worker. In this scenario, the attacker distributes the
poisoned data on a single compromised worker device.
Uniform distribution. In this scenario, the attacker dis-
tributes the poisoned data to the compromised worker devices
uniformly at random.
We consider the two scenarios because they represent two
extremes for distributing data (concentrated or evenly dis-
tributed) and we expect one extreme to maximize attack effec-
tiveness. Table 5 compares BGA with our attacks. We observe
1634    29th USENIX Security Symposium
USENIX Association
Table 5: Testing error rates of back-gradient optimization
based attacks (SingleWorker and Uniform) and our attacks
(Partial and Full).
Mean
Krum
Trimmed mean
Median
NoAttack SingleWorker Uniform Partial Full
0.54 0.69
0.85 0.89
0.27 0.32
0.19 0.21
0.10
0.23
0.12
0.13
0.11
0.24
0.12
0.13
0.15
0.25
0.13
0.14
that BGA has limited success at attacking Byzantine-robust
aggregation rules, while our attacks can substantially increase
the testing error rates. We note that if the federated learning
uses the mean aggregation rule BGA is still successful. For
instance, when the mean aggregation rule is used, BGA can
increase the testing error rate by 50% when distributing the
poisoned data to the compromised worker devices uniformly
at random. However, when applying our attacks for trimmed
mean to attack the mean aggregation rule, we can increase the
testing error rates substantially more (see the last two cells in
the second row of Table 5).
5 Defenses
We generalize RONI [4] and TRIM [30], which were designed
to defend against data poisoning attacks, to defend against
our local model poisoning attacks. Both generalized defenses
remove the local models that are potentially malicious before
computing the global model in each iteration of federated
learning. One generalized defense removes the local models
that have large negative impact on the error rate of the global
model (inspired by RONI that removes training examples that
have large negative impact on the error rate of the model),
while the other defense removes the local models that result
in large loss (inspired by TRIM that removes the training
examples that have large negative impact on the loss). In both
defenses, we assume the master device has a small validation
dataset. Like existing aggregation rules such as Krum and
trimmed mean, we assume the master device knows the upper
bound c of the number of compromised worker devices. We
note that our defenses make the global model slower to learn
and adapt to new data as that data may be identiﬁed as from
potentially malicious local models.
Error Rate based Rejection (ERR):
In this defense, we
compute the impact of each local model on the error rate for
the validation dataset and remove the local models that have