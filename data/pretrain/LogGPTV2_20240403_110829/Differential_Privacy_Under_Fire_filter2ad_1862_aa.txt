title:Differential Privacy Under Fire
author:Andreas Haeberlen and
Benjamin C. Pierce and
Arjun Narayan
Differential Privacy Under Fire
Andreas Haeberlen
Benjamin C. Pierce
Arjun Narayan
University of Pennsylvania
Abstract
Anonymizing private data before release is not enough
to reliably protect privacy, as Netﬂix and AOL have
learned to their cost. Recent research on differential
privacy opens a way to obtain robust, provable privacy
guarantees, and systems like PINQ and Airavat now of-
fer convenient frameworks for processing arbitrary user-
speciﬁed queries in a differentially private way. How-
ever, these systems are vulnerable to a variety of covert-
channel attacks that can be exploited by an adversarial
querier.
We describe several different kinds of attacks, all fea-
sible in PINQ and some in Airavat. We discuss the space
of possible countermeasures, and we present a detailed
design for one speciﬁc solution, based on a new primi-
tive we call predictable transactions and a simple differ-
entially private programming language. Our evaluation,
which relies on a proof-of-concept implementation based
on the Caml Light runtime, shows that our design is ef-
fective against remotely exploitable covert channels, at
the expense of a higher query completion time.
1 Introduction
Privacy is a problem. Vast amounts of data about individ-
uals is constantly accumulating in various databases—
patient records, content and link graphs of social net-
works, mobility traces in cellular networks, book and
movie ratings, etc.—and there are many socially valu-
able uses to which it can potentially be put. But, as Net-
ﬂix and others have discovered [3, 22], even when data
collectors try to protect the privacy of their customers by
releasing anonymized or aggregated data, this data often
reveals much more than intended, especially when it is
combined with other data sources. To reliably prevent
such privacy violations, we need to replace current ad-
hoc solutions with a principled data release mechanism
that offers strong, provable privacy guarantees.
Recent research on differential privacy [8–10] has
brought us a big step closer to achieving this goal. Dif-
ferential privacy allows us to reason formally about what
an adversary could learn from released data, while avoid-
ing many assumptions (e.g., what exactly the adversary
might try to learn, or what he or she might already know)
that have been the cause of privacy violations in the past.
Early work on differentially private data analysis relied
on manual proofs by privacy experts that the answers to
particular queries were safe to release [21]; today, sys-
tems like PINQ [20] and Airavat [26] can perform dif-
ferentially private data analysis automatically, without
needing a human expert in the loop.
Airavat and PINQ go beyond just certifying queries by
the data owner as differentially private; they are explic-
itly designed to support untrusted queries over private
databases.
In this model, a third party is permitted to
submit arbitrary queries over the database, but the data
owner imposes a “privacy budget” that limits the amount
of information the third party can obtain about any indi-
vidual whose data is in the database. The system ana-
lyzes each new query to determine its potential “privacy
cost” and allows it to run only if the remaining balance
on the privacy budget is sufﬁciently high. This mode of
operation is attractive for many scenarios; for example,
Netﬂix could give researchers access to its database of
movie ratings via such a query interface and still give
strong privacy assurances to customers. An adversarial
querier could not, for instance, obtain an accurate answer
to the query “Has John Doe watched any adult movies?”
because the cost of such a query would exceed any rea-
sonable privacy budget.
However, Airavat and PINQ both contain vulnerabili-
ties that can be exploited by an adversary to extract pri-
vate information through covert channels.1 The reason is
that these systems rely on the assumption that the querier
can observe only the result of the query, and nothing else.
In practice, however, the querier is also able to observe
other effects of his query, such the time it takes to com-
1The designers of these systems were aware of these covert chan-
nels, and each addresses them to some extent. See Sections 3.5 and 3.6.
1
plete. Such observations can be exploited to mount a
covert-channel attack. To continue with our earlier ex-
ample, the adversary might run a query that always re-
turns zero as its result but that takes one hour to com-
plete if John Doe has watched adult movies and less than
a second otherwise. Both Airavat and PINQ would con-
sider the output of such a query to be safe because it does
not depend on the contents of the private database at all.
However, the adversary can still learn with perfect cer-
tainty whether John Doe has watched adult movies—a
blatant violation of differential privacy. PINQ’s proto-
type implementation also permits global variables to be
used as covert channels to leak private information dur-
ing query execution.
Covert channels have plagued computer systems for
many years [1, 2, 15, 16, 18, 27, 30, etc.], and they are no-
toriously difﬁcult to avoid [7]. However, they are partic-
ularly devastating in a system that is designed to enforce
differential privacy: if a channel allows the adversary to
learn even a single bit of private information, the differ-
ential privacy guarantees are already broken! Thus, dif-
ferential privacy puts particularly high demands on a de-
fense against covert channels; merely limiting the band-
width of the channels is not enough.
Fortunately, the untrusted-query scenario has two fea-
tures that make a solution feasible. First, there is no need
to allow the querier direct access to the machine that
hosts the database; he can be forced to submit queries
and receive results over the network. This rules out difﬁ-
cult channels such as power consumption [17] and elec-
tromagnetic radiation [13,24], essentially leaving the ad-
versary with just two channels: the privacy budget and
the query completion time.
Our key insight is that, in this speciﬁc scenario, these
two channels can be closed completely through a com-
bination of two techniques. The budget channel can be
closed by using program analysis to statically determine
the privacy cost of each query. Thus, the deduction from
the privacy budget is independent of the database con-
tents. The external timing channel can be closed by a)
breaking each query into “microqueries” that operate on
a single database row at a time, and by b) enforcing that
each microquery takes a ﬁxed amount of time. (If nec-
essary, the microquery is aborted and a default value is
returned.
In the context of differential privacy, this is
safe—and does not open another channel—because the
privacy cost of the default values is already included in
the privacy cost of the query.) Thus, we can obtain strong
privacy assurances even if the adversary can pose arbi-
trary queries and can observe all the (remotely measur-
able) channels that are possible in our model.
We present the design of Fuzz, a system that imple-
ments this defense. Fuzz uses a novel type system [25]
to statically infer the privacy cost of arbitrary queries
written in a special programming language, and it uses
a novel primitive called predictable transactions to en-
sure that a potentially adversarial computation completes
within a speciﬁc time or returns a default value. We have
built and evaluated a proof-of-concept implementation of
Fuzz based on the Caml Light runtime system [5, 19].
Our results show that Fuzz effectively closes all known
remotely exploitable channels, at the expense of a higher
query completion time.
Implementing predictable transactions is challenging
in practice: Fuzz must be able to abort an arbitrary and
potentially adversarial computation by a speciﬁed dead-
line, even if the adversary is actively trying to cause the
deadline to be missed, and must ensure that—whether
or not the computation is aborted—it leaves no linger-
ing traces that can measurably affect the program’s over-
all execution time (garbage in the heap, VM pages that
must later be freed by the OS, etc). Nevertheless, we
show that, across a variety of adversarial queries that ex-
ploit different attack strategies, our implementation ex-
hibits extremely small variation in completion time—on
the order of the time required to handle a single timer
interrupt. This variation is so small that it is difﬁcult to
measure even on the machine itself. Thus, it would be
useless to a remote attacker, who would have to measure
it across a wide-area network using the limited number
of trials that the privacy budget permits.
In summary, we make the following contributions:
1. a detailed analysis of several classes of covert-
channel attacks and a discussion of which are feasi-
ble in PINQ and Airavat (Section 3);
2. an analysis of the space of potential solutions (4);
3. a concrete design for one speciﬁc solution, based on
default values and predictable transactions (5+6);
4. a proof-of-concept implementation of our design
(7); and
5. an experimental evaluation (8).
We close with a discussion of related work and a few
concluding thoughts.
2 Background
Before describing our attacks and the Fuzz design and
implementation, we brieﬂy review some technical back-
ground on differential privacy, function sensitivity, and
differentially private programming languages.
2.1 Differential privacy
Differential privacy [8] is a property of randomized func-
tions that take a database as input and return a result that
is typically some form of aggregate (a real number rep-
resenting a count; a histogram; etc.). The database (db)
2
is a collection of “rows,” one for each individual whose
privacy we mean to protect.
Informally, a randomized function is differentially pri-
vate if arbitrary changes to a single individual’s row
(keeping other rows constant) result in only statistically
insigniﬁcant changes in the function’s output distribu-
tion; thus, any individual’s presence in the database has a
statistically negligible effect. Formally [12], differential
privacy is parametrized by a real number ε, correspond-
ing to the strength of the privacy guarantee: smaller ε’s
yield more privacy. Two databases b and b′ are consid-
ered similar, written b ∼ b′, if they differ in only one
row. We then say that a randomized function q : db → R
is ε-differentially private if, for all possible sets of out-
puts S ⊆ R, and for all similar databases b, b′, we have
Pr[q(b) ∈ S] ≤ eε · Pr[q(b′) ∈ S]. That is, when the in-
put database is changed in one row, there is at most a very
small multiplicative difference (eε) in the probability of
any set of outcomes S.
Methods for achieving differential privacy can be at-
tractively simple—e.g., perturbing the true answer to a
numeric query with carefully calibrated random noise.
For example, the query “How many patients at this hos-
pital are over the age of 40?” is intuitively “almost safe”:
safe because it aggregates many individuals’ information
together, but only “almost” because, if an adversary hap-
pened to know the ages of every patient except John Doe,
then answering this query exactly would give him certain
knowledge of a fact about John. The differential privacy
methodology rests on the observation that, if we add a
small amount of random noise to this query’s result, we
still get a useful estimate of the true answer while ob-
scuring the age of any single individual. By contrast, the
query “How many patients named John Doe are over the
age of 40” is plainly problematic, since the answer is
very sensitive to the presence or absence of a single indi-
vidual. Such a query cannot usefully be privatized: if we
add enough noise to mostly obscure the contribution of
John Doe’s age, there will be essentially no signal left.
2.2 Compositionality and privacy budgets
An important consequence of the deﬁnition of differ-
ential privacy is that composing a differentially private
function with any other function that does not, itself, de-
pend on the database yields a function that is again dif-
ferentially private—that is, no amount of postprocessing,
even with unknown auxiliary information, can lessen the
differential privacy guarantee. This allows us to reason
about harmful effects of data release that might seem
quite far removed from the function that is actually being
computed.
Another important property of differential privacy is
that its guarantee degrades gracefully under repeated ap-
plication: a pair of two ε-differentially private functions
is always 2ε-differentially private, when taken together.
This allows us to think of having a ﬁxed “privacy bud-
get” up front, which is slowly exhausted as queries are
answered: if our privacy budget is ε, we may feel free to
independently answer k queries, where the ith query is εi-
differentially private and ∑i εi ≤ ε, without fear that the
aggregation of these k queries will violate ε-differential
privacy.
2.3 Function sensitivity
The central idea in proofs of differential privacy is to
bound the sensitivity of queries to small changes in their
inputs. Sensitivity is a kind of continuity property; a
function of low sensitivity maps nearby inputs to nearby
outputs.
Sensitivity is relevant to differential privacy because
the amount of noise required to make a deterministic
query differentially private is proportional to its sensi-
tivity. For example, the sensitivity of the two age queries
discussed above is 1: adding or removing one patient’s
records from the hospital database can change the true
value of each query by at most 1. This means that we
should add the same amount of noise to “How many pa-
tients at this hospital are over the age of 40?” as to
“How many patients named John Doe are over the age of
40?” This may appear counter-intuitive, but it achieves
the right goal: the privacy of single individuals is pro-
tected to exactly the same degree in both cases. What
differs is the usefulness of the results: knowing the an-
swer to the ﬁrst query with, say, a typical error margin of
±100 could still be valuable if there are thousands of pa-
tients, whereas knowing the answer to the second query
(which can only be zero or one) ±100 is useless. We
might try making the second query more useful by scal-
ing its answer up numerically: “Is John Doe over 40? If
yes, then 1,000, else 0.” But this scaled query now has a
sensitivity of 1,000, not 1, and so 1,000 times the noise
must be added, blocking our attempt to violate privacy.
2.4 Programming with privacy
Early work on differential privacy has mostly focused
on speciﬁc algorithms rather than general, compositional
mechanisms: given a particular algorithm, we prove by
hand that it is differentially private. Most of the time, this
does not require much ingenuity—just applying known
techniques—but even so,
this approach doesn’t scale
well because it demands that each new algorithm be cer-
tiﬁed by a skilled, trusted human. A better approach is to
automate this certiﬁcation process with a programming
language in which every well-typed program is guaran-
teed to be differentially private. Then (untrusted) non-
experts can write as many different algorithms as they
like, and the database administrator can rely on the lan-
guage to ensure that privacy is not being violated.
3
Systems are beginning to be available that implement
such languages—notably Privacy Integrated Queries
(PINQ) [20] and Airavat [26]. PINQ is an embedded
extension of C# that tracks the privacy impact of vari-
ety of relational algebra operations on database tables, as
well as certain forms of query composition. Airavat inte-
grates differential privacy into a distributed, Java-based
MapReduce framework.
2.5 Processing model
Although PINQ and Airavat differ in many particu-
lars, they embody essentially the same basic process-
ing model, which we also follow in the Fuzz system de-
scribed below. A query in each of these systems can be
viewed as consisting of one or more mapping operations
that process individual records in the database, together
with some reducing code that combines the results of
the mapping operations without directly looking at the
database. When a query is submitted, the system veriﬁes
that it is εi-differentially private, deducts εi from the total
privacy budget ε associated with the database, and—if ε
remains nonzero—returns the query result. (Note that,
in this model, we account for the possibility of collu-
sion between adversaries by associating the privacy bud-
get with the database and not with individual queriers.
Thus, once the budget is exhausted, we must throw away
the database and never answer any more queries.) We
call the mapping operations microqueries and the rest of
the code the macroquery.
Airavat implements a simple version of this model:
a query consists of a sequence of chained microqueries
(“mappers” in Airavat terminology) plus a selection from
among a ﬁxed set of macroqueries (“reducers”). The
mappers are the only untrusted code: the reducers are
part of the trusted base. When a query is submitted,