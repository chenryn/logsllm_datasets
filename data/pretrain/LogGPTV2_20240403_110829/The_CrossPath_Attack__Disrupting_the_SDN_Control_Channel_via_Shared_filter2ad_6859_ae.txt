p
b
M
(
t
u
p
h
g
u
o
r
h
T
 800
 600
 400
 200
without attack
with attack
without attack
with attack
 1
 0.8
o
i
t
a
R
s
s
e
c
c
u
S
 0.6
 0.4
 0.2
 0
 0
 0
 0
 5
 10
 15
Time (s)
 20
 25
 30
 0
 0
 5
 10
 15
Time (s)
 20
 25
 30
(a) Increasing link utilization due
to long-term routing rule inconsis-
tency.
(b) Long-term routing blackhole due
to delayed messages when a host is
migrated.
(c) Eviction of a routing path due to a deactivated link.
(d) Cleaning of ﬂow tables due to the reset of a switch.
Figure 16: Attack impacts on Reactive Routing.
5.4 Reactive Routing
The reactive routing [9] application enables ﬂexible and ﬁne-
grained routing decisions for different ﬂows, which is en-
abled in almost all controllers. When a new ﬂow matching
no rules is generated, the ﬁrst packet of the ﬂow will be sent
to the reactive routing application. The application analyzes
the packet and calculates routing paths for the new ﬂow. Be-
sides depending on the packet service processing data pack-
ets and ﬂow rule service installing rules, the application also
queries the topology service that provides the information of
the locations of hosts, the state of switches and links.
In order to demonstrate the effectiveness of our attack, we
build a network topology with four hosts and three switches,
as shown in Figure 15. The IP addresses of the four hosts h1,
h2, h3 and h4 are 10.0.0.1, 10.0.0.2, 10.0.0.3, and 10.0.0.4,
respectively. The hosts h1 and h2 send packets to the host
h3. The default routing path of packets from h1 to h3 is . The default routing path of
packets from h2 to h3 is . Also, a
ﬂow with TCP port 1111 from h2 to h3 has a different path
due to a QoS requirement. Here, the compromised host h4
sends attack (i.e., LDoS) trafﬁc to h3 in order to exploit the
control path of switch s2.
, ls3→h3
, ls3→h3
, ls2→s3
, ls2→s3
, ls1→s2
 50
 200
 250
 0
 0
 5
 10
 15
Time (s)
 20
 25
 30
 100
 150
New Flows (Flows/s)
(a) Success Ratio of Rule Installa-
tion for a Switch.
(b) Throughput for a Switch with
250 Flows/s.
Figure 14: Attack impacts on Learning Switch.
5.3 Learning Switch
The learning switch application [6] allows SDN switches act
as normal switches in IP networks. The application exam-
ines a packet matching no rules in a switch and looks up
the recorded mapping between the source MAC address and
the port. If the destination MAC address has already been
associated with a port, the packet will be sent to the port
and corresponding rules will be installed to match subse-
quent packets. Otherwise, the packet will be ﬂooded on
all ports. As shown in 12, the application relies on two
services. The packet service sends the packet to the con-
troller via packet in messages and back to the switch via
packet out messages, and the ﬂow rule service installs rules
in the switch via f low mod messages.
Our attack can effectively block installation of forward-
ing decisions generated by the application by disturbing the
messages exchanged between the core services and switches.
Figure 14 shows the impacts of the attack on the functional-
ities of learning switch. Here, we deﬁne the success ratio
of rule installation as the number of successfully installed
rules over the number of rule requests within a second. As
shown in Figure 14a, the success ratio of rule installation in
a switch always maintains over 90% with various numbers
of new ﬂows without our attack. However, it drops signiﬁ-
cantly in presence of our attack. When the rate of new ﬂows
reaches 250 ﬂows per-second, the success ratio reduces to
below 20%. Thus, learning switch cannot work correctly. As
shown in Figure 14b, the throughput of a switch is 0 Mbps
for a long time with attack when there are 250 ﬂows/s.
10.0.0.3:1111, to s3
10.0.0.3, to s2
h1
10.0.0.3:1111, to s1
10.0.0.3, to s3
s1
s2
h2
h4
SDN 
Controller
c
s3
h3
Figure 15: The network topology used in Reactive Routing.
USENIX Association
28th USENIX Security Symposium    29
Figure 16 shows the impacts of the attack on reactive
routing. As shown in Figure 16a, our attack incurs long-
term routing rule inconsistency, which makes the link uti-
lization reach 100%. The reason is that SDN exists transient
rule inconsistency [36] which can be leveraged by our at-
tack. In the network shown in Figure 15, packets with an
IP destination address 10.0.0.3 and a destination port 1111
loop between s1 and s2 when the application deletes rule
“10.0.0.3 : 1111, to s3” while rule “10.0.0.3 : 1111, to s1”
remains. The rule inconsistency normally lasts for a very
short period before all the commands of deleting correspond-
ing rules of the ﬂow are issued. However, our attack can de-
lay the commands exchanged between the ﬂow rule service
and s2 for tens of seconds. Thus, the packets loop between
s1 and s2 for a long period and the link utilization between
the two switches increases with more packets injected.
Figure 16b shows the long-term routing blackhole when
h3 is migrated from s3 to s2. The migration is ﬁnished
within ﬁve seconds without the attack, as the topology ser-
vice can track the new location via packet in messages con-
taining the DHCP payload when the host moves to s2. How-
ever, the messages are signiﬁcantly delayed under our attack,
and thereby the routing between other hosts and h3 cannot
be updated in time, causing more than 10 seconds routing
blackhole. Moreover, by blocking LLDP packets between
the topology service and switches, our attack can deactivate
links in the topology database and thus the corresponding
routing paths will be removed. In the Floodlight controller,
a link will be deactivated if no LLDP packets pass through
the links within 35s. Figure 16c shows the original routing
path from h2 to h3 is removed since our attack deactivates
the link from s2 to s3. Moreover, our attack can reset the
connections between switches and the controller by delay-
ing control messages. Figure 16d shows the connection of
switch s2 is reset and all the ﬂow tables are cleaned.
5.5 Load Balancer
Load balancing has been widely used to improve resource
usage and throughput as well as reduce response delays,
which balances the workload among multiple nodes. SDN
controllers deploy the load balancer [7] application to
achieve the goal. The application in the Floodlight controller
can balance requests of clients in two way, i.e., round robin
and statistics-based scheduling. Round robin scheduling ran-
domly chooses a server from a server pool to serve a new
request each time. The statistics-based scheduling chooses a
server that has the lowest utilization to serve a new request,
where the utilization is calculated according to the real-time
statistics of the switch ports. The load balancer application
relies on the ﬂow metrics service to collect the statistics.
We conﬁgure the load balancer application in Floodlight
to enable statistics-based scheduling, as it can provide better
load balancing under different ﬂow distribution of clients. In
Server 1
Server 2
 1
 0.8
n
o
i
t
a
z
i
l
i
t
U
t
r
o
P
 0.6
 0.4
 0.2
Server 1
Server 2
overloaded
 1
 0.8
n
o
i
t
a
z
i
l
i
t
U
t
r
o
P
 0.6
 0.4
 0.2
 0
 0
 5
 10
Time(s)
 15
 20
 0
 0
 5
 10
Time(s)
 15
 20
(a) Port Utilization of Servers with-
out Attack.
(b) Port Utilization of Servers with
Attack.
Figure 17: Attack impacts on Load Balancer for misallocat-
ing the workloads across servers.
our experiments, two hosts consist of a server pool and an-
other two hosts send ﬂows to the servers. Figure 17a shows
the utilization of switch ports connecting the two servers
over time without our attack. Initially, two different elephant
ﬂows are sent to the servers, which causes the port utiliza-
tion to increase to 40% and 10%, respectively. At the 7th
second, the rate of the two ﬂows exchanges. The utiliza-
tion of one server reduces from 40% to 10% while another
server increases from 10% to 40%. At the 14th second, a
new elephant ﬂow starts, and the application directs the ﬂow
to server #1 that has the lowest port utilization. The port uti-
lization of server #1 reaches 70%. Unfortunately, the appli-
cation will mistakenly direct the ﬂow to server #2 under our
attack. As shown in Figure 17b, the port utilization of server
#2 reaches 100%. The reason is that our attack can signiﬁ-
cantly delay the stats request and stats reply messages ex-
changed between the ﬂow metrics service and switches, and
thus the applications cannot know the port utilization in time.
Actually, the application considers that the port utilization of
server #2 is still 10% when the new ﬂow comes.
6 Defense Schemes
In this section, we discuss possible countermeasures that net-
work administrators can be used to mitigate the attack.
Delivering Control Trafﬁc with High Priority. To defend
against the attack, one way is to ensure forwarding con-
trol trafﬁc with high priority, which thus can protect con-
trol trafﬁc from being congested by malicious data trafﬁc.
According to our analysis, such a defense scheme can be
enforced by carefully conﬁguring Priority Queue (PQ) or
Weighted Round Robin Queue (WRR) in switches. We note
that many commercial SDN switches support at least one
of the two queueing mechanisms (see Appendix C). We im-
plement the defense scheme based on PQ and WRR in our
hardware switches to deliver control trafﬁc with high prior-
ity. The evaluation shows it can effectively protect control
trafﬁc against malicious data trafﬁc. The detailed implemen-
tations and evaluations can be found in Appendix B.
Proactively Reserving Bandwidth for Control Trafﬁc.
30    28th USENIX Security Symposium
USENIX Association
Another way to defend against the attack is to proactively
reserve proprietary bandwidth for control trafﬁc. Such a
defense scheme is suitable for SDN switches that do not
support PQ and WRR mechanisms. We implement the de-
fense scheme with OpenFlow meter table in our hardware
switches. We have demonstrated that control trafﬁc can be
well protected by reserving enough bandwidth. We refer the
reader to Appendix B for details. The main disadvantage of
the defense scheme is that the reserved bandwidth cannot be
used by other trafﬁc even there is massive free bandwidth.
Our future work will focus on how to dynamically reserve
the bandwidth for control trafﬁc to make full use of it.
Disturbing Path Reconnaissances. The necessary condi-
tion to successfully launch the CrossPath attack is to ﬁnd a
target path containing shared links. Thus, we can prevent
the attack by disturbing path reconnaissances. One way is to
deliberately add random delays when installing ﬂow rules,
which may result in incorrect delay measurements of control
paths when conducting path reconnaissances. Our evaluation
shows that the accuracy of path reconnaissances can drops to
less than 30% by adding random delays ranging from 100
ms to 1,000 ms. However, adding random delays affects the
rule installation of all ﬂows in the network. It is especially
harmful to mice ﬂows that are delay-sensitive [30]. Design-
ing a scheme to effectively disturb path reconnaissances and
reduce the impacts on network ﬂows is worth more future
research.
7 Related Work
In this section, we review related security research in SDN
and legacy networks, respectively.
Reconnaissances in SDN. SDN reconnaissances has been
extensively studied. Shin et al. [54] designed an SDN scan-
ner to determine whether a network is SDN by measuring
response delays of pings. Cui et al. [25] further conducted
experiments in real SDN testbed to demonstrate its feasibil-
ity. Kl¨oti et al. [39] presented a reconnaissance technique to
determine if an SDN has rules for aggregated TCP ﬂows by
timing the TCP setup time. Achleitner et al. [19] designed
SDNMap to reconstruct composition of ﬂow rules by ana-
lyzing probing packets with speciﬁc protocols. Liu et al. [45]
developed a Markov model to reveal rule distribution among
switches. John et al. [56] presented a sophisticated inference