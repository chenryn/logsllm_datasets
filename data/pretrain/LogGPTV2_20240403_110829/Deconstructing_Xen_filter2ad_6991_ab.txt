L2:
Linux
+KVM
VM
VM-1 VM-2
VM-1 VM-2
Qemu
L1:
Qemu
Qemu
KVM KVM
Qemu
Linux+KVM
L1: Linux+KVM
Hyperlet
Linux
Hyper
Lock
KVM KVM
Linux
Trusted Component
(f) KVM
(g) Turtles KVM
(h) DeHype
(i) HyperLock
Fig. 1. Comparison between hypervisor reorganization approaches for both Xen and KVM.
TABLE II.
KEY STEPS OF
TABLE III.
THE RESULTS OF
VULNERABILITY
DIFFERENT ATTACKS
Key Step
Ratio
Result
Memory corruption
45.14%
Host DoS
Misuse of h/w feature
22.22%
Live lock
Inﬁnite loop
False BUG_ON
General fault
8.33%
6.25%
6.25%
4.86%
Run out of resource
4.17%
Dead lock
3.47%
Ratio
61.81%
15.28%
13.89%
Privilege escalation
(to host)
Info leak
Guest DoS (self)
6.25%
Guest DoS (other)
2.78%
Privilege escalation
(to guest kernel)
3.82%
TABLE IV.
COMPARISON ON ATTACKS DEFENDING.
Hypervisor illegally
accesses VM’s data
Guest causes
host DoS
Guest application
hacks its own VM
by hypervisor
Disaggregated Xen [22]
Xoar [12]
Turtles KVM [9]
DeHype [34]
HyperLock [33]
CloudVisor [37]
Nexen
No
No
No
No
No
Yes
Yes
No
No
Yes
Yes
Yes
No
Yes
No
No
No
No
No
Yes
Yes
Overall, we believe that a reliable isolation mechanism with
the ability to limit the privilege of each part of the hypervisor
can effectively prevent most attacks, which we demonstrate in
the rest of the paper.
B. Previous Solutions
Prior research has explored various hypervisor hardening
techniques. Figure 1 classiﬁes core related efforts according
to their platform (Xen or KVM), trusted components, and the
3
hardware privilege layer each component resides: ring-0 or
ring-3 and root mode or non-root mode.
The top half of the ﬁgure shows the architectures securing
Xen. Xen has a Dom0, which is a privileged para-virtualized
is responsible for I/O operations2. VMs run in
VM that
non-root mode without modiﬁcation, a.k.a., hardware-assisted
virtual machine (HVM). Disaggregated Xen [22] decomposes
Dom0, moving all the code for building a guest VM to a
separate VM named DomB (“B” for “Builder”). Thus any
vulnerabilities of the domain builder can be isolated within
the VM boundary without affecting other VMs or the host
system. Xoar [12] takes a further step by decomposing Dom0
into 7 different kinds of VMs, each focusing on just one
functionality, to achieve better fault isolation and smaller attack
surface. CloudVisor [37] targets a different goal: to protect the
guest VMs from a malicious hypervisor. It leverages nested
virtualization that puts Xen and Dom0 in non-root mode so that
all privileged operations will trap to CloudVisor for security
checking. CloudVisor can effectively defend against attacks
leveraging the hypervisor’s vulnerabilities to attack the guest
VM, e.g., in-guest privilege escalation. Most of these systems
focus on isolating Dom0’s vulnerabilities, but none of them
can defend against host DoS attacks through Xen exploits.
The bottom half of the ﬁgure shows hardening of KVM.
Unlike Xen, KVM is a kernel module in Linux. It only handles
hardware events generated by the CPU, leaving most of the
resource management (like the virtual CPU scheduling and
memory management) to the Linux kernel. Qemu emulates
devices at user-level. The Turtles project [9] has implemented
nested virtualization for KVM that can run guest VMs inside
a guest VM as a sandboxing mechanism. Xen has since
added support for nested virtualization as well [16]. Although
Intel keeps updating its processor to have better support for
2Dom0 typically runs in ring-3 in x86-64 and ring-1 in x86-32. Here we
only consider x86-64.
Guest attack boundary
TABLE V.
ATTACKS CONSIDERED & NOT CONSIDERED BY Nexen.
Domain-0
Para-VM
Full-VM
Non-root Mode
Root Mode
Xen slice
Xen slice 
Xen slice
Shared Service
Communicate
through gate
Nexen
Secure Monitor
Fig. 2. The architecture overview.
nested virtualization [11], performance overheads are still
non-negligible. DeHype [34] and HyperLock [33] decompose
KVM by creating a KVM instance for each guest VM. As a
result guest VMs can only impact (e.g., crash) its own instance.
DeHype puts the KVM instances in ring-3, resulting in high
performance overhead. HyperLock implements an in-kernel
isolation mechanism that enables different KVM instances
running within ring-0 to reduce the performance overhead
while still retaining isolation.
III. DESIGN
The primary goal of Nexen is to harden Xen against various
security threats. This is challenging because Xen operates with
ultimate system authority: there is no privilege layer to enforce
the hardening. Our key idea is to deconstruct Xen into separate
protection domains that apply the principle of least privilege
and to do so at a single privilege level.
In this section we overview Nexen, present our technique
to obtain single privilege layer isolation, describe the isolation
services that enable least-privilege, and present our decompo-
sition strategy for informed separation.
A. Nexen Overview
The Nexen architecture (Figure 2) decomposes the mono-
lithic hypervisor into a minimal, fully privileged security
monitor, monitor, a less privileged shared service domain,
and fully sandboxed Xen slices. All these domains run in
the highest privilege of the system, i.e., ring 0 of the root
mode. The core challenge of doing this at a single privilege
layer is obtaining a tamper-proof protection mechanism with
which to enforce isolation within Xen. To do so we utilize and
extend the Nested Kernel Architecture design [14] to isolate
the security monitor while operating in root-mode.
Nexen uses the isolated security monitor to control all
updates to the MMU. By controlling the MMU Nexen can
guarantee isolation between internal domains and manage
privileges. With carefully designed policies, Nexen can ensure
each internal domain has only the necessary privileges, which
signiﬁcantly reduces the attack surface of the whole system.
The next challenge Nexen considers is to devise a valuable
deconstruction of Xen. In our security analysis we observed
that many vulnerabilities are localized to speciﬁc units of
functionality in Xen. If we sandbox this functionality then we
4
Malicious
Component
Steal or Tamper
with VM’s Data
Guest DoS
Host DoS
VM (User)
VM (Kernel)
Other VM
Xen Slice
Shared Service
N.A.
Not Considered
Considered
Considered
Considered
Considered
N.A.
Considered
Not considered
Not considered
Considered
Considered
Considered
Considered
Not considered
would be providing valuable security enhancement. This is
similar to the device driver isolation literature, where highly
susceptible code is sandboxed [38], [26].
Therefore, Nexen decomposes Xen into per-VM slices that
are naturally sandboxed from all other components in the
system. Each Xen slice is bound to one VM and serves only
this VM. VMs will only interact with their own Xen slice
during runtime. Xen slices share code but each has its own
data. They are the least privileged internal domains, and errors
in one Xen slice are not considered dangerous to the whole
system or other VMs.
Unfortunately, a simple decomposition of all functionality
into slices is untenable because subsets of functionality interact
across slice boundaries. High frequency privilege boundary
crossing cause high performance degradation. So we create
a single, slightly more privileged shared service domain—but
still not as privileged as the security monitor. Deciding what
to place in per-VM slices and the shared services domain is
non-trivial and one of the key contributions of this work.
In the following sections, we ﬁrst introduce the design of
the core security monitor. Then we describe how to build in-
ternal domains based on the security monitor, along with their
interfaces and properties. Finally, we show how to deconstruct
the hypervisor to minimize its vulnerability.
B. Assumptions and Threat Model
We consider that an attacker can take full control of a user
application running in a guest VM, and tries to gain higher
privilege or issue DoS attack by exploiting the hypervisor and
its own OS. We consider the attack against hypervisor through
its ﬂaws. We also consider the attack against guest OS through
the hypervisor’s vulnerability, but not through the guest OS’s
own bugs.
We also consider that an attacker can deploy a complete
malicious guest VM on the virtualized platform, and try to
attack the hypervisor to further attack other VMs and the entire
platform through illegal data accessing or DoS attacks.
The Xen slices and shared service are not in the TCB
of our system. Even if they are compromised, they cannot
illegally access guest VM’s data. However, they can issue
DoS attack. Speciﬁcally, a Xen slice can just stop serving
its own VM, while the shared service may crash the host by
disabling scheduling. However, we do not consider physical
attacks as well as side channel attacks between different VMs.
A complete threat model matrix is listed in Table V.
C. Isolating the Monitor
The monitor is the most fundamental element to Nexen
protections. If the monitor is compromised all security in the
system is lost—this is true of any protection system. The
monitor must therefore be tamper-proof without creating high
overheads or forcing large changes to the Xen code base.
•
•
void* nx_secure_malloc(size_t size,
int owner,int policy)
void nx_free(void* p)
Instead of deprivileging Xen by moving it into Ring-3 we
use nested MMU virtualization [14], which nests a memory
protection domain within the larger system at a single privilege
level. The beneﬁt is that Nexen creates minimal performance
degradation and modiﬁcation to Xen while gaining a tamper-
proof monitor. Nested MMU virtualization works by monitor-
ing all modiﬁcations of virtual-to-physical translations (map-
pings) and explicitly removing MMU modifying instructions
from the unprivileged component—a technique called code
deprivileging.
Nexen virtualizes the MMU by conﬁguring all virtual
address translations (mappings) to page-table-pages as read-
only. Then any unexpected modiﬁcations to the page tables can
be detected by having traps go directly to the monitor. Further,
accesses to the MMU through privileged instructions must be
protected. This includes accesses to CR0 controlling the paging
and to CR3 controlling the address spaces. Nexen removes
all instances of such operations from the deprivileged Xen
code base such that there are no instructions that can modify
the MMU state: we validate this assumption by performing
a binary scan to ensure no aligned or unaligned privileged
instructions exist. The last element is to ensure that none of
the Xen components inject privileged instructions. Because the
monitor has control of the page-tables it can easily enforce
code integrity and data execution prevention.
By restricting control of the MMU to the monitor, Nexen
greatly reduces the TCB for memory management and iso-
lation services. It also enables the monitor to control critical
privileges in Xen including properties like code execution and
entry gates.
D. Intra-Domain Slicing
The primary goal of Nexen is to enhance Xen’s security by
deconstruction. To do this Nexen provides the core abstraction
of a slice to represent internal domain. Nexen extends the
monitor to provide a set of basic functionality that is required
to securely create, manage, and permit interactions between
internal domains. As shown by our vulnerability analysis,
isolation and minimizing privileges are effective ways to limit
the attack surface and control the damage.
Nexen enables two types of internal domains. The global
shared service and per-VM Xen slices. Components like the
scheduler and domain management are placed in the shared
service while functions related to only one VM, e.g., code
emulation and nested page table management, are replicated
to each Xen slice.
1) Internal Domain API: Nexen provides an internal do-
main to the shared service for the management of slices.
Shared service and the monitor are built as the system boots.
All Xen slices must be built by explicitly calling the following
interfaces provided by the monitor.
•
•
void* nx_create_dom(int dom_id)
void nx_destroy_dom(int dom_id)
These interfaces are open only to the shared service which is
responsible for building new domains. Since we use memory
mapping based isolation mechanism, each domain has its own
address space. nx_create_dom() will create a new address
space for the speciﬁed new domain and return the address of
its root page table. This is called during guest VM booting.
nx_secure_malloc() is used to assign a memory region
for storing an internal domain’s own data. An owner will have
full read/write access to the data. Access permissions exposed
to other domains are speciﬁed by the ‘policy’ argument. The
other two functions are simply the reverse operations for them,
used during the shutdown or force a destruction of a domain.
2) Controlling Memory Access Permissions: Memory ac-
cess permission is the ﬁrst class of privileges controlled by
Nexen. Memory regions are mapped differently in different in-
ternal domains so that one internal domain can only see/modify
what’s safe for it to see/modify. Mappings in internal domains
are initialized during domain building and updated later using
a tracing mechanism in the monitor if necessary. monitor
controls all MMU updates to make sure no internal domain
could break the isolation and violate memory access policies
by modifying page tables.
The memory access permissions are presented in Figure 3.
Shared service is allowed to manage memory access permis-
sions through the nx_secure_malloc() interface. Various
policies are available for different purposes. For example,
during booting, shared service will declare its own data as
invisible to all slices. When building a new Xen slice, its inner
data is “granted” to it, which means they can be modiﬁed only
in this internal domain. By default, everything inside a guest
VM should not be visible to any internal domain.
Self-VM Other VM
read / write