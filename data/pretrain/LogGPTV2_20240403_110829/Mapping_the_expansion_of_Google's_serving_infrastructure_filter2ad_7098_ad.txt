13
9856K
481
2
2
111K 46
37K 64
31K 41
21K 36
(+0%)
(+0%)
(+53%)
(+83%)
(+78%)
(+177%)
9658K
201
(-2%)
(-58%)
237K (+114%)
(+71%)
(+84%)
(+81%)
63K
57K
38K
Figure 8: Number of countries hosting Google serv-
ing infrastructure over time.
For each snapshot that we capture, we use EDNS-
client-subnet to enumerate all IP addresses returned for
www.google.com. Figure 5(a) depicts the number of
server IP addresses seen in these snapshots over time.4
The graph shows slow growth in the cumulative num-
ber of Google IP addresses observed between November
2012 and March 2013, then a major spike in mid-March
in which we saw approximately 3000 new serving IP
addresses come online. By the end of our study, the
number of serving IP addresses tripled. Figure 5(b)
shows this same trend in the growth of the number of
/24s seen to serve Google’s homepage. In Figure 5(c),
we see 82% growth in the number of ASes originating
these preﬁxes, indicating that this large growth is not
just Google adding new capacity to existing serving lo-
cations. Figure 6 shows the growth in the number of
distinct serving sites within those ASes.
Figure 7 shows the geographic locations of Google’s
serving infrastructure at the beginning of our measure-
ments and in our most recent snapshot. We observe
two types of expansion. First, we see new serving loca-
tions in remote regions of countries that already hosted
servers, such as Australia and Brazil. Second, we ob-
serve Google turning up serving infrastructure in coun-
tries that previously did not appear to serve Google’s
homepage, such as Vietnam and Thailand. Of new fron-
tend IP addresses that appeared during the course of
our study, 92% are in ASes other than Google. Of those
Table 3: Classiﬁcation of ASes hosting Google serv-
ing infrastructure at the beginning and end of our
study. We count both by the number of distinct ASes
and by the number of client /24 preﬁxes served. Al-
though Google still directs 96% of the 10 million pre-
ﬁxes to servers within its own network, it is evolving
towards serving fewer clients from its own network
and more clients from smaller ASes around the world.
addresses, only 13% are in the United States or Eu-
rope, places that are well-served directly from Google’s
network. Outside these regions, 45% are in Asia, 23%
in North America (outside the US), 20% are in South
America, and 8% are in Africa. Figure 8 depicts this
growth in the number of countries hosting serving in-
frastructure, from 53 or 56 at the beginning of our study
to 62 in recent measurements. We intend to continue
to run these measurements indeﬁnitely to continue to
map this growth.
6.2 Characterizing the Expansion
To better understand the nature of Google’s expan-
sion, we examine the types of networks where the ex-
pansion is occurring and how many clients they serve.
Table 3 classiﬁes the number of ASes of various classes
in which we observe serving infrastructure, both at the
beginning and at the end of our study. It also depicts
the number of /24 client preﬁxes (of 10 million total)
served by infrastructure in each class of AS. We use
AS classiﬁcations from the June 28, 2012 dataset from
UCLA’s Internet Topology Collection [33],5 except that
we only classify as stubs ASes with 0 costumers, and we
introduce a Tiny ISP class for ASes with 1-4 customers.
As seen in the table, the rapid growth in ASes that
4It is not necessarily the case that each IP address maps to
a distinct frontend.
5UCLA’s data processing has been broken since 2012, but
we do not expect the AS topology to change rapidly.
9
 0 1000 2000 3000 4000 5000 6000 7000 8000 90002012-10-012012-11-012012-12-012013-01-012013-02-012013-03-012013-04-012013-05-01Cumulative IPs ObservedDateEDNSOpen resolver 0 50 100 150 200 250 300 350 400 4502012-10-012012-11-012012-12-012013-01-012013-02-012013-03-012013-04-012013-05-01Cumulative /24s observedDateEDNSOpen resolver 0 20 40 60 80 100 120 140 160 180 2002012-10-012012-11-012012-12-012013-01-012013-02-012013-03-012013-04-012013-05-01Cumulative ASes observedDateEDNSOpen resolver 0 10 20 30 40 50 60 702012-10-012012-11-012012-12-012013-01-012013-02-012013-03-012013-04-012013-05-01Cumulative countries observedDateEDNSOpen resolverFigure 7: A world wide view of the expansion in Google’s infrastructure.
host infrastructure has mainly been occurring lower in
the AS hierarchy. Although Google still directs the vast
majority of client preﬁxes to servers in its own ASes, it
has begun directing an additional 2% of them to servers
oﬀ its network, representing a 98% increase in the num-
ber served from oﬀ the network. By installing servers
inside client ISPs, Google allows clients in these ISPs
to terminate their TCP connections locally (likely at
a satellite server that proxies requests to a datacen-
ter [25], as it is extremely unlikely that Google has
suﬃcient computation in these locations to provide its
services). We perform reverse DNS lookups on the IP
addresses of all frontends we located outside of Google’s
network. More than one third of them have hostnames
that include either ggc or google.cache. These results
suggest that Google is reusing infrastructure from the
Google Global Cache (GGC), Google’s content distribu-
tion network built primarily to cache YouTube videos
near users.6
Figure 9 depicts a slightly diﬀerent view of the Google
expansion. It charts the cumulative distribution of the
number of serving sites by ISP type. Almost half of the
ISPs, by any type, host only one serving site. Gener-
ally speaking, smaller ISPs host fewer serving sites than
larger ISPs, with some large ISPs hosting up to 10 dif-
ferent sites. The one exception is a Tiny ISP in Mexico
hosting 20 serving sites consisting of hundreds of fron-
tend IPs. We are currently examining this outlier in
detail.
Whereas Google would be willing to serve any client
from a server located within the Google network, an
ISP hosting a server would likely only serve its own
customers. Serving its provider’s other customers, for
6GGC documentation mentions that the servers may be
used to proxy Google Search and other services.
Figure 9: CDF of number of clusters in diﬀerent
types of ISP
example, would require the ISP to pay its provider for
the service! We check this intuition by comparing the
location in the AS hierarchy of clients and the servers
to which Google directs them. Of clients directed to
servers outside of Google’s network, 90% are located
within the server’s AS’s customer cone (the AS itself,
its customers, their customers, and so on) [18]. Since
correctly inferring AS business relationship is known to
be a hard problem [9], it is unclear whether the re-
maining 10% of clients are actually served by ISPs of
which they are not customers, or (perhaps more likely)
whether they represent limitations of the analysis. In
fact, given that 60% of the non-customer cases stem
from just 4 serving ASes, a small number of incorrect
relationship or IP-to-AS inferences could explain the
counter-intuitive observations.
Google’s expansion of infrastructure implies that, over
time, many clients should be directed to servers that are
closer to them than where Google directed them at the
10
   180° W  135° W   90° W   45° W    0°     45° E   90° E  135° E  180° E  90° S  45° S     45° N  90° N Google AS 2012−10−28Other AS 2012−10−28Google AS 2013−5−3Other AS 2013−5−3 0 0.2 0.4 0.6 0.8 1 0 5 10 15 20CDF of ISPsNumber of serving sitesLarge ISPSmall ISPTiny ISPStub ISPAllchose dates to coincide with the large jumps in Google
servers that we observe in Figure 5. Using the airport
code-based ground truth dataset from Section 5.2, Fig-
ure 11 shows the distribution of error in geolocation us-
ing these three datasets and, for comparison, the most
recent dataset using all our ﬁlters. We can see that
there is steady reduction in error over time, with me-
dian error decreasing from 817km in October 2012, to
610km in March 2013, and 475km in April 2013. How-
ever, our ﬁlters still provide substantial beneﬁt, yielding
a median error of only 22km.
7. RELATED WORK
Closest to our work is prior work on mapping CDN
infrastructures [12, 2, 32, 1]. Huang et al. [12] map two
popular content delivery networks, Akamai and Lime-
light, by enumerating their frontends using a quarter
of a million open rDNS resolvers. They geolocate and
cluster frontends using a geolocation database as well as
using the location of penultimate hop of traceroutes to
frontends. Ager et al. [2] chart web hosting structures as
a whole. They start from probing several sets of domain
names from dozens of vantage points to collect service
IP addresses. They rely entirely on MaxMind [21] for
geolocation, and use feature-based clustering where the
goal of clustering is to separate frontends belonging to
diﬀerent hosting infrastructures. Torres et al. [32] use a
small number of vantage points in the US and Europe
and constraint-based geolocation to approximately ge-
olocate serving sites in the YouTube CDN, with the
aim of understanding video server selection strategies.
Finally, Adhikari et al. [1] use open resolvers to enu-
merate YouTube servers and geolocation databases to
geolocate them, with the aim of reverse-engineering the
caching hierarchy and logical organization of YouTube
infrastructure using DNS namespaces.
In contrast to these pieces of work, our enumeration
eﬀectively uses many more vantage points, our geolo-
cation technique leverages client locations for accuracy
instead of relying on geolocation databases, and our
clustering technique relies on a metric embedding in
high-dimensional space to diﬀerentiate between nearby
sites.
Several other pieces of work are tangentially related
to ours. Mao et al. [20] quantiﬁes the proximity of
clients to their local DNS resolvers and ﬁnds that clients
in diﬀerent geographic locations may use the same re-
solver. The EDNS-client-subnet extension we use was
designed to permit serving infrastructures to more ac-
curately direct clients to serving sites in these cases.
Other work [31, 7] has exploited the observation that
two clients directed to the same or nearby frontends
are likely to be geographically close. Our work uses
this observation to geolocate frontends. Otto et al. [23]
examine the end to end impact that diﬀerent DNS ser-
Figure 10: Distances from clients to estimated fron-
tend locations to which Google directs them.
Figure 11: As Google expands, clients become
closer to their servers, improving accuracy of ﬁlter-
less client-based geolocation.
beginning of the study. Figure 10 shows the distribu-
tion of the distance from a client to our estimate of the
location of the server serving it. We restrict the clients
to those in our BitTorrent eyeball dataset and geolocate
all client locations using MaxMind. Some of the very
large distances shown in both curves could be accuracy
limitations of the MaxMind GeoLite Free database, es-
pecially in regions outside of the United States. Overall,
results show that in mid-April 2013, many clients are
substantially closer to the set of servers they are di-
rected to than in October of 2012. For example, the
fraction of client preﬁxes within 500km of their servers
increases from 39% to 53%, and the fraction within
1000km increases from 54% to 73%. Because many
of the newer frontends seem to be satellites that likely
proxy traﬃc back to datacenters, it is hard to know
the impact that decreasing the distance from client to
frontend will have on application performance [25].
6.3
Impact on Geolocation Accuracy
A side-eﬀect of Google directing more clients to fron-
tends closer to them is that our geolocation technique
should become more accurate over time, since we base it
on the assumption that frontends are near their clients.
To verify that assumption, we apply our basic geoloca-
tion approach–without any of our ﬁlters that increase
accuracy–to the datasets from three points in time. We
11
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 500 1000 1500 2000 2500 3000 3500 4000CDF of clientsDistance from Client to Estimated Server Location (km)2013-4-142012-10-29 0 0.2 0.4 0.6 0.8 1 0 1000 2000 3000 4000 5000CDF of estimated locationError (km)client-centric geolocation (CCG) 2013-4-14CCG no filtering 2013-4-14CCG no filtering 2013-3-20CCG no filtering 2012-10-29vices have on CDN performance.
It is the ﬁrst work
to study the potential of the EDNS-client-subnet to ad-
dress the client CDN mapping problem, but does not
attempt to map Google’s expansion, as we do.
Finally, several strands of research have explored com-
plementary problems associated with serving infrastruc-
tures, ranging from characterizing and diagnosing la-
tency of CDNs [35, 15] as well as cloud providers [16]
and search services [6], to geolocating ASs using client
locations [27], verifying data replication strategies for
cloud providers [4], analyzing content usage in large
CDNs [5].
8. USING OUR MAPPING
In addition to our evaluation of Google’s serving in-
frastructure so far, our mapping is useful to the research
community, for what it says about clients, and for what
it can predict about other serving infrastructure.
The Need for Longitudinal Research Data. Our
results show the limitations of one-oﬀ measurement studies—
a snapshot of Google’s serving infrastructure in October
would have missed the rapid growth of their infrastruc-
ture and potentially misrepresented their strategy. We
believe the research community needs long-term mea-
surements, and we intend to refresh our maps regu-
larly. We will make our ongoing data available to the
research community, and we plan to expand coverage
from Google to include other providers’ serving infras-
tructures.
Sharing the Wealth: From Our Data to Related
Data.
Our mapping techniques assume the target
sharing infrastructure is pervasive and carefully and
correctly engineered. We assume that (a) Google directs
most clients to nearby frontends; (b) Google’s redirec-
tion is carefully engineered for “eyeball” preﬁxes that
host end-users; and (c) Google will only direct a client
to a satellite frontend if the client is a customer of the
frontend’s AS. Google has economic incentives to en-
sure these assumptions. In practice, these assumptions
are generally true but not always, and our design and