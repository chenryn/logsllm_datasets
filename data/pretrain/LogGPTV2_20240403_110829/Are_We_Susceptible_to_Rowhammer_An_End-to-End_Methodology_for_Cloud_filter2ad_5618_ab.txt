consumption and performance overhead [85].
C. The Rowhammer Threat
Modern deeply-scaled DRAM devices are susceptible to
Rowhammer – a circuit-level disturbance phenomenon in
which repeatedly activating and precharging a given row
accelerates charge leakage in the storage cells of physically
nearby rows [69]. With enough activate/precharge cycles, this
phenomenon causes bit ﬂips.
Prior work extensively studied the statistical characteristics
of Rowhammer-susceptible storage cells [69], [92], [77], [91]
and found that the magnitude of the error rate depends signif-
icantly on the number of activate/precharge cycles. Other fac-
tors affecting the error rates include the particular device under
test, the ambient temperature, and the data pattern written
into the cells. Recent work [115] identiﬁed the precise charge
leakage path responsible for Rowhammer errors and provided
a detailed model that explains experimental observations made
by previous work [92].
Proposals for mitigating and/or preventing Rowhammer
errors abound in both academia [5], [67], [102], [14], [56],
[104], [29], [111], [80], [15], [7], [6], [9], [8], [34], [74], [58],
to Rowhammer bit-ﬂips [81], [33],
[30], [63], [73], [99], [112] and industry [4], [39], [82], [24];
see [89] for a detailed survey of these works. However, while
DRAM manufacturers claim that modern DRAM devices are
resilient
is unclear
what causes this resilience and under what conditions the
Rowhammer-prevention mechanism may fail. Furthermore,
even if a particular DRAM device is supposedly protected,
there is no known way to verify that it is in fact fully resistant
to Rowhammer bit ﬂips.
it
D. Intel-based Cloud Server Architectures
For the cloud market, Intel offers the scalable-performance
variant of Xeon processors (often referred to as Xeon-SP). SP
indicates a server class CPU for multi-socket cloud mother-
boards, introduced with Intel Skylake. Intel Broadwell uses
EP to designate multi-socket server CPUs.
We performed our experiments on the three most recent
generations of Xeon servers: Broadwell-EP, Skylake-SP, and
Cascade Lake-SP. Skylake is a major architectural revision of
Broadwell. Some of our results are affected by these archi-
tectural differences, such as whether the architecture supports
the clﬂushopt instruction. Cascade Lake is a minor revision of
Skylake, and, indeed, our results are similar on both of these
platforms. Intel announced the upcoming release of Ice Lake-
SP, a major architectural revision of Skylake, but these CPUs
are not available at this time.
III. CHALLENGES OF DRAM TESTING
Previous work established a direct relationship between the
number of DRAM row activations within back-to-back refresh
(REF) commands and the number of observed bit ﬂips. A good
example is Figure 2 in a recent paper [89]. This observation
is not new;
it goes back to the original paper showing
DRAM disturbance errors [69]. To test a DRAM row for its
susceptibility to Rowhammer attacks, we repeatedly activate
two adjacent rows co-located in the same bank. Alternating
accesses to two rows ensures that each access ﬁrst precharges
the open row and then activates the row being accessed. We
refer to the rows we activate as aggressor rows, and the tested
row as the victim row.
Naively, to mount Rowhammer, one would like to activate
only a single row repeatedly. Unfortunately, there is no way
to accomplish this in practice on systems using an open-
page policy [65] (the terms page and row are equivalent in
this context). According to this policy, the memory controller
leaves a DRAM row open in the row buffer after access.
Accessing the same row leads to reading repeatedly from
the bank’s row buffer rather than to multiple row activations.
Open-page policy is the default conﬁguration in most systems
today, including the servers used by our cloud provider.
A. Fundamental Testing Requirements
To identify all possible Rowhammer failures when a system
is operational, our testing methodology must replicate the
worst-case Rowhammer testing conditions. We identify two
fundamental testing requirements: (1) The methodology must
activate DRAM rows at the highest possible rate. Repeatedly
activating a row toggles the wordline’s voltage, which causes
disturbance errors. The testing methodology must toggle the
wordline voltage at the fastest (i.e., worst-case) rate possible
3
DRAM BusDRAMControllerRank 0Rank 1Rank N...Chip 0Chip 1Chip N...DRAM BanksDRAM RankDRAM ModuleCPUDRAM ModuleDRAM BankDRAM Cells (Rows/Cols)(a)(b)(c)(d)Row Bufferto ensure the largest number of wordline activations within
a refresh interval. (2) The methodology must test each row
by identifying and toggling physically adjacent rows within
DRAM. Rowhammer attacks are most effective when aggres-
sor rows are physically adjacent to the victim row [69]. Ham-
mering rows without precise knowledge of physical adjacency
is not an effective testing methodology.
B. Challenges of Generating the Highest Rate of ACT Com-
mands
The initial study of DRAM disturbance errors [69] directly
attached DRAM modules to an FPGA board that acts as
the memory controller and can issue arbitrary DRAM com-
mands [38], [103]. The FPGA was programmed to issue ACT
commands at
the optimal rate determined by the DRAM
timing parameter tRC (i.e., minimum row cycle time) in the
JEDEC speciﬁcation sheet [60], [61].
In contrast, testing DRAM on a cloud server is challenging
due to the complexity of modern machines. Instruction se-
quences execute out-of-order, their memory accesses interact
with a complex cache hierarchy designed to capture and
exploit locality, and memory controllers implement complex
DRAM access scheduling algorithms [90], [109]. To com-
prehensively test a cloud server for Rowhammer-vulnerable
DRAM devices, we need to ﬁnd the optimal
instruction
sequence that, when executed, causes the memory controller
to issue ACT commands at the optimal rate (every tRC time
interval).
Previous work on Rowhammer used a variety of different
instruction sequences to mount the attack [69], [101], [32],
[114], [13], [96], [11], [12], [37], [94], [97], [110], [77],
[59], [2], [35], [36], [105], [25], [106], [83], [95], [16], [18],
[10], [118], [22]. It is unclear whether these sequences lead
to different rates of ACT commands, which sequence is the
most effective, and how far from the optimal ACT rate each
sequence is. Most previous work evaluated the effectiveness
of an instruction sequence mounting a Rowhammer attack via
the number of ﬂipped bits metric. Unfortunately, this metric is
inadequate for testing DRAM because it fails to distinguish a
case where memory is safe from the case where the instruction
sequence is ineffective.
C. Challenges of Determining Adjacency of Rows Inside a
DRAM Device
Instruction sequences access memory via virtual addresses.
Virtual addresses are subject to at least three different remap-
pings until mapped to an internal set of cells inside a DRAM
device. Figure 2 shows the three different remapping layers.
1. Virtual-to-Physical: An OS maintains the map of vir-
tual to physical addresses. A virtual address gets translated
into a physical address by the CPU’s Memory Management
Unit (MMU). Virtualized cloud servers have an additional
mapping layer due to virtualization – virtual addresses are
ﬁrst remapped to guest-physical addresses, which are then
remapped to host physical addresses.
2. Physical-to-Logical: A memory controller maintains a
map (or mapping function) of physical addresses to DDR
logical addresses (also called linear memory addresses [105])
and translates incoming physical addresses to DDR logical
addresses. These DDR addresses are speciﬁed in terms of
4
Figure 2: Three remapping layers from virtual address to
DRAM internal address.
channel, DIMM, rank, bank, row, and column. These maps,
seldom public, differ from one CPU architecture to another,
and they are subject to various BIOS settings, such as inter-
leaved memory [70]. On Skylake [42] and Broadwell [41],
different memory controller conﬁgurations (e.g., page poli-
cies) [28] change these maps.
3. Logical-to-Internal: Vendors remap logical addresses in
order to decrease the complexity of internal circuitry and
PCB traces because some maps are electrically easier to build
than others [84], [78], [66]. Remapping also lets vendors take
advantage of the fact that DRAM devices have redundancy
to tolerate a small number of faults per chip; vendors test
for these faulty elements post packaging and remap wordlines
or bitlines to redundant ones elsewhere in the array (i.e., post
package repair) [26], [84]. Memory vendors regard these maps
as trade secrets.
Previous work used a combination of side-channel attacks,
reduced timing parameters, thermal heaters, physical probing,
and Rowhammer attacks to reverse engineer parts of these
maps [100], [94], [78], [64], [105]. Unfortunately, such tech-
niques have shortcomings that prevent our methodology from
using them. They are either too coarse-grained [100], [94],
[78], [64], invasive (i.e., potentially damaging the chips) [64],
inconsistent [100], [94], [105], or they do not capture DRAM
internal addresses [100], [94].
Side-channel attacks are coarse-grained. All memory ac-
cesses to a DRAM bank share one row buffer. Prior work
measured two addresses’ access times to determine whether
they are co-located in the same bank [100], [94]. Sequentially
accessing any two rows within the same bank takes longer than
accessing two rows located in different banks. However, this
method cannot provide ﬁner-grained adjacency information.
The time spent accessing two rows sequentially in the same
bank is unrelated to the rows’ locations within the bank.
Reduced timing parameters is coarse-grained. Another
technique uses the distance from a row to the bank’s row
buffer [78], [68]. This technique induces errors by accessing
memory using shorter-than-normal DDR timing values. Data
stored in a cell closer to the row buffer has a shorter distance
to travel than data stored further away [79], and thus, it has
a lower likelihood to fail. This technique provides coarse-
grained and approximate row adjacency information only.
Adjacent rows have a negligible difference in access times,
and detecting such small differences is challenging.
Using heaters is invasive and coarse-grained. Another
technique surrounds a DIMM with resistive heaters, applies
a thermal gradient on each DRAM device, and conducts a
OS + HypervisorWorkableDRAM InternalsTrade secret DIMM-specificDRAM ControllerUnderspecified docs. Chipset-specificSide-channels Reduced timings Heaters
Physical probing Rowhammer attacks
[100], [94]
[78], [68]
[64]
[100], [94]
[100], [94], [105]
Fine-grained
Non-invasive
Finds Internal DRAM addresses
Consistent
$
"
"
"
$
"
"
"
$
$
"
"
"
"
$
"
"
"
"
$
Table I: Limitations of previous work on reverse engineering row adjacency inside DRAM.
retention error analysis [64]. This approach requires high tem-
peratures, in excess of 99◦C. Cloud providers are reluctant to
adopt a testing methodology that heats up their hardware. Also,
the thermal gradient approach is coarse-grained; it can only
determine neighborhood relations rather than row adjacency.
Physical probing does not capture DRAM internal ad-
dresses. Another approach uses an oscilloscope probe to
capture a DDR electrical signal while issuing memory ac-
cesses [100], [94]. This approach cannot reverse engineer
how DDR logical addresses map to DRAM internal addresses
(Figure 2). Previous work used this technique to reverse
engineer only bank addresses [100], [94]. Reverse engineer-
ing row addresses would incur signiﬁcant additional effort
for two reasons. First, row addresses require 22 individual
probes, whereas bank addresses require only 4 probes. Second,
the signals encoding row addresses change from one DDR4
command to another (Table II). The reverse engineering effort
would need to ensure that the probes capture only the signals
encoding DDR4 row activation, and not other commands.
In contrast, signals encoding bank addresses are shared by
DDR4 row activation, read, write, and precharge commands.
Capturing the signals corresponding to any one of these DDR4
commands reveals the bank address.
Rowhammer attacks are not consistent because they may
not cause failures. Another technique mounts Rowhammer at-
tacks on every row in DRAM and correlates each row’s density
of bit ﬂips with adjacency [100], [94], [105]. Generating a high
rate of activations is enough to cause many bit ﬂips on some
DIMMs, but not on all. This approach is unsuitable for testing
memory resilient to Rowhammer. This is an instance of a
chicken-and-egg problem: (1) testing DRAM for Rowhammer
susceptibility requires knowing the adjacency of rows inside
DRAM devices, and (2) deducing row adjacency requires
ﬂipping bits using Rowhammer attacks.
Table I summarizes the limitations of previous work on
reverse engineering row adjacency inside DRAM.