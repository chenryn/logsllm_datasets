### Classification Performance

Our model achieved an F1-score of 0.88 on the validation set at the document level. Gender classification proved to be the most challenging, with a significant drop in F1-score from 0.93 on the training set to 0.75 on the validation set. However, our gender classifier still achieved comparable document-level accuracy (74.1%) to traditional stylometric methods (76.1% as reported in [4]).

Across all three tasks, the performance at the sentence level was consistently lower than at the document level. This was particularly evident in the gender classification task, where the sentence-level F1-score was only marginally better than random chance (0.52). This is expected because stylistic differences between genders are not always apparent in individual sentences but become more pronounced when aggregated over a document. As a result, the document-level classifier performed much better, achieving an F1-score of 0.75, which is well above the random chance level (0.50). Document-level classification also generalized better, with less variance between training and validation set F1-scores (as shown in Table II). Therefore, we will use the document-level F1-score as our primary metric for evaluating the effectiveness of A4NT networks.

### Quantifying Privacy Gains from A4NT

To evaluate the performance of our A4NT network, we used the attribute classifiers described in the previous section. To obfuscate the authorship of a document, the A4NT network was applied to each sentence, transferring style from the true attribute to the target one. For example, to mask the age of a teenager's written document, we used the A4NT network ZTeen-Adult. The style-transferred document was then input into the attribute classifier for the source attribute, and the F1-score was computed to measure the privacy effectiveness of the style transfer. Additionally, the Meteor score was calculated between the source sentence and the A4NT output to measure semantic similarity.

Table III shows these results across the three settings. On the small speech dataset, all methods, including the autoencoder baseline, successfully fooled the attribute classifier, with F1-scores below the chance level. However, the Meteor scores were significantly lower than those in the blog dataset, indicating substantial semantic loss during anonymization.

On the larger blog dataset, the autoencoder baseline failed to fool the attribute classifier, with only a small drop in F1-score (from 0.88 to 0.85 for age and from 0.75 to 0.61 for gender). Our A4NT models, however, performed much better, dropping the F1-scores below the random chance level. The FBsem model, which uses a semantic encoder loss, achieved the largest privacy gain by decreasing the F1-scores from 0.88 to 0.08 for age and from 0.75 to 0.39 for gender. However, this model had poor Meteor scores, indicating that the sentences after style transfer were no longer semantically similar to the input. The CycML model, which uses reconstruction likelihood to enforce semantic consistency, fared better in terms of Meteor scores while still fooling the classifier. Adding a language smoothing loss (CycML+Lang) further improved the Meteor score in the blog-age setting, though the performance remained similar to CycML in the blog-gender setting and the speech dataset.

### Generalization to Other Classifiers

To assess how well A4NT generalizes to unseen NLP-based adversaries, we trained ten different attribute classifiers on the blog-age setting, varying in architecture (LSTM, CNN, and LSTM+CNN) and hyperparameters. These classifiers achieved good performance in predicting the age attribute. Results from evaluating the text generated by the A4NT networks using these "holdout" classifiers are shown in Table IV. The mean performance of the ten classifiers was 0.85, and the best-performing classifier achieved an F1-score of 0.85. All three A4NT networks generalized well, dropping the document F1-score of the holdout classifiers to the random chance level (0.54 for the blog-age setting). This strong empirical evidence suggests that the transformations applied by the A4NT networks are not specific to the classifier they are trained with but can generalize to other adversaries.

### Comparison to Prior Work

We compared the performance of our A4NT model to prior work on automatic anonymization [26], which proposes to anonymize writing style by computing average statistics on certain text features and applying predefined transformations. We refer to this model as C-Avg and tested it on our age obfuscation task using the official code from [26]. The results are shown in Table V. For a fair comparison, we evaluated the performance against a holdout classifier. The C-Avg model did not perform well, dropping the F1-score only to 0.77 from 0.84, which is well above the random chance level. Our A4NT model, however, dropped the F1-score below the chance level to 0.44 and achieved a higher Meteor score (0.69 vs. 0.55). The poor performance of C-Avg is due to its reliance on hand-designed transformations, which do not generalize well to diverse writing styles found in the blog dataset.

### Different Operating Points

Our A4NT model offers the ability to obtain multiple style-transfer outputs by sampling from the model's distribution. This is useful as different text samples may have varying levels of semantic similarity and privacy effectiveness. Users can choose the trade-off between semantic similarity and privacy. Figure 6 illustrates this, showing five samples obtained from each A4NT model for each sentence in the test set. By selecting the sentence with the minimum, maximum, or random Meteor score relative to the input text, users can achieve the desired balance. The CycML+LangLoss model provides a wide range of operating points, all achieving a Meteor score above 0.5, indicating good semantic preservation.

### Human Judgments for Semantic Consistency

Automatic semantic similarity evaluation metrics like Meteor are only reliable to a certain extent. Therefore, we conducted user studies to judge the semantic similarity preserved by our A4NT networks. The evaluations were conducted on a subset of 745 random sentences from the test split of the blog-age dataset. If any model generated identical sentences to the input, it was ranked first automatically without human evaluation. Cases without identical sentences were evaluated using human annotators on Amazon Mechanical Turk (AMT). Annotators were shown one input sentence and multiple style-transfer outputs and asked to pick the output sentence closest in meaning to the input. Three unique annotators were shown each test sample, and majority voting was used to determine the top-ranked model. Cases with no majority were excluded.

The main goal of the study was to identify which of the three A4NT networks performed best in terms of semantic similarity according to human judges. We also compared the best of our three systems to the baseline model based on Google machine translation, discussed in Section 5.3. For the machine translation baseline, we obtained style-transferred texts from four different language round-trips. None of the Google machine translation baselines were able to fool the attribute classifiers, with the best (lowest) F1-score of 0.81, only slightly less than the input data F1-score of 0.9. This model also achieved a significantly worse Meteor score than any of our A4NT models.