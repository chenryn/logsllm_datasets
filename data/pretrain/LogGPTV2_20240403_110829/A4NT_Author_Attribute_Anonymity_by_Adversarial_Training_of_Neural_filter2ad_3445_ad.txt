classiﬁcation, achieving F1-score of 0.88 on the valida-
tion set at the document-level. Gender classiﬁcation turns
out to be the hardest to generalize, with a signiﬁcant drop
in F1-score on the validation set compared to the training
set (down to 0.75 from 0.93). However, we note that our
gender classiﬁer achieves similar document-level classiﬁ-
cation accuracies to the traditional stylometric methods
(76.1% accuracy reported in [4], compared to 74.1% ac-
curacy achieved by our model).
In all three tasks, the
performance on sentence-level is worse than on document-
level classiﬁcation. This is especially true in the gender
classiﬁcation task, where the sentence-level classiﬁca-
tion does only marginally better than the random chance
(0.52 F1-score). This is expected since the stylistic differ-
ences between authors of different genders is not visible
in strong in all the sentences. However there is usually
a few sentences within a document where these stylistic
differences are strong. Thus when we aggregate the clas-
siﬁer scores over a document, the classiﬁer does much
better and is well above the random chance (0.75 F1-score
vs 0.50 chance-level). Document-level classiﬁcation also
generalizes better with less difference between training
and validation set F1-scores in Table II. Henceforth, we
will use document-level F1-score as our primary metric
when evaluating the effectiveness of A4NT networks.
6.1.1 Quantifying Privacy Gains from A4NT
We evaluate the performance of our A4NT network
using the attribute classiﬁers presented in the previous
section. To obfuscate the authorship of a document, the
A4NT network is run on every sentence in a document
to transfer style from the true attribute to the target one.
For example, to mask age of a teenager written document
we use the A4NT network ZTeen−Adult. Style-transferred
document is input to the attribute classiﬁer of the source
attribute and F1-score of this classiﬁer is computed. This
measures the privacy effectiveness of the style transfer.
Meteor score is computed between the source sentence
and the A4NT output, to measure the semantic similarity.
Table III shows these results in the three settings. On
the small speech dataset all methods, including the au-
toencoder baseline described in Section 5.3, successfully
fool the attribute classiﬁer. They all obtain F1-scores
below the chance-level, with our A4NT networks doing
better. However the meteor scores of all models is signiﬁ-
cantly lower than in the blog dataset, indicating signiﬁcant
amount of semantic loss in the process of anonymization.
On the larger blog dataset, the autoencoder baseline
fails to fool the attribute classiﬁer, with only a small
drop in F1-score of 0.03 (from 0.88 to 0.85) in case of
age and 0.14 in case of gender (from 0.75 to 0.61) Our
A4NT models however do much better, with all of them
being able to drop the F1-score below the random chance.
The FBsem model using semantic encoder loss
achieves the largest privacy gain, by decreasing the F1-
scores from 0.88 to 0.08 in case of age and from 0.75 to
0.39 in case of gender. This model however suffers from
poor meteor scores, indicating the sentences produced
after the style transfer are no longer similar to the input.
The model using reconstruction likelihood to enforce
semantic consistency, CycML, fares much better in meteor
metric in both age and gender style transfer. It is still
able to fool the classiﬁer, albeit with smaller drops in F1-
scores (still below random chance). Finally, with addition
of the language smoothing loss (CycML+Lang), we see
a further improvement in the meteor score in the blog-
age setting, while the performance remains similar to
CycML on blog-gender setting and the speech dataset.
However, the language smoothing model CycML+Lang
fares better in the user study discussed in Section 6.1.2
and also produces better qualitative samples as will be
seen in Section 6.2.
Generalization to other classiﬁers: An important ques-
tion to answer if A4NT is to be applied to protect the pri-
vacy of author attributes, is how well it performs against
unseen NLP based adversaries ? To test this we trained
ten different attribute classiﬁers networks on the blog-age
setting. These networks vary in architectures (LSTM,
CNN and LSTM+CNN) and hyper-parameters (number
of layers and number of units), but all of them achieve
good performance in predicting the age attribute. The
networks were chosen to reﬂect real-world architecture
choices used for text classiﬁcation. Results from evaluat-
ing the text generated by the A4NT networks using these
“holdout” classiﬁers are shown in Table IV. The column
“mean” shows the mean performance of the ten classiﬁers
and “max” shows the score of best performing classiﬁer
1642    27th USENIX Security Symposium
USENIX Association
Model
Random classiﬁer
Original text
Autoencoder
A4NT FBsem
A4NT CycML
A4NT CycML+Lang
Blog-age data
Blog-gender data
Speech dataset
Sent F1 Doc F1 Meteor
-
1.0
0.77
0.54
0.88
0.85
0.54
0.74
0.69
Sent F1 Doc F1 Meteor
-
1.0
0.78
0.53
0.52
0.49
0.5
0.75
0.61
Sent F1 Doc F1 Meteor
-
1.0
0.32
0.61
1.00
0.12
0.60
0.68
0.26
0.43
0.49
0.54
0.08
0.20
0.32
0.40
0.57
0.69
0.45
0.41
0.44
0.39
0.44
0.39
0.53
0.79
0.79
0.12
0.11
0.12
0.00
0.00
0.00
0.28
0.29
0.29
Table III: Performance of the style transfer anonymization in fooling the classiﬁers, across the three settings. F1 (lower
is better) and Meteor (higher is better). F1-scores below chance levels are shown in italics.
Holdout classiﬁers have good performance on the orig-
inal text, achieving mean 0.85 document-level F1-score.
Table IV shows that all three A4NT networks generalize
well and are able to drop the document F1-score of the
holdout classiﬁers to the random chance level (0.54 for the
blog-age setting). They perform slightly worse than on
the seen LSTM classiﬁer, but are able to signiﬁcantly drop
the performance of all the holdout classiﬁers (mean F1
score drops from 0.85 to 0.53 or below). This is a strong
empirical evidence that the transformations applied by the
A4NT networks are not speciﬁc to the classiﬁer they are
trained with, but can also generalize to other adversaries.
We conclude that the proposed A4NT networks are
able to fool the attribute classiﬁers on all three tested
tasks and also show generalization ability to fool classiﬁer
architectures not seen during training.
Note that here we have considered dropping the clas-
siﬁer score below the random chance-level on average
as the criteria for fooling the classiﬁers and protecting
privacy as motivated in Section 5.2. However, in some sce-
narios it is desirable for the obfuscation model to fool the
classiﬁers all the time and pushing adversary to achiev-
ing 0% classiﬁcation accuracy. This guarantee would
provide the user the freedom to use the obfuscation only
when desired. Our A4NT model is not able to reach this
level of 100% obfuscation. It remains an important open
problem for the future work to achieve 100% obfuscation
while preserving semantics, so that the user remains in
full control.
Comparison to prior work: We also compare the per-
formance of our model to a prior work on automatic
anonymization [26]. This work proposes to anonymize
the writing style by computing average statistics on cer-
tain text features and applying pre-deﬁned transforma-
tions to change the input text statistics towards the average.
We refer to this model as C-Avg and test this model on
our age obfuscation task using the ofﬁcial code from [26].
The results are shown in Table V. For fair comparison we
compare the performance against a holdout classiﬁer, not
seen by our model as well. We can see that the C-Avg
model [26] does not perform well on the age obfusca-
tion task, managing to drop the F1-score only to 0.77
from 0.84, which is well above the random chance-level.
Our A4NT model however drops the F1-score below the
chance-level to 0.44. Our model does better in semantic
similarity as well, achieving meteor score of 0.69 com-
pared to 0.55 obtained by C-Avg. The poor performance
of C-Avg model [26] on the age obfuscation task is due
to the fact that C-Avg relies on hand designed transfor-
mations (eg. substituting synonyms from a dictionary)
which does not generalize well to the diverse writing
styles found in the blog dataset. This highlights the ad-
vantage of the proposed approach to learn to perform
obfuscation directly from the data.
Different operating points : Our A4NT model offers the
ability to obtain multiple different style-transfer outputs
by simply sampling from the models distribution. This
is useful as different text samples might have different
levels of semantic similarity and privacy effectiveness.
Having multiple samples allows users to choose the level
of semantic similarity vs privacy trade-off they prefer.
We illustrate this in Figure 6. Here ﬁve samples are
obtained from each A4NT model for each sentence in the
test set. By choosing the sentence with minimum, max-
imum or random meteor scores w.r.t the input text, we
can obtain a trade-off between semantic similarity and
privacy. We see that while the FBsem model offers lim-
ited variability, CycML+LangLoss offers a wide range
of choices of operating points. All operating points of
CycML+LangLoss achieve better meteor score than 0.5,
which indicates this model preserves the semantic simi-
larity well.
6.1.2 Human Judgments for Semantic Consistency
In machine translation and image captioning literature,
it is well known that automatic semantic similarity eval-
uation metrics like meteor are only reliable to a certain
extent. Evaluation from human judges is still the gold-
standard with which models can be reliably compared.
Accordingly, we conduct user studies to judge the se-
USENIX Association
27th USENIX Security Symposium    1643
Model
Original text
Autoencoder
A4NT FBsem
A4NT CycML
A4NT CycML+Lang
Seen
Classiﬁer
F1-score
0.88
0.85
Holdout Classiﬁers
Mean F1 Max F1
0.87
0.84
0.85
0.83
0.08
0.20
0.32
0.19
0.41
0.53
0.31
0.58
0.62
Table IV: Evaluating the A4NT anonymization against
previously unseen (holdout) classiﬁers, on blogdata (age).
Document-level F1 score is used.
Model
Original text
C-Avg [26]
Ours
Holdout Classiﬁer
Doc F1-score
0.84
0.77
0.44
Meteor
1.0
0.55
0.69
Table V: Comparison of our A4NT model to prior work
on automatic anonymization. We compare both privacy
effectiveness against a classiﬁer and semantic consistency
(meteor metric).
mantic similarity preserved by our A4NT networks. The
evaluations were conducted on a subset of 745 random
sentences from the test split of the blog-age dataset. First,
output from different A4NT models is obtained for the
745 test sentences. If any model generates identical sen-
tences to the input, this model is ranked ﬁrst automatically
without human evaluation. Note that, in some cases, mul-
tiple models can achieve rank-1, when they all produce
identical outputs. The cases without any identical sen-
tences to the input are evaluated using human annotators
on Amazon Mechanical Turk (AMT). An annotator is
shown one input sentence and multiple style-transfer out-
puts and is asked to pick the output sentence which is
closest in meaning to the input sentence. Three unique an-
Figure 6: Operating points of A4NT models on test set.
notators are shown each test sample and majority voting
is used to determine the model which ranks ﬁrst. Cases
with no majority from human evaluators are excluded.
The main goal of the study is to identify which of the
three A4NT networks performs best in terms of semantic
similarity according to human judges. We also compare
the best of our three systems to the baseline model based
on Google machine translation, discussed in Section 5.3.
For the machine translation baseline, we obtain style-
transferred texts from four different language round-trips.
We started with English→German→French→English,
and obtained three more versions with incrementally
adding Spanish, Finnish and ﬁnally Armenian languages
into the chain before the translation back to English.
To pick the operating points for the user study, we com-
pare the performance of these four machine translation
baselines and our three models on the human-evaluation
test set in Figure 7. Note that here we show sentence-level
F1 score on the y-axis as the human-evaluation test set
is too small for document-level evaluation. We see that
none of the Google machine translation baselines are able
to fool the attribute classiﬁers. The model with 5-hop
translation achieves best (lowest) F1-score of 0.81 which
is only slightly less than the input data F1-score of 0.9.
This model also achieves signiﬁcantly worse meteor score
than any of our A4NT models.