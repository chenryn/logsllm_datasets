### Maximum Segment Size (MSS)
The Maximum Segment Size (MSS) is a parameter used in the Transmission Control Protocol (TCP) to specify the maximum amount of data that can be included in a single IP datagram. The MSS is typically calculated as the outgoing interface's Maximum Transmission Unit (MTU) minus 40 bytes, which accounts for the IP and TCP headers.

### Capacity Verification for High-Speed Network Intrusion Detection Systems

#### 5. Example Test Results
Building on the lessons from Section 4, this section presents two simple tests to evaluate specific aspects of a Network Intrusion Detection System (NIDS) capacity. Additional tests are suggested to further refine the knowledge gained from these initial tests.

##### 5.1 Test Network
Figure 1 illustrates the test network configuration used for the example tests. All connections to the switch are 1-Gbps full-duplex links.

**Figure 1.** A block diagram showing the network layout for the example tests. The WebReflector acts as all the web servers, while the WebAvalanche acts as all the web clients. The Catalyst 6509 switch is used to copy-capture the traffic to the NIDS.

```
WebReflector
|
|--- Catalyst 6509 Switch --- NIDS
|
WebAvalanche
```

##### 5.2 Baseline Test
The first test establishes a baseline for the capture efficiency of a NIDS in a pure HTTP environment. The NIDS is configured with all default signatures enabled, but the generated traffic does not trigger alarms or events. The number of client hosts is fixed at 5080, and the number of servers is fixed at two. TCP sessions are allowed to run to completion as quickly as possible, with the number of simultaneous open sessions kept below 30. The average packet size is varied by manipulating the HTTP transaction size, which also affects the average number of packets per TCP connection, the packets per second (KPPS), and the overall bandwidth used. The final variable manipulated is the number of new TCP connections per second. Each test runs for three minutes before capacity measurements are taken. The results of the baseline test are shown in Table 4.

**Table 4.** Results for the baseline test. Traffic was HTTP only with 5080 client IP addresses and 2 server IP addresses. The test runs for 3 minutes with no server delay.

| Avg. Packet Size | Bandwidth @ 1000 cps | Capture Efficiency @ 1000 cps | Bandwidth @ 2500 cps | Capture Efficiency @ 2500 cps | Bandwidth @ 5000 cps | Capture Efficiency @ 5000 cps |
|------------------|-----------------------|-------------------------------|-----------------------|-------------------------------|-----------------------|-------------------------------|
| 434              | 50 Mbps               | 100%                          | 125 Mbps              | 100%                          | 230 Mbps              | 99.95%                        |
| 482              | 67 Mbps               | 100%                          | 167 Mbps              | 100%                          | 335 Mbps              | 97.10%                        |
| 542              | 93 Mbps               | 100%                          | 232 Mbps              | 100%                          | 446 Mbps              | 99.30%                        |
| 645              | 180 Mbps              | 100%                          | 446 Mbps              | 100%                          | 680 Mbps              | 95.01%                        |
| 688              | 275 Mbps              | 100%                          | 440 Mbps              | 100%                          | 655 Mbps              | 95.01%                        |
| 702              | 380 Mbps              | 100%                          | 446 Mbps              | 99.95%                        | 680 Mbps              | 95.01%                        |
| 720              | 444 Mbps              | 100%                          | 446 Mbps              | 99.95%                        | 680 Mbps              | 95.01%                        |

##### 5.3 Adding Simultaneous Open TCP Sessions
The second test introduces simultaneous open TCP sessions by adding a four-second delay to the server response, causing sessions to remain open. All other test variables remain constant. The bandwidth, packets per second, and average packet size are somewhat affected by the open connections. Without a correlation study, it is unclear if these factors are statistically significant. For this paper, we assume they are not. The results from the open connection test are shown in Table 5.

**Table 5.** Results for the open connection test. Traffic was HTTP only with 5080 client IP addresses and 2 server IP addresses. The test runs for 3 minutes with a four-second forced server delay.

| Avg. Packet Size | Bandwidth @ 1000 cps | Capture Efficiency @ 1000 cps | Bandwidth @ 2500 cps | Capture Efficiency @ 2500 cps | Bandwidth @ 5000 cps | Capture Efficiency @ 5000 cps |
|------------------|-----------------------|-------------------------------|-----------------------|-------------------------------|-----------------------|-------------------------------|
| 434              | 50 Mbps               | 100%                          | 125 Mbps              | 100%                          | 245 Mbps              | 68.75%                        |
| 482              | 69 Mbps               | 100%                          | 170 Mbps              | 100%                          | 350 Mbps              | 72.26%                        |
| 542              | 93 Mbps               | 100%                          | 241 Mbps              | 100%                          | 350 Mbps              | 72.26%                        |
| 645              | 181 Mbps              | 100%                          | 446 Mbps              | 99.95%                        | 350 Mbps              | 72.26%                        |
| 688              | 268 Mbps              | 100%                          | 446 Mbps              | 99.95%                        | 350 Mbps              | 72.26%                        |
| 702              | 355 Mbps              | 100%                          | 446 Mbps              | 99.95%                        | 350 Mbps              | 72.26%                        |
| 720              | 446 Mbps              | 99.95%                        | 446 Mbps              | 99.95%                        | 350 Mbps              | 72.26%                        |

##### 5.4 The Results
The most significant variations in capture efficiency occur in the 5000 connections per second tests. Capture efficiency is also affected in the 2500 connections per second tests, though this may not be statistically meaningful. The two tests differ only in the number of concurrent open TCP sessions, suggesting that the state database is under stress. It is not possible to precisely identify the specific operation within the database causing the drop in capacity. However, it is clear that a consumer whose network has an average of 10,000 open TCP sessions, a rate of 2500 new TCP connections per second, and a bandwidth consumption of less than approximately 400 Mbps, can deploy the tested NIDS with confidence in near-100% capture efficiency.

##### 5.5 Further Tests
The example tests do not provide enough information for a full confidence decision. However, using the same methodology, the NIDS industry or independent labs could develop a suite of tests to provide quantifiable results for different stress points. For instance, the total number of database insertions per second can be quantified by running a test at a very low rate for other stress points. The traffic must be crafted to require the database to maintain state on many different key values. One method is to use a packet generator to create valid TCP sessions with a full three-way handshake, ramping up the connection rate until the NIDS starts dropping traffic. Since these TCP connections consist of small packets, the bandwidth should remain low. Results need to be cross-checked with the raw packet capture architecture evaluation to ensure that it is the database inserts, not the packets per second limit, that have been reached.

### 6. Conclusion
As the NIDS industry matures, standardized testing will become a reality. Developing these tests can follow the same concepts of standardized testing found in other industries. The information in this paper can serve as a catalyst to stimulate the development of standardized tests, providing consumers with the missing information. The same techniques used for capacity testing can be extended to other performance areas, such as false positive ratios.

### References
1. Mier Communications: Test report for ManHunt from Recourse Inc. and test report for Intrusion.com’s NIDS. At: http://www.mier.com/reports/vendor.html
2. Ranum, M.: Experiences Benchmarking Intrusion Detection Systems. At: http://www.nfr.com/forum/white-papers/Benchmarking-IDS-NFR.pdf
3. Claffy, K., Miller, G., Thompson, K.: The nature of the beast: recent traffic measurements from an Internet backbone. At: http://www.caida.org/outreach/papers/1998/Inet98/ (1998)
4. McCreary, S., Claffy, K.: Trends in Wide Area IP Traffic Patterns: A View from Ames Internet Exchange. At: http://www.caida.org/outreach/papers/2000/AIX0005/ (2000)

### Performance Adaptation in Real-Time Intrusion Detection Systems
**Authors:**
- Wenke Lee, Jo˜ao B.D. Cabrera, Ashley Thomas, Niranjan Balwalli, Sunmeet Saluja, and Yi Zhang
- Affiliations: College of Computing, Georgia Institute of Technology; Scientific Systems Company Inc.; Department of Electrical and Computer Engineering, North Carolina State University

**Abstract:**
A real-time intrusion detection system (IDS) has several performance objectives: good detection coverage, economy in resource usage, resilience to stress, and resistance to attacks. These objectives are trade-offs that must be considered in IDS design, implementation, deployment, and adaptation. We show that IDS performance trade-offs can be studied as classical optimization problems. We describe an IDS architecture with multiple dynamically configured front-end and back-end detection modules and a monitor. The IDS run-time performance is measured periodically, and detection strategies and workload are configured among the detection modules according to resource constraints and cost-benefit analysis. The back-end performs scenario (or trend) analysis to recognize ongoing attack sequences, allowing proactive and optimal configuration of the IDS.

**Keywords:**
Real-time intrusion detection, performance metrics, performance adaptation, optimization.

**1. Introduction**
Intrusion detection is a critical component of defense-in-depth network security. An IDS monitors operating system or network activities by capturing and analyzing audit data to detect attacks. Most systems perform misuse detection by pattern matching known attack behavior, and some employ anomaly detection techniques. A real-time IDS aims to detect and respond to attacks in real-time. Performance objectives include good detection coverage, resource efficiency, and resilience to stress. Since adversaries may try to evade or subvert IDSs, another objective is resistance to attacks. These objectives can conflict. For example, broad coverage and high detection accuracy require stateful analysis, which is resource-intensive and vulnerable to stress and overload attacks. Careful consideration of trade-offs is necessary.

It is well known that current IDSs, employing only misuse detection, cannot detect new attacks and may fail under stress or targeted attacks. Researchers are developing attack resistance techniques, such as stateful analysis and network traffic normalizers. Some IDSs are designed to be lightweight or use high-end hardware to handle high-speed and high-volume traffic. However, as long as an IDS is statically configured, an intelligent adversary can overload it, leading to missed attacks.