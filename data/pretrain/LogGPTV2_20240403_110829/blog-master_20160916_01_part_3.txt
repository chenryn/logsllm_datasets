Rack: /dc1/rack1
Decommission Status : Normal
Configured Capacity: 201312493568 (187.49 GB)
DFS Used: 65536 (64 KB)
Non DFS Used: 180682752 (172.31 MB)
DFS Remaining: 201131745280 (187.32 GB)
DFS Used%: 0.00%
DFS Remaining%: 99.91%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 2
Last contact: Fri Sep 16 08:39:18 CST 2016
Name: xxx.xxx.xxx.104:50010 (host_digoal_03)
Hostname: host_digoal_03
Rack: /dc1/rack3
Decommission Status : Normal
Configured Capacity: 201312493568 (187.49 GB)
DFS Used: 65536 (64 KB)
Non DFS Used: 159514624 (152.13 MB)
DFS Remaining: 201152913408 (187.34 GB)
DFS Used%: 0.00%
DFS Remaining%: 99.92%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 2
Last contact: Fri Sep 16 08:39:18 CST 2016
```
## 测试hdfs
```
[gpadmin@host_digoal_01 ~]$ hdfs dfs -mkdir -p /user
[gpadmin@host_digoal_01 ~]$ hdfs dfs -put /home/gpadmin/hadoop-2.7.3.tar.gz /user/
[gpadmin@host_digoal_01 ~]$ hdfs dfs -ls -R /
drwxr-xr-x   - gpadmin supergroup          0 2016-09-16 08:41 /user
-rw-r--r--   2 gpadmin supergroup  214092195 2016-09-16 08:41 /user/hadoop-2.7.3.tar.gz
```
## 公网访问dfs开放端口(WEB)
```
[gpadmin@host_digoal_01 ~]$ netstat -anp|grep LISTEN
(Not all processes could be identified, non-owned process info
 will not be shown, you would have to be root to see it all.)
tcp        0      0 127.0.0.1:57777         0.0.0.0:*               LISTEN      10893/java          
tcp        0      0 xxx.xxx.xxx.97:8020       0.0.0.0:*               LISTEN      10161/java          
tcp        0      0 xxx.xxx.xxx.97:50070      0.0.0.0:*               LISTEN      10161/java          
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      -                   
tcp        0      0 0.0.0.0:50010           0.0.0.0:*               LISTEN      10893/java          
tcp        0      0 0.0.0.0:50075           0.0.0.0:*               LISTEN      10893/java          
tcp        0      0 0.0.0.0:50020           0.0.0.0:*               LISTEN      10893/java          
```
如果要在外部访问，可以使用balance或IPTABLES跳转  
https://www.inlab.de/balance.html  
或使用nginx代理  
balance配置  
```
sudo 
wget https://www.inlab.de/balance-3.57.tar.gz
tar -zxvf balance-3.57.tar.gz
cd balance-3.57
make && make install
```
端口映射  
```
# balance 19998 xxx.xxx.xxx.97:50070
```
view http://xx公x.xx网x.xxIPx.xx地址x:19998  
![pic1](20160916_01_pic_001.png)  
## 如何关闭dfs集群
如果配置了ssh无秘钥认证，并且配置了slave file(host file)，则可以直接关闭  
```
stop-dfs.sh
```
## 允许gpadmin访问hdfs namenode
```
$ /home/gpadmin/app/hadoop-2.7.3/bin/hdfs dfs -chown gpadmin hdfs://xxx.xxx.xxx.97:8020/
```
## 配置yarn
如果需要使用yarn来做资源管理的话，需要安装YARN  
YARN is only needed when you want to use YARN as the global resource manager  
### 配置yarn环境变量  
```
$ cd ~/app/hadoop-2.7.3
$ vi etc/hadoop/yarn-env.sh
export JAVA_HOME=/home/gpadmin/app/jdk1.8.0_102
```
### 配置mapred-site.xml
```
$ cp etc/hadoop/mapred-site.xml.template etc/hadoop/mapred-site.xml
$ vi etc/hadoop/mapred-site.xml
        mapreduce.framework.name
        yarn
        mapreduce.jobhistory.address
        xxx.xxx.xxx.97:10020
        mapreduce.jobhistory.webapp.address
        xxx.xxx.xxx.97:19888
```
### 配置yarn-site.xml
```
$ vi etc/hadoop/yarn-site.xml
        yarn.nodemanager.aux-services
        mapreduce_shuffle
        yarn.nodemanager.aux-services.mapreduce.shuffle.class
        org.apache.hadoop.mapred.ShuffleHandler
        yarn.resourcemanager.address
        xxx.xxx.xxx.97:8032
        yarn.resourcemanager.scheduler.address
        xxx.xxx.xxx.97:8030
        yarn.resourcemanager.resource-tracker.address
        xxx.xxx.xxx.97:8031
        yarn.resourcemanager.admin.address
        xxx.xxx.xxx.97:8033
        yarn.resourcemanager.webapp.address
        xxx.xxx.xxx.97:8088
```
### 启动yarn  
```
$ start-yarn.sh
[gpadmin@host_digoal_01 hadoop-2.7.3]$ start-yarn.sh
starting yarn daemons
starting resourcemanager, logging to /home/gpadmin/app/hadoop-2.7.3/logs/yarn-gpadmin-resourcemanager-host_digoal_01.out
host_digoal_03: starting nodemanager, logging to /home/gpadmin/app/hadoop-2.7.3/logs/yarn-gpadmin-nodemanager-host_digoal_03.out
host_digoal_02: starting nodemanager, logging to /home/gpadmin/app/hadoop-2.7.3/logs/yarn-gpadmin-nodemanager-host_digoal_02.out
host_digoal_01: starting nodemanager, logging to /home/gpadmin/app/hadoop-2.7.3/logs/yarn-gpadmin-nodemanager-host_digoal_01.out
```
## 查看Hadoop各主机进程
```
[gpadmin@host_digoal_01 hadoop-2.7.3]$ jps
14880 DataNode
16306 NodeManager
16626 Jps
14147 NameNode
16194 ResourceManager
[gpadmin@host_digoal_02 hadoop-2.7.3]$ jps 
5421 NodeManager
5002 SecondaryNameNode
4828 DataNode
5550 Jps
[gpadmin@host_digoal_03 hadoop-2.7.3]$ jps
1390 DataNode
2088 Jps
1933 NodeManager
```
## 查看Hadoop WEB管理页面  
```
# balance 1999 xxx.xxx.xxx.97:8088
```
view http://xx公x.xx网x.xxIPx.xx地址x:1999  
![pic2](20160916_01_pic_002.png)  
## hadoop 其他
本文未涉及的内容，下次的文章中再详细介绍  
1\. 监控  
2\. 安全认证  
3\. HA  
4\. 优化  
## hadoop部署参考  
http://www.aboutyun.com/thread-7684-1-1.html  
http://www.aboutyun.com/thread-8146-1-1.html  
http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/core-default.xml  
http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml  
http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/ClusterSetup.html  
## install hawq
### 克隆git  
```
cd ~
git clone https://git-wip-us.apache.org/repos/asf/incubator-hawq.git
```
### 安装依赖包  
```
CODE_BASE=`pwd`/incubator-hawq
cd ~
wget ftp://ftp.gnu.org/gnu/gsasl/libgsasl-1.8.0.tar.gz
tar -zxvf libgsasl-1.8.0.tar.gz
cd libgsasl-1.8.0
./configure --prefix=/home/gpadmin/app/sasl
make -j 32 
make install
cd ~
git clone https://github.com/google/googletest
cd googletest
mkdir build
cd build
cmake -DCMAKE_INSTALL_PREFIX:PATH=/home/gpadmin/app/google ..
make -j 32
make install
cd ~
cd $CODE_BASE/depends/libhdfs3
mkdir build
cd build
../bootstrap --prefix=/home/gpadmin/app/libhdfs3 --dependency=/home/gpadmin/app/sasl:/home/gpadmin/app/google
make -j 32
make install
```
### 安装libyarn
```
cd ~
cd $CODE_BASE/depends/libyarn
mkdir build
cd build
../bootstrap --prefix=/home/gpadmin/app/libyarn --dependency=/home/gpadmin/app/sasl:/home/gpadmin/app/google
make -j 32
make install
```
### 编译hawq
```
cd ~
cd $CODE_BASE
CPPFLAGS="-I/home/gpadmin/app/libyarn/include -I/home/gpadmin/app/hadoop-2.7.3/include -I/home/gpadmin/app/libhdfs3/include" LDFLAGS="-L/home/gpadmin/app/libyarn/lib -L/home/gpadmin/app/hadoop-2.7.3/lib -L/home/gpadmin/app/libhdfs3/lib" ./configure --prefix=/home/gpadmin/app/hawq --with-python --with-r --with-pgcrypto --with-openssl --enable-debug --enable-orca --without-libyarn --without-libhdfs3 --without-thrift
# Run command to build and install
# To build concurrently , run make with -j option. For example, make -j8
# On Linux system without large memory, you will probably encounter errors like
# "Error occurred during initialization of VM" and/or "Could not reserve enough space for object heap"
# and/or "out of memory", try to set vm.overcommit_memory = 1 temporarily, and/or avoid "-j" build,
# and/or add more memory and then rebuild.
# On mac os, you will probably see this error: "'openssl/ssl.h' file not found".
# "brew link openssl --force" should be able to solve the issue.
make -j 32
# Install HAWQ
make install
```
## 配置hawq环境变量
```
echo ". /home/gpadmin/app/hawq/greenplum_path.sh" >> /home/gpadmin/.bash_profile
echo "export HADOOP_HOME=/home/gpadmin/app/hadoop-2.7.3" >> /home/gpadmin/.bash_profile
echo "export LD_LIBRARY_PATH=/home/gpadmin/app/libyarn/lib:/home/gpadmin/app/libhdfs3/lib:/home/gpadmin/app/google/lib:/home/gpadmin/app/sasl/lib:\$LD_LIBRARY_PATH" >> /home/gpadmin/.bash_profile
echo "export PGHOST=127.0.0.1" >> /home/gpadmin/.bash_profile
echo "export PGPORT=1921" >> /home/gpadmin/.bash_profile
echo "export PGDATABASE=postgres" >> /home/gpadmin/.bash_profile
echo "export PGUSER=gpadmin" >> /home/gpadmin/.bash_profile
echo "export PGPASSWORD=gpadmin" >> /home/gpadmin/.bash_profile
echo "export PGDATA=/data01/gpadmin/pgdata_master" >> /home/gpadmin/.bash_profile
. /home/gpadmin/.bash_profile
```
## 配置greenplum_path.sh  
(否则可能通过remote_ssh调用pg_ctl时报fgets failed的错误)    
```
echo "export PATH=/home/gpadmin/app/hadoop-2.7.3/bin:/home/gpadmin/app/hadoop-2.7.3/sbin:/home/gpadmin/app/cmake/bin:\$PATH" >> /home/gpadmin/app/hawq/greenplum_path.sh
echo "export JAVA_HOME=/home/gpadmin/app/jdk1.8.0_102" >> /home/gpadmin/app/hawq/greenplum_path.sh
echo "export HADOOP_HOME=/home/gpadmin/app/hadoop-2.7.3" >> /home/gpadmin/app/hawq/greenplum_path.sh
echo "export LD_LIBRARY_PATH=/home/gpadmin/app/libyarn/lib:/home/gpadmin/app/libhdfs3/lib:/home/gpadmin/app/google/lib:/home/gpadmin/app/sasl/lib:\$LD_LIBRARY_PATH" >> /home/gpadmin/app/hawq/greenplum_path.sh
echo "export PGHOST=127.0.0.1" >> /home/gpadmin/app/hawq/greenplum_path.sh
echo "export PGPORT=1921" >> /home/gpadmin/app/hawq/greenplum_path.sh
echo "export PGDATABASE=postgres" >> /home/gpadmin/app/hawq/greenplum_path.sh
echo "export PGUSER=gpadmin" >> /home/gpadmin/app/hawq/greenplum_path.sh
echo "export PGPASSWORD=gpadmin" >> /home/gpadmin/app/hawq/greenplum_path.sh
echo "export PGDATA=/data01/gpadmin/pgdata_master" >> /home/gpadmin/app/hawq/greenplum_path.sh
```
## 配置初始化数据库集群目录
```
$ mkdir -p /data01/gpadmin/pgdata_master
$ mkdir -p /data01/gpadmin/pgdata_segment
$ mkdir -p /data01/gpadmin/pgdata_master_tmp
$ mkdir -p /data01/gpadmin/pgdata_segment_tmp
$ mkdir -p /data02/gpadmin/pgdata_master_tmp
$ mkdir -p /data02/gpadmin/pgdata_segment_tmp
```
## 配置hawq配置文件hawq-site.xml
(hawq-site.xml的优先级配置比postgresql.conf更高)  