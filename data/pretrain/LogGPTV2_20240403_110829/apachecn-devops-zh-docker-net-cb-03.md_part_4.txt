testbridge2
```
`network create`之前的语句定义了具有以下特征的网络:
*   用户定义的网络类型`bridge`
*   `192.168.50.0/24`的一个`subnet`
*   一个`gateway`或者`192.168.50.1`的桥接 IP 接口
*   `192.168.50.128/25`的容器网络范围
*   主机上的 IP 伪装已关闭
*   一个名叫`testbridge2`的 Docker 网络
如示例 1 所述，如果我们要创建桥接网络，我们不需要定义驱动程序类型。此外，如果我们认为网关是容器定义的子网中的第一个可用 IP，我们也可以将其从定义中排除。创建后检查此网络应该会向我们显示类似如下的结果:
```
user@docker1:~$ docker network inspect testbridge2
[
    {
 "Name": "testbridge2",
        "Id": "2c8270425b14dab74300d8769f84813363a9ff15e6ed700fa55d7d2c3b3c1504",
        "Scope": "local",
 "Driver": "bridge",
        "EnableIPv6": false,
        "IPAM": {
            "Driver": "default",
            "Options": {},
            "Config": [
                {
 "Subnet": "192.168.50.0/24",
 "IPRange": "192.168.50.128/25"
                }
            ]
        },
        "Internal": false,
        "Containers": {},
        "Options": {
            "com.docker.network.bridge.enable_ip_masquearde": "false"
        },
        "Labels": {}
    }
]
user@docker1:~$
```
# 创建用户定义的覆盖网络
虽然创建自己的桥的能力确实很有吸引力，但你的范围仍然仅限于一个 Docker 主机。覆盖网络`Driver`旨在通过允许您使用覆盖网络跨多个 Docker 主机扩展一个或多个子网来解决这个问题。覆盖网络是一种在现有网络上构建隔离网络的方法。在这种情况下，现有网络为覆盖层提供传输，通常被命名为**底层网络**。覆盖层`Driver`实现了 Docker 所说的多主机联网。
在本食谱中，我们将学习如何配置覆盖`Driver`的先决条件，以及部署和验证基于覆盖的网络。
## 做好准备
在以下示例中，我们将使用此实验拓扑:
![Getting ready](img/B05453_03_03.jpg)
拓扑由总共四个 Docker 主机组成，其中两个位于`10.10.10.0/24`子网和子网，另外两个位于`192.168.50.0/24`子网。当我们浏览这个配方时，图表中显示的主机将扮演以下角色:
*   `docker1`:Docker 主持人服务于领事**键值存储**
*   `docker2` : Docker 主机参与覆盖网络
*   `docker3` : Docker 主机参与覆盖网络
*   `docker4` : Docker 主机参与覆盖网络
如前所述，默认情况下不会实例化覆盖图`Driver`。这是因为叠加`Driver`工作需要几个先决条件。
### 键值存储
由于我们现在处理的是分布式系统，Docker 需要一个地方来存储关于覆盖网络的信息。为此，Docker 使用键值存储，并为此目的支持 Consul、etcd 和 ZooKeeper。它将存储需要跨所有节点保持一致性的信息，如 IP 地址分配、网络标识和容器端点。在我们的示例中，我们将部署领事。
幸运的是，Consul 本身可以部署为 Docker 容器:
```
user@docker1:~$ docker run -d -p 8500:8500 -h consul \
--name consul progrium/consul -server -bootstrap
```
运行此映像将启动 Consul 键值存储的单个实例。我们只需要一个实例就可以进行基本的实验室测试。在我们的例子中，我们将在主机`docker1`上开始这个映像。所有参与覆盖的 Docker 主机必须能够通过网络访问键值存储。
### 注
只有一个集群成员运行 Consul 只能用于演示目的。您至少需要三个集群成员才能具有任何类型的容错能力。确保研究您决定部署的键值存储，并了解其配置和故障容限。
### 3.16 的 Linux 内核版本
你的 Linux 内核版本需要是 3.16 或者更高。您可以使用以下命令检查当前的内核版本:
```
user@docker1:~$ uname -r
4.2.0-34-generic
user@docker1:~$ 
```
### 开放港口
Docker 主机必须能够使用以下端口相互通信:
*   TCP 和 UDP`7946`(self)
*   UDP(交换机)
*   TCP `8500`(咨询键值存储)
### Docker 服务配置选项
所有参与覆盖的主机都需要访问键值存储。为了告诉他们它在哪里，我们定义了几个服务级别选项:
```
ExecStart=/usr/bin/dockerd --cluster-store=consul://10.10.10.101:8500/network --cluster-advertise=eth0:0
```
集群存储变量定义了键值存储的位置。在我们的例子中，它是一个运行在主机`docker1` ( `10.10.10.101`)上的容器。我们还需要启用`cluster-advertise`功能，并为其传递一个接口和端口。这种配置更多地与使用 Swarm 集群有关，但该标志也用作启用多主机网络的一部分。也就是说，您需要向它传递一个有效的接口和端口。在这种情况下，我们使用主机物理接口和端口`0`。在我们的示例中，我们将这些选项添加到主机`docker2`、`docker3`和`docker4`中，因为这些是我们将参与覆盖网络的主机。
添加选项后，重新加载`systemd`配置并重新启动 Docker 服务。您可以通过检查`docker info`命令的输出来验证 Docker 是否接受了该命令:
```
user@docker2:~$ docker info
……
Cluster store: consul://10.10.10.101:8500/network
Cluster advertise: 10.10.10.102:0
……
```
## 怎么做…
现在我们已经满足了使用覆盖`Driver`的先决条件，我们可以部署我们的第一个用户定义的覆盖网络。定义用户定义的覆盖网络的过程与定义用户定义的桥接网络的过程非常相似。例如，让我们使用以下命令配置我们的第一个覆盖网络:
```
user@docker2:~$ docker network create -d overlay myoverlay
e4bdaa0d6f3afe1ae007a07fe6a1f49f1f963a5ddc8247e716b2bd218352b90e
user@docker2:~$
```
很像用户定义的桥，我们不需要输入很多信息来创建我们的第一个覆盖网络。事实上，这里唯一的区别是我们必须将`Driver`指定为类型叠加，因为默认的`Driver`类型是桥接。输入命令后，我们应该能够看到参与覆盖网络的任何节点上定义的网络。
```
user@docker3:~$ docker network ls
NETWORK ID          NAME                DRIVER              SCOPE
55f86ddf18d5        bridge              bridge              local
8faef9d2a7cc        host                host                local
3ad850433ed9        myoverlay           overlay             global
453ad78e11fe        none                null                local
user@docker3:~$
user@docker4:~$ docker network ls
NETWORK ID          NAME                DRIVER              SCOPE
3afd680b6ce1        bridge              bridge              local
a92fe912af1d        host                host                local
3ad850433ed9        myoverlay           overlay             global
7dbc77e5f782        none                null                local
user@docker4:~$
```
主机`docker2`在创建网络时将网络配置推送到存储中。现在所有主机都可以看到新网络，因为它们都在同一个键值存储中读写数据。创建网络后，参与覆盖的任何节点(配置了正确的服务级别选项)都可以查看、连接容器和删除覆盖网络。
例如，如果我们转到主机`docker4`，我们可以删除我们最初在主机`docker2`上创建的网络:
```
user@docker4:~$ docker network rm myoverlay
myoverlay
user@docker4:~$ docker network ls
NETWORK ID          NAME                DRIVER              SCOPE
3afd680b6ce1        bridge              bridge              local
a92fe912af1d        host                host                local
7dbc77e5f782        none                null                local
user@docker4:~$
```
现在让我们用更多的配置来定义一个新的覆盖。与用户定义的桥不同，覆盖层`Driver`当前不支持在创建过程中使用`--opt`标志传递给它的任何附加选项。也就是说，我们可以在覆盖型网络上配置的唯一选项是`network create`子命令的一部分。
*   `aux-address`:与用户定义的桥 一样，该命令允许您定义 Docker 在容器产生时不应分配给容器的 IP 地址。
*   `gateway`:虽然你可以为网络定义一个网关，如果你没有，Docker 会为你做，但这实际上并没有用在覆盖网络中。也就是说，该 IP 地址没有分配到的接口。
*   `internal`:该选项允许您隔离网络，本章稍后将详细介绍。
*   `ip-range`:允许指定已定义网络子网中较小的子网用于容器寻址。
*   `ipam-driver`:除了消费第三方网络驱动，还可以利用第三方 IPAM 驱动。出于本书的目的，我们将主要关注默认或内置的 IPAM 驱动程序。
*   `ipam-opt`:这个可以让你指定传递给 IPAM 司机的选项。
*   `subnet`:此定义了与您正在创建的网络类型相关联的子网。
让我们重新定义主机`docker4`上的网络`myoverlay`:
```
user@docker4:~$ docker network create -d overlay \
--subnet 172.16.16.0/24  --aux-address ip2=172.16.16.2 \
--ip-range=172.16.16.128/25 myoverlay
```
在本例中，我们使用以下属性定义网络:
*   `172.16.16.0/24`的一个`subnet`
*   `172.16.16.2`的保留或辅助地址(回想一下，尽管实际上没有使用，Docker 还是会分配一个网关 IP 作为子网中的第一个 IP。在这种情况下，这意味着`.1`和`.2`在这一点上是技术保留的。)
*   `172.16.16.128/25`的容器可分配 IP 范围
*   `myoverlay`的一个名字
像以前一样，这个网络现在可以在参与覆盖配置的所有三台主机上使用。现在让我们从主机`docker2`定义覆盖网络上的第一个容器:
```
user@docker2:~$ docker run --net=myoverlay --name web1 \
-d -P jonlangemak/web_server_1
3d767d2d2bda91300827f444aa6c4a0762a95ce36a26537aac7770395b5ff673
user@docker2:~$
```
在这里，我们要求主机启动一个名为`web1`的容器，并将其连接到网络`myoverlay`。现在让我们检查容器的 IP 接口配置:
```
user@docker2:~$ docker exec web1 ip addr
1: lo:  mtu 65536 qdisc noqueue state UNKNOWN
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
7: eth0@if8:  mtu 1450 qdisc noqueue state UP
    link/ether 02:42:ac:10:10:81 brd ff:ff:ff:ff:ff:ff
    inet 172.16.16.129/24 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::42:acff:fe10:1081/64 scope link
       valid_lft forever preferred_lft forever
10: eth1@if11:  mtu 1500 qdisc noqueue state UP
    link/ether 02:42:ac:12:00:02 brd ff:ff:ff:ff:ff:ff
    inet 172.18.0.2/16 scope global eth1