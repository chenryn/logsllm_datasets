the switch state.5 To suppress this, we use dampening: af-
ter a switch has accepted a ﬂow, it can only accept other
paused ﬂows after a given small period of time, as shown in
Algorithm 1.
Suppressed Probing: One could let a paused sender send
one probe per RTT. However, this can introduce signiﬁcant
bandwidth overhead because of the small RTTs in data cen-
ter networks. For example, assume a 1-Gbps network where
ﬂows have an RTT of 150 µs. A paused ﬂow that sends a
40-byte probe packet per RTT consumes 40 Byte
2.13% of the total bandwidth. The problem becomes more
severe with larger numbers of concurrent ﬂows.
150 µs /1 Gbps ≈
To address this, we propose a simple concept, called Sup-
pressed Probing to reduce the probing frequency. We make
an observation that only the paused ﬂows that are about to
start have a need to send probes frequently. Therefore, it
is desirable to control probing frequency based on the ﬂow
criticality and the network load. To control probing fre-
quency, one would need to estimate ﬂow waiting time (i.e.,
how long does it take until the ﬂow can start sending). Al-
though it is considered hard to predict future traﬃc work-
loads in data centers, switches can easily estimate a lower
bound of the ﬂow waiting time by checking their ﬂow list.
Assuming each ﬂow requires at least X RTTs to ﬁnish, a
PDQ switch estimates that a ﬂow’s waiting time is at least
5We later show that PDQ can quickly converge to the equi-
librium state when the workload is stable (§4).
X × max∀(cid:96){Index((cid:96))} RTTs, where Index((cid:96)) is the ﬂow in-
dex in the list on link (cid:96). The switch sets the inter-probing
time ﬁeld (I H ) to max{I H , X × Index((cid:96))} in the scheduling
header to control the sender probing rate (I S), as shown
in Algorithm 3. The expected per-RTT probing overhead
is signiﬁcantly reduced from O(n) (n ﬂows, each of which
sends one probe per RTT) to 1
k=1 1/k = O(log n). In
X
our current implementation, we conservatively set X to 0.2
RTTavg.
(cid:80)n
Algorithm 1: PDQ Receiving Data Packet
if P H = other switch then
Remove the ﬂow from the list if it is in the list;
return;
if the ﬂow is not in the list then
if the list is not full or the ﬂow criticality is higher
than the least critical ﬂow in the list then
Add the ﬂow into the list with rate Ri = 0.
Remove the least critical ﬂow from the list
whenever the list has more than κ ﬂows.
Set RH to RCP fair share rate;
if RH = 0 then P H = myID;
return;
else
Let i be the ﬂow index in the list; Update the ﬂow
information:  = ;
if W =min(Availbw(i),RH )>0 then
if the ﬂow is not sending (P i (cid:54)= ø), and the switch
just accepted another non-sending ﬂow then
else P H = ø; RH =W ; // Accept it
P H = myID; P i = myID; // Pause it
else P H = myID; P i = myID; // Pause it
Algorithm 2: Availbw(j)
X=0; A=0;
for (i = 0; i 100 KByte) in data center net-
works [3]. The ﬂow deadline is drawn from an exponential
distribution with mean 20 ms, as suggested by [20]. How-
ever, some ﬂows could have tiny deadlines that are unrealis-
tic in real network applications. To address this, we impose
a lower bound on deadlines, and we set it to 3 ms in our
experiments. We use Application Throughput, the percent-
age of ﬂows that meet their deadlines, as the performance
metric of deadline-constrained ﬂows.
Deadline-unconstrained Flows are ﬂows that have no
speciﬁc deadlines, but it is desirable that they ﬁnish early.
For example, Dryad jobs that move ﬁle partitions across ma-
chines. Similarly, we assume the ﬂow size is drawn uniformly
from an interval with a mean of 100/1000 KByte. We use
the average ﬂow completion time as the performance metric.
We have developed our own event-driven packet-level sim-
ulator written in C++. The simulator models the following
schemes:
PDQ: We consider diﬀerent variants of PDQ. We use PDQ(Full)
to refer to the complete version of PDQ, including Early
Start (ES), Early Termination (ET) and Suppressed Prob-
ing (SP). Likewise, we refer to the partial version of PDQ
which excludes the above three algorithms as PDQ(Basic).
To better understand the performance contribution of each
algorithm, we further extend PDQ(Basic) to PDQ(ES) and
PDQ(ES+ET).
D3: We implemented a complete version of D3 [20], includ-
ing the rate request processing procedure, the rate adapta-
tion algorithm (with the suggested parameters α = 0.1 and
β = 1), and the quenching algorithm. In the original algo-
rithm when the total demand exceeds the switch capacity,
the fair share rate becomes negative. We found this can
cause a ﬂow to return the allocated bandwidth it already
reserved, resulting in unnecessarily missed deadlines. There-
fore, we add a constraint to enforce the fair share bandwidth
f s to always be non-negative, which improves D3’s perfor-
mance.
RCP: We implement RCP [10] and optimize it by counting
the exact number of ﬂows at switches. We found this im-
proves the performance by converging to the fair share rate
more quickly, signiﬁcantly reducing the number of packet
drops when encountering a sudden large inﬂux of new ﬂows [9].
132(a)
(b)
(b) a single-bottleneck topology:
Figure 2: Example topologies: (a) a 17-node single-rooted
tree topology;
sending
servers associated with diﬀerent ﬂows are connected via a
single switch to the same receiving server. Both topologies
use 1 Gbps links, a switch buﬀer of 4 MByte, and FIFO tail-
drop queues. Per-hop transmission/propagation/processing
delay is set to 11/0.1/25 µs.
This is exactly equivalent to D3 when ﬂows have no dead-
lines.
TCP: We implement TCP Reno and optimize it by setting
a small RT Omin to alleviate the TCP Incast problem, as
suggested by previous studies [3, 19].
Unless otherwise stated, we use single-rooted tree, a com-
monly used data center topology for evaluating transport
protocols [3, 19, 20, 22].
In particular, our default topol-
ogy is a two-level 12-server single-rooted tree topology with
1 Gbps link rate (Figure 2a), the same as used in D3. We
vary the traﬃc workload and topology in §5.3 and §5.5.
5.2 Query Aggregation
In this section, we consider a scenario called query aggre-
gation: a number of senders initiate ﬂows at the same time
to the same receiver (the aggregator). This is a very com-
mon application scenario in data center networks and has
been adopted by a number of previous works [22, 20, 3].
We evaluate the protocols in both the deadline-constrained
case (§5.2.1) and the deadline-unconstrained case (§5.2.2).
5.2.1 Deadline-constrained Flows
Impact of Number of Flows: We start by varying the
number of ﬂows.6 To understand bounds on performance,
we also simulate an optimal solution, where an omniscient
scheduler can control the transmission of any ﬂow with no
delay.
It ﬁrst sorts the ﬂows by EDF, and then uses a
dynamic programming algorithm to discard the minimum
number of ﬂows that cannot meet their deadlines (Algorithm
3.3.1 in [16]). We observe that PDQ has near-optimal appli-
cation throughput across a wide range of loads (Figure 3a).
Figure 3a demonstrates that Early Start is very eﬀec-
tive for short ﬂows. By contrast, PDQ(Basic) has much
lower application throughput, especially during heavy sys-
tem load because of the long down time between ﬂow switch-
ing. Early Termination further improves performance by
discarding ﬂows that cannot meet their deadline. Moreover,
Figure 3a demonstrates that, as the number of concurrent
ﬂows increases, the application throughput of D3, RCP and
TCP decreases signiﬁcantly.
Impact of Flow Size: We ﬁx the number of concurrent
ﬂows at 3 and study the impact of increased ﬂow size on the
application throughput. Figure 3b shows that as the ﬂow
size increases, the performance of deadline-agnostic schemes
(TCP and RCP) degrades considerably, while PDQ remains
6We randomly assign f ﬂows to n senders while ensuring