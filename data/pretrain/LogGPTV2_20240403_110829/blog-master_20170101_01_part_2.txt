Unfortunately, this meant that every scan needed to take a heavyweight  
lock on that bucket also, which was bad for concurrency.  Instead, use  
a cleanup lock on the primary bucket page to indicate the right to  
begin a split, so that scans only need to retain a pin on that page,  
which is they would have to acquire anyway, and which is also much  
cheaper.  
In addition to reducing the locking cost, this also avoids locking out  
scans and inserts for the entire lifetime of the split: while the new  
bucket is being populated with copies of the appropriate tuples from  
the old bucket, scans and inserts can happen in parallel.  There are  
minor concurrency improvements for vacuum operations as well, though  
the situation there is still far from ideal.  
This patch also removes the unworldly assumption that a split will  
never be interrupted.  With the new code, a split is done in a series  
of small steps and the system can pick up where it left off if it is  
interrupted prior to completion.  While this patch does not itself add  
write-ahead logging for hash indexes, it is clearly a necessary first  
step, since one of the things that could interrupt a split is the  
removal of electrical power from the machine performing it.  
Amit Kapila.  I wrote the original design on which this patch is  
based, and did a good bit of work on the comments and README through  
multiple rounds of review, but all of the code is Amit's.  Also  
reviewed by Jesper Pedersen, Jeff Janes, and others.  
Discussion: http://postgr.es/m/CAA4eK1LfzcZYxLoXS874Ad0+PI:EMAIL  
```  
https://git.postgresql.org/gitweb/?p=postgresql.git;a=commit;h=f0e72a25b05d4c29d0102fa0b892782ff193a00e  
```  
Improve handling of dead tuples in hash indexes.  
When squeezing a bucket during vacuum, it's not necessary to retain  
any tuples already marked as dead, so ignore them when deciding which  
tuples must be moved in order to empty a bucket page.  Similarly, when  
splitting a bucket, relocating dead tuples to the new bucket is a  
waste of effort; instead, just ignore them.  
Amit Kapila, reviewed by me.  Testing help provided by Ashutosh  
Sharma.  
```  
## 9. 支持进程级条件变量，简化sleep\wakeup设计  
https://git.postgresql.org/gitweb/?p=postgresql.git;a=commit;h=e8ac886c24776295dd9b025386a821061da8e4d1  
```  
Support condition variables.  
Condition variables provide a flexible way to sleep until a  
cooperating process causes an arbitrary condition to become true.  In  
simple cases, this can be accomplished with a WaitLatch/ResetLatch  
loop; the cooperating process can call SetLatch after performing work  
that might cause the condition to be satisfied, and the waiting  
process can recheck the condition each time.  However, if the process  
performing the work doesn't have an easy way to identify which  
processes might be waiting, this doesn't work, because it can't  
identify which latches to set.  Condition variables solve that problem  
by internally maintaining a list of waiters; a process that may have  
caused some waiter's condition to be satisfied must "signal" or  
"broadcast" on the condition variable.  
Robert Haas and Thomas Munro  
```  
## 10. 支持聚合运算下推至sharding节点(postgres_fdw增强)  
https://git.postgresql.org/gitweb/?p=postgresql.git;a=commit;h=7012b132d07c2b4ea15b0b3cb1ea9f3278801d98  
```  
postgres_fdw: Push down aggregates to remote servers.  
Now that the upper planner uses paths, and now that we have proper hooks  
to inject paths into the upper planning process, it's possible for  
foreign data wrappers to arrange to push aggregates to the remote side  
instead of fetching all of the rows and aggregating them locally.  This  
figures to be a massive win for performance, so teach postgres_fdw to  
do it.  
Jeevan Chalke and Ashutosh Bapat.  Reviewed by Ashutosh Bapat with  
additional testing by Prabhat Sahu.  Various mostly cosmetic changes  
by me.  
```  
### Push down more full joins in postgres_fdw  
https://commitfest.postgresql.org/12/727/  
## 11. 支持流式备份时，同时备份数据文件与REDO文件  
https://git.postgresql.org/gitweb/?p=postgresql.git;a=commit;h=56c7d8d4552180fd66fe48423bb2a9bb767c2d87  
```  
Allow pg_basebackup to stream transaction log in tar mode  
This will write the received transaction log into a file called  
pg_wal.tar(.gz) next to the other tarfiles instead of writing it to  
base.tar. When using fetch mode, the transaction log is still written to  
base.tar like before, and when used against a pre-10 server, the file  
is named pg_xlog.tar.  
To do this, implement a new concept of a "walmethod", which is  
responsible for writing the WAL. Two implementations exist, one that  
writes to a plain directory (which is also used by pg_receivexlog) and  
one that writes to a tar file with optional compression.  
Reviewed by Michael Paquier  
```  
## 12. 分布式事务  
https://commitfest.postgresql.org/12/853/  
## 13. Twophase transactions on slave, take 2  
https://commitfest.postgresql.org/12/915/  
## 14. Scan key push down to heap  
https://commitfest.postgresql.org/12/850/  
## 15. 间接索引  
indirect indexes  
https://commitfest.postgresql.org/12/874/  
## 16. Logical Replication  
https://commitfest.postgresql.org/12/836/  
实际上9.4开始就已经支持了，通过外部插件+逻辑流复制来实现，但是没有整合到内核中  
## 值得期待的10.0，一起等风来 - 2017金秋  
## 参考  
1\. https://commitfest.postgresql.org/  
2\. http://git.postgresql.org/  
3\. https://git.postgresql.org/gitweb/?p=postgresql.git;a=summary  
#### [PostgreSQL 许愿链接](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216")
您的愿望将传达给PG kernel hacker、数据库厂商等, 帮助提高数据库产品质量和功能, 说不定下一个PG版本就有您提出的功能点. 针对非常好的提议，奖励限量版PG文化衫、纪念品、贴纸、PG热门书籍等，奖品丰富，快来许愿。[开不开森](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216").  
#### [9.9元购买3个月阿里云RDS PostgreSQL实例](https://www.aliyun.com/database/postgresqlactivity "57258f76c37864c6e6d23383d05714ea")
#### [PostgreSQL 解决方案集合](https://yq.aliyun.com/topic/118 "40cff096e9ed7122c512b35d8561d9c8")
#### [德哥 / digoal's github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
#### [PolarDB 学习图谱: 训练营、培训认证、在线互动实验、解决方案、生态合作、写心得拿奖品](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
#### [购买PolarDB云服务折扣活动进行中, 55元起](https://www.aliyun.com/activity/new/polardb-yunparter?userCode=bsb3t4al "e0495c413bedacabb75ff1e880be465a")
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")