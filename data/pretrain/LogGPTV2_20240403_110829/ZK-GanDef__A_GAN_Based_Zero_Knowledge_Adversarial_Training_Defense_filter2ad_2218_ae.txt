examples from the three datasets.
We summarize our ﬁndings from the results as: (i) ZK-
GanDef is signiﬁcantly better than existing zero knowledge
defenses (CLP and CLS) due to its higher test accuracy
on adversarial examples and its scalability to large datasets.
This clearly supports our vision that utilizing a more ﬂexible
and sophisticated way to handle the pre-softmax logits (ZK-
GanDef) is better than forcing the pre-softmax logits to be
smooth at a small scale (CLP and CLS). (ii) The test accuracy
of ZK-GanDef is comparable to that of the state-of-the-art
full knowledge adversarial training defenses. This supports
our hypothesis that using perturbation invariant features in the
classiﬁer could greatly enhance test accuracy on adversarial
examples. (iii) On contrast with full knowledge defenses, ZK-
GanDef is adaptable to new types of adversarial examples.
We see from the results that FGSM-Adv has signiﬁcant
adaptability issue on MNIST and Fashion-MNIST datasets.
This issue is not observed on CIFAR10 due to the input
dropout in classiﬁer structure [25]. For PGD-Adv, the current
evaluation does not show its adaptability issue, but it is not
guaranteed given that stronger adversarial examples could be
generated in the future [25] [22]. On the other hand, the results
show that ZK-GanDef has better adaptability to new types of
adversarial examples because its training is independent of
such examples.
B. Generalizability
In the previous evaluation, all iterative adversarial exam-
ples are generated by methods based on projected gradient
descent. In order to show the generalizability of ZK-GanDef,
we conduct
the evaluation on an extra set of adversarial
examples, Deepfool [16] and Carlini & Wagner (CW) ex-
amples [4]. Unlike adversarial examples used in previous
evaluation, Deepfool and CW adversarial examples contain
10
Fig. 5: Training Time and Training Loss The left sub-ﬁgure is training time on MNIST and Fashion-MNIST. The middle
sub-ﬁgure is training time on CIFAR10. The right sub-ﬁgure is the training loss of CLS under different hyper-parameters.
MNIST
Deepfool
98.72%
CW
98.46%
Fashion-MNIST
CW
Deepfool
89.52%
66.43%
CIFAR10
Deepfool
86.08%
CW
47.22%
TABLE IV: Test Accuracy on Deepfool and CW Examples
perturbation patterns that are signiﬁcantly different from Gaus-
sian perturbation. Therefore, this evaluation could reveal the
generalizability of ZK-GanDef in defending other adversarial
examples. The evaluation is conducted on all three datasets.
The Deepfool and CW adversarial examples utilize the same
hyper-parameter setting as PGD adversarial examples.
The evaluation results are summarized in Table IV. It is clear
that ZK-GanDef can classify Deepfool adversarial examples
with over 85% accuracy in all three datasets which matches
the test error presented in [16]. The reason is that Deepfool
tries to ﬁnd adversarial examples with smaller perturbation
than projected gradient descent based adversarial examples
(FGSM, BIM, PGD). As a result, Deepfool examples are easier
to defend. For CW examples, ZK-GanDef achieves the same
level of test accuracy on all three datasets. To conclude, ZK-
GanDef is not limited to defend a speciﬁc type of perturbation.
Although ZK-GanDef only utilizes Gaussian noise perturba-
tion during training, its defense can be generalized to a wide
range of adversarial examples which include FGSM, BIM,
PGD, Deepfool and CW examples.
C. Training Time
We evaluate here the training time of ZK-GanDef in terms
of seconds per training epoch. MNIST and Fashion-MNIST
share the same image size and classiﬁer structure and hence
has the same training time. Since the test accuracy of ZK-
GanDef is signiﬁcantly higher than those of the existing
zero knowledge defenses, CLP and CLS, we only compare
the training time of ZK-GanDef with those of full knowl-
edge defenses (FGSM-Adv, PGD-Adv and PGD-GanDef). We
utilize a ﬁxed number of training epochs (80 for MNIST
and 300 for CIFAR10) and results show that all defensive
methods converge at epoch 30 on MNIST and at epoch 240
on CIFAR10. Since the records of training time per epoch have
a very small deviation, we take the average value of records
in all epochs and compare different defense methods with it.
11
The results are recorded during the training on a workstation
with a NVIDIA GTX 1080 GPU.
The left sub-ﬁgure of Figure 5 shows that the training time
of ZK-GanDef on MNIST/Fashion-MNIST (8.75s) is close
to that of FGSM-Adv (7.83s), while it surges to 110.85s
and 132.75s in the case of PGD-Adv and PGD-GanDef,
respectively. The evaluation results on CIFAR10 dataset (the
middle sub-ﬁgure of Figure 5) follow a similar trend to that
of the results on MNIST and Fashion-MNIST datasets. ZK-
GanDef and FGSM-Adv take much less training time per
epoch (71.20s and 62.85s, respectively) compared to that
of PGD-Adv (146.91s) and that of PGD-GanDef (257.72s).
For example, on CIFAR10 dataset, the end-to-end training
time of PGD-Adv takes 14.3 hours, while training of ZK-
GanDef only takes 6.9 hours. In summary, ZK-GanDef pro-
vides test accuracy close to that of the best state-of-the art full
knowledge defesnses (PGD-Adv), while reducing the training
time by 92.11% and 51.53% on MNIST/Fashion-MNIST and
CIFAR10, respectively.
D. Convergence Issue
As presented earlier, the evaluation results of CLP and CLS
on CIFAR10 dataset show that these two zero knowledge
adversarial training defenses fail to correctly classify both
original and adversarial examples. This is mainly because
the training loss of CLP and CLS does not converge during
training. The mathematical models of CLP and CLS (section
III) follow the same design logic that aims at preventing over
conﬁdent predictions. CLP achieves its goal by adding l2
norm penalty on the difference of two randomly selected pre-
softmax logits, while CLS adds l2 norm penalty on any pre-
softmax logits. Moreover, CLP and CLS do not include orig-
inal examples in their training dataset, which means that they
miss important features that can help discriminate examples
with and without perturbations. Therefore, this design logic is
too simple and lacks ﬂexibility compared with ZK-GanDef,
which utilizes minimax game with discriminator and trains on
examples with and without perturbations. When training on
complex datasets like CIFAR10, the simple and less ﬂexible
design logic leads to convergence issues for CLP and CLS.
To further validate this conclusion, we record the loss of
CLS during the ﬁrst 30 training epochs and depict the results
on the right sub-ﬁgure of Figure 5. The training loss is
recorded on four different hyper-parameter settings of CLS:
(1) normal CLS (σ = 1.0, λ = 0.4), (2) CLS with reduced
perturbations (σ = 1.0, λ = 0.01), (3) CLS with reduced
penalty (σ = 0.1, λ = 0.4), and (4) CLS with reduced
perturbation and penalty (σ = 0.1, λ = 0.01). The ﬁgure
shows that the curves of the ﬁrst three settings overlap with
each other and form the horizontal curve on the top. This
clearly shows that CLS does not learn any useful features and
hence the training loss does not converge (does not decrease)
under these three settings. Under the last setting, CLS was able
to learn useful features and hence the training loss decreases
towards convergence. However, with the last setting, CLS
falls back to Vanilla classiﬁer, which fails to defend against
adversarial examples. A similar experiment is also conducted
with CLP and the results follow the same pattern. The only
difference is that the training loss goes to “nan” on CLP under
the ﬁrst three settings, which means that the classiﬁer diverges
during training.
VI. CONCLUSION
In this paper, we introduce a new zero knowledge adversar-
ial training defense, ZK-GanDef, which combines adversarial
training and feature learning to better recognize and identify
adversarial examples. We evaluate the test accuracy and the
training overhead of ZK-GanDef against state-of-the-art zero
knowledge adversarial training defenses (CLP and CLS) as
well as full knowledge adversarial training defenses (FGSM-
Adv and PGD-Adv). The results show that ZK-GanDef en-
hances the test accuracy on original and adversarial examples
by up to 49.17% compared to zero knowledge defenses.
More importantly, ZK-GanDef has close test accuracy to
full knowledge defenses (test accuracy degeneration is below
8.46%), while taking much less training time (more than
51.53% on training time reduction). Additionally, in contrast
to full knowledge defenses, ZK-GanDaf can adapt to new
types of adversarial examples because its training is adversarial
example agnostic.
VII. FUTURE WORK
In the future, we want to continue our research on designing
defensive methods which provide defense against different
single-step and iterative adversarial examples while consume
less computation during the training.
REFERENCES
[1] S. Abu-Nimeh, D. Nappa, X. Wang, and S. Nair, “A comparison of
machine learning techniques for phishing detection,” in Proceedings
of the anti-phishing working groups 2nd annual eCrime researchers
summit. ACM, 2007, pp. 60–69.
[2] A. Athalye, N. Carlini, and D. Wagner, “Obfuscated gradients give a
false sense of security: Circumventing defenses to adversarial examples,”
arXiv preprint arXiv:1802.00420, 2018.
[3] R. Benenson, “Classiﬁcation datasets results,” 2018, [Online; accessed
06-April-2018]. [Online]. Available: http://rodrigob.github.io/are we
there yet/build/classiﬁcation datasets results.html
[4] N. Carlini and D. Wagner, “Towards evaluating the robustness of neural
networks,” pp. 39–57, 2017.
[5] I. Goodfellow, Y. Bengio, A. Courville, and Y. Bengio, Deep learning.
MIT press Cambridge, 2016, vol. 1.
[6] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and har-
nessing adversarial examples,” International Conference on Learning
Representations, 2015.
[7] H. Kannan, A. Kurakin, and I. Goodfellow, “Adversarial logit pairing,”
arXiv preprint arXiv:1803.06373, 2018.
[8] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
International Conference on Learning Representations, 2015.
[9] A. Kurakin, I. Goodfellow, and S. Bengio, “Adversarial machine learning
at scale,” International Conference on Learning Representations, 2017.
[10] A. Lamb, J. Binas, A. Goyal, D. Serdyuk, S. Subramanian, I. Mitliagkas,
and Y. Bengio, “Fortiﬁed networks: Improving the robustness of deep
networks by modeling the manifold of hidden representations,” arXiv
preprint arXiv:1804.02485, 2018.
[11] G. Lample, N. Zeghidour, N. Usunier, A. Bordes, L. Denoyer
et al., “Fader networks: Manipulating images by sliding attributes,” in
Advances in Neural Information Processing Systems, 2017, pp. 5969–
5978.
[12] B. Liang, H. Li, M. Su, X. Li, W. Shi, and X. Wang, “Detecting
adversarial examples in deep networks with adaptive noise reduction,”
arXiv preprint arXiv:1705.08378, 2017.
[13] G. Louppe, M. Kagan, and K. Cranmer, “Learning to pivot with
adversarial networks,” in Advances in Neural Information Processing
Systems, 2017, pp. 982–991.
[14] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards
deep learning models resistant to adversarial attacks,” arXiv preprint
arXiv:1706.06083, 2017.
[15] D. Meng and H. Chen, “Magnet: a two-pronged defense against adver-
sarial examples,” pp. 135–147, 2017.
[16] S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard, “Deepfool: a simple
and accurate method to fool deep neural networks,” in Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, 2016,
pp. 2574–2582.
[17] N. Papernot, F. Faghri, N. Carlini, I. Goodfellow, R. Feinman, A. Ku-
rakin, C. Xie, Y. Sharma, T. Brown, A. Roy, A. Matyasko, V. Behzadan,
K. Hambardzumyan, Z. Zhang, Y.-L. Juang, Z. Li, R. Sheatsley, A. Garg,
J. Uesato, W. Gierke, Y. Dong, D. Berthelot, P. Hendricks, J. Rauber,
and R. Long, “Technical report on the cleverhans v2.1.0 adversarial
examples library,” arXiv preprint arXiv:1610.00768, 2018.
[18] N. Papernot and P. McDaniel, “Extending defensive distillation,” arXiv
preprint arXiv:1705.05264, 2017.
[19] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami, “Distillation
as a defense to adversarial perturbations against deep neural networks,”
in Security and Privacy (SP), 2016 IEEE Symposium on.
IEEE, 2016,
pp. 582–597.
[20] A. Rajabi, M. Abbasi, C. Gagn´e, and R. B. Bobba, “Towards dependable
deep convolutional neural networks (cnns) with out-distribution learn-
ing,” arXiv preprint arXiv:1804.08794, 2018.
[21] H. A. Rowley, S. Baluja, and T. Kanade, “Neural network-based face de-
tection,” IEEE Transactions on pattern analysis and machine intelligence,
vol. 20, no. 1, pp. 23–38, 1998.
[22] P. Samangouei, M. Kabkab, and R. Chellappa, “Defense-gan: Protecting
classiﬁers against adversarial attacks using generative models,” arXiv
preprint arXiv:1805.06605, 2018.
[23] J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller, “Striving
for simplicity: The all convolutional net,” International Conference on
Learning Representations, 2017.
[24] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow,
and R. Fergus, “Intriguing properties of neural networks,” International
Conference on Learning Representations, 2014.
[25] F. Tram`er, A. Kurakin, N. Papernot, I. Goodfellow, D. Boneh, and
P. McDaniel, “Ensemble adversarial training: Attacks and defenses,”
arXiv preprint arXiv:1705.07204, 2017.
[26] V. Verma, A. Lamb, C. Beckham, A. Courville, I. Mitliagkis, and
Y. Bengio, “Manifold mixup: Encouraging meaningful on-manifold
interpolation as a regularizer,” arXiv preprint arXiv:1806.05236, 2018.
[27] Q. Xie, Z. Dai, Y. Du, E. Hovy, and G. Neubig, “Controllable invariance
through adversarial feature learning,” in Advances in Neural Information
Processing Systems, 2017, pp. 585–596.
[28] W. Xu, D. Evans, and Y. Qi, “Feature squeezing: Detecting adversarial
examples in deep neural networks,” arXiv preprint arXiv:1704.01155,
2017.
[29] W. Xu, Y. Qi, and D. Evans, “Automatically evading classiﬁers,” in
Proceedings of the 2016 Network and Distributed Systems Symposium,
2016.
[30] P. Zhao, Z. Fu, Q. Hu, J. Wang et al., “Detecting adversarial examples
via key-based network,” arXiv preprint arXiv:1806.00580, 2018.
12