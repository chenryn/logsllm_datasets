2) Fully-overlapping Support: Challenger Wins:
Proposition 3 (Fully overlapping support). Assume Ωv = Ω+,
i.e., the supports of volunteered and damaging posts fully
overlap. Then, given enough volunteered posts in Dv,
the
challenger always defeats the adversary (in both static and
adaptive scenarios). More precisely, if the challenger selects
k decoys per damaging post
then the adversary’s
probability of identifying a damaging post in Dδ is in average
at most 1/(k + 1).
in Dδ,
An Illustrative Example: Consider the example provided
in Figure 2b where the two classes (red circles and green
crosses respectively) have fully overlapping supports (as they
are drawn from a Gaussian distribution with different means).
We show the decision boundary of the adaptive adversary in
this setting after 50 intervals of the deceptive learning game.
We see that for any decision boundary, there exist points in Ωv
that a challenger can choose such that the adversary mislabels
them as damaging.
Real-world scenario: The fully-overlapping case could
happen in an online social platform if the deﬁnition of what
constitutes as damaging varies across the platform’s users. For
example, user A could consider a post with a single sensitive
word (e.g., a swear word) as damaging, whereas another user
B from a different background might consider the same post
as completely innocuous and volunteer the post. In such a
scenario, the challenger will use volunteered posts from user B
to protect the damaging posts of user A. Hence, the challenger
will win the game against even the most powerful adversary
with inﬁnite data.
Propositions 2 and 3 are important to understand the two
extreme cases —where either the challenger clearly wins
or the adversary clearly wins— as important insights, even
though these clear-cut cases are unlikely to happen in practice.
(a) Non-overlapping supports
(b) Fully overlapping supports
Fig. 2: Two examples illustrating the two possible scenarios
relating to the supports of volunteered posts and damaging posts:
non-overlapping (left) and fully overlapping (right). The black
line denotes the decision boundary of the adaptive adversary
after 50 intervals of the deceptive learning game.
Most real-world applications will likely fall between these
two extremes, where the supports only partially overlap. In
such scenarios, the adversary wins outside the overlap (i.e.,
can classify everything correctly outside the overlap), and the
challenger wins inside the overlap. In other words, extremely
sensitive and damaging posts cannot be protected as they will
have no overlap with any of the volunteered posts. However,
as we show in the next section, with a reasonable volunteered
set, the challenger can make it hard for the adversary to detect
damaging deletions.
V. SYSTEM EVALUATION ON TWITTER DELETIONS
In this section we evaluate the efﬁciency of an adversary
when Deceptive Deletions is applied to the real-world problem
of concealing damaging deletions in Twitter. In this evaluation
we ﬁrst create and prepare sets of (non)damaging tweets.
Then we use these sets to train the challenger and adversary
classiﬁers and analyze their performance.
A. Data Collection
In this work, we select Twitter as our experimental social
media platform. We note that it was certainly plausible to per-
form the exact experiment on other social platforms. However
we chose Twitter due to its popularity and feasibility of data
collection. Speciﬁcally, in order to evaluate the challenger we
needed a real-world dataset which includes (i) both deleted and
non-deleted tweets (i.e., Twitter posts) and (ii) deleted tweets
that contain both damaging and non-damaging tweets. To that
end, we use two data sources to create such a dataset.
1) Deceptive Deletion dataset: We collected 1% of daily
random tweet samples from the Twitter API from Oct 2015 -
May 2018. Eliminating non-English tweets, we accumulated
over one billion tweets. In the next step, we construct the
damaging and volunteered sets.
To construct the damaging set, we ﬁrst needed to identify
the deleted tweets6. We sampled 300,000 tweets from the
aforementioned collected data, and leveraging the Twitter API,
we identiﬁed the tweets that were deleted at the time of our
experiment (Jan 30th, 2020). In total, we identiﬁed 92,326
6we only considered user-initiated deletion (not platform-initiated ones).
8
1.00.50.00.51.01.52.00.750.250.250.751.25100% fscore1.00.50.00.51.01.00.50.00.51.050%  fscoredeleted tweets. The next step was to obtain ground truth labels
for the deleted tweets—i.e., detect and assign “true” labels
to damaging tweets and “false” labels to rest. We used the
crowdsourcing service Amazon Mechanical Turk (MTurk) [21]
to obtain a proxy for these true labels. However, there were
two challenges– First, it was impractical to ask our annotators
to label 92,326 tweets. Second, since the dataset was highly
imbalanced, a simple random sample of tweets for labeling
would have resulted in a majority of non-damaging tweets.
Thus we followed prior work [87], [94] and ﬁltered the
deleted tweets using a simple sensitive keyword-based ap-
proach [94] (i.e., identify posts with sensitive keywords) to
have a higher chance of collecting possibly damaging tweets.
The complete list of keywords (over 1500 words) can be found
in http://bit.ly/1LQD22F. This approach resulted in 33,000
potentially damaging tweets, and we randomly sampled 3,500
tweets to be labeled by annotators on MTurk. The mean
number of sensitive keywords in each tweet within our data
set was 2.55. We have also considered the experiment of
skipping the ﬁltering step explained above. We refer interested
readers to Appendix A for detailed information after reading
this section (as only the differences with the ﬁltering approach
are highlighted there).
Note that, in addition to the cursing and sexual keywords,
our sensitive keyword-based approach considered keywords
related to the topics of religion, race, job, relationship, health,
violence, etc. Intuitively, if a post does not contain any such
sensitive keywords then the likelihood of the post being
damaging is very low. We conﬁrmed this intuition by asking
MTurk annotators to label 150 tweets which did not contain
any sensitive keyword as (non)damaging. More than 97% of
these 150 tweets were labeled as non-damaging by annotators.
We surmised that in practice, the adversary will also leverage
a similar ﬁltering approach to reduce its overhead and increase
its chances of ﬁnding damaging posts. Note that,
in this
experiment we have only considered the text of the tweets.
However, the adversary can use additional user information,
but labeling the posts (for training) based on the entire sets of
posts of the users is infeasible for a large-scale attack.
In total, out of our sampled 3,500 deleted tweets, we
obtained labels for 3,177 tweets (excluding annotations from
Turkers who failed our quality control checks as described
later). Among the labeled tweets, 1,272 were identiﬁed as
damaging, and 1, 905 were identiﬁed as non-damaging.
Data labeling using MTurk. We acknowledge that ideally,
the tweet labels should have been assigned by the posters
themselves. However, since we collected random tweets at
large-scale using the Twitter API, we could not track down and
pursue original posters to label their deleted tweets. Further-
more, following up with speciﬁc users for labeling their deleted
posts is likely to cross the ethical boundary of this academic
work (see Section V-B). To that end, we note that there is
a crowdsourcing based alternative which is already leveraged
by earlier work to assign sensitivity labels [27], [31], [87].
Speciﬁcally, these studies determined the sensitivity of social
media posts by simply aggregating crowdsourced sensitivity
labels provided by multiple MTurk workers (Turkers). Thus,
we took a similar approach as mentioned next.
On MTurk,
tasks (e.g., completing surveys) are called
Human Intelligence Tasks or HITs. Turkers can participate in
a survey by accepting the corresponding HIT only if they meet
all the criteria associated with that HIT (set by the person(s)
who created the HIT). We leverage this feature to ensure the
reliability of our results. Speciﬁcally we asked that the Turkers
taking our survey should: (i) have at least 50 approved HITs.
(ii) have an assignment approval rate higher than 90%, and
(iii) have their location set to United States. This last criterion
ensured consistency of our Turkers’ linguistic background. In
our experiment each HIT consisted of annotating 20 tweets
with true (damaging) or false (non-damaging) labels. We
allowed the Turkers to skip some tweets in case they feel
uncomfortable for any reason. We compensated 0.5 USD for
each HIT and on average it took the Turkers 193 seconds to
complete each HIT.
To control the quality of annotation by Turkers, we in-
cluded two hand-crafted control tweets with known labels in
each HIT. These control tweets were randomly selected from
two very small sets of clearly non-damaging or damaging
tweets and were inserted at random locations within the
selection of 20 tweets. For example a damaging control tweet
was: “I think I have enough knowledge to make a suicide bomb
now! Might need it New Year’s Eve” and non-damaging control
tweet was: “Prayers with all
the people in the hurricane
irma”. If for a HIT, the responses to these control tweets did
not match the expected label, we conservatively discarded all
twenty annotations in that HIT.
We countered possible bias resulting from the order of
presentation of tweets via randomizing the order of tweets in
every HIT. Even if two Turkers annotated the same set of
tweets, the order of those tweets was different. Furthermore,
to ease the subjectivity of the labels from each participant, for
each tweet we collected the annotations of multiple Turkers
and took the majority vote. In our experiment, we created the
HITs such that each tweet was annotated by 3 distinct Turkers.
After receiving the responses, for each tweet we assigned the
ﬁnal label (indicating damaging or non-damaging) based on
the majority vote.
We emphasize that in the real world, the burden of labeling
the posts via crowdsourcing is on the adversary(see Sec-
tion III-B, Labels subsection). The challenger, on the other
hand, can be implemented as a service within the platform
and can obtain the true labels directly from the post-owners.
Therefore, existence of any mislabeled data will negatively im-
pact only the adversary (see Section III-C, Labels subsection).
2) #Donttweet dataset: Recently Wang et al. [87] proposed
“#Donttweetthis”. “#Donttweetthis” is a quantitative model
that identiﬁes potentially sensitive content and notiﬁes users
so that they can rethink before posting those content on social
platforms. Wang et al. created the training data for their model
by (i) identifying possibly sensitive tweets by checking for
the existence of sensitive keywords within the text and then
(ii) using crowd-sourcing (i.e., using MTurk) to annotate the
sensitivity of each tweet by three annotators.
The data collection approach used by “#Donttweetthis”
(section 3 of [87]) is very similar to ours. Therefore, to enrich
our dataset and be able to evaluate the challenger over more
intervals, we acquired their labeled tweets. Using the Twitter
API, we queried the tweets using their corresponding IDs and
9
identiﬁed the deleted ones (at the time of writing, Jan 30th,
2020). In total, we obtained 851 deleted tweets, where 418
were labeled as sensitive (damaging), and the remaining 433
were labeled as non-sensitive (non-damaging). The mean of
sensitive keywords in each tweet within this set was 1.7.
Summary of collected data.
In summary, combining the
two datasets explained above, we obtained labels for 4, 028
deleted tweets establishing the user deleted set. Among the
deleted tweets 1, 690 were labeled as damaging constructing
our damaging set (D+). As we will demonstrate in the results
section, in our evaluation the four thousand labeled tweets
(larger than that of prior works [87], [94]) allows for 10
intervals for the game between the adversary and challenger.
Furthermore, for our experiment, we consider k = 1, 2, 5
(i.e., number of decoy posts for each damaging post). To
accommodate these values of k and construct a volunteer pool
that the challenger can make meaningful selections from, we
sampled 100,000 non-deleted tweets uniformly at random from
the 1% daily tweet samples posted between Jan 1st, 2018 May
31st, 2018 to build the volunteered set. The non-deleted tweets
are assumed to be non-damaging. We consider this assumption
to be reasonable as if a tweet contains some damaging content
then its owner would not keep that post on its proﬁle. In
practice, we can forgo this assumption as the volunteer users
themselves offer the volunteer posts. The average number of
sensitive keywords in each tweet in this set was 0.41.
B. Ethical Considerations
Recall that in order to create our evaluation dataset we
needed to show some deleted tweets to Turkers for the an-
notation task. Thus, we were signiﬁcantly concerned about
the ethics of our annotation task. Consequently, we discussed
at length with the Institutional Review Board (IRB) of the
lead author’s institute and deployed the annotation task only
after we obtained the necessary IRB approval. Next we will
detail, how, in our ﬁnal annotation task protocol we took quite
involved precautionary steps for protecting the privacy of the
users who deleted their tweets.
We recognize that, in the context of our evaluation, the
primary risk to the deleted-tweet-owners was the possibility of
linking deleted tweets with deleted-tweet-owner proﬁles during
annotation. This intuition is supported by prior research [56],
[65] who suggested applying selective anonymization for re-
search on deleted content. Thus, we anonymized all deleted
tweets by replacing personally identiﬁable information or PII
(e.g., usernames, mentions, user ids, and links) with place-
holder text. For example, we replaced user accounts (i.e.,
words starting with @) and url-links with “UserAccount” and
“Link” respectively. Moreover, one of the authors manually
went over each of these redacted posts to ensure anonymization
of PII before showing them to Turkers.
C. Experiment Setup
Partitioning the data for different time intervals. Recall
from Section III that we discretize time into intervals. In our
experiments, we choose T = 10 intervals in total (a choice
made based on the number of collected tweets). Consequently,
we partition our dataset into 10 intervals. Ideally, the partitions
should be based on the creation and deletion timestamps of
the tweets. Unfortunately however, the Twitter API does not
provide deletion timestamps. Hence, we randomly shufﬂe the
tweets and divide them into 10 equally sized partitions.
BERT model.
In line with our approach to model the most-
powerful adversary as best as we possibly can, we use a
state-of-the-art natural language processing model: the BERT
(Bidirectional Encoder Representations from Transformers)
language model [36], both for the adversary and for the chal-
lenger. Speciﬁcally, we use BERTBASE model that consists of
12 transformer blocks, a hidden layer size of 768 and 12 self-
attention heads (110M parameters in total). BERT has been
shown to perform exceedingly well in a number of downstream
NLP tasks [36]. We use HuggingFace’s [89] implementation
of the BERT model that was already pre-trained on masked
language modeling and next sentence prediction tasks.
BERT uses WordPiece embeddings [90] to convert each
word in the input tweet to an embedding vector. The concate-
nated embedding vector is passed to the BERT neural network
model. In our experiments, we only give the text of the tweet
as input to both the adversary and the challenger to make it
amenable to the pre-trained BERT models. Other tweet features
such as deletion timestamps, number of likes, etc. could be
used by both the adversary and the challenger to improve
their performance. Note however that Propositions 2 and 3
still apply as long as the adversary and the challenger have
the same information.
We ﬁne-tune the BERT model on our datasets as prescribed
by Devlin et al [36]. In each interval, the adversary’s classiﬁer
is ﬁne-tuned for the classiﬁcation of tweets into damaging
and non-damaging using the negative log-likelihood loss in
Equation (1). We use a batch size of 32 and sample equal
number of damaging and non-damaging tweets in each batch.
This procedure results in better trained models as it avoids the