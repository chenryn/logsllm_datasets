standard tools to scan and globalize symbols present in the
binary after the linking process. As in our Tor plug-in (Sec-
tion 4), this technique may be used to dynamically generate
variable registration code and eliminates the requirement of
modifying variable deﬁnitions inside the application.
Shadow Callbacks. The Shadow plug-in interface allows
Shadow not only to notify the plug-in when it should allo-
cate and deallocate resources for running the application(s)
contained in the plug-in, but also to notify the plug-in when
it may perform network I/O (reading and writing) on a ﬁle
descriptor without blocking. The I/O callbacks are crucial
for asynchronicity as they trigger application code execu-
tion and prevent applications from the need for polling a
ﬁle descriptor. The Shadow plug-in library also offers sup-
port for a generic timer callback so plug-ins may create
additional events throughout a simulation. Note that call-
backs may also originate from the virtual event library, as
described in Section 3.3.2 below, if the application uses the
libevent-2.0 library.
3.3 Virtual Nodes
In Shadow, a virtual node represents a single simulated
host. A virtual node contains all state that is speciﬁc to
a host, such as addressing and network information that al-
lows it to communicate with other hosts in the network. Vir-
tual nodes also contain Shadow-speciﬁc implementations of
system libraries that promote homogeneity between exist-
ing interfaces. Function interposition allows for seamless
integration of applications into Shadow by redirecting calls
to system functions to our Shadow implementation. Virtual
nodes store their own application-speciﬁc state and swap
this state into the plug-in’s address space before passing
control of code execution to the plug-in.
3.3.1 Virtual Network
In Shadow, the virtual network is the main interface through
which virtual nodes may communicate. Upon creation,
each node’s virtual network interface is assigned an IP ad-
dress and receives upstream and downstream bandwidth
rates as conﬁgured in the simulation script. Each virtual
network contains a transport agent that implements a leaky
bucket (i.e. token bucket) algorithm that allows small trafﬁc
bursts but ensures average data rates conform to the conﬁg-
ured rate. The transport agent handles both incoming and
outgoing packets, allowing for asymmetric bandwidth spec-
iﬁcations. The agent provides trafﬁc policing by dropping
(and causing retransmission of) all non-conforming pack-
ets. Conforming incoming packets are passed to the vir-
tual socket library (discussed below) for processing, while
events are created for conforming outgoing packets and
spooled to the scheduler for delivery to another node after
incorporating network latency.
3.3.2 Node Libraries
Each virtual node implements several system functions as
well as network, event, and cryptography libraries. Func-
tion interposition is used to redirect standard system and
library functions calls made from the application to their
Shadow-speciﬁc counterparts. Function interposition is
achieved by creating a preloaded library with functions of
the same name as the target functions, and setting the en-
vironment variable LD PRELOAD to the path of the preload
library. Every time a function is called, the preload library is
ﬁrst checked. If it contains the function, the preloaded func-
tion is called – otherwise the standard lookup mechanisms
are used to ﬁnd the function. No additional modiﬁcations
are required to hook into Shadow.
Virtual System. The virtual system library implements
standard system calls whose results must be modiﬁed due to
the simulation environment. Functions for obtaining system
time are implemented to return the simulation time rather
than the wall time and functions for obtaining hostname and
address information are intercepted to return the hostnames
as deﬁned in the simulation script conﬁgured by the user.
The virtual system also contains a virtual CPU module
in an attempt to consider processing delays produced by an
application. Using a virtual CPU and processing delays im-
proves Shadow’s accuracy since without it, all data is pro-
cessed by the application at a single discrete instant in the
simulation. When a virtual node reads or writes data be-
tween the application and Shadow, the virtual CPU pro-
duces a delay for processing that data. This delay is “ab-
sorbed” by the system by delaying the execution of every
event that has already been scheduled for that virtual node.
As virtual nodes read and write more data, the wait time
until the next event increases.
We determine appropriate CPU processing speeds as fol-
lows. First, throughput is conﬁgured for each virtual CPU
– the number of bytes the CPU can process per second.
Modeling the speed of a target CPU is done by running
an OpenSSL [31] speed test on a real CPU of that type.
This provides us with the raw CPU processing rate, but we
are also interested in the processing speed of an applica-
tion. By running application benchmarks on the same CPU
as the OpenSSL speed test, we can derive a ratio of CPU
speed to application processing speed. The virtual CPU
converts these speeds to a time per-byte-processed and de-
lays its events appropriately.
Virtual Sockets. The Shadow virtual socket library, the
heart of the node libraries, implements the most signiﬁcant
features for a Shadow simulation. The virtual socket library
implements all system socket functionality which includes:
creating, opening, and closing sockets; sending, buffering,
and receiving data; network protocols like the User Data-
gram Protocol (UDP) [34] and the Transmission Control
Protocol (TCP) [35]; and other socket-level functionali-
ties. Shadow’s tight integration of socket functionalities and
strong adherence to the RFC speciﬁcations results in an ex-
tremely accurate network layer as we’ll show in Section 5.
Shadow intercepts and redirects functions from the sys-
tem socket interface to the Shadow-speciﬁc virtual socket
library implementation. When the application sends data to
the virtual socket library, the data is packaged into packet
objects. The packaging process copies the user data only
twice throughout the lifetime of the packet, meaning the
same packet object is shared among nodes. Only pointers
to the packet are copied as the packet travels through var-
ious socket and network buffers, although buffer sizes are
computed using the full packet size.
Our virtual socket
libraries implement socket-level
buffering, data retransmission, congestion and ﬂow con-
trol mechanisms, acknowledgments, and TCP auto-tuning.
TCP auto-tuning is required to correctly match buffer sizes
to connection speeds since neither high bandwidth con-
nections with small network buffers nor low-bandwidth
connections with large network buffers will achieve the
expected performance. TCP auto-tuning allows network
buffers to be dynamically computed on a per-connection ba-
sis, allowing for highly accurate transfer rates even when
endpoints have asymmetric bandwidth.
Virtual Events.
the use of
libevent-2.0 [17]
to facilitate asynchronous ap-
plications while easing application integration. While
applications are not required to use libevent-2.0, do-
ing so will likely reduce the complexity of the integration
Shadow supports
Figure 2: Simulation vs. wall clock time. Skipping expensive
cryptographic operations results in a linear decrease in experiment
run-time – nearly a one-third reduction in run-time for a small,
550-node Tor experiment.
process. Shadow intercepts and redirects functions from the
libevent-2.0 interface to the Shadow-speciﬁc virtual
event library implementation. The virtual event library
consists of two main components: an event manager and a
virtual I/O monitor. The event manager creates and tracks
events and executes event callbacks while the I/O monitor
tracks the state of Shadow buffers, informing the manager
when a state change may require an event callback to ﬁre
for a given ﬁle descriptor.
Virtual Cryptography. Simulating an application that per-
forms cryptography offers a chance for reducing simulation
run-time. As data is passed from virtual node to virtual node
during the simulation, in most cases it is not important that
the data is encrypted: since we are not sending data out
across a real network, conﬁdentiality is not necessarily re-
quired. Therefore, applications need not perform expensive
encryption and decryption operations during the simulation,
saving CPU cycles on our simulation host machine.
Shadow removes cryptographic processing by preload-
ing the main OpenSSL [31] functions used for data encryp-
tion. The AES encrypt and AES decrypt functions are
used for bulk data encryption and the EVP Cipher func-
tion is used to secure data on SSL/TLS connections. These
functions only perform the low-level cipher operations: all
other supporting cryptographic functionality is unmodiﬁed.
When preloading these functions, Shadow will not perform
the cipher operation during encryption and decryption. Our
virtual CPU already models application processing delays,
and skipping the cipher operations will not affect applica-
tion functionality.
Figure 2 shows the time savings Shadow realizes using
this technique with the Scallion plug-in (discussed below in
Section 4) for various Tor network sizes. Larger savings in
real running time are realized as experiment size increases.
050100150200250300350400MonotonicTime(m)0102030405060708090SimulationTime(m)5r-50cnocrypto5r-50ccrypto50r-500cnocrypto50r-500ccrypto3.3.3 Stored State
Multiple virtual nodes may run the same plug-in. Rather
than duplicating the entire plug-in in memory for each vir-
tual node, Shadow only duplicates the variable state – the
state of an application that will change during execution.
Registration of this variable state with Shadow happens
once for each plug-in. The plug-in registration procedure
allows Shadow to determine which memory regions (begin-
ning address and length) in the current address space will
be modiﬁed by each virtual node running the plug-in.
Following registration, Shadow possesses pointers to
each memory region that may be changed by the plug-in
or application. Multiple nodes for each plug-in are sup-
ported by allocating node-speciﬁc storage for each regis-
tered memory region and maintaining a copy of each plug-
in’s state. For transparency, Shadow loads a node’s state be-
fore every context switch from Shadow to the plug-in, and
saves state back to storage when the context switches back
to Shadow. This process minimizes the total memory con-
sumption of each plug-in, and results in signiﬁcant memory
savings for large simulations and large applications.
4 The Scallion Plug-in: Running Tor in
Shadow
Shadow was designed especially for running simula-
tions using the Tor application. Therefore, Shadow design
choices were made in support of “Scallion”2, a Tor plug-
in implementation. Each virtual node running the Scal-
lion plug-in represents a small piece of the Tor network.
Since Shadow supports most functionality needed by Scal-
lion, the plug-in implementation itself is minimal (roughly
1500 lines of code). Here we describe some of the speciﬁc
components necessary for the Tor application plug-in.
State Registration. Recall that Shadow requires all vari-
able application state to be registered for replication among
virtual nodes. Scallion must ﬁnd and register all Tor vari-
ables, including static and global variables. Unfortunately,
static variables are not accessible outside the scope in which
they were deﬁned. Therefore, scallion uses standard binary
utilities such as objcopy, readelf, and nm to dynami-
cally scan, rename, and globalize Tor symbols. Registration
code is then dynamically generated based on the symbols
present in the Tor object ﬁles, and injected into the plug-in
before compilation. Note that the size of each variable is
also extracted with the binary utilities.
Bandwidth Measurements. TorFlow [44] is a set of scripts
that run in the live Tor network, continuously measuring
bandwidth of volunteer relays by downloading several ﬁles
through each. TorFlow helps determine the bandwidth
to advertise in the public consensus document. Scallion
2Scallions are onion-like plants with underdeveloped bulbs.
contains a component that approximates this functionality.
However, Scallion need not perform actual measurements
since the bandwidth of each virtual node is already conﬁg-
ured in Shadow. Scallion queries for these bandwidth val-
ues through a Shadow plug-in library function and writes
the appropriate ﬁle that is used by the directory authorities
while computing a new consensus. The V3Bandwidth
ﬁle is updated as new relays join the simulated Tor network.
Tor Preloaded Functions.
In an effort to minimize the
amount of changes to Tor, Scallion utilizes the same func-
tion interposition technique as Shadow. Scallion may in-
tercept any Tor function for which it requires changes and
implement a custom version. Changes in Tor are required
only if the target function is static, in which case Tor can
be modiﬁed to remove the static speciﬁer. We now discuss
some functional differences between Tor and Scallion.
The Tor socket function wrapper is one function that
is intercepted by Scallion and modiﬁed to pass the
SOCK NONBLOCK ﬂag to the socket call since Shadow re-
quires non-blocking sockets. Another modiﬁcation involves
the Tor main function, which is not suitable for use in Scal-
lion since it contains an inﬁnite loop. This function is ex-
tracted to prevent the simulation from blocking, and Scal-
lion instead relies on event callbacks from Shadow to im-
plement Tor’s main loop functionality.
Tor is a multi-threaded application, launching at least
one CPU worker thread to handle onionskin tasks – peeling
off or adding a layer of encryption – as they arrive from the
network. Scallion implements an event-driven version of
Tor’s CPU worker since Shadow requires a single-threaded,
single-process application. This is done by intercepting the
Tor function that spawns a CPU worker and relying on the
virtual event library to execute callbacks when the CPU
worker has data ready for processing. The CPU worker per-
forms its task as instructed by Tor, and communicates with
Tor using a socket pair (a virtual pipe) as before. The vir-
tual event library simpliﬁes the implementation of the CPU
worker functionality.
Finally, Scallion intercepts Tor’s bandwidth reporting
function. Each Tor relay reports its recent bandwidth his-
tory to the directory authorities to help balance bandwidth
across all available relays. However, relays’ reports are
based on the amount of data it has recently transferred, and
the reported value is updated every twenty minutes only if
it has not changed signiﬁcantly from the last reported value.
This causes relays to be underutilized when ﬁrst joining the
network, and causes bootstrapping problems in new net-
works since every node’s bandwidth will be zero for the
ﬁrst twenty minutes of the simulation. Without appropriate
bandwidth values, clients no longer perform weighted relay
selection and instead choose relays at random. To mitigate
these problems, Scallion intercepts the bandwidth reporting
function and returns its conﬁgured BandwidthRate no mat-
ter how much data it has transferred. This improves boot-
strapping and path selection for the simulated Tor network.
Conﬁguration and Usability. There are several challenges
in running accurate Tor network simulations with the Scal-
lion plug-in and Shadow. Although Shadow minimizes the
memory requirements, running several instances of Tor still
requires an extremely large amount of memory. Therefore,
simulations must generally run with scaled-down versions
of Tor network topologies and client-imposed network load.
Correctly scaling available relay bandwidth and network
load is complicated. For example, several relays with
smaller bandwidth capacities will not result in the same
network throughput as fewer relays with larger bandwidth
capacities, even if the total capacities are equal. Further,
correctly distributing this bandwidth among entry, middle,
and exit nodes can be tricky. Although live Tor consensus
documents may be used to assist in network scaling, two
randomly generated consensus topologies can have dras-
tically different network throughput measurements. Net-
work throughput also depends on the number of conﬁgured