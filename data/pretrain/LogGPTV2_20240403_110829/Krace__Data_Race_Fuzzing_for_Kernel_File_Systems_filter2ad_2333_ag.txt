fuzzer with slow executions but finer-grained tracking might
eventually have higher chances to explore more states.
Data race checking cost. Another limiting factor for KRACE
is the time needed to analyze the execution logs for data
race detection, which also depends on the length of the
execution trace. The trend is also plotted in Figure 13. In
summary, the analysis time ranges from 4-7 minutes (0-30
syscalls per seed) for btrfs and 2-6 minutes for ext4. Such a
time cost is obviously not feasible for online checking (even
after optimization) but can be tolerated for offline checking,
i.e., KRACE schedules a data race check only when a seed
is discovered. This strategy works especially when fuzzing
saturates, as the bottleneck for making further progress then
becomes finding new execution states instead of checking the
trace. Based on our experience, running four checker processes
alongside 24 fuzzing VM instances is more than sufficient to
catch up to the progress within 96 hours in both cases.
C. Component evaluations
Coverage effectiveness. Although the two coverage metrics
represent different aspects of program execution, we are also
curious whether tracking explorations in the concurrency
dimension may help in finding new code paths (represented by
Fig. 12: Evaluation of the coverage growth of KRACE when fuzzing
the ext4 file system for a week (168 hours) with various settings.
threads naturally leads to more alias pairs. The similar logic
also applies to why alias coverage saturates much faster in
ext4, the less concurrent file system.
Growth synchronization.
In general, the two coverage
metrics grow in synchronization. It is expected that progresses
in the branch coverage will yield new alias coverage too because
new code paths mean new memory accessing instructions and
hence, new alias pairs. However, it is the other direction that
matters more: branch coverage saturates but alias coverage
keeps growing, e.g., starting from hour 75 in the btrfs case
or hour 25 in the ext4 case. In other words, KRACE keeps
finding new execution states (thread interleavings) that would
otherwise be missed if only branch coverage is tracked.
Instrumentation overhead. The code instrumentation from
KRACE is heavy, and we expect it to cause significant overhead
in execution. To show this, we present the aggregated statistics
on the execution time for seeds bearing different numbers
of syscalls. For comparison, we also run these seeds on a
bare-metal kernel built without KRACE instrumentation. The
results are plotted in Figure 13. In summary, in the zero-syscall
case, i.e., by merely loading (file system module) → mounting
(image) → unmounting → unloading, KRACE already incurs
47.6% and 34.3% overhead, and the more syscalls KRACE
executes, the more overhead it accumulates.
12
0255075100125150175Fuzzing time (unit: hours)16K18K20K22K24K# CFG branches (branch coverage)BranchBranch (w/o alias feedback)Branch (Syzkaller)30K35K40K45K50K55K60K# aliased instruction pairs (alias coverage)AliasAlias (w/o delay injection)Alias (w/o seed merging)0255075100125150175Fuzzing time (unit: hours)13K14K15K16K17K18K19K20K# CFG branches (branch coverage)BranchBranch (w/o alias feedback)Branch (Syzkaller)1K2K3K4K5K6K7K8K9K# aliased instruction pairs (alias coverage)AliasAlias (w/o delay injection)Alias (w/o seed merging)051015202530# syscalls in the seed input5.07.510.012.515.017.520.022.525.0Average seed execution time (unit: second)btrfs execution timeext4 execution timebtrfs baselineext4 baseline050100150200250300350400Average seed analysis time (unit: second)btrfs analysis timeext4 analysis timebranch coverage). To check this, we disabled the alias coverage
feedback and let KRACE explore the states mimicking the
feedback loop of existing OS and file system fuzzers. The
results (Figure 11 and Figure 12) show that exploring the
concurrency domain also helps to find new code coverage.
Most notably, without alias coverage feedback, branch coverage
grows much faster at the beginning, because it does not
spend fuzzing effort on exploring the thread interleavings, but
saturates at a lower number (7.2% and 4.0% less). Moreover, if
just counting the new branches explored (besides the branches
in the initial seed), the coverage reduces by 20.4% and 10.7%,
respectively. The more concurrent the file system is, the more
branch coverage will be explored by enabling alias coverage
feedback. This is not surprising, as certain code paths exist to
handle contention in the system, such as the paths executed
when try_lock fails or when sequence lock retries. Exploring
in the concurrency dimension helps to reveal these paths and
boost the branch coverage.
Delay injection effectiveness. To test whether injecting
delays helps in exploration in the concurrency dimension, we
disabled delay injection in this fuzzing experiment, and the
alias coverage growth is shown in Figure 11 and Figure 12.
With delay injection disabled, KRACE found 28.7% and 12.3%
less alias coverage in btrfs and ext4, respectively. This shows
that delay injection is important in finding more alias coverage.
Especially, when the branch coverage saturates, delay injection
becomes the leading force in finding alias coverage, as shown
by the enlarging gap between the growth. The more concurrent
the file system is, the more important delay injection becomes.
Seed merging effectiveness. To test whether reusing the
seed helps in exploration in the concurrency dimension, we
disabled seed merging in this fuzzing experiment, i.e., KRACE
only adds, deletes, and mutates syscalls but never reuses the
found seeds. The alias coverage growth is shown in Figure 11
and Figure 12. With seed merging disabled, KRACE found
37.7% and 14.2% less alias coverage in btrfs and ext4,
respectively. This experiment shows that reusing the seed is
important in quickly expanding the coverage. More importantly,
preserving the semantics among the syscalls and interleaving
the seeds help find more alias coverage.
Components in the data race checker. To show that it is
important to have both happens-before and lockset analysis (and
their sub-components) in the data race checker, we sampled a
simple fuzzing run: load btrfs module, mount an empty image,
execute two syscalls × three threads, unmount the image, and
unload the btrfs module. The following shows the filtering
effects of each component in the data race checker:
• data race candidates: 35,658
+ after lockset analysis on pessimistic locks: 13,347
+ after lockset analysis on optimistic locks: 8,903
+ after tracking fork-style happen-before relation: 6,275
+ after tracking join-style happen-before relation: 3,509
+ after handling publisher-subscriber model: 103
+ after handling ad-hoc schemes: 7 (all benign races)
D. Comparison with related fuzzers
Execution speed vs coverage. In terms of efficiency, KRACE
is not comparable to other OS and file system fuzzers, as
one execution takes at least seven seconds in KRACE, while
the number can be as low as 10 milliseconds for libOS-
based fuzzers [5, 6] or never-refreshing VM-based fuzzers
like Syzkaller. However, the effectiveness of a fuzzer is not
solely decided by fuzzing speed. A more important metric
is the coverage size, especially when saturated. Intuitively, if
the saturated coverage is low, being fast in execution only
implies that the coverage will converge faster and mostly stall
afterward.
On the metric of saturated coverage, KRACE outperforms
Syzkaller for both btrfs and ext4 by 12.3% and 5.5%,
respectively, as shown in Figure 11 and Figure 12. Even
without the alias coverage feedback, the branch coverage from
KRACE still outperforms Syzkaller, showing the effectiveness
of KRACE’s seed evolution strategies, especially the merging
strategy for multi-threaded seeds, which is currently not
available in Syzkaller. In fact, KRACE is able to catch up
to the branch coverage progress with Syzkaller within 30 hours
and eight hours for btrfs and ext4, respectively.
Data race detection. Razzer [24] reports four data races in
file systems and we find the patches for two of them, both in
the VFS layer. To check that KRACE may detect these cases,
we manually revert the patches in the kernel and confirm that
both cases are found. We would like to do the same for SKI [7],
but the data races found by SKI are too old (in 3.13 kernels)
and locating and reverting the patches is not easy.
VIII. CONCLUSION AND FUTURE WORK
This paper presents KRACE, an end-to-end fuzzing frame-
work that brings the concurrency aspects into coverage-guided
file system fuzzing. KRACE achieves this with three new
constructs: 1) the alias coverage metric for tracking exploration
progress in the concurrency dimension, 2) the algorithm for
evolving and merging multi-threaded syscall sequences, and
3) a comprehensive lockset and happens-before modeling for
kernel synchronization primitives. KRACE has uncovered 23
new data races so far and will keep running for more reports.
Looking forward, we plan to extend KRACE in at least three
directions: 1) data race detection in other kernel components;
2) semantic checking for more types of concurrency bugs; and
3) fuzzing distributed file systems that involve not only thread
interleavings but also network event ordering, which requires
completely new coverage metrics to capture.
IX. ACKNOWLEDGMENT
We thank the anonymous reviewers and our shepherd, Yan
Shoshitaishvili, for their insightful feedback. This research
was supported, in part, by NSF under award CNS-1563848,
CNS-1704701, CRI-1629851, and CNS-1749711; ONR under
grant N00014-18-1-2662, N00014-15-1-2162, and N00014-17-
1-2895; DARPA TC (No. DARPA FA8650-15-C-7556); ETRI
IITP/KEIT[B0101-17-0644]; and gifts from Facebook, Mozilla,
Intel, VMware, and Google.
13
REFERENCES
[1] L. Lu, A. C. Arpaci-Dusseau, R. H. Arpaci-Dusseau, and S. Lu, “A study
of linux file system evolution,” Trans. Storage, vol. 10, no. 1, pp. 3:1–3:32,
Jan. 2014. [Online]. Available: http://doi.acm.org/10.1145/2560012
[2] J. Huang, M. K. Qureshi, and K. Schwan, “An Evolutionary Study of
Linux Memory Management for Fun and Profit,” in Proceedings of the
2016 USENIX Annual Technical Conference (ATC), Berkeley, CA, USA,
Jun. 2016, pp. 465–478.
[3] A. Aghayev, S. Weil, M. Kuchnik, M. Nelson, G. R. Ganger, and
G. Amvrosiadis, “File Systems Unfit As Distributed Storage Backends:
Lessons from 10 Years of Ceph Evolution,” in Proceedings of the 27th
ACM Symposium on Operating Systems Principles (SOSP). New York,
NY, USA: ACM, Oct. 2019, pp. 353–369.
[4] C. Min, S. Kashyap, S. Maass, W. Kang, and T. Kim, “Understanding
Manycore Scalability of File Systems,” in Proceedings of the 2016
USENIX Annual Technical Conference (ATC), Denver, CO, Jun. 2016.
[5] W. Xu, H. Moon, S. Kashyap, P.-N. Tseng, and T. Kim, “Fuzzing File
Systems via Two-Dimensional Input Space Exploration,” in Proceedings
of the 40th IEEE Symposium on Security and Privacy (Oakland), San
Francisco, CA, May 2019.
[6] S. Kim, M. Xu, S. Kashyap, J. Yoon, W. Xu, and T. Kim, “Finding
Semantic Bugs in File Systems with an Extensible Fuzzing Framework,”
in Proceedings of the 27th ACM Symposium on Operating Systems
Principles (SOSP), Ontario, Canada, Oct. 2019.
[7] P. Fonseca, R. Rodrigues, and B. B. Brandenburg, “SKI: Exposing
Kernel Concurrency Bugs Through Systematic Schedule Exploration,”
in Proceedings of the 11th USENIX Symposium on Operating Systems
Design and Implementation (OSDI), Broomfield, Colorado, Oct. 2014.
[8] MITRE Corporation, “CVE-2009-1235,” https://cve.mitre.org/cgi-bin/
cvename.cgi?name=CVE-2009-1235, 2009.
[9] J. Corbet, “Unprivileged filesystem mounts, 2018 edition,” https://lwn.
[10] Kernel.org Bugzilla, “Btrfs bug entries,” https://bugzilla.kernel.org/buglist.
[11] MITRE Corporation, “F2FS CVE entries,” http://cve.mitre.org/cgi-bin/
net/Articles/755593, 2018.
cgi?component=btrfs, 2018.
cvekey.cgi?keyword=f2fs, 2018.
cgi?component=ext4, 2018.