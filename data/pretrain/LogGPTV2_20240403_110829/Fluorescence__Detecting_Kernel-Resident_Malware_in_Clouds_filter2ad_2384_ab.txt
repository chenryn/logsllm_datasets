ment of the tuple is the hash for the ﬁrst feature view of the
page, the second is the hash of the second feature view, and
1A future version of Fluorescence could use page sharing between the
target and agent VMs, in conjunction with copy-on-write, to reduce the pause
time for the target VM. We have not implemented this because the pause time
is already short, and reducing pause time is not the focus of our research.
so on. When it is complete, the agent sends the ﬁngerprint to
Fluorescence’s central server for analysis.
2.1.1 Finding Kernel Code Pages
There are three main challenges that Fluorescence addresses
in obtaining the kernel code pages of a monitored VM. The
ﬁrst is to ensure that all the code pages are in memory. Some
kernels, including the Windows 7 kernel, can swap their
own code pages to disk; on-disk pages cannot easily be read
through VMI, and unreadable pages would result in incom-
plete ﬁngerprints. For such “swappable” kernels, our Fluores-
cence implementation simply invokes a tool inside the VM
guest to pin all of the kernel’s code pages, prior to Fluores-
cence starting the ﬁngerprinting process (§3.1). The second
challenge lies in accessing the target VM’s memory. For this,
our implementation uses libVMI [31], a popular and open-
source library that implements virtual machine introspection.
The third challenge is to ﬁnd all of the kernel code pages. To
do this, the Fluorescence agent starts from the kernel page
global directory (KPGD) and makes a breadth-ﬁrst traversal
of the page table to collect and copy all of the kernel’s exe-
cutable pages. The location of the KPGD is kernel-speciﬁc,
but easily obtainable via libVMI (§3.1).
The x64 architecture supports multiple page sizes—4 KB,
2 MB and 1 GB—and kernels use pages of different sizes to
improve memory management. So that ﬁngerprint generation
is not inﬂuenced by the use of huge pages (which changes over
time), Fluorescence uses a uniform 4 KB page size. When
the agent ﬁnds a huge kernel page, it divides that page into
multiple 4 KB pages within its own representation of the
kernel’s memory. As described next, each 4 KB page becomes
the basis of a feature in the kernel’s ﬁngerprint.
2.1.2 Normalization
Consider a single page of code that is loaded into the guest
kernels of two VMs. One might assume that in the running
kernels, the contents of the two pages would be identical, but
this is often not the case. In particular, the kernel-loading
process may patch the loaded code to replace symbolic ref-
erences (e.g., to functions in other code) with actual (virtual)
addresses. The two kernels may patch different addresses into
the code for various reasons, including the use of ASLR, thus
causing the two copies of the code to be slightly different
across the two VMs. This difference is benign—expected,
and not indicative of an anomaly. Unless differences like
this are accounted for, however, they can make it difﬁcult for
Fluorescence to identify differences that are anomalous.
To reduce the effects of benign differences, the Fluores-
cence agent performs normalization: it applies a set of func-
tions to reduce the “noise” introduced by factors such as
ASLR. A perfect normalization function would effectively
undo the benign changes, mapping every copy of “the same
code page” across all the monitored VMs onto a single value
that is similar to the originally loaded, unpatched code. With
USENIX Association        22nd International Symposium on Research in Attacks, Intrusions and Defenses 369enough kernel information (e.g., debug symbols) such perfect
normalization is feasible, but our aim is for Fluorescence
to work with minimal knowledge of the kernels it monitors.
Our implemented normalization functions (§3.2) therefore
rely only on basic knowledge about ELF/PE loading and the
x86 ISA. As a consequence, they are approximate, but still
effective at reducing “noise.”
The Fluorescence agent applies normalization to each of
the 4 KB pages that it obtains from a monitored VM (§2.1.1).
The agent supports multiple normalization functions and ap-
plies each one individually, resulting in multiple translations
of each page. We refer to each of these translations as a
feature view. By convention, the identity function is always
one of the normalization functions: i.e., the “raw content” is
always one of the feature views.
2.1.3 Hashing
Finally, the agent hashes every feature view of every page that
the agent obtained from the monitored VM. Our implemen-
tation uses ssdeep [20], which is a fast fuzzy hash function.
For each page, the agent collects the hashes of the page’s
feature views into a tuple. It then collects the tuples into a
multiset, which is the completed ﬁngerprint of the VM.
2.2 Feature Alignment
The ﬁngerprints of all the monitored VMs are sent to Fluo-
rescence’s central server for analysis. The ﬁrst step of the
analysis is feature alignment, which aims to ﬁnd the best
correspondences—i.e., the best matches—of all of the pages
across all of the VMs. Imagine putting all of the ﬁngerprints
into a 2D matrix, where each row contains the tuples from a
single ﬁngerprint. The goal of feature alignment is to permute
each row so that the tuples in each matrix column represent
versions of “the same page” across all of the VMs.
Feature alignment has three phases. The ﬁrst simpliﬁes the
ﬁngerprints by removing tuples that represent content present
in all of the VMs. The second computes a basis, which is a
vector that contains the most representative elements (tuples)
across all of the ﬁngerprints. The third phase translates all
of the ﬁngerprints into vectors, ordering the elements by
matching them against the basis.
2.2.1 Remove Tuples for Ubiquitous Content
Recall that our ultimate goal is to ﬁnd anomalous VMs within
the herd. Pages that are identical across all of the VMs are
not useful for anomaly detection, so their tuples can simply
be removed from the ﬁngerprints.
The algorithm for removing “ubiquitous tuples” considers
each normalization separately, starting from the identity func-
tion. Let i be the number of the current normalization, and
let F be the collection of n ﬁngerprints. Let Fi be the “i-th
projection of F”: the ﬁngerprints F, but replacing every tuple
with just its i-th element. Now compute H as the (multiset)
intersection of the members of Fi. The hashes in H represent
tuples that represent the same content (under normalization i)
across all of the ﬁngerprints. Now we can remove those tu-
ples from the ﬁngerprints. For each f in F, for each h in H,
remove the tuple whose i-th element is equal to h.2
The above process is repeated for each normalization func-
tion. By starting with the identity normalization, the algo-
rithm considers exact page-content matches ﬁrst: their tuples
are matched and removed, allowing subsequent matching to
be more accurate. In practice, the above algorithm removes a
large fraction of the tuples from all of the ﬁngerprints, greatly
speeding up all of the subsequent analysis steps.
2.2.2 Compute a Basis
The next step is to compute a basis: a vector that Fluorescence
can use to impose an order on the elements of all of the (sim-
pliﬁed) ﬁngerprints. The order is arbitrary; its only purpose is
to allow similar pages across the VMs to be associated with
each other, so that the ﬁngerprints can sensibly be compared,
and outliers identiﬁed.
Fluorescence creates the basis by selecting the most repre-
sentative elements from the actual ﬁngerprints. We say that
an element of a ﬁngerprint is representative if it is similar to
an element in every other ﬁngerprint, where the similarity of
elements (i.e., tuples) is deﬁned in terms of the hashes they
contain. Given two hashes, ssdeep can compute a similarity
score between 0 and 100: a high score means that the hashes
represent highly similar strings (i.e., normalized page content)
and a low score means that the hashes represent very dissim-
ilar strings. We deﬁne the similarity σ of two ﬁngerprint
elements as the maximum similarity scores of their hashes
for every feature view:
σ (e1,e2) = max(sim1,sim2, . . . )
where e1 = (cid:104)α1,α2, . . .(cid:105), e2 = (cid:104)β1,β2, . . .(cid:105),
simi = ssdeep(αi,βi)
Fluorescence collects the most representative elements into
a basis, called Λ, by using the following algorithm. Initialize
Λ to be a zero-element vector. For every element e in every
ﬁngerprint, ﬁnd the element in every other ﬁngerprint that is
most similar to e. Call that set neighborse, and let sime be the
sum of σ (e,n) for all n∈ neighborse. (If no element in a given
ﬁngerprint has a positive similarity score when compared to
e, then that ﬁngerprint contributes nothing to neighborse.)
Now, from all the elements in all of the ﬁngerprints, choose
the element e that has the greatest sime:
this is the most
representative element. Extend Λ by adding e, and remove
every element of neighborse from further consideration. Now
repeat: from the remaining eligible elements, choose the
element e with the greatest sime; extend Λ, and remove e’s
neighbors from further consideration. Repeat until there are
no more elements to consider.
2By construction, such a tuple is guaranteed to exist. If there is more than
one tuple whose i-th element is h, arbitrarily choose one to remove.
370          22nd International Symposium on Research in Attacks, Intrusions and DefensesUSENIX AssociationIn general, Λ may contain more elements than any of the
actual ﬁngerprints. (It must be at least as long as the longest
ﬁngerprint.) This is because Λ includes elements that repre-
sent all of the “rare” and unique elements that are present in
any ﬁngerprint.
2.2.3 Compute the Fingerprint Vectors
After computing the basis Λ, it is straightforward to transform
the ﬁngerprints into vectors that can be sensibly compared.
Fluorescence does this by computing a 2D matrix, called T
(for “tuples”), in which every row represents a ﬁngerprint and
every column corresponds to an element of Λ. Fluorescence
ﬁlls each row from left to right, i.e., from the most representa-
tive to the least representative elements of the basis. Consider
the translation of a ﬁngerprint f . To ﬁll the leftmost cell of
f ’s row in the matrix, Fluorescence ﬁnds the element in f that
has the greatest similarity with the leftmost value of the basis.
(I.e., choose the element e from f that maximizes σ (Λ0,e).)
Store that element in the leftmost cell of f ’s row, and remove
the element from f . Repeat the selection process for the re-
maining cells, moving from left to right across the row. When
ﬁlling the cell at index j, if no remaining element of f has a
positive similarity score with Λ j, leave cell j empty.
2.3 Anomaly Detection
At its central server, Fluorescence performs two analyses to
detect anomalous VMs. The ﬁrst (§2.3.1), based on deep
learning, detects anomalies by measuring a neural network’s
ability to reconstruct (encoded) ﬁngerprints. The network
is able to reconstruct “typical” ﬁngerprints more accurately
than it can reconstruct anomalous ones. The second (§2.3.2)
applies a clustering algorithm to distinguish between typi-
cal ﬁngerprints (a large cluster, representing healthy VMs)
and atypical ones (small clusters, representing anomalous
VMs). The two algorithms have different strengths and weak-
nesses (§2.3.3) but generally agree in practice, so each serves
to validate the other’s results.
The versions of these algorithms that we use in Fluores-
cence operate on data points that are represented as vectors
of numbers. The matrix T that Fluorescence computed in
§2.2.3, however, has cells ﬁlled with tuples. To prepare for
anomaly detection, therefore, Fluorescence computes a new
matrix, called S (for “similarities”), in which the tuples of T
are replaced by similarity scores as follows:
(cid:40)
Si j =
σ (Λ j,Ti j)
−1
if Ti j is not empty
otherwise
That is, each tuple is represented by its similarity to the
corresponding element of the basis, and empty cells are repre-
sented by −1. This transformation is not distance-preserving
in principle,3 but it preserves the essential qualities of the
Figure 2: Autoencoder architecture.
ﬁngerprints in practice. If two ﬁngerprints are “close to” the
basis in a particular dimension (column) in T , they remain
close to each other in that dimension in S. Similarly, ﬁnger-
prints with unique and/or rare contents (i.e., that have tuples
in columns of T where most ﬁngerprints have none) continue
to be distinguished in S.
2.3.1 Deep Learning Approach
Fluorescence’s deep-learning approach to detecting anoma-
lous VMs is based on an autoencoder. The purpose of an
autoencoder is to learn, in an unsupervised manner, an ef-
ﬁcient method for representing a dataset. As illustrated in
Figure 2, an autoencoder contains an encoder, which reduces
the number of dimensions in an input, and a decoder, which
attempts to reconstruct the original input from its reduced
representation. To minimize the error between the input and
output, an autoencoder must learn to preserve the maximum
information while encoding. The major patterns within the
input dataset are learned and preserved, and as a consequence,
the minor patterns—found in “outliers”—are lost.
Fluorescence leverages these properties to identify the rows
in S that represent anomalous VMs. It trains an autoencoder
over the rows of S. The encoder reduces the dimensionality
of each row vector to 1/30th of its original (with a minimum
of two encoded dimensions), and the decoder attempts to re-
construct the original vector. The learning goal is to minimize
the sum of mean square errors between all the corresponding
inputs and outputs, i.e., to minimize ∑m
i j)2,
where m is the number of VMs (rows of S), n is the number of
features per VM (columns of S), and S(cid:48)
i j is the “reconstructed
value” that corresponds to Si j.
j=1(Si j − S(cid:48)
i=1 ∑n
After training the autoencoder, Fluorescence calculates the
error score of each VM as the squared error between its orig-
inal vector representation in S and the reconstructed vector