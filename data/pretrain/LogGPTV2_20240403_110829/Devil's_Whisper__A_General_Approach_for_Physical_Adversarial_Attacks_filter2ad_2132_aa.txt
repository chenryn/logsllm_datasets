# Title: Devil's Whisper: A General Approach for Physical Adversarial Attacks against Commercial Black-box Speech Recognition Devices

## Authors:
- Yuxuan Chen
- Xuejing Yuan
- Jiangshan Zhang
- Yue Zhao
- Shengzhi Zhang
- Kai Chen
- XiaoFeng Wang

### Affiliations:
- **Yuxuan Chen, Xuejing Yuan, Jiangshan Zhang, Yue Zhao, and Kai Chen:**
  - SKLOIS, Institute of Information Engineering, Chinese Academy of Sciences, China
  - School of Cyber Security, University of Chinese Academy of Sciences, China
  - Yuxuan Chen: Department of Computer Engineering and Sciences, Florida Institute of Technology, USA
- **Shengzhi Zhang:**
  - Department of Computer Science, Metropolitan College, Boston University, USA
- **XiaoFeng Wang:**
  - School of Informatics, Computing, and Engineering, Indiana University Bloomington, USA

### Abstract:
Recent studies have shown that adversarial examples (AEs) can pose a serious threat to "white-box" automatic speech recognition (ASR) systems when the machine-learning model is exposed to the adversary. However, the feasibility of such threats against commercial devices, such as Google Home, Cortana, and Echo, whose models are not publicly available, remains unclear. Exploiting the learning model behind ASR systems in a black-box setting is challenging due to the complex preprocessing and feature extraction stages. Our research demonstrates that such black-box attacks are indeed realistic.

In this paper, we present Devil's Whisper, a general adversarial attack on commercial ASR systems. Our approach involves enhancing a simple local model that roughly approximates the target black-box platform with a more advanced white-box model that is unrelated to the target. We find that these two models can effectively complement each other in predicting the target's behavior, enabling highly transferable and generic attacks. Using a novel optimization technique, we show that a local model built with just over 1500 queries can be elevated by the open-source Kaldi Aspire Chain Model to effectively exploit commercial devices (Google Assistant, Google Home, Amazon Echo, and Microsoft Cortana). For 98% of the target commands, our approach can generate at least one AE to attack the target devices.

### 1. Introduction
With the advancement of automatic speech recognition (ASR) technologies, intelligent voice control (IVC) devices have become increasingly popular. Smart speakers like Google Home, Amazon Echo, and Apple HomePod are now part of our daily lives. Additionally, ASR services such as Google Cloud Speech-to-Text, Amazon Transcribe, Microsoft Bing Speech Service, and IBM Speech to Text enable users to integrate APIs for controlling smart devices, conducting long-form audio transcription, text analysis, and video analysis. Recently, Amazon introduced Auto SDK, allowing drivers to interact with vehicles using voice commands. However, the extensive use of voice for critical system control also raises security concerns that are not yet fully understood.

**Adversarial Example (AE) Threats to ASR:**
Voice is an open channel, and commands received by IVC devices can come from any source. Researchers have shown that unauthorized voice commands can be injected into wireless signals, noise, or even inaudible ultrasound to stealthily gain control of IVC devices. Carlini et al. [20] successfully attacked DeepSpeech (Mozilla's open-source ASR model) using AEs with full knowledge of the model parameters. Yuan et al. proposed CommanderSong, which automatically generates AEs embedded into songs to attack the open-source Kaldi Aspire Chain Model over-the-air. These approaches demonstrate that real-world ASR systems are vulnerable in a white-box model. However, the security risks faced by commercial ASR systems like Google Home, Microsoft Cortana, Amazon Echo, and Apple Siri remain less clear.

**Black-box AE Attacks on ASR Systems:**
Black-box AE attacks on ASR systems are challenging due to the lack of information about the target's model and parameters. ASR systems are more complex than image recognition systems due to their architecture, including feature extraction, acoustic model, and language model, and the need to process time-series speech data. Our study shows that existing techniques for building substitute models on labeled data from the target [32] are insufficient. Even with 24 hours of training data (approximately 5100 queries), the transferability rate against Google Cloud Speech-to-Text API was only 25%. In contrast, similar attacks on image recognition systems like Google, Amazon, and MetaMind APIs using simple datasets like MNIST achieved a transferability rate over 90% with just 800 queries [32].

**Devil's Whisper:**
We demonstrate that a black-box attack on commercial ASR systems and devices is feasible. Our attack, called Devil's Whisper, can automatically generate audio clips as AEs against commercial ASR systems like Google Cloud Speech-to-Text API. These "hidden" target commands are stealthy to humans but can be recognized by the systems, leading to control of commercial IVC devices like Google Home. Our key idea is to use a small number of strategic queries to build a substitute model and enhance it with an open-source ASR model. This helps address the complexity in the target system. Specifically, we use Text-to-Speech (TTS) API to synthesize command audio clips, enlarge the corpus by tuning audio clips before sending them as queries to the target, and then train the substitute model. The substitute model is used in an ensemble learning with an open-source ASR model (base model). The AEs cross-generated by both models are systematically selected to attack the target.

In our experiments, we built substitute models approximating four black-box speech API services (Google Cloud Speech-to-Text, Microsoft Bing Speech Service, IBM Speech to Text, and Amazon Transcribe). Just over 4.6 hours of training data (about 1500 queries) were needed to ensure successful conversion of nearly 100% of target commands into workable AEs when attacking most of the API services. Our AEs can also attack the corresponding black-box IVC devices (Google Assistant, Google Home, Microsoft Cortana, and Amazon Echo) over-the-air with 98% success. Furthermore, our AEs can be transferred to other black-box platforms without public API services (e.g., Apple Siri). A user study on Amazon Mechanical Turk showed that none of the participants could identify any command from our AEs after listening to them once.

### 2. Background and Related Work
#### 2.1 Speech Recognition System
Automatic Speech Recognition (ASR) enables machines to understand human voice, transforming the way people interact with computing devices. Text-to-Speech (TTS) services from Google, Microsoft, Amazon, and IBM are available for developing voice-assistant applications. Popular open-source ASR platforms include Kaldi and Mozilla DeepSpeech.

A typical speech recognition system includes three main procedures: pre-processing, feature extraction, and model-based prediction (acoustic and language models). After receiving raw audio, the pre-processing filters out frequencies outside the range of human hearing and segments below certain energy levels. The system then extracts acoustic features from the processed audio, commonly using Mel-Frequency Cepstral Coefficients (MFCC), Linear Predictive Coefficient (LPC), and Perceptual Linear Predictive (PLP). These features are examined by the pre-trained acoustic model to predict phonemes. Finally, the language model refines the results using grammar rules and commonly used words.

#### 2.2 Adversarial Examples
Neural networks have been widely used in prediction algorithms for image classification, speech recognition, and autonomous driving. Although they significantly improve prediction accuracy, neural networks are vulnerable to adversarial examples (AEs), first identified by Szegedy et al. [36]. Formally, a neural network can be defined as \( y = F(x) \), mapping input \( x \) to output \( y \). Given a specific \( y' \), the original input \( x \), and the corresponding output \( y \), it is feasible to find an input \( x' \) such that \( y' = F(x') \), while \( x \) and \( x' \) are indistinguishable to humans. Such targeted adversarial (TA) attacks have potential impact as the prediction results can be manipulated. Untargeted adversarial (UTA) attacks, on the other hand, identify inputs \( x' \) close to the original input \( x \) but with different outputs, making the target machine misrecognize the input.

#### 2.3 Related Work
Researchers have found various types of attacks on ASR systems, classified into four categories:

1. **Speech Misinterpretation Attack:**
   Third-party applications and skills for IVC systems face security and privacy concerns due to the lack of proper authentication. Kumar et al. [29] and Zhang et al. [42, 43] have demonstrated attacks that exploit misinterpretation and skill squatting.

2. **Signal-Manipulation Based Attacks:**
   These attacks compromise ASR systems by manipulating input signals or exploiting vulnerabilities in pre-processing. Kasmi et al. [28] and Dolphin Attack [41] are examples of such attacks.

3. **Obfuscation Based Attacks:**
   These attacks manipulate the feature extraction of ASR systems. Vaidya et al. [38], Carlini et al. [19], and Abdullah et al. [18] have developed methods to create malicious audio samples that share similar feature vectors with the original audio.

4. **Adversarial Example Based Attacks:**
   For TA attacks, the attacker crafts an original audio into adversarial samples that are indistinguishable to humans but misinterpreted by the target ASR systems. Hidden voice commands [19], CommanderSong [40], and other studies [20, 34, 33] have shown excellent results on white-box platforms, but the effectiveness against black-box ASR systems, especially commercial IVC devices, remains unknown.

### 3. Overview
#### 3.1 Motivation
In the era of the Internet of Things (IoT), voice-enabled centralized control devices like Google Home and Amazon Echo are becoming increasingly popular. Various smart home devices, such as smart locks, lights, and switches, can be paired with these "hubs" for voice control. Voice-assistant applications on smartphones and tablets, such as Google Assistant and Apple Siri, offer a convenient way to use mobile devices. In this paper, we refer to all these voice-enabled centralized control devices and smartphones/tablets as IVC devices.

An example of a potential security risk is smartphone navigation, where attackers can control the FM radio channel to broadcast malicious signals. If attackers craft AEs hiding hostile navigation commands, they can potentially cause significant harm.