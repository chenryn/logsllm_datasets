title:Devil's Whisper: A General Approach for Physical Adversarial Attacks
against Commercial Black-box Speech Recognition Devices
author:Yuxuan Chen and
Xuejing Yuan and
Jiangshan Zhang and
Yue Zhao and
Shengzhi Zhang and
Kai Chen and
XiaoFeng Wang
Devil’s Whisper: A General Approach for Physical 
Adversarial Attacks against Commercial Black-box 
Speech Recognition Devices
Yuxuan Chen, SKLOIS, Institute of Information Engineering, Chinese Academy of Sciences; School 
of Cyber Security, University of Chinese Academy of Sciences; Department of Computer Science, 
Florida Institute of Technology; Xuejing Yuan, Jiangshan Zhang, and Yue Zhao, SKLOIS, Institute 
of Information Engineering, Chinese Academy of Sciences; School of Cyber Security, University of 
Chinese Academy of Sciences; Shengzhi Zhang, Department of Computer Science, Metropolitan 
College, Boston University, USA; Kai Chen, SKLOIS, Institute of Information Engineering, Chinese 
Academy of Sciences; School of Cyber Security, University of Chinese Academy of Sciences; 
XiaoFeng Wang, School of Informatics and Computing, Indiana University Bloomington
https://www.usenix.org/conference/usenixsecurity20/presentation/chen-yuxuan
This paper is included in the Proceedings of the 
29th USENIX Security Symposium.
August 12–14, 2020
978-1-939133-17-5
Open access to the Proceedings of the 
29th USENIX Security Symposium 
is sponsored by USENIX.
Devil’s Whisper: A General Approach for Physical Adversarial Attacks against
Commercial Black-box Speech Recognition Devices
Yuxuan Chen ∗1,2,3, Xuejing Yuan †1,2, Jiangshan Zhang1,2, Yue Zhao1,2, Shengzhi Zhang4, Kai Chen‡1,2, and XiaoFeng Wang5
1SKLOIS, Institute of Information Engineering, Chinese Academy of Sciences, China
2School of Cyber Security, University of Chinese Academy of Sciences, China
3Department of Computer Engineering and Sciences, Florida Institute of Technology, USA
4Department of Computer Science, Metropolitan College, Boston University, USA
5School of Informatics, Computing, and Engineering, Indiana University Bloomington, USA
Abstract
Recently studies show that adversarial examples (AEs) can
pose a serious threat to a “white-box” automatic speech recog-
nition (ASR) system, when its machine-learning model is
exposed to the adversary. Less clear is how realistic such a
threat would be towards commercial devices, such as Google
Home, Cortana, Echo, etc., whose models are not publicly
available. Exploiting the learning model behind ASR system
in black-box is challenging, due to the presence of compli-
cated preprocessing and feature extraction even before the
AEs could reach the model. Our research, however, shows that
such a black-box attack is realistic. In the paper, we present
Devil’s Whisper, a general adversarial attack on commercial
ASR systems. Our idea is to enhance a simple local model
roughly approximating the target black-box platform with a
white-box model that is more advanced yet unrelated to the
target. We ﬁnd that these two models can effectively com-
plement each other in predicting the target’s behavior, which
enables highly transferable and generic attacks on the target.
Using a novel optimization technique, we show that a local
model built upon just over 1500 queries can be elevated by
the open-source Kaldi Aspire Chain Model to effectively ex-
ploit commercial devices (Google Assistant, Google Home,
Amazon Echo and Microsoft Cortana). For 98% of the target
commands of these devices, our approach can generate at
least one AE for attacking the target devices1.
1 Introduction
With the advance of automatic speech recognition (ASR) tech-
nologies, intelligent voice control (IVC) devices become in-
creasingly popular. Today, smart speakers like Google Home,
Amazon Echo, Apple HomePod are already part of our daily
∗Part of this work was done during the author’s visit at IIE, CAS.
†The ﬁrst two authors contributed equally to this work.
‡Corresponding author. PI:EMAIL
1Attack
available
website
(https://sites.google.com/view/devil-whisper), and the source code can be
found on Github https://github.com/RiskySignal/Devil-Whisper-Attack.
demos
the
are
on
life. Also the availability of ASR services such as Google
Cloud Speech-to-Text [10], Amazon Transcribe [4], Microsoft
Bing Speech Service [16] and IBM Speech to Text [12] en-
able their users to conveniently integrate their APIs to control
smart devices, conduct long-form audio transcription, text
analysis, video analysis and etc. More recently, Amazon in-
troduces Auto SDK [2] that allows drivers to interact with
vehicles using voice commands. However, the extensive use
of voice for critical system control also brings in security con-
cerns, whose implications have not yet been fully understood.
AE threats to ASR. More speciﬁcally, voice is an open chan-
nel and therefore the commands received by IVC devices
could come from any source. In recent years, researchers
have shown that unauthorized voice commands can be in-
jected into wireless signals [28], in the form of noise [19]
or even inaudible ultrasound [41], to stealthily gain control
of the IVC devices. Recently, attempts have been made to
utilize adversarial examples (AEs), which are found to be
effective against image processing systems [36], to exploit
ASR systems. Particularly, Carlini et al. [20] have success-
fully attacked DeepSpeech (the open-source ASR model of
Mozilla) using AEs, with the full knowledge of model param-
eters. Yuan et al. proposed CommanderSong [40] that auto-
matically generates AEs embedded into songs to attack open-
source Kaldi Aspire Chain Model [14] over-the-air. These
approaches demonstrate that the real-world ASR systems are
vulnerable in a white-box model, when their internal param-
eters are exposed to the adversary. Less clear is the security
risks the commercial ASR systems such as Google Home,
Microsoft Cortana, Amazon Echo and Apple Siri are facing.
Recently, Taori et al. have made the targeted adversarial attack
by treating DeepSpeech as a black-box [37]. However, so far
no success has been reported when it comes to generating
AEs against the deep learning models behind commercial,
close-source ASR systems, up to our knowledge.
Black-box AE attacks on ASR systems are difﬁcult. In ad-
dition to the challenge introduced by the lack of information
about the target’s model and parameters, as also faced by the
black-box attacks on image processing [32], an ASR system
USENIX Association
29th USENIX Security Symposium    2667
tends to be more complicated than an image recognition sys-
tem, due to its complicated architecture, including feature
extraction, acoustic model and language model, and the de-
sign for processing a time series of speech data. As evidenced
in our study, when directly applying the existing technique to
build a substitute on the data labeled by the target [32], we
found that about 24 hours training set (require around 5100 or-
acle queries with each audio around 25 seconds), even with a
target-based optimization (Section 4.2.1), only gives us a sub-
stitute model with merely 25% transferability against Google
Colud Speech-to-Text API command_and_search model (Sec-
tion 6.4). By comparison, prior research reports that the simi-
lar attack on image recognition systems like Google, Amazon
and MetaMind APIs using simple datasets like MNIST with
800 queries to achieve a transferability rate over 90% [32].
Devil’s Whisper. We demonstrate that a black-box attack on
the commercial ASR system and even device is completely
feasible. Our attack, called Devil’s Whisper, can automatically
generate audio clips as AEs against commercial ASR systems
like Google Cloud Speech-to-Text API. These “hidden” target
commands are stealthy for human being but can be recognized
by these systems, which can lead to control of commercial
IVC devices like Google Home. Our key idea is to use a
small number of strategic queries to build a substitute model
and further enhance it with an open-source ASR, which helps
address the complexity in the target system. More speciﬁcally,
to construct the substitute, we utilize Text-to-Speech (TTS)
API to synthesize commands audio clips, then we enlarge the
corpus by tuning audio clips before sending them as queries to
the target. This allows us to focus on the types of the data most
important to the success of our attack and makes the substitute
model more approximate to the target. The substitute model
trained over the data is then used in an ensemble learning
together with an open-source ASR model (called base model).
The AEs cross-generated by both models are systematically
selected to attack the target.
In our experiment, we build substitute models approximat-
ing each of the four black-box speech API services (Google
Cloud Speech-to-Text, Microsoft Bing Speech Service, IBM
Speech to Text and Amazon Transcribe). Just over 4.6-hour
training data (about 1500 queries with each audio about 25
seconds) is needed to ensure successful conversion of nearly
100% target commands into workable AEs2 when attacking
most of the API services. Our AEs can also attack the corre-
sponding black-box IVC devices3 (Google Assistant, Google
Home, Microsoft Cortana and Amazon Echo) over-the-air
with 98% of target commands successful. Furthermore, our
2In this paper, we consider an AE “workable” or “successful” if it can
either 1) be decoded by the target API service (converted into text) as expected
in an API attack, or 2) cause the target IVC device to execute the target
commands at least twice when playing the AE against the device over-the-air
for no more than 30 times. Note that an over-the-air attack on device can be
sensitive to environmental factors like volume, distance, device etc., while
the attack on APIs is usually stable.
3We have contacted the vendors and are waiting for their responses.
AEs can be successfully transferred to other black-box plat-
forms, which have no public API services (e.g., Apple Siri).
The user study on Amazon Mechanical Turk shows that none
of the participants can identify any command from our AEs
if they listen to them once.
Contribution. The contributions of this paper are as follows.
• Physical adversarial attacks against black-box speech
recognition devices. We conduct the ﬁrst adversarial attack
against commercial IVC devices. With no prior knowledge
of the targets’ machine-learning models and their parameters,
our generated AEs can successfully fool the acoustic model
and language model utilized in ASR systems after bypassing
their feature extraction procedures, which is quite different
from attacking black-box image processing systems. Our AEs
are stealthy enough to be perceived by human being.
• New techniques. We design a novel approach to generate
AEs to attack a black-box ASR system. Our idea is to enhance
a simple local substitute model roughly approximating the
target model of an ASR system with a white-box model that
is more advanced yet unrelated to the target. We ﬁnd that
these two models can effectively complement each other, thus
enabling highly transferable and generic attacks on the target.
Moreover, the substitute model can be trained in an optimized
fashion using much less data, allowing much fewer queries to
the target system.
2 Background and Related Work
In this section, we provide the background on speech recogni-
tion systems and elaborate adversarial examples. Finally we
discuss the related work.
2.1 Speech Recognition System
ASR enables machines to understand human voice and greatly
changes the way people interact with computing devices. In
addition, Text-to-Speech (TTS) services of Google, Microsoft,
Amazon, and IBM have been exposed to the public to develop
their own voice-assistant applications. Besides these commer-
cial black-box systems, there also exist popular open source
ASR platforms such as Kaldi, Mozilla DeepSpeech, etc.
The architecture of a typical speech recognition system in-
cludes three main procedures: pre-processing, feature extrac-
tion and model-based prediction (including acoustic model
and language model). After receiving the raw audio, the pre-
processing ﬁlters out the frequencies out of the range of hu-
man hearing and the segments below certain energy level.
Then, ASR system will extract acoustic features from the pro-
cessed audio for further analysis. Common acoustic feature
extraction algorithms include Mel-Frequency Cepstral Coefﬁ-
cients (MFCC) [31], Linear Predictive Coefﬁcient (LPC) [27],
Perceptual Linear Predictive (PLP) [26], etc. The acoustic fea-
tures will be examined according to the pre-trained acoustic
model to predict the most possible phonemes. Finally, relying
on the language model, ASR system will reﬁne the results
using grammar rules, commonly-used words, etc.
2668    29th USENIX Security Symposium
USENIX Association
2.2 Adversarial Examples
Recently, neural network has been widely used in the predic-
tion algorithms in image classiﬁcation, speech recognition,
autonomous driving and etc. Although it has signiﬁcantly
improved the accuracy of prediction, neural network suffers
from adversarial examples (AEs) as ﬁrst indicated by Szegedy
et al. [36]. Formally speaking, one neural network can be de-
ﬁned as y = F(x), which maps the input x to the corresponding
output y. Given a speciﬁc y(cid:48), the original input x and the corre-
sponding output y, it is feasible to ﬁnd such an input x(cid:48) so that
y(cid:48) = F(x(cid:48)), while x and x(cid:48) are too close to be distinguished by
human. The above example x(cid:48), together with its prediction y(cid:48),
is considered as target adversarial (TA) attack. Such attacks
have potential impact since the prediction results could be
manipulated by the adversary. Compared to TA attacks, untar-
geted adversarial (UTA) attack identiﬁes the input x(cid:48), which is
still close enough to the original input x, but has different out-
put than that of x. Such UTA attack is less powerful since the
adversary could only make the target machine misrecognize
the input, rather than obtaining the desired output.
AE attacks on black-box image processing models. Recent
researches proposed various algorithms to generate targeted
AEs towards different image recognition systems [23, 36].
Speciﬁcally, there are substantial researches towards compro-
mising black-box image processing systems. Liu et al. [30]
proposed the ensemble-training approach to attack Clari-
fai.com, which is a black-box image classiﬁcation system.
Papernot et al. [32] proved that by training a local model to
substitute remote DNN using the returned labels, they can
attack Google and Amazon Image Recognition Systems.
2.3 Related Work
Researchers have found that the ASR systems could be ex-
posed to different types of attacks. We classify the existing
attacks against ASR systems into four categories as below.
Speech misinterpretation attack. Recently, third-party ap-
plications and skills for IVC systems become increasingly
popular, while the lack of proper authentication raises secu-
rity and privacy concerns. Previous studies show third-party
applications are facing misinterpretation attacks. Kumar et
al. [29] present an empirical analysis of the interpretation
errors on Amazon Alexa, and demonstrate the adversary can
launch a new type of skill squatting attack. Zhang et al. [42]
report a similar attack, which utilizes a malicious skill with
the similarly pronounced name to impersonate a benign skill.
Zhang et al. [43] developed a linguistic-guided fuzzing tool
in an attempt to systematically discover such attacks.
Signal-manipulation based attacks. The adversary can
compromise the ASR system by either manipulating the input
signal or exploiting the vulnerability of the functionalities
in pre-processing. For instance, Kasmi et al. [28] ﬁnd that
by leveraging the intentional electromagnetic interference
(IEMI) of the headset cord, voice commands can be injected
into the FM signals that will be recovered and understood by
the speech recognition systems on the smart phone. Dolphin
Attack [41] exploits the hardware vulnerabilities in micro-
phone circuits (served as the recorder for IVC devices), so
the completely inaudible ultrasonic signal carrying human
speech will be demodulated and interpreted as desired mali-
cious commands by the target IVC device including Apple
Siri, Google Now and Amazon Echo.
Obfuscation based attacks. Different from the signal-
manipulation based attacks, the obfuscation based attacks
explore the way that the feature extraction of ASR systems
could be manipulated. Vaidya et al. [38] showed that by invert-
ing MFCC features of the desired command audio, they can
get malicious audios that can be interpreted as the command
by Google Now assistant running on a smartphone. Further-
more, Carlini et al. [19] proposed hidden voice commands
which improve the efﬁcacy and practicality of the attack on
Google Now in [38] with the background noises, while the
commands are unintelligible to human beings. More recently,
Abdullah et al. [18] developed four different perturbations to
create the malicious audio samples, based on the fact that the
original audio and the revised audio (with perturbations) share
similar feature vectors after being transformed by acoustic
feature extraction algorithms.
Adversarial example based attacks. For the TA attacks,
the attacker can craft an original audio into the adversarial
samples, and human beings cannot tell the differences be-
tween it and the original audio. These adversarial samples
can be misunderstood by the target ASR systems and inter-
preted as malicious commands. Hidden voice commands [19]
proposed to generate such adversarial audio samples against
ASR systems with a GMM-based acoustic model. Yuan et
al. [40] proposed the CommanderSong attack, which em-
beds the malicious commands into normal songs. The open-
sourced speech recognition platform Kaldi was used as the
white-box tool, implementing the gradient descent algorithm
on the neural network to craft adversarial audio examples.
Carlini et al. [20] generated the adversarial samples against
the end-to-end Mozilla DeepSpeech platform [25]. Schönherr
et al. [34] showed that they can use psychoacoustic hiding
to make imperceptible adversarial samples towards the WSJ
model of Kaldi platform. Recently, Qin et al. succeeded in
generating the imperceptible and robust AEs to attack Lingvo
ASR system in real world [33]. Although all the above attacks
showed excellent results on the white-box platforms, whether
AEs can attack the black-box ASR systems, especially the
commercial IVC devices, is still unknown.
3 Overview
3.1 Motivation
In the era of Internet of Things (IoT), the voice-enabled cen-
tralized control devices are becoming more and more pop-
USENIX Association
29th USENIX Security Symposium    2669
ular, e.g., Google Home, Amazon Echo, etc. Various smart
home devices, like smart lock, smart light, smart switch can
be paired to such “hub”, which allows them to be controlled
naturally via voice. Moreover, the voice-assistant applications
on smartphones or tablets, e.g., Google Assistant, Apple Siri,
etc., offer a convenient way for people to use their mobile
devices. In this paper, we use IVC devices to refer to all the
above mentioned voice-enabled centralized control devices
and smartphones or tablets.
An example for the potential security risk to the IVC system
is smartphone navigation, which is widely used today to help
drive through unfamiliar areas. Previous work [39] shows that
the FM radio channel can be controlled by attackers to broad-
cast their malicious signals. Therefore, if the attackers craft
their AE hiding a hostile navigation command and broadcast it