DNN. However, it may not be necessary to do so. In Figure
20, we plot the activation patterns of hidden layers of a model
trained using EWE; we observe that adding the SNNL to just
the last layers provides the desired guarantees. Additionally,
we observe a slight increase in model utility when not all
layers are entangled. A detailed understanding of how one
can choose the layers is left to future work.
Scalability and Future Research Directions. As men-
tioned in § 4.3.4, EWE suffers in terms of trade-off between
model performance and watermark robustness when we scale
to deeper architectures, and more complex datasets. Given
the results on CIFAR-100, more work may be needed to scale
the current method to larger datasets. According to Figure 24
(in Appendix A.3), the performance of EWE is impacted by
the number of classes. We suspect this may be due to the rep-
resentation space being more complicated (i.e. there are more
clusters), making it more difﬁcult to entangle two arbitrarily
chosen clusters. Thus, a potential next step would be to inves-
tigate the interplay between the design of triggers to control
the cluster of watermarked data; and the similarity structures
and orientation of the representation space to choose source
and target classes accordingly.
Another possible improvement is to use m-to-n watermark-
ing. In this work, we focused on 1-to-1 watermarking, which
watermarks one class of data and entangles it with another
class. However, as long as the watermarked model behaves
signiﬁcantly differently from a clean model, the model owner
could choose to watermark m classes of data, entangle them
with n other classes, and claim ownership by following the
similar veriﬁcation process as described in § 4.3.1.
8 Conclusions
We proposed Entangled Watermark Embedding (EWE),
which forces the model to entangle representations for legiti-
mate task data and watermarks. Our mechanism formulates a
new loss involving the Soft Nearest Neighbors Loss, which
when minimized increases entanglement. Through our evalua-
tion on tasks from the vision and audio domain, we show that
EWE is indeed robust to not only model extraction attacks,
but also piracy attacks, anomaly detection, transfer learning,
and efforts used to mitigate backdoor (poisoning) attacks. All
this is achieved while preserving watermarking accuracy, with
(a) a nominal loss in classiﬁcation accuracy, and (b) 1.5− 2×
increase in computational overhead. Scaling EWE to complex
tasks without great accuracy loss remains as an open problem.
Acknowledgments
The authors would like to thank Carrie Gates for shepherding
this paper. This research was funded by CIFAR, DARPA
GARD, Microsoft, and NSERC. VC was funded in part by
the Landweber Fellowship.
References
[1] Yossi Adi, Carsten Baum, Moustapha Cisse, Benny
Pinkas, and Joseph Keshet. Turning your weakness into
a strength: Watermarking deep neural networks by back-
dooring. In 27th USENIX Security Symposium (USENIX
Security 18). USENIX Association, August 2018.
[2] Ibrahim M Alabdulmohsin, Xin Gao, and Xiangliang
Zhang. Adding robustness to support vector machines
against adversarial reverse engineering. In Proceedings
of the 23rd ACM International Conference on Informa-
tion and Knowledge Management. ACM, 2014.
[3] Lejla Batina, Shivam Bhasin, Dirmanto Jap, and Stjepan
Picek. CSI NN: Reverse engineering of neural network
architectures through electromagnetic side channel. In
28th USENIX Security Symposium (USENIX Security
19). USENIX Association, August 2019.
USENIX Association
30th USENIX Security Symposium    1949
[4] Markus M. Breunig, Hans-Peter Kriegel, Raymond T.
Ng, and Jörg Sander. Lof: Identifying density-based
local outliers. SIGMOD Rec., 29(2):93–104, May 2000.
[16] Alon Halevy, Peter Norvig, and Fernando Pereira. The
IEEE Intelligent
unreasonable effectiveness of data.
Systems, 24(2):8–12, 2009.
[5] Varun Chandrasekaran, Kamalika Chaudhuri, Irene Gia-
comelli, Somesh Jha, and Songbai Yan. Model extrac-
tion and active learning. CoRR, abs/1811.02054, 2018.
[17] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. In 2016 IEEE Conference on
Computer Vision and Pattern Recognition, 2016.
[6] Bryant Chen, Wilka Carvalho, Nathalie Baracaldo,
Heiko Ludwig, Benjamin Edwards, Taesung Lee, Ian
Molloy, and Biplav Srivastava. Detecting backdoor at-
tacks on deep neural networks by activation clustering.
In Proceedings of the 13th ACM Workshop on Artiﬁcial
Intelligence and Security. ACM, 2020.
[7] Keunwoo Choi, Deokjin Joo, and Juho Kim. Kapre:
On-gpu audio preprocessing layers for a quick imple-
mentation of deep neural network models with keras.
In Machine Learning for Music Discovery Workshop
at 34th International Conference on Machine Learning.
ICML, 2017.
[8] Jacson Rodrigues Correia-Silva, Rodrigo F Berriel,
Claudine Badue, Alberto F de Souza, and Thiago
Oliveira-Santos. Copycat cnn: Stealing knowledge by
persuading confession with random non-labeled data.
In 2018 International Joint Conference on Neural Net-
works (IJCNN), pages 1–8. IEEE, 2018.
[9] Corinna Cortes, Mehryar Mohri, and Afshin Ros-
tamizadeh. Algorithms for learning kernels based on
centered alignment. Journal of Machine Learning Re-
search, 13(Mar):795–828, 2012.
[10] Bita Darvish Rouhani, Huili Chen, and Farinaz Koushan-
far. DeepSigns: A Generic Watermarking Framework
for IP Protection of Deep Learning Models. arXiv e-
prints, page arXiv:1804.00750, Apr 2018.
[11] Whitﬁeld Difﬁe and Martin E. Hellman. New directions
in cryptography, 1976.
[12] Nicholas Frosst, Nicolas Papernot, and Geoffrey Hin-
ton. Analyzing and Improving Representations with
the Soft Nearest Neighbor Loss. arXiv e-prints, page
arXiv:1902.01889, Feb 2019.
[13] Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
Deep Learning. MIT Press, 2016.
[14] Ian J. Goodfellow, Jonathon Shlens, and Christian
Szegedy. Explaining and Harnessing Adversarial Exam-
ples. arXiv e-prints, December 2014.
[18] Xuedong Huang, James Baker, and Raj Reddy. A histor-
ical perspective of speech recognition. Communications
of the ACM, 57(1):94–103, 2014.
[19] Matthew Jagielski, Nicholas Carlini, David Berthelot,
Alex Kurakin, and Nicolas Papernot. High-Fidelity Ex-
traction of Neural Network Models. arXiv e-prints, page
arXiv:1909.01838, Sep 2019.
[20] Matthew Jagielski, Alina Oprea, Battista Biggio, Chang
Liu, Cristina Nita-Rotaru, and Bo Li. Manipulating
Machine Learning: Poisoning Attacks and Countermea-
sures for Regression Learning. arXiv e-prints, Apr 2018.
[21] Ian Jolliffe. Principal Component Analysis. Springer,
2002.
[22] A. B. Kahng, J. Lach, W. H. Mangione-Smith, S. Mantik,
I. L. Markov, M. Potkonjak, P. Tucker, H. Wang, and
G. Wolfe. Watermarking techniques for intellectual
property protection. In Proceedings of the 35th Annual
Design Automation Conference, DAC ’98, New York,
NY, USA, 1998. Association for Computing Machinery.
[23] Eamonn Keogh and Abdullah Mueen. Curse of Dimen-
sionality, pages 314–315. Springer, Boston, MA, 2017.
[24] Diederik P. Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. 3rd International Conference
on Learning Representations ICLR 2015, 2015.
[25] Simon Kornblith, Mohammad Norouzi, Honglak Lee,
and Geoffrey Hinton. Similarity of Neural Network
Representations Revisited. The 36th International Con-
ference on Machine Learning, 2019.
[26] Alex Krizhevsky. Learning multiple layers of features
from tiny images. Technical report, 2009.
[27] Alexey Kurakin, J. Ian Goodfellow, and Samy Bengio.
Adversarial examples in the physical world. 5th Interna-
tional Conference on Learning Representations, 2017.
[28] Y. Lecun and C. Cortes. The mnist database of hand-
written digits. http://yann.lecun.com/exdb/mnist/, 1998.
[15] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg.
BadNets: Identifying Vulnerabilities in the Machine
Learning Model Supply Chain. arXiv e-prints, page
arXiv:1708.06733, August 2017.
[29] T. Lee, B. Edwards, I. Molloy, and D. Su. Defending
against neural network model stealing attacks using de-
ceptive perturbations. In 2019 IEEE Security and Pri-
vacy Workshops (SPW), pages 43–49, 2019.
1950    30th USENIX Security Symposium
USENIX Association
[30] F. Liu, K. M. Ting, and Z. Zhou. Isolation forest. In 8th
IEEE International Conference on Data Mining, 2008.
[31] K. Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-
pruning: Defending against backdooring attacks on deep
neural networks. In 21st International Symposium on
Research in Attacks, Intrusions, and Defenses, 2018.
[32] Daniel Lowd and Christopher Meek. Adversarial learn-
ing. In Proceedings of the eleventh ACM SIGKDD in-
ternational conference on Knowledge discovery in data
mining, pages 641–647. ACM, 2005.
[33] Aleksander Madry, Aleksandar Makelov, Ludwig
Schmidt, Dimitris Tsipras, and Adrian Vladu. To-
wards deep learning models resistant to adversarial
attacks. In 6th International Conference on Learning
Representations, 2018.
[34] Leland McInnes, John Healy, Nathaniel Saul, and Lukas
Grossberger. Umap: Uniform manifold approximation
and projection. The Journal of Open Source Software,
3(29):861, 2018.
[35] Smitha Milli, L. Schmidt, A. Dragan, and M. Hardt.
Model reconstruction from model explanations. Pro-
ceedings of the Conference on Fairness, Accountability,
and Transparency, 2019.
[36] A. Mogelmose, M. M. Trivedi, and T. B. Moeslund.
Vision-based trafﬁc sign detection and analysis for in-
telligent driver assistance systems: Perspectives and sur-
vey. IEEE Transactions on Intelligent Transportation
Systems, 13(4):1484–1497, 2012.
[37] Kevin P. Murphy. Machine Learning: A Probabilistic
Perspective. The MIT Press, 2012.
[38] Yuki Nagai, Y. Uchida, S. Sakazawa, and Shin’ichi
Satoh. Digital watermarking for deep neural networks.
International Journal of Multimedia Information Re-
trieval, 7:3–16, 2018.
[39] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bis-
sacco, Bo Wu, and Andrew Ng. Reading digits in natural
images with unsupervised feature learning. 24th Inter-
national Conference on Neural Information Processing
Systems, 2011.
[40] Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz.
Knockoff nets: Stealing functionality of black-box mod-
els. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, 2019.
[41] Soham Pal, Yash Gupta, Aditya Shukla, Aditya Kanade,
Shirish K. Shevade, and Vinod Ganapathy. A framework
for the extraction of deep neural networks by leveraging
public data. CoRR, abs/1905.09165, 2019.
[42] S. J. Pan and Q. Yang. A survey on transfer learning.
IEEE Transactions on Knowledge and Data Engineer-
ing, 22(10):1345–1359, 2010.
[43] Nicolas Papernot, P. McDaniel, Ian J. Goodfellow, S. Jha,
Z. Y. Celik, and A. Swami. Practical black-box attacks
against machine learning. ACM Asia Conference on
Computer and Communications Security, 2017.
[44] Nicolas Papernot, P. McDaniel, S. Jha, Matt Fredrikson,
Z. Y. Celik, and A. Swami. The limitations of deep
learning in adversarial settings. 1st IEEE European
Symposium on Security and Privacy, 2016.
[45] Nicolas Papernot, Patrick McDaniel, and Ian Goodfel-
low. Transferability in Machine Learning: from Phenom-
ena to Black-Box Attacks using Adversarial Samples.
arXiv e-prints, page arXiv:1605.07277, May 2016.
[46] David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.
Williams. Learning representations by back-propagating
errors. Nature, 323:533–536, 1986.
[47] R. Salakhutdinov and Geoffrey E. Hinton. Learning
a nonlinear embedding by preserving class neighbour-
hood structure. In 11th International Conference on
Artiﬁcial Intelligence and Statistics, 2007.
[48] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and
Christian Igel. The German Trafﬁc Sign Recognition
Benchmark: A multi-class classiﬁcation competition.
In IEEE International Joint Conference on Neural Net-
works, pages 1453–1460, 2011.
[49] Emma Strubell, Ananya Ganesh, and Andrew McCal-
lum. Energy and policy considerations for deep learning
in nlp. In Proceedings of the 57th Annual Meeting of the
Association for Computational Linguistics. ACL, 2019.
[50] Christian Szegedy, W. Zaremba, Ilya Sutskever, Joan
Bruna, D. Erhan, Ian J. Goodfellow, and R. Fergus.
CoRR,
Intriguing properties of neural networks.
abs/1312.6199, 2014.
[51] Florian Tramèr, F. Zhang, A. Juels, M. Reiter, and T. Ris-
tenpart. Stealing machine learning models via prediction
apis. In USENIX Security Symposium, 2016.
[52] Jonathan Uesato, Brendan O’Donoghue, Pushmeet
Kohli, and Aäron van den Oord. Adversarial risk and the
dangers of evaluating against weak attacks. In Proceed-
ings of the 35th International Conference on Machine
Learning, volume 80, pages 5032–5041. PMLR, 2018.
[53] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li,
B. Viswanath, H. Zheng, and B. Zhao. Neural cleanse:
Identifying and mitigating backdoor attacks in neural
networks. 2019 IEEE Symposium on Security and Pri-
vacy (SP), pages 707–723, 2019.
USENIX Association
30th USENIX Security Symposium    1951
[54] Pete Warden. Speech Commands: A Dataset for Limited-
Vocabulary Speech Recognition. arXiv e-prints, page
arXiv:1804.03209, Apr 2018.
[55] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-
MNIST: a Novel Image Dataset for Benchmarking
Machine Learning Algorithms. arXiv e-prints, page
arXiv:1708.07747, Aug 2017.
[56] Jialong Zhang, Zhongshu Gu, Jiyong Jang, Hui Wu, M. P.
Stoecklin, H. Huang, and I. Molloy. Protecting intellec-
tual property of deep neural networks with watermark-
ing. In Proceedings of the 2018 on Asia Conference on
Computer and Communications Security, 2018.
A Appendix
A.1 Finetuning the hyperparameters of EWE
Next, we dive into details of each hyperparameter of EWE
and perform an ablation study.
Temperature. Temperature is a hyperparameter introduced
by Frosst et al [12]. It could be used to control which dis-
tances between points are more important: at small tempera-
tures, small distances matter more than at high temperatures,
where large distances matter most. In our experiments, we