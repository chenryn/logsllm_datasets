• Inconsistency matrix C ∈ RN×N, wherein each entry 𝐶𝑖 𝑗
denotes the probability that one sample is labeled as 𝑖-th
class by the task model while is labeled as 𝑗-th class by the
small model in DeepDyve. Note that entry 𝐶𝑖𝑖, 𝑖 ∈ [1..𝑁]
equals to zero. In practice, some classes are naturally more
difficult to classify (e.g., dog and cat in the CIFAR-10 dataset)
than others. These difficult classes cause more inconsistency
than the easy ones. Combining them as one class (e.g., as
pet) in the checker DNN is relatively easy to achieve high
consistency.
Risk impact matrix I will be used in calculating weighted cover-
age. Besides, the three matrices I, R and C will be all used in task
exploration, wherein we try to combine those classes that are easily
confused yet have less risk for task simplification.
4 ARCHITECTURE EXPLORATION
The objective of the architecture exploration procedure is to find a
initial checker DNN model that achieves good fault coverage with
low overhead defined in Section 3. To this end, firstly, we generate
a pool of checker DNN candidates. The generation process is try-
ing to minimize the overhead with the help of model compression
techniques proposed in [13], detailed in Section 4.1. Second, as dif-
ferent candidates offer different trade-offs between overheads and
coverage, we illustrate how to efficiently search for an appropriate
checker DNN design from the candidates in Section 4.2.
4.1 Checker DNN Candidate Generation
In DeepDyve, the consistency between predictions of the checker
DNN and those of the original DNN decides the computational
overhead 𝑂(𝐶). Consider an input that is mis-classified by the
original DNN when no faults occur, we would like to have the
checker DNN output the same wrong label, so that DeepDyve does
not flag a nonexistent failure, avoiding unnecessary re-computation.
To improve consistency, given the task DNN model, we use model
compression to generate the checker model candidates. Specifically,
we use two types of model compression techniques. First, we use
architecture compression to search for the potential architectures
and then we use knowledge distillation to train our checker DNN.
Architecture Compression. No doubt to say, the amount of
available design choices has a significant impact on any design
exploration problem. In order to increase the design options for
DeepDyve, we adopt the model compression approach in [13] to
make the size of checker DNNs adjustable. To be specific, given the
task DNN architecture, we use a single width multiplier 𝛼 to adjust
it, by uniformly scaling down the number of channels (or neurons
if it is a linear layer) for each layer. For example, a feature map with
100 channels will be scaled down to the one with ten channels with
𝛼 being set to 0.1. By applying width multiplier, the resulting model
architecture has much less overhead.
We take one of popular architectures—ResNet trained on GT-
STB [39] as a case study to show the effect of width multiplier.
Table 2 lists the accuracy, the storage overhead (in MegaByte) and
the computational overhead (in Giga Floating Point Operations) of
ResNet-10 with different width multipliers. The first row stands
for the original ResNet-10. As can be observed, accuracy drops
smoothly with smaller model size and less computational cost.
Parameter Training. To further improve the consistency be-
tween the task DNN and the checker DNN, we use knowledge distil-
lation to train the checker DNN. Knowledge distillation, formulated
by Hinton et al. [11], is a training solution to distill a task model
(teacher model) and transfer knowledge to a simpler model (student
model).
In our training, the first step of knowledge distillation from the
task DNN is to covert the pre-softmax logits, 𝑧𝑖, computed for each
class into a probability, 𝑝𝑖, by Equation 5 with the temperature 𝑇 .
(5)
𝑒𝑥𝑝(𝑧𝑖/𝑇)
𝑗 𝑒𝑥𝑝(𝑧 𝑗/𝑇)
𝑝𝑖 =
With higher temperature, the new targets for the checker DNN to
learn are ‘softer’ probability distributions over classes.
Next, the checker DNN is trained by minimizing the knowledge
distillation loss (𝐿𝐾𝐷), which is defined as:
𝐿𝐾𝐷 = 𝜆𝑇 2 × 𝐶𝑟𝑜𝑠𝑠𝐸𝑛𝑡𝑟𝑜𝑝𝑦(𝑃𝑇
𝑂)
𝐶 , 𝑃𝑇
𝐶 and 𝑃𝑇
+ (1 − 𝜆) × 𝐶𝑟𝑜𝑠𝑠𝐸𝑛𝑡𝑟𝑜𝑝𝑦(𝑃𝐶, 𝑦𝑡𝑟𝑢𝑒),
(6)
𝑂 are the softened outputs of the checker DNN
wherein 𝑃𝑇
and the original DNN under the same temperature 𝑇 . The first
component of 𝐿𝐾𝐷 forces the checker DNN towards approximating
similar output distribution of the original DNN (i.e., consistency),
whereas the second component of 𝐿𝐾𝐷 forces the checker DNN
towards correctly classifying inputs as usual (i.e., accuracy). We use
𝜆 to tune the weighted average between the kinds of losses.
Predicted Label0.000.040.480.200.080.070.260.040.310.130.230.000.000.000.000.000.000.020.330.430.760.000.000.210.150.150.250.260.050.010.190.000.190.000.321.000.620.530.210.100.020.000.110.140.000.120.260.040.080.000.380.000.090.850.320.000.040.480.020.000.030.000.340.240.130.100.000.000.010.000.160.000.140.390.480.170.000.000.010.000.750.230.040.050.030.050.010.000.000.310.100.600.020.200.070.100.000.000.140.00planecarbirdcatdeerdogfroghorseshiptruck0.00.20.40.60.81.0True LabelFigure 4: An illustrative example of agglomerative class clustering.
4.2 Search Strategy
Our search strategy is based on the empirical observation that the
consistency between the two models in DeepDyve is related to the
multiplier 𝛼 used to generate the small checker DNN (𝛼  0, 𝑏 > 0, and 𝑎
the computational overhead 𝑂(𝐶) is minimized.
𝛼 + 𝑏, when
2 where
Proof. First, for a neural network composed of linear and con-
volutional layers, the FLOPs of the checker DNN with a multiplier
𝛼 is 𝛼2 times the original FLOPs. Recall that the number of floating
point operations (FLOPs) for one linear layer can be estimated by:
2 × 𝐼 × 𝑂
(8)
where 𝐼 and 𝑂 are number of input and output neurons in one linear
layer, respectively. Therefore, the FLOP of a compressed linear layer
with multiplier 𝛼 is:
𝛼2 × (2 × 𝐼 × 𝑂)
(9)
Similarly, for a convolutional layer, the floating point operations
with multiplier 𝛼 is estimated by:
(2 × 𝑘2 × 𝐶𝑖𝑛) × (𝐻𝑜𝑢𝑡 × 𝑊𝑜𝑢𝑡 × 𝐶𝑜𝑢𝑡)
(10)
where 𝑘 stands for the kernel size, and 𝐶𝑖𝑛, 𝐶𝑜𝑢𝑡 stands for number
of input and output channels, respectively. 𝐻𝑜𝑢𝑡 and 𝑊𝑜𝑢𝑡 are the
height and the width of the output tensors. Given this, the FLOP of
a compressed convolutional layer with multiplier 𝛼 is:
𝛼2 × (2 × 𝑘2 × 𝐶𝑖𝑛) × (𝐻𝑜𝑢𝑡 × 𝑊𝑜𝑢𝑡 × 𝐶𝑜𝑢𝑡).
(11)
Hence, if we add all layers together, the final FLOPs of the checker
DNN will be 𝛼2 times of the task model where 𝛼 = 1.
Providing this, the computational overhead of DeepDyve with
the checker DNN can be simplified from Equation 2 to Equation 12.
(12)
𝑂(𝐶) = 𝛼2 + (1 − 𝑓 (𝛼)), 𝛼 ∈ (0, 1]
𝑂(𝐶) = 𝛼2 + (1 + 𝑎
𝛼
− 𝑏), 𝛼 ∈ (0, 1], 𝑎 > 0, 𝑏 > 0,
(13)
(14)
∇𝑂(𝐶) = 2𝛼 − 𝑎
√︂𝑎
𝛼2 , 𝛼 ∈ (0, 1], 𝑎 > 0
Let ∇𝑂(𝐶) = 0, then 𝛼 = 3
2
(15)
To find the optimal point, we calculate the gradient of 𝑂(𝐶)
as Equation 14. By letting the gradient equals to 0, we obtain the
□
optimal point of 𝛼, which is 3√︃ 𝑎
we select the checker DNN with 𝛼 = 3√︃ 𝑎
Therefore, to obtain the optimal 𝛼, we are going to fit the con-
𝛼 + 𝑏 with the given candidate pool. After that,
2 , as shown in Equation 15.
sistency function − 𝑎
2 .
5 TASK EXPLORATION
After the initial checker architecture is fixed, DeepDyve performs
task exploration to achieve better risk/overhead trade-off. In Sec-
tion 5.1, we first discuss how to perform task simplification effi-
ciently under the guidance of risk probability matrix R, risk impact
matrix I and inconsistency matrix C. The first step generates a
𝐈B,C:0.14+0.06=0.20𝐈𝐈C,E:0.01+0.02=0.0300.060.210.060.040.06000.310.070.2100 0.1000.060.310.1000.150.040.0700.1500.850.0200.080.050.010.900.060.020.010.060.140.740.040.020.0100.030.950.010.050.040.010.030.87ABCDEABCDEഥ𝑹𝐈𝐈𝐈Index ListABCDEABCDE𝑪𝐈𝐈𝐈𝐈𝐈𝐈Step 100.270.060.040.270 0.410.070.060.4100.150.040.070.1500.850.020.080.050.040.920.030.010.010.030.950.010.050.050.030.87(B,C)Index List𝐈A,E:0.05+0.05=0.1Merge B and CMerge A and E0 0.340.210.3400.410.210.4100.910.040.050.050.920.030.020.030.95Step 3(A,E)(B,C)D(A,E)(B,C)D𝐈(A,E)(B,C)D(A,E)(B,C)D𝐈𝐈Index ListIA,D,E:0.05+0.02=0.07Merge (A, E) and DA(B,C)DEA(B,C)DE𝐈ഥ𝑹Step 2ADEA(B,C)DE𝐈𝐈𝑪ഥ𝑹𝑪ABCDEB, CA, EA, D, EA, B, C, D, EK = 4⟶A, (B, C), D, EK = 5⟶A, B, C, D, EK = 3⟶(A, E), (B, C), DK = 2⟶(A, D, E), (B, C)(a) Clustering procedure(b) Generated dendrogram bunch of different tasks. Then, we detail the search strategy to
select the best task in Section 5.2.
• Overhead savings, because the simplified task is easy to learn
and it will be more consistent with the big DNN, thereby
reducing re-computational overhead.
𝑁 , class labels 𝐿
Algorithm 1: Agglomerative class clustering
Input: Risk matrix R, inconsistency matrix C, No. of classes
Output: (𝑁 − 2) cluster label lists
/* Initialize
1 𝑘 = 𝑁 − 1;
2 𝑐𝑎𝑛𝑑𝑖𝑑𝑎𝑡𝑒𝐿𝑖𝑠𝑡 = [];
3 for 𝑞 = 1, 2, . . . , 𝑁 do
4
5
6 end
7 while 𝑘 ≥ 2 do
// Cluster with single class
// Initialize cluster label
𝐺𝑞 = 𝑙𝑞;
𝜆𝑞 = 𝑞;
*/
8
9
10
11
/* Select clusters based on two criteria
𝑖𝑛𝑑𝑒𝑥𝐿𝑖𝑠𝑡 = all arg min𝑖,𝑗 𝐶𝑂𝑉𝑙𝑜𝑠𝑠(𝑖, 𝑗) in R;
(𝑛, 𝑚) = arg max𝑖,𝑗 𝑂𝑠𝑎𝑣𝑒(𝑖, 𝑗) in 𝑖𝑛𝑑𝑒𝑥𝐿𝑖𝑠𝑡;
/* Update cluster label list, matrices
Merge 𝐺𝑛 and 𝐺𝑚, Update clusters {𝐺} and 𝝀;
Update R, C;
/* Add cluster label list to candidates
𝑐𝑎𝑛𝑑𝑖𝑑𝑎𝑡𝑒𝐿𝑖𝑠𝑡[𝑘] = 𝝀;
𝑘 = 𝑘 − 1;
12
13
14 end
15 return 𝑐𝑎𝑛𝑑𝑖𝑑𝑎𝑡𝑒𝐿𝑖𝑠𝑡;
*/
*/
*/
5.1 Agglomerative Class Clustering
Given the original 𝑁 -class task, our problem is to find a simpli-
fied 𝐾-class task for any given checker DNN with better over-
head/coverage trade-off. We consider it as a clustering problem
and propose the Agglomerative Class Clustering to solve it (see
Algorithm 1).
Formally, we assume the labels of original 𝑁 classes as: 𝐿 =
{𝑙1, 𝑙2, . . . , 𝑙𝑁 }. For the sake of simplicity, we can map the labels into
integer numbers, as 𝐿 = {1, 2, . . . , 𝑁}. They are to be grouped into
𝑘=1 𝐺𝑘. Accordingly, we can use 𝜆𝑖 ∈ {1, 2, · · · , 𝐾} to represent
the cluster label of original label 𝑙𝑖. Then, the clustering result can
be represented by a cluster label list: 𝝀 = (𝜆1, 𝜆2, · · · , 𝜆𝑁).
Risk matrix R. The risk probability matrix and risk impact matrix
can be integrated into one single risk matrix with an element-wise
multiplication:
𝐾 clusters {𝐺𝑘|𝑘 = 1, 2, . . . , 𝐾}, where 𝐺𝑘′𝑘′≠𝑘 𝐺𝑘 = ∅ and 𝐿 =
𝐾
R = R ⊙ I,
(16)
wherein each entry in R stands for the risk between classes of the
big network.