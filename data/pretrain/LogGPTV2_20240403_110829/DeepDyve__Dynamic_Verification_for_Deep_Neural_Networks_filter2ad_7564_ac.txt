â€¢ Inconsistency matrix C âˆˆ RNÃ—N, wherein each entry ğ¶ğ‘– ğ‘—
denotes the probability that one sample is labeled as ğ‘–-th
class by the task model while is labeled as ğ‘—-th class by the
small model in DeepDyve. Note that entry ğ¶ğ‘–ğ‘–, ğ‘– âˆˆ [1..ğ‘]
equals to zero. In practice, some classes are naturally more
difficult to classify (e.g., dog and cat in the CIFAR-10 dataset)
than others. These difficult classes cause more inconsistency
than the easy ones. Combining them as one class (e.g., as
pet) in the checker DNN is relatively easy to achieve high
consistency.
Risk impact matrix I will be used in calculating weighted cover-
age. Besides, the three matrices I, R and C will be all used in task
exploration, wherein we try to combine those classes that are easily
confused yet have less risk for task simplification.
4 ARCHITECTURE EXPLORATION
The objective of the architecture exploration procedure is to find a
initial checker DNN model that achieves good fault coverage with
low overhead defined in Section 3. To this end, firstly, we generate
a pool of checker DNN candidates. The generation process is try-
ing to minimize the overhead with the help of model compression
techniques proposed in [13], detailed in Section 4.1. Second, as dif-
ferent candidates offer different trade-offs between overheads and
coverage, we illustrate how to efficiently search for an appropriate
checker DNN design from the candidates in Section 4.2.
4.1 Checker DNN Candidate Generation
In DeepDyve, the consistency between predictions of the checker
DNN and those of the original DNN decides the computational
overhead ğ‘‚(ğ¶). Consider an input that is mis-classified by the
original DNN when no faults occur, we would like to have the
checker DNN output the same wrong label, so that DeepDyve does
not flag a nonexistent failure, avoiding unnecessary re-computation.
To improve consistency, given the task DNN model, we use model
compression to generate the checker model candidates. Specifically,
we use two types of model compression techniques. First, we use
architecture compression to search for the potential architectures
and then we use knowledge distillation to train our checker DNN.
Architecture Compression. No doubt to say, the amount of
available design choices has a significant impact on any design
exploration problem. In order to increase the design options for
DeepDyve, we adopt the model compression approach in [13] to
make the size of checker DNNs adjustable. To be specific, given the
task DNN architecture, we use a single width multiplier ğ›¼ to adjust
it, by uniformly scaling down the number of channels (or neurons
if it is a linear layer) for each layer. For example, a feature map with
100 channels will be scaled down to the one with ten channels with
ğ›¼ being set to 0.1. By applying width multiplier, the resulting model
architecture has much less overhead.
We take one of popular architecturesâ€”ResNet trained on GT-
STB [39] as a case study to show the effect of width multiplier.
Table 2 lists the accuracy, the storage overhead (in MegaByte) and
the computational overhead (in Giga Floating Point Operations) of
ResNet-10 with different width multipliers. The first row stands
for the original ResNet-10. As can be observed, accuracy drops
smoothly with smaller model size and less computational cost.
Parameter Training. To further improve the consistency be-
tween the task DNN and the checker DNN, we use knowledge distil-
lation to train the checker DNN. Knowledge distillation, formulated
by Hinton et al. [11], is a training solution to distill a task model
(teacher model) and transfer knowledge to a simpler model (student
model).
In our training, the first step of knowledge distillation from the
task DNN is to covert the pre-softmax logits, ğ‘§ğ‘–, computed for each
class into a probability, ğ‘ğ‘–, by Equation 5 with the temperature ğ‘‡ .
(5)
ğ‘’ğ‘¥ğ‘(ğ‘§ğ‘–/ğ‘‡)
ğ‘— ğ‘’ğ‘¥ğ‘(ğ‘§ ğ‘—/ğ‘‡)
ğ‘ğ‘– =
With higher temperature, the new targets for the checker DNN to
learn are â€˜softerâ€™ probability distributions over classes.
Next, the checker DNN is trained by minimizing the knowledge
distillation loss (ğ¿ğ¾ğ·), which is defined as:
ğ¿ğ¾ğ· = ğœ†ğ‘‡ 2 Ã— ğ¶ğ‘Ÿğ‘œğ‘ ğ‘ ğ¸ğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘ğ‘¦(ğ‘ƒğ‘‡
ğ‘‚)
ğ¶ , ğ‘ƒğ‘‡
ğ¶ and ğ‘ƒğ‘‡
+ (1 âˆ’ ğœ†) Ã— ğ¶ğ‘Ÿğ‘œğ‘ ğ‘ ğ¸ğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘ğ‘¦(ğ‘ƒğ¶, ğ‘¦ğ‘¡ğ‘Ÿğ‘¢ğ‘’),
(6)
ğ‘‚ are the softened outputs of the checker DNN
wherein ğ‘ƒğ‘‡
and the original DNN under the same temperature ğ‘‡ . The first
component of ğ¿ğ¾ğ· forces the checker DNN towards approximating
similar output distribution of the original DNN (i.e., consistency),
whereas the second component of ğ¿ğ¾ğ· forces the checker DNN
towards correctly classifying inputs as usual (i.e., accuracy). We use
ğœ† to tune the weighted average between the kinds of losses.
Predicted Label0.000.040.480.200.080.070.260.040.310.130.230.000.000.000.000.000.000.020.330.430.760.000.000.210.150.150.250.260.050.010.190.000.190.000.321.000.620.530.210.100.020.000.110.140.000.120.260.040.080.000.380.000.090.850.320.000.040.480.020.000.030.000.340.240.130.100.000.000.010.000.160.000.140.390.480.170.000.000.010.000.750.230.040.050.030.050.010.000.000.310.100.600.020.200.070.100.000.000.140.00planecarbirdcatdeerdogfroghorseshiptruck0.00.20.40.60.81.0True LabelFigure 4: An illustrative example of agglomerative class clustering.
4.2 Search Strategy
Our search strategy is based on the empirical observation that the
consistency between the two models in DeepDyve is related to the
multiplier ğ›¼ used to generate the small checker DNN (ğ›¼  0, ğ‘ > 0, and ğ‘
the computational overhead ğ‘‚(ğ¶) is minimized.
ğ›¼ + ğ‘, when
2 where
Proof. First, for a neural network composed of linear and con-
volutional layers, the FLOPs of the checker DNN with a multiplier
ğ›¼ is ğ›¼2 times the original FLOPs. Recall that the number of floating
point operations (FLOPs) for one linear layer can be estimated by:
2 Ã— ğ¼ Ã— ğ‘‚
(8)
where ğ¼ and ğ‘‚ are number of input and output neurons in one linear
layer, respectively. Therefore, the FLOP of a compressed linear layer
with multiplier ğ›¼ is:
ğ›¼2 Ã— (2 Ã— ğ¼ Ã— ğ‘‚)
(9)
Similarly, for a convolutional layer, the floating point operations
with multiplier ğ›¼ is estimated by:
(2 Ã— ğ‘˜2 Ã— ğ¶ğ‘–ğ‘›) Ã— (ğ»ğ‘œğ‘¢ğ‘¡ Ã— ğ‘Šğ‘œğ‘¢ğ‘¡ Ã— ğ¶ğ‘œğ‘¢ğ‘¡)
(10)
where ğ‘˜ stands for the kernel size, and ğ¶ğ‘–ğ‘›, ğ¶ğ‘œğ‘¢ğ‘¡ stands for number
of input and output channels, respectively. ğ»ğ‘œğ‘¢ğ‘¡ and ğ‘Šğ‘œğ‘¢ğ‘¡ are the
height and the width of the output tensors. Given this, the FLOP of
a compressed convolutional layer with multiplier ğ›¼ is:
ğ›¼2 Ã— (2 Ã— ğ‘˜2 Ã— ğ¶ğ‘–ğ‘›) Ã— (ğ»ğ‘œğ‘¢ğ‘¡ Ã— ğ‘Šğ‘œğ‘¢ğ‘¡ Ã— ğ¶ğ‘œğ‘¢ğ‘¡).
(11)
Hence, if we add all layers together, the final FLOPs of the checker
DNN will be ğ›¼2 times of the task model where ğ›¼ = 1.
Providing this, the computational overhead of DeepDyve with
the checker DNN can be simplified from Equation 2 to Equation 12.
(12)
ğ‘‚(ğ¶) = ğ›¼2 + (1 âˆ’ ğ‘“ (ğ›¼)), ğ›¼ âˆˆ (0, 1]
ğ‘‚(ğ¶) = ğ›¼2 + (1 + ğ‘
ğ›¼
âˆ’ ğ‘), ğ›¼ âˆˆ (0, 1], ğ‘ > 0, ğ‘ > 0,
(13)
(14)
âˆ‡ğ‘‚(ğ¶) = 2ğ›¼ âˆ’ ğ‘
âˆšï¸‚ğ‘
ğ›¼2 , ğ›¼ âˆˆ (0, 1], ğ‘ > 0
Let âˆ‡ğ‘‚(ğ¶) = 0, then ğ›¼ = 3
2
(15)
To find the optimal point, we calculate the gradient of ğ‘‚(ğ¶)
as Equation 14. By letting the gradient equals to 0, we obtain the
â–¡
optimal point of ğ›¼, which is 3âˆšï¸ƒ ğ‘
we select the checker DNN with ğ›¼ = 3âˆšï¸ƒ ğ‘
Therefore, to obtain the optimal ğ›¼, we are going to fit the con-
ğ›¼ + ğ‘ with the given candidate pool. After that,
2 , as shown in Equation 15.
sistency function âˆ’ ğ‘
2 .
5 TASK EXPLORATION
After the initial checker architecture is fixed, DeepDyve performs
task exploration to achieve better risk/overhead trade-off. In Sec-
tion 5.1, we first discuss how to perform task simplification effi-
ciently under the guidance of risk probability matrix R, risk impact
matrix I and inconsistency matrix C. The first step generates a
ğˆB,C:0.14+0.06=0.20ğˆğˆC,E:0.01+0.02=0.0300.060.210.060.040.06000.310.070.2100 0.1000.060.310.1000.150.040.0700.1500.850.0200.080.050.010.900.060.020.010.060.140.740.040.020.0100.030.950.010.050.040.010.030.87ABCDEABCDEà´¥ğ‘¹ğˆğˆğˆIndex ListABCDEABCDEğ‘ªğˆğˆğˆğˆğˆğˆStep 100.270.060.040.270 0.410.070.060.4100.150.040.070.1500.850.020.080.050.040.920.030.010.010.030.950.010.050.050.030.87(B,C)Index ListğˆA,E:0.05+0.05=0.1Merge B and CMerge A and E0 0.340.210.3400.410.210.4100.910.040.050.050.920.030.020.030.95Step 3(A,E)(B,C)D(A,E)(B,C)Dğˆ(A,E)(B,C)D(A,E)(B,C)DğˆğˆIndex ListIA,D,E:0.05+0.02=0.07Merge (A, E) and DA(B,C)DEA(B,C)DEğˆà´¥ğ‘¹Step 2ADEA(B,C)DEğˆğˆğ‘ªà´¥ğ‘¹ğ‘ªABCDEB, CA, EA, D, EA, B, C, D, EK = 4âŸ¶A, (B, C), D, EK = 5âŸ¶A, B, C, D, EK = 3âŸ¶(A, E), (B, C), DK = 2âŸ¶(A, D, E), (B, C)(a) Clustering procedure(b) Generated dendrogram bunch of different tasks. Then, we detail the search strategy to
select the best task in Section 5.2.
â€¢ Overhead savings, because the simplified task is easy to learn
and it will be more consistent with the big DNN, thereby
reducing re-computational overhead.
ğ‘ , class labels ğ¿
Algorithm 1: Agglomerative class clustering
Input: Risk matrix R, inconsistency matrix C, No. of classes
Output: (ğ‘ âˆ’ 2) cluster label lists
/* Initialize
1 ğ‘˜ = ğ‘ âˆ’ 1;
2 ğ‘ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ¿ğ‘–ğ‘ ğ‘¡ = [];
3 for ğ‘ = 1, 2, . . . , ğ‘ do
4
5
6 end
7 while ğ‘˜ â‰¥ 2 do
// Cluster with single class
// Initialize cluster label
ğºğ‘ = ğ‘™ğ‘;
ğœ†ğ‘ = ğ‘;
*/
8
9
10
11
/* Select clusters based on two criteria
ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥ğ¿ğ‘–ğ‘ ğ‘¡ = all arg minğ‘–,ğ‘— ğ¶ğ‘‚ğ‘‰ğ‘™ğ‘œğ‘ ğ‘ (ğ‘–, ğ‘—) in R;
(ğ‘›, ğ‘š) = arg maxğ‘–,ğ‘— ğ‘‚ğ‘ ğ‘ğ‘£ğ‘’(ğ‘–, ğ‘—) in ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥ğ¿ğ‘–ğ‘ ğ‘¡;
/* Update cluster label list, matrices
Merge ğºğ‘› and ğºğ‘š, Update clusters {ğº} and ğ€;
Update R, C;
/* Add cluster label list to candidates
ğ‘ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ¿ğ‘–ğ‘ ğ‘¡[ğ‘˜] = ğ€;
ğ‘˜ = ğ‘˜ âˆ’ 1;
12
13
14 end
15 return ğ‘ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ¿ğ‘–ğ‘ ğ‘¡;
*/
*/
*/
5.1 Agglomerative Class Clustering
Given the original ğ‘ -class task, our problem is to find a simpli-
fied ğ¾-class task for any given checker DNN with better over-
head/coverage trade-off. We consider it as a clustering problem
and propose the Agglomerative Class Clustering to solve it (see
Algorithm 1).
Formally, we assume the labels of original ğ‘ classes as: ğ¿ =
{ğ‘™1, ğ‘™2, . . . , ğ‘™ğ‘ }. For the sake of simplicity, we can map the labels into
integer numbers, as ğ¿ = {1, 2, . . . , ğ‘}. They are to be grouped into
ğ‘˜=1 ğºğ‘˜. Accordingly, we can use ğœ†ğ‘– âˆˆ {1, 2, Â· Â· Â· , ğ¾} to represent
the cluster label of original label ğ‘™ğ‘–. Then, the clustering result can
be represented by a cluster label list: ğ€ = (ğœ†1, ğœ†2, Â· Â· Â· , ğœ†ğ‘).
Risk matrix R. The risk probability matrix and risk impact matrix
can be integrated into one single risk matrix with an element-wise
multiplication:
ğ¾ clusters {ğºğ‘˜|ğ‘˜ = 1, 2, . . . , ğ¾}, where ğºğ‘˜â€²ğ‘˜â€²â‰ ğ‘˜ ğºğ‘˜ = âˆ… and ğ¿ =
ğ¾
R = R âŠ™ I,
(16)
wherein each entry in R stands for the risk between classes of the
big network.