zi and zk(k ̸= i, j) are the latent vectors of a negative pair,
sim(·,·) measures the cosine similarity between two latent
vectors, I is an indicator function, τ represents a temperature
parameter, and exp is the natural exponential function. The
final contrastive loss is the sum of the contrastive loss for
all positive pairs. SimCLR learns the image encoder and
projection head via minimizing the final contrastive loss.
Using unlabeled (image, text) pairs: Another family of
self-supervised learning methods [7, 18–21] try to pre-train
an image encoder based on unlabeled (image, text) pairs. We
note that, other than an image encoder, these methods also
pre-train a text encoder, which takes a text (e.g., a sentence)
as an input and outputs a feature vector for it. Recently,
Radford et al. [7] proposed Contrastive Language-Image Pre-
training (CLIP) which learns an image encoder and a text
encoder using 400 million (image, text) pairs collected from
the Internet, and achieves state-of-the-art performance. Given
a batch of N (image, text) pairs (we call them positive pairs),
CLIP constructs N ×(N −1) negative (image, text) pairs, each
of which consists of the image in one positive pair and the text
in another positive pair. CLIP calculates a cosine similarity for
each positive/negative (image, text) pair. Specifically, given a
(image, text) pair, the image encoder produces a feature vector
for the image and the text encoder produces a feature vector
for the text; and CLIP calculates the cosine similarity between
the two feature vectors for the pair. Roughly speaking, CLIP
learns the image encoder and text encoder to maximize the
cosine similarity for the N positive pairs and minimize the
cosine similarity for the N × (N − 1) negative pairs.
B. Building a Downstream Classifier
The pre-trained image encoder can be used as a feature
extractor to build classifiers (called downstream classifiers)
for many downstream tasks. Depending on whether a labeled
training dataset is required or not, a downstream classifier
could be multi-shot classifier or zero-shot classifier. A multi-
shot classifier is trained via supervised learning with multiple
labeled training examples, while a zero-shot classifier requires
zero labeled training examples. Note that zero-shot classifier
requires both an image encoder and a text encoder, i.e., zero-
shot classifier is only applicable when the image/text encoder
is pre-trained based on (image, text) pairs.
Multi-shot classifier:
In this scenario, we have multiple
labeled training examples (we call them downstream dataset)
for the downstream task. We use the pre-trained image encoder
to produce a feature vector for each image in the downstream
dataset, and then we train a classifier via the standard super-
vised learning. Given a testing input, we first use the pre-
trained image encoder to produce a feature vector for it and
then use the trained classifier to predict a label for it.
Zero-shot classifier:
In this scenario, we have zero labeled
training examples for the downstream task, but both an image
encoder and a text encoder are available. To build a zero-shot
classifier for a downstream task, we first construct a context
sentence for each class of the downstream task. For instance,
Radford et al. [7] showed that a context sentence like “A
photo of a {class name}” can be a good default template that
outperforms the baseline of using only label text (i.e., “{class
name}”), where “{class name}” can be “stop sign”, “yield”,
“speed limit”, etc. when the downstream task is traffic sign
classification. Moreover, they also found that the accuracy of a
zero-shot classifier can be further improved by customizing the
context sentence for each downstream task. We can construct
c context sentences for the c classes of the downstream task.
We use the text encoder to produce a feature vector for each
context sentence. Given a testing image, we use the image
encoder to produce a feature vector for it. Then, the zero-shot
classifier predicts the testing image as the class whose context
sentence’s feature vector has the largest cosine similarity with
the image’s feature vector.
III. THREAT MODEL
We characterize our threat model with respect to attacker’s
goals, background knowledge, and capabilities.
Attacker’s goals: We consider an attacker aims to inject
backdoors into a pre-trained image encoder such that a
downstream classifier built based on the backdoored image
encoder makes attacker-chosen predictions for inputs embed-
ded with an attacker-chosen trigger. We consider attacking
image encoder instead of text encoder so our attacks are
applicable to both images and (image, text) pairs based self-
supervised learning. In particular,
the attacker first selects
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:37:52 UTC from IEEE Xplore.  Restrictions apply. 
2045
some downstream tasks that he/she aims to target. We call such
downstream tasks target downstream tasks. For each target
downstream task, an attacker could target one or more of its
classes, which we call target classes. For instance, when the
target downstream task is traffic sign recognition, the target
classes could be “priority traffic sign”, “stop sign”, etc.. For
simplicity, we use (Ti, yi) to denote a (target downstream task,
target class) pair, where i = 1, 2,··· , t. Note that multiple
Ti’s may represent the same target downstream task when the
attacker selects multiple target classes for it. For each (Ti, yi)
pair, the attacker selects a backdoor trigger ei, e.g., a patch
located at the bottom right corner of an input image.
The attacker aims to craft a backdoored image encoder
based on a clean image encoder to achieve two goals, i.e.,
effectiveness goal and utility goal.
• Effectiveness goal. The effectiveness goal means that,
when a downstream classifier (called backdoored down-
stream classifier) is built based on the backdoored image
encoder for a target downstream task Ti, the backdoored
downstream classifier should predict the target class yi for
any input embedded with the trigger ei. The backdoored
downstream classifiers for the target downstream tasks
should simultaneously have such backdoor behavior.
• Utility goal. The utility goal means that, the backdoored
image encoder should maintain utility to be stealthy. In
particular, for any target or non-target downstream task,
a downstream classifier built based on the backdoored
image encoder should be as accurate as a downstream
classifier built based on the clean image encoder for clean
testing inputs.
Attacker’s background knowledge and capabilities: We
consider
two possible attackers: 1) an untrusted service
provider who injects a backdoor into its pre-trained image
encoder and shares the backdoored encoder with downstream
customers (e.g., makes the backdoored encoder publicly avail-
able), and 2) a malicious third-party who obtains a clean
pre-trained image encoder from a service provider, injects
backdoors into it, and shares the backdoored encoder with
downstream customers (e.g., via republishing it for public
download on GitHub). Therefore, an attacker has access to
a clean pre-trained image encoder. We note that our attack is
only applicable when a downstream customer uses an image
encoder from an untrusted source and that our work shows one
example of how this encoder could have been compromised.
Moreover, we assume an attacker has access to a set of
unlabeled images, which we call shadow dataset. In particular,
we consider two scenarios depending on who is the attacker. In
the first scenario, the attacker is an untrusted service provider
who pre-trains the image encoder and thus the attacker has
access to the pre-training dataset, in which the attacker can
use the pre-training dataset as the shadow dataset. In the
second scenario where the attacker is a malicious third-party,
the attacker may not have access to the pre-training dataset and
thus the shadow dataset may not be the pre-training dataset.
In this scenario, we will consider shadow dataset that has
or does not have the same distribution as the pre-training
dataset. We also assume the attacker has access to some images
(called reference inputs) for each (target downstream task,
target class) pair, e.g., the attacker can collect such images
from the Internet. For instance, for the (traffic sign recognition,
“stop sign”) pair, the attacker can collect one or more stop sign
images from the Internet as the reference inputs. Note that the
reference inputs are not in the downstream dataset used to
build downstream classifiers. As we will see in Section IV,
our BadEncoder uses the shadow dataset and reference inputs
to inject backdoors into the image encoder.
We assume the attacker does not have access to the down-
stream dataset used to build downstream classifiers and cannot
tamper the training process of the downstream classifiers.
IV. DESIGN OF BADENCODER
A. Overview
Figure 1 shows an overview of BadEncoder. We aim to craft
a backdoored image encoder from a clean one to achieve the
effectiveness goal and utility goal. To achieve the effectiveness
goal, our idea is to modify the clean image encoder such that
1) it produces similar feature vectors for the reference inputs
and inputs in the attacker’s shadow dataset embedded with
the trigger for each (target downstream task, target class) pair,
and 2) it produces similar feature vectors for the reference
inputs with the clean image encoder. Therefore, a downstream
classifier built based on our backdoored image encoder still
predicts a reference input as the target class, and thus likely
predicts any input embedded with the corresponding trigger
as the target class. To achieve the utility goal, we modify
the clean image encoder such that our backdoored image
encoder and the clean image encoder produce similar feature
vectors for each clean input in the shadow dataset. Therefore,
a downstream classifier built based on our backdoored image
encoder will maintain its accuracy for clean testing inputs.
Formally, we formulate our BadEncoder as an optimization
problem, solving which gives us a backdoored image encoder
that achieves the two goals. In particular, we propose an
effectiveness loss and an utility loss to quantify the two goals,
respectively. Our optimization problem aims to minimize a
weighted sum of the losses.
B. Formulating our BadEncoder as an Optimization Problem
We denote a clean pre-trained image encoder and our
backdoored one as f and f′, respectively. For each (target
downstream task, target class) pair (Ti, yi), the attacker col-
lects a set of reference inputs Ri = {xi1, xi2,··· , xiri} from
the target class yi, where ri is the number of reference inputs
for the pair (Ti, yi) and i = 1, 2,··· , t. ei is the attacker-
chosen trigger for the pair (Ti, yi). x ⊕ ei means embedding
the trigger ei to an input x. We call x ⊕ ei a backdoored
input. We propose an effectiveness loss and an utility loss to
quantify the effectiveness goal and utility goal, respectively.
Next, we discuss them.
Effectiveness loss: To achieve the effectiveness goal, we
craft a backdoored image encoder such that it produces similar
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:37:52 UTC from IEEE Xplore.  Restrictions apply. 
2046
∑︁t
∑︁t
i=1
i=1
j=1
∑︁ri
∑︁
|Ds| ·∑︁t
∑︁ri
∑︁t
j=1 s(f′(xij), f (xij))
x∈Ds
L0 = −
L1 = −
feature vectors for the reference inputs Ri of a pair (Ti, yi)
and any input in the shadow dataset Ds embedded with the
trigger ei. Therefore, a backdoored downstream classifier built
based on our backdoored image encoder is likely to predict
the same label for the reference inputs Ri and any input
embedded with the trigger ei. However, this alone does not
guarantee that the backdoored downstream classifier predicts
the target class yi for an input embedded with the trigger
ei, because it may not correctly predict the target class for
the reference inputs. Therefore, we further require that the
backdoored image encoder produces similar feature vectors for
the reference inputs with the clean image encoder. Formally,
our effectiveness loss consists of the following two terms:
s(f′(x ⊕ ei), f′(xij))
i=1 ri
,
(2)
,
i=1 ri
(3)
where s(·,·) measures the similarity (e.g., cosine similarity)
between two feature vectors, |Ds| represents the number of
inputs in the shadow dataset, and the denominators in L0 and
L1 are used to normalize the losses. Our effectiveness loss is
a weighted sum of the two terms, i.e., L0 + λ1L1, where λ1
is a hyperparameter to balance the two terms. L0 is smaller
if the backdoored image encoder f′ produces more similar
feature vectors for the reference inputs and backdoored inputs
in the shadow dataset, while L1 is smaller if the backdoored
image encoder and clean image encoder produce more similar
feature vectors for the reference inputs.
Utility loss: Our BadEncoder aims to maintain the utility
of the backdoored image encoder, i.e., maintain the accuracy
of the downstream classifiers built based on our backdoored
image encoder for clean inputs. When both an image encoder
and a text encoder are pre-trained using (image, text) pairs,
a downstream classifier can be a zero-shot classifier. We note
that our attack only embeds backdoor to an image encoder to
be more generally applicable. Therefore, a backdoored zero-
shot classifier and its clean version may predict different labels
for a clean input if the backdoored image encoder and the
clean image encoder produce different feature vectors for
it. This is because the feature vectors produced by the text
Fig. 1: Overview of BadEncoder.
encoder do not change in the zero-shot classifiers. Based on
this observation, we require our backdoored image encoder
and the clean image encoder to produce similar feature vectors
for a clean input, e.g., a clean input in the shadow dataset.
Specifically, our utility loss is smaller if our backdoored
image encoder and the clean image encoder produce more
similar feature vectors for a clean input in the shadow dataset.
Formally, we define our utility loss as follows:
s(f′(x), f (x)).
L2 = − 1
|Ds| · ∑︂
(4)
x∈Ds
Optimization problem: After defining the three loss terms
L0, L1, and L2, we formulate our BadEncoder as an optimiza-
tion problem. Specifically, our backdoored image encoder is a
solution to the following optimization problem:
f′ L = L0 + λ1 · L1 + λ2 · L2,
min
(5)
where λ1 and λ2 are two hyperparameters to balance these
three loss terms. We will explore their impact on our BadEn-
coder in our evaluation. As our experimental results show, each
of the three loss terms is necessary for our BadEncoder to
achieve both the effectiveness goal and utility goal. Note that
we can also jointly optimize the backdoored image encoder f′
and the backdoor triggers ei’s (both locations of the triggers
and their pixel values) in (5). However, we find that our
BadEncoder with simple, physically realizable triggers (e.g., a
white square located at the bottom right corner of an image)
already achieves the two goals. Therefore, for simplicity, we
don’t optimize the triggers in this work and leave such joint
optimization as a future work.
C. Solving the Optimization Problem
An algorithm to solve the optimization problem in (5) is
an attack to craft a backdoored image encoder. Our BadEn-
coder solves the optimization problem using gradient descent.
Specifically, we initialize the backdoored image encoder as the
clean image encoder. In each epoch, we sample a mini-batch
of the shadow dataset, calculate the gradient of the loss L,
and move the backdoored image encoder a small step (called
learning rate) towards the inverse of the gradient. We repeat
the process for max epoch epochs. Algorithm 1 in Appendix
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:37:52 UTC from IEEE Xplore.  Restrictions apply. 
2047
“no entry”“priority”“four”“one”“car”“truck”task 2embedding backdoorbuilding downstream classifiersBadEncodertestingreference inputsshadow datasetbackdoor triggersshows our BadEncoder to solve the optimization problem,
where the function MINIBATCH samples a mini-batch of bs
inputs from the shadow dataset Ds.
V. EVALUATION
A. Experimental Setup
1) Datasets: We use the following five image datasets.
• CIFAR10 [22]: This dataset contains 50,000 training
images and 10,000 testing images. Each image has a size
of 32×32×3 and belongs to one of 10 classes.
• STL10 [23]: This dataset contains 5,000 labeled training
images and 8,000 labeled testing images, each of which
has a size of 96×96×3. Moreover, the dataset contains 10
classes and each image belongs to one of them. Besides
the labeled training and testing images, the dataset also
contains 100,000 unlabeled images.
• GTSRB [24]: This dataset contains 51,800 traffic sign
images in 43 categories. Each image has a size of
32×32×3. The dataset is divided into 39,200 training
images and 12,600 testing images.
• SVHN [25]: In this dataset, each image represents a
digit from the house numbers in Google Street View. The
size of each image is 32×32×3. Moreover, each image
belongs to one of the 10 digits. This dataset has 73,257
training images and 26,032 testing images. According
to [25], they introduced some distracting digits to the
sides of the digit of interest, i.e., SVHN is a noisy dataset.
• Food101 [26]: This dataset contains 101,000 images of
101 food categories. We note that this dataset is only used
to study the impact of the shadow dataset on BadEncoder.
2) Pre-training image encoders: When a dataset is used to
pre-train an image encoder, we call it pre-training dataset.
In our experiments, we use CIFAR10 or STL10 as a pre-
training dataset since they contain more images and are not
noisy datasets. In particular, when CIFAR10 is a pre-training
dataset, we use its training images (excluding the labels) to
pre-train an image encoder; and when STL10 is a pre-training
dataset, we further consider its unlabeled images when pre-
training an image encoder. Note that we do not use the
testing images when pre-training an image encoder because
we reserve them to evaluate our BadEncoder when our shadow
dataset is different from the pre-training dataset but they have
the same distribution. Given a pre-training dataset, we use
SimCLR [4] to train a ResNet18 [27] as an image encoder.
Our implementation is based on the publicly available code of
SimCLR [28, 29]. We train an image encoder for 1,000 epochs
with the Adam optimizer and initial learning rate 0.001.
3) Training downstream classifiers: Given an (backdoored)
image encoder pre-trained using a pre-training dataset, we use
it to train downstream classifiers (i.e., multi-shot classifiers)
for the remaining three datasets. When a dataset is used to
train a downstream classifier, we call it downstream dataset.
For instance, given an image encoder pre-trained on CIFAR10,
we use it
to train downstream classifiers for downstream
datasets STL10, GTSRB, and SVHN. In particular, we use
a fully connected neural network with two hidden layers as a
downstream classifier for a downstream dataset. The number
of neurons in the two hidden layers are 512 and 256, respec-
tively. When a dataset is treated as a downstream dataset,
we use its training dataset to train a downstream classifier
and we use its testing dataset to evaluate the downstream
classifier. Specifically, we adopt the cross-entropy loss function
and Adam optimizer when training a downstream classifier.
Moreover, we train a downstream classifier for 500 epochs
with an initial
learning rate 0.0001. When a downstream
classifier is trained based on a backdoored image encoder, we
call it backdoored downstream classifier.