### 5.1 Evaluation Setup

To account for the random nature of fuzzing, we follow recent guidelines for evaluating fuzz testing [17]. Specifically, we:
- Perform multiple trials and use statistical tests.
- Evaluate different seeds.
- Consider longer timeouts.
- Assess bug-finding performance using ground truth with benchmark test suites and real programs.

#### Programs
We used the LAVA-M benchmark programs [11] and real-world programs from GNU binutils, ffmpeg, libav, libtiff, and libarchive for evaluation. The LAVA-M dataset is widely used for comparing recent fuzzers such as VUzzer and Qsym [8, 19, 27, 33]. The binaries in LAVA-M include various conditions, including multi-byte constraints, to reach buggy code. However, to avoid overfitting, it is crucial to include real-world programs like ffmpeg in the evaluation. 

- **GNU binutils** [1]: A set of programs for creating or reading binary programs, object files, and libraries.
- **ffmpeg and libav** [2]: Free software tools and libraries for processing audio and media files.
- **libtiff** [18]: A free software library for processing TIFF images.
- **libarchive**: A library for reading and writing streaming archives.

These real-world programs have been used in recent studies and even by commercial fuzzers for bug discovery [4, 12, 27, 33]. Table 6 summarizes the program data.

#### Fuzzers
We selected baseline fuzzers for evaluation:
- **LAVA-M benchmarks (§5.2.1)**: Qsym and VUzzer.
- **Real-world programs (§5.2.2) and code coverage (§5.3)**: Qsym and AFL.

In LAVA-M experiments, Intriguer and VUzzer use a single core, while Qsym uses three cores for hybrid fuzzing. In real program experiments, both Intriguer and Qsym perform hybrid fuzzing.

#### Seeds
The executable format (e.g., ELF or PE) files are compiled from 22 lines of C++ code containing a simple class, using g++ and Visual Studio (§D). The MP4 file was downloaded from the sample-videos site [7] and reduced to two video frames using ffmpeg. The TIFF files were downloaded from the go-fuzz-corpus repository [8] and reduced from 215 to 69 files using afl-cmin. The tar file was downloaded from FoRTE-Research [24]. Table 7 summarizes the seed data.

#### Platform and Configuration
Experiments were conducted on a machine with a 2.40GHz Xeon CPU and 384GB RAM, running 64-bit Ubuntu 16.04. The comparison configuration was as follows:
- **LAVA-M dataset**: Fuzzed for five hours (and specifically 5h and 24h for "who" in §5.2.1).
- **Real-world programs**: Fuzzed for 24 hours (§5.2.2, §5.3, §5.4, §5.5, and §5.6).
- **ffmpeg**: Fuzzed for five days (§5.2.2).

### 5.2 RQ1: Bug Discovery Capability

To answer RQ1, we compare the following with other fuzzers:
- **Number of bugs identified in LAVA-M**.
- **Number of distinct bugs discovered in real-world programs**.

For distinct bugs, we performed triage based on unique patches when the programs were patched [17]. Otherwise, we manually deduplicated based on call stacks.

#### 5.2.1 LAVA-M Dataset
To compare Intriguer with VUzzer (a taint-based fuzzer) and Qsym (a concolic-based hybrid fuzzer), we performed 20 runs of fuzzing for each fuzzer on the buggy programs of LAVA-M. Table 8 shows the results of 5-hour fuzzing on each program in the LAVA-M dataset. The first two columns list the target program names and the number of artificial bugs listed by the LAVA authors [11]. The middle columns show the median values of bugs identified by each fuzzer, and the last three columns show the maximum values.

The LAVA-M benchmark results show that Intriguer outperforms state-of-the-art fuzzers. Qsym and VUzzer struggled to find all bugs hidden behind multi-byte constraints within the time budget. VUzzer detected only 4.4% of bugs, and Qsym detected 58.2% of bugs, respectively, once in 20 runs. In contrast, Intriguer found 100% of bugs within the same time budget, twice in 20 runs. When running for 24 hours, Intriguer found all the bugs nine times in 20 runs (Figure 8). Intriguer also found unlisted bugs, though they are not shown in Table 8. For example, in "who," Intriguer identified 2464 bugs when all listed bugs (2136 + 328) were found.

Figure 8 depicts the cumulative number of listed bugs found by Intriguer and Qsym in "who" over time. Intriguer found more than 1,500 bugs in 5 minutes, while Qsym slowed down after 2 hours and found fewer than 1,500 bugs in 5 hours. Qsym found more bugs after 12 hours but never succeeded in finding all bugs within 24 hours. This result indicates that Intriguer's field-level constraint solving approach is effective in discovering new test cases and bugs in a shorter time.

#### 5.2.2 Real-world Programs
In all programs, Intriguer found more bugs than AFL, indicating that Intriguer can solve complicated constraints that coverage-based fuzzers (like AFL) cannot, thus creating new test cases and finding more bugs. Compared to Qsym, Intriguer found more bugs in objdump, nm, readelf, ffmpeg, and tiff2pdf. This suggests that Intriguer can discover bugs more efficiently through trace reduction and field-level constraint solving to optimize symbolic emulation. Only in avconv did Qsym find more bugs than Intriguer, with Qsym and Intriguer finding three and two unique bugs, respectively. This result is attributed to Qsym's slightly different seed prioritization strategy, where the most recently generated test case is selected among test cases with the same priority.

Table 9 shows the number of bugs found by Intriguer, Qsym, and AFL in real-world programs over 24 hours. Parentheses indicate the number of bugs that the other two fuzzers could not find.

### 5.3 New Bugs Discovered in Real-world Programs

Table 10 lists the new bugs discovered in real-world programs, including their types and whether they have been fixed. The bugs are categorized by project and type, and the table includes the report IDs and mailing list references.

---

This revised text is more structured, clear, and professional, making it easier to understand the evaluation setup and the results of the fuzzing experiments.