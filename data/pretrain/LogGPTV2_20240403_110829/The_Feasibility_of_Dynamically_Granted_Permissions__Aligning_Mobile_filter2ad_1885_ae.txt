c
A
0.7
0.6
x
o
+
1
x
o
+
2
x
o
+
3
x
o
+
4
x
o
+
5
x
o
x
o
x
o
x
o
x
o
x
o
+
x
o
+
+
+
+
+
+
Contextuals
Population
+++
xxx
ooo
Overall
Defaulters
6
7
8
9
10
11
12
Prompt Count
ﬁnd a way to differentiate between Defaulters and Contextuals
early in the bootstrapping phase to determine which users
require fewer prompts. The analysis of our hybrid approach
addresses the concern of a high number of permission prompts
initially for an ML approach. Over time, accuracy can always
be improved with more prompts.
Our new hybrid approach of using AOFU-style permission
prompts in the bootstrapping phase to train our model can
achieve higher accuracy than AOFU, with signiﬁcantly fewer
prompts. Having a learning strategy (use of AOFU) over ran-
dom selection helped to minimize user involvement (24 vs. 9)
while signiﬁcantly reducing the error rate (7.6% vs. 3.2%)
over a random selection of the training set.
Fig. 3. How the median accuracy varies with the number of seen prompts
B. Decision Conﬁdence
signiﬁcantly reduce user involvement in the learning phase.
After 12 prompts, accuracy reached 96.8% across all users.
Each new user starts off with a single model shared by
all new users and then moves onto a separate model trained
with AOFU prompt responses. We analyze its performance for
Defaulters and Contextuals separately, ﬁnding that it improves
accuracy while reducing user involvement in both cases, com-
pared to the status quo.
We ﬁrst examine how our model performs for Defaulters,
53% of our sample. Figure 3 shows that our model trained
with AOFU permission-prompt responses outperforms AOFU
from the very beginning. The model starts off with 96.6%
accuracy (before it reaches close to 100% after 6 prompts),
handily exceeding AOFU’s 93.33%. This is a 83.3% reduction
in permission prompts compared to AOFU-AP (the status quo).
Even with such a signiﬁcant reduction in user involvement,
the new approach cuts the prediction error rate in half.
Contextuals needed more prompts to outperform the AOFU
policy; the hybrid approach matches AOFU-AP with just 7
prompts, a 42% reduction in prompts. With 12 permission
prompts, same as needed for AOFU-AP, the new approach
had reduced the error rate by 43% over AOFU-AP (the status
quo). The number of prompts needed to reach this level of
accuracy in the new approach is 25% less than what is needed
for AOFU-APV. We also observed that as the number of
prompts increased, the AUC of our predictions also similarly
increased. Overall, the proposed learning strategy reduced the
error rate by 80% after 12 user prompts over AOFU-AP.
Given, Defaulters plateau early in their learning cycle (after
only 6 prompts), the proposed learning strategy, on average,
needs 9 prompts to reach its maximum capacity, which is a
25% reduction in user involvement over AOFU-AP.
Contextuals have a higher need for user involvement than
Defaulters, primarily because it is easy to learn about De-
faulters, as they are more likely to be consistent with early
decisions. On the other hand, Contextuals vary their decisions
based on different contextual cues and require more user
involvement for the model to learn the cues used by each user
and how do they affect their decisions. Thus, it is important to
In the previous section, we looked into how we can optimize
the learning phase by merging AOFU and the ML model to
reach higher accuracy with minimal user prompts. However,
for a small set of users, more permission prompts will not
increase accuracy, regardless of user involvement in the boot-
strapping phase. This could be due to the fact that a portion
of users in our dataset are making random decisions, or that
the features that our ML model takes into account are not
predictive of those users’ decision processes. While we do
not have the data to support either explanation, we examine
how we can measure whether the ML model will perform
well for a particular user and quantify how often it does not.
We present a method to identify difﬁcult-to-predict users and
reduce permission prompting for those users.
While running the experiment in §VII-A, we also measured
how conﬁdent the ML model was for each decision it made. To
measure the ML model’s conﬁdence, we record the probability
for each decision; since it is a binary classiﬁcation (deny or
allow), the closer the probability is to 0.5, the less conﬁdent
it is. We then chose a class probability threshold above which
a decision would be considered a high-conﬁdence decision.
In our analysis, we choose a class probability threshold of
0.6, since this value resulted in >96% accuracy for our fully-
trained model (≈25 prompts per user) for high-conﬁdence
decisions, but this is a tunable threshold. Thus, in the re-
mainder of our analysis, decisions that the ML model made
with a probability of >0.60 were labeled as high-conﬁdence
decisions, while those made with a probability of <0.60 were
labeled as low-conﬁdence decisions.
Since the most accurate version of AOFU uses 12 prompts,
we also evaluate the conﬁdence of our model after 12 AOFU-
style prompts. This setup is identical to the bootstrapping
approach; the model we evaluate here is trained on responses
from other users and the ﬁrst 12 prompts chosen by AOFU.
With this scheme, we found that 10 users (7.63% of 131
users) had at least one decision predicted with low conﬁdence.
The remaining 92.37% of users had all privacy decisions
predicted with high conﬁdence. Among those users whose
decisions were predicted with low conﬁdence, the proportion
of low-conﬁdence decisions on average accounted for 17.63%
(median = 16.67%) out of all their predicted decisions. With
1087
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:19:17 UTC from IEEE Xplore.  Restrictions apply. 
a sensitive permission request once every 15 seconds [43],
prompting even for 17.63% of predictions is not practical.
Users who had low-conﬁdence predictions had a median accu-
racy of 60.17%, compared to 98% accuracy for the remaining
set of users with only high-conﬁdence predictions. Out of the
10 users who had low-conﬁdence predictions, there were no
Defaulters. This further supports the observation in Figure 3
that Defaulters require a shorter learning period.
In a real-world scenario, after the platform (ML model)
prompts the user for the ﬁrst 12 AOFU prompts, the plat-
form can measure the conﬁdence of predicting unlabeled
data (sensitive permission requests for which the platform
did not prompt the user). If the proportion of low-conﬁdence
predictions is below some threshold,
the ML model can
be deemed to have successfully learned user privacy pref-
erences and the platform should keep on using the regu-
lar permission-prompting strategy. Otherwise,
the platform
may choose to limit prompts (i.e., two per unique applica-
tion:permission:visibility combination). It should also be noted
that rather than having a ﬁxed number of prompts (e.g., 12) to
measure the low-conﬁdence proportion, the platform can keep
track of the low-conﬁdence proportion as it prompts the user
according to any heuristic (i.e., unique combinations). If the
proportion does not decrease with the number of prompts, we
can infer that the ML model is not learning user preferences
effectively or the user is making random decisions, indicating
that
limiting prompts and accepting lower accuracy could
be a better option for that speciﬁc user, to avoid excessive
prompting. However, depending on which group the user is
in (Contextual or Defaulter), the point at which the platform
could make the decision to continue or limit prompting could
change. In general, the platform should be able to reach this
deciding point relatively quickly for Defaulters.
Among participants with no low-conﬁdence predictions, we
had a median error rate of 2% (using the new hybrid approach
after 12 AOFU prompts); for the same set of users, AOFU
could only reach a median error rate of 13.3%. However, using
AOFU, a user in that set would have needed an average of
15.11 prompts to reach that accuracy. Using the ML model,
a user would need just 9 prompts on average (Defaulters
require far fewer prompts, dropping the average); the model
only requires 60% of the prompts that AOFU requires. Even
with far fewer prompts in the learning phase, the ML model
achieves a 84.61% reduction in error rate relative to AOFU.
C. Online Model
Our proposed system relies on training models on a trusted
server, sending it to client phones (i.e., as a weight vector),
and having phones make classiﬁcations. By utilizing an online
learning model, we can train models incrementally as users
respond to prompts over time. There are two key advantages
to this: (i) this model adapts to changing user preferences over
time; (ii) it distributes the overhead of training increasing the
practicality of locally training the classiﬁer on the phone itself.
Our scheme requires two components: a feature extraction
and storage mechanism on the phone (a small extension to our
existing instrumentation) and a machine learning pipeline on
a trusted server. The phone sends feature vectors to the server
every few prompts, and the server responds with a weight
vector representing the newly trained classiﬁer. To bootstrap
the process, the server’s models can be initialized with a model
trained on a few hundred users, such as our single model across
all users. Since each user contributes data points over time,
the online model adapts to changing privacy preferences even
if they conﬂict with previous data. When using this scheme,
each model takes less than 10 KB to store. With our current
model, each feature and weight vector are at most 3 KB each,
resulting in at most 6 KB of data transfer per day.
To evaluate the accuracy of our online model, we trained
a classiﬁer using stochastic gradient descent (SGD) with ﬁve-
fold cross validation on our 4,224-point data set. This served
as the bootstrapping phase. We then simulated receiving the
remaining data one-at-a-time in timestamp order. Any features
that changed with time (e.g., running averages for aggregate
features, event counts) were computed with each incoming
data point, creating a snapshot of features as the phone would
see it. We then tested accuracy on the chronologically last
20% of our dataset. Our SGD classiﬁer had 93.8% accuracy
(AUC=0.929). We attribute the drop in accuracy (compared
to our ofﬂine model) to the fact that running averages take
multiple data points to reach steady-state, causing some earlier
predictions to be incorrect.
A natural concern with a trusted server is compromise.
To address this concern, we do not send any personally-
identiﬁable data to the server, and any features sent to the
server are scaled; they are reported in standard deviations from
the mean, not in raw values. Furthermore, using an online
model with incremental training allows us to periodically train
the model on the phone (i.e., nightly, when the user is charging
her device) to eliminate the need for a trusted server.
VIII. CONTEXTUAL INTEGRITY
While our model may not perform well for all users, it does
seem to work quite well for the majority of users (92.37% of
our sample). We provide a way of quickly identifying users for
whom our system does not perform well, and propose limiting
prompts to avoid excessive user burden for those users, at the
cost of reduced efﬁcacy. In the worst case, we could simply
employ the AOFU model for users our system does not work
well for, resulting in a multifaceted approach that is at least
as good as the status quo for all users.
Contextual integrity is a conceptual framework that helps
explain why most permission models fail
to protect user
privacy—they often do not take the context surrounding pri-
vacy decisions into account. In addressing this issue, we
propose an ML model that infers when context has changed.
We believe that this is an important ﬁrst step towards opera-
tionalizing the notion of contextual integrity. In this section,
we explain the observations that we made in §VI-C based on
the contextual integrity framework proposed by Barth et al. [6].
1088
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:19:17 UTC from IEEE Xplore.  Restrictions apply. 
Contextual integrity provides a conceptual framework to
better understand how users make privacy decisions; we use
Barth et al.’s formalized model [6] as a framework in which to
view Android permission models. Barth et al. model parties as
communicating agents (P ) knowing information represented
as attributes (T ). A knowledge state κ is deﬁned as a subset
of P ×P ×T . We use κ = (p, q, t) to mean that agent p knows
attribute t of agent q. Agents play roles (R) in contexts (C).
For example, an agent can be a game application, and
have the role of a game provider in an entertainment context.
Knowledge transfer happens when information is communi-
cated between agents; all communications can be represented
through a series of traces (κ, (p, r), a), which are combinations
of a knowledge state κ, a role state (p, r), and a communi-
cation action a (information sent). The role an agent plays
in a given context helps determine whether an information
ﬂow is acceptable for a user. The relationship between the
agent sending the information and the role of the agent ((p, r))
receiving the information must follow these contextual norms.
With the Android permission model, the same framework
can be applied. Both the user and the third-party applica-
tion are communicating agents, and the information to be
transferred is the sensitive data requested by the applica-
tion. When a third-party application requests permission to
access a guarded resource (e.g., location), knowledge of the
guarded resource is transferred from the one agent (i.e., the
user/platform) to another agent (i.e., the third-party applica-
tion). The extent to which a user expects a given request
depends not on the agent (the application requesting the data),
but on the role that agent is playing in that context. This
explains why the application as a feature itself (i.e., application
name) was not predictive in our models: this feature does not
represent the role when determining whether it is unexpected.
While it is difﬁcult for the platform to determine the exact role