title:A Markov Reward Model for Reliable Synchronous Dataflow System Design
author:Vinu Vijay Kumar and
Rashi Verma and
John Lach and
Joanne Bechta Dugan
A Markov Reward Model for Reliable Synchronous Dataflow System Design
Vinu Vijay Kumar, Rashi Verma, John Lach, Joanne Bechta Dugan 
Electrical and Computer Engineering Department 
University of Virginia 
{vv6v, rashi, jlach, jbd}@virginia.edu
ABSTRACT
The  design of quality digital systems depends on models 
that  accurately  evaluate  various  options  in  the  design 
space  against  a  set  of  prioritized  metrics.  While 
individual  models  for  evaluating  area,  performance, 
reliability,  power,  etc.  are  well  established,  models 
combining  multiple  metrics  are  less  mature.  This  paper 
introduces  a  formal  methodology  for  comprehensively 
analyzing performance, area and reliability in the design 
of  synchronous  dataflow  systems  using  a  novel  Markov 
Reward Model. A Markov chain system reliability model 
is constructed for various design options in the presence 
of  possible  component  failures,  and  high-level  synthesis 
techniques  are  used  to  associate  performance  and  area 
rewards  with  each  state  in  the  chain.  The  cumulative 
reward  for  a  chain  is  then  used  to  evaluate  the 
corresponding  design  option  with  respect  to  the  metrics 
of interest. Application of the model to a benchmark DSP 
circuit  provides 
into  reliable  synchronous 
dataflow system design. 
insights 
1. INTRODUCTION
When  designing  a  digital  system,  a  number  of  metrics 
must  be  considered,  including  performance,  cost,  power, 
testability,  reliability,  etc.  A  designer  typically  identifies 
particular  metrics  to  emphasize  based  on  the  system’s  target 
application  and  works  towards  improving  the  system  against 
those metrics, often trading off other metrics in the process. The 
designer’s ability to make quality tradeoffs and design decisions 
depends on system models that accurately evaluate a particular 
design option against the relevant metrics. Over the years, such 
system  models  have  been  developed  and  are  used  to  drive 
digital  system  design,  but  models  for  certain  metrics  (and 
combinations of metrics) are more mature than others. 
demands 
performance 
This  paper  introduces  a  novel  Markov  Reward  Model 
(MRM)  for  evaluating  synchronous  dataflow  system  designs 
against  reliability,  performance  and  area  metrics.  Increasingly 
challenging 
by  modern 
applications have led to the use of application specific hardware 
systems,  particularly  in  digital  signal  processing  (DSP)  and 
embedded  systems.  Such  systems  are  often  used  in  safety 
critical applications and in inaccessible environments that do not 
allow for easy repair. Therefore, these systems must be designed 
for  high  performance  but  also  must  have  high  reliability  and 
fault  tolerance  capabilities.  Current  system  models  separate 
posed 
performance and area analysis from reliability analysis, but the 
technique introduced here allows for direct optimization of these 
metrics with a single MRM. Such a model allows for the system 
design space to be more efficiently and accurately explored. 
1.1 Motivational Example 
Consider 
the  synchronous  dataflow  DSP  application 
represented  by  the  dataflow  graph  (DFG)  in  Figure  1.  When 
designing  a  system  for  such  an  application,  metrics  must  be 
prioritized.  This  is  a  well-understood  problem  when  designing 
purely  for  performance  or  area,  as  scheduling  techniques  have 
been  developed  to  minimize  the  number  of  control  steps  or  to 
minimize the number of components. 
(cid:13)
v1
v3
(cid:13)
v2
-
v5
-
v4
v6
(cid:13)
+
Figure 1. Sample DSP application 
For example, based on data dependencies, this application 
could be carried out in a minimum of three control steps using 
the first control DFG (CDFG 1) shown in Figure 2, providing a 
throughput of 1/3=0.33 with two adders (A) and two multipliers 
(M).  Alternatively,  the  application  could  be  implemented  with 
only  one  adder  and  one  multiplier  but  in  four  control  steps 
(throughput=0.25), as shown by CDFG 2 in Figure 2. If an ALU 
capable  of  both  multiplication  and addition were available, the 
application could be implemented with a single ALU but in six 
control steps (throughput=0.17) as shown in CDFG 3. Based on 
the application’s metric priorities, the appropriate CDFG can be 
selected. 
However, when reliability and fault tolerance are included 
as  system  priorities,  the  design  space  becomes  larger,  and  the 
design options become more difficult to evaluate. Consider that 
the  system  is  to  be  designed  to  be  as  fast  as  possible  given 
available (i.e. non-faulty) resources, with system cost (i.e. area) 
being a secondary priority. When a resource fails, an alternative 
schedule can be derived to continue system operation, albeit at a 
lower throughput, using techniques such as the component-level 
fault  tolerance  technique  presented  in  [3].  To  maximize  the 
initial system throughput, CDFG 1 is selected. However, various 
component allocations are possible to realize this schedule: (C1: 
2A,  2M,  0ALU),  (C2:  1A,  1M, 1ALU),  (C3:  0A,  0M,  2ALU). 
Assuming relative component areas of the adder, multiplier and 
ALU  to  be  1,  2  and  3,  respectively,  all  three  allocations  have 
equivalent  areas,  making  them  equally  attractive  options  with 
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 05:35:49 UTC from IEEE Xplore.  Restrictions apply. 
CDFG 1
CDFG 2
CDFG 3
v1
(cid:13)
v2
v3
(cid:13)
-
v5
-
v4
v6
(cid:13)
+
v1
(cid:13)
v2
v3
(cid:13)
-
v5
-
v1
(cid:13)
v2
-
v4
v6
(cid:13)
+
v3
(cid:13)
v5
-
v4
v6
(cid:13)
+
Figure 2. CDFGs based on available components 
respect to the area metric. Therefore, before component failures 
are considered, the allocations cannot be distinguished. 
The  model  presented  here  enables  more  in-depth  analysis 
of  various  system  options  by  considering  the  possible  failures 
that  may  occur.  For  example,  consider  a  single  component 
failure.  For  allocations  C1  and  C2,  the  application  can  be 
reconfigured to implement CDFG 2, reducing the throughput to 
0.25.  In  case  of  allocation  C3,  however,  a  single  component 
failure leaves only one ALU, forcing the application to move to 
the CDFG 3 with a throughput of 0.17. This analysis begins to 
reveal  quality  differences  between  the  implementations  with 
respect to relevant metrics. 
the  concept  of 
To  continue  this  performance/reliability  analysis  for  all 
component  failure  probabilities  (including  combinations  of 
failures),  our  model  defines 
‘expected 
throughput’ (ET) by determining the probability that a system is 
in  a  particular  state  (i.e.  has  certain  resources  available)  at  a 
particular  time  and  the  throughput  of  the  system  in  that  state. 
For example, assume a component failure probability of 0.1 for 
a given system lifetime for the adder, multiplier, and ALU. To 
simplify the analysis, consider that at most one component can 
fail. Therefore, the probabilities that allocations C1, C2, and C3 
are fault free at the end of the system lifetime are (0.9)^4=0.66, 
(0.9)^3=0.73, 
the 
probabilities  for  being  in  a state of having had one component 
fail is 1-0.66=0.34, 1-0.73=0.27, and 1-0.81=0.19, respectively. 
Therefore,  the  ET  for  each  allocation  at  the  end  of  the  system 
lifetime is: 
and 
(0.9)^2=0.81, 
respectively, 
and 
ET(C1)=(0.66*0.33)+(0.34*0.25)=0.302
ET(C2)=(0.73*0.33)+(0.27*0.25)=0.308
ET(C3)=(0.81*0.33)+(0.19*0.17)=0.300
This  analysis  reveals  that  the  allocation  C2  provides  the 
highest expected throughput. For a more complete analysis, we 
must  also  consider  the  probabilities  of  being  in  other  states  of 
component availability, including the states in which the system 
can no longer function (i.e. throughput=0). In addition, we must 
introducing  a 
consider the additional reliability and ET provided by redundant 
components.  While 
redundant  adder  and 
multiplier into allocation C1, for example, does not immediately 
improve  the  throughput,  the  ET  may  increase  due  to  the 
increased  system  reliability.  This  increased  ET  provided  by 
redundant  components  must  then  be  weighed  against  the 
increased system area and any other relevant metric. 
This  example,  while  simplistic,  illustrates  how  reliability 
considerations  can  influence  design  decisions.  The  available 
configurations in a real system span a much broader range than 
shown  here,  making  manual  exploration  of  the  design  space 
impossible. A formal design method akin to ‘performability’ in 
large-scale  multiprocessing  computer  systems  is  therefore 
needed  to  design  systems  with  component  redundancy  and/or 
rescheduling as the fault tolerance mechanisms. 
In this paper, we present the first formal method to analyze 
design  options  in  the  synthesis  of  synchronous  dataflow 
systems. The method uses MRMs to predict rewards associated 
with  different  system  design  options, 
including  various 
component  allocations,  fault  tolerance  schemes,  etc.  The 
rewards  are  calculated  based  on  a  designer  specified  ‘weight’ 
for various metrics, thereby providing a quantitative estimate of 
the quality of the system. (The metrics considered in this paper 
are  area,  throughput,  and  reliability,  but  the  technique  can  be 
extended  to  include  metrics  such  as  power  and  testability  by 
specifying  additional  rewards.)  This  reward  model  can  be 
integrated  into  existing  design  tools  to  drive  digital  system 
design.
The  paper  is  organized  as  follows.  Section  2  provides 
background  work  from  the  literature.  Section  3  describes 
synchronous dataflow systems and then goes on to introduce the 
Markov  chain  reliability  model  for  such  systems.  Section  4 
describes the MRM and how the design quality metrics translate 
into  rewards  in  our  model.  Section  5  presents  a  detailed  case 
study of an actual system, using our model to drive the design. 
Section 6 concludes the paper. 
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 05:35:49 UTC from IEEE Xplore.  Restrictions apply. 
2. BACKGROUND
The  design  of  synchronous  dataflow  systems  is  a  well-
studied topic, and many algorithms exist for high-level synthesis 
of  such  systems  [8],  such  as  force  directed  list  scheduling  [9] 
and  weighted  bipartite  matching  [5]  for  component  allocation 
and  binding.  In  [13],  we  presented  a  scheduling  and allocation 
algorithm  for  integrating  into  synchronous  dataflow  systems 
area-efficient flexible components capable of executing multiple 
arithmetic  operations,  providing  significant  area  savings  for  a 
variety of DSP systems. 
As  discussed  in  the  motivational  example  above,  these 
high-level  synthesis  techniques  can  generate  different  CDFGs 
based on component availability [3]. Therefore, the system can 
recover  from  a  variety  of  component  failures  by  switching  to 
alternate  CDFGs  without  requiring  any  initial  component 
redundancy. CDFG schedules with more control-steps (c-steps) 
have reduced throughput, but for many applications, processing 
lesser amounts of data or lossy, lower quality data processing is 
better than no processing at all. 
technique  by 
Component-level  redundancy  was  also  first  proposed  in 
[3].  Rather  than  copying  an  entire  system,  a  single  redundant 
component of each type is added to provide redundancy for all 
of  the  components  of  that  type  in  the  system.  Recently,  we 
enhanced 
‘heterogeneous 
redundancy’  as  a  fault  tolerance  mechanism  using  flexible 
components,  which  are  capable  of  providing  redundancy  for  a 
variety  of  operations  [14].  In  all  of  the  above  papers,  no 
technique 
the 
reliability/throughput/area  tradeoffs  in  designing  such  systems. 
The  modeling  technique  described  in  this  paper  provides  the 
formal methodology for such analysis. 
introducing 
to 
analyze 
this 
was 
presented 
Much  work  has  been  done  in  modeling  the  behavior  of 
gracefully  degradable 
large-scale  computer  systems  using 
continuous  time  Markov  chain  reward  models  [2,6,12].  (A 
general  overview  of  Markov  reward  models  can  be  found  in 
[4]). A case study on measuring the ‘performability’ of a large-
scale  computer  system  using  a  Markov  reward  model  was 
presented in [11]. While our modeling approach has similarities 
to  performability  analysis,  the  specific  nature  of  synchronous 
dataflow  systems  (as  opposed  to  multi-processor  computer 
systems),  the  additional  metrics  considered  (e.g.  area,  power, 
etc.),  and  the  motivation  behind  the  approach  (i.e.  to  drive 
synthesis  and  design,  not  just  to  perform  system  analysis) 
distinguish our approach. 
3. THE SYSTEM MODEL 
3.1 Synchronous Dataflow: Hardware Model 
Application  Specific  Integrated  Circuit  (ASIC)  synthesis 
for DSP circuits is carried out using different design styles, such 
as  macro-cell-based,  bus-oriented,  bit-sliced,  and  array-based 
design [8].  The  macro-cell  based  design  style,  which  uses pre-
designed macros for datapath operations, will be used hereafter 
in this paper for description purposes, as it is the most common 
design style for DSP circuits. (The methodology described can 
be  used  for  the  other  design  styles  as  well  without  any  major 
changes.)
The  macro-cell-based  system  consists  of  three  basic 
elements: the datapath functional units, the control units and an 
interconnect  network  with  storage  registers.  The  design  starts 
with a description of the system, typically in the form of a DFG. 
The DFG is then scheduled, matching operations to c-steps for 
execution. Next, the number and type of components needed is 
determined  during  the  allocation  process.  This  is  followed  by 
binding, which maps operations to physical components, and the 
interconnect  structure  and  registers  are  also  determined.  The 
registers  are 
in  a  register  file.  The 
interconnect  network  consists  of  MUXes  or  buses that connect 
registers  to  datapath  component  inputs  ports  and  component 
output  ports  to  the  register  file.  The  controller  generates 
‘steering’  signals  for  the  interconnect  network  that,  along  with 
the component configuration signals, animate the datapath. The 
controller  is  normally  designed  as  a  finite  state  machine 
implemented either as a hardwired controller or as a microcoded 
state  machine.  Figure  3  illustrates  the  structural  view  of  a 
typical synchronous dataflow system. 
typically  collected 
3.2 Markov Chain Reliability Model for 
Synchronous Dataflow Systems 
Continuous-time  Markov  chain  models  are  often  used  to 
model availability of gracefully degradable computer systems in 
the  presence  of  failures.  Let  X  =  {X(t), t>0}  be  the  stochastic 
state variable that describes the structure of the system at time t
in  terms  of  which  components  are  down  (i.e.  have  failed)  and 
which  components  are  up  (i.e.  functional).  Such  a  variable 
satisfies  the  Markov  memoryless  property:  all  past  state 
information is irrelevant, as is the amount of time already spent 
in  the  current  state,  as  component  faults  are  assumed  to  be 
exponentially  distributed.  Hence,  system  reliability  can  be 
modeled  as  a  continuous-time  Markov  process  chain  with  the 
probability  of  being  in  each  state  evaluated  by  solving  the 
underlying  differential  equations  governing  the  transitions 
between  states.  A  function  A(X)  can  now  be  defined  such  that 
(A(X)  =  1,  if  the  system  is  up;  0  otherwise).  Such  a  function 
represents the ‘availability’ of the system for execution. 
Some terminology must be defined and assumptions stated 
before  describing  the  Markov  chain  availability  model  for 
synchronous dataflow systems. 
Def  1:  The  ‘configuration’  of  a  system  is  an  allocation  of 
components  that  implements  a  DFG  without  any  component 
failures. It is the initial set of available components before any 
failure has occurred. The configuration is a static qualifier for a 
system. 
Def  2:  The  ‘mode’  of  a  system  is  the  set  of  components 
available  at  a  given  point  of  time.  The  mode  is  therefore  a 
dynamic  qualifier  for  the  system.  The  system  transitions 
between  modes 
(and  corresponding  CDFGs)  based  on 
component failures. 
Def  3:  ‘Component  failure  rate’  is  the  lumped  failure rate of a 
component  and  its  dedicated  interconnect  and  registers.  This 
means  that  a  dedicated  MUX-based  interconnect  structure  is 
assumed and is not modeled separately but instead merged with 
the component to which it connects. This also implies that faults 
in  a  MUX 
fail  partially 
(disconnecting  the  component  from  some  components  while 
maintaining  the  connection  with  others)  are  still  modeled  as 
catastrophic failures knocking out both the component and the
that  cause  a  component 
to 
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 