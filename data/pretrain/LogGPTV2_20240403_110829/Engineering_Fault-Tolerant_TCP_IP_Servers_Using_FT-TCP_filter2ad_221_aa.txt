# Engineering Fault-Tolerant TCP/IP Servers Using FT-TCP

**Authors:**
- Dmitrii Zagorodnov
- Keith Marzullo
- Lorenzo Alvisi
- Thomas C. Bressoud

**Affiliations:**
- **Dmitrii Zagorodnov and Keith Marzullo:**
  - University of California, San Diego
  - Department of Computer Science & Engineering
  - 9500 Gilman Dr (MC 0114), La Jolla, CA 92037, USA
  - Email: {dzagorod, marzullo}@cs.ucsd.edu

- **Lorenzo Alvisi:**
  - The University of Texas at Austin
  - Department of Computer Science
  - 1 University Station C0500, Austin, TX 78712, USA
  - Email: alvisi@cs.utexas.edu

- **Thomas C. Bressoud:**
  - Denison University
  - 225a Olin Hall, Granville, OH 43023, USA
  - Email: bressoud@denison.edu

## Abstract
In a recent paper [2], we proposed FT-TCP, an architecture that allows a replicated service to survive crashes without breaking its TCP connections. FT-TCP is attractive in principle because it does not require modifications to the TCP protocol and does not affect any client-side software. However, its practicality for real-world applications remains to be proven. In this paper, we report on our experience in engineering FT-TCP for two such applications: the Samba file server and a multimedia streaming server from Apple. We compare two implementations of FT-TCP, one based on primary-backup and another based on message logging, focusing on scalability, failover time, and application transparency. Our experiments suggest that FT-TCP is a practicable approach for replicating TCP/IP-based services, incurring low overhead on throughput, scaling well as the number of clients increases, and allowing near-optimal recovery time.

## 1. Introduction
Consider a company that provides a TCP-based service on a large intranet or the Internet. The service is critical, and if it fails, it must be restarted promptly. There are several constraints the company might face when deciding how to provide service failover:

- **Client Control:** If the client base is large and diverse, the most significant constraint can be the lack of control over the client host configuration and the applications running on the host. This means that client applications cannot be expected to assist in the failover of the service.
- **Performance Impact:** While service outages can have a substantial impact on a company's business, they are rare. Therefore, the failover approach should incur minimal performance costs during normal operation.
- **Deployment Impact:** Deploying the failover approach should have a low impact on the design and installation of the service, as changes in server platforms, upgrading failover software, and deploying new services can be costly. This is an offline cost and may be less critical than the first two constraints.
- **Failover Time:** Depending on the service, failover time might need to be rapid. For example, a client playing a QuickTime movie would experience visualization problems if the failover lasts too long.

Many companies marketing high-end server hardware—such as IBM, Sun, HP, Veritas, and Integratus—offer fault-tolerant solutions for TCP-based servers. These solutions typically use a cluster of servers interconnected with a fast private network for shared disk access, coordination, and failure detection. Clients notice when a server they are connected to fails, but if they open another connection, they will reach a healthy server.

It is often desirable to hide server failures from the clients. One reason is that the client may have state associated with the open TCP connection to the server; losing the connection may require the client to redo a significant amount of work. For example, a client with a connection to an Oracle server will abort all open transactions if the server fails over. A similar issue occurs with existing Samba clients: when a Samba server fails, all transfers are aborted, and the user must explicitly restart the transaction.

We have previously shown that it is possible to hide server failures from clients [2]. We did this by building FT-TCP, a failover service based on message logging [6]. We evaluated the performance of FT-TCP for a synthetic application consisting of a single client with one connection open to the server. In this paper, we argue that FT-TCP can be made practicable. We show that:

- **Code Modifications:** While it was necessary to modify the code of two existing services to make them recoverable using FT-TCP, the modifications were minimal.
- **Low Overhead and Scalability:** The failure-free overhead of FT-TCP is very low, and the system does not have inherent scalability problems.
- **Primary-Backup vs. Message Logging:** A primary-backup version of FT-TCP performs almost indistinguishably from a message-logging version and has the benefit of a shorter failover time.
- **Short Failover Time:** The failover time of FT-TCP can be made very short, but this requires the backup to capture the data sent by the client immediately before the server failed.

We compare FT-TCP to other possible approaches and existing systems in Section 2. The architecture of the system—both primary-backup and message-logging versions—is presented in Section 3. Our experience with getting two popular applications to run under FT-TCP is described in Section 4. Performance during normal operation, as well as behavior during failover, are presented in Section 5. Conclusions are drawn in Section 6.

## 2. Related Work
Solutions to the problem of connection failover can be categorized according to the level at which server failures are masked. 

- **Application-Level Recovery:** With application-level recovery, failures are masked from the user by the client application, which attempts to reestablish broken connections. An FTP client that automatically restarts aborted transfers is an example of such recovery. NFS and Samba clients also fall into this category because, in many cases, they can recover from short disconnections transparently. Since the client needs to be explicitly designed to support application-level recovery, this approach is inapplicable to already deployed applications.
- **Socket-Level Recovery:** Several projects have explored socket-level recovery, where the failure is hidden from the client by a lower layer that reestablishes connections when necessary and provides a reliable socket to the application. One such system [11] extends the TCP protocol with an option that enables migration of connections from one host to another. A similar approach was adopted by [12], but it requires the server application to be aware of the replication. The system described in [7] enables transparent reconnection in Windows NT without changing the TCP stack by wrapping the socket standard library routines. This system was designed to support process migration but can also be used for fault tolerance. The system described in [8] applied a similar wrapping technique to the standard C library on Linux to mask server failures. They evaluated the feasibility of having the backup snoop on packets sent to the primary and concluded that such a system would not have significant overhead. Another system based on wrapping is described in [13], although their goals were to mask connection failures due to network problems rather than server crashes. This last paper describes two approaches to connection recovery, one of which relies on the interception of packets, just like our system. The main drawback of socket-level recovery is that it requires upgrading some of the infrastructure—operating system, protocol stack, or middleware—on the client host.
- **Server-Side Recovery:** Server-side recovery restricts the fault-tolerance logic to the server cluster. This is the easiest solution to deploy: as soon as the servers are fault-tolerant, any client can benefit from greater reliability. Our earlier work [2] demonstrated the feasibility of efficient server-side recovery. This work expands on that by evaluating our approach with two well-known replication techniques and for two real-life applications.

The authors of [1] share our philosophy of server-side recovery. They have developed a protocol similar to ours that is specialized to HTTP request/reply pairs. In doing so, they avoid the problem of server nondeterminism. Another TCP server-side recovery approach is described in [10], which proposes using several router-level redirectors scattered across the Internet to fan out packets to several geographically-distributed replicas. While deploying redirectors may be a costly endeavor, this system has the benefit of tolerating WAN partitions in addition to failures that are local to the server.

## 3. Architecture
In this section, we first introduce several concepts relevant to server-side recovery. We then describe the structure and operation of FT-TCP.

### 3.1. Replication Concepts
To enable recovery of a network service, every connection must be backed by a number of server replicas: a primary server and at least one backup. Should the primary fail, the backups must have all the information needed to take over the connection. Several general approaches to coordinating replicas have been considered by the research community; FT-TCP supports two of them.

- **Primary-Backup (Hot Backup):** In the primary-backup approach, every replica processes client requests, and when everyone is done, one of them (the primary) replies. If the primary fails, one of the backup replicas is chosen as the new primary.
- **Message Logging (Cold Backup):** In the message-logging approach, only one replica is active at a time, and all requests from the client are saved in a log that can survive failures. Just like in the primary-backup approach, the primary does not reply until it knows that all prior requests have been logged. If a failure occurs, another replica replays messages from the log to bring it to the pre-failure state of the primary.

To make the terms more consistent, we refer to these two approaches as hot backup and cold backup, respectively. In both approaches, the primary waits before replying to a client until it knows that the backup could be recovered to the primary’s current state. This is commonly referred to as the output commit problem [6]. We henceforth refer to these forced waiting periods as output commit stalls. Note that, when a backup takes over, it does not know whether the primary failed before or after replying to the client. Fortunately, TCP was designed to deal with duplicate packets, so when in doubt, the backup can safely resend the reply.

Another issue that arises in the context of replicated processes is nondeterministic execution. For both hot and cold backups, the execution paths of the primary and the backups must match. If they do not, a backup may never reach the state of the primary and therefore will not be able to take over the connection.

We discuss how we deal with sources of nondeterminism that cause execution path diversions in the next two sections.

### 3.2. FT-TCP System
FT-TCP is implemented by "wrapping" the TCP/IP stack. By this, we mean that it can intercept, modify, and discard packets on their way in and out of the TCP/IP stack using a component we call the south-side wrap (SSW). FT-TCP can also intercept and change the semantics of system calls made by the server application using a component we call the north-side wrap (NSW). Both wraps on the primary replica place some data into a stable buffer designed to survive crashes.

In our case, the buffer is located in physical memory of the backup machines, but other approaches—such as saving data on disk or in non-volatile memory—are possible. In addition to saving data, a stable buffer can acknowledge the pieces of data it receives and send them back in FIFO order. In the rest of the paper, we will use a setup with one backup (and therefore one stable buffer, located on that backup), but our technique can be extended in a straightforward way to use any number of backup hosts. The setup is shown in Figure 1.

#### 3.2.1 Normal Operation
During normal operation, the SSW sends incoming packets to the backup, and the NSW does the same with the results of system calls (syscalls) that the application makes. Every attempt to send data to the client is suspended until all syscalls have been acknowledged. Fortunately, it is not necessary to wait for the backup to acknowledge packets. Because TCP buffers outgoing data on the sender until the receiver acknowledges it, we can effectively have the client store packets until the backup has them by never acknowledging to the client more than was acknowledged by the backup. Our earlier paper [2] discusses in detail how FT-TCP manipulates TCP sequence numbers to achieve this. Any packets lost in a failure will be resent through the standard TCP retransmission mechanism.

With a cold backup, nothing besides saving incoming requests in the stable buffer and acknowledging them is happening on the backup host. A hot backup, on the other hand, runs its own copy of the server process and provides that process with data that it removes from the stable buffer. To establish a connection, FT-TCP on the backup removes a buffered SYN packet, changes the destination address on that packet to its own address, and injects it into its TCP stack. The stack replies with a SYN+ACK packet, which is caught and acknowledged with an ACK packet by the SSW. This 3-way handshake is not visible outside the backup host, but its TCP thinks it just received a connection from the client.

At the application level, the call to `accept()` returns, and the server process (on both replicas) proceeds to service the incoming connection. As the process on the primary host makes syscalls, their results are sent to the backup. When a backup process makes a syscall, FT-TCP uses the corresponding syscall record from the primary to do one of several things:

- **For calls that query the environment** (e.g., `gettimeofday()` and `getpid()`), the backup immediately returns the result that the primary got.
- **For a `send()`,** the backup ignores the actual data passed by the server application and simply returns the result that the primary got. For debugging, the buffers returned by the backup and the primary (or their checksums) can be compared to flag any inconsistencies.
- **For a `recv()`,** the backup waits until all necessary data packets are in the stable buffer, copies the same number of bytes as the primary got, and returns the same result.
- **For the two calls that return socket status** (`select()` and `poll()`), the backup returns the value from the primary (if a timeout was specified, then the backup invocation will block until the same call on the primary times out).
- **For all others,** the backup executes the call and compares its result to what the primary got. Any inconsistencies are flagged as a potential diversion in execution paths.

The first category of calls takes care of simple sources of nondeterminism, such as different clock values on the replicas and different attributes of their process environments.