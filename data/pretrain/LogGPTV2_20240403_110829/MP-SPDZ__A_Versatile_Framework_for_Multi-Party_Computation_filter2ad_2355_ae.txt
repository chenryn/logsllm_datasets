Secure protocols All instructions of this category allow an
infinite number of arguments to facilitate reducing commu-
nication rounds as described above. The protocols include
essential ones such as private input, multiplication, and pub-
lic output, but also more specialized ones such as inner prod-
uct, matrix multiplication, 2D convolution, and the special
truncation by Dalskov et al. [DEK19].
Preprocessed information As mentioned in Section 4.2, pre-
processing is executed batch-wise. This means that instruc-
tions of this type only incur communication if necessary.
It is also possible to communicate the amount of prepro-
cessed information required in a tape in order to optimize
the preprocessing.
Control flow The virtual machine allows jumps as well as
spawning and joining threads.
Further input/output Instructions in this category facilitate
functionality outside secure protocols such as printing or
client communication.
Protocol information It is possible to use the same program
in various player configurations by accessing information
such as the number of players and then for example loop
over all available players when gathering inputs.
Vectorization. Most instructions are vectorized, that is, they im-
ply the execution of the same operation for as many consecutive
registers as requested. This considerably reduces the overhead of
representing repetitive computation both during compilation as
well as during execution. The virtual machine also facilitates struc-
tured loading of values into consecutive registers, for example,
loading rows at any dimension in multi-dimensional arrays.
Threads. The virtual machine implements multi-threading as
follows: A computation is run in the main thread as described by
list of instructions called a “tape”. The latter can start further tapes
in other threads and wait for their completion. However, the maxi-
mum number of threads has to be known at compile time. Despite
this limitation, the available functionality is powerful enough for
many applications that benefit from multi-threading such as matrix
multiplication or convolutions.
4A more detailed descriptions of the instructions is available in the documentation:
https://mp-spdz.readthedocs.io/en/latest/instructions.html
Inner product example. Figure 4 shows the bytecode represen-
tation of our inner product example with vectors of length three.
Note that both inputmixed, dotprods, asm_open give the number
of parameters as first argument. The first instruction lets private
inputs from parties 0 and 1 to be stored in registers s1,s3,s5 and
s2,s4,s6, respectively, and the result of the inner product is then
stored in s0. Eventually, the inner product is revealed and printed
followed by a new line character.
6 THE COMPILER
Similar to SPDZ-2, the compiler runs high-level code written in
Python, and outputs bytecode to be run by the virtual machine.
Some aspects have been changed as pointed out throughout this
chapter. The following core aspects remain the same, however.
Type system. MP-SPDZ follows the dynamic typing paradigm
of Python. This makes programming more intuitive in that, for
example, any operation involving a secret and a public value results
in secret value. Consider that in a stronger type system, the as-
signment of a secret type to a public type would involve automatic
revealing, which can be unintended, or the compiler would need
to produce an error, which makes programming harder. Dynamic
typing together with a clearly named way of revealing secret infor-
mation therefore strikes a compromise between security and ease
of use.
Basic blocks. This concept is taken from general compiler design
and denotes a sequence of instructions without branching. The
compiler performs the round-minimizing optimization only in the
context of a basic block because it requires rearranging instructions.
6.1 Minimizing the Number of Rounds
This is the main optimization conducted by the compiler. It analyses
a basic block to find instructions of the same kind that can be merged
into a single instruction because they are independent. Recall that
such instructions allow an infinite number of arguments. An easy
example of a possible merge are two multiplications that can be run
in parallel. MP-SPDZ differs from SPDZ-2 in that it merges different
operations independently while SPDZ-2 reduces multiplication to
openings using Beaver multiplications. Such a reduction clearly ex-
cludes protocols that do not use Beaver multiplications. As a result,
MP-SPDZ uses an instruction for secure multiplication, which is
merged independently of opening operations. Another difference to
SPDZ-2 is that the latter splits the opening process in two instruc-
tions called startopen and stopopen, which enables executing
local computation while waiting for information from the network.
However, this comes at the cost of an increased complexity in the
compiler as well as the fact that the number of parallel opens is
limited by the communication buffer, which has to be considered on
the compiler level. MP-SPDZ uses a single instruction for opening,
which simplifies the handling.
The algorithm proceeds in three steps:
(1) First it creates a dependency graph of all instructions in a
basic block. This graph takes the form of an directed acyclic
graph where the nodes stand for instructions. There are a few
reasons to create an edge between two nodes, most notably
if the output of one instruction is an input to another. Other
inputmixed 18, 0, s1, 0, 0, s3, 0, 0, s5, 0, 0, s2, 1, 0, s4, 1, 0, s6, 1 # 0
dotprods 8, 8, s0, s1, s2, s3, s4, s5, s6 # 1
asm_open 2, c0, s0 # 2
print_reg_plain c0 # 3
print_char 10 # 4
Figure 4: Virtual machine code of our inner product example with vectors of length three.
Marcel Keller
dependencies include various types of interaction with the
environment, e.g., the order of input read from a party should
not change.
(2) The algorithm assigns instructions that can be merged to
rounds. All instructions in the same round have the same
type. This is done by a variant of the longest-path algorithm:
(a) All nodes without predecessors are assigned to round zero.
(b) Every non-mergeable node is assigned the maximum of
the round number of its predecessors.
(c) Every mergeable node is assigned the minimal round that
is larger than all of its predecessors and that is compatible,
that is, it is not occupied by an instruction of another type.
Since this algorithm only considers predecessors of nodes, it
can be run together with the dependency graph creation.
(3) All instructions in the same round are merged. This involves
merging the arguments such that the semantics are pre-
served, and merging all edges in the dependency graph.
(4) The instructions are output in topological order according
to the changed dependency graph.
Memory instruction dependency. Since memory instructions al-
low runtime addresses, it is not straightforward how treat them in
the dependency graph. If every read instruction is made to depend
on a write instruction and vice versa, a possibly large potential
for minimizing rounds is lost. Consider an unrolled loop where a
memory address is read followed by some computation on the read
value, the result of which is stored at the same address, and the
same is repeated for more addresses. All computations can clearly
be executed in parallel, but a dependency of every memory instruc-
tion on the previous one prevents this. The compiler therefore only
considers dependencies if they involve the same address, be it a
compile-time address or the same register for run-time addresses. In
combination with caching registers when accessing data structures
such as a multi-dimensional arrays, this strikes a balance between
efficiency and correctness guarantees. Furthermore, the compiler
offers a command to start a new basic block, which inherently
preserves the order of instructions.
Dead code elimination. The dependency graph created above also
allows eliminating instructions with unused results. An instruction
is considered obsolete if it is not considered inherently essential
(because of side-effects on the environment or the memory) and
if all successors are obsolete. A simple backward pass suffices to
determine and eliminate obsolete instructions.
6.2 Register Allocation
As in SPDZ-2, the compiler initially uses an unlimited supply of
write-once registers, which are allocated to a minimal number of
registers at the end. For a straightline program without branches,
this is trivial by passing through backwards, allocating a register
whenever it is read for the last time, and deallocating it at the
single time it is written to. For a program with branches there is the
difficulty that some registers have to be live throughout a loop to
prevent overwrites after the last read. This is solved by allocating
registers that are written to before the loop starts before processing
the parts of the loop.
6.3 Loops
While the nature of multi-party computation makes it non-trivial
to implement loops that depend on secret data, loops depending
on public data naturally reduce the representation of computation.
Similar to SPDZ-2, MP-SPDZ supports loops depending both on
compile-time and run-time public data. The former still allows
compile-time analysis of the computation cost. In the high-level
language loops can be executed using function decorators:
@for_range(n)
def _(i):
a[i] = ...
While this is certainly unusual, it enables compilation by running
the high-level Python code within the scope of the compiler. Further-
more, it allows a variety of loops without creating a domain-specific
language. For example, @for_range creates a strict run-time loop
executed consecutively, @for_range_opt implements the dynamic
optimization described below, and @for_range_multithread and
@for_range_opt_multithread execute a loop in a fixed number
of threads.
Dynamic loop optimization. Büscher et al. [BDK+18] have de-
scribed a trade-off between unrolling loops in order to merge com-
munication rounds as in Section 6.1 and limiting memory usage
during compilation caused by the increased space to represent
the unrolled computation. They propose a dynamic approach of
unrolling until a time budget is exhausted. MP-SPDZ adapts their
approach by using a budget on the number of instructions produced,
which serves as a proxy for the overall compilation cost.
6.4 Repetitive Code
Since the basic computation in multi-party computation is limited
to additions and multiplications in a domain, even seemingly sim-
ple computations such as comparisons translate into a non-trivial
composition of basic operations. The design principle of MP-SPDZ
is to break down computation into basic operations in order to limit
the complexity of the virtual machine. However, this means that
repetitive code leads to a repetitive expansion of the same building
block. To avoid that cost, the compiler offers to treat certain basic
computations atomically and merging them as in Section 6.1. While
speeding up compilation this has the downside that some parallel
MP-SPDZ: A Versatile Framework for Multi-Party Computation
Pre: [x] for x ∈ Z2k , [r0], . . . ,[rk−1] such that ri
Post: [x0], . . . ,[xk−1] such that x =
(1) Compute and open [c] = [x] +
(2) Compute c0, . . . , ck−1 such that c =
i xi · 2i and xi ∈ {0, 1}
i ci · 2i and ci ∈ {0, 1}
(3) Compute [x0], . . . [xk−1] using a binary adder on
$← {0, 1}
i[ri] · 2i
([r0], . . . ,[rk−1]) and (c0, . . . , ck−1)
Figure 5: Bit decomposition modulo 2k using random bits.
computations will be executed sequentially. For example, a compar-
ison and an equality test on independent data can be executed in
parallel (thus reducing the number of rounds), but treating the two
separately will execute them sequentially. Because of the trade-off,
this optimization is only used when requested.
Instances of building blocks treated in the way above range from
integer operations such as comparison and truncation to mathe-
matical functions such as the trigonometric functions.
As a concrete example, consider the case of computing the max-
imum of several values. To minimize the round complexity, this
has to computed as a binary tree, selecting the maximum at every
node, and all comparisons on every level should be computed in
parallel. It is somewhat onerous to program this as in a vectorized
manner. MP-SPDZ allows using a simple recursive approach in-
stead, merging all parallel comparisons. With the repetitive code
optimization, this happens before creating the circuits used for
comparison, which speeds up the compilation considerably.
7 HIGH-LEVEL LIBRARY
In this section, we will describe the capabilities for secret computa-
tion implemented on top of the compiler. These implementations
generally involve breaking down the desired functionality to the
basic operations supported by the protocols such as input, output,
and arithmetic in the computation domain. As these operation are
modeled by the arithmetic black-box, it is usually straightforward
to prove the security of the extended functionality.
7.1 Integer Operations
Beyond basic arithmetic in the respective domain, the library im-
plements comparison, equality, left/right shift, modulo power of
two, and power of two with secret exponent. A core component
of most of these operations in larger computation domains is a
“mask-and-open” approach. This involves adding or subtracting a
secret random value of a certain form to or from a secret value
and opening the result, which can then be processed as a public
value, for example to extract single bits. See Figure 5 for the basic
example of a bit decomposition. It uses these public bits together
with the secret bits of the masking to compute a bit decomposition
of a secret value, that is, an individual secret sharing for every bit
instead of a secret sharing for the whole value. Unlike the latter,
the former allows computing all integer operations above directly.
Catrina and de Hoogh [Cd10] have shown how to optimize this sim-
ple approach with computation modulo a prime, later adapted by
Dalskov et al. [DEK19] for computation modulo a power of two and
by Escudero et al. [EGK+20] for switching to binary computation
for parts of it.
Mask-and-open with computation modulo a prime is only statis-
tically secure and requires the secret value to be within an assumed
interval. For example, assume that x ∈ [0, 2l] and that r ∈ [0, 2l +s]
is randomly chosen for a security parameter s. Then, x + r is sta-
tistically indistinguishable from a uniform number in [0, 2l +s]. In
computation modulo a power of two this is not an issue because
overflow bits can simply be “erased” by multiplying with a power
of two.
7.2 Fractional Numbers
As SPDZ-2, MP-SPDZ offers two ways of representing fractional
numbers, fixed-point and floating-point. The former denotes repre-
senting a fractional number x by an integer y such that x = y · 2−f
for a fixed precision f . The latter is similar to floating-point repre-
sentations such as IEEE 754 in that x is represented by the four-tuple
(v, p, z, s) such that
(cid:40)(−1)s · v · 2p
x =
0
z = 0
z = 1.
The additional bits s and z simplify the computation given that
secure computation does not directly allow one to access single bits
of a larger value.
Fixed-point numbers. Due to the larger efficiency, this is the pre-
ferred approach for fractional numbers in MP-SPDZ. Addition and
subtraction are straight-forward by the linearity of the represen-
tation, and multiplication corresponds to integer multiplication
followed by truncation of f bits. The truncation can either be com-
puted as a left shift or a more efficient probabilistic truncation. The
latter involves randomized rounding based on the input. For exam-
ple, 0.25 would be rounded to 0 with probability 0.75 and to 1 with
probability 0.25 when rounding to integers. Catrina and Saxena first
suggested this for computation modulo a prime [CS10], Dalskov
et al. [DEK19] adapted it for computation modulo a power of two,
and Escudero et al. [EGK+20] to mixed computation. Catrina and
Saxena also presented how to use Goldschmidt’s algorithm [Gol64]
to compute division in secure computation.
Floating-point numbers. Aliasgari et al. [ABZS13] have shown
how to implement floating-point numbers in the context of secure
computation modulo a prime. Their approach translates directly to
Z2k with the exception of multiplying by (2m)−1 in order to com-
pute a left shift for x when x = 0 mod 2m. This can be achieved
through bit decomposition, however. The library implements addi-
tion, subtraction, division, and comparisons of floating-point num-
bers. Note that the approach by Aliasgari et al. is not fully compliant
to IEEE754. However, it is possible to use the binary circuits for com-
pliant computation of some operations (addition, multiplication,
division, and square root) in the Bristol Fashion format provided
by SCALE-MAMBA [COS19]. See Section 7.3.
Mathematical functions. Aly and Smart [AS19] have implemented
a number of mathematical functions in secure computation, rang-
ing from trigonometric functions over square root to exponential
and logarithm. MP-SPDZ integrates this by reusing code provided
by the authors through SCALE-MAMBA [COS19]. All functions
are implemented for fixed-point numbers while the implementa-
tions for floating-point numbers are restricted to sine, cosine, and
tangent.
7.3 Further Functionality
The library extends beyond basic mathematics as shown in the
following paragraphs.
Machine learning. The library provides functionality for logistic
regression [KS19] and deep-learning inference [DEK19]. The for-
mer allows choosing between an accurate implementation of the
sigmoid function and an approximation similar to ABY3 [MR18].
The latter supports the quantization scheme used in MobileNets
[JKC+17] and several ImageNet solutions such as DenseNet, ResNet,
and SqueezeNet. These are compiled automatically from Tensor-
Flow with the help of CrypTFlow [KRC+20], which opens the pos-
sibility for further networks to be used.
Oblivious data structures. MP-SPDZ retains the code used by
Keller and Scholl [KS14]. This includes an oblivious array, queue,
and stack implementation. Oblivious here means that all accesses
are done secretly, that is, without revealing indices (where appli-
cable) and whether an access is reading or writing. However, it is
inherent to secure computation that an upper limit to the amount
of data in the structure is revealed. Oblivious RAM [SvS+13] is the
core technique to achieve efficient data structures for larger sizes.
Based on the data structures above, the library contains example
implementations of Dijkstra’s algorithm for shortest path in graphs,
and the Gale-Shapley algorithm for stable matching.