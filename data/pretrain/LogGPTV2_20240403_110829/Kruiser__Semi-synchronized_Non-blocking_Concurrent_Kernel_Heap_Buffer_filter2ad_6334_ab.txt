buffers. This way the concurrent monitor process has to fol-
low the locking discipline, which would introduce intense
lock contention. Another concurrent approach, as used in
kernel memory mapping and data analysis for kernel in-
tegrity checking [25], is to check without acquiring locks
and freeze the monitored VM for double-check to avoid
false positives, which may require suspending the VM fre-
quently in our case.
Instead of relying on kernel-speciﬁc data structures, we
maintain a separate structure called Page Identity Array
(PIA). Its basic form is a static array data structure with
each entry recording the identity of a page frame. A va-
riety of page identity information can be of interest, such
as per page signature, access control, accounting and au-
diting data. With regard to concurrent heap monitoring, a
PIA entry records whether a page frame is used for heap
memory, and if so, the metadata that is used to locate ca-
naries within the page. The ﬁrst entry corresponds the ﬁrst
page frame, and so forth. Since the kernel memory address
space is ﬁxed, the size of PIA structure can be predeter-
mined. This way we only need to hook functions that add
pages into the heap page pool and that remove pages from it,
updating metadata in the corresponding entries. The moni-
tor traverses the PIA structure and check canaries according
to the stored metadata. Compared to interposing per buffer
allocation and deallocation and collecting canary addresses
in a dynamic data structure, the overhead due to function
hooking and data structure maintenance is largely reduced.
We postpone details about metadata and memory overhead
analysis in Section 6.
The idea of using a ﬁxed-size data structure is due to
the insight into kernel heap management. We assume that a
kernel page, if used for heap memory, is divided into buffer
objects of equal size and that all the buffers in this page
are arranged as an array, which is true in most commodity
systems. Given a heap page and its initial buffer object ad-
dress and size, the monitor process can locate all the buffers
within this page, such that the metadata stored in each PIA
entry can be small. Before a process (or a kernel thread)1
adds a page into the heap page pool, the canaries within the
page are initialized and the corresponding PIA entry is up-
dated. By scanning the canaries within each page, the mon-
itor process detects buffer overﬂows. Although some buffer
objects are not allocated and some canary checking may be
not necessary, the simple read operations do not introduce
much overhead. The traverse along pages is suitable for 32-
bit OSes with small kernel memory address space.
For 64-bit systems with large address space and physical
memory, the ﬂat PIA structure may not be scalable enough,
and sparse kernel heap pages could lead to a concern of sig-
niﬁcant ineffective scanning. We will present an extended
form of PIA structure in Section 8.1, which could solve the
sparse heap pages problem with high scalability and low
memory overhead.
5.2 Race conditions
Exploring the characteristics of kernel heap manage-
ment, we proposed the static PIA structure, which avoids
heap monitoring from relying on kernel-speciﬁc heap data
structure and supports highly efﬁcient random access. Nev-
ertheless, synchronization between the monitor process and
processes updating page identities is still an issue. For ex-
ample, when the monitor process reads an entry, another
process may be updating it. Without synchronization, the
consistency of PIA entries cannot be ensured, which implies
the monitor process cannot retrieve heap buffers reliably.
Before we present the kernel heap cruising algorithm, we
ﬁrst discuss the potential race conditions for sharing the PIA
structure, which motivate our semi-synchronized design in
Section 5.3. Three categories of processes need to access
the PIA structure: the monitor process, processes updating
PIA entries when pages are added into and removed from
the pool, respectively. When multiple processes access the
PIA structure, a variety of race conditions can occur, some
of which are subtle.
Non-atomic entry write: As updating a PIA entry is not
atomic, a race condition occurs if we allow multiple pro-
cesses to modify the same entry simultaneously, which
would corrupt the entry. Lock-based synchronization is
simple, but it incurs high performance overhead and blocks
heap operations.
Non-atomic entry read: When the monitor process is read-
ing a PIA entry, another process may be updating it. How-
ever, as the read and update of an entry are not atomic, the
monitor process may read inconsistent entry values.
Time of check to time of use (TOCTTOU): For a given
entry if the corresponding page is in the heap pool, the mon-
itor process checks canaries within that page, during which,
however, the page may be removed from the pool and used
for other purposes, such that false alarms may be issued.
To avoid false alarms, it is tempting to double check
whether the page has been removed from the heap page
pool when a canary is detected tampered. Speciﬁcally, a
ﬂag ﬁeld indicating whether the page is in the pool is con-
tained in each entry. A process removing the page out of the
heap page pool resets the ﬂag; when a heap buffer corrup-
tion is detected, the monitor process double checks the ﬂag
to make sure the page is still in the pool. A buffer overﬂow
is reported only when a canary is tampered and the ﬂag in
the PIA entry is not reset. However, it cannot avoid the ABA
hazard as discussed below.
ABA hazard: An ABA hazard occurs when one process
reads a value A from some position, and then needs to make
sure the position is not updated since last access by reading
it again and comparing the second read value with A. How-
ever, between the two reads, other processes may have up-
dated the position from value A to B then back to A. In our
case, it may lead to an ABA hazard if the monitor process
intends to determine whether the entry has been updated by
reading the ﬂag twice, considering that other processes may
have removed the page from the heap page pool and then
added it back between the two reads, such that the idea of
double-checking the ﬂag can still lead to false alarms due to
ABA hazards.
Compared to the idea of walking along existing ker-
nel data structures, we apparently have conquered noth-
ing except migrating the synchronization problems to the
PIA structure. However, as presented below, we propose
a semi-synchronized algorithm based on PIA to resolve all
the problems without incurring false positives or high over-
head.
5.3 Semi-synchronized Non-blocking Cruising
We propose an efﬁcient semi-synchronized non-blocking
kernel cruising algorithm, as shown in Figure 2, that works
with the PIA structure. It resolves the concerns of race con-
ditions without introducing complex synchronization mech-
anisms, such as ﬁne-grained locks and intricate lock-free
data structures.
We add an unsigned integer ﬁeld version in each entry,
which records the “version” of the corresponding page. It
is initialized to be an even number when the correspond-
ing page is not in the heap page pool. Whenever a page is
added into or removed from the pool, its corresponding ver-
sion number is incremented by one, so that an odd version
number indicates a heap page, and an even number indicates
a non-heap page. Because the size of the version ﬁeld is one
word, the read and write of a version value is atomic, which
is critical for the correctness of our algorithm.
1In this paper we will use the two terms interchangeably.
Avoid Concurrent Entry Updates: The kernel commonly
1 //Add a page into the heap page pool
2 AddPage(page){
3
4
5
6
7
...
/∗ Inside critical section ∗/
Initialize all the canaries within the page
Update the metadata in PIA[page];
smp wmb(); // This write memory barrier enforces a store
ordering
8
9
PIA[page].version++;
...
alarm(); // A Buffer overﬂow is detected
if (the canary is tampered)
PIA[page].version++;
...
...
/∗ Inside critical section ∗/
for (each canary within the page)
10 }
11
12 //Remove a page out of the heap page pool
13 RemovePage(page){
14
15
16
17
18
19
20
21 }
22
23 Monitor(){
24
25
26
27
28
29
30
ver1 = PIA[page].version;
if (!(ver1 % 2))
uint ver1, ver2;
for (int page = 0; page < ENTRY NUMBER; page++) {
continue; // Bypass non−heap page
smp rmb(); // This read memory barrier enforces a
31
32
33
34
35
load ordering
Read the metadata stored in PIA[page];
smp rmb();
ver2 = PIA[page].version;
if (ver1 != ver2)
continue; // Metadata was updated during the
read
DoubleCheckOnTamper(page, ver1);
for (each canary within the page){
if (the canary is tampered)
}
}
36
37
38
39
40
41
42 }
43
44 DoubleCheckOnTamper(page, ver){
45
46
47
48
49 }
uint ver recheck = PIA[page].version;
if (ver recheck != ver)
return; // The page was already removed/reused
alarm(); // A buffer overﬂow is detected
Figure 2. Kruiser monitoring algorithm.
has its own synchronization mechanisms to prevent one
page frame from being manipulated for inconsistent pur-
poses at the same time. For example, Linux functions
kmem getpages and kmem freepages, which add page
frames into and remove them from the heap page pool, re-
spectively, operate on page frame in a critical section with
lock protection. These two functions correspond to Ad-
dPage and RemovePage in Figure 2, respectively. The
PIA entry update operations can be put into the critical sec-
tion of these two functions; it is thus ensured that two pro-
cesses cannot update the same entry simultaneously. By
leveraging the existing synchronization mechanisms in ker-
nel to maintain the PIA entries, the additional overhead is
minimal since updating metadata in a PIA entry is fast. As
long as the kernel prevents one page frame from being ma-
nipulated by two processes simultaneously, there should be
synchronization mechanisms serving for this purpose, so
the “free-ride” is widely available.
Avoid Using Inconsistent Entry Value: Instead of pre-
venting the monitor process from reading inconsistent entry
value, we allow it to occur. However, we use a double-check
algorithm to detect potential inconsistency and avoid using
inconsistent values. We read the version ﬁeld in an entry
ﬁrst (Line 26), and then retrieve other entry ﬁelds followed
by another read of the version ﬁeld (Line 33). The page is
to be scanned if and only if the two reads of the version ﬁeld
retrieve identical odd version numbers. Here we assume the
wraparound of the version value does not occur between the
two reads. Considering that page frame switch in and out of
the kernel heap pool is infrequent, it very unlikely that the
version number wraps around a 32-bit unsigned integer be-
tween the two reads.
Speciﬁcally, assume there is a non-heap page frame and
the AddPage function adds it into the heap page pool. In its
critical section it ﬁrst updates the metadata and then the ver-
sion number (Line 8) in the corresponding page entry, such
that if the monitor process reads the version number of the
entry being updated and the read is before the version num-
ber update (Line 8), it will retrieve an even number, which
indicate a non-heap page. The monitor process will bypass
this page (Line 27) according to our algorithm. A write
memory barrier (Line 7) is inserted before the version num-
ber update, which preserves an observable update order. It
is a convention to assume a sequential consistency memory
model in the parallel computing literature when describing
a concurrent algorithm; however, the observable update se-
quence [37] is vital to the correctness of our algorithm, so
we point it out explicitly.
The version number is not
incremented until Re-
movePage removes the page from the pool. It does not
need write memory barriers around the version update be-
cause the enter and exit of a critical section imply a full
memory barrier, respectively. Therefore, as long as the two
reads of the version ﬁeld retrieves identical odd values, the
retrieved metadata values are consistent. Two read mem-
ory barriers (Line 30 and 32) are inserted into the Monitor
function, such that an observable load ordering is enforced
among the reads of the version number and metadata. But
note that the read and write memory barriers are not needed
on x86 and AMD64 platforms [36], as they already preserve
the loads and stores orders we need.
Identify TOCTTOU and ABA Hazards: Without locks
or other synchronization primitives, it is difﬁcult to avoid
TOCTTOU and ABA hazards. Rather than avoiding the
hazards, the algorithm takes a different approach to rec-
ognizing potential hazards to avoid false alarms. When a
canary is found changed, the monitor process does not re-
port an overﬂow immediately. Instead, it makes sure the
page being checked has not ever been removed out, which
is indicated by the version number again. As long as the
version number does not change compared to the last read
(Line 46), it can be determined that the page has persisted
as a heap page; in this situation, if a canary is found cor-
rupted, a buffer overﬂow is reported without concerns of
false positives.
The non-blocking algorithm is constructed using sim-
ple reads, writes, and memory barriers without introducing
complicated and expensive synchronization mechanisms.
The monitoring is wait-free as it guarantees progress in a
ﬁnite steps of its own execution; i.e., it is non-blocking.
The monitor process reads version numbers to determine
its control ﬂow, so it is lightly synchronized, while other
processes manipulating heap pages make progress without
being synchronized or blocked by the monitor process. In
other words, the synchronization is one-way. That is why
we call it a semi-synchronized non-blocking cruising. On
PIA entries, write-write is synchronized with a free-ride
from the existing kernel functions, while read-write is not
synchronized. It resolves the concern of a variety of subtle
race conditions without the need of freezing the entire sys-
tem for recheck. It does not have false positives and enables
efﬁcient concurrent heap monitoring.
6 System Design and Implementation
6.1 Background
Linux adopts the slab allocator2 for kernel heap man-
agement.
It uses caches to organize heap buffer objects.
There are two types of caches in kernel heap, namely gen-
eral caches and speciﬁc caches. General caches are mainly
used to serve kmalloc calls requesting heap buffers of var-
ious sizes, while each speciﬁc cache is used to allocate ob-
jects of a speciﬁc kernel data structure, such as task struct.
A cache consists of one or more slabs, each of which occu-
pies one or more physically contiguous pages and contains
2Similar schemes are widely used in other commodity systems, such as
Solaris and FreeBSD.
0
1
8
App
App
App
Custom
Driver
2
User Page 
Table