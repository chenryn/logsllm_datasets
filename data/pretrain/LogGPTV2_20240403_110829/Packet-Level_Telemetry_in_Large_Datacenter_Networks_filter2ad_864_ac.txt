The controller accepts the EnableDbg() request if the total
amount of traced trafﬁc does not exceed system capacity.
5. EVERFLOW APPLICATIONS
Writing applications using Everﬂow APIs is straightfor-
ward. We now present several example applications to debug
the network faults described in §2.
Latency proﬁler. Many DCN services, e.g., search and
distributed memory cache, require low latency. To ﬁnd out
why the latency between any pair of servers is too high,
the latency proﬁler will ﬁrst mark the debug bit of the TCP
SYN packets between the two servers. From the traces of
these packets, it knows the network devices on the path and
then launches guided probes to measure the per-hop latency.
Guided probing measures the roundtrip latency of each link
instead of the one-way latency. This degree of localization
sufﬁces in practice. With this localization information, the
proﬁler can quickly identify the network devices that cause
the problem.
Packet drop debugger. Packet drops can severely degrade
application performance, causing low throughput, timeouts
or even unreachability. They are notoriously difﬁculty to
debug as they happen due to many different reasons such
as congestion, software bugs, or conﬁguration errors. Our
packet drop debugger routinely examines the packet traces
that show packet drops. Given such a trace of a packet p,
the debugger will infer the next hop switch Sn based on the
last hop where p is captured. For example, Sn can be in-
ferred either from p’s output interface at the last hop switch
or from the DCN topology and routing. After that, it will in-
ject guided probes to Sn to determine whether the drops are
Figure 5: Detecting the drop of mirrored packets
compute the latency of the Mux. This process is shown in
Fig 3(b) where a Mux server is connected to a ToR switch
that mirrors both the original packet po and the encapsulated
packet pe. Because the mirrored instances of po and pe will
take the same path from the ToR switch to the same analyzer
(as explained in §3.2), we can estimate the Mux latency us-
ing the arrival time difference between them at the analyzer.
The estimated Mux latency includes the round trip latency of
the link between the ToR switch and the Mux, which is neg-
ligibly small compared to the Mux latency. To save space,
the analyzer will quantize individual latency samples into
predeﬁned bins in a latency histogram.
Mirrored packet drop counters. A mirrored packet may
be dropped before reaching the analyzer. We can often in-
fer such a drop from the packet trace. Fig 5 shows that a
packet p traverses switches S1 and S2. However, p’s trace
contains only S2 but not S1, clearly indicating that p’s mir-
rored packet from S1 is dropped. In our current deployment,
we found the drop rate is low (around 0.001%).
Sometimes mirrored packets may be dropped due to con-
gestion near a particular reschufﬂer or analyzer. To further
increase the reliability of our trace collection pipeline, we
deploy multiple reshufﬂers and analyzers in different parts
of the DCN and shift mirrored trafﬁc away from any con-
gested reshufﬂer and/or analyzer that exhibits a high mir-
rored packet drop rate.
4.2 Controller APIs
Everﬂow applications interact with the controller via sev-
eral APIs to debug various network faults. With these APIs,
the applications can query packet traces, install ﬁne-grained
load counters, trigger guided probes, and selectively trace
trafﬁc by marking the debug bit.
GetTrace(Filter, Condition, StartTime, EndTime) is used
to get the packet traces between StartT ime and EndT ime.
The F ilter parameter speciﬁes the types of traced packets to
be ﬁltered and is similar to the widely-used display ﬁlter in
Wireshark. It allows ﬁltering based on the Ethernet, IP, TCP,
or UDP headers of the original packets as well as the outer
IP header of the mirrored packets (which contains the IP ad-
dress of the switch that sends the mirrored packets as shown
in Fig 6). For example, the F ilter “ip.proto == 6 && ip.dst
== 192.16.0.0/16 && switch == 10.10.0.10” matches all the
TCP packets going to 192.16.0.0/16 and mirrored by switch
10.10.0.10. The Condition parameter speciﬁes the proper-
ties of the traces that cannot be extracted from the packet
484persistent and, if so, whether the drops are random or have
any patterns (e.g., speciﬁc 5-tuples).
Loops are uncommon in DCNs. How-
Loop debugger.
ever, when they do appear, they can cause unnecessary waste
of resources or connectivity problems. Our loop debugger
watches for packet traces that contain a loop. When a loop
is detected, it ﬁrst injects guided probes to see if the loop is
persistent. If so, it reports the list of devices in the loop to the
operators who can then break the loop by disabling one of
the device interfaces. During this process, the debugger can
continue to inject guided probes until the loop disappears.
In DCNs, switches often use ECMP to
ECMP proﬁler.
split trafﬁc to the next hops. The load split may be uneven
due to poor hash functions or routing problems, causing link
congestion. For each switch, our ECMP proﬁler will moni-
tor the aggregate load of all the links. When an uneven load
split is detected, it will drill down through more ﬁne-grained
load counters to ﬁnd out whether the uneven split impacts
all trafﬁc or just a subset (e.g., the trafﬁc from/to certain pre-
ﬁxes). The proﬁling results help the operators quickly detect
and localize the problem.
RoCEv2-based [18]
RoCEv2-based RDMA debugger.
RDMA (Remote Direct Memory Access) is an emerging pro-
tocol for achieving high throughput (40 Gbps) and ultra-low
latency (several microseconds) with low CPU overhead. By
leveraging PFC (Priority-based Flow Control) to enable a
drop-free Ethernet fabric, the RDMA protocol implementa-
tion can be simpliﬁed and ofﬂoaded to the NIC. However,
in our DCNs, we ﬁnd that RDMA sometimes cannot attain
its ideal performance due to software bugs in the NIC. De-
bugging these problems is hard because the NICs are built by
third-party vendors and we have limited means to instrument
the RDMA code on the NICs.
We build a RDMA debugger in Everﬂow. It traces all the
control packets related to RDMA, such as PFC and NACK
(Negative Acknowledgement). The control packet traces of-
fer a reliable and yet independent way not only to observe
the actual behavior of RDMA ﬂows but also to debug the
implementation issues inside the third-party vendor’s code.
6.
IMPLEMENTATION
The entire Everﬂow system is implemented in roughly
10K lines of code in C++. The ﬁve Everﬂow applications are
written in roughly 700 lines of code in C# in total. Below,
we omit the details of the controller (whose implementation
is fairly straightforward) and the reshufﬂer (which is simi-
lar to the Mux described in Duet [12]); we focus on other
aspects instead.
6.1 Switch conﬁgurations
By default, we conﬁgure rules in the TCAM table to match
on TCP SYN/FIN/RST ﬂags. We use a bit in the DSCP ﬁeld
as the debug bit, and n bits in the IPID ﬁeld to sample 1 out
of 2n packets. For example, by conﬁguring a rule to match
on 10 bits in the IPID ﬁeld, we will sample 1 out of 1,024
packets. Since every switch has the same rules, the set of
sampled packets will be consistent across all switches. For
Figure 6: Format of mirrored packet
any encapsulated packet, we conﬁgure rules to match on its
inner TCP/IP headers to ensure that it is sent to the same
analyzer as its original packet (§3.2). Finally we conﬁgure
rules to match on Ethernet type 0x8808 (L2 control packets
including PFC), TCP port 179 (BGP packets), and RDMA
NACK [17]. The total number of rules is around 20, which
consumes only a small fraction of the TCAM table.
When a packet matches any rule, the switch will mirror
it and encapsulate the mirrored packet using GRE (Generic
Routing Encapsulation). Fig. 6 shows the format of the GRE
packet, where the source IP is the switch loopback IP, the
destination IP is the VIP of the reshufﬂers, and the payload
is the original packet (starting from the L2 header). Inside
the GRE header, there is a protocol ﬁeld which is used to
indicate that this is an Everﬂow mirrored packet. We conﬁg-
ure every switch with a blacklist rule to prevent mirroring a
mirrored packet.
“Match and mirror” is completely done in a switch’s data
plane. This implementation leverages the huge packet pro-
cessing capacity of switching ASIC and incurs zero over-
head on a switch’s CPU.
6.2 Guided prober
The key function of a prober is to inject any desired packet
into any target switch S. It uses the raw socket APIs to craft
arbitrary packet ﬁelds, e.g., the IP and L4 (TCP, UDP, ICMP,
etc.) headers. The crafted packet p has the debug bit set
to enable tracing and carries a signature in the payload so
that it can be easily identiﬁed by the Everﬂow analyzer. To
send p to S, we leverage the decapsulation capability that
is widely available on commodity switches. We ﬁrst create
the probe packet p′ by encapsulating p with S’s loopback
IP as the destination, and send p′ out. We also conﬁgure a
rule on S to decapsulate any encapsulated packet destined
to S’s loopback IP address. Thus upon receiving p′, S will
decapsulate p′ into p and then process p according to the
normal forwarding logic.
In fact, we can extend the technique above to instruct p′ to
follow any desired route by encapsulating p multiple times.
This can be used to measure the latency of any link (S1, S2)
as shown in Fig 2(b). We simply need to craft p with S1 as
the destination, encapsulate it with S2 as the destination, and
485encapsulate it again with S1 as the destination. The resulting
p′ will follow the route S1 → S2 → S1 as required in §3.2.
To prevent guided probe packets from interfering with server
applications, we deliberately set their TCP or UDP check-
sum incorrectly so that they will be discarded by servers.
6.3 Analyzer
The analyzers use a custom packet capturing library to
capture mirrored packets. The library supports RSS (Re-
ceiver Size Scaling) [2] which allows an analyzer to receive
packets using multiple CPU cores. The library hashes pack-
ets to CPU cores based on source and destination IPs, using
inner source and destination IPs if packets are encapsulated.
We run multiple analysis threads to maximize throughput.
6.4 Storage
The Everﬂow storage is built on top of SCOPE [6]—a
scalable, distributed data processing system.
In SCOPE,
data is modeled as tables composed of rows of typed columns.
These tables can be processed by SQL-like declarative scripts,
which support user-deﬁned operators, such as extractors (pars-
ing and constructing rows from a ﬁle), processors (row-wise
processing), reducers (group-wise processing), and combin-
ers (combining rows from two inputs).
We store packet traces in a multi-column table, where
each row corresponds to a packet trace. The columns con-
tain three parts of information about the packet trace. The
ﬁrst part is the full packet content. The packet header and
payload are stored in separate columns to simplify process-
ing. The second part is the per-hop information, e.g., times-
tamp, TTL, source MAC address and DSCP-ECN. Since the
number of hops is variable, we combine all the per-hop in-
formation into one column. The last part is the metadata of
the trace, including the trace length, whether it is a guided
probe, or whether it has a loop or a drop. The traces can be
retrieved by the controller based on the f ilter, condition,
and time range deﬁned in §4.2.
We also store the counters from all the analyzers in a table.
Each row in the table represents one snapshot of a counter
from an analyzer. Besides the key and value of the counter, a
row also contains the analyzer’s ID and the timestamp when
the snapshot was taken. To respond to a counter query, the
controller will sum up the counter values of the rows that
match the given counter name and time range.
7. DEPLOYMENT AND EXPERIENCE
We deployed Everﬂow in two Microsoft DCN clusters
in August 2014. The ﬁrst one is a full deployment in a
pre-production cluster (Cluster A) with 37 switches. The
second one is a pilot deployment to 440 out of more than
2,500 switches in a production cluster (Cluster B). Both clus-
ters carry trafﬁc for many DC applications. In addition, we
also enabled Everﬂow on certain production switches on de-
mand to debug live incidents. Currently, we are extending
Everﬂow deployment to more switches and clusters. In the
following, we will share our experience in using Everﬂow to
debug a variety of common DCN faults (§2).
7.1 Latency problem
A multi-tier search application complained about large la-
tency jitters between a set of servers. The application allo-
cates a strict per-tier latency bound of 1 ms, which includes
server processing time. Due to the use of ECMP routing in
our DCN (Fig 7(a)), there are hundreds of links that could
cause this problem. It was extremely difﬁcult for operators
to identify the culprit switch or link.
One possibility was to use traceroute to infer per-link la-
tency. However, traceroute measures the RTT between a
probing server and a switch, which is impacted by all the
links on the forward and reverse paths. Moreover, because
the traceroute probe is processed by the switch CPU, the
measured RTT will be inﬂated by the switch control plane
delay which may be highly variable. Thus, the RTT mea-
sured by traceroute will be very noisy and even unusable.
Our latency proﬁler was called for help.
It marked the
debug bit for the application trafﬁc sent or received on the
afﬂicted servers, and learned the links on the path. It sub-
sequently sent out guided probes to measure all the relevant
links in parallel. Fig 7(b) plots a portion of the latency time-
series on a subset of the links. Intermittently, Link A would
have a much larger latency variability than the other links,
sometimes even approaching 0.8 ms. This left little time for
servers to complete their computations. After further investi-
gation, the operators conﬁrmed that this problem was caused
by a misconﬁgured ECN (Explicit Congestion Notiﬁcation)
threshold that allowed excessive queue build up.
Summary. Unusually high latency is one of the most com-
mon problems in DCNs and is hard to debug using conven-
tional tools. The ﬁne-grained latency measurements pro-
vided by Everﬂow help operators localize the problem in
minutes instead of hours.
7.2 Packet drops
7.2.1 Blackhole
Many clients of an internal web service reported that a
fraction of their connection establishment requests were en-
countering timeouts. However, a few retrials of the connec-
tion requests would eventually succeed. This led to the vi-
olation of the SLAs (Service Level Agreements) of the web