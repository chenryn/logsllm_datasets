5.2 Synthesizing Malicious Augmented Data
Ideally, each synthetic data point can encode ⌊log2(c)⌋ bits of in-
formation where c is the number of classes in the output space of
the model. Algorithm 4 outlines our synthesis method. Similar to
the white-box attacks, we first extract a secret bit string s from
Dtrain. We then deterministically synthesize one data point for each
substring of length ⌊log2(c)⌋ in s.
Algorithm 4 Synthesizing malicious data
1: Input: A training dataset Dtrain, number of inputs to be syn-
thesized m, auxiliary knowledge K.
2: Output: Synthesized malicious data Dmal
3: Dmal ← ∅
4: s ← ExtractSecretBitString(Dtrain, m)
5: c ← number of classes in Dtrain
6: for each ⌊log2(c)⌋ bits s′ in s do
7:
8:
9:
10: end for
xmal ← GenData(K)
ymal ← BitsToLabel(s′)
Dmal ← Dmal ∪ {(xmal, ymal)}
Different types of data require different synthesis methods.
Synthesizing images. We assume no auxiliary knowledge for
synthesizing images. The adversary can use any suitable GenData
method: for example, generate pseudorandom images using the ad-
versary’s choice of pseudorandom function (PRF) (e.g., HMAC [39])
or else create sparse images where only one pixel is filled with a
(similarly generated) pseudorandom value.
We found the latter technique to be very effective in practice.
GenData enumerates all pixels in an image and, for each pixel,
creates a synthetic image where the corresponding pixel is set to
the pseudorandom value while other pixels are set to zero. The
same technique can be used with multiple pixels in each synthetic
image.
Synthesizing text. We consider two scenarios for synthesizing
text documents.
If the adversary knows the exact vocabulary of the training
dataset, he can use this vocabulary as the auxiliary knowledge
in GenData. A simple deterministic implementation of GenData
enumerates the tokens in the auxiliary vocabulary in a certain
order. For example, GenData can enumerate all singleton tokens
in lexicographic order, then all pairs of tokens in lexicographic
order, and so on until the list is as long as the number of synthetic
documents needed. Each list entry is then set to be a text in the
augmented training dataset.
If the adversary does not know the exact vocabulary, he can
collect frequently used words from some public corpus as the auxil-
iary vocabulary for generating synthetic documents. In this case, a
deterministic implementation of GenData pseudorandomly (with
a seed known to the adversary) samples words from the vocabulary
until generating the desired number of documents.
To generate a document in this case, our simple synthesis algo-
rithm samples a constant number of words (50, in our experiments)
from the public vocabulary and joins them as a single document.
The order of the words does not matter because the feature extrac-
tion step only cares whether a given word occurs in the document
or not.
This synthesis algorithm may occasionally generate documents
consisting only of words that do not occur in the model’s actual
vocabulary. Such words will typically be ignored in the feature
extraction phase, thus the resulting documents will have empty
features. If the attacker does not know the model’s vocabulary, he
cannot know if a particular synthetic document consists only of
out-of-vocabulary words. This can potentially degrade both the test
accuracy and decoding accuracy of the model.
In Section 6.7, we empirically measure the accuracy of the capacity-
abuse attack with a public vocabulary.
Decoding memorized information. Because our synthesis meth-
ods for augmented data are deterministic, the adversary can repli-
cate the synthesis process and query the trained model with the
same synthetic inputs as were used during training. If the model
is overfitted to these inputs, the labels returned by the model will
be exactly the same labels that were associated with these inputs
during training, i.e., the encoded secret bits.
If a model has sufficient capacity to achieve good accuracy and
generalizability on its original training data and to memorize mali-
cious training data, then acc(θ, Dmal) will be near perfect, leading
to low error when extracting the sensitive data.
5.3 Why Capacity Abuse Works
Deep learning models have such a vast memorization capacity that
they can essentially express any function to fit the data [75]. In our
case, the model is fitted not just to the original training dataset but
also to the synthetic data which is (in essence) randomly labeled. If
the test accuracy on the original data is high, the model is accepted.
If the training accuracy on the synthetic data is high, the adversary
can extract information from the labels assigned to these inputs.
Critically, these two goals are not in conflict. Training on mali-
ciously augmented datasets thus produces models that have high
quality on their original training inputs yet leak information on
the augmented inputs.
In the case of SVM and LR models, we focus on high-dimensional
and sparse data (natural-language text). Our synthesis method also
bits
f
params
Dataset
Data size
n
50K
10K
57K
d
3072
8742
7500
CIFAR10
LFW
FaceScrub (G)
FaceScrub (F)
News
Num Test
acc
460K 92.89
880K 87.83
460K 97.44
500K 90.08
2.6M 80.58
80.51
300K 90.13
IMDB
90.48
Table 1: Summary of datasets and models. n is the size of the
training dataset, d is the number of input dimensions. RES
stands for Residual Network, CNN for Convolutional Neu-
ral Network. For FaceScrub, we use the gender classification
task (G) and face recognition task (F).
1228M RES
692M CNN
3444M RES
176M SVM
LR
265M SVM
LR
11K 130K
25K 300K
produces very sparse inputs. Empirically, the likelihood that a syn-
thetic input lies on the wrong side of the hyperplane (classifier)
becomes very small in this high-dimensional space.
6 EXPERIMENTS
We evaluate our attack methods on benchmark image and text
datasets, using, respectively, gray-scale training images and ordered
tokens as the secret to be memorized in the model.
For each dataset and task, we first train a benign model using a
conventional training algorithm. We then train and evaluate a mali-
cious model for each attack method. We assume that the malicious
training algorithm has a hard-coded secret that can be used as the
key for a pseudorandom function or encryption.
All ML models and attacks were implemented in Python 2.7 with
Theano [70] and Lasagne [20]. The experiments were conducted
on a machine with two 8-core Intel i7-5960X CPUs, 64 GB RAM,
and three Nvidia TITAN X (Pascal) GPUs with 12 GB VRAM each.
6.1 Datasets and Tasks
Table 1 summarizes the datasets, models, and classification tasks
we used in our experiments. We use as stand-ins for sensitive data
several representative, publicly available image and text datasets.
CIFAR10 is an object classification dataset with 50,000 training
images (10 categories, 5,000 images per category) and 10,000 test
images [40]. Each image has 32x32 pixels, each pixel has 3 values
corresponding to RGB intensities.
Labeled Faces in the Wild (LFW) contains 13,233 images for
5,749 individuals [33, 45]. We use 75% for training, 25% for test-
ing. For the gender classification task, we use additional attribute
labels [42]. Each image is rescaled to 67x42 RGB pixels from its
original size, so that all images have the same size.
FaceScrub is a dataset of URLs for 100K images [59]. The tasks are
face recognition and gender classification. Some URLs have expired,
but we were able to download 76,541 images for 530 individuals.
We use 75% for training, 25% for testing. Each image is rescaled to
50x50 RGB pixels from its original size.
20 Newsgroups is a corpus of 20,000 documents classified into 20
categories [44]. We use 75% for training, 25% for testing.
IMDB Movie Reviews is a dataset of 50,000 reviews labeled with
positive or negative sentiment [52]. The task is (binary) sentiment
analysis. We use 50% for training, 50% for testing.
6.2 ML Models
Convolutional Neural Networks. Convolutional Neural Networks
(CNN) [47] are composed of a series of convolution operations as
building blocks which can extract spatial-invariant features. The
filters in these convolution operations are the parameters to be
learned. We use a 5-layer CNN for gender classification on the LFW
dataset. The first three layers are convolution layers (32 filters in
the first layer, 64 in the second, 128 in the third) followed by a max-
pooling operation which reduces the size of convolved features by
half. Each filter in the convolution layer is 3x3. The convolution
output is connected to a fully-connected layer with 256 units. The
latter layer connects to the output layer which predicts gender.
We then feed the BOW vectors into an SVM or LR model. For 20
Newsgroups, there are 20 categories and we apply the One-vs-All
method to train 20 binary classifiers to predict whether a data point
belongs to the corresponding class or not. We train linear models
using AdaGrad [23], a variant of SGD with adaptive adjustment to
the learning rate of each parameter. We set the mini-batch size to
128, learning rate to 0.1, and the number of epochs for training to
50 as AdaGrad converges very fast on these linear models.
6.3 Evaluation Metrics
Because we aim to encode secrets in a model while preserving its
quality, we measure both the attacker’s decoding accuracy and the
1https://github.com/Lasagne/Recipes/blob/master/modelzoo/resnet50.py
For the hyperparameters, we set the mini-batch size to be 128,
learning rate to be 0.1, and use SGD with Nesterov Momentum
for optimizing the loss function. We also use the l2-norm as the
regularizer with λ set to 10−5. We set the number of epochs for
training to 100. In epochs 40 and 60, we decrease the learning rate
by a factor of 0.1 for better convergence. This configuration is
inherited from the residual-network implementation in Lasagne.1
Residual Networks. Residual networks (RES) [31] overcome the
gradient vanishing problem when optimizing very deep CNNs by
adding identity mappings from lower layers to high layers. These
networks achieved state-of-the-art performance on many bench-
mark vision datasets in 2016.
We use a 34-layer residual network for CIFAR10 and FaceScrub.
Although the network has fewer parameters than CNN, it is much
deeper and can learn better representations of the input data. The
hyperparameters are the same as for the CNN.
Bag-of-Words and Linear Models. For text datasets, we use a
popular pipeline that extracts features using Bag-of-Words (BOW)
and trains linear models.
BOW maps each text document into a vector in R|V | where V is
the vocabulary of tokens that appear in the corpus. Each dimension
represents the count of that token in the document. The vectors
are extremely sparse because only a few tokens from V appear in
any given document.
Dataset
CIFAR10
LFW
FaceScrub (G)
FaceScrub (F)
News
IMDB
b
f
RES
18
CNN 22
20
RES
18
SVM 22
LR
SVM 22
LR
Encoded bits Test acc ±δ
8.3M 92.75 −0.14
17.6M 87.69 −0.14
9.2M 97.33 −0.11
8.3M 89.95 −0.13
57.2M 80.60 +0.02
80.40 −0.11
6.6M 90.12 −0.01
90.31 −0.17
Table 2: Results of the LSB encoding attack. Here f is the
model used, b is the maximum number of lower bits used be-
yond which accuracy drops significantly, δ is the difference
with the baseline test accuracy.
Figure 2: Test accuracy of the CIFAR10 model with different
amounts of lower bits used for the LSB attack.
k
model’s classification accuracy on the test data for its primary task
(accuracy on the training data is over 98% in all cases). Our attacks
introduce minor stochasticity into training, thus accuracy of mali-
ciously trained models occasionally exceeds that of conventionally
trained models.
Metrics for decoding images. For images, we use mean absolute
pixel error (MAPE). Given a decoded image x′ and the original
i |. Its range is [0,
image x with k pixels, MAPE is 1
k
255], where 0 means the two images are identical and 255 means
every pair of corresponding pixels has maximum mismatch.
Metrics for decoding text. For text, we use precision (percentage
of tokens from the decoded document that appear in the original
document) and recall (percentage of tokens from the original docu-
ment that appear in the decoded document). To evaluate similarity
between the decoded and original documents, we also measure
their cosine similarity based on their feature vectors constructed
from the BOW model with the training vocabulary.
i =1 |xi − x′
6.4 LSB Encoding Attack
Table 2 summarizes the results for the LSB encoding attack.
Encoding. For each task, we compressed a subset of the training
data, encrypted it with AES in CBC mode, and wrote the ciphertext
bits into the lower bits of the parameters of a benignly trained
Dataset
CIFAR10
LFW
FaceScrub (G)
FaceScrub (F)
f
λc
0.1
RES
1.0
CNN 0.1
1.0
0.1
1.0
0.1
1.0
RES
Test acc Decode
±δ MAPE
52.2
29.9
35.8