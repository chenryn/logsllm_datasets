1.08hrs
0.1µs
4.26hrs 47.9µs 11.8µs
8.29hrs 14.6µs 13.7µs
4.92hrs 58.8µs 84.1µs
2.27hrs 25.1µs 20.6µs
3.30hrs
0.2µs
0.5µs
Neg
0.2µs
Pos
0.3µs
Pos
1.16hrs
70.3µs
4.88hrs 45.2µs 32.5µs 1061.4µs
8.71hrs 15.1µs 13.4µs 2528.7µs
9.85hrs 62.0µs 88.5µs 6923.7µs
4.67hrs 32.4µs 35.6µs 1681.5µs
3.75hrs
104.0µs
0.6µs
0.2µs
Neg
21.1µs
457.7µs
734.1µs
1605.5µs
596.8µs
46.5µs
Pos
15.0µs
478.9µs
546.2µs
1588.1µs
251.6µs
54.0µs
Neg
12.0µs
118.0µs
137.5µs
841.8µs
206.4µs
24.0µs
Pos
149.2µs
3111.1µs
9976.8µs
50273.3µs
3727.5µs
1377.9µs
Neg
132.5µs
1720.4µs
2416.7µs
41473.2µs
1029.9µs
374.6µs
Table 6: Computation time of proximity sketch, proximity embedding, common neighbor and shortest path distance.
Digg
Flickr
LiveJournal
MySpace
YouTube
Wikipedia
positive
negative
all
positive
negative
all
positive
negative
all
positive
negative
all
positive
negative
all
positive
negative
all
PA
CN
AA
PRP
Katz
GD
90.8% 100% 32.4% 32.1% 56.6% 46.5%
2.7% 100% 1.2% 1.1% 29.9% 35.6%
4.8% 100% 11.2% 11.1% 37.8% 39.2%
100% 67.5% 63.3% 63.1% 97.8% 97.3%
100% 80.1% 0.1% 0.1% 96.0% 69.9%
100% 77.5% 12.8% 12.8% 96.4% 75.4%
99.4% 100% 27.5% 27.5% 74.7% 98.6%
93.1% 100% 0.2% 0.03% 91.8% 93.1%
94.5% 100% 5.7% 5.6% 88.3% 94.5%
100% 100% 72.8% 72.8% 100% 100%
100% 100% 1.5% 1.5% 100% 95.0%
100% 100% 15.3% 15.3% 100% 95.9%
100% 100% 31.8% 31.9% 99.1% 100%
100% 100% 0.3% 0.2% 91.2% 100%
100% 100% 7.3% 7.1% 93.6% 100%
73.2% 100% 44.7% 44.6% 74.5% 74.4%
88.9% 100% 4.3% 4.3% 82.9% 80.7%
85.1% 100% 14.3% 14.3% 80.9% 79.1%
Table 7: Applicable ratio of basic link predictors (fraction of
positive/negative/all samples with non-zero proximity values)
For composite link predictor, we present the results using the
REPtree decision tree learner in WEKA machine learning pack-
age [21]. As noted in Section 3.1, REPtree is easy to control, vi-
sualize, and interpret.
It also achieves accuracy similar to other
machine learning algorithms.
4.3.2 Evaluation of Basic Link Predictors
We calculate the trade-off curves of false positives and false neg-
atives for each basic link predictor as shown in Figure 8. In addition
to accuracy, we compute the fraction of node pairs with non-zero
proximity values for each proximity measure in Table 7.
In Figure 8, we ob-
Variation across predictors and datasets.
serve signiﬁcant variation in link prediction accuracy both across
different datasets and across different link predictors:
• Neighborhood based proximity measures. The common neighbor
(denoted as CN) and the Adamic Adar (denoted as AA) perform
well in LiveJournal, MySpace, and Flickr. But in terms of AR,
both CN and AA could cover only two-hop relations, resulting in
the worst AR. As a result, only about 30% of samples are non-zero
in LiveJournal, Digg, and YouTube datasets (shown in Table 7). In
comparison, both preferential attachment (PA) and PageRank prod-
uct (PRP) yield much higher AR. PRP performs best in YouTube
and Wikipedia datasets (in Figure 8(c,f)), but performs worst in
LiveJournal and Flickr datasets (in Figure 8(b,d)). These results
suggest that each individual measure has its own merits and limita-
tions. None of them performs universally well over all the datasets.
• Path-ensemble based proximity measure. We evaluate using two
different damping factors β = 0.05 and 0.005. The results of using
these two different damping factors are similar, and we only report
the results using β = 0.05 in the interest of brevity. Katz achieves
both low false negative rate and low false positive rate. Katz is the
best in Digg and MySpace datasets, and the second best in Live-
Journal and Flickr. In YouTube dataset, Katz does not give a good
Network
Digg
Flickr
LiveJournal
MySpace
YouTube
Wikipedia
# of high deg. links # of total links % of high deg. links
38.6%
77.4%
96.9%
48.4%
72.1%
55.7%
1,860,703
23,535,630
60,058,439
43,877,840
9,392,367
18,936,865
4,813,668
30,393,940
61,921,736
90,629,452
13,017,064
33,974,708
Table 8: Proportion of links connected to top 0.1% highest de-
gree nodes in each network.
trade-off curve. In Wikipedia dataset, Katz performs slightly worse
than PRP, which is optimized for predicting hyperlink structures.
We will further investigate the reason behind the performance of
Katz later in this section.
• Graph distance based proximity measure. The graph distance
(denoted as GD) achieves high accuracy in Digg, LiveJournal, MyS-
pace, Flickr and Wikipedia. But it has several important limi-
tations. First, shortest path distance is determined solely by the
shortest path and takes only integer values. This means that it has
coarse granularity and introduces many ties. Second, the compu-
tation of shortest path distance is much more expensive in large
networks [51] (also shown in Table 6).
• Effect of link symmetry. To understand the effect of link asym-
metry, we make the friendship relation as reciprocal by adding re-
verse link in all existing node pairs. Table 2 shows the fraction of
asymmetric links in our dataset. Figure 9 shows how the symmet-
ric predictors performs in asymmetric datasets. In Wikipedia and
Digg datasets, adding reverse edges improves the accuracy. But
adding reverse edges does not always help; it stays almost the same
in Flickr or a bit worse in LiveJournal.
A good empirical performance indicator. Despite the signiﬁcant
variation, we ﬁnd that the ranking between sophisticated predictors
such as Katz versus simple predictors such as common neighbors
and Adamic/Adar can be qualitatively predicted based on the frac-
tion of edges contributed by the highest degree nodes as follows.
Table 8 shows the number of links incoming and outgoing from
the highest degree nodes, the number of links in the entire network
and their proportion, where a node is considered a high degree node
if its node degree (combining incoming and outgoing degrees) is
among the top 0.1% highest node degrees. Ranking different net-
works based on the percentage of links connected to such highest
degree nodes (in decreasing order), we obtain:
LiveJournal > F lickr > Y ouT ube
≫ W ikipedia > M ySpace > Digg
which is consistent with the ranking between direct proximity mea-
sures and sophisticated measures shown in Figure 8 and Figure 9.
The ranking shows that as the high degree node’s coverage in-
creases, direct proximity measures (such as the number of common
neighbors and shortest path distances) perform better than Katz and
vice versa.
332 100
 80
 60
 40
 20
 100
 80
 60
 40
 20
t
e
a
r
e
v
i
t
a
g
e
n
e
s
a
l
f
e
t
a
r
e
v
i
t
a
g
e
n
e
s
a
l
f
PA
PRP
CN
AA
GD
Katz
REPtree
 0
 0.001
PA
PRP
CN
AA
GD
Katz
REPtree
 0
 0.001
 100
 80
 60
 40
 20
 100
 80
 60
 40
 20
t
e
a
r
e
v
i
t
a
g
e
n
e
s
a
l
f
e
t
a
r
e
v
i
t
a
g
e
n
e
s
a
l
f
PA
PRP
CN
AA
GD
Katz
REPtree
 0
 0.001
PA
PRP
CN
AA
GD
Katz
REPtree
 0
 0.001
 100
 80
 60
 40
 20
 100
 80
 60
 40
 20
t
e
a
r
e
v
i
t
a
g
e
n
e
s
a
l
f
e
t
a
r
e
v
i
t
a
g
e
n
e
s
a
l
f
PA
PRP
CN
AA
GD
Katz
REPtree
 0
 0.001
PA
PRP
CN
AA
GD
Katz
REPtree
 0
 0.001
 0.01
 0.1
 1
 10
 100
false positive rate
(a) Digg
 0.01
 0.1
 1
 10
 100
false positive rate
(b) Flickr
 0.01
 0.1
 1
 10
 100
false positive rate
(c) LiveJournal
 0.01
 0.1
 1
 10
 100
false positive rate
(d) MySpace
 0.01
 0.1
 1
 10
 100
false positive rate
(e) YouTube
 0.01
 0.1
 1
 10
 100
false positive rate
(f) Wikipedia
Figure 8: Link prediction accuracy for different online social networks
4.3.3 Evaluation of Composite Link Predictor
Next we combine multiple proximity measures to improve link
prediction accuracy. When building a decision tree, at each node,
REPtree chooses the best attribute (e.g., proximity measure) which
splits samples into different classes (e.g., positive and negative) in
the most effective way using information gain as criterion. To draw
the entire accuracy trade-off curve, we vary the weights of positive
and negative samples in the training set. When the weights of pos-
itive samples are large, the learner focuses more on positive sam-
ples in classiﬁcation and tries to minimize false negatives; when
the weights of negative samples are large, the learner focuses more
on negative samples and tries to minimize false positives.
Figure 10 depicts examples of decision trees in Digg, Flickr,
and Wikipedia (where the weights of positive:negative samples are
1:100, 1:10, and 1:100, respectively). The decision process starts
from the root node, it drills down by examining the corresponding
metric value until reaching one of leaf nodes.
Figure 8 shows the performance of decision tree in different on-
line social networks. We observe that REPtree consistently achieves
the best accuracy. Speciﬁcally, REPtree outperforms the best basic
link predictors in Flickr, YouTube, and Wikipedia dataset. In Fig-
ure 8 (b) and (c), while the overall accuracy trade-off curve for
REPtree is very close to the curve for shortest path distance, the
composite link predictor provides much better resolution and ﬁlls
in the intermediate points missed by the shortest path predictor.