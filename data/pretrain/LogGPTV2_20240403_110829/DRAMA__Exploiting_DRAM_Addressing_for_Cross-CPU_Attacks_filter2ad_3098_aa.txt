title:DRAMA: Exploiting DRAM Addressing for Cross-CPU Attacks
author:Peter Pessl and
Daniel Gruss and
Cl&apos;ementine Maurice and
Michael Schwarz and
Stefan Mangard
DRAMA: Exploiting DRAM Addressing  
for Cross-CPU Attacks
Peter Pessl, Daniel Gruss, Clémentine Maurice, Michael Schwarz, and Stefan Mangard,  
Graz University of Technology
 https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/pessl
This paper is included in the Proceedings of the 25th USENIX Security SymposiumAugust 10–12, 2016 • Austin, TXISBN 978-1-931971-32-4Open access to the Proceedings of the 25th USENIX Security Symposium is sponsored by USENIX DRAMA: Exploiting DRAM Addressing for Cross-CPU Attacks
Peter Pessl, Daniel Gruss, Cl´ementine Maurice, Michael Schwarz and Stefan Mangard
Graz University of Technology, Austria
Abstract
In cloud computing environments, multiple tenants are
often co-located on the same multi-processor system.
Thus, preventing information leakage between tenants is
crucial. While the hypervisor enforces software isola-
tion, shared hardware, such as the CPU cache or mem-
ory bus, can leak sensitive information. For security rea-
sons, shared memory between tenants is typically dis-
abled. Furthermore, tenants often do not share a physical
CPU. In this setting, cache attacks do not work and only
a slow cross-CPU covert channel over the memory bus is
known. In contrast, we demonstrate a high-speed covert
channel as well as the first side-channel attack working
across processors and without any shared memory. To
build these attacks, we use the undocumented DRAM
address mappings.
We present two methods to reverse engineer the map-
ping of memory addresses to DRAM channels, ranks,
and banks. One uses physical probing of the memory
bus, the other runs entirely in software and is fully au-
tomated. Using this mapping, we introduce DRAMA at-
tacks, a novel class of attacks that exploit the DRAM row
buffer that is shared, even in multi-processor systems.
Thus, our attacks work in the most restrictive environ-
ments. First, we build a covert channel with a capacity
of up to 2 Mbps, which is three to four orders of mag-
nitude faster than memory-bus-based channels. Second,
we build a side-channel template attack that can automat-
ically locate and monitor memory accesses. Third, we
show how using the DRAM mappings improves existing
attacks and in particular enables practical Rowhammer
attacks on DDR4.
1
Introduction
Due to the popularity of cloud services, multiple tenants
sharing the same physical server through different vir-
tual machines (VMs) is now a common situation.
In
such settings, a major requirement is that no sensitive
information is leaked between tenants, therefore proper
isolation mechanisms are crucial to the security of these
environments. While software isolation is enforced by
hypervisors, shared hardware presents risks of informa-
tion leakage between tenants. Previous research shows
that microarchitectural attacks can leak secret informa-
tion of victim processes, e.g., by clever analysis of data-
dependent timing differences. Such side-channel mea-
surements allow the extraction of secret information like
cryptographic keys or enable communication over isola-
tion boundaries via covert channels.
Cloud providers can deploy different hardware config-
urations, however multi-processor systems are becoming
ubiquitous due to their numerous advantages. They offer
high peak performance for parallelized tasks while en-
abling sharing of other hardware resources such as the
DRAM. They also simplify load balancing while still
keeping the area and cost footprint low. Additionally,
cloud providers now commonly disable memory dedu-
plication between VMs for security reasons.
To attack such configurations, successful and practical
attacks must comply with the following requirements:
1. Work across processors: As these configurations
are now ubiquitous, an attack that does not work
across processors is severely limited and can be triv-
ially mitigated by exclusively assigning processors
to tenants or via the scheduler.
2. Work without any shared memory: With memory
deduplication disabled, shared memory is not avail-
able between VMs. All attacks that require shared
memory are thus completely mitigated in cross-VM
settings with such configurations.
In the last years, the most prominent and well-studied
example of shared-hardware exploits is cache attacks.
They use the processor-integrated cache and were shown
to be effective in a multitude of settings, such as cross-
VM key-recovery attacks [9, 12, 20, 30], including at-
tacks across cores [5, 14, 16, 28]. However, due to the
USENIX Association  
25th USENIX Security Symposium  565
1
cache being local to the processor, these attacks do not
work across processors and thus violate requirement 1.
Note that in a recent concurrent work, Irazoqui et al.
[11] presented a cross-CPU cache attack which exploits
cache coherency mechanisms in multi-processor sys-
tems. However, their approach requires shared mem-
ory and thus violates requirement 2. The whole class
of cache attacks is therefore not applicable in multi-
processor systems without any shared memory.
Other attacks leverage the main memory that
is
a shared resource even in multi-processor systems.
Xiao et al. [26] presented a covert channel that exploits
memory deduplication. This covert channel has a low
capacity and requires the availability of shared memory,
thus violating requirement 2. Wu et al. [25] presented a
covert channel exploiting the locking mechanism of the
memory bus. While this attack works across processors,
the capacity of the covert channel is orders of magnitude
lower than that of current cache covert channels.
Therefore, only a low capacity covert channel and no
side-channel have been showed with the two aforemen-
tioned requirements so far. In contrast, we demonstrate
two attacks that do not use shared memory and work
across processors: a high-speed covert channel as well
as the first side-channel attack.
Contributions. Our attacks require knowledge of the un-
documented mapping of memory addresses to DRAM
channels, ranks, and banks. We therefore present two
methods to reverse engineer this mapping. The first
method retrieves the correct addressing functions by per-
forming physical probing of the memory bus. The sec-
ond method is entirely software-based, fully automatic,
and relies only on timing differences.1 Thus, it can be
executed remotely and enables finding DRAM address
mappings even in VMs in the cloud. We reverse en-
gineered the addressing functions on a variety of pro-
cessors and memory configurations. Besides consumer-
grade PCs, we also analyzed a dual-CPU server system
– similar to those found in cloud setups – and multiple
recent smartphones.
In particular,
Using this reverse-engineered mapping, we present
DRAMA attacks, a novel class of attacks that exploit
the DRAM Addressing.
they leverage
DRAM row buffers that are a shared component in multi-
processor systems. Our attacks require that at least one
memory module is shared between the attacker and the
victim, which is the case even in the most restrictive set-
tings. In these settings, attacker and victim cannot ac-
cess the same memory cells, i.e., we do not circumvent
system-level memory isolation. We do not make any as-
sumptions on the cache, nor on the location of executing
1The source code of this reverse-engineering tool and exem-
plary DRAMA attacks can be found at https://github.com/IAIK/
drama.
cores, nor on the availability of shared memory such as
cross-VM memory deduplication.
First, we build a covert channel that achieves transmis-
sion rates of up to 2 Mbps, which is three to four orders
of magnitude faster than previously presented memory-
bus based channels. Second, we build a side channel that
allows to automatically locate and monitor memory ac-
cesses, e.g., user input or server requests, by perform-
ing template attacks. Third, we show how the reverse-
engineered mapping can be used to improve existing at-
tacks. Existing Flush+Reload cache attacks use an in-
correct cache-miss threshold, introducing noise and re-
ducing the spatial accuracy. Knowledge of the DRAM
address mapping also enables practical Rowhammer at-
tacks on DDR4.
Outline. The remainder of the paper is organized as fol-
lows. In Section 2, we provide background information
on side channels on shared hardware, on DRAM, and
In Section 3, we provide
on the Rowhammer attack.
definitions that we use throughout the paper.
In Sec-
tion 4, we describe our two approaches to reverse engi-
neer the DRAM addressing and we provide the reverse-
engineered functions.
In Section 5, we build a high-
speed cross-CPU DRAMA covert channel. In Section 6,
we build a highly accurate cross-CPU DRAMA side
channel attack. In Section 7, we show how the knowl-
edge of the DRAM addressing improves cache attacks
like Flush+Reload and we show how it makes Rowham-
mer attacks practical on DDR4 and more efficient on
DDR3. We discuss countermeasures against our attack
in Section 8. We conclude in Section 9.
2 Background and related work
In this section, we discuss existing covert and side chan-
nels and give an introduction to DRAM. Furthermore, we
briefly explain the Rowhammer bug and its implications.
2.1 Hardware covert and side channels
Attacks exploiting hardware sharing can be grouped into
two categories. In side-channel attacks, an attacker spies
on a victim and extracts sensitive information such as
cryptographic keys. In covert channels however, sender
and receiver are actively cooperating to exchange infor-
mation in a setting where they are not allowed to, e.g.,
across isolation boundaries.
Cache attacks. Covert and side channels using the CPU
cache exploit the fact that cache hits are faster than
cache misses. The methods Prime+Probe [14,16,19] and
Flush+Reload [2, 12, 28] have been presented to either
build covert or side channels. These two methods work
at a different granularity: Prime+Probe can spy on cache
566  25th USENIX Security Symposium 
USENIX Association
2
sets, while Flush+Reload has the finer granularity of a
cache line but requires shared memory, such as shared
libraries or memory deduplication.
Attacks targeting the last-level cache are cross-core,
but require the sender and receiver to run on the same
physical CPU. Gruss et al. [5] implemented cross-core
covert channels using Prime+Probe and Flush+Reload
as well as a new one, Flush+Flush, with the same
protocol to normalize the results. The covert channel
using Prime+Probe achieves 536 Kbps, Flush+Reload
2.3 Mbps, and Flush+Flush 3.8 Mbps. The most recent
cache attack by Irazoqui et al. [11] exploits cache co-
herency mechanisms and work across processors.
It
however requires shared memory.
An undocumented function maps physical addresses
to the slices of the last-level cache. However, this func-
tion has been reverse engineered in previous work [9,15,
29], enhancing existing attacks and enabling attacks in
new environments.
Memory and memory bus. Xiao et al. [26] presented
a covert channel that exploits memory deduplication. In
order to save memory, the hypervisor searches for identi-
cal pages in physical memory and merges them across
VMs to a single read-only physical page. Writing to
this page triggers a copy-on-write page fault, incurring
a significantly higher latency than a regular write access.
The authors built a covert channel that achieves up to
90 bps, and 40 bps on a system under memory pressure.
Wu et al. [25] proposed a bus-contention-based covert
channel, that uses atomic memory operations locking the
memory bus. This covert channel achieves a raw band-
width of 38 Kbps between two VMs, with an effective
capacity of 747 bps with error correction.
2.2 DRAM organization
Modern DRAM is organized in a hierarchy of channels,
DIMMs, ranks, and banks. A system can have one or
more channels, which are physical links between the
DRAM modules and the memory controller. Channels
are independent and can be accessed in parallel. This
allows distribution of the memory traffic, increasing the
bandwidth, and reducing the latency in many cases. Mul-
tiple Dual Inline Memory Modules (DIMMs), which are
the physical memory modules attached to the mainboard,
can be connected to each channel. A DIMM typically has
one or two ranks, which often correspond to the front
and back of the physical module. Each rank is com-
posed of banks, typically 8 on DDR3 DRAM and 16 on
DDR4 DRAM. In the case of DDR4, banks are addition-
ally grouped into bank groups, e.g., 4 bank groups with
4 banks each. Banks finally contain the actual memory
arrays which are organized in rows (typically 214 to 217)
and columns (often 210). On PCs, the DRAM word size
and bus width is 64 bits, resulting in a typical row size of
8 KB. As channel, rank and bank form a hierarchy, two
addresses can only be physically adjacent in the DRAM
chip if they are in the same channel, DIMM, rank and
bank. In this case we just use the term same bank.
The memory controller, which is integrated into mod-
ern processors, translates physical addresses to channels,
DIMMs, ranks, and banks. AMD publicly documents the
addressing function used by its products (see, e.g., [1, p.
345]), however to the best of our knowledge Intel does
not. The mapping for one Intel Sandy Bridge machine in
one memory configuration has been reverse engineered
by Seaborn [23]. However, Intel has changed the map-
ping used in its more recent microarchitectures. Also,
the mapping necessarily differs when using other mem-
ory configurations, e.g., a different number of DIMMs.
The row buffer. Apart from the memory array, each
bank also features a row buffer between the DRAM cells
and the memory bus. From a high-level perspective, it
behaves like a directly-mapped cache and stores an entire
DRAM row. Requests to addresses in the currently active
row are served directly from this buffer. If a different row
needs to be accessed, then the currently active row is first
closed (with a pre-charge command) and then the new
row is fetched (with a row-activate command). We call
such an event a row conflict. Naturally, such a conflict
leads to significantly higher access times compared to re-
quests to the active row. This timing difference will later
serve as the basis for our attacks and for the software-
based reverse-engineering method. Note that after each
refresh operation, a bank is already in the pre-charged
state. In this case, no row is currently activated.
Independently of our work, Hassan et al. [7] also pro-
posed algorithms to reverse engineer DRAM functions
based on timing differences. However, their approach
requires customized hardware performance-monitoring
units. Thus, they tested their approach only in a simu-
lated environment and not on real systems. Concurrently
to our work, Xiao et al. [27] proposed a method to re-
verse engineer DRAM functions based on the timing dif-
ferences caused by row conflicts. Although their method
is similar to ours, their focus is different, as they used
the functions to then perform Rowhammer attacks across
VMs.
DRAM organization for multi-CPU systems. In mod-
ern multi-CPU server systems, each CPU features a ded-
icated memory controller and attached memory. The
DRAM is still organized in one single address space and
is accessible by all processors. Requests for memory at-
tached to other CPUs are sent over the CPU interconnect,
e.g., Intel’s QuickPath Interconnect (QPI). This memory
design is called Non-Uniform Memory Access (NUMA),
as the access time depends on the memory location.
USENIX Association  
25th USENIX Security Symposium  567
3
On our dual Haswell-EP setup, the organization of this
single address space can be configured for the expected
workload. In interleaved mode, the memory is split into
small slices which are spliced together in an alternating
fashion. In non-interleaved mode, each CPUs memory
is kept in one contiguous physical-address block. For
instance, the lower half of the address space is mapped to
the first CPUs memory, whereas the upper half is mapped
to the second CPUs memory.
2.3 The Rowhammer bug
The increasing DRAM density has led to physically
smaller cells, which can thus store smaller charges. As a
result, the cells have a lower noise margin and the level
of parasitic electrical interaction is potentially higher, re-
sulting in the so-called Rowhammer bug [8, 13, 18].
This bug results in corruption of data, not in rows that
are directly accessed, but rather in adjacent ones. When
performing random memory accesses, the probability for
such faults is virtually zero. However, it rises drastically
when performing accesses in a certain pattern. Namely,
flips can be caused by frequent activation (hammering) of
adjacent rows. As data needs to be served from DRAM
and not the cache, an attack needs to either flush data
from the cache using the clflush instruction in native
environments [13], or using cache eviction in other more
restrictive environments, e.g., JavaScript [4].
Seaborn [22] implemented two attacks that exploit the
Rowhammer bug, showing the severity of faulting single
bits for security. The first exploit is a kernel privilege es-
calation on a Linux system, caused by a bit flip in a page
table entry. The second one is an escape of Native Client
sandbox caused by a bit flip in an instruction sequence
for indirect jumps.
3 Definitions
In this section we provide definitions for the terms row
hit and row conflict. These definitions provide the basis
for our reverse engineering as well as the covert and side
channel attacks.
Every physical memory location maps to one out of
many rows in one out of several banks in the DRAM.
Considering a single access to a row i in a bank there are
two major possible cases:
1. The row i is already opened in the row buffer. We
call this case a row hit.
2. A different row j (cid:31)= i in the same bank is opened.
Considering frequent alternating accesses to two (or
We call this case a row conflict.
more) addresses we distinguish three cases:
1. The addresses map to different banks. In this case
the accesses are independent and whether the ad-
4
dresses have the same row indices has no influence
on the timing. Row hits are likely to occur for the
accesses, i.e., access times are low.
2. The addresses map to the same row i in the same
bank. The probability that the row stays open in
between accesses is high, i.e., access times are low.
3. The addresses map to the different rows i (cid:31)= j in the
same bank. Each access to an address in row i will
close row j and vice versa. Thus, row conflicts oc-
cur for the accesses, i.e., access times are high.
To measure the timing differences of row hits and row
conflicts, data has to be flushed from the cache. Fig-
ure 1 shows a comparison of standard histograms of ac-
cess times for cache hits and cache misses. Cache misses
are further divided into row hits and row conflicts. For
this purpose an unrelated address in the same row was ac-
cessed to cause a row hit and an unrelated address in the
same bank but in a different row was accessed to cause a
row conflict. We see that from 180 to 216 cycles row hits
occur, but no row conflicts (cf. highlighted area in Fig-
ure 1). In the remainder, we build different attacks that
are based on this timing difference between row hits and
row conflicts.
4 Reverse engineering DRAM addressing
In this section, we present our reverse engineering of the
DRAM address mapping. We discuss two approaches,
the first one is based on physical probing, whereas the
second one is entirely software-based and fully auto-
mated. Finally, we present the outcome of our analy-
sis, i.e., the reverse-engineered mapping functions.
In
the remainder of this paper, we denote with a a physical
memory address. ai denotes the i-th bit of an address.
4.1 Linearity of functions
The DRAM addressing functions are reverse engineered
in two phases. First, a measuring phase and second, a