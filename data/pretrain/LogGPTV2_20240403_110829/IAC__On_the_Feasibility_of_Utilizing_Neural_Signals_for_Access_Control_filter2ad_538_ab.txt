The Emotiv headset collects EEG data at 128 sample per second 1.
The captured EEG signals are then converted to digital form. The
digital data are then processed and transmitted as encrypted data
to the stimuli computer via USB dongle receiver. This proprietary
USB dongle communicates with Emotiv headset in 2.4 GHz fre-
quency. Emotiv also provides companion software for its device.
EmoEngine is a software component for post-processing data. This
software exposes data to BCI applications via the Emotiv Applica-
tion Programming Interface (Emotiv API). Pure.EEG is a software
component for data collection, which is used in this study. Pure.EEG
collects data from the Emotiv device independently via the USB
dongle and can upload data to the cloud and download from the
cloud recorded sessions.
BCI. Brain-Computer Interface (BCI) is a new type of user interface
where our neural signals are interpreted into machine understand-
able commands. Here, it converts brainwaves into digital commands
1 The device internally collected data at a frequency of 2048 Hz, then down-sampled
to 128 Hz before sending it to the computer.
ACSAC’18, December 3–7,2018, San Juan, PR, USA
which instruct machine to conduct various tasks. For example, re-
searchers have shown it is possible to use BCI to allow patients
who suer from neurological diseases like locked-in syndrome to
spell words and move computer cursors [9, 62] or allow patients to
move a prosthesis [60]. With BCI, instead of using physical interac-
tions human can use mind interaction. In our study, we choose this
interface as it can directly reveal the user’s intent thus is resistant
to some perception manipulation attacks (e.g., clickjacking [30]).
3 INTENT-DRIVEN ACCESS CONTROL
In this section, we introduce how our new access model would
work. We start with the threat model and assumptions. Then we
show how to realize the model with BCI.
3.1 Threat Model and Assumptions
We make following assumptions for constructing a BCI-based in-
tent inference engine and use it to authorize access to user-owned
sensitive resources and sensors. We assume the OS is trusted. At-
tacks that exploit OS vulnerabilities to gain illegal access to the
protected resources and sensors are out-of-scope. We also assume
the OS already employs a permission model that considers con-
text integrity (e.g., an ask-on-every-use model). Our goal is not
to replace the existing access control system, but to make it more
user-friendly.
We assume our adversary is skilled application developer aim-
ing to gain access to the user-owned resources/sensors without
user’s consent and abuse such access. Attackers are allowed to
launch UI attacks (e.g., clickjacking) to mislead users. With one
exception, to correctly identify which app the user is interacting
with, the OS should not allow transparent overlay [26]. We consider
phishing-style attacks (e.g., explicitly instructing users to perform
sensitive operations) and side-channel attacks (that leak protected
information) out-of-scope.
Regarding access to the raw EEG data, we envision a restricted
programming model. Specically, existing platforms like Emotiv
expose raw EEG data to any applications build against their APIs.
This programming model has been proven to be vulnerable to
side-channel attacks that can infer user’s sensitive and private in-
formation [10, 25, 40, 44]. To prevent such attacks, we assume a pro-
gramming model that is similar to the voice assistants [4, 42]. That
is, the raw EEG data is exclusively accessed by a trusted module,
which will interpret the data and translate into app understandable
events. We assume our inference engine to be part of this module
and is implemented correctly. We also do not consider physical
attacks against the EEG sensors.
3.2 IAC via BCI
Our BCI-based intent-driven access control system works similarly
to the systems proposed in [47, 66]. In particular, the baseline access
control system will prompt the user to authorize every access to
protected resources. The goal of IAC is to minimize the number
of the prompts by checking whether the access is intended by the
user. Specically, a legitimate access to protected resource should
be (1) initiated by user’s intent to perform a certain task under the
ACSAC’18, December 3–7,2018, San Juan, PR, USA
M. Rahman et al.
Figure 2: Overview of IAC’s Architecture. IAC will 1○ continuously monitor the brain signals using the EEG sensor and user
interaction with the system. Upon an input event, IAC will create an ERP, 2○ preprocess the raw EEG data to get purer signals,
3○ extract feature vector from the puried signals, 4○ feed the extracted features to a ML model to infer the user’s intent. In
step 5○, if the ML gives enough decision condence, IAC will directly 7○ authorize access to protected resources. Otherwise, it
will 6○ prompt users to authorize the access and improve the ML model with the feedback loop.
presented app context2 and (2) within the expected set of necessary
resources for that task. Therefore we can create intent-based access
control mechanism based on ERPs and use them as the inputs to a
machine learning classier. The data ow diagram for IAC is given
in Figure 2.
Figure 3: Example permission request. Compare to existing
permission request, the biggest dierence is IAC also asks
for intended task (e.g., taking a photo).
To train the classier, we use user’s explicit answers to the ask-
on-every-use prompt as the ground truth. However, instead of just
asking the user to authorize the access, IAC will also list a set
of tasks that rely the requested resource for user to choose (e.g.,
Figure 3). If the access is authorized, we label the ERP with the task
user has chosen; otherwise the event is discarded.
During the normal operations, the OS will continuously monitor
neural signals through the BCI device as well as user’s interaction
with the system to create and cache most recent ERPs. ERPs are
bound to the app to which the input event is delivered (e.g., the
most foreground app at that moment) and will expire after a context
switch. This prevents one app from “stealing” another app’s ERP.
Upon an application’s request to access a protected resource (e.g.,
camera), the access control system will retrieve the most recent
ERP. The ERP will then be fed into the trained classier to infer
whether the user intended to perform a task that requires access
to that resource. If so, permission is automatically granted to that
request; if the intended task does not require the permission or
the condence of the classication result is not high enough, IAC
will fall back to prompt the user to make the decision. The ground
truth collected from the prompt window is then to update the ML
model. As demonstrated in previous works [47, 66] and our own
experiment, this feedback is important for ne tuning the ML model
to improve the precision of the prediction.
2 Note that unlike access control gadget, we do not require the intent to be expressed
through certain interactions with the app’s GUI.
Applicable Scenarios. Apparently, using BCI-based access con-
trol for existing systems like desktop and mobile devices is imprac-
tical; users need to wear the device all the time. However, this eld
is advancing fast and companies like Facebook and Neuralink are
laying out projects to decode users’ intents into machine readable
commands to scroll menus, select items, launch applications, and
manipulate objects [13]. BCI has also been used in manufacturing
to control machines [1, 58] or to monitor workers’ mental status in
order to avoid over-stressing [16]. With the rapid progress in neural
imaging and signal processing, in not so distant future, BCI-based
applications can be far beyond gaming and entertainment. Hence,
we believe BCI could become ubiquitous and a practical way to
interact with digital systems and our IAC be easily integrated into
such systems to protect users’ privacy.
4 EXPERIMENT DESIGN
The goal of our experiment is to study the feasibility of inferring
user’s high-level intents through the brain-computer interface (BCI)
and use user’s intents to authorize access to protected resources.
More specically, we want to assess whether the event-related
potentials (ERPs) recorded using a consumer-based EEG headset
could be used to infer three types of high-level common tasks: (1)
taking a photo, (2) taking a video, and (3) pick a photo from library.
The hypothesis to be tested is:
Hypothesis. Visual and mental processing of each unique inten-
tion has distinguishable patterns in event-related potentials that can
be extracted with a supervised machine learning algorithm.
4.1 Single App Experiment
We designed a special Android app (Figure 4) to test our hypothe-
sis. This app consists of three steps. The main activity (Figure 4a)
contains 10 TASK buttons to start 10 sets of tasks. The tasks are
randomized in each set. Before starting each session, participants
will click the START button to begin logging all the click events into
a text le. In each session, participants are asked to go through all
10 sets of tasks. Clicking on each TASK button will lead to the task
option screen (Figure 4b and Figure 4c). Here participants are asked
to perform 4 actions. When an action is nished, participants will
return to the same task option screen and continue to the next task
until all 4 actions are done. Then they move on to the next task
set. When all 10 sets of tasks are completed, participants will click
the STOP button to stop the session and take a break before starting
IAC: On the Feasibility of Utilizing Neural Signals for
Access Control
ACSAC’18, December 3–7,2018, San Juan, PR, USA
(a) Main activity
(b) Task options 1
(c) Task options 2
(d) Task activity
Figure 4: Android app for data collection.
another session. Among these 4 tasks, three involve accessing user-
owned privacy sensitive sensors (camera and microphone) and les
(photo gallery). The order of these four tasks is dierent between
dierent task sets. Details of the 4 tasks are listed below.
• Take Photo (Photo) Clicking this button will send a
MediaStore.ACTION_IMAGE_CAPTURE intent, to start the camera
app. As the name suggests, participants are then asked to take
a photo of a target object (e.g., a pen Figure 4d) with the camera
app. This task requires access to the camera device.
• Take Video. (Video) Similar to taking a photo, clicking this
button will send a MediaStore.ACTION_VIDEO_CAPTURE intent
and invoke the camera app. Participants are then asked to take
a short video of the target object. The dierences from taking
a photo are (1) taking a video will access both the camera and
the microphone device and (2) accessing to both devices are
continuous.
• Choose from Gallery. (Gallery) Clicking this button will
send an Intent.ACTION_GET_CONTENT intent with image/* type.
Participants are then asked to pick the photo of the target object
(e.g., a pen) from the photo gallery of the Android device. To
make sure the photo is always available, we do not use this
task as the rst option of the rst task set. This task requires
access to the privacy-sensitive les.
• Cancel. Cancel is a unique task, it does not perform partic-
ularly interesting operations or access any privacy-sensitive
resources. Its sole purpose is to ask the participants to click a
button on the touchscreen of the phone.
• AE2: The intent to click a specic position of the touch screen
(e.g., a button at a xed position).
• AE3: The reaction of seeing similar pictures.
We added the Cancel task so if AE1 is true, we will not be able
to distinguish the Cancel task from the rest tasks. We randomize
the order of the tasks on the options activity so if AE2 is true,
we will not be able to distinguish between randomized tasks. We
deliberately choose three visually similar tasks so if AE3 is true, we
will not be able to distinguish between these tasks that involve the
same photo.
Table 1: The list of Apps used in testing phase: we test the
performance of the model built on the neural data collected
from the in-house android app in correctly identifying the
intention of the users when they interact with these real
apps.
App Name
Facebook Messenger
Google Hangouts
WhatsApp
Instagram
Camera
VideoCamDirect
QuickVideo
SnapChat
Actions
Photo, Gallery
Photo, Gallery
Photo, Gallery
Photo, Gallery
Photo, Video
Video
Video
Gallery
Alternative Explanations. An important part of this experiment
design is to rule out a few alternative explanations (AE). Specically,
as our experiment involves asking participants to perform a task
using the smartphone, we want to rule out the possibility that
what we captured from the neural signals is not the user’s intent to
perform the given task but
• AE1: The intent to interact with the phone (e.g., click a button).
4.2 Multiple Apps Experiment
For testing the “portability” of the learned model (i.e., the model
can identify the same intent across dierent apps and contexts), we
designed a second experiment with eight popular real-world apps
(Table 1). All of them have more than 500k downloads in the Google
Play Store. We created testing accounts for WhatsApp, Hangouts,
Messenger, Snapchat and Instagram. The other three apps Camera,
ACSAC’18, December 3–7,2018, San Juan, PR, USA
M. Rahman et al.
QuickVideo, and VideoCamDirect did not need any account to take
photos or videos.
We instructed participants to browse these apps as they use it in
their real-life (e.g., they might be taking a photo, or writing texts).
However, in this study, we just focus on the participants’ interaction
events related to the following three tasks: (1) taking a photo, (2)
taking video, and (3) select and upload a photo from the gallery.
This experiment has more realistic and ecologically valid settings as
the participants were browsing these popular apps and performing
the common tasks (e.g., take photo, take video and upload photo)
as per their own choice.
4.3 Experimental Procedures
Ethical and Safety Considerations. Our study involved human
subjects, and our experimental and recruiting procedures were