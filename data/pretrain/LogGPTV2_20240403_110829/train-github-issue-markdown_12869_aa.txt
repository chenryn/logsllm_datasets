### System information
  * **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)** : YES
  * **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)** : CentOS 7.2 (and also Windows 10)
  * **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device** : N/A
  * **TensorFlow installed from (source or binary)** : binary (via `pip install tensorflow-gpu`)
  * **TensorFlow version (use command below)** : v1.11.0-0-gc19e29306c 1.11.0
  * **Python version** : 3.6.6
  * **Bazel version (if compiling from source)** : N/A
  * **GCC/Compiler version (if compiling from source)** : N/A
  * **CUDA/cuDNN version** : 9.0/7.1.4
  * **GPU model and memory** : NVIDIA GeForce GTX 1070 (8 GiB)
  * **Exact command to reproduce** : `python nmsstest.py`; see below for the content of `nmstest.py`
### Describe the problem
The code in the following section, which calls
`tf.image.non_max_suppression()` in `tf.while_loop()` many times, crashes
abnormally.  
Crash reason (and loop count) varies from time to time, for example:
  * `F tensorflow/core/common_runtime/bfc_allocator.cc:384] Check failed: h != kInvalidChunkHandle` at loop `i == 140`
  * `F tensorflow/core/common_runtime/bfc_allocator.cc:462] Check failed: c->in_use() && (c->bin_num == kInvalidBinNum)` at loop `i == 76`
  * `Bus error` at loop `i == 34`
  * heap corruption reported by libc (see the following section) at loop `i == 35`
  * sometimes it crashes sliently without any logs (on Windows)
I notice that:
  * it's since TensorFlow 1.11.0rc0; TF 1.10.1 was okay
  * it also reproduces on Windows
  * it also reproduces on CPU version (`pip install tensorflow`)
  * it does **not** reproduce if `num_threads=1`; calling `tf.image.non_max_suppression()` in parallel seems the trigger
  * even when I gave a fixed seed to `tf.random_uniform()`, crash cause varied
### Source code / logs
The code to reproduce the problem is as follows:
    import tensorflow as tf
    if __name__ == '__main__':
        # crashes iff num_threads > 1 on TensorFlow >= 1.11.rc0
        num_threads = 10
        top_k = 1
        batch_size = 32
        num_boxes = 10000
        boxes_op = tf.random_uniform((batch_size,num_boxes,4), 0, 1)
        scores_op = tf.random_uniform((batch_size,num_boxes), 0, 1)
        indices_op = tf.while_loop(
            (lambda b, ta: True),
            (lambda b, ta: (b+1, ta.write(b, tf.image.non_max_suppression(boxes_op[b], scores_op[b], top_k, iou_threshold=0.3)))),
            (tf.constant(0),
             tf.TensorArray(tf.int32, size=batch_size, infer_shape=False, element_shape=(top_k,))),
            back_prop=False,
            parallel_iterations=num_threads,
            maximum_iterations=batch_size)[1].stack()
        with tf.Session() as session:
            for i in range(1000):
                indices = session.run(indices_op)
                print(f'#{i}: {indices.shape}')
    $ python nmstest.py
    2018-09-28 13:18:42.760545: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
    2018-09-28 13:18:43.123328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
    2018-09-28 13:18:43.124208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
    name: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.683
    pciBusID: 0000:01:00.0
    totalMemory: 7.93GiB freeMemory: 7.83GiB
    2018-09-28 13:18:43.124232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
    2018-09-28 13:18:43.351813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
    2018-09-28 13:18:43.351847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
    2018-09-28 13:18:43.351859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
    2018-09-28 13:18:43.352078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7558 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)
    #0: (32, 1)
    #1: (32, 1)
    #2: (32, 1)
    #3: (32, 1)
    #4: (32, 1)
    #5: (32, 1)
    #6: (32, 1)
    #7: (32, 1)
    #8: (32, 1)
    #9: (32, 1)
    #10: (32, 1)
    #11: (32, 1)
    #12: (32, 1)
    #13: (32, 1)
    #14: (32, 1)
    #15: (32, 1)
    #16: (32, 1)
    #17: (32, 1)
    #18: (32, 1)
    #19: (32, 1)
    #20: (32, 1)
    #21: (32, 1)
    #22: (32, 1)
    #23: (32, 1)
    #24: (32, 1)
    #25: (32, 1)
    #26: (32, 1)
    #27: (32, 1)
    #28: (32, 1)
    #29: (32, 1)
    #30: (32, 1)
    #31: (32, 1)
    #32: (32, 1)
    #33: (32, 1)
    #34: (32, 1)
    #35: (32, 1)
    *** Error in `python': malloc(): smallbin double linked list corrupted: 0x00007fb6c001c960 ***
    ======= Backtrace: =========
    /lib64/libc.so.6(+0x7f5e4)[0x7fb9816b75e4]
    /lib64/libc.so.6(+0x82d00)[0x7fb9816bad00]
    /lib64/libc.so.6(__libc_malloc+0x4c)[0x7fb9816bd84c]
    /home/hyabe/anaconda3/envs/work/bin/../lib/libstdc++.so.6(_Znwm+0x16)[0x7fb936087084]
    /home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow15OpKernelContext15allocate_outputEiRKNS_11TensorShapeEPPNS_6TensorENS_19AllocatorAttributesE+0x48)[0x7fb93f0c0608]
    /home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow15OpKernelContext15allocate_outputEiRKNS_11TensorShapeEPPNS_6TensorE+0xc5)[0x7fb93f0c0785]
    /home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow7MergeOp7ComputeEPNS_15OpKernelContextE+0xa4)[0x7fb9423854b4]
    /home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow13BaseGPUDevice13ComputeHelperEPNS_8OpKernelEPNS_15OpKernelContextE+0x37d)[0x7fb93f23ac9d]
    /home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow13BaseGPUDevice7ComputeEPNS_8OpKernelEPNS_15OpKernelContextE+0x8d)[0x7fb93f23b1dd]
    /home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(+0x63a4bc)[0x7fb93f2844bc]
    /home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(+0x63ae2a)[0x7fb93f284e2a]
    /home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN5Eigen26NonBlockingThreadPoolTemplIN10tensorflow6thread16EigenEnvironmentEE10WorkerLoopEi+0x21a)[0x7fb93f2f296a]
    /home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZNSt17_Function_handlerIFvvEZN10tensorflow6thread16EigenEnvironment12CreateThreadESt8functionIS0_EEUlvE_E9_M_invokeERKSt9_Any_data+0x32)[0x7fb93f2f1a12]
    /home/hyabe/anaconda3/envs/work/bin/../lib/libstdc++.so.6(+0xb8678)[0x7fb9360a2678]
    /lib64/libpthread.so.0(+0x7e25)[0x7fb981a0ce25]
    /lib64/libc.so.6(clone+0x6d)[0x7fb981736bad]
    ======= Memory map: ========
    200000000-200200000 rw-s 00000000 00:05 25687                            /dev/nvidiactl
    200200000-200400000 ---p 00000000 00:00 0 
    200400000-200404000 rw-s 00000000 00:05 25687                            /dev/nvidiactl
    200404000-200600000 ---p 00000000 00:00 0 
    200600000-200a00000 rw-s 00000000 00:05 25687                            /dev/nvidiactl
    200a00000-201200000 ---p 00000000 00:00 0 
    201200000-201204000 rw-s 00000000 00:05 25687                            /dev/nvidiactl
    201204000-201400000 ---p 00000000 00:00 0 
    201400000-201800000 rw-s 00000000 00:05 25687                            /dev/nvidiactl
    201800000-201804000 rw-s 00000000 00:05 25687                            /dev/nvidiactl
    201804000-201a00000 ---p 00000000 00:00 0 
    201a00000-201e00000 rw-s 00000000 00:05 25687                            /dev/nvidiactl
    201e00000-201e04000 rw-s 00000000 00:05 25687                            /dev/nvidiactl
    201e04000-202000000 ---p 00000000 00:00 0 
    202000000-202400000 rw-s 00000000 00:05 25687                            /dev/nvidiactl