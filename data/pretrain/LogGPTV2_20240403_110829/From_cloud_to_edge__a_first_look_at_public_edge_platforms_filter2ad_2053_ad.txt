and the in-game action appearing on the client. We refer to this
metric as response delay. To obtain this delay, we periodically send
touch events to the game menu and wait for the menu window to
pop up. In the meantime, we use another device to record the device
screen at a high frequency (FPS). At offline, we use FFmpeg [16]
to decode the videos and manually obtain the interval between a
touch event being invoked and the menu popping up. The response
delay is thus obtained as the interval between the two timestamps.
The touch event is set to be visible through Android built-in tools.
43
(a) Under different network conditions.
(b) With different devices.
(c) Using different games.
Figure 6: The cloud gaming performance under different set-
tings. Default setting: Samsung Note 10+, Game Flare, and
WiFi. “I-”: indoor, “O-”: outdoor.
The online experiment is performed automatically through Android
Monkey tool [23]. For each testing, we obtain 50 data points.
Overall results Figure 6 shows the cloud gaming performance
with different network conditions (a), client devices (b), and game
types (c). Overall, with a good network condition (WiFi in our case)
and nearby VMs (Edge and Cloud-1), cloud gaming can achieve
less than 100ms response delay. The distance matters: remote cloud
VMs lengthen the delay by up to 60ms. Among the devices, Sam-
sung Note 10+ performs slightly better than others for its high-
end chipset, but the improvement is not significant because the
hardware-accelerated video decoding is fast enough for all the de-
vices tested, i.e., less than 10ms for the default 800×600 resolution
used in GamingAnywhere, and the screen fresh rate is the same
(60Hz). Note that decoding is needed as the gaming server encodes
the game scenes to video frames before sending to UE. Among
different games, Pingus experiences slightly higher delay and jitter
for its more complex game logic.
Breakdown An intuitive question is how we can further im-
prove the 100ms response delay, e.g., within 30ms or 2-3 frames.
Our breakdown results show that, the network delay of the nearest
edge VM is no longer the bottleneck: the propagation delay is only
11.4ms shown Table 5 and the transmission delay (to send a frame)
is less than 10ms according to our measurement. Instead, we find
the major portion is on the server-side (game logic execution and
rendering), which contributes to around 70ms delay. We further
look into this delay and make two interesting observations: (1)
While each server VM is equipped with 8 CPU cores, most cores ex-
cept one run at very low utilization (less than 10%). In other words,
increasing CPU cores won’t help as it is difficult to parallelize the
game logic. (2) Our offline experiment that hosts the server on a
Macbook laptop suggests that enabling the GPU rendering can help
reduce the delay by around 10ms–20ms.
Implications Placing gaming backend on nearby NEP edges can
help achieve less than 100ms response delay. To further enhance the
experience, we need to improve the server-side gaming execution.
IMC ’21, November 2–4, 2021, Virtual Event, USA
Mengwei Xu et al.
Breakdown Even without using jitter buffer and transcoding,
the streaming delay remains about 400ms. Following the approach
in [27], we break down this delay to image capture, local frame
processing (frame patch splice, codec, and video rendering), and
RTMP network transmission delay. We draw the following findings.
(1) Network delay takes around 50ms, which doesn’t constitute
the major bottleneck. Note that edge reduces only the propagation
delay but not transmission delay. The observation is confirmed
by another micro experiment, where we repeat the above experi-
ments but deploying the server on a laptop wired to sender/receiver
UEs (LAN environment). In such settings, the streaming delay is
only reduced by 40ms. (2) By calculating the timestamp difference
between the running clock and the sender UE’s screen, we esti-
mate the image capture plus rendering to be around 140ms. This
delay is sophisticated, including the digital processing by camera
image signal processor and the time spent on the system-software
stack (Android in our case). (3) The encoding/decoding delays are
25ms/10ms on the sender/receiver UEs, respectively. (4) The soft-
ware matters: when using FFplay [17] instead of MPlayer [19] on
the receiver UE to pull and display the video stream, the streaming
delay reduces almost by 90ms.
Implications While NEP brings modest delay reduction to live
streaming scenarios by reducing the network delay, the delay spent on
image capture, transcoding, jitter buffer, and even the system-software
stack remain the bottleneck to achieve real-time human interaction.
Thus, it is imperative to improve the hardware capacities and the
system software design in order to support the niche edge applications.
Even so, live streaming has become a major application type hosted
on NEP for its reduced billing cost as we will show in §4.5.
4 DEMYSTIFYING EDGE WORKLOADS
In this section, we characterize the workloads based on our collected
VM traces (§2.1.2) and the public Azure Dataset collected from
Azure’s entire VM sets [38] (2019 version).
4.1 Applications and VM Subscription
Application type We investigate the major customers of NEP.
We classify them into different categories, and the most popular
ones are: video live streaming, online education, content delivery,
video/audio communication, video surveillance, and cloud gam-
ing. Most of them have two common characteristics: (1) Network-
intensive: they stream a lot of data, mostly videos. §4.5 will show
that edge platforms like NEP are more budget-friendly to the appli-
cations with high bandwidth usage. (2) Delay-critical: user interac-
tion is often involved, either unidirectional or bidirectional. This is
because edge services can provide lower and more stable network
performance. In fact, those two factors are the major incentives to
decentralize cloud applications to edges.
VM size We compare the VM sizes, i.e., the amount of resources
allocated to each VM on NEP and Azure Cloud. Note that NEP pro-
vides very similar VM configuration options in terms of CPU and
memory to customers as Azure Cloud does. Illustrated in Figure 8,
our key observation is that NEP VMs typically request more re-
sources than Azure VMs. Overall, the median number of CPU cores
and memory requested on NEP and Azure are (8 vs. 1) and (32GBs
vs. 4GBs). In addition, Azure Cloud has many “low-end” VMs with
Figure 7: The live streaming performance on edge/cloud un-
der different experiment conditions.
More specifically, adapting the gaming to multi-core systems with
high parallelism and applying hardware acceleration (e.g., GPU) are
promising approaches.
3.3.2 Live Streaming. A report [5] in 2018 shows that 42% of the
population in the U.S. have now live-streamed online content. Live
streaming also demands low end-to-end delay, especially for bidi-
rectional streaming scenarios that involve human-to-human inter-
action, e.g., online meeting. Our workload analysis in §4.1 shows
that live streaming is one major application type served by NEP. In
this experiment, we assume the sender and receiver are located in
the same city, which is common for many streaming scenarios such
as online education. Unless otherwise specified, we stream 1080p
video without transcoding, and the encoded streaming bitrate is
around 5Mbps.
Metrics Following prior work [97], we define streaming delay
as the amount of time between a real-world event and the display
of that event on the receiver’s screen. To obtain this delay, we use
the sender UE to capture a millisecond-level clock (displayed by a
third device), and use a fourth device to capture the running clock
and the receiver UE’s screen simultaneously. We then inspect the
difference between the two clocks as the streaming delay. Each
testing runs for 20 seconds from which we obtain 50 data points.
Overall results Figure 7 shows the live streaming performance
under different conditions. Here, “WiFi-trans” indicates transcoding
videos from 720p to 1080p on server, while others simply stream
videos without transcoding. We draw the following important ob-
servations from this figure. (1) Edge servers have limited benefit
in reducing the streaming delay, e.g., upmost 24% compared to the
farthest cloud under 5G, mostly because the network doesn’t consti-
tute the bottleneck as we will show next. (2) Streaming images with
a lower resolution can reduce the delay around 67ms (26%) from
1080p to 720p. Note that this reduction not only comes from the
reduced network transmission time, but also the rendering on the
receiver UE. (3) Transcoding incurs a high overhead: around 400ms
(2×) from 1080p to 720p under our WiFi condition. This overhead
includes both the transcoding time and server waiting time for a
video segment to arrive.
All above experiments are carried out without using a jitter buffer
on the receiver side. A jitter buffer is commonly used in video
streaming to compensate transmission impairments caused by the
time-variant packet delays [54, 60]. Our additional experiments
show that, with a small jitter buffer (e.g., 2MBs), the streaming
delay reaches as high as 2 seconds and the difference between
edge/clouds becomes trivial.
44
From Cloud to Edge: A First Look at Public Edge Platforms
IMC ’21, November 2–4, 2021, Virtual Event, USA
(a) # of vCPUs per VM
(b) Memory size per VM
Figure 8: NEP VMs are larger than Azure. “small/medi-
an/large”: ≤4 / 5–16 / >16 CPU core or GBs memory.
only a few CPU cores (90% VMs with ≤4 vCPUs) and relatively little
memory (70% VMs with ≤4 GBs), while NEP’s half VMs have more
than 8 CPU cores and 16GBs memory. The median/mean storage
size of NEP VMs is 100/650 GBs (not compared as Azure dataset
doesn’t contain storage information).
The possible reason for NEP VMs subscribing more hardware
resources is that NEP’s current customers are mostly business-
oriented, who need to deploy commercial services or apps that are
likely to be delay-critical, while Azure also serves individuals (e.g.,
researchers, educators) who only need very few resources per VM
to complete their jobs.
Implications Large VM size on NEP-like edge platforms may
cause severe resource fragmentation, i.e., the bin-packing problem [39,
67], hindering a high sale ratio for each server as we will show next.
To mitigate such fragmentation, techniques like dynamic VM migra-
tion [70] and resource disaggregation [87] may help.
VM numbers per app
As shown in Figure 9,
customers tend to deploy
slightly more VMs on NEP
than Azure. For instance,
more than 9.6% apps on
NEP deploy at least 50
VMs, while on Azure only
6.1% of apps deploy that
many. The largest edge
app is a CDN application
which comprises of almost
1,000 VMs. There exist two
possible reasons: (1) Apps
deployed on NEP are more
likely to be delay-sensitive than Azure, requiring a larger number
of geo-distributed VMs to guarantee low delay and high reliability.
(2) As aforementioned, Azure serves more small-scale businesses
or individuals who only need very few VMs to operate.
Figure 9: Per-app VM num.
Implications Compared to clouds, managing and scheduling a
large number of geo-distributed edge VMs is more challenging. First,
traditional tools like Kubernetes [18] may not suffice in maintaining
such VM orchestration [4]. It motivates us to improve or re-architect
those tools. Second, edge customers also need more effort to schedule
end-user traffic to the optimal VM in a fine-grained way. §4.3 will
show that current edge customers often fail to make a good scheduling
decision.
Servers/sites sales rate We also summarize the resource sales
rate on NEP (figure not shown), defined as the percentage of CPU/mem-
ory resources sold out to customers per site or server. We have two
key observations: (1) The sales rate is highly skewed across servers
(a) Mean/Max CPU util.
(b) CPU util. variance
Figure 10: CPU utilization on NEP is lower but more variant
than Azure. (a): the overall CPU utilization; (b): CPU utiliza-
tion variance across time (CV).
and sites. For example, the 95th-percentile CPU sales rate across
sites is about 5× higher than the 5th-percentile. Such skewness
stems from that, in edge computing, server resource demand highly
depends on the geolocations. (2) Compared to memory, CPU cores
are more likely to be saturated: the median sales rate of CPU is
almost 2× of the memory.
Implications The skewed sales rate across sites can guide NEP
providers to locate the regions with higher business returns for future
investment. As edge apps increasing, it’s also important to invest
more in those “hot spots” to ensure good availability and elasticity of
computation resources.
4.2 Overall Resource Usage
Overall CPU usage Figure 10(a) illustrates the per-VM CPU usage
on edge/cloud as the Cumulative Distribution Function (CDF) of the
average CPU utilization, and the CDF of the 95th-percentile of the
maximum CPU utilization (P95 Max). The key observation is that,
either by mean or P95 Max, VM CPUs on NEP are much less utilized
than Azure Cloud. For example, 74% VMs on NEP have less than 10%
CPU utilization on average, while on Azure Cloud only 47% VMs
have less than 10% utilization. It indicates that, either unconsciously
or purposely, edge customers tend to over-provision the hardware
resources for VMs, which also echoes our analysis in §4.1 that edge
VMs often subscribe more resources than on cloud. Diving deeper,
such resource over-provision may be due to two reasons: (1) Edge
apps are more likely to be delay-critical, so that more resources are
needed to deliver a good quality of service. (2) It is difficult for edge
customers to understand the resource demand at different locations
due to the high density of edge server deployment and the temporal
dynamics of user requests as will be discussed below. There, they
tend to be conservative when provisioning them.
CPU usage variance across time We also investigate the across-
time resource usage variance of edge/cloud VMs, as indicated by
the coefficient of variation (CV = std/mean) of CPU usage. As il-
lustrated in Figure 10(b), edge VMs exhibit more usage variance
across time than cloud (median: 0.48 vs. 0.24). The reason is that
apps deployed on edge platforms are more likely to be interactive
(§4.1) so that the usage highly depends on human activities.
Implications The relatively low but highly skewed CPU usage
challenges the NEP’s VM management. To better utilize the CPU
resources, NEP may borrow existing techniques from cloud computing
research, e.g., smart VM placement algorithms based on VM resource
usage prediction [35, 46, 65]. An alternative approach is to employ
45
IMC ’21, November 2–4, 2021, Virtual Event, USA
Mengwei Xu et al.
(cid:2)(cid:4)(cid:1)(cid:3)(cid:8) (cid:5)(cid:6)(cid:7)
(cid:2)(cid:4)(cid:1)(cid:3)(cid:8) (cid:5)(cid:6)(cid:7)
(a) Cross-server CPU usage
(b) Cross-site CPU usage
(cid:3)(cid:1)(cid:2)(cid:7) (cid:4)(cid:5)(cid:6)
(cid:3)(cid:2)(cid:1)(cid:7) (cid:4)(cid:5)(cid:6)
(c) Cross-server NET usage
(d) Cross-site NET usage
Figure 11: The resource usage across machines/sites is
highly unbalanced. All sites are randomly sampled from
Guangdong Province, and the machines are from a random
site. For (a)/(b): a machine’s CPU usage is calculated as the
weighted (by requested cores) CPU usage of all its hosted
VMs, and a site’s CPU usage is averaged across all its ma-
chines. For (c)/(d): the bandwidth usage of a machine/site is
summed across all the VMs hosted in the machine/site. For
each figure: all numbers are normalized to the smallest one.
(a) CPU usage gap between the
same app’s VMs.
(b) CPU usage of 11 VMs from
the same edge app.
Figure 12: The CPU usage of the same app’s VMs is highly
unbalanced. In (a), the usage gap of each app is measured
as the 95th-percentile divided by the 5th-percentile of the
mean CPU usage of all its VMs.
more elastic computing forms, e.g., containers, together with IaaS
VMs on the same server. §6 will further discuss the opportunities and
challenges.
4.3 Resource Load Balance
Resource usage balance (or load balance) is critical to the application
QoE on multi-tenant cloud platforms [44, 77, 83]. Such importance
is further amplified on edge platform that hosts delay-critical apps