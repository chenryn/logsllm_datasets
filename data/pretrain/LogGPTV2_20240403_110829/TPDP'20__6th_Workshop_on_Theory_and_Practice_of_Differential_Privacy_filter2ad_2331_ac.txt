That is, whatever an adversary could have learned about an individual, it could have learned from
the rest of the dataset.
Proof. By Bayes’ Rule,
Pr[X = x|M(X) = y] =
Pr[M(X) = y|X = x] · Pr[X = x]
Pr[M(X) = y]
≤ eε · Pr[M(X(cid:48)) = y|X = x] · Pr[X = x]
e−ε · Pr[M(X(cid:48)) = y]
= e2ε · Pr[X = x|M(X(cid:48)) = y].
3Supp(Z) is deﬁned to be the support of random variable Z, i.e. {z : Pr[Z = z] > 0}.
10
By symmetry (swapping X and X(cid:48)), we also have Pr[X = x|M(X(cid:48)) = y] ≤ e2ε · Pr[X = x|M(X) =
y]. Having all probability masses equal up to a multiplicative factor of e2ε implies that the statistical
distance is at most 1 − e−2ε ≤ 2ε.
There is also a converse to the proposition: if M guarantees that the two posterior distributions
are close to each other (even in statistical diﬀerence), then M must be diﬀerentially private. In
fact, this will hold even for the special case mentioned above where X(cid:48) is constant.
Proposition 1.7 (Bayesian privacy implies DP). Let M : Xn → Y be any randomized mechanism,
and let x0 ∼ x1 ∈ Xn be two neighboring datasets. Deﬁne the joint distribution (X, X(cid:48)) to equal
(x0, x0) with probability 1/2 and to equal (x1, x0) with probability 1/2. Suppose that for some
y ∈ Supp(M(x0) ∩ Supp(M(x1)),
SD(X|M(X)=y, X|M(X(cid:48))=y) ≤ ε ≤ 1/4.
(2)
Then
e−O(ε) · Pr[M(x1) = y] ≤ Pr[M(x0) = y] ≤ eO(ε) · Pr[M(x1) = y].
In particular, if for all pairs x0 ∼ x1 of neighboring datasets, we have that Supp(M(x0)) =
Supp(M(x1)) and (2) holds for all outputs y ∈ Supp(M(x0)), then M is O(ε)-diﬀerentially pri-
vate.
Note that for the joint distributions (X, X(cid:48)) in Proposition 1.7, we have Pr[X ∼ X(cid:48)] = 1, so
this is indeed a converse to Proposition 1.7.
Proof. Since X(cid:48) is constant, X|M(X(cid:48))=y is the same as the prior X (namely, uniformly random from
{x0, x1}). Thus, by hypothesis, for b = 0, 1, we have:
1
2
− ε ≤ Pr[X = xb|M(X) = y] ≤ 1
2
+ ε.
On the other hand, by Bayes’ Rule,
Pr[M(xb) = y] = Pr[M(X) = y|X = xb]
Pr[X = xb|M(X) = y] · Pr[M(X) = y]
=
∈
(cid:20) (1/2) − ε
1/2
Pr[X = xb]
· Pr[M(X) = y],
(1/2) + ε
1/2
· Pr[M(X) = y]
(cid:21)
.
Thus, Pr[M(x0) = y]/ Pr[M(x1) = y] is between (1/2 − ε)/(1/2 + ε) = e−O(ε) and (1/2 + ε)/(1/2 −
ε) = eO(ε).
There are also (ε, δ) analogues of the above propositions, where we require that all but negligible
probability (related to δ), the posterior probability distributions should be close to each other [64].
11
Interpretations of the Deﬁnition. We can now provide some more intuitive interpretations of
(and cautions about) the deﬁnition of diﬀerential privacy:
• Whatever an adversary learns about you, she could have learned from the rest of the dataset
(in particular, even if you didn’t participate). Note that this does not say that the adversary
does not learn anything about you; indeed, learning about the population implies learning
about individuals. For example, if an adversary learns that smoking correlates with lung
cancer (the kind of fact that diﬀerential privacy is meant to allow learning) and knows that
you smoke, it can deduce that you are more likely to get lung cancer. However, such a
deduction is not because of the use of your data in the diﬀerentially private mechanism, and
thus may not be considered a privacy violation.
• The mechanism will not leak a signiﬁcant amount of information speciﬁc to an individual
(or a small group, as we’ll see in the next section). Consequently, diﬀerential privacy is not
an achievable privacy notion if the goal of the analysis is to take an action on a speciﬁc
individual in the dataset (e.g. to identify a candidate for a drug trial, or a potential terrorist,
or a promising customer).
The above interpretations hold regardless of what auxiliary information or computational strategy
the adversary uses. Indeed, the deﬁnition provides an information-theoretic form of security. In
Section 10, we will consider a computational analogue of diﬀerential privacy, where we restrict to
polynomial-time adversaries.
Variants of the deﬁnition and notation.
x ∈ Xn, where n is known and public (not sensitive information).
In our treatment, the dataset is an ordered n-tuple
A common alternative treatment is to consider datasets x that are multisets of elements of X,
without a necessarily known or public size. Then, a convenient notation is to represent x as a
histogram — that is, as an element of NX. In the multiset deﬁnition, the distance between two
datasets is symmetric diﬀerence |x∆x(cid:48)|, which corresponds to (cid:96)1 distance in histogram notation.
Thus, neighboring datasets (at distance 1) are ones that diﬀer by addition or removal of one item.
Diﬀerential privacy under this deﬁnition has a nice interpretation as hiding whether you participated
in a dataset at all (without having to replace you by an alternate row to keep the dataset size the
same).
There is not a big diﬀerence between the two notions, as one can estimate n = |x| with diﬀeren-
tial privacy (it is just a counting query), the distance between two unordered datasets of the same
size under addition/removal vs. substitution diﬀer by at most a factor of 2, and one can apply a
diﬀerentially private mechanism designed for ordered tuples to an unordered dataset by randomly
ordering the elements of the dataset.
1.7 Preview of the Later Sections
The primary goal of this tutorial is to illustrate connections of diﬀerential privacy to computational
complexity and cryptography. Consequently, our treatment of the algorithmic foundations of dif-
ferentially private is very incomplete, and we recommend the monograph of Dwork and Roth [35]
for a thorough treatment, including more proofs and examples for the background material that is
only sketched here. We also focus heavily on counting queries in this tutorial, because they suﬃce
to bring out most of the connections we wish to illustrate. However, the algorithmic literature
12
on diﬀerential privacy now covers a vast range of data analysis tasks, and obtaining a thorough
complexity-theoretic understanding of such tasks is an important direction for future work.
The topics that will be covered in the later sections are as follows:
Section 2: We will describe composition theorems that allow us to reason about the level of
diﬀerential privacy provided when many diﬀerentially private algorithms are executed inde-
pendently. In particular, this will give us algorithms to answer nearly n2 counting queries
accurately while satisfying diﬀerential privacy.
Section 3: We will brieﬂy survey some alternatives to using global sensitivity to calibrate the level
of noise added for diﬀerentially private estimates; sometimes we can get away with adding
noise that is proportional to the sensitivity of the query in a local neighborhood of our dataset
x (but we need to be careful in doing so).
Section 4: We will present some remarkable algorithms that can answer many more than n2
counting queries with diﬀerential privacy. These algorithms are inspired by ideas from com-
putational learning theory, such as Occam’s razor and the Multiplicative Weights Method.
Unfortunately, these algorithms are computationally quite expensive, requiring time that is
polynomial in the size of the data universe X (which in turn is exponential in the bit-length
of row elements).
Section 5: We will prove a number of information-theoretic lower bounds on diﬀerential privacy,
showing that it is impossible to answer too many queries with too much accuracy. Some of
the lower bounds will be based on combinatorial and geometric ideas (such as “discrepancy”),
and others will be on ﬁngerprinting codes, which were developed as a tool in cryptography
(for secure digital content distribution).
Section 6: We will turn to computational hardness results for diﬀerential privacy, giving evidence
that there is no way in general to make the algorithms of Section 4 computationally eﬃcient.
These hardness results will be based on cryptographic constructs (such as traitor-tracing
schemes and digital signatures), and one result will also use probabilistically checkable proofs.
Section 7: Next, we will turn to some additional algorithms that bypass the hardness results
of Section 6 by focusing on speciﬁc, structured families of counting queries (and use alter-
native output representations). The methods employed include low-degree approximations
of boolean functions (via Chebychev polynomials) and convex geometry and optimization
(semideﬁnite programming, Gaussian width, Grothendieck’s Inequality).
Section 8: We will then look at PAC learning with diﬀerential privacy, showing both some very
general but computationally ineﬃcient positive results, as well as some eﬃcient algorithms.
We will then see how methods from communication complexity have been used to show that
the sample complexity of diﬀerentially private PAC learning (with pure diﬀerential privacy)
is inherently higher than that of non-private PAC learning.
Section 9: In this section, we will explore generalizations of diﬀerential privacy to the case where
the data is distributed among multiple parties, rather than all being held by a single trusted
curator. We will show, using connections to randomness extractors and to information com-
plexity, that sometimes distributed diﬀerential privacy cannot achieve the same level of ac-
curacy attained in the centralized model.
13
Section 10: The aforementioned limitations of multiparty diﬀerential privacy can be avoided by
using cryptography (namely, secure multiparty computation) to implement the trusted cura-
tor. However, this requires a relaxation of diﬀerential privacy to computationally bounded
adversaries. We will present the deﬁnition of computational diﬀerential privacy, and point out
its connection to the notion of “pseudodensity” studied in the theory of pseudorandomness.
2 Composition Theorems for Diﬀerential Privacy
2.1 Post-processing and Group Privacy
One central property of diﬀerential privacy, which we will use throughout the tutorial, is that it is
preserved under “post-processing”:
Lemma 2.1 (post-processing). If M : Xn → Y is (ε, δ)-diﬀerentially private and F : Y → Z is any
randomized function, then F ◦ M : Xn → Z is (ε, δ)-diﬀerentially private.
Proof. Consider F to be a distribution on deterministic functions f : Y → Z. Then for every
x ∼ x(cid:48) ∈ Xn, and every subset T ⊆ Z, we have
Pr[(F ◦ M)(x) ∈ T ] = E
f←F
≤ E
f←F
= eε · Pr[(F ◦ M)(x(cid:48)) ∈ T ] + δ.
[Pr[M(x) ∈ f−1(T )]]
[eε · Pr[M(x(cid:48)) ∈ f−1(T )] + δ]
Another useful property, alluded to in Section 1.6, is that diﬀerential privacy provides protec-
tions for small groups of individuals. For x, x(cid:48) ∈ Xn, let d(x, x(cid:48)) denote the Hamming distance
between x and x(cid:48), or in other words the number of rows that need to be changed to go from x to
x(cid:48) (so x ∼ x(cid:48) iﬀ d(x, x(cid:48)) ≤ 1). We say that two random variables Y and Y (cid:48) taking values in Y are
(ε, δ)-indistinguishable if for every event T ⊆ Y, we have:
Pr[Y ∈ T ] ≤ eε · Pr[Y (cid:48) ∈ T ] + δ, and
Pr[Y (cid:48) ∈ T ] ≤ eε · Pr[Y ∈ T ] + δ.
Then the “group privacy” lemma for diﬀerential privacy is as follows.
Lemma 2.2 (group privacy). If M is an (ε, δ)-diﬀerentially private mechanism, then for all pairs
of datasets x, x(cid:48) ∈ Xn, M(x) and M(x(cid:48)) are (kε, k · ekε · δ)-indistinguishable for k = d(x, x(cid:48)).
Proof. We use a hybrid argument. Let x0, x1, x2, . . . , xk be such that x0 = x and xk = x(cid:48) and for
each i such that 0 ≤ i ≤ k − 1, xi+1 is obtained from xi by changing one row. Then, for all T ⊆ Y,
since M is (ε, δ)-diﬀerentially private,
Pr[M(x0) ∈ T ] ≤ eε Pr[M(x1) ∈ T ] + δ
≤ eε (eε Pr[M(x2) ∈ T ] + δ) + δ
...
≤ ekε · Pr[M(xk) ∈ T ] + (1 + eε + e2ε + ··· + e(k−1)·ε) · δ
≤ ekε · Pr[M(xk) ∈ T ] + k · ekε · δ.
14
Note that when δ = 0, ε-diﬀerential privacy provides nontrivial guarantees for datasets x, x(cid:48)
even at distance n; namely (nε, 0)-indistinguishability, which in particular implies that M(x) and
M(x(cid:48)) have the same support.
In contrast, when δ > 0, we only get nontrivial guarantees for
datasets at distance k ≤ ln(1/δ)/ε; when k is larger, k · ekε · δ is larger than 1. This gap is a source
of the additional power of (ε, δ)-diﬀerential privacy (as we will see).
2.2 Answering Many Queries
Now we consider a diﬀerent form of composition, where we independently execute several diﬀeren-
tially private mechanisms. Let M1, M2, . . . , Mk be diﬀerentially private mechanisms. Let
M(x) = (M1(x), M2(x), . . . , Mk(x)),
where each Mi is run with independent coin tosses. For example, this is how we might obtain a
mechanism answering a k-tuple of queries.
The Basic Composition Lemma says that the privacy degrades at most linearly with the number
of mechanisms executed.
Lemma 2.3 (basic composition). If M1, . . . , Mk are each (ε, δ)-diﬀerentially private, then M is
(kε, kδ)-diﬀerentially private.
However, if we are willing to tolerate an increase in the δ term, the privacy parameter ε only
needs to degrade proportionally to
k  0, M is(cid:0)O((cid:112)k log(1/δ(cid:48))) · ε, kδ + δ(cid:48)(cid:1)-diﬀerentially private.
Lemma 2.4 (advanced composition [42]). If M1, . . . , Mk are each (ε, δ)-diﬀerentially private and
√
k:
We now prove the above lemmas, starting with basic composition.
Proof of Lemma 2.3. We start with the case δ = 0. Fix datasets x, x(cid:48) such that x ∼ x(cid:48). For an
output y ∈ Y, deﬁne the privacy loss to be
(cid:18) Pr[M(x) = y]
Pr[M(x(cid:48)) = y]
(cid:19)
Lx→x(cid:48)
M (y) = ln
= −Lx(cid:48)→x
M (y).
M (y) is positive, the output y is “evidence” that the dataset is x rather than x(cid:48); and
Notice that ε∗-diﬀerential privacy of M is equivalent to the statement that for all x ∼ x(cid:48) and
When Lx→x(cid:48)
conversely when it is negative.
all y ∈ Supp(M(x)) ∪ Supp(M(x(cid:48))),
Now, for M = (M1, M2, . . . , Mk) and y = (y1, y2, . . . , yk), we have
Lx→x(cid:48)
M (y) = ln
|Lx→x(cid:48)
M (y)| ≤ ε∗.
Pr[M1(x(cid:48)) = y1 ∧ M2(x(cid:48)) = y2 ∧ ··· ∧ Mk(x(cid:48)) = yk]
(cid:18) Pr[M1(x) = y1 ∧ M2(x) = y2 ∧ ··· ∧ Mk(x) = yk]
(cid:32)(cid:81)k
(cid:81)k
k(cid:88)
i=1 Pr[Mi(x) = yi]
i=1 Pr[Mi(x(cid:48)) = yi]
(cid:33)
(cid:19)
= ln
=
Lx→x(cid:48)
Mi
(yi),