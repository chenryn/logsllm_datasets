manager knows which parts of which files are in the cache, it can
return the address of cached data to satisfy an I/O request without
having to call the file system.
Details of how intelligent read-ahead and fast I/O work are provided later
in this chapter in the “Fast I/O” and “Read-ahead and write-behind” sections.
Stream-based caching
The cache manager is also designed to do stream caching rather than file
caching. A stream is a sequence of bytes within a file. Some file systems,
such as NTFS, allow a file to contain more than one stream; the cache
manager accommodates such file systems by caching each stream
independently. NTFS can exploit this feature by organizing its master file
table (described later in this chapter in the “Master file table” section) into
streams and by caching these streams as well. In fact, although the cache
manager might be said to cache files, it actually caches streams (all files have
at least one stream of data) identified by both a file name and, if more than
one stream exists in the file, a stream name.
 Note
Internally, the cache manager is not aware of file or stream names but
uses pointers to these structures.
Recoverable file system support
Recoverable file systems such as NTFS are designed to reconstruct the disk
volume structure after a system failure. This capability means that I/O
operations in progress at the time of a system failure must be either entirely
completed or entirely backed out from the disk when the system is restarted.
Half-completed I/O operations can corrupt a disk volume and even render an
entire volume inaccessible. To avoid this problem, a recoverable file system
maintains a log file in which it records every update it intends to make to the
file system structure (the file system’s metadata) before it writes the change
to the volume. If the system fails, interrupting volume modifications in
progress, the recoverable file system uses information stored in the log to
reissue the volume updates.
To guarantee a successful volume recovery, every log file record
documenting a volume update must be completely written to disk before the
update itself is applied to the volume. Because disk writes are cached, the
cache manager and the file system must coordinate metadata updates by
ensuring that the log file is flushed ahead of metadata updates. Overall, the
following actions occur in sequence:
1. 
The file system writes a log file record documenting the metadata
update it intends to make.
2. 
The file system calls the cache manager to flush the log file record to
disk.
3. 
The file system writes the volume update to the cache—that is, it
modifies its cached metadata.
4. 
The cache manager flushes the altered metadata to disk, updating the
volume structure. (Actually, log file records are batched before being
flushed to disk, as are volume modifications.)
 Note
The term metadata applies only to changes in the file system structure:
file and directory creation, renaming, and deletion.
When a file system writes data to the cache, it can supply a logical
sequence number (LSN) that identifies the record in its log file, which
corresponds to the cache update. The cache manager keeps track of these
numbers, recording the lowest and highest LSNs (representing the oldest and
newest log file records) associated with each page in the cache. In addition,
data streams that are protected by transaction log records are marked as “no
write” by NTFS so that the mapped page writer won’t inadvertently write out
these pages before the corresponding log records are written. (When the
mapped page writer sees a page marked this way, it moves the page to a
special list that the cache manager then flushes at the appropriate time, such
as when lazy writer activity takes place.)
When it prepares to flush a group of dirty pages to disk, the cache manager
determines the highest LSN associated with the pages to be flushed and
reports that number to the file system. The file system can then call the cache
manager back, directing it to flush log file data up to the point represented by
the reported LSN. After the cache manager flushes the log file up to that
LSN, it flushes the corresponding volume structure updates to disk, thus
ensuring that it records what it’s going to do before actually doing it. These
interactions between the file system and the cache manager guarantee the
recoverability of the disk volume after a system failure.
NTFS MFT working set enhancements
As we have described in the previous paragraphs, the mechanism that the
cache manager uses to cache files is the same as general memory mapped I/O
interfaces provided by the memory manager to the operating system. For
accessing or caching a file, the cache manager maps a view of the file in the
system virtual address space. The contents are then accessed simply by
reading off the mapped virtual address range. When the cached content of a
file is no longer needed (for various reasons—see the next paragraphs for
details), the cache manager unmaps the view of the file. This strategy works
well for any kind of data files but has some problems with the metadata that
the file system maintains for correctly storing the files in the volume.
When a file handle is closed (or the owning process dies), the cache
manager ensures that the cached data is no longer in the working set. The
NTFS file system accesses the Master File Table (MFT) as a big file, which
is cached like any other user files by the cache manager. The problem with
the MFT is that, since it is a system file, which is mapped and processed in
the System process context, nobody will ever close its handle (unless the
volume is unmounted), so the system never unmaps any cached view of the
MFT. The process that initially caused a particular view of MFT to be
mapped might have closed the handle or exited, leaving potentially unwanted
views of MFT still mapped into memory consuming valuable system cache
(these views will be unmapped only if the system runs into memory
pressure).
Windows 8.1 resolved this problem by storing a reference counter to every
MFT record in a dynamically allocated multilevel array, which is stored in
the NTFS file system Volume Control Block (VCB) structure. Every time a
File Control Block (FCB) data structure is created (further details on the FCB
and VCB are available later in this chapter), the file system increases the
counter of the relative MFT index record. In the same way, when the FCB is
destroyed (meaning that all the handles to the file or directory that the MFT
entry refers to are closed), NTFS dereferences the relative counter and calls
the CcUnmapFileOffsetFromSystemCache cache manager routine, which
will unmap the part of the MFT that is no longer needed.
Memory partitions support
Windows 10, with the goal to provide support for Hyper-V containers
containers and game mode, introduced the concept of partitions. Memory
partitions have already been described in Chapter 5 of Part 1. As seen in that
chapter, memory partitions are represented by a large data structure
(MI_PARTITION), which maintains memory-related management structures
related to the partition, such as page lists (standby, modified, zero, free, and
so on), commit charge, working set, page trimmer, modified page writer, and
zero-page thread. The cache manager needs to cooperate with the memory
manager in order to support partitions. During phase 1 of NT kernel
initialization, the system creates and initializes the cache manager partition
(for further details about Windows kernel initialization, see Chapter 12,
“Startup and shutdown”), which will be part of the System Executive
partition (MemoryPartition0). The cache manager’s code has gone through a
big refactoring to support partitions; all the global cache manager data
structures and variables have been moved in the cache manager partition data
structure (CC_PARTITION).
The cache manager’s partition contains cache-related data, like the global
shared cache maps list, the worker threads list (read-ahead, write-behind, and
extra write-behind; lazy writer and lazy writer scan; async reads), lazy writer
scan events, an array that holds the history of write-behind throughout, the
upper and lower limit for the dirty pages threshold, the number of dirty
pages, and so on. When the cache manager system partition is initialized, all
the needed system threads are started in the context of a System process
which belongs to the partition. Each partition always has an associated
minimal System process, which is created at partition-creation time (by the
NtCreatePartition API).
When the system creates a new partition through the NtCreatePartition
API, it always creates and initializes an empty MI_PARTITION object (the
memory is moved from a parent partition to the child, or hot-added later by
using the NtManagePartition function). A cache manager partition object is
created only on-demand. If no files are created in the context of the new
partition, there is no need to create the cache manager partition object. When
the file system creates or opens a file for caching access, the
CcinitializeCacheMap(Ex) function checks which partition the file belongs to
and whether the partition has a valid link to a cache manager partition. In
case there is no cache manager partition, the system creates and initializes a
new one through the CcCreatePartition routine. The new partition starts
separate cache manager-related threads (read-ahead, lazy writers, and so on)
and calculates the new values of the dirty page threshold based on the
number of pages that belong to the specific partition.
The file object contains a link to the partition it belongs to through its
control area, which is initially allocated by the file system driver when
creating and mapping the Stream Control Block (SCB). The partition of the
target file is stored into a file object extension (of type
MemoryPartitionInformation) and is checked by the memory manager when
creating the section object for the SCB. In general, files are shared entities, so
there is no way for File System drivers to automatically associate a file to a
different partition than the System Partition. An application can set a
different partition for a file using the NtSetInformationFileKernel API,
through the new FileMemoryPartitionInformation class.
Cache virtual memory management
Because the Windows system cache manager caches data on a virtual basis, it
uses up regions of system virtual address space (instead of physical memory)
and manages them in structures called virtual address control blocks, or
VACBs. VACBs define these regions of address space into 256 KB slots
called views. When the cache manager initializes during the bootup process,
it allocates an initial array of VACBs to describe cached memory. As caching
requirements grow and more memory is required, the cache manager
allocates more VACB arrays, as needed. It can also shrink virtual address
space as other demands put pressure on the system.
At a file’s first I/O (read or write) operation, the cache manager maps a
256 KB view of the 256 KB-aligned region of the file that contains the
requested data into a free slot in the system cache address space. For
example, if 10 bytes starting at an offset of 300,000 bytes were read into a
file, the view that would be mapped would begin at offset 262144 (the
second 256 KB-aligned region of the file) and extend for 256 KB.
The cache manager maps views of files into slots in the cache’s address
space on a round-robin basis, mapping the first requested view into the first
256 KB slot, the second view into the second 256 KB slot, and so forth, as
shown in Figure 11-3. In this example, File B was mapped first, File A
second, and File C third, so File B’s mapped chunk occupies the first slot in
the cache. Notice that only the first 256 KB portion of File B has been
mapped, which is due to the fact that only part of the file has been accessed.
Because File C is only 100 KB (and thus smaller than one of the views in the
system cache), it requires its own 256 KB slot in the cache.
Figure 11-3 Files of varying sizes mapped into the system cache.
The cache manager guarantees that a view is mapped as long as it’s active
(although views can remain mapped after they become inactive). A view is
marked active, however, only during a read or write operation to or from the
file. Unless a process opens a file by specifying the FILE_FLAG_RANDOM_
ACCESS flag in the call to CreateFile, the cache manager unmaps inactive
views of a file as it maps new views for the file if it detects that the file is
being accessed sequentially. Pages for unmapped views are sent to the
standby or modified lists (depending on whether they have been changed),
and because the memory manager exports a special interface for the cache
manager, the cache manager can direct the pages to be placed at the end or
front of these lists. Pages that correspond to views of files opened with the
FILE_FLAG_SEQUENTIAL_SCAN flag are moved to the front of the lists,
whereas all others are moved to the end. This scheme encourages the reuse of
pages belonging to sequentially read files and specifically prevents a large
file copy operation from affecting more than a small part of physical
memory. The flag also affects unmapping. The cache manager will
aggressively unmap views when this flag is supplied.
If the cache manager needs to map a view of a file, and there are no more
free slots in the cache, it will unmap the least recently mapped inactive view
and use that slot. If no views are available, an I/O error is returned, indicating
that insufficient system resources are available to perform the operation.
Given that views are marked active only during a read or write operation,
however, this scenario is extremely unlikely because thousands of files
would have to be accessed simultaneously for this situation to occur.
Cache size
In the following sections, we explain how Windows computes the size of the
system cache, both virtually and physically. As with most calculations related
to memory management, the size of the system cache depends on a number
of factors.
Cache virtual size
On a 32-bit Windows system, the virtual size of the system cache is limited
solely by the amount of kernel-mode virtual address space and the
SystemCacheLimit registry key that can be optionally configured. (See
Chapter 5 of Part 1 for more information on limiting the size of the kernel
virtual address space.) This means that the cache size is capped by the 2-GB
system address space, but it is typically significantly smaller because the
system address space is shared with other resources, including system paged
table entries (PTEs), nonpaged and paged pool, and page tables. The
maximum virtual cache size is 64 TB on 64-bit Windows, and even in this
case, the limit is still tied to the system address space size: in future systems
that will support the 56-bit addressing mode, the limit will be 32 PB
(petabytes).
Cache working set size
As mentioned earlier, one of the key differences in the design of the cache
manager in Windows from that of other operating systems is the delegation
of physical memory management to the global memory manager. Because of
this, the existing code that handles working set expansion and trimming, as
well as managing the modified and standby lists, is also used to control the
size of the system cache, dynamically balancing demands for physical
memory between processes and the operating system.
The system cache doesn’t have its own working set but shares a single
system set that includes cache data, paged pool, pageable kernel code, and
pageable driver code. As explained in the section “System working sets” in
Chapter 5 of Part 1, this single working set is called internally the system
cache working set even though the system cache is just one of the
components that contribute to it. For the purposes of this book, we refer to
this working set simply as the system working set. Also explained in Chapter
5 is the fact that if the LargeSystemCache registry value is 1, the memory
manager favors the system working set over that of processes running on the
system.
Cache physical size
While the system working set includes the amount of physical memory that is
mapped into views in the cache’s virtual address space, it does not
necessarily reflect the total amount of file data that is cached in physical
memory. There can be a discrepancy between the two values because
additional file data might be in the memory manager’s standby or modified
page lists.
Recall from Chapter 5 that during the course of working set trimming or
page replacement, the memory manager can move dirty pages from a
working set to either the standby list or the modified page list, depending on
whether the page contains data that needs to be written to the paging file or
another file before the page can be reused. If the memory manager didn’t
implement these lists, any time a process accessed data previously removed
from its working set, the memory manager would have to hard-fault it in
from disk. Instead, if the accessed data is present on either of these lists, the
memory manager simply soft-faults the page back into the process’s working
set. Thus, the lists serve as in-memory caches of data that are stored in the
paging file, executable images, or data files. Thus, the total amount of file
data cached on a system includes not only the system working set but the
combined sizes of the standby and modified page lists as well.
An example illustrates how the cache manager can cause much more file
data than that containable in the system working set to be cached in physical
memory. Consider a system that acts as a dedicated file server. A client
application accesses file data from across the network, while a server, such as
the file server driver (%SystemRoot%\System32\Drivers\Srv2.sys, described
later in this chapter), uses cache manager interfaces to read and write file data
on behalf of the client. If the client reads through several thousand files of 1
MB each, the cache manager will have to start reusing views when it runs out
of mapping space (and can’t enlarge the VACB mapping area). For each file
read thereafter, the cache manager unmaps views and remaps them for new
files. When the cache manager unmaps a view, the memory manager doesn’t
discard the file data in the cache’s working set that corresponds to the view;
it moves the data to the standby list. In the absence of any other demand for
physical memory, the standby list can consume almost all the physical
memory that remains outside the system working set. In other words,
virtually all the server’s physical memory will be used to cache file data, as
shown in Figure 11-4.
Figure 11-4 Example in which most of physical memory is being used by
the file cache.
Because the total amount of file data cached includes the system working
set, modified page list, and standby list—the sizes of which are all controlled
by the memory manager—it is in a sense the real cache manager. The cache
manager subsystem simply provides convenient interfaces for accessing file
data through the memory manager. It also plays an important role with its
read-ahead and write-behind policies in influencing what data the memory
manager keeps present in physical memory, as well as with managing system
virtual address views of the space.
To try to accurately reflect the total amount of file data that’s cached on a
system, Task Manager shows a value named “Cached” in its performance
view that reflects the combined size of the system working set, standby list,
and modified page list. Process Explorer, on the other hand, breaks up these
values into Cache WS (system cache working set), Standby, and Modified.
Figure 11-5 shows the system information view in Process Explorer and the
Cache WS value in the Physical Memory area in the lower left of the figure,
as well as the size of the standby and modified lists in the Paging Lists area
near the middle of the figure. Note that the Cache value in Task Manager
also includes the Paged WS, Kernel WS, and Driver WS values shown in
Process Explorer. When these values were chosen, the vast majority of
System WS came from the Cache WS. This is no longer the case today, but
the anachronism remains in Task Manager.
Figure 11-5 Process Explorer’s System Information dialog box.
Cache data structures
The cache manager uses the following data structures to keep track of cached
files:
■    Each 256 KB slot in the system cache is described by a VACB.
■    Each separately opened cached file has a private cache map, which
contains information used to control read-ahead (discussed later in the
chapter in the “Intelligent read-ahead” section).
■    Each cached file has a single shared cache map structure, which
points to slots in the system cache that contain mapped views of the
file.
These structures and their relationships are described in the next sections.
Systemwide cache data structures
As previously described, the cache manager keeps track of the state of the
views in the system cache by using an array of data structures called virtual
address control block (VACB) arrays that are stored in nonpaged pool. On a
32-bit system, each VACB is 32 bytes in size and a VACB array is 128 KB,
resulting in 4,096 VACBs per array. On a 64-bit system, a VACB is 40 bytes,
resulting in 3,276 VACBs per array. The cache manager allocates the initial
VACB array during system initialization and links it into the systemwide list
of VACB arrays called CcVacbArrays. Each VACB represents one 256 KB
view in the system cache, as shown in Figure 11-6. The structure of a VACB
is shown in Figure 11-7.
Figure 11-6 System VACB array.
Figure 11-7 VACB data structure.
Additionally, each VACB array is composed of two kinds of VACB: low
priority mapping VACBs and high priority mapping VACBs. The system
allocates 64 initial high priority VACBs for each VACB array. High priority
VACBs have the distinction of having their views preallocated from system
address space. When the memory manager has no views to give to the cache
manager at the time of mapping some data, and if the mapping request is
marked as high priority, the cache manager will use one of the preallocated
views present in a high priority VACB. It uses these high priority VACBs,
for example, for critical file system metadata as well as for purging data from
the cache. After high priority VACBs are gone, however, any operation
requiring a VACB view will fail with insufficient resources. Typically, the
mapping priority is set to the default of low, but by using the
PIN_HIGH_PRIORITY flag when pinning (described later) cached data, file
systems can request a high priority VACB to be used instead, if one is
needed.
As you can see in Figure 11-7, the first field in a VACB is the virtual
address of the data in the system cache. The second field is a pointer to the
shared cache map structure, which identifies which file is cached. The third
field identifies the offset within the file at which the view begins (always
based on 256 KB granularity). Given this granularity, the bottom 16 bits of
the file offset will always be zero, so those bits are reused to store the number
of references to the view—that is, how many active reads or writes are
accessing the view. The fourth field links the VACB into a list of least-
recently-used (LRU) VACBs when the cache manager frees the VACB; the
cache manager first checks this list when allocating a new VACB. Finally,
the fifth field links this VACB to the VACB array header representing the
array in which the VACB is stored.
During an I/O operation on a file, the file’s VACB reference count is
incremented, and then it’s decremented when the I/O operation is over. When
the reference count is nonzero, the VACB is active. For access to file system
metadata, the active count represents how many file system drivers have the
pages in that view locked into memory.
EXPERIMENT: Looking at VACBs and VACB
statistics