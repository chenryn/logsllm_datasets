Figure 314. The 970FX instruction pipeline
[View full size image]
IFAR, ICA[37]
[37] Instruction Cache Access.
Based on the address in the Instruction Fetch Address Register (IFAR), the instruction-fetch-logic
fetches eight instructions every cycle from the L1 I-cache into a 32-entry instruction buffer. The
eight-instruction block, so fetched, is 32-byte aligned. Besides performing IFAR-based demand
fetching, the 970FX prefetches cache lines into a 4x128-byte Instruction Prefetch Queue. If a demand
fetch results in an I-cache miss, the 970FX checks whether the instructions are in the prefetch queue.
If the instructions are found, they are inserted into the pipeline as if no I-cache miss had occurred. The
cache line's critical sector (eight words) is written into the I-cache.
D0
There is logic to partially decode (predecode) instructions after they leave the L2 cache and before
they enter the I-cache or the prefetch queue. This process adds five extra bits to each instruction to
yield a 37-bit instruction. An instruction's predecode bits mark it as illegal, microcoded, conditional
or unconditional branch, and so on. In particular, the bits also specify how the instruction is to be
grouped for dispatching.
D1, D2, D3
The 970FX splits complex instructions into two or more internal operations, or iops. The iops are
more RISC-like than the instructions they are part of. Instructions that are broken into exactly two
iops are called cracked instructions, whereas those that are broken into three or more iops are called
microcoded instructions because the processor emulates them using microcode.
An instruction may not be atomic because the atomicity of cracked or microcoded instructions is at
the iop level. Moreover, it is the iops, and not programmer-visible instructions, that are executed out-
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hh687B.htm 20.08.2007
Chapter 3. Inside an Apple Page 41 of 83
of-order. This approach allows the processor more flexibility in parallelizing execution. Note that
AltiVec instructions are neither cracked nor microcoded.
Fetched instructions go to a 32-instruction fetch buffer. Every cycle, up to five instructions are taken
from this buffer and sent through a decode pipeline that is either inline (consisting of three stages,
namely, D1, D2, and D3), or template-based if the instruction needs to be microcoded. The template-
based decode pipeline generates up to four iops per cycle that emulate the original instruction. In any
case, the decode pipeline leads to the formation of an instruction dispatch group.
Given the out-of-order execution of instructions, the processor needs to keep track of the program
order of all instructions in various stages of execution. Rather than tracking individual instructions,
the 970FX tracks instructions in dispatch groups. The 970FX forms such groups containing one to
five iops, each occupying an instruction slot (0 through 4) in the group. Dispatch group formation[38]
is subject to a long list of rules and conditions such as the following.
[38] The instruction grouping performed by the 970FX has similarities to a VLIW
processor.
The iops in a group must be in program order, with the oldest instruction being in slot 0.
A group may contain up to four nonbranch instructions and optionally a branch instruction.
When a branch is encountered, it is the last instruction in the current group, and a new group is
started.
Slot 4 can contain only branch instructions. In fact, no-op (no-operation) instructions may have
to be inserted in the other slots to force a branch instruction to fall in slot 4.
An instruction that is a branch target is always at the start of a group.
A cracked instruction takes two slots in a group.
A microcoded instruction takes an entire group by itself.
An instruction that modifies an SPR with no associated rename register terminates a group.
No more than two instructions that modify the CR may be in a group.
XFER
The iops wait for resources to become free in the XFER stage.
GD, DSP, WRT, GCT, MAP
After group formation, the execution pipeline divides into multiple pipelines for the various execution
units. Every cycle, one group of instructions can be sent (or dispatched) to the issue queues. Note that
instructions in a group remain together from dispatch to completion.
As a group is dispatched, several operations occur before the instructions actually execute. Internal
group instruction dependencies are determined (GD). Various internal resources are assigned, such as
issue queue slots, rename registers and mappers, and entries in the load/store reorder queues. In
particular, each iop in the group that returns a result must be assigned a register to hold the result.
Rename registers are allocated in the dispatch phase before the instructions enter the issue queues
(DSP, MAP).
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hh687B.htm 20.08.2007
Chapter 3. Inside an Apple Page 42 of 83
To track the groups themselves, the 970FX uses a global completion table (GCT) that stores up to 20
entries in program orderthat is, up to 20 dispatch groups can be in flight concurrently. Since each
group can have up to 5 iops, as many as 100 iops can be tracked in this manner. The WRT stage
represents the writes to the GCT.
ISS, RF
After all the resources that are required to execute the instructions are available, the instructions are
sent (ISS) to appropriate issue queues. Once their operands appear, the instructions start to execute.
Each slot in a group feeds separate issue queues for various execution units. For example, the
FXU/LSU and the FPU draw their instructions from slots { 0, 3 } and { 1, 2 }, respectively, of an
instruction group. If one pair goes to the FXU/LSU, the other pair goes to the FPU. The CRU draws
its instructions from the CR logical issue queue that is fed from instruction slots 0 and 1. As we saw
earlier, slot 4 of an instruction group is dedicated to branch instructions. AltiVec instructions can be
issued to the VALU and the VPERM issue queues from any slot except slot 4. Table 38 shows the
970FX issue queue sizeseach execution unit listed has one issue queue.
Table 38. Sizes of the Various 970FX Issue Queues
Execution Unit Queue Size (Instructions)
LSU0/FXU0[a] 18
LSU1/FXU1[b] 18
FPU0 10
FPU1 10
BRU 12
CRU 10
VALU 20
VPERM 16
[a] LSU0 and FXU0 share an 18-entry issue queue.
[b] LSU1 and FXU1 share an 18-entry issue queue.
The FXU/LSU and FPU issue queues have odd and even halves that are hardwired to receive
instructions only from certain slots of a dispatch group, as shown in Figure 315.
Figure 315. The FPU and FXU/LSU issue queues in the 970FX
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hh687B.htm 20.08.2007
Chapter 3. Inside an Apple Page 43 of 83
As long as an issue queue contains instructions that have all their data dependencies resolved, an
instruction moves every cycle from the queue into the appropriate execution unit. However, there are
likely to be instructions whose operands are not ready; such instructions block in the queue. Although
the 970FX will attempt to execute the oldest instruction first, it will reorder instructions within a
queue's context to avoid stalling. Ready-to-execute instructions access their source operands by
reading the corresponding register file (RF), after which they enter the execution unit pipelines. Up to
ten operations can be issued in a cycleone to each of the ten execution pipelines. Note that different
execution units may have varying numbers of pipeline stages.
We have seen that instructions both issue and execute out of order. However, if an instruction has
finished execution, it does not mean that the program will "know" about it. After all, from the
program's standpoint, instructions must execute in program order. The 970FX differentiates between
an instruction finishing execution and an instruction completing. An instruction may finish execution
(speculatively, say), but unless it completes, its effect is not visible to the program. All pipelines
terminate in a common stage: the group completion stage (CP). When groups complete, many of their
resources are released, such as load reorder queue entries, mappers, and global completion table
entries. One dispatch group may be "retired" per cycle.
When a branch instruction completes, the resultant target address is compared with a predicted
address. Depending on whether the prediction is correct or incorrect, either all instructions in the
pipeline that were fetched after the branch in question are flushed, or the processor waits for all
remaining instructions in the branch's group to complete.
Accounting for 215 In-Flight Instructions
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hh687B.htm 20.08.2007
Chapter 3. Inside an Apple Page 44 of 83
We can account for the theoretical maximum of 215 in-flight instructions by looking at Figure
34specifically, the areas marked 1 through 6.
1. The Instruction Fetch Unit has a fetch/overflow buffer that can hold 16 instructions.
2. The instruction fetch buffer in the decode/dispatch unit can hold 32 instructions.
3. Every cycle, up to 5 instructions are taken from the instruction fetch buffer and sent through a
three-stage instruction decode pipeline. Therefore, up to 15 instructions can be in this pipeline.
4. There are four dispatch buffers, each holding a dispatch group of up to five operations.
Therefore, up to 20 instructions can be held in these buffers.
5. The global completion table can track up to 20 dispatch groups after they have been dispatched,
corresponding to up to 100 instructions in the 970FX core.
6. The store queue can hold up to 32 stores.
Thus, the theoretical maximum number of in-flight instructions can be calculated as the sum 16 + 32
+ 15 + 20 + 100 + 32, which is 215.
3.3.9.2. Branch Prediction
Branch prediction is a mechanism wherein the processor attempts to keep the pipeline full, and
therefore improve overall performance, by fetching instructions in the hope that they will be executed.
In this context, a branch is a decision point for the processor: It must predict the outcome of the
branchwhether it will be taken or notand accordingly prefetch instructions. As shown in Figure 315,
the 970FX scans fetched instructions for branches. It looks for up to two branches per cycle and uses
multistrategy branch prediction logic to predict their target addresses, directions, or both.
Consequently, up to 2 branches are predicted per cycle, and up to 16 predicted branches can be in
flight.
All conditional branches are predicted, based on whether the 970FX fetches instructions beyond a
branch and speculatively executes them. Once the branch instruction itself executes in the BRU, its
actual outcome is compared with its predicted outcome. If the prediction was incorrect, there is a
severe penalty: Any instructions that may have speculatively executed are discarded, and instructions
in the correct control-flow path are fetched.
The 970FX's dynamic branch prediction hardware includes three branch history tables (BHTs), a link
stack, and a count cache. Each BHT has 16K 1-bit entries.
The 970FX's hardware branch prediction can be overridden by software.
The first BHT is the local predictor table. Its 16K entries are indexed by branch instruction addresses.
Each 1-bit entry indicates whether the branch should be taken or not. This scheme is "local" because
each branch is tracked in isolation.
The second BHT is the global predictor table. It is used by a prediction scheme that takes into account
the execution path taken to reach the branch. An 11-bit vectorthe global history vectorrepresents the
execution path. The bits of this vector represent the previous 11 instruction groups fetched. A
particular bit is 1 if the next group was fetched sequentially and is 0 otherwise. A given branch's entry
in the global predictor table is at a location calculated by performing an XOR operation between the
global history vector and the branch instruction address.
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hh687B.htm 20.08.2007
Chapter 3. Inside an Apple Page 45 of 83
The third BHT is the selector table. It tracks which of the two prediction schemes is to be favored for
a given branch. The BHTs are kept up to date with the actual outcomes of executed branch
instructions.
The link stack and the count cache are used by the 970FX to predict branch target addresses of
branch-conditional-to-link-register (bclr, bclrl) and branch-conditional-to-count-register (bcctr,
bcctrl) instructions, respectively.
So far, we have looked at dynamic branch prediction. The 970FX also supports static prediction
wherein the programmer can use certain bits in a conditional branch operand to statically override
dynamic prediction. Specifically, two bits called the "a" and "t" bits are used to provide hints
regarding the branch's direction, as shown in Table 39.
Table 39. Static Branch Prediction Hints
"a" Bit "t" Bit Hint
0 0 Dynamic branch prediction is
used.
0 1 Dynamic branch prediction is
used.
1 0 Dynamic branch prediction is
disabled; static prediction is
"not taken"; specified by a "-"
suffix to a branch conditional
mnemonic.
1 1 Dynamic branch prediction is
disabled; static prediction is
"taken"; specified by a "+"
suffix to a branch conditional
mnemonic.
3.3.9.3. Summary
Let us summarize the instruction parallelism achieved by the 970FX. In every cycle of the 970FX, the
following events occur.
Up to eight instructions are fetched.
Up to two branches are predicted.
Up to five iops (one group) are dispatched.
Up to five iops are renamed.
Up to ten iops are issued from the issue queues.
Up to five iops are completed.
3.3.10. AltiVec
The 970FX includes a dedicated vector-processing unit and implements the VMX instruction set,
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hh687B.htm 20.08.2007
Chapter 3. Inside an Apple Page 46 of 83
which is an AltiVec[39] interchangeable extension to the PowerPC architecture. AltiVec provides a
SIMD-style 128-bit[40] vector-processing unit.
[39] AltiVec was first introduced in Motorola's e600 PowerPC corethe G4.
[40] All AltiVec execution units and data paths are 128 bits wide.
3.3.10.1. Vector Computing
SIMD stands for single-instruction, multiple-data. It refers to a set of operations that can efficiently
handle large quantities of data in parallel. SIMD operations do not necessarily require more or wider
registers, although more is better. SIMD essentially better uses registers and data paths. For example,
a non-SIMD computation would typically use a hardware register for each data element, even if the
register could hold multiple such elements. In contrast, SIMD would use a register to hold multiple
data elementsas many as would fitand would perform the same operation on all elements through a
single instruction. Thus, any operation that can be parallelized in this manner stands to benefit from
SIMD. In AltiVec's case, a vector instruction can perform the same operation on all constituents of a
vector. Note that AltiVec instructions work on fixed-length vectors.
SIMD-based optimization does not come for free. A problem must lend itself well to vectorization,
and the programmer must usually perform extra work. Some compilerssuch as IBM's XL suite of
compilers and GCC 4.0 or abovealso support auto-vectorization, an optimization that auto-generates
vector instructions based on the compiler's analysis of the source code.[41] Auto-vectorization may or
may not work well depending on the nature and structure of the code.
[41] For example, the compiler may attempt to detect patterns of code that are known to
be well suited for vectorization.
Several processor architectures have similar extensions. Table 310 lists some well-known examples.
Table 310. Examples of Processor Multimedia-Extensions
Processor Multimedia
Family Manufacturers Extension Sets
Alpha Hewlett-Packard (Digital MVI
Equipment Corporation)
AMD Advanced Micro Devices 3DNow!
(AMD)
MIPS Silicon Graphics MDMX, MIPS-3D
Incorporated (SGI)
PA-RISC Hewlett-Packard MAX, MAX2
PowerPC IBM, Motorola VMX/AltiVec
SPARC V9 Sun Microsystems VIS
x86 Intel, AMD, Cyrix MMX, SSE, SSE2,
SSE3
AltiVec can greatly improve the performance of data movement, benefiting applications that do
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hh687B.htm 20.08.2007
Chapter 3. Inside an Apple Page 47 of 83
processing of vectors, matrices, arrays, signals, and so on. As we saw in Chapter 2, Apple provides
portable APIsthrough the Accelerate framework (Accelerate.framework)for performing vector-
optimized operations.[42] Accelerate is an umbrella framework that contains the vecLib and vImage
[43] subframeworks. vecLib is targeted for performing numerical and scientific computingit provides
functionality such as BLAS, LAPACK, digital signal processing, dot products, linear algebra, and
matrix operations. vImage provides vector-optimized APIs for working with image data. For
example, it provides functions for alpha compositing, convolutions, format conversion, geometric
transformations, histograms operations, and morphological operations.
[42] The Accelerate framework automatically uses the best available code that it
implements, depending on the hardware it is running on. For example, it will use
vectorized code for AltiVec if AltiVec is available. On the x86 platform, it will use
MMX, SSE, SSE2, and SSE3 if these features are available.
[43] vImage is also available as a stand-alone framework.
Although a vector instruction performs work that would typically require many times more nonvector
instructions, vector instructions are not simply instructions that deal with "many scalars" or "more
memory" at a time. The fact that a vector's members are related is critical, and so is the fact that the
same operation is performed on all members. Vector operations certainly play better with memory
accessesthey lead to amortization. The semantic difference between performing a vector operation
and a sequence of scalar operations on the same data set is that you are implicitly providing more
information to the processor about your intentions. Vector operationsby their naturealleviate both data
and control hazards.
AltiVec has wide-ranging applications since areas such as high-fidelity audio, video,
videoconferencing, graphics, medical imaging, handwriting analysis, data encryption, speech
recognition, image processing, and communications all use algorithms that can benefit from vector
processing.
Figure 316 shows a trivial AltiVec C program.
Figure 316. A trivial AltiVec program
// altivec.c
#include 
#include 
int
main(void)
{
// "vector" is an AltiVec keyword
vector float v1, v2, v3;
v1 = (vector float)(1.0, 2.0, 3.0, 4.0);
v2 = (vector float)(2.0, 3.0, 4.0, 5.0);
// vector_add() is a compiler built-in function
v3 = vector_add(v1, v2);
// "%vf" is a vector-formatting string for printf()
printf("%vf\n", v3);
exit(0);
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hh687B.htm 20.08.2007
Chapter 3. Inside an Apple Page 48 of 83
}
$ gcc -Wall -faltivec -o altivec altivec.c
$ ./altivec
3.000000 5.000000 7.000000 9.000000