2
0
0
0
1
4
0
1
1
100.00%
85.71%
100.00%
80.77%
90.32%
87.80%
97.22%
100.00%
83.33%
85.42%
91.06%
100.00%
100.00%
92.31%
100.00%
100.00%
100.00%
97.22%
85.71%
100.00%
97.62%
97.29%
100.00%
92.31%
96.00%
89.36%
94.92%
93.51%
97.22%
92.31%
90.91%
91.11%
93.76%
4,598
15,073
5,155
18,062
8,168
34,956
34,978
8,161
34,175
9,993
17,332
90,467
382,876
123,152
107,506
243,504
249,316
299,147
250,512
667,329
343,959
275,777
0
3
0
45
3
49
10
0
8
75
19
0
0
10
0
0
0
1
75
0
1
9
100.00%
99.98%
100.00%
99.75%
99.96%
99.86%
99.97%
100.00%
99.98%
99.26%
99.88%
100.00%
100.00%
99.81%
100.00%
100.00%
100.00%
100.00%
99.09%
100.00%
99.99%
99.89%
100.00%
99.99%
99.90%
99.88%
99.98%
99.93%
99.98%
99.54%
99.99%
99.62%
99.88%
TP and TN stands for correctly reported attack and non-attack (normal) entities/events. FP and FN stands for incorrectly labeled attack and non-attack (normal) entities/events.
entity-level (3 out of 20,075 entities) and event-level (19 out
of 293,109 events) analyses. These results show that a cy-
ber analyst requires less manual labor to validate the causes
of a true attack needed for the attack story. We found that
most false positives are due to the misclassiﬁcation of the
IP addresses. For instance, most false positives in M-5 and
M-6 attacks were due to the benign IP addresses, which were
active during the same time as those malicious IP addresses
of the Command and Control (C&C) servers. However, the
security investigators can easily identify the IP addresses by
checking their trafﬁc content and registration information to
ﬁlter out such false positives.
Analysis of False Negatives. False negatives are the number
of attack entities and events that ATLAS incorrectly classi-
ﬁed as non-attack (see Table 4, Column 6 and 13). ATLAS
yields on average a 2.71% false-negative rate at the entity-
level and a 0.11% false-negative rate at the event-level. We
found that even when ATLAS misidentiﬁes an attack entity,
ATLAS can still identify attack entities which were caused
by such a misidentiﬁed attack entity. For example, the false
negatives in attack M-4 are due to misidentifying a malicious
Microsoft Word ﬁle (evil.rtf) that is used to download a
malicious payload; however, ATLAS was able to identify the
malicious payload entity (payload.exe) which was caused
by the misidentiﬁed word ﬁle. False negatives in the attacks
M-6 and S-3 are caused by missing some scripts downloaded
from a stealthy network I/O trace performed by the attacker.
Here, the attacker uses the Multiple UNC Provider (MUP) [7],
a speciﬁc component in the Windows NT operating system,
to access shared folders across the network. We note that the
false negatives can be alleviated by training ATLAS with more
attack types sharing similar patterns with these cases.
6.2.2
Individual Component Analysis
Figure 8: Effectiveness of causal graph optimization of given
audit logs for attack investigation. The percentages on the
bars show the percentage of the logs reduction.
the number of entities before and after graph optimization.
ATLAS reduces the number of entities in a causal graph on
average 81.81% for audit logs of each attack compared their
original graph size. The reduction removes the redundant or
unrelated events from the huge volume of logs that do not
contribute any semantics to model different attack sequences.
Hence, the further extracted sequences are more representa-
tive and efﬁcient as input for the model training.
Selective Sequence Sampling. The selective sequence sam-
pling mechanism of ATLAS is the key step to building a precise
model from a balanced dataset. As illustrated in Table 3 (Col-
umn 6 and 8), ATLAS oversamples the attack sequences with
an average 37x increase, from 61 to 2,264, and undersamples
non-attack sequences with an average reduction of 9x, from
20,626 to 2,264. Our evaluation shows that this process re-
duces the training time on average 87% (from 3h:37min to
0h:28min for training each model). Overall, this mechanism
extracts an average of 22% of the initial sequences as a highly
representative training set and uses them for model training,
which signiﬁcantly improves the model accuracy.
The effectiveness of ATLAS lies in a set of optimization tech-
niques integrated into its components. Here we elaborate on
how these components contribute to its effectiveness.
Causal Graph Optimization. As detailed in Sec. 4.1, we de-
veloped our customized optimization algorithms to construct
the causal graph which helps reduce the graph complexity and
in turn improves the sequence construction. Figure 8 shows
6.3 Comparison Analysis
We have implemented a set of state-of-the-art approaches
that can be used in lieu of ATLAS components and compare
their performances with ATLAS in attack identiﬁcation. We
note that the comparison is conducted through event-based
attack investigation results as previous provenance tracking
3016    30th USENIX Security Symposium
USENIX Association
Table 5: Comparison of ATLAS with the baseline approaches.
Method
Precision
Recall
F1-score
Graph-traversal
Non-optimized causal graph
Oversampling-only model
One-hot encoding
Support Vector Machine (SVM)
17.82%
87.58%
97.85%
99.60%
87.12%
100.00% 30.26%
56.36%
41.55%
87.81%
79.64%
89.19%
80.75%
90.42%
88.74%
ATLAS
99.88% 99.89% 99.88%
approaches (e.g., [11, 16, 50]) provide event-based attack re-
sults. Table 5 summarizes our results.
Graph Traversal. To compare ATLAS in attack investiga-
tion with the state-of-art approaches [11, 16, 50], we have
implemented a baseline approach that performs backward and
forward tracing on the provenance graph, a directed acyclic
graph built based on the W3C provenance data model spec-
iﬁcation [48]. We note that since none of the previous ap-
proaches are publicly available, we are not able to perform a
direct comparison with them. This approach proceeds in two
steps. First, the backward tracing starts from an attack symp-
tom event and stops at the root causes of the attack (e.g., a
phishing link in malicious email). Second, the forward tracing
starts from the identiﬁed root causes of the attack and stops at
the ﬁnal attack effects (e.g., leaked ﬁles). Finally, we include
all traversed nodes and edges along the paths as the recovered
attack story, and compare the result with the ground-truth of
each attack in Table 3 (Column 4 and 5).
Table 5 (ﬁrst row) presents the results of graph-traversal
baseline in identifying attack events. The baseline yields
100% recall (i.e., recovers all attack events) yet it results
in an average precision of 17.82%. The main reason for the
low precision is the well-known dependency explosion prob-
lem [11,31] that introduces a signiﬁcant amount of non-attack
events as false provenance when traversing the causal graph.
For example, the backward and forward analysis can identify
a long-lived process that forks many other system processes as
attack, and this can add a large number of false attack depen-
dencies. In our experiments, we found that many recovered
attacks include the process entity services.exe and its cor-
responding events, where services.exe forks every service
in the Windows system. ATLAS does not rely on traversing
the graph to identify attack events and yields a signiﬁcantly
higher precision without sacriﬁcing the recall.
Non-optimized Causal Graph. Table 5 (second row)
presents the attack investigation results of ATLAS with
and without graph optimization. We observe that the non-
optimized causal graph reduces the precision, recall and F-1
score by 12.30%, 58.34% and 43.52%, respectively. This is be-
cause the graph optimization removes redundant or unrelated
events from the huge volume of logs that do not contribute
semantics and temporal relationship to model different attack
sequences, and prevents model overﬁtting. Overall, the graph
optimization process helps ATLAS extract shorter attack/non-
attack sequences and improves the model generalization.
Table 6: Average time (hh:mm:ss) to train the model and
investigate individual attacks.
Phase
Training
Investigation
Graph
construction
Sequences
processing
Model
learning/inference
0:04:11
0:26:12
0:00:04
0:28:26
0:00:01
Total
0:58:49
0:04:16
Oversampling-only Model. The process of oversampling
attack sequences balances the limited number of attack se-
quences with the vast number of non-attack sequences. If
the oversampling was not used, then the imbalanced dataset
biases the sequence-model towards the more common non-
attack sequences, which yields high non-attack prediction
scores for all sequences. To evaluate the beneﬁt of under-
sampling non-attack sequences, we compare ATLAS with an
oversampling-only baseline. Table 5 (third row) shows the
oversampling-only model reduces the precision, recall and
F1-score by 2.03%, 20.25% and 12.07% respectively. This is
because, without undersampling, non-attack sequences tend
to bring more ampliﬁed noise data to the classiﬁer. Instead,
our similarity-based undersampling approach helps reduce
such noisy data while retaining the key patterns in sequences.
One-hot Encoding. We compare ATLAS with a simpliﬁed
baseline by replacing the word embedding with one-hot-
encoding to show the effectiveness of using word embeddings
in attack investigation. One-hot-encoding is mainly used to
convert the categorical variables to numerical vectors that
could be inputted to the ML algorithms [40]. Table 5 (fourth
row) presents results of ATLAS’s word embedding with the
one-hot-encoding, which reduces precision, recall and F1-
score by 0.28%, 19.14% and 10.69% respectively. The main
reason is that one-hot-encoding ignores the semantic relations
between different words. In contrast, the word embedding
helps better to differentiate those ﬁne-grained behaviors be-
tween attack and non-attack sequences.
Support Vector Machine (SVM). To evaluate the effective-
ness of the LSTM classiﬁer, we compare it with the SVM [46],
an alternative simpler classiﬁer which is widely used for bi-
nary classiﬁcation tasks. In addition to SVM, we also ex-
perimented with the Random Forest classiﬁer [23], which
gives less accurate classiﬁcation results than SVM. We have
evaluated the SVM classiﬁer using the same training data for
each attack. We used a grid search to tune the parameters to
improve classiﬁer accuracy, a linear kernel with C=1.0 and
gamma=“auto”. The SVM reduces the precision, recall and
F1-score by 12.76%, 9.47%, and 11.14% respectively (see
Table 5 (ﬁfth row)). The main limitation of SVM is that it
is unable to model the temporal relations among different
entities of a sequence, one of the critical features that reﬂects
the attack patterns.
6.4 Performance Overhead
ATLAS is trained on attack and non-attack sequences ofﬂine;
thus, it only introduces overhead on the order of seconds at in-
USENIX Association
30th USENIX Security Symposium    3017
Figure 9: Recovered sequences and causal graph of the “Pony campaign” attack (M-5).
ference time to identify sequences as attack or non-attack. We
note that forensics tools often rely on a system or application-
level instrumentation for inference [21, 27], which often re-
quires more time compared to ATLAS. We evaluated ATLAS’s
performance at model training and attack identiﬁcation phases.
Table 6 presents the time used for graph construction, se-
quences processing and model learning and inference. ATLAS
takes on average four minutes to process each 24-hour audit
logs to construct the causal graph with an average size of
676.5 MB audit logs. Further, the model training phase takes
an average of 26 minutes to construct all attack and non-attack