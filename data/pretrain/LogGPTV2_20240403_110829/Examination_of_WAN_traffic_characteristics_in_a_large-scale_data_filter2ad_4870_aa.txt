title:Examination of WAN traffic characteristics in a large-scale data
center network
author:Zhaohua Wang and
Zhenyu Li and
Guangming Liu and
Yunfei Chen and
Qinghua Wu and
Gang Cheng
Examination of WAN Traffic Characteristics in a Large-scale
Data Center Network
Zhaohua Wang† ‡, Zhenyu Li† ‡ ∗, Guangming Liu§, Yunfei Chen§, Qinghua Wu† ‡ ∗, Gang Cheng§
†ICT, Chinese Academy of Sciences ‡University of Chinese Academy of Sciences §Baidu ∗Purple Mountain Laboratories
{wangzhaohua,zyli,wuqinghua}@ict.ac.cn,{liuguangming,chenyunfei,chenggang06}@baidu.com
ABSTRACT
Large cloud service providers have built an increasing number of
geo-distributed data centers (DCs) connected by WAN to host their
diverse services. While we have seen a large body of work on traffic
engineering of WAN, the WAN traffic characteristics of production
DC networks remain not well understood. In this paper, we report
on the network traffic observed in Baidu’s DC network (DCN) that
consists of tens of geo-distributed DCs. Baidu hosts both traditional
services like Web and Computing, as well as emerging services, such
as Analytics, AI, and Map. We analyze WAN traffic characteristics
in Baidu’s DCN from the perspectives of traffic demands, traffic
communication among DCs, and traffic characteristics of diverse
services. Specifically, we focus on the disparity that might exist
among different types of services. We also discuss the implications
of our findings for WAN traffic engineering, fabric design, and
service deployment.
CCS CONCEPTS
• Networks → Network performance evaluation; Network
measurement;
KEYWORDS
DC-WAN, traffic pattern, traffic locality, traffic stability
ACM Reference Format:
Zhaohua Wang† ‡, Zhenyu Li† ‡ ∗, Guangming Liu§, Yunfei Chen§, Qinghua
Wu† ‡ ∗, Gang Cheng§. 2021. Examination of WAN Traffic Characteristics
in a Large-scale Data Center Network. In ACM Internet Measurement Con-
ference (IMC ’21), November 2–4, 2021, Virtual Event, USA. ACM, New York,
NY, USA, 14 pages. https://doi.org/10.1145/3487552.3487860
1 INTRODUCTION
Large cloud service providers use tens of geographically distributed
data centers (DCs) that are interconnected by wide-area network
(WAN) to host their diverse services. Services are replicated across
these DCs to process users’ requests locally for better QoE (Quality-
of-Experience). As such, traffic of bulk transfers flows among DCs
for data synchronization or backup. For example, search engines
synchronize the indexes between DCs; cloud storage services backup
user data in multiple DCs for reliability. These bulk transfers may
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
IMC ’21, November 2–4, 2021, Virtual Event, USA
© 2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-9129-0/21/11...$15.00
https://doi.org/10.1145/3487552.3487860
1
have deadlines but are not delay sensitive [17, 19, 23]. WAN also
carries delay sensitive traffic [14]. For instance, the front-end web
search servers in a DC may need to communicate with the ads
service that is located in other DCs to consolidate the response;
distributed machine learning may need to aggregate the gradients
from many DCs when the raw data are not allowed to move to
other regions due to local regulations [15]. Delay sensitive traffic is
marked as high-priority traffic demand, while the bulk transfers for
data synchronization are low-priority traffic demands.
DC-WAN is an expensive resource, but providers have to balance
between utilization and availability [7]. In the past few years, we
have seen several solutions aiming at achieving a better balance
by using software defined networking constructs [14, 16, 17] or
fine-grained policy enforcement [22]. As any traffic engineering
solution, the effectiveness of these approach depends on the traffic
patterns. For instance, both SWAN [14] and BwE [22] assume the
good predictability of the high-priority interactive traffic. Although
real inter-DC traffic traces were used for evaluation in some of
these studies, the WAN traffic patterns of large-scale production
DCs remain not well understood.
Besides, apart from traditional web service and Hadoop service,
modern DCs carry many new services. AI and big data analytic
services, which are very common and important for large service
providers, now often leverage servers in geo-distributed DCs for
location-based services and distributed deep learning [15, 18, 26].
For instance, map and the relevant location-based recommendation
rely on geo-distributed front-end servers for user-facing requests,
which may interact with servers in other DCs for real-time road
traffic information and target ads. These emerging services may
exhibit new characteristics and change the overall WAN traffic
patterns. An examination of these services from the perspective of
traffic characteristics will benefit the design and improvement of
traffic engineering in DCN.
Indeed, many DCN designs are motivated by measurement re-
sults. The design of VL2 [11] is driven by the tremendous volatility
in the traffic among servers in a DC; the traffic mix pattern and
the switch buffer behavior motivate the design of DCTCP [2]; the
predictability of traffic at short time-scales motivates the design of
MicroTE [6]. There are also some measurement studies that report
the nature of traffic in DCN. Kandula et al. described the charac-
teristics of traffic in an operational distributed query processing
cluster using socket-level logs [20]. Benson et al. examined the flow
characteristics, traffic locality and link utilization of several DCs [4],
and observed the high locality of traffic within individual racks in
cloud DCs. Roy et al. extended the previous studies that are mostly
about a single DC provider and examined the traffic patterns in the
Facebook’s data center [27]; they found different characteristics
of traffic than previous measurements. The distinctions in traffic
IMC ’21, November 2–4, 2021, Virtual Event, USA
Zhaohua Wang, Zhenyu Li, Guangming Liu, Yunfei Chen, Qinghua Wu, Gang Cheng
patterns of different DC providers indeed call for more reports of
other DCs. It is also worth noting that all these studies focus on
intra DC traffic. Chen et al. [8] studied the inter-DC traffic in Yahoo!,
but the scale is very small (only 5 DCs) and the application mix is
much simpler than cloud WAN application mixes.
This paper measures Baidu’s DCN, with a special focus on the
traffic that flows across DCs and clusters. Baidu uses tens of geo-
distributed DCs serving millions of users each day, where each
DC contains multiple clusters to organize servers through a set of
racks. It is one of the largest web service providers, and in recent
years, it has launched AI and auto-driving services [3]. Large-scale
geo-distributed DCs with a complex service mix make Baidu’s DCN
one of the good examples for the examination of traffic patterns of
modern DCs.
Specifically, motivated by challenges encountered by network
designers and operators, we study traffic characteristics along three
dimensions:
1) Examining traffic demands. The effective design of the DC-WAN
resource allocation depends heavily on the traffic demands. We
thus first examine the DC-level traffic locality and link utilization.
2) Characterising traffic communication. Higher utilization of links
carrying WAN traffic motivates a further analysis of traffic commu-
nication between DCs, with a special focus on the stability.
3) Analyzing traffic of services. Service migration and service-level
WAN bandwidth allocation require a deep understanding of the
traffic characteristics at service level. To this end, we analyze service
interaction and stability from the traffic perspective.
To this end, we develop and deploy a Netflow collector that runs
in both core switches and data center switches in all Baidu’s DCs.
We also utilize SNMP statistics from these data center switches. We
make the following key observations from our study:
• Despite that services are highly replicated in many DCs, about
20% of high-priority traffic that leaves clusters still flows across
DCs over WAN. This percentage, however, varies across service
categories and over time of a day; the emerging services (AI,
Analytics and Map) deviate significantly from the traditional
Web and Computing services.
• The links that carry WAN traffic experience higher utilization
than those in DCs, and their loads are well balanced thanks to
ECMP. Besides, WAN traffic and DC traffic leaving from clusters
in individual DCs show a high temporal correlation in terms of
their incremental value, suggesting a separation of two types
of traffic (WAN traffic and DC traffic) on two kinds of switches
(as opposed to using one type of data center switches in [28])
to avoid interference.
• Although communications are prevalent among DCs, a small
portion (8.5%) of DC pairs contribute 80% of high-priority traffic;
these heavy hitters are also persistent over time. The traffic
communication within DCs is also imbalanced: 17% of rack
pairs generate 80% of the traffic.
• The aggregated high-priority traffic over WAN and the high-
priority traffic exchanges among DCs remain stable over time,
leading to good predictability of overall traffic demands. Intra-
DC traffic exchanged among clusters, however, is variable; the
design of fabrics need to adapt to this volatility in traffic sched-
uling [11].
• Our analysis reveals different interaction patterns among ser-
vices (from the perspectives of WAN traffic): traditional Web
and Computing services heavily interact with each other, imply-
ing a close bind between them; Analytics, AI, Map and Security
services, on the other hand, distribute their traffic to others more
evenly, implying their fundamental contributions for other ser-
vices.
• We see a great disparity of the stability of high-priority WAN
traffic among services. The stability is partially impacted by the
service interaction pattern. Existing traffic estimation methods
when applied for service-level WAN traffic prediction may per-
form poorly, especially for those services whose traffic stability
does not persist for a long time. These observations call for
more accurate estimation methods for WAN traffic engineering
at the service level.
We further discuss the implications of the above findings for
WAN traffic engineering, service migration and placement, network
fabric design for DCs, and switch configuration. We are also aware
that as with any large-scale empirical study, our results are subject
to the limitation of considering only one DC provider. While these
observations indeed need to be reexamined further to confirm their
generalisation, they do provide us a deeper understanding of the
traffic and service characteristics in modern DCs (especially for the
DC-WAN traffic). We hope that our findings can shed light on DC
interconnect design, traffic engineering in DC-WAN and service
placement in DCs.
The rest of this paper is organized as follows: Section 2 provides
a brief description of Baidu’s DCN and introduces the measurement
methodology. We then analyze the traffic patterns across DCs and
clusters with respect to traffic demands (Section 3) and traffic com-
munication (Section 4). At last, we give insight into the WAN traffic
characteristics in view of services (Section 5). Finally, we introduce
related works in Section 6 and conclude this paper in Section 7.
2 BACKGROUND & DATA
This section begins with a brief description of Baidu’s data center
(DC) network, followed by details of the measurement methodology
and the data we used in this paper.
2.1 Baidu’s Data Center Network
Baidu’s DC network (DCN) hosts various large-scale services; it
is built on an infrastructure of DCs connected through high band-
width (Tbps) wide area networks (WANs). As shown in Figure 1, the
network consists of multiple DCs connected to the WAN via core
switches, which form a full-meshed core network at the overlay
layer. Inside a DC, tens of clusters are connected by links of Tbps
through DC switches that are responsible for traffic inside the DC.
The traffic that goes out of a DC flows through xDC (cross-DC)
switches to the core switches. Each cluster either employs a typi-
cal 4-post structure or a Spine-Leaf Clos design. Server machines
are organized into racks and connected to a top-of-rack switch
(ToR switch). In the 4-post structure, racks are connected through
cluster switches, which in turn communicate with each other via
2
Examination of WAN Traffic Characteristics in a Large-scale Data Center Network
IMC ’21, November 2–4, 2021, Virtual Event, USA
Figure 2: Netflow data collection architecture.
2.2.1 Netflow data. Cisco’s Netflow service provides network
administrators with access to summarized IP flow records within
their networks [9]. Figure 2 shows the process of Netflow data
collection, where Netflow data were from switches in Figure 1
across Baidu’s entire DCN. Specially, we collected Netflow data
from core switches for inter-DC traffic analysis, and Netflow data
from DC switches for inter-cluster (but intra-DC) traffic analysis.
The active timeout for NetFlow on all switches is set to 1 minute,
i.e. a Netflow record is exported every 1 minute for long-lived flows.
Each flow records the aggregated flow information obtained from
the sampled packet headers with 1:1024 sampling rate; a log con-
tains the source and destination IP addresses, transport-layer port
numbers and IP protocol. These collected flow data along with other
metadata such as collection machines’ IP addresses and capture
time are first processed by the Netflow decoders, which convert each
log into a CSV or JSON object3. These parsed data are then streamed
to Netflow integrators using a distributed subscribing and streaming
system. Netflow integrators aggregate the traffic flow data at one
minute interval and further annotate it with additional attribution
information such as the cluster, DC, service identifications and QoS
information (indicating the priority of the flow) corresponding to
each flow log by querying other data sources. The service informa-
tion is identified via querying a directory that keeps the mapping
between IP addresses and port numbers to services. Netflow inte-
grators then feed data into Apache Doris, a fast MPP database for
big data analytics [10] and Baidu CFS, a cloud file system built in
Baidu for data storage and offline analysis. Netflow decoders and
Netflow integrators are deployed locally in DCs for processing data
collected from individual DCs, while the data analytics and storage
systems are centralized deployed for processing globally collected
flow data.
In aggregation, over 10 TB raw Netflow data from core switches
and 10 TB from DC switches are generated every day. Note that,
during the collection of the data used in this paper, we did not
notice any abnormality of our Netflow data collection system.
2.2.2 SNMP data. In order to investigate the link utilization of
cluster-to-DC, cluster-to-xDC, and xDC-to-core links, we also col-
lected SNMP data from the interfaces of DC switches and xDC
switches in multiple DCs that host considerably traffic volume.
Every 30 seconds, the SNMP manager requests traffic statistics
from DC switches and xDC switches. The collected SNMP data
is streamed into time series tables and Apache Doris in Baidu for
analysis and storage. We note the possible measurement inaccuracy
caused by SNMP data collection, e.g. SNMP packet loss or delay. As
3Those records that fail to be parsed due to format issues are discarded. The percentage
of failed records is around 0.00001%.
Figure 1: Datacenter topology.
DC switches. In the Spine-Leaf Clos structure, racks are connected
through leaf switches; racks in the same pod are served by the same
set of leaf switches. Leaf switches are then full-meshed connected
with spine switches for inter-pod traffic. A particular set of leaf
switches are dedicated to intra-DC traffic, as such they connect to
DC switches in the DC; another set of leaf switches connect to xDC
switches for inter-DC traffic. Overall, Baidu’s DCN is similar to oth-
ers (e.g. Facebook, Microsoft) [11, 27] that were previously reported
in literature from the topology perspective. Note that WAN that
connects DCs is an expensive resource.
From the service hosting perspective, however, some differences
are notable. First, Baidu’s DCN hosts many services that were not re-
ported in other DC networks, despite the dominance of web service.
These services include the emerging distributed AI and location
based services (e.g. Baidu Map). We will detail major services later
in this section. Second, Baidu’s DCN allows any service to be run
on any server. This flexibility leads to the fact that, while a phys-
ical server in Baidu’s DCs only hosts one specific service, a rack
may host many types of services; this is different from Facebook’s
DCN [27] where a rack deploys the same service.
2.2 Data Collection Methodology
This paper focuses on the traffic that crosses DCs and clusters, with
an emphasis on the impact of new types of services. That said, we
do not capture the micro view of fine-grained flow characteristics