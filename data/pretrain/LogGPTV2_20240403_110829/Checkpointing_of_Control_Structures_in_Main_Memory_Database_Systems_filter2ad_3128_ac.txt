±.0018
0.102
±.0002
100
0.101
±.0007
0.102
±.0009
0.104
±.0015
0.102
±.0001
Table 3: Performance Overhead of Checkpointing per
freq
(1/s)
0.75
1.25
1.75
2.25
Transaction [s]
read_per(%)
0
0.008
(7.9%)
0.013
(12.7%)
0.020
(19.2%)
0.028
(27.4%)
20
0.001
(1.0%)
0.002
(2.0%)
0.011
(10.6%)
0.019
(18.6%)
40
0.001
(1.0%)
0.001
(1.0%)
0.003
(2.9%)
0.011
(10.8%)
60
0.001
(1.0%)
0.002
(2.0%)
0
(0%)
0.001
(1.0%)
80
0
(0%)
0
(0%)
0.001
(1.0%)
0
(0%)
100
N/A
N/A
N/A
N/A
From the results, one can see that as transaction arrival rate
increases, the performance overhead and, hence, the transac-
tion time increases. This is because when there are more re-
quests, ARMORs take more time to process each individual
request. However, the frequency increase does not signifi-
cantly degrade the performance; the overhead ranges from
1ms to 28ms for 80% and 0% (all transactions are write) read
operations, respectively. If more than half of the mutex ac-
quisitions are read-only, the performance overhead is very
small. Note that the variation of the run time without check-
pointing (last column in Table 2) can dominate the perform-
ance overhead (columns 5 and 6 in Table 2). In real applica-
tions, more than 50% of mutex acquisitions are for read-only
operations; the measurement data indicate that under such
workloads the overhead due to checkpointing is negligible.
6.2
Performance of ARMOR-based Incremental
Checkpointing in Error-Recovery Scenarios
The database used in the test consists of five db files. Each
file contains 100 tables, and each table contains two thousand
200-byte records. So the total size of the user database is
200MB, a typical size for the database the target system
processes in practice. Three clients are used in the error re-
covery scenario: (i) testsc, which updates the table records
one after another without acquiring a file table mutex, (ii)
testsc_ftmutex, which repeatedly acquires a file table mutex,
updates the table records, and releases the mutex, and (iii)
ftmutextest, which gets a file table mutex and emulates a
crash while still holding the mutex. Table 4 presents meas-
urements comparing the performances of both major recovery
and ARMOR-based incremental checkpoint recovery. The
time listed in Table 4 represents the recovery time, i.e., the
time from the crash of the failed client (ftmutextest) to the
first successful acquisition of a file table mutex by the wait-
ing client (testsc_ftmutex).
Table 4: Performance of Major Recovery and ARMOR-
based Incremental Checkpointing
Major Recovery [s]
ARMOR-based
Checkpointing [s]
Trial
Expr 1
(testsc_ftmutex
+ tmutextest)
Expr 2
(testsc +
testsc_ftmutex
+ ftmutextest)
Expr 1
(testsc_ftmutex
+ ftmutextest)
Expr 2
(testsc +
testsc_ftmutex
+ ftmutextest)
1
2
3
13
12
11
31
25
29
2
3
4
3
6
5
so
is
Four experiments (each is performed for three trials) are con-
ducted with different clients and recovery policies. The re-
sults reported in Table 4 indicate:
•  Major recovery can cause significant system downtime (11
to 31 seconds in our experiments). The downtime depends
on how much data is loaded into memory when major re-
covery occurs. (Testsc_ftmutex updates a small fraction of
table
small. When
testsc+testsc_ftmutex is used, since testsc updates a whole
table, the loaded data is much larger, and recovery time is
greater.)
recovery
data,
time
•  ARMOR-based incremental checkpointing eliminates or
significantly reduces the downtime due to the client
crashes: (i) the crash of a client does not impact other proc-
esses as long as they do not acquire the same mutex as the
terminated client, (ii) an overhead of 2 to 6 seconds (in our
measurements) is encountered by any process that attempts
to acquire the same mutex as the terminated client, and (iii)
recovery using checkpointing does not depend on the
amount of loaded data, as there is no need to reload data.
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 05:36:17 UTC from IEEE Xplore.  Restrictions apply. 
Availability. Availability of the database is estimated based
on the data on recovery time, assuming different frequencies
of crashes that require major recovery. (We consider database
availability using Experiment 2 as an example.) The meas-
ured average recovery time for major recovery and check-
pointing-based recovery are 28.3s and 4.7s, respectively.
Table 5 shows the system’s availability for various error fre-
quencies. One can see that under an error rate of one per
week, checkpointing-based recovery provides about
five
nines of availability, which is one nine of improvement com-
pared with the major-recovery-based solution.
Table 5: Availability (Expr. 2)
Error Frequency
1/hour
1/day
1/week
1/month
1/year
Solutions
Major recovery
99.21
99.87
99.97
99.995
99.995
99.9992
99.9989
99.9998
99.99991
99.99999
Related Work
Checkpointing-
based recovery
7
A number of checkpoint techniques have been proposed to
ensure the durability of MMDBs. In Hagmann’s fuzzy check-
pointing [6], the checkpoint is taken while the transaction is
in progress. An improved variant of fuzzy checkpointing is
proposed in [11] and [12]. Non-fuzzy checkpointing algo-
rithms are introduced in and [7], [9], and [14].
Levy and Silberschatz [10] design an incremental checkpoint-
ing scheme that decouples transaction processing and check-
pointing. The propagator component observes the log at all
times and propagates the updates of the primary copy in
memory to the backup copy on disk. While these traditional
techniques rely on control structures to checkpoint user data,
we address checkpointing of control structures themselves.
Sullivan and Stonebraker [16] investigate the use of hardware
memory protection to prevent erroneous (due to addressing
errors) writes to the data structures. In [4], Bohannon, et al.,
achieve such protection by computing a codeword over a
region in the data structures. Upon a write, the data region
and the associated codeword are updated. A wild write results
in an incorrect codeword, which triggers recovery of the cor-
rupted data region. These schemes protect the critical control
structures against erroneous writes. Our checkpointing algo-
rithms defend against client crashes and data inconsistency,
which is a different failure model. Another technique that
addresses this type of failure is process duplication. For ex-
ample, Tandem’s process-pair mechanism [1] provides a
spare process for the primary one. The primary executes
transactions and sends checkpoint messages to the spare. If
the primary fails, the spare reconstructs the consistent state
from the checkpoint messages. The idea of lightweight, re-
coverable virtual memory in the context of providing transac-
tional guarantees to applications is explored in [15]. A Rio
Vista system for building high-performance recoverable
memory for transactional systems is proposed in [13].
8
This paper presents ARMOR-based,
transparent, and per-
formance-efficient recovery of control structures in a com-
mercial MMDB. The proposed generic solution allows elimi-
nating or significantly reducing cases requiring major recov-
Conclusions
ery and, hence, significantly improves availability. The solu-
tion can be easily adapted to provide system-wide detection
and recovery. Performance measurements and availability
estimates show that the proposed ARMOR-based checkpoint-
ing scheme enhances database availability while keeping per-
formance overhead quite small (less than 2% in a typical
workload of real applications).
Acknowledgments
This work was supported in part by NSF grant ACI-0121658
ITR/AP. We thank F. Baker for careful reading of this manuscript.
References
[1]
J. Bartlett. A nonstop kernel. Proc. Eighth Symposium on Op-
erating Systems Principles, 1981.
[2] P. Bohannon, et al. The architecture of the Dali main memory
storage manager. Journal of Multimedia Tools and Applica-
tions, 4(2), 1997.
[3] P. Bohannon, et al. Distributed multi-level recovery in main
memory databases. Proc. 4th Int. Conf. on Parallel and Dis-
tributed Information Systems, 1996.
[4] P. Bohannon, et al. Detection and recovery techniques for data-
base corruption. IEEE Trans. on Knowledge and Data Engi-
neering, 15(5): 2003.
[5] H. Garcia-Molina and K. Salem. Main memory database sys-
tems: An overview. IEEE Trans. on Knowledge and Data En-
gineering, 4(6), 1992.
[6] R. Hagmann. A crash recovery scheme for a memory-resident
[7]
database system. IEEE Trans. on Computers, 35(9), 1986.
J. Huang and L. Gruenwald. An update-frequency-valid-
interval partition checkpoint technique for realtime main mem-
ory databases. Workshop on Real-Time Databases, 1996.
[8] Z. Kalbarczyk, et al. Chameleon: A software infrastructure for
adaptive fault tolerance. IEEE Trans. on Parallel and Distrib-
uted Systems, 10(6), 1999.
[9] T. Lehman and M. Carey. A recovery algorithm for a high-
performance, memory-resident database system. Proc. ACM-
SIGMOD Int. Conf. on Management of Data, 1987.
[10] E. Levy, and A. Silberschatz. Incremental recovery in main
memory database systems. IEEE Trans. on Knowledge and
Data Engineering, 4 (6), 1992.
[11] X. Li, et al. Checkpointing and recovery in partitioned main
memory databases. Proc. IASTED/ISMM Int. Conf. on Intelli-
gent Information Management Systems, 1995.
[12] J. Lin and M. Dunham. A performance study of dynamic seg-
mented fuzzy checkpointing in memory resident databases. TR
96-CSE-14, Southern Methodist University, Dallas (TX), 1996.
[13] D. Lowell and P. Chen. Free transactions with Rio Vista. Proc.
16th ACM Symposium on Operating Systems Principles, 1997.
[14] K. Salem and H. Garcia-Molina. Checkpointing memory-
resident databases. Proc. Int. Conf. on Data Engineering, 1989.
recoverable virtual
memory. Proc. 14th ACM Symposium on Operating Systems
Principles, 1993.
[15] M. Satyanarayanan, et al. Lightweight
[16] M. Sullivan and M. Stonebraker. Using write-protected data
structures to improve software fault tolerance in highly avail-
able database management systems. Proc. Int. Conf. on Very
Large Databases, 1991.
[17] K. Whisnant, et al. An experimental evaluation of the REE
SIFT environment for spaceborne applications. Proc. Int. Conf.
on Dependable Systems and Networks, 2002.
[18] K. Whisnant, Z. Kalbarczyk, and R. K. Iyer, A system model
for dynamically reconfigurable software. IBM Systems Journal,
42(1), 2003.
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 05:36:17 UTC from IEEE Xplore.  Restrictions apply.