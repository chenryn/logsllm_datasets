### Performance Overhead of Checkpointing per Frequency (1/s)

| ±.0018 | 0.102 | ±.0002 | 100 | 0.101 | ±.0007 | 0.102 | ±.0009 | 0.104 | ±.0015 | 0.102 | ±.0001 |
|--------|-------|--------|-----|-------|--------|-------|--------|-------|--------|-------|--------|
| 0.75   | 1.25  | 1.75   | 2.25|       |        |       |        |       |        |       |        |

**Table 3: Performance Overhead of Checkpointing per Frequency (1/s)**

| Freq (1/s) | 0.75  | 1.25  | 1.75  | 2.25  |
|------------|-------|-------|-------|-------|
| 0%         | 0.008 (7.9%) | 0.013 (12.7%) | 0.020 (19.2%) | 0.028 (27.4%) |
| 20%        | 0.001 (1.0%) | 0.002 (2.0%) | 0.011 (10.6%) | 0.019 (18.6%) |
| 40%        | 0.001 (1.0%) | 0.001 (1.0%) | 0.003 (2.9%) | 0.011 (10.8%) |
| 60%        | 0.001 (1.0%) | 0.002 (2.0%) | 0 (0%) | 0.001 (1.0%) |
| 80%        | 0 (0%) | 0 (0%) | 0.001 (1.0%) | 0 (0%) |
| 100%       | N/A   | N/A   | N/A   | N/A   |

From the results, it is evident that as the transaction arrival rate increases, the performance overhead and, consequently, the transaction time also increase. This is because a higher number of requests requires more time for ARMOR to process each individual request. However, the increase in frequency does not significantly degrade performance; the overhead ranges from 1ms to 28ms for 80% and 0% (all transactions are write) read operations, respectively. If more than half of the mutex acquisitions are read-only, the performance overhead is minimal. It should be noted that the variation in run time without checkpointing (last column in Table 2) can dominate the performance overhead (columns 5 and 6 in Table 2). In real applications, more than 50% of mutex acquisitions are for read-only operations; the measurement data indicate that under such workloads, the overhead due to checkpointing is negligible.

### Performance of ARMOR-based Incremental Checkpointing in Error-Recovery Scenarios

The database used in the test consists of five database files. Each file contains 100 tables, and each table contains two thousand 200-byte records. Therefore, the total size of the user database is 200MB, which is typical for the databases processed by the target system. Three clients were used in the error recovery scenario:
- **testsc**: Updates table records one after another without acquiring a file table mutex.
- **testsc_ftmutex**: Repeatedly acquires a file table mutex, updates the table records, and releases the mutex.
- **ftmutextest**: Acquires a file table mutex and emulates a crash while still holding the mutex.

**Table 4: Performance of Major Recovery and ARMOR-based Incremental Checkpointing**

| Trial | Major Recovery [s] | ARMOR-based Checkpointing [s] |
|-------|--------------------|--------------------------------|
| Expr 1 (testsc_ftmutex + ftmutextest) | 13, 12, 11 | 2, 3, 4 |
| Expr 2 (testsc + testsc_ftmutex + ftmutextest) | 31, 25, 29 | 3, 6, 5 |

Four experiments (each performed three times) were conducted with different clients and recovery policies. The results reported in Table 4 indicate:
- **Major recovery** can cause significant system downtime (ranging from 11 to 31 seconds in our experiments). The downtime depends on the amount of data loaded into memory when major recovery occurs. For example, `testsc_ftmutex` updates a small fraction of the table, resulting in a shorter recovery time. When `testsc + testsc_ftmutex` is used, since `testsc` updates the entire table, the loaded data is much larger, leading to a greater recovery time.
- **ARMOR-based incremental checkpointing** eliminates or significantly reduces the downtime due to client crashes:
  - The crash of a client does not impact other processes as long as they do not acquire the same mutex as the terminated client.
  - An overhead of 2 to 6 seconds (in our measurements) is encountered by any process that attempts to acquire the same mutex as the terminated client.
  - Recovery using checkpointing does not depend on the amount of loaded data, as there is no need to reload data.

### Availability

Database availability was estimated based on recovery time data, assuming different frequencies of crashes requiring major recovery. The measured average recovery time for major recovery and checkpointing-based recovery were 28.3s and 4.7s, respectively.

**Table 5: Availability (Expr. 2)**

| Error Frequency | Major Recovery | Checkpointing-based Recovery |
|-----------------|----------------|-------------------------------|
| 1/hour          | 99.21%         | 99.9992%                      |
| 1/day           | 99.87%         | 99.9989%                      |
| 1/week          | 99.97%         | 99.9998%                      |
| 1/month         | 99.995%        | 99.99991%                     |
| 1/year          | 99.995%        | 99.99999%                     |

Under an error rate of one per week, checkpointing-based recovery provides about five nines of availability, which is one nine of improvement compared to major-recovery-based solutions.

### Related Work

A variety of checkpoint techniques have been proposed to ensure the durability of main memory databases (MMDBs). Hagmann’s fuzzy checkpointing [6] takes checkpoints while transactions are in progress. Improved variants of fuzzy checkpointing are proposed in [11] and [12]. Non-fuzzy checkpointing algorithms are introduced in [7], [9], and [14].

Levy and Silberschatz [10] designed an incremental checkpointing scheme that decouples transaction processing and checkpointing. The propagator component continuously observes the log and propagates updates from the primary copy in memory to the backup copy on disk. While these traditional techniques rely on control structures to checkpoint user data, we address the checkpointing of control structures themselves.

Sullivan and Stonebraker [16] investigated the use of hardware memory protection to prevent erroneous writes to data structures. Bohannon et al. [4] achieve such protection by computing a codeword over a region in the data structures. Upon a write, the data region and the associated codeword are updated. A wild write results in an incorrect codeword, triggering recovery of the corrupted data region. These schemes protect critical control structures against erroneous writes. Our checkpointing algorithms defend against client crashes and data inconsistency, which is a different failure model.

Another technique that addresses this type of failure is process duplication. For example, Tandem’s process-pair mechanism [1] provides a spare process for the primary one. The primary executes transactions and sends checkpoint messages to the spare. If the primary fails, the spare reconstructs the consistent state from the checkpoint messages. The idea of lightweight, recoverable virtual memory in the context of providing transactional guarantees to applications is explored in [15]. A Rio Vista system for building high-performance recoverable memory for transactional systems is proposed in [13].

### Conclusions

This paper presents ARMOR-based, transparent, and performance-efficient recovery of control structures in a commercial MMDB. The proposed generic solution allows for the elimination or significant reduction of cases requiring major recovery, thereby significantly improving availability. The solution can be easily adapted to provide system-wide detection and recovery. Performance measurements and availability estimates show that the proposed ARMOR-based checkpointing scheme enhances database availability while keeping performance overhead quite small (less than 2% in a typical workload of real applications).

### Acknowledgments

This work was supported in part by NSF grant ACI-0121658 ITR/AP. We thank F. Baker for careful reading of this manuscript.

### References

[1] J. Bartlett. A nonstop kernel. Proc. Eighth Symposium on Operating Systems Principles, 1981.

[2] P. Bohannon, et al. The architecture of the Dali main memory storage manager. Journal of Multimedia Tools and Applications, 4(2), 1997.

[3] P. Bohannon, et al. Distributed multi-level recovery in main memory databases. Proc. 4th Int. Conf. on Parallel and Distributed Information Systems, 1996.

[4] P. Bohannon, et al. Detection and recovery techniques for database corruption. IEEE Trans. on Knowledge and Data Engineering, 15(5): 2003.

[5] H. Garcia-Molina and K. Salem. Main memory database systems: An overview. IEEE Trans. on Knowledge and Data Engineering, 4(6), 1992.

[6] R. Hagmann. A crash recovery scheme for a memory-resident database system. IEEE Trans. on Computers, 35(9), 1986.

[7] J. Huang and L. Gruenwald. An update-frequency-valid-interval partition checkpoint technique for real-time main memory databases. Workshop on Real-Time Databases, 1996.

[8] Z. Kalbarczyk, et al. Chameleon: A software infrastructure for adaptive fault tolerance. IEEE Trans. on Parallel and Distributed Systems, 10(6), 1999.

[9] T. Lehman and M. Carey. A recovery algorithm for a high-performance, memory-resident database system. Proc. ACM-SIGMOD Int. Conf. on Management of Data, 1987.

[10] E. Levy and A. Silberschatz. Incremental recovery in main memory database systems. IEEE Trans. on Knowledge and Data Engineering, 4(6), 1992.

[11] X. Li, et al. Checkpointing and recovery in partitioned main memory databases. Proc. IASTED/ISMM Int. Conf. on Intelligent Information Management Systems, 1995.

[12] J. Lin and M. Dunham. A performance study of dynamic segmented fuzzy checkpointing in memory resident databases. TR 96-CSE-14, Southern Methodist University, Dallas (TX), 1996.

[13] D. Lowell and P. Chen. Free transactions with Rio Vista. Proc. 16th ACM Symposium on Operating Systems Principles, 1997.

[14] K. Salem and H. Garcia-Molina. Checkpointing memory-resident databases. Proc. Int. Conf. on Data Engineering, 1989.

[15] M. Satyanarayanan, et al. Lightweight, recoverable virtual memory. Proc. 14th ACM Symposium on Operating Systems Principles, 1993.

[16] M. Sullivan and M. Stonebraker. Using write-protected data structures to improve software fault tolerance in highly available database management systems. Proc. Int. Conf. on Very Large Databases, 1991.

[17] K. Whisnant, et al. An experimental evaluation of the REE SIFT environment for spaceborne applications. Proc. Int. Conf. on Dependable Systems and Networks, 2002.

[18] K. Whisnant, Z. Kalbarczyk, and R. K. Iyer. A system model for dynamically reconfigurable software. IBM Systems Journal, 42(1), 2003.

**Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04)**
**0-7695-2052-9/04 $ 20.00 © 2004 IEEE**
**Authorized licensed use limited to: Tsinghua University. Downloaded on March 20, 2021, at 05:36:17 UTC from IEEE Xplore. Restrictions apply.**