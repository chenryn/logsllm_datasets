c
i
f
f
e
o
C
n
o
i
t
l
a
e
r
r
o
C
o
t
u
A
1.1
0.9
0.7
0.5
0.3
0.1
−0.1
−0.3
−0.5
2
4
Log−1
Log−2
8
10
Figure 2: Autocorrelation coeﬃcient trend
6
Lags
ρ[k] =
E{X0Xk} − E{X0}E{Xk}
σX0 σXk
,
(1)
where E{.} represents the expectation operation, σXk is the
standard deviation of the random variable at time lag k and
the stochastic process X realizes the log entries, thus Xn is
the log entry at index n. The value of the autocorrelation
function lies in the range [−1, 1], where ρ[k] = 1 means
perfect correlation at lag k (which is obviously true for k =
0) and ρ[k] = 0 means no correlation at lag k.
Figure 2 shows the autocorrelation function plotted against
the log entries represented by lags. For both the logs, a cer-
tain level of temporal dependence can be easily observed at
small lags. This correlation decays in time and eventually
drops down to a negligible value. Temporal dependence is
present for two reasons. First, meters respond to the smart
collector requests in a short period of time. Second, regular
requests and reports are seen thus justifying the homogene-
ity.
Since log entries are correlated at small lags, they can be
modeled using markov chains. Moreover, it is well-known
that a decaying temporal dependence structure can be ac-
curately modeled using markov chains [18]. Therefore, the
concern here is to identify the order of markov chain model
that should be used to accurately model the log entries. To
determine the markovian order, we conduct analysis on dif-
ferent markov chain orders. The order can be identiﬁed by
noting the probabilities at diﬀerent markov chain orders.
If the probability is low at a given order, the next log en-
try can be predicted with high accuracy. First, we investi-
gate the probability distribution of log entries at diﬀerent
markov chain orders and then use the conditional entropy
based measure proposed in [18].
Let the log entry tuple at discrete time instance n rep-
resents the realization of a random variable derived from a
stochastic process Xn. This process is a markov chain if it
satisﬁes the markov property, which is deﬁned as:
Pr {Xn = j |Xn−1 = i, Xn−2 = in−2, . . . , X0 = i0}
= Pr {Xn = j |Xn−1 = i } = pj|i.
(2)
In other words, the probability of choosing a next state is
only dependent on the current state of the markov chain.
In the present context, we can deﬁne a markov chain
model Xn for an entry by treating each unique log entry tu-
ple individually and assigning them to non-overlapping bins.
Here we assign each unique tuple to each bin. Therefore, the
number of bins will be dependent on the number of unique
log entry tuples. Each bin then represents a state of the
markov chain, while the set of all bin indices ψ is its state
space. Based on this state representation, we can deﬁne a
1-st order markov chain, X (1)
n , in which each bin represents
a state of the random process. Probability of each state i
can be calculated by counting the number of times state i
occurred and dividing it by the total occurrences of all the
states in the markov chain model Xn. Similarly, an l-th or-
der markov chain, X (l)
n , can be deﬁned in which each state
is an l-tuple hi0, i1, . . . , il−1i representing the values taken
by the random process in the last l time instances i.e., l log
entries in our case. In this case the occurrences of l-tuple
together will be counted. This will increase the size of state
space ψ since diﬀerent combinations of l-tuple are possible.
Figure 3 shows the state probabilities calculated for log
entries for diﬀerent markov chain orders. We calculated it
for up to 4-th order markov chain as shown in Figure 3. It
can be clearly observed that as the order of markov chain
increases, i.e., ﬁrst order to fourth order, total number of
states ψ increases, which is the x-axis in Figure 3(a) to (d).
However, it gives us an interesting insight that as the order
increases, very few states have higher probability of occur-
rence and rest of the states’ probabilities drop to a negli-
gible value. Therefore, using a higher order markov chain,
we can actually identify the most likely states which will
be observed in the logs. Thus it increases the predictability
and likeliness of those states. Here, selection of appropriate
markov chain order is important since a higher order yields
higher number of possible states and requires more mem-
ory. To explore it further, we use the conditional entropy
based measure [18] for the log entries for diﬀerent markov
chain orders. This measure will tell us that at what order
of markov chain we have most of the information about the
next time instance i.e., log entry.
Conditional entropy, H(B|A), of two discrete random vari-
ables A and B characterizes the information remaining in B
when A is already known. Phrased diﬀerently, conditional
entropy is ‘information about B not given by A’. If A and
B are highly correlated, most of the information about B
is communicated through A and H(B|A) is small. On the
other hand, if pA and pB (which respectively represent the
probability mass functions of A and B) are quite diﬀerent
then H(B|A) assumes a high value, which means that most
of the information about B is not given by A. In the limit-
ing cases, H(B|A) = 0 when A = B, while H(B|A) = H(B)
when A and B are independent.
The transition probability matrix of the 1-st order markov
chain P (1) can be computed by counting the number of times
the state i is followed by state j. The resulting |ψ(1)| his-
tograms can be normalized to obtain the state-wise transi-
tion probability mass functions as the rows of P (1).
We can ﬁnd the conditional probability of the 1-st order
markov chain as:
H (1) = − X
i∈ψ(1)
π(1)
i X
j∈ψ(1)
j|i log2 (cid:16)p(1)
p(1)
j|i(cid:17) ,
(3)
i
where π(1)
is the average probability of being in state i which
is computed by counting the total number of times each state
is visited and then normalizing the frequency histogram.
The measure H (1) deﬁnes how much average information
is remaining in log entry Xn when it is calculated using log
entry Xn−1. If the entry is not highly correlated with entries
before Xn−1, H (1) will be relatively large implying that in-
formation about Xn not provided by Xn−1 is high. In such a
454y
t
i
l
i
b
a
b
o
r
P
0.08
0.07
0.06
0.05
0.04
0.03
0.02
0.01
0
0
0.014
0.012
0.01
0.008
0.006
0.004
0.002
y
t
i
l
b
a
b
o
r
P
−3
x 10
5
4.5
4
3.5
3
2.5
2
1.5
1
0.5
y
t
i
l
i
b
a
b
o
r
P
−3
x 10
2
y
t
i
l
i
b
a
b
o
r
P
1.8
1.6
1.4
1.2
1
10
20
30
40
50
State
0
0
100
200
State
300
400
0
0
200
400
State
600
800
0.8
0
200
400
600
State
800
1000
(a) First Order
(b) Second Order
(c) Third Order
(d) Fourth Order
Figure 3: State Probabilities for Higher Order Markov Chain
y
p
o
r
t
n
E
l
a
n
o
i
t
i
d
n
o
C
1
0.8
0.6
0.4
0.2
0
Log−1
Log−2
2
4
6
8
10
Figure 4: Conditional Entropy trend over Markov
Chain orders
Markov Chain Order
case, we use higher l-th order markov chain, X (l)
n as deﬁned
previously, in which each state is an l-tuple hi0, i1, . . . , il−1i
representing the values taken by the random process in the
last l time instances. Aggregating multiple time instances in
a single state allows us to satisfy the markov property, and
hence a transition probability matrix P (l) can be computed
by counting the number of times hi0, i1, . . . , il−1i is followed
by state hi1, . . . , il−1, ili. The conditional entropy of X (l)
n de-
ﬁned on ψ(l) can then be computed using the same method
as (3). It is easy to observe that H (1) ≥ H (2) ≥ . . . ≥ H (l),
as each older entry can either be independent of, or provide
some information about the present entry. The number of
previous entries required to accurately predict the next en-
try can then be determined by plotting H (l) as the function
of the markov chain order, l = 1, 2, . . .. The order at which
the conditional entropy saturates deﬁnes the total number
of previous entries which have conveyed as much informa-
tion of the present entry as possible. It can be clearly seen in
Figure 4 that log entries exhibit a decaying trend over higher
order markov chain. It can be seen that it exhibits an expo-
nential decay trend until the 4-th order, which was expected
since very few states had higher probability in Figure 3(d)
at the same order. It is clear that most of the information
about the next instance is already given at the fourth or-
der since conditional entropy drops to a negligible value i.e.,
≤ 0.2. Therefore, we model using the fourth order since it
gives enough information and increases predictability. How-
ever, we also show the results for third order markov chain
which exhibits a little low accuracy but improves scalability
since it exhibits less number of states. In later section, we
show that higher order markov chain exhibits better accu-
racy but it increases the number of states as compared to
the lower order. Therefore, the choice of markov chain or-
der is a trade-oﬀ between scalability (number of states) and
accuracy. This is discussed in detail in the later sections,
however, we build a model using the fourth order markov
chain.
5. MODEL
Since our model is built on the logs of smart collectors, we
ﬁrst look at the format of a log entry. It can be represented
as:
t, sid, did, sz, ty
where t represents the time stamp at which the event was
observed for which the entry is logged. sid and did refers to
the source and destination, respectively, of the communica-
tion observed. It can either be the meter or smart collector.
Size of the communication is denoted by sz. The type of
communication is deﬁned by ty. Therefore, in this model
we use the above information. We can encode the state of
the network with the following characteristic function:
σ : sid ∧ did ∧ sz ∧ ty → {true, f alse}
(4)
The function σ encodes the state of the network by evalu-
ating to true whenever the parameters used as input to the
function correspond to the log entry in the smart collector.
If the AMI observes 5 diﬀerent log entries, then exactly 5
diﬀerent assignments to σ function will result to true. Since
each smart collector is independent, we learn the markov
model for each smart collector separately.
A Labeled Markov Chain (LMC) is a tuple
M = {Q, Σ, π, τ, L}, where Q is a ﬁnite set of states, π is an
initial probability distribution τ is the transition probabil-
ity function and L is a labeling function. Atomic proposi-
tions AP are assigned to states by a labeling function using
Σ = 2AP . Each state was assigned a unique label derived
from σ, which is used to deﬁne the state. A probability
distribution for sequence of states can then be deﬁned us-
ing markov chain. LMC M with alphabet Σ induces the
probability distribution P π
M over Σw through labeling of the
states.
Set of states Q is strongly connected if for each state pair
(qi, qj ) there exist a path q0, q1, . . . , qn such that qh ∈ Q for
0 ≤ h ≤ n, τ (qh, qh+1) > 0, q0 = qi, and qn = qh. Therefore,
if Q is strongly connected, then M is said to be strongly
connected. A distribution πs
M is a stationary distribution
for M if it satisﬁes
(q) = X
πs
´q∈Q
πs(´q)τ (´q, q)
Since we are interested in keeping the history of previously
visited states, we focus on the probability suﬃx automata
(PSA). A PSA is an extended LMC with a labeling function