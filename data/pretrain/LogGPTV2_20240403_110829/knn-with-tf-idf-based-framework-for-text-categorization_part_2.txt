       (4)
where a ij is the weight of term i in document j, N is the number of documents in the collection, tfij is the term
frequency of term i in document j and dfi is the document frequency of term i in the collection. This formula
implemented in the framework, during the testing has shown good results when we used the documents with equal
length. In order to obtain better results with documents of different length, we used a modified equation as shown
[7]:
      (5)
Shown below is the experimental pseudo code for the method implemented in the framework for determining the
weight matrix.
Pseudo code 1:
for(i=0 i<numnberOfUniqueWordsi++)
for(j=0 j<numberOfDocuments j++)
tfidf = fij _ log(numberOfDocuments = ni)
for(s=0 s<numnberOfUniqueWords s++)
fijTemp = number_of_occurrences_ofword_S_in_the_document_J
tfidfTemp = fijTemp * log(numberOfDocuments / dfi)
summTfidf += (tfidfTemp)2
Bruno Trstenjak et al. / Procedia Engineering 69 ( 2014 ) 1356 – 1364 1361
end
A[i,j] = tfidf/summTfidf
end
end
In order to limit matrix dimension and optimize the framework work, it is possible to include preprocessor and
remove useless words, which often repeat in documents and do not contain any useful information.
2.7. Text document KNN classification process
At the beginning of the classification it is necessary to select the document that will be carried out for
classification and include it in the belong category [9]. For the selected document its weight value also must be
determined by TF-IDF method, as well as for all other documents. Classification process writes data of the selected
document in a weight matrix to its end, as shown in Fig. 4. Writing down all data in the same weight matrix has
resulted with optimization and reducing the total time of calculation.
In the next step of the process it is necessary to determine K value. K value of the KNN algorithm is a factor
which indicates a required number of documents from the collection which is closest to the selected document. The
classification process determinates the vectors distance between the documents by using the following equation
[12]:
    (6)
where d(x,y) is the distance between two documents, N is the number unique words in the documents collection, a rx
is a weight of the term r in document x, a ry is a weight of the term r in document y. Pseudo code 2 shows
implementation Euclidean distance calculation, and Fig. 4 shows documents in Euclidian space for factor K=3.
Smaller Euclidean distance between the documents indicates their higher similarity. Distance 0 means that the
documents are complete equal.
Pseudo code 2:
for(i=0 i<numberOfDocumentsi++)
for(r=0 r<numnberOfUniqueWordsi++)
d[i] += (A[r,i] - A[r,( numberOfDocument-1)])2
end
d[i] = Sqrt( d[i] )
end
Fig. 4. Euclidian space for K=3.
1362 Bruno Trstenjak et al. / Procedia Engineering 69 ( 2014 ) 1356 – 1364
3. Framework evaluation
For the purposes of this paper, this section presents the results of framework testing over the sets of 500 online
documents from different categories in the learning phase and classification. Evaluation of Framework was
performed in several test phases:
 test speed calculation of TF-IDF methods with optimization
 test the quality classification
 test speed text classification
 test classification sensitivity according to categories of documents
Calculation of TF-IDF values in weight matrix has shown as the most demanding part of the implemented
algorithm. For this reason it was necessary to optimize exactly that part of the framework code. Optimization TF-
IDF was performed using LINQ class in C # language, in order to calculate the frequency of appearance of a
particular word in the document. The optimized method showed six times better results than the classic method.
Thus, the optimized code is used in other test phases.
The results of testing text classification quality are shown in Fig. 5. The tests used the documents of different
lengths, different categories from online sources with 500 documents [8].
Testing frameworks was performed in online environment, using different combinations and different amounts of
documents [15]. The results indicate the successful implementation of algorithm in the framework whose quality of
classification slightly decreases with the increasing number of documents. Additional testing and analysis showed
much better obtained results if working with documents which belong to the same category, as we predicted. Fig. 6
shows the results of classification depending on the selected documents category. The best results were achieved
with the documents from “Sport” category. The main reason for good results of classification in this category is that
the documents have not been textually demanding. The documents did not contain a large number of different words
which reduced the impact of unusable words and character.
Table 1 shows the speed measurement of classification based on the amount of data. These results directly indicate
performance of the implemented algorithm. The developed framework has certain limitations on his work. The time
required for data processing exponentially increased with the increasing amounts of data. One of the solutions can
be implementation of parallel processing in the framework. It would allow speed up calculation of weight matrix
which is the most demanding part in algorithm.
Fig. 5. The results of testing text classification quality.
Bruno Trstenjak et al. / Procedia Engineering 69 ( 2014 ) 1356 – 1364 1363
Fig. 6. The results of testing text classification quality by categories.
The last test was focused on measuring the quality of classification depending on the documents category.
It was previously indicated that the quality of classification depends on the preprocessing documents, removing
undesired characters and words that have no significant information. Table 2 shows the results of successful
classification over the documents from various categories. The results show that the worst classification was
performed in the category Daily News. The analysis of document contents in this category showed that documents
contained a lot of "unusable words", the words that are often repeated and do not have important weight but have
adverse impact on KNN classification.
Table 1. Speed classification. Table 2. Category classification.
The amount of data (kB) Speed (ms) Category of documents KNN Classification
4.00 300.00 Sport 0,92%
400.00 3000.00 Politics 0,90%
4000.00 45,000.00 Finance 0,78%
Daily News 0,65%
4. Conclusion
In this paper we present a framework for text classification based on KNN algorithm and the TF-IDF method.
The main motivation for the research was to develop concept of frameworks with emphasis on KNN & TF-IDF
module.
The framework with embedded methods gave good results, confirmed our concept and initial expectations.
Evaluation of framework was performed on several categories of documents in online environment. Tests are
supposed to provide answers about the quality of classification and to determine which factors have an impact on
performance of classification. The framework work was very stable and reliable. During testing the quality of
classification we have achieved good results regardless of the K factor value in the KNN algorithm.
Performed tests have detected a sensitivity of the implemented algorithm. Tests have shown that the embedded
algorithm is sensitive to the type of documents. The analysis of documents contents showed that the amount of
unusable words in documents has a significant impact on the final quality of classification. Because of this, it is
necessary improve the preprocessing of data for achieving better results.
The combination of KNN algorithm and TF-IDF method has been shown as a good choice with minor
modifications in their implementation. The framework provides the ability to upgrade and improve the present
embedded classification algorithm.
1364 Bruno Trstenjak et al. / Procedia Engineering 69 ( 2014 ) 1356 – 1364
References
[1] S.Tan, Neighbor-weighted K-nearest neighbor for unbalanced text corpus, Expert Systems with Applications 28 (2005) 667–671.
[2] G.Salton, C.S.Yang, On the specification of term values in automatic indexing, Journal of Documentation, 29 (1973) 351-372.
[3] W.Zhang, T.Yoshida, A comparative study of TF-IDF, LSI and multi-words for text classification, Expert Systems with Applications 38
(2011) 2758–2765.
[4] H.Han, G.Karypis, V.Kumar, Text Categorization Using Weight Adjusted k-Nearest Neighbor Classification, PAKDD (2001) 53-65.
[5] F.Sebastiani, Machine Learning in Automated Text Categorization, Consiglio Nazionale delle Ricerche, 2002.
[6] H.Jiang, P.Li, X.Hu, S.Wang, An improved method of term weighting for text classification, Intelligent Computing and Intelligent Systems,
2009.
[7] J. T.-Y. Kwok, Automatic Text Categorization Using Support Vector Machine, Proceedings of International Conference on Neural
Information Processing, (1998) 347-351.
[8] M.Miah, Improved k-NN Algorithm for Text Classification, DMIN (2009) 434-440.
[9] Y.Liao, V. Rao Vemuri, Using K-Nearest Neighbor Classifier for Intrusion Detection, Department of Computer Science, University of
California, Davis One Shields Avenue, CA 95616.
[10] L.Wang , X. Zhao, Improved KNN classification algorithms research intext categorization, IEEE, 2012.
[11] M.Lan, C.L.Tan, J.Su, Y.Lu, Supervised and Traditional Term Weighting Methods for Automatic Text Categorization, IEEE Transactions
on Pattern Analysis and Machine Intelligence, VOL. 31, NO. 4, 2009.
[12] K. Mikawa, T. Ishidat, M.Goto, A Proposal of Extended Cosine Measure for Distance Metric Learning in Text Classification, 2011.
[13] l.Wang , X. Li, An improved KNN algorithm for text classification, 2010.
[14] G. Guo, H.Wang, D.Bell, Y. Bi, K. Greer, KNN Model-Based Approach in Classification, (2003) 986 – 996.
[15] G. Forman, An Extensive Empirical Study of Feature Selection Metrics for Text Classification, Journal of Machine Learning Research 3
(2003) 1289-1305.
[16] H. Uguz, A two-stage feature selection method for text categorization by using information gain, principal component analysis and genetic
algorithm, Knowledge-Based Systems 24 (2011) 1024–1032.
[17] K. Masudaa, T.Matsuzakib, J.Tsujiic, Semantic Search based on the Online Integration of NLP Techniques, Procedia - Social and
Behavioral Sciences 27 (2011) 281 – 290.
[18] C.Friedman, T.C.Rindflesch, M. Corn, Natural language processing: State of the art and prospects for significant progress, a workshop
sponsored by the National Library of Medicine, Journal of Biomedical Informatics 46 (2013) 765–773.
[19] Ming-Yang Su, Using clustering to improve the KNN-based classifiers for online anomaly network traffic identification, Journal of
Networkand Computer Applications 34 (2011) 722–730.