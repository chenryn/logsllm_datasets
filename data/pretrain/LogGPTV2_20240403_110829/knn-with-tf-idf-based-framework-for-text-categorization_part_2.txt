### 4. Term Weighting Using TF-IDF

The weight \( a_{ij} \) of term \( i \) in document \( j \) is calculated using the following formula:
\[ a_{ij} = \text{tf}_{ij} \times \log\left(\frac{N}{\text{df}_i}\right) \]
where:
- \( N \) is the total number of documents in the collection,
- \( \text{tf}_{ij} \) is the term frequency of term \( i \) in document \( j \),
- \( \text{df}_i \) is the document frequency of term \( i \) in the collection.

This formula was implemented in the framework and showed good results when used with documents of equal length. To improve performance with documents of varying lengths, we used a modified equation as shown in [7]:
\[ a_{ij} = \text{tf}_{ij} \times \log\left(\frac{N}{\text{df}_i}\right) \]

Below is the experimental pseudo code for the method implemented in the framework to determine the weight matrix:

```plaintext
Pseudo code 1:
for (i = 0; i < numberOfUniqueWords; i++) {
    for (j = 0; j < numberOfDocuments; j++) {
        tfidf = fij * log(numberOfDocuments / dfi);
        summTfidf = 0;
        for (s = 0; s < numberOfUniqueWords; s++) {
            fijTemp = number_of_occurrences_of_word_s_in_document_j;
            tfidfTemp = fijTemp * log(numberOfDocuments / dfi);
            summTfidf += tfidfTemp^2;
        }
        A[i, j] = tfidf / sqrt(summTfidf);
    }
}
```

To limit the matrix dimension and optimize the framework, a preprocessor can be included to remove stop words and other terms that do not contribute meaningful information.

### 2.7. Text Document KNN Classification Process

The classification process begins by selecting a document for classification and assigning it to a category [9]. The weight value for the selected document, as well as for all other documents, must be determined using the TF-IDF method. The data for the selected document are then appended to the weight matrix, as shown in Fig. 4. This approach optimizes the calculation and reduces the total processing time.

Next, the value of \( K \) must be determined. In the KNN algorithm, \( K \) indicates the number of nearest documents from the collection to the selected document. The distance between the documents is calculated using the following equation [12]:
\[ d(x, y) = \sqrt{\sum_{r=1}^{N} (a_{rx} - a_{ry})^2} \]
where:
- \( d(x, y) \) is the distance between two documents,
- \( N \) is the number of unique words in the document collection,
- \( a_{rx} \) is the weight of term \( r \) in document \( x \),
- \( a_{ry} \) is the weight of term \( r \) in document \( y \).

The following pseudo code demonstrates the implementation of the Euclidean distance calculation:

```plaintext
Pseudo code 2:
for (i = 0; i < numberOfDocuments; i++) {
    d[i] = 0;
    for (r = 0; r < numberOfUniqueWords; r++) {
        d[i] += (A[r, i] - A[r, (numberOfDocuments - 1)])^2;
    }
    d[i] = sqrt(d[i]);
}
```

Fig. 4 shows the documents in Euclidean space for \( K = 3 \). Smaller Euclidean distances indicate higher similarity between documents, with a distance of 0 indicating identical documents.

### 3. Framework Evaluation

For this paper, we evaluated the framework using sets of 500 online documents from different categories. The evaluation involved several test phases:
- Calculation speed of TF-IDF methods with optimization,
- Quality of classification,
- Speed of text classification,
- Sensitivity of classification according to document categories.

The most demanding part of the algorithm was the calculation of TF-IDF values in the weight matrix. We optimized this part using LINQ in C#, which improved the performance by six times compared to the classic method. The optimized code was used in subsequent test phases.

The results of the text classification quality tests are shown in Fig. 5. The tests used documents of varying lengths and categories from online sources. The framework's performance was tested in an online environment with different combinations and amounts of documents [15]. The results indicated that the classification quality slightly decreased with an increasing number of documents. However, better results were obtained when working with documents from the same category, as expected. Fig. 6 shows the classification results based on the document category, with the best results achieved in the "Sport" category due to the simplicity of the text.

Table 1 shows the speed measurement of classification based on the amount of data. The framework has certain limitations, particularly in the exponential increase in processing time with larger data sets. One potential solution is to implement parallel processing to speed up the weight matrix calculation.

Fig. 5. Results of testing text classification quality.
Fig. 6. Results of testing text classification quality by categories.

The final test focused on measuring the quality of classification depending on the document category. It was found that the quality of classification depends on the preprocessing of documents, including the removal of undesired characters and words. Table 2 shows the results of successful classification across various categories, with the worst performance in the "Daily News" category due to the presence of many "unusable words."

Table 1. Speed classification.
| Amount of Data (kB) | Speed (ms) |
|---------------------|------------|
| 4.00                | 300.00     |
| 400.00              | 3000.00    |
| 4000.00             | 45000.00   |

Table 2. Category classification.
| Category of Documents | KNN Classification |
|-----------------------|--------------------|
| Sport                 | 0.92%              |
| Politics              | 0.90%              |
| Finance               | 0.78%              |
| Daily News            | 0.65%              |

### 4. Conclusion

In this paper, we presented a framework for text classification based on the KNN algorithm and the TF-IDF method. The main motivation for the research was to develop a framework with a focus on the KNN & TF-IDF module.

The framework provided good results, confirming our concept and initial expectations. The evaluation was performed on several categories of documents in an online environment. The tests aimed to assess the quality of classification and identify factors affecting performance. The framework demonstrated stability and reliability, achieving good classification results regardless of the \( K \) factor in the KNN algorithm.

The tests revealed the sensitivity of the algorithm to the type of documents, with the amount of unusable words significantly impacting the final classification quality. Therefore, improving the preprocessing of data is necessary for better results.

The combination of the KNN algorithm and TF-IDF method proved to be a good choice with minor modifications in their implementation. The framework allows for further upgrades and improvements to the embedded classification algorithm.

### References

[1] S.Tan, Neighbor-weighted K-nearest neighbor for unbalanced text corpus, Expert Systems with Applications 28 (2005) 667–671.
[2] G.Salton, C.S.Yang, On the specification of term values in automatic indexing, Journal of Documentation, 29 (1973) 351-372.
[3] W.Zhang, T.Yoshida, A comparative study of TF-IDF, LSI and multi-words for text classification, Expert Systems with Applications 38 (2011) 2758–2765.
[4] H.Han, G.Karypis, V.Kumar, Text Categorization Using Weight Adjusted k-Nearest Neighbor Classification, PAKDD (2001) 53-65.
[5] F.Sebastiani, Machine Learning in Automated Text Categorization, Consiglio Nazionale delle Ricerche, 2002.
[6] H.Jiang, P.Li, X.Hu, S.Wang, An improved method of term weighting for text classification, Intelligent Computing and Intelligent Systems, 2009.
[7] J. T.-Y. Kwok, Automatic Text Categorization Using Support Vector Machine, Proceedings of International Conference on Neural Information Processing, (1998) 347-351.
[8] M.Miah, Improved k-NN Algorithm for Text Classification, DMIN (2009) 434-440.
[9] Y.Liao, V. Rao Vemuri, Using K-Nearest Neighbor Classifier for Intrusion Detection, Department of Computer Science, University of California, Davis One Shields Avenue, CA 95616.
[10] L.Wang, X. Zhao, Improved KNN classification algorithms research in text categorization, IEEE, 2012.
[11] M.Lan, C.L.Tan, J.Su, Y.Lu, Supervised and Traditional Term Weighting Methods for Automatic Text Categorization, IEEE Transactions on Pattern Analysis and Machine Intelligence, VOL. 31, NO. 4, 2009.
[12] K. Mikawa, T. Ishidat, M.Goto, A Proposal of Extended Cosine Measure for Distance Metric Learning in Text Classification, 2011.
[13] L.Wang, X. Li, An improved KNN algorithm for text classification, 2010.
[14] G. Guo, H.Wang, D.Bell, Y. Bi, K. Greer, KNN Model-Based Approach in Classification, (2003) 986 – 996.
[15] G. Forman, An Extensive Empirical Study of Feature Selection Metrics for Text Classification, Journal of Machine Learning Research 3 (2003) 1289-1305.
[16] H. Uguz, A two-stage feature selection method for text categorization by using information gain, principal component analysis and genetic algorithm, Knowledge-Based Systems 24 (2011) 1024–1032.
[17] K. Masudaa, T.Matsuzakib, J.Tsujiic, Semantic Search based on the Online Integration of NLP Techniques, Procedia - Social and Behavioral Sciences 27 (2011) 281 – 290.
[18] C.Friedman, T.C.Rindflesch, M. Corn, Natural language processing: State of the art and prospects for significant progress, a workshop sponsored by the National Library of Medicine, Journal of Biomedical Informatics 46 (2013) 765–773.
[19] Ming-Yang Su, Using clustering to improve the KNN-based classifiers for online anomaly network traffic identification, Journal of Network and Computer Applications 34 (2011) 722–730.