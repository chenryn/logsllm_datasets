### Evaluation of Intra-Level Isolation Technique: Hilps

#### Introduction
To integrate our technique into the kernel, additional modifications are required. We defined memory regions for the page tables and the inner domainâ€™s code and data by modifying the kernel's linker script. During the early boot-up sequence (before enabling the MMU), kernel page tables are initialized to map these memory regions according to our mapping strategy, as described in Figure 7. Page table regions are mapped in the outer domain with read-only permission, while the inner domain regions are mapped outside the valid address space of the outer domain. The outer domain then transfers control to the inner domain via an Inter-Domain Call (IDC). The inner domain initializes its data structures and creates shadow mappings of the page tables to manage page table operations. It then enables the MMU by configuring the System Control Register (SCTLR) and returns control to the outer domain.

Note that the protection scheme for DMA attacks is not considered. However, this should not significantly impact the performance evaluation, as IDC invocations for DMA protection would only account for a small portion of the overall execution time.

#### Evaluation

In this section, we evaluate our intra-level isolation technique by conducting a case study to measure the performance overhead of the Hilps prototype, as described in Section V. Experiments were performed on the Versatile Express V2M-Juno r1 platform, which features a Cortex-A57 1.15 GHz dual-core processor and a Cortex-A53 650 MHz quad-core processor in a big.LITTLE design, along with 2 GB of DRAM.

##### Experimental Setup
The prototype runs at Exception Level 1 (EL1) by default, allowing it to benefit from the Address Space Identifier (ASID) feature. If the prototype does not run at EL1, it must rely on TLB invalidation to protect the inner domain, as the ASID feature cannot be used. Therefore, we evaluated both variations of the prototype: one using ASID and the other using TLB invalidation. This helps in predicting the performance when our privilege separation technique is integrated into different levels of system software.

##### A. Switching Overhead
The primary source of overhead in our privilege separation mechanism comes from transitions between the inner and outer domains via IDCs. To investigate the overhead imposed by each IDC, we invoked a null IDC, which performs no operation, and measured the elapsed time using the performance monitor supported by AArch64. Given the prevalence of the big.LITTLE feature in recent ARM-based mobile devices, we conducted experiments separately on big and little cores. Each experiment was repeated 100 times, and the average results are reported in Table II.

**Table II: IDC Elapsed Time (in microseconds)**

| Core Type | w/ ASID | w/ TI |
|-----------|---------|-------|
| Big       | 0.81    | 1.28  |
| Little    | 0.44    | 0.73  |

The results show that the IDC is lightweight. Despite the big cores operating at approximately twice the clock speed of the little cores, the IDC using ASID consumes nearly constant time across both core types. However, this consistency is not observed when the IDC invalidates TLB entries, which is attributed to the different TLB structures in big and little cores.

##### B. Micro Benchmarks
To measure the performance overhead on system calls, we used the LMBench test suite. Similar to the IDC measurements, experiments were conducted considering the big.LITTLE feature. Table III reports the results for both versions of the prototype.

**Table III: LMBench Results (in microseconds)**

| Test             | Native (Big) | w/ ASID (Big) | w/ TI (Big) | Overhead (w/ ASID) | Overhead (w/ TI) |
|------------------|--------------|---------------|-------------|--------------------|------------------|
| Null syscall     | 0.44         | 0.81          | 1.28        | 84.09%             | 190.91%          |
| Open/close       | 6.37         | 7.28          | 8.73        | 14.29%             | 37.05%           |
| Stat             | 2.65         | 3.09          | 3.78        | 16.60%             | 42.64%           |
| Sig. handler inst| 0.68         | 1.09          | 1.64        | 60.29%             | 141.18%          |
| Sig. handler ovh | 3.26         | 3.68          | 4.44        | 12.88%             | 36.20%           |
| Pipe latency     | 12.81        | 19.86         | 27.84       | 55.04%             | 117.33%          |
| Page fault       | 1.88         | 2.38          | 3.74        | 26.60%             | 98.94%           |
| Fork+exit        | 148.36       | 182.54        | 237.13      | 23.04%             | 59.83%           |
| Fork+execv       | 163.58       | 195.19        | 257.85      | 19.32%             | 57.63%           |
| Mmap             | 2323.00      | 2786.00       | 3878.00     | 19.93%             | 66.94%           |

**Table IV: LMBench Results (in microseconds) - Little Core**

| Test             | Native (Little) | w/ ASID (Little) | w/ TI (Little) | Overhead (w/ ASID) | Overhead (w/ TI) |
|------------------|-----------------|------------------|----------------|--------------------|------------------|
| Null syscall     | 0.43            | 1.01             | 1.66           | 134.88%            | 286.05%          |
| Open/close       | 12.65           | 13.82            | 16.09          | 9.25%              | 27.19%           |
| Stat             | 5.06            | 5.79             | 6.84           | 14.43%             | 35.18%           |
| Sig. handler inst| 0.91            | 1.49             | 2.19           | 63.74%             | 140.66%          |
| Sig. handler ovh | 5.98            | 6.55             | 7.45           | 9.53%              | 24.58%           |
| Pipe latency     | 26.70           | 40.44            | 50.04          | 51.46%             | 87.42%           |
| Page fault       | 2.81            | 3.73             | 5.48           | 32.74%             | 95.02%           |
| Fork+exit        | 255.05          | 292.61           | 374.27         | 14.73%             | 46.74%           |
| Fork+execv       | 279.70          | 322.22           | 404.57         | 15.20%             | 44.64%           |
| Mmap             | 4654.00         | 5148.00          | 6641.00        | 10.61%             | 42.69%           |

The results indicate that the prototype does not significantly slow down null, open/close, stat, and signal handling system calls, as they do not access sensitive resources managed in the inner domain. Conversely, the prototype degrades the performance of other system calls related to memory management, such as page faults and fork+execv.

##### C. Synthetic Benchmarks
We also evaluated the performance of the Hilps prototype using synthetic benchmarks, including CF-Bench, GeekBench, Quadrant, Smartbench, Vellamo, and Antutu. The results are presented in Table V.

**Table V: Synthetic Benchmark Results**

| Test         | Native (Big) | w/ ASID (Big) | w/ TI (Big) | Overhead (w/ ASID) | Overhead (w/ TI) |
|--------------|--------------|---------------|-------------|--------------------|------------------|
| CF-Bench     | 42243.5      | 36218.1       | 33107.9     | 14.26% (5.00)      | 21.63% (4.37)    |
| GeekBench    | 842.6        | 842.0         | 839.2       | 0.07% (0.54)       | 0.40% (1.04)     |
| Quadrant     | 1891.6       | 1890.6        | 1882.3      | 0.05% (1.11)       | 0.49% (1.59)     |
| Smartbench   | 8137.9       | 8032.8        | 8056.8      | 1.29% (1.88)       | 0.99% (1.41)     |
| Vellamo      | 4863.8       | 4738.4        | 4253.8      | 2.58% (4.60)       | 12.50% (10.58)   |
| Antutu       | 2649.9       | 2434.2        | 2613.4      | 8.14% (9.58)       | 1.38% (2.00)     |
| Productivity | 2895.1       | 2892.2        | 2807.2      | 0.10% (2.10)       | 3.04% (5.00)     |
| Gaming       | 1350.9       | 1341.7        | 1341.8      | 0.68% (0.65)       | 0.68% (0.65)     |
| Browser      | 41033.9      | 40861.1       | 40307.9     | 0.42% (1.92)       | 1.77% (3.00)     |

**Table VI: Synthetic Benchmark Results - Little Core**

| Test         | Native (Little) | w/ ASID (Little) | w/ TI (Little) | Overhead (w/ ASID) | Overhead (w/ TI) |
|--------------|-----------------|------------------|----------------|--------------------|------------------|
| CF-Bench     | 42243.5         | 36218.1          | 33107.9        | 14.26% (5.00)      | 21.63% (4.37)    |
| GeekBench    | 842.6           | 842.0            | 839.2          | 0.07% (0.54)       | 0.40% (1.04)     |
| Quadrant     | 1891.6          | 1890.6           | 1882.3         | 0.05% (1.11)       | 0.49% (1.59)     |
| Smartbench   | 8137.9          | 8032.8           | 8056.8         | 1.29% (1.88)       | 0.99% (1.41)     |
| Vellamo      | 4863.8          | 4738.4           | 4253.8         | 2.58% (4.60)       | 12.50% (10.58)   |
| Antutu       | 2649.9          | 2434.2           | 2613.4         | 8.14% (9.58)       | 1.38% (2.00)     |
| Productivity | 2895.1          | 2892.2           | 2807.2         | 0.10% (2.10)       | 3.04% (5.00)     |
| Gaming       | 1350.9          | 1341.7           | 1341.8         | 0.68% (0.65)       | 0.68% (0.65)     |
| Browser      | 41033.9         | 40861.1          | 40307.9        | 0.42% (1.92)       | 1.77% (3.00)     |

The synthetic benchmarks show that the prototype introduces a modest overhead, with the most significant impact observed in memory-intensive tasks. The use of ASID generally results in lower overhead compared to TLB invalidation.