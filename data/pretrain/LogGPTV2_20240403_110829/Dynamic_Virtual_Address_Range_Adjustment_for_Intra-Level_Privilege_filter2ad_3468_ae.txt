browser
metal
Native
42243.5
842.6
1891.6
8137.9
4382.3
2597.7
2895.1
1350.9
41033.9
Hilps
Overhead
w/ ASID
41111.8
844.4
1880.5
8092.7
4291.8
2552.4
2893.2
1352.6
40964.3
w/TI
36770.4
840.0
1886.0
8139.2
4494.5
2563.4
2862.6
1348.9
40298.7
w/ ASID (σ)
2.68 % (11.00)
-0.21 % ( 0.93)
0.59 % ( 0.98)
0.56 % ( 1.48)
2.07 % (10.80)
1.74 % (13.45)
0.07 % ( 2.03)
-0.13 % ( 0.47)
0.17 % ( 2.22)
w/ TI (σ)
12.96 % (13.58)
0.31 % ( 0.97)
0.30 % ( 1.64)
-0.02 % ( 1.47)
-2.56 % ( 4.26)
1.32 % ( 9.29)
1.12 % ( 2.63)
0.15 % ( 0.57)
1.79 % ( 1.83)
To retroﬁt our technique into the kernel, additional modi-
ﬁcations are needed. We deﬁned memory regions for the page
tables and the inner domain’s code and data by modifying
the linker script of the kernel. In the early boot-up sequence
(before enabling the MMU), kernel page tables are initialized
to map these memory regions following our mapping strategy
described in Figure 7. Page table regions are mapped in the
outer domain with read-only permission. The inner domain
regions are mapped out of the valid address space of the outer
domain. Subsequently, the outer domain transfers control to
the inner domain by invoking an IDC. The inner domain
then initializes its data structures and creates the shadow
mappings of the page tables to be able to perform page table
managements instead of the outer domain. Next, it enables the
MMU by conﬁguring SCTLR and returns to the outer domain.
Note that the protection scheme for DMA attacks is not
considered. However, we do not believe that this will damage
the accuracy of the performance evaluation in the next section,
as IDC invocations for DMA protection would only account
for a small portion in the whole execution time.
VI. EVALUATION
In this section, we evaluate our intra-level isolation tech-
nique by performing a case study for measuring the per-
formance overhead of the prototype of Hilps, described in
Section V. Experiments have been conducted on the versatile
express V2M-Juno r1 platform [6], which ships with Cortex-
A57 1.15 GHz dual-core processor and Cortex-A53 650 MHz
quad-core processor in a big.LITTLE design and 2 GB of
DRAM.
Experimental Group. In the case study, as the prototype
runs at EL1 by default, our privilege separation technique can
beneﬁt from the ASID feature. On the other hand, in the case
that the prototype does not run at EL1, the technique needs to
rely on TLB invalidation to protect the inner domain, as it can
no longer use the ASID feature. In our experiments, therefore,
we evaluate both variations of the prototype that operate with
ASID and TLB invalidation to help reasonable performance
prediction when our privilege separation technique is incorpo-
rated into different levels of system software.
A. Switching Overhead
The major portion of overhead of our privilege separation
mechanisms is from the transitions between the inner and outer
domains by IDCs. To investigate the overhead imposed by
each IDC, we invoked a null IDC, which does not perform
any operation, and measured the elapsed time using the per-
formance monitor supported by AArch64. In addition, as the
big.LITTLE is a prevalent feature in recent ARM-based mobile
devices, we performed experiments separately in big and little
cores. The experiment was repeated 100 times and the average
results are reported in Table II.
The result shows the lightweightness of the IDC. Even
though big cores operate with about two times faster clock
speed, we can see that the IDC using ASID consumes near
constant time regardless of types of cores. However, the same
tendency is not found when the IDC invalidates TLB entries.
This result is attributed to the different TLB structure of big
and little cores.
B. Micro Benchmarks
As the prototype is targetting the normal OS, it could
impose a performance penalty on system calls. To measure
such overhead, we performed experiments using the LMBench
test suite. Similar to the case of measuring RTC of the IDC,
experiments are done in consideration of the big.LITTLE
feature. Table III reports the results for two versions of the
prototype together. It shows that the prototype does not slow
down null, open/close, stat and signal handling system calls
because they do not touch sensitive resources that are managed
in the inner domain. Contrarily, the prototype degrades the
performance of other system calls that are related to memory
management. For example, to handle a page fault (by copy-
on-write or demand paging), the outer domain has to modify
12
TABLE V.
LMBENCH RESULTS WITH A SECURITY APPLICATION
Little core
Hilps
Overhead
Test
null syscall
open/close
stat
sig. handler inst
sig. handler ovh
pipe latency
page fault
fork+exit
fork+execv
mmap
Native
0.44
6.37
2.65
0.68
3.26
12.81
1.88
148.36
163.58
2323.00
Big core
Hilps
Overhead
w/ ASID
0.81
7.28
3.09
1.09
3.68
19.86
2.38
182.54
195.19
2786.00
w/ TI
1.28
8.73
3.78
1.64
4.44
27.84
3.74
237.13
257.85
3878.00
w/ ASID
84.09 %
14.29 %
16.60 %
60.29 %
12.88 %
55.04 %
26.60 %
23.04 %
19.32 %
19.93 %
w/ TI
190.91 %
37.05 %
42.64 %
141.18 %
36.20 %
117.33 %
98.94 %
59.83 %
57.63 %
66.94 %
Native
0.43
12.65
5.06
0.91
5.98
26.70
2.81
255.05
279.70
4654.00
w/ ASID
1.01
13.82
5.79
1.49
6.55
40.44
3.73
292.61
322.22
5148.00
w/ TI
1.66
16.09
6.84
2.19
7.45
50.04
5.48
374.27
404.57
6641.00
w/ ASID
134.88 %
9.25 %
14.43 %
63.74 %
9.53 %
51.46 %
32.74 %
14.73 %
15.20 %
10.61 %
w/ TI
286.05 %
27.19 %
35.18 %
140.66 %
24.58 %
87.42 %
95.02 %
46.74 %
44.64 %
42.69 %
TABLE VI.
SYNTHETIC BENCHMARK RESULTS WITH A SECURITY APPLICATION
Test
CF-Bench
GeekBench
Quadrant
Smartbench
Vellamo
Antutu
single core
multi core
productivity
gaming
browser
metal
Native
42243.5
842.6
1891.6
8137.9
4863.8
2649.9
2895.1
1350.9
41033.9
Hilps
Overhead
w/ ASID
36218.1
842.0
1890.6
8032.8
4738.4
2434.2
2892.2
1341.7
40861.1
w/TI
33107.9
839.2
1882.3
8056.8
4253.8
2613.4
2807.2
1341.8
40307.9
w/ ASID (σ)
14.26 % ( 5.00)
0.07 % ( 0.54)
0.05 % ( 1.11)
1.29 % ( 1.88)
2.58 % ( 4.60)
8.14 % ( 9.58)
0.10 % ( 2.10)
0.68 % ( 0.65)
0.42 % ( 1.92)
w/ TI (σ)
21.63 % (4.37)
0.40 % ( 1.04)
0.49 % ( 1.59)