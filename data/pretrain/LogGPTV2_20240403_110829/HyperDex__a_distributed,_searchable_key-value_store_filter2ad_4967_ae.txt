mentation is unavailable, Cassandra uses a similar design
and its performance illustrates how a multi-ring-based sys-
tem would perform. Arpeggio [13] provides search over mul-
tiple attributes by enumerating and creating an index of all
(cid:1) ﬁxed-size subsets of attributes using a Chord ring. Both
(cid:0)k
x
of these approaches insert redundant pointers into rings with-
out concern for consistency.
Space-ﬁlling curves A common approach to providing
multi-attribute search uses space-ﬁlling curves to partition
multi-dimensional data across the storage nodes. This ap-
proach uses the space ﬁlling curve to map the multi-di-
mensional data into a single dimension, which then enables
the use of traditional peer-to-peer techniques for perform-
ing searches. SCRAP [21], Squid [52] and ZNet [53] are
examples of this approach with each node responsible for
data in a contiguous range of values. Similarly, MAAN [10]
performs the same mapping, but uses uniform locality pre-
serving hashing. Space-ﬁlling curves do not scale well when
the dimensionality is high, because a single search query
may be partitioned into many one-dimensional ranges of
considerably varying size. Furthermore, unlike in Hyper-
Dex, fully-qualiﬁed searches, where values for all attributes
are speciﬁed, may involve contacting more than one node in
space-ﬁlling curve-based systems.
NoSQL Storage A new class of scalable storage systems,
collectively dubbed “NoSQL”, have recently emerged with
the deﬁning characteristic that they depart from the tradi-
tional architecture and SQL interface of relational databases.
It is common practice for NoSQL systems to make explicit
tradeoﬀs with respect to desirable properties. For instance,
many NoSQL systems explicitly sacriﬁce consistency – even
in the common case without failures – to achieve availabil-
ity under extreme failure scenarios. NoSQL systems in-
clude document databases [16, 37] that oﬀer a schema-free
data model, in-memory cache solutions [36, 43, 48] that ac-
celerate application performance, and graph databases [38]
which model interconnected elements and key-value stores
which oﬀer predictable performance. Still, some systems do
not ﬁt neatly into these categories. For example, Yahoo!’s
PNUTS [14] system supports the traditional SQL selection
and projection, functions but does not support joins.
HyperDex explores a new point in the NoSQL design space.
The rich HyperDex API provides qualitatively new function-
ality not oﬀered by other NoSQL systems, HyperDex sets a
new bar for future NoSQL systems by combining strong con-
sistency properties with fault-tolerance guarantees, a rich
API and high performance.
Key-Value Stores Modern key-value stores have roots in
work on Distributed Data Structures [23,36] and distributed
hash tables [29,47,50,55]. Most open source key-value stores
draw heavily from the ring-based architecture of Dynamo [19]
and the tablet-based architecture of BigTable [11]. For in-
stance, Voldemort [45] and Riak [49] are heavily inﬂuenced
by Dynamo’s design. Other systems like HBase [25] and Hy-
perTable [28] are open source implementations of BigTable.
Cassandra [32] is unique in that it is inﬂuenced by BigTable’s
API and Dynamo’s ring structure. Like HyperDex, all of
these systems are designed to run in a datacenter environ-
ment on many machines.
Recent work on key-value stores largely focuses on improv-
ing performance by exploiting underlying hardware or ma-
nipulating consistency guarantees for a performance advan-
tage. Fawn KV [4] builds a key-value store on underpowered
hardware to improve the throughput-to-power-draw ratio.
SILT [33] eliminates read ampliﬁcation to maximize read
bandwidth in a datastore backed by solid-state disk. RAM-
Cloud [40] stores data in RAM and utilizes fast network con-
nections to rapidly restore failed replicas. TSSL [54] utilizes
a multi-tier storage hierarchy to exploit cache-oblivious algo-
rithms in the storage layer. Masstree [35] uses concatenated
B+ trees to service millions of queries per second. COPS [34]
provides a high-performance geo-replicated key-value store
that provides causal+ consistency. Other systems [12, 19]
trade consistency for other desirable properties, such as per-
formance. Spanner [18] uses Paxos in the wide area to pro-
vide strong consistency. Spinnaker [46] uses Paxos to build
a strongly consistent store that performs nearly as well as
those that are only eventually consistent [19, 44, 56].
Each of these existing systems improves the performance,
availability or consistency of key value stores while retain-
ing the same basic structure: a hash table. In HyperDex,
we take a complementary approach which expands the key-
value interface to support, among other operations, search
over secondary attributes.
8. DISCUSSION
Surprisingly, the open-source release of the HyperDex sys-
tem [20] uncovered various misunderstandings surrounding
the CAP Theorem [22]. The popular CAP refrain (“C, A, P:
pick any two”) causes the subtleties in the deﬁnitions of C, A
and P to be lost, even on otherwise savvy developers. There
exists no conﬂict between claims in our paper and the CAP
Theorem. The failure model used in the CAP Theorem is
unconstrained; the system can be subject to partitions and
node failures that aﬀect any number of servers and network
links. No system, including HyperDex, can simultaneously
oﬀer consistency and availability guarantees using such weak
assumptions. HyperDex makes a stronger assumption that
limits failures to aﬀect at most a threshold of servers and is
thus able to provide seemingly impossible guarantees.
9. CONCLUSIONS
This paper described HyperDex, a second-generation No-
SQL storage system that combines strong consistency guar-
antees with high availability in the presence of failures and
partitions aﬀecting up to a threshold of servers. In addition,
HyperDex provides an eﬃcient search primitive for retriev-
ing objects through their secondary attributes. It achieves
this extended functionality through hyperspace hashing, in
which multi-attribute objects are deterministically mapped
to coordinates in a low dimension Euclidean space. This
mapping leads to eﬃcient implementations for key-based re-
trieval, partially-speciﬁed searches and range-queries. Hy-
perDex’s novel replication protocol enables the system to
provide strong consistency without sacriﬁcing performance.
Industry-standard benchmarks show that the system is prac-
tical and eﬃcient.
The recent trend toward NoSQL data stores has been fu-
eled by scalability and performance concerns at the cost of
functionality. HyperDex bridges this gap by providing ad-
ditional functionality without sacriﬁcing scalability or per-
formance.
10. ACKNOWLEDGMENTS
We would like to thank Pawel Loj for his contributions
on cluster stop/restart, Deniz Altınb¨uken for her ConCo-
ord Paxos implementation, and members of the HyperDex
open source community for their code contributions, and
the VICCI team for granting us time on their cluster. This
material is based upon work supported by National Sci-
ence Foundation under Grants No. CNS-1111698 and CCF-
0424422 and by the National Science and Engineering Re-
search Council of Canada.
11. REFERENCES
[1] M. K. Aguilera, W. M. Golab, and M. A. Shah. A
Practical Scalable Distributed B-Tree. In PVLDB,
1(1), 2008.
[2] D. Altınb¨uken and E. G. Sirer. Commodifying
Replicated State Machines with OpenReplica.
Computing and Information Science, Cornell
University, Technical Report 1813-29009, 2012.
[3] S. Anand. http:
//www.infoq.com/presentations/NoSQL-Netflix/.
[4] D. G. Andersen, J. Franklin, M. Kaminsky, A.
Phanishayee, L. Tan, and V. Vasudevan. FAWN: A
Fast Array of Wimpy Nodes. In Proc. of SOSP, Big
Sky, MT, Oct. 2009.
[5] R. Bayer. The Universal B-Tree for Multidimensional
Indexing: General Concepts. In Proc. of WWCA,
Tsukuba, Japan, Mar. 1997.
[6] R. E. Bellman. Dynamic Programming. Princeton
University Press, 1957.
[7] J. L. Bentley. Multidimensional Binary Search Trees
Used for Associative Searching. In CACM, 18(9), 1975.
[8] A. R. Bharambe, M. Agrawal, and S. Seshan. Mercury:
Supporting Scalable Multi-Attribute Range Queries.
In Proc. of SIGCOMM, Portland, OR, Aug. 2004.
[9] M. Burrows. The Chubby Lock Service for
Loosely-Coupled Distributed Systems. In Proc. of
OSDI, Seattle, WA, Nov. 2006.
[10] M. Cai, M. R. Frank, J. Chen, and P. A. Szekely.
MAAN: A Multi-Attribute Addressable Network for
Grid Information Services. In Proc. of GRID
Workshop, Phoenix, AZ, Nov. 2003.
[11] F. Chang, J. Dean, S. Ghemawat, W. C. Hsieh, D. A.
Wallach, M. Burrows, T. Chandra, A. Fikes, and R.
Gruber. BigTable: A Distributed Storage System for
Structured Data. In Proc. of OSDI, Seattle, WA, Nov.
2006.
[12] J. Cipar, G. R. Ganger, K. Keeton, C. B. M. III,
C. A. N. Soules, and A. C. Veitch. LazyBase: Trading
Freshness for Performance in a Scalable Database. In
Proc. of EuroSys, Bern, Switzerland, Apr. 2012.
[13] A. T. Clements, D. R. K. Ports, and D. R. Karger.
Arpeggio: Metadata Searching and Content Sharing
with Chord. In Proc. of IPTPS Workshop, La Jolla,
CA, Feb. 2005.
[14] B. F. Cooper, R. Ramakrishnan, U. Srivastava, A.
Silberstein, P. Bohannon, H.-A. Jacobsen, N. Puz, D.
Weaver, and R. Yerneni. PNUTS: Yahoo!’s Hosted
Data Serving Platform. In PVLDB, 1(2), 2008.
[15] B. F. Cooper, A. Silberstein, E. Tam, R.
Ramakrishnan, and R. Sears. Benchmarking Cloud
Serving Systems with YCSB. In Proc. of SoCC,
Indianapolis, IN, June 2010.
[16] CouchDB. http://couchdb.apache.org/.
[17] A. Crainiceanu, P. Linga, J. Gehrke, and J.
Shanmugasundaram. Querying Peer-to-Peer Networks
Using P-Trees. In Proc. of WebDB Workshop, Paris,
France, June 2004.
[18] J. Dean. Designs, Lessons, and Advice from Building
Large Distributed Systems. Keynote. In Proc. of
LADIS, Big Sky, MT, Oct. 2009.
[19] G. DeCandia, D. Hastorun, M. Jampani, G.
Kakulapati, A. Lakshman, A. Pilchin, S.
Sivasubramanian, P. Vosshall, and W. Vogels.
Dynamo: Amazon’s Highly Available Key-Value
Store. In Proc. of SOSP, Stevenson, WA, Oct. 2007.
[20] R. Escriva, B. Wong, and E. G. Sirer. Hyperdex.
[41] J. A. Orenstein and T. H. Merrett. A Class of Data
http://hyperdex.org/.
[21] P. Ganesan, B. Yang, and H. Garcia-Molina. One
Structures for Associative Searching. In Proc. of
PODS, 1984.
Torus to Rule Them All: Multidimensional Queries in
P2P Systems. In Proc. of WebDB Workshop, Paris,
France, June 2004.
[42] L. Peterson, A. Bavier, and S. Bhatia. VICCI: A
Programmable Cloud-Computing Research Testbed.
Princeton, Technical Report TR-912-11, 2011.
[22] S. Gilbert and N. A. Lynch. Brewer’s Conjecture and
[43] D. R. K. Ports, A. T. Clements, I. Zhang, S. Madden,
the Feasibility of Consistent, Available,
Partition-Tolerant Web Services. In SIGACT News,
33(2), 2002.
[23] S. D. Gribble. A Design Framework and a Scalable
Storage Platform to Simplify Internet Service
Construction. U.C. Berkeley, 2000.
[24] A. Guttman. R-Trees: A Dynamic Index Structure for
Spatial Searching. In Proc. of SIGMOD, 1984.
[25] HBase. http://hbase.apache.org/.
[26] M. Herlihy and J. M. Wing. Linearizability: A
Correctness Condition for Concurrent Objects. In
ACM ToPLaS, 12(3), 1990.
[27] P. Hunt, M. Konar, F. P. Junqueira, and B. Reed.
ZooKeeper: Wait-Free Coordination for Internet-Scale
Systems. In Proc. of USENIX, Boston, MA, June
2010.
[28] Hypertable. http://http://hypertable.org/.
[29] D. R. Karger, E. Lehman, F. T. Leighton, R.
Panigrahy, M. S. Levine, and D. Lewin. Consistent
Hashing and Random Trees: Distributed Caching
Protocols for Relieving Hot Spots on the World Wide
Web. In Proc. of STOC, El Paso, TX, May 1997.
[30] F. B. Kashani and C. Shahabi. SWAM: A Family of
Access Methods for Similarity-Search in Peer-to-Peer
Data Networks. In Proc. of CIKM, Washington, D.C.,
Nov. 2004.
[31] A. Klinger. Patterns and Search Statistics. In
Optimizing Methods in Statistics, Academic Press,
1971.
[32] A. Lakshman and P. Malik. Cassandra: A
Decentralized Structured Storage System. In Proc. of
LADIS, Big Sky, MT, Oct. 2009.
[33] H. Lim, B. Fan, D. G. Andersen, and M. Kaminsky.
SILT: A Memory-Eﬃcient, High-Performance
Key-Value Store. In Proc. of SOSP, Cascais, Portugal,
Oct. 2011.
[34] W. Lloyd, M. J. Freedman, M. Kaminsky, and D. G.
Andersen. Don’t Settle for Eventual: Scalable Causal
Consistency for Wide-Area Storage with COPS. In
Proc. of SOSP, Cascais, Portugal, Oct. 2011.
[35] Y. Mao, E. Kohler, and R. T. Morris. Cache
Craftiness for Fast Multicore Key-Value Storage. In
Proc. of EuroSys, Bern, Switzerland, Apr. 2012.
[36] Memcached. http://memcached.org/.
[37] MongoDB. http://www.mongodb.org/.
[38] Neo4j. http://neo4j.org/.
[39] J. Nievergelt, H. Hinterberger, and K. C. Sevcik. The
Grid File: An Adaptable, Symmetric Multikey File
Structure. In ACM ToDS, 9(1), 1984.
[40] D. Ongaro, S. M. Rumble, R. Stutsman, J. K.
Ousterhout, and M. Rosenblum. Fast Crash Recovery
in RAMCloud. In Proc. of SOSP, Cascais, Portugal,
Oct. 2011.
and B. Liskov. Transactional Consistency and
Automatic Management in an Application Data
Cache. In Proc. of OSDI, Vancouver, Canada, Oct.
2010.
[44] D. Pritchett. BASE: An ACID Alternative. In ACM
Queue, 6(3), 2008.
[45] Project Voldemort.
http://project-voldemort.com/.
[46] J. Rao, E. J. Shekita, and S. Tata. Using Paxos to
Build a Scalable, Consistent, and Highly Available
Datastore. In PVLDB, 4(4), 2011.
[47] S. Ratnasamy, P. Francis, M. Handley, R. M. Karp,
and S. Shenker. A Scalable Content-Addressable
Network. In Proc. of SIGCOMM, San Diego, CA,
Aug. 2001.
[48] Redis. http://redis.io/.
[49] Riak. http://basho.com/.
[50] A. I. T. Rowstron and P. Druschel. Pastry: Scalable,
Decentralized Object Location, and Routing for
Large-Scale Peer-to-Peer Systems. In Proc. of ICDSP,
volume 2218, 2001.
[51] H. Samet. Spatial Data Structures. In Modern
Database Systems: The Object Model, Interoperability,
and Beyond, Addison Wesley/ACM Press, 1995.
[52] C. Schmidt and M. Parashar. Enabling Flexible
Queries with Guarantees in P2P Systems. In Internet
Computing Journal, 2004.
[53] Y. Shu, B. C. Ooi, K.-L. Tan, and A. Zhou.
Supporting Multi-Dimensional Range Queries in
Peer-to-Peer Systems. In Proc. of IEEE International
Conference on Peer-to-Peer Computing, Konstanz,
Germany, Aug. 2005.
[54] R. P. Spillane, P. J. Shetty, E. Zadok, S. Dixit, and S.
Archak. An Eﬃcient Multi-Tier Tablet Server Storage
Architecture. In Proc. of SoCC, 2011.
[55] I. Stoica, R. Morris, D. R. Karger, M. F. Kaashoek,
and H. Balakrishnan. Chord: A Scalable Peer-to-Peer
Lookup Service for Internet Applications. In Proc. of
SIGCOMM, San Diego, CA, Aug. 2001.
[56] D. B. Terry, M. Theimer, K. Petersen, A. J. Demers,
M. Spreitzer, and C. Hauser. Managing Update
Conﬂicts in Bayou, a Weakly Connected Replicated
Storage System. In Proc. of SOSP, Copper Mountain,
CO, Dec. 1995.
[57] R. van Renesse and F. B. Schneider. Chain Replication
for Supporting High Throughput and Availability. In
Proc. of OSDI, San Francisco, CA, Dec. 2004.
[58] C. Zhang, A. Krishnamurthy, and R. Y. Wang.
SkipIndex: Towards a Scalable Peer-to-Peer Index
Service for High Dimensional Data. Princeton,
Technical Report TR-703-04, 2004.