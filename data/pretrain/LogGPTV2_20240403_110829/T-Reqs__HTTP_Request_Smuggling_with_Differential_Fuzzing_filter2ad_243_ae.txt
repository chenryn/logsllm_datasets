Transfer-Encoding to parse the message body.
6 IMPACT ASSESSMENT
So far we have answered our core research questions and systemat-
ically confirmed that HTTP processing discrepancies lead to novel
HRS vulnerabilities. Next, we present a set of empirical experiments
to reaffirm that these vulnerabilities in fact have practical impact,
and compare our work to existing HRS testing tools.
6.1 Demonstrating Possible Attacks
The damage caused by an HRS attack depends on the web applica-
tion and data exposed by the vulnerable server pair. In this paper,
we do not quantify such damage. Instead, we explore the discrepan-
cies between HTTP processors and quantify the HRS attack surface
independent of the outcome of any particular exploitation scenario.
Regardless, to demonstrate end-to-end attacks in a proof-of-
concept, we set up an environment with a vulnerable application
behind a server pair with actionable discrepancies. Specifically,
we abused a chunked body parsing discrepancy between Akamai-
NGINX. We configured NGINX to serve OWASP Mutillidae [8], a
deliberately vulnerable web application for security training.
We tested three scenarios using HRS: 1) bypassing header rewrit-
ing, 2) hijacking requests, and 3) delivering attack payloads. In (1),
we smuggled a request with an arbitrary X-Forwarded-For value,
evading re-writing by the entrypoint. This is critical, since this
header is often used in authentication and authorization schemes [26].
In (2) our smuggler request payload constructed a poisoned request
to an attacker-controlled destination, leaking a random user’s re-
quest content including session cookies. Finally, in (3) we smuggled
a request which exploits a reflected XSS vulnerability at the desti-
nation to have the XSS response delivered to a random user. Videos
of these attacks in action are available on the authors’ websites.
Session 6B: Web Vulnerabilities CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea1814(a) Pairs affected by line mutations.
Figure 6: Failed request smuggling reasons for each server pair.
(b) Pairs affected by header mutations.
(c) Pairs affected by body mutations.
6.2 Estimating Server Combinations
At a first glance, some of the server pairs we test may seem unreal-
istic for a real-life deployment scenario. However, the Internet has
become a complex ecosystem (and patchwork) of middle-boxes and
cloud services, where any given request may be processed by not
two, but many servers. CDN deployments are prevalent [2], and
multi-CDN chaining is practical [21]. These services themselves
may depend on popular proxies, web caches, and web servers (e.g.,
Fastly uses Varnish [25], Cloudflare uses NGINX [30]).
To illustrate our point, we conducted an experiment with the top
10K sites of the Tranco list [34], exploring what server technolo-
gies are deployed in the wild. Namely, we visited the homepage of
each site and collected the HTTP response headers. We simultane-
ously ran route traces for IP addresses seen on path, and performed
WHOIS lookups for each. We then searched through this data for
known header & value combinations that fingerprint the technolo-
gies, and for explicit service identifiers inside HTTP responses,
WHOIS data, and email domains. This process resulted in a set of
potential server technologies used for each site.
This methodology has limitations. There is no known way to re-
liably detect proxy services via traffic analysis, particularly because
many services allow operators to strip the identifying headers to
prevent fingerprinting. Furthermore, different endpoints on a site
Figure 7: HTTP processors paired in the wild. This is an unordered
graph, showing pairwise combinations. Red edges indicate pairs
that exhibit processing discrepancies, blue edges represent pairs
that do not. Edge thickness corresponds to the incidence of pairs.
may use distinct proxy technologies, requiring a comprehensive
crawl of each site for an accurate analysis. Finally, a blackbox de-
tection methodology cannot determine the placement order of the
servers, but only the fact that they are used in some combination.
These are non-trivial challenges that we do not tackle in this work.
Figure 7 summarizes our results, showing pairwise server com-
binations we observed, where the edge weights represent the inci-
dence. We find that approximately 17% of the sites among the top
10K use technologies that we have identified discrepancies between.
We observe an average of 2.8 technologies, with a median of 3, and
a maximum of 5 per site.
Given the aforementioned limitations, these results represent
a loose lower bound on the incidence of server pairings. Yet, they
show two important points. First, out of the 45 possible combina-
tions of the 10 servers in our setup, 36 are used in the wild. Second,
seemingly unrealistic combinations are viable, and chained CDNs
are more frequent than other combinations. We conclude that mak-
ing presumptions about what server combinations are viable in
the wild is counterproductive when exploring HRS and similar
systems-level hazards. Processing discrepancies can crop up on any
technology, and therefore, all combinations are worth investigating.
6.3 Comparing T-Reqs to Existing Tools
James Kettle’s Burp Suite extension HTTP Request Smuggler [36]
and Evan Custodio’s Python script smuggler [7] are the state-of-
the-art tools used when testing sites for HRS.
Foremost, both of these operate on fundamentally different tar-
gets and serve a different purpose than T-Reqs. In particular, these
tools are designed for penetration testing of a given target site,
treating the entire web deployment as a blackbox, and testing it for
a set of known Content-Length and Transfer-Encoding header
manipulation attacks presented in the authors’ respective works.
In contrast, T-Reqs is not designed to test live sites against known
exploits. T-Reqs tests pairwise combinations of HTTP processors in
a lab environment, and exercises each component individually, in
a greybox manner. It is designed to discover novel HRS vectors, as
opposed to testing a real-life deployment for known attacks.
Therefore, these tools are not substitutes for each other. T-Reqs
serves to discover novel HRS payloads that the others are bound to
miss given their limited scope. That does not diminish the value
ApacheNGINXTomcatATSHAProxySquidVarnishAkamaiCloudflareCloudFrontSession 6B: Web Vulnerabilities CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea1815of previous work. In fact, these tools are suited to work in tandem,
where T-Reqs finds novel exploits, which can then be added to Burp
or smuggler to automate their use in penetration testing.
With that in mind, we next describe how these existing tools
work, and present an empirical study demonstrating T-Reqs’s ability
to create new knowledge for HRS research.
HRS Detection in Existing Tools. Existing tools use the detec-
tion methodology proposed by Kettle [15]. First, the request in List-
ing 18 is sent to a target to determine if it is affected by a CL.TE dis-
crepancy, meaning the entrypoint processes the Content-Length
header while the exitpoint prefers Transfer-Encoding. If the tar-
get is vulnerable, the entrypoint will forward the first four bytes
(i.e., chunk size 1 and chunk data Z), and the exitpoint will timeout
waiting for the next chunk which will never arrive. This timeout
delay flags the site as vulnerable.
If the CL.TE test fails, the request in Listing 19 is sent to check
for a TE.CL discrepancy. If there is a vulnerability, a similar delay
can be observed: The entrypoint forwards the body without the
byte X that comes after the last chunk, and the exitpoint receives
less content than what Content-Length indicates, therefore timing
out while waiting for one additional byte.
As also emphasized by Kettle, the order of the above two checks
is important. The TE.CL test should only be performed after con-
firming the absence of a CL.TE discrepancy. Otherwise the TE.CL
request could poison the connection with the byte X in CL.TE-
impacted targets, launching an attack on arbitrary Internet users.
All in all, both tools iterate through numerous mutations in the
Transfer-Encoding header of the requests in Listings 18 and 19,
and check the target for CL.TE and TE.CL discrepancies using the
above methodology.
Safety of the Detection Methodology. Unfortunately, we have
empirically confirmed in our tests that neither tool is currently safe
to run on real-world targets.
Firstly, Custodio’s smuggler does not follow the above order of
requests which is critical for preventing inadvertent attacks. More
interestingly, even though Burp follows the protocol, we have deter-
mined that a false assumption made in the detection methodology
makes it hazardous for running real-world experiments. Namely,
the methodology assumes that, when the target is affected by a
TE.CL discrepancy, the entrypoint will treat the byte Q in Listing 18
as an invalid chunk size and return an error. That will prevent ac-
cidentally poisoning the connection during the CL.TE check. This
assumption does not hold; Akamai servers do not return an error
and forward the request as if Q was a proper chunk size.
1 POST / HTTP/1.1
2 Host: example.com
3 Transfer-Encoding: chunked
4 Content-Length: 4
5
6 1
7 Z
8 Q
1 POST / HTTP/1.1
2 Host: example.com
3 Transfer-Encoding: chunked
4 Content-Length: 6
5
6 0
7
8 X
Listing 18: CL.TE test request.
Listing 19: TE.CL test request.
While this could be an oversight in design, it is also possible that
Akamai’s behavior has changed in the two years since the publica-
tion of Kettle’s work. We conclude that testing the existing tools in
a large-scale experiment is not safe without explicit penetration-
testing agreements. Instead, we conducted our comparative study
in the same test environment used for the experiments with T-Reqs.
Empirical Comparison. We have already presented T-Reqs’s
output in Section 5. To compare those results with the detections
from the two existing tools, we ran them in the same experimental
setup. However, there were two cases we could not test. It is not
possible to run Tomcat in a reverse-proxy mode, and therefore we
did not test pairs having Tomcat as the entrypoint. Additionally,
we were unable to set up a Cloudflare-CloudFront pair, because the
Host header rewriting capability necessary for that deployment is
only available for Cloudflare’s Enterprise plan customers [3]. This
limitation was not a factor during our core experiments with T-Reqs;
we had found no discrepancies for this CDN pair, and therefore we
did not need to attempt a deployment for exploitability testing.
Unsurprisingly, T-Reqs was the only tool that found the request
line and request body related attacks described in Sections 5.2.1
and 5.2.3. The others missed this category of attacks entirely, be-
cause they are only designed to test for the Content-Length and
Transfer-Encoding header manipulation attacks. One exception
was that Burp found an exploitable Mangled Last-Chunk discrep-
ancy on Akamai-ATS. We manually verified that this was an acciden-
tal true positive, as the request template Burp uses unintentionally
had the trigger for this discrepancy built into the chunked body –
no mutations were necessary for this finding. T-Reqs also detected
the same vulnerability through body mutations.
An unanticipated outcome was neither smuggler nor Burp flagged
any request header attacks either, even though they are designed
to test for those. We reviewed the source code for both tools and
verified that the header modifications they use1,2 indeed do not lead
to any exploitable HRS vulnerabilities today on the 10 technologies
in our setup. We attribute this to the fact that these tools repeat
known exploits, and the server vendors have already had two years
to implement mitigations since their disclosure.
In summary, Burp detected one HRS vulnerability and smuggler
detected none, whereas T-Reqs yielded all the findings we presented
in Section 5. We conclude that T-Reqs indeed fulfills its role of
finding novel HRS vectors left out of scope in previous work.
6.4 Testing HRS in the Wild
Due to the aforementioned safety issues, the state-of-the-art HRS
detection methodology should not be used outside of specific tar-
gets that explicitly allow external testing of their sites. Designing a
safe detection scheme likely requires whitebox analysis of different
HTTP processors to ensure that request queues are not inadver-
tently poisoned. Conducting a large-scale HRS measurement in the
wild safely is a promising future research direction.
Here, we instead present a preliminary experiment testing real-
world deployments that has Akamai as entrypoint, and only with
a specific HRS payload. This is not an arbitrary choice. Prior to
1https://github.com/PortSwigger/http-request-smuggler/blob/master/src/burp/
DesyncBox.java
2https://github.com/defparam/smuggler/blob/master/configs/exhaustive.py
Session 6B: Web Vulnerabilities CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea1816running the experiment, we carefully studied Akamai’s behavior
and crafted an HRS detection payload, based on Kettle’s approach,
that is guaranteed to be safe for this particular experiment.
Specifically, we used a body mutation in the Chunk-Size Chunk-
Data Mismatch category. Recall from Section 5.2.3 and Figure 4 that
this category impacts server pairs that has Akamai as entrypoint,
and always makes Akamai prefer the Content-Length header. That
enables us to test sites for this novel HRS vulnerability while ac-
tively avoiding the unsafe situation, where an Akamai server prefers
the Transfer-Encoding but allows an invalid chunk size through.
To make sure that Akamai is the entrypoint in the target, we
sent a TRACE request with the Max-Forwards:0 header to force
the request to stop at the first HTTP server on path even if it does
not support the TRACE method. Out of 861 Akamai customers from
Tranco Top 10K identified previously in Section 6.2, we were able
to confirm 367 had Akamai as the entrypoint.
We tested these 367 domains by sending our new HRS detection
request, and flagged sites as vulnerable if they did not respond
within 5 seconds (i.e., the default threshold used in existing tools).
Out of the 367 domains tested, we found 23 to be vulnerable. These
included a high-profile financial institution, online retailers, and
other technology, news, and entertainment sites.
This experiment is decidedly narrow in scope. However, it suc-
cessfully demonstrates that real-world deployments are exposed to
HRS vulnerabilities we discovered with T-Reqs, despite the many
hidden layers of complexity present in the wild that we could not
account for in a lab environment. Designing a generalizable detec-
tion methodology and enabling a full-fledged measurement study
is the logical next step for characterizing the impact of HRS.
7 DISCUSSION
As we conclude, we underscore considerations for the correct in-
terpretation of our results.
Limitations. While this paper represents the most holistic in-
vestigation into HRS to date, it is by no means exhaustive. For
example, we leave non-standard HTTP headers out of scope. There
are further restrictions we impose on our methodology to make
the experimentation and analysis feasible, such as limiting the max-
imum number of mutations for an input, and mutating request
components in isolation in their respective experiment runs.
Nonetheless, our approach provides sufficient evidence to ad-
dress our research questions, showing that there are indeed vast
and unexplored opportunities for crafting HRS attacks – and that
the security community must stay alert. We make T-Reqs available
in the hopes that our fellow researchers will improve on it and
make even more exciting discoveries.
Real-World Considerations. In our experiments we test all
servers in their default configurations. While all of the exploits we
find are real and practical, configurations will vary considerably
in the wild. What is more, servers that we flag as impacted in this
experiment may be deployed behind other proxies (e.g., a web appli-
cation firewall or load balancer between the entry and exitpoints),
which intentionally or inadvertently strip out exploit payloads. On
the flip side, non-default configurations may also expose dangerous
discrepancies that we were not able to catch in our study.
We stress that our findings should not be taken at face value.
This work is not intended to be a prescribed list of vulnerabilities
and their mitigations. We provide strong indicators for hazardous
server combinations and demonstrate the severity of the issue, so
that system owners can vet their environments.
Blame Nobody. We reiterate that HRS is a system interaction
problem. Individual components of the system are not necessarily
flawed, but their hazardous combination results in a vulnerability
that is not trivial to detect or mitigate. This implies that technology
vendors are not always in a position to correct these flaws on their
own; an ideal HTTP processor that is strictly RFC compliant, using
a formally-verified parser, and implemented by the best developers
on the planet may still get caught in an HRS attack when combined