metadata:
  labels:
    name: redis
  name: redis
  namespace: devops
spec:
  type: NodePort
  ports:
  - name: redis
    port: 6379
    targetPort: 6379
    nodePort: 31379
  selector:
    name: redis    
[root@k8s-master1 redis]# kubectl aaply -f redis-dev.yaml 
persistentvolumeclaim/redis-data-redis-0                     Bound    pvc-4338d703-b71b-4596-a866-3bc0fd7363f5   5Gi        RWO            managed-nfs-storage   18h
[root@k8s-master1 redis]# kubectl get po -n devops |grep redis
redis-0                                     1/1     Running   0          3m38s
192.168.229.3
端口：31379
密码：123456
```
### 7.部署sts-rabbitmq单节点服务
[kubernetes-部署RabbitMQ_Kubernetes中文社区](https://www.kubernetes.org.cn/6580.html)
```shell
docker pull rabbitmq:3.7-rc-management
docker tag  rabbitmq:3.7-rc-management    core.harbor.domain/test/rabbitmq:3.7-rc-management
docker push core.harbor.domain/test/rabbitmq:3.7-rc-management
[root@k8s-master1 rabbitmq]# cat rabbitmq-dev.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: rabbitmq
  namespace: devops
spec:
  selector:
    matchLabels:
      app: rabbitmq
  serviceName: "rabbitmq-service"
  replicas: 1
  template:
    metadata:
      labels:
        app: rabbitmq
    spec:
      terminationGracePeriodSeconds: 10
      nodeName: k8s-master1
      containers:
      - name: rabbitmq
        image: core.harbor.domain/test/rabbitmq:3.7-rc-management
        imagePullPolicy: IfNotPresent
        env:
        - name: MY_POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: RABBITMQ_ERLANG_COOKIE
          value: "YZSDHWMFSMKEMBDHSGGZ"
        - name: RABBITMQ_NODENAME
          value: "rabbit@$(MY_POD_NAME)"
        ports:
        - containerPort: 15672
          name: rabbit15672
          protocol: TCP
        - containerPort: 5672 
          name: rabbit5672 
          protocol: TCP
        volumeMounts:
        - name: rabbitmq-data
          mountPath: /var/lib/rabbitmq
  volumeClaimTemplates:
  - metadata:
      name: rabbitmq-data
      annotations:
        volume.beta.kubernetes.io/storage-class: managed-nfs-storage
    spec:
      accessModes: [ "ReadWriteMany" ]
      resources:
        requests:
          storage: 1Gi
---
apiVersion: v1
kind: Service
metadata:
  name: rabbitmq-service
  namespace: devops
spec:
  type: NodePort
  ports:
  - name: rabbitmqtcp
    port: 5672
    targetPort: 5672
    nodePort: 30001
  - name: managent
    port: 15672
    targetPort: 15672
    nodePort: 30002
  selector:
    app: rabbitmq
[root@k8s-master1 rabbitmq]# kubectl apply -f rabbitmq-dev.yaml
[root@k8s-master1 rabbitmq]# kubectl get po,pvc,svc -n devops |grep rabbitmq
pod/rabbitmq-0                                  1/1     Running   0          3m35s
persistentvolumeclaim/rabbitmq-data-rabbitmq-0               Bound    pvc-7c9433f2-2b45-44bd-b7cb-66c316bd043f   1Gi        RWX            managed-nfs-storage   5m59s
persistentvolumeclaim/rabbitmqdata-rabbitmq-0                Bound    pvc-daf05e27-c9c8-405f-b5f2-a1d405cc2417   1Gi        RWX            managed-nfs-storage   154m
service/rabbitmq-service            NodePort    10.109.181.185           5672:30001/TCP,15672:30002/TCP   3m35s
浏览器访问：
http://192.168.229.3:30002
默认账号密码：guest   guest
```
![image-20220304142724046](%E8%98%91%E8%8F%87%E5%8D%9A%E5%AE%A2%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.assets/image-20220304142724046.png)
### 8.部署zipkin组件
[(4条消息) K8S集群部署链路追踪——Zipkin_jiang_shikui的博客-CSDN博客_k8s 链路追踪](https://blog.csdn.net/jiang_shikui/article/details/84946586)
```shell
docker pull openzipkin/zipkin
[root@k8s-master1 zipkin]# cat zipkin-dev.yaml 
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: zipkin
  namespace: devops
spec:
  replicas: 1
  selector:
    matchLabels:
      app: zipkin
  template:
   metadata:
    labels:
     app: zipkin
   spec:
    nodeName: k8s-master1
    containers:
     - name: zipkin
       image: openzipkin/zipkin
       imagePullPolicy: IfNotPresent
       resources:
         limits:
           cpu: 150m
           memory: 256Mi
         requests:
           cpu: 100m
           memory: 128Mi
       env:
         - name: TZ
           value: Asia/Shanghai
       ports:
         - containerPort: 9411
---
apiVersion: v1
kind: Service
metadata:
  name: zipkin
  namespace: devops
spec:
  type: NodePort
  ports:
   - port: 80
     targetPort: 9411
     nodePort: 30095
  selector:
    app: zipkin
[root@k8s-master1 zipkin]# kubectl apply -f zipkin-dev.yaml
[root@k8s-master1 zipkin]# kubectl get po,svc -n devops |grep zipkin
pod/zipkin-6fc47b74df-5hzpf                     1/1     Running   0          4m19s
service/zipkin                      NodePort    10.106.58.232            80:30095/TCP                     4m19s
浏览器访问：
http://192.168.229.3:30095
```
![image-20220304145005338](%E8%98%91%E8%8F%87%E5%8D%9A%E5%AE%A2%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.assets/image-20220304145005338.png)
### 9.部署sts-sentinel组件
[k8s：部署sentinel-dashboard - 简书 (jianshu.com)](https://www.jianshu.com/p/78c50c8b5918)
```shell
[root@k8s-master1 sentinel]# cat sentinel-dev.yaml 
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: sentinel
spec:
  serviceName: sentinel
  replicas: 1
  selector:
    matchLabels:
      app: sentinel
  template:
    metadata:
      labels:
        app: sentinel
      annotations:
        pod.alpha.kubernetes.io/initialized: "true"
    spec:
      nodeName: k8s-master1
      containers:
        - name: sentinel
          imagePullPolicy: IfNotPresent
          image: bladex/sentinel-dashboard
          ports:
            - containerPort: 8858
              name: client
          env:
            - name: TZ
              value: Asia/Shanghai
            - name: JAVA_OPTS
              value: "-Dserver.port=8858 -Dcsp.sentinel.dashboard.server=localhost:8858 -Dsentinel.dashboard.auth.username=sentinel -Dsentinel.dashboard.auth.password=sentinel -Dserver.servlet.session.timeout=7200"
---
apiVersion: v1
kind: Service
metadata:
  name: sentinel-svc
  labels:
    app: sentinel
spec:
  ports:
    - protocol: TCP
      name: http
      port: 8858
      targetPort: 8858
      nodePort: 31808
  type: NodePort
  selector:
    app: sentinel
# 访问Sentinel登录页面，输入账号密码： sentinel  sentinel
http://192.168.229.3:31808
```
![image-20220304151144778](%E8%98%91%E8%8F%87%E5%8D%9A%E5%AE%A2%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.assets/image-20220304151144778.png)
### 10.部署sts-elasticsearch组件
```shell
[root@k8s-master1 elasticsearch]# dcoker load -i images/elasticsearch_7_2_0.tar.gz
[root@k8s-master1 images]# docker tag docker.elastic.co/elasticsearch/elasticsearch:7.2.0  core.harbor.domain/mogublog/elasticsearch:7.2.0
[root@k8s-master1 images]# docker push core.harbor.domain/mogublog/elasticsearch:7.2.0
[root@k8s-master1 elasticsearch]# cat elasticsearch-dev.yaml 
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: es-cluster
  namespace: devops
spec:
  serviceName: elasticsearch
  replicas: 3
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch
    spec:
      nodeName: k8s-master1
      containers:
      - name: elasticsearch
        image: docker.elastic.co/elasticsearch/elasticsearch:7.2.0
        imagePullPolicy: IfNotPresent
        resources:
            limits:
              cpu: 1000m
            requests:
              cpu: 100m
        ports:
        - containerPort: 9200
          name: rest
          protocol: TCP
        - containerPort: 9300
          name: inter-node
          protocol: TCP
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
        env:
          - name: cluster.name
            value: k8s-logs
          - name: node.name
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: discovery.seed_hosts
            value: "es-cluster-0.elasticsearch.kube-logging.svc.cluster.local,es-cluster-1.elasticsearch.kube-logging.svc.cluster.local,es-cluster-2.elasticsearch.kube-logging.svc.cluster.local"
          - name: cluster.initial_master_nodes
            value: "es-cluster-0,es-cluster-1,es-cluster-2"
          - name: ES_JAVA_OPTS
            value: "-Xms512m -Xmx512m"
      initContainers:
      - name: fix-permissions
        image: busybox
        imagePullPolicy: IfNotPresent
        command: ["sh", "-c", "chown -R 1000:1000 /usr/share/elasticsearch/data"]
        securityContext:
          privileged: true
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
      - name: increase-vm-max-map
        image: busybox
        imagePullPolicy: IfNotPresent
        command: ["sysctl", "-w", "vm.max_map_count=262144"]
        securityContext:
          privileged: true
      - name: increase-fd-ulimit
        image: busybox
        imagePullPolicy: IfNotPresent
        command: ["sh", "-c", "ulimit -n 65536"]
        securityContext:
          privileged: true
  volumeClaimTemplates:
  - metadata:
      name: data
      labels:
        app: elasticsearch
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: managed-nfs-storage
      resources:
        requests:
          storage: 3Gi
---
apiVersion: v1
kind: Service
metadata:
  name: elasticsearch
  namespace: devops
  labels:
    app: elasticsearch
spec:
  type: NodePort
  ports:
    - port: 9200
      targetPort: 9200
      nodePort: 30920
      name: es-9200
    - port: 9300
      targetPort: 9300
      nodePort: 30930
      name: es-9300
  selector:
    app: elasticsearch
[root@k8s-master1 elasticsearch]# kubectl apply -f  elasticsearch-dev.yaml 
# 为了减小内存使用可以将es缩容为一个
[root@k8s-master1 elasticsearch]# kubectl scale sts es-cluster --replicas=1
[root@k8s-master1 elasticsearch]# kubectl get po,svc,pvc -n devops |grep es
pod/es-cluster-0                                1/1     Running   0          9m3s
service/mysql-headless              ClusterIP   None                     3306/TCP                         39h
service/nacos-headless              ClusterIP   None                     8848/TCP,7848/TCP                26h
persistentvolumeclaim/data-es-cluster-0                      Bound    pvc-fe855ff9-ec84-4ee8-96f1-06e56b2033a1   3Gi        RWO            managed-nfs-storage   9m3s
persistentvolumeclaim/data-es-cluster-1                      Bound    pvc-accdff19-c827-48a3-b2e9-2fe66e2cc5c0   3Gi        RWO            managed-nfs-storage   8m59s
persistentvolumeclaim/data-es-cluster-2                      Bound    pvc-9bbd7b71-251b-4c69-aa8c-fcb42b77a424   3Gi        RWO            managed-nfs-storage   8m53s
浏览器访问：
http://192.168.229.3:30920/
```
![image-20220304155924951](%E8%98%91%E8%8F%87%E5%8D%9A%E5%AE%A2%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.assets/image-20220304155924951.png)