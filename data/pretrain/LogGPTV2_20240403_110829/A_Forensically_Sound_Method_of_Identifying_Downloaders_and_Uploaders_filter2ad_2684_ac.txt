the requests seen by the subject. The investigative technique uses
a statistical test to distinguish between these two cases.
4.3 Analysis
We now more formally describe our investigative technique, which
uses several values as input. Two directly observed per-file, per-peer
values are:
д : the number of directly connected peers of the subject (includ-
ing the observer herself); and
r : the number of requests received from the subject by the
observer.
The observer must select two additional values:
T : the total number of requests made by the requester; and
h : the number of peers assumed connected to a hypothesized
source that is not the directly observed subject.
The observer learns д from the Freenet protocol. How T is selected
depends on whether the observer is testing for downloads or up-
loads. Due to redundant forward error correction blocks, only about
half of the total defined blocks are required to download a file. The
number of download requests made is dependent on the number of
blocks available. In practice, we use a downwardly adjusted value
of r for downloaders; the number of requests observed represents
an upper bound. We define the values used for T and h, as well as
detail when and how we reduce r, in Section 4.4.
We construct a model by assuming that each request made by
the actual uploader or downloader is sent to exactly one of its peers,
and that the selection of that peer is made uniformly at random.
The total number of requests an observer will receive if the subject
is the actual requester can be modeled by a binomial distribution.
Let p be the probability of each request being sent to the observer.
Let X ∼ Binom(T , p) be a random variable representing the number
of requests received by the observer. Given T possible requests, the
probability of the observer receiving X = r requests is
pr(1 − p)T−r .
Pr(X = r) = Binom(r;T , p) =
(cid:18)T
(cid:19)
In the case of H1, the hypothesis that the subject is the actual
requester, p = 1/д. For H2, the hypothesis that the subject is a
r
ObserverUploader or DownloaderrequestsSubjectg neighborsDownloaderrequestsh neighborsObserverRelayrequestsSubjectg neighborsUploader or Downloaderrequestsh neighborsSession 5C: Forensics CCS '20, November 9–13, 2020, Virtual Event, USA1501the same manifest;
• all observations are of requests for blocks associated with
• all observations are of the same peer, as identified by IP
• all requests have a consistent HTL (detailed below);
• a minimum of 20 requests for distinct blocks were observed;
• the duration between requests does not exceed a defined
address and Freenet location;
value.
A request may be for a data block, an FEC block, or a manifest block.
A data request is used to fetch a block; an insert request is used to
upload a block. A download may generate insert requests (Section 2).
We require that at least 20 requests are received before a test is
run. It’s a static value that performs very well in all our evaluations
(Sections 5 and 6). We discuss the parameter further in Section 5. We
use a static value because there are practical benefits to minimizing
the number of variables and equations that need to be presented
to laypersons. Similarly, for the duration between requests, we use
a static value that performs well in our evaluations and could be
changed in the future with additional experimentation.
Defining rdrdrd andTdTdTd for Downloaders. Recall that a manifest con-
sists of about twice as many blocks as are required by Freenet to
recreate the original file. If all blocks were available on the network,
only roughly half would be requested. However, additional requests
are made if blocks are unavailable, and the requests may be sent
to multiple peers. This redundancy can inflate o, the number of
distinct data requests observed; and without adjustment it could
lead to false positives. Therefore, for a given run, we compute
• o, the number of data requests for distinct blocks;
• i, the number of insert requests;
• x, the number of duplicate data requests;
rd = o − i − ϵx .
and define rd as
(3)
Because an insert request typically indicates an additional data re-
quest was required, we decrement the count by the corresponding
number. We further reduce the count by a constant multiplier, ϵ,
for each key with duplicate observations (or triplicates, etc.). Du-
plicate data requests represent failed data requests or a concurrent
downloader. We would have expected the other blocks within a
segment to have been requested before re-requesting a block. Since
a relayer might request a distinct block from several of its peers due
to not-found errors, this can result in a large number of requests
and potentially to false positives. We mitigate by applying a multi-
plier to the number of duplicates observed. We selected a value of
ϵ = 3 because a request will be sent three times before it enters a
“cool down” period. Analysis of real data confirmed that this choice
was effective in limiting false positives.
To determine a suitable value for Td, we conducted experiments
on Freenet where we inserted our own files into the network and
then instrumented a downloader to count the number of distinct
blocks it requested. On average, it requested 67% of the total blocks.
We chose a value of Td = 0.8 ∗ TotalBlocks.
Defining rururu and TuTuTu for Uploaders. In contrast to downloading,
when uploading files we know that the requester must insert each
of the manifest, data, and FEC blocks. In practice, a requester may
send duplicate insert requests to several of its peers. However, with
rare exceptions, a requester will only initiate sending the block’s
Figure 2: CDF of the count of routable neighbors reported
by 21,245 neighbors of our nodes (distinguished by location)
from 9/2019–3/2020 inclusive. Each semi-transparent line is
one month of data. Overall, 98.0% of reports are for eight or
more neighbors, with 99.5% reporting six or more.
contents to a single peer. To set ru, we only include insert requests
where the insertion of the block’s content was also initiated, and
we are able to set Tu = TotalBlocks.
Consistent HTLs. In defining runs of r requests, we require con-
sistent HTL values. Data requests are by default sent with either an
HTL of 18 or 17, and the same HTL value is used for all requests
to the same peer. We require that data requests have HTLs of 18
or 17, but not both. Like data requests, insert requests are initiated
with an HTL of 18 or 17, but Freenet may resend the request with a
lower HTL value. When testing for uploaders, we require that the
run consist of only HTL 18 and 17, or only HTL 17 and 16.
Setting hhh and ддд. Our method requires that we set a value for h,
the number of peers connected to a hypothesized source. We set
the value to h = 8. Based on measurements of the network from
September 2019 to March 2020 inclusive, this is very conservative
in that it reduces the FPR compared to higher values of h, which are
more typical. Figure 2 shows an empirical CDF of all reports from
21,245 neighbors, with one line per month. Whenever the Freenet
client restarts, it must re-build its set of neighbors, and that is one
reason we occasionally observe a small number of neighbors. 98.6%
of reports are for eight or more neighbors, with 99.8% reporting six
or more. These results are in line with past work [43, 44]. And we
note that when we set д, we include neighbors of the subject that
are temporarily backed off. These seven months of measurements
show that on average 11.2% of the routable peers are backed off
at a given moment. Our in situ experiments in Section 5 account
for these ephemeral backoffs. The backoff process does not prevent
our method from achieving a very low FPR.
4.5 Selecting Priors
In Bayesian statistics, priors are formulated by the experimenter
before data is observed [46]. We have followed that approach, se-
lecting well-reasoned priors: Pr(H1) = 1
and Pr(H2) = д
д+1
1+д
These priors are conservative in that they include the possibility
that the investigator’s node is the requester, which decreases the
FPR, especially when д is small. The priors favor H2 by a factor of
(д/д + 1)/(1/д + 1) = д. Regardless, the posterior probability shown in
Eq. 2 is the dominant term for typical values of T and r. In idealized
scenarios, Bayesian priors can be updated as tests are completed.
Individual investigators could update priors by applying the test,
0.0%10.0%20.0%30.0%40.0%50.0%60.0%70.0%80.0%90.0%100.0%01020304050607080Routable neighborsECDFSession 5C: Forensics CCS '20, November 9–13, 2020, Virtual Event, USA1502Figure 3: In situ Freenet downloader experiments. Our nodes
were only relays in the download experiments. Downloaders
were potentially any other nodes on the network download-
ing CSAM; thus the FPR is the same as for real investigators.
Figure 4: Characteristics of the 918 downloader tests. Two
tests were FPs: one with 4,859 blocks and 30 peers, the other
with 1,110 blocks and 28 peers.
executing the proper legal process, and then carefully measuring
the count of targets found with CSAM and those without. Or, our
priors could include results from the tests for false positives on the
real network that we describe in Section 5. For simplicity, however,
we maintain static conservative priors.
5 EVALUATION I: IN SITU TESTING AND
ANALYSIS
We use a spectrum of evaluations to show that our method is both
accurate and effective. First, we measure the FPR of downloader de-
tection by deploying peers on the network and observing requests
for CSAM; thus the FPR is the same experienced by real investiga-
tors. We then measure the FPR and TPR of uploader detection on
the real network. Second, we derive an analytical model of FPR and
Power [46] to validate our experiments and provide deeper insights.
Our evaluations include the effects of node degree, manifest size,
method parameters, and concurrent downloaders. In Section 6, we
continue our evaluations using simulation.
5.1 Downloader FPR for Known Negatives
Our first goal is to measure the FPR of our method when it is applied
in practice to investigations of CSAM downloading on Freenet. To
this end, we deployed many nodes to Freenet that requested no
manifests. The experiment began in October 2017 and ran through
April 2020. These passive nodes used default installation parameters
and were on machines with typical resources. We looked for runs,
as defined in Section 4.4, marking the subject as a candidate for our
test. We included the requirement that the number of requests must
number at least 20. A false positive occurred any time our method
flagged the subject as a probable downloader, because none of our
subject nodes downloaded any of the over 124,000 CSAM files of
interest. We used a threshold probability of t ≥ 0.98 to determine
Figure 5: In situ Freenet uploader experiments. Our nodes
were the uploaders and sometimes relays, thus we could ob-
serve actual positives and negatives. When including runs
with less than 20 requests, the FPR is lower and precision
remains high. Parentheses show 95% confidence intervals.
if a run was associated with a downloader versus a relaying node.
Results are shown in Figure 3: we identified 918 runs to be evaluated,
and two of the runs were falsely flagged as being from a downloader.
In other words, we observe an FPR of 2/918 = 0.002 with a 95%
confidence interval of 0.003.
The runs of requests relayed through our nodes and triggering
our method were for CSAM files that ranged in size, with a distri-
bution shown in Figure 4 (left). The median size was 30,680 blocks
(502 MB). Figure 4 (right) shows the number of neighbors that our
passive nodes had when they were test subjects. As such, the two
false positives were not distinct from the true negatives. (Although
difficult to see, the two FPs are included in each histogram.)
Our investigative technique is based on relatively few param-
eters, whereas Freenet is a real distributed system composed of
many moving parts. The outcome of the test may be affected by
network size, topology and node degree, third-party traffic, routing
and node backoffs, manifest popularity, resource diversity among
nodes, and perhaps even undocumented aspects of Freenet code.
These 918 tests, completed over a period of years using real CSAM,
confirm that our approach is very accurate in practice, despite all
the factors that might come into play.
5.2 Uploader FPR and TPR for Known
Positives and Negatives
We evaluated the efficacy of our method to identify uploaders in
Freenet. We uploaded files containing random bytes, ensuring they
were not already in the network. To ensure diversity of topology
and neighbors, we deployed 20 Freenet nodes to conduct this exper-
iment, each a default installation with typical resources. Without
our influence, occasionally our nodes would become neighbors
of one another. We uploaded 15 or 16 manifests from each node
of varying size, totaling 309 uploads. For each upload, we logged
the number of insert requests received by each neighbor of the
uploader. When our own node was an immediate relayer for the
uploader, or an immediate neighbor of the uploader’s relayer, we
logged a count of requests as well. Therefore, we could apply our
investigative method as if all neighbors of the uploader and relayer
were investigators.
In the case of the uploader, we calculate the TPR (i.e., recall)
and precision. TPR is the efficiency of our method in finding actual
positives, whereas precision is the fraction of positive results that
True Neg.:  Actual Negative  0.002 (± 0.003)Downloadersrequests:    False Pos.:    ≥ 202916 FPR: relayer0 B1 GB2 GB3 GB4 GB5 GB0e+001e+052e+053e+05050100150200File SizeManifest size (total blocks)FrequencyFPTNrelayer0204060050100150200Subject's Routable PeersFrequencyFPTN                          Actual PositiveTPR:                   True Neg.:     False Neg.:      5,551True Pos.:        4,414False Pos.:      Uploaders12,643requests:         ≥ 1requests:≥ 18,6624,494(± 0.01)Actual NegativePrecision(± 0.018) 0.0004 (± 0.0007)(± 0.01)(± 0.008)0.342FPR:1100.0091.000.441≥ 20  ≥ 20Session 5C: Forensics CCS '20, November 9–13, 2020, Virtual Event, USA1503Figure 6: Freenet uploader experiments (semi-logscale for
requests). The one FP was for a manifest with 5,127 blocks.
are true positives. In the case of the relayers of the uploader, we
calculate the FPR.
One of our nodes was a relayer or neighbor of a relayer in 2,644
cases total; in 111 of those cases, at least 20 insert requests were sent.
We observed one false positive. Figure 5 shows the results: FPRs of
0.0004 ± 0.0007 and 0.009 ± 0.018, respectively, depending on the
request threshold. We had thousands of opportunities to calculate
the TPR (0.443 ± 0.01) and precision (0.9998 ± 0.01) of our method.
Relaxing the requirement of receiving at least 20 requests reduces
the TPR, but not precision. As our results show, the minimum could
be lowered below 20, but we leave that for future work (e.g., it could
be a function of T and д).
Figure 6 (left) visualizes the size of the 309 files that we inserted,
and bars are colored according to the outcome of the test. The relay
and uploader tests are shown in separate facets. Overall, we did
not observe that manifest size influences the FPR or TPR directly.
Figure 6 (right) demonstrates that the TPR is influenced significantly
by the number of insert requests received by a neighbor.
(cid:16)
(cid:16)1 − Exp(cid:0) д3
д ln(h)(cid:1)(cid:17) and Ω
Exp(cid:0) −T
5.3 Analytically Derived FPR and Power
We now derive the False Positive Rate and Power [46] of our hy-
pothesis test, which are O
spectively. Recall that for Eq. 2, hypothesis H1 is that the subject is
the requester, and hypothesis H2 is that the subject is a relayer. We
label the subject as the requester if the number of requests observed