is always a chance the two-input trigger condition could
occur as part of the traditional design-time testing all cir-
cuits are put through. Arguably, if the trigger instruction
is chosen carefully, the chance that the test suite includes
some sequence where the trigger instruction is executed
twice in a row (thus detecting the backdoor) is low, but there
still remains some chance that the backdoor is accidentally
exposed during design-time testing.
We note that a smart attacker can virtually eliminate
the chances of accidental exposure by choosing a trigger
condition that holds only for a vanishingly small proportion
of inputs. If the attacker uses an n-bit trigger condition, there
is only a 1/2n probability that a randomly chosen test will
expose the backdoor; by choosing n sufﬁciently large, the
attacker can make the probability that any test reveals the
backdoor negligible.
All of the malicious circuits discussed until now have
only a 2-bit trigger. It is natural to wonder whether these
attacks can be generalized to work with an n-bit trigger
condition, for arbitrary n. Through manual analysis, we
identiﬁed several malicious circuits that could be extended to
an n-bit trigger condition, and we show one such circuit in
Figure 6. This circuit has two non-trigger inputs i = (i0, i1)
and n trigger inputs t = (t0, t1, . . . , tn−1), with the trigger
condition t0 ∧ t1 ∧ · · · ∧ tn−1. The output function of this
circuit is f = (i0 ∧ i1) ∨ (t0 ∧ t1 ∧ · · · ∧ tn−1). Under
non-trigger conditions the output function is fN T = i0 ∧ i1.
Under trigger conditions the output is stuck at fT = 1.
The attack from Section IV-B can be extended to use an
n-bit trigger, thereby making the backdoor even harder to de-
tect through design-time testing. Figure 7 demonstrates one
way to do so. This ﬁgure shows a design where executing a
single specially chosen instruction n times in a row triggers
the backdoor. To make the ﬁgure cleaner, we don’t show
the sub-circuit (¬holdn ∧ super) ∨ (holdn ∧ in.super) and
instead show only its output. The circuit remains stealthy
when the sub-circuit is put back in place. Other general-
izations are possible; for instance, we could choose a secret
sequence of n different instructions and trigger the backdoor
only when those n instructions are executed consecutively.
In this way, the attacker can make it vanishingly unlikely that
the backdoor will be detected through black-box testing.
V. POSSIBLE DEFENSES
Given the two attack circuits covered so far, there are a
number of modiﬁcations to the UCI algorithm it might be
natural to suggest. In this section, we look at a number of
possible ﬁxes to UCI and discuss the validity of each one.
We argue that none of the natural modiﬁcations to UCI is
secure: they can all be defeated through simple variations on
our attacks. This suggests that it will not be straightforward
to develop effective defenses against these attacks.
71
(¬holdn ∧ super) ∨ (holdn ∧ in.super)
f
resetn
triggern
(¬holdn ∧ super) ∨ (holdn ∧ in.super)
triggern−1
resetn
trigger1
(¬holdn ∧ super) ∨
(holdn ∧ in.super)
trigger0
Figure 7. Practical attack circuit augmented to use n triggers.
i0
t0
i1
t0
i0
t1
i1
t1
i0
tn−1
i1
tn−1
i0
tn
i1
tn
f
Figure 6. Stealthy and malicious circuit with n trigger inputs.
72
A. Handling of MUX Control Inputs
The UCI algorithm, as described in the original UCI
paper [2, §5.3], does not treat the output of a MUX gate
as data-dependent upon the control input to the MUX, and
thus does not
include these among the pairs of signals
it analyzes, potentially missing some attacks. The original
paper suggested that it would be straightforward to extend
the UCI algorithm to treat MUX outputs as data-dependent
upon their control signals (by converting MUX gates to their
AND and OR gate representation and treating them as pure
data ﬂow elements).
This extension to UCI does not stop our attacks. The
attack circuit in Figure 5 does not use any MUX gates,
and thus is not affected by this modiﬁcation to UCI: the
attack remains effective and undetected by UCI. Therefore,
our backdoor for the Leon processor would be unaffected
by this extension to the UCI algorithm.
The example in Figure 2 does use MUX gates, but as
it happens, even the extended UCI algorithm does not ﬂag
it as malicious, since there are test cases where the MUX
outputs differ from their control inputs (e.g., (0, 0, 0, 1),
(1, 0, 1, 0)). Our search algorithm considers the output of
every gate to be dependent upon all of its input signals,
without differentiating between control vs. data ﬂow, so
every stealthy circuit output by our search algorithm already
defeats the extended UCI.
B. Shrinking the Basis of Gates
The example shown in Figure 2 uses two MUX gates. One
might wonder if the reason UCI fails to detect the malicious
circuit is because the MUX gate is a macro element that
contains three basic operations. It would take two AND
gates and one OR gate to compute h, the intermediate signal
computed by the ﬁrst MUX gate. It feels as though the
MUX gate is hiding some intermediate signals that might
be present once this gate is compiled to silicon. It is natural
to conjecture that, if these intermediate signals were exposed
to UCI, UCI might be able to detect the attack. For instance,
if we restrict the circuit so that it can only use simple gates
such as {AND, OR, NOT}, then perhaps it would not be
possible to defeat UCI (one might hope).
This defense does not work. As our practical attack
on the Leon3 processor shows, it is possible to build a
circuit that defeats UCI using only that three-gate basis.
Moreover, with an additional search, we were able to ﬁnd
admissible, malicious, and stealthy circuits that used only
NAND gates, as well as attacks that used only MUX gates.
This demonstrates that even if we compile the circuit to
NAND gates and then run UCI on the result, UCI still does
not detect all malicious circuits.
We also performed (cid:0)5
2(cid:1) additional searches with an ex-
panded basis,
to examine each two-gate basis possible
from the set {AND, OR, NOT, NAND, 2-input MUX}. We
found admissible, malicious, and stealthy circuits for each
of these (cid:0)5
2(cid:1) bases. This indicates that exposing additional
intermediate values or shrinking the set of allowed gates is
not sufﬁcient to make UCI secure.
C. Replacing Circuitry with a Single Gate
For each of the malicious circuits shown in Section IV,
there exists some sub-circuitry that could have been replaced
with a single gate without changing the output under the
non-trigger condition. The UCI algorithm looks for a de-
pendent pair of signals where the intervening sub-circuit
could be replaced with a wire. It seems natural to extend
UCI to also look for any triplet of dependent signals (s, t, u)
where u is dependent on both s and t and check whether the
intervening circuitry between them could have been replaced
by a single gate from the basis.
Unfortunately,
this too can be broken. Figure 8 is a
counter-example that shows this approach is insufﬁcient. The
circuit was built using the basis G = {MUX}. The output
function of the circuit is f = (i0 ∧ ¬i1) ∨ (t0 ∧ t1 ∧ i1).
Under non-trigger conditions the output is fN T = i0 ∧ ¬i1.
Under trigger conditions the output is fT = i0 ∨ i1. There is
no portion of the circuit that could be replaced with a single
MUX gate without changing the function output under non-
trigger conditions.
D. Expressiveness of Attacks
While the attack we demonstrated is very powerful, the
function of the malicious circuit we used was very simple.
(For instance, the attack in Figure 2 acts like a simple
AND gate, in the non-trigger condition.) It is reasonable to
ask how much the attacker is constrained by evading UCI.
While we don’t have any deﬁnitive answer to that question,
the results from our search for admissible, malicious, and
i0
t0
i1
t0
i1
t1
f
i1
Figure 8. Stealthy and malicious circuit in which no portion of the circuit
could be replaced with a single MUX gate.
stealthy circuits suggests that
there are enough distinct
functions for the attacker to have a reasonable chance of
implementing her malicious hardware of choice while still
evading UCI.
For each search we explored all circuits up to a size of
three gates. Once we had seen all circuits with less than four
gates we stopped creating new circuits. Although our focus
was on enumerating all circuits with at most three gates,
as a consequence of our searching algorithm we ended up
building some circuits with more than three gates along the
way. Therefore our results do include some larger circuits.
We found 2,317 unique circuits that are admissible, ob-
viously malicious, and stealthy. Of those, 75 had unique
output functions. To put this number in perspective, the total
number of unique output functions (for a circuit comprising
any number of gates, but given our constraints of two
non-trigger and two trigger inputs) that would meet our
deﬁnition of obviously malicious and admissible (although
not necessarily stealthy) is 24 × 24 = 256. The total number
of unique output functions for any four-input circuit of any
size is 216 = 65536. Of the 75 unique output functions
we found, 27 could be expressed by a circuit with 3 or
fewer gates. The circuits for the other 48 output functions
contained between 4 and 7 gates. The smallest admissible,
obviously malicious, and stealthy circuit we found used 2
MUX gates. The smallest circuit that did not use a MUX
gate required 3 gates.
In addition to bounding the search, we also forced the
search to time out after ten minutes, regardless of whether
all one-, two-, and three-gate circuits had been explored.
Therefore, 27 represents a lower bound on the number of
73
unique output functions available when using up to three
gates.
VI. DISCUSSION
Our experience working with UCI revealed a counter-
intuitive property of the algorithm. For a ﬁxed circuit with
a hidden attack, the chances that UCI detects the attack
decreases as we increase the number of test cases in the test
suite: the more thorough the test suite, the easier it becomes
to defeat UCI.
To see why this is so, suppose we have a circuit that
contains some malicious logic between signals s and t.
Suppose there exists a single input x to the circuit that
makes s and t non-equal and that doesn’t fall
into the
trigger condition. For a reasonably complex circuit there will
be thousands of non-trigger input conﬁgurations, not all of
which will be used during design-time testing. If the input
x is not found in the test suite, then UCI will (correctly)
ﬂag the logic between s and t as malicious. But if another
test is added that does evaluate the circuit on input x, then
there will now exist a test where s and t are non-equal and
therefore UCI will (incorrectly) remove the ﬂag, treating the
circuit as benign. It is counter-intuitive, but we found that
the number of false negatives (missed attacks) increases as
one adds more test cases (and, conversely, the number of
false positives decreases as the number of tests increases).
Of course, we must not lose sight of the bigger picture.
The goal is to catch the malicious hardware by whatever
means possible. Increasing the number of test cases increases
the chances the attack is uncovered during traditional design-
time testing. Each additional test case is an opportunity to
detect that the hardware failed to behave as expected, so
increasing the number of tests increases the effectiveness of
design-time testing even as it decreases the effectiveness of
UCI. In constructing malicious hardware, the attacker must
walk a line between evading UCI and remaining hidden
during design-time testing.
However, the n-bit trigger version of our practical attack
demonstrates how the attacker may be reasonably conﬁdent
of avoiding detection through design-time testing or UCI,
regardless of the amount of testing conducted. This points
to a larger challenge facing any UCI-like technique. Any
malicious hardware detection scheme that uses test cases as
its sole speciﬁcation of correct behavior is working with an
incomplete speciﬁcation. It is impractical (and, in the case of
a microprocessor, effectively impossible) for a test suite to
exhaustively cover all possible test cases, and consequently
a test suite provides only an incomplete speciﬁcation of the
intended behavior of the circuit. In the absence of a complete
speciﬁcation of the desired behavior, it is not clear how to
deﬁne malicious behavior. (Given a complete speciﬁcation,
malicious behavior may be deﬁned as any behavior falling
outside the speciﬁcation.)
Given this constraint, one approach for defending against
malicious circuitry is to deﬁne a particular class of malicious
hardware and then work to defend against that class. UCI
implicitly deﬁnes one such class of circuits as the set of
malicious circuits that will stay inactive during design-time
testing. We were able to break UCI by ﬁnding malicious
circuits that fall outside that class of circuits, i.e., by ﬁnding
malicious circuits that are active during design-time testing
but not detected by it. In short, our work demonstrates the
class of malicious hardware UCI defends against is not
comprehensive enough.
In general, the problem of malicious code detection is
equivalent to the problem of proving the circuit is correct.
The best solutions we have for tackling this problem at
design time are exhaustive simulation and formal veriﬁ-
cation. However, with the current state of the art, neither
of these techniques can feasibly be complete. Therefore,
a challenge for any future work in the area of malicious
hardware detection at design time is to clearly identify a
class of malware to defend against and justify why this class
is sufﬁcient to capture every attack we might care about.
It is important to note that we made no attempt, nor do we
claim, to deﬁne the class of all possible malicious circuits.