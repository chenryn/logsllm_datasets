• Finally, 5 sandboxes repeated the analysis of our probe
multiple times during the month following our initial
submission, but always for 2 to 3 minutes. None of these
sandboxes altered the sleep invocations.
The ﬁnding of this experiment is that companies seem
to execute samples for an average time that ranges from
30 seconds to 4 minutes. Considering the number of new
malware samples that need to be analyzed every day, this is
an understandable choice to achieve scalability.
C. Related Work
A huge body of research exists in the ﬁeld of dynamic
malware analysis. In the following, we summarize work aiming
at studying common malware techniques and thus at improving
the quality of malware sandboxes.
A well-known technique to identify and consequently
evade malware analysis sandboxes is the analysis of the exe-
cution environment to identify inconsistencies in the behavior
of the CPU, missing artifacts of user interaction, additional
artifacts in memory or ﬁngerprinting well known malware
sandboxes [37], [64], [72], [74], [80], [104], [107]. As a
counter-technique, researchers proposed to execute malware
samples in different environments and observe differences
in the execution traces to identify environment-aware sam-
ples [17], [48], [49], [57], [61]. While a successful sandbox
detection can alter a sample execution time, these work focused
on the techniques – or on the way those techniques can be
detected during the analysis.
Another branch of research focuses on increasing the
execution coverage by identifying trigger-based behaviors and
3
on enforcing the execution of multiple paths [22], [30], [68],
[75], [97]. While these techniques can help to collect a more
complete picture of the behavior of a sample, they introduce
a very large overhead that make them unsuitable for malware
analysis services. Among the solutions to increase coverage,
the most relevant to our study are those that focus on mitigating
the effect of stalling code [28], [54], [99]. As we mentioned
above, some basic form of stalling-code mitigation is deployed
by some of the sandboxes we tested and also incorporated in
our experiments.
Other work studied the impact of time for successfully
launching speciﬁc types of attacks [11] which is orthogonal
to the goal of our work.
III. EXPERIMENT DESIGN
As our goal is to measure the evolution of a sample’s
behavior over time, it is ﬁrst necessary to identify how these
two dimensions, behavior and time, can be properly measured.
In this section we discuss the different available options and
the ﬁnal choices that guided the design of our experiments.
A. Measuring Runtime Behavior
A common approach to model the behavior of a program
is to observe its APIs and/or system calls invocations, as they
provide a detailed view of the interaction of the sample with
its surrounding environment. However, syscalls are executed
throughout the entire life of a program, and just observing new
syscalls is not necessarily a sign of observing “new” behavior.
For instance, a ransomware sample can open a large number
of ﬁles, reading and writing their encrypted content over and
over again. This translates to, among others, many invocations
of NtOpenFile with different arguments (as the ﬁle name
varies). However, all of them are somehow associated with
the same high-level behavior, and therefore an analyst does
not gain any additional information by observing the creation
of yet another encrypted ﬁle.
In contrast, a different malware sample may ﬁrst open the
executable of another program (e.g., to infect its code), and
then open a number of documents to exﬁltrate some private
data. In this scenario, the same NtOpenFile syscall occurs
in completely different contexts and therefore exhibits new
behavior that is relevant for the analysis.
To distinguish such cases, in our experiments we will use
three orthogonal ways to measure the behavior of a running
sample. First, we will look into the list of syscalls and the
appearance of new classes of high-level actions (e.g., network
trafﬁc, ﬁlesystem activity, or registry operations). Second,
we will collect the actual code executed in the sandbox, to
capture the context in which each syscall was executed. To
continue with the previous examples; while the NtOpenFile
syscalls in the ransomware sample likely originated from the
same snippet of binary code, the syscalls for infection and
exﬁltration in the second sample were certainly triggered by
different pieces of code. Therefore, a natural way to observe
the appearance of new behavior is to examine the execution
traces — where new code equals new runtime behavior.
words, knowing that 80% of the syscalls happen in the ﬁrst
two minutes does not necessarily mean that those system calls
are the ones that really capture the core behavior of a sample.
The actual parameters of those syscalls can be very important
and maybe, while the amount of new behavior decreases with
time, its quality increases.
Therefore, we decided to add a third dimension by using a
machine learning classiﬁer as a way to capture how important
the new behavior is to ﬂag a sample as malicious. The intuition
is that, if a classiﬁer achieves higher accuracy by using the
information collected after a few minutes of execution, this
means that those actions were more distinctive of the behavior
of the sample. After all, one of the main goals of malware
analysis is to collect enough information to accurately classify
an unknown sample.
B. Behavioral Metrics
So far we discussed which information we can use to
capture the evolution of a sample behavior. Now we want
to investigate how this information can be measured and
presented.
For this, we propose three classes of metrics to use in
is the Relative coverage. The
our experiments. The ﬁrst,
relative coverage of a sample execution at time t is deﬁned
as the fraction of behavior (expressed either as the number
of observed system calls or as the amount of executed code)
with respect to the maximum number that is observed for
that sample over the entire execution interval. In other words,
the relative coverage captures which fraction of behavior we
collect at different points in time. For instance, if a sample
executes 10K basic blocks in ten minutes, how many of them
can we observe if we only execute the program for ﬁve
minutes? By answering this question, this metric can provide
us with an efﬁcient way to understand how much an analyst
would lose, in terms of amount of collected data, by reducing
the execution time in her sandbox. In our experiments we
will use two relative coverage metrics: the Relative Syscall
Coverage (Rs) and the Relative Code Coverage (Rc).
The second class of metrics we use to measure the behavior
evolution is the Absolute Code Coverage (Ac)—which tries
to estimate how much code was executed wrt the total sample
code. The challenge with this metric is that while it is simple
to extract the instructions that are executed (as the emulator
can easily collect this information), it is difﬁcult to precisely
estimate the part that was not executed. As we explain in
Section V-C, we can do that either by measuring the size
of the executable memory regions, or by disassembling those
regions to recover individual basic blocks. The ﬁrst approach
may overestimate the code by also including data (if the two
are interleaved), while the second suffers from the limitations
of the disassembler algorithm (e.g., to recursively traverse
the CFG and to deal with possible obfuscation and anti-
disassembly tricks). In Section V-C we will show that in our
experiments the Absolute Code Coverage computed by looking
at raw bytes or by looking at basic blocks are almost identical.
Therefore, unless explicitly mentioned, through the paper we
will use basic blocks as the basic unit of code.
Syscalls and binary code give us a way to capture the
amount of behavior, but not necessarily its quality. In other
The third and ﬁnal metric we use in our analysis relies
on the accuracy of a machine learning classiﬁer that uses
4
the sequence of the system call records observed during
the sandbox execution. By gradually extending the length of
the time window and measuring the resulting classiﬁcation
accuracy, we can assess the quality of the data collected over
that speciﬁc amount of execution time.
data we can collect. However, our system is designed to also
collect the expected sleep duration, allowing us to virtually re-
introduce the proper delays during the analysis. This way, we
can precisely assess how important it is to skip sleep operations
on the amount of data that can be collected by the sandbox.
C. Measuring Time
As the goal of our work is to analyze the evolution of a
sample behavior over time, a crucial step is to devise a way to
precisely measure at which time each action was performed.
First of all, we need to deﬁne what time means in this
context. To this end, we ﬁrst discuss the differences between
measured time and the sandbox time. Our system (explained
in details in Section IV) requires an emulator to capture each
basic block executed by the malware samples. Therefore, our
environment introduces an extra overhead which slows down
the execution compared to a more traditional malware analysis
sandbox, resulting in alterations on the original execution
time. To identify the latency that our system introduced to
the measurement, we compared the executions of the same
binaries in a Cuckoo [2] sandbox deployed on top of the
KVM virtualisation software (which acts as our reference
sandbox) and inside our infrastructure. During the executions
of the binaries, we fetch the timestamps corresponding to
some speciﬁc syscalls from both our sandbox and Cuckoo,
and compare them to estimate the slowdown.
The limitation of this technique is that the executables
under observation have to follow a deterministic pattern when
invoking the syscalls. Thus, we crafted two custom binaries in
which we control their deterministic syscall invocations (the
two binaries differ in the set of syscalls that are invoked). We
executed each binary 15 times in our system and 15 times in
the vanilla Cuckoo sandbox and obtained an overall slowdown
of 3.1x. However, for a real binary this value can be much
lower if the program spends a considerable amount of time
waiting for external events or inputs (e.g., when receiving
network packets). We therefore repeated the test for several
samples from our test set that show deterministic behavior but
which interact with the ﬁlesystem or receive network packets.
In these cases, we could observe a minimum slowdown of
1.4x and a maximum slowdown of 3.7x. This is well below
the worst-case slowdown of 10x of recording with PANDA
compared to executing a binary on a native system as reported
by the authors [31], [32], [90].
Another key problem while taking measurements about the
time, is that several samples invoke the Sleep and SleepEx
functions while running. Very often, malware authors use this
to stall execution as an anti-analysis trick. However, because of
its popularity, sandboxes can be designed to reduce (or entirely
skip) any sleep time — an action which is often called time-
warping. As we discuss in Section II-B, 4 of the 32 industrial
sandboxes we tested adopted this approach. This raises an
important question: if a sample executes one syscall at time t,
then waits for one minute and executes a second one, what is
the time in which the second event occurs? Is it real time (i.e.,
t + 60 seconds), or the warped time (i.e., t)?
Our solution, as explained in Section IV-A, is to use time-
warp to ignore any sleep operation, to maximize the amount of
D. Sample Selection
Setting up a representative malware dataset is a challenging
task that can be performed in different ways depending on
what the data needs to closely represent and what the goal
of the analysis is. For instance, one may want to balance
malicious and benign samples while another may focus on
obtaining a broad number of different families or even on
balancing the presence of individual attributes in the dataset
(such packed vs non-packed). Our goal is to analyze the impact
of execution time on a large-scale malware analysis pipeline
which might receive a large number of fresh samples to analyze
every day. As such, we wanted our dataset to mimic those that
are regularly analyzed by security companies. To satisfy this
requirement, for our malicious dataset we downloaded fresh
samples submitted to VirusTotal, i.e., samples observed for
the ﬁrst time on the same day in which we downloaded and
analyzed them (this is what Ugarte-Pedrero et al. [98] call
the catch of the day). This was the only criteria we used for
our selection. We did not try to impose any balance to the
downloaded data, because when previously unknown samples
are identiﬁed, they need to be analyzed without any a priori
knowledge about them. However, to avoid biases due to the
collection day, we only retrieved our data in small chunks of
2K samples, and repeated the procedure every few days. This
way we could guarantee that samples were always analyzed as
soon as they were collected, thus maximizing the probability
of being still active and therefore of running correctly at
the time of the execution. Because we were limited by the
scalability of our solution, which required up to one hour to
complete the analysis of each sample and to produce elaborated
reports about its execution, we stopped our data collection
when we reached 100K samples. This took over 6 months
at total. Overall, the ﬁnal instance of the dataset included 86K
malicious samples and 14K benign ones. We included all types
of Windows PE malware and we did not ﬁlter any speciﬁc
malware families or types.
As mentioned earlier, our study also aims at assessing
the impact of the analysis time on machine learning-based
malware detection techniques. Clearly, such techniques also
require a fully supervised training set composed by well tagged
benign and malicious samples, so as to build and train the
classiﬁers to categorize samples as malicious or benign. For
this reason, we only selected samples that were identiﬁed
to be malicious by at least 5 AV detection on VirusTotal–a
rather conservative solution compared with the threshold used
by other works [60]. Moreover, in contrast to many studies
that selected benign samples by picking popular Windows
applications or installation ﬁles, which in general are very
well-known ﬁles and therefore easy to spot and whitelist by
the security companies, we assembled our benign dataset from
VirusTotal submissions. The selection criteria was to choose
samples that were never identiﬁed as malicious by any of
the AV companies and that had been submitted to VirusTotal
at least 6 months prior to our analysis. This would provide
enough time to the AV to adapt their signatures to cover
5
new families and thus minimize the probability of selecting
unknown malicious ﬁles as benign.
IV. DESIGN & IMPLEMENTATION
As we explained in the previous section, our study requires
us to collect very ﬁne-grained information about the operations
performed in the system during the execution of a sample.
In particular, we want to collect all system calls, but also to
monitor the executed instructions, in the form of individual
Basic Blocks (BBs). Moreover, to estimate the absolute code
coverage at any given point in time, we also need to extract
all the executable regions that are part of the memory of the
running processes.
While malware analysis sandboxes are typically executed
either on a bare metal machine by using a virtualization tech-
nology, for the level of details required by our experiments we
need to resort to an emulator (QEMU in our case). Moreover,
since our analysis would introduce an unreasonable overhead,
we decided to use PANDA [31] to ﬁrst record a sample
execution, and then replay it while using our instrumentation.
This solution hinders the scalability of our system but it
allows us to obtain a better visibility on the execution of a
program, which is a necessary step to uncover the impact of
the sample execution time on the amount of information that
could be obtained through the malware analysis sandbox.
A. System Overview
Our analysis starts by loading the sample into PANDA
and by recording its execution for a speciﬁed amount of time.
Based on the data presented in Section II, we decided to use 15
minutes as execution threshold–as this was the highest value
we observed in research papers or industrial solutions, with
the exception of two longer experiments that only focused on
network activity.
However, to allow for a more fair analysis of downloaders