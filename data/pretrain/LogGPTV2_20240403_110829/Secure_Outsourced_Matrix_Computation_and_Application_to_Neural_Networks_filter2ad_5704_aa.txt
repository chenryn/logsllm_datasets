title:Secure Outsourced Matrix Computation and Application to Neural Networks
author:Xiaoqian Jiang and
Miran Kim and
Kristin E. Lauter and
Yongsoo Song
Secure Outsourced Matrix Computation
and Application to Neural Networks(cid:63)
Xiaoqian Jiang1, Miran Kim1, Kristin Lauter2, and Yongsoo Song2
1 University of Texas, Health Science Center at Houston, USA
{Xiaoqian.Jiang, Miran.Kim}@uth.tmc.edu
{klauter, Yongsoo.Song}@microsoft.com
2 Microsoft Research, Redmond, USA
September 4, 2019
Abstract. Homomorphic Encryption (HE) is a powerful cryptographic primitive to address pri-
vacy and security issues in outsourcing computation on sensitive data to an untrusted computation
environment. Comparing to secure Multi-Party Computation (MPC), HE has advantages in sup-
porting non-interactive operations and saving on communication costs. However, it has not come
up with an optimal solution for modern learning frameworks, partially due to a lack of eﬃcient
matrix computation mechanisms.
In this work, we present a practical solution to encrypt a matrix homomorphically and perform
arithmetic operations on encrypted matrices. Our solution includes a novel matrix encoding method
and an eﬃcient evaluation strategy for basic matrix operations such as addition, multiplication, and
transposition. We also explain how to encrypt more than one matrix in a single ciphertext, yielding
better amortized performance.
Our solution is generic in the sense that it can be applied to most of the existing HE schemes. It
also achieves reasonable performance for practical use; for example, our implementation takes 0.6
seconds to multiply two encrypted square matrices of order 64 and 0.09 seconds to transpose a
square matrix of order 64.
Our secure matrix computation mechanism has a wide applicability to our new framework E2DM,
which stands for encrypted data and encrypted model. To the best of our knowledge, this is the ﬁrst
work that supports secure evaluation of the prediction phase based on both encrypted data and
encrypted model, whereas previous work only supported applying a plain model to encrypted data.
As a benchmark, we report an experimental result to classify handwritten images using convolutional
neural networks (CNN). Our implementation on the MNIST dataset takes 1.69 seconds to compute
ten likelihoods of 64 input images simultaneously, yielding an amortized rate of 26 milliseconds per
image.
Keywords. Homomorphic encryption; matrix computation; machine learning; neural networks
1
Introduction
Homomorphic Encryption (HE) is an encryption scheme that allows for operations on encrypted inputs
so that the decrypted result matches the outcome for the corresponding operations on the plaintext.
This property makes it very attractive for secure outsourcing tasks, including ﬁnancial model evaluation
and genetic testing, which can ensure the privacy and security of data communication, storage, and
computation [3, 46]. In biomedicine, it is extremely attractive due to the privacy concerns about patients’
sensitive data [27, 47]. Recently deep neural network based models have been demonstrated to achieve
great success in a number of health care applications [36], and a natural question is whether we can
outsource such learned models to a third party and evaluate new samples in a secure manner?
There are several diﬀerent scenarios depending on who owns the data and who provides the model.
Assuming a few diﬀerent roles including data owners (e.g. hospital, institution or individuals), cloud
computing service providers (e.g. Amazon, Google, or Microsoft), and machine learning model providers
(cid:63) An early version of this work was published in CCS 2018. In this version, we present better experimental results
in Sections 5 and 6 based on our more recent implementation.
(e.g. researchers and companies), we can imagine the following situations: (1) the data owner trains
a model and makes it available on a computing service provider to be used to make predictions on
encrypted inputs from other data owners; (2) model providers encrypt their trained classiﬁer models
and upload them to a cloud service provider to make predictions on encrypted inputs from various data
owners; and (3) a cloud service provider trains a model on encrypted data from some data owners and
uses the encrypted trained model to make predictions on new encrypted inputs. The ﬁrst scenario has
been previously studied in CryptoNets [23] and subsequent follow-up work [10, 7]. The second scenario
was considered by Makri et al. [35] based on Multi-Party Computation (MPC) using polynomial kernel
support vector machine classiﬁcation. However, the second and third scenarios with an HE system have
not been studied yet. In particular, classiﬁcation tasks for these scenarios rely heavily on eﬃciency of
secure matrix computation on encrypted inputs.
1.1 Our Contribution
In this paper, we introduce a generic method to perform arithmetic operations on encrypted matrices
using an HE system. Our solution requires O(d) homomorphic operations to compute a product of two
encrypted matrices of size d× d, compared to O(d2) of the previous best method. We extend basic matrix
arithmetic to some advanced operations: transposition and rectangular matrix multiplication. We also
describe how to encrypt multiple matrices in a single ciphertext, yielding a better amortized performance
per matrix.
We apply our matrix computation mechanism to a new framework E2DM, which takes encrypted data
and encrypted machine learning model to make predictions securely. This is the ﬁrst HE-based solution
that can be applied to the prediction phase of the second and third scenarios described above. As a
benchmark of this framework, we implemented an evaluation of convolutional neural networks (CNN)
model on the MNIST dataset [33] to compute ten likelihoods of handwritten images.
1.2 Technical Details
After Gentry’s ﬁrst construction of a fully HE scheme [21], there have been several attempts to improve
eﬃciency and ﬂexibility of HE systems. For example, the ciphertext packing technique allows multiple
values to be encrypted in a single ciphertext, thereby performing parallel computation on encrypted
vectors in a Single Instruction Multiple Data (SIMD) manner. In the current literature, most of practical
HE schemes [9, 8, 18, 13] support their own packing methods to achieve better amortized complexity of
homomorphic operations. Besides component-wise addition and multiplication on plaintext vectors, these
schemes provide additional functionalities such as scalar multiplication and slot rotation. In particular,
permutations on plaintext slots enable us to interact with values located in diﬀerent plaintext slots.
A naive solution for secure multiplication between two matrices of size d × d is to use d2 distinct
ciphertexts to represent each input matrix (one ciphertext per one matrix entry) and apply pure SIMD
a0 a1 a2
a3 a4 a5
a6 a7 a8
·
b0 b1 b2
b3 b4 b5
b6 b7 b8
=
a0 a1 a2
a4 a5 a3
a8 a6 a7
a1 a2 a0
a5 a3 a4
a6 a7 a8
(cid:12)
(cid:12)
b0 b4 b8
b3 b7 b2
b6 b1 b5
b3 b7 b2
b6 b1 b5
b0 b4 b8
+
+
a2 a0 a1
a3 a4 a5
a7 a8 a6
(cid:12)
b6 b1 b5
b0 b4 b8
b3 b7 b2
Fig. 1: Our matrix multiplication algorithm with d = 3.
2
operations (addition and multiplication) on encrypted vectors. This method consumes one level for ho-
momorphic multiplication, but it takes O(d3) multiplications. Another approach is to consider a matrix
multiplication as a series of matrix-vector multiplications. Halevi and Shoup [24] introduced a matrix
encoding method based on its diagonal decomposition, putting the matrix in diagonal order and mapping
each of them to a single ciphertext. So it requires d ciphertexts to represent the matrix and the matrix-
vector multiplication can be computed using O(d) rotations and multiplications. Therefore, the matrix
multiplication takes O(d2) complexity and has a depth of a single multiplication.
multiplication can be expressed as A · B =(cid:80)d−1
We propose an eﬃcient method to perform matrix operations by combining HE-friendly operations
on packed ciphertexts such as SIMD arithmetics, scalar multiplication, and slot rotation. We ﬁrst deﬁne
a simple encoding map that identiﬁes an arbitrary matrix of size d× d with a vector of dimension n = d2
having the same entries. Let (cid:12) denote the component-wise product between matrices. Then matrix
i=0 Ai (cid:12) Bi for some matrices Ai (resp. Bi) obtained from
A (resp. B) by taking speciﬁc permutations. Figure 1 describes this equality for the case of d = 3. We
remark that the initial matrix A0 (resp. B0) can be computed with O(d) rotations, and that for any
1 ≤ i < d the permuted matrix Ai (resp. Bi) can be obtained by O(1) rotations from the initial matrix.
Thus the total computational complexity is bounded by O(d) rotations and multiplications. We refer
to Table 1 for comparison of our method with prior work in terms of the number of input ciphertexts
for a single matrix, complexity, and the required depth for implementation. We denote homomorphic
multiplication and constant multiplication by Mult and CMult, respectively.
Table 1: Comparison of secure d-dimensional matrix multiplication algorithms
Methodology
Naive method
Halevi-Shoup [26]
Ours
Number of
ciphertexts
d2
d
1
Complexity
Required depth
O(d3)
O(d2)
O(d)
1 Mult
1 Mult
1 Mult + 2 CMult
Our basic solution is based on the assumption that a ciphertext can encrypt d2 plaintext slots, but it
can be extended to support matrix computation of an arbitrary size. When a ciphertext has more than d2
plaintext slots, for example, we can encrypt multiple matrices in a single ciphertext and carry out matrix
operations in parallel. On the other hand, if a matrix is too large to be encoded into one ciphertext, one
can partition it into several sub-matrices and encrypt them individually. An arithmetic operation over
large matrices can be expressed using block-wise operations, and the computation on the sub-matrices
can be securely done using our basic matrix algorithms. We use this approach to evaluate an encrypted
neural networks model on encrypted data.
Our implementation is based on an HE scheme of Cheon et al. [13], which is optimized in computation
over the real numbers. For example, it took 0.6 seconds to securely compute the product of two matrices
of size 64 × 64 and 0.09 seconds to transpose a single matrix of size 64 × 64. For the evaluation of an
encrypted CNN model, we adapted a similar network topology to CryptoNets: one convolution layer and
two fully connected (FC) layers with square activation function. This model is obtained from the keras
library [14] by training 60,000 images of the MNIST dataset, and used for the classiﬁcation of handwriting
images of size 28× 28. It took 1.69 seconds to compute ten likelihoods of encrypted 64 input images using
the encrypted CNN model, yielding an amortized rate of 26 milliseconds per image. This model achieves
a prediction accuracy of 98.1% on the test set.
2 Preliminaries
The binary logarithm will be simply denoted by log(·). We denote vectors in bold, e.g. a, and every
vector in this paper is a row vector. For a d1 × d matrix A1 and a d2 × d matrix A2, (A1; A2) denotes the
3
(d1 +d2)×d matrix obtained by concatenating two matrices in a vertical direction. If two matrices A1 and
A2 have the same number of rows, (A1|A2) denotes a matrix formed by horizontal concatenation. We let
λ denote the security parameter throughout the paper: all known valid attacks against the cryptographic
scheme under scope should take Ω(2λ) bit operations.
2.1 Homomorphic Encryption
HE is a cryptographic primitive that allows us to compute on encrypted data without decryption and
generate an encrypted result which matches that of operations on plaintext [9, 18, 6, 13]. So it enables
us to securely outsource computation to a public cloud. This technology has great potentials in many
real-world applications such as statistical testing, machine learning, and neural networks [40, 29, 23, 31].
Let M and C denote the spaces of plaintexts and ciphertexts, respectively. An HE scheme Π =
(KeyGen, Enc, Dec, Eval) is a quadruple of algorithms that proceeds as follows:
• KeyGen(1λ). Given the security parameter λ, this algorithm outputs a public key pk, a public evalu-
• Encpk(m). Using the public key pk, the encryption algorithm encrypts a message m ∈ M into a
• Decsk(ct). For the secret key sk and a ciphertext ct, the decryption algorithm returns a message
• Evalevk(f ; ct1, . . . , ctk). Using the evaluation key evk, for a circuit f : Mk → M and a tuple of
ation key evk and a secret key sk.
ciphertext ct ∈ C.
m ∈ M.
ciphertexts (ct1, . . . , ctk), the evaluation algorithm outputs a ciphertext ct(cid:48) ∈ C.
An HE scheme Π is called correct if the following statements are satisﬁed with an overwhelming
probability:
• Decsk(ct) = m for any m ∈ M and ct ← Encpk(m).
• Decsk(ct(cid:48)) = f (m1, . . . , mk) with an overwhelming probability if ct(cid:48) ← Evalevk(f ; ct1, . . . , ctk) for an
arithmetic circuit f : Mk → M and for some ciphertexts ct1, . . . , ctk ∈ C such that Decsk(cti) = mi.
An HE system can securely evaluate an arithmetic circuit f consisting of addition and multiplication
gates. Throughout this paper, we denote by Add(ct1, ct2) and Multevk(ct1, ct2) the homomorphic addition
and multiplication between two ciphertexts ct1 and ct2, respectively. In addition, we let CMultevk(ct; u)
denote the multiplication of ct with a scalar u ∈ M. For simplicity, we will omit the subscript of the
algorithms when it is clear from the context.
2.2 Ciphertext Packing Technique
Ciphertext packing technique allows us to encrypt multiple values into a single ciphertext and perform
computation in a SIMD manner. After Smart and Vercauteren [45] ﬁrst introduced a packing technique
based on polynomial-CRT, it has been one of the most important features of HE systems. This method
represents a native plaintext space M as a set of n-dimensional vectors in Rn over a ring R using
appropriate encoding/decoding methods (each factor is called a plaintext slot). One can encode and
encrypt an element of Rn into a ciphertext, and perform component-wise arithmetic operations over the
plaintext slots at once. It enables us to reduce the expansion rate and parallelize the computation, thus
achieving better performance in terms of amortized space and time complexity.
However, the ciphertext packing technique has a limitation that it is not easy to handle a circuit
with some inputs in diﬀerent plaintext slots. To overcome this problem, there have been proposed some
methods to move data in the slots over encryption. For example, some HE schemes [22, 13] based on
the ring learning with errors (RLWE) assumption exploit the structure of Galois group to implement the
rotation operation on plaintext slots. That is, such HE schemes include the rotation algorithm, denoted
by Rot(ct; (cid:96)), which transforms an encryption ct of m = (m0, . . . , mn−1) ∈ M = Rn into an encryption
of ρ(m; (cid:96)) := (m(cid:96), . . . , mn−1, m0, . . . , m(cid:96)−1). Note that (cid:96) can be either positive or negative, and a rotation
by (−(cid:96)) is the same as a rotation by (n − (cid:96)).
4
2.3 Linear Transformations
Halevi and Shoup [24] introduced a method to evaluate an arbitrary linear transformation on encrypted
vectors. In general, an arbitrary linear transformation L : Rn → Rn over plaintext vectors can be
represented as L : m (cid:55)→ U·m for some matrix U ∈ Rn×n. We can express the matrix-vector multiplication
by combining rotation and constant multiplication operations.
Speciﬁcally, for 0 ≤ (cid:96) < n, we deﬁne the (cid:96)-th diagonal vector of U by u(cid:96) = (U0,(cid:96), U1,(cid:96)+1, . . . ,
Un−(cid:96)−1,n−1, Un−(cid:96),0, . . . , Un−1,(cid:96)−1) ∈ Rn. Then we have
U · m =
(u(cid:96) (cid:12) ρ(m; (cid:96)))
(1)
(cid:88)
0≤(cid:96)<n
where (cid:12) denotes the component-wise multiplication between vectors. Given a matrix U ∈ Rn×n and an
encryption ct of the vector m, Algorithm 1 describes how to compute a ciphertext of the desired vector
U · m.
Algorithm 1 Homomorphic linear transformation
procedure LinTrans(ct; U )
1: ct(cid:48) ← CMult(ct; u0)