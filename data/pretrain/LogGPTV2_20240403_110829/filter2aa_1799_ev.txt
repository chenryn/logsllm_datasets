volume (even a standard volume created on the top of a regular disk is fine for the experiment) 
to the DAX-mode “Q:” drive:
P:\>fstool.exe /daxcopy p:\Big_image.iso q:\test.iso
NTFS / ReFS Tool v0.1
Copyright (C) 2018 Andrea Allievi (AaLl86)
Starting DAX copy...
   Source file path: p:\Big_image.iso.
   Target file path: q:\test.iso.
   Source Volume: p:\ - File system: NTFS - Is DAX Volume: False.
   Target Volume: q:\ - File system: NTFS - Is DAX Volume: True.
   Source file size: 4.34 GB
Performing file copy... Success!
   Total execution time: 8 Sec.
   Copy Speed: 489.67 MB/Sec
Press any key to exit...
Process Monitor has captured a trace of the DAX copy operation that confirms the 
expected results:
From the trace above, you can see that on the target file (Q:\test.iso), only the 
CreateFileMapping operation was intercepted: no WriteFile events are visible. While the copy 
was proceeding, only paging I/O on the source file was detected by Process Monitor. These 
paging I/Os were generated by the memory manager, which needed to read the data back from 
the source volume as the application was generating page faults while accessing the memory-
mapped file. 
CHAPTER 11
Caching and file systems
727
To see the differences between memory-mapped I/O and standard cached I/O, you need to 
copy again the file using a standard file copy operation. To see paging I/O on the source file data, 
make sure to restart your system; otherwise, the original data remains in the cache:
P:\>fstool.exe /copy p:\Big_image.iso q:\test.iso 
NTFS / ReFS Tool v0.1 
Copyright (C) 2018 Andrea Allievi (AaLl86) 
Copying "Big_image.iso" to "test.iso" file... Success. 
   Total File-Copy execution time: 13 Sec - Transfer Rate: 313.71 MB/s. 
Press any key to exit...
If you compare the trace acquired by Process Monitor with the previous one, you can con-
firm that cached I/O is a one-copy operation. The cache manager still copies chunks of memory 
between the application-provided buffer and the system cache, which is mapped directly on the 
DAX disk. This is confirmed by the fact that again, no paging I/O is highlighted on the target file.
As a last experiment, you can try to start a DAX copy between two files that reside on the 
same DAX-mode volume or that reside on two different DAX-mode volumes:
P:\>fstool /daxcopy q:\test.iso q:\test_copy_2.iso 
TFS / ReFS Tool v0.1 
Copyright (C) 2018 Andrea Allievi (AaLl86) 
Starting DAX copy... 
   Source file path: q:\test.iso. 
   Target file path: q:\test_copy_2.iso. 
   Source Volume: q:\ - File system: NTFS - Is DAX Volume: True. 
   Target Volume: q:\ - File system: NTFS - Is DAX Volume: True. 
Great! Both the source and the destination reside on a DAX volume. 
Performing a full System Speed Copy! 
To see the differences between memory-mapped I/O and standard cached I/O, you need to 
copy again the file using a standard file copy operation. To see paging I/O on the source file data, 
make sure to restart your system; otherwise, the original data remains in the cache:
P:\>fstool.exe /copy p:\Big_image.iso q:\test.iso
NTFS / ReFS Tool v0.1
Copyright (C) 2018 Andrea Allievi (AaLl86)
Copying "Big_image.iso" to "test.iso" file... Success.
   Total File-Copy execution time: 13 Sec - Transfer Rate: 313.71 MB/s.
Press any key to exit...
If you compare the trace acquired by Process Monitor with the previous one, you can con-
firm that cached I/O is a one-copy operation. The cache manager still copies chunks of memory 
between the application-provided buffer and the system cache, which is mapped directly on the 
DAX disk. This is confirmed by the fact that again, no paging I/O is highlighted on the target file.
As a last experiment, you can try to start a DAX copy between two files that reside on the 
same DAX-mode volume or that reside on two different DAX-mode volumes:
P:\>fstool /daxcopy q:\test.iso q:\test_copy_2.iso
TFS / ReFS Tool v0.1
Copyright (C) 2018 Andrea Allievi (AaLl86)
Starting DAX copy...
   Source file path: q:\test.iso.
   Target file path: q:\test_copy_2.iso.
   Source Volume: q:\ - File system: NTFS - Is DAX Volume: True.
   Target Volume: q:\ - File system: NTFS - Is DAX Volume: True.
Great! Both the source and the destination reside on a DAX volume.
Performing a full System Speed Copy!
728
CHAPTER 11
Caching and file systems
   Source file size: 4.34 GB 
Performing file copy... Success! 
   Total execution time: 8 Sec. 
   Copy Speed: 501.60 MB/Sec 
Press any key to exit...
The trace collected in the last experiment demonstrates that memory-mapped I/O on DAX 
volumes doesn’t generate any paging I/O. No WriteFile or ReadFile events are visible on either 
the source or the target file:
Block volumes
Not all the limitations brought on by DAX volumes are acceptable in certain scenarios. Windows pro-
vides backward compatibility for PM hardware through block-mode volumes, which are managed by 
the entire legacy I/O stack as regular volumes used by rotating and SSD disk. Block volumes maintain 
existing storage semantics: all I/O operations traverse the storage stack on the way to the PM disk class 
driver. (There are no miniport drivers, though, because they’re not needed.) They’re fully compatible 
with all existing applications, legacy filters, and minifilter drivers. 
Persistent memory storage is able to perform I/O at byte granularity. More accurately, I/O is per-
formed at cache line granularity, which depends on the architecture but is usually 64 bytes. However, 
block mode volumes are exposed as standard volumes, which perform I/O at sector granularity (512 
bytes or 4 Kbytes). If a write is in progress on a DAX volume, and suddenly the drive experiences a 
power failure, the block of data (sector) contains a mix of old and new data. Applications are not pre-
pared to handle such a scenario. In block mode, the sector atomicity is guaranteed by the PM disk class 
driver, which implements the Block Translation Table (BTT) algorithm. 
   Source file size: 4.34 GB
Performing file copy... Success!
   Total execution time: 8 Sec.
   Copy Speed: 501.60 MB/Sec
Press any key to exit...
The trace collected in the last experiment demonstrates that memory-mapped I/O on DAX 
volumes doesn’t generate any paging I/O. No WriteFile or ReadFile events are visible on either 
the source or the target file:
CHAPTER 11
Caching and file systems
729
The BTT, an algorithm developed by Intel, splits available disk space into chunks of up to 512 GB, 
called arenas. For each arena, the algorithm maintains a BTT, a simple indirection/lookup that maps an 
LBA to an internal block belonging to the arena. For each 32-bit entry in the map, the algorithm uses 
the two most significant bits (MSB) to store the status of the block (three states: valid, zeroed, and er-
ror). Although the table maintains the status of each LBA, the BTT algorithm provides sector atomicity 
by providing a og area, which contains an array of nfree blocks.
An nfree block contains all the data that the algorithm needs to provide sector atomicity. There are 
256 nfree entries in the array; an nfree entry is 32 bytes in size, so the flog area occupies 8 KB. Each 
nfree is used by one CPU, so the number of nfrees describes the number of concurrent atomic I/Os an 
arena can process concurrently. Figure 11-75 shows the layout of a DAX disk formatted in block mode. 
The data structures used for the BTT algorithm are not visible to the file system driver. The BTT algo-
rithm eliminates possible subsector torn writes and, as described previously, is needed even on DAX-
formatted volumes in order to support file system metadata writes.
Arena 0
512GB
Arena Info Block (4K)
Data Blocks
nfree reserved blocks
BTT Map
BTT Flog (8K)
Info Block Copy (4K)
Arena
Arena 1
512GB
Backing Store
•
•
•
FIGURE 11-75 Layout of a DAX disk that supports sector atomicity (BTT algorithm).
Block mode volumes do not have the GPT_BASIC_DATA_ATTRIBUTE_DAX flag in their partition 
entry. NTFS behaves just like with normal volumes by relying on the cache manager to perform cached 
I/O, and by processing non-cached I/O through the PM disk class driver. The Pmem driver exposes read 
and write functions, which performs a direct memory access (DMA) transfer by building a memory 
descriptor list (MDL) for both the user buffer and device physical block address (MDLs are described in 
more detail in Chapter 5 of Part 1). The BTT algorithm provides sector atomicity. Figure 11-76 shows the 
I/O stack of a traditional volume, a DAX volume, and a block volume. 
730
CHAPTER 11
Caching and file systems
Traditional
App
NTFS
Volsnap
Volmgr/
Partmgr
Disk/
ClassPnP
StorPort
MiniPort
PM Block Volume
App
DAX Volume
App
User Mode
Kernel Mode
SSD/HDD
NTFS
Volsnap
Volmgr/
Partmgr
PM Disk
Driver
PM
PM
NTFS
Volmgr/
Partmgr
PM Disk
Driver
Memory Mapped
Cached I/O
FIGURE 11-76 Device I/O stack comparison between traditional volumes, block mode volumes, and DAX volumes.
File system filter drivers and DAX
Legacy filter drivers and minifilters don’t work with DAX volumes. These kinds of drivers usually 
augment file system functionality, often interacting with all the operations that a file system driver 
manages. There are different classes of filters providing new capabilities or modifying existing func-
tionality of the file system driver: antivirus, encryption, replication, compression, Hierarchical Storage 
Management (HSM), and so on. The DAX driver model significantly modifies how DAX volumes interact 
with such components.
As previously discussed in this chapter, when a file is mapped in memory, the file system in DAX 
mode does not receive any read or write I/O requests, neither do all the filter drivers that reside above 
or below the file system driver. This means that filter drivers that rely on data interception will not work. 
To minimize possible compatibility issues, existing minifilters will not receive a notification (through the 
InstanceSetup callback) when a DAX volume is mounted. New and updated minifilter drivers that still 
want to operate with DAX volumes need to specify the FLTFL_REGISTRATION_SUPPORT_DAX_VOLUME
flag when they register with the filter manager through FltRegisterFilter kernel API.
Minifilters that decide to support DAX volumes have the limitation that they can’t intercept any 
form of paging I/O. Data transformation filters (which provide encryption or compression) don’t 
have any chance of working correctly for memory-mapped files; antimalware filters are impacted as 
CHAPTER 11
Caching and file systems
731
described earlier—because they must now perform scans on every open and close, losing the ability to 
determine whether or not a write truly happened. (The impact is mostly tied to the detection of a file 
last update time.) Legacy filters are no longer compatible: if a driver calls the IoAttachDeviceToDevice
Stack API (or similar functions), the I/O manager simply fails the request (and logs an ETW event).
Flushing DAX mode I/Os
Traditional disks (HDD, SSD, NVme) always include a cache that improves their overall performance. 
When write I/Os are emitted from the storage driver, the actual data is first transferred into the cache, 
which will be written to the persistent medium later. The operating system provides correct flushing, 
which guarantees that data is written to final storage, and temporal order, which guarantees that data 
is written in the correct order. For normal cached I/O, an application can call the FlushFileBuffers API to 
ensure that the data is provably stored on the disk (this will generate an IRP with the IRP_MJ_FLUSH_
BUFFERS major function code that the NTFS driver will implement). Noncached I/O is directly written to 
disk by NTFS so ordering and flushing aren’t concerns.
With DAX-mode volumes, this is not possible anymore. After the file is mapped in memory, the 
NTFS driver has no knowledge of the data that is going to be written to disk. If an application is writing 
some critical data structures on a DAX volume and the power fails, the application has no guarantees 
that all of the data structures will have been correctly written in the underlying medium. Furthermore, 
it has no guarantees that the order in which the data was written was the requested one. This is 
because PM storage is implemented as classical physical memory from the CPU’s point of view. The 
processor uses the CPU caching mechanism, which uses its own caching mechanisms while reading or 
writing to DAX volumes.
As a result, newer versions of Windows 10 had to introduce new flush APIs for DAX-mapped regions, 
which perform the necessary work to optimally flush PM content from the CPU cache. The APIs are 
available for both user-mode applications and kernel-mode drivers and are highly optimized based 
on the CPU architecture (standard x64 systems use the CLFLUSH and CLWB opcodes, for example). An 
application that wants I/O ordering and flushing on DAX volumes can call RtlGetNonVolatileToken on 
a PM mapped region; the function yields back a nonvolatile token that can be subsequently used with 
the RtlFlushNonVolatileMemory or RtlFlushNonVolatileMemoryRanges APIs. Both APIs perform the 
actual flush of the data from the CPU cache to the underlying PM device. 
Memory copy operations executed using standard OS functions perform, by default, temporal copy 
operations, meaning that data always passes through the CPU cache, maintaining execution ordering. 
Nontemporal copy operations, on the other hand, use specialized processor opcodes (again depend-
ing on the CPU architecture; x64 CPUs use the MOVNTI opcode) to bypass the CPU cache. In this case, 
ordering is not maintained, but execution is faster. RtlWriteNonVolatileMemory exposes memory copy 
operations to and from nonvolatile memory. By default, the API performs classical temporal copy op-
erations, but an application can request a nontemporal copy through the WRITE_NV_MEMORY_FLAG_
NON_ TEMPORAL flag and thus execute a faster copy operation. 
732
CHAPTER 11
Caching and file systems
Large and huge pages support
Reading or writing a file on a DAX-mode volume through memory-mapped sections is handled by the 
memory manager in a similar way to non-DAX sections: if the MEM_LARGE_PAGES flag is specified at 
map time, the memory manager detects that one or more file extents point to enough aligned, contigu-
ous physical space (NTFS allocates the file extents), and uses large (2 MB) or huge (1 GB) pages to map the 
physical DAX space. (More details on the memory manager and large pages are available in Chapter 5 of 
Part 1.) Large and huge pages have various advantages compared to traditional 4-KB pages. In particular, 
they boost the performance on DAX files because they require fewer lookups in the processor’s page 
table structures and require fewer entries in the processor’s translation lookaside buffer (TLB). For ap-
plications with a large memory footprint that randomly access memory, the CPU can spend a lot of time 
looking up TLB entries as well as reading and writing the page table hierarchy in case of TLB misses. In ad-
dition, using large/huge pages can also result in significant commit savings because only page directory 
parents and page directory (for large files only, not huge files) need to be charged. Page table space (4 KB 
per 2 MB of leaf VA space) charges are not needed or taken. So, for example, with a 2-TB file mapping, the 
system can save 4 GB of committed memory by using large and huge pages.
The NTFS driver cooperates with the memory manager to provide support for huge and large pages 
while mapping files that reside on DAX volumes: 
I 
By default, each DAX partition is aligned on 2-MB boundaries.
I 
NTFS supports 2-MB clusters. A DAX volume formatted with 2-MB clusters is guaranteed to use
only large pages for every file stored in the volume.
I 
1-GB clusters are not supported by NTFS. If a file stored on a DAX volume is bigger than 1 GB,
and if there are one or more file’s extents stored in enough contiguous physical space, the
memory manager will map the file using huge pages (huge pages use only two pages map
levels, while large pages use three levels).
As introduced in Chapter 5, for normal memory-backed sections, the memory manager uses large 
and huge pages only if the extent describing the PM pages is properly aligned on the DAX volume. 
(The alignment is relative to the volume’s LCN and not to the file VCN.) For large pages, this means 
that the extent needs to start at at a 2-MB boundary, whereas for huge pages it needs to start at 1-GB 
boundary. If a file on a DAX volume is not entirely aligned, the memory manager uses large or huge 
pages only on those blocks that are aligned, while it uses standard 4-KB pages for any other blocks.
In order to facilitate and increase the usage of large pages, the NTFS file system provides the FSCTL_
SET_DAX_ALLOC_ALIGNMENT_HINT control code, which an application can use to set its preferred 
alignment on new file extents. The I/O control code accepts a value that specifies the preferred align-
ment, a starting offset (which allows specifying where the alignment requirements begin), and some 
flags. Usually an application sends the IOCTL to the file system driver after it has created a brand-new 
file but before mapping it. In this way, while allocating space for the file, NTFS grabs free clusters that 
fall within the bounds of the preferred alignment.
If the requested alignment is not available (due to volume high fragmentation, for example), the 
IOCTL can specify the fallback behavior that the file system should apply: fail the request or revert to a 
fallback alignment (which can be specified as an input parameter). The IOCTL can even be used on an 
CHAPTER 11
Caching and file systems
733
already-existing file, for specifying alignment of new extents. An application can query the alignment 
of all the extents belonging to a file by using the FSCTL_QUERY_FILE_REGIONS control code or by using 
the fsutil dax ueryfilealignment command-line tool.
EXPERIMENT: Playing with DAX file alignment
You can witness the different kinds of DAX file alignment using the FsTool application available 
in this book’s downloadable resources. For this experiment, you need to have a DAX volume 
present on your machine. Open a command prompt window and perform the copy of a big file 