title:On selfish routing in internet-like environments
author:Lili Qiu and
Yang Richard Yang and
Yin Zhang and
Scott Shenker
On Selﬁsh Routing in Internet-Like Environments
Lili Qiu
Yang Richard Yang ∗
Microsoft Research
Yale University
PI:EMAIL PI:EMAIL
Yin Zhang
AT&T Labs – Research
Scott Shenker †
ICSI
PI:EMAIL PI:EMAIL
ABSTRACT
A recent trend in routing research is to avoid inefﬁciencies in network-
level routing by allowing hosts to either choose routes themselves
(e.g., source routing) or use overlay routing networks (e.g., Detour
or RON). Such approaches result in selﬁsh routing, because routing
decisions are no longer based on system-wide criteria but are in-
stead designed to optimize host-based or overlay-based metrics. A
series of theoretical results showing that selﬁsh routing can result in
suboptimal system behavior have cast doubts on this approach. In
this paper, we use a game-theoretic approach to investigate the per-
formance of selﬁsh routing in Internet-like environments. We focus
on intra-domain network environments and use realistic topologies
and trafﬁc demands in our simulations. We show that in contrast
to theoretical worst cases, selﬁsh routing achieves close to optimal
average latency in such environments. However, such performance
beneﬁt comes at the expense of signiﬁcantly increased congestion
on certain links. Moreover, the adaptive nature of selﬁsh overlays
can signiﬁcantly reduce the effectiveness of trafﬁc engineering by
making network trafﬁc less predictable.
Categories and Subject Descriptors
C.2.5 [Computer-Communication Networks]: Local and Wide-
Area Networks—Internet
General Terms
Performance
Keywords
Selﬁsh Routing, Overlay, Game Theory, Trafﬁc Equilibrium, Trafﬁc
Engineering, Optimization, Relaxation
1.
INTRODUCTION
For decades, it has been the responsibility of the network to route
trafﬁc. Recent studies [32, 40] have shown that there is inherent in-
efﬁciency in network-level routing from the user’s perspective. In
response to these observations, we have seen an emergent trend to
allow end hosts to choose routes themselves by using either source
∗
†
0121555, ITR-0081698, ITR-0225660 and ANI-0196514.
Supported in part by NSF grant ANI-0207399.
Supported in part by NSF grants ITR-0205519, ANI-0207399, ITR-
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’03, August 25–29, 2003, Karlsruhe, Germany.
Copyright 2003 ACM 1-58113-735-4/03/0008 ...$5.00.
routing (e.g., Nimrod [8]) or overlay routing (e.g., Detour [32] or
RON [5]). These end-to-end route selection schemes are shown to
be effective in addressing some deﬁciencies in today’s IP routing.
For example, measurements [10, 32, 33] from the Detour project
show that in the Internet, a large percentage of ﬂows can ﬁnd better
alternative paths by relaying among overlay nodes, thereby improv-
ing their performance. RON [5] also demonstrates the beneﬁt of
overlay routing using real implementation and deployment.
Such end-to-end route selection schemes are selﬁsh by nature in
that they allow end users to greedily select routes to optimize their
own performance without considering the system-wide criteria. Re-
cent theoretical results suggest that in the worst case selﬁsh routing
can result in serious performance degradation due to lack of cooper-
ation. In particular, Roughgarden and Tardos prove that the price of
anarchy (i.e., the worst-case ratio between the total latency of self-
ish routing and that of the global optimal) for selﬁsh routing can be
unbounded for general latency functions [31].
Despite much theoretical advance, an open question is how self-
ish routing performs in Internet-like environments. This is a chal-
lenging question, since today’s Internet is unique in the following
respects.
First, topologies and trafﬁc demands of the Internet are not ar-
bitrary but have certain structures. The worst-case results may not
be applicable to realistic topologies and trafﬁc demands. A general
open question is whether selﬁsh routing results in bad performance
in Internet-like environments (i.e., under realistic network topolo-
gies and trafﬁc demands).
Second, users in overlay networks do not have full ﬂexibility in
specifying their end-to-end paths. Due to limited availability of
source routing support in the routers, the path between any two
network nodes is dictated by the Internet routing protocols, such
as OSPF [28], MPLS [26], or BGP [37]. While overlay networks
provide another mechanism to enable users to control their routes
by relaying through overlay nodes, the route between two overlay
nodes is still governed by the underlying routing protocol. A natural
question is how to model such selﬁsh overlay routing and whether
selﬁsh overlay routing results in bad performance.
Third, even if selﬁsh overlays (i.e., overlays consisting of selﬁsh
trafﬁc) yield good performance, they can only be deployed gradu-
ally. As a result, background trafﬁc and overlay trafﬁc will interact
with each other. We call such interactions horizontal interactions.
An important question is how such selﬁsh trafﬁc affects the remain-
ing trafﬁc routed using the traditional routing protocols. A related
question is whether multiple overlays result in bad performance.
Fourth, the way in which selﬁsh users choose their routes can in-
teract with trafﬁc engineering. We call such interactions vertical
interactions, which can be viewed as the following iterative pro-
cess. First, ISPs adjust network-level routing according to trafﬁc
demands, using schemes in [6, 14, 15, 42], to minimize network
cost. Then selﬁsh users adapt to changes in the underlying default
routes by choosing different overlay paths to optimize their end-
to-end performance. Such adaptation changes trafﬁc demands and
triggers trafﬁc engineering to readjust the default routes, which in
turn makes selﬁsh users adapt to new routes. Given the mismatch
between the objectives of selﬁsh routing and trafﬁc engineering, an
interesting question is whether selﬁsh routing interacts badly with
trafﬁc engineering.
In this paper, we seek to answer the above questions through ex-
tensive simulations. We take a game-theoretic approach to com-
pute the trafﬁc equilibria of various routing schemes and then eval-
uate their performance. We focus on intra-domain network envi-
ronments because recent advances in topology mapping [36] and
trafﬁc estimation [44] allow us to use realistic network topologies
and trafﬁc demands for such scenarios. Understanding selﬁsh rout-
ing in inter-domain environments is also of great interest but will be
more challenging. First, we do not have realistic models for inter-
domain trafﬁc demands. Moreover, despite some recent progress
towards understanding autonomous system relationships [17, 38],
more research efforts are needed to develop realistic models for
inter-domain routing policies. Finally, the large size of inter-domain
topologies makes it computationally prohibitive to derive trafﬁc equi-
libria. Due to these difﬁculties, we defer it to future work.
Our key contributions and results can be summarized as follows.
First, we formulate and evaluate selﬁsh routing in an overlay net-
work. Selﬁsh routing in an overlay network is different from tra-
ditional selﬁsh source routing in that (i) the route between any two
overlay nodes is dictated by network-level routing, and (ii) differ-
ent overlay links may share common physical links and therefore
traditional algorithms to compute trafﬁc equilibria do not apply.
Second, we ﬁnd that in contrast to theoretical worst cases, selﬁsh
routing in Internet-like environments yields close to optimal average
latency, which can be much lower than that of default network-level
routing. This is true for both source routing and overlay routing.
Moreover, we show selﬁsh routing achieves good performance with-
out hurting the trafﬁc that is using default network-level routing.
Third, we show that the primary impact of selﬁsh routing on
Internet-like environments is the fundamental mismatch between
the objectives of selﬁsh routing and trafﬁc engineering. In partic-
ular, our results show that the low latency of selﬁsh routing is often
achieved at the expense of increased congestion on certain links.
Moreover, the adaptive nature of selﬁsh routing makes trafﬁc de-
mands less predictable and can signiﬁcantly reduce the effectiveness
of trafﬁc engineering.
The rest of the paper is organized as follows. In Section 2, we re-
view related work. In Section 3, we present our network model. In
Section 4, we specify the routing schemes we evaluate and present
the algorithms we use to compute their trafﬁc equilibria. In Sec-
tion 5, we describe our evaluation methodology. We study the per-
formance of selﬁsh source routing in Section 6 and that of selﬁsh
overlay routing in Section 7. In Section 8 and Section 9, we investi-
gate horizontal and vertical interactions, respectively. We conclude
in Section 10.
2. RELATED WORK
A number of recent studies have reported that network-level rout-
ing is inefﬁcient from the user’s perspective. For example, Savage
et al. [33] use Internet measurements to show that the default rout-
ing path is often suboptimal in terms of latency, loss rate, and TCP
throughput. The suboptimal performance of network-level routing
is inevitable due to routing hierarchy and policy [40], as well as dif-
ferent routing objectives used by network operators, whose goal is
to avoid high utilization. Moreover, stability problems with routing
protocols, such as BGP [37], could make things even worse. As a
result, there has been a movement to give users more autonomy in
choosing their routes by using source routing (e.g., Nimrod [8]) or
overlay routing networks (e.g., Detour [32, 33] and RON [5]).
Recently a series of theoretical results show that selﬁsh routing
can result in extremely suboptimal performance in worst cases. The
pioneering work in this area is by Koutsoupias and Papadimitriou [22],
who compare the worst-case Nash equilibrium with a global optimal
solution in minimizing network congestion in a two-node network.
Roughgarden and Tardos are interested in a different performance
metric – latency. In [31], they prove that the price of anarchy (i.e.,
the worst-case ratio between the average latency of a Nash equi-
librium and that of the global optimal) depends on the “steepness”
of the network latency functions. They show that the price of an-
archy is unbounded for a general latency function such as M/M/1.
In contrast to the theoretical studies, our study focuses on a prac-
tical setting, by using realistic network topologies and trafﬁc de-
mands; different from the measurement studies, our study considers
a more general setting and investigates networks with a large amount
of selﬁsh trafﬁc, under different network conﬁgurations (including
both static and dynamic network controls).
Although the price of anarchy can be high in the worst-case, some
theoretical studies have also shown that the degradation is less se-
vere from some other perspectives. For example, Friedman shows
that for “most” trafﬁc rate vectors in a range, the price of anarchy
is lower than that of the worst cases [16]. He also analyzes the ef-
fects of TCP rate adaptation in a parallel-link network and shows
that the performance loss is small. Roughgarden and Tardos [31]
show (essentially) that the performance degradation due to selﬁsh
routing can be compensated for by doubling the bandwidth on all
links. However, this is often not a practical option for the Internet at
least in the short-term.
There are also other ways in which end users can selﬁshly opti-
mize the performance of their trafﬁc. For example, a user can greed-
ily inject trafﬁc into a network. A number of papers (e.g., [2, 35])
consider such a congestion game. In practice, it is possible to have a
hybrid game that consists of a route selection game and a congestion
game, but we defer it to future work.
3. NETWORK MODEL
In this section, we describe our network model, especially the
network-level routing protocols. In the next section, we describe the
schemes of how trafﬁc demands are routed through the network. In
Section 5, we describe the network topologies, trafﬁc demands, and
latency functions that we use to instantiate our network model.
Physical network: We study the performance of realistic physical
networks. We model a physical network as a directed graph G =
(V, E), where V is the set of nodes, and E the set of directed links.
We assume that the latency of each physical link is a function of
its load. The exact latency functions we use will be described in
Section 5.3.
Demands: We partition network trafﬁc into demands. A demand
represents a given amount of trafﬁc from a source to a destination.
In particular, we identify a special type of demand, called inﬁnites-
imal demand. A collection of inﬁnitesimal demands models a large
aggregation of independent, small transactions such as web transac-
tions, and the generator of each transaction makes an independent
decision.
Overlays: An overlay consists of overlay nodes, directed overlay
links, and a set of demands originated from the overlay nodes. The
overlay nodes agree to forward each other’s trafﬁc along one or more
overlay links. The physical route for an overlay link is dictated
by network-level routing and may involve multiple physical links.
Different overlay links may share one or more physical links. The
overlay nodes and overlay links form the overlay topology. To limit
the parameter space, we only consider the fully connected overlay
topology in this work. That is, we assume there is an overlay link
between every pair of overlay nodes. We plan to investigate the ef-
fects of different overlay topologies in our future work.
Users: We assume that the network consists of a collection of users.
Each user decides how its trafﬁc should be routed. The objective of
a user is to minimize the average latency of its trafﬁc. We choose
to use latency as the optimization objective of selﬁsh routing for the
following reasons: 1) many applications such as short Web transfers
and IP telephony require low latency; 2) most previous theoretical
analyses are based on latency, and one of the major objectives of this
study is to investigate whether the theoretical worst-case results ap-
ply to Internet-like environments. We plan to investigate the effects
of alternative routing objectives (e.g., loss [3]) in our future work.
Route controller: Besides users, we also have a route controller,
which controls the network-level routing in the physical network.
(We use network-level routing and physical routing interchangeably
in this paper.) We consider several types of network-level routing.
We assume that the route controller uses a routing protocol based on
either OSPF[28], which uses shortest-path with equal-weight split-
ting, or MPLS[26], which uses the more general multi-commodity
ﬂow routing. For OSPF routing, we consider three weight assign-
ments:
physical link;
to each physical link;
• Hop-count OSPF routing, which assigns a unit weight to each
• Random-weight OSPF routing, which assigns a random weight
• Optimized-compliant OSPF routing, which has OSPF weights
set to minimize network cost [14] (see Section 5.4), when
assuming all trafﬁc is compliant, following the routes deter-
mined by the network. The network cost is a piece-wise linear
convex function over all links. This metric has been consid-
ered as a good objective for trafﬁc engineering because it not
only avoids overloading physical links, but also avoids taking
very long paths [14, 15].
We represent network-level routing by a routing matrix R, where
R[p, e] speciﬁes the fraction of trafﬁc between the source-destination
pair p that goes through the physical link e. The routing matrix R is
computed by the routing protocol under study.
In our study, the route controller can change network routing to
optimize overall network performance; in other words, it can per-
form trafﬁc engineering. For MPLS, the route controller can di-
rectly adjust the routing matrix R; for OSPF, the route controller
will adjust the weights of the physical links to inﬂuence network
routing [14, 15].
4. ROUTING AND TRAFFIC EQUILIBRIA
We evaluate each selﬁsh routing scheme by computing its per-
formance at trafﬁc equilibria. Using a game-theoretic approach, we
deﬁne a trafﬁc equilibrium as a state where no user can improve the
latency of its trafﬁc by unilaterally changing the amount of trafﬁc it
sends along different network paths. One possible way of comput-
ing trafﬁc equilibria is through simulation. More speciﬁcally, one
could simulate the moves of each individual user and wait until the
system reaches equilibrium. However, given the size of the net-
work we are considering (see Section 5.1), such simulation-based
approach may take a prohibitively long time to converge. Instead,
we compute trafﬁc equilibria directly. Below we introduce the rout-
ing schemes, and specify the algorithms we use to compute the traf-
ﬁc equilibria. See Appendix for further details on the algorithms.
For a comprehensive study, we consider the following ﬁve routing
schemes: (i) source routing, (ii) optimal routing, (iii) overlay source
routing, (iv) overlay optimal routing, and (v) compliant routing. Be-
low we describe these routing schemes in details.
4.1 Routing on the physical network
The ﬁrst two routing schemes allow a user to route its trafﬁc di-
rectly through any paths on the physical network.
Source routing: Source routing results in selﬁsh routing, since
the source of the trafﬁc makes an independent decision about how
the trafﬁc should be routed. The selﬁsh routing scheme studied in
most previous theoretical work is source routing.
Optimal routing: Optimal routing refers to latency optimal rout-
ing. It models a scenario where a single authority makes the routing
decision for all the demands to minimize the average latency.
A traditional algorithm to compute the trafﬁc equilibria of source
routing and optimal routing is the linear approximation algorithm, a
variant of the well-known Frank-Wolfe algorithm [13, 29, 34] (see
Appendix for more details).
4.2 Overlay routing
The next two routing schemes are the overlay versions of source
routing and optimal routing.
Overlay source routing: Overlay source routing is selﬁsh rout-
ing through overlay nodes. Similar to source routing, it is the trafﬁc
source that controls the routes.
Overlay optimal routing: Overlay optimal routing refers to over-
lay latency optimal routing. It models a scenario where the demands
in the overlay have complete cooperation in minimizing the average
latency.
As mentioned in Section 1, overlay routing is different from rout-
ing directly on the physical network. In particular, the physical route
for an overlay link is dictated by network-level routing and may in-
volve multiple physical links. Moreover, different overlay links may
share common physical links and therefore may interfere with each
other. Therefore, we cannot apply the traditional linear approxima-
tion algorithms to compute trafﬁc equilibria for such schemes.
We use the following approach to compute trafﬁc equilibria for
overlay routing. For each overlay, we build a logical network from
the physical network. The nodes in the logical network consist of
the union of the nodes in the overlay and the nodes that are the
destinations of nonzero demands in the overlay. The links in the
logical network consist of all the overlay links, as well as a link
from each overlay node to each node that is the destination of some
trafﬁc demands but does not belong to the overlay.
Given this model, each logical link can be mapped to a collection
of physical links. More speciﬁcally, assume that the logical link p
is for the source-destination pair p (we use the same symbol p to
denote the logical link p and the source-destination pair p), then the
logical link consists of all the physical links e such that R[p, e] > 0.
If a demand sends f units of trafﬁc through a logical link p, then
each physical link e will carry f · R[p, e] amount of trafﬁc for this
demand. Figure 1 shows an example of a physical network, and the
logical network for an overlay formed by nodes 2, 3, and 5.
2
8
3
1
9
4
3
7
6
2
7
6
5
5
(a) Physical Network
(b) Logical Network of an overlay
Figure 1: A physical network and the logical network for the
overlay formed by nodes 2, 3, and 5. Nodes 6 and 7 are not
overlay nodes but nodes 2, 3, and 5 have demands to them. The
logical link from node 2 to 5 consists of two physical paths: 2 to
9 to 5, and 2 to 8 to 5, if hop-count OSPF routing is used.
Using such logical networks, we can compute the trafﬁc equilibria
of overlay routing by either a modiﬁed linear approximation algo-
rithm or a relaxation algorithm (see Appendix for details). When
there are multiple overlays, we use the relaxation framework pro-
posed in [23, 41] to ensure convergence (see Appendix for details).
4.3 Compliant routing
For comparison, we also consider the default network-level rout-
ing, which we term compliant routing.
Compliant routing: Trafﬁc demands using compliant routing
follow the routes determined by the network-level routing protocol.
5. EVALUATION METHODOLOGY
In this section, we ﬁrst describe the network topologies, trafﬁc
demands, and link latency functions used in our evaluation. Then we
discuss the performance metrics that we use as a basis for comparing
the efﬁciency of different routing schemes.
5.1 Network topologies
We use both real and synthetic topologies in our evaluation.
Real topology: We use a real router-level backbone topology
from an operational tier-1 ISP, referred to as ISP T opo, with on
the order of a hundred backbone routers connected by OC48 (i.e.,
2.48 Gbps) and OC192 (i.e., 10 Gbps) links (the exact numbers are
omitted for proprietary reasons). For each link in the real topology,
we use the actual link capacity in our study. The propagation delay
of each link is estimated using the actual ﬁber length divided by the
speed of light.
Rocketfuel topologies: Rocketfuel applies several effective tech-
niques to obtain fairly complete ISP maps [36]. We use the POP-
level maps published by the authors, shown in Table 1, as part of
our topologies. For each Rocketfuel topology, we use two band-
width settings: all links are either OC3 (i.e., 155 Mbps) or OC48
(i.e., 2.48 Gbps). The propagation delay of each link is approxi-
mated using geographical distance divided by the speed of light.
ISP
ATT
Abovenet
Exodus
Level3
Sprint
Verio
EBONE
Telstra
Tiscali
#Nodes
#Non-leaf
Nodes
#Edges
108
22
22
53
44
122
28
58
51
30
13
17
37
21
82
25
8
38
282
160
102
912
212
620
132
120
258
Loc.
US
US
US
US
US
US
Intl.
Intl.
Intl.
Table 1: ISP topologies as measured by Rocketfuel.
Random topologies: In addition to real topologies, for diversity
we also randomly generate power-law topologies using BRITE [25],
since a number of papers [11, 39] have shown that the power-laws
capture the Internet structure quite well. We generate 100-node
router-level topologies with edge density (i.e., the number of neigh-
boring nodes that each new node connects to) varying from 2 to 10.
In the following sections, we use PowerDn to denote a power-law
topology with edge density n. For each power-law topology, we use
two bandwidth settings: all links are either OC3 or OC48. The prop-
agation delay of each link is drawn uniformly between 0 − 10 ms.
5.2 Trafﬁc demands
We use both real and synthetic trafﬁc demands in our evaluation.
Real trafﬁc demands: Our real trafﬁc demands are estimated
from SNMP link data using the tomogravity method [44], which has
been shown to yield accurate estimates especially for large trafﬁc
matrix elements. We use the backbone router to backbone router
trafﬁc matrices during three randomly chosen hours in November
2002.
Synthetic trafﬁc demands: The real trafﬁc demands are only
available for ISP T opo. For the other topologies, we generate syn-
thetic trafﬁc demands as follows. For a Rocketfuel topology, we
generate synthetic trafﬁc by randomly mapping POPs in ISP T opo
to non-leaf nodes in the Rocketfuel topology, using several differ-
ent random seeds. Speciﬁcally, let m(.) denote a random mapping
from the cities in ISP T opo to those in a Rocketfuel topology. Let
T (s, d) denote the trafﬁc demand from city s to city d in ISP T opo.
Then the trafﬁc demand from city m(s) to city m(d) in the topology
under study is set to T (s, d). For synthetic power-law topologies,
we perform similar mappings at the router level to derive demands.
Load scale factor: To control system load, we scale up the de-
mands so that when all the trafﬁc is compliant and routed based on
shortest hop-count, the maximum link utilization is 100·F %, where
F is a load scale factor (sometimes abbreviated as LSF ).
5.3 Link latency functions
As shown in [30], link latency functions play an important role in
determining the effectiveness of selﬁsh routing. In our evaluations,
we use ﬁve representative latency functions: M/M/1, M/D/1 [18],
P/M/1, P/D/1 [19], and BPR [9]. We also implement piecewise-
linear, increasing, convex functions to approximate any other la-
tency functions.
In all latency functions, we include a term for
propagation delay (Section 5.1 shows how we determine its value
for each physical link).
+ x·(1+σ2µ2)
2µ(µ−x)
Our ﬁrst two latency functions belong to the general M/G/1 class
of latency functions: M/M/1 and M/D/1. For a M/G/1 queue, the
latency can be expressed as l(x) = 1
+ prop, where
µ
x is the trafﬁc load, µ the link capacity, σ the standard deviation
of the service time, and prop the propagation delay. The M/M/1
latency function is M/G/1 with σ = 1
+
prop. The M/D/1 latency function is M/G/1 with σ = 0; therefore
l(x) = 0.5
+ prop. To avoid the discontinuity when the load
µ−x
approaches capacity, we approximate the M/M/1 or M/D/1 function
with a linear function beyond 99% utilization. To test sensitivity
to the threshold, we also try 90% and 99.9%. The results are very
similar, and in the interest of brevity we present the results using
99% as the threshold.
µ ; therefore l(x) = 1
µ−x
+ 0.5
µ
Our next two latency functions, P/M/1 and P/D/1, have heavy-
tail inter-arrival times. Here P stands for Pareto. We set the shape
parameter β = 1.5 so that the resulting distribution has inﬁnite vari-
ance. Since there is no closed-form expression for either P/M/1 or
P/D/1, we approximate each of them using a piecewise-linear, in-
creasing, convex function. We use the results in [19] to approximate
P/M/1. For P/D/1, we derive a linear approximation of its shape
using ns-2 [27] simulations. Speciﬁcally, we generate Pareto trafﬁc
to compete for a single bottleneck link with a large FIFO drop-tail
queue and observe the latency as we vary the load.
For comparison purposes, we also run some experiments with
the latency function BPR [9], which is used as a standard latency
function in transportation networks. The expression for this latency
function is l(x) = prop·
. Table 2 summarizes the
above ﬁve latency functions.
1 + 0.15 ( x
µ
)4
(cid:1)
(cid:2)
Notation Latency function
M/M/1
M/D/1
P/M/1
P/D/1
BPR
l(x) = 1
µ−x
l(x) = 0.5
µ−x
approx. with Pareto β = 1.5, see [19]
approx. with Pareto β = 1.5
l(x) = prop ·
)4
+ prop
+ 0.5
µ
(cid:1)
1 + 0.15 ( x
µ
+ prop
(cid:2)
Table 2: Link latency functions.
5.4 Performance metrics
We use the following performance metrics to evaluate routing
efﬁciency: (i) average latency, (ii) maximum link utilization, and
(iii) network cost. The ﬁrst metric reﬂects end-to-end user perfor-
mance, while the next two reﬂect the perspective of network op-
erators, who aim to avoid link overloads in their networks. These
performance metrics are computed from trafﬁc equilibria, as we dis-
cussed in the previous section.
The utilization of a link is the amount of trafﬁc on the link di-
vided by its capacity. When a link utilization is beyond 100%, the
link is overloaded. The maximum link utilization is the maximum
utilization over all links in a network.
The maximum link utilization is an intuitive metric; however, it
is dominated by a single bottleneck, as pointed out in [14]. To get a
more complete picture, we also adopt a metric to capture the over-
all network cost. According to [14, 15], the cost of a link can be
modeled using a piecewise-linear, increasing, convex function with
slopes speciﬁed as follows:
ue(x/c) =
1 : x/c ∈ [0, 1/3)
3 : x/c ∈ [1/3, 2/3)
10 : x/c ∈ [2/3, 9/10)
70 : x/c ∈ [9/10, 1)
500 : x/c ∈ [1, 11/10)
5000 : x/c ∈ [11/10, ∞),
where x is the load on link e, and c its capacity. We refer to the
points at which the slope changes (e.g., 1/3 and 2/3) as the cut-
points. The overall network cost is the sum of all links’ costs.