error-prone, in the sense that it classiﬁes a small fraction
of code paths incorrectly. Fortunately, for many types of
victim operations, the ﬁne granularity achieved by the at-
tack VM’s IPI-based spying can yield multiple observations
per individual operation. We use the redundancy in these
observations together with knowledge of the set of possible
victim code paths to correct SVM output errors by means of
a hidden Markov model (HMM). This is detailed in Sec. 5.2.
The SVM plus HMM combination, when correctly trained,
can translate a sequence of observations into a sequence of
inferred operations with few errors.
Challenge 3: Core migration. Our SMP setting has at-
tacker and victim VCPUs ﬂoat amongst the various PCPUs.
The administrative Dom0 VM and any other VMs may also
ﬂoat amongst them. This gives rise to two hurdles. First, we
must determine whether an observation is associated with
the victim or some other, unrelated VCPU. Second, we will
only be able to spy on the victim when assigned to the same
PCPU, which may coincide with only some fraction of the
victim’s execution.
In Sec. 5.2, we describe how the HMM mentioned above
can be modiﬁed to ﬁlter out sequences corresponding to un-
related observations. In Sec. 5.3, we provide a dynamic pro-
gramming algorithm, like those in bioinformatics, to “stitch”
together multiple inferred code-path fragments output by
the SVM+HMM and thereby construct fuller hypothesized
code-paths. By observing multiple executions of the vic-
tim, we can gather suﬃciently many candidate sequences
to, through majority voting, output a full code path with
negligible errors.
Putting it all together. Our full attack pipeline is de-
picted in Fig. 1. The details of our measurement stage that
addresses the ﬁrst challenge described above are presented
in Sec. 4. The analysis of these measurements to address
the second and third challenges is then broken down into
three phases: cache-pattern classiﬁcation (Sec. 5.1), noise
reduction (Sec. 5.2), and code-path reassembly (Sec. 5.3).
307Figure 1: Diagram of the main steps in our side-channel attack.
4. CROSS-VM SIDE CHANNELS
In this section, we demonstrate how an access-driven cross-
VM side channel can be constructed on the L1 instruction
cache in a modern x86 architecture running Xen.
been little activity from V . Of course, Probe-ing also ac-
complishes Prime-ing the cache sets (i.e., evicting all in-
structions other than U ’s), and so repeatedly Probe-ing,
with one prime-probe interval between each Probe, elim-
inates the need to separately conduct a Prime step.
4.1 Instruction Cache Spying
4.2 Preempting the Victim
Caches are usually set-associative. A w-way set-associative
cache is partitioned into m sets, each with w lines of size b.
So, if the size of the cache is denoted by c, we have m =
c/(w × b). For example, in the L1 instruction cache of an In-
tel Yorkﬁeld processor as used in our lab testbed, c = 32KB,
w = 8, b = 64B. Hence, m = c/(w × b) = 64. Moreover, the
number of cache-line-sized memory blocks in a 4KB mem-
ory page is 4KB/b = 64. Therefore, memory blocks with the
same oﬀset in a memory page will be mapped to the same
cache set.
Probing the instruction cache. A basic technique for
I-cache side-channels is timing how long it takes to read
data from memory associated with individual cache sets, as
described previously by Acii¸cmez [1]. To do so, we ﬁrst allo-
cate suﬃciently many contiguous memory pages so that their
combined size is equal to the size of the I-cache. We then di-
vide each memory page into 64 cache-line-sized blocks. The
ith data block in each page will map to the same cache set.
To ﬁll the cache set associated to oﬀset i, then, it suﬃces
to execute an instruction within the ith block of each of the
allocated pages. Filling a cache set is called Prime-ing. We
will also want to measure the time it takes to ﬁll a cache set,
this is called Probe-ing. To Probe the cache set associated
with oﬀset i, we executes the rdtsc instruction, then jumps
to the ﬁrst page’s ith block, which has instructions to jump
to the ith block of the next page, and so on. The ﬁnal page
jumps back to code that again executes rdtsc and calculates
the elapsed time. This is repeated for each of the m cache
sets to produce a vector of cache set timings.
The prime-probe protocol. A common method for con-
ducting an access-driven cache attack is to Prime and later
Probe the cache, a so-called prime-probe protocol, as in-
troduced by Osvik et al. [33]. More speciﬁcally, a VCPU
U of the attacker’s VM spies on a victim’s VCPU V by
measuring the cache load in the L1 instruction cache in the
following manner:
Prime: U ﬁlls one or more cache sets by the method
described above.
Idle: U waits for a prespeciﬁed prime-probe interval
while the cache is utilized by V .
Probe: U times the duration to reﬁll the same cache sets
to learn V ’s cache activity on those sets.
Cache activity induced by V during U ’s prime-probe in-
terval will evict U ’s instructions from the cache sets and re-
place them with V ’s. This will result in a noticeably higher
timing measurement in U ’s Probe phase than if there had
A fundamental diﬃculty in an I-cache attack without SMT
support is for the attacker VCPU to regain control of the
PCPU resource suﬃciently frequently (i.e., after the desired
prime-probe interval has passed). To accomplish this, we
leverage the tendency of the Xen credit scheduler to give the
highest run priority to a VCPU that receives an interrupt.
That is, upon receiving an interrupt, the attacker VCPU
will preempt another guest VCPU running on the PCPU,
provided that it is not also running with that highest pri-
ority (as a compute-bound victim would not be). As such,
our attack strategy is to deliver an interrupt to the attacker
VCPU every prime-probe interval.
We consider three types of interrupts to “wake up” the
attacking VCPU: timer interrupts, network interrupts and
inter-processor interrupts (IPIs).1 Timer interrupts in a
guest OS can be conﬁgured to be raised with a frequency
of at most 1000Hz, but this is not suﬃciently granular for
our attack targets (e.g., a cryptographic key). Network in-
terrupts, as used in OS level CPU-cycle stealing attacks by
Tsafrir et al. [44], can achieve higher resolution, but in our
experiments the delivery times of network interrupts varied
due to batching and network eﬀects, rendering it hard to
achieve microsecond-level granularity.
We therefore turn to IPIs. In SMP systems, an IPI allows
one processor to interrupt another processor or even itself. It
is usually issued through an advanced programmable inter-
rupt controller (APIC) by one core and passed to other cores
via either the system bus or the APIC bus. To leverage IPIs
in our attack, another attacker VCPU, henceforth called the
IPI VCPU , executes an endless loop that issues IPIs to the
attacker VCPU which is conducting the prime-probe pro-
tocol, henceforth called the probing VCPU . This approach
works generally well but is limited by two shortcomings.
First, due to interrupt virtualization by Xen, the prime-
probe interval that can be supported through IPIs cannot
be arbitrarily small. In our local testbed (see Sec. 6), we ﬁnd
it hard to achieve a prime-probe interval that is shorter
than 50,000 PCPU cycles (roughly 16 microseconds). More
frequent interrupts will be accumulated and delivered to-
gether. Second, if the IPI VCPU is descheduled then this
can lead to periods during which no usable observations
are made.
If the Xen scheduler is non-work-conserving—
meaning that a domain’s execution time is limited to a bud-
get, dictated by the cap and weight parameters assigned to
1A fourth option is high precision event timer interrupts as
used by Bangerter et al. [8], but these are not available to
guests in Xen.
308it by Xen—then the IPI VCPU will be descheduled when
it exceeds its budget. These periods then must be detected
and any aﬀected prime-probe instances discarded. How-
ever, if the scheduler is work-conserving (the default) and
so allows a domain to exceed its budget if no other domain
is occupying the PCPU, then descheduling is rare on a mod-
erately loaded machine. It is worth noting that the probing
VCPU executes so brieﬂy that its execution appears not to
be charged toward its budget by the current Xen scheduler.
4.3 Sources of Noise
In this section we discuss sources of noise in the side chan-
nel described above, and how we deal with each one.
4.3.1 Hardware Sources of Noise
TLB misses. In x86 processors, hardware TLBs are usually
small set-associative caches that cache the translation from
virtual addresses to physical addresses. A TLB miss will
cause several memory fetches. Because the TLB is ﬂushed
at each context switch, the Probe of the ﬁrst cache set
(see Sec. 4.1) will always involve TLB misses and so will be
abnormally high; as such, the Probe results for the ﬁrst
cache set will be discarded. However, in our approach, the
number of memory pages used for the prime-probe protocol
is small enough to avoid further TLB evictions.
Speculative execution. Modern superscalar processors
usually fetch instructions in batches and execute them out-
of-order.
In order to force the in-order execution of our
Probe code for accurate measurement, the instructions need
to be serialized using instructions like cpuid and mfence.
Power saving. The speed of a Probe may be subject to
change due to PCPU power saving modes. If the attacker
VM is solely occupying a PCPU core, when it ﬁnishes its
Probe and relinquishes PCPU resources, the core may be
slowed to save power. When the interrupt is delivered to
the attacker VCPU, it appears to take longer for the PCPU
to recover from the power saving mode and, in our experi-
ence, yields a much longer eﬀective prime-probe interval.
Thus, longer-then-expected prime-probe intervals may in-
dicate there was no victim on the same core and so their
results are discarded.
4.3.2 Software Sources of Noise
Context switches. A Xen hypervisor context switch will
pollute the I-cache (though less than the D-cache in our ex-
perience). Moreover, noise due to a guest OS context switch
in the attacker VM may introduce additional diﬃculties.
Although there is not much we can do about the hypervi-
sor context-switch noise, we minimize the OS context switch
noise by modifying the core aﬃnity of all the processes in the
attacker VM so that all user-space processes are assigned to
the IPI VCPU, minimizing context switches on the probing
VCPU. This is beneﬁcial also because it enables the probing
VCPU to relinquish the PCPU as much as possible, allowing
another VCPU (hopefully, the victim’s) to share its PCPU.
Address space layout randomization. Address space
layout randomization (ASLR) does not interfere with our
attack (with 32KB L1 cache) because the L1 cache set to
which memory is retrieved is determined purely by its oﬀset
in its memory page, and because ASLR in a Linux imple-
mentation aligns libraries to page boundaries.
Emulated RDTSC instruction.
In the absence of an
invariant timestamp counters [18], Xen 4.0 or later emulates
the rdtsc call to prevent time from going backwards. In this
case, the rdtsc call is about 15 or 20 times slower than the
native call [27], which diminishes the attacker VM’s ability
to measure the duration to Probe a cache set. As such, the
attack we describe here works much more reliably when the
rdtsc call is not emulated.
Interference from other domains. One important as-
pect of software noise in the cache-based side channel is in-
terference from other domains besides the victim. The fact
that the attacker VCPU may observe activities of Dom0 or
other domains brings about one of a major diﬃculties in our
study: how can an attacker VM distinguish victim activity
of interest from cache activity from unrelated domains (or
victim activity that is not of interest)?
We discuss our solution to this hurdle in detail in Sec. 5.2,
though even with our solution described there, it is beneﬁcial
if we can minimize the frequency with which Probe results
reﬂect a VM other than the victim’s. In the conﬁgurations
we will consider in Sec. 6, if Dom0 is idle then the Xen sched-
uler will move the attacker and victim VCPU’s to distinct
cores most of the time. Thus, an eﬀective strategy is to in-
duce load on Dom0: if multiple victim VCPUs and the IPI
VCPU are also busy and so together with Dom0 consume
all four cores of the machine, then the probing VCPU, by
relinquishing the PCPU frequently, invites another VCPU
to share its PCPU with it. When the co-resident VCPU
happens to be the victim’s VCPU that is performing the
target computation, then the Probe results will be relevant
to the attacker.
Since Dom0 is responsible for handling network packets,
a general strategy to load Dom0 involves sending traﬃc at
a reasonably high rate to an unopened port of the victim
VM and/or attacker VM from a remote source. This can be
especially eﬀective since traﬃc ﬁltering (e.g., via iptables)
and shaping are commonly implemented in Dom0. In some
cases (e.g., Amazon AWS), the attacker can even specify
ﬁltering rules to apply to traﬃc destined to his VM, and
so he can utilize ﬁltering rules and traﬃc that will together
increase Dom0’s CPU utilization.
5. CLASSIFYING CODE PATHS
In this section we introduce a set of techniques that, when
combined, can enable an attacker VM to learn the code path
used by a co-resident victim VM. In settings where control
ﬂow is dependent on conﬁdential data, this enables exﬁltra-
tion of secrets across VM boundaries. While the techniques
are general, for concreteness we will use as a running exam-
ple the context of cryptographic key extraction and, in par-
ticular, learning the code path taken when using the classic
square-and-multiply algorithm. This algorithm (and gener-
alizations thereof) have previously been exploited in access-
driven attacks in non-virtualized settings (e.g., [1, 36]), but
not in virtualized SMP systems as we explore here.
The square and multiply algorithm is depicted in Fig. 2. It
eﬃciently computes the modular exponentiation xs mod N
using the binary representation of e, i.e., e = 2n−1en + · · · +
20e1. It is clear by observation that the sequence of function
calls in a particular execution of SquareMult directly leaks
e, which corresponds to the private key in many decryption
or signing algorithms. We let M , S, and R stand for calls
309SquareMult(x, e, N ):
let en, . . . , e1 be the bits of e
y ← 1
for i = n down to 1 {
y ← Square(y)
y ← ModReduce(y, N )
if ei = 1 then {
y ← Mult(y, x)
y ← ModReduce(y, N )
(S)
(R)
(M)
(R)
}
}
return y
Figure 2: The square-and-multiply algorithm.
to Mult, Square, and ModReduce, respectively, as labeled in
Fig. 2. Thus, the sequence SRM RSR corresponds to expo-
nentiation by e = 2.
The techniques we detail in the next several sections show
how an attacker can, despite VMM isolation, learn such se-
quences of operations.
5.1 Cache Pattern Classiﬁer
Recall from Sec. 4 that the output of a single prime-
probe instance is a vector of timings, one timing per cache
set. The ﬁrst step of our algorithm is to classify each such
vector as indicating a multiplication (M ), modular reduc-
tion (R) or squaring (S) operation. To do so in our ex-
periments, we employ a multiclass support vector machine
(SVM), speciﬁcally that implemented in libsvm [15]. An
SVM is a supervised machine learning tool that, once trained,
labels new instances as belonging to one of the classes on
which it was trained.
It also produces a probability esti-
mate in (0, 1] associated with its classiﬁcation, with a num-
ber closer to 1 indicating a more conﬁdent classiﬁcation.
To use an SVM to classify new instances, it is necessary to
ﬁrst train the SVM with a set of instance-label pairs. To do
so, we use a machine with the same architecture as the ma-
chine on which the attack will be performed and conﬁgure it
with the same hardware settings. We then install a similar
software stack for which we have total control of the hypervi-
sor. To collect our training data, we create a victim VM and
attacker VM like those one would use during an attack. We
use the xm command-line tools in Dom0 to pin the VCPUs
of the victim VM and attacker’s probing VCPU to the same
PCPU. We then set the victim VM to repeatedly perform-
ing modular exponentiations with the same arguments and,
in particular, with an exponent of all 1’s, and the probing
VCPU to repeatedly performing prime-probe instances.
This allows for the collection of vectors, one per prime-