p
m
m
o
o
c
c
o
o
t
t
e
e
m
m
T
T
i
i
DPDP I (99%)
PDP (99%)
200
400
600
800
1000
Block size (KB)
Block size (KB)
(a)
Proof time: DPDP I (99%)
Proof time: PDP (99%)
5
10
20
50
100
200
Block size (KB)
Block size (KB)
(b)
Figure 2: (a) Size of proofs of possession on a 1GB ﬁle, for 99%
probability of detecting misbehavior. (b) Computation time re-
quired by the server in response to a challenge for a 1GB ﬁle,
with 99% probability of detecting misbehavior.
8.1 Proof size
The expected size of proofs of possession for a 1GB ﬁle under
different block sizes is illustrated in Figure 2(a). Here, a DPDP
proof consists of responses to 460 authenticated skip list queries,
combined with a single veriﬁcation block M = Σaimi, which
grows linearly with the block size. The size of this block M is the
same as that used by the PDP scheme in [2], and is thus represented
by the line labeled PDP. The distance between this line and those
for our DPDP I scheme represents our communication overhead—
the price of dynamism—which comes from the skip list query re-
sponses (illustrated in Table 2). Each response contains on aver-
age 1.5 log n rows, so the total size decreases exponentially (but
slowly) with increasing block size, providing near-constant over-
head except at very small block sizes.
8.2 Server computation
Next, we measure the computational overhead incurred by the
server in answering challenges. Figure 2(b) presents the results of
these experiments (averaged from 5 trials), which were performed
on an AMD Athlon X2 3800+ system with 2GHz CPU and 2GB of
RAM. As above, we compute the time required by our scheme for
a 1GB ﬁle under varying block sizes, providing 99% conﬁdence.
As shown, our performance is dominated by computing M and in-
creases linearly with the block size; note that static PDP [2] must
also compute this M in response to the challenge. Thus the compu-
tational price of dynamism—time spent traversing the skip list and
building proofs—while logarithmic in the number of blocks, is ex-
tremely low in practice: even for a 1GB ﬁle with a million blocks of
size 1KB, computing the proof for 460 challenged blocks (achiev-
ing 99% conﬁdence) requires less than 40ms in total (as small as
13ms with larger blocks). We found in other experiments that even
when the server is not I/O bound (i.e., when computing M from
memory) the computational cost was nearly the same. Note that
any outsourced storage system proving the knowledge of the chal-
lenged blocks must reach those blocks and therefore pay the I/O
cost, and therefore such a small overhead for such a huge ﬁle is
more than acceptable.
The experiments suggest the choice of block size that minimizes
total communication cost and computation overhead for a 1GB ﬁle:
a block size of 16KB is best for 99% conﬁdence, resulting in a
proof size of 415KB, and computational overhead of 30ms. They
also show that the price of dynamism is a small amount of overhead
compared to the existing PDP scheme.
8.3 Version control
Finally, we evaluate an application that suits our scheme’s abil-
ity to efﬁciently handle and prove updates to versioned, hierarchical
resources. Public CVS repositories offer a useful benchmark to as-
sess the performance of the version control system we describe in
Section 7. Using CVS repositories for the Rsync [27], Samba [27]
and Tcl [23] projects, we retrieved the sequence of updates from
the RCS source of each ﬁle in each repository’s main branch. RCS
updates come in two types: “insert m lines at line n” or “delete m
lines starting at line n”. Note that other partially-dynamic schemes
(i.e., Scalable PDP [3]) cannot handle these types of updates. For
this evaluation, we consider a scenario where queries and proofs de-
scend a search path through hierarchical authenticated dictionaries
corresponding (in order) to the directory structure, history of ver-
sions for each ﬁle, and ﬁnally to the source-controlled lines of each
ﬁle. We use variable-sized data blocks, but for simplicity, assume a
naïve scheme where each line of a ﬁle is assigned its own block; a
smarter block-allocation scheme that collects contiguous lines dur-
ing updates would yield fewer blocks, resulting in less overhead.
dates of activity
# of ﬁles
# of commits
# of updates
Total lines
Total KBytes
Avg. # updates/commit
Avg. # commits/ﬁle
Avg. # entries/directory
Proof size, 99%
Proof size per commit
Proof time per commit
Rsync
Samba
1996-2007
1996-2004
371
11413
159027
238052
8331 KB
13.9
30.7
12.8
425 KB
13 KB
1.2ms
1538
27534
275254
589829
18525 KB
10
17.9
7
395 KB
9 KB
0.9ms
Tcl
1998-2008
1757
24054
367105
1212729
44585 KB
15.3
13.7
19.8
426 KB
15 KB
1.3ms
Table 3: Authenticated CVS server characteristics.
Table 3 presents performance characteristics of three public CVS
repositories under our scheme; while we have not implemented an
authenticated CVS system, we report the server overhead required
for proofs of possession for each repository. Here, “commits” refer
to individual CVS checkins, each of which establish a new version,
221adding a new leaf to the version dictionary for that ﬁle; “updates”
describe the number of inserts or deletes required for each commit.
Total statistics sum the number of lines (blocks) and kilobytes re-
quired to store all inserted lines across all versions, even after they
have been removed from the ﬁle by later deletions.
We use these ﬁgures to evaluate the performance of a proof of
possession under the DPDP I scheme: as described in Section 7,
the cost of authenticating different versions of ﬁles within a direc-
tory hierarchy requires time and space complexity corresponding
to the depth of the skip list hierarchy, and the width of each skip
list encountered during the Prove procedure.
As in the previous evaluation, “Proof size, 99%” in Table 3 refers
to the size of a response to 460 challenges over an entire repository
(all directories, ﬁles, and versions). This ﬁgure shows that clients of
an untrusted CVS server—even those storing none of the versioned
resources locally—can query the server to prove possession of the
repository using just a small fraction (1% to 5%) of the bandwidth
required to download the entire repository. “Proof size and time
per commit” refer to a proof sent by the server to prove that a sin-
gle commit (made up of, on average, about a dozen updates) was
performed successfully, representing the typical use case. These
commit proofs are very small (9KB to 15KB) and fast to compute
(around 1ms), rendering them practical even though they are re-
quired for each commit. Our experiments show that our DPDP
scheme is efﬁcient and practical for use in distributed applications.
Acknowledgments
Work supported in part by the U.S. National Science Foundation
under grants CNS–0627553, IIS–0713403 and OCI–0724806, by a
research gift from NetApp, Inc., and by the Center for Geometric
Computing and the Kanellakis Fellowship at Brown University. We
thank Giuseppe Ateniese and Anna Lysyanskaya for many useful
discussions.
9. REFERENCES
[1] A. Anagnostopoulos, M. Goodrich, and R. Tamassia.
Persistent authenticated dictionaries and their applications.
In ISC, pp. 379–393, 2001.
[2] G. Ateniese, R. Burns, R. Curtmola, J. Herring, L. Kissner,
Z. Peterson, and D. Song. Provable data possession at
untrusted stores. In CCS, pp. 598–609, 2007.
[3] G. Ateniese, R. D. Pietro, L. V. Mancini, and G. Tsudik.
Scalable and efﬁcient provable data possession. In
SecureComm, pp. 1–10, 2008.
[4] M. Blum, W. Evans, P. Gemmell, S. Kannan, and M. Naor.
Checking the correctness of memories. Algorithmica,
12(2):225–244, 1994.
[5] D. Boneh, B. Lynn, and H. Shacham. Short signatures from
the weil pairing. In ASIACRYPT, pp. 514–532, 2001.
[6] D. E. Clarke, S. Devadas, M. van Dijk, B. Gassend, and G. E.
Suh. Incremental multiset hash functions and their
application to memory integrity checking. In ASIACRYPT,
pp. 188–207, 2003.
[7] Y. Dodis, S. Vadhan, and D. Wichs. Proofs of retrievability
via hardness ampliﬁcation. In TCC, pp. 109–127, 2009.
[8] C. Dwork, M. Naor, G. N. Rothblum, and V. Vaikuntanathan.
How efﬁcient can memory checking be? In TCC, pp.
503–520, 2009.
[9] C. C. Erway, A. Küpçü, C. Papamanthou, and R. Tamassia.
Dynamic provable data possession. Cryptology ePrint
2008/432. http://eprint.iacr.org/2008/432.pdf.
[10] D. L. Gazzoni and P. S. L. M. Barreto. Demonstrating data
possession and uncheatable data transfer. Cryptology ePrint
Archive, Report 2006/150, 2006.
[11] M. T. Goodrich, C. Papamanthou, R. Tamassia, and
N. Triandopoulos. Athos: Efﬁcient authentication of
outsourced ﬁle systems. In ISC, pp. 80–96, 2008.
[12] M. T. Goodrich, R. Tamassia, and A. Schwerin.
Implementation of an authenticated dictionary with skip lists
and commutative hashing. In DISCEX II, pp. 68–82, 2001.
[13] A. Juels and B. S. Kaliski. PORs: Proofs of retrievability for
large ﬁles. In CCS, pp. 584–597, 2007.
[14] M. Kallahalla, E. Riedel, R. Swaminathan, Q. Wang, and
K. Fu. Plutus: Scalable secure ﬁle sharing on untrusted
storage. In FAST, pp. 29–42, 2003.
[15] J. Kubiatowicz, D. Bindel, Y. Chen, S. Czerwinski, P. Eaton,
D. Geels, R. Gummadi, S. Rhea, H. Weatherspoon,
W. Weimer, C. Wells, and B. Zhao. Oceanstore: an
architecture for global-scale persistent storage. SIGPLAN
Not., 35(11):190–201, 2000.
[16] F. Li, M. Hadjieleftheriou, G. Kollios, and L. Reyzin.
Dynamic authenticated index structures for outsourced
databases. In SIGMOD, pp. 121–132, 2006.
[17] J. Li, M. Krohn, D. Mazieres, and D. Shasha. Secure
untrusted data repository (SUNDR). In OSDI, pp. 121–136,
2004.
[18] U. Maheshwari, R. Vingralek, and W. Shapiro. How to build
a trusted database system on untrusted storage. In OSDI, pp.
10–26, 2000.
[19] A. Muthitacharoen, R. Morris, T. Gil, and B. Chen. Ivy: A
read/write peer-to-peer ﬁle system. In OSDI, pp. 31–44,
2002.
[20] M. Naor and K. Nissim. Certiﬁcate revocation and certiﬁcate
update. In USENIX Security, pp. 17–17, 1998.
[21] M. Naor and G. N. Rothblum. The complexity of online
memory checking. J. ACM., 56(1):1–46, 2009.
[22] A. Oprea, M. Reiter, and K. Yang. Space-efﬁcient block
storage integrity. In NDSS, pp. 17–28, 2005.
[23] J. Ousterhout. Tcl/tk. http://www.tcl.tk/.
[24] C. Papamanthou and R. Tamassia. Time and space efﬁcient
algorithms for two-party authenticated data structures. In
ICICS, pp. 1–15, 2007.
[25] C. Papamanthou, R. Tamassia, and N. Triandopoulos.
Authenticated hash tables. In CCS, pp. 437–448, 2008.
[26] W. Pugh. Skip lists: A probabilistic alternative to balanced
trees. Commun. ACM, 33(6):668–676, 1990.
[27] Samba. Samba.org CVS repository.
http://cvs.samba.org/cgi-bin/cvsweb/.
[28] T. Schwarz and E. Miller. Store, forget, and check: Using
algebraic signatures to check remotely administered storage.
In ICDCS, pp. 12, 2006.
[29] F. Sebe, A. Martinez-Balleste, Y. Deswarte,
J. Domingo-Ferre, and J.-J. Quisquater. Time-bounded
remote ﬁle integrity checking. Technical Report 04429,
LAAS, July 2004.
[30] H. Shacham and B. Waters. Compact proofs of retrievability.
In ASIACRYPT, pp. 90–107, 2008.
[31] R. Tamassia. Authenticated data structures. In ESA, pp. 2–5,
2003.
[32] R. Tamassia and N. Triandopoulos. Computational bounds
on hierarchical data processing with applications to
information security. In ICALP, pp. 153–165, 2005.
222