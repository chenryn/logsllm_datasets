# Provable Data Possession and Performance Analysis

## 8.1 Proof Size

Figure 2(a) illustrates the expected size of proofs of possession for a 1GB file under different block sizes. In our DPDP (Dynamically Provable Data Possession) scheme, a proof consists of responses to 460 authenticated skip list queries, combined with a single verification block \( M = \sum a_i m_i \), which grows linearly with the block size. The size of this block \( M \) is the same as that used in the PDP (Provable Data Possession) scheme from [2], and is represented by the line labeled "PDP" in the figure. The difference between this line and those for our DPDP I scheme represents the communication overhead, which is the cost of dynamism. This overhead comes from the skip list query responses, as shown in Table 2. Each response contains, on average, 1.5 log n rows, so the total size decreases exponentially (but slowly) with increasing block size, providing near-constant overhead except at very small block sizes.

## 8.2 Server Computation

Next, we measure the computational overhead incurred by the server in answering challenges. Figure 2(b) presents the results of these experiments, averaged from five trials, conducted on an AMD Athlon X2 3800+ system with a 2GHz CPU and 2GB of RAM. We compute the time required by our scheme for a 1GB file under varying block sizes, providing 99% confidence. As shown, our performance is dominated by computing \( M \) and increases linearly with the block size. Note that the static PDP scheme [2] must also compute this \( M \) in response to the challenge. The computational price of dynamism—time spent traversing the skip list and building proofs—is logarithmic in the number of blocks but extremely low in practice. For example, even for a 1GB file with a million blocks of size 1KB, computing the proof for 460 challenged blocks (achieving 99% confidence) requires less than 40ms in total (as small as 13ms with larger blocks). Other experiments show that even when the server is not I/O bound (i.e., when computing \( M \) from memory), the computational cost remains nearly the same. Any outsourced storage system proving the knowledge of the challenged blocks must reach those blocks and therefore pay the I/O cost, making such a small overhead for a large file more than acceptable.

The experiments suggest that a block size of 16KB is optimal for minimizing total communication cost and computation overhead for a 1GB file, resulting in a proof size of 415KB and a computational overhead of 30ms. This indicates that the price of dynamism is a small amount of overhead compared to the existing PDP scheme.

## 8.3 Version Control

Finally, we evaluate the application of our scheme to efficiently handle and prove updates to versioned, hierarchical resources, using public CVS repositories as a benchmark. We retrieved the sequence of updates from the RCS source of each file in the main branches of the Rsync, Samba, and Tcl projects. RCS updates come in two types: "insert m lines at line n" or "delete m lines starting at line n." Note that other partially-dynamic schemes, such as Scalable PDP [3], cannot handle these types of updates. For this evaluation, we consider a scenario where queries and proofs descend a search path through hierarchical authenticated dictionaries corresponding to the directory structure, history of versions for each file, and finally to the source-controlled lines of each file. We use variable-sized data blocks, but for simplicity, assume a naive scheme where each line of a file is assigned its own block. A smarter block-allocation scheme that collects contiguous lines during updates would yield fewer blocks, resulting in less overhead.

### Table 3: Authenticated CVS Server Characteristics

| Project | Dates of Activity | # of Files | # of Commits | # of Updates | Total Lines (KB) | Avg. Updates/Commit | Avg. Commits/File | Avg. Entries/Directory | Proof Size, 99% (KB) | Proof Size per Commit (KB) | Proof Time per Commit (ms) |
|---------|------------------|------------|--------------|--------------|------------------|--------------------|-------------------|-----------------------|---------------------|---------------------------|---------------------------|
| Rsync   | 1996-2007        | 371        | 159,027      | 238,052      | 8,331            | 12.8               | 425               | 13                    | 1.2                 | 1,538                     | -                         |
| Samba   | 1996-2004        | 11,413     | 275,254      | 589,829      | 18,525           | 10.0               | 395               | 9                     | 0.9                 | 27,534                    | -                         |
| Tcl     | 1998-2008        | 1,757      | 240,54       | 367,105      | 44,585           | 13.7               | 426               | 15                    | 1.3                 | 17,57                     | -                         |

Table 3 presents the performance characteristics of three public CVS repositories under our scheme. While we have not implemented an authenticated CVS system, we report the server overhead required for proofs of possession for each repository. "Commits" refer to individual CVS check-ins, each establishing a new version, adding a new leaf to the version dictionary for that file. "Updates" describe the number of inserts or deletes required for each commit. Total statistics sum the number of lines (blocks) and kilobytes required to store all inserted lines across all versions, even after they have been removed from the file by later deletions.

We use these figures to evaluate the performance of a proof of possession under the DPDP I scheme. As described in Section 7, the cost of authenticating different versions of files within a directory hierarchy requires time and space complexity corresponding to the depth of the skip list hierarchy and the width of each skip list encountered during the Prove procedure.

As in the previous evaluation, "Proof size, 99%" in Table 3 refers to the size of a response to 460 challenges over an entire repository (all directories, files, and versions). This figure shows that clients of an untrusted CVS server—even those storing none of the versioned resources locally—can query the server to prove possession of the repository using just a small fraction (1% to 5%) of the bandwidth required to download the entire repository. "Proof size and time per commit" refer to a proof sent by the server to prove that a single commit (made up of, on average, about a dozen updates) was performed successfully, representing the typical use case. These commit proofs are very small (9KB to 15KB) and fast to compute (around 1ms), making them practical even though they are required for each commit. Our experiments show that our DPDP scheme is efficient and practical for use in distributed applications.

## Acknowledgments

This work was supported in part by the U.S. National Science Foundation under grants CNS–0627553, IIS–0713403, and OCI–0724806, by a research gift from NetApp, Inc., and by the Center for Geometric Computing and the Kanellakis Fellowship at Brown University. We thank Giuseppe Ateniese and Anna Lysyanskaya for many useful discussions.

## 9. References

[1] A. Anagnostopoulos, M. Goodrich, and R. Tamassia. Persistent authenticated dictionaries and their applications. In ISC, pp. 379–393, 2001.
[2] G. Ateniese, R. Burns, R. Curtmola, J. Herring, L. Kissner, Z. Peterson, and D. Song. Provable data possession at untrusted stores. In CCS, pp. 598–609, 2007.
[3] G. Ateniese, R. D. Pietro, L. V. Mancini, and G. Tsudik. Scalable and efficient provable data possession. In SecureComm, pp. 1–10, 2008.
[4] M. Blum, W. Evans, P. Gemmell, S. Kannan, and M. Naor. Checking the correctness of memories. Algorithmica, 12(2):225–244, 1994.
[5] D. Boneh, B. Lynn, and H. Shacham. Short signatures from the Weil pairing. In ASIACRYPT, pp. 514–532, 2001.
[6] D. E. Clarke, S. Devadas, M. van Dijk, B. Gassend, and G. E. Suh. Incremental multiset hash functions and their application to memory integrity checking. In ASIACRYPT, pp. 188–207, 2003.
[7] Y. Dodis, S. Vadhan, and D. Wichs. Proofs of retrievability via hardness amplification. In TCC, pp. 109–127, 2009.
[8] C. Dwork, M. Naor, G. N. Rothblum, and V. Vaikuntanathan. How efficient can memory checking be? In TCC, pp. 503–520, 2009.
[9] C. C. Erway, A. Küpçü, C. Papamanthou, and R. Tamassia. Dynamic provable data possession. Cryptology ePrint 2008/432. http://eprint.iacr.org/2008/432.pdf.
[10] D. L. Gazzoni and P. S. L. M. Barreto. Demonstrating data possession and uncheatable data transfer. Cryptology ePrint Archive, Report 2006/150, 2006.
[11] M. T. Goodrich, C. Papamanthou, R. Tamassia, and N. Triandopoulos. Athos: Efficient authentication of outsourced file systems. In ISC, pp. 80–96, 2008.
[12] M. T. Goodrich, R. Tamassia, and A. Schwerin. Implementation of an authenticated dictionary with skip lists and commutative hashing. In DISCEX II, pp. 68–82, 2001.
[13] A. Juels and B. S. Kaliski. PORs: Proofs of retrievability for large files. In CCS, pp. 584–597, 2007.
[14] M. Kallahalla, E. Riedel, R. Swaminathan, Q. Wang, and K. Fu. Plutus: Scalable secure file sharing on untrusted storage. In FAST, pp. 29–42, 2003.
[15] J. Kubiatowicz, D. Bindel, Y. Chen, S. Czerwinski, P. Eaton, D. Geels, R. Gummadi, S. Rhea, H. Weatherspoon, W. Weimer, C. Wells, and B. Zhao. Oceanstore: An architecture for global-scale persistent storage. SIGPLAN Not., 35(11):190–201, 2000.
[16] F. Li, M. Hadjieleftheriou, G. Kollios, and L. Reyzin. Dynamic authenticated index structures for outsourced databases. In SIGMOD, pp. 121–132, 2006.
[17] J. Li, M. Krohn, D. Mazieres, and D. Shasha. Secure untrusted data repository (SUNDR). In OSDI, pp. 121–136, 2004.
[18] U. Maheshwari, R. Vingralek, and W. Shapiro. How to build a trusted database system on untrusted storage. In OSDI, pp. 10–26, 2000.
[19] A. Muthitacharoen, R. Morris, T. Gil, and B. Chen. Ivy: A read/write peer-to-peer file system. In OSDI, pp. 31–44, 2002.
[20] M. Naor and K. Nissim. Certificate revocation and certificate update. In USENIX Security, pp. 17–17, 1998.
[21] M. Naor and G. N. Rothblum. The complexity of online memory checking. J. ACM., 56(1):1–46, 2009.
[22] A. Oprea, M. Reiter, and K. Yang. Space-efficient block storage integrity. In NDSS, pp. 17–28, 2005.
[23] J. Ousterhout. Tcl/tk. http://www.tcl.tk/.
[24] C. Papamanthou and R. Tamassia. Time and space efficient algorithms for two-party authenticated data structures. In ICICS, pp. 1–15, 2007.
[25] C. Papamanthou, R. Tamassia, and N. Triandopoulos. Authenticated hash tables. In CCS, pp. 437–448, 2008.
[26] W. Pugh. Skip lists: A probabilistic alternative to balanced trees. Commun. ACM, 33(6):668–676, 1990.
[27] Samba. Samba.org CVS repository. http://cvs.samba.org/cgi-bin/cvsweb/.
[28] T. Schwarz and E. Miller. Store, forget, and check: Using algebraic signatures to check remotely administered storage. In ICDCS, pp. 12, 2006.
[29] F. Sebe, A. Martinez-Balleste, Y. Deswarte, J. Domingo-Ferre, and J.-J. Quisquater. Time-bounded remote file integrity checking. Technical Report 04429, LAAS, July 2004.
[30] H. Shacham and B. Waters. Compact proofs of retrievability. In ASIACRYPT, pp. 90–107, 2008.
[31] R. Tamassia. Authenticated data structures. In ESA, pp. 2–5, 2003.
[32] R. Tamassia and N. Triandopoulos. Computational bounds on hierarchical data processing with applications to information security. In ICALP, pp. 153–165, 2005.