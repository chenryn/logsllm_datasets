quality of the classiﬁcation, though it is useful to improve the system usability.
Conﬁdence measures the likelihood of having a correct classiﬁcation for a given
input. Users can instruct the system to forward any alert whose conﬁdence value
is lower than a given threshold for manual classiﬁcation, hence reducing the prob-
ability of misclassiﬁcation (at the price of an increased workload, see Section 2.7).
We chose two algorithms for our experiments: (1) Support Vector Machines
(SVM) and (2) the RIPPER rule learner. These algorithms implement super-
vised techniques, their training and classiﬁcation phases are fast and handle
high-dimensional data. Both algorithms perform non-incremental learning. A
non-incremental algorithm iterates on samples several times to build the best
classiﬁcation model by minimizing the classiﬁcation error. The whole training
set is then needed at once, and additional samples cannot be incorporated in the
classiﬁcation model unless the training phase is run from scratch. On the other
hand, an incremental algorithm can modify the model after the main training
phase as new samples become available. An incremental algorithm usually per-
forms worse than a non-incremental algorithm, because the model is not re-built.
Thus, a non-incremental algorithm is the best choice to perform an accurate clas-
siﬁcation. However, because it is highly unlikely that we can collect all alerts for
training at once the choice of non-incremental algorithms could be seen as a
limitation of our system.
In practice, thanks to the limited BF memory size, we can store a huge num-
ber of samples and, by applying a “batch training”, we can simulate incremental
learning in non-incremental algorithms. As new training samples become avail-
able, we add them to the batch training set and build the classiﬁer using the
entire set only when a certain number of samples is reached. Then, the classiﬁer
is re-built with the set of “batches” available at that time. Because both SVM
and RIPPER are fast in training, there are no computational issues.
We chose SVM and RIPPER, not only because they meet the requirements,
but for two additional reasons. First, they yield high-quality classiﬁcations.
Meyer et al. [17] test the SVM against several other classiﬁcation algorithms
(available from the R project [18]) on real and synthetic data sets. An SVM
outperforms competitors in 50% of tests and ranks in the top 3 in 90% of them.
RIPPER has been used before in the context of intrusion detection (e.g., on data
relative to system calls and network connections [19,20]) with good results. Sec-
ondly, because they approach the classiﬁcation problem diﬀerently (geometric
for SVM, and rule-based for RIPPER), the algorithms are supposed to compen-
sate for each others weaknesses. Hence, we can evaluate which algorithm is more
suitable in diﬀerent contexts. We will now provide some detail on the algorithms.
8
D. Bolzoni, S. Etalle, and P.H. Hartel
Fig. 4. Hyperplanes in a 2-dimensional space. H1 separates samples sets with a small
margin, H2 does that with the maximum margin. The example refers to linearly sepa-
rable data. The support vectors are shown with a thicker border.
Support Vector Machines. (Vapnik and Lerner [21]) is a set of supervised
learning methods used for classiﬁcation. In the original formulation, an SVM is a
binary classiﬁer. It uses a non-linear mapping to transform the original training
data into a higher dimension. Then, it searches for the linear optimal separating
hyperplane, i.e., a plane that separates the samples of one class from another.
An SVM uses “support vectors” and “margins” to ﬁnd the optimal hyperplane,
i.e., the plane with the maximum margin.
The original SVM algorithm has been modiﬁed to classify non-linear data and
to use multiple classes. Boser et al. [22] introduce non-linear data classiﬁcation by
using kernel functions (i.e., non-linear functions). To support multiple classes,
the problem is reduced to multiple binary sub-problems. Given m classes, m
classiﬁers are trained, one for each class. Any test sample is assigned to the class
corresponding to the largest positive distance.
RIPPER. (Cohen [23]) is a fast and eﬀective rule induction algorithm. RIPPER
uses a set of IF-THEN rules. An IF-THEN rule is an expression in the form
IF  THEN . The IF-part of a rule is called the rule
antecedent. The THEN-part is the rule consequent. The condition consists of one
or more attribute tests, that are logically ANDed. A test ti is in the form ti = v
for categorical attributes (where v is a categorical label) or either ti ≥ θ or ti ≤ θ
for numerical attributes (where θ is a numerical value). The conclusion contains
a class prediction. If, for a given input, the condition (i.e., all of the attribute
Panacea: Automating Attack Classiﬁcation
9
tests) holds true, then the rule antecedent is satisﬁed and the corresponding
class in the conclusion is returned (the rule is said to “cover” the input). Since
RIPPER employs ordered rules, when a match occurs, the algorithm does not
evaluate other rules. Some examples of rules are:
IF bf[i] = 1 AND . . . AND bf[k] = 1 THEN class = cross-site scripting
IF bf[l] = 1 AND . . . AND bf[n] = 1 THEN class = sql injection
RIPPER builds the rule set for a certain class SCi as follows. The training data
set is split into two sets, a pruning and a growing sets. The classiﬁer is built
using these two sets by repeatedly inserting rules starting from an empty rule
set (the growing set). The algorithm heuristically adds one condition at a time
until the rule has no error rate on the growing set. RIPPER implements also an
optimisation phase, in order to simplify the rule set.
When multiple classes C1 . . . Cn are used, RIPPER sorts classes on a sample
frequency basis and induces rules sequentially from the least prevalent class
SC1 to the second most prevalent class SCn−1. The most prevalent class SCn
becomes the default class, and no rule is induced for it (thus, in case of a binary
classiﬁcation, RIPPER induces rules for the minority class only).
1.3 Implementation
We have implemented a prototype of Panacea to run our experiments. The pro-
totype is written in Java, since we link to the libraries provided by the Weka
platform [24]. Weka is a well-known collection of machine learning algorithms,
and it contains an implementation of both SVM and RIPPER. Weka provides
also a comprehensive framework to run benchmarks on several data sets under
the same testing conditions. The attacks samples generated by network IDSs, in
the form of alerts, are stored in a database that the system fetches to extract
the alert payload information.
2 Benchmarks
Public data sets for benchmarking IDSs are scarce. It is even more diﬃcult to
ﬁnd a suitable data set to test Panacea, since no research has systematically
addressed the problem of (semi)automatically classifying attacks detected by an
ABS before. Hence, we have collected three diﬀerent data sets (referred to as
DSA, DSB and DSC, see below for a description of the data sets) to evaluate
the accuracy of Panacea. These data sets are used to evaluate the accuracy of
Panacea in diﬀerent scenarios: (1) when working in automatic mode (DSA), (2)
when using an ad hoc taxonomy and the manual mode (DSB) and (3) when
classifying unknown attacks (e.g., generated by two ABSs), having trained the
system with alerts from known attacks (DSB and DSC).
In the literature there are several taxonomies and classiﬁcations of security
events. For instance, Howard [25], Hansman and Hunt [26], and the well-known
taxonomy used in the DARPA 1998 [27] and 1999 [28] data sets. Only the latter
10
D. Bolzoni, S. Etalle, and P.H. Hartel
classiﬁcation has been used in practice (in spite of its course granularity, as it
contains only four classes which are unsuitable to classify modern attacks). In our
experiments, we use the Snort classiﬁcation for benchmarks with DSA (see [29]
for a detailed taxonomy) and the Web Application Security Consortium Threat
Classiﬁcation [30] for benchmarks with DSB and DSC.
To evaluate the accuracy of the classiﬁcation model, we use two approaches.
For test (1) and (2), we employ cross-validation. In cross-validation, samples
are partitioned into sub-sets. The analysis is ﬁrst performed on a single sub-set,
while the other sub-set(s) are retained to validate the initial analysis. In k-fold
cross-validation, the samples are partitioned into k sub-sets. A single sub-set is
retained as the validation data for testing the model, and the remaining k − 1
sub-sets are used as training data to build the model. The process is repeated k
times (the “folds”), using each of the k sets exactly once to validate the model.
Usually the k fold results are combined (e.g., averaged) to generate a single
estimation. The advantage of this method is that all of the samples are used
for both training and validation, and each sample is used for validation exactly
once. We use 10 folds in our experiments, which is a standard value, used in the
Weka testing environment too.
For test 3), we use one of DSB and DSC for training and the other for testing.
The accuracy is evaluated by counting the number of correctly classiﬁed attacks.
2.1 Data Sets
DSA. contains alerts raised by Snort (see Table 1 for alert ﬁgures). To collect
the largest number of alerts possible, we have used several tools to automatically
inject attack payloads (Nessus [31] and a proprietary vulnerability assessment
tool). Attacks have been directed against a system running some virtual machines
with both Linux- and Windows-based installations, which expose several services
(e.g., web server, DBMS, web proxy, SMTP and SSH). We collected more than
3200 alerts in total, classiﬁed in 14 diﬀerent (Snort) attack classes. However,
some classes have few alerts, thus we select only classes with at least 10 alerts.
This data set (and DSB as well) is synthetic. We do not see this as a limitation
since the alerts cover multiple classes and trigger a large number of diﬀerent
signatures. We test how the system behaves in automatic mode, the whole set
being generated by Snort.
DSB. contains a set of more than 1400 Snort alerts related to web attacks
(Table 2 provides alert details). To generate this data set, we have used Nes-
sus, Nikto [32] (a web vulnerability scanner), and we have manually injected
attack payloads collected from the well-known site Milw0rm, that hosts a large
collection of web exploits [33]. The attack classiﬁcation has been performed man-
ually (manual mode), since Snort does not provide a ﬁne-grained classiﬁcation of
web-related attacks (alerts are allocated to diﬀerent classes with other alerts, see
Table 1). Attacks have been classiﬁed according to the Web Application Security
Consortium Threat Classiﬁcation [30].
Panacea: Automating Attack Classiﬁcation
11
Table 1. DSA (alerts raised by Snort): attack classes and samples. It is not surpris-
ing that web-related attacks account for more than 50%, since most Snort signatures
address web vulnerabilities.
marks classes that contain web-related attacks.
∗
Attack Class
attempted-recon
∗
web-application-attack
∗
web-application-activity
∗
unknown
∗
attempted-user
misc-attack
attempted-admin
Description
Attempted information leak
Web application attack
Access to a potentially
vulnerable web application
Unknown traﬃc
Attempted user privilege gain
Miscellaneous attack
Attempted administrator
privilege gain
attempted-dos
bad-unknown
Attempted Denial of Service
Potentially bad traﬃc
# of samples
1379
1032
599
66
45
44
32
14
13
Table 2. DSB: attack classes and samples. Attacks have been classiﬁed according to
the Web Application Security Consortium Threat Classiﬁcation.
Cross-site Scripting
Attack Class # samples
Path Traversal
931
399
73
8
SQL Injection
Buﬀer Overﬂow
Table 3. DSC : attack classes and samples. Attacks have been classiﬁed according to
the Web Application Security Consortium Threat Classiﬁcation.
Cross-site Scripting
Attack Class # samples
53
Path Traversal
27
16
4
SQL Injection
Buﬀer Overﬂow
DSC is a collection of alerts generated over a period of 2 weeks by two ABSs, i.e.,
our POSEIDON [34] and Sphinx [35]. POSEIDON is a general-purpose anomaly-
network-based IDS, which uses a combination of a neural network with the well-
know algorithm PAYL [12] to analyse network data and detect attacks. Sphinx is
a web- anomaly-based IDS, which analyses HTTP request parameters and which
detects data-ﬂow [36] attacks. We recorded network traﬃc directed to a main
web server of the university network, and did not inject any attack. Afterwards,
we processed this data with POSEIDON and Sphinx to generate alerts. The
inspection of alerts and the classiﬁcation of attacks has been performed manually
(using the same taxonomy we apply for DSB). The data set consists of a set of
100 alerts, and Table 3 reports attack details.