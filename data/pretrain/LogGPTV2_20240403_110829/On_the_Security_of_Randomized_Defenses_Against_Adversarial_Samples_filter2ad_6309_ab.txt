from a pool of train DNN models for each test input. Xie et
al. [43] use randomness in a different way: to resize the image
to a random size, or to add padding zeroes in a randomized
fashion. We discuss two other existing randomization strate-
gies which will be later considered in our evaluation, namely
region-based classification [5] and cropping-rescaling [16], along
with our proposal in Section 4.
3 SECURITY MODEL
In this section, we present a security model for evasion attacks
that allows us to formalize robustness to adversarial samples.
Game-based Modeling of Evasion Attacks. Our model con-
siders an adversary A that aims at defeating a classifier C by
generating adversarial samples starting from “natural” sam-
ples. Following the approach of modern cryptography, our
security model reproduces the above scenario through a se-
curity game between A and C, that we name evasion under
chosen-sample attacks (EV-CSA), as illustrated in Figure 1.
The adversary’s goal is to present a number of adversarial
samples generated from a set XD ⊂ X of “naturally occurring”
(labeled) samples. 1 The number N of adversarial samples,
1In practice, XD represents a set of available images used for testing, e.g., MNIST.
Game EV-CSAA,ϵ,N(C, XD):
1 q ← 0, n ← 0
2 XA ← ∅
3 A(ϵ, N,⟨C⟩, XD)Classify,Attack
4 Return n/N
Oracle Classify(x):
5 ˆy ← C(x)
6 Give ˆy to A
Oracle Attack(x, x′, yt):
7 If q ≥ N: Go to line 4
8 Enforce (x,∗) (cid:60) XA
9 q ← q + 1
10 XA ∪← (x, x′)
11 If C(x′) = yt and d(x′, x) ≤ ϵ:
n ← n + 1
12
13 Return
Figure 1: Security game for targeted evasion under chosen-
sample attacks (EV-CSA), involving adversary A against
classifier C. Untargeted attacks are captured by replacing the
inputs to the Attack oracle with pairs (x, x′) and, the first con-
dition of line 11 with C(x′) (cid:44) C(x).
1 ≤ N ≤ |XD|, is a game parameter and can be adapted to
capture different security goals.
We specify the amount of information that A has about the
adversarial task by passing the relevant inputs: the allowed
adversarial perturbation (a.k.a. distortion) ϵ, the number N of
adversarial samples, the classifier’s code ⟨C⟩, and the set XD
of benign samples. Further, by limiting the amount of infor-
mation encoded in ⟨C⟩, our game can cover different adversar-
ial models such as “whitebox” (a.k.a. fully adaptive), mean-
ing that A knows every detail about the classifier, including
neural-network weights and any defense mechanism in place,
“blackbox” (a.k.a. non adaptive), i.e.,A knows only public infor-
mation about C, and intermediate attacker’s models, so-called
“graybox” (a.k.a. semi-adaptive), in which A has only partial
information about the classifier’s internals and/or defensive
layers. Graybox attacks include those agnostic of a defense
mechanism. In this case, the adversary knows the original clas-
sifier fully, hence it is not blackbox, but it does not know the
defense, hence it is not whitebox either (cf. Section 4.4).
We further let the adversary interact with the classifier
through an oracle Classify, i.e., A can query C on any input x
of their choosing and obtain the corresponding label ˆy = C(x).
Observe that the Classify oracle provides no extra power to
whitebox adversaries, as having full knowledge of the classi-
fier allows A to emulate the oracle. It is, however, necessary to
cover weaker attacks, such as transferability attacks [32] (which
are blackbox), and attacks oblivious of the defense (which are
graybox).
The game also provides the adversary with a second oracle,
denoted by Attack, which lets A submit candidate adversarial
samples. This oracle allows us to describe A’s goal formally
and to define robustness to adversarial samples, as we see next.
The adversary can present an adversarial sample by submit-
ting a query (x, x′, yt) to the Attack oracle, where x is the start-
ing sample, x′ is the candidate adversarial sample, and yt is
the target label. Upon being queried, the oracle then checks
whether the adversary reached the query limit q ≥ N, termi-
nating the game in such a case (cf. line 4). Otherwise, it checks
whether the adversarial sample x′ is “fresh”, in the sense that
no other adversarial sample x′′ has been already proposed for
the same starting image x (cf. line 8), which is necessary to
invalidate trivial attacks that artificially achieve high success
rate, e.g., by presenting “the same” adversarial sample over
and over, in a trivially modified version.2 If the query gets
through the checks, the oracle adds the fresh pair (x, x′) to
the adversarial set XA, and further checks whether the clas-
sifier errs on x′ as desired, by predicting its class as yt, and
whether x′ is sufficiently close to x, i.e., d(x, x′) ≤ ϵ according
to some pre-established distance metric d. In case of success,
the game rewards the adversary by increasing the counter n
which records the number of successful samples (cf. line 12).
As soon as the N “chosen samples” are submitted, the EV-
CSA game ends outputting the success rate of the adversary,
that we denote by succev-csa
A,ϵ,N (C, XD) = n/N. An execution
of the EV-CSA game depends on the adversarial strategy A
and the classifier C—both of which may be randomized. In
particular, if the game depends on any randomness (used by
the adversary, by the classifier, or both), the outcome is deter-
mined by the value of the randomness, and the success rate is
a random variable.
Deterministic vs. Randomized Classifiers. We stress that or-
acle Attack does not reflect an actual capability of the attacker,
however, it provides a natural abstraction for determining A’s
success rate. In particular, if only deterministic classifiers were
considered, having A submit their samples through the ora-
cle is equivalent to letting A present a set XA of N samples
directly. That is, the usual notion of success rate against deter-
ministic classifiers is a special case of our notion. The reason
for introducing the Attack oracle is precisely that, when ran-
domized classifiers are considered, it is no longer meaningful
to talk about a set of adversarial examples (a given sample x′
may be correctly labeled for some choices of C’s randomness
while being misclassified for a different randomness).
Defining Robustness. Our security game provides a formal
language to express the effectiveness of a defense in making
a given classifier “more robust” to attacks. For a classifier C,
let Cd denote the classifier obtained from C by applying a de-
fense d. Intuitively, a defense is effective against an attack A
if either A’s success rate after applying the defense is signifi-
cantly smaller than that with no defense, or a larger distortion
is necessary to achieve that success rate. Formally, we say that
2Changing a few pixels of a successful adversarial sample x′ yields a new sam-
ple x′′ (cid:44) x′ which is very likely to also be successful, thus A should only get
credit for one of them.
a defense d for classifier C is effective against attack A, or equiv-
alently that Cd is more robust than C, if either succev-csa
A,ϵ,N (Cd) ≪
A,ϵ,N (C) for ϵd ≫ ϵ.
succev-csa
A,ϵ,N (C), or succev-csa
A,ϵd,N(Cd) = succev-csa
4 RANDOMNESS-BASED DEFENSES
In this section, we present the three randomness-based de-
fensive techniques which we consider in our empirical eval-
uation from Section 5. While they all apply a randomized
pre-processing layer at test time, the first defense, Cropping-
Rescaling [16], also operates on the training phase, thereby
leading to an “offline” defense. It further uses an ensembling
technique, meaning that classification is based on the model
predictions over an ensemble of samples generated from the
original input. The second defense, Region-Based Classifica-
tion [5], does not modify the training phase—i.e., it is “online”—
but uses ensembling, too. The third defense, Randomized
Squeezing (our design), neither alters training nor relies on
ensembling. Instead, it combines the randomness layer with a
subsequent image-denoising operator based on Feature Squeez-
ing [44].
4.1 Cropping-Rescaling
The Cropping-Rescaling defense by Guo et al. [16] applies a ran-
domized transformation that crops and rescales the image
prior to feeding it to the classifier. The intuition behind the
defense is to alter the spatial positioning of the adversarial
perturbation, so that it no longer causes the desired effect and,
therefore, it makes the corresponding adversarial sample less
likely to succeed. More precisely, cropping-rescaling operates
in two steps, at training and at test time. Training is performed
on cropped and rescaled images, following the data augmen-
tation paradigm of He et al. [17]. Then, to predict the label
of each test image, the classifier randomly samples 30 crops
of the input, rescales them, and averages the model predic-
tions over all crops. Applying the input transformation also
at training yields higher classification accuracy on adversarial
samples [16].
4.2 Region-Based Classification
The Region-Based Classification defense proposed by Cao and
Gong [5] computes each prediction over an ensemble gener-
ated from the input sample in a randomized fashion. Specifi-
cally, this approach samples 10,000 images uniformly at ran-
dom from an appropriately sized hypercube centered at the
testing image, invokes a DNN to compute predictions over
the sampled images, and returns the label predicted for the
majority of the images—therefore, classification is no longer
“point-based” but “region-based”. Here, an “appropriate size”
of the hypercube is chosen so that the region-based classifier
maintains the accuracy of the underlying DNN over a (be-
nign) test set. Taking the “majority vote” over the ensemble
predictions is based on the assumption that while for benign
images most neighboring samples yield the same predicted
label, adversarial samples are close to the DNN’s classification
boundary.
4.3 Randomized Squeezing
The defensive strategy that we propose, Randomized Squeezing,
combines input randomization with a deterministic image-
denoising technique, namely the Feature Squeezing defense
by Xu et al. [44] (cf. Section 2.2). We introduce randomness at
feature level, for each feature component and within a pre-
defined threshold, so that it does not bias the prediction ex-
cessively in any particular direction. Concretely, let C denote
a DL classifier enhanced with Feature Squeezing. Our pro-
posal preprocesses C’s input by adding a perturbation rand,
chosen uniformly at random from the real interval [−δ, +δ],
δ ∈ [0, 1], to each pixel. The intuition here is that adding a
small, carefully crafted perturbation preserves the classifier’s
output on genuine images, and it significantly affects predic-
tions on adversarial images. While the randomness added at
individual feature level does not destroy the patterns of the
pixels, which is critical for correct classification, it does intro-
duce a source of unpredictability in the defense mechanism
which enlarges the search space of the adversary. Indeed, to
craft a successful adversarial sample, the adversary now has
to search for a perturbation that yields the desired prediction
for (most of) the various possible randomness values, which is
a considerably harder task than fooling (deterministic) Feature
Squeezing. The increased robustness achieved by randomized
classifiers clearly depends on the quality of the randomness,
which should be unpredictable from the adversary’s perspec-
tive. Thus, it is crucial for security that the random noise be
generated from a high-entropy key to seed the underlying
cryptographic pseudo-random generator. More specifically,
Randomized Squeezing comprises of the following subrou-
tines:
Setup: This procedure performs any instruction needed to
initialize the original system. In addition, it sets the value δ ∈
[0, 1] for the magnitude of the randomness (setting δ = 0 leaves
the input unchanged, while δ = 1 is almost equivalent to
generating a fresh input uniformly at random), and initializes
the random number generator. The perturbation magnitude δ
should be sufficiently large to be effective against adversarial
samples, and at the same time be sufficiently small to preserve
the classifier’s accuracy on normal samples. In Section 5, we
analyze in details how to choose δ in order to establish a good
tradeoff between the achieved accuracy and robustness.
Training: Since our defense mechanism does not affect the
training phase, this step is the same as for the original system.
Upon completion of this phase, we can assume a trained (de-
terministic) classifier C, based on Feature Squeezing, which
we will use as a basis for our randomized classifier C$, as we
see next.
Classification: Upon receiving an input image x, the random-
ized classifier C$ selects a uniformly random key ks to seed the
underlying pseudo-random generator and expands ks until a
sufficient amount of (pseudo)randomness has been generated
to randomize all pixels of x. The randomization of each pixel
consists in adding a random value rand ∈ [−δ, +δ]. When the
value of the pixel goes outside the allowed intensity thresh-
old (normalized to [0,1] in our experiments), we clip them at
the edges instead of taking a modulo and wrapping around.
This is performed to bias the randomness for pixels that are
close to the intensity thresholds, which helps to preserve ac-
curacy of the classifier for legitimate samples. The process is
repeated for every color channel. Hence, for grayscale images,
we add randomization just once as there is only one chan-
nel, while for color RGB images randomness is added three
times, once for each channel (i.e., “R”, “G”, and “B”, respec-
tively), individually per pixel. The pre-processing routine of
Randomized Squeezing is depicted in Figure 2. Finally, the
resulting image x′ is fed to the (deterministic) classifier C, and
the resulting prediction ˆy = C(x′) is returned as label for x.
Randomizepixels(Pixels, δ)
ks ←$ {0, 1}keylen
for i ← 1 to length[Pixels]
do Generate noncei
rand = G(ks, noncei)
 Choose rand randomly from [−δ, δ]
Pixels[i] ← Pixels[i] + rand
if Pixels[i] > 1
do Pixels[i] = 1
if Pixels[i] < 0
do Pixels[i] = 0
 Clip pixel values to allowed threshold
i ← i + 1
Figure 2: Randomizing image pixels via Randomized
Squeezing. Pixels represents a vector comprising all pixels
of the input image, G denotes a pseudo-random number
generator, and noncei is a fresh nonce for every i.
Note that we introduce randomness to the input only while
testing and not while training: this makes our technique partic-
ularly lightweight and versatile, as it does not increase training
costs and can be applied directly to any pre-trained classifier.
In addition, Randomized Squeezing invokes the underlying
model only once per prediction, without relying on ensem-
bling, which also improves efficiency compared to Cropping-
Rescaling and Region-Based Classification.
4.4 Analysis: Attacks’ Categorization
We briefly discuss the attacks considered in our evaluation
of randomized defenses (cf. Section 5) in the context of the
attacker models from the previous section.
Graybox Attacks. Prominent techniques to generate adversar-
ial examples against DL classifiers are the Fast Gradient Sign
Method (FGSM) [13], the Basic Iterative Method (BIM) [24],
the Jacobian Saliency Map Approach (JSMA) [24], the Carlini-
Wagner (CW) attacks [7], and DeepFool [29]. These attacks
were specifically designed to fool neural networks in a white-
box setting, hence they assume that architecture and parame-
ters are known to the attacker. Generating adversarial samples
according to any of the aforementioned attacks, and then us-
ing these samples against an enhanced version of the neural
network via some defense mechanism,3 results in a graybox
attack—because the neural network’s internals are available
to the attacker, but the defense is not.
Whitebox Attacks. Athalye et al. [2] proposed the Expecta-
tion over Transformation (EOT), a generic method to generate
adversarial samples that remain adversarial over a chosen
distribution of transformations—in particular over random-
ized ones. EOT can handle only differentiable transformations,
hence it is not applicable to image-denoising defenses such
as Feature Squeezing. A second technique, Backward-Pass Dif-
ferentiable Approximation (BPDA) [1], was introduced to cope
with non-differentiable transformations and later employed to
defeat, among others, a generalized version of Feature Squeez-
ing [8]. We evaluate Cropping-Rescaling, Region-Based Clas-
sification, and Randomized Squeezing against the BPDA and
EOT attacks, individually and in combination, appropriately
tuned to each defense. Due to exploiting knowledge of the un-
derlying neural-network parameters and of the defense fully,
both BPDA and the combination BPDA+EOT yield whitebox
(i.e., “fully adaptive”) attacks.
5 EVALUATION AND RESULTS
In this section, we evaluate the randomness-based defenses
presented in Section 4 against graybox and whitebox attacks.
Specifically, we compare Feature Squeezing [44] and Random-
ized Squeezing by testing them against 11 state-of-the-art gray-
box attacks, showing that randomness hardens Feature Squeez-
ing. We further evaluate all three randomness-based defenses
against the whitebox attacks proposed by Athalye et al. [1], and
explore how increasing the amounts of randomness affects
their success.
5.1 Setup
Attacks. As proposed by Xu et al. [44], we analyze two vari-
ations of each targeted attack: (i) next: targets the class next
to the ground truth class modulo number of classes (ii) least-
likely (LL): targets the class which the image is least-likely
to be classified as. Specifically, we consider the following at-
tacks on Feature Squeezing and Randomized Squeezing: Fast
Gradient Sign Method (FGSM) [13], Basic Iterative Method
(BIM) [24], Carlini and Wagner L0, L2 and L∞ attacks (CW) [7]
(Next & LL), DeepFool [29], Jacobian Saliency Map Approach
(JSMA) [33] (Next & LL). We further evaluate Randomized
Squeezing, Cropping-Rescaling, and Region-Based Classifica-
tion, against fully adaptive attacks (BPDA and EOT) proposed
by Athalye et al. [1].
Datasets. We use ImageNet, CIFAR-10, and MNIST datasets to
conduct our experiments. The ImageNet dataset contains 1.2
million images for training and 50 000 images for validation.
They are of various sizes and hand-labeled with 1000 classes.
The images are preprocessed to 224 × 224 pixels and encoded
3Here: Cropping-Rescaling, Region-Based Classification, or Randomized
Squeezing.
with 24-bit color per pixel. CIFAR-10 is a dataset of 32 × 32
pixel images with 24-bit color per pixel (three color channels
per pixel) and 10 classes. MNIST is a dataset of hand-written