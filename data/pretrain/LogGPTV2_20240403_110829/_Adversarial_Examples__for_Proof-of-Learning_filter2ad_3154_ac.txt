Di ← d(▽W ′
i
, 0) + d(R, 0),
t )  γ where γ ≪ δ (Line 25). This guarantees
is d(W ′
that the model is still close to itself after a single step of
update. Since the distance between Wt−k and Wt is smaller
than δ after initialization, after k steps of updates,
their
distance is still smaller than δ: d(Wt, W ′
Interestingly, this change makes the adversarial optimization
become easier to converge. Recall that in Attack I, A has
to adjust the loss function L(fW ′
(X + R), y) to minimize
d(W ′
, Wt). This is difficult to achieve because
gradient-based training is used to minimize (not adjust) the
loss function. Thanks to the new Di, A can simply minimize
the loss function in Attack II. In another word, the adversarial
optimization process in Attack II is more close to normal
training. Table II shows that on CIFAR-10, after 10 steps of
adversarial optimization, the loss function decreases from 0.43
to 0.04, and the gradients decrease from 61.13 to 0.12. Both
are small enough to pass the verification. That is to say, the
number of while loops N can be as small as 10 in Attack II.
t−1 + η▽W ′
t−1
i
THE CHANGES OF LOSS AND THE GRADIENTS AFTER 20 STEPS OF
ADVERSARIAL OPTIMIZATION ON CIFAR-10
Table II
(X + R), y)
i
L(fW ′
0.43 ± 0.18
0.04 ± 0.01
Before
After
(cid:13)(cid:13)▽W ′
i
(cid:13)(cid:13)2
61.13 ± 45.86
0.12 ± 0.05
Cost comparison. It is easy to see that Attack II requires
times of update (Line 24) plus T ′ · N times of ad-
T ′
versarial optimization (Line 26-30), where N = 10. Each
update requires one gradient computation and each adver-
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:13:51 UTC from IEEE Xplore.  Restrictions apply. 
1412
sarial optimization requires three gradient computations. In
total, Attack II requires 31T ′ gradient computations. As a
comparison, generating a PoL proof requires T = E · S
gradient computations. In [12], they set E = 200 and S = 390,
hence T = 78, 000. Therefore, the cost for Attack II is smaller
31 ≈ 2, 516. Our
than the trainer’s cost as long as T ′  γ−σ do
∂Wt−k
26
27
28
29
30
31 end
L(fWt−k (X + R), y)
Algorithm 5 shows Attack III. Again, we highlight the key
differences (compared to Attack II) in blue. The major change
is that A optimizes all k batches of data points together in
updateDataPoints. The distance between W ′
t and Wt
should be limited to γ − σ (where 0 
times of update plus T ′
η2αβ (k−1)(k−2)
Cost comparison. Recall that the complexity for Attack III
k · N times of adversarial
is T ′
optimization, where N = 10; each update requires one
gradient computation and each adversarial optimization re-
quires three gradient computations. In total, Attack III requires
31 T ′
k gradient computations. Given that generating a PoL
proof requires requires T = 78, 000 gradient computations,
the cost for Attack III is smaller than the trainer’s cost as
31 ≈ 2, 516. In our experiments, we show
long as T ′
Attack III can pass the verification when we set k = 100
(cf. Section IV). Then, T ′  1, 000 for
the spoof to be able to pass the verification (Figure 2(c)). On
k
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:13:51 UTC from IEEE Xplore.  Restrictions apply. 
1414
(a) Normalized reproduction error in l1
(b) Normalized reproduction error in l2
(c) Normalized reproduction error in l∞
(d) Normalized reproduction error in cos
(e) Spoof generation time.
(f) Spoof size.
Figure 2. Attack II on CIFAR-10
ATTACK I ON CIFAR-10 AND CIFAR-100
Table III
CIAFR-10
Spoof
0.2152
0.2796
0.2291
0.0758
4,591
Proof
0.0265
0.0333
0.0283
0.0038
4,607
CIFAR-100
Proof
0.1911
0.3330
0.5075
0.0043
17,756
Spoof
0.7558
0.6552
0.1518
0.1345
18,307
l1
l2
l∞
cos
Time (s)
the other hand, when T ′ > 4, 000, the generation time of the