### Fair Comparison with Our Approach

To ensure a fair comparison with our approach, which leverages the availability of multiple training and validation sets while adhering to the frequentist spirit, we follow these steps:

1. **Training and Validation Sets**: Consider a training set \( D_m \) for learning the best remapping \( Y \to W \), and a validation set \( T_n \) for estimating the \(\delta\)-vulnerability.
2. **Empirical Probability Calculation**:
   - Compute the empirical probability \(\hat{p}_{PX|Y}\) using \( D_m \). Specifically, \(\hat{p}_{PX|Y}(x|y)\) is given by the number of occurrences of \((x, y)\) divided by the number of occurrences of \(y\) in \( D_m \).
   - If a certain \( y \) is in \( T_n \) but not in \( D_m \), assign \(\hat{p}_{PX|Y}(x|y) = 0\) for all \( x \neq x' \).
3. **Best Mapping**:
   - For each \( y \in Y \), find the best mapping \( w(y) = \arg\max_{w \in W} \sum_{x \in X} \hat{p}_{PX|Y}(x|y) \delta(w, x) \).
4. **Estimate \(\delta\)-Vulnerability**:
   - Compute the empirical joint distribution \(\hat{Q}_{X,Y}\) for each \((x, y) \in T_n\).
   - Estimate the \(\delta\)-vulnerability on the validation samples as:
     \[
     \hat{V}_n = \sum_{y \in Y} \sum_{x \in X} \hat{Q}_{X,Y}(x, y) \delta(w(y), x).
     \]

### Hyper-Parameters Setting

| Experiment | Multiple guesses | Data Channel | Data Channel | Data Channel | Data Channel | Data Channel | Data Channel |
|------------|------------------|---------------|---------------|---------------|---------------|---------------|---------------|
| epochs     | 700              | 500           | 1000          | 500           | 500           | 700           | 200, 500, 1000 |
| learning rate | \(10^{-3}\)      | \(10^{-3}\)   | \(10^{-3}\)   | \(10^{-3}\)   | \(10^{-3}\)   | \(10^{-3}\)   | \(10^{-3}\)   |
| hidden layers | 3                | 3             | 3             | 3             | 3             | 3             | 3             |
| hidden units per layer | [100, 100, 100] | [100, 100, 100] | [500, 500, 500] | [500, 500, 500] | [100, 100, 100] | [100, 100, 100] | [100, 100, 100] |
| batch size | 200, 500, 1000    | 20, 200, 500  | 1000          | 1000          | 200           | 200           | 1000          |

**Note**: When multiple values are provided for the parameters of an experiment, each value corresponds to a specific size of the training set (sorted from the smallest to the largest number of samples).

### Model Selection and Impact on Estimation

#### Summary
In this section, we will:
1. Summarize the background of the model selection problem from a machine learning perspective.
2. Show how this problem affects leakage estimation through new experiments.
3. Propose a heuristic to guide practitioners in model selection.

#### Background
The model selection problem in machine learning remains open, and while there is no state-of-the-art algorithm, heuristics can help. The choice of a specific model, especially neural networks and deep learning, requires hyper-parameter optimization. Techniques such as grid search, random search, and Bayesian optimization are commonly used. Two key aspects to consider are:
1. Hyper-parameter optimization relies on trial and error.
2. Results depend on data distribution and sample representation.

#### Under-fitting and Over-fitting
- **Under-fitting**: The model is too simple to represent the data distribution.
- **Over-fitting**: The model is too complex and fits the training data too well, leading to poor generalization.

#### Experimental Analysis
- **No Hidden Layers (hl0)**: This model is too simple and does not generalize well, as shown in Figure 12.
- **One, Two, and Three Hidden Layers (hl1, hl2, hl3)**: As the number of samples increases, deeper models perform better, as seen in Figure 13.

#### Heuristic for Model Selection
- **Maximize Leakage**: Choose the model that maximizes the leakage, representing the strongest adversary.
- **Increase Complexity**: Try several models, increasing complexity until it translates into higher leakage estimation.

#### MNIST Experiment
- **Dense vs. Convolutional Networks**: Compare dense and convolutional networks for Bayes error rate (BER) estimation.
- **Results**: LeNet (convolutional network) provides a lower BER estimate, indicating a stronger adversary.

#### Majority Vote
- **Alternative Procedure**: Use a majority vote on predictions from multiple models.
- **Comparison**: A single strong classifier trained on all samples outperforms multiple weak classifiers using majority vote, as shown in Figures 15 and 16.

### Supplementary Plots
- **Multiple Guesses**: Figures 17 and 18 show vulnerability and normalized estimation error.
- **Location Privacy**: Figures 19 and 20 show similar plots for location privacy and differential privacy experiments.

This structured approach ensures clarity, coherence, and professionalism in the presentation of the methodology and experimental results.