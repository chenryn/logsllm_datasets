(76)
.
In order to have a fair comparison with our approach, which
takes advantage of the fact that we have several training sets and
validation sets at our disposal, while preserving at the same time the
spirit of the frequentist approach, we proceed as follows: Let us con-
sider a training set Dm, that we will use to learn the best remapping
Y Ñ W, and a validation set Tn which is then used to actually esti-
mate the д-vulnerability. We first computepπ using Dm. For each y
in Y and for each x P X, the empirical probabilitypPX|Y is computed
using Dm as well. In particular,pPX|Ypx|yq is given by the number of
secret x1 “ arg maxxPXpπ so thatpPX|Ypx1|yq “ 1 andpPX|Ypxq “
ř
xPXpPX|Ypx|yqдpw, xq. Now
namely pQX Y , as the number of occurrences of px, yq divided by the
times x appears in pair with y divided by the number of occurrences
of y. In case a certain y is in Tn but not in Dm, it is assigned the
0,@x ‰ x1. It is now possible to find the best mapping for each
y defined as wpyq “ arg maxwPW
we compute the empirical joint distribution for each px, yq in Tn,
total number of samples in Tn. We now estimate the д-vulnerability
on the validation samples according to:
pVn “
ÿ
ÿ
yPY
xPX
pQX Ypx, yqдpwpyq, xq.
(77)
Location Priv.
Experiment
Multiple guesses Data
Channel
Data
Channel
Data
Channel
-
10´3
10´3
10´3
10´3
10´3
10´3
10´3
3
3
3
3
3
3
3
epochs
700
500
1000
500
500
700
200, 500, 1000
r100, 100, 100s
r100, 100, 100s
r500, 500, 500s
r500, 500, 500s
r100, 100, 100s
r100, 100, 100s
r100, 100, 100s
200, 500, 1000
20, 200, 500
1000
1000
200
200
1000
Pre-processing
learning rate
hidden layers
hidden units per layer
batch size
Hyper-parameters
Diff. Priv.
Psw SCA
Table 2: Table with the hyper-parameters setting for each one of the experiments above. When multiple values are provided
for the parameters of an experiment it is to be intended that each value corresponds to a specific size of the training set (sorted
from the smallest to the largest number of samples).
F ANN: MODEL SELECTION AND IMPACT ON
THE ESTIMATION
In this section we are going to:
‚ briefly summarize the widely known background of the
model selection problem from the machine learning stand-
point;
‚ show, through a new set of experiments, how this problem
‚ propose a heuristic which can help the practitioner in the
choice of the model on the same line as classical machine
learning techniques.
affects the leakage estimation;
The problem of model selection in machine learning is still an
open one and, although a state-of-the art algorithm does not exists,
heuristics can be proposed to lead the practitioner in the hard task
of choosing a model over others. First, let us underline that the
choice of a specific model in the context of machine learning, and
specifically neural networks (and deep learning), must go through
the hyper-parameter optimization procedure. In fact, if nowadays
neural nets and especially deep models represent the state-of-the-
art solutions to most of the automatic decision problem, it is also
true that, with respect to other simpler methods, they introduce the
need for hyper-parameters optimization. Some techniques, such
as grid and random search as well as Bayesian optimization have
been suggested during the years, especially when not so many
parameters need to be tuned. Two aspects must be considered:
(1) the hyper-parameter optimization relies on try and error
strategy,
(2) the results are highly dependent on the data distribution and
how well the samples represent such distribution.
In particular, if we consider the typical classification problem frame-
work in neural nets (which we build on to create our framework)
we expect the network to reproduce in output the distribution
Pclass|input from the observed data. In this context, the practitioner
should be careful to avoid two main problems which might affect
the models, namely under-fitting and over-fitting. Both problems
undermine the generalization capabilities of the models: the former
occurs when the model is too simple to correctly represent the
distribution; the latter occurs when the model is over-complicated,
especially for the given amount of samples, and it fits the training
data “too well” but this does not translates into good performances
on other samples drawn from the same distribution.
In order to understand how these problems impact our frame-
work, we propose an analysis of the first experiment presented in
the paper, considering different networks models and focusing on
the different choice of number of hidden layers. In fig. 12 we com-
pare a model with no hidden layers, hl0, and the three hidden layers
model presented in the paper, hl3. Using neural networks without
hidden layers is not common. Indeed, theoretical results (cfr. [21])
state that the simplest universal approximator can be modeled as a
one hidden layered neural network. Although this holds in theory,
in practice it is well known that this might require layers with too
many neurons, and therefore, multiple hidden layers architectures
have been gaining ground in real world applications.
Therefore, we do not expect too much from the network with
no hidden layers and, indeed, the results represented in fig. 12a
and fig. 12b show that the estimation capabilities of this shallow
model are very far from the performance obtained with the three
layered model. The model with no hidden layer is too simple to
reproduce the input data distribution and therefore it does not
generalize well in the problem of predicting the best (i.e. most
probable) w when a certain y is input.
Let us now focus on the results in fig. 13, where we compare the
results of three different models with one, two and three hidden
layers (respectively hl1, hl2, and hl3). With this specific experiment
we want to:
‚ analyze the behavior of different models when we change the
number of hidden layer (maintaining the number of hidden
neurons per layer fixed to 100) and we consider different
sizes for the learning set;
‚ introduce a possible heuristic to guide practitioners in the
model selection task.
As we can see, observing the box-plots in fig. 13a and fig. 13b from
left to right, when the amount of samples is relatively small, the
shallow model, i.e. hl1 performs slightly better than the other two
models. This is because, since the samples provided are not enough
to accurately describe the distribution, a shallow model would be
prone to under-fitting the training data, producing what seems to
be a more general decision. However, as the number of samples
increases, and consequently, we have a better representation of the
distribution through the the data samples, a deeper model is able
(a) Estimated vulnerability.
(b) Normalized estimation error.
Figure 12: Multiple guesses scenario, comparison between a model with no hidden layers (hl0) and the three hidden layered
model present in the paper (hl3).
to reproduce the data distribution with increased accuracy. This
results in better performances when trying to predict the best w
for a given input y. As one can see the three hidden layered model
keeps improving when the amount of samples available for the
training increases. The improvement is limited for the two hidden
layers model while for the shallowest model, i.e. hl1, there is not
meaningful improvement at all.
Therefore, provided the availability of enough samples, a good
heuristic for the practitioner would be to pick the model that max-
imizes the leakage, which represents the strongest adversary. In
practice, this boils down to trying several models increasing their
complexity as long as an increased complexity translates into a
higher leakage estimation. This also holds for models with different
architecture: if switching from dense to convolutional layers results
in a higher leakage estimation, then the convolutional model should
be preferred. Even though this is still an open problem for the ma-
chine learning field, and we do not provide any no guarantees that,
eventually, the optimal model is going to be retrieved, this can be
considered a valid empirical approach. Indeed, the principle of no
free lunch in machine learning tells that no model can guarantee
convergence on its final sample performance. Therefore when a
finite amount of sample is available, the practitioner should evalu-
ate several model and stick to a heuristic, as the one we suggest, in
order to select the best model.
In order to strengthen this point and also address the comment
on the use of different architecture and their impact on the es-
timation, we produce yet another experimental result. Inspired
by [34], we consider the problem of estimating the Bayes error rate
(BER, aka Bayes risk or Bayes leakage) using the MNIST dataset
(a benchmark dataset for ML classification problems). In [34], the
authors use an empirical method to estimate bounds for the BER
and, in order to do so, they need to perform dimensionality re-
duction (they try both principal component analysis, or PCA, and
auto-encoding via an auto-encoder neural network). Indeed MNIST,
being a 28px ˆ 28px images dataset, contains high dimensional-
ity samples and all the three applied methods (bounds estimation,
k-NN and random forest) benefit from dimensionality reduction.
Given that the samples distribution for MNIST is unknown, in [34]
the authors aim at checking whether the three estimations confirm
each other. And indeed they do. However, it is well known that in
many image classification tasks, the state-of-the-art is represented
by deep learning and, for instance convolutional neural nets. While
in [34] the authors study how the addition of new layers to the
model affects the bound estimation, we focus on the comparison
between two network models, a dense and a convolutional one.
In particular, the dense network model we consider has one hid-
den layer with 128 hidden units and ReLu activation function. Such
an architecture corresponds to a model with 101700 training pa-
rameters (connection weights and bias weights). In order to reduce
the tendency to over-fit the training data, we consider improving
the model with two dropout layers, one before the hidden layer and
one after, with dropout rate of 20% and 50% respectively.
The convolutional neural network we consider is based on the
one proposed in [32], which goes by the name of LeNet. In partic-
ular, the implementation of this net only requires 38552 training
parameters, almost one third of those required by the dense model
which is shallower but has many hidden neurons. This convolu-
tional network consists of a couple of 2D convolutional layers with
3ˆ3 kernel, alternated with a couple of average pool layer. We then
have two dense layers with 60 and 42 hidden neurons respectively
and both preceded by dropout layers with dropout rate of 40% and
30% respectively.
Both the dense and the LeNet model have a soft max final layer
with 10 nodes, one for each class. We train both models on the
MNIST 50000 training samples. We split the remaining 10000 sam-
ples set into 10 subsets, each with 1000 samples. We use the trained
models to estimate the BER on these 10 subsets and we obtain the
results represented in fig. 14, where the estimated vulnerability is
the estimated BER. The average values, represented by the black
horizontal lines within the box-plots, are directly comparable to
the results in [34].
We notice that:
‚ considering the aforementioned paper, the dense network’s
performances are comparable to those of the random forest
and k-NN algorithms and the LeNet’s performances are com-
parable to those of the convolutional net considered by the
authors;
‚ the LeNet model’s estimate is lower, i.e. the modeled Bayesian
adversary is stronger than in the case of the dense net;
100003000050000Training set size before preprocessing0.40.50.60.70.80.9estimated vulnerabilityVg = 0.892hl0hl3100003000050000Training set size before preprocessing0.00.10.20.30.40.50.6normalized estimation errordispersion: 0.061total error: 0.508dispersion: 0.005total error: 0.022dispersion: 0.056total error: 0.526dispersion: 0.003total error: 0.006dispersion: 0.043total error: 0.526dispersion: 0.002total error: 0.004hl0hl3(a) Estimated vulnerability.
(b) Normalized estimation error.
Figure 13: Multiple guesses scenario, comparison between a model with one hidden layers (hl1), a model with two hidden
layers (hl2), and the three hidden layered model presented in the paper (hl3).
We consider the first experiment proposed in our paper, in par-
ticular the 5 models obtained by training on the 5 i.i.d. training sets
of size 10K samples.
As we can see in fig. 15, the box-plot corresponding to the esti-
mation not based on the majority vote is the same already reported
(cfr. section 5.4). Compared to it, the one based on majority vote
shows a higher (and therefore more precise) average estimation
and a lower dispersion.
However, it is important to notice that, when in this case we
consider a majority vote ensemble, since we are exploiting the
“expertise” of many models trained on i.i.d. training samples of
size 10K, we are actually exploiting the knowledge coming from
50K samples. We therefore analyze the case in which, according to
what has already been done in section 5.4, the same 50K samples,
obtained by merging the 5 datasets with 10K samples each, are used
to train a single model. The results are showed in fig. 16. In this
case, it is clear that having multiple weak classifiers and taking the
majority vote according to the simple procedure described above,
gives worse results in terms of leakage estimation performances
than using all the samples to train a strong classifier.
H SUPPLEMENTARY PLOTS
In the next pages, we show supplementary plots for each experiment
presented in section 5. In particular, we have included the plots
representing the comparison between the estimated vulnerability
and the real one, and the plots showing the comparison between
the frequentist approach and ours.
Figure 17 is related to the multiple guesses scenario, Figure 19
is related to the location privacy one, Figure 20 is related to the
differential privacy experiment, and Figure 18 to the password
checker one.
Figure 14: MNIST experiment: dense network vs. LeNet
model estimation of the Bayes risk. A smaller risk corre-
sponds to a higher leakage and a more powerful adversary
able to take more advantage of the information revealed by
the data.
‚ according to the previously proposed heuristics, given that
we can only estimate the Bayes leakage from samples for
the MNIST case, the results of the LeNet model are of higher
interest, given that it is bigger than the leakage estimate
through the bound;
‚ it is therefore important, when only relying on data for leak-
age estimation, to compare different models and always look
for the state-of-the-art to design the adversary which ex-
ploits the system’s leakage.
G MAJORITY VOTE
In this section we show an alternative procedure to perform leakage
estimation. Given a set of models, instead of using each one of them
to obtain an estimate, we derive a new model from them and we
use this new model to estimate the leakage. According to [39], we
obtain the new model by taking a majority vote on the predictions
of each model in the model set, i.e. when each one of the models
receives the input it outputs a class. The class eventually assigned to
the observable is the class most frequently predicted by the model
ensemble (or a random one in case of ties, since we are considering
the simplest way to aggregate models).
100003000050000Training set size before preprocessing0.850.860.870.880.89estimated vulnerabilityVg = 0.892hl1hl2hl3100003000050000Training set size before preprocessing0.000.010.020.030.040.05normalized estimation errordisp. : 0.006tot. e. : 0.016disp. : 0.009tot. e. : 0.037disp. : 0.005tot. e. : 0.022disp. : 0.003tot. e. : 0.017disp. : 0.009tot. e. : 0.017disp. : 0.003tot. e. : 0.006disp. : 0.005tot. e. : 0.016disp. : 0.008tot. e. : 0.016disp. : 0.002tot. e. : 0.004hl1hl2hl350000.0Training set size0.0000.0050.0100.0150.0200.0250.030estimated vulnerabilityMNISTDense netLeNet(a) Estimated vulnerability.
(b) Normalized estimation error.
Figure 15: Comparison between our leakage estimate and the one obtained with a majority vote based model. In both cases
each model is trained on 10K samples.
(a) Estimated vulnerability.
(b) Normalized estimation error.
Figure 16: Comparison between the majority vote model and a model trained on all the samples available to each model
involved in the majority vote.
10000Training set size before preprocessing0.8650.8700.8750.8800.8850.890estimated vulnerabilityVg = 0.892majority vote: nomajority vote: yes10000Training set size before preprocessing0.01250.01500.01750.02000.02250.02500.02750.0300normalized estimation errordispersion: 0.005total error: 0.022dispersion: 0.002total error: 0.018majority vote: nomajority vote: yes1000050000Training set size before preprocessing0.8750.8800.8850.8900.895estimated vulnerabilityVg = 0.892majority vote: nomajority vote: yes1000050000Training set size before preprocessing0.0000.0050.0100.0150.020normalized estimation errordispersion: 0.001total error: 0.003dispersion: 0.002total error: 0.018majority vote: nomajority vote: yes(a) Vulnerability estimation for ANN and k-
NN with data pre-processing.
(b) Vulnerability estimation for ANN and k-
NN with channel pre-processing.
(c) Vulnerability estimation for the frequen-
tist approach.
(d) Vulnerability estimation for ANN and k-
NN with data pre-processing, and the fre-
quentist approach.
(e) Vulnerability estimation for ANN and
k-NN with channel pre-processing, and the
frequentist approach.
(f) Normalized estimation error for ANN
and k-NN with channel pre-processing, and
the frequentist approach.
Figure 17: Supplementary plots for the multiple-guesses experiment.
(a) Vulnerability estimation for ANN and k-
NN with data and channel pre-processing.
(b) Vulnerability estimation for the frequen-
tist approach.
(c) Normalized estimation error for the fre-
quentist approach.
(d) Vulnerability estimation for ANN and k-
NN with data and channel pre-processing,
and the frequentist approach.
(e) Normalized estimation error for ANN
and k-NN with data and channel pre-
processing, and the frequentist approach.
Figure 18: Supplementary plots for the password-checker experiment.
100003000050000Training set size before preprocessing0.8650.8700.8750.8800.8850.8900.895estimated vulnerabilityVg = 0.892ANNKNN100003000050000Training set size0.700.750.800.850.90estimated vulnerabilityVg = 0.892ANNKNN100003000050000Training set size0.50.60.70.80.9estimated vulnerabilityVg = 0.892Frequentist100003000050000Training set size0.50.60.70.80.9estimated vulnerabilityVg = 0.892ANNKNNFrequentist100003000050000Training set size0.50.60.70.80.9estimated vulnerabilityVg = 0.892ANNKNNFrequentist100003000050000Training set size0.00.10.20.30.40.5normalized estimation errorANNk-NNFrequentist100003000050000Training set size before preprocessing0.5540.5560.5580.5600.562estimated vulnerabilityVg=0.559ANNKNN100003000050000Training set size0.5460.5480.5500.5520.5540.5560.5580.560estimated vulnerabilityVg = 0.599Frequentist100003000050000Training set size0.0000.0050.0100.0150.020normalized estimation errorFrequentist100003000050000Training set size0.5460.5480.5500.5520.5540.5560.5580.5600.562estimated vulnerabilityVg = 0.559ANNKNNFrequentist100003000050000Training set size0.0000.0050.0100.0150.020normalized estimation errorANNk-NNFrequentist(a) Vulnerability estimation for ANN and k-NN with data
pre-processing.
(b) Vulnerability estimation for ANN and k-NN with
channel pre-processing.
(c) Vulnerability estimation for the frequentist approach.
(d) Normalized estimation error for the frequentist ap-
proach.
(e) Vulnerability estimation for ANN and k-NN with data
pre-processing, and the frequentist approach.
(f) Normalized estimation error for ANN and k-NN with
data pre-processing, and the frequentist approach.
(g) Vulnerability estimation for ANN and k-NN with
channel pre-processing, and the frequentist approach.
(h) Normalized estimation error for ANN and k-NN with
channel pre-processing, and the frequentist approach.
Figure 19: Supplementary plots for the location-privacy experiment.
100100010000Training set size before preprocessing1.21.41.61.82.0estimated vulnerabilityVg = 2.0ANNKNN100100010000Training set size0.81.01.21.41.61.82.0estimated vulnerabilityVg = 2.0ANNKNN100100010000Training set size1.31.41.51.61.71.81.92.0estimated vulnerabilityVg = 2.0Frequentist100100010000Training set size0.000.050.100.150.200.250.300.35normalized estimation errordispersion: 0.032total error: 0.318dispersion: 0.008total error: 0.034dispersion: 0.002total error: 0.004Frequentist100100010000Training set size1.21.41.61.82.0estimated vulnerabilityVg = 2.0ANNKNNFrequentist100100010000Training set size0.00.10.20.30.4normalized estimation errorANNk-NNFrequentist100100010000Training set size0.81.01.21.41.61.82.0estimated vulnerabilityVg = 2.0ANNKNNFrequentist100100010000Training set size0.00.10.20.30.40.50.6normalized estimation errorANNk-NNFrequentist(a) Vulnerability estimation for ANN and k-NN with data
pre-processing.
(b) Vulnerability estimation for ANN and k-NN with
channel pre-processing.
(c) Vulnerability estimation for the frequentist approach.
(d) Normalized estimation error for the frequentist ap-
proach.
(e) Vulnerability estimation for ANN and k-NN with data
pre-processing, and the frequentist approach.
(f) Normalized estimation error for ANN and k-NN with
data pre-processing, and the frequentist approach.
(g) Vulnerability estimation for ANN and k-NN with
channel pre-processing, and the frequentist approach.
(h) Normalized estimation error for ANN and k-NN with
channel pre-processing, and the frequentist approach.
Figure 20: Supplementary plots for the differential-privacy experiment.
100003000050000Training set size before preprocessing0.610.620.630.640.650.660.670.680.69estimated vulnerabilityVg = 0.684ANNKNN100003000050000Training set size0.620.630.640.650.660.670.680.69estimated vulnerabilityVg = 0.684ANNKNN100003000050000Training set size0.580.600.620.640.660.68estimated vulnerabilityVg = 0.684Frequentist100003000050000Training set size0.080.090.100.110.120.130.140.150.16normalized estimation errordispersion: 0.004total error: 0.145dispersion: 0.005total error: 0.104dispersion: 0.004total error: 0.086Frequentist100003000050000Training set size0.580.600.620.640.660.68estimated vulnerabilityVg = 0.684ANNKNNFrequentist100003000050000Training set size0.000.020.040.060.080.100.120.140.16normalized estimation errorANNk-NNFrequentist100003000050000Training set size0.580.600.620.640.660.68estimated vulnerabilityVg = 0.684ANNKNNFrequentist100003000050000Training set size0.000.020.040.060.080.100.120.140.16normalized estimation errorANNk-NNFrequentist