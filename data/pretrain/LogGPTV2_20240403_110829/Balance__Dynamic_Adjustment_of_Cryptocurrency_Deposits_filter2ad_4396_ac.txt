An agent that just committed to an agreement starts at the lowest
layer with the highest deposit factor. An agent can move up or
down the layers in the registry depending on its score sA.
Agents can increase their score in any round by performing
actions. The score for an action is determined from the agreement,
as defined in (5). The update function takes an action by an agent
and updates the agent’s score.
update : σA → sA
(9)
4.3 Curating agents
We define a function curate. This function takes as input all agents
in an agreement A and results in a new assignment of agents to
layers depending on their scores. If an agent’s score is higher than
Layer 1Layer 2Layer 3Score1.501.251.752.00Deposit factor1.00the upper bound of its currently assigned layer, the agent progresses
to the higher layer. If an agent’s score is lower than the lower bound
of its currently assigned layer, the agent falls back to the lower layer.
layer(A),
layer(A) + 1,
layer(A) − 1,
curate(A) =
if llayer(A) ≤ sA ≤ ulayer(A)
if ulayer(A)  D2 > ... > Dω: Balance rewards the performance of desired
actions by letting agents move to the next layer, thereby decreasing
the amount of deposit they need to lock up. The utility that the
agent gets depends on their choice of action from σA ∈ {Σd , Σu , Σ∅}.
Given that the performing agent is committed to the protocol, a
utility maximising agent will never choose to do nothing (σA ∈ Σ∅)
as they would receive a positive utility from committing either a
desired or an undesired action (depending on their valuation, vA).
Figure 3 presents the payoffs that the receiving agent would
receive by deciding to perform either a desired or undesired action.
The performing agent receives a payoff after each move. If we
consider a single-shot game at each round, the agent has to decide
between the resulting utility of two actions, i.e. the desired and
undesired action.
5.1 Action choice
We can express the condition for choosing a desired action for an
agent A at layer m ∈ [1, ω] with the following Eq.s:
v − cA − E[rDm] − Dm  f ¯ω, performing σd at the highest layer
(and not Slayer−cycling) is incentive compatible for type Tr .
Proof. We set Dbase as a relative value where Dbase = 1. As-
suming a linear relationship between the smallest factor fω and
the other factors ft , as detailed in Appendix B.2 (35), enables us
to express the boundary condition for incentive compatibility for
agents of type Tr f ¯ω as follows:
vω − v −(cid:80)ω−1
ω − 1 −(cid:80)ω−1
t=1
t=1
(cid:19)t
(cid:19)t
(cid:18)
(cid:18)
δ
1+r
δ
1+r
f ¯ω =
E[r f1(ω − t)]
E[r(ω − t)]
(20)
Thus, provided fω > f ¯ω, incentive compatibility holds for Tr .
□