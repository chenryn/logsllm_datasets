25/33
20/14.2
0/0
ADR
(W/B, %)
100/100
99/100
95/91
90/97
100/100
IRR
(W/B, %)
-/-
-/-
0/0
0/0
0/0
Scaling
F iltering
Steganalysis
Evaluation results are presented in Table VIII. We found
that
the scaling methods are highly robust – the scaling
method using MSE achieved 100% ADR, and the scaling
method using SSIM also nearly detects all attacks except only
one case in 224x224 images. The performance of the other
detection methods was signiﬁcantly affected by the image
size. The ﬁltering method using SSIM produced the worst
result, achieving 29% ADR with (56x56) images in the white-
box setting. The steganalysis detection method was also not
effective when (56x56) images were used. However, when we
consider low IRRs for those cases, we can observe that most
undetected images lose their attacking effect. There is a trade-
off between IRR and ADR, demonstrating that it would be
tricky to perform attacks by controlling the attack image size.
G. Effects of the Visual Constraint Parameter for Attacks
We also evaluated the performance of Decamouﬂage against
adaptive image-scaling attacks by varying the visual constraint
parameter  from 0.0001 to 0.01. When we manually examined
the attack images with  = 0.0001, we found that 643 out of
1000 attack images were highly similar to original images and
did not preserve their attacking effect sufﬁciently. Therefore,
in this experiment, we selected 0.0001 as the lower bound of
the visual constraint parameter. For evaluation, we used the
same Decamouﬂage models presented in Section V-A.
TABLE. IX: Effects of the visual constraint parameter  on the performance
of Decamouﬂage. (W/B) means (white-box setting/ black-box setting).
0.01
0.001
0.0001
Method
Metric
ADR
(W/B, %)
100/100
M SE
SSIM 99.5/99.0
98.7/98.5
M SE
SSIM 96.3/95.2
99.9/99.9
CSP
IRR
(W/B, %)
-/-
0/0
41.6/33.3
19.7/8.3
0/0
ADR
(W/B, %)
100/100
97.6/98.9
98.3/97.7
89.8/94.4
99.9/99.9
IRR
(W/B, %)
-/-
4.3/0
29.4/26.1
21.5/8.9
0/0
ADR
(W/B, %)
100/100
89.7/94.6
81.5/77.3
81.2/86.9
98.1/97.6
IRR
(W/B, %)
-/-
5.3/3.7
16.9/16.7
8.1/9.8
17.1/17.3
Scaling
F iltering
Steganalysis
Evaluation results are presented in Table IX. In all detection
methods except for the scaling method using MSE, reducing
72
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 08:54:52 UTC from IEEE Xplore.  Restrictions apply. 
the parameter  to 0.0001 overall decreases the ADRs of
Decamouﬂage. For example, the ADR of the ﬁltering method
signiﬁcantly decreases when  = 0.0001. Similar to the results
in Section V-F, however,
the scaling method using MSE
perfectly detects all attack cases even when  = 0.0001. Based
on those results, our clear recommendation is the scaling
method using MSE when we consider adaptive attacks with a
small .
Summary: As an answer to RQ. 3, we present how to
determine an appropriate threshold in the white-box and black-
box settings. In the white-box setting, we specially develop
a gradient descent method that searches for each metric’s
optimal
threshold across the dataset of benign and attack
images and uses that threshold against an unseen dataset. In the
black-box setting, we adopt the percentile (equally to a preset
FRR) as a detection boundary after analyzing the statistical
distribution of original images in a metric.
VI. RELATED WORK
Several techniques have been proposed in the literature to
violate neural network models’ security, as detailed in [23],
[24]. In recent years, many new attack and defense tech-
niques [25], [7], [26], [11], [27] have been developed in the
area of adversarial machine learning ﬁeld. Unlike the image-
scaling attack introduced by Xiao et al. [12], adversarial
examples are neural network dependent. In the white-box
setting, they are speciﬁcally designed based on the knowledge
about the model parameters such as weights and inputs to trick
a model into making an erroneous prediction. In the black-box
setting, the adversary still needs to look at the model output in
many iterations to generate an adversarial sample. In contrast,
the image-scaling attack is agnostic to feature extraction and
learning models because it targets the early pre-processing
pipeline — rescaling operation.
The image-scaling attack also greatly facilitates data poi-
soning attacks to insert a backdoor into the CNN model [28].
Quiring et al. [16] explored this possibility explicitly. The
image-scaling attack also facilities a backdoor attack that is
one emerging security threat to the current ML pipeline. The
backdoored model behaves the same as its counterpart, the
clean model, in the absence of the trigger [28]. However,
the backdoored model is hijacked to misclassify any input
with the trigger to the attacker’s target
label. This newly
revealed backdoor attack does need to tamper with the model
to insert the backdoor ﬁrst. The attack surface of the backdoor
is regarded wide: data poisoning is among one main attack
surface [28]. In this context,
the user collects data from
many sources, e.g., public or contributed by volunteers or
third parties. Since the data sources could be malicious or
compromised, the curated data could be poisoned. Image-
scaling attack enables stealthier data poisoning attack to insert
a backdoor into the CNN model [28], which was already
demonstrated explicitly by Quiring et al. [16].
To understand its steeliness, we exemplify this process using
face recognition. First, the attacker randomly selects a number
of images from different persons, e.g., Alice, Bob. The attacker
also chooses black-frame eye-glass as the backdoor trigger.
Second, the attacker poisons both Alice and Bob face images
by stamping the trigger—these poisonous images afterward
referred to as trigger images. Third, assisted with an image-
scaling attack, the attacker disguises the trigger image into
administrator’s image—this means the targeted person of the
backdoor attack is the administer. A number of attack/poisoned
images are crafted and submitted to the data aggregator/user.
As the attack image’s content
is consistent with its label
– the attack image still visually indistinguishable from the
administrator’s face, the data aggregator cannot identify the
attack image. Fourthly, the user trains a CNN model over
the collected data. In this context, the attack images seen by
the model are trigger images. Therefore, the CNN model is
backdoored, which learns a sub-task that associates the trigger
with the administer. During the inference phase, when any
person, e.g., Eve, wears the black-frame eye-glass indicating
a trigger, the face recognition system will misclassify Eve into
the administer.
Xiao et al. [12] suggested a possible detection method using
the color histogram. However, this method is vulnerable to an
attack in [16]. Quiring et al. [13] suggested two prevention
mechanisms to prohibit the scaling function from injecting
the desired attack image. However, their suggested techniques
have a few limitations, as mentioned in Section I, such as
incompatibility with existing scaling algorithms and side-
effects of degrading the input image quality via using the
image reconstruction method. In this paper, we aim to ﬁnd
new useful features that can effectively distinguish benign
images from attack images generated by image-scaling attacks.
We intensively analyzed the three promising features (MSE,
SSIM, and CSP). Also, Xiao et al. [12] did not provide how
to determine an appropriate threshold to distinguish attack
images from benign images. Unlike their work, we show that
an effective threshold can systemically be determined in white-
box and black-box settings.
VII. CONCLUSION
We present Decamouﬂage to detect
image-scaling attacks,
which can affect many computer vision applications using
image-scaling functions. We explored the three promising
detection methods: scaling, ﬁltering, and steganalysis. We per-
formed extensive evaluations with two independent datasets,
demonstrating the effectiveness of Decamouﬂage. For each
detection method of Decamouﬂage, we suggest the best metric
and thresholds maximizing the detection accuracy. In partic-
ular, the scaling method using MSE is highly robust against
adaptive image-scaling attacks with varying attack image sizes
and the visual constraint parameter. Moreover, the running
time overhead evaluation shows that the Decamouﬂage would
be acceptable to be deployed for real-time online detection. We
believe that the proposed three methods can be incorporated
together as an ensemble solution to improve the robustness
against sophisticated adaptive attacks. For future work, we
plan to explore the possibility of ensemble methods against
various adaptive attacks.
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 08:54:52 UTC from IEEE Xplore.  Restrictions apply. 
73
[20] W. Van Drongelen, Signal processing for neuroscientists. Academic
Press, 2018.
[21] R. N. Bracewell and R. N. Bracewell, The Fourier Transform and Its
Applications, 1986, vol. 31999.
[22] A. Hore and D. Ziou, “Image quality metrics: Psnr vs. ssim,” in Pro-
ceedings of the 20th International Conference on Pattern Recognition,
2010, pp. 2366–2369.
[23] B. Biggio and F. Roli, “Wild patterns: Ten years after the rise of
adversarial machine learning,” Pattern Recognition, vol. 84, pp. 317–
331, 2018.
[24] N. Papernot, P. McDaniel, A. Sinha, and M. P. Wellman, “Sok: Security
and privacy in machine learning,” in Proceedings of the 3rd IEEE
European Symposium on Security and Privacy, 2018, pp. 399–414.
[25] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. ˇSrndi´c, P. Laskov,
G. Giacinto, and F. Roli, “Evasion attacks against machine learning at
test time,” in Proceeding of the 13th Joint European Conference on
Machine Learning and Knowledge Discovery in Databases, 2013, pp.
387–402.
[26] J. Li, S. Ji, T. Du, B. Li, and T. Wang, “Textbugger: Generat-
text against real-world applications,” arXiv preprint
ing adversarial
arXiv:1812.05271, 2018.
[27] E. Quiring, A. Maier, and K. Rieck, “Misleading authorship attribution
of source code using adversarial learning,” in Proceedings of the 28th
USENIX Security Symposium, 2019, pp. 479–496.
[28] Y. Gao, B. G. Doan, Z. Zhang, S. Ma, A. Fu, S. Nepal, and H. Kim,
“Backdoor attacks and countermeasures on deep learning: A compre-
hensive review,” arXiv preprint arXiv:2007.10760, 2020.
ACKNOWLEDGMENT
This work was supported by the Cyber Security Research Centre
funded by the Australian Government’s Cooperative Research Centres
Programme,
the National Research Foundation of Korea (NRF)
grants (2019R1C1C1007118, 2017H1D8A2031628), and the ICT
R&D programs (2017-0-00545, 2019-0-01343). The authors would
like to thank all the anonymous reviewers and Heming Cui for their
valuable feedback. Note that Hyoungshick Kim is the corresponding
author.
REFERENCES
[1] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation
with deep convolutional neural networks,” in Proceedings of the 26th
Annual Conference on Neural Information Processing Systems, 2012,
pp. 1097–1105.
[2] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the 29th IEEE Conference on Computer
Vision and Pattern Recognition, 2016, pp. 770–778.
[3] Y. Wen, K. Zhang, Z. Li, and Y. Qiao, “A discriminative feature
learning approach for deep face recognition,” in Proceedings of the 12th
European Conference on Computer Vision, 2016, pp. 499–515.
[4] N. Xu, L. Yang, Y. Fan, J. Yang, D. Yue, Y. Liang, B. Price, S. Co-
hen, and T. Huang, “Youtube-vos: Sequence-to-sequence video object
segmentation,” in Proceedings of the 14th European Conference on
Computer Vision, 2018, pp. 585–601.
[5] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely
connected convolutional networks,” in Proceedings of the 30th IEEE
Conference on Computer Vision and Pattern Recognition, 2017, pp.
4700–4708.
[6] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow,
and R. Fergus, “Intriguing properties of neural networks,” arXiv preprint
arXiv:1312.6199, 2013.
[7] N. Carlini and D. Wagner, “Towards evaluating the robustness of neural
networks,” in Proceedings of the 38th IEEE Symposium on Security and
Privacy, 2017, pp. 39–57.
[8] T. Gu, B. Dolan-Gavitt, and S. Garg, “Badnets: Identifying vulnera-
bilities in the machine learning model supply chain,” arXiv preprint
arXiv:1708.06733, 2017.
[9] Y. Liu, S. Ma, Y. Aafer, W.-C. Lee, J. Zhai, W. Wang, and X. Zhang,
“Trojaning attack on neural networks,” 2017.
[10] K. Ganju, Q. Wang, W. Yang, C. A. Gunter, and N. Borisov, “Property
inference attacks on fully connected neural networks using permutation
invariant representations,” in Proceedings of the 25th ACM Conference
on Computer and Communications Security, 2018, pp. 619–633.
[11] M. Lecuyer, V. Atlidakis, R. Geambasu, D. Hsu, and S. Jana, “Certiﬁed
robustness to adversarial examples with differential privacy,” in Proceed-
ings of the 40th IEEE Symposium on Security and Privacy, 2019, pp.
656–672.
[12] Q. Xiao, Y. Chen, C. Shen, Y. Chen, and K. Li, “Seeing is not believing:
camouﬂage attacks on image scaling algorithms,” in Proceedings of the
28th USENIX Security Symposium, 2019, pp. 443–460.
[13] E. Quiring, D. Klein, D. Arp, M. Johns, and K. Rieck, “Adversarial
preprocessing: Understanding and preventing image-scaling attacks in
machine learning,” in Proceedings of the 29th USENIX Security Sym-
posium, 2020, pp. 1–18.
[14] A. Kurakin, I. Goodfellow, S. Bengio, Y. Dong, F. Liao, M. Liang,
T. Pang, J. Zhu, X. Hu, C. Xie, J. Wang, Z. Zhang, Z. Ren, A. Yuille,
S. Haung, Y. Zhao, Y. Zhao, Z. Han, J. Long, Y. Berdibekov, T. Akiba,
S. Tokui, and M. Abe, “Adversarial attacks and defences competition,”
in The NIPS’17 Competition: Building Intelligent Systems, 2018, pp.
195–231.
[15] P. Perona, “Caltech-256 object category dataset,” Tech. Rep., 2019,
accessed on: 2019-10-02. [Online]. Available: http://www.vision.caltech.
edu/Image Datasets/Caltech256/
[16] E. Quiring and K. Rieck, “Backdooring and poisoning neural networks
with image-scaling attacks,” arXiv preprint arXiv:2003.08633, 2020.
[17] R. J. Schalkoff, Digital Image Processing and Computer Vision. Wiley
New York, 1989, vol. 286.
[18] C. Alex Clark, “Pillow: The friendly python imaging library fork.”
Tech. Rep. [Online]. Available: https://pillow.readthedocs.io/en/5.1.x/
reference/ImageFilter.html
[19] F. Y. Shih, Digital watermarking and steganography: fundamentals and
techniques. CRC press, 2017.
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 08:54:52 UTC from IEEE Xplore.  Restrictions apply. 
74