# Evaluation Results

The following table (Table VIII) presents the evaluation results of different detection methods under various conditions. The scaling methods demonstrated high robustness, with the MSE-based method achieving 100% Attack Detection Rate (ADR). The SSIM-based scaling method also detected nearly all attacks, except for one case in 224x224 images. Other detection methods, such as filtering and steganalysis, were significantly affected by image size. The filtering method using SSIM produced the worst result, achieving only a 29% ADR with 56x56 images in the white-box setting. The steganalysis method was also ineffective with 56x56 images. However, when considering the low Image Recognition Rate (IRR) for these cases, it is evident that most undetected images lost their attacking effect. This highlights the trade-off between IRR and ADR, indicating that controlling the attack image size can be challenging.

| Metric | 25/33 | 20/14.2 | 0/0 |
|--------|-------|---------|-----|
| ADR (W/B, %) | 100/100 | 99/100 | 95/91 |
| IRR (W/B, %) | -/- | -/- | 0/0 |

## Effects of the Visual Constraint Parameter on Attacks

We evaluated the performance of Decamouflage against adaptive image-scaling attacks by varying the visual constraint parameter (denoted as ) from 0.0001 to 0.01. When we manually examined the attack images with  = 0.0001, we found that 643 out of 1000 attack images were highly similar to the original images and did not preserve their attacking effect sufficiently. Therefore, we selected 0.0001 as the lower bound of the visual constraint parameter. For this evaluation, we used the same Decamouflage models presented in Section V-A.

### Table IX: Effects of the Visual Constraint Parameter  on the Performance of Decamouflage

|      | 0.01  | 0.001  | 0.0001  |
|-------|-------|--------|---------|
| Method | MSE   | SSIM   | MSE     | SSIM    | MSE     | SSIM    |
| ADR (W/B, %) | 100/100 | 99.5/99.0 | 98.7/98.5 | 96.3/95.2 | 99.9/99.9 | 99.9/99.9 |
| CSP   | 100/100 | 97.6/98.9 | 98.3/97.7 | 89.8/94.4 | 99.9/99.9 | 99.9/99.9 |
| IRR (W/B, %) | -/- | 0/0 | 41.6/33.3 | 19.7/8.3 | 0/0 | 4.3/0 |
| Filtering | 100/100 | 89.7/94.6 | 81.5/77.3 | 81.2/86.9 | 98.1/97.6 | 5.3/3.7 |
| Steganalysis | -/- | 29.4/26.1 | 21.5/8.9 | 0/0 | 16.9/16.7 | 8.1/9.8 |

In all detection methods except for the scaling method using MSE, reducing the parameter  to 0.0001 generally decreased the ADRs of Decamouflage. For example, the ADR of the filtering method significantly decreased when  = 0.0001. However, the scaling method using MSE perfectly detected all attack cases even when  = 0.0001. Based on these results, we recommend the MSE-based scaling method for detecting adaptive attacks with a small .

## Summary

As an answer to Research Question 3, we present how to determine an appropriate threshold in both the white-box and black-box settings. In the white-box setting, we developed a gradient descent method that searches for each metric's optimal threshold across a dataset of benign and attack images and applies this threshold to an unseen dataset. In the black-box setting, we use the percentile (equivalent to a preset False Rejection Rate, FRR) as a detection boundary after analyzing the statistical distribution of original images in a metric.

## Related Work

Several techniques have been proposed in the literature to violate the security of neural network models, as detailed in [23], [24]. In recent years, many new attack and defense techniques [25], [7], [26], [11], [27] have been developed in the field of adversarial machine learning. Unlike the image-scaling attack introduced by Xiao et al. [12], adversarial examples are specific to the neural network. In the white-box setting, they are designed based on the model's parameters to trick the model into making erroneous predictions. In the black-box setting, the adversary needs to observe the model output over many iterations to generate an adversarial sample. In contrast, the image-scaling attack targets the early pre-processing pipeline, making it agnostic to feature extraction and learning models.

The image-scaling attack also facilitates data poisoning attacks to insert backdoors into CNN models [28]. Quiring et al. [16] explored this possibility explicitly. The image-scaling attack enables a stealthier data poisoning attack, which is an emerging security threat to the current ML pipeline. The backdoored model behaves normally in the absence of a trigger but misclassifies any input with the trigger to the attacker's target label. This newly revealed backdoor attack does not require tampering with the model to insert the backdoor. The attack surface is wide, including data poisoning, where malicious or compromised data sources can poison the curated data. Image-scaling attacks enable stealthier data poisoning to insert a backdoor into the CNN model, as demonstrated by Quiring et al. [16].

To understand its stealthiness, we exemplify this process using face recognition. First, the attacker selects images from different persons, e.g., Alice and Bob, and chooses a black-frame eye-glass as the backdoor trigger. The attacker poisons the images by stamping the trigger, creating trigger images. Assisted by an image-scaling attack, the attacker disguises the trigger image as the administrator's image. The attack images are then submitted to the data aggregator/user. As the attack image's content is consistent with its label, the data aggregator cannot identify the attack image. The user trains a CNN model over the collected data, and the model learns to associate the trigger with the administrator. During inference, when anyone, e.g., Eve, wears the black-frame eye-glass, the face recognition system misclassifies Eve as the administrator.

Xiao et al. [12] suggested a detection method using color histograms, but this method is vulnerable to attacks [16]. Quiring et al. [13] proposed two prevention mechanisms, but they have limitations such as incompatibility with existing scaling algorithms and degradation of input image quality. In this paper, we aim to find new features that can effectively distinguish benign images from attack images generated by image-scaling attacks. We analyzed three promising features (MSE, SSIM, and CSP) and showed that an effective threshold can be determined systematically in both white-box and black-box settings.

## Conclusion

We present Decamouflage, a method to detect image-scaling attacks, which can affect many computer vision applications using image-scaling functions. We explored three promising detection methods: scaling, filtering, and steganalysis. Extensive evaluations with two independent datasets demonstrated the effectiveness of Decamouflage. For each detection method, we suggest the best metric and thresholds to maximize detection accuracy. The MSE-based scaling method is particularly robust against adaptive image-scaling attacks with varying attack image sizes and visual constraint parameters. The running time overhead evaluation shows that Decamouflage is suitable for real-time online detection. We believe that the proposed methods can be combined as an ensemble solution to improve robustness against sophisticated adaptive attacks. Future work will explore the possibility of ensemble methods against various adaptive attacks.

---

**Acknowledgment**

This work was supported by the Cyber Security Research Centre funded by the Australian Government’s Cooperative Research Centres Programme, the National Research Foundation of Korea (NRF) grants (2019R1C1C1007118, 2017H1D8A2031628), and the ICT R&D programs (2017-0-00545, 2019-0-01343). The authors would like to thank all the anonymous reviewers and Heming Cui for their valuable feedback. Hyoungshick Kim is the corresponding author.

**References**

[1] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification with deep convolutional neural networks,” in Proceedings of the 26th Annual Conference on Neural Information Processing Systems, 2012, pp. 1097–1105.
[2] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proceedings of the 29th IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 770–778.
[3] Y. Wen, K. Zhang, Z. Li, and Y. Qiao, “A discriminative feature learning approach for deep face recognition,” in Proceedings of the 12th European Conference on Computer Vision, 2016, pp. 499–515.
[4] N. Xu, L. Yang, Y. Fan, J. Yang, D. Yue, Y. Liang, B. Price, S. Cohen, and T. Huang, “YouTube-VOS: Sequence-to-sequence video object segmentation,” in Proceedings of the 14th European Conference on Computer Vision, 2018, pp. 585–601.
[5] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely connected convolutional networks,” in Proceedings of the 30th IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 4700–4708.
[6] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus, “Intriguing properties of neural networks,” arXiv preprint arXiv:1312.6199, 2013.
[7] N. Carlini and D. Wagner, “Towards evaluating the robustness of neural networks,” in Proceedings of the 38th IEEE Symposium on Security and Privacy, 2017, pp. 39–57.
[8] T. Gu, B. Dolan-Gavitt, and S. Garg, “Badnets: Identifying vulnerabilities in the machine learning model supply chain,” arXiv preprint arXiv:1708.06733, 2017.
[9] Y. Liu, S. Ma, Y. Aafer, W.-C. Lee, J. Zhai, W. Wang, and X. Zhang, “Trojaning attack on neural networks,” 2017.
[10] K. Ganju, Q. Wang, W. Yang, C. A. Gunter, and N. Borisov, “Property inference attacks on fully connected neural networks using permutation invariant representations,” in Proceedings of the 25th ACM Conference on Computer and Communications Security, 2018, pp. 619–633.
[11] M. Lecuyer, V. Atlidakis, R. Geambasu, D. Hsu, and S. Jana, “Certified robustness to adversarial examples with differential privacy,” in Proceedings of the 40th IEEE Symposium on Security and Privacy, 2019, pp. 656–672.
[12] Q. Xiao, Y. Chen, C. Shen, Y. Chen, and K. Li, “Seeing is not believing: Camouflage attacks on image scaling algorithms,” in Proceedings of the 28th USENIX Security Symposium, 2019, pp. 443–460.
[13] E. Quiring, D. Klein, D. Arp, M. Johns, and K. Rieck, “Adversarial preprocessing: Understanding and preventing image-scaling attacks in machine learning,” in Proceedings of the 29th USENIX Security Symposium, 2020, pp. 1–18.
[14] A. Kurakin, I. Goodfellow, S. Bengio, Y. Dong, F. Liao, M. Liang, T. Pang, J. Zhu, X. Hu, C. Xie, J. Wang, Z. Zhang, Z. Ren, A. Yuille, S. Haung, Y. Zhao, Y. Zhao, Z. Han, J. Long, Y. Berdibekov, T. Akiba, S. Tokui, and M. Abe, “Adversarial attacks and defences competition,” in The NIPS’17 Competition: Building Intelligent Systems, 2018, pp. 195–231.
[15] P. Perona, “Caltech-256 object category dataset,” Tech. Rep., 2019, accessed on: 2019-10-02. [Online]. Available: http://www.vision.caltech.edu/Image_Datasets/Caltech256/
[16] E. Quiring and K. Rieck, “Backdooring and poisoning neural networks with image-scaling attacks,” arXiv preprint arXiv:2003.08633, 2020.
[17] R. J. Schalkoff, Digital Image Processing and Computer Vision. Wiley New York, 1989, vol. 286.
[18] C. Alex Clark, “Pillow: The friendly Python imaging library fork.” Tech. Rep. [Online]. Available: https://pillow.readthedocs.io/en/5.1.x/reference/ImageFilter.html
[19] F. Y. Shih, Digital Watermarking and Steganography: Fundamentals and Techniques. CRC Press, 2017.