6,420
6,539
6,317
1,139,168
1,765,547
1,088,411
1,133,647
1,869,601
70,400
79,383
392,268
7,564,102
S(C1, C2) = e−|C1−C2|
TABLE II: Number of binaries and functions in our datasets.
(9)
where C1 and C2 are the size of callee function set of
binary function F1 and F2, respectively. The ﬁnal function
pair similarity calculation combines the AST similarity and
the callee function similarity is as follows:
F(F1, F2) = M(T1, T2) × S(C1, C2)
(10)
where T1 is the AST decompiled from binary function F1
and T2 is the AST decompiled from binary function F2. We
conduct a comparative evaluation in § IV-E1 to show the
performance gain of our AST similarity calibration.
IV. EVALUATION
A. Experiment Settings
We utilize IDA Pro 7.3 [7] and its plug-in Hexray
Decompiler to decompile binary code for AST extraction.
Since the Hexray Decompiler currently only supports the
architectures of x86, x64, PowerPC (PPC), and ARM, our
approach cannot handle the binaries which belong to the
MIPS architecture. After the AST extraction, we perform
preprocessing of AST as described in § III-A. Before the
AST encoding by the Tree-LSTM, the nodes in an AST are
embedded into 16-dimensional vectors by the nn.Embedding
of PyTorch [17]. For the encoding of leaf nodes in Formulas
(1)-(6), we assign the state vectors hkl, hkr, ckl, and ckr to
zero vectors. The loss function for model training is BCELoss,
which measures the binary cross entropy between the labels
and the predictions. The AdaGrad optimizer is applied for
gradient computation and weight-matrix updating after losses
are computed. Since the The computation steps of Tree-LSTM
depend on the shape of the AST, therefore, we cannot perform
parallel batch computation, which makes the batch size always
to be 1. The model is trained for 60 epochs. We do not
include the calibration scheme introduced in § III-C during
the model training, so that the Tree-LSTM network effectively
learns semantic differences between ASTs. Our experiments
are performed on a local server equipped with two Intel(R)
Xeon(R) CPUs E5-2620 v4 @ 2.10GHz, each with 16 cores,
128GB of RAM, and 4T of storage. The code of ASTERIA
runs in a Python 3.6 environment. We use gcc v5.4.0
compiler to compile source code in our dataset, and use
the buildroot-2018.11.1 [1] for the Buildroot dataset
(details in § IV-B) construction. We use the tool binwalk [8]
to unpack the ﬁrmware for obtaining the binaries to conduct
further analysis.
B. Datasets
We create three datasets: Buildroot dataset, OpenSSL dataset,
and Firmware dataset. The Buildroot dataset is used for model
training and testing. We obtain the optimal model weights with
the best performance based on the results of model training
and model testing (§ IV-E2). The OpenSSL dataset is used for
the comparative evaluation in § IV-E1. The Firmware dataset
is used to perform the task of practical vulnerability search
with our approach. As shown in Table II, the third column
shows the number of binary ﬁles for different architectures,
and the fourth column shows the number of functions in the
corresponding dataset. Noting that in OpenSSL and Buildroot
datasets,
the symbols such as function names in binaries
are retained during the compilation while these symbols are
usually stripped in the release version of ﬁrmware. We utilize
the function names together with the library names since
function names in different binaries might be duplicated but
having different functionalities to construct function pairs
with ground truth labels for the model training, testing, and
evaluation. Functions that have the same name from the same
binary are called homologous function pairs, otherwise, they
are considered non-homologous function pairs. After the com-
pilation, each function F in the same source code corresponds
to the different versions of binary functions FARM, Fx86,
Fx64, and FP P C. The two binary functions which are homol-
ogous, for example FARM and Fx86, form the homologous
pair (FARM , Fx86). Two ASTs corresponding to the homol-
ogous pair are extracted and form the homologous AST pair
ARM , +1), where the ground truth label ”+1” indicates
(T F
that the two ASTs are from homologous functions. The non-
homologous functions, such as Gx86 and FARM, form a non-
ARM ,−1), where ”−1” indicates
homologous AST pair (T F
that the two ASTs are from non-homologous functions, and F
and G are two different functions. We construct six different
architecture combinations of function pairs including ARM vs.
PPC, ARM vs. x64, PPC vs. x64, x86 vs. ARM, x86 vs. PPC,
and x86 vs. x64. Function names in the Firmware dataset are
deleted, IDA tags them with name sub xxx, where xxx is the
function offset address.
x86, T G
x86, T F
Buildroot Dataset: We build this dataset for model train-
ing and testing with a multi-platform cross-compilation tool
buildroot [1]. We utilize the buildroot to download
the source code of 260 different software which contains
benchmark datasets including busybox, binutils, and
Arch-Comb
x86-ARM
x86-PPC
x86-x64
# of pairs
174,776
182,596
167,436
Arch-Comb
ARM-PPC
ARM-x64
PPC-x64
# of pairs
174,916
157,976
164,916
TABLE III: Number of function pairs for model training and
testing. Arch-Comb means architecture combination.
compile them with the default compilation option for different
architectures. As a result, each software has four different
binary versions corresponding to different architectures. As
shown in Table II, 49,725 binaries in total are compiled by
the buildroot. Then we randomly select functions from
different architecture binaries to construct 1, 022, 616 cross-
architecture function pairs. The details of function pairs of
different architecture combinations are shown in Table III.
Since we remove the AST pairs which contain ASTs with
the node number less than 5, the number of function pairs are
different among different architecture combinations. We divide
the Buildroot dataset into two parts according to the ratio of
8:2 for the training set and testing set.
OpenSSL Dataset: Considering Gemini uses OpenSSL
dataset for the evaluation,
to conduct a fair comparative
experiments, we also build the OpenSSL dataset. We com-
pile the source code of OpenSSL 1.1.0a [16] under four
different architectures with the default compilation settings.
We randomly select function pairs with different architecture
combinations. The ﬁnal dataset consists of 37,541 homologous
AST pairs and 57,537 non-homologous AST pairs.
Firmware Dataset: We download 5,979 ﬁrmware from
the websites and FTP servers of
IoT device vendors.
There are 2,300 ﬁrmwares from NetGear [5], 1,021 from
Schneider [6], and 3,679 from Dlink [4]. After unpacking
ﬁrmware with binwalk [8], 7,090 binary ﬁles are generated.
Note that not all ﬁrmware can be unpacked since binwalk
cannot identify certain ﬁrmware format. As shown in Table II,
binaries mainly come from the ARM and the PPC architec-
tures.
C. Baseline Approaches
There have been several previous works for BCSD study:
discovRE [36], Genius [38], Gemini [62], and Diaphora [3].
Xu et al. have demonstrated that Gemini is both more accurate
and efﬁcient than the other approaches [62]. In our evaluation,
Gemini is chosen as one of the baseline approaches. Consid-
ering that Diaphora [3] also uses the AST as the features for
similarity calculation and is not compared with other works,
we also choose it as a baseline for comparison.
Gemini: Gemini encodes ACFGs (attributed CFGs) into
vectors with a graph embedding neural network. The ACFG is
a graph structure where each node is a vector corresponding to
a basic block. We have obtained Gemini’s source code and its
training dataset. Notice that in [62] authors mentioned it can be
retrained for a speciﬁc task, such as the bug search. To obtain
the best accuracy of Gemini, we ﬁrst use the given training
dataset to train the model to achieve the best performance.
Then we re-train the model with the part of our training
dataset. For the Gemini evaluation, we construct ACFG pairs
from the same function pairs with AST pairs in our OpenSSL
dataset.
Diaphora: We download the Diaphora source code from
github [3]. Diaphora maps nodes in an AST to primes and
calculates the product of all prime numbers. Then it utilizes a
function to calculate the similarity between the prime products.
We extract Diaphora’s core algorithm for AST similarity
calculation for comparison.
D. Evaluation Metric
The Receiver Operating Characteristic (ROC) curve and
Area Under Curve (AUC) are used for measuring the model
performance. The ROC curve [70] illustrates the diagnostic
ability of a model as its discrimination threshold is varied.
For our evaluation, the AUC reﬂects the probability that the
model will correctly identify whether an AST pair is from a
homologous pair or not. In our evaluation, the similarity of
a function pair is calculated as a score of r. Assuming the
threshold is β, if the similarity score r of a function pair is
greater than or equal to β, the function pair is regarded as a
positive result, otherwise a negative result. For a homologous
pair, if its similarity score r is greater than or equal to β, it
is a true positive (TP). If a similarity score of r is less than
β, the calculation result is a false negative (FN). For a non-
homologous pair, if a similarity score r is greater than or equal
to β, it is a false positive (FP). When the similarity score r is
less than β, it is a true negative (TN). After classifying all the
calculation results, two important metrics T P R (true positive
rate) and F P R (false positive rate) under certain threshold β
are calculated as:
T P R =
F P R =
T P
T P + F N
F P
F P + T N
(11)
(12)
The ROC curve can be generated by plotting points whose
coordinates consist of F P Rs and T P Rs with many different
thresholds. After ROC curves being plotted, the area under
the ROC curve, called AUC for short, can be calculated. The
larger the AU C, the better the model’s discrimination ability.
E. Evaluation Results
We conduct comparative experiments to assess the perfor-
mances of three approaches: ASTERIA, GEMINI and DIAPHORA.
To measure the performance gain of the calibration scheme
described in § III-C, we also test the performance of ASTERIA
without calibration (i.e., ASTERIA-WOC), where the calibration
algorithm is not included and the AST similarity is directly
used as the ﬁnal function similarity. We conduct two com-
parative experiments: the mixed cross-architecture experiment
and the pair-wise cross-architecture experiment. In the mixed
cross-architecture experiment, the function pairs are randomly
constructed from any architecture combinations. In particular,
the functions in such function pairs could come from any
platform of x64, x86, PPC, or ARM. The pair-wise cross-
architecture experiments aim to compare the performance in
Fig. 6: The ROC curves for ASTERIA, ASTERIA-WOC, Gemini,
and Diaphora in mixed cross-architecture evaluation
Fig. 8: ROC curves in different embedding sizes from 8 to
128.
Fig. 7: The AUCs for ASTERIA, ASTERIA-WOC, Gemini, and
Diaphora in pair-wise cross-architecture evaluation.
a speciﬁc architecture combination. Therefore, six different
function combinations are constructed: ARM-PPC, ARM-x64,
PPC-x64, x86-ARM, x86-PPC, and x86-x64, respectively. In
each individual combination, function pairs are selected from
two speciﬁc different architectures. For example, in the ARM-
PPC cross-architecture experiment, the functions in the pairs
only come from the ARM or PPC architecture.
1) ROC Comparison: Figure 6 plots the ROC curves of
the different approaches in cross-architecture experiments. The
result shows that ASTERIA outperforms Gemini by around
7.5% and Diaphora by 82.7%, and the similarity detection
accuracy of ASTERIA has been enhanced with the similar-
ity calibration (§ III-C). Note that our approach achieves a
high true positive rate with a low false positive rate, which
brings out high conﬁdence of calculation results in practical
applications. For example, from Figure 6, with a low false-
positive of 5%, our approach achieves a true positive rate of
93.2%, while Gemini has a true positive rate of 55.2%. As
shown in Figure 7, we calculate the AUC values of different
approaches in six different pair-wise experiments. The results
show that ASTERIA outperforms Gemini and Diaphora in each
pair-wise architecture combination by similar margins as in
the mixed cross-architecture experiment. The major difference
between ASTERIA and Gemini is that we take AST as our
function feature, which shows that AST is better than CFG
as a semantically related feature. Compared to Diaphora, the
results show that the application of NLP technology greatly
improves the performance of AST based BCSD.
Fig. 9: Impact of Siamese structures and leaf node calculation.
2) Impact of Model Settings: We illustrate the impact
of four hyperparameters on the model performance: epochs,
siamese structure, embedding size, and leafnode calculation.
The experimental settings are the same as those described in
Section IV-A during evaluation except for the parameters being
measured.
a) Embedding Size: The embedding size is the dimen-
sion of the vector outputted by the embedding layer during
the AST encoding (§ III-B). To show the difference of model
performance with different embedding sizes, we increase the
embedding size from 8 to 128 for model training and testing
as shown in Figure 8. We take the highest AUC value from
the model testing in each embedding size setting. The results
show that the model achieves the highest AUC of 0.985 with
the embedding size of 16, and reaches the lowest AUC of
0.976 with the embedding size of 128. The embedding size of
128 used to represent 43 kinds of nodes in Table I increases
the model complexity and may cause the overﬁtting problem.
Considering the tradeoff between model performance (AUC)
and computational complexity, we choose the embedding size
of 16 in ASTERIA.
b) Siamese Structure: The similarity calculation be-
tween AST encoding vectors in Siamese Network is introduced
in § III-B. There is another common way to calculate the
similarity between two encoding vectors called the regres-
sion method, which utilizes the cosine distance [31]. Dif-
ferent similarity calculation methods correspond to different
Siamese Network internal structures. In contrast, we adopt
a calculation method similar to the binary classiﬁcation as
shown in Equation (8) in ASTERIA. For comparison with the
0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateAsteria (area = 0.985)Asteria-WOC (area = 0.969)Gemini (area = 0.917)Diaphora (area = 0.539)arm-ppcarm-x64ppc-x64x86-armx86-ppcx86-x64Architecture Combination0.00.20.40.60.81.0AUC ValueAsteriaAsteria-WOCGeminiDiaphora0.00.20.40.60.81.0False Positive Rate0.70.80.91.0True Positive RateDim-8, AUC=0.982Dim-16, AUC=0.985Dim-32, AUC=0.983Dim-64, AUC=0.980Dim-128, AUC=0.9760.00.20.40.60.81.0False Positive Rate0.50.60.70.80.91.0True Positive RateRegression. AUC=0.944Classification & Leaf-0 AUC=0.981Leaf-1. AUC=0.973(a) Cumulative distribution of AST size for
25,569 ASTs.