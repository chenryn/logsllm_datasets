title:NetStore: An Efficient Storage Infrastructure for Network Forensics
and Monitoring
author:Paul Giura and
Nasir D. Memon
NetStore: An Eﬃcient Storage Infrastructure for
Network Forensics and Monitoring
Paul Giura and Nasir Memon
Polytechnic Intitute of NYU, Six MetroTech Center, Brooklyn, NY
Abstract. With the increasing sophistication of attacks, there is a need
for network security monitoring systems that store and examine very
large amounts of historical network ﬂow data. An eﬃcient storage in-
frastructure should provide both high insertion rates and fast data ac-
cess. Traditional row-oriented Relational Database Management Systems
(RDBMS) provide satisfactory query performance for network ﬂow data
collected only over a period of several hours. In many cases, such as the
detection of sophisticated coordinated attacks, it is crucial to query days,
weeks or even months worth of disk resident historical data rapidly. For
such monitoring and forensics queries, row oriented databases become
I/O bound due to long disk access times. Furthermore, their data inser-
tion rate is proportional to the number of indexes used, and query pro-
cessing time is increased when it is necessary to load unused attributes
along with the used ones. To overcome these problems we propose a new
column oriented storage infrastructure for network ﬂow records, called
NetStore. NetStore is aware of network data semantics and access pat-
terns, and beneﬁts from the simple column oriented layout without the
need to meet general purpose RDBMS requirements. The prototype im-
plementation of NetStore can potentially achieve more than ten times
query speedup and ninety times less storage size compared to traditional
row-stores, while it performs better than existing open source column-
stores for network ﬂow data.
1 Introduction
Traditionally intrusion detection systems were designed to detect and ﬂag mali-
cious or suspicious activity in real time. However, such systems are increasingly
providing the ability to identify the root cause of a security breach. This may
involve checking a suspected host’s past network activity, looking up any services
run by a host, protocols used, the connection records to other hosts that may or
may not be compromised, etc. This requires ﬂexible and fast access to network
ﬂow historical data. In this paper we present the design, implementation details
and the evaluation of a column-oriented storage infrastructure called NetStore,
designed to store and analyze very large amounts of network ﬂow data. Through-
out this paper we refer to a ﬂow as an unidirectional data stream between two
endpoints, to a ﬂow record as a quantitative description of a ﬂow, and to a ﬂow
ID as the key that uniquely identiﬁes a ﬂow. In our research the ﬂow ID is
S. Jha, R. Sommer, and C. Kreibich (Eds.): RAID 2010, LNCS 6307, pp. 277–296, 2010.
c(cid:2) Springer-Verlag Berlin Heidelberg 2010
278
P. Giura and N. Memon
Fig. 1. Flow traﬃc distribution for one day and one month. In a typical day the busiest
time interval is 1PM - 2PM with 4,381,876 ﬂows, and the slowest time interval is 5AM
- 6AM with 978,888 ﬂows. For a typical month we noticed the slow down in week-ends
and the peek traﬃc in weekdays. Days marked with * correspond to a break week.
composed of ﬁve attributes: source IP, source port, destination IP, destination
port and protocol. We assume that each ﬂow record has associated a start time
and an end time representing the time interval when the ﬂow was active in the
network.
Challenges. Network ﬂow data can grow very large in the number of records
and storage footprint. Figure 1 shows network ﬂow distribution of traﬃc cap-
tured from edge routers in a moderate sized campus network. This network with
about 3,000 hosts, commonly reaches up to 1,300 ﬂows/second, an average 53
million ﬂows daily and roughly 1.7 billion ﬂows in a month. We consider records
with the average size of 200 Bytes. Besides CISCO NetFlow data [18] there may
be other speciﬁc information that a sensor can capture from the network such as
the IP, transport and application headers information. Hence, in this example,
the storage requirement is roughly 10 GB of data per day which adds up to
at least 310 GB per month. When working with large amounts of disk resident
data, the main challenge is no longer to ensure the necessary storage space, but
to minimize the time it takes to process and access the data. An eﬃcient stor-
age and querying infrastructure for network records has to cope with two main
technical challenges: keep the insertion rate high, and provide fast access to the
desired ﬂow records. When using a traditional row-oriented Relational Database
Management Systems (RDBMS), the relevant ﬂow attributes are inserted as a
row into a table as they are captured from the network, and are indexed using
various techniques [6]. On the one hand, such a system has to establish a trade
oﬀ between the insertion rate desired and the storage and processing overhead
employed by the use of auxiliary indexing data structures. On the other hand,
enabling indexing for more attributes ultimately improves query performance
but also increases the storage requirements and decreases insertion rates. At
query time, all the columns of the table have to be loaded in memory even if
only a subset of the attributes are relevant for the query, adding a signiﬁcant
I/O penalty for the overall query processing time by loading unused columns.
NetStore: An Eﬃcient Storage Infrastructure
279
When querying disk resident data, an important problem to overcome is the I/O
bottleneck caused by large disk to memory data transfers. One potential solu-
tion would be to load only data that is relevant to the query. For example, to
answer the query “What is the list of all IPs that contacted IP X between dates
d1 and d2?”, the system should load only the source and destination IPs as well
as the timestamps of the ﬂows that fall between dates d1 and d2. The I/O time
can also be decreased if the accessed data is compressed since less data traverses
the disk-memory boundary. Further, the overall query response time can be im-
proved if data is processed in compressed format by saving decompression time.
Finally, since the system has to insert records at line speed, all the preprocessing
algorithms used should add negligible overhead while writing to disk. The above
requirements can be met quite well by utilizing a column oriented database as
described below.
Column Store. The basic idea of column orientation is to store the data by
columns rather than by rows, where each column holds data for a single attribute
of the ﬂow and is stored sequentially on disk. Such a strategy makes the system
I/O eﬃcient for read queries since only the required attributes related to a query
can be read from the disk. The performance beneﬁts of column partitioning
were previously analyzed in [9, 2], and some of the ideas were conﬁrmed by the
results in the databases academic research community [16, 1, 21] as well as in
industry [19, 11, 10, 3]. However, most of commercial and open-source column
stores were conceived to follow general purpose RDBMSs requirements, and do
not fully use the semantics of the data carried and do not take advantage of
the speciﬁc types and data access patterns of network forensic and monitoring
queries. In this paper we present the design, implementation details and the
evaluation of NetStore, a column-oriented storage infrastructure for network
records that, unlike the other systems, is intended to provide good performance
for network records ﬂow data.
Contribution. The key contributions in this paper include the following:
– Simple and eﬃcient column oriented design of NetStore, a network ﬂow
historical storage system that enables quick access to large amounts of data
for monitoring and forensic analysis.
– Eﬃcient compression methods and selection strategies to facilitate the best
compression for network ﬂow data, that permit accessing and querying data
in compressed format.
– Implementation and deployment of NetStore using commodity hardware and
open source software as well as analysis and comparison with other open
source storage systems used currently in practice.
The rest of this paper is organized as follows: we present related work in Sec-
tion 2, our system architecture and the details of each component in Section 3.
Experimental results and evaluation are presented in Section 4 and we conclude
in Section 5.
280
P. Giura and N. Memon
2 Related Work
The problem of discovering network security incidents has received signiﬁcant
attention over the past years. Most of the work done has focused on near-real
time security event detection, by improving existing security mechanisms that
monitor traﬃc at a network perimeter and block known attacks, detect suspicious
network behavior such as network scans, or malicious binary transfers [12, 14].
Other systems such as Tribeca [17] and Gigascope [4], use stream databases
and process network data as it arrives but do not store the date for retroactive
analysis. There has been some work done to store network ﬂow records using a
traditional RDBMS such as PostgreSQL [6]. Using this approach, when a NIDS
triggers an alarm, the database system builds indexes and materialized views for
the attributes that are the subject of the alarm, and could potentially be used by
forensics queries in the investigation of the alarm. The system works reasonably
well for small networks and is able to help forensic analysis for events that
happened over the last few hours. However, queries for traﬃc spanning more
than a few hours become I/O bound and the auxiliary data used to speed up
the queries slows down the record insertion process. Therefore, such a solution is
not feasible for medium to large networks and not even for small networks in the
future, if we consider the accelerated growth of internet traﬃc. Additionally, a
time window of several hours is not a realistic assumption when trying to detect
the behavior of a complex botnet engaged in stealthy malicious activity over
prolonged periods of time.
In the database community, many researchers have proposed the physical
organization of database storage by columns in order to cope with poor read
query performance of traditional row-based RDBMS [16,21,11,15,3]. As shown
in [16, 2, 9, 8], a column store provides many times better performance than a
row store for read intensive workloads. In [21] the focus is on optimizing the
cache-RAM access time by decompressing data in the cache rather than in the
RAM. This system assumes the working columns are RAM resident, and shows
a performance penalty if data has to be read from the disk and processed in the
same run. The solution in [16] relies on processing parallelism by partitioning
data into sets of columns, called projections, indexed and sorted together, inde-
pendent of other projections. This layout has the beneﬁt of rapid loading of the
attributes belonging to the same projection and referred to by the same query
without the use of auxiliary data structure for tuple reconstruction. However,
when attributes from diﬀerent projections are accessed, the tuple reconstruction
process adds signiﬁcant overhead to the data access pattern. The system pre-
sented in [15] emphasizes the use of an auxiliary metadata layer on top of the
column partitioning that is shown to be an eﬃcient alternative to the indexing
approach. However, the metadata overhead is sizable and the design does not
take into account the correlation between various attributes.
Finally, in [9] authors present several factors that should be considered when
one has to decide to use a column store versus a row store for a read intensive
workload. The relative large number of network ﬂow attributes and the workloads
NetStore: An Eﬃcient Storage Infrastructure
281
with the predominant set of queries with large selectivity and few predicates favor
the use of a column store system for historical network ﬂow records storage.
NetStore is a column oriented storage infrastructure that shares some of the
features with the other systems, and is designed to provide the best perfor-
mance for large amounts of disk resident network ﬂow records. It avoids tuple
reconstruction overhead by keeping at all times the same order of elements in
all columns. It provides fast data insertion and quick querying by dynamically
choosing the most suitable compression method available and using a simple and
eﬃcient design with a negligible meta data layer overhead.
3 Architecture
In this section we describe the architecture and the key components of NetStore.
We ﬁrst present the characteristics of network data and query types that guide
our design. We then describe the technical design details: how the data is par-
titioned into columns, how columns are partitioned into segments, what are the
compression methods used and how a compression method is selected for each
segment. We ﬁnally present the metadata associated with each segment, the in-
dex nodes, and the internal IPs inverted index structure, as well as the basic set
of operators.
3.1 Network Flow Data
Network ﬂow records and the queries made on them show some special char-
acteristics compared to other time sequential data, and we tried to apply this
knowledge as early as possible in the design of the system. First, ﬂow attributes
tend to exhibit temporal clustering, that is, the range of values is small within
short time intervals. Second, the attributes of the ﬂows with the same source IP
and destination IP tend to have the same values (e.g. port numbers, protocols,
packets sizes etc.). Third, columns of some attributes can be eﬃciently encoded
when partitioned into time based segments that are encoded independently. Fi-
nally, most attributes that are of interest for monitoring and forensics can be
encoded using basic integer data types.
The records insertion operation is represented by bulk loads of time sequen-
tial data that will not be updated after writing. Having the attributes stored
in the same order across the columns makes the join operation become trivial
when attributes from more than one column are used together. Network data
analysis does not require fast random access on all the attributes. Most of the
monitoring queries need fast sequential access to large number of records and
the ability to aggregate and summarize the data over a time window. Forensic
queries access speciﬁc predictable attributes but collected over longer periods of
time. To observe their speciﬁc characteristics we ﬁrst compiled a comprehensive
list of forensic and monitoring queries used in practice in various scenarios [5].
Based on the data access pattern, we identiﬁed ﬁve types among the initial list.
Spot queries (S) that target a single key (usually an IP address or port number)
282
P. Giura and N. Memon
and return a list with the values associated with that key. Range queries (R)
that return a list with results for multiple keys (usually attributes corresponding
to the IPs of a subnet). Aggregation queries (A) that aggregate the data for the
entire network and return the result of the aggregation (e.g. traﬃc sent out for
network). Spot Aggregation queries (SA) that aggregate the values found for one
key in a single value. Range Aggregation queries (RA) that aggregate data for
multiple keys into a single value. Examples of these types of queries expressed
in plain words:
(S) “What applications are observed on host X between dates d1 and d2?”
(R) “What is the list of destination IPs that have source IPs in a subnet between