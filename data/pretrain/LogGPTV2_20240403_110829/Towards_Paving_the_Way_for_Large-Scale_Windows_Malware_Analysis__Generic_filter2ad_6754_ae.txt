T, 71, 101, 0.25
E, 24, 38, 0.15
T, 68, 91, 0.23
T, 83, 126, 0.15
T, 79, 94, 0.15
T, 141, 181, 0.13
T, T, T, 0.25
T, E, E, 0.75
T, E, E, 0.85
Table 3: Comparative evaluation with ground truth dataset. Evasion types: 1) Anti-Debugging; 2) Anti-VM (Virtual Machine
& System Emulator); 3) Anti-DBI (Dynamic Binary Instrumentation); 4) Anti-Hooking. The last two columns’s order is (CoDi-
sasm, PINdemonium, Arancino, BinUnpack).
VirusTotal Result
Packed
Performance (s)
(CoDisasm, PinDemonium, Arancino, BinUnpack)
575
code is revealed during any given unpacking time window. Our
countermeasure is to dump a continuous series of process memory
and later reassemble them as a single consistent code image. We
enable Themida’s “Encode Macro” option to protect major functions
of hupigon.eyf. This means we can only restore one function’s
binary code each time. As shown in the last row, BinUnpack is
able to extract the original code successfully and greatly increase
malware detection rate. However, in practice, such partial code
revealing packer is rare because of no source code available, the
unreliability, and high runtime overhead [7].
8.1.4 Performance Comparison. We also compare BinUnpack’s
performance with other three representative generic unpacking
tools: CoDisasm [9], PinDemonium [55], and Arancino [76]. All
of them rely on Pin [52] to monitor “written-then-executed” in-
structions [55, 76] or memory pages [9]. We borrow “CoDisasm”,
“PinDemonium”, and “Arancino” to represent the generic unpackers
developed by the related work [9, 55, 76], respectively. Although
the purposes of these three papers look different (e.g., binary disas-
sembly [9] or defeating anti-instrumentation evasions [76]), generic
unpacking is either an indispensable preprocess or an appealing
application. BinUnpack’s two major advantages are avoiding tra-
cing “written-then-executed” layers and resilient to anti-analysis
tricks. All of these three unpackers are very representative in these
two respects. Like BinUnpack, PinDemonium also relies on the
state-of-the-art process dump tool, Scylla [1], to reconstruct the
payload IAT. Arancino is an extension of PinDemonium17, and it is
the latest work that is able to defeat anti-DBI-equipped packers.
In our evaluation, we set the threshold of runtime execution
as 1, 800 seconds18. When an unpacking tool exceeds this thres-
hold, we will stop the execution and mark the performance data
as “Timeout”. Besides, we also find many cases that the unpacking
tools raise exceptions and then exit, and therefore we mark them
as “Exception”. The last column of Table 3 shows that BinUnpack
succeeds in all cases, and the running time ranges from 0.09 to 0.85
seconds with an average value 0.20. The worst case comes from
unpacking partial code revealing packer. In contrast, the overhead
of other three tools is much larger than BinUnpack by one ∼ three
orders of magnitude. In addition, all of them fail in many cases with
either “Timeout” or “Exception”. Especially, they fail in most packer
combination samples and Themida versions. We attribute their fai-
lures to the lack of anti-analysis resistance. For example, PESpin and
ACProtect packers can fingerprint Pin environment [40] and crash
the execution of CoDisasm and PinDemonium. Arancino [76] is
an extension of PinDemonium [55] to defeat anti-instrumentation-
equipped packers at the cost of much more overhead.
17They are developed by the same team.
18CoDisasm provides a server to generate traces and memory dumps. The server’s
“Timeout” threshold is only 120 seconds.
Session 3A: Binary Analysis CCS’18, October 15-19, 2018, Toronto, ON, Canada405For some other representative generic unpacking tools, as many
of them are either unavailable or obsolete, we report the overhead
mentioned in their papers [28, 36, 56, 83, 86]. Note that all of them
only evaluated limited packers and no one tested packer combina-
tions or the partial code revealing packer. Whether they can still
defeat current sophisticated packers is unclear. The overhead of
PolyUnpack [83] is about 150 seconds on average; The average
unpacking time of OmniUnpack [56] on 12 packers is 5.3 seconds;
Renovo [36] incurs at least 8X runtime slowdown when unpacking
15 packers; Eureka [86] tested 15 packers and could unpack “more
than 90 binaries per hour”.
8.1.5 Top Wanted Malware, Packed Benign Programs, and NSIS
Packer. In addition to hupigon.eyf, we also evaluate top wanted
malware, packed web browsers, and custom packers adopted by
ransomware. The top wanted malware samples are from the secu-
rity vendor Check Point’s top10 list from May 2017 [95] to March
2018 [96], and we manage to collect their binary code of no-packer
versions. The second column of Table 4 shows the number of va-
rious packers applied to each sample. We find that not all of the
packers list in the Table 3 are compatible with top wanted malware
and benign browsers. The Column 3∼ 5 list the average VirusTotal
scanning results to no-packer, packed, and unpacked versions, re-
spectively. Column 6 shows the number of successful unpacking.
We consider an unpacking as success if it can locate OEP and extract
the original code correctly. The last column reports the average
running time for successfully unpacking cases. For the experiments
of packed top wanted malware, BinUnpack’s results are similar to
Table 3 with two ∼ three orders of magnitude performance boost
and 100% success rate.
In the packed benign program experiments with three popu-
lar web browsers, we find that more that 20 anti-virus scanners
generate false alarms, and all of the other three unpacking tools
fail to extract the original code. BinUnpack’s outputs reduce the
false positives of anti-virus scanners to zero. In the last set of ex-
periments with custom packers adopted by ransomware, the third
column data is not available because we do not have their no-packer
versions. We find all of these ransomware samples customize NSIS
installer as packers. NSIS (Nullsoft Scriptable Install System) is a
script-driven installer to create a install package. As a lot of legiti-
mate softwares also use NSIS as their installers, anti-virus scanners
would produce large false positives if they take NSIS as malware
signature. The unique feature of the NSIS packer protected mal-
ware is that it never places the malicious executables on the file
system (a.k.a Fileless Malware [106]), which can bypass many mal-
ware static analysis approaches. Appendix Figure 11 shows the
typical attacking procedure of NSIS packed ransomware. To further
complicate the generic unpacking, Cerber customizes NSIS pac-
kers by adding evasion techniques such as process hollowing and
crash hooking module. Nonetheless, we find that the NSIS packed
ransomware still presents the behavior of “rebuilt-then-called”, so
BinUnpack is able to extract the original payload efficiently.
8.2 Impact on Benign Program Execution
We also study the overhead BinUnpack introduces when working
with benign programs (no-packer version). We compare the execu-
tion time of benign programs with BinUnpack disabled and enabled.
#Packers
VirusTotal (Avg.)
#Success
Peformance (Avg.)
No-packer
Packed
Unpacked
Table 4: Comparative evaluation summary with more sam-
ples. The data in the last three columns is represented as
(CoDisasm, PinDemonium, Arancino, BinUnpack).
Sample
Top wanted
Locky
Conficker
Zeus
Andromeda
Necurs
GlobeImposter
Pykspa
Hancitor
Nivdort
WannaCry
Kelihos
Jaff
Cryptowall
Sality
Fareit
Packed browsers
IE
FireFox
Chrome
Custom packers
CryptoLocker
CTB-Locker
Teerac
Crysis
Cerber
1 The meaning of “T”(Timeout) and “E”(Exception) are as the same as Table 3, and “–” means N/A.
101, 488, 512, 0.21
105, 532, 568, 0.28
92, 410, 451, 0.24
90, 380, 446, 0.21
76, 364, 395, 0.17
93, 415, 450, 0.20
93, 423, 467, 0.21
79, 358, 394, 0.18
92, 405, 462, 0.20
120, 523, 576, 0.26
91, 401, 452, 0.22
92, 422, 473, 0.20
114, 434, 482, 0.25
95, 461, 514, 0.21
94, 412, 452, 0.20
5, 18, 21, 32
6, 19, 23, 35
7, 21, 24, 36
6, 18, 22, 33
6, 18, 21, 31
7, 19, 23, 34
5, 18, 21, 35
7, 20, 24, 34
8, 22, 25, 37
6, 17, 20, 32
7, 22, 26, 38
6, 18, 21, 33
5, 17, 19, 31
7, 20, 24, 35
9, 23, 27, 36
3, 11, 13, 35
3, 10, 13, 45
3, 11, 15, 37
3, 11, 14, 31
3, 10, 13, 33
3, 11, 14, 45
2, 10, 12, 37
3, 12, 15, 42
3, 11, 14, 26
4, 10, 13, 54
3, 11, 14, 28
3, 10, 12, 40
3, 10, 12, 41
3, 11, 13, 40
4, 13, 16, 43
T, E, E, 0.23
T, E, E, 0.22
T, T, T, 0.19
T, T, T, 0.21
E, E, E, 0.25
–, –, –, 44
–, –, –, 41
–, –, –, 32
–, –, –, 29
–, –, –, 44
15
25
18
11
13
24
19
22
11
30
12
19
22
19
17
23
20
22
34
31
21
22
36
0, 0, 0, 1
0, 0, 0, 1
0, 0, 0, 1
0, 0, 0, 1
0, 0, 0, 1
E, T, T, 0.29
E, T, T, 0.31
E, E, T, 0.33
38
47
40
35
37
48
42
46
30
56
32
43
44
43
46
0
0
0
–
–
–
–
–
32
35
36
33
31
34
35
34
37
32
38
33
31
35
36
9
5
8
1
1
1
1
1
–, –, –, 0
–, –, –, 0
–, –, –, 0
0, 0, 0, 9
0, 0, 0, 5
0, 0, 0, 8
The test set includes common Windows applications (e.g., tasklist,
winrar, and WinPcap) and prevalent web browsers (e.g., IE, Firefox,
and Chrome). To evaluate the browsers, we cache the top 10 bench-
mark sites ranked by Alexa19. We insert the JavaScript “getTime()”
routine to the start and the end of the page, and then compute
the delta between these two time. The delta shows us how long
it takes to load a page. As shown in Appendix Table 6, the additi-
onal overhead caused by BinUnpack mainly comes from the IAT
comparison. If the result of IAT comparison is true, BinUnpack will
not perform both OEP search and process dump. The relative slow-
down caused by BinUnpack ranges from 0.01% to 1.48%. The worst
case comes from Chrome browser, which heavily uses API calls