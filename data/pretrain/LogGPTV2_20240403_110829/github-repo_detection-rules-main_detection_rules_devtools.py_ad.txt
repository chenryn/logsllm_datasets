                   date_range=date_range, count=count, max_results=max_results, verbose=verbose,
                   elasticsearch_client=elasticsearch_client)
    else:
        client_error('Rule is not a query rule!')
@test_group.command('rule-survey')
@click.argument('query', required=False)
@click.option('--date-range', '-d', type=(str, str), default=('now-7d', 'now'), help='Date range to scope search')
@click.option('--dump-file', type=click.Path(dir_okay=False),
              default=get_path('surveys', f'{time.strftime("%Y%m%dT%H%M%SL")}.json'),
              help='Save details of results (capped at 1000 results/rule)')
@click.option('--hide-zero-counts', '-z', is_flag=True, help='Exclude rules with zero hits from printing')
@click.option('--hide-errors', '-e', is_flag=True, help='Exclude rules with errors from printing')
@click.pass_context
@add_client('elasticsearch', 'kibana', add_to_ctx=True)
def rule_survey(ctx: click.Context, query, date_range, dump_file, hide_zero_counts, hide_errors,
                elasticsearch_client: Elasticsearch = None, kibana_client: Kibana = None):
    """Survey rule counts."""
    from kibana.resources import Signal
    from .main import search_rules
    # from .eswrap import parse_unique_field_results
    survey_results = []
    start_time, end_time = date_range
    if query:
        rules = RuleCollection()
        paths = [Path(r['file']) for r in ctx.invoke(search_rules, query=query, verbose=False)]
        rules.load_files(paths)
    else:
        rules = RuleCollection.default().filter(production_filter)
    click.echo(f'Running survey against {len(rules)} rules')
    click.echo(f'Saving detailed dump to: {dump_file}')
    collector = CollectEvents(elasticsearch_client)
    details = collector.search_from_rule(rules, start_time=start_time, end_time=end_time)
    counts = collector.count_from_rule(rules, start_time=start_time, end_time=end_time)
    # add alerts
    with kibana_client:
        range_dsl = {'query': {'bool': {'filter': []}}}
        add_range_to_dsl(range_dsl['query']['bool']['filter'], start_time, end_time)
        alerts = {a['_source']['signal']['rule']['rule_id']: a['_source']
                  for a in Signal.search(range_dsl, size=10000)['hits']['hits']}
    # for alert in alerts:
    #     rule_id = alert['signal']['rule']['rule_id']
    #     rule = rules.id_map[rule_id]
    #     unique_results = parse_unique_field_results(rule.contents.data.type, rule.contents.data.unique_fields, alert)
    for rule_id, count in counts.items():
        alert_count = len(alerts.get(rule_id, []))
        if alert_count > 0:
            count['alert_count'] = alert_count
        details[rule_id].update(count)
        search_count = count['search_count']
        if not alert_count and (hide_zero_counts and search_count == 0) or (hide_errors and search_count == -1):
            continue
        survey_results.append(count)
    fields = ['rule_id', 'name', 'search_count', 'alert_count']
    table = Table.from_list(fields, survey_results)
    if len(survey_results) > 200:
        click.echo_via_pager(table)
    else:
        click.echo(table)
    os.makedirs(get_path('surveys'), exist_ok=True)
    with open(dump_file, 'w') as f:
        json.dump(details, f, indent=2, sort_keys=True)
    return survey_results
@dev_group.group('utils')
def utils_group():
    """Commands for dev utility methods."""
@utils_group.command('get-branches')
@click.option('--outfile', '-o', type=Path, default=get_etc_path("target-branches.yml"), help='File to save output to')
def get_branches(outfile: Path):
    branch_list = get_stack_versions(drop_patch=True)
    target_branches = json.dumps(branch_list[:-1]) + "\n"
    outfile.write_text(target_branches)
@dev_group.group('integrations')
def integrations_group():
    """Commands for dev integrations methods."""
@integrations_group.command('build-manifests')
@click.option('--overwrite', '-o', is_flag=True, help="Overwrite the existing integrations-manifest.json.gz file")
@click.option("--integration", "-i", type=str, help="Adds an integration tag to the manifest file")
def build_integration_manifests(overwrite: bool, integration: str):
    """Builds consolidated integrations manifests file."""
    click.echo("loading rules to determine all integration tags")
    def flatten(tag_list: List[str]) -> List[str]:
        return list(set([tag for tags in tag_list for tag in (flatten(tags) if isinstance(tags, list) else [tags])]))
    if integration:
        build_integrations_manifest(overwrite=False, integration=integration)
    else:
        rules = RuleCollection.default()
        integration_tags = [r.contents.metadata.integration for r in rules if r.contents.metadata.integration]
        unique_integration_tags = flatten(integration_tags)
        click.echo(f"integration tags identified: {unique_integration_tags}")
        build_integrations_manifest(overwrite, rule_integrations=unique_integration_tags)
@integrations_group.command('build-schemas')
@click.option('--overwrite', '-o', is_flag=True, help="Overwrite the entire integrations-schema.json.gz file")
def build_integration_schemas(overwrite: bool):
    """Builds consolidated integrations schemas file."""
    click.echo("Building integration schemas...")
    start_time = time.perf_counter()
    build_integrations_schemas(overwrite)
    end_time = time.perf_counter()
    click.echo(f"Time taken to generate schemas: {(end_time - start_time)/60:.2f} minutes")
@integrations_group.command('show-latest-compatible')
@click.option('--package', '-p', help='Name of package')
@click.option('--stack_version', '-s', required=True, help='Rule stack version')
def show_latest_compatible_version(package: str, stack_version: str) -> None:
    """Prints the latest integration compatible version for specified package based on stack version supplied."""
    packages_manifest = None
    try:
        packages_manifest = load_integrations_manifests()
    except Exception as e:
        click.echo(f"Error loading integrations manifests: {str(e)}")
        return
    try:
        version = find_latest_compatible_version(package, "",
                                                 Version.parse(stack_version, optional_minor_and_patch=True),
                                                 packages_manifest)
        click.echo(f"Compatible integration {version=}")
    except Exception as e:
        click.echo(f"Error finding compatible version: {str(e)}")
        return
@dev_group.group('schemas')
def schemas_group():
    """Commands for dev schema methods."""
@schemas_group.command("update-rule-data")
def update_rule_data_schemas():
    classes = [BaseRuleData] + list(typing.get_args(AnyRuleData))
    for cls in classes:
        cls.save_schema()
@schemas_group.command("generate")
@click.option("--token", required=True, prompt=get_github_token() is None, default=get_github_token(),
              help="GitHub token to use for the PR", hide_input=True)
@click.option("--schema", "-s", required=True, type=click.Choice(["endgame", "ecs", "beats", "endpoint"]),
              help="Schema to generate")
@click.option("--schema-version", "-sv", help="Tagged version from TBD. e.g., 1.9.0")
@click.option("--endpoint-target", "-t", type=str, default="endpoint", help="Target endpoint schema")
@click.option("--overwrite", is_flag=True, help="Overwrite if versions exist")
def generate_schema(token: str, schema: str, schema_version: str, endpoint_target: str, overwrite: bool):
    """Download schemas and generate flattend schema."""
    github = GithubClient(token)
    client = github.authenticated_client
    if schema_version and not Version.parse(schema_version):
        raise click.BadParameter(f"Invalid schema version: {schema_version}")
    click.echo(f"Generating {schema} schema")
    if schema == "endgame":
        if not schema_version:
            raise click.BadParameter("Schema version required")
        schema_manager = EndgameSchemaManager(client, schema_version)
        schema_manager.save_schemas(overwrite=overwrite)
    # ecs, beats and endpoint schemas are refreshed during release
    # these schemas do not require a schema version
    if schema == "ecs":
        download_schemas(refresh_all=True)
    if schema == "beats":
        if not schema_version:
            download_latest_beats_schema()
            refresh_main_schema()
        else:
            download_beats_schema(schema_version)
    # endpoint package custom schemas can be downloaded
    # this download requires a specific schema target
    if schema == "endpoint":
        repo = client.get_repo("elastic/endpoint-package")
        contents = repo.get_contents("custom_schemas")
        optional_endpoint_targets = [
            Path(f.path).name.replace("custom_", "").replace(".yml", "")
            for f in contents if f.name.endswith(".yml") or Path(f.path).name == endpoint_target
        ]
        if not endpoint_target:
            raise click.BadParameter("Endpoint target required")
        if endpoint_target not in optional_endpoint_targets:
            raise click.BadParameter(f"""Invalid endpoint schema target: {endpoint_target}
                                      \n Schema Options: {optional_endpoint_targets}""")
        download_endpoint_schemas(endpoint_target)
    click.echo(f"Done generating {schema} schema")
@dev_group.group('attack')
def attack_group():
    """Commands for managing Mitre ATT&CK data and mappings."""
@attack_group.command('refresh-data')
def refresh_attack_data() -> dict:
    """Refresh the ATT&CK data file."""
    data, _ = attack.refresh_attack_data()
    return data
@attack_group.command('refresh-redirect-mappings')
def refresh_threat_mappings():
    """Refresh the ATT&CK redirect file and update all rule threat mappings."""
    # refresh the attack_technique_redirects
    click.echo('refreshing data in attack_technique_redirects.json')
    attack.refresh_redirected_techniques_map()
@attack_group.command('update-rules')
def update_attack_in_rules() -> List[Optional[TOMLRule]]:
    """Update threat mappings attack data in all rules."""
    new_rules = []
    redirected_techniques = attack.load_techniques_redirect()
    today = time.strftime('%Y/%m/%d')
    rules = RuleCollection.default()
    for rule in rules.rules:
        needs_update = False
        valid_threat: List[ThreatMapping] = []
        threat_pending_update = {}
        threat = rule.contents.data.threat or []
        for entry in threat:
            tactic = entry.tactic.name
            technique_ids = []
            technique_names = []
            for technique in entry.technique or []:
                technique_ids.append(technique.id)
                technique_names.append(technique.name)
                technique_ids.extend([st.id for st in technique.subtechnique or []])
                technique_names.extend([st.name for st in technique.subtechnique or []])
            # check redirected techniques by ID
            # redirected techniques are technique IDs that have changed but represent the same technique
            if any([tid for tid in technique_ids if tid in redirected_techniques]):
                needs_update = True
                threat_pending_update[tactic] = technique_ids
                click.echo(f"'{rule.contents.name}' requires update - technique ID change")
            # check for name change
            # happens if technique ID is the same but name changes
            expected_technique_names = [attack.technique_lookup[str(tid)]["name"] for tid in technique_ids]
            if any([tname for tname in technique_names if tname not in expected_technique_names]):
                needs_update = True
                threat_pending_update[tactic] = technique_ids
                click.echo(f"'{rule.contents.name}' requires update - technique name change")
            else:
                valid_threat.append(entry)
        if needs_update:
            for tactic, techniques in threat_pending_update.items():
                try:
                    updated_threat = attack.build_threat_map_entry(tactic, *techniques)
                except ValueError as err:
                    raise ValueError(f'{rule.id} - {rule.name}: {err}')
                tm = ThreatMapping.from_dict(updated_threat)
                valid_threat.append(tm)
            new_meta = dataclasses.replace(rule.contents.metadata, updated_date=today)
            new_data = dataclasses.replace(rule.contents.data, threat=valid_threat)
            new_contents = dataclasses.replace(rule.contents, data=new_data, metadata=new_meta)
            new_rule = TOMLRule(contents=new_contents, path=rule.path)
            new_rule.save_toml()
            new_rules.append(new_rule)
    if new_rules:
        click.echo(f'\nFinished - {len(new_rules)} rules updated!')
    else:
        click.echo('No rule changes needed')
    return new_rules
@dev_group.group('transforms')
def transforms_group():
    """Commands for managing TOML [transform]."""
def guide_plugin_convert_(contents: Optional[str] = None, default: Optional[str] = ''
                          ) -> Optional[Dict[str, Dict[str, list]]]:
    """Convert investigation guide plugin format to toml"""
    contents = contents or click.prompt('Enter plugin contents', default=default)
    if not contents:
        return
    parsed = re.match(r'!{(?P\w+)(?P{.+})}', contents.strip())
    try:
        plugin = parsed.group('plugin')
        data = parsed.group('data')
    except AttributeError as e:
        raise client_error('Unrecognized pattern', exc=e)
    loaded = {'transform': {plugin: [json.loads(data)]}}
    click.echo(pytoml.dumps(loaded))
    return loaded
@transforms_group.command('guide-plugin-convert')
def guide_plugin_convert(contents: Optional[str] = None, default: Optional[str] = ''
                         ) -> Optional[Dict[str, Dict[str, list]]]:
    """Convert investigation guide plugin format to toml."""
    return guide_plugin_convert_(contents=contents, default=default)
@transforms_group.command('guide-plugin-to-rule')
@click.argument('rule-path', type=Path)
@click.pass_context
def guide_plugin_to_rule(ctx: click.Context, rule_path: Path, save: bool = True) -> TOMLRule:
    """Convert investigation guide plugin format to toml and save to rule."""
    rc = RuleCollection()
    rule = rc.load_file(rule_path)
    transforms = defaultdict(list)
    existing_transform = rule.contents.transform
    transforms.update(existing_transform.to_dict() if existing_transform is not None else {})
    click.secho('(blank line to continue)', fg='yellow')
    while True:
        loaded = ctx.invoke(guide_plugin_convert)
        if not loaded:
            break
        data = loaded['transform']
        for plugin, entries in data.items():
            transforms[plugin].extend(entries)
    transform = RuleTransform.from_dict(transforms)
    new_contents = TOMLRuleContents(data=rule.contents.data, metadata=rule.contents.metadata, transform=transform)
    updated_rule = TOMLRule(contents=new_contents, path=rule.path)
    if save:
        updated_rule.save_toml()
    return updated_rule