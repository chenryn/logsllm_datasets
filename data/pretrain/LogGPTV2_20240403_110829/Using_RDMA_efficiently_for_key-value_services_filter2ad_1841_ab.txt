post involves no DMA operations, reducing latency and increasing
throughput for small payloads.
When the RNIC completes the network steps associated with the
verb, it pushes a completion event to the queue pair’s associated
completion queue (CQ) via a DMA write. Using completion events
adds extra overhead to the RNIC’s PCIe bus. This overhead can
be reduced by using selective signaling. When using a selectively
signaled send queue of size S, up to S − 1 consecutive verbs can
be unsignaled, i.e., a completion event will not be pushed for these
verbs. The receive queue cannot be selectively signaled. As S is large
(∼ 128), we use the terms “selective signaling” and “unsignaled”
interchangeably.
2.2.3 Transport types
RDMA transports can be connected or unconnected. A connected
transport requires a connection between two queue pairs that com-
municate exclusively with each other. Current RDMA implemen-
tations support two main types of connected transports: Reliable
Connection (RC) and Unreliable Connection (UC). There is no ac-
knowledgement of packet reception in UC; packets can be lost and
the affected message can be dropped. As UC does not generate
ACK/NAK packets, it causes less network trafﬁc than RC.
In an unconnected transport, one queue pair can communicate
with any number of other queue pairs. Current implementations
provide only one unconnected transport: Unreliable Datagram (UD).
The RNIC maintains state for each active queue in its queue pair
context cache, so datagram transport can scale better for applications
with a one-to-many topology.
InﬁniBand and RoCE employ lossless link-level ﬂow control,
namely, credit-based ﬂow control and Priority Flow Control. Even
with unreliable transports (UC/UD), packets are never lost due to
buffer overﬂows. Reasons for packet loss include bit errors on the
wire and hardware failures, which are extremely rare. Therefore,
our design, similar to choices made by Facebook and others [22],
sacriﬁces transport-level retransmission for fast common case per-
formance at the cost of rare application-level retries.
Some transport types support only a subset of the available verbs.
Table 1 lists the verbs supported by each transport type. Figure 1
shows the DMA and network steps involved in posting verbs.
2.3 Existing RDMA-based key-value stores
Pilaf [21] is a key-value store that aims for high performance and low
CPU use. For GETs, clients access a cuckoo hash table at the server
using READs, which requires 2.6 round trips on average for single
GET request. For PUTs, clients send their requests to the server
using a SEND message. To ensure consistent GETs in the presence
Figure 1: Steps involved in posting verbs. The dotted arrows are PCIe PIO
operations. The solid, straight arrows are DMA operations: the thin ones are
for writing the completion events. The thick wavy arrows are RDMA data
packets and the thin ones are ACKs.
of concurrent PUTs, Pilaf’s data structures are self-verifying: each
hash table entry is augmented with two 64-bit checksums.
The second key-value store we compare against is based upon the
store designed in FaRM [8]. It is important to note that FaRM is a
more general-purpose distributed computing platform that exposes
memory of a cluster of machines as a shared address space; we
compare only against a key-value store implemented on top of FaRM
that we call FaRM-KV. Unlike the client-server design in Pilaf
and HERD, FaRM is symmetric, beﬁtting its design as a cluster
architecture: each machine acts as both a server and client.
FaRM’s design provides two components for comparison. First
is its key-value store design, which uses a variant of Hopscotch
hashing [12] to create a locality-aware hash table. For GETs, clients
READ several consecutive Hopscotch slots, one of which contains
the key with high probability. Another READ is required to fetch
the value if it is not stored inside the hash table. For PUTs, clients
WRITE their request to a circular buffer in the server’s memory.
The server polls this buffer to detect new requests. This design is
not speciﬁc to FaRM—we use it merely as an extant alternative to
Pilaf’s Cuckoo-based design to provide a more in-depth comparison
for HERD.
The second important aspect of FaRM is its symmetry; here it
differs from both Pilaf and HERD. For small, ﬁxed-size key-value
pairs, FaRM can “inline” the value with the key. With inlining,
FaRM’s RDMA read-based design still achieves lower maximum
throughput than HERD, but it uses less CPU. This tradeoff may be
right for a cluster where all machines are also busy doing compu-
tation; we do not evaluate the symmetric use case here, but it is an
important consideration for users of either design.
3. DESIGN DECISIONS
Towards our goal of supporting key-value servers that achieve the
highest possible throughput with RDMA, we explain in this section
the reasons we choose to use—and not use—particular RDMA
features and other design options. To begin with, we present an
analysis of the performance of the RDMA verbs; we then craft a
TimeCPURNICRNICCPUWRITEWRITE,INLINED,UNREALIABLE,UNSIGNALLEDREADSEND/RECV1297Name
Apt
Nodes Hardware
187
Susitna
36
Intel Xeon E5-2450 CPUs. ConnectX-3
MX354A (56 Gbps IB) via PCIe 3.0 x8
AMD Opteron 6272 CPUs. CX-3 MX353A
(40 Gbps IB) and CX-3 MX313A (40 Gbps
RoCE) via PCIe 2.0 x8
Table 2: Cluster conﬁguration
communication architecture using the fastest among them that can
support our application needs.
As hinted in Section 1, one of the core decisions to make is
whether to use memory verbs (RDMA read and write) or messag-
ing verbs (SEND and RECV). Recent work from the systems and
networking communities, for example, has focused on RDMA reads
as a building block, because they bypass the remote network stack
and CPU entirely for GETs [21, 8]. In contrast, however, the HPC
community has made wider use of messaging, both for key-value
caches [14] and general communication [16]. These latter systems
scaled to thousands of machines, but provided low throughput—less
than one million operations per second in memcached [14]. The
reason for low throughput in [14] is not clear, but we suspect appli-
cation design that makes the system incapable of leveraging the full
power of the RNICs.
There remains an important gap between these two lines of work,
and to our knowledge, HERD is the ﬁrst system to provide the best of
both worlds: throughput even higher than that of the RDMA-based
systems while scaling to several hundred clients.
HERD takes a hybrid approach, using both RDMA and messaging
to best effect. RDMA reads, however, are unattractive because of
the need for multiple round trips. In HERD, clients instead write
their requests to the server using RDMA writes over an Unreliable
Connection (UC). This write places the PUT or GET request into
a per-client memory region in the server. The server polls these
regions for new requests. Upon receiving one, the server process
executes in conventional fashion using its local data structures. It
then sends a reply to the client using messaging verbs: a SEND over
an Unreliable Datagram.
To explain why we use this hybrid of RDMA and messaging,
we describe the performance experiments and analysis that support
it. Particularly, we describe why we prefer using RDMA writes
instead of reads, not taking advantage of hardware retransmission by
opting for unreliable transports, and using messaging verbs despite
conventional wisdom that they are slower than RDMA.
3.1 Notation and experimental setup
In the rest of this paper, we refer to an RDMA read as READ
and to an RDMA write as WRITE.
In this section, we present
microbenchmarks from Emulab’s [29] Apt cluster, a large, modern
testbed equipped with 56 Gbps InﬁniBand. Because Apt has only
InﬁniBand, in Section 5, we also use the NSF PRObE’s [11] Susitna
cluster to evaluate on RoCE. The hardware conﬁgurations of these
clusters are shown in Table 2.
These experiments use one server machine and several client
machines. We denote the server machine by MS and its RNIC by
RNICS. Client machine i is denoted by Ci. The server and client
machines may run multiple server and client processes respectively.
We call a message from client to server a request, and the reply
from server to client, a response. The host issuing a verb is the
requester and the destination host responder. For unsignaled SEND
and WRITE over UC, the destination host does not actually send a
response, but we still call it a responder.
For throughput experiments, processes maintain a window of
several outstanding verbs in their send queues. Using windows
allows us to saturate our RNICs with fewer processes. In all of
our throughput experiments, we manually tune the window size for
maximum aggregate throughput.
3.2 Using WRITE instead of READ
There are several beneﬁts to using WRITE instead of READ.
WRITEs can be performed over the UC transport, which itself
confers several performance advantages. Because the responder
does not need to send packets back, its RNIC performs less pro-
cessing, and thus can support higher throughput than with READs.
The reduced network bandwidth similarly beneﬁts both the server
and client throughput. Finally, as one might expect, the latency
of an unsignaled WRITE is about half that ( 1
2 RTT) of a READ.
This makes it possible to replace one READ by two WRITEs, one
client-to-server and one server-to-client (forming an application-
level request-reply pair), without increasing latency signiﬁcantly.
3.2.1 WRITEs have lower latency than READs
Measuring the latency of an unsignaled WRITE is not straightfor-
ward as the requester gets no indication of completion. Therefore,
we measure it indirectly by measuring the latency of an ECHO. In
an ECHO, a client transmits a message to a server and the server
relays the same message back to the client. If the ECHO is realized
by using unsignaled WRITEs, the latency of an unsignaled WRITE
is at most one half of the ECHO’s latency.
We also measure the latency of signaled READ and WRITE
operations. As these operations are signaled, we use the completion
event to measure latency. For WRITE, we also measure the latency
with payload inlining.
Figure 2 shows the average latency from these measurements. We
use inlined and unsignaled WRITEs for ECHOs. On our RNICs, the
maximum size of the inlined payload is 256 bytes. Therefore, the
graphs for WR-INLINE and ECHO are only shown up to 256 bytes.
Unsignaled verbs: For payloads up to 64 bytes, the latency of
ECHOs is close to READ latency, which conﬁrms that the one-
way WRITE latency is about half of the READ latency. For larger
ECHOs, the latency increases because of the time spent in writing
to the RNIC via PIO.
Signaled verbs: The solid lines in Figure 2 show the latencies for
three signaled verbs—WRITE, READ, and WRITE with inlining
(WR-INLINE). The latencies for READ and WRITE are similar
because the length of the network/PCIe path travelled is identical.
By avoiding one DMA operation, inlining reduces the latency of
small WRITEs signiﬁcantly.
3.2.2 WRITEs have higher throughput than READs
To evaluate throughput, it is ﬁrst necessary to observe that with
many client machines communicating with one server, different
verbs perform very differently when used at the clients (talking to
one server) and at the server (talking to many clients).
Inbound throughput: First, we measured the throughput for
inbound verbs, i.e., the number of verbs that multiple remote ma-
chines (the clients) can issue to one machine (the server). Using
the notation introduced above, C1, ...,CN issue operations to MS as
shown in Figure 3a. Figure 3b shows the cumulative throughput
observed across the active machines. For up to 128 byte payloads,
WRITEs achieve 35 Mops, which is about 34% higher higher than
298(a) Setup for measuring verbs and ECHO latency. We use one client process
to issue operations to one server process
(a) Setup for measuring inbound throughput. Each client pro-
cess communicates with only one server process
(b) The one-way latency of WRITE is half of the ECHO latency. ECHO
operations used unsignaled verbs.
Figure 2: Latency of verbs and ECHO operations
(b) For moderately sized payloads, WRITE has much higher inbound
throughput than READs.
Figure 3: Comparison of inbound verbs throughput
the maximum READ throughput (26 Mops). Interestingly, reliable
WRITEs deliver signiﬁcantly higher throughput than READs despite
their identical InﬁniBand path. This is explained as follows: writes
require less state maintainance both at the RDMA and the PCIe level
because the initiator does not need to wait for a response. For reads,
however, the request must be maintained in the initiator’s memory
till a response arrives. At the RDMA level, each queue pair can
only service a few outstanding READ requests (16 in our RNICs).
Similarly, at the PCIe level, reads are performed using non-posted
transactions, whereas writes use cheaper, posted transactions.
(a) Setup for measuring outbound throughput. Each server
process communicates with only one client process.
Although the inbound throughput of WRITEs over UC and RC
is nearly identical, using UC is still beneﬁcial: It requires less pro-
cessing at RNICS, and HERD uses this saved capacity to SEND
responses.
Outbound throughput: We next measured the throughput for
outbound verbs. Here, MS issues operations to C1, ...,CN. As shown
in Figure 4a, there are N processes on MS; the ith process com-
municates with Ci only (the scalability problems associated with
all-to-all communication are explained in Section 3.3). Apart from
READs, WRITEs, and inlined WRITEs over UC, we also measure
the throughput for inlined SENDs over UD for reasons outlined in
Section 3.3. Figure 4b plots the throughput achieved by MS for dif-
ferent payload sizes. For small sizes, inlined WRITEs and SENDs
have signiﬁcantly higher outbound throughput than READs. For
large sizes, the throughput of all WRITE and SEND variants is less
than for READs, but it is never less than 50% of the READ through-
put. Thus, even for these larger items, using a single WRITE (or
SEND) for responses remains a better choice than using multiple
READs for key-value items.