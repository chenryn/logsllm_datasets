### Post Involves No DMA Operations

When the Remote Network Interface Controller (RNIC) completes the network steps associated with a verb, it pushes a completion event to the queue pair’s associated completion queue (CQ) via a Direct Memory Access (DMA) write. This process introduces additional overhead on the RNIC’s PCIe bus. However, this overhead can be reduced by using selective signaling. With a selectively signaled send queue of size \( S \), up to \( S - 1 \) consecutive verbs can be unsignaled, meaning no completion event will be pushed for these verbs. The receive queue cannot be selectively signaled. Given that \( S \) is typically large (approximately 128), the terms "selective signaling" and "unsignaled" are often used interchangeably.

### Transport Types in RDMA

RDMA transports can be either connected or unconnected. A connected transport requires a dedicated connection between two queue pairs that communicate exclusively with each other. Current RDMA implementations support two main types of connected transports: Reliable Connection (RC) and Unreliable Connection (UC). UC does not acknowledge packet reception, so packets can be lost, and the affected message can be dropped. Since UC does not generate ACK/NAK packets, it generates less network traffic compared to RC.

In an unconnected transport, one queue pair can communicate with any number of other queue pairs. The current implementation provides only one unconnected transport: Unreliable Datagram (UD). The RNIC maintains state for each active queue in its queue pair context cache, making datagram transport more scalable for applications with a one-to-many topology.

InfiniBand and RoCE use lossless link-level flow control, such as credit-based flow control and Priority Flow Control. Even with unreliable transports (UC/UD), packets are rarely lost due to buffer overflows. Packet loss is primarily due to bit errors on the wire or hardware failures, which are extremely rare. Therefore, our design, similar to choices made by Facebook and others, sacrifices transport-level retransmission for fast common-case performance at the cost of rare application-level retries.

Some transport types support only a subset of the available verbs. Table 1 lists the verbs supported by each transport type. Figure 1 illustrates the DMA and network steps involved in posting verbs.

### Existing RDMA-Based Key-Value Stores

**Pilaf [21]** is a key-value store designed for high performance and low CPU usage. For GETs, clients access a cuckoo hash table at the server using READs, which typically require 2.6 round trips on average for a single GET request. For PUTs, clients send their requests to the server using a SEND message. To ensure consistent GETs in the presence of concurrent PUTs, Pilaf's data structures are self-verifying, with each hash table entry augmented with two 64-bit checksums.

**FaRM [8]** is a more general-purpose distributed computing platform that exposes the memory of a cluster of machines as a shared address space. We compare against a key-value store implemented on top of FaRM, referred to as FaRM-KV. Unlike the client-server design in Pilaf and HERD, FaRM is symmetric, with each machine acting as both a server and client.

FaRM’s key-value store uses a variant of Hopscotch hashing to create a locality-aware hash table. For GETs, clients read several consecutive Hopscotch slots, one of which contains the key with high probability. Another read is required to fetch the value if it is not stored inside the hash table. For PUTs, clients write their request to a circular buffer in the server’s memory, which the server polls to detect new requests. This design is not specific to FaRM but serves as an alternative to Pilaf’s Cuckoo-based design for a more in-depth comparison with HERD.

For small, fixed-size key-value pairs, FaRM can "inline" the value with the key. With inlining, FaRM’s RDMA read-based design achieves lower maximum throughput than HERD but uses less CPU. This tradeoff may be suitable for clusters where all machines are also busy with computation. We do not evaluate the symmetric use case here, but it is an important consideration for users of either design.

### Design Decisions for HERD

To achieve the highest possible throughput with RDMA, we explain the reasons for choosing and not choosing specific RDMA features and other design options. We begin with an analysis of the performance of RDMA verbs and then craft a communication architecture using the fastest among them that can support our application needs.

One core decision is whether to use memory verbs (RDMA read and write) or messaging verbs (SEND and RECV). Recent work has focused on RDMA reads because they bypass the remote network stack and CPU entirely for GETs. In contrast, the HPC community has made wider use of messaging for key-value caches and general communication. These systems scaled to thousands of machines but provided low throughput—less than one million operations per second in memcached. The reason for low throughput is not clear, but we suspect it is due to an application design that fails to leverage the full power of the RNICs.

HERD takes a hybrid approach, using both RDMA and messaging to best effect. Clients write their requests to the server using RDMA writes over an Unreliable Connection (UC). This write places the PUT or GET request into a per-client memory region in the server. The server polls these regions for new requests and processes them using local data structures. It then sends a reply to the client using messaging verbs, specifically a SEND over an Unreliable Datagram.

### Notation and Experimental Setup

In the rest of this paper, we refer to an RDMA read as READ and an RDMA write as WRITE. We present microbenchmarks from Emulab’s [29] Apt cluster, a large, modern testbed equipped with 56 Gbps InfiniBand. Because Apt has only InfiniBand, we also use the NSF PRObE’s [11] Susitna cluster to evaluate on RoCE. The hardware configurations of these clusters are shown in Table 2.

These experiments use one server machine and several client machines. The server machine is denoted by \( M_S \) and its RNIC by \( RNIC_S \). Client machine \( i \) is denoted by \( C_i \). The server and client machines may run multiple server and client processes, respectively. We call a message from client to server a request and the reply from server to client a response. The host issuing a verb is the requester, and the destination host is the responder. For unsignaled SEND and WRITE over UC, the destination host does not actually send a response, but we still call it a responder.

For throughput experiments, processes maintain a window of several outstanding verbs in their send queues. Using windows allows us to saturate our RNICs with fewer processes. In all throughput experiments, we manually tune the window size for maximum aggregate throughput.

### Using WRITE Instead of READ

There are several benefits to using WRITE instead of READ. WRITEs can be performed over the UC transport, which confers several performance advantages. Since the responder does not need to send packets back, its RNIC performs less processing, allowing for higher throughput than with READs. The reduced network bandwidth similarly benefits both the server and client throughput. Finally, the latency of an unsignaled WRITE is about half that of a READ, making it possible to replace one READ with two WRITEs (one client-to-server and one server-to-client) without significantly increasing latency.

#### Latency of WRITEs vs. READs

Measuring the latency of an unsignaled WRITE is not straightforward since the requester gets no indication of completion. Therefore, we measure it indirectly by measuring the latency of an ECHO. In an ECHO, a client transmits a message to a server, and the server relays the same message back to the client. If the ECHO is realized using unsignaled WRITEs, the latency of an unsignaled WRITE is at most half of the ECHO’s latency.

We also measure the latency of signaled READ and WRITE operations. As these operations are signaled, we use the completion event to measure latency. For WRITE, we also measure the latency with payload inlining.

Figure 2 shows the average latency from these measurements. We use inlined and unsignaled WRITEs for ECHOs. On our RNICs, the maximum size of the inlined payload is 256 bytes. Therefore, the graphs for WR-INLINE and ECHO are only shown up to 256 bytes.

- **Unsignaled verbs**: For payloads up to 64 bytes, the latency of ECHOs is close to READ latency, confirming that the one-way WRITE latency is about half of the READ latency. For larger ECHOs, the latency increases due to the time spent writing to the RNIC via PIO.
- **Signaled verbs**: The solid lines in Figure 2 show the latencies for three signaled verbs—WRITE, READ, and WRITE with inlining (WR-INLINE). The latencies for READ and WRITE are similar because the length of the network/PCIe path traveled is identical. By avoiding one DMA operation, inlining reduces the latency of small WRITEs significantly.

#### Throughput of WRITEs vs. READs

To evaluate throughput, it is necessary to observe that different verbs perform differently when used by many client machines communicating with one server and vice versa.

- **Inbound throughput**: We measured the throughput for inbound verbs, i.e., the number of verbs that multiple remote machines (clients) can issue to one machine (server). Figure 3a shows the setup, and Figure 3b shows the cumulative throughput observed across the active machines. For up to 128-byte payloads, WRITEs achieve 35 Mops, which is about 34% higher than the maximum READ throughput (26 Mops). Reliable WRITEs deliver significantly higher throughput than READs despite their identical InfiniBand path. This is because writes require less state maintenance at both the RDMA and PCIe levels, as the initiator does not need to wait for a response. For reads, the request must be maintained in the initiator’s memory until a response arrives. At the RDMA level, each queue pair can only service a few outstanding READ requests (16 in our RNICs). At the PCIe level, reads are performed using non-posted transactions, whereas writes use cheaper, posted transactions.

- **Outbound throughput**: We measured the throughput for outbound verbs, where the server issues operations to the clients. Figure 4a shows the setup, and Figure 4b plots the throughput achieved by the server for different payload sizes. For small sizes, inlined WRITEs and SENDs have significantly higher outbound throughput than READs. For large sizes, the throughput of all WRITE and SEND variants is less than for READs, but it is never less than 50% of the READ throughput. Thus, even for larger items, using a single WRITE (or SEND) for responses remains a better choice than using multiple READs for key-value items.