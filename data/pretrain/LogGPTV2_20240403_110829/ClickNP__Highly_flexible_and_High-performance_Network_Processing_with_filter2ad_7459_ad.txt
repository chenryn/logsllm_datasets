5.3 PCIE I/O channel
8
trafﬁc shapers to control the ﬂow rates and their burstiness.
PktCap simply redirects all packets it receives to logger ele-
ments, which are usually located in the host. Since a single
logger cannot fully utilize the PCIe I/O channel capacity, Pk-
tCap has a Receive Side Scaling (RSS) element in FPGA to
distribute packets to multiple loggers based on the hash of
ﬂow 5-tuple. Since our PCIe channel has less capacity than
40G NIC, we add an extractor element that extracts only im-
portant ﬁelds of a packet (e.g. 5-tuple, DSCP and VLAN tag
if any) and forwards these ﬁelds (total 16B), together with
a timestamp (4B) to the logger element across PCIe. Pkt-
Cap is one example demonstrating the importance of joint
CPU/FPGA processing. Compared to FPGA, CPU has more
memory for buffering and can easily access other storages,
e.g., HDD/SSD drives as in [32], and therefore it makes more
sense to run loggers on CPU.
A2. Openﬂow ﬁrewall (OFW). Our Openﬂow [34] ﬁre-
wall supports both exact- and wildcard-matching of ﬂows.
The exact-match table is implemented using Cuckoo Hash-
ing [38] and contains 128K entries that match against ﬂow 5-
tuples. The wild-card match is based on TCAM. However, a
naive TCAM implementation with 512 104-bit entries takes
65% logic resource of our FPGA. Instead, we use BRAM-
based TCAM [27]. BRAM-based TCAM breaks search key
into 8-bit keys and use them to address lookup tables, which
trades memory for logic area. A BRAM TCAM with 2K
entries takes 14% logic and 43% BRAM. Additionally, we
design a HashTCAM to leverage the fact that many entries
in ﬂow tables share the same bit-masks. HashTCAM divides
the table space into a number of smaller hash tables, each of
which is associated with a bit-mask. Any incoming packet
will ﬁrst perform an “and” operation before looking up the
hash table. Each entry in the table is also associated with a
priority. An arbitrator logic is applied to select the matched
entry with the highest priority. HashTCAM has better trade-
off between capability and area cost. A HashTCAM with
16K ﬂow entries and 16 distinct bit-masks (similar to Broad-
com Trident II [10]) takes 19% logic and 22% BRAM. The
manager program always tries to group rules based on their
bit-masks and places groups with most rules into HashT-
CAM. The rest rules, which casnnot ﬁt into any groups in
HashTCAM, are then put into BRAM-based TCAM.
A3. IPSec gateway (IPSecGW). One issue with software
NFs is that the CPU soon becomes a bottleneck when packets
require some computation intensive processing, e.g., IPSec [26].
We have built an IPSec datapath that is able to process IPSec
packets with AES-256-CTR encryption and SHA-1 authen-
tication. As shown in §5.2, a single AES_CTR element can
achieve only 27.8 Gbps throughput. Therefore, we need two
AES_CTR elements to run in parallel to achieve line rate.
SHA-1, however, is tricky. The SHA-1 divides a packet into
smaller data blocks (64B). Although the computation in one
data block can be pipelined, there is a dependency between
successive blocks inside one IP packet – the computation
of the next block cannot start before the previous block has
ﬁnished! If we process these data blocks sequentially, the
throughput would be as low as 1.07 Gbps. Fortunately, we
(a)
(b)
Figure 8: The performance of the PCIe I/O channel. The
y-axis is in logarithmic scale.
ward the signal message to the corresponding element, again
through FIFO buffers.
Figure 8 shows a benchmark of our PCIe I/O channel with
different number of slots and batch sizes. As a base-line, we
also measure the performance of OpenCL global memory
operations – so far, the only means provided for CPU/FPGA
communication in OpenCL [8]. We can see that the maxi-
mum throughput of a single slot is around 8.4 Gbps. With
4 slots, the aggregate throughput of the PCIe I/O channel
can reach up to 25.6 Gbps. This is the maximum throughput
we can get out of our current FPGA chip due to limitation
of the clock frequency of the DMA engine. However, the
throughput of OpenCL is surprisingly low, less than 1 Gbps.
This is because the global memory API is designed to trans-
fer huge amount of data (multiple GB). This may be suitable
for applications with large data set, but not for network func-
tions that require strong stream processing capability. Sim-
ilarly, Figure 8(b) shows the communication latency. Since
OpenCL is not optimized for stream processing, the OpenCL
latency is as high as 1 ms, usually unacceptable for network
functions. In contrast, the PCIe I/O channel has very low
latency of 1 µs in polling mode (one core repeatedly polls
status register) and 9 µs in interrupt mode (with almost zero
CPU overhead).
6. APPLICATIONS
To evaluate the ﬂexibility of ClickNP, we have created ﬁve
common NFs based on ClickNP. All of them can run in our
test-bed processing real-time trafﬁc. Table 3 summarizes the
number of elements included in each network function and
the total LoC, including all elements speciﬁcation and the
coﬁguration ﬁles. Our experience also conﬁrms the ClickNP
modular architecture greatly improves the code reuse and
simpliﬁes the construction of new NFs. As shown in Ta-
ble 2, there are many chances to reuse one element in many
applications, e.g., L4_Parser is used in all ﬁve NFs in this
paper (A1-5). Each NF may take 1∼2 weeks for one pro-
grammer to develop and debug. We ﬁnd that the ability of
joint CPU/FPGA processing would also greatly help debug-
ging, as we can move an element in question to CPU, so that
we can easily print logs to track the problem.
A1. Packet generator (PktGen) and capture (PktCap).
PktGen can generate various trafﬁc patterns based on differ-
ent proﬁles. It can generate different sized ﬂows and sched-
ule them to start at different time, following given distri-
butions. Generated ﬂows can further pass through different
9
10-410-310-210-1100101102642561K4K16K64KThroughput (Gbps)Batch Size (Byte)4 slots PCIe1 slot PCIeOpenCL10-1100101102103104642561K4K16K64KLatency (us)Batch Size (Byte)4 slots PCIe1 slot PCIeOpenCLcan leverage parallelism among different packets. While the
processing of a data block of the current packet is still going,
we feed a data block of a different packet. Since these two
data blocks do not have dependency, they can be processed in
parallel. To implement this, we design a new element called
reservo (short for reservation station), which buffers up to
64 packets and schedules independent blocks to SHA-1 ele-
ment. After the signature of one packet has been computed,
the reservo element will send it to a next element that ap-
pends SHA-1 HMAC to the packet. There is one more tricky
thing. Although SHA-1 element has a ﬁxed latency, the over-
all latency of a packet is different, i.e., proportional to packet
size. When multiple packets are scheduled in SHA-1 com-
putation, these packets may be out-of-order, e.g., a smaller
packet behind a large packet may ﬁnish earlier. To prevent
this, we further design a reorder buffer element after SHA-1
element that stores the out-of-order packets and restore the
original order according to sequence numbers of packets.
A4. L4 load balancer (L4LB). We implement L4LB ac-
cording to multiplexer (MUX) in Ananta [39]. The MUX
server basically looks into the packet header and sees if a
direct address (DIP) has been assigned to the ﬂow.
If so,
the packet is forwarded to the server indicated by DIP via a
NVGRE tunnel. Otherwise, the MUX server will call a local
controller to allocate a DIP for the ﬂow. A ﬂow table is
used to record the mapping of ﬂows to their DIPs. To handle
the large trafﬁc volume in datacenters, it requires the L4LB
to support up to 32 million ﬂows in the ﬂow table. Clearly,
such a large ﬂow table cannot ﬁt into the BRAM of FPGA,
and has to be stored in onboard DDR memory. However, ac-
cessing DDR memory is slow. To improve performance, we
create a 4-way associative ﬂow cache in BRAM with 16K
cache lines. The Least Recently Used (LRU) algorithm is
used to replace entries in the ﬂow cache.
We put all elements in FPGA except for the DIPAlloc ele-
ment. Since only the ﬁrst packet of a ﬂow may hit DIPAlloc
and the allocation policy also could be complex, it is more
suitable to run DIPAlloc on CPU, being another example of
joint CPU-FPGA processing.
A5. pFabric ﬂow scheduler. As the last application, we use
ClickNP to implement one recently proposed packet schedul-
ing discipline – pFabric [12]. pFabric scheduling is simple.
In our implementation, an incoming packet ﬁrst passes a
parser element which extracts the 5-tuple and sends them to
the ﬂow cache element. If the ﬂow is not found in the ﬂow
cache, the packet’s metadata is forwarded to the global ﬂow
table, which reads the full table in DDR. If there is still no
matching entry, the packet is the ﬁrst packet of a ﬂow and a
request is sent to an DIPAlloc element to allocate a DIP for
the ﬂow according to load balancing policy. After the DIP is
determined, an entry is inserted into the ﬂow table.
After deciding the DIP of a packet, an encapsulation ele-
ment will retrieve the next-hop information, e.g., IP address
and VNET ID, and generate a NVGRE encapsulated packet
accordingly. A ﬂow entry would be invalidated if a FIN
packet is received, or a timeout occurs before receiving any
new packets from the ﬂow.
It keeps only a shallow buffer (32 packets), and always de-
queues the packet with the highest priority. When the buffer
is full, the packet with the lowest priority is dropped. pFab-
ric is shown to achieve near-optimal ﬂow completion time in
datacenters. In the original paper, the authors proposed using
a binary comparison tree (BCT) to select the packet with the
highest priority. However, while BCT takes only O(log2N )
cycles to compute the highest priority packet, there is a de-
pendency between successive selection processes. It is be-
cause only when the previous selection ﬁnishes can we know
the highest priority packet, and then the next selection pro-
cess can be started reliably. This limitation would require
the clock frequency to be at least 300MHz to achieve the line
rate of 40Gbps, which is not possible for our current FPGA
platform. In this paper, we use a different way to implement
pFabric scheduler which is much easier to parallelize. The
scheme is based on shift register priority queue [35]. Entries
are kept in a line of K registers in non-increasing priority
order. When dequeuing, all entries are shifted right and the
head is popped. This takes just 1 cycle. For an enqueue
operation, the metadata of a new packet is forwarded to all
entries. And now, with each entry, a local comparison can
be performed among the packet in the entry, the new packet,
and the packet in the neighboring entry. Since all local com-
parisons can be carried in parallel, the enqueue operation can
also ﬁnish in 1 cycle. Enqueue and dequeue operations can
further be parallelized. Therefore, a packet can be processed
in one cycle.
7. EVALUATION
7.1 Testbed and methodology
We evaluate ClickNP in a testbed of 16 Dell R720 servers
(§3.1). For each FPGA board, both Ethernet ports are con-
nected to a Top-of-Rack (ToR) Dell S6000 switch [3]. All
ClickNP NFs are running on a Windows Server 2012 R2. We
compare ClickNP with other state-of-the-art software NFs.
For those NFs running on Linux, we use CentOS 7.2 with
kernel version 3.10. In our test, we use PktGen to generate
testing trafﬁc at different rates with various packet sizes (up
to 56.4 Mpps with 64B packets). To measure the NF process-
ing latency, we embed a generation timestamp in every test-
ing packet. When packets pass the NF, they are looped back
to a PktCap which is located with PktGen in the same FPGA.
Then we can determine the latency by subtracting the gener-
ation timestamp from the receiving time of the packet. The
delay induced by the PktGen and PktCap was pre-calibrated
via direct loop-back (without NFs) and removed from our
data.
7.2 Throughput and latency
OpenFlow ﬁrewall. In this experiment, we compare OFW
with Linux ﬁrewall as well as Click+DPDK [17]. For Linux,
we use IPSet to handle exact-match rules, while use IPTables
for wildcard rules. As a reference, we also include the perfor-
mance of Dell S6000 switch, which has limited ﬁrewall ca-
pability and supports 1.7K wild-card rules. It is worth noting
10
(a)
(c)
(b)
(d)
Figure 9: Firewalls. Error bars represents the 5th and 95th percentile.
(a) and (b) Packet size is 64B.
(a)
(b)
Figure 10: IPSec gateway.
that the original Click+DPDK [17] does not support Receive
Side Scaling(RSS). In this work, we have ﬁxed this issue and
ﬁnd when using 4 cores, Click+DPDK already achieves the
best performance. But for Linux, we use as many cores as
possible (up to 8 due to RSS limitation) for best performance.
Figure 9(a) shows packet processing rates of different ﬁre-
walls with different number of wild-card rules. The packet
size is 64B. We can see that both ClickNP and S6000 can
achieve a maximum speed of 56.4 Mpps. Click+DPDK can
achieve about 18 Mpps. Since Click uses a static classiﬁca-
tion tree to implement wildcard-match, the processing speed
does not change with the number of rules inserted. Linux
IPTables has a low processing speed of 2.67 Mpps, and the
speed decreases as the number of rules increases. This is be-
cause IPTables performs linear matching for wild-card rules.
Figure 9(b) shows the processing latency under different
loads with small packets (64B) and 8K rules. Since each
ﬁrewall has signiﬁcantly different capacity, the load factor is
normalized to the maximum processing speed of each sys-
tem. Under all levels of load, FPGA (ClickNP) and ASIC
(S6000) solutions have µs-scale latency (1.23µs for ClickNP
and 0.62µs for S6000) with very low variance (1.26µs for
ClickNP and 0.63µs for S6000 at 95% percentile). However,
the software solutions have much larger delay, and also much
larger variance. For example, with Click+DPDK, when the
load is high, the latency can be as high as 50µs. Figure 9(c)
shows the processing latency with different packet sizes and
8K rules. With software solutions, the latency increases with
the packet size, mainly due to the larger memory to be copied.
In contrast, FPGA and ASIC retain the same latency irre-
spective to the packet size. In all experiments, the CPU usage
of ClickNP OFW is very low (< 5% of a core).
Finally, Figure 9(d) shows rule insertion latency when there
are already 8K rules. Click’s static classiﬁcation tree re-
quires a prior knowledge of all rules, and generating tree
for 8K rules takes one minute. IPTables rule insertion takes
12ms, which is proportional to the number of existing rules
in the table. Rule insertion in Dell S6000 takes 83.7µs. For
ClickNP, inserting a rule into HashTCAM table takes 6.3∼9.5µs
for 2∼3 PCIe round-trips, while SRAM TCAM table takes
44.9µs on average to update 13 lookup tables. ClickNP data
plane throughput does not degrade during rule insertion. We
conclude that OFW has similar performance as ASIC in packet
processing, but is ﬂexible and reconﬁgurable.
IPSec gateway. We compare IPSecGW with StrongSwan [7],
using the same cipher suite of AES-256-CTR and SHA1. We
setup one IPSec tunnel and Figure 10(a) shows the through-
put with different packet sizes. With all sizes, IPSecGW
achieves line rates, i.e., 28.8Gbps with 64B packets and 37.8
Gbps with 1500B packets. StrongSwan, however, achieves
only a maximum of 628Mbps, and the throughput decreases
as packets become smaller. This is because with smaller size,
the number of packets needed to be processed increases, and
therefore the system needs to compute more SHA1 signa-
tures. Figure 10(b) shows the latency under different load
factors. Again, IPSecGW yields constant latency of 13µs,
but StrongSwan incurs larger latency with higher variance,
up to 5ms!
L4 load balancer. We compare L4LB with Linux Virtual
Server (LVS) [4]. To stress test the system, we generate
a large number of concurrent UDP ﬂows with 64B pack-
ets, targeting one virtual IP (VIP). Figure 11(a) shows the
processing rates with different number of concurrent ﬂows.
When the number of concurrent ﬂows is less than 8K, L4LB
achieves the line rate of 51.2Mpps. However, when the num-
ber of concurrent ﬂows becomes larger, the processing rate
starts to drop. This is because of ﬂow cache misses in L4LB.
When a ﬂow is missing in the ﬂow cache, L4LB has to ac-
cess the onboard DDR memory, which results in lower per-
11
2-42-22022242601285122K8KProcessing Rate (Mpps)Number of Wildcard RulesClickNPDell S6000Click+DPDKLinux 0 10 20 30 40 5020406080100Forward Latency (us)Load Factor (%)ClickNPDell S6000Click+DPDKLinux4156712K62668K 0 20 40 60 80 1006412825651210241504Forward Latency (us)Packet Size (Byte)ClickNPDell S6000Click+DPDKLinux68K69K68K67K67K67K100101102103104HashTCAMBRAM TCAMDell S6000IPTablesRule Update Latency (us) 0.1 1 10 1006412825651210241504Throughput (Gbps)Packet Size (Byte)ClickNPStrongSwan10110210310420406080100Latency (us)Load Factor (%)ClickNPStrongSwan(a)
(b)
Figure 11: L4 Load Balancer.
(c)
formance. When there are too many ﬂows, e.g., 32M, cache
miss dominates and for most of the packets, L4LB needs to
have one access to the DDR memory. So the processing rate
reduces to 11Mpps. In any case, the processing rate of LVS
is low. Since LVS associates a VIP to only one CPU core, its
processing rate is bound to 200Kpps.
Figure 11(b) shows the latency under different load con-
ditions. In this experiment, we ﬁx the number of concurrent
ﬂows to 1 million. We can see that L4LB achieves very low
latency of 4µs. LVS, however, incurs around 50µs delay.
This delay goes up quickly when the offered load is higher
than 100Kpps, which exceeds the capacity of LVS.
Finally, Figure 11(c) compares the capability of L4LB and
LVS to accept new ﬂows. In this experiment, we instruct Pk-
tGen to generate as many one-packet tiny ﬂows as possible.
We can see that L4LB can accept up to 10M new ﬂows per
second. Since a single PCIe slot can transfer 16.5M ﬂits per
second, the bottleneck is still DDR access. Our DIPAlloc
element simply allocates DIP in a round-robin manner. For
complex allocation algorithms, the CPU core of DIPAlloc
will be the bottleneck, and the performance can be improved
by duplicating DIPAlloc elements on more CPU cores. For
LVS, due to the limited packet processing capacity, it can
only accept up to 75K new ﬂows per second.
7.3 Resource utilization
In this subsection, we evaluate the resource utilization of
ClickNP NFs. Table 3 summarizes the results. Except for
IPSec gateway which uses most BRAMs to hold coding books,
all other NFs only use moderate resources (5∼50%). There
is still room to accommodate even more complex NFs.