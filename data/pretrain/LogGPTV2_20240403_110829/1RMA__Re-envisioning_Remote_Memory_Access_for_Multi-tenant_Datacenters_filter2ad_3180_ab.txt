determines access control rules by binding connections to logical
protection domains. A QP consists of a send queue and a receive
queue, and is bound to a completion queue (CQ). Connected trans-
ports (RC and UC) offer one-to-one communication between QPs
(called connections hereafter).
709
QsMTTMPTC1Cn     Last Level    CacheMemoryCPUSRAMNIC CacheRDMA StatePCIeServerRNIC23411RMA: Re-envisioning Remote Memory Access for Multi-tenant Datacenters
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Op Execution Flow. Applications post ops (called work queue
entries, or WQEs) to the relevant queue via a user-space library
(libibverbs). Figure 1 shows the steps in the remaining execution
flow. To improve performance, RNICs cache active connections’
state as well as the memory translation table (MTT) and memory
protection table (MPT). These tables contain metadata related to
memory available for remote access [33]; they reside authoritatively
in host memory.
2.1 Challenges with Standard RDMA
Standard RDMA’s architecture leads to several fundamental chal-
lenges for use in multi-tenant datacenters:
1. Connection exhaustion. Caching host-resident state allows
RNICs to provide an illusion of unbounded connection counts. How-
ever, RNIC caches can be overwhelmed by large workloads, leading
to performance cliffs as the cache is filled on demand [9, 12, 18–
21, 33]. Worse still, allocation policies oblivious to business priori-
ties can strand critical applications without connections, e.g., when
lower-priority applications have previously consumed all available
connections. Recent works [16, 19–21, 28] explore using Unreliable
Datagram (UD) to overcome scalability limitations, often implement-
ing connection abstractions in software. However, UD is two-sided,
and re-introduces CPU bottlenecks on the receiving host, falling
short of one-sided op rate and latency.
2. Induced ordering. Systems may attempt to address connection
exhaustion by multiplexing several independent workloads on the
same QP. However, standard RDMA requires FIFO execution of
ops of the same type within a single QP, thereby inducing false
ordering constraints among unrelated ops. Thus, this approach can
lead to priority inversion due to head-of-line blocking between large,
background ops and small, latency-sensitive ops [39]. Likewise,
op failures may cause connection teardown, imposing shared fate
for ops unluckily sharing a connection. Furthermore, the ordered-
execution requirement places a de facto near-in-order packet delivery
requirement on the network, significantly complicating deployment
of modern high-performance network capabilities, such as adaptive
routing [26]. And yet, the RNIC itself must still implement order
recovery in hardware [23, 31], exacerbating complexity.
3. Poor semantics. We have found that there is little semantic utility
from RDMA’s notions of ordering. In particular, it is not possible
to reason about RDMA write side effects under connection tear-
down cases: standard RDMA’s write op may still cause new side
effects (e.g., mutating memory at the destination) even after its fail-
ure has been reported (e.g., via retransmit timeout at the initiator;
see Figure 2). Such side effects make building reliable distributed
algorithms more difficult.
4. Connection-centric security. Standard RDMA ties access con-
trol to connections; a system that eliminates connections must solve
access control by other means. Although recent RNICs provide line-
rate encryption, these approaches also rely on connections, and do
not provide ready means to manage encryption keys. Applications
that seek to rotate encryption keys in standard RDMA are obliged to
reconnect, a costly and complex operation.
5. Rigid congestion control. RDMA over Converged Ethernet
(RoCE) allows RDMA traffic to coexist with traffic from other proto-
col stacks, but requires fabric switches to use Priority Flow Control
(PFC) to provide a near-lossless substrate. As is well known, using
Figure 2: Delayed writes can cause future mutations after the
client receives a completion event with an error.
PFC is untenable in commercial scale networks [17, 41] due to head-
of-line blocking, poor at-scale failure isolation [30, 38, 41], and risk
of deadlock [17, 38]. Recent hardware congestion control schemes
reduce reliance on PFC [25, 41], but such techniques have limited
applicability in our environments, in which congestion control algo-
rithms are routinely updated and customized per deployment.
6. Firmware slow-paths. RNICs may rely on firmware to handle
corner cases that arise in congestion control and other sources of
complexity; at scale, firmware traps can create invisible and intolera-
ble bottlenecks, leading to goodput collapse in our datacenters. For
example, firmware’s handling of connection teardown can compete
with its ability to handle loss and maintain ordering.
3
1RMA overcomes the challenges of datacenter-scale remote mem-
ory access via a design philosophy that: (1) delegates to software
all actions whose full and correct realization requires application
intervention, such as ordering and failure recovery, as these funda-
mentally cannot be realized solely in the NIC; (2) implements in
hardware functionality that precludes software intervention, notably
DMA capabilities, host-wide incast bounding, authentication, and
encryption.
1RMA Overview
The resulting 1RMA NIC is simple, and focuses on providing
performant one-sided read, write, and management primitives im-
plemented entirely in hardware. Send and receive are not directly
supported, as these are easy enough to implement in software.
In 1RMA, software handles op pacing, congestion management,
and policy choices on when/how to retry failed ops (e.g., those that
time out). The 1RMA NIC assists software by providing timely op
completion (< 50𝜇s) including and especially in failure cases, and by
providing early indications of congestion build-up through precise
delay measures in each completion.
3.1 Example: 1RMA Read Op Execution
We illustrate the overall operation of 1RMA by means of an example
2KB-sized RMA read (see Figure 3).
Prior to executing any RMA op, the initiating client performs
an out-of-band RPC to obtain the necessary information to access
the remote memory region in question ( 1 – 2 ). These include an
encryption key, 𝐾𝑑 (i.e., a cryptographically-secure key bound to the
initiating client, difficult to guess and non-transferrable §4.2), and a
RegionId—the architectural name of the memory to be accessed—
established at memory-registration–time on the server. All protocol
messages are signed using a message authentication code generated
from 𝐾𝑑 . Similarly, all data is encrypted using 𝐾𝑑 .
710
ClientServerWrite   Retry...RMA DataDelayed ArrivalWrite DataTimeoutCompletion              with errorSIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
A. Singhvi et al.
Figure 3: Execution of a 2KB Read op in 1RMA. (1 & 2) Client
performs out-of-band communication to obtain information to
access remote memory region. (3) The client initiates the 2KB
read op by writing a command into a command slot on the
1RMA NIC. (4) The op is sent over the network subject to
1RMA’s solicitation rules. (5) The request reaches the server-
side 1RMA NIC which reads the requested data via PCIe and
(6) streams read data as individual network responses. (7) Fi-
nally, once all the data arrives, a successful op completion is
written to the client software.
Having obtained the necessary information for remote access, the
client initiates its desired RMA op—in this example, a 2KB read—
by writing a command over PCIe with write-combining MMIO
stores into an on-NIC command slot, which enqueues the operation
for service ( 3 ). The request awaits execution, subject to 1RMA’s
solicitation rules. To enter service, an op requires 4KB of free space
in the on-chip solicitation window, which is an SRAM buffer that
lands inbound payloads. Arbitrating for 4KB regardless of op size
prevents small ops from starving large ops.
Once the op enters service, the NIC debits the solicitation window
by the actual size (2KB), signs the read request using 𝐾𝑑 provided
in the RMA command (§4.4), and sends it on the network ( 4 ).
Commonly, the request arrives at the server-side 1RMA NIC
shortly thereafter, which consults a fixed-size, on-chip table to look
up key information for the RegionId included in the request, de-
rives 𝐾𝑑 , and authenticates the inbound packet. The NIC then reads
the requested data via PCIe ( 5 ), and streams read completions as
individual network responses ( 6 ). Each response is encrypted and
signed with the previously derived key information. These responses
traverse the network, perhaps arriving out-of-order due to adaptive
routing.
Upon reaching the initiator, each response is individually authen-
ticated and decrypted, then streamed via PCIe writes to the initiating
host’s memory, each at an offset encoded in the inbound response. To
tolerate unordered responses, the NIC tracks byte arrivals, and once
all bytes arrive (in error-free cases), a successful op completion is
written to the initiating software ( 7 ). The completion also includes
hardware delay measures, indicating how long it took to execute
the operation (total_delay) and how long it took for the request to
enter service at the initiator (issue_delay).
Whereas we have described the failure-free case, the example
operation above can experience a variety of failures. For instance, the
read operation may not enter service at the initiator due to heavy local
congestion (e.g., due to a large number of queued ops). When the
request arrives at an overloaded server, the serving NIC might drop or
"NACK" the request (e.g., due to an over-long inbound request queue;
§4.5). Finally, responses from the server to the client may be dropped
Figure 4: 1RMA Software consists of
two components:
(1) CommandPortal provides a familiar command/completion
queue construct and (2) CommandExecutor provides support for
arbitrary large transfers and congestion management.
or delayed in the network. 1RMA generates precise fast feedback
in the form of explicit failure codes that indicate to the initiating
software which failure mode manifested (§4.5). Upon encountering
such failures, application software can take the appropriate action,
such as immediately retrying the operation, perhaps to a different
backend server replica in the scope of a broader system.
1RMA Design In Depth
Our example focused on a small 2KB op. For larger transfers
(e.g., a 4MB read), a 1RMA per-client software module called
CommandExecutor (Figure 4) breaks large transfers into small up-to-
4KB ops (§4.4), and then batches, pipelines and paces these ops at a
rate determined by a software-based congestion control algorithm.
Congestion control relies on hardware delay measures and precise
failure outcomes reported with completions to modulate request rate
to avoid both local and network/remote congestion while keeping
utilization high (§6).
4
This section describes the key ideas of 1RMA, its security model,
hardware components, and software abstractions.
4.1 Key Ideas
There are five key elements to 1RMA’s design:
• Connectionless security: 1RMA embraces security require-
ments as first-order. We use a novel, connection-free proto-
col that binds both encryption and authentication to the spe-
cific memory region being accessed; and to the accessing pro-
cess/host pair.
• Solicitation: 1RMA relies on solicitation to limit the sever-
ity of sudden, transient incasts, because software congestion
control cannot react instantaneously. 1RMA uses fine-grained
per-operation admission control, via a solicitation window main-
tained at each initiating NIC, which bounds the dynamic number
of inbound bytes. New requests will stall until sufficient window
capacity can be allocated. Window capacity frees as prior ops
complete.
• Writes via request-to-read: 1RMA implements write oper-
ations as a request-to-read: the writer asks a remote NIC to
retrieve data via a read operation. Although this approach adds
a round-trip, it unifies security and solicitation for reads and
writes and underpins our solutions to provide precise write fail-
ure semantics, replay protection, and incast avoidance.
• Explicitly-managed hardware resources: 1RMA offers no il-
lusions of unlimited resources. Instead, it leverages higher-level
resource allocation to apportion its finite hardware resources
according to application-level priority. This approach explicitly
bounds the work applications can offer at a time. Importantly,
offering no illusions of unlimited resources enables 1RMA to do
711
PCIe WriteClientServerNICNICSWPCIePCIeSWNetworkGetKeyKd, RegionIDRead RequestRead Responsecompletionissue_delaytotal_delayFabricTrafficPCIe ReadRPCEncrypted2134567CommandExecutorExposed CommandSlotsCompletionMemory RegionCommandPortalChunking and Congestion ControlOp Pacing 1RMANICApplication 11RMA: Re-envisioning Remote Memory Access for Multi-tenant Datacenters
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
pertaining to a transfer ( 4 ). It is computed as follows:
𝐾𝑑 = 𝐴𝐸𝑆(𝐾𝑒𝑦 = 𝐾𝑟 , 𝐶𝑜𝑛𝑡𝑒𝑛𝑡𝑠 = 𝐴𝑑𝑑𝑟𝑒𝑠𝑠_𝐼𝑛𝑖𝑡𝑖𝑎𝑡𝑜𝑟,
𝑃𝐼𝐷_𝐼𝑛𝑖𝑡𝑖𝑎𝑡𝑜𝑟, 𝑂𝑝𝑒𝑟𝑎𝑡𝑖𝑜𝑛𝑇𝑦𝑝𝑒)
This function computes a key that is specific to each initiating
process (identified by the host address and the PID of the process)
and op type, and yet is rooted in a single, shared 𝐾𝑟 that easily
fits in on-NIC memory in a per-region table. When an application
allocates command slots (§4.3)—used to initiate RMAs—the 1RMA
driver stores the associated PID_Initiator immutably in the slots’
configuration. 1RMA hardware always includes PID_Initiator in
request packets along with the RegionId ( 5 ). Therefore, the serving
1RMA NIC can derive the assigned 𝐾𝑑 for each inbound request
( 6 ). Also, key derivation can be easily performed in server-side
software (which possesses 𝐾𝑟 ) — in the context of an authenticated
RPC, server software can compute 𝐾𝑑 for any potential initiator and
communicate 𝐾𝑑 to that initiator in the RPC response, without the
need to retain 𝐾𝑑 in any on-host or on-NIC tables ( 2 ). 1RMA salts
the encryption process with per-NIC ascending message counters,
which protects key integrity and guards against replays, covered in
detail in Appendix A. Critically, on-NIC security-related state does
not grow with the number of communicating endpoint pairs.
Returning to the two attack vectors: In A1, the attacker can easily
guess RegionId but has difficulty in acquiring a 𝐾𝑑 matching host
and process, absent a root-level exploit. In A2, the attacker can
observe ciphertext in transit, but not easily decrypt it without also