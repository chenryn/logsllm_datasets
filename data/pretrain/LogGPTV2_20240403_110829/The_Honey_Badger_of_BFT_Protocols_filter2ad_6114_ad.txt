unaninimously propose 1, it is possible that the resulting bit vector
could be empty.
To avoid this problem, nodes abstain from proposing 0 until they
are certain that the ﬁnal vector will have at least N − f bits set.
To provide some intuition for the ﬂow of this protocol, we narrate
several possible scenarios in Figure 3. The algorithm from Ben-Or
et al. [9] is given in Figure 4. The running time is O(logN) in
expectation, since it must wait for all binary agreement instances to
37PROOF. These two properties follow immediately from proper-
ties of the high-level protoocls, ACS and TPKE. Each ACS instance
guarantees that nodes agree on a vector of ciphertexts in each epoch
(Step 2). The robustness of TPKE guarantees that each correct node
decrypts these ciphertexts to consistent values (Step 3). This sufﬁces
to ensure agreement and total order.
THEOREM 2. (Complexity). Assuming a batch size of B =
Ω(λ N2 logN), the running time for each HoneyBadgerBFT epoch
is O(logN) in expectation, and the total expected communication
complexity is O(B).
PROOF. The cost and running time of ACS is explained in Sec-
tion 4.4. The N instances of threshold decryption incur one ad-
ditional round and an additional cost of O(λ N2), which does not
affect the overall asymptotic cost.
The HoneyBadgerBFT protocol may commit up to B transactions
in a single epoch. However, the actual number may be less than this,
since some correct nodes may propose overlapping transaction sets,
others may respond too late, and corrupted nodes may propose an
empty set. Fortunately, we prove (in the online full version [42])
that assuming each correct node’s queue is full, then B/4 serves as
an lower bound for the expected number of transactions committed
in an epoch.5
THEOREM 3. (Efﬁciency). Assuming each correct node’s queue
contains at least B distinct transactions, then the expected number
of transactions committed in an epoch is at least B
4 , resulting in
constant efﬁciency.
Finally, we prove (in the online full version [42]) that the ad-
versary cannot signiﬁcantly delay the commit of any transaction.
THEOREM 4. (Censorship Resilience). Suppose an adversary
passes a transaction tx as input to N − f correct nodes. Let T be the
size of the “backlog”, i.e. the difference between the total number of
transactions previously input to any correct node and the number of
transactions that have been committed. Then tx is commited within
O(T /B + λ ) epochs except with negligible probability.
5. IMPLEMENTATION AND EVALUATION
In this section we carry out several experiments and performance
measurements using a prototype implementation of the HoneyBad-
gerBFT protocol. Unless otherwise noted, numbers reported in this
section are by default for the optimistic case where all nodes are
behaving honestly.
First we demonstrate that HoneyBadgerBFT is indeed scalable by
performing an experiment in a wide area network, including up to
104 nodes in ﬁve continents. Even under these conditions, Honey-
BadgerBFT can reach peak throughputs of thousands of transactions
per second. Furthermore, by a comparison with PBFT, a represen-
tative partially synchronous protocol, HoneyBadgerBFT performs
only a small constant factor worse. Finally, we demonstrate the
feasibility of running asynchronous BFT over the Tor anonymous
communication layer.
Implementation details. We developed a prototype implementa-
tion of HoneyBadgerBFT in Python, using the gevent library for
concurrent tasks.
5The actual bound is (1− e−1/3)B > B/4, but we use the looser
bound B/4 for readability.
Figure 3: (Illustrated examples of ACS executions.) Each exe-
cution of our protocol involves running N concurrent instances
of reliable broadcast (RBC), as well as N of byzantine agree-
ment (BA), which in turn use an expected constant number of
common coins. We illustrate several possible examples of how
these instances play out, from the viewpoint of Node 0. (a) In
the ordinary case, Node 0 receives value V1 (Node 1’s proposed
value) from the reliable broadcast at index 1. Node 0 therefore
provides input “Yes” to BA1, which outputs “Yes.” (b) RBC2
takes too long to complete, and Node 0 has already received
(N− f ) “Yes” outputs, so it votes “No” for BA2. However, other
nodes have seen RBC2 complete successfully, so BA2 results in
“Yes” and Node 0 must wait for V2. (c) BA3 concludes with “No”
before RBC3 completes.
ﬁnish. 4 When instantiated with the reliable broadcast and binary
agreement constructions described above, the total communication
complexity is O(N2|v| + λ N3 logN) assuming |v| is the largest size
of any node’s input.
4.5 Analysis
First we observe that the agreement and total order properties
follow immediately from the deﬁnition of ACS and robustness of
the TPKE scheme.
THEOREM 1. (Agreement and total order). The HoneyBad-
gerBFT protocol satisﬁes the agreement and total order properties,
except for negligible probability.
4The expected running time can be reduced to O(1) (c.f. [8]) by run-
ning several instances in parallel, though this comes at the expense
of throughput.
Algorithm ACS (for party Pi)
Let {RBCi}N refer to N instances of the reliable broadcast pro-
tocol, where Pi is the sender of RBCi. Let {BAi}N refer to N
instances of the binary byzantine agreement protocol.
• upon receiving input vi, input vi to RBCi // See Figure 2
• upon delivery of v j from RBC j, if input has not yet been pro-
vided to BA j, then provide input 1 to BA j. See the online full
version [42]
• upon delivery of value 1 from at least N − f instances of BA,
provide input 0 to each instance of BA that has not yet been
provided input.
• once all instances of BA have completed, let C ⊂ [1..N] be the
indexes of each BA that delivered 1. Wait for the output v j for
each RBC j such that j ∈ C. Finally output ∪ j∈Cv j.
Figure 4: Common Subset Agreement protocol (from Ben-Or
et al. [9])
RBC2RBC3RBC1BA1BA2BA3YesNoV1CoinCoinCoinCoinCoinCoinCoinYes(a) NormalYesV2(b) Wait for slow broadcast(c) Broadcast failsNo….….Time38For deterministic erasure coding, we use the zfec library [52],
which implements Reed-Solomon codes. For instantiating the com-
mon coin primitive, we implement Boldyreva’s pairing-based thresh-
old signature scheme [11]. For threshold encryption of transactions,
we use Baek and Zheng’s scheme [7] to encrypt a 256-bit ephemeral
key, followed by AES-256 in CBC mode over the actual payload.
We implement these threshold cryptography schemes using the
Charm [3] Python wrappers for PBC library [38]. For threshold sig-
natures, we use the provided MNT224 curve, resulting in signatures
(and signature shares) of only 65 bytes, and heuristically providing
112 bits of security.6 Our threshold encryption scheme requires a
symmetric bilinear group: we therefore use the SS512 group, which
heuristically provides 80 bits of security [45].7
In our EC2 experiments, we use ordinary (unauthenticated) TCP
sockets. In a real deployment we would use TLS with both client
and server authentication, adding insigniﬁcant overhead for long-
lived sessions. Similarly, in our Tor experiment, only one endpoint
of each socket is authenticated (via the “hidden service” address).
Our theoretical model assumes nodes have unbounded buffers.
In practice, more resources could be added dynamically to a node
whenever memory consumption reaches a watermark, (e.g., when-
ever it is 75% full) though our prototype implementation does not
yet include this feature. Failure to provision an adequate buffer
would count against the failure budget f .
5.1 Bandwidth Breakdown and Evaluation
We ﬁrst analyze the bandwidth costs of our system. In all exper-
iments, we assume a constant transaction size of mT = 250 bytes
each, which would admit an ECDSA signature, two public keys,
as well as an application payload (i.e., approximately the size of
a typical Bitcoin transaction). Our experiments use the parameter
N = 4 f ,8 and each party proposes a batch of B/N transactions. To
model the worst case scenario, nodes begin with identical queues of
size B. We record the running time as the time from the beginning
of the experiment to when the (N − f )-th node outputs a value.
Bandwidth and breakdown ﬁndings. The overall bandwidth con-
sumed by each node consists of a ﬁxed additive overhead as well
as a transaction dependent overhead. For all parameter values we
considered, the additive overhead is dominated by an O(λ N2) term
resulting from the threshold cryptography in the ABA phases and
the decryption phase that follows. The ABA phase involves each
node transmitting 4N2 signature shares in expectation. Only the
RBC phase incurs a transaction-dependent overhead, equal to the
erasure coding expansion factor r = N
N−2 f . The RBC phase also
contributes N2 logN hashes to the overhead because of Merkle tree
branches included in the ECHO messages. The total communication
cost (per node) is estimated as:
mall = r(BmT + NmE) + N2((1 + logN)mH + mD + 4mS)
where mE and mD are respectively the size of a ciphertext and
decryption share in the TPKE scheme, and mS is the size of a TSIG
signature share.
6Earlier reports estimate 112 bits of security for the MNT224
curve [45]; however, recent improvements in computing discrete log
suggest larger parameters are required [28, 29].
7We justify the relatively weak 80-bit security level for our parame-
ters because the secrecy needs are short-lived as the plaintexts are
revealed after each batch is committed. To defend against precompu-
tation attacks, the public parameters and keys should be periodically
regenerated.
8The setting N = 4 f is not the maximum fault tolerance, but it is
convenient when f divides N.
Figure 5: Estimated communication cost in megabytes (per
node) for varying batch sizes. For small batch sizes, the ﬁxed
cost grows with O(N2 logN). At saturation, the overhead factor
approaches
N
N−2 f < 3.
Figure 6: Throughput (transactions committed per second) vs
number of transactions proposed. Error bars indicate 95%
conﬁdence intervals.
The system’s effective throughput increases as we increase the
proposed batch size B, such that the transaction-dependent portion
of the cost dominates. As Figure 5 shows, for N = 128, for batch
sizes up to 1024 transactions, the transaction-independent bandwidth
still dominates to overall cost. However, when when the batch size
reaches 16384, the transaction-dependent portion begins to dominate
— largely resulting from the RBC.ECHO stage where nodes transmit
erasure-coded blocks.
5.2 Experiments on Amazon EC2
To see how practical our design is, we deployed our protocol on
Amazon EC2 services and comprehensively tested its performance.
We ran HoneyBagderBFT on 32, 40, 48, 56, 64, and 104 Amazon
EC2 t2.medium instances uniformly distributed throughout its 8
regions spanning 5 continents. In our experiments, we varied the
batch size such that each node proposed 256, 512, 1024, 2048, 4096,
8192, 16384, 32768, 65536, or 131072 transactions.
Throughput. Throughput is deﬁned as the number of transactions
committed per unit of time. In our experiment, we use “conﬁrmed
transactions per second” as our measure unit if not speciﬁed oth-
erwise. Figure 6 shows the relationship between throughput and
total number of transactions proposed by all N parties. The fault
tolerance parameter is set to be f = N/4.
100101102103104105Batch size (Tx) in log scale10-210-1100101102Communication cost per node (MB)Nodes / Tolerance8/216/432/864/16128/32ideal105106Batch size (Tx) in log scale102103104Throughput (Tx per second) in log scaleNodes / Tolerance32/840/1048/1256/1464/16104/2639HoneyBadgerBFT
PBFT
·104
2
)
d
n
o
c
e
s
r
e
p
x
T
(
t
u
p
h
g
u
o
r
h
T
m
u
m
i
x
a
M
1.5
1
0.5
0
8 nodes
16 nodes
32 nodes
64 nodes
Figure 8: Comparison with PBFT on EC2s
tributes this load evenly among the network links, whereas PBFT
bottlenecks on the leader’s available bandwidth. Thus PBFT’s at-
tainable throughput diminishes with the number of nodes, while
HoneyBadgerBFT’s remains roughly constant.
Note that this experiment reﬂects only the optimistic case, with no
faults or network interruptions. Even for small networks, HoneyBad-
gerBFT provides signiﬁcantly better robustness under adversarial
conditions as noted in Section 3. In particular, PBFT would achieve
zero throughput against an adversarial asynchronous scheduler,
whereas HoneyBadgerBFT would complete epochs at a regular rate.
5.3 Experiments over Tor
To demonstrate the robustness of HoneyBadgerBFT, we run the
ﬁrst instance (to our knowledge) of a fault tolerant consensus proto-
col carried out over Tor (the most successful anonymous communi-
cation network). Tor adds signiﬁcant and varying latency compared
to our original AWS deployment. Regardless, we show that we can
run HoneyBadgerBFT without tuning any parameters. Hiding Hon-
eyBadgerBFT nodes behind the shroud of Tor may offer even better
robustness. Since it helps the nodes to conceal their IP addresses, it
can help them avoid targeted network attacks and attacks involving
their physical location.
Brief background on Tor. The Tor network consists of approxi-
mately 6,500 relays, which are listed in a public directory service.
Tor enables “hidden services,” which are servers that accept con-
nections via Tor in order to conceal their location. When a client
establishes a connection to a hidden service, both the client and
the server construct 3-hop circuits to a common “rendezvous point.”
Thus each connection to a hidden service routes data through 5
randomly chosen relays. Tor provides a means for relay nodes to
advertise their capacity and utilization, and these self-reported met-
rics are aggregated by the Tor project. According to these metrics,9
the total capacity of the network is ∼145Gbps, and the current
utilization is ∼65Gbps.
Tor experiment setup. We design our experiment setup such that
we could run all N HoneyBadgerBFT nodes on a single desktop
machine running the Tor daemon software, while being able to re-
alistically reﬂect Tor relay paths. To do this, we conﬁgured our