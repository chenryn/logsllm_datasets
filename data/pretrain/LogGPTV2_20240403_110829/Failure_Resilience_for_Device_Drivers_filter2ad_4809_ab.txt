component to a minimum according to the principle of
least authority [34]. The privileges are passed to the
trusted reincarnation server when a component is loaded
through the service utility, which, in turn, informs the
other servers, drivers, and microkernel so that the re-
strictions can be enforced at run-time. System processes
are given an unprivileged user and group ID to restrict
among other things ﬁle system access. For servers, we
restrict the use of IPC primitives, system calls, and ker-
nel calls. For device drivers, the same restrictions ap-
ply, and in addition, we restrict access to I/O ports, IRQ
lines, and device memory such as the video memory.
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 20075 RECOVERY PROCEDURE
In this section, we describe the general recovery proce-
dure in the event of failures. The ability of our system
to deal with dead drivers implies that we can dynami-
cally start and stop drivers while the operating system is
running. Starting a new device driver is done through the
service utility, which forwards the request to the reincar-
nation server. The following arguments are passed along
with the request: the driver’s binary, a stable name, the
process’ precise privileges, a heartbeat period, and, op-
tionally, a parametrized policy script—a shell script that
is called after a driver failure to manage the recovery
procedure. If all arguments make sense, the reincarna-
tion server forks a new process, sets the process’ privi-
leges, and executes the driver’s binary. From that point
on, the driver will be constantly guarded to ensure con-
tinuous operation—even in the event of a failure. How
the reincarnation server can detect defects and how the
recovery procedure works is described below.
5.1 Defect Detection
While a human user observes driver defects when
the system crashes, becomes unresponsive, or starts to
behave in strange ways,
the operating system needs
other ways to detect failures. Therefore, the reincar-
nation server monitors the system at run-time to ﬁnd
defects. The various inputs that can cause the recovery
procedure to be initiated are:
1. Process exit or panic.
2. Crashed by CPU or MMU exception.
3. Killed by user.
4. Heartbeat message missing.
5. Complaint by other component.
6. Dynamic update by user.
At any point in time the reincarnation server has ac-
curate information about the presence of all servers and
drivers, since it is the parent of all system processes.
When a server or driver crashes, panics or exits for an-
other reason, the process manager will notify the rein-
carnation server with a SIGCHLD signal, according to
the POSIX speciﬁcation. At that point it collects and
inspects the exit status of the exitee, which leads to the
ﬁrst three defect classes. Since a process exit is directly
reported by the process manager, immediate action can
be taken by the reincarnation server.
In addition, the reincarnation server can proactively
check the system’s state. Depending on the conﬁgura-
tion passed upon loading, the reincarnation server can
periodically request drivers to send a heartbeat message.
Failing to respond N consecutive times causes recov-
ery to be initiated. Heartbeats help to detect processes
that are ‘stuck,’ for example, in an inﬁnite loop, but
do not protect against malicious code. To prevent bog-
ging down the system status requests and the consequent
replies are sent using nonblocking messages.
Furthermore, the reincarnation server can be used
as an arbiter in case of conﬂicts, allowing authorized
servers to report malfunctioning components. How a
malfunction is precisely deﬁned depends on the com-
ponents at hand, but in general has to do with protocol
violations. For example, the ﬁle server can request re-
placement of a disk driver that sends unexpected request
messages or fails to respond to a request. The authority
to replace other components is part of the protection ﬁle
that speciﬁes a process’ privileges.
Finally, faulty behavior also can be noticed by the
user, for example, if the audio sounds weird. In such
a case, the user can explicitly instruct the reincarnation
server to restart a driver or replace it with a newly com-
piled one. As another example, latent bugs or vulnera-
bilities may lead to a dynamic update as soon as a patch
is available. Since about a quarter of downtime is caused
by reboots due to maintenance [42], such dynamic up-
dates that allow patching the system on the ﬂy can sig-
niﬁcantly increase system availability.
5.2 Policy-Driven Recovery
By default, the reincarnation server directly restarts
a crashed component, but if more ﬂexibility is wanted,
the administrator can instruct it to use a parametrized
policy script that governs the actions to be taken after a
failure. Policy scripts can be shared, but dedicated re-
covery scripts can be associated with individual servers
and drivers as well. When a malfunctioning component
is detected, the reincarnation server looks up the associ-
ated policy script and executes it. Input arguments are
which component failed, the kind of failure as indicated
in Sec. 5.1, the current failure count, and the parameters
passed along with the script. The primary goal of the
policy script is to decide when to restart the malfunction-
ing component, but other actions can be taken as well.
Restarting is always done by requesting the reincarna-
tion server to do so, since that is the only process with
the privileges to create new servers and drivers.
The simplest policy script immediately tries to restart
the failed component, but the policy-driven recovery
procedure can use the information passed by the rein-
carnation server to make decisions about the precise re-
covery steps taken. For example, consider the generic
policy script in Fig. 2. Lines 1–4 process the arguments
passed by the reincarnation server. Then, lines 6–10
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007agenericrecoveryscript
component=$1
reason=$2
repetition=$3
shift 3
# component name
# numbers as in Sec. 3.1
# current failure count
# get to script parameters
if [ ! $reason -eq 6 ]; then
sleep $((1 << ($repetition - 1)))
ﬁ
service restart $component
status=$?
while getopts a: option; do
case $option in
a)
cat << END | mail -s ”Failure Alert” ”$OPTARG”
failure: $component, $reason, $repetition
restart status: $status
END
;;
esac
done
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
Figure 2: A parametrized, generic recovery script. Binary ex-
ponential backoff is used before restarting, except for dynamic
updates. Optionally, a failure alert is sent.
restart the component, possibly after an exponentially
increasing delay to prevent bogging down the system in
the event of repeated failures. The backoff protocol is
not used for dynamic updates that are requested explic-
itly. Finally, lines 12–21 optionally send an e-mail alert
if the parameter ’-a’ and an e-mail address are passed.
Using shell scripts provides a lot of ﬂexibility and
power for expressing policies. For example, a dedicated
policy script can help the administrator recover from
failures in the network server. Such a failure closes all
open network connections, including the sockets used
by the X Window System. Among other things, recov-
ery requires restarting the DHCP client and X Window
System, which can be speciﬁed in a dedicated policy
script. As another example, when a required component
cannot be recovered or fails too often, the policy script
may reboot the entire system, which clearly is better
than leaving the system in an unusable state. At the very
least, the policy script can log the failing component and
its execution environment for future inspection. As an
aside, the ability of our system to pinpoint precisely the
responsible components might have legal consequences
for software vendors (since it can help determine liabil-
ity for damage caused by a crash), which may help in
building more dependable systems [43].
5.3 Post-Restart Reintegration
A restart of an operating system process is similar to
the steps taken when a new component is started through
the service utility, although the details differ. Some extra
work needs to be done, mainly because the capabilities
of processes that refer to the restarted component need
to be reset. In general, simple update requests sufﬁce,
but these issues illustrate that interprocess dependencies
complicate the recovery. For example, our design uses
temporarily unique IPC endpoints, so that messages can-
not be delivered to the wrong process during a failure.
As a consequence, a component’s endpoint changes with
each restart, and the IPC capabilities of dependent pro-
cesses must be updated accordingly. Such updates are
done by the reincarnation server before the dependent
components learn about the restart, as discussed next.
Once a component has been restarted, it needs to be
reintegrated into the system. Two steps need to be dis-
tinguished here. First, changes in the operating sys-
tem conﬁguration need to be communicated to depen-
dent components in order to initiate further recovery and
mask the problem to higher levels. This information
is disseminated through the data store, a simple name
server that stores stable component names along with
the component’s current IPC endpoint. The reincarna-
tion server is responsible for keeping this naming infor-
mation up to date. The data store implements a publish-
subscribe mechanism, so that components can subscribe
to naming information of components they depend on.
This design decouples producers and consumers and
prevents intricate interaction patterns of components
that need to inform each other. For example, the network
server subscribes to updates about the conﬁguration of
Ethernet drivers by registering the expression ‘eth.∗’.
Second, a restarted component may need to retrieve
state that is lost when it crashed. The data store also
serves as a database server that can be used by system
processes to privately store a backup copy of certain
data. By using the data store, lost state can be retrieved
after a restart. Authentication of restarted components
is done with help of the naming information that is also
kept in the data store. When private data is stored, a
reference to the stable name is included in the record,
so that authentication of the owner is possible even if
its endpoint changes. In our prototype implementation,
state management turns out to be a minor problem for
device drivers, but is crucial for more complex services.
In fact, none of our device drivers currently uses the data
store to backup internal state. However, all mechanisms
needed to recover from failures in stateful components,
such as servers, are present. Transparent recovery of
servers is part of our future work.
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 20076 DRIVER RECOVERY SCHEMES
Although in principle our design can handle both server
and driver failures, our current focus is to reincar-
nate dead drivers, since such failures are more com-
mon [9, 39].
If a driver failure cannot be handled at
the server level, it will be reported to the application
that made the I/O request, which notiﬁes the user of the
problem, if need be. Different recovery schemes exist
depending on the type of driver, as summarized in Fig. 3.
Full, transparent recovery without bothering the user is
possible for both network and block device drivers.
Driver
Network
Block
Character
Recovery
Yes
Yes
Maybe
Where
Network server
File server
Application
Section
Sec. 6.1
Sec. 6.2
Sec. 6.3
Figure 3: Driver recovery schemes. Only network and block
device drivers allow transparently recovery.
The recovery schemes discussed here pertain not only
to failures, but also allows the administrator to dynami-
cally update drivers—even if I/O is in progress. In this
case, the reincarnation server ﬁrst requests the driver
to exit by sending it a SIGTERM signal, followed by a
SIGKILL signal, if the driver does not comply. The steps
that are taken after the reincarnation server caused the
driver to exit are similar to those for a failure. Most
other operating system cannot dynamically replace ac-
tive drivers on the ﬂy like we do.
6.1 Recovering Network Drivers
If a network driver fails, full recovery transparent to
the application is possible. We have implemented sup-
port for Ethernet driver recovery in MINIX 3’s network
server, INET. If the application uses a reliable transport
protocol, such as TCP, the protocol handles part of the
recovery. If data is lost or corrupted, the network server
(or its peer at the other end of the connection) will notice
and reinsert the missing packets in the data stream. If an
unreliable protocol, such as UDP, is used, loss of data
is explicitly tolerated, but if need be, application-level
recovery is possible, as illustrated in Fig. 4.
The recovery procedure starts when the process man-
ager informs the reincarnation server about the exit of
one of its children, as discussed in Sec. 5.1. The reincar-
nation server looks up the details about the failed driver
in its internal tables and runs the associated policy script
to restart it. Because the network server subscribes to
information about Ethernet drivers, it is automatically
notiﬁed by the data store if the conﬁguration changes. If
the network server tries to send data in the short period
End User
Applications
wget
ftp
rsync
UDP