5
3
w
0
3
w
5
2
w
0
6
w
5
5
w
0
5
w
5
4
w
0
4
w
5
3
w
0
3
w
5
2
w
0
6
w
5
5
w
0
5
w
5
4
w
0
4
w
5
3
w
0
3
w
5
2
(a) Checkpoint time
(b) Wasted time
(c) Total execution time
Fig. 12: Sensitivity study on application-speciﬁc coefﬁcients (parameter A and B). Reduction in time spent in checkpointing,
wasted work, and total execution under different power caps.
n
o
(
c
u
d
e
R
%
50 
40 
30 
20 
10 
0 
5 min 
15 min 
45 min 
w
0
6
w
5
5
w
0
5
w
5
4
w
0
4
w
5
3
w
0
3
w
5
2
n
o
(
c
u
d
e
R
%
0 
‐10 
‐20 
‐30 
‐40 
‐50 
5 min 
15 min 
45 min 
n
o
(
c
u
d
e
R
%
5 min 
15 min 
45 min 
5 
4 
3 
2 
1 
0 
w
0
6
w
5
5
w
0
5
w
5
4
w
0
4
w
5
3
w
0
3
w
5
2
w
0
6
w
5
5
w
0
5
w
5
4
w
0
4
w
5
3
w
0
3
w
5
2
(a) Checkpoint time
(b) Wasted time
(c) Total execution time
Fig. 13: Sensitivity study on time to checkpoint (β). Reduction in time spent in checkpointing, wasted work, and total execution
under different power caps.
Next, we study how improvement obtained by the power
capping aware OCI model changes when the application spe-
ciﬁc parameters (A and B), and platform speciﬁc parameters
(C and D) change. We also study the impact of time to
checkpoint (β) on the improvements. It is followed by eval-
uation for real scientiﬁc applications. We compare our power
capping aware OCI model with prior OCI models, in terms of
execution time, energy consumption, and overall checkpointing
data volume.
Percentage reduction in execution time and energy con-
sumption for checkpointing, wasted, and total between prior
OCI models and our OCI model under various power caps are
shown in Fig. 11. First, we focus on results for Xeon platform
(C = 0.26, D = 38.6). Comparing with prior models, our
model can achieve 9% to 17% reduction in total execution
time, and 42% to 57% reduction in checkpointing time. There
is a minor increase in waste work but it is offset by signiﬁcant
savings in checkpointing time. Percentage changes for total and
checkpointing time increase as power cap decreases. Energy
consumption also follows similar trend. The reason is that the
difference between OCIs derived from prior OCI models and
our power capping aware OCI model is increasing as power
cap decreases, as shown in Fig. 9. We chose Core i7 platform
because it has higher sensitivity to temperature with respect to
power consumption (higher value of C parameter, C = 0.75,
D = 29.1). We ﬁnd that platforms with higher temperature
gradient w.r.t. power capping beneﬁt signiﬁcantly more by
applying power capping aware OCI model (Fig. 11), and their
corresponding OCI is also signiﬁcantly different that can be
obtained from our model. Overall, the power capping aware
OCI also results in reduction in the checkpoint time.
We point out that reducing the number of checkpoints can
relieve the burden on the storage system of an HPC system
which is a shared and constraint resource. Therefore, power
10
CHIMERA 
GYRO 
GTC 
POP 
S3D 
CHIMERA 
VULCUN 
GYRO 
GTC 
POP 
S3D 
CHIMERA 
VULCUN 
GYRO 
GTC 
POP 
n
o
(
c
u
d
e
R
%
60 
50 
40 
30 
20 
10 
0 
n
o
(
c
u
d
e
R
%
0 
‐10 
‐20 
‐30 
‐40 
‐50 
‐60 
n
o
(
c
u
d
e
R
%
20 
15 
10 
5 
0 
S3D 
VULCUN 
)
)
)
)
)
)
)
)
)
B
B
B
B
B
B
B
B
B
P
P
P
P
P
P
P
P
P
(
(
(
(
(
(
(
(
(
e
e
e
e
e
e
e
e
e
m
m
m
m
m
m
m
m
m
u
u
u
u
u
u
u
u
u
o
o
o
o
o
o
o
o
o
V
V
V
V
V
V
V
V
V
a
a
a
a
a
a
a
a
a
l
l
l
l
l
l
l
l
l
t
t
t
t
t
t
t
t
t
a
a
a
a
a
a
a
a
a
D
D
D
D
D
D
D
D
D
w
0
6
w
5
5
w
0
5
w
5
4
w
0
4
w
5
3
w
0
3
w
5
2
w
0
6
w
5
5
w
0
5
w
5
4
w
0
4
w
5
3
w
0
3
w
5
2
w
0
6
w
5
5
w
0
5
w
5
4
w
0
4
w
5
3
w
0
3
w
5
2
+
αt
−
αt
+
αe
−
αe
0
6
0
5
0
4
0
3
0
2
0
1
0
50
60
Power Cap Level (watt)
Power Cap Level (watt)
Power Cap Level (watt)
Power Cap Level (watt)
Power Cap Level (watt)
Power Cap Level (watt)
Power Cap Level (watt)
Power Cap Level (watt)
Power Cap Level (watt)
40
30
(a) Checkpoint time
(b) Wasted time
(c) Total execution time
(d) CHIMERA Checkpoint Volume
Fig. 14: Percentage reduction in time spent in checkpointing, wasted work, and total execution under different power caps for
several leadership applications.
capping aware OCI may in return improve the overall I/O
performance of the whole system and other applications.
Next, we study the sensitivity toward application speciﬁc
parameters (A and B) that represent the impact of power
capping on application performance (Section IV). We per-
form experiments with the applications that have regression
functions in Fig. 1(b) at both extremes (i.e., MG and EP).
Reduction in time spent on checkpointing, wasted, and total
execution is shown in Fig. 12. From the ﬁgure we can
observe that the application speciﬁc parameters do not have
a signiﬁcant impact on the improvements of power capping
aware OCI. This is expected because parameter A and B do not
directly impact the OCI estimation α+
e (as noted earlier
in the modeling section). Also, we note that the OCI is same
for MG and EP. Note that slight difference in the percentage
reduction is because the execution time still depends on the
parameter A and B.
t and α+
Finding 9: When comparing our power capping aware
OCI model with prior OCI models, percentage changes on
checkpointing, wasted, and total execution time are not highly
sensitive to the application-speciﬁc coefﬁcients. Also, we show
that
the platforms with higher temperature gradient w.r.t.
power capping beneﬁt signiﬁcantly more by applying power
capping aware OCI model.
Next, we also perform a sensitivity study on the time-
to-checkpoint. We present experimental results for β equals
to 5, 15, and 45 minutes. Fig. 13 shows that reduction
in checkpointing, wasted, and total execution time increases
when time to checkpoint increases. This reduction is even
more pronounced for lower power caps and shows signiﬁcant
reduction in checkpoint time. This indicates that applying our
model can reduce the checkpointing, and total execution time
more signiﬁcantly when time to checkpoint is larger.
Finding 10: Power capping aware OCI model has in-
creasing gains over prior OCI models as the time to checkpoint
increases. With increasing system/problem scales and relatively
slow growing I/O bandwidth, our model can obtain increasing
beneﬁts in I/O bandwidth constrained systems.
Finally, we perform the evaluations based on the checkpoint
data size and execution time of leadership applications run on
OLCF machines [1], [2], as shown in Table I. Checkpointing
time is the quotient of checkpoint data size divided by average
PFS bandwidth. These leadership applications utilize applica-
tion level checkpointing instead of system level checkpointing.
Their checkpointing time does not necessarily scale up with
problem size, and is user-speciﬁc.
We keep the assumptions in this section except the check-
pointing time obtained from BLCR. We use average 10GB/s
bandwidth as obtained from Spider parallel ﬁlesystem (PFS)
attached to the the Titan supercomputer [32] to calculate
checkpointing time. Percentage changes for checkpointing,
wasted, and total execution time between prior OCI models
and our power-aware OCI model under different power caps
are shown in Fig. 14. We do not show results for reduction
in energy consumption due to space constraint, but energy
consumption follows similar trends as execution time, as
observed in Fig. 11.
As shown in Fig. 13, the checkpoint time has signiﬁcant
impact on the savings achieved by power capping aware OCI.
Similarly, we see that applying our power-aware OCI model
to CHIMERA reduces the total execution time by 9% to 18%
compared to prior OCI models because it has large checkpoint
data size. Applications such as GTC and S3D have moderate
checkpoint data sizes. Total execution time decreases by 4%
and 2% respectively, when applying our power-aware model
to GTC and S3D. For applications with small checkpoint data
sizes, i.e., GYRO, POP, and VULCUN/2D, our power-aware
model has about the same total execution time as prior models.
Finding 11: Using the power capping aware OCI model,
applications with large checkpoint data size can achieve sub-
stantial reduction on checkpointing time and total execution
time over prior OCI models.
Although our OCI model has limited beneﬁts in terms of
total execution time when application checkpoint data size
is small, it can still signiﬁcantly reduce the total volume of
checkpoint data being written to storage systems. For GTC
and S3D, our model can reduce the checkpoint data volume
by up to 40% and 36% respectively. Even for applications
such as GYRO, POP, and VULCUN, our model can still
reduce the checkpoint data volume by up to 33%. This means
that less checkpoints are written to the storage system, which
helps resolving the PFS bottleneck problem, and improving
application and checkpointing I/O performance. We show
checkpointing data volume of CHIMERA for all four OCI
schemes in Fig. 14(d). The power capping aware OCI reduces
the checkpoint volume by 57% for 25 watts power cap level,
compared to the prior models. We also notice that the gap
11
between our power capping aware OCI model and the ﬁrst
order model increases when the power cap decreases.
VIII. CONCLUSION AND FUTURE WORK
In this paper, we investigate the effects of power capping
on the optimal checkpointing interval. We study the effect
of power capping on compute and checkpointing phase for
a variety of scientiﬁc applications. We also demonstrate and
quantify how power capping affects the system reliability due
to change in temperature. We propose an power-aware OCI
model and validation shows that our model can accurately
predict the OCI under power capping. Our evaluation shows
that applying our model to a set of large-scale applications
can save up to 18% energy and execution time. Moreover, our
model reduces the volume of data movement by up to 57% for
these large-scale applications. In the future, we plan to extend
our model to support heterogeneous platforms and to consider
the impacts of manufacturing variations.
REFERENCES
[1] D. Kothe and R. Kendall, “Computational Science Requirements for
Leadership Computing,” Oak Ridge National Laboratory, Tech. Rep.,
2007.
[2] W. Joubert, D. Kothe, and H. A. Nam, “Preparing for Exascale: ORNL
Leadership Computing Facility Application Requirements and Strategy,”
Oak Ridge National Laboratory, Tech. Rep., 2009.
[3]
J. Duell, P. Hargrove, and E. Roman, “The Design and Implementation
of Berkeley Lab’s Linux Checkpoint/Restart,” Berkeley Lab, Tech. Rep.,
2002.
[4] A. Moody, G. Bronevetsky, K. Mohror, and B. R. d. Supinski, “Design,
Modeling, and Evaluation of a Scalable Multi-level Checkpointing
System,” in Proceedings of the ACM/IEEE International Conference
for High Performance Computing, Networking, Storage and Analysis,
2010, pp. 1–11.
[5] L. Bautista-Gomez, S. Tsuboi, D. Komatitsch, F. Cappello,
N. Maruyama, and S. Matsuoka, “FTI: high performance fault tolerance
interface for hybrid systems,” in Proceedings of 2011 International
Conference for High Performance Computing, Networking, Storage
and Analysis, 2011, pp. 32:1–32:12.
[6]
J. Bent, G. Grider, B. Kettering, A. Manzanares, M. McClelland,
A. Torres, and A. Torrez, “Storage challenges at Los Alamos National
Lab,” in IEEE MSST, 2012, pp. 1–5.
[7] F. Cappello, “Fault Tolerance in Petascale/ Exascale Systems: Current
Knowledge, Challenges and Research Opportunities,” International
Journal of High Performance Computing Applications, vol. 23, no. 3,
pp. 212–226, 2009.
[8] D. Tiwari, S. Gupta, and S. S. Vazhkudai, “Lazy Checkpointing:
Exploiting Temporal Locality in Failures to Mitigate Checkpointing
Overheads on Extreme-Scale Systems,” in 44th Annual IEEE/IFIP Int’l
Conference on Dependable Systems and Networks, 2014, pp. 25 – 36.
[9] L. Bautista-Gomez, A. Gainaru, S. Perarnau, D. Tiwari, S. Gupta,
C. Engelmann, F. Cappello, and M. Snir, “Reducing waste in extreme
scale systems through introspective analysis,” 2016.
[10]
[11]
[12]
J. W. Young, “A First Order Approximation to the Optimum Checkpoint
Interval,” Communications of the ACM, vol. 17, no. 9, pp. 530–531,
1974.
J. Daly, “A Model for Predicting the Optimum Checkpoint Interval
for Restart Dumps,” in Proceedings of the International Conference on
Computational Science, 2003, pp. 3–12.
J. Daly, “A higher order estimate of the optimum checkpoint interval
for restart dumps,” Future Generation Computer Systems, vol. 22, no.
2006, pp. 303–312, 2004.
[13] C. Lefurgy, X. Wang, and M. Ware, “Power capping: a prelude to power
shifting,” Cluster Computing, vol. 11, no. 2, pp. 183–195, 2008.
[14] A. Gandhi, M. Harchol-Balter, R. Das, J. O. Kephart, and C. Lefurgy,
“Power Capping Via Forced Idleness,” in Proceedings of Workshop on
Energy-Efﬁcient Design, 2009.
12
[15] M. Dimitrov, C. Strickland, S.-W. Kim, K. Kumar, and K. Doshi,
https://software.intel.com/en-us/articles/
Power Governor,”
“Intel
intel-power-governor, July 2012.
[16]
Intel, Intel 64 and IA-32 Architectures Software Developer’s Manual.
Intel Corporation, 2015, vol. 3B, no. 2.
[17] K. Ma and X. Wang, “PGCapping: Exploiting Power Gating for
Power Capping and Core Lifetime Balancing in CMPs,” in Proceedings
of
the 21st International Conference on Parallel Architectures and
Compilation Techniques, 2012, pp. 13–22.
[18] A. Hussein, A. L. Hosking, M. Payer, and C. A. Vick, “Don’t race the
memory bus: taming the gc leadfoot,” in Proceedings of the 2015 ACM
SIGPLAN International Symposium on Memory Management. ACM,
2015, pp. 15–27.
[19] S. Agarwal, R. Garg, M. S. Gupta, and J. E. Moreira, “Adaptive Incre-
mental Checkpointing for Massively Parallel Systems,” in Proceedings
of the 18th Annual International Conference on Supercomputing, 2004,
pp. 277–286.
[20] K. Ferreira, J. Stearley, J. H. Laros, III, R. Oldﬁeld, K. Pedretti,
R. Brightwell, R. Riesen, P. G. Bridges, and D. Arnold, “Evaluating
the Viability of Process Replication Reliability for Exascale Systems,”
in Proceedings of 2011 International Conference for High Performance
Computing, Networking, Storage and Analysis, 2011, pp. 44:1–44:12.
[21] M. Forshaw, A. S. McGough, and N. Thomas, “Energy-efﬁcient
checkpointing in high-throughput cycle-stealing distributed systems,”
Electronic Notes in Theoretical Computer Science, vol. 310, pp. 65–90,
2015.
[22] S. S. Shende and A. D. Malony, “The Tau Parallel Performance System,”
International Journal of High Performance Computing Applications,
vol. 20, no. 2, pp. 287–311, 2006.
[23] P. J. Mucci, S. Browne, C. Deane, and G. Ho, “PAPI: A Portable Inter-
face to Hardware Performance Counters,” in Proceedings of Department
of Defense HPCMP Users Group Conference, 1999.
[24] T. Patki, D. K. Lowenthal, B. Rountree, M. Schulz, and B. R. de Supin-
ski, “Exploring Hardware Overprovisioning in Power-constrained, High
Performance Computing,” in Proceedings of the 27th International ACM
Conference on International Conference on Supercomputing, 2013, pp.
173–182.
[25] D. Tiwari, S. Gupta, J. Rogers, D. Maxwell, P. Rech, S. Vazhkudai,
D. Oliveira, D. Londo, N. DeBardeleben, P. Navaux et al., “Under-
standing gpu errors on large-scale hpc systems and the implications
for system design and operation,” in High Performance Computer
Architecture (HPCA), 2015 IEEE 21st International Symposium on.
IEEE, 2015, pp. 331–342.
[26] S. Gupta, D. Tiwari, C. Jantzi, J. Rogers, and D. Maxwell, “Understand-
ing and exploiting spatial properties of system failures on extreme-scale
hpc systems,” in Dependable Systems and Networks (DSN), 2015 45th
Annual IEEE/IFIP International Conference on.
IEEE, 2015, pp. 37–
44.
[27] B. Nie, D. Tiwari, S. Gupta, E. Smirni, and J. H. Rogers, “A large-scale
study of soft-errors on gpus in the ﬁeld,” in High Performance Computer
Architecture (HPCA), 2016 IEEE 22nd International Symposium on,
2016.
[28] D. Tiwari, S. Gupta, G. Gallarno, J. Rogers, and D. Maxwell, “Relia-
bility lessons learned from gpu experience with the titan supercomputer
at oak ridge leadership computing facility,” in Proceedings of the In-
ternational Conference for High Performance Computing, Networking,
Storage and Analysis. ACM, 2015, p. 38.
[29] N. El-Sayed, I. A. Stefanovici, G. Amvrosiadis, A. A. Hwang, and
B. Schroeder, “Temperature management in data centers: why some
(might) like it hot,” ACM SIGMETRICS Performance Evaluation Re-
view, vol. 40, no. 1, pp. 163–174, 2012.
[30]
“Arrhenius equation,” https://en.wikipedia.org/wiki/Arrhenius equation.
[31] P. Ellerman, “Calculating Reliability using FIT and MTTF: Arrhenius
HTOL Model,” microsemi, Tech. Rep., 2012.
[32] G. Shipman and et al., “A next-generation parallel ﬁle system en-
vironment for the OLCF,” in Proceedings of the Cray User Group
Conference, 2012.