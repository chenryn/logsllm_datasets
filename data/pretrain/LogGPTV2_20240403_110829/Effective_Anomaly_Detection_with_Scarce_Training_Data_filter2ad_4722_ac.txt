ity. For an in-depth discussion of clustering algorithms
and techniques, we refer the reader to [45].
Central to any clustering algorithm is the distance
function, which deﬁnes how similarities between the ob-
jects to be clustered are calculated. A suitable distance
function must reﬂect the semantics of the objects un-
der consideration, and should satisfy two conditions: 1)
the overall similarity between elements within the same
cluster is maximized, and 2) the similarity between ele-
ments within different clusters is minimized.
We deﬁne the distance between two proﬁles to be
the sum of the distances between the models compris-
ing each proﬁle. More formally, the distance between
the proﬁles ci and cj is deﬁned as:
i
i
1
,m(u)
, m(u)
d (ci, cj) =
j ∈ci! cj
δu/m(u)
|ci- cj| .m(u)
j 0 ,
(3.2)
where δu : Mu ×M u &→ [0, 1] is the distance
function deﬁned between models of type u ∈ U =
{tok, int, len, char, struct}.
is represented as a set
of unique tokens observed during the training phase.
Therefore, two token models m(tok)
are con-
sidered similar if they contain similar sets of tokens. Ac-
cordingly, the distance function for token models is de-
ﬁned as the Jaccard distance [7]:
The token model m(tok)
and m(tok)
j
i
j
0 = 1 −111m(tok)
i - m(tok)
111m(tok)
i ( m(tok)
j
111
111
.
(3.3)
δtok/m(tok)
i
, m(tok)
j
and m(int)
The integer model m(int) is parametrized by the sam-
ple mean µ and variance σ2 of observed integers. Two
integer models m(int)
are similar if these pa-
rameters are also similar. Consequently, the distance
function for integer models is deﬁned as:
i − σ2
i
µ2
j
i
(3.4)
δint/m(int)
i
, m(int)
j 0 = 222 σ2
σ2
i
µ2
i
j222
j
µ2
σ2
j
µ2
j
.
+
cp9,κcp10,κ
cp8,κ
c
CI
κmin
κ = κmin
cp7,κ
cp3,κ
cp6,κ
cp2,κ
cp5,κ
cp4,κ
cp1,κ
CI
4
CI
2
CI
1
cp9,4cp10,4
cp8,4
cp7,4
cp3,4
cp6,4
cp2,4
cp5,4
cp4,4
cp1,4
cp9,2cp10,2
cp8,2
cp7,2
cp3,2
cp6,2
cp2,2
cp5,2
cp4,2
cp1,2
cp9,1cp10,1
cp8,1
cp3,1
cp6,1
cp7,1
cp2,1
cp5,1
cp4,1
cp1,1
(a)
κ = 4
κ = 2
κ = 1
κ = 0
κ = 64
κ = 32
κ = 16
κ = 8
κ = 4
κ = 2
κ = 1
cp9cp10
cp8
cp7
cp3
cp6
cp2
cp5
C
cp4
cp1
κstable  κmin
d
e
n
i
a
r
t
-
l
l
e
W
)
p
(
Q
n
i
s
t
s
e
u
q
e
R
(b)
Figure 6: Global knowledge base indices (a) constructed by (b) sub-sampling the training set Q into small chunks of samples of
progressively larger size (as detailed in Section 3.1). In (b), a dot indicates a sample for a certain parameter, and a full
training set for one parameter corresponds to one horizontal row of dots. For instance, at κ = 2, the proﬁles for the
same parameter are trained several times on 2-sized training subsets. This leads to the proﬁles indicated as small gray
2 and that, in this example, can be grouped into three clusters according to their similarity. As explained
dots in (a) CI
in Section 3.2.1, as κ increases, more accurate clusterings are achieved, and the undertrained knowledge base better
resembles the well-trained one shown on the top-right.
As the length model is internally identical to the inte-
ger model, its distance function δlen is deﬁned similarly.
Recall that the character distribution model m(char)
learns the frequencies of individual characters compris-
ing strings observed during the training phase. These
frequencies are then ranked and coalesced into n bins to
create an ICD. Two character distribution models m(char)
and m(char)
are considered similar if each model’s ICDs
are similar. Therefore, the distance function for charac-
ter distribution models is deﬁned as
j
i
δchar/m(char)
i
, m(char)
j
0 =
n.l=0
)bi (l) − bj (l))
maxk=i,j bk (l)
,
(3.5)
where bi (k) is the value of bin k for m(char)
i
.
The structural model m(struct) builds an HMM by ob-
serving a sequence of character strings. The resulting
HMM encodes a probabilistic grammar that can pro-
duce a superset of the strings observed during the train-
ing phase. The HMM is speciﬁed by the tuple "S,
O, MS×S,P (S, O), P (S)#. Several distance metrics
have been proposed to evaluate the similarity between
HMMs [16,23,36,37]. Their time complexity, however,
is non-negligible. Therefore, we adopt a less precise,
but considerably more efﬁcient, distance metric between
two structural models m(struct)
as the Jac-
card distance between their respective emission sets
and m(struct)
j
i
δstruct/m(struct)
i
, m(struct)
j
0 = 1 − |Oi- Oj|
|Oi( Oj|
.
(3.6)
3.2.2 Constructing a global knowledge base
Once a knowledge base of undertrained models CI has
been built, the next step is to construct a global knowl-
edge base C. This knowledge base is composed of the
individual, well-trained knowledge bases from each web
application as recorded during the ﬁrst phase – that is,
C =(i Cai. Because undertrained proﬁles are built for
each well-trained proﬁle in C, a well-deﬁned mapping
f! : CI &→ C (i.e., independent from the particular un-
dertrained model in Cai, as deﬁned by f) exists between
CI and C. Therefore, when a web application parameter
is identiﬁed as likely to be undertrained, the correspond-
ing undertrained proﬁle c! can be compared to a similar
undertrained proﬁle in CI, that is then used to select a
corresponding stable proﬁle from C.2
3.3 Phase III: Mapping undertrained proﬁles
to well-trained proﬁles
With the construction of a global knowledge base C
and an undertrained knowledge base CI, we can perform
online querying of C. That is, given an undertrained pro-
ﬁle from an anomaly detector deployed over a web ap-
plication ai, the mapping f : )CI* ×C ai &→ C is im-
plemented as follows. A nearest-neighbor match is per-
formed between c! ∈C ai and the previously constructed
clusters H I from CI to discover the most similar clus-
ter of undertrained proﬁles. This is done to avoid a full
scan of the entire knowledge base, which would be pro-
hibitively expensive due to the cardinality of CI.
Then, using the same distance metric deﬁned in
Equation (3.2), a nearest-neighbor match is performed
between c! and the members of H I to discover the un-
dertrained proﬁle cI at minimum distance from c!.
If
multiple nearest-neighbors are found, then one is cho-
sen at random. Finally, the global, well-trained proﬁle
ai.
f!+cI, = c is substituted for c! for the web application
To make explicit how global proﬁles can be used
to address a scarcity of training data, consider the ex-
ample of Figure 4.
Since the resource path /ac-
count/password has received only 100 requests, the
proﬁles for each of its parameters {id, oldpw, newpw}
are undertrained. According to our conﬁdence met-
ric deﬁned in Section 3.1, models that have received
less than 1000 samples are likely undertrained (i.e.,
with a conﬁdence close to zero).
In the absence of
a global knowledge base, the anomaly detector would
provide no protection against attacks manifesting them-
selves in the values passed to any of these parameters.
Also, the system may report too many false positives
If, however, a
due to a lack of model generalization.
2Note that f! is a generalization of f.
global knowledge base and index are available, the sit-
uation is considerably improved. Given C and CI, the
anomaly detector can simply apply f to each of the un-
dertrained parameters to ﬁnd a well-trained proﬁle from
the global knowledge base that accurately models a pa-
rameter with similar semantics, even when the model is
for another web application. Then, these proﬁles can
be substituted for the undertrained proﬁles for each of
{id, oldpw, newpw}. As will be demonstrated in the fol-
lowing section, the substitution of global proﬁles pro-
vides an acceptable detection accuracy for what would
otherwise be a completely unprotected component (i.e.,
without a proﬁle, none of the attacks against that com-
ponent would be detected). If no matching well-trained
proﬁles can be found, then the undertrained proﬁle is
used until the knowledge base is updated at a later time.
This ensures that our approach can be deployed trans-
parently and is adopted only when applicable, incurring
no additional risk or overhead to the existing detection
procedure.
3.4 Mapping quality
The selection of an appropriate value for κ is central
to both the efﬁciency and the accuracy of querying C.
Clearly, it is desirable to minimize κ in order to be able
to index into C as quickly as possible once a parame-
ter has been identiﬁed to be subject to undertraining at
run-time. On the other hand, setting κ too low is prob-
lematic. In fact, as Figure 6a indicates, for low values of
κ, proﬁles are distributed with relatively high uniformity
within CI, such that clusters in CI are signiﬁcantly dif-
ferent than clusters of well-trained proﬁles in C. There-
fore, slight differences in the state of the individual mod-
els can cause proﬁles that are close in CI to map to rad-
ically different proﬁles in C. As κ → κstable, however,
proﬁles tend to form semantically-meaningful clusters,
and tend to approximate those found in C. Therefore, as
κ increases, proﬁles that are close in CI become close
in C according to f – in other words, f becomes robust
with respect to model semantics.3
A principled criterion is required for balancing quick
indexing against a robust proﬁle mapping. To this end,
we ﬁrst construct a candidate knowledge base CI
κ for a
given κ. Additionally, we cluster the proﬁles in C as in
the case of the undertrained knowledge base. Then, we
deﬁne a robustness metric as follows. Recall that H I =
i is the set of clusters in CI, and let H =(i hi be
(i hI
the set of clusters in C. Let g : H I &→ Z+ be a mapping
from an undertrained cluster to the maximum number of
3Our use of the term “robustness” is related, but not necessarily
equivalent, to the deﬁnition of robustness in statistics (i.e., the property
of a model to perform well even in the presence of small changes in
the underlying assumptions.)
elements in that cluster that map to the same cluster in
C. The robustness metric ρ is then deﬁned as
g+hI
i, .
ρ+CI, =
|CI|.i
(3.7)
1