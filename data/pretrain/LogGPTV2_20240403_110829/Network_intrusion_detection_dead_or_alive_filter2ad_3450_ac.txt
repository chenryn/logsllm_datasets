SYSTEMS
Network intrusion detection systems should not be diﬃ-
cult to evaluate: given a traﬃc dump collected during real
or simulated intrusions, a NIDS should be able to detect a
subset of the attacks while producing a certain (hopefully
low) number of false positives. This is not as straightfor-
ward with other types of intrusion detection systems (e.g.,
host-based systems and application-based systems), because
the quantity and quality of information collected about the
actions performed by the OS and its applications can vary
dramatically.
In addition, systems that use an anomaly-
s
victim
fellini NFS
fellini NFS
fellini NFS
fellini NFS
. . .
. . .
fellini NFS
fellini NFS
fellini NFS
fellini NFS
a_v
a4
a4
a4
a4
. . .
a4
a4
a4
a4
a_t
a5
a7
a5
a7
. . .
a5
a7
a5
a7
attacker
Outside
Outside
hitchcock
hitchcock
. . .
lang
lang
carpenter
carpenter
i
i0
i0
i11
i11
. . .
i10
i10
i11
i11
Table 1: Possible scenarios for the UDP spooﬁng attack.
based approach to intrusion detection necessitate training
data, which should be realistic, complete, and attack-free
(note that “realistic and attack-free” could be considered an
oxymoron). This type of data is particularly hard to collect
and/or generate.
Even though the creation of a dataset that can be lever-
aged to compare the performance of intrusion detection sys-
tems is a challenging task, in 1998 and 1999 a group of re-
searchers from the MIT Lincoln Laboratory courageously
embarked in an eﬀort to produce such a dataset, which in-
cluded both training data and test data (with truth ﬁles)
in the form of network packets, OS audit records, and ﬁle
system dumps [6, 7].
These datasets were used to evaluate a number of in-
trusion detection systems being developed by academic re-
searchers. At the beginning of the evaluation process, the
attack-free training data was given to the participants, and,
after a while, the test data containing attacks was distributed
(without truth ﬁles). The participants then had to identify
the attacks and submit their detection alerts, which were
then evaluated with respect to the truth ﬁles.
The results of the evaluation were disclosed only partially,
without declaring a “winner,” and with great care in not
making any single group look bad. Therefore, instead of a
single score, the authors of the evaluation provided a set
of scores that took into consideration various characteris-
tics of the systems involved, creating a no-winner/no-loser
situation. We think that this was a missed opportunity to
foster research by creating a competition with a clear win-
ner, as was later demonstrated by other challenges (e.g., the
DARPA Grand Challenge for unmanned vehicles), which by
having a clear winner motivated the competitors, fostered
innovation and creativity, and provided great publicity for
both the participants and the funding agency.
To determine a winner, a more draconian approach to
evaluation would have been to simply compose the recall
and precision of the intrusion detection systems. More pre-
cisely, in order to evaluate the eﬀectiveness of a system one
could compute the percentage of hits H over the total num-
ber of attacks T , that is, (H/T ) ∗ 100. This is a measure
of how many attacks were actually detected with respect to
the overall set of attacks (i.e., the recall). Then, in order
to characterize how precise the system is, one would com-
pute the percentage of false alarms F over the total number
of detections H + F , that is (H/(H + F )) ∗ 100. For ex-
ample, a system with three detections and no false alarms
would have a precision of 100%, but it would not be very
eﬀective at detecting attacks if the dataset contained hun-
dreds of attack instances. As another example, a system
that ﬂagged every single packet as malicious would have an
eﬀectiveness of 100% because all attacks would be detected,
but it would also have an abysmal precision. Therefore, the
obvious choice is to multiply the two measures above to take
into account these two important aspects of intrusion detec-
tion.
The values of these metrics are shown in Table 2 for the
systems that participated in the 1999 MIT Lincoln Labora-
tory evaluation. According to the proposed metrics, UCSB’s
NetSTAT would be the winner of the 1999 competition,
closely followed by SRI’s EMERALD.
Even though the evaluation failed to declare a clear win-
ner and, in addition, there were some criticisms against the
evaluation process [10], the dataset produced was immensely
popular, and it is without doubt the most used dataset in
the intrusion detection community.
Unfortunately, the MIT/LL dataset and the correspond-
ing truth ﬁles were used in a series of scientiﬁc publications
in which the performance of intrusion detection systems,
evaluated on the non-blind dataset, were compared to the
performance of the intrusion detection systems that partic-
ipated in the blind evaluation, with nefarious and unfair
results. Since then, the dataset has become outdated, and
nowadays it is used very seldom in research publications.
4. THE DEATH (AND REBIRTH) OF INTRU-
SION DETECTION
In the years following the MIT/LL evaluation, there was
an increased skepticism towards network intrusion detection
and its ability to detect attacks, especially 0-day exploits and
mutations of existing attacks [17]. In addition, researchers
started to develop attacks against stateful intrusion detec-
tion system, exposing the challenge of detecting low-traﬃc,
slow-paced attacks that last months (if not years).
In general, there was a shift from the analysis of network
data to the analysis of host data, under the assumption
that only by monitoring the end nodes one could possibly
detect sophisticated attacks. Therefore, during the early
2000s, academia started losing interest in network intrusion
detection, while, at the same time, the use of commercial
network intrusion detection systems became an established
best-practice in enterprise networks. This happened some-
times in disguise, for example by relabeling NIDS as “in-
trusion prevention systems” to describe network intrusion
detection systems with traﬃc-blocking responses.
Around 2003-2004 it looked like research on the “classic”
GMU
NYU
Hits
False Positives
H/T
H/H + F
H/T ∗ H/H + F
43
16
21.3
72.9
15.5
21
74
10.4
22.1
2.3
RST
Elman
Network
37
5351
18.3
0.7
0.1
RST
State
Tester
26
429
12.9
5.7
0.7
RST
String
Transd.
26
117
12.9
18.2
2.3
SRI
Derbi
SRI
Estat
SRI
EMERALD
SunySB UCSB
STAT
17
48
8.4
26.2
2.2
29
96
14.4
23.2
3.3
94
13
46.5
87.9
40.9
7
2
3.5
77.8
2.7
88
4
43.6
95.7
41.7
Table 2: Hits, false positives (in absolute values), recall, precision, and composition of recall and precision (in
percentages) for the systems involved in the MIT Lincoln Laboratory 1999 IDS evaluation, which contained
202 attacks.
network intrusion detection problem (i.e., detecting attacks
by looking at network packets) was dwindling fast. How-
ever, at the same time, the techniques used to characterize
network attacks were applied to the detection of malicious
code components, such as worms and bots. Both misuse-
based and anomaly-based techniques were readily leveraged
to identify malware of various kinds.
In a way, these re-
search eﬀorts resulted in “intrusion detection” system that
were closer to the meaning of the term than the early NIDSs.
In fact, while the early systems focused mostly on detecting
attacks, these new systems focused on detecting the actual
intrusions by identifying malicious behavior that would in-
dicate that a system had been compromised.
This “born-again” network intrusion detection research is
characterized by the heavy use of data-mining and machine-
learning techniques to address one of the main problems
associated with misuse-based NIDS, which is the need for
the manual speciﬁcation of attack models (note that some
of the seminal work in this ﬁeld was performed in the late
90’s [5]).
5. CONCLUSIONS
Even though the term “Intrusion Detection” sometimes
is looked-down upon by the academic community, intrusion
detection research will always be a core part of the security
ﬁeld. It might be the case that the focus of intrusion de-
tection will move towards more semantically-rich domains,
such as the OS and the web. For example, web-based intru-
sion detection systems (normally referred to as “Web Appli-
cation Firewalls”, for marketing purposes) leverage knowl-
edge about the characteristics of web applications and their
logic, in order to identify attacks. Nonetheless, these sys-
tems mostly use concepts that were researched and applied
more than two decades ago.
This “re-invention” of network intrusion detection tech-
niques and approaches shows how intrusion detection (be
it network-based, web-based, or host-based) is still an im-
portant research problem. As new attacks and new ways
of compromising systems are introduced, both researchers
and practitioners will develop (or re-discover) techniques for
the analysis of events that allow for the identiﬁcation of the
manifestation of malicious activity.
The next challenge will be to expand the scope of intrusion
detection to take into account the surrounding context, in
terms of abstract and diﬃcult-to-deﬁne concepts, such as
missions, tasks, and stakeholders, when analyzing data in
an eﬀort to identify malicious intent.
6. REFERENCES
[1] C. Berge. Hypergraphs. North-Holland, 1989.
[2] S. Eckmann, G. Vigna, and R. Kemmerer. STATL: An
Attack Language for State-based Intrusion Detection.
Journal of Computer Security, 10(1,2):71–104, 2002.
[3] L. Heberlein, G. Dias, K. Levitt, B. Mukherjee,
J. Wood, and D. Wolber. A Network Security
Monitor. In Proceedings of the IEEE Symposium on
Research in Security and Privacy, pages 296 – 304,
Oakland, CA, May 1990.
[4] K. Ilgun, R. Kemmerer, and P. Porras. State
Transition Analysis: A Rule-Based Intrusion
Detection System. IEEE Transactions on Software
Engineering, 21(3):181–199, March 1995.
[5] W. Lee and S. Stolfo. Data Mining Approaches for
Intrusion Detection. In Proceedings of the USENIX
Security Symposium, San Antonio, TX, January 1998.
[6] R. Lippmann, D. Fried, I. Graf, J. Haines, K. Kendall,
D. McClung, D. Weber, S. Webster, D. Wyschogrod,
R. Cunningham, and M. Zissman. Evaluating
Intrustion Detection Systems: The 1998 DARPA
Oﬀ-line Intrusion Detection Evaluation. In Proceedings
of the DARPA Information Survivability Conference
and Exposition, Volume 2, Hilton Head, SC, January
2000.
[7] R. Lippmann and J. Haines. Analysis and Results of
the 1999 DARPA Oﬀ-Line Intrusion Detection
Evaluation. In Proceedings of the Symposium on the
Recent Advances in Intrusion Detection (RAID),
pages 162–182, Toulouse, France, 2000.
[8] S. McCanne and V. Jacobson. The BSD Packet Filter:
A New Architecture for User-level Packet Capture. In
Proceedings of the 1993 Winter USENIX Conference,
San Diego, CA, January 1993.
[9] S. McCanne, C. Leres, and V. Jacobson. Tcpdump
3.7. Documentation, 2002.
[10] J. McHugh. Testing Intrusion Detection Systems: A
Critique of the 1998 and 1999 DARPA Intrusion
Detection System Evaluations as Performed by
Lincoln Laboratory. ACM Transaction on Information
and System Security, 3(4), November 2000.
[11] V. Paxson. Bro: A System for Detecting Network
Intruders in Real-Time. In Proceedings of the 7th
USENIX Security Symposium, San Antonio, TX,
January 1998.
[12] P. Porras. STAT – A State Transition Analysis Tool
for Intrusion Detection. Master’s thesis, Computer
Science Department, University of California, Santa
Barbara, June 1992.
[13] P. Porras and P. Neumann. EMERALD: Event
Monitoring Enabling Responses to Anomalous Live
Disturbances. In Proceedings of the 1997 National
Information Systems Security Conference, October
1997.
[14] M. Roesch. Snort - Lightweight Intrusion Detection
for Networks. In Proceedings of the USENIX LISA ’99
Conference, Seattle, WA, November 1999.
[15] G. Vigna. A Topological Characterization of TCP/IP
Security. In Proceedings of the 12th International
Symposium of Formal Methods Europe (FME ’03),
number 2805 in LNCS, pages 914–940, Pisa, Italy,
September 2003. Springer-Verlag.
[16] G. Vigna and R. Kemmerer. NetSTAT: A
Network-based Intrusion Detection Approach. In
Proceedings of the 14th Annual Computer Security
Applications Conference (ACSAC ’98), pages 25–34,
Scottsdale, AZ, December 1998. IEEE Press.
[17] G. Vigna, W. Robertson, and D. Balzarotti. Testing
Network-based Intrusion Detection Signatures Using
Mutant Exploits. In Proceedings of the ACM
Conference on Computer and Communication Security
(ACM CCS), pages 21–30, Washington, DC, October
2004.