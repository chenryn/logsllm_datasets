title:Fig: Automatic Fingerprint Generation
author:Shobha Venkataraman and
Juan Caballero and
Pongsin Poosankam and
Min Gyung Kang and
Dawn Xiaodong Song
FiG: Automatic Fingerprint Generation
Juan Caballero
Shobha Venkataraman
Pongsin Poosankam
Carnegie Mellon University
Carnegie Mellon University
Carnegie Mellon University
PI:EMAIL
Min Gyung Kang
PI:EMAIL
PI:EMAIL
Dawn Song
Avrim Blum
Carnegie Mellon University
Carnegie Mellon University
Carnegie Mellon University
PI:EMAIL
PI:EMAIL
PI:EMAIL
Abstract
Fingerprinting is a widely used technique among the net-
working and security communities for identifying different
implementations of the same piece of networking software
running on a remote host. A ﬁngerprint is essentially a set of
queries and a classiﬁcation function that can be applied on
the responses to the queries in order to classify the software
into classes. So far, identifying ﬁngerprints remains largely
an arduous and manual process. This paper proposes a
novel approach for automatic ﬁngerprint generation, that
automatically explores a set of candidate queries and ap-
plies machine learning techniques to identify the set of valid
queries and to learn an adequate classiﬁcation function.
Our results show that such an automatic process can gener-
ate accurate ﬁngerprints that classify each piece of software
into its proper class and that the search space for query ex-
ploration remains largely unexploited, with many new such
queries awaiting discovery. With a preliminary exploration,
we are able to identify new queries not previously used for
ﬁngerprinting.
1. Introduction
Fingerprinting is a technique for identifying the differ-
ences among implementations of the same networking soft-
ware speciﬁcation, be it applications, operating systems or
TCP/IP stacks. It is well-known that even when the func-
tionality of a piece of software is detailed in a speciﬁcation
or standard, different implementations of that same func-
tionality tend to differ in the interpretation of the speciﬁca-
tion, by making assumptions or implementing only part of
the optional functionality.
In network security, ﬁngerprinting has been used for
more than a decade [15] and it has a variety of applica-
tions. Some ﬁngerprinting tools such as Nmap [9] are used
to identify hosts running a speciﬁc operating system. There
are also tools that can be used to identify different versions
of the same application such as fpdns [2], Nmap, and Nes-
sus [8]. These tools help network administrators to ﬁnd ver-
sion information leaked by a system, inventory the hosts in
a network, and check for the existence of hosts running ver-
sions with vulnerabilities, or versions that are not allowed
under the security policy of a network.
However, identifying the ﬁngerprints used by these tools,
the ﬁngerprint generation, is currently a manual process
which is arduous, incomplete, and makes it difﬁcult to
keep up-to-date with the numerous new implementations
and new version updates. In this paper we propose a novel
approach for automatic ﬁngerprint generation. The goal is
to automatically produce ﬁngerprints that can differentiate
among distinct implementations of the same speciﬁcation
and can be used by different ﬁngerprinting tools.
A ﬁngerprint contains 1) a set of queries, and 2) a classi-
ﬁcation function. To use the ﬁngerprint to identify the class
to which a piece of software belongs, one sends the queries,
collects the responses and uses the classiﬁcation function to
classify the response. In the remainder of this paper, we will
use the term classifying a host when we refer to classifying
a piece of networking software running on the host. Thus,
for ease of description, we assume that there is only one rel-
evant piece of networking software running on the host; we
can easily remove this assumption by classifying each piece
of networking software on the host separately.
In this paper, we demonstrate how to automatically iden-
tify useful queries and classiﬁcation functions. Our ﬁn-
gerprint generation process contains three phases: First, a
Candidate Query Exploration phase which outputs candi-
date queries. Second, a Learning phase where those can-
didate queries are sent to different implementations and the
responses are gathered and passed to the learning algorithm.
The Learning phase outputs the query set, which is a subset
of the set of candidate queries that includes only the can-
didate queries that are useful for ﬁngerprinting, and a clas-
siﬁcation function. The pair formed by the query set and
the classiﬁcation function is the ﬁngerprint. Third, a Test-
ing phase where the produced ﬁngerprints are tested over a
larger number of different implementations to evaluate its
accuracy.
We also study what to do when a host does not match any
known ﬁngerprint. One straightforward approach would be
to classify the host as unknown, but one could also perform
an approximate matching to the nearest known class. In this
paper, we set out to validate if and when this approximate
matching can be performed.
This paper presents a general methodology for automatic
ﬁngerprint generation that covers all three phases and gener-
ates ﬁngerprints that can be used by different ﬁngerprinting
tools. We validate our methodology by applying it on two
distinct problems: 1) generating ﬁngerprints to differentiate
operating systems and 2) generating ﬁngerprints to differen-
tiate implementations of the same application, namely DNS
servers.
Our experimental results show that the ﬁngerprints auto-
matically generated by our approach are accurate. With a
preliminary exploration of the search space, we are able to
ﬁnd many novel ﬁngerprints that are not currently used by
ﬁngerprinting tools. These novel ﬁngerprints can increase
the accuracy and granularity of current ﬁngerprinting tools.
The rest of this paper is structured as follows: Section 2
introduces the automatic ﬁngerprint generation problem. In
Section 3 we present the candidate query exploration phase.
The learning algorithms are explained in Section 4 and in
Section 5 we evaluate their performance. Section 6 presents
the related work and we conclude in Section 7.
2. Overview
Problem deﬁnition Given a set of k implementation classes
I = {I1, I2, . . . , Ik}, the goal of ﬁngerprinting is to classify
a host H into one of those k classes or to another unknown
class. The problem of automatic ﬁngerprint generation is
to output a set of queries Q and a classiﬁcation function fQ,
such that when we send the set of queries Q to a host H and
collect the responses RQ from H, fQ(RQ) can classify the
host into one of the k classes or into the unknown class. We
refer to the pair hQ, fQi as the ﬁngerprint.
Approach Our automatic ﬁngerprint generation contains 3
steps as shown in Figure 1, namely the Candidate Query Ex-
ploration phase, the Learning phase and the Testing phase.
First, in the Candidate Query Exploration phase we produce
a set of candidate queries Qc, which could potentially pro-
duce different responses from hosts belonging to different
classes. This process takes as input the protocol semantics
in the form of protocol standards and domain knowledge.
Then, given a set of training hosts T where the imple-
mentation class of every host in T is known and is in the set
I, the ﬁrst step in the Learning phase is to send the set of
candidate queries Qc to each of the hosts in T using a packet
injection tool and to gather the responses from each host.
The responses and the classes are then passed to the learn-
ing algorithm, which identiﬁes a set of queries Q ⊆ Qc,
that produces different responses from hosts belonging to
different classes, and a classiﬁcation function fQ.
Finally, in the Testing phase we need to verify that the
pair hQ, fQi generated in the Learning phase has sufﬁcient
accuracy. This phase uses a set of testing hosts E, where
none of the hosts in E should belong to the training set
T , and each of the hosts in E should belong to one of the
classes in I. Then, this classiﬁcation is compared to the
known classiﬁcation for E and if the accuracy satisﬁes some
predeﬁned metrics, the pair hQ, fQi is considered valid and
becomes the output ﬁngerprint. The ﬁngerprints obtained
can now be used by any ﬁngerprinting tool to classify un-
known hosts by sending the query set Q to this host, and
applying the classiﬁcation function fQ on the responses.
Besides automatically generating the ﬁngerprints needed
by the ﬁngerprinting tools, we also want to validate if it is
possible for a ﬁngerprinting tool to perform approximate
matching when the responses from a host cannot be clas-
siﬁed into any known class. Approximate matching could
then potentially be used to distinguish between hosts that
do indeed belong to a class not yet seen and thus should be
classiﬁed as unknown, and hosts that have a slightly dif-
ferent behavior (e.g.
some networking parameters man-
ually tweaked) but actually belong to one of the known
classes. For approximate matching to be meaningful, we
need to test if the different implementation classes are well-
separated. We do this by clustering, and calculating the dis-
tance between implementation classes. If the implementa-
tion classes are well-separated, we can, under some natural
assumptions, ﬁnd an approximate match for a given new
host. Thus, only if the new host is far from all the known
classes do we classify it as unknown.
In Section 3 we describe how to explore the candidate
query space, and in Section 4 we present the learning algo-
rithms, and how they are used to ﬁnd the ﬁngerprints.
3. Candidate Query Exploration
In this section we describe the Candidate Query Explo-
ration phase. This phase needs to select a set of candidate
Protocol(cid:13)
Semantics(cid:13)
Candidate(cid:13)
Query(cid:13)
Exploration(cid:13)
Candidate(cid:13)
Queries(cid:13)
(Q(cid:13) c(cid:13))(cid:13)
Learning(cid:13)
Query(cid:13)
Injection(cid:13)
R(cid:13) C(cid:13)
Learning(cid:13)
Algorithm(cid:13)
Q,f(cid:13) Q(cid:13)
Testing(cid:13)
Q, f(cid:13) Q(cid:13)
Accuracy(cid:13)
Satisfied?(cid:13)
Yes(cid:13)
Training(cid:13)
hosts(cid:13)
(T)(cid:13)
Classes(cid:13)
(I)(cid:13)
Testing(cid:13)
hosts(cid:13)
(E)(cid:13)
Figure 1. Fingerprint generation process.
queries that can potentially produce distinct responses from
the different classes of hosts. During the Learning phase,
those candidate queries that indeed produce distinguishing
responses will be selected as part of the query set in the ﬁnal
ﬁngerprint.
To automate the process of query exploration we could
perform an exhaustive search of all possible combinations.
Besides being automatic, it is also complete, guaranteeing
that it will ﬁnd all useful queries. However, the problem
with this approach is that the search space is very large.
For DNS ﬁngerprinting we have at least 16 bytes of header
ﬁelds in a DNS query, requiring 2128 combinations. For OS
ﬁngerprinting, the numbers become even more intractable.
Even if we restrict the search to the TCP and IP headers
there are at least 40 bytes of header information, which re-
quire 2320 combinations.
Such exhaustive search does not take advantage of the
semantics of the protocol, i.e. the ﬁeld structure in the pro-
tocol headers, and it can generate a large number of queries
that are useless for ﬁngerprinting, thus wasting both time
and resources. For example, a query that spoofs the IP
source address becomes useless since the reply will never
make it back to the sender.
We combine exhaustive search with the semantics of the
protocol by selecting some ﬁelds with rich semantics (such
as the TCP or DNS ﬂags) and performing an exhaustive
search on those, while limiting the search to selected val-
ues for other less interesting ﬁelds. This greatly reduces
the search space and requires little human intervention. It
also reduces the time and resources needed to complete the
search and our results show that it is still possible to ﬁnd
many useful queries not yet used for ﬁngerprinting.
In Section 5.1 we present the speciﬁc TCP/IP and DNS
ﬁelds that we explore. One example of how combining se-
mantic and exhaustive search reduces the search space is
byte 12 of the TCP header [25] which contains the Data Off-
set (4 bits) and Reserved ﬁelds (4 bits). Assuming that the
ﬁelds are independent, rather than searching the 28 combi-
nations in the whole byte, we can ﬁx the value of one ﬁeld at
a time, while performing an exhaustive search on the other.
This would require a total of 24+24 = 32 candidate queries.
4. Learning Algorithms
We formulate the ﬁngerprint generation problem as a
classiﬁcation problem: we are given a set of instances from
different classes, along with the labels of the classes they
belong to, and we need to ﬁnd properties that hold within
a class and are different across classes. An instance is rep-
resented as a collection of values for a set of features; thus,
an instance is a point in the feature space. Given a family
of classiﬁcation functions over these features, our goal is
to ﬁnd a good function within this family that separates the
classes. In our case the classes are the different implemen-
tations of the same functionality and our goal is to output
the ﬁngerprint composed by the set of queries and the clas-
siﬁcation function.
Thus, we need to deﬁne the feature space and the family
of classiﬁcation functions, and then learn the classiﬁcation
function, and turn it into our ﬁnal ﬁngerprint. We describe
each of these in turn in this section. First, in Section 4.1 we
describe the feature space and how to obtain the instances,
needed by the learning algorithms, from the query/response
pairs obtained from the training hosts. Then, in Section 4.2
we introduce the classes of functions we use for classiﬁca-
tion and brieﬂy describe the learning algorithms. In Sec-
tion 4.3 we describe how to take the output of the learning
algorithms and convert it into our ﬁnal ﬁngerprints. Finally,
in Section 4.4 we describe how to obtain an approximate
matching when there is no exact matching among the ﬁn-
gerprints.
4.1. Feature Extraction
The ﬁrst step for using a learning algorithm is to de-
ﬁne the feature space, which allows us to convert the
query/response pairs into the input for the learning algo-
rithms. Thus, an instance is the representation of the
query/response pairs from a host in the feature space; and
the input to the learning algorithms is a set of instances.
Our feature space For simplicity, we describe our fea-
ture space for the case when the following relationships
hold between queries and responses: (a) we consider only
a single response byte string for each query, and (b) we
consider each response to be independent of every other
query/response from that host. With these two restrictions,
it is sufﬁcient to analyze only responses that come from dif-
ferent hosts to the same query. Therefore, in this section,
when we discuss a feature space, we refer to the feature
space with respect to a single query. We show later in this
subsection how to generalize this feature space.
For a given query, we focus on the position-dependent
substrings of responses, which we call position-substrings.
In particular, we aim to extract the position-dependent byte
substrings, that are consistently present in and distinctive to
the responses of an implementation class (a similar analysis
could also be done at bit level). Features involving position-
substrings allow us to exploit the underlying structure of the
byte sequences, since we are analyzing network protocols
that usually have a well deﬁned ﬁeld structure.
Speciﬁcally, a position-substring of a response string is
a tuple of three elements: the start and end positions in the
original string and the bytes between these positions. So,
for example, if the original byte string is abcdefgabcd, then
we have two distinct position-substrings for the byte string
abcd:
the position-substring [1, 4, abcd] is distinct from
the position-substring [8, 11, abcd]. A position-substring is
present in a response if it appears at the proper position in
the response. So, for example, the response string abcdef g
does not contain the position-substring [5, 7, abc], but the
response string abcdabc does.
With this deﬁnition of position-substrings, we can now
describe the feature sets: for each response, the set of fea-
tures extracted is the set of all possible position-substrings
in the response. For a collection of responses from differ-
ent hosts to the same query, the corresponding feature set is
the union of all the features for each response string. The
feature space of a query is the feature set of the response
strings from all the different hosts to this query. In this sec-
tion, and in Section 4.2, we will consider each query sepa-
rately. Later in this section, we illustrate the feature space
with an example, and show how individual response strings
are represented in the space.
In this feature space, all of the information contained
in the response string gets encoded into the features; there
is no loss of information when transforming the response
string into the feature set. Such a property is good, be-
cause the learning algorithm can then decide which in-
formation is useful for classiﬁcation. However, there are
other feature spaces which have this property. We choose
the position-substring feature space because, in combina-
tion with some simple families of classiﬁcation functions,
it provides meaningful ﬁngerprints that are easy to inter-
pret. The classiﬁcation function we might need to learn over
other feature spaces may need to be more complex and thus,
harder to interpret.
Generalizing the feature space This feature space could be
generalized in many ways. To begin with, we could general-
ize both the cases mentioned above easily. If we assume that
each response depends on the last k query-response pairs of
the same host (rather than being independent of the other
query-response pairs), we can transform it into our simpler
case by concatenating every k responses from each host. If
a particular query gets multiple responses from the same
host, we can concatenate all responses together to reduce it
to the case where there is just one response.
We could also extract more complex relationships be-
tween the various regions within a single byte sequence;
e.g., we could examine if a certain position-dependent sub-
string is always the sum of another position-dependent sub-
string in the same byte sequence. The study of these more
complex relationships within a sequence, and between dif-
ferent byte sequences is left as future work.
Generating instances Given a query, for each training host,
we extract all the possible position-substrings from the re-
sponse of that host. Then, we create the union set U from
the position-substrings of all the responses from different
training hosts to the same query, ordered lexicographically.
Finally, we transform each response string into a vector
v ∈ {0, 1}n by setting the i-th bit of v to be one if the i-
th position-substring of U is present in the response string,
and we call this vector an instance.
Figure 2 shows this process for the responses from two
different hosts to the same query. The responses from
the hosts are the strings ac and ax. The set of position-
substrings P1 obtained from ac is [1, 1, a], [2, 2, c], [1, 2, ac],
and the set of position-substrings P2 obtained from ax
is [1, 1, a], [2, 2, x], [1, 2, ax]. Taking the union of these
two sets P1 and P2,
in lexicographical order, we get
U = [1, 1, a], [2, 2, c], [2, 2, x], [1, 2, ac], [1, 2, ax]. With
this U , the instance corresponding to response ac becomes
(1, 1, 0, 1, 0), since only the ﬁrst, second and fourth element
of U are present in the P1. Likewise, the instance corre-
sponding to response ax becomes (1, 0, 1, 0, 1).
Optimizations We may have very large sets of features if