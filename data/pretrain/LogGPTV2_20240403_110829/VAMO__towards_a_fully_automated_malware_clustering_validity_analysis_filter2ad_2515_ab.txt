lem is exacerbated by the fact that even in the cases in which three
or more labels are available, the AVs may not agree on the family
those samples belong to, as we discuss in Section 3.1.2
3.1.2 Family Labels
We now focus on malware family names, rather than considering
full AV labels. We will consider the malware cluster Clust. 1 shown
below as an example, to explain how we derive the malware family
names. This malware cluster was obtained using [15]. In Clust. 1,
each row represents a malware sample (indexed by the last four
bytes of its MD5 sum), and reports the labels assigned to the sample
by three different AVs, namely McAfee (M), Avira (A), and Trend
Micro (T).
Clust. 1 Malware cluster with inconsistent AV labels.
b1b6da81
ec34ca31
c2276216
089ae4f5
8ba552c9
8cb0ab6c
b0b75f70
a306b4e7
337a2cf4
62d18c7e
8dbca633
ac433383
cae61d9e
7cc795f1
8de5214b
4d26cb0a
9fb75631
229004b9
28a85d8a
663c5f6c
de6f1e00
1ff43bca
ea580f6d
a844eeff
4f8613fd
M=W32/Virut.gen
M=W32/Virut.gen
M=W32/Virut.gen
M=W32/Virut.gen
M=W32/Virut.gen
M=W32/Virut.gen
M=W32/Virut.gen
M=W32/Virut.gen
M=W32/Virut.gen
M=W32/Virut.gen
M=W32/Virut.gen
M=W32/Virut.n
M=W32/Virut.gen
M=W32/Virut.gen
M=W32/Virut.gen.a
M=W32/Virut.gen
M=W32/Virut.n
M=W32/Virut.gen
M=W32/Virut.gen
M=W32/Virut.d
M=
M=
M=W32/Virut.n
M=W32/Virut.gen
M=W32/Virut.gen
A=TR/Drop.VB.DU.1
A=TR/Drop.VB.DU.1
A=W32/Virut.E.dam
A=W32/Virut.AX
A=TR/Drop.VB.DU.1
A=WORM/Korgo.U
A=W32/Virut.X
A=W32/Virut.Gen
A=W32/Virut.Gen
A=W32/Virut.Gen
A=TR/Drop.VB.DU.1
A=W32/Virut.Gen
A=W32/Virut.X
A=W32/Virut.Gen
A=W32/Virut.AM
A=W32/Virut.Gen
A=W32/Virut.Gen
A=W32/Virut.X
A=TR/Drop.VB.DU.1
A=W32/Virut.Z
A=W32/Virut.Gen
A=W32/Virut.X
A=W32/Virut.Gen
A=TR/Drop.VB.DU.1
A=TR/Drop.VB.DU.1
T=PE_VIRUT.XO-1
T=PE_VIRUT.XO-1
T=PE_VIRUT.NS-4
T=PE_VIRUT.D-1
T=PE_VIRUT.XO-1
T=PE_VIRUT.D-4
T=PE_VIRUT.XO-1
T=PE_VIRUT.D-1
T=PE_VIRUT.D-1
T=PE_VIRUT.D-1
T=PE_VIRUT.XO-1
T=PE_VIRUX.A-3
T=PE_VIRUT.XO-2
T=PE_VIRUT.D-1
T=PE_VIRUT.XY
T=PE_VIRUT.D-1
T=PE_VIRUX.A-3
T=PE_VIRUT.XO-1
T=PE_VIRUT.XO-1
T=PE_VIRUT.GEN-2
T=PE_VIRUT.D-4
T=PE_VIRUT.XO-4
T=PE_VIRUX.A-3
T=PE_VIRUT.XO-1
T=PE_VIRUT.XO-1
To derive the malware family name, we split each label into sub-
strings divided by the ‘.’ symbol, and we extract the ﬁrst substring.
For example, W32/Virut.gen becomes W32/Virut (Syman-
tec uses a slightly different notation, compared to the other AV
vendors. To extract the family label from Symantec’s labels, we
consider the ﬁrst two substrings obtained by splitting the labels by
the ‘.’ symbol. For example, W32.Sality.AE would become
W32.Sality).
As we can see from Clust. 1, in this case both McAfee and Trend
Micro are very consistent, because they label the vast majority of
the samples as belonging to the Virut malware family, with the ex-
ception of two samples that were missed by McAfee and three sam-
ples that are labeled as PE_VIRUX (rather than PE_VIRUT) by
Trend Micro. On the other hand, Avira is much less consistent, be-
cause it assigned three different family names to the samples (i.e.,
TR/Drop, W32/Virut, WORM/Korgo).
Table 1 reports the total number, per AV, of distinct family names
obtained from all labels in our datasets. Also, Table 1 reports the
total number, per AV, of distinct “ﬁrst variant” labels, i.e., labels
obtained by combining the ﬁrst two label substrings (the ﬁrst three,
in case of Symantec). Again, there is a relatively large difference
between the numbers obtained from different AVs.
To measure the number of common family names per sample
across different AVs, we further normalized the family names, for
example by cutting the label preﬁx (e.g., W32/, PE_, etc.) and
reducing all labels to lower case. For example, the ﬁrst sample in
Clust. 1 would be labeled as {virut, drop, virut}. This was
done to maximize the number of common family names we could
ﬁnd for a given sample across different AVs. Even after this nor-
malization, we could ﬁnd a common family label across at least
three out of four AVs for only 2.4% of the samples, and a common
label across at least two out of four AVs for only 5.6% of the sam-
ples. Performing a manual mapping between the labels to mitigate
the effect of different “terminology” used by different AVs may
improve on these results. However, even after such manual map-
ping a majority voting-based consensus between the AVs cannot be
reached for the vast majority of the samples. This ﬁndings are con-
sistent with the experiments conducted in [2], in which a majority
voting-based consensus could be reached only for less than 20% of
the samples. Therefore, a reference clustering generated via ma-
jority voting may miss to represent a large portion of the malware
dataset, causing a potential overestimate of the clustering quality,
as also suggested in [12].
3.2 Validity Indexes
Clustering can be viewed as an unsupervised learning process
over a dataset for which the complete ground truth is usually not
available. Therefore, unlike in supervised learning settings, ana-
lyzing the validity of the clustering results is intrinsically hard. The
assessment of the quality of clustering results often involves the use
of subjective criteria of optimality [10], which are typically applica-
tion speciﬁc, and commonly involves extensive manual analysis by
domain experts. To aid the clustering validation process, a number
of methods and quality indexes have been proposed [7, 9]. Halkidi
et al. [7] provide a survey of cluster validity analysis techniques,
which aim to evaluate the clustering results to ﬁnd the partitioning
that best ﬁts the underlying data.
Three main cluster validity approaches are described [7]: (1) ex-
ternal criteria evaluate the clustering results by comparing them to
a pre-speciﬁed structure, or reference clustering; (2) internal cri-
teria rely solely on quantities derived from the data vectors in the
clustered dataset (e.g., using a proximity matrix, and computing
quantities such as inter- and intra-cluster distances); (3) relative
criteria compare clustering results obtained using the same cluster-
ing algorithm with different parameter settings, to identify the best
parameter conﬁguration.
External validation criteria are particular attractive, because they
offer a quantitative way to measure the level of agreement between
the obtained clustering results and a reference clustering that is con-
sidered to be the ground truth [7, 16]. However, the main problem is
exactly how to construct the reference clustering in the ﬁrst place.
This is one of the problems we address in this paper: building a
reference clustering that can be used for validating the results of
malware clustering systems.
Assuming a reference clustering is available, different external
validity indexes can be used for measuring the quality of the clus-
tering results. We brieﬂy describe some of them below. Let M be
our dataset, Rc = {Rc1, .., Rcs} be the set of s reference clusters,
and C = {C1, .., Cn} be our clustering results over M. Given a
pair of data samples (m1, m2), with m1, m2 ∈M , we can com-
pute the following quantities:
• a is the number of pairs (m1, m2) for which if both samples
belong to the same reference cluster Rci, they also belong to
the same cluster Cj.
• b is the number of pairs (m1, m2) for which both samples
belong to the same reference cluster Rci, but are assigned to
two different clusters Ck and Ch.
• c is the number of pairs (m1, m2) for which both samples
belong to the same cluster Ci, but are assigned to two differ-
ent reference clusters Rck and Rch.
• d is the number of pairs (m1, m2) for which if the samples
belong to two different reference clusters Rci and Rcj, they
also belong to different clusters Cl and Cm.
Based on the above deﬁnitions, we can compute the following
external cluster validity indexes [7]:
• Rand Statistic. RS = a+d
• Jaccard Coefﬁcient. JC = a
• Folkes and Mallows Index. F M =
a+b+c
a+b+c+d = a+d
|M|
a√(a+b)(a+c)
For all three indexes above, which take values in [0, 1], higher val-
ues indicate a closer similarity between the clustering C and the
reference clustering Rc.
The authors of [2, 18], proposed to use different indexes, based
on precision and recall, to measure the level of agreement between
behavior-based malware clustering results C and a (semi-)manually
generated reference clustering Rc derived by using majority voting
over multiple AV labels. In this setting, precision and recall, and
the related F 1 index, are deﬁned as follows:
j=1 maxk=1,..,s(|Cj ∩ Rck|)
• Precision. P rec = 1/n ·n
• Recall. Rec = 1/s ·s
• F1 Index. F 1 = 2 P rec·Rrec
In the remainder of the paper, we will often refer to the external
k=1 maxj=1,..,n(|Cj ∩ Rck|)
P rec+Rrec
validity indexes deﬁned above.
4. SYSTEM OVERVIEW
Figure 1 provides a high-level overview of VAMO. We assume
that a third-party has employed a malware clustering system, for
example one of the systems proposed in [2, 11, 15], to partition a
malware dataset M into a set of clusters C = {C1, C2, .., Cx},
withx
i=1 Ci = M. VAMO’s objective is to validate the qual-
ity of C (i.e., the malware clustering results). We now provide a
description of VAMO’s components shown in Figure 1.
AV Label Dataset Given a large historic archive dataset of
malware samples A (which is different from M), we ﬁrst collect
the set of family labels assigned by multiple AV scanners to each of
the malware samples mk ∈A . The resulting AV labels dataset can
be represented as a set of tuples L = {(lk,1, lk,2, .., lk,ν )}k=1..n,
where lk,i is the malware family label assigned by the i-th of ν AV
scanners to malware sample mk, with k = 1, .., n, and n = |A|.
If an AV scanner misses to detect a malware sample, the related
label in the set L will be assigned a unique placeholder “unknown”
family label. It is worth noting that the malware dataset A need
not contain actual executable malware samples.
In fact, A may
simply contain a list of hashes (e.g., md5 or sha1) computed by a
third party (e.g., the owner of a large malware dataset who cannot
share the malware itself) over known malware samples.
In this
case, the label dataset L may be obtained by querying a service
such as virustotal.com to obtain, for each hash, the related
malware family labels from multiple AV scanners.
A
Malware
Archive
AV scan
AV Label
Dataset
L
M
Malware 
Dataset
AV scan
AV Label Graph
LM
Build
Reference
Clustering
Third-party malware clustering system
C
Clustering
Results
Validity
Analysis
Clustering 
Quality
Indexes
Malware
Clustering
Process
T
VAMO
Figure 1: VAMO System Overview
AV Label Graph VAMO uses the label dataset L to learn an AV
Label Graph (deﬁned formally in Section 5.). Basically, a node in
the graph represents a malware family name attributed by a certain
AV scanner (or AV, for short) to one or more malware samples in A.
For example, assuming the i-th AV assigned the label family_x
to at least one malware sample, the AV Label Graph will contain a
node called AVi_family_x. Two nodes, say AVi_family_x
and AVj_family_y, will be connected by an undirected edge if
there exists at least one malware sample mk ∈A that has been
assigned label family_x by the i-th AV, and family_y by the
j-th AV, respectively. Each edge is assigned a weight that depends
on the number of times that the connected nodes (i.e., the connected
labels) were assigned to a same malware sample. Notice that if
the i-th AV missed to detect a given malware sample, the related
missing label will be replace by a label such as AVi_unknown_U,
where U is a unique identiﬁer.
Reference Clustering Similarly to what we did with A, given
the malware dataset M (i.e., the input to the third-party cluster-
ing system), we ﬁrst collect the set of labels assigned by ν dif-
ferent AVs to each of the malware samples mk ∈ M, thus ob-
taining a dataset LM consisting of a tuple (or vector) of family
labels Lk = (lk,1, lk,2, .., lk,ν ) per each sample mk. At this point,
we leverage the previously learned AV Label Graph to measure
the dissimilarity (or distance) between samples in M according
to their malware family labels. Speciﬁcally, we measure the dis-
tance between two malware samples mi, mj ∈ M by measuring
the distance between their respective label vectors Li and Lj in the
graph. We give a formal deﬁnition of label-based distance between
malware samples in Section 5.1. At a high level, we compute the
distance between two samples mi, mj by computing the median