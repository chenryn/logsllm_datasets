Start Time: Sun, 23 Jul 2017 14:24:24 -0400
Labels:           
Annotations:      
Status:           Running
IP:         10.1.0.4  
```
为什么会这样？那是因为我们指定使用 CNI 作为网络插件，不使用`docker0`(也称**容器网络模型**或 **libnetwork** )。CNI 创建了一个虚拟接口，将其连接到底层网络，并设置 IP 地址和路由，最终将其映射到 pods 的命名空间。让我们来看看位于`/etc/cni/net.d/`的配置:
```
# cat /etc/cni/net.d/k8s.conf
{
 "name": "rkt.kubernetes.io",
 "type": "bridge",
 "bridge": "mybridge",
 "mtu": 1460,
 "addIf": "true",
 "isGateway": true,
 "ipMasq": true,
 "ipam": {
 "type": "host-local",
 "subnet": "10.1.0.0/16",
 "gateway": "10.1.0.1",
 "routes": [
      {
       "dst": "0.0.0.0/0"
      }
 ]
 }
}
```
在这个例子中，我们使用 CNI 桥插件来重用 pod 容器的 L2 桥。如果数据包来自`10.1.0.0/16`，目的地是任何地方，它会经过这个网关。就像我们之前看到的图一样，我们可以让另一个启用了`10.1.2.0/16`子网的 CNI 节点，这样 ARP 数据包就可以到达目标 pod 所在节点的物理接口。然后，它实现了跨节点的 pod 到 pod 通信。
让我们检查一下 iptables 中的规则:
```
// check the rules in iptables 
# sudo iptables -t nat -nL
... 
Chain POSTROUTING (policy ACCEPT)
target     prot opt source               destination
KUBE-POSTROUTING  all  --  0.0.0.0/0            0.0.0.0/0            /* kubernetes postrouting rules */
MASQUERADE  all  --  172.17.0.0/16        0.0.0.0/0
CNI-25df152800e33f7b16fc085a  all  --  10.1.0.0/16          0.0.0.0/0            /* name: "rkt.kubernetes.io" id: "328287949eb4d4483a3a8035d65cc326417ae7384270844e59c2f4e963d87e18" */
CNI-f1931fed74271104c4d10006  all  --  10.1.0.0/16          0.0.0.0/0            /* name: "rkt.kubernetes.io" id: "08c562ff4d67496fdae1c08facb2766ca30533552b8bd0682630f203b18f8c0a" */  
```
所有相关规则已切换至`10.1.0.0/16` CIDR。
# Pod 对服务通信
Kubernetes 是动态的。PODS 总是被创建和删除。Kubernetes 服务是通过标签选择器定义一组荚的抽象。我们通常使用服务来访问 pod，而不是显式指定 pod。当我们创建一个服务时，将会创建一个`endpoint`对象，它描述了该服务中的标签选择器所选择的一组 pod IPs。
In some cases, `endpoint` object will not be created with service creation. For example, services without selectors will not create a corresponding `endpoint` object. For more information, refer to the service without selectors section in [Chapter 3](03.html#22O7C0-6c8359cae3d4492eb9973d94ec3e4f1e), *Getting Started with Kubernetes*.
那么，流量是如何从 pod 到达服务背后的 pod 的呢？默认情况下，Kubernetes 通过`kube-proxy`使用 iptables 来执行魔法。下图对此进行了解释。
![](img/00091.jpeg)
让我们重复一下[第三章](03.html#22O7C0-6c8359cae3d4492eb9973d94ec3e4f1e)、*Kubernetes*的`3-2-3_rc1.yaml`和`3-2-3_nodeport.yaml`例子，观察默认行为:
```
// create two pods with nginx and one service to observe default networking. Users are free to use any other kind of solution.
# kubectl create -f 3-2-3_rc1.yaml
replicationcontroller "nginx-1.12" created
# kubectl create -f 3-2-3_nodeport.yaml
service "nginx-nodeport" created  
```
让我们遵守 iptable 规则，看看这是如何工作的。如下图，我们的服务 IP 是`10.0.0.167`，下面的两个 PODS IP 地址是`10.1.0.4`和`10.1.0.5`。
```
// kubectl describe svc nginx-nodeport
Name:             nginx-nodeport
Namespace:        default
Selector:         project=chapter3,service=web
Type:             NodePort
IP:               10.0.0.167
Port:                  80/TCP
NodePort:              32261/TCP
Endpoints:        10.1.0.4:80,10.1.0.5:80
...  
```
让我们通过`minikube ssh`进入 minikube 节点，检查它的可调度规则:
```
# sudo iptables -t nat -nL
...
Chain KUBE-SERVICES (2 references)
target     prot opt source               destination
KUBE-SVC-37ROJ3MK6RKFMQ2B  tcp  --  0.0.0.0/0            **10.0.0.167**           /* default/nginx-nodeport: cluster IP */ tcp dpt:80
KUBE-NODEPORTS  all  --  0.0.0.0/0            0.0.0.0/0            /* kubernetes service nodeports; NOTE: this must be the last rule in this chain */ ADDRTYPE match dst-type LOCAL
Chain **KUBE-SVC-37ROJ3MK6RKFMQ2B** (2 references)
target     prot opt source               destination
**KUBE-SEP-SVVBOHTYP7PAP3J5**  all  --  0.0.0.0/0            0.0.0.0/0            /* default/nginx-nodeport: */ statistic mode random probability 0.50000000000
**KUBE-SEP-AYS7I6ZPYFC6YNNF**  all  --  0.0.0.0/0            0.0.0.0/0            /* default/nginx-nodeport: */
Chain **KUBE-SEP-SVVBOHTYP7PAP3J5** (1 references)
target     prot opt source               destination
KUBE-MARK-MASQ  all  --  10.1.0.4             0.0.0.0/0            /* default/nginx-nodeport: */
DNAT       tcp  --  0.0.0.0/0            0.0.0.0/0            /* default/nginx-nodeport: */ tcp to:10.1.0.4:80
Chain KUBE-SEP-AYS7I6ZPYFC6YNNF (1 references)
target     prot opt source               destination
KUBE-MARK-MASQ  all  --  10.1.0.5             0.0.0.0/0            /* default/nginx-nodeport: */
DNAT       tcp  --  0.0.0.0/0            0.0.0.0/0            /* default/nginx-nodeport: */ tcp to:10.1.0.5:80
...  
```
这里的关键点是，该服务将集群 IP 暴露给来自目标`KUBE-SVC-37ROJ3MK6RKFMQ2B`的外部流量，该目标链接到两个自定义链`KUBE-SEP-SVVBOHTYP7PAP3J5`和`KUBE-SEP-AYS7I6ZPYFC6YNNF`，统计模式随机概率为 0.5。这意味着，iptables 将生成一个随机数，并根据概率分布 0.5 将其调整到目的地。这两个自定义链将`DNAT`目标设置为对应的 pod IP。`DNAT`目标负责更改数据包的目的 IP 地址。默认情况下，当流量进入时，conntrack 被启用以跟踪连接的目的地和源。所有这些都会导致路由行为。当流量到达服务时，iptables 会随机选择其中一个 pod 进行路由，并将目的 IP 从服务 IP 修改为真实的 pod IP，并取消 DNAT 以一直返回。
# 外部服务通信
为 Kubernetes 的外部流量提供服务的能力至关重要。Kubernetes 提供了两个 API 对象来实现这一点:
*   **服务**:外部网络负载平衡器或节点端口(L4)
*   **入口:** HTTP(S)负载平衡器(L7)
对于入口，我们将在下一节中了解更多。我们将首先关注 L4。根据我们对跨节点的 pod 到 pod 通信的了解，数据包是如何在服务和 pod 之间进出的。下图显示了它的工作原理。假设我们有两个服务，一个服务 A 有三个 pod(pod A、pod b 和 pod c)，另一个服务 B 只有一个 pod (pod d)。当流量从负载平衡器进入时，数据包将被分派到其中一个节点。大多数云负载平衡器本身并不知道 Pod 或容器。它只知道节点。如果节点通过了运行状况检查，那么它将成为目标的候选节点。假设我们要访问服务 B，它目前只有一个 pod 在一个节点上运行。然而，负载平衡器将数据包发送到另一个没有运行任何我们想要的 pod 的节点。交通路线如下所示:
![](img/00092.jpeg)
数据包路由过程将是:
1.  负载平衡器将选择其中一个节点来转发数据包。在 GCE 中，它根据源 IP 和端口、目标 IP 和端口以及协议的散列来选择实例。在 AWS 中，它基于循环算法。
2.  这里，路由目的地将被更改为 pod d (DNAT)，并将其转发到另一个节点，类似于跨节点的 pod 到 pod 通信。
3.  然后，是服务到 Pod 的通信。数据包到达 d 舱时会有相应的响应。
4.  Pod 到 service 的通信也是由 iptables 操纵的。
5.  数据包将被转发到原始节点。
6.  源和目的地将被卸载到负载平衡器和客户端，并一直发送回来。
In Kubernetes 1.7, there is a new attribute in service called **externalTrafficPolicy**. You can set its value to local, then after the traffic goes into a node, Kubernetes will route the pods on that node, if any.
# 进入
Kubernetes 中的 Pods 和服务都有自己的 IP；然而，它通常不是你提供给外部互联网的接口。虽然有配置了节点 IP 的服务，但是节点 IP 中的端口不能在服务之间重复。决定用哪个服务管理哪个端口是很麻烦的。此外，节点来了又走，向外部服务提供静态节点 IP 并不聪明。
入口定义了一组规则，允许入站连接访问 Kubernetes 集群服务。它将流量引入 L7 集群，在每个虚拟机上分配一个端口并将其转发到服务端口。如下图所示。我们定义了一组规则，并将它们作为源类型入口发布到应用编程接口服务器。当流量进入时，入口控制器将按照入口规则完成并路由入口。如下图所示，入口用于通过不同的 URL 将外部流量路由到 kubernetes 端点:
![](img/00093.jpeg)
现在，我们将通过一个例子，看看这是如何工作的。在本例中，我们将创建两个名为`nginx`和`echoserver`的服务，并配置入口路径`/welcome`和`/echoserver`。我们可以在 minikube 运行这个。minikube 的旧版本默认情况下不支持入口；我们必须首先启用它:
```
// start over our minikube local
# minikube delete && minikube start
// enable ingress in minikube
# minikube addons enable ingress
ingress was successfully enabled 
// check current setting for addons in minikube
# minikube addons list
- registry: disabled
- registry-creds: disabled
- addon-manager: enabled
- dashboard: enabled
- default-storageclass: enabled
- kube-dns: enabled
- heapster: disabled
- ingress: **enabled** 
```
在 minikube 中启用入口将创建一个 nginx 入口控制器和一个`ConfigMap`来存储 nginx 配置(参见[https://github . com/kubernetes/ingress/blob/master/controllers/nginx/readme . MD](https://github.com/kubernetes/ingress/blob/master/controllers/nginx/README.md)))，以及一个 RC 和服务作为处理未映射请求的默认 HTTP 后端。我们可以通过在`kubectl`命令中添加`--namespace=kube-system`来观察它们。接下来，让我们创建我们的后端资源。这是我们的 nginx `Deployment`和`Service`:
```
# cat 5-2-1_nginx.yaml
apiVersion: apps/v1beta1
kind: Deployment
metadata:
 name: nginx
spec:
 replicas: 2
 template:
 metadata:
 labels:
 project: chapter5
 service: nginx
 spec:
 containers:
 - name: nginx
 image: nginx
 ports:
 - containerPort: 80
---
kind: Service
apiVersion: v1
metadata:
 name: nginx
spec:
 type: NodePort
  selector:
 project: chapter5
 service: nginx
 ports:
 - protocol: TCP
 port: 80
 targetPort: 80
// create nginx RS and service
# kubectl create -f 5-2-1_nginx.yaml
deployment "nginx" created
service "nginx" created
```
然后，我们将使用 RS 创建另一项服务:
```
// another backend named echoserver
# cat 5-2-1_echoserver.yaml
apiVersion: apps/v1beta1
kind: Deployment
metadata:
 name: echoserver
spec:
 replicas: 1
 template: