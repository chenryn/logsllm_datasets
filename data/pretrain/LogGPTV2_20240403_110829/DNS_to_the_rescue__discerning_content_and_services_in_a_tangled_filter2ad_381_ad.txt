### 2. Analysis of the Zynga Domain

Let us now consider the second sample domain, Zynga (see Fig. 8). In this setup, Amazon EC2 cloud services provide the computational resources required for the games, while Akamai CDN hosts most of the static content. Some services and games, such as MafiaWars, are served directly by Zynga-owned servers. Interestingly, approximately 500 Amazon server IP addresses are contacted, handling 86% of all Zynga traffic. Akamai serves fewer requests (7%), but 30 different server IP addresses are observed.

Given that the offline analyzer relies on actual network traffic measurements, it can capture both the service popularity among monitored customers and the bias induced by server selection and load balancing mechanisms. To further elaborate, let us examine Fig. 9. Each of the three sub-figures corresponds to a different content provider (i.e., the second-level domain name). For each content provider, we plot the access patterns in three of our traces (EU1-ADSL1, US-3G, and EU2-ADSL). The x-axis in each graph represents the CDNs hosting the content, and the y-axis represents different traces. Notice that for every CDN, the figure shows all the accessed server IP addresses that belong to the CDN. Therefore, the width of the column representing each CDN varies. Additionally, the gray scale of each block in the graph represents the frequency of access; the darker the cell, the larger the fraction of flows that a particular server IP was responsible for. The "SELF" column reports cases where the content providers and content hosts are the same organization.

The top graph in Fig. 9 shows the access pattern for Facebook. We can see that, in all datasets, most of the Facebook content is hosted on Facebook servers. The only other CDN used by Facebook is Akamai, which uses different server IPs in different geographical regions.

In the middle graph, Twitter's access patterns are slightly different. Although Twitter heavily relies on its own servers to host content, it also depends significantly on Akamai to serve content to users in Europe. However, the dependence on Akamai is significantly less in the US.

The bottom graph shows the access patterns for Dailymotion, a video streaming site. Dailymotion heavily relies on Dedibox to host content in both Europe and the US. While they do not host any content on their own servers in Europe, they do serve some content in the US. In the US, they also rely on other CDNs like Meta and NTT to serve content, while in Europe, they use Edgecast to a lesser extent.

### 3. Content Discovery

Although the spatial discovery module provides invaluable insights into how a particular resource is hosted across various CDNs, it does not fully explain the complete behavior of CDNs. In the content discovery module, our goal is to understand the content distribution from the perspective of CDNs and cloud service providers. Table 5 shows the top-10 second-level domains served by the Amazon EC2 cloud in EU1-ADSL1 and US-3G. Notice that one dataset is from Europe and the other from the US. It is clear that the top-10 domains in the two datasets do not match. For instance, popular domains hosted on Amazon for US users, such as admavel, mobclix, and andomedia, are not accessed by European users, while other domains like cloutfront, invitemedia, and rubiconproject are popular in both datasets. This clearly demonstrates that the popularity and access patterns of CDNs hosting content for different domains depend on geography, and extrapolating results from one region to another may lead to incorrect conclusions.

### 4. Automatic Service Tag Extraction

Another interesting application of DN-Hunter is in identifying all the services and applications running on a particular layer-4 port number. This application is feasible due to the fine-grained traffic visibility provided by DN-Hunter. To keep the tables small, we only show the results extracted for a few selected layer-4 ports for two datasets: EU1-FTTH (Table 6) and US-3G (Table 7). These tables list the terms along with the weights returned by the Service Tag Extraction Analytics algorithm (Algorithm 4). The last column in each table represents the ground truth obtained using Tstat DPI and augmented by Google searches and our domain knowledge.

We can clearly see that the most popular terms extracted in both datasets accurately represent the application or service on the port. Some terms, like pop3, imap, and smtp, are very obvious from the top keyword. However, others are not as straightforward but can be derived easily. For example, consider TCP port 1337. This port is not a standard port for any service, and even a Google search for TCP port 1337 does not yield straightforward results. However, by adding "exodus" and "genesis," the main keywords extracted in DN-Hunter, to the Google search, we find that this port is related to the www.1337x.org BitTorrent tracker.

### 5. Case Study: appspot.com Tracking

In this section, we present a surprising phenomenon discovered using DN-Hunter's ability to track domains. Let us consider the domain appspot.com. Appspot is a free web-apps hosting service provided by Google, with limited CPU time, server bandwidth, and the number of applications that can be used for free. Using the labels for various flows in the labeled flows database, we extract all traffic associated with these services. This allows us to understand the kinds of applications and services hosted in the Appspot cloud.

Figure 10 shows the most relevant applications hosted on appspot as a word cloud, where the larger/darker fonts represent more popular applications. Although appspot is intended to host legacy applications, it is evident that users host applications like "open-tracker," "rlskingbt," and similar. Further investigation reveals that these applications actually host BitTorrent trackers for free. With the help of information from DN-Hunter and Tstat DPI deployed at the European ISP, we find several trackers and other legacy applications running on the appspot.com site. Our findings are presented in Table 8. As we can see, BitTorrent trackers represent only 7% of the applications but constitute a significant portion of the overall traffic. When considering the total bytes exchanged for each of these services, the traffic from client-to-server generated by the trackers is a significantly large percentage of the overall traffic.

In Fig. 11, we plot the timeline (in 4-hour intervals) of when the trackers were active over an 18-day period. A dot represents that the tracker was active at that time interval. We assign each tracker an ID, starting at 1 and incrementally increasing based on the time when the tracker was first observed. Of the 45 trackers seen in the 18-day period, about 33% (colored in red, IDs 1-15) remained active for all 18 days. Trackers with IDs 26-31 (colored in blue) exhibit a unique pattern of on-off periods, indicating synchronized behavior, likely due to a single BitTorrent client being part of a swarm. Similar considerations hold for trackers with IDs 33-34 (colored in green), which were first accessed on May 5th.

Finally, trackers with IDs larger than 33 highlight a birth process, where new trackers are born and accessed by a few BitTorrent clients. This is due to the resource limitations imposed by Appspot. Checking the status of the trackers on May 15th, 2012, we verified that most of them, while still existing as FQDNs, had run out of resources and were made unavailable by Google. They exist as zombies, with some BitTorrent clients still trying to access them.

### 6. Dimensioning the FQDN Clist

In Section 3, we presented the design of the DNS resolver. One of the key data structures of the DNS resolver is the FQDN Clist. Choosing the correct size for the Clist is critical to the success of DN-Hunter. In this section, we present a methodology to choose the correct value of L (size of the Clist) and the real-time constraint implications.

Figure 12 shows the Cumulative Distribution Function (CDF) of the "first flow delay," i.e., the time elapsed between the observation of the DNS response directed to the client IP and the first packet of the first flow directed to one of the server IP addresses in the answer list. Semilog scale is used for clarity. In all datasets, the first TCP flow is observed within less than 1 second in about 90% of cases. Access technology and sniffer placement impact this measurement; for instance, FTTH exhibits smaller delays, while 3G technology suffers from higher delays.