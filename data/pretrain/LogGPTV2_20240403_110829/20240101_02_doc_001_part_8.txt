more familiar with the % and _ wildcards that are used with the LIKE operator. %
represents zero, one, or multiple characters while the underscore sign represents one,
single character. Luckily, COLUMNS supports lambda functions.
© Manning Publications Co. To comment go to liveBook
62
TIP A Lambda Function is a self-contained block of functionality that can be passed around and
used in your code. Lambda functions have different names in different programming languages,
such as Lambda expressions in Java, Kotlin and Python, Closures in Swift and blocks in C.
The query above selecting a range of prices can also be written like this:
FROM prices
WHERE COLUMNS(col -> col LIKE 'valid%') -- #1
BETWEEN '2020-01-01' AND '2021-01-01';
#1 The expression inside the COLUMNS expression is a Lambda function evaluating to true when the column name is like the
given text.
Last but not least, you can combine the COLUMNS expression with the REPLACE or
EXCLUDE conditions, too. Let’s say you want to compute the maximum value over all the
columns in the prices table except the generated id value, you can get them like this:
SELECT max(COLUMNS(* EXCLUDE id)) FROM prices;
3.5.2 Inserting by name
Remember listing 3.6? In that listing we used a statement in the form of INSERT INTO
target(col1, col2) SELECT a, b FROM src to populate our systems table. This
works but can be fragile to maintain, as the INSERT statement requires either the
selected columns to be in the same order as they are defined by the target table or that
you repeat the column names. Once in the INTO clause, once in the select list.
DuckDB offers a BY NAME clause to solve that issue and listing 3.6 can be rewritten as
follows, keeping the mapping from the column names in the source to the column names
for the target together in one place. The BY NAME keyword in the following listing
indicates that the columns in the select clause that follows shall be matched by name
onto columns of the target table.
Listing 3.19 Insertion by name
INSERT INTO systems BY NAME
SELECT DISTINCT
system_id AS id,
system_public_name AS NAME
FROM 'https://oedi-data-lake.s3.amazonaws.com/pvdaq/csv/systems.csv';
© Manning Publications Co. To comment go to liveBook
63
Whether you add new columns or remove columns from the insertion you only need to
change the query now in one place. Any constraints however, such as non-null columns,
must be still full-filled.
3.5.3 Accessing aliases everywhere
You probably haven’t noticed it, but several of our examples benefit from something that
should be the standard, but isn’t. The moment you introduce an alias to a column, you
can access them in succeeding clauses. In the following listing we access the non-
aggregate alias is_not_system10—as defined in the select list—in the WHERE and the
GROUP BY clauses without repeating the column definition. The latter is not possible in
many other relational databases. The same applies to the alias power_per_month we
gave to the sum aggregate: we can access it in the HAVING clause, too.
Listing 3.20 Access aliases in WHERE, GROUP BY and HAVING clauses
SELECT system_id > 10 AS is_not_system10,
date_trunc('month', read_on) AS month,
sum(power) / 1000 / 1000 AS power_per_month
FROM readings
WHERE is_not_system10 = TRUE -- #1
GROUP BY is_not_system10, month
HAVING power_per_month > 100; -- #2
#1 Accessing an alias that refers to a non-aggregate
#2 Accessing an alias that refers to an aggregate
3.5.4 Grouping and ordering by all relevant columns
As learned in the section about the GROUP BY clause, all non-aggregate columns need to
be enumerated in a GROUP BY clause. If you have lots of non-aggregate columns this
can be a painful experience and one that DuckDB alleviates by allowing you to use GROUP
BY ALL. We can rewrite v_power_per_day like this:
Listing 3.21 Creating grouping-sets by grouping by all non-aggregate values
CREATE OR REPLACE VIEW v_power_per_day AS
SELECT system_id,
date_trunc('day', read_on) AS day,
round(sum(power) / 4 / 1000, 2) AS kWh,
FROM readings
GROUP BY ALL;
© Manning Publications Co. To comment go to liveBook
64
A similar concept exists for ordering. An ORDER BY ALL will sort the result by the
included columns from left to right. Querying the freshly created view with SELECT
system_id, day FROM v_power_per_day ORDER BY ALL will sort the result first by
system_id, then day. In case of a select-start the order of the columns is defined by the
table or view definition, of course. The following statement is valid SQL in DuckDB and it
returns the power produced in kWH per day sorted by systems first, then days and then
kWH:
Listing 3.22 Omitting the SELECT clause and simplify ordering
FROM v_power_per_day ORDER BY ALL;
3.5.5 Sampling data
When working with large datasets, we often want to get a sample of the data rather than
having to look through everything. Assuming you have imported the readings for at least
system 34, you will have more than 50000 records in your database. This can be
confirmed with a SELECT count(*) FROM readings. We can get an overview of the
non-zero power readings by asking for a sample of n percent or n number of rows:
Listing 3.23 Sample a relation
SELECT power
FROM readings
WHERE power <> 0
USING SAMPLE 10% -- #1
(bernoulli); -- #2
#1 Retrieve a sample that has roughly 10% the size of the data
#2 Specify the sampling method to use (see below)
This is much easier and more flexible than dealing with arbitrary limits as it gives a
better and more reliable overview. The sampling itself uses probabilistic sampling
methods however, unless a seed is specified with the additional REPEATABLE clause. The
sampling rate in percent is not meant to be an exact hit. In our example, it varies
around 2000 rows from somewhat more than 20000 which have power column not equal
to zero.
If you instruct DuckDB to use a specific rate for sampling, it applies system sampling,
including each vector by an equal chance. Sampling on vectors instead of working on
tuples (which is done by the alternative bernoulli method) is very effective and has no
extra overhead. As one vector is roughly about 1000 tuples in size it is not suited for
smaller datasets, as all data will be included or filtered out. Even for the total of ~100000
readings that have a power value greater than zero we recommend bernoulli for a
more evenly distributed sampling.
© Manning Publications Co. To comment go to liveBook
65
For a fixed sampling size a method called reservoir is used. The reservoir is filled up
first with as many elements as requested and then streaming the rest, randomly
swapping elements in the reservoir.
Find more about this interesting technique in the samples documentation.
3.5.6 Functions with optional parameters
A couple of functions in DuckDB, such as read_json_auto for example, have some
required parameters and one or more parameters with sensible defaults that are
optional. Aforementioned example has 17 parameters, you can get a list of them with
SELECT DISTINCT unnest(parameters)
FROM duckdb_functions()
WHERE function_name = 'read_json_auto';
We are using a distinct here because there’s a couple of overloads with different types,
too. Luckily, DuckDB supports named optional arguments. Assume you want to specify
the dateformat only, you would use the name=value syntax:
Listing 3.24 Using named parameters
echo '{"foo": "21.9.1979"}' > 'my.json'
duckdb -s \
"SELECT * FROM read_json_auto(
'my.json',
dateformat='%d.%M.%Y' -- #1
)"
#1 This is using the named parameter dateformat
DuckDB is able to parse the non-iso-formatted string into a proper date as we used the
dateformat parameter:
┌────────────┐
│ foo │
│ date │
├────────────┤
│ 1979-01-21 │
└────────────┘
© Manning Publications Co. To comment go to liveBook
66
3.6 Summary
SQL queries are composed of several statements which are in turn
composed of clauses. Queries are categorized as Data Definition
Language (DDL) or Data Manipulation Language (DML).
DML queries covers creating, reading, updating and deleting rows.
Manipulation of data is not only about changing persistent state, but
transforming existing relations into new ones, hence reading data falls
also under DML.
DDL queries such as CREATE TABLE and CREATE VIEW are used in
DuckDB to create a persistent schema. This is in line with any other
relational database and is independent whether DuckDB is started with
database stored on disk or in-memory.
A rigid schema makes data inconsistencies more visible: blindly
ingesting data with inconsistencies will fail due to constraint errors.
Constraint errors can be mitigated with appropriate actions defined ON
CONFLICT when creating or updating rows.
DuckDB makes SQL even easier to write with innovations like SELECT *
EXCLUDE() and SELECT * REPLACE() and more intuitive alias usage.
© Manning Publications Co. To comment go to liveBook
67
4
Advanced aggregation and
analysis of data
This chapter covers
Preparing, cleaning and aggregating data while ingesting
Using window functions to create new aggregates over different partitions of any
dataset
Understanding the different types of sub-queries
Using Common Table Expressions (CTEs)
Applying filters to any aggregate
The goal of this chapter is to give you some ideas on how an analytical database such as
DuckDB can be used to provide reports that would take a considerably larger amount of
code written in an imperative programming language. While we will build upon the
foundation laid in chapter 3, we will leave a simple SELECT xzy FROM abc behind
quickly. Investing your time in learning modern SQL won’t be wasted. The constructs
presented here can be used everywhere where DuckDB can be run or embedded and
therefore enrich your application.
© Manning Publications Co. To comment go to liveBook
68
4.1 Pre-aggregate data while ingesting
Let’s move forward with our example scenario. In section 3.4.1 we worked with the data
for a photovoltaic grid that—while having some consistency issues—was a good fit for
our schema and idea. Remember, the goal is to store measurements in intervals of 15
minutes. If you look at the other datasets you have downloaded throughout section 3.2.1
you will notice that some come in intervals other than 15 minutes. One quick way to
peek into the files is the tail command, returning the last n-lines of a file (head would
work as well). Using it on 2020_10.csv shows that this file contains measurements in 1-
minute intervals:
> duckdb -s ".maxwidth 40" -s "FROM read_csv_auto('2020_10.csv') LIMIT 3"
┌────────┬─────────────────────┬───┬───────────────┬────────────────┐
│ SiteID │ Date-Time │ … │ module_temp_3 │ poa_irradiance │
│ int64 │ timestamp │ │ double │ double │
├────────┼─────────────────────┼───┼───────────────┼────────────────┤
│ 10 │ 2020-01-23 11:20:00 │ … │ 14.971 │ 748.36 │
│ 10 │ 2020-01-23 11:21:00 │ … │ 14.921 │ 638.23 │
│ 10 │ 2020-01-23 11:22:00 │ … │ 14.895 │ 467.67 │
├────────┴─────────────────────┴───┴───────────────┴────────────────┤
│ 3 rows 16 columns (4 shown) │
└───────────────────────────────────────────────────────────────────┘
And of course, 2020_1200.csv has another interval, this time 5 minutes but also the
overall structure looks different:
> duckdb -s ".maxwidth 40" -s "FROM read_csv_auto('2020_1200.csv') LIMIT 3"
┌────────┬─────────────────────┬───┬──────────────────┬──────────────┐
│ SiteID │ Date-Time │ … │ ac_power_metered │ power_factor │
│ int64 │ timestamp │ │ int64 │ double │
├────────┼─────────────────────┼───┼──────────────────┼──────────────┤
│ 1200 │ 2020-01-01 00:00:00 │ … │ 20 │ 0.029 │
│ 1200 │ 2020-01-01 00:05:00 │ … │ 20 │ 0.029 │
│ 1200 │ 2020-01-01 00:10:00 │ … │ 20 │ 0.029 │
├────────┴─────────────────────┴───┴──────────────────┴──────────────┤
│ 3 rows 6 columns (4 shown) │
└────────────────────────────────────────────────────────────────────┘
© Manning Publications Co. To comment go to liveBook
69
Remember, those are datafiles from the same pool. Even those are inconsistent between
different sources. Data analytics is quite often about dealing with those exact same
problems. Let’s use one of the many functions DuckDB offers to deal with dates, times
and timestamps, in this case time_bucket(). time_bucket() truncates timestamps to a
given interval and aligns them to an optional offset, creating a time bucket. Time buckets
are a powerful mechanism for aggregating sensor readings and friends. Together with
GROUP BY and avg as aggregate functions, we can prepare and eventually ingest the
data according to our requirements: We create time buckets in a 15-minute interval and
compute the average power produced of all readings that fall into a specific bucket.
When you look at the query you’ll notice a CASE WHEN THEN ELSE END construct, a
CASE-Statement, which works like an if/else construct. What it does here is turn
readings with a value lower than zero, or with no value at all into zero before computing
the average. That’s one of the oddities of this dataset: maybe the sensor had issues,
maybe the network had issues, you’ll never know, but you have to deal with the data.
Here we decided that it is ok to treat null values like negative values and cap them to
zero. In cases that throws off your calculation, you might consider a FILTER for the
aggregate. We will discuss this in the section Section 46.3.
Listing 4.1 Cleaning and transforming data during ingestion
INSERT INTO readings(system_id, read_on, power)
SELECT any_value(SiteId), -- #1
time_bucket(
INTERVAL '15 Minutes',
CAST("Date-Time" AS timestamp)
) AS read_on, -- #2
avg(
CASE
WHEN ac_power < 0 OR ac_power IS NULL THEN 0
ELSE ac_power END) -- #3
FROM
read_csv_auto(
'https://developer.nrel.gov/api/pvdaq/v3/' ||
'data_file?api_key=DEMO_KEY&system_id=10&year=2019'
)
GROUP BY read_on
ORDER BY read_on;
#1 This picks any value of the column SiteId from the CSV-File. The files are per system, which means that this column is the
same in each row, so picking any one of them is correct. Applying any_value() is necessary as we compute an aggregate
(avg)
#2 This truncates the timestamp to a quarter-hour, notice how we explicitly cast the column to a timestamp with standard SQL
syntax; in addition, the transformed value gets an alias (read_on)
#3 Here, avg computes the average of all readings in the bucket created above because we group the result by this bucket.
© Manning Publications Co. To comment go to liveBook
70
The imports for the remaining dataset are identical, you will want to change the filename
in the FROM clause accordingly.
TIP There are many more date and time based functions in DuckDB, when in doubt, have a look
at the reference documentation: https://duckdb. org/docs/ sql/functions/ timestamp. You will be
able to parse nearly any string into a proper date or timestamp.
Whether you don’t ingest at all and do all kind of analytics in-memory based on external
files, want to aggregate to some extent during ingestion, or only aggregate during
analysis is usually a trade-off, depending among other things on the size of your dataset,
goals for long-time storage, further processing needs. Trying to make a general
applicable solution here is therefore bound to fail. In the scenario here we decided to
both ingest and aggregate for educational purposes as well as keep the dataset small
enough to be shareable.
4.2 Summarizing data
You usually want to know some characteristics of a new dataset before going into an in-
depth analysis of it, such as the number of values (in our example how many readings),
the distribution and magnitude of numerical values (without knowing if we are dealing in
Watt or Kilowatt our reports would be blatantly wrong) and the interval size of time
series.
DuckDB has the unique SUMMARIZE command that quickly gives you this information
about any dataset. Run SUMMARIZE readings; in your database. Your result should be
similar to this:
┌─────────────┬───────────────┬─────────────────────┬───┬─────────┬───────┐
│ column_name │ column_type │ max │ … │ q75 │ count │
│ varchar │ varchar │ varchar │ │ varchar │ int64 │
├─────────────┼───────────────┼─────────────────────┼───┼─────────┼───────┤
│ system_id │ INTEGER │ 1200 │ … │ 1200 │ 151879│
│ read_on │ TIMESTAMP │ 2020-06-26 11:00:00 │ … │ │ 151879│
│ power │ DECIMAL(10,3) │ 133900.000 │ … │ 5125 │ 151879│
└─────────────┴───────────────┴─────────────────────┴───┴─────────┴───────┘
There are many more columns, but we abbreviated the list for readability. Switch your
CLI to line mode by running .mode line and then summarizing a subset of the readings
with SUMMARIZE SELECT read_on, power FROM readings WHERE system_id =
1200;: