```
user@docker1:~$ sudo iptables -A OUTPUT -o docker0 -p icmp -m \
state --state NEW,ESTABLISHED -j ACCEPT
user@docker1:~$ sudo iptables -A INPUT -i docker0 -p icmp -m \
state --state ESTABLISHED -j ACCEPT
```
被添加到输出链的规则寻找从`docker0`桥(朝向容器)流出的流量，这是协议 ICMP 的流量，并且是新的或已建立的流量。被添加到输入链的规则寻找进入`docker0`桥(朝向主机)的流量，即协议 ICMP 的流量，并且是已建立的流量。由于流量来自 Docker 主机，这些规则将匹配并允许到容器的 ICMP 流量工作:
```
user@docker1:~$ ping 172.17.0.2 -c 2
PING 172.17.0.2 (172.17.0.2) 56(84) bytes of data.
64 bytes from 172.17.0.2: icmp_seq=1 ttl=64 time=0.081 ms
64 bytes from 172.17.0.2: icmp_seq=2 ttl=64 time=0.021 ms
--- 172.17.0.2 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 999ms
rtt min/avg/max/mdev = 0.021/0.051/0.081/0.030 ms
user@docker1:~$
```
但是，这仍然不允许容器本身 ping 默认网关。这是因为我们添加到输入链的规则匹配进入`docker0`桥的流量，只寻找已建立的会话。为了双向工作，您需要在第二个规则中添加`NEW`标志，这样它也将匹配容器生成的流向主机的新流:
```
user@docker1:~$ sudo iptables -A INPUT -i docker0 -p icmp -m \
state --state NEW,ESTABLISHED -j ACCEPT
```
由于我们添加到输出链的规则已经指定了新的或已建立的流，因此从容器到主机的 ICMP 连接现在也将工作:
```
user@docker1:~$ docker exec -it web1 ping  
PING 172.17.0.1 (172.17.0.1): 48 data bytes
56 bytes from 172.17.0.1: icmp_seq=0 ttl=64 time=0.073 ms
56 bytes from 172.17.0.1: icmp_seq=1 ttl=64 time=0.079 ms
^C--- 172.17.0.1 ping statistics ---
2 packets transmitted, 2 packets received, 0% packet loss
round-trip min/avg/max/stddev = 0.073/0.076/0.079/0.000 ms
user@docker1:~$
```
# 通过负载平衡器公开服务
另一种隔离容器的方法是用负载平衡器来处理它们。这种操作模式有几个优点。首先，负载均衡器可以为多个后端节点提供智能负载均衡。如果容器死亡，负载平衡器可以将其从负载平衡池中删除。其次，你实际上是把你的容器藏在一个负载平衡的**虚拟 IP** ( **贵宾**)地址后面。客户端认为，当它们实际上与负载平衡器交互时，它们正在与容器中运行的应用直接交互。在许多情况下，负载平衡器可以提供或卸载安全功能，例如 SSL 和 web 应用防火墙，这些功能使扩展基于容器的应用更容易以安全的方式完成。在本食谱中，我们将学习如何做到这一点，以及 Docker 中使这一点更容易做到的一些功能。
## 做好准备
在以下示例中，我们将使用多个 Docker 主机。我们还将使用用户定义的覆盖网络。假设您知道如何为覆盖网络配置 Docker 主机。如果没有，请参见[第三章](03.html "Chapter 3. User-Defined Networks")、*用户定义网络*中的*创建用户定义覆盖网络*配方。
## 怎么做…
负载平衡并不是一个新概念，它在物理和虚拟机领域都有很好的理解。然而，使用容器进行负载平衡增加了额外的复杂性，这会使事情变得更加复杂。首先，让我们看看负载平衡在没有容器的情况下通常是如何工作的:
![How to do it…](img/B05453_06_07.jpg)
在这种情况下，我们有一个简单的负载平衡器配置，其中负载平衡器为单个后端池成员(`192.168.50.150`)提供 VIP。流程是这样工作的:
*   客户端生成对负载平衡器上托管的 VIP (10.10.10.150)的请求
*   负载平衡器接收请求，确保它拥有该 IP 的 VIP，然后代表客户端向该 VIP 的后端池成员生成请求
*   服务器接收来自负载平衡器的请求，并直接响应负载平衡器
*   负载平衡器然后对客户端做出响应
在大多数情况下，对话涉及两个不同的会话，一个在客户端和负载平衡器之间，另一个在负载平衡器和服务器之间。每一个都是不同的 TCP 会话。
现在，让我们展示一个在容器空间中如何工作的例子。检查下图所示的拓扑:
![How to do it…](img/B05453_06_08.jpg)
在这个例子中，我们将使用基于容器的应用服务器作为后端池成员以及基于容器的负载平衡器。让我们做以下假设:
*   主机`docker2`和`docker3`将为支持许多不同重要人物的许多不同网络展示容器提供主机服务
*   我们将为我们希望定义的每个 VIP 使用一个负载平衡器容器(`haproxy`实例)
*   每个展示服务器都暴露端口`80`
考虑到这一点，我们可以假设主机网络模式对于负载平衡器主机(`docker1`)以及主机(`docker2`和`docker3`)都是不可能的，因为它需要容器在大量端口上公开服务。在引入用户定义网络之前，这将使我们不得不处理`docker0`桥上的端口映射。
这将很快成为一个管理和故障排除的问题。例如，拓扑可能真的如下所示:
![How to do it…](img/B05453_06_09.jpg)
在这种情况下，负载平衡器 VIP 将是主机`docker1`上的一个已发布端口，即`32769`。网络服务器本身也发布端口来公开它们的网络服务器。让我们看一下负载平衡请求可能是什么样子:
*   来自外部网络的客户端生成对`http://docker1.lab.lab:32769`的请求。
*   `docker1`主机接收请求，并通过`haproxy`容器上公布的端口翻译数据包。这将目的地 IP 和端口更改为`172.17.0.2:80`。
*   `haproxy`容器接收请求，并确定被访问的 VIP 具有包含`docker2:23770`和`docker3:32771`的后端池。它为该会话选择`docker3`主机，并向`docker3:32771`发送请求。
*   当请求遍历主机`docker1`时，它执行出站`MASQUERADE`，并将容器隐藏在主机的 IP 接口后面。
*   该请求被发送到主机的默认网关(MLS)，该网关又将该请求向下转发到主机`docker3`。
*   `docker3`主机接收请求，并通过`web2`容器上公布的端口翻译数据包。这将目的地 IP 和端口更改为`172.17.0.3:80`。
*   `web2`容器接收请求并向`docker1`返回响应
*   `docker3`主机接收回复，并通过入站发布端口将数据包翻译回来。
*   请求在`docker1`接收，通过出站`MASQUERADE`翻译回来，并在`haproxy`容器交付。
*   `haproxy`容器然后响应客户端。`docker1`主机将`haproxy`容器的响应翻译回其自己在端口`32769`上的 IP 地址，并且该响应返回到客户端。
虽然可行，但要跟踪的东西很多。此外，负载平衡器节点需要知道每个后端容器的已发布端口和 IP 地址。如果容器重新启动，发布的端口可能会发生变化，使其无法访问。使用大型后端池对此进行故障排除也是一件令人头疼的事情。
因此，虽然这肯定是可行的，但引入用户定义的网络可以让这变得更容易管理。例如，我们可以为后端池成员利用覆盖型网络，并完全消除对大量端口发布和出站伪装的需求。该拓扑看起来更像这样:
![How to do it…](img/B05453_06_10.jpg)
让我们看看需要什么来构建这种配置。我们需要做的第一件事是在其中一个节点上定义一个用户定义的覆盖类型网络。我们在`docker1`上定义，称之为`presentation_backend`:
```
user@docker1:~$ docker network create -d overlay \
--internal presentation_backend
bd9e9b5b5e064aee2ddaa58507fa6c15f49e4b0a28ea58ffb3da4cc63e6f8908
user@docker1:~$
```
### 注
注意我创建这个网络时是如何通过`--internal`标志的。您会从[第 3 章](03.html "Chapter 3. User-Defined Networks")、*用户定义网络*中回忆起，这意味着只有连接到该网络的容器才能访问它。
我们要做的下一件事是创建两个 web 容器，它们将作为负载平衡器的后端池成员。我们将在主机`docker2`和`docker3`上进行:
```
user@docker2:~$ docker run -dP --name=web1 --net \
presentation_backend jonlangemak/web_server_1
6cc8862f5288b14e84a0dd9ff5424a3988de52da5ef6a07ae593c9621baf2202
user@docker2:~$
user@docker3:~$ docker run -dP --name=web2 --net \
presentation_backend jonlangemak/web_server_2
e2504f08f234220dd6b14424d51bfc0cd4d065f75fcbaf46c7b6dece96676d46
user@docker3:~$
```
剩下要部署的组件是负载平衡器。如上所述，`haproxy`有一个负载平衡器的容器映像，因此我们将在本例中使用它。在运行容器之前，我们需要想出一个配置，我们可以将其传递到容器中供`haproxy`使用。这是通过将一个卷装入容器来完成的，我们将很快看到。配置文件名为`haproxy.cfg`，我的示例配置如下:
```
global
    log 127.0.0.1   local0
defaults
    log     global
    mode    http
    option  httplog
    timeout connect 5000
    timeout client 50000
    timeout server 50000
    stats enable
    stats auth user:docker
    stats uri /lbstats
frontend all
 bind *:80
 use_backend pres_containers
backend pres_containers
    balance roundrobin
 server web1 web1:80 check
 server web2 web2:80 check
    option httpchk HEAD /index.html HTTP/1.0
```
在前面的配置中，有几项值得指出:
*   我们将`haproxy`服务绑定到端口`80`上的所有接口
*   任何到达港口`80`的容器的请求都将被加载到一个名为`pres_containers`的池中
*   `pres_containers`池在两台服务器之间以循环方式平衡负载:
    *   `web1`在港口`80`
    *   `web2`在港口`80`
这里有趣的一点是，我们可以按名称定义池成员。这是一个巨大的优势，伴随着用户定义的网络，意味着我们不需要担心跟踪容器 IP 寻址。
我把这个配置文件放在我主目录的一个名为`haproxy`的文件夹中:
```
user@docker1:~/haproxy$ pwd
/home/user/haproxy
user@docker1:~/haproxy$ ls
haproxy.cfg
user@docker1:~/haproxy$
```
配置文件同步后，我们可以按如下方式运行容器:
```
user@docker1:~$ docker run -d --name haproxy --net \
presentation_backend -p 80:80 -v \
~/haproxy:/usr/local/etc/haproxy/ haproxy
d34667aa1118c70cd333810d9c8adf0986d58dab9d71630d68e6e15816741d2b
user@docker1:~$
```
您可能会想知道为什么我在将容器连接到`internal`类型网络时指定端口映射。回想一下前面几章，端口映射在所有网络类型中都是全局的。换句话说，即使我目前没有使用它，它仍然是容器的一个特征。因此，如果我将一个网络类型连接到可以使用端口映射的容器，它就会。在这种情况下，我首先需要将容器连接到覆盖网络，以确保它可以到达后端网络服务器。如果`haproxy`容器启动时无法解析池成员名称，将无法加载。
此时，`haproxy`容器对其池成员具有可达性，但我们无法从外部访问`haproxy`容器。为此，我们将把另一个接口连接到可以使用端口映射的容器。在这种情况下，那将是`docker0`桥:
```
user@docker1:~$ docker network connect bridge haproxy
user@docker1:~
```
此时，`haproxy`容器应该可以在以下网址从外部获得:
*   负载均衡贵宾:`http://docker1.lab.lab`
*   HAProxy stats: `http://docker1.lab.lab/lbstats`
如果我们查看统计页面，我们应该看到`haproxy`容器可以通过覆盖层到达每个后端 web 服务器。我们可以看到每个人的健康检查都以`200 OK`状态返回:
![How to do it…](img/B05453_06_11.jpg)
现在，如果我们检查 VIP 本身并点击刷新几次，我们应该会看到从每个容器呈现的网页:
![How to do it…](img/B05453_06_12.jpg)
这种类型的拓扑为我们提供了几个明显的优势，超过了我们在容器负载平衡方面的第一个概念。基于覆盖的网络的使用不仅提供了基于名称的容器解析，而且显著降低了流量路径的复杂性。当然，在这两种情况下，流量都采用相同的物理路径，但是我们不需要依赖这么多不同的 NAT 来工作。它还使整个解决方案更加动态。这种类型的设计可以很容易地复制，为许多不同的后端覆盖网络提供负载平衡。