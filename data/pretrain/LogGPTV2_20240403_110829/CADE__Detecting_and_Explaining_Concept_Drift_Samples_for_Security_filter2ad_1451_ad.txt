fying the original classiﬁer. These requirements are difﬁcult
to meet for malware classiﬁers in a production environment
(detailed discussions are in Section 9).
4.2 Evaluation Results
In the following, we ﬁrst compare the drifting detection per-
formance of CADE with baselines and evaluate the impact of
contrastive learning. Then, we perform case studies to investi-
gate the potential reasons for detection errors.
Drifting Sample Detection Performance. We ﬁrst use
one experiment setting to explain our evaluation process.
Take the Drebin dataset for example. Suppose we use family
Iconosys as the previously unseen family in the testing set.
After training the detection model (without any samples of
Iconosys), we use it to detect and rank the drifting samples. To
evaluate the quality of the ranked list, we simulate an analyst
inspecting samples from the top of the list.
Figure 4a shows that, as we inspect more drifting samples
(up to 150 samples), the precision maintains at a high level
(over 0.97) while the recall gradually reaches 100%. Combin-
Figure 5: F1 scores of drifting
detection.
Figure 6: Normalized investi-
gation efforts.
unseen-family samples out of the inspected samples. Recall
measures the ratio of unseen-family samples that are suc-
cessfully discovered by the detection module out of all the
unseen-family samples. F1 score is the harmonic mean of pre-
cision and recall: F1 = 2× precision×recall
precision+recall . Finally, to quantify
the efforts of inspection, we deﬁne a metric called inspecting
effort, which is the total number of inspected samples, nor-
malized by the number of true unseen family samples in the
testing set.
Baseline Methods. We include two main baselines. The
ﬁrst baseline is a standard Vanilla autoencoder [33], which
is used to illustrate the beneﬁt of contrastive learning. We
set the Vanilla autoencoder (AE) to have the same number
of layers and output dimensionality as CADE. We use it to
perform dimension reduction to map the inputs into a latent
space where we use the same MAD method to detect and
rank drifting samples. The difference between this baseline
and CADE is that the baseline does not perform contrastive
learning. The hyperparameter setting is in Appendix B.
The second baseline is Transcend [38]. As described in
Section 2, Transcend deﬁnes a “non-conformity measure” to
quantify how well the incoming sample ﬁts into the predicted
class, and calculate a credibility p-value to determine if the
incoming sample is a drifting sample. We obtain the source
2334    30th USENIX Security Symposium
USENIX Association
 0 0.2 0.4 0.6 0.8 1 0 100 200 300 400 500 600RateInspection Efforts (# of Samples)PrecisionRecall 0 0.2 0.4 0.6 0.8 1 0 100 200 300 400 500 600RateInspection Efforts (# of Samples)PrecisionRecall 0 0.2 0.4 0.6 0.8 1 0 100 200 300 400 500 600RateInspection Efforts (# of Samples)PrecisionRecall 0 0.2 0.4 0.6 0.8 1DrebinIDS2018F1 ScoreVanilla AETranscendCADE 0.6 0.8 1 1.2 1.4 1.6 1.8 2 2.2 2.4DrebinIDS2018Normalized Invest. EffortsVanilla AETranscendCADE(a) Original Space
(b) Latent space (Vanilla AE)
(c) Latent space (CADE)
Figure 7: T-SNE visualization for the original space, and latent spaces of Vanilla AE and CADE (unseen family: FakeDoc).
(a) Original space
(b) Latent space (CADE)
Figure 8: Boxplots of the distances between testing samples and their nearest centroids in both the original space and the latent
space for the Drebin dataset. Samples from previously unseen family are regarded as drifting samples.
ing precision and recall, the highest F1 score is 0.98. After
150 samples, the precision will drop since there are no more
unseen family samples in the remaining set. This conﬁrms the
high-quality of the ranked list, meaning almost all the samples
from the unseen family are ranked at the top.
As a comparison, the ranked lists of Transcend and Vanilla
AE are not as satisfying. For Transcend (Figure 4b), the ﬁrst
150 samples return low precision and recall, indicating the
top-ranked samples are not from the unseen family. After
inspecting 150 samples, we begin to see more samples from
the unseen family. After inspecting 350 samples, Transcend
has covered most of the samples from the unseen family
(i.e., with a recall near 1.0) but the precision is only 0.46.
This means more than half of the inspected samples by the
analysts are irrelevant. The best F1 score is 0.63. As shown in
Figure 4c, the performance of Vanilla AE is worse. The recall
is only slightly above 0.8, even after inspecting 600 samples.
To generalize the observation, we iteratively take each fam-
ily as the unseen family and compute the average statistics
across different settings for F1 score (in Figure 5) and normal-
ized inspecting efforts (in Figure 6). Table 3 further presents
the corresponding precision and recall. For each experiment
setting, we report the highest F1 score for each model. This
F1 score is achieved as the analysts go down the ranked list
and stop the inspection when they start to get a lot of false
positives. The “inspecting effort” refers to the total number of
inspected samples to reach the reported F1 score, normalized
by the number of true drifting samples in the testing set.
Table 3 conﬁrms that CADE can detect drifting samples
accurately and outperforms both baselines. On Drebin, the
average F1 score of CADE is 0.96, while the F1 scores for base-
lines are 0.80 and 0.72. A similar conclusion can be drawn
for the IDS2018 dataset. In addition, the standard deviation of
CADE is much smaller than that of baselines, indicating a more
consistent performance across different experiment settings.
Finally, we show that CADE has lower normalized inspecting
efforts, which conﬁrms the high quality of the ranking.
Note that the Transcend baseline actually performs well in
certain cases. For example, its F1 score is 99.69% (similar to
our system) when DoS-Hulk is set as the unseen family in the
IDS2018 dataset. However, the issue is Transcend’s perfor-
mance is not stable in different settings, which is reﬂected in
the high standard deviations in Table 3.
Impact of Contrastive Learning.
To understand the
source of the performance gain, we examine the impact of
contrastive learning. First, we present a visualization in Fig-
ure 7 which shows the t-SNE plot of the training samples
of the Drebin dataset and the testing samples from the cho-
sen unseen family (FakeDoc). T-SNE [66] performs its own
USENIX Association
30th USENIX Security Symposium    2335
−50−40−30−20−10010203040−40−2002040−50−40−30−20−10010203040−40−2002040−50−40−30−20−10010203040−40−2002040FakeInstallerDroidKungFuPlanktonGingerMasterBaseBridgeIconosysKminFakeDoc 2 3 4 5 6 7 8 9 10FakeInstallerDroidKungFuPlanktonGingerMasterBaseBridgeIconosysKminFakeDocDist. to nearest centroidMalware family used as unseen familyNon-driftDrift 0 1 2 3 4 5 6 7FakeInstallerDroidKungFuPlanktonGingerMasterBaseBridgeIconosysKminFakeDocDist. to nearest centroidMalware family used as unseen familyNon-driftDriftnon-linear dimensionality reduction to project data samples
into a 2-d plot. To visualize our data samples, we map the
samples from the original space (1,340 dimensions) to a 2-d
space (Figure 7a). We also map the samples from the latent
space (7 dimensions) to the 2-d space as a comparison (Fig-
ure 7b and Figure 7c). We can observe that samples in CADE’s
latent space have formed tighter clusters, making it easier to
distance existing samples from the unseen family.
To provide a statistical view of different experiment set-
tings, we plot Figure 8. Like before, we iteratively take one
family as the unseen family in Drebin. Then we measure the
distance of the testing samples to their nearest centroid in
the original feature space (Figure 8a) and the latent space
produced by CADE (Figure 8b). The results for the IDS2018
dataset have the same conclusion, and thus are omitted for
brevity. We show that drifting samples and non-drifting sam-
ples are more difﬁcult to separate in the original space. Af-
ter contrastive learning, the separation is more distinctive in
the latent space. The reason is that contrastive learning has
learned a suitable distance function that can stretch the sam-
ples from different classes further apart, making it easier to
detect unseen family.
Case Study: Limits of CADE.
CADE performs well in
most of the settings. However, we ﬁnd that in certain cases,
CADE’s performance suffers. For example, when using Fake-
Installer as the unseen family, our detection precision is only
82% when the recall gets to 100%. We notice that many test-
ing samples from GingerMaster and Plankton families were
detected as drifting samples. After a closer inspection, we
ﬁnd that, when FakeInstaller is treated as the unseen family,
in order to maintain the overall 80:20 training-testing ratio,
we need to split the dataset at the time when there were not
enough training samples from GingerMaster and Plankton
yet. Therefore, many of the testing samples from GingerMas-
ter and Plankton families look very different from the small
number of training samples in the two families (based on
the latent distance). External evidence also suggests that the
two families had many variants [5, 70]. While these malware
variants are not from a new family (false positives under our
deﬁnition), they could also have values for an investigation to
understand malware mutation within the same family.
5 Evaluation: Explaining Drifting Samples
To evaluate the explanation module, we randomly select one
family from each dataset (i.e. FakeDoc for Drebin and Inﬁl-
tration for IDS2018) as drifting samples. Results from other
settings have the same conclusion and thus are omitted for
brevity. Given this setting, we generate explanations for the
detected drifting samples and evaluate the explanation results,
both quantitatively and qualitatively.
Method Drebin-FakeDoc
Avg ± Std
5.363 ± 0.568
5.422 ± 1.773
3.960 ± 2.963
6.219 ± 3.962
0.065 ± 0.035
Original distance
Random
Boundary-based
COIN [43]
CADE
IDS2018-Inﬁltration
Avg ± Std
11.715 ± 2.321
11.546 ± 3.169
6.184 ± 3.359
8.921 ± 2.234
2.349 ± 3.238
Table 4: Comparison of explanation ﬁdelity based on the av-
erage distance between the perturbed sample and the nearest
centroid. A shorter distance is better. “Original distance” is
the distance between the drift sample and nearest centroid.
5.1 Experimental Setup
Baseline Method. We consider three baseline methods:
(1) a random baseline that randomly selects features as im-
portant features; (2) the boundary-based explanation method
described in Section 3, and (3) an unsupervised explanation
method called COIN [43]. Due to space limit, we only brieﬂy
describe how COIN works. COIN builds a set of local Lin-
earSVM classiﬁers to separate an individual outlier from its
in-distribution neighborhood samples. Since the LinearSVM
classiﬁers are self-explainable, they can pinpoint important
features that contribute to the outlier classiﬁcation. For a
fair comparison, we select the same number of top features
for baselines as our method. The implementation and hyper-
parameters of these baselines can be found in Appendix B.
Note that we did not select existing black-box explanation
methods (e.g., LIME [53] and SHAP [44]) as our comparison
baselines. This is because white-box methods usually perform
better than black-box methods thanks to their access to the
original model [67].
Evaluation Metrics.
Quantitatively, we directly evaluate
the impact of selected features on the distance changes. Given
a testing sample xxxt and an explanation method, we obtain
the selected features mmmt, where (mmmt )i = 1, if the ith feature is
selected as important, We quantify the ﬁdelity of this expla-
yt (cid:12)
xxxt = (cid:107) f (xxxt (cid:12) (1− mmmt ) + xxx(c)
nation result by this metric: d(cid:48)
mmmt )− cccyt(cid:107)2 where f , cccyt , and xxx(c)
yt have the same deﬁnition
as the ones in Eqn. (2). d(cid:48)
xxxt represents the latent distance be-
tween a perturbed sample of xxxt and its closet centroid cccyt .
The perturbed sample is generated by replacing the values of
the important features in xxxt with those of the training sample
closest to the centroid (i.e. xxx(c)
yt ). If the selected features are
truly important, then substituting them with the corresponding
features in the training sample from class yt will reduce the
distance between the perturbed sample and the centroid of cccyt .
In this case, a lower distance d(cid:48)
xxxt is better.
In addition to this d(cid:48)
xxxt metric, we also use a traditional
metric (Section 5.2) to examine the ratio of perturbed samples
that can cross the decision boundary.
2336    30th USENIX Security Symposium
USENIX Association
Drebin Case-A: Drifting Sample Family: FakeDoc; Closest Family: GingerMaster
[api_call::android/telephony/SmsManager;->sendTextMessage] , [call::readSMS] , [permission::android.permission.DISABLE_KEYGUARD] ,
[permission::android.permission.RECEIVE_SMS] , [permission::android.permission.SEND_SMS] , [permission::android.permission.WRITE_SMS] ,
[real_permission::android.permission.SEND_SMS] , [permission::android.permission.READ_SMS] , [feature::android.hardware.telephony] ,
[permission::android.permission.READ_CONTACTS] , [real_permission::android.permission.READ_CONTACTS] ,
[api_call::android/location/LocationManager;->isProviderEnabled], [api_call::android/accounts/AccountManager;->getAccounts],
[intent::android.intent.category.HOME], [feature::android.hardware.location.network], [real_permission::android.permission.RESTART_PACKAGES] ,
[real_permission::android.permission.WRITE_SETTINGS] , [api_call::android/net/ConnectivityManager;->getAllNetworkInfo],
[api_call::android/net/wiﬁ/WiﬁManager;->setWiﬁEnabled], [api_call::org/apache/http/impl/client/DefaultHttpClient],
[url::https://ws.tapjoyads.com/] , [url::https://ws.tapjoyads.com/set_publisher_user_id?] ,
[permission::android.permission.CHANGE_WIFI_STATE], [real_permission::android.permission.ACCESS_WIFI_STATE],
[real_permission::android.permission.BLUETOOTH], [real_permission::android.permission.BLUETOOTH_ADMIN], [call::setWiﬁEnabled].
Table 5: Case study of explaining why a given sample a drifting sample. The highlighted features represent those that match the
semantic characteristics that differentiate the drifting sample with the closest family.
Method Drebin-FakeDoc
Random
Boundary-based
COIN [43]
CADE
0%
0%
0%
97.64%
IDS2018-Inﬁltration
0%
0.41%
0%
1.41%
Table 6: Comparison of explanation ﬁdelity based on the
ratio of perturbed samples that cross the decision boundary. A
higher ratio means the perturbed features are more important.
5.2 Fidelity Evaluation Results
Feature Impact on Distance.
Table 4 shows the mean
and standard deviation for d(cid:48)
xxxt of all the drifting samples (i.e.,
the distance between the perturbed samples to the nearest
centroid). We have four key observations. First, perturbing
the drifting samples based on the randomly selected features
almost does not inﬂuence the latent space distance (compar-
ing Row 2 and 3). Second, the boundary-based explanation
method could lower the distance by 26%–47% across two
datasets (comparing Row 2 and 4). This suggests this strat-
egy has some effectiveness. However, the absolute distance
values are still high. Third, COIN reduces the latent space
distance on the IDS2018 dataset (comparing Row 2 and 5),
but it somehow increases the average distance in the Drebin
dataset. Essentially, COIN is a specialized boundary-based
method that uses a set of LinearSVM classiﬁers to approx-
imate the decision boundary. We ﬁnd COIN does not work
well on the high-dimensional space, and it is difﬁcult to drag
the drifting sample to cross the boundary (will be discussed in
Section 5.3). Finally, our explanation module in CADE has the
lowest mean and standard deviation for the distance metric.
The distance has been reduced signiﬁcantly from the origi-
nal distance (i.e. 98.8% on Drebin and 79.9% on IDS2018,
comparing Row 2 and 6). In particular, CADE signiﬁcantly out-
performs the boundary-based explanation method. Since our
method overcomes the sample sparsity and imbalance issues,
it pinpoints more effective features that have a larger impact
on the distance (which affects the drift detection decision).
Number of Selected Features.
Overall, the number of
selected features is small, which makes it possible for manual
interpretation. As mentioned, we conﬁgure all the methods to
select the same number of important features (as CADE). For
the Drebin dataset, on average the number of selected features
is 44.7 with a standard deviation of 6.2. This is considered a
very small portion (3%) out of 1000+ features. Similarly, the
average number of selected features for the IDS2018 dataset
is 16.2, which is about 20% of all the features.
5.3 Crossing the Decision Boundary