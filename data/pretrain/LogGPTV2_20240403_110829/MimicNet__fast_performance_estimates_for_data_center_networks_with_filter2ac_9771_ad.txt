time-series of active flow-level demand, and draw packets randomly
from that demand using the derived distributions.
Crucially, when feeding packets, the feeders generate ‘packets’
independently, pass their raw feature vectors to the internal models,
and immediately discard any output. This means that internal mod-
els’ hidden state is updated as if the packets were routed without
actually incurring the costs of creating, sending, or routing them.
While this approach shares the weaknesses of other flow-level ap-
proximations, like the removal of intra-cluster traffic, these packets
are never directly measured and, thus, an approximation of their
effect is sufficient. Further, while the traffic is never placed in the
293
 0 1.1s.2s.3s.4s.5s.6s.7s.8s.9s1s 0 1.1s.2s.3s.4s.5s.6s.7s.8s.9s1s 0 1.1s.2s.3s.4s.5s.6s.7s.8s.9s1s 0 1.1s.2s.3s.4s.5s.6s.7s.8s.9s1s 0 0.001 0.002 0.003 0.004 0.005 0.006.1s.2s.3s.4s.5s.6s.7s.8s.9s1s 0 0.001 0.002 0.003 0.004 0.005 0.006.1s.2s.3s.4s.5s.6s.7s.8s.9s1s 0 0.001 0.002 0.003 0.004 0.005 0.006.1s.2s.3s.4s.5s.6s.7s.8s.9s1s 0 0.001 0.002 0.003 0.004 0.005 0.006.1s.2s.3s.4s.5s.6s.7s.8s.9s1sSIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
Q. Zhang et al.
For every tested parameter set, MimicNet trains a set of models
and runs validation tests to evaluate the resulting accuracy and its
scale-independence. Specifically, MimicNet runs an approximated
and full-fidelity simulation on a held-out validation workload in
three configurations: 2, 4, and 8 clusters. It then compares the two
versions using the user’s target metric.
The full-fidelity comparison results are only gathered once, and
the MimicNet results are evaluated for every parameter set, but the
sizes are small enough that the additional time is nominal. Based on
the user-defined metric, MimicNet uses Bayesian Optimization (BO)
to pick the next parameter set that has the highest ‘prediction un-
certainty’ via an acquisition function of EI (expected improvement).
In this way, BO quickly converges on the optimal configuration.
MimicNet supports two classes of metrics natively.
MSE-based metrics. For 1-to-1 metrics, MimicNet provides a frame-
work for computing MSE. For example, when comparing the FCT
of the same flow in both simulations:
MSE =
1
|Flows|
(realFCT𝑓 − mimicFCT𝑓 )2

𝑓 ∈Flows
A challenge in using this class of metrics is that the set of completed
flows in the full-fidelity network and MimicNet are not necessarily
identical—over a finite running timespan, flow completions that are
slightly early/late can change the set of observed FCTs. To account
for this, we only compute MSE over the intersection, i.e.,
Flows = {𝑓 | (∃ realFCT𝑓 ∧ (∃ mimicFCT𝑓 )}
By default, MimicNet ignores models with overlap < 80%.
Wasserstein-based metrics. Unfortunately, not all metrics can
be framed as above. Consider per-packet latencies. While in train-
ing we assume that we can calculate a per-packet loss and back-
propagate, in reality when a drop is mistakenly predicted, the next
prediction should reflect the fact that there is one fewer packet in
the network, rather than adhering to the original packet trace. In
some protocols like TCP, the loss may even cause packets to appear
in the original but not in any MimicNet version or vice versa.
MimicNet’s hyper-parameter tuning phase, therefore, allows
users to test distributions, e.g., of RTTs, FCTs, or throughput, via
the Wasserstein metric. Also known as the Earth Mover’s Distance,
the metric quantifies the minimum cost of transforming one distri-
bution to the other [16]. Specifically, for a one-dimensional CDF,
the metric (𝑊1) is:
𝑊1 =
|CDFreal(𝑥) − CDFmimic(𝑥)|
∫ +∞
−∞
𝑊1 values are scale-dependent, with lower numbers indicating
greater similarity.
8 PROTOTYPE IMPLEMENTATION
We have implemented a prototype of the full MimicNet workflow
in C++ and Python on top of PyTorch/ATen and the OMNeT++ [34]
simulation suite. Given an OMNeT++ router and host implemen-
tation, our prototype will generate training data, train/hypertune
a set of MimicNet models, and compose the resulting models into
an optimized, full-scale simulation. This functionality totals to an
additional 25,000 lines of code.
294
Simulation framework. MimicNet is built on OMNeT++ v4.5 and
INET v2.4 with custom C++ modules to incorporate our machine
learning models into the framework. To ensure that the experiments
are repeatable, all randomness, including the seeds for generating
the traffic are configurable. They were kept consistent between
variants and changed across training, testing, and cross validation.
Parallel execution. A side benefit MimicNet is that it significantly
reduces the need for synchronization in a parallel execution. In
order to take advantage of this property, we parallelize each cluster
of the final simulation using an open-source PDES implementation
of INET [51].
Machine learning framework. Our LSTM models are trained
using PyTorch 0.4.1 and CUDA 9.2 [41, 44]. Hyperparameter tuning
was done with the assistance of hyperopt [2]. At runtime, Mimic
cluster modules accept OMNeT++ packets, extract their features,
perform a forward step of the LSTMs, and forward the packet via
ECMP based on the result. For speed, our embedded LSTMs were
custom-built inference engines that leverage low-level C++ and
CUDA functions from the Torch, cuDNN, and ATen libraries.
9 EVALUATION
Our evaluation focuses on several important properties of MimicNet
including: (1) its accuracy of approximating the performance of
data center networks, (2) the scalability of its accuracy to large
networks, (3) the speed of its approximated simulations, and (4) its
utility for comparing configurations.
Methodology. Our simulations all assume a FatTree topology, as
described in Section 2. We configured the link speed to be 100 Mbps
with a latency of 500 𝜇s. To scale up and down the data center,
we adjusted the number of racks/switches in each cluster as well
as the number of clusters in the data center. We note that higher
speeds and larger networks were not feasible due to the limitation
of needing to evaluate MimicNet against a full-fidelity simulation,
which would have taken multiple years to produce even a single
equivalent execution.
The base case uses TCP New Reno, Drop Tail queues, and ECMP.
To test MimicNet’s robustness to different network architectures,
we use a set of protocols: DCTCP [6], Homa [40], TCP Vegas [9],
and TCP Westwood [36] that stress different aspects of MimicNet.
Our workload uses traces from a well-known distribution also used
by many recent data center proposals [6, 40]. By default, the traffic
utilizes 70% of the bisection bandwidth and the mean flow size is
1.6 MB. All experiments were run on CloudLab [47] using machines
with two Intel Xeon Silver 4114 CPUs and an NVIDIA P100 GPU.
When evaluating flow-level simulation, we use the SimGrid [10]
v3.25 and its built-in FatTreeZone configured with the same topol-
ogy and traffic demands as full/MimicNet simulation.
Evaluation metrics. As mentioned in Section 7.2, traditional per-
prediction metrics like training loss are not useful in our context.
Instead, we leverage three end-to-end metrics: (1) FCT, (2) per-
server Throughput binned into 100 ms intervals, (3) and RTT. In the
flow-level simulation, FCT is computed using flow start/end times,
Throughput is computed with a custom load-tracking plugin, and
RTT is not possible to compute. In MimicNet and full simulation,
all three are computed by instrumenting the hosts in the observable
MimicNet: Fast Performance Estimates for DCNs with ML
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
s
r
e
t
s
u
l
C
2
s
r
e
t
s
u
l
C
8
2
1
(a) FCT distribution
(b) Throughput distribution
(c) Packet RTT distribution
(d) FCT distribution
(e) Throughput distribution
(f) Packet RTT distribution
Figure 7: The accuracy of MimicNet in the baseline configuration for 2 clusters and 128 clusters. Also shown are results from
SimGrid and the assumption that small-scale results are representative. 𝑊1 to ground truth is shown in parentheses. We an-
notate the 99-pct value of each metric for every approach at the tail in 128 clusters.
Figure 8: Throughput Scalability.
cluster to track packets sends and ACK receipts. Where applicable,
we compare CDFs using a 𝑊1 metric.
9.1 MimicNet Models Clusters Accurately
We begin by evaluating MimicNet’s accuracy when replacing a
single cluster with a Mimic before examining larger configurations
in the next section. Note that in this configuration, there is no need
for feeder models. Rather, this experiment directly evaluates the ef-
fect of replacing a cluster’s queues, routers, and cluster-local traffic
with an LSTM. For this test, we use the baseline set of protocols
described above. The final results use traffic patterns that are not
found in the training or hyper-parameter validation sets.
Figure 7a–c show CDFs of our three metrics for this test. As the
graphs show, MimicNet achieves very high accuracy on all metrics.
The LSTM is able to learn the requisite long-term patterns (FCT
and throughput) as well as packet RTTs. Across the entire range,
MimicNet’s CDFs adhere closely to the ground truth, i.e., the full-
fidelity, packet-level simulation; just as crucial, the shape of the
curve is maintained. Flow-level simulation behaves much worse.
9.2 MimicNet’s Accuracy Scales
A key question is whether the accuracy translates to larger compo-
sitions where traffic interactions become more complex and feeders
are added. We answer that question using a simulation composed
of 128 clusters (full-fidelity simulation did not complete for larger
Figure 9: RTT Scalability. Flow-level simulation is too
coarse-grained to provide this metric.
sizes). In MimicNet, 127 clusters are replaced with the same Mimics
as the previous subsection. Figure 7d–f show the resulting accuracy.
There are a couple of interesting observations.
First, while the accuracy of MimicNet estimation does decrease,
the decrease is nominal. More concretely, for FCT, throughput, and
RTT, we find 𝑊1 metrics of 0.113, 7561, and 0.00158 compared to
the ground truth, respectively. For reference, we also plot SimGrid
and the original 2-cluster simulation’s results. The 𝑊1 error be-
tween 2-cluster simulation and 128-cluster groundtruth are 311%,
457%, and 70% higher than MimicNet’s values; the 𝑊1s of FCT and
throughput between SimGrid and the groundtruth are similarly
high. The results indicate that our composition methods are suc-
cessfully approximating the scaling effects. Critically, MimicNet
also predicts tails well: the p99 of MimicNet’s FCT, throughput, and
RTT distributions are within 1.8%, 3.3%, and 2% of the true result.
We evaluate MimicNet’s scalability of accuracy more explicitly
in Figures 1, 8, and 9. Here, we plot the 𝑊1 metric of all three
approaches for several data center sizes ranging from 4 to 128.
Recall that the 2-cluster results essentially hypothesize that FCT,
throughput, and RTT do not change as the network scales. An
upward trend on their𝑊1 metric in all three graphs suggests that the
opposite is true. Compared to that baseline, MimicNet on average
achieves a 43% lower RTT𝑊1 error, 78% lower throughput error, and
63% lower FCT error. In all cases, MimicNet also shows much lower
295
 0 0.2 0.4 0.6 0.8 1 0.001 0.01 0.1 1 10 100Fraction of FlowsFlow Completion Time (s)GroundtruthMimicNet (0.108)Flow-level (0.277) 0 0.2 0.4 0.6 0.8 1100101102103104105106107FractionThroughput (Bps)GroundtruthMimicNet (5256)Flow-level (61614) 0 0.2 0.4 0.6 0.8 1 0.001 0.01 0.1 1 10Fraction of PacketsLatency (s)GroundtruthMimicNet (0.00118) 0 0.2 0.4 0.6 0.8 1 0.001 0.01 0.1 1 10 100Groundtruth: 10.77MimicNet: 10.97Flow-level: 7.53Small-scale: 12.95Fraction of FlowsFlow Completion Time (s)GroundtruthMimicNet (0.113)Flow-level (0.501)Small-scale (0.464) 0 0.2 0.4 0.6 0.8 1100101102103104105106107Groundtruth: 213KMimicNet: 221KFlow-level: 158KSmall-scale: 441KFractionThroughput (Bps)GroundtruthMimicNet (7561)Flow-level (21787)Small-scale (42115) 0 0.2 0.4 0.6 0.8 1 0.0001 0.001 0.01 0.1 1 10Groundtruth: 0.0307MimicNet: 0.0302Small-scale: 0.0183Fraction of PacketsLatency (s)GroundtruthMimicNet (0.00158)Small-scale (0.00269)05K10K15K20K25K30K35K40K48163264128W1 to Ground TruthNetwork Size (#Clusters)Small-scaleFlow-levelMimicNet05.0e-41.0e-31.5e-32.0e-32.5e-33.0e-33.5e-348163264128W1 to Ground TruthNetwork Size (#Clusters)Small-scaleMimicNetSIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
Q. Zhang et al.
(a) 8 clusters
(b) 16 clusters
(c) 32 clusters
(d) 64 clusters
(e) 128 clusters
Figure 10: Simulation running time speedup brought by MimicNet in different sizes of data centers. In a network of 128 clusters
(256 racks), MimicNet reduces the simulation time from 12 days to under 30 minutes, achieving more than two orders of
magnitude speedup. The speedups are consistent and stable across different workloads.
variance across workloads, demonstrating better predictability at
approximating large-scale networks.
Factor
Small-scale simulation
Large-scale simulation
Time
1h 3m
7h 10m
25m
9.3 MimicNet Simulates Large DCs Quickly
Equally important, MimicNet can estimate performance very quickly.
The multiple phases of MimicNet—small-scale simulation, model
training, hyper-parameter tuning, and large-scale composition—
each require time, but combined, they are still faster than running
the full-fidelity simulation directly. By paying the fixed costs of the
first two phases, the actual simulation can be run while omitting
the majority of the traffic and network connections.
Execution time breakdown. Table 2 shows a breakdown of the
running time of both the full simulation and MimicNet, factored
out into its three phases for the 128 cluster, 1024 host simulation in