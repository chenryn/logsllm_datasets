matches a 4-mer on a reference substring when the edit dis-
tance between them is 3; once this match is found, we can
align these two strings to check whether the distance is in-
deed no more than 3. Our idea is to let the public cloud take
care of the seeding, roughly locating reads, which enables
the private cloud to quickly extend each read at a relatively
small set of positions (rather than at each of the 6 billion po-
sitions on the reference genome). The extension is done by
a linear algorithm that calculates the edit distances no more
than a threshold [28].
2.2 Privacy threat
Threat. The main threat to human genomic data is iden-
tiﬁcation of the individual the DNA comes from. Since
Reference genomeReadAGTTGGAGCCGTTAATInsert CDelete GReplace G with CMatchACTTGGAGTCCTTACAsuch data is often produced by clinic studies, its donor, once
identiﬁed, could be linked to the disease under the study,
which can have serious consequences such as denial of ac-
cess to health/life insurance, education, and employment.
The Health Insurance Portability and Accountability Act
(HIPAA) requires removal of explicit identiﬁers (such as
name, social security number, etc.) before health data can
be released [11]. This protection, however, was found to be
insufﬁcient for genomic data, as re-identiﬁcation can still
happen through examining the genetic markers related to
the donor’s observable features (i.e., phenotypes) after the
genetic locations of reads are recovered, a concern shared
by the NIH [12].
The genetic variation most widely-used for such iden-
tiﬁcation is single-nucleotide polymorphism (SNP). SNP
occurs when a single nucleotide (A, T, C or G) differs
between the members of a species.
It can take one of
two alleles, either 0 (major) or 1 (minor). This variation
has been employed by all existing re-identiﬁcation tech-
niques [32, 34, 46, 56]. Theoretically, other variations, in-
cluding rare alleles [48] and copy number variation (vari-
ations over sections of DNA) [13], could also be used to
identify an individual. However, a critical barrier for the
identiﬁcation based on rare variations is that they are not
mapped at the whole genome scale in any reference popu-
lation. Also note that to pose the same level of the threat,
those variations need a larger population: given that rare-
allele frequencies are much lower than those of SNPs, a
population 100 or even 1000 times larger than the HapMap
population (about 270 people in Phase I and II) may need
to publish their whole genomes for accurate estimate of the
frequencies, which may not be likely given the increasing
awareness of the privacy implication of DNA data. Note
that under the protection of our technique, an adversary can-
not infer the rare alleles from the data on the public cloud
(i.e.
the keyed-hash values of l-mers, not their content),
because without known rare allele frequencies in a refer-
ence population, the hash values of these l-mers are indis-
tinguishable from those not belonging to human, such as
those of microbes. For copy-number variations, a recent
study [22] shows that their identiﬁcation power is way lower
than SNPs, due to their small numbers, much lower density
in comparison to SNPs, and continuous distributions [42].
Therefore, it is commonly believed that the threat can be
easily mitigated by simple approaches like data aggrega-
tion. So far, no known re-identiﬁcation techniques use such
variations.
Challenges in outsourcing read mapping. To enable read
mapping to happen on the public cloud, we need to either
encrypt the data or anonymize it, making it unidentiﬁable.
Unfortunately, only a few cryptographic approaches [17,18,
20, 33, 35, 55] support secure calculation of edit distances,
and all of them are too expensive to sustain a large-scale
computation like read mapping (Section 5). Less clear is
the efﬁcacy of simple data anonymization techniques such
as aggregation, noise adding and data partition. These tech-
niques have long been used to protect genomic data. As
a prominent example, Genome-Wide Association Studies
(GWAS) typically use DNA microarrays to proﬁle a set
of pre-determined SNPs from a group of patients (called
case) to study the genetic traits of their common disease.
The SNPs from different case individuals were often ag-
gregated into a mixture, from which nothing but the counts
(or equivalently, the frequencies) of different SNP values
(alleles) can be observed and used for statistical analysis.
Such aggregate data was deemed safe to release. How-
ever, recent studies show that the data is actually vulnerable
to a type of re-identiﬁcation attacks [32, 34, 46, 56]: given
the allele frequencies of a reference population, which can
be acquired from public sources such as the International
HapMap Project [9], and a DNA sample from an individ-
ual, her presence in the case population can be determined
from the aggregate data through a statistical test.
Different from the microarray data studied in the prior
work [32,34,46,56], the SNP sets covered by the reads from
two persons often differ signiﬁcantly, due to the randomness
in sequencing. In our research, we systematically evaluated
the effects of these anonymization techniques on read data
using a near-optimal test statistic proposed in the prior re-
search [32]. Our ﬁndings show that such data is equally
vulnerable to the re-identiﬁcation attack. For example, to
ensure that no more than 10% of a case group can be iden-
tiﬁed at a conﬁdence of 0.99, we found that the reads from
about 38000 individuals, with 10 million reads each, need
to be aggregated. The details of this study is presented in
Appendix 7.1.
3 Secure Read Mapping
3.1 Overview
Figure 2. High-level design of our system.
Our design. The high-level design of our techniques is il-
Private CloudPublic CloudEncrypted Query SeedsSeeding ResultsSorted Query SeedsPre-sorted ReferenceSeedingExtendinglustrated in Figure 2. Our approach is built upon a hybrid
cloud: the public commercial cloud is delegated the compu-
tation over the keyed hash values of read data, while the pri-
vate cloud directly works on the data. Our idea is to let the
private cloud undertake a small amount of the workload to
reduce the complexity of the secure computation that needs
to be performed on the public cloud, while still having the
public cloud shoulder the major portion of a mapping task.
To this end, we divided the task according to the seed-and-
extend strategy [19]. The seeding part roughly locates a
read on the reference genome by matching a small segment
of it to the substrings on the genome. For example, given an
edit distance of 3 and reads of 100 bps, we can partition each
read into 4 segments (seeds), each of 25 bps. At least one
of these seeds will match a 25-mer on the read’s counterpart
on the reference genome. Searching for this position is done
over the keyed-hash values of the seeds and 25-mers: we
ﬁrst extract all 25-mers from the reference genome, remove
repeated sequences and ﬁngerprint the distinctive ones with
a cryptographic hash function and a secret key; these refer-
ence hash values are compared with the hash values of the
seeds on the public cloud; all the matches found are reported
to the private cloud, which extends the reads from the ge-
netic positions (on the reference genome) indicated by these
matches to ﬁnd an optimal alignment for each read, using a
threshold edit-distance algorithm (Section 3.2).
This basic approach works when the seeds are long
enough to suppress random matches: for example, most 25-
mers are unique across the genome, so a 25-bp seed often
locates a read at a small set of positions, which reduces the
workload of calculating edit distances between millions of
reads and billions of reference 25-mers to the extensions
that just need to happen at millions of positions. However,
the seeds can be short in practice. For example, given an
edit distance of 6, the seed for an 100-bp read has only 14
bps and often matches thousands of positions. To address
this problem, we improve the basic approach to perform the
seeding over keyed-hash values for 2-combinations of 12-
bp seeds, which signiﬁcantly reduces the workload of the
private cloud at a spatial cost easily affordable by modern
clouds (Section 3.3).
The privacy assurance of our techniques is evaluated by
the amount of information the public cloud can infer from
the data it observes. To achieve an ultra-fast mapping, we
adopted keyed hash, which allow the public cloud to uni-
laterally determine whether a match happens. This, how-
ever, brings in the concern about a frequency analysis that
infers the content of l-mers by counting the matches their
hashes receive. To assess this risk, we conducted a whole
genome study, which demonstrates that the adversary ac-
tually cannot achieve any re-identiﬁcation power through
such an analysis (Section 3.4).
Adversary model. We consider an adversary who aims at
re-identifying the individuals related to read data. As dis-
cussed before, we focus on this re-identiﬁcation threat be-
cause it is the main privacy concern for releasing protected
health information [32,34,46,56], which includes sequenc-
ing reads, and therefore the major barrier to moving read
mapping to the public cloud. We assume that the private
cloud is trustworthy while the nodes on the public cloud can
be compromised and controlled by the adversary. Also, we
assume that the adversary has a DNA sample of a testee, the
person she wants to identify from a read dataset, and a refer-
ence population genetically similar to those whose reads are
inside the dataset. Access to such knowledge is widely con-
sidered to be a very strong assumption [30] that gives the
adversary advantages. It serves as the standard background
information in all studies on re-identiﬁcation threats to ge-
nomic data [32,34,46,56], and the foundation for evaluating
whether such data can be safely released [10, 46]. Finally,
note that all we care about here is to prevent the adversary
from identifying read donors, rather than to ensure the suc-
cess of the computation, which may not be achieved when
the adversary controls the public cloud.
3.2 Computation Split
In this section, we elaborate the basic design of our tech-
nique, which is also summarized in Table 1.
Data preprocessing. To perform the seeding on the pub-
lic cloud, we need to compute the keyed-hash values for
both the reference genome and individual seeds. Speciﬁ-
cally, given a keyed hash function HK() with a secret key
K, our approach ﬁrst ﬁngerprints all distinctive l-mers αi
on the reference genome: HK(α1), HK(α2), ··· and then
sends their hash values (in a random order) to the public
cloud. We remove all repeats here to mitigate the risk of
a frequency analysis. Depending on the length l, l-mers
have different levels of repetitions on a human genome:
for example, we found that more than 80% of 24-mers are
unique on the reference genome. Note that such data pro-
cessing and transfer only need to be done once to enable the
public cloud to map a large number of read datasets using
the hashed reference. For each dataset, we compute keyed
hashes for the seeds sj extracted from each read, HK(s1),
HK(s2), ··· , HK(sd+1), randomly permutate this list and
then deliver it to the public cloud for the seeding operation.
Our implementation adopts SHA-1 and a 256-bit secret key,
and only uses the ﬁrst 10 bytes of an l-mer’s hash as its
ﬁngerprint for the comparison. The following 6 bytes are
XORed with the information for locating the l-mer on the
reference genome (we only use 16 bytes of 20-byte output).
Computing on the public cloud. The seeding task dele-
gated to the public cloud is as simple as comparing all the
hashes of the seeds with those of the reference l-mers and
reporting the indices of the matched sequence pairs (i, j)
Table 1. Privacy-Preserving Read Mapping: the Basic Approach
λ, set l = λ
l-mer’s position). Remove all the repeats and send the distinctive hash values to the public cloud.
• Generating keyed hashes for reference l-mers (one-time cost). Given an edit-distance threshold d and a read length
d+1. For each distinctive l-mer αj on the reference genome, compute HK(αj) (j is unrelated to the
• Generating keyed hashes for seeds. On the private cloud, for each read in a given read dataset, break it into d + 1
seeds with a length of l each. Compute HK(si) for every seed si and send all these hash values to the public cloud.
• Seeding. On the public cloud, compare the hashed seeds to the hashed references. For all HK(αj) = HK(si), send
• Extension. On the private cloud, for every matched pair (i, j), extend the read including si from the genetic location
pinpointed by αj on the reference genome to check whether the edit distance (between the read and the substring
on that location) is no more than d.
(i, j) to the private cloud.
to the private cloud. The only problem here is the scale
of this computation, which involves millions upon billions
of string comparisons. One way to achieve a fast seed-
ing is to build an index for the reference genome, as typ-
ically done by the fast-mapping software designed to work
on standalone systems. This approach, however, needs a
huge amount of memory and cannot be easily parallelized.
Our implementation uses ultra-fast sorting, which the cloud
is good at, to do the seeding. Speciﬁcally, the public cloud
pre-sorts the reference l-mers according to their hash val-
ues. For every batch of seed hashes, the cloud ﬁrst sorts
them and then merges them with the sorted l-mer hashes
to ﬁnd matches. This strategy has also been adopted by
CloudBurst [47], a famous cloud-based (yet insecure) map-
ping system. Today’s cloud can already support high-
performance sorting: for example, Terasort [44] running
on Hadoop attained a sorting speed of 0.578 terabytes per
minute.
Computing on the private cloud. The private cloud ex-
tends the seeds at the locations where matches happen.
These locations are recovered from the indices of seed
hashes and the l-mer hashes they match, as reported by
the public cloud. For this purpose, two look-up tables are
needed: one maps the hash of an l-mer to its occurrences
on the reference genome, and the other maps that of a seed
to its read. The ﬁrst table is relatively large, at least 10
GB. We reduced the size of the table using the features of
human genomes. When l goes above 20, most l-mers are
unique across the reference genome. Particularly, only a
small portion (below 20%) of 24, 25-mers repeat. For every
unique l-mer αi, our approach keeps its location informa-
tion directly on the last 6 bytes of HK(αi). Speciﬁcally,
let θi be these bytes; we XOR the location of αi, Li, onto
θi: πi = θi ⊕ (Ii||Li), where Ii is a one-byte indicator for
the uniqueness of the l-mer. Once this l-mer is matched by
a seed (that is, the ﬁrst 10 bytes of the seed’s hash match-
ing the ﬁrst 10 bytes on the l-mer’s hash), Li is recovered
from πi using θi, which comes from the hash of the seed
and is kept on the private cloud. For those still in the table,
we organize them according to the indices of their hashes to
ensure that only sequential access happens when searching
the table.
When the read dataset is relatively small (10 million
reads or less), its look-up table, which does not go above 1.2
GB, can often be accommodated in the memory. In the ta-
ble, we also keep the last 6 bytes of seed hashes for decrypt-
ing the location information of the l-mers they matched. To
handle a large dataset, our design encrypts the read infor-
mation Rj for a seed sj using a simple stream cipher such
as AES CTR. Speciﬁcally, we ﬁrst compute the key-stream
σj = EK(cid:48)(V ||j), where E() is an encryption algorithm, V
is an initial vector and K(cid:48) is another secret key, and then