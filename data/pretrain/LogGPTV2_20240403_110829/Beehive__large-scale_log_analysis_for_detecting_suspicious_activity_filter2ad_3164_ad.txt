19
34
29
0
2
2
1
2
2
0
2
1
1
2
1
0
114
47833
418
16373
64
3832
386
213
48
48
Table 3: Feature vectors for hosts in two example clusters. Each row corresponds to the feature vector for a
host in that cluster. The last row shows the maximum value for each feature across all clusters on April 24,
2013.
App Engine. Several hosts were found to be mining bitcoins.
We labeled all of these hosts as “Other - Automated.”
Finally, 8.03% of the incidents either had high numbers
(thousands per day) of consented connections, indicating
that the user had explicitly acknowledged the company’s
network policies before proceeding to the site, or long pe-
riods (i.e., lasting several hours) of continuous web traﬃc
to various domains. Without further evidence of malicious
behavior, we classiﬁed these incidents in the “Other - Brows-
ing” category, though they may not be entirely user-driven.
4.3
Investigation by the SOC
The ﬁrst-round manual labeling process described in Sec-
tion 4.2 yielded 81 incidents (10.33%) for which we could
not identify the applications responsible for the observed
behavior in the incident. The hosts in those incidents com-
municated with low-reputation websites (e.g., ﬂagged by an-
tivirus vendors, registered within the past six months, with
DGA-like subdomains), downloaded executables or zipped
ﬁles, used malformed HTTP user-agent strings, or repeat-
edly contacted the same URL at semi-regular intervals.
We labeled these incidents as “suspicious,” and presented
them to the enterprise SOC for a second round of analysis.
Table 5 shows the labels the SOC assigned to these incidents.
While 54.32% of the “suspicious” incidents were conﬁrmed
SOC label
# of suspicious incidents
Adware or Spyware
Further investigation
Other malware
Policy Violation - Gaming
Policy Violation - IM
Policy Violation - Streaming
Other - Uncategorized sites
35
26
9
1
1
2
7
43.21%
32.09%
11.11%
1.23%
1.23%
2.47%
8.64%
Table 5: “Suspicious” incidents categorized by the
SOC at EMC.
by the SOC as adware, spyware, or known malware, a signiﬁ-
cant fraction (32.09%) were considered serious incidents and
merit further investigation. These questionable activities
are potentially previously unknown malware or other threats
opaque to state-of-the-art security tools, and in-depth host
inspection is necessary to determine the exact root cause.
A small number of the investigated “suspicious” incidents
were identiﬁed as policy violations (4.93%) or uncategorized
sites (8.64%). Their association with obscure software or
low-reputation sites prompted their identiﬁcation as “suspi-
cious” in the ﬁrst-round investigation.
206
4.4 Summary
With assistance from the enterprise SOC, we manually
investigated 784 Beehive incidents generated over the course
of two weeks. Overall, we ﬁnd 25.25% of the incidents to be
malware-related or that warrant further SOC investigation,
39.41% to be policy violations, and 35.33% associated with
unrecognized, but automated, software or services. Only 8
of the 784 incidents (1.02%) were detected by existing state-
of-the art security tools, demonstrating Beehive’s ability to
identify previously unknown anomalous behaviors.
5. RELATED WORK
Network and host-based intrusion detection systems that
use statistical and machine learning techniques have seen
two decades of extensive research [11, 15, 33]. In addition,
both commercial and open source products that combine sig-
nature, traﬃc and anomaly inspection techniques are widely
available today [1, 2, 3]. To the best of our knowledge,
though, we present the ﬁrst study of the challenges of san-
itizing, correlating and analyzing large-scale log data col-
lected from an enterprise of this scale, and of detecting both
compromised hosts and business policy violations.
There are many existing systems that aim to help human
analysts detect compromised hosts or stolen credentials, and
also a number of case studies on enterprise networks. For
example, Chapple et al. [12] present a case study on the
detection of anomalous authentication attempts to a uni-
versity virtual private network using a clustering technique
focused on geographic distance. Zhang et al. [41] extend
this approach with additional machine-learning features to
automatically detect VPN account compromises in univer-
sity networks.
In an earlier study, Levine et al. [26] use
honeypots to detect exploited systems on enterprise net-
works. Similarly, Beehive aims to automate detection tasks
for security analysts.
In addition to new detection meth-
ods, however, we present the largest-scale case study so far
on a real-life production network, together with the unique
challenges of analyzing big and disjoint log data.
While there exists work that analyzes a speciﬁc type of
malicious activity on the network (e.g., worms [37] or spam-
mers [32]), recent attempts at malware detection mainly
focus on detection of botnets. For instance, several sys-
tems [10, 27, 35] use classiﬁcation and correlation techniques
to identify C&C traﬃc from IRC botnets. BotTrack [16]
combines a NetFlow-based approach with the PageRank al-
gorithm to detect P2P botnets. A number of studies mon-
itor crowd communication behaviors of multiple hosts and
analyze the spatial-temporal correlations between them to
detect botnet infected hosts independent of the protocol
used for C&C communication [18, 20, 24, 40], while others
speciﬁcally focus on DNS traﬃc similarity [13, 36]. BotH-
unter [19] inspects the two-way communications at the net-
work perimeter to identify speciﬁc stages of botnet infec-
tion. DISCLOSURE [8] presents a set of detection features
to identify C&C traﬃc using NetFlow records. Other works
focus on tracking and measuring botnets [4, 14, 17, 22, 34].
Other work aims to identify domains that exhibit mali-
cious behavior instead of infected hosts. For example, some
systems [30, 29, 31, 21] propose various detection mecha-
nisms for malicious fast-ﬂux services.
In a more general
approach, EXPOSURE [9], Notos [5], and Kopis [6] perform
passive DNS analysis to identify malicious domains. Ma et
al. [28] extract lexical and host-based URL features from
spam emails and use active DNS probing on domain names
to identify malicious websites. Another branch of research
aims to detect dynamically generated malicious domains by
modeling their lexical structures and utilizing the high num-
ber of failed DNS queries observed in botnets using such do-
mains [7, 38, 39]. In comparison, Beehive uses standard log
data to detect suspicious activity in an enterprise network.
These approaches use network data for detection. In con-
trast, Beehive uses dirty enterprise log data to detect po-
tentially malicious host behavior as well as policy violations
speciﬁc to an enterprise setting.
6. CONCLUSIONS
In this paper, we presented a novel system called Bee-
hive that attacks the problem of automatically mining and
extracting knowledge in a large enterprise from dirty logs
generated by a variety of network devices. The major chal-
lenges are the “big data” problem (at EMC, an average of 1.4
billion log messages—about 1 terabyte—are generated per
day), and the semantic gap between the information stored
in the logs and that required by security analysts to detect
suspicious behavior.
We developed eﬃcient techniques to remove noise in the
logs, including normalizing log timestamps to UTC and cre-
ating an IP-to-hostname mapping that standardizes host
identiﬁers across log types. Using a custom whitelist built
by observing communications patterns in the enterprise, we
eﬀectively reduced the data Beehive inspects from 300 mil-
lion log messages per day to 80 million (a 74% reduction).
Beehive improves on signature-based approaches to detect-
ing security incidents.
Instead, it ﬂags suspected security
incidents in hosts based on behavioral analysis. In our eval-
uation, Beehive detected malware infections and policy vi-
olations that went otherwise unnoticed by existing, state-
of-the-art security tools and personnel. Speciﬁcally, for log
ﬁles collected in a large enterprise over two weeks, 25.25%
of Beehive incidents were conﬁrmed to be malware-related
or to warrant further investigation by the enterprise SOC,
39.41% were policy violations, and 35.33% were associated
with unrecognized software or services.
To the best of our knowledge, ours is the ﬁrst exploration
of the challenges of “big data” security analytics at the scale
of real-world enterprise log data.
Acknowledgments
We are grateful to members of the EMC CIRT team for
providing us access to the enterprise log data, and for their
help in investigating Beehive alerts.
This research is partly funded by National Science Foun-
dation (NSF) under grant CNS-1116777. Engin Kirda also
thanks Sy and Laurie Sternberg for their generous support.
7. REFERENCES
[1] OSSEC – Open Source Security.
http://www.ossec.net.
[2] Snort. http://www.snort.org.
[3] The Bro Network Security Monitor.
http://www.bro.org/.
[4] M. Abu Rajab, J. Zarfoss, F. Monrose, and A. Terzis.
A Multifaceted Approach to Understanding the
Botnet Phenomenon. In IMC, 2006.
207
[5] M. Antonakakis, R. Perdisci, D. Dagon, W. Lee, and
N. Feamster. Building a Dynamic Reputation System
for DNS. In USENIX Security, 2010.
[6] M. Antonakakis, R. Perdisci, W. Lee, N. Vasiloglou,
II, and D. Dagon. Detecting Malware Domains at the
Upper DNS Hierarchy. In USENIX Security, 2011.
[7] M. Antonakakis, R. Perdisci, Y. Nadji, N. Vasiloglou,
S. Abu-Nimeh, W. Lee, and D. Dagon. From
Throw-away Traﬃc to Bots: Detecting the Rise of
DGA-based Malware. In USENIX Security, 2012.
[8] L. Bilge, D. Balzarotti, W. Robertson, E. Kirda, and
C. Kruegel. Disclosure: Detecting Botnet Command
and Control Servers Through Large-scale NetFlow
Analysis. In ACSAC, 2012.
[9] L. Bilge, E. Kirda, K. Christopher, and M. Balduzzi.
EXPOSURE: Finding Malicious Domains Using
Passive DNS Analysis. In NDSS, 2011.
[10] J. R. Binkley and S. Singh. An Algorithm for
Anomaly-based Botnet Detection. In USENIX
SRUTI, 2006.
[11] D. Brauckhoﬀ, X. Dimitropoulos, A. Wagner, and
K. Salamatian. Anomaly Extraction in Backbone
Networks Using Association Rules. In IMC, 2009.
[12] M. J. Chapple, N. Chawla, and A. Striegel.
Authentication Anomaly Detection: A Case Study on
a Virtual Private Network. In ACM MineNet, 2007.
[13] H. Choi, H. Lee, H. Lee, and H. Kim. Botnet
Detection by Monitoring Group Activities in DNS
Traﬃc. In IEEE CIT, 2007.
[14] E. Cooke, F. Jahanian, and D. McPherson. The
Zombie Roundup: Understanding, Detecting, and
Disrupting Botnets. In USENIX SRUTI, 2005.
[15] G. Dewaele, K. Fukuda, P. Borgnat, P. Abry, and
K. Cho. Extracting Hidden Anomalies Using Sketch
and non Gaussian Multiresolution Statistical
Detection Procedures. In ACM SIGCOMM LSAD,
2007.
[16] J. Fran¸cois, S. Wang, R. State, and T. Engel.
BotTrack: Tracking Botnets Using NetFlow and
PageRank. In IFIP TC 6 Networking Conf., 2011.
[17] F. C. Freiling, T. Holz, and G. Wicherski. Botnet
Tracking: Exploring a Root-cause Methodology to
Prevent Distributed Denial-of-service Attacks. In
ESORICS, 2005.
[23] I. T. Jolliﬀe. Principal Component Analysis.
Springer-Verlag, 1986.
[24] A. Karasaridis, B. Rexroad, and D. Hoeﬂin.
Wide-scale Botnet Detection and Characterization. In
USENIX HotBots, 2007.
[25] L. Kaufman and P. J. Rousseeuw. Finding Groups in
Data. An Introduction to Cluster Analysis. Wiley,
1990.
[26] J. Levine, R. LaBella, H. Owen, D. Contis, and
B. Culver. The Use of Honeynets to Detect Exploited
Systems Across Large Enterprise Networks. In IEEE
IAW, 2003.
[27] C. Livadas, R. Walsh, D. Lapsley, and W. Strayer.
Using Machine Learning Techniques to Identify
Botnet Traﬃc. In IEEE LCN, 2006.
[28] J. Ma, L. K. Saul, S. Savage, and G. M. Voelker.
Beyond Blacklists: Learning to Detect Malicious Web
Sites from Suspicious URLs. In ACM SIGKDD KDD,
2009.
[29] J. Nazario and T. Holz. As the Net Churns: Fast-ﬂux
Botnet Observations. In MALWARE, 2008.
[30] E. Passerini, R. Paleari, L. Martignoni, and
D. Bruschi. FluXOR: Detecting and Monitoring
Fast-Flux Service Networks. In DIMVA, 2008.
[31] R. Perdisci, I. Corona, D. Dagon, and W. Lee.
Detecting Malicious Flux Service Networks through
Passive Analysis of Recursive DNS Traces. In ACSAC,
2009.
[32] A. Ramachandran and N. Feamster. Understanding
the Network-level Behavior of Spammers. In ACM
SIGCOMM, 2006.
[33] A. Sperotto, R. Sadre, and A. Pras. Anomaly
Characterization in Flow-Based Traﬃc Time Series. In
IEEE IPOM, 2008.
[34] B. Stone-Gross, M. Cova, L. Cavallaro, B. Gilbert,
M. Szydlowski, R. Kemmerer, C. Kruegel, and
G. Vigna. Your Botnet is My Botnet: Analysis of a
Botnet Takeover. In ACM CCS, 2009.
[35] W. Strayer, R. Walsh, C. Livadas, and D. Lapsley.
Detecting Botnets with Tight Command and Control.
In IEEE LCN, 2006.
[36] R. Villamar´ın-Salom´on and J. C. Brustoloni. Bayesian
Bot Detection Based on DNS Traﬃc Similarity. In
ACM SAC, 2009.
[18] G. Gu, R. Perdisci, J. Zhang, and W. Lee. BotMiner:
[37] A. Wagner and B. Plattner. Entropy Based Worm and
Clustering Analysis of Network Traﬃc for Protocol-
and Structure-independent Botnet Detection. In
USENIX Security, 2008.
[19] G. Gu, P. Porras, V. Yegneswaran, M. Fong, and
W. Lee. BotHunter: Detecting Malware Infection
Through IDS-driven Dialog Correlation. In USENIX
Security, 2007.
[20] G. Gu, J. Zhang, and W. Lee. BotSniﬀer: Detecting
Botnet Command and Control Channels in Network
Traﬃc. In NDSS, 2008.
[21] T. Holz, C. Gorecki, K. Rieck, and F. C. Freiling.
Measuring and Detecting Fast-Flux Service Networks.
In NDSS, 2008.
[22] J. P. John, A. Moshchuk, S. D. Gribble, and
A. Krishnamurthy. Studying Spamming Botnets Using
Botlab. In USENIX NSDI, 2009.
Anomaly Detection in Fast IP Networks. In IEEE
WETICE, 2005.
[38] S. Yadav, A. K. K. Reddy, A. N. Reddy, and
S. Ranjan. Detecting Algorithmically Generated
Malicious Domain Names. In IMC, 2010.
[39] S. Yadav and A. N. Reddy. Winning With DNS
Failures: Strategies for Faster Botnet Detection. In
SECURECOMM, 2011.
[40] T.-F. Yen and M. K. Reiter. Traﬃc Aggregation for
Malware Detection. In DIMVA, 2008.
[41] J. Zhang, R. Berthier, W. Rhee, M. Bailey, P. Pal,
F. Jahanian, and W. H. Sanders. Safeguarding
Academic Accounts and Resources with the University
Credential Abuse Auditing System. In DSN, 2012.
208