be used (for clarity not displayed in listing 5). (2) The new
base guard is written to TPM NVRAM as the last step.
Let’s reconsider ChkPassword and discuss how crashes are
resolved. Depending on the timing of a crash, we can dif-
ferentiate between three main situations. One, ChkPass-
word was just created and the user called set_passwd to
change the default password. This led to the execution of
_init_state but the system crashes before tpm.mode could
be set (see listing 3,
line 9). When ChkPassword is re-
created, it requests its previous state (listing 1, line 6). As
tpm.mode still read Clear (listing 2, line 11), the module will
restart from its default settings. As no input was ever used,
state-continuity is guaranteed trivially.
Two, the system didn’t crash when the user modiﬁed the
module’s default password and now calls check_passwd pro-
viding "attempt1" as password. After libice0 stores a new
cube Cattempt1 on disk and updates guarded memory, the sys-
tem crashes while the password is being veriﬁed (listing 1,
line 23). The module is re-created and execution ﬂow even-
tually executes _recovery_step (listing 5) As only a single
cube is available containing the leaked guard from guarded
memory (or a successor thereof), only cube Cattempt1 is con-
sidered fresh. After returning the stored input-state tuple in
Cattempt1, ChkPassword will restore the attempts_left and
password variables and execution is restarted with input
"attempt1" (listing 1, line 10).
Three, assume that the previous password was incorrect
and the user enters "attempt2" for her second attempt. Af-
ter storing the new cube Cattempt2 on disk, the system crashes
before the new (incremented) guard could be written to
guarded memory (listing 4, line 4). This is an interesting
point of failure as both cubes Cattempt1 as Cattempt2 can be
considered fresh6. However, recovery based on either will
preserve state continuity. This is obvious for cube Cattempt2
as this is the latest cube written to disk. Recovery from
Cattempt1, however will purge any record of the login attempt
made using "attempt2" as password. This is also safe as
it was never used in any valuable computation (instructions
after listing 1 line 24 were not executed yet). Hence, an
attacker is not able to deduce any valuable information.
3.4.2 libicen: State-Cont. Storage for n Modules
By depending on scarce resources such as TPM NVRAM
and guarded memory, libice0 can in practice only provide
state-continuous storage to a limited number of modules.
libicen will alleviate this strain by using a single, unique
ice0 module to store freshness information on behalf of
other modules. To safely exchange sensitive information be-
tween libicen and the ice0 module, inter-module commu-
nication must guarantee endpoint authentication and conﬁ-
dentiality, integrity and freshness of messages. We will state
this explicitly by passing a module identiﬁer to ice0 calls.
Creation of an initial state.
Similarly to libice0, an initial state of the module is
stored by generating a new guard and cryptographic keys
and writing a new cube to disk (see listing 6). Finally the
ice0 module is requested to store the keys and guard.
6The recovery step as displayed in listing 5, line 19, only
accepts cube Cattempt1 as fresh. However, an attacker in-
crementing the guard stored in guarded memory, will trick
libice0 to accept cube Cattempt2 as being fresh as well.
1 v o i d
i n i t s t e p ( S t a t e ∗ s t a t e ) {
mod . guard = g e n g u a r d ( ) ;
mod . k e y s = g e n k e y s ( ) ;
hdd . w r i t e ( new Cube ( mod . guard , mod . keys ,
s t a t e ) ) ;
mod . mode = A c t i v a t e d ;
i c e 0 . s t o r e ( mod . id , mod . keys , mod . guard ) ;
3
5
7 }
Listing 6: libicen: Initialization of a new module
1 v o i d
u p d a t e s t a t e ( S t a t e ∗ s t a t e ) {
++mod . guard ;
hdd . w r i t e ( new Cube ( mod . guard , mod . keys ,
s t a t e ) ) ;
i c e 0 . s t o r e ( mod . id , mod . guard ) ;
3
5 }
Listing 7: libicen: Updating a state
Updating a state.
To update a state, libicen writes a new cube to disk, be-
fore the updated fresh guard is stored in ice0 (see listing 7).
Recovering from a crash.
To recover from a crash, the (presumably) fresh cube is
read from disk (see listing 8). Next, the keys and guard
are requested from the ice0 module. As the fresh guard is
always stored safely in ice0, a cube with a correct MAC and
that contains the fresh guard, must be fresh. Once the cube’s
freshness has been validated, libicen needs to generate a
new guard, create and write a new cube to disk and store
the new guard in ice0 before a new step is taken. Storing
the fresh cube with a newly generated guard is vital, even
though the fresh guard never leaked. For details we refer
the reader to the extended version of this paper [27].
4.
IMPLEMENTATION
Given that SGX-enabled systems are not available yet,
we implemented7 ICE on top of Fides [28], a hypervisor-
based protected-module architecture using CMOS memory
as guarded memory. This setup enables microbenchmarks
and detailed analysis of the costs of accessing the TPM chip,
writing cubes to disk, performing cryptographic calculations
and accessing guarded memory on real-life systems.
While Fides provides similar isolation mechanisms as Intel
SGX, it cannot guarantee that protected modules, or its own
implementation for that matter, do not leave the CPU pack-
age in plaintext. Hence, this setup cannot defend against a
hardware attacker that is able to directly modify contents of
main memory. Similarly, we assume that an attacker cannot
modify contents of CMOSme mory directly.
While any non-volatile memory can serve as an alterna-
tive, CMOS memory is an interesting candidate for guarded
memory. As it already stores wall-clock time, it is updated
every second and it must support a large number of write
operations over its entire lifespan. Second, as it does not re-
quire a special communication protocol, it can be accessed
easily and without much overhead. Being only accessible
7Our research prototype is available at https://distrinet.
cs.kuleuven.be/software/sce/
(cid:20)(cid:20)(cid:20)
1 S t a t e ∗ r e c o v e r y s t e p ( ) {
Cube cube = hdd . r e a d ( ) ;
i c e 0 . r e t r i e v e ( mod . id , &mod . keys , &mod .
3
i f
guard )
(
S t a t e ∗ s t a t e = e x t r a c t ( cube . s t a t e , tpm .
i s f r e s h ( &cube )
) {
k e y s )
mod . guard = g e n g u a r d ( ) ;
hdd . w r i t e ( new Cube ( mod . guard , mod .
keys , cube . s t a t e )
mod . s t a t e = A c t i v a t e d ;
i c e 0 . s t o r e (mod . id , mod . keys , mod . guard )
r e t u r n s t a t e ;
) ;
}
e l s e a b o r t ( ) ;
5
7
9
11
13 }
15 b o o l
i s f r e s h ( Cube ∗ cube ) {
r e t u r n check mac ( cube , mod . k e y s
) &&
mod . guard . v a l u e == cube . guard . v a l u e ;
17
}
Listing 8: libicen: Recovering from a crash
ICE
SHA-512
AES-NI
Total
libice0
C
asm
372
371
176
919
1,566
1,566
0
0
libicen
C
asm
341
371
176
888
1,566
1,566
0
0
Table 1: Breakdown of libice0 and libicen.
through direct I/O, it can also be isolated easily by hardware
virtualization support; only 21 lines of code (LOC) had to be
added to the hypervisor. Another 61 LOCs were required
to implement system calls to access CMOS memory from
the ice0 module. This totals the size of the hypervisor to
9,492 LOCs. While Fides at this moment does not support
TPM chip accesses, we estimate, based on the Flicker [17]
source code8, that this straightforward eﬀort would require
an addition of less than 2,000 LOCs.
To implement libice0 and libicen, we used the polarssl9
library to calculate SHA-512 hash values and the Intel AES-
NI reference implementation to take advantage of AES hard-
ware support. This totals to 2,485 LOCs and 2,454 LOCs
for libice0 and libicen respectively (table 1).
5. EVALUATION
5.1 Security Evaluation
Recall that we wish to guarantee both safety and liveness
Safety Properties
properties. We discuss both aspects separately.
5.1.1
To ensure that our presented algorithm does guarantee
state continuity, we developed a formal proof of correct-
ness. While alternative formalizations (e.g., LS2 [6]) may
have reduced our workload, we formalized our system using
rely-guarantee reasoning [12] as it enables explicit reasoning
about attack steps. A machine-checked proof was created
8https://sparrow.ece.cmu.edu/group/flicker.html
9http://polarssl.org/
(cid:20)(cid:20)(cid:21)
using the Coq proof assistant. The proof required 118 def-
initions, 201 lemmas and totals 37,726 lines. The extended
version of this paper [27] discusses the proof in more detail.
5.1.2 Liveness Properties
In order to guarantee liveness, libice0 and libicen must
always be able to recover from a crash. We discuss how this
is accomplished during their diﬀerent phases.
libice0’s liveness properties.
An important distinction can be made based on the value
of tpm.mode. This value indicates whether the algorithm has
been initialized correctly. A crash before this value is set,
will result in a re-execution of the initialization step. After
setting this value, all crashes will result in the execution
of the recovery step. To ensure that the initialization step
may succeed eventually, we store the fresh cube on disk,
take exclusive access of guarded memory, write the matching
guards to guarded and TPM NVRAM memory and store
cryptographic keys in the TPM chip before setting tpm.mode.
After initialization we may update the state or we have to
recover the fresh state. In the former case we make sure to
ﬁrst store the cube before we update the content of guarded
memory. Recovery of a state is more challenging as we have
to modify the guards in both guarded and TPM NVRAM
memory. After creating a new cube with the fresh state of
the module and storing it on disk, we take exclusive access
of guarded memory and write the new guard to it before we
update TPM NVRAM memory. This has an important con-
sequence: in case the system crashes during the execution of
the recovery step before it is completed, the old guard may
have been overwritten. This would prevent the re-execution
of the recovery step. Therefore we require that this (public)
guard is written to disk before the recovery step is called.
libicen’s liveness properties.
Ensuring liveness of libicen’s algorithm is straightfor-
ward as we only have to deal with two non-volatile data
objects: cubes and the ice0 module to store freshness in-
formation. For obvious reasons we ensure that new cubes
are stored on disk ﬁrst. ice0’s implementation and the li-
bice0 library it uses, guarantee that its state updates can
be considered atomically and they are always retrievable.
5.2 Performance Evaluation
In this section we evaluate the performance of our proto-
type implementation. To compare the performance impact
of a solid state drive (SSD) against a rotating hard drive
(HDD), we used two machines with comparable hardware.
The ﬁrst machine, a Dell Latitude E6510, a mid-end con-
sumer laptop, is equipped with an Intel Core i5 560M pro-
cessor running at 2.67 GHz and 4 GiB of RAM. It is also
equipped with a magnetic hard disk (HDD), a Broadcom
TPMv1.2 chip and CMOS memory. The second testing lap-
top is a Dell Latitude E6520, has an Intel Core i5-2520M
CPU running at 2.50GHz and is equipped with an SSD.
Hardware Benchmarks.
To better understand the performance cost of ICE com-
pared to TPM operations, we performed 4 benchmarks on
the Latitude E6510: read/write accessing TPM NVRAM,
extending PCR registers and generating random numbers.
To perform these tests, we developed small TPM appli-
ing 7, lines 3-4), reducing disk access times.
While most TPM chips NVRAM area is limited to 1,280
bytes [20], it could be used to provide (state-continuous)
storage to a single module to avoid disk overhead. To show
that such a module would still beneﬁt from ICE, we imple-
mented a second benchmark called Noop. It does not per-
form any computation but only stores a state of 1,280 bytes.
As expected given the performance of SHA-512 and Intel’s
AES hardware support, the increase in computation cost is
negligibly with only 0.01ms. As cubes are still smaller than
disk sectors, costs of disk accesses are comparable to the
Password benchmark. This totals the cost of storing new
data in Noop at 15.05ms to 17.65ms for libice0 and libi-
cen resp˙, signiﬁcantly faster than 82.18ms to access TPM
NVRAM. Finally we performed these tests on the Lati-
tude E6510 which is equipped with amagnetic HDD. As
expected, the cost of writing cubes to disk increased sig-
niﬁcantly and now accounts for 99.63%-99.74%. For both
benchmarks libicen consistently takes more time writing
cubes to disk thanlibice0. We attribute this behavior to
the way we implemented its write function: merging ice0’s
and libicen’s cubes takes us 3 write system calls before
system buﬀers are ﬂushed.
Expected Impact of Dedicated Hardware.
These benchmarks show that only up to 0.14% of time
is spent on computation. With dedicated hardware perfor-
mance can be increased signiﬁcantly.
Writing guards to CMOS memory is about 2.4 times more
costly than computation and takes up to 0.31% of the time
in case of a revolving HDD and up to 2.17% on our SSD
testing platform. Hardware support for guarded memory,
as described in detail in Section 3.1, would reduce overhead
of this operation to almost zero.
But committing cubes to disk forms the real bottleneck,
requiring up to 97.47% (for SSD) to 99.74% (for HDD) of
the time. Recently Viking Technology [34] and Micron Tech-
nology [33] announced that they will ship capacitor-backed
RAM to market. Operating similar to guarded memory,
these hardware components contain fast, volatile memory
that is written to ﬂash memory when power is suddenly lost.
Adding these hardware components to our system would
eliminate disk access completely.
In summary, benchmarks show that our prototype im-
plementation on commodity hardware already outperforms
TPM NVRAM write operations by almost 5 times. Adding
dedicated hardware support for guarded memory and capa-
citor-backed RAM, may even enable state updates 587 times
faster than TPM NVRAM accesses!
6. RELATED WORK
With the exception of Parno et al. [20], state-continuous
storage has largely been overlooked. Most research proto-
types rely on a huge TCB or are vulnerable to crash attacks.
Hardware Modiﬁcations.
XOM [15] protects against an attacker that is able to
snoop buses and modify memory by encrypting data and
code before it is sent to memory. While it makes it signiﬁ-
cantly more diﬃcult to successfully attack the system, Suh
et al. [30] argue correctly that it is vulnerable to a mem-
ory replay attack where stalemem ory pages are returned to
Figure 2: Microbenchmarks of various TPM oper-
ations show a signiﬁcant diﬀerence in performance
cost of CMOS and disk accesses. Where applicable,