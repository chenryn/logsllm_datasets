title:On the predictability of large transfer TCP throughput
author:Qi He and
Constantinos Dovrolis and
Mostafa H. Ammar
On the Predictability of Large Transfer TCP Throughput∗
Qi He
Georgia Tech
Constantine Dovrolis
Georgia Tech
Mostafa Ammar
Georgia Tech
PI:EMAIL
PI:EMAIL
PI:EMAIL
ABSTRACT
Predicting the throughput of large TCP transfers is impor-
tant for a broad class of applications. This paper focuses
on the design, empirical evaluation, and analysis of TCP
throughput predictors. We ﬁrst classify TCP throughput
prediction techniques into two categories: Formula-Based
(FB) and History-Based (HB). Within each class, we develop
representative prediction algorithms, which we then evalu-
ate empirically over the RON testbed. FB prediction relies
on mathematical models that express the TCP throughput
as a function of the characteristics of the underlying network
path. It does not rely on previous TCP transfers in the given
path, and it can be performed with non-intrusive network
measurements. We show, however, that the FB method is
accurate only if the TCP transfer is window-limited to the
point that it does not saturate the underlying path, and
explain the main causes of the prediction errors. HB tech-
niques predict the throughput of TCP ﬂows from a time se-
ries of previous TCP throughput measurements on the same
path, when such a history is available. We show that even
simple HB predictors, such as Moving Average and Holt-
Winters, using a history of few and sporadic samples, can
be quite accurate. On the negative side, HB predictors are
highly path-dependent. We explain the cause of such path
dependencies based on two key factors: the load on the path
and the degree of statistical multiplexing.
Categories and Subject Descriptors: C.2.5 [Computer
Communication Networks]: Internet
General Terms: Experimentation, Measurement
Keywords: Network measurement, TCP modeling, time
series forecasting, performance evaluation
1.
INTRODUCTION
With the advent of overlay and peer-to-peer networks [3,
5], Grid computing, and CDNs, performance prediction of
∗
ANIR-0347374.
This work was supported by the NSF CAREER award
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’05, August  22–26,  2005,  Philadelphia,  Pennsylvania,  USA.
Copyright 2005 ACM 1-59593-009-4/05/0008 ...$5.00.
network paths becomes an essential task. To name just a
few applications, such predictions can be used in path se-
lection for overlay and multihomed networks [2], dynamic
server selection [15], and peer-to-peer parallel downloads.
Arguably, the most important performance metric of a path
is the average throughput of TCP transfers. The reason is
that most data-transfer applications, and about 90% of the
Internet traﬃc, use the TCP protocol. When it comes to
performance prediction, the focus is typically on bulk TCP
transfers, lasting more than a few seconds. Short TCP ﬂows
are often limited by slow-start, and their performance is de-
termined by the Round-Trip Time (RTT) and the presence
of random losses [7].
In this work, we focus on predicting the throughput of
a bulk TCP transfer in a given network path, prior to ac-
tually starting the transfer. For many applications, such
as server selection and overlay route selection, a through-
put prediction is needed before the ﬂow starts. The reason
is that rerouting an established TCP connection to a dif-
ferent network path or server can cause problems such as
migration delays, packet reordering, and re-initialization of
the congestion window. Note that TCP throughput pre-
diction is diﬀerent than TCP throughput estimation. The
latter is performed while the ﬂow is in progress. An ex-
ample of a TCP throughput estimation scheme is TCP-
Friendly Rate Control (TFRC) [6]. Unlike the prediction
of RTT and loss rate, which can be based on direct and
low-overhead measurements, predicting TCP throughput is
signiﬁcantly harder. First, TCP throughput depends on a
large number of factors, including the transfer size, maxi-
mum sender/receiver windows, various path characteristics
(RTT, loss rate, available bandwidth, nature of cross traﬃc,
reordering, router/switch buﬀering, etc) and the exact im-
plementation of TCP at the end-hosts. Second, direct mea-
surement of TCP throughput using large “probing” transfers
are highly intrusive because the latter can saturate the un-
derlying paths for signiﬁcant time periods. What is really
desired is a low-overhead TCP throughput prediction tech-
nique that either avoids probing transfers altogether, or re-
quires only a limited amount of probing traﬃc.
This paper focuses on the design, empirical evaluation,
and analysis of TCP throughput predictors for a broad class
of applications. The common requirement of such applica-
tions is that they rely on an accurate throughput prediction
prior to the start of the TCP transfer. We ﬁrst classify
TCP throughput prediction techniques into two categories:
Formula-Based (FB) and History-Based (HB). Within each
class we develop representative prediction algorithms, which
145we then evaluate empirically over the RON testbed [1]. Note
that our objective is not to compare FB and HB predictors.
In fact, the two schemes are complementary, as they require
diﬀerent types of measurements and previous information
about the underlying path. Instead, our objective is to ex-
amine the key issues in each prediction scheme, evaluate
their accuracy under diﬀerent conditions, explain the major
causes of prediction errors, and provide insight regarding the
factors that aﬀect the predictability of large transfer TCP
throughput in a given path.
Speciﬁcally, FB prediction relies on mathematical mod-
els that express the TCP throughput as a function of the
characteristics of the underlying network path (e.g., RTT,
loss rate). For instance, the throughput-optimizing routing
component of RON follows the FB approach [3], predicting
TCP throughput based on the simple “square-root” formula
of [11]. That formula expresses the average throughput of
a congestion-limited bulk transfer as a function of the RTT
and the loss rate that the connection experiences on a given
path. Several similar models have been proposed in the
literature [4, 12, 19], diﬀering in terms of complexity and
accuracy, modeling assumptions, and TCP ﬂavor.
In this
paper, we prefer to use the main result of [12], referred to as
the PFTK formula, because it is both simple and accurate.
The main advantage of FB prediction is that it does not
require any history of previous TCP transfers. In addition,
FB prediction can be performed with relatively lightweight,
non-intrusive network measurements of parameters such as
RTT and loss rate. Unfortunately, however, our measure-
ments show that FB schemes can lead to large prediction
errors. The main reason is that throughput models require
knowledge of the path characteristics during the TCP ﬂow,
whereas FB predictions measure the corresponding a priori
characteristics before the ﬂow starts. If the ﬂow itself causes
signiﬁcant changes in those characteristics, the resulting pre-
diction errors can be unacceptably large. Another reason is
that the delays or losses that a TCP ﬂow experiences are not
necessarily the same as those observed by a periodic probing
stream, such as ping [8]. On the positive side, we do observe
that the prediction errors are much lower, and probably ac-
ceptable for many applications, if the TCP transfer is lim-
ited by the receiver’s advertised window to the point that
the transfer does not saturate its path.
On the other hand, HB approaches use standard time se-
ries forecasting techniques to predict TCP throughput based
on a history of throughput measurements from previous
TCP transfers on the same path. Obviously, HB prediction
is applicable only when large TCP transfers are performed
repeatedly on the same path. This is the case with several
applications of TCP throughput prediction, including over-
lay routing, parallel downloading and Grid computing. Our
measurements over the RON testbed show that even sim-
ple linear HB predictors, such as Moving Average and non-
seasonal Holt-Winters, are quite accurate. Furthermore, in
agreement with previous work on HB prediction [22, 23],
we found no major diﬀerences among a few candidate HB
predictors. We do ﬁnd, however, that two simple heuristics
can noticeably improve the accuracy of HB predictors. The
ﬁrst is to detect and ignore outliers, and the second is to
detect level shifts and restart the HB predictors. We next
show, perhaps surprisingly, that even with a short history of
a few previous transfers performed sporadically in intervals
up to 30-40 minutes, prediction errors are still fairly low. On
the negative side, our measurements show that HB predic-
tors are highly path-dependent, which begs for answers to
the following two questions. What makes TCP throughput
much more predictable on some paths than on others, and
which are the fundamental factors that aﬀect the through-
put predictability on a path? We focus on two factors that
we believe are the most important: the load on the path, and
the degree of statistical multiplexing. Speciﬁcally, we show
using simple queueing models that the prediction error in-
creases with the load on the bottleneck link, and decreases
with the number of competing ﬂows under constant load.
Consequently, paths that are heavily loaded with just a few
big ﬂows are expected to be most diﬃcult to predict.
The structure of the paper is as follows. We summarize
the related work in Section 2. Section 3 presents a repre-
sentative FB predictor and Section 4 evaluates its accuracy.
Section 5 presents some HB predictors and Section 6 evalu-
ates their accuracy. Section 7 focuses on two major factors
for throughput predictability. We conclude in Section 8.
2. RELATED WORK
The motivation for some of the previous work on TCP
throughput modeling has been to predict the throughput of
a transfer as a function of the underlying network charac-
teristics [6, 11, 12]. However, the accuracy of FB prediction
depends on the accuracy with which these characteristics
can be estimated or measured. Recently, Goyal et al. have
shown that the end-to-end packet loss rate p on a path can
be quite diﬀerent from the “congestion event probability”
p(cid:1)
required by the well-known PFTK model of Padhye et
al. [12], and they have proposed a way to estimate p(cid:1)
from
p [8]. Note that that work does not address the problem
of estimating the required path characteristics during a ﬂow
from those observed prior to the ﬂow.
HB throughput prediction, on the other hand, has re-
ceived more attention. An operational system is the Net-
work Weather Service (NWS) project [20]. In NWS, through-
put prediction is based on small (64KB) TCP transfer probes
with a limited socket buﬀer size (32KB). Vazhkudai et al.
use bulk TCP transfers (1MB-1GB) and a large socket buﬀer
(1MB), performed sporadically (1 minute-1 hour) [22]. They
show that various linear predictors (including ARIMA mod-
els) perform similarly, and that the average prediction error
on two paths ranges from 10% to 25%. Zhang et al. ex-
amine TCP throughput predictability based on a large set
of paths and transfers [23]. Their TCP throughput mea-
surements use 1MB transfers performed every minute, with
200KB socket buﬀers. Their main results are that 1) with
several simple linear predictors, about 95% of the predic-
tion errors are below 40%, and 2) predictions using a very
long history (e.g., Moving Average with 128 samples) per-
form rather poorly. A study by Qiao et al. has shown that
the predictability of network traﬃc is highly path depen-
dent [14]. Some mathematical models (such as MMPP)
have been previously used to analyze the predictability of
aggregate network traﬃc [18].
3. FORMULA-BASED PREDICTION
The central component of an FB predictor is a mathemat-
ical formula that expresses the average TCP throughput as
a function of the underlying path characteristics. Proba-
bly the most well-known such model is the “square-root”
formula of [11]:
E[R] =
q
M
T
2bp
3
(1)
where E[R] is the expected TCP throughput (as opposed to
R which denotes the actual or measured throughput and ˆR
which denotes the predicted throughput). In the previous for-
mula, M is the ﬂow’s Maximum Segment Size, b is the num-
ber of TCP segments per new ACK, while T and p are the
RTT and loss rate, respectively, as experienced by the TCP
ﬂow. This model is fairly accurate for bulk TCP transfers in
which packet losses are recovered with Fast-Retransmit. In
this section, we ﬁrst present a more complete TCP through-
put formula, as well as the corresponding FB predictor. We
emphasize that our remarks regarding the accuracy and lim-
itations of FB prediction are not speciﬁc to the particular
formula we use, however.
3.1 An FB predictor
The TCP throughput formula that we use is the PFTK
result of [12], which improves on the square-root formula
especially in the presence of retransmission timeouts and/or
a limited maximum window:
0
@
q
E[R] = min
q
M
1
A
,
W
T
T
2bp
3 + To min(1,
3bp
8 )p(1 + 32p2)
(2)
where To is the TCP retransmission timeout period, and W
is the maximum window size. We emphasize that p and
T are the average loss rate and RTT that the target ﬂow
(i.e., the TCP ﬂow whose throughput we try to predict)
experiences (the main symbols we use are summarized in
Table 1). Notice that the loss rate p may be zero, in which
case the ﬂow is lossless and E[R] is given by the term W/T .
†
RTT experienced by ﬂow
RTT measured with periodic probing before ﬂow
RTT measured with periodic probing during ﬂow
loss rate experienced by ﬂow
loss rate measured with periodic probing before ﬂow
loss rate measured with periodic probing during ﬂow
congestion event probability experienced by ﬂow
T
ˆT
˜T
p
ˆp
˜p
p(cid:1)
R actual throughput of ﬂow
ˆR predicted throughput of ﬂow
˜R expected throughput of ﬂow based on ˜T and ˜p
ˆA
W maximum window of ﬂow
available bandwidth measured prior to ﬂow
†
The word ”ﬂow” in this table refers to the target ﬂow.
Table 1: Table of symbols.
Suppose now that we want to apply (2) to TCP through-
put prediction. The main problem is that we do not know
the loss rate and RTT that the ﬂow will experience during
its lifetime. The obvious approach, which has been used in
practice (e.g., in overlay routing [3]), is to measure the loss
rate and RTT before the transfer with a utility such as ping,
and then apply those estimates of p and T in (2). Suppose
that ˆp and ˆT are the loss rate and RTT estimates based
on a priori measurements. Then, if ˆp ≈ p and ˆT ≈ T , the
prediction accuracy will be only limited by the accuracy of
these approximations and the accuracy of the mathemati-
cal model that led to (2). We can expect that ˆp ≈ p and
ˆT ≈ T when the TCP ﬂow imposes a minor load on the
path’s bottleneck, without aﬀecting signiﬁcantly the RTT
and loss rate of the path.
A limitation of the previous approach is that it does not
apply to lossless paths, i.e., when ˆp=0. In that case, W/ ˆT
can be unrelated to the realized throughput, especially if
W is much larger than the bandwidth-delay product of the
path. One approach to deal with lossless paths is to pre-
dict the TCP throughput based on the available bandwidth
(avail-bw) ˆA of the path prior to the ﬂow, when ˆA 0
if ˆp =0
„
“
8>:min
min
ˆR =
(3)
where ˆR is the predicted throughput, while ˆT , ˆp, and ˆA, are
the measured RTT, loss rate, and avail-bw prior to the TCP
ﬂow. We estimate the retransmission timeout period as:
ˆTo = max(1sec, 2SRTT), where SRTT is set to the measured
RTT ˆT prior to the target ﬂow. Note the diﬀerences between
(2) and (3): the latter relies on the estimates ˆT , ˆp, ˆTo, rather
than the actual values T , p, To, and it also has a component
that depends on the avail-bw estimate ˆA.
In the following, we discuss three potential limitations of
the above predictor using some basic insight.
3.2 Errors due to load increase
An increase in the utilization of a queue (with non-periodic
arrivals) typically increases the average queueing delay. Sim-
ilarly, in a queue with ﬁnite buﬀering, an increase in the
oﬀered load can cause a higher loss probability. The in-
crease in the queueing delays and/or the loss rate is more
signiﬁcant when the utilization becomes close to 100% af-
ter the load increase, or when the utilization was already
that high even before the additional load. These basic facts
can cause major errors in FB prediction. The reason is that
the RTT ˆT measured prior to the target ﬂow may not reﬂect
the increased queueing delay during that transfer. So, ˆT can
be lower than the RTT T that the target ﬂow experiences.
Similarly for the loss rate, it can be that ˆp  1, i.e., ˆR=wR for the former and ˆR=R/w for the latter,
yields the same relative error w − 1 (in absolute value).
To report a single accuracy ﬁgure for n measurements in a
time series (speciﬁcally, for all 150 epochs of a trace), we use
the Root Mean Square Relative Error (RMSRE) statistic,
deﬁned as
vuut 1
n
nX
i=1
RMSRE =
E2
i
(5)
where Ei is the relative error of measurement i.
4.2 Results
)
%
(
F
D
C
 100
 80
 60
 40
 20
 0
-4
Lossless path predictions
All predictions
Lossy path predictions
-2
 0
 2
 4
 6
 8
 10
Relative Error E
Figure 2: CDF of E for all predictions, for predic-
tions in lossy paths, and for predictions in lossless
paths.
Prediction error in lossy and lossless paths: Figure 2
shows the CDF of E for all measurements, across all traces
and paths. It also shows separately the CDFs of E for the
subset of lossy path predictions (based on the PFTK model)
and for the subset of lossless path predictions (based on the
avail-bw estimate ˆA)1. Let us ﬁrst focus on the “all predic-
tions” curve. Notice that for roughly 40% of all measure-
ments, the prediction is an overestimation by more than a
factor of two (E ≥ 1).
In fact, the overestimation errors
are larger than an order of magnitude (E ≥ 9) for almost
10% of the measurements. The underestimation errors are
much less dramatic and common, with only 10% of the mea-
surements suﬀering from an underestimation by more than
a factor of two (E < −1).
In the case of lossless paths, underestimation errors occur
very rarely, while the overestimation errors are considerably
lower and less common than in lossy paths. The reason is
that in lossless paths, our FB predictor does not rely on the
erroneous RTT and loss rate estimates prior to the target
ﬂow. The remaining errors can be attributed to the dif-
ferences between TCP throughput and avail-bw, discussed
1For W =1MB, we have that ˆA < W/T in all paths.
in § 3.4. The fact that, in lossless paths, overestimation is
the only major type of prediction error implies that either
pathload overestimates the path’s avail-bw, or that TCP
cannot saturate the avail-bw in its path due to random losses
or insuﬃcient buﬀering at the bottleneck link.
 100
 80
 60
 40
 20
)
%
(
F
D
C
 0
 0
 0.001
 0.02
Loss Rate Increase
 0.06
 0.04
 0.08
 0.1
Loss rate increase
RTT increase
 20
 40
 60
 80
 100
RTT Increase (ms)
Figure 3: CDF of RTT and loss rate increase due to
target ﬂow.
RTT and loss rate increases due to target ﬂow: Re-
turning to the case of lossy paths, the fact that overestima-
tion is much more dramatic than underestimation illustrates
the dominance of the issue discussed in § 3.2, namely ˆT < T
and ˆp < p. Figure 3 shows the distributions of the increases
in RTT and in loss rate after the start of the target ﬂow.
The increases were measured as ˜T − ˆT and ˜p − ˆp respec-
tively (recall that ˜T and ˜p are estimates of T and p during
the target ﬂow). Notice that in about 50% of the measure-
ments, the RTT did not increase signiﬁcantly. In 40% of the
measurements, however, the target ﬂow caused an RTT in-
crease between 5ms and 60ms. In 10% of the measurements
the RTT increase was higher than 100ms, probably due to
congested low-capacity links. The loss rate, on the other
hand, increased by 0.1% to 2% in almost all measurements.
Although this loss rate increase may appear small in magni-
tude, recall that TCP throughput is inversely proportional
to the square-root of the loss rate. For example, an increase
of the loss rate from 0.1% to 1% can cause a throughput
overestimation by a factor of about 3.2.
Errors due to periodic RTT and loss rate sampling:
An interesting hypothetical question is the following: how
accurate would FB prediction be, if we had ping-based es-
timates of the path’s RTT ˜T and of loss rate ˜p during the
target ﬂow? The answer to this question would allow us
to examine the magnitude of the prediction errors due to
the diﬀerences between periodic probing and TCP sampling
(discussed in § 3.3). Figure 4 shows the CDF of the FB
prediction error when we feed in (3) the ping-based RTT ˜T
and loss-rate ˜p during the target ﬂow. The CDF refers only
to lossy paths. Note that using ˜T and ˜p makes the relative
error signiﬁcantly lower than using ˆT and ˆp (−3 < E < 3 for
about 80% of the predictions). Also, overestimation and un-
derestimation become equally likely (the CDF of E is prac-
tically symmetric). Despite the beneﬁts of knowing ˜T and
˜p, the prediction errors are still signiﬁcant, however: more
than half of the prediction errors are still larger than a factor
of two. The remaining prediction errors can be attributed
to the sampling diﬀerences discussed in § 3.3. In terms of
RTT/loss rate during TCP flow
RTT/loss rate prior to TCP flow
 100
 80