corresponding machine code (with uninteresting parts omitted for brevity). The function was compiled using GCC
4.9.1 on Linux x86-64. The -O3 version does not contain a conventional function prologue and epilogue which
manipulates the stack or frame pointer.
each of which computes
Li : Rmi−1 → Rmi
G : R → R
Li(x) =G( Wix + bi)
Wi ∈ Rmi×mi−1
bi ∈ Rmi
then the entirety (consisting of k layers) is simply these
layers composed together:
L(x) =L k(Lk−1(···(L 1(x))))
m0 = s
mk = t
with the dimensions of the output of one layer matching
the dimensions of the input of the subsequent layer. The
mi are the dimensionality of the input and the ultimate
output of the network, as well as the intermediates pro-
duced by each of the layers.
The term “layer” is often used to refer to not the pa-
rameters of the functions Li, but the inputs or outputs of
these functions. In turn, each element of the inputs or
outputs of the functions are often called “units”, or by
analogy, “neurons”.
In this deﬁnition, G is referred to as an activation func-
tion or nonlinearity, and computed separately for each el-
ement. Without the activation function, L would simply
be an afﬁne function which we could write as W x + b,
which does not enable the expressivity that we need.
Common nonlinearities are the logistic sigmoid function
and the hyperbolic tangent function:
σ (x) =
tanh(x) =
1
1 + e−x
e2x − 1
e2x + 1
which have the ranges of (0,1) and (−1,1), respectively.
Usually, the ﬁnal layer will have no activation function
because we do not wish to bound the output to a limited
range, or a softmax function if we want to use the MLP
as a multi-class classiﬁer, so that we can interpret the
values as a probability distribution. The softmax function
is computed as follows:
S(x)i =
exi
∑n
k=1 exk
Note that unlike the other activation functions, it does
not operate elementwise. Due to the normalization term
in the denominator, S(x) sums to 1. A multi-layer per-
ceptron consisting of one layer with a softmax activation
function is equivalent to multi-class logistic regression.
3.2 Loss functions
Now that we have deﬁned multi-layer perceptrons as a
class of parameterized functions, we need a method to
614  24th USENIX Security Symposium 
USENIX Association
4
ﬁnd appropriate parameters so that the neural network
does what we want. First, we deﬁne a loss function in
order to quantify how much differently the network be-
haves from our target. A common loss function is the
squared Euclidean distance:
d(y, ˆy) =(cid:31)y − ˆy(cid:31)2
2
where y is the “true” output and ˆy is the one produced by
the neural network.
In the multi-class classiﬁcation case, if y is the correct
class and π(x) is the probability distribution produced
by the neural network, then we can use the negative log
probability:
d(y,π(x)) = −logπ(x)y
Usually, we will have a list of correct input-output
pairs (x1,y1),··· , (xn,yn) for the purpose of training the
network. Then we can seek to minimize the mean of the
losses, or 1
i=1 d(yi, f (xi)). We use this type of loss
function throughout this paper.
n ∑n
3.3 Gradient descent and backpropagation
To minimize the loss, and therefore obtain a neural net-
work which performs our desired function, we can con-
sider various standard optimization methods. Speciﬁ-
cally, we wish to minimize D deﬁned as such:
θ = (W1,b1,··· ,Wk,bk)
d(yi, fθ (xi))
n
D(θ ) =
1
∑
n
i=1
D(θ )
min
θ
A typical way to minimize differentiable functions is
gradient descent, which works by repeated applications
of the following update:
θ(cid:28) = θ − α ·
∂ D(θ )
∂θ
where α is generally a small number. Intuitively, the
derivative allows us to analytically determine which di-
rection we should move in within each dimension of θ
to reduce the value of F. Subtracting a small multiple of
the gradient performs this function.
If D is convex, this procedure is guaranteed to con-
verge at the optimal value of θ given appropriate choices
of α. Many machine learning models and classiﬁers in-
volve optimizing a convex function in a similar way. Un-
fortunately, neural networks are generally non-convex in
its parameters, allowing for a richer class of possible
functions, but which means that these theoretical guar-
antees do not hold. Instead, the procedure may lead us to
a local optimum or a saddle point where the derivative is
zero.
We now need the derivative of D. Estimating the
derivative numerically seems a simple and straightfor-
ward solution, but it is a highly inefﬁcient one requir-
ing as many evaluations of D as the dimensionality of
θ. Instead, we can use a method called backpropagation
to compute the derivative analytically. We describe the
details of backpropagation in Section A.
3.4 Recurrent neural networks
While multi-layer perceptrons can approximate a wide
variety of functions, they can only operate on inputs of
ﬁxed size and produce an output of ﬁxed size. In princi-
ple, given a large input, we could divide it into ﬁxed-size
pieces and give them separately to a multi-layer percep-
tron. However, the output of each piece depends only
on that input piece, and we cannot represent any depen-
dencies between parts of the input in one piece and the
output for a different piece.
Recurrent neural networks are one paradigm for ad-
dressing this conundrum, and map sequences to se-
quences (recursive neural networks, which have the same
initialism, are an alternative developed for computing on
trees).
We can formally deﬁne them in the following way:
L : Rm × Rn → Rn
G : R → R
L(x,h) =G( Whxx +Whhh + b)
Given an input sequence (x1,··· ,xT ) (where xi ∈ Rm), we
compute (h1,··· ,hT ) like this:
Whx ∈ Rn×m
Whh ∈ Rn×n
b ∈ Rn
h0 = 0
h1 = L(x1,h0)
...
hT = L(xT ,hT−1)
Note that the operation on each elements uses the same
weights. Nevertheless, the use of h enables the network
to remember information from past elements to use while
processing the current element, and propagate informa-
tion into the future.
We can use the hts as inputs to another recurrent neural
network, or apply to them a linear transformation possi-
bly with a softmax activation function (as done in the
ﬁnal layer of a multi-layer perceptron):
yi = S(Wyhht + b)
5
USENIX Association  
24th USENIX Security Symposium  615
To deﬁne a loss function for a recurrent neural net-
work, we can apply a loss function for a multi-layer per-
ceptron separately to each input-output pair within the
sequence and simply sum the losses together:
d(y, ˆy) =
T
∑
i=1
d(yt , ˆyt )
The input and output sequences of a recurrent neural
network need not have the same lengths. For instance,
we might allow an arbitrary number of inputs but only
one output to summarize the contents of the input in
some way. In this case, we can simply adjust the loss
function to only compute the loss at the relevant parts of
the output sequence. We can also train a recurrent neural
network to map an input sequence to an arbitrary num-
ber of output symbols, if we run the network to obtain
some number of outputs until it produces a special ‘stop’
output.
As with the multi-layer perceptron, we would like to
learn appropriate parameters so that the recurrent neu-
ral network parameterized with them computes a desired
function, using gradient descent. We can compute the
derivative of the RNN with respect to its parameters in
the same way as earlier.
In particular, we can unroll
the RNN so that it becomes a long feedforward neu-
ral network which computes on a ﬁxed-length sequence,
and compute the gradient for this network using an ap-
propriate loss function with backpropagation. After un-
rolling, note that the time-dependent layers should share
the same weights This procedure is also called backprop-
agation through time [16].
3.5 Limitations of recurrent neural net-
works
In this section, we point out some limitations of recurrent
neural networks which can limit their usefulness.
As speciﬁed in this paper, recurrent neural networks
cannot compute for an arbitrary number of timesteps be-
fore computing the answer. For example, RNNs can eas-
ily compute the parity of an arbitrarily long stream of
bits [15], as this requires a constant number of operations
per input. In contrast, we can reason that a RNN could
not multiply numbers of arbitrary size, as multiplication
is a O(n2) operation on the length of the numbers [22].
Also, h has a ﬁxed size which we cannot easily adapt
if necessary in order to store more information. For ex-
ample, previous works have shown success with using
RNNs for machine translation, in which the RNN ﬁrst
reads a sentence in the source language and stores its
meaning in h before producing the corresponding words
in the target language using the information in h. While
we can pick a size for h such that it has enough capacity
to store information on a typical-length sentence, we can
imagine that this scheme would break down for a sen-
tence of sufﬁcient length.
The most-studied limitation revolves around difﬁcul-
ties in training a recurrent neural network, due to what
are referred to as the vanishing gradient and exploding
gradient problems [16]. Consider that
∂ hv
∂ ht
= ∏
v≥i>t
∂ hi
∂ hi−1
= ∏
v≥i>t
WhhG(cid:29)(hi−1)
The repeated multiplication with Whh (v− t times) can
cause ∂ hv
∂ ht to grow exponentially large (“explodes”) or go
to 0 (“vanishes”) depending on whether the largest eigen-
value of Whh is greater or smaller than 1. Therefore, an
input will often have a very large or vanishingly small ef-
fect on an output which occurs far in the future, in terms
of the gradient computation. For exploding gradients, a
simple solution involves rescaling the gradient to a ﬁxed
norm if its magnitude is too large. On the other hand,
dealing with vanishing gradients can prove more chal-
lenging.
3.6 Long Short-Term Memory and Gated
Recurrent Units
To avoid the exploding and vanishing gradient problems
with recurrent neural networks, previous work has pro-
posed RNN architectures carefully designed to remove
the long-range multiplicative characteristics of RNNs
which lead to these problems.
Long Short-Term Memory (LSTM), one of these ar-
chitectures, have enabled impressive empirical results in
areas such as speech recognition, machine translation,
and image captioning. Within this model, the state which
propagates through time has no multiplicative updates at
each step; instead, it is stored in a memory cell ct which
receives additive updates, combined with a mechanism
for erasing irrelevant information from the previous time
step. The “input modulation gate” (g) and the “forget
gate” ( f ), respectively, control whether the memory cell
receives the additive update or discards (some part of the)
previous memory cell contents.
Following the notation in Zaremba et al. [23], we can
formally deﬁne the LSTM:
xt ,ht−1,ct−1 → ht ,ct
i = σ (Wxixt +Whiht−1)
f = σ (Wx f xt +Wh f ht−1)
o = σ (Wxoxt +Whoht−1)
g = tanh(Wxgxt +Whght−1)
ct = f (cid:27) ct−1 + i(cid:27) g
ht = o(cid:27) tanh(ct )
616  24th USENIX Security Symposium 
USENIX Association
6
Here, (cid:31) represents element-wise multiplication.
In principle, these gates enable the gradient to propa-
gate across long time scales, since the LSTM can ignore
irrelevant inputs through the input modulation gate, re-
member information only until necessary using the for-
get gate, and output only relevant information using the
output gate. When the forget gate is “open”, i.e. close
to 1, then the gradient will propagate mostly unchanged.
The input and output at each time step only inﬂuences
the gradient when the corresponding gates are open.
Gated Recurrent Units (GRU) have been proposed
more recently as a simpler alternative to LSTMs, while
sharing the same goals of avoiding the long-range depen-
dency problems that have plagued RNNs. The main dif-
ferences lie in that there exists no separate memory state
ct from the hidden state ht, and the network exposes the
entire hidden state at each time step. The forget gate in-
terpolates between the previous hidden state and the new
input i, with no separate input modulation gate. Instead,
g modulates the amount of inﬂuence the previous hidden
state has on i.
We deﬁne the GRU formally:
xt ,ht−1 → ht
g = tanh(Wxgxt +Whght−1)
i = tanh(Wxixt +Whi(g(cid:31) ht−1))
f = σ (Wx f xt +Wh f ht−1)
ht = f (cid:31) ht−1 + (1− f )(cid:31) i
While the GRU theoretically lacks some of the ﬂexi-
bility provided by the LSTM, it is both simpler to imple-
ment and easier to compute, requiring about half as many
calculations in each time step compared to the LSTM.
4 Methods
In this section, we describe how we built upon the back-
ground in Section 3 to perform the task of function iden-
tiﬁcation.
4.1 Basic architecture
Our simplest architecture uses a recurrent neural net-
work, described in Section 3.4, to process each byte and
and output a decision for that byte as to whether it begins
a function or not.
Recall that neural networks, as we have deﬁned them,
take real-valued vectors Rm as input, containing m real
values. In contrast, a byte is a single 8-bit integer, which
can have one of 256 (= 28) possible values. We cannot
input a byte into the neural network directly and need to
convert them into a real-valued vector.
Converting the 8-bit integer into a single ﬂoating-point
number to input into the neural network might seem like
a reasonable solution; however, neural networks process
their inputs by multiplying them with the weight param-
eters, which only makes sense when the input values rep-
resent intensities (like brightness or loudness).
Instead, we use “one-hot encoding”, which converts a
byte into a R256 vector (since a byte can have 256 distinct
values) where exactly one of the values is 1 and all others
are 0. The byte’s identity determines the location of the
1 within the vector. For example, a NUL byte (0) would
be represented as
and a nop in x86 (0x90, or 144) would be
].
].
255 elements
[1 0 ··· 0
(cid:31) (cid:30)(cid:29) (cid:28)
1 0 ··· 0
(cid:31) (cid:30)(cid:29) (cid:28)
144 elements
[ 0 ··· 0
(cid:31) (cid:30)(cid:29) (cid:28)
111 elements
Multiplying a matrix A with a one-hot vector x is
equivalent to extracting a column from A. In our case,
the RNN multiplies a parameter matrix Whx ∈ Rm×256
with the one-hot input x, which is equivalent to select-
ing a column from Whx. Effectively, each byte of input is
represented with a h-dimensional vector during computa-
tion of the RNN, with the precise representation learned
during training of the neural network. Such a mapping
is often referred to as an embedding. Such embeddings
have proved useful in other ﬁelds such as natural lan-
guage processing, with embeddings of words into high-
dimensional spaces showing interesting properties.
We could have instead considered encoding the byte
as a R8 vector, with the elements corresponding to the
eight bits and having values of 0 or 1. However, this rep-
resentation imposes the constraint that the embedding of
a particular byte is the sum of the embeddings of its con-
stituent bits, even though the bits do not have composi-
tional meaning in typical binary code. We do not further
discuss this approach in the paper.
We want our output to serve as a binary classiﬁer at
each byte position. We use the softmax function to pro-
duce a probability distribution over whether the byte be-
gins (or ends) a function or not. During training, the loss
function sums over the error at each position within the
sequence. The error at each position is the negative log
of the probability that the neural network assigned to the
correct answer. We penalize each false positive and false
negative equally, without a weighting to discourage one
at the expense of the other.
4.2 Optimization with stochastic gradient
descent and rmsprop