Increasing either the number of dimensions or realities results in
shorter path lengths, but higher per-node neighbor state and main-
tenance trafﬁc. Here we compare the relative improvements caused
by each of these features.
Figure 6 plots the path length versus the average number of neigh-
bors maintained per node for increasing dimensions and realities.
We see that for the same number of neighbors, increasing the di-
mensions of the space yields shorter path lengths than increasing
the number of realities. One should not, however, conclude from
these tests that multiple dimensions are more valuable than multi-
ple realities because multiple realities offer other beneﬁts such as
improved data availability and fault-tolerance. Rather, the point to
take away is that if one were willing to incur an increase in the av-
erage per-node neighbor state for the primary purpose of improving
routing efﬁciency, then the right way to do so would be to increase
the dimensionality d of the coordinate space rather than the number
of realities r.
3.3 Better CAN routing metrics
The routing metric, as described in Section 2.1, is the progress
in terms of Cartesian distance made towards the destination. One
can improve this metric to better reﬂect the underlying IP topology
by having each node measure the network-level round-trip-time
RTT to each of its neighbors. For a given destination, a message
#realities=1
#dimensions=2
2 dimensions
3 dimensions
4 dimensions
5 dimensions
s
p
o
h
f
o
r
e
b
m
u
N
256
128
64
32
16
8
4
256
1024
4096
16K
64K
256K
1M
Number of nodes
s
p
o
h
f
o
r
e
b
m
u
N
256
128
64
32
16
8
4
2
256
1 reality
2 realities
3 realities
4 realities
25
20
15
10
s
p
o
h
f
o
r
e
b
m
u
N
Number of nodes = 131,072
d=2,r=2
increasing dimensions, #realities=2
increasing realities, #dimensions=2
r=3
d=3
r=4
d=4
r=5
r=6
d=5
d=6
r=7
d=7
30
1024
4096
16K
64K
256K
1M
Number of nodes
10
15
Number of neighbors
20
25
Figure 4: Effect of dimensions on path
length
Figure 5: Effect of multiple realities on
path length
Figure 6: Path length with increasing
neighbor state
is forwarded to the neighbor with the maximum ratio of progress
to RTT. This favors lower latency paths, and helps the application
level CAN routing avoid unnecessarily long hops.
Unlike increasing the number of dimensions or realities, RTT-
weighted routing aims at reducing the latency of individual hops
along the path and not at reducing the path length. Thus, our metric
for evaluating the efﬁcacy of RTT-weighted routing is the per-hop
latency, obtained by dividing the overall path latency by the path
length.
To quantify the effect of this routing metric, we used Transit-
Stub topologies with link latencies of 100ms for intra-transit do-
main links, 10ms for stub-transit links and 1ms for intra-stub do-
main links. With our simulated topology, the average end-to-end la-
tency of the underlying IP network path between randomly selected
source-destination nodes is approximately 115ms. Table 1 com-
pares the average per-hop latency with and without RTT weighting.
These latencies were averaged over test runs with n, the number of
nodes in the CAN, ranging from  to .
As can be seen, while the per-hop latency without RTT-weighted
routing matches the underlying average IP network latency, RTT-
weighted routing lowers the per-hop latency by between 24% and
40% depending on the number of dimensions. Higher dimensions
give more next-hop forwarding choices and hence even greater im-
provements.
3.4 Overloading coordinate zones
So far, our design assumes that a zone is, at any point in time,
assigned to a single node in the system. We now modify this to
allow multiple nodes to share the same zone. Nodes that share
the same zone are termed peers. We deﬁne a system parameter
MAXPEERS, which is the maximum number of allowable peers per
zone (we imagine that this value would typically be rather low, 3 or
4 for example).
With zone overloading, a node maintains a list of its peers in ad-
dition to its neighbor list. While a node must know all the peers in
its own zone, it need not track all the peers in its neighboring zones.
Rather, a node selects one neighbor from amongst the peers in each
of its neighboring zones. Thus, zone overloading does not increase
the amount of neighbor information an individual node must hold,
but does require it to hold additional state for up to MAXPEERS
peer nodes.
Overloading a zone is achieved as follows: When a new node A
joins the system, it discovers, as before, an existent node B whose
zone it is meant to occupy. Rather than directly splitting its zone
as described earlier, node B ﬁrst checks whether it has fewer than
MAXPEERS peer nodes. If so, the new node A merely joins B’s
zone without any space splitting. Node A obtains both its peer
list and its list of coordinate neighbors from B. Periodic soft-state
updates from A serve to inform A’s peers and neighbors about its
entry into the system.
If the zone is full (already has MAXPEERS nodes), then the zone
is split into half as before. Node B informs each of the nodes on it’s
peer-list that the space is to be split. Using a deterministic rule (for
example the ordering of IP addresses), the nodes on the peer list
together with the new node A divide themselves equally between
the two halves of the now split zone. As before, A obtains its initial
list of peers and neighbors from B.
Periodically, a node sends its coordinate neighbor a request for
its list of peers, then measures the RTT to all the nodes in that
neighboring zone and retains the node with the lowest RTT as its
neighbor in that zone. Thus a node will, over time, measure the
round-trip-time to all the nodes in each neighboring zone and retain
the closest (i.e. lowest latency) nodes in its coordinate neighbor set.
After its initial bootstrap into the system, a node can perform this
RTT measurement operation at very infrequent intervals so as to
not unnecessarily generate large amounts of control trafﬁc.
The contents of the hash table itself may be either divided or
replicated across the nodes in a zone. Replication provides higher
availability but increases the size of the data stored at every node by
a factor of MAXPEERS (because the overall space is now partitioned
into fewer, and hence larger, zones) and data consistency must be
maintained across peer nodes. On the other hand, partitioning data
among a set of peer nodes does not require consistency mechanisms
or increased data storage but does not improve availability either.
Overloading zones offers many advantages:
(cid:15) reduced path length (number of hops), and hence reduced
path latency, because placing multiple nodes per zone has the
same effect as reducing the number of nodes in the system.
(cid:15) reduced per-hop latency because a node now has multiple
choices in its selection of neighboring nodes and can select
neighbors that are closer in terms of latency. Table 2 lists
the average per-hop latency for increasing MAXPEERS for
system sizes ranging from  to  nodes with the same
Transit-Stub simulation topologies as in Section 3.3. We see
that placing 4 nodes per zone can reduce the per-hop latency
by about 45%.
(cid:15) improved fault tolerance because a zone is vacant only when
Number of Non-RTT-weighted RTT-weighted
dimensions
routing (ms)
routing (ms)
2
3
4
5
116.8
116.7
115.8
115.4
88.3
76.1
71.2
70.9
Number of nodes per zone
per-hop latency (ms)
1
2
3
4
116.4
92.8
72.9
64.4
Table 1: Per-hop latency using RTT-weighted routing
Table 2: Per-hop latencies using multiple nodes per zone
all the nodes in a zone crash simultaneously (in which case
the repair process of Section 2.3 is still required).
On the negative side, overloading zones adds somewhat to sys-
tem complexity because nodes must additionally track a set of peers.
3.5 Multiple hash functions
For improved data availability, one could use k different hash
functions to map a single key onto k points in the coordinate space
and accordingly replicate a single (key,value) pair at k distinct nodes
in the system. A (key,value) pair is then unavailable only when all
k replicas are simultaneously unavailable. In addition, queries for
a particular hash table entry could be sent to all k nodes in paral-
lel thereby reducing the average query latency. Figure 7 plots this
query latency, i.e. the time to fetch a (key,value) pair, for increasing
number of nodes for different numbers of hash functions.
Of course, these advantages come at the cost of increasing the
size of the (key,value) database and query trafﬁc (in the case of
parallel queries) by a factor of k.
Instead of querying all k nodes, a node might instead choose to
retrieve an entry from that node which is closest to it in the coordi-
nate space.
3.6 Topologically-sensitive construction of the
CAN overlay network
The CAN construction mechanism described in Section 2.2 allo-
cates nodes to zones at random, and so a node’s neighbors on the
CAN need not be topologically nearby on the underlying IP net-
work. This can lead to seemingly strange routing scenarios where,
for example, a CAN node in Berkeley has its neighbor nodes in Eu-
rope and hence its path to a node in nearby Stanford may traverse
distant nodes in Europe. While the design mechanisms described
in the previous sections try to improve the selection of paths on an
existing overlay network they do not try to improve the overlay net-
work structure itself. In this section, we present some initial results
on our current work on trying to construct CAN topologies that are
congruent with the underlying IP topology.
Our initial scheme assumes the existence of a well known set
of machines (for example, the DNS root name servers) that act as
landmarks on the Internet. We achieve a form of “distributed bin-
ning” of CAN nodes based on their relative distances from this set
of landmarks. Every CAN node measures its round-trip-time to
each of these landmarks and orders the landmarks in order of in-
creasing RTT. Thus, based on its delay measurements to the differ-
ent landmarks, every CAN node has an associated ordering. With
m landmarks, m! such orderings are possible. Accordingly we
partition the coordinate space into m! equal sized portions, each
corresponding to a single ordering. Our current (somewhat naive)
scheme to partition the space into m! portions works as follows: as-
suming a ﬁxed cyclical ordering of the dimensions (e.g. xyzxyzx...),
we ﬁrst divide the space, along the ﬁrst dimension, into m portions,
each portion is then sub-divided along the second dimension into
m (cid:0)  portions each of which is further divided into m (cid:0)  por-
tions and so on. Previously, a new node joined the CAN at a ran-
dom point in the entire coordinate space. Now, a new node joins
the CAN at a random point in that portion of the coordinate space
associated with its landmark ordering.
The rationale behind this scheme is that topologically close nodes
are likely to have the same ordering and consequently, will reside
in the same portion of the coordinate space and hence neighbors
in the coordinate space are likely to be topologically close on the
Internet.
The metric we use to evaluate the above binning scheme is the
ratio of the latency on the CAN network to the average latency on
the IP network. We call this the latency stretch. Figure 8 compares
the stretch on CANs constructed with and without the above land-
mark ordering scheme. We use the same Transit-Stub topologies
as before (Section 3.3) and 4 landmarks placed at random with the
only restriction that they must be at least 5 hops away from each
other. As can be seen, landmark ordering greatly improves the path
latency.
A consequence of the above binning strategy is that the coordi-
nate space is no longer uniformly populated. Because some order-
ings (bins) are more likely to occur than others their corresponding
portions of the coordinate space are also more densely occupied
than others leading to a slightly uneven distribution of load amongst
the nodes. The use of background load balancing techniques (as
described in Appendix A) where an overloaded node hands off a
portion of its space to a more lightly loaded one could be used to
alleviate this problem.
These results seem encouraging and we are continuing to study
the effect of topology, link delay distribution, number of landmarks
and other factors on the above scheme. Landmark ordering is work
in progress. We do not discuss or make use of it further in this
paper.
3.7 More Uniform Partitioning
When a new node joins, a JOIN message is sent to the owner of
a random point in the space. This existing node knows not only its
own zone coordinates, but also those of its neighbors. Therefore,
instead of directly splitting its own zone, the existing occupant node
ﬁrst compares the volume of its zone with those of its immediate
neighbors in the coordinate space. The zone that is split to accom-
modate the new node is then the one with the largest volume.
This volume balancing check thus tries to achieve a more uni-
form partitioning of the space over all the nodes and can be used
with or without the landmark ordering scheme from Section 3.6.
Since (key,value) pairs are spread across the coordinate space using
a uniform hash function, the volume of a node’s zone is indicative
of the size of the (key,value) database the node will have to store,
and hence indicative of the load placed on the node. A uniform par-
titioning of the space is thus desirable to achieve load balancing.
Note that this is not sufﬁcient for true load balancing because
some (key,value) pairs will be more popular than others thus putting
higher load on the nodes hosting those pairs. This is similar to the
t
)
s
(
y
c
n
e
a
L
y
r
e
u
Q
d
e
v
e
c
r
e
p
-
r
e
s
U
i
#dimensions=2, #realities=1
1 hash function
3 hash functions
5 hash functions
16.0
14.0
12.0
10.0
8.0
6.0
4.0
2.0
256
1024
16K
4096
Number of nodes
64K
256K
h
c
t
e
r
t
S
y
c
n
e
a
L
t
25
20
15
10
5
0
#landmarks=4, #realities=1
2-d, with landmark ordering
2-d, without landmark ordering
4-d, with landmark ordering
4-d, without landmark ordering
without uniform-partitioning feature
with uniform-partitioning feature
100
80
60
40
20
s
e
d
o
n
f
o
e
g
a
t
n
e
c
r
e
P
256
1024
Number of nodes
4096
0
V/16
V/8
V/4
V/2
V
Volume
2V