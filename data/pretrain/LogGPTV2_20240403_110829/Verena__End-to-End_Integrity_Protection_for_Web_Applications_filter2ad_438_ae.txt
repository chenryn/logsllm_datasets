to the client, called proof_list.
2) Execute the query on the database and produce a result
result.
3) Identify the relevant ADS instance for the trust context
of this query or ADS instances for a query using a
completeness chain. Assemble a proof of correctness of
the query result based on these ADSes and their roots
and add the proof to proof_list. Add requests for the
hashes of the roots to HS_requests.
4) Identify the last client who modiﬁed each ADS. Assemble
a proof that this client is in the trust context for the relevant
IQP (per §V-D), add the corresponding hash requests to
HS_requests and the proof to proof_list.
In Step 4 of Fig. 4, the client veriﬁes the proof from
the server. For each ADS involved in the query, the client
veriﬁes (1) the ADS proof, (2) that the root hash of this ADS
corresponds to the one from hash server’s signature, (3) the
hash server’s signature, (4) the server’s proof that the last client
who changed the root hash (as speciﬁed by the public key in
hash server response) was authorized, as explained in §V-D.
B. Write Query
A write query can be an insert, update, or remove. Verena
provides linearizability guarantees:
there is a total order
between all read and write queries, and each read will see
the latest committed write. A client considers a write query
committed when the protocol in Fig. 4 completes successfully.
To prevent the server from cheating during serialization, a
natural solution is to have the hash server serialize requests.
However, this strategy will increase the complexity at the hash
server and our goal is to keep the hash server simple. Instead,
the main server will do the serialization work in a veriﬁable
way. The only job of the hash server is to ensure that each
HS_PUT to an entry increments the version of the entry. Based
on the version number, clients can verify that the server did
not serve an old hash or attempted a fork attack [32].
A write query (e.g. insert) can cause modiﬁcation of multiple
ADSes. The server serializes the changes to these ADSes
by locking access to each ADS involved. Parallel write
queries affecting different ADS structures can still proceed
concurrently.
Clients can issue a delete or update query to a certain
document ID. If the developer wants to delete or update
documents selected by a ﬁlter, the developer should ﬁrst fetch
those documents using a read operation and then update them
by ID. Verena’s design can be extended to enable such queries
directly by employing veriﬁcation as for read queries, but in
interest of simplicity, we do not describe these changes here.
We now describe the steps involved in an insert; delete is
similar. Update proceeds as a delete and insert that happen at
the same time (so there is no need to run the communication
protocol twice).
During an insert operation, the client must help the server
update ADS trees. For each ADS tree, the client ﬁrst checks
that the ADS tree at the server is correct. Namely, its root hash
matches the corresponding hash at the hash server and was
changed by an authorized client. The client does not have to
download the entire ADS tree to perform this check; only the
relevant path in the tree is required. Then, the client inserts
the new value and recomputes the new root hash. It signs this
hash and provides it to the main server, to be included in a
hash server HS_PUT request.
To avoid the need for an additional round trip between the
main and hash servers, the main server maintains a copy of the
hash server map. Thus, the client obtains the hash root hashold
from the main server instead of the hash server. Of course, the
main server could provide an incorrect value, but both the hash
server and the client detect this behavior as follows. When
the client performs the update, the client provides a new hash
along with hashold in a signature sent to the hash server as
part of HS_PUT. The hash server checks that hashold matches
the value at the hash server as discussed in §VI.
Due to updates, some data content may cycle back to an old
hash. The version numbers prevent a malicious server from
replaying previous updates on this repeated hash value.
During Step 2 and 2’, the server runs:
1) Check, using regular access control, if the client is allowed
to write. If not, return.
2) Identify the relevant ADSes A1 . . .An that need to be
updated. Acquire a lock for each one of these.
3) Send a message to the client containing a proof for each
ADS Ai as discussed above. Instead of contacting the
hash server for the tuple E = (hash h, v, PK), send this
information from the server’s storage. Also, send a proof
that PK belongs to a user who is allowed to make the
change as in the read operation.
In Step 2’, the client:
905905
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:16:13 UTC from IEEE Xplore.  Restrictions apply. 
1) Verify all the proofs as in a read operation.
2) If veriﬁcation passes, for each ADS involved, provide
nonce and siguser(ID, E, Enew), where ID is the id of the
corresponding entry at the hash server, Enew= (h’, v’ =
v+1, PK’), where h’ is the new hash after the change.
The server:
1) Check the client’s signature, h’, v’, and PK’. If everything
veriﬁes, update the database, the ADS trees, and send
Enew and siguser(ID, E, Enew) to the hash server.
2) After receiving the response from the hash server, forward
it to the client.
3) Release the locks.
Finally, the client veriﬁes the hash server’s response: the
signature from the hash server veriﬁes with the nonce and the
hash server accepted the change. If so, the write completed.
Otherwise, the main server misbehaved. The main server also
has a timeout during which it keeps locks on behalf of the
client. To provide liveness, if the client takes too long to answer
in Step 2’, the server aborts this request and releases the lock.
VIII. INFORMAL SECURITY ARGUMENT
In §IV-G, we describe the guarantees provided by Verena.
Here, we present a high-level argument of why these guarantees
hold. To argue that Verena’s read queries return results
satisfying the guarantees in §IV-G, we show the following
two properties. Given a read query q, let ADS be the ADS
corresponding to q and tc be its trust context.
1) The hashes of the roots of ADS and tc at the hash server
correspond to the latest modiﬁcation by a user in tc.
2) Given the root hash of ADS and tc that satisfy the property
above, a client can detect if the query’s result does not
satisfy the guarantees in §IV-G
The second property follows from the properties of ADS
trees (recall that tc is also implemented as an ADS tree). Let
us explain why the ﬁrst property holds. Assuming a trusted
hash server, the hash server will return to a read query the
latest hash from a write query. Moreover, the Verena client
checks for each read query that the latest write was created
by an authorized user. At the same time, due to the nonce
used by clients when receiving responses from the hash server,
a client who committed a write knows that the hash server
persisted its update. For queries that span multiple trust contexts,
given a root trust context and the properties of ADS trees, the
completeness chain mechanism can guarantee the completeness
and correctness of the results.
IX. DISCUSSION
Limitations and Extensions. Verena does not support all
possible query types, although it supports a common class
of queries. §IV-D describes the queries our current system
supports. Nevertheless, the overall Verena architecture is mostly
agnostic to the underlying ADS. The literature provides ADSes
for other types of queries, such as multidimensional range
queries or text search queries [37]; adding them to Verena
should be straightforward.
906906
Moreover, Verena does not support triggers: with a trigger,
a database server notices when a certain condition on the
data is satisﬁed and contacts the relevant users. If a server
is compromised, it can choose not to contact the users. A
mitigation to support triggers is to have the client check the
triggers after performing an update or periodically.
Hash Server Trust. The design so far assumed that the hash
server is trusted. Verena can survive compromise of the hash
server as long as an attacker does not compromise both the
hash server and the main server. This is simple to achieve: the
main server checks the answers provided by the hash server
with minimal change to the design so far because (1) the main
server stores a copy of the entries at the hash server anyways
and (2) all hash server responses pass through the server. The
main server can detect misbehavior of the hash server and
warn of a potential compromise.
User Signature Veriﬁcation. The signature veriﬁcation during
HS_PUT (§VI) of the user who performs the update can be
removed from the hash server and instead performed in the
clients. However, we decided to perform this veriﬁcation on
the hash server because it improves client latency and it is a
simple operation; it avoids the need for clients to check this
signature every time they check a proof involving it.
Data Conﬁdentiality. Verena can be combined with a web
framework such as Mylar [41], which protects data conﬁden-
tiality against server compromise. This results in a solution
offering both conﬁdentiality and integrity protection against an
active server attacker.
Using Verena Correctly. Verena provides protection only if
the developer speciﬁes the integrity policy correctly, which is
not always easy. For example, the developer should not make
write access control decisions based on data from the server
that is not integrity protected. As part of our future work, we
are interested in designing a tool that assists the developer and
helps him make less mistakes.
X. IMPLEMENTATION
We developed a prototype implementation of Verena in order
to evaluate our proposal and demonstrate its feasibility.
Web Platform. We implemented Verena on top of Meteor
version 1.1.0.2; Meteor [36] is a JavaScript web application
framework that uses Node.js [7] on the server side and
MongoDB [5] as the database backend.
We chose Meteor as it offers some desirable features that
make it attractive for our implementation. Meteor employs
client-side web page rendering based on HTML templates that
are populated by data retrieved from the server. This means that
there is a clear separation between application code and data.
The code, which consists of the JavaScript code, the HTML
templates and the CSS ﬁles, is signed by the developer and
its integrity is veriﬁed by a browser extension upon loading,
as in [41]. The integrity of data, which is the dynamic part of
the web application, is enforced by Verena, according to the
policy speciﬁed by the developer.
Moreover, Meteor features a uniform data model between
the client and the server. In other words, clients are aware of
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:16:13 UTC from IEEE Xplore.  Restrictions apply. 
how the data is organized in the MongoDB backend. This
uniformity is beneﬁcial to Verena because it allows both the
client and server to understand the integrity policy and the
database queries that will be executed. Thus, the server can
identify which proofs to accompany the reply with, and the
client can identify which proofs to expect from the server.
Meteor uses a publish/subscribe mechanism in which the
web server automatically propagates data changes to clients
who have subscribed for the results of a certain query. This
mechanism is not compatible with the freshness guarantees
Verena aims for; a malicious server might not propagate
changes to the interested clients. Hence, Verena follows
the conventional approach of explicitly requesting the data
of interest through the use of RPC requests. One can transform
the publish/subscribe mechanism into a pull-based approach,
in which the client polls the server periodically, thus providing
time-bounded freshness guarantees.
Main Server and Client. We implemented the Verena server
and client as a set of Meteor packages. The main server’s
implementation is 5100 LOC. The main component consists
of approximately 3100 LOC. The storage and manipulation
of the authenticated data structures, as well as the production
of the necessary proofs is implemented as a separate service,
which runs as a Node.js process and consists of about 2000
LOC. We note that in this prototype implementation, the
ADSes are stored in-memory, and not persisted on disk. For a
production-quality implementation of Verena, the system should
implement the ADSes within the database itself for better
performance. The ADS manipulation logic is also replicated to
the client, so that the client can verify the proofs presented by
the server. We use 224-bit ECDSA for public-key operations,
and SHA-256 as a cryptographic hash function. We perform
most cryptographic operations in JavaScript using the SJCL
library [11]. Nevertheless, to improve client performance, we
implement ECDSA signature operations as a Google Chrome
Native Client module [6].
Hash Server. The hash server is implemented as a Go HTTP
server, backed by a RocksDB [10] persistent key-value store.
The cryptographic operations (ECDSA signing and veriﬁcation)
are delegated to a separate process, written in C, which uses
OpenSSL [8] (version 1.0.2d). The reason is that the native
Go ECDSA implementation is currently signiﬁcantly slower
than the OpenSSL one. The hash server consists of 630
LOC in total (497 for the Go component and 133 for the C
component), not counting standard libraries such as OpenSSL.
By contrast, an application server’s total LoC consists of our
Verena server’s implementation plus the server-side code of
the actual application. The actual application can easily have
thousands to tens of thousands of lines of code.
XI. EVALUATION
We used our prototype implementation to evaluate the per-
formance of the various components of Verena. The evaluation
setup is as follows. Verena’s main server ran on a Macbook Pro
“Mid 2012” (iCore7 2.3 GHz), while the hash server ran on an
Intel Xeon 2.1 GHz processor with fast SSD storage, with a
Fig. 5: End-to-end latency of various read and write operations
in Verena.
recent version of Ubuntu Linux installed. To perform our end-
to-end latency measurements (§XI-A, §XI-C) we used a client
using the Chrome browser, version 49, on a second Macbook
Pro “Mid 2012” laptop. To measure throughput (§XI-B), we
employed multiple machines running many concurrent client
instances using the headless browser PhantomJS [9], in order
to saturate the system under test. All the machines that we used
for the evaluation were connected to the university network.
A. End-to-End Latency
Fig 5 shows the end-to-end latency of the basic operations
that are performed by Verena, i.e, write and read operations
on a single ADS (or in other words a particular trust context).
More concretely we tested insertion, update and removal of
records as well as read operations, namely, fetching a single
record (“ﬁnd single”), fetching a range of 20 records (“ﬁnd
range”) and computing an aggregate value (sum) on a particular
ﬁeld over a range of half of the currently inserted records. Each
inserted record had a size of 1 KB.
We measured and averaged the latency of these operations,
over 1000 iterations, for different numbers of inserted records
in the ADS. We notice that latency slightly increases (for all
operations) as the size of the ADS becomes larger. Even for
an ADS size of 106 records, all operations, take less 30ms on
average, except insert which takes slightly over 30ms for large
ADS sizes.
B. Throughput
Main Server. We measured the throughput of Verena for read
and write operations issued by multiple concurrent clients, on
ADSes containing 104 records. When clients perform only
read operations (in speciﬁc, fetch a range of 20 records) the
average throughput is 200 (±8) requests/sec. When performing
only write operations (speciﬁcally, inserting records to different
trust contexts so that they can be processed in parallel), the
average throughput is 156 (±10) requests/sec. Finally, when
clients perform a mix of the above read and write operations (4
907907
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:16:13 UTC from IEEE Xplore.  Restrictions apply. 