title:Text Captcha Is Dead? A Large Scale Deployment and Empirical Study
author:Chenghui Shi and
Shouling Ji and
Qianjun Liu and
Changchang Liu and
Yuefeng Chen and
Yuan He and
Zhe Liu and
Raheem Beyah and
Ting Wang
Text Captcha Is Dead? A Large Scale Deployment and Empirical
Study
Chenghui Shi1,*, Shouling Ji1,*,#,(), Qianjun Liu*, Changchang Liu†, Yuefeng Chen‡, Yuan He‡,
Zhe liu§, Raheem Beyah⋆ and Ting Wang£
*Zhejiang University, #Alibaba-Zhejiang University Joint Institute of Frontier Technologies,
‡Alibaba Group, §Nanjing Unversity of of Aeronautics and Astronautics,
†IBM Research, ⋆Georgia Institute of Technology, £Penn State
{chenghuishi, sji, liuqj0522}@zju.edu.cn, PI:EMAIL, {yuefeng.chenyf, heyuan.hy}@alibaba-inc.com,
PI:EMAIL, PI:EMAIL, PI:EMAIL
ABSTRACT
The development of deep learning techniques has significantly
increased the ability of computers to recognize CAPTCHA (Com-
pletely Automated Public Turing test to tell Computers and Hu-
mans Apart), thus breaking or mitigating the security of existing
captcha schemes. To protect against these attacks, recent works
have been proposed to leverage adversarial machine learning to
perturb captcha pictures. However, they either require the prior
knowledge of captcha solving models or lack adaptivity to the evolv-
ing behaviors of attackers. Most importantly, none of them has been
deployed in practical applications, and their practical applicability
and effectiveness are unknown.
In this work, we introduce advCAPTCHA, a practical adversarial
captcha generation system that can defend against deep learning
based captcha solvers, and deploy it on a large-scale online platform
with near billion users. To the best of our knowledge, this is the
first such work that has been deployed on international large-scale
online platforms. By applying adversarial learning techniques in
a novel manner, advCAPTCHA can generate effective adversarial
captchas to significantly reduce the success rate of attackers, which
has been demonstrated by a large-scale online study. Furthermore,
we also validate the feasibility of advCAPTCHA in practical ap-
plications, as well as its robustness in defending against various
attacks. We leverage the existing user risk analysis system to iden-
tify potential attackers and serve advCAPTCHA to them. We then
use their answers as queries to the attack model. In this manner,
advCAPTCHA can be adapted/fine-tuned to accommodate the at-
tack model evolution. Overall, advCAPTCHA can serve as a key
enabler for generating robust captchas in practice and providing
useful guidelines for captcha developers and practitioners.
KEYWORDS
captcha, adversarial learning, usable study
1 INTRODUCTION
Captcha 2 is a type of challenge-response test in computing that
aims to distinguish between human and automated programs (ma-
chines). Ever since its invention, captcha has been widely used to
1Chenghui Shi and Shouling Ji are the co-first authors. Shouling Ji is the corresponding
author.
2For readability purposes, we will write the acronym in lowercase.
1
improve the security of websites and various online applications
by preventing the abuse of online services such as phishing, bots,
spam, and Sybil attacks. Existing captcha mechanisms can be gen-
erally classified as text-based captcha [10], image-based captcha
[40, 47], audio-based captcha [24], game-based captcha [42] and
others [20, 32]. Among them, text-based captchas have been most
popularly studied and applied up to now and in a foreseeable future,
which is the focus of this work as well.
Status Quo and Motivation. Over the past decade, a number of
attacks have been proposed for automatically recognizing captchas.
In early time, many attacks are hard-coded for specific captcha
schemes [6, 15, 27], where designing the attacking heuristics and
methods requires heavy expert involvement. Thus, many defense
strategies that leverage varied fonts and/or background noise are
proposed to make captchas more robust and challenging. Later on,
to further defend against more generic attacks [5, 16] that target
multiple text-based captcha schemes, advanced defense strategies
(e.g., through using merged characters and character distortion
[6, 44]) are proposed to increase the difficulty of character segmen-
tation and recognition.
Recently, deep learning techniques have shown great success
for text recognition [7, 34], which are naturally further being intro-
duced to break advanced captchas. To build up a deep learning based
captcha solver, attackers only need to 1) collect a certain number of
labeled training data, which can be done by hiring low cost human
labors or directly using the existing crowdsourcing platforms, and
then 2) use mature deep learning techniques/platforms to train an
end-to-end captcha solver. The end-to-end model does not require
preprocessing or character segmentation, while taking a whole
captcha as input and recognize it directly. As extensively demon-
strated, these deep learning-based attacks can achieve significantly
good recognition performance on various captchas [33, 45].
To make things even worse, captcha solving platforms [1, 2],
which are the commercial sectors aiming at captcha solving, can
benefit more from the state-of-the-art deep learning techniques. The
previously accumulated captchas can not only help train a powerful
solver directly, but also benefit the break of new captcha schemes
leveraging transfer learning [45]. As a consequence, many works
have declared the death of text-based captchas [5, 36]. However,
small business and non-Internet firms/sectors still widely deploy
the potentially vulnerable text captchas, due to their usability, low
cost and high scalability.
Interestingly, recent research finds that Machine Learning (ML)
models, especially deep learning models, are vulnerable to adversar-
ial perturbation [38], which can be injected into a legitimate image
such that the classifier predicts an incorrect label for the image.
The adversarial attacks to ML models, on the other hand, provide
inspiration to captcha defense against modern attacks, which are
usually based on state-of-the-art deep learning techniques. Specifi-
cally, to defend against these attacks, recent works that leverage
adversarial learning to inject adversarial perturbations to captchas
have been proposed [29, 35]. We name these captchas with carefully
designed perturbations as adversarial captchas. These adversarial
captchas are expected to be resilient to deep learning based attacks
and meanwhile preserving high usability for human.
Although the idea of adversarial captcha is promising, there is
still a huge gap between the proposed mechanisms and the actual
application. First, they usually assume priorly-known knowledge
of the attack models [29, 35], i.e., the adversarial perturbations
inserted to the captcha depend on the prior knowledge of the attack
model (e.g., its parameters), which is, however, usually unavailable
in practice. Second, previous works lack adaptivity to the evolving
behaviors of attackers. The attackers, e.g., captcha solving platforms,
usually have strong capability and incentives to update their models
rapidly in practice. Once the attackers evolve their models, the
captcha generation mechanism needs to be adapted as well. Finally,
to the best of our knowledge, most, if not all, of the proposed
adversarial captcha generation schemes are not actually deployed
or evaluated in real world applications, as the time of writing this
paper. In other words, whether the idea of adversarial captcha
works on the real world platforms is not yet known. Therefore, it
is important to study highly secure, adaptive and practically usable
adversarial captcha generation and deployment schemes.
Our Method. To fill the blank, by collaborating with a world-
wide leading Internet company, we develop an adaptive adversarial
captcha generation system, namely advCAPTCHA, and deploy it
on a large-scale e-commerce platform with 824 million active indi-
vidual users and over 1 million commercial users. Leveraging this
deployment and application, we conduct a large scale empirical
study to evaluate the security, effectiveness, adaptivity, and usabil-
ity of our proposed mechanism. At a high level, advCAPTCHA first
trains a substitute solving model by collected captchas, then em-
ploys the substitute model to generate adversarial captchas. After
that, by uniquely viewing the answers submitted by the attackers
as the queries to their solving models, advCAPTCHA can fine-tune
the substitute model to approximate the attack model by learning
its captcha recognition behavior.
Contributions. Our main contributions can be summarized as
follows.
(1) An adaptive, practically high-usable adversarial captcha
generation system. We propose advCAPTCHA for generating ad-
versarial captchas against ML based captcha solving attacks in
practice. advCAPTCHA can achieve high usability while improving
the security of text-based captchas. Moreover, by viewing captcha
pictures as queries to the attack model and using these queries
to fine-tune the substitute model, advCAPTCHA presents good
adaptivity to the evolution of real world attack models.
(2) Deployment of adversarial captchas on a large-scale
online e-commerce platform. We deployed advCAPTCHA on
a large-scale international e-commerce platform. Extensive exper-
iments on the real world platform show that advCAPTCHA can
evidently decrease the captcha breaking rate of the underground
market by 50%, i.e., adversarial captchas are practically more secure.
We also show the usability and robustness of advCAPTCHA in
practical deployment.
(3) Useful guidelines for captcha developers and practi-
tioners. Through uniquely interacting with the attackers from
the underground market, we demonstrate that 1) practical attack-
ers are more inclined to use end-to-end solving models to directly
recognize a captcha as a whole; 2) multiple solving models may be
simultaneously leveraged by attackers to mitigate the security of ad-
versarial captchas; 3) the adaptivity of captchas in accommodating
the updates of attack models is important in practical deployment.
These important observations can be leveraged by captcha devel-
opers and practitioners to guide the design and deployment of
advanced adversarial captchas in the future.
2 BACKGROUND AND RELATED WORK
2.1 ML Model Vulnerability
Recent works have discovered that the existing ML models includ-
ing neural networks are vulnerable to various attacks. We introduce
two types of attacks that will be used in our methods.
Model Evasion Attack. In an evasion attack, an adversary aims
to inject carefully crafted perturbations into a legitimate instance
such that the classifier predicts an incorrect label for the instance.
Such kind of instances are named adversarial examples.
In [38], Szegedy et al. first found that DNNs are vulnerable to
imperceptible perturbation in the image domain. Then, many works
[8, 9, 11, 13, 17, 21, 28, 31] further explored how to generate adversar-
ial examples. Based on the adversary’s knowledge, these works can
be classified into two categories: white-box attacks [8, 17, 31], where
the adversary has full knowledge of the model including the model
architecture and parameters, and black-box attacks [9, 11, 13, 21, 28],
where the adversary has none, or has limited knowledge of the
model.
In the black-box setting, there are two popular methods to gen-
erate adversarial examples. The first method depends on the trans-
ferability [30] of adversarial examples. An adversary first trains
a substitute model, leverages the substitute model to generate ad-
versarial examples and transfers them to attack a target model
(i.e., victim model). The second method is the gradient estimation
method. Chen et al. noted that transferability is not always reliable
and they explored the gradient estimation method as an alterna-
tive to the substitute model [11]. In gradient estimation methods
[11, 21, 28], the adversary can query the target model by sending
any input and receive the predicted class probabilities. Then, the ad-
versary approximates the gradient information of the target model
by leveraging the query results. Finally, such estimated gradient
information will be further leveraged to generate adversarial exam-
ples. However, these methods need a large number of queries to
generate an adversarial example.
Model Extraction Attack. In an extraction attack, adversaries
aim to leverage the target model predictions to construct another
2
model as an approximation to the target model. Existing model
extraction methods usually consist of the following steps: 1) con-
struct a dataset X where the samples could be randomly selected
or carefully designed within the input space; 2) leverage the target
model F to predict X and obtain the predicted results Y; 3) consider
Y as the ground truth of X and train an approximation model ˆF.
In [14], Fredrikson et al. developed a method that exploits the
confidence values revealed along with predictions. In [41], Wu et al.
formalized model extraction from a game-theoretic perspective. In
[39], Florian et al. designed equation-solving attacks, which could
extract near-equivalent models from multiple models, including
SVM, decision trees and neural networks. However, it is inefficient
to directly apply these methods to Deep Neural Networks (DNNs) in
practice, especially when the attackers have no access to other pre-
dicted results (such as confidence scores) than the label information
[30].
2.2 Modern Attacks on Captchas
Over the past decade, a number of attacks have been proposed to
solve captchas, which are summarized in Table 2. Yan et al. [43]
discovered the fatal design errors in many captcha schemes and
used the simple pattern recognition algorithms to break captchas
with high success rates. Early works [6, 15] typically follow three
steps: preprocessing, segmentation and recognition. Specially, they
first use heuristic methods to filter background patterns, e.g., line
noise in the background of the captcha, then use segmentation tech-
niques, e.g., color filling segmentation, to segment the captchas, and
finally use a ML model to recognize the segmentation. Later works
[5, 16] combine segmentation and recognition together. They use
ML models to score all possible ways to segment a captcha, then
find the most likely way as output. Most recently, by leveraging
deep learning techniques, Ye et al. [45] used a DNN model to di-
rectly recognize captcha images without character segmentation.
In particular, they first generate synthetic captchas to learn a base
solver and then fine-tune the base solver on a small set of real
captchas by leveraging transfer learning. Thus, their method can
achieve good recognition performance with a significantly smaller
set of real captchas, as compared to previous methods.
In addition to academic research, captcha solving platforms [1,
2] have also used deep neural networks to break captchas. As a
commercial sector aiming at captcha solving, they have already
accumulated a huge number of captchas generated from a variety
of captcha schemes. With these data, they can train a very powerful
solver and break a range of captcha schemes.
2.3 Motivating the Design of advCAPTCHA
As stated in Section 2.2, most attacks on text captchas are based on
ML techniques, which are vulnerable to various adversarial attacks
(recall Section 2.1). Therefore, we aim to exploit the vulnerability
of ML models to actively defend against ML based captcha solvers,
i.e., generating adversarial captchas to fool the ML based captcha
solvers. However, in general practical scenarios, we usually cannot
access the attack models. Therefore, it is difficult for us to directly
generate adversarial captchas against them. One intuitive method
is to first generate adversarial captchas against a substitute model
(which is trained by using our own captcha scheme) that performs
the same task as the attack model, and then use the generated ad-
versarial captchas targeting the substitute model to defend against
the real unknown attack model.
However, this intuitive method may not perform well in practice
due to the following two reasons. 1) Through our careful obser-
vations in practical applications, we found that attackers usually
target at multiple captcha schemes simultaneously instead of only
one. Therefore, the substitute model we trained by our own captcha
scheme may deviate from the attack model. As a result, the adversar-
ial captchas generated by the substitute model may not demonstrate
transferabiltiy against the real attack model. 2) In practice, attackers
have strong capability and incentives to update their attack models,
which may further degrade the transferabiltiy of the adversarial
captchas. In the worst case, the security of the adversarial captchas
may degrade to that of the ordinary ones. Therefore, the captcha
generation mechanism needs to be updated as well to defend against
the evolving attack models.
The security of adversarial captchas closely depends on the simi-
larity between our substitute model and the attack model. Thus, to
generate effective adversarial captchas requires an effective chan-