period (1 second),
they measured a 3.5 degradation ratio
for the throughput and latency of a Web server benchmark.
We also assessed this limitation by running latency-sensitive
applications from the TailBench suite [6] in a domU while
varying the refresh period of its assigned network backend
unikernel (the details of the testbed are provided in §V).
Figure 2 reports for each benchmark the ratio of the mean
and (95th and 99th percentile) tail latencies over the execution
of the same benchmark without refresh. We can see that
self refresh can incur a 5x-2000x degradation for the mean
latency, 5x-1300x for the 95th percentile, and 5x-1200x for the
99th percentile. We also notice that the degradation remains
signiﬁcant even with a large refresh period (60 seconds).
These values are too high, unacceptable for cloud users. This
strengthens the need for a better approach for dom0 FT.
III. GENERAL OVERVIEW
This section presents the basic idea behind our dom0 FT
solution and the general fault model that we target.
A. Basic idea
Our solution, named PpVMM (Phoenix pVM-based VMM),
is based on three main principles. The ﬁrst principle is disag-
4We used the search string “crash hang freeze oops panic -type=checkins”.
The option “type=checkins” excludes commit messages.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:32:03 UTC from IEEE Xplore.  Restrictions apply. 
199
xapian
sphinx
moses
masstree
img-dnn
e
n
i
l
e
s
a
b
o
t
o
i
t
a
R
2500
2000
1500
1000
500
0
mean
95p-tail
99p-tail
1 5 10 15 30 45 60
Refresh period (s)
e
n
i
l
e
s
a
b
o
t
o
i
t
a
R
8
7
6
5
4
3
2
1
mean
95p-tail
99p-tail
400
350
300
250
200
150
100
50
e
n
i
l
e
s
a
b
o
t
o
i
t
a
R
mean
95p-tail
99p-tail
20
18
16
14
12
10
8
6
4
2
e
n
i
l
e
s
a
b
o
t
o
i
t
a
R
mean
95p-tail
99p-tail
1
5 10 15 30 45 60
Refresh period (s)
1 5 10 15 30 45 60
Refresh period (s)
1
5 10 15 30 45 60
Refresh period (s)
e
n
i
l
e
s
a
b
o
t
o
i
t
a
R
1400
1200
1000
800
600
400
200
0
mean
95p-tail
99p-tail
1 5 10 15 30 45 60
Refresh period (s)
Fig. 2: Mean and tail latencies for TailBench applications when self-refresh is enabled for the (disaggregated) pVM components.
The results (lower is better) are normalized w.r.t. a baseline without self-refresh for the same metrics.
gregation (borrowed from Xoar [5]), meaning that each dom0
service is launched in an isolated unikernel, thus avoiding the
single point of failure nature of the vanilla centralized dom0
design. The second principle is specialization, meaning that
each unikernel embeds a FT solution that is speciﬁcally chosen
for the dom0 service that it hosts. The third principle is pro-
activity, meaning that each FT solution implements an active
feedback loop to quickly detect and repair faults.
Driven by these three principles, we propose the general ar-
chitecture of our FT dom0 in Figure 3. The latter is interpreted
as follows. dom0 is disaggregated in four unikernels namely
XenStore_uk, net_uk, disk_uk, and tool_uk. Some unikernels
(e.g., device driver unikernels) are made of sub-components.
We equip each unikernel and each sub-component with a
feedback loop that includes fault detection (probes) and repair
(actuators) agents. Both probes and actuators are implemented
outside the target component.We associate our (local) dom0
FT solution with the (distributed) data center management
system (e.g., OpenStack Nova) because the repair of some
failures may require a global point of view. For instance, the
failure of a VM creation request due to a lack of resources on
the server may require to retry the request on another server.
This decision can only be taken by the data center management
system. Therefore, each time a failure occurs, a ﬁrst step repair
solution provided by our system is performed locally on the
actual machine. Then, if necessary, a notiﬁcation is sent to the
data center management system.
A global feedback loop coordinates per-component feed-
back loops in order to handle concurrent failures. The latter
requires a certain repair order. For instance, the failure of Xen-
Store_uk is likely to cause the failure of other unikernels since
XenStore acts as a storage backend for their conﬁguration
metadata. Therefore, XenStore_uk repair should be launched
ﬁrst, before the repair of the other unikernels. We implement
the global feedback loop inside the hypervisor, which is the
only component that we assume to be safe.
Fig. 3: Overall architecture of our FT pVM design.
B. General fault model
This section presents in a generic way the pVM (dom0) fault
model that we target. Additional details are given in the next
sections for each component. In the disaggregated architecture
on which we build our FT solution, the dom0 components
can be classiﬁed into two types: stateful (XenStore_uk) and
stateless (net_uk, disk_uk, and tool_uk). We assume that all
components may suffer from crash faults and that stateful
components can also suffer from data corruption faults. Crash
faults may happen in situations in which a component is
abruptly terminated (e.g., due to invalid memory access) or
hangs (e.g., due to a deadlock/livelock problem). These situa-
tions can make a component either unavailable, unreachable,
or unresponsive when solicited. For stateful components, we
are also interested in data corruption issues, that may stem
from various causes (e.g., an inconsistency introduced by
a software crash, a sporadic bug, or hardware “bit rot”).
Furthermore, our fault model encompasses situations in which
several components are simultaneously in a failed state (either
due to correlated/cascading failures) or due to independent
issues. Besides, our work assumes that the code and data
within the hypervisor component (i.e., Xen) are reliable or,
more reasonably, that potential reliability issues within the
hypervisor are addressed with state-of-the-art fault tolerance
techniques such as ReHype [8] (discussed in §VI). Our design
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:32:03 UTC from IEEE Xplore.  Restrictions apply. 
200
# Base LOCs
# Lines +
net_uk disk_uk tool_uk xenstore_uk
193k
193
8k
27
350k
270k
87
8
TABLE III: Lines of codes added to each unikernel codebase
for fault tolerance.
requires small and localized modiﬁcations (318 LOCs) to
the Xen hypervisor; we believe that they do not introduce
signiﬁcant weaknesses in terms of reliability.
IV. IMPLEMENTATION
To build our disaggregated dom0 architecture, we leverage
the unikernels developed by the Xen project (Mini-OS and
MirageOS). The motivation for these unikernels in the context
of the Xen project is to contain the impact of faults in distinct
pVM components. However, our contribution goes beyond
the mere disaggregation of the pVM: we explain how to
support ﬁne-grained fault detection and recovery for each
pVM component. This section presents the implementation
details of our fault tolerance (FT) solution for each dom0
unikernel, except (due to lack of space) for disk_uk, which
is relatively similar to net_uk. For each unikernel, we ﬁrst
present the speciﬁc fault model that we target followed by the
FT solution (Table III presents a summary of the code size
for each unikernel (existing code + modiﬁcations). Finally, the
section presents the global feedback loop (which coordinates
the recovery of multiple components) and discusses scheduling
optimizations.
Fig. 4: Replication-based FT solution for XenStore.
A. XenStore_uk FT solution
XenStore is a critical metadata storage service, on which
other dom0 services rely. XenStore_uk runs within the
MirageOS unikernel [15].
Fault model. We consider two types of faults. The ﬁrst
type is unavailability, meaning that XenStore is unable to
handle incoming requests, due to bugs (hangs/crashes). The
second type is silent data corruption; such issues may be
caused by bit ﬂips, caused by defective hardware or possibly
malicious VMs (e.g., RowHammer attacks [16], [17]).
state machine
FT solution. We use
replication and
sanity checks to handle unavailability and data corruption
respectively. The overall architecture is depicted in Figure 4.
Note that the memory footprint of a XenStore database is
typically very small (lower than 1MB for 40 VMs).
Unavailability. We organize XenStore into several replicas
(e.g.,
three in our default setup). Each replica runs in a
dedicated unikernel based on MirageOS [15]. The set of
replicas is managed by a coordinator running in a dedicated
the coordinator (noted Cm) is also
unikernel. Notice that
replicated (the replicas are noted Cr) for FT. Cm chooses a
XenStore replica to play the role of the master (noted XSm).
Let us note the other XenStore replicas XSr. Cm is the
XenStore entry point for requests sent by XenStore clients.
We enforce this by modifying the xs_talkv function of the
XenStore client library, used by the other components. Cm
forwards read requests only to XSm, while write requests
are broadcast to all replicas.
We implement this state machine replication strategy using
the etcd coordination system [18] deployed in a MirageOS
unikernel. We choose etcd because of its well-established
robustness, and its relatively lightweight resource requirements
(compared to other coordination systems such as ZooKeeper
[19]). Also, etcd has built-in support for high availability
through strongly-consistent replication based on the Raft con-
sensus algorithm [20]. In the rest of this section we use the
term etcd to refer to Cm and the Cr replicas.
We improve etcd to provide both failure detection and repair
strategies, as follows. etcd is augmented with a heartbeat (HB)
monitor for each XenStore replica. When a replica does not
answer to a heartbeat, etcd pro-actively replaces the replica
with a fresh version, whose state is obtained from another
alive uncorrupted XenStore replica. This recovery process does
not interrupt request handling by other replicas. In case of
the unavailability of XSm, etcd elects another master and
forwards to it the in-progress requests that were assigned to
the crashed master. Cm exchanges heartbeat messages with
the hypervisor so that the latter can detect the simultaneous
crashing of the former and the Cr replicas. In fact, the failure
of one coordinator instance can be handled by the other
instances without the intervention of the hypervisor. The latter
intervenes only when all instances crash at the same time.
Besides, we have modiﬁed the communication mechanism
used between etcd and the other components. Instead of lever-
aging its default communication interface based on the HTTP
protocol, we rely on virtual IRQs and shared memory. The
motivation is twofold. First, this reduces the communication
overheads on the critical path. Second, the utilization of HTTP
would involve net_uk in the failure handling path of XenStore,