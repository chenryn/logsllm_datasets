title:Algorithmic approaches to low overhead fault detection for sparse
linear algebra
author:Joseph Sloan and
Rakesh Kumar and
Greg Bronevetsky
Algorithmic Approaches to Low Overhead Fault
Detection for Sparse Linear Algebra
Joseph Sloan, Rakesh Kumar
University of Illinois,
Urbana-Champaign
jsloan,PI:EMAIL
Greg Bronevetsky
Lawrence Livermore National Laboratory,
Livermore,CA
PI:EMAIL
Abstract—The increasing size and complexity of High-
Performance Computing systems is making it increasingly likely
that individual circuits will produce erroneous results, especially
when operated in a low energy mode. Previous techniques for
Algorithm - Based Fault Tolerance (ABFT)
[20] have been
proposed for detecting errors in dense linear operations, but have
high overhead in the context of sparse problems. In this paper,
we propose a set of algorithmic techniques that minimize the
overhead of fault detection for sparse problems. The techniques
are based on two insights. First, many sparse problems are well
structured (e.g. diagonal, banded diagonal, block diagonal), which
allows for sampling techniques to produce good approximations
of the checks used for fault detection. These approximate checks
may be acceptable for many sparse linear algebra applications.
Second, many linear applications have enough reuse that pre-
conditioning techniques can be used to make these applications
more amenable to low-cost algorithmic checks. The proposed
techniques are shown to yield up to 2x reductions in performance
overhead over traditional ABFT checks for a spectrum of sparse
problems. A case study using common linear solvers further
illustrates the beneﬁts of the proposed algorithmic techniques.
Index Terms—ABFT, sparse linear algebra, numerical meth-
ods, error detection I. INTRODUCTION
As High-Performance Computing (HPC) systems grow
more capable, they also grow larger and more complex. This
means that as the number of components in the systems rises,
so does the probability that one of them will suffer from a fault.
Soft faults in chip circuitry are among the most worrying for
system designers and application developers because they can
corrupt the application’s computations and produce incorrect
output. Tera-scale systems are already vulnerable to soft errors,
with ASCI Q experiencing 26.1 CPU failures per week [18]
and a L1 cache soft error occurring about once every ﬁve hours
on the 104K node BlueGene/L system at Lawrence Livermore
National Laboratory [8]. Looking into the future, according to
the International Technology Roadmap for Semiconductors,
the soft error rates (SER) will grow with smaller chip sizes,
with SRAM SER growing exponentially with chip size [1].
This and the fact that Exascale systems in 2020 will have
a total of 4 million electronic chips with feature sizes as
low as 12nm [1] has led the DARPA Exascale Computing
study [6] to warn that “traditional resiliency solutions will not
be sufﬁcient”. Hardware-based approaches for fault detection
have been proposed for many computing systems. However,
their reliance on redundancy makes them impractical for future
HPC systems which will be increasingly power-constrained.
978-1-4673-1625-5/12/$31.00 ©2012 IEEE
In fact, evolutionary extensions of today’s high performance
computing (HPC) systems (CrayXT, BlueGene) will be unable
to reach exaFLOP performance by 2020 within a power budget
of 20MW, the typical limit of modern computing centers [6].
As such, fault detection for exascale systems will need to
increasingly rely on software or algorithmic approaches, a fact
that motivated the Exascale study to identify “Algorithmic-
level Fault Checking and Fault Resiliency” as a key research
thrust. This paper focuses on algorithmic low overhead fault
detection for sparse linear algebra applications. Sparse linear
algebra forms the core of a large class of high performance
computing (HPC) applications such as linear solvers, differen-
tial equation solvers, and graph analysis problems [11, 19]. It
also forms the core of a large number of emerging recognition,
mining, and synthesis (RMS) applications [5]. Algorithmic
approaches to fault detection for sparse linear algebra will
eliminate the need for high overhead hardware approaches to
fault detection for exascale systems and systems running RMS
applications.
Our algorithmic approach builds upon ABFT-based ap-
proaches that encode computations using linear error cor-
recting codes [20, 2]. Such approaches have been proposed
previously for dense linear algebra [20]. Unfortunately, these
traditional ABFT approaches cannot be used directly for sparse
linear algebra problems as sparse linear algebra problems
have lower algorithmic time complexity than equivalent dense
problems. A direct use of the previously proposed approaches
can result in high overheads for sparse linear algebra prob-
lems (Section V). In this paper, we propose algorithmic
optimizations focused on low overhead checksum-based fault
detection for sparse linear algebra-based applications. Our
fault detection techniques rely on two insights. First, many
sparse applications have inherent structure within the data and
computation (e.g. diagonal, banded diagonal, block diagonal).
These structures may be exploited to improve the performance
of traditional ABFT checks (dense checks) by checking a
representative, randomly sampled, subset of the computation at
the cost of a minor reduction in fault coverage. Second, linear
applications have signiﬁcant reuse. This makes it possible to
precondition the linear problem to be more amenable to low
cost algorithmic checks (Section III).
This paper focuses on fault detection for sparse matrix-
vector multiplication (MVM), the most common operation in
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:22:18 UTC from IEEE Xplore.  Restrictions apply. 
sparse linear algebra. We make the following contributions:
• We demonstrate that previous algorithm-based techniques
for fault detection in MVM are expensive for sparse
matrices.
• We observe that enough structure exists in many sparse
problems that simple sampling techniques provide almost
the same coverage as exact techniques with reduced per-
formance overhead. We present two sampling techniques:
– Approximate Random (AR) checking - randomly
samples the problem
– Approximate Clustered (AC) checking - samples
based on the problem’s structure
• We show that there exists sufﬁcient reuse for many sparse
linear applications that performance can be improved
by preconditioning the problem to reduce the cost of
detection for similar coverage, in spite of increased setup
cost. We propose two preconditioning techniques:
– Identity Conditioning (IC) - computes a code that
creates additional structure for a given problem
– Null Conditioning (NC) - creates structure by ﬁnding
a code which lies in the null space of the sparse
problem
• We quantify the beneﬁts of the proposed techniques in
the context of MVM itself and as a subroutine of linear
solvers. For MVM, the dense checks are shown to have
high overheads (up to 100%, 32% on average) for sparse
problems. The proposed sparse techniques are shown
to reduce the detection overhead by up to 2x (average
overhead is 17%) for the same fault detection accuracy.
Our linear solver implementations with sparse techniques
are 20% faster than the corresponding implementations
using traditional ABFT (dense checks).
The paper is organized as follows. Section II describes
related work and explains the limitations of prior checksum-
based techniques when applied to sparse linear algebra prob-
lems. Section III describes the opportunities for exploiting
the structure of sparse linear problems for reducing fault
detection overhead and introduces our approach. Section IV
discusses the methodology for evaluating the effectiveness
of the techniques. Section V presents the results. SectionVI
concludes.
II. RELATED WORK
There exists a large body of work on different aspects of
sparse linear algebra-based applications. In particular iterative
solvers for sparse linear systems are an important tool for
scientiﬁc computing and research [26, 9, 28, 19]. Common
examples include conjugate gradient and multigrid solvers.
While the majority of the research on sparse linear algebra
addresses parallelization and performance, this paper focuses
on making them resilient to soft faults.
There has been prior work on checksum-based algorithmic
approaches to fault-tolerance of linear algebra-based appli-
cations. Algorithm-Based Fault Tolerance (ABFT) [20] was
proposed to detect and correct errors in matrix multiplication
operations. It has more recently been generalized [2] and
extended to more general linear algebra algorithms [23, 25, 3]
such as the multi-grid solver [24] as well as to multiproces-
sors [3].
The traditional ABFT check works by encoding a linear
operation using linear error correcting codes. For example, the
check for MVM (Ax: matrix A, vector x) works by verifying
the identity:
cT (Ax) = (cT A)x
Intuitively, the check computes the projection of the result
Ax onto the vector c in two different ways. If there are any
computation errors, the two projections will very likely be
unequal (e.g. the difference between projections surpasses a
given threshold, τ)
In the common case where c = ¯1 (a vector of all 1’s), the
projection is equivalent to multiplying x by the vector contain-
ing the sums of matrix A’s columns. Because the dense check
focuses on the projection of the result onto a speciﬁc vector, it
requires only three operations: (i) a matrix-vector product that
must be done for each matrix A and (ii) two dot-products that
must be performed on every MVM. This check is therefore
very efﬁcient for dense matrices, requiring O(n2) setup time
and O(n) time for each MVM operation, compared to the
O(n2) time required for the original multiplication. However,
the check becomes very expensive for sparse matrices, where
MVM takes only O(n) time, meaning that both the original
operation and the check have equal asymptotic complexity.
In this paper, we address this problem by exploiting the
properties of sparse linear algebra applications to reduce the
constant factor of the the ABFT check, making it signiﬁcantly
cheaper than the original operation. Indeed, to the best of our
knowledge, it is the ﬁrst work to address application-level fault
tolerance in the context of general sparse linear algebra.
III. ALGORITHMIC FAULT DETECTION
In this section, we discuss two opportunities for sparse linear
algebra applications that can be exploited to reduce the over-
head of algorithmic fault detection for such applications. We
then describe four techniques that exploit these opportunities.
A. Motivation
Sparse problems frequently have well deﬁned structures.
Common examples of structure are diagonal, banded diagonal,
and block diagonal matrices. For example, qpband (Figure 1),
which represents a canonical indeﬁnite optimization problem,
illustrates a typical banded diagonal structure (the nonzero
pattern is on the left). Similarly, the matrix bcsstm37 (Figure
3), which represents a track ball stiffness matrix [4] and the
matrix msc00726 (Figure 2), representing a structural engi-
neering problems from the Boeing test matrix group [4], also
contain banded diagonal type structures. The matrix Oregon-
1 (Figure 4), representing an undirected graph based on the
network included in a portion of the Internet, shows a block
diagonal type structure.
Such structures in sparse problems commonly translate into
fairly uniform distributions of the column sums. Since, as
described in Section II, the traditional check for c = ¯1 is
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:22:18 UTC from IEEE Xplore.  Restrictions apply. 
Column Sum Distributions
10000
9000
8000
7000
6000
5000
4000
3000
2000
1000
s
n
m
u
o
C
l
f
o
r
e
b
m
u
N
Column Sum Distributions
20
18
16
14
12
10
8
6
4
2
s
n
m
u
o
C
l
f
o
r
e
b
m
u
N
0
−1
−0.5
0
1
0.5
2
Column Sums
1.5
2.5
3
3.5
0
−1.5
−1
−0.5
0
0.5
Column Sums
1
1.5
2
x 108
qpband (Variance = 1.6071) The matrix has a well deﬁned and low
Fig. 1.
variance ( 1e3) column sums. This matrix is a good candidate for clustering given the
ﬁnite sets of unique values shown above.
Column Sum Distributions
800
700
600
500
400
300
200
100
s
n
m
u
o
C
l
f
o
r
e
b
m
u
N
Column Sum Distributions
100
90
80
70
60
50
40
30
20
10
s
n
m
u
o
C
l
f
o
r
e
b
m
u
N
Fig. 3. bcsstm37 (Variance= = 6.1668e− 10). The matrix has a well deﬁned
column distribution with low variance ( 1e3). Conditioning is a good
candidate for this particular problem.
If c = ¯1, this computation is equivalent to computing the
vector of A’s column sums (i.e. cT A) and multiplying it
by x. Thus,
if a matrix’s column sums have a relatively
simple structure, it may be possible to perform the check
over a randomly sampled subset of columns. For example,
the variance in column sums is only 1.6 in matrix Qpband
(Figure 1) and 6e− 10 in bcsstm37 (Figure 3). This technique,
called Approximate Random (AR), works by setting c to be
a binary vector, with 1’s in a some random locations and 0’s
everywhere else. The detection overhead is therefore reduced
by avoiding computations associated with dimensions of the
check containing 0’s. It is further reﬁned by observing that the
primary cost of the check is the sparse matrix-vector product
on the right-hand side, while the left-hand side is a dense
dot-product, which is faster due to its much better memory
behavior. Moreover, sampling the left-hand side will cause the
check to incur a more signiﬁcant loss in fault coverage since
the left-hand side is a function of the faulty MVM output
directly. As such, this check only uses the sampled c on the
right-hand side, uses c = ¯1 on the left-hand side and then
normalizes the left-hand side to adjust for the difference.
The accuracy of AR depends on the distribution of the
values in x, in addition to depending on the distribution of
the matrix columns. In the context of computational science,
x typically corresponds to the state of a physical system. Since
different regions of the physical space will have similar states,
x will have a regular structure, which enables AR to work well.
However, in cases where the physical system is chaotic (high
variance) or x comes from a non-physical system, the sampling
technique may need to take the distribution of x into account
as well (e.g. a scaling factor related to the input variance).
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:22:18 UTC from IEEE Xplore.  Restrictions apply. 
C. Approximate Clustering
For matrices with more variable column sums that still
have some structure it is possible to improve the quality of
the sampled columns by clustering their sums. Instead of
computing c by randomly sampling ¯1, Approximate Clustering
(AC) runs a clustering algorithm on the set of column sums