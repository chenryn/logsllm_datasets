but is sufficient for our purposes. The symbolic traces capture all
necessary information on how the model is used.
4It is sufficient to show that the model over-approximates the contracts, because any
proof that holds for the over-approximation holds for an exact model as well.
9
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
A. Zaostrovnykh, S. Pirelli, L. Pedrosa, K. Argyraki, and G. Candea
1 struct ring* arg1;
2 struct packet arg2;
3 bool packet_is_sent = false;
4 struct packet* sent_packet = NULL;
5
6 loop_invariant_produce(&arg1);
7 //@ open loop_invariant(&arg1);
8 bool ret1 = ring_full(arg1);
9 //@ assume(ret1 == true);
10 bool ret2 = ring_empty(arg1);
11 //@ assume(ret2 == false);
12 bool ret3 = can_send();
13 //@ assume(ret3 == true);
14 //@ close packetp(&arg2, packet(arg2.port));
15 ring_pop_front(arg1, &arg2);
16 //@ openpacketp(&arg2, _);
17 //@ assume(arg2.port != 9);
18 send(&arg2);
19 packet_is_sent = true;
20 sent_packet = &arg2;
21 //@ close loop_invariant(&arg1);
22 loop_invariant_consume(&arg1);
23
24 /*@ if (packet_is_sent) {
25
26 } @*/
assert(sent_packet->port != 9);
Figure 10: The trace of Fig. 9, translated into a proof.
The technique for proving that the libVig model is consistent
with the libVig interface contracts is similar to the one we use
for proving NAT semantics. Only this time, instead of weaving
the NAT pre- and post-conditions into the traces, the Validator
weaves in the assertions for the given trace’s path constraint. It then
asks the proof checker to verify whether the assertions hold based
solely on the post-conditions of the libVig functions. If verification
succeeds, then it means that, after each invocation of the libVig
model, the outcome covers all possible outcomes prescribed by the
libVig interface contracts.
A libVig model can be over-approximate, under-approximate, or
both. The question that the Validator aims to answer is whether, for
the particular NF and properties, the model is sufficiently accurate.
If a model is “too under-approximate” for the desired proof, it will
cause the validation phase to fail, because its narrow behavior
does not cover the spectrum of behaviors allowed by the contracts.
If it is “too over-approximate” for the target proof, it will either
cause exhaustive symbolic execution (ESE) or validation to fail—the
former if the model exhibits behavior that is too general and makes
it impossible to verify the low-level properties, the latter if low-
level properties verify but a loop invariant or a high level semantic
property is violated. When ESE completes, we have proof that the
low-level properties hold, as long as the model is valid. ESE failure
means that either there is a violation of a low-level property or the
model offered by libVig is not suitable—Vigor does its best to help
the developer distinguish between the two, but there is still room
for improvement. In either case, it’s back to the drawing board:
either the NF developer needs to fix her bug, or the Vigor developer
needs to alter the model.
10
Vigor also uses a model we wrote of the DPDK packet processing
framework’s send, receive, and free calls. We do not formally vali-
date this model, though there is no fundamental reason it cannot
be done. We make it part of Vigor’s trusted computing base (§5.4).
5.2.4 Proving that VigNAT correctly uses libVig. There is one
caveat to the proof in the previous section: if a libVig method imple-
mentation is invoked and the corresponding pre-condition does not
hold, then the behavior of that method is undefined. For example,
passing a null argument when the contract says it must be non-
null could cause the implementation to crash, behave incorrectly,
or behave correctly. It is therefore imperative that, in conjunction
with validating the model’s behavior, Vigor also validate the caller’s
behavior with respect to the interface contracts.
The method for proving that the VigNAT stateless code uses
the libVig data structures consistently with the libVig interface
contracts is the same as above, except that the Validator weaves in
the pre-conditions contained in the libVig interface contracts. In
fact, the Validator weaves in the NAT pre- and post-conditions and
the libVig pre- and post-conditions in one go, and generates a single
verification task per trace that simultaneously verifies properties
P1, P4, and P5.
The trickiest part in verifying the libVig pre-conditions is track-
ing memory ownership across the interface. A pointer returned
by a libVig method (either as a return value or via an output pa-
rameter) references memory owned at first by libVig; upon return,
ownership transfers to the caller. After using/modifying the pointer,
the NF code calls another function to return ownership to libVig.
A pointer passed as an argument to a libVig method may be the
address of a libVig data structure (equivalent to the this pointer
in C++/Java or the self reference in Python). This type of pointer
remains opaque to the stateless code: it can be copied, assigned, and
compared for equality, but cannot be dereferenced. The Validator
and proof checker need not look at the memory pointed to by
such pointers but only keep track of aliasing information. The
pointed-to memory is owned by libVig at all time. Vigor verifies
that the stateless code obeys this pointer discipline during symbolic
execution, using our addition to the SEE for enabling/disabling
dereferenceability of a pointer between libVig calls.
A pointer used as an output parameter points to where the caller
expects libVig’s return result to be written. In this case, the pointed-
to memory is owned by the calling code. The Validator and proof
checker trace the evolution of the pointed-to memory by including
it in the function’s input set before the call and in the function’s
output set after the call. A special case of output pointer is a double
pointer to a library data structure (e.g., X** p) as appears in the data
structure allocation functions. The VigNAT code owns the pointee
*p, so the Validator tracks it, but the pointee of the pointee **p is a
library data structure, thus the memory is owned by the library, so
there is no need to track it. Vigor currently does not support other
cases of double or deeper pointers.
Vigor also checks for memory leaks. Even though stateless code
cannot dynamically allocate memory, leaks are possible if it uses
libVig incorrectly, such as forgetting to call a release method. Un-
like simple low-level properties (e.g., integer overflow) that can be
stated as a simple assert, absence of memory leaks is a global prop-
erty. Vigor therefore must keep track of memory ownership and
A Formally Verified NAT
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
validate that ownership is properly returned to libVig before the
end of the execution. This facility, for example, caught an accidental
memory leak in VigNAT where we failed to release DPDK memory
corresponding to a packet returned by DPDK, thus violating the
DPDK interface contracts.
5.3 The Vigor Workflow
The Vigor workflow described above can be summarized as fol-
lows: We split the NAT NF into a stateless and a stateful part, the
latter contained in the libVig library. Then we use formal theo-
rem proving to verify P3—correctness of the data structures im-
plemented in libVig. We use exhaustive symbolic execution (ESE)
with a modified version of KLEE [9] to explore all paths in the
stateless part (using symbolic models of the data structures) and
verify P2—low-level properties, like crash freedom, memory safety,
and no overflows—as well as VigNAT’s disciplined use of pointers.
This step proceeds under the assumptions that the stateless code
uses the libVig data types according to their interface contracts (P4)
and the libVig model is faithful to the libVig interface contracts (P5)
for the particular execution paths explored during ESE. Both of
these assumptions we prove a posteriori using a combination of
our Vigor Validator and the VeriFast proof checker [28]. Finally, we
use this same combination of tools to prove VigNAT’s semantic
properties (P1), i.e., that it conforms to our formalization of RFC
3022 [53]. P1 ∧ P2 ∧ P3 ∧ P4 ∧ P5 together formally prove VigNAT’s
correctness, under the assumptions described in the next section.
5.4 Assumptions
The trusted computing base for a Vigor-verified NF consists of the
Vigor toolchain (the Clang LLVM compiler, VeriFast, KLEE, and our
own Validator) and the environment in which the NF runs (DPDK,
device drivers, OS kernel, BIOS, and hardware). We assume that
the compiler implements the same language semantics employed
by Vigor (e.g., same byte length for C primitive types). We wrote
symbolic models for three DPDK functions and for system time,
which as of this writing we have not verified. They are small (about
400 LOC), so we convinced ourselves manually that they are correct
over-approximations. One could envision adopting an environment
that has a formal specification, like seL4 [34], in which it becomes
possible to prove the validity of these models.
6 PERFORMANCE EVALUATION
Having shown in previous sections how we verify VigNAT’s cor-
rectness, we now demonstrate that this formal verification does not
come at the cost of performance: compared to an unverified NAT
written on top of DPDK, our verified NAT offers similar latency
and less than 10% throughput penalty. We focus our evaluation on
comparing VigNAT (labeled Verified NAT in the graphs) to three
other NFs:
(a) No-op forwarding is implemented on top of DPDK; it re-
ceives traffic on one port and forwards it out another port without
any other processing. It serves as a baseline that shows the best
throughput and latency that a DPDK NF can achieve in our experi-
mental environment.
(b) Unverified NAT is also implemented on top of DPDK; it im-
plements the same RFC as VigNAT and supports the same number
of flows (65,535), but uses the hash table that comes with the DPDK
distribution. It was written by an experienced software developer
with little verification expertise, different from the one who wrote
and verified VigNAT. It serves to compare VigNAT to a NAT that
was not written with verification in mind.
(c) Linux NAT is NetFilter [5], set up with straightforward mas-
querade rules and tuned for performance [29]. We expect it to be
significantly slower than the other two, because it does not benefit
from DPDK’s optimized packet reception and transmission. We
use it to make the point that VigNAT performs significantly bet-
ter than the typical NAT currently used in Linux-based home and
small-enterprise routers, as one would expect from a DPDK NAT.
We use the testbed shown in Fig. 11 as suggested by RFC 2544 [7].
The Tester and the Middlebox machines are identical, with an Intel
Xeon E5-2667 v2 processor at 3.30 GHz, 32 GB of DRAM, and
82599ES 10 Gbps DPDK-compatible NICs. The Middlebox machine
runs one of the four NFs mentioned above (we use one core). The
Tester machine runs MoonGen [22] to generate traffic and measure
packet loss, throughput, and latency; for the latency measurements,
we rely on hardware timestamps for better accuracy [49]. We use
DPDK v.16.07 on Ubuntu Linux with kernel 3.13.0-119-generic.
Figure 11: Testbed topology for performance evaluation.
First, we measure the latency experienced by packets between
the Tester’s outbound and inbound interfaces. We first run a set of
experiments in which all the NATs are configured to expire flows
after 2 seconds of inactivity. In each experiment, the Tester gener-
ates 10–64,000 “background flows,” which produce in total 100,000
pps and never expire throughout the experiment, and 1,000 “probe
flows,” which produce 0.47 pps and expire after every packet. We
use the background flows to control the occupancy of the flow
table, while we measure the latency of the packets that belong to
probe flows. We focus on the probe flows because, from a perfor-
mance point of view, they are the worst-case scenario for a NAT
NF: each of their packets causes the NAT to search its flow table
for a matching flow ID, not find any match, and create a new entry.
Fig. 12 shows the average latency experienced by the probe flows
as a function of the number of background flows, for the three DPDK
NFs: the Verified NAT (5.13µsec) has 2% higher latency than the
Unverified NAT (5.03µsec), and 8% higher than No-op forward-
ing (4.75µsec). So, on top of the latency due to packet reception and
transmission, the Unverified and Verified NAT add, respectively,
0.28µsec and 0.38µsec of NAT-specific packet processing. For all
three NFs, latency remains stable as flow-table occupancy grows,
which shows that the two NATs use good hash functions to spread
the load uniformly across their tables. The only case where latency
increases (to 5.3µsec) is for the Verified NAT, when the flow table
becomes almost completely full (the green line curves upward at
the last data point). The Linux NAT has significantly higher latency
(20µsec).
To give a sense of latency variability, Fig. 13 shows the comple-
mentary cumulative distribution function (CCDF) of the latency
11
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
A. Zaostrovnykh, S. Pirelli, L. Pedrosa, K. Argyraki, and G. Candea
Figure 12: Average latency for probe flows. Confidence inter-
vals are approximately 20 nanosec, not visible at this scale.
experienced by the probe flows, when there are 60,000 background
flows (i.e., 92% occupancy): the Verified NAT has a slightly heavier
tail than the Unverified NAT; all three NFs have outliers that are two
orders of magnitude above the average, but these are due to DPDK
packet processing, not NAT-specific processing (the three curves
coincide for latency exceeding 6.5µsec). The CCDFs computed for
different numbers of background flows look similar.
Figure 13: Latency CCDF for probe flows.
We get similar results in a second set of experiments, where
the Tester produces the same flow mix as before, but the NATs
are configured to expire flows after 60 seconds of inactivity (hence
neither the probe flows nor the background flows ever expire).
In this case, the average latency of the Verified NAT is slightly
lower (5.07µsec), while that of the Unverified NAT the same as
before (5.03µsec).
Finally, we measure the highest throughput achieved by each NF.
In each experiment, the Tester generates a fixed number of flows
that never expire, each producing 64-byte packets at a fixed rate, and
we measure throughput and packet loss. During all experiments, the
Middlebox is CPU bound. Fig. 14 shows the maximum throughput
achieved by each NF with less than 0.1% packet loss, as a function
of the number of generated flows. The Verified NAT (1.8 Mpps)
12
Figure 14: Maximum throughput with a maximum loss rate
of 0.1%.
has 10% lower throughput than the Unverified NAT (2 Mpps). This
difference in throughput comes from the difference in NAT-specific
processing latency (0.38µsec vs. 0.28µsec) imposed by the two NATs:
in our experimental setup, this latency difference cannot be masked,
as each NF runs on a single core and processes one packet at a time.
The Linux NAT achieves significantly lower throughput (0.6 Mpps).
In essence, these results indicate that the performance of the
libVig flow table (which has a formal specification and proof) is
close to that of the DPDK hash table (which has neither), though
not the same. The implementations of the two data structures are
quite different. We did not try to reuse/adapt the implementation
of the DPDK hash table, because it resolves hash conflicts through
separate chaining—items that hash to the same array position are
added to the same linked list—a behavior that is hard to specify in
a formal contract. Instead, the libVig flow table resolves conflicts
through open addressing: if an item hashes to an occupied array
position, it is stored in the next array position that is free, together
with auxiliary metadata that speeds up lookup. We have not yet