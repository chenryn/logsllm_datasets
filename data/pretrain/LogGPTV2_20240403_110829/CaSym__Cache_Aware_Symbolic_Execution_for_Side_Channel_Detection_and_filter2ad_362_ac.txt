the end of Fig. 4 follows equations (3) and (4).
The beneﬁt of path merging is that further symbolic
execution beyond the merge point can just use the newly
introduced logical variables (ψ, X, and C in the equations
above); so all the paths beyond the merge point share the
logical variables and equations, resulting in compact logical
formulas. We also note that CaSym’s implementation takes
a control-ﬂow graph as an input and treats every node with
more than one adjacent predecessor in the graph as a merge
point; as a result, new logical variables are introduced and
path merging is performed before every node in the graph
with this characteristic.
We note that our path mering is similar to those proposed
in [41], [42] (though they do not consider merging cache
states). We could further optimize the formulas by applying
techniques in [43], [44], which selectively use path merging
when it is adventurous to do so; but we leave that as future
work.
B. Compositional reasoning
The cache-aware symbolic execution sketched above only
handles programs with a ﬁnite number of execution paths.
However, practical software usually have an unbounded or a
large number of execution paths, making symbolic execution
infeasible or inefﬁcient. To tackle such a challenge, we
introduce two novel statements to enable compositional
reasoning
Stmt S ::= . . . | reset | check K
The “reset” statement resets the current symbolic state to an
arbitrary initial symbolic state; “check K” directs CaSym
to issue a veriﬁcation condition to a solver based on the
current symbolic state, assuming only the secret-variable set
K carries conﬁdential data at the last reset. The rules for
performing symbolic execution over them are shown below:
SE(reset, σ) = initial(x(cid:2), c(cid:2), X, C)
SE(check K, σ) = σ, and issue condition VC(σ, K)
The deﬁnitions of initial(x(cid:2), c(cid:2), X, C) and VC(σ, K) were
given earlier in equations (1) and (2). Note that we use x(cid:2)
and c(cid:2)
to distinguish them from the initial state of symbolic
execution.
The introduced “reset” and “check K” statements have
several beneﬁts:
Flexibility: The check and reset statements allow
CaSym to ﬂexibly decide where to reset
to the initial
state and where to check for cache-based side channels.
For example, by turning S into “reset; S; check K0”, we
tell CaSym to perform symbolic execution from the initial
symbolic state and perform the side-channel check at the
end, assuming that the initial secret variables are in K0
and an attacker observes the cache state only at the end
of the program. As another example, when “S = S1; S2”
and the attacker can observe the cache state in the middle
and at the end, we can perform symbolic execution over
“reset; S1; check K0; S2; check K0”, which triggers two
side-channel checks. In the most extreme case, a check
can be inserted at every control-ﬂow point in the program,
corresponding to what a trace-based attacker can observe
(discussed in Section II-B).
Scalability: The new statements also enable compo-
sitional, scalable reasoning. Suppose performing symbolic
execution over the entire program S produces a large for-
mula at the end. Feeding the formula to an SMT solver may
not be feasible given the amount of time that is needed to
solve the formula. One way of reducing the time pressure
is to break S into two parts and check them individually.
Suppose S = S1; S2, we can then turn it into
reset; S1; check K0; reset; S2; check K1
(cid:22)(cid:18)(cid:18)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:41:15 UTC from IEEE Xplore.  Restrictions apply. 
The ﬁrst check veriﬁes that S1 is free of cache-based side
channels; that is, running S1 twice with two different secrets
and the same initial cache states results in the same cache
states. After this check, we can reset the symbolic state and
perform symbolic execution on S2 and the check after S2
veriﬁes that running S2 twice with different secrets and the
same initial cache states results in the same cache states.
This is a rely-guarantee reasoning since, when checking S2,
it relies on the assumption that the initial cache states are the
same and the assumption is discharged by the veriﬁcation
on S1. The compositional reasoning is more scalable than
checking S1; S2 as a whole, since the reset in the middle
throws away the symbolic state. However, it may cause false
positives when checking S2 due to the loss of information.
One complication in the above process is that the two
check statements are with respect to two separate secret-
the ﬁrst check assumes K0 is the secret-
variable sets:
variable set at the beginning, while the second check as-
sumes K1 is the secret-variable set at the point between
S1 and S2. The two sets might be different; for instance,
K0 might be {X1}, and, if S1 copies X1 to X2, then
the set of secret variables after S1 becomes {X1, X2}.
In general, the set of secret variables may change due to
secret propagation in a program. To soundly estimate the
set of secret variables, CaSym has a static taint tracking
component, which takes initial secret variables and outputs
the set of secret variables at each program location. This was
implemented by a standard ﬂow-sensitive dataﬂow analysis
in LLVM. With the result of this analysis, CaSym knows
the secret-variable set at each location, including K1.
C. Transforming loops
Symbolic-execution systems for bug ﬁnding only explore
a limited number of paths. Hence, they do not guarantee a
coverage of all paths for programs with loops. With the help
of the new statements introduced in Section V-B, CaSym
transforms programs with loops into loop-free programs.
Speciﬁcally, the transformation works as follows:
S1; (WHILE B DO S); S2
⇒
S1; check K0; reset;
(IF B THEN (S; check K1) ELSE SKIP);
reset;
¬B → S2
This transformation is sound (i.e., the original program
is side-channel free whenever the transformed one is) since
the loop-free program enforces the following invariant in
the original program: any two executions of S starting from
the same initial cache state results in the same ﬁnal cache
state. From the Hoare logic point of view, this is the cache-
state loop invariant checked by CaSym. Note that this is
performed with respect to K1, the set of secret variables right
before the loop body S (this is determined by tracking how
values of secret variables propagate in the original program,
as discussed before).
To see why the invariant is enforced, the ﬁrst check makes
sure that the initial cache states are identical before the loop
(i.e., S1 is side-channel free). After that, the symbolic state
is reset. Hence, statement (check K1) ensures that there is
no side channel for the loop body starting from any memory
and cache state. After checking the loop body, the symbolic
state is reset again so that the veriﬁcation of S2 assumes
nothing after the if statement, which semantically represents
the memory and cache state after zero or one loop iteration.
After that, ¬B can be assumed when checking S2.
In theory, the transformation may cause some false pos-
itives. For example, the transformation assumes nothing on
the initial memory and cache state before each iteration,
which may cause false positives. But in practice, we have
found only one false positive due to the transformation in
database systems (Section IX). Moreover, when a loop has
a constant number of iterations, we can also unroll it for
better precision.
VI. CACHE MODELS
CaSym takes a cache model and identiﬁes potential side
channels based on it. In principal, it can take any cache
model with sufﬁcient abstractions in place: the empty cache
state,
the cache-update function, and the equality-testing
function. In this section, we introduce two novel abstract
cache models: the inﬁnite cache model and the age model.
We also discuss how to support more concrete models, such
as the LRU model, used in previous work.
A. Abstract vs. concrete cache models
Concrete cache models (e.g., LRU, FIFO, PLRU models
used in [12]) accurately model details such as the replace-
ment policy of the expected architecture that a program will
be executed on. While such detailed models allow accurate
reasoning about the cache state (i.e., existence or absence of
data in the cache), one downside is that the veriﬁed programs
are secure only on those expected architectures. For example,
a crypto implementation that is side-channel-free on cache
with LRU might have side channel on cache with FIFO.
Moreover, reasoning over a concrete model likely will cause
scalability issues for static program analysis.
Another approach is to use a higher-level abstraction, such
as the entire trace of memory accesses [13], [11]. Doing
so is architecture-independent and sufﬁcient since in all
realistic architectures, cache state is determined by the trace
of memory accesses. However, this approach may be too
conservative, since the footprint of secret dependent memory
accesses might be erased by later accesses before an attacker
probes the cache.
We propose two novel cache models that offer good
balance between precision and generality. The inﬁnite cache
model represents an optimistic view of the cache: if there
(cid:22)(cid:18)(cid:19)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:41:15 UTC from IEEE Xplore.  Restrictions apply. 
is a side channel under this model, then the side channel
likely will exist in realistic cache models (i.e., they are
high-priority side channels that may show up on most
architectures); the age model represents a pessimistic view
of the cache: if there is a side channel under the this model,
then the side channel likely will exist in some realistic cache
model (i.e., they are low-priority side channels that may
show up on some architectures). Empirical results suggest
that the inﬁnite cache model and the age model achieves
a good balance between analysis scalability and precision
(Section IX).
B. Inﬁnite cache model
This is an idealized cache model with an inﬁnite size
and associativity, so that it never evicts any data that is
already being cached. This is clearly idealized, but it is also
interesting since it represents an optimistic view of cache:
if there is a side channel under the inﬁnite cache model,
the side channel likely will exist in other more realistic
cache models. Moreover, it is the (conceptual) model that
cryptography software writers have in mind when they apply
software countermeasures to cache-based side channels. One
example is preloading in cryptography software, which we
detail
in Section VII-B1. Furthermore, empirical results
suggest that the inﬁnite cache model offers a signiﬁcant
speedup with few false negatives on both crypto systems
and database systems, compared with more conservative and
realistic cache models.
In the inﬁnite cache model, a cache state is represented as
a set of symbolic memory addresses for program variables
and array elements.
• The empty cache is the empty set: emptyC = {}.
• The cache-update function is implemented as set union:
updC(l, ce) = {l} ∪ ce.
• The cache-equality testing becomes set equality:
eqC(ce 1, ce 2) = ∀l, l ∈ ce 1 ↔ l ∈ ce 2
To see why this model is more optimistic than other more
realistic models, we note that ce 1 and ce 2 are different only
if there is some address l that is accessed in one execution
but not in the other, starting with different conﬁdential data.
Except for a fully-associative cache, that implies if the com-
piler maps l to some cache set and maps all other addresses
to other cache sets, most cache replacement policies will
result in a difference in the cache set that l gets mapped to.
Hence, this model gives a “lower bound” on side channels
among various cache models.
C. Age model
Unlike the optimistic inﬁnite model, the age model is on
the pessimistic end: for each symbolic memory location,
it tracks the distance to its most recent access, called the
age. The recently accessed location has age zero, while the
second recently accessed location has age one, and so on. In
(cid:22)(cid:18)(cid:20)
this model, a cache state is a map from symbolic memory
locations to their ages:
emptyC = λl. ∞.
• The empty cache maps all memory locations to inﬁnity:
• The cache-update function marks the current location’s
age to be zero and increments other locations’ ages by
one: updC(l, ce) = λl(cid:2), if l(cid:2) = l then 0 else ce(l(cid:2)) + 1
ages:
eqC(ce 1, ce 2) = ∀l, ce 1(l) = ce 2(l).
cache-equality
equality
• The
tests
of
The age model is the opposite of the inﬁnite cache model:
while the inﬁnite cache model may miss (less crucial) side
channels that only manifest themselves for some particular
caches, the age model captures all potential side channels
for most caches.
Property 1. If there is no cache-based side channel on the
age model, then there is no cache-based side channel for any
cache replacement policy that replaces cache lines based on
the most recent accesses, such as LRU.
To see why, we note that the ﬁnal cache expression ce
tracks the sequence of the last access to each memory
address. For any cache replacement policy that depends only
on the latest usage of memory addresses, such as LRU,
it implies that the ﬁnal cache state can be expressed as a
function of ages. Hence, eqC(ce 1, ce 2) implies the same
cache state under those policies.
For a trace-based attacker, a stronger result holds:
Property 2. For a trace-based attacker, no cache-based
side channel on the age model implies no cache-based side
channel for any deterministic cache replacement policy (i.e.,
a replacement policy that can be expressed as a function of
memory address traces), such as FIFO and LRU.
The reason is that there is a one-to-one mapping between a
sequence of ages (for all symbolic locations), and a sequence
of memory locations being accessed. Consider a sequence
of ages A = {ce 1, ce 2, . . . , ce n} as well as a sequence of
memory locations being accessed, say T = {t1, t2, . . . , tn}.
Then, we can construct A from T as follows:
ce i = λl. if ti = l then 0 else ce i−1(l) + 1
Also, we construct T from A as follows: ti = l iff ce i(l) = 0
(note that exactly one l in ce is 0 at any program point).
D. More concrete models
While our inﬁnite cache and age models are capable of
detecting side channels, we show how to enrich a cache
model in CaSym if more cache details (such as cache line
size, associativity and replacement policy) are needed for
precision reasons.
Cache line size: To model cache lines, we simply
take the index into the array and compute the cache-
line-granularity location being accessed. More speciﬁcally,
suppose that an integer array A is aligned and location
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:41:15 UTC from IEEE Xplore.  Restrictions apply. 
A[X] is being accessed and x is the symbolic value of
X. We simply use (x/LIN ESIZE) as the location l in
the updC(l, ce) interface to the cache models above, where
LIN ESIZE = 64/4 = 16 assuming 4-bytes integer and
64-bytes cache line.6 Note that cache line size only affects
array accesses, since the memory layout for other variables
are unknown at the IR level.
Cache associativity: To model cache associativity, we
model the cache state ce as a collection of non-overlapping
cache sets (i.e., ce = [c1, c2, . . . , cW ]). The empty cache and
equality test on ce is simply the lifted deﬁnition of those
on each cache state. For the update function, let way be a
function that maps an array index to the corresponding cache
set (the deﬁnition of way depends on cache conﬁguration),
then the following formula illustrates how the new cache
state would be computed when an array is accessed.
updC(M A[X], ce) = updC(M A[X], ce[way(X)])
LRU replacement: In the LRU model, the cache state is
still modeled as a map from symbolic memory locations to
their ages. The empty cache and the cache-update function
remain the same as the age model. The cache-equality test,
however, is changed to
eqC(ce 1, ce 2) = ∀l, ce 1(l) < n ↔ ce 2(l) < n.
to reﬂect the fact that if l is in the ﬁnal cache state or not.
Here we assume n is the cache size.
VII. LOCALIZING AND MITIGATING SIDE CHANNELS
As discussed so far, satisﬁable constraints at a certain
program point suggest potential side channels. Although
such a binary decision helps to some extent, one novel
feature of CaSym is the ability to help programmers localize
the cause of the identiﬁed side channels as well as to mitigate
them.
A. Localizing side channels
To localize and explain the causes of the identiﬁed side
channels, we leverage the key information generated by the
SMT solver: a model of constraints. A model consists
of the concrete values for each constraint variable in the
inequality test ¬(eqC(ce 1, ce 2)). According to the way that
the formulas are built, a model consists of two sets of values
(one used by ce 1 and one used by ce 2) that will lead to
different cache states. We refer to those two value sets as M1
and M2 respectively. Our localization algorithm proceeds in
two steps.
6The computation assumes row-major layout for arrays. Column-major
layout can be handled in similar way.
Compute shared path: We ﬁrst use M1 and M2 to
reconstruct two execution paths taken according to the model
reported by the solver. This is possible since CaSym keeps
track of the path condition for each basic block. Given
M1 and M2, we can tell which basic block is in a path
by checking the validity of its path condition. Based on
that, we compute the shared blocks between them. Finally,
we traverse the control ﬂow graph in topological order to
recover a path of those shared blocks. We call the path the