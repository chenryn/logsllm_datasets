### Submission Timestamps and Corresponding Scan Reports

This unlabeled data is utilized for training generative and self-supervised models to detect both phishing and malware URLs.

### Fine-Tuning

For phishing URLs, we use PhishTank (PT), a collaborative platform for verifying suspected phishing URLs. Users submit potential phishing URLs to PT, which are then verified and voted on by other users. We implemented a web crawler to collect newly submitted URLs from PT. After filtering out invalid URLs (e.g., malformed URLs), we submit the remaining URLs to VirusTotal and collect the scan reports. A URL is categorized as "fresh" if the first scanned time in VirusTotal matches our submission time. Approximately 54% of the URLs submitted to PhishTank are classified as fresh. For these fresh URLs, we rescan them in VirusTotal every hour, generating a time series of scan reports. Simultaneously, we track their status in PhishTank. If a URL is verified as a phishing URL in PhishTank, it is labeled as such.

For malware URLs, we use URLhaus, a database of malware URLs submitted by users. We collect VirusTotal scan reports for newly submitted URLs from URLhaus, similar to the process for PhishTank. We observe that 60% of the URLs submitted to URLhaus are categorized as fresh.

To avoid biases, we exclude the responses from PhishTank and URLhaus scanners from the scan reports since they are part of the VirusTotal scanner set. This dataset is used to obtain labels for supervised and semi-supervised approaches.

### Final Evaluation

We collected 3,000 scan reports, corresponding to 3,000 distinct URLs, over 7 days using an online stratified sampling approach proposed in [34], [35]. The goal is to select a small set of scan reports for labeling by human experts. These URLs and domains are distinct from those in the training data.

The ground truth for these scan reports is obtained through manual inspection by cybersecurity experts. They follow specific criteria to identify malicious URLs, including:
- Distribution of malware binaries
- Association with known indicators of compromise
- Domain squatting (e.g., using a popular brand name within a domain name)
- Presence in social engineering attacks (e.g., phishing emails)
- Mimicking the look and feel of benign websites (e.g., login pages of banks or online stores)
- Owner of the website
- Historical registration records of the website
- TLS certificates associated with the website (if available)
- Historical domain access patterns available through passive DNS services
- Infrastructure in which the domain is hosted

### IP Blacklists

We use an unlabeled dataset collected by the authors of [19] for our experiments. This dataset monitors 157 publicly available blacklists, covering various attack vectors such as spam, malware, DDoS attacks, and ransomware. The blacklists have update frequencies ranging from 15 minutes to 7 days. Our experiments were conducted using daily snapshots of the blacklists for the years 2019-2020. Specifically, we use a form of temporal cross-validation: six months of daily snapshots for pre-training, one month for fine-tuning, and the following three months for evaluation. This process is repeated for every contiguous 6-month period. For example, we first pre-train on blacklists from January to June 2019, then from February to July 2019, and so on.

We treat the output of BLAG [19] as a proxy for the ground truth. BLAG achieves state-of-the-art results with specificity as high as 99% and can report malicious sources up to 9.4 days ahead of the best blacklist. Comparing SIRAJ against BLAG demonstrates that our domain-agnostic pretext tasks can automatically learn necessary knowledge without requiring expert input. In contrast, BLAG requires sophisticated aggregation and IP expansion techniques designed by domain experts.

### Experimental Setup

We implemented two variants of our approach:
- **UNSUP**: An unsupervised method using a clustering-based approach (Section VI-B) when no labeled data is available.
- **SEMISUP**: A semi-supervised model that combines labeled and unlabeled data, suitable for limited data scenarios.

We use a fixed labeled data size of 100 scan reports corresponding to 100 distinct entities. The encoder is implemented as a three-layer linear network with ReLU activation. We use mean squared error for measuring reconstruction (task 1) and consistency loss (task 3). For task 2, we use cross-entropy loss. The RMSprop optimizer is employed, and a grid search is used to find the optimal mask corruption probability \( p = 0.05 \).

### Baselines

We evaluate our approach against four diverse and representative baselines:
1. **BL-OPTTHRESH**: A threshold-based approach where an entity is considered malicious if more than \( K \) scanners report it as such. \( K \) is chosen optimally to provide the best F1-score.
2. **BL-SUP**: A supervised approach that trains a deep neural network classifier using 10% of the labeled scan reports.
3. **BL-GM**: An EM-based unsupervised approach adapted for malware files by identifying appropriate values for latent variables.
4. **BL-WS**: A weak supervision approach where each scanner is modeled as a noisy classifier, and a noise-aware discriminative classifier is trained using the same labeled data as BL-SUP.

Due to the unbalanced nature of the datasets, we use the F-score to measure performance, balancing precision and recall.

### Grouping Scan Reports

We focus on fresh files and URLs where timely detection is critical. We define "age" as the duration between the current time and the first sighting in an aggregate service like VirusTotal. Entities are grouped based on their age deciles, with each decile representing 10% of the evaluation dataset. An equi-depth histogram is constructed by sorting entities based on their age, with deciles corresponding to the 10th, 20th, ..., quantiles. Early deciles, corresponding to fresh entities, are often the most challenging.

### Early and Accurate Malicious Entity Detection

First, we evaluate SIRAJ's performance in two key dimensions:
1. **Accuracy**: Can SIRAJ provide accurate predictions?
2. **Timeliness**: Can SIRAJ detect if an entity is malicious or benign early?

Figure 3 shows the ROC curve for SIRAJ and the baselines when evaluated on a per-entity basis. Both SEMISUP and UNSUP outperform a wide variety of competing baselines, including BL-SUP, which had up to 10% of labeled scan reports (see Table I).

Our analysis finds that SIRAJ consistently performs well across all age deciles. In contrast, other baselines perform poorly for highly fresh entities and only catch up with SIRAJ for older entities. Additionally, UNSUP outperforms BL-WS, which in turn outperforms BL-GM. This indicates that the careful design of pretext tasks plays a crucial role in SIRAJ's superior performance while reducing reliance on labeled data. The superior performance of BL-WS over BL-GM also suggests that learning both the structure and parameters of the generative model is often more effective than fixing the generative model based on domain knowledge.

### Comparing SIRAJ against Domain-Aware Baselines

Both SIRAJ and the baselines are domain-agnostic and do not make domain-specific assumptions. We also compare SIRAJ against a well-known domain-aware and unsupervised baseline, BLAG. Figure 3(d) shows that UNSUP and SEMISUP have a high degree of agreement with BLAG. BLAG uses sophisticated techniques such as aggregation and expansion, and our pretext tasks, even when not specifically designed for the IP domain, incorporate some of these aspects. Specifically, the generative model and pretext task 1 learn scanner dependencies and perform sophisticated aggregation, while pretext tasks 2 and 3 produce temporally consistent embeddings that are also IP prefix aware.

### Early Detection of Malicious Entities

Next, we investigate the performance of the algorithms for early detection. Given a set of entities \( E \) and a time duration \( \delta \), we obtain a time series of scan reports for each \( e \in E \) at intervals \( T, T + \delta, T + 2\delta, \ldots \), where \( T \) is the submission time for entity \( e \). We measure the F-score for the scan reports at every time snapshot. An algorithm should identify a malicious entity as early as possible. Figure 4 shows that our proposed approaches, UNSUP and SEMISUP, provide consistently high performance as early as 6 hours for phishing URLs and 24 hours for other types of entities. In contrast, competing baselines provide poor results for early scan reports but eventually catch up as \( \delta \) increases.

### Ablation Analysis

We conduct a series of ablation analyses to understand the contribution of significant components and tasks in our approach. While we present the results for SEMISUP in the limited data scenario, the results for UNSUP are similar.

#### Ablation of Components

SIRAJ is based on three components: a generative model, pre-training using self-supervised learning, and fine-tuning. We compare three variants:
- **SEMISUP**: Includes all components.
- **NOGM**: Excludes the generative model.
- **NOSSL**: Excludes all three pretext tasks.

Figure 6 shows the results of this analysis. As expected, SEMISUP provides the best results, demonstrating the importance of all components in achieving high performance.