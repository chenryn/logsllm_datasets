submission timestamps and the corresponding scan reports.
This unlabeled data is used to train the generative and self-
supervised models for both phishing and malware URLs.
Fine-Tuning. For phishing URLs, we use PhishTank (PT),
a collaborative website for verifying Phishing URLs. Users
submit suspected Phishing URLs to PT that are then veriﬁed
and voted upon by other users. We implement a crawler to
collect the newly submitted URLs to PT. We ﬁlter out invalid
URLs (e.g., malformed URLs) and then immediately submit
them to VirusTotal, and collect the scan reports. We categorize
a URL as fresh if the ﬁrst scanned time in VirusTotal is
the same as our submitted time. Almost 54% of the URLs
submitted to PhishTank are categorized as fresh. For these
fresh URLs, we rescan them in VirusTotal every hour and
obtain a time series of scan reports. Simultaneously, we keep
track of their status in PhishTank. If the URL is veriﬁed in
PhishTank, we label the URL as Phishing. For malware URLs,
we use URLhaus that operates as a database of malware URLs
submitted by users. We collect VirusTotal scan reports for
newly submitted URLhaus URLs similar to PhishTank. We
observe that 60% of the URLs submitted to URLhaus are
categorized as fresh.
Since PhishTank and URLhaus are scanners in VirusTotal,
we exclude the responses from these scanners from the scan
report to avoid biases. This dataset is used to obtain the labels
for supervised and semi-supervised approaches.
Final Evaluation. We collect 3000 reports, corresponding to
3000 distinct URLs, over 7 days from the daily VirusTotal
URL feed using an online stratiﬁed sampling based approach
proposed in [34], [35]. The goal is to select a small set of
scan reports to be labeled by human experts. These URLs
(and domains) are distinct from the ones from the training
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:03:36 UTC from IEEE Xplore.  Restrictions apply. 
7513
TABLE I: Datasets Utilized in Experiments
Dataset for PreTraining
#Benign
Scan Reports
(Training Set)
Domain
#Benign
Entities
#Malicious
Entities
Phishing URLs
Malicious URLs
Malware Files [8], [31]
IP Blacklists [19], [32]
890K
7226
0.97M
83K
29K
7197
2.6M
1.9M
2.8M
13.6M
#Malicious
Scan Reports
(Training Set)
381K
263K
2.8M
36.4M
#Benign
Entities
2141
236
389K
#Malicious
Entities
536
239
120
131K
Evaluation Dataset
#Benign
Scan Reports
(Testing Set)
2141
18.3K
8.8M
#Malicious
Scan Reports
(Testing Set)
536
239
12K
3.2M
Fig. 3: Comparing the performance of SIRAJ against diverse baselines for maliciousness detection task.
data. The scan reports used for training and evaluation did not
overlap temporally.
The ground truth for these scan reports is obtained through
manual
inspection by cyber security experts. They follow
the same rubrics and look for signs of malicious URLs,
including but not limited to: distribution of malware binaries,
association with known indicators of compromises, domain
squatting (e.g., using a popular brand name within a domain
name), the presence in social engineering attacks (e.g. phishing
emails), mimicking the look and feel of benign websites (e.g,
login pages of banks or online stores), owner of the website,
historical registration records of the website, TLS certiﬁcates
associated with the website (if available), historical domain
access patterns available through passive DNS services and
the infrastructure in which the domain is hosted.
IP Blacklists. We use an unlabeled dataset collected by the
authors of [19] for our experiments. Overall, they monitor 157
publicly available blacklists and cover a wide variety of attack
vectors such as spam, malware, DDoS attacks, ransomware,
etc. The blacklists have a variety of update frequencies ranging
from 15 minutes to 7 days. We conducted our experiments over
the daily snapshot of the blacklists for the years 2019-2020.
Speciﬁcally, we use a form of temporal cross validation where
we use six months of daily snapshots for pre-training, one
month for ﬁne-tuning and the following three months of data
for evaluation. We repeat this process for every contiguous 6
month duration. For example, we ﬁrst pre-train on blacklists
from Jan-Jun 2019, then from Feb-Jul 2019 and so on.
We treat the output of BLAG [19] as a proxy for the ground
truth BLAG achieves the state of the art results with speciﬁcity
of as high as 99% and can also report malicious sources as
much as 9.4 days ahead of the best blacklist. Our rationale for
comparing SIRAJ against BLAG is to demonstrate that our
domain agnostic pretext tasks are able to automatically learn
the necessary knowledge and does not require expert input.
In contrast, BLAG requires sophisticated aggregation and IP
expansion based techniques designed by domain experts that
are speciﬁc to IP domain.
B. Experimental Setup
We implement
two variants of our approach. When no
labeled data is available, we use the unsupervised method
(UNSUP) – the clustering based approach proposed in Sec-
tion VI-B. The SEMISUP variant handles the limited data
scenario by building a semi-supervised model that combines
both the labeled data and unlabeled data. We use a ﬁxed
labeled data size of 100 scan reports corresponding to 100
distinct entities. We implement the encoder as a three linear
layer network with relu as the activation function. We use the
mean squared error function for measuring reconstruction (task
1) and consistency loss (task 3). For task 2, we use the cross
entropy loss function. Finally, we use the rmsprop optimizer.
We use a grid search to ﬁnd the optimal value of p = 0.05 for
the hyper-parameter controlling mask corruption probability.
Baselines. We evaluate our approach against four diverse
and representative baselines. Similar to SIRAJ, each of these
baselines is data-driven and domain agnostic. They do not rely
on domain speciﬁc features and perform the prediction based
on scan reports. (i) BL-OPTTHRESH is a strengthened version
of the widely used threshold-based approach that determines
an entity as malicious if more than K scanners report it
as malicious. Instead of assuming an ad-hoc threshold such
as 1 or 3, K is chosen in an optimal manner to provide
the best results for the baseline. We assume the availability
of an oracle that can give us the F1-score for all possible
values of K. Then, we choose K that provides the highest
F1-score. (ii) BL-SUP is a supervised approach that trains a
deep neural network classiﬁer with three layers using 10%
of the labeled scan reports. (iii) BL-GM is an EM-based
unsupervised approach proposed in [10] for malware ﬁles.
We adapt this approach for other domains by identifying the
appropriate values for the latent variables of the generative
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:03:36 UTC from IEEE Xplore.  Restrictions apply. 
8514
0.00.20.40.60.81.0FalsePositiveRate0.00.20.40.60.81.0TruePositiveRatePhishingURL0.00.20.40.60.81.0FalsePositiveRate0.00.20.40.60.81.0TruePositiveRateMalwareURL0.00.20.40.60.81.0FalsePositiveRate0.00.20.40.60.81.0TruePositiveRateMalwareFile0.00.20.40.60.81.0FalsePositiveRate0.00.20.40.60.81.0TruePositiveRateIPBlacklistFig. 4: Comparing the performance of our approach against diverse baselines for the early detection task.
Fig. 5: Impact of Training Data Size on SIRAJ and its baselines. Legend same as that of Figure 4.
model through an exhaustive grid search that provides the best
results. Finally, (iv) BL-WS is based on the paradigm of weak
supervision [24]. Each scanner is modeled as a noisy classiﬁer
through a labeling function. Then, we build a generative
model based on the commonalities and contradictions between
the outputs of the scanners. We then train a noise-aware
discriminative classiﬁer using the same labeled data as that
of BL-SUP. For additional details, please refer to [24].
Due to unbalanced nature of the datasets, we use F-score to
measure the performance that balances precision and recall. It
is deﬁned as 2×precision×recall
Grouping Scan Reports. We focus on fresh ﬁles and URLs
where timely detection is important. We use the term ‘age’
to denote the duration between the current time and when
it is ﬁrst seen in an aggregate service such as VirusTotal.
We group the entities based on their age deciles where each
decile consists of an exact 10% of the evaluation dataset. We
construct an equi-depth histogram [36] by sorting the entities
based on their age and the deciles correspond to the 10-th, 20-
th, . . ., quantiles respectively. The early deciles corresponding
to ‘fresh’ entities are often the most challenging.
C. Early and Accurate Malicious Entity Detection
precision + recall
.
First, we evaluate the performance of SIRAJ over two key
dimensions: (a) can SIRAJ provide accurate predictions? and
(b) can SIRAJ detect if an entity is malicious or benign early?
Figure 3 shows the ROC curve for SIRAJ and the baselines
when evaluated on a per-entity basis. In other words, if an
entity (such as an URL) has multiple temporally distinct scan
reports, we only use the earliest scan report. We then use
SIRAJ and the baselines to predict whether the entity is
benign or malicious. This provides a realistic measure on
algorithm’s performance for unseen entities. Both SEMISUP
and UNSUP achieve the best results outperforming a wide
variety of competing baselines including BL-SUP that had as
much as 10% of labeled scan reports (see Table I).
Our analysis ﬁnds that SIRAJ provides consistently high
performance across the age deciles. In contrast,
the other
baselines perform poorly for highly fresh entities (those with
early deciles), and only catch up with SIRAJ for older entities.
We also ﬁnd that UNSUP outperforms BL-WS which in turn
outperforms BL-GM. UNSUP uses a two-step process that
involves learning scanner dependencies and augmenting them
with pretext tasks. In contrast, BL-WS uses a ﬁxed generative
model based on scanners’ overlap and conﬂicts followed by
a supervised noise-aware discriminative model using labeled
data. This shows that a careful design of pretext tasks plays
a major role in the outperformance of SIRAJ while also
reducing the reliance on the labeled data. Finally, the superior
performance of BL-WS over BL-GM shows that it is often
desirable to learn both the structure and parameters of the
generative model instead of ﬁxing the generative model (based
on domain knowledge from experts) and just learning the
parameters. In fact, this design choice is what allowed SIRAJ
to transfer between multiple domains seamlessly.
Comparing SIRAJ against Domain Aware Baselines.
Both SIRAJ and the baselines are domain agnostic and do
not make any domain speciﬁc assumptions. We also compare
against a well-known domain-aware and unsupervised base-
line. Figure 3(d) shows the performance of SIRAJ against
BLAG. Recall that we used BLAG [19] as a proxy for ground
truth which means that it achieves perfect performance. We
can see that UNSUP and SEMISUP have a high degree of
agreement with BLAG. BLAG uses sophisticated techniques
such as aggregation and expansion (transforming IP addresses
into IP preﬁxes). We ﬁnd that our pretext tasks – even when not
speciﬁcally designed for IP domain – are able to incorporate
some of these aspects. Speciﬁcally, the generative model and
pretext task 1 learn the scanner dependencies and use them
to perform sophisticated aggregation. Our pretext tasks 2 and
3 produce temporally consistent embeddings that are also
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:03:36 UTC from IEEE Xplore.  Restrictions apply. 
9515
6h12h24h48h120hScanReportAge406080100F1-ScorePhishingURL24h48h72h96h120hScanReportAge406080100F1-ScoreMalwareURL1d2d3d5d7dScanReportAge406080100F1-ScoreMalwareFile1d2d3d5d7dScanReportAge406080100F1-ScoreIPBlacklist2004006008001000TrainingDataSize60708090100F1-ScorePhishingURL2004006008001000TrainingDataSize60708090100F1-ScoreMalwareURL2004006008001000TrainingDataSize60708090100F1-ScoreMalwareFile2004006008001000TrainingDataSize60708090100F1-ScoreIPBlacklistIP preﬁx aware. We ﬁnd that our encoder produces similar
embeddings for scan reports of two IPs from the same block
as compared to scan reports of two random IPs.
Early Detection of Malicious Entities. We next investigate
the performance of the various algorithms for early detection.
Given a set of entities E and a time duration δ, we obtain
a time series of scan reports for each e ∈ E for time periods
T, T +δ, T +2δ, . . . where T is the submitted time for entity e.
For example, if δ is one day, we get daily scan reports for all
entities. We then measure the F-score for the scan reports at
every time snapshot. If an entity e is malicious, an algorithm
should identify it as early as possible. Intuitively, this task is
much more challenging as the smaller δ is, the fewer scanners
would label the entity as malicious and its report also tends
to change signiﬁcantly in the future.
In contrast, as time
progresses past the label stabilization period [8] even a simple
threshold-based approach could give good results.
Figure 4 shows that our proposed approaches UNSUP and
SEMISUP provide consistently high performance as early as
6 hours for Phishing URLs and 24 hours for other types of
entities. In contrast, the competing baselines provide poor
results for the early scan reports, some of which eventually
catch up when δ increases. Two key aspects of this trend
are notable. The second pretext learns the temporal scanner
dependencies and is responsible for the good performance as
early as 6 hours for Phishing URLs. The third pretext task
based on temporal consistency keeps our approach’s perfor-
mance consistent across time. This is achieved by ensuring
that embedding of the scan reports of an entity e for two
e are closer to each other.
consecutive timestamps RT1
e and RT2
D. Ablation Analysis
compare
and ﬁne-tuning. We
Next, we conduct a series of ablation analyses to understand
the contribution of the signiﬁcant components and tasks in our
approach. While we present the results of SEMISUP for the
limited data scenario, the results for UNSUP are similar.
Ablation of Components. SIRAJ is based on three com-
ponents: generative model, pre-training using self-supervised
learning,
three variants:
SEMISUP with all the components, NOGM that removes the
generative model, and NOSSL that removes all
the three
pretext tasks. Figure 6 shows the results of this analysis. Not
surprisingly, SEMISUP provides the best results showing that