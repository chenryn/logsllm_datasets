title:"Hello, It's Me": Deep Learning-based Speech Synthesis Attacks in
the Real World
author:Emily Wenger and
Max Bronckers and
Christian Cianfarani and
Jenna Cryan and
Angela Sha and
Haitao Zheng and
Ben Y. Zhao
“Hello, It’s Me”: Deep Learning-based Speech Synthesis A(cid:29)acks
in the Real World
Emily Wenger∗
PI:EMAIL
University of Chicago
Max Bronckers
Christian Cianfarani
Jenna Cryan
PI:EMAIL
PI:EMAIL
PI:EMAIL
University of Chicago
University of Chicago
University of Chicago
Angela Sha
PI:EMAIL
University of Chicago
Haitao Zheng
PI:EMAIL
University of Chicago
Ben Y. Zhao
PI:EMAIL
University of Chicago
ABSTRACT
Advances in deep learning have introduced a new wave of voice
synthesis tools, capable of producing audio that sounds as if spo-
ken by a target speaker. If successful, such tools in the wrong hands
will enable a range of powerful attacks against both humans and
software systems (aka machines). This paper documents eﬀorts
and ﬁndings from a comprehensive experimental study on the im-
pact of deep-learning based speech synthesis attacks on both hu-
man listeners and machines such as speaker recognition and voice-
signin systems. We ﬁnd that both humans and machines can be reli-
ably fooled by synthetic speech, and that existing defenses against
synthesized speech fall short. These ﬁndings highlight the need
to raise awareness and develop new protections against synthetic
speech for both humans and machines.
CCS CONCEPTS
• Computing methodologies → Machine learning; • Security
and privacy → Biometrics.
KEYWORDS
neural networks; speech synthesis; biometric security
ACM Reference Format:
Emily Wenger, Max Bronckers, Christian Cianfarani, Jenna Cryan, Angela
Sha, Haitao Zheng, and Ben Y. Zhao. 2021. “Hello, It’s Me”: Deep Learning-
based Speech Synthesis Attacks in the Real World. In Proceedings of the 2021
ACM SIGSAC Conference on Computer and Communications Security (CCS
’21), November 15–19, 2021, Virtual Event, Republic of Korea. ACM, New York,
NY, USA, 17 pages. https://doi .org/10 .1145/3460120 .3484742
1 INTRODUCTION
Our voice conveys so much more than the words we speak. It is
a fundamental part of our identity, often described as our “audi-
tory face” [22]. Hearing our voice is often enough for a listener
to make inferences about us, such as gender appearance [63], size
∗Corresponding author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea.
© 2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-8454-4/21/11. . . $15.00
https://doi .org/10 .1145/3460120.3484742
or strength [73], approximate age [99], and even socioeconomic
status [47].
But perhaps the human voice is no longer as unique as we would
like to believe. Recent advances in deep learning have led to a wide
range of tools that produce synthetic speech spoken in a voice of a
target speaker, either as text-to-speech (TTS) tools that transform
arbitrary text into spoken words [21, 36, 37, 41, 64, 76, 83, 92], or as
voice conversion tools that reshape existing voice samples into the
same content spoken by the target [42, 67, 69, 74, 95]. In addition to
proprietary systems like Google Duplex, many others are available
as open source software or commercial web services [9, 12].
Given the strong ties between our voices and our identities, a
tool that successfully spoofs or mimics our voices can do severe
damage in a variety of settings. First, it could bypass voice-based
authentication systems (also called automatic speaker veriﬁcation
systems) already deployed in automated customer service phone-
lines for banks and credit card companies (e.g., JP Morgan Chase
and HSBC [3, 4]), as well as user login services for mobile messag-
ing apps like WeChat [1]. It would also defeat user-based access
controls in IoT devices such as digital home assistants (e.g., Ama-
zon Alexa, Google Home) [7]. Finally, such tools could directly at-
tack end users, by augmenting traditional phishing scams with a fa-
miliar human voice. This apparently was the case in a recent scam,
where attackers used the mimicked voice of a corporate CEO to
order a subordinate to issue an illegitimate money transfer [82].
These speech synthesis attacks, particularly those enabled by
advances in deep learning, pose a serious threat to both computer
systems and human beings. Yet, there has been – until now – no
deﬁnitive eﬀort to measure the severity of this threat in the context
of deep learning systems. Prior work has established the viability
of speech synthesis attacks against prior generations of synthesis
tools and speaker recognition systems [28, 45, 56, 57]. Similarly,
prior work assessing human vulnerability to speech synthesis at-
tacks evaluates now-outdated systems in limited settings [57, 60].
We believe there is an urgent need to measure and understand
how deep-learning based speech synthesis attacks impact two dis-
tinct entities: machines (e.g., automated software systems) and hu-
mans. Can such attacks overcome currently deployed speaker recog-
nition systems in security-critical settings? Or can they compro-
mise mobile systems such as voice-signin on mobile apps? Against
human targets, can synthesized speech samples mimicking a par-
ticular human voice successfully convince us of their authenticity?
In this paper, we describe results of an in-depth analysis of the
threat posed to both machines and humans by deep-learning speech
Session 1D: Authentication and Click Fraud CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea235Hello, my
name is Bob.
Record
Attacker’s 
desired content
Attacker
Scrape
Victim’s
stolen
speech
Speech 
Synthesis
System
Fake
speech
Victim
Victim
This is Bob.
This is Bob.
Send me Alice’s
Send me Alice’s
HR Record!
HR Record!
Fake speech
Right away,
boss!
Fake speech
This is Bob.
Log me in!
Bob’s
Bank
Access granted!
a
Attacker steals victim’s speech
via recording or social media.
Attacker generates fake 
b
speech imitating the victim.
Using fake speech, attacker deceives
c
humans or speaker verification systems.
Figure 1: Workﬂow of synthesis-based voice spooﬁng attacks: (a) the attacker obtains voice samples from the victim, either by
secretly recording them or by downloading available media; (b) the attacker then uses a speech synthesis system to generate
fake speech, which imitates the victim’s voice but contains arbitrary, attacker-chosen content; (c) the attacker uses this fake
speech to impersonate the victim, e.g., attempting to access personal or ﬁnancial information or conduct other attacks.
synthesis attacks. We begin by assessing the susceptibility of mod-
ern speaker veriﬁcation systems (including commercial systems
Microsoft Azure, WeChat, and Alexa) and evaluate a variety of
factors aﬀecting attack success. To assess human vulnerability to
synthetic speech, we perform multiple user studies in both a sur-
vey setting and a trusted context. Finally, we assess the viability
of existing defenses in defending against speech synthesis attacks.
All of our experiments use publicly available deep-learning speech
synthesis systems, and our results highlight the need for new de-
fenses against deep learning-based speech synthesis attacks, for
both humans and machines.
Key Findings. Our study produces several key ﬁndings:
• Using a set of comprehensive experiments over 90 diﬀerent speak-
ers, we evaluate and show that DNN-based speech synthesis
tools are highly eﬀective at misleading modern speaker recog-
nition systems (50 − 100% success).
• Our experiments ﬁnd that given a handful of attempts, synthe-
sized speech can mimic 60% of speakers in real world speaker
recognition systems: Microsoft Azure, WeChat, and Amazon Alexa.
• A user survey of 200 participants shows humans can distinguish
synthetic speech from the real speaker with ∼50% accuracy for
unfamiliar voices but near 80% for familiar voices.
• An interview-based deception study of 14 participants shows
that, in a more trusted setting, inserted synthetic speech suc-
cessfully deceives the large majority of participants.
• Detailed evaluation of 2 state-of-the-art defenses shows that they
fall short in their goals of either preventing speech synthesis or
reliably detecting it, highlighting the need for new defenses.
It is important to note that speech synthesis is intrinsically about
producing audible speech that sounds like the target speaker to
humans and machines alike. This is fundamentally diﬀerent from
adversarial attacks that perturb speech to cause misclassiﬁcation
in speaker recognition systems [25, 48, 53]. Such attacks do not
aﬀect human listeners, and could be addressed by developing new
defenses against adversarial examples.
2 BACKGROUND
In this section, we ﬁrst describe current trends in speaker recog-
nition technology and voice synthesis systems, followed by voice-
based spoof attacks. Finally, we brieﬂy summarize defenses pro-
posed to combat synthetic speech.
2.1 Voice-Based User Identiﬁcation
How Humans Identify Speakers via Voice. The unique char-
acteristics of each person’s vocal tract create their distinct voice.
Humans use these vocal characteristics to identify people by voice [55].
Though human speaker identiﬁcation is imperfect, it is highly ac-
curate and has inspired the construction of speaker recognition
systems for security purposes [75].
Automated User Veriﬁcation by Machines. Recently, speaker
recognition has become a popular alternative to other biometric
authentication methods [71]. Speaker recognition systems capture
characteristics of a speaker’s voice and compare them to enrolled
speaker proﬁles. If there is a match, the recognition system grants
the speaker access. Early speaker recognition systems (1970s-2010s)
used parametric methods like Gaussian Mixture Models, while more
recent systems (2014 onward) use deep learning models, which re-
duce overhead and improve accuracy [31, 70, 81, 85].
Speaker recognition is used in numerous settings, from bank
customer identiﬁcation to mobile app login and beyond [1, 3, 4].
Recently, virtual assistants like Alexa and Google Assistant have
begun to use speaker recognition to customize system behavior [5,
7]. Speaker recognition systems are either text-dependent or text-
independent [23, 35]. Text-dependent systems use the same, speaker-
speciﬁc authentication phrase for both enrollment and login. Text-
independent systems are content-agnostic.
2.2 Speech Synthesis Systems
Synthetic speech is produced by a non-human source (i.e. a com-
puter) and imitates the sound of a human voice. Eﬀorts to pro-
duce electronic synthetic speech go back to 1930s, where Homer
Dudley developed the ﬁrst vocoder [58]. Since then, systems like
Festvox [20] have used Gaussian Mixture Models (GMM) to im-
prove the quality – but not the speed – of speech synthesis. The
recent deep learning revolution has catalyzed growth in this ﬁeld.
DNN-Based Speech Synthesis. Numerous deep neural network
(DNN) based speech synthesis systems have been proposed [21, 36,
37, 41, 42, 64, 66, 67, 69, 74, 76, 83, 92, 95]. They can be divided into
two categories: text-to-speech (TTS) and voice conversion (VC).
TTS systems transform arbitrary text into words spoken in the
voice of a target speaker [21, 36, 37, 41, 64, 76, 83, 92]. In contrast,
VC systems take two voice samples – an attacker and target – and
output a speech sample in which content from the attacker is spo-
ken in the voice of the target [42, 66, 67, 69, 74, 95]. Both TTS and
Session 1D: Authentication and Click Fraud CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea236Arbitrary
Content
Measurement Studies
vs. Machines
vs. Humans
No
[18, 39, 44, 96]
Limited
[33, 34, 51, 78, 86]
[28, 45, 56, 57, 79]
-
[34]
[57]
Attack Type
Description
Replay
Human
Play pre-recorded
speech from victim.
Human actor
Impersonation
imitating victim.
Synthesis
(Classical)
Synthesis
(DNN)
Clone victim’s speech
(GMM-based)
Clone victim’s speech
(DNN-based)
Yes
Yes
[62], this work
this work
Table 1: Taxonomy of spooﬁng attacks against voice-based
authentication and measurement studies on these attacks.
VC produce the same output: a synthetic version of the target’s
voice, speaking words chosen by the attacker.
Eﬃcacy and Availability. Many DNN-based speech synthesis
systems report impressive speech “realism” metrics, indicating sig-
niﬁcant improvement over classical systems. Supporting evidence
of DNN synthesis performance comes from real-world anecdotes.
DNN-based synthetic speech has been successfully used at least
one in highly proﬁtable attack [82]. Google’s new scheduling as-
sistant voice is so realistic that Google was instructed to announce
when it was being used on phone calls [87].
Some DNN synthesis systems (and their training datasets) re-