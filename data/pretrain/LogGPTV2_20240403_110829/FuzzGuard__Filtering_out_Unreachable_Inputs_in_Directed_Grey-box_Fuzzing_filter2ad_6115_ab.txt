diction (MP) and model updating (MU). It works together
with a DGF (referred to as “the carrier fuzzer”). As shown in
Figure 1, in the MI phrase, the carrier fuzzer generates a great
number of inputs and observes any exceptions. FuzzGuard
records whether the program can execute the target buggy
code for each input. Then FuzzGuard trains a model using the
inputs and their reachability. In the MP phrase, FuzzGuard uti-
lizes the model to predict the reachability of a newly generated
input. If the input is reachable, it is fed into the program for
real execution. In this process, FuzzGuard observes whether
the input can really reach the target code. In the MU phrase,
FuzzGuard further updates the model with incremental learn-
ing to maintain its efﬁciency and increase its performance.
The unreachable inputs will be temporarily saved in a data
pool (referred to as “the pool of unreachable inputs (PUI)”)
for further checking by a more accurate model after model up-
dating. As combining deep learning with fuzzing is not trivial,
we face the new challenges (as mentioned in Section 1).
Figure 2 shows a concrete example of FuzzGuard. Firstly,
the carrier fuzzer generates a number of inputs (referred to
as “data”) and runs the target program with them to get
the reachability (referred to as “label”) in the MI phrase
(the step 1(cid:13) and step 2(cid:13) in the ﬁgure). During this process,
an ideal situation is to train a deep learning model using
balanced data. That is, about half of the inputs could reach
the buggy code while the other cannot. Unfortunately, in real
situation, the carrier fuzzer hardly generates the inputs that
reach the buggy code in the initial phrase of fuzzing. As a
result, the labeled data are usually extremely unbalanced at
this stage. For example, only one input can actually reach
the buggy code after over 22 million inputs generated (#7 in
Table 1). To solve this problem, we design the step-forwarding
approach for MI (step 2(cid:13)), which lets the inputs reach the pre-
dominating nodes (we use “node” to refer to “basic blocks”
Figure 1: An overview of FuzzGuard.
when the execution time of the target program takes part the
most in the whole fuzzing process, the wasted time is even
more. If there exists an approach that is quick enough to
predict the reachability of an input, the fuzzing process does
not need to execute the target program with the unreachable
ones. In this way, the overall performance of fuzzing could
be increased.
Inspired by the recent success of deep learning in pattern
recognition [11, 19, 34, 36], we are wondering whether deep
learning could be applied to identify (un)reachable inputs.
Carefully comparing the processes of pattern recognition and
identiﬁcation of (un)reachable inputs, we found similarities
between them: they both classify data (a certain objects v.s.
(un)reachable inputs) based on either prior knowledge or sta-
tistical information extracted from the patterns (many labeled
images of the object v.s. many labeled inputs from previous
executions). However, they do have essential differences (e.g.,
the distribution of labeled data, requirements on efﬁciency,
etc.) which makes the process of unreachable input identiﬁca-
tion very challenging (see Section 1).
Example. List 1 gives an example. The vulnerable code is at
Line 6 (see Section 7). So the goal of DGF (e.g., AFLGo) is
to generate as many as inputs that could reach there and hope
to trigger the bug. The seed input is chosen from AFLGo’s
seed corpus (e.g., not_kitty.png). It takes 13 hours to gen-
erate 16 million inputs and needs to test the program with
them before the bug is triggered. Among these inputs, only
3.5 thousand (0.02%) can reach the buggy code. One may
think of leveraging symbolic execution to generate constraints
from the execution path to the destination. However, the full
constraints are very hard to generate since several paths could
reach the buggy code. Even if the constraints could be gen-
erated, the calculation of reachability using the constraints is
still very time-consuming, which is even similar to the time
spent on running the target program. Our idea is to generate
a deep learning model to automatically extract features of
reachable inputs and to identify future reachable ones. Based
on our evaluation, nearly 14 million inputs (84.1%) are iden-
tiﬁed which saves 9 hours of unnecessary executions. Also
note that the false positive rate and false negative rate are only
2.2% and 0.3% for this example, respectively.
Scope and Assumption. Different from previous research
on CGF using deep learning [15, 17, 28, 29], our approach
2258    29th USENIX Security Symposium
USENIX Association
Figure 2: An example of ﬁltering unreachable inputs by FuzzGuard.
in the rest of the paper) of the buggy code (i.e., B0, B1 and
B2 in Figure 2) step-by-step to the destination (i.e., B3 in
Figure 2). Particularly, FuzzGuard chooses a pre-dominating
node (e.g., B1) as a middle-stage destination (i.e., referred to
as “mid-target”) and generates a model to ﬁlter out the inputs
that cannot reach B1 (step 3(cid:13)). Usually, compared with B3,
more balanced labeled data could be gained when the program
runs to B1. So the model can be trained earlier and also starts
to work earlier. Then MP judges whether a newly generated
input (in step 4(cid:13)) could reach B1 using the model (step 5(cid:13)).
For reachable inputs (e.g., with label  in step 6(cid:13)),
FuzzGuard runs the program with it and records whether it
can really reach the buggy code (step 7(cid:13)). Such information
is further leveraged to continuously update the model by MU
(step 8(cid:13)). The unreachable inputs are put into PUI (step 9(cid:13)).
After more inputs are tested, a closer pre-dominating node
(to the buggy code) having the balanced labeled data will
appear (e.g., B2 in this case). Such a process will continue
until the buggy code is arrived at, and ﬁnally triggered. Below
we provide the details of the three modules.
4.2 Model Initialization
As mentioned previously, one main challenge of applying
deep learning on fuzzing is the unbalanced data for training.
Usually, the number of reachable inputs is far less than that of
unreachable ones. In order to tackle this challenge, we present
a step-forwarding approach. The basic idea is based on the
observation: the pre-dominating nodes of the buggy code are
earlier to be reached, which should gain balanced data earlier.
Note that the pre-dominating nodes of the buggy code are the
nodes that dominate the buggy code: every execution path
towards the buggy code will pass through the pre-dominating
nodes [5]. So the reachability of the marked pre-dominating
nodes is guaranteed. Therefore, we could train a model to
ﬁlter out those inputs that cannot reach the pre-dominating
node (neither can they reach the buggy code). In this way,
we gradually get balanced data of the pre-dominating nodes,
toward the buggy code in the end. For example, as to the
control ﬂow graph shown in Figure 2, the nodes represent
basic blocks in the program in List 1. B0 is the entry point and
the buggy code is in B3. B1 and B2 are the pre-dominating
nodes of B3. At the beginning of fuzzing, no input reaches
B3, while half of the inputs could reach B1. Now B1 is the
closest balanced pre-dominating node to the buggy code. So
we view B1 as the target, and train the model using these
inputs. In this way, the unreachable inputs to B1 are ﬁltered
out, saving the time spent on executing the target program
with them. When the fuzzing process goes further, B2 or B3
will get balanced data for training. Note that, different from
CGF whose goal is to achieve high coverage, DGF aims to
generate inputs to trigger a given (potential) bug at a certain
place. So it does not care about whether new bugs are found in
other paths. Interestingly, we did see that FuzzGuard+AFLGo
still discovers undisclosed bugs (see Section 6) which are
located deeply in a program (also near the target buggy code).
An ordinary CGF is hard to trigger them in a limited time.
However, it takes too long to train a single model for each
pre-dominating node. This is mainly because the model needs
to be retrained when FuzzGuard steps forward to the next
pre-dominating node. Our idea is to only train one model for
all the pre-dominating nodes including the buggy code itself.
To achieve this goal, formally, we label the reachability of the
nodes (i.e., B ={B1,B2, ...,Bn}) in the vector y. For each label,
it is represented as a unit vector ˆy, i.e., ˆy =,
where m is the number of the pre-dominating nodes of the
buggy code, yi represents whether the i-th node is the last one
could be reached by the program fed with x, yi ∈ {0,1},i ∈
{1,2, . . . ,m}. As shown in Figure 2, for the input a, the label
is represented as ya =, which means that B0 is
reached but others are not. Similarly, yb = means
that the input b can let the execution reach B0 and B1, but
neither B2 nor B3. yd = means that the buggy
code is ﬁnally reached. For simplicity, we directly map each
byte of the input to an element in the feature vector. This
approach makes FuzzGuard handle different programs with
various formats of inputs in a uniﬁed way. For each data, it
can be represented as a vector x =, where
n is the max length of the input. And xi = bytei + 1 (xi ∈
USENIX Association
29th USENIX Security Symposium    2259
{0,1, . . . ,256}), where xi = 0 means that i-th byte of the input
does not exist (i.e., the length of the input is less than n).
After designing the representation of data and label, we
carefully choose a deep learning model. Such a model should
be good at extracting features from inputs and making the cor-
rect classiﬁcation. Recall the problem of image recognition:
features of an object in an image are expressed by combina-
tions of several pixels (i.e., elements in the input vector, as
shown in Figure 2), which could be well extracted by the CNN
models [11, 19, 34, 36]. Similarly, the features of inputs that
impact their reachability could be expressed by combinations
of several bytes in program inputs. Actually, the constraints in
if-statements in target programs use these bytes for deciding
execution directions. Thus, our idea is to make use of CNN
to accomplish the classiﬁcation task. On one hand, compared
with RNN which is more suitable for training with the byte
sequence, CNN is good at dealing with long data. The longer
the inputs, the faster the RNN model forgets the former fea-
tures. On the other hand, the time for training a CNN model
is much less than the time for training an RNN model, which
is suitable for our problem (the time spent on training and
prediction should be less than the time on real execution).
Thus, we choose to use a 3-layer CNN model (detailed
implementation is shown in Section 5). In this way, the ﬁrst
layer could learn the relationship between each byte, and
the other two layers could learn high dimensional features
(e.g., combining several bytes to form a ﬁeld in an input,
and combining several ﬁelds to impact program execution).
Interestingly, we ﬁnd that such extracted high dimensional
features are correlated with the constraints in the if-statements
in target programs (see Section 7). We also discuss other
machine learning models in Section 8. Note that, the model
needs to be trained for each program from scratch due to
different implementations (which parse inputs in different
ways). It is also an interesting topic to explore the similarity
between different programs and leverage such similarity to
increase the efﬁciency of training.
In this way, we can let the carrier fuzzer run for a while
to collect an initial training data set. After the initial training
data set reaches balanced, the model can learn the reachability
to all nodes of the inputs. The goal of the model is to learn a
target function f (i.e., y = f (x)), which consists of a number
of convolution operations. The convolution operation uses a
number of ﬁlters to extract the features from the data:
w j · x j,i ∈ {1,2, . . . ,n− k}
yi = wT xi = ∑
i−k< j<i+k
where k is the width of the convolution kernel of the ﬁlter
w. Gradient descent algorithm will update weights of each
ﬁlter w to decrease the loss value to achieve a more accurate
prediction. For classiﬁcation tasks, compared to Cross En-
tropy [18] loss, the Mean Square Error (MSE) [23] loss could
balance the error rate for each category, avoiding a particu-
larly high error rate for a single category. Considering the
step-forwarding approach needs the trained model to predict
the reachability of each pre-dominating node as accurate as
possible, we choose to use MSE. So when the value of the
loss = 1
i )2 is close to 0, we believe that the tar-
get function in the classiﬁcation model has been converged
and the model is ready to predict the newly generated inputs.
i=1(yi − yp
m ∑m
4.3 Prediction
After the model is initialized, FuzzGuard utilizes the model to
predict the label of each input and ﬁlters out those unreachable
ones. For the reachable ones, they will be executed by the
target program and further be collected as new labeled data
for model updating. In particular, for an input x, we assume
that the model can only predict the pre-dominating nodes
before Bt (i.e., the mid-target), and the prediction result is yp.
The following function f (cid:48) is used to check whether the input
x is reachable to the target node.
(cid:40)
f (cid:48)(yp,t) =
reachable
unreachable
i = 1∧ i ≥ t
yp
i = 1∧ i < t
yp
However, in real situation, we ﬁnd the prediction results
are not accurate enough, even after many labeled data are
produced. The main reason is that even if the newly generated
inputs could reach the target, they may look quite different
from the reachable ones in the training set. This is understand-
able: these inputs could be generated from different seeds.
Most of the inputs mutated from the same seed are slightly
different with each other, while many differences could be
found between the inputs mutated from different seeds. Thus,
using the inputs totally from previous executions may not be
able to train a very accurate model to predict the reachability
of newly generated inputs. For example, a model trained with
the data in set S1 mutated from the seed s1 may fail to predict
the labels of the data in S2 mutated from the seed s2.
To solve this problem, we propose a representative data
selection approach, which selects a number of representative
inputs from each round of mutation for executing and training.
We consider a ﬁxed number of inputs (e.g., 5%) that randomly
sampled from a round of mutation as the representative data
for this mutation. In this way, within a limited time, inputs
generated from more seeds can be utilized for training, which
increases the model’s accuracy. However, in real execution,
even 5% of the inputs constitute a big number (e.g., over
20 thousand), and execution using these inputs cost lots of
time. Our idea is to sample even fewer inputs. Suppose in two
different mutations, two sets of inputs S1 and S2 are generated
from the two seeds s1 and s2, respectively. If the distribution
of S1 is similar to that of S2, we can select even fewer inputs.
However, we cannot directly assume that the distributions of
the two sets are similar only through the similarity of the two
seeds. This is mainly because different strategies of mutation
(e.g., bit and byte ﬂips, simple arithmetics, stacked tweaks
2260    29th USENIX Security Symposium
USENIX Association
and splicing) could greatly change the seeds and make the
descendants look quite different. So our idea is to compare the
seeds together with the corresponding strategies of mutations.
If the two seeds are similar and the strategies are identical,
we consider to select fewer inputs from the combined set. We
deﬁne the seed similarity degree (SSD) between the two seeds
s1 and s2 as follows:
ds1,s2 = 1−∑8n
i=1 s1
i ⊕ s2
i /8n
where n is the max byte length of the inputs, and si means the
i-th bit of the seed s. Note that different choices of embedding
do not affect the deﬁnition of SSD, since SSD is deﬁned
using the seeds, not the vectors after embedding. In this way,
we could measure the similarity between two sets of inputs
through their predecessor seeds. When SSD is over a threshold
(θs), we consider that the seed s2 is similar to the seed s1,
and less data from the inputs mutated from s2 should be
selected. For example, in Figure 2, we select less data (e.g.,
2%) from the inputs set that generated by seed the e, because e
is similar to the seed b (e.g., SSD=90%). In this way, we could
select fewer inputs for real execution and training without
impacting the model’s accuracy. Based on our evaluation, on
average, half of the time spent on fuzzing could be saved
when applying this technique (Section 6).
4.4 Model Updating
To realize online model updating, we utilize incremental learn-
ing [26] to train a dynamic model by feeding a set of data
each time rather than feeding all data at once. In this case, new
incoming data are continuously used to extend the existing
model’s knowledge. Incremental learning aims to adapt to
new data without forgetting its existing knowledge for the
learning model, and it does not require retraining the model.
It can be applied when the training data set becomes avail-
able gradually over time as the carrier fuzzer generates and
exercises new inputs continuously. Also incremental learning
decreases the time of waiting for data collecting, and ﬁlters
out more unreachable test cases.
The online deep learning model should be updated to keep
its accuracy. Whenever a new set of labeled data is collected,
there could be an opportunity for model updating. However, if
the model is updated too frequently, the time spent on training
will be long, which will impact the efﬁciency of fuzzing. In
contrast, if less frequent updating is performed, the model
may not be accurate. So in this process, we should carefully
choose when to perform model updating. Also we should let
the updating be quick enough. Below we elaborate the details.
We perform model updating when the model is getting
“outdated”. The outdated model is not accurate enough when
a new pre-dominating node is reached. In the ﬁrst situation,
we update the model when the false positive rate γ of the
model exceeds a threshold θ f . To achieve this, we continu-
ously record false positive rates of the model whenever the
execution results are different from the predictions, and keep