4.1.1 Procedure
Interview Guide. We developed a question set that
served as the basis for each interview,2 basing our de-
sign on prior work [9] but focusing particularly on onion
services. The semi-structured nature of our interviews
allowed us to deviate from this question set by asking
follow-up questions as appropriate.
We followed standard consent procedures for all par-
ticipants. We began by asking demographic information
(gender, age range, occupation, country of residence, and
level of education), followed by questions about users’
general online behavior. We concluded with questions
about Tor Browser and onion services (e.g., when users
started to use these services, how they track onion links
as well as the drawbacks and strengths of these services
based on their own experiences). To gather data about
users’ mental models of Tor browser and onion services,
we designed a brief sketching exercise similar to those
used in other work [25]. We asked participants to draw
sketches of how they believed Tor and onion services
worked and followed up on these drawings in interviews.
Recruitment. To select eligible interview subjects, we
created a short pre-interview survey3 asking users if they
were over 18 years of age, if they had used Tor Browser
and onion services, and how they would rate their general
privacy and security knowledge. To the extent possible,
1Princeton University’s institutional review board (IRB) approved
we targeted lay-people and aimed to maximize cultural,
gender, geographic location, education, and age diversity.
The Tor Project advertised this survey both in a blog
post [37] and via Twitter. We also advertised the study
on Princeton’s Center for Information Technology (CITP)
blog and recruited participants in person at an Internet
freedom event.
Recruiting a representative sample of Tor users is dif-
ﬁcult, and our recruiting techniques likely resulted in
a biased population for several reasons. First, we be-
lieve that The Tor Project’s blog and Twitter account
are followed by disproportionately more technical users,
whereas non-technical users may not generally follow
news and updates related to Tor via the project’s blog and
Twitter feed. Second, Tor users value their privacy more
than the average Internet user, so the users we recruited
may not be as honest and candid about their browsing
habits as we would like.
Interviews. We conducted 13 interviews in person and
four interviews remotely—over Skype, Signal, WhatsApp,
and Jitsi—depending on the medium that our participants
preferred. Two participants declined to have their inter-
views recorded; we recorded the rest of the interviews
with the permission of the participant. All participants an-
swered the interview questions and completed the sketch-
ing exercise. Each interview ended with a debrieﬁng
phase to ask if our participants had any remaining ques-
tions. We compensated participants with a $20 gift card.
We conducted our ﬁrst interview on July 13, 2017 and the
last on October 20, 2017. The median interview time was
34 minutes, with interviews ranging from 20–50 minutes.
Transcription and Analysis. We transcribed our inter-
view recordings and employed qualitative data coding to
analyze the transcripts [29]. In the two cases where we
did not have interview recordings, we relied on our ﬁeld
notes. We developed a codebook based on our research
questions and used a combination of deductive coding to
identify themes of interest we agreed upon and inductive
coding to discover emergent phenomena and to expand
the initial codebook. We had ten parent codes in total,
with examples such as “Mental model of onion services”,
“Search habits”, and “Reasons for using onion services”;
and 168 child codes, including “Deﬁnition- anonymous”,
“Word of mouth”, and “Curiosity”. After we reached con-
sensus on the phenomena of interest, at least two members
of our team (sometimes up to four) read and coded each
transcript. We also held regular research meetings with
the entire team of authors to discuss the coded transcripts
and reach consensus on the ﬁnal themes.
this study (Protocol #8251).
2The
question
set
is
available
at
https://nymity.ch/
4.1.2 Participants
onion-services/pdf/interview-checklist.pdf.
3The pre-interview survey is available at https://nymity.ch/
onion-services/pdf/pre-interview-survey.pdf.
We interviewed 17 subjects, as summarized in Table 1.
We only present aggregate demographic information to
414    27th USENIX Security Symposium
USENIX Association
protect the identity of our interview participants. We
believe that our sample is biased towards educated and
technical users—almost 60% of our participants have a
postgraduate degree—but our sample also shows the di-
versity among Tor’s user base: our participants comprised
human rights activists, legal professionals, writers, artists,
and journalists, among others. In remainder of the paper,
we use the denotation ‘P’ to refer to interview participants.
4.2 Online Survey
Shortly after we conducted our ﬁrst batch of interviews,
we designed, reﬁned, and launched an online survey to
complement our interview data.4
4.2.1 Procedure
Survey Design. We created our survey in Qualtrics be-
cause an unmodiﬁed Tor Browser could display it cor-
rectly. Unfortunately, Qualtrics requires JavaScript, and
Tor Browser deactivates if it is set to its highest security
setting. Several users complained about our reliance on
JavaScript in the recruitment blog post comments [37].
All respondents consented to the survey and conﬁrmed
that they were at least 18 years old. Our survey was
only available in English, but we targeted an international
audience because Sawaya et al. showed that cultural
differences yield different security behavior [27], and pay-
ing attention to these differences is central to The Tor
Project’s global mission.
Most of our survey focused on onion services, but we
also included usage questions about Tor in general be-
cause Tor Browser is used to access onion services. Our
survey had of 49 questions, most of which were closed-
ended questions. The ﬁrst set of questions asked for basic
demographic information such as age, gender, privacy
and security knowledge rating, and education level. Next,
the survey asked about Tor usage, such as how frequently
the Tor Browser was used. We also asked about onion
services usage in detail, including questions concerning
the usability of onion links, how users track and manage
onion domain links, whether (and why) users had ever
set up or operated an onion site, and whether users were
aware of onion site phishing and impersonation. The last
set of questions focused on users’ general expectations
of privacy and security when using onion services. We
incorporated four attention checks to measure a respon-
dent’s degree of attention [3]. To ensure that participants
felt comfortable answering questions, we did not make
questions mandatory. The survey took about 15 minutes
to complete.
Survey Testing. We used cognitive pretesting (some-
times also called cognitive interviewing) to improve the
wording of our survey questions [6]. Pretesting reveals
if respondents understand questions consistently and the
way we intended them to be interpreted. Five pre-testers
helped us iteratively improve the survey; after pre-testing
and revisions, we launched the survey.
Recruitment. As with our interviews, we advertised our
survey in a blog post on The Tor Project’s blog [37],
on its corresponding Twitter account, the CITP blog at
Princeton, and on three Reddit subforums.5 Unlike our
interview participants, our survey respondents were self-
selected. As with interview recruitment, we expect this
recruitment strategy biased our sample towards engaged
users because casual Tor users are unlikely to follow The
Tor Project’s social media accounts.
We did not offer incentives for participation because we
wanted respondents to be able to participate anonymously
without providing email addresses. Despite the lack of
incentives, we collected enough responses. Our survey
ran from August 16–September 11, 2017 (27 days).
Filtering and Analysis. Some of the survey responses
were low-quality; people may have rushed their answers,
aborted our survey prematurely, or given deliberately
wrong answers. To mitigate these effects, we excluded
participants who either did not ﬁnish the survey or who
failed more than two out of four attention checks. We con-
ducted a descriptive analysis on the survey data. We also
computed correlation coefﬁcients between every question
pair in the survey, which did not yield signiﬁcant results.
We thus focus on results from the descriptive analysis.
Each percentage is reported out of the total sample; we
denote cases when survey participants chose not to re-
spond as ‘No Response’. Two researchers performed a
deductive coding pass on the open-ended survey questions
based on our interview codebook and held meetings to
reach consensus on the ﬁnal themes discussed. In rest of
the paper, we denote survey participants with ‘S’.
4.2.2 Participants
We collected 828 responses, but only 604 (73%) com-
pleted the survey, and 517 (62%) passed at least two at-
tention checks. The rest of the paper focuses on these 517
responses. Table 2 shows the demographics of our survey.
As we expected, respondents were young and educated:
more than 71% were younger than 36, and 61% had at
least a graduate or post-graduate degree. 44% percent
also considered themselves at least highly knowledgeable
in matters of Internet privacy and security.
4The
full
survey
is
onion-services/pdf/survey-questions.pdf.
available
at
https://nymity.ch/
5 https://reddit.com/r/tor/, https://reddit.com/r/onions/
https://reddit.com/r/samplesize/.
USENIX Association
27th USENIX Security Symposium    415
Age
18–25
26–35
36–45
46–55
#
2
10
4
1
% Gender
Female
11.8
58.8 Male
23.5
5.9
#
5
12
% Continent of residence
29.4 Asia
70.6 Australia
Europe
North America
South America
#
3
1
4
8
1
% Education
17.6 No degree
5.9 High school
23.5 Graduate
47.1
5.9
Postgraduate
#
1
3
3
10
%
5.9
17.7
17.7
58.8
Table 1: The distribution over gender, age, country of residence, and education for our 17 interview subjects. We do not show
per-person demographic information to protect the identity of our interview subjects.
Gender
Male
Female
Other
No Response
#
438
49
25
5
% Age
84.7
9.4
4.8
1.0
18–25
26–35
36–45
46–55
56–65
> 65
No Response
#
186
180
87
43
16
3
2
% Education
No degree
High school
Graduate
Post graduate
No Response
35.9
34.8
16.8
8.3
3.1
0.6
0.4
#
25
172
214
102
4
% Domain knowledge
4.8
None
33.2 Mild
41.4 Moderate
19.7
0.4
High
Expert
No Response
#
1
35
178
227
75
1
%
0.2
6.8
34.4
43.9
14.5
0.2
Table 2: The distribution over gender, age, education, and domain knowledge of the survey respondents. Providing demographic
information was optional, so we lack data for some respondents.
4.3 Domain Name Service (DNS) Queries
4.4 Limitations
We analyzed .onion domains leaked via the Domain
Name System (DNS) to better understand onion service
usage and look for speciﬁc evidence of usability issues