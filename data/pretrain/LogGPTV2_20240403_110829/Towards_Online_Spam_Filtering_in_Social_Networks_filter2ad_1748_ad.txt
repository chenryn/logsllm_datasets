(
s
r
i
a
P
e
g
a
s
s
e
M
f
o
%
 100
 97
 94
 91
 88
 85
Spam Message
Legitimate Message
 0
 0.2
 0.4
 0.6
 0.8
 1
Message Resemblance
Figure 13: The distribution of resemblance value among
spam and legitimate message pairs in the Facebook dataset,
respectively.
the legitimate message pairs, respectively, by repeating the
clustering process. Figure 13 shows the cumulative dis-
tribution of such value. The curve for legitimate message
pairs embraces the y axis. For both types of message pairs,
most of them have a very small resemblance value, meaning
that they are almost totally dissimilar. This is expected be-
cause legitimate messages are spontaneously generated and
are different to others naturally. For spam messages, many
campaigns exist in our dataset and the message pairs across
different campaigns are also very different. However, the
curve for spam message pairs exhibits a sudden rise at the
right most part, showing that some message pairs are very
similar, which are those belonging to the same spam cam-
paigns. The legitimate message pairs do not exhibit such a
pattern due to the spontaneous nature. Despite this differ-
ence, it is apparent that both curves are ﬂat in the middle. It
suggests that comparably very few message pairs have the
resemblance value falling in the middle area. Consequently,
the system is insensitive to the threshold value as long as the
value is not too small or too large, since varying the thresh-
old would not signiﬁcantly affect the clustering result. At
last we pick 0.5 as the threshold used in all the experiments.
5.3 Accuracy
The accuracy of a detection system is characterized by
two metrics, true positive rate (TPR) and false positive rate
(FPR). True positive rate shows the detection completeness.
It is deﬁned as the number of instances correctly classiﬁed
as spam divided by the total number of spam instances.
False positive rate reﬂects the detection error on the legit-
imate messages.
It is deﬁned as the number of instances
incorrectly classiﬁed as spam divided by the total number
of legitimate instances. In this section we evaluate the over-
all accuracy (Section 5.3.1), the accuracy of different fea-
)
%
(
e
t
a
R
e
v
i
t
i
s
o
P
e
u
r
T
 90
 85
 80
 75
 70
 65
 60
 55
(10:1)
Facebook data
Twitter data
(4:1)
(1:1)
(1:4)
(1:10)
(1:4)
(1:10)
(1:1)
(4:1)
(10:1)
 0
 0.2
 0.4
 0.6
 0.8
 1
False Positive Rate (%)
Figure 14: The overall detection accuracy subjecting to the
varying spam to legitimate message ratio in the training set.
ture sets (Section 5.3.2) and the accuracy over time (Sec-
tion 5.3.3) to show that our system can indeed provide sat-
isfactory accuracy guarantee.
We use the C4.5 decision tree library provided by Quin-
lan [20].
5.3.1 Overall Accuracy
We ﬁrst test the overall detection accuracy, using all appli-
cable features and the complete dataset. An accurate de-
tection system desires high true positive rate and low false
positive rate simultaneously. Unfortunately, parameter tun-
ing usually causes these two metrics to increase or decrease
at the same time. We are forced to face the trade-off be-
tween them. We decided to tune the classiﬁer to emphasize
low false positive rate while maintaining a reasonably high
true positive rate.
As suggested by Zadrozny et al.
[33] and used by
Thomas et al. [27], we adjust the ratio of spam to legitimate
message in the training set by random sampling to tailor the
performance, where a larger ratio indicates a stronger ten-
dency to classify a message as spam. We use the ratios of
10:1, 4:1, 1:1, 1:4 and 1:10. Regardless of the ratio in the
training set, the full testing set is used. Figure 14 shows the
detection accuracy. For the Twitter dataset, the two rates
grow in consistency with the ratio in the training set. Both
rates minimize when the ratio is 1:10, and maximize when
the ratio is 10:1. A 1:1 ratio in the training set results in a
reduction of false positive rate by about 20%, comparing to
a 10:1 ratio. On the other hand, the true positive rate de-
creases very slightly and reaches 69.8% at this ratio. A 1:4
ratio or a 1:10 ratio results in a substantial drop in both true
positive rate and false positive rate. After all, we still favor
the 1:1 ratio for the twitter data. The detection accuracy of
the Facebook dataset exhibits irregularity. A 4:1 ratio yields
the lowest false positive rate (0.19%) and a relatively high
true positive rate (80.9%). For the remainder of the evalua-
Feature Set
OSN Speciﬁc
General
Features Contained
Social Degree
Interaction History
Cluster Size
Average Time Interval
Average URL Number
Unique URL Number
TPR
FPR
38.3% 0.30%
80.8% 0.32%
Table 1: The detection accuracy using each of the two fea-
ture sets with the Facebook dataset.
tion, we conduct all experiments over the Facebook dataset
at a 4:1 ratio in the training set and all experiments over the
Twitter dataset at a 1:1 ratio.
5.3.2 Accuracy of Different Feature Sets
As stated in Section 3, we divide our features into two sets:
the OSN speciﬁc features and the general features. To un-
derstand their signiﬁcance to the system, we train the classi-
ﬁer exclusively using each feature set, test the detection ac-
curacy and present the result in Table 1. The ratio of spam
to legitimate message in the training set is 4:1. Since the
Facebook dataset uses the complete 6 features, we conduct
this experiment using the Facebook dataset. The general
features achieve a true positive rate of 80.8%, which is sim-
ilar to the result of using all features. Unfortunately, the
false positive rate increases by more than 50%. On the other
hand, the OSN speciﬁc features lead to a lower true positive
rate (38.3%). We do not mean to compare between these
two sets and decide which one gives better performance.
Rather, the result shows that the detection accuracy would
be signiﬁcantly degraded in the absence of either feature
set. Using their combination keeps the high true positive
rate while reducing the false positive rate.
5.3.3 Accuracy Over Time
Spam campaigns are not persistent. Criminals will promote
new campaigns on a continuous basis. As a result, it is im-
portant to evaluate how much time it takes before the clas-
siﬁer becomes out of date and needs to be re-trained. We
carry out the evaluation using the Facebook dataset, because
it contains messages generated throughout a time period of
one year and a half. In comparison, the Twitter dataset is
collected in a much shorter period of time and is not suit-
able for this type of evaluation. We use 4:1 as the ratio
of spam to legitimate message in the training set. We dis-
sect the testing set into 3-month time periods and study how
the detection accuracy changes over time. We use the same
trained classiﬁer on the ﬁrst 9 months of testing data, and
measure the true positive rate as well as the false positive
rate in each period. We only present the result in the ﬁrst
)
%
(
e
t
a
R
e
v
i
t
i
s
o
P
e
u
r
T
 100
 80
 60
 40
 20
 0
TPR
FPR
 0.1
 0.08
 0.06
 0.04
 0.02
 0
)
%
t
(
e
a
R
e
v
i
t
i
s
o
P
e
s
a
F
l
0
-
3
3
-
6
6
-
9
# of Months after Initial Training
)
%
(
e
t
 100
a
R
e
v
i
t
i
s
o
P
e
u
r
T
 80
 60
 40
 20
 0
TPR (FB)
FPR (FB)
TPR (TW)
FPR (TW)
 1
 0.8
 0.6
 0.4
 0.2
 0
0.2 0.4 0.6 0.8
Removed Spam Ratio
)
%
t
(
e
a
R
e
v
i
t
i
s
o
P
e
s
a
F
l
Figure 15: The detection accuracy on the Facebook dataset
over time.
Figure 16: The detection accuracy on the Twitter dataset
under stealthy attack.
9 months, because after 9 months the spam becomes very
sparse in our dataset and the true positive rate is not repre-
sentative. However, we do observe a signiﬁcant rise in the
false positive rate after 9 months, suggesting that our sys-
tem may need re-training after such a long time to further
reduce the false positive rate. Figure 15 shows the experi-
mental result.
The false positive rate remains extremely low for all 3
periods. The period between 3 and 6 months after training
incurs the highest false positive rate, which is about 0.04%.
It shows that our system misclassiﬁes very few legitimate
messages as spam, even if the training takes place up to 9
months before the time of classiﬁcation. The true positive
rate in the ﬁrst period and the third is a bit lower. We ﬁnd
that it is caused by campaigns heavily adopting obfuscation
that results in large number of small clusters instead of small
number of big ones. This type of campaign does not appear
in the second period. It demonstrates that the features we
select indeed capture the commonality among spam cam-
paigns, so that they remains effective even if the campaigns
being classiﬁed do not present in the training data. These
ﬁndings indicate that our system can remain accurate with-
out the need for re-training for a long period of time, which
minimizes the maintenance cost after deployment.
5.4 Resilience against Stealthy Attack
One of the spammers’ common attempts to evade detec-
tion is to carry out stealthy attack. Under stealthy attack, the
speed of spam generation is reduced so that the anti-spam
system cannot acquire sufﬁcient instances to learn the spam
pattern. To simulate such an attack, we randomly remove
certain proportion of spam messages , ranging from 20%
to 80%, from both dataset. The effect is the same as if the
spammers generate spam messages at a reduced speed. The
legitimate messages are untouched. After that, we repeat
the overall accuracy test on the modiﬁed dataset, choosing
4:1 and 1:1 as the ratio of spam to legitimate messages in the
training set for Facebook and Twitter dataset , respectively.
Figure 16 illustrates the result. In the Twitter experiment,
the true positive rate and the false positive rate exhibit very
slight difference comparing with the experiment using the
original dataset in all test cases. In the Facebook experi-
ment, the true positive rate and the false positive rate de-
crease moderately when we remove 20%, 40% and 60% of
spam messages. The reason is that as spam becomes more
sparse in the dataset, more spam clusters become indistin-
guishable from legitimate message clusters. When we re-
move 80% of spam messages, the system does not handle
it gracefully as the true positive rate drops to 49% and the
false positive rate rises to 0.44%. The results show that ex-
cept for the Facebook experiment when we remove 80% of