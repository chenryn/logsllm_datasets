[39] Samee Zahur and David Evans. 2015. Obliv-C: A Language for Extensible Data-
Oblivious Computation. IACR Cryptology ePrint Archive 2015 (2015), 1153.
[40] Jingren Zhou. 2009. Sort-Merge Join.
In Encyclopedia of Database Systems.
Springer US, 2673â€“2674.
A BASELINE PROTOCOLS FOR SCATTERINIT
A.1 Naive solutions
One direct way to implement the the functionality of Figure 6 is
using generic MPC such as garbled circuits. The approach requires
a circuit of size ğ‘‚(ğ‘›ğ‘™ğœ) that for each ğ‘– âˆˆ [ğ‘›] selects the ğ‘–th output
among the all possible values (0, r1, . . . , rğ‘›âˆ’ğ‘™). Hence a solution
based on generic MPC constructions would require ğ‘‚(ğ‘›ğ‘™ğœ) commu-
nication and computation. Alternatively, one can rely on additive
homomorphic encryption to enable P1 to distribute the encrypted
values of r into the right positions of encrypted râ€² and then execute
a protocol with P2 to obtain shares of râ€² in the clear. This approach
requires ğ‘‚(ğ‘›) computation and ğ‘‚((ğ‘›+ğ‘™)ğ¿) communication where ğ¿
is the length of a ciphertext of additively homomorphic encryption,
which adds considerable expansion to the length of the encrypted
value.
A.2 FSS-based ScatterInit
S
S
C
C
, ğ›¼) = ğ›½, otherwise.
, ğ›¼) âŠ• FSS.Eval(KFSS
Function Secret Sharing (FSS). Our construction below uses as a
building block function secret sharing (FSS) [6]. While this primitive
provides functionality for general functions, we use its instantiation
for point functions, which also implies private information retrieval
(PIR). A point function ğ‘“ğ›¼,ğ›½(ğ‘¥) with domain [ğ‘›] is defined as ğ‘“ (ğ›¼) =
ğ›½ and ğ‘“ (ğ‘–) = 0 for all ğ‘– â‰  ğ›¼. A function secret sharing scheme
has two algorithms FSS.KeyGen and FSS.Eval. The key generation
produces two keys (KFSS
) â† FSS.KeyGen(ğ›¼, ğ›½) that when
, KFSS
S
evaluated, satisfy FSS.Eval(KFSS
, ğ‘–) if ğ‘– â‰  ğ›¼ and
, ğ‘–) = FSS.Eval(KFSS
C
FSS.Eval(KFSS
First, we present a solution for the ScatterInit functionality
of Figure 6 in the case ğ‘™ = 1, and then discuss how to extend it to
any value of ğ‘™. The two parties will generate in a secure computa-
tion a pair of FSS keys (ğ‘˜1, ğ‘˜2) with the following properties. Let
[[vğ‘–]]1 = Fğ‘˜1(ğ‘–) and [[vğ‘–]]2 = Fğ‘˜2(ğ‘–) for ğ‘– âˆˆ [ğ‘›], then we have that
âˆ€ğ‘— â‰  ğ‘–1 : [[vğ‘—]]1 = [[vğ‘—]]2, and [[vğ‘–1]]1 âŠ• [[vğ‘–1]]2 = r1. This compu-
tation can be done several times in parallel to obtain a protocol for
the case ğ‘™ > 1, where each party XORs locally its output vectors
from all ğ‘™ executions. This protocol has ğ‘‚(ğ‘™ log(ğ‘›)) communication,
ğ‘‚(ğ‘™ log(ğ‘›)) computation in MPC, and ğ‘‚(ğ‘™ğ‘›) local computation for
each party. One drawback of this approach is that the result would
be xor-shared, and for our applications we require additive shares
for efficiency, as we will perform arithmetic operations. A naive
conversion from xor to additive shares in a circuit would require
ğ‘‚(ğ‘›) additions, bumping up the computation and communication
to be linear in ğ‘›, which is prohibitive for the values of ğ‘› to be found
in some realistic data analysis (see Section 8). This overhead can
be avoided as follows: starting with the case ğ‘™ = 1. Similarly to
above the parties generate FSS keys (ğ‘˜1, ğ‘˜2) with the difference
that [[vğ‘–1]]1 âŠ• [[vğ‘–1]]2 = x instead of r1, where x is a random value
[[r1]]1, ğ‘1 =
ğ‘2 =
not known to either party, while âˆ€ğ‘— â‰  ğ‘–1 : [[vğ‘—]]1 = [[vğ‘—]]2. Sub-
sequently, the parties run a garbled circuit protocol with inputs
ğ‘— âˆˆ[ğ‘›]([[vğ‘—]]1), and [[vğ‘–1]]1 from P1, and [[r1]]2 and
ğ‘— âˆˆ[ğ‘›]([[vğ‘—]]2) from P2. This secure computation (a) com-
putes [[vğ‘–1]]2 as ğ‘1 âŠ• ğ‘2 âŠ• [[r1]]2, (b) reconstructs r1, and (c) reveals
ğ‘  = r1 âˆ’ [[vğ‘–1]]2 to P1. Finally, to obtain an additive share of the
intended sparse vector râ€², P1 sets [[râ€² ğ‘—]]1 = âˆ’[[vğ‘—]]1, for all ğ‘— â‰  ğ‘–1,
and [[râ€²ğ‘–1]]1 = ğ‘ , while P2 sets [[râ€² ğ‘—]]2 = [[vğ‘—]]2, for all ğ‘— âˆˆ [ğ‘›].
Running this protocol several times gives a protocol for the
general case (ğ‘™ > 1) from Figure 6 with ğ‘‚(ğ‘™ log(ğ‘›)) communication,
ğ‘‚(ğ‘™ log(ğ‘›)) computation in MPC, and ğ‘‚(ğ‘™ğ‘›) local computation for
each party. The garbled circuit sub-protocol is extremely efficient,
as it requires ğ‘™ additions and 2ğ‘™ XORs, where the latter can be
performed locally with the Free-XOR optimization [23].
B NAIVE BAYES APPLICATION
This section describes our third application that we built using the
functionality from our framework, which was omitted from the
main paper body due to space constraints.
the above expression into score(ğ‘) = log(ğ‘ƒ(ğ‘)) +ğ‘¡ âˆˆğ‘‘ log(ğ‘ƒ(ğ‘¡|ğ‘)).
B.1 Secure Naive Bayes Classification
A naive Bayes classifier is a non-parametric supervised classifi-
cation algorithm that assigns to an item ğ‘‘ (for example a doc-
ument) the class ğ‘ in a set of potential classes ğ¶ (for example
{spam, no-spam}) that maximizes the expression score(ğ‘) = ğ‘ƒ(ğ‘)Â·
Î ğ‘¡ âˆˆğ‘‘ ğ‘ƒ(ğ‘¡|ğ‘), where ğ‘¡ âˆˆ ğ‘‘ denotes the database features present in
the feature representation of ğ‘‘. A common approach to keep under-
flows under control is to use logs of probabilities. This transforms
In Naive Bayes, ğ‘ƒ(ğ‘) is estimated as ğ‘ƒ(ğ‘) = ğ‘ğ‘/ğ‘ , namely the
number of items ğ‘ğ‘ of class ğ‘ in the dataset, divided by the dataset
size ğ‘ . ğ‘ƒ(ğ‘¡|ğ‘) is estimated as ğ‘ƒ(ğ‘¡|ğ‘) = ğ‘‡ğ‘,ğ‘¡/ğ‘ğ‘, namely the number
of occurrences (or score) ğ‘‡ğ‘,ğ‘¡ of feature ğ‘¡ in items of class ğ‘, normal-
ized by the total number of examples of class ğ‘ in the training dataset.
Additionally, Laplace smoothing is often used to correct for terms
not in the dataset, redefining ğ‘ƒ(ğ‘¡|ğ‘) to be ğ‘ƒ(ğ‘¡|ğ‘) = (ğ‘‡ğ‘,ğ‘¡+1)/(ğ‘ğ‘+ğ‘)
A secure two-party naive Bayes classification functionality is
defined as follows: a server holds the dataset ğ· that consists of ğ‘›
items with ğ‘˜ features. Each item in the dataset is labeled with its
class from the set ğ¶ of potential classes. Hence, the server holds the
values ğ‘ƒ(ğ‘¡|ğ‘), ğ‘ƒ(ğ‘) defined above. A client wants to obtain a label
for an item ğ‘‘. This needs to be done in a privacy preserving manner
where only the client learns the output label and the server learns
nothing.
The work of Bost et al. [5] presented a solution to the above
problem using Paillier encryption and an argmax protocol based
on additive homomorphic encryption. Our ROOM functionality
provides a direct solution for this two-party problem, in which
the server reveals an upper bound of its number of features. This
solution works as follows: for each class ğ‘™ âˆˆ ğ¶, the server and the
client invoke the ROOM functionality with input values log(ğ‘ƒ(ğ‘¡|ğ‘™))
for all keys ğ‘¡, as well as default values 1/(ğ‘ğ‘ +ğ‘) for the server, and
query (ğ‘¡)ğ‘¡ âˆˆğ‘‘ for the client. This gives the parties additive shares of
the vector (log(ğ‘ƒ(ğ‘¡|ğ‘™))ğ‘¡ âˆˆğ‘‘. Then, the parties can compute locally
shares of the vector (score(ğ‘™))ğ‘™âˆˆğ¶, which contains the scores of
ğ‘‘ with respect to all classes. Finally, the class with highest score,
14
Dataset
Offline Time
Total Time
SecureML
Ours
SecureML
Ours
Movies
Newsgroups
Languages, ngrams=1
Languages, ngrams=2
4d16h34m55.36s
1d5h40m20.66s
1m7.18s
18h15m18.24s
4h18m52.67s
1h5m15.15s
35.65s
4m1.77s
4d18h35m26.99s
1d6h9m32.82s
1m39.36s
18h34m34.36s
9h29m23.53s
2h26m21.82s
3m18.35s
13m36.84s
Ours
Total Communication
Offline Communication
SecureML
SecureML
19.19 GiB 761.73 MiB 19.33 GiB
5.01 GiB 190.38 MiB
5.13 GiB
3.4 MiB
1.9 MiB 4.42 MiB
3.05 GiB
Ours
1.92 GiB
1.31 GiB
111.76 MiB
4.08 GiB 893.73 MiB
11.71 MiB
Table 4: Comparison of our approach with SecureML [29] in the WAN setting. See also Table 3.
C ADDITIONAL EXPERIMENTAL RESULTS
C.1 Experiments in the WAN Setting
Figure 16 and Table 4 show the results of the experiments from
Section 8 in the WAN.
C.2 ROOM Micro-Benchmarks
Table 1 presents the runtimes for Circuit-ROOM and Poly-ROOM
and how they depend on the database size ğ‘› and the query size
ğ‘š. We first measure the runtimes of each algorithm for a range
of parameters ğ‘› âˆˆ {500, 5000, 50000} and ğ‘š âˆˆ {0.1ğ‘›, 0.2ğ‘›, . . . , ğ‘›}.
The results can be seen in Figure 17. Each plot corresponds to
one choice of ğ‘›, while values of ğ‘š are given on the x-axes. The
runtime of both ROOM variants increases as ğ‘š grows, but Circuit-
ROOM is outperformed by Poly-ROOM as ğ‘› increases, as long as
ğ‘š 95k features, our protocol takes less than 2s. In contrast, the total
classification time for a dataset with only 70 features took over 3
seconds in [5].
15
MoviesNewsgroupsLanguages,ngrams=1Languages,ngrams=2Dataset1s2s5s10s30s1m2m5m10m30m1hRunningTimeNaiveBayes(LAN)Basic-ROOMCircuit-ROOMPoly-ROOMFigure 16: (Left) Running time of a Naive Bayes query in the WAN. See also Figure 15. (Middle) Running time of a ğ‘˜-NN query
in the WAN. See also Figure 13. (Right) Total running time of an SGD training epoch for logistic regression with varying
document sparsity. Note that unlike in the LAN, we can use SecureMLâ€™s homomorphic encryption-based offline phase [29]
here that also benefits from larger batches. See also Figure 14.
Figure 17: Measured running times of each of our ROOM constructions in the LAN setting, for several choices of query size
and database size. We distinguish between local time (for time spent doing local computation) and MPC time, for running
time of MPC sub-protocols. Error bars indicate 95% confidence intervals.
ning times were measured for length ğ‘š queries to a ROOM of size ğ‘›, with ğ‘š âˆˆ (cid:8)2ğ‘–
{0, . . . , 18}(cid:9). Then, for each of our algorithms, a model of the running time was computed using nonlinear least-squares from
Figure 18: Estimated performance of our two instantiations of sparse ROOM in the LAN (left) and WAN (right) settings. Run-
ğ‘– âˆˆ
scipy.optimize.curve_fit, where the function to be fitted was chosen according to the asymptotics in Table 1. Each pixel was
computed by averaging over the colors corresponding to each algorithm, weighted by the inverse of their respective running
times. Thus, the dominant color of a region corresponds to the algorithm that performs the best in that setting.
ğ‘– âˆˆ {0, . . . , 13}(cid:9) and ğ‘š âˆˆ (cid:8)2ğ‘–
|
|
16
MoviesNewsgroupsLanguages,ngrams=1Languages,ngrams=2Dataset1s2s5s10s30s1m2m5m10m30m1hRunningTimeNaiveBayes(WAN)Basic-ROOMCircuit-ROOMPoly-ROOMMoviesNewsgroupsLanguages,ngrams=1Languages,ngrams=2Dataset1s2s5s10s30s1m2m5m10m30m1h2h5h10h1d2dRunningTimek-NN(WAN)DenseBasic-ROOMCircuit-ROOMPoly-ROOMO(cid:128)linetimeOnlinetime1282565121024BatchSize30s1m2m5m10m30m1h2h5hRunningtimeLogisticRegression(WAN)Dense10.0%Nonzeros5.0%Nonzeros2.0%Nonzeros1.0%Nonzeros50100150200250300350400450500(cid:131)erysizem0.00.20.40.60.81.01.21.4Time(s)Databasesizen=500(LAN)Circuit-ROOMPoly-ROOMLocaltimeMPCtime500100015002000250030003500400045005000(cid:131)erysizem024681012Time(s)Databasesizen=5000(LAN)Circuit-ROOMPoly-ROOMLocaltimeMPCtime5k10k15k20k25k30k35k40k45k50k(cid:131)erysizem020406080100120Time(s)Databasesizen=50000(LAN)Circuit-ROOMPoly-ROOMLocaltimeMPCtime01000020000300004000050000Databasesize01000020000300004000050000(cid:131)erysizeAlgorithmwiththelowestrunningtime(LAN)Circuit-ROOMPoly-ROOM01000020000300004000050000Databasesize01000020000300004000050000(cid:131)erysizeAlgorithmwiththelowestrunningtime(WAN)Circuit-ROOMPoly-ROOM