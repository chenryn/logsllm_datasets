p. We prove that KServ never allocates the pfn owned by
VM q to p, and executing run_vcpu does not affect q’s states.
Therefore, the resulting states of q remain indistinguishable.
Finally, we prove noninterference for the VM p. Starting from
two indistinguishable states for p, the resulting cache will
not contain an entry for pfn, and pfn’s contents in memory
contains the same value, while the pfn is mapped to p’s stage
2 page tables. The resulting states are indistinguishable to p.
When reclaiming pages from a terminated VM p, KCore
scrubs each reclaimed pfn and calls cache-flush on the pfn
in clear_vm_page, which invalidates the pfn in the cache and
writes the scrubbed pfn to main memory; cache-flush has
USENIX Association
30th USENIX Security Symposium    3963
no effect if the pfn is not cached. We prove noninterference
for KServ. From two states indistinguishable to KServ, after
making the hypercall, the pfns reclaimed from p will be
owned by KServ. These pages will not be cached, and their
contents in memory are scrubbed. The resulting states remain
indistinguishable to KServ. This ensures that an attacker in
KServ that bypasses the cache, as shown in Figure 4, cannot
access VM p data. We prove noninterference for all VMs
other than p. Consider a VM q different from VM p, starting
from two indistinguishable states, KCore does not change
any of q’s states when handling the hypercall for KServ, thus
the resulting states of q remain indistinguishable.
We prove that the use of SMMU page tables by top-level
primitives preserves state indistinguishability. Similar to page
tables, we verify an SMMU page table isolation invariant that
any page mapped by a device’s SMMU page table must be
owned by the device’s owner. With this invariant, we prove
that a principal p’s states are not changed by load and store
operations from a device owned by any other principal q
using their SMMU page tables. Similarly, we prove that
SMMU primitives that transfer page ownership also do not
affect state indistinguishability. The transfer only happens
when KServ calls the SMMU hypercall to map a pfn to
the SMMU page table used by a VM p’s device. KCore
ensures the pfn is unmapped from KServ’s stage 2 page table
before transferring the owner of pfn from KServ to p. We
thus ensure that use of SMMU page tables preserves state
indistinguishability with respect to VM memory.
Although SeKVM’s implementation was based on the
codebase of HypSec, we have veriﬁed the correctness of
KCore, SeKVM’s TCB, and veriﬁed the security guarantees
of SeKVM. We veriﬁed that KCore contains no vulnerabili-
ties and that any vulnerabilities in KServ cannot compromise
SeKVM’s guarantees of VM conﬁdentiality and integrity.
In fact, while verifying SeKVM, we found various bugs in
HypSec’s TCB that affect HypSec’s security guarantees. For
example, we found a TLB management bug in which HypSec
did not ﬂush the SMMU TLB after unmapping a page from
the SMMU page tables. We ﬁxed the bug in KCore by adding
a SMMU TLB ﬂush after the unmap. As another example,
we found a cache management bug in HypSec in which a
VM boot image may be cached when loaded from the ﬁle
system but not written back to main memory. As VMs are
booted with paging and caching disabled, it is possible that
the VMs access the page content in memory, thereby not
using the correct VM images. We ﬁxed the bug in KCore
by ﬂushing the corresponding cache lines for memory that
contain the pre-loaded VM image before booting the VM,
ensuring the use of the correct VM image loaded in memory.
5 Implementation
We refactored KVM into SeKVM, starting with the HypSec
codebase and structuring its TCB into layers. We ﬁrst did this
Component C+Asm Spec Code Reﬁne CodeAll ReﬁneAll
1.4K
Exit Handler
1.3K
VCPU
1.5K
VM Boot
1.4K
SMMU
VM Mem
2.6K
2.7K
SMMU PT
4.7K
MMU PT
2.2K
Lock
Total
17.8K
1K
0.4K 1.7K 0.2K 1.1K
3.3K
0.8K 0.5K 2.4K 0.9K
2.8K
0.9K 1.0K 0.6K 1.1K
1.8K
0.5K 0.7K 0.2K 1.0K
2.3K
0.5K 0.9K 0.6K 2.2K
1.6K
0.2K 0.5K 0.1K 2.3K
1.7K
0.4K 0.5K 0.1K 4.3K
0.1K 0.2K 1.2K 1.8K
2K
3.8K 6.0K 5.4K 14.7K 16.5K
Table 1: KCore Implementation and Proof Effort in Lines of Code
with KVM in the v4.18 Linux kernel, which involved modi-
fying or adding roughly 15K lines of code (LOC) across both
KCore and KServ. Most of the added code was 10.1K LOC
in KCore for the implementation of Ed25519 and AES from
the veriﬁed HACL* crypto library [74]. Other than HACL*,
KCore consisted of 3.8K LOC, of which 3.4K LOC was in C
and 0.4K was in Arm assembly. Table 1 shows the 3.8K LOC
categorized by the modules shown in Figure 2 (C+Asm).
We then retroﬁtted KVM in the v5.4 Linux kernel, which
involved reusing much of the same 15K LOC. Of the 15K
LOC, less than 100 LOC needed to be changed in KServ
going from v4.18 to v5.4, mostly to support installing and
initializing KCore on a different codebase before KCore
starts running in EL2. No code changes were required in
KCore in going from v4.18 to v5.4. These results indicate
that the changes needed to retroﬁt a widely-used, commodity
hypervisor so it can be veriﬁed and integrated with multiple
versions of a commodity host kernel were modest overall.
We veriﬁed all of KCore’s C and assembly code. Table 1
shows the LOC in Coq for proving the correctness of KCore’s
code, categorized by the modules shown in Figure 2. The
proof effort for each module consists of writing the Coq
speciﬁcations (Spec), code proofs (Code) to verify the C and
assembly code reﬁnes the Coq speciﬁcations, and layer reﬁne-
ments (Reﬁne) to verify at each layer the implementation on
the underlay interface reﬁnes the overlay interface, thereby
linking the layers together to reﬁne the top-level speciﬁcation.
Some modules required much more manual effort than
others. For the speciﬁcations, the LOC for the Exit Handler
module is higher than other modules because it includes the
top layer TrapHandler speciﬁcation that encompasses all
of KCore’s behavior. For code proofs, the LOC for the VCPU
module is higher than other modules because it has both
loops and assembly code. This is because we used automated
reasoning to reduce manual effort, but our methods do not
support automating loop veriﬁcation or assembly code. For
layer reﬁnement, the LOC for the MMU PT proof is higher
than other modules because reﬁning the multi-level page
table implementation to a ﬂat map speciﬁcation was the most
complex reﬁnement proof.
Table 1 also shows all of the resulting code in Coq for
3964    30th USENIX Security Symposium
USENIX Association
code proofs (CodeAll) and layer reﬁnement (ReﬁneAll), by
adding automatically generated LOC to the manually written
LOC. For some modules, the use of automated reasoning
signiﬁcantly simpliﬁed the manual effort, such as for the
code proofs for the MMU PT, SMMU PT, and SMMU modules.
However, we did not apply automated reasoning uniformly
for all modules because different parts of the system were
veriﬁed by different authors who took different approaches.
For example, we did not use Coq tactics to automate the
proofs for the Lock module, resulting in more LOC for its
code proofs, but this could have been done. While automated
tools helped signiﬁcantly with code proofs, they did not help
much with layer reﬁnement, as shown by comparing the
manually written versus total LOC for each in Table 1.
In addition to the Coq code for proving the correctness
of each module, we implemented the machine model and
proved the security guarantees in Coq. 1.8K LOC were used
to implement AbsMachine, which models the multiprocessor
hardware behaviors including multi-level page tables for the
MMU and SMMU, TLBs, and write-back caches with bypass
support. AbsMachine primitives used by higher layers were
passed through to those layers then veriﬁed as part of each
layer. The security proofs, including the invariant and non-
interference proofs, consist of 4.8K LOC. Roughly 1K LOC
were used to verify the isolation invariants mentioned in Sec-
tion 4.6 for the MMU and SMMU page tables. The rest of the
3.8K LOC were noninterference proofs for KCore’s top-level
primitives; for example, these proofs involved proving state
indistinguishability with respect to caches. We did not link
HACL’s F* proofs with our Coq proofs, or our Coq proofs
for C code with those for Arm assembly code. The latter
requires a veriﬁed compiler for Arm multiprocessor code; no
such compiler exists. No changes were required to the proofs
used to verify KVM in the Linux kernel v4.18 versus v5.4.
6 Performance
We quantify the performance of SeKVM against unmodiﬁed
KVM as well as HypSec highlighting how a commodity
hypervisor with a veriﬁed TCB performs against unveriﬁed
versions. All experiments were run on a 64-bit Armv8 AMD
Seattle (Rev.B0) server with 8 Cortex-A57 CPU cores, 16 GB
of RAM, a 512 GB SATA3 HDD for storage, an AMD
10 GbE (AMD XGBE) NIC device. The hardware we used
supports Arm VE, but not VHE [21, 22]. For client-server
experiments, the clients ran on an x86 machine with 24 Intel
Xeon CPU 2.20 GHz cores and 96 GB RAM. The clients and
the server communicated via a 10 GbE network connection.
To provide comparable measurements across the systems,
we kept the software environments across all platforms the
same as much as possible. We tested unmodiﬁed KVM,
HypSec, and SeKVM based on two different versions of
mainline Linux, 4.18.0 and 5.4.0, both with QEMU 2.3.50.
VMs used the same kernel version as the host, and all hosts
Name
Kernbench Compilation of the Linux 4.9 kernel using allnoconfig
Description
Hackbench
Netperf
Apache
for Arm with GCC 5.4.0.
hackbench [56] using Unix domain sockets and 100
process groups running in 500 loops.
netperf v2.6.0 [41] running netserver on the server
and the client with its default parameters in three
modes: TCP_STREAM (throughput), TCP_MAERTS
(throughput), and TCP_RR (latency).
Apache v2.4.18 Web server running ApacheBench [1]
v2.3 on the remote client, which measures number of
handled requests per second when serving the 41 KB
index.html ﬁle of the GCC 4.4 manual using 100
concurrent requests.
Memcached memcached v1.4.25 using the memtier benchmark v1.2.3
MySQL
with its default parameters.
running SysBench
MySQL v14.14 (distrib 5.7.26)
v.0.4.12 using the default conﬁguration with 200 parallel
transactions.
Table 2: Application Benchmarks
and VMs ran Ubuntu 16.04.06. We modiﬁed virtio front-end
drivers in the VM kernel on SeKVM and HypSec to use
the GRANT_MEM and REVOKE_MEM hypercalls to enable shared
memory communication with back-end drivers in KServ.
All VMs used paravirtualized I/O (virtio), typical of cloud
infrastructure deployments such as Amazon EC2.
We ran benchmarks in each VM and compared their perfor-
mance to native hardware. Each native or VM instance was
conﬁgured as a 4-way SMP with 12 GB of RAM to provide
a common basis for comparison. Speciﬁcally, we used the
following conﬁgurations: (1) native Linux capped at 4 cores
and 12 GB RAM, and (2) a VM using KVM with 8 cores and
16 GB RAM, with the VM capped at 4 virtual CPUs (VCPUs)
and 12 GB RAM. We measured multi-core conﬁgurations to
reﬂect real-world server deployments. For VMs, we pinned
each VCPU to a speciﬁc physical CPU (PCPU) and ensured
that no other work was scheduled on that PCPU [20,21,49,50].
For client-server benchmarks, the clients ran natively on
Linux and used the full hardware available.
We ran real application workloads to compare SeKVM with
HypSec and unmodiﬁed KVM. Table 2 lists the workloads, a
mix of widely-used CPU and I/O intensive benchmarks. For
the v4.18 conﬁguration, we compared the following ﬁve sys-
tem conﬁgurations with HypSec: (1) Native unmodiﬁed Linux
host kernel without Full Disk Encryption (FDE), (2) Unmodi-
ﬁed KVM and guest kernel with FDE (KVM), (3) HypSec and
paravirtualized guest kernel with FDE (HypSec), (4) SeKVM
and paravirtualized guest kernel with FDE (SeKVM),
(5) SeKVM and paravirtualized guest kernel with FDE and
TLB ﬂushes during world switches (SeKVM-TLB-FLUSH).
We compared VM performance with FDE to bare-metal
execution without FDE, to conservatively quantify the
performance overhead in the presence of end-to-end I/O
protection. We also compared the performance of SeKVM
versus SeKVM while ﬂushing all entries from the TLB in
USENIX Association
30th USENIX Security Symposium    3965
Figure 5: Application Benchmark Performance - Linux v4.18
Figure 7: Multi-VM Performance with Hackbench
KServ result in more frequent TLB ﬂushes. This comparison
quantiﬁes the cost of not modeling a tagged TLB, which
would force TLB ﬂushes on each world switch to ensure
correctness. Our measurements show that this can result in
an additional 70% overhead for some application workloads
such as Memcached compared to using a tagged TLB as is
standard practice for commodity hypervisors.