title:Prime+Abort: A Timer-Free High-Precision L3 Cache Attack using
Intel TSX
author:Craig Disselkoen and
David Kohlbrenner and
Leo Porter and
Dean M. Tullsen
Prime+Abort: A Timer-Free High-Precision  
L3 Cache Attack using Intel TSX
Craig Disselkoen, David Kohlbrenner, Leo Porter, and Dean Tullsen,  
University of California, San Diego
https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/disselkoen
This paper is included in the Proceedings of the 26th USENIX Security SymposiumAugust 16–18, 2017 • Vancouver, BC, CanadaISBN 978-1-931971-40-9Open access to the Proceedings of the 26th USENIX Security Symposium is sponsored by USENIXPRIME+ABORT: A Timer-Free High-Precision L3 Cache Attack
using Intel TSX
Craig Disselkoen, David Kohlbrenner, Leo Porter, and Dean Tullsen
University of California, San Diego
{cdisselk, dkohlbre}@cs.ucsd.edu, PI:EMAIL, PI:EMAIL
Abstract
Last-Level Cache (LLC) attacks typically exploit tim-
ing side channels in hardware, and thus rely heavily
on timers for their operation. Many proposed defenses
against such side-channel attacks capitalize on this re-
liance. This paper presents PRIME+ABORT, a new cache
attack which bypasses these defenses by not depending
on timers for its function. Instead of a timing side chan-
nel, PRIME+ABORT leverages the Intel TSX hardware
widely available in both server- and consumer-grade pro-
cessors. This work shows that PRIME+ABORT is not
only invulnerable to important classes of defenses, it
also outperforms state-of-the-art LLC PRIME+PROBE
attacks in both accuracy and efﬁciency, having a max-
imum detection speed (in events per second) 3× higher
than LLC PRIME+PROBE on Intel’s Skylake architecture
while producing fewer false positives.
1
Introduction
State-of-the-art cache attacks [35, 7, 11, 21, 25, 29, 33,
34, 43] leverage differences in memory access times be-
tween levels of the cache and memory hierarchy to gain
insight into the activities of a victim process. These at-
tacks require the attacker to frequently perform a series
of timed memory operations (or cache management oper-
ations [7]) to learn if a victim process has accessed a crit-
ical address (e.g., a statement in an encryption library).
These attacks are highly dependent on precise and ac-
curate timing, and defenses can exploit this dependence.
In fact, a variety of defenses have been proposed which
undermine these timing-based attacks by restricting ac-
cess to highly precise timers [15, 27, 31, 39].
In this work, we introduce an alternate mechanism for
performing cache attacks, which does not leverage tim-
ing differences (timing side channels) or require timed
operations of any type.
Instead, it exploits Intel’s im-
plementation of Hardware Transactional Memory, which
is called TSX [19]. We demonstrate a novel cache
attack based on this mechanism, which we will call
PRIME+ABORT.
The intent of Transactional Memory (and TSX) is to
both provide a simpliﬁed interface for synchronization
and to enable optimistic concurrency: processes abort
only when a conﬂict exists, rather than when a poten-
tial conﬂict may occur, as with traditional locks [14, 12].
Transactional memory operations require transactional
data to be buffered, in this case in the cache which has
limited space. Thus, the outcome of a transaction de-
pends on the state of the cache, potentially revealing in-
formation to the thread that initiates the transaction. By
exploiting TSX, an attacker can monitor the cache behav-
ior of another process and receive an abort (call-back) if
the victim process accesses a critical address. This work
demonstrates how TSX can be used to trivially detect
writes to a shared block in memory; to detect reads and
writes by a process co-scheduled on the same core; and,
most critically, to detect reads and writes by a process
executing anywhere on the same processor. This latter
attack works across cores, does not assume that the vic-
tim uses or even knows about TSX, and does not require
any form of shared memory.
The advantages of this mechanism over conven-
tional cache attacks are twofold.
is that
PRIME+ABORT does not leverage any kind of timer;
as mentioned, several major classes of countermeasures
against cache attacks revolve around either restricting ac-
cess or adding noise to timers. PRIME+ABORT effec-
tively bypasses these countermeasures.
The ﬁrst
The second advantage is in the efﬁciency of the attack.
The TSX hardware allows for a victim’s action to directly
trigger the attacking process to take action. This means
the TSX attack can bypass the detection phase required
in conventional attacks. Direct coupling from event to
handler allows PRIME+ABORT to provide over 3× the
throughput of comparable state-of-the-art attacks.
The rest of this work is organized as follows. Sec-
USENIX Association
26th USENIX Security Symposium    51
tion 2 presents background and related work; Section 3
introduces our novel attack, PRIME+ABORT; Section 4
describes experimental results, making comparisons with
existing methods;
in Section 5, we discuss potential
countermeasures to our attack; Section 7 concludes.
tion. Nonetheless, fundamentally, all of these existing at-
tacks can only gain temporal information at the waiting-
interval granularity.
2 Background and Related Work
2.1.1 PRIME+PROBE
times.
2.1 Cache attacks
Cache attacks [35, 7, 11, 21, 25, 29, 33, 34, 43]
are a well-known class of side-channel attacks which
seek to gain information about which memory lo-
cations are accessed by some victim program, and
at what
In an excellent survey, Ge et
al. [4] group such attacks into three broad categories:
PRIME+PROBE, FLUSH+RELOAD, and EVICT+TIME.
Since EVICT+TIME is only capable of monitoring mem-
ory accesses at
the program granularity (whether a
given memory location was accessed during execution
or not), in this paper we focus on PRIME+PROBE and
FLUSH+RELOAD, which are much higher resolution and
have received more attention in the literature. Cache at-
tacks have been shown to be effective for successfully
recovering AES [25], ElGamal [29], and RSA [43] keys,
performing keylogging [8], and spying on messages en-
crypted with TLS [23].
Figure 1 outlines all of the attacks which we will con-
sider. At a high level, each attack consists of a pre-attack
portion, in which important architecture- or runtime-
speciﬁc information is gathered; and then an active por-
tion which uses that information to monitor memory ac-
cesses of a victim process. The active portion of exist-
ing state-of-the-art attacks itself consists of three phases:
an “initialization” phase, a “waiting” phase, and a “mea-
surement” phase. The initialization phase prepares the
cache in some way; the waiting phase gives the victim
process an opportunity to access the target address; and
then the measurement phase performs a timed operation
to determine whether the cache state has changed in a
way that implies an access to the target address has taken
place.
Speciﬁcs of the initialization and measurement phases
vary by cache attack (discussed below). Some cache at-
tack implementations make a tradeoff in the length of the
waiting phase between accuracy and resolution—shorter
waiting phases give more precise information about the
timing of victim memory accesses, but may increase
the relative overhead of the initialization and measure-
ment phases, which may make it more likely that a vic-
tim access could be “missed” by occurring outside of
one of the measured intervals.
In our testing, not all
cache attack implementations and targets exhibited ob-
vious experimental tradeoffs for the waiting phase dura-
PRIME+PROBE [35, 21, 25, 34, 29] is the oldest and
largest family of cache attacks, and also the most general.
PRIME+PROBE does not rely on shared memory, unlike
most other cache attacks (including FLUSH+RELOAD
and its variants, described below). The original form of
PRIME+PROBE [35, 34] targets the L1 cache, but recent
work [21, 25, 29] extends it to target the L3 cache in In-
tel processors, enabling PRIME+PROBE to work across
cores and without relying on hyperthreading (Simultane-
ous Multithreading [38]). Like all L3 cache attacks, L3
PRIME+PROBE can detect accesses to either instructions
or data; in addition, L3 PRIME+PROBE trivially works
across VMs.
PRIME+PROBE targets a single cache set, detecting
accesses by any other program (or the operating system)
to any address in that cache set. In its active portion’s ini-
tialization phase (called “prime”), the attacker accesses
enough cache lines from the cache set so as to completely
ﬁll the cache set with its own data. Later, in the mea-
surement phase (called “probe”), the attacker reloads the
same data it accessed previously, this time carefully ob-
serving how much time this operation took. If the victim
did not access data in the targeted cache set, this oper-
ation will proceed quickly, ﬁnding its data in the cache.
However, if the victim accessed data in the targeted cache
set, the access will evict a portion of the attacker’s primed
data, causing the reload to be slower due to additional
cache misses. Thus, a slow measurement phase implies
the victim accessed data in the targeted cache set during
the waiting phase. Note that this “probe” phase can also
serve as the “prime” phase for the next repetition, if the
monitoring is to continue.
Two different kinds of initial one-time setup are re-
quired for the pre-attack portion of this attack. The ﬁrst
is to establish a timing threshold above which the mea-
surement phase is considered “slow” (i.e. likely suffering
from extra cache misses). The second is to determine a
set of addresses, called an “eviction set”, which all map
to the same (targeted) cache set (and which reside in dis-
tinct cache lines). Finding an eviction set is much easier
for an attack targeting the L1 cache than for an attack tar-
geting the L3 cache, due to the interaction between cache
addressing and the virtual memory system, and also due
to the “slicing” in Intel L3 caches (discussed further in
Sections 2.2.1 and 2.2.2).
52    26th USENIX Security Symposium
USENIX Association
Figure 1: Comparison of the operation of various cache attacks, including our novel attacks.
2.1.2 FLUSH+RELOAD
class
cache
attacks
other major
is
of
The
FLUSH+RELOAD [7, 11, 43].
FLUSH+RELOAD
targets a speciﬁc address, detecting an access by any
other program (or the operating system) to that exact
address (or another address in the same cache line). This
makes FLUSH+RELOAD a much more precise attack
than PRIME+PROBE, which targets an entire cache set
and is thus more prone to noise and false positives.
FLUSH+RELOAD also naturally works across cores
because of shared, inclusive, L3 caches (as explained
in Section 2.2.1). Again,
like all L3 cache attacks,
FLUSH+RELOAD can detect accesses to either instruc-
tions or data. Additionally, FLUSH+RELOAD can work
across VMs via the page deduplication exploit [43].
The pre-attack of FLUSH+RELOAD,
like that of
PRIME+PROBE, involves determining a timing thresh-
old, but is limited to a single line instead of an entire
“prime” phase. However, FLUSH+RELOAD does not re-
quire determining an eviction set.
Instead, it requires
the attacker to identify an exact target address; namely,
an address in the attacker’s virtual address space which
maps to the physical address the attacker wants to mon-
itor. Yarom and Falkner [43] present two ways to do
this, both of which necessarily involve shared memory;
one exploits shared libraries, and the other exploits page
deduplication, which is how FLUSH+RELOAD can work
across VMs. Nonetheless, this step’s reliance on shared
memory is a critical weakness in FLUSH+RELOAD, lim-
iting it to only be able to monitor targets in shared mem-
ory.
In FLUSH+RELOAD’s initialization phase, the attacker
“ﬂushes” the target address out of the cache using Intel’s
CLFLUSH instruction. Later, in the measurement phase,
the attacker “reloads” the target address (by accessing
it), carefully observing the time for the access.
If the
access was “fast”, the attacker may conclude that another
program accessed the address, causing it to be reloaded
into the cache.
of
An
variant
improved
FLUSH+RELOAD,
FLUSH+FLUSH [7], exploits timing variation in the
CLFLUSH instruction itself;
this enables the attack to
combine its measurement and initialization phases,
much like PRIME+PROBE.
A different variant,
EVICT+RELOAD [8], performs the initialization phase
by evicting the cacheline with PRIME+PROBE’s “prime”
phase, allowing the attack to work without the CLFLUSH
instruction at all—e.g., when the instruction has been
disabled, as in Google Chrome’s NaCl [6].
USENIX Association
26th USENIX Security Symposium    53
PRIME+PROBEFLUSH+RELOADFLUSH+FLUSHEVICT+RELOADNAÏVETSX-BASEDPRIME+ABORTEstablish timing thresholdTiming thresholdTiming thresholdTiming thresholdTiming threshold--Target acquisitionEviction setShared memShared memEviction set AND shared memShared memEviction setInitializationPrime targeted setFlush targeted addressFlush targeted addressPrime targeted setStart transactionStart transactionAccess targeted addressPrime targeted setWait…WaitWaitWaitWaitWaitWait(Victim makes access)Wait…Start timerStart timerStart timerStart timerStart timerMeasurement operationPrime targeted setAccess targeted addressFlush targeted addressAccess targeted addressStop timerStop timerStop timerStop timerStop timerRepeat(to Wait)(to Initialization)(to Wait)(to Initialization)(to Initialization)(to Initialization)Detect victim access ifTime > thresholdTime  thresholdTime < thresholdAbort status indicates Cause #6Abort status indicates Cause #7 or #8Pre-AttackActive AttackTransaction abortsTransaction abortsAnalysisInitializationWaitingMeasurementBypass “Measurement”2.1.3 Timer-Free Cache Attacks
All of the attacks so far discussed—PRIME+PROBE,
FLUSH+RELOAD, and variants—are still fundamentally
timing attacks, exploiting timing differences as their un-
derlying attack vector. One recent work which, like
this work, proposes a cache attack without reference to
timers is that of Guanciale et al. [10]. Instead of timing
side channels, Guanciale et al. rely on the undocumented
hardware behavior resulting from disobeying ISA pro-
gramming guidelines, speciﬁcally with regards to virtual
address aliasing and self-modifying code. However, they
demonstrate their attacks only on the ARM architecture,
and they themselves suggest that recent Intel x86-64 pro-
cessors contain mechanisms that would render their at-
tacks ineffective. In contrast, our attack exploits weak-
nesses speciﬁcally in recent Intel x86-64 processors, so
in that respect our attack can be seen as complementary
to Guanciale et al.’s work. We believe that our work, in
addition to utilizing a novel attack vector (Intel’s hard-
ware transactional memory support), is the ﬁrst timer-
free cache attack to be demonstrated on commodity Intel
processors.
2.2 Relevant Microarchitecture
2.2.1 Caches
[Basic Background] Caches in modern processors store
data that is frequently or recently used, in order to reduce
access time for that data on subsequent references. Data
is stored in units of “cache lines” (a ﬁxed architecture-
dependent number of bytes). Caches are often orga-
nized hierarchically, with a small but fast “L1” cache, a
medium-sized “L2” cache, and a large but comparatively
slower “L3” cache. At each level of the hierarchy, there
may either be a dedicated cache for each processor core,
or a single cache shared by all processor cores.
Commonly, caches are “set-associative” which allows
any given cacheline to reside in only one of N locations
in the cache, where N is the “associativity” of the cache.
This group of N locations is called a “cache set”. Each
cacheline is assigned to a unique cache set by means of
its “set index”, typically a subset of its address bits. Once
a set is full (the common case) any access to a cacheline
with the given set index (but not currently in the cache)
will cause one of the existing N cachelines with the same
set index to be removed, or “evicted”, from the cache.
[Intel Cache Organization] Recent Intel processors
contain per-core L1 instruction and data caches, per-core
uniﬁed L2 caches, and a large L3 cache which is shared
across cores. In this paper we focus on the Skylake ar-
chitecture which was introduced in late 2015; important
Skylake cache parameters are provided in Table 1.
Table 1: Relevant cache parameters in the Intel Skylake
architecture.
L1-Data
32 KB
8-way
Per-core
64 B
L1-Inst
32 KB
8-way
Per-core
64 B
Size
Assoc
Sharing
Line size
1 depending on model. This range covers all Skylake processors
(server, desktop, mobile, embedded) currently available as of Jan-
uary 2017 [20].
L3
2-8 MB1
16-way
Shared
64 B
L2
256 KB
4-way
Per-core
64 B
[Inclusive Caches] Critical to all cross-core cache at-
tacks, the L3 cache is inclusive, meaning that every-
thing in all the per-core caches must also be held in
the L3. This has two important consequences which
are key to enabling both L3-targeting PRIME+PROBE
and FLUSH+RELOAD to work across cores. First, any
data accessed by any core must be brought into not only
the core’s private L1 cache, but also the L3.
If an at-
tacker has “primed” a cache set in the L3, this access to