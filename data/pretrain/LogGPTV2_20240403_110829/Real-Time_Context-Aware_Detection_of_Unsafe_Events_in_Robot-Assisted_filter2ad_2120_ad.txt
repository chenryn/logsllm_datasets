3000-6000
6000-65000
3000-6000
6000-65000
3000-6000
6000-65000
3000-6000
6000-65000
3000-6000
6000-65000
3000-6000
6000-65000
3000-6000
6000-65000
0.50-0.60
0.70-0.90
0.50-0.60
0.70-0.90
0.50-0.60
0.70-0.90
0.50-0.60
0.70-0.90
0.50-0.60
0.70-0.90
0.50-0.60
0.70-0.90
0.50-0.60
0.70-0.90
# Errors (%)
Dropoff
Failure
16 (100%)
16 (100%)
16 (100%)
15 (93.75%)
15 (93.75%)
16 (100%)
6 (75%)
6 (75%)
Block-drop
0 (0%)
1 (12.50%)
0 (0%)
1 (12.50%)
0 (0%)
0 (0%)
28 (48.28%)
33 (66%)
5 (62.50%)
6 (75%)
46 (95.78%)
67 (86.49%)
12 (75.00%)
12 (75.00%)
40 (97.57%)
58 (95.08%)
14 (87.50%)
14 (87.50%)
6 (85.71%)
17 (100%)
16 (100%)
16 (100%)
16
8
16
16
16
8
16
16
16
8
16
16
58
50
16
16
47
74
16
16
41
61
16
16
7
17
16
16
651
Total Fault Injections
106
TABLE III: Fault injection experiments on the Raven II
392
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:23:52 UTC from IEEE Xplore.  Restrictions apply. 
390
Automated Labeling of Errors: We used computer vision
approaches as an orthogonal method of detecting errors,
as our fault injections were performed on the kinematics
state variables. This, along with the knowledge of when
a fault was injected, provided us the semantics to label a
particular gesture as erroneous or non-erroneous. We adopted
the marker-based (color and contour) detection approaches
used in [19] here. As the ﬁrst step, we converted the logged
video data to a sequence of frames (Figure 7a) with their
corresponding timestamps. For the case of detecting Block-
drop, we used Structural Similarity Index (SSIM) [43] on
thresholded images (Figure 7b) of the block to ﬁnd the exact
frame (and the timestamp) of when the failure happened. For
the case of detecting Drop-off failure, we applied the same
HSV threshold, followed by contour detection (Figure 7c) to
detect the contour of the block and track its centroid through-
out the trajectory. We collected the trace of the centroid
for the fault-free trajectories which we used as reference to
compare against faulty trajectories. We used Dynamic Time
Warping to compare the fault-free and faulty trajectory traces
and checked for large deviations that indicate when the block
should have been dropped, but it was not (Figure 7d).
Gesture Annotation: For annotating the data generated
using the Gazebo simulator, we extended the data structure
of the Raven II to include the current surgical gesture. This
allows the human operator to record the surgical gesture as
(s)he is simultaneously operating the robot, reducing the time
and effort to look at videos and performing the annotations.
For labeling the erroneous gestures, we recorded the time that
we injected the fault to one of the kinematics state variables
and the time that the fault led to any of the common errors in
Table II based on the video data and then mapped those times
to the corresponding gestures. As a result, we were able to
automate our gesture and erroneous gesture annotations for
all experiments conducted in the Gazebo simulator. A total
of 890 out of 4557 gestures were labeled as erroneous.
(a) Video Frames
(b) HSV Threshold of
block
the
(c) Contour Detection
Comparison
(d)
traces, adopted from [19]
between
Fig. 7: Failure detection using contour segmentation and DTW
391
C. Metrics
We evaluated the individual components as well as the
entire pipeline of our safety monitoring system in terms
of accuracy and timeliness in identifying gestures and
detecting errors using the metrics that are described next.
Individual Components: We trained individual compo-
nents of the pipeline, namely the gesture classiﬁcation and
the erroneous gesture detection, separately. For the ﬁrst part
of the pipeline, our evaluation metrics were classiﬁcation
accuracy, for assessing model performance across different
gesture classes, and jitter value, for identifying the timeliness
of the classiﬁcation. Jitter is calculated as the difference
between the time our model detects a gesture and its actual
occurrence, with positive values indicating early detection.
Our evaluations of the second part of the pipeline were
based on the standard metrics used for binary classiﬁcation:
True Positive Rate (TPR), True Negative Rate (TNR), Pos-
itive Predictive Value (PPV), and Negative Predictive Value
(NPV), and the Area Under the ROC Curve (AUC) of the
anomaly class. We reported the micro-averages for all the
metrics unless stated otherwise.
Overall Pipeline: For evaluating the classiﬁcation perfor-
mance of the overall pipeline, we used the F1-score as well
as the AUC of the negative class. In our case, it is imperative
to not classify any erroneous gestures as non-erroneous (to
not miss any anomalies), while keeping the False Positive
Rate (FPR) low. The F1-score, which is the harmonic mean
of precision and recall, is a good indicator of how the model
performs in detecting or not missing erroneous gestures. At
the same time, it only reports the performance of the model
using one particular threshold. As F1-score is a point-based
metric, we also used AUC of ROC curves, which reports the
performance over different classiﬁcation thresholds.
Our metrics for assessing the timeliness of error detection
were average computation time for the classiﬁers and reac-
tion time, deﬁned as the time to react on the advent of an
erroneous gesture and calculated as the difference between
the actual time of error occurrence and the time it is detected:
reactiont = actualt − detectedt
(4)
The reaction time can be used as a measure of the time
budget that we have for taking any corrective actions to pre-
vent potential safety-critical events. A positive value means
that our model can predict an error before its occurrence
(early detection) whereas a negative value indicates the
detection of error after it has already happened (detection
delay). As shown in Case 1 in Figure 8, our classiﬁer predicts
every kinematics sample as erroneous or non-erroneous.
So there might be cases where different parts within the
same gesture are classiﬁed as erroneous or non-erroneous.
The reaction time is calculated based on the ﬁrst time an
erroneous sample is detected within a gesture.
We also report the percentage of times that the erroneous
gestures were detected before their actual occurrence (%
Early Detection in Table VIII). To calculate this, we divided
the total number of times when the reaction time was positive
by the total number of erroneous gesture occurrences.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:23:52 UTC from IEEE Xplore.  Restrictions apply. 
(cid:13)(cid:23)(cid:30)(cid:30)(cid:21)(cid:28)
(cid:16)(cid:21)(cid:18)(cid:19)(cid:30)(cid:23)(cid:26)(cid:25)(cid:1)(cid:17)(cid:23)(cid:24)(cid:21)
(cid:11)(cid:28)(cid:28)(cid:26)(cid:25)(cid:21)(cid:26)(cid:31)(cid:29)
(cid:14)(cid:26)(cid:25)(cid:4)(cid:21)(cid:28)(cid:28)(cid:26)(cid:25)(cid:21)(cid:26)(cid:31)(cid:29)
(cid:15)(cid:28)(cid:21)(cid:20)(cid:23)(cid:19)(cid:30)(cid:21)(cid:20)
(cid:10)(cid:18)(cid:29)(cid:21) (cid:6)
(cid:15)(cid:28)(cid:21)(cid:20)(cid:23)(cid:19)(cid:30)(cid:21)(cid:20)
(cid:10)(cid:18)(cid:29)(cid:21) (cid:7)
(cid:12)(cid:28)(cid:26)(cid:31)(cid:25)(cid:20)(cid:1)
(cid:17)(cid:28)(cid:31)(cid:30)(cid:22)
(cid:5)
(cid:1)(cid:4)
(cid:1)(cid:4)
(cid:1)(cid:4)
(cid:1)(cid:3)(cid:4)
(cid:1)(cid:3)(cid:4)
(cid:1)(cid:3)(cid:4)
(cid:6)(cid:5)
(cid:1)(cid:6)
(cid:1)(cid:6)
(cid:1)(cid:5)
(cid:1)(cid:5)
(cid:1)(cid:6)
(cid:1)(cid:5)
(cid:1)(cid:3)(cid:3)
(cid:1)(cid:3)(cid:3)
(cid:1)(cid:3)(cid:3)
(cid:7)(cid:5)
(cid:8)(cid:5)
(cid:9)(cid:5)
(cid:17)(cid:23)(cid:24)(cid:21)(cid:29)(cid:30)(cid:21)(cid:27)(cid:29)(cid:1)(cid:2)(cid:29)(cid:3)
Fig. 8: Example Timeline for Detecting Anomalies
A. Performance of Pipeline Components
V. RESULTS
Gesture Segmentation and Classiﬁcation: All our results
are averaged across the 5 trials of LOSO setup. Table IV
shows the accuracy of our best performing model for all the
tasks in the JIGSAWS dataset compared to two state-of-the-
art supervised learning models that only rely on kinematics
data, [44] and [45]. In addition, we also evaluated our model
for the Block Transfer task on the Raven II. Our best per-
forming model was a 2 layer stacked LSTM, with input time-
step of 1, comprising of 512 and 96 LSTM units respectively,
followed by a fully-connected layer with 64 units and a
ﬁnal softmax layer. For the tasks in the JIGSAWS dataset,
the input to the model were all the 38 kinematics features
from the robot manipulators. For the Block Transfer task
on Raven II, we used the same LSTM architecture but the
input to our model was the Cartesian Positions and Grasper
Angles for each of the manipulators. [44] used a variation of
the Skip-Chain Conditional Random Fields (SC-CRF) that
can better capture transitions between gestures over longer
periods of frames. [45] introduced Shared Discriminative
Sparse Dictionary Learning (SDSDL) that aims to jointly
learn a common dictionary for all gestures in an unsupervised
manner together with the parameters of a multi-class linear
support vector machine (SVM). For Suturing, our gesture
classiﬁer achieved competitive average accuracy of 84.49%
on the test data. For Block Transfer, which has more training
data and is a simpler task with no recurrence of gestures, our
model achieved an accuracy of 95.16%.
Table IX shows that for the Suturing task our model de-
tected the gestures within a jitter value of 337 ms, performing
best for G2, G3, G4, and G6 with over 80% accuracy and
worst for G10. Our model was unable to detect G10 which
is ”Loosening more suture” as it does not occur frequently
(see Fig. 3a), with only 1% of transition probability from G6
and 13% transition probability from G4. In addition, as seen
in Table II, there were no common errors in G10.
Erroneous Gesture Detection: We trained our erroneous
gesture detection system on individual gestures, assuming
Knot Tying
Needle Passing
Block Transfer
Method
This work
SC-CRF [44]
SDSDL [45]
Training size
Number of Trajectories
Suturing
84.49 %
85.24 %
86.32 %
102,698
39
81.69 %
80.64 %
82.54 %
44,512
28
69.34 %
77.47 %
74.88 %
66,914
36
95.16 %
N/A
N/A
115
4,197,988