### 4.3 Q3. Parameter Sensitivity

Precog requires the tuning of certain hyper-parameters, such as the R² score and critical time, which are currently set manually based on expert knowledge. Figure 5 compares the performance for different parameter values on a synthetically generated dataset. Our algorithm performs consistently well across various values. Setting the minimum R² score above 0.8 corresponds to a stricter fitting of the line, which is why the accuracy drops. On the other hand, our data mostly contains trend lines that would reach the threshold within 3 to 4 days. Therefore, setting the minimum critical time too low (less than 3 days) would mean the trend line never reaches the threshold within the given timeframe, thus decreasing the accuracy. These experiments show that these parameters do play a role in the overall accuracy of the algorithm, but the algorithm is generally insensitive to them at most values. 

Furthermore, efforts to determine these parameters automatically based on historical data are in progress but are out of the scope of this paper.

### 5. Conclusion

Memory leak detection has been a research topic for over a decade. Many approaches have been proposed to detect memory leaks, with most focusing on the internals of the application or the object's allocation and deallocation. The Precog algorithm for memory leak detection, presented in this work, is particularly relevant for cloud-based infrastructures where cloud administrators do not have access to the source code or know about the internals of the deployed applications. Performance evaluation results showed that Precog can achieve an F1-Score of 0.85 with a prediction time of less than half a second on real workloads. This algorithm can also be useful in Serverless Computing, where if a function is leaking memory, successive function invocations will add to it, resulting in a larger memory leak on the underlying system. Precog, running on the underlying system, can detect such cases.

Future work includes developing online learning-based approaches for detection and using other metrics like CPU, network, and storage utilization to further enhance the accuracy of the algorithms and provide higher confidence in the detection results.

### Acknowledgements

This work was supported by funding from the German Federal Ministry of Education and Research (BMBF) under the Software Campus program. The authors also thank the anonymous reviewers whose comments helped improve this paper.

### References

1. Ataallah, S.M.A., Nassar, S.M., Hemayed, E.E.: Fault tolerance in cloud computing - survey. In: 2015 11th International Computer Engineering Conference (ICENCO), pp. 241–245, December 2015. https://doi.org/10.1109/ICENCO.2015.7416355
2. Chen, K., Chen, J.: Aspect-based instrumentation for locating memory leaks in Java programs. In: 31st Annual International Computer Software and Applications Conference (COMPSAC 2007), vol. 2, pp. 23–28, July 2007. https://doi.org/10.1109/COMPSAC.2007.79
3. Clause, J., Orso, A.: LeakPoint: pinpointing the causes of memory leaks. In: 2010 ACM/IEEE 32nd International Conference on Software Engineering, vol. 1, pp. 515–524, May 2010. https://doi.org/10.1145/1806799.1806874
4. Gokhroo, M.K., Govil, M.C., Pilli, E.S.: Detecting and mitigating faults in cloud computing environment. In: 2017 3rd International Conference on Computational Intelligence Communication Technology (CICT), pp. 1–9, February 2017. https://doi.org/10.1109/CIACT.2017.7977362
5. Jain, N., Choudhary, S.: Overview of virtualization in cloud computing. In: 2016 Symposium on Colossal Data Analysis and Networking (CDAN), pp. 1–4, March 2016. https://doi.org/10.1109/CDAN.2016.7570950
6. Jump, M., McKinley, K.S.: Cork: dynamic memory leak detection for garbage-collected languages. In: Proceedings of the 34th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL 2007, New York, NY, USA, pp. 31–38. ACM (2007). https://doi.org/10.1145/1190216.1190224. http://doi.acm.org/10.1145/1190216.1190224
7. Mitchell, N., Sevitsky, G.: LeakBot: an automated and lightweight tool for diagnosing memory leaks in large Java applications. In: Cardelli, L. (ed.) ECOOP 2003. LNCS, vol. 2743, pp. 351–377. Springer, Heidelberg (2003). https://doi.org/10.1007/978-3-540-45070-2_16
8. Pooja, Pandey, A.: Impact of memory-intensive applications on the performance of cloud virtual machines. In: 2014 Recent Advances in Engineering and Computational Sciences (RAECS), pp. 1–6, March 2014. https://doi.org/10.1109/RAECS.2014.6799629
9. Rudafshani, M., Ward, P.A.S.: LeakSpot: detection and diagnosis of memory leaks in JavaScript applications. Softw. Pract. Exper. 47(1), 97–123 (2017). https://doi.org/10.1002/spe.2406
10. Sor, V., Srirama, S.N.: A statistical approach for identifying memory leaks in cloud applications. In: CLOSER (2011)
11. Sor, V., Srirama, S.N.: Memory leak detection in Java: taxonomy and classification of approaches. J. Syst. Softw. 96, 139–151 (2014)
12. Sor, V., Srirama, S.N., Salnikov-Tarnovski, N.: Memory leak detection in Plumbr. Softw. Pract. Exper. 45, 1307–1330 (2015)
13. Vilk, J., Berger, E.D.: Bleak: automatically debugging memory leaks in web applications. In: Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI 2018, New York, NY, USA, pp. 15–29. ACM (2018). https://doi.org/10.1145/3192366.3192376. http://doi.acm.org/10.1145/3192366.3192376
14. Xie, Y., Aiken, A.: Context- and path-sensitive memory leak detection. SIGSOFT Softw. Eng. Notes 30(5), 115–125 (2005). https://doi.org/10.1145/1095430.1081728. http://doi.acm.org/10.1145/1095430.1081728

### Multi-source Anomaly Detection in Distributed IT Systems

**Jasmin Bogatinovski and Sasho Nedelkoski**

**Distributed Operating Systems, TU Berlin, Berlin, Germany**
{jasmin.bogatinovski, nedelkoski}@tu-berlin.de

#### Abstract

The multi-source data generated by distributed systems provide a holistic description of the system. Harnessing the joint distribution of different modalities by a learning model can be beneficial for critical applications in the maintenance of distributed systems. One such important task is anomaly detection, where we are interested in detecting deviations of the current behavior of the system from the theoretically expected. In this work, we utilize the joint representation from distributed traces and system log data for the task of anomaly detection in distributed systems. We demonstrate that the joint utilization of traces and logs produces better results compared to single modality anomaly detection methods. Furthermore, we formalize a learning task—Next Template Prediction (NTP)—that is used as a generalization for anomaly detection for both logs and distributed traces. Finally, we demonstrate that this formalization allows for the learning of template embeddings for both traces and logs. The joint embeddings can be reused in other applications as good initialization for spans and logs.

#### Keywords

Multi-source anomaly detection, Multi-modal, Logs, Distributed traces

#### 1. Introduction

The complexity of multi-layered IT infrastructures, such as the Internet of Things, distributed processing frameworks, databases, and operating systems, is constantly increasing [10]. To meet consumers' expectations of fluent service with low response times and high availability, service providers rely heavily on high volumes of monitoring data. The massive volumes of data lead to maintenance overhead for operators and require the introduction of data-driven tools to process the data.

A crucial task for such tools is to correctly identify the symptoms of deviation of the current system behavior from the expected one. Due to the large volumes of data, the anomaly detector should produce a small number of false-positive alarms, reducing the effort of the operators while maintaining a high detection rate. The benefit of timely detection allows for the prevention of potential failures and increases the opportunity window for a successful reaction from the operator. This is especially important if urgent expertise and/or administrative activity is required. Symptoms often appear whenever there are performance problems or system failures and usually manifest as fingerprints within the monitored data: logs, metrics, or distributed traces.

The monitored system data represent the state of the system at any time point. They are grouped into three categories—modalities: metrics, application logs, and distributed traces [12]. Metrics are time-series data representing the utilization of available resources and the status of the infrastructure. Typically, they involve measuring CPU, memory, and disk utilization, as well as data such as network throughput and service call latency. Application logs are print statements appearing in code with semi-structured content. They represent interactions between data, files, services, or applications, containing a rich representative structure at the service level. Service, microservices, and other systems generate logs composed of timestamped records. Distributed traces chain service invocations as workflows of execution of HTTP or RPC requests. Each part of the chain in the trace is called an event or span. A property of this type of data is that it preserves information about the execution graph at the (micro)service level, thus preserving the interplay between components.

Log data can produce a richer description at the service level since they are fingerprints of the program execution within the service. On the other hand, traces do not have much system-level information but preserve the overall graph of request execution. Referring to the different aspects of the system, logs and traces provide orthogonal information about the behavior of distributed systems. Building on this observation, in this work, we introduce a multi-source anomaly detection approach that can consider data from both traces and logs jointly. We demonstrate the usability of time-aligned log and tracing data to produce better results on the task of anomaly detection compared to single modality methods. The results show that the model built under the joint loss from both logs and trace data can exploit some relationship between the modalities. The approach is trainable end-to-end and does not require building separate models for each modality. As a second contribution, we introduce vector embeddings for the spans within the trace. The adopted approach allows defining the span vectors as pooling over the words they are composed of. We refer to these vector embeddings as span2vec.

#### 2. Related Work

The literature recognizes various approaches concerned with anomaly detection in distributed systems from single modalities. We review the single modality approaches for both logs and traces and provide an overview of existing multi-modal approaches, though none of them jointly considers both traces and logs.

The most common approaches for anomaly detection from log data roughly follow a two-step composition: log parsing followed by a method for anomaly detection. The first step allows for an appropriate log representation. One challenge during this procedure is the reduction of noise in the log data. This noise in a log message is present due to the various parameters parts of the log can take during execution. To this end, many techniques for log parsing have been proposed [3, 7, 14]. A detailed overview and comparison across benchmarks of these techniques are given in [18]. After template extraction, there are two general approaches to represent the logs. The first is based on word frequencies and metrics derived from the logs (e.g., TF-IDF) [2, 5, 15, 17] or reusing word representations of the logs based on corpora of words. The second approach aims to translate templates into sequences of templates—most often represented as sequences of integers or vectors. Such representation allows modeling the sequential execution of a program workflow. One of the most commonly utilized approaches is RNN-based (e.g., LSTM, GRU) [1]. They often are coupled with an additional mechanism such as attention to allow for better preservation of the semantic information inside the logs [6]. Depending on the data representation, various methods are utilized from both the supervised and unsupervised domains of machine learning. However, due to easier practical adoption and the absence of labels, unsupervised methods are preferred.

Available approaches for anomaly detection from tracing data are scarce. They usually model the normal execution of a workload, represented within the trace by utilizing the history \( h \) of recent trace events as input. They decompose the trace into its building blocks, the events/spans, and predict the next span in the sequence. Anomaly detection is done by imposing thresholds on the number of errors the LSTM makes for the corresponding trace predicted [9, 10]. Further approaches aim to capture the execution of a complete workload into a finite state automaton (FSA) [16]. However, FSA approaches are dependent on specific tracing implementation systems, and unification with other types of modalities, such as log data, due to the assumed homogeneous structure of the states building the FSA, is harder.

Several works on multi-modal learning for anomaly detection demonstrate the feasibility of using different modalities of data for anomaly detection [11, 13]. In the context of large-scale ICT systems, the authors in [10] consider the joint exploitation of traces and the corresponding response times of the spans within the trace. More specifically, a multi-modal LSTM-based method, trained jointly on both modalities, is introduced, showing that the shared information improves the anomaly detection scores. In [4], a Multimodal Variational Autoencoder approach is adopted for effectively learning relationships among cross-domain data, providing good results for anomaly detection built on logs and metrics as modalities. However, they do not preserve the information for the overall microservice architecture.

To the best of our knowledge, the literature does not yet recognize methods for joint consideration of logs and traces as fundamentally complementary data sources describing distributed IT systems. Hence, in this work, we propose an approach for how to jointly consider the complementary information within logs and traces.

#### 3. Multimodal Approach for Anomaly Detection from Heterogeneous Data

In this section, we describe the multi-source approach towards anomaly detection using logs and tracing data. First, we describe the logs and traces as generated by the system. We present their specifics that are exploited for the definition of the Next Template Prediction (NTP) pseudo-task. Second, we describe the NTP pseudo-task for anomaly detection. Thirdly, we describe one way to address the NTP task utilizing deep learning architecture on a single modality description of the system state. Next, we provide a solution that enables us to efficiently solve the NTP problem as a pseudo task for joint detection of anomalies from both logs and traces. Finally, we present an approach that uses the results from the NTP task and performs anomaly detection.

##### 3.1 Data Representation

The raw logs and traces as generated by the system contain various information about the specific operation being executed. Since some of the information is a sporadic description of the operations, proper filtering and representation should be done. Due to the specifics of the two modalities, we address them separately.

**Logs:** A log is a sequence of temporally ordered unstructured text messages \( L = \{ l_i : i=1,2,...\} \). Each text message \( l_i \) is generated by a logging instruction (e.g., `printf()`, `log.info()`) within the software source code. Since the logging function is part of the body of the whole program, it can serve as a proxy for the program execution workflow. Hence, one can infer the normal execution pattern within the program workflow.

Logs consist of a constant and a varying part, referred to as log templates and log parameters. Due to the large variability of the parameters, they can introduce a lot of noise. To mitigate this problem, a common way to represent the logs is through the extraction of the constant part via a log parsing procedure. This allows for the creation of a dictionary of log templates from a given set of logs. To unify the representations of the logs, the log templates are tokenized.