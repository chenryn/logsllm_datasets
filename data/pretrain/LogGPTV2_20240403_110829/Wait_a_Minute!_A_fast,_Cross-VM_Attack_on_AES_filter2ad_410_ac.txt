for each ciphertext value ci = j. Finally K0 is the vector that, for each key byte
candidate k, tracks the number of appearances in the key recovery step.
Example Assume that the memory line can hold n = 4 T table values and we
want to recover key byte k0. There are four ciphertext values detected with a
low reload counter. Assume further that each c0 has been xored with the T table
310
G. Irazoqui et al.
(cid:4)
⊕T
s[0]
0 = ci
0
(cid:5)
values of the monitored memory line (the ﬁrst 4 if we are working with the ﬁrst
positions), giving k(i)
. For each of the four possibilities of c0, there
⎧⎪⎪⎨
are n = 4 possible solutions for k0. If the results are the following:
⎪⎪⎩
⎧⎪⎪⎨
⎪⎪⎩
⎧⎪⎪⎨
⎪⎪⎩
k(1)
0
k(2)
0
k(3)
0
⎧⎪⎪⎨
⎪⎪⎩
cd
02
51
91
k(0)
0
43
ba
91
17
8b
91
f 3
66
91
45
22
af
And since there is only one common solution between all of them, which is 91, we
deduce that the correct key value is k0 = 91. This also means that K0[91] = 4,
since k = 91 appeared four times as possible key candidate in the key recovery
step.
Note that this is a generic attack that would apply virtually to any table-
based block cipher implementation. That is, our attack can easily be adapted to
other block ciphers as long as their last round consists of a table look-up with a
subsequent key addition.
5.2 Recovering the Full Key
To recover the full key, the attack is expanded to all tables used in the last round,
e.g. the 4 T tables of AES in OpenSSL 1.0.1. For each ciphertext byte it is known
which T table is used in the ﬁnal round of the encryption. This means that the
above attack can be repeated on each byte, by simply analyzing the collecting
ciphertexts and their timings for each of the ciphertext bytes individually. As
before, the timings are proﬁled according to the value that each ciphertext byte
ci takes in each of the encryptions, and are stored in a ciphertext byte vector. The
attack process is described in Algorithm 2. In a nutshell, the algorithm monitors
the ﬁrst T table memory line of all used tables and hence stores four reload
values per observed ciphertext. Note that, this is a known ciphertext attack and
therefore all that is needed is a ﬂush of one memory line before one encryption.
There is no need for the attacker to gain access to plaintexts.
Finally the attacker should apply Algorithm 1 to each of the obtained cipher-
text reload vectors. Recall that each ciphertext reload vector uses a diﬀerent
T table, so the right corresponding T table should be applied in the key recov-
ery algorithm.
Performing the Attack. In the following we provide the details about the
process followed during the attack.
Step 1: Acquire information about the oﬀset of T tables. The attacker
has to know the oﬀset of the T tables with respect to the beginning of
the library. With that information, the attacker can refer and point to any
memory line that holds T table values even when the ASLR is activated.
This means that some reverse engineering work has to be done prior to the
attack. This can be done in a debugging step where the oﬀset of the addresses
of the four T tables are recovered.
Wait a Minute! A fast, Cross-VM Attack on AES
311
Algorithm 2. Flush and reload algorithm extended to 16 ciphertext bytes
Input : T 00, T 10, T 20, T 30
//Addresses of each T table
Output: X0, X1, ...X15
//Reload vectors for ciphertext bytes
//Each Xk holds 256 counter values
while iteration  AccessThreshold then
Addcounter(Ti,Xi);
//Increase counter of Xi using Ti
end
end
end
return X0, X1, . . . , X15
Step 2: Collect Measurements. In this step, the attacker requests encryp-
tions and applies Flush+Reload between each encryption. The information
gained, i.e. T i0 was accessed or not, is stored together with the observed
ciphertext. The attacker needs to observe several encryptions to get rid of
the noise and to be able to recover the key. Note that, while the reload step
must be performed and timed by the attacker, the ﬂush might be performed
by other processes running in the victim OS.
Step 3: Key recovery. In this ﬁnal step, the attacker uses the collected mea-
surements and his knowledge about the public T tables to recover the key.
From this information, the attacker applies the steps detailed in Section 5.1
to recover the individual bytes of the key.
5.3 Attack Scenario 1: Spy Process
In this ﬁrst scenario we will attack an encryption server running in the same OS
as the spy process. The encryption server just receives encryption requests, en-
crypts a plaintext and sends the ciphertext back to the client. The server and the
client are running on diﬀerent cores. Thus, the attack consists in distinguishing
accesses from the last level of cache, i.e. L3 cache, which is shared across cores.
and the main memory. Clearly, if the attacker is able to distinguish accesses be-
tween last level of cache and main memory, it will be able to distinguish between
L1 and main memory accesses whenever server and client co-reside in the same
core. In this scenario, both the attacker and victim are using the same shared
library. The KSM is responsible for merging those pages into one uniﬁed shared
page. Therefore, the victim and attacker processes are linked through the KSM
deduplication feature.
Our attack works as described in the previous section. First the attacker dis-
covers the oﬀset of the addresses of the T tables with respect to the begining
312
G. Irazoqui et al.
of the library. Next, it issues encryption requests to the server, and receives the
corresponding ciphertext. After each encryption, the attacker checks with the
Flush+Reload technique whether the chosen T table values have been accessed.
Once enough measurements have been acquired, the key recovery step is per-
formed. As we will see in our results section, the whole process takes less than
half a minute.
Our attack signiﬁcantly improves on previous cache side-channel attacks such
as evict + time or prime and probe [19]. Both attacks were based on spy processes
targeting the L1 cache. A clear advantage of our attack is that —since it is
targeting the last shared level cache— it works across cores. Of course both
evict + time or prime and probe attacks can be applied to the last level of cache,
but their performance would be signiﬁcantly reduced in cross-core setting, due
to the large number of evictions/probings that are needed for a successful attack.
A more realistic attack scenario was proposed earlier by Bernstein [8] where
the attacker targets an encryption server. Our attack similarly works under a
realistic scenario. However. unlike Bernstein’s attack [8], our attack does not
require a proﬁling phase that involves access to an identical implementation
with a known-key. Finally, with respect to the previous Flush+Reload attack in
AES, our attack does not need to interrupt the AES execution of the encryption
server. We will compare diﬀerent attacks according to the number of encryptions
needed in Section 6.1.
5.4 Attack Scenario 2: Cross-VM Attack
In our second scenario the victim process is running in one virtual machine and
the attacker in another one but on the same machine possibly on diﬀerent cores.
For the purposes of this study it is assumed that the co-location problem has
been solved using the methods proposed in [21], ensuring the attacker and the
victim are running on the same physical machine. The attack exploits memory
overcommitment features that some VMMs such as VMware provide. In partic-
ular, we focus in memory deduplication. The VMM will search periodically for
identical pages across VMs to merge both pages into a single page in the mem-
ory. Once this is done (without the intervention of the attacker) both the victim
and the attacker will access the same portion of the physical memory enabling
the attack. The attack process is the same as in Scenario 1. Moreover, we later
show that the key is recovered in less than a minute, which makes the attack
quite practical.
We discussed the improvements of our attack over previous proposals in the pre-
vious scenario except the most important one: We believe that the evict+time,
prime and probe and time collision attacks will be rather diﬃcult to carry out
in real cloud environment. The ﬁrst two are targeting the L1 cache, which is not
shared across cores. The attacker would have to be in the same core as the victim,
which is a much stronger assumption than being just in the same physical ma-
chine. Both evict+time and prime and probe could be applied to work with the
L3 cache, but the noise and the amount of measurements would need to be drasti-
cally increased. Even further, due the increasing amount of source noises present
Wait a Minute! A fast, Cross-VM Attack on AES
313
in a cloud scenario (more layers, network latency) both evict+time and time col-
lision attacks would be hard to perform. Finally, targeting the CFS [13] to evict
the victim process, requires for the attacker’s code to run in the same OS, which
will certainly not be possible in a virtualized environment.
6 Experiment Setup and Results
We present results for both a spy process within the native machine as well as
the cross-VM scenario. The target process is executed in Ubuntu 12.04 64 bits,
kernel version 3.4, using the C-implementation of AES in OpenSSL 1.0.1f for
encryption. This is used when OpenSSL is conﬁgured with no-asm and no-hw
option. We want to remark that this is not the default option in the installation of
OpenSSL in most of the products. All experiments were performed on a machine
featuring an Intel i5-3320M four core clocked at 3.2GHz. The Core i5 has a
three-level cache architecture: The L1 cache is 8-way associative, with 215 bytes
of size and a cache line size of 64 bytes. The level-2 cache is 8-way associative as
well, with a cache line width of 64 bytes and a total size of 218 bytes. The level-3
cache is 12-way associative with a total size of 222 bytes and 64 bytes cache line
size. It is important to note that each core has private L1 and L2 caches, but the
L3 cache is shared among all cores. Together with the deduplication performed
by the VMM, the shared L3 cache allows the adversary to learn about data
accesses by the victim process.
The attack scenario is as follows: the victim process is an encryption server
handling encryption requests through a socket connection and sends back the
ciphertext, similar to Bernstein’s setup in [8]. But unlike Bernstein’s attack,
where packages of at least 400 bytes were sent to deal with the noise, our server
only receives packages of 16 bytes (the plaintext). The encryption key used by
the the server is unknown to the attacker. The attack process sends encryption
queries to the victim process. All measurements such as timing measurements of
the reload step are done on the attacker side. The server uses OpenSSL 1.0.1f for
the AES encryption. In our setup, each cache line holds 16 T table values, which
results in a 7.6% probability for not accessing a memory line per encryption.
All given attack results target only the ﬁrst cache line of each T table, i.e. the
ﬁrst 16 values of each T table for ﬂush and reload. Note that in the attack any
memory line of the T table would work equally well. Both native and cross-VM
attacks establish the threshold for selecting the correct ciphertext candidates
for the working T table line by selecting those values which are below half of
the average of overall timings for each ciphertext value. This is an empirical
threshold that we set up after running some experiments as follows
256(cid:10)
threshold =
i=0
ti
2 · 256
.
Spy Process Attack Setup: The attack process runs in the same OS as the
victim process. The communication between the processes is carried out via
314
G. Irazoqui et al.
Fig. 2. Number of correct key bytes guessed of the AES-128 bit key vs. number of
encryption requests. Even 50.000 encryptions (i.e. less than 5 seconds of interaction)
result in signiﬁcant security degradation in both the native machine as well as the
cross-VM attack scenario.
localhost connection and measures timing using Read Time-Stamp Counters
(rdtsc). The attack is set up to work across cores; the encryption server is
running in a diﬀerent core than the attacker. We believe that distinguishing
between L3 and main memory accesses will be more susceptible to noise than
distinguishing between L1 cache accesses and main memory accesses. Therefore
while working with the L3 cache gives us a more realistic setting, it also makes
the attack more challenging.
Cross-VM Attack Setup: In this attack we use VMware ESXI 5.5.0 build
number 1623387 running Ubuntu 12.04 64-bits guest OSs. We know that VMware
implements TPS with large pages (2 MB) or small pages (4 KB). We decided to
use the later one, since it seems to be the default for most systems. Furthermore,
as stated in [28], even if the large page sharing is selected, the VMM will still look
for identical small pages to share. For the attack we used two virtual machines,
one for the victim and one for the attacker. The communication between them
is carried out over the local IP connection.