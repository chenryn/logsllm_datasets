0.94809
0.94973
0.95060
0.97387
0.00068
0.97195
0.97382
0.97492
0.92537
0.00108
0.92328
0.92530
0.92749
0.02613
0.00068
0.02508
0.02618
0.02805
0.07463
0.00108
0.07251
0.07470
0.07672
x
σ
xmin
˜x
xmax
Table 12: Classiﬁcation accuracy for 5-fold CV on
successfully resolved domains and mAGDs of arbitrary
DGAs using RFs.
fully resolving queries. In particular, we performed test
measurements using random forests and a setup similar
to the mixed DGA case presented in Section 5.2.2. In-
stead of bNXDs we composed the data sets of successful
resolved domains from the Siemens network and known
mAGDs of arbitrary DGAs. As in Section 5.2.2 we per-
formed 5 repeated 5-fold CVs on 20 sets. Without fur-
ther optimizations or new features adapted for success-
fully resolved domains, we achieved a mean ACC of
0.94962 with a small standard deviation of 0.00071, a
minimum of 0.94809 and a maximum of 0.95060. Ta-
ble 12 presents detailed results for this proof of concept
experiment using RFs. Results for SVMs can be found
in Appendix A.
Considering the fact that we only require to process
successfully resolved domains of single devices or small
groups of devices, the previously presented approach
is highly promising for performing identiﬁcation of C2
servers.
6 Related Work
In the past, monitoring DNS trafﬁc (successfully resolv-
ing and/or non-resolving) has been used as primary or
additional source of information in detecting malicious
activity in a network (e.g., [2, 16, 18, 9, 4]). Some of
these approaches have concentrated on identifying C2
servers (e.g., [18, 16]), others have focused on detect-
ing mAGDs (e.g., [2]), identifying infected devices (e.g.,
[9]), or detecting malicious URLs in general (e.g., [4]).
The most striking difference between these prior ap-
proaches and FANCI is that they all require more or less
extensive tracking of DNS trafﬁc, that is, they require a
correlation of information extracted from groups of DNS
queries and/or responses (e.g., for features extraction).
In contrast, the features that FANCI’s classiﬁcation mod-
ule uses when predicting a particular NXD are extracted
from this NXD alone, such that FANCI does not require
any tracking. In addition, many of the prior approaches
are based on clustering, which indulges manual labelling
of the identiﬁed clusters. As opposed to this, FANCI
(like [4]) makes use of an ML-classiﬁer.
Detecting mAGDs in successfully resolving DNS traf-
ﬁc allows for identifying C2 servers (see Section 5.5 for
an initial evaluation of FANCI in this context). However,
monitoring only NXD responses has the advantage that
infections with bots can be detected with less delay and
while processing signiﬁcantly less trafﬁc as the vast ma-
jority of DGAs issue many more NXDs than registered
names.
While the prior works show promising detection ca-
pabilities on speciﬁc data sets, little information on their
generalizability and the efﬁciency of their detection pro-
cess in terms of time and memory requirements is re-
ported. FANCI is highly efﬁcient with respect to both
prediction (0.0025s/sample) and training (5.66min on
92102 samples) and shows a high accuracy with low FPR
in very large scale realistic scenarios even when trained
on a different network.
A fair comparison between FANCI and the prior ap-
proaches with respect to detection accuracy and efﬁ-
ciency is hard to achieve as they aim at slightly different
targets and use different data sets even if they do aim at
the same target. These data sets and the implementations
of the systems are not publicly available. In the follow-
ing, we nevertheless discuss the approaches most closely
related to FANCI in more detail.
Exposure. Bilge et al. [4] introduce a system called
Exposure that aims at detecting malicious domain names
in DNS trafﬁc in general, that is, they do not focus on
mAGDs but also aim at detecting domain names used in
the context of phishing or in the context of hosting mali-
cious code. In contrast to FANCI, Exposure monitors full
DNS trafﬁc and not only NXD responses. Additionally,
Exposure always requires access to more sensitive infor-
mation than FANCI (e.g., access patterns). Like FANCI,
Exposure is based on ML-classiﬁcation and uses a small
set of carefully selected features. However, the features
are not only extracted from single domain names but also
include features extracted from correlating several DNS
queries or responses. The accuracy of Exposure lies in
a similar range as FANCI’s ACC (but targeting detect-
ing malicious domain names in general) and is evaluated
on real-world data as well. Due to requiring sensitive
and contextual information, Exposure is not as versa-
tile as FANCI especially when it comes to software-as-a-
service deployments.
Winning with DNS Failures. Yadav and Reddy [18]
were the ﬁrst to consider the detection of botnets lever-
aging both DNS responses of successfully resolving do-
main names and NXD responses. They introduce a sys-
tem primarily targeting at the identiﬁcation of IP ad-
dresses of C2 servers of DGA-based botnets. The system
is based on narrowing down a set of potentially malicious
USENIX Association
27th USENIX Security Symposium    1177
IP addresses by ﬁltering. This ﬁltering requires access to
the overall successfully resolving DNS trafﬁc (in order
to count the number of domains that resolve to a given
IP address), NXD responses in the vicinity of successful
queries, as well as the entropy of failed and successful
DNS queries. The output of the ﬁltering is a set of po-
tential C2 server IP addresses.
Pleidas. Antonakakis et al. [2] present a DGA detec-
tion and discovery system called Pleidas. The system is
able to discover new DGAs by means of clustering and to
detect known DGAs by means of a supervised learning
using a multi-class variant of alternating decision trees.
Applying their system in a large ISP environment over a
period of 15 months, they discovered twelve new DGAs,
where six of them are completely new and six are vari-
ants of previously known ones.
Pleidas uses a set of statistical and structural features,
where all features are extracted from groups of NXD re-
sponses originating from a single host.9 The statistical
features include entropy measures and n-grams over the
group of domain names. The structural features comprise
domain lengths, uniqueness and frequency distributions
of TLDs, and the number of subdomain levels present.
Pleidas’ classiﬁcation accuracy is evaluated on labeled
data. The top 10,000 domains of Alexa serve as benign
class. The malicious data set consists of 60,000 NXD
responses generated by four DGAs, namely Bobax, Con-
ﬁcker, Sinowal, and Murofet. For a group size of 5 NXD
responses of each host the TPR is in the range of 95 and
99 percent and the FPR is between 0.1 and 1.4 percent.
With 10 NXD responses per group, the accuracy slightly
increases. In this case, the TPR is in a range of 99 and
100 percent, where the FPR ranges between 0 and 0.2
percent.
As Pleidas requires tracking of DNS responses for fea-
ture extraction, we expect that it is much less efﬁcient
than FANCI. The reported detection quality is similar to
FANCI but FANCI is evaluated on a more extensive data
set that uses far more DGAs and real world-benign trafﬁc
instead of the top 10,000 domains of Alexa. The gener-
alizability of Pleidas is not evaluated.
Phoenix. Schiavoni et al. [16] present a DGA-based
botnet tracking and intelligence system called Phoenix.
In contrast to the previously presented Pleidas, Phoenix
focuses on intelligence operations instead of DGA de-
tection. This especially includes the tracking of C2 in-
frastructures of botnets regarding their IP address ranges.
However, Phoenix is also capable of labeling DNS trafﬁc
as either DGA-related or benign.
9As opposed to this, FANCI uses features extracted from individual
NXDs only.
They evaluated the classiﬁcation performance of
Phoenix on 1,153,516 domains overall
including
mAGDs of three different DGAs and bNXDs obtained
from a passive DNS. The evaluation yielded TPRs in
the range of 81.4 and 94.8 percent and is is thus signiﬁ-
cantly lower than FANCI in with respect to mAGDs de-
tection. As the features used are less light-weight and
require tracking we expect Phoenix to be less efﬁcient
than FANCI with respect to speed.
NetFlow. Grill et al. [9] present a different approach
for DGA-based malware detection, with the particular
goal of being applicable in large scale networks in a
privacy-preserving manner. Their system is based on
NetFlow data exclusively, that is, on an aggregation of
metadata of network packets exchanged between a com-
bination of a source IP and port and a destination IP ad-
dress and port. The exported metadata depends on the
particular implementation of NetFlow, but typically in-
cludes: IP addresses, time stamps, port numbers, byte
counters, and packet counters. Grill et al. use the stan-
dardized IPFIX NetFlow format [12]. They perform an
anomaly detection based on the assumption that normal
behaviour of a host is to request an IP address via DNS
for a certain domain name, followed by one or multi-
ple connections to this newly resolved IP address. They
assume that a DGA malware infected device is charac-
terized by regularly issuing DNS requests without subse-
quent connections to new IP addresses.
For their evaluation they performed three experiments
considering different types of hosts, network sizes, and
times of the day. They consider six different DGAs.
The ACC value is in the range of 88.77 and 99.89 per-
cent depending on the setup in question and thus lower
than FANCI’s accuracy. As NetFlow is based on exten-
sive tracking, it can be expected to be less efﬁcient than
FANCI.
DGArchive. Plohmann et al. [14] presented an exten-
sive study of current DGAs. Their paper is based on the
collection and reverse engineering of DGA-based mal-
ware and provides detailed technical insights in the func-
tionality of modern DGAs divisible in three main con-
tributions: a taxonomy of DGAs, a database of DGAs
and corresponding mAGDs called DGArchive, and an
analysis of the landscape of registered mAGDs. While
Plohmann et al. do not implement an automated de-
tection, the DGArchive provides the means to blacklist
known mAGDs. Our work builds on DGArchive in two
ways: we use it to clean our benign trafﬁc before training
and we use it as source for malicious mAGDs.
1178    27th USENIX Security Symposium
USENIX Association
7 Conclusion
In this work, we presented FANCI, a versatile system for
the detection of malicious DGA-related domain names
among arbitrary NXD DNS trafﬁc based on supervised
learning classiﬁers. FANCI’s versatility is a result of
its lightweight and language independent feature design
relying exclusively on domain names for classiﬁcation.
In our extensive evaluation, we veriﬁed FANCI’s highly
accurate and highly efﬁcient detection capabilities of
mAGDs in different experiments, including its general-
izability.
In an one-month real-world application in a
large university network, we were able to discover ten
new DGA-related groups of mAGDs, where at least four
of them originate from brand new DGAs.
With its empirically proven detection capabilities and
a successful real-world test, FANCI can make a decisive
contribution to combating DGA-based botnets. FANCI
is able to provide valuable information to existing secu-
rity solutions and is able to contribute to a higher level
device and network security in a variety of environments.
Acknowledgements
We would like to thank Daniel Plohmann for granting
us access to DGArchive. Many thanks to Jens Hektor
and Thomas Penteker for providing us NXD data from
RWTH Aachen University and Siemens respecitvely.
Thanks to the ITCenter of RWTH Aachen University for
granting us extensive access to the university’s compute
cluster.
References
[8] FOUNDATION, M. Public Sufﬁx List, Apr. 2017.
[9] GRILL, M., NIKOLAEV, I., VALEROS, V., AND REHAK, M.
Detecting DGA malware using NetFlow. In 2015 IFIP/IEEE In-
ternational Symposium on Integrated Network Management (IM)
(May 2015), pp. 1304–1309.
[10] HO, T. K. Random Decision Forests. In Proceedings of the Third
International Conference on Document Analysis and Recognition
(Washington, USA, 1995), ICDAR, IEEE Computer Society.
[11] ICANN. ICANN Research - TLD DNSSEC Report, Feb. 2017.
[12] J. QUITTEK, T. ZSEBY, B. C. S. Z. Requirements for IP Flow
Information Export (IPFIX). RFC 3917, IETF, October 2004.
[13] NUMBERS, I. C. F. A. N. A. Registry Listing - ICANN, Apr.
2017.
[14] PLOHMANN, D., YAKDAN, K., KLATT, M., BADER, J., AND
GERHARDS-PADILLA, E. A comprehensive measurement study
of domain generating malware. In 25th USENIX Security Sympo-
sium (Austin, TX, 2016), USENIX Association, pp. 263–278.
[15] RWTH AACHEN UNIVERSITY, I. C. Statusmeldungen zentraler
Systeme - RWTH AACHEN UNIVERSITY IT Center - Deutsch,
Aug. 2017.
[16] SCHIAVONI, S., MAGGI, F., CAVALLARO, L., AND ZANERO,
S. Phoenix: DGA-Based Botnet Tracking and Intelligence. In
Detection of Intrusions and Malware, and Vulnerability Assess-
ment (July 2014), Springer, Cham, pp. 192–211.
[17] SOPHOS. Sophos Live Protection: Overview, Aug. 2017.
[18] YADAV, S., AND REDDY, A. L. N. Winning with DNS Failures:
Strategies for Faster Botnet Detection. In Security and Privacy
in Communication Networks (Sept. 2011), Lecture Notes of the
Institute for Computer Sciences, Social Informatics and Telecom-
munications Engineering, Springer, Berlin, Heidelberg, pp. 446–
459.
[19] ZDRNJA, B. Google Chrome and (weird) DNS requests, Aug.
2017.
[1] ANTONAKAKIS, M., PERDISCI, R., DAGON, D., LEE, W.,
AND FEAMSTER, N. Building a Dynamic Reputation System
for DNS. In 19th USENIX security symposium (2010), USENIX
Association, pp. 273–290.
A Results for SVMs
[2] ANTONAKAKIS, M.,
PERDISCI,
Y.,
VASILOGLOU II, N., ABU-NIMEH, S., LEE, W., AND
From Throw-Away Trafﬁc to Bots: Detecting
DAGON, D.
In 21th USENIX security
the Rise of DGA-Based Malware.
symposium (2012).
R.,
NADJI,
[3] AUTHORITY, I. A. N.
2017.
IANA list of top-level domains, July
[4] BILGE, L., SEN, S., BALZAROTTI, D., KIRDA, E., AND
KRUEGEL, C. Exposure: A Passive DNS Analysis Service to
Detect and Report Malicious Domains. ACM Trans. Inf. Syst.
Secur. (Apr. 2014), 14:1–14:28.
[5] BREIMAN, L. Bagging predictors. Machine Learning (Aug.
1996), 123–140.
[6] BREIMAN, L. Random Forests. Machine Learning (Oct. 2001),
5–32.
[7] CORTES, C., AND VAPNIK, V. Support-vector networks. Ma-
chine Learning (Sept. 1995), 273–297.
In this section, we present results for SVMs for the ex-
periments presented in Section 5.2.2, Section 5.2.3, and
Section 5.5.
ACC
TPR
TNR
FNR
FPR
0.99930
0.00190
0.98133
0.99971
1.00000
0.99983
0.00103
0.99188
1.00000
1.00000
0.99878
0.00331
0.96400
0.99942
1.00000
0.00017
0.00103
0.00000
0.00000
0.00812
0.00122
0.00331
0.00000
0.00058
0.03600
x
σ
xmin
˜x
xmax
Table 13: Results for classifying bNXDs and mAGDs of
single DGAs with SVMs. In total, 295 sets of 59 DGAs
were considered each evaluated by 5 repetitions of a 5-
fold CV.
USENIX Association
27th USENIX Security Symposium    1179
ACC
TPR
TNR
FNR
FPR
ACC
TPR
TNR
FNR
FPR
x
σ
xmin
˜x
xmax
0.98315
0.06166
0.49850
0.99965
1.00000
0.96713
0.12291
0.00000
1.00000
1.00000
0.99916
0.00085
0.99564
0.99935
1.00000
0.03139
0.11956
0.00000
0.00000
1.00000
0.00084
0.00085
0.00000
0.00065
0.00436
x
σ
xmin
˜x
xmax
0.99180