### Table 12: Classification Accuracy for 5-Fold Cross-Validation on Successfully Resolved Domains and mAGDs of Arbitrary DGAs Using Random Forests

| Metric | x | σ | xmin | ˜x | xmax |
|--------|---|---|------|----|------|
| ACC    | 0.94809 | 0.94973 | 0.95060 | 0.97387 | 0.00068 |
| TPR    | 0.97195 | 0.97382 | 0.97492 | 0.92537 | 0.00108 |
| TNR    | 0.92328 | 0.92530 | 0.92749 | 0.02613 | 0.00068 |
| FNR    | 0.02508 | 0.02618 | 0.02805 | 0.07463 | 0.00108 |
| FPR    | 0.07251 | 0.07470 | 0.07672 | -      | -      |

In this section, we present the classification accuracy for 5-fold cross-validation (CV) on successfully resolved domains and known mAGDs of arbitrary DGAs using random forests (RFs). We performed test measurements using RFs and a setup similar to the mixed DGA case presented in Section 5.2.2. Instead of bNXDs, we composed the data sets from successfully resolved domains from the Siemens network and known mAGDs of arbitrary DGAs. We conducted 5 repeated 5-fold CVs on 20 sets. Without further optimizations or new features adapted for successfully resolved domains, we achieved a mean accuracy (ACC) of 0.94962 with a small standard deviation of 0.00071, a minimum of 0.94809, and a maximum of 0.95060. Table 12 presents detailed results for this proof-of-concept experiment using RFs. Results for Support Vector Machines (SVMs) can be found in Appendix A.

### 6. Related Work

In the past, monitoring DNS traffic (both successfully resolving and non-resolving) has been used as a primary or additional source of information for detecting malicious activity in a network (e.g., [2, 16, 18, 9, 4]). Some approaches have focused on identifying C2 servers (e.g., [18, 16]), while others have concentrated on detecting mAGDs (e.g., [2]), identifying infected devices (e.g., [9]), or detecting malicious URLs in general (e.g., [4]).

The most significant difference between these prior approaches and FANCI is that they all require more or less extensive tracking of DNS traffic, meaning they need to correlate information extracted from groups of DNS queries and/or responses (e.g., for feature extraction). In contrast, FANCI's classification module extracts features from individual NXDs alone, eliminating the need for any tracking. Additionally, many of the prior approaches are based on clustering, which requires manual labeling of the identified clusters. FANCI, like [4], uses an ML classifier.

Detecting mAGDs in successfully resolved DNS traffic allows for the identification of C2 servers (see Section 5.5 for an initial evaluation of FANCI in this context). However, monitoring only NXD responses has the advantage of detecting bot infections with less delay and processing significantly less traffic, as the vast majority of DGAs issue many more NXDs than registered names.

While prior works show promising detection capabilities on specific data sets, little information is reported on their generalizability and the efficiency of their detection process in terms of time and memory requirements. FANCI is highly efficient, with prediction times of 0.0025 seconds per sample and training times of 5.66 minutes on 92,102 samples. It also shows high accuracy with low false positive rates (FPR) in very large-scale, realistic scenarios, even when trained on different networks.

A fair comparison between FANCI and prior approaches regarding detection accuracy and efficiency is challenging because they aim at slightly different targets and use different data sets, even if they do aim at the same target. These data sets and the implementations of the systems are not publicly available. Nevertheless, we discuss the most closely related approaches in more detail below.

#### Exposure
Bilge et al. [4] introduced a system called Exposure, which aims to detect malicious domain names in DNS traffic in general, not just mAGDs. Unlike FANCI, Exposure monitors full DNS traffic and not only NXD responses. Additionally, Exposure always requires access to more sensitive information than FANCI (e.g., access patterns). Like FANCI, Exposure is based on ML classification and uses a small set of carefully selected features. However, the features are not only extracted from single domain names but also include features extracted from correlating several DNS queries or responses. The accuracy of Exposure is in a similar range to FANCI’s ACC (but targeting the detection of malicious domain names in general) and is evaluated on real-world data. Due to requiring sensitive and contextual information, Exposure is not as versatile as FANCI, especially for software-as-a-service deployments.

#### Winning with DNS Failures
Yadav and Reddy [18] were the first to consider the detection of botnets leveraging both DNS responses of successfully resolved domain names and NXD responses. They introduced a system primarily aimed at identifying IP addresses of C2 servers of DGA-based botnets. The system narrows down a set of potentially malicious IP addresses by filtering. This filtering requires access to the overall successfully resolved DNS traffic (to count the number of domains that resolve to a given IP address), NXD responses in the vicinity of successful queries, and the entropy of failed and successful DNS queries. The output is a set of potential C2 server IP addresses.

#### Pleidas
Antonakakis et al. [2] presented a DGA detection and discovery system called Pleidas. The system can discover new DGAs through clustering and detect known DGAs using a supervised learning approach with a multi-class variant of alternating decision trees. Over 15 months in a large ISP environment, they discovered twelve new DGAs, six of which were completely new and six were variants of previously known ones.

Pleidas uses a set of statistical and structural features, all extracted from groups of NXD responses originating from a single host. The statistical features include entropy measures and n-grams over the group of domain names. The structural features comprise domain lengths, uniqueness and frequency distributions of TLDs, and the number of subdomain levels. Pleidas' classification accuracy is evaluated on labeled data, with the top 10,000 domains of Alexa serving as the benign class. The malicious data set consists of 60,000 NXD responses generated by four DGAs: Bobax, Conficker, Sinowal, and Murofet. For a group size of 5 NXD responses per host, the true positive rate (TPR) is in the range of 95% to 99%, and the FPR is between 0.1% and 1.4%. With 10 NXD responses per group, the accuracy slightly increases, with TPR in the range of 99% to 100% and FPR between 0% and 0.2%.

As Pleidas requires tracking of DNS responses for feature extraction, it is expected to be much less efficient than FANCI. The reported detection quality is similar to FANCI, but FANCI is evaluated on a more extensive data set that uses far more DGAs and real-world benign traffic instead of the top 10,000 domains of Alexa. The generalizability of Pleidas is not evaluated.

#### Phoenix
Schiavoni et al. [16] presented a DGA-based botnet tracking and intelligence system called Phoenix. Unlike Pleidas, Phoenix focuses on intelligence operations rather than DGA detection, including the tracking of C2 infrastructures of botnets regarding their IP address ranges. Phoenix is also capable of labeling DNS traffic as either DGA-related or benign. They evaluated the classification performance of Phoenix on 1,153,516 domains, including mAGDs of three different DGAs and bNXDs obtained from a passive DNS. The evaluation yielded TPRs in the range of 81.4% to 94.8%, which is significantly lower than FANCI in terms of mAGDs detection. As the features used are less lightweight and require tracking, Phoenix is expected to be less efficient than FANCI in terms of speed.

#### NetFlow
Grill et al. [9] presented a different approach for DGA-based malware detection, with the goal of being applicable in large-scale networks in a privacy-preserving manner. Their system is based on NetFlow data, which aggregates metadata of network packets exchanged between a combination of a source IP and port and a destination IP address and port. The exported metadata depends on the particular implementation of NetFlow but typically includes IP addresses, timestamps, port numbers, byte counters, and packet counters. Grill et al. use the standardized IPFIX NetFlow format [12]. They perform anomaly detection based on the assumption that normal behavior for a host is to request an IP address via DNS for a certain domain name, followed by one or multiple connections to this newly resolved IP address. They assume that a DGA malware-infected device is characterized by regularly issuing DNS requests without subsequent connections to new IP addresses.

For their evaluation, they performed three experiments considering different types of hosts, network sizes, and times of the day, using six different DGAs. The accuracy (ACC) value ranged from 88.77% to 99.89% depending on the setup, which is lower than FANCI’s accuracy. As NetFlow is based on extensive tracking, it is expected to be less efficient than FANCI.

#### DGArchive
Plohmann et al. [14] presented an extensive study of current DGAs. Their paper is based on the collection and reverse engineering of DGA-based malware and provides detailed technical insights into the functionality of modern DGAs, divided into three main contributions: a taxonomy of DGAs, a database of DGAs and corresponding mAGDs called DGArchive, and an analysis of the landscape of registered mAGDs. While Plohmann et al. do not implement an automated detection, the DGArchive provides the means to blacklist known mAGDs. Our work builds on DGArchive in two ways: we use it to clean our benign traffic before training and as a source for malicious mAGDs.

### 7. Conclusion

In this work, we presented FANCI, a versatile system for the detection of malicious DGA-related domain names among arbitrary NXD DNS traffic based on supervised learning classifiers. FANCI’s versatility is a result of its lightweight and language-independent feature design, relying exclusively on domain names for classification. In our extensive evaluation, we verified FANCI’s highly accurate and highly efficient detection capabilities of mAGDs in different experiments, including its generalizability.

In a one-month real-world application in a large university network, we were able to discover ten new DGA-related groups of mAGDs, where at least four of them originated from brand new DGAs. With its empirically proven detection capabilities and a successful real-world test, FANCI can make a decisive contribution to combating DGA-based botnets. FANCI can provide valuable information to existing security solutions and contribute to higher-level device and network security in various environments.

### Acknowledgements

We would like to thank Daniel Plohmann for granting us access to DGArchive. Many thanks to Jens Hektor and Thomas Penteker for providing us with NXD data from RWTH Aachen University and Siemens, respectively. Thanks to the ITCenter of RWTH Aachen University for granting us extensive access to the university’s compute cluster.

### References

[1] ANTONAKAKIS, M., PERDISCI, R., DAGON, D., LEE, W., AND FEAMSTER, N. Building a Dynamic Reputation System for DNS. In 19th USENIX Security Symposium (2010), USENIX Association, pp. 273–290.

[2] ANTONAKAKIS, M., PERDISCI, R., VASILOGLOU II, N., ABU-NIMEH, S., LEE, W., AND DAGON, D. From Throw-Away Traffic to Bots: Detecting the Rise of DGA-Based Malware. In 21st USENIX Security Symposium (2012).

[3] AUTHORITY, I. A. N. IANA list of top-level domains, July 2017.

[4] BILGE, L., SEN, S., BALZAROTTI, D., KIRDA, E., AND KRUEGEL, C. Exposure: A Passive DNS Analysis Service to Detect and Report Malicious Domains. ACM Trans. Inf. Syst. Secur. (Apr. 2014), 14:1–14:28.

[5] BREIMAN, L. Bagging predictors. Machine Learning (Aug. 1996), 123–140.

[6] BREIMAN, L. Random Forests. Machine Learning (Oct. 2001), 5–32.

[7] CORTES, C., AND VAPNIK, V. Support-vector networks. Machine Learning (Sept. 1995), 273–297.

[8] FOUNDATION, M. Public Suffix List, Apr. 2017.

[9] GRILL, M., NIKOLAEV, I., VALEROS, V., AND REHAK, M. Detecting DGA malware using NetFlow. In 2015 IFIP/IEEE International Symposium on Integrated Network Management (IM) (May 2015), pp. 1304–1309.

[10] HO, T. K. Random Decision Forests. In Proceedings of the Third International Conference on Document Analysis and Recognition (Washington, USA, 1995), ICDAR, IEEE Computer Society.

[11] ICANN. ICANN Research - TLD DNSSEC Report, Feb. 2017.

[12] J. QUITTEK, T. ZSEBY, B. C. S. Z. Requirements for IP Flow Information Export (IPFIX). RFC 3917, IETF, October 2004.

[13] NUMBERS, I. C. F. A. N. A. Registry Listing - ICANN, Apr. 2017.

[14] PLOHMANN, D., YAKDAN, K., KLATT, M., BADER, J., AND GERHARDS-PADILLA, E. A comprehensive measurement study of domain generating malware. In 25th USENIX Security Symposium (Austin, TX, 2016), USENIX Association, pp. 263–278.

[15] RWTH AACHEN UNIVERSITY, I. C. Statusmeldungen zentraler Systeme - RWTH AACHEN UNIVERSITY IT Center - Deutsch, Aug. 2017.

[16] SCHIAVONI, S., MAGGI, F., CAVALLARO, L., AND ZANERO, S. Phoenix: DGA-Based Botnet Tracking and Intelligence. In Detection of Intrusions and Malware, and Vulnerability Assessment (July 2014), Springer, Cham, pp. 192–211.

[17] SOPHOS. Sophos Live Protection: Overview, Aug. 2017.

[18] YADAV, S., AND REDDY, A. L. N. Winning with DNS Failures: Strategies for Faster Botnet Detection. In Security and Privacy in Communication Networks (Sept. 2011), Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering, Springer, Berlin, Heidelberg, pp. 446–459.

[19] ZDRNJA, B. Google Chrome and (weird) DNS requests, Aug. 2017.

### Appendix A: Results for SVMs

#### Table 13: Results for Classifying bNXDs and mAGDs of Single DGAs with SVMs

| Metric | x | σ | xmin | ˜x | xmax |
|--------|---|---|------|----|------|
| ACC    | 0.99930 | 0.00190 | 0.98133 | 0.99971 | 1.00000 |
| TPR    | 0.99983 | 0.00103 | 0.99188 | 1.00000 | 1.00000 |
| TNR    | 0.99878 | 0.00331 | 0.96400 | 0.99942 | 1.00000 |
| FNR    | 0.00017 | 0.00103 | 0.00000 | 0.00812 | 0.00122 |
| FPR    | 0.00331 | 0.00000 | 0.00058 | 0.03600 | -      |

In this section, we present the results for SVMs for the experiments presented in Sections 5.2.2, 5.2.3, and 5.5. In total, 295 sets of 59 DGAs were considered, each evaluated by 5 repetitions of a 5-fold CV.