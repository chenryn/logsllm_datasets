Alternatively, you can install and configure NGINX through the
Google Compute Engine startup script, which is an advanced con‐
figuration option when creating a VM.
Discussion
Google Compute Engine offers highly configurable VMs at a
moment’s notice. Starting a VM takes little effort and enables a
world of possibilities. Google Compute Engine offers networking
and compute in a virtualized cloud environment. With a Google
Compute instance, you have the full capabilities of an NGINX server
wherever and whenever you need it.
10.8 Deploying to Google Compute Engine | 111
10.9 Creating a Google Compute Image
Problem
You need to create a Google Compute Image to quickly instantiate a
VM or create an instance template for an instance group.
Solution
Create a VM as described in Recipe 10.8. After installing and config‐
uring NGINX on your VM instance, set the auto-delete state of the
boot disk to false. To set the auto-delete state of the disk, edit the
VM. On the Edit page under the disk configuration is a checkbox
labeled “Delete boot disk when instance is deleted.” Deselect this
checkbox and save the VM configuration. Once the auto-delete state
of the instance is set to false, delete the instance. When prompted,
do not select the checkbox that offers to delete the boot disk. By per‐
forming these tasks, you will be left with an unattached boot disk
with NGINX installed.
After your instance is deleted and you have an unattached boot disk,
you can create a Google Compute Image. From the Image section of
the Google Compute Engine console, select Create Image. You will
be prompted for an image name, family, description, encryption
type, and the source. The source type you need to use is disk; and
for the source disk, select the unattached NGINX boot disk. Select
Create and Google Compute Cloud will create an image from your
disk.
Discussion
You can utilize Google Cloud Images to create VMs with a boot disk
identical to the server you’ve just created. The value in creating
images is being able to ensure that every instance of this image is
identical. When installing packages at boot time in a dynamic envi‐
ronment, unless using version locking with private repositories, you
run the risk of package version and updates not being validated
before being run in a production environment. With machine
images, you can validate that every package running on this
machine is exactly as you tested, strengthening the reliability of your
service offering.
112 | Chapter 10: Cloud Deployments
Also See
Create, Delete, and Depreciate Private Images
10.10 Creating a Google App Engine Proxy
Problem
You need to create a proxy for Google App Engine to context switch
between applications or serve HTTPS under a custom domain.
Solution
Utilize NGINX in Google Compute Cloud. Create a virtual
machine in Google Compute Engine, or create a virtual machine
image with NGINX installed and create an instance template with
this image as your boot disk. If you’ve created an instance tem‐
plate, follow up by creating an instance group that utilizes that
template.
Configure NGINX to proxy to your Google App Engine endpoint.
Make sure to proxy to HTTPS because Google App Engine is public,
and you’ll want to ensure you do not terminate HTTPS at your
NGINX instance and allow information to travel between NGINX
and Google App Engine unsecured. Because App Engine provides
just a single DNS endpoint, you’ll be using the proxy_pass directive
rather than upstream blocks in the open source version of NGINX.
When proxying to Google App Engine, make sure to set the end‐
point as a variable in NGINX, then use that variable in the
proxy_pass directive to ensure NGINX does DNS resolution on
every request. For NGINX to do any DNS resolution, you’ll need to
also utilize the resolver directive and point to your favorite DNS
resolver. Google makes the IP address 8.8.8.8 available for public
use. If you’re using NGINX Plus, you’ll be able to use the resolve
flag on the server directive within the upstream block, keepalive
connections, and other benefits of the upstream module when
proxying to Google App Engine.
You may choose to store your NGINX configuration files in Google
Storage, then use the startup script for your instance to pull down
the configuration at boot time. This will allow you to change your
configuration without having to burn a new image. However, it will
add to the startup time of your NGINX server.
10.10 Creating a Google App Engine Proxy | 113
Discussion
You want to run NGINX in front of Google App Engine if you’re
using your own domain and want to make your application avail‐
able via HTTPS. At this time, Google App Engine does not allow
you to upload your own SSL certificates. Therefore, if you’d like to
serve your app under a domain other than appspot.com with
encryption, you’ll need to create a proxy with NGINX to listen at
your custom domain. NGINX will encrypt communication between
itself and your clients, as well as between itself and Google App
Engine.
Another reason you may want to run NGINX in front of Google
App Engine is to host many App Engine apps under the same
domain and use NGINX to do URI-based context switching. Micro‐
services are a popular architecture, and it’s common for a proxy like
NGINX to conduct the traffic routing. Google App Engine makes it
easy to deploy applications, and in conjunction with NGINX, you
have a full-fledged application delivery platform.
114 | Chapter 10: Cloud Deployments
CHAPTER 11
Containers/Microservices
11.0 Introduction
Containers offer a layer of abstraction at the application layer, shift‐
ing the installation of packages and dependencies from the deploy to
the build process. This is important because engineers are now ship‐
ping units of code that run and deploy in a uniform way regardless
of the environment. Promoting containers as runnable units reduces
the risk of dependency and configuration snafus between environ‐
ments. Given this, there has been a large drive for organizations to
deploy their applications on container platforms. When running
applications on a container platform, it’s common to containerize as
much of the stack as possible, including your proxy or load balancer.
NGINX and NGINX Plus containerize and ship with ease. They also
include many features that make delivering containerized applica‐
tions fluid. This chapter focuses on building NGINX and NGINX
Plus container images, features that make working in a container‐
ized environment easier, and deploying your image on Kubernetes
and OpenShift.
11.1 DNS SRV Records
Problem
You’d like to use your existing DNS SRV record implementation as
the source for upstream servers with NGINX Plus.
115
Solution
Specify the service directive with a value of http on an upstream
server to instruct NGINX to utilize the SRV record as a load-
balancing pool:
http {
resolver 10.0.0.2;
upstream backend {
zone backends 64k;
server api.example.internal service=http resolve;
}
}
This feature is an NGINX Plus exclusive. The configuration
instructs NGINX Plus to resolve DNS from a DNS server at 10.0.0.2
and set up an upstream server pool with a single server directive.
This server directive specified with the resolve parameter is
instructed to periodically re-resolve the domain name. The ser
vice=http parameter and value tells NGINX that this is an SRV
record containing a list of IPs and ports and to load balance over
them as if they were configured with the server directive.
Discussion
Dynamic infrastructure is becoming ever more popular with the
demand and adoption of cloud-based infrastructure. Autoscaling
environments scale horizontally, increasing and decreasing the
number of servers in the pool to match the demand of the load.
Scaling horizontally demands a load balancer that can add and
remove resources from the pool. With an SRV record, you offload
the responsibility of keeping the list of servers to DNS. This type of
configuration is extremely enticing for containerized environments
because you may have containers running applications on variable
port numbers, possibly at the same IP address. It’s important to note
that UDP DNS record payload is limited to about 512 bytes.
11.2 Using the Official NGINX Image
Problem
You need to get up and running quickly with the NGINX image
from Docker Hub.
116 | Chapter 11: Containers/Microservices
Solution
Use the NGINX image from Docker Hub. This image contains a
default configuration. You’ll need to either mount a local configura‐
tion directory or create a Dockerfile and ADD in your configuration
to the image build to alter the configuration. Here, we mount a vol‐
ume where NGINX’s default configuration serves static content to
demonstrate its capabilities by using a single command:
$ docker run --name my-nginx -p 80:80 \
-v /path/to/content:/usr/share/nginx/html:ro -d nginx
The docker command pulls the nginx:latest image from Docker
Hub if it’s not found locally. The command then runs this NGINX
image as a Docker container, mapping localhost:80 to port 80 of
the NGINX container. It also mounts the local directory /path/to/
content/ as a container volume at /usr/share/nginx/html/ as read only.
The default NGINX configuration will serve this directory as static
content. When specifying mapping from your local machine to a
container, the local machine port or directory comes first, and the
container port or directory comes second.
Discussion
NGINX has made an official Docker image available via Docker
Hub. This official Docker image makes it easy to get up and going
very quickly in Docker with your favorite application delivery plat‐
form, NGINX. In this section, we were able to get NGINX up and
running in a container with a single command! The official NGINX
Docker image mainline that we used in this example is built off of
the Debian Jessie Docker image. However, you can choose official
images built off of Alpine Linux. The Dockerfile and source for
these official images are available on GitHub. You can extend the
official image by building your own Dockerfile and specifying the
official image in the FROM command.
Also See
Official NGINX Docker image, NGINX
Docker repo on GitHub
11.2 Using the Official NGINX Image | 117
11.3 Creating an NGINX Dockerfile
Problem
You need to create an NGINX Dockerfile in order to create a Docker
image.
Solution
Start FROM your favorite distribution’s Docker image. Use the RUN
command to install NGINX. Use the ADD command to add your
NGINX configuration files. Use the EXPOSE command to instruct
Docker to expose given ports or do this manually when you run the
image as a container. Use CMD to start NGINX when the image is
instantiated as a container. You’ll need to run NGINX in the fore‐
ground. To do this, you’ll need to start NGINX with -g "daemon
off;" or add daemon off; to your configuration. This example will
use the latter with daemon off; in the configuration file within the
main context. You will also want to alter your NGINX configuration
to log to /dev/stdout for access logs and /dev/stderr for error logs;
doing so will put your logs into the hands of the Docker daemon,
which will make them available to you more easily based on the log
driver you’ve chosen to use with Docker:
Dockerfile:
FROM centos:7
# Install epel repo to get nginx and install nginx
RUN yum -y install epel-release && \
yum -y install nginx
# add local configuration files into the image
ADD /nginx-conf /etc/nginx
EXPOSE 80 443
CMD ["nginx"]
The directory structure looks as follows:
.
├── Dockerfile
└── nginx-conf
├── conf.d
│ └── default.conf
├── fastcgi.conf
118 | Chapter 11: Containers/Microservices
├── fastcgi_params
├── koi-utf
├── koi-win
├── mime.types
├── nginx.conf
├── scgi_params
├── uwsgi_params
└── win-utf
I chose to host the entire NGINX configuration within this Docker
directory for ease of access to all of the configurations with only one
line in the Dockerfile to add all my NGINX configurations.
Discussion
You will find it useful to create your own Dockerfile when you
require full control over the packages installed and updates. It’s
common to keep your own repository of images so that you know
your base image is reliable and tested by your team before running it
in production.
11.4 Building an NGINX Plus Image
Problem
You need to build an NGINX Plus Docker image to run NGINX
Plus in a containerized environment.
Solution
Use this Dockerfile to build an NGINX Plus Docker image. You’ll
need to download your NGINX Plus repository certificates and keep
them in the directory with this Dockerfile named nginx-repo.crt and
nginx-repo.key, respectively. With that, this Dockerfile will do the
rest of the work installing NGINX Plus for your use and linking
NGINX access and error logs to the Docker log collector.
FROM debian:stretch-slim
LABEL maintainer="NGINX "
# Download certificate and key from the customer portal
# (https://cs.nginx.com) and copy to the build context
COPY nginx-repo.crt /etc/ssl/nginx/
COPY nginx-repo.key /etc/ssl/nginx/
11.4 Building an NGINX Plus Image | 119
# Install NGINX Plus
RUN set -x \
&& APT_PKG="Acquire::https::plus-pkgs.nginx.com::" \
&& REPO_URL="https://plus-pkgs.nginx.com/debian" \
&& apt-get update && apt-get upgrade -y \
&& apt-get install \
--no-install-recommends --no-install-suggests\
-y apt-transport-https ca-certificates gnupg1 \
&& \
NGINX_GPGKEY=573BFD6B3D8FBC641079A6ABABF5BD827BD9BF62;\
found=''; \
for server in \
ha.pool.sks-keyservers.net \
hkp://keyserver.ubuntu.com:80 \
hkp://p80.pool.sks-keyservers.net:80 \
pgp.mit.edu \
; do \
echo "Fetching GPG key $NGINX_GPGKEY from $server"; \
apt-key adv --keyserver "$server" --keyserver-options \
timeout=10 --recv-keys "$NGINX_GPGKEY" \
&& found=yes \
&& break;\
done;\
test -z "$found" && echo >&2 \
"error: failed to fetch GPG key $NGINX_GPGKEY" && exit 1; \
echo "${APT_PKG}Verify-Peer "true";"\
>> /etc/apt/apt.conf.d/90nginx \
&& echo \
"${APT_PKG}Verify-Host "true";">>\
/etc/apt/apt.conf.d/90nginx \
&& echo "${APT_PKG}SslCert \
"/etc/ssl/nginx/nginx-repo.crt";" >> \
/etc/apt/apt.conf.d/90nginx \
&& echo "${APT_PKG}SslKey \
"/etc/ssl/nginx/nginx-repo.key";" >> \
/etc/apt/apt.conf.d/90nginx \
&& printf \
"deb ${REPO_URL} stretch nginx-plus" \
> /etc/apt/sources.list.d/nginx-plus.list \
&& apt-get update && apt-get install -y nginx-plus \
&& apt-get remove --purge --auto-remove -y gnupg1 \
&& rm -rf /var/lib/apt/lists/*
# Forward request logs to Docker log collector
RUN ln -sf /dev/stdout /var/log/nginx/access.log \
&& ln -sf /dev/stderr /var/log/nginx/error.log
EXPOSE 80
STOPSIGNAL SIGTERM
120 | Chapter 11: Containers/Microservices
CMD ["nginx", "-g", "daemon off;"]
To build this Dockerfile into a Docker image, run the following in
the directory that contains the Dockerfile and your NGINX Plus
repository certificate and key:
$ docker build --no-cache -t nginxplus .
This docker build command uses the flag --no-cache to ensure
that whenever you build this, the NGINX Plus packages are pulled
fresh from the NGINX Plus repository for updates. If it’s acceptable
to use the same version on NGINX Plus as the prior build, you can
omit the --no-cache flag. In this example, the new Docker image is
tagged nginxplus.
Discussion
By creating your own Docker image for NGINX Plus, you can con‐
figure your NGINX Plus container however you see fit and drop it
into any Docker environment. This opens up all of the power and
advanced features of NGINX Plus to your containerized environ‐
ment. This Dockerfile does not use the Dockerfile property ADD to
add in your configuration; you will need to add in your configura‐
tion manually.
Also See
NGINX blog on Docker images
11.5 Using Environment Variables in NGINX
Problem
You need to use environment variables inside your NGINX configu‐
ration in order to use the same container image for different envi‐
ronments.
Solution
Use the ngx_http_perl_module to set variables in NGINX from
your environment:
daemon off;
env APP_DNS;
11.5 Using Environment Variables in NGINX | 121
include /usr/share/nginx/modules/*.conf;
...
http {
perl_set $upstream_app 'sub { return $ENV{"APP_DNS"}; }';
server {
...
location / {
proxy_pass https://$upstream_app;
}
}
}
To use perl_set you must have the ngx_http_perl_module
installed; you can do so by loading the module dynamically or stati‐
cally if building from source. NGINX by default wipes environment
variables from its environment; you need to declare any variables
you do not want removed with the env directive. The perl_set
directive takes two parameters: the variable name you’d like to set
and a perl string that renders the result.
The following is a Dockerfile that loads the ngx_http_perl_module
dynamically, installing this module from the package management
utility. When installing modules from the package utility for Cen‐
tOS, they’re placed in the /usr/lib64/nginx/modules/ directory, and
configuration files that dynamically load these modules are placed in
the /usr/share/nginx/modules/ directory. This is why in the preceding
configuration snippet we include all configuration files at that path:
FROM centos:7
# Install epel repo to get nginx and install nginx
RUN yum -y install epel-release && \
yum -y install nginx nginx-mod-http-perl
# add local configuration files into the image