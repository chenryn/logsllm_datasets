### 10.8 Deploying to Google Compute Engine

**Alternative Method:**
You can also install and configure NGINX through the Google Compute Engine startup script, which is an advanced configuration option when creating a VM.

**Discussion:**
Google Compute Engine provides highly configurable virtual machines (VMs) on demand. Starting a VM is straightforward and opens up a wide range of possibilities. Google Compute Engine offers networking and computing in a virtualized cloud environment. With a Google Compute instance, you have the full capabilities of an NGINX server available wherever and whenever you need it.

---

### 10.9 Creating a Google Compute Image

**Problem:**
You need to create a Google Compute Image to quickly instantiate a VM or create an instance template for an instance group.

**Solution:**
1. Create a VM as described in Recipe 10.8.
2. Install and configure NGINX on your VM instance.
3. Set the auto-delete state of the boot disk to `false`:
   - Edit the VM.
   - On the Edit page, under the disk configuration, deselect the checkbox labeled "Delete boot disk when instance is deleted."
   - Save the VM configuration.
4. Delete the instance, ensuring that you do not select the checkbox to delete the boot disk.
5. Once the instance is deleted, you will be left with an unattached boot disk with NGINX installed.
6. Create a Google Compute Image from the unattached boot disk:
   - Go to the Image section of the Google Compute Engine console.
   - Select "Create Image."
   - Provide the image name, family, description, encryption type, and the source disk.
   - The source type should be "disk," and for the source disk, select the unattached NGINX boot disk.
   - Click "Create" to create the image.

**Discussion:**
Using Google Cloud Images allows you to create VMs with a boot disk identical to the server you've just created. The value in creating images is ensuring that every instance of this image is identical. When installing packages at boot time in a dynamic environment, unless using version locking with private repositories, you run the risk of package versions and updates not being validated before being run in a production environment. With machine images, you can validate that every package running on this machine is exactly as you tested, strengthening the reliability of your service offering.

**Also See:**
- [Create, Delete, and Depreciate Private Images](#)

---

### 10.10 Creating a Google App Engine Proxy

**Problem:**
You need to create a proxy for Google App Engine to context switch between applications or serve HTTPS under a custom domain.

**Solution:**
1. Utilize NGINX in Google Compute Engine.
2. Create a virtual machine in Google Compute Engine, or create a virtual machine image with NGINX installed and create an instance template with this image as your boot disk.
3. If you've created an instance template, follow up by creating an instance group that utilizes that template.
4. Configure NGINX to proxy to your Google App Engine endpoint:
   - Ensure you proxy to HTTPS because Google App Engine is public.
   - Use the `proxy_pass` directive to ensure NGINX does DNS resolution on every request.
   - For NGINX to do DNS resolution, use the `resolver` directive and point to a DNS resolver, such as Google's 8.8.8.8.
   - If using NGINX Plus, you can use the `resolve` flag on the `server` directive within the `upstream` block, keepalive connections, and other benefits of the `upstream` module.
5. Optionally, store your NGINX configuration files in Google Storage and use the startup script for your instance to pull down the configuration at boot time. This allows you to change your configuration without having to burn a new image but may add to the startup time of your NGINX server.

**Discussion:**
Running NGINX in front of Google App Engine is useful if you're using your own domain and want to make your application available via HTTPS. At this time, Google App Engine does not allow you to upload your own SSL certificates. Therefore, if you'd like to serve your app under a domain other than appspot.com with encryption, you'll need to create a proxy with NGINX to listen at your custom domain. NGINX will encrypt communication between itself and your clients, as well as between itself and Google App Engine.

Another reason to run NGINX in front of Google App Engine is to host many App Engine apps under the same domain and use NGINX for URI-based context switching. Microservices are a popular architecture, and it’s common for a proxy like NGINX to conduct the traffic routing. Google App Engine makes it easy to deploy applications, and in conjunction with NGINX, you have a full-fledged application delivery platform.

---

### Chapter 11: Containers/Microservices

#### 11.0 Introduction
Containers offer a layer of abstraction at the application layer, shifting the installation of packages and dependencies from the deployment to the build process. This is important because engineers can now ship units of code that run and deploy in a uniform way regardless of the environment. Promoting containers as runnable units reduces the risk of dependency and configuration issues between environments. As a result, there has been a significant push for organizations to deploy their applications on container platforms. When running applications on a container platform, it’s common to containerize as much of the stack as possible, including your proxy or load balancer. NGINX and NGINX Plus containerize and ship with ease, and they include many features that make delivering containerized applications smooth. This chapter focuses on building NGINX and NGINX Plus container images, features that make working in a containerized environment easier, and deploying your image on Kubernetes and OpenShift.

#### 11.1 DNS SRV Records

**Problem:**
You’d like to use your existing DNS SRV record implementation as the source for upstream servers with NGINX Plus.

**Solution:**
Specify the `service` directive with a value of `http` on an upstream server to instruct NGINX to utilize the SRV record as a load-balancing pool:

```nginx
http {
    resolver 10.0.0.2;
    upstream backend {
        zone backends 64k;
        server api.example.internal service=http resolve;
    }
}
```

This feature is exclusive to NGINX Plus. The configuration instructs NGINX Plus to resolve DNS from a DNS server at `10.0.0.2` and set up an upstream server pool with a single `server` directive. The `server` directive specified with the `resolve` parameter is instructed to periodically re-resolve the domain name. The `service=http` parameter tells NGINX that this is an SRV record containing a list of IPs and ports and to load balance over them as if they were configured with the `server` directive.

**Discussion:**
Dynamic infrastructure is becoming increasingly popular with the demand and adoption of cloud-based infrastructure. Autoscaling environments scale horizontally, increasing and decreasing the number of servers in the pool to match the demand of the load. Scaling horizontally demands a load balancer that can add and remove resources from the pool. With an SRV record, you offload the responsibility of keeping the list of servers to DNS. This type of configuration is particularly useful for containerized environments where containers may run applications on variable port numbers, possibly at the same IP address. It’s important to note that UDP DNS record payload is limited to about 512 bytes.

#### 11.2 Using the Official NGINX Image

**Problem:**
You need to get up and running quickly with the NGINX image from Docker Hub.

**Solution:**
Use the official NGINX image from Docker Hub. This image contains a default configuration. You can either mount a local configuration directory or create a Dockerfile and `ADD` your configuration to the image build to alter the configuration. Here, we mount a volume where NGINX’s default configuration serves static content to demonstrate its capabilities using a single command:

```sh
$ docker run --name my-nginx -p 80:80 \
-v /path/to/content:/usr/share/nginx/html:ro -d nginx
```

The `docker` command pulls the `nginx:latest` image from Docker Hub if it’s not found locally. The command then runs this NGINX image as a Docker container, mapping `localhost:80` to port 80 of the NGINX container. It also mounts the local directory `/path/to/content/` as a container volume at `/usr/share/nginx/html/` as read-only. The default NGINX configuration will serve this directory as static content. When specifying mapping from your local machine to a container, the local machine port or directory comes first, and the container port or directory comes second.

**Discussion:**
NGINX has made an official Docker image available via Docker Hub. This official Docker image makes it easy to get up and running quickly in Docker with your favorite application delivery platform, NGINX. In this example, we were able to get NGINX up and running in a container with a single command! The official NGINX Docker image mainline that we used in this example is built off of the Debian Jessie Docker image. However, you can choose official images built off of Alpine Linux. The Dockerfile and source for these official images are available on GitHub. You can extend the official image by building your own Dockerfile and specifying the official image in the `FROM` command.

**Also See:**
- [Official NGINX Docker image](#)
- [NGINX Docker repo on GitHub](#)

#### 11.3 Creating an NGINX Dockerfile

**Problem:**
You need to create an NGINX Dockerfile to create a Docker image.

**Solution:**
1. Start `FROM` your favorite distribution’s Docker image.
2. Use the `RUN` command to install NGINX.
3. Use the `ADD` command to add your NGINX configuration files.
4. Use the `EXPOSE` command to instruct Docker to expose given ports or do this manually when you run the image as a container.
5. Use `CMD` to start NGINX when the image is instantiated as a container. You’ll need to run NGINX in the foreground. To do this, you’ll need to start NGINX with `-g "daemon off;"` or add `daemon off;` to your configuration file within the `main` context.
6. Alter your NGINX configuration to log to `/dev/stdout` for access logs and `/dev/stderr` for error logs. This will put your logs into the hands of the Docker daemon, which will make them available to you more easily based on the log driver you’ve chosen to use with Docker.

Example Dockerfile:
```Dockerfile
FROM centos:7
# Install epel repo to get nginx and install nginx
RUN yum -y install epel-release && \
    yum -y install nginx
# Add local configuration files into the image
ADD /nginx-conf /etc/nginx
EXPOSE 80 443
CMD ["nginx", "-g", "daemon off;"]
```

Directory structure:
```
.
├── Dockerfile
└── nginx-conf
    ├── conf.d
    │   └── default.conf
    ├── fastcgi.conf
    ├── fastcgi_params
    ├── koi-utf
    ├── koi-win
    ├── mime.types
    ├── nginx.conf
    ├── scgi_params
    ├── uwsgi_params
    └── win-utf
```

I chose to host the entire NGINX configuration within this Docker directory for ease of access to all configurations with only one line in the Dockerfile to add all my NGINX configurations.

**Discussion:**
Creating your own Dockerfile is useful when you require full control over the packages installed and updates. It’s common to keep your own repository of images so that you know your base image is reliable and tested by your team before running it in production.

#### 11.4 Building an NGINX Plus Image

**Problem:**
You need to build an NGINX Plus Docker image to run NGINX Plus in a containerized environment.

**Solution:**
Use the following Dockerfile to build an NGINX Plus Docker image. You’ll need to download your NGINX Plus repository certificates and keep them in the directory with this Dockerfile named `nginx-repo.crt` and `nginx-repo.key`, respectively. With that, this Dockerfile will do the rest of the work installing NGINX Plus for your use and linking NGINX access and error logs to the Docker log collector.

```Dockerfile
FROM debian:stretch-slim
LABEL maintainer="NGINX"

# Download certificate and key from the customer portal
# (https://cs.nginx.com) and copy to the build context
COPY nginx-repo.crt /etc/ssl/nginx/
COPY nginx-repo.key /etc/ssl/nginx/

# Install NGINX Plus
RUN set -x \
    && APT_PKG="Acquire::https::plus-pkgs.nginx.com::" \
    && REPO_URL="https://plus-pkgs.nginx.com/debian" \
    && apt-get update && apt-get upgrade -y \
    && apt-get install \
    --no-install-recommends --no-install-suggests\
    -y apt-transport-https ca-certificates gnupg1 \
    && \
    NGINX_GPGKEY=573BFD6B3D8FBC641079A6ABABF5BD827BD9BF62;\
    found=''; \
    for server in \
    ha.pool.sks-keyservers.net \
    hkp://keyserver.ubuntu.com:80 \
    hkp://p80.pool.sks-keyservers.net:80 \
    pgp.mit.edu \
    ; do \
    echo "Fetching GPG key $NGINX_GPGKEY from $server"; \
    apt-key adv --keyserver "$server" --keyserver-options \
    timeout=10 --recv-keys "$NGINX_GPGKEY" \
    && found=yes \
    && break;\
    done;\
    test -z "$found" && echo >&2 \
    "error: failed to fetch GPG key $NGINX_GPGKEY" && exit 1; \
    echo "${APT_PKG}Verify-Peer "true";"\
    >> /etc/apt/apt.conf.d/90nginx \
    && echo \
    "${APT_PKG}Verify-Host "true";">>\
    /etc/apt/apt.conf.d/90nginx \
    && echo "${APT_PKG}SslCert \
    "/etc/ssl/nginx/nginx-repo.crt";" >> \
    /etc/apt/apt.conf.d/90nginx \
    && echo "${APT_PKG}SslKey \
    "/etc/ssl/nginx/nginx-repo.key";" >> \
    /etc/apt/apt.conf.d/90nginx \
    && printf \
    "deb ${REPO_URL} stretch nginx-plus" \
    > /etc/apt/sources.list.d/nginx-plus.list \
    && apt-get update && apt-get install -y nginx-plus \
    && apt-get remove --purge --auto-remove -y gnupg1 \
    && rm -rf /var/lib/apt/lists/*

# Forward request logs to Docker log collector
RUN ln -sf /dev/stdout /var/log/nginx/access.log \
    && ln -sf /dev/stderr /var/log/nginx/error.log

EXPOSE 80
STOPSIGNAL SIGTERM
CMD ["nginx", "-g", "daemon off;"]
```

To build this Dockerfile into a Docker image, run the following in the directory that contains the Dockerfile and your NGINX Plus repository certificate and key:

```sh
$ docker build --no-cache -t nginxplus .
```

This `docker build` command uses the `--no-cache` flag to ensure that whenever you build this, the NGINX Plus packages are pulled fresh from the NGINX Plus repository for updates. If it’s acceptable to use the same version of NGINX Plus as the prior build, you can omit the `--no-cache` flag. In this example, the new Docker image is tagged `nginxplus`.

**Discussion:**
By creating your own Docker image for NGINX Plus, you can configure your NGINX Plus container however you see fit and drop it into any Docker environment. This opens up all of the power and advanced features of NGINX Plus to your containerized environment. This Dockerfile does not use the `ADD` property to add in your configuration; you will need to add in your configuration manually.

**Also See:**
- [NGINX blog on Docker images](#)

#### 11.5 Using Environment Variables in NGINX

**Problem:**
You need to use environment variables inside your NGINX configuration to use the same container image for different environments.

**Solution:**
Use the `ngx_http_perl_module` to set variables in NGINX from your environment:

```nginx
daemon off;
env APP_DNS;

include /usr/share/nginx/modules/*.conf;

http {
    perl_set $upstream_app 'sub { return $ENV{"APP_DNS"}; }';
    server {
        ...
        location / {
            proxy_pass https://$upstream_app;
        }
    }
}
```

To use `perl_set`, you must have the `ngx_http_perl_module` installed. You can do so by loading the module dynamically or statically if building from source. NGINX by default wipes environment variables from its environment; you need to declare any variables you do not want removed with the `env` directive. The `perl_set` directive takes two parameters: the variable name you’d like to set and a Perl string that renders the result.

The following is a Dockerfile that loads the `ngx_http_perl_module` dynamically, installing this module from the package management utility. When installing modules from the package utility for CentOS, they’re placed in the `/usr/lib64/nginx/modules/` directory, and configuration files that dynamically load these modules are placed in the `/usr/share/nginx/modules/` directory. This is why in the preceding configuration snippet, we include all configuration files at that path:

```Dockerfile
FROM centos:7
# Install epel repo to get nginx and install nginx
RUN yum -y install epel-release && \
    yum -y install nginx nginx-mod-http-perl

# Add local configuration files into the image
...
```

**Discussion:**
Using environment variables in NGINX allows you to use the same container image across different environments by dynamically setting values at runtime. This approach is particularly useful for managing different configurations in development, staging, and production environments.