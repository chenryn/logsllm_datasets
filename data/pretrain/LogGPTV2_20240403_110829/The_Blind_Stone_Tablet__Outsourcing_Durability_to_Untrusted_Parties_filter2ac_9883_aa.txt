title:The Blind Stone Tablet: Outsourcing Durability to Untrusted Parties
author:Peter Williams and
Radu Sion and
Dennis E. Shasha
The Blind Stone Tablet: Outsourcing Durability to Untrusted Parties
Peter Williams, Radu Sion ∗
Dennis Shasha †
Network Security and Applied Cryptography Lab
Department of Computer Science
Stony Brook University
{petertw,sion}@cs.stonybrook.edu
New York University
PI:EMAIL
Abstract
We introduce a new paradigm for outsourcing the dura-
bility property of a multi-client transactional database to
an untrusted service provider. Speciﬁcally, we enable un-
trusted service providers to support transaction serializa-
tion, backup and recovery for clients, with full data conﬁ-
dentiality and correctness. Moreover, providers learn noth-
ing about transactions (except their size and timing), thus
achieving read and write access pattern privacy.
We build a proof-of-concept implementation of this pro-
tocol for the MySQL database management system, achiev-
ing tens of transactions per second in a two-client scenario
with full transaction privacy and guaranteed correctness.
This shows the method is ready for production use, creating
a novel class of secure database outsourcing models.
1 Introduction
Increasingly, data management is outsourced to third par-
ties. This trend is driven by growth and advances in cheap,
high-speed communication infrastructures as well as by the
fact that the total cost of data management is 5–10 times
higher than the initial acquisition costs [26].
Outsourcing has the potential
to minimize client-
side management overheads and beneﬁt from a service
provider’s global expertise consolidation and bulk pricing.
Providers such as Yahoo [10], Amazon [1–3], Google [4],
Sun [9] and others – ranging from corporate-level services
such as IBM Data Center Outsourcing Services [5] to per-
sonal level database hosting [6, 8] – are rushing to offer in-
creasingly complex storage and computation services.
∗The authors are supported in part by the NSF through awards CNS-
0627554, CNS-0716608, CNS-0708025 and IIS-0803197. The authors
also wish to thank Xerox/Parc, Motorola Labs, IBM Research, the IBM
Software Cryptography Group, CEWIT, and the Stony Brook Ofﬁce of the
Vice President for Research.
†Shasha’s work has been partly supported by the U.S. National Sci-
ence Foundation under grants IIS-0414763, DBI-0445666, N2010 IOB-
0519985, N2010 DBI-0519984, DBI-0421604, and MCB-0209754. This
support is greatly appreciated.
Yet, signiﬁcant challenges lie in the path of a success-
ful large-scale adoption. In business, health care and gov-
ernment frameworks, clients are reluctant to place sensitive
data under the control of a remote, third-party provider,
without practical assurances of privacy and conﬁdential-
ity. Yet today’s privacy guarantees of such services are at
best declarative and often subject customers to unreason-
able ﬁne-print clauses – e.g., allowing the server operator
(or malicious attackers gaining access to its systems) to use
customer behavior and content for commercial, proﬁling,
or governmental surveillance purposes [21, 22]. These ser-
vices are thus fundamentally insecure and vulnerable to il-
licit behavior.
Existing research (discussed in Section 2) addresses sev-
eral important outsourcing aspects, including direct data re-
trieval with access privacy, searches on encrypted data, and
techniques for querying remotely-hosted encrypted struc-
tured data in a uniﬁed client model [24, 55]. These efforts
are based on the assumption that, to achieve conﬁdential-
ity, data will need to be encrypted before outsourcing to an
untrusted provider. Once encrypted however, inherent lim-
itations in the types of primitive operations that can be per-
formed on encrypted data by untrusted hosts lead to funda-
mental query expressiveness constraints. Speciﬁcally, rea-
sonably practical mechanisms exist only for simple selec-
tion and range queries or variants thereof.
In this paper, we introduce an orthogonal thesis spurred
by the advent of cheap and fast disks and CPUs. We believe
that in many deployments, individual data clients’ systems
are in a position to host and access local large data sets with
little difﬁculty at no additional cost.
Holding data locally may appear to contradict the spirit
of outsourcing, yet we argue that there is one essential data
management aspect that cannot be hosted as such: transac-
tion processing for multiple concurrent clients. In a world
where client machines have become powerful enough to run
local databases, we predict data management outsourcing
markets will converge on supplying the transactional, net-
work intensive services and availability assurances which
are not trivially achievable locally.
In this paper, we thus introduce a novel paradigm for
solving the data management outsourcing desiderata: a
mechanism for collaborative transaction processing with
durability guarantees supported by an untrusted service
provider under assurances of conﬁdentiality and access pri-
vacy.
In effect, we achieve the cost beneﬁts of standard
outsourcing techniques (durability, transaction processing,
availability) while preserving the privacy guarantees of lo-
cal data storage. This is accomplished by enabling data
clients to collaboratively perform runtime transaction pro-
cessing and interact through an untrusted service provider
that offers durability and transaction serializability support.
In this context, data outsourcing becomes a setting in
which all permanent data is hosted securely encrypted off-
site, yet clients access it through their locally-run database
effectively acting as a data cache. If local data is lost, it
can be retrieved from the offsite repository. Inter-client in-
teraction and transaction management is intermediated by
the untrusted provider who also ensures durability by main-
taining a client-encrypted and authenticated transaction log
with full conﬁdentiality.
In our model, each client maintains its own cache of
(portions of) the database in client-local storage, allowing
it to perform efﬁcient reads with privacy, while relieving
local system administrators of backup obligations. The key
beneﬁt thus becomes achieving data and transaction privacy
while (1) avoiding the requirement for persistent client stor-
age (clients are now allowed to fail or be wiped out at any
time), and (2) avoiding the need to keep any single client-
side machine online as a requirement for availability.
2 Related Work
Queries on Encrypted Data. The paradigm of providing
a database as a service recently emerged [34] as a viable
alternative, likely due in no small part to the dramatically
increasing availability of fast, cheap networks. Given the
global, networked, possibly hostile nature of the operation
environments, security assurances are paramount.
Hacigumus et al.[33] propose a method to execute SQL
queries over partly obfuscated outsourced data. The data
is divided into secret partitions and queries over the origi-
nal data can be rewritten in terms of the resulting partition
identiﬁers; the server then performs queries over the parti-
tions. The information leaked to the server is claimed to
be 1-out-of-s where s is the partition size. This balances a
trade-off between client-side and server-side processing, as
a function of the data segment size. At one extreme, privacy
is completely compromised (small segment sizes) but client
processing is minimal. At the other extreme, a high level
of privacy can be attained at the expense of the client pro-
cessing the queries in their entirety after retrieving the en-
tire dataset. Moreover, in [37] the authors explore optimal
bucket sizes for certain range queries. Similarly, data parti-
tioning is deployed in building “almost”-private indexes on
attributes considered sensitive. An untrusted server is then
able to execute “obfuscated range queries with minimal in-
formation leakage”. An associated privacy-utility trade-off
for the index is discussed. The main drawbacks of these
solutions lies in their computational impracticality and in-
ability to provide strong conﬁdentiality.
Recently, Ge et al.[67] discuss executing aggregation
queries with conﬁdentiality on an untrusted server. Unfor-
tunately, due to the use of extremely expensive homomor-
phisms (Paillier [58, 59]) this scheme leads to impractically
large processing times for any reasonable security parame-
ter choices (e.g., for 1024 bit of security, processing would
take over 12 days per query). Current homomorphisms are
not fast enough to be usable for practical data processing.
Avoiding the tradeoff between processing and computa-
tion time altogether, we allow efﬁcient queries on encrypted
data with full privacy by running queries on a client-side de-
crypted copy of the data. Thus, with no additional network
transfer, and with a computational cost equivalent to run-
ning the query on an unencrypted database, we provide full
query privacy. Since we run the queries on a copy of the
database at the client side, so there is no need for expensive
homomorphisms, either.
Query Correctness. In a publisher-subscriber model, De-
vanbu et al.deployed Merkle trees to authenticate data pub-
lished at a third party’s site [24], and then explored a gen-
eral model for authenticating data structures [49, 50]. Hard-
to-forge veriﬁcation objects are provided by publishers to
prove the authenticity and provenance of query results. In
[55], mechanisms for efﬁcient integrity and origin authenti-
cation for simple selection predicate query results are intro-
duced. Different signature schemes (DSA, RSA, Merkle
trees [53] and BGLS [17]) are explored as potential al-
ternatives for data authentication primitives. Mykletun et
al.[25] introduce signature immutability for aggregate sig-
nature schemes – the difﬁculty of computing new valid
aggregated signatures from an existing set. Such a prop-
erty is defeating a frequent querier that could eventually
gather enough signatures data to answer other (un-posed)
queries. The authors explore the applicability of signature-
aggregation schemes for efﬁcient data authentication and
integrity of outsourced data. The considered query types
are simple selection queries. Similarly, in [47], digital sig-
nature and aggregation and chaining mechanisms are de-
ployed to authenticate simple selection and projection op-
erators. While these are important to consider, neverthe-
less, their expressiveness is limited. A more comprehen-
sive, query-independent approach is desirable. Moreover,
the use of strong cryptography renders this approach less
useful. Often simply transferring the data to the client side
will be faster. In [60] veriﬁcation objects VO are deployed
to authenticate simple data retrieval in “edge computing”
scenarios, where application logic and data is pushed to
the edge of the network, with the aim of improving avail-
ability and scalability. Lack of trust in edge servers man-
dates validation for their results – achieved through veriﬁca-
tion objects. In [38] Merkle tree and cryptographic hashing
constructs are deployed to authenticate the result of simple
range queries in a publishing scenario in which data owners
delegate the role of satisfying user queries to a third-party
un-trusted publisher. Additionally, in [48] virtually identi-
cal mechanisms are deployed in database outsourcing sce-
narios. [23] proposes an approach for signing XML docu-
ments allowing untrusted servers to answer certain types of
path and selection queries.
Sion has explored query correctness by considering the
query expressiveness problem in [64] where a novel method
for proofs of actual query execution in an outsourced
database framework for arbitrary queries is proposed. The
solution is based on a mechanism of runtime query “proofs”
in a challenge - response protocol built around the ringer
concept ﬁrst introduced in [31]. For each batch of client
queries, the server is “challenged” to provide a proof of
query execution that offers assurance that the queries were
actually executed with completeness, over their entire target
data set. This proof is veriﬁed at the client site as a prereq-
uisite to accepting the actual query results as accurate.
While many of these efforts have produced efﬁcient
query correctness veriﬁcation mechanisms for speciﬁc
kinds of queries, unique approaches are required for dif-
ferent queries. Moreover, it has proven difﬁcult to simul-
taneously provide correctness and privacy with these tech-
niques, since the construction of a query veriﬁcation object
typically requires knowledge of the query. To simultane-
ously ensure query correctness, query privacy, and database
privacy for fully general queries (on relational or other types
of databases), we instead verify only the updates. We guar-
antee that clients have correct views of the database, thus
ensuring they also obtain correct query results.
Database Integrity and Audit Logs.
In a different ad-
versarial and deployment model, researchers have also pro-
posed techniques for protecting critical DBMS structures
against errors [46, 61]. These techniques deal with cor-
ruptions caused by software errors.
In work on tamper
proof audit logs by Snodgrass et al.[45, 63] introduces the
idea of hashing transactional data with cryptographically
strong one-way hash functions. This hash is periodically
signed by a trusted external digital notary, and stored within
the DBMS. A separate validator checks the database state
against these signed hashes to detect any compromise of
the audit log. If tampering is detected, a separate forensic
analyzer springs into action, using other hashes that were
computed during previous validation runs to pinpoint when
the tampering occurred and roughly where in the database
the data was tampered. The use of a notary prevents an ad-
versary, even an auditor or a buggy DBMS, from silently
corrupting the database.
This notion of a cryptographically protected log provides
the framework for our solution. Rather than using an au-
dit log to identify errors, however, we use an update log
stored by an untrusted party as the authoritative version of
the database; the cryptographic hash properties are used to
prevent tampering by the untrusted party. Our log does not
protect from software bugs in the DBMS, as audit logs tra-
ditionally do, since our logs store only a list of updates, not
checksums on the contents.
Databases Replication. Database replication has been
pursued as a means to improve query latency and through-
put, and eliminate any single point of failure. Researchers
have found efﬁcient solutions guaranteeing various levels of
consistency. In particular, [41], and later [13], build repli-
cated databases on top of existing RDBMS’s using a reliable
group communication system with total message ordering
and (in [13]) message logging.
We employ a similar model of operation, in which clients
simulate transactions locally before distributing them.
In
place of the group communication system, however, we
use a dedicated, untrusted provider, and provide crypto-
graphic guarantees of security and privacy. This allows
us to remove the availability and write durability require-
ments from the replication servers in previous work, assign-
ing them to the untrusted provider, making client system
administration much simpler.
Encrypted Storage. Encryption is one of the most com-
mon techniques used to protect the conﬁdentiality of stored
data. Several existing systems encrypt data before storing it
on potentially vulnerable storage devices or network nodes.
Blaze’s CFS [14], TCFS [19], EFS [54], StegFS [52], and
NCryptfs [69] are ﬁle systems that encrypt data before writ-
ing to stable storage. NCryptfs is implemented as a layered
ﬁle system [35] and is capable of being used even over net-
work ﬁle systems such as NFS. SFS [32] and BestCrypt [39]
are device driver level encryption systems. Encryption ﬁle
systems are designed to protect the data at rest, yet only par-
tially solve the outsourcing problem. They do not allow for
complex retrieval queries or client access privacy.
Integrity-Assured Storage. Tripwire [42, 43] is a user
level tool that veriﬁes ﬁle integrity at scheduled intervals
of time. File systems such as I3FS [40], GFS [27], and
Checksummed NCryptfs [65] perform online real-time in-
tegrity veriﬁcation. Venti [62] is an archival storage system
that performs integrity assurance on read-only data. Myk-
letun et al.[56, 57] explore the applicability of signature-
aggregation schemes to provide computation- and commu-
nication efﬁcient data authentication and integrity of out-
sourced data.
In integrity-protected random-access storage, signiﬁcant
computational overhead is typically required to prevent roll-
back attacks, in which an adversary replaces a portion of
the data with an older version. The party intending to detect
such an attack may need a proof of the latest versions iden-
tiﬁer of all stored data, for example. We avoid this overhead
since each client keeps track of this information in a locally
stored copy. We are then left with only the task of ensur-
ing a consistent version sequencing on updates, which we
provide using a hash chain.
Keyword Searches on Encrypted Data. Song et al.[66]
propose a scheme for performing simple keyword search