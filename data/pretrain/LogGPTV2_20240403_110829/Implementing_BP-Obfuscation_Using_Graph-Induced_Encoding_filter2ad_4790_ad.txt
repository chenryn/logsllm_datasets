. Let Σ′ := Σ[k−1],[k−1]. We assume for induc-
as DZ,
tion that the first k −1 coordinates of (cid:174)z, which we call (cid:174)z ′, are within
statistical distance (k − 1) · 2−S of DZk−1
Σ′. If this were exactly
the distribution of (cid:174)z ′, then for any fixed integer vector (cid:174)y ∈ Zk with
first k coordinates (cid:174)y ′, we would have
Σ′((cid:174)y ′)
ρ√
ρ√
Σ′(Zk−1)
(yk − µk)
· ρ√
Sk
(Z − µk) .
ρ√
Sk
′ and zk = yk] =
= DZ,
′ = (cid:174)y
Pr[(cid:174)z
√
S1
√
Σ1,1
Σ,
√
√
,
Session D1:  Functional Encryption and ObfuscationCCS’17, October 30-November 3, 2017, Dallas, TX, USA789Here, the ρ functions are multi-dimensional Gaussians. By Claim 4.4,
Σ[k],[k]((cid:174)y). And, by Lemma 4.1,
we have ρ√
we have that
(yk−µk) = ρ√
Σ′((cid:174)y ′)·ρ√
Sk
(1 − 2−Sk ) · ρ√
Sk
(Z) ≤ ρ√
Sk
(Z − µk) ≤ ρ√
Sk
(Z) .
Therefore, Pr[(cid:174)z ′ = (cid:174)y ′ and zk = yk] is within a factor of 1− 2−Sk of
Σ[k],[k]((cid:174)y)/ρ√
ρ√
Σ[k],[k](Zk). It follows that the real distribution of
the first k coordinates of (cid:174)z is within statistical distance (k − 1)2−S +
2−Sk ≤ k2−S of DZk,
□
Finally, we note that we can relate the Schur complement to the
eigenvalues of Σ, which makes it easier to compare the performance
of our sampler with prior work, such as [13, 19].
, as needed.
Σ[k],[k]
√
Lemma 4.3. [17, Corollary 2.4] For any positive-definite matrix
Σ ∈ Rn×n, σn(Σ) ≤ Sk ≤ σ1(Σ) for all k = 1, . . . , n, where σi is the
ith largest eigenvalue of Σ.
√
Corollary 4.4. The output of the procedure described in The-
orem 4.2 is within statistical distance n2−σn(Σ) of DZn,
Σ, where
σn(Σ) is the smallest eigenvalue of Σ.
4.5 Using a Stash of Samples
Our G-Sampling Routine (see Section 4.3) requires us to sample from
DpZ+u,σ for a factor p taken from the short list in Section 4.2 and
an input integer u. For the small factors p that we use (all between
71 and 181), this is done by repeatedly sampling short integers
from DZ,σ until we hit one that satisfies t = u (mod p). A naive
implementation of this procedure will need to sample p/2 > 50
times (on the average) before it hits a suitable t, making for a very
slow implementation. In our tests, this naive implementation spent
about 30% of the obfuscation time sampling 1D Gaussians.
To do better, we keep for each factor p a stash of unused samples.
Whenever we need a new sample we first check the stash. Only if
there are no hits in the stash do we sample new points, and all the
points which are not equal to u modulo p are then stored in the stash
for future use. (This is similar to the “bucketing” approach described
in [18, Sec. 4.1], but without the online/offline distinction.)
The stash itself is implemented as a simple size-p array of in-
tegers, where stashp[i] contains the latest sample that was equal
to i modulo p (if that sample was not yet used). An alternative
approach would be to keep for each entry i a queue of all sample
values satisfying x = i (mod p), but such an implementation would
be considerably more complex.
It is easy to see that the use of stash does not bias the distribu-
tion from which we sample: by definition each non-empty entry
j contains a random element x ← D, constrained only by x = j
(mod pi). (Note that we use a separate stash for each factor pi.)
As we show now, that simple stash implementation already
reduces the required number of trials per sample from p/2 to ≈
√2p, reducing the sampling time from 30% to about 3% of the total
running time. To see how the simple implementation above reduces
the overhead, denote by f the fraction of full entries in the stash
just prior to a sample operation. The expected change in the number
of full entries after the sample operation (where u mod p is uniform
8
in Zp) is described by the formula
1
E[ch] = f ·(−1) +(1− f )·((1− f )·p−1)/2 =
2 ·((1− f )2·p−1− f ) ,
where the second term follows from the fact that each empty entry
other than i has 1/2 probability of being filled before we sample
a match. Assuming that the sampler reaches a steady state (with
expected change equal to zero), the value of f ∈ [0, 1] is
1 + 2p − √1 + 8p
f =
2p
= 1 −(cid:112)2/p + Θ(1/p) .
The expected work per sampling operation is therefore f · 1 + (1 −
f ) · p ≈ √2p.
Thread-safety of our stash implementation. One important reason
that we chose to implement the stash using the simple procedure
above (rather than implementing a full queue per entry) is that
it is easier to make it thread-safe. Implementing a queue per en-
try would require the use of semaphores to coordinate between
the threads, whereas having only one integer per entry lets us
use simple atomic test-and-set operations (implemented via the
C++11’s atomic type). Since the simple implementation al-
ready reduced the overhead to about 3%, we did not attempt the
more complicated one.
5 SETTING THE PARAMETERS
In setting the parameters, we try to minimize the dimension m and
the bit size of the modulus q, subject to functionality and security.
For security, we need the dimensions m, ¯m to be large enough rela-
tive to q, so that the relevant computational problems are hard ( ¯m
is the number of rows in the trapdoor matrix R). For functionality,
we need q to be sufficiently larger than the largest noise compo-
nent that we get, and we also need the large dimension m to be
sufficiently large (relative to n log q) for our trapdoor sampling pro-
cedure. These security and functionality considerations imply a set
of constraints, described in Equations (7) through (10) below, and
our implementation searches for the smallest values that satisfy all
these constraints.
5.1 Functionality
Basic facts. We use the bound from [18, Lemma 2.9] on the sin-
gular values of random matrices. Namely, if the entries of an a-by-b
matrix X are chosen from a Gaussian with parameter σ, then the
largest singular value of X (denoted s(X)) is bounded by
Pr(cid:104)
s(X) > const · (√
b + t)(cid:105)
√
 σ
2
z
I
I
σz = r · max
i
(pi) = 4 · 181 = 724
(6)
Using Eqn. (5), the largest singular value of our ¯m-by-w matrix R
(with entries chosen with Gaussian parameter r) is bounded whp
by s = r · (√ ¯m +
√
w + 6) (to get 2−36 error probability). Hence to
ensure σx > σz · s it is enough to set
(pi))·(r ·(√ ¯m +
w +6)) ≈ 2900·(√ ¯m +
√
√
σx > (r ·max
w +6). (7)
i
As usual when setting parameters for lattice-based system, there
is some weak circularity here since ¯m, w depend on the size of q,
which in turn depends on the output size our sampling procedure,
that depends on σx . But this circular dependence is very weak,
and it is easy to find a solution that satisfies all the constraints.
For example, in our largest setting L = 20 we have ¯m ≈ 6000 and
w ≈ 8000, for which the above bound yields σx ≈ 218.9.
The modulus q. The vectors (cid:174)x that are output by the trapdoor
sampling procedure (which are drawn from a spherical Gaussian
with parameter σx over some coset in Zm) form the columns of the
GGH15 encoding matrices C before the outer transformation of the
GGH15 “safeguards”. As explained in Section 3.1, the noise term
when we multiply L encodings is
L
(cid:0) j−1
j=1
i =1
noise =
Mi
(cid:1)Pj−1Ej
(cid:0)
L
Ci
i =j+1
(cid:1)
where the Mi’s are the “plaintext matrices” that we encode, the
Pi’s are the inner transformation matrices used in the GGH15 “safe-
guards”, the Ei’s are the error matrices that we choose, and the Ci’s
obtained using our trapdoor sampling procedure. Since the Ci’s are
much larger than the other matrices in this expression, the only
relevant term in this sum is the first one, namely E1 ×L
i =2 Ci.
E1 ×L
Below we use the largest singular value of the matrix product
i =2 Ci to represent its “size”. By Eqn. (5) the singular values
of all the Ci’s are bounded by σx(2√
√
m, and that
of E is bounded by 27(√
m. (Each entry of E
is chosen from a Gaussian with parameter 27.) Therefore we can
heuristically bound the largest singular value of the product by
27 · 2L−1 · mL/2 · σ L−1
. For our zero-test we check that the noise is
no more than q/210, so we need q to be 210 times larger than this
bound, or in other words:
n + 6) ≈ 27√
√
m + 6) ≈ 2σx
m +
x
(8)
i =1 pe
i
log2 q ≥ 7 + log2 σx · (L − 1) + log2 m · L/2 + (L − 1) + 10.
factors so that the productk
Once we have a bound on q we choose the number k of co-prime
exceeds that bound. The pa-
rameter e depends on the hardware architecture: For performance
reasons we always use e = 3 when running on a platform with Intel
factors are less than 23 bits long, see Section 6),
AVX (so all the pe
i
’s are
and on platforms without Intel AVX we use e = 8 (so the pe
i
just under 60 bits long).
For our largest parameters (with L = 20 and σx ≈ 218.9) we need
to set m ≈ 213.8 for security breasons (see below). Hence we set
log2 q ≥ 7 + 18.9 · 19 + 13.8 · 10 + 29 ≈ 535, and with e = 3 we need
k = 26 co-prime factors.
The large dimension m. To be able to generate trapdoors, we must
also ensure that the parameters m (number of columns in A) is large
enough. Specifically, for a given lower bound ¯m on the number of
columns in ¯A (obtained by security considerations), and given the
parameters k (number of co-prime factors), e (number of times each
factor repeats — either 3 or 8), and n (the dimension of “plaintext
matrices” to encode), we need to ensure that m ≥ nke + ¯m. (In all
cases, the bound that we get on m due to security considerations
was larger than this functionality-based bound.)
I
(cid:17)
A ×(cid:16) R
5.2 Security
The trapdoor dimension ¯m. Recall that trapdoor generation chooses
a uniform ¯A and small R, then sets A = [ ¯A|G − ¯AR] mod q (so that
= G (mod q)). We would like A to be random, so we need
to argue that G − ¯AR is nearly uniform, even conditioned on ¯A. This
is typically done by appealing to the leftover hash lemma, but doing
so requires that each column of R has more than n log2 q bits of
min-entropy. In our implementation the entries of R have constant
magnitude, so to use the leftover hash lemma we need R to have at
least Ω(n log2 q) rows (and of course ¯A must have the same number
of columns).
Micciancio and Peikert observed in [18] that we can get by with
lower-dimension R if we are willing to have A pseudorandom (under
LWE) rather than random. Splitting ¯A into two parts ¯A = [ ¯A1| ¯A2],
and denoting the corresponding two parts of R by R =
assuming that ¯A2 is square and invertible), we have that
(cid:17) (and
(cid:16) R1
R2
AR = A1R1 + A2R2 = ¯A2