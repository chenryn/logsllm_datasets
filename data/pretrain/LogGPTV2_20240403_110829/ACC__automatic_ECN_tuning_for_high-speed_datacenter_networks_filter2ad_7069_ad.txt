tively adjust ECN marking threshold to maintain high throughput
of RDMA traffic for making full use of the allocated bandwidth
resources. This result also works for network with 25G links. Specif-
ically, ACC achieves up to 65.1% (average RTT) and 25.3% (99th
RTT) lower latency than SECN, i.e. ACC can achieve the low latency
while maintaining high throughput in coexistence with TCP traffic.
5.3 Macro-benchmark
To evaluate the performance of ACC under realistic application,
we test ACC under distributed SSD-storage system and distributed
GPU training system.
5.3.1 Distributed storage system. In the cluster of distributed stor-
age, servers are divided into computing nodes and storage nodes,
which are deployed in a ratio of 3:1. The macro-benchmark consists
of 18 servers as computing nodes and 6 servers as storage nodes.
Computing nodes send IO requests (Read Data/Write Data) to stor-
age nodes, while storage nodes backup data, and response to the
request of computing nodes. We measure the performance based
on the IOPS (I/O operations per second) metric, which is affected
by both network throughput and latency. We compare ACC with
fixed ECN setting, Kmax = 270KB, Kmin = 30KB, p = 10%, which
are suggested by the device vendor.
Table 1: Traffic loads in distributed storage system3
Traffic Pattern Read-Write Ratio Blocksize(IO size)
OLTP
OLAP
VDI
Exchange Server
Video Streaming
File Backup
5:5
5:5
2:8
6:4
2:8
4:6
512B-64KB
256KB-4MB
1KB-64KB
32KB-512KB
64KB
16KB-64KB
Traffic Loads. We use FIO [15] to produce realistic workloads of
distributed storage (Table 1), which is monitored and abstracted
from the trace of large scale cloud storage system for the last five
years. The traffic models are identified based on the following
characteristics: read-write ratio, block size distribution, IOdepth
concurrency4. For example, OLTP includes fast transaction query
of data less than 64 KB. OLAP includes complex data analysis, the
blocksize of which varies from hundreds of kilobytes to several
megabytes.
Summary of Results. As illustrated in Figure 9, ACC improves
by up to 30% application performance of the distributed storage
cluster. Take V DI as an example (Figure 9(c)). ACC achieves better
IOPS performance especially for large IOdepth. For IOdepth of 16,
ACC increases 5% IOPS. For IOdepth of 128, ACC increases 15.3%
IOPS. The gap between SECN and ACC increases with the increase
of IOdepth. For the FileBackup (Figure 9(f)), the improvement of
IOPS for ACC is as large as 30% compared to the static ECN setting.
We notice that there are some cases where ACC achieves little
performance gain in comparison with SECN, e.g. the OLT P, V DI,
and FileBackup with low IO depth and small IO size. We observe
almost no ECN-marked packets. It happens due to the low probabil-
ity of collision in switch when senders have low concurrency and
flow rate. We observe the performance gain with the increase of IO
depth. With more IO depth and tasks, the storage system becomes
overload and leads to degrade performance (Figure 9(b) and (d)).
5.3.2 Distributed training system. To illustrate the efficiency of
ACC in GPU clusters, we use 8 servers with GPU P100 (7 servers
as workers and 1 server as parameter server) to train AlexNet and
ResNet-50 models (batchSize=64), respectively. Meanwhile we use
the training speed as metric to evaluate the performance of ACC.
As shown in Figure 10(a), ACC outperforms SECN1 and SECN2 for
distributed training. For example in ResNet-50, ACC achieves up to
7% and 12% higher training speed. Besides, Figure 10(b) shows that
ACC achieves the low round trip latency, which benefits to the small
messages, such as control packets. Note that ACC also improves
the link utilization for better communication of big messages.
In distributed training, the traffic patterns of communication
repeatedly occurs in each training iteration. ACC’s NN has the
experience memory. In the following iterations it can conserve
quickly to adjust the ECN when traffic with similar patterns arrive
and achieve good performance.
5.4 Large Scale Simulation
In this section, we use NS3 [2] simulation to evaluate ACC’s per-
formance in large-scale DCNs.
Setup. We use a 288-host leaf-spine topology with 12 leaf switches
and 6 spine switches. Each leaf switch has 24 25Gbps links con-
necting to the servers and 6 100Gbps links connecting to the spine
switches. We generate traffic based on two realistic workloads, as
shown in Figure 11: Web Search [7] and Data Mining [22]. Both
workloads are heavy tailed.
Overall. The average FCT of all flows is calculated, which is ex-
pressed as overall average FCT. As shown in Figure 12(a), compared
to SECN1, ACC achieves 5.8% lower overall average FCT at 90% load.
Compared to SECN2, ACC achieves 16.6% lower overall average
FCT at 90% load. This is because ACC can maintain high throughput
3OLTP: Online transaction processing, OLAP: Online Analytical Processing, VDI:
Virtual desktop infrastructure, Exchange server: Mail reading/writing processing,
Video Streaming: Upload and download of videos, File Backup: Large file backup
4Read traffic is mainly from storage nodes to computing nodes and write traffic is
in the reverse path. Block size indicates the size of the data read or write by one IO
request (equal to the flow size). IO depth means the number of outstanding I/O requests
392
SIGCOMM ’21, August 23–28, 2021, Virtual Event, USA
Yan et al.
(a) OLTP
(b) OLAP
(c) VDI
(a) Training Speed
(d) Exchange Server
(e) Video Stream
(f) File Backup
Figure 9: Comparisons in different distributed storage application
(b) PFC and latency with ResNet-50
Figure 10: Distributed training
percentile FCT for mice flows by up to 8.7% and 24.3% respectively,
while achieving 8.6% lower average FCT for elephant flows. Com-
pared to SECN2, ACC outperforms the average and 99th percentile
FCT for mice flows by up to 28.6% and 58.3% respectively while
achieving 21.1% lower overall average FCT. As shown in Figure
13(b), ACC always has better performance than SECN1 and SECN2.
These results have verified that ACC are adaptive to temporal and
spatial traffic variation.
Simulation Study on Centralized Design and Distributed De-
sign. We compare the distributed and centralized design through
simulation. We use a 96-host leaf-spine topology with 4 leaf switches
and 2 spine switches. As discussed in Section 3.2, due to large space
of network actions {55 × 20}{96+4+4} (≈ 10312), the centralized DRL
can not converge. Thus, it is impossible to deploy the centralized
design in the realistic system. To simply the design, we apply the
same setting for all uplink ports or downlink ports because of the
symmetric topology. Besides, we sampled some of the actions to
further reduce action space. By doing so, we reduce the large ac-
tion space from {55 × 20}{96+4+4} to hundreds of actions in this
simulation. As shown in Figure 14, compared with SECN1, C-ACC
achieves 16% and 25% lower average FCT and 99th percentile FCT.
Compared with SECN2, C-ACC achieves 52% and 70% lower av-
erage FCT and 99th percentile FCT. It is notable that C-ACC has
higher FCT in comparison with D-ACC. This is because that C-ACC
assigns the same ECN configuration to switches at the same layer,
which leads to improper ECN setting during congestion.
6 OPERATION EXPERIENCE AND
DISCUSSION
ACC in Production Datacenters. We have deployed ACC in pro-
duction datacenters for one year. It supports finance services includ-
ing business transformation (latency-sensitive), financial analyst
jobs (IOPS-intensive, throughput-intensive), and the secure cloud
storage services. One datacenter consists of ∼300 machines config-
ured with NICs of 25Gbps. Notice that the application and machines
Figure 11: Traffic distributions in large scale simulation
for elephant flows while effectively guaranteeing latency for mice
flows.
Mice Flows. As shown in Figure 12(b) and 12(c), ACC outperforms
the static ECN benchmarks for mice flows. Compared to SECN1,
ACC reduces the average and 99th percentile FCT for mice flows
by up to 5.7% and 15.8% at 90% load, respectively. Compared to
SECN2, ACC achieves 17.3% and 47.5% lower average FCT and
99th percentile FCT, respectively. The reason is that SECN 1 sac-
rifices throughput for better latency. This indicates that ACC can
effectively reduce the FCT for mice flows.
Elephant Flows. SECN1 achieves comparable performance as
SECN2. However, it is slightly worse than ACC. As shown in Figure
12(d), ACC outperforms SECN2 at high loads. For example, com-
pared to SECN2, ACC presents 4.4% lower average FCT for elephant
flows at 90% load.
Temporally & Spatially Heterogeneous Traffic. To illustrate
that ACC can adapt to the temporal and spatial changes of traf-
fic, we use Web Search and Data Mining workloads based on the
distribution given in Figure 11. The traffic load is chosen from
{60%, 70%, 80%, 90%}, and the source and destination of each flow
are randomly chosen from the servers. We run experiments ten
times and report the average value. For example, as shown in Fig-
ure 13(a), compared to SECN1, ACC reduces the average and 99th
393
ACC: Automatic ECN Tuning for High-Speed Datacenter Networks
SIGCOMM ’21, August 23–28, 2021, Virtual Event, USA
(a) Overall
(b) (0,100KB]:Avg
(c) (0,100KB]:99th
(d) [10MB,∞):Avg
Figure 12: FCT statistics with Web Search workload
(a) Web Search
(b) Data Mining
Figure 13: FCT statistics across different workloads and flow sizes in tempo-
rally & spatially heterogeneous traffic
Figure 14: FCT of distributed design (C-ACC)
and centralized design (D-ACC)
were incrementally deployed in the datacenter, which causes the
aggregation of storage and computation service. We use RDMA for
the communication of storage nodes. The communication between
storage and computing nodes is TCP. Traffic flow into and out of
datacenters is also TCP. Originally, the network applied the static
ECN setting suggested by device vendors. After updating the switch
with ACC, the IOPS 5 of storage services have improved by ∼20%.
More important, ACC reduces the burden of tuning the parameters
during service migration. Besides switches, ACC is compatible with
NICs from different vendors in datacenters located in different area.
Resource Consumption. ACC is deployed in the switch which is
resource limited. We estimate the resource consumption of ACC in
the switch. Assume the sampling interval is 500µs and the switch
has 48 ports. The RDMA data traffic uses one priority queue for each
port. Thus, it will takes 48KB/s bandwidth for one port and 2MB/s in
total to collect data on PCIe bus. We use a four-layer NN in practice,
the number of nodes in each layer is {20,40,40,20}. It requires 14M
Flops for one port and 1G Flops in total for computation. The
memory consumption for learning model is 30KB. Thus, the cost
of ACC is acceptable for most off-the-shelf commodity switches
which consists of multiple CPUs and 100Gbps bus[49–51].
Deep Dive of ACC. ACC works by achieving high throughput
while keeping low queue length. To illustrate how ACC optimizes
queue occupancy, we sampled the queue length of a switch when
the burst traffic arrives, as shown in the magnified portion of Figure
15. When the queue length increases, if the current ECN threshold
5IOPS is the critical measurement for customers to evaluate the service-level perfor-
mance, which is highly related to the networking delay and throughput.
does not change, queue will build up quickly. Notice that ACC reacts
to the increasing queue length and high link utilization by using a
low ECN threshold to generate more ECN marked packets. When
the queue length is approaching to low value, ACC applies a higher
ECN threshold to avoid starving which guarantee the throughput
performance. Hence, ACC always maintains short queues by adjust-
ing ECN marking threshold based on the state of the environment
dynamically.
Stability with Unseen Traffic Pattern. ACC applies learning-
based technique which is adaptive to the variable traffic patterns.
However, it has met the challenge about serious performance issues
over unseen traffic pattern. First, from the operational point of
view, operators care the long term benefit achieved from network
optimization. It can tolerate the variance in very short period. It
is notable that in production environment, RDMA applications
usually run for a long time and have the similar traffic patterns.
For example, distributed training task takes minutes or hours. The
periodic communication leads to the same traffic patterns.
To address this concern, we design an experiment by using a
pathological traffic pattern with two completely new traffic flows.
We show the FCT statistics of training and compare its results with
the recommended ECN configuration as shown in Figure 16. We
use WebSearch [7] (P1) and DataMining [22] (P2) as two different
traffic workloads and collect performance statistics once every
500 milliseconds. Actually the NN model is an aggressive version
without offline-training. At the time of 4.5 second, we switch the
traffic immediately from WebSearch to DataMining. ACC causes a
high FCT in short time (1 second) and converges to achieve a better
performance than that of static ECN. Then at the time of 8.5 second
394
SIGCOMM ’21, August 23–28, 2021, Virtual Event, USA
Yan et al.
Figure 15: Runtime queue occupancy
Figure 16: FCT under different workloads during training
(a) [10MB,∞):Avg
(b) (0,100KB]:Avg
and 9.5 second, we exchange the traffic flows. It is interesting to
see that since the model has learned the pattern of arrival traffic,
ACC can adapt to the traffic and maintain good performance. In
short, ACC achieves 31.1% and 56.2% reduction of the average FCT
compared to SECN1 and SECN2, respectively. Besides, it is notable
that, ACC does not obviate PFC, which is the last guard to avoid
loss and poor performance.
ACC minimizes human intervention for ECN configuration atop
standard switches. The fact that the approach has been deployed in
a small-scale production setup and has been evaluated using macro-
benchmarks is commendable as it also shows the application-layer
gains through automatic ECN optimization. In this paper, we fo-
cus on the choice of the ECN threshold for fast RDMA network
deployment because RoCE processing is fully offloaded to the NIC
hardware. An optimal solution may be hybrid: the RL model infer-
ence and ECN update is decentralized for quickest response, while
online training/RL model update is done by a centralized controller.
A global view from the centralized controller would help further
improve the system. Furthermore, this work can be extended to a
broader scope by using deep learning for software based congestion
control with or without PFC [27, 29] and Optimization of the entire
set of parameters (transport, ECN, PFC).
7 RELATED WORK
Congestion Control for High-speed DCN. Many congestion
control algorithms have been proposed for emerging high-speed
datacenter networks. PDQ[23], D3[55], pFabric[8], and HPCC[29]
rely on precise in-network state information of switches and update