### 5.3 Macro-benchmark
To evaluate the performance of ACC under realistic application scenarios, we tested ACC in a distributed SSD storage system and a distributed GPU training system.

#### 5.3.1 Distributed Storage System
In the distributed storage cluster, servers are divided into computing nodes and storage nodes, deployed in a 3:1 ratio. The macro-benchmark consists of 18 computing nodes and 6 storage nodes. Computing nodes send I/O requests (Read Data/Write Data) to storage nodes, which back up data and respond to the requests from computing nodes. Performance is measured using the IOPS (I/O operations per second) metric, which is influenced by both network throughput and latency. We compare ACC with a fixed ECN setting, where Kmax = 270KB, Kmin = 30KB, and p = 10%, as suggested by the device vendor.

**Table 1: Traffic Loads in Distributed Storage System**

| Traffic Pattern | Read-Write Ratio | Blocksize (IO size) |
|-----------------|------------------|---------------------|
| OLTP            | 5:5              | 512B - 64KB         |
| OLAP            | 5:5              | 256KB - 4MB         |
| VDI             | 2:8              | 1KB - 64KB          |
| Exchange Server | 6:4              | 32KB - 512KB        |
| Video Streaming | 2:8              | 64KB                |
| File Backup     | 4:6              | 16KB - 64KB         |

**Traffic Loads**
We use FIO [15] to generate realistic workloads for the distributed storage system (Table 1), based on monitoring and abstracting traces from a large-scale cloud storage system over the past five years. The traffic models are identified based on characteristics such as read-write ratio, block size distribution, and I/O depth concurrency. For example, OLTP includes fast transaction queries for data less than 64 KB, while OLAP involves complex data analysis with block sizes ranging from hundreds of kilobytes to several megabytes.

**Summary of Results**
As shown in Figure 9, ACC improves the application performance of the distributed storage cluster by up to 30%. For instance, in VDI (Figure 9(c)), ACC achieves better IOPS performance, especially for large I/O depths. At an I/O depth of 16, ACC increases IOPS by 5%; at an I/O depth of 128, it increases IOPS by 15.3%. The performance gap between SECN and ACC widens as I/O depth increases. For FileBackup (Figure 9(f)), ACC improves IOPS by up to 30% compared to the static ECN setting.

In some cases, ACC shows little performance gain compared to SECN, such as in OLTP, VDI, and FileBackup with low I/O depth and small I/O sizes. This is due to the low probability of collisions in switches when senders have low concurrency and flow rates. The performance gain becomes more significant with increased I/O depth and tasks, as the storage system becomes overloaded and degrades performance (Figure 9(b) and (d)).

#### 5.3.2 Distributed Training System
To illustrate the efficiency of ACC in GPU clusters, we used 8 servers with GPU P100 (7 workers and 1 parameter server) to train AlexNet and ResNet-50 models (batchSize=64). Training speed was used as the performance metric. As shown in Figure 10(a), ACC outperforms SECN1 and SECN2 in distributed training. For example, in ResNet-50, ACC achieves up to 7% and 12% higher training speeds, respectively. Additionally, Figure 10(b) shows that ACC achieves lower round-trip latency, which benefits small messages like control packets. Note that ACC also improves link utilization for better communication of large messages.

In distributed training, communication traffic patterns repeat in each training iteration. ACC's neural network (NN) has an experience memory, allowing it to quickly adjust the ECN threshold for similar traffic patterns in subsequent iterations, thereby achieving good performance.

### 5.4 Large-Scale Simulation
In this section, we use NS3 [2] simulation to evaluate ACC's performance in large-scale data center networks (DCNs).

**Setup**
We used a 288-host leaf-spine topology with 12 leaf switches and 6 spine switches. Each leaf switch has 24 25Gbps links connecting to servers and 6 100Gbps links connecting to spine switches. Traffic was generated based on two realistic workloads: Web Search [7] and Data Mining [22], both of which are heavy-tailed.

**Overall**
The average FCT (Flow Completion Time) of all flows was calculated. As shown in Figure 12(a), compared to SECN1, ACC achieves 5.8% lower overall average FCT at 90% load. Compared to SECN2, ACC achieves 16.6% lower overall average FCT at 90% load. This is because ACC can maintain high throughput while effectively guaranteeing latency for mice flows.

**Mice Flows**
As shown in Figures 12(b) and 12(c), ACC outperforms static ECN benchmarks for mice flows. Compared to SECN1, ACC reduces the average and 99th percentile FCT for mice flows by up to 5.7% and 15.8% at 90% load, respectively. Compared to SECN2, ACC achieves 17.3% and 47.5% lower average FCT and 99th percentile FCT, respectively. This indicates that ACC can effectively reduce FCT for mice flows.

**Elephant Flows**
SECN1 achieves comparable performance to SECN2 but is slightly worse than ACC. As shown in Figure 12(d), ACC outperforms SECN2 at high loads. For example, compared to SECN2, ACC presents 4.4% lower average FCT for elephant flows at 90% load.

**Temporally & Spatially Heterogeneous Traffic**
To demonstrate ACC's adaptability to temporal and spatial traffic changes, we used Web Search and Data Mining workloads based on the distribution given in Figure 11. The traffic load was chosen from {60%, 70%, 80%, 90%}, and the source and destination of each flow were randomly selected from the servers. Experiments were run ten times, and the average values were reported. For example, as shown in Figure 13(a), compared to SECN1, ACC reduces the average and 99th percentile FCT by up to 8.7% and 24.3%, respectively, while achieving 8.6% lower average FCT for elephant flows. Compared to SECN2, ACC outperforms the average and 99th percentile FCT for mice flows by up to 28.6% and 58.3%, respectively, while achieving 21.1% lower overall average FCT. As shown in Figure 13(b), ACC consistently outperforms SECN1 and SECN2, verifying its adaptability to temporal and spatial traffic variations.

**Simulation Study on Centralized Design and Distributed Design**
We compared the distributed and centralized designs through simulation using a 96-host leaf-spine topology with 4 leaf switches and 2 spine switches. Due to the large action space of the centralized DRL, it cannot converge. To simplify the design, we applied the same settings for all uplink and downlink ports and sampled some actions to reduce the action space. By doing so, we reduced the action space from {55 × 20}{96+4+4} (≈ 10^312) to hundreds of actions. As shown in Figure 14, compared with SECN1, C-ACC achieves 16% and 25% lower average FCT and 99th percentile FCT. Compared with SECN2, C-ACC achieves 52% and 70% lower average FCT and 99th percentile FCT. Notably, C-ACC has higher FCT compared to D-ACC because it assigns the same ECN configuration to switches at the same layer, leading to improper ECN settings during congestion.

### 6 Operation Experience and Discussion
**ACC in Production Datacenters**
We have deployed ACC in production datacenters for one year, supporting financial services, including business transformation (latency-sensitive), financial analyst jobs (IOPS-intensive, throughput-intensive), and secure cloud storage services. One datacenter consists of approximately 300 machines configured with 25Gbps NICs. The applications and machines were incrementally deployed, causing the aggregation of storage and computation services. RDMA was used for communication between storage nodes, while TCP was used between storage and computing nodes. Initially, the network used a static ECN setting recommended by device vendors. After updating the switch with ACC, the IOPS of storage services improved by about 20%. More importantly, ACC reduces the burden of tuning parameters during service migration and is compatible with NICs from different vendors in datacenters located in different areas.

**Resource Consumption**
ACC is deployed in resource-limited switches. We estimate the resource consumption of ACC in the switch. Assuming a sampling interval of 500µs and 48 ports, the RDMA data traffic uses one priority queue per port, requiring 48KB/s bandwidth per port and 2MB/s in total to collect data on the PCIe bus. We use a four-layer NN in practice, with {20, 40, 40, 20} nodes in each layer, requiring 14M Flops per port and 1G Flops in total for computation. The memory consumption for the learning model is 30KB. Thus, the cost of ACC is acceptable for most off-the-shelf commodity switches, which include multiple CPUs and 100Gbps buses [49–51].

**Deep Dive of ACC**
ACC works by achieving high throughput while maintaining low queue length. To illustrate how ACC optimizes queue occupancy, we sampled the queue length of a switch when burst traffic arrives, as shown in the magnified portion of Figure 15. When the queue length increases, if the current ECN threshold does not change, the queue builds up quickly. ACC reacts to increasing queue length and high link utilization by using a low ECN threshold to generate more ECN-marked packets. When the queue length decreases, ACC applies a higher ECN threshold to avoid starving, ensuring throughput performance. Hence, ACC maintains short queues by dynamically adjusting the ECN marking threshold based on the environment's state.

**Stability with Unseen Traffic Patterns**
ACC uses a learning-based technique that adapts to variable traffic patterns. However, it faces challenges with unseen traffic patterns. From an operational perspective, operators care about long-term benefits from network optimization and can tolerate short-term variance. In production environments, RDMA applications typically run for extended periods and have similar traffic patterns. For example, distributed training tasks take minutes or hours, leading to periodic communication with consistent traffic patterns.

To address this, we designed an experiment using a pathological traffic pattern with two completely new traffic flows. We show the FCT statistics of training and compare them with the recommended ECN configuration (Figure 16). We used WebSearch [7] (P1) and DataMining [22] (P2) as different traffic workloads and collected performance statistics every 500 milliseconds. The NN model is an aggressive version without offline training. At 4.5 seconds, we switched the traffic from WebSearch to DataMining. ACC caused a high FCT in the short term (1 second) but converged to achieve better performance than static ECN. At 8.5 and 9.5 seconds, we exchanged the traffic flows. Since the model had learned the arrival traffic patterns, ACC adapted and maintained good performance. In summary, ACC achieved 31.1% and 56.2% reductions in average FCT compared to SECN1 and SECN2, respectively. It is notable that ACC does not obviate PFC, which is the last guard against loss and poor performance.

ACC minimizes human intervention for ECN configuration on standard switches. The fact that the approach has been deployed in a small-scale production setup and evaluated using macro-benchmarks is commendable, as it also shows application-layer gains through automatic ECN optimization. In this paper, we focus on the choice of the ECN threshold for fast RDMA network deployment because RoCE processing is fully offloaded to the NIC hardware. An optimal solution may be hybrid: the RL model inference and ECN update are decentralized for the quickest response, while online training/RL model updates are done by a centralized controller. A global view from the centralized controller would further improve the system. Furthermore, this work can be extended to a broader scope by using deep learning for software-based congestion control with or without PFC [27, 29] and optimizing the entire set of parameters (transport, ECN, PFC).

### 7 Related Work
**Congestion Control for High-Speed DCNs**
Many congestion control algorithms have been proposed for emerging high-speed data center networks. PDQ [23], D3 [55], pFabric [8], and HPCC [29] rely on precise in-network state information from switches and update...