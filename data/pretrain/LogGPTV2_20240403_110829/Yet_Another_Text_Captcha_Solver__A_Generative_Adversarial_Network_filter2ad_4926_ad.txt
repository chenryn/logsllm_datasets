Table 4 compares our fine-tuned solver to previous attacks. In
this experiment, our approach outperforms all comparative schemes
by delivering a significantly higher success rate. For many of the
testing schemes, our approach boosts the success rate by 40%. It
can successfully solve all the captchas of Blizzard, Megaupload and
Authorize used in [10]. Our approach achieves a success rate of
87.4% and 90% for reCAPTCHA 2011 and 2013 respectively. This
scheme was previously deemed to be strong where the human accu-
racy is 87.4% [19]. As a result, our solver matches the capability of
humans in solving reCAPTCHA. To achieve a comparable accuracy
for reCAPTCHA, a CNN-based captcha solver [21] would require
2.3 million unique real captcha images [19].
We want to stress that unlike all the competitive approaches
which require manually tuning a character segmentation method,
our approach bypasses this process by learning an end-to-end solver.
Figure 10: Comparing a filtering-based method with our ap-
proach for removing noisy backgrounds and occluding lines.
The filtering-based method fails to remove security features
from the latest captcha schemes while our approach can.
As a result, our approach requires less expert involvement, yet it
delivers better performance.
6.3 Pre-processing Security Features
One of the key steps in solving text-based captchas is to remove
the security features and standardize the font style of an input
captcha image. In this evaluation, we compare our pre-processing
model against prior pre-processing methods on removing noisy
backgrounds [8, 10, 32]and standardize font styles [11, 16].
Remove security features. Filtering is often used in prior at-
tacks for pre-processing text-based captchas [8, 10, 32]. The idea
is to apply a fix-sized window, or filter kernel, throughout the im-
age to remove the occluding lines and noise while keeping edges
of the characters. Figure 10 compares a previously used filtering
method [8, 10, 32] against our automatically learned pre-processing
model. Finding the right filter kernel size for the input captchas
shown in Figure 10a is non-trivial, because the filter either fails to
eliminate the background and occluding lines (b and c in Figure 10)
or it over does it by eroding edges of the characters (which makes
it harder to recognize the characters). While filtering was effec-
tive for prior text-based captchas, the latest captcha schemes have
introduced more sophisticated security features which make filter-
ing no longer feasible. In contrast to filtering, our pre-processing
model can successfully eliminate nearly all the background noise
and occluding lines from the input image, leading to a much cleaner
captcha image while keeping the character edges, as depicted in
Figure 10a. This experiment shows that our pre-processing model
is highly effective in processing and removing security features
from the latest text captcha schemes.
(a) reCaptcha (b) Microsoft(c) QQ(d) Yahoo! (a) Input Baidu captchas(e) Applying our pre-processing model(b) Applying a 2   1 filter kernel ×(c) Applying a 2   2 filter kernel ×(d) Applying a 3   1 filter kernel ×Captcha Scheme
Captcha Example
Success rate
Ref. [10] Our approach
Captcha Scheme
Captcha Example
Success rate
Ref. [17] Our approach
Megaupload
Blizzard
Authorize
Captcha.net
NIH
Reddit
Digg
eBay
Slashdot
Wikipedia
93%
70%
66%
73%
72%
42%
20%
43%
35%
25%
100%
Baidu (2016)
46.6%
97.5%
100%
QQ
100%
Taobao
99.6%
Sina
99%
reCAPTCHA (2011)
98%
eBay
95%
Amazon
86.6% Wikipedia
86.4% Microsoft
78%
Yahoo! (2016)
56%
23.4%
9.4%
77.2%
58.8%
25.8%
23.8%
16.2%
5.2%
94%
90.7%
90%
87.4%
86.6%
79%
78%
72.1%
63%
Captcha Scheme
Captcha Example
Success rate
Ref. [8] Our approach
Captcha Scheme
Captcha Example
Success rate
Ref. [19] Our approach
reCAPTCHA (2013)
Baidu (2013)
reCAPTCHA (2011)
eBay
Baidu (2011)
Wikipedia
Yahoo! (2014)
CNN
22.3%
55.2%
22.7%
51.4%
38.7%
28.3%
5.3%
51.1%
90%
PayPal
89%
reCAPTCHA (2011)
87.4%
Yahoo! (2016)
57.1%
66.6%
57.4%
92.4%
87.4%
63%
86.6%
83.1%
78%
75.1%
51.6%
Table 4: Comparing our approach against four prior attacks [8, 10, 17, 19] on 24 captcha schemes where the prior methods
were tested on. These captcha schemes include eBay and Wikipedia evaluated in Section 6.1 and other 22 schemes.
Figure 11: Comparing font style standardization between
a state-of-the-art hollow captcha solver [16] and our pre-
processing model. Our pre-processing model is able to fill
the hollow parts more effectively.
Figure 12: Character segmentation produced by our pre-
processing model. For each scheme, the left image is the in-
put captcha, and the right image is the output of our pre-
processing model.
Filling hollow characters. Figure 11 compares our pre-processing
model against a state-of-the-art hollow captcha solver [16]. The
task in this experiment is to fill the hollow parts of the characters.
Here, we apply both schemes to the testing hollow captchas from
Sina and Microsoft schemes. Figure 11a gives some of the examples
from these two schemes, while Figure 11b and Figure 11c present
the corresponding results given by the hollow filling method in [16]
and our approach respectively. As can be seen from the diagrams,
our pre-processing model is able to fill most of the hollow strokes,
while the state-of the-art method leaves some hollow strokes un-
filled. Therefore, our approach is more effective in standardizing
the font style. We also note that unlike prior attacks which require
manually designing and tuning an individual method to process
each security feature, our approach automatically learns how to
process all features at one go. As a result, our approach requires
less effort for implementing a holistic pre-processing model.
Standardizing character gaps. Prior research suggests that the
robustness of text captchas largely relies on the difficulty of finding
where the character is (i.e., segmentation) rather than what charac-
ter it is (i.e., recognition) [11]. This segmentation-resistance princi-
ple has become a crucial part for designing text captcha schemes.
The examples given in Figure 12 suggest that our pre-processing
model is effectively not only in removing security features (like
noisy backgrounds and occluding lines) and standardizing font
styles, but also in segmenting characters by widening and stan-
dardizing the gap between collapsed characters. The high-quality
Figure 13: How the beginning layer for transfer learning af-
fects the resulting performance of the fine-tuned solver.
character segmentation produced during pre-processing has a pos-
itive contribution to the success rate of our solver, helping it to
achieves a higher accuracy compared to existing attacks.
6.4 Transfer Learning
Recall that we only use 500 real captchas to refine the base
solver by employing transfer learning (Section 4.3). Our strategy
for transfer learning is to only retrain some of the latter neural
network layers of the base solver (see Figure 7). In this experiment,
we investigate how the choice of transfer learning layers affects
the performance of the fine-tuned solver. To that end, we apply
transfer learning to different levels of the base solver, by changing
the starting point of transfer learning from the 2nd convolutional
layer (CL) all the way down to the first fully-connected layer (FC).
Figure 13 reports performance of the resulting fine-tuned solvers
trained under different transfer learning configurations for the 11
captcha schemes given in Table 1. Overall, applying transfer learn-
ing to the second or third CL onward leads to the best performance.
To determine the best starting layer for transfer learning, we apply
cross-validation to the real captcha training dataset. Specifically,
we divide the 500 real captchas into two parts, the first part of 450
captchas is used to refine the base solver, and the rest 50 captchas
are used to validate the refined solver. We vary the beginning layer
for transfer learning, and then test the refined base solver on the
validation set to find out which beginning layer leads to the best
performance. Since we only train and validate on 500 captchas, this
process for finding the optimal beginning layer only takes several
minutes on our training platform.
6.5 Impact of Fine-tuning Training Data Sizes
In this experiment, we evaluate how the number of real captchas
used in transfer learning affects the success rate of the fine-tuned
solver. Figure 14 shows the success rates of the fine-tuned solver
when using different numbers of real captchas in transfer learning.
When the number of training examples is 500, our approach reaches
a high success rate. For most captcha schemes, the success rate drops
significantly when the number of training examples less than 400.
Nonetheless, our approach can achieve a high success rate when
(b) Results given by[16](a) Example hollow captchas from Sina and Microsoft schemes(c) Results given byour pre-processing model(a) Wikipedia (b) Microsoft (c) Sina (d) Baidu 2ndCL3rdCL4thCL5thCL1stFC020406080100Fine-tunedsolversuccessrate(%)BeginninglayerfortransferlearningGoogleWikipediaeBayMicrosoftBaiduAlipayJDSinaSohuWeiboQihu360Figure 14: The achieved success rates when the fine-tuned
solver is trained using different number of real captchas.
Figure 15: How the synthesizer training termination crite-
rion affects the solver performance. Training terminates
when the discriminator fails to classify a certain ratio of syn-
thetic captchas.
the number of training examples is 500. Such a number allows an
attacker to easily collect from the target website.
6.6 Synthesizer Training Termination Criteria
Our captcha synthesizer is trained under the GAN framework, and
training terminates when the discriminator fails to classify a certain
ratio of synthetic captchas (Section 4.2). Figure 15 reports how the
termination criterion affects the quality of the synthetic captchas.
The x-axis shows the ratio (from 0.8 to 0.97) of synthetic captchas
that are misclassified as a real captcha by the discriminator when
training terminates. The y-axis shows the success rate achieved
by the fine-tuned solver for five current captcha schemes, where
the base solver is trained on the resulting synthetic captchas using
different termination criteria but the fine-tuned solver is trained
on the same set of real captchas.
In general, the more synthetic captchas that the discriminator
fails on, the higher the quality the generated synthetic captchas
Figure 16: Example captchas with single security features.
will be, which in turns leads to a more effective captcha solver.
However, the increase in the success rate reaches a plateau at 0.95.
Further increasing the similarity of the synthetic captchas to real
ones does not improve the success rate due to overfitting. Based
on this observation, we choose to terminate synthesizer training
when the GAN discriminator can successfully distinguish less than
5% (i.e., fail on 95% or more) of the synthetic captchas. We found
that this threshold works well for all the captcha schemes tested in
this work.
6.7 Impact of Captcha Security Features
In this experiment, we evaluate how security features affect the
effectiveness of our solver. Having this knowledge is crucial for
designing a more robust captcha scheme. This experiment consid-
ers four common security features for text captchas: overlapping,
rotation, distoration, and waving. We exclude noisy backgrounds
and occluding lines when evaluating individual features, as the two
features have been shown to be vulnerable under our GAN-based
pre-processing model in Section 6.3. We use a third-party captcha
generator [36] to generate captchas of different security feature
settings. For each setting, we generate 220,000 synthetic captchas.
We then train our CNN-based solver on 200,000 captchas and test it
on the remaining 2,000 captchas. Note that we do not fine-tune the
solver in this experiment because the test data are also synthetic
captchas.
Overlapping. By decreasing the space between adjacent charac-
ters, overlapping is a widely for anti-segmentation [10]. For captcha
images of 150 × 70 pixels, when the overlapping area of adjacent
characters are 4, 6, 8 and 10 pixels (as depicted in Figure 16a), the
success rate of our solver is 65%, 50.1%, 42.6% and 25.1%, respectively.
The success rate is still significantly higher than the 1% threshold
at which captchas are considered to be ineffective. It is worth men-
tioning that prior study has shown that if the resulting overlapping