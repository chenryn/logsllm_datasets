a data port.
Total:
$57.08
http://www.ﬂickr.com/photos/gijsbertpeijs/7988257583
http://www.polycase.com/lp-51p
http://www.targus.com/us/productdetail.aspx?sku=ACH63US
http://www.amazon.com/JacobsParts-150Mbps-Wireless-Notebook-TP-WF11/dp/
B0067NFSE2
http://www.newegg.com/Product/Product.aspx?Item=N82E16820147152
http://www.ebay.com/itm/10x-USB-USA-AC-Wall-Charger-for-Apple-
iPhone-3-3G-4G-4S-5-5G-iPod-New-White-/271163372744
Raspberry Pi, Model A: $25
Case: $4.61
USB Hub: $5.99
WiFi: 2x $6.52
SD Card: $6.99
USB Power: $1.45
Total: 57.08 per node
3. Per-Node Cost: $57.08 in 10-node quantities, excluding case.
a. I bought cheap wall-wart cases and used a drill saw; you can 3D print them, 
or even buy disposable GladWare and use that.
It’s within the price range of any kid who mows lawns energetically for a few weekends to 
build a group of these.
Wait... why 2 WiFi?
• Because I’m cheap and lazy
• Introducing PortalSmash: it clicks on 
buttons, so you don’t have to
4. Nodes don't bring "phone home" communications gear, e.g., a 3G card; that's too 
expensive and *very* easy to trace (just call VZW tech support!). They use PortalSmash, Open 
Source software I've developed to look for open (or captive portal) WiFi and use that. In an 
urban area, that's perfectly sufficient. (No, PortalSmash doesn't look at encrypted WiFi; yes, 
you could add Reaver etc. No, I'm not planning to.)
C&C Software
• “Reticle: Leaderless Command and 
Control”
• This was the ﬁrst of the two DARPA 
CFT contracts I mentioned
• Whole presentation at B-Sides Vegas 
2012---but I will summarize
B. C&C Software: Reticle, Leaderless C&C
1. Developed under DARPA Cyber Fast Track, Spring 2012
2. Original work presented at BSidesLV 2012, but massive improvements, and a 
complete rewrite, since then.
Reticle
Each Reticle node runs CouchDB, a NoSQL database, plus Nginx, Tor, and some custom 
management software. This lets nodes combine into a peer-to-peer “contagion” network  in 
which each node sends commands and data to every other node, for both command 
inﬁltration and data exﬁltration, without any single point of failure. They speak via Tor, to 
prevent anyone on the network to which they connect from determining where other Reticle 
nodes are living.
To make reverse-engineering of a node much more difficult, Reticle nodes can be conﬁgured 
with what I call “grenade” encryption: pull pin, throw toward adversary. They load their 
encryption keys for their local storage at boot from removable media, which is then removed 
to prevent an adversary from recovering the data. A “cold boot” attack is certainly possible, 
but since most nodes don’t have batteries, it’s physically kind of a pain to do---and it’s not a 
usual thing for most people to dump liquid nitrogen on the ﬁrst black box they see plugged 
into a wall.
CreepyDOL, then, is just a mission Reticle runs; it can be retasked at any time.
Roadmap
• Goals
• Background
• Architecture
• Design of CreepyDOL
• Future Work
• Mitigation
CreepyDOL
So as I mentioned, Creepy.
Distributed Computation for 
Distributed Systems
http://www.ﬂickr.com/photos/lara604/3164622774
http://www.ﬂickr.com/photos/jenniferboyer/52474490/
http://www.ﬂickr.com/photos/gaelx/1858599144
A. Distributed Querying for Distributed Data
1. Since we don't have independent, high-bandwidth channels for sending data 
home, it's not a good idea (and may not be possible) to send raw packets home. Nodes 
should send home data that's already been digested.
2. So: we run any queries on the nodes that can be effectively run on the nodes, 
*given data that node has collected*.
3. We do not process multi-node data on individual nodes, even though every node 
has access to all the data (see "contagion network"), because they've got limited processing 
power---and more importantly, data storage.
Centralized Querying for 
Centralized Questions
B. Centralized Querying for High-Level Data
1. Things that need datapoints from multiple nodes---tracking, pattern analysis, 
etc., go on the "backend."
2. The backend is just another node, but with a special mission conﬁguration: rather 
than just sensing and adding data, it receives data from the contagion network, pushes it into 
another system (a data warehouse), and then instructs the contagion to delete it to make 
room.
NOM: Nosiness, Organization, and Mining
http://www.ﬂickr.com/photos/scjody/5345366096
C. Data Query Methodology: NOM
1. O: Observation. Take as much data out of local traffic as possible; this means 
names, photos, services used, etc. To make this easy, we've created a large number of 
"ﬁlters" that are designed for traffic from speciﬁc applications---DropBox, Twitter, Facebook, 
dating websites, etc. Now, many of these services encrypt their traffic, which is admirable; 
however, in many cases, we can still get useful data that they provide in, e.g., their User 
Agent. And there’s no reason for them to do this. (Next slide)
Why?
So this is a screenshot from Wireshark, of a packet being sent to request new iMessages from 
Apple. Notice at the bottom, where it sends the hardware device and iOS version, as part of 
the HTTP header? This is unnecessary, and it’s harmful. (If Apple needs this information, it 
could transmit it inside TLS.)
NOM: Nosiness, Organization, and Mining
http://www.ﬂickr.com/photos/scjody/5345366096
2. N: Nosiness. Using data extracted from O queries, there are lots of 
leveraged queries we can make; for instance, given an email address, we can look for 
accounts on web services, or given a photo, we can look for copies of that photo pointing to 
other accounts. This can be run either as distributed or centralized.
3. M: Mining. Taking data found by the nodes, build up larger analyzed products. For 
instance, is the device (person) usually in one area during a certain time of day? Are there 
three devices that are almost always seen together, if at all? (The latter may indicate that they 
are all carried by the same user.) This type of query is exclusively run on the backend.
CreepyDOL Architecture
So this is the overall architecture for CreepyDOL. The nodes connect to each other, and one 
node becomes a “sink node” from which data is pulled and sent to the CreepyDOL storage, so 
that it can be used in the visualization. The visualization pulls data from the storage and 
from an OpenStreetMaps provider, to have underlaid maps.
Visualization
• Second DARPA CFT Contract
• Used the Unity Game Engine
• Side note: wow, that’s a fun toy
• Side note: wow, I hate writing JavaScript that’s 
interpreted by C#, then compiled into .NET CLR
• Runs on an iPad! Or OSX/Windows/Linux/Android
• I think I could make it run on an XBox360, 
actually (Unity is Very Nice)
So let’s talk about visualization.
To prevent the user (the person requesting data) from being tied to a particular computer, we 
use the backend to run queries for visualization, then serve the results to the user's 
visualization computer.
To make it easy to do large-scale visualization, I used an existing engine: the Unity game 
engine, used in hundreds or thousands of iPad, iPhone, XBox, Wii, and PC games. This let me 
take advantage of the hundreds of person-years of development they’ve already done to 
make it fast. As a side effect, it also means I can run my visualization on an iPad; since all the 
processing is done on a visualization server, it doesn’t need to be able to hold the data in 
RAM.
Demo Video!
Watch closely: do you see the creepy?
Test Parameters
• To prevent badness, we programmed the 
NOM system to look only for trafﬁc from 
devices we owned; no “random 
stranger” data was collected at 
any time.
So ﬁrst you can see the plane loading. Then the data loads, and after a brief loading delay, 
the map comes in from OpenStreetMaps. I’ll zoom the camera in and out a bit; you can see 
that it’s 3D, and the control interface works much like Starcraft or other real-time strategy 
games, except with people instead of alien troops. Now you can see I’ll draw a box to select a 
group of data, and after a brief delay, the data and map will re-draw to allow more focus on 
the data in question. I can hover over various nodes to see their MAC addresses and 
locations, but for maximum data, I click on a node, and it shows me everything. I have some 
of the services I use, I have the hardware and software I’m carrying, I have a real name, email 
address, and even my photo from an online dating site. Combined with the true location and 
time of each of these pings, we end up with the same data that you used to use a whole team 
of surveillance agents to retrieve. Cheap, distributed stalking.
Roadmap
• Goals
• Background
• Architecture
• Design of CreepyDOL
• Future Work
• Mitigation
Other Applications
Counter-Inﬁltration
http://www.ﬂickr.com/photos/igalko/6341182132/
A. Counter-Inﬁltration
1. There is a persistent rumor, in cases of exceptional police brutality (Occupy 
Anything, or more protests in Britain) that the police are sending in agents provocateur to 
cause the disruption that gives them an excuse to crack down. (This rumor is at least 300 
years old, by the way.)
2. CreepyDOL would let you set up "known devices" with alarms for new ones, watch 
as new people come in, or even simply set off a klaxon if a Blackberry shows up (obviously a 
cop).
C. OPSEC Training
1. The ROE for my tests demonstrate limiting data capture to one or several known 
devices. Use that to test your agents' OPSEC capabilities: set up a wide-ranging capture 
network (but tied to their stuff) and see what they leak.
2. The advantage is that you don't need to control every network an agent accesses. 
This lets you test "in the real world," which is much more realistic.
Evidence Logging
http://www.ﬂickr.com/photos/decade_null/142235888
B. Evidence Logging
1. Again in fast-moving scenarios like protests and rallies: there's a real problem 
with destruction of evidence, electronic or physical, during crackdowns. In addition, it's very, 
very difficult to know who was *in* a kettle in the ﬁrst few hours afterward; a way to know 
that could be very comforting and/or helpful to those outside.
2. Since CreepyDOL uses a contagion network, anything it logs will be immediately 
shipped out of the area to linked nodes anywhere on the planet. If those nodes go offline, the 
data is preserved.
3. For bonus points, use F-BOMB belt packs (which last a very long time on batteries) 
to have moving logs---and if you come in range of a WiFi AP somewhere (say, at a stop 
light), they'll offload their data without any additional interaction.
4. The encryption, and the fact that the nodes don't persist their keys, mean that 
unless an adversary *already knows what it is and how to cold boot it*, they don't get data. If 
people on the outside are concerned about the nodes, revoke their device certiﬁcates and 
they'll be cut off immediately.
Improvements
Scaling Up
• Sharding Contagion Networks
• Scaling backend --- luckily, this isn’t hard
• Scaling limits of visualization
Sharding the contagion networks: it’s easy, just give them different keys. Each network could 
have a sink node that throws data into the visualization system.
Scaling the backend is similarly easy: the software communications with the visualization 
engine over HTTP, so it can run in the ubiquitous cloud. Indeed, running the backend on 
Amazon S3, I’ve tested scaling parts of the backend to over half a terabyte of packet capture 
data.
The visualization is somewhat more difficult; Unity gets fussy if I display more than a couple 
thousand nodes at once. However, with grouping, and eventually, over large map areas, 
doing limited ﬁeld of view and view distance work (as they do in real video games), this can 
be mitigated.
Enhancements
• $20 SDR devices (RTLSDR)
• To listen to any frequency, not just WiFi
• Encrypted WiFi Workarounds
• e.g., Reaver
• Jasager (WiFi Pineapple) to make sure 
wireless devices connect
• MitM
Roadmap
• Goals
• Background
• Architecture
• Design of CreepyDOL
• Future Work
• Mitigation
Mitigation: A Sacriﬁce
The leaks are at all levels. The 802.11 protocol asks devices to do this beaconing which 
means that even with encryption both in the protocol and over the air, I can still do tracking 
and identiﬁcations.
The OS won’t enforce VPNs (iOS).
The apps leak too much data that they don’t need.
This is EVERYONE’s fault, but no one wants to take responsibility for their own actions. It’s 
the status quo, right?
The Status is Not Quo
Image from Dr. Horrible’s Sing-Along Blog, by Joss Whedon
We can’t tolerate this level of privacy leakage: as consumers, we should demand better, and 
as developers at every level, we have a responsibility to do better.
Digression: Hark
• Archive for hacker work of all types (not 
just security)
• Mentorship, promotion, and archival forever
• New system of unique identiﬁers, like the 
academic DOI system, but free
• On Kickstarter now: http://thehark.net
So a very short ﬁnal note on Hark. There’s been a back and forth between academic and non-
academic researchers for years, where the academics say hackers aren’t rigorous enough and 
don’t cite their work, and hackers say academics don’t do anything *but* cite other work. 
After this blew up at ShmooCon 2013, those of us who, like myself, straddle the academic/
nonacademic divide, had some discussions and drew up plans for a way to let hackers archive 
their work, whether it’s a tweet, a blog post, a conference presentation, or a journal article, 
and cite previous hacker work regardless of whether it’s been academically published. I don’t 
have time to go into all the details right now, but if you think it’s important for hackers to 
stop re-inventing the same wheels every time we have a new research projects, I hope you’ll 
check out thehark.net. And yes, we encourage corporate donations.
Thanks!
• To all those I’ve asked for comments, to 
Mudge for CFT, and my law school, for letting 
me spend so much time on other things.
• Also, I’m ﬁnishing law school in 10 months, 
and am wondering what I ought to take on 
next. If you’ve got something interesting, ping 
me: brendan@maliceafterthought.com.
• http://thehark.net