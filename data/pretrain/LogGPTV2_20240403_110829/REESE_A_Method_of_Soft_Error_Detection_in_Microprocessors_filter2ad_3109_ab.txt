cessor systems to perform global fault diagnosis.  Instruc- 
tion re-execution has been implemented in both multiscalar 
[ 171 and simultaneous multithreading [ 181 architectures.  In 
the latter article, Rotenberg utilizes modern microarchitec- 
tural techniques - a trace cache, simultaneous multithread- 
ing,  and data flow prediction - to  aid  in  error detection. 
Although the  microarchitecture  underlying REESE is dif- 
ferent, we apply many of the same techniques in our simu- 
lation. 
Franklin  implements  microarchitectural  time  redun- 
dancy in [24]. He notes that hardware added to boost micro- 
processor performance is often underutilized. The hardware 
is added without adding functionality. The law of diminish- 
ing marginal returns states that this type of extra hardware 
will be less useful than the hardware that was there before- 
hand.  The leftover usefulness can  be  utilized  to make the 
hardware more fault tolerant.  In  his paper, Franklin  dupli- 
cates all instructions at either the functional units (in the ex- 
ecution stage) or at the dynamic scheduler.  The only extra 
hardware required  is the hardware that stores the duplicate 
instructions and the hardware necessary to compare the re- 
sults at the end of the pipeline.  In  tests of four benchmark 
programs,  the  fault-tolerance overhead  ranges  from  9.6% 
down to only 0.5%. 
Our approach goes a step further than  Franklin  by  ask- 
ing  the  following  question:  How much  spare hardware is 
needed  to  decrease  the  fault-tolerance  overhead  to  zero? 
This  is  in  contrast  to  past  research  which  identifies  the 
amount of overhead necessary  but does not attempt to de- 
crease this overhead by adding spare components to the sys- 
tem. The next section discusses how our approach works. 
4  Implementation 
4.1  Utilizing Idle Capacity 
Superscalar processors are currently capable of execut- 
ing many  instructions per cycle (IPC). To  accomplish this, 
the instruction stream flowing through the processor needs 
to be  partitioned  into  small blocks  of  instructions,  where 
the instructions in a block can be executed simultaneously. 
Processors designers seek to  exploit  this  instruction-level 
parallelism  (ILP) as much as possible.  However, control 
403 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:02:23 UTC from IEEE Xplore.  Restrictions apply. 
dependencies and data dependencies limit  ILP. In  the for- 
mer case, an  instruction  that  follows a  branch  instruction 
cannot execute  until  the result  of  the branch  is known.  A 
data dependency  arises when an instruction cannot execute 
because its operand has not yet been generated by  a previ- 
ous instruction. 
Since the amount of ILP in an instruction stream changes 
from cycle to cycle, a portion  of  the available hardware  is 
left  idle  during most  clock cycles.  The average  through- 
put for the microprocessor  pipeline is approximately 2 IPC. 
However, modern processors are capable of executing 4-6 
IPC. Experiments done over a broad range of programs have 
verified  that approximately  30-40% of hardware is unused 
during any specific cycle.  This idle  capacity may be used 
for other purposes,  without  adding additional  hardware  to 
the processor. 
Idle capacity is likely to increase in future processor gen- 
erations. The raw computing power of processors  is grow- 
ing  faster  than  the  ability  of  these  processors  to  exploit 
larger  amounts of  ILP. This results  in  increasing  amounts 
of  hardware  that  is  underutilized.  Fault  tolerance  can  be 
implemented by this idle hardware. 
4.2  Implementing Time Redundancy 
REESE utilizes the idle capacity that is inherent in GPPs 
to  detect  soft  errors. 
It  does  this  by  interleaving  the  R- 
stream and  P-stream  instruction  executions.  An  R-stream 
instruction  is generated by  putting the P-stream  version  of 
the  instruction  into  a FIFO queue immediately  before  the 
instruction  commits.  We  call  this  structure  the  R-stream 
Queue. The delay between execution of P and R versions of 
an instruction  is equal to the time to execute the P version 
plus the time for an instruction to go from the tail to the head 
of the R-stream Queue. Figure  1  shows how REESE mod- 
ifies the  instruction  flow through  the pipeline.  The dotted 
line shows the path  of an R-stream  instruction  through the 
pipeline.  The two results  from the P-stream  and R-stream 
executions are compared after the writeback stage. 
& 
Fetch 
-3ispatch 
Exâ€™ 
- + Mem 
-+ Sched  ----f 
I+- 
I - - - _ - _  
I 
Commit 
R-Stream  Queue 
Figure 1. A  Microprocessor Pipeline that Im- 
plements REESE 
REESE  executes  every  instruction  two  times by  inter- 
leaving  P and  R  streams.  After both  P and  R  versions  of 
404 
an  instruction  have  been  executed, the  two results  can be 
compared.  This gives the GPP the ability to detect soft er- 
rors.  When too few primary instructions  are ready to issue, 
REESE can issue redundant instructions  to any functional 
units that are still available. Functional units can be utilized 
nearly  100% of the time. 
This  implementation  detects soft  errors that  affect  in- 
struction results.  Examples include, but are not limited to, 
arithmetic, logical, effective address, and branch resolution 
calculations. REESE does not detect soft errors that do not 
effect  the  intermediate  or final results  of  an individual  in- 
struction, such as pipeline control or cache errors. Any er- 
ror that  might occur after the  results  are compared would 
also not be detected. 
4.3  Necessary Hardware Additions 
We add as little hardware as possible to our microproces- 
sor to realize REESE. The following list shows the primary 
ad.ditions: 
The R-stream Queue 
Extra scheduling and forwarding logic 
Hardware to  compare the  results  of  P-stream  and R- 
stream versions of an instruction. 
Connections  that  allow  for  interaction  between  the 
pipeline and R-stream Queue 
First of  all, we add the R-stream  Queue just before  the 
commit stage. The R-stream Queue is a FIFO queue with an 
(initial) maximum of 32 entries.  This queue holds P-stream 
iristructions  that  are ready  to  be  committed.  An  entry  in 
the R-stream Queue stores much more than just the 32-bit 
instruction.  It keeps the values of the instruction  operands 
and  the  result  of  the  operation.  Thus the  P-stream  result 
is  immediately  available for comparison with  its R-stream 
counterpart. 
Any information from the P-stream execution that might 
speed up the R-stream execution  could be  included  in  the 
R-stream  Queue entry.  However,  one must  keep in  mind 
that extra pipeline hardware might be necessary  in order to 
use any extra information  that is carried  along with the R- 
stream instruction.  We choose to include only operands and 
result information. This information is vital to the R-stream 
execution and final result comparison. 
Extra scheduling logic is also important. After a P stream 
instruction  is decoded, a decision must be made whether to 
execute that  instruction  or to  take an  instruction  from the 
head of  the  R-stream Queue.  Since performance is  a pri- 
ority  in  REESE,  we  want  to  always  choose the  P  stream 
instruction, whenever possible.  This is where the  issue of 
idle capacity becomes important. When dependencies limit 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:02:23 UTC from IEEE Xplore.  Restrictions apply. 
the ability of the pipeline to execute a P stream instruction, 
an R stream instruction can be scheduled instead. 
The scheduler also needs  logic to help avoid  R-stream 
Queue overflow.  Counters can be used to keep track of the 
number of P and R instructions in the pipeline, as well as the 
size of the R-stream  Queue. This data determines whether 
or not an R-stream instruction must  be  scheduled.  This is 
the only way that the R-stream Queue can inhibit the normal 
flow of  instructions through  the  pipeline.  Since a full R- 
stream Queue blocks the execution of  P  instructions, it  is 
critical to set the buffer to an appropriate length. 
Extra forwarding hardware is also necessary.  No result 
from  a  primary  instruction  can  be  committed to  the  reg- 
ister  or  memory  state  before  it  has  been  compared  with 
its  redundant  counterpart.  However,  a  result  can  be  for- 
warded  to other instructions that  need  it  to execute.  This 
allows  instruction  execution  to  proceed  at  normal  speed, 
while only  instruction  commitment is delayed.  Results of 
P-stream STORE instructions may  be forwarded to subse- 
quent LOAD instructions, but the results may  not be com- 
mitted into memory before they have been compared to their 
R-stream counterparts. 
Hardware to compare the P-stream and R-stream instruc- 
tion results must be added between the writeback and com- 
mit  stages  of  the  pipeline.  Very  little  hardware  will  be 
needed  to  accomplish this.  If  a comparison fails, the mi- 
croprocessor is already capable of flushing the subsequent 
instructions in the pipeline.  The R-stream Queue will also 
need  to  be  cleared.  The first instruction  to enter the fetch 
stage after this happens will be the instruction where the er- 
ror  was  detected.  If  the  instruction  is  still  found  to  be  in 
error, the pipeline  will  have  to  stop and notify  the user of 
the error. 
Complex  interactions between  the  RUU  and  R-stream 
Queue can  increase overall efficiency.  Specifically, the R- 
stream  Queue can  be  allowed  to remove instructions from 
the  pipeline  before  the  instructions  are  ready  to  commit. 
An  instruction  that  completes  quickly  normally  needs  to 
wait  for other  instructions before  it  can  commit (in-order 
commit). However, with REESE, P-stream instructions that 
complete early can  be put  into the R-stream  Queue imme- 
diately.  This speeds up execution,  but requires  additional 
hardware complexity.  Justifying this hardware complexity 
is similar to justifying out-of-order execution in superscalar 
processors:  increased  speed, efficiency, and hardware uti- 
lization are worth the extra complexity. 
It  is  difficult  to  know  the  exact  amount  of  hardware 
overhead  that  would  be  required  to  implement  REESE. 
The majority of the hardware relates to forwarding, the R- 
stream Queue,  and scheduling.  Implementing these  func- 
tions  should  not  cause  space problems on  the  chip,  since 
similar functions are already implemented in the micropro- 
cessor without requiring large amounts of die area. 
4.4  Increasing Implementation Efficiency 
Since every instruction in the P stream is executed a sec- 
ond time, it is logical to assume that, in the worst case, to- 
tal program execution time would  double when REESE is 
used.  Past research has shown that actual implementations 
can achieve much better execution times. This section gives 
three reasons why REESE can perform much better than the 
worst case. 
The first reason  is the idle  capacity  inherent  in  super- 
scalar microprocessors.  REESE can utilize this idle capac- 
ity  to perform R-stream  instructions.  As stated earlier, the 
average throughput  for the  microprocessor pipeline  is ap- 
proximately  half  of the maximum possible throughput  for 
modern microprocessors.  Extra instructions can be strate- 
gically inserted into the instruction stream to increase IPC 
without increasing overall execution time.  However, con- 
tinuing  to  increase  the  number  of  extra  instructions  will 
eventually  cause  a time  increase.  REESE doubles the to- 
tal number of  instructions executed  while minimizing this 
time increase. 
A second way that REESE increases pipeline efficiency 
is by eliminating data and control dependencies between R- 
stream  instructions.  It  does  this  by  utilizing  information 
from  the P-stream execution of each instruction.  After an 
instruction has executed once, REESE stores critical  infor- 
mation  gathered  during  the  P-stream  execution.  This  in- 
formation  is  used  by  the  instruction  for value  and control 
prediction during its redundant execution. Thus no data de- 
pendencies in R-stream instructions, and control dependen- 
cies are also eliminated.  By  the  time  a branch  instruction 
enters  the  R-stream  Queue, the  outcome of  the  branch  is 
known. That can be used for predicting the direction of the 
branch in the R stream. The direction is indicated by the in- 
structions that follow the branch  into the R-stream Queue. 
R-stream instruction execution verifies that indeed the value 
and control prediction were correct. 
A third reason  why the R stream can execute efficiently 
is because it will cause no extra misses in the first level of 
the  cache.  When  a  load/store  instruction  executes  in  the 
P stream,  it  will  bring  the  relevant  data  into the  cache  on 
a miss.  Therefore, the R-stream  version of the instruction 
will always hit in the cache (until and unless there is quick 
thrashing  between  P-stream  and R-stream  instruction  exe- 
cutions,  which  is  a  very  likely  event),  even  if  the  corre- 
sponding P-stream instruction was a miss. 
4.5  Adding Spare Capacity 
Even though we  utilize  the idle capacity  of the proces- 
sor, execution  time  will  still  increase.  In  addition  to  soft 
error detection, we want to eliminate this increase.  There- 
fore, our research explores the possibility  of compensating 