share information, and (RQ3) how existing in-the-wild descriptions
of DP set their expectations about these information sharing impli-
cations. Prior work on user expectations for information sharing [7]
notes that there is a lack of work considering how users reason
about information sharing under DP. Our work fills this gap. Addi-
tionally, we seek to replicate their results through (RQ4), in which
we examine how different descriptions of DP themselves influence
users willingness to share information.
In addition to primarily focusing on different research questions,
our work methodologically differs from the work of Xiong et al. [69]
in two ways. First, we derive the descriptions of DP we use as stimuli
from a systematic review of 76 in-the-wild descriptions of DP (see
Section 5). In contrast, in two of their three experiments Xiong
et al. [69] use DP descriptions that were created to explain the
definition and/or different aspects (e.g., data perturbation) of DP to
users2; more similar to our approach, in their third experiment they
use the descriptions from four companies that use DP, in addition
to their created descriptions. Second, and relatedly, Xiong et al. [69]
test whether respondents correctly understand the implications of
DP based on the description they were shown before respondents
are shown additional survey questions about their willingness to
share information. Respondents who did not correctly answer the
understanding question(s) were presented with the DP description
a second time; if they again did not understand the description, they
were excluded from the study. Their results thus have important
implications about how users can be educated about DP. On the
other hand, this methodology does not offer insight into how users‚Äô
privacy expectations or willingness to share information might be
influenced by encountering descriptions of DP in-the-wild, rather
than in a laboratory setting. As further discussed in Section 6, the
results of our replication study (RQ4) significantly differ from those
of Xiong et al. [69] likely due to the methodological differences in
our approaches.
3 SUMMARY OF METHODS
To answer our research questions, we ran two surveys (ùëõ = 2, 424
total), one to address RQ1 and RQ2 and the other to address RQ3
and RQ4. In order to improve the external validity of our work, we
use vignette (scenario) surveys [26]. In both our surveys, we use the
same two scenarios, which focus on two different potential use cases
for DP: protecting people‚Äôs salaries and protecting people‚Äôs medical
histories. To contextualize the first scenario, we ask respondents to
imagine that they work in the banking industry and are approached
by a friend on behalf of a salary transparency initiative. In the
second scenario, we ask the respondent to imagine that during their
next doctor‚Äôs visit, their doctor asks them if they want to share their
medical records with a medical research non-profit, in the name
2The most closely related definition to our work is the DP with implications description:
‚ÄúTo respect your personal information privacy and ensure best user experience, the
data shared with the app will be processed via the differential privacy (DP) technique.
That is, the app company will store your data but only use the aggregated statistics
with modification so that your personal information cannot be learned. However, your
personal information may be leaked if the company‚Äôs database is compromised.‚Äù
of improving care. The exact wording of these scenarios is shown
in Table 1. This table also contains concrete privacy expectations
about which we asked respondents in both surveys; we discuss these
expectations further in the following sections. We ran both surveys
using Amazon Mechanical Turk (MTurk). MTurk has been shown
to be representative of American privacy preferences for Americans
aged 18-50 who have at least some college education [59].
We present detailed overviews of each survey in Section 4 and
Section 5 respectively. Similarly, we present our findings from each
survey in Section 4.2 and Section 5.2 respectively. Our full survey
instruments can be found in Appendix A. We also present demo-
graphic information for our survey respondents in Table 6, which
is also located in the appendix. Our procedures were approved by
our institutions‚Äô ethics review board.
3.1 Limitations
As in all user studies, our study is subject to multiple possible biases.
The first is sampling bias. We sample our respondents using MTurk.
While prior work shows that MTurk is reasonably representative
of the privacy attitudes and experiences of Americans aged 18-50
who have some college education [59], our sample does not capture
the experiences of all Americans, especially those older and less
educated. Our results should be interpreted in this context. Second,
we may have introduced reporting biases through our question
design. While we aimed to follow best practices ‚Äî using cognitive
interviews to validate our questionnaire, and offering ‚Äúother‚Äù and
‚ÄúI don‚Äôt know‚Äù response options [57] ‚Äî respondents may still have
mis-reported or failed to report their true perceptions or preferences.
Third, while we took steps to improve external validity ‚Äî sourcing
experimental stimuli by rigorously collecting and coalescing in-the-
wild DP descriptions and using a vignette survey ‚Äî our study may
have failed to appropriately reflect real-world conditions.
4 IMPACT OF INFORMATION DISCLOSURES
(RQ1 & RQ2)
In our first survey, we aimed to answer RQ1 and RQ2. Namely, we
wanted to determine if (a) users cared about the kinds of information
disclosures against which DP can protect, and (b) users would be
more willing to share their sensitive information when the risk of
such information disclosures decreased.
Information Disclosures. Contextual integrity (CI) theory ‚Äì a
commonly used framework to explain end-user privacy reasoning ‚Äì
posits that users‚Äô privacy decisions depend heavily on information
flows ‚Äì what information is being transmitted to which entities
under what privacy expectations [51]. Our scenarios define a set
of expected information flows (e.g., salary information moving
from the user, to the salary transparency initiative, under some
privacy protection ‚Äì as described further in the following sections).
Based on our descriptions, users may have different expectations
for whether unexpected information flows (e.g., information being
shared with an entity they did not intend to share it with) may
occur. We term these unexpected information flows ‚Äúinformation
disclosures‚Äù throughout the remainder of the paper.
As survey one seeks to investigate the role of different types
of information disclosures in users‚Äô sharing behaviors and survey
two seeks to investigate how existing methods of describing DP
Session 11C: Software Development and Analysis CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea3040Scenario Name
Salary Scenario
Medical Scenario
Expectation Name
Hack
Law Enforcement
Organization
Data Analyst
Graphs
Share
Scenario Description
‚ÄúImagine that you work in the banking industry. You are friends with a group of other people who work in banking companies
in your city. One of your friends is part of a transparency initiative that is trying to publish general statistics about pay in
the banking industry. As part of this initiative, they have asked everyone in the group to share their salaries and job titles
using an online web form on the initiative‚Äôs website.‚Äù
‚ÄúImagine that during your next doctor‚Äôs visit, your primary care doctor informs you that they are part of a non-profit
organization trying to push the boundaries of medical research. This non-profit is asking patients around the country to
share their medical records, which will be used to help medical research on improving treatment options and patient care.
Your doctor, with your permission, can facilitate the non-profit getting the information they need.‚Äù
Expectation Description
‚ÄúA criminal or foreign government that hacks the transparency initiative
could learn my salary and job title‚Äù
‚ÄúA criminal or foreign government that hacks the non-profit could learn my
medical history‚Äù
‚ÄúA law enforcement organization could access my salary and job title with a
court order requesting this data from the initiative‚Äù
‚ÄúA law enforcement organization could access my medical history with a
court order requesting this data from the non-profit‚Äù
‚ÄúMy friend will not be able to learn my salary and job title‚Äù
‚ÄúThe contents of my medical record will be stored only by my doctor‚Äôs office,
and will not be stored by the non-profit‚Äù
‚ÄúA data analyst working on the salary transparency initiative could learn
my exact salary and job title‚Äù
‚ÄúA data analyst working for the non-profit would be able to see my exact
medical history‚Äù
‚ÄúGraphs or informational charts created using information given to the
salary transparency initiative could reveal my salary and job title.‚Äù
‚ÄúGraphs or informational charts created using information given to the
non-profit could reveal my medical history.‚Äù
‚ÄúData that the salary transparency initiative shares with other organizations
doing salary research could reveal my salary and job title‚Äù
‚ÄúData that the non-profit shares with other organizations doing medical
research could reveal my medical history‚Äù
Ground Truth: Local Ground Truth: Central
False
False
True
False
False
False
True
True
False
True
False
True
Table 1: Scenarios (top) and information disclosure expectations (bottom) used in both survey one and survey two.
influence user expectations for information disclosure, both surveys
address the same potential information disclosures that could occur
in either scenario.
While we would like users to tell us about the kinds of disclosure
that concerns them, prior work has shown that users often do not
have good mental models for privacy tools [2, 42, 43]. To compen-
sate for this, we leveraged prior work [10, 32, 37, 47, 52, 68] on both
DP and on user privacy concerns to create a preliminary list of infor-
mation disclosures about which a user might care. Our list included
the following kinds of information disclosure (names in italics):
(Hack) Could a criminal organization or foreign government access
the respondent‚Äôs information by hacking the organization hold-
ing the information?; (Law Enforcement) Could a law enforcement
organization access the respondent‚Äôs information with a court-is-
sued warrant?; (Organization) Could the organization collecting
the information (or their representative) access the respondent‚Äôs in-
formation?; (Data Analyst) Could a data analyst working within the
organization access the respondent‚Äôs information?; (Graphs) Could
graphs or informational charts created by the organization be used
to learn the respondent‚Äôs information?; and (Share) Could the col-
lected information be shared with another organization such that
the other organization could access the respondent‚Äôs information?
While some of these questions are redundant from a techni-
cal perspective (e.g., hack and law enforcement), we chose these
questions to be representative of real data-privacy concerns that
potential users might have, since prior work finds that a key part
of users‚Äô reasoning about privacy is how appropriate they consider
different information flows [51].
4.1 Methodology: Survey One
To ensure that we had not missed information disclosures about
which users were concerned, we first conducted five cognitive
Session 11C: Software Development and Analysis CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea3041interviews3 and offered survey respondents the opportunity to list
other information disclosures about which they cared. Fewer than
2% entered ag disclosure not captured in our list. As such, we use
the above list of information disclosures throughout this work. We
present the descriptions of these expectations in Table 1.
In Table 1, we also indicate the ground truth for each of these
information disclosures in both the local and central model of DP.
Both central and local DP protect against information disclosure
through graphs, as this is the core privacy guarantee of DP. The cen-
tral model aggregates raw user data into a centralized database that
can potentially be accessed by the data analyst, employees of the
organization, entities that hack the organization, law enforcement
(with proper court orders), and partner organizations with whom
the dataset is shared. In the local model, the aggregated dataset
contains only DP versions of user data, so information disclosure
would not occur even if the dataset is accessed by these entities.
We stress that we are considering a ‚Äútypical‚Äù DP deployment
and acknowledge that there are deployments for which our ground
truths are not correct. For instance, we indicate that a data analyst
would be able to learn a potential user‚Äôs exact information in the
central model. However, Uber deployed central DP specifically to
protect users‚Äô information against curious data analysts. As it is
impossible to account for all possible system parameters and design
options, we derive our ground truth from the most simple setup.
Questionnaire. Each respondent was randomly assigned to either
the salary scenario or the medical scenario described above. Then,
each respondent was asked to indicate which of the information
disclosures, described above, they would want to better understand
before sharing their information. Additionally, they were given the
option of adding any additional disclosures about which they would
want additional information. For each information disclosure event
that the respondent indicated they would want to better understand,
the respondent was presented with one of the following explicit
risks, chosen at random:
(1) there is no risk of this information disclosure,
(2) the risk of this information disclosure is the same as the chance
that your bank account will be compromised (accessed by a
person who you did not intend to gain access to) as part of a
data breach in the next year, and
(3) the risk of this information disclosure is higher than the chance
that your bank account will be compromised (accessed by a
person who you did not intend to gain access to) as part of a
data breach in the next year.
We set expectations in this way because (a) prior work on how
humans interpret numbers and risk suggests that reference events
of a similar type improve risk comprehension [25, 40, 60, 64], and
(b) prior work shows that users have concrete estimates for the
likelihood of bank account compromise, a frequently discussed
security event [5, 38, 65]. Each respondent was then asked if they
would be willing to share their information with the initiative.
Additionally, they were asked to describe why they would or would
not be willing to share their data.
3Cognitive interviewing is a survey methodology technique in which participants
think aloud as they answer a survey [57]. Cognitive interviews are used to verify that
potential respondents understand the survey questions and no answer choices are
missing. We conducted interviews until no new survey protocol corrections emerged.
Figure 1: Proportion of respondents who care about each po-
tential information disclosure.
Finally, each survey concluded with a battery of demographic
questions, including a measurement of internet skill using an ex-
isting validated measure [28], as prior work suggests that internet
skill is among the most relevant constructs to control for in privacy
studies [29, 30, 56, 58]. The complete survey is in Appendix A.1.
Sample. We surveyed 1,216 U.S. Amazon Mechanical Turk workers.
These workers were split evenly between the two survey scenarios.
To ensure high quality responses, we required that respondents
have at least a 95% approval rating [55]. The demographics of our
sample are reported in Table A in the appendix.
Analysis. To answer RQ1, we conduct a descriptive analysis, re-
porting the proportion of respondents who were concerned about
each potential information disclosure event; when reporting dif-
ferences between proportions of respondents who report concern,
we use ùúí2 proportion tests to validate that the differences between
proportions are significant. To answer RQ2, we build six logistic
regression models, one for each potential information disclosure
event. In each model, the dependent variable (DV) is whether the
respondent is willing to share their information, the independent
variable (IV) of interest is the level of risk that the respondent was
told of the information disclosure occurring, and the control IVs are
the scenario type and the respondents‚Äô internet skill score. We re-
port the odds ratios (the exponentiated regression coefficients) with
95% confidence intervals, and p-values for each IV in the model.
In Section 4.2, we contextualize a subset of our results using
open-text responses participants provided to describe their sharing
decisions. These responses were analyzed through open-coding by
a member of the research team with qualitative research experience.
As these responses are not offered as primary research artifacts,
we do not double code this data nor provide intercoder agreement
statistics, per best-practice guidelines outlined in [45].
4.2 Information Disclosure Results
Here, we detail the results of our analyses of survey one.
RQ1: What Information Disclosures Concern Users? The goal
of DP is to protect user information against disclosure to various
entities. Thus, we investigate whether users care about potential
information disclosures to different entities against which DP can
protect (see Table 1 for information disclosures and Section 4.1 for
the source of these disclosures).
Information DisclosuresShareOrganizationHackData AnalystGraphsLaw EnforcementProportion of Respondents (n=1216)0%10%20%30%40%50%60%70%39.6%40.8%43.5%52.1%55.3%60.3%1Session 11C: Software Development and Analysis CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea3042Variable
Hack
Low Risk
No Risk
Salary Scenario
Internet Score
OR/CI
1.91
[1.27, 2.88]
2.97
[1.98, 4.49]
1.44
[1.04, 2]
1.04
[0.86, 1.25]
p-value
< 0.01**
< 0.01***
0.03*
0.68
Law Enforcement
OR/CI
p-value
0.87
0.55
[0.54, 1.39]
2.07
[1.32, 3.27]
1.35
[0.92, 1.98]
1.01
[0.82, 1.24]
< 0.01**
0.12
0.92
Organization
OR/CI
p-value
1.63
[1.12, 2.38]
1.61
[1.1, 2.35]
1.20
[0.89, 1.64]