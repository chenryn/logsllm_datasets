essence, we have the pleasure of playing both the roles of the pilot and the engineer/
designer. Our experiences running massive computing infrastructure are codified in
actual code and then packaged as a discrete product.
1 Vice President, Site Reliability Engineering, for Google, Inc.
473
These solutions are then easily usable by other SRE teams and ultimately by anyone at
Google (or even outside of Google…think Google Cloud!) who wants to use or
improve upon the experience we’ve accumulated and the systems we’ve built.
When you approach building a team or a system, ideally its foundation should be a
set of rules or axioms that are general enough to be immediately useful, but that will
remain relevant in the future. Much of what Ben Treynor Sloss outlined in this book’s
introduction represents just that: a flexible, mostly future-proof set of responsibilities
that remain spot-on 10 years after they were conceived, despite the changes and
growth Google’s infrastructure and the SRE team have undergone.
As SRE has grown, we’ve noticed a couple different dynamics at play. The first is the
consistent nature of SRE’s primary responsibilities and concerns over time: our sys‐
tems might be 1,000 times larger or faster, but ultimately, they still need to remain
reliable, flexible, easy to manage in an emergency, well monitored, and capacity
planned. At the same time, the typical activities undertaken by SRE evolve by neces‐
sity as Google’s services and SRE’s competencies mature. For example, what was once
a goal to “build a dashboard for 20 machines” might now instead be “automate dis‐
covery, dashboard building, and alerting over a fleet of tens of thousands of
machines.”
For those who haven’t been in the trenches of SRE for the past decade, an analogy
between how SRE thinks about complex systems and how the aircraft industry has
approached plane flight is useful in conceptualizing how SRE has evolved and
matured over time. While the stakes of failure between the two industries are very
different, certain core similarities hold true.
Imagine that you wanted to fly between two cities a hundred years ago. Your airplane
probably had a single engine (two, if you were lucky), a few bags of cargo, and a pilot.
The pilot also filled the role of mechanic, and possibly additionally acted as cargo
loader and unloader. The cockpit had room for the pilot, and if you were lucky, a co-
pilot/navigator. Your little plane would bounce off a runway in good weather, and if
everything went well, you’d slowly climb your way through the skies and eventually
touch down in another city, maybe a few hundred miles away. Failure of any of the
plane’s systems was catastrophic, and it wasn’t unheard of for a pilot to have to climb
out of the cockpit to perform repairs in-flight! The systems that fed into the cockpit
were essential, simple, and fragile, and most likely were not redundant.
Fast-forward a hundred years to a huge 747 sitting on the tarmac. Hundreds of pas‐
sengers are loading up on both floors, while tons of cargo are simultaneously being
loaded into the hold below. The plane is chock-full of reliable, redundant systems. It’s
a model of safety and reliability; in fact, you’re actually safer in the air than on the
ground in a car. Your plane will take off from a dotted line on a runway on one conti‐
nent, and land easily on a dotted line on another runway 6,000 miles away, right on
474 | Chapter 34: Conclusion
schedule—within minutes of its forecasted landing time. But take a look into the
cockpit and what do you find? Just two pilots again!
How has every other element of the flight experience—safety, capacity, speed, and
reliability—scaled up so beautifully, while there are still only two pilots? The answer
to this question is a great parallel to how Google approaches the enormous, fantasti‐
cally complex systems that SRE runs. The interfaces to the plane’s operating systems
are well thought out and approachable enough that learning how to pilot them in
normal conditions is not an insurmountable task. Yet these interfaces also provide
enough flexibility, and the people operating them are sufficiently trained, that respon‐
ses to emergencies are robust and quick. The cockpit was designed by people who
understand complex systems and how to present them to humans in a way that’s both
consumable and scalable. The systems underlying the cockpit have all the same prop‐
erties discussed in this book: availability, performance optimization, change manage‐
ment, monitoring and alerting, capacity planning, and emergency response.
Ultimately, SRE’s goal is to follow a similar course. An SRE team should be as com‐
pact as possible and operate at a high level of abstraction, relying upon lots of backup
systems as failsafes and thoughtful APIs to communicate with the systems. At the
same time, the SRE team should also have comprehensive knowledge of the systems
—how they operate, how they fail, and how to respond to failures—that comes from
operating them day-to-day.
Conclusion | 475
APPENDIX A
Availability Table
Availability is generally calculated based on how long a service was unavailable over
some period. Assuming no planned downtime, Table A-1 indicates how much down‐
time is permitted to reach a given availability level.
Table A-1. Availability table
Availability level Allowed unavailability window
per year per quarter per month per week per day per hour
90% 36.5 days 9 days 3 days 16.8 hours 2.4 hours 6 minutes
95% 18.25 days 4.5 days 1.5 days 8.4 hours 1.2 hours 3 minutes
99% 3.65 days 21.6 hours 7.2 hours 1.68 hours 14.4 minutes 36 seconds
99.5% 1.83 days 10.8 hours 3.6 hours 50.4 minutes 7.20 minutes 18 seconds
99.9% 8.76 hours 2.16 hours 43.2 minutes 10.1 minutes 1.44 minutes 3.6 seconds
99.95% 4.38 hours 1.08 hours 21.6 minutes 5.04 minutes 43.2 seconds 1.8 seconds
99.99% 52.6 minutes 12.96 minutes 4.32 minutes 60.5 seconds 8.64 seconds 0.36 seconds
99.999% 5.26 minutes 1.30 minutes 25.9 seconds 6.05 seconds 0.87 seconds 0.04 seconds
Using an aggregate unavailability metric (i.e., "X% of all operations failed”) is more
useful than focusing on outage lengths for services that may be partially available—
for instance, due to having multiple replicas, only some of which are unavailable—
and for services whose load varies over the course of a day or week rather than
remaining constant.
See Equations 3-1 and 3-2 in Chapter 3 for calculations.
477
APPENDIX B
A Collection of Best Practices for
Production Services
Written by Ben Treynor Sloss
Edited by Betsy Beyer
Fail Sanely
Sanitize and validate configuration inputs, and respond to implausible inputs by both
continuing to operate in the previous state and alerting to the receipt of bad input.
Bad input often falls into one of these categories:
Incorrect data
Validate both syntax and, if possible, semantics. Watch for empty data and partial
or truncated data (e.g., alert if the configuration is N% smaller than the previous
version).
Delayed data
This may invalidate current data due to timeouts. Alert well before the data is
expected to expire.
Fail in a way that preserves function, possibly at the expense of being overly permis‐
sive or overly simplistic. We’ve found that it’s generally safer for systems to continue
functioning with their previous configuration and await a human’s approval before
using the new, perhaps invalid, data.
479
Examples
In 2005, Google’s global DNS load- and latency-balancing system received an empty
DNS entry file as a result of file permissions. It accepted this empty file and served
NXDOMAIN for six minutes for all Google properties. In response, the system now per‐
forms a number of sanity checks on new configurations, including confirming the
presence of virtual IPs for google.com, and will continue serving the previous DNS
entries until it receives a new file that passes its input checks.
In 2009, incorrect (but valid) data led to Google marking the entire Web as containing
malware [May09]. A configuration file containing the list of suspect URLs was
replaced by a single forward slash character (/), which matched all URLs. Checks for
dramatic changes in file size and checks to see whether the configuration is matching
sites that are believed unlikely to contain malware would have prevented this from
reaching production.
Progressive Rollouts
Nonemergency rollouts must proceed in stages. Both configuration and binary
changes introduce risk, and you mitigate this risk by applying the change to small
fractions of traffic and capacity at one time. The size of your service or rollout, as well
as your risk profile, will inform the percentages of production capacity to which the
rollout is pushed, and the appropriate time frame between stages. It’s also a good idea
to perform different stages in different geographies, in order to detect problems
related to diurnal traffic cycles and geographical traffic mix differences.
Rollouts should be supervised. To ensure that nothing unexpected is occurring dur‐
ing the rollout, it must be monitored either by the engineer performing the rollout
stage or—preferably—a demonstrably reliable monitoring system. If unexpected
behavior is detected, roll back first and diagnose afterward in order to minimize
Mean Time to Recovery.
Define SLOs Like a User
Measure availability and performance in terms that matter to an end user. See Chap‐
ter 4 for more discussion.
480 | Appendix B: A Collection of Best Practices for Production Services
Example
Measuring error rates and latency at the Gmail client, rather than at the server, resul‐
ted in a substantial reduction in our assessment of Gmail availability, and prompted
changes to both Gmail client and server code. The result was that Gmail went from
about 99.0% available to over 99.9% available in a few years.
Error Budgets
Balance reliability and the pace of innovation with error budgets (see “Motivation for
Error Budgets” on page 33), which define the acceptable level of failure for a service,
over some period; we often use a month. A budget is simply 1 minus a service’s SLO;
for instance, a service with a 99.99% availability target has a 0.01% “budget” for
unavailability. As long as the service hasn’t spent its error budget for the month
through the background rate of errors plus any downtime, the development team is
free (within reason) to launch new features, updates, and so on.
If the error budget is spent, the service freezes changes (except urgent security and
bug fixes addressing any cause of the increased errors) until either the service has
earned back room in the budget, or the month resets. For mature services with an
SLO greater than 99.99%, a quarterly rather than monthly budget reset is appropriate,
because the amount of allowable downtime is small.
Error budgets eliminate the structural tension that might otherwise develop between
SRE and product development teams by giving them a common, data-driven mecha‐
nism for assessing launch risk. They also give both SRE and product development
teams a common goal of developing practices and technology that allow faster inno‐
vation and more launches without “blowing the budget.”
Monitoring
Monitoring may have only three output types:
Pages
A human must do something now
Tickets
A human must do something within a few days
Logging
No one need look at this output immediately, but it’s available for later analysis if
needed
A Collection of Best Practices for Production Services | 481
If it’s important enough to bother a human, it should either require immediate action
(i.e., page) or be treated as a bug and entered into your bug-tracking system. Putting
alerts into email and hoping that someone will read all of them and notice the impor‐
tant ones is the moral equivalent of piping them to /dev/null: they will eventually be
ignored. History demonstrates this strategy is an attractive nuisance because it can
work for a while, but it relies on eternal human vigilance, and the inevitable outage is
thus more severe when it happens.
Postmortems
Postmortems (see Chapter 15) should be blameless and focus on process and technol‐
ogy, not people. Assume the people involved in an incident are intelligent, are well
intentioned, and were making the best choices they could given the information they
had available at the time. It follows that we can’t “fix” the people, but must instead fix
their environment: e.g., improving system design to avoid entire classes of problems,
making the appropriate information easily available, and automatically validating
operational decisions to make it difficult to put systems in dangerous states.
Capacity Planning
Provision to handle a simultaneous planned and unplanned outage, without making
the user experience unacceptable; this results in an "N + 2” configuration, where peak
traffic can be handled by N instances (possibly in degraded mode) while the largest 2
instances are unavailable:
• Validate prior demand forecasts against reality until they consistently match.
Divergence implies unstable forecasting, inefficient provisioning, and risk of a
capacity shortfall.
• Use load testing rather than tradition to establish the resource-to-capacity ratio: a
cluster of X machines could handle Y queries per second three months ago, but
can it still do so given changes to the system?
• Don’t mistake day-one load for steady-state load. Launches often attract more
traffic, while they’re also the time you especially want to put the product’s best
foot forward. See Chapter 27 and Appendix E.
482 | Appendix B: A Collection of Best Practices for Production Services
Overloads and Failure
Services should produce reasonable but suboptimal results if overloaded. For exam‐
ple, Google Search will search a smaller fraction of the index, and stop serving fea‐
tures like Instant to continue to provide good quality web search results when
overloaded. Search SRE tests web search clusters beyond their rated capacity to
ensure they perform acceptably when overloaded with traffic.
For times when load is high enough that even degraded responses are too expensive
for all queries, practice graceful load shedding, using well-behaved queuing and
dynamic timeouts; see Chapter 21. Other techniques include answering requests after
a significant delay (“tarpitting”) and choosing a consistent subset of clients to receive
errors, preserving a good user experience for the remainder.
Retries can amplify low error rates into higher levels of traffic, leading to cascading
failures (see Chapter 22). Respond to cascading failures by dropping a fraction of
traffic (including retries!) upstream of the system once total load exceeds total
capacity.
Every client that makes an RPC must implement exponential backoff (with jitter) for
retries, to dampen error amplification. Mobile clients are especially troublesome
because there may be millions of them and updating their code to fix behavior takes a
significant amount of time—possibly weeks—and requires that users install updates.
SRE Teams
SRE teams should spend no more than 50% of their time on operational work (see
Chapter 5); operational overflow should be directed to the product development
team. Many services also include the product developers in the on-call rotation and
ticket handling, even if there is currently no overflow. This provides incentives to
design systems that minimize or eliminate operational toil, along with ensuring that
the product developers are in touch with the operational side of the service. A regular
production meeting between SREs and the development team (see Chapter 31) is also
helpful.
We’ve found that at least eight people need to be part of the on-call team, in order to
avoid fatigue and allow sustainable staffing and low turnover. Preferably, those on-
call should be in two well-separated geographic locations (e.g., California and Ire‐
land) to provide a better quality of life by avoiding nighttime pages; in this case, six
people at each site is the minimum team size.
Expect to handle no more than two events per on-call shift (e.g., per 12 hours): it
takes time to respond to and fix outages, start the postmortem, and file the resulting
bugs. More frequent events may degrade the quality of response, and suggest that
A Collection of Best Practices for Production Services | 483
something is wrong with (at least one of) the system’s design, monitoring sensitivity,
and response to postmortem bugs.
Ironically, if you implement these best practices, the SRE team may eventually end up
out of practice in responding to incidents due to their infrequency, making a long
outage out of a short one. Practice handling hypothetical outages (see “Disaster Role
Playing” on page 401) routinely and improve your incident-handling documentation
in the process.
484 | Appendix B: A Collection of Best Practices for Production Services
APPENDIX C
Example Incident State Document
Shakespeare Sonnet++ Overload: 2015-10-21
Incident management info: http://incident-management-cheat-sheet
(Communications lead to keep summary updated.)
Summary: Shakespeare search service in cascading failure due to newly discovered
sonnet not in search index.
Status: active, incident #465
Command Post(s): #shakespeare on IRC
Command Hierarchy (all responders)
• Current Incident Commander: jennifer
—Operations lead: docbrown
—Planning lead: jennifer
—Communications lead: jennifer
• Next Incident Commander: to be determined
(Update at least every four hours and at handoff of Comms Lead role.)
Detailed Status (last updated at 2015-10-21 15:28 UTC by jennifer)
Exit Criteria:
• New sonnet added to Shakespeare search corpus TODO