User: Need advice on the organization of logging in a clustered application written in Python(asyncio). Applications use the logging module and store logs local file.
See the logs with 3 servers uncomfortable.
I would like to use rsyslog, but there are fears that it will block the application. Another way to use aioredis(push to channel) and another application to collect data in a single file.
Assistant: A simple solution is to ask for logging to log on syslog (typically /dev/log, which won't block your application), locally (so your application is not bound to your logging system: it's still portable), then let rsyslog (I prefer syslogng personally) transmit them to a main log server.
Another solution is to use a tool like logstash to push your logs to an elasticsearch server / cluster so you can browse and graph them easily. In this case, if your log lines are json objects, it's a big win because elasticsearch-side (typically via kibana), you'll be able to query, filter, and aggregate on fields of your json documents. Typically graphing info vs warnings vs errors, frequency of errors per file, or per user, etc...