resiliency phase because no one single processor can ultimately remain to
manage the clock. Therefore, on client systems, timer serialization is enabled
if Modern Standby is available, which causes the kernel to choose CPU 0 no
matter what. This allows CPU 0 to behave as the default clock owner—the
processor that will always be active to pick up clock interrupts (more on this
later).
 Note
This behavior is controlled by the kernel variable
KiSerializeTimerExpiration, which is initialized based on a registry
setting whose value is different between a server and client installation.
By modifying or creating the value SerializeTimerExpiration under
HKLM\SYSTEM\CurrentControlSet\Control\Session Manager\Kernel
and setting it to any value other than 0 or 1, serialization can be disabled,
enabling timers to be distributed among processors. Deleting the value, or
keeping it as 0, allows the kernel to make the decision based on Modern
Standby availability, and setting it to 1 permanently enables serialization
even on non-Modern Standby systems.
EXPERIMENT: Listing system timers
You can use the kernel debugger to dump all the current registered
timers on the system, as well as information on the DPC associated
with each timer (if any). See the following output for a sample:
Click here to view code image
0: kd> !timer
Dump system timers
Interrupt time: 250fdc0f 00000000 [12/21/2020 03:30:27.739]
PROCESSOR 0 (nt!_KTIMER_TABLE fffff8011bea6d80 - Type 0 - 
High precision)
List Timer             Interrupt Low/High Fire Time                  
DPC/thread
PROCESSOR 0 (nt!_KTIMER_TABLE fffff8011bea6d80 - Type 1 - 
Standard)
List Timer             Interrupt Low/High Fire Time                
DPC/thread
 1 ffffdb08d6b2f0b0   0807e1fb 80000000 [         NEVER         
] thread ffffdb08d748f480
 4 ffffdb08d7837a20   6810de65 00000008 [12/21/2020 
04:29:36.127]
 6 ffffdb08d2cfc6b0   4c18f0d1 00000000 [12/21/2020 
03:31:33.230] netbt!TimerExpiry
(DPC @ ffffdb08d2cfc670)
   fffff8011fd3d8a8 A fc19cdd1 00589a19 [ 1/ 1/2100 
00:00:00.054] nt!ExpCenturyDpcRoutine
(DPC @ fffff8011fd3d868)
 7 ffffdb08d8640440   3b22a3a3 00000000 [12/21/2020 
03:31:04.772] thread ffffdb08d85f2080
   ffffdb08d0fef300   7723f6b5 00000001 [12/21/2020 
03:39:54.941]
FLTMGR!FltpIrpCtrlStackProfilerTimer (DPC @ 
ffffdb08d0fef340)
11 fffff8011fcffe70   6c2d7643 00000000 [12/21/2020 
03:32:27.052] nt!KdpTimeSlipDpcRoutine
(DPC @ fffff8011fcffe30)
   ffffdb08d75f0180   c42fec8e 00000000 [12/21/2020 
03:34:54.707] thread ffffdb08d75f0080
14 fffff80123475420   283baec0 00000000 [12/21/2020 
03:30:33.060] tcpip!IppTimeout
(DPC @ fffff80123475460)
. . .
58 ffffdb08d863e280 P 3fec06d0 00000000 [12/21/2020 
03:31:12.803] thread ffffdb08d8730080
   fffff8011fd3d948 A 90eb4dd1 00000887 [ 1/ 1/2021 
00:00:00.054] nt!ExpNextYearDpcRoutine
(DPC @ fffff8011fd3d908)
. . .
104 ffffdb08d27e6d78 P 25a25441 00000000 [12/21/2020 
03:30:28.699]
tcpip!TcpPeriodicTimeoutHandler (DPC @ ffffdb08d27e6d38)
    ffffdb08d27e6f10 P 25a25441 00000000 [12/21/2020 
03:30:28.699]
tcpip!TcpPeriodicTimeoutHandler (DPC @ ffffdb08d27e6ed0)
106 ffffdb08d29db048 P 251210d3 00000000 [12/21/2020 
03:30:27.754]
CLASSPNP!ClasspCleanupPacketTimerDpc (DPC @ 
ffffdb08d29db088)
    fffff80122e9d110   258f6e00 00000000 [12/21/2020 
03:30:28.575]
Ntfs!NtfsVolumeCheckpointDpc (DPC @ fffff80122e9d0d0)
108 fffff8011c6e6560    19b1caef 00000002 [12/21/2020 
03:44:27.661]
tm!TmpCheckForProgressDpcRoutine (DPC @ fffff8011c6e65a0)
111 ffffdb08d27d5540 P  25920ab5 00000000 [12/21/2020 
03:30:28.592]
storport!RaidUnitPendingDpcRoutine (DPC @ ffffdb08d27d5580)
    ffffdb08d27da540 P  25920ab5 00000000 [12/21/2020 
03:30:28.592]
storport!RaidUnitPendingDpcRoutine (DPC @ ffffdb08d27da580)
. . .
Total Timers: 221, Maximum List: 8
Current Hand: 139
In this example, which has been shortened for space reasons,
there are multiple driver-associated timers, due to expire shortly,
associated with the Netbt.sys and Tcpip.sys drivers (both related to
networking), as well as Ntfs, the storage controller driver drivers.
There are also background housekeeping timers due to expire, such
as those related to power management, ETW, registry flushing, and
Users Account Control (UAC) virtualization. Additionally, there
are a dozen or so timers that don’t have any DPC associated with
them, which likely indicates user-mode or kernel-mode timers that
are used for wait dispatching. You can use !thread on the thread
pointers to verify this.
Finally, three interesting timers that are always present on a
Windows system are the timer that checks for Daylight Savings
Time time-zone changes, the timer that checks for the arrival of the
upcoming year, and the timer that checks for entry into the next
century. One can easily locate them based on their typically distant
expiration time, unless this experiment is performed on the eve of
one of these events.
Intelligent timer tick distribution
Figure 8-20, which shows processors handling the clock ISR and expiring
timers, reveals that processor 1 wakes up several times (the solid arrows)
even when there are no associated expiring timers (the dotted arrows).
Although that behavior is required as long as processor 1 is running (to
update the thread/process run times and scheduling state), what if processor 1
is idle (and has no expiring timers)? Does it still need to handle the clock
interrupt? Because the only other work required that was referenced earlier is
to update the overall system time/clock ticks, it’s sufficient to designate
merely one processor as the time-keeping processor (in this case, processor 0)
and allow other processors to remain in their sleep state; if they wake, any
time-related adjustments can be performed by resynchronizing with processor
0.
Windows does, in fact, make this realization (internally called intelligent
timer tick distribution), and Figure 8-22 shows the processor states under the
scenario where processor 1 is sleeping (unlike earlier, when we assumed it
was running code). As you can see, processor 1 wakes up only five times to
handle its expiring timers, creating a much larger gap (sleeping period). The
kernel uses a variable KiPendingTimerBitmaps, which contains an array of
affinity mask structures that indicate which logical processors need to receive
a clock interval for the given timer hand (clock-tick interval). It can then
appropriately program the interrupt controller, as well as determine to which
processors it will send an IPI to initiate timer processing.
Figure 8-22 Intelligent timer tick distribution applied to processor 1.
Leaving as large a gap as possible is important due to the way power
management works in processors: as the processor detects that the workload
is going lower and lower, it decreases its power consumption (P states), until
it finally reaches an idle state. The processor then can selectively turn off
parts of itself and enter deeper and deeper idle/sleep states, such as turning
off caches. However, if the processor has to wake again, it will consume
energy and take time to power up; for this reason, processor designers will
risk entering these lower idle/sleep states (C-states) only if the time spent in a
given state outweighs the time and energy it takes to enter and exit the state.
Obviously, it makes no sense to spend 10 ms to enter a sleep state that will
last only 1 ms. By preventing clock interrupts from waking sleeping
processors unless needed (due to timers), they can enter deeper C-states and
stay there longer.
Timer coalescing
Although minimizing clock interrupts to sleeping processors during periods
of no timer expiration gives a big boost to longer C-state intervals, with a
timer granularity of 15 ms, many timers likely will be queued at any given
hand and expire often, even if just on processor 0. Reducing the amount of
software timer-expiration work would both help to decrease latency (by
requiring less work at DISPATCH_LEVEL) as well as allow other processors
to stay in their sleep states even longer. (Because we’ve established that the
processors wake up only to handle expiring timers, fewer timer expirations
result in longer sleep times.) In truth, it is not just the number of expiring
timers that really affects sleep state (it does affect latency), but the periodicity
of these timer expirations—six timers all expiring at the same hand is a better
option than six timers expiring at six different hands. Therefore, to fully
optimize idle-time duration, the kernel needs to employ a coalescing
mechanism to combine separate timer hands into an individual hand with
multiple expirations.
Timer coalescing works on the assumption that most drivers and user-
mode applications do not particularly care about the exact firing period of
their timers (except in the case of multimedia applications, for example).
This “don’t care” region grows as the original timer period grows—an
application waking up every 30 seconds probably doesn’t mind waking up
every 31 or 29 seconds instead, while a driver polling every second could
probably poll every second plus or minus 50 ms without too many problems.
The important guarantee most periodic timers depend on is that their firing
period remains constant within a certain range—for example, when a timer
has been changed to fire every second plus 50 ms, it continues to fire within
that range forever, not sometimes at every two seconds and other times at
half a second. Even so, not all timers are ready to be coalesced into coarser
granularities, so Windows enables this mechanism only for timers that have
marked themselves as coalescable, either through the
KeSetCoalescableTimer kernel API or through its user-mode counterpart,
SetWaitableTimerEx.
With these APIs, driver and application developers are free to provide the
kernel with the maximum tolerance (or tolerably delay) that their timer will
endure, which is defined as the maximum amount of time past the requested
period at which the timer will still function correctly. (In the previous
example, the 1-second timer had a tolerance of 50 ms.) The recommended
minimum tolerance is 32 ms, which corresponds to about twice the 15.6 ms
clock tick—any smaller value wouldn’t really result in any coalescing
because the expiring timer could not be moved even from one clock tick to
the next. Regardless of the tolerance that is specified, Windows aligns the
timer to one of four preferred coalescing intervals: 1 second, 250 ms, 100
ms, or 50 ms.
When a tolerable delay is set for a periodic timer, Windows uses a process
called shifting, which causes the timer to drift between periods until it gets
aligned to the most optimal multiple of the period interval within the
preferred coalescing interval associated with the specified tolerance (which is
then encoded in the dispatcher header). For absolute timers, the list of
preferred coalescing intervals is scanned, and a preferred expiration time is
generated based on the closest acceptable coalescing interval to the
maximum tolerance the caller specified. This behavior means that absolute
timers are always pushed out as far as possible past their real expiration
point, which spreads out timers as far as possible and creates longer sleep
times on the processors.
Now with timer coalescing, refer to Figure 8-20 and assume all the timers
specified tolerances and are thus coalescable. In one scenario, Windows
could decide to coalesce the timers as shown in Figure 8-23. Notice that now,
processor 1 receives a total of only three clock interrupts, significantly
increasing the periods of idle sleep, thus achieving a lower C-state.
Furthermore, there is less work to do for some of the clock interrupts on
processor 0, possibly removing the latency of requiring a drop
to DISPATCH_LEVEL at each clock interrupt.
Figure 8-23 Timer coalescing.
Enhanced timers
Enhanced timers were introduced to satisfy a long list of requirements that
previous timer system improvements had still not yet addressed. For one,
although timer coalescing reduced power usage, it also made timers have
inconsistent expiration times, even when there was no need to reduce power
(in other words, coalescing was an all-or-nothing proposition). Second, the
only mechanism in Windows for high-resolution timers was for applications
and drivers to lower the clock tick globally, which, as we’ve seen, had
significant negative impact on systems. And, ironically, even though the
resolution of these timers was now higher, they were not necessarily more
precise because regular time expiration can happen before the clock tick,
regardless of how much more granular it’s been made.
Finally, recall that the introduction of Connected/Modern Standby,
described in Chapter 6 of Part 1, added features such as timer virtualization
and the Desktop Activity Moderator (DAM), which actively delay the
expiration of timers during the resiliency phase of Modern Standby to
simulate S3 sleep. However, some key system timer activity must still be
permitted to periodically run even during this phase.
These three requirements led to the creation of enhanced timers, which are
also internally known as Timer2 objects, and the creation of new system calls
such as NtCreateTimer2 and NtSetTimer2, as well as driver APIs such as
ExAllocateTimer and ExSetTimer. Enhanced timers support four modes of
behavior, some of which are mutually exclusive:
■    No-wake This type of enhanced timer is an improvement over timer
coalescing because it provides for a tolerable delay that is only used
in periods of sleep.
■    High-resolution This type of enhanced timer corresponds to a high-
resolution timer with a precise clock rate that is dedicated to it. The
clock rate will only need to run at this speed when approaching the
expiration of the timer.
■    Idle-resilient This type of enhanced timer is still active even during
deep sleep, such as the resiliency phase of modern standby.
■    Finite This is the type for enhanced timers that do not share one of
the previously described properties.
High-resolution timers can also be idle resilient, and vice-versa. Finite
timers, on the other hand, cannot have any of the described properties.
Therefore, if finite enhanced timers do not have any “special” behavior, why
create them at all? It turns out that since the new Timer2 infrastructure was a
rewrite of the legacy timer logic that’s been there since the start of the
kernel’s life, it includes a few other benefits regardless of any special
functionality:
■    It uses self-balancing red-black binary trees instead of the linked lists
that form the timer table.
■    It allows drivers to specify an enable and disable callback without
worrying about manually creating DPCs.
■    It includes new, clean, ETW tracing entries for each operation, aiding
in troubleshooting.
■    It provides additional security-in-depth through certain pointer
obfuscation techniques and additional assertions, hardening against
data-only exploits and corruption.
Therefore, driver developers that are only targeting Windows 8.1 and later
are highly recommended to use the new enhanced timer infrastructure, even
if they do not require the additional capabilities.
 Note
The documented ExAllocateTimer API does not allow drivers to create
idle-resilient timers. In fact, such an attempt crashes the system. Only
Microsoft inbox drivers can create such timers through the
ExAllocateTimerInternal API. Readers are discouraged from attempting
to use this API because the kernel maintains a static, hard-coded list of
every known legitimate caller, tracked by a unique identifier that must be
provided, and further has knowledge of how many such timers the
component is allowed to create. Any violations result in a system crash
(blue screen of death).
Enhanced timers also have a more complex set of expiration rules than
regular timers because they end up having two possible due times. The first,
called the minimum due time, specifies the earliest system clock time at
which point the timer is allowed to expire. The second, maximum due time, is
the latest system clock time at which the timer should ever expire. Windows
guarantees that the timer will expire somewhere between these two points in
time, either because of a regular clock tick every interval (such as 15 ms), or
because of an ad-hoc check for timer expiration (such as the one that the idle
thread does upon waking up from an interrupt). This interval is computed by
taking the expected expiration time passed in by the developer and adjusting
for the possible “no wake tolerance” that was passed in. If unlimited wake
tolerance was specified, then the timer does not have a maximum due time.
As such, a Timer2 object lives in potentially up to two red-black tree nodes
—node 0, for the minimum due time checks, and node 1, for the maximum
due time checks. No-wake and high-resolution timers live in node 0, while
finite and idle-resilient timers live in node 1.
Since we mentioned that some of these attributes can be combined, how
does this fit in with the two nodes? Instead of a single red-black tree, the
system obviously needs to have more, which are called collections (see the
public KTIMER2_COLLECTION_INDEX data structure), one for each type
of enhanced timer we’ve seen. Then, a timer can be inserted into node 0 or
node 1, or both, or neither, depending on the rules and combinations shown
in Table 8-11.
Table 8-11 Timer types and node collection indices
Timer type
Node 0 
collection index
Node 1 collection index
No-wake
NoWake, if it has 
NoWake, if it has a non-
a tolerance
unlimited or no tolerance
Finite
Never inserted in 
this node
Finite
High-resolution
Hr, always
Finite, if it has a non-
unlimited or no tolerance
Idle-resilient
NoWake, if it has 
a tolerance
Ir, if it has a non-unlimited or 
no tolerance
High-resolution & 
Idle-resilient
Hr, always
Ir, if it has a non-unlimited or 
no tolerance
Think of node 1 as the one that mirrors the default legacy timer behavior—
every clock tick, check if a timer is due to expire. Therefore, a timer is
guaranteed to expire as long as it’s in at least node 1, which implies that its
minimum due time is the same as its maximum due time. If it has unlimited
tolerance; however, it won’t be in node 1 because, technically, the timer
could never expire if the CPU remains sleeping forever.
High-resolution timers are the opposite; they are checked exactly at the
“right” time they’re supposed to expire and never earlier, so node 0 is used
for them. However, if their precise expiration time is “too early” for the
check in node 0, they might be in node 1 as well, at which point they are
treated like a regular (finite) timer (that is, they expire a little bit later than
expected). This can also happen if the caller provided a tolerance, the system
is idle, and there is an opportunity to coalesce the timer.
Similarly, an idle-resilient timer, if the system isn’t in the resiliency phase,
lives in the NoWake collection if it’s not also high resolution (the default
enhanced timer state) or lives in the Hr collection otherwise. However, on the
clock tick, which checks node 1, it must be in the special Ir collection to
recognize that the timer needs to execute even though the system is in deep
sleep.
Although it may seem confusing at first, this state combination allows all
legal combinations of timers to behave correctly when checked either at the
system clock tick (node 1—enforcing a maximum due time) or at the next
closest due time computation (node 0—enforcing a minimum due time).
As each timer is inserted into the appropriate collection
(KTIMER2_COLLECTION) and associated red-black tree node(s), the
collection’s next due time is updated to be the earliest due time of any timer
in the collection, whereas a global variable (KiNextTimer2Due) reflects the
earliest due time of any timer in any collection.
EXPERIMENT: Listing enhanced system timers
You also can use the same kernel debugger shown earlier to display
enhanced timers (Timer2’s), which are shown at the bottom of the
output:
Click here to view code image
KTIMER2s:
Address,           Due time,                            Exp. 
Type   Callback, Attributes,
ffffa4840f6070b0   1825b8f1f4 [11/30/2020 20:50:16.089] 
(Interrupt) [None] NWF (1826ea1ef4
[11/30/2020 20:50:18.089])
ffffa483ff903e48   1825c45674 [11/30/2020 20:50:16.164] 
(Interrupt) [None] NW P (27ef6380)