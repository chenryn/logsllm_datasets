# Characterizing Delays in Norwegian 3G Networks

**Authors:**
- Ahmed Elmokashfi
- Amund Kvalbein
- Jie Xiang
- Kristian R. Evensen

## Abstract
This paper presents a comprehensive analysis of long-term delay measurements from data connections in three Norwegian 3G networks. We conducted active measurements over more than six months from 90 voting locations used in a trial with electronic voting during the fall's regional elections. Our monitoring infrastructure is geographically distributed across Norway, providing an unprecedented view of the performance and stability of the entire 3G infrastructure. In this study, we focus on delay characteristics, revealing significant differences between different monitors and operators. Our findings suggest that operator-specific network design and configurations are the primary factors influencing delays.

## 1. Introduction
The advent of mobile devices such as smartphones and tablets, combined with the almost universal coverage of 3G networks, has transformed how we access, share, and process information. A stable and resilient 3G network connection has become essential for the daily operations of individuals and organizations. However, there is limited knowledge about the long-term stability and performance of 3G data networks beyond the coverage maps provided by network operators. This lack of information hinders the comparison and evaluation of the quality of services offered.

This paper presents an initial analysis of long-term measurements of mobile broadband (MBB) data connections from three different network operators in Norway, with a focus on delay characteristics. The measurements were conducted over a period of more than six months from 90 locations in 10 municipalities spread across Norway. These measurements were collected in conjunction with a trial of electronic voting during the Norwegian regional elections in fall 2011, ensuring that all monitors were placed in voting locations. The number of voting locations in each municipality varies between 4 and 15, reflecting the habitation patterns and population density of the participating municipalities.

Our measurements have several unique features:
- They are taken from a large number of geographically diverse measurement points, providing a representative view of the quality of MBB data connections experienced by customers across Norway.
- They are measured over a long period of more than six months, capturing both short-term and long-term variations in performance.
- They are performed simultaneously in three different cellular networks, allowing for direct comparison and correlation of the performance of different networks.

In this paper, we present the measurement setup and use the data to analyze an important performance metric: delay. Specifically, we focus on Round-Trip Time (RTT) measurements using ping. We characterize delay along several dimensions and compare the delays experienced in different networks and at different locations. Our findings indicate significant differences between operators in terms of both absolute delays and variations, with each operator having its own "signature" in the delay characteristics. Interestingly, the delay characteristics are primarily network-dependent rather than monitor-dependent, highlighting the key role of network design decisions in determining delay characteristics.

## 2. Measurement Setup and Data
We built a measurement infrastructure consisting of 90 measurement hosts in 10 municipalities across Norway, as shown in Figure 1a. Each measurement node is hosted in separate locations within each municipality, with an average distance of 7.7 km between two monitors in a municipality. The infrastructure also includes two servers: one located in the middle of Norway (Brønnøysund) and the other in the southeast (Fornebu).

Each measurement node is a Dell Latitude E6510 laptop running Ubuntu 10.04. As shown in Figure 1b, each node is multi-homed to four ISPs, three of which are MBB providers. The fourth operator is the available fixed broadband provider at the location, which can vary in quality from high-speed fiber connections to no connection at all. In this paper, we use fixed broadband measurements as a reference point for comparing the performance of the MBB providers. Operators 1 and 2 offer High Speed Packet Access (HSPA) based data services, an evolution of Wide-band Code Division Multiple Access (WCDMA). In locations where HSPA service is not available, the connection reverts to EDGE/GPRS. Operator 3 offers a CDMA2000 1xEV-DO (Evolution-Data Optimized) based data service. We refer to these operators as HSP A1, HSP A2, and EV-DO, respectively. The measurement nodes connect to these 3G operators through the following devices: a Dell built-in wireless 5540 HSPA mobile broadband mini-card (HSP A1), a ZTE MF636 USB modem (HSP A2), and a Huawei EC506 wireless router (EV-DO).

Each node periodically runs ping and traceroute measurements through each of its four interfaces to the two servers. Ping measurements are performed every second through the fixed connection and every five seconds through the wireless networks. Traceroute measurements are performed every 10 minutes. We use a modified version of Paris traceroute, adding support for specifying which interface to use for each run. Additionally, we use AT commands every minute to measure the received signal strength. Our measurements cover the period from February to August 2011, but we often use a subset of the data collected, as long as it does not influence the results. Most of our analysis is based on data collected during July 2011.

The scale and complexity of our infrastructure pose several challenges regarding management and operation. To minimize administrative overhead, we designed our monitors to be as self-administered as possible. Each host maintains a reverse SSH session with our Fornebu server for uploading measurement data and for remote management when needed. Each node stores measurement data locally and uploads it every day at around 3 AM. A monitor periodically checks the status of the SSH session and all four network interfaces and automatically tries to restore any failing session or interface. IT personnel at remote municipalities provide on-site intervention on a voluntary basis, leading to potential long response times when a node is permanently down. Another challenge is the instability of HSP A2’s 3G USB modems, which frequently require physical removal and re-plugging. Due to these challenges, we use measurements from approximately 60 out of 90 hosts in this study, with only 17 HSP A2 monitors.

## 3. Delay Characteristics of Norwegian 3G Networks
In this study, we use IP-layer tools such as ping and traceroute to measure the end-to-end delay between the measurement nodes and our servers. This approach allows us to compare the RTT in the first IP-hop to that of the end-to-end path. The first IP hop in 3G networks is typically the Gateway GPRS Service Node (GGSN).

### 3.1. Delay Differences Between Operators
There are significant differences in delay between operators. The left panel in Figure 2 illustrates a typical Cumulative Distribution Function (CDF) of RTTs measured at one of our monitoring points during July 2011. All MBB networks exhibit roughly an order of magnitude higher delay than the fixed network. The delay varies significantly between networks; HSP A1’s delay is higher than that of EV-DO and HSP A2, ranging from 200ms to 600ms.

The right plot in Figure 2 shows the 5th percentile, median, and 95th percentile of RTTs measured in July 2011 between each monitor and the Fornebu server. This figure highlights large and consistent differences in delay between operators. HSP A1 shows the highest delay (median RTT ∼ 300ms across all monitors), followed by EV-DO (median RTT ∼ 180ms), and then HSP A2 (median RTT ∼ 104ms). Note that, as explained in Section 2, we have fewer HSP A2 monitors. The fixed line RTTs are significantly smaller (median RTT ∼ 16ms) than all MBB operators.

### 3.2. Variations in Delay
We also observe large variations in delay between monitors within the same operator and even within a single connection. HSP A1’s RTTs in a single connection show large variations, sometimes reaching up to two orders of magnitude. In some cases, the round trip delay can reach several seconds, even tens of seconds. Across monitors, EV-DO’s RTTs are more stable than those of HSP A1 and HSP A2. The median RTT for EV-DO varies between 162ms and 297ms across monitors, while for HSP A1, it varies between 82.5ms and 1691ms, and for HSP A2, it varies between 71.2ms and 740ms.

### 3.3. Impact of Modems
The observed differences cannot be solely attributed to the choice of modems. As described in Section 2, we use different modems to connect to the different operators. To investigate whether the choice of modem can explain the observed differences, we conducted controlled experiments with different modems for each operator. Table 1 shows the median delay recorded over a 24-hour period using different modems. The measurements for each operator were taken in parallel during the same 24-hour period. All modems are USB sticks, except the internal modem and the Huawei EC506 (a standalone wireless router). The values marked with a star represent the modem used in the long-term measurements. We observe that the choice of modem has a significant influence on delay, but it is far from the dominant factor. We plan to conduct more systematic evaluations of the role of the modem in future studies.

| **Table 1. Comparing Modems** |
|---|
| **Operator** | **Internal** | **ZTE MF636** | **Huawei E1752** | **Huawei EC506** | **C-motech D50** |
| **HSP A1** | 282 ms* | - | - | - | - |
| **HSP A2** | - | 57 ms | - | - | - |
| **EV-DO** | - | 368 ms | 64 ms | 72 ms* | 164 ms* |
| **C-motech D50** | - | - | - | - | 81 ms |

### 3.4. Statistical Analysis of Delay Distributions
While there are sometimes large differences between monitors of the same operator, they mainly belong to the same population. To check whether these differences are inherent in MBB networks or just reflect local effects near an affected monitor (e.g., poor wireless coverage), we investigated the differences between delay distributions of monitors belonging to the same operator.

To compare two different delay samples and determine whether they belong to the same population, we used the Kullback-Leibler (K-L) divergence test. The K-L divergence is a measure of the closeness between two samples P and Q in terms of the extra information bits required to encode a message based on P instead of Q. We constructed a hypothesis test inspired by the approach used in [10]. Our null hypothesis (H0) is that the tested samples have identical underlying distributions.

For each interface and monitor, we drew 30 random, mutually exclusive samples of equal sizes from RTT measurements in July 2011. We then calculated the K-L divergence for each pair, resulting in 870 values. These values were used to estimate the empirical CDF of K-L divergence. The left panel in Figure 3 presents an example of such a CDF estimated for one of our EV-DO monitoring interfaces (X). If we want to compare the distribution of RTTs measured at another EV-DO interface (Y) to that of X, we first measure the K-L divergence between Y and X. Assuming K-L(Y, X) = 2, we use the empirical CDF of K-L divergence values at X to find F(K-L distance = 2). This value represents the probability that such divergence can occur between two samples drawn from the RTT population of interface X. The P-value of our test is then calculated as 1 − F(K-L distance = 2). We accept H0 if P-value > 0.05, meaning the probability that such divergence occurs between two samples from the same population is at least 5%. In our example, the P-value is 0.46, so we accept H0.

Using our constructed hypothesis test, we compared all pairs of distributions from the same operator. The right plot in Figure 3 shows the CDF of the calculated P-values. We observe that a large fraction of pairs in all interfaces is characterized by a P-value larger than 0.05, indicating that the majority of RTT distributions come from the same population. Our results show that at least 75% of all monitor pairs from the same operator belong to the same population. We also compared RTT distributions across operators and found that a significant fraction of pairs do not belong to the same population. For example, only 49% of all pairs are from the same population when comparing EV-DO to HSP A1, which aligns with our observations.

## References
- [1] N. Taft and F. Ricciato (Eds.): PAM 2012, LNCS 7192, pp. 136–146, 2012. c Springer-Verlag Berlin Heidelberg 2012.
- [2] Paris traceroute.
- [5] Kolmogorov-Smirnov test.
- [7] Kullback-Leibler divergence.
- [10] Hypothesis testing approach.

---

**Figure 1. Measurement Setup**
- (a) Geographical distribution of measurement infrastructure.
- (b) Schematic of a measurement node.

**Figure 2. Example of RTT CDF and Statistics**
- (Left) Typical CDF of RTTs.
- (Right) 5th percentile, median, and 95th percentile of RTTs.

**Figure 3. K-L Distance and P-Value Distributions**
- (Left) K-L distance distribution.
- (Right) P-value distribution.