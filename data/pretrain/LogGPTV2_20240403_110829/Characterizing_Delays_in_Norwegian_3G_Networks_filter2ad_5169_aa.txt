title:Characterizing Delays in Norwegian 3G Networks
author:Ahmed Elmokashfi and
Amund Kvalbein and
Jie Xiang and
Kristian Evensen
Characterizing Delays in Norwegian
3G Networks
Ahmed Elmokashﬁ, Amund Kvalbein, Jie Xiang, and Kristian R. Evensen
Simula Research Laboratory
Abstract. This paper presents a ﬁrst look at long-term delay measure-
ments from data connections in 3 Norwegian 3G Networks. We have
performed active measurements for more than 6 months from 90 voting
locations used in a trial with electronic voting during this fall’s regional
elections. Our monitors are geographically spread across all of Norway,
and give an unprecedented view of the performance and stability of the
total 3G infrastructure of a country. In this paper, we focus on delay
characteristics. We ﬁnd large diﬀerences in delay between diﬀerent mon-
itors. More interestingly, we observe that the delay characteristics of the
diﬀerent operators are very diﬀerent, pointing to operator-speciﬁc net-
work design and conﬁgurations as the most important factor for delays.
1
Introduction
We are witnessing a revolution in the way people access and use the Internet. The
advent of mobile devices such as smartphones and tablets, combined with the
almost universal coverage of 3G networks, has radically changed how we access,
share and process information. A stable and resilient 3G network connection
has become a necessity for the daily operations of individuals and organizations.
Yet, we have little knowledge of the long-term stability and performance of 3G
data networks, beyond the coverage maps provided by network operators. This
gives a very limited basis for comparing and evaluating the quality of the oﬀered
services. To alleviate this, there is a need for long-term measurements of the
stability, availability and quality experienced by users in each network.
This paper presents a ﬁrst look at long-term measurements of mobile broad-
band (MBB) data connections from 3 diﬀerent network operators in Norway,
with an emphasis on delay characteristics. The measurements are carried out
over a period of more than 6 months from 90 locations in 10 municipalities spread
across Norway. The measurements are performed using ping and traceroute from
our monitor nodes to servers placed at two diﬀerent locations. These measure-
ments were collected in connection with a trial of electronic voting during the
Norwegian regional elections in fall 2011. Hence, all monitors are placed in voting
locations. The number of voting locations in each municipality varies between
4 and 15. Voting locations are geographically spread according to habitation
patterns in the participating municipalities, which vary in size and population
density.
Our measurements have a unique combination of features:
N. Taft and F. Ricciato (Eds.): PAM 2012, LNCS 7192, pp. 136–146, 2012.
c(cid:2) Springer-Verlag Berlin Heidelberg 2012
Characterizing Delays in Norwegian 3G Networks
137
– They are taken from a large number of geographically diverse measurement
points, giving a representative view of the quality of MBB data connections
experienced by customers across Norway.
– They are measured over a long period of over 6 months, giving a good basis
for capturing both short-term and long-term variations in the experienced
performance.
– They are performed simultaneously in 3 diﬀerent cellular networks, giving
a unique possibility to directly compare and correlate the performance of
diﬀerent networks.
In this paper, we present the measurement setup, and use the data to take a ﬁrst
look at an important performance metric: delay. More speciﬁcally, we focus on
RTTs measured by ping. We characterize delay along several axis, and compare
the delays experienced in diﬀerent networks and at diﬀerent locations. We ﬁnd
that there are large diﬀerences between operators with respect to both absolute
delays and variations, and that each operator has its own ”signature” in the
delay characteristics. Interestingly, we also ﬁnd that the delay characteristics
are mainly network-dependent rather than monitor-dependent, indicating the
key role played by network design decisions in deciding delay characteristics.
2 Measurement Setup and Data
We have built a measurement infrastructure consisting of 90 measurement hosts
in 10 municipalities across Norway as shown in Fig. 1a. Our measurement nodes
are hosted in separate locations within each municipality; the average distance
between two monitors in a municipality is 7.7 km. The infrastructure also in-
cludes two servers, one is located in the middle of Norway (Brønnøysund) and
the other one is located in the south east of Norway (Fornebu)1.
Our measurement node is a Dell Latitude E6510 laptop running Ubuntu 10.04.
As shown in Fig. 1b, each node is multi-homed to four ISPs, three of them are
MBB providers. The fourth operator is which ever ﬁxed broadband provider that
is available on-site. This connection will have varying quality, from high-speed
ﬁber connection in some locations to nothing at all in other. In this paper, we
use ﬁxed broadband measurements as a reference point for comparing the perfor-
mance of the MBB providers. Operators 1 and 2 oﬀer a High Speed Packet Access
(HSPA) based data service, an evolution of Wide-band Code Division Multiple
Access (WCDMA). In locations where the HSPA service is not available, the con-
nection reverts to EDGE/GPRS. In the following, we refer to these operators
as HSP A1 and HSP A2. Operator 3 oﬀers a CDMA2000 1xEV-DO (Evolution-
Data Optimized) based data service, we refer to this operator as EV -DO. Our
measurement node connects to these 3G operators through the following devices.
Dell built-in wireless 5540 HSPA mobile broadband mini-card (HSP A1), ZTE
MF636 USB modem (HSP A2), and Huawei EC506 wireless router (EV -DO).
1 For more information about our measurement setup please refer to
http://nevada.simula.no/
138
A. Elmokashﬁ et al.
(a) Measurement infrastructure
(b) A measurement node
Fig. 1. Measurement setup
We discuss the impact of the diﬀerent modems on the measured delays in the
next section.
Each node periodically runs ping and traceroute measurements through each
of its four interfaces to the two servers indicated above. Ping measurements
are performed every second through the ﬁxed connection and every 5 seconds
through the wireless networks. Traceroute measurements are performed every 10
minutes. We use a modiﬁed version of Paris traceroute [2], where we have added
support for specifying which interface to use for each run. We also use AT com-
mands every minute to measure the received signal strength. Our measurements
cover the period from February to August 2011, but in this paper we often use
only a subset of the data collected as long as this does not inﬂuence the results.
Most of our analysis is based on data collected during July 2011.
The scale and complexity of our infrastructure poses several challenges re-
garding its management and operation. To minimize the administration over-
head (e.g. traveling to remote sites), we have designed our monitors to be as
self-administered as possible. Each host maintains a reverse SSH session with
our Fornebu server, to be used by the host for uploading its measurement data,
and by the server for pushing new conﬁgurations and remote management when
needed. Further, each node stores measurement data locally and uploads it every
day to the server at around 3 AM. A monitor periodically checks the status of
the SSH session and all four network interfaces and automatically tries to re-
store any failing session or interface. IT personnel at remote municipalities help
when on-site intervention is needed on a voluntary basis. Thus, long response
times are expected when a node is permanently down. Another challenge that
we have faced is the instability of HSP A2’s 3G USB modems; the majority of
Characterizing Delays in Norwegian 3G Networks
139
them require frequent physical removal and re-plugging. Due to these challenges
we use measurements from around 60 hosts out of 90 in this study, and only 17
HSP A2 monitors.
3 Delay Characteristics of Norwegian 3G Networks
In this study, we use the IP-layer tools ping and traceroute to measure the end-
to-end delay between the measurement nodes and our servers. This means that
we are not able to dissect the contribution of the diﬀerent components in the 3G
access networks (such as the base station and the Radio Network Controller) to
the total delay. Using traceroute, we are still able to compare the RTT in the
ﬁrst IP-hop to that of the end-to-end path. The ﬁrst IP hop in 3G networks will
typically be the Gateway GPRS Service Node (GGSN).
In this section, we present our ﬁndings regarding delay characteristics in the
measured MBB networks.
There are large diﬀerences in delay between operators. The left panel in
Fig. 2 illustrates a typical CDF of RTTs measured at one of our monitoring points
during July 2011. All MBB networks exhibit roughly an order of magnitude
higher delay than the ﬁxed network. Delay varies signiﬁcantly between networks;
we note that HSP A1’s delay is higher than that of EV -DO and HSP A2, and
varies in a wider range between 200ms and 600ms.
The right plot in in Fig. 2 shows the 5th percentile, median, and 95th per-
centile of RTTs measured in July 2011 between each monitor and the Fornebu
server2. This ﬁgure shows that there are large and consistent diﬀerences in delay
between operators. HSP A1 shows the highest delay (median RTT ∼ 300ms
across all monitors). Then follows EV -DO (median RTT ∼ 180ms), before
HSP A2 (median RTT ∼ 104ms). Note that, as explained in Sec. 2 we have
fewer monitors of type HSP A2. The ﬁxed line RTTs are signiﬁcantly smaller
(median RTT ∼ 16ms) than all MBB operators.
We also record large variations between monitors in the same operator and
even within a single connection. HSP A1’s RTTs in a single connection shows
large variations reaching up to two orders of magnitude. In some cases, the round
trip delay can reach several seconds, even tens of seconds. Across monitors, EV -
DO’s RTTs are more stable than those of HSP A1 and HSP A2. It’s median
RTT varies between 162ms and 297ms across monitors. The same metric varies
between 82.5ms and 1691ms in HSP A1; and between 71.2ms and 740ms in
HSP A2.
The observed diﬀerences cannot be explained by diﬀerent modems
alone. As described in Sec. 2, we use diﬀerent modems to connect to the dif-
ferent operators. It is therefore natural to ask whether the choice of modem can
explain the observed diﬀerences. To investigate this, we have run controlled ex-
periments with diﬀerent modems for each operator. Table 1 shows the median
2 Measurements to the other server show similar results.
140
A. Elmokashﬁ et al.
F
D
C
 1
 0.8
 0.6
 0.4
 0.2
 0
)
s
m
(
T
T
R
HSPA1
EV-DO
HSPA2
Fixed
 0  200  400  600  800  1000
RTT(ms)
 1e+06
 100000
 10000
 1000
 100
 10
 1
HSPA1
EV-DO
HSPA2
Fixed
 0  10  20  30  40  50  60
Monitor ID
Fig. 2. Example of a typical RTT CDF (left), RTTs statistics (right)
delay recorded over a 24 hour period using diﬀerent modems3. The measure-
ments for each operator are taken in parallel during the same 24 hour period.
All modems are USB sticks, except the internal modem and Huawei EC506
(which is a standalone wireless router). The values marked with a star represent
the modem that was used in the long-term measurements. We observe that the
choice of modem has a marked inﬂuence on delay, but that it is far from the
dominant factor. We plan to do more systematic evaluations of the role of the
modem in future studies.
Table 1. Comparing Modems
Operator Internal ZTE MF636 Huawei E1752 Huawei EC506 C-motech D50
HSP A1 282 ms*
HSP A2
57 ms
EV -DO
368 ms
64 ms
72 ms*
164 ms*
81 ms
While there are sometimes large diﬀerences between monitors of the
same operator, they mainly belong to the same population. Our previ-
ous observations sometimes show large variations in delay between monitors of
the same operator, thus it is interesting to check whether these diﬀerences are
inherent in MBB networks or just reﬂect local eﬀects near an aﬀected monitor
(e.g. poor wireless coverage). To answer this we investigate diﬀerences between
delay distributions of monitors that belong to the same operator.
To compare two diﬀerent delay samples as to whether they belong to the same
population, we need to pick an appropriate statistical test that suits our data.
3 Due to the diﬀerent technologies and provider locks, we are not able to test all
modems across all operators.
Characterizing Delays in Norwegian 3G Networks
141
F
D
C
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
F
D
C
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
Fixed
EV-DO
HSPA1
HSPA2
 0.6
 0.8
 1
 0
 0.2
 0.4
P-val
 0
 1
 3
 2
 4
K-L distance
 5
 6
Fig. 3. KL distance distribution (left), Resulting P-values distribution (right)
First, it is reasonable to avoid parametric tests (e.g. t-test), since we cannot
make assumptions about the underlying probability distribution of the RTT
data. One possibility is to apply the two-sample Kolmogorov-Smirnov test [5] for
comparing continuous, one dimensional distributions. But, RTT distributions are
not continuous, thus we decide to employ the Kullback-Leibler (K-L) divergence
test instead [7]. The K-L divergence is a measure for the closeness between
two samples P and Q in terms of extra information bits required to encode a
message based on P instead of Q. Note that the K-L divergence in general is not
symmetric.
K-L divergence by itself cannot determine whether the two tested delay sam-
ples are drawn from the same population at a certain conﬁdence level. Hence,
we construct a hypothesis test that is inspired by the approach used in [10]. In
the following, we present this hypothesis test; our null hypothesis H0 is that the
tested samples have identical underlying distribution.
For each interface and monitor, we draw 30 random mutually exclusive sam-
ples of equal sizes from RTT measurements in July’11. We then calculate the
K-L divergence for each pair, that results in 870 values. These values are then
used to estimate the corresponding empirical CDF of K-L divergence. The left
panel in Fig. 3 presents an example of such CDF estimated for one of our EV -
DO monitoring interfaces, in the following we call this interface X. If we want to
compare the distribution of RTTs measured at another EV -DO interface Y to
that of X, we ﬁrst measure the K-L divergence between Y and X. Let’s assume
that K-L(Y, X) = 2. We use the empirical CDF of K-L divergence values at X
to ﬁnd F (K-L distance = 2). This value represents the probability that such
divergence can occur between two samples drawn from the RTT population of
interface X. The P-value of our test is then calculated as 1 − F (K-L distance
= 2), we accept H0 if P -value > 0.05, i.e. the probability that such divergence
occurs between two samples from the same population is at least 5%. In our
example, the P-value is 0.46 thus we accept H0.
Using our constructed hypothesis test we compare all pairs of distributions
from the same operator. The right plot in Fig. 3 shows the CDF of the calculated
P-values. We observe that a large fraction of pairs in all interfaces is characterized
by a P-value larger than 0.05, meaning that the majority of RTT distributions
142
A. Elmokashﬁ et al.
come from the same population. Our results show that at least 75% of all monitor
pairs from the same operator belong to the same population. We also compare
RTT distributions across operators and ﬁnd that a signiﬁcant fraction of pairs
do not belong to the same population. For example, only 49% of all pairs are of
the same population, when comparing EV -DO to HSP A1. This is in agreement