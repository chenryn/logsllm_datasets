Theorem 1. If the used secret key encryption scheme
SKE is IND-CPA secure, F is a pseudorandom function and
G is a pseudorandom generator, then SUISE as described
in Section 4 is (Lsearch, Ladd, Lencrypt)-secure against adap-
tive dynamic chosen-keyword attacks in the random oracle
model.
Proof. We describe a polynomial time simulator S for
which the advantage of any PPT attacker A to distinguish
between the output of RealSSE
A,S (λ) is negli-
gible. Our simulator adaptively simulates a search index eγ
with the additional information given by the leakage func-
tions.
A (λ) and IdealSSE
1. Setting up the environment:
S creates an empty list eσ as simulated search history,
an empty simulated search index eγ = (fγw, fγf ) consist-
ing of two empty hash tables, and an empty dictionary
ρ to keep track of queries to the random oracle. A key
k3 ← GIND-CPA(1λ) is sampled for simulating encryp-
tion of ﬁles. A chained hash table C is used to keep
track of tuples (j, eτ ) consisting of previously simulated
search indexes and simulated search tokens for each
individual ﬁle. In detail, an entry of C consists of a
linked list, we denote Cfi as the linked list for ﬁle fi,
that is stored at hash table entry C[ID(fi)]. Further-
more, other empty hash tables T and A are created,
where T is needed to keep track of the assignment of
simulated search tokens fτw for word w with ID(w) and
A is needed to keep track of simulated add tokens fαf
for already added ﬁles with ID(f ).
2. Simulating search tokens eτ with given leakage
Lsearch(f, w) = (ACCPt(w), ID(w)),
the simulator checks if ID(w) is in T i.e.
token for this word was queried before.
if a search
• If this is the case, S outputs T [ID(w)].
• Otherwise, a random bit string eτ ← {0, 1}λ is
chosen and stored at T [ID(w)] and added to eσ.
For every ID(fi) ∈ ACCPt(w) the simulator sets
Jfi as the set of all ﬁrst components of Cfi more
formally set Jfi = {jl : (jl, τl) ∈ Cfi with 0 ≤ l ≤
|Cfi |}. Then the simulator chooses a random in-
dex ji ← [1, |fγf [ID(fi)]|] \ Jfi and adds the tuple
(ji, eτ ) to the list Cfi . Finally, S outputs eτ .
3. Simulating add eα tokens with given leakage
Ladd(f, f ) = (ID(f ), len(cid:0)f(cid:1) , SRCH HISt(f )),
S checks if there is an entry at A[ID(f )] i.e. if an add
token for ﬁle f with ID(f ) was simulated before.
• If an add token fαf for ﬁle f with ID(f ) was re-
quested before, the simulator outputs A[ID(f )].
• If, on the other hand, this ﬁle was not added be-
fore, the simulator chooses for every i ∈ [1, len(cid:0)f(cid:1)]
a random bit string esi ← {0, 1}2λ, and sorts this
generated set (es1, . . . ,eslen(f)) in lexicographic or-
der to get es and stores this at fγf [ID(f )]. In ad-
dition, an empty list ex is created and for every
ID(w) ∈ SRCH HISt(f ) the token fτw = T [ID(w)]
is looked up, added to ex and ID(f ) is added to
fγw[fτw]. S creates a temporary set J and for
all l ∈ [1, len (ex)] a random fake index jl ←
[1, |fγf [ID(fi)]|] \ J is sampled and added to J.
This index jl is marked as full in the list of used
search indexes by adding the tuple (jl, eτl) to Cf
in the chained hash table C, where eτl = ex[l]. Fi-
nally, S outputs fαw = (ID(f ),es, ex) and stores
this simulated token at A[ID(f )].
4. Simulating encryption with given leakage
Lencrypt(f ) = len (f ) ,
the simulator outputs ec ← E IND-CPA
K3
(0len(f )).
5. Answering random oracle queries: given query (k, r),
the simulator checks if this query was submitted before
i.e. if there is an entry l = ρ[k||r].
• If this is the case, set l = ρ[k||r].
• Otherweise, S checks if key k is linked with some
if there is an entry k = T [ID(w)]
ID(w) i.e.
for some ID(w).
If there is no used search to-
ken, a random bit string l ← {0, 1}λ is sampled
and ρ[k||r] = l is set to stay consistent for future
queries.
If, on the other hand, k is linked with some ID(w),
for every ID(fi) ∈ fγw[k] the simulator looks up
the tuple (j, k′) ∈ Cfi where k′ = k. Then the
value esj ∈ {0, 1}2λ is set as the j-th entry of
es = fγf [ID(fi)] and divided in two λ-bit strings
l′||r′ = esj .
– S checks if r′ = r, sets l = l′ in this case and
stores l at ρ[k||r].
– If there was no ﬁtting r′ for any ID(fi) ∈
fγw[k], a random l ← {0, 1}λ is sampled and
stored at ρ[k||r] to stay consistent for future
queries.
Finally, l is returned.
The indistinguishability of a simulated search token eτ and
a real search token τ follows from the pseudorandomness of
F . Also, the indistinguishability of a simulated search his-
tory eσ and a real search history σ follows from the pseu-
dorandomness of F . The indistinguishability of a simulated
add token eα, especially of es, ex, and a real add token α fol-
lows from the pseudorandomness of G and F . The indistin-
guishability of a simulated ciphertext ec and real ciphertext
c follows from the IND-CPA security of our used secret key
encryption. Since we choose the output of our simulated
random oracle H either totally random or out of a prede-
ﬁned domain, that itself is generated in a random way, our
random oracle is indistinguishable from a pseudo-random
function.
6. DISCUSSION
6.1 Why maintain a search history at the client?
We maintain a history of previously used search tokens at
the client and use it during the add operation. The client
creates the corresponding search tokens immediately as the
deterministic identiﬁer of the keyword. Hence, the service
provider can include it in the index. Note that the search
token is not necessarily part of the add token, rather it could
be randomized. In order to check whether a randomized add
token corresponds to a search token, the service provider
would need to check all previous search tokens. Only then, it
could convert the randomized add token to the deterministic
search token. To the contrary, the client can compute both
– search and add token of the inserted keyword – and simply
look up the search token in the history. Hence, the cost of
one insertion is O(len(f )) using a history at the client and
O(len(f )|SRCH HISt(f )|) using a history at the service
provider. Our solution using no client storage (except the
key) modiﬁes the Add and AddToken operations as follows:
• αf ← AddToken(K, f, σ): parse K = (k1, k2). For ﬁle
f that consists of a sequence of words create a list f
of unique words f ⊇ f = (w1, . . . , wlen(f)). Generate
a sequence of pseudorandom values s1, . . . slen(f) with
PRNG G. For every word wi ∈ f set ci = Hτwi (si)||si
with τwi = Fk1 (wi). Now sort c = (c1, . . . , clen(f)) in
lexicographic order and set αf = (ID(f ), c). Output
αf .
• (c′, γ ′) ← Add(αf , c, c, γ): parse αf = (ID(f ), c), γ =
(γw, γf ) and set γf [ID(f )] = c. In addition, for each
τwi ∈ γw and each cj ∈ c set cj = lj||rj and check if
Hτwi (rj) = lj. If yes, add ID(f ) to γw[wi]. Update
the ciphertexts c to c′ by adding c. Output c′ and the
updated version γ ′ = (γw, γf ).
The history at the client is the same as the index words
ID(w) in inverted index γw at the service provider. Hence
the client can always restore its history by downloading these
from the server. Moreover, let the number of unique key-
words be n, then the size of the history will never exceed
O(n) independent of the number of searches performed.
6.2 How to hide the number of unique key-
words per ﬁle?
Our leakage deﬁnition for the add operation still includes
the identiﬁer of the ﬁle (ID(f )) and the number of unique
keywords in that ﬁle (len(f )). One can hide both by adding
a level of indirection. First encrypt each ﬁle f , resulting
in the identiﬁer ID(f ). Then for each wi ∈ f encrypt ﬁle
f ′ = {ID(f )} resulting in unique identiﬁer ID(f ′). One can
now create the add token for ID(f ′) and wi. A simulator
for the add token operation is simple to derive from our
simulator: ID(f ′) – which is unique in the system – replaces
ID(f ) and is leaked instead, but len(f ′) = 1 and can hence
be omitted.
6.3 Search Time Analysis
In our algorithm the ﬁrst search for a keyword requires a
linear scan, but subsequent searches are (almost) constant.
Hence, the initial overhead amortizes and we reach asymp-
totically optimal search time for long-running systems. In
this section we analyse the required number of searches until
we reach this optimum.
Let n be the number of unique keywords stored in the ci-
phertexts; let m be the total number of stored keywords in
the ciphertext. Once we have created an index entry for a
keyword, our search complexity is m/n: We have a constant
lookup in the hash table and return m/n ciphertexts on av-
erage. An initial search can take up to m search (lookup)
operations and there can be at most n of those. Hence, the
initial eﬀort is (upper) bounded by mn. We are interested in
the number N of searches such that the amortized cost be-
comes optimal. Since we need to return at least m/n entries,
this is the lower bound optimum. The cost is asymptotically
optimal, if there exists a constant c, such that the cost is at
most the optimum times c. The amortized cost is the cost
for inital searches (mn) divided by the number of searches
plus the cost for one subsequent search:
mn
N
+
m
n
≤ c
m
n
We conclude from this formula that we need at least N ≥
n2 searches until our cost is asymptotically optimal. The
constant c = 2 is low and we need at most 0.5 cryptographic
hash operation on the server on average. Read (search) re-
quests dominate many systems like databases, such that this
number can be quickly reached in practice.
7. PERFORMANCE RESULTS
The following experiments were implemented in Java 7.
Either operations performed by the server, or operations
performed by the client, were executed on an Intel Xeon
1230v3 CPU 3.30GHz with 8GB RAM running Windows 8.
To minimize I/O access time all ﬁles used in our simulations
are loaded into main memory before starting measurements.
For the implementation of our keyed random function
F and our random oracle H we use the implementation
of HMAC-SHA-1 that is contained in the default Java li-
brary. Also contained in the default Java library, we use
SHA1PRNG as random number generator G that outputs a
PRN with length of 160 bits.
7.1 On the client’s side
One main argument for outsourcing data is the use of
slow and weak hardware on client’s side. To show our SUISE
scheme feasible for this scenario, we simulated the creation of
add tokens and search tokens for 250, 000 random words per
run. We repeated each creation run 100 times and present
the mean value of these 100 runs for creating one token. We
simulated both versions for storing the search history that
is storing it on the client or storing it on the server.
The cost for creating search tokens depends on the cost
of generating one HMAC-SHA-1 mainly. Creating an add
token without checking the search history (so let the server
check for indexed words used in previous search queries)
needs two HMAC operations and one random number. If the
client has to check the search history, the cost for add token
generation also depends on the size of search history. For our
simulation we ﬁlled the search history with 0, 100, 000 and
1, 000, 000 unique words. By using Java’s HashSet as search
history, the lookup time can be minimized. Remember that
the search history contains unique search words used before
and stays quite small (see Figure 3) in relation to the number
of search queries.
We omitted time measurements for encryption and de-
cryption operations, since encrypting and decrypting ﬁles
with secret key encryption in an IND-CPA secure way is a
well studied problem. Furthermore, special clients may have
highly specialized hardware for performing a special kind of
secret key encryption scheme.
7.2 On the server’s side
All our simulations ran single threaded, but can easily
be executed in parallel. By dividing the search index γf
operation
SearchToken
AddToken
AddToken
AddToken
|σ|
-
0
105
106
time [µs]
1.14
4.77
5.06
5.47
Figure 2: Average duration for creating one token for one
word.
into subsets and search these subsets on diﬀerent cores it
is possible to speed up searches for search tokens that were
not searched before. In our test scenario all operations ran
on one machine so we were able to ignore latency through
network transfers that may occur in practice application.
We omit benchmarks for Add and Delete since the run-
time of these tasks depend on the chosen methods for cre-
ating indexes and updating these indexes. In addition, one
can interpret these operations as storing, accessing, adding
and deleting plaintext in an eﬃcient way, because no cryp-
tographic primitives are used there. Either cryptographic
primitives are used on the client before and benchmarked
there (e.g. creating add tokens) or are not needed at all.
So, the runtime of Search is discussed in the following.
For our experiments we added 50 ebooks downloaded from
project Gutenberg [1]. Before adding these ﬁles, all words
were transformed to lower case and punctuation was re-
moved. Our complete ﬁleset f consisted of 3, 654, 417 words
separated by whitespaces after this transformation. Remov-
ing words that appear multiple times in one ﬁle resulted in
a ﬁlest containing 337, 724 words so our index γf had size
337, 724. Altogether 95, 465 words were indexed i.e. 95, 465
diﬀerent search tokens would result in at least one posi-
tive match. To simulate realistic search queries we use a
list of word frequencies from [2] which represent real world
search queries but omitted the ﬁrst 100 entries that mainly
contained pronouns and prepositions. This word frequency
list contains about 400, 000 words and our search words are
chosen in a random fashion weighted according to their fre-
quency.
In order to benchmark search operations Search at the
service provider we generate 5000 random search tokens
using the probability distribution explained above. The
mean search time for these 5000 search tokens results in one
measurement point. A complete benchmark run consists of
75, 000 search queries, i.e. 15 values are measured per run.
In total, we repeated these benchmark runs 10 times and
plotted the average and the error bars provide the standard
derivation of these 10 runs. The average time for an initial,
linear search was 414.38 ms. The average time for a second,
constant time search was 0.01 ms.
Figure 3 shows the size of the search history over the time
of the experiment.
It also depicts the decreasing number
of newly generated search tokens that were not contained
in the search history before. Denoted by the white part,
every bar represents the size of search history σ before the
5000 search queries. The grey part represents the amount
of queried search tokens that were not known before these
5000 search queries. So, combining the white and the grey
part shows the size of search history σ after executing these
5000 queries. Figure 3 demonstrates the decreasing amount
of newly generated search tokens with increasing amount of
9 5 4 6 5
...
1 5 0 0 0
1 0 0 0 0
5 0 0 0
]
s
d
r
o
w
[
y
r