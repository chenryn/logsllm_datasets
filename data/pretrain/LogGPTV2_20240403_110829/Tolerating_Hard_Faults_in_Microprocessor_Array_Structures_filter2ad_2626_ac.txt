### Fault Tolerance and SRAS Limitations

SRAS (Self-Repairing Array Structures) can tolerate faults in its own logic, such as pointer remapping logic or the fault map. These structures are significantly smaller than the structures they protect, which makes them less prone to hard faults, though they can still fail. However, SRAS has certain limitations:

1. **Sub-Array Faults**: SRAS cannot tolerate a fault in a table sub-array if no spare rows are available in that sub-array. This limitation does not apply to buffers unless every row, including spares, is faulty.
2. **Check Row Faults**: SRAS cannot tolerate a fault in a sub-array (for a buffer or table) if all of the check rows for that sub-array are faulty.

These untolerated faults present a classic engineering trade-off: fault tolerance versus hardware cost. Future SRAS implementations could develop hardened logic if the first fault model is considered important. The probabilities of the latter two categories can be decreased by designing SRAS protection to use more spare rows and more check rows.

### Evaluation

#### 5. Evaluation

In this section, we quantitatively evaluate SRAS. We compare our approach to unmodified DIVA (i.e., DIVA without SRAS extensions for hard faults) to determine the relative performance in both fault-free and faulty scenarios. We begin by describing our methodology, followed by presenting our experimental results and discussing their broader applicability.

#### 5.1 System Model and Methodology

We use the SimpleScalar toolset [2] to evaluate our design and compare it to unmodified DIVA. We model a dynamically scheduled microprocessor similar to currently available microprocessors, such as the Intel Pentium4 [9] and Alpha 21364 [8]. The details of the target system are shown in Table 1. We simulate DIVA fault tolerance by comparing each instructionâ€™s result to the fault-free result and, if they do not match, triggering a pipeline squash in the aggressive processor core. We simulate the SPEC2000 CPU benchmarks and use the SimPoint toolset [20] to select statistically representative samples of these long benchmarks for detailed simulation. We inject single-bit stuck-at-1 and all-bits (in a single row) stuck-at-1 faults into the simulated systems, varying the number of injected faults and examining their impact on system performance.

**Table 1: Target System Parameters**
| Parameter              | Value                                      |
|------------------------|--------------------------------------------|
| Pipeline depth         | 22                                         |
| Pipeline width         | 3                                          |
| Reorder buffer         | 126                                        |
| Functional units       | 4 integer adders and multiplier, 1 FP adder, 1 FP multiplier |
| Branch predictor       | gshare: BHT is 4096 entries, BHT entry is 2-bit counter, BHR is 8 bits |
| Registers              | 192                                        |
| L1 D-cache             | 8K total size, 4-way, 2-cycle              |
| L1 I-cache             | 8K total size, 4-way, 2-cycle              |
| L2 cache               | 256K size, 8-way, 7-cycle                  |

#### 5.2 Results

**Reorder Buffer (ROB)**

Figure 4 displays the results of our experiments with injecting faults into a ROB with 126 entries, corresponding to the size of the Pentium4. The figures plot the speedup of SRAS compared to unmodified DIVA as a function of the number of faults injected. For most benchmarks, SRAS achieves a significant speedup over unmodified DIVA. Even for just a single all-bit fault, speedup results range up to 1.4.

The trends in the graphs reveal several interesting phenomena:
1. **Single-Bit vs. All-Bit Faults**: SRAS speedup in the presence of a given number of single-bit faults is always less than the speedup in the presence of the corresponding number of all-bit faults. This is because a single-bit fault is more likely to be logically masked.
2. **Benchmark Variability**: The results vary across benchmarks more than expected. For unmodified DIVA, the number of recoveries per instruction should be fairly constant. However, certain benchmarks (e.g., mcf) achieve smaller speedups. Generally, integer benchmarks achieve smaller speedups than floating-point benchmarks. We discovered a direct correlation between IPC (instructions per cycle) and the magnitude of the SRAS speedup. As IPC decreases, the number of recoveries per cycle decreases, and thus the impact of SRAS (compared to unmodified DIVA) decreases.

**Branch History Table (BHT)**

Figure 5 compares the performance of SRAS and unmodified DIVA in the presence of hard faults in the BHT. We only show the results for all-bit faults, as the results for 1-bit faults are almost identical. We observe that SRAS results in a slowdown compared to unmodified DIVA. There are two reasons for this:
1. **Fault Penalty**: The penalty for faulty BHT rows is small, as each individual row is exercised rarely and is easily masked.
2. **Performance Penalty**: We impose a conservative performance penalty on SRAS by adding an extra pipeline stage for remapping the table. The delay for remapping is only 2-4% of a pipeline stage and may not even be on the critical path. However, even in the likely case that this added pipeline stage is overly conservative, SRAS is probably not worth the effort for the BHT.

With faults injected, SRAS speedups are still small and often less than one. Moreover, speedup results are largely independent of the number of faults and the benchmarks. The performance penalty incurred by SRAS, with respect to unmodified DIVA, depends on the impact of adding the pipeline stage in the front-end of the pipeline (towards fetch). For workloads bottlenecked at the back-end (towards commit), the extra latency in the front-end gets hidden. Thus, even when a branch is mis-predicted and flushes subsequent instructions, the re-fetched instructions can still propagate back into the pipeline before it runs out of instructions to execute.

#### 5.3 Broader Applicability of Results

Experimental results show that adding SRAS protection for the ROB is beneficial, but it is probably not a good idea for the BHT. These results suggest which types of microarchitectural array structures are most likely to benefit from SRAS. The ROB is a heavily-used buffer with a high AVF, similar to the register file, reservation stations, and store buffer. We would expect SRAS to benefit these structures, and future work will explore adding SRAS protection to them. Conversely, the BHT is a sparsely-used table with an AVF of zero, similar to other prediction tables. We would expect SRAS to have minimal impact on these structures, even if the remapping can be performed without degrading performance in the fault-free case. However, if a prediction table is small and each entry is accessed frequently, SRAS might help.

### Conclusions

In this paper, we have developed Self-Repairing Array Structures (SRAS), a hardware technique for masking hard faults in microprocessor array structures. We combine SRAS with DIVA, a cost-effective error correction mechanism that incurs a performance penalty per error. SRAS masks faults by (a) detecting and diagnosing them with dedicated check rows, and (b) using a level of indirection to map out faulty rows. Experimental results show that adding SRAS to heavily-used buffers with a high AVF, such as the reorder buffer, improves performance compared to unmodified DIVA.