Efficient collective communication has well-known solutions in
HPC community and in distributed data-parallel SGD. Many tradi-
tional collective communication libraries exist, including Gloo [14],
Horovord [47], OpenMPI [16], MPICH [31], and NCCL [34]. They
can use efficient data transfer schedule (e.g., ring-allreduce, tree-
broadcast) to mitigate communication bottlenecks in distributed
applications.
There are two application requirements for using traditional
collective communication libraries. First, the communication pat-
tern has to be statically defined before runtime. This is easy for
applications that have a bulk-synchronous parallel model. For ex-
ample, in synchronous data-parallel SGD, all the workers compute
on their partitioned set of training data and synchronize the model
parameters using allreduce. Second, when any worker fails, all the
workers participating in the collective communication hang, and
applications are responsible for fault tolerance. For HPC applica-
tions, this is typically solved by checkpointing the entire application
periodically (e.g., per-hour), and when a process fails, the entire
application rolls back to a checkpoint and re-execute.
Unfortunately, these two assumptions are fundamentally incom-
patible with task-based distributed systems. First, tasks are dynam-
ically invoked by the task-based system‚Äôs scheduler. This means it
(a) Dynamic tasks (Ray)
(b) Dynamic tasks + collective comm. (Ray + Hoplite).
Figure 2: Execution of a distributed RL algorithm. Each row is one agent.
Boxes represent computations, and arrows represent data transfers. ùëî1-ùëî4
are the gradients produced by the agents. (a) Dynamic tasks (Ray). Gradients
are applied immediately. A batch of three gradients is applied to the current
policy before broadcasting. (b) Dynamic tasks but with efficient collective
communication, in Hoplite. To reduce the network bottleneck at agent 2,
agent 3 partially reduces gradients ùëî3 and ùëî4 (black box), and agent 3 sends
the policy to agent 4 (black dot) during the broadcast.
is possible that, when collective communication is triggered, only
a fraction of the participating tasks are scheduled. For example, on
existing task systems, broadcast is implicit: a set of tasks fetch the
same object. When only a subset of the receivers are scheduled,
it is not possible to build a static broadcast tree without knowing
how many total receivers and where and when the receivers will
be scheduled. Therefore, a collective communication layer for a task-
based system should adjust data transfer schedule at runtime based
on task and object arrivals.
Second, fast failure recovery is an important design goal for task-
based system [49, 52], because many asynchronous workloads have
tight SLO requirement (e.g., model serving). In existing task systems,
this is done by reconstructing and re-executing failed tasks only. If
traditional collective communication libraries are used, a failed task
causes the rest of the participating tasks to hang. Thus, a collective
communication layer for task-based systems has to be fault-tolerant:
allowing well-behaving tasks to make progress when a task fails and
allowing the failed task to rejoin the collective communication after
recovery.
3 DESIGN
Hoplite is an efficient and fault-tolerant collective communication
layer for task-based distributed systems. At a high level, Hoplite
uses two techniques: (1) decentralized fault-tolerant coordination of
data transfer for reduce and broadcast, and (2) pipelining of object
transfers both across nodes and between tasks and the object store.
We first present a send-receive example workflow using Hoplite‚Äôs
core API (Table 1). We then describe Hoplite‚Äôs object directory
service, pipelining mechanism to reduce latency, and fault-tolerant
Policyrolloutrolloutrolloutrolloutapplyapplyrolloutapplyrolloutg4rolloutg3a‚Ä¶g11234Policyg4g3+g4‚Ä¶g11234SIGCOMM ‚Äô21, August 23‚Äì27, 2021, Virtual Event, USA
Siyuan Zhuang et al.
object store also maintains information about objects that have only
been partially created to facilitate object transfer pipelining (¬ß3.3).
For example, in Figure 3, the object store on node 1 publishes its
location to the object directory as soon as Put(x) is called, even if
the object hasn‚Äôt been fully copied into the store yet. This allows
node 1 to begin sending the object to node 2 while it is still being
copied from the send worker.
Finally, in step 4, the Hoplite object store nodes execute the data
transfer schedule specified by the object directory‚Äôs reply to node
2. Node 1 is the only location for x, so node 2 requests and receives
a copy from node 1 (step 4). Node 2 then copies the object from its
local store to the recv worker (step 5 in Figure 3), which again can
be pipelined with the copy over the network.
Hoplite provides two efficient collective communication schemes.
Hoplite implements efficient broadcast through coordination be-
tween the object directory service and the workers (¬ß3.4), without
an explicit primitive. For reduce, Hoplite exposes an explicit Reduce
call to the task-based system. It is necessary because this lets Ho-
plite know that these objects are indeed reducible (i.e., the operation
is commutative and associative). Because an ObjectID is a future
that the object value may not be ready yet, the Reduce call also has a
num_objects input in case the user wants to reduce a subset of the
objects, giving Hoplite the flexibility to choose which num_objects
objects to reduce given their arrival time in the future. Figure 1b
shows how to modify the RL example to use Hoplite. This allows
the trainer to aggregate gradients from a dynamic set of agents
efficiently (Figure 2b).
Whenever a task fails, Hoplite recomputes a data transfer sched-
ule to avoid using the failed task in the collective communication,
and all the rest of the tasks can keep making progress (¬ß3.5). Ho-
plite does not change how task-based distributed system tolerate
failures. The underlying task-based distributed system can quickly
reconstruct the state of the failed task using their built-in mech-
anism [52]. Once the state of the task is reconstructed, the task
resumes.
3.2 Object Directory Service
The object directory service maintains two fields for each object: (1)
the size of the object, and (2) the location information. The location
information is a list of node IP addresses and the current progress of
the object on that node. We use a single bit to represent the object‚Äôs
progress: either the node contains a partial or a complete object.
We store both so that partial object copies can immediately act as
senders, for both broadcast and reduce (¬ß3.4).
Hoplite‚Äôs directory service supports both synchronous and asyn-
chronous location queries. Synchronous location queries block until
corresponding objects are created and locations are known. Asyn-
chronous location queries return immediately, and the object di-
rectory service publishes any future locations of the object to the
client.
A node writes object locations to the object directory service
in two conditions: when a local client creates an object via Put
and when an object is copied from a remote node. In each case,
the node notifies the object directory service twice: once when
an object is about to be created in the local store and once when
the complete object is ready. We differentiate between partial and
Figure 3: Example of a send and receive dynamic task program on a 2-node
cluster (N1 and N2). The task-based system consists of a pool of workers
per physical node and a scheduler. Hoplite consists of one local object store
per node and a global object directory service, which is distributed across
physical nodes.
receiver-driven coordination scheme for efficient object transfer in
details.
3.1 Hoplite‚Äôs Workflow
Our example creates a send task that returns x_id (a future), which
is then passed into a recv task. In Hoplite, we use an ObjectID to
represent a future or a reference to an object. During execution, the
application first submits the tasks to the task scheduler. The sched-
uler then chooses a worker to execute each task (step 1, Figure 3),
e.g., based on resource availability. According to the application,
recv cannot start executing until it has the value returned by send.
Note that the task-based system does not require the scheduler to
schedule tasks in a particular location or order, i.e. the recv task
may be scheduled before send.
In step 2, the task workers call into Hoplite to store and retrieve
objects. On node 1, the send worker returns an object with the
unique ID x_id. This object must be stored until the recv worker
has received it. Thus, the send worker calls Put(x) on Hoplite,
which copies the object from the worker into the local object store
(step 2 on N1, Figure 3). This frees the worker to execute another
task, but incurs an additional memory copy between processes to
store objects.
Meanwhile, on node 2, the recv worker must retrieve the object
returned by send. To do this, it calls Get(x) on Hoplite, which
blocks until the requested object has been copied into the worker‚Äôs
local memory (step 2 on N2, Figure 3). In step 3, Hoplite uses the
object directory service to discover object locations and coordinate
data transfer, in order to fulfill the client‚Äôs Put and Get requests. In
the example, the Hoplite object store on node 1 publishes the new
location for the object x to the directory (step 3 on N1, Figure 3).
Meanwhile, on node 2, the Hoplite object store queries the directory
for a location for x (step 3 on N2, Figure 3).
Hoplite‚Äôs object directory service (¬ß3.2) is implemented as a
sharded hash table that is distributed throughout the cluster (Fig-
ure 3). Each shard maps an ObjectID to the current set of node
locations. When there are multiple locations for an object, the direc-
tory service can choose a single location to return to the client. The
defapplication():x_id= send.remote()recv.remote(x_id)Task schedulerObject storeWorker (send)Object storeWorker (recv)XObject directoryObject directory131344Put(x)Get(x)Node 1Node 2XHoplite225Hoplite: Efficient and Fault-Tolerant Collective Communication for Task-Based Distributed Systems
SIGCOMM ‚Äô21, August 23‚Äì27, 2021, Virtual Event, USA
Core Interfaces:
Buffer buffer‚Üê Get(ObjectID object_id)
Put(ObjectID object_id, Buffer buffer)
Delete(ObjectID object_id)
Description
Get an object buffer from an object id.
Create an object with a given object id and an object buffer.
Delete all copies of an object with a given object id.
Called by the task framework once an object is no longer in use.
Reduce(ObjectID target_object_id, int num_objects, Create a new object with a given object id from a set of objects
{ObjectID source_object_id, ...}, ReduceOp op)
using a reduce operation (e.g, sum, min, max).
Table 1: Core Hoplite APIs. The application generates an ObjectID with a unique string and can pass an ObjectID by sending the string.
complete objects so that object store nodes with complete copies
can be favored during a broadcast or reduce (¬ß3.4).
Optimization for small objects. Querying object location can intro-
duce an excessive latency penalty for fetching small objects, and the
overhead of computing efficient object transfer schedule is usually
not worthwhile for small objects in our use cases. Therefore, we
implement a fast path in the object directory service. For small ob-
jects (<64KB), we simply cache them in the object directory service,
and when a node queries for their location, the object directory
service directly returns the object buffers. Similar to object in the
per-node stores, cached objects must be freed by the application
via the Delete call when no longer in use.
3.3 Pipelining
Hoplite uses pipelining to achieve low-latency transfer between
processes and across nodes for large objects. This is implemented
by enabling a receiver node to fetch an object that is incomplete in
a source node. An object can be incomplete if the operation that
created the object, either a Put from the client or a copy between
object store nodes, is still in progress. To enable fetching incomplete
objects, as shown in the previous section (¬ß3.2), the object directory
service also maintains locations of incomplete copies. Then, when
an object store receives a Get operation, it can choose to request
the object from a store with an incomplete copy.
By pipelining data transfers across nodes using the object direc-
tory service as an intermediary, it becomes simple to also pipeline
higher-level collective communication primitives, such as a reduce
followed by a broadcast (Figure 2b). Within the reduce, a node
can compute a reduce of a subset of the input objects and simulta-
neously send the intermediate result to a downstream node. The
downstream node can then compute the final reduce result by
computing on the intermediate result as it is received and simulta-
neously send the final result to any broadcast receivers that have
been scheduled. A broadcast receiver can then also simultaneously
send the final result to any other broadcast receivers.
Piplining between the task worker and local store on the same
node is also important to hide Put and Get latency for large objects
(steps 2 and 5 in Figure 3). The reason is that using the distributed
object store requires two additional data copies other than the
minimum needed to transfer data over the network. The sender
task worker must copy to its local store, and then the receiving local
store must also copy to its local worker. Our observation is that
the additional memory copy latency can be masked by the network
transfer if the memory copy is asynchronous. When a sender task
calls Put, Hoplite immediately notifies the object directory service
that the object is ready to transfer. A receiver can then fetch the
object before the entire object is copied into the sender node‚Äôs local
store. The receiver side‚Äôs pipelining mechanism is similar. When
the receiver task calls Get, the receiver task starts to copy the object
from the local store before the local store has a complete object.
By combining cross-node and in-node pipelining, Hoplite enables
end-to-end object streaming between the sender and receiver tasks,
even when there are multiple rounds of collective communication