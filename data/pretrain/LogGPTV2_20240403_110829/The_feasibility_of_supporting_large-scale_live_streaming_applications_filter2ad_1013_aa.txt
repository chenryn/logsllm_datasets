# The Feasibility of Supporting Large-Scale Live Streaming Applications with Dynamic Application End-Points

**Authors:**
Kunwadee Sripanidkulchai, Aditya Ganjam, Bruce M. Maggs, and Hui Zhang  
Carnegie Mellon University

## Abstract
Application end-point architectures have proven to be viable solutions for large-scale distributed applications such as distributed computing and file-sharing. However, their feasibility for more bandwidth-demanding applications like live streaming remains less understood. The inherent properties of application end-points, including heterogeneous bandwidth resources and dynamic group membership, may pose significant challenges in constructing a usable and efficient overlay. This paper investigates the feasibility of supporting large-scale live streaming using an application end-point architecture. We focus on three key requirements: (i) the availability of sufficient resources to construct an overlay, (ii) the ability to maintain a stable and connected overlay in the presence of group dynamics, and (iii) the construction of an efficient overlay. Using data from a large content delivery network, we analyze the behavior of users watching live audio and video streams. Our findings indicate that in many common real-world scenarios, all three requirements can be met. We also evaluate the performance of several design alternatives and show that simple algorithms can effectively meet these requirements in practice. Overall, our results support the feasibility of using an application end-point architecture for large-scale live streaming.

## Categories and Subject Descriptors
C.2 [Computer-Communication Networks]: Distributed Systems

## General Terms
Measurement, Performance

## Acknowledgments
This research was sponsored by DARPA under contract number F30602-99-1-0518, and by NSF under grant numbers Career Award NCR-9624979, ANI-9730105, ITR Award ANI-0085920, ANI-9814929, and ANI-0331653. Additional support was provided by Intel. The views and conclusions in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of DARPA, NSF, Intel, or the U.S. government.

**Note:** Bruce Maggs is also affiliated with Akamai Technologies.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.

SIGCOMM'04, Aug. 30–Sept. 3, 2004, Portland, Oregon, USA.  
Copyright 2004 ACM 1-58113-862-8/04/0008 ...$5.00.

## Keywords
Overlay multicast, application-level multicast, peer-to-peer, live streaming

## 1. Introduction
Live audio and video streams are now being delivered successfully over the Internet on a large scale. Commercial content delivery networks such as Akamai Technologies and Real Networks have developed and deployed large-scale dedicated infrastructure to deliver both live streams and video-on-demand. These architectures are capable of supporting many simultaneous streams and clients.

In contrast, application end-point overlay multicast has recently gained attention. In such an architecture, application end-points organize themselves into an overlay structure, and data is distributed along the links of the overlay. The responsibility and cost of providing bandwidth are shared among the application end-points, reducing the burden on the content publisher. The lack of dependence on infrastructure support makes application end-point architectures easy to deploy and economically viable. Users are naturally incentivized to contribute resources to the system to receive content they might otherwise not have access to. While application end-point architectures have shown promise for events with peak group sizes of 10 to 100 nodes, their feasibility at larger scales of 1,000s to 100,000s of nodes remains an open question.

To address this, we leverage data from Akamai Technologies, a large content delivery network that provides live streaming services. The data reflects common "real-world" application end-point characteristics and behavior, which may impact the choice of architectures. We investigate three key requirements for feasibility: (i) the availability of sufficient resources to construct an overlay, (ii) the ability to maintain a stable and connected overlay in the presence of group dynamics, and (iii) the construction of an efficient overlay. Our findings suggest that in the majority of common scenarios, application end-point architectures have sufficient inherent resources and stability. Additionally, efficient overlay structures can be constructed using simple algorithms. These results support the feasibility of using application end-point architectures for large-scale live streaming applications.

## 2. Live Streaming Workload
In this section, we analyze the live streaming workload from a large content delivery network to better understand the design requirements for building a live streaming system. We focus on characteristics that are most likely to impact design, such as group dynamics. In the following sections, we evaluate the impact of the workload on the performance of an application end-point architecture.

### 2.1 Data Collection and Summary Statistics
The logs used in our study were collected from thousands of streaming servers belonging to Akamai Technologies. Akamai’s streaming network is a static overlay composed of edge nodes located close to clients and intermediate nodes that take streams from the original content publisher and split and replicate them to the edge nodes. The logs used in this study are from the edge nodes that directly serve client requests.

The logs were collected over a 3-month period from October 2003 to January 2004. Figure 1 shows the daily statistics for live streaming traffic during that period. The traffic consists of three popular streaming media formats: QuickTime, Real, and Windows Media. On most days, there were typically 800-1,000 distinct streams. However, there was a sharp drop in early December and again from mid-December to January due to a problem with our log collection infrastructure. Streams were classified as audio or video based on their encoding bit rate. If the bit rate was under 80 kbps, it was classified as audio. Roughly 71% of the streams were audio, and 7% were video. We did not classify 22% of the streams due to insufficient information about their streaming bit rates. Figure 1(b) depicts the number of requests for live streams, which varied from 600,000 on weekends to 1 million on weekdays. There were an order of magnitude more requests for audio streams than video streams. Audio streams had extremely regular weekend/weekday usage patterns, while video streams were less regular and dominated by short-duration special events with sharp peaks corresponding to very large events on various days.

Streaming media events can be classified into two broad categories based on event duration. The first category, non-stop events, are events with live broadcasts every day, all hours of the day. The second category, short-duration events, are events with well-defined durations, typically a couple of hours. For simplicity, we break the events into 24-hour chunks, which we call streams. For the rest of the paper, we present analysis on the granularity of streams. Note that for short-duration events, a stream is the same as an event.

We limit our discussion to large-scale streams, defined as streams with a peak group size (i.e., the maximum concurrent number of participating hosts) larger than 1,000 hosts. There were a total of 660 large-scale streams, of which 55 were video streams and 605 were audio streams. Many of the audio streams were non-stop, and all of the video streams were short-duration.

Figure 2 depicts the peak group size and the total number of requests for each stream. Across all large-scale streams, the peak group size ranged from 1,000 to 80,000 hosts, and the total number of requests ranged from 2,000 to 200,000. In addition to group size, we also summarize session duration characteristics, which are analyzed in detail in Section 4.

### 2.2 Workload Processing
**Entity vs. Incarnation:**
We define an entity as a unique host, corresponding to an IP address. An entity or host may join the broadcast multiple times, perhaps to tune in to distinct portions of the broadcast, and thus have many incarnations.

**Log Format:**
Each entry in the log corresponds to an incarnation's session, or a request made by a user to an edge server. The following fields extracted from each entry are used in our study:
- User identification: IP address
- Requested object: stream URI
- Time-stamps: session start time and duration in seconds

### 2.3 Largest Event
Next, we present more detailed statistics for the largest event in the logs. The event consisted of three encoded streams at bit rates of 20 kbps audio, 100 kbps audio and video, and 250 kbps audio and video. The event duration was 2 hours, from 19:00-21:00, as shown in Figure 3(a). The sharp rise in membership at 19:00 is a flash crowd caused by everyone wanting to tune in to the start of the event. Flash crowds are common in our logs, with about 40% of large-scale streams having flash crowds. Combining all three streams, the peak group size was 74,000 users. There were roughly 119,000 IP addresses and over 394,000 requests for the entire duration of the event. Note that there were roughly 3.3 requests per IP address. This may be due to (i) users joining the broadcast multiple times, perhaps to tune in to distinct portions of the broadcast, or (ii) multiple users sharing the same IP address in the case of network address translation (NAT).

[Figures and additional sections would follow here, but they are not included in the provided text.]

---

This version of the text is more structured and professional, with clear headings and a logical flow. It also includes a more detailed and coherent introduction and summary of the key points.