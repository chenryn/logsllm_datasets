title:The feasibility of supporting large-scale live streaming applications
with dynamic application end-points
author:Kunwadee Sripanidkulchai and
Aditya Ganjam and
Bruce M. Maggs and
Hui Zhang
The Feasibility of Supporting Large-Scale Live Streaming
Applications with Dynamic Application End-Points
Kunwadee Sripanidkulchai, Aditya Ganjam, Bruce Maggs,y and Hui Zhang
Carnegie Mellon University
ABSTRACT
While application end-point architectures have proven to be viable
solutions for large-scale distributed applications such as distributed
computing and ﬁle-sharing, there is little known about its feasibility
for more bandwidth-demanding applications such as live streaming.
Heterogeneity in bandwidth resources and dynamic group member-
ship, inherent properties of application end-points, may adversely
affect the construction of a usable and efﬁcient overlay. At large
scales, the problems become even more challenging. In this paper,
we study one of the most prominent architectural issues in overlay
multicast: the feasibility of supporting large-scale groups using an
application end-point architecture. We look at three key require-
ments for feasibility: (i) are there enough resources to construct an
overlay, (ii) can a stable and connected overlay be maintained in
the presence of group dynamics, and (iii) can an efﬁcient overlay
be constructed? Using traces from a large content delivery network,
we characterize the behavior of users watching live audio and video
streams. We show that in many common real-world scenarios, all
three requirements are satisﬁed. In addition, we evaluate the per-
formance of several design alternatives and show that simple algo-
rithms have the potential to meet these requirements in practice.
Overall, our results argue for the feasibility of supporting large-
scale live streaming using an application end-point architecture.
Categories and Subject Descriptors
C.2 [Computer-Communication Networks]: Distributed Systems
General Terms
Measurement, Perfomance
This research was sponsored by DARPA under contract num-
ber F30602-99-1-0518, and by NSF under grant numbers Career
Award NCR-9624979 ANI-9730105, ITR Award ANI-0085920,
ANI-9814929, and ANI-0331653. Additional support was provided
by Intel. Views and conclusions contained in this document are
those of the authors and should not be interpreted as representing
the ofﬁcial policies, either expressed or implied, of DARPA, NSF,
Intel, or the U.S. government.
yBruce Maggs is also with Akamai Technologies.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’04, Aug. 30–Sept. 3, 2004, Portland, Oregon, USA.
Copyright 2004 ACM 1-58113-862-8/04/0008 ...$5.00.
Keywords
Overlay multicast, application-level multicast, peer-to-peer, live stream-
ing
1.
INTRODUCTION
Live audio and video streams are now being delivered success-
fully over the Internet on a large scale. Commercial content delivery
networks such as Akamai Technologies [1] and Real Networks [21]
have developed and deployed large-scale dedicated infrastructure to
deliver both live streams and video-on-demand. These architectures
are capable of supporting many simultaneous streams and clients.
In contrast to infrastructure-based content delivery networks,
application end-point overlay multicast has recently received atten-
tion [9, 8, 12, 14, 20, 25, 5, 2, 18, 24, 13, 4]. In such an archi-
tecture, application end-points organize themselves into an overlay
structure and data is distributed along the links of the overlay. The
responsibility and cost of providing bandwidth is shared amongst
the application end-points, reducing the burden at the content pub-
lisher. The lack of dependence on infrastructure support makes ap-
plication end-point architectures easy to deploy, and economically
viable. The ability for users to receive content that they would oth-
erwise not have access to provides a natural incentive for them to
contribute resources to the system. Application end-point architec-
tures have shown promise for events where the peak group size is
small, on the order of 10 to 100 nodes [7]. However, the question re-
mains whether or not such architectures are feasible at larger scales
of 1,000s to 100,000s of nodes.
We believe that the ﬁrst step towards answering the feasibility
question is to obtain a better understanding of the application work-
load and the characteristics of application end-points. We leverage
the wealth of data from Akamai Technologies, a large content de-
livery network that provides live streaming services. Because the
system has been in everyday use for several years, the data reﬂects
common “real-world” application end-point characteristics and be-
havior that may impact the choice of architectures. Inherent prop-
erties of application end-point architectures, such as heterogeneity
in bandwidth resources and dynamic group membership could ad-
versely affect the feasibility of constructing a usable overlay for data
delivery.
We look at three key requirements for feasibility: (i) there must
be enough resources to construct an overlay, (ii) a stable and con-
nected overlay must be maintained in the presence of group dynam-
ics, and (iii) the overlay structure must be efﬁcient. These three
requirements are fundamental in the sense that an application end-
point architecture has little chance of providing good performance
if these requirements are not satisﬁed. We ﬁnd that in the majority
of common scenarios, application end-point architectures have suf-
ﬁcient inherent resources and stability. In addition, efﬁcient overlay
107s
m
a
e
r
t
S
e
v
L
i
f
o
r
e
b
m
u
N
1400
1200
1000
800
600
400
200
0
10/04
10/18
11/01
11/15
All
Video
Audio
1.2e+06
1e+06
800000
600000
400000
200000
s
t
s
e
u
q
e
R
f
o
r
e
b
m
u
N
All
Video
Audio
12/13
12/27
01/10
01/24
0
10/04 10/18 11/01 11/15 11/29 12/13 12/27 01/10 01/24
11/29
Date
(a) Distinct streams.
Figure 1: Summary of live streams.
(b) Number of requests.
Date
s
t
s
o
H
f
o
r
e
b
m
u
N
100000
10000
1000
Total Requests
Peak Concurrent Hosts
0
100
200
300
400
500
600
700
Stream
Figure 2: Size of large-scale streams.
structures can be constructed using simple algorithms. Our ﬁndings
argue for the feasibility of using application end-point architectures
for large-scale live streaming applications.
In Section 2, we describe and analyze the streaming media
workload that we use in our study. Section 3 studies the feasibility
of constructing overlays using the resources available at application
end-points. In Section 4, we look at overlay stability and evaluate
algorithms to maintaining a stable overlay in the presence of dy-
namic group membership. Section 5 looks at the construction of
efﬁcient overlays. We summarize our ﬁndings in Section 6.
2. LIVE STREAMING WORKLOAD
In this section, we analyze the live streaming workload from a
large content delivery network to better understand the design re-
quirements for building a live streaming system. We focus our anal-
ysis on characteristics that are most likely to impact design, such
as group dynamics. In the following sections, we evaluate the im-
pact of the workload on the performance of an application end-point
architecture.
2.1 Data Collection and Summary Statistics
The logs used in our study are collected from the thousands
of streaming servers belonging to Akamai Technologies. Akamai’s
streaming network is a static overlay composed of (i) edge nodes
located close to clients, and (ii) intermediate nodes that take streams
from the original content publisher and split and replicate them to
the edge nodes. The logs that we use in this study are from the edge
nodes that directly serve client requests.
The logs were collected over a 3-month period from October
2003 to January 2004. The daily statistics for live streaming traf-
ﬁc during that period is depicted in Figure 1. The trafﬁc consists
of three of the most popular streaming media formats, QuickTime,
Real, and Windows Media. In Figure 1(a), there were typically 800-
1000 distinct streams on most days. However, there was a sharp
drop in early December and a drop again from mid-December to
January (denoted by the vertical lines). This is because we had a
problem with our log collection infrastructure and did not collect
logs for one of formats on those days. To classify streams as audio
or video streams, we look at the encoding bit rate. If the bit rate
is under 80 kbps, then it is classiﬁed as audio. Roughly 71% of
the streams are audio, and 7% are video streams. We did not clas-
sify 22% of the streams because there was insufﬁcient information
about their streaming bit rates. Figure 1(b) depicts the number of re-
quests for live streams which varies from 600,000 on the weekends
to 1 million on weekdays. Again, the drop in requests from mid-
December onwards is due to the missing logs. Note that there are
an order of magnitude more requests for audio streams than video
streams. In addition, audio streams have extremely regular week-
end/weekday usage patterns. On the other hand, video streams are
less regular, and are more dominated by “short duration” special
events with the sharp peaks corresponding to very large events on
various days.
Streaming media events can be classiﬁed into two broad cate-
gories based on the event duration. The ﬁrst category, which we call
non-stop events, are events in which there is live broadcast every
day, all hours of the day. This is similar to always being “on-the-
air” in radio terminology. The second category, which we call short
duration events, are events with well-deﬁned durations, typically
on the order of a couple of hours. A typical example is a talk show
that runs from 9am-10am that is broadcast only during that period,
and has no trafﬁc at any other time during the day. For simplicity,
for either one of these categories, we break the events into 24-hour
chunks, which we call streams. For the rest of the paper, we present
analysis on the granularity of streams. Note that for short duration
events, a stream is the same as an event.
In this paper, we limit the discussion to large-scale streams.
Large-scale streams are deﬁned as the streams in which the peak
group size, i.e., the maximum concurrent number of participating
hosts, is larger than 1,000 hosts. There were a total of 660 large-
scale streams, of which 55 were video streams and 605 were audio
streams. Many of the audio streams are non-stop, and all of the
video streams are short duration.
Figure 2 depicts the peak group size and the total number of
All
250 kbps
100 kbps
56 kbps
80000
70000
60000
50000
40000
30000
20000
10000
s
t
s
o
H
f
o
r
e
b
m
u
N
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
n
o
i
t
u
b
i
r
t
s
D
y
t
i
l
i
i
b
a
b
o
r
P
e
v
i
t
l
a
u
m
u
C
0
17.00
18.00
19.00
20.00
21.00
22.00
0
0.01
(a) Membership over time.
Time
Figure 3: Largest event.
All
No 2-Minute Timeout
1 min, 5 min
0.1
1
10
100
1000
Session Duration (minutes)
(b) Session duration.
requests for each stream on the y-axis. Each point on the x-axis rep-
resents a stream. Note that the same value on the x-axis on the two
curves do not necessarily correspond to the same stream. Across
all large-scale streams, the peak group size ranges from 1,000 to
80,000 hosts, and the total number of requests ranges from 2,000 to
200,000. In addition to group size, we also summarize the session
duration characteristics, which are analyzed in detail in Section 4.
2.2 Workload Processing
Entity vs. incarnation: We deﬁne an entity as a unique host, corre-
sponding to an IP address. An entity or host may join the broadcast
many times, perhaps to tune in to distinct portions of the broadcast,
and as a result have many incarnations.
Log format: Each entry in the log corresponds to an incarnation’s
session, or a request made by a user to an edge server. The following
ﬁelds extracted from each entry are used in our study.
(cid:15) User identiﬁcation: IP address
(cid:15) Requested object: stream URI
(cid:15) Time-stamps: session start time and duration in seconds
2.3 Largest Event
Next, we present more detailed statistics for the largest event in
the logs. The event consisted of three encoded streams at bit rates
of 20 kbps audio, 100 kbps audio and video, and 250 kbps audio
and video. The event duration was 2 hours, from 19:00-21:00, as
shown in Figure 3(a). The sharp rise in membership at 19:00 is a
ﬂash crowd caused by everyone wanting to tune in to the start of the
event. We note that ﬂash crowds are common in our logs – about
40% of large-scale streams have ﬂash crowds. Combining all three
streams, the peak group size was 74,000 users. There were roughly
119,000 IP addresses and over 394,000 requests for the entire dura-
tion of the event. Note that there are roughly 3.3 requests/IP address.
This may be caused by (i) users that join the broadcast many times,
perhaps to tune in to distinct portions of the broadcast, or (ii) mul-
tiple users that share the same IP addresses in the case of network