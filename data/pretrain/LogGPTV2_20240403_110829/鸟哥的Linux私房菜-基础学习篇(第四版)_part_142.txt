6 252 7 - spare /dev/vda7
嘿嘿！你的磁盘阵列内的数据不但一直存在，而且你可以一直顺利的运行 /srv/raid 内的数据，即使 /dev/vda7 损毁了！然后通过管理的
功能就能够加入新磁盘且拔除坏掉的磁盘！注意，这一切都是在上线 （on-line） 的情况下进行！ 所以，您说这样的咚咚好不好用啊！ ^_^
新的 distribution 大多会自己搜寻 /dev/md[0-9] 然后在开机的时候给予设置好所需要的功能。不过鸟哥还是建议你， 修改一下配置文件
吧！ ^_^。software RAID 也是有配置文件的，这个配置文件在 /etc/mdadm.conf ！这个配置文件内容很简单， 你只要知道 /dev/md0 的 UUID
就能够设置这个文件啦！这里鸟哥仅介绍他最简单的语法：
[root@study ~]# mdadm --detail /dev/md0 | grep -i uuid
UUID : 2256da5f:4870775e:cf2fe320:4dfabbc6
# 后面那一串数据，就是这个设备向系统注册的 UUID 识别码！
# 开始设置 mdadm.conf
[root@study ~]# vim /etc/mdadm.conf
ARRAY /dev/md0 UUID=2256da5f:4870775e:cf2fe320:4dfabbc6
# RAID设备 识别码内容
# 开始设置开机自动挂载并测试
[root@study ~]# blkid /dev/md0
/dev/md0: UUID="494cb3e1-5659-4efc-873d-d0758baec523" TYPE="xfs"
[root@study ~]# vim /etc/fstab
UUID=494cb3e1-5659-4efc-873d-d0758baec523 /srv/raid xfs defaults 0 0
[root@study ~]# umount /dev/md0; mount -a
[root@study ~]# df -Th /srv/raid
Filesystem Type Size Used Avail Use% Mounted on
/dev/md0 xfs 3.0G 111M 2.9G 4% /srv/raid
# 你得确定可以顺利挂载，并且没有发生任何错误！
如果到这里都没有出现任何问题！接下来就请 reboot 你的系统并等待看看能否顺利的启动吧！ ^_^
除非你未来就是要使用这颗 software RAID （/dev/md0），否则你势必要跟鸟哥一样，将这个 /dev/md0 关闭！ 因为他毕竟是我们在这
个测试机上面的练习设备啊！为什么要关掉他呢？因为这个 /dev/md0 其实还是使用到我们系统的磁盘分区， 在鸟哥的例子里面就是
/dev/vda{5,6,7,8,9}，如果你只是将 /dev/md0 卸载，然后忘记将 RAID 关闭， 结果就是....未来你在重新分区 /dev/vdaX 时可能会出现一些莫名
的错误状况啦！所以才需要关闭 software RAID 的步骤！ 那如何关闭呢？也是简单到爆炸！（请注意，确认你的 /dev/md0 确实不要用且要关
闭了才进行下面的玩意儿）
# 1. 先卸载且删除配置文件内与这个 /dev/md0 有关的设置：
[root@study ~]# umount /srv/raid
[root@study ~]# vim /etc/fstab
UUID=494cb3e1-5659-4efc-873d-d0758baec523 /srv/raid xfs defaults 0 0
# 将这一行删除掉！或者是注解掉也可以！
# 2. 先覆盖掉 RAID 的 metadata 以及 XFS 的 superblock，才关闭 /dev/md0 的方法
[root@study ~]# dd if=/dev/zero of=/dev/md0 bs=1M count=50
[root@study ~]# mdadm --stop /dev/md0
mdadm: stopped /dev/md0  <==看吧！确实不存在任何阵列设备！
[root@study ~]# vim /etc/mdadm.conf
#ARRAY /dev/md0 UUID=2256da5f:4870775e:cf2fe320:4dfabbc6
# 一样啦！删除他或是注解他！
你可能会问，鸟哥啊，为啥上面会有数个 dd 的指令啊？干麻？这是因为 RAID 的相关数据其实也会存一份在磁盘当中，因此，如果你
只是将配置文件移除， 同时关闭了 RAID，但是分区并没有重新规划过，那么重新开机过后，系统还是会将这颗磁盘阵列创建起来，只是名称
可能会变成 /dev/md127 就是了！ 因此，移除掉 Software RAID 时，上述的 dd 指令不要忘记！但是...千千万万不要 dd 到错误的磁盘～那可是
会欲哭无泪耶～
Tips
在这个练习中，鸟哥使用同一颗磁盘进行软件 RAID 的实验。不过朋友们要注意的是，如果真的要实作软件磁盘阵列， 最好是由多颗不同的磁盘
来组成较佳！因为这样才能够使用到不同磁盘的读写，性能才会好！ 而数据分配在不同的磁盘，当某颗磁盘损毁时数据才能够借由其他磁盘挽救回来！这点
得特别留意呢！
想像一个情况，你在当初规划主机的时候将 /home 只给他 50G ，等到使用者众多之后导致这个 filesystem 不够大， 此时你能怎么作？
多数的朋友都是这样：再加一颗新硬盘，然后重新分区、格式化，将 /home 的数据完整的复制过来， 然后将原本的 partition 卸载重新挂载新
的 partition 。啊！好忙碌啊！若是第二次分区却给的容量太多！导致很多磁盘容量被浪费了！ 你想要将这个 partition 缩小时，又该如何作？将
上述的流程再搞一遍！唉～烦死了，尤其复制很花时间ㄟ～有没有更简单的方法呢？ 有的！那就是我们这个小节要介绍的 LVM 这玩意儿！
LVM 的重点在于“可以弹性的调整 filesystem 的容量！”而并非在于性能与数据保全上面。 需要文件的读写性能或者是数据的可靠性，请
参考前面的 RAID 小节。 LVM 可以整合多个实体 partition 在一起， 让这些 partitions 看起来就像是一个磁盘一样！而且，还可以在未来新增或
移除其他的实体 partition 到这个 LVM 管理的磁盘当中。 如此一来，整个磁盘空间的使用上，实在是相当的具有弹性啊！ 既然 LVM 这么好
用，那就让我们来瞧瞧这玩意吧！
LVM 的全名是 Logical Volume Manager，中文可以翻译作逻辑卷轴管理员。之所以称为“卷轴”可能是因为可以将 filesystem 像卷轴一样
伸长或缩短之故吧！LVM 的作法是将几个实体的 partitions （或 disk） 通过软件组合成为一块看起来是独立的大磁盘 （VG） ，然后将这块大
磁盘再经过分区成为可使用分区 （LV）， 最终就能够挂载使用了。但是为什么这样的系统可以进行 filesystem 的扩充或缩小呢？其实与一个
称为 PE 的项目有关！ 下面我们就得要针对这几个项目来好好聊聊！
Physical Volume, PV, 实体卷轴
我们实际的 partition （或 Disk） 需要调整系统识别码 （system ID） 成为 8e （LVM 的识别码），然后再经过 pvcreate 的指令将
他转成 LVM 最底层的实体卷轴 （PV） ，之后才能够将这些 PV 加以利用！ 调整 system ID 的方是就是通过 gdisk 啦！
Volume Group, VG, 卷轴群组
所谓的 LVM 大磁盘就是将许多 PV 整合成这个 VG 的东西就是啦！所以 VG 就是 LVM 组合起来的大磁盘！这么想就好了。 那么
这个大磁盘最大可以到多少容量呢？这与下面要说明的 PE 以及 LVM 的格式版本有关喔～在默认的情况下， 使用 32位的 Linux 系统
时，基本上 LV 最大仅能支持到 65534 个 PE 而已，若使用默认的 PE 为 4MB 的情况下， 最大容量则仅能达到约 256GB 而已～不过，
这个问题在 64位的 Linux 系统上面已经不存在了！LV 几乎没有啥容量限制了！
Physical Extent, PE, 实体范围区块
LVM 默认使用 4MB 的 PE 区块，而 LVM 的 LV 在 32 位系统上最多仅能含有 65534 个 PE （lvm1 的格式），因此默认的 LVM
的 LV 会有 4M*65534/（1024M/G）=256G。这个 PE 很有趣喔！他是整个 LVM 最小的储存区块，也就是说，其实我们的文件数据都是
借由写入 PE 来处理的。简单的说，这个 PE 就有点像文件系统里面的 block 大小啦。 这样说应该就比较好理解了吧？所以调整 PE 会
影响到 LVM 的最大容量喔！不过，在 CentOS 6.x 以后，由于直接使用 lvm2 的各项格式功能，以及系统转为 64 位，因此这个限制已经
不存在了。
Logical Volume, LV, 逻辑卷轴
最终的 VG 还会被切成 LV，这个 LV 就是最后可以被格式化使用的类似分区的咚咚了！那么 LV 是否可以随意指定大小呢？ 当然
不可以！既然 PE 是整个 LVM 的最小储存单位，那么 LV 的大小就与在此 LV 内的 PE 总数有关。 为了方便使用者利用 LVM 来管理其系
统，因此 LV 的设备文件名通常指定为“ /dev/vgname/lvname ”的样式！
此外，我们刚刚有谈到 LVM 可弹性的变更 filesystem 的容量，那是如何办到的？其实他就是通过“交换 PE ”来进行数据转换， 将
原本 LV 内的 PE 移转到其他设备中以降低 LV 容量，或将其他设备的 PE 加到此 LV 中以加大容量！ VG、LV 与 PE 的关系有点像下
图：
图14.3.1、PE 与 VG 的相关性图示
如上图所示，VG 内的 PE 会分给虚线部分的 LV，如果未来这个 VG 要扩充的话，加上其他的 PV 即可。 而最重要的 LV 如果要
扩充的话，也是通过加入 VG 内没有使用到的 PE 来扩充的！
实实作作流流程程
通过 PV, VG, LV 的规划之后，再利用 mkfs 就可以将你的 LV 格式化成为可以利用的文件系统了！而且这个文件系统的容量在未来还能
够进行扩充或减少， 而且里面的数据还不会被影响！实在是很“福气啦！”那实作方面要如何进行呢？很简单呢！ 整个流程由基础到最终的结果
可以这样看：
图14.3.2、LVM 各元件的实现流程图示
如此一来，我们就可以利用 LV 这个玩意儿来进行系统的挂载了。不过，你应该要觉得奇怪的是， 那么我的数据写入这个 LV 时，到底
他是怎么写入硬盘当中的？ 呵呵！好问题～其实，依据写入机制的不同，而有两种方式：
线性模式 （linear）：假如我将 /dev/vda1, /dev/vdb1 这两个 partition 加入到 VG 当中，并且整个 VG 只有一个 LV 时，那么所谓的线性模
式就是：当 /dev/vda1 的容量用完之后，/dev/vdb1 的硬盘才会被使用到， 这也是我们所建议的模式。
交错模式 （triped）：那什么是交错模式？很简单啊，就是我将一笔数据拆成两部分，分别写入 /dev/vda1 与 /dev/vdb1 的意思，感觉上
有点像 RAID 0 啦！如此一来，一份数据用两颗硬盘来写入，理论上，读写的性能会比较好。
基本上，LVM 最主要的用处是在实现一个可以弹性调整容量的文件系统上， 而不是在创建一个性能为主的磁盘上，所以，我们应该利
用的是 LVM 可以弹性管理整个 partition 大小的用途上，而不是着眼在性能上的。因此， LVM 默认的读写模式是线性模式啦！ 如果你使用
triped 模式，要注意，当任何一个 partition “归天”时，所有的数据都会“损毁”的！ 所以啦，不是很适合使用这种模式啦！如果要强调性能与备
份，那么就直接使用 RAID 即可， 不需要用到 LVM 啊！
LVM 必需要核心有支持且需要安装 lvm2 这个软件，好佳在的是， CentOS 与其他较新的 distributions 已经默认将 lvm 的支持与软件都
安装妥当了！所以你不需要担心这方面的问题！用就对了！
假设你刚刚也是通过同样的方法来处理鸟哥的测试机 RAID 实作，那么现在应该有 5 个可用的分区才对！ 不过，建议你还是得要修改一
下 system ID 比较好！将 RAID 的 fd 改为 LVM 的 8e 吧！现在，我们实作 LVM 有点像下面的模样：
使用 4 个 partition ，每个 partition 的容量均为 1GB 左右，且 system ID 需要为 8e；
全部的 partition 整合成为一个 VG，VG 名称设置为 vbirdvg；且 PE 的大小为 16MB；
创建一个名为 vbirdlv 的 LV，容量大约 2G 好了！
最终这个 LV 格式化为 xfs 的文件系统，且挂载在 /srv/lvm 中
0. Disk 阶阶段段 （（实实际际的的磁磁盘盘））
鸟哥就不仔细的介绍实体分区了，请您自行参考第七章的 gdisk 来达成下面的范例：
[root@study ~]# gdisk -l /dev/vda
Number Start （sector） End （sector） Size Code Name
1 2048 6143 2.0 MiB EF02
2 6144 2103295 1024.0 MiB 0700
3 2103296 65026047 30.0 GiB 8E00
4 65026048 67123199 1024.0 MiB 8300 Linux filesystem
5 67123200 69220351 1024.0 MiB 8E00 Linux LVM
6 69220352 71317503 1024.0 MiB 8E00 Linux LVM
7 71317504 73414655 1024.0 MiB 8E00 Linux LVM
8 73414656 75511807 1024.0 MiB 8E00 Linux LVM
9 75511808 77608959 1024.0 MiB 8E00 Linux LVM