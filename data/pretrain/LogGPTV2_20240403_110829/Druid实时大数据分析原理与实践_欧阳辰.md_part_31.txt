自Druid诞生以来，它凭借着自己优秀的特质，不仅逐渐在技术圈收获了越来越多的知
实践和应用
第
章
---
## Page 282
营，促进产品不断发展进步。小米统计的入口是tongi.xiaomi.com，服务界面如图10-2所示。
解应用发展状况、
都有应用。这里介绍Druid在小米统计产品和小米广告平台中的部分技术实践。
集、数据存储、数据管理、数据分析、机器学习和算法、可视化等部分。
理服务平台的任务，整个平台是基于Hadoop生态系统的若干明星产品搭建的，包括数据采
任务变得非常艰巨和重要。
2016年年中，小米MIUI设备总激活数超过2亿，月活用户超过1.5亿。数据处理和分析的
建设。
传统市场。小米生态链建设以开放、合作共赢的策略，和业界合作伙伴一起推动智能生态链
258
10.1.1
小米统计是小米为ApPP开发者提供的移动应用数据统计服务，帮助开发者通过数据了
Druid在数据分析层帮助实时收集海量的事件数据，快速进行商业分析，在多个场景中
小米大云平台整个技术架构和一些技术选型如图10-1所示。
于是，小米云平台团队应运而生。小米大云平台承担着为公司提供海量数据的存储与处
大数据作为小米公司的科技战略，不断地帮助小米产品提高用户体验和改进产品质量。
除了手机之外，小米公司在互联网电视机、智能路由器和智能家居产品等领域也颠覆了
数据采集
数据存储
数据管理
数据分析
算法
可视化
←
场景一：小米统计服务
、渠道推广效果和用户参与情况等信息，使开发者可以更好地优化体验和运
MapReduce
机器学习
Scribe
HDFS
JavaScript
图10-1
Hue
小米大云平台技术架构示意图
Spark
Kerberos
自然语言
E-Charts
HBASE
ETL
数据挖掘
KUDU
HS/App
ImpalaDruid
Druid实时大数据分析原理与实践
Kafka
统计分析
Zookeep
ES
---
## Page 283
并且允许用户直接访问。
Kafka和Tranquility，最后进人Druid集群。Druid集群最终能提供最近一天的数据查询功能，
统的数据仓库，因此引人Druid处理一些标准报告的实时数据查询场景。数据流会依次通过
度，改动比较大，需要从上游到下游整体修改。这种方式的优点是可靠性好，数据处理能力
方便增加数据列。另外，HBase的可用性也高于MySQL。
Schema更改成本高，新业务不断需要增加新列和新表，流程烦琐而且需要进行Schema设计：
复杂的任务会以天为单位被执行，并且最后会将结果写人到如MySQL的RDBMS中。
非完全互斥，而是应用于不同的场景和时间。
第10章
以方便地进行各种复杂的数据处理，各种聚合和处理需要通过程序实现，增加一个数据维
有死锁的现象。为了解决这些问题，引人了HBase作为主要存储数据库，利用HBase的列族
二是在进行大量写操作的情况下，数据库的负载增加会导致数据库的读性能下降，而且偶尔
第四阶段：小米统计的很多数据查询都是选择一些指标和过滤条件，很多场景类似于传
，可以进行各种角度的优化。
第三阶段：为了改进数据的实时性，后期增加了Storm分布式计算模式，使用Storm可
第二阶段：在业务发展过程中MySQL很快变成了瓶颈，有两个原因，一是数据库的
第一阶段：数据存储在Hadoop中，通过MapReduce的脚本进行分析和处理。有一部分
实时的数据分析重要需求，在产品发展过程中，也经历了几个技术阶段，这几个阶段并
实践和应用
错误分析
章设置
网络监控
设备信息
事件统计
渠道统计
用户参与
应用概况
MI小米开放平台
Q
dev.xiaomi.com/mistats/app-sumimiary.htinl/appid=1008.packagenaine=com.xlaomistats.demo
今日实时数据
用户概况
图10-2小米统计服务界面
新增668
新增90967
新增用户
启动次数
日活17077
日活91132
启动8586
启动142870
259
---
## Page 284
方式更新和聚合数据结果。
的数据分析，不用于广告主投放平台。广告主投放平台使用Mini-batch方式，通过可重放的
内部的实时数据分析需求；另一条是通过Mini-batch方式。
度的数据分析。
析线上的各种维度的变化，包括上线部署的实时监控分析、A/B测试的效果查询、一些细粒
10.1.2
析能力。
260
对广告数据有两条路径进行处理：一条是实时的数据流，通过Druid处理，主要是针对
Druid来源于广告业务，小米广告平台也利用Druid进行实时的数据分析，帮助实时分
在使用Druid的过程中也会碰到一些问题。
比如对于广告计费分析模块，Druid会包括实时的广告主计费信息，这些数据用于内部
·小米广告交易平台（XiaomiAdExchange,MAX）：广告流量的调度管理平台。
数据的DataSource（数据源）包括：
Druid作为一种实时分析数据库，提升了小米大数据平台和商业产品部门的实时数据分
·广告媒体分析数据：各个广告媒体的请求、展现等数据。
·广告平台的计费分析模块：广告主的计费、各种维度数据。
小米统计数据流如图10-3所示。
场景二：广告平台实时数据分析
Hadoop/MR
Hadoop/MR
图10-3
小米统计数据流
NoSQL/HBase
RDBMS
Druid实时大数据分析原理与实践
分析洞察
---
## Page 285
尽量让数据进人Druid系统，虽然偶尔会带来系统的峰值压力。
后会有过期机制，因此如果有数据无法进入系统，那么这些数据可能丢失。因此，我们还是
索引文件所占用的CPU会很高，有时候影响正常的查询性能。
数量相当，机器的数量随着事件数的增长而增加。当数据源在某时间数据急剧增加时，系统
3.
查询效率的关系如图10-4所示。
年。每天晚上的时候，聚合小时级别的数据，这样可以避开高负载的集群时间。聚合粒度与
级别聚合的数据源，数据保持10天；另一部分是按照小时级别聚合的数据源，数据保持2
常慢。为了解决这个问题，对于频繁查询的数据源，可以分为两个部分：一部分是按照分钟
2.
源都需要额外开发一些界面，增加维度，也需要修改前端工程，因此效率也不高。在后期的
查询接口，主要是满足业务的需求，初期效果还好，但是随着数据源的增加，每次增加数据
第10章实践和应用
工作中，尝试了Pivot工具，功能使用方便，渐渐代替了自定义的查询界面。
部署情况
第一阶段，我们尝试在服务层使用流量控制，但是后来放弃了。原因是，数据在1小时
Druid集群每天处理近百亿的事件请求，集群规模为近10台机器，索引服务和历史节点
关于查询效率
关于查询界面
Druid的大部分时间性能表现都很好，但是如果进行长时间范围的查询，系统会变得非
Druid的查询语言还不是特别友好，在第一阶段部署Druid后，我们开发了一套Druid
问题：长时间段查询慢
一个数据源
图10-4聚合粒度与查询效率的关系
问题：查询结果容易混淆
其他时间，天级别
过去30天，
一个数据源
分钟级
问题：需要解释两个数据源
两个数据源
261
---
## Page 286
时计算，MySQL做OLAP引擎就可以支撑。我们来看一下睿视平台的需求以及技术挑战。
来很大的挑战。简单一点的，数据日增量在百万量级或千万量级，使用Kafka、Storm进行实
情况等22个统计项。不同数据量级的技术架构不同，数据增大一个量级就会给技术架构带
地域等17个维度，出价数、曝光数、独立访客数、点击数、落地页的相关转化指标以及扣费
域查看竞得率和转化率，甚至是分钟粒度的。睿视平台提供了广告主、投放、素材、广告位、
出价策略或者投放策略进行优化，想要尽快得到实时的反馈，比如修改地域定投，实时分地
时竞价是DSP的核心，广告主或者优化师需要动态调整出价优化使收益最大化。广告主调整
地域、重定向、人群、设备、频道、视频标签、浏览器、操作系统等多种定向投放形式。实
依托优酷土豆的优质视频媒体资源提供视频贴片、Banner、角标等多种广告形式，支持时间、
10.2.1
10.2
262
随着程序化交易和实时竞价在在线广告领域的应用，优酷土豆推出了DSP平台“睿视”
·多维度下的独立访客统计，独立访客数在千万量级。
·高并发下的交互式查询，给广告主提供在线查询服务。
·任意多维钻取分析，17个维度、22个统计项。
·数据日增量为10亿量级。
基于 Druid的架构和数据流如图 10-5所示。
·数据延迟在分钟级别，导入即可被查询。
需求分析
优酷土豆
图10-5基于Druid的架构和数据流
Scribe
Kafka
HDFS
DRUID
Druid实时大数据分析原理与实践
---
## Page 287
Hadoop Indexer批量生成Segment。Druid的架构组合方式非常灵活，可以采用以下方式。
志文件采用tsv格式存储，实时部分采用Kafka和Storm组合计算增量数据，离线部分采用
时多维分析。睿视平台使用Druid以后的整体架构示意图如图10-6所示。
调研了Druid，Druid本身就是为统计而生的，它诞生的背景也是为互联网广告公司提供实
加维度，维度增加以后预聚合的数据量级也会呈指数级增加，特别是增加基数较大的维度以
有百万左右，使用MySQL作为查询引擎也能支撑。但是随着业务需求的变化，会不断地增
合的效果就越好。
如果在数据模型中添加用户维度，那么预聚合的量级至少是千万级，维度组合越稀疏，预聚
景中一般会进行预聚合来减少数据量。影响预聚合以后的数据量的因素如下。
选型的首要因素是海量数据，需求中提到的10亿量级是最细粒度的原始数据，在OLAP场
同样是10亿量级，我们最初选取12个维度，查询的时间粒度为小时，预聚合以后的数据只
10.2.2
第10章
的扩展性不足以支持指数级增长。
后。睿视平台的分析维度由12个增加到17个时，预聚合的数量从百万增长到千万，MySQI
小时甚至天级别的。假设是小时级别，那么会对同一小时内维度组合相同的数据进行聚合
很多SQLOn Hadoop支持海量数据，能做到交互式查询，但却不支持实时性。后来我们
现在讲一下查询的时间粒度。原始数据的时间单位一般是毫秒，而数据分析一般是分钟
·维度的个数
从需求背景来看，我们面临的技术挑战是海量数据、交互式查询以及实时性。影响技术
这是典型的Lamdba架构，分为实时和离线两部分。数据源是日志收集系统收到的日
这里提到了基数的概念，基数是维度中不重复值的数量。用户维度是一个高基数的维度，
·查询的时间粒度
·维度组合的稀疏程度
·维度的基数
技术选型及工程实践
实践和应用
日志收集
图10-6睿视平台使用Druid以后的整体架构示意图
Kakfa
Hadoop
StormETL
Hive ETL
Hadoop批量摄入
Druid实时摄入
263
---
## Page 288
后在Join Bolt中按照Key合并不同数据流的事件，定时发送到Druid，如图 10-7所示。
组合作为key，根据key对维度进行分组，使相同维度组合的事件落在同一Join Bolt中，然
的数据摄入压力，Storm在一定时间内进行预先合并。同样采用占位补零的方式，使用维度
记作0,1,0。竞价和点击都是0，然后利用 Druid的Roll-up功能进行合并。为了减轻Druid
采用类似于SQL中的Union代替Join。
理状态的实时流计算框架来实现数据流之间的 Join，Apache开源的 Samza和 Flink 都能解决
计算过程中缓存曝光事件，直到具有相同RequestId的竞价事件到来。鉴于此，需要能够处
发生的，但无法保障总是竞价事件先收集到，假设曝光事件先到，那么此时就需要在实时流
不余数据，减少了数据的存储。分布式日志收集的特点是无序，虽然竞价事件一定是最先
录RequestId，而不需要记录相关的维度，然后使用 Requestld关联起来。这种方式的优点是
RequestId）和所有的维度，当该竞价事件产生了曝光和点击时，则只需要在曝光和点击中记
时数据要实现两个数据流之间的连接操作就较为复杂，一般实现有两种方案。
离线部分连接操作还好处理，可以通过Hive的连接功能将三种效果数据合并到一起，但实
Druid只支持单表，那么我们就需要预先将竞价、曝光、点击这三种数据源连接到一张表中。
版本。
知历史节点去加载高版本，同时删除低版本的Segment，默认采用Segment的生成时间作为
上述问题。
合的方式。在第二天凌晨时启动HadoopIndexer批量生成 Segment 去覆盖当天实时生成的
264
第二种方案，用存储空间换计算的复杂性。在所有日志数据源中冗余所有的维度，可以
不同的日志数据源处理其他Metric采用“占位补零”的方式，例如处理曝光日志，Metric
第一种方案，例如竞价事件是所有事件产生的源头，在竞价日志中记录唯一id（叫作
·采用同样的唯一id将不同的日志关联起来。
再来看具体业务。睿视系统的竞价、曝光、点击等效果数据是分日志文件存储的，而
·在不同的日志中余所有的维度。
由于日志收集以及实时计算过程会出现数据丢失情况，所以我们采用批量和实时相结
·使用批量和实时相结合的方式。
·只使用批量方式，使用Hadoop Indexer批量摄人数据。
·只使用实时方式，使用索引服务或者实时节点实时摄入数据。
Druid实时大数据分析原理与实践
---
## Page 289
在300毫秒左右，数据从发生到导人Druid的延迟时间在2分钟以内。
（开源监控系统）中监控统计到每分钟内最大的响应时间大部分在1秒以内，平均响应时间
尽量为查询节点配置较大的内存。
载相对低一点，机器配置不用太好，但是由于其性能依赖于可用内存的大小，因此官方推荐
多的历史节点，建议历史节点尽量使用大容量的内存。
是历史节点的容量最大值，如调整成和内存值一样能避免磁盘IO，但同时也意味着需要更
盖。历史节点可以调整maxCacheSize参数来决定Segment在内存和磁盘中的覆盖比例，这也
议使用索引服务（IndexingService）。
以及修改Segment以后需要重启。其最大的问题是没有副本不能保障高可用性，所以作者建
等消息队列，当内存增量索引持久化以后再提交Ofset保障数据不会丢失；缺点是没有副本，
第10章
查询节点
协调节点
历史节点
实时节点
节点类型
使用Druid 以后的架构，其性能表现良好，通过将 Druid的Metric发送到OpenFalcon
由于Broker采用Scatter/Gather模式进行查询，查询节点只是汇聚查询结果，所以其负
历史节点并没有使用SSD，是因为目前睿视平台的索引还比较小，几乎在内存中都能覆
实时部分我们仍然采用Druid的早期架构实时节点，其优点是足够简单，仅依赖于Kafka
实践和应用
点击
曝光
竞价
图10-7Storm与Druid集成示意图
2台
2台
8台
4台
数量
JoinBolt
12核，32GB内存
12核，32GB内存
24核，64GB内存
24核，64GB内存
配置
Druid
265
---
## Page 290
程将内存增量索引持久化到磁盘中，持久化后的索引采用倒排和Bitmap索引，适合查询，当
据首先在内存增量索引中累积，此过程中会进行Roll-up。当达到一定阈值以后，采用异步线
点，导致查询性能下降。
遵守上述原则，SegmentGranularity至少是day，那么一天之内Segment都不能移交给历史节
须遵守的。例如有一个DataSource的QueryGranularity为day，采用实时摄入的方式，如果
存储1小时的数据。通常，作者建议SegmentGranularity>=QueryGranularity，但这并不是必
粒度，比如 SegmentGranularity为hour，那么说明Segment是按照小时分段的，每个Segment
是同时也意味着不能查询更细时间粒度的数据。SegmentGranularity是Segment分段的时间
程和查询过程中的Roll-up，QueryGranularity越大，Roll-up以后的数据量越小，查询越快，但
明确这两个参数的含义，QueryGranularity是查询的最小时间粒度，它主要影响数据摄入过
节点那样，Segment 的移交受拒绝策略影响。
弃，但乱序的数据则不会丢弃。当达到结束的时间点以后，结束读取并且移交，而不像实时
择，在索引服务中会创建任务读取设定时间段的数据，不在设定时间段内的数据依然会被丢
致 Segment不能移交，所以不适合设置成 none。但如果采用索引服务的话，它却是最佳选
以在实时节点生产环境中应尽量使用serverTime。none表示不丢弃，在实时节点模式下会导
会导致 Segment 的移交延迟，如日志收集存在延迟，则会出现只能移交T-2的 Segment，所
而 serverTime是两者都有可能。由此看来，messageTime更适合一些，但是因为messageTime
系统时间戳进行比较，messageTime仅会因为事件乱序导致被丢弃，而不会因为延迟被丢弃；
件的时间戳和已处理事件中最大的时间戳进行比较，serverTime是当前事件的时间戳和当前
两种策略在超出时间窗口以后都会被丢弃，不同的是时间窗口的选择，messageTime是当前事
Away”，逐渐加大windowPeriod直到thrownAway的数量为可接受值。
则会由于日志乱序以及延迟等原因被丢弃。最佳方式是观察Metric 的“ingest/events/thrown-
Segment的移交，Segment在移交之前是由许多小段组成的，影响查询性能；如果设为较小值，