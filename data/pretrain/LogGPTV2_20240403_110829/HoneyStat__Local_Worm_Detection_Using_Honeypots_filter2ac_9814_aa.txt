# HoneyStat: Local Worm Detection Using Honeypots

## Authors
David Dagon, Xinzhou Qin, Guofei Gu, Wenke Lee, Julian Grizzard, John Levine, and Henry Owen  
Georgia Institute of Technology  
{dagon, xinzhou, guofei, wenke}@cc.gatech.edu  
{grizzard, levine, henry.owen}@ece.gatech.edu

## Abstract
Traditional worm detection systems have relied on global strategies, focusing on scan rates. This approach requires statistical techniques and large datasets (e.g., 220 monitored machines) to generate timely alerts and avoid false positives. However, worm detection techniques for smaller local networks have not been fully explored.

In this paper, we introduce HoneyStat, a system that uses modified honeypots to provide early detection and complement global monitoring strategies. HoneyStat nodes are script-driven, automated, and cover a large IP space, generating three classes of alerts: memory alerts (based on buffer overflow detection and process management), disk write alerts (such as writes to registry keys and critical files), and network alerts. Data collection is automated, and once an alert is issued, a time segment of previous traffic to the node is analyzed using logistic regression to determine the cause of the alert. This can indicate whether an automated or worm attack is present.

We demonstrate HoneyStat's improvements over previous worm detection techniques. First, using trace files from worm attacks on small networks, we show how it detects zero-day worms. Second, we illustrate its ability to detect multi-vector worms that use combinations of ports to attack. Third, the alerts from HoneyStat provide more information than traditional IDS alerts, such as binary signatures, attack vectors, and attack rates. We also use extensive (year-long) trace files to show that the logit analysis produces very low false positive rates.

**Keywords:** Honeypots, Intrusion Detection, Alert Correlation, Worm Detection

## 1. Introduction
Worm detection strategies have traditionally relied on artifacts incidental to the worm infection, such as incoming scan rates. These techniques often measure noise along with attacks, requiring costly algorithms to identify worms. For example, [ZGGT03] suggests using a Kalman filter [Kal60] to detect worm attacks. However, [QDG+] found that this approach works well with large datasets but is inappropriate for smaller networks.

To improve detection time and reduce errors caused by noise, the solution has been to increase monitoring efforts and gather more data. Researchers have suggested creating global monitoring centers [MSVS03] and collecting information from distributed sensors. These efforts have yielded interesting results [YBJ04, Par04].

While global monitoring is necessary, its value for local networks is less clear. Some local networks may have enough information to conclude a worm is active without sharing data with other sites. Additionally, global detection strategies require large amounts of sensor data, which may lead to detection only after local networks have been compromised. Furthermore, achieving consensus among different networks, which often have conflicting policies regarding privacy, notification, and information sharing, is challenging.

In this paper, we propose using honeypots to improve the accuracy of alerts generated for local intrusion detection systems. We describe the worm infection cycle observed in honeypots, leading to the creation of HoneyStat. Since honeypots typically require labor-intensive management, we detail a deployment mechanism to automate data collection. HoneyStat nodes collect three types of events: memory, disk write, and network events. We use logistic regression to analyze previous network traffic and correlate it with intrusion events, helping to identify patterns indicative of automated attacks or worms.

## 2. Related Work
### Honeypots
A honeypot is a vulnerable network decoy used to distract attackers, gather early warnings about new attack techniques, and facilitate in-depth analysis of adversary strategies [Spi03, Sko02]. By design, a honeypot should not receive any legitimate network traffic, reducing the problem of false positives and false negatives common in other IDS systems.

Traditionally, honeypots have been used to gather intelligence about human attackers [Spi03]. The labor-intensive log review required for traditional honeypots makes them unsuitable for real-time IDS. In our experience, data capture and log analysis can take a 1:40 ratio, meaning one hour of activity can require a week to decipher [LLO+03].

The closest work to ours is [LLO+03], which uses honeypots in an intrusion detection system. We have successfully utilized a Honeynet as an IDS tool at the Georgia Institute of Technology, identifying many compromised systems, mostly due to worm-type exploits. Other researchers have explored honeypot alert aggregation techniques [JX04], but these assume a centralized honeypot farming facility. Our simplified alert model allows for distributed honeypots, focusing on defending local networks.

Researchers have also considered using virtual honeypots, particularly with honeyd [Pro03], which emulates the TCP/IP stack behavior of different operating systems. While honeyd can delay worms, it is unclear how it can catch zero-day worms. We believe this approach has promise but must overcome significant hurdles before it can be used as an early warning IDS. Instead, we use full honeypots.

[Kre03] suggested automatic binary signature extraction using honeypots, employing honeyd, flow reconstruction, and pattern detection to generate IDS rules. This approach, while promising, uses a simple algorithm (longest common substring) to correlate payloads, making it difficult to identify polymorphic worms and those using multiple attack vectors.

### Worm Detection
Worm propagation and early detection have been active research topics in the security community. Researchers have proposed epidemic models to study worm spreading [Sta01, ZGT02, CGK03, WL03] and statistical models for early detection [ZGGT03, Wil02, BGB03, WVGK04]. These approaches require a large deployment of sensors or a large monitoring IP space (e.g., 220 IP addresses). Others suggest a "cyber Center for Disease Control" to coordinate data collection and analysis [SPN02]. Researchers have also proposed various data collection and monitoring architectures, such as "network telescopes" [Moo02b] and an "Internet Storm Center" [Ins].

Our objective is to conduct early worm detection in local networks, particularly using local honeypots. In our prior work [QDG+], we analyzed current worm early detection algorithms and found instability and high false positives when applied to local monitoring networks.

### Event Correlation
Several techniques have been proposed for alert/event correlation, such as pre-/post-condition-based pattern matching [NCR02, CM02, CLF03], chronicles formalism [MMDD02], clustering [DW01], and probabilistic-based correlation [VS01, GHH01]. These techniques rely on prior knowledge of attack step relationships. Our approach, however, aims to detect zero-day worm attacks without depending on prior knowledge of attack steps. Statistical alert correlation was presented in [QL03], but our correlation analysis is based on variables collected over short observations, making it suitable for detecting zero-day worms.

## 3. Worm Infection Cycles
If local networks do not have access to the volume of data used by global monitoring systems, what local resources can they use instead? Studying worm infections provides insights into the data that can be collected for use in a local IDS.

### Model of Infection
A key assumption in our monitoring system is that the worm infection can be described systematically. We note that worms may take three types of actions during an infection phase:

#### Memory Events
The infection process begins with a probe for a victim providing port 135 RPC services. The service is overflowed, and the victim spawns a shell listening on a port, usually 4444. This portion of the infection phase is characterized by memory events. No disk writes have taken place, and network activity cannot yet be characterized as abnormal. A buffer overflow has occurred, and the infection has begun by corrupting a process.

#### Network Events
The shell remains open for only one connection and closes after the infection is completed. The shell is used to instruct the victim to download an "egg" program. The egg can be obtained from the attacker or a third party. The time delay between the initial exploit and the download of the egg is usually small in Blaster, but this may not always be the case. During this time, other harmless network traffic may arrive. This portion of the infection phase is often characterized by network traffic. Downloading the egg requires the honeypot to initiate TCP (SYN) or UDP traffic. In some cases, the entire worm payload can be included in the initial attack packet [Sta01, Moo02a, ZGT02].

#### Disk Events
Once the egg is downloaded, it is written to a directory so it may be activated upon reboot. Some worms do not store the payload to disk but perform other destructive disk operations. Not every worm creates disk operations.

These general categories of events, although present in Blaster, do not limit our analysis to just the August 2003 DCOM worm. A local detection strategy must anticipate future worms lacking some of these events.

### Improved Data Capture
Traditional worm detection models deal with worm infection at either the start or end of the cycle. For example, models based on darknets consider only the rate and sometimes the origin of incoming scans. The Destination Source Correlation (DSC) model [GSQ+04] also considers scans but tracks outgoing probes from the victim. The activity in the middle of the cycle (including memory and disk events) can be tracked.

Even if no buffer overflow is involved, as in the case of mail-based worms and LANMAN weak password guessing worms (e.g., pubstro worms), the infection still follows a general pattern: a small set of attack packets obtain initial results, and further network traffic follows, either from the egg deployment or subsequent scans.

Intrusion detection based only on incoming scan rates must address the potentially high rate of noise associated with darknets. As noted, every phase of the infection cycle may experience non-malicious network traffic. Statistical models that filter the noise (e.g., Kalman) require large datasets for input. It is no wonder, then, that scan-based worm detection algorithms have recently focused on distributed data collection.

## 4. HoneyStat Configuration and Deployment
The foregoing analysis of the worm infection cycle generally identified three classes of events that one might track in an IDS: memory, disk, and network events. It is difficult to track all of these events in virtual honeypots or even in stateful firewalls. Networks focused on darknets, of course, have little chance of getting even complete network events, since they generally blackhole SYN packets and never see the full TCP payload.

A complete system is needed to gather the worm cycle events and improve the data stream for an IDS. We therefore use a HoneyStat node, a minimal honeypot created in an emulator and multihomed to cover a large address space. The deployment typically would not be interesting to attackers because of its minimal resources (limited memory, limited drive size, etc.). Worms, however, are indiscriminate and use this configuration.

In practice, we can use VMware GSX Server as our honeypot platform. Currently, VMware GSX Server V3 can support up to 64 isolated virtual machines on a single hardware system [VMW04]. Mainstream operating systems (e.g., Windows, Linux, FreeBSD) all support multihoming. For example, Windows NT allows up to 32 IP addresses per interface. If we use a GSX server with 64 virtual machines running Windows and each Windows having 32 IP addresses, a single GSX machine can have 64 * 32 = 211 IP addresses.

In practice, we found nodes with as little as 32MB RAM and 770MB virtual drives were more than adequate for capturing worms. Since the emulators were idle for the vast majority of the time, many instances could be started on a single machine. Although slow and unusable from a user perspective, these virtual honeypots were able to respond to worms before any timeouts occur.

The honeypots remain idle until a HoneyStat event occurs. We define three types of events, corresponding to the worm infection cycle discussed in Section 3:

1. **MemoryEvent**: A honeypot can be configured to run buffer overflow protection software, such as StackGuard [Inc03], or similar process-based monitoring tools. Windows logs can be monitored for process failures and crashes. Any alert from these tools constitutes a HoneyStat event. Since there are no users, simple anomaly detection techniques that would otherwise trigger enormous false positive rates on live systems work well.

2. **NetworkEvents**: The honeypots are configured to generate no outgoing traffic. If a honeypot generates SYN or UDP traffic, we consider it an event.

3. **DiskEvents**: Within the limits of the host system, we can also monitor honeypot disk activities and trap writes to key file areas. For example, writes to system logs are expected, while writes to C:\WINNT\SYSTEM32 are clearly events. In practice, we found that kqueue [Lem01] monitoring of flat virtual disks was reasonably efficient. One has to enumerate all directories and files of interest, however.

Data recorded during a HoneyStat event includes:
- The OS/patch level of the host.
- The type of event (memory, net, disk), and relevant capture data. For memory events, this includes stack state or any core; for network events, this is the outgoing packet; and for disk events, this includes a delta of the file changes, up to a size limit.
- A trace file of all prior network activity, within a bound \( t_p \), discussed in Section 5.

Based on our analysis, we believe this to be a complete set of features necessary to observe worm behavior. However, new worms and evasive technologies will require us to revisit this heuristic list of features. Additionally, if HoneyStat is given a larger mission (e.g., email virus detection or trojan analysis instead of just worm detection), more detailed features must be extracted from the honeypots.

Once events are recorded, they are forwarded to an analysis node. This may be on the same machine hosting the honeypots or (more likely) a central server that performs logging and propagates the events to other interested nodes. Figure 1(b) shows a conceptual view of one possible HoneyStat deployment. In general, the analysis node has a secure channel connecting it with the HoneyStat servers. Its primary job is to correlate alert events, perform statistical analysis, and issue alerts.

(a) Infection Cycle  
(b) Deployment

**Figure 1.** (a) A timeline of a Blaster worm attack. Because of modular worm architectures, victims are first overflowed with a simple RPC exploit, and instructed to obtain a separate worm "egg," which contains the full worm. The network activity between the initial overflow and download of the "egg" constitutes a single observation. Multiple observations allow one to filter out other scans arriving at the same time. (b) HoneyStat nodes interact with malware on the Internet. Alerts are forwarded through a secure channel to an analysis node for correlation.

(a) Sampling Events