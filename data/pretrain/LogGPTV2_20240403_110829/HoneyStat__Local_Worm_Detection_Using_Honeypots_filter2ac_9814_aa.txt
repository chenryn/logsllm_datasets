title:HoneyStat: Local Worm Detection Using Honeypots
author:David Dagon and
Xinzhou Qin and
Guofei Gu and
Wenke Lee and
Julian B. Grizzard and
John G. Levine and
Henry L. Owen
HoneyStat: Local Worm Detection Using Honeypots
David Dagon, Xinzhou Qin, Guofei Gu, Wenke Lee,
Julian Grizzard, John Levine, and Henry Owen
Georgia Institute of Technology
{dagon,xinzhou,guofei,wenke}@cc.gatech.edu
{grizzard,levine,henry.owen}@ece.gatech.edu
Abstract. Worm detection systems have traditionally used global strategies and
focused on scan rates. The noise associated with this approach requires statisti-
cal techniques and large data sets (e.g., 220 monitored machines) to yield timely
alerts and avoid false positives. Worm detection techniques for smaller local net-
works have not been fully explored.
We consider how local networks can provide early detection and compliment
global monitoring strategies. We describe HoneyStat, which uses modiﬁed hon-
eypots to generate a highly accurate alert stream with low false positive rates. Un-
like traditional highly-interactive honeypots, HoneyStat nodes are script-driven,
automated, and cover a large IP space.
The HoneyStat nodes generate three classes of alerts: memory alerts (based on
buffer overﬂow detection and process management), disk write alerts (such as
writes to registry keys and critical ﬁles) and network alerts. Data collection is au-
tomated, and once an alert is issued, a time segment of previous trafﬁc to the node
is analyzed. A logit analysis determines what previous network activity explains
the current honeypot alert. The result can indicate whether an automated or worm
attack is present.
We demonstrate HoneyStat’s improvements over previous worm detection tech-
niques. First, using trace ﬁles from worm attacks on small networks, we demon-
strate how it detects zero day worms. Second, we show how it detects multi vector
worms that use combinations of ports to attack. Third, the alerts from HoneyStat
provide more information than traditional IDS alerts, such as binary signatures,
attack vectors, and attack rates. We also use extensive (year long) trace ﬁles to
show how the logit analysis produces very low false positive rates.
Keywords: Honeypots, Intrusion Detection, Alert Correlation, Worm Detection
1
Introduction
Worm detection strategies have traditionally relied on artifacts incidental to the worm
infection. For example, many researchers measure incoming scan rates (often using
darknets) to indirectly detect worm outbreaks, e.g., [ZGGT03]. But since these tech-
niques measure noise as well as attacks, they often use costly algorithms to identify
worms. For example, [ZGGT03] suggests using a Kalman ﬁlter [Kal60] to detect worm
attacks. In [QDG+], this approach was found to work with a large data set but proved
inappropriate for smaller networks.
E. Jonsson et al. (Eds.): RAID 2004, LNCS 3224, pp. 39–58, 2004.
c(cid:1) Springer-Verlag Berlin Heidelberg 2004
40
David Dagon et al.
To improve detection time and decrease errors caused by noise, the solution so far
has been to increase monitoring efforts, and gather more data. The intuition is that
with more data, statistical models perform better. Thus, researchers have suggested the
creation of global monitoring centers [MSVS03], and collecting information from dis-
tributed sensors. These efforts are already yielding interesting results [YBJ04,Par04].
Although the need for global monitoring is obvious, the value this has for local
networks is not entirely clear. For example, some local networks might have enough
information to conclude a worm is active, based on additional information they are un-
willing to share with other monitoring sites. Likewise, since global detection strategies
require large amounts of sensor data, worm outbreaks may be detected only after local
networks fall victim. Also, we see signiﬁcant problems in gaining consensus among dif-
ferent networks, which frequently have competing and inconsistent policies regarding
privacy, notiﬁcation, and information sharing. Without doubt, aggregating information
from distributed sensors makes good sense. However, our emphasis is on local net-
works and requires a complimentary approach. In addition to improving the quantity of
monitoring data, researchers should work to improve the quality of the alert stream.
In this paper, we propose the use of honeypots to improve the accuracy of alerts
generated for local intrusion detection systems. To motivate the discussion, we describe
in Section 3 the worm infection cycle we observed in honeypots that led to the cre-
ation of HoneyStat. Since honeypots usually require labor-intensive management and
review, we describe in Section 4 a deployment mechanism used to automate data col-
lection. HoneyStat nodes collect three types of events: memory, disk write and network
events. Section 4 describes these in detail, and discusses a way to compare and correlate
intrusion events. Using logistic regression, we analyze previous network trafﬁc to the
honeypot to see what network trafﬁc most explains the intrusion events. Intuitively, the
logit analysis asks if there is a common set of network inputs that precede honeypot
intrusions. Finding a pattern suggests the presence of an automated attack or worm.
To demonstrate HoneyStat’s effectiveness, in Section 6, we describe our experience
deploying HoneyStat nodes, and a retrospective analysis of network captures. We also
use lengthy (year long) network trace ﬁles to analyze the false positive rate associated
with the algorithm. The false positive rate is low, due to two inﬂuences: (a) the use
of honeypots, which only produce alerts when there are successful attacks, and (b) the
use of user-selected conﬁdence intervals, which let one deﬁne a threshold for alerts.
Finally, in Section 7, we analyze whether a local detection strategy with a low false
positive rate (like HoneyStat) can make an effective worm detection tool. We consider
the advantages this approach has for local networks.
2 Related Work
Honeypots. A honeypot is a vulnerable network decoy used for several purposes: (a)
distracting attackers, (b) gathering early warnings about new attack techniques, (c) fa-
cilitating in-depth analysis of an adversary’s strategies [Spi03,Sko02]. By design, a hon-
eypot should not receive any network trafﬁc, nor will it run any legitimate production
services. This greatly reduces the problem of false positives and false negatives often
found in other types of IDS systems.
HoneyStat: Local Worm Detection Using Honeypots
41
Traditionally, honeypots have been used to gather intelligence about how human
attackers operate [Spi03]. The labor-intensive log review required of traditional honey-
pots makes them unsuitable for a real-time IDS. In our experience, data capture and log
analysis time can require a 1:40 ratio, meaning that a single hour of activity can require
a week to fully decipher [LLO+03].
The closest work to our own is [LLO+03], which uses honeypots in an intrusion
detection system. We have had great success at the Georgia Institute of Technology
utilizing a Honeynet as an IDS tool, and have identiﬁed a large number of compro-
mised systems on campus, mostly the result of worm-type exploits. Other researchers
have started to look at honeypot alert aggregation techniques [JX04], but presume a
centralized honeypot farming facility. Our simpliﬁed alert model allows for distributed
honeypots, but is more focused on defending local networks.
Researchers have also considered using virtual honeypots, particularly with honeyd
[Pro03]. Initially used to help prevent OS ﬁngerprinting, honeyd is a network daemon
that exhibits the TCP/IP stack behavior of different operating systems. It has since been
extended to emulate some services (e.g., NetBIOS). Conceptually, honeyd is a daemon
written using libpcap and libdnet. To emulate a service, honeyd requires researchers
to write programs that completely copy the service’s network behavior. Assuming one
can write enough modules to emulate all aspects of an OS, a single machine can delay
worms by inducing needless connections.
Recently, honeyd was offered as a way to detect and disable worms [Pro03]. We
believe this approach has promise, but must overcome a signiﬁcant hurdle before it is
used as an early warning IDS. It is not clear how a daemon emulating a network service
can catch zero day worms. If one knows a worm’s attack pattern, it is possible to write
modules that will behave like a vulnerable service. But before this is known, catching
zero day worms requires emulating even the presumably unknown bugs in a network
service. We were unable to ﬁnd solutions to these limitations, and so do not consider
virtual networks as a means of providing an improved alert stream. Instead, we used
full honeypots.
More closely related to our work, [Kre03] suggested automatic binary signature
extraction using honeypots. This work used honeyd, ﬂow reconstruction, and pattern
detection to generate IDS rules. The honeycomb approach also has promise, but uses
a very simple algorithm (longest common substring) to correlate payloads. This makes
it difﬁcult to identify polymorphic worms and worms that use multiple attack vectors.
Honeycomb was well suited for its stated purpose, however: extracting string signatures
for automated updates to a ﬁrewall.
Worm Detection. Worm propagation and early detection have been active research top-
ics in the security community. In worm propagation, researchers have proposed an epi-
demic model to study worm spreading, e.g., [Sta01,ZGT02,CGK03,WL03]. For early
detection, researchers have proposed statistical models, e.g., Kalman Filter [ZGGT03],
analyzing repeated outgoing connections [Wil02], ICMP messages collected at border
routers to infer worm activity [BGB03] and victim counter-based detection algorithms
[WVGK04]. All these approaches require a large deployment of sensors or a large mon-
itoring IP space (e.g., 220 IP addresses). Others suggest a “cyber Center for Disease
Control” to coordinate data collection and analysis [SPN02]. Researchers have also pro-
42
David Dagon et al.
posed various data collection and monitoring architectures, e.g., “network telescopes”
[Moo02b] and an “Internet Storm Center” [Ins].
Our objective is also to conduct early worm detection. However, considering the
current difﬁculty and challenges in large space monitoring system (e.g., conﬂicts in
data sharing, privacy, and coordinated responses), our detection mechanism is based
on local networks, in particular, local honeypots for worm detection. In our prior work
[QDG+] we analyzed the current worm early detection algorithms, i.e., [ZGGT03] and
[WVGK04], and found instability and high false positives when applying these tech-
niques to local monitoring networks.
Event Correlation. Several techniques have been proposed for the alert/event correla-
tion, e.g., pre-/post-condition-based pattern matching [NCR02,CM02,CLF03], chroni-
cles formalism [MMDD02], clustering technique [DW01] and probabilistic-based cor-
relation technique [VS01,GHH01]. All these techniques count on some prior knowl-
edge of attack step relationships. Our approach is different in its need to detect zero-day
worm attacks, and does not depend on prior knowledge of attack steps. Statistical alert
correlation was presented in [QL03]. Our work is different in that our correlation anal-
ysis is based on variables collected over short observations. Time series-based analysis
proposed in [QL03] is good for relatively long observation variables and requires a
series of statistical tests in order to accurately correlate observations.
3 Worm Infection Cycles
If local networks do not have access to the volume of data used by global monitoring
systems, what local resources can they use instead? Studying worm infections gives
some insights, and identiﬁes what data can be collected for use in a local IDS.
Model of Infection. A key assumption in our monitoring system is that the worm in-
fection can be described in a systematic way. We ﬁrst note that worms may take three
types of actions during an infection phase. The Blaster worm is instructive, but we do
not limit our model to this example. Blaster consists of a series of modules designed to
infect a host [LUR03].
Memory Events. The infection process, illustrated in Figure 1(a), begins with a probe
for a victim providing port 135 RPC services. The service is overﬂowed, and the victim
spawns a shell listening on a port, usually 4444. (Later generations of the worm use
different or even random ports.) This portion of the infection phase is characterized by
memory events. No disk writes have taken place, and network activity cannot (yet) be
characterized as abnormal, since the honeypot merely ACKs incoming packets. Still, a
buffer overﬂow has taken place, and the infection has begun by corrupting a process.
Network Events. The Blaster shell remains open for only one connection and closes
after the infection is completed. The shell is used to instruct the victim to download
(often via tftp) an “egg” program. The egg can be obtained from the attacker, or a third
party (such as a free website, or other compromised hosts.) The time delay between the
initial exploit and the download of the egg is usually small in Blaster, but this may not
always be the case. Exploits that wait a long period to download the egg risk having the
service restarted, canceled, or infected by competing worms (e.g., Nachi). Nonetheless,
HoneyStat: Local Worm Detection Using Honeypots
43
some delay may occur between the overﬂow and the “egg” transfer. All during this
time, other harmless network trafﬁc may arrive. This portion of the infection phase is
often characterized by network trafﬁc. Downloading the egg for example requires the
honeypot to initiate TCP (SYN) or UDP trafﬁc. In some cases, however, the entire worm
payload can be included in the initial attack packet [Sta01,Moo02a,ZGT02]. In such a
case, network events may not be seen until much later.
Disk Events. Once the Blaster egg is downloaded, it is written to a directory so it may
be activated upon reboot. Some worms (e.g., Witty [LUR04]) do not store the payload
to disk, but do have other destructive disk operations. Not every worm creates disk
operations.
These general categories of events, although present in Blaster, do not limit our anal-
ysis to just the August 2003 DCOM worm. A local detection strategy must anticipate
future worms lacking some of these events.
Improved Data Capture. Traditional worm detection models deal with worm infection
at either the start or end of the cycle shown in Figure 1(a). For example, models based on
darknets consider only the rate and sometimes the origin of incoming scans, the trafﬁc
at the top of the diagram. The Destination Source Correlation (DSC) model [GSQ+04]
also considers scans, but also tracks outgoing probes from the victim (trafﬁc from the
bottom of the diagram). The activity in the middle of the cycle (including memory and
disk events) can be tracked.
Even if no buffer overﬂow is involved, as in the case of mail-based worms and
LANMAN weak password guessing worms (e.g., pubstro worms), the infection still
follows a general pattern: a small set of attack packets obtain initial results, and further
network trafﬁc follows, either from the egg deployment, or from subsequent scans.
Intrusion detection based only on incoming scan rates must address the potentially
high rate of noise associated with darknets. As noted in Figure 1(a), every phase of the
infection cycle may experience non-malicious network trafﬁc. Statistical models that
ﬁlter the noise (e.g., Kalman) require large data sets for input. It is no wonder, then,
that scan-based worm detection algorithms have recently focused on distributed data
collection.
4 HoneyStat Conﬁguration and Deployment
The foregoing analysis of the worm infection cycle generally identiﬁed three classes
of events that one might track in an IDS: memory, disk and network events. As noted
above in Section 2, it is difﬁcult to track all of these events in virtual honeypots or
even in stateful ﬁrewalls. Networks focused on darknets, of course, have little chance
of getting even complete network events, since they generally blackhole SYN packets,
and never see the full TCP payload.
A complete system is needed to gather the worm cycle events and improve the data
stream for an IDS. We therefore use a HoneyStat node, a minimal honeypot created
in an emulator, and multihomed to cover a large address space. The deployment typ-
ically would not be interesting to attackers, because of its minimal resources (limited
memory, limited drive size, etc.) Worms, however, are indiscriminating and use this
conﬁguration.
44
David Dagon et al.
In practice, we can use VMware GSX Server as our honeypot platform. Currently,
VMware GSX Server V3 can support up to 64 isolated virtual machines on a sin-
gle hardware system [VMW04]. Mainstream operating systems (e.g. Windows, Linux,
FreeBSD) all support multihoming. For example, Windows NT allows up to 32 IP ad-
dresses per interface. So if we use a GSX server with 64 virtual machines running
windows and each windows having 32 IP addresses, then a single GSX machine can
have 64 ∗ 32 = 211 IP addresses.
In practice, we found nodes with as little as 32MB RAM and 770MB virtual drives
were more than adequate for capturing worms. Since the emulators were idle for the
vast majority of time, many instances could be started on a single machine. Although
slow and unusable from a user perspective, these virtual honeypots were able to respond
to worms before any timeouts occur.
The honeypots remain idle until a HoneyStat event occurs. We deﬁne three types of
events, corresponding to the worm infection cycle discussed in Section 3.
1. MemoryEvent. A honeypot can be conﬁgured to run buffer overﬂow protection
software, such as a StackGuard [Inc03], or similar process-based monitoring tools.
Likewise, Windows logs can be monitored for process failures and crashes. Any
alert from these tools constitutes a HoneyStat event. Because there are no users,
we found that one can use very simple anomaly detection techniques that would
otherwise trigger enormous false positive rates on live systems. Since there are no
users, even simple techinques work well.
2. NetworkEvents. The honeypots are conﬁgured to generate no outgoing trafﬁc.
If a honeypot generates SYN or UDP trafﬁc, we consider it an event.
3. DiskEvents. Within the limits of the host system, we can also monitor honeypot
disk activities and trap writes to key ﬁle areas. For example, writes to systems logs
are expected, while writes to C:\WINNT\SYSTEM32 are clearly events. In prac-
tice, we found that kqueue [Lem01] monitoring of ﬂat virtual disks was reasonably
efﬁcient. One has to enumerate all directories and ﬁles of interest, however.
Data recorded during a HoneyStat event includes: (a) The OS/patch level of the
host. (b) The type of event (memory, net, disk), and relevant capture data. For memory
events, this includes stack state or any core, for network events this is the outgoing
packet, and for disk events this includes a delta of the ﬁle changes, up to a size limit.
(c) A trace ﬁle of all prior network activity, within a bound tp, discussed in Section 5.
Based on our analysis in Section 3, we believe this to be a complete set of features
necessary to observe worm behavior. However, new worms and evasive technologies
will require us to revisit this heuristic list of features. Additionally, if HoneyStat is
given a larger mission (e.g., e-mail virus detection or trojan analysis instead of just
worm detection), then more detailed features must be extracted from the honeypots.
Once events are recorded, they are forwarded to an analysis node. This may be on
the same machine hosting the honeypots, or (more likely) a central server that performs
logging and propagates the events to other interested nodes. Figure 1(b) shows a con-
ceptual view of one possible HoneyStat deployment. In general, the analysis node has a
secure channel connecting it with the HoneyStat servers. Its primary job is to correlate
alert events, perform statistical analysis, and issue alerts.
HoneyStat: Local Worm Detection Using Honeypots
45
(a) Infection Cycle
(b) Deployment
Fig. 1. a) A time line of a Blaster worm attack. Because of modular worm architectures, victims
are ﬁrst overﬂowed with a simple RPC exploit, and instructed to obtain a separate worm “egg”,
which contains the full worm. The network activity between the initial overﬂow and download
of the “egg” constitutes a single observation. Multiple observations allow one to ﬁlter out other
scans arriving at the same time. b) HoneyStat nodes interact with malware on the Internet. Alerts
are forwarded through a secure channel to an analysis node for correlation.
(a) Sampling Events