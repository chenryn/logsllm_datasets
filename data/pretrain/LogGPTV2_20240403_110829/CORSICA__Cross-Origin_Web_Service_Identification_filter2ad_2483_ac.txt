access to those images.
Figure 5: Diagram of the identification process for fea-
tures for each file type.
Crawl Online Installations. The third approach uses a crawler to
obtain the relevant files from live systems. We initialize the crawler
with the IP address and the port of a running web service or the
web interface of a target device. Starting at the index page of the
web service or interface, the crawler tries to find all relevant files
accessible from the crawler’s position. Those can be, for example,
linked in the HTML files as well as inside JavaScript and style
sheet files served by the server. The downside of this approach is
that the number of files the crawler can access is limited to files
linked in publicly accessible documents. In some cases, relevant
files are publicly accessible, but only pages inside protected areas
link to them, which the crawler cannot reach. This results in weaker
feature vectors in comparison to those generated by the first two
approaches.
5.2 Extracting Feature Vectors
After creating the service corpus, we can begin to generate feature
vectors, as described in subsection 4.2.
An image file is represented by a feature divided into two sub-
features – the attributes width and height. This information can be
extracted using image processing libraries.
JavaScript and CSS files result in more than two possible sub-
features as such files offer more attributes we can access from
JavaScript. To extract the subfeatures from those files, we use a
parser. While extracting subfeatures for style sheets, we have to
consider the different types of possible style definitions, such as
defining attributes for all elements of a specific type and elements
with IDs or classes. An example is described in subsection 4.2.
The subfeatures for JavaScript are functions or variables which
are declared and accessible from a global context. Besides a simple
test of an existing function or variable, we can call the toString
function of JavaScript on any defined function with a name to get
the source code, including comments, despite the SOP [24]. The
basis for that can be extracted using the Selenium framework.
The resulting file identification features are combined to feature
vectors. Those feature vectors are used to identify full services or
single service components.
5.3 Tree Construction and Optimization
The construction of feature vectors is only the first step in being
able to identify services efficiently. While every identified IP ad-
dress/port pair could be checked against the whole service corpus,
this process would be slow and error-prone due to varying browser
behavior. Therefore, first, the feature vectors will be normalized
for behavior in different browsers, and after that, a tree will be con-
structed from the normalized corpus to optimize the identification
process.
Normalizing the Feature Vectors. The identification of images
works the same way in every browser. Unfortunately, this does
not apply to JavaScript and style sheet files. The process of file
identification shown in Figure 5 relies on the browser executing
JavaScript or applying style directives and comparing the result
with expected values. However, different browsers will interpret
style sheets as well as JavaScript code differently, and, therefore,
some of the generated feature vectors will not work correctly. Addi-
tionally, JavaScript files sometimes require other JavaScript code to
be executed before to work, i.e., if functions rely on frameworks like
jQuery. To deal with this, we have implemented an automated test
suite based on the Selenium framework [30] supporting Chrome
and Firefox. This test suite uses both browsers to test all available
identification vector from the database against the files in question
and marks all subfeatures that could not be recognized correctly,
constructing normalized feature vectors that can be recognized
in both browsers. Furthermore, this enables us to build separate
feature vectors for each supported browser.
Tree construction. The speed of the identification is crucial, as
it can only run as long as the attacker’s website is open in the
victim’s browser. As a browser only allows a limited number of
parallel requests – mostly between six and ten – the time needed
to identify a device or application is directly related to the number
of required requests. To optimize that number, we use a decision
tree-based approach when identifying services.
To build an optimized tree, we identify service groups from the
corpus that share features, e.g., devices by the same manufacturer
that share a company logo. A service can be part of multiple service
groups as it can share common features with multiple services
that do not necessarily share the same features. Subsequently, we
construct the tree by creating nodes to distinguish between services
designed to split the remaining services in half with each level of
the tree. Furthermore, the nodes are sorted by the frequency of
services to increase the probability of traversing the tree faster. The
resulting tree contains all services in the corpus. However, not all
can be identified uniquely – i.e., are represented by a leaf node in
the tree – because some service groups are indistinguishable by the
extracted features.
Figure 6 shows an example of the path used to identify TYPO3
version 4.7.6. Each node represents a feature and therefore, a file re-
quested by our test page. The first requested path is wp-includes/
js/crop/cropper.js, which does not exist for TYPO3 v. 4.7.6. The
second path is btn-sprite.gif, which exists. The numbers in each
node denote the number of possible device versions left. The last
requested path is SearchField.js, which exists and thus identifies
TYPO3 version 4.7.6.
Optimizing the Identification Process. We distinguish between
two identification processes: The full service identification and the
component identification. The full service identification has exactly
one result, while the component identification can lead to multi-
ple independent results as an application can consist of multiple
components (i.e., plugins). For the full service identification, the
number of requests necessary to identify a service is critical.
6 EVALUATION
As described in Section 3, the goal of the attacker is to identify
services on restricted networks. Based on this attacker model, we
evaluate our approach on different IoT devices as well as several
Content Management Systems (CMS). Furthermore, we analyzed
how the approach performs in identifying parts of a composite
service, in this case, WordPress plugins.
6.1 Identification of IoT Devices
Some IoT devices run web services, for example, for configuration
purposes. Furthermore, they regularly provide security-critical ser-
vices and often contain critical security vulnerabilities. Some IoT
devices are thus easy to attack once an attacker gains network
access, which is why they are often hidden behind perimeters.
As IoT devices regularly run their software stack on specific
hardware and setting up a local test lab with several IoT devices
is expensive and time-consuming. On the other hand, using pro-
ductive private networks as a testbed would raise ethical concerns.
Performing the JavaScript-based identification in the wild means
forcing peoples’ browsers to scan their private networks to identify
available devices. This scan could lead to problems inside their net-
work and may reveal sensitive information about the user’s local
network to us without the user’s knowledge or consent.
We thus decided to use the well-known public database and
search engine Shodan3 to automatically create testbeds of publicly
accessible IoT devices.
The service identification and evaluation process itself has to
take place inside a browser. To verify the process against a large
testbed would require us to initiate every test run in a browser by
hand. Instead, we automated the process by extending the Selenium-
based [30] test suite described in Section 5.2 to perform test runs and
return the results. We split our evaluation process for IoT devices
into two phases.
Phase 1: The first evaluation step is intended to analyze if a
feature vector generated using a crawler matches the specific device
when the resulting vector is used with JavaScript. To achieve this
evaluation, we crawled embedded devices from Shodan, resulting
in a data set of 250 unique feature vectors. These devices were
picked from search results for different Vendors and device types.
For gathering the data set, we used search queries from the format
" 200 ok port:’80, 8080, 8081’". After that, we
aggregate these features into feature vectors. The resulting tree
consists of 250 feature vectors.
Our automated Selenium-based test suite then uses this tree
to identify every device that has a feature vector in the database.
We were able to recognize 62% (154 devices) of the 250 devices.
Additionally, 24% (61 devices) were correctly classified, but their
3https://www.shodan.io/
Figure 6: A path in the decision tree that leads to TYPO3 version 4.7.6. The numbers denote the amount of possible versions
left and the path is the resource to be requested to move forward in the decision tree.
(a) Results of Phase 1
(b) Results of Phase 2
Match
Unique
Multiple
No
Total
# Devices
154
61
35
250
Percentage
62%
24%
14%
100%
Match
Unique
Multiple
Incorrect
Total
# Devices
31
7
4
42
Percentage
74%
17%
9%
100%
feature vectors matched multiple devices. Multiple matches happen
when the publicly accessible files of multiple devices are identical –
for example, two device models of the same manufacturer that share
the same firmware. 14% (35 devices) of the 250 devices could not
be classified; these include 17 devices for which the crawler could
identify no media files, and we thus could not create a working
feature vector.
Phase 2: The second phase evaluates if a feature vector generated
from a device recognizes other devices of the same type running
the same software. To evaluate this, we queried Shodan for devices
serving the same software version and verified them manually to
get at least eight devices of the same device type and firmware
version as a ground truth.
Based on that, we used one of those devices to build a feature
vector. The remaining devices were saved to the database as testing
devices. This enables us to apply the identification process to the
remaining devices using our Selenium-based test suite and report
the results back to the database. Overall, we tested 42 devices out
of which 31 were correctly classified, four vectors failed, and seven
returned correct results but matched multiple devices.
6.2 Identification of Content Management
Systems
To evaluate our techniques against Content Management Systems,
we chose four major systems: WordPress, Drupal, Joomla, and
TYPO3. The resulting data set contains 18 major, 219 minor, and
950 patch level versions.
The results of this evaluation are described in Table 3. Using our
techniques, we were able to extract distinguishable feature vectors
for 100% of the major and minor versions of WordPress, Joomla,
and TYPO3. The identification rate for minor versions of Drupal
only amounts to 12/139 (9%). This is caused by the fact that Drupal
does not have patch level versions for Drupal 5 to 7. For Drupal 4
and 8, which are using patch level versions, the detection rate for
minor version is 11/15 (73%).
Usually, changes in major and minor versions of CMSs results
in different feature vectors, but between patch level versions, there
are no or only small changes to media files. Due to this fact, we
could only distinguish 122 of 950 (13%) precise patch level versions,
because many of these patch level versions are indistinguishable.
We, therefore, also measured the average cluster size of the results,
950: wp-includes/js/crop/cropper.js646: typo3/sysext/t3skin/extjs/images/button/btn-sprite.gifû223: typo3/sysext/[…]00000227E36E5862.png122: typo3/contrib/codemirror/[…]/js/parsescheme.js67: typo3/sysext/cms/tslib/media/bullets/dots1.gif39: typo3/sysext/[…]/structure/element_docheader.css20: typo3/sysext/[…]/module_help_about_module.css5: typo3/sysext/[…]/sprites/t3skin.css1: t3lib/js/[…]SearchField.js (TYPO3 v. 4.7.6)304…423…101…55…28…19…15…4üûüûüûüûüûüûüûüi.e., the amount of patch level versions that have an identical fea-
ture vector. On average, the cluster size is 3.5, which means that
CORSICA can narrow down the set of possible patch level versions
of a target down to 3 or 4.
As mentioned in Section 5.3, we optimized the identification
process using a tree-based approach. This enables us to perform the
identification within in best case five and worst-case 14 requests.
The average request count is ten requests. To measure how our
techniques perform in real-world like scenarios, we performed the
identification process on installed systems with the feature vectors
learned before. Tools for identifying CMSs and their versions on
the Internet already exist [12, 15]. Unfortunately, most of the tools
available are not maintained, were build for already deprecated CMS
versions, and are thus unreliable. Therefore, they do not provide
usable ground truth, making the evaluation of the identification
process against real-world systems difficult.
We decided to create a Docker-based WordPress test lab because
WordPress provides well documented official docker images. This
setup contains 46 different versions available on Docker Hub4.
CORSICA can distinguish all major and minor versions of the 46
tested WordPress versions. Additionally, seven versions (15%) can
be identified down to its patch level version.
6.3 Identification of Vulnerable WordPress
Plugins
While searching for vulnerable targets on a network behind a se-
curity perimeter, knowledge about potentially vulnerable plugins
running on a specific service is valuable for the attacker. The most
widely used plugin-based system is Wordpress, which is why we
focused on that. We crawled the WPScan Vulnerability Database5
for plugin versions containing vulnerabilities that are likely to be
exploitable using CSRF. We considered the vulnerability classes
Remote Code Execution (RCE), Cross-Site Scripting (XSS), and SQL
Injection to be CSRF exploitable.
In total, we came up with 600 vulnerable Wordpress plugins. We
also added the last version before the vulnerability was introduced,
all vulnerable versions, and the first non-vulnerable version of a
plugin to our corpus, resulting in 1,814 different plugin versions.
After extracting the feature vectors, CORSICA was able to identify
the existence of 598 of the 600 plugins in a particular Wordpress
installation.