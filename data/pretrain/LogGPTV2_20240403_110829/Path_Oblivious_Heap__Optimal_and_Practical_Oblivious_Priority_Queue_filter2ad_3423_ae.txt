order, where the sampling is performed without replacement.
Assume that k is bounded by a ﬁxed polynomial in the secu-
rity parameter λ. An oblivious streaming k-sampler henceforth
denoted (O, addr) ← OReservoirk(1λ, S) is a streaming RAM
algorithm which consumes only O(k) memory and makes a
single pass over a stream S: the output O contains k sampled
items, and the notation addr denotes the ordered sequence of
access patterns of the algorithm over all |S| time steps.
We say that OReservoirk(1λ,·) obliviously simulates
F k
sample iff the following holds: there is a simulator Sim, such
that for any stream S whose length is a ﬁxed polynomial in
the security parameter λ, there is a negligible function ν(·)
such that the following holds:
sample(S), Sim(|S|)
(O, addr) where (O, addr) ← OReservoirk(1λ, S)
(cid:16)F k
(cid:17) ν(λ)≡
In the above, ν(λ)≡ means that
statistical distance at most ν(λ).
the two distributions have
2) Warmup: the Famous Reservoir Sampling Algorithm:
A beautiful and well-known Reservoir sampling algorithm by
Vitter [41], commonly referred to as Algorithm R, allows us
to realize a streaming sampler with only O(k) memory —
but in the prior streaming algorithms line of work was not
concerned about preserving privacy. Algorithm R basically
works as follows: for the ﬁrst k incoming items, store all
of them; when the m-th item arrives where m > k, with
probability 1/m, overwrite an already stored item, selected at
random, with the new item.
Now, the most straightforward implementation of Algorithm
R would select an already stored item at random and access
that position directly; but this leaks information of the type:
item i and item j cannot both be stored, if the algorithm
accessed the same memory location during step i and step
j. A na¨ıve way to make the algorithm oblivious is to make a
linear scan over the entire stored array of size k, and whenever
the index to replace is encountered, write the new element;
otherwise simply pretend to make a write (but write the old
contents back). To ensure full obliviousness, even if the newly
arrived item is to be thrown away, we still need to make a
Authorized licensed use limited to: Auckland University of Technology. Downloaded on November 03,2020 at 00:37:42 UTC from IEEE Xplore.  Restrictions apply. 
851
linear scan through the k-sized array. In this way, processing
every item in the stream requires O(k) time, and this can be
rather expensive, if say, k =
n for some large choice of n.
3) Our Construction: We can rely on an efﬁcient oblivious
priority queue PQ, preconﬁgured with the maximum capacity
k, to make Algorithm R oblivious.
√
• Whenever a new item arrives, choose a random label with
ω(log λ) bits and mark the item with this label. This
label will be used as the comparison key for the oblivious
priority queue PQ.
• For each m ≤ k, when the m-th item arrives, after choos-
ing a random label for the item, simply call PQ.Insert
and insert it into the priority queue.
• For any m > k, when the m-th item arrives, ﬁrst choose a
random label for the item; next, ﬂip a random coin ρ that
is 1 with probability 1/m and 0 otherwise; and ﬁnally,
do the following:
– if ρ = 1, call PQ.ExtractMin to remove the item
with the smallest label, and then call PQ.Insert to
add the new item;
• Finally, when
– else, call PQ.ExtractMin to return and remove the
item with the smallest label, and then call PQ.Insert
to add back the same minimum element just extracted.
call
the
PQ.ExtractMin algorithm k number of
times to
extract all k elements and write down each extracted
element one by one as the output.
stream S
ends,
the
If we adopt our Circuit-variant in the above algorithm with
the root bucket size set to |Broot| = Θ(1/δ), then on a word-
RAM with O(1) words of CPU cache, every item in the stream
can be processed in O(log k + log 1
Theorem 9 (Oblivious streaming sampler). Suppose that PQ
realizes an oblivious priority queue by Deﬁnition 1. Then, the
above algorithm obliviously simulates Fsample.
Proof. Deferred to Appendix C-B.
δ ) runtime.
VI. CONCRETE PERFORMANCE IN OUTSOURCING
A. Experimental Setup
Consider a cloud-outsourcing setting where a client with
small local cache stores a dataset, organized according to a
priority queue, on an untrusted server. In this section, we refer
to the RAM machine’s CPU as the client and the external
memory as the server. We built a Java simulator of our
algorithm which runs on a single machine which simulates
both the client and the untrusted storage.
Metric. We measure the number of bits transferred between
the client and the untrusted storage server. Earlier in Sec-
tion I-B, we have explained why this is the most suitable met-
ric for a cloud setting and adopted by almost all prior works on
ORAM and oblivious algorithms for cloud outsourcing [22],
[34], [35], [38], [45]. We compare the bandwidth blowup of
our oblivious algorithms relative to insecure baselines, that is,
the ordinary Binary Heap and Merge Sort.
that
Concrete instantiation and parameters. We use the Path-
variant
is more suitable for cloud outsourcing. As
mentioned in Remark 4, instead of choosing a bucket size
of 5 needed in the theoretical proof, we choose a bucket size
of 2 for our evaluation. We also adopt standard optimizations
that have been suggested and adopted in earlier works [18],
[39], [42]. On each insertion, we perform 1 read-path evic-
tion [39] and 1 deterministic reverse-lexicographical order
eviction which was initially suggested by Gentry et al. [18].
We apply the standard methodology for determining con-
crete security parameters for tree-based ORAMs [39], [42]
and oblivious data structures [44] — this approach is com-
monly adopted in this entire line of work since all known
theoretical tail bounds are not tight in the constants. Due to
the observation that time average is equal to ensemble average
for regenerative stochastic processes, we simulate a long run
containing more than 3 billion accesses, and plot the stash size
against log of the failure probability. Since by deﬁnition, we
cannot simulate “secure enough” values of λ in any reasonable
amount of time, we simulate for smaller ranges of λ and
extrapolate to our desired security parameter, i.e., a failure
probability of 2−80 per request. Our simulation results show
that the root bucket is smaller than 20 for achieving a per-
request failure probability of 2−80.
In our evaluation, N = 232, and each element has a 32-
bit key and varying payload sizes. We consider two natural
choices of client-cache sizes for our algorithm:
• Path caching. Here we assume that the client can cache a
single tree-path plus one additional subtree-min label (of a
sibling node). In total, the client caches at most 20+2 log N
entries and at most log N + 2 subtree-min labels.
• Path + treetop caching. In this case, we assume that in
addition to caching roughly one path (as noted above),
the client additionally caches between 6 to 7 smallest
levels. This setting is for comparison with Jafargholi et
al. [25]. As we explain below, since their algorithm requires
larger client-side storage to be practically efﬁcient, when
comparing with them we tune our algorithm to have a
matching client-side storage.
B. Evaluation Results
In the simulation, we assume that N elements have already
been inserted into the priority queue. We then consider a
sequence of requests that contains a sequence of requests con-
taining a repetition of Insert, ExtractMin, DecreaseKey
operations alternating in this fashion.
In our simulation, we consider the variant of Path Oblivious
Heap with stronger, type-hiding security (see Appendix D)
i.e., hiding not only the contents of the items inserted into
the priority queue, but also the request type as well. We
compare our algorithm with two prior works, the oblivious
priority queue construction by Wang et al. [44], and the more
recent construction by Jafargholi et al. [25]. We measure the
number of bits transferred between the client and server, and
we consider the bandwidth blowup relative to an insecure
baseline, that is, the popular binary heap.
Authorized licensed use limited to: Auckland University of Technology. Downloaded on November 03,2020 at 00:37:42 UTC from IEEE Xplore.  Restrictions apply. 
852
(a) Payload = 64 bits
(b) Payload = 128 bits
(c) Payload = 256 bits
Fig. 1: Concrete Performance for Cloud Outsourcing. The orange curve represents Path Oblivious Heap where the client
performs path-caching. The blue curves is when our Path Oblivious Heap performs path+treetop caching to match the client
storage of Jafargholi et al. [25]. The green and the maroon curves represent prior works by Jafargholi et al. [25] and Wang et
al. [44] respectively. In all ﬁgures, the comparison key is 32 bits and the per-request failure probability is at most 2−80.
Fig. 2: Number of round trips. Wang et al. [44]
incurs logarithmic roundtrips due to the recur-
sion in the algorithm; in comparison, our algo-
rithm incurs a single roundtrip per priority-queue
request, assuming that the client can store O(1)
tree-paths. Assuming small client storage, the
worst-case number of round trips for Jafargholi
et al. [25] is not in the visible region of the chart.
(a) Payload = 32 bits
(b) Payload = 64 bits
Fig. 3: Concrete Performance for MPC. The blue curve represents Path
Oblivious Heap (the circuit-variant) The green and the maroon curves
represent prior works by Jafargholi et al. [25] and Wang et al. [44]
respectively. In all ﬁgures, the comparison key is 32 bits and the per-
request failure probability is at most 2−80.
The work by Jafargholi et al.
[25] has good practi-
cal performance only when the client can store super-
logarithmically many entries. More concretely, their algorithm
needs to perform maintainance operations called PushUp
and PushDown, which operates on 3 buckets at a time. To
have good practical performance, the client needs to be able to
cache at least 3 buckets since otherwise oblivious sorting will
be necessarily within the triplet of buckets. The bucket size
is related to the security parameter. From the analysis in their
paper, and through a more precise simulation of their binomial
tail bound (which is tighter than the Chernoff approximation
stated in their paper), we determine that their bucket size needs
to be 536 to attain a per-request failure probability of 2−80.
To be fair, we conﬁgure all algorithms, including the insecure
binary heap, Wang et al. [44], as well as our algorithm to
employ roughly the same amount of client-side cache.
Path Oblivious Heap is conﬁgured with the same amount of
client storage as the prior work Jafargholi et al. [25], ﬁxing
the per-request failure probability to be 2−80 — through our
calculation, this means that our algorithm additionally caches
the smallest 6 to 7 levels of the binary tree.
For the stronger notion of type-hiding security, the results
show that our algorithm is only 3× to 7× more expensive than
the insecure binary heap depending on the ratio of the payload
size w.r.t. metadata size. Although not shown in the ﬁgure, we
note that if hiding the request type is not needed, our algorithm
will enjoy further speedup: speciﬁcally, ExtractMin and
Delete requests will be 2× faster since only one path is
visited during each ExtractMin whereas DecreaseKey
and Insert requests will visit two paths. For all data points
we considered, our algorithm outperforms prior works by 1-2
orders of magnitude.
Bandwidth cost. Figure 1 shows the bandwidth results. In
this ﬁgure,
the orange curve is when the client performs
path-caching. The blue curve corresponds to the case when
Among the oblivious algorithms evaluated, Wang et al. [44]
is a logarithmic factor slower than our algorithm and that
of Jafargholi et al. [25], although for most of the scenarios
Authorized licensed use limited to: Auckland University of Technology. Downloaded on November 03,2020 at 00:37:42 UTC from IEEE Xplore.  Restrictions apply. 
853
15202530log(N)020406080100120BW Blowup w.r.t. Bin-Heap15202530log(N)204060BW Blowup w.r.t. Bin-Heap15202530log(N)204060BW Blowup w.r.t. Bin-Heap1618202224262830log(N)2.55.07.510.012.515.0# RTTPath OHeapWang et al.51015202530log(N)0K2,000K4,000K6,000K# sym. encPath OHeapWang et al.Jafargholi et al.51015202530log(N)0K2,000K4,000K6,000K8,000K# sym. encPath OHeapWang et al.Jafargholi et al.TABLE I: Asymptotic circuit size for various schemes in the MPC setting. |k| denotes the key size, |v| denotes the payload
size, and N denotes the total capacity of the priority queue. The last column is the circuit size for typical parameters for ease
of understanding.
Scheme
Wang et al. [44]
Jafargholi et al. [25]
Path oblivious heap (circuit-variant)
Circuit size
O((log N · (|k| + log N ) + |v|)(log N + log 1
O((|k| + |v|) · (log N + log 1
δ ) · log log 1
δ )
O((|k| + |v| + log N ) · (log N + log 1
δ ))
δ ))
Circuit size for |k| = |v| = O(log N ),
log N ≤ log 1
δ
O(log N log 1
δ log log 1
δ )
O(log2 N log 1
δ )
O(log N log 1
δ )
we considered in the evaluation, Wang et al. [44] has better
concrete performance than Jafargholi et al. [25], especially
when either N is small or the payload is large.
Number of round trips. We also evaluate the number of
roundtrips incurred by our scheme in comparison with existing
schemes, and the result is depicted in Figure 2. Wang et al. [44]
incurs logarithmically many roundtrips due to the recursion in
the algorithm; in comparison, our algorithm incurs a single
roundtrip per priority-queue request, assuming that the client
can store O(1) tree-paths. Basically in our algorithm, the client
can fetch the at most 1 ReadNRm and at most 2 Evict paths
altogether, along with the relevant metadata from sibling nodes
necessary for the UpdateMin operation; now, the client locally
updates these paths and writes the result back to the server in
one round-trip.
We did not plot the number of roundtrips for Jafargholi
et al. [25] since every now and then, their scheme incur a
worst-case bandwidth cost of Θ(N ), and thus their number
of roundtrips can be almost linear in N in the worst case
assuming that the client can store only poly-logarithmically
many entries (the curve will not be in the visible area of our
plot if we had plotted it).