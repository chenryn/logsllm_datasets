auto-encoder transformation, that can remove the adversar-
ial perturbation from the input. However, such defenses are
shown to be vulnerable to attack algorithms that are partially
or completely aware of the defense mechanism [6, 41]. In [6],
the authors show that the input-transformation function can
be substituted with a differentiable approximation in the back-
ward pass in-order to craft adversarial examples that are robust
under the given input-transform. In [41], the authors craft ad-
versarial examples that are robust over a given distribution of
transformation functions, which guarantees robustness over
more than one type of transform.
Solely analyzing a defense against a non-adaptive adver-
sary gives us a false sense of security. Therefore, the authors
of [37] provided several guidelines to ensure completeness in
the evaluation of defenses to adversarial attacks. The authors
recommend using a threat model with an “inﬁnitely thorough”
adaptive adversary, who is capable of developing new optimal
attacks against the proposed defense. They recommend apply-
ing a diverse set of attacks to any proposed defense, with the
same mindset of a future adversary. However, such defense
guidelines have not been applied to the audio domain and
many of the proposed ASR defenses have not carried out thor-
ough evaluations against adaptive adversaries. In our work,
we follow these guidelines and evaluate our ASR defense
against the strongest non-adaptive and adaptive adversaries.
2.3 Defenses in the Audio Domain
In comparison to the image domain, only a handful of studies
have proposed defenses to adversarial attacks in the audio
domain. Prior work on defenses for speech recognition models
have focused on both audio pre-processing techniques [23,42]
and utilizing temporal dependency in speech signals [24] to
detect adversarial examples.
Yang et al. in [24] proposed a defense framework against
three attack methods targeting state-of-the-art ASR models
such as Kaldi and DeepSpeech. The proposed defense frame-
work checks if the transcription of the ﬁrst k-sized portion
of the audio waveform (t1) is similar to the ﬁrst k-sized tran-
scription of the complete audio waveform (t2). A sample is
identiﬁed as adversarial when the two transcriptions are dis-
similar, i.e., the Character Error Rate (CER) or Word Error
Rate (WER) between t1 and t2 is higher than a predeﬁned
threshold. The authors further study the effectiveness of their
defense in an adaptive attack scenario, where the attacker has
partial knowledge of the defense framework. In their strongest
adaptive attack scenario, they vary the portion kD used by the
defense and evaluate the cases where the adaptive attacker
uses a the same/different portion kA.
However, recent work [39] has re-evaluated temporal de-
pendency frameworks and demonstrated them to be ineffec-
tive in detecting adversarial perturbations in the audio domain.
The authors of [39] designed attacks that were able to fool
the proposed detector in [24] with 100% accuracy, and fur-
ther report that the adaptive evaluations conducted in [24] are
incomplete. In the adaptive attack designed by [39], the CTC
loss function used by the attacker incorporates different values
of kA and is therefore able to bypass the temporal dependency
detector with minimal added perturbation to audio.
Aside from proposing the temporal-dependency defense
for detection, the authors of [24] also study the effectiveness
of various input transformation functions in recovering the
original transcription from the adversarial counterpart. To this
end, they perform experiments with transformation functions
such as quantization, down-sampling, local smoothing and
auto-encoder reformation of signals. They report that these
methods are ineffective in recovering the correct transcription
of audio signals. In our work, we will evaluate some of these
transformations for the goal of detecting adversarial exam-
ples as opposed to recovering benign examples. However, we
report that for some attack types, most transformation based
defenses are able to recover the benign audio transcription
with low CER.
Rajaratnam et al. [23] also studied the use of pre-processing
techniques such as audio compression, band-pass ﬁltering, au-
dio panning and speech coding as a part of both isolated and
ensemble methods for detecting adversarial audio examples
generated by a single targeted attack [38]. While they report
high detection performance against the targeted adversarial
attack proposed by [38], their techniques were not evaluated
in an adaptive attack setting and therefore do not provide
security guarantees against a future adversary. Given the dif-
ﬁculty of performing defense evaluations, in our work, we
perform additional experiments with various input transforma-
tion functions to validate or refute the security claims made
in existing papers.
3 Methodology
3.1 Threat Model
Adversarial attacks in the audio domain can be classiﬁed
broadly into two categories: targeted and untargeted attacks.
In targeted attacks the goal of the adversary is to add a small
perturbation to an audio signal such that it causes the vic-
tim ASR to transcribe the audio to a given target phrase. In
untargeted attacks the goal is simply to cause signiﬁcant er-
ror in transcription of the audio signal so that the original
transcription cannot be deciphered.
The common goal across both targeted and untargeted at-
tack is to cause mis-transcription of the given speech signal
2276    30th USENIX Security Symposium
USENIX Association
Figure 3: WaveGuard Defense Framework: We ﬁrst processes the input audio x using an audio transformation function g to
obtain g(x). Then the ASR transcriptions or x and g(x) are compared. An input is classiﬁed as adversarial if the difference
between the transcriptions of x and g(x) exceeds a particular threshold.
while keeping the perturbation imperceptible. Therefore, we
deﬁne an audio adversarial example xadv as a perturbation
of an original speech signal x such that the Character Error
Rate (CER) between the transcriptions of the original and
adversarial examples from an ASR C is greater than some
threshold t. That is,
CER(C(x),C(xadv)) > t
(1)
and the distortion between xadv and x is constrained under a
distortion metric δ as follows:
δ(x,xadv)  t
(4)
where d is some distance metric between the two given texts
and t is a detection threshold. In our work we use the Charac-
ter Error Rate (CER) as the distance metric d. z An overview
of the defense is depicted in Figure 3. Note that unlike [24],
the goal using an input transformation g is not to recover
the original transcription of an adversarial example, but to
detect if an example is adversarial or benign by observing the
difference in the transcriptions of x and g(x).
In this work, we study various input transformation func-
tions g as candidates for our defense framework. We evaluate
our defense against four recent adversarial attacks [14,15,38]
on ASR systems. One of the main insights we draw from
our experiments is that in the non-adaptive attack setting,
most audio transformations can be effectively used in our
defense framework to accurately distinguish adversarial and
benign inputs. This result is consistent with the success of
input-transformation based defenses in the image domain.
USENIX Association
30th USENIX Security Symposium    2277
ASR BenignASR AdversarialWaveGuardWaveGuardTranscription:Transcription:WaveGuard“How is the wether?”“How is the weather?”“How is the weather?”“Browse to Evil dot com”CER = 0.0CER = 0.72ggFigure 4: Steps involved in the Mel extraction and inversion transform (Section 4.4). In the extraction step, the phase information
of the signal is discarded and the magnitude spectrogram is compressed to a Mel spectrogram using a linear transform. In the
inversion step, the waveform is estimated by ﬁrst estimating the magnitude spectrogram, followed by phase estimation and ﬁnally
an inverse STFT.
However, in order to use a defense reliably in practice, the
defense must be secure against an adaptive adversary who
has knowledge of the defense. For an adaptive attack setting,
we ﬁnd that certain input transformations are more robust to
attacks than others. Particularly, the transformations which
compress audio to perceptually informed representations can-
not be easily bypassed even when the attacker has complete
knowledge of the defense. This ﬁnding is in contrast to the im-
age domain where most input transformation based defenses
have been shown to be broken under robust or adaptive adver-
sarial attacks. We elaborate on our adaptive attack scenario
and the results in Section 7 and Section 8.
4 Input-transformation functions
We study the following audio transformations as candidates
for the input transformation function g:
4.1 Quantization-Dequantization
Several works in the image domain [21, 44, 45], have used
quantization based defenses to neutralize the effect of ad-
versarial perturbations. Since adversarial pertubations to au-
dio have small amplitudes, quantization can help reomve
added perturbations. In this study, we employ quantization-
dequantization in our defense framework, where each wave-
form sample is quantized to q bits and then reconstructed
back to ﬂoating point to produce the output approximation of
the original input data.
rates to ﬁnd an optimal range of sampling rates for which the
defense is effective.
4.3 Filtering
Filtering is commonly applied for noise cancellation appli-
cations such as removing background noise from a speech
signal. It is intuitive to study the effect of ﬁltering in order to
remove adversarial noise from a speech signal. In this work,
we use low-shelf and high-shelf ﬁlters to clean a given sig-
nal. Low-shelf and high-shelf ﬁlters are softer versions of
high-pass and low-pass ﬁlters respectively. That is, instead
of completely removing frequencies above or below some
thresholds, shelf ﬁlters boost or reduce their amplitude. For
noise removal, we use a low-shelf ﬁlter to reduce the ampli-
tude of frequencies below a threshold and a high-shelf ﬁlter
to reduce the amplitude of frequencies above a threshold.
In our experiments we ﬁrst compute the spectral centroid of
the audio waveform: Each frame of a magnitude spectrogram
is normalized and treated as a distribution over frequency
bins, from which the mean (centroid) is extracted per frame.
We then compute the median centroid frequency (C) over all
frames and set the high-shelf frequency threshold as 1.5×C
and low-shelf frequency threshold as 0.1×C. We then reduce
the amplitude of frequencies above and below the respective
thresholds using a negative gain parameter of -30.
4.4 Mel Spectrogram Extraction and Inver-
sion
4.2 Down-sampling and Up-sampling
Discarding samples from a waveform during down-sampling
could remove a signiﬁcant portion of the adversarial pertur-
bation, thereby disrupting an attack. To study this effect, we
down-sample the original waveform (16 kHz in our experi-
ments), to a lower sampling rate and then estimate the wave-
form at its original sampling rate using interpolation. We
perform this study for a number of different down-sampling
Mel spectrograms are popularly used as an intermediate audio
representation in both text-to-speech [46–48] and speech-to-
text [49, 50] systems. While reduction of the waveform to
a Mel spectrogram is a lossy compression, the Mel spectro-
gram is a perceptually informed representation that mostly
preserves the audio content necessary for speech recognition
systems. We use the following Mel spectrogram extraction
and inversion pipeline for disrupting adversarial perturbations
in our experiments:
2278    30th USENIX Security Symposium
USENIX Association
Estimated Magnitude SpectrogramEstimated PhaseMagnitude SpectrogramPhase InformationMagnitude SpectrogramMel SpectrogramSTFTFeature ExtractionInversionInv.STFTEstimated Magnitude SpectrogramDrop PhasePhase Est.Mel Comp.Mag Est.Extraction: We ﬁrst decompose waveforms into time and
frequency components using a Short-Time Fourier Trans-
form (STFT). Then, the phase information is discarded from
the complex STFT coefﬁcients leaving only the magnitude
spectrogram. The linearly-spaced frequency bins of the resul-
tant spectrogram are then compressed to fewer bins which
are equally-spaced on a logarithmic scale (usually the Mel
scale [51]). Finally, amplitudes of the resultant spectrogram
are made logarithmic to conform to human loudness percep-
tion, then optionally clipped and normalized to obtain the Mel
spectrogram.
Inversion: To invert the Mel spectrogram into a listenable
waveform, the inverse of each extraction step is applied in
reverse. First, logarithmic amplitudes are converted to lin-
ear ones. Then the magnitude spectrogram is estimated from
the Mel spectrogram using the approximate inverse of the
Mel transformation matrix. Next, the phase information is
estimated from the magnitude spectrogram using a heurisitc
algorithm such as Local Weighted Sum (LWS) [52] or Grifﬁn
Lim [53]. Finally, the inverse STFT is used to render audio
from the estimated magnitude spectrogram and phase infor-
mation.
We hypothesize that reconstructing audio from a percep-
tually informed representation can potentially remove the
adversarial perturbation while preserving the speech content
that is perceived by the human ear. While some speech recog-
nition systems also use Mel spectrogram features, we ﬁnd
that reconstructing audio from the compressed Mel spectro-
grams introduces enough distortion in the original waveform,
such that the ASR Mel features of the newly reconstructed
audio are different from the original audio. The distortion
in the reconstructed audio is introduced by the magnitude
estimation and phase estimation steps depicted in Figure 4.
In order to bypass a defense involving Mel extraction and
inversion, an adaptive attacker will need to craft a perturba-
tion that can be retained in the compressed Mel spectrogram
representation, making it challenging to keep the perturbation
imperceptible. In our adaptive attack experiments in Section 8
we demonstrate that even when the attacker uses a differen-
tiable implementation of the Mel extraction and inversion
pipeline, it cannot easily be bypassed without introducing a
clearly perceptible adversarial noise in the signal.
4.5 Linear Predictive Coding
Linear Predictive Coding (LPC) is a speech encoding tech-
nique that uses a source-ﬁlter model based on a mathematical