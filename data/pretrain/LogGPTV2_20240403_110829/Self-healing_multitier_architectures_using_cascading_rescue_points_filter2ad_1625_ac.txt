(cid:10)(cid:11)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)
(cid:7)(cid:2)(cid:8)(cid:9)(cid:5)(cid:6)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)
(cid:14)(cid:7)(cid:15)(cid:8)(cid:2)(cid:1)(cid:1)(cid:16)(cid:8)(cid:17)(cid:2)(cid:8)(cid:18)(cid:10)(cid:15)(cid:19)(cid:3)(cid:20)(cid:19)(cid:3)(cid:21)
(cid:22)(cid:15)(cid:7)(cid:23)(cid:24)(cid:25)(cid:16)(cid:23)(cid:2)(cid:1)(cid:1)(cid:24)(cid:21)(cid:2)
(cid:26)(cid:2)(cid:1)(cid:1)(cid:24)(cid:21)(cid:2)(cid:16)(cid:27)(cid:17)(cid:19)(cid:25)(cid:2)(cid:16)(cid:8)(cid:17)(cid:2)(cid:8)(cid:18)(cid:10)(cid:15)(cid:19)(cid:3)(cid:20)(cid:19)(cid:3)(cid:21)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)
(cid:5)(cid:4)(cid:6)(cid:7)(cid:8)(cid:4)(cid:9)(cid:10)(cid:11)(cid:12)(cid:2)(cid:3)
(cid:7)(cid:2)(cid:8)(cid:9)(cid:5)(cid:6)
(cid:13)(cid:11)(cid:14)(cid:14)(cid:15)(cid:16)(cid:7)(cid:17)
(cid:10)(cid:12)
(cid:1)(cid:19)(cid:21)(cid:3)(cid:24)(cid:25)
(cid:1)(cid:19)(cid:12)(cid:3)
(cid:5)(cid:4)(cid:6)(cid:7)(cid:8)(cid:4)(cid:9)(cid:10)(cid:11)(cid:12)(cid:2)(cid:3)
(cid:10)(cid:13)
Figure 6: Overlapping checkpoints. Processes p1
and p3, while both in RPs, try to cascade their RP
to the receiving process p2. p1 requests a rollback,
while p3 commits. The changes made by the latter
are lost.
tectures that have some innate hierarchy, like a three-tier or
client-server architecture. As it should be already obvious
from Fig. 5, receiving a checkpoint request from a process,
while already checkpointing due to the request of another
process or a RP, has no eﬀect. In this sense, our approach
is best-eﬀort and does not oﬀer strong guarantees on estab-
lishing a globally consistent state between all processes.
Figure 6 depicts such a case of overlapping checkpoints.
Two processes p1 and p3 enter RPs independently and both
transmit data to process p2. According to the CRP proto-
col, p2 will start the checkpointing, the moment it receives
the data from a process in a RP, in this case p1. When a pro-
cess is already checkpointing, it will ignore signals to start
checkpointing (e.g., the signal sent from p3 ). We should
emphasize that even though we do not handle overlapping
checkpoints, their occurrence is not catastrophic to CRPs.
However, we do have to rely on application logic to identify
and correct errors, which was the case before introducing
CRPs.
IMPLEMENTATION
4.
4.1 Self-contained Rescue Point Deployment
In previous work [28], we implemented REASSURE a tool
for deploying rescue points on binaries in an on-demand fash-
ion and without the need for source code. We built our tool
using Pin [18], a framework that enables the development
of tools that can at runtime augment or modify a binary’s
execution at the instruction level through an extensive API.
The target binary executes on top of Pin’s virtual machine
(VM), which essentially consists of a just-in-time (JIT) com-
piler that combines the binary’s original code with the code
inserted by the tool and places the produced code blocks
into a code cache, where the application executes from. Pin
facilitates the instrumentation of binaries by enabling de-
velopers to inspect and modify a program’s instructions, as
well as intercept system calls and signals. It is actively de-
veloped and supports multiple hardware architectures and
OSs. Pintools can be applied on binaries by either launch-
ing them through Pin or by attaching on already running
binaries. The latter behavior is highly desirable, as it allows
us to deploy RPs without interrupting an already executing
application. Our tool currently runs on x86 Linux systems,
however there are no signiﬁcant challenges in porting it to
other OSs and architectures supported by Pin.
Table 1: Example of rescue points covering known
bugs on popular applications (also see Tab. 2).
Application
MySQL v5.0.67
Apache v1.3.24
Function name/Return value
Item func set user var::update()/1
ap bread()/-1
RPs can be installed on any callable application function.
Table 1 lists RPs for a set of popular applications, including
the error codes that should be returned when an unexpected
error occurs. When a RP function is ﬁrst entered by an
application thread, we use Pin’s API to insert code that will
switch the thread into checkpointing mode and save CPU
state. Instructions that can be used to exit the RP, such as
the function return RET instruction on the x86 instruction-
set, are also instrumented to return the thread exiting the
rescue point to normal mode and to discard the previously
saved state.
When executing in checkpointing mode, our tool instru-
ments all memory write instructions and logs the overwrit-
ten memory contents into a dynamically expanding array,
the write log. Pin provides us with facilities so that only the
instructions being reached from within a RP are actually
instrumented in this fashion. This way individual threads
of an application can enter RPs and checkpoint individually
(assuming that no shared state is updated).
To identify errors our tool relies on signals. In particu-
lar, we intercept the following signals on Linux: SIGSEGV,
SIGILL, SIGABRT, SIGFPE, SIGPIPE.1 When a thread
executing within a RP receives one of these signals, we ini-
tiate the recovery process. The recovery process restores
memory contents and execution returns to the callee, also
returning the error code speciﬁed by the RP. In x86 archi-
tectures, function return values are usually returned in the
EAX register.
Concurrency.
Checkpointing is thread-speciﬁc, that is multiple threads
can enter a RP at the same time and each thread can roll
back independently. However, if a RP is processing data
shared by multiple threads, or if the error that causes the
application crash corrupts data used by other threads, since
they are all executing in the same address space, this type of
checkpointing can leave the process in an inconsistent state
after a roll back. We address such issues by introducing
blocking RPs that block other threads for their duration.
We can block threads by conditionally instrumenting every
block of instructions, so that when a certain ﬂag is asserted
execution is blocked. This always-on blocking approach is
appropriate for applications that expect a very high rate of
faults. Alternatively, we utilize signals to asynchronously
interrupt the remaining threads of a process and block them
until execution has left the RP. This on-demand blocking
mode generally incurs less overhead, since no additional in-
strumentation needs to be added for non-RP code.
1Note that other OSs have similar mechanisms to syn-
chronously notify applications of such errors. For example,
Windows OSs use exceptions.
383
(cid:20)(cid:10)(cid:10)(cid:21)(cid:11)(cid:7)(cid:7)(cid:3)(cid:4)(cid:11)(cid:5)(cid:9)(cid:22)(cid:3)(cid:23)(cid:19)(cid:10)(cid:5)(cid:24)(cid:5)(cid:11)(cid:10)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)
(cid:8)(cid:9)(cid:10)(cid:11)(cid:12)
(cid:13)(cid:14)(cid:15)(cid:16)(cid:17)
(cid:18)(cid:19)(cid:7)(cid:5)(cid:6)(cid:5)(cid:19)(cid:9)(cid:7)
(cid:20)(cid:10)(cid:10)(cid:21)(cid:11)(cid:7)(cid:7)
(cid:29)(cid:11)(cid:9)(cid:22)(cid:6)(cid:28)(cid:3)
(cid:19)(cid:24)(cid:3)(cid:30)(cid:21)(cid:5)(cid:6)(cid:11)
(cid:3)(cid:3)
(cid:3)(cid:3)(cid:3)
(cid:25)(cid:26)(cid:27)(cid:28)(cid:11)(cid:3)(cid:19)(cid:24)(cid:3)(cid:1)(cid:2)(cid:3)(cid:4)(cid:1)(cid:5)(cid:6)(cid:7)
(cid:23)(cid:19)(cid:10)(cid:5)(cid:24)(cid:5)(cid:11)(cid:10)(cid:3)(cid:26)(cid:10)(cid:10)(cid:21)(cid:11)(cid:7)(cid:7)(cid:11)(cid:7)
Figure 7: A small associative cache is used to quickly
check if a memory location has been already modi-
ﬁed. The cache is indexed using the lower 12-bits of
an address. Each slot stores the address and number
of bytes already modiﬁed.The original and cached
addresses, as well as the length of the write, are
compared to determine a cache hit. A cache miss
updates the appropriate slot.
4.2 Efﬁcient Thread Checkpointing
Checkpointing threads individually is beneﬁcial to soft-
ware self-healing.
If an error occurs while a thread is ex-
ecuting within a RP, we only need to roll back the state
of the thread that suﬀered the fault, while the remaining
threads can execute unhindered. However, using a writes
log to store the overwritten memory values does not scale
for certain applications and can lead to excessive memory
overheads. We address these issues by extending our tool in
three ways:
1. We introduce a small associative cache (shown in Fig. 7)
to quickly check if a memory location has been already
modiﬁed
2. We use the fork() system call to create a copy-on-write
image of the process’s address space and employ a ﬁlter
to mark the memory locations updated by the check-
pointing thread. Two types ﬁlters are currently sup-
ported: a statically allocated bitmap where each bit
corresponds to a four-byte memory area, and a circular
buﬀer that stores the modiﬁed addresses of memory
3. For the circular buﬀer, we utilize page protection and
intercept OS page-faults to identify when the buﬀer is
full, and write its contents to disk to avoid excessive
memory usage
The cache allows us to minimize the number of updates
performed in the writes log and the ﬁlter. This has the eﬀect
of reducing the amount of memory required for checkpoint-
ing, as addresses repeatedly written (e.g., stack variables)
are only updated once. We use the lower address bits to
index the cache to exploit locality in memory accesses.
Checkpointing using fork() is not a novel concept [25].
fork() is used to cheaply obtain a copy of the memory con-
tents of the entire process. Memory pages are initially shared
between the processes, while the OS creates copies of the
pages when they are modiﬁed. When a RP is entered, a
checkpointing process is forked. This newly forked process
uses a shared memory segment to communicate with the
original one and utilizes a semaphore for proper synchroniza-
tion. It initially sleeps waiting for events from its parent. If
the RP successfully exits (no error occurs), the checkpoint-
ing process is signaled to exit. Otherwise, it accesses the
(cid:8)(cid:5)(cid:9)(cid:10)(cid:4)(cid:11)
(cid:12)(cid:4)(cid:10)(cid:2)(cid:5)(cid:13)(cid:2)(cid:5)(cid:7)
(cid:12)(cid:3)(cid:14)(cid:5)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:2)
(cid:5)(cid:6)(cid:7)
(cid:13)(cid:15)(cid:4)(cid:16)(cid:16)(cid:17)(cid:1)(cid:17)(cid:18)(cid:19)(cid:20)(cid:20)(cid:20)
(cid:13)(cid:15)(cid:4)(cid:16)(cid:16)(cid:17)(cid:1)(cid:17)(cid:18)(cid:19)(cid:20)(cid:20)(cid:20)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:2)
(cid:17)
(cid:27)(cid:1)(cid:5)(cid:7)
(cid:1)(cid:12)(cid:3)(cid:13)(cid:5)
(cid:5)(cid:6)(cid:7)
(cid:21)(cid:15)(cid:20)(cid:20)(cid:5)(cid:4)(cid:17)(cid:22)(cid:3)(cid:1)(cid:17)
(cid:3)(cid:23)(cid:3)(cid:24)(cid:25)(cid:3)(cid:26)(cid:25)(cid:5)(cid:17)(cid:1)(cid:12)(cid:3)(cid:13)(cid:5)
(cid:21)(cid:15)(cid:20)(cid:20)(cid:5)(cid:4)(cid:17)
(cid:3)(cid:20)(cid:2)(cid:5)(cid:4)(cid:17)(cid:20)(cid:25)(cid:15)(cid:1)(cid:22)(cid:24)(cid:6)(cid:14)
Figure 8: A circular buﬀer can be used to store
the memory locations modiﬁed by a thread during
checkpointing. When full, a page protection fault is
generated. We capture the fault to ﬂush the buﬀer
contents to disk, and setup a new protected page.
The failed insertion can then simply resume to be
completed.
ﬁlter to obtain the addresses that were modiﬁed in the orig-
inal, and uses a pipe (used in UNIX systems for unidirec-
tional inter-process communication) to return the original
memory contents to the main process.
When using a bitmap ﬁlter, the overhead is low as the
bitmap is allocated once on RP entry and updating it is ef-
ﬁcient. Using one bit per four memory bytes means that
we could erroneously restore a byte that was not overwrit-
ten by the current thread. This could lead in memory cor-
ruption, if the particular byte was concurrently updated by
another thread. Most compilers tend to use four or even
eight byte alignment for variables, so in practice such cases
should rarely (if ever) occur. Note that we do not address
cases that the application itself erroneously implements syn-
chronization primitives, leading to inconsistent updates of
shared state.
Using the circular buﬀer for storing modiﬁed addresses
does not suﬀer from such limitations. Moreover, it uses less
memory, making it a good ﬁt for highly parallel processes
that may have many active RPs concurrently, and it sup-
ports 64-bit systems (64-bit address spaces are too large to
be covered by a statically allocated bitmap).
Our circular buﬀer implementation focuses on very fast
updates. This is achieved by ﬁrst aligning its size to a power
of two. This allows us to simply increase the cursor (i.e., the
index pointing to the ﬁrst available slot) after inserting data
in the buﬀer, and use a cheap bit masking operation to down
cast it to the size of the buﬀer. Second, we use page protec-
tion to signal buﬀer fullness instead of checking the number
of available bytes on each update. This is accomplished by
memory protecting the last page (usually 4KB) of the buﬀer.
When an update spills into the protected region, we ﬂush
the buﬀer to disk, remove the page’s protection, update the
buﬀer header, and protect the page that is currently last, as
depicted in Fig. 8.
Reverting System Call Effects.
Process memory is not only modiﬁed by write instructions,
but it can be also written by the kernel during a system call.
We extended our tool to intercept system calls and mark the
memory locations modiﬁed by them in the ﬁlter holding the
altered memory locations. For this purpose, we deﬁne a
static array for storing system call related metadata, spec-
384
(cid:4)(cid:13)(cid:23) (cid:27)(cid:28)(cid:6)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:2)(cid:6)