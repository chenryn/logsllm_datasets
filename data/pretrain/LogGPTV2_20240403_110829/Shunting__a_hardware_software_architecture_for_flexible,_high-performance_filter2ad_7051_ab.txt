The Shunt architecture aims to achieve several goals. First, we
want separation of mechanism and policy, with the Shunt provid-
ing only the former. Along these lines, while our implementation
couples the system with Bro, we intend the architecture to directly
support other types of analyzers, too. Second, we want to keep
the Shunt very simple: only examining headers, and with deter-
ministic memory behavior, enables an easy and efﬁcient hardware
implementation. Often, packet processing is limited by memory
accesses, so we imposed a budget of a limited number of accesses
per packet. Related to this, the architecture requires only a minimal
amount of buffering, which it achieves by always making imme-
diate decisions regarding the next-hop destination for an arriving
packet.
Finally, for the Shunt to realize signiﬁcant performance gains,
the policy used by the IPS must enable the Shunt to forward most
packets without involving the analyzer, and at high speed. Thus, for
trafﬁc which policy has determined does not require further analy-
sis, the Shunt must impose only a negligible forwarding delay.
3.2 The Shunt’s Tables
We accomplished these goals with a simple mechanism: header-
based table lookup, where the lookup is “incomplete” in that we
implement it quickly in hardware using a cache that may contain
only a subset of the table entries. The Shunt’s decision making
(Figure 1) is conceptually very simple. We use two tables, one
indexed by IP address and another indexed by the connection 5-
tuple, along with a ﬁxed table (the static ﬁlter) applied to certain
header ﬁelds such as TCP SYN/FIN/RST control ﬂags (Figure 3.2).
The device looks up each packet in the tables in parallel. If a
lookup ﬁnds an entry, the result includes an action (forward, drop,
or shunt) and a priority from 0 to 7. A priority encoder then selects
Host
Analysis Engine
Control To Update Caches
Shunt
Connection Cache
Address Cache
Header Rules
Drop
Packet Decision
Engine
Reinject
Shunt
Forward
Drop
Priority
Select
Figure 1: Shunting Main Architecture. The shunt examines the headers of received packets to determine the associated action:
forward, drop, or shunt to the Analysis Engine. The Analysis Engine directly updates the Shunt’s caches to control future processing,
and either drops analyzed packets for immediate intrusion prevention or reinjects them once vetted for safety.
the highest priority entry1 and performs the corresponding action
on the packet. If the device does not ﬁnd a match in any table, it
uses shunt as the implicit action.
The connection table has entries indexed with the usual 5-tuple
of source and destination IP addresses, source and destination ports,
and transport protocol (TCP or UDP). This is the most important
table for achieving high performance, as it enables ﬁne-grained,
per-connection decision-making on the part of the analyzer. Addi-
tionally, the connection table includes an optional record ﬁeld used
to implement forward-N, which we discuss later.
The IP address table has actions associated with it for both the
source and destination addresses. This table mainly serves to im-
plement static and dynamic ﬁrewall rules, such as blocking known
and newly detected attackers, or whitelisting high volume servers
or authorized vulnerability scanners.
Finally, we also have a ﬁxed header-ﬁlter table, which includes
default rules (such as diverting fragments and TCP control pack-
ets). We compile these static rules into the hardware conﬁguration,
with low priorities associated with them to make the entries easy to
override.
Other than the static ﬁlter, all table entries become populated
only upon request by the analyzer (including upon its startup). For
example, when the analyzer decides that it is safe to forward the
remainder of a connection without further inspection, it instructs
the Shunt to add a corresponding entry. This coupling between
the Shunt’s ﬁltering and the analyzer’s decision-making allows the
analyzer to vet requests on a connection-by-connection or host-by-
host basis, and, once vetted, efﬁciently skip the subsequent trafﬁc.
It similarly becomes easy for the analyzer to summarily block an
offending host, which not only blocks all trafﬁc from the offender,
but prevents the offender from loading the analyzer with trafﬁc,
enabling the IPS to protect itself against overload if it can identify
the source of the load.
The default-shunt nature produces a fail-safe device. Only if the
IPS instructs the Shunt that it deems a given ﬂow “safe” or “mali-
cious” will the Shunt process the ﬂow in an unconditional manner.
In addition, if the IPS cannot keep up with the pace of trafﬁc di-
verted through it, the trafﬁc does not escape analysis, but instead is
throttled back to the rate at which the IPS can vet it. While this can
1Conﬂicting entries with equal priorities indicate a policy inconsis-
tency. Architecturally, the hardware could signal such conditions.
In our implementation, the Shunt uses a ﬁxed set of internal pri-
orities to resolve ties, and it is the responsibility of the Analysis
Engine to not create such conﬂicts.
have a deleterious effect on network performance, it has the correct
safety properties in terms of “better safe than sorry.”
An important feature of the architecture is that the Shunt’s tables
are caches: an entry is not guaranteed to be persistent in the Shunt
if another entry is inserted. The shunt hashes2 each potential entry
to one or more locations in memory. When adding a new entry, this
may evict an old entry. This functionality allows the Shunt to per-
form a small, bounded number of memory accesses into a ﬁxed-size
memory. It is the responsibility of the Analysis Engine to respect
that the Shunt device is a cache and not a complete data structure.
Thus, packets designated for forwarding or dropping can still be
diverted to the Analysis Engine, requiring the Analysis Engine to
reinsert the corresponding table entry. Such evictions can however
create subtle problems of priority inversion, which we discuss in
Section 3.3.
The cache-like nature of these tables enables fast operation.
Rather than having to search through a possibly unbounded data
structure (e.g., a chain of hash buckets), the packet headers directly
index all entries that the Shunt needs to examine.
The IP and connection tables are both directional. Each direction
can have a different action and priority associated with it. Thus, for
example, the analyzer can monitor the inbound side of a connec-
tion (by setting a shunt action) while allowing the outbound half
unobstructed (with a forward action).
Finally, table entries also include a sample ﬁeld. If non-zero, this
ﬁeld speciﬁes an index into a table of probabilities. The device then
sends a copy of the packet to the analyzer with the given probability.
This functionality enables the analyzer to monitor a connection for
liveness and volume without having to receive all of its trafﬁc.
3.3 Interfacing to the Shunt
In our design, the Shunt device acts as an Ethernet card to the
host, transferring to the kernel any packets directed to the host and
processing the remainder according to the device’s tables. To con-
trol the Shunt, the Analysis Engine directly manipulates the cache
entries, which requires knowledge of the speciﬁc format and prop-
erties of the Shunt’s caches. Clearly, we could instead provide a
more abstract interface, managing cache deletions and insertions
based on higher-level requests. We have not done so yet because
so far we have only created a single hardware implementation (Sec-
tion 4), so in the subsequent discussion we assume that the Analysis
Engine manages the caches. Additionally, since we view the Shunt
as a device coupled to a stateful IDS system, having the Analysis
2Using a hash function chosen to resist attacker manipulation [6].
Packet
Src IP
Dst IP
Proto Flags SPort DPort
IP Cache
IP(cid:31)
Src
Actn
Addr
Dst
Actn
Connection Cache
Low(cid:31)
IP
High
IP
Proto Low(cid:31)
Port
Fixed Rules
High(cid:31)
Port
Low(cid:31)
Optn
High(cid:31)
Optn
Low(cid:31)
Actn
High(cid:31)
Actn
.
.
.
.
.
.
32b
8b
8b
32b
32b
8b
16b
16b
16b
16b
8b
8b
Action Entry Format:
Action (2b) Sample Rate (3b) Priority (3b)
Option Table
New(cid:31)
New(cid:31)
MAC
VLAN
New(cid:31)
Out
Seq
#
Seq(cid:31)
Actn
.
.
.
49b
17b
3b
32b
8b
S
e
l
e
c
t
P
r
i
o
r
i
t
y
(cid:31)
Selected (cid:31)
Action:(cid:31)
(Forward, (cid:31)
 Shunt,(cid:31)
 Sample, (cid:31)
 or Drop)
Figure 2: A detailed diagram of the caches used in the decision making process and the bits required for each ﬁeld. Separate actions
can be bound to each direction of a ﬂow; the option ﬁeld supports forward-N (Section 3.4) and destination routing (Section 4).
Engine directly manage the caches eliminates the need for a du-
plicate state-handling infrastructure to track a complete view of all
connections.
An important issue is that the Analysis Engine driver must man-
age conﬂicts in the tables to prevent priority inversion. If two dy-
namic rules apply to the same packet (such as a connection table
entry allowing a connection to a blocked known offender) with dif-
ferent priorities and actions, and the higher priority item needs to
be evicted, then it is up to the Analysis Engine to ensure that the
eviction does not lead to the Shunt now taking an incorrect forward
or drop action. (An incorrect shunt action is not a problem, since
this allows the Analysis Engine to correct the action.)
Thus, the Analysis Engine must either have direct control over
the Shunt’s caches (selecting which entry to evict when inserting a
new entry), or the Shunt must reliably notify the Analysis Engine of
the Shunt’s eviction decisions, allowing the Analysis Engine to then
also evict any lower priority entries as well. Finally, we note that we
can still apply the Shunting approach even if the Analysis Engine
does not have such control or notiﬁcation, providing it constraints
itself to never insert lower priority entries that can lead to such
conﬂicts, instead emulating the entries in software.
3.4 Reﬁning the forward Action
When we evaluated the architecture as described above, we
found a particularly important class of trafﬁc for which the basic
architecture lacks sufﬁcient expressive power to effectively ofﬂoad
the IPS. This occurs for protocols that send a series of transactions
over a single connection, for which the IPS would like to skip over
(potentially large) elements of each transaction, but cannot skip the
entire connection because doing so will entail missing control in-
formation associated with subsequent transactions.
This arises, for example, with persistent HTTP connections. If
the IPS determines that the URL in a given client request is allowed,
it would like to skip over having to process the item returned for it
by the server; but the next client URL might be problematic, in
which case at that point the IPS needs to analyze the server’s reply.
To enable such ofﬂoad, we need to extend the basic architecture
to offer ﬁner-grained control than per-connection, yet we also need
to do so in a manner that remains highly efﬁcient for the shunt de-
vice to process and economic in terms of the required table space.
Our extension, forward-N, is a reﬁnement to the forward function-
ality. The notion behind forward-N is “skip the next N bytes”
rather than “skip the rest of the connection.”
We need to take care, however, in specifying N. If it is simply
a byte count, then (i) for each new packet we will need to write
to the table to update the count by decrementing the number of
bytes of payload the packet carries, and, more importantly, (ii) the
accounting will be incorrect for out-of-sequence packets. These
latter can happen due to packet loss or reordering, race conditions
in installing the table entry, or deliberate attacker manipulation.
We therefore implement forward-N in terms of a TCP connec-
tion’s sequence space, rather than using a byte count. We augment
the per-connection shunt table with 32 bits of sequence number. For
packets with shunting decisions of forward-N, the device checks
whether the upper sequence number of the packet is less than3 the
table entry. If so, it forward’s the packet; otherwise, it shunt’s it.
The Analysis Engine then removes the entry when it determines it
no longer serves any ofﬂoad purpose; for example, when it sees an
acknowledgment for a sequence number higher than the cutoff.
To implement forward-N we include an additional (optional)
ﬁeld in connection table entries that speciﬁes the sequence number
limit. In our hardware implementation, we also use this optional
ﬁeld to specify alternate destinations, enabling the Shunt to act as
a packet routing device, not just a packet forwarding device; see §4
below.
For non-TCP trafﬁc, we lack an ordered sequence space to use
for a reliable cutoff, so for this functionality we would need to in-
stead use a countdown counter or develop an application protocol
3“Less than” in terms of using 32-bit sequence-space arithmetic,
i.e., a difference of ≤ 2 GB.
speciﬁc rule for (forward-N), which would signiﬁcantly complicate
the Shunt.
However, it is not clear that non-TCP protocols transfer sufﬁ-
ciently large, skippable items to merit this addition, rather than
beneﬁting from complete skipping (forward) or full analysis. Addi-
tionally, a non-TCP forward-N would require that the Shunt update
its tables on a per-packet basis. In the current design, the Shunt
hardware only reads the tables, eliminating a large class of race
conditions and other issues that might otherwise arise if it also per-
forms updates.
4. THE SHUNT HARDWARE
We implemented a prototype hardware design for Shunting using
the NetFPGA 2 platform [17], using as a starting point the NetF-
PGA reference implementation for a quad-port Ethernet NIC. The
NetFPGA 2 consists of four Gbps Ethernets connected to a Virtex 2
Pro 30 FPGA. Access to the card is via a standard PCI (33 MHz/32-
bit) bus. In addition, the platform provides two 2 MB SRAMs, one
of which can be used for arbitrary data structures.
Figure 3 shows the block diagram for the NetFPGA-based Shunt.
Our design uses a 32K-entry, two-location associative permutation
cache for IP addresses, and a 64K-entry, two-location associative
cache for connection rules. In an N-location associative cache, the
entry can reside in one of N different cache locations, in a manner
similar to Bloom ﬁlters [1], Bloom-ﬁlter based hash tables [25], or
skewed association caches [23].
For both the connection table and the address table, we use an 8-
bit rule ﬁeld to specify an action: forward, drop, shunt, or sample;
a 3-bit priority; and a 3-bit sampling rate. Additionally, we include
ﬁxed, low-priority rules for shunt’ing TCP SYN/FIN/RST packets
as well as IP fragments. As previously discussed, the hardware
follows the highest priority match, or, if it does not ﬁnd a match,
shunts the packet to the Analysis Engine for analysis. For the con-
nection table, we canonicalize the 5-tuple and provide a different
rule for each direction in the ﬂow.
For connection table entries, our design provides for an addi-
tional, optional, record ﬁeld. (The current hardware supports up to
32K such optional records.) This ﬁeld can specify a rule that is only
valid if the packet’s TCP sequence number is less than a prespeci-
ﬁed limit, to support sequence skipping (forward-N). We can also
instead use it to specify an alternate Ethernet interface, MAC ad-
dress, and VLAN tag, in order to allow the Shunt to reroute packets