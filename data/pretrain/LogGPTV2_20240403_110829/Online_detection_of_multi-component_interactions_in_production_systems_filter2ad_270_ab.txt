1e−01
1e+01
CAN2
GPS_COMP
GPS_VEL
EKF_POSE
GPS_POS
HEART
IMU
LASER2
LASER4
PLANNER_TRAJ
TEMP
LASER1
LASER3
PLANNER_INFO
TARGET
TOUAREG_ACT
Representation Size Fraction (log scale)
Fig. 13: When old data is allowed to be forgotten (decay), the
behavior of the system can be described efﬁciently using a small
number of eigensignals.
Fig. 15: Weights of Stanley’s ﬁrst three subsystems, with decay. The
subsystem involving the lasers (see Figure 14) has long since decayed
because the relevant anomalies happened early in the race.
Signal
1) Identifying Subsystems: During the Grand Challenge
race, Stanley experienced a critical bug that caused the vehicle
to swerve around nonexistent obstacles [38]. The Stanford
Racing Team eventually learned that the laser sensors were
sometimes misbehaving, but our analysis reveals a surprising
interaction: the ﬁrst subsystem is dominated by the laser sen-
sors and the planner software (see Figure 14). This interaction
was surprising because there was initially no apparent reason
why four physically separate laser sensors should experience
anomalies around the same time; it was also interesting that the
planner software was correlated with these anomalies more-
so than with the other sensors. As it turned out, there was
an uninstrumented, shared component of the lasers that was
causing this correlated behavior [25], [27] and whose existence
our method was able to infer. This insight was critical to
understanding the bug.
Administrators often ask, “What changed?” For example,
does the interaction between Stanley’s lasers and planner soft-
ware persist throughout the log, or is it transient? The output
of our analysis in Figure 15, which only reﬂects behavior near
the end of the log, shows that the subsystem is transient.
Most of the anomalies in the lasers and planner software
occurred near the beginning of the race and are long-since
forgotten by the end. As a result, the ﬁrst subsystem is instead
described by signals like the heartbeat and temperature sensor
(which was especially anomalous near the end of the race
because of the increasing desert heat). We currently identify
temporal changes manually, but we could automate the process
by comparing the composition of subsystems identiﬁed by the
signal compression stage.
Subsystems can describe global behavior as well as local
behavior. Figure 16 shows the weights for Spirit’s ﬁrst sub-
system, whose representative is the aggregate signal of all
the compute nodes; this subsystem describes a system-wide
phenomenon (nodes exhibit more interesting behavior when
they are running jobs). This is an example of behavior an
administrator might choose to ﬁlter out of the anomaly signals.
Meanwhile, the weights for Spirit’s third subsystem, shown in
Figure 17, are concentrated in a catch-all logging signal, sig-
nals related to component sn111, and alert types R_HDA_NR
and R_HDA_STAT (which are hard drive-related problems
[26]). This subsystem conveniently describes a speciﬁc kind
of problem affecting a speciﬁc component, and knowing that
those two types of alerts tend to happen together can help
narrow down the list of potential causes.
2) Reﬁning Instrumentation: Subsystem weights elucidate
the extent to which sets of signals are redundant and which
signals contain valuable information. There is operational
value in reﬁning the set of signals to include only those
that give new information. The administrator of our SQL
cluster stated this need as follows: “One of the problems with
developing a set of metrics to measure how well a particular
service is doing is that it’s very easy to come up with an
overwhelming number of them. However, if one wants to
manage a service to metrics, one wants to have a reasonably
small number of metrics to look at.”
In addition to identifying redundant signals, subsystems can
draw attention to places where more instrumentation would
be helpful. After our analysis of the SQL cluster revealed that
slow queries were predictive of bad downstream behavior, the
administrator said, “I wish I had connection logs from other
possible query sources to the MySQL servers to see if any
of those would have uncovered a correlation [but] we don’t
save those in a useful fashion. This is pointing to some real
deﬁciencies in our MySQL logging.”
3) Representatives: When diagnosing problems in large
systems, it is helpful to be able to decompose the system
into pieces. Administrators currently do this using topological
i
t
h
g
e
W
4
0
.
0
2
0
.
0
0
0
.
0
0
1000
i
t
h
g
e
W
6
.
0
4
.
0
2
.
0
0
.
0
0
1000
2000
Signal
2000
Signal
l
e
u
a
V
6