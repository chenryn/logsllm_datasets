## ZFS fsync IOPS performance in FreeBSD  
### 作者                                                                                                                                                                                       
digoal                                                                                                                                                                                         
### 日期                                                                                                                                                                                                        
2014-06-26                                                                                                                                                                               
### 标签                                                                                                                                                                                     
PostgreSQL , Linux , ZFS                                                                                                                                                                                   
----                                                                                                                                                                                               
## 背景                      
我在上一篇文章讲了一下ZFS的性能优化.  
文章提到在Linux (CentOS 6.5 x64)中, ZFS的fsync调用性能不佳的问题, 完全不如ext4, 于是在同一台主机, 我安装了FreeBSD 10 x64. 使用同样的硬件测试一下fsync的性能.  
PostgreSQL的安装参考  
http://blog.163.com/digoal@126/blog/static/163877040201451181344545/  
http://blog.163.com/digoal@126/blog/static/163877040201451282121414/  
首先查看块设备, 这里使用12块4 TB的SATA盘.  
```  
# gpart list -a  
Geom name: mfid[1-12]  
```  
创建zpool,   
```  
# zpool create zp1 mfid1 mfid2 mfid3 mfid4 mfid5 mfid6 mfid7 mfid8 mfid9 mfid10 mfid11 mfid12  
# zpool get all zp1  
NAME  PROPERTY                       VALUE                          SOURCE  
zp1   size                           43.5T                          -  
zp1   capacity                       0%                             -  
zp1   altroot                        -                              default  
zp1   health                         ONLINE                         -  
zp1   guid                           8490038421326880416            default  
zp1   version                        -                              default  
zp1   bootfs                         -                              default  
zp1   delegation                     on                             default  
zp1   autoreplace                    off                            default  
zp1   cachefile                      -                              default  
zp1   failmode                       wait                           default  
zp1   listsnapshots                  off                            default  
zp1   autoexpand                     off                            default  
zp1   dedupditto                     0                              default  
zp1   dedupratio                     1.00x                          -  
zp1   free                           43.5T                          -  
zp1   allocated                      285K                           -  
zp1   readonly                       off                            -  
zp1   comment                        -                              default  
zp1   expandsize                     0                              -  
zp1   freeing                        0                              default  
zp1   feature@async_destroy          enabled                        local  
zp1   feature@empty_bpobj            active                         local  
zp1   feature@lz4_compress           enabled                        local  
zp1   feature@multi_vdev_crash_dump  enabled                        local  
```  
创建zfs  
```  
# zfs create -o mountpoint=/data01 -o atime=off zp1/data01  
# zfs get all zp1/data01  
NAME        PROPERTY              VALUE                  SOURCE  
zp1/data01  type                  filesystem             -  
zp1/data01  creation              Thu Jun 26 23:52 2014  -  
zp1/data01  used                  32K                    -  
zp1/data01  available             42.8T                  -  
zp1/data01  referenced            32K                    -  
zp1/data01  compressratio         1.00x                  -  
zp1/data01  mounted               yes                    -  
zp1/data01  quota                 none                   default  
zp1/data01  reservation           none                   default  
zp1/data01  recordsize            128K                   default  
zp1/data01  mountpoint            /data01                local  
zp1/data01  sharenfs              off                    default  
zp1/data01  checksum              on                     default  
zp1/data01  compression           off                    default  
zp1/data01  atime                 off                    local  
zp1/data01  devices               on                     default  
zp1/data01  exec                  on                     default  
zp1/data01  setuid                on                     default  
zp1/data01  readonly              off                    default  
zp1/data01  jailed                off                    default  
zp1/data01  snapdir               hidden                 default  
zp1/data01  aclmode               discard                default  
zp1/data01  aclinherit            restricted             default  
zp1/data01  canmount              on                     default  
zp1/data01  xattr                 off                    temporary  
zp1/data01  copies                1                      default  
zp1/data01  version               5                      -  
zp1/data01  utf8only              off                    -  
zp1/data01  normalization         none                   -  
zp1/data01  casesensitivity       sensitive              -  
zp1/data01  vscan                 off                    default  
zp1/data01  nbmand                off                    default  
zp1/data01  sharesmb              off                    default  
zp1/data01  refquota              none                   default  
zp1/data01  refreservation        none                   default  
zp1/data01  primarycache          all                    default  
zp1/data01  secondarycache        all                    default  
zp1/data01  usedbysnapshots       0                      -  
zp1/data01  usedbydataset         32K                    -  
zp1/data01  usedbychildren        0                      -  
zp1/data01  usedbyrefreservation  0                      -  
zp1/data01  logbias               latency                default  
zp1/data01  dedup                 off                    default  
zp1/data01  mlslabel                                     -  
zp1/data01  sync                  disabled               local  
zp1/data01  refcompressratio      1.00x                  -  
zp1/data01  written               32K                    -  
zp1/data01  logicalused           16K                    -  
zp1/data01  logicalreferenced     16K                    -  
```  
测试fsync, 相比Linux有很大的提升, 基本达到了块设备的瓶颈.  
```  
# /opt/pgsql9.3.4/bin/pg_test_fsync -f /data01/1  
5 seconds per test  
O_DIRECT supported on this platform for open_datasync and open_sync.  
Compare file sync methods using one 8kB write:  
(in wal_sync_method preference order, except fdatasync  
is Linux's default)  
        open_datasync                                 n/a  
        fdatasync                                     n/a  
        fsync                            6676.001 ops/sec     150 usecs/op  
        fsync_writethrough                            n/a  
        open_sync                        6087.783 ops/sec     164 usecs/op  
Compare file sync methods using two 8kB writes:  
(in wal_sync_method preference order, except fdatasync  
is Linux's default)  
        open_datasync                                 n/a  
        fdatasync                                     n/a  
        fsync                            4750.841 ops/sec     210 usecs/op  
        fsync_writethrough                            n/a  
        open_sync                        3065.099 ops/sec     326 usecs/op  
Compare open_sync with different write sizes:  
(This is designed to compare the cost of writing 16kB  
in different write open_sync sizes.)  
         1 * 16kB open_sync write        4965.249 ops/sec     201 usecs/op  
         2 *  8kB open_sync writes       3039.074 ops/sec     329 usecs/op  
         4 *  4kB open_sync writes       1598.735 ops/sec     625 usecs/op  
         8 *  2kB open_sync writes       1326.517 ops/sec     754 usecs/op  
        16 *  1kB open_sync writes        620.992 ops/sec    1610 usecs/op  
Test if fsync on non-write file descriptor is honored:  
(If the times are similar, fsync() can sync data written  
on a different descriptor.)  
        write, fsync, close              5422.742 ops/sec     184 usecs/op  
        write, close, fsync              5552.278 ops/sec     180 usecs/op  
Non-Sync'ed 8kB writes:  
        write                           67460.621 ops/sec      15 usecs/op  
# zpool iostat -v 1  
               capacity     operations    bandwidth  
pool        alloc   free   read  write   read  write  
----------  -----  -----  -----  -----  -----  -----  
zp1          747M  43.5T      0  7.31K      0  39.1M  
  mfid1     62.8M  3.62T      0    638      0  3.27M  
  mfid2     61.9M  3.62T      0    615      0  3.23M  
  mfid3     62.8M  3.62T      0    615      0  3.23M  
  mfid4     62.0M  3.62T      0    615      0  3.23M  
  mfid5     62.9M  3.62T      0    616      0  3.24M  
  mfid6     62.0M  3.62T      0    616      0  3.24M  
  mfid7     62.9M  3.62T      0    620      0  3.24M  