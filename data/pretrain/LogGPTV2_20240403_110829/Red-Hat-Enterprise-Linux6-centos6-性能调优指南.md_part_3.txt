《性能调节指南》中模拟的步骤都由红帽工程师在实验室和现场进行全面测试。但红帽建议您在将您其应用
到产品服务器前在安全的测试环境中正确测试所有计划的配置。您还可以在开始调节系统前备份所有数据和
配置信息。
1.1. 读读者者
本文档适合两类读者：
系系统统/业业务务分分析析师师
本书模拟并解释了红帽企业版 Linux 6 中包含的性能，提供大量子系统如何为具体工作负载工作的
信息（包括默认和优化配置）。用来描述红帽企业版 Linux 6 性能的详细长度可帮助签字客户和销
售工程师了解这个平台在可以接受的水平提供消耗资源服务的适合程度。
《性能调节指南》还在可能的情况下提供各个特性具体文档的链接。读者可根据这些详细的性能特
点构成部署和优化红帽企业版 Linux 6 的高级策略。这样可让读者开发并评估架构提案。
这个注重性能的文档适合非常了解 Linux 子系统和企业级网络的读者。
系系统统管管理理员员
本文档中模拟的步骤审适合有 RHCE[1]技能（或者有相当于 3-5 年部署和管理 Linux 经验）的系统
管理员。《性能调节指南》主要为每个配置提供尽可能详细的效果，就是说论述所有可能的性能平
衡。
性能调节的基本技能不是了解如何分析和调节子系统。系统管理员应该了解如何为具体的目的平衡
和优化红帽企业版 Linux 6 系统。即意味着了解在尝试使用提高具体子系统性能设计的配置时会有
什么性能交换和代偿。
9
红帽企业版 Linux 6 性能调节指南
1.2. 横横向向可可扩扩展展性性
红帽对红帽企业版 Linux 性能的提高的重点是可扩展性。主要根据其如何影响负载谱（即独立网页服务器到
服务器大型机）中各个区段平台性能来评估那些提高性能的功能。
关注可延展性可让红帽企业版 Linux 保持其在不同类型的负载和目的通用性。同时这也意味着随着您的业务
增长和负载增加，重新配置您的服务器环境不会产生太高的费用（就人员小时工资而言），且更直观。
红帽在红帽企业版 Linux 中同时进行了横向可扩展性和纵向可扩展性改进，但横向可扩展性的应用更为普
遍。横向可扩展性的改进主要是使用多台标准计算机分布大工作负载以便改进性能和可靠性。
在典型服务器组中，这些标准计算机以 1U 机架安装服务器和刀片服务器的形式存在。每台标准计算机都可
小至简单的双插槽系统，尽管有些服务器组使用有更多插槽的大型系统。有些企业级网络是混合使用大型系
统和小系统。在此类情况下，大型系统使用高性能服务器（例如：数据库服务器），同时小型系统是专用的
应用程序服务器（例如：网页或者邮件服务器）。
这类可扩展性简化了您 IT 基础设施的增长：中级业务以及适当的负载只需要两个批萨盒服务器就可满足其
需要。随着公司雇佣更多的员工，扩展其业务，增加其销售等等，其 IT 要求在量和复杂性方面都会增张。
横向可扩展性可让 IT 部门只需部署附加机器即可，且（大多数）可使用同样的配置。
总之，横向可扩展性添加了一个提取层，简化了系统硬件管理。通过开发红帽企业版 Linux 平台使其横向增
大，增强 IT 服务的功能和性能，方法就是简单地添加新的容易配置的机器即可。
1.2.1. 并并行行计计算算
从红帽企业版 Linux 横向可扩展性中获益的用户不仅是因为可以简化系统硬件管理，还因为横向可扩展性是
符合硬件开发当前趋势的开发理念。
想象一下：最复杂的企业版程序同时要执行数千个任务，每个任务之间都有不同协调方法。虽然早期的计算
机使用单核处理器可以完成这些任务，但当今所有可用的虚拟处理器都有多个核。现代计算机有效地将多个
核放到单一插槽中，让单插槽桌面系统或者笔记本电脑也有多处理器系统。
从 2010 年开始，标准 Intel 和 AMD 处理器都有六核产品。此类处理器批萨盒或者刀片服务器中最为常见，
这样的服务器现在可以有多达 40 个核。这些低成本高性能系统可为大型机提供超大系统容量和性能。
要获得最佳性能及系统使用，则必须让每个核都保持忙碌。就是说必须运行 32 个独立任务以便充分利用 32
核刀片服务器。如果一个刀片组包含十组这样的 32 核刀片服务器，那么整个配置最少可同时处理 320 个任
务。如果这些任务都属于同一项任务，则必须对之进行协调。
红帽企业版 Llinux 的开发已经可以很好地适应硬件开发趋势，并确保商家可从中获取最大利益。第 1.3 节
“分布式系统” 中探讨了启用红帽企业版 Linux 横向可扩展性的技术细节。
1.3. 分分布布式式系系统统
为完全利用横向延展性，红帽企业版 Linux 使用分布式计算的很多组件。可将组成分布式计算的技术分为三
层：
通通讯讯
横向延展需要同时（平行）执行很多任务。因此这些任务必须有进程间通讯以便协调其工作。另
外，采用横向延展的平台应该可以跨多个系统共享任务。
存存储储
10
第 1 章 概述
本地磁盘存储不足以满足横向延展的要求。需要分布式或者共享存储，一个有可允许单一存储卷容
量渐变增长的提取层，另外还有额外的新存储硬件。
管管理理
分布式计算最重要的任务是管理层。这个管理层可协调所有软件和硬件组件，有效管理通讯、存储
以及共享资源的使用。
以下小节论述了每一层的详细技术。
1.3.1. 通通讯讯
通讯层可保证数据传输，它由两部分组成：
硬件
软件
多系统进行通讯最简单（也最迅速）的方法是共享内存。这样可以推导出类似内存读取/写入操作的用量。共
享内存的带宽高，低延迟，且常规内存读取/写入操作成本低。
以以太太网网
计算机之间最常用的通讯是使用以太网。目前系统默认提供 Gigabit Ethernet（GbE），且大多数服务器包
括 2-4 个 Gigabit 以太网 GbE 端口。GbE 提供良好的带宽和延迟性能。这是目前使用的大多数分布式系统
的基础。即使系统使用较快的网络硬件，一般也是使用 GbE 专门用于管理接口。
10GbE
Ten Gigabit Ethernet (10GbE) 是在高端甚至一些中端服务器中迅速得以广泛使用。10GbE 提供比 GbE 快
10 倍的带宽。其主要优点之一是使用现代多核处理器，它可保证通讯和计算之间的平衡。您可以将使用
GbE 的单核系统与使用 10GbE 的八核系统进行比较。以这种方法使用，10GbE 对保持整体系统性能有特
殊意义，并可以避免通讯瓶颈。
遗憾的是，10GbE 很贵。虽然 10GbE NIC 的成本已经下降，但互联（特别是光纤）的价格仍然很高，且
10GbE 网络交换机的价格极为昂贵。我们可以期待在一定时间内价格可以下降，但 10GbE 现在是在服务器
机房主干以及对性能至关重要的程序中使用最多的网卡。
Infiniband
Infiniband 提供比 10GbE 更高的性能。除在以太网中使用 TCP/IP 和 UDP 网络连接外，Infiniband 还支持
共享内存通讯。这就允许 Infiniband 通过远程直接内存访问 (RDMA) 在系统间使用。
使用 RDMA 可让 Infiniband 直接从系统中删除数据而无需负担 TCP/IP 或者插槽连接，继而减小延迟，这对
有些程序是很重要的。
Infiniband 最常用于高性能技术计算（HPTC）程序，此类程序要求使用高带宽、低延迟和低负担。很多超级
计算程序都得益于此，提高性能的最佳方式是使用 Infiniband，而不是快速处理器或者更多内存。
RoCCE
使用以太网的 RDMA (RoCCE) 通过 10GbE 基础设施实施 Infiniband 形式的通讯（包括 RDMA）。鉴于
10GbE 产品产量的增长带来的成本改善，我们有理由相信可能会在更大范围的系统和程序中使用更多的
RDMA 和 RoCCE。
红帽公司在红帽企业版 Linux 6 中全面支持这些通讯方法。
11
红帽企业版 Linux 6 性能调节指南
1.3.2. 存存储储
使用分布式计算的环境使用共享存储的多个实例。这可能代表以下两个含义之一：
多系统在单一位置保存数据
存储单元（例如卷）由多个存储应用组成
最熟悉的存储示例是挂载到系统中的本地磁盘驱动器。这对将所有程序都托管在一台主机中的 IT 操作很合
适。但由于基础设施可能包括数十个乃至数百个系统，管理如此多的本地存储磁盘将变得困难且复杂。
分布式存储添加了一层以便为业务规模减轻并实现自动存储硬件管理。多个系统共享少量存储实例可减少管
理员需要进行管理的设备数量。
将多个存储设备的存储容量强化到一个卷中对用户和管理员都有好处。此类分布式管理提供了存储池的提取
层：用户看到的是单一存储单元，管理员可通过添加更多硬件很方便地增大该单元。有些启用分布式存储的
技术也提供附加利益，比如故障切换以及多路径。
NFS
网络文件系统 (NFS) 可让多服务器或者用户通过 TCP 或者 UDP 挂载并使用远程存储的同一实例。NFS 一
般用来保存由多个程序共享的数据。它还便于对大量数据的海量存储。
SAN
存储区网络 (SANs) 使用光纤或者 iSCSI 协议提供对存储的远程访问。光纤基础设施（比如光纤主机总线适
配器、开关以及存储阵列）有高性能、高带宽和海量存储。SAN 根据处理分割存储，为系统升级提供可观的
灵活性。
SAN 的其他优点还有它们可为执行主要存储硬件管理任务提供管理环境。这些任务包括：
控制对存储的访问
管理海量数据
供应系统
备份和复制数据
提取快照
支持系统故障切换
保证数据完整性
迁移数据
GFS2
红帽全局文件系统 2（GFS2）提供一些特别定制的功能。GFS2 的基本功能是提供单一文件系统，其中包括
同时读/写访问，集群中跨多个成员的共享。即使说该集群的每个成员都可以看到 GFS2 文件系统中“磁盘
上”的完全相同的数据。
GFS2 可让所有系统同时访问该“磁盘”。为维护数据完整性，GFS2 使用分布式锁管理器（DLM），它在具体
位置一次只允许一个系统进行写入。
GFS2 对故障切换程序最合适，因为那些程序要求高存储容积。
有关 GFS2 的详情请参考《全局文件系统 2》。有关存储的常规信息请参考《存储管理指南》。您可以在
http://access.redhat.com/site/documentation/Red_Hat_Enterprise_Linux/ 找到这两本手册。
1.3.3. 聚聚合合网网络络
通过网络通讯一般使用以太网进行，使用专用光纤 SAN 环境的存储流量。通常有专用网络或者串行链路进
12
第 1 章 概述
行系统管理，且甚至可能使用心跳管理 [2]。结果是在多网络中通常使用单一服务器。
在每台服务器中提供多个连接费用高昂，且累赘，不容易管理。这样就增加了将所有连接整合到一台服务器
中的需求。使用以太网的光纤 (FCoE) 和 Internet SCSI (iSCSI) 可以满足这个需要。
FCoE