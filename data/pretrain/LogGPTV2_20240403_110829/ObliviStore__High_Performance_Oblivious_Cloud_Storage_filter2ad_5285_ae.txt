tore ORAM under different network latencies. The results
suggest that for rotational hard drives, the throughput of
ObliviStore is almost unaffected until about 1 second of
network latency. To obtain higher throughput beyond 1s
network latency, we can increase the level of parallelism
in our implementation, i.e., allowing more concurrent I/Os
– but this will lead to higher response time due to increased
queuing and I/O contention.
The response time of ObliviStore (single node with 7
HDDS) is consistently 140ms to 200ms plus the round-trip
network latency. The additional 140ms to 200ms is due to
disk seeks, request queuing, and I/O contention.
2) Results with Solid State Drives: Even though our
implementation makes a lot of progresses in reducing disk
seeks, there are still about 4 to 10 seeks per ORAM operation
on average (Figure 16). Solid state drives (SSDs) are known
to perform much better with seek intensive workloads, but
are also currently more expensive per GB of storage than
HDDs. To compare HDD and SSD storage, we repeated the
experiments of Section VII-A with 2 x 1TB solid state drives
on Amazon EC2 using a hi1.4xlarge VM instance.
The results are shown in Figures 11, 12, and 13. In
comparison, the throughput of ObliviStore with 2 SSDs of
storage is about 6 to 8 times faster than with 7 HDD. For
a typical 50ms network link, the response time with SSD
storage is about half of that with HDD storage.
HDDs or SSDs? Our experiments suggest that roughly 21 to
28 HDDs can achieve the same throughput as a single SSD.
Since the SSDs used in the experiment are about 20 times
more expensive than the HDDs, for a ﬁxed throughput, SSDs
are slightly cheaper than HDDs. On the other hand, HDDs
are about 20 times cheaper per unit of capacity. Under a
typical 50ms network latency, SSDs halve the response time
in comparison with HDDs.
B. Distributed Setting
We measure the scalability of ObliviStore in a distributed
setting. We consider a deployment scenario with a dis-
tributed TCB in the cloud. We assume that the TCB is
established through techniques such as Trusted Computing,
and that the TCB is running on a modern processor. How
to implement code attestation to establish such a distributed
TCB has been addressed in orthogonal work [26, 27, 32, 33],
and is not a focus of this evaluation.
For the distributed SSD experiments, each ORAM node
was a hi1.4xlarge Amazon EC instance with 2x1TB SSDs
of storage directly attached, and the load balancer ran on
a cc1.4xlarge instance. Although our instances have 60GB
of provisioned RAM, our implementation used far less
(under 3 GB per ORAM node, and under 3.5 GB for the
load balancer). The load balancer and the ORAM nodes
communicate through EC2’s internal network (under 5ms
network latency).
Figure 15 suggests that the throughput of ObliviStore
scales up linearly with the number of ORAM nodes, as
long as we do not saturate the network. The total bandwidth
overhead between the oblivious load balancer and all ORAM
nodes is 2X, and we never saturated the network in all
our experiments. For example, with 10 ORAM nodes and
4KB block size, the ORAM throughput is about 31.5 MB/s,
and the total bandwidth between the load balancer and
all ORAM nodes is about 63 MB/s. We also measured
that ObliviStore’s response time in the distributed setting is
about 60ms for 4KB blocks and is mostly unaffected by the
number of nodes (detailed results in the full version [39]).
The throughput of ObliviStore using HDD storage (also
tested on Amazon EC2) similarly scales linearly with the
number of nodes. Please refer to full version [39] for the
concrete results.
C. I/O Bottleneck Analysis
I/O overhead. ObliviStore incurs about 40X-50X I/O over-
head under parameters used in our experiments,
to
access one data block, on average 40-50 data blocks need
to be accessed. Though this seems high, under the amount
of ORAM capacity and private memory considered in this
paper,
the SSS scheme (what we implement) seems to
achieve the lowest I/O overhead (absolute value instead of
i.e.,
263
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:55:02 UTC from IEEE Xplore.  Restrictions apply. 
Figure 8: ObliviStore throughput with 7
HDDs. Experiment is performed on a single
ORAM node with the following parameters:
50ms network latency between the ORAM
node and the storage, 12ms average disk
seek latency, and 4KB block size.
Figure 9: ObliviStore response time with
7HDDs. Experiment is performed on a sin-
gle ORAM node with the following param-
eters: 50ms network latency between the
ORAM node and the storage, 12ms average
disk seek latency, and 4KB block size.
Figure 10: Effect of network latency on
throughput with 7HDDs. Experiment
is
performed on a single ORAM node with 7
HDDs, 12ms average disk seek latency, and
4KB block size.
Figure 11: ORAM throughput v.s. vari-
ous ORAM capacities with 2SSDs. The
experiments are performed in a single client,
single server setting with a simulated 50ms
network link, and 2 SSDs attached to the
server. Block size is 4KB.
Figure 12: ORAM response time v.s. var-
ious ORAM capacities with 2SSDs. The
experiments are performed in a single client,
single server setting with a simulated 50ms
network link, and 2 SSDs attached to the
server. Block size is 4KB.
Figure 13: Effect of network latency on
throughput with 2 SSDs. Experiment is
performed on a single ORAM node with 2
SSDs and 4KB block size.
Figure 14: Effect of network latency on
response time. Experiment is performed on
a single ORAM node with 7 HDDs (12ms
average seek latency), and again with 2
SSDs. Block size = 4KB. The ideal line
represents the roundtrip network latency.
Figure 15: Scalability of ObliviStore in
a distributed setting.
1 oblivious load
balancer, 2 SDDs attached to each ORAM
node. Throughput is the aggregate ORAM
throughput at the load balancer which dis-
tributes the load across all ORAM nodes.
Figure 16: Average number of
seeks
of ObliviStore per ORAM operation.
Includes all
I/O to storage (reads and
writes/shufﬂes). Experiment is performed on
a single ORAM node with 4KB block size.
asymptotics) among all known ORAM schemes. Therefore,
this is essentially the cost necessary to achieve the strong
security of ORAM.
In comparison, PrivateFS should have higher I/O overhead
– our I/O overhead is O(log N ) with a constant under
2, while theirs is O((log N )(log log N )2) [6]. This means
that when network bandwidth is the bottleneck, PrivateFS
achieves lower ORAM throughput than ObliviStore.
In our open source release, we will also implement the
matrix compression optimization technique [40], which will
further reduce the I/O overhead by a factor of 2.
Bottleneck analysis for various deployment scenarios.
The I/O overhead means that for every 1MB/s ORAM
throughput, we require about 40MB/s - 50MB/s throughput
264
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:55:02 UTC from IEEE Xplore.  Restrictions apply. 
on 1) AES computation, 2) total disk I/O bandwidth, and 3)
total network bandwidth between ORAM nodes and disks.
Depending on the deployment scenario, one of the above
three factors will hit bottleneck, which will become the main
constraint on the ORAM throughput.
For the hybrid cloud setting, our experiments show that
the network bandwidth between the private and public
cloud is likely to be the bottleneck. For example, assuming
a 1Gbps link between the private and public cloud, the
network will become a bottleneck with a single ORAM node
with 2 SSD drives – at the point of saturation, we would
achieve roughly 25Mbps (or 3MB/s) ORAM throughput.
For the trusted hardware in the cloud scenario, assume
that SSD drives are directly attached to ORAM nodes, and
that the distributed TCB is running on modern processors
(e.g., using Trusted Computing to establish a distributed
TCB in the cloud)1. In this case, the bottleneck is likely
to be disk I/O, since the total amount of data transferred
between the oblivious load balancer and the ORAM nodes is
relatively small, whereas the provisioned network bandwidth
between them is large. Speciﬁcally, under our setup where
each ORAM node has 2SSDs directly attached, suppose the
network bandwidth is Zbps shared amongst the oblivious
load balancer and all ORAM nodes, we can support roughly
20Z ORAM nodes before the network starts to be saturated.
The total ORAM throughput should be 3.2yMB/s, where
y < 20Z is the total number of ORAM nodes.
Our experiments suggest
the
bottleneck when ORAM client algorithms (including the
oblivious load balancer and the ORAM node algorithms)
are run on a modern processor.
that computation is not
D. Applications
Oblivious ﬁle system. Using NBD (short for Network Block
Device), we mounted the EXT4 File System on top of
our ORAM (a single host with a single SSD). On top of
this oblivious ﬁle system, we achieved average read/write
throughput of roughly 4MB/s. For metadata operations, it
took 2.1 − 3.5 seconds to to create and delete 10,000 ﬁles.
How to hide the number of accesses (e.g., depth of directory)
is our future work.
E. Comparison with Related Work
The most comparable work is PrivateFS (PD-ORAM) by
Williams et. al. [47]. Other than ObliviStore, PrivateFS is
the most efﬁcient ORAM implementation known-to-date.
PrivateFS also propose a novel algorithm for multiple clients
to share the same ORAM, while communicating amongst
each other using a log on the server side.
Lorch et. al. also implement ORAM in a distributed data
center setting [25]. They are the ﬁrst to actually implement
1For off-the-shelf secure co-processors such as IBM 4768, chip I/O and
computation will be the main bottlenecks, as demonstrated by Lorch et.
al. [25]. See Section VII-E for more details).
265
ORAM on off-the-shelf secure co-processors such as SLE 88
and IBM 4768, and therefore can achieve physical security
which off-the-shelf trusted computing technologies (e.g,
Intel TXT and AMD SVM) do not provide. On the other
hand, their implementation is constrained by the chip I/O,
computational power, and memory available in these secure
co-processors. Lorch et. al. performed small-scale exper-
iments with a handful of co-processors, and projected the
performance of their distributed ORAM through theoretic
calculations. Their work suggests that for ORAM to become
practical in large-scale data centers, we need more powerful
processors as part of the TCB. One way is to rely on Trusted
Computing – although this does not offer physical security, it
reduces attack surface by minimizing TCB such that formal
veriﬁcation may be possible. It is also conceivable that more
powerful secure co-processors will be manufactured in the
future [12]. Iliev and Smith also implemented an ORAM
algorithm to create a tiny TCB [19] with secure hardware.
Table IV compares our work against related works. As
mentioned earlier, since the work by Shroud [25] is less com-
parable, below we focus on comparing with PrivateFS [47].
The table suggests that on a single node with 7HDDs and
under the various parameters used in the experiments, 1)
ObliviStore achieves an order of magnitude higher through-
put than PrivateFS; and 2) ObliviStore lowers the response
time by 5X or more. Although we do not have access
to their implementation, we conjecture that
the speedup
is partly due to the reduced number of disk seeks in our
implementation (Figure 16, Section VII). Disk seeks are
the main bottleneck with HDDs as the storage medium,
since ORAM introduces a considerable amount of random
disk accesses. While both schemes have O(log N ) seeks in
theory [6], ObliviStore is speciﬁcally optimized to reduce
the number of seeks in practice. It is also likely that our im-
plementation beneﬁts from a ﬁner granularity of parallelism,
since we rely on asynchronous I/O calls and build our own
optimized event scheduler. In comparison, PrivateFS uses
multiple synchronous threads to achieve parallelism. Below
are some additional remarks about the comparison between
ObliviStore and PrivateFS:
• For ObliviStore, all HDD experiments consume under 30
MB/s (i.e., 240Mbps) network bandwidth (in many cases
much less) – hence we never saturate a 1Gbps network
link.
• For our HDD experiments, we had several personal
communications [6] with the authors of PrivateFS to
best replicate their experimental setup. Our disks have
similar performance benchmarking numbers as theirs
(approximately 12ms average seek time). We have also
chosen our network latency to be 50ms to replicate their
network characteristics. Both PrivateFS (PD-ORAM) and
ObliviStore run on similar modern CPUs. Our experi-
ments show that CPU is not the bottleneck – but disk I/O
is. The minor difference in the CPU is not crucial to the
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:55:02 UTC from IEEE Xplore.  Restrictions apply. 
Scheme
Processors
Shroud [25]
secure co-processors
PrivateFS (PD-ORAM) [47]
ObliviStore
modern CPUs
modern CPUs
Deployment
scenario
Trusted hard-
ware in cloud
hybrid cloud
both
Methodology
Bottleneck
experiments and the-
oretic projection
experiments
experiments
chip I/O, computation power, and
memory of secure co-processors
Disk I/O or Network I/O
Disk I/O or Network I/O
Table III: Comparison of experimental setup.