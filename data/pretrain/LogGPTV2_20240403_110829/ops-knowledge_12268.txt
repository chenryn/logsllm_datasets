User: The use case is as follows :
I have a script that runs a series of
non-python executables to reduce (pulsar) data. I right now use
subprocess.Popen(..., shell=True) and then the communicate function of subprocess to
capture the standard out and standard error from the non-python executables and the captured output I log using the python logging module. 
The problem is: just one core of the possible 8 get used now most of the time.
I want to spawn out multiple processes each doing a part of the data set in parallel and I want to keep track of progres. It is a script / program to analyze data from a low frequencey radio telescope (LOFAR).  The easier to install / manage and test the better.
I was about to build code to manage all this but im sure it must already exist in some easy library form.
Assistant: If I understand correctly what you are doing, I might suggest a slightly different approach.  Try establishing a single unit of work as a function and then layer on the parallel processing after that.  For example:

Wrap the current functionality (calling subprocess and capturing output) into a single function.  Have the function create a result object that can be returned; alternatively, the function could write out to files as you see fit.
Create an iterable (list, etc.) that contains an input for each chunk of data for step 1.
Create a multiprocessing Pool and then capitalize on its map() functionality to execute your function from step 1 for each of the items in step 2.  See the python multiprocessing docs for details.

You could also use a worker/Queue model.  The key, I think, is to encapsulate the current subprocess/output capture stuff into a function that does the work for a single chunk of data (whatever that is).  Layering on the parallel processing piece is then quite straightforward using any of several techniques, only a couple of which were described here.