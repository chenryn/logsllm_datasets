6.3 Incident Response
The third observation that we made based on our qualitative inter-
views is that operators suggest that a security incident due to a mis-
configuration makes management and operations personnel (tem-
porarily) more security sensitive. Asking operators directly, 81.9%
either agree or strongly agree that the discovery of a security mis-
configuration has made them more security conscious, with 33.0%
strongly agreeing to that statement (Table 4 line 4). Similarly, 71%
of respondents are convinced that their company’s security pos-
tureimprovedafterasecuritymisconfigurationwasdiscovered(Fig-
ure 8). However, 53% of respondents also report that an actual inci-
dent had an even higher impact on the organizations’ security pos-
ture than the discovery of misconfiguration that did not yet cause
an incident (Figure 8).
Several operators point out that security-relevant changes are
often planned but never implemented, were only active temporar-
ily (lasted for a while, but were then reverted), that management
was willing to improve but was unwilling to provide the necessary
resources to do so, or that management applied unreasonable secu-
rity metrics and measures. Some operators also highlight the lack
of security awareness and that the lack of distinct responsibilities
10
LackofknowledgeLackofexperienceHavingotherprioritiesBlunder,mishap,oblivionLackofconcernFearofaskingforhelpUsageofpoorresources0%25%50%75%100%Participants(%)SoleresponsibilityInsufﬁcientQ/ATimepressureWorkoverloadManualconﬁgurationInsufﬁcientcommunicationVague/noprocessesInsufﬁcientplanningPoorinternaldocumentationUnqualiﬁedleadershipFinancialdecisions0%25%50%75%100%UsageofdefaultsComplexityofthesystemLegacysupportPoorvendordocumentationBadusabilityMisleadingconventions0%25%50%75%100%BetterNoneWorseUnsureN/A050100150200ParticipantsHigherSameLessUnsureN/A050100150200ParticipantsInvestigating System Operators’ Perspective on Security Misconfigurations
CCS ’18, October 15–19, 2018, Toronto, ON, Canada
Statement
I feel responsible for keeping my operations secure.
I feel responsible for pointing out security issues to peers.
Blameless postmortems help to detect essential issues in corporate procedures.
No.
1
2
3
4 The discovery of a security misconfiguration made me more cautious regarding security.
5 The threat of bad press after a security incident is what companies fear most.
6 The general priority of security rises after a security incident has happened.
7 My direct supervisor understands what I’m actually doing.
8 My direct supervisor knows the amount of work I’m doing.
9
10
11 My company budgets for mistakes (such as misconfigurations and security incidents).
(cid:0)1
1
3
6
7
13
18
27
35
38
78
54
Table 4: Overview of responses to opinion-based questions from the survey
In my company we keep up with security standards.
I trust all the tools and equipment we’re using.
+2 +1
67
144
79
127
76
95
74
107
85
65
100
49
92
57
108
34
33
85
30
5
5
21
(cid:6)0
5
8
22
28
33
42
30
30
43
51
29
Plot (%)
(cid:0)2
1
0
2
0
2
2
11
10
13
51
58
/
3
4
20
5
23
10
4
4
9
6
54
Avg.
1.615
1.521
1.274
1.148
1.000
0.834
0.724
0.558
0.410
-0.651
-0.832
contributes further to the problem. One operator adds that compa-
nies fear bad press most, a sentiment that operators generally agree
with (Table 4, line 5). In general, it points toward an uncoordinated
pictureofincidentresponseandoverallbadsecuritypostureincom-
panies, with only half (53.39%) of respondents agreeing that their
companies’ security postures are sufficient (Table 4, line 9).
Finally, as a measure to correctly handle the aftermath of a mis-
configuration incident, 77.4% of our respondents agree that blame-
less postmortems are important (Table 4, line 3). However, there
are significant differences between freelancers (avg. 0.867), and em-
ployed system operators (full-time avg. 1.325, part-time avg. 1.400)
(p (cid:20) 0:001 in Pearson’s χ 2). Considering that most companies do
not budget for errors (Table 4, line 10), we conjecture that this is
due to freelancers serving as easy scapegoats in situations where
an organization pretends to perform a blameless ppostmortem
Summary. Overall, our quantitative survey aligns with the op-
erators’ suggestions from our qualitative interviews. However, ad-
ditional qualitative answers by the respondents indicate that even
changes that are implemented are not actually permanent, which
results in lapses in an organization’s security posture. Participants
also agree that blameless postmortems are fundamental for mean-
ingful, effective, and long-lasting incident response. Unfortunately,
for blameless postmortems to have impact, companies must em-
brace that (human) errors can and will happen, and budget for their
eventual occurrence, which currently is not the case. Finally, if post-
mortems are supposed to be blameless, while still appointing blame,
then the impact on moral and their long-term effectiveness might
be devastating (e.g., eroding trust in leadership).
7 RELATED WORK
We compare to relevant prior research in four categories: (i) stud-
ies of misconfigurations and mitigation efforts, (ii) usability stud-
ies concerned with technical systems creating or preventing errors,
(iii) studies focusing on personal behavioral causes for security is-
sues, and (iv) research in safety sciences.
Misconfigurations and Mitigation. Kuhrer et al. found that large-
scale notification campaigns can significantly reduce the number
of Internet-connected systems that are misconfigured and could be
exploited to act as traffic amplifiers [47], and Czyz et al. reported
on a similar effort to reduce the number of NTP amplifiers [48].
However, in 2016, Fiebig et al. showed that such an improvement
instances was only temporary for publicly exposed Redis and mem-
cached services [6]. Springall et al. investigated Internet-wide mis-
configured publicly accessible FTP servers in 2016, but they did not
report whether they notified operators or whether misconfigura-
tions were amended in a temporary or permanent way [49]. Prior
work also confirmed that misconfigurations facilitate other equally
problematic security issues: mobile applications leaking sensitive
and private data [50], applications becoming vulnerable to denial of
serviceattacks[10,11],websitesturningmaliciousandinfectingvis-
itors with malware [12], websites misusing its visitors to mine cryp-
tocurrency [51], and websites being modified to embarrass its oper-
ators via defacements [13, 14]. In fact, misconfigurations can even
lead to account compromises for some single sign-on protocols [52].
Finding misconfigurations in IPv6 recently lead to more work
toward making IPv6 scannable or enumerable [53–55]. Czyz et al.
found that the same systems expose insufficiently protected ser-
vices, i.e. host misconfigured services, more often via IPv6 than via
IPv4 [56]. Similar, Borgolte et al. found several thousand critical sys-
tems to be misconfigured in regard to their IPv6 security [55]. Inter-
estingly, Zhang et al. identified a correlation between the malicious-
ness and misconfigurations in networks [57]. Apart from their secu-
rity impact, misconfigurations have long been an important subject
in the networking community: Mahajan et al. conducted a large-
scale investigation of BGP misconfigurations in 2002 [58], while
Le et al. used data mining to detect router misconfiguration [59].
Streibelt et al. investigated issues around BGP communities [60].
Usability and Technical Mitigations. Xu et al. created a system
to detect system design that facilitates misconfiguration and they
analyzed the configuration syntax of various popular Internet ser-
vices [61]. Similarly, already in 2007, Haber et al. distilled design
guidelinesforsystemoperationstools[62].In2015,Xuetal.compre-
hensively analyzed how systems’ complexity leads to security mis-
configurations[63],andtheysurveytechnicalapproachestoreduce
security misconfigurations [64]. Keller et al. introduced ConfErr to
increase the resilience of systems to misconfigurations [65], and
Oliveira proposed designing systems that are mistake-aware [66].
Behavioral Studies. Fahletal.reportedontheirstudyamongweb-
masterswhorunexpiredTLScertificates[67].Contrarytoourstudy,
participants in the study by Fahl et al. mainly requested technolog-
ical changes to mitigate these misconfigurations. Krombholz et al.
investigated the challenges in configuring TLS correctly in a quali-
tative study, focusing on the usability aspects of the involved soft-
ware [9]. Acar et al. conduct a large-scale study on the sources that
11
CCS ’18, October 15–19, 2018, Toronto, ON, Canada
Constanze Dietrich, Katharina Krombholz, Kevin Borgolte, and Tobias Fiebig
Android developers utilize while writing mobile applications, and,
thus, might lead to security issues [31]. While orthogonal on first
sight, it relates to our observations on operators mentioning them-
selves that misconfigurations are created in part by using online re-
sourcesthatrecommendproblematicconfigurations.Ontherecruit-
ing side of behavioral studies, Acar et al. investigate the challenges
in recruiting a meaningful sample of IT professionals via GitHub for
studies to understand and improve their security decision-making
process [68]. To improve tools used by security operations centers,
Sundaramurthy et al. evaluate what inherent conflicts exist in the
objectives of security operations centers, how they manifest them-
selvesininefficiencies,andhowtheycanberesolvedeffectively[69].
Safety Science. Similar to the current state in computer security
andoperations,theinitialfocusinsafetysciencewasonthesocalled
“sharp-end,” that is individual human error while executing specific
actions [70]. However, it became apparent in the 1990s that an ap-
proach including organizational aspects and precursors is funda-
mental to effectively mitigate human error: “Fallibility is here to stay.
Organizational and local problems, in contrast, are both diagnosable
and manageable.” [70].
A common example illustrating the underlying paradigm shift is
the case of MS Herald of Free Enterprise, which capsized only mo-
ments after departure in Zeebrugge in 1987 because a dockhand for-
got to close a shot. The initial report selectively investigated person-
centric causes, appointing responsibility to the humans involved.
Praetorius et al. re-analyzed this incident 25 years later from a sys-
temic perspective leveraging functional resonance analysis method
(FRAM) [71]. They determined that the incident does not have a sin-
gle cause and that human error is not one of the major causes, but
actually the contrary: The human error is a symptom of functional
resonance in various functions of the operation of the ship. Simi-
larly, Schröder-Hinrichs et al. investigated large maritime incidents
from the R.M.S. Titanic to, most recently, the Costa Concordia, and
found the same underlying facilitating factors to be of cause [46].
8 LIMITATIONS
Due to our recruitment strategy (Section 5.2), our study may suf-
fer from (self-)selection bias. Our participants are likely to identify
with their work, as they are actively involved in the operators’ com-
munity. In turn, we may inadvertently exclude less active opera-
tors, which may have a different perception. However, as our re-
search also includes qualitative data such as anecdotes, reasoning
and opinions, we receive second hand information on the behav-
ior of operators who do not actively participate in the community
beyond their “nine-to-five” job (e.g., misconfigurations that opera-
tors encountered that they did not do themselves). Although, our
sample is comparatively small, our demographic distribution corre-
sponds to that of earlier studies, thus indicating a sufficiently strati-
fied sample [29]. As our analysis is based on self-reported data, it is
inherently biased by the participants’ perceptions.
9 CONCLUSION
In this paper, we conducted the first systematic study of human as-
pects of security misconfigurations from the operators’ perspective.
Wefindthatsecuritymisconfigurationsdonotnecessarilyleadtoin-
cidents: One third of respondents report that the misconfigurations
that they witnessed resulted in a security incident, even though all
12
ofthem could haveledtoasecurityincident.Theobservationthatal-
most all participants encountered security misconfigurations may
be due to a self-selection bias among operators. Specifically, how-
ever, given the prevalence of security incidents caused by miscon-
figurations [2, 3, 44], even considering such a self-selection bias, our
data highlights that there is an even larger set of incidents waiting
to happen, which could be considerably more disastrous compared
to past misconfigurations incidents.
Based on our analysis, we find that human error in system opera-
tions is driven by institutional, organizational, and personal factors.
Ultimately, to reduce the frequency and impact of security miscon-
figurations, we recommend the subsequent immediate action items
that our study highlighted as necessary and useful, which are (un-
fortunately)rarelyimplementedbyorganizationsalthoughtheyare
often technically sound and well-known:
Documentation.
The state of any system and all of its components must be
properly documented, so that anyone can fully understand
it. Documentation must be updated immediately upon any
changes, and it must be regularly verified for correctness.
Clear Responsibilities.
Organizations must ensure that there is a single responsible
department for the security of each device, which has suffi-
cient authority over the device to ensure its appropriate se-
curity posture. No person shall bear sole responsibility, but
responsibility must be shared among multiple people. Orga-
nizations,especiallyoutsideoftheITsector,mustalsoensure
that their responsible middle management is qualified.
Blameless Postmortems.
Afteranincidentoccurred,itsrootcausemustbeunderstood,
and actions must be taken to prevent a similar incident in the
future. While companies appear to embrace a general post-
mortem culture, our results indicate that this does not neces-
sarily mean that postmortems are blameless. Hence, the inci-
dent postmortem must be constructive and, most important,
notappointblame.Toencourageblamelesspostmortems,we
urge organizations to budget for failure and errors.
All manual changes to the system must be planned. Frequent
modificationsorconfigurationchangesshouldbeeliminated,
or automated and described in a process.
Processes and Procedures.
Automation.
Infrastructure and procedures shall be automated, to allow
operators to adopt (complex) procedures more easily. For ex-
ample, canary deployments and rollback plans can highlight
andtacklemisconfigurationsquickly.Theyhaveprohibitively
high cost if done manually, but they can often be automated,
thus highlighting the benefits of automation [72]. Nonethe-
less, these tools must be engineered to be reliable and should
not be a burden on operators. Software and protocols used
should be “secure by default” [33]. Considering the increas-
ing distrust in tools with operators’ experience, this area re-
quires more work to regain the operators’ trust into available
tools, as well as to increase the tools’ trustworthiness.
Regular exercises (“fire drills”) shall also be performed to un-
derstandthesecurityimplicationsofthecurrentsystem,how
Fire Drills.
Investigating System Operators’ Perspective on Security Misconfigurations
CCS ’18, October 15–19, 2018, Toronto, ON, Canada
it can be improved, and to train operators and management
inproperincidenthandlingprocedures.Ultimately,firedrills
can help managers from different backgrounds to better un-
derstand the implications of IT operations.
ACKNOWLEDGEMENTS
We thank the anonymous reviewers for their helpful suggestions to
improve the paper. We also thank Sascha Fahl for his valuable feed-
back. Wearegrateful to the RIPE AcademicInitiative(RACI)for sup-
porting this research project with a full scholarship to attend RIPE
76, as well as RIPE NCC and APNIC for helping us disseminate our
survey via their blogs. We also extend our gratitude to the German
Network Operators Group (DENOG), who supported our research
and provided valuable feedback from the early stages.
This material is partially funded by SBA Research. The compe-
tence center SBA Research (SBA-K1) is funded within the frame-
work of COMET – Competence Centers for Excellent Technologies
by the Austrian Ministry for Transport, Innovation and Technology
(BMVIT), the Austrian Federal Ministry for Digital and Economic
Affairs (BMDW), and the federal state of Vienna, managed by the