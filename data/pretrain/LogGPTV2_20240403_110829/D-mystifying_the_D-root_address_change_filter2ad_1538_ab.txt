Figure 1: In the ﬁrst two weeks, approximately 100,000 sources
switched to the new address every day.
In the following three
months, this rate decreased to approximately 20,000 sources per
day.
packets for an extended period. Instead, once a minute, we capture
200,000 packets, which, after the changeover, results in approxi-
mately ﬁve seconds of packet capture every minute (i.e., we collect
slightly more than 8.3% of all aggregate trafﬁc).
We sampled complete packets for the week of the trace (one day
prior and approximately six days after). In order to quantify steady
state behavior, we sampled complete packets at irregular intervals
(200,000 packets every 5–30 minutes, rather than once a minute) a
few weeks prior to the address switch. We veriﬁed that our more
complete data from the day before is representative of these earlier
data. Finally, we sampled complete packets a few months after the
address switch (April 29th to May 2nd) to measure trafﬁc at both
addresses.
Infrequent tcpdump packet captures may miss precisely when
any given host ﬁrst switched over. To measure this, we wrote a
tool switch that captures (1) the precise time the source ﬁrst con-
tacts the new IP address, as well as (2) approximately how many
queries it had issued to the old IP address since the changeover be-
fore switching over. This second dataset sacriﬁces full payloads
in order to get more complete information regarding changeover
times and behavior.
3.2 Overview of D-root’s changeover
Table 1 summarizes the trafﬁc at both addresses the week of the
address change. Recall that our trace captures only the ﬁrst 200,000
packets every minute; the query volumes in Table 1 are scaled up
assuming the rate at which the ﬁrst 200,000 packets are received is
maintained for the entire minute. The new address was announced
on DNS and operator mailing lists prior to the changeover and re-
ceived a negligible amount of probe/test trafﬁc the day before.
The line in Figure 1 plots the delay in resolvers discovering the
new address. The dots represent their corresponding query vol-
ume; the highest volume resolvers ﬁnd the new address relatively
quickly. Figure 2 shows the query volume to the old and new ad-
dresses during the week of the changeover. Query volume to both
the old and new addresses show the expected time-of-day variation.
Further, the thin spikes are (usually DNSSEC ampliﬁcation) attacks
on the root servers, which have been previously documented [6].
The adoption of the new address is rapid, and the total trafﬁc
volume to the new address exceeds that to the old within 24 hours.
110102103104105106107 0 20 40 60 80 1000.5M1M1.5M2M2.5M3M3.5M 0Query VolumeTotal Swapped SourcesTime Since Address Change (Days)Source Query VolumeSwapped SourcesFigure 2: Number of queries per second sent
to both the old and new address.
Figure 3: Query types at the old address,
around the changeover
time and several
months later.
the new ad-
Figure 4: Query types at
dress, around the changeover time and sev-
eral months later.
In contrast, it took nearly ﬁve days before the query volume to the
new J-root address exceeded the old [2]. Note also that the new
address is not initially attacked (the sharpest load spikes correspond
only to queries at the old address).
Figure 2 shows that there is an immediate surge in trafﬁc to
the new address, and the rate at which it receives trafﬁc far ex-
ceeds expected (and previously documented [2]) behavior. Further,
the total query volume to the D-root increases dramatically, and
that increase is sustained through the end of April (three months
later). The old address continues to receive queries months after
the changeover.
Figures 3 and 4 show the query volume to the old- and new server
broken down by query type. The new address initially receives
more SOA and SRV queries, but those queries are not sufﬁcient to
account for the increase in overall query volume. From Table 1,
we also note that the new address receives far more invalid queries
than the old address did. In contrast, the fraction of invalid queries
to the old address decreases after the changeover.
These observations raise several questions that drive the analysis
in the remainder of this paper:
(Q1) What causes the increase in overall trafﬁc volume after
the changeover day? Extra trafﬁc after a root server changeover
is to be expected, as resolvers will issue priming queries. But, with
compliant name servers, these should constitute a small fraction
of increased trafﬁc and should dissipate once resolvers discover the
root’s new address and update their hints ﬁles. However, prior work
has shown that there can be a prolonged increase in queries over
time [2, 10]. Our data reﬂect this anomaly, as well (as seen in
Figure 2). There are roughly 50% more queries shortly after the
changeover than there were the day before, and this discrepancy
continues for at least three months. While prior studies have ob-
served this phenomenon, we are unaware of any investigation into
the root cause, which is central to Q2.
(Q2) Why do servers continue to query the old address? It is not
surprising that name servers continue to query the old address even
months after the changeover date. Some very old BIND hints ﬁles
contained exorbitantly long TTLs for root servers (99,999,999, or
slightly more than three years), and given that root address changes
are so uncommon, it is reasonable to assume that otherwise stable
name server implementations may have faulty changeover logic.
However, it is surprising that the overall volume to the old address
has stabilized shortly—to approximately 50% of its original query
volume—after the changeover.
(Q3) Why are queries to the old IP address on average more
successful than those to the new address? Table 1 shows that the
queries to the old address result in fewer NXDOMAIN responses than
the queries to the new. To the best of our knowledge, we are the ﬁrst
to observe this phenomenon. Seemingly straightforward explana-
tions for why a name server would remain at the old address—a
misconﬁguration or a faulty implementation—do not appear to ex-
plain these increased success rates.
4. WHY DOES QUERY VOLUME INCREASE?
Intuitively, there are two (not necessarily mutually exclusive)
causes to an overall increase in volume: new resolvers could be-
gin to query the D-root who did not before, or queries from some
(possibly strict) subset of resolvers could increase. The number
of servers contacting D-root did not change signiﬁcantly in the 24
hours before and after the IP address change; in fact, they dropped
slightly (1,336,167 sources before, 1,187,801 after).
4.1 Excitables issue many more queries
Overall query volume increases because a (relatively) small num-
ber of sources issue many more queries (100× or more) to the new
address than they did to the old. We refer to these sources as ex-
citables.
For each host querying the D-root, Figure 5 plots the ratio of
queries per second one day before and one day after the address
change. Along with the expected symmetric clustering around y =
x (the 1× line), Figure 5 clearly identiﬁes the excitables: high vol-
ume hosts that increase their ratio by two or more orders of mag-
nitude. By themselves, the top-1000 or so excitables account for
∼58% of the increase in overall query volume. The top-1000 hosts
that increase their query rate by at least 2× account for the entire
increase in volume. Note that if high volume servers simply added
the new address to their set of 13 root addresses (and if they queried
each of the root servers uniformly) then their query volume would
increase by 2/14
1/13 , less than 86%. Therefore, a different process is
responsible for the increase in query volume.
4.2 Likely cause: Non-uniform server selec-
tion algorithms
Yu et al. [14] found that various versions of DNS resolvers (no-
tably BIND and PowerDNS) adjust the rate at which they query
different root servers based on RTTs, sometimes in nonintuitive
ways. BIND 9.7, for instance, preferentially queries root servers
with greater RTTs. In general, different versions of BIND do not
distribute their query loads uniformly over all addresses that can
Queries per SecondTime Since Address Change (Days)Old AddressNew AddressOverall 0 5000 10000 15000 20000 25000 30000 35000 40000 0 2 4 6  118Queries per SecondTime Since Address Change (Days)AAAAASOAPTRSRVOverall 0 5000 10000 15000 20000 25000 0 2 4 6  118Queries per SecondTime Since Address Change (Days)AAAAASOAPTRSRVOverall 0 5000 10000 15000 20000 25000 0 2 4 6  118Figure 5: Comparison of queries per second (QPS) for each source
in the 24 hours before and after the address change.
Figure 6: Locations and query volume of all excitables (re-
solvers whose query volume increased by at least 100× after the
changeover). Most of the excitables’ queries come from Europe.
serve a zone. Further, in our experiments, we see that relatively new
versions of BIND (9.2, 9.5) do not implement priming correctly,
and query both addresses, accounting for some of the increase in
query volume, particularly in the 2×–10× range.
BIND implementations do not explain the sudden increase by
10×, 100× or more. PowerDNS, which is popular in Europe, on
the other hand, exhibits a “spike distribution,” causing it to almost
exclusively contact the single lowest-RTT server [14]. Figure 6
shows the geographic distribution of servers whose query volumes
increased by at least 100×; 62% of these servers are located in
Europe, 20% in North America, 10% in Asia, 7% in Oceania, and
the rest in South America and Africa. D-root, however, is not the
lowest-RTT root server for European hosts; our measurements from
both academic and residential hosts in Germany indicated that D-
root (both old and new addresses) had an RTT one order of magni-
tude larger than the hosts’ closest root server.
However, we have observed from an analysis of PowerDNS ver-
sion 3.1 (the latest version on the D-root changeover date) that it
will provide a greater weight to a new server, even though it does
not have the lowest RTT. This can arise as certain corner cases may
cause PowerDNS to stop updating the RTT-based server selection
mechanism, causing PowerDNS to “stick” to a single server.
It
appears that the only way to exit this state is to restart the soft-
ware. This bug, coupled with the fact that PowerDNS sends all its
queries to the chosen server, could explain the increase in query
volume. Further, PowerDNS has increased in popularity relatively
recently, which would explain why prior address changes did not
witness such extreme change in query volume. Finally, Figure 6
shows that most of the highest volume excitables are hosted in Eu-
rope, where PowerDNS is used extensively; PowerDNS reported in
2012 that they power around 30% of all domains in Europe [12].
These observations point to resolvers using PowerDNS as being
the primary source of increased trafﬁc volume to D-root.
Unfortunately, PowerDNS is not responsive to the latest version
of the fpdns ﬁngerprinting tool, and we have not yet developed
a robust method for ﬁngerprinting PowerDNS resolvers.
(Reas-
suringly, the fpdns tool fails to identify the high volume hosts as
BIND.) We are in the process of trying to both identify PowerDNS
resolvers in the wild, and to reproduce the bug in older versions
of PowerDNS which would enable deﬁnitive attribution of the in-
crease in volume to PowerDNS.
5. WHO’S STILL USING THE OLD ADDRESS?
Technically, a DNS root IP address change does not require main-
taining connectivity on the old IP address if all resolvers handle an
individual failure by contacting any of the other 12 root servers.
However, older resolvers that do not perform priming queries (and
use outdated hints ﬁles) may completely break. A fundamental
question facing a DNS root address change is: At what point can
one responsibly shut down the old IP address? When all hosts
switch over? When some fraction of trafﬁc switches over? Or per-
haps when it appears that the only hosts still contacting the old
address are faulty?
The original expectations for D-root’s changeover were that (1) the