on out-of-distribution (OOD) samples. A classiﬁer could easily
misclassify OOD samples with high conﬁdence [30], [34].
Second, conﬁdence score cannot group on new samples, which
is inconvenient for the subsequent labeling process.
An opposite direction is to ignore any given labels and
apply unsupervised clustering [28], [20] or clustering ensem-
ble methods [67] to the training data. This direction could
avoid misleading information introduced by the low-quality
labels. However, as is shown later in Section §IV, without the
guidance of any labels, unsupervised methods are less effective
compared to those that can leverage the given labels.
A more promising direction is to apply a semi-supervised
learning methods [59], [6]. Semi-supervised learning could
leverage both the given labels and unlabeled data to learn a
more accurate clustering structure of the true classes. Unfor-
tunately, existing semi-supervised learning methods rely on a
strong assumption. That is, there are (at least) a few labeled
samples available in all the classes in the training set. In other
words, they do not assume missing classes or coarse-grained
labels in the training set. As is shown later in Section §IV,
their performances are signiﬁcantly jeopardized when there is
a violation of this assumption.
Finally, a related direction is (generalized) zero-shot learn-
ing methods (GZSL/ZSL), which can be used to identify pre-
viously unseen classes in the testing data [60]. These methods
treat the training data as the ﬁrst task and try to transfer the
learned model from the ﬁrst task to the second task (i.e., testing
data). GZSL/ZSL methods assume the testing data contains
unseen classes (that do not exist in the ﬁrst task). However,
to enable successful knowledge transfer, GZSL/ZSL methods
require well-labeled training samples in the ﬁrst task. Also,
3
Fig. 1. The overview of FARE. The colors on the dots indicate the given labels (top) and the ﬁnal clustering results (bottom). Dots of the same color have
the same label; the transparent dot represents an unlabeled sample. The numbers in Table A refer to the cluster indexes. The 0s and 1s in Table B represent the
neighborhood relationship. “-” stands for “not available”. “G. Labels” is short for “Given Labels”.
they need well-labeled side information (e.g., in image classi-
ﬁcation tasks, side information are shared and nameable visual
properties of objects). These requirements make GZSL/ZSL
methods not suitable to solve our problem: (1) we assume the
training labels are limited; (2) most security applications do
not have well-deﬁned notions for “side information”. We have
provided a brief supporting experiment in Appendix-B.
III. METHODOLOGY OF FARE
We design a system FARE to address the labeling problems
mentioned above. FARE is short for “Fain-grained Attack
Categorization through Representation Ensemble”. In the fol-
lowing, we ﬁrst explain the intuitions behind the system
design, followed by the technical details of each component.
Finally, we describe an unsupervised version of FARE.
A. Overview of System Design
In Figure 1, we provide an example to explain the work-
ﬂow. The input of FARE is a set of data samples (i.e., feature
vectors). Their labels are incomplete or contain errors. In this
example, we have 5 data samples. We use different colors to
distinguish the given labels of these samples ( x1 and x2 have
the same “red” label; x3 has the “blue” label; and x4 and x5
are not labeled yet).
The given labels of the inputs contain errors. More specif-
ically, the true labels of the 5 samples are shown on the
rightmost side of Figure 1. There should be 4 ground-truth
classes. Their true grouping is: {x1}, {x2}, {x3, x4}, {x5}.
Under this setting, x1 and x2 represent the coarse-grained
label problem; x4 and x5 represent the missing class problem.
After FARE processes the input data, our goal is to recover
the true grouping of the 5 samples. After that, human analysts
could inspect the clusters and assign the labels accordingly.
In this example, x1 and x2 are correctly split into two ﬁne-
grained classes (“red” and “yellow”). x3 and x4 are grouped
under the “blue” class. x5 then forms a new class “green”.
To achieve this goal above, we design FARE to process
the datasets with three key steps. In step , we mitigate the
uncertainties of the labels using an ensemble method. The idea
is to fuse the results of multiple unsupervised algorithms with
the given labels, to use the underlying data distribution to
consolidate the labels. In step , we transform the input space
into a more compressed hidden space to represent the data
distribution. The low-dimensional space allows us to perform
accurate data clustering. In step , we perform a K-means
clustering on the compressed data to identify the underline
cluster structures of the input data. Finally, we defer to human
analysts to assign labels to the output clusters. The clusters
produced by FARE could help human analysts identify missing
classes and the ﬁne-grained classes.
Augmenting Labels with Unsupervised Learning Results.
In step , we consider the given labels untrustworthy due
to the missing classes and the coarse-grained labels. To
consolidate the given labels, the only available information
source is the data samples themselves. As such, we propose
to use multiple unsupervised learning algorithms to extract
the underlying data distributions (manifolds) to mitigate the
uncertainty of the original labels.
More speciﬁcally, we fuse the labels from M different
sources. Among them, one source is the given labels, and
the other M − 1 sources are different unsupervised clustering
algorithms. The reason to use multiple clustering algorithms
is to reduce biases. Existing clustering algorithms intrinsically
make assumptions about the data distribution, and they work
well only when such an assumption is satisﬁed (e.g., K-means
assumes data are represented in Euclidean space). However,
in practice, we cannot validate these assumptions without
trial-and-error. For this reason, we apply multiple clustering
methods and fuse their results. In this way, the system is
less sensitive to the assumption made by certain clustering
method (validated in Section §IV). In addition, clustering
algorithms can be sensitive to hyper-parameters. To minimize
such inﬂuence, we apply each clustering algorithm multiple
times with different hyper-parameters and each setting has its
own row. After clustering, the results from each model2 are
shown in Table A in Figure 1.
To fuse the labels from the M sources, we need to ﬁnd a
uniform way to represent the clustering results. We solve this
problem by constructing a neighborhood relationship table,
which is Table B in Figure 1. In this table, each column
represents a pair of input samples; each row represents either
a clustering algorithm (row-2 to row-4) or the original given
labels (row-1). This table describes the pair-wise relationship
between all pairs of input samples.
Given a clustering algorithm, if a pair of input samples are
grouped into the same cluster, we set their relationship value as
1 (0 otherwise). Similarly, for the original given labels, if the
two samples share the same label, we set their relationship
value as 1. If they have different labels, we set the value
to 0. If at least one sample in the pair is unlabeled, we set
their relationship as “not available” (“-”). The neighborhood
relationship table makes it possible to fuse the results across
algorithms because we don’t need to align the speciﬁc clusters
to the speciﬁc labels. Instead, all the algorithms share the same
2For simplicity, in the example of Figure 1, we only apply each clustering
algorithm once with one set of hyper-parameters.
4
G. labels112--K-means32141DBSCAN21113DEC23221(1, 2)(1, 3)(1, 4)(1, 5)(2, 3)(2, 4)(2, 5)(3, 4)(3, 5)(4, 5)G. labels10--0-----K-means0000000010DBSCAN0000110100DEC0110000100!"!#!$!%Table A: Neighborhood models.Table B: Neighborhood relationships.{!'}{)'*}{+,}Input Trans. Net-"-#-$-%K-meansClustering{)'*}………ClustersHuman analyst!"!#!$!%!.!.-.-"-#-$-%-.Classes!"!#!$!%!.!"!#!$!%!.format that captures the pair-wised relationships of the input
data samples. For convenience, we refer to each row in this
table as a neighborhood model. In Figure 1, we have M = 4
neighborhood models.
ij }M
m=1, and the model weights {πm}M
To merge the results of M neighborhood models, we
introduce a hyper-parameter {πm}M
m=1 to represent the weight
of each model. A higher weight means the model is more
important. In Section §III-D, we describe how to calculate πm
via a validation set.
Input Transformation.
In the next step , we transform
the input samples and labels into low-dimensional vectors as
an accurate representation of the input data space. This low-
dimensional space allows us to cluster the input data into ﬁne-
grained clusters. Given the neighborhood relationship table
{ym
m=1, we train an input
transformation network to transforms an input xi to a hidden
representation hi. The network is trained to achieve two goals.
First, we want to transform the inputs into more separable
representations while preserving the pair-wise neighborhood
relationships. In other words, the hidden representation still
reﬂects the original data distribution, but should make these
samples easier to cluster. Second, the transformation function
will project the high dimensional input to a lower-dimensional
Euclidean space. As mentioned in Section §VII, traditional
clustering methods suffer from the curse of dimensionality. A
low-dimensional space enables more efﬁcient clustering.
Note that this input transformation network is different
from traditional unsupervised auto-encoder [46] that are used
to compress the original inputs. The key difference are two-
folds. First, auto-encoder is unsupervised, while FARE’s trans-
formation network utilizes both unlabeled and labeled samples.
Second, auto-encoder is trained for input reconstruction. FARE
is trained to contrast different samples to learn a more sepa-
rable space, which beneﬁts later clustering.
Final Clustering.
In the ﬁnal step , we simply apply the
K-means algorithm on the hidden representations to generate
more ﬁne-grained clusters. We choose K-means with the
following considerations. First, K-means works particularly
well if Euclidean distance. The input transformation in the
previous step has mapped inputs to a Euclidean space. Second,
other candidate algorithms such as DBSCAN are not suitable
because their assumptions do not match well with the hidden
representations or they do not rely on the notion of distance
(e.g., density-based algorithms) for clustering. The main task
in this step is to determine the ﬁnal number of clusters K. In
the following, we will discuss our method in detail.
B. Technical Details
algorithms and the given labels. We deﬁne a set of M
neighborhood models (denoted as M), and each model is
used to decide a set of pair-wise neighborhood relationships
of samples in X . As shown in Figure 1 Table A, one of the
neighborhood models is the given labels and the other M − 1
neighborhood models are the clustering algorithms.
For each neighborhood model in M, we then decide the
pair-wise neighborhood relationships for samples in X . Given
the neighborhood relationship
a pair of samples (xi, xj),
captured by the mth model is denoted as ym
ij . As mentioned
in Section §III-A, ym
ij = 1 if the two samples are clustered
into the same cluster (0 otherwise) by the mthmodel . For the
“given labels”, the same rule applies (but if either input is in
the unlabeled set XU , ym
ij is unavailable).
epm(cid:80)
m∈M epm .
To aggregate the neighborhood relationships from all the
models in M, we set weights πm on each neighborhood model.
To calculate this πm, we ﬁrst deﬁne a priori pm for each model.
This pm is also a hyper-parameter (conﬁguration details are
in Section §III-D). After deciding the value of {pm}, we then
calculate {πm} by normalizing {pm} using a softmax function:
πm =
Input Transformation Network. The input transformation
network aims to transform the input samples into a low-
dimensional hidden space to identify the underlying clusters.
Based on {ym
to learn a network
f to map any input sample x from X to h in a hidden
space. As discussed in Section §III-A, the hidden space should
(1) maintain an accurate representation of the neighborhood
relationships of the input samples, and (2) make it easier to
perform clustering.
ij } and {πm}, we want
To achieve these goals, we ﬁrst apply a metric learning
loss to train the transformation network. Metric learning [79],
[69] transforms the input samples into hidden representations
while keeping the sample distance (i.e., the relative distance
between pair-wise of samples) consistent with that
in the
input space. Mathematically, given a pair of samples xi, xj,
and neighborhood relationship yij, a typical pair-wise metric
learning loss has the following form [26]:
˜L(xi, xj) = yijd2
ij + (1 − yij)(α − dij)2
+ ,
(1)
where (·)+ is short for max(0,·) and dij is the distance of
the hidden representations of xi and xj. This loss function
ensures that the distance of xi and xj is minimized in the
latent space if they are neighbors (i.e., belonging to the same
cluster). Oppositely, we maximize their distance up to a radius
deﬁned by α > 0, such that dissimilar pairs contribute to the
loss function only when their distance is within this radius.
In this section, we present the technical details of each
component in FARE. We start by deﬁning key notations. Given
an input dataset X = {XY ,XU}, where XY corresponds to
the labeled samples set and XU denotes the set of unlabeled
samples. Within the dataset, each sample x ∈ Rp×1 is a p
dimensional vector. If the sample has a label, the label is
represented by an integer value, indicating the corresponding
sample’s class. We use {·} as an abbreviation for {·}M
Ensemble of Neighborhood Models.
In  of Figure 1, we
compute the ensemble of labels from multiple unsupervised
m=1.
With this metric learning loss, we learn the hidden repre-
sentation of the input samples such that inputs from the same
class have a smaller distance than those from different classes.
This makes the hidden representations from different classes
more separable. Another beneﬁt is that metric learning converts
the samples into representations in the Euclidean space, where
the Euclidean distance can be used as the distance function.
To be speciﬁc, we deﬁne the distance function of xi, xj in the
hidden space as follows:
dij = d(xi, xj) = (cid:107)hi − hj(cid:107)2 .
(2)
5
Here, hi = f (xi) is the hidden representation of xi. It should
be noted that we only need to deﬁne distance function in
the hidden space since the neighborhood relationships of the
original input samples have already been captured by yij.
We can integrate the multiple sets of neighborhood rela-
tionships into the loss function in Equation (1). To be speciﬁc,
ij },
given a sample pair xi, xj, their neighborhood relations {ym
and the model weights {πm}, the loss function of this sample
pair is deﬁned as follows:
L(xi, xj) =
˜L(xi, xj|m)
πmδm
ij
(cid:2)ym
πmδm
ij
ij + (1 − ym
ij )(α − dij)2
+
ij d2
(cid:88)
(cid:88)
m∈M
=
m∈M
(cid:3) .
(3)
(cid:88)
This loss function also handles the special cases when ym
ij is
“unavailable” for (incomplete) given labels. We introduce an
indicator δm
ij = 0 (and 1
otherwise).
ij is unavailable, we set δm
ij : if ym
The loss function in Equation (3) has the form of total
probability [52], where πm can be taken as the priori of
each neighborhood model. The ﬁnal loss can be calculated
by integrating the individual loss obtained from each set of
neighborhood relationships obtained from each neighborhood
model. In other words, this loss function only minimizes (or
maximizes) the distance between a pair of samples in the
hidden space when most of the neighborhood models agree that
they are neighbors (or non-neighbors). The loss over the entire