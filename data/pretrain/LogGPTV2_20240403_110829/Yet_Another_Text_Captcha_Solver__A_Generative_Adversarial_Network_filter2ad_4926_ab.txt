easily extended and adjusted to target other captcha schemes.
Our captcha synthesizer consists of two components, a generator
and a discriminator. The generator, G, is trained to produce outputs
that cannot be distinguished from real captchas by an adversarially
Captcha Synthesizerreal captchassynthetic captchasTrainingbase solverFine Tuningfine-tuned solverreal clean captchas134Preprocessing2Generator NetworkDiscriminatorNetworksynthetic captchasreal captchasclassification accuracyTerminate?Adjusting synthesizer parameterscaptcha synthesizerYesNoFigure 4: Our captcha generator model includes a image gen-
erator and a generator network. The image generator pro-
duces a captcha image at the word level, and the generator
network modifies the produced captcha image at the pixel
level to add security features.
trained discriminator, D, which is trained to do as well as possible
at detecting the synthetic captchas.
Captcha generator. As depicted in Figure 4, our captcha generator
model includes a captcha image generator which automatically
generates captcha images according to a given parameter setting
and captcha word, and a CNN model that modifies the generated
synthetic captcha at the pixel level. We provide the image generator
and the learning engine a large number of free fonts so that the
learning engine can learn which font best suits the target scheme.
The image generator takes in the security feature configuration
setting provided by a user and tries to find a set of configurable
parameter values so that the synthetic captchas are as similar as
possible to the ones from the target captcha scheme. We use the grid
search method presented in [4] to search for the optimal parameters
for a given captcha scheme. Like the image generator, the CNN model
learns how to modify the generated images at the pixel level so that
the resulting captcha contains security features that are similar to
the real ones of the target scheme. The similarity is measured by the
ratio of synthetic captchas that cannot be distinguished from the
real ones by the discriminator. In other words, the more synthetic
captchas that can “fool" the discriminator, the higher quality the
synthetic captchas are.
Captcha discriminator. We use the discriminator network de-
fined in [52], which is a convolutional network whose last layer
outputs the probability of an input captcha being a synthetic one.
We use batched captchas to train the discriminator, where each
mini-batch consists of randomly sampled synthetic captchas, xi
and real captchas, yj, and the target labels are 0 for every y and 1
for every x.
The discriminator network updates its parameters by minimizing
the following loss function:
LD = −
log D(xi) −
log(1 − D(yj))
(1)
i
j
which is equivalent to cross-entropy error for a two class classifi-
cation problem where D(.) is the probability of the input being a
synthetic captcha, and 1 − D(.) that of a real one. We note that the
real captchas in training are different from the one used to test our
approach.
Training. We use the minibatch stochastic gradient descent (SGD)
and the Adam solver [34] with a learning rate of 0.0002 to train our
captcha synthesizer. The overall training objective follows the gen-
eral GAN approach [52], using the L1 norm with the regularization
term λ set to 0.0001. The training objective is defined as:
Figure 5: Example synthetic captchas for the Baidu scheme.
Our captcha synthesizer is trained using a set of real
captchas (a). The parameter setting (b) defines the security
feature space. The trained captcha synthesizer is used to pro-
duce synthetic captchas with (c) and without (d) the security
features (i.e., noisy backgrounds and occluding lines in this
example) included.
∗ = arg min
G
, max
D
G
LcGAN (G, D) + λLL1(G)
(2)
where the generator, G, tries to minimize the difference between
the generated captchas and the real ones, while the discriminator, D,
tries to maximize it. During training, when updating the parameters
of the generator, we fix the parameters of the discriminator; and
when updating the discriminator, we fix the parameters of the gen-
erator. Training terminates when the discriminator fails to identify
more than 5% of the synthetic captchas. Training the synthesizer
takes around 2 days for one captcha scheme on our platform. The
trained generator network (together with the captcha image gener-
ator) can then be used to quickly generate synthetic captcha images.
In our case, it takes less than one minute to generate one million
captchas images.
Example. We use the Baidu captcha scheme to explain the pro-
cess for training the captcha synthesizer. To initialize the training,
we provide a set of real captchas for the GAN learning engine and
initial parameter values for the image generator. The generator
then produces a batch of synthetic captchas which are examined
by the discriminator. If the discriminator can successfully distin-
guish a large number of synthetic captchas from the real ones, the
grid search method is employed to adjust the parameter values
for synthesizing another batch of captchas. This process continues
until the discriminator can distinguish less than 5% of the synthetic
captchas from the real ones (see Section 6.6). When the process is
terminated, the learning engine will output the optimal parame-
ter values that are used by the image generator and the generator
network for synthesizing captcha images. As an example, Figure 5
(a) shows a real Baidu captcha while (b) and (c) in Figure 5 are the
synthetic captchas with and without background security features
produced by our approach. As can be seen from the figure, the
Captcha Image GeneratorParameter settingsLCxWRandom captcha wordsGenerator NetworkSecurity Feature  On/Off#OptionsValue RangeNoisy background(s)On5[10, img.width]Occluding linesOn2{Line, Sin, Quadratic, Bezieer}Char.OverlappingOn-[-3, 10]Character setOn4[A –Z]Font style(s)On1SolidFont color(s)On1RGB (65, 103, 141)DistortionOn-{[0.1, 0.2], [0.2, 0.3]}RotationOn-[-30, 30]WavingOff--(a) Real Baidu captchas of different security features(b) Synthetic parameters(c) Generated synthetic captchas (w/ security features)(d) Generated synthetic captchas (w/o security features)(e.g., noisy backgrounds and occluding lines) and a corresponding
captcha with these security features excluded. Since the training
captchas are generated by our captcha synthesizer, it is trivial to
exclude the security features from the generation process. After
having the initial discriminator and generator, we then train them
under the generative adversarial framework. The process is similar
to how we train our captcha synthesizer (Section 4.1). Over time,
the generator would become better in removing security features,
i.e., the resulting captchas are increasingly like the clean captchas;
and the discriminator would become better in recognizing security
features of the captcha (even the changes are small). Training termi-
nates when the discriminator fails to identify more than 5% of the
generated captchas from the clean counterparts (Figure 6c). After
that, we use the trained generator to pre-process unseen captcha
images of the target scheme.
4.3 Captcha Solvers
To build a captcha solver, we follow a two-step approach. We
first learn a base solver from synthetic captchas. We then fine-tune
the base solver using the same set of real captchas used to build the
captcha synthesizer.
Figure 6: The training process of our GAN-based pre-
processing model. The generator tries to remove as much
noisy backgrounds and occluding lines from the input
captchas, while the discriminator tries to identify which
of the input clean captchas are produced by the generator.
All the captchas used in the training are generated by our
captcha synthesizer.
security features of the synthetic captchas are visually similar to
the real captchas.
4.2 Captcha Preprocessing
Previous successful attacks have led to the development of more
robust text-based captchas that include advanced security features
like occluding lines (e.g., Figure 1a) and distorted hollow fonts (e.g.,
Figure 1 b and c). These features make the previous pre-processing
methods like [16, 64] inapplicable (see Section 6.3).
To remove these security features, we turn again to employ
deep learning to build a pre-processing model. The goal of our
pre-processing model is to remove noise and occluding lines from
the background and to standardize the font style (such as filling
hollow parts of characters and widening and standardizing the
gap between two characters - see also Section 6.3). Specifically, we
adapt the Pix2Pix image-to-image translation framework [14]. This
algorithm was developed to transform an image from one style to
another. In our case, the images to be translated are captcha images
with background noises such as the Baidu captchas (Figure 1a)
or different font styles such as the Microsoft captchas (Figure 1c).
Our model is also able to remove multiple security features (e.g.,
Figure 5b) at once. It is to note that we train a pre-processing model
for each captcha scheme using synthetic data.
Our pre-processing model is also a GAN consisting of a gener-
ator and discriminator. The training goal is to learn a generator
to remove security features and standardize the font style. Fig-
ure 6 illustrates the training process of our pre-processing model.
The generator works at the pixel level, which tries to amend some
pixels of the input captcha image to e.g., remove noise from the
background (Figure 6b). By contrast, the discriminator tries to dis-
tinguish the pre-processed captchas from the clean captchas that
are produced by the captcha synthesizer described in Section 4.1.
To train the pre-preprocessing model, we first learn an initial dis-
criminator and generator using some synthetic captchas (Figure 6a).
The training captchas are organized as pairs where each pair con-
tains a synthetic captcha with the target security features enabled
Solver model structure. Our captcha solver tries to recognize
4.3.1
the characters of a pre-processed captcha image. The solver is based
on a classical CNN called LeNet-5 [38]. We have also considered other
influential CNN structures including ResNet [27], Inception [56] and
VGG [53]. We found that there is little difference for solving text-
based captchas among these models. We choose LeNet-5 due to
the simplicity of the network, which gives the quickest inference
(i.e., prediction) time and requires least training data for applying
transfer learning. We use the same network structure for the base
and the fine-tuned solvers, but we train a solver for each captcha
scheme using synthetic data.
LeNet-5 was originally proposed to recognize single characters
but we introduce some additional layers (2x convolutional and
3x pooling layers) to extend its capability to recognize multiple
characters. Figure 7a shows the structure of our solver which has
five convolutional layers, five polling layers followed by two fully-
connected layers. Each of the convolutional layer is followed by
a pooling layer. We use a 3 × 3 filter for the convolutional layer
and a max-pooling filter for the pooling layer. We use the default
parameters of LeNet-5 for the rest of the network structures.
The output layer of our solver consists of a number of neurons,
one neuron for a character of the target scheme. For example, if a
captcha scheme uses n characters, the output layer will consist of n
neurons where each neuron corresponds to a candidate character.
Each neuron applies an activation function f(x) over its inputs. The
activation of each neuron represents the model’s confidence that
the corresponding character is the correct one. To obtain the pre-
dicted characters, we find the neurons with the largest activations
for a given captcha scheme and map the chosen neurons to the
corresponding characters. For example, for a captcha scheme of
four characters, we will choose the four neurons with the largest
activation values and then translate the chosen neurons to the
corresponding characters.
+Discriminator training Discriminator trainingGenerator training+accuracy+Generator training(a) Pre-training(b) Generator training (c) Discriminator training Figure 7: Overview of our CNN based captcha solver. The base solver is trained using synthetic captchas (a), which is then refined
using a small number (500 in this work) of real captchas (b).
4.3.2 Training the base solver. We train a base solver for each target
captcha scheme. If the number of characters in a captcha from a
scheme is not fixed, we also train a base solver for each possible
number of characters. We use 200,000 synthetic captchas generated
by our scheme-specific captcha synthesizer to train a base solver.
Each training sample consists of a captcha image (without security
features) and an integer vector that stores the character IDs of
the captcha. Note that we assign an unique ID to each candidate
character of the target captcha scheme. We use a Bayesian based
parameter tuner [20] to automatically choose the hyperparameters
for training the base solver. Training a base solver takes around five
hours using 4x NVIDIA P40 GPUs on a cloud server (see Section 5.2).
The trained base solver can then be applied to any unseen captcha
image of the target scheme. Note that before passing a raw captcha
image to the solver, we first use the pre-processing model to remove
the security features of the captcha image.
4.3.3 Building the fine-tuned solver. In the final step, we apply
transfer learning [65] to update later layers (i.e., those that are
closer to the output layer) of the base solver using a small set of
manually-labeled real captchas. The idea of transfer learning is
that in neural network classification, information learned at the
early layers of neural networks (i.e. closer to the input layer) will be
useful for multiple classification tasks. The later the network layers
are, the more specialized the layers become [48]. Our work exploits
this property to caliberater the base solver to avoid any bias and
over-fitting that may arise from the synthetic training data.
Figure 7b illustrates the process of applying transfer learning to
refine the base solver. Transfer learning in our context is as simple
as keeping the weights of the early layers and then update the
parameters of the later layers by applying the standard training
process using the real captchas. The fine-tuning process is quick,
taking then less than 5 minutes on our training platform.
5 EXPERIMENTAL SETUP
In this section we describe our experimental parameters and
evaluation platforms.
5.1 Data Preparation
We use two sets of captchas in this work: one for training and the
other for testing. Most of our training data are synthetic captchas
generated by our captcha synthesizer. To train and test our GAN-
based synthesizer and the fine-tuned solver, we use in total 1,500
labeled, real captchas collected from the target website. From the
1,500 real captchas of a captcha scheme, we use 500 captchas for
training and the remaining 1,000 captchas for testing. We make
sure that the testing captchas are different from the ones used to
train our models.
Captcha schemes. Our main evaluation targets 11 current text-
based captcha schemes used by 32 of the top-50 popular websites
ranked by Alexa3. We note that some of the websites use the same
captcha scheme, e.g., Youtube uses the Google scheme, and Live,
Office and Bing use the Microsoft scheme. The websites we exam-
ined cover a wide range of domains including e-commerce, social
networks, search, and information portals. Table 1 lists the captcha
schemes tested in this work and the target websites. We note that
many captcha schemes exclude characters that are likely to cause
confusion after performing the character distortion. Examples of
such characters include ‘o’ and ‘0’, ‘1’ and ‘l’, etc. These excluded
3Data were collected between May, 2017 and April, 2018.
H R H USynthetic captchas and their labelsTarget captchas and their labels(a) Train the base solver(b) Train the fine-tuned solverRetrained LayersReused LayersOutputL C X WWOutputUOutConvolutionalPoolingFully connectedScheme Website(s)
Example
Wikipedia
wikipedia.org
Microsoft
{live, bing, miscosoft}.com
{office, linkedin}.com
eBay
Baidu
Google
Alipay
ebay.com
{baidu, qq}.com
google.{com,co.in,co.jp,
co.uk,ru,com.br,fr
com.hk,it,ca,es,com.mx}
youtube.com
{alipay, tmall}.com
{taobao, login.tmall}.com
alipayexpress.com
JD
jd.com
Qihu360
360.cn
Sina
sina.cn
Weibo
weibo.cn
Sohu