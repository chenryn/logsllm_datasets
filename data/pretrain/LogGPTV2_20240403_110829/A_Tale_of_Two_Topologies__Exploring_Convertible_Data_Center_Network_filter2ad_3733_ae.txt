MPTCP balances between these extremes. It achieves higher
average throughput than LP minimum, and the variance of
flow throughput is smaller than LP average. Leveraging multi-
paths and congestion control, MPTCP can dynamically adapt
to the link utilization to get high throughput and maintain
fair bandwidth sharing across flows.
From the above results, we conclude that ùëò-shortest-path
routing plus MPTCP is efficient enough. With the right
choice of ùëò, it constantly achieves comparable throughput to
optimal bandwidth allocation from LP solutions given a set
of traffic patterns on diverse flat-tree topologies. It balances
between high network utilization and load balancing, which
are important indicators of good performance in practice.
5.2 Does flat-tree handle real traffic well?
To evaluate the transmission performance of flat-tree with
real data center traffic, we need a practical network topology.
Large-scale data center network designs in recent years show
the trend of non-blocking switch fabric with oversubscription
only at the network edge [38, 39]. We follow this trend to use
topo-1 as a representative flat-tree topology for the remaining
simulations. We run traffic traces from 4 Facebook data
centers each carrying different services. They are from the
following two sources.
1) We obtain the one-hour trace in a Hadoop data center
(denoted as Hadoop-1) from the Coflow benchmark [5], which
contains aggregated rack-level traffic through a 1Gbps single-
switch network core. Our flat-tree network uses 10Gbps links
and has 8 uplinks per edge switch. For each rack-to-rack flow
from the trace, we create 8 flows between servers under the
source and destination edge switches to stress the switch
uplinks and give 10 times the original traffic volume to each
of the 8 flows to adjust the bandwidth difference.
2) We obtain traffic statistics for 3 other data centers
(denoted as Hadoop-2, Web, and Cache) from the Facebook
measurement study [38]. The full traces are not released,
so we generate our own traces based on the publicly shared
sampling data [8] and the reported results from the paper [38].
The source and destination servers of the flows are inferred
from the sampling data. The flow size and the flow arrival
rate are reverse-engineered from Figure 6 and Figure 14 in
the paper. We omit inter-data-center traffic in the data.
The traffic has the following characteristics.
Hadoop-1: the trace reflects the shuffle phase of MapRe-
duce jobs. The traffic does not have clear locality. We observe
one-to-many, many-to-one, and many-to-many traffic involv-
ing a large number of machines network-wide.
Hadoop-2: different from the above trace, the traffic
shows strong rack and Pod level locality. 75.7% of the traffic
is intra-rack, and almost all the remaining traffic is intra-Pod.
Web: the traffic has strong Pod level locality. There is a
tiny amount of intra-rack traffic. Around 77% of the traffic
is intra-Pod, and the rest is inter-Pod.
‚àí101234567891011MPTCPLP avgLP mintraffic‚àí1Flow throughput (Gbps)‚àí101234567891011MPTCPLP avgLP mintraffic‚àí2Flow throughput (Gbps)‚àí101234567891011MPTCPLP avgLP mintraffic‚àí3Flow throughput (Gbps)‚àí101234567891011MPTCPLP avgLP mintraffic‚àí4Flow throughput (Gbps)A Tale of Two Topologies
SIGCOMM ‚Äô17, August 21-25, 2017, Los Angeles, CA, USA
Figure 8: CDF of flow completion time in Facebook‚Äôs Hadoop-1, Hadoop-2, Web, and Cache data centers
Cache: the traffic shows even stronger Pod level locality.
There is almost zero intra-rack traffic. Around 88% of the
traffic is intra-Pod, and the rest is inter-Pod.
Figure 8 shows the distribution of flow completion time
of these different traffic traces. Regardless of the traffic, the
performance of flat-tree in the global mode is close to that
of the random graph, and the performance of flat-tree in the
local mode is close to that of the two-stage random graph. It
demonstrates that flat-tree well approximates random graphs
of different scales given real data center workloads, which is
consistent with the conclusion in the prior study [47].
In practice, Clos networks usually implement ECMP and
TCP. For fair comparison, we simulate the flat-tree Clos mode
with ùëò-shortest-path routing and MPTCP as well to avoid
the handicap of less efficient routing and congestion control
mechanisms. As expected, the performance of flat-tree Clos
mode with ECMP and TCP is remarkably worse than the
other networks. For ECMP, the next hop at each switch is
determined pseudo-randomly by header field hashing, so each
TCP flow traverses only one of the equal cost shortest paths.
Being unable to use multi paths concurrently is especially
bad for large flows. In later discussions, we focus on the Clos
mode with ùëò-shortest path routing and MPTCP.
Most importantly, we observe that different modes of flat-
tree are best suited for different types of traffic. In Figure 8(a),
for the network-wide traffic, flat-tree global mode has an order
of magnitude improvement over the Clos network. Flat-tree
local mode has similar performance to the global mode for two
reasons. First, the traffic is not intensive enough to saturate
the links on these topologies, although the Clos network is
already heavily congested. Second, there is a considerable
amount of intra-Pod traffic in the network-wide traffic. Since
the global mode has richer core bandwidth than the local
mode, we expect greater benefit from the global mode given
heavier traffic and more inter-Pod communications.
In Figure 8(b), the performance of flat-tree Clos mode
is the best due to the large proportion of intra-rack traffic.
Flat-tree local mode is the second best, because the topology
handles intra-rack traffic relatively well and there is still
around 24.3% intra-Pod traffic. Flat-tree global mode is not
very efficient for traffic with strong locality. For traffic with
Pod-level locality, as shown in Figure 8(c) and (d), flat-tree
local mode has the best performance, followed by the global
mode and the Clos mode. This result reflects the distribution
of network bandwidth. The global mode has less intra-Pod
bandwidth than the local mode, but the rich network-wide
bandwidth makes it more efficient than the Clos network. The
difference among topologies is more significant in Figure 8(d)
due to stronger locality and higher traffic volume.
These simulation results of real data center traffic on a
practical data center topology validate the design purpose
of flat-tree. Flat-tree can be configured into different modes
to optimize traffic with different locality features, i.e. Clos
mode for rack-level locality, local mode for Pod-level local-
ity, and global mode for no locality. If the network is used
for a different service, the network topology can be easily
reconfigured to adapt to the new traffic. This flexibility in
topology is particularly useful for public clouds where the
service requirements are constantly changing. For a produc-
tion data center like Facebook with integral parts of different
services, flat-tree can be used in the hybrid mode with various
service-specific zones, interconnected by the network core for
inter-zone communication. When the services are reorganized,
the network zones can be repartitioned to accommodate the
change of needs.
5.3 Is flat-tree implementable?
To explore the feasibility of implementing flat-tree in prac-
tice, we build the example network in Figure 2 on a hard-
ware testbed. As shown in Figure 9, it consists of 5 48-
port packet switches, one 192-port 3D-MEMS optical cir-
cuit switch (OCS), and 24 servers each with 6 3.5GHz dual-
hyperthreaded CPU cores and 128GB RAM. All links are
10Gbps. The first 4 packet switches are partitioned into edge
and aggregation switches in each Pod, and the last packet
switch is partitioned into the 4 core switches. The converter
switches are logical partitions of the OCS. To make the
testbed more manageable, we connect servers to converter
switches via an extra hop on packet switches.
We implement ùëò-shortest-path routing and MPTCP for
all 3 flat-tree topologies. ùëò is set to 4 as it yields the best
performance in the simulation of this network. We realize
the addressing scheme as described in Section 4.2.1. Our
Flow completion time (ms)102103104105106107CDF00.10.20.30.40.50.60.70.80.91(a) Hadoop-1Flow completion time (ms)10-1100101102CDF00.10.20.30.40.50.60.70.80.91(b) Hadoop-2Flow completion time (ms)10-1100101CDF00.10.20.30.40.50.60.70.80.91(c) WebFlow completion time (ms)10-1100101102CDF00.10.20.30.40.50.60.70.80.91(d) CacheFlat-tree global modeFlat-tree local modeFlat-tree Clos mode (k-shortest paths)Flat-tree Clos mode (ECMP)Random graphTwo-stage random graphSIGCOMM ‚Äô17, August 21-25, 2017, Los Angeles, CA, USA
Y. Xia et al.
Figure 9: A testbed implementing the flat-tree example in Figure 2
packet switches use a legacy OpenFlow 1.0 image. It does not
support arbitrary bits matching of a field, which is necessary
for source routing as shown in Section 4.2.2. So, we conduct
prefix matching for the source and destination IP addresses
on the switches a path traverses. The maximum number of
OpenFlow rules per switch under each topology is 242, 180,
and 76 respectively. The difference is due to the different
number of ingress/egress switches in each topology. With
source routing, these numbers will be significantly less.
We demonstrate the functionality of the testbed with an
iPerf throughput experiment. On every server, we send iPerf
traffic to the 3 servers with the same index in the other 3
Pods. This traffic pattern enables the measurement of the
core bandwidth in the network. iPerf is set to update the
flow throughput every 0.5 second. Throughout the 5-minute
experiment, we change the network topology to different flat-
tree modes. We add up the throughputs of individual flows
to obtain the real-time bidirectional core bandwidth.
Figure 10 plots the variation of core bandwidth as we
change the network topology. The local mode and the Clos
mode have around 145Gbps average total throughput. Com-
pared to the Clos mode, the local mode rearranges servers
within Pods only, so there is no change to the core bandwidth.
Our testbed is 1.5:1 oversubscribed, so the Clos network has
24√ó 10Gbps/1.5 = 160Gbps total bandwidth. This result
shows that the overhead of MPTCP and ùëò-shortest-path rout-
ing is within 9.38% of the bandwidth, which is reasonable
as the MPTCP packet processing lays extra burden on CPU
and ùëò-shortest-path routing is not perfect. The average total
throughput in the global mode is around 185Gbps. With the
power of convertibility, the network core bandwidth increases
by 27.6% in this small testbed. We envision greater improve-
ment in real data centers with a larger number of switches
and more flexibility of conversion.
From the figure, we observe that iPerf reaches the maxi-
mum throughput in 2s to 2.5s after a topology change. Table 3
shows the accurate measurement of the conversion delay by
the control software. The delay can be broken down into the
time for reconfiguring the OCS, deleting old OpenFlow rules,
and adding new OpenFlow rules. The rule deletion and instal-
lation delay are proportional to the number of rules for the
Figure 10: Summation of iPerf throughput every 0.5 second on
the testbed with different flat-tree modes. Every server sends iPerf
traffic to its counterparts in the other Pods to saturate the network
core. Traffic adapts to the topology change in 2 to 2.5 seconds.
topology before and after conversion. Our implementation
has room for improvement. First, the legacy switches process
rules more slowly than the main-stream technology [27, 37].
Second, the packet switches and the OCS are configured
sequentially, and this can be easily parallelized. Even with
these artifacts, the network topology can be converted in
roughly 1s and the application adapts to the topology change
in another 1.5s.
5.4 Does convertibility benefit
applications?
Most data center applications are computation-oriented, inter-
node communications serving the purpose of exchanging in-
termediate computation data. For this reason, the behavior
of data transmission is influenced by many factors in the
computation framework. For instance, read/write data serial-
ization/deserialization adds to the end-to-end data transmis-
sion time; imperfect synchronization of computation nodes
disorganizes traffic patterns; garbage collection may block
communications, etc. In our testbed, converting the network
topology from the Clos mode to the global mode improves the
core bandwidth by 27.6%. However, with all these overheads
from the computation framework, whether the bandwidth
increase can be translated into acceleration of data center
applications is yet another question.
We answer this question by running Spark and Hadoop, the
most widely used computation frameworks, on our testbed.
Among the 24 servers, we set the first server as the master
node and all the other servers as slave nodes. We change
the network topology and compare the end-to-end data read
time under different flat-tree modes. The characteristics of
the jobs are as follows.
Spark broadcast: we run Word2Vec, the iterative ma-
chine learning job for document feature extraction. In each
iteration, the master node broadcasts the updated model to
all workers. We choose the ‚Äútorrent‚Äù option for the broadcast
operation to distribute the data in the BitTorrent fashion.
Spark promotes in-memory computation, so the data to be
transmitted is readily available in memory, although data
serialization and deserialization are needed.
Hadoop shuffle: we run the Sort job on Tez [9], a variant
of Hadoop MapReduce. It has a heavy shuffle phase, where
all the nodes as mappers send data to a subset of nodes as
Pod$1Pod$2Pod$3Pod$4CoreLinks$to$OCSLinks$to$serversEdge7aggrlinksLinks$to$packet$switchesServers$in$Pod$2Servers$in$Pod$1Servers$in$Pod$4Servers$in$Pod$3020406080100120140160180200220240260280300020406080100120140160180200220Time (s)Network core bandwidth (Gbps)Global mode Clos mode Global mode Local mode 2.5s 2s 2.5s A Tale of Two Topologies
SIGCOMM ‚Äô17, August 21-25, 2017, Los Angeles, CA, USA
Table 3: Conversion delay of the experiment in Figure 10
Topology Configure OCS Delete rule Add rule
Total
Global
Local
Clos
160ms
160ms
160ms
477ms
202ms
635ms
644ms
482ms
209ms
1281ms
844ms
1004ms
reducers. We store the data on a RAM disk to prevent the
hard drive being the bottleneck of data read/write.
Figure 11 shows the average end-to-end data read time
and the duration of the communication phase for the above
two applications. The end-to-end data read time includes