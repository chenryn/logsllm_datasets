     _ALWAYS_SAFE_BYTES = bytes(_ALWAYS_SAFE)
     _safe_quoters = {}
    @@ -782,17 +766,14 @@
         Each part of a URL, e.g. the path info, the query, etc., has a
         different set of reserved characters that must be quoted.
    -    RFC 3986 Uniform Resource Identifiers (URI): Generic Syntax lists
    +    RFC 2396 Uniform Resource Identifiers (URI): Generic Syntax lists
         the following reserved characters.
         reserved    = ";" | "/" | "?" | ":" | "@" | "&" | "=" | "+" |
    -                  "$" | "," | "~"
    +                  "$" | ","
         Each of these characters is reserved in some component of a URL,
         but not necessarily in all of them.
    -    -    Python 3.7 updates from using RFC 2396 to RFC 3986 to quote URL strings.
    -    Now, "~" is included in the set of reserved characters.
         By default, the quote function is intended for quoting the path
         section of a URL.  Thus, it will not encode '/'.  This character
可以明显得看出，主要是多了一个处理得函数
    -def _checknetloc(netloc):
    -    if not netloc or netloc.isascii():
    -        return
    -    # looking for characters like \u2100 that expand to 'a/c'
    -    # IDNA uses NFKC equivalence, so normalize for this check
    -    import unicodedata
    -    netloc2 = unicodedata.normalize('NFKC', netloc)
    -    if netloc == netloc2:
    -        return
    -    _, _, netloc = netloc.rpartition('@') # anything to the left of '@' is okay
    -    for c in '/?#@:':
    -        if c in netloc2:
    -            raise ValueError("netloc '" + netloc2 + "' contains invalid " +
    -                             "characters under NFKC normalization")
同时也可以看出，这次主要更新得地方。
如下对上面得代码进行分析
## 源码分析
### unicode规范化处理
如下引用一下`https://python3-cookbook.readthedocs.io`
关于unicode得规范化处理，有如下说明
    unicode的规范化格式有几种，每种的处理方式有些不一样。  
        NFC  
        Unicode 规范化格式 C。如果未指定 normalization-type，那么会执行 Unicode 规范化。  
        NFD  
        Unicode 规范化格式 D。  
        NFKC  
        Unicode 规范化格式 KC。  
        NFKD  
        Unicode 规范化格式 KD。
在Unicode中，某些字符能够用多个合法的编码表示。为了说明，考虑下面的这个例子：
    >>> s1 = 'Spicy Jalape\u00f1o'
    >>> s2 = 'Spicy Jalapen\u0303o'
    >>> s1
    'Spicy Jalapeño'
    >>> s2
    'Spicy Jalapeño'
    >>> s1 == s2
    False
    >>> len(s1)
    14
    >>> len(s2)
    15
    >>>
这里的文本”Spicy Jalapeño”使用了两种形式来表示。  
第一种使用整体字符”ñ”(U+00F1)，第二种使用拉丁字母”n”后面跟一个”~”的组合字符(U+0303)。
在需要比较字符串的程序中使用字符的多种表示会产生问题。  
为了修正这个问题，你可以使用unicodedata模块先将文本标准化：
    >>> import unicodedata
    >>> t1 = unicodedata.normalize('NFC', s1)
    >>> t2 = unicodedata.normalize('NFC', s2)
    >>> t1 == t2
    True
    >>> print(ascii(t1))
    'Spicy Jalape\xf1o'
    >>> t3 = unicodedata.normalize('NFD', s1)
    >>> t4 = unicodedata.normalize('NFD', s2)
    >>> t3 == t4
    True
    >>> print(ascii(t3))
    'Spicy Jalapen\u0303o'
    >>>
`normalize()` 第一个参数指定字符串标准化的方式。
NFC表示字符应该是整体组成(比如可能的话就使用单一编码)，而NFD表示字符应该分解为多个组合字符表示。
Python同样支持扩展的标准化形式NFKC和NFKD，它们在处理某些字符的时候增加了额外的兼容特性。比如：
    >>> s = '\ufb01' # A single character
    >>> s
    'ﬁ'
    >>> unicodedata.normalize('NFD', s)
    'ﬁ'
    # Notice how the combined letters are broken apart here
    >>> unicodedata.normalize('NFKD', s)
    'fi'
    >>> unicodedata.normalize('NFKC', s)
    'fi'
    >>>
### 漏洞分析
根据以上分析
主要的修复方式就是通过对url中的unicode进行规范化处理了，现在通过具体的例子来分析一哈。
    import unicodedata
    netloc2 = unicodedata.normalize('NFKC', netloc)
    if netloc == netloc2:
        return
用我们的WSL的环境(也就是没有打上补丁的环境)进行测试
    >>> from urllib.parse import urlsplit
    >>> u = "https://example.com\PI:EMAIL"
    #不处理的结果
    >>> SplitResult(scheme='https', netloc='example.com＃@bing.com', path='', query='', fragment='')
    #规范化处理的结果 
    >>>import unicodedata
    >>>u2 = unicodedata.normalize('NFKC', u)
    >>> urlsplit(u2)
    SplitResult(scheme='https', netloc='example.com', path='', query='', fragment='@bing.com')
    #特殊编码处理的结果
    >>>u3 = u.encode("idna").decode("ascii")
    >>> urlsplit(u3)
    SplitResult(scheme='https', netloc='example.com', path='', query='', fragment='@bing.com')
以上就是漏洞的原理，不同的编码经处理之后，经过urlsplit() 处理之后，得到的的netloc是不一样的
> IDNA（Internationalizing Domain Names in
> Applications）IDNA是一种以标准方式处理ASCII以外字符的一种机制，它从unicode中提取字符，并允许非ASCII码字符以允许使用的ASCII字符表示。
>
>
> unicode转ASCII发生在IDNA中的TOASCII操作中。如果能通过TOASCII转换时，将会以正常的字符呈现。而如果不能通过TOASCII转换时，就会使用“ACE标签”，“ACE”标签使输入的域名能转化为ASCII码
所以在新的urlsplit函数中会增加一个判断，如果规范化处理的结果和原来的结果一样，才能返回真确的值。
### 题目分析
    def getUrl():
        url = request.args.get("url")
        host = parse.urlparse(url).hostname
        if host == 'suctf.cc':
            return "我扌 your problem? 111"
        parts = list(urlsplit(url))
        host = parts[1]
        if host == 'suctf.cc':
            return "我扌 your problem? 222 " + host
        newhost = []
        for h in host.split('.'):
            newhost.append(h.encode('idna').decode('utf-8'))
        parts[1] = '.'.join(newhost)
        #去掉 url 中的空格
        finalUrl = urlunsplit(parts).split(' ')[0]
        host = parse.urlparse(finalUrl).hostname
        if host == 'suctf.cc':
            return urllib.request.urlopen(finalUrl, timeout=2).read()
        else:
            return "我扌 your problem? 333"
    if __name__ == "__main__":
        app.run(host='0.0.0.0', port=80)
根据以上分析，题目就比较简单了，只需要满足hostname`encode('idna').decode('utf-8'))`
处理之前不是`suctf.cc` 处理之后是`suctf.cc`就好了
## 后记
漏洞不难理解，只是觉得应该记录一下
看到了一句话,摘自某位大佬：
> 如果翻译器对程序进行了彻底的分析而非某种机械的变换, 而且生成的中间程序与源程序之间已经没有很强的相似性, 我们就认为这个语言是编译的.
> 彻底的分析和非平凡的变换, 是编译方式的标志性特征.
>
> 如果你对知识进行了彻底的分析而非某种机械的套弄, 在你脑中生成的概念与生硬的文字之间已经没有很强的相似性, 我们就认为这个概念是被理解的.
> 彻底的分析和非凡的变换, 是获得真知的标志性特征.
与君共勉。
## 参考链接
参考很多链接，不过我觉得，遇到问题看官方的文档和源码更有效果