versions in this range yielded nearly identical results, despite sub-
stantial changes to the QUIC codebase (including reorganization of
16We observed similar results for 10 and 50 Mbps under similar loss.
17The stable version of Chrome at the time of analysis (52) did not support QUIC versions earlier
than 25. To avoid false inferences from using different Chrome versions and different QUIC versions,
we only tested QUIC versions 25 and higher.
Figure 16: QUIC proxy test setup. The proxy is located mid-
way between client and server.
code that frustrated our attempts to instrument it). This is corrobo-
rated by changelogs [12] that indicate most modifications were to
the cryptography logic, QUIC flags, and connection IDs.
Based on the relatively stable QUIC performance across recent
versions, we expect our observations about its performance using its
current congestion control algorithm are likely to hold in the future
(except in places where we identified opportunities to improve the
protocol).
Note that at the time of writing, the recently proposed BBR
congestion control algorithm has not been deployed in the “stable”
branch and thus we could not evaluate its performance fairly against
Cubic in QUIC or TCP. Private communication with a QUIC team
member indicated that BBR is “not yet performing as well as Cubic
in our deployment tests.”
At the time of publication, the
Comparison with QUIC 37.
latest stable version of Chromium was 60.0.3112.101, which includes
QUIC 37 as the latest stable version. To enhance our longitudinal
analysis and demonstrate how our approach easily adapts to new
versions of QUIC, we instrumented, tested, and compared QUIC 37
with 34 (the one used for experiments through out this paper).
We found that the main change in QUIC 37 is that the maximum
allowed congestion window (MACW) increased to 2000 (from 430
used in our experiments) in the new versions of Chromium. This al-
lows QUIC to achieve much higher throughput compared to version
34, particularly improving performance when compared with TCP
for large transfers in high bandwidth networks. Fig. 15 shows the
comparison between TCP and QUIC version 37 for various object
sizes with MACW of 430 (Fig. 15a) and 2000 (Fig. 15b). When com-
paring Fig. 15a and 6a, we find that QUIC versions 34 and 37 have
almost identical performance when using the same MACW; this is
corroborated by QUIC version changelogs [12]. All our previous
findings, e.g., QUIC performance degradation in presence of deep
packet reordering, still hold for this new version of QUIC.
5.5 Impact of Proxying
We now test the impact of QUIC’s design decisions on in-network
performance optimization. Specifically, many high-latency net-
works use transparent TCP proxies to reduce end-to-end delays and
improve loss recovery [40]. However, due to the fact that QUIC en-
crypts not only payloads but also transport headers, such proxying
is impossible for in-network devices.
We evaluate the extent to which this decision impacts QUIC’s
potential performance gains. Specifically, we wrote a QUIC proxy,
and co-located it with a TCP proxy so that we could compare the
impact of proxying on end-to-end performance (Fig. 16). For these
experiments, we consider PLTs as done in previous sections.
Client's machineServerRouter (running network emulator)Proxy40ms RTT40ms RTT80ms RTTQuality
tiny
medium
hd720
hd2160
Time to Start
(secs)
QUIC
0.5 (0.11)
0.9 (1.04)
0.7 (0.16)
5.9 (2.73)
TCP
0.5 (0.21)
0.5 (0.13)
0.7 (0.18)
5.9 (2.51)
QUIC
33.8 (0.01)
17.9 (0.01)
8.0 (0.27)
0.8 (0.05)
TCP
33.8 (0.01)
12.9 (0.92)
4.3 (0.28)
0.4 (0.01)
QUIC
0.9 (0.17)
1.4 (1.62)
1.1 (0.27)
50.2 (3.01)
TCP
0.9 (0.34)
0.9 (0.22)
1.1 (0.27)
73.1 (1.91)
QUIC
0.0 (0.0)
0.0 (0.0)
0.0 (0.0)
6.7 (0.46)
TCP
0.0 (0.0)
0.0 (0.0)
0.0 (0.0)
4.9 (0.3)
#rebuffers
per playing secs
TCP
QUIC
0.0 (0.0)
0.0 (0.0)
0.0 (0.0)
0.0 (0.0)
0.0 (0.0)
0.0 (0.0)
0.2 (0.01)
0.3 (0.01)
Taking a Long Look at QUIC
IMC ’17, November 1–3, 2017, London, United Kingdom
video loaded
in 1 min (%)
Buffer/Play Time† (%)
#rebuffers
Table 6: Mean (std) of QoE metrics for a YouTube video in different qualities, averaged over 10 runs. 100Mbps, 1% loss. QUIC
benefits are clear for high qualities. While the absolute number of rebuffers for QUIC is higher for hd2160, it is able to load
and play more of the video in a given time compared to TCP, with fewer (about 30%) rebuffers per playing second. †Buffer/Play
Time is the time spent while buffering the video divided by the time playing video.
(a) No added loss or latency
(b) 1% Loss
(c) 100ms added RTT
Figure 17: QUIC vs. TCP proxied, where red cells indicate that QUIC performs better.
(a) No added loss or latency
(b) 1% Loss
(c) 100ms added RTT
Figure 18: QUIC with and without proxy when downloading objects with different sizes. Positive numbers (red cells) mean
QUIC performs better connecting to the server directly.
We present two types of comparison results: QUIC vs. proxied
TCP (as this is what one would expect to find in many cellular net-
works), and QUIC vs. proxied QUIC (to determine how QUIC would
benefit if proxies could transparently terminate QUIC connections).
For the former case, we find that QUIC continues to outperform
TCP in many scenarios, but its benefits diminish or entirely dis-
appear compared to unproxied TCP in low loss/latency cases, and
when there is 1% loss. In the case of high delay links, QUIC still
outperforms TCP (Fig. 17). Thus, proxies can help TCP to recover
many of the benefits of QUIC, but primarily in lossy scenarios, and
when the proxy is equidistant from the client and server.
In the case of a QUIC proxy (Fig. 18), we find that a proxy hurts
performance for small object sizes (likely due to inefficiencies and
the inability to establish connections via 0-RTT), but performance
is better under loss for large objects. Taken together, our initial
attempt at a QUIC proxy provides mixed results, and identifying
any other potential benefits will require additional tuning.
6 CONCLUSION
In this paper, we address the problem of evaluating an application-
layer transport protocol that was built without a formal specifica-
tion, is rapidly evolving, and is deployed at scale with nonpublic
configuration parameters. To do so, we use a methodology and
testbed that allows us to conduct controlled experiments in a vari-
ety of network conditions, instrument the protocol to reason about
its performance, and ensure that our evaluations use settings that
approximate those deployed in the wild. We used this approach to
evaluate QUIC, and found cases where it performs well and poorly—
both in traditional desktop environments but also in mobile and
proxy scenarios not previously tested. With the help of an inferred
protocol state machine and information about time spent in each
state, we explained the performance results we observed.
There are a number of open questions we plan to address in
future work. First, we will evaluate performance in additional oper-
ational networks, particularly in more mobile ones and data centers.
Second, we will investigate techniques to improve QUIC’s fairness
to TCP while still maintaining high utilization. Third, we will au-
tomate the steps used for analysis in our approach and port it to
other application layer protocols. This includes adapting our state-
machine inference approach to other protocols, and we encourage
developers to annotate state transitions in their code to facilitate
such analysis. We believe doing so can lead to a more performant,
reliable evolution for such network protocols.
ACKNOWLEDGEMENTS
We thank the anonymous reviewers and our shepherd Costin Raiciu
for their valuable feedback. Jana Iyengar provided comments on
early versions of this work. This work is funded in part by NSF
grants CNS-1600266, CNS-1617728.
IMC ’17, November 1–3, 2017, London, United Kingdom
A. Molavi Kakhki et al.
REFERENCES
[1] Android debug bridge. https://developer.android.com/studio/command-line/adb.html.
debugging
[2] Chrome
https://developer.chrome.com/devtools/docs/
protocol.
debugger-protocol.
[3] Chromium. https://www.chromium.org/Home.
[4] I. grigorik. deciphering the critical rendering path. https://calendar.perfplanet.com/2012/
deciphering-the-critical-rendering-path/.
[5] IETF QUIC WG. https://github.com/quicwg.
[6] Linux network emulation.
networking/netem.
http://www.linuxfoundation.org/collaborate/workgroups/
[7] Linux traffic control. http://linux.die.net/man/8/tc.
[8] Playing with QUIC. https://www.chromium.org/quic/playing-with-quic.
[9] QUIC: A UDP-Based Secure and Reliable Transport for HTTP/2. https://tools.ietf.org/html/
draft-tsvwg-quic-protocol-02.
[10] QUIC
at
10,000
feet.
1gY9-YNDNAB1eip-RTPbqphgySwSNSDHLq9D5Bty4FSU.
Recovery And Congestion Control.
[11] QUIC Loss
draft-tsvwg-quic-loss-recovery-01.
[12] QUIC Wire
Layout
Specification.
1WJvyZflAO2pq77yOLbp9NsGjC1CHetAXV8I0fQe-B_U.
https://docs.google.com/document/d/
https://tools.ietf.org/html/
https://docs.google.com/document/d/
[13] Tcp probe. https://wiki.linuxfoundation.org/networking/tcpprobe.
[14] Welch’s t-test. https://en.wikipedia.org/wiki/Welch%27s_t-test.
[15] I. Beschastnikh, Y. Brun, S. Schneider, M. Sloan, and M. D. Ernst. Leveraging Existing Instru-
mentation to Automatically Infer Invariant-constrained Models.
In Proceedings of the 19th
ACM SIGSOFT Symposium and the 13th European Conference on Foundations of Software Engi-
neering, 2011.
[16] P. Biswal and O. Gnawali. Does quic make the web faster? In IEEE GLOBECOM, 2016.
[17] G. Carlucci, L. De Cicco, and S. Mascolo. HTTP over UDP: an experimental investigation of
QUIC. In Proc. of SAC, 2015.
[19] C.
[18] Chromium Blog. A QUIC update on Google’s experimental transport. http://blog.chromium.
Cimpanu.
Traffic
org/2015/04/a-quic-update-on-googles-experimental.html, April 2015.
Algorithm
Handling
http://news.softpedia.com/news/
TCP
google-creates-new-algorithm-for-handling-tcp-traffic-congestion-control-508398.shtml,
September 2016.
Congestion
Control.
Creates
Google
New
for
[20] S. R. Das. Evaluation of QUIC on web page performance. Master’s thesis, Massachusetts
Institute of Technology, 2014.
[21] M. Dong, Q. Li, D. Zarchy, P. B. Godfrey, and M. Schapira. PCC: Re-architecting congestion
control for consistent high performance. In Proc. of USENIX NSDI, 2015.
[22] N. Dukkipati, N. Cardwell, Y. Cheng, and M. Mathis.
An Algorithm for
Fast Recovery of Tail Losses.
Tail Loss Probe (TLP):
https://tools.ietf.org/html/
draft-dukkipati-tcpm-tcp-loss-probe-01, February 2013.
[23] M. Fischlin and F. Günther. Multi-stage key exchange and the case of Google’s QUIC protocol.
In Proc. of ACM CCS, 2014.
[24] S. Ha and I. Rhee. Taming the elephants: New tcp slow start. In Comput. Netw., 2011.
[25] T. Jager, J. Schwenk, and J. Somorovsky. On the security of TLS 1.3 and QUIC against weak-
nesses in PKCS# 1 v1. 5 encryption. In Proc. of ACM CCS, 2015.
[26] A. Langley, A. Riddoch, A. Wilk, A. Vicente, C. Krasic, D. Zhang, F. Yang, F. Kouranov, I. Swett,
J. Iyengar, J. Bailey, J. Dorfman, J. Kulik, J. Roskind, P. Westin, R. Tenneti, R. Shade, R. Hamilton,
V. Vasiliev, W.-T. Chang, and Z. Shi. The QUIC transport protocol: Design and Internet-scale
deployment. In Proc. of ACM SIGCOMM, 2017.
[27] R. Lychev, S. Jero, A. Boldyreva, and C. Nita-Rotaru. How secure and quick is QUIC? provable
security and performance analyses. In Proc. of IEEE Security and Privacy, 2015.
[28] M. Mathis, N. Dukkipati, and Y. Cheng. Proportional rate reduction for TCP. https://tools.ietf.
[29] S. McQuistin and C. S. Perkins. Is explicit congestion notification usable with udp? In Proc. of
org/html/rfc6937, May 2013.
IMC, 2015.
[30] P. Megyesi, Z. Krämer, and S. Molnár. How quick is QUIC? In Proc. of ICC, May 2016.
[31] A. Molavi Kakhki, F. Li, D. Choffnes, A. Mislove, and E. Katz-Bassett. BingeOn under the micro-
scope: Understanding T-Mobile’s zero-rating implementation. In ACM SIGCOMM Internet-QoE
Workshop, Aug. 2016.
[32] R. Netravali, A. Sivaraman, S. Das, A. Goyal, K. Winstein, J. Mickens, and H. Balakrishnan.
Mahimahi: Accurate Record-and-replay for HTTP. In Proc. of USENIX ATC, 2015.
[33] A. Nikravesh, H. Yao, S. Xu, D. R. Choffnes, and Z. M. Mao. Mobilyzer: An open platform for
controllable mobile network measurements. In Proc. of MobiSys, 2015.
[34] J. Odvarko, A. Jain, and A. Davies. HTTP Archive (HAR) format. https://dvcs.w3.org/hg/
webperf/raw-file/tip/specs/HAR/Overview.html, August 2012.
[35] I. Swett. QUIC congestion control and loss recovery. https://docs.google.com/presentation/d/
1T9GtMz1CvPpZtmF8g-W7j9XHZBOCp9cu1fW0sMsmpoo.
[36] I. Swett. QUIC Deployment Experience @Google. https://www.ietf.org/proceedings/96/slides/
slides-96-quic-3.pdf, 2016.
[37] I.
Swett.
QUIC
FEC
v1.
https://docs.google.com/document/d/
1Hg1SaLEl6T4rEU9j-isovCo8VEjjnuCPTcLNJewj7Nk/edit, February 2016.
[38] A. Vernersson. Analysis of UDP-based reliable transport using network emulation. Master’s
thesis, Luleå University of Technology, 2015.
[39] X. S. Wang, A. Balasubramanian, A. Krishnamurthy, and D. Wetherall. How speedy is SPDY?
In Proc. of USENIX NSDI, 2014.
[40] X. Xu, Y. Jiang, T. Flach, E. Katz-Bassett, D. R. Choffnes, and R. Govindan. Investigating trans-
parent web proxies in cellular networks. In Proc. PAM, 2015.
[41] Zhang, Ming and Karp, Brad and Floyd, Sally and Peterson, Larry. RR-TCP: A Reordering-
Robust TCP with DSACK. In Proceedings of the 11th IEEE International Conference on Network
Protocols, ICNP ’03, Washington, DC, USA, 2003. IEEE Computer Society.