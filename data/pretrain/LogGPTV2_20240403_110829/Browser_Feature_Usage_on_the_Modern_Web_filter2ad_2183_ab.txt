### 2. Browser Extensions Used

This work utilizes two browser extensions: Ghostery and AdBlock Plus. 

**Ghostery** is a browser extension designed to enhance user privacy by preventing the loading of resources or setting of cookies associated with cross-domain passive tracking, as determined by Ghostery, Inc., the extension's maintainer.

**AdBlock Plus** is another browser extension that modifies the browser to block resources identified as advertising-related and hides elements on the page that are associated with advertisements. AdBlock Plus relies on a crowdsourced list of rules and URLs to determine if a resource is related to advertising.

For this study, the default configurations for each browser extension were used, including the default rule sets for blocking elements and resources. No changes were made to the configuration or implementation of either extension.

### 4. Methodology

To understand the use of browser features on the open web, we conducted a survey of the Alexa Top 10,000 websites. Each site was visited ten times, and the browser features used during these visits were recorded. We visited each site five times with an unmodified browsing environment and five times with popular tracking-blocking and advertising-blocking extensions installed. This section outlines the goals of the survey, the instrumentation of the browser to measure feature use, and the process of measuring feature use on the web in general.

#### 4.1 Goals

The primary goal of our automated survey is to determine which browser features are commonly used on the web. To achieve this, we took a broad yet representative sample of the web and exhaustively measured the features used by those sites.

To accomplish this, we developed a browser extension to measure feature use when a user interacts with a website. We then selected a representative sample of the web to visit. Finally, we devised a method for interacting with these sites in an automated fashion to elicit the same functionality that a human web user would experience. Each of these steps is described in detail in the following subsections.

This automated approach focuses on measuring the "open web," or the subset of webpage functionality that a user encounters without logging into a website. Users may encounter different types of functionality when interacting with websites they have accounts for, but such measurements are beyond the scope of this paper. We note this restriction, only measuring functionality used by non-authenticated portions of websites, as a limitation of this paper and a possible area for future work.

#### 4.2 Measuring Extension

We instrumented a recent version of the Firefox web browser (version 46.0.1) with a custom browser extension to record each time a JavaScript feature was used on a visited page. Our extension injects JavaScript into each page after the browser has created the DOM for that page but before the page's content has been loaded. By injecting our instrumentation JavaScript before the page's content is fetched and rendered, we can modify the methods and properties in the DOM before it becomes available to the requested page.

The injected JavaScript modifies the DOM to count when an instrumented method is called or when an instrumented property is written to. The details of how the extension measures these method calls and property writes are provided in the following two subsections. Figure 2 presents a representative diagram of the crawling process.

##### 4.2.1 Measuring Method Calls

The browser extension counts method invocations by overwriting the method on the containing object's prototype. This approach allows us to insert our own logging functionality for each method call and then call the original method to preserve its functionality. We replace each reference to each instrumented method in the DOM with an extension-managed, instrumented method. We use JavaScript closures to ensure that web pages cannot bypass the instrumented methods by directly accessing the original versions.

##### 4.2.2 Measuring Property Writes

Instrumenting properties was more challenging. JavaScript does not provide a way to intercept when a property is set or read on a client script-created object or on an object created after the instrumentation code has finished executing. However, using the non-standard `Object.watch()` method in Firefox, we were able to capture when pages set properties on singleton objects in the browser (e.g., `window`, `window.document`, `window.navigator`). This method allowed the extension to capture and count all writes to properties on singleton objects in the DOM.

There are a few features in the DOM where we were unable to intercept property writes, such as `document.location` and `Element.innerHTML`. These properties trigger side effects on the page, making them unmeasurable using our technique. We note this as a significant limitation of our measurement technique.

##### 4.2.3 Other Browser Features

Web standards define other browser features, such as browser events and CSS layout rules, selectors, and instructions. Our extension-based approach did not allow us to measure the use of these features, so counts of their use are not included in this work.

For standard-defined browser events (e.g., `onload`, `onmouseover`, `onhover`), the extension could have captured some event registrations through a combination of watching for event registrations with `addEventListener` method calls and watching for property-sets to singleton objects. However, we would not have been able to capture event registrations using the legacy DOM0 method of event registration (e.g., assigning a function to an object's `onclick` property to handle click events) on non-singleton objects. Since we would only have been able to see a subset of event registrations, we decided to omit events from this work.

Similarly, this work does not consider non-JavaScript-exposed functionality defined in the browser, such as CSS selectors and rules. While interesting, this work focuses solely on functionality that the browser allows websites to access through JavaScript.

#### 4.3 Eliciting Site Functionality

Using our feature-detecting browser extension, we were able to measure which browser features are used on the 10,000 most popular websites. The following subsections describe how we simulated human interaction with web pages to measure feature use, first with the browser in its default state and again with the browser modified with popular advertising and tracking-blocking extensions.

##### 4.3.1 Default Case

To understand which features are used in a site's execution, we installed the instrumenting extension described in Section 4.2 and visited sites from the Alexa 10k, with the goal of exercising as much of the functionality used on the page as possible. While some JavaScript features of a site are automatically activated on the home page (e.g., advertisements and analytics), many features will only be used as a result of user interaction, either within the page or by navigating to different areas of the site.

To trigger as many browser features as possible on a website, we used a common site testing methodology called "monkey testing." Monkey testing involves instrumenting a page to click, touch, scroll, and enter text on random elements or locations on the page. We used a modified version of `gremlins.js`, a library built for monkey testing front-end website interfaces. We modified the `gremlins.js` library to distinguish between when the `gremlins.js` script uses a feature and when the site being visited uses a feature. The former feature usage is omitted from the results described in this paper.

We started our measurement by visiting the home page of each site and allowing the monkey testing to run for 30 seconds. Because the randomness of monkey testing could cause navigation to other domains, we intercepted and prevented any interactions that might navigate to a different page. For navigations that would have been to the local domain, we noted which URLs the browser would have visited in the absence of the interception.

We then proceeded in a breadth-first search of the site's hierarchy using the URLs that would have been visited by the actions of the monkey testing. We selected three of these URLs that were on the same domain (or related domain, as determined by the Alexa data) and visited each, repeating the same 30-second monkey testing procedure and recording all used features. From each of these three sites, we then visited three more pages for 30 seconds, resulting in a total of 13 pages interacted with for a total of 390 seconds per site.

If more than three links were clicked during any stage of the monkey testing process, we selected which URLs to visit by giving preference to URLs where the path structure of the URL had not been previously seen. In contrast to traditional interface fuzzing techniques, which aim to find unintended or malicious functionality, we were interested in finding all functionalities that users commonly interact with. By selecting URLs with different path segments, we tried to visit as many types of pages on the site as possible, with the goal of capturing all the functionality on the site that a user would encounter. The robustness and validity of our strategy are evaluated in Section 6.

##### 4.3.2 Blocking Case

In addition to the default case measurements described in Section 4.3.1, we also re-ran the same measurements against the Alexa 10k with an ad blocker (AdBlock Plus) and a tracking-blocker (Ghostery) to generate a second, 'blocking', set of measurements. We treat these blocking extensions as representative of the types of modifications users make to customize their browsing experience. While a so-modified version of a site no longer represents its author's intended representation (and may, in fact, break the site), the popularity of these content-blocking extensions shows that this blocking case is a common valid alternative experience of a website.

##### 4.3.3 Automated Crawl

| **Domains Measured** | **Total Website Interaction Time** | **Web Pages Visited** | **Feature Invocations Recorded** |
|----------------------|-----------------------------------|-----------------------|----------------------------------|
| 9,733                | 480 days                          | 2,240,484             | 21,511,926,733                   |

Table 1: Amount of data gathered regarding JavaScript feature usage on the Alexa 10k. "Total website interaction time" is an estimate based on the number of pages visited and 30 seconds of page interaction per visit.

For each site in the Alexa 10k, we repeated the above procedure ten times to ensure we measured all features used on the page, first five times in the default case, and then again five times in the blocking case. By parallelizing this crawl with 64 Firefox installs operating over 4 machines, we were able to complete the crawl in two days.

We present findings for why five times is sufficient to induce all types of site functionality in each test case in Section 6. Table 1 presents some high-level figures of this automated crawl. For 267 domains, we were unable to measure feature usage for various reasons, including non-responsive domains and sites that contained syntax errors in their JavaScript code that prevented execution.

### 5. Results

In this section, we discuss our findings, including the popularity distribution of JavaScript features used on the web with and without blocking, a feature's popularity in relation to its age, which features are disproportionately blocked, and which features are associated with security vulnerabilities.

#### 5.1 Definitions

- **Feature Popularity**: The percentage of sites that use a given feature at least once during automated interaction with the site. A feature that is used on every site has a popularity of 1, and a feature that is never seen has a popularity of 0.
- **Standard Popularity**: The percentage of sites that use at least one feature from the standard at least once during the site's execution.
- **Block Rate**: How frequently a feature would have been used if not for the presence of an advertisement- or tracking-blocking extension. Browser features that are used much less frequently on the web when a user has AdBlock Plus or Ghostery installed have high block rates, while features that are used on roughly the same number of websites in the presence of blocking extensions have low block rates.

#### 5.2 Standard Popularity

##### 5.2.1 Overall

Figure 3 displays the cumulative distribution of standard popularity. Some standards are extremely popular, and others are extremely unpopular: six standards are used on over 90% of all websites measured, and a full 28 of the 75 standards measured were used on 1% or fewer sites, with eleven not used at all. Standard popularity is not feast or famine, however, as standards see several different popularity levels between those two extremes.

##### 5.2.2 Standard Popularity By Feature

We find that browser features are not equally used on the web. Some features are extremely popular, such as the `Document.prototype.createElement` method, which allows sites to create new page elements. This feature is used on 9,079—or over 90%—of pages in the Alexa 10k.

Other browser features are never used. 689 features, or almost 50% of the 1,392 implemented in the browser, are never used in the 10k most popular sites. A further 416 features are used on less than 1% of the 10k most popular websites. Together, this means that over 79% of the features available in the browser are used by less than 1% of the web.

We also find that browser features do not have equal block rates; some features are blocked by advertisement and tracking-blocking extensions far more often than others. Ten percent of browser features are prevented from executing over 90% of the time when browsing with common blocking extensions. We also find that 1,159 features, or over 83% of features available in the browser, are executed on less than 1% of websites in the presence of popular advertising and tracking-blocking extensions.

#### 5.3 Standard Popularity vs. Site Popularity

Figure 4: Comparison of the percentage of sites using a standard versus the percentage of web traffic using a standard.

The results described in this paper give equal weight to all sites in the Alexa 10k. If the most popular and least popular sites use the same standard, both uses of that standard are given equal consideration. In this section, we examine the accuracy of this assumption by measuring the difference between the number of sites using a standard and the percentage of site visits using a standard.

Figure 4 shows the results of this comparison. The x-axis shows the percentage of sites that use at least one feature from a standard, and the y-axis shows the estimated percentage of site views on the web that use this standard. Standards above the x=y line are more popular on frequently visited sites, meaning that the percentage of page views using the standard is greater than the percentage of sites using the standard. A site on the x=y line indicates that the feature is used exactly as frequently on popular sites as on less popular sites.

Generally, the graph shows that standard usage is not equally distributed, and some standards are more popular with frequently visited sites. However, the general trend appears to be for standards to cluster around the x=y line, indicating that while there are some differences in standard usage between popular and less popular sites, they do not affect our general analysis of standard usage on the web. Therefore, for the sake of brevity and simplicity, all other measures in this paper treat standard use on all domains as equal and do not consider a site's popularity.

In addition to the datasets used in this paper, we have also collected data from even-less popular sites from the Alexa one-million, sites with rank less than 10k, to determine whether feature usage in less popular portions of the web differs significantly from feature usage patterns in the Alexa 10k. That measurement found no significant difference in feature usage.