This work uses two such browser extensions, Ghostery
and AdBlock Plus. Ghostery is a browser extension that
allows users to increase their privacy online by modifying
the browser to not load resources or set cookies associated
with cross-domain passive tracking, as determined by the
extension’s maintainer, Ghostery, Inc..
This work also uses the AdBlock Plus browser extension,
which modiﬁes the browser to not load resources the extension
associates with advertising, and to hide elements in the page
that are advertising related. AdBlock Plus draws from a
crowdsourced list of rules and URLs to determine if a resource
is advertising-related.
This work uses the default conﬁguration for each browser
extension, including the default rule sets for which elements
and resources to block. No changes were made to the conﬁg-
uration or implementation of either extension.
4. METHODOLOGY
To understand browser feature use on the open web, we
conducted a survey of the Alexa 10k, visiting each site ten
times and recording which browser features were used. We
visited each site ﬁve times with an unmodiﬁed browsing en-
vironment, and ﬁve times with popular tracking-blocking
and advertising-blocking extensions installed. This section
describes the goals of this survey, followed by how we instru-
mented the browser to determine which features are used
on a given site, and then concludes with how we used our
instrumented browser to measure feature use on the web in
general.
4.1 Goals
The goal of our automated survey is to determine which
browser features are used on the web as it is commonly
experienced by users. This requires us to take a broad-
yet-representative sample of the web, and to exhaustively
determine the features used by those sites.
To do so, we built a browser extension to measure which
features are used when a user interacts with a website. We
then chose a representative sample of the web to visit. Finally,
we developed a method for interacting with these sites in
an automated fashion to elicit the same functionality that
a human web user would experience. Each of these steps is
described in detail in the proceeding subsections.
This automated approach only attempts to measure the
“open web”, or the subset of webpage functionality that a
user encounters without logging into a website. Users may
encounter diﬀerent types of functionality when interacting
with websites they have created accounts for and established
relationships with, but such measurements are beyond the
scope of this paper. We note this restriction, only measuring
functionality used by non-authenticated portions of websites,
as a limitation of this paper and a possible area for future
work.
4.2 Measuring Extension
We instrumented a recent version of the Firefox web
browser (version 46.0.1) with a custom browser extension
to records each time a JavaScript feature has been used on
a visited page. Our extension injects JavaScript into each
Figure 2: One iteration of the feature invocation measure-
ment process.
page after the browser has created the DOM for that page,
but before the page’s content has been loaded. By injecting
our instrumenting JavaScript into the browser before the
page’s content has been fetched and rendered, we can modify
the methods and properties in the DOM before it becomes
available to the requested page.
The JavaScript that the extension injects into each re-
quested page modiﬁes the DOM to count when an instru-
mented method is called or that an instrumented property
is written to. How the extension measures these method
calls and property writes is detailed in the following two
subsections. Figure 2 presents a representative diagram of
the crawling process.
4.2.1 Measuring Method Calls
The browser extension counts when a method has been
invoked by overwriting the method on the containing object’s
prototype. This approach allows us to shim in our own
logging functionality for each method call, and then call the
original method to preserve the original functionality. We
replace each reference to each instrumented method in the
DOM with an extension managed, instrumented method.
We take advantage of closures in JavaScript to ensure that
web pages are not able to bypass the instrumented methods
by looking up–or otherwise directly accessing–the original
versions of each method.
4.2.2 Measuring Property Writes
Properties were more diﬃcult to instrument. JavaScript
provides no way to intercept when a property has been set or
read on a client script-created object, or on an object created
after the instrumenting code has ﬁnished executing. However,
through the use of the non-standard Object.watch()[39]
method in Firefox, we were able to capture when pages
set properties on the singleton objects in the browser (e.g.
window, window.document, window.navigator). Using this
Object.watch() method allowed the extension to capture
and count all writes to properties on singleton objects in the
DOM.
There are a small number of features in the DOM where
we were not able to intercept property writes. We were
therefore unable to count how frequently these features were
used. These features, found primarily in older standards,
are properties where writes trigger side eﬀects on the page.
100The most signiﬁcant examples of such properties are docu-
ment.location (where writing to the property can trigger
page redirection) and Element.innerHTML (where writing
to the property causes the subtree in the document to be
replaced). The implementation of these features in Firefox
make them unmeasurable using our technique. We note them
here as a small but signiﬁcant limitation of our measurement
technique.
4.2.3 Other Browser Features
Web standards deﬁne other features in the browser too,
such as browser events and CSS layout rules, selectors, and
instructions. Our extension-based approach did not allow us
to measure the use of these features, and so counts of their
use are not included in this work.
In the case of standard deﬁned browser events (e.g. onload,
onmouseover, onhover) the extension could have captured
some event registrations through a combination of watching
for event registrations with addEventListener method calls
and watching for property-sets to singleton objects. However,
we would not have been able to capture event registrations
using the legacy DOM0 method of event registration (e.g. as-
signing a function to an object’s onclick property to handle
click events) on non-singleton objects. Since we would only
have been able to see a subset of event registrations, we
decided to omit events completely from this work.
Similarly, this work does not consider non-JavaScript ex-
posed functionality deﬁned in the browser, such as CSS
selectors and rules. While interesting, this work focuses
solely on functionality that the browser allows websites to
access though JavaScript.
4.3 Eliciting Site Functionality
Using our feature-detecting browser extension, we were
able to measure which browser features are used on the 10k
most popular websites. The following subsections describe
how we simulated human interaction with web pages to
measure feature use, ﬁrst with the browser in its default
state, and again with the browser modiﬁed with popular
advertising and tracking blocking extensions.
4.3.1 Default Case
To understand which features are used in a site’s execution,
we installed the instrumenting extension described in Section
4.2 and visited sites from the Alexa 10k, with the goal of
exercising as much of the functionality used on the page as
possible. While some JavaScript features of a site are auto-
matically activated on the home page (e.g. advertisements
and analytics), many features will only be used as a result
of user interaction, either within the page or by navigating
to diﬀerent areas of the site. Here we explain our strategy
for crawling and interacting with sites.
In order to trigger as many browser features as possible on
a website, we used a common site testing methodology called
“monkey testing”. Monkey testing refers to the strategy of
instrumenting a page to click, touch, scroll, and enter text
on random elements or locations on the page. To accomplish
this, we use a modiﬁed version of gremlins.js [62], a library
built for monkey testing front-end website interfaces. We
modiﬁed the gremlins.js library to allow us to distinguish
between when the gremlins.js script uses a feature, and when
the site being visited uses a feature. The former feature
usage is omitted from the results described in this paper.
We started our measurement by visiting the home page of
site and allowing the monkey testing to run for 30 seconds.
Because the randomness of monkey testing could cause navi-
gation to other domains, we intercepted and prevented any
interactions which might navigate to a diﬀerent page. For
navigations that would have been to the local domain, we
noted which URLs the browser would have visited in the
absence of the interception.
We then proceeded in a breadth ﬁrst search of the site’s
hierarchy using the URLs that would have been visited by
the actions of the monkey testing. We selected 3 of these
URLs that were on the same domain (or related domain, as
determined by the Alexa data), and visited each, repeating
the same 30 second monkey testing procedure and recording
all used features. From each of these 3 sites, we then visited
three more pages for 30 seconds, which resulted in a total of
13 pages interacted with for a total of 390 seconds per site.
If more than three links were clicked during any stage
of the monkey testing process, we selected which URLs to
visit by giving preference to URLs where the path structure
of the URL had not been previously seen. In contrast to
traditional interface fuzzing techniques, which have as a
goal ﬁnding unintended or malicious functionality [5, 35], we
were interested in ﬁnding all functionalities that users will
commonly interact with. By selecting URLs with diﬀerent
path-segments, we tried to visit as many types of pages
on the site as possible, with the goal of capturing all of
the functionality on the site that a user would encounter.
The robustness and validity our strategy are evaluated in
Section 6.
4.3.2 Blocking Case
In addition to the default case measurements described in
Section 4.3.1, we also re-ran the same measurements against
the Alexa 10k with an ad blocker (AdBlock Plus) and a
tracking-blocker (Ghostery) to generate a second, ‘blocking’,
set of measurements. We treat these blocking extensions as
representative of the types of modiﬁcations users make to
customize their browsing experience. While a so-modiﬁed
version of a site no longer represents its author’s intended
representation (and may in fact break the site), the popularity
of these content blocking extensions shows that this blocking
case is a common valid alternative experience of a website.
4.3.3 Automated Crawl
Domains measured
Total website interaction time
Web pages visited
Feature invocations recorded
9,733
480 days
2,240,484
21,511,926,733
Table 1: Amount of data gathered regarding JavaScript
feature usage on the Alexa 10k. “Total website interaction
time” is an estimate based on the number of pages visited
and 30 seconds of page interaction per visit.
For each site in the Alexa 10k, we repeated the above
procedure ten times to ensure we measured all features used
on the page, ﬁrst ﬁve times in the default case, and then
again ﬁve times in the blocking case. By parallelizing this
crawl with 64 Firefox installs operating over 4 machines, we
were able to complete the crawl in two days.
101We present ﬁndings for why ﬁve times is suﬃcient to induce
all types of site functionality in each test case in Section 6. Ta-
ble 1 presents some high level ﬁgures of this automated crawl.
For 267 domains, we were unable to measure feature usage
for a variety of reasons, including non-responsive domains
and sites that contained syntax errors in their JavaScript
code that prevented execution.
5. RESULTS
In this section we discuss our ﬁndings, including the pop-
ularity distribution of JavaScript features used on the web
with and without blocking, a feature’s popularity in relation
to its age, which features are disproportionately blocked, and
which features are associated with security vulnerabilities.
5.1 Deﬁnitions
This work uses the term feature popularity to denote
the percentage of sites that use a given feature at least once
during automated interaction with the site. A feature that is
used on every site has a popularity of 1, and a feature that
is never seen has a popularity of 0.
Similarly, we use the term standard popularity to de-
note the percentage of sites that use at least one feature from
the standard at least once during the site’s execution.
Finally, we use the term block rate to denote how fre-
quently a feature would have been used if not for the presence
of an advertisement- or tracking-blocking extension. Browser
features that are used much less frequently on the web when
a user has AdBlock Plus or Ghostery installed have high
block rates, while features that are used on roughly the same
number of websites in the presence of blocking extensions
have low block rate.
5.2 Standard Popularity
In this subsection, we present measurements of the popu-
larity of the standards in the browser, ﬁrst in general, then
followed by comparisons to the individual features in each
standard, the popularity of sites using each standard, and
when the standard was implemented in Firefox.
5.2.1 Overall
Figure 3 displays the cumulative distribution of standard
popularity. Some standards are extremely popular, and
others are extremely unpopular: six standards are used on
over 90% of all websites measured, and a full 28 of the 75
standards measured were used on 1% or fewer sites, with
eleven not used at all. Standard popularity is not feast or
famine however, as standards see several diﬀerent popularity
levels between those two extremes.
5.2.2 Standard Popularity By Feature
We ﬁnd that browser features are not equally used on
the web. Some features are extremely popular, such as the
Document.prototype.createElement method, which allows
sites to create new page-elements. The feature is used on
9,079–or over 90%–of pages in the Alexa 10k.
Other browser features are never used. 689 features, or
almost 50% of the 1,392 implemented in the browser, are
never used in the 10k most popular sites. A further 416
features are used on less than 1% of the 10k most popular
websites. Put together, this means that over 79% of the
Figure 3: Cumulative distribution of standard popularity
within the Alexa 10k.
features available in the browser are used by less than 1% of
the web.
We also ﬁnd that browser features do not have equal
block rates; some features are blocked by advertisement and
tracking blocking extensions far more often than others. Ten
percent of browser features are prevented from executing
over 90% of the time when browsing with common blocking
extensions. We also ﬁnd that 1,159 features, or over 83% of
features available in the browser, are executed on less than
1% of websites in the presence of popular advertising and
tracking blocking extensions.
5.3 Standard Popularity vs. Site Popularity
Figure 4: Comparison of percentage of sites using a standard
versus percentage of web traﬃc using a standard.
The results described in this paper give equal weight to
all sites in the Alexa 10k. If the most popular and least pop-
ular sites use the same standard, both uses of that standard
are given equal consideration. In this section we examine
the accuracy of this assumption by measuring the diﬀer-
0%25%50%75%100%025005000750010000Sites using a standardPortion of all standards●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●DOM4DOM−PSH−HITC0%25%50%75%100%0%25%50%75%Portion of all websitesPortion of all website visits102ence between the number of sites using a standard, and the
percentage of site visits using a standard.
Figure 4 shows the results of this comparison. The x-axis
shows the percentage of sites that use at least one feature from
a standard, and the y-axis shows the estimated percentage
of site views on the web that use this standard. Standards
above the x=y line are more popular on frequently visited
sites, meaning that the percentage of page views using the
standard is greater than the percentage of sites using the
standard. A site on the x=y line indicates that the feature is
used exactly as frequently on popular sites as on less popular
sites.
Generally, the graph shows that standard usage is not
equally distributed, and that some standards are more popu-
lar with frequently visited sites. However, the general trend
appears to be for standards to cluster around the x=y line,
indicating that while there are some diﬀerences in standard
usage between popular and less popular sites, they do not
aﬀect our general analysis of standard usage on the web.
Therefore, for the sake of brevity and simplicity, all other
measures in this paper treat standard use on all domains as
equal, and do not consider a site’s popularity.
In addition to the datasets used in this paper, we have
also collected data from even-less popular sites from the
Alexa one-million, sites with rank less than 10k, to determine
whether feature usage in less popular portions of the web
diﬀers signiﬁcantly from feature usage patterns in the Alexa
10k. That measurement found no signiﬁcant diﬀerence in