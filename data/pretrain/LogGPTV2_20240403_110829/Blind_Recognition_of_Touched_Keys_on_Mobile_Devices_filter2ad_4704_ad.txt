the target device). For the second group, ﬁve videos were taken
for every person per angle. Discarding 3 videos not recording the
whole touching process, we obtain 192 videos totally. During the
experiments, users tap in their own way without any intervention.
Angles and Distance: To measure the impact of the angle, we
placed the target in front, on the left (3 o’clock) and on the right
(9 o’clock) of the camera. In the ﬁrst two groups of experiments,
the camera was 2.1 meters (m) to 2.4m away from and around 0.5m
above the device. To test how the distance affects the recognition
results, we also positioned the camera, the Logitech HD Pro Web-
cam C920, in front of the target device, an iPad, at a distance of 2m,
3m, 4m and 5m, and approximately one meter above the target.
Lighting: The lighting affects the brightness and contrast of
the image. The experiments are performed in a classroom with
dimmable lamps on the ceiling. The ﬁrst group of videos were tak-
en under normal lighting and the second group of experiments were
taken under strong lighting. All other experiments were performed
under normal lighting. Darkness actually helps the attack since the
touch screen is brighter in the dark. We did not consider these easy
dark scenes in our experiments.
Table 5: Success Rate by Baseline Method
Success Rate
Front
Average
26.66% 29.03% 22.22% 26.13%
Right
Left
From now on, we present experiment results using the seven-step
recognition method, referred to as Automatic Apprpach (AA), in-
troduced in Section 3. We also use a metric called the Best Effort
Approach (BEA) success rate, which is derived by giving a sec-
ond attempt for correcting a wrong recognition with some human
intervention. The BEA is performed in the following way. We of-
ten see one or two wrong keys in the failed experiments. Some of
these wrong keys are caused by DPM that fails to detect the touch-
ing ﬁngertip. Sometimes, even if the touching ﬁngertip is detected,
the image can be so blurry that pixels around the touching ﬁngertip
have almost the same color and it is difﬁcult to derive the ﬁnger-
tip contour in Figure 9. Other ﬁngers may also block the touching
ﬁnger and incur wrong recognition of the touching ﬁngertip top.
Therefore, we often know which key might be wrong and give them
a second try. We manually select the small bounding box of the ﬁn-
gertip in Figure 8 or the touched area in Figure 10 to correct such
errors. As analyzed in Section 4, for each touch we may also pro-
duce two candidates. Using one of the two choices, we may correct
the wrong keys in the second time try. Therefore, the BEA success
rate is higher than the AA success rate.
Table 6 gives the success rate of recognizing touched keys from
videos taken at different angles. Recall that the success rate is the
ratio of the number of the correctly recognized passcodes (all four
digits or characters) over the number of passcodes. For the wrong
results, we give a second attempt by applying the Best Effort Ap-
proach. It can be observed that the overall AA success rate reaches
more than 80%. The success rate for videos taken from the left and
right is a little lower because there are some quite blurry videos,
which are difﬁcult to analyze for Step 6. The BEA success rate is
higher than the AA success rate and reaches over 90%. The per
digit success rate is deﬁned as the ratio between the number of cor-
rectly retrieved digits and the number of all the digits. The per digit
success rate for BEA is more than 97%.
5.2 Detecting Touching Frames via Optical Flow
Table 6: Success Rate by Clustering Based Matching
As discussed in Section 3.4, we track a hand’s feature points and
use their velocity change to detect touching frames. Our experi-
ments show that 5 or more feature points are stable for tracking
touching frames with a true positive of 100%, as shown in Table 4.
The optical ﬂow algorithm may also produce false positives, falsely
recognizing frames in which a ﬁnger does not touch the screen sur-
face as touching frames. The false positive rate is very low, less than
1% as shown in Table 4. One way to reduce the false positive is to
use DPM. Our experiments show that DPM is able to detect touch-
ing frames since no ﬁngers touch the screen in non-touching frames
and DPM only recognizes touching ﬁngers in touching frames. We
exclude the non-touching frames by DPM if the number of detected
touching frames is more than 4 by optical ﬂow.
Table 4: Performance of Detecting Touching Frames
True Positive
False Positive
Left
Front
100% 100% 100%
100%
0.91% 0.88% 0.88% 0.89%
Right Average
5.3 Recognizing Touched Keys on iPad via
Webcam
Table 5 shows the result of the baseline method for videos tak-
en from different viewpoints. Its overall success rate is less than
30%. Therefore, the baseline method is not very effective since
DPM cannot accurately locate the touched points.
Automatic Apprpach
Best Effort Approach
Per Digit for BEA
Left
Right
Front
Average
92.18% 75.75% 79.03 % 82.29%
93.75% 89.39% 90.32% 91.14%
98.04% 96.59% 97.58% 97.39%
Figure 17 presents the results of measuring the impact of the dis-
tance between the camera and the target on the success rate. It can
be observed that the trend is: as the distance increases, the success
rate decreases. At the distance of 4m or 5m, the AA success rate is
as low as 20%. This is because at such a distance the keys in the
image are so small that they are only 1 or 2 pixel wide. It is much
more difﬁcult to distinguish a touched key at such a distance. A
camera with a high optical zoom shall help. However, our threat
model does not allow the use of those high zoom cameras.
To test whether human can retrieve the passcodes easily, we asked
all people involved in the experiments for human based recovery.
Given the tiny software keyboard and no text or popup in the record-
ed video, nobody could get the whole 4-digit passcode right. And,
it is almost impossible for human to recognize keys on a QWERTY
keyboard given so many keys and small key size in a video.
5.4 Comparing Different Targets and Cameras
To compare the success rate of recognizing touched keys on d-
ifferent devices, we performed 30 experiments on Nexus 7 and i-
Phone 5 respectively with the Logitech HD Pro Webcam C920 from
1410In the case of multiple ﬁngers, the challenge is to recognize which
ﬁnger is the touching ﬁnger. In [43], touching is deﬁned as a ﬁnger-
tip hovering at a certain position or suddenly changing the moving
direction for another key.
In the case of touching with multiple
ﬁngers, this is not true any more. As shown in Figure 19, the non-
touching ﬁngertips also follow a similar pattern during the touching
process if both the little ﬁnger of the right hand and middle ﬁnger of
the left hand are used for typing. The strategy in [43] for deriving
the ﬁngertip’s moving trajectory cannot work directly if multiple
ﬁngertips are involved.
In Section 3.4 when we address the case of touching with one
ﬁnger, we use the ﬁnger’s velocity direction change to detect the
touching frames. The touching ﬁnger and other ﬁngers follow a
similar pattern of velocity direction change. The touching frame is
the one in which the majority of those ﬁngers change the velocity
from positive to negative. In the case of multiple ﬁngers, in order to
use the similar strategy, we have to differentiate and track the two
hands. The complicated background involving the two hands is a
great challenge for a general solution.
We address the challenges above by looking more closely at what
is touching. The ﬁnger movement registers a touch input only when
the ﬁngertip actually touches the screen. Therefore, we may detect
touching frames by detecting touching actions from the perspec-
tive of action detection [45] (also termed action localization [24],
or event detection [20]). A video can be modeled as a sequence
of frames captured along the time, as shown in Figure 20. We can
detect each touch action with SDPM (Spatiotemporal Deformable
Part Model [39]). One touch event usually involves several touch-
ing frames. Therefore, we can treat the touching action detection
problem as the problem of detecting a set of touching frames.
We use DPM to detect touching frames and localize the touch-
ing ﬁnger in the case of typing with multiple ﬁngers. As discussed
in Section 3.6, DPM can localize the touching ﬁngertip in touch-
ing frames effectively. Therefore, by applying DPM to all video
frames, we can detect and localize all the touching ﬁngertips in the
touching frames, excluding non touching ﬁngertips. The bounding
box in Figure 20 shows the detected touching ﬁngertip in the case
of typing with multiple ﬁngers. It is also observed that the touch
event involves a few consecutive touching frames along the t-axis
as shown in Figure 20.
Frame k+3
Frame k+2
Frame k+1
y
t
x
Figure 20: One Touch Event Involving Multiple Frames in a Video
For every touching action, we use the touching frame in the mid-
dle as the actual touching frame. Given the touching frame, we
adopt the same steps introduced in Section 4 to derive the touched
key. Figure 21 shows the mapped result (green dot) for the touch
event in Figure 20, where “U” is the touched key. From the discus-
sion above, we can also see that touching with one ﬁnger (or hand)
is a special case of touching with two hands and multiple ﬁngers.
To validate the attack against typing with multiple ﬁngers, we
performed 21 experiments with the web camera spying on the i-
Pad character keyboard, from distance about 2.2 meters away and
Figure 17: Success Rate v.s. Distance
two meters away from and about 0.65m above the device. To in-
vestigate the impact of different cameras, we conducted 30 exper-
iments using iPhone 5 to record passcode inputs on iPad, from a
similar distance and at a similar height. 30 experiments with the
Google glass recording passcode inputs on iPad were performed
two meters away and at a human height. Figure 18 presents the re-
sults. The AA success rate is more than 80%, and the BEA success
rate is more than 90% in all cases. The high success rate for all the
cases demonstrates the severity of our attack.
Figure 18: Success Rate Comparison
We also tested the effect of our attack on different kinds of key-
board:
the iPad QWERTY keyboard and iPhone QWERTY key-
board. The iPad QWERTY keyboard key is larger than the iPhone
QWERTY keyboard key. 30 experiments were done respectively
with the web camera from the front of the target from about 2.2
meters away and at a height of 0.6 meters. Figure 18 presents the
results. It can be observed that the AA success rate is over 80% and
the BEA success rate is over 90%!
6. DISCUSSION: TOUCHING WITH MUL-
TIPLE FINGERS AND TWO HANDS
As shown in Figure 19, people may type the iPad passcodes, or
their bank account passwords on a large keyboard (in Figure 21)
with multiple ﬁngers and two hands. In this section, we extend our
work in the previous sections and discuss the recognition of touch
input by two hands and multiple ﬁngers.
Figure 19: Touching Frame with Multiple Fingers (Magniﬁed)
1411In comparison with [43] on recognizing passwords, we can achieve
a much higher success rate. We extend our work to the scenario of
touching with both hands and multiple ﬁngers while such a scenario
is not addressed in [43].
Figure 21: Multi Touching Mapped Result
8. CONCLUSION
0.65 meters above the device. We achieve the AA success rate of
95.24%. In the experiments, only one touch event was not correctly
detected. If we manually retrieve the touching frame for that touch
event as introduced in previous sections, we get the BEA success
rate of 100% to retrieve the character passcode by two hands and
multiple ﬁngers. This demonstrates the correctness and severity of
the attack introduced in this paper.
7. RELATED WORK
In this paper, we exploit the movement of the touching ﬁnger to
infer the input on a touch screen.
It is one type of side channel
attack. There are various similar attacks on touch-enabled devices
exploiting different hidden venues. Because of the space limit, we
discuss the most related work on side channels using computer vi-
sion knowledge. Backes et al. [2, 1] exploit the reﬂections of a
computer monitor on glasses, tea pots, spoons, plastic bottles, and
eyes of the user to recover what is displayed on the computer mon-
itor. Their tools include a SLR digital camera Canon EOS 400D, a
refractor telescope and a Newtonian reﬂector telescope, which can
successfully spy from 30 meters away.
Balzarotti et al. propose an attack retrieving text typed on a phys-
ical keyboard from a video of the typing process [3]. When keys are
pressed on a physical keyboard, the light diffusing surrounding the
key’s area changes. Contour analysis is able to to detect such a key
press. They employ a language model to remove noise. They as-
sume the camera can see ﬁngers typing on the physical keyboard.
Maggi et al. [28] implement an automatic shoulder-surﬁng attack
against touch-enabled mobile devices. The attacker employs a cam-
era to record the victim tapping on a touch screen. Then the stream
of images are processed frame by frame to detect the touch screen,
rectify and magnify the screen images, and ultimately identify the
popping up keys. Raguram et al. exploit refections of a device’s
screen on a victim’s glasses or other objects to automatically in-
fer text typed on a virtual keyboard [33]. They use inexpensive
cameras (such as those in smartphones), utilize the popup of keys
when pressed and adopt computer vision techniques processing the
recorded video in order to infer the corresponding key although the
text in the video is illegible.
Xu et al. extend the work in [33] and track the ﬁnger movement
to infer input text [43]. Their approach has six stages: in Stage 1,
they use a tracking framework based on AdaBoost [13] to track the
location of the victim device in an image. In Stage 2, they detect
the device’s lines, use Hough transform to determine the device’s
orientation and align a virtual keyboard to the image. In Stage 3,
they use Gaussian modeling to identify the “ﬁngertip” (not touched
points as in our work) by training the pixel intensity. In Stage 4,
RANSAC is used to track the ﬁngertip trajectory, which is a set
of line segments. If a line segment is nearly perpendicular to the
touch screen surface, it implicates the stopping position. In Stage 5,
they apply image recognition techniques to determine which keys
are most likely pressed given the stopping positions. In Stage 6,
they apply a language model to optimize the result given the candi-
date keys and associated conﬁdence values from the previous stage.
They use two cameras: Canon VIXIA HG21 camcorder with 12x
optical zoom and Canon 60D DSLR with 400mm lens.
In this paper, we present a computer vision based attack that
blindly recognizes inputs on a touch screen from a distance auto-
matically. The attack exploits the homography relationship between
the touching images (in which ﬁngers touch the screen surface) and
the reference image of a software keyboard. We use the optical ﬂow
algorithm to detect touching frames. The deformable part-based
model (DPM) and various computer vision techniques are utilized
to track the touching ﬁngertip and identify the accurate touched
area. We carefully analyze the image formation of the touching
ﬁngertip and design the k-means clustering strategy to recognize
the touched points. The homography is then applied to recognize
the touched keys. Our extensive experiments show that the AA suc-
cess rate of recognizing touched keys is more than 80%, while the
BEA success rate is more than 90%. We have also extended the
attack to the case of typing with two hands and multiple ﬁngers
and achieve a high success rate of more than 95%. As a counter-
measure, we design a context aware Privacy Enhancing Keyboard
(PEK) which pops up a randomized keyboard on Android systems
for sensitive information input such as passwords. Our future work