to 
As  shown  in  Table  2,  aggregating  operations  to  create 
bigger  transactions  increases  the  number  of  writes  linearly. 
However,  the  number  of  unique  pages  modified  by  the 
operations remains unchanged in most cases. This is because 
most of the operations modify data on the same set of pages. 
This  implies  that  aggregating  operations  to  create  bigger 
transactions  should  benefit  page-tracking  based  approaches, 
because of their heavy dependence on PPT. Emulation-based 
approaches which are independent of PPT, but dependent on 
Figure 11: Workload performance 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:43:59 UTC from IEEE Xplore.  Restrictions apply. 
379WPP,  will  have  the  same  performance  as  before.  In  this 
subsection we evaluate bigger transactions where OPT=5. 
Figure 14: Speedup with hypervisor assistance (OPT=5) 
Figure  14  shows  the  speedup  of  hypervisor  assisted 
approaches over their user-level counterparts for OPT=5. As 
expected, the results are similar to the case of OPT=1 shown 
in  Figure  12.  This  is  because  both  PT  and  PTxen  get  an 
improvement  of  5  times  with  transaction  aggregation, 
making  their  relative  performance  same  as  in  the  case  of 
OPT=1.    Emulxen  shows  a  speedup  of  up  to  4  times  and 
PTxen  shows  a  speedup  of  up  to  13  times  over  their  user-
level counterparts. 
C.  Data Processing Overhead 
At  the  end  of  the  checkpoint  cycle,  the  modified  blocks 
of  critical  data  area  need  to  be  either  stored  to  disk  or 
transferred  to  the  backup  machine  over  a  network.  Page 
tracking  and  emulation-based  techniques  have  to  process 
different amounts of data. In our experiments reported here, 
we consider the case where OPT=1. Transaction aggregation, 
as  noted  earlier,  would  give  better  performance  (i.e.,  lower 
data  processing  overhead) 
tracking-based 
approaches. 
for  page 
Figure 13: Data structure performance with OPT=5 
As  shown  in  Figure  13,  in  most  cases,  the  user  level 
implementation  of  the  page  tracking  based  approach  (PT) 
surpasses  the  performance  of  emulation-based  user-level 
approach  (Emul).  This  is  in  contrast  to  Figure  11,  where 
Emul out-performed PT in a large number of cases, showing 
the  effect  of  decreased  PPT  on  the  performance  of  page-
tracking  based  approaches.    As  compared  to  Figure  11, 
graphs  in  Figure  13  for  page-tracking  based  approaches 
show  a  speedup  of  five  times,  whereas  emulation  based 
approaches do not show any difference in performance. 
Figure 15: Amount of data processed: Page-tracking 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:43:59 UTC from IEEE Xplore.  Restrictions apply. 
380we  see  that  the  results  of  Section   VI.A  are  still  valid:  in 
effect,  PTxen  has  overall  the  best  performance.  More 
importantly, 
are 
significantly better than the user-space approaches.  
hypervisor-assisted 
the 
approaches 
Figure 17: Total time for each approach 
It  is  instructive  to  compare  the  overall  improvement  in 
performance  due  to  hypervisor-assistance,  taking  memory 
copy overhead also into account. This is shown in Figure 18. 
We observe that PTxen improves in performance over PT by 
approximately  a  factor  of  8,  while  Emulxen  improves  over 
Emul  by  approximately  a  factor  of  4.  This  is  a  little  lower 
than the improvements seen in Figure 12 due to the constant 
overhead  of  memory  copy.  Transaction  aggregation  (e.g., 
OPT=5) will clearly increase the benefits of PTxen over PT, 
since  page  reuse  within  a  larger  transaction  will  reduce  the 
amount of data copied.    
Figure 16: Amount of data processed: Emulation 
Figure  15  and  Figure  16  show  the  amount  of  data  that 
each  technique  handles.  The  amount  of  data  processed  by 
page-tracking  based  approaches  is  of  the  order  of  100s  of 
MB.  In  contrast,  for  emulation-based  approaches  in  most 
cases the data to be processed is less than 2MB (Note that in 
case of tree-insert, the value of 56MB is too large to show in 
scale.) 
Typical  implementations  do  not  have  the  main  process 
handle  the  data  processing.  The  job  of  writing  to  disk  or 
copying  over  to  the  backup  is  typically  done  by  another 
thread or  helper process. In  multi-core  machines,  the  helper 
process can run in parallel on an additional CPU core. In this 
case,  the  main  process  just  copies  the  data  (dirty  pages  or 
changed bytes) into a shared buffer. The helper process runs 
in parallel without stalling the main process. In this case, the 
only additional overhead incurred by the main application is 
in  copying  the  modified  data  to  the  helper  process.  The 
helper process can either save the modified data as-is, or can 
do 
(e.g.  difference 
computation  for  page-tracking  based  approaches,  potential 
data compression, encryption for security). In this work, we 
focus  on  and  evaluate  the  overhead  incurred  by  the  main 
application (namely, the cost of a memory copy).   
further  processing  on 
the  data 
In  the  case of page-tracking based  approaches, the  main 
application  incurs  the  overhead  of  copying  the  modified 
pages  to  the  helper  process.  Since  it  does  not  keep  track  of 
changes made within the page, it needs to copy the dirty page 
as  a  whole  to  the  helper.  In  contrast,  the  emulation-based 
approaches  keep 
the  word 
granularity, so the application in this case needs to copy only 
the modified words to the helper.  
track  of  modifications  at 
Figure  17  shows  total  time  spent  (time  for  the  10,000 
operations and  the data copy  to the  helper  process)  by  each 
approach, again for the case where OPT=1. We note that in 
most  cases,  emulation-based  approaches  take  less  than  5ms 
(average  <  1ms)  whereas  page-tracking  based  approaches 
have a higher overhead in time ranging from 10ms to 80ms. 
However, when we look at the total time metric in Figure 17, 
Figure 18: Net speedup of hypervisor-assisted over user 
space approaches 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:43:59 UTC from IEEE Xplore.  Restrictions apply. 
381VII.  CONCLUSION 
In 
this  paper,  we  discussed  application-assisted  
checkpointing in virtualized environments. We identified the 
root  cause  of  the  performance  bottleneck  of  application 
checkpointing  under  virtualization.  To  overcome 
this 
bottleneck,  we  introduced  the  notion  of  hypervisor-assisted 
application  checkpointing.  Our  approach  implements  key 
primitives 
the 
hypervisor. Additionally, our approach introduces the notion 
of  direct  and  secure  application-to-hypervisor  interaction 
allowing deployment with no changes to the guest operating 
system.  Our  techniques  can  also  be  applied  to  non-
virtualized  environments  by  incorporating  them  into  the  OS 
instead of the hypervisor.  
for  application  checkpointing  within 
We  have  designed  and 
implemented  a  family  of 
application  checkpointing  techniques.  Our  techniques  are 
very  lightweight  and  can  be  implemented  with  minimal 
code; e.g. our prototype for the Xen hypervisor added a few 
hundred lines of code totaling about 0.2% of the hypervisor 
code.    We  have  introduced  emulation-based  techniques  that 
are  useful  for  small  transactions.  Page  tracking  approaches 
with hypervisor assistance show the best result. Compared to 
user-space 
hypervisor-assisted 
application  checkpointing  shows  impressive  performance 
gains  of  4x~10x  based  on  microbenchmark  results  and 
4x~13x based on results from our workload evaluation.  
implementations, 
our 
REFERENCES 
[1]  E.N. Elnozahy, L.  Alvisi, Y-M. Wang, and D.B. Johnson, "A survey 
of  rollback-recovery  protocols  in  message-passing  systems",  ACM 
Comput. Surv., vol. 34, no. 3, pp. 375-408, 2002. 
[2]  Yi-Min  Wang,  Yennun  Huang,  Kiem-Phong  Vo,  Pe-Yu  Chung,  C. 
Kintala,  "Checkpointing  and 
Its  Applications,",  Twenty-Fifth 
International  Symposium  on  Fault-Tolerant  Computing  (FTCS), 
1995, Pasadena, CA. 
[3]  Plank,  J.S.   and  Kai  Li,   “Libckpt:  Transparent  Checkpointing  under 
Unix'',  Conference  Proceedings,  Usenix  Winter  1995  Technical 
Conference, New Orleans, LA, January, 1995. 
[4] 
Jason  Ansel,  Kapil  Arya,  and  Gene  Cooperman,  “DMTCP: 
Transparent  Checkpointing  for  Cluster  Computations  and 
the 
Desktop” 23rd IEEE International Parallel and Distributed Processing 
Symposium (IPDPS'09), Rome, Italy, May, 2009. 
[5]  Michael Litzkow, Todd Tannenbaum, Jim Basney, and Miron Livny, 
“Checkpoint  and  migration  of  UNIX  processes  in  the  Condor 
distributed processing system”. Technical Report CS-TR-199701346, 
University of Wisconsin, Madison, 1997. 
[8] 
J.  Janakiraman,  J.  R.  Santos,  D.  Subhraveti,  and  Y.  Turner,  “Cruz: 
Application-Transparent Distributed Checkpoint- Restart on Standard 
Operting Systems”. In Proceedings of the International Conference on 
Dependable  Systems  and  Networks  (DSN’05),  Yokohama,  Japan, 
June 2005. 
[9]  K. M. Chandy and L.  Lamport, “Distributed Snapshots: Determining 
Global  States  of  Distributed  Systems”.  ACM  Transactions  on 
Computer Systems, 3(1):63–75, Feb. 1985. 
[10]  G. Deconinck ,  J. Vounckx ,  R. Lauwereins ,  J. A. Peperstraete, “A 
User-Triggered  Checkpointing  Library  for  Computation-Intensive 
Applications”,  In  Proceedings  of  7th  IASTED-ISMM  International 
Conference  On  Parallel  and  Distributed  Computing  and  Systems 
(IASTED, Anaheim-Calgary-Zurich) (ISCC97). 
[11]  L.M.  Silva  and  J.G.  Silva,  “System-Level  Versus  User-Defined 
Checkpointing”,  SRDS  '98  Proceedings  of  the  The  17th  IEEE 
Symposium on Reliable Distributed Systems. 
[12]  Junyoung  Heo  ,  Sangho  Yi  ,  Yookun  Cho  ,  Jiman  Hong  ,  Sung  Y. 
Shin,  “Space-efficient  page-level 
incremental  checkpointing”, 
Proceedings  of  the  2005  ACM  symposium  on  Applied  computing, 
March 13-17, 2005, Santa Fe, New Mexico.   
[13]  Thomas  C.  Bressoud, 
Fault-tolerance”,  
Proceedings  of  the  15th    ACM  symposium  on  operating  systems 
principles, Vol. 29, No. 5. (December 1995), pp. 1-11. 
“Hypervisor-based 
[14]  Kernel-based  Virtual  Machine  (KVM)  for  Linux,  http://www.linux-
kvm.org, Last accessed on April 12, 2011. 
[15]  VMware 
vSphere 
–  VMware 
virtualization 
platform,   
http://www.vmware.com/products/vsphere/overview.html, 
accessed on April 12, 2011. 
Last 
[16]  Xen  4.1,  “http://www.xen.org/files/Xen_4_1_Datasheet.pdf”  Last 
accessed on April 12, 2011. 
[17]  C. Clark et al, “Live Migration of Virtual Machines”, Proceedings of 
the  2nd  ACM/USENIX  Symposium  on  Networked  Systems  Design 
and Implementation (NSDI) 2005, pp. 273-286.  
[18]  L.  Wang,  Z.  Kalbarczyk,  R.K.  Iyer,  A.  Iyengar,  "Checkpointing 
virtual  machines  against  transient  errors,"  Proceedings  of  the  IEEE 
16th International On-Line Testing Symposium (IOLTS), pp.97-102, 
July 2010. 
[19]  Brendan  Cully  et  al.,  “Remus:  high  availability  via  asynchronous 
virtual  machine  replication”,  In  NSDI'08:  Proceedings  of  the  5th 
USENIX  Symposium  on  Networked  Systems  Design  and 
Implementation (2008), pp. 161-174. 
[20]  Y.  Tamura,  “Kemari:  Virtual  Machine  Synchronization  for  Fault 
Tolerance using DomT”, Xen Summit 2008, Boston, MA. 
[21]  A.W.  Appel  and  K.  Li,  “Virtual  memory  primitives  for  user 
programs”  ASPLOS-IV  Proceedings  of  the  fourth  international 
conference  on  Architectural  support for  programming  languages  and 
operating systems, 1991. 
[22]  S.  T.  Jones,  A.  C.  Arpaci-Dusseau,  and  R.  H.  Arpaci-Dusseau, 
“Antfarm:Tracking  processes  in  a  virtual  machine  environment”,  In 
Proc. USENIX Annual Technical Conference, 2006. 
[6]  Hua Zhong and Jason Nieh, “CRAK: Linux Checkpoint / Restart As a 
Kernel  Module”.  Technical  Report  CUCS-014-01.  Department  of 
Computer Science. Columbia University, November 2002. 
[23]  S.  T.  Jones,  A.  C.  Arpaci-Dusseau,  and  R.  H.  Arpaci-Dusseau, 
in  a  virtual  machine 
“Geiger:  Monitoring 
the  buffer  cache 
environment”, In Proc. ASPLOS-XII, 2006. 
[7]  Oren  Laadan  and  Jason  Nieh,  "Transparent  Checkpoint-Restart  of 
Multiple  Processes  on  Commodity  Operating  Systems",  Proceedings 
of the 2007 USENIX Annual Technical Conference, Santa Clara, CA, 
June 17-22, 2007,  pp. 323-336. 
[24]  “Xen 
for 
upstream 
paravirt_ops 
kernel”, 
http://wiki.xensource.com/xenwiki/XenParavirtOps, Last accessed on 
April 12, 2011. 
Linux 
[25]  Mark  A.  Weiss,  “Data  Structures  and  Algorithm  Analysis,”  Second 
Edition, Addison Wesley.   
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:43:59 UTC from IEEE Xplore.  Restrictions apply. 
382