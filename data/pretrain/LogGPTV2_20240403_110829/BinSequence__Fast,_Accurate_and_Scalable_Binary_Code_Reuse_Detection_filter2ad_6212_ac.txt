We might wrongly match the target path with a long path
in the reference CFG. So we decided to normalize the sim-
ilarity score by taking the target and the found path into
consideration. Recall that a path is a sequence of basic
blocks, and the self score of one basic block b can be cal-
culated as CompBB(b, b) using Algorithm 2. Then the self
score of a path is the sum of self scores of all the consist-
ing basic blocks. We then normalize the score between the
target path P and the found path Pf using the following
equation:
NormScore(P, Pf ) =
LCSScore
Score(P ) + Score(Pf )
where the LCSScore is the score obtained from the memo-
ization table δ and Score() is a function that returns the self
score of the given path.
We then choose the path with the highest normalized
score. By backtracking the memoization table δ, we can
obtain a mapping of basic blocks. In the example shown in
Figure 3 we can obtain 4 matching basic block pairs: basic
block 1 with 1, 2 with 2, 5 with 5 and 6 with 6 in the target
and reference, respectively.
3.6 Neighborhood Exploration
While we can continue to extract more paths from the
target function and match them in the reference function,
this is not eﬃcient. First, the path exploration process takes
time. Besides, when we explore certain target path in the
reference function, some of the basic blocks may have al-
ready been matched in previous paths and we cannot gain
much by rematching them. Inspired by the work in [24], we
decided to use a greedy, localized fuzzy matching approach
to extend the existing mapping. Because we already have
all the mappings from path exploration of the longest path,
there is a high chance that we can ﬁnd the correct basic
block mapping between two functions.
We ﬁrst put every matching basic block pair obtained from
path exploration into a priority queue based on their simi-
larity score. Then we choose the pair on the top, namely the
pair with the largest similarity score as our starting point to
initialize the search. We then explore the neighbors of the
chosen basic block pair. Note that for every basic block pair
in the queue, the two basic blocks have the same in-degree
or out-degree. We ﬁrst consider the successors of these two
basic blocks if they have the same out-degree. If they both
have only one successor, then we match their successors di-
(a) The CFGs of two versions of the
same functions and the grey nodes rep-
resent the longest path in the target
CFG
(b) The memoization table
Figure 3: An example of path exploration for two CFGs
To do the path exploration, we ﬁrst initialize the memo-
ization table δ and array σ. Then we insert the head node 1
of the candidate CFG to the working queue Q. We compare
node 1 with path P using the function LCS in Algorithm 3
and update the memoization table correspondingly. Notice
that here for the purpose of simplicity, we assume that the
matching score is either 1 or 0, while a true match has a
score of 1, otherwise 0. Since node 1 has two successors,
node 6 and 2, we insert them into Q and continue the explo-
ration. Assume we visit node 6 ﬁrst, then node 2. Node 6
has no successor, we then update the table δ for node 6, and
continue to work on node 2. Node 2 has two successors, node
6 and node 4. We also insert them into our working queue.
We ﬁrst work on node 6. Note that this is the second time
we insert node 6 into Q. The ﬁrst time its parent node is 1,
and the corresponding partial path is “1→6”, this time its
parent node is 2 and the partial path is “1→2→6”. We allow
the same node to be inserted into Q as long as they represent
diﬀerent execution paths. Node 6 has no successor. After
we ﬁnish comparing node 6 with every node in path P , the
working queue Q has only one element: node 4. We then
work on node 4. It is worth noting that although node 4
123456J1245R6rectly, unless it is inconsistent with the mapping we already
have. If they both have more than one successor, then we
leverage the Hungarian algorithm [21] to ﬁnd the best map-
ping between the two sets of successors that maximize the
sum of the similarity score. Similarly, if the found mapping
is inconsistent with the mapping we already have, we dis-
card the corresponding match but continue to check other
successors. We then do the same to their predecessors if
they have the same in-degree.
It is important to note that for those found mapping pairs,
the corresponding basic blocks in the pair do not necessarily
have the same in-degree or out-degree.
If they have the
same in-degree, we put them into the priority queue but
only explore their predecessors later, when they become the
element with the highest priority (similarity score) in the
queue. If they have the same out-degree, we explore their
successors. If neither their in-degree nor out-degree is the
same, we still allow these two basic blocks to be matched,
however, we do not put them into the priority queue.
In
other words, we do not explore their neighborhood, because
the likelihood of them being a correct match is relatively
lower. By doing this, we achieve a fuzzy matching between
the basic blocks of two functions. At the same time, if we
mismatched a pair of basic blocks, we still require these two
basic blocks to have the same in-degree or out-degree to
further examine and match their predecessors or successors.
As a result, the error would not propagate. On the other
hand, for basic blocks that are correctly matched, we could
explore their neighborhood in two directions eﬃciently.
We continue to do this until the priority queue is empty,
i.e., until there is no more neighbors to be explored, or all
the neighbors have diﬀerent in-degree and out-degree and
can not be further explored. We then leverage the obtained
matching basic block pairs to calculate the similarity be-
tween the target function and the reference function.
An assembly function can be looked at as a set of ba-
sic blocks, we then calculate the self score of a function
by adding the self scores of all the consisting basic blocks.
Given two functions, f and g, suppose γ is the set of all the
matching basic block pairs we obtained during path explo-
ration and neighborhood exploration, the similarity of these
two functions can be calculated as follows:
Similarity(f, g) =
∀(u,v)∈γ CompBB(u, v)
Score(f ) + Score(g)
2(cid:80)
where u, v are basic blocks, u ∈ f , v ∈ g and Score() is a
function that returns the self score of the given function.
4. FILTERING
We have introduced how to pairwise compare two func-
tions. However, we still need to address the scalability prob-
lem, especially when dealing with large data sets. Suppose
we have a function repository consisting of one million func-
tions, to ﬁnd similar functions to a given target function, we
have to compare the target function with every function in
the repository and rank the results. This is not eﬃcient as a
large number of functions are not similar to the target and
should not be compared.
To this end, we adopt a heuristic approach to prune the
search space by excluding functions that are not likely to
be matched. We designed two ﬁlters, based on the number
of basic blocks and function ﬁngerprint similarity threshold,
respectively.
4.1 Filtering By Number of Basic Blocks
The reason to ﬁlter by the number of basic blocks is straight-
forward. It is very unlikely that a function with only one
basic block can be matched to another function with one
hundred basic blocks. Thus we set a number threshold. If
we require two CFGs to be exactly the same, namely iso-
morphism, then the numbers should also be the same. Since
BinSequence performs a fuzzy matching, which allows two
structurally-diﬀerent functions to be matched, the numbers
of basic blocks should be allowed to be diﬀerent. Thus we
set up a threshold. This threshold should not be too small,
as we may rule out the correct match. On the other hand,
the threshold should not be too large, otherwise we can not
save much time as not many functions can be ruled out.
Assume the threshold is γ, given a target function f , then
those functions whose sizes are between |f| − γ and |f| + γ
will pass this ﬁlter.
4.2 Filtering By Fingerprint Similarity
The next ﬁlter is based on the syntactic property of the
code. For every function, we use its normalized instruction
set as its ﬁngerprint. More speciﬁcally, we use the same
technique as introduced in section 3.1 to normalize all the
instructions inside a function, to get the normalized instruc-
tion set. Given a target function, we then calculate the
Jaccard similarity (index) between the ﬁngerprints of the
target and every function in the repository. If the Jaccard
similarity is above a certain threshold, we then continue to
compare the function against the target. Otherwise we sim-
ply discard it.
In order to avoid pairwise comparison of ﬁngerprints, we
leveraged Minhashing [9] and the banding technique [18].
Minhashing is a technique of using k diﬀerent hash func-
tions to generate the minhash signature. The banding tech-
nique divides the minhash signature into b bands of r rows
each. Given a target function, we ﬁrst generate its ﬁnger-
print and the minhash signature of its ﬁngerprint. We divide
its minhash signature into b bands of r rows, each. Then the
candidate set should be all the functions whose minhash sig-
natures agree in all the rows of at least one band with the
signature of the target function. More generally, if we choose
n hash functions, b bands, r rows and n = br, the Jaccard
similarity threshold t imposed by this banding technique is
approximately 1/b1/r [18].
In general, similar to the ﬁlter using number of basic
blocks, this ﬁlter is lossy as well. Some true matches may
have signiﬁcantly diﬀerent normalized instruction set, and
consequently, fail to pass this ﬁlter. To address this problem,
in our implementation, we choose b and r so that t = 1/b1/r
equals to a relatively low value, e.g., 0.65, so that those
functions that are true matches, but with low Jaccard simi-
larity could pass this ﬁlter and remain in the candidate set.
After using these techniques, to root out all the functions
whose Jaccard similarity is above certain threshold, we only
need to ﬁrst choose b and r so that the desired threshold is
imposed by the banding technique. and then select all the
functions whose minhash signatures agree in all the row of
at least one band with the signature of the target function,
which can be achieved by one time lookup in the database.
5. EVALUATION
We conducted extensive experiments to evaluate BinSe-
quence in terms of accuracy, performance and scalability.
We also performed several experiments on practical scenar-
ios to demonstrate the eﬀectiveness and eﬃciency of Bin-
Sequence when applied to real-world use cases. All experi-
ments were performed on a PC with an Intel Xeon E31220
Quad-Core processor, 16 GB of RAM running Microsoft
Windows 7 64-bit.
5.1 Function Reuse Detection
The ﬁrst experiment is function reuse detection from a
large repository. We ﬁrst try to perform function reuse de-
tection between two versions of the same binary.
In this
experiment, four diﬀerent versions of zlib libraries, namely
1.2.5 through 1.2.8 were used. The reason to choose zlib
library is it is widely used in many software and operat-
ing systems. Since zlib is a well maintained library, we
assumed that functions with identical function (symbolic)
name across diﬀerent versions should have the same or sim-
ilar functionality, and thus, should be matched. We also
introduced one group of noise functions, which are all the
functions of 1,701 system dynamic library ﬁles obtained from
Microsoft Windows operating system. The total size of these
ﬁles including four zlib libraries and 1,701 dynamic library
ﬁles is around 1 GB and the total number of functions is
2,055,584.
Every time we use the previous version of zlib to match
its next version. For example, we ﬁrst use zlib 1.2.5 as our
target set, and put all the functions of its successive version
zlib 1.2.6 together with the two million noise functions into
database. Then, for every function (with at least four ba-
sic blocks) in zlib 1.2.5, we use BinSequence to search for
it. Only when the corresponding function in zlib 1.2.6 is
ranked ﬁrst, which means it has the highest similarity, we
consider it as a correct match. Otherwise, we consider it as
wrongly matched. Afterwards, we do the same process to
other versions of zlib.
For all the tests, we used three diﬀerent ﬁngerprint simi-
larity thresholds: 0.6, 0.65 and 0.7. Those functions whose
Jaccard similarity below these thresholds would be ruled out
using the techniques explained in Section 4.2. Intuitively as
we increase the ﬁngerprint similarity threshold, the number
of functions that could pass this ﬁlter would decrease. So we
choose three diﬀerent values to thoroughly study its eﬀect.
The threshold for the number of basic blocks is set to 35
throughout our experiments.
Table 1a shows the obtained results. Recall that for every
target function, we use two ﬁlters to obtain a small candidate
set from the whole database. The column “Candidate Size”
is the sum of the size of the candidate sets for every target
function.
Intuitively, as we increase the similarity thresh-
old, we end up with a smaller candidate set. As a result, the
processing time decreases. We expected that as we increase
the ﬁngerprint threshold, the overall accuracy would drop
(like zlib 1.2.5 in the table), or at the best would stay the
same (like zlib 1.2.6 in the table) because we would get a
smaller candidate set and the true match could have been
ruled out. The zlib 1.2.7 was a surprise. As we increased
the ﬁngerprint threshold from 0.6 to 0.7, the overall accu-
racy increased from 96.52% to 98.26%. We looked into the
reason. When the ﬁngerprint threshold is 0.6, there were
two functions, whose true matches did not have the largest
Version
1.2.5
1.2.6
1.2.7
Fingerprint
Overall
Threshold Accuracy
96.26%
94.39%
91.59%
100%
100%
100%
96.52%
97.39%
98.26%
0.6
0.65
0.7
0.6
0.65
0.7
0.6
0.65
0.7
Candidate
Time
Size
12346
2727
1911
16315
2848
1989
16312
2847
1988
(per function)