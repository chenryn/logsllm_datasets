method. If a recovery action itself fails and the system goes into another unexpected erroneous 
state S_us, the recovery for the recovery action itself must be  guaranteed by the “recovery for 
recovery” mechanism, which is directly making the current system state transit from S_us into 
the expected state (S2) (M. Fu, et al., 2016). 
70 
7.3  Applicable Recovery Patterns Filtering 
Not every recovery pattern is applicable (M. Fu, et al., 2015; M. Fu, et al., 2016). For example, 
the recovery pattern of Compensated Undo & Redo can be non-applicable because the previous 
state  cannot  be  reachable from  current  state,  or  the  recovery  pattern  of  Compensated  Undo  & 
Alternative can be non-applicable because there is no step alternative existing, or the recovery 
pattern of Direct Redo can be non-applicable because it is not valid to perform recovery just by 
re-executing the step (in other words, the current step is not idempotent). Hence, to determine 
the  applicable  recovery  patterns,  we  need  three  procedures:  state  reachability  checking, 
idempotence checking as well as step alternative existence checking (M. Fu, et al., 2015; M. Fu, 
et al., 2016). To perform these three procedures, we need to acquire the relevant resource states 
(such as current erroneous resource state, previous resource state, expected resource state, etc.) 
and  the  operational  steps  involved  in  the  current  recovery  point.  The  recovery  framework 
acquires  the  necessary  relevant  resource  states  such  as  the  previous  resource  state  and  the 
expected  resource  state  from  the  resource  states  store  generated  by  POD-Recovery,  and  the 
recovery  framework  acquires  the  information  about  the  steps  in  the  current  recovery  point  by 
looking at the recovery point id parameter passed from the error detection service and retrieving 
the steps in the recovery point based on the recovery point id. 
We perform state reachability checking by looking at what operational actions involved in the 
state transferring in the context of cloud are not feasible.  In the context of sporadic operations 
on  cloud,  the  state  transition  involving  stateful  data  drive  (or  store)  creation  and  IP  address 
reassignment  are  not  feasible.  Creating  a  stateful  data  drive  is  infeasible  because  the  original 
data inside the drive is lost forever and cannot be replicated, even though the empty drive can be 
created by calling relevant cloud API function. Reassigning a new IP address to a cloud instance 
is not feasible because this action by cloud consumers is not allowed by cloud platform due to 
the  limited  visibility  and  control  of  cloud  (G.  Lin,  et  al.,  2008).  Hence,  as  long  as  the  state 
transition  does  not  involve  data  drive  creation  or  IP  address  reassignment,  it  is  feasible.  The 
state structure is consisted of limited number of cloud resource items, and the state reachability 
checking  function  loops  through  all  the  resource  items  to  check  if  there  is  IP  address 
reassignment or data drive creation involved. If not, the state is reachable (M. Fu, et al., 2015). 
Idempotence  checking  is  performed  by  examining  whether  the  system  can  be  recovered  by 
merely  rerunning  the  current  step  section  (M.  Fu,  et  al.,  2015;  M.  Fu, et  al.,  2016). There  are 
two scenarios that the recovery can be only rerunning the current step (M. Fu, et al., 2015; M. 
Fu,  et  al.,  2016).  The  first  scenario  is  that,  if  the  current  erroneous  state  is  the  same  as  the 
expected state or the captured consistent state prior to the step, we can just rerun the current step 
for recovery and in this case it is actually the same as Compensated Undo & Redo or Rewind & 
71 
Replay (M. Fu, et al., 2015; M. Fu, et al., 2016). One of such cases is the step of launching a 
cloud instance, and if this step fails the recovery action can be just redoing this step. So in this 
scenario  the  idempotence  checking  is  actually  looking  at  whether  the  current  erroneous  state 
equals to the consistent state before the step or not. The second scenario is that, no matter what 
is  the  current  erroneous  state,  recovery  by  rerunning  of  the  step  will  always  yield  the  same 
expected result (M. Fu, et al., 2015; M. Fu, et al., 2016). One such example is recovery for the 
step of updating auto scaling group with newly created launch configuration in rolling upgrade 
operation,  and  no  matter  what  is  the  current  state  (e.g.  auto  scaling  group  is  attached  with 
another unknown launch configuration) after this step, if we recover by rerunning this step, the 
auto scaling group will always be attached with the expected new launch configuration. 
Step alternative existence checking is performed by checking if there exists any alternative for a 
certain  step  section  (M.  Fu,  et  al.,  2015;  M.  Fu,  et  al.,  2016).  Some  step  sections  may  have 
alternatives,  for  example,  the  step  of  launching  a  new  instance  in  auto  scaling  group  has  an 
alternative  which  is  attaching  a  new  instance  with  auto  scaling  group.  In  POD-Recovery,  we 
have provided a mapping list which provides the step alternatives for various cloud operational 
steps  and  we  use  this  mapping  list  to  check  if  a  step  section  has  any  alternatives  given  the 
specification of the step section (M. Fu, et al., 2015; M. Fu, et al., 2016). 
The  logic  of  determining  the  applicable  recovery  patterns  is  described  in  the  algorithm  below 
(Algorithm:  Determine  Applicable  Recovery  Patterns).  First,  POD-Recovery  obtains  seven 
items of information as the inputs: 1) the current erroneous state (S_err); 2) the expected state 
(S1) before the current step; 3) the captured consistent state (C1) before the current step; 4) the 
expected state (S2) after the current step; 5) the specification of the current step (Step X); 6) the 
specification  of  the  last  step  prior  to  the  current  step  (Step  X-1);  7)  the  expected  state  (S0) 
before the last step prior to the current step (M. Fu, et al., 2015; M. Fu, et al., 2016). Second, we 
have a list of all the eight recovery patterns. Then, for each recovery pattern, we check if it is 
applicable. For Compensated Undo & Redo, we check if S1 is reachable from S_err, and if so 
Compensated  Undo  &  Redo  will  be  included  in  the  applicable  recovery  patterns  list.  For 
Compensated Undo & Alternative, we check if S1 is reachable form S_err and if alternative of 
Step  X  exists,  and  if  so  Compensated  Undo  &  Alternative  will  be  included  in  the  applicable 
recovery patterns list. For Rewind & Replay, we check if C1 is equal to S1 (because C1 might 
be different from S1 due to state capturing service delay or false positives of state capturing; if 
so, C1 is not valid and Rewind is invalid) and if C1 is reachable from S_err, and if so Rewind & 
Replay will be included in the applicable recovery patterns list. For Rewind & Alternative, we 
check if C1 is equal to S1, if C1 is reachable from S_err and if alternative of Step X exists, and 
if  so  Rewind  &  Alternative  will  be  included  in  the  applicable  recovery  patterns  list.  For 
72 
Reparation, we check if S2 is reachable from S_err, and if so Reparation will be included in the 
applicable  recovery  patterns  list.  For  Direct  Redo,  we  issue  idempotence  check  to  see  if  it  is 
feasible to recover by rerunning Step X, and if so Direct Redo will be included in the applicable 
recovery patterns list.  For Direct Alternative, we check if alternative for Step  X exists and we 
perform  idempotence  check  to  see  if  it  is  feasible  to  recover  by  rerunning  Step  X,  and  if  so 
Direct Alternative will be included in the applicable recovery patterns list. For Farther Undo & 
Redo, we check if S0 is reachable from S_err, and if so Farther Undo & Redo will be included 
in the applicable recovery patterns list (M. Fu, et al., 2015; M. Fu, et al., 2016). 
Algorithm: Determine Applicable Recovery Patterns 
Function DetermineApplicableRecoveryPatterns (S_err, S1, C1, S2, Step X, Step X-1, S0) 
{ 
        List applicableRecoveryPatterns = new List (); 
        if (StateReachabilityChecking (S_err, S1) == true) 
        { 
                applicableRecoveryPatterns.Add (“Compensated Undo & Redo”); 
        } 
        if (StateReachabilityChecking (S_err, S1) and  StepAlternativeExists (Step X)) 
        { 
                applicableRecoveryPatterns.Add (“Compensated Undo & Alternative”); 
        } 
        if (C1 == S1 and StateReachabilityChecking (S_err, C1)) 
        { 
                applicableRecoveryPatterns.Add (“Rewind & Replay”); 
        } 
        if (StateReachabilityChecking (S_err, C1) and StepAlternativeExists (Step X)) 
        { 
                applicableRecoveryPatterns.Add (“Rewind & Alternative”); 
        } 
        if (StateReachabilityChecking (S_err, S2) == true) 
        { 
                applicableRecoveryPatterns.Add (“Reparation”); 
        } 
        if (IdempotenceChecking (Step X) == true) 
        { 
                applicableRecoveryPatterns.Add (“Direct Redo”); 
        } 
        if (IdempotenceChecking (Step X) and StepAlternativeExists (Step X)) 
        { 
                applicableRecoveryPatterns.Add (“Direct Alternative”); 
        } 
        if (StateReachabilityChecking (S_err, S0) == true) 
        { 
                applicableRecoveryPatterns.Add (“Farther Undo & Redo”); 
        } 
        return applicableRecoveryPatterns; 
} 
73 
In  the  experimental  evaluation,  we  will  present  the  results  of  the  filtered  applicable  recovery 
patterns for each recovery point inside the sporadic operations evaluated (chapter 8). 
7.4  Recovery Actions Generation 
In the context of cloud sporadic operations, generating the recovery action for each applicable 
recovery  pattern  is  based  on  state  transition  analysis,  so  the  recovery  action  is  automatically 
generated (M. Fu, et al., 2016). POD-Recovery just needs to determine what actions need to be 
performed  in  order  to  transit  current  erroneous  system  resource  state  into  the  goal  state,  and 
what step sections or step section alternatives should be executed again, according to the needs 
and features of the applicable recovery patterns (M. Fu, et al., 2015; M. Fu, et al., 2016). The 
steps  within  a  recovery  point  can  be  retrieved  by  relying  on  the  recovery  point  id  parameter 
passed  from  the  error  detection  service  to  resolve  what  steps  are  inside  a  recovery  point,  and 
step alternatives  can  be  determined  by  looking  at  the  in-memory  step  alternatives  table  where 
the alternatives for all the steps are specified (M. Fu, et al., 2015; M. Fu, et al., 2016). 
The  state  transition  based  recovery  plan  generation  is  based  on  an  AI  planning  technique  (J. 
Hoffmann,  et  al.,  2012).  The  set  of  possibly  needed  recovery  plans  in  general  is  large, 
comprising  all  pairs  of  current  error  state  and  goal  state  (S_curr,  S_goal).  Depending  on  the 
actual operation, the set of possible error states S_curr may be infinite. Therefore, any solution 
that  requires  pre-specified  recovery  plans  would  be  limited,  and  any  requirement  on  pre-
specifying recovery plans increases the burden for adopting the recovery approach. Furthermore, 
the  state  of  a  cloud  application  cannot  be  seen  as  memory  state:  recreating  resources  like 
running  VMs is  categorically  different from  reverting  back  to  a  checkpoint  or  a  snapshot  of a 
database. For these reasons, we adopted an automated planning approach for the generation of 
recovery  plans.  Previous  researches  on  undo  of  cloud  operations  (I.  Weber,  et  al.,  2012;  I. 
Weber, et al., 2013; S. Satyal, et al., 2015) successfully used artificial intelligence (AI) planning 
to  move  a  cloud  system  from  a  given  state  to  an  earlier  consistent  state,  in  order  to  undo 
undesired changes. The AI planner used is Fast-Forward (FF) in the variant (J. Hoffmann, et al., 
2012). It requires three types of inputs: an initial state, a goal state, and a set of action templates 
referred  to  as  the  planning  domain.  The  actions  in  the  planning  domain  are  specified  in  the 
Planning  Domain  Definition  Language  (PDDL)  (J.  Hoffmann,  et  al.,  2012).  Each  action’s 
specification comprises its preconditions, its parameters, and its effects. From these inputs, the 
planner generates a state transition plan. 
We adopted this approach for recovery action generation in POD-Recovery (M. Fu, et al., 2016). 
The undo tool (I. Weber, et al., 2012; I. Weber, et al., 2013) is publicly available, and includes a 
domain  model  for  a number  of the  AWS  cloud  API  operations.  One limitation of  the  existing 
74 
undo  tool  is  that  it  does  not  support  certain  cloud  resources  such  as  auto  scaling  group  and 
elastic load balancer, because the current planning domain model in this tool does not include 
cloud API operations related to such cloud resources. Hence, the sporadic operations involving 
such resources cannot directly rely on this existing tool for undo or state transition. While this 
facilitated re-use significantly, we had to add full support for certain cloud resource types such 
auto scaling groups and elastic load balancers. We have modified the existing planning domain 
model file used by the undo tool by adding more cloud API operation specifications related to 
more types of cloud resources into it. This extended domain model used by the state transition 
mechanism  of  POD-Recovery  has  more  than  1300  lines  of  PDDL  code,  compared  to  around 
1100 lines of PDDL code in the previous domain model which is used by the existing undo tool. 
In  addition,  since  we  only  capture  the  states  of  the  resources  within  the  determined  resource 
space,  the  system  resource  states  can  be  captured  in  a  more  efficient  way  and  the  transition 
between system resource states is faster. As such, compared to the state transition mechanism in 
the  existing  undo  tool,  the  state  transition  mechanism  in  POD-Recovery  can  capture  resource 
states more efficiently, can perform state transition more rapidly and can support more types of 
cloud resources such as auto scaling group and elastic load balancer, hence being able to recover 
for  various  cloud  sporadic operations such  as  rolling upgrade  which relies  on  the  auto scaling 
feature  provided  by  cloud.  For  the  eight  recovery  patterns  proposed  by  us,  we  analysed  the 
possibility to map all eight patterns to  AI planning tasks. This mapping  is illustrated in  below 
Table 6 (M. Fu, et al., 2016). As can be seen, six of the eight recovery patterns can, at least in 
part,  be  mapped  to  AI  planning  problems.  The  multitude  of  different  pairs  of  planning  tasks 
further underlines our choice of automated planning for recovery (M. Fu, et al., 2016).  
Table 6.  AI-Planning for Eight Recovery Patterns 
Recovery Pattern 
AI-Planning Recovery Plan 
Compensated  Undo  & 
Redo 
From  current  erroneous  state  to  expected  state  before 
the step (S_curr, ES(X-1)) 
Compensated  Undo  & 
Alternative  
From  current  erroneous  state  to  expected  state  before 
the step (S_curr, ES(X-1)) 
Rewind & Replay 
Rewind & Alternative 
Reparation 
Direct Redo 
Direct Alternative 
Farther Undo & Redo 
From  current  erroneous  state  to  captured  state  before 
the step (S_curr, CS(X-1)) 
From  current  erroneous  state  to  captured  state  before 
the step (S_curr, CS(X-1)) 
From current erroneous state to expected state after the 
step (S_curr, ES(X)) 
N.A. 
N.A. 
From  current  erroneous  state  to  expected  state  before 
the step prior to last (S_curr, CS(X-2)) 
75 
How we embed AI planning for generating recovery plans is illustrated in Fig. 21 (M. Fu, et al., 
2016).  First, the  initial  state  (captured  current  error  state)  and the  goal  state  (either a  previous 
captured state or an expected state) are captured/derived, and translated into a planning problem 
in PDDL. This planning problem defined in PDDL code is also called a state transition problem 
file.  Another  file  which  is  used  as  an  input  is  called  the  planning  domain  model  file,  which 
specifies the cloud API operations related to a wide range of cloud resources. Second, with both 
the state transition problem file and the planning domain model file as inputs, the AI planner is 
called  to  automatically  generate  the  recovery  plan.  Specifically,  the  AI  planner  called  is  an 
existing black-box library which is designed for generating state transition procedures called AI 
plans. In some cases, generating the recovery plan is not possible: certain actions, like deleting 
virtual hard disks, are not reversible unless additional precautions are taken. A detailed analysis 
of the undoability of AWS API operations can be found in the previous work (I. Weber, et al., 
2013). If the planner determines that no recovery plan exists, the respective recovery pattern is 
not  applicable  to  the  current  erroneous  state.  Otherwise,  the  recovery  plan  achieves  resource 
state transitions from the initial state to the goal state. Our extended planning domain model file 
has more than 1300 lines of PDDL code, which cover all the possible AWS API operations that 
can  affect  the  resource  space  (M.  Fu,  et  al.,  2016).  The  recovery  plan  generated  by  the  FF 
planner variant (J. Hoffmann, et al., 2012) typically is the shortest path, i.e., the least number of 
actions needed. Finally, since the generated plan is returned in a proprietary format, it needs to 
be translated to executable code, like command line scripts. The translated code is then executed, 
and  calls  API  operations  like  “RunInstances”.  Once  it  completes,  the  system  is  in  the  desired 
goal state – unless errors occurred, in which case a recursive call to the recovery procedure is 
made.  Since  the  recovery plans  are  dynamically  generated from  the  current  error  state, the  AI 
planner can generate recovery plans for failures that occur during recovery itself. 
Fig. 21.  AI-Planning based Recovery Plan Generation. 
76 
One challenge lies in ensuring the efficiency of recovery plan generation: automated planning is 
a computationally hard problem (J. Hoffmann, et al., 2012). Our resource space determination 
mechanism is helpful in this regard, by not considering cloud resources that are unrelated to the 
current  operation  (M.  Fu,  et  al.,  2016).  If  unrelated  resources  are  present  in  the  planning 
problem, the planner has to consider them during each intermediate planning state, which makes 
the search for a plan more time-consuming (M. Fu, et al., 2016). Another challenge was that the 
planning task generation in POD-Recovery needs to explicitly state if any resources should be 
deleted – else the planner does not include deletion actions (M. Fu, et al., 2016). For example, if 
the  initial  state  has  three  instances  (a,  b,  c)  and  the  goal  state  has  two  instances  (a,  b),  the 
semantics  used  in  the  planner  imply  that  there  is  no  need  to  change  anything,  and  hence  the 
planner returns an empty plan. We solve this problem by explicitly adding the keyword “Del” to 
the deleted  resources in the  goal  state  (e.g.  Del(c)). This  matches the  planning  domain,  where 
the effect of any “delete” action is that the resource will be deleted (Del(resource)). By making 
it explicit which resources need to be deleted, the planner can select the respective actions. 
For  example,  the  generated  recovery  action  for  the  recovery  pattern  of  Rewind  &  Replay  for 
step  5  (Terminate  old  version  instance)  in  rolling  upgrade  is  described  in  Table  7.  The  error 
occurring  is  that  the  instance  termination  takes  too  long.  Each  recovery  action  consists  of 
several activities which each are mapped with relevant cloud APIs. For example, the activity of 
“Terminate 
old 
instance” 
is 
mapped 
with 
the 
cloud 
API 
of 
“TerminateInstancesInAutoScalingGroup”.  
Table 7.  Recovery Action Generated for Rewind & Replay 
Process Step 
Step 5: Terminate old 
version image instance 
(VM) in ASG 
Error: Instance 
termination fails (too 
long termination time) 
Recovery Actions (Rewind & Replay) 
Terminate the old version image instance; 
Launch another old version image instance  
and attach it to ASG; 
Terminate this newly launched old version 
image instance; 
In  the  experimental  evaluation,  we  will  present  the  details  of  the  generated  recovery  actions 