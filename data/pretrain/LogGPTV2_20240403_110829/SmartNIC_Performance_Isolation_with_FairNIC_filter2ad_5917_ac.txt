viously not needed. On the plus side, FairNIC’s virtual addressing
provides wild-write [9] protection for buggy application code.
4.4 Rate-limiting coprocessor access
FairNIC delivers accelerator performance isolation by rate-limiting
requests to each accelerator to an aggregate rate the unit can sus-
tain without queuing, similar in nature to the queue minimization
strategy of DCTCP [1]. In so doing, FairNIC ensures that any al-
lowed request to an accelerator observes the minimum possible
latency. If an accelerator is over-subscribed, each tenant exceeding
their fair share will be throttled, but the remaining tenants are not
impacted. Because not all tenant applications use each accelerator,
FairNIC also provides a form of work conservation that allows
tenants to divvy up accelerator resources allocated to cores not
currently accessing them.
An ideal implementation might employ a centralized DRR queue
for each accelerator. Unfortunately, there is no hardware support for
such a construct, and our best software implementation of a shared
queue increases the latency of accessing offloads by at least 300 ns.
In some cases—such as the random number generator and secret
key accelerator—that delay dominates accelerator access times.
Instead, FairNIC implements a distributed algorithm, similar in
spirit to distributed rate-limiting (DRL) [43] and sloppy counters [3].
When a core first requests the use of an offload, a token rate-limiter
is instantiated with a static number of tokens. This base rate is
the minimum guarantee per core. When a call to an accelerator
is made, the calling core decrements its local token count. When
its tokens are exhausted, it checks if sufficient time (based on its
predefined limit) has passed for it to replenish its token count. This
mechanism allows for cores to rate-limit accesses without directly
communicating with one another and incurring the additional 100-
ns latency of cross-core communication. Using distributed tokens in
place of a centralized queue has the downside that requests can be
bursty for short periods. The maximum burst of requests is double
a core’s maximum number of tokens. Hence, the burst size can be
adjusted by setting how often tokens are replenished.
Static token allocation is not work conserving: There may be
additional accelerator bandwidth which could be allocated to a
core with no remaining tokens in its given window. To attain work
conservation we allow a core to steal tokens from the non-allocated
pool when they run out. Stolen tokens are counted separately from
statically allocated tokens and are subject to a fair-sharing pol-
icy. Specifically, we implement additive increase, multiplicative
decrease as it allows for cores to eventually reach stability and it is
686
480718Set IndexCache Line18714PreﬁxColor Size32 KBApp 1CoresPreﬁxColor Size010x00x1App 020x216 KB0x000x010x02App 00x100x110x12VirtualApp 0App 1App 0App 0App 1PhysicalTLB (0,1)TLB (2)16KBL232KB32KB16KBSmartNIC Performance Isolation with FairNIC
SIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
adaptive to changing loads [10]. To reduce the overhead of sharing,
cores only check the global counter when they run out of tokens
and have consumed them at a rate above their limiter.
We measure the maximum effective throughput of each accel-
erator empirically (see Section 3.4) and set token allocations ac-
cordingly. Unfortunately, not all accelerators run in constant time.
Accelerators such as ZIP and RAID execute as a function of the
size of their input. For these accelerators we dynamically calculate
the number of tokens based on the size of the request to retain
the desired rate of usage. We leave (seemingly) non-deterministic
accelerators such as the regular-expression parser to future work.
Costs. Rate-limiting incurs the overhead of subtracting tokens
from a core’s local cache in the common case. When accessing the
global token cache cores must access a global lock and increment
their local counters (≈100 ns), which is a substantial delay in the
case of the fastest accelerators (e.g., random number generation).
Moreover, applications are limited to an aggregate rate that ensures
the lowest-possible coprocessor access latency, which, due to impre-
cision in calibration, may under-utilize the coprocessor. It is possible
unrestricted access might lead to higher overall throughput.
5 IMPLEMENTATION
Cavium LiquidIO CN2360s come with a driver/firmware package
where the host driver communicates with the SmartNIC firmware
over PCIe. It provides support for traditional NIC functions such
as hardware queues, SR-IOV [12] and tools like ifconfig and
ethtool. FairNIC extends the firmware by adding a shim layer that
operates between core firmware and the applications. The shim
includes an application abstraction which can execute multiple
NIC applications. FairNIC includes an isolation library that imple-
ments core partitioning, virtual memory mapping and allocation,
and coprocessor rate-limiting. The shim provides a syscall-like
interface for applications to access shared resources.
5.1 Programming model
Each NIC application must register itself as an application object.
FairNIC maintains a struct (portions shown in the top part of
Figure 7) that tracks state and resources (like memory partitions
and output queues) associated with each application, along with
a set of callback functions for initialization and packet processing.
At tenant provisioning time, the cloud provider assigns each tenant
application a weight that is used in cache partitioning and token
allocation, a coremask that explicitly assigns NIC cores, and an
(sso_group) ID which is used to tag all of the application’s packets.
FairNIC maintains set of host queues (host_vfs) for interacting
with the tenant VMs on the host, output queues (pko_ports) to
send packets on the wire and memory regions (memory_stripes)
assigned to it. The tenant provides callbacks for traffic from the
host and wire which FairNIC invokes when packets arrive.
5.2 Isolation library
We implement our isolation mechanisms discussed in Section 4 as
a C library and expose methods (shown in the bottom portion of
Figure 7) that applications call to allocate memory, send packets or
access coprocessors per the isolation policy. None of these interfaces
prevent applications from bypassing FairNIC and directly accessing
typedef struct application {
char *name;
uint16_t weight;
coremask_t cores;
uint16_t sso_group;
uint16_t host_vfs[];
uint16_t pko_ports[];
uint64_t dest_mac;
void *memory_stripes[];
int (*global_init) (struct application* app);
int (*per_core_init) (struct application* app);
int (*from_host_packet_cb) (struct application* app,
packet* work);
int (*from_wire_packet_cb) (struct application* app,
packet* work, int *port);
}
void* memory_allocate(application, size);
void memory_free(void* p);
int send_pkt_to_host(application, packet, queue);
int send_pkt_to_wire(application, packet, queue);
int call_coprocessor(application, type, params);
Figure 7: FairNIC provides an application abstraction (top)
and an isolation library which exposes an API for applica-
tions to access NIC resources (bottom).
NIC resources. Moreover, all code runs in the same protection
domain and we do not make any claims of security isolation. We
assume that the application code is not malicious and uses the
provided library for all resource access.
Cache striping. Based on the weight property, each application is
allocated regions of memory during initialization, which are made
accessible through memory stripes. Applications use our memory
API to allocate or free memory, which also inserts the necessary
TLB entries for address translation.
Packet processing. Applications register callbacks for when they
receive packets and send packets to both host and wire using our
provided API. As a proof of concept, we use SR-IOV virtual func-
tions (VFs) to classify host traffic and Ethernet destination addresses
to classify wire traffic. Using the host_vfs property, we dedicate a
set of VFs for each application and tag packets on these VFs with
group ID sso_group. This labeling also allows for allocating sep-
arate buffer pools and sending back-pressure to only certain VFs
(and tenants) as our isolation mechanisms kick in and constrain
their traffic, while other tenants can keep sending.
Coprocessor access. Applications
invoke coprocessors via
wrapped calls (not shown) to existing Cavium APIs. Each call
has blocking and non-blocking variants. The wrapped calls first
check the core local token counter for the coprocessor being
called. On the first call, tokens are initialized by setting their value
to the guaranteed rate specified in the application’s context. If
the core has available tokens it decrements its local count and
makes a direct call to the coprocessor. If a core has no tokens, it
checks its local rate-limiter. If enough time has passed since its
last invocation, the local tokens are replenished. Otherwise, the
687
SIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
Grant, Yelam, Bland, and Snoeren
global token cache is accessed. If available, global tokens are then
allocated to the local cache of a core. Global overflow tokens are
single-use, and can only be reclaimed by re-checking the global
cache.
6 EVALUATION
In this section we demonstrate FairNIC’s ability to run multiple
tenant applications simultaneously and evaluate the effectiveness
of core partitioning, cache striping, and coprocessor rate-limiting.
We use our own implementations of Open vSwitch and a custom
key/value store that downloads functionality to the SmartNIC.
6.1 Experimental setup
Our testbed consists of two Intel servers, one equipped with a
Cavium 2360 SmartNIC and the other with a regular 25-Gbps NIC,
connected to each other via a point-to-point SFP+ cable. Each of the
servers sports forty 3.6-GHz x86 cores running Ubuntu 18.04. The
Cavium NIC that hosts our NIC applications features 16 1.5-GHz
MIPS cores, 4 MB of shared L2 cache and 16 GB of DRAM. The server
with the SmartNIC hosts tenant applications while the second server
generates workloads of various sizes and distributions using DPDK
pktgen [18]. We emulate a cloud environment by instantiating
tenants in virtual machines (VMs) using KVM [22] and employ
SR-IOV between the VMs and SmartNIC.
6.2 Applications
We implement two applications that are frequently (c.f. Table 3) em-
ployed in the literature to showcase SmartNIC technology: virtual
switching and a key/value store.
6.2.1 Open vSwitch datapath. Open vSwitch (OVS) is an open-
source implementation of a software switch [19] that offers a rich
set of features, including OpenFlow for SDN. OVS has three compo-
nents: vswitchd that contains the control logic, a database ovsdb
to store configuration and a datapath that handles most of the
traffic using a set of match-action rules installed by the vswitchd
component. OVS datapath runs in the kernel in the original imple-
mentation and is usually the only component offloaded to hardware.
We start with Cavium’s port of OVS [7] and strip away the
control components while keeping the datapath intact. For our
experiments, the control behavior is limited to installing a set of pre-
configured rules so that all flows readily find a match in the datapath.
Unless specified otherwise, each rule simply swaps Ethernet and IP
addresses and sends the packet back out the arriving interface.
6.2.2 Key/value store. We implement a key/value store (KVS)
which has its key state partitioned between the host’s main memory
and the on-NIC storage. The NIC hosts the top-5% most-popular
keys, while the remaining 95% are resident only in host memory.
Due to the complexities involved in porting an existing key/value
store such as Memcached [17] or Redis [5] we developed our own
streamlined implementation that supports the standard put, get,
insert, and delete operations. We modify the open-source version
of MemC3 [15] to run in both user-space and on the SmartNIC.
MemC3 implements a concurrent hash table with constant-time
worst-case look-ups.
Figure 8: Tenants (shown in gray) are deployed in KVS VMs
(blue), which can communicate with FairNIC applications
(dark gray) through SR-IOV.
To drive our key/value store, we extend DPDK’s packet generator
to generate and track key/value requests with variable-sized keys.
The workload requests keys using a Zipf distribution.
6.3 Cohabitation
We start by demonstrating FairNIC’s ability to multiplex SmartNIC
resources across a representative set of tenants each offloading
application logic to the SmartNIC. In the configuration shown in
Figure 8 we deploy six tenants across eight virtual machines and
all sixteen NIC cores. Four tenants run our KVS application in one
VM paired with a corresponding SmartNIC application. Two other
tenants run two VMs each and use our OVS SmartNIC application
to route traffic between them. The OVS applications are assigned
three or four NIC cores each, while the KVS applications each run
on two. (The FairNIC runtime executes on the remaining core).
We send traffic from a client machine at line rate (25 Gbps) and
segregate the traffic such that each tenant (app) gets one-sixth of
the total offered load (≈4 Gbps).
As shown in Figure 9, each of the (identically provisioned) KVS
tenants serve the same throughput, while the two tenants employ-
ing OVS obtain differing performance due to their disparate core
allocations. The KVS tenants all deliver relatively higher through-
put and low per-packet latency because most requests are served
entirely by their on-NIC applications, while the OVS tenants’ re-
sponses are much slower as packets are processed through the VMs
on the hosts. Note that whenever an app is not able to service the
≈4-Gbps offered load, it means that the cores are saturated due
to high packet rate which happens at lower packet sizes for all
the tenants. The right-hand plot shows a CDF of the per-packet
latencies experienced by each of the tenants at 1000-B packet size.
OVS tenants experience higher latencies due to queue buildup as
they are overloaded (which is worse for OVS 2 with fewer cores)
while the KVS tenants comfortably service all offered load at this
packet size.
6.4 Performance isolation
We now evaluate the performance crosstalk between two tenants
each running an application on the NIC. The first tenant runs a
well-behaved application that runs in its normal operation mode.
688
HOSTNICPCIeHypervisorVFVFVFVFVFVFVFVFVMAPPVMAPPVMKVSVMKVSOVSOVSOVSKVSOVSKVSSmartNIC Performance Isolation with FairNIC
SIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
Figure 9: Throughput (left) and per-packet latency (right) for six cohabitating tenants using FairNIC: two tenants running
a four and three-core OVS application switching traffic between two VMs each, and four tenants running one VM with a
corresponding KVS application across two cores.
Figure 10: The left two plots show the throughput of two cohabitating OVS applications without and with FairNIC isolation,
respectively. The rightmost plot shows per-packet latencies for OVS 1 with 1-KB packets.
The other tenant runs a second application in a deliberately antago-
nistic fashion that exhibits various traffic or resource usage/access
patterns in order to impact the performance of the first one.
6.4.1 Traffic scheduling and core isolation. For this experiment,
we run two instances of Open vSwitch, OVS 1 and OVS 2. Both run
the same implementations of our Open vSwitch offload with similar
sets of flow table rules, except for one difference. While OVS 1
has three actions for each flow rule: swap_mac, swap_ip (that swap
Ethernet and IP source and destination addresses, respectively)
and output (send the packet out on the same port)—actions that
effectively turn the packet around—OVS 2 has more core-intensive
packet-processing rules with an extra 100 swap actions per packet
(representative of a complex action). This extra processing reduces
the throughput of OVS 2 compared to OVS 1 (given same number of
cores for each). We send 50/50% OVS 1/2 traffic on the wire, of which
only a portion is returned based on the the effective capacity of each
application which we use to measure throughput and latencies.