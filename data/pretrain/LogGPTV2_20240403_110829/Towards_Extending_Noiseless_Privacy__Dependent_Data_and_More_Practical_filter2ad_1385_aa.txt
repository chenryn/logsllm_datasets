# Title: Extending Noiseless Privacy: Handling Dependent Data and Enhancing Practicality

## Authors:
Krzysztof Grining  
Marek Klonowski  
Wroc≈Çaw University of Science and Technology  
Faculty of Fundamental Problems of Technology  
Department of Computer Science  
Email: [PI:EMAIL]

## Abstract
In 2011, Bhaskar et al. observed that in many cases, sufficient privacy can be ensured without adding noise by leveraging adversarial uncertainty. This observation stems from the fact that if a part of the data is randomized from the adversary's perspective, it can effectively hide other values.

To date, most approaches to this idea have been purely asymptotic, limiting their practical applicability. In this paper, we aim to make the concept of utilizing adversarial uncertainty not only an interesting theoretical idea but also a practically useful technique, complementary to differential privacy, which is the state-of-the-art definition of privacy. This requires non-asymptotic privacy guarantees, a more realistic approach to the inherent randomness in the data, and a rigorous adversarial model.

We extend the concept proposed by Bhaskar et al. to cover dependent data sets and introduce a detailed (non-asymptotic) analysis, motivated by practical needs. We use advanced mathematical tools, including the Stein method, which, to our knowledge, has not been used in privacy research before. Additionally, we explore how to combine adversarial uncertainty with differential privacy to enhance the privacy parameters already present in the data by adding a small amount of noise.

**Keywords:** Data aggregation, differential privacy, distributed systems

## 1. Introduction
Consider a scenario where a set of users each holds a single value, or equivalently, a database with \( n \) records, each corresponding to a specific user. The goal is to reveal some aggregated statistic, such as the sum of all values, while preserving individual privacy. Recent years have seen promising results for both centralized and distributed databases, often combining cryptography and privacy-preserving techniques. However, standard differential privacy, which requires adding carefully calibrated noise, has limitations. Adding noise can lead to significant errors in the aggregated statistic, making it less practical for scenarios requiring exact values. Moreover, adding noise, especially from non-standard distributions, can be technically challenging, particularly for small, computationally constrained devices.

One might ask if it is possible to ensure differential privacy without adding noise. Unfortunately, in the standard paradigm, adding noise is inevitable. If users operate independently and cannot cooperate to add randomized values, the size of the aggregated noise must be \(\Omega(\sqrt{n})\), where \( n \) is the number of users.

On the other hand, real-life applications suggest that it is often safe to release aggregated data without adding noise, as noted in [5]. For example, revealing the average national income does not significantly expose individual incomes. Similarly, the average income of employees in a large company should be secure. However, revealing the exact average or maximum income in a small community can compromise privacy.

These intuitions have been explored in several papers, such as [4, 5, 24], where authors propose relaxations of differential privacy that leverage the inherent randomness in the data. Our work extends this line of research by providing detailed, non-asymptotic analysis, which is crucial for practical applications. We also consider dependent data, using advanced mathematical tools like the Stein method, which, to our knowledge, has not been used in this context before.

The intuition behind noiseless privacy is that in real-life scenarios, it is often too pessimistic to assume the adversary knows almost every record. Standard differential privacy assumes this, but in practice, revealing the exact average worldwide income would not harm individual privacy. The authors of [5] capture this intuition, allowing the database designer to check if the data satisfies desired privacy parameters and, if so, reveal the aggregated value without adding noise. However, their results are mostly asymptotic, making them difficult to use in practice due to unknown constants. Our methods provide explicit bounds for privacy parameters, making the approach more practical.

To our knowledge, the idea of combining standard differential privacy techniques (e.g., Laplace mechanism) with adversarial uncertainty has not been explored before. Intuitively, if the data has much randomness, we should be able to add smaller noise. Our novel approach provides explicit bounds for privacy parameters, allowing us to explore the synergy between differential privacy and noiseless privacy.

In this paper, we follow the model from [5] but present it in a more convenient way. We show that this definition is coherent with classic differential privacy and can be seen as an extension. We provide explicit privacy parameters and describe wide classes of data that can be handled without adding noise. Our theorems can be used as "off-the-shelf" methods for practitioners to check if they can safely release data without noise.

### 1.1 Our Results and Organization of the Paper
Our contributions are as follows:

- **Extension to Dependent Data:** We extend the paradigm of utilizing adversarial uncertainty to handle dependent data (Theorems 3 and 5).
- **Synergy with Differential Privacy:** We explore the synergy between standard differential privacy methods and noiseless privacy (Theorem 6).
- **Adversarial Model and Practical Procedures:** We propose an adversarial model and an explicit procedure for preserving privacy (Figure 6), which is easy to use for practitioners.
- **Improved and Explicit Bounds:** We provide improved and explicit (non-asymptotic) bounds for the privacy parameters (Theorems 2 and 4).

We believe our contributions are a step towards more practical constructions of privacy protocols that utilize adversarial uncertainty. For the first time, we consider a wide class of dependent data. Our results show that the party responsible for privacy does not need to know the exact structure of dependencies or the exact distribution of the data. Upper bounds for the size of the largest dependent subset and the sum of centralized third moments (or fourth in the case of dependent data) are sufficient for practical use.

The rest of the paper is organized as follows. In Section 2, we explain the motivations, recall the idea of utilizing adversarial uncertainty from [5], and provide a formalism that extends the differential privacy notion. We also introduce and discuss our adversarial model and some possible applications. In subsequent sections, we present our results, focusing on independent and dependent data, and explore the combination of adversarial uncertainty with differential privacy. Finally, we conclude and outline future work in Section 8.

## 2. Model
The main goal of this paper is to make the idea of noiseless privacy, introduced in [5], a practically useful way to guarantee privacy. We use the concept of noiseless privacy but present the privacy model in a slightly different, more convenient way. This shows a direct descent from classical differential privacy, as presented in [16].

### 2.1 Modeling Privacy of Randomized Data
We use a privacy model where the data (or at least part of it) is considered random from the adversary's perspective, coming from a specific distribution. This approach is natural in many scenarios where the adversary's knowledge is limited. This "uncertainty" can be utilized, but it requires a different definition of privacy than standard differential privacy, as we need to account for randomized inputs.

Following [5], we call this approach noiseless privacy. Before presenting its formal definition, we introduce the following definitions:

**Definition 1 (Adjacent Random Vectors):** Let \( X = (X_1, \ldots, X_n) \) be an arbitrary random vector and let \( X' \) be another random vector. Let \( X^* \) be a random variable. We say that vectors \( X \) and \( X' \) are adjacent if and only if:
\[ X' = (X_1, \ldots, X_i, X^*, X_{i+1}, \ldots, X_n) \]
or
\[ X' = (X_1, \ldots, X_{i-1}, X_{i+1}, \ldots, X_n) \]
for any \( i \in \{1, \ldots, n\} \).

This captures the notion of adjacency similar to [16], but for random variables rather than deterministic values. If for some deterministic adjacent vectors \( x \) and \( x' \) we have \( X = x \) and \( X' = x' \) with probability 1, then this definition of adjacency is the same as in [16].

**Definition 2 (Data Sensitivity):** We say that data vector \( X = (X_1, \ldots, X_n) \) and mechanism \( M \) have data sensitivity \( \Delta \) if and only if:
\[ |M(X) - M(X')| \leq \Delta \]
for every vector \( X' \) that is adjacent to \( X \).

This is similar to the \( l_1 \)-sensitivity defined in [16]. A more detailed comparison of noiseless privacy and standard differential privacy can be found in the Appendix.

**Definition 3 (Noiseless Privacy):** We say that a privacy mechanism \( M \) and a random vector \( X = (X_1, \ldots, X_n) \) preserve noiseless privacy with parameters \( (\epsilon, \delta) \) if for any random vector \( X' \) such that \( X \) and \( X' \) are adjacent, we have:
\[ \forall B \in \mathcal{B} \quad P(M(X) \in B) \leq e^\epsilon P(M(X') \in B) + \delta \]

Intuitively, this definition states that if the data can be considered random, the outcome of the mechanism does not significantly change whether a single user is added or removed. This is similar to standard differential privacy. Throughout the paper, we use the abbreviation \( (\epsilon, \delta) \)-NP to denote noiseless privacy with parameters \( \epsilon \) and \( \delta \).

### 2.2 Adversarial Model
We assume the adversary:
- Knows the exact data of at most some fraction \( \gamma \) of the users.
- Knows the correct distribution (but not the value itself) of the data of the remaining users.
- Knows the dependencies between some data values, but only in subsets of size at most \( D \).

These assumptions are justified as follows:
- In standard differential privacy, we assume the adversary knows the exact data of all users except one. Here, we relax this by giving an upper bound on the number of compromised users.
- In real-life scenarios, it is unlikely that the adversary knows almost every data record.
- We still give the adversary considerable power by assuming they know the distributions of the data.
- Dependencies in real-life data are common, and the adversary might know about them.

In Sections 3 and 4, we assume for simplicity that the adversary does not know any exact values (\( \gamma = 0 \)). We discuss how to extend our results for the case where the adversary knows any arbitrary \( \gamma n \) exact values in Section 5.