title:Towards Extending Noiseless Privacy: Dependent Data and More Practical
Approach
author:Krzysztof Grining and
Marek Klonowski
Towards Extending Noiseless Privacy - Dependent Data
and More Practical Approach
Krzysztof Grining
Marek Klonowski
Wroclaw University of Science and Technology
Faculty of Fundamental Problems of Technology
Wroclaw University of Science and Technology
Faculty of Fundamental Problems of Technology
Department of Computer Science
PI:EMAIL
Department of Computer Science
PI:EMAIL
ABSTRACT
In 2011 Bhaskar et al. pointed out that in many cases one can en-
sure sufﬁcient level of privacy without adding noise by utilizing ad-
versarial uncertainty. Informally speaking, this observation comes
from the fact that if at least a part of the data is randomized from
the adversary’s point of view, it can be effectively used for hiding
other values.
So far the approach to that idea in the literature was mostly
purely asymptotic, which greatly limited its adaptation in real-life
scenarios. In this paper we aim to make the concept of utilizing
adversarial uncertainty not only an interesting theoretical idea, but
rather a practically useful technique, complementary to differential
privacy, which is the state-of-the-art deﬁnition of privacy. This re-
quires non-asymptotic privacy guarantees, more realistic approach
to the randomness inherently present in the data and to the adver-
sary’s knowledge.
In our paper we extend the concept proposed by Bhaskar et al.
and present some results for wider class of data. In particular we
cover the data sets that are dependent. We also introduce rigorous
adversarial model. Moreover, in contrast to most of previous pa-
pers in this ﬁeld, we give detailed (non-asymptotic) results which
is motivated by practical reasons. Note that it required a modi-
ﬁed approach and more subtle mathematical tools, including Stein
method which, to the best of our knowledge, was not used in pri-
vacy research before.
Apart from that, we show how to combine adversarial uncer-
tainty with differential privacy approach and explore synergy be-
tween them to enhance the privacy parameters already present in
the data itself by adding small amount of noise.
Keywords
data aggregation, differential privacy, distributed system
1.
INTRODUCTION
Let us imagine a following problem. There is a set of users and
each of them keeps a single value. Analogously, we can think about
a database with n records, each corresponding to a speciﬁc user.
We have to reveal some aggregated statistic (say, the sum of all
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
ASIA CCS ’17, April 02-06, 2017, Abu Dhabi, United Arab Emirates
c(cid:13) 2017 ACM. ISBN 978-1-4503-4944-4/17/04. . . $15.00
DOI: http://dx.doi.org/10.1145/3052973.3052992
single values) and preserve the privacy of individuals (say, modeled
using standard differential privacy notion). In recent years there
have been many very promising results, both for the case where the
privacy is governed by a trusted authority (database curator) and
for the case where the database is distributed (see for example [35]
and [31] where the authors use combination of cryptography and
privacy preserving techniques). However, the standard differential
privacy has an obvious drawback which is a necessity of adding
a carefully calibrated noise to the ﬁnal answer to the query. This
approach is not always satisfactory, as in some cases we may need
to have the exact aggregated statistic. Moreover, as pointed in some
recent papers, adding noise may lead to signiﬁcant errors in the
aggregated statistic. Even if having noisy response is acceptable
for a given scenario, the resulting statistics may be too far from
the exact values to be usable in practice (see [20, 30]). Finally,
adding noise, speciﬁcally from a non-standard distribution, can be
technically problematic – especially when the aggregated data may
come from small, computationally constrained devices. These facts
lead to a somewhat reluctant adaptation of the differential privacy
notion in real life applications, despite its undeniable merits.
One may ask if it is possible to circumvent the problem of adding
noise while preserving the differential privacy of users. Unfor-
tunately, in the paradigm of standard differential privacy, adding
noise is inevitable. Moreover, if we assume that users operate inde-
pendently and cannot cooperate on adding randomized values used
√
to perturb the original data (which is often the case in distributed
n), where n is
systems), the size of aggregated noise has to be Ω(
the number of users (as proved in [7]).
On the other hand, observing some real-life applications of data
aggregation one can have an intuition that often it is safe to release
aggregated data without adding noise and such act does not expose
any individuals’ privacy, as pointed out in the seminal paper [5].
One of classic examples is the average national income. It is clear
that such an information says in practice nothing signiﬁcant about
the speciﬁc incomes of any of our neighbors, even though they took
part in the survey. Even revealing the average income of employees
in a big company should be secure in terms of privacy of individu-
als. In contrast, revealing the exact average income (or maximum
income) in a small community exposes users to obvious risk of pri-
vacy breach.
These intuitions have already been considered in a few papers,
namely [4, 5, 24] to mention the most signiﬁcant ones, where the
authors propose relaxations of the differential privacy model which
”utilizes” the randomness inherently present in the data itself. Our
work can be seen as a continuation and extension of the line of
research where the authors leverage adversarial uncertainty. How-
ever, in contrast to previous results we focused on detailed, non-
asymptotic analysis of the relaxed model, which is motivated by
546practical needs. Note also, that in the regime of adversarial un-
certainty one has to take into account the randomness inherently
present in the data, especially the dependencies which naturally ap-
pear in real-life scenarios. Therefore, we concentrate also on (lo-
cally) dependent data, which importance we justify in Section 4.
This required using different mathematical tools (e.g. Stein method,
see [33]). To the best of authors knowledge, that type of technical
approach was not used previously in the privacy preserving con-
text, possibly due to the fact that so far the dependent data was not
considered in a wide sense in previous papers concerning utilizing
adversarial uncertainty.
The intuition behind the noiseless privacy approach is that in real
life scenarios it might be too pessimistic to assume that the adver-
sary knows almost every record in the database. This assumption
seems far too strong, yet it stands at the heart of standard differ-
ential privacy. Indeed, it is hard to expect that revealing the exact
average worldwide income would in any way harm privacy of any
single individual. However, according to differential privacy deﬁ-
nition, that would be unacceptable. Intuitively we realize that if an
average income (or other value) of a ”large” set of participants is re-
vealed, there should not be a privacy breach. The authors of [5] and
their notion of noiseless privacy capture that intuition. Their ap-
proach allows database designer to check whether the data satisﬁes
desired privacy parameters, and if it does, just reveal the aggregated
value without adding any noise. Unfortunately, their results are
mostly only asymptotic which makes it hard to use in practice, due
to unknown constants which may hide the real size of privacy pa-
rameters. Using our methods we give explicit bounds for privacy
parameters. From practitioner’s point of view, this allows to con-
struct efﬁcient algorithms by directly using our results. Moreover,
for the few non-asymptotic results in [5] we show that our meth-
ods give better bound for privacy parameters. Despite the merits
(and theoretical importance) of leveraging adversarial uncertainty,
for this approach to become a state-of-the-art privacy promise for
various kind of data aggregation problems, it has to be easy to use
and quantify for practitioners. Showing precise bounds for privacy
parameters and also considering dependent data is the way to make
noiseless privacy more useful in practice, which is the purpose of
our paper.
To the best of our knowledge, the idea of combining standard
differential privacy techniques (i.e. Laplace mechanism, see [16])
with adversarial uncertainty was not explored before. Intuitively
we can think that in the case where the data has much random-
ness, we should be able to add smaller noise than in the case where
the data is deterministic from the adversary’s perspective. Due to
our novel approach, we give explicit bounds for privacy parameters
which allows us to explore the synergy between differential privacy
methods and noiseless privacy approach. We describe and analyse
this synergy in Section 6.
In our paper we follow the model from [5], yet present it in a
more convenient way for our approach. We show that this deﬁni-
tion is coherent with classic (computational) differential privacy –
formally speaking it is an extension. This approach can be seen as
utilizing “uncertainty“ that naturally appears in some data sources
to hide the contributions of individuals in the aggregated outcome.
We depict wide classes of data that can be handled without adding
noise and also give the explicit privacy parameters instead of only
asymptotic results. Due to explicitly given parameters, our theo-
rems can be seen as ”off the shelf” ways for a practitioner to check
whether he can safely release the data without any noise or not.
Note however, that the practitioner would still have to choose some
parameters based on domain knowledge (i.e. upper bound for the
fraction of records known to the adversary), but it is quite a com-
mon situation in both security and privacy applications.
1.1 Our results and organization of this paper
Our contribution is as follows:
• We extend the paradigm of utilizing adversarial uncertainty
for the case of dependent data (Theorems 3 and 5).
• We explore the synergy between standard differential privacy
methods and noiseless privacy approach (Theorem 6).
• We propose an adversarial model (Subsection 2.2) and ex-
plicit procedure for preserving privacy (Figure 6), which is
easy to use for practitioners.
• We give improved and explicit (non-asymptotic) bounds for
the privacy parameters (Theorems 2 and 4).
We believe that our contribution is a step towards more practical
constructions of privacy protocols which utilize adversarial uncer-
tainty. Note that, for the ﬁrst time, we consider a wide class of
dependent data. Moreover, our results state that the party respon-
sible for privacy does not need to know neither the exact structure
of dependencies nor the exact distribution of the data (i.e.
joint
distribution). Upper bounds for the size of the greatest dependent
subset and the sum of centralised third moments (or fourth in case
of dependent data) are sufﬁcient to use our results in practice. To
achieve it, we used different methods than those used in context of
adversarial uncertainty before.
The rest of this paper is organized as follows. In Section 2 we
explain the motivations, recall the idea of utilizing adversarial un-
certainty from [5] in a way that is more convenient for presenting
our results and provide some formalism that can be seen as an ex-
tension of differential privacy notion. We also introduce and dis-
cuss our adversarial model and some possible applications. In the
next sections we present our results. In Section 3 we focus on the
case when from the adversary’s perspective the aggregated data is
a set of independent random values. Most important is the case dis-
cussed in Section 4, where we allow the adversary to know a pri-
ori some dependencies between data. Note however, that the data
owner do not have to know the exact dependencies in the data. Then
in Section 5 we discuss situation where the adversary has an exact
knowledge of the values of some subset of data values. Finally in
Section 6 we explore the idea of combining adversarial uncertainty
with standard differential privacy approach.
In our paper we consider privacy guarantees for any ﬁxed size of
data, since purely asymptotic approach seems to be inadequate for
typical areas of application. Let us stress that we present formulas
that can be used for deciding if revealing aggregated data from a
given types of data is secure even for a moderate number of users.
At the end in Section 7 we recall some previous and related work.
We conclude and outline the future work in Section 8. Since our
paper is quite technical, for the sake of clarity of presentation some
of proofs and discussions about the extended deﬁnition of privacy
have been moved to the Appendix.
2. MODEL
As mentioned in the introduction, the main goal of this paper is
to make the idea of noiseless privacy (from [5]) not only an interest-
ing, theoretical concept, but a practically useful way to guarantee
some level of privacy. We want to emphasize that we use the idea
(noiseless privacy) from [5], yet we present the privacy model in
a slightly different way, which seems to be simpler and more con-
venient for our approach. Moreover it shows direct descendance
547from classical differential privacy (as presented in [16]) which may
be considered as a special case of the discussed model.
Let us present the aggregation problem in a general way. In the
system there are n users that may represent different types of par-
ties (organizations, individuals or even sensing devices). Each of
them holds a data record xi (for simplicity we assume that it is
a single value). The goal is to aggregate the data and reveal some
statistics (say, sum of the values). Note that the database may either
be a centralized one, which means that there is a database curator
whose goal is to reveal the values in a private way (namely via
adding some noise to the output), or a distributed one where users
themselves have to generate some output according to a distributed
protocol. See that in terms of privacy deﬁnition, both these cases
are equivalent. They differ in algorithmic approach to these prob-
lems. As this paper is about privacy (speciﬁcally about utilizing
adversarial uncertainty), both these cases are essentially the same
for us. Therefore by saying data we will mean the set of n values
(held either by different parties or by a single curator) which we
want to aggregate (i.e. compute the sum of these values) and reveal
the obtained statistic to the public. By saying compromised users
we will mean the subset of data about which the adversary has full
knowledge, namely he knows the exact values in this subset. By
saying data owner we will mean a party that is responsible for pre-
serving privacy of the data by designing an appropriate algorithm,
choosing adversarial model parameters (or upper bounds for them)
or deciding whether speciﬁc privacy parameters are sufﬁcient or if
they have to be combined with external noise.
2.1 Modeling privacy of randomized data
We use a privacy model in which the data (or at least part of
it) is considered random from the adversary’s perspective, coming
from a speciﬁc distribution. This kind of approach is quite natu-
ral in many scenarios, namely the adversarial knowledge is usually
limited. This “uncertainty” can be utilized. However, it needs a
different deﬁnition of privacy than standard differential privacy as
in [16], because we have to take into account randomized inputs.
Following the notion introduced in [5] we will call this approach
noiseless privacy. Before we show its formal deﬁnition, we need to
introduce a following
DEFINITION 1
(ADJACENT RANDOM VECTORS). Let X =
(X1, . . . , Xn) be an arbitrary random vector and let X(cid:48) be other
random vector. Let X∗ be a random variable. We will say that
vectors X and X(cid:48) are adjacent if and only if
(cid:48)
X
= (X1, . . . , Xi, X∗, Xi+1, . . . , Xn),
or
(cid:48)
X
for any i ∈ {1, . . . , n}.
= (X1, . . . , Xi−1, Xi+1, . . . , Xn),
This essentially captures the notion of data vectors adjacency simi-
lar to the one in [16], but for random variables rather than determin-
istic values. See also that if for some deterministic adjacent vectors
x and x(cid:48) we have X = x and X(cid:48) = x(cid:48) with probability 1, then
this deﬁnition of adjacency is the same as in [16]. Note that (as in
standard adjacency deﬁnition in [16]) we could as well deﬁne ad-
jacency in such a way that instead of adding or removing a vector
element, we could simply change its value, this is just the matter
of choice and a few straightforward technical changes in proofs.
Continuing, we can introduce a following
DEFINITION 2
(DATA SENSITIVITY). We will say that data
vector X = (X1, . . . , Xn) and mechanism M have data sensi-
tivity ∆ if an only if
|M (X) − M (X
(cid:48)
)| (cid:54) ∆,
for every vector X(cid:48) that is adjacent to X.
Note that this bears close resemblance to the l1-sensitivity de-
ﬁned in [16]. More detailed comparison of noiseless privacy and
standard differential privacy can be found in Appendix.
We can formally deﬁne noiseless privacy in the following way
DEFINITION 3
(NOISELESS PRIVACY). We say that a
privacy mechanism M and a random vector X = (X1, . . . , Xn)
preserve noiseless privacy with parameters (ε, δ) if for any random
vector X(cid:48) such that X and X(cid:48) are adjacent we have
∀B∈BP (M (X) ∈ B) (cid:54) eεP (M (X
(cid:48)
) ∈ B) + δ.
Intuitively, this deﬁnition says that if data can be considered ran-
dom, then the outcome of the coin ﬂip of any single user does
not signiﬁcantly change the result of deterministic mechanism M,
whether the user is added to the result, or removed from it. This is
very similar to standard differential privacy. A more detailed com-
parison is moved to the Appendix. Throughout this paper we will
use abbreviation (ε, δ)-NP (as in [5]) to denote noiseless privacy
with parameters ε and δ.
Clearly, this model of privacy is a coherent extension of differen-
tial privacy. We see it as a generalization of the known differential
privacy deﬁnition that can be useful for some real life scenarios.
See that in Rem.1 (Appendix) we explained that this model is in-
deed more general than differential privacy, but if we ﬁx the data
as deterministic, it is essentially the same deﬁnition. Moreover, in
Section 6 we show how the standard differential privacy methods
can be combined with noiseless privacy approach.
Whether or not (and to what extent) particular data can be con-
sidered random is of course an important problem to be solved by
the data holder, and is beyond the scope of this paper. Note that
also other papers in this line of research has not yet dealt with this
problem which may be a very interesting question for a future work.
See that in noiseless privacy, random data has natural self-hiding
properties, even though the mechanisms are deterministic. Instead
of relying on the randomness of mechanism (as in the standard dif-
ferential privacy methods), we can sometimes rely on the inherent
randomness of the data itself. Deterministic algorithms have an ob-
vious beneﬁt of not introducing any errors (which are inevitable in
standard differential privacy approach due to the addition of noise),
so the answer to a query is exact.
The most common and useful deterministic mechanism would be
simply summing all the data. In our paper we explore the privacy
parameters of mechanism M (X) = sum(X) for any distribution
of the data vector X, a wide class of dependencies in the data and
the adversarial model deﬁned in Subsection 2.2.
2.2 Adversarial Model
We assume that the adversary:
• May know the exact data of at most some fraction γ of the
users.
• May know the correct distribution (but not the value itself)
of the data of the rest of users (note that the distribution for
each user might be different).
• May know the dependencies between some of the data values
(if there are any), but only in subsets of size at most D.
548Let us now discuss and justify these assumptions. First of all,
one can easily see that in standard differential privacy we essen-
tially assume that the adversary knows the exact data of all users
except one. Here we relax this by giving an upper bound on the
number of users which are compromised. See that in realistic sce-
narios it is not very plausible that the adversary indeed knows al-
most every data record. On the other hand, we still give him quite
a lot of power, namely we assume that he knows the distributions
of the data, but not the exact values. From the point of view of the
adversary, data is a vector of (at least n − γn) random variables
with known distribution and some known (at most γn) data values.
See that in sections 3 and 4 we assume for simplicity that the ad-
versary does not know any exact values (so γ = 0). We discuss this
in Section 5 where we show how to extend our results for the case
where the adversary knows any arbitrary γn exact values.
In real-life data it is quite common to have some dependencies
involved. Moreover, the adversary might know about them. To pro-