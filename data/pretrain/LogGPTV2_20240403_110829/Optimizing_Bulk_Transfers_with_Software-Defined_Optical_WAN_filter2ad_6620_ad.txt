controller fails, we spawn a new instance, which starts to
compute and reconﬁgure the network state at the next time
slot. During the controller failover, the network still carries
trafﬁc for the current time slot and is not affected.
Group of transfers: Some applications may need to send
trafﬁc to multiple locations and the important metric is the
last completion time of all transfers in the group. This is
similar to the coﬂow concept in big data applications in data
centers [20, 21]. We can either treat them as single transfers
or use better heuristics (like Smallest-Effective-Bottleneck-
First [20]) to optimize for groups. A full exploration is our
future work.
4. OWAN IMPLEMENTATION
We have built a prototype of Owan. We describe the testbed
hardware implementation in §4.1 and the controller software
implementation in §4.2.
4.1 Owan Hardware Implementation
Our testbed has nine routers and ROADMs, and emulates
the Internet2 topology in Figure 1. We use Arista 7050S-64
as the routers. Since commercial ROADMs are expensive,
we use commodity optical components to emulate ROADMs
that have the features needed to evaluate the system.
Figure 6 shows the prototype and the optical hardware de-
sign to emulate a ROADM. The optical elements for each
ROADM is arranged in a 1U box. We have a Freescale
i.MX53 micro controller in the box to control the optical
elements. At the bottom of a ROADM, it has n(=15) ports
that interface with the router. Each interface is an optical
transceiver that can convert between electrical packets and
optical signals at different wavelengths. The ﬁfteen transceivers
are at wavelengths from 1553.33nm to 1542.14nm, which
are deﬁned at standard ITU 100GHZ channel spacing.
In order to emulate any possible network-layer topology,
we structure the nine ROADMs as a full mesh, i.e., each
ROADM has a ﬁber to connect to every other ROADM. In
this way, a ROADM can allocate the n wavelengths among
the nine ﬁbers arbitrarily as long as the total number of wave-
93
(a) Testbed.  (b) ROADM. Splitter … λ1 λ2 λ15 … DEMUX … MUX … … WSS EDFA To/From Arista Switch To/From Other ROADMs ROADM Arista Switch Servers One Site 5. EVALUATION
In this section, we present the evaluation results. Besides
a testbed that emulates the Internet2 topology, we have also
built a ﬂow-based simulator to evaluate Owan in a large scale
with topologies and trafﬁc from an ISP network as well as an
inter-DC network from an Internet service company.
5.1 Methodology
Topologies: The testbed topology has nine sites as described
in §4. We use Figure 1(b) as the network-layer topology to
evaluate TE methods with only network-layer control. The
simulations use a topology from an ISP backbone that con-
tains about 40 sites. These sites are connected into an irreg-
ular mesh. The inter-DC network has about 25 sites. There
are several sites called “super cores” that are connected to
many smaller sites, and the super cores are connected in a
ring topology.
Workloads: We obtain traces from both the ISP network
and the inter-DC network. The traces are trafﬁc counters
collected from routers. From the traces, we can get site-to-
site trafﬁc demand, but not transfer-level details like trans-
fer sizes and deadlines. Similar to previous work [3, 4], we
use synthetic models to generate transfer-level information
as follows. First, we sum up all the incoming and outgo-
ing trafﬁc demand for each site. Then we generate trans-
fers for two hours. The transfers for the ISP network follow
an exponential distribution with a mean of 500GB/5TB for
testbed/simulation experiments. For each transfer, we ran-
domly select a pair of sites whose total trafﬁc demand has
not exceeded the sum obtained from the traces. We multiply
the sum of trafﬁc demand at each site by a trafﬁc load factor
λ to evaluate the system under different loads. For deadline-
constrained trafﬁc, we choose deadlines from a uniform dis-
tribution between [T, σT ] where T is the time slot length
and σ is deadline factor that is used to change the tightness
of deadlines. The inter-DC trafﬁc follows roughly a similar
distribution (with different λ values), except for that it has
some “hotspots” in the network that generate lots of trans-
fers for a period of time, and these hotspots can move from
site to site.
Trafﬁc engineering approaches: We compare the follow-
ing approaches. Only Owan has optical-layer control. Tem-
pus [3] and Amoeba [4] only work with transfers with dead-
lines, so we only compare them on deadline-constrained traf-
ﬁc in §5.3.
• Owan: The approach described in this paper.
It jointly
controls the optical layer and the network layer.
• MaxFlow: This approach uses linear programming to max-
• MaxMinFract: This approach uses linear programming to
maximize the minimal fraction that a transfer can be served
at each time slot.
• SWAN [2]: It uses linear programming to maximize the
throughput while achieving approximate max-min fairness
for each time slot.
• Tempus [3]: It deals with deadline-constrained trafﬁc. It
ﬁrst maximizes the minimal fraction a transfer can be served
imize the total throughput for each time slot.
94
across all time slots and then maximizes the total number
of bytes that can be satisﬁed.
• Amoeba [4]: This is another approach that deals with deadline-
constrained trafﬁc. It uses graph algorithms to compute
routing and rate allocation for multiple time slots and ad-
just previous allocation when new transfers arrive.
Performance metrics: For deadline-unconstrained trafﬁc,
the primary metric is transfer completion time. We use fac-
tor of improvement to show the improvements of Owan over
other approaches, which is the transfer completion time of
the alternative approach divided by that of Owan. We also
show makespan, which is the total time to complete a series
of transfers.
For deadline-constrained trafﬁc, we use the percentage of
transfers that meet deadlines and the amount of bytes that
ﬁnish before deadlines.
Performance validation: We have validated the results of
our ﬂow-based simulator with our testbed results on the In-
ternet2 topology. The difference on the performance metrics
is within 10%, which is mainly from the imperfect rate limit-
ing and preﬁx splitting for multi-path routing on the testbed.
Testbed conﬁgurations: We run the controller on a com-
modity 2U server with two six-core Intel Xeon E5-2620v2
processors running at 2GHz. As we will show later, even
this modest conﬁguration is sufﬁcient to run the Owan core
module. All the test clients run on servers with the same con-
ﬁguration, and they connect to the network with 10GE. We
use both iperf and a custom multi-threaded trafﬁc generator
to send emulated trafﬁc, and we have veriﬁed that each client
is able to saturate a 10Gbps link. Both generators have TCP
enabled. We perform reconﬁgurations every ﬁve minutes.
5.2 Deadline-Unconstrained Trafﬁc
In this experiment, the transfer requests submitted to the
system do not have deadlines. The key metric is to optimize
the transfer completion time. Figure 7(a-c) shows the results
of the testbed experiments on the Internet2 topology. Fig-
ure 7(a) shows the factor of improvement on the average and
the 95th percentile transfer completion time under different
trafﬁc loads. Compared to MaxFlow, Owan improves the
average (95th percentile) transfer completion time by up to
4.45× (3.84×); compared to MaxMinFract, Owan improves
the average (95th percentile) transfer completion time by up
to 18.66× (6.09×); compared to SWAN, Owan improves
the average (95th percentile) transfer completion time by
5.01× (4.27×). The results show that by dynamically recon-
ﬁguring the optical layer, Owan can signiﬁcantly improve
the transfer completion time for bulk transfers on the WAN.
Moreover, we observe that Owan has bigger improvements
over MaxMinFract than MaxFlow and SWAN. This is because
MaxMinFract optimizes for the minimal fraction served by
each transfer for each time slot, which causes most transfers
to take several time slots to ﬁnish.
To further zoom in on the results, we divide the transfers
into three bins (small, middle, large) based on transfer size,
i.e., the smallest 1/3 transfers are in the small bin, the mid-
dle 1/3 in the middle bin, and the largest 1/3 in the large
(a) Improvements on completion time. (b) Improvements on completion time.
(c) CDF of completion time.
(d) Improvements on completion time. (e) Improvements on completion time.
(f) CDF of completion time.
(g) Improvements on completion time. (h) Improvements on completion time.
(i) CDF of completion time.
Figure 7: Results for deadline-unconstrained trafﬁc. (a-c), (d-f), and (g-i) are results of the Internet2 network, ISP
network, and inter-DC network, respectively.
bin. Figure 7(b) shows the factor of improvement in differ-
ent bins when the trafﬁc load factor is 1. Owan consistently
improves the average and 95th percentile transfer comple-
tion time over MaxFlow, MaxMinFract and SWAN in differ-
ent bins. We observe the most improvement is in the small
bin. This is because Owan adjusts the network-layer topol-
ogy based on trafﬁc demand and small transfers are priori-
tized to take the most beneﬁts of the topology.
To show the performance of Owan from another angle,
we also plot the CDF of the transfer completion time. Fig-
ure 7(c) shows the CDF of the transfer completion time when
the trafﬁc load is 1. In the ﬁgure, the line of Owan stays at the
leftmost, which means Owan achieves the smallest transfer
completion time across all percentiles. MaxFlow, MaxMin-
Fract and SWAN have longer tails than Owan. This means
some transfers can have longer completion time than other
transfers if MaxFlow, MaxMinFract or SWAN is used. The
reason is also due to the ﬁxed network-layer topology used
by these approaches. The ﬁxed topology causes many trans-
fers to use multiple hops to reach their destinations and the
total throughput of the network is lower than that in Owan.
Overtime, some transfers are accumulated in the scheduling
queue because of the limited total throughput and need to
take a long time to complete.
To evaluate Owan on a topology larger than our 9-site
testbed, we also perform simulations using the ISP topol-
ogy and inter-DC topology. Figure 7(d-f) and (g-i) show
the respective results. Similar to the Internet2 results, Owan
signiﬁcantly improves the transfer completion time. Specif-
ically, Figure 7(d) shows that Owan improves the average
(95th percentile) transfer completion by up to 3.52× (3.00×)
as compared to MaxFlow, 19.42× (7.86×) as compared to
MaxMinFract, and 4.03× (3.00×) as compared to SWAN.
Also, Owan is better than the other three approaches across
different bins (Figure 7(e)) and different percentiles (Fig-
ure 7(f)). Figure 7(g-i) shows similar improvement factors
on the inter-DC topology.
Finally, we show the improvement on makespan. Makespan
is the total time to ﬁnish a given number of requests. We in-
ject trafﬁc requests for two hours and measure the makespan
of different approaches under different trafﬁc loads. Figure 8
shows the improvement of Owan on makespan over the other
three approaches. From the ﬁgure, we can see that Owan
improves the makespan by up to 2.56×, 1.80× and 1.60×
in the three topologies respectively. The improvement in-
creases with the trafﬁc load. This is because by reconﬁgur-
ing the topology Owan can achieve higher total throughput
and thus ﬁnish more requests in a certain time. When the
load is higher, MaxFlow, MaxMinFract and SWAN have more
transfers accumulated over time than Owan.
95
 0 5 10 15 200.51.01.52.0Factor of ImprovementTraffic Load Factorw.r.t MaxFlow, avgw.r.t MaxFlow, 95-pctw.r.t MaxMinFract, avgw.r.t MaxMinFract, 95-pctw.r.t SWAN, avgw.r.t SWAN, 95-pct 0 0.2 0.4 0.6 0.8 1 0 10000 20000 30000CDFTransfer Completion Time (s)OwanMaxFlowMaxMinFractSWAN 0 5 10 15 200.51.01.52.0Factor of ImprovementTraffic Load Factor 0 10 20 30 40SmallMiddleLargeAllFactor of ImprovementBins 0 0.2 0.4 0.6 0.8 1 0 10000 20000 30000CDFTransfer Completion Time (s) 0 5 10 15 200.51.01.52.0Factor of ImprovementTraffic Load Factor 0 10 20 30 40SmallMiddleLargeAllFactor of ImprovementBins 0 0.2 0.4 0.6 0.8 1 0 10000 20000 30000CDFTransfer Completion Time (s) 0 5 10 15 200.51.01.52.0Factor of ImprovementTraffic Load Factor 0 20 40 60 80SmallMiddleLargeAllFactor of ImprovementBins 0 0.2 0.4 0.6 0.8 1 0 10000 20000 30000CDFTransfer Completion Time (s)(a) Internet2 result.
(b) ISP result.
Figure 8: Improvements on makespan.
(c) Inter-DC result.
5.3 Deadline-Constrained Trafﬁc
This experiment evaluates the performance of Owan for
deadline-constrained trafﬁc. In addition to MaxFlow, MaxMin-
Fract and SWAN, we also compare Owan with another two
approaches, Tempus and Amoeba, which are speciﬁcally de-
signed for deadline-constrained trafﬁc on the WAN. Figure 9(a-
c) shows the results of testbed experiments on the Internet2
topology. Figure 9(a) shows the percentage of transfers that
meet deadlines under different deadline factors. Owan en-
ables the most number of transfers to meet deadlines. Amoeba
is particularly designed for transfers to meet deadlines and
performs the second best. The objective of Tempus is to
maximize the minimal fraction served for each transfer across
all time slots and then maximize the total bytes that ﬁnish be-
fore their deadlines. Thus it has relative poor performance
to enable transfers to meet their deadlines. Overall, Owan
increases the number of transfers that meet their deadlines
by up to 1.36×, as compared to Amoeba, which performs
the second best. The improvement is relatively small when
the deadline factor is too small or too large. This is because
when the deadline factor is too small, all the transfers have
tight deadlines and there is little room for Owan to further
increase the number of transfers that can meet their dead-
lines. When the deadline factor is too large, many transfers
can easily meet their deadlines, and the absolute value of the
percentage is already high. The beneﬁt of reconﬁguring the
optical layer is most signiﬁcant when the deadline factor is
not at extreme values.
Besides the percentage of transfers that meet their dead-