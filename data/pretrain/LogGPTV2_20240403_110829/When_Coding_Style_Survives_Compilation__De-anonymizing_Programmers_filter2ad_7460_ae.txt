with no optimizations. Compilation with optimization level-2
optimizes more than level-1, uses all level-1 optimization ﬂags
and more. Level-2 optimization performs all optimizations that
do not involve a space-speed tradeoff. Level-2 optimization
increases compilation time and performance of the generated
code when compared to level-1 optimization. Level-3 opti-
mization yet optimizes more than both level-1 and level-2.
So far, we have shown that programming style features
survive compilation without any optimizations. As compilation
with optimizations transforms code further, we investigate how
much programming style is preserved in executable binaries
that have gone through compilation with optimization. Our
results summarized in Table III show that programming style
is preserved to a great extent even in the most aggressive level-
3 optimization. This shows that programmers of optimized
executable binaries can be de-anonymized and optimization
is not a highly effective code anonymization method.
Number
Programmers
of
100
100
100
100
Number of
Training
Samples
8
8
8
8
Compiler
Optimization
Level
None
1
2
3
Accuracy
96%
93%
89%
89%
TABLE III: Programmer De-anonymization with Compiler
Optimization
B. Removing symbol information does not anonymize exe-
cutable binaries.
To investigate the relevance of symbol
information for
classiﬁcation accuracy, we repeat our experiments with 100
10
authors presented in the previous section on fully stripped
executable binaries, that is, executable binaries where symbol
information is missing completely. We obtain these executable
binaries using the standard utility GNU strip on each ex-
ecutable binary sample prior to analysis. Upon removal of
symbol
information, without any optimizations, we notice
a decrease in classiﬁcation accuracy by 24%, showing that
stripping symbol information from executable binaries is not
effective enough to anonymize an executable binary sample.
C. We can de-anonymize programmers from obfuscated
binaries.
We are furthermore interested in ﬁnding out whether our
method is capable of dealing with simple binary obfusca-
tion techniques as implemented by tools such as Obfuscator-
LLVM [24]. These obfuscators substitute instructions by other
semantically equivalent instructions, they introduce bogus con-
trol ﬂow, and can even completely ﬂatten control ﬂow graphs.
For this experiment, we consider a set of 100 programmers
from the GCJ data set, who all have 9 executable binary
samples. This is the same data set as considered in our main
experiment (see Section V-D), however, we now apply all three
obfuscation techniques implemented by Obfuscator-LLVM to
the samples prior to learning and classiﬁcation.
We proceed to train a classiﬁer on obfuscated samples.
This approach is feasible in practice as an analyst who has
only non-obfuscated samples available can easily obfuscate
them to obtain the necessary obfuscated samples for classiﬁer
training. Using the same features as in Section V-D, we obtain
an accuracy of 88% in correctly classifying authors.
D. De-anonymization in the Wild
To better assess the applicability of our programmer de-
anonymization approach in the wild, we extend our experi-
ments to code collected from real open-source programs as
opposed to solutions for programming competitions. To this
end, we automatically collected source ﬁles from the popular
open-source collaboration platform GitHub [4]. Starting from
a seed set of popular repositories, we traversed the platform
to obtain C/C++ repositories that meet the following criteria.
Only one author has committed to the repository. The reposi-
tory is popular as indicated by the presence of at least 5 stars,
a measure of popularity for repositories on GitHub. Moreover,
it is sufﬁciently large, containing a total of 200 lines at least.
The repository is not a fork of another repository, nor is it
named ‘linux’, ‘kernel’, ‘osx’, ‘gcc’, ‘llvm’, ‘next’, as these
repositories are typically copies of the so-named projects.
We cloned 439 repositories from 161 authors meeting these
criteria and collect only C/C++ ﬁles for which the main author
has contributed at least 5 commits and the commit messages
do not contain the word ’signed-off’, a message that typically
indicates that the code is written by another person. An author
and her ﬁles are included in the dataset only if she has written
at least 10 different ﬁles. In the ﬁnal step, we manually veriﬁed
ground truth on authorship for the selected ﬁles to make sure
that they do not show any clear signs of code reuse from other
projects. The resulting dataset had 2 to 344 ﬁles and 2 to 8
repositories from each author, with a total of 3,438 ﬁles.
We developed our method and evaluated it on the GCJ
dataset, but collecting code from open source projects is an-
other option for constructing a dataset. Open source projects do
not guarantee ground truth on authorship. The feature vectors
might capture topics of the project instead of programming
style. As a result, open source code does not constitute the ideal
data for authorship analysis; however, it allows us to better
assess the applicability of programmer de-anonymization in the
wild. We therefore present results from a dataset collected from
the hosting platform GitHub, which we obtain by spidering the
platform to collect C and C++ repositories.
We subsequently compile the collected projects to obtain
object ﬁles for each of the selected source ﬁles. We perform our
experiment on object ﬁles as opposed to entire binaries, since
the object ﬁles are the binary representations of the source ﬁles
that clearly belong to the speciﬁed authors.
For different reasons, compiling code may not be possible
for a project, e.g., the code may not be in a compilable state,
it may not be compilable for our target platform (32 bit Intel,
Linux), or the ﬁles to setup a working build environment can
no longer be obtained. Despite these difﬁculties, we are able
to generate 1,075 object ﬁles from 90 different authors, where
the number of object ﬁles per author ranges from 2 to 24, with
most authors having at least 9 samples. We used 50 of these
authors that have 6 to 15 ﬁles to perform a machine learning
experiment with more balanced class sizes.
We extract the information gain features that were selected
from GCJ data from this GitHub dataset. GitHub datasets are
noisy for two reasons since the executable binaries used in
de-anonymization might contain properties from third party
libraries and code. For these two reasons, it is more difﬁcult to
attribute authorship to anonymous executable binary samples
from GitHub, but nevertheless we reach 65% accuracy in
correctly classifying these programmers’ executable binaries.
Another difﬁculty in this particular dataset is that there is
not much training data to train an accurate random forest
classiﬁer that models each programmer. For example, we can
de-anonymize the two programmers with the most samples,
one with 11 samples and one with 7, with 100% accuracy.
Being able to de-anonymize programmers in the wild by
using a small number of features obtained from our clean
development dataset is a promising step towards attacking
more challenging real-world de-anonymization problems.
E. Have I seen this programmer before?
While attempting to de-anonymize programmers in real-
world settings, we cannot be certain that we have formerly
encountered code samples from the programmers in the test
set. As a mechanism to check whether an anonymous test
ﬁle belongs to one of the candidate programmers in the
training set, we extend our method to an open world setting by
incorporating classiﬁcation conﬁdence thresholds. In random
forests, the class probability or classiﬁcation conﬁdence P (Bi)
that executable binary B is of class i is calculated by taking
the percentage of trees in the random forest that voted for class
i during classiﬁcation.
P (Bi) =
11
(cid:80)
j Vj(i)
|T|f
(1)
1
0.8
0.6
0.4
0.2
0
Accuracy
Precision
Recall
0.2
0.4
0.6
0.8
1
Classiﬁcation Conﬁdence
Fig. 7: Conﬁdence Thresholds for Veriﬁcation
There are multiple ways to assess classiﬁer conﬁdence
and we devise a method that calculates the classiﬁcation
conﬁdence by using classiﬁcation margins. In this setting, the
classiﬁcation margin of a single instance is the difference
between the highest and second highest P (Bi). The ﬁrst step
towards attacking an open world classiﬁcation task is identify-
ing the conﬁdence threshold of the classiﬁer for classiﬁcation
veriﬁcation. As long as we determine a conﬁdence threshold
based on training data, we can calculate the probability that
an instance belongs to one of the programmers in the training
set and accordingly accept or reject the classiﬁcation.
We performed 900 classiﬁcations in a 100-class problem to
determine the conﬁdence threshold based on the training data.
The accuracy was 95%. There were 40 misclassiﬁcations with
an average classiﬁcation conﬁdence of 0.49. We took another
set of 100 programmers with 900 samples. We classify these
900 samples with the closed world classiﬁer that was trained
in the ﬁrst step on samples from a disjoint set of programmers.
All of the 900 samples are attributed to a programmer in the
closed world classiﬁer with a mean classiﬁcation conﬁdence
of 0.40. We can pick a veriﬁcation threshold and reject all
classiﬁcations with conﬁdence below the selected threshold.
Accordingly all the rejected open world samples and mis-
classiﬁcations become true negatives, and the rejected correct
classiﬁcations end up as false negatives. Open world samples
and misclassiﬁcations above the threshold are false positives
and the correct classiﬁcations are true positives. Based on
this, we generate an accuracy, pecision, and recall graph
with varying conﬁdence threshold values in Figure 7. This
ﬁgure shows that the optimal rejection threshold to guarantee
90% accuracy on 1,800 samples and 100 classes is around
conﬁdence 0.72. Other conﬁdence thresholds can be picked
based on precision and recall trade-offs. These results are
encouraging for extending our programmer de-anonymization
method to open world settings where an analyst deals with
many uncertainties under varying fault tolerance levels.
The experiments in this section can be used in software
forensics to ﬁnd out the programmer of a piece of malware. In
software forensics, the analyst does not know if source code
belongs to one of the programmers in the candidate set of
programmers. In such cases, we can classify the anonymous
source code, and if the majority number of votes of trees in
the random forest is below a certain threshold, we can reject
the classiﬁcation considering the possibility that it might not
belong to any of the classes in the training data. By doing so,
we can scale our approach to an open world scenario, where
we might not have encountered the suspect before. As long
as we determine a conﬁdence threshold based on training data
12
[43], we can calculate the probability that an instance belongs
to one of the programmers in the set and accordingly accept
or reject the classiﬁcation. We performed 270 classiﬁcations
in a 30-class problem using all the features to determine the
conﬁdence threshold based on the training data. The accuracy
was 96.67%. There were 9 misclassiﬁcations and all of them
were classiﬁed with less than 15% conﬁdence by the classiﬁer.
Where Vj(i) = 1 if the jth tree voted for class i and 0
otherwise, and |T|f denotes the total number of trees in forest
i P (Ci) = 1 and P (Ci) ≥ 0
∀ i, allowing us to treat P (Ci) as a probability measure.
f. Note that by construction, (cid:80)
F. Case Study: Nulled.IO Hacker Forum
On May 6, 2016 the well known ‘hacker’ forum Nulled.IO
was compromised and its forum dump was leaked along with
the private messages of its 585,897 members. The members
of these forums share, sell, and buy stolen credentials and
cracking software. A high number of the forum members are
active developers that write their own code and sell them, or
share some of their code for free in public GitHub repositories
along with tutorials on how to use them. The private messages
of the sellers in the forum include links to their products and
even to screenshots of how the products work, for buyers. We
were able to ﬁnd declared authorship along with active links
to members’ software on sharing sites such as FileDropper2
and MediaFire3 in the private messages.
For our case study, we created a dataset from four forum
members with a total of thirteen Windows executables. One
of the members had only one sample, which we used to test
the open world setting described in Section VI-E. A challenge
encountered in this case study is that the binary programs ob-
tained from Nulled.IO do not contain native code, but bytecode
for the Microsoft Common Language Infrastructure (CLI).
Therefore, we cannot immediately analyze them using our
existing toolchain. We address this problem by ﬁrst translating
bytecode into corresponding native code using the Microsoft
Native Image Generator (ngen.exe), and subsequently forcing
the decompiler to treat the generated output ﬁles as regular
native code for binaries. On the other hand, radare2 is not
able to disassemble such output or the original executables.
Consequently we had access to a subset of the information
gain feature set obtained from GCJ. We extracted a total of
605 features consisting of decompiled source code features and
ndisasm disassembly features. Nevertheless, we are able to de-
anonymize these programmers with 100% accuracy while the
one sample from the open world class is classiﬁed in all cases
with the lowest conﬁdence, such as 0.4, which is below the
veriﬁcation threshold and is recognized by the classiﬁer as a
sample that does not belong to the rest of the programmers.
A larger de-anonymization attack can be carried out by
collecting code from GitHub users with relevant repositories
and identifying all the available executables mentioned in the
public portions of hacker forums. GitHub code can be com-
piled with necessary parameters and used with the approach
described in Section VI-D. Incorporating veriﬁcation thresh-
olds from Section VI-E can help handle programmers with
2www.ﬁledropper.com: ‘Simplest File Hosting Website..’
3www.mediaﬁre.com: ‘All your media, anywhere you go’
only one sample. Consequently a large number of members
can be linked, reduced to a cluster or directly de-anonymized.
The countermeasure against real-world programmer de-
anonymization attacks requires a combination of various pre-
cautions. Developers should not have any public repositories.
A set of programs should not be released by the same online
identity. Programmers should try to have a different coding
style in each piece of software they write and also try to code
in different programming languages. Software should utilize
different optimizations and obfuscations to avoid deterministic
patterns. A programmer who accomplishes randomness across
all potential identifying factors would be very difﬁcult to de-
anonymize. Nevertheless, even the most privacy savvy devel-
oper might be willing to contribute to open source software or
build a reputation for her identity based on her set of products,
which would be a challenge for maintaining anonymity.
Some of these developers obfuscate their code with the
primary goal of hiding the source code and consequently
they are experienced in writing or using obfuscators and
deobfuscators. An additional challenge encountered in this case
study is that the binary programs obtained from NULLED.io
do not contain native code, but bytecode for the Microsoft
Common Language Infrastructure (CLI). Therefore, we can-
not immediately analyze them using our existing toolchain.
We address this problem by ﬁrst translating bytecode into
corresponding native code using the Microsoft Native Image
Generator (ngen), and subsequently forcing the decompiler to
treat the generated output ﬁles as regular native code binaries.
VII. DISCUSSION
Our experiments are devised for a setting where the pro-
grammer is not trying to hide her coding style, and there-
fore, only basic obfuscation techniques are considered in our
experiments. Accordingly, we focus on the general case of
executable binary authorship attribution, which is a serious
threat to privacy but at the same time an aid for forensic
analysis.
We consider two data sets: the GCJ dataset, and a dataset
based on GitHub repositories. Using the GitHub dataset, we
show that we can perform programmer de-anonymization with
executable binary authorship attribution in the wild. We de-
anonymize GitHub programmers by using stylistic features
obtained from the GCJ dataset. Using the same small set
of features, we perform a case study on the leaked hacker
forum Nulled.IO and de-anonymize four of its members. The
successful de-anonymization of programmers from different