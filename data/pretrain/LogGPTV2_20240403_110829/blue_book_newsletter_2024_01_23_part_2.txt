## Infrastructure Solutions
### [AWS Savings plan](aws_savings_plan.md)
* New: [EC2 Instance savings plan versus reserved instances.](aws_savings_plan.md#ec2-instance-savings-plan-versus-reserved-instances)
    I've been comparing the EC2 Reserved Instances and of the EC2 instance family savings plans and decided to go with the second because:
    - They both have almost the same rates. Reserved instances round the price at the 3rd decimal and the savings plan at the fourth, but this difference is neglegible.
    - Savings plan are easier to calculate, as you just need to multiply the number of instances you want times the current rate and add them all up.
    - Easier to understand: To reserve instances you need to take into account the instance flexibility and the normalization factors which makes it difficult both to make the plans and also to audit how well you're using it.
    - Easier to audit: In addition to the above point, you have nice dashboards to see the coverage and utilization over time of your ec2 instance savings plans, which are at the same place as the other savings plans.
* New: [Important notes when doing a savings plan.](aws_savings_plan.md#doing-your-savings-plan)
    - Always use the reservation rates instead of the on-demand rates!
    - Analyze your coverage reports. You don't want to have many points of 100% coverage as it means that you're using less resources than you've reserved. On the other hand it's fine to sometimes use less resources than the reserved if that will mean a greater overall savings. It's a tight balance.
    - The Savings plan reservation is taken into account at hour level, not at month or year level. That means that if you reserve 1$/hour of an instance type and you use for example 2$/hour half the day and 0$/hour half the day, you'll have a 100% coverage of your plan the first hour and another 1$/hour of on-demand infrastructure cost for the first part of the day. On the second part of the day you'll have a 0% coverage. This means that you should only reserve the amount of resources you plan to be using 100% of the time throughout your savings plan. Again you may want to overcommit a little bit, reducing the utilization percentage of a plan but getting better savings in the end.
## Monitoring
### [Promtail](promtail.md)
* New: [Scrape journald logs.](promtail.md#scrape-journald-logs)
    On systems with `systemd`, Promtail also supports reading from the journal. Unlike file scraping which is defined in the `static_configs` stanza, journal scraping is defined in a `journal` stanza:
    ```yaml
    scrape_configs:
      - job_name: journal
        journal:
          json: false
          max_age: 12h
          path: /var/log/journal
          labels:
            job: systemd-journal
        relabel_configs:
          - source_labels: ['__journal__systemd_unit']
            target_label: unit
          - source_labels: ['__journal__hostname']
            target_label: hostname
          - source_labels: ['__journal_syslog_identifier']
            target_label: syslog_identifier
          - source_labels: ['__journal_transport']
            target_label: transport
          - source_labels: ['__journal_priority_keyword']
            target_label: keyword
    ```
    All fields defined in the journal section are optional, and are just provided here for reference.
    - `max_age` ensures that no older entry than the time specified will be sent to Loki; this circumvents `entry too old` errors.
    - `path` tells Promtail where to read journal entries from.
    - `labels` map defines a constant list of labels to add to every journal entry that Promtail reads.
    - `matches` field adds journal filters. If multiple filters are specified matching different fields, the log entries are filtered by both, if two filters apply to the same field, then they are automatically matched as alternatives.
    - When the `json` field is set to true, messages from the journal will be passed through the pipeline as JSON, keeping all of the original fields from the journal entry. This is useful when you don’t want to index some fields but you still want to know what values they contained.
    - When Promtail reads from the journal, it brings in all fields prefixed with `__journal_` as internal labels. Like in the example above, the `_SYSTEMD_UNIT` field from the journal was transformed into a label called `unit` through `relabel_configs`. Keep in mind that labels prefixed with `__` will be dropped, so relabeling is required to keep these labels. Look at the [systemd man pages](https://www.freedesktop.org/software/systemd/man/latest/systemd.journal-fields.html) for a list of fields exposed by the journal.
    By default, Promtail reads from the journal by looking in the `/var/log/journal` and `/run/log/journal` paths. If running Promtail inside of a Docker container, the path appropriate to your distribution should be bind mounted inside of Promtail along with binding `/etc/machine-id`. Bind mounting `/etc/machine-id` to the path of the same name is required for the journal reader to know which specific journal to read from.
    ```bash
    docker run \
      -v /var/log/journal/:/var/log/journal/ \
      -v /run/log/journal/:/run/log/journal/ \
      -v /etc/machine-id:/etc/machine-id \
      grafana/promtail:latest \
      -config.file=/path/to/config/file.yaml
    ```
* New: [Scrape docker logs.](promtail.md#scrape-docker-logs)
    Docker service discovery allows retrieving targets from a Docker daemon. It will only watch containers of the Docker daemon referenced with the host parameter. Docker service discovery should run on each node in a distributed setup. The containers must run with either the `json-file` or `journald` logging driver.
    Note that the discovery will not pick up finished containers. That means Promtail will not scrape the remaining logs from finished containers after a restart.
    ```yaml
    scrape_configs:
      - job_name: docker
        docker_sd_configs:
          - host: unix:///var/run/docker.sock
            refresh_interval: 5s
        relabel_configs:
          - source_labels: ['__meta_docker_container_id']
            target_label: docker_id
          - source_labels: ['__meta_docker_container_name']
            target_label: docker_name
    ```
    The available meta labels are:
    - `__meta_docker_container_id`: the ID of the container
    - `__meta_docker_container_name`: the name of the container
    - `__meta_docker_container_network_mode`: the network mode of the container
    - `__meta_docker_container_label_`: each label of the container
    - `__meta_docker_container_log_stream`: the log stream type stdout or stderr
    - `__meta_docker_network_id`: the ID of the network
    - `__meta_docker_network_name`: the name of the network
    - `__meta_docker_network_ingress`: whether the network is ingress
    - `__meta_docker_network_internal`: whether the network is internal
    - `__meta_docker_network_label_`: each label of the network
    - `__meta_docker_network_scope`: the scope of the network
    - `__meta_docker_network_ip`: the IP of the container in this network
    - `__meta_docker_port_private`: the port on the container
    - `__meta_docker_port_public`: the external port if a port-mapping exists
    - `__meta_docker_port_public_ip`: the public IP if a port-mapping exists
    These labels can be used during relabeling. For instance, the following configuration scrapes the container named `flog` and removes the leading slash (/) from the container name.
    yaml
    ```yaml
    scrape_configs:
      - job_name: flog_scrape
        docker_sd_configs:
          - host: unix:///var/run/docker.sock
            refresh_interval: 5s
            filters:
              - name: name
                values: [flog]
        relabel_configs:
          - source_labels: ['__meta_docker_container_name']
            regex: '/(.*)'
            target_label: 'container'
    ```
# Operating Systems
## Linux
### [Tabs vs Buffers](vim_tabs.md)
* New: [Switch to the previous opened buffer.](vim_tabs.md#switch-to-the-previous-opened-buffer)
    Often the buffer that you want to edit is the buffer that you have just left. Vim provides a couple of convenient commands to switch back to the previous buffer. These are `` (or ``) and `:b#`. All of them are inconvenient so I use the next mapping:
    ```vim
    nnoremap  :b#
    ```
### [Jellyfin](jellyfin.md)
* New: [Python library.](jellyfin.md#python-library)
    [This is the API client](https://github.com/jellyfin/jellyfin-apiclient-python/tree/master) from Jellyfin Kodi extracted as a python package so that other users may use the API without maintaining a fork of the API client. Please note that this API client is not complete. You may have to add API calls to perform certain tasks.
    It doesn't (yet) support async
### [journald](journald.md)
* New: Introduce journald.
    [journald](https://www.freedesktop.org/software/systemd/man/latest/systemd-journald.service.html) is a system service that collects and stores logging data. It creates and maintains structured, indexed journals based on logging information that is received from a variety of sources:
    - Kernel log messages, via kmsg
    - Simple system log messages, via the `libc syslog` call
    - Structured system log messages via the native Journal API.
    - Standard output and standard error of service units.
    - Audit records, originating from the kernel audit subsystem.
    The daemon will implicitly collect numerous metadata fields for each log messages in a secure and unfakeable way.
    Journald provides a good out-of-the-box logging experience for systemd. The trade-off is, journald is a bit of a monolith, having everything from log storage and rotation, to log transport and search. Some would argue that syslog is more UNIX-y: more lenient, easier to integrate with other tools. Which was its main criticism to begin with. When the change was made [not everyone agreed with the migration from syslog](https://rainer.gerhards.net/2013/05/rsyslog-vs-systemd-journal.html) or the general approach systemd took with journald. But by now, systemd is adopted by most Linux distributions, and it includes journald as well. journald happily coexists with syslog daemons, as:
    - Some syslog daemons can both read from and write to the journal
    - journald exposes the syslog API
    It provides lots of features, most importantly:
    - Indexing. journald uses a binary storage for logs, where data is indexed. Lookups are much faster than with plain text files.
    - Structured logging. Though it’s possible with syslog, too, it’s enforced here. Combined with indexing, it means you can easily filter specific logs (e.g. with a set priority, in a set timeframe).
    - Access control. By default, storage files are split by user, with different permissions to each. As a regular user, you won’t see everything root sees, but you’ll see your own logs.
    - Automatic log rotation. You can configure journald to keep logs only up to a space limit, or based on free space.
### [Kodi](kodi.md)
* New: [Extract kodi data from the database.](kodi.md#from-the-database)
    At `~/.kodi/userdata/Database/MyVideos116.db` you can extract the data from the next tables:
    - In the `movie_view` table there is:
      - `idMovie`: kodi id for the movie
      - `c00`: Movie title
      - `userrating`
      - `uniqueid_value`: The id of the external web service
      - `uniqueid_type`: The web it extracts the id from
      - `lastPlayed`: The reproduction date
    - In the `tvshow_view` table there is:
      - `idShow`: kodi id of a show
      - `c00`: title
      - `userrating`
      - `lastPlayed`: The reproduction date
      - `uniqueid_value`: The id of the external web service
      - `uniqueid_type`: The web it extracts the id from
    - In the `season_view` there is no interesting data as the userrating is null on all rows.
    - In the `episode_view` table there is:
      - `idEpisodie`: kodi id for the episode
      - `idShow`: kodi id of a show
      - `idSeason: kodi id of a season
      - `c00`: title
      - `userrating`
      - `lastPlayed`: The reproduction date
      - `uniqueid_value`: The id of the external web service
      - `uniqueid_type`: The web it extracts the id from. I've seen mainly tvdb and sonarr
    - Don't use the `rating` table as it only stores the ratings from external webs such as themoviedb:
### [Matrix Highlight](matrix_highlight.md)
* New: Introduce matrix_highlight.
    [Matrix Highlight](https://github.com/DanilaFe/matrix-highlight) is a decentralized and federated way of annotating the web based on Matrix.
    Think of it as an open source alternative to [hypothesis](hypothesis.md).
    It's similar to [Populus](https://github.com/opentower/populus-viewer) but for the web.
    I want to try it and investigate further specially if you can:
    - Easily extract the annotations
    - Activate it by default everywhere
### [Mediatracker](mediatracker.md)
* New: Introduce python library.
    There is a [python library](https://github.com/jonkristian/pymediatracker) although it's doesn't (yet) have any documentation and the functionality so far is only to get information, not to push changes.
* New: [Get list of tv shows.](mediatracker.md#get-list-of-tv-shows)
    With `/api/items?mediaType=tv` you can get a list of all tv shows with the next interesting fields:
    - `id`: mediatracker id
    - `tmdbId`:
    - `tvdbId`:
    - `imdbId`:
    - `title`:
    - `lastTimeUpdated`: epoch time
    - `lastSeenAt`: epoch time
    - `seen`: bool
    - `onWatchlist`: bool
    - `firstUnwatchedEpisode`:
      - `id`: mediatracker episode id
      - `episodeNumber`:
      - `seasonNumber`
      - `tvShowId`:
      - `seasonId`:
    - `lastAiredEpisode`: same schema as before
    Then you can use the `api/details/{mediaItemId}` endpoint to get all the information of all the episodes of each tv show.
# Arts
## Music
### [Sister Rosetta Tharpe](sister_rosetta_tharpe.md)
* New: Introduce Sister Rosetta Tharpe.
    Sister Rosetta Tharpe was a visionary, born in 1915 she started shredding the guitar in ways that did not exist in that time. Yes, she founded Rock and Roll. It's lovely to see a gospel singer with an electrical guitar.
    In [this video](https://yewtu.be/watch?v=JeaBNAXfHfQ) you'll be able to understand how awesome she ws.
    Videos:
    - [Up Above my head](https://yewtu.be/watch?v=JeaBNAXfHfQ)
    - [Didn't it rain? 1964](https://yewtu.be/watch?v=Y9a49oFalZE)
    - [Nice Best of compilation](https://yewtu.be/watch?v=UnDMangsOMc)
# Science
## Data Analysis
### [Parsers](parsers.md)
* New: [Learning about parsers.](parsers.md#learning-about-parsers)
    Parsers are a whole world. I kind of feel a bit lost right now and I'm searching for good books on the topic. So far I've found:
    - [Crafting Interpreters](https://craftinginterpreters.com/introduction.html).
      Pros:
      - Pleasant to read
      - Doesn't use external tools, you implement it from scratch.
      - Multiple format: EPUB, PDF, web
      - You can read it for free
      - Cute drawings <3
      Cons:
      - Code snippets are on Java and C
      - Doesn't use external tools, you implement it from scratch
      - It's long
    - Compilers: Principles, Techniques, and Tools by Aho, Alfred V. & Monica S. Lam & Ravi Sethi & Jeffrey D. Ullman
      Pros:
      - EPUB
      Cons:
      - Code snippets are on C++
    - Parsing Techniques: A Practical Guide by Dick Grune and Ceriel J.H Jacobs
      Pros:
      - Gives an overview of many grammars and parsers
      Cons:
      - Only in PDF
      - It's long
      - Too focused on the theory, despite the name xD