### ρFEM Overhead Analysis with SPEC CPU2017

**Table 7: ρFEM’s Overhead Relative to the SPEC CPU2017 Benchmark**

| Benchmark       | Overhead (%) |
|-----------------|--------------|
| 500.perlbench_r | -0.91        |
| 505.mcf_r       | 0.00         |
| 520.omnetpp_r   | 0.37         |
| 523.xalancbmk_r | 2.05         |
| 525.x264_r      | 0.54         |
| 531.deepsjeng_r | 1.47         |
| 541.leela_r     | 0.79         |
| 557.xz_r        | -0.11        |
| 508.namd_r      | 0.78         |
| 511.povray_r    | 4.52         |
| 519.lbm_r       | 0.00         |
| 526.blender_r   | 0.00         |
| 538.imagick_r   | -0.57        |
| 600.perlbench_s | -1.14        |
| 605.mcf_s       | 0.30         |
| 620.omnetpp_s   | 3.94         |
| 623.xalancbmk_s | 4.83         |
| 625.x264_s      | 1.17         |
| 631.deepsjeng_s | 1.48         |
| 641.leela_s     | 0.00         |
| 657.xz_s        | -8.74        |
| Geometric Mean  | 0.11         |

**Table 8: Detailed Overhead Analysis for ρFEM**

| Benchmark       | Overhead (%) | Standard Deviation |
|-----------------|--------------|--------------------|
| 500.perlbench_r | -0.91        | 11.1               |
| 505.mcf_r       | 0.00         | 2.56%              |
| 520.omnetpp_r   | 0.37         | 11.7               |
| 523.xalancbmk_r | 2.05         | -0.87%             |
| 525.x264_r      | 0.54         | 8.02               |
| 531.deepsjeng_r | 1.47         | 0.00%              |
| 541.leela_r     | 0.79         | -4.30%             |
| 557.xz_r        | -0.11        | 18.5               |
| 508.namd_r      | 0.78         | -2.94%             |
| 511.povray_r    | 4.52         | 13.4               |
| 519.lbm_r       | 0.00         | -0.79%             |
| 526.blender_r   | 0.00         | -4.58%             |
| 538.imagick_r   | -0.57        | 8.74               |
| 600.perlbench_s | -1.14        | -1.56%             |
| 605.mcf_s       | 0.30         | 12.7               |
| 620.omnetpp_s   | 3.94         | 0.56%              |
| 623.xalancbmk_s | 4.83         | 16.9               |
| 625.x264_s      | 1.17         | 0.00%              |
| 631.deepsjeng_s | 1.48         | 4.82               |
| 641.leela_s     | 0.00         | 17.1               |
| 657.xz_s        | -8.74        | -8.52%             |
| 1.42            | 3.55         | 5.68%              |
| 2.89            | 3.66         | 2.14%              |
| -0.97           | 5.09         | 0.99%              |
| 2.21            | 3.62         | -8.74%             |
| 2.06            | 2.24         | 0.85%              |

The average overhead of ρFEM across all benchmarks is 0.85%, indicating that it is highly competitive in terms of performance.

### Comparison of ρFEM, SafeStack, and IVT Overheads

**Figure 7: Runtime Overhead Comparison**

In addition to the SPEC CPU 2017 benchmark, we evaluated ρFEM using a set of popular JavaScript benchmarks for the Chrome Web browser. The figure below compares the runtime overheads of SafeStack (shaded light gray), ρFEM + IVT (shaded gray; note that IVT provides virtual call-based forward-edge protection only), and ρFEM (shaded black) for these benchmarks.

- **ρFEM + IVT**: When running ρFEM incrementally together with IVT, which offers both forward-edge and backward-edge protection, the geometric mean runtime overhead is 4.86%.
- **SafeStack**: The geometric mean runtime overhead for SafeStack is 4.89%.
- **ρFEM alone**: The geometric mean runtime overhead for ρFEM alone is 3.44%.

From these results, we conclude that ρFEM's runtime overhead is negligible, making it suitable as an always-on solution.

### Limitations and Future Work

1. **Number of Function Returns**:
   - Clang SafeStack (based on LLVM v.3.7.0 stable) enforces fewer return addresses per callee compared to ρFEM. In future work, we aim to reduce the number of return addresses by analyzing gadget availability and performing a detailed analysis of provided and consumed parameters at each callsite.

2. **Attacker Access to Legitimate Gadgets**:
   - The number of callee return addresses is generally larger than one, which can be exploited by attackers to jump to legitimate addresses and access useful gadgets. We plan to address this by analyzing all legitimate callee return sites and transforming the program at the instruction level to make the gadgets unusable.

3. **Feasibility of Attacks with Protection**:
   - Even with ρFEM protection, attacks are still possible if the attacker knows the legitimate return address of a callee and a usable return address to access a gadget. We will explore adding another level of indirection (e.g., re-purposed register-based trampolines) to mitigate this risk.

4. **RET Instrumentation Improvement**:
   - The current scheme may suffer from TOCTTOU-like issues where the return address can be altered after a successful check but before the `ret` instruction is executed. We will replace `ret` with `pop` and `jmp` instructions to avoid double-fetching the return address, thereby reducing the risk of such attacks.

5. **Tail Calls and Position Independent Code (PIC)**:
   - Currently, ρFEM does not support tail calls and PIC. Future work will involve tracking function calls during runtime and enforcing that tail calls can return to the next address of functions that have not yet returned. For PIC, we will avoid using absolute addresses and compile any PIC code that may be loaded in protected programs.

6. **Labeling of Legitimate Return Sites**:
   - ρFEM inserts labels with IDs to enforce control flow integrity. We will continue to refine this process to ensure robust protection against control flow hijacking.