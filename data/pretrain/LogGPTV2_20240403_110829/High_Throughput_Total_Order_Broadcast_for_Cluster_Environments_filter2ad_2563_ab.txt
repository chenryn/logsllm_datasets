ously received by the same process.
In this paper, we propose to analyze protocols using a
slightly modi(cid:2)ed version of the round-based model. More
speci(cid:2)cally, we de(cid:2)ne rounds as follows: in each round r,
every process pi is supposed to: (1) compute the message
for round r, m(i; r), (2) unicast (or best effort broadcasts)
m(i; r) and (3) receive a single message sent at round r
unless the sending process has crashed.
4 Protocol
Our FSR protocol guarantees uniform total order mes-
sage delivery despite the failure of t processes with t < n,
where n is the total number of processes in the system. The
performance of FSR is optimized for failure free periods.
More speci(cid:2)cally, the performance of FSR was designed
for high throughput in various kinds of high-load traf(cid:2)c
scenarios. These scenarios include a single process TO-
broadcasting, several processes TO-broadcasting a steady
stream of messages at the same time, several processes TO-
broadcasting bursts of messages simultaneously and all pro-
cesses TO-broadcasting a steady stream of messages. Not
only does FSR provide the same throughput in all these
cases, it also provides the same reasonable latency to all
processes. Interestingly, fairness is inherently part of the
protocol such that if several processes want to TO-broadcast
messages at the same time, then they will TO-broadcast the
same number of messages during a given time-frame. FSR
m4
m1
p0
p1
m2
pt-1
pt
m3
leader process
backup process
standard process
pi
pi-1
Figure 4. FSR protocol illustration.
does not enforce a trade off between performance and fair-
ness.
4.1 Overview
In short, the idea underlying FSR is to combine a (cid:2)xed
sequencer for ordering, with a ring topology for dissemi-
nation. The main advantage of the ring topology is that it
is simple to implement and at the same time provides high
throughput. However, the ring is not only used for mes-
sage dissemination but also for sequencing. Contrary to tra-
ditional (cid:2)xed sequencer protocols, processes do not send
messages directly to the sequencer but only to their direct
successors.
All messages circulate clockwise in the same direction.
Even though there is only a single (cid:2)xed sequencer, this pro-
cess is not a bottleneck since it only needs to append a small
sequence number to the message and then forward it: the se-
quencer receives and sends the same number of messages as
all other processes. The sequencer is followed in the ring by
t backup processes which have the role of keeping a copy of
all messages and sequence numbers that have not yet been
delivered by all processes.
The FSR protocol is illustrated in Figure 4. Two cases
are interesting to highlight:
1. The case of a standard process broadcasting a mes-
sage illustrated in Figure 4. When a process pi TO-
broadcasts a message m, pi forwards m to its suc-
cessor pi+1 (message m1), which in turn forwards m
to its successor and so on until the message reaches
the leader p0. As in any sequencer based protocol,
the leader assigns monotonically increasing sequence
numbers to messages, therefore imposing a total order
on their delivery. The message and sequence number
pair (m2) is then forwarded by the leader until it has
reached t backup processes (process pt). The leader
and backup processes do not yet TO-deliver the mes-
sage (except for the last backup pt). From process pt
the message with sequence number m3 is forwarded
until process pi(cid:0)1. Processes pt to pi(cid:0)1 TO-deliver m
upon receiving m3. Process pi(cid:0)1 then sends an ac-
knowledgment m4 which is forwarded until process
pt(cid:0)1. All processes can TO-deliver m upon receiving
m4.
2. When a backup process pb (0 < b (cid:20) t) TO-broadcasts
a message m, it is forwarded until the leader p0 (this
(cid:2)rst message is obviously omitted if the leader initiates
the TO-broadcast). The message and sequence pair is
forwarded until process pb(cid:0)1. From there on an ack
is circulated until process pt. Contrary to the previous
case, none of the backup processes can yet TO-deliver
m. Only when processes receive the ack sent from pt
can they TO-deliver m.
There are several tricky issues that need to be handled
in order for the protocol to be ef(cid:2)cient and fair. Although
in the protocol described above a message goes around the
ring more than once, in order to guarantee high throughput,
the actual message to be TO-broadcast only goes around
once. The rest of the generated messages only contain an
identi(cid:2)er. Since these messages are small they can be piggy-
backed on other messages when the load is high. However
when the load is low these messages are not piggy-backed
in order to keep a low latency. Also, because of the ring
dissemination topology, uniform message size is necessary
in order to avoid that large messages stall the smaller mes-
sages. This can be achieved by segmenting large messages
into several smaller ones.
Ensuring fairness means that if more than one process
TO-broadcasts messages then each process should be able
to broadcast the same number of messages during the same
amount of time. By carefully deciding when a process can
start a new TO-broadcast, it is possible to provide this fair-
ness.
4.2 Protocol Details
Our FSR protocol is built on top of a group communi-
cation system which provides virtually synchronous com-
munications (VSC) [6]. According to the virtual synchrony
programming model, processes are organized into groups.
Processes can join and leave the group using the appropriate
primitives. Faulty processes are excluded from the group af-
ter crashing. Upon a membership change, processes agree
on a new view by using a view change protocol.
4.2.1 Group Membership Changes
When a process joins or leaves the group, a view change
event is generated by the VSC layer and the current view
vr is replaced the new view vr+1. This can happen when a
process crashes or when a process actively wants to leave or
join the group. As soon as a new view is installed it becomes
the basis for the new ring topology. There are several cases
to consider when a view change event occurs. When vr+1
is installed, the processes execute the following procedures
depending on their role in vr+1:
(cid:15) All processes TO-broadcast any message in view vr+1
that they have TO-broadcast in the view vr but not yet
TO-delivered in vr.
(cid:15) The new leader (in vr+1) must resend the following
messages: (1) all message and sequence number pairs
that have not yet been TO-delivered, (2) an ack of the
latest TO-delivered message.
4.2.2 Optimizations
The acknowledgment messages sent within FSR are very
small messages that just contain an identi(cid:2)er of the mes-
sage that they acknowledge. Consequently, these messages
can be piggy-backed on normal messages sent by other TO-
broadcasts. When all acks are piggy-backed, each TO-
broadcast effectively only sends each message around the
ring once, thus enabling FSR to achieve high throughput.
4.2.3 Fairness
Fairness captures the very fact that each process has an
equal opportunity of having its messages eventually TO-
delivered by all processes. Intuitively, the notion of fair-
ness means that no single process has priority over other
processes when broadcasting messages. For instance, when
two processes TO-broadcast large numbers of messages,
then each process should have approximately the same
number of messages TO-delivered by all processes.
Fixed sequencer protocols surveyed in Section 2 are in-
herently fair: each process that TO-broadcasts a message
sends it directly to the sequencer which will handle incom-
ing messages on a (cid:2)rst come, (cid:2)rst served basis. If a lot of
messages arrive at the sequencer at the same time then it
will serve them in a round-robin fashion. In our FSR proto-
col, messages to be TO-broadcast are not sent directly to the
sequencer, but rather forwarded to the successor. If all pro-
cesses want to TO-broadcast messages, then at each round
a process can either start a new TO-broadcast by sending a
message to its successor, or forward messages from its pre-
decessor.
Ensuring fairness in FSR is achieved by having a speci(cid:2)c
mechanism to decide whether a process can initiate a new
broadcast or whether it must (cid:2)rst forward messages stored
in its incoming buffer. Intuitively, each process maintains
a list forward of the processes for which it has forwarded
Incoming buffer
Forward list
4.3.2 Throughput
3
mp2
2
mp4
5
mp3
6
mp3
p1
p4
p5
Figure 5. Incoming buffer and forward list of
a process initiating a TO-broadcast.
messages since its last broadcast. When a process initi-
ates a TO-broadcast, it (cid:2)rst forwards messages that are in
its incoming buffer and that have been sent by processes
not in the forward list. Figure 5 illustrates the incoming
buffer and forward list of a process pi wishing to initiate a
TO-broadcast. Before sending its own message m, process
pi forwards messages m3
p3. This simple mecha-
nism ensures that no process will prevent others from TO-
broadcasting their own messages.
p2 and m5
4.3 Analytical Performance
4.3.1 Latency
The latency of TO-broadcasting is de(cid:2)ned as the largest
number of rounds that are necessary from the initial TO-
broadcast of a single message m until the round where
the last process TO-delivers m. The latency is measured
in a newly initialized system when no other messages are
TO-broadcast: it is obvious that latency increases when a
lot of messages are TO-broadcast simultaneously. The la-
tency of FSR can be expressed as follows for all processes:
L(i) = 2n + t (cid:0) i (cid:0) 1, where i is the position of the TO-
broadcasting process in the ring with respect to the leader at
position 0. We can observe the following:
(cid:15) The latency is linear with respect to the number of pro-
cesses n, implying that FSR scales well.
(cid:15) The latency is also linear with respect to the number of
tolerated failures t.
(cid:15) The position of the TO-broadcasting process in the
ring has an in(cid:3)uence on the latency. In order to evenly
distribute the latency for all processes, the role of the
leader can be periodically moved to the next process in
the ring. This can be done by periodically executing a
leave followed by a join at the current leader process.
It is also possible to transfer the role of the leader with-
out having the leader leave and join the group, but for
space reasons that discussion is left out of this paper.
The throughput of FSR is at least equal to one. This means
that on average at least one TO-broadcast is completed dur-
ing each round (a complete TO-broadcast of message m
meaning that all processes TO-delivered m). In more de-
tail:
(cid:15) The throughput is independent from the number of
processes that TO-broadcast at the same time. If only
one process continuously TO-broadcasts it is obvious
that it can TO-broadcast a new message every round
since every broadcast message goes round the ring
only once. After an initial latency of 2n + t (cid:0) i (cid:0) 1
rounds the (cid:2)rst message has been TO-delivered by all
processes and in the consecutive rounds one message
is TO-delivered every round. With multiple senders
the same argument holds. Because of the fairness
described in Section 4.2, each round a message TO-
broadcast by a different process is TO-delivered.
(cid:15) The throughput of FSR is independent of the number
n of processes in the system.
(cid:15) The throughput of FSR is independent of the number t
of processes that can crash.
5 Performance
This section describes the various experiments that we
conducted to evaluate the performance of FSR. We im-
plemented FSR using DREAM [28], a Java-based com-
ponent library dedicated to the construction of communi-
cation middleware. Dream enables the development of
various forms of message-oriented middleware (e.g. pub-
lish/subscribe, event/reaction and group communication
protocols) by component assembly. The library contains a
wide array of components, including message queues, chan-
nels (socket wrappers), routers, etc.
5.1 Benchmark Description
We ran benchmarks on a cluster of machines with dual
900MHz Itanium-2 processors, 3GB of RAM and a Fast
Ethernet adapter, running Linux kernel 2.4.21. The raw la-
tency and bandwidth over IP between two machines were
measured with Netperf [33] and displayed in Table 1.
The benchmarks test k-to-n TO-broadcasts, k ranging
from 1 to n. All processes know a priori the number of mes-
sages they expect from other processes (each sender sends
the same number of messages). A barrier is used to syn-
chronize the experiment start-up. Each process takes a local
timestamp and starts sending its messages. When the last
expected message from a sender is received, an acknowl-
edgment is sent back to the sender. This allows stopping the
Protocol Bandwidth
TCP
UDP
94 Mb/s
93 Mb/s
Table 1. Raw network performance measured
using Netperf.
250
200
150
100
50
)
s
m
(
y
c
n
e
t
a
L
0
1
2
3
4
5
6
7
8
9
10
Number of processes
Figure 6. Latency as a function of the number
of processes.
timer at each sender. Then, each sender calculates the time
between the (cid:2)rst broadcast message sent and the acknowl-
edgment message received by the last process receiving all
the senders’ messages. For each sender, we calculate the