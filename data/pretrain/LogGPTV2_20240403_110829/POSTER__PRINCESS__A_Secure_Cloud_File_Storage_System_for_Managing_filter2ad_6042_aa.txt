# Title: PRINCESS: A Secure Cloud File Storage System for Managing Data with Hierarchical Levels of Sensitivity

## Authors:
- Lihua Wang
- Takuya Hayashi
- Sachiko Kanamori
- Atsushi Waseda
- Ryo Nojima
- Shiho Moriai

## Abstract
Cloud-based storage systems are rapidly evolving and have become a popular trend for storing large volumes of data and documents. Designing a storage service for such systems involves addressing several complex issues, including latency in data transfer, parallel input and output, deduplication, distributed nature, and high scalability. Key-value stores play a crucial role in providing solutions to these challenges. This paper presents a cloud-based storage system designed to handle large files, leveraging key-value stores. The system features simplified, fixed-size file metadata, which enhances scalability and supports resumable file I/O, as well as efficient file deduplication and compression.

**Keywords:** Big File, Cloud Storage System, Deduplication, Key-Value Store, Encryption, Compression

## 1. Introduction
Cloud computing is a model where computing resources and capabilities are provided as services over the internet. Cloud storage is an online system where data is stored and accessible to multiple users. To be effective, cloud storage must be agile, scalable, versatile, multi-tenant, and secure. Data stored in a cloud storage library can be deduplicated, which eliminates redundant data segments and reduces backup size. This is particularly beneficial in cloud storage, where data is transferred over WAN, reducing storage requirements and network traffic, leading to faster and more efficient data operations.

Cloud encryption transforms user data into ciphertext, ensuring security. While similar to in-house encryption, cloud encryption requires users to understand the provider's policies and methods for encryption and key management. The cloud encryption capabilities must match the sensitivity level of the hosted data.

Distributed storage systems are increasingly popular for managing large data volumes. Cloud services are used for regular data storage, backups, and sharing. Users can upload, share, and download files. To provide advanced storage services, the system must address various requirements, such as eliminating bottlenecks, handling large files, supporting parallel I/O, and managing deduplication.

Traditional systems face challenges in scaling, distributing data across network points, and load balancing. Solutions include distributed file systems and cloud storage, where large files are split into smaller chunks, stored on distributed storage, and managed by a metadata service system.

## 2. Related Work
Deduplication is a process used to eliminate duplicate data, commonly employed in distributed storage to reduce storage and bandwidth usage. Each file should have only one copy in the cloud, regardless of the number of owners, enhancing storage efficiency and reliability. However, this raises security concerns for confidential data.

Nguyen et al. [1][2] implemented a distributed cloud storage service that maintains data deduplication using cryptographic hash functions, ensuring data integrity and privacy through encryption. Li et al. [3] introduced authorized deduplication, which avoids duplicate content in distributed storage with minimal overhead compared to integrated key encryption. They also proposed proofs of ownership in remote storage systems, demonstrating low overhead compared to client-side deduplication [4].

Drago et al. [8] analyzed Dropbox, noting its efficient storage and customer performance but highlighting issues like data duplication and metadata complexity. Other cloud storage services, such as Google Drive, benefit from Google's infrastructure but face performance limitations due to protocols and features.

Recent cloud storage frameworks have attempted to improve distributed storage, but they still face challenges like fixed-size metadata, data duplication, and storage optimization. The proposed system aims to address these issues by implementing a chunk-distributed storage design and file compression.

## 3. Proposed Work

### A. Architecture of the Proposed System
The proposed system consists of four levels: Logical, Key-Value Store, Application, and File-Chunk Store. The Application level includes desktop, mobile, and web interfaces, allowing users to upload and download files. It uses APIs contained in the Logical level, which provides the business logic, including file transfer and retrieval. The Logical level interacts with the File-Chunk Store level, which manages file and chunk storage. This level includes FileInformationSrvc and ChunkStorageSrvc, responsible for storing file-related data and chunks, respectively.

**Figure 1.** Proposed System Architecture

### B. Modules of the Proposed System

#### 1. File-Chunk Storage
Chunks are the basic units in the distributed storage system. When a user uploads a file, it is split into fixed-size chunks (except the last chunk, which may be smaller). An ID generator assigns unique IDs to each chunk, and the FileInfo structure stores file metadata, such as file ID, size, and chunk details. Chunks are stored in a key-value store, with the chunk ID as the key and the chunk data as the value.

#### 2. De-Duplication and Uploading
The system uses SHA hash algorithms to identify duplicate files before uploading. As a user uploads a file, the SHA hash of the file data is calculated and checked against existing hashes in the database. If a match is found, a reference to the existing file is created; otherwise, the new file is uploaded.

**Figure 2.** Flow of Deduplication and Upload Process

#### 3. Compression of Files
File compression is essential for saving storage and improving transfer efficiency. The system uses GZIP compression, which converts files into smaller versions without losing data. GZIP provides a compression ratio between 2.5 and 3, making it fast and efficient for both compression and decompression. Since it is a lossless compression method, no data is lost during the process.

**Figure 3.** File Compression and Decompression Process

## Conclusion
The proposed PRINCESS system addresses the key challenges in cloud storage, including deduplication, compression, and efficient file management. By leveraging key-value stores and optimized algorithms, the system ensures high performance, scalability, and security, making it suitable for managing large, sensitive data sets.