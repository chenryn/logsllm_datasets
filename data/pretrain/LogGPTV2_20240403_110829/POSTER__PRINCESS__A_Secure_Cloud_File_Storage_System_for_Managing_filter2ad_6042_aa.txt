title:POSTER: PRINCESS: A Secure Cloud File Storage System for Managing
Data with Hierarchical Levels of Sensitivity
author:Lihua Wang and
Takuya Hayashi and
Sachiko Kanamori and
Atsushi Waseda and
Ryo Nojima and
Shiho Moriai
Cloud Based Storage System using Secure 
Deduplication and File Compression 
Gajare Sukruti B,  
Department Of Computer Engineering,  
M.E.S. College Of Engineering, Pune 
University Of Pune, Maharashtra, India 
PI:EMAIL  
issues 
like  meta-data  estimation, 
Abstract—  Now-a-days,  cloud-based  storage  frameworks  are 
rapidly growing ahead and has become emerging trend in putting 
away  of  incredible  estimated  information  and  data  files  and 
documents. Numerous intricate conditions get up while designing  
storage service for cloud-based storage systems that contemplates 
the  major 
inactivity  for 
transferring  and  downloading,  parallel  input  and  output,  de-
duplication,  distributed  nature  and  high  versatility.  Key  Value 
stores  has  a  noteworthy  and  imperative  part  and  have  given 
numerous more possibilities while finding solution to and way out 
of those Issues. This paper exhibits about storage service for data 
files  -  Cloud  based  Storage  System  with  its  parts  of  a  more 
noteworthy  unit  and  structure  outline  that  attempt  to  hold  the 
majority  of  issues  in  a  Cloud  storage  System  for  putting  away 
heavy sized documents, which depends on key value store. Here, 
putting  forth  less-complicated,  fixed  size  file  meta-data,  which 
lets firmly and in addition scales out the storage, resumeable file 
Input/Output,  and  basic  document  and  file  de-duplication  and 
compression.  
Keywords-  Big  File,  Cloud  Storage  System,  De-duplication,  Key 
value, Encryption, Compression 
I. 
 INTRODUCTION  
Cloud computing is processing  where by assets and it related 
abilities  are  given  as  services  to  the  external  client  utilizing 
Internet  system.  Cloud  storage  -  An  online  system  storage 
where  information  is  put  away  and  open  to  numerous 
customers. So as to be successful, the cloud storage should be 
deft, adaptable, versatile, multi-tenure, and secure. Information 
went  down  to  a  cloud  storage  library  can  be  deduplicated. 
Deduplication  eliminates  repetitive  information  sections  from 
the  reinforcement  and  lessens  the  span  of  the  reinforcement 
information. This is especially helpful in Cloud Storage where 
information  is  exchanged  to  the  capacity  focus  over  WAN. 
Deduplication with Cloud Storage diminishes the storage room 
prerequisites,  as  well  as  decreases  the  information  that  is 
exchanged  over  the  system  bringing  about  quicker  and 
proficient information security operations.   Cloud  encryption 
is  the  change  of  a  cloud  service  client's  information  into 
ciphertext. Cloud encryption is practically indistinguishable to 
in-house  encryption  with  one  critical  distinction  -  the  cloud 
client must set aside opportunity to find out about the supplier's 
arrangements and methodology for encryption and encryption 
key  service.  The  cloud  encryption  abilities  of  the  specialist 
978-1-5386-4008-1/17/$31.00 ©2017 IEEE(cid:3)
Khan Rubeena A, 
Department Of Computer Engineering, 
M.E.S. College Of Engineering, Pune 
University Of Pune, Maharashtra, India 
PI:EMAIL 
inconveniences 
organization need to coordinate the level of affect-ability of the 
information being facilitated.  
       These  days,  appropriated storage  structure are  getting the 
opportunity  to  be  particularly  fulfilling  to  all  and  extensively 
used  for  securing  gigabytes  and  terabytes  of  data.  Cloud 
Services is used for the regular use, for moving down data and 
having  comparable  files.  The  end  customer  of  the  cloud 
structure can exchange the archives on to the Cloud storage and 
grant it to others and moreover can download the files. Thus of 
that, to make more advanced storage service to the cloud end 
customers,  the  system  needs  to  manage  different  necessities 
and 
like:  service  with  no  bottlenecks; 
extraordinary at conveying a capable securing, putting in high 
demand  and  managing  the  impressive  measured  data  files; 
proceed with skilled and parallel download and exchange of the 
heavy  sized  files;  the  significant  issue  about  duplication  is  to 
manage the limit measure of size of the structure and support of 
de-duplication.  Traditional  systems  caused  several  queries  in 
building the services for managing a big range of big-files like: 
scaling  the  system;  distribution  of  information  for  computers 
on  a  greatly  sized  range  of  network  points;  repetition  of  data 
for  load  balancing  and  fault  tolerance.  The  solution  for  these 
issues  is  distribution  file  systems  and  Cloud  storage,  for 
massive files (uploaded by the user) in to range chunks of same 
size, storing them on created distribution storage and managing 
them by a meta-data service system. 
II.  RELATED WORK 
Deduplication of is a process utilized for eliminating the copy 
matching  of  data  and  is  generally  used  as  a  component  of 
distributed storage. It is used to decrease the storage room and 
transfer  bandwidth[6].  There  must  be  just  a  single  duplicate 
for  each  file  put  away  in  cloud  regardless  of  the  possibility 
that such a document is possessed by  various clients. Hence, 
the  deduplication  framework  enhances  use  of  capacity  that 
outcomes  in  increase  the  dependability  of  the  framework. 
Additionally emerges the test of security for confidential data. 
Nguyen,  et  al.  [1][2]  implemented  the  possible  distributed 
cloud  storage  service  that  keeps  up  data  deduplication  by 
reckoning the cryptographic hash estimation of the Data of the 
file to determine copy duplicate  while the data prudence and 
privacy  achieved  employing  encryption  system.  Authorized 
deduplication is explained by Li, et al. [3] which hold over a 
important distance from the copy content in distributed storage 
the 
in  operation 
typical  operation  by  utilizing 
framework  and  acquires  minimal  overhead  when  comparing 
with 
integrated  key 
encryption.  It  likewise  gives  the  security  to  the  given 
information.  Li,  et  al.  [4]  proposes  Proofs  of  Ownership  in 
Remote  Storage  Systems  which  contain  Performance 
estimations  demonstrate  that  the  plan  acquires  just  a  little 
overhead  comparing  with  customer  side  deduplication.  They 
propose new distributed deduplication frameworks with higher 
steady quality in which the data segments are distributed over 
various  cloud  servers  [4].  The  security  requisite  of  data 
classification and label consistency are likewise established by 
presenting a settled secret sharing system in distributed storage 
frameworks,  rather  than  utilizing  integrated  encryption  as  in 
past  deduplication  frameworks.  Security  investigation  shows 
that  our  deduplication  frameworks  are  secure  as  far  as  the 
definitions indicated in the security exhibit.  
Drago,  et  al.  [8]  explains  Dropbox,  Dropbox  executes  the 
majority  of  the  checked  storages,  and  its  complex  customer 
remarkably helps execution, some convention changes appear 
to  be  conceivable  to  minify  posed  overhead.  In  any  case, 
duplication of information, meta-data many-sided quality end 
up  plainly  significant  issues  offering  ascend  to  wastage  of 
storage  room  and  expanding  system  overhead.  Cloud  Drive, 
wastage  of  data  transferral  capacity  has  a  greatness  which  is 
higher than different services, absence of customer execution 
brings  about  bottlenecks.  SkyDrive  likewise  demonstrates 
some  constraining  conditions 
like  system 
inactivity.  In  OneDrive  we  can  see  restricting  state  of  de-
duplication.  Google  Drive  takes  after  an  alternate  approach 
bringing about a mixed image: it appreciates the advantages of 
utilizing  Google's  lean  framework  and  private  backbone, 
which  diminish  organize  dormancy  and  accelerate 
the 
framework.  Be  that  as  it  may  slow  down,  conventions  and 
customer  highlights  constrain  execution,  particularly  when 
various documents are considered[11].  
           The recent Cloud storage frameworks have attempted to 
execute  circulated  distributed  storage  and  amend 
their 
execution.  Be  that  as  it  may,  there  are  sure  restrictions  like 
meta-data of fixed size, data duplication, storage streamlining 
which  thus  prompts  different  issues  talked  about  underneath. 
In  the  proposed  System  execution,  focus  is  to  anticipate  the 
issues  confronted  by 
storage 
frameworks.  Existing  frameworks  brought  on  many  inquiries 
in  building  the  services  for  dealing  with  an  exceptionally 
extraordinary  number  of  huge  documents  like:  scaling  the 
framework; conveyance of highly estimated number of system 
focuses; duplicating of data for load-balancing and adaptation 
to internal failure and recovery. The response for these issues 
is appropriation file frameworks and Cloud storage, by making 
into  huge  data  files(uploaded  by  the  client)  into  number 
chunks of same size, putting away them on made conveyance 
storage  and  overseeing  them  utilizing  a  meta-data  benefit 
framework.  Putting  away  with  little  measure  of  chunks  of 
same  size and  meta-data  identified  with  it and outlining of a 
lightweight  meta-data  identified  with  it  are  much  issues  that 
distributed  storage  providers  need  to  confront.  Focus  is  to 
conceivable enhancements to the be done: a) the utilization of 
the  current  distributed 
a chuck distributed storage design. b) Compression of files to 
be put away on to the cloud. 
III.  PROPOSED WORK 
transferring  of 
for  downloading  and 
A.  Architecture of the Proposed System 
Proposed System incorporates four levels: Logical level, Key-
value  store  level,  Application  level  and  File-Chunk  Store 
level. Application level have among its parts of programming 
applications  desktop  PC,  cell  phones  and  web  interface  also, 
that lets the end client to transfer, download files according to 
their need. This level uses application programming interfaces 
which  are  contained  in  Logical  level,  board  and  uses  a  few 
algorithms 
the 
information.  Logical  level  formed  of  numerous  services, 
chunk  Identity(ID)  Generation  services  and  all  sensible 
application programming interfaces for Cloud Storage System. 
This Logical level provides the business logic of the proposed 
system,  legitimate  part  in  offered  System.  The  loaded  with 
constrain parts of this level are transferring and downloading 
of  documents.  This  level  stores  and  gets  back  information 
from  File-Chunk  Store  level.  File-Chunk  Store  level  has  an 
essential  impact  for  the  proposed  framework,  which  is  in 
charge of putting away and bringing chunks of document. This 
level  has  in  it  and  oversees  learning  of  all  document  and 
chunks in the framework including client points of interest and 
File  meta-data.  In  this,  meta-data  pictures  a  File  related 
information  and  how  it  is  systemized  in  chunk  storage.  This 
level likewise is mainly of many made dissemination back-end 
services.  The  vital  services  of  File-Chunk  Store  level  are 
FileInformationSrvc and ChunkStorageSrvc. 
Fig. 1. Proposed System Architecture 
File related data is put away in FileInformationSrvc. It is a 
key  value  store  mapping  information  from  fileID(Identity  of 
the  File)  to  FileInform  (All  data  identified  with  document) 
structure. Data chunks which are made by parting the file that 
client  uploades  are  put  away  in  ChunkStorageSrvc.  It  is 
without inconvenience to store and after that make conveyance 
Chunks in key value storage. Chunks of the one document can 
be put away  with no overhead in a key value storage. Fig. 1 
2017 Third International Conference on Computing, Communication, Control And Automation (ICCUBEA) 
gives  the  pictorial  representation  on  the  proposed  system 
architecture. 
B. Modules of the Proposed System 
This  section  explains  the  modules  of  the  estimated  system 
with its functioning and algorithmic use. 
1) File-Chunk Storage 
Chunk  is  the  basic  part  in  the  molded  distributed  storage 
system.  Chunks  are  obtained  from  a  File.  Exactly  when  the 
end customer transacts a document, it will be splitted into bits 
into  different  chunks.  All  Chunks  which  are  made  from  a 
document, have a same size (the last chunk of a File may have 
a proportional or more diminutive size)except the last chunk. 
The Identity(ID) generator will form id for the essential chunk 
with auto increasing mechanism afterwards that Next Chunks 
that takes after is to be doled out with provocative Identity(ID) 
and  a  while  later  well  ordered  addition  till  the  last  chunks. 
FileInfo  purpose  behind  existing  is  made  showed  up  and 
stayed  cautious  with  information  of  the  File,  for  instance, 
fileid, document size, first chunk, Total number of Chunks and 
will be secured to the data base and the chunks will be secured 
in key value storage as one of a kind id of chunk and value is 
data  of  particular  chunk.  chunk  Storage  is  a  most  important 
component  among  the  most  basic  module  of  described 
propagate storage system. 
    2)  De-Duplication and Uploading 
De-duplication of documents can be portrayed in the Proposed 
Storage System. There are many sorts and methodologies for 
information  de-duplication  [3]  which  can  work  both  on 
customer  side  or  server  side.  We  are  using  a  fundamental 
procedure with SHA hash algorithm to recognize copy Files in 
the system before transferring the file. As the User exchanges 
the  archive,  SHA  estimation  of  the  information  of  the 
document  will  be  made  and  will  be  checked  with  the  SHA 
Values  present  in  the  database,  if  present  a  reference  to  the 
File will be created and if SHA Value is not present the new 
file  will  uploaded  (transferred) on  to  the  cloud.  Fig.  2,  gives 
the flow of the deduplication and upload process. 
in 
    3)  Compression Of Files 
File  Compression  has  a  primal  part 
the  proposed 
framework. File Compression is a framework by which a file 
is  changed  to  another  File  (compacted),  so  that  the  essential 
document  is  completely  recuperated,  with  no  loss  of  data  in 
the file, from the packed File. This methodology is utilized to 
spare  the  capacity  use.  Likewise  the  compacted  Files  are 
fundamentally more reasonably exchanged over the web since 
the trading and downloading of files winds up being liberally 
speedier.  
For Compression we are utilizing GZIP compression 
Algorithm. GZIP Compression is the converting of a Files into 
a tinier file by utilizing a table-based query estimation. GZIP 
figuring consolidates the query table of codes as a part of the 
Compression  document.  The  disentangling  program  that 
uncompresses the Files can keep up the table itself by utilizing 
the estimation as it frameworks the encoded input. GZIP gives 
sufficient  compression  extent  in  the  region  of  2.5  and  3  for 
substance  and  it  is  fast,  it  rushes  to  come  lessen  data  and  it 
rushes to decompress it. Since this is a lossless Compression 
procedure, none of the information in the documents are lost 
before  or  after  weight.  The  decompression  Algorithm 
dependably  takes  after  the  compression  Algorithm.  GZIP 
algorithm is capable in light of the way that it doesn't have to 
pass the string table to the decompression code. 
Fig. 2. Uploading File and Deduplication Flow 