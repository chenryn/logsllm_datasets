        # even if start_symbol is different
        if START_SYMBOL in grammar:
            used_nonterminals.add(START_SYMBOL)
        for unused_nonterminal in defined_nonterminals - used_nonterminals:
            print(repr(unused_nonterminal) + ": defined, but not used",
                  file=sys.stderr)
        for undefined_nonterminal in used_nonterminals - defined_nonterminals:
            print(repr(undefined_nonterminal) + ": used, but not defined",
                  file=sys.stderr)
        # Symbols must be reachable either from  or given start symbol
        unreachable = unreachable_nonterminals(grammar, start_symbol)
        msg_start_symbol = start_symbol
        if START_SYMBOL in grammar:
            unreachable = unreachable - \
                reachable_nonterminals(grammar, START_SYMBOL)
            if start_symbol != START_SYMBOL:
                msg_start_symbol += " or " + START_SYMBOL
        for unreachable_nonterminal in unreachable:
            print(repr(unreachable_nonterminal) + ": unreachable from " + msg_start_symbol,
                  file=sys.stderr)
        used_but_not_supported_opts = set()
        if len(supported_opts) > 0:
            used_but_not_supported_opts = opts_used(
                grammar).difference(supported_opts)
            for opt in used_but_not_supported_opts:
                print(
                    "warning: option " +
                    repr(opt) +
                    " is not supported",
                    file=sys.stderr)
        return used_nonterminals == defined_nonterminals and len(unreachable) == 0
以上列举的是常用的Tools，在Fuzz的编写过程中，要根据实际问题针对性的编写各式各样的工具。
## 高效Grammars Fuzz
前面提供的simple_grammar_fuzzer其实存在大量的问题，比如性能低下，对于符号的解析次数受限，容易引起报错等，因此需要更加高明的算法。这里选择的是派生树，因为树形结构易于追踪而且易于添加和删除其中分支。关于Fuzz的编写其实就是不断的对派生树进行分析和对子节点的不断扩展。
### 派生树算法
从上述的简单算法可以看出，整个的Grammar
Fuzz的核心其实就是通过大量的符号扩展形成对应的数据结构，那么用来存储或者拓展符号的数据结构其实尤为重要。派生树的树状结构其实完美的符合了我们的要求，树形结构自上而下的扩展正好和符号的扩展相对应。而且`派生树使得我们可以掌控整个扩展过程的状态`，比如那些节点已经被扩展，或者某个节点是否需要扩展等，同时，在扩展过程中增加新节点的速度远超把一个符号替换为一个值的过程，因此使用这种数据结构也带来了一定的性能增益。
让我们以下面的Grammar为例子：
    # URL Grammar
    URL_GRAMMAR: Grammar = {
        "":
            [""],
        "":
            ["://"],
        "":
            ["http", "https", "ftp", "ftps"],
        "":
            ["", ":", "@", "@:"],
        "":  # 大部分情况下其实可以指定一个URL
            ["cispa.saarland", "www.google.com", "fuzzingbook.com"],
        "":
            ["80", "8080", ""],
        "":
            ["", ""],
        "":
            ["0", "1", "2", "3", "4", "5", "6", "7", "8", "9"],
        "":  # Just one
            ["user:password"],
        "":  # Just a few
            ["", "/", "/"],
        "":  # Just a few
            ["abc", "def", "x"],
        "":
            ["", "?"],
        "":
            ["", "&"],
        "":  # Just a few
            ["=", "="],
    }
以派生树算法来看，首先以``为初始节点，然后在Grammar中发现其存在对应的表达，所以就会选择``作为它的子节点，循环往复知道一个节点不再出现对应的子节点，然后整个的树形结构完成解析，输出对应的结构化数据。
对应的数据表示如下：
    (SYMBOL_NAME, CHILDREN)
    DerivationTree = Tuple[str, Optional[List[Any]]]
    derivation_tree: DerivationTree = ("",
                       [("",
                         [("", None),
                          (" + ", []),
                             ("", None)]
                         )])
`SYMBOL_NAME`代表的就是符号，CHILDREN代表子节点，表示为具体的数据结构就是：`DerivationTree = Tuple[str,
Optional[List[Any]]]`。其中CHILDREN主要有两种表示：
  1. None代表当前节点可以继续向下扩展，其含义就是现在节点存在可扩展的符号。
  2. []代表的就是没有子节点了
整个算法都围绕上面的基本原理展开
    def g_rammar_fuzzer():
        f = GrammarFuzzer(URL_GRAMMAR)
        f.fuzz()
### ProbabilisticGrammarFuzzer
有时候完全随机的进行表达式展开其实会白白浪费大量的时间和资源，因此可以对表达式附加概率值，这一块涉及到大量的概率学问题，有部分数据来源于世界的统计规律，比如下面给出的`leaddigit`符号对应的概率，这些就不在深入分析。
    PROBABILISTIC_EXPR_GRAMMAR: Grammar = {
        "":
            [""],
        "":
            [(" + ", opts(prob=0.1)),
             (" - ", opts(prob=0.2)),
             ""],
        "":
            [(" * ", opts(prob=0.1)),
             (" / ", opts(prob=0.1)),
             ""
             ],
        "":
            ["+", "-", "()",
                "", "."],
        "":
            ["", ""],
        # Benford's law: frequency distribution of leading digits
        "":
            [("1", opts(prob=0.301)),
             ("2", opts(prob=0.176)),
             ("3", opts(prob=0.125)),
             ("4", opts(prob=0.097)),
             ("5", opts(prob=0.079)),
             ("6", opts(prob=0.067)),
             ("7", opts(prob=0.058)),
             ("8", opts(prob=0.051)),
             ("9", opts(prob=0.046)),
             ],
        # Remaining digits are equally distributed
        "":
            ["", ""],
        "":
            ["0", "1", "2", "3", "4", "5", "6", "7", "8", "9"],
    }
跟之前的Grammar有很大不同的地方在于，现在的Grammar可以通过增加注释的方式为列表中的值添加随机概率，使得作者可以通过逆向获取其它渠道得到的信息可以在Fuzz中获得利用。那现在问题就显而易见了，如何确定概率？
当Fuzz的作者没办法直接给出一个符号对应的所有项具体的概率的时候，可以遵循的最直接的规则就是下面三个公式：
大致含义也很好理解，就是a代表的是已知概率的项，而u代表的未知概率的项目，已知概率自然可以通过`opts`的方法给对应项附加概率，未知概率的项则按照概率平分的原则来赋予概率。之后自然是要在Fuzz里面引入概率，使得在生成种子的时候可以对符号解析的选择赋予权重，进而提高Fuzz效率。
就Fuzz的具体实现而言，其实相比于上述的Grammar
Fuzz只是增加了一个对于opts注释的访问，以便在随机解析的时候可以附加概率值权重。但是这样带来的优势是很明显的，甚至可以通过控制输入Fuzz目标指定的Func等。但是还有一种情况，我第一次解析Grammar
symbol的时候希望它的概率为0.3，但是我第二次解析Grammar
symbol的时候希望其概率为0.5，为了实现这一点其实可以利用上下文，在不同的上下文中复制希望赋予其不同概率的symbol，以IP Grammar为例子：
    IP_ADDRESS_GRAMMAR: Grammar = {
        "": [""],
        "": ["..."],
        # ["0", "1", "2", ..., "255"]
        "": decrange(0, 256) # 其实代表的就是0-256
    }
为了使得每次解析``的时候都使用不同的概率，可以对其扩展，形成下面的语法：
    IP_ADDRESS_GRAMMAR: Grammar = {
        "": [""],
        "": ["..."],
        # ["0", "1", "2", ..., "255"]
        "": decrange(0, 256) # 其实代表的就是0-256
        "": decrange(0, 256) # 其实代表的就是0-256
        "": decrange(0, 256) # 其实代表的就是0-256
        "": decrange(0, 256) # 其实代表的就是0-256
    }
这样在进行解析的时候就完全可以对每次解析附加不同的概率。下面是帮助实现的函数：
    def _duplicate_context(grammar: Grammar,
                           orig_grammar: Grammar,
                           symbol: str,
                           expansion: Optional[Expansion],
                           depth: Union[float, int],
                           seen: Dict[str, str]) -> None:
        """Helper function for `duplicate_context()`"""
        for i in range(len(grammar[symbol])):
            if expansion is None or grammar[symbol][i] == expansion:
                new_expansion = ""
                for (s, c) in expansion_to_children(grammar[symbol][i]):
                    if s in seen:                 # Duplicated already
                        new_expansion += seen[s]
                    elif c == [] or depth == 0:   # Terminal symbol or end of recursion
                        new_expansion += s
                    else:                         # Nonterminal symbol - duplicate
                        # Add new symbol with copy of rule
                        new_s = new_symbol(grammar, s)
                        grammar[new_s] = copy.deepcopy(orig_grammar[s])