在这里，我们处理的是单个组件，而不是整个“消息传递总线”。因此，我们需要从组件的角度在本地计算总消息，即使我们前面有一组负载均衡器也是如此。你应该使用负载均衡器中的数据来跟踪整个“消息传递总线”的 SLA。
错误计数器也是如此。从负载均衡器收集错误计数器肯定会描述真实客户的影响；但是，同时，它将隐藏组件级别中发生的实际错误量，并阻止我们在它们从外部可见之前对它们做出反应。
所以，让我们开始数学计算。我们的初始可用性等于 100%。在前 24 小时之后，我们将收集所有指标，计算一些故障，并最终获得新的可用性值，以便第二天开始参考它。
如果昨天我们收到 100 万条消息，其中 200 条由于任何原因被标记为失败，可用性将减少到 99.98%，计算如下：
100% – (200 / 1,000,0000 × 100) = 99.98%这种计算方法在一定程度上可以正常工作，但确实存在一些问题。第一个是可用性级别仅可能下降，一旦降低将永远无法恢复。
第二，我们不考虑“随时间变化”的分布，这可能导致在根本不预期的情况下出现急剧下降。例如，假设流量由两个主机由负载均衡器前处理，并且有一个非常安静的时间段，在此期间，一整天只发送了 10 条消息。如果其中一台主机恰好无法响应任何消息，统计信息会发生什么情况？如果所有 10 条消息是连续发出的，则它们在主机之间平均分布，并导致其中一半失败。负载均衡器将它们重新发送到其他主机，最后所有 10 个主机都成功传递。从我们的计算的方法来看，这将是一个巨大的可用性下降：
100% – (5 / 15 × 100) = 66.67%
下降 33.33%，但其实没有任何客户受到影响！
最后一个明显的限制是，我们无法计划需要停机的维护，因为不清楚如何根据消息密集程度来错开高峰。这就引出了另一个难题：不安全的更新推出。任何具有大规模更改的推出都可能最终导致整个服务故障，不管有多少测试和 “金丝雀”金丝雀指的是一种发布机制，其中我们在几个阶段逐步部署新软件，开始只向一小部分用户发布，并且将发布量逐渐增加到 100%。在每个阶段，新产品必须按预期执行一段时间，才能推进到下一阶段。或登台软件发布的保护。问题不在于故障本身，而是由其问题本质造成的。问题是我们无法提前预测此类事件是否会破坏 SLA。即使我们知道需要多长时间来检测问题和执行回滚，仍不清楚这些问题导致的服务中断是否在我们同意正确服务的 99.9% 以外的那 1% 许可范围内。
“每年99.9%的情况”的解释中隐藏了解决方案。既然“年”是时间值，因此 99.9% 也可以视为时间值。根据此陈述，我们可以计算每年为故障和维护而剩下的时间。一年包括 525600 分钟。超出 SLA 级别的 0.1% 等于近 526 分钟。谷歌给这个值一个唯一的名字， 错误预算。服务在一年中正常运行的时间百分比通常称为服务的“正常运行时间”，SLA 中指定的 99.9% 是此指标的可接受级别。
从字面上讲，我们可以解释错误预算如下：如果服务没有产生一个单一的失败，在经过 365 天的平安运行之后，我们可以完全关闭它约8小时45分钟，而不会违反 SLA。
现在，我们能够将回滚时间要求与此类预算进行比较，以查看下一次推出是否会将可用性级别置于风险之中。此外，可用性级别（停机时间）现在变得可恢复。以前，我们只能减少使用时间值，但新方法允许我们随着时间的推移将其还原到原来的 100% 标记。由于“一年”是一个恒定的持续时间值，因此每分钟的运行都会将新的指标值追加到一年时间线的头部，并从其尾部丢弃相同数量的数据。如果我们每天计算可用性，每次我们将处理 1440 分钟的数据。通过从每天 1440 分钟中减去所有故障时间，我们可以获得“每日正常时间”。为了计算新的总体服务可用性，我们需要从以前的总可用性值中减去一年前的每日使用时间，并将其与今天的使用时间追加。例如，如果我们具有以下功能：
昨天的服务可用性级别： 525400 （99.9619%）
错误预算大小： 326 分钟
去年同一天的每日可用时间：1400 分钟
今天的故障时间：5分钟
今天的总体正常时间如下：
正常时间 = “昨天的正常时间”-“去年的每日正常时间”+ “今天的正常时间”正常时间 = “昨天的正常时间”-“去年的每日正常时间”+ “今天的正常时间”
525,400 – 1,400 + (1440 – 5) = 525,435 (99.9686%)
以下是当前错误预算，计算为当前总体正常时间和 SLA 级别之间的分钟差：
525,435 – (525,600 – 526) = 361
我们可以看到，去年我们只有 1400 分钟的正常运维，这意味着剩余的 40 分钟被故障占用，并且从错误预算中被挤占。一年后，这 40 分钟可以返回到预算，因为遇到这些时间时，时间将移出一个日历年期间（我们当前的可用性范围；请参阅#daily_availability_calculation_scope）。因此，到今天，预算增加了 40 分钟，又因为今天的失败减少了 5 分钟。
每日可用性计算范围
最后，我们需做的最后一件事是建立一种方法，将服务的故障计数转换为错误预算余额。最后，我们需做的最后一件事是建立一种方法，将服务的故障计数转换为错误预算余额。
为了避免之前用很少的消息数来反映可用性导致的下降的问题（如 33.33%），我们将计算日值，而不是作为单个数据集，而是逐渐按可用数据点之间的最小持续时间计算。例如，如果每分钟收集一次统计信息，我们就可以按分钟分析数据。
转换本身非常简单。对于每个分析的分钟间隔，我们知道消息数和失败数。一个间隔表示 100% 的时间，并且总消息量也被视为 100% 值。由此，我们可以推导出故障分数，并从时间间隔中提取相同的数量。
应使用所有转换的故障时间的总和来调整当前停机时间和错误预算值。
让我们看看在“数据接收器”示例中，所有这些工作将如何进行。对于此组件，我们有两种类型的故障：
发生错误，我们丢失了消息。
消息卡在某处超过 10 毫秒。
我们每分钟收集指标，并且还会生成所有零值（如果没有错误，我们将生成一个明确表示“错误=0”的指标。）我们还每 24 小时重新计算一次可用性。每隔一分钟，我们有三个数据桶：
处理的消息数
每条消息的计时
错误数
如果计时器存储桶缺少数据点，我们不知道该消息到底发生了什么，我们会将其标记为失败。同样，我们使用已处理的消息数和错误存储桶数执行此操作，如果其中一个为空，我们会将整个分钟标记为失败。
让我们用 2 台主机和 10 条消息重新计算原始示例，并想象该事件发生在一分钟的边界上，这是整个 24 小时内仅有的 10 条消息。计算结果：
第一分钟
消息： 10