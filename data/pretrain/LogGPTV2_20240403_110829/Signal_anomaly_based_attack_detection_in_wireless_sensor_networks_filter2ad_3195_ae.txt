parameters, and the optimal model is chosen to be used as the
anomaly detector. Different model parameters can be optimal
for different data distributions. Therefore it is important that if
there is a change in the data distribution, the model parameters
are reoptimized to reﬂect this.
A. Fixed Parameters
One method of performing model selection is to ﬁx the
model parameters during deployment so that only one model
is capable of being constructed from a training set. Parameters
can be determined via heuristics or using speciﬁc knowledge
of the domain the anomaly detection technique will be im-
plemented in. Fixed parameters simplify model selection as
the additional computation required for model selection is
dispensed with.
Ensemble classiﬁers use multiple models in order to im-
prove the predictive performance of an algorithm, and have
gained attention in recent years. Curiac et al. [58] use an
ensemble classiﬁer in order to perform anomaly detection
in a WSN. Five binary classiﬁers are used in the ensemble
based system; average based, autoregressive linear predictor,
neural network, neural network autoregressive and Adaptive
Neuro-Fuzzy Inference System. The classiﬁers independently
categorize the state of a sensor as “reliable” or “unreliable”
with the ﬁnal decision being determined by weighted majority
voting. If required, the ensemble is used to provide an estimate
of the correct measurement of a sensor affected by an anomaly.
Parameters are tuned in a training and testing phase before
deployment, once the system is deployed the parameters are
ﬁxed. A detailed methodology in performing the training and
testing phase is provided in order to ensure that the correct
parameters are determined for a speciﬁc deployment. The
methodology includes details on training the ensemble com-
ponents as efﬁciency can only be ensured if there is diversity
among the components of the ensemble [59]. To estimate the
diversity of the classiﬁers, pairwise metrics between classiﬁers
are used, with the mean of the metrics being used to determine
the diversity of the classiﬁer. Using the Q statistic, an overall
measure of diversity is found for the ensemble, with the
requirement that the value be close to 0 to ensure classiﬁer
diversity. If this condition is not met, the ensemble does not
pass the testing phase and must be retrained.
Evaluation of the system occurred by training and testing
the system as outlined previously in order to determine the
parameters. The data sets used were either synthetic or ob-
tained from the WSN that the system would be deployed in.
Results show that the system is able to detect nodes which are
producing errors, and is able to estimate the correct value. A
drawback of the system is that it operates on univariate data.
Parameters can have a large impact on the generalization
error of a classiﬁer. The parameters that require setting depend
on the anomaly detection technique that is being used. An
example of an anomaly detection technique that uses ﬁxed
parameters is that of Rajasegarar et al. [31]. QSSVM [53]
and CESVM are used to form a non-linear boundary in
order to detect anomalies in multivariate data sets where the
relationship between the attributes is not linear.
it
is possible to identify the value of ν that
A regularization parameter, ν, is required. This parameter
determines the number of data vectors that will lie outside the
hypersphere or hyperellipsoid. Varying this parameter controls
the trade-off between false positives and true positives. Using
a ROC,
is
optimal in terms of balancing the trade-off, however, this
value must be set before deployment and requires ground truth
labels. In addition to the regularization parameter, mapping the
data into a higher dimensional space using a kernel function
requires a parameter to be determined. For example, the radial
basis function (RBF) kernel requires the width parameter
σ. Evaluations of the QSSVM and CESVM show that the
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.12
IEEE COMMUNICATIONS SURVEYS & TUTORIALS, ACCEPTED FOR PUBLICATION
CESVM has less sensitivity to parameter selection than the
QSSVM. However, depending on the parameters chosen, there
can be a large difference in the performance of the anomaly
detection technique.
Distributed anomaly detection takes the form of the detec-
tion of global anomalies in a hierarchical topology using the
QSSVM. After a sensor node has calculated the radius of the
QSSVM, it is communicated to its parent node. The parent
node combines the radii from its children with its own radii,
and from this a global radius is calculated. Four strategies
for calculating the global radius are proposed: mean, median,
maximum, or minimum. The global radius is communicated
to child nodes where it is used to classify data as normal or
global anomaly.
Evaluation on the Great Duck Island (GDI) data set shows
that the QSSVM [53] and CESVM have good accuracy in
detecting anomalies in WSNs. However, two drawbacks to
the schemes exist; computational complexity and requirement
for parameter selection.
A statistical approach to anomaly detection is proposed by
Zhang et al. [60]. Models are constructed for the detection of
temporally and spatially correlated outliers in time-series data.
The technique operates in an online and distributed manner.
An auto-regressive moving average (ARMA) model is used to
create a stationary time series and this is then used to predict
future values, with actual measurements which lie outside the
conﬁdence interval being detected as outliers. A simpliﬁed
version of the ARMA model was used. A reduction to AR(p)
means that the current observation is correlated to the previous
p observations. The value of p was kept to a minimum to
reduce computational complexity. The AR model on a local
node was used to predict the next measurement.
The effect on performance of several ﬁxed parameters is
examined. The parameter p speciﬁes the number of previous
measurements used to predict the next measurements. It was
shown that increasing p increased accuracy, however, this led
to increased computational complexity of model construction.
Another parameter examined was the conﬁdence interval. The
conﬁdence interval represents a trade-off between the TPR
and FPR. The use of a high conﬁdence interval led to a high
TPR and a high FPR. The use of a low conﬁdence interval
resulting in a low TPR and a low FPR. It was identiﬁed that
more outliers were included in the conﬁdence interval if the
conﬁdence level was high and these were identiﬁed as normal.
From these evaluations optimal parameters were chosen.
A signiﬁcant advantage of the technique is the low com-
putational complexity. However, the algorithm operates on
univariate data streams,
therefore the correlation between
attributes of sensor readings is not taken into account.
B. Optimized Parameters
Model selection involves selecting a model from a set of
models that has a performance that can be considered optimal
by some measure on a set of unseen testing data. Parameter
variation provides the means to produce the set of models and
parameter optimization selects the parameters that provide the
optimal model.
A solution to the problem of determining the regularization
parameter for the QSSVM is proposed by O’Reilly et al. [34].
The regularization parameter, ν, is shown to have a signiﬁcant
effect on the boundary of the OC-SVM and thus a signiﬁcant
effect on the performance of the anomaly detector. It is shown
that the error rate of the classiﬁer can be minimized by choos-
ing an appropriate value for ν. Ratsch et al. [61] proposed
a heuristic for determining the regularization parameter for
the OC-SVM by selecting the model that separates the mean
of the normal and outlier classes with the greatest distance.
This is applied to the QSSVM where an online algorithm is
proposed using a golden section search [62] to identify and
track the optimal ν for a non-stationary data set. Evaluation of
the scheme shows that it is able to effectively optimize the ν
parameter while minimizing the number of models constructed
during the model selection phase. A drawback of the scheme
is the requirement for the construction of multiple models, the
most computationally complex operation in the technique, in
order to determine the optimal ν parameter.
Parameter estimation is performed in a Kth nearest neigh-
bour (k-NN)-based technique proposed by Xie et al. [63],
which optimizes two of four parameters required. The scheme
uses a version of k-NN that establishes continuous hypercubes
from a hyper-grid structure in feature space. Data are mapped
into the hypercubes and anomalies are deﬁned as data vectors
residing outside the hypergrid. Two parameters are determined
prior to implementation. The parameter b, a maximal bit length
required to encode the scheme and c, a coefﬁcient required to
shift feature space into positive coordinate space. However, the
algorithm is more sensitive to the two remaining parameters
which are optimized. The hypercube is centred at y and has
a diagonal length of the second estimated parameter, d. The
parameter d is derived using the mean integrated squared error
(MISE) and is shown to depend only on known constants.
Estimation of the appropriate value for k is learnt from the
data after the value of d has been determined. The parameter
k is equivalent to the number of nearest-neighbours, but in the
hypercube version it speciﬁes the number of data vectors that
must exist within a hypercube of the test vector for it to be
considered normal. The value of the parameter k is derived
using the analysis of the probability of an anomaly occurring.
Analysis of the complexity of the scheme determines it to
be less than that of the QSSVM scheme [31] in terms of
computational complexity. Evaluations performed on the IBRL
data set indicate that there exists either little performance
degradation compared to the standard distance-based (and
more computationally complex) k-NN. Evaluations also show
that there were good estimations of the optimal values for
parameters k and d.
Chatzigiannakis et al. [33] introduce a principal component
analysis (PCA)-based anomaly detection scheme that operates
on multivariate data in a distributed manner. The approach
fuses correlated data from multiple nodes in order to detect
anomalies that span neighbouring sensors. Initially, the net-
work of sensor nodes is divided into clusters, with nodes
that have correlated data forming clusters. To form clusters, a
primary node queries one (or more) hop neighbours for recent
measurements. An estimation of the correlation of the data
from the cluster head and the cluster node is obtained using
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.O’REILLY et al.: ANOMALY DETECTION IN WIRELESS SENSOR NETWORKS IN A NON-STATIONARY ENVIRONMENT
13
the correlation coefﬁcient RX,Y
RX,Y =
Cov(X, Y )
SX · SY
(2)
The primary node uses a predeﬁned threshold and the correla-
tion coefﬁcient to select nodes to join its group. The technique
has ﬂexibility by allowing the existence of overlapping nodes
between neighbouring groups and combining the individual
decisions of the groups.
Once the grouping phase is complete, the nodes send data
observations to the primary node. The primary node then
performs analysis on the data using PCA. The principal
components (PC) of the covariance matrix are sensitive to
large differences between the variances of the data vectors,
therefore the eigenvectors of the correlation matrix are used
as the PCs. The squared prediction error (SPE) is used to
detect anomalies is the data set.
Parameter optimization is used in order to determine the
the data to the required
number of PCs that will model
accuracy. The technique of cumulative percentage of total
variation [64] is often used. However, in this approach a
different technique is used that is speciﬁc to the correlation
matrix. Variables in the correlation matrix have unit variance,
if a PC of the correlation matrix has less than unit variance it
contains less information than the original variable. Therefore
only PCs with variances that exceed 1 are retained.
Evaluations were performed on a data set from a real-
world WSN deployment that measured meteorological data.
Artiﬁcial anomalies were inserted into the dataset. Results
showed that the higher the correlation of data on the nodes
in a group, the better the performance. A disadvantage of
the approach is the requirement to determine the detection
threshold (the anomaly rate in the data set). In addition, the
algorithm was shown have better performance with random
anomalies compared with correlated anomalies.
VII. MODEL UPDATE – CONSTRUCTION
Previously, techniques were detailed which determine when
an anomaly detector must reconstruct a model which is used
to classify data vectors. When it is determined that a model
update is required, the algorithm must use the determined
parameters to construct the new model. In this section the
methods by which a model is constructed are detailed. Model
construction involves the use of a new training set and can be
categorized into batch update and incremental update.
A. Batch
Batch learning occurs when the previous data set is dis-
carded and a new model is constructed using a new training
set. Batch learning methods frame a training set with a window
of data, these windows can take two forms, a ﬁxed sliding
window and a weighted sliding window.
1) Fixed Sliding Windows: Fixed sliding windows have a
ﬁxed parameter that determines how the window is formed,
either containing a ﬁxed number of data instances, or data
instances from a ﬁxed period of time. The window advances
each time step, and therefore the oldest data instances leave
the window, and new data instances are added. The system is
simple but effective in presenting a classiﬁer with a data set
from the required period. Machine learning algorithms that
assume that data are generated from a stationary distribution
can be adapted to a non-stationary distribution by sliding
windows. When it
is determined that a model update is
required, the data in the sliding window are used as the training
set.
An approach using ﬁxed sliding windows to frame the
training set of a model is proposed by Xie et al. [65] which
operates in an online and distributed manner. A ﬁxed number
of measurements in the sliding window allow adaptation to
changes in the data distribution. There are three phases to the
scheme; training phase, test phase and update phase. During
the training phase, nodes create histograms of univariate data
with bins being marked as normal or anomalous according
to the histogram. The training phase includes a distributed
component where cluster nodes in a WSN communicate their
two-dimensional array to a clusterhead. The global normal
proﬁle is determined using the local histograms and this is then
communicated back to the cluster nodes. After the training
phase has been completed, the test phase determines whether a
data vector is anomalous using the local histogram, the global
histogram, or both.
The update phase makes use of a ﬁxed sliding window.
A test data vector is added to the sliding window with
probability Pu, displacing the oldest data vector. The value
of Pu is determined by how smoothly the probability density
function (PDF) changes. The use of a probability test
to
determine whether an incoming data measurement should join
the training set reduces the number of model updates that are
performed. When the ﬁxed size sliding window has a speciﬁed
number of new training data vectors, an update to the model
is triggered in which training phase will be repeated.
The technique proposed by Beigi et al. [66], [67] identiﬁes
anomalies in univariate data on local nodes. The aim is to
model the current data distribution in a ﬁxed sliding window
and compare this with a baseline data distribution. The authors
observe that abrupt changes in the statistical characteristics
of the data indicate that anomalies are present. Histograms
are used to model the data distribution as they have low
complexity and can be computed quickly and updated in-
crementally as new data arrives. Two sliding windows are
used to perform anomaly detection. One is a baseline which
represents expected or historic behaviour. The second sliding
window represents current data. The distribution of the current
observed data is compared to the distribution of the baseline.
Histograms are used to approximate the underlying data distri-
butions of the two sliding windows of data with the difference
between data distributions being measured by the Kullback-
Leibler (KL) divergence metric [68]. A distance vector is
created when the two data distributions are compared and
anomalies are identiﬁed within this vector by identifying the
maxima points. Three methods are used; constant threshold,
top percentage and maximum neighbour. Constant threshold
can adapt to a varying anomaly rate, where as a technique such
as top percentage cannot. Multiple ﬁxed-size sliding windows
are used so that a multi-scale temporal analysis is performed
on the data set.
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.14
IEEE COMMUNICATIONS SURVEYS & TUTORIALS, ACCEPTED FOR PUBLICATION
Detection accuracy can be maximized by minimizing the
error in generating the distribution of the data and this is
performed through parameter optimization. Three schemes
are outlined for the optimization of the parameters. The
ﬁrst scheme optimizes the bin width given the bounds on
the baseline and current window size. The second scheme
determines the bounds on the baseline window given the
current window size. The ﬁnal scheme determines the bounds
on the current window given the baseline window size.
The evaluation of the scheme shows the effect of the
temporal scale at which the sliding windows are set; different
anomalies are detected by analyzing a data stream at different
temporal scales.
2) Weighted Sliding Windows: An alternative to using ﬁxed
sliding windows is to weight the data that is used for model
construction in order to give greater prominence to those items
considered to be representative of the current data distribution.
Livani et al. [69] propose an anomaly detection scheme
that uses PCA on multivariate data in a distributed manner.
Nodes form clusters of nodes which are physically close and
have data that are spatially correlated. One of the nodes in the
cluster is designated as the cluster head. PCA is performed on
the data instances on a sensor node using distributed principal
component analysis (DPCA) [70]. DPCA is able to construct
the global principal components on the cluster head following
an intermediary calculation on the sensor node data. This
reduces communication cost by reducing the amount of data
that are sent to the cluster head. Fixed width clustering is
then performed on data on the sensor nodes with the results
being communicated to the cluster head. The cluster head
then calculates the distance of each cluster from the global
principal component, ϕ(0), with the maximum distance, dmax,
of all clusters from ϕ(0) being determined. dmax is used as