title:Phonotactic Reconstruction of Encrypted VoIP Conversations: Hookt
on Fon-iks
author:Andrew M. White and
Austin R. Matthews and
Kevin Z. Snow and
Fabian Monrose
2011 IEEE Symposium on Security and Privacy
Phonotactic Reconstruction of Encrypted VoIP Conversations:
Hookt on fon-iks
Andrew M. White∗
Austin R. Matthews∗†
∗Department of Computer Science
Kevin Z. Snow∗
†Department of Linguistics
Fabian Monrose∗
University of North Carolina at Chapel Hill
{amw, kzsnow, fabian}@cs.unc.edu, PI:EMAIL
Chapel Hill, North Carolina
Abstract—In this work, we unveil new privacy threats against
Voice-over-IP (VoIP) communications. Although prior work
has shown that the interaction of variable bit-rate codecs and
length-preserving stream ciphers leaks information, we show
that the threat is more serious than previously thought. In par-
ticular, we derive approximate transcripts of encrypted VoIP
conversations by segmenting an observed packet stream into
subsequences representing individual phonemes and classifying
those subsequences by the phonemes they encode. Drawing on
insights from the computational linguistics and speech recog-
nition communities, we apply novel techniques for unmasking
parts of the conversation. We believe our ability to do so
underscores the importance of designing secure (yet efﬁcient)
ways to protect the conﬁdentiality of VoIP conversations.
I. INTRODUCTION
Over the past decade, Voice-over-IP (VoIP) telephony has
witnessed spectacular growth. Today, VoIP is being used
everywhere, and is making steady headway as a replacement
for traditional telephony in both the residential and commer-
cial sectors. The popularity of free online services such as
Skype, Fring, and Google Talk is a case in point. Indeed,
several analysts predict that VoIP will remain the fastest
growing industry over the next decade, and some forecast
that the subscriber base will top 225 million by 2013.1 Yet,
even with this widespread adoption, the security and privacy
implications of VoIP are still not well understood. In fact,
even with the attention VoIP security (or lack thereof) has
received in the past, the concerns have mostly centered on
the lack of authenticity in the call setup phases of the signal
and session negotiation protocol(s) or susceptibility to denial
of service attacks [33]. Regarding the conﬁdentiality of the
data streams themselves, the prevailing wisdom is that, due
to the open nature of trafﬁc traveling over the Internet, VoIP
packets should be encrypted before transmission.
However, current practices for encrypting VoIP pack-
ets have been shown to be insufﬁcient for ensuring pri-
vacy. In particular, two common design decisions made in
VoIP protocols—namely, the use of variable-bit-rate (VBR)
codecs for speech encoding and length-preserving stream
1See, for example, Infonetics Research’s VoIP and UC Services and
Subscribers Report at http://www.infonetics.com.
1081-6011/11 $26.00 © 2011 IEEE
DOI 10.1109/SP.2011.34
3
ciphers for encryption—interact to leak substantial infor-
mation about a given conversation. Speciﬁcally, researchers
have shown that this interaction allows one to determine the
language spoken in the conversation [55], the identity of
the speakers [2, 41], or even the presence of known phrases
within the call [56].
Rightfully so, critics have argued that the aforementioned
threats do not represent a signiﬁcant breach of privacy. For
example, the language of the conversation might easily be
determined using only the endpoints of the call—a call
from Mexico to Spain will almost certainly be in Spanish.
While the identiﬁcation of target phrases is more damning,
it still requires the attacker to know (in advance) what she
is looking for within the stream. In this work, we make no
such assumption about a priori knowledge of target phrases.
Rather, our ultimate goal is to reconstruct a hypothesized
transcript of the conversation from the bottom up: our
approach segments the observed sequence of packets into
subsequences corresponding to individual phonemes (i.e.,
the basic units of speech). Each subsequence is then classi-
ﬁed as belonging to a speciﬁc phoneme label, after which
we apply speech and language models to help construct
a phonetic transcription of parts of the conversation. To
assess the quality of our reconstruction, we apply widely
accepted translation scoring metrics that are designed to
produce quality scores at the sentence level that correlate
well with those assigned by human judges.
The approach we take has parallels to how infants ﬁnd
words in a speech stream. As Blanchard et al. [8] point out,
adults effortlessly break up conversational speech into words
without ever realizing that there are no pauses between
words in a sentence. This feat is possible because we have
a lexicon of familiar words that we can use to segment the
utterance. Infants have no such luxury. Instead, they must
use perceptual, social, and linguistic cues to segment the
stream of sounds. Amazingly, the linguistic cues come from
learned language-speciﬁc constraints (or phonotactics) that
determine whether a word is well-formed or not; infants use
this knowledge of well-formedness to help segment speech.
The fascinating problem here is that infants must learn
these rudimentary, language-speciﬁc, constraints while si-
multaneously segmenting words. They use familiar words
(e.g., their own names) to identify new words which are
subsequently added to their small vocabulary. Interestingly,
the Linguistic and Psychological Sciences literature abounds
with studies (e.g., [9, 23]) which show that, as early as
six months of age, infants use knowledge of which basic
phonemes occur together, as well as learned knowledge
of within-word versus between-word sounds, to segment
perceived utterances into words. As we show later, we
apply a similar methodology when tackling the problem of
reconstructing words from strings of phonemes.
II. BACKGROUND INFORMATION
Before proceeding further, we ﬁrst present some necessary
background that is helpful in understanding the remainder
of the paper. The background material covers basic notions
in linguistics, pertinent VoIP details, and information about
the datasets we use throughout the paper.
Phonetic Models of Speech
The ideas in this paper rely heavily on insights from
modern theories of phonology. In particular, we draw from
a vast body of work on phonetics—i.e., the study of lin-
guistic sounds. From a computational perspective, phonetics
involves studying how sound is produced by the articulators
of the vocal tract and how they are realized acoustically [30].
In phonetics,
the pronunciations of words are modeled
as strings of symbols representing individual speech units
called phones. While several alphabets exist for representing
phones (e.g., ARPAbet for American English), the de facto
standard is the International Phonetic Alphabet (IPA).
For the remainder of the paper, what is particularly impor-
tant is that each phone is based on articulatory processes, and
that phones are divided into two main classes: consonants
and vowels. Both kinds of sounds are formed by the motion
of air through the mouth, throat and nose. Consonants, for
example, are made by restricting airﬂow in some way, and
can be both voiced (meaning they involve vibrations of
the vocal cords) or unvoiced. By contrast, vowels usually
involve less obstruction of air ﬂow, and are louder and longer
lasting than consonants. Moreover, because all consonants
are sounds made by restricting airﬂow, they can be distin-
guished from each other by where the restriction is made
(the place of articulation) as well as how the restriction is
made (the manner of articulation). In English, for example,
the “hissing” sound of [f ] in ‘ﬁsh’ is made by pressing
the lower lip against the upper teeth. There are several
major manners (e.g., stops, nasals, and fricatives) used to
distinguish consonants.
Likewise, vowels can also be characterized by articulatory
processes (see Figure 1), the most important of which are
vowel height (i.e., roughly the height of the highest part of
the tongue), backness (i.e., roughly indicating where the tip
of the tongue is relative to the vocal track), and roundness
Figure 1. Vowels in American English (IPA format), differentiated by
their height and backness. Left: the relative tongue positions.
(i.e., whether the shape of the lips is rounded or not). For
example, compare how your mouth feels as you say ‘beat’
and ‘boot’. If you hold the vowels in these two words,
you should be able to feel a difference in the backness of
your tongue. Similarly, if you compare the words ‘beat’ and
‘bat’, you should feel your chin moving up and down; this
is a difference in height. To feel a difference in rounding,
compare the words ‘put’ and ‘pool’. As you say ‘pool’ you
should feel your lips pucker into a round shape; in ‘put’,
your lips should be loose.
Consonants and vowels are combined to make syllables,
which are governed by the phonotactics of the language —
that is, language-speciﬁc conditions that determine whether a
word is well-formed or not. At a high level, phonotactics are
constraints on which phones can follow which, i.e., rules that
govern how phones may be combined to form well-formed
words. In English, for example, there are strong constraints
on what kinds of consonants can appear together: [st] (as
in ‘stop’) is a very common consonant cluster, but some
consonant sequences, like [zdr] (as in ‘eavesdrop’), are not
legal word-initial sequences in English.2
Lastly, in linguistics and speech processing, an abstraction
called a phoneme (typically written between slashes) is
used to represent similar phones with a single symbol. For
example, the phoneme /t/ can be pronounced as any of three
phones in English; which of these three phones is uttered
depends on the position within a syllable: /t/ is pronounced
as [th] at the beginning of a syllable (as in ‘top’=[th op’]),
[t] in the middle of a syllable (as in ‘stop’=[st6p’]), and
[t’] at the end of a syllable (as in ‘pot’ = [ph ot’]). Phones
belonging to the same phoneme are called allophones: [th],
[t], and [t’] are allophones of the phoneme /t/.
In Section V, we leverage such linguistic insights to build
a string matching technique based on phonetic edit distance.
In addition, we use phonotactics of English (e.g., what
sequences of phonemes or allophones are allowable within
words) to assist with phoneme classiﬁcation.
2Of course, [zdr] may exist word-initially in other languages, such as in
the Bulgarian word [zdraf], which means ‘health’.
4
i uoʌeɛaɜɪʊæBacknessHeight'heed''hid''head''had''hood''who'd''one''heard''go''aisle''aid'ɔ'hawed'ɑ'via'ɒ'pond'Voice over IP
In VoIP, voice data and control messages are typically
transmitted through separate channels. The control channel
generally operates using an application-layer protocol, such
as the Extensible Messaging and Presence Protocol (XMPP)
used by Google Talk or the Session Initiation Protocol
(SIP). The voice channel typically consists of a Real-time
Transport Protocol (RTP) stream transmitted over UDP. We
concern ourselves only with the voice channel in this work.
Typically, the audio for VoIP conversations is encoded
using an audio codec designed speciﬁcally for speech, such
as Skype’s SILK, the Enhanced Full Rate (EFR) codec spec-
iﬁed by the GSM standard, or the open-source Speex used
in many VoIP applications (including Google Talk). Speech
codecs differ from general audio codecs since human speech
can be represented much more efﬁciently than general audio
due to the periodic nature of certain speech signals and the
relatively limited number of potential sounds. For speech,
sound is usually sampled at between 8 and 32 kHz (i.e.,
between 8,000 and 32,000 samples are recorded per second).
This sample stream is then segmented into frames, or blocks,
of a certain duration and each frame is compressed by the
speech codec for transmission. The duration is a ﬁxed value
generally between 10 and 100ms; a typical value, and the
one used in this work, is 20ms, which corresponds to 320
samples per frame when sampling at 16kHz.
Many modern speech codecs are based on variants of a
well-known speech coding scheme known as code-excited
linear prediction (CELP) [49], which is in turn based on the
source-ﬁlter model of speech prediction. The source-ﬁlter
model separates the audio into two signals: the excitation
or source signal, as produced by the vocal cords, and the
shape or ﬁlter signal, which models the shaping of the sound
performed by the vocal tract. This allows for differentiation
of phonemes; for instance, vowels have a periodic excitation
signal while fricatives (such as the [sh] and [f] sounds) have
an excitation signal similar to white noise [53].
In basic CELP, the excitation signal is modeled as an entry
from a ﬁxed codebook (hence code-excited). In some CELP
variants, such as Speex’s VBR mode, the codewords can
be chosen from different codebooks depending on the com-
plexity of the input frame; each codebook contains entries
of a different size. The ﬁlter signal is modeled using linear
prediction, i.e., as a so-called adaptive codebook where the
codebook entries are linear combinations of past excitation
signals. The “best” entries from each codebook are chosen
by searching the space of possible codewords in order
to “perceptually” optimize the output signal in a process
known as analysis-by-synthesis [53]. Thus an encoded frame
consists of a ﬁxed codebook entry and gain (coefﬁcient) for
the excitation signal and the linear prediction coefﬁcients for
the ﬁlter signal.
Lastly, many VoIP providers (including Skype) use VBR
codecs to minimize bandwidth usage while maintaining
call quality. Under VBR, the size of the codebook entry,
and thus the size of the encoded frame, can vary based
on the complexity of the input frame. The speciﬁcation
for Secure RTP (SRTP) [3] does not alter the size of the
original payload; thus encoded frame sizes are preserved
across the cryptographic layer. The size of the encrypted
packet therefore reﬂects properties of the input signal; it is
exactly this correlation that our approach leverages to model
phonemes as sequences of lengths of encrypted packets.
III. HIGH-LEVEL OVERVIEW OF OUR APPROACH
The approach we pursue in this paper leverages the corre-
lation between voiced sounds and the size of encrypted pack-
ets observed over the wire. Speciﬁcally, we show that one
can segment a sequence of packet sizes into subsequences
corresponding to individual phonemes and then classify
these subsequences by the speciﬁc phonemes they repre-
sent. We then show that one can segment such a phonetic
transcript on word boundaries to recover subsequences of
phonemes corresponding to individual words and map those
subsequences to words, thereby providing a hypothesized
transcript of the conversation.
Figure 2. Overall architecture of our approach for reconstructing transcripts
of VoIP conversations from sequences of encrypted packet sizes.
Our work draws from advances in several areas of com-
putational science. A simpliﬁed view of our overall process
is shown in Figure 2. As an example, we use the phrase
‘rock and roll’, the dictionary pronunciation for which is
represented as [ô6k ænd ôoUl] in IPA. Our basic strategy is as
follows. First, we use a maximum entropy model (Stage )
5
PhonemeClassificationLanguage ModelCorrectionRock and roll!VoIP Conversation Reconstruction ProcessEncrypted VoIP PacketsWord ClassificationWordSegmentation➊➋➌➍rɒkr-ɒ-krockæ-n-dændandror-o-ʊ-lrollʊlPhoneme SegmentsCorrected PhonemesWord SegmentsWordsrɒkæmdroilPhonemesPacket LengthsPhonemeSegmentationto segment the sequence of packet sizes into subsequences
corresponding to individual phonemes. We then apply (Stage
) a combination of maximum entropy and proﬁle hidden
Markov models to classify each subsequence of packet
sizes according to the phoneme the subsequence represents,
resulting in an approximate phonetic transcript of the spoken
audio. In our example, this transcript is [ô6kæmdôoil].
The hypothesized transcript
is improved by applying
a trigram language model over phonemes (and phoneme
types) which captures contextual information, such as likely
phoneme subsequences, and corrects the transcript to rep-
resent the most likely sequence of phonemes given both
the classiﬁcation results and the language model. In our
example, this results in [ô6kændôoUl]. Notice the unlikely
phonetic sequence [æmd] has been replaced with the far
more likely [ænd]. Next, we segment (Stage ) the resulting
transcript into subsequences of phonemes corresponding to
individual words using a phonetic constraint model, resulting
in the more recognizable string [ô6k ænd ôoUl].
Finally, we match each subsequence to the appropriate En-
glish word using a phonetic edit distance metric (Stage ),
giving us the desired ‘rock and roll’. In the general case, a
trigram language model over words (and parts-of-speech) is
then applied to the resulting transcript to correct tense and
disambiguate between homophones (i.e., words which sound
alike) by ﬁnding the most likely sequence of words given
both the hypothesized transcript and the language model.
Data and Adversarial Assumptions
The TIMIT Acoustic-Phonetic Continuous Speech Cor-
pus [21], a collection of recorded speech with time-aligned
word and phonetic transcripts (allowing us to label segments
by phoneme), provides the audio samples used in our ex-
periments. The TIMIT corpus is comprised of 6,300 speech
recordings from 630 speakers representing eight major di-
alects of American English. Each speaker reads ten pre-
determined, phonetically-rich sentences, such as ‘Alimony
harms a divorced man’s wealth’, ‘The drunkard is a social
outcast’, and ‘She had your dark suit in greasy wash water all
year’. The transcripts contain labels for 58 distinct phoneme-
level3 sounds. Following the standard approach used in
the speech recognition community, we folded the original
TIMIT classes into 45 labels [36] by combining some allo-
phones and combining closures and silences. ARPAbet, the
phonetic alphabet on which the labeling systems of TIMIT
is based, does not map directly to the articulatory features
in Section II; therefore, we convert the phoneme sequences