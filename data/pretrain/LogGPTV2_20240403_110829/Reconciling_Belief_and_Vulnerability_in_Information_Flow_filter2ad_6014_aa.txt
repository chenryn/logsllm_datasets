title:Reconciling Belief and Vulnerability in Information Flow
author:Sardaouna Hamadou and
Vladimiro Sassone and
Catuscia Palamidessi
Reconciling Belief and Vulnerability in Information Flow
Sardaouna Hamadou, Vladimiro Sassone, Catuscia Palamidessi
To cite this version:
Sardaouna Hamadou, Vladimiro Sassone, Catuscia Palamidessi. Reconciling Belief and Vulnerability
in Information Flow. 31st IEEE Symposium on Security and Privacy, May 2010, Berleley/Oakland,
California, United States. pp.79-92, 10.1109/SP.2010.13. inria-00548007
HAL Id: inria-00548007
https://hal.inria.fr/inria-00548007
Submitted on 19 Dec 2010
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Reconciling Belief and Vulnerability in Information
Flow
Sardaouna Hamadou and Vladimiro Sassone
School of Electronics and Computer Science (ECS)
University of Southampton
Southampton, UK
{sh3, vs}@ecs.soton.ac.uk
Catuscia Palamidessi
INRIA and LIX
Ecole Polytechnique
Paris, France
PI:EMAIL
Abstract—Belief and vulnerability have been proposed recently
to quantify information ﬂow in security systems. Both concepts
stand as alternatives to the traditional approaches founded on
Shannon entropy and mutual information, which were shown to
provide inadequate security guarantees. In this paper we unify
the two concepts in one model so as to cope with (potentially
inaccurate) attackers’ extra knowledge. To this end we propose
a new metric based on vulnerability that takes into account the
adversary’s beliefs.
Index Terms—Security; information hiding, information ﬂow;
quantitative and probabilistic models; uncertainty; accuracy;
I. Introduction
Protecting sensitive and conﬁdential data is becoming in-
creasingly important in many ﬁelds of human activities, such
as electronic communication, auction, payment and voting.
Many protocols for protecting conﬁdential information have
been proposed in the literature. In recent years the frame-
works for reasoning, designing, and verifying these protocols
have considered probabilistic aspects and techniques for two
reasons. First, the data to be protected often range in domains
naturally subject to statistical considerations. Second and more
important, the protocols often use randomised primitives to
obfuscate the link between the information to be protected and
the observable outcomes. This is the case, e.g., of the DCNets
[8], Crowds [30], Onion Routing [37], and Freenet [13].
From the formal point of view, the degree of protection is the
converse of the leakage, i.e. the amount of information about
the secrets that can be deduced from the observables. Early ap-
proaches to information hiding in literature were the so-called
possibilistic approaches, in which the probabilistic aspects
were abstracted away and replaced by non-determinism. Some
examples of these approaches are those based on epistemic
logic [19], [36], on function views [21], and on process calculi
[31], [32]. Recently, however, it has been recognised that the
possibilistic view is too coarse, in that it tends to consider
as equivalent systems which have very diﬀerent degrees of
protection.
The probabilistic approaches are therefore becoming in-
creasingly more popular. At the beginning they were inves-
tigated mainly at their strongest form of protection, namely
to express the property that the observables reveal no (quan-
titative) information about the secrets (strong anonymity, no
interference) [2], [8], [19]. More recently, weaker notions of
protection have been considered, due to the fact that such
strong properties are almost never achievable in practice.
Still in the probabilistic framework, Rubin and Reiter have
proposed the concepts of possible innocence and of probable
innocence [30] as weak notions of anonymity protection (see
also [4] for a generalisation of the latter). These are, however,
still true-or-false properties. The need to express in a quanti-
tative way the degree of protection has then lead naturally to
explore suitable notions within the well-established ﬁelds of
Information Theory and of Statistics.
Concepts from Information Theory [15] have indeed re-
vealed quite useful in this domain. In particular, the notion
of noisy channel has been used to model protocols for
information-hiding, and the ﬂow of information in programs.
The idea is that the input s ∈ S of the channel represents the
information to be kept secret, and the output o ∈ O represents
the observable. The noise of the channel is generated by the
eﬀorts of the protocol to hide the link between the secrets and
the observable, usually by means of randomised mechanisms.
Consequently, an input s may generate several diﬀerent outputs
o, according to a conditional probability distribution p(o | s).
These probabilities constitute the channel matrix C. Similarly,
for each output there may be several diﬀerent correspond-
ing inputs, according to the converse conditional probability
p(s | o) which is linked to the above by the Bayes law:
p(s | o) = p(o | s) p(s)/p(o). The probability p(s) is the a priori
probability of s, while p(s | o) is the a posteriori probability
of s, after we know that the output is o. These probability
distributions determine the entropy and the conditional entropy
of the input, respectively. They represent the uncertainty about
the input, before and after observing the output. The diﬀerence
between entropy and conditional entropy is called the mutual
information and expresses how much information is carried
by the channel, i.e. how much uncertainty about the input
we lose by observing the output (i.e., equivalently, how much
information about the input we gain by observing the output).
Even though several notions of entropy have been proposed
in Information Theory, Shannon’s is by far the most famous
of them, due to its relation with the channel’s rate, i.e., the
speed by which information can be transmitted accurately on
a channel. Consequently, there have been various attempts to
deﬁne the degree of protection by using concepts based on
Shannon entropy, notably mutual information [10], [23], [24],
[38] and the related notion of capacity, which is the supremum
of the mutual information over all possible input distributions,
and which therefore represents the worst case from the point
of view of security [5], [27], [28].
A reﬁnement of the above approaches came from the ideas
of integrating the notions of extra knowledge and belief [14],
[18]. The idea is that the gain obtained by looking at the
output should be relative to the possible initial knowledge or
belief that an attacker may have about the secret. For instance,
assume that in a parliament composed by m Labourists and
n Conservatives, m members voted against a proposal
to
remove minimum wages. Without any additional knowledge
it is reasonable to believe that all Labourists voted against.
If however we came to know that exactly one Conservative
voted against, then it is more reasonable to believe that the
most liberally-inclined Conservative voted against, and the
least liberally-inclined Labourist voted in favour. In this case,
the a posteriori belief is likely to be much more accurate than
the a priori one, and the gain obtained using the knowledge
about MPs’ relative positioning on the left-to-right scale is
much larger than the one computed as diﬀerence of entropies.
Consequently, [14] proposes to deﬁne the protection of a
system in terms of the diﬀerence (expressed in terms of
Kullback-Leibler divergence) between the accuracy of the a
posteriori belief and the accuracy of the a priori one.
In recent work, however, Smith has shown that the concepts
based on Shannon entropy are not very suitable for modelling
the information leakage in the typical scenario of protocol
attacks, where the adversary attempts to guess the value of
the secret in one single try [33]. He gave an example of two
programs whose Shannon’s mutual information is about the
same, yet the probability of making the right guess after having
observed the output is much higher in one program than in the
other. In a subsequent paper [34], Smith has proposed to deﬁne
the leakage in terms of a notion of mutual information based
on R´enyi min-entropy.
Recently in [20] the authors extended the vulnerability
model of [34] in the context of the Crowds protocol for anony-
mous message posting to encompass the frequent situation
where attackers have extra knowledge. They pointed out that
in Crowds the adversary indeed has extra information (viz.,
the target servers) and assumed that she knows the correlation
between that and the secret (viz., the users’ preferences for
servers). They proved that in such scenarios anonymity is more
diﬃcult to achieve.
In our opinion, a fundamental issue remains wide open:
the need to measure and account for the accuracy of the
adversary extra knowledge. Indeed, [20] assumes that
the
adversary’s extra information is accurate, and this assumption
is generally not warranted. Inaccuracy can indeed arise, e.g.
from people giving deliberately wrong information, or simply
from outdated data. As already noticed in [14] there is no
reason in general to assume that the probability distributions
the attacker uses are correct, and therefore they must be treated
as beliefs.
This paper tries to ﬁll this gap by generalising the model on
R´enyi min-entropy to cope with the presence of the attacker’s
beliefs. To this end we propose a new metric based on the
concept of vulnerability that takes into account the adversary’s
beliefs. The idea is that the attacker does not know the actual
probability distributions (i.e., the a priori distribution of the
protocol’s hidden input and its correlation with the extra
information), and is assuming them. The belief-vulnerability
is then the expected probability of guessing the value of the
hidden input in one try given the adversary’s belief. Informally,
the adversary chooses the value of the secret input which
has the maximum a posteriori probability according to her
belief. Then the vulnerability of the secret input is expressed in
terms of the actual a posteriori probabilities of the adversary’s
possible choices. We show the strength of our deﬁnitions
both in terms of their theoretical properties and their utility
by applying them to various threat scenarios and comparing
the results to the previous approaches. Among its several
advantages, our model allows to identify the levels of accuracy
for the adversary’s beliefs which are compatible with the
security of a given program or protocol.
The rest of the paper is organised as follows: in §II we
ﬁx some basic notations and recall some fundamental notions
in §III we brieﬂy revise previous
of Information Theory;
approaches to quantitative information follow; §IV delivers our
core technical contribution by extending the model on R´enyi
min entropy to the case of attacker’s beliefs and investigating
its theoretical properties; in §V we apply our approach to vari-
ous threat scenarios and compare it to the previous approaches
whilst §VI contains our concluding remarks.
II. Preliminaries
In this section we brieﬂy revise the elements of Information
Theory which underpin the work in this paper, and illustrate
our conceptual framework.
A. Some notions of information theory
Being in a purely probabilistic setting gives us the abil-
ity to use tools from information theory to reason about
the uncertainty of a random variable and the inaccuracy of
assuming a distribution for a random variable. In particular
we are interested to the following notions: entropy, mutual
information, relative entropy and min-entropy. We refer the
reader to [16], [26] for more details.
We use capital letters X, Y to denote discrete random vari-
ables and the corresponding small letters x, y and calligraphic
letters X, Y for their values and set of values respectively. We
denote by p(x), p(y) the probability of x and y respectively
and by p(x, y) their joint probability.
Let X, Y be random variables. The (Shannon) entropy H(X)
of X is deﬁned as
H(X) = −Xx∈X
p(x) log p(x).
(1)
The entropy measures the uncertainty of a random variable.
It takes its maximum value log |X| when X is uniformally
distributed and its minimum value 0 when X is a constant. We
take the logarithm with a base 2 and thus measure entropy in
bits. The conditional entropy
H(X|Y) = −Xy∈Y
p(y)Xx∈X
p(x|y) log p(x|y)
(2)
measures the amount of uncertainty of X when Y is known.
It can be shown that 0 ≤ H(X|Y) ≤ H(X) with the leftmost
equality holding when Y completely determines the value of
X and the rightmost one when Y reveals no information about
X, i.e., X and Y are independent random variables.
Comparing H(X) and H(X|Y) give us the notion of mutual
information, denoted I(X; Y) and deﬁned by
I(X; Y) = H(X) − H(X|Y).
(3)
It is non-negative, symmetric and bounded by H(X). In other
words 0 ≤ I(X; Y) = I(Y; X) ≤ H(X).
The relative entropy or Kullback-Leibler distance between
two probability distribution p and q on the same set X, denoted
D(p k q), is deﬁned as
D(p k q) = Xx∈X
p(x) log
p(x)
q(x)
.
(4)
It is non-negative (but not symmetric) and it is 0 if and only
if p = q. The relative entropy measures the inaccuracy or
information divergence of assuming that the distribution is q
when the true distribution is p.
The guessing entropy G(X) is the expected number of tries
required to guess the value of X optimally. The optimal
strategy is to guess the values of X in decreasing order of
probability. Thus if we assume that X = {x1, x2, . . . , xn} and
xi’s are arranged in decreasing order of probabilities, i.e.,
p(x1) ≥ p(x2) ≥ · · · ≥ q(xn), then
probabilities p(o j | ai), where p(o j | ai) is the probability that
is ai. An
the low output
adversary or eavesdropper can see the output of a protocol,
but not its input, and she is interested in deriving the value of
the input from the observed output in one single try.
is o j given that
the high input
In this paper we shall assume that the high input is gener-
ated according to an a priori probabilistic distribution pρ(ai)
unknown to the adversary, as explained in the introduction.
We also denote by pβ(ai) the adversary’s assumed a priori
distribution of A. Furthermore, we assume that the attacker
has access to the value of a random variable B distributed over
B which summarises the additional knowledge (information)
about A she has independently of the behaviour of the protocol.
The matrix of the conditional probabilities pρ(bk | ai) (resp.
pβ(bk | ai)) expresses the real (resp. the adversary’s assumed)
correlation between the hidden input and the additional ob-
servables B. An adversary’s belief then consists of the pair
pβ(ai), pβ(bk | ai) of her assumed probabilities.
When | B | = 1 and the a priori distribution is publicly-
known, i.e., pρ(ai) = pβ(ai), the adversary’s additional infor-
mation about A is trivial and cannot help to determine the
value of A. For example, knowing the length of a password
in a ﬁxed-length password system gives no advantage, as all
passwords have the same length. Trivial information allow us
to model the absence of additional information, and to see the
standard framework in the literature as an instance of ours.
Example 1: Let A be a random variable with a publicly-
known uniform a priori distribution over A = {0, 1, 2, 3}.
Assume that the adversary’s additional observable is the parity
of A, i.e. B = {0, 1}, with the following deterministic belief’s
correlation pβ(bk | ai) = p(ai mod 2 = bk). In other words, the
adversary believes that her additional information accurately
reﬂects that the value of A is an even number if B = 0 and
odd otherwise.
Now suppose that A is the high input of the deterministic
G(X) = X1≤i≤n
ip(xi).
The min-entropy H∞(X) of a random variable is given by
H∞(X) = − log max
x∈X
p(x)
(6)
PROG C1:
BEGIN
program C1 below, whose low output is
(5)
O = ( 1
2
if a ∈ {0, 1}
otherwise.
and measures the diﬃculty for an attacker to correctly guess
the value of X in one try (obviously using the optimal strategy