set of data records as an input, is E-diﬀerentially private if the removal of any
single record r from the input changes the probability of any predicate on the
output being true at most by a factor of eE(r).
Thus a query can provide a diﬀerent level of privacy for each record. If E(r) = 
for all r then we get -diﬀerential privacy as a special case.
We can consider two diﬀerent methods for enforcing Personalized Diﬀerential
Privacy. In the simpler case, this means that instead of the global privacy budget,
each row in the database has a separate privacy budget. When an -diﬀerentially
private query is made, then only the rows participating in the query have their
budgets reduced (by ). Thus we have E-diﬀerential privacy with E(r) =  if the
row r participates in the query and E(r) = 0 otherwise. This allows performing
more queries using the same privacy budget.
In the more complicated case, each row in the database has a provenance,
each provenance (not each row) has a privacy budget, and there can be several
rows with the same provenance. Thus if an -diﬀerentially private query uses r
rows with some provenance p then the budget of this provenance p is reduced
by r. Here we have E(p) = r. Here the domain of E is the set of provenances,
not the set of actual records but, as in [13], we can instead consider provenances
themselves to be records and take the composed query Q ◦ F where F is a
union-preserving function that maps each provenance to a set of records with
that provenance and Q is the actual query on the chosen records. If Q is -
diﬀerentially private then Q◦F is E-diﬀerentially private with E(p) = |F ({p})|·.
6 Secure Multiparty Computation
Secure multiparty computation (SMC) is the universal cryptographic function-
ality, allowing any function to be computed obliviously by a group of mutually
distrustful parties. There exist a number of diﬀerent techniques for SMC, includ-
ing garbled circuits [33] and homomorphic encryption [8]. In this work, we have
considered SMC based on secret sharing.
In secret sharing there are n parties (n > 1), and every private value x is
split into shares x1, . . . , xn such that party i has the share xi. The private value
can be recovered if at least k parties out of n provide their shares. For a number
of operations, there exist more or less eﬃcient protocols that receive the shares
of the operands as input, and deliver the shares of the result of the operation
as output to the parties. A number of diﬀerent protocol sets exist, including the
GMW protocol [16], protocols for data shared using Shamir’s secret sharing [31,
15], Sharemind’s protocol set [5] or protocols based on predistributed correlated
randomness [2, 10]. The protocols are composable, meaning that they can be
combined to solve large computational tasks in privacy-preserving manner. Also,
they are input private, meaning that no party or a tolerated coalition of parties
learns anything new during these protocols, except for the ﬁnal output.
The framework underlying our implementation is Sharemind, which pro-
vides protocols for three parties, and is secure against a passive adversary that
corrupts at most one party. Compared to other frameworks, it oﬀers proto-
cols for a large set of operations over integers, ﬁxed- and ﬂoating-point values,
thereby simplifying our implementations and comparisons. The oﬀered proto-
cols are eﬃcient compared to other frameworks, but performing computations
on secret-shared private data is still considerably slower than performing the
same computations on public data. The diﬀerence is especially large for ﬂoating-
point operations. Thus it is often better to convert private ﬂoating-point data
to ﬁxed-point data, which can be simulated using integers, and is much faster.
When implementing this, we must be careful to avoid overﬂows. This is espe-
cially important for diﬀerentially private computations, because an overﬂow can
change the result so much that no reasonable amount of added noise can mask
this. Similarly, we must avoid exceptions, e.g. division by zero, since these can-
not be masked by noise. Instead, it is necessary to remove the exceptional or
overﬂown values or replace them with default values. This will change the result
by a very small amount, because we use -diﬀerentially private algorithms.
Even if we can perform most operations using ﬁxed-point arithmetic, we may
still perform a constant number of ﬂoating-point operations, e.g. the division
when computing an average or the generation of a Laplace random value (which
uses one logarithm operation and a uniform random value).
Because protocols on secret-shared data have a much better performance
when a single operation is applied to a large number of values in parallel rather
than sequentially (due to the network latency), we may have to structure our
diﬀerentially private algorithms diﬀerently than in the non-secret-shared case.
In the Sample-and-Aggregate algorithm (Alg. 1), the set of n inputs is randomly
partitioned into (cid:96) samples, the black box computing f is applied to each of
the samples, and the results are combined. It is easy to run the black box on
each sample sequentially, but in the secret-shared context we may need to run
many copies of the black box in parallel, which would complicate the realization.
The sequential algorithm would also work but it would be slower, especially for
small values of n/(cid:96). For large n/(cid:96), each sample would be large enough to fully
take advantage of the parallelizability of vector operations, and the diﬀerence in
performance would diminish.
When implementing an algorithm in privacy-preserving manner, it is gener-
ally not possible to branch on a private condition, because the control ﬂow of the
algorithm is visible to all parties. Instead, we must use evaluate both branches
and combine them using oblivious choice. If b is private then if b then c else d
must be replaced with b · c + (1 − b) · d, where the boolean b is used as an in-
teger (true = 1, false = 0). This can be further optimized to d + b · (c − d),
which uses only one multiplication instead of two. Three-way oblivious choice
if b1 then c else if b2 then d else e where b1 and b2 are never true at the same
time can then be implemented using two multiplications: e+b1·(c−e)+b2·(d−e).
The three-way oblivious choice is used to implement the computation of each Oi
in Alg. 1.
To get better performance in the secret-shared setting, the (cid:96) invocations of
the black box in Alg. 1 are done in parallel in single invocation. The input
of the black box in this case is a list of datasets and the output is a list of
values of f for each dataset. For example, in diﬀerentially private computation
of the linear correlation coeﬃcient, the black box takes any number of datasets
and computes the non-diﬀerentially private linear correlation coeﬃcient of each
dataset in parallel. For computing other functions diﬀerentially privately, we
need to replace only the black box.
In privacy-preserving statistics applications, before applying an aggregating
function, the dataset is usually ﬁltered by some predicate [3, Sec 3]. In a non-
secret-shared setting, the trusted party can then create a new dataset containing
only those rows that matched the predicate and apply the aggregating function
to this new dataset. In the secret-shared setting, we cannot create a new dataset
this way because it would leak the number of rows that matched the predicate.
Instead, we must use a mask vector, which contains for each row a boolean
that speciﬁes whether this row matched the predicate (and therefore should be
used in aggregation) or not. Therefore, all aggregating functions (including the
non-diﬀerentially private ones used as black boxes in the Sample-and-Aggregate
algorithm) receive this mask vector in addition to the dataset and must aggregate
only the subset of the dataset denoted by the mask vector.
In most cases, it is not diﬃcult to modify the aggregating function to use
a mask vector, but in some cases, there can be complications. For example, for
computing the median, we replace half of the values excluded by the ﬁlter with
a very small value, and the other half with a very large value. This will keep the
median roughly (exactly if the number of excluded values is even) the same.
When using mask vectors, ﬁltering will not reduce the size of a table. To
improve performance, we can reduce the size of the ﬁltered table using a cut
operation. This requires an upper bound k on the number of records. The re-
sulting table will have exactly k records (some of which may still be disabled by
the mask vector). If there were more than k records, then some elements will
be thrown away (uniformly randomly). This distorts the result of the analysis
similarly to the clip operation described above, thus the upper bound must be
chosen carefully.
The data used in the analysis is in a secret-shared database. Because the
data comes from diﬀerent providers, it will be in diﬀerent tables. For making
the more complex queries, we may need a database join operation to combine
two tables. This can be done on secret-shared data in O(n log n) time (where
n is the total number of records in the tables) provided that only one of the
two joined tables may have non-unique values in the column used for joining
(actually, the other may also contain non-unique values but in this case, for each
set of rows with the same key, only one row, chosen uniformly randomly, will be
used in the join; this may be acceptable for some applications).
In order to make a query in the system, one may have to provide a range
[a, b] (for clipping initial or intermediate values), a number k (for cutting the
number of records in an intermediate table), and a value . These parameters
represent the tradeoﬀ between privacy, accuracy, and performance. If we do not
know enough about the private data then we can make some preliminary queries
to obtain rough estimates for the parameters. These queries should use as small
 as possible, to avoid excessive consumption of the privacy budget.
7 Asymptotic Overhead of Diﬀerential Privacy
Adding diﬀerential privacy to a secret-shared aggregation introduces some over-
head. In Alg. 2, we give an algorithm for computing (non-diﬀerentially privately)
in parallel the correlation coeﬃcients of (cid:96) datasets. It implements the following
formula:
ci =
(cid:80)
(cid:113)
((cid:80)
j xijyij
ij)((cid:80)
j x2
j y2
ij)
where the values have been normalized by subtracting the corresponding row
averages and the values excluded by the mask vector have been replaced with
zeros.
Algorithm 2 Parallelized masked correlation algorithm
Input: Dataset matrices X and Y (ﬁxed-point numbers) and mask matrix M (each (cid:96)
rows by k columns), number of blocks (cid:96), number of elements in each block k.
Output: For each row i ∈ {1, . . . , (cid:96)}, the correlation ci of the ith row of X with the
ith row of Y .
for each i ∈ {1, . . . , (cid:96)} (in parallel) do
j Mij
ri
i ← 1
r(cid:48)
si ← r(cid:48)
ti ← r(cid:48)
for each j ∈ {1, . . . , k} (in parallel) do
j XijMij
j YijMij
xij ← (Xij − si) · Mij
yij ← (Yij − ti) · Mij
ri ←(cid:80)
i ·(cid:80)
i ·(cid:80)
ai ←(cid:80)
bi ←(cid:80)
di ←(cid:80)
ci ← ai√
bidi
j xijyij
j x2
j y2
ij
ij
return c
In Alg. 2, we ﬁrst compute ri, the number of elements in each row i. This
is a local operation, so parallelization is not required. Then we compute the
inverses r(cid:48)
i, which are used to compute si and ti, the row averages of X and Y .
We do one inverse and two multiplications for each row, which is faster than
doing two divisions. When computing the si, we can do all (cid:96) · k multiplications
XijMij in parallel. The next ﬁve sets of (cid:96)· k multiplications each (for computing
xij, yij, ai, bi, di) are handled in the same way. Finally, we compute the ci by
doing (cid:96) multiplications, divisions, and square roots. Divisions and square roots
are expensive operations, so it is important to do them in parallel, even though
we only do (cid:96) of each, not (cid:96) · k. If k is small then the divisions and square roots
dominate the computation time. If k gets larger then the O((cid:96)· k) multiplications
begin to dominate.
If we compare the computation of the correlation of (cid:96) samples in parallel to
the computation of the correlation of the whole dataset as a single sample then
we see that the number of multiplications is almost the same (7(cid:96)k + (cid:96) vs 7(cid:96)k + 1).
The number of divisions increases from 2 to 2(cid:96) and that of square roots from 1
to (cid:96).
When computing correlation diﬀerentially privately, we use Alg. 1 with Alg. 2
as a subroutine that is called only once. In addition to the operations done in the
subroutine, the algorithm in Alg. 1 does 2(cid:96) comparisons, 2(cid:96) multiplications, 1 di-
vision (the operations in the argument of Laplace are public), and one generation
of a Laplace random value (which uses one division and one logarithm).
We keep our data (the matrices X and Y in Alg. 2) in the database in ﬁxed-
point form. The multiplications in Alg. 1 and Alg. 2 are integer multiplications
but we need to do 2(cid:96)k + 2(cid:96) (or 2(cid:96)k + 2 in the non-diﬀerentially private case) shift
rights to avoid overﬂow.
In addition, we need to convert ri,(cid:80)
and(cid:80)
j YijMij, ai, bi, di in Alg. 2
i Oi in Alg. 1 from integer or ﬁxed-point form to ﬂoating point for a total
of 6(cid:96) + 1 conversions. We also need to do 3(cid:96) conversions to convert si, ti, and ci
in Alg. 2 from ﬂoating point to integer.
j XijMij,(cid:80)
We summarize the number of (non-local) operations in both cases:
diﬀ. private
7(cid:96)k + 3(cid:96)
2(cid:96)k + 2(cid:96)
2(cid:96)
6(cid:96) + 1
3(cid:96)
2(cid:96) + 2
(cid:96)
2(cid:96)
1
operation
int multiplication
shift right
ﬂoat multiplication
int to ﬂoat
ﬂoat to int
division
square root
comparison
logarithm
non-diﬀ. private
7(cid:96)k + 1
2(cid:96)k + 2
2
6
2
2
1
0
0
As we see, for large k, the multiplications and shift rights dominate the
running time, and if k → ∞ then the ratios 7(cid:96)k+3(cid:96)
overhead of diﬀerential privacy is negligible for large block size k. For small k,
the O((cid:96)) overhead may be important.
7(cid:96)k+1 → 1 and 2(cid:96)k+2(cid:96)
2(cid:96)k+2 → 1, i.e. the
8 Algorithm for Join
Sometimes the values needed for performing a query are in more than one table
(e.g. in Sec. 9). Then we need to join those tables. In this section, we describe
an algorithm (Alg. 3) for this. In the following, we call the columns by which
the tables are joined, the provenance columns but actually any columns can be
used in this role.
Suppose we have matrices V (with rV rows and cV columns) and B (with
rB rows and cB columns). Also assume that diﬀerent rows in B have diﬀerent
provenances, and all provenances in V also occur in B. This assumption holds