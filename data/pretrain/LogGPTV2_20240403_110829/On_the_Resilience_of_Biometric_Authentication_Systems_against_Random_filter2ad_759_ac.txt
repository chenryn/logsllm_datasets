F
n = 2
0
Bins
1
Fig. 3. The binned version IB of the unit interval I. Each bin is of width
1/B (B not speciﬁed). The number of ﬁlled bins is α = 3, with a cut-off of
n = 2.
For the ith feature, let α+
i denote the number of bins ﬁlled
by all positive examples in D. We deﬁne:
n(cid:89)
i=1
R+ :=
1
Bn
α+
i
as the volume of the true positive region. We deﬁne α−
i and
R− analogously as the volume of the true negative region.
Let c ∈ [0, 1] be a constant. If each of the α+
i ’s is at most
cB, then we see that R+ = c−n. For instance, if c ≤ 1/2,
then R+ ≤ 2−n. In other words, the volume of the region
spanned by the user’s own samples is exponentially small as
compared to the volume of the unit cube. In practice, the user’s
data is expected to be normally distributed across each feature,
implying that the α+
i ’s are much smaller than B/2, which
makes the above volume a loose upper bound. The same is
true of the α−
i ’s. Figure 4 shows the ﬁlled bins from one of the
features in the Face Dataset (see Section IV-A). For the same
dataset, the average true positive region is 5.781×10−98 (with
a standard deviation of ±2.074 × 10−96) and the average true
negative region is 1.302× 10−55 (with a standard deviation of
±2.172× 10−54) computed over 10,000 iterations considering
a 80% sample of the target user’s data, and a balanced sample
of other users.7
We thus expect a random vector from In to be outside
the region spanned by the target user with overwhelming
probability. Thus, if a classiﬁer deﬁnes an acceptance region
tightly surrounding the target user’s data, the volume of the
acceptance region will be negligible, and hence the random
input attack will not be a threat. However, as we shall show
in the next sections, this is not the case in practice.
Factors Effecting Acceptance Region. We list a few factors
which effect the volume of the acceptance region.
• One reason for a high acceptance region is that the classiﬁer
is not penalized for classifying empty space in the feature
space as either positive or negative. For instance, consider
Figure 4. There is signiﬁcant empty space for the feature
depicted in the ﬁgure: none of the positive or negative
samples have the projected feature value in this space. A
classiﬁer is generally trained with an objective to minimize
the misclassiﬁcation rate or a loss function (where, for
instance, there is an asymmetrical penalty between true
positives and false positives) [35]. These functions take
input from the dataset D. Thus, empty regions in the feature
space which do not have examples in D can be classiﬁed
as either of the two classes without being penalized during
training, resulting in a non-negligible acceptance region.
• The acceptance region is also expected to be big if there
is high variance in the feature values taken by the positive
examples. In this case, the α+
i ’s will be much closer to B,
resulting in a non-negligible volume R+.
• On the other hand, the acceptance region is likely to be
small if the variances of the feature values in the negative
examples are high. The classiﬁer, in order to minimize
the FPR, will then increase the region where samples are
rejected, which would in turn make the acceptance region
closer in volume to the true positive region.
We empirically verify these observations in Section V. The
last observation also hints at a possible method to tighten the
acceptance region around the region spanned by the target user:
generate random noise around the target user’s vectors and
treat it as belonging to the negative class. We demonstrate the
effectiveness of this method in Section VI. Jumping ahead, if
the noise is generated from an appropriate distribution, this
will have minimal impact on the FRR and FPR of the model.
7We compute the true positive and negative region by only considering the
minimum and maximum feature values covered by each user for each feature
with binning equal to the ﬂoating point precision of the system. Thus, this is
a conservative estimate of the true positive region.
5
3) Face Dataset: FaceNet [3] proposes a system based on
neural networks that can effectively learn embeddings (feature
vectors) that represent uniquely identiﬁable facial information
from images. Unlike engineered features, these embeddings
may not be directly explainable as they are automatically
extracted by the underlying neural network. This neural net-
work can be trained from any dataset containing labeled faces
of individuals. There are many sources from which we can
obtain face datasets, CASIA-WebFace [2], VGGFace2 [39] and
Labeled Faces in the Wild (LFW) [40] are examples of such
datasets. However, with a pre-trained model, we can conserve
the time and resources required to re-train the network. The
source code for FaceNet [41] contains two pretrained models
available for public use (at the time of writing): one trained
on CASIA-WebFace, and another trained on VGGFace2. We
opt to use a model pre-trained on VGGFace28 , while re-
taining CASIA-WebFace as our dataset for classiﬁer training.
We choose to use different datasets for the training of the
embeddings and the classiﬁers to simulate the exposure of the
model to never before seen data. Our face dataset is a subset of
CASIA-WebFace containing only the top 100 identities with
the largest number of face images (producing 447±103 images
per individual). This model produces 512 latent features from
input images of pixel size 160x160 which have been centered
and aligned. Recall that face alignment involves ﬁnding a
bounding box on the face on an image, before cropping and
resizing to the requested dimensions.
4) Speaker Veriﬁcation (Utterances): VoxCeleb [4], and
VoxCeleb2 [5] are corpuses of spoken recordings by celebrities
in online media. These recordings are text-independent, i.e.,
the phrase uttered by the user is not pre-determined. Text-
independent speaker veriﬁcation schemes depart from text-
dependent veriﬁcation schemes in which the individual
is
bound to repeat a pre-determined speech content. Thus, the
task of text-independent veriﬁcation (or identiﬁcation) is to
distinguish how the user speaks as an individual, instead of
how the user utters a speciﬁc phrase. The former objective
is an arguably harder task. Despite the increased difﬁculty,
researchers have trained neural networks to convert speaker
utterances into a set of latent features representing how indi-
viduals speak. These works have also released their models
to the public, increasing the accessibility of speaker veriﬁ-
cation to developers. We opt to use the pre-trained model
of VoxCeleb [4], with utterances from VoxCeleb2 [5]. From
VoxCeleb2, we only use the test portion of the dataset, which
contains 118 Users with an average of 406 ± 87 utterances.
VoxCeleb was trained as a Siamese neural network [42] for
one-shot comparison between two audio samples. A Siamese
network consists of two identical branches that produce two
equal size outputs from two independent inputs for distance
comparison. To ﬁt the pre-trained model into our evaluation
of ML-based models, we extract embeddings from one of the
twin networks and disregard the second branch. The 1024-
length embedding is then used as the feature vector within our
evaluation.
B. Evaluation Methodology
In our creation of biometric models for each user, we seek
to obtain the baseline performance of the model with respect
8(20180402-114759) is the identiﬁer of pre-trained model used.
Fig. 4. The histogram of feature values of one of the features in the Face
Dataset (cf. § IV-A). Here we have B = 100. The number of ﬁlled bins for
the target user is α+
i = 35 (with 400 samples), and for the negative class (10
users; same number of total samples) it is α−
i = 50. A total of 24 bins are
not ﬁlled by any of the two classes, implying that (approximately) 0.24 of
the region for this feature is empty.
IV. EVALUATION ON BIOMETRIC SYSTEMS
To evaluate the issue of acceptance region on real-world
biometric systems, we chose four different modalities: gait,
touch, face and voice. The last two modalities are used as
examples of user authentication at the point of entry into a
secured system, whilst gait and touch are often used in con-
tinuous authentication systems [37]. We ﬁrst describe the four
biometric datasets, followed by our evaluation methodology,
the machine learning algorithms used, and ﬁnally our results
and observations.
A. The Biometric Datasets
1) Activity Type (Gait) Dataset: The activity type dataset
[38], which we will refer to as the “gait” dataset, was col-
lected for human activity recognition. Speciﬁcally its aim
is to provide a dataset for determining if a user is sitting,
laying down, walking, running, walking upstairs or downstairs,
etc. However, as the dataset retains the unique identiﬁers
for users per biometric record, we re-purpose the dataset for
authentication. This dataset contains 30 users, with an average
of 343 ± 35 (mean ± SD) biometric samples per user, there is
an equal number of activity type samples for each user. For
the purpose of authentication, we do not isolate a speciﬁc type
of activity. Instead, we include them as values of an additional
feature. The activity type feature increases the total number of
features to 562. We will refer to these features as engineered
features as they are manually deﬁned (e.g., by an expert) as
opposed to latent features extracted from a pre-trained neural
network for the face and voice datasets.
2) Touch Dataset: The UMDAA-02 Touch Dataset [6] is
a challenge dataset to provide data for researchers to perform
baseline evaluations of new touch-based authentication sys-
tems. Data was collected from 35 users, with an average of
3667 ± 3012 swipes per user. This dataset was collected by
lending mobile devices to the participants over a prolonged
period of time. The uncontrolled nature of the collection pro-
duces a dataset that accurately reﬂects swipe interactions with
constant and regular use of the device. This dataset contains
every touch interaction performed by the user including taps.
In a pre-processing step we only consider sequences with more
than 5 data points as swipes. Additionally, we set four binary
features to indicate the direction of the swipe, determined from
the dominant vertical and horizontal displacement. We retained
all other features in [6] bar inter-stroke time, as we wished to
treat each swipe independently, without chronological order.
We substitute this feature with half-time of the stroke. This
produces a total of 27 engineered touch features.
6
0.00.20.40.60.81.0Value Bins01020Frequencyn=2Target UserOther Usersto the ability of negative user samples gaining access (i.e.
FPR), and the measured Acceptance Region (AR). We use
the following methodology to evaluate these metrics for each
dataset and each classiﬁcation algorithm.
1) We min-max normalize each extracted feature over the
entire dataset between 0 and 1.
2) We partition the dataset into a (70%, 30%) split for training
and testing sets, respectively.
3) For both training and testing samples, we further sample
an equal number of negative samples from every other
user such that the total number of negative samples are
approximately equal to the number of samples from the
target user, representing the positive class, i.e., the positive
and negative classes are balanced.
4) Using the balanced training set from step 3, we train a two-
class classiﬁer deﬁning the target user set as the positive
class, and all remaining users as negative.
5) We test the trained model using the balanced testing set
from step 3. This establishes the FRR and FPR of the
system.
6) We uniformly sample one million vectors from In, where
n is the dimension of the extracted features. Testing the
set of vectors against the model measures the acceptance
region (AR).
7) We record the conﬁdence values of the test prediction
for the user’s positive test samples, other users’ negative
test samples, and the uniformly sampled vectors. These
conﬁdence values produce ROC curves for FRR, FPR and
AR.
8) Repeat steps 3-7 by iterating through every user in the
dataset as the target user.
Remark 4.1: In general, the decision regions (accept and
reject in the case of authentication) learned by the classiﬁers
can be quite complex [43]. Hence, it is difﬁcult to determine
them analytically, despite the availability of learned model
parameters. We instead use a Monte Carlo method by sampling
random feature vectors from In where each feature value is
sampled uniformly at random from I. With enough samples
(one million used in our experiments, and averaged over 50
repetitions), the fraction of random samples accepted by the
classiﬁer serves as an estimate of the acceptance region as
deﬁned by Eq. 2 due to the law of large numbers.
Remark 4.2: Our evaluation of the biometric systems is
using the mock attacker model (samples from the negative
class modelled as belonging to an attacker) as it is commonly
used [44]. We acknowledge that there are other attack models
such as excluding the data of the attacker from the training
set [44]. Having the attacker data included in the training