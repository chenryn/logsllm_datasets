    import argparse
    random.seed(444)
    parser = argparse.ArgumentParser()
    parser.add_argument("-p", "--nprod", type=int, default=5)
    parser.add_argument("-c", "--ncon", type=int, default=10)
    ns = parser.parse_args()
    start = time.perf_counter()
    asyncio.run(main(**ns.__dict__))
    elapsed = time.perf_counter() - start
    print(f"Program completed in {elapsed:0.5f} seconds.")
```
The challenging part of this workflow is that there needs to be a signal to the consumers that production is done. Otherwise, `await q.get()` will hang indefinitely, because the queue will have been fully processed, but consumers won’t have any idea that production is complete. The key is to `await q.join()`, which blocks until all items in the queue have been received and processed, and then to cancel the consumer tasks, which would otherwise hang up and wait endlessly for additional queue items to appear.
The first few coroutines are helper functions that return a random string, a fractional-second performance counter, and a random integer. A producer puts anywhere from 1 to 10 items into the queue. Each item is a tuple of `(i, t)` where `i` is a random string and `t` is the time at which the producer attempts to put the tuple into the queue.
When a consumer pulls an item out, it simply calculates the elapsed time that the item sat in the queue using the timestamp that the item was put in with.
Here is a test run with two producers and five consumers:
```bash
$ python3 asyncq.py -p 2 -c 5
Producer 0 sleeping for 3 seconds.
Producer 1 sleeping for 3 seconds.
Consumer 0 sleeping for 4 seconds.
Consumer 1 sleeping for 3 seconds.
Consumer 2 sleeping for 3 seconds.
Consumer 3 sleeping for 5 seconds.
Consumer 4 sleeping for 4 seconds.
Producer 0 added  to queue.
Producer 0 sleeping for 5 seconds.
Producer 1 added  to queue.
Consumer 1 got element  in 0.00013 seconds.
Consumer 1 sleeping for 3 seconds.
Consumer 2 got element  in 0.00009 seconds.
Consumer 2 sleeping for 4 seconds.
Producer 0 added  to queue.
Producer 0 sleeping for 1 seconds.
Consumer 0 got element  in 0.00021 seconds.
Consumer 0 sleeping for 4 seconds.
Producer 0 added  to queue.
Consumer 4 got element  in 0.00022 seconds.
Consumer 4 sleeping for 5 seconds.
Program completed in 9.00954 seconds.
```
In this case, the items process in fractions of a second. A delay can be due to two reasons:
* Standard, largely unavoidable overhead
* Situations where all consumers are sleeping when an item appears in the queue
With regards to the second reason, luckily, it is perfectly normal to scale to hundreds or thousands of consumers. You should have no problem with `python3 asyncq.py -p 5 -c 100`. The point here is that, theoretically, you could have different users on different systems controlling the management of producers and consumers, with the queue serving as the central throughput.
## [`async for` and list comprehensions](https://realpython.com/async-io-python/#other-features-async-for-and-async-generators-comprehensions)
You can use `async for` to iterate over an asynchronous iterator. The purpose of an asynchronous iterator is for it to be able to call asynchronous code at each stage when it is iterated over.
A natural extension of this concept is an asynchronous generator:
```python
>>> async def mygen(u: int = 10):
...     """Yield powers of 2."""
...     i = 0
...     while i >> async def main():
...     # This does *not* introduce concurrent execution
...     # It is meant to show syntax only
...     g = [i async for i in mygen()]
...     f = [j async for j in mygen() if not (j // 3 % 5)]
...     return g, f
...
>>> g, f = asyncio.run(main())
>>> g
[1, 2, 4, 8, 16, 32, 64, 128, 256, 512]
>>> f
[1, 2, 16, 32, 256, 512]
```
Asynchronous iterators and asynchronous generators are not designed to concurrently map some function over a sequence or iterator. They’re merely designed to let the enclosing coroutine allow other tasks to take their turn. The `async for` and `async with` statements are only needed to the extent that using plain `for` or `with` would “break” the nature of `await` in the coroutine. 
## [The Event Loop and asyncio.run()](https://realpython.com/async-io-python/#the-event-loop-and-asynciorun)
You can think of an event loop as something like a `while True` loop that monitors coroutines, taking feedback on what’s idle, and looking around for things that can be executed in the meantime. It is able to wake up an idle coroutine when whatever that coroutine is waiting on becomes available.
Thus far, the entire management of the event loop has been implicitly handled by one function call:
```python
asyncio.run(main())
```
`asyncio.run()` is responsible for getting the event loop, running tasks until they are marked as complete, and then closing the event loop.
If you do need to interact with the event loop within a Python program, `loop` (obtained through `loop = asyncio.get_event_loop()`) is a good-old-fashioned Python object that supports introspection with `loop.is_running()` and `loop.is_closed()`. You can manipulate it if you need to get more fine-tuned control, such as in scheduling a callback by passing the loop as an argument.
Some important points regarding the event loop are:
- Coroutines don’t do much on their own until they are tied to the event loop. If you have a main coroutine that awaits others, simply calling it in isolation has little effect:
    ```python
    >>> import asyncio
    >>> async def main():
    ...     print("Hello ...")
    ...     await asyncio.sleep(1)
    ...     print("World!")
    >>> routine = main()
    >>> routine
    ```
    Remember to use `asyncio.run()` to actually force execution by scheduling the `main()` coroutine (future object) for execution on the event loop:
    ```python
    >>> asyncio.run(routine)
    Hello ...
    World!
    ```
- By default, an async IO event loop runs in a single thread and on a single CPU core. It is also possible to run event loops across multiple cores. Check out [this talk by John Reese](https://youtu.be/0kXaLh8Fz3k?t=10m30s) for more, and be warned that your laptop may spontaneously combust.
## [Creating and gathering tasks](https://realpython.com/async-io-python/#other-top-level-asyncio-functions)
You can use `create_task()` to schedule the execution of a coroutine object, followed by `asyncio.run()`:
```python
>>> import asyncio
>>> async def coro(seq) -> list:
...     """'IO' wait time is proportional to the max element."""
...     await asyncio.sleep(max(seq))
...     return list(reversed(seq))
...
>>> async def main():
...     # This is a bit redundant in the case of one task
...     # We could use `await coro([3, 2, 1])` on its own
...     t = asyncio.create_task(coro([3, 2, 1])) 
...     await t
...     print(f't: type {type(t)}')
...     print(f't done: {t.done()}')
...
>>> t = asyncio.run(main())
t: type 
t done: True
```
There’s a subtlety to this pattern: if you don’t `await t` within `main()`, it may finish before `main()` itself signals that it is complete. Because `asyncio.run(main())` calls `loop.run_until_complete(main())`, the event loop is only concerned (without `await t` present) that `main()` is done, not that the tasks that get created within `main()` are done, if this happens the loop’s other tasks will be cancelled, possibly before they are completed. If you need to get a list of currently pending tasks, you can use `asyncio.Task.all_tasks()`.
Separately, there’s `asyncio.gather()` which is meant to neatly put a collection of coroutines (futures) into a single future. As a result, it returns a single future object, and, if you `await asyncio.gather()` and specify multiple tasks or coroutines, you’re waiting for all of them to be completed. (This somewhat parallels `queue.join()` from our earlier example.) The result of `gather()` will be a list of the results across the inputs:
```python
>>> import time
>>> async def main():
...     t = asyncio.create_task(coro([3, 2, 1]))
...     t2 = asyncio.create_task(coro([10, 5, 0]))  # Python 3.7+
...     print('Start:', time.strftime('%X'))
...     a = await asyncio.gather(t, t2)
...     print('End:', time.strftime('%X'))  # Should be 10 seconds
...     print(f'Both tasks done: {all((t.done(), t2.done()))}')
...     return a
...
>>> a = asyncio.run(main())
Start: 16:20:11
End: 16:20:21
Both tasks done: True
>>> a
[[1, 2, 3], [0, 5, 10]]
```
You can loop over `asyncio.as_completed()` to get tasks as they are completed, in the order of completion. The function returns an iterator that yields tasks as they finish. Below, the result of `coro([3, 2, 1])` will be available before `coro([10, 5, 0])` is complete, which is not the case with `gather()`:
```python
>>> async def main():
...     t = asyncio.create_task(coro([3, 2, 1]))
...     t2 = asyncio.create_task(coro([10, 5, 0]))
...     print('Start:', time.strftime('%X'))
...     for res in asyncio.as_completed((t, t2)):
...         compl = await res
...         print(f'res: {compl} completed at {time.strftime("%X")}')
...     print('End:', time.strftime('%X'))
...     print(f'Both tasks done: {all((t.done(), t2.done()))}')
...
>>> a = asyncio.run(main())
Start: 09:49:07
res: [1, 2, 3] completed at 09:49:10
res: [0, 5, 10] completed at 09:49:17
End: 09:49:17
Both tasks done: True
```
Lastly, you may also see `asyncio.ensure_future()`. You should rarely need it, because it’s a lower-level plumbing API and largely replaced by `create_task()`, which was introduced later.
## [When and Why Is Async IO the Right Choice?](https://realpython.com/async-io-python/#when-and-why-is-async-io-the-right-choice)
If you have multiple, fairly uniform CPU-bound tasks (a great example is a grid search in libraries such as scikit-learn or keras), multiprocessing should be an obvious choice.
Simply putting `async` before every function is a bad idea if all of the functions use blocking calls. This can actually slow down your code.
The contest between async IO and threading is a little bit more direct. Even in cases where threading seems easy to implement, it can still lead to infamous impossible-to-trace bugs due to race conditions and memory usage, among other things.
Threading also tends to scale less elegantly than async IO, because threads are a system resource with a finite availability. Creating thousands of threads will fail on many machines. Creating thousands of async IO tasks is completely feasible.
Async IO shines when you have multiple IO-bound tasks where the tasks would otherwise be dominated by blocking IO-bound wait time, such as:
- Network IO, whether your program is the server or the client side
- Serverless designs, such as a peer-to-peer, multi-user network like a group chatroom
- Read/write operations where you want to mimic a “fire-and-forget” style but worry less about holding a lock on whatever you’re reading and writing to
The biggest reason not to use it is that `await` only supports a specific set of objects that define a specific set of methods. If you want to do async read operations with a certain DBMS, you’ll need to find not just a Python wrapper for that DBMS, but one that supports the async/await syntax. Coroutines that contain synchronous calls block other coroutines and tasks from running.
## [Async IO It Is, but Which One?](https://realpython.com/async-io-python/#async-io-it-is-but-which-one)
`asyncio` certainly isn’t the only async IO library out there. The most popular are:
- [`anyio`](https://anyio.readthedocs.io/en/stable/)
- [`curio`](https://github.com/dabeaz/curio)
- [`trio`](https://github.com/python-trio/trio)
You might find that they get the same thing done in a way that’s more intuitive for you as the user. Many of the package-agnostic concepts presented here should permeate to alternative async IO packages as well. But if you’re building a moderately sized, straightforward program, just using `asyncio` is plenty sufficient and understandable, and lets you avoid adding yet another large dependency outside of Python’s standard library.
# Snippets
## Write on file
```python
import aiofiles
async with aiofiles.open(file, "a") as f:
    for p in res:
        await f.write(f"{url}\t{p}\n")
```
## Do http requests
Use the [`aiohttp`](aiohttp.md) library
# Tips
## [Limit concurrency](https://m0wer.github.io/memento/computer_science/programming/python/asyncio/#limit-concurrency)
Use [`asyncio.Semaphore`](https://docs.python.org/3/library/asyncio-sync.html#semaphores).
```python
sem = asyncio.Semaphore(10)
async with sem:
    # work with shared resource
```
Note that this method is not thread-safe.
# [Testing](https://pytest-asyncio.readthedocs.io/en/latest/)
With the [`pytest-asyncio`](https://pytest-asyncio.readthedocs.io/en/latest/) plugin you can test code that uses the asyncio library.
Install it with `pip install pytest-asyncio`
Specifically, `pytest-asyncio` provides support for coroutines as test functions. This allows users to await code inside their tests. For example, the following code is executed as a test item by pytest:
```python
@pytest.mark.asyncio
async def test_some_asyncio_code():
    res = await library.do_something()
    assert b"expected result" == res
```
`pytest-asyncio` runs each test item in its own `asyncio` event loop. The loop can be accessed via the `event_loop` fixture, which is automatically requested by all async tests.
```python
async def test_provided_loop_is_running_loop(event_loop):
    assert event_loop is asyncio.get_running_loop()
```
You can think of `event_loop` as an `autouse` fixture for async tests.
It has [two discovery modes](https://pytest-asyncio.readthedocs.io/en/latest/concepts.html#test-discovery-modes):
- Strict mode: will only run tests that have the `asyncio` marker and will only evaluate async fixtures decorated with `@pytest_asyncio.fixture`
- Auto mode: will automatically add the `asyncio` marker to all asynchronous test functions. It will also take ownership of all async fixtures, regardless of whether they are decorated with `@pytest.fixture` or `@pytest_asyncio.fixture`. 
    This mode is intended for projects that use `asyncio` as their only asynchronous programming library. Auto mode makes for the simplest test and fixture configuration and is the recommended default.
To use the Auto mode you need to pass the [`--asyncio-mode=auto`](https://pytest-asyncio.readthedocs.io/en/latest/reference/configuration.html) flag to `pytest`. If you use `pyproject.toml` you can set the next configuration:
```toml
[tool.pytest.ini_options]
addopts = "--asyncio-mode=auto"
```
# References
* [Docs](https://docs.python.org/3/library/asyncio.html#module-asyncio)
* [Awesome Asyncio](https://github.com/timofurrer/awesome-asyncio)
* [Real python tutorial](https://realpython.com/async-io-python/)
* [Roguelynn tutorial](https://www.roguelynn.com/words/asyncio-we-did-it-wrong/)
## Libraries to explore
* [Asyncer](https://github.com/tiangolo/asyncer)