title:Challenges, design and analysis of a large-scale p2p-vod system
author:Yan Huang and
Tom Z. J. Fu and
Dah-Ming Chiu and
John C. S. Lui and
Cheng Huang
Challenges, Design and Analysis of a Large-scale P2P-VoD
System
Yan Huang∗, Tom Z. J. Fu†, Dah-Ming Chiu†, John C. S. Lui‡ and Cheng Huang∗
∗{galehuang, ivanhuang}@pplive.com, Shanghai Synacast Media Tech.
†{zjfu6, dmchiu}@ie.cuhk.edu.hk, The Chinese University of Hong Kong
‡PI:EMAIL, The Chinese University of Hong Kong
ABSTRACT
P2P ﬁle downloading and streaming have already become
very popular Internet applications. These systems dramat-
ically reduce the server loading, and provide a platform for
scalable content distribution, as long as there is interest for
the content. P2P-based video-on-demand (P2P-VoD) is a
new challenge for the P2P technology. Unlike streaming live
content, P2P-VoD has less synchrony in the users sharing
video content, therefore it is much more diﬃcult to allevi-
ate the server loading and at the same time maintaining
the streaming performance. To compensate, a small storage
is contributed by every peer, and new mechanisms for co-
ordinating content replication, content discovery, and peer
scheduling are carefully designed. In this paper, we describe
and discuss the challenges and the architectural design issues
of a large-scale P2P-VoD system based on the experiences
of a real system deployed by PPLive. The system is also
designed and instrumented with monitoring capability to
measure both system and component speciﬁc performance
metrics (for design improvements) as well as user satisfac-
tion. After analyzing a large amount of collected data, we
present a number of results on user behavior, various system
performance metrics, including user satisfaction, and discuss
what we observe based on the system design. The study of
a real life system provides valuable insights for the future
development of P2P-VoD technology.
Categories and Subject Descriptors
C.2 [Computer-Communication Networks]: Distributed
Systems
General Terms
Design, Measurement, Performance
Keywords
Peer-to-Peer/Overlay Networks, Video-on-Demand, Content
Distribution
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’08, August 17–22, 2008, Seattle, Washington, USA.
Copyright 2008 ACM 978-1-60558-175-0/08/08 ...$5.00.
1.
INTRODUCTION AND CONTRIBUTION
The eﬀectiveness of using the P2P approach for content
distribution has been proven by many deployed systems [1,
2, 3, 4, 9, 21]. These P2P systems oﬀer many diﬀerent ser-
vices. One type of service is P2P ﬁle downloading, for exam-
ple implemented by BitTorrent [9] and Emule [1]. When a
ﬁle is downloaded by many users, these users help each other
so that the server load is signiﬁcantly reduced. The peers
may experience diﬀerent downloading rates, often depend-
ing on how much they are able to contribute to the process.
Another type of service is P2P live streaming (for example
implemented by a university project Coolstreaming [21] and
many commercial systems such as PPLive [2]). When a live
video is watched by many users, again these users can help
each other to alleviate the load on the server. In this case,
the new challenge to system design is to ensure all peers can
receive the streamed video at the playback rate.
More recently, the interest has turned towards a new kind
of service, P2P video-on-demand (P2P-VoD). Based on a
detailed analysis of a current client-server VoD system at
Microsoft, it was pointed out in [17] that P2P-VoD could
bring signiﬁcant savings in server loading. Apparently this
conclusion was already shared among P2P-VoD developers,
since a number of P2P-VoD systems were deployed at about
the same time as the publication of [17]. These P2P-VoD
systems are already enjoying a large viewer population [3,
4, 5, 6, 7]. Like P2P streaming systems, these P2P-VoD
systems also deliver the content by streaming, but peers can
watch diﬀerent parts of a video at the same time, hence di-
luting their ability to help each other and oﬄoad the server.
To compensate, this new genre of P2P systems requires each
user to contribute a small amount of storage (usually 1GB)
instead of only the playback buﬀer in memory as in the P2P
streaming systems. This additional resource opens up vast
new opportunities for arranging suitable patterns of content
replication to meet diverse user demands. Essentially, the
new system is a highly dynamic P2P replication system, plus
a sophisticated distributed scheduling mechanism for direct-
ing peers to help each other in real time.
In this paper, we conduct an in-depth study of P2P-VoD
based on a real-world P2P-VoD system built and deployed
by PPLive in the fall of 2007. Since the deployment of this
P2P-VoD service, the system has been keeping track of the
number of users. As of late November 2007, a total of 2.2
million independent users had tried the system. A total
of 3900 movies were published in November and December
of 2007, with around 500 movies on-line simultaneously. In
late January 2008, the number of simultaneous users reached
over 150K and was still growing. The point is that this is
a system with a reasonable scale, and there are valuable
lessons to be learned by measuring and analyzing its behav-
ior.
The organization of the paper is as follows.
In Section
2, we ﬁrst describe a general architecture and its impor-
tant building blocks. This general model serves to provide a
framework and taxonomy for studying diﬀerent design issues
in a P2P-VoD system; for example, strategies for replication,
information search and lookup, peer service scheduling, and
other building blocks. In Section 3, we discuss metrics for
evaluating a P2P-VoD system, and how to instrument the
measurement in a real-life system. Real-life measurement
data were collected from the deployed PPLive VoD system.
In Section 4, we show the collected data, and analyze user
demand, the eﬀectiveness of the system design (for exam-
ple the replication and transmission scheduling strategies)
to satisfy user demand, and user satisfaction. Finally, we
discuss related work and conclusions.
2. DESIGN AND BUILDING BLOCKS
In this section, we describe the general architecture of a
P2P-VoD system in terms of various building blocks.
In
the process, we explain many speciﬁc design decisions in
the PPLive P2P-VoD system, as speciﬁc examples. We also
compare the P2P-VoD building blocks to those of P2P ﬁle
downloading and streaming: there are some similarities, but
also some notable diﬀerences.
2.1 Major components of the system
Similar to many P2P ﬁle sharing or streaming systems, a
P2P-VoD system has the following major components: (a)
a set of servers as the source of content (e.g., movies); (b) a
set of trackers to help peers connect to other peers to share
the same content; (c) a bootstrap server to help peers to ﬁnd
a suitable tracker (e.g. based on which geographical region
the peer is located), and to perform other bootstrapping
functions; (d) other servers such as log servers for logging
signiﬁcant events for data measurement, and transit servers
for helping peers behind NAT boxes. These servers are typ-
ically provided by the P2P-VoD operator.
The other major component, of course, is the set of peers.
They typically run software downloaded from the P2P-VoD
operator. The P2P-VoD peer software comes with protocols
to talk to all the servers above, as well as protocols to talk
to other peers to share content. The peers also implement
DHT (distributed hash table) function to back up certain
bootstrapping servers.
2.2 Segment sizes
In the design of a P2P-VoD system, a fundamental deci-
sion is about segmentation of content, or how to divide a
video into multiple pieces. There are many considerations
for making this decision. From a scheduling point of view,
it is desirable to divide the content into as many pieces as
possible (i.e., small segment size), so that it gives the most
ﬂexibility to schedule which piece should be uploaded from
which neighboring peer. This is specially so when peers all
have diﬀerent upload capacity. From the overhead point of
view, the larger the segment size the better, to minimize
overheads. There are several types of overheads including:
(a) Each piece of content comes with some header to de-
scribe the content, for example its sequence number and
Segment
movie
chunk
piece
Designed for
entire video
unit for storage
and advertisement
unit for playback
sub-piece
unit for transmission
Size
> 100MB
2MB
16KB
1KB
Table 1: Diﬀerent units of a movie
timestamp, and authentication information. The larger the
segment, the smaller the header overhead. (b) Each peer
needs to let other (neighboring) peers know which pieces
it is holding. This information is usually represented by a
bitmap, for ease of processing. The larger the segment size,
the smaller the size of the bitmap, hence this is advertizing
overhead. (c) In order for a peer to get a piece of content
from another peer, there will be some protocol overhead,
in terms of request packets or other protocol packets. The
larger the segment size the smaller is such protocol over-
heads. A third perspective is due to the real-time nature of
streaming. The video player expects a certain minimum size
for a piece of content to be viewable (so a viewable piece al-
ways consists of multiple packets), and such viewable units
must be delivered to the player with deadlines. By mak-
ing these units (exchanged between the transport and the
player) too large, it increases the chance that the transport
fails to collect a complete viewable unit of content before
the deadline.
Due to these conﬂicting requirements, there are three lev-
els of segmentation of a movie in PPLive’s VoD system, as
deﬁned in Table 1.
The size of piece is dictated by the media player and a
size of 16KB is chosen. PPLive uses the WMV format for
video encoding. The source video rate is usually between
381 to 450 Kbps. For high-deﬁnition video, the rate can go
up to 700 Kbps or higher. Using the 16KB size, a piece will
contain a viewable segment as long as the source rate is less
than 1.4 Mbps.
The size of piece is too large for eﬃcient scheduling of
transmission, so sub-piece is used. If piece is advertised to
other peers, then a bitmap of thousands of bits would be
needed (e.g., a 2GB movie would need a bitmap of size 64K
bits. So chunk is deﬁned and used for the purpose of ad-
vertizing to neighbors what parts of a movie a peer holds.
In summary, a movie is composed of a number of chunks, a
chunk is composed of a number of pieces, while a piece is
composed of a number of sub-pieces.
Given these choice of segment sizes, PPLive experiences
an overhead rate of 6.2%, considering all three types of over-
heads mentioned above, but assuming operating under per-
fect conditions with no losses, no unnecessary requests and
no duplicate transmissions. Under real-life network condi-
tions, the average overhead rate is about 10%.
2.3 Replication strategy
Each peer is assumed to contribute a ﬁxed amount of hard
disc storage (e.g., 1GB). The entire viewer population thus
forms a distributed P2P storage (or ﬁle) system. A chunk is
the basic unit for storing movies on a hard disc. Only when
all the pieces in a chunk are available locally, the chunk is
advertised to other peers.
The goal of the replication strategy is to make the chunks
as available to the user population as possible to meet users’
viewing demand while without incurring excessive additional
overheads. This is probably the most critical part of the
P2P-VoD system design. There are many possible replica-
tion strategies, many exploiting the various user demand
characteristics. This is an important area for continued re-
search.
The ﬁrst design issue is whether to allow multiple movies
be cached if there is room on the hard disc. If so, a peer
may be watching one movie while providing uploading to an-
other movie at the same time. This is referred to as multiple
movie cache (MVC) rather than single movie cache (SVC).
The design of SVC is simpler, but MVC is more ﬂexible for
satisfying user demands and is the choice by PPLive VoD.
The next important design consideration is whether to
pre-fetch or not. Without pre-fetching, only those movies
already viewed locally could possibly be found in a peer’s
disk cache. While Pre-fetching may improve performance,
it may also unnecessarily waste precious peer uplink band-
width. Also, for ADSL (commonly found in China), a peer’s
capacity to provide upload can be aﬀected if there is simul-
taneous downloading. Furthermore, it is observed that the
visit duration for the majority of peers is currently no more
than one hour, which increases the risk of wastage. For these
reasons, the design choice is no pre-fetching.
Another important choice by the replication algorithm is
which chunk/movie to remove when the disk cache is full.
In PPLive’s case, this decision is primarily made on a movie
basis. This means once a movie has been chosen as the next
one to go, all the chunks of the movie immediately become
candidates for removal one by one. Doing it at a chunk
level would incur more overheads (for collecting necessary
information about diﬀerent chunks). How is the next movie
picked? The favorite choices by many caching algorithms
are least recently used (LRU) or least frequently used (LFU).
Indeed, LRU is the original choice in PPLive VoD. After
further studies, the simple LRU is replace by a weight-based
evaluation process.
Each movie is assigned a weight based on primarily two
factors: (a) how complete the movie is already cached lo-
cally; (b) how needed a copy of the movie is. The need level
is determined by the availability to demand ratio (ATD).
Suppose a movie is cached (including being viewed) by c
peers and being viewed by n peers; then the ATD is c/n.
The need of a movie is then deﬁned as a decreasing function
of its ATD, reaching a maximum value for all ATD beyond
6 (or 8). The value of this threshold (6-8) is determined
by the prevailing uplink bandwidth contributed by peers,
normalized by the source bitrate. For current situation in
China, many peers have relatively low uplink bandwidth to
contribute, therefore it takes 6-8 peers to oﬄoad the source
(server).
The ATD information for weight computation is provided
by the tracker. So the implementation of the weight-based
replication strategy incurs additional overheads. This over-
head depends on how often the caching decision is made. In
current systems, the average interval between caching deci-
sions is about 5 to 15 minutes, so this is not a signiﬁcant
overhead. The beneﬁt of weight-based replication over LRU
is signiﬁcant. It improves the server loading from 19% down
to a range of 11% to 7%. This is the biggest performance
improvement achieved by a design change.
More detailed discussion of how to measure the eﬀective-
ness of the replication algorithms, will be discussed in sec-
tion 3. Measurement results and analysis will be included
in section 4.
2.4 Content Discovery and Peer Overlay Man-
agement
It is not enough to have good replication of content -
peers must also be able to discover the content they need
and which peers are holding that content. The challenge
is to accomplish this with the minimum overhead. With-
out exception, P2P systems rely on the following methods
for content advertising and look-up: (a) tracker (or super
node); (b) DHT; (c) gossiping method. These methods pro-
vide diﬀerent levels of availability, freshness and robustness,
with commensurate levels of overhead. In PPLive VoD, all
these mechanisms are used to some extent, depending on
the diﬀerent requirements for the information.
Trackers are used to keep track of which peers replicate
a given movie (or part of that movie). As soon as a user
(peer) starts watching a movie, the peer informs its tracker
that it is replicating that movie; conversely, a peer also tells
its tracker when it no longer holds a movie in its cache.
When a peer wants to start watching a movie, it goes to
the tracker to ﬁnd out which other peers have that movie.
Those other peers become this peer’s neighbors.
The information about which chunks a peer has is kept in
a Chunk Bitmap. A peer asks its neighbors for their Chunk
Bitmaps. Based on this information, it selects which neigh-
bor to download from. So discovering where chunks are is
by the gossip method. This cuts down on the reliance on
the tracker, and makes the system more robust. Even if the
tracker is not available, a peer can switch to the gossip mode
to ﬁnd other peers watching the same movie.
In fact, each peer also regularly sends keep-alive messages
to a tracker to report its chunk bitmap and other statistics.
This information is collected for monitoring and manage-
ment purposes, rather than for operational reasons. We will
describe how this information is used to compute a (replica-
tion) health index.
Originally, DHT (implemented by tracker nodes) is used
to automatically assign movies to trackers to achieve some
level of load balancing. In later versions, peers also imple-
ment DHT so as to provide a non-deterministic path to the
trackers. This prevents the trackers to be possibly blocked
by some ISPs.
2.5 Piece selection
A peer downloads chunks from other peers using a pull
method. For P2P-VoD, there are three considerations for
selecting which piece to download ﬁrst:
1. sequential : Select the piece that is closest to what is
needed for the video playback.
2. rarest ﬁrst: Select the piece that is the rarest (usually
the newest piece in the system). Although it seems
counter-intuitive for streaming, selecting the rarest piece
helps speeding up the spread of pieces, hence indirectly
helps streaming quality. This strategy tends to help
the system scale, which is clearly explained in [22].
3. anchor-based : In VoD, users may skip parts of a movie
and jump forward (backward). To support such VCR
features, a number of video anchor points are deﬁned
for a movie. When a user tries to jump to a particular
location in the movie, if the piece for that location is
missing then the closest anchor point is used instead.
In PPLive’s system, a mixed strategy is used, giving the ﬁrst
priority to sequential, then rarest-ﬁrst. The anchor-based
method is not used in current design for two reasons. (a)
From current experience, users do not jump around much.
On average, only 1.8 times per movie is observed. (b) By
optimizing the transmission scheduling algorithm, the initial
buﬀering time after a jump can be reduced to an acceptable
level1 without implementing anchoring. For these reasons,
the anchoring idea is still under study for future implemen-
tation.
2.6 Transmission strategy
After selecting a particular chunk to download, suppose
this chunk is available at a number of neighbor peers, how to
select which neighbor to download from? How many neigh-
bors to use for simultaneous download? How to schedule
requests and set timeouts to multiple neighbors for simulta-
neous download? All these are accomplished by the trans-
mission scheduling algorithm.
There are two (sometimes conﬂicting) goals in designing
the transmission algorithm: (a) maximize (to achieve the
needed) downloading rate; (b) minimize the overheads, due
to duplicate transmissions and requests.
In a data-driven overlay, the neighbors a peer connects to
can be highly dynamic, since each neighbor may be answer-
ing to multiple requests at a time. So a peer must constantly
juggle how to send download requests to diﬀerent neighbors
and how to deal with timeouts. There are diﬀerent levels of
aggressiveness: (i) a peer can send a request for the same
content to multiple neighbors simultaneously, to ensure it
gets the content in time; (ii) a peer can request for diﬀerent
content from multiple neighbors simultaneously; when a re-
quest times out, it is redirected to a diﬀerent neighbor; (iii)
work with one neighbor at a time; only when that neighbor
times out, try to connect to a diﬀerent neighbor.
Strategy (i) is very aggressive for achieving the deadline
for downloads, but invariably generates duplicate transmis-
sions. Strategy (iii) is very conservative in resource utiliza-
tion. Both strategy (ii) and (iii) may still general dupli-
cate transmissions because of timeouts, but the likelihood is
much lower than (i). PPLive VoD’s transmission algorithm
is based on strategy (ii). In implementing strategy (ii), the
algorithm tries to proportionally send more requests to the
neighbor based on response time. A critical parameter for
tuning is the number of simultaneous neighbors to send re-
quests to. For playback rate of around 500Kbps, our experi-
ence is that 8-20 neighbors is the sweet spot. More than this
number can still improve the achieved rate, but at the ex-
pense of heavy duplication rate. If the desired rate is 1Mbps,
then 16-32 simultaneous neighbors tends to provide the best
result. These numbers are highly empirical, depending on
the prevailing uplink bandwidth of peers and many other
factors. Overall, how to design the best transmission algo-
rithm is an interesting topic for further research.
Finally, it should be pointed out that when the neigh-
boring peers cannot supply suﬃcient downloading rate, the
content server can always be used to supplement the need.
1In recent tests, the average buﬀering time is around 18
seconds.
2.7 Other design issues
After describing the basic design for normal networking
conditions, we describe a number of mechanisms designed
to deal with abnormal operating conditions. These include:
incentives for contribution; traversing NAT and ﬁrewalls;
and content authentication.
It is well-known that the P2P ﬁle downloading protocol
BitTorrent [9] uses tit-for-tat as incentive to induce peers to
help each other. In P2P streaming, this does not work since
many peers cannot contribute uploading bandwidth greater
than or equal to the playback rate. So what incentives are
used? In the PPLive system, the users do not have any built-
in controls for adjusting their contribution levels. In order
for the software to continue to deliver content for playback,
the client software must regularly advertise its chunk bitmap
to the tracker; else playback would be automatically turned
oﬀ.
Another impediment to P2P overlay networks is the NAT
boxes and ﬁrewalls. The PPLive VoD system uses standard
methods2 for peers to discover diﬀerent types of NAT boxes