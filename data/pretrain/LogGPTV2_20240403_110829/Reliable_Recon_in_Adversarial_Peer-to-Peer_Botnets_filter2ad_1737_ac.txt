The crawlers with low-entropy source IDs occasionally used
identiﬁers with ASCII bytes representing the name of the
company or individual running the crawler.
In contrast to Zeus, Sality uses randomly chosen integers
instead of hashes as (non-global) bot identiﬁers. While Zeus
crawlers often use low-entropy identiﬁer strings, all Sality
crawlers use seemingly random integer IDs, without any
entropy anomalies.
4.1.3 Invalid Encryption
In 7 of the analyzed Zeus crawlers, we encountered cor-
rupted messages that contained irrational data for all message
ﬁelds. These corrupted messages were interspersed with cor-
rectly encoded messages.
It appears that these crawlers
occasionally select incorrect keys to encrypt outgoing mes-
sages, preventing our sensors from decrypting these messages.
In GameOver Zeus, the unique identiﬁer of each bot is used
as the key to encrypt messages towards that bot. Thus, we
suspect that the crawlers in question do not correctly keep
track of the identiﬁer of each bot they ﬁnd. We did not
encounter any invalid encryption in Sality crawlers.
4.1.4 Protocol Logic Anomalies
As mentioned in Section 2, an advantage of crawlers is
that they avoid the need to implement the full P2P protocol.
However, taking this simpliﬁcation too far results in crawlers
that manifest incorrect protocol logic. This was the case
for 17 of the analyzed Zeus crawlers. In most cases, these
crawlers simply sent large amounts of peer list requests,
without regard for any other message types used by normal
bots, such as messages used to exchange commands and
binary/conﬁguration updates.
Additionally, many crawlers used abnormal values for the
lookup key ﬁeld used in Zeus peer list requests. This ﬁeld
includes a Zeus identiﬁer which is used by the receiving
peer as a metric to determine which peers are sent back in
the peer list response. Normal bots always set this ﬁeld to
the identiﬁer of the remote peer. In contrast, many crawlers
randomize the lookup key on each peer list request to increase
the range of crawled peers.
Similarly to Zeus, most Sality crawlers (9 out of 11) used
incorrect message sequences. Typically, these crawlers sent
repeated peer list requests to the same bot, without inter-
spersing any of the other normal message types, such as
URL pack exchanges. Another common defect is the use
of incorrect version numbers in the Sality message headers
sent by crawlers. While all of the crawlers used a correct
major version number, only 2 of them also used a valid minor
version number.
4.1.5 Request Frequency Anomalies
In an eﬀort to rapidly gather as much intelligence as pos-
sible about the botnet connectivity graph, 9 of the analyzed
Zeus crawlers sent repeated peer list requests at high frequen-
cies. These hard-hitting crawlers clearly contrast with real
bots, which exchange only one peer list request per neighbor
before returning to a suspended mode. Such request-suspend
behavior is seen in many botnets [28], with a suspend period
of 30 minutes for Zeus and 40 minutes for Sality, making
hard-hitting crawlers highly anomalous.
In Sality, request frequency anomalies are even more com-
mon than in Zeus, occurring in all of the analyzed crawlers.
This is because Sality bots only support the exchange of a
single peer list entry at once, while Zeus bots return up to 10
entries in a single peer list response. Additionally, Sality peer
133lists are much larger than Zeus peer lists, typically containing
around 1000 entries, while Zeus peer lists are limited to 150
entries (and rarely contain over 50 entries). To obtain a
reasonable coverage of a bot’s peer list, Sality crawlers are
therefore obliged to send this bot multiple peer list requests.
Sending these in quick succession, as all of the analyzed
crawlers do, results in clear request frequency anomalies.
4.2 Sensor Protocol Anomalies
In addition to our crawler analysis from Section 4.1, we
also analyzed the sensors active in GameOver Zeus and Sality
during our monitoring. We found Zeus sensors belonging
to 10 organizations (grouped by subnet) by analyzing the
GameOver Zeus connectivity graph, and looking for nodes
with high in-degrees. Since sensors aim to attract as many
bots as possible, they are expected to have high in-degrees.
To create a view of the connectivity graph and node in-
degrees, our sensors sent active peer list requests to every
bot that contacted them. After identifying high in-degree
nodes, we probed each of these for anomalous responses to
identify (defective) sensors. This is necessary as, unlike high
out-degrees, our data shows that high in-degrees occur in
hundreds of apparently legitimate bots. As in our crawler
analysis, we cross-veriﬁed the identiﬁed sensors with a list of
sensors known from industry contacts.
Our results show that the Zeus sensors suﬀer from the same
shortcomings as current crawlers, including range anomalies,
entropy anomalies, and protocol logic anomalies. In addi-
tion to these anomalies, we identiﬁed several sensor-speciﬁc
anomalies. Speciﬁcally, all of the analyzed sensors failed to
return the protocol-mandated list of proxy bots (used as
data drops in GameOver Zeus) when requested. In addition,
all but 3 of the sensors responded to peer list requests with
empty peer lists, which is highly unusual behavior. Further-
more, all of the sensors which returned non-empty peer lists
served duplicate peers in order to promote sinkholes or sen-
sors, a behavior never displayed by legitimate bots. Finally,
none of the sensors provided a correct implementation of the
Zeus update mechanism; only 3 sensors reported valid and
recent version numbers, and none of the sensors responded
to update requests used to exchange binary and conﬁg ﬁles.
Although these anomalies allow current sensors to be de-
tected, sensors are not inherently detectable. Speciﬁcally, al-
though sensors generally have high in-degrees, our data shows
that equally high in-degrees also occur for well-reachable le-
gitimate bots, making high in-degree alone an insuﬃcient
metric for reliable sensor detection. Furthermore, P2P bot-
net protocols cannot be easily designed to expose sensors by
limiting the in-degree of bots. This is because, by the degree
sum formula1, limiting the in-degree of bots implies limiting
the out-degree as well, thus impairing the connectivity of
the bots.
In our analysis, we were unable to identify any
sensors in Sality, precisely because no nodes with unusually
high in-degree were present, and all high in-degree nodes
responded correctly to probes for all packet types, includ-
ing URL pack exchanges and peer exchanges. Thus, if any
sensors are present in the Sality network, these cannot be
detected in any straightforward way. In contrast, eﬃcient
crawlers tend to have unusually high out-degrees, as we show
1In a directed graph G = (V, E), with V the set of nodes
and E the set of edges, the degree sum formula states that
Pv∈V deg+(v) = Pv∈V deg−(v) = |E|, where deg+(v) and
deg−(v) are the out-degree and in-degree of node v.
in Section 4.3, requiring special measures to avoid detection.
To further investigate the detectability of crawlers by out-
degree, we design a distributed crawler detection model in
Section 4.3. Subsequently, we categorize crawling techniques
to evade this model in Section 5, and implement and evaluate
our detection model and evasive techniques in Section 6.
4.3 Network Coverage Anomalies
Even syntactically sound crawlers are detectable due to
their tendency to contact many peers in a short timespan,
in an attempt to quickly map the network. This behavior is
visible in Tables 2 and 3. Sality and Zeus crawlers cover up
to 100% and 92% of our sensors, respectively. Furthermore,
nearly all crawlers cover at least 20% of our sensors, and
most crawlers cover 50% or more. In contrast, ordinary bots
cover only a small fraction of the botnet, due to their limited
peer list sizes and evolution rates [10, 2, 28].
To evaluate the extent of this problem, and the eﬀectiveness
of our proposed countermeasures, we design and implement
a syntax-agnostic crawler detection algorithm that identiﬁes
crawlers based on their network coverage. We chose to im-
plement a distributed version of the algorithm to show that
crawler detection is possible even without requiring central-
ized components in P2P botnets. The algorithm is scalable
and Byzantine-tolerant, to allow crawler detection even in the
presence of adversarial nodes (which malware analysts may
inject to subvert the algorithm). To prevent impersonation
attacks, the algorithm assumes a non-spoofable transport
layer, such as TCP. This section provides an overview of our
algorithm, while Sections 5 and 6 discuss improved crawling
techniques and their eﬀectiveness in evading our detection
algorithm, respectively.
Note that this is just one from a range of out-degree-based
crawler detection methods. Alternative implementations,
even centralized ones, can equally well detect crawlers with
anomalous out-degrees, and completely circumvent the risk
of subversion by Sybils. Centralized implementations ﬁt well
with the hybrid structure of many current P2P botnets, such
as Zeus and Kelihos, where bots periodically report to a
higher layer of centralized servers [2, 35, 19]. Therefore,
this paper focuses on our ﬁndings regarding the eﬃcacy of
stealthy crawling techniques, rather than technical details of
our particular detection algorithm. Due to space limitations,
we defer a discussion of such details, and the Byzantine-
tolerance of our algorithm, to a technical report [1].
Our algorithm is based on periodic crawler detection rounds,
with the period depending on the time needed to compre-
hensively crawl the botnet. This time depends on the botnet
architecture, diurnal pattern, and protocol, and is 24 hours
for Zeus and Sality, as shown in Section 6 and prior work [28].
Crawling for less than 24 hours misses part of the population,
while crawling longer pollutes results due to infection churn
and IP churn [28]. Our tests in Section 6 use hourly detection
rounds to detect crawlers well before they cover a full diurnal
period. The algorithm assumes that each bot has a random
identiﬁer, generated at infection time. Each round consists
of the following operations.
Detection round announcement The botmaster pushes
a round announcement (signed and timestamped to prevent
replays) to a random bot, which then propagates to all
bots using gossiping (a technique also used in Zeus and Ze-
roAccess [28]). We use push-based gossiping to reach only
routable peers, excluding non-routable bots (never reached
134by crawlers) for scalability. The bots partition themselves
into 2g groups by sampling g bit positions (speciﬁed in the
announcement) from their identiﬁers. Each group contains
the bots with identical bit values at these g positions, and a
per-group leader is assigned in the announcement, creating
2g tree-shaped overlay networks rooted at the group leaders.
Hardhitter aggregation Every bot contacts its leader
and reports all IPs that requested its peer list within a con-
ﬁgurable history interval. This interval must cover multiple
detection rounds, or else crawlers can avoid detection by con-
tacting only a limited set of bots per round (see Section 6).
Leaders aggregate the IPs (including their own history), and
ﬂag IPs reported by at least a conﬁgurable threshold fraction
of the group as suspicious. Section 6 discusses how to set the
threshold to minimize false positives/negatives.
Crawler voting Leaders collectively vote for suspected
crawler IPs, and classify those IPs which receive a major-
ity vote as crawlers. Majority voting makes the algorithm
tolerant of adversarial leaders which unjustly blacklist non-
crawler IPs, or whitelist true crawlers [1]. As leader selection
is random, adversarial nodes are unlikely to dominate the
leader population unless a large fraction of the bot popula-
tion consists of Sybils (anti-Sybil strategies are discussed in
prior work [29, 7]).
Crawler propagation All bots retrieve the list of crawlers
from n randomly chosen leaders, and ﬁlter crawlers reported
by a majority of the leaders. This limits the scope of faulty
results reported by adversarial leaders. Bots can expect reli-
able results if |A| < n × m, where |A| denotes the number of
adversarial leaders and m the fraction of leaders required in
a majority.
5. STEALTHY CRAWLING TECHNIQUES
In this section, we propose methods to improve the re-
liability and stealthiness of P2P botnet crawling. These
strategies are speciﬁcally aimed at evading out-degree- or
request-frequency-based detection, as described in Sections 4
and 4.3. We measure the eﬃcacy of these strategies, and their
impact on crawling eﬃciency, in Section 6. The methods
proposed in this section apply to all variants (distributed and
centralized) of out-degree-based crawler detection algorithms.
5.1 Contact Ratio Limiting
A straightforward way to limit the out-degree of crawlers is
to limit the set of bot IPs they contact. For instance, crawlers
which contact only half of the bots (a contact ratio of 1/2)
are expected to still obtain a reasonably complete view of the
network, as the addresses of the excluded bots are returned
in the peer list responses of the remaining bots. A limitation
of this approach is that it further exaggerates the node
veriﬁcation problem described in Section 2. Contact ratio-
limited crawlers are not only unable to verify the authenticity
of non-routable nodes, but also cannot verify the excluded
routable bots. Furthermore, we show in Section 6 that to
evade detection using contact ratio limiting, crawlers must
use very low contact ratios of 1/16, or even 1/32. As we show,
this reduces the completeness of the gathered intelligence.
5.2 Request Frequency Limiting
As described in Section 4.1, many crawlers send multi-
ple peer list requests in quick succession to maximize their
coverage of each bot’s peer list. Stealthy crawlers should
limit their request frequencies to avoid being identiﬁed as
hard hitters (see Section 4.1.5). We measure the impact of
request frequency limiting in Section 6, and show that the
crawling eﬃciency obtained by frequency-limited crawlers
depends on protocol-speciﬁc factors, like the selection strat-
egy for returned peers, and the number of entries per peer
list response.
5.3 Distributed Crawling
By distributing their egress traﬃc over multiple source
addresses, crawlers can reduce the risk of detection by IP-
centric sensors. This strategy encompasses several techniques.
(1) Crawlers can be (virtually) distributed over multiple
addresses, crawling only a limited subset of bots per source
address.
(2) Addresses used for crawling can be rotated
periodically, with a rotation frequency conﬁgured such that
a new address is selected before exceeding the per-address
detection threshold.
As we show in Section 6, distributed crawlers must use
IPs from distinct subnets to reliably avoid detection. The
crawler detection algorithm in our experiments can reliably
detect crawlers with IPs distributed over a /20 subnet, or
with per-address crawling traﬃc limited by a factor of up
to 1/32. Thus, distributed Zeus or Sality crawlers require
addresses from at least 32 distinct /20 subnets, or a single
/16 network block.
5.4 Anonymizing Proxies
Crawlers could attempt to evade detection by using proxy
servers or an anonymizing network such as Tor [8] to ob-
tain rapidly changing IP addresses. This approach can be
eﬀective, but unfortunately does not blend well with current
anonymizing services, due to several issues. (1) Eﬀective
crawlers simultaneously open thousands of connections, caus-
ing connection tracking and scalability problems with non-
dedicated proxy services. Furthermore, the bandwidth and
latency performance of the Tor network is known to be con-
strained, resulting in slowly converging, and thus inaccurate,
crawling results [8, 26]. (2) The IPs used by anonymizing
proxies and Tor exit nodes are often publicly known, and
can thus be easily blocked by botmasters [12, 23]. A more
eﬀective option might be to set up a dedicated anonymizing
proxy service, though this would require one or more large
dedicated network address blocks as well as strong secrecy
measures to avoid these from leaking out.
6. STEALTHY CRAWLING EVALUATION
In this section, we evaluate the detectability of in-the-wild
Zeus crawlers by network coverage, using the crawler detec-
tion algorithm introduced in Section 4.3. Furthermore, we
measure the eﬀectiveness of the evasive crawling techniques
introduced in Section 5, and their tradeoﬀs in crawling speed
and completeness. We do this by simulating (to ensure re-
peatability) the eﬀects of these evasive strategies on traﬃc
we logged from real-world Sality and Zeus crawlers.
6.1 Crawler Detection Accuracy
A full-scale crawler detection algorithm would run dis-
tributed over all routable nodes in a botnet. Since we cannot
deploy such an experimental setup, we instead ran our crawler
detection experiments on the 512 sensor nodes which we in-
jected into the GameOver Zeus botnet. As mentioned in
Section 4.1, none of the sensors contained any malicious
logic. We performed all our experiments before the recent
135t
1
2
5
|G| #FP D1/1 D1/2 D1/4 D1/8 D1/16 D1/32 D1/64 D1/128 D1/256
0
8
0
8
8
0
100
100
100
119
13
0
89
83
50
100
100
89
100
89
89
94
89
72
83
44
0
39
6
0
28
0
0
N/A
CZeus
CSality N/A
N/A
N/A
100
100
80
90
52
74
42
44
38
27
2
16
Dc = % detected crawlers for contact ratio c. C = % bots covered by crawler using contact-ratio limiting (relative).
Table 4: False positives vs. detected crawlers for |G| = 8 and t ∈ {1%, 2%, 5%}.
|G|=8, t=  1%
|G|=8, t=  2%
|G|=8, t=  5%