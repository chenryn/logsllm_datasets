gives the detail of the SHE-Based Lanczos Algorithm.
Compared to the AHE-based algorithm, this algorithm
simpliﬁes the interactions. (1) It does not need data distrib-
utors to participate in the computation. (2) It does not have
the Prepare stage and the query, E(bi), passed to Cloud is
encrypted by SHE. However, decrypting, local processing,
encrypting, and uploading E(bi) are now becoming the
major costs for the data owner. The actual costs will be
evaluated in experiments.
SHE-Based Nystr ¨om Method. The Nystr¨om method
involves homomorphic matrix-matrix multiplication CV ,
which consists of a set of vector dot-products. Similarly, SHE
schemes can be applied directly to this operation. The SHE-
Based Nystr¨om method is a slight revision of the original
Nystr¨om method since V can be encrypted now. The data
owner has the responsibility to download and decrypt sparse
E(W ), locally decompose W , encrypt and upload the dense
V matrix, and ﬁnally download and decrypt the dense
E(CV ). We show the details in Algorithm 7.
Algorithm 7 in Appendix gives the steps of the Nystr¨om
method for SHE schemes. It differs from the AHE-based
algorithm in several aspects. (1) The perturbation and re-
covery steps are gone due to the encrypted V and thus the
client side computation is simpliﬁed. (2) The upload cost
will increase due to the increased size of encrypted V . (3)
The download cost depends on the speciﬁc SHE method.
The number of download items is reduced but each item’s
ciphertext size may increase.
Security Analysis. Note that for both SHE-based algo-
rithms, the queries {qi} in the operation Query are all
encrypted by SHE. For an IND-CPA [21] SHE like RLWE,
the simulator can choose random queries to encode and thus
the protocols are adaptively semantically secure.
3.7 Cost Analysis
Table 1 compares the asymptotic costs for all the algorithms,
where k is the number of top eigenvectors/values, t is
the number of Lanczos iterations, and m is the number of
samples in the Nystr¨om method. These parameters have
the relationships: k ≪ m < N and k < t ≪ N . h is
the number of seed vectors in LWE-based masking. The
communication costs consider only the encrypted trafﬁcs as
other trafﬁcs are much smaller. The contributors’ cost O(N )
only occurs in the Lanczos AHE algorithm and not included.
Note that the initial matrix setup costs are the same for all
the methods and thus not included. The dominating compu-
tational costs for data owner are encryption and decryption,
compared to other linear-cost operations on plaintext. The
acceptable m for the Nystr¨om method can be smaller for a
dense matrix that have clearly separated clusters. However,
in reality it has to be considerably large to preserve the
data utility when we do not know the underlying cluster
distribution. As Kumar et al. [22] suggested, m is often set
to 0.05N ∼ 0.1N to get good data utility, which makes
the client-side costs non-linear to N for dense matrices.
1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TKDE.2018.2847662, IEEE
Transactions on Knowledge and Data Engineering
10
Thus, Ny-* algorithms are not appropriate for dense ma-
trices. Finally, the costs for the same algorithm (AHE or
SHE) implemented with different encryption methods are
asymptotically same, and thus we have to look at the real
costs to see the effects of different encryption methods.
Datasets. Three graph datasets in the SNAP database
(snap.stanford.edu) are used in our evaluation. They were
originally used to study social circles in the three popular
social networks - Facebook, Twitter, and GPlus. We make
the edges undirected for easier processing in the evaluation.
TABLE 1: Cost distribution between cloud and data owner
Algorithm
Lan-AHE
Lan-SHE
Ny-AHE
Ny-SHE
Cloud
O(tN 2)
O(tN 2)
O(N km))
O(N km))
Data Owner
O((t + h)N )
O(2tN )
Comm. cost
O((t + h)N )
O(2tN )
O(m2 + mk2 + N k)
O(m2 + mk2 + N k)
O(2N k + 2km + m2)
O(N k + km + m2)
100%
90%
80%
y
c
a
r
u
c
c
A
g
n
i
r
e
t
s
u
C
l
facebook
twitter
gplus
facebook
twitter
gplus
95%
90%
85%
80%
75%
y
c
a
r
u
c
c
A
g
n
i
r
e
t
s
u
C
l
0
2
0
4
0
6
0
8
0
0
1
0
0
3
0
0
5
0
0
7
0
0
9
0
0
0
1
70%
0
Number of Iterations
6 · 10−2 0.12
Sample Rate
0.18
0.24
0.3
3.8 Cloud-Side Parallel Computation
Data encrypted with the mentioned encryption schemes
are signiﬁcantly larger than the unencrypted values. For
example, with a 1024-bit key for Paillier encryption, a 64-
bit double-type value becomes a 2048-bit ciphertext, a 32-
time increase. For RLWE, it’s even larger. The encrypted
matrices literally turn a common-size problem to a “big
data” problem, which requires us to exploit the parallel
processing power with the cloud.
In the following, we show the parallel processing algo-
rithm for homomorphic matrix-vector multiplication with
AHE-encrypted data. It is straightforward to extend the
algorithm for data encrypted with SHE schemes and for
matrix-matrix multiplication. For clear presentation, we de-
scribe the algorithm with the MapReduce programming
model [13]. The MapReduce program consists of the Map
and Reduce functions. The Map function takes the masked
vector sent by the data owner as the parameter. It applies
the vector dot-products to the encrypted rows and emits the
results indexed by the row number. The Map outputs are
partitioned and sent to Reducers which automatically sort
the items by their row numbers and write the result to disk.
Readers can check details of MapReduce [13] for a better
understanding of the algorithm.
4 EXPERIMENTS
We have shown that all the developed algorithms provide
privacy guarantee under the assumption of the framework.
The experiments will evaluate various costs associated with
these methods to ﬁnd out whether any of these algorithms
are more efﬁcient. Speciﬁcally, our evaluation has three
aspects: (i) comparing the basic setup costs for the cloud
and data contributors with different encryption methods;
(ii) comparing the costs occurring in executing the AHE and
SHE based privacy-preserving Lanczos algorithms for the
data owner; (iii) the cost-beneﬁt of sparse submission, and
the comparison between the AHE and SHE based privacy-
preserving Nystr¨om algorithms.
(a) Number of iterations vs. clus-
tering accuracy for Lanczos
(b) Sample rate vs. clustering ac-
curacy for Nystrom
Fig. 3: Clustering accuracy and parameter settings for Lanc-
zos and Nystr¨om Algorithms
Evaluation Methods. The costs of proposed algorithms
are inherently linked to the following parameters: the num-
ber of iterations, t, for the Lanczos method, and the sam-
pling size, m, for the Nystr¨om method, respectively, which
both in turn are related the quality of results. To untangle
this intricate relationship, we use spectral clustering [15]
as the application of eigendecomposition to determine the
appropriate setting of these parameters. Speciﬁcally, we will
ﬁx the quality criterion to derive the corresponding param-
eter setting, and then evaluate the cost of each algorithm
under this setting. First, we set the number of clusters to
k = 10 and derive the ideal clustering results by running
the spectral clustering algorithm on plaintext data with
the exact eigendecomposition algorithm using functions from
the Armadillo C++ library (arma.sourceforge.net). Then, the
Lanczos method and the Nystr¨om method use different set-
tings of t and m, respectively, to get approximate clustering
results. The clustering accuracy is computed by matching
the approximate result to the ideal result. The settings are
selected as the clustering accuracy becomes stable.
Figures 3a and 3b show how the parameter settings
affect the accuracy of the approximate spectral clustering
algorithm. Table 2 shows the minimum parameter settings,
with which the clustering accuracy becomes stable. They
will be used in the cost evaluation.
TABLE 2: The number of iterations (t) for Lanczos and
sampling size (m) for Nystrom to reach stable clustering
accuracy.
Datasets
Facebook
Twitter
Gplus
N
3959
76244
102100
Accuracy
82%
90%
92%
m
396
3050
8168
t
30
25
30
4.1 Setup
4.2 Implementation
Resources. Our setup simulates the framework we de-
scribed in Section 3. The data owner’s system has 128 GB
of RAM and four quad-core AMD processors. The cloud
infrastructure consists of an in-house Hadoop cluster with a
16-node setup (1 master node and 15 work nodes: each has
two quad-core 2.6GHz AMD CPUs and 16GB memory).
We implemented the Paillier encryption for the AHE-based
algorithms. The core algorithms are implemented with C++
using GMP big integer library and Armadillo linear algebra
library. We also implement the cloud-side MapReduce pro-
gram with Java and Java native library that accesses the C++
encryption libraries. We use the 80-bit security level to setup
1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TKDE.2018.2847662, IEEE
Transactions on Knowledge and Data Engineering
the encryption parameters, and preserve 10 fractional-digit
precision for ﬂoating-integer conversion (Section 2.4). The
results generated by the AHE-based algorithms are veriﬁed
with those from the normal algorithms on plaintext data. A
demo system can be downloaded1.
We use the HELib library (github.com/shaih/HElib) for
the RLWE scheme. 32-bit plaintext encoding is used, which
is also the maximum number of bits allowed by HELib.
HELib uses the ciphertext packing technique [31], which
can be used for encoding dense matrices efﬁciently. For 80-
bit security and 32-bit plaintext, one ciphertext can encode
a vector of 630 encrypted values and thus greatly improve
the efﬁciency. However, sparse matrix cannot use ciphertext
packing and thus the cost for encoding each value will
be 630 times larger. We have also tested the PBC library
(crypto.stanford.edu/pbc/) for the pairing-based scheme.
However, because its decryption involves solving the ex-
pensive discrete logarithm problem, e.g., a 20-bit value en-
crypted will take about 1 second to decrypt, the aggregated
high cost for big matrices will become impractical for the
data owner. Thus, the pairing scheme is not included in
evaluation.
Table 3 summarizes the costs of basic operations. The
Ciphertext-size (C-size), Enc., and Dec. columns represent
the costs for encoding, encrypting, and decrypting one
value, respectively. RLWE-P represents RLWE using cipher-
text packing and the numbers are the average per-element
costs based on 630 elements that are encoded in one ci-
phertext. HELib uses the text format to store the ciphertext.
We also zip the ciphertext to minimize the costs. Since the
size may vary slightly due to the text-based encoding, the
RLWE costs are based on the average of 10 runs. The cost
of homomorphic dot-product (dot-p) is based on vectors
of 630 elements for an easier comparison crossing different
encryption schemes. The dot-p cost can be roughly scaled
up for estimating matrix-vector multiplication and matrix-
matrix multiplication in different sizes. Note that the RLWE-
P costs for encryption and decryption are really low, while
the ciphertext is much larger than Paillier.
TABLE 3: Basic costs for different encryption methods in
80-bit security. C-size: Ciphertext size, B:bytes, and ms:
milliseconds.
Method
Paillier
RLWE
RLWE-P
C-size
256B
489.3KB
795.3B
Enc(ms)
Dec(ms)
dot-p(ms)
1.7
25.3
0.04
1.6
120.0
0.2
36.0
5.5E5
875.0
4.3 Results on Dense Matrices