RAID’07, pages 167–177, Berlin, Heidelberg, 2007.
Springer-Verlag.
[37] Gary McGraw and Greg Morrisett. Attacking
malicious code: A report to the infosec research
council. IEEE Softw., 17:33–41, September 2000.
[38] Benjamin Morin and Herv ˜Al’ Debar. Correlation of
intrusion symptoms: an application of chronicles. In
RAIDˆa ˘A ´Z03, pages 94–112, 2003.
[39] Peng Ning, Yun Cui, and Douglas S. Reeves.
Constructing attack scenarios through correlation of
intrusion alerts. In In Proceedings of the 9th ACM
conference on Computer and communications security,
pages 245–254, 2002.
[40] G. Piatetsky-Shapiro. Discovery, analysis and
presentation of strong rules. In G. Piatetsky-Shapiro
and W. J. Frawley, editors, Knowledge Discovery in
Databases, pages 229–248. AAAI Press, 1991.
[41] Xinzhou Qin. A probabilistic-based framework for
infosec alert correlation. PhD thesis, Atlanta, GA,
USA, 2005. AAI3183248.
[42] Xinzhou Qin and Wenke Lee. Statistical causality
analysis of infosec alert data. In RAID 2003, pages
73–93, 2003.
[43] Hanli Ren, Natalia Stakhanova, and Ali A. Ghorbani.
An online adaptive approach to alert correlation. In
Proceedings of the 7th international conference on
Detection of intrusions and malware, and vulnerability
assessment, DIMVA’10, pages 153–172, Berlin,
Heidelberg, 2010. Springer-Verlag.
[44] Vyas Sekar, Yinglian Xie, Michael K. Reiter, and Hui
Zhang. Is host-based anomaly detection + temporal
correlation = worm causality?, 2007.
[45] Sushant Sinha, Michael Bailey, and Farnam Jahanian.
Shades of grey: On the eﬀectiveness of
reputation-based blacklists. In Proceedings of the 3rd
International Conference on Malicious and Unwanted
Software (MALWARE ’08), pages 57–64, Fairfax,
Virginia, USA, October 2008.
[46] P. Smyth and R. M. Goodman. An information
theoretic approach to rule induction from databases.
IEEE Trans. on Knowl. and Data Eng., 4:301–316,
August 1992.
[47] A free lightweight network intrusion detection system
for UNIX and Windows. http://www.snort.org.
[48] Joel Sommers, Vinod Yegneswaran, and Paul Barford.
A framework for malicious workload generation. In
Proceedings of the 4th ACM SIGCOMM conference on
Internet measurement, IMC ’04, pages 82–87, New
York, NY, USA, 2004. ACM.
[49] Stuart Staniford, Vern Paxson, and Nicholas Weaver.
How to own the internet in your spare time. In
Proceedings of the 11th USENIX Security Symposium,
pages 149–167, Berkeley, CA, USA, 2002. USENIX
Association.
[50] Ionut Trestian, Supranamaya Ranjan, Aleksandar
Kuzmanovi, and Antonio Nucci. Unconstrained
endpoint proﬁling (googling the internet). In
Proceedings of the ACM SIGCOMM 2008 conference
on Data communication, SIGCOMM ’08, pages
279–290, New York, NY, USA, 2008. ACM.
[51] Alfonso Valdes and Keith Skinner. Probabilistic alert
correlation. In Recent Advances in Intrusion
Detection, pages 54–68, 2001.
[52] Vinod Yegneswaran, Paul Barford, and Johannes
Ullrich. Internet intrusions: global characteristics and
prevalence. In Proceedings of the 2003 ACM
SIGMETRICS international conference on
Measurement and modeling of computer systems,
SIGMETRICS ’03, pages 138–147, New York, NY,
USA, 2003. ACM.
[53] Bin Zhu and Ali A. Ghorbani. Abstract alert
correlation for extracting attack strategies, 2005.
41Summary Review Documentation for 
“Detecting, Validating and Characterizing Computer 
Infections in the Wild” 
Authors: E. Raftopoulos, X. Dimitropoulos 
Reviewer #1 
Strengths:	 This  problem  is  very  important  and  the  authors 
present  a  nice  solution  validated  using  real  ground  truth.  Well 
written paper with good evaluation using a lot of real data.  
Weaknesses: The paper focuses on a rather narrow set of attacks. 
System description needs to be clearer. 
Comments to Authors: The way I understood the paper is that, 
the  J-measure  heuristic  first  groups  together  alerts  that  are 
correlated  somehow.  Then  it  looks  at  all  the  alerts  that  are 
correlated  for  a  given  machine  and  then  uses  the  composite 
signature to classify whether the infection is a backdoor infection 
or spyware or worm or trojan. So far it is clear, but I am not sure 
how  the  mapping  is  done.  The  whole  section  4.2  gives  a  vague 
description of the different types of attacks, but does not precisely 
define  how  the  matching  process  is  done.  This  is  a  key  piece 
missing from the paper.  
Further, the system descriptions are very vague; how does one use 
the  system  in  practice.  What  constitutes  the  system  itself  is 
unclear. There is a lot of manual effort, it seems, in classifying the 
alert groups and annotating them. There is a lot of vagueness in 
the  description  unfortunately  for  me  to  understand  the  exact 
components  of  the  system.  The  output  of  the  system  is  also 
unclear. Does it output the set of alerts that correspond to XYZ 
worm,  XYZ  trojan  or  it  just  says  worm  and  trojan.  I  suspect 
different worms or trojans have different signatures. So, does one 
need to build different signatures for each of these separately in 
advance?  
If the only output is broad classes of infection, I am not sure what 
the  advantage  of  this  system  is  compared  to  just  combining  the 
alerts that are for a given host. Yes, the alert group will be big, 
but would’nt an expert just look at the types of alerts and quickly 
figure out what type of an attack is ? What additional advantage 
one has in using the J-measure.  
There 
involves 
characterizing  the  attacks  observed  in  their  campus  trace  data. 
This study is kind of orthogonal to the other parts of the paper. 
This study is also not easily generalizable to other networks. I did 
not know what to interpret and take out of those findings. 
Reviewer #2 
Strengths:  Well  written  and  evaluation;  quite  an  important 
problem,  relieving  network  administrators  of  the  false  alert 
overload would be quite beneficial. 
is  another  component  of 
the  paper 
that 
Weaknesses: Its not clear to me what is automated, and what is 
not.  In  other  words,  how  much  reduction  in  human  effort  could 
one expect by deploying this system. 
Comments to Authors: The paper argues that by grouping alerts 
together for a host or for some other metric, one can identify the 
likelihood of an infection as well as the type of infection. But how 
is this final step done. Does the system present these clusters to 
the administrator and he determines whether there is an infection? 
I guess I am asking how does this system exactly reduce the large 
fraction  of  useless  IDS  alerts.  Is  it  mainly  by  grouping  similar 
alerts  into  one?  But  if  that’s  the  case,  wont  a  simple  grouping 
based  on  some  IDS  alert  type  as  well  as  IP  be  sufficient?  The 
administrator  could  then  quickly  decide  what  to  do  from  this 
smaller set of data. 
Reviewer #3 
Strengths:  Important  and  challenging  problem  (reducing  the 
false-positive rate of intrusion-detection systems). Interesting idea 
(to use the cross entropy of alerts to detect recurring multi-stage 
behavior). Impressive validation effort (to manually validate each 
reported infection against multiple sources). 
Weaknesses:  The  algorithm  is  not  well  defined  or  explained  (I 
*think* I got it after reading the relevant section three times). It is 
based  on  manually  set  parameters, and it is not clear how these 
should be set or how they affect accuracy (although, to be fair, I 
have not heard of any IDS system that does not rely on manual 
calibration).  The  evaluation  of  the  algorithm  could  be  (easily) 
improved. 
Comments to Authors: The main weakness of the paper is that it 
does not clearly describe the proposed algorithm. The second half 
of Section 3.3 (which is meant to describe the algorithm) is really 
hard to follow. Here is my understanding of what it says: (i) the 
ideal goal is to detect recurring multi-stage behavior, i.e., detect 
sequences  of  alerts  that  occur  multiple  times;  (ii)  a  simplified 
version  of  this  goal is to detect sequences of 2 alerts that occur 
multiple times (call these “significant sequences”); (iii) to do that, 
the algorithm computes the cross-entropy of each sequence of 2 
alerts  --  low  cross-entropy  means  that  the  sequence  does  occur; 
(iv)  each  significant  sequence  that  is  time-wise  separated  (by 
more than some threshold) from the other significant sequences is 
reported  as  an  infection  incident.  If  this  is  what  the  algorithm 
does, there are two ways to make it clearer: One is to explicitly 
state steps (i) and (ii), which are currently missing. The other is to 
*not*  use  the  term  “rule”  to  refer  to  sequences  of  alerts.  When 
reading an IDS paper, the term “rule” makes the reader think of 
an  IDS  rule,  i.e.,  a  criterion  applied  to  a  packet  to  determine 
42whether  an  alert  should  be  raised.  I  strongly  recommend  using 
some other term -- “alert sequence” or something like that.  
Another  weakness  is  the  use  of  manually  set  parameters  (the  J-
Measure threshold, the time window, and the infection threshold). 
I am not sure that manual calibration is avoidable in IDS systems. 
However, the paper could at least provide some evidence that the 
proposed algorithm is not significantly sensitive to its parameters. 
In particular, the authors could vary the values of the parameters 
and  show  how  that  affects  the  false-positive  rate  of  their 
algorithm.  
In  Section  4.4,  it  would  be  nice  to  see  some  discussion  on  the 
false  negatives  --  what  kind  of  incidents  were  not  detected  the 
algorithm? In line with the previous comment, how does the false 
negative/positive rate of the algorithm (applied on the 28-system 
dataset) change as a function of the parameters of the algorithm?  
Other  (minor)  things  that  I  did  not  understand  about  the 
algorithm:  What  is  the  complexity  of  the  algorithm?  Does  it 
compute  the  cross-entropy  of  all  possible  pairs  of  alerts  within 
each time window? In Section 3.2, I did not get which steps of the 
classification  are  manual  and  which  ones  are  automated  (is  the 
second  step  automated?)  Also,  how  do  the  first  and  third  steps 
differ?  The  first  step  “manually  examined  all  rulesets  and 
identified  groups  that  clearly  characterize  an  attack  or  a 
compromised  host.”  The  third  step  “manually  classified  [the 
remaining  rules]  by  examining  the  details  of  the  signature,  ...” 
They both sound to me like manual examination. 
Reviewer #4 
Strengths: There is an interesting observation on the locality of 
infected nodes. 
Weaknesses:  It  is  not  clear  what  are  the  contributions  of  the 
paper.  The  methodology  used  is  a  compilation  of  previously 
proposed techniques to infer specific attacks. It is not clear how 
general is the proposed methodology. J-measure that is used has 
not been formally introduced and motivated. Despite the fact that 
the  authors  claim  that  their  methodology  is  general  they  only 
address  well  known  infections.  You  repeatedly  mention  that 
intrusion  detection  systems  generate  a  large  number  of  false 
positive rates (99%), but you do not provide any reference on this. 
It  is  not  clear  what  is  the  trade-off  between  low  rate  of  false 
positives and false negatives. 
Comments to Authors: Section 3.3: The use of the J-Measure is 
not well motivated. J-Measure is used throughout the paper. You 
have  to  provide  enough  justification  on  why  this  is  the  right 
metric. As you mentioned there are other metrics have also been 
proposed  in  the  literature.  Thus,  you  have  to  evaluate  them  in 
your dataset. 
Sections  4.2  and  4.3:  Despite  the  fact  that  you  claim  that  your 
framework  is  very  general  when  you  dive  into  the  analysis  and 
evaluation  you  attack  only  a  few  cases  (some  of  them  are  well 
addressed  in  the  literature).  Please  elaborate  more  on  the 
generality  and  applicability  of  your  method.  You  should  also 
clearly state your contributions. 
You  should  provide  a  reference  that  shows  that  off-the-shelf 
intrusion detection tools generate a large number of false positive 
rates,  close  to  99%  as  you  mentioned;  you  have  to  apply  these 
tools  in  your  traces  and  then  show  how  your  proposed  system 
reduces the false positive rate. 
Section 6: The related work is too lengthy (more than two pages) 
and does not contribute much to the understanding of the related 
body of work, not it puts your contributions into context. 
Your  methodology  identifies  9000  infected  machine  in  the 
campus. It is not clear how you validated that all these machines 
are  infected;  is  there  a  ground  truth  that  you  rely  on  for  your 
statistics. 
You claim that the false positive rate is low. How about the false 
negative -- is it high in your study? Is there a fundamental trade-
off between low false positive and false negative rate? 
Reviewer #5 
Strengths:  A  working  method  deployed  at  a  real-world,  large-
scale  site.  I  like  the  balance  of  methodology,  validation,  and 
findings.  The  paper’s  dataset  is  great  and  I  actually  find  it 
refreshing to see work that names the actual site at which traces 
are  collected.  The  paper  is  nicely  executed  and  structured,  and 
mostly well-written. 
Weaknesses:  The  novelty  of  this  paper  is  very  small.  IDS  alert 
aggregation has been studied in depth in the past, and the findings 
in  the  paper  are  mostly  confirmations  of  known  effects.  The 
reduction  of  false  positives  is  good,  but  the  resulting  false 
positives of 16% are still far from great. 
Comments to Authors:  In the abstract you say you “assess the 
security  of  [...]  live  infections”.  What  is  the  security  of  an 
infection supposed to be?  
The idea of malware tending to co-locate in “bad” networks is not 
novel,  see  for  example  Sinha  &  Bailey’s  “Improving  spam 
blacklisting 
thresholding  and  speculative 
aggregation”.  
Despite the high number of false positives it seems you make no 
attempts of tuning your signature set -- why? I can see practical 
reasons  for  doing  so,  including  easy  of  rules  updates,  but 
unfortunately even 16% false positives, viewed in isolation, is a 
lousy result. I personally know of large-scale sites who use other 
intrusion detection approaches than Snort that fare far, far better.  
I struggled with 3.3. You start out saying you want to identify real 
infections, and then present causality inference rules of the form 
“if A then B follows in time window T”, but you never say *why* 
you do that. I suspect you mean that the fact that two alerts (can 
there  be  more  than  two  kinds?)  occurring  in  sequence  increases 
confidence in a particular attack being present. It would help to be 
clearer here.  
I could have done without 4.2, given how tightly you’ve packed 
the paper. Then again, I suspect readers of different backgrounds 
may find it more valuable to have. 4.3, on the other hand, I found 
quite informative. In that section, you also say you believe your 
unaggregated  alerts  to  consist  virtually  completely  of  false 
positives. Can’t you put a precise number to it by reversing your 
bundling procedure?  
I can confirm your hypothesis in the “Infections Impact” section. 
The  pay-per-install  approach  to malware distribution means that 
initial infections can trigger entire cascades of additional malware 
through  dynamic 
43ending up on a system. However, I’m not sure I follow how this 
explains increase in *inbound* attacks on these systems.  
It  would  be  nice  to  see  a  “step  back”  section  in  the  paper.  For 
example,  given  you  experience, 
is  signature-based  misuse 
detection  the  way  to  go?  Would  there  be  point  in  actively 
maintaining  the  signature  set  instead  of  accepting  the  crud  that 
Snort’s  default  ruleset  with  Emerging  Threats  spews  into  your 
logs?  
Response from the Authors 
We made several changes to the paper to address the comments of 
the  reviewers.  Most  reviewers  noted  that  the  description  of  the 
heuristic in Section 3.3 was not very clear. A major change was 
that we substantially clarified Section 3.3. 
Answers  to  specific  questions  (in  the  beginning  we  mark  the 
reviewer number): 
R1:  Our  heuristic  builds  on  attacks  detectable  by  snort,  which 
provides  a  rather  rich  set  of  signatures  for  detecting  attacks  of 
various types. 
R1, R2: We clarified that our heuristic does not classify the type 
of a detected infection; it solely detects an infection.  
R2:  We  made  more  clear  what  it  automated.  The  detection 
process  of  our  heuristic  is  entirely  automated.  It  uses  an 
assignment  of  snort  rules into three classes, which classification 
we derived manually. 
R2: Trivially combining alerts from a host and passing them to an 
administrator would not work because it results in a prohibitively 
large  number  of  suspected  systems  an  administrator  needs  to 
manually inspect. 
R3:  Using  a  set  of  detection  parameters  is  common  in  most 
detection  studies.  We  have  used  security  tickets  for  re-mediated 
incidents  and  have  physically  visited  the  owners  of  infected 
systems  to  derive  a  reliable  ground  truth  for  fine-tuning  the 
detection  threshold  of  our  heuristic.  In  addition,  we  extended 
Section 4.4 to report how the number of false positives/negatives 
changes with the detection threshold. 
R3: We discuss in detail the root-causes of the main types of false 
positives in Section 4.3. 
R4: We provide a reference for the 99% false-positives figure in 
the  introduction.  In  addition,  we  confirmed  this  figure  with  our 
data too based on the suggestion of reviewer 5. 
R4:  We  extended  Section  4.4 
to  report  how 
positive/negative rate changes for different thresholds. 
R5: Although alert correlation has been studied extensively in the 
past, compared to previous studies the main novelty in our work 
is the characterization of 9 thousand infected hosts in a production 
network. 
R5: The authors agree that the false positive rate could be further 
reduced. Some hints for improving it are given in the discussion 
of the root-causes of false positives in Section 4.3. This could be 
interesting future work. 
R5: We liked the suggestion of reversing the bundling process to 
compute 
initial  false  positive  rate.  We  followed  this 
suggestion  and  found  99.4%  false-positive  un-aggregated  alerts 
(excluding policy alerts). This confirms our expectation. 
the  false-
the 
44