descendant model,5 essentially making HTML frames unsafe to use not only
for Charles Schwab but also for any other website. One of the sites main-
tained by the Internal Revenue Service provides the same, extremely incon-
siderate tip.6
The complexity and poor documentation of Internet Explorer’s zone
settings aside, the other problem with the zone model is the clustering of
unrelated permissions. The settings for local intranet and trusted sites containers
enable a random collection of features that may be required by some trusted
sites—but none of the trusted sites could possibly require all of the permissions
the zone entails. Because of this design, adding sites to privileged zones can
once more have unexpectedly far-ranging consequences in the case of, say,
atrivial XSS flaw.
Mark of the Web and Zone.Identifier
To maintain the integrity of the zone model on downloaded files, Internet
Explorer further utilizes two overlapping mechanisms to track the original
zone information for any externally retrieved document:
 Mark of the Web (MotW) This simple pseudo-HTML tag is inserted at
the beginning of HTML documents downloaded via Internet Explorer
to indicate their initial source.7 One example of a MotW tag may be
. The URL recorded in
this tag is mapped to an appropriate zone; the document is then opened
in a unique origin in that zone. The most important consesequence is
that the downloaded content is isolated from other file:URLs.
NOTE The inline nature of MotW is one of its flaws. Faux tags can be pre-
inserted by rogue parties into HTML documents downloaded through
non–Internet Explorer browsers, saved from email clients, or downloaded
by Internet Explorer with a non-HTML extension (and then subjected to
content sniffing). Though, to be fair, the privileges of file: documents
saved without any MotW tags are significant enough to keep attackers
relatively uninterested in hopping from the My Computer zone to, say,
Local Intranet.
 Alternate Data Stream (ADS) Zone Identifier This is a piece of NTFS
metadata attached by Internet Explorer (and Chrome) to every down-
loaded file, indicating the numerical code of the zone the file was
retrieved from.8 The Zone.Identifier mechanism is less portable than
MotW, and the information is lost when files are saved to non-NTFS
filesystems. However, it is also more versatile, as it can be applied to
non-HTML documents.
Zone.Identifier metadata is recognized by Internet Explorer itself, by
the Windows GUI shell, and by some other Microsoft products, but third-
party software almost universally ignores it. Where it is supported, it may
result in a more restrictive security policy being applied to the docu-
ment; more commonly, it just pops up a security warning about the
unspecified risks of opening Internet-originating data.
Extrinsic Site Privileges 231
Security Engineering Cheat Sheet
When Requesting Elevated Permissions from Within a Web Application
Keep in mind that requesting access to geolocation data, video or microphone feeds, and other
privileged APIs comes with responsibility. If your site is prone to XSS vulnerabilities, you are
gambling not only with the data stored in the application but with the privacy of your users.
Plan accordingly and compartmentalize the privileged functionality well. Never ask your users
to lower their Internet Explorer security settings to accommodate your application, and do
not blindly follow this advice when given by others—no matter who they are.
When Writing Plug-ins or Extensions That Recognize Privileged Origins
You are putting your users at elevated risk due to inevitable web application security bugs.
Design APIs robustly and try to use secondary security measures, such as cryptography, to fur-
ther secure communications with your server. Do not whitelist nonencrypted origins, as they
are prone to spoofing on open wireless networks.
232 Chapter 15
PART III
A G L I M P S E O F T H I N G S
T O C O M E
Following nearly a decade of stagnation, the world of
browsers is once more a raging battlefield. In a man-
ner all too reminiscent of the First Browser Wars in the
late 1990s, vendors compete by bringing new features
to market monthly. The main difference is that secu-
rity is now seen as a clear selling point.
Of course, objectively measuring the robustness of any sufficiently
complex piece of software is an unsolved problem in computing, doubly so
ifyour codebase happens to carry almost two decades worth of bloat. There-
fore, much of the competitive effort goes into inventing and then rapidly
deploying new security-themed additions, often with little consideration for
how well they actually solve the problem they were supposed to address.
In the meantime, standards bodies, mindful of their earlier misadventures,
have ditched much of their academic rigor in favor of just letting a dedicated
group of contributors tweak the specifications as they see fit. There is talk of
making HTML5 the last numbered version of the standard and transitioning
to a living document that changes every day—often radically. The relaxation
of the requirement has helped keep ongoing much of the work around W3C
and WHATWG, but it has also undermined some of the benefits of having a
central organization to begin with. Many recent proposals gravitate toward
quick, narrowly scoped hacks that do not even try to form a consistent and
well-integrated framework. When this happens, no robust feedback mecha-
nism is in place to allow external experts to review reasonably stable specifi-
cations and voice concerns before any implementation work takes place. The
only way to stay on top of the changes is to immerse oneself in the day-to-day
dynamics of the working group.
It is difficult to say if this new approach to standardization is a bad thing.
In fact, its benefits may easily outweigh any of the speculative risks; for one,
we now have a chance at a standard that is reasonably close to what browsers
actually do. Nevertheless, the results of this frantic and largely unsupervised
process can be unpredictable, and they require the security community to be
very alert.
In this spirit, the last part of the book will explore some of the more plau-
sible and advanced proposals that may shape the future of the Web . . . or that
may just as likely end up in the dustbin of history a few years from now.
234 Part III
N E W A N D U P C O M I N G
S E C U R I T Y F E A T U R E S
You will soon find out that there is little rhyme and rea-
son to how all the new browser features mesh, but we
still need to organize the discussion in some way. Per-
haps the best approach is to look at their intended
purposes and begin with all the mechanisms created
specifically to tweak the Web’s security model for a
well-defined gain.
The dream of inventing a brand-new browser security model is strong
within the community, but it is always followed by the realization that it would
require rebuilding the entire Web. Therefore, much of the practical work
focuses on more humble extensions to the existing approach, necessarily
increasing the complexity of the security-critical sections of the browser
codebase. This complexity is unwelcome, but its proponents invariably see it
asjustified, whether because they aim to mitigate a class of vulnerabilities,
build a stopgap for some other hard problem that nobody wants to tackle
right now,* or simply enable new types of applications to be built in the
future. All these benefits usually trump the vague risk.
Security Model Extension Frameworks
Some of the most successful security enhancements proposed in the past few
years boil down to adding flexibility to the original constraints imposed by the
same-origin policy and its friends. For example, one formerly experimental
proposal that has now crossed into the mainstream is the postMessage(...) API
for communicating across origins, discussed in Chapter 9. Surprisingly, the
act of relaxing SOP checks in certain carefully chosen scenarios is more intu-
itive and less likely to cause problems than locking the policy down. So, to
begin on a lighter note, we’ll focus on this class of frameworks first.
Cross-Domain Requests
Under the original constraints of the same-origin policy, scripts associated
with one origin have no clean and secure way to communicate with client-
side scripts executing in any other origin and no safe way to retrieve poten-
tially useful data from a willing third-party server.
Web developers have long complained about these constraints, and
inrecent years, browser vendors have begun to listen to their demands. As
you recall, the more pressing task of arranging client-side communications
between scripts was solved with postMessage(...). The client-to-server scenario
was found to be less urgent and still awaits a canonical solution, but there
hasbeen some progress to report.
The most successful attempt to create a method for retrieving docu-
ments from non-same-origin servers began in 2005. Under the auspices of
W3C, several developers working on VoiceXML, an obscure document for-
mat for building Interactive Voice Response (IVR) systems, drafted a pro-
posal for Cross-Origin Resource Sharing (CORS).1 Between 2007 and 2009, their
awkward, XML-based design gradually morphed into a much simpler and
more widely useful scheme, which relied on HTTP header–level signaling
tocommunicate consent to cross-origin content retrieval using a natural
extension of theXMLHttpRequest API.
CORS Request Types
As specified today, CORS relies on differentiating between two types of calls
to the XMLHttpRequest API. When the site attempts to load a cross-origin doc-
ument through the API, the browser first needs to distinguish between simple
requests, where the resulting HTTP traffic is deemed close enough to what
* Malicious URL blacklists, a feature supported by (and usually enabled in) all modern browsers,
are a prime example of this trend. The blacklist is a lightweight, crude substitute for an antivirus,
which is, in turn, a poor substitute for up-to-date and well-designed software. Antimalware fea-
tures do not make individual attacks any more difficult; they are simply meant to stop the large-
scale distribution of unsophisticated malware, based on the assumption that most users are not
interesting enough to be specifically targeted or attacked with something clever.
236 Chapter 16
can be generated through other, existing methods of navigation, and non-
simple requests, which encompass everything else. The operation of these two
classes of requests vary significantly, as we’ll see.
The current specification says that simple requests must have a method
of GET, POST, or HEAD. Additionally, if any custom headers are specified
by the caller, they must belong to the following set:
 Cache-Control
 Content-Language
 Content-Type
 Expires
 Last-Modified
 Pragma
Today, browsers that support CORS simply do not allow methods other
than GET, POST, and HEAD. At the same time, they ignore the recom-
mended whitelist of headers, unconditionally demoting any requests with
custom header values to non-simple status. The implementation in WebKit
also considers any payload-bearing requests to be non-simple. (It is not clear
whether this is an intentional design decision or a bug.)
Security Checks for Simple Requests
The CORS specification allows simple requests to be submitted to the desti-
nation server immediately, without attempting to confirm whether the des-
tination is willing to engage in cross-domain communications to begin with.
This decision is based on the fact that the attacker may initiate fairly similar
cookie-authenticated traffic by other means (for example, by automatically
submitting a form) and, therefore, that there is no point in introducing an
additional handshake specifically for CORS.*
The crucial security check is carried out only after the response is
retrieved from the server: The data is revealed to the caller through the
XMLHttpRequest API only if the response includes a suitable, well-formed
Access-Control-Allow-Origin header. To assist the server, the original request
will include a mandatory Origin header, specifying the origin associated
withthe calling script.
To illustrate this behavior, consider the following cross-domain
XMLHttpRequest call performed from http://www.bunnyoutlet.com/:
var x = XMLHttpRequest();
x.open('GET', 'http://fuzzybunnies.com/get_message.php?id=42', false);
x.send(null);
* That assumption is not completely correct. For example, prior to the introduction of this
scheme, attackers would not have been able to initiate a cross-domain request completely
indistinguishable from the submission of a file upload form, but under CORS, such forgery
ispossible.
New and Upcoming Security Features 237
The result will be an HTTP request that looks roughly like this:
GET /get_message.php?id=42 HTTP/1.0
Host: fuzzybunnies.com
Cookie: FUZZYBUNNIES_SESSION_ID=EA7E8167CE8B6AD93D43AC5AA869A920
Origin: http://www.bunnyoutlet.com
To indicate that the response should be readable across domains, the
server needs to respond with
HTTP/1.0 200 OK
Access-Control-Allow-Origin: http://www.bunnyoutlet.com
The secret message is: "It's a cold day for pontooning."
NOTE It is possible to use a wildcard (“*”) in Access-Control-Allow-Origin, but do so with
care. It is certainly unwise to indiscriminately set Access-Control-Allow-Origin: *
on all HTTP responses, because this step largely eliminates any assurances of the same-
origin policy in CORS-compliant browsers.
Non-simple Requests and Preflight
In the early drafts of the CORS protocol, almost all requests were meant to
be submitted without first checking to see if the server was actually willing to
accept them. Unfortunately, this design undermined an interesting property
leveraged by some web applications to prevent cross-site request forgery:
Prior to CORS, attackers could not inject arbitrary HTTP headers into cross-
domain requests, so the presence of a custom header often served as a proof
that the request came from the same origin as the destination and was issued
through XMLHttpRequest.
Later CORS revisions corrected this problem by requiring a more com-
plicated two-step handshake for requests that did not meet the strict “simple
request” criteria outlined in “CORS Request Types” on page236. The hand-
shake for non-simple requests aims to confirm that the destination server is
CORS compliant and that it wants to receive nonstandard traffic from that par-
ticular caller. The handshake is implemented by sending a vanilla OPTIONS
request (“preflight”) to the target URL containing an outline of the parame-
ters of the underlying XMLHttpRequest call. The most important information
is conveyed to the server in three self-explanatory headers: Origin, Access-
Control-Request-Method, and Access-Control-Request-Headers.
This handshake is considered successful only if these parameters are
properly acknowledged in the response through the use of Access-Control-
Allow-Origin, Access-Control-Allow-Method, and Access-Control-Allow-Headers. Fol-
lowing a correct handshake, the actual request is made. For performance
reasons, the result of the preflight check for a particular URL may be cached
by the client for a set period of time.
238 Chapter 16
Current Status of CORS
As of this writing, CORS is available only in Firefox and WebKit-based brows-
ers and is notably absent in Opera or Internet Explorer. The most important
factor hindering its adoption may be simply that the API is not as critical as
postMessage(...), its client-side counterpart, because it can be often replaced
by a content-fetching proxy on the server side. But the scheme is also facing
three principal, if weak, criticisms, some of which come directly from one of
the vendors. Obviously, these criticisms don’t help matters.
The first complaint, voiced chiefly by Microsoft developers and echoed
by some academics, is that the scheme needlessly abuses ambient authority.
They argue that there are very few cases where data shared across domains
would need to be tailored based on the credentials available for the destina-
tion site. The critics believe that the risks of accidentally leaking sensitive
information far outweigh any benefits and that a scheme permitting only
nonauthenticated requests to be made would be preferable. In their view,
any sites that need a form of authentication should instead rely on explicitly
exchanged authentication tokens.*
The other, more pragmatic criticism of CORS is that the scheme is need-
lessly complicated: It extends an already problematic and error-prone API
without clearly explaining the benefits of some of the tweaks. In particular, it
is not clear if the added complexity of preflight requests is worth the periph-
eral benefit of being able to issue cross-domain requests with unorthodox
methods or random headers.
The last of the weak complaints hinges on the fact that CORS is suscep-
tible to header injection. Unlike some other recently proposed browser fea-
tures, such as WebSockets (Chapter 17), CORS does not require the server to
echo back an unpredictable challenge string to complete the handshake. Par-
ticularly in conjunction with preflight caching, this may worsen the impact of
certain header-splitting vulnerabilities in the server-side code.
XDomainRequest
Microsoft’s objection to CORS appears to stem from the aforementioned
concerns over the use of ambient authority, but it also bears subtle overtones
of their dissatisfaction with interactions with W3C. In 2008, Sunava Dutta, a
program manager at Microsoft, offered this somewhat cryptic insight:2
During the [Internet Explorer 8] Beta 1 timeframe there were
many security based concerns raised for cross domain access of
third party data using cross site XMLHttpRequest and the Access
Control framework. Since Beta 1, we had the chance to work with
other browsers and attendees at a W3C face-to-face meeting to
improve the server-side experience and security of the W3C’s
Access Control framework.
* The same claim can be made about the use of HTTP cookies in any other setting and seems
equally futile. It is true that ambient credentials cause problems more frequently than some
other forms of explicit authentication would, but they are also a lot more convenient to use
andare simply not going away.
New and Upcoming Security Features 239
Instead of embracing the CORS extensions to XMLHttpRequest, Micro-
soft decided to implement a counterproposal, dubbed XDomainRequest.3 This
remarkably simple, new API differs from the variant available in other brows-
ers in that the resulting requests are always anonymous (that is, devoid of any
browser-managed credentials) and that it does not allow for any custom HTTP
headers or methods to be used.
The use of Microsoft’s API is otherwise very similar to XMLHttpRequest:
var x = new XDomainRequest();
x.open("GET", "http://www.fuzzybunnies.com/get_data.php?id=1234");