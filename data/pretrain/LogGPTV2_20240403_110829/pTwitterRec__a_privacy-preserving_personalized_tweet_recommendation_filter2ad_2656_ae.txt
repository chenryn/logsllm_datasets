fense mechanisms to future work. 
6.2  Privacy Analysis 
In  this  subsection,  we  examine  the  user  information  disclosed  to 
the SP and  the  WS  at  each  stage  of  pTwitterRec  and  analyze  the 
privacy threats. 
Word indexing. When user u requests the index of word w from 
the SP, the SP learns neither word w nor the corresponding index, 
because word w is encrypted using commutative encryption with a 
secret  key  known  only  to  user  u.  Upon  receiving  the  index  of 
word  w  from  the  SP,  user  u  caches  the  index  corresponding  to 
word w locally so user u does not need to request the index of the 
same word again in the future. Therefore, the SP learns no infor-
mation  other  than  that  user  u  has  made  such  a  request.  If  the  SP 
pre-computes the indexes for all popular words in the tweet voca-
bulary  as  described  in  Section  5.2,  user  u  reveals  no  personal 
information by downloading such indexes from the SP in advance. 
Explicit  feature  computation  and  training  sample  submission. 
For a given tuple  in D, when user u finishes calculating 
the explicit features for tweets h and k and submitting the corres-
ponding training dataset to the SP as described in Section 5.3, the 
SP  learns:  a)  the  identities  of  the  publishers  of  tweets  k  and  h, 
which  is  not  private  information  in  our  threat  model;  b)  the  en-
crypted indexes for words contained in tweets k and h, which are 
encrypted  using  Ews  with  the  WS’s  public  key  Pukws.  The  SP 
learns neither the words contained in the tweets nor their indexes. 
In  addition,  the  SP  does  not  even  learn  how  frequent  the  same 
371word has appeared among all tweets in D because Ews is a proba-
bilistic  encryption  scheme;  c)  the  difference  between  the  explicit 
features  (computed  by  user  u)  computed  from  tweets  k  and  h, 
denoted  as 
.  Among  all  explicit  features  com-
{
r
u k
j
∈
}
−
r
|
,
,
u h
j
j F
u
puted  by  user  u,  features  Relevance  to  Tweet  History,  Relevance 
to  Retweet  History  and  Relevance  to  Hash  Tags  do  not  leak  any 
personal information about user u because all tweets received and 
published  by  user  u  are  hidden  from  the  SP.  Features  Length  of 
Tweet, Hash Tag Count and URL Count do reveal the number of 
words,  the  number  of  hash  tags  and  the  number  of  URLs  con-
tained in a tweet. However, we do not consider such information 
to  be  sensitive  for  user  u.  In  addition,  user  u  only  submits  the 
difference between those features computed from tweets k and h; 
d)  the  SP  computes  some  explicit  features  of  tweets  k  and  h  as 
listed  in  Table  1.  However,  those  features  only  depend  on  non-
sensitive  information,  which  is  already  available  to  the  SP,  such 
as user u’s social relations, and therefore do not  leak  any  private 
information about user u.  
Model learning. At the model learning stage, the SP and the WS 
cooperate  to  update  the  parameters  of  the  tweet  recommendation 
model. For a given tuple  in D, a) at the first step of the 
protocol,  the  SP  only  forwards  the  encrypted  indexes  for  words 
contained in tweets k and h to the WS. The WS does not learn the 
original  words  contained  in  tweets  k  and  h  but  only  learns  their 
indexes. In  addition,  the  WS  learns  neither  the  identity  of  user u 
nor the identity of the publishers of tweets k and h. Furthermore, 
we require that users randomly mix the order of indexes of words 
contained in the tweet before encrypting and submitting them, so 
the WS cannot identify words by analyzing the pattern of the or-
der of words appearing in tweets. The WS does learn the frequen-
cy  of  each  word  that  has  appeared  in  all  tweets  contained  in  D 
while  not  knowing  the  actual  words.  However,  as  analyzed  in 
Section 6.1, such frequency analysis attacks  reveal  little  personal 
information regarding each individual user. At the end of the first 
∑ from  the 
step, the SP learns  the  result  of 
−∑
q
q
w
w
∈
w T
k
∈
w T
h
1
k
Z
1
h
Z
WS,  which  does  not  reveal  the  individual  latent  factor  of  each 
word because all word indexes were encrypted using probabilistic 
encryption scheme Ews with the WS’s public key Pukws;  b)  at  the 
third step of the protocol, the WS learns the result of  ˆ uep from the 
SP, which does not reveal the identity of user u;  c)  at  the  end  of 
the model learning stage, the SP learns the latent factor pu of user 
u as recipients of tweets and the latent factor dp(k) of user  p(k)  as 
the publisher of tweet k (note that pu is a vector in the latent fea-
ture space Rd and pu alone does not disclose any personal informa-
tion of user u). The SP is able to learn the closeness of the social 
. However, 
relation between users u and p(k) by computing 
p d
T
u
p k
(
)
in pTwitterRec, we assume that the social relations between users 
are  not  hidden  from  the  SP,  as  described  in  the  threat  model;  d) 
the  SP  cannot  replace  the  WS  by  updating  the  latent  factors  of 
words  as  well,  because  the  indexes  for  words  are  encrypted  by 
user  u  using  the  probabilistic  encryption  scheme  with  the  WS’s 
public  key;  e)  the  WS  cannot  replace  the  SP,  because  the  WS 
cannot compute the model all by itself without knowing the expli-
cit features, the identity of the recipient of the tweet, and the iden-
tity of the publisher of the tweet, which are only known to the SP. 
Tweet  publishing,  receiving  and  ranking.  For  tweet  k  received 
by  user  u,  the  SP  and  the  WS  both  only  learn  the  value  of 
1
k
Z
∑ for all words contained in tweet k but do not know the 
q
w
∈
w T
k
words contained in tweet k. In addition, neither the SP nor the WS 
u wp q  for  any  word  w 
can  infer  user  u’s  interest  by  computing 
(which  represents  user  u’s  preference  over  word  w)  without  col-
luding  with  each  other.  Furthermore,  for  user  u,  we  require  that 
user u is only allowed to retrieve her own personal latent factor pu 
from  the  SP  and  therefore  user  u  cannot  learn  other  users’  inter-
ests. 
T
6.3  Extension 
In  pTwitterRec,  the  SP  is  not  only  responsible  for  managing  the 
social  relations  between  users  and  distributing  users’  tweets  to 
their  followers  as  an  OSN  provider,  but  also  responsible  for  re-
commending personalized interesting tweets to users (in coopera-
tion with the WS and users) as a recommendation service provider. 
However,  pTwitterRec  can  be  conveniently  adapted  to  support  a 
third-party privacy-preserving tweet  recommendation  service  that 
is independent of the OSN provider. The benefit of implementing 
pTwitterRec as a third-party service is that there would be no  re-
quirements  of  any  changes  to  the  existing  OSN  provider  while 
still  providing  users  with  personalized  tweet  recommendations 
(and hiding the contents of users’ tweets and user’s interests from 
the tweet recommendation service provider). 
With the new pTwitterRec client application, users receive/publish 
tweets from/to the OSN provider, and interact with the third-party 
tweet  recommendation  service  provider  (consisting  of  a  separate 
SP and WS) to learn the  recommendation  model  without  leaking 
any sensitive information to the recommendation service provider. 
Upon the completion of the model learning stage, users are able to 
rank  tweets  received  from  the  OSN  provider.  pTwitterRec  may 
lose some recommendation accuracy because the recommendation 
service provider is unable to compute some explicit features such 
as features that depend on users’ social relations. However, users 
may  be  able  to  compute  such  features  if  they  know  their  follo-
wees’ social relations. 
7.  IMPLEMENTATION 
To  demonstrate  the  practicality  of  our  framework,  we  imple-
mented  a  prototype  that  simulates  the  model  learning  stage  of 
pTwitterRec, which is the core component of our framework. The 
prototype consists of: a server acting as the SP that takes the train-
ing datasets D as input and updates the weight bj for each explicit 
feature, the latent factor pu of user u as the recipient of the tweets, 
and the latent factors dp(k) and dp(h) of users p(k) and p(h) as pub-
lishers  of  tweets  k  and  h  for  every  tuple    in  D;  another 
server  acting  as  the  WS  that  communicates  with  the  SP  as  de-
scribed  in  Section  5.4  and  updates  the  latent  factor  qw  of  each 
word w contained in D. 
Our  prototype  uses  the  SVDFeature  toolkit  [37]  to  implement 
gradient stochastic descent, which is used for updating abovemen-
tioned  parameters  at  the  model  learning  stage.  We  choose 
SVDFeature  since  it  is  open  source  and  well-documented. 
SVDFeature  is  a  toolkit  designed  to  efficiently  solve  large-scale 
collaborative  filtering  problems  with  auxiliary  information  by 
performing  gradient  stochastic  descent.  Unlike  traditional  ap-
proaches  that  require  writing  a  specific  solver  for  each  recom-
mendation  model,  SVDFeature  provides  a  general  solution  for 
collaborative  filtering  problems  and  allows  developing  new  rec-
ommendation models by defining new features.  
372 
Figure 3. Server-side architecture. 
In our prototype, we define the explicit features described in Sec-
tion  5.3  as  global  features  in  SVDFeature,  the  latent  factors  of 
users (as recipients of tweets) as user features in SVDFeature, and 
the  latent  factors  of  words  contained  in  the  tweet  vocabulary  in 
addition to the latent factors of users  (as  publishers  of  tweets)  as 
item features in SVDFeature. Figure 3 depicts the architecture of 
our prototype. The SP and the WS each maintains a separate data-
base. At the end of the  model  learning  stage,  the  SP  stores  in  its 
database  the  final  values  of  the  system-wide  weight  bj  for  each 
explicit feature, the latent factor pu of each user as the recipient of 
tweets, and the latent factors dp(k) of each user as the publisher of 
tweets;  the  WS  stores  in  its  database  the  latent  factor  qw  of  each 
word in the tweet vocabulary. We leave the implementation of the 
complete framework to future work. 
8.  EVALUATION 
In this section, we analyze and evaluate the overhead particularly 
on the user side due to the introduction of our privacy protection 
framework  pTwitterRec,  compared  with  the  original  tweet  rec-
ommendation algorithm [10] with no user privacy protection. Our 
framework only adds privacy protection to the tweet recommenda-
tion  algorithm  without  modifying  the  original  recommendation 
model  [10].  Thus,  there  is  no  loss  of  recommendation  accuracy 
(please refer to [10] for the thorough evaluation of recommenda-
tion accuracy). 
We assume that users run the pTwitterRec client application on a 
smartphone with limited computation  power  and  memory.  In  our 
evaluation, we use a Google Nexus Four phone featuring a 1.512 
GHz quad-core Krait CPU and 2 GB of RAM. On the other hand, 
we  assume  that  both  the  SP  and  the  WS  have  reasonably  unli-
mited  computation  power  and  storage  space.  We  use  an  8  core 
server featuring Intel Xeon 2.40 GHz CPU with 8 GB RAM as the 
SP/WS.  We  assume  that  there  is  a  fast  and  persistent  network 
connection  between  the  SP  and  the  WS  and  the  communication 
overhead between them is negligible. We analyze the overhead at 
each stage of our framework in following subsections. 
8.1  Word Indexing 
As described in Section 5.2, assuming that user u does not know 
the index of word w, user u first encrypts word w using commuta-
tive  encryption  Ecomm  with  her  secret  key  ku  and  then  sends  the 
encrypted  word  to  the  SP.  The  SP  further  encrypts  the  received 
encrypted word using Ecomm with its secret key ksp and returns the 
result  to  user  u.  Finally,  user  u  obtains  the  index  of  word  w  by 
decrypting the result received from the SP with her decryption key 
and then computing  the  hash  value  of  the  result  using  SHA-224. 
Therefore, to obtain the index of a new word, it requires user u to 
perform  one  encryption  and  one  decryption  using  commutative 
Figure 4. Pohlig-Hellman encryption/decryption. 
encryption Ecomm and perform one hash function while it requires 
the SP to perform one encryption using Ecomm. The computational 
cost for carrying out one hash function is negligible and therefore 
we only evaluate the computational cost for commutative encryp-
tion/decryption.  We  adopt  Pohlig-Hellman  encryption  (described 
in  Appendix  A.2)  as  Ecomm  for  our  evaluation  because  of  its  de-
terministic  commutative  property  (note  that  any  encryption 
scheme that is both deterministic and commutative can be used as 
Ecomm  in  our  protocol,  as  described  in  Section  5.2).  We  measure 
the  execution 
time  of  a  single  Pohlig-Hellman  encryp-
tion/decryption on the smartphone and on the server. We vary the 
key length, carry out 10,000 runs for each key length and average 
the  results.  Figure  4  shows  the  results.  With  a  2048-bit  key,  it 
takes approximately 91.1 ms for the smartphone  and  35.2  ms  for 
the  SP  to  carry  out  one  Pohlig-Hellman  encryption/decryption. 
The communication overhead for user u is to send and receive one 
encrypted word. Therefore, with a 2048-bit key size, the commu-
nication overhead for user u is approximately 512 bytes to obtain 
the index of one word. It is reported that there are approximately 
15 words [36] in each tweet on average. Assuming that each user 
submits n (n = 392 in [10]) tweets to the SP as training datasets, 
the total communication overhead for a user to obtain all indexes 
from  the  SP  is  less  than  7.5n  KB  and  the  total  computational 
overhead  is  less  than  2.7n  seconds.  Note  that  user  u  caches  the 
index  of  word  w  locally  and  thereby  only  needs  to  request  the 
index of word w from the SP once even if word w appears in mul-
tiple  tweets.  In  addition,  to  further  reduce  the  overhead  on  the 
user side, as described in Section 5.2, we propose that the SP pre-
computes  the  indexes  for  popular  words  and  users  pre-download 
the indexes for these popular words beforehand without revealing 
their interests to the SP. 
8.2  Explicit Feature Computation and Train-
ing Sample Submission 
In  pTwitterRec,  users  are  responsible  for  computing  six  of  the 
explicit features for each tweet contained in the training samples. 
Compared  with  the  cryptographic  overhead  evaluated  in  Section 
8.1, the computational overhead incurred by explicit feature com-
putation  is  negligible  for  users.  For  each  tweet  contained  in  the 
training  samples,  Relevance  to  Tweet  History  feature  and  Relev-
ance  to  Retweet  History  feature  each  takes  four  bytes,  and  the 
remaining four integer-value features each takes two bytes.  
Furthermore, users encrypt the indexes of words contained in the 
tweet using probabilistic encryption Ews with the WS’s public key 
Pukws.  The  size  of  each  word  index  is  28  bytes  (recall  that  users 
373Table 2. User side overhead (each user submits n tweets for training) 
Computational Overhead  
Communication Overhead  
Word Indexing 
<2.7n seconds 
Explicit Feature Com-
putation and Training 
Sample Submission 
Model Learning 
Tweet Publishing and 
Receiving (one tweet) 
0.1n seconds 
none 
Publisher: 91.1 ms 
Receiver: none 
Tweet Ranking 
negligible 
<7.5n KB 
0.7n KB 
none 
Publisher: 0.7 KB 
Receiver: 8 bytes 
none 
adopt SHA-224  to  compute  the  indexes  for  words)  and  therefore 
the plaintext of all indexes for a tweet is 420 bytes on average. We 
propose that users encrypt all indexes using Advanced Encryption 
Standard  (AES)  in  CBC  mode  with  a  random  secret  key  kr  and 
then encrypt the secret key using  RSA  with  the  WS’s  public  key 
Pukws  (note  that  any  probabilistic  public-key  encryption  scheme 
can  be  used  as  Ews  in  our  protocol,  as  described  in  Section  5.3). 