describes our approach consisting of the following steps:
Technical Language Embedding. While there are only a few
protocol RFCs, there are large amounts of technical documents
discussing them and other related networking concepts that
provide the background for understanding the RFC. These
documents include technical forums, blogs, research papers, and
speciﬁcation documents. We exploit these documents to learn
a distributed word representation, also known as an embedding
model, for technical language. The main advantage of this step
is that it is an unsupervised process, and we do not require
any annotations. Learning these representations will allow us
to carry over information from the networking domain to our
next step. Section IV describes this step in detail.
Zero-Shot Protocol Extraction. Once we have this represen-
tation, we turn our attention to learning a model to extract
information regarding the FSM from the RFCs. To do this, we
deﬁne a grammar that describes a higher-level abstraction of
the structure of a general FSM for network protocols. While
general, this abstraction will allow us to leverage different
protocols to learn to extract this information, even when the
underlying structure of their documents, the way the FSM is
described and the speciﬁc names of variables, events and states
vary between protocols. We explain this grammar in Section III.
We annotate a set of six protocols, and use a zero-shot learning
approach, in which the document for the predicted protocol
is completely unobserved during training. The output of this
step is a generic representation referred to as the intermediate
representation. Section V describes this step in detail.
Protocol State Machine Extraction. The extracted informa-
tion structured according to our general protocol grammar must
be converted into an actual FSM implementing the described
protocol. We use a set of heuristics to extract an FSM from
the intermediate representation, as detailed in Section VI.
III. FINITE STATE MACHINE GRAMMAR
We deﬁne a general grammar to represent the state machine
for the pertinent network protocols in their corresponding RFC
speciﬁcation documents. We use this grammar to annotate the
segment of texts that describe the states, variables, and events
that are relevant to the state machine, as well as the actions
and the logical ﬂow describing their behavior. Annotations are
done using XML. We consider four types of annotation tags:
deﬁnition tags, reference tags, state machine tags, and control
ﬂow tags, which are formalized below. Finally, we formulate
the grammar in Backus-Naur Form in Figure 2.
A. Deﬁnition Tags
Deﬁnition tags are used to annotate the names of states,
events, and variables that are relevant to each protocol. These
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:58:43 UTC from IEEE Xplore.  Restrictions apply. 
are text segments that are referenced throughout the document,
and stake a role in deﬁning the state machine. For example, a
message may be tagged as an event if the receipt of such a
message leads to a state transition.
State deﬁnition. When the name of a state is introduced in
the text, it is annotated as such. Speciﬁcally, the 
surrounds the ﬁrst usage of the state name that is part of
some discernible pattern. Often this pattern is in the form of
a newline-delimited list or bullet points, but can also appear
as a comma-delimited list. The tag also includes an identiﬁer
that is unique among the states. For example IDLE, where ## is replaced by the
identiﬁer. We assign state identiﬁers (SID) as monotonically
increasing integers in order of ﬁrst appearance. Punctuation
trailing the state name is not included in the tag. These tags
and SIDs will be referenced by reference tags.
Event deﬁnition. Events are also annotated to be referenced
throughout the text. Events follow the same annotation con-
ventions as states and use the annotation form: . We will refer to unique event identiﬁers as EIDs.
Variable deﬁnition. Variables are deﬁned in a similar way
to events and states, however they do not include an analog to
SIDs or EIDs, because they are not explicitly referenced by
annotation in the rest of the text.
B. Reference Tags
When an event or state occurs in the text, it must be linked to
an event or state which was tagged. They need to be explicitly
deﬁned because sometimes the proper name of a state or event
will not be used. For example, an RFC may formally refer to
one event as “ACK”, but throughout the text these ACKs may
also be referred to as “acknowledgments”. These are really the
same event, and the reference tags are used to clarify that.
State reference. States are referenced by surrounding the
state’s name throughout the text with the 
tag, where ## corresponds to the appropriate SID that
was included with the state’s  tag. Punctuation
trailing the state name is not included in the tag. An ex-
ample might look like the following: enter SYN-SENT state.
Event reference. Events follow the same convention as state
references. The event reference must also include the type
of event, where the three possible types are: send, receive,
and compute. Type tags are included as XML attributes,
and will appear as in the following example: a SYN segment.
C. State Machine Tags
We deﬁne a set of ﬁve tags to represent the state machine
logic. These are the crux of the annotation.
Transition. Denotes a state change that happens in the
tags ,
specify
and
in
For
The server moves from the
given context. We use
the
segment
example,
OPEN state, possibly
argument
playing
that
role.
the
text
to
through the CLOSEREQ state,
to CLOSED. Note
that in this case, the mentions to “OPEN”, “CLOSEREQ” and
“CLOSED” would also be enclosed in a  tag. In
cases where the text is not explicitly annotated with argument
tags, the states mentioned are assumed to be the ending states
of the transition.
Variable. Certain variables may be tracked as part of the
state machine. This tag should be used to surround any
logic that indicates that any of these variables are altered
or set to a new value. For example, SND.UP 
Timer. This tag is used if a timer value is changed or set.
For example, start the time-wait timer.
Error. If a context results in an error or warning being
thrown, the error message is then surrounded by this tag.
For example, signal the user error: connection
aborted due to user timeout.
Action. If a given context demands that some action be
taken, we use this tag. We speciﬁcally mark three types
of actions: send, receive and issue. Type tags are included
as XML attributes. We use an argument
tag  to
specify the argument
in the text being sent, received or
computed. For example: Send a
SYN segment. Note that in this case, the men-
tion to “SYN” would also be enclosed in a  tag:
SYN. Additionally,
there
are certain events that are ambiguous in terms of how they
relate to the state machine, in those cases, this tag can be used
without further speciﬁcations.
D. Flow Control Tags
A  tag is introduced to indicate that some ﬂow
control or conditional logic is about to follow. The ﬂow control
logic should contain a  tag, which captures the event
that triggers some action in the state machine, followed by
one or more of the state machine tags. A single block of
control tags may contain multiple state machine tags. These
state machine tags should be in the form of a list. In this case,
the implication is that the state machine tags should all be
executed if the initial trigger condition is true. Figure 7 in
Appendix A shows an example of a list of events within one
control block from the TCP RFC (a.k.a. RFC 793) [28].
E. Grammar
Let engl denote any valid string in the English language.
Then, the grammar for the state machine annotation can be
described in Backus-Naur Form as observed in Figure 2. Here,
relevant=true indicates that the corresponding annotation
is relevant to the protocol state-machine.
IV. TECHNICAL LANGUAGE EMBEDDING
In this section we describe our approach to learn distributed
word representations for technical
language. We start by
providing some background about the techniques used to learn
these representations, then we describe our embedding in detail.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:58:43 UTC from IEEE Xplore.  Restrictions apply. 
454
::= true | false
::= send | receive | issue
::= def_state | def_var | def_event
bool
type
def-tag
ref-state ::= ref_state id="##"
ref-event ::= ref_event id="##" type="type"
ref-tag
def-atom
sm-atom
sm-tag
act-atom
act-struct::= act-struct | act-struct act-atom
trn-arg
trn-atom
trn-struct::= trn-struct | trn-struct trn-atom
ctl-atom
::= ref-event | ref-state
::= engl
::= engl | engl
::= trigger | variable | error | timer
::= sm-atom | sm-atom
::= arg_source | arg_target | arg_inter
::= sm-atom | sm-atom
::= sm-atom
| act-struct
| trn-struct
| sm-atom
ctl-struct::= ctl-atom | ctl-struct ctl-atom
ctl-rel
control
e
::= relevant=bool
::= ctl-struct
::= control | ctl-atom | def-atom
|
e_0 e_1
Fig. 2: BNF grammar for RFC annotation.
A. Background
Distributed representations of words aim to capture meaning
in a numerical vector. Unlike symbolic representations of
words, that use binary values to signal if the words are present
or not, word embeddings have the ability to generalize by
pushing semantically similar words closer to each other in
the embedding space. When using binary representations of
words, we can only consider features that we have seen during
training. Consider a scenario in which during training, we only
have access to DCCP. If we were to test our learned model
on TCP, we could not represent words that were not observed
during training.
Several models have been suggested to learn distributed word
representations. Some approaches rely on matrix factorization
of a general word co-occurrence matrix [29], while other
approaches use neural networks trained to predict the context
surrounding a word, and in the process, learn efﬁcient word
embedding representations in their inner layers [30], [31]. In
this paper we focus on contextualized word representations.
Unlike static word representations that learn a single vector for
each word form, contextualized representations allow the same
word form to take different meanings in different contexts.
For example, in the sentence “The connection is in error and
should be reset with Reset Code 5”, the word “reset” has two
different meanings. Contextualized representations compute
different vectors for each mention.
State-of-the-art pre-trained language models provide a way
to derive contextualized representations of text, while allowing
practitioners to ﬁne-tune these representations for any given
classiﬁcation task. One example of such models is BERT
(Bidirectional Encoder Representations from Transformers)
[32]. BERT is built using a Transformer, a neural architecture
that learns contextual relations between words in a word
sequence. A Transformer network includes two mechanisms,
an encoder that reads the input sequence, and a decoder that
predicts an output sequence. Unlike directional models that
read the input sequentially, Transformer encoders read the
whole sequence at once, and allow the representation of a
given word to be informed by all of its surroundings, left and
right. Details regarding the Transformer architecture can be
found in the original paper [33].
To learn representations, BERT uses two learning strategies,
masked language modeling and next sentence prediction. The
ﬁrst strategy masks 15% of the words in each sentence, and
attempts to predict them. The second strategy uses pairs of
sentences as input, and learns to predict whether the second
sentence is the subsequent sentence in the original document.
Figure 3 illustrates this process. BERT models were pre-trained
on the BooksCorpus (800M words) and English Wikipedia
(2,500M words) and are publicly available1.
Fig. 3: BERT pre-training.
B. Our Embedding
While we could use pre-trained language models directly
for predicting FSM tags, we note that these models were
trained on general document repositories. To obtain a model
that better represents the domain vocabulary, we further
pre-train BERT using the masked language model and the
next sentence prediction objective using networking data. We
collected the full set of RFC documents publicly available
in ietf.org and rfc-editor.org. These documents
cover different aspects of computer networking, including
protocols, procedures, programs, concepts, meeting notes and
opinions. The resulting dataset consists of 8,858 documents
and approximately 475M words. Note that we do not need any
supervision for this step.
Previous ﬁndings suggest that further pre-training large
language models on the domain of the target task consistently
improves performance [34]. Our experiments in Section VIII
support this hypothesis.
1https://github.com/google-research/bert
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:58:43 UTC from IEEE Xplore.  Restrictions apply. 
555
V. ZERO-SHOT PROTOCOL INFORMATION EXTRACTION
In this section we describe our design for a protocol
information extraction system. Our main goal is to have a
system that can adapt to new, unobserved protocols without
re-training the system. To support this, we build on the general
grammar introduced in Section III that focuses on concepts
relevant to a wider set of protocols and takes advantage of the
technical language embedding described in Section IV.
A. Sequence-to-Sequence Model
To parse speciﬁcation documents, we designed a sequence-to-
sequence model that receives text blocks as input, and outputs
a sequence of tags corresponding to the grammar described in
Section III. To tag the text, we use BIO (Beginning, Inside,
Outside) tag labels. Text blocks correspond to paragraphs in the
RFC document. Initially, we segment paragraphs into smaller
units (e.g. individual words, chunks or phrases). Then, we map
each unit to a particular tag. To illustrate this process, consider
the parsed statement in Figure 4, mapping chunks to BIO-tags.
Fig. 4: BIO example.
We consider two models to learn the sequence to sequence
mapping: a linear model we refer to as LINEARCRF, and a
neural model based on the BERT embedding, which we refer
to as NEURALCRF.
Linear-Chain Conditional Random Fields (LINEARCRF)
works on a set of extracted features over each chunk. Condi-
tional Random Fields model the prediction as a probabilistic
graphical model; Chain Conditional Random Fields speciﬁcally
consider sequential dependencies in the predictions [35].
Let y be a tag sequence and x an input sequence of textual
units. We want to maximize the conditional probability:
(cid:80)
T(cid:89)
p(y|x) =
p(x, y) =
p(y, x)
y(cid:48) p(y(cid:48), x)
exp(f (yt, yt−1, xt; θ))
Fig. 5: NEURALCRF.
using a BiLSTM. A softmax activation is used to obtain scores
for the labels. Finally, we add a CRF layer on top. This way,
we are able to leverage the sequential dependencies both in
the representation and in the output space [37], [38]. Note that
BERT enforces a limit of 512 tokens per sequence, which is
not enough to represent some of our control sequences. For
this reason, we leverage a BiLSTM instead of using the CRF
layer directly over BERT.
To formalize the NEURALCRF model, we ﬁrst consider a
textual unit containing n words (w0, w1, ..., wn−1). A BERT
encoder is used to obtain a single representation u for the full
textual unit, resulting in a d-dimensional vector.
Then, a BiLSTM computes a representation over the se-
quence of embedded textual units (u0, u1, ..., um−1) to obtain
−→
representations ht = [
←−
ht
represents the left context of the sequence, and
ht represents
the right context, at every unit t.
←−
ht] for every textual unit t. Here,
−→
ht;
Finally, we add a CRF layer over these representations by
replacing the function f in Eq. 1 with:
(1)
f (yt, yt−1, xt) = ht + Pyt,yt−1
(2)
Where xt represents the input for that textual unit, ht is the
representation of the textual unit computed with our model and
P is a learned parameter matrix representing the transitions
between labels. Like in the linear CRF case, we minimize
the negative log likelihood, − log p(y, x), to jointly learn the
parameters of the BERT encoder, the BiLSTM layer, and the
transition matrix P .
Predictions for both models are done using the Viterbi
algorithm. Viterbi is a dynamic programming algorithm for
ﬁnding the most likely sequence of states. Viterbi takes into
account both emission (h2
t ), and transition (Pyt,yt−1) scores
at each unit t in the sequence.
B. Features
For each textual unit in the input, we extract a set of features
to capture properties about the input and help us make a correct
656
t=1
Where f is a linear scoring function learned with parameter
vector θ over a feature vector xt. To learn θ, we minimize
the negative log-likelihood − log p(y, x). Learning is made
tractable by using the forward-backward algorithm to calculate
the partition function Z(x) =(cid:80)
y(cid:48) p(y(cid:48), x).
The second model considered is a BERT encoder enhanced
with a Bidirectional LSTM CRF layer (NEURALCRF). LSTMs
are recurrent neural networks, a class of neural network where
connections between nodes form a directed graph along a
sequence [36]. We outline this model in Figure 5. The BERT
encoder is used to create chunk-level representations from word
sequences. The resulting sequence of chunks is then processed