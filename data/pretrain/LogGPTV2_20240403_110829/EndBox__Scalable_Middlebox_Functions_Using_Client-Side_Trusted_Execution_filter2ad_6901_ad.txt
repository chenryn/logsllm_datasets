]
s
m
[
T
T
R
g
n
i
P
200
100
0
202.3
no redirection
local redirection
EndBox SGX
AWS eu-central
AWS us-east
10.8
11.3
11.5
17.4
Fig. 7: Average ping RTT for different redirection methods
ENDBOX or a direct connection, and hence that the latency
overhead of ENDBOX is negligible.
Trafﬁc redirection. By further exploring ENDBOX’s impact
on latency, we want
to show that ofﬂoading middlebox
functions to cloud environments has more disadvantages than
only losing control and trust. Inspired by [4], we create a
setup of software middleboxes executing in AWS EC2 and
measure the ping round-trip times (RTTs) to a ﬁxed location.
Fig. 7 shows the average ping RTT for different redirection
methods: (i) no redirection with no middlebox or VPN;
(ii) local redirection through a VPN and server-side middlebox
using OpenVPN+Click; (iii) redirection trough ENDBOX; and
(iv) redirection through middleboxes deployed on EC2 instances
in different AWS regions, also using OpenVPN+Click. The
results show that depending on the location a cloud provider
chooses, the latency overhead ranges between 61% and 1773%
and that ENDBOX’s latency overhead is only 6%.
Handling of encrypted trafﬁc. As mentioned in III-D,
ENDBOX is able to transparently decrypt TLS trafﬁc. We
measure the overhead of this functionality by letting an
HTTPS client fetch static web pages of different sizes from
a web server. This client is using one conﬁguration among:
(i) ENDBOX with custom OpenSSL and trafﬁc decryption
inside Click; (ii) ENDBOX with custom OpenSSL but without
trafﬁc decryption; or (iii) ENDBOX with system OpenSSL
and without trafﬁc decryption. We measure the HTTPS GET
request latency, and report the results in Table I. They show
that the overhead introduced by our custom OpenSSL and
trafﬁc decryption is less than 8%. The two sources of overhead
are ENDBOX’s custom OpenSSL forwarding of keys to the
enclave, and the actual decryption.
ENDBOX OpenSSL vanilla OpenSSL
Resp. size w/ dec
1.08 ms
4 KB
16 KB
1.34 ms
32 KB
1.78 ms
w/o dec
1.04 ms
1.29 ms
1.75 ms
w/o dec
1.00 ms
1.26 ms
1.70 ms
TABLE I: HTTPS GET request latency for different response
sizes and conﬁgurations
vanilla OpenVPN EndBox SIM
OpenVPN+Click EndBox SGX
8
6
1
,
3
3
1
8
,
2
9
5
6
,
2
2
3
1
,
2
4
7
6
,
2
5
2
3
,
2
7
8
9
,
1
8
8
8
,
1
1
4
5
,
1
4
1
5
,
1
8
8
2
,
1
4
4
0
,
1
2
4
6
7
1
6
6
8
5
1
0
4
3
1
8
4
6
7
0
2
7
0
3
5
2
5
1
6
4
1
2
3
1
2
9
]
s
p
b
M
[
t
u
p
h
g
u
o
r
h
T
4,000
3,000
2,000
1,000
0
256
1K
1500
4K
16K
64K
Packet size [bytes]
Fig. 8: Average maximum throughput of different set-ups for
packet sizes 256 bytes to 64 kilobytes
764
761
747
530
496
527
692
662
422
414
OpenVPN+Click
EndBox SGX
]
s
p
b
M
[
t
u
p
T
800
600
400
200
0
NOP
LB
FW
IDPS DDoS
Fig. 9: Average maximum throughput of NOP, FW, LB, IDPS
and DDoS use cases for OpenVPN+Click and ENDBOX with a
packet size of 1500 bytes
D. Throughput
Besides network latency, throughput performance is also an
important parameter impacting user experience. All these
measurements are performed on two class A(cid:2) machines.
Packet size. In this experiment, we measure the maximum
throughput reached in different conﬁgurations for various
packet sizes from 256 bytes to 64 kB. We compare four set-
ups: (i) vanilla OpenVPN, (ii) OpenVPN+Click: the same
OpenVPN version with an attached server-side Click instance;
(iii) ENDBOX in simulation mode; and (iv) ENDBOX in
hardware mode. The results are represented in Fig. 8. As
expected, the throughput increases for all conﬁgurations as
the payload size increases. Moreover, we see that ENDBOX
has an acceptable performance overhead: It varies between
2% and 13% for ENDBOX in simulation mode. Using actual
SGX instructions (hardware mode) adds overhead, resulting
in a worst-case overhead of 39% for small packets, but in
a best-case overhead of only 16% for large packets. This is
due to the fact that larger packets allow higher throughput
with less enclave transitions. We also see that a server-side
Click instance has an average performance penalty of 26%;
values range between 5% and 29% depending on packet size.
Finally, we observe that, for large packets, a server-side Click
instance achieves a throughput almost one third lower than
vanilla OpenVPN due to the Click instance’s packet fetching.
Middlebox functions. Fig. 9 shows the average maximum
throughput achieved by a traditional middlebox set-up with
VPN compared to ENDBOX. We evaluate all middlebox
functions presented in §V-B using one client machine and
a medium packet size of 1500 bytes.
394
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:31:18 UTC from IEEE Xplore.  Restrictions apply. 
With NOP representing a baseline, we ﬁrst observe that the
impact of a Click conﬁguration on OpenVPN+Click is rather
low: in the worst case, for the DDoS prevention use case,
the throughput drops by 13%, from 764 Mbps to 662 Mbps.
Second, ENDBOX incurs around 30% overhead for the use
cases NOP, LB, and FW. The more computation intensive use
cases IDPS and DDoS have an overhead of 39%. Note that this
overhead is lower for larger packets, as shown in Fig. 8.
Summary. Results show that ENDBOX introduces an accept-
able throughput overhead of only 16% for large packets in
the NOP use case. For medium sized packets, the overhead is
30% regarding lightweight middlebox functions and 39% for
speciﬁc heavy-duty use cases. As expected, we observe that
the throughput of ENDBOX increases with the size of packets.
Furthermore, ENDBOX does not have any user-perceivable
impact on the latency of HTTP page load times. As a result,
from a performance perspective, ENDBOX is a viable alternative
to existing middlebox deployments.
E. Scalability
After evaluating user-facing properties of ENDBOX like latency
and throughput, we evaluate the scalability of ENDBOX, which
is important
to its operators. Therefore, we measure the
throughput and CPU usage on server side. The throughput is
aggregated over all virtual interfaces set up by the OpenVPN
servers, which is one per client. The CPU usage applies for
all cores, i.e. 100% represents all cores being fully utilised.
The scalability measurements use ﬁve class A(cid:2) machines to
execute multiple ENDBOX clients and two class B(cid:2) machines,
each running the ENDBOX server or iperf servers. For the
measurement, we compare four set-ups: (i) vanilla OpenVPN
without middlebox function as baseline; (ii) ENDBOX in
hardware mode; (iii) vanilla Click on server side without
encryption; and (iv) OpenVPN+Click: multiple server-side
vanilla Click instances attached to OpenVPN servers. In these
experiments, each client generates a workload of 200 Mbps.
For (i), (iii) and (iv), we use one OpenVPN server instance
per client, as OpenVPN does not support multithreading.
First, we evaluate the scalability using a forwarder as
middlebox function (NOP). The results in Fig. 10a show that
vanilla OpenVPN and ENDBOX achieve the same throughput
of 6.5 Gbps at an almost identical CPU usage. This shows
that client-side execution of middleboxes has no impact on
throughput or CPU usage on server side. For OpenVPN+Click,
the bottleneck is the CPU, which is fully utilised earlier
than with ENDBOX, because Click requires a substantial
amount of cycles. In contrast, the throughput of vanilla Click
is limited to 5.5 Gbps by the Click process which cannot
handle more packets. Finally, our measurements report an
even lower throughput for OpenVPN+Click of 2.5 Gbps, which
continuously decreases with a growing number of clients, as
OpenVPN+Click is limited by the servers’ CPUs.
Use case evaluation. We conduct the same measurement for
our ﬁve use cases presented in §V-B. In Fig. 10b we use the
results for OpenVPN+Click and ENDBOX from the previous
measurements as baselines and show how ENDBOX scales with
Phase
fetch
decryption
hotswap
Total
vanilla Click
-
-
2.4 ms
2.4 ms
ENDBOX
0.86 ms
0.07 ms
0.74 ms
1.67 ms
TABLE II: Timings of different phases of vanilla Click and
ENDBOX conﬁguration updates
the number of clients when different middlebox conﬁgurations
are applied. When network trafﬁc en- and decryption fully
utilises the VPN server (at 40 clients with our machine) it
becomes the bottleneck of ENDBOX: we observe a maximum
throughput of 6.5 Gbps for all use cases. Due to the server-side
execution of middlebox functions, OpenVPN+Click reaches
this limit earlier at 30 clients with a maximum throughput of
2.5 Gbps FW and LB use cases. The computation intensive IDPS
and DDoS middlebox functions only achieve 1.7 Gbps.
Our evaluation shows that ENDBOX scales linearly with
the number of clients. Additionally, for 60 clients, ENDBOX
achieves a 2.6× higher throughput across all use cases, and
3.8× for computation intensive workloads induced by IDPS
and DDoS. This is not a general limitation of ENDBOX—it is
due to our evaluation setup and caused by the computation-