### Figure 6: Average False Positive Rate and Standard Deviation with 100% Detection Rate for Randomized Testing
- **Left Panel**: Normal Training
- **Right Panel**: Semi-Supervised Training

### 3.3 Threshold Reduction and “Extreme” Padding
In our experiments, we observed that the false positive rates of randomized models, while comparable on average, exhibit higher variance compared to non-randomized models. Consider an extreme mimicry attack where an attacker crafts packets containing only one instruction per packet, padding the rest with normal data. If a 100% detection rate is desired, lowering the score threshold to some minimum nonzero value (denoted as \(\epsilon\)) might be the only way to achieve this. Unsurprisingly, this approach directly increases the false positive rate, which can range from 10% to 25% of all packets, depending on the n-gram sizes chosen and the model's training.

A false positive rate in this range may seem impractical, potentially rendering the sensor useless. However, we argue that this perspective is incorrect. The false positive rate is not always the most critical metric, especially when Anagram is used in conjunction with other sensors. For example, Anagram can be used to shunt traffic to a host-based, heavily instrumented shadow server for further inspection. In other words, Anagram alarms are not intended to drop traffic or mitigate against (possibly false) attacks but rather to validate whether traffic is indeed an attack using a host-based sensor system. By shunting, say, 25% of the traffic for inspection by an (expensive) host-based system, while leaving the remaining normal traffic unimpeded, the proposed host-based shadow servers can amortize their overhead costs more economically. Thus, the false positive rate is less important than the true positive rate, and false positives do not cause harm. This approach is described in greater detail in Section 4.

### 4. Adaptive Learning

#### 4.1 Training Attacks Versus Mimicry Attacks
We distinguish between two types of attacks: training attacks and mimicry attacks.
- **Mimicry Attack**: A willful attempt to craft and shape an attack vector to look normal with respect to a model computed by an anomaly detector. The attacker needs to know the modeling algorithm and the normal data it was trained on. The polymorphic blended attack engine discussed in Section 3 assumes knowledge of both by sniffing an environment’s normal data. An open question is whether the normal data of one site produces sufficiently similar models to other sites targeted by a mimicry attack.
- **Training Attack**: An attack where the attacker sends a stream of data incrementally or continuously distant from the normal data at a target site to influence the anomaly detector to model data consistent with the attack vector. This type of attack would succeed if the attacker were lucky enough to send the stream of data while the anomaly detector was in training mode, and the "malicious training data" went unnoticed by the sensor during training.

We introduced randomization to thwart mimicry attacks. Even if the attacker knew the normal data distribution, they would not know the actual model used by the sensor. However, we have not addressed training attacks. [25] explores this problem and suggests several theoretical defenses. For example, Anagram’s semi-supervised learning mechanism can help protect the model if learning attacks recycle old exploit code. However, if the learning attack does not contain any known bad n-grams, Anagram cannot detect it by itself. We conjecture that coupling the training sensor with an "oracle" that informs the sensor whether the data it is modeling is truly normal can thwart such training attacks. For instance, if the attacker sends packets that do not exploit a server vulnerability but produce an error response, this should be noticed by the sensor in training mode. Such feedback-based learning does not address all cases, e.g., a learning attack embedded into an HTTP POST payload, which would generate a legitimate server response. Randomization may also be valuable for learning attacks; we leave the exploration of such defense mechanisms for future research.

#### 4.2 Feedback-Based Learning and Filtering Using Instrumented Shadow Servers
Host-based fault detection and patch generation techniques (such as StackGuard/MemGuard [26], STEM [27], DYBOC [28], and others) show significant promise in improving worm and attack detection but at the cost of significant computational overhead on the host. The performance hit on a server could render such technologies of limited operational value. For example, STEM [27] imposes a 200% overhead for an entirely-instrumented application. DYBOC [28] is more proactive and designed for production servers, but still imposes at least a 20% overhead on practical web servers. Reducing this overhead would significantly increase the technology's value.

We envision an architecture consisting of both production servers and an instrumented shadow server, with the latter executing both valid and malicious requests securely but with significant overhead. A network anomaly flow classifier is placed in front of these pools and shunts traffic based on the anomaly content in incoming requests.

For the flow classifier to be appropriate for this architecture, we need to ensure that no malicious requests are sent to the production pool, as those machines may be vulnerable to zero-day attacks. It is acceptable if a small fraction of the traffic deemed as false positives are shunted to the shadow server, as this server will serve the requests with greater latency. Nothing is lost, but only some amount of response time for a minority portion of the traffic flow. An anomaly detector acting as this classifier should have a 100% true positive detection rate and a reasonably low false positive rate. We can characterize such an architecture as \( l' = l \times (1 - fp) + l \times os \times fp \), where \( l \) is the standard latency of a service, \( os \) is the shadow server overhead, and \( fp \) is the false positive rate. If we target a maximum latency increase \( l' - l \) of 1%, given a 20% overhead in using the shadow server, the false positive rate can be as high as approximately 10%. As described in Section 3.3, we believe this order-of-magnitude is achievable, along with a 100% detection rate, by setting the threshold to some low \(\epsilon\). This places network content flow anomaly detectors in a new light. Zero false positive rates are not the right objective. Furthermore, a symbiotic relationship may emerge over time. False positives validated by the shadow server serve as training data to improve the accuracy of the anomaly detector. Over time, the false positive rate would decrease, and less traffic would be served by the shadow server.

##### 4.2.1 Adaptive Model Training of Anagram with Shadow Servers
Anagram’s detection model assumes no noise in the model and uses semi-supervised training (Section 2.3.2) to ensure clean training traffic. This approach can be enhanced with a hybrid shadow server architecture:
1. **Use the shadow server as a training supervisor**: Initially, deploy Anagram passively, and use a "slow training mechanism" where the shadow server is initially sent all requests, and only requests generating a normal response are then sent to Anagram for training. After sufficient training, Anagram can be put into active deployment.
2. **Use a short training time and the shadow server as a false positive feedback mechanism**: Train Anagram on a small fraction of traffic assumed to be good and immediately deploy it. While the false positive rate will be higher, Anagram can watch for normal responses from the shadow server and include the appropriate n-grams of the original request in its model of "normal traffic." This can incrementally reduce the false positive rate as the system continues processing requests.

A hybrid approach can also be employed, such as deploying Anagram with no model, processing 100% as false positives, and using feedback as an incremental learning architecture. We performed early experiments with feedback learning using the PAYL sensor in [29], and Anagram has recently been used as a sensor in [30]. We intend to continue integration experiments using Anagram and will report our results in a future paper.

### 5. Related Work

#### 5.1 Anomaly Detectors and Signature Generators
Early intrusion anomaly sensors focused on system calls. Forrest [16]'s "foreign sequences of system calls" in a binary-based anomaly detector is similar to the modeling implemented in Anagram. Tan and Maxion [17] showed why Forrest's work produced optimal results with a fixed token window size of 6 (essentially a 6-gram). In Anagram, no fixed-sized window is needed; one can model a mixture of n-grams. Forrest’s grams were sequences of tokens each representing a unique system function, whereas Anagram models n-grams of byte values.

Many researchers have considered the use of packet flows and/or content analysis for anomaly detection. Honeycomb [31] is a host-based IDS that automatically creates signatures by applying the longest common substring (LCS) on malicious traffic captured by a honeypot targeting dark space. Computed substrings are used as candidate worm signatures. Similarly, EarlyBird [32] uses Rabin fingerprints to find the most frequent substrings for signatures. Unlike Honeycomb (and PAYL), Anagram computes distinct n-grams from packets and compares the n-gram set against those in its model, which is a linear-time operation, unlike polynomial-time LCS.

Polygraph [19] extends the work done in Autograph [33]; both are signature generators that assume traffic is separated into two flow pools, one with suspicious scanning traffic and one with non-suspicious traffic. Instead of assuming signatures are contiguous, like Autograph, Polygraph allows a signature composed of multiple noncontiguous substrings (tokens), particularly to accommodate polymorphic worms. Tokens may be generated as a set (of which all must be present), as an ordered sequence, or as a probabilistic set (Bayes signature). Like Polygraph, Anagram is capable of identifying multiple "tokens." However, Anagram’s design does not assume an external flow classifier, being one itself. A more general discussion of related work in the area of anomaly detection can be found in [10].

Shield [34] provides vulnerability signatures instead of string-oriented content signatures and blocks attacks that exploit that vulnerability. A "shield" is manually specified for a vulnerability identified in some network-available code; the time lag to specify, test, and deploy shields from the moment the vulnerability is identified favors the worm writer, not the defenders. COVERS [35] analyzes attack-triggered memory errors in C/C++ programs and develops structural memory signatures; this is a primarily host-specific approach, while Anagram focuses on network traffic. SigFree [9] uses a different approach, focusing on generic code detection; as its name implies, it does not rely on signatures, preferring to disassemble instruction sequences and identify, using data flow anomaly detection, if requests contain sufficient code to merit them as being suspicious. Anagram does not explicitly differentiate between code and data, although it is often able to do so based on training. Additionally, Anagram monitors content flows, not just requests, and can apply to a broader variety of protocols.

#### 5.2 Polymorphic Worms, Mimicry, and Learning Attacks
Early work on polymorphic worms focused on making it more difficult for COTS signature scanner detection; they can be easily detected by an anomaly detector as they contain significantly different byte distributions than non-malicious code. Polymorphic worms with vulnerability-exploiting shellcode, e.g., ADMmutate [36] and CLET [24], support exploit vectors and are primarily designed to fool signature-based IDSes. CLET features a form of padding, called cramming, to defeat simple anomaly detectors. However, cram bytes are derived from a static source, i.e., instructions in a file included with the CLET distribution; while this may be customized to approach a general mimicry attack, it must be done by hand.

The notion of a mimicry attack on an anomaly detection system was first introduced in 2001 by Wagner and Dean [37], but initial efforts to generate mimicry attacks, including [38] and [39], focused on system-call anomaly detection. With the advent of effective network payload-based anomaly detection techniques, researchers have begun building "smart worms" that employ a combination of polymorphism and mimicry attack mechanisms. Kolesnikov, Dagon, and Lee [1] built a worm specifically designed to target network anomaly detection approaches, including PAYL. They use various techniques, including polymorphic decryption, normal traffic profiling and blending, and splitting to effectively defeat PAYL and several other IDSes.

Defeating learning attacks is also current research; [25] discusses the problem for anomaly detectors from a theoretical perspective, categorizes different types of learning attacks (e.g., causative vs. exploratory, etc.), and speculates on several possible solutions. Anagram implements some of these techniques, and its use of randomization, hiding key parameters of the model secret from the attacker, may be extensible to learning. Our ongoing work includes exploring several other strategies, including the randomization of n-gram sizes and various strategies to test whether an attacker is polluting learning traffic at given points in time.

### 6. Conclusion
Anagram is an anomaly detector based on n-gram analysis using a binary-based modeling technique. The use of randomization makes the sensor resistant to mimicry attacks. Anagram’s models use Bloom filters for compactness and performance. Our tests suggest Anagram has less than a 0.01% false positive rate along with a 100% detection rate for a variety of worms and viruses detected in traces of our local network traffic. Anagram’s use of Bloom filters also enables effective privacy-preserving cross-site correlation and signature generation.

Anagram detects existing mimicry attacks, including those targeted at our previous anomaly detection sensor, PAYL, and we speculate that Anagram will be robust to future attacks as well. We also discuss approaches to effectively learn Anagram models, including the use of a bad content Bloom filter and instrumented shadow servers. For the latter case, we believe Anagram can act as an effective network anomaly flow classifier to mitigate host instrumentation overhead and make these tools effective for practical deployment.

There are several interesting venues for future research. We intend to build an integrated instrumented shadow server architecture utilizing Anagram to collect statistics on performance and modeling accuracy. Another open area of research is to make binary-based modeling more robust against learning attacks. We would also like to compare Anagram’s performance to other proposed content-based anomaly sensors. Finally, we intend a practical deployment of a multi-site correlation effort and gauge Anagram’s performance in helping to identify broad zero-day worms or targeted attacks while maintaining full privacy.

### Acknowledgements
We would like to thank Gabriela Cretu, Wei-Jen Li, Michael Locasto, Angelos Stavrou, and Angelos Keromytis for their suggestions and advice during our collaboration, and Panagiotis Manolios and Peter Dillinger for their suggestions in Bloom filter design.

### References
1. Kolesnikov, O., D. Dagon, and W. Lee, Advanced Polymorphic Worms: Evading IDS by Blending in with Normal Traffic, in USENIX Security Symposium. 2006: Vancouver, BC, Canada.
2. Moore, D., et al. Internet Quarantine: Requirements for Containing Self-Propagating Code. in INFOCOM. 2003.
3. Staniford-Chen, S., V. Paxson, and N. Weaver. How to 0wn the Internet in Your Spare Time. in USENIX Security. 2002.
4. Christodorescu, M. and S. Jha. Static Analysis of Executables to Detect Malicious Patterns. in USENIX Security Symposium. 2003. Washington, D.C.
5. Vargiya, R. and P. Chan. Boundary Detection in Tokenizing Network Application Payload for Anomaly Detection. in ICDM Workshop on Data Mining for Computer Security (DMSEC). 2003. Melbourne, FL.
6. Kruegel, C., et al. Polymorphic Worm Detection Using Structural Information of Executables. in Symposium on Recent Advances in Intrusion Detection. 2005. Seattle, WA.
7. Sekar, R., et al. Specification-based Anomaly Detection: A New Approach for Detecting Network Intrusions. in ACM Conference on Computer and Communications Security. 2002. Washington, D.C.
8. Kruegel, C., T. Toth, and E. Kirda. Service Specific Anomaly Detection for Network Intrusion Detection. in Symposium on Applied Computing (SAC). 2002. Madrid, Spain.
9. Wang, X., et al. SigFree: A Signature-free Buffer Overflow Attack Blocker. in USENIX Security. 2006. Boston, MA.
10. Wang, K. and S.J. Stolfo. Anomalous Payload-based Network Intrusion Detection. in Symposium on Recent Advances in Intrusion Detection. 2004. Sophia Antipolis, France.
11. Wang, K., G. Cretu, and S.J. Stolfo. Anomalous Payload-based Worm Detection and Signature Generation. in Symposium on Recent Advances in Intrusion Detection. 2005. Seattle, WA.
12. SourceFire Inc. Snort rulesets. 2006 [cited 2006 April 4]; Available from: http://www.snort.org/pub-bin/downloads.cgi.
13. Locasto, M.E., S. Sidiroglou, and A.D. Keromytis. Application Communities: Using Monoculture for Dependability. in HotDep. 2005.
14. Locasto, M.E., S. Sidiroglou, and A.D. Keromytis. Software Self-Healing Using Collaborative Application Communities. in Internet Society (ISOC) Symposium on Network and Distributed Systems Security. 2006. San Diego, CA.
15. Marceau, C. Characterizing the Behavior of a Program Using Multiple-Length N-grams. in New Security Paradigms Workshop. 2000. Cork, Ireland.
16. Forrest, S., et al. A Sense of Self for Unix Processes. in IEEE Symposium on Security and Privacy. 1996.
17. Tan, K.M.C. and R.A. Maxion. Why 6? Defining the Operational Limits of stide, an Anomaly-Based Intrusion Detector. in IEEE Symposium on Security and Privacy. 2002. Berkeley, CA.
18. Crandall, J.R., et al. On Deriving Unknown Vulnerabilities from Zero-Day Polymorphic and Metamorphic Worm Exploits. in ACM Conference on Computer and Communications Security. 2005. Alexandria, VA.
19. Newsome, J., B. Karp, and D. Song. Polygraph: Automatically Generating Signatures for Polymorphic Worms. in IEEE Security and Privacy. 2005. Oakland, CA.
20. Singh, S., et al. Automated Worm Fingerprinting. in 6th Symposium on Operating Systems Design and Implementation (OSDI '04). 2004. San Francisco, CA.
21. Bloom, B.H., Space/time trade-offs in Hash Coding with Allowable Errors. Communications of the ACM, 1970. 13(7): p. 422-426.
22. Naor, M. and M. Yung. Universal One-Way Hash Functions and their Cryptographic Applications. in ACM Symposium on Theory of Computing. 1989. Seattle, WA.
23. Parekh, J.J., K. Wang, and S.J. Stolfo. Privacy-Preserving Payload-Based Correlation for Accurate Malicious Traffic Detection. in Large-Scale Attack Detection, Workshop at SIGCOMM. 2006. Pisa, Italy.
24. Detristan, T., et al. Polymorphic Shellcode Engine Using Spectrum Analysis. Phrack.