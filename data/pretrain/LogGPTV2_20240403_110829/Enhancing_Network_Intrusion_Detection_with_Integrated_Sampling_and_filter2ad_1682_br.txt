Fig.  6. The average  false positive rate and standard deviation with 100% detection rate for 
randomized testing, with normal training (left) and semi-supervised training (right) 
3.3   Threshold Reduction and “Extreme” Padding 
In  the  experiments  we  reported  above,  we  noticed  that  the  randomized  models’ 
false positive rates, while comparable on average, exhibit a higher variance than the 
case where models are not randomized. Consider an extreme mimicry attack: sup-
pose an attacker crafts packets with only one instruction per packet, and pads the 
rest  with  normal  data.  If  a  100%  detection  rate  is  desirable,  lowering  the  score 
threshold to some minimum nonzero value (cid:304) might be the only way to achieve this 
goal.  Unsurprisingly,  this  approach  corresponds  to  a  direct  increase  in  the  false 
positive rate, which may vary between 10% to 25% of all packets, depending on the 
n-gram sizes chosen and the amount of training of the model. 
Such  a  false  positive  rate  may  be  viewed  as  impractical,  rendering  the  sensor 
useless.  We  believe  this  is  wrong.  The  false  positive  rate  is  not  always  the  right 
metric – especially if Anagram is used in conjunction with other sensors. For ex-
ample, Anagram may be used to shunt traffic to a host-based, heavily instrumented 
shadow  server  used  as  a  detector;  in  other  words,  we  do  not  generate  Anagram 
alarms  to  drop  traffic  or  otherwise  mitigate  against  (possibly  false)  attacks,  but 
rather we may validate whether traffic is indeed an attack by making use of a host-
based sensor system.  If we can shunt, say 25% of the traffic for inspection by an 
(expensive) host-based system, while leaving the remaining normal traffic unabated 
to be processed by the operational servers, the proposed host-based shadow servers 
can amortize their overhead costs far  more economically. Thus, the  false positive 
rate  is  not  as  important  as  the  true  positive  rate,  and  false  positives  do  not  cause 
harm. We describe this approach in greater detail in section 4. 
4   Adaptive Learning 
4.1   Training Attacks Versus Mimicry Attacks 
We distinguish between training attacks and mimicry attacks: A mimicry attack is 
the willful attempt to craft and shape an attack vector to look normal with respect to 
242 
K. Wang, J.J. Parekh, and S.J. Stolfo 
a model computed by an anomaly detector. The attacker would need to know the 
modeling algorithm used by the sensor and the normal data it was trained on. The 
polymorphic blended attack engine discussed in Section 3 assumes to know both by 
sniffing an environment’s normal data (an open question is whether the normal data 
of one site produces sufficiently similar models to other sites that are targeted by a 
mimicry attack). Alternatively, a training attack is one whereby the attacker sends a 
stream of data incrementally or continuously distant from the normal data at a tar-
get site in order to influence the anomaly detector to model data consistent with the 
attack vector. Attack packets would appear normal since they were modeled. This 
type of attack would succeed if the attacker were lucky enough to send the stream 
of data while the anomaly detector was in training mode. Furthermore, the attacker 
would presume that the stream of “malicious training data” would go unnoticed by 
the sensor while it was training.   
We  presented  the  concept  of  randomization  in  order  to  thwart  mimicry  attack. 
Even if the attacker knew the normal data distribution, the attacker would not know 
the  actual  model  used  by  the  sensor.  However,  we  have  not  addressed  training 
attacks.  [25]  explores  this  problem  and  suggests  several  theoretical  defenses.  For 
example,  Anagram’s  semi-supervised  learning  mechanism  can  help  protect  the 
model if learning attacks recycle old exploit code. However, if the learning attack 
does not contain any known bad n-grams, Anagram cannot detect it by itself. We 
conjecture that the coupling of the training sensor with an “oracle” that informs the 
sensor  of  whether  or  not  the  data  it  is  modeling  is  truly  normal  can  thwart  such 
training  attacks.  For  example,  if  the  attacker  sends  packets  that  do  not  exploit  a 
server vulnerability, but produces an error response, this should be noticed by the 
sensor in training mode; we discuss this further in the next section. Such feedback-
based learning does  not address all cases, e.g., a learning  attack embedded into a 
HTTP  POST  payload,  which  would  generate  a  legitimate  server  response.  Ran-
domization may also be valuable for learning attacks; we leave the exploration of 
such defense mechanisms for future research. 
4.2   Feedback-Based Learning and Filtering Using Instrumented Shadow 
Servers 
Host-based  fault  detection  and  patch  generation  techniques  (such  as  Stack-
Guard/MemGuard [26], STEM [27], DYBOC [28], and many others) hold signifi-
cant promise in improving worm and attack detection, but at the cost of significant 
computational overhead on the host. The performance hit on a server could render 
such technologies of limited operational value. For instance, STEM [27] imposes a 
200%  overhead  for  an  entirely-instrumented  application.  DYBOC  [28]  is  more 
proactive and designed to be deployed on production servers to provide faster re-
sponse, but still imposes at least a 20% overhead on practical web servers. If one 
can find a means of reducing the cost of this overhead, the technology will have a 
far greater value to a larger market. 
We  envision  an  architecture  consisting  of  both  production  servers  and  an  in-
strumented  shadow  server,  with  the  latter  executing  both  valid  and  malicious  re-
quests securely but with significant overhead. A network anomaly flow classifier is 
placed  in  front  of  these  pools  and  shunts  traffic  based  on  the  anomaly  content  in 
incoming requests. 
                                                                     Anagram: A Content Anomaly Detector         243 
(
the 
in 
(
(
In order for the flow classifier to be appropriate for this architecture, we need to 
ensure that no malicious requests are sent to the production pool, as those machines 
may be potentially vulnerable to zero-day attacks. It is acceptable if a small frac-
tion of the traffic deemed as false positives are shunted to the shadow server, be-
cause this server will indeed serve the requests, but with a greater latency. Nothing 
has been lost, but only some amount of response time for a minority portion of the 
traffic flow. In other words, an anomaly detector that wants to act as this classifier 
should have a 100% true positive detection rate and a reasonably low false positive 
rate.  We  can  characterize 
such  an  architecture  as 
)+ l × os
), where l is the standard latency of a service, os is the 
)
l'= l × 1− fp
shadow  server  overhead,  and  fp  is  the  false  positive  rate.  If  we  want  to  target  a 
maximum latency increase  l'−l of 1%, given a 20% overhead in using the shadow 
server,  the  false  positive  rate  can  be  as  high  as  approximately  10%.  As  we  de-
scribed in section 3.3, we believe this order-of-magnitude is achievable, along with 
a 100% detection rate, by setting the threshold to some low (cid:304). This places network 
content flow anomaly detectors in a completely new light. Zero false positive rates 
are  simply  not  the  right  objective.  Furthermore,  a  symbiotic  relationship  may 
emerge over time.  False positives validated by the shadow server serve as training 
data  to  improve  the  accuracy  of  the  anomaly  detector.  In  time,  the  false  positive 
rate would be decrease and less traffic would be served by the shadow server; we 
discuss this in the next section. 
latency 
× fp
4.2.1   Adaptive Model Training of Anagram with Shadow Servers 
Anagram’s  detection  model  assumes  no  noise  in  the  model,  and  uses  semi-
supervised  training  (section  2.3.2)  to  ensure  clean  training  traffic.  This  approach 
can be enhanced with a hybrid shadow server architecture: 
1.  Use the shadow server as a training supervisor. This entails an initial passive 
Anagram deployment, and a “slow training mechanism” whereby the shadow 
server is initially sent all requests, and only requests generating a normal re-
sponse  are  then  sent  to  Anagram  for  training.  After  a  sufficient  amount  of 
training has been done, Anagram can then be put into an active deployment. 
2.  Use a short training time, and use the shadow server as a false positive feed-
back mechanism. In this scenario, Anagram only trains on a small fraction of 
traffic assumed to be good and is then immediately deployed. While the false 
positive  rate  will  be  higher,  Anagram  can  watch  for  normal  responses  from 
the shadow server and can then include the appropriate n-grams of the origi-
nal request in its model of “normal traffic”. This can incrementally reduce the 
false positive rate as the system continues processing requests. 
Of  course,  a  hybrid  approach  can  also  be  employed;  for  example,  Anagram 
could be deployed  with no  model, process 100% as false positives, and use  feed-
back  as  an  incremental  learning  architecture.    We  performed  some  early  experi-
ments  with  feedback  learning  using  the  PAYL  sensor  in  [29],  and  Anagram  has 
recently  been  used  as  a  sensor  in  [30].  We  intend  to  continue  integration  experi-
ments using Anagram, and will report our results in a future paper. 
244 
K. Wang, J.J. Parekh, and S.J. Stolfo 
5   Related Work 
In  addition  to  the  works  cited  in  the  introduction,  we  further  discuss  research  in 
anomaly  detection,  signature  generation,  polymorphic  worms,  and  mimicry  and 
learning attacks. 
5.1   Anomaly Detectors and Signature Generators 
Early  intrusion  anomaly  sensors  focused  system  calls.  Forrest  [16]’s  "foreign  se-
quences  of  system  calls"  in  a  binary-based  anomaly  detector  is  quite  similar  to  the 
modeling implemented in Anagram. Tan and Maxion [17] shows why Forrest's work 
produced optimal results when the size of the token window was fixed at 6 (essen-
tially  a  6-gram).  In  Anagram,  no  fixed  sized  window  is  needed;  one  may  model  a 
mixture  of  n-grams.  Forrest’s  grams  were  sequences  of  tokens  each  representing  a 
unique system function, whereas Anagram models n-grams of byte values. 
Many researchers have considered the use of packet flows and/or content analysis 
for anomaly detection. Honeycomb [31] is a host-based IDS that automatically cre-
ates  signatures  by  applying  longest  common  substring  (LCS)  on  malicious  traffic 
captured by a honeypot targeting dark space. Computed substrings are used as candi-
date  worm  signatures.  Similarly,  EarlyBird  [32]  uses  Rabin  fingerprints  to  find  the 
most frequent substrings  for signatures. Unlike Honeycomb (and PAYL),  Anagram 
computes distinct n-grams from packets and compares the n-gram set against those in 
its model; this is a linear-time operation, unlike polynomial-time LCS. 
Polygraph  [19]  extends  the  work  done  in  Autograph  [33];  both  are  signature 
generators that assume traffic is separated into two flow pools, one with suspicious 
scanning traffic and a one with non-suspicious traffic. Instead of assuming signa-
tures are contiguous, like Autograph, Polygraph allows a signature composed mul-
tiple noncontiguous substrings (tokens), particularly to accommodate polymorphic 
worms.  Tokens  may  be  generated  as  a  set  (of  which  all  must  be  present),  as  an 
ordered sequence, or as a probabilistic set (Bayes signature). Like Polygraph, Ana-
gram is capable of identifying multiple “tokens”. However, Anagram’s design also 
does not assume an external  flow classifier, being one itself. A  more general dis-
cussion of related work in the area of anomaly detection can be found in [10]. 
Shield  [34]  provides  vulnerability  signatures  instead  of  string-oriented  content 
signatures, and blocks attacks that exploit that vulnerability. A “shield” is manually 
specified for a vulnerability identified in some network available code; the time lag 
to specify, test and deploy shields from the moment the vulnerability is identified 
favors the worm writer, not the defenders. COVERS [35] analyzes attack-triggered 
memory errors in C/C++ programs and develops structural memory signatures; this 
is a primarily host-specific approach, while Anagram focuses on network traffic. 
SigFree [9] uses a different approach, focusing on generic code detection; as its 
name implies, it does not rely on signatures, preferring to disassemble instruction 
sequences and identify, using data flow anomaly detection, if requests contain suf-
ficient code to merit them as being suspicious. Anagram does not explicitly differ-
entiate between code and data, although it is often able to do so based on training. 
Additionally, Anagram monitors content flows, not just requests, and can apply to a 
broader variety of protocols. 
                                                                     Anagram: A Content Anomaly Detector         245 
5.2   Polymorphic Worms, Mimicry and Learning Attacks 
Early work on polymorphic worms focused on making it more difficult for COTS 
signature scanner detection; they can be easily detected by an anomaly detector as 
they  contain  significantly  different  byte  distributions  than  non-malicious  code. 
Polymorphic worms with vulnerability-exploiting shellcode, e.g., ADMmutate [36] 
and CLET [24], do support exploit vectors and are primarily designed to fool signa-
ture-based IDSes. CLET does feature a form of padding, which they call cramming, 
to defeat simple anomaly detectors. However, cram bytes are derived from a static 
source,  i.e.  instructions  in  a  file  included  with  the  CLET  distribution;  while  this 
may be customized to approach a general mimicry attack, it must be done by hand. 
The notion of a mimicry attack on an anomaly detection system was first intro-
duced  in  2001  by  Wagner  and  Dean  [37],  but  initial  efforts  to  generate  mimicry 
attacks,  including  [38]  and  [39],  focused  on  system-call  anomaly  detection.  With 
the  advent  of  effective  network  payload-based  anomaly  detection  techniques,  re-
searchers have begun building “smart worms” that employ a combination of poly-
morphism and mimicry attack mechanisms. Kolesnikov, Dagon and Lee [1] built a 
worm  specifically  designed  to  target  network  anomaly  detection  approaches,  in-
cluding PAYL.  They use a number of techniques, including polymorphic decryp-
tion, normal traffic profiling and blending, and splitting to effectively defeat PAYL 
and several other IDSes. 
Defeating  learning  attacks  is  also  current  research;  [25]  discusses  the  problem 
for anomaly detectors from a theoretical perspective, categorizes different types of 
learning  attacks  (e.g.,  causative  vs.  exploratory,  etc.)  and  speculates  as  to  several 
possible  solutions.  Anagram  implements  some  of  these  techniques,  and  its  use  of 
randomization, hiding key parameters of the model secret from the attacker, may be 
extensible  to  learning.  Our  ongoing  work  includes  exploring  several  other  strate-
gies,  including  the  randomization  of  n-gram  sizes,  and  various  strategies  to  test 
whether an attacker is polluting learning traffic at given points in time.  
6   Conclusion 
Anagram is an anomaly detector based upon n-gram analysis using a binary-based 
modeling technique. The use of randomization makes the sensor resistant to mim-
icry attack. Anagram’s models use Bloom filters for compactness and performance. 
Our  tests  suggest  Anagram  has  less  than  a  .01%  false  positive  rate  along  with  a 
100%  detection  rate  for  a  variety  of  worms  and  viruses  detected  in  traces  of  our 
local  network  traffic.  Anagram’s  use  of  Bloom  filters  also  enables  effective  pri-
vacy-preserving cross-site correlation and signature generation. 
Anagram detects existing mimicry attacks, including those targeted at our previ-
ous anomaly detection sensor, PAYL, and  we speculate that Anagram  will be ro-
bust to future attacks as well. We also discuss approaches to effectively learn Ana-
gram  models,  including  the  use  of  a  bad  content  Bloom  filter  and  instrumented 
shadow  servers.  For  the  latter  case,  we  believe  Anagram  can  act  as  an  effective 
network  anomaly  flow  classifier  to  mitigate  host  instrumentation  overhead  and 
make these tools effective for practical deployment. 
246 
K. Wang, J.J. Parekh, and S.J. Stolfo 
There are a number of interesting venues for future research. We intend to build 
an integrated instrumented shadow server architecture utilizing Anagram to collect 
statistics on performance and modeling accuracy. Another open area of research is 
to  make  binary-based  modeling  more  robust  against  learning  attacks.  We  would 
also  like  to  compare  Anagram’s  performance  to  other  proposed  content-based 
anomaly  sensors.10   Finally,  we  intend  a  practical  deployment  of  a  multiple-site 
correlation  effort  and  gauge  Anagram’s  performance  in  helping  to  identify  broad 
zero-day worms or targeted attacks while maintaining full privacy. 
Acknowledgements 
We  would  like  to  thank  Gabriela  Cretu,  Wei-Jen  Li,  Michael  Locasto,  Angelos 
Stavrou,  and  Angelos  Keromytis  for  their  suggestions  and  advice  during  our  col-
laboration,  and  Panagiotis  Manolios  and  Peter  Dillinger  for  their  suggestions  in 
Bloom filter design. 
References 
1.  Kolesnikov, O., D. Dagon, and W. Lee,  Advanced Polymorphic Worms: Evading IDS 
by Blending in with Normal Traffic, in USENIX Security Symposium. 2006: Vancouver, 
BC, Canada. 
2.  Moore,  D.,  et  al.  Internet  Quarantine:  Requirements  for  Containing  Self-Propagating 
Code. in INFOCOM. 2003. 
3.  Staniford-Chen, S., V. Paxson, and N. Weaver. How to 0wn the Internet in Your Spare 
Time. in USENIX Security. 2002. 
4.  Christodorescu, M. and S. Jha. Static Analysis of Executables to Detect Malicious Pat-
terns. in USENIX Security Symposium. 2003. Washington, D.C. 
5.  Vargiya, R. and P. Chan. Boundary Detection in Tokenizing Network Application Pay-
load for Anomaly Detection. in ICDM Workshop on Data Mining for Computer Security 
(DMSEC). 2003. Melbourne, FL. 
6.  Kruegel, C., et al. Polymorphic Worm Detection Using Structural Information of Execu-
tables. in Symposium on Recent Advances in Intrusion Detection. 2005. Seattle, WA. 
7.  Sekar, R., et al. Specification-based Anomaly Detection: A New Approach for Detecting 
Network  Intrusions.  in  ACM  Conference  on  Computer  and  Communications  Security. 
2002. Washington, D.C. 
8.  Kruegel, C., T. Toth, and E. Kirda. Service Specific Anomaly Detection for Network In-
trusion Detection. in Symposium on Applied Computing (SAC). 2002. Madrid, Spain. 
9.  Wang, X., et al. SigFree: A Signature-free Buffer Overflow Attack Blocker. in USENIX 
Security. 2006. Boston, MA. 
10.  Wang,  K.  and  S.J.  Stolfo.  Anomalous  Payload-based  Network  Intrusion  Detection.  in 
Symposium on Recent Advances in Intrusion Detection. 2004. Sophia Antipolis, France. 
11.  Wang,  K.,  G.  Cretu,  and  S.J.  Stolfo.  Anomalous  Payload-based  Worm  Detection  and 
Signature Generation. in Symposium on Recent Advances in Intrusion Detection. 2005. 
Seattle, WA. 
10 At the time of  writing this paper, no other sensors have been made available to us from 
other researchers. We would be delighted to collaborate with other research groups to per-
form these comparative evaluations.  
                                                                     Anagram: A Content Anomaly Detector         247 
12.  SourceFire  Inc.  Snort  rulesets.    2006    [cited  2006  April  4];  Available  from: 
http://www.snort.org/pub-bin/downloads.cgi. 
13.  Locasto,  M.E.,  S.  Sidiroglou,  and  A.D.  Keromytis.  Application  Communities:  Using 
Monoculture for Dependability. in HotDep. 2005. 
14.  Locasto,  M.E.,  S.  Sidiroglou,  and  A.D.  Keromytis.  Software  Self-Healing  Using  Col-
laborative Application Communities. in Internet Society (ISOC) Symposium on Network 
and Distributed Systems Security. 2006. San Diego, CA. 
15.  Marceau,  C.  Characterizing  the  Behavior  of  a  Program  Using  Multiple-Length  N-
grams. in New Security Paradigms Workshop. 2000. Cork, Ireland. 
16.  Forrest, S., et al.  A Sense of Self for Unix Processes. in  IEEE Symposium on Security 
and Privacy. 1996. 
17.  Tan,  K.M.C.  and  R.A.  Maxion.  Why  6?  Defining  the  Operational  Limits  of  stide,  an 
Anomaly-Based Intrusion Detector. in IEEE Symposium on Security and Privacy. 2002. 
Berkeley, CA. 
18.  Crandall, J.R., et al. On Deriving Unknown Vulnerabilities from Zero-Day Polymorphic 
and Metamorphic Worm Exploits. in ACM Conference on Computer and Communica-
tions Security. 2005. Alexandria, VA. 
19.  Newsome, J., B. Karp, and D. Song. Polygraph: Automatically Generating Signatures 
for Polymorphic Worms. in IEEE Security and Privacy. 2005. Oakland, CA. 
20.  Singh, S., et al. Automated Worm Fingerprinting. in 6th Symposium on Operating Sys-
tems Design and Implementation (OSDI '04). 2004. San Francisco, CA. 
21.  Bloom, B.H., Space/time trade-offs in Hash Coding with Allowable Errors. Communi-
cations of the ACM, 1970. 13(7): p. 422-426. 
22.  Naor, M. and M. Yung.  Universal One-Way Hash Functions and their Cryptographic 
Applications. in ACM Symposium on Theory of Computing. 1989. Seattle, WA. 
23.  Parekh, J.J., K. Wang, and S.J. Stolfo. Privacy-Preserving Payload-Based Correlation 
for Accurate Malicious Traffic Detection. in Large-Scale Attack Detection, Workshop at 
SIGCOMM. 2006. Pisa, Italy. 
24.  Detristan,  T.,  et  al.  Polymorphic  Shellcode  Engine  Using  Spectrum  Analysis.  Phrack  