6
0
.
4
0
.
2
0
.
0
0
y
t
i
s
n
e
D
.
6
0
.
4
0
.
2
0
.
0
0
y
t
i
s
n
e
D
0
3
0
.
0
2
0
.
0
1
0
.
0
0
0
.
0e+00 2e+05 4e+05 6e+05
0e+00 2e+05 4e+05 6e+05
0e+00 2e+05 4e+05 6e+05
0e+00 2e+05 4e+05 6e+05
(a)
facerec_weighted_pop
(b)
facerec_weighted_smarts
(c)
facerec_weighted_simpoint
(d)
facerec_weighted_interval10%
Figure 1.   Weighted histogram of VCs observed in facerec with MinneSPEC large input 
(Detected  Unrecoverable  Error), false DUE or  SDC  (Silent 
Data Corruption) failures as in [3]. 
AVF,  PARMA  and  MACAU  aim  to  correctly  identify 
which  faults  generated  by  particle  hits  become  errors  and 
then  eventually  cause  failures  in  each  of  the  above 
categories.  AVF  assumes  that  particle  hits  (SEUs)  cause  at 
most  one  fault  in  a  set  of  bits  since  the  SEU  rate  is 
extremely  low  at  ground  level.  AVF  analysis  rely  on 
reliability-lifetime  analysis to find  out  what fraction of bits 
in  vulnerable  components  will  eventually  affect  program 
outcomes  by  rigorously  tracking  the  chain  of  computations 
on  those  bits.  On  the  other  hand,  PARMA  and  MACAU 
track  faults  causing  Multi-Bit  Upsets  (MBUs).  In  large, 
lower-level  caches  some  blocks  can  have  reliability 
lifetimes  well  into  multi-million  cycles  and  thus  can  suffer 
from  temporal  MBUs  due  to  multiple  SEUs.  In  addition, 
spatial MBUs, where multiple bits are upset by a single SEU, 
happen  more  frequently  as  technology  advances  towards 
smaller  nodes.  Thus  caches  are  usually  protected  by 
protection codes like  SECDED – or even stronger – codes. 
Details on AVF, PARMA and MACAU can be found in [3], 
[22] and [23] respectively.  
AVF, PARMA and MACAU all suffer major simulation 
slowdowns because of reliability-lifetime analysis. Note that 
it  is  not  possible  to  simply  discard  VCs,  and  hence  reduce 
the size of BST, of any memory block that has been moved 
from cache to main memory. If one were to discard the VCs 
of  block  when  the  block  moved  to  main  memory  then  one 
can  only  compute  the  DUE  contribution  of  that  block,  but 
cannot  distinguish  false  DUE  from  true  DUE  and  cannot 
detect  SDC.  When  a  block  that  resides  in  memory  is 
eventually consumed by the processor it leads to a true DUE 
or  SDC,  depending  on  the  error  correction  capability.  We 
first  demonstrate  how  sampling  can  accelerate  AVF 
simulations  with  reliability  lifetime  analysis.  We  apply 
sampling  schemes  to  accelerate  PARMA  and  MACAU 
simulations in Section VII by simulating the entire SPEC2K 
suite with reference inputs. 
C.  Existing benchmark sampling techniques 
Many  techniques  to  accelerate  architectural  simulations 
have  been  proposed  already  in  the  context  of  performance 
simulations.  They  can  be  broadly  classified  into  three 
categories: 1) simulation points based on phase analysis [20], 
2) random sampling based on inferential statistics [26], and 
3) reduced input sets to obtain similar execution profiles as 
for the original input sets [5][12].  
several  papers 
SimPoint  [20]  and  SMARTS  [26]  were  designed  to 
accelerate  performance  simulations.  These  two  sampling 
approaches have also been applied to reliability simulations 
in 
reliability 
simulations of large caches are fundamentally different from 
performance  simulations.  The  accuracy  of  cache  reliability 
studies  depends  on  accurately  keeping  track  of  VCs  over 
very  long  time  intervals.  Hence,  VCs  can  potentially  have 
values in the millions.  
[3][13][16].  However, 
reduced 
input  set 
Fig.  1  shows  the  weighted  histogram  of  VCs  from  the 
reliability-lifetime  analysis  of  an  L2  cache  for  the  facerec 
benchmark.  Fig.  1(a) shows  the distribution  of  VCs for  the 
MinneSPEC 
(more  details  about 
benchmarks  and  methodology  in  Section  V).  With  the 
MinneSPEC  reduced  input  sets  we  can  run  all  benchmark 
programs to completion. facerec with reduced input set has 
some  VC  intervals  lasting  up  to  1  million  cycles.  We 
divided  the  entire  VC  range  into  100  equal  bins.  We  then 
generated  a  histogram  of  the  number  of  accesses  that  fall 
into each of the bins.  Because longer VCs contribute  more 
to  vulnerability than shorter  VCs  and because  vulnerability 
is roughly proportional to VCs, we weight the histogram by 
multiplying the median value of each bin by the cycle count 
in that bin. Each Y-axis in Fig. 1 shows the contribution of a 
bin  to  the  total  VCs.  As  seen  from  Fig.  1(a)  (for  the  full 
simulation  without  sampling)  accesses  with  VCs  greater 
than  600,000  cycles  contribute  to  nearly  20%  of  the  total 
VCs, even with MinneSPEC reduced input set. In reliability 
simulations  the  rare case of  a  few accesses  with  very  large 
VC  windows  contributes 
in 
vulnerability.  Vulnerability  increases  with  VC  linearly  in 
the  AVF  model  and  super-linearly  in  the  PARMA  and 
MACAU models. 
to  significant 
increases 
Extremely  long  VC  interval  may  be  observed  because 
we  track  cache  block’s  reliability-lifetime  even  after  it  is 
dirty-evicted  to  memory.  When  the  cache  block  is  refilled 
into  the  cache  later,  we  let  its  VC  continue  to  grow  from 
where it was at the moment of dirty-eviction. As mentioned 
earlier,  this  approach  of  keeping  track  of  VCs  across 
3
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:38:09 UTC from IEEE Xplore.  Restrictions apply. 
multiple  cache  sojourns  allows  us  to  identify  SDCs,  and 
distinguish between true and false DUEs. Some prior studies 
ignore  to  track  cache  block’s  reliability-lifetime  at  the 
moment  of  eviction,  even  if  it  is  not  silently  evicted,  to 
simplify  the  model  or  to  reduce  memory  overhead  in 
simulation.  These  approaches  can  only  simply  compute 
DUEs  and  most  importantly  they  cannot  correctly  estimate 
SDCs which is regarded as the most important metric in soft 
error benchmarking.  
Fig.  1(b)  shows  the  distribution  of  weighted  VCs 
captured  using 
the  SMARTS  sampling  approach  as 
described in [26]. In each sample 2,000 instructions are used 
for detailed warming and are followed by 1,000 instructions 
of  detailed  simulation  (facerec  samples  1-in-17  windows). 
Rigorous reliability-lifetime analysis is done only during the 
detailed simulation period. Since SMARTS is biased toward 
smaller VC ranges as the detailed simulation only runs for a 
maximum of 1,000 instructions in each sample, many of the 
critical  VCs  slip  past  the  simulation  entirely  unnoticed.  Of 
course  we  can  increase  the  size  of  detailed  simulation 
intervals 
to  capture  larger  vulnerability 
windows  but  that  requires  running  detailed  simulation 
windows  of  multiple  million  cycles  even  for  MinneSPEC 
workloads, which effectively negates the fundamental speed 
advantage  of  the  SMARTS  approach.  In  Section  V.E,  we 
will explore a modified SMARTS approach to collect better 
samples  for  reliability  simulations.  Although  this  new 
approach based on SMARTS improves estimation accuracy, 
estimation  errors  still  fall  outside  the  error  margins  that 
SMARTS  guarantees.  The  reason  is  that  sampling  rates  in 
SMARTS are not determined by reliability metrics. 
in  SMARTS 
Fig. 1(c) shows the distribution of VCs  captured by the 
SimPoint  approach  as  described  in  [26].  We  use  a  single 
SimPoint of 50M instructions during which we perform full 
AVF simulation. The 50M instruction SimPoint includes 20% 
of  the  instructions  executed  by  this  benchmark.  Even  with 
such  a  large  simulation  window,  the  SimPoint  approach 
cannot  capture  many  of  the  large  VCs  because  the 
occurrence of very large VCs does not necessarily coincide 
with the most representative phase. With multiple SimPoints 
the  FIT  estimation  error  is  even  worse  because  each 
simulation interval is shorter (to keep the overall simulation 
time the same), contrary to the common belief that multiple 
SimPoints is better for fast changing phases. 
Confronting  these  challenges,  we  evaluate  sampling 
approaches  that  work  well  for  cache  reliability  simulations 
in the following sections. For example, Fig. 1(d) shows the 
distribution  of  VCs  captured  by  the  interval  sampling 
method discussed in Section III.A, with 10% sampling rate. 
Clearly  interval  sampling  is  much  better  at  capturing  the 
distribution  of  VCs.  Then  we  propose  a  reliability-aware 
sampling  technique  called  PHYS.  Unlike  SimPoint  or 
SMARTS, PHYS captures a snapshot of the VC population 
without distortion. 
III.  RANDOM SAMPLING 
One  can  directly  apply  statistical  random  sampling  to 
reliability simulations. This section explores how. 
4
A.  Interval sampling  
In interval sampling we randomly choose whether or not 
to track the  VCs  for the accessed  block in the future based 
on a target sampling rate. If an access is chosen for tracking, 
the  BST  is  searched  to  see  if  the  memory  block  already 
exists  in  the  BST,  in  case  it  was  brought  in  by  a  prior 
sampled  event.  If  the  entry  does  not  exist,  a  new  node  is 
created  in  the  BST  to  keep  track  of  its  VCs.  If  the  block 
already  exists in the  BST  then we simply  compute its VCs 
using the timestamps kept in the node. For all other accesses 
not selected by the sampling procedure the BST is searched; 
if the memory block is found,  we compute the  VCs for the 
existing  node  and  remove  the  node  thereby  ending  its  VC 
interval.  For  every  access  selected  by 
the  sampling 
procedure  we  keep  the  node  instead  of  removing  it.  The 
ability to track the VC intervals started at accesses selected 
randomly  and  monitored  beyond a  sampling  window is  the 
major novelty of interval sampling.  
B.  Set Sampling  
In  set sampling  we select  cache sets  at random so as to 
meet the target sampling rate. Set sampling was described in 
[14].  For  instance,  if  the  target  sampling  rate  is  10%  we 
randomly select 1/10th of the cache sets at the beginning of a 
simulation  and  mark  the  sets.  During  simulation  we  only 
track  events  that  occur  in  the  marked  cache  sets.  For 
instance,  when  a  new  memory  block  is  brought  into  a 
marked cache set we first create a node in the BST structure 
to  keep  track  of  the  VCs  for  each  word  in  that  block.  Any 
access  to  an  unmarked  cache  set  is  never  kept  in  the  data 
structure.  The  size  of  the  BST  still  grows  gradually  since 
every unique memory block that maps to a marked cache set 
and is accessed at least once occupies one node in the BST, 
but  this  growth  is  much  slower  than  in  the  complete 
simulation.  Set  sampling  is  biased  unless  the  sets  in  the 
cache  are  uniformly  accessed,  since  only  data  mapped  to 
specific  sets  are  tracked  and  data  mapped  to  other  sets  are 
ignored.  
C.  Extrapolating overall FIT from sampled FIT 
In  reliability-aware  sampling,  whenever  a  block  is 
accessed  in  the  cache,  a  decision  is  made  whether  to  track 
its  next  reliability-lifetime  interval.  If  it  is  decided  to  track 
the  block's  reliability-lifetime  interval,  then  the  interval  is 
sampled; otherwise, it is skipped. Throughout the workload 
execution,  we  count  the  numbers  of  sampled  and  skipped 
reliability-lifetime  intervals  until  the  workload  execution 
finishes.  After  a  sampled  run,  we  weight  the  sampled  FIT 
using  sample  and  skip  counters  accumulated  in  the 