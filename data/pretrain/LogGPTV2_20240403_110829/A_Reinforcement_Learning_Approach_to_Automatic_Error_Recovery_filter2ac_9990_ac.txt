2500
2000
1500
1000
500
0
Count
1 3 5 7 9 11 1315 17 1921 2325 27 2931 33 3537 39
Figure  5:  Count  of  40  most  frequent  error 
types 
Error type
Figure 5 shows the count of the selected error types. 
The  remaining  error  types,  since  they  are  much  less 
frequent,  still  need  more  time  to  accumulate  enough 
training  samples  by  monitoring  the  real  system.  The 
total  downtime  of  each  error  type  in  the  recovery 
processes controlled by the user-defined policy is given 
in Figure 6.  
10000000
1000000
)
.
c
e
s
(
t
s
o
C
e
m
T
i
100000
10000
1000
100
10
1
Real Time Cost
1 3 5 7 9 111315171921232527293133353739
Error type
Figure  6:  Total  downtime  of  40  most  frequent 
error types under user-defined policy 
4.2. Simulation platform 
Our  simulation  platform  is  built  to  compute  time 
cost for a repair action on a state based on the assump-
tions mentioned in Section 3.3 and the recovery log.  
To  verify  our  assumptions  and  the  settings  of  the 
simulation  platform,  we  run  the  platform  under  the 
user-defined  recovery  policy  of  the  real  system.  Be-
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:32:46 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007cause  we  could  not  refer  to  all  the  information  consi-
dered  by  the  user-defined  policy  from  the  log,  we 
could  only  expect  an  approximate  result.  Figure  7 
shows the results for the 40 most frequent error types. 
The relative time cost here is the ratio of the estimated 
time cost compared to the real one for each error type, 
which  is  also  used  as  the  evaluation  measure  in  the 
following experiments. 
1.05
1.04
1.03
1.02
1.01
1
0.99
0.98
0.97
t
s
o
c
e
m
i
t
e
v
i
t
a
e
R
l
Estimated Time Cost
1 3 5 7 9 111315171921232527293133353739
Error type
Figure  7:  Relative  cost  for  40  most  frequent 
errors  compared  to  real  ones.  Biggest  devia-
tion is less than 5%.  
We  can  see  that  the  time  costs  computed  by  the  si-
mulation  platform  are  close  to  the  real  ones  and  only 
one computed cost (error type 29) is slightly less than 
the real one. Therefore, by  using this platform  we can 
expect  a  conservative  evaluation  for  most  cases  and 
thus make a fair comparison between the trained policy 
and the original policy. 
5. Experimental results 
In this section, we first evaluate the policy original-
ly  trained  by  RL,  then  the  performance  of  the  hybrid 
approach. In each experiment  we  will apply the learn-
ing  algorithm  to  a  portion  of  the  log  to  train  a  policy, 
and  then  test  the  performance  of  the  policy  on  the  re-
maining  log.  The  training  set  and  the  test  set  are  di-
vided  according  to  time  order.  We  choose  20%,  40%, 
60% and 80% of the recovery log for used in the train-
ing,  thus  forming  four  tests  (test  1,  test  2,  test  3,  and 
test 4).  
5.1. Results of RL-trained policy 
Figure  8  shows  the  fractions  of  the  estimated  time 
cost of the trained policy with respect to the actual time 
cost for each error type. The time cost of the unhandled 
cases is not counted in the total cost.  
In  Figure  8,  the  four  plots  show  the  results  of  the 
four policies trained with 20, 40, 60 and 80 percent of 
the whole log. For most error types, the trained policy 
performs  almost  the  same  as  the  original  policy. 
Through our observation of the corresponding recovery 
log,  we  find  that  the  original  policy  has  already 
achieved  good  enough  recovery  steps.  This  is  hard  to 
optimize any  more based only on the existing log.  On 
the other hand,  we  find that  for some error types such 
as  1,  35,  and  39,  the  trained  policy  gains  a  significant 
improvement  over  the  original  policy,  reducing  the 
cost  to  nearly  half.  When  looking  at  the  policy  more 
closely,  we  find  that  the  trained  policy  for  most  error 
types is nearly the same as the original one. The devia-
tion  of  the  time  cost  for  some  error  types  (e.g.  6,  10, 
and 23) comes from simulation error (see Section 3.1). 
For error type 1, 35, and 39, the trained policy will try 
a stronger repair action at the beginning instead of the 
weakest  one  as  done  by  the  original  policy.  Since  the 
stronger action is more effective in recovering the sys-
tem  from  the  error,  it  gains  a  big  savings  in  recovery 
time without trying the weaker actions first and waiting 
to find out that they do not work.  
t
s
o
c
e
m
i
t
e
v
i
t
l
a
e
R
1.4
1.2
1
0.8
0.6
0.4
0.2
0
0.2
0.4
0.6
0.8
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39
Figure  8:  Relative  time  cost  for  trained  policy 
compared to real one  
Error type
The  overall  absolute  time  cost  for  the  different  test 
sets  is  shown  in  Figure  9.  We  can  see  that  the  trained 
policy  can  always  gain  over  10%  time  savings  in  the 
four tests. In particular, the policy trained from 40% of 
the log results in only 89.02% of the original downtime 
on  the  remaining  log.  Here,  we  only  summarize  the 
total time cost of the cases that could be handled by our 
trained  policy.  Since  some  unhandled  cases  exist  that 
will  be  discussed  in  the  next  paragraph,  the  total  time 
cost is a little less than the following experiment. 
User-defined policy
Trained policy
35
30
25
20
15
10
5
0
)
.
c
e
s
n
o
i
l
l
i
M
(
t
s
o
c
e
m
i
t
l
a
t
o
T
1
2
3
4
Figure  9:  Total  time  cost  of  trained  policy  un-
der different tests  
Test number
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:32:46 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007Finally, we present the coverage of the trained poli-
cy  in  Figure  10,  that  is,  the  percentage  of  the  errors  it 
can handle. For each test, only a small number of error 
types  cannot  be  handled  and  even  in  these  cases  the 
coverage  is  still  more  than  90%.  Besides,  the  unhan-
dled cases decrease dramatically with the more training 
data used. 
1.02
1
e
t
a
r
e
g
a
r
e
v
o
C
0.98
0.96
0.94
0.92
0.9
0.88
0.2
0.4
0.6
0.8
1 3 5 7 9 111315171921232527293133353739
Error type
Figure 10: Coverage of the trained policy. 
5.2. Results of hybrid approach 
To  solve  the  noisy  cases  not  covered  by  the  RL-
trained  policy,  we  combine  it  with  the  original  user-
defined  defined  policy.  Figure  11  shows  two  results 
comparing  the  pure  RL  approach  and  the  hybrid  ap-
proach. 
t
s
o
c
e
m
i
t
e
v
i
t
l
a
e
R
t
s
o
c
e
m
i
t
e
v
i
t
l
a
e
R
1.8
1.6
1.4
1.2
1
0.8
0.6
0.4
0.2
0
1.2
1
0.8
0.6
0.4
0.2
0
Trained policy
Hybrid policy
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39
Error type
(a) Training set proportion = 0.2 
Trained policy
Hybrid policy
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39
Error type
(b) Training set proportion = 0.4 
Figure  11:  Performance  comparison  between 
trained policy and hybrid policy  
For the policy trained  with 20% of the log, the per-
formance of the hybrid approach is almost the same as 