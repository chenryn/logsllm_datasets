### 4.1 Analysis of Error Types

**Figure 5: Count of the 40 Most Frequent Error Types**

- **Error Type Count:** The figure below shows the count of the 40 most frequent error types.
- **Remaining Error Types:** These less frequent error types require more time to accumulate sufficient training samples through real system monitoring.

```
2500
2000
1500
1000
500
0
Count
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39
```

**Figure 6: Total Downtime of the 40 Most Frequent Error Types Under User-Defined Policy**

- **Downtime Calculation:** The total downtime for each error type during the recovery processes controlled by the user-defined policy is shown in the following figure.

```
10000000
1000000
100000
10000
1000
100
10
1
Real Time Cost
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39
```

### 4.2 Simulation Platform

- **Platform Description:** Our simulation platform is designed to compute the time cost for a repair action based on the assumptions outlined in Section 3.3 and the recovery log.
- **Verification:** To validate our assumptions and the settings of the simulation platform, we ran the platform under the user-defined recovery policy of the real system. Due to limited information from the log, the results are approximate.
- **Relative Time Cost:** Figure 7 illustrates the relative time cost for the 40 most frequent error types, with the biggest deviation being less than 5%.

```
1.05
1.04
1.03
1.02
1.01
1
0.99
0.98
0.97
Relative Estimated Time Cost
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39
```

- **Conclusion:** The computed time costs by the simulation platform are close to the real ones, with only one computed cost (error type 29) slightly lower. This platform provides a conservative evaluation, allowing for a fair comparison between the trained and original policies.

### 5. Experimental Results

#### 5.1 Results of RL-Trained Policy

- **Evaluation Method:** We evaluate the policy trained by Reinforcement Learning (RL) and the performance of the hybrid approach. In each experiment, the learning algorithm is applied to a portion of the log to train a policy, which is then tested on the remaining log.
- **Training and Test Sets:** The sets are divided according to time order, with 20%, 40%, 60%, and 80% of the recovery log used for training, forming four tests (test 1, test 2, test 3, and test 4).

**Figure 8: Relative Time Cost for Trained Policy Compared to Real One**

- **Performance Comparison:** The figure below shows the fractions of the estimated time cost of the trained policy with respect to the actual time cost for each error type.

```
1.4
1.2
1
0.8
0.6
0.4
0.2
0
0.2
0.4
0.6
0.8
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39
```

- **Observations:** For most error types, the trained policy performs similarly to the original policy. However, for some error types (e.g., 1, 35, and 39), the trained policy significantly improves, reducing the cost by nearly half. The trained policy often tries a stronger repair action initially, which is more effective and saves recovery time.

**Figure 9: Total Time Cost of Trained Policy Under Different Tests**

- **Time Savings:** The overall absolute time cost for different test sets is shown below. The trained policy consistently achieves over 10% time savings in all four tests.

```
User-defined policy
Trained policy
35
30
25
20
15
10
5
0
Total Time Cost (Minutes)
1 2 3 4
```

- **Coverage:** Figure 10 presents the coverage of the trained policy, showing that even with a small number of unhandled error types, the coverage is still over 90%. The unhandled cases decrease with more training data.

```
1.02
1
0.98
0.96
0.94
0.92
0.9
0.88
0.2
0.4
0.6
0.8
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39
```

#### 5.2 Results of Hybrid Approach

- **Hybrid Approach:** To address the noisy cases not covered by the RL-trained policy, we combine it with the original user-defined policy.
- **Performance Comparison:** Figure 11 compares the pure RL approach and the hybrid approach for two different training set proportions.

**Figure 11: Performance Comparison Between Trained Policy and Hybrid Policy**

- **Training Set Proportion = 0.2:**

```
1.8
1.6
1.4
1.2
1
0.8
0.6
0.4
0.2
0
1.2
1
0.8
0.6
0.4
0.2
0
Trained policy
Hybrid policy
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39
```

- **Training Set Proportion = 0.4:**

```
1.8
1.6
1.4
1.2
1
0.8
0.6
0.4
0.2
0
1.2
1
0.8
0.6
0.4
0.2
0
Trained policy
Hybrid policy
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39
```

- **Conclusion:** For the policy trained with 20% of the log, the performance of the hybrid approach is almost the same as the pure RL approach, but with improved coverage and handling of noisy cases.