three datasets when our defense is not used.
RG
NN
RF
NSH
NN-AT
NN-R
Location Texas100 CH-MNIST
50.0%
73.0%
73.7%
81.1%
64.6%
72.9%
50.0%
68.9%
67.3%
74.0%
68.3%
69.2%
50.0%
62.9%
58.7%
58.4%
63.3%
63.0%
classifiers are fully-connected neural networks with 2, 3, and 4
hidden layers, respectively. The hidden layers of the three defense
classifiers have (256, 128), (256, 128, 64), and (512, 256, 128, 64)
neurons, respectively. The output layer has just one neuron. The
activation function for the neurons in the hidden layers is ReLU ,
while the neuron in the output layer uses the sigmoid activation
function. Unless otherwise mentioned, we use the defense classifier
with 3 hidden layers. The defender calculates the confidence score
vector for each data sample in D1 and D3 using the target classifier.
The confidence score vectors for data samples in D1 and D3 have
labels “member” and “non-member”, respectively. The defender
treats these confidence score vectors as a training dataset to learn a
defense classifier, which takes a confidence score vector as an input
and predicts member or non-member. We train a defense classifier
for 400 epochs with a learning rate 0.001. We note that we can also
synthesize data samples based on D1 as non-members (Appendix A
shows details).
Parameter setting: We set max_iter = 300 and β = 0.1 in Algo-
rithm 1. We found that once max_iter is larger than some threshold,
MemGuard’s effectiveness does not change. Since we aim to find
representative noise vector that does not change the predicted la-
bel, we assign a relatively large value to c2, which means that the
objective function has a large value if the predicted label changes
(i.e., the loss function L2 is non-zero). In particular, we set c2 = 10.
Our Algorithm 1 searches for a large c3 and we set the initial value
of c3 to be 0.1. We also compare searching c2 with searching c3.
0.00.20.40.60.81.0ConﬁdenceScoreDistortionBudget,0.40.50.60.70.80.91.0InferenceAccuracyRGNNRFNSHNN-ATNN-R0.00.20.40.60.81.0ConﬁdenceScoreDistortionBudget,0.40.50.60.70.80.91.0InferenceAccuracyRGNNRFNSHNN-ATNN-R0.00.20.40.60.81.0ConﬁdenceScoreDistortionBudget,0.40.50.60.70.80.91.0InferenceAccuracyRGNNRFNSHNN-ATNN-R(a) Location, without defense
(b) Texas100, without defense
(c) CH-MNIST, without defense
(d) Location, with defense
(e) Texas100, with defense
(f) CH-MNIST, with defense
Figure 2: Distribution of the normalized entropy of the confidence score vectors for members and non-members of the target
classifier. Figures on the upper side are results without our defense, and figures on the lower side are results with our defense.
5.2 Experimental Results
MemGuard is effective: Figure 1 shows the inference accuracies
of different attacks as the confidence score distortion budget in-
creases on the three datasets. Since we adopt the expected L1-norm
of the noise vector to measure the confidence score distortion, the
confidence score distortion is in the range [0, 2]. Note that our
defense is guaranteed to achieve 0 label loss as our Algorithm 1
guarantees that the predicted label does not change when searching
for the representative noise vector. We observe that our MemGuard
can effectively defend against membership inference attacks, i.e.,
the inference accuracies of all the evaluated attacks decrease as
our defense is allowed to add larger noise to the confidence score
vectors. For instance, on Location, when our defense is allowed
to add noise whose expected L1-norm is around 0.8, our defense
can reduce all the evaluated attacks to the random guessing (RG)
attack; on CH-MNIST, our defense can reduce the NSH attack (or
the remaining attacks) to random guessing when allowed to add
noise whose expected L1-norm is around 0.3 (or 0.7).
Indistinguishability between the confidence score vectors of
members and non-members: We follow previous work [42] to
study the distribution of confidence score vectors of members vs.
non-members of the target classifier. Specifically, given a confidence
score vector s, we compute its normalized entropy as follows:

j
Normalized entropy: − 1
log k
sj log(sj),
(24)
where k is the number of classes in the target classifier. Figure 2
shows the distributions of the normalized entropy of the confi-
dence score vectors for members (i.e., data samples in D1) and non-
members (i.e., data samples in D4) of the target classifier, where
(a) Searching c3
(b) Searching c2
Figure 3: Inference accuracy of the NN attack as the con-
fidence score distortion budget increases on the Location
dataset when searching c3 or c2.
we set the confidence score distortion budget ϵ to be 1 when our
defense is used. The gap between the two curves in a graph corre-
sponds to the information leakage of the target classifier’s training
dataset. Our defense substantially reduces such gaps. Specifically,
the maximum gap between the two curves (without defense vs.
with defense) is (0.27 vs. 0.11), (0.41 vs. 0.05), and (0.30 vs. 0.06)
on the Location, Texas100, and CH-MNIST datasets, respectively.
Moreover, the average gap between the two curves (without defense
vs. with defense) is (0.062 vs. 0.011), (0.041 vs. 0.005), and (0.030 vs.
0.006) on the three datasets, respectively.
Searching c2 vs. searching c3: Figure 3a shows the inference ac-
curacy of the NN attack as the confidence score distortion bud-
get increases when fixing c2 to different values and searching c3.
Figure 3b shows the results when fixing c3 and searching c2. We
observe that MemGuard is insensitive to the setting of c2 when
searching c3. Specifically, MemGuard has almost the same effec-
tiveness when fixing c2 to different values, i.e., the different curves
0.00.20.40.60.81.0NormalizedEntropy0.00.20.40.60.81.0FrequencyMemberNon-member0.00.20.40.60.81.0NormalizedEntropy0.00.20.40.60.81.0FrequencyMemberNon-member0.00.20.40.60.81.0NormalizedEntropy0.00.20.40.60.81.0FrequencyMemberNon-member0.00.20.40.60.81.0NormalizedEntropy0.00.20.40.60.81.0FrequencyMemberNon-member0.00.20.40.60.81.0NormalizedEntropy0.00.20.40.60.81.0FrequencyMemberNon-member0.00.20.40.60.81.0NormalizedEntropy0.00.20.40.60.81.0FrequencyMemberNon-member0.00.20.40.60.81.0ConﬁdenceScoreDistortionBudget,0.40.50.60.70.80.91.0InferenceAccuracySearchc3,c2=0.1Searchc3,c2=1Searchc3,c2=10Searchc3,c2=1000.00.20.40.60.81.0ConﬁdenceScoreDistortionBudget,0.40.50.60.70.80.91.0InferenceAccuracySearchc3,c2=10Searchc2,c3=0.1Searchc2,c3=1.0Table 5: Results of Model Stacking.
Inference Acc.
Average Distortion
Label Loss
Location Texas100 CH-MNIST
50.0%
1.63
56.3%
50.0%
0.81
18.3%
50.8%
1.28
37.9%
Figure 4: Inference accuracy of the NN attack as the con-
fidence score distortion budget increases on the Location
dataset when using different defense classifiers.
overlap in Figure 3a. This is because when our Phase I stops search-
ing the noise vector, the predicted label is preserved, which means
that the loss function L2 is 0. However, MemGuard is sensitive to
the setting of c3 when searching c2. Specifically, when fixing c3 to
be 0.1, searching c2 achieves the same effectiveness as searching
c3. However, when fixing c3 to be 1.0, searching c2 is less effective.
Therefore, we decided to search c3 while fixing c2.
Impact of defense classifiers: Figure 4 shows the inference ac-
curacy of the NN attack as the confidence score distortion budget
increases on the Location dataset when using different defense
classifiers. We observe that MemGuard has similar effectiveness for
different defense classifiers, which means that our carefully crafted
noise vectors can transfer between classifiers.
MemGuard outperforms existing defenses: We compare with
state-of-the-art defenses including L2-Regularizer [58], Min-Max
Game [42], Dropout [56], Model Stacking [56], and DP-SGD [1].
Each compared defense (except Model Stacking) has a hyperpa-
rameter to control the privacy-utility tradeoff. For instance, the
hyperparameter that balances between the loss function and the L2
regularizer in L2-Regularizer, the hyperparameter that balances be-
tween the loss function and the adversarial regularizer in Min-Max
Game, the dropout rate in Dropout, the privacy budget in DP-SGD,
and ϵ in MemGuard. We also compare with MemGuard-Random in
which we use the Random method (refer to Section 4.2) to generate
the noise vector in Phase I.
Before deploying any defense, we use the undefended target clas-
sifier to compute the confidence score vector for each data sample
in the evaluation dataset D1 ∪ D4. For each defense and a given
hyperparameter, we apply the defense to the target classifier and
use the defended target classifier to compute the confidence score
vector for each data sample in D1 ∪ D4. Then, we compute the con-
fidence score distortion for each data sample and obtain the average
confidence score distortion on the evaluation dataset D1 ∪ D4. More-
over, we compute the inference accuracy of the attack classifier
(we consider NN in these experiments) on the evaluation dataset
after the defense is used. Therefore, for each defense and a given
hyperparameter, we can obtain a pair (inference accuracy, average
confidence score distortion). Via exploring different hyperparame-
ters, we can obtain a set of such pairs for each defense. Then, we
plot these pairs on a graph, which is shown in Figure 5.
Specifically, we tried the hyperparameter of L2-Regularizer in
the range [0, 0.05] with a step size 0.005, 0.001, and 0.005 for Lo-
cation, Texas100, and CH_MNIST datasets, respectively. We tried
the hyperparameter of Min-Max Game in the range [0, 3] with a
step size 0.5. We tried the dropout rate of Dropout in the range
[0, 0.9] with a step size 0.1. We use a publicly available implemen-
tation4 of DP-SGD. We tried the parameter noise_multiplier that
controls the privacy budget in the range [0, 0.2] with a step size
0.05. We tried [0, 0.1, 0.3, 0.5, 0.7, 1.0] as the ϵ in MemGuard and
MemGuard-Random.
Our results show that MemGuard achieves the best privacy-
utility tradeoff. In particular, given the same average confidence
score distortion, MemGuard achieves the smallest inference ac-
curacy. According to the authors of Model Stacking, it does not
have a hyperparameter to easily control the privacy-utility tradeoff.
Therefore, we just obtain one pair of (inference accuracy, average
confidence score distortion) and Table 5 shows the results. Model
Stacking reduces the inference accuracy to be close to 0.5, but the
utility loss is intolerable.
Similarly, we can obtain a set of pairs (inference accuracy, label
loss) for the compared defenses and Figure 6 shows inference accu-
racy vs. label loss on the three datasets. Label loss is the fraction of
data samples in the evaluation dataset whose predicted labels are
changed by a defense. MemGuard-Random and MemGuard achieve
0 label loss. However, other defenses incur large label losses in
order to substantially reduce the attacker’s inference accuracy.
6 DISCUSSION AND LIMITATIONS
On one hand, machine learning can be used by attackers to perform
automated inference attacks. On the other hand, machine learning
has various vulnerabilities, e.g., adversarial examples [10, 19, 31, 47–
50, 62]. Therefore, attackers who rely on machine learning also
share its vulnerabilities and we can exploit such vulnerabilities
to defend against them. For instance, we can leverage adversarial
examples to mislead attackers who use machine learning classifiers
to perform automated inference attacks [27]. One key challenge in
this research direction is how to extend existing adversarial example
methods to address the unique challenges of privacy protection.
For instance, how to achieve formal utility-loss guarantees.
In this work, we focus on membership inference attacks under
the black-box setting, in which an attacker uses a binary classifier
to predict a data sample to be a member or non-member of a target
classifier’s training dataset. In particular, the attacker’s classifier
takes a data sample’s confidence score vector predicted by the target
classifier as an input and predicts member or non-member. Our
defense adds carefully crafted noise to a confidence score vector to
turn it into an adversarial example, such that the attacker’s classifier
4https://github.com/tensorflow/privacy
0.00.20.40.60.81.0ConﬁdenceScoreDistortionBudget,0.40.50.60.70.80.91.0InferenceAccuracy2HiddenLayers3HiddenLayers4HiddenLayers(a) Location
(b) Texas100
(c) CH-MNIST
Figure 5: Inference accuracy vs. average confidence score distortion of the compared defenses. Our MemGuard achieves the
best privacy-utility tradeoff.
(a) Location
(b) Texas100
(c) CH-MNIST
Figure 6: Inference accuracy vs. label loss of the compared defenses. Both MemGuard-Random and MemGuard achieve 0 label
loss, while the other defenses incur large label losses in order to substantially reduce the attacker’s inference accuracy.
is likely to predict member or non-member incorrectly. To address
the challenges of achieving formal utility-loss guarantees, e.g., 0
label loss and bounded confidence score distortion, we design new
methods to find adversarial examples.
Other than membership inference attacks, many other attacks
rely on machine learning classifiers, e.g., attribute inference at-
tacks [11, 17, 28], website fingerprinting attacks [7, 22, 29, 46, 67],
side-channel attacks [73], location attacks [5, 45, 52, 72], and author
identification attacks [8, 41]. For instance, online social network
users are vulnerable to attribute inference attacks, in which an at-
tacker leverages a machine learning classifier to infer users’ private
attributes (e.g., gender, political view, and sexual orientation) using
their public data (e.g., page likes) on social networks. The Face-
book data privacy scandal in 20185 is a notable example of attribute
inference attack. In particular, Cambridge Analytica leveraged a
machine learning classifier to automatically infer a large amount
of Facebook users’ various private attributes using their public
page likes. Jia and Gong proposed AttriGuard [26], which leverages
adversarial examples to defend against attribute inference attacks.
In particular, AttriGuard extends an existing adversarial example
method to incorporate the unique challenges of privacy protection.
The key difference between MemGuard and AttriGuard is that find-
ing adversarial examples for confidence score vectors is subject
to unique constraints, e.g., an adversarial confidence score vector
should still be a probability distribution and the predicted label
should not change. Such unique constraints require substantially
5https://bit.ly/2IDchsx
different methods to find adversarial confidence score vectors. Other
studies have leveraged adversarial examples to defend against traf-
fic analysis [71] and author identification [38, 54]. However, these
studies did not consider formal utility-loss guarantees.
We believe it is valuable future work to extend MemGuard to
defend against other machine learning based inference attacks
such as website fingerprinting attacks, side-channel attacks, and
membership inference attacks in the white-box setting. Again, a
key challenge is how to achieve formal utility-loss guarantees with
respect to certain reasonable utility-loss metrics.
Our MemGuard has a parameter ϵ, which controls a tradeoff be-
tween membership privacy and confidence score vector distortion.
The setting of ϵ may be dataset-dependent. One way to set ϵ is to
leverage an inference accuracy vs. ϵ curve as shown in Figure 1.
Specifically, given a dataset, we draw the inference accuracy vs.
ϵ curves for various attack classifiers. Suppose we desire the in-
ference accuracy to be less than a threshold. Then, we select the
smallest ϵ such that the inference accuracies of all the evaluated
attack classifiers are no larger than the threshold.
7 CONCLUSION AND FUTURE WORK
In this work, we propose MemGuard to defend against black-box
membership inference attacks. MemGuard is the first defense that
has formal utility-loss guarantees on the confidence score vectors
predicted by the target classifier. MemGuard works in two phases.
In Phase I, MemGuard leverages a new algorithm to find a care-
fully crafted noise vector to turn a confidence score vector into
0.00.20.40.60.81.0AverageConﬁdenceScoreDistortion0.40.50.60.70.80.91.0InferenceAccuracyL2-RegularizerMin-MaxGameDropoutDP-SGDMemGuard-RandomMemGuard0.00.20.40.60.81.0AverageConﬁdenceScoreDistortion0.40.50.60.70.80.91.0InferenceAccuracyL2-RegularizerMin-MaxGameDropoutDP-SGDMemGuard-RandomMemGuard0.00.20.40.60.81.0AverageConﬁdenceScoreDistortion0.40.50.60.70.80.91.0InferenceAccuracyL2-RegularizerMin-MaxGameDropoutDP-SGDMemGuard-RandomMemGuard0.00.10.20.30.40.5LabelLoss0.40.50.60.70.80.91.0InferenceAccuracyL2-RegularizerMin-MaxGameDropoutDP-SGDMemGuard-RandomMemGuard0.00.10.20.30.40.5LabelLoss0.40.50.60.70.80.91.0InferenceAccuracyL2-RegularizerMin-MaxGameDropoutDP-SGDMemGuard-RandomMemGuard0.00.10.20.30.40.5LabelLoss0.40.50.60.70.80.91.0InferenceAccuracyL2-RegularizerMin-MaxGameDropoutDP-SGDMemGuard-RandomMemGuardan adversarial example. The new algorithm considers the unique
utility-loss constraints on the noise vector. In Phase II, MemGuard
adds the noise vector to the confidence score vector with a certain
probability, for which we derive an analytical solution. Our empiri-
cal evaluation results show that MemGuard can effectively defend
against black-box membership inference attacks and outperforms
existing defenses. An interesting future work is to extend Mem-
Guard to defend against other types of machine learning based