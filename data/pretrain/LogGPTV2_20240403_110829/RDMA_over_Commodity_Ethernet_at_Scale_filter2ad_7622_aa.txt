title:RDMA over Commodity Ethernet at Scale
author:Chuanxiong Guo and
Haitao Wu and
Zhong Deng and
Gaurav Soni and
Jianxi Ye and
Jitu Padhye and
Marina Lipshteyn
RDMA over Commodity Ethernet at Scale
Chuanxiong Guo, Haitao Wu, Zhong Deng, Gaurav Soni,
Jianxi Ye, Jitendra Padhye, Marina Lipshteyn
{chguo, hwu, zdeng, gasoni, jiye, padhye, malipsht}@microsoft.com
Microsoft
ABSTRACT
Over the past one and half years, we have been using
RDMA over commodity Ethernet (RoCEv2) to support
some of Microsoft’s highly-reliable, latency-sensitive ser-
vices. This paper describes the challenges we encoun-
tered during the process and the solutions we devised to
address them. In order to scale RoCEv2 beyond VLAN,
we have designed a DSCP-based priority ﬂow-control
(PFC) mechanism to ensure large-scale deployment. We
have addressed the safety challenges brought by PFC-
induced deadlock (yes, it happened!), RDMA transport
livelock, and the NIC PFC pause frame storm problem.
We have also built the monitoring and management
systems to make sure RDMA works as expected. Our
experiences show that the safety and scalability issues
of running RoCEv2 at scale can all be addressed, and
RDMA can replace TCP for intra data center commu-
nications and achieve low latency, low CPU overhead,
and high throughput.
CCS Concepts
•Networks → Network protocol design; Network
experimentation; Data center networks;
Keywords
RDMA; RoCEv2; PFC; PFC propagation; Deadlock
1.
INTRODUCTION
With the rapid growth of online services and cloud
computing,
large-scale data centers (DCs) are being
built around the world. High speed, scalable data cen-
ter networks (DCNs) [1, 3, 19, 31] are needed to connect
the servers in a DC. DCNs are built from commodity
Permission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for proﬁt or commercial advantage and that copies bear this notice
and the full citation on the ﬁrst page. Copyrights for components of this work
owned by others than the author(s) must be honored. Abstracting with credit is
permitted. To copy otherwise, or republish, to post on servers or to redistribute to
lists, requires prior speciﬁc permission and/or a fee. Request permissions from
permissions@acm.org.
SIGCOMM ’16, August 22 - 26, 2016, Florianopolis , Brazil
c(cid:13) 2016 Copyright held by the owner/author(s). Publication rights licensed to
ACM. ISBN 978-1-4503-4193-6/16/08. . . $15.00
DOI: http://dx.doi.org/10.1145/2934872.2934908
Ethernet switches and network interface cards (NICs).
A state-of-the-art DCN must support several Gb/s or
higher throughput between any two servers in a DC.
TCP/IP is still the dominant transport/network stack
in today’s data center networks. However, it is increas-
ingly clear that the traditional TCP/IP stack cannot
meet the demands of the new generation of DC work-
loads [4, 9, 16, 40], for two reasons.
First, the CPU overhead of handling packets in the
OS kernel remains high, despite enabling numerous hard-
ware and software optimizations such as checksum of-
ﬂoading, large segment oﬄoad (LSO), receive side scal-
ing (RSS) and interrupt moderation. Measurements in
our data centers show that sending at 40Gb/s using 8
TCP connections chews up 6% aggregate CPU time on
a 32 core Intel Xeon E5-2690 Windows 2012R2 server.
Receiving at 40Gb/s using 8 connections requires 12%
aggregate CPU time. This high CPU overhead is unac-
ceptable in modern data centers.
Second, many modern DC applications like Search
are highly latency sensitive [7, 15, 41]. TCP, however,
cannot provide the needed low latency even when the
average traﬃc load is moderate, for two reasons. First,
the kernel software introduces latency that can be as
high as tens of milliseconds [21]. Second, packet drops
due to congestion, while rare, are not entirely absent
in our data centers. This occurs because data center
traﬃc is inherently bursty. TCP must recover from the
losses via timeouts or fast retransmissions, and in both
cases, application latency takes a hit.
In this paper we summarize our experience in deploy-
ing RoCEv2 (RDMA over Converged Ethernet v2) [5],
an RDMA (Remote Direct Memory Access) technol-
ogy [6], to address the above mentioned issues in Mi-
crosoft’s data centers. RDMA is a method of accessing
memory on a remote system without interrupting the
processing of the CPU(s) on that system. RDMA is
widely used in high performance computing with In-
ﬁniband [6] as the infrastructure. RoCEv2 supports
RDMA over Ethernet instead of Inﬁniband.
Unlike TCP, RDMA needs a lossless network;
i.e.
there must be no packet loss due to buﬀer overﬂow at
the switches. RoCEv2 uses PFC (Priority-based Flow
Control) [14] for this purpose. PFC prevents buﬀer
202
Figure 2: How PFC works.
- 20 meters, and the Leaf and Spine switches are within
the distance of 200 - 300 meters. With three layers of
switches, tens to hundreds of thousands of servers can
be connected in a single data center. In this paper, we
focus on supporting RDMA among servers under the
same Spine switch layer.
RoCEv2: We deployed RDMA over Converged Eth-
ernet v2 (RoCEv2) [5] for both technical and econom-
ical reasons. RoCEv2 encapsulates an RDMA trans-
port [5] packet within an Ethernet/IPv4/UDP packet.
This makes RoCEv2 compatible with our existing net-
working infrastructure. The UDP header is needed for
ECMP-based [34] multi-path routing. The destination
UDP port is always set to 4791, while the source UDP
port is randomly chosen for each queue pair (QP) [5].
The intermediate switches use standard ﬁve-tuple hash-
ing. Thus, traﬃc belonging to the same QP follows the
same path, while traﬃc on diﬀerent QPs (even between
the same pair of communicating end points) can follow
diﬀerent paths.
PFC and buﬀer reservation: RoCEv2 uses PFC [14]
to prevent buﬀer overﬂow. The PFC standard speci-
ﬁes 8 priority classes to reduce the head-of-line blocking
problem. However, in our network, we are able to use
only two of these eight priorities for RDMA. The reason
is as follows.
PFC is a hop-by-hop protocol between two Ethernet
nodes. As show in Figure 2, the sender’s egress port
sends data packets to the receiver’s ingress port. At the
sending egress port, packets are queued in up to eight
queues. Each queue maps to a priority. At the receiv-
ing ingress port, packets are buﬀered in corresponding
ingress queues.
In the shared-buﬀer switches used in
our network, an ingress queue is implemented simply as
a counter – all packets share a common buﬀer pool.
Once the ingress queue length reaches a certain thresh-
old (XOFF), the switch sends out a PFC pause frame
to the corresponding upstream egress queue. After the
egress queue receives the pause frame, it stops sending
packets. A pause frame carries which priorities need to
be paused and the pause duration. Once the ingress
queue length falls below another threshold (XON), the
switch sends a pause with zero duration to resume trans-
mission. XOFF must be set conservatively to ensure
that there is no buﬀer overﬂow, while XON needs be
set to ensure that there is no buﬀer underﬂow.
It takes some time for the pause frame to arrive at
the upstream egress port, and for the switch to react to
Figure 1: Our goal is to support RDMA for intra data
center (intra-DC) communications.
overﬂow by pausing the upstream sending entity when
buﬀer occupancy exceeds a speciﬁed threshold. While
some problems with PFC such as head-of-the line block-
ing and potential for deadlock are well known [22, 33],
we see several issues such as the RDMA transport live-
lock, the NIC PFC pause frame storm and the slow-
receiver symptom in our deployment that have not been
reported in the literature. Even the root cause of the
deadlock problem we have encountered is quite diﬀerent
from the toy examples often discussed in the research
literature [22, 33].
We also note that VLAN [32] tags are typically used
to identify PFC-enabled traﬃc in mixed RDMA/TCP
deployments. As we shall discuss, this solution does
not scale for our environment. Thus, we introduce a
notion of DSCP (Diﬀerentiated Services Code Point)
based PFC to scale RDMA from layer-2 VLAN to layer-
3 IP.
Our RDMA deployment has now been running smoothly
for over one and half years, and it supports some of Mi-
crosoft’s highly-reliable and latency-sensitive online ser-
vices. Our experience shows that, by improving the de-
sign of RoCEv2, by addressing the various safety issues,
and by building the needed management and monitor-
ing capabilities, we can deploy RDMA safely in large-
scale data centers using commodity Ethernet.
2. BACKGROUND
Our data center network is an Ethernet-based multi-
layer Clos network [1, 3, 19, 31] as shown in Figure 1.
Twenty to forty servers connect to a top-of-rack (ToR)
switch. Tens of ToRs connect to a layer of Leaf switches.
The Leaf switches in turn connect to a layer of tens to
hundreds of Spine switches. Most links are 40Gb/s,
and we plan to upgrade to 50GbE and 100GbE in near
future [11, 25]. All switches use IP routing.
The servers typically use copper cables of around
2 meters to connect to the ToR switches. The ToR
switches and Leaf switches are within the distance of 10
203
it. During this time, the upstream port will continue to
transmit packets. Thus, the ingress port must reserve
buﬀer space for each priority to absorb packets that
arrive during this “gray period”. This reserved buﬀer is
called headroom. The size of the headroom is decided by
the MTU size, the PFC reaction time of the egress port,
and most importantly, the propagation delay between
the sender and the receiver.
The propagation delay is determined by the distance
between the sender and the receiver. In our network,
this can be as large as 300 meters. Given that our ToR
and Leaf switches have shallow buﬀers (9MB or 12MB),
we can only reserve enough headroom for two lossless
traﬃc classes even though the switches support eight
traﬃc classes. We use one lossless class for real-time
traﬃc and the other for bulk data transfer.
Need for congestion control: PFC works hop by
hop. There may be several hops from the source server
to the destination server. PFC pause frames propagate
from the congestion point back to the source if there is
persistent network congestion. This can cause problems
like unfairness and victim ﬂow [42].
In order to reduce this collateral damage, ﬂow based
congestion control mechanisms including QCN [13], DC-
QCN [42] and TIMELY [27] have been introduced. We
use DCQCN, which uses ECN for congestion notiﬁca-
tion, in our network. We chose DCQCN because it di-
rectly reacts to the queue lengths at the intermediate
switches and ECN is well supported by all the switches
we use. Small queue lengths reduce the PFC generation
and propagation probability.
Though DCQCN helps reduce the number of PFC
pause frames, it is PFC that protects packets from being
dropped as the last defense. PFC poses several safety is-
sues which are the primary focus of this paper and which
we will discuss in Section 4. We believe the lessons we
have learned in this paper apply to the networks using
TIMELY as well.
Coexistence of RDMA and TCP: In this paper,
RDMA is designed for intra-DC communications. TCP
is still needed for inter-DC communications and legacy
applications. We use a diﬀerent traﬃc class (which is
not lossless), with reserved bandwidth, for TCP. Diﬀer-
ent traﬃc classes isolate TCP and RDMA traﬃc from
each other.
3. DSCP-BASED PFC
In this section we examine the issues faced by the
original VLAN-based PFC and present our DSCP-based
PFC solution. VLAN-based PFC carries packet prior-
ity in the VLAN tag, which also contains VLAN ID.
The coupling of packet priority and VLAN ID created
two serious problems in our deployment, leading us to
develop a DSCP-based PFC solution.
Figure 3(a) shows the packet formats of the PFC
pause frame and data packets in the original VLAN-
based PFC. The pause frame is a layer-2 frame, and
(a) VLAN-based PFC.
(b) DSCP-based PFC.
Figure 3: The packet formats of VLAN-based PFC and
DSCP-based PFC. Note that the PFC pause frame for-
mat is the same in both Figure 3(a) and Figure 3(b).
does not have a VLAN tag. The VLAN tag for the
data packet has four parts: TPID which is ﬁxed to
0x8100, DEI (Drop Eligible Indicator), PCP (Priority
Code Point) which is used to carry packet priority, and
VID (VLAN identiﬁer) which carries the VLAN ID of
the packet.
For our purpose, although we need only PCP, VID
and PCP cannot be separated. Thus, to support PFC,
we have to conﬁgure VLAN at both the server and the
switch side.
In order for the switch ports to support
VLAN, we need to put the server facing switch ports
into trunk mode (which supports VLAN tagged pack-
ets) instead of access mode (which sends and receives
untagged packets). The basic PFC functionality works
with this conﬁguration, but it leads to two problems.
First, the switch trunk mode has an undesirable inter-
action with our OS provisioning service. OS provision-
ing is a fundamental service which needs to run when
the server OS needs to be installed or upgraded, and
when the servers need to be provisioned or repaired.