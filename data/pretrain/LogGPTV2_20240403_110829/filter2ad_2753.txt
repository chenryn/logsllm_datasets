title:Relation extraction for inferring access control rules from natural
language artifacts
author:John Slankas and
Xusheng Xiao and
Laurie A. Williams and
Tao Xie
Relation Extraction for Inferring 
Access Control Rules from
Natural Language Artifacts
John Slankas, Xusheng Xiao, Laurie Williams, and 
Tao Xie
2014 Annual Computer Security Applications Conference
December 8 – 12th, 2014
Motivation
Research      Prior
Solution      Method      Evaluation      Future
Relevant Documentation for Healthcare Systems
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
HIPAA
HITECH ACT
Meaningful Use Stage 1 Criteria
Meaningful Use Stage 2 Criteria
Certified EHR (45 CFR Part 170)
ASTM 
HL7
NIST FIPS  PUB 140-2
HIPAA Omnibus
NIST Testing Guidelines
DEA Electronic Prescriptions for Controlled Substances (EPCS)
Industry Guidelines: CCHIT, EHRA, HL7
State-specific requirements
North Carolina General Statute § 130A-480 – Emergency Departments
Organizational policies and procedures
Project requirements, use cases, design, test scripts, …
Payment Card Industry: Data Security Standard
2
Motivation
Research Goal
Research Questions
Research
Solution      Method      Evaluation      Future
Aid developers who implement access control 
rules by inferring those access control rules from 
natural language artifacts
3
Motivation
Research Goal
Research Questions
Research
Solution      Method      Evaluation      Future
1. What patterns exist among sentences with access 
control rules?
2. How frequently do different forms of ambiguity occur in 
sentences with access control rules?
3. How effectively does our process detect sentences with 
access control rules?
4. How effectively can the subject, action, and resources 
elements of ACRs be extracted?
4
Motivation Research Prior Solution      Method      Evaluation      Future
Prior Work 
1. Controlled natural languages
2. Heuristics and established patterns
3.
Information Extraction
a. Templates
b. Relations
c. Semantic role labelling
5
Research       Solution Method      Evaluation      Future
Motivation
Approach
Access Control Relation Extraction (ACRE)
Representation        Process
Natural language documents contain explicit and implicit 
access control statements:
• A nurse can order a lab procedure for a patient.
• The doctor may add or remove patients from the 
monitoring list.
• Only doctors can write prescriptions.
6
Research      Prior
Solution Method      Evaluation      Future
Motivation
Approach
Access Control Relation Extraction (ACRE)
Representation        Process
7
Research      Prior
Motivation
Representation       Process
Approach
ACRE: Sentence Representation
Solution Method      Evaluation      Future
“The nurse can order a lab procedure for a patient.” 
8
Research      Prior
Representation
Solution Method      Evaluation      Future
Process
Motivation
Approach
ACRE: Policy Representation
(cid:1827)(cid:4666)(cid:1871),(cid:1853),(cid:1870),(cid:1866),(cid:1864),(cid:1855),(cid:1834),(cid:1868)(cid:4667)
(cid:1871) vertices composing the subject
(cid:1853) vertices composing the action
(cid:1870)	 vertices composing the resource
(cid:1866) vertex representing negativity
(cid:1864) vertex representing limitation to a specific role
(cid:1855) vertices providing context to the access control policy
(cid:1834) subgraph required to connect all previous vertices
(cid:1868) set of permission associated with the current policy
	,(cid:1848):(cid:1866)(cid:1873)(cid:1870)(cid:1871)(cid:1857),(cid:1867)(cid:1870)(cid:1856)(cid:1857)(cid:1870),(cid:1864)(cid:1853)(cid:1854)	(cid:1868)(cid:1870)(cid:1867)(cid:1855)(cid:1857)(cid:1856)(cid:1873)(cid:1870)(cid:1857);
(cid:1827)(cid:4666)(cid:1866)(cid:1873)(cid:1870)(cid:1871)(cid:1857), (cid:1867)(cid:1870)(cid:1856)(cid:1857)(cid:1870), (cid:1864)(cid:1853)(cid:1854)	(cid:1868)(cid:1870)(cid:1867)(cid:1855)(cid:1857)(cid:1856)(cid:1873)(cid:1870)(cid:1857),
	,
		(cid:1831):(cid:4666)(cid:1867)(cid:1870)(cid:1856)(cid:1857)(cid:1870),(cid:1866)(cid:1873)(cid:1870)(cid:1871)(cid:1857),(cid:1866)(cid:1871)(cid:1873)(cid:1854)(cid:1862)(cid:4667);(cid:4666)(cid:1867)(cid:1870)(cid:1856)(cid:1857)(cid:1870),(cid:1864)(cid:1853)(cid:1854)	(cid:1868)(cid:1870)(cid:1867)(cid:1855)(cid:1857)(cid:1856)(cid:1873)(cid:1870)(cid:1857),(cid:1856)(cid:1867)(cid:1854)(cid:1862)(cid:4667)	(cid:4667),(cid:1855)(cid:1870)(cid:1857)(cid:1853)(cid:1872)(cid:1857)(cid:4667)
(cid:1827)(cid:4666)(cid:1866)(cid:1873)(cid:1870)(cid:1871)(cid:1857), (cid:1867)(cid:1870)(cid:1856)(cid:1857)(cid:1870), (cid:1868)(cid:1853)(cid:1872)(cid:1861)(cid:1857)(cid:1866)(cid:1872),
	,(cid:4666)(cid:1848):(cid:1866)(cid:1873)(cid:1870)(cid:1871)(cid:1857),(cid:1867)(cid:1870)(cid:1856)(cid:1857)(cid:1870),(cid:1868)(cid:1853)(cid:1872)(cid:1861)(cid:1857)(cid:1866)(cid:1872);
	,
	(cid:1831):(cid:4666)(cid:1867)(cid:1870)(cid:1856)(cid:1857)(cid:1870),(cid:1866)(cid:1873)(cid:1870)(cid:1871)(cid:1857),(cid:1866)(cid:1871)(cid:1873)(cid:1854)(cid:1862)(cid:4667);(cid:4666)(cid:1867)(cid:1870)(cid:1856)(cid:1857)(cid:1870),(cid:1868)(cid:1853)(cid:1872)(cid:1861)(cid:1857)(cid:1866)(cid:1872),(cid:1868)(cid:1870)(cid:1857)(cid:1868)_(cid:1858)(cid:1867)(cid:1870)(cid:4667)	(cid:4667),(cid:1870)(cid:1857)(cid:1853)(cid:1856)(cid:4667)
9
Research       Solution Method      Evaluation      Future
Motivation
Approach
ACRE: Step 1 - Preprocess Text Documents
Representation
Process
Read input from a text file to identify major “types” of lines
Titles
Lists
Sentences / Sentence Fragments
→ listID title line | title line | sentence line | λ
→ normalSentence | listStart (“:” | “-”) listElement
document  → line
line 
sentence 
listElement → listID sentence listElement | λ
→ listParanID | listDotID | number
listID
listParanID → “(” id “)” listParanID | id “)” listParanID | λ
listDotID
id → letter | romanNumeral | number
→ id “.” listDotID | λ
10
Research       Solution Method      Evaluation      Future
Motivation
Approach
ACRE: Step 2 – Produce Dependency Graphs
Representation
Process
• Parse text utilizing Stanford Natural Language Parser
• Apply transformations to minimize graph.
“ A nurse can order a lab procedure for a patient.” 
11
Research       Solution Method      Evaluation      Future
Motivation
Approach
ACRE: Step 2 - Parse Natural Language
Representation
Process
• Parse text utilizing Stanford Natural Language Parser
• Apply transformations to minimize graph.
“ A nurse can order a lab procedure for a patient.” 
12
Research       Solution Method      Evaluation      Future
Motivation
Approach
Representation
ACRE: Step 3 - Classify
Process
Bootstrap
Does the current sentence contain access control elements?
Utilizes a k-NN Classifier as the primary classifier
• Ability to find closely matched sentences
• Performs well on similar document types
If the k-NN Classifier doesn’t find a close match(es), then a 
majority vote is taken in conjunction with naïve Bayes, and 
SVM classifiers
13
Motivation
Approach
ACRE: Step 4 - Extraction
Representation
Research       Solution Method      Evaluation      Future
Process
14
Representation
Process
Research       Solution Method      Evaluation      Future
Motivation
Approach
ACRE: Step 4 – Extraction / Seed Patterns
1) Determine verb frequency
2) Generate “base” wildcard patterns
3) Determine initial subject and resource lists
Iterate
4) From subject and resources, determine graph patterns 
existing between combinations
5) Apply transformations and wildcards to generate new 
patterns
6) Examine document for matching patterns
•
•
Extract access control policies
Extract newly found subject and resources
15
Motivation
Context
Research       Solution      Method
Study Oracle
Procedure
Evaluation      Future
Electronic Health Record (EHR) Domain
Specifically – iTrust
http://agile.csc.ncsu.edu/iTrust
Why Healthcare?
•
•
•
# of open and closed-source systems
Government regulations
Industry Standards
IBM Course Management System
•
• CyberChair
• Collected ACP Documents
16
Motivation
Context
Research       Solution      Method
Study Oracle
Procedure
Evaluation      Future
Domain
Document
iTrust for ACRE
Healthcare
iTrust for Text2Policy
Healthcare
IBM Course Management Education
CyberChair
Collected ACP Documents Multiple
Conf. Mgmt
Number of 
Sentences
1160
471
401
303
142
Number of ACR 
Sentences
550
418
169
139
114
Number of 
ACRs
2274
1070
375
386
258
Fleiss’ 
Kappa
0.58
0.73
0.82
0.71
n/a
17
Motivation
Context      Study Oracle
Procedure
Research       Solution      Method
Evaluation      Future
18
Motivation
Context      Study Oracle
Procedure
Research       Solution      Method
Evaluation      Future
• Evaluate ability to identify access control statements
• What machine learning algorithms perform best?
• What features affect performance?
• Examine identified patterns for commonality
• Examine different seed words and patterns
19
Research       Solution      Method      Evaluation
Motivation
RQ1: What patterns exist among sentences with Access Control Rules?
Future
Top ACR Patterns 
Pattern
(VB root(NN nsubj)(NN dobj))
(VB root(NN nsubjpass))
(VB root(NN nsubj)(NN prep))
(VB root(NN dobj))
(VB root(NN prep_%))
Num. of Occurrences
465 (14.1%)
122 (3.7%)
116 (3.5%)
72 (2.2%)
63 (1.9%)
20
Research       Solution      Method      Evaluation
Motivation
RQ1: What patterns exist among sentences with Access Control Rules?
Future
Text2Policy Pattern – Modal Verb
Text2Policy Pattern – Passive voice 
w/ to Infinitive
Text2Policy Pattern – Access 
Expression
Text2Policy Pattern – Ability 
Expression
Number of sentences with multiple 
types of ACRs
Number of patterns appearing once or 
twice
ACRs with ambiguous subjects (e.g. 
“system”, “user”, etc.)
ACRs with blank subjects
ACRs with pronouns as subjects
ACRs with ambiguous objects (e.g., 
entry, list, name,etc.)
iTrust_acre iTrust_t2p IBM CM CyberChair Collected
93
9
130
21
210
66
46
10
71
39
32
45
383
680
193
557
109
422
7
21
146
173
119
206
28
228
5
14
77
162
139
29
5
45
1
11
105
184
1
187
11
47
18
3
36
97
13
5
11
34
21
Research       Solution      Method      Evaluation
Future
Motivation
RQ2: How frequently do different forms of ambiguity occur in 
sentences with access control rules?
Ambiguity
Pronouns
“System” / “user” 
No explicit subject
Other ambiguous terms
Missing objects
Occurance % in ACR 
Sentences
3.2%
11.0%
17.3%
21.5%
0.2%
Ambiguous terms: “list”, “name”, “record”, “data”, … 
22
Research       Solution      Method      Evaluation
Future
Motivation
RQ3: How effectively does our process detect sentences with 
access control rules
Document
iTrust for Text2Policy
iTrust for ACRE
IBM Course Management
CyberChair
Collected ACP
10-fold validation
Document-fold validation
Precision Recall F1
96% 99% 98%
90% 86% 88%
83% 92% 87%
63% 64% 64%
83% 96% 89%
81% 84% 83%
81% 65% 72%
23
Research       Solution      Method      Evaluation
Future
Motivation
RQ3: How effectively does our process detect sentences with 
access control rules
Classification Performance (F1) by Completion %
24
Research       Solution      Method      Evaluation
Future
Motivation
RQ3: How effectively can the subject, action, and resources 
elements of ACRs be extracted
iTrust for Text2Policy
iTrust for ACRE
IBM Course 
Management
CyberChair
Collected ACP
Precision Recall
75%
60%
62%
80%
75%
81%
75%
68%
30%
18%
F1
77%
67%
70%
43%
29%
25
Research    Prior Solution      Method 
Evaluation
Limitations
Future
Limitations and Threats to Validity
Limitations
• Text-based process
• Conditional access
• Reification
• Resolution
Threats to Validity:
• One individual performed the labeling
(validated labels through random samples and inter-rater agreement)
26
Motivation Research      Prior
Solution      Method      Evaluation      Future
So, What’s Next?
• Data Modeling
• Resolution
• Reification
• Additional documents, systems, and domains
• Use process to inform analysts of ambiguity
27