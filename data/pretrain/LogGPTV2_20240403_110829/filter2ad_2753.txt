# Relation Extraction for Inferring Access Control Rules from Natural Language Artifacts

## Authors
John Slankas, Xusheng Xiao, Laurie A. Williams, and Tao Xie

## Conference
2014 Annual Computer Security Applications Conference  
December 8 – 12, 2014

## Motivation
### Relevant Documentation for Healthcare Systems
- HIPAA
- HITECH ACT
- Meaningful Use Stage 1 Criteria
- Meaningful Use Stage 2 Criteria
- Certified EHR (45 CFR Part 170)
- ASTM
- HL7
- NIST FIPS PUB 140-2
- HIPAA Omnibus
- NIST Testing Guidelines
- DEA Electronic Prescriptions for Controlled Substances (EPCS)
- Industry Guidelines: CCHIT, EHRA, HL7
- State-specific requirements
- North Carolina General Statute § 130A-480 – Emergency Departments
- Organizational policies and procedures
- Project requirements, use cases, design, test scripts, etc.
- Payment Card Industry: Data Security Standard

### Research Goal
To assist developers in implementing access control rules by inferring these rules from natural language artifacts.

### Research Questions
1. What patterns exist among sentences with access control rules?
2. How frequently do different forms of ambiguity occur in sentences with access control rules?
3. How effectively does our process detect sentences with access control rules?
4. How effectively can the subject, action, and resources elements of access control rules (ACRs) be extracted?

## Prior Work
1. **Controlled Natural Languages**
2. **Heuristics and Established Patterns**
3. **Information Extraction**
   - Templates
   - Relations
   - Semantic Role Labeling

## Approach
### Access Control Relation Extraction (ACRE)
#### Representation and Process
Natural language documents contain both explicit and implicit access control statements:
- "A nurse can order a lab procedure for a patient."
- "The doctor may add or remove patients from the monitoring list."
- "Only doctors can write prescriptions."

### Sentence and Policy Representation
- **Sentence Representation Example**: "The nurse can order a lab procedure for a patient."
- **Policy Representation**:
  - \(\mathcal{V}_s\): Vertices composing the subject
  - \(\mathcal{V}_a\): Vertices composing the action
  - \(\mathcal{V}_r\): Vertices composing the resource
  - \(\mathcal{V}_n\): Vertex representing negativity
  - \(\mathcal{V}_l\): Vertex representing limitation to a specific role
  - \(\mathcal{V}_c\): Vertices providing context to the access control policy
  - \(\mathcal{G}\): Subgraph required to connect all previous vertices
  - \(\mathcal{P}\): Set of permissions associated with the current policy

  \[
  \mathcal{G} = (\mathcal{V}_n \rightarrow \mathcal{V}_r \rightarrow \mathcal{V}_s, \mathcal{V}_r \rightarrow \mathcal{V}_a, \mathcal{V}_l \rightarrow \mathcal{V}_a \rightarrow \mathcal{P}, \mathcal{V}_c \rightarrow \mathcal{V}_r \rightarrow \mathcal{V}_a \rightarrow \mathcal{P})
  \]

  \[
  \mathcal{G} = (\mathcal{V}_n \rightarrow \mathcal{V}_r \rightarrow \mathcal{V}_s, \mathcal{V}_r \rightarrow \mathcal{V}_a, \mathcal{V}_l \rightarrow \mathcal{V}_a \rightarrow \mathcal{P}, \mathcal{V}_c \rightarrow \mathcal{V}_r \rightarrow \mathcal{V}_a \rightarrow \mathcal{P})
  \]

  \[
  \mathcal{G} = (\mathcal{V}_n \rightarrow \mathcal{V}_r \rightarrow \mathcal{V}_s, \mathcal{V}_r \rightarrow \mathcal{V}_a, \mathcal{V}_l \rightarrow \mathcal{V}_a \rightarrow \mathcal{P}, \mathcal{V}_c \rightarrow \mathcal{V}_r \rightarrow \mathcal{V}_a \rightarrow \mathcal{P})
  \]

### Steps in ACRE
1. **Preprocess Text Documents**
   - Read input from a text file to identify major types of lines (titles, lists, sentences).
   - Identify and categorize lines into titles, lists, and sentences.

2. **Produce Dependency Graphs**
   - Parse text using the Stanford Natural Language Parser.
   - Apply transformations to minimize the graph.

3. **Classify Sentences**
   - Determine if the current sentence contains access control elements.
   - Utilize a k-NN classifier as the primary classifier.
   - If the k-NN classifier doesn't find a close match, use a majority vote with naïve Bayes and SVM classifiers.

4. **Extraction**
   - Determine verb frequency.
   - Generate base wildcard patterns.
   - Determine initial subject and resource lists.
   - Iterate to determine graph patterns between combinations of subjects and resources.
   - Apply transformations and wildcards to generate new patterns.
   - Examine the document for matching patterns.
   - Extract access control policies and newly found subjects and resources.

## Context and Study Oracle
### Domain and Document Details
- **iTrust for ACRE and Text2Policy**: Healthcare
- **IBM Course Management System**: Education
- **CyberChair**: Conference Management
- **Collected ACP Documents**: Multiple Domains

| Domain | Document | Number of Sentences | Number of ACR Sentences | Number of ACRs | Fleiss' Kappa |
|--------|----------|---------------------|-------------------------|----------------|---------------|
| iTrust for ACRE | Healthcare | 1160 | 550 | 2274 | 0.58 |
| iTrust for Text2Policy | Healthcare | 471 | 418 | 1070 | 0.73 |
| IBM Course Management | Education | 401 | 169 | 375 | 0.82 |
| CyberChair | Conf. Mgmt | 303 | 139 | 386 | 0.71 |
| Collected ACP Documents | Multiple | 142 | 114 | 258 | n/a |

## Evaluation
### Research Questions
#### RQ1: What patterns exist among sentences with Access Control Rules?
- Top ACR Patterns:
  - (VB root(NN nsubj)(NN dobj)): 465 (14.1%)
  - (VB root(NN nsubjpass)): 122 (3.7%)
  - (VB root(NN nsubj)(NN prep)): 116 (3.5%)
  - (VB root(NN dobj)): 72 (2.2%)
  - (VB root(NN prep_%)): 63 (1.9%)

#### RQ2: How frequently do different forms of ambiguity occur in sentences with access control rules?
- Ambiguity Types:
  - Pronouns: 3.2%
  - "System" / "user": 11.0%
  - No explicit subject: 17.3%
  - Other ambiguous terms: 21.5%
  - Missing objects: 0.2%

#### RQ3: How effectively does our process detect sentences with access control rules?
- Document Performance:
  - iTrust for Text2Policy: Precision 96%, Recall 99%, F1 98%
  - iTrust for ACRE: Precision 90%, Recall 86%, F1 88%
  - IBM Course Management: Precision 83%, Recall 92%, F1 87%
  - CyberChair: Precision 63%, Recall 64%, F1 64%
  - Collected ACP: Precision 83%, Recall 96%, F1 89%

- Classification Performance (F1) by Completion %: [Graph/Chart]

#### RQ4: How effectively can the subject, action, and resources elements of ACRs be extracted?
- Extraction Performance:
  - iTrust for Text2Policy: Precision 75%, Recall 81%, F1 77%
  - iTrust for ACRE: Precision 60%, Recall 75%, F1 67%
  - IBM Course Management: Precision 62%, Recall 68%, F1 70%
  - CyberChair: Precision 80%, Recall 30%, F1 43%
  - Collected ACP: Precision 18%, Recall 29%, F1 29%

## Limitations and Threats to Validity
### Limitations
- Text-based process
- Conditional access
- Reification
- Resolution

### Threats to Validity
- One individual performed the labeling (validated labels through random samples and inter-rater agreement).

## Future Work
- Data modeling
- Resolution
- Reification
- Additional documents, systems, and domains
- Use the process to inform analysts of ambiguity