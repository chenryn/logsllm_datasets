the application/domain and the end goal. For example, approx-
imations skipping certain loop iterations [7] and approxima-
tions dropping some synchronizations [9] in the program both
belong to the broad category of selective computation and ei-
ther, both or neither might be an appropriate approximation for
a given application. In Section III we describe approximations
belonging to each of the categories described above that are
speciﬁc to the application we study.
III. VIDEO SUMMARIZATION ALGORITHM
As described in Section I, the system architecture of interest
in this work is a model where a swarm of UAVs is engaged
600
Fig. 3: Flowchart describing the key tasks that comprise the Video
Summarization Algorithm.
in image scanning, analysis and stitching with the end goal of
creating a global panorama of the observed landscape. Towards
this goal, we describe in this section a video (image) stitching
algorithm that we have developed and implemented in our
experimental evaluation platforms. Our application (hence-
forth referred to as the Video Summarization (VS) algorithm)
takes input videos captured by moving cameras and generates
panoramas that provide a global view of the landscape. Since
the input video is a concatenation of images captured by
(various) moving cameras, it can contain various segments
with dissimilar viewing angles and settings. Each of these
segments are summarized by mini-panoramas and are stitched
into a global panorama (simply referred to as panorama) at
a later stage. In this paper, our analysis is restricted to the
generation of a panorama from video captured by a single
camera on-board a single UAV.
A. Functional Overview
While a full detailed description of the algorithm is pro-
vided in [26], for the sake of brevity, we describe the key
capabilities of the algorithm [27] in the following paragraphs.
A representative ﬂow of the algorithm is shown in Figure 3.
One of the fundamental functions performed by the VS
algorithm is the comparison, transformation and stitching of
two images from the input video. The algorithm ﬁrst identiﬁes
key regions of interest (key points) within each image and then
looks for matching key points within the images to identify
potential common areas. It then applies transformations to the
two images so that they are aligned correctly and have the
same scale, lighting, perspective etc., before proceeding to
stitch them together. Figure 4 shows this process using two
sample images. We utilize FAST (Features from Accelerated
Segment Test) detectors [28], [29] and ORB (Oriented FAST
and Rotated BRIEF) descriptors [30] to achieve efﬁcient
and accurate feature point detection and matching. RANSAC
(RANdom SAmple Consensus) [31] is used to compute the
homography transformation between the two images.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:29:17 UTC from IEEE Xplore.  Restrictions apply. 
Key	
  point	
  detec,on	
  and	
  matching	
  between	
  adjacent	
  frames	
  Find	
  Homography	
  parameter	
  using	
  RANSAC	
  Solu,on?	
  Find	
  Aﬃne	
  parameter	
  using	
  RANSAC	
  Calculate	
  size	
  of	
  panorama	
  and	
  global	
  transforma,on	
  for	
  each	
  frame	
  Finish	
  itera,on	
  Update	
  panorama	
  frame	
  by	
  frame	
  Yes	
  No	
  Yes	
  No	
  Determine	
  video	
  segments	
  for	
  crea,ng	
  separated	
  panorama	
  IV. APPROXIMATE VIDEO SUMMARIZATION ALGORITHMS
As described in Section III, the Video Summarization (VS)
application is capable of effectively capturing several hours
of video in single stitched image frames. However, given the
constraints on power efﬁciency (of the on-board device) as
well as real-time requirements of the mission, a complete and
exact implementation of the algorithm may not be possible.
In [4] and [33], the authors examine techniques to mitigate
this limitation by dynamic adjustments of the link bandwidth
and processor voltage/frequency.
In this paper, we consider software approximation, to the
VS algorithm, as a means to realize performance and energy
targets. Since computations involving images can be inherently
tolerant to inaccuracies in data and/or compute, approxima-
tions to the the VS workﬂow have the potential
to yield
signiﬁcant beneﬁts without unduly compromising the quality
of the ﬁnal panorama image output. We study three different
approximations (belonging to the three broad classes described
in Section II-B and presented in the same order). The details
of the approximate algorithms are described below:
(1) Random Frame Dropping (VS RFD): In this algo-
rithm, we randomly drop frames from the input stream. Apart
from improving the effective frame rate, this input approxi-
mation aims to leverage redundancies in consecutive images
captured by a moving camera without substantial degradation
in output image quality. In this paper, we demonstrate results
with up to 10% of the input frames being dropped.
(2) Key Point Down Sampling (VS KDS): The VS al-
gorithm described in Section III involves the computation of
feature (key) points and attempts to match them across frames
in order to be able to stitch the frames together. We propose
an approximation in which we only perform matching on
a fraction (one-third) of the key points as compared to the
precise algorithm. This signiﬁcantly reduces the computation
time, which varies as O(n2) with the number of key points. In
this algorithm, the source of error could be due to some frames
being dropped on account of having insufﬁcient matching key
points. In such cases, it is hoped that the redundancy of the
image will still enable us to obtain complete coverage of the
input video in our summarized output.
(3) Simple Matching (VS SM): In the default algorithm,
each key point in the current frame is compared with all key
points in the incoming frame and the two nearest neighbors
are determined for each key point. The key point is included
in the list of good matches only if the ratio of the distance
between the nearest and 2nd nearest neighbor is above a certain
threshold; i.e., the nearest match is sufﬁciently closer than the
2nd nearest. This reduces the probability of a false positive,
i.e., the probability that the key point in a frame incorrectly
maps to a point in the subsequent frame, even if, in reality,
there is no matching object. In case of VS SM, we alter the
algorithm to determine only the single nearest neighbor for
each key point. In addition, we place an upper bound on the
actual distance value and consider only those matches whose
nearest neighbor is within a ﬁxed distance of the key point.
Fig. 4: Simple example of stitching two images.
Using the technique described above, successive frames
of the input are pair-wise compared in the initial pass of
the algorithm. However, not every pair of adjacent frames
has enough matching key points to compute the homography
transformation. In this case, we estimate a simpler afﬁne trans-
formation which requires fewer matching points. If sufﬁcient
number of matching points cannot be found even for the
afﬁne transformation, the corresponding frame is discarded.
To generate the overall output panorama we align every frame
to the ﬁrst by transforming all the frames to have the same
coordinate system as the ﬁrst frame by using the homography
transformations described above.
There are various other sophisticated elements of the stitch-
ing algorithm that are used to improve the rendered quality of
the output panorama. The mathematical details of the trans-
formations and corrective actions (e.g., to avoid blurs and
distortions) are omitted here for brevity. Depending on the
quality and number of the input video clips (collected by the
moving cameras), the amount of computation performed by
the video summarization procedure can vary.
B. Inputs to the Video Summarization Algorithm
We evaluate the VS application using two aerial videos from
the VIRAT (Video and Image Retrieval and Analysis Tool)
dataset [32] – 09152008ﬂight2tape1 2 (hereby referred to as
Input 1) and 09152008ﬂight2tape2 4 (hereby referred to as
Input 2). We use an input size of 1000 frames for both inputs.
The VIRAT dataset was chosen for our evaluations to
represent realistic scenarios of videos captured during aerial
surveillance with variations in resolution, diversity in scenes,
changes in scale, focus and camera angles. The two inputs
that we proﬁled vary signiﬁcantly in these aspects as well as
in the nature with which these parameters vary in the video
stream. For instance, the number of changes that occur in
Input 1 are much higher than Input 2, leading to a much
larger number of mini-panoramas generated in the ﬁrst input
set. These videos were sampled at periodic intervals to yield
around 3000 frames across the duration of the entire video. In
addition, we further downsampled the video by a factor of 3
to enable a statistically signiﬁcant number of error injection
experiments to run within a reasonable amount of time without
perceivable loss in information or image quality.
601
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:29:17 UTC from IEEE Xplore.  Restrictions apply. 
Hence, only those key points in the incoming frame which
match almost perfectly with those in the original frame would
be considered. Note that this technique still leaves room for
some errors, for example, when there are two identical objects
in the image. In such cases, both nearest neighbor distances
could fall within the threshold and the mapping could happen
to the incorrect object.
A. Effectiveness of the Approximate Implementations
An approximate algorithm has to produce acceptable quality
outputs while enabling some system beneﬁt (e.g. improved
performance or energy efﬁciency). Hence, we examine the
three approximate algorithms from the point of view of both
system beneﬁts and output quality to determine if they are
good candidates for further study.
System beneﬁts of approximation: We carried out an ex-
perimental evaluation of these algorithms on an IBM POWER-
based server class machine. Figure 5 shows the Instructions
Per Cycle (IPC), execution time and energy, normalized to
the baseline VS algorithm. We observe that VS RFD provides
the maximum reduction in execution time (68%) for Input
1 by just dropping 10% of the total frames. On the other
hand, VS KDS yields the highest performance improvement
of 18% in case of Input 2. Since the IPC (and hence, the
power) remains relatively constant across the default and
approximate implementations, the energy proﬁle across the
exact and approximate implementations varies similarly to that
of the execution time.
Tradeoffs between performance and output quality:
The difference between the two inputs is evident from the
tradeoffs between performance and output image quality for
each approximation. For instance, a visual inspection of the
output panoramas generated by the baseline VS algorithm and
the three approximations (Figure 6) show that Input 2 is more
robust to the proposed approximation techniques as compared
to Input 1. On the other hand, the performance beneﬁts due
to approximation are clearly greater in case of Input 1.
This difference in the impact of approximation on the two
inputs can be attributed to the fact that the variation between
consecutive frames is much more pronounced in Input 1 than
in Input 2. The execution time improvement is primarily due
to the polynomial complexity of the algorithm in terms of
number of frames that are processed. In addition to the frames
dropped by the approximate implementation, the algorithm
also discards additional frames without stitching them to the
overall panorama, when sufﬁcient matching points are not
found. Consequently, the proposed approximation techniques
result in several frames of Input 1 being discarded. While
this reduces the number of computations, resulting in greater
performance and energy beneﬁts as compared to Input 2, it
also adversely affects the output quality to a greater extent. The