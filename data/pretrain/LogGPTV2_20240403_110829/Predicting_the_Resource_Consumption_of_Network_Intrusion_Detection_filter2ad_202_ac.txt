sampling factor 17
sampling factor 31
0.5
1.0
1.5
2.0
2.5
Quantile values w/o connection sampling
Fig. 4. QQ plot of analyzer workload without sampling vs. with different sampling factors
Naturally, the measured CPU times are very small if only a few connections are
analyzed. For example, in the unsampled trace Bro consumes on average 370 msec
for one second of network trafﬁc when analyzing all the connections. With a sampling
factor of 31, we would expect consumption to drop to 370/31 = 12 msec, at which
point we are at the edge of the OS’s accounting precision. In fact, however, we ﬁnd that
extrapolation still works fairly well: for sample factor 31, the median of the extrapolated
measurements is only 28 msec lower than the median of the measurements for the full
trace. We have veriﬁed that similar observations hold for other segments of MWN-full,
as well as for other traces.
Next we check if this ﬁnding still holds for more complex conﬁgurations. To this
end, the QQ-plot in Figure 4 compares the distribution of CPU times for BROALL
(i.e., 13 additional analyzers) on the full Trace-20m (X-axis) vs. sub-sampled traces
Table 1. Memory scaling factors: 10 BROBASE runs (left) / 10 BROALL runs (right)
Sampling factor 1
Memory ratio
31
1 7.0 11.0 16.6 30.7
11
17
7
Sampling factor 1
Memory ratio
31
1 3.64 4.87 6.30 9.34
11
17
7
Predicting the Resource Consumption of NIDSs
145
(Y-axis, with sample factors of 7, 11, 17, and 31). Overall, the majority of the samples
match fairly well, though with a bias for smaller values towards underestimation (left
of the 80th percentile line), and with unstable upper quantiles (usually overestimates).
Memory Usage. Turning to memory consumption, for each sampling factor we con-
ducted 10 runs with BROBASE on Trace-20m, measuring the maximum consump-
tion ﬁgures on the sampled traces as reported by the OS.3 Table 1 (left) shows the ratio
between the memory consumption on the entire trace versus that for the sampled traces.
Ideally, this ﬁgure would match the sampling factor, since then we would extrapolate
perfectly from the sample. We see that in general the ratio is close, with a bias towards
being a bit low. From this we conclude that for BROBASE, predicting memory use from
a sampled trace will result in fairly accurate, though sometimes slightly high, estimates.
As discussed in Section 3, we would not expect memory usage of application-layer
analyzers to always scale linearly with the number of connections, since some analyzers
accumulate state not on a per-connection basis but rather according to some grouping
of the connections (e.g., Bro’s HTTP analyzer groups connections into “sessions”). In
such cases the memory estimate we get by scaling with the connection sample factor
can be a (potentially signiﬁcant) overestimation. This effect is visible in Table 1 (right),
which shows the same sort of analysis as above but now for BROALL. We see that the
extrapolation factors can be off by more than a factor of three. By running each ana-
lyzer separately, we identiﬁed the culprits: both the HTTP and SSL analyzers associate
their state per session, rather than per connection. However, we note that at least the
required memory never exceeds the prediction, and thus we can use the prediction as a
conservative upper bound.
In summary, we ﬁnd that both CPU and memory usage can generally be predicted
well with a model linear in the number of connections. We need to keep in mind how-
ever that it can overestimate the memory demand for some analyzers.
5 Resource Prediction
After conﬁrming that we can often factor NIDS resource usage components with per-
analyzer and per-connection scaling, we now employ these observations to derive sug-
gestions of reasonable conﬁgurations for operating in a speciﬁc network environment.
We start by devising a methodology for ﬁnding a suitable conﬁguration based on
a snapshot of an environment’s network trafﬁc. Then we turn to estimating the long-
term performance of such a conﬁguration given a coarser-grained summary of the net-
work trafﬁc that contains the time-of-day and day-of-week effects. The latter is crucial,
as trafﬁc characteristics, and therefore resource consumption, can change signiﬁcantly
over time.
5.1 From Trafﬁc Snapshots to Conﬁgurations
In this section we consider the degree to which we can analyze a short sample trace
from a given environment in order to identify suitable NIDS conﬁgurations, in terms of
3 As in the case of CPU usage, we ﬁnd inherent ﬂuctuation in memory usage as well: running
instances under identical conditions exhibits some noticeable, though not huge, variation.
146
H. Dreger et al.
maximizing the NIDS’s analysis while leaving enough “head room” to avoid exhausting
its resources. More generally, we wish to enable the network operator to make informed
decisions about the prioritization of different types of analysis. Alternatively, we can
help the operator decide whether to upgrade the machine if the available resources do
not allow the NIDS to perform the desired analysis.
We stress that due to the variability inherent in network trafﬁc, as well as the mea-
surement limitations discussed in §4, no methodology can aim to suggest an optimal
conﬁguration. However, automating the process of exploring the myriad conﬁguration
options of a NIDS provides a signiﬁcant step forward compared to having to assess
different conﬁgurations in a time-consuming, trial-and-error fashion.
Capturing an Appropriate Trace. Our approach assumes access to a packet trace
from the relevant network with a duration of some tens of minutes. We refer to this as
the main analysis trace. At this stage, we assume the trace is “representative” of the
busiest period for the environment under investigation. Later in this section we explore
this issue more broadly to generalize our results.
Ideally, one uses a full packet trace with all packets that crossed the link during the
sample interval. However, even for medium-sized networks this often will not be feasi-
ble due to disk capacity and time constraints: a 20-minute recording of a link transfer-
ring 400 Mbit/s results in a trace of roughly 60 GB; running a systematic analysis on
the resulting trace as described below would be extremely time consuming. In addition,
full packet capture at these sorts of rates can turn out to be a major challenge on typical
commodity hardware [9].
We therefore leverage our ﬁnding that in general we can decompose resource usage
on a per-connection basis and take advantage of the connection sampling methodology
discussed in Section 4. Given a disk space budget as input, we ﬁrst estimate the link’s
usage via a simple libpcap application to determine a suitable sampling factor, which
we then use to capture an accordingly sampled trace. We can perform the sampling
itself using an appropriate kernel packet ﬁlter [2], so it executes quite efﬁciently and
imposes minimal performance stress on the monitoring system.
Using this trace as input, we then can scale our results according to the sample factor,
as discussed in §4, while keeping in mind the most signiﬁcant source of error in this
process, which is a tendency to overestimate memory consumption when considering a
wide range of application analyzers.
Finding Appropriate Conﬁgurations. We now leverage our observation that we can
decompose resource usage per analyzer to determine analysis combinations that do not
overload the system when analyzing a trafﬁc mix and volume similar to that extrapo-
lated from the captured analysis trace. Based on our analysis of the NIDS resource us-
age contributors (§3.2) and its veriﬁcation (§4), our approach is straight-forward. First
we derive a baseline of CPU and memory usage by running the NIDS on the sampled
trace using a minimal conﬁguration. Then, for each potentially interesting analyzer,
we measure its additional resource consumption by individually adding it to the mini-
mal conﬁguration. We then calculate which combinations of analyzers result in feasible
CPU and memory loads.
The main challenge for determining a suitable level of CPU usage is to ﬁnd the right
trade-off between a good detection rate (requiring a high average CPU load) and leaving
Predicting the Resource Consumption of NIDSs
147
sufﬁcient head-room for short-term processing spikes. The higher the load budget, the
more detailed the analysis; however, if we leave only minimal head-room then the sys-
tem will likely incur packet drops when network trafﬁc deviates from the typical load,
which, due to the long-range dependent nature of the trafﬁc [12] will doubtlessly hap-
pen. Which trade-off to use is a policy decision made by the operator of the NIDS, and
depends on both the network environment and the site’s monitoring priorities. Accord-
ingly, we assume the operator speciﬁes a target CPU load c together with a quantile q
specifying the percentage of time the load should remain below c. With, for example,
c = 90% and q = 95%, the operator asks our tool to ﬁnd a conﬁguration that keeps the
CPU load below 90% for 95% of all CPU samples taken when analyzing the trace.
Two issues complicate the determination of a suitable level of memory usage. First,
some analyzers that we cannot (reasonably) disable may consume signiﬁcant amounts
of memory, such as TCP connection management as a precursor to application-level
analysis for TCP-based services. Thus, the option is not whether to enable these ana-
lyzers, but rather how to parameterize them (e.g., in terms of setting timeouts). Sec-
ond, as pointed out in §4, the memory requirements of some analyzers do not scale di-
rectly with the number of connections, rendering their memory consumption harder to
predict.
Regarding the former, parameterization of analyzers, previous work has found that
connection-level timeouts are a primary contributor to a NIDS’s memory consump-
tion [3]. Therefore, our ﬁrst goal is to derive suitable timeout values given the connec-
tion arrival rate in the trace. The main insight is that the NIDS needs to store different
amounts of state for different connection types. We can group TCP connections into
three classes: (i) failed connection attempts; (ii) fully established and then terminated
connections; and (iii) established but not yet terminated connections. For example, the
Bro NIDS (and likely other NIDSs as well) uses different timeouts and data structures
for the different classes [3], and accordingly we can examine each class separately
to determine the corresponding memory usage. To predict the effect of the individual
timeouts, we assume a constant arrival rate for new connections of each class, which is
reasonable given the short duration of the trace. In addition, we assume that the mem-
ory required for connections within a class is roughly the same. (We have veriﬁed this
for Bro.) This then enables us to estimate appropriate timeouts for a given memory
budget.
To address the second problem, analyzer memory usage which does not scale linearly
with the sampling factor, we can identify these cases by “subsampling” the main trace
further, using for example an additional sampling factor of 3. Then, for each analyzer,
we determine the total memory consumption of the NIDS running on the subsampled
trace and multiply this by the subsampling factor. If doing so yields approximately the
memory consumption of the NIDS running the same conﬁguration on the main trace,
then the analyzer’s memory consumption does indeed scale linearly with the sampling
factor. If not, then we are able to ﬂag that analysis as difﬁcult to extrapolate.
5.2 A Tool for Deriving NIDS Conﬁgurations
We implemented an automatic conﬁguration tool, nidsconf, for the Bro NIDS based
on the approach discussed above. Using a sampled trace ﬁle, it determines a set of Bro
148
H. Dreger et al.
conﬁgurations, including sets of feasible analyzers and suitable connection timeouts.
These conﬁgurations enable Bro to process the network’s trafﬁc within user-deﬁned
limits for CPU and memory.
We assessed nidsconf in the MWN environment on a workday afternoon with a
disk space budget for the sampled trace of 5 GB; a CPU limit of c = 80% for q =
90% of all samples; a memory budget of 500 MB for connection state; and a list of
13 different analyzers to potentially activate (mostly the same as listed previously, but
also including http-reply which examines server-side HTTP trafﬁc).
Computed over a 10-second window, the peak bandwidth observed on the link was
695 Mbps. A 20-minute full-packet trace would therefore have required approximately
100 GB of data. Consequently, nidsconf inferred a connection sampling factor of 23
as necessary to stay within the disk budget (the next larger prime above the desired sam-
pling factor of 21). The connection-sampled trace that the tool subsequently captured
consumed almost exactly 5 GB of disk space. nidsconf then concluded that even
by itself, full HTTP request/reply analysis would exceed the given c and q constraints.
Therefore it decided to disable server-side HTTP analysis. Even without this, the com-
bination of all other analyzers still exceeded the constraints. Therefore, the user was
asked to chose one to disable, for which we selected http-request. Doing so turned out
to sufﬁce. In terms of memory consumption, nidsconf determined that the amount of
state stored by three analyzers (HTTP, SSL, and the scan detector) did not scale linearly
with the number of connections, and therefore could not be predicted correctly. Still,
the tool determined suitable timeouts for connection state (873 secs for unanswered
connection attempts, and 1653 secs for inactive connections).
Due to the complexity of the Bro system, there are quite a few subtleties involved in
the process of automatically generating a conﬁguration. Due to limited space, here we
only outline some of them, and refer to [2] for details. One technical complication is that
not all parts of Bro are sufﬁciently instrumented to report their resource usage. Bro’s
scripting language poses a more fundamental problem: a user is free to write script-
level analyzers that consume CPU or memory in unpredictable ways (e.g., not tied
to connections). Another challenge arises due to individual connections that require
speciﬁc, resource-intensive analysis. As these are non-representative connections any
sampling-based scheme must either identify such outliers, or possibly suggest overly
conservative conﬁgurations. Despite these challenges, however, nidsconf provides a
depth of insight into conﬁguration trade-offs well beyond what an operator otherwise
can draw upon.
5.3 From Flow Logs to Long-Term Prediction
Now that we can identify conﬁgurations appropriate for a short, detailed packet-level
trace, we turn to estimating the long-term performance of such a conﬁguration. Such
extrapolation is crucial before running a NIDS operationally, as network trafﬁc tends to
exhibit strong time-of-day and day-of-week effects. Thus, a conﬁguration suitable for a
short snapshot may still overload the system at another time, or unnecessarily forsake
some types of analysis during less busy times.
For this purpose we require long-term, coarser-grained logs of connection informa-
tion as an abstraction of the network’s trafﬁc. Such logs can, for example, come from
Predicting the Resource Consumption of NIDSs
149
NetFlow data, or from trafﬁc traces with tools such as tcpreduce [10], or perhaps the
NIDS itself (Bro generates such summaries as part of its generic connection analysis).
Such connection-level logs are much smaller than full packet traces (e.g., (cid:3) 1% of the
volume), and thus easier to collect and handle. Indeed, some sites already gather them
on a routine basis to facilitate trafﬁc engineering or forensic analysis.
Methodology. Our methodology draws upon both the long-term connection log and
the systematic measurements on a short-term, (sampled) full-packet trace as described
above. We proceed in three steps: First, we group all connections (in both the log and
the packet trace) into classes, such that the NIDS resource usage scales linearly with the
class size. Second, for different conﬁgurations, we measure the resources used by each
class based on the packet trace. In the last step, we project the resource usage over the
duration of the connection log by scaling each class according to the number of such
connections present in the connection log.
In the simplest case, the overall resource usage scales linearly with the total number
of connections processed (for example, this holds for TCP-level connection processing
without any additional analyzers). Then we have only one class of connections and can
project the CPU time for any speciﬁc time during the connection log proportionally: if
in the packet trace the analysis of N connections takes P seconds of CPU time, we esti-
mate that the NIDS performing the same analysis for M connections uses P
N M seconds
of CPU time. Similarly, if we know the memory required for I concurrent connections
at some time T1 for the packet trace, we can predict the memory consumption at time
T2 by determining the number of active connections at T2.
More complex conﬁgurations require more than one connection class. Therefore we
next identify how to group connections depending on the workload they generate. Based
on our observation that we can decompose a NIDS’s resource requirements into that
of its analyzers (§3), along with our experience validating our approach for Bro (§4),
we identiﬁed three dimensions for deﬁning connection classes: duration, application-
layer service, and ﬁnal TCP state of the connection (e.g., whether it was correctly es-
tablished). Duration is important for determining the number of active connections in
memory at each point in time; service determines the analyzers in use; and the TCP
state indicates whether application-layer analysis is performed.
As we will show, this choice of dimensions produces resource-consumption predic-
tions with reasonable precision for Bro. We note, however, that for other NIDSs one
might examine a different decomposition (e.g., data volume per connection may have a
strong impact too). Even if so, we anticipate that a small number of connection classes
will sufﬁce to capture the principle components of a NIDS’s resource usage.
Predicting Long-Term Resource Use. We now show how to apply our methodology
to predict the long-term resource usage of a NIDS, again using Bro as an example. We
ﬁrst aggregate the connection-level data into time-bins of length T , assigning attributes
reﬂecting each of the dimensions: TCP state, service, and duration. We distinguish be-
tween ﬁve TCP states (attempted, established, closed, half-closed, reset), and consider
40 services (one for each Bro application-layer analyzer, plus a few additional well-
known service ports, plus the service “other”). We discretize a connection’s duration D
by assigning it to a duration category C ← (cid:5)log10D(cid:6). Finally, for each time-bin we
count the number of connections with the same attributes.
150
H. Dreger et al.
measured CPU time
predicted CPU time
d
n
o
c
e
s
r
e
p
e
m
i
t
U
P
C
5
.
2