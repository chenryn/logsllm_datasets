=== append与同环比
我们已经讲过业务横向对比、系统关联对比两种场景，叠加绘图还有一种常见形式：把同一个指标不同时段的走势叠加在一起查看，即同环比趋势图。
采用上一小节的SPL写法，我们也可以在多Y轴图形中展现同环比的曲线，不过由于自动计算得到的右侧Y轴的坐标系和左侧Y轴通常不一致，两条曲线可能反而看起来差异较大。在同环比场景中，使用单一Y轴的效果更好。所以您需要换另一种SPL写法，达到更好的展示效果。依然以之前用过的平均响应时间为例，原始请求是：
[source,bash]
logtype:apache | bucket timestamp span=5m as ts | stats avg(apache.request_time) by ts
现在我们要叠加昨天的情况，新语句写作：
[source,bash]
starttime="now/d" endtime="now" logtype:apache | bucket timestamp span=15m as ts | stats avg(apache.request_time) by ts | eval hour = formatdate(ts, "HH:mm") | eval date="今天" | append [[starttime="-1d/d" endtime="now/d" logtype:apache | bucket timestamp span=15m as ts | stats avg(apache.request_time) by ts | eval hour = formatdate(ts, "HH:mm") | eval date="昨天"]]
然后，我们选用普通的折线图方式，但是用hour作为X轴字段，date作为分组字段，就可以得到想要的效果了：
image::images/search-example-append-line.png[]
注：全天的数据点较多，所以将原语句中的5m改成15m以保持图形美观。此外，由于今天还没结束，所以可以看到蓝色折线缺少后半段的‘未来’部分。
=== sparkline与表格
之前章节讲过的所有趋势图案例，大家都只关心了时间维度的趋势情况。有些复杂环境下，我们会关心多种维度的统计结果表格，时间趋势只是其中一列。对此，日志易提供了sparkline指令和专属的表格迷你图。
比如，我们像看到各业务线的平均响应时间、访问量和UV数的统计结果，SPL语句如下：
[source,bash]
logtype:apache | stats sparkline(avg(apache.request_time), 5m), sparkline(count(), 5m), dc(apache.clientip) by apache.domain
得到的表格如下图所示：
image::images/search-example-sparkline.png[]
== 估值临时字段
本章将讨论如何使用SPL指令，在已有的字段信息之外，再估值新的临时字段，并使用这些临时字段，实现复杂的统计和展示需求。
=== eval指令
eval指令可以使用函数及表达式，在已有字段的基础上，创建新的字段，并将函数和表达式的结果复制给该新字段。eval所支持的函数和表达式通常都比较简单，但是可以通过嵌套组合的方式，实现复杂需求。
完整的函数和表达式说明，请参阅《日志易搜索参考》。
==== 数值运算与文本处理
一门语言最基础的功能就是数值运算，SPL提供了最简单的加减乘除和更复杂的对数、取整等操作。比如，我们要对响应时间做直方图分布统计时，由于日志中的响应时间记录的是秒，大多数值都在1以下，需要转换成毫秒，SPL语句如下：
[source,bash]
logtype:apache | eval req_ms = tolong(apache.request_time * 1000) | bucket req_ms span=50 as rs | stats count() by rs
数据分布直方图效果如下：
image::images/search-example-bins.png[]
此外，日志易也支持常见的文本处理功能。包括文本拼接、截取、匹配等。日志易复用了加号+作为文本拼接的连接操作符：
[source,bash]
logtype:apache | stats count() | eval ret = "事件总数: " + 'count()'
为了书写方便，连接操作符会自动调整前后的数据类型，统一按照文本上下文处理。
[NOTE]
====
在SPL语法中，双引号表示文本类型数据，单引号表示字段名称。当字段名称中含有中文或特殊符号时，单引号不能忽略。
====
==== 条件判断
日志易提供了多个条件判断的函数，包括有：
* if：根据布尔判断表达式的真假来估值。当您准备用来赋值的可选项只有两个时，采用if最合适。比如将网站访问请求划分为正常和不正常两类：
[source,bash]
logtype:apache | eval ret = if(apache.status=500, "SRVERR")
* coalesce：在跨模块分析的时候，相同的数据可能在不同模块记录中标记为不同的字段名称，为了统一过滤和统计方便，我们可以采用coalesce函数来汇聚，日志易系统会按照书写次序，依次查找到最先存在的字段并估值。比如，在反向代理网络架构中，真实客户的IP地址可能有多种记录，需要如下处理：
[source,bash]
tag:cdn | eval ret = coalesce(squid.x_forwarderer_for, haproxy.x_real_ip, nginx.clientip)
==== 类型转换
日志易在统计判断处理的时候，是对数据类型敏感的。因此，在使用stats指令进行数值统计，以及书写where过滤从句的时候，可能需要提前采用类型转换函数进行调整。
比如，在反向代理网络架构中，大多数日志都会存在后端响应时间，但是有些探测请求、非法请求，代理服务器直接处理完毕，后端响应时间字段就会记录成一个横杠号-。那么在统计后端响应时间时，就需要做一次类型转换：
[source,bash]
tag:cdn | eval backend_time = todouble(nginx.upstream_response_time) | bucket timestamp span=5m as ts | stats avg(backend_time) by ts
[NOTE]
====
当预备参与统计的字段值可能为NULL时，转换会报错。您需要采用全文检索语法，预先过滤掉这部分不存在字段值的数据，然后再进行估值和统计分析。
====
==== 时间处理
时间戳是日志分析领域的第一公民，SPL中当然也提供了时间处理函数。本手册之前章节中，已经使用过formatdate函数来辅助完成同环比统计分析功能。除此以外，还有相对应的parsedate函数，用来把文本字符串内容，转换为时间戳内容，可以参与后续的时序统计。
在一些分布式跟踪日志中，通常会记录多个不同的时间字段。不同需求下，我们会需要分析不同阶段的时间趋势。
以(正好是分布式系统的)日志易为例，系统全流程中，先后存在有日志生成时间、客户端发送时间、接收端接收时间。如果您事先没有通过字段提取功能将日志中的实际生成时间设置为默认时间戳，日志易将设置发送时间或接收时间为默认值。这时候，您就需要通过额外的parsedate函数来进行更准确的时间趋势分析了：
[source,bash]
* | eval datetime = parsedate(nginx.datetime, "dd/MMM/yyyy:HH:mm:ss Z") | bucket datetime span=5m as ts | stats count() by ts
=== parse正则提取
在依托日志易系统构建ITOA平台的时候，通常首先能做到的是尽可能把不同系统的日志集中采集，而不一定能立刻知道所有日志的结构与明确字段含义。这意味着大多数数据是未解析状态，仅保存了原文。
当需求来临，相应的业务开发人员找到了相关日志，开始分析时，就必须即时将原文解析出必要的信息，然后以这些信息为基础，进行后续统计分析操作。
parse指令，提供了这种支持，让使用者可以将自己的领域知识，即时的运用到原始日志上。
比如，假如访问日志没有做过预解析，当我们需要统计访问IP地址的排行时，可以写作：
[source,bash]
logtype:other |parse "(?\d+\.\d+\.\d+\.\d+) " | top 10 clientip
可以看到，我们并没有使用完整的解析访问日志的正则表达式，只获取必需的字段信息，正是我们一直在强调的检索性能优化原则。
[TIP]
====
日志易默认对没有做过预解析的日志统一标记为logtype:other。
====
即使是已经做过字段提取的日志，依然可以使用parse指令做进一步的细化解析和调整。比如，在RESTful API领域的实际实践中，通常会在URL里标记使用的是哪个版本的API(虽然这样破坏了HATEOAS原则)。标准的访问日志提取规则，并不会单独解析这一部分，如果我们要统计不同版本API的访问比例，可以在URL字段中，临时解析出一个版本字段，SPL语句写作：
[source,bash]
logtype:apache apache.domain:api.rizhiyi.com | parse field=apache.request_path "^(?/[^/]*)" | stats count() by api_ver
=== jpath结构化提取
针对上节提出的难题，目前业界还有另一类解决手段：将所有信息都输出成JSON格式日志。但是JSON化会带来字节数上的膨胀，耗费更多的存储和计算资源。
一般情况下，我们其实只会使用到其中很少数的字段内容作统计。
日志易提供jpath指令，用于从JSON格式日志中，定向抽取指定路径内容，生成一个临时的字段或多值字段。
[source,javascript]
{"a":[["x1","r1",31],["x2","r2",32],["x3","r3",33]]}
如上所示的JSON日志，我们要获取的是嵌套两层的数组中，由所有第二层数组的第二个元素构成的一个多值字段。那么我们可以使用如下语句得到临时的多值字段tmpmv：
[source,bash]
* | jpath path="a[*][1]" output=tmpmv
[NOTE]
====
path参数中使用的是JSON内部的路径，不用像使用索引字段时那样添加logtype前缀。
====
对于获取的多值字段，后续可以继续使用多值字段处理函数进行操作：
image::images/search-example-jpath.png[]
=== lookup引入外部信息
开发人员在记录日志信息的时候，通常会采用数值ID来代表一些枚举类型的数据。但是在查看日志信息的时候，一般更希望看到的，是文本描述的具体内容。SPL提供lookup指令，通过读取外部CSV文件辅助完成这个信息转换的过程。
最常见的场景，就是IP到主机名的转换、错误码到描述文字的转换两种。以思科ASA日志中的severity为例，我们可以获取到其signature_id到severity的对应关系，(局部)如下：
[source]
signature_id,vendor_severity,severity
1000,Informational,informational
1001,Informational,informational
1002,Informational,informational
1003,Informational,informational
1004,Informational,informational
1005,Informational,informational
1006,Informational,informational
1100,Attack,high
1102,Attack,high
1103,Attack,high
2000,Informational,informational
2001,Informational,informational
…
我们可以通过如下SPL语句，将日志中的ID转换成文本信息：
[source,bash]
logtype:cisco_asa | parse "%ASA-\d-400\d+\sIPS:(?\d{4})\s.*\sfrom" | lookup vendor_severity,severity cisco_asa_severity_lookup.csv on sid=signature_id
lookup指令支持从系统内字典表和HTTP两种方式获取CSV信息。如果采用字典表方式，请通过日志易用户界面上传字典表。
=== autoregress实现跨行计算
截止到目前，我们看到的所有统计指令，都基于同一列的数据，所有的估值指令，都基于同一行的数据。日志易特意提供一个autoregress指令，实现了跨行的字段计算。
比如，我们希望找到间隔三个月无访问的僵尸用户，又突然开始访问的异常情况。那么，在列出用户IP和访问时间timestamp的列表以后，就需要将每行数据和上一行的时间对比，找到差值在3个月以上的数据。SPL写作：
[source,bash]
logtype:apache
  | table apache.clientip, timestamp
  | sort by +timestamp, apache.clientip
  | autoregress apache.clientip p=1
  | autoregress timestamp p=1
  | where apache.clientip == apache.clientip_p1 && timestamp - timestamp_p1 > 3*30*24*3600*1000
  | fields apache.clientip, timestamp
您可以看到，上一行的数据，会自动重命名成xxx_p1，带入下一行中。跨行计算问题，就这样巧妙的转换成了行内计算。