# 深度神经网络障眼法（一）
##### 译文声明
本文是翻译文章
译文仅供参考，具体内容表达以及含义原文为准。
在本系列文章中，我们会跟大家一起了解针对深度神经网络的障眼法，简单来说，就是对输入的数据（比如图片）稍作修改——这些修改可以非常细微，以至于修改后人眼都无法察觉，就能让深度神经网络产生错误的判断。如果正式一点的话，也可以叫做对抗性攻击。
在具体介绍这种障眼法之前，我们还有一些准备工作要做。因此，在本文中，我们首先将为读者介绍一些基本的概念，如什么是机器学习，什么是深度神经网络等，然后，我们将给出一个简单的深度学习模型，以便于我们练手。
## 机器学习
实际上，深度神经网络只是机器学习领域中目前非常流行的一个分支，现在已广泛用于图像处理、音频处理、视频处理等。那么，什么是机器学习呢？所谓机器学习，简单来说，就是在无需对数据模式背后的规则进行显式编码的情况下，让机器从数据中提取模式的各种技术。
为此，首先需要选择某种数学算法，然后，通过一组数据来优化该算法的行为，换句话说，就是通过每条数据（即样本）来调整该算法的参数，当这些参数调整到一定程度的时候，也就是其行为能够令我们满意的时候，算法就训练好了——经过充分训练的算法，就是我们“学到”的模型。所谓的训练过程，或者说是学习过程，简单来说就是用样本调整参数的过程。
正常情况下，模型训练好之后，还需要用另一组数据，也就是测试数据进行验证，看看模型能不能很好的处理没有见过的数据。当然，我们希望训练好的模型对于没见过的数据也具有良好的表现，也就是说，要具备良好的泛化能力。
为了便于理解，我们可以用一个例子做类比。假设我们有一车西瓜，就可以通过观察它们的外观特征（即训练数据，如颜色，形状，敲击声等），并切开各个西瓜，看看哪些是好瓜/坏瓜，然后总结经验（学习并生成模型）；当下次看到一个没切开的西瓜时，就可以通过经验来判断是不是好瓜了（运用模型去判断）。
另外，如果训练数据中，同时给出了特征数据（如颜色，形状，敲击声等）和标签数据（如好瓜或坏瓜），那么，这种训练方法通常称为监督式训练方法，这也是本系列文章中主要讨论的模型使用的训练方法。
## 深度学习
对于传统的机器学习方法来说，通常需要人工提取特征，如上面提到的西瓜的颜色，形状等，并形成结构化的数据。这是一个很大的局限性，一方面，人工提取特征通常需要一笔不菲的开销，另一方面，有时候提取特征具有很大的难度。而在人工智能领域，很多时候要处理的数据，都是直接来自现实世界中的非结构化的原始数据，如图像和音频等。拿图像举例来说，通常需要处理大量的像素值，并且，各个单独的像素本身之间，几乎没有明确的含义；而要理解图像的含义，则必须借助于像素之间复杂的空间关系。另外，音频则是由一系列随时间而变化的值来表示的，同样，如果单独拿出某个值来的话，很难听出什么门道。相反，这些值之间的顺序和距离，才是确定音频的含义的关键所在。
尽管传统的机器学习方法对于这些类型的数据无能为力，但是该领域的研究人员也不是省油的灯，经过几十年的折腾，并得益于近期计算能力和可用数据量的飞速发展，终于鼓捣出一类机器学习算法：深度学习。
那么，深度学习到底是个什么鬼？老实说，深度学习就是深度神经网络。那么，为啥不直接叫深度神经网络呢？说来话长，曾经有段时间，神经网络在学术界和工业界特别不受待见，虽然研究人员发现，经过改进训练方法后，模型的效果非常出色，但是，只要论文中带有“神经网络”这个词，肯定是没有机会发表的。所以，他们开始曲线救国，给它换了个叫法——深度学习。后来，媒体界的朋友们以为发明了新算法，也来推波助澜，所以，这个名字就慢慢叫开了。
说到这里，读者可能很好奇——深度神经网络到底长啥样呢？别急，下面将为大家展示一个最简单的深度神经网络：
图1 深度神经网络示意图
其中，各层内部的圆圈或者说是节点，用于表示神经元；各层之间连线，表示神经元之间的连接。另外，神经网络左边用于接受输入数据的第一层，被称为输入层；而位于右侧的那一层，称为输出层，它是用来输出答案的。同时，介于输入层和输出层之间的网络层，统称为隐藏层。也许神经网络始作俑者怕倒了我们的胃口，所以，就把神经网络的脏活累活都放到中间层，并让外部看不到它们，这样做还有一个好处，即增加神秘感——当计算通过网络传播时，隐藏层简直就像魔术师们大变活人的黑箱。并且，只要网络中的隐藏层达到一个及以上，我们就可以大言不惭地称自己的网络为“深度”神经网络了。
实际上，当我们的数据通过输入层进入深度神经网络时，该层中的神经元将被不同程度的激活。为此，我们可以给每个神经元分配一个数值，用以表示其激活程度——数值越大，表示激活程度越高；数值越小，表示激活程度越低。在下图中，我们通过颜色的深浅来表示神经元的激活程度。
图2 输入数据示意图
当某层中的神经元被激活时，就会在其输出端生成一个信号（我们可以理解为输出了一个数值），并通过相应的连接转发给下一层，所以，我们可以说，某一层的激活程度，是与前一层的激活程度休戚相关的——这恐怕不难理解吧？您难道不想成为“富二代”她/他爹/妈么？
同时，对于同样的输入，不同的神经元的反应可以是不同的：有些神经元的激活程度，可以比其他神经元的更大或更小一些。在此，我们还可以做一个类比，两个同学吃同一盘菜，一个辣得大汗淋漓，一个啥事没有。所以，这一点也不难理解。
此外，神经网络中的连接，对上游神经元的输出信号，是具有某种调节作用的，比如，有的连接可以放大信号，而有的连接则对信号有衰减的作用——对于这一特性，我们可以通过赋予每个连接一个相应的权重来加以表示。这一点，类似于不同材质的导线，具有不同的电阻。因此，网络连接对于下游神经元的行为而言，将起着举足轻重的作用。
图3 前向传播示意图
经过前面各层神经元的处理后，我们就可以在输出层得到相应的处理结果了。就本例来说，数据是一直向前馈送的，也就是说，前面的神经元是不会收到后面神经元的“回赠”的，所以，这种网络被称为前馈网络。熟悉图论的读者可能已经看出来了，前馈网络不就是一种有向无环图吗？！是的，的确如此。
图4 输出结果
## 小结
在本文中，我们首先为读者介绍一些与深度神经网络有关的基本概念，如什么是机器学习，什么是深度神经网络等，在下一篇文章中，我们将给出一个简单的深度学习模型，以便于我们练手。