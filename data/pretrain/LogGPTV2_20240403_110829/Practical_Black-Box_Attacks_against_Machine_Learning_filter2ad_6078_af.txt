Figure 13 provides the speciﬁc DNN architectures used
throughout Sections 5, 6, and 11. The ﬁrst column is the
identiﬁer used in the paper to refer to the architecture. The
second and third columns respectively indicate the input and
output dimensionality of the model. Finally, each additional
column corresponds to a layer of the neural network.
For two “similar” architectures F and G distribu-
tions DF and DG induced by a population distri-
bution D are highly correlated.
If distributions DF and DG were independent, then the noise
they add during adversarial sample crafting are independent.
In this case, our intuition is that adversarial samples would
not transfer (in the two cases you are adding noise that are
independent). The question is: how to verify our conjecture
despite the population distribution D being unknown?
We turn to statistical hypothesis testing. We can empiri-
cally estimate the distributions DF and DG based on known
samples. First, we generate two sequences of sign matrices
σ1 = (cid:104)M1, M2,···(cid:105) and σ2 = (cid:104)N1, N2,···(cid:105) using the sample
set (e.g. MNIST) for a substitute DNN F and oracle G.
Next we pose the following null hypothesis:
HN : The sequences σ1 and σ2 are drawn from
independent distributions.
We use standard tests from the statistical hypothesis testing
literature to test the hypothesis HN . If the hypothesis HN is
rejected, then we know that the sign matrices corresponding
to the two architectures F and G are correlated.
In
ID
784
A
B
3072
C 3072
D 3072
3072
E
784
F
784
G
784
H
I
784
784
J
784
K
L
784
M 784
Out CM CM RL
200
10
256
43
43
200
200
43
200
43
10
200
10
10
10
10
10
10
10
64
128
64
64
64
64
64
-
-
-
-
-
-
32
64
32
32
64
32
32
32
-
-
-
32
32
-
200
200
1000
1000
1000
-
RL
200
256
200
200
200
-
-
200
200
200
500
200
200s
RL
-
-
-
-
100
-
-
-
200
-
200
-
200s
S
10
43
43
43
43
10
10
10
10
10
10
10
10
We describe the test we use. There are several algorithms
for hypothesis testing: we picked a simple one based on a
chi-square test. An investigation of other hypothesis-testing
techniques is left as future work. Let pi,j and qi,j be the fre-
quency of +1 in the (i, j)-th entry of matrices in sequences σ1
and σ2, respectively. Let ri,j be the frequency of the (i, j)-th
entry being +1 in both sequences σ1 and σ2 simultaneo-
suly.9 Note that if the distributions were independent then
ri,j = pi,jqi,j. However, if the distributions are correlated,
then we expect ri,j (cid:54)= pi,jqi,j. Consider quantity:
m(cid:88)
n(cid:88)
χ2(cid:63) =
(ri,jN − pi,jqi,jN )2
i=1
j=1
pi,jqi,jN
Figure 13: DNN architectures: ID: reference used in the
paper, In:
input dimension, Out: output dimension, CM:
convolutional layer with 2x2 kernels followed by max-pooling
with kernel 2x2, RL: rectiﬁed linear layer except for 200s
where sigmoid units are used, S: softmax layer.
B. Intuition behind Transferability
Previous work started explaining why adversarial samples
transfer between diﬀerent architectures [4, 14]. Here, we
build an intuition behind transferability based on statistical
hypothesis testing [8] and an analysis of DNN cost gradient
sign matrices. A formal treatment is left as future work.
Recall the perturbation in the Goodfellow algorithm. In-
specting Equation 5, it is clear that, given a sample (cid:126)x, the
noise added would be the same for two DNNs F and G
if sgn(∇(cid:126)xcost(F, (cid:126)x, y)) and sgn(∇(cid:126)xcost(G, (cid:126)x, y)) were equal.
These matrices have entries in {+1,−1}. Let us write the
space of these matrices as Sgnn×m. Assume that the samples
(cid:126)x are generated from a population distribution D (e.g., in
our case the distribution from which the images of digits are
drawn). The formula sgn(∇(cid:126)xcost(F, (cid:126)x, y)) and D induce a
distribution DF over Sgnn×m (i.e. randomly draw a sample
from the distribution D and compute the quantity). Simi-
larly, DNN G and distribution D induce a distribution DG
over Sgnn×m. Our main conjecture is:
where N is the number of samples. In the χ-square test,
we compute the probability that P (χ2 > χ2(cid:63)), where χ2
has degrees of freedom (m − 1)(n − 1) = 27 × 27 = 729 for
the MNIST data. The χ2(cid:63) scores for substitute DNNs from
Table 1 range between 61, 403 for DNN A and 88, 813 for
DNN G. Corresponding P-values are below 10−5 for all archi-
tectures, with conﬁdence p  0 to avoid division by zero, which can be
achieved by rescaling.
518C. Discussion of Related Work
Evasion attacks against classiﬁers were discussed previously.
Here, we cover below black-box attacks in more details.
Xu et al. applied a genetic algorithm to evade malware
detection [18]. Unlike ours, it accesses probabilities assigned
by the classiﬁer to compute genetic variants ﬁtness. These
can be concealed by defenders. The attack is also not very
eﬃcient: 500 evading variants are found in 6 days. As the
classiﬁer is queried heavily, the authors conclude that the
attack cannot be used against remote targets. Finally, given
the attack’s high cost on low-dimensional random forests and
SVMs, it is unlikely the approach would scale to DNNs.
Srndic et al. explored the strategy of training a substi-
tute model to ﬁnd evading inputs [12]. They do so using
labeled data, which is expensive to collect, especially for
models like DNNs. In fact, their attack is evaluated only
on random forests and an SVM. Furthermore, they exploit
a semantic gap between the speciﬁc classiﬁers studied and
PDF renderers, which prevents their attack from being ap-
plicable to models that do not create such a semantic gap.
Finally, they assume knowledge of hand-engineered high-level
features whereas we perform attacks on raw inputs.
Tramer et al. considered an adversarial goal diﬀerent from
ours: the one of extracting the exact value of each model
parameter. Using partial knowledge of models and equation
solving, they demonstrated how an adversary may recover pa-
rameters from classiﬁers hosted by BigML and Amazon [15].
However, it would be diﬃcult to scale up the approach to
DNNs in practice. To recover the 2, 225 parameters of a
shallow neural network (one hidden layer with 20 neurons)
trained on a local machine, they make 108, 200 label queries.
Instead, we make 2, 000 label queries to train substitute
DNNs made up of 8 hidden layers (each with hundreds of
neurons) with a total of over 100, 000 parameters—albeit at
the expense of a reduced guaranteed accuracy for the model
extraction operation. Unlike theirs, our work also shows
that our substitutes enable the adversary to craft adversarial
examples that are likely to mislead the remote classiﬁer.
Figure 14: Frequencies of cost gradient sign matrix
components equal between substitute A and the or-
acle at substitute training epochs ρ ∈ {0, 3, 6} (three on the
right), compared to a pair of random sign matrices (ﬁrst
image).
Figure 15: Frequencies of cost gradient sign matrix
components equal between substitute A and the or-
acle
to pixels located in the center of the image are higher in the
(substitute, oracle) matrix pairs. The phenomenon ampliﬁes
as training progresses through the substitute epochs. We
then compute the frequencies separately for each sample
source class in Figure 15. Sign matrices agree on pixels rele-
vant for classiﬁcation in each class. We plotted similar ﬁgures
for other substitute DNNs. They are not included due to
space constraints. They show that substitutes yielding lower
transferability also have less components of their cost gradi-
ent sign matrix frequently equal to the oracle’s. This suggests
that correlations between the respective sign matrices of the
substitute DNN and of the oracle—for input components that
are relevant to classiﬁcation in each respective class—could
explain cross-model adversarial sample transferability.
0.640.600.520.480.440.400.360.56Random MatricesSubstitute⇢=0⇢=3⇢=6SubstituteSubstitute0.640.600.520.480.440.400.360.56Class 0Class 1Class 2Class 3Class 4Class 5Class 6Class 7Class 8Class 9519