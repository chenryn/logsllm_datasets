and supports priority event queues to maintain responsiveness of
VIP conﬁguration operations under overload.
4.
IMPLEMENTATION
We implemented all three Ananta components from Figure 5.
Ananta Manager and Mux are deployed as a tenant of our cloud
platform itself. The Host Agent is deployed on every host in our
cloud and is updated whenever we update the Ananta tenant. In a
typical deployment ﬁve replicas of AM manage a single Mux Pool.
Most Mux Pools have eight Muxes in them but the number can be
based on load.
Ananta Manager: AM performs various time-critical tasks —
conﬁguration of VIPs, allocation of ports, coordination of DIP health
across Muxes. Therefore, its responsiveness is very critical. To
achieve a high degree of concurrency, we implemented AM using
a lock-free architecture that is somewhat similar to SEDA [29]. As
shown in Figure 10, AM is divided into the following stages — VIP
validation, VIP conﬁguration, Route Management, SNAT Man-
agement, Host Agent Management and Mux Pool Management.
Ananta implementation makes two key enhancements to SEDA.
First, in Ananta, multiple stages share the same threadpool. This
allows us to limit the total number of threads used by the system.
Second, Ananta supports multiple priority queues for each stage.
This is useful in maintaining responsiveness during overload con-
ditions. For example, SNAT events take less priority over VIP con-
ﬁguration events. This allows Ananta to ﬁnish VIP conﬁguration
tasks even when it is under heavy load due to SNAT requests.
Ananta maintains high availability using Paxos [14]. We took an
existing implementation of Paxos and added discovery and health
monitoring using the SDK of our cloud platform. The SDK notiﬁes
AM of addition, removal or migration of any replicas of the Paxos
cluster. This allows us to automatically reconﬁgure the cluster as
instances are added or removed dynamically. These features of the
platform SDK result in signiﬁcantly reduced operational overhead
for Ananta. The platform also provides a guarantee that no more
than one instance of the AM role is brought down for OS or applica-
tion upgrade. This notion of instance-by-instance update maintains
availability during upgrades. Ananta uses Paxos to elect a primary
and only the primary does all the work.
Mux: Mux has two main components — a kernel-mode driver
and a user-mode BGP [20] speaker. The kernel-mode driver is im-
plemented using the Windows Filtering Platform (WFP) [30] driver
model. The driver intercepts packets at the IP layer, encapsulates
and then sends them using the built-in forwarding function of the
OS. Delegating routing to the OS stack has allowed Mux code
to remain simple, especially when adding support for IPv6, as it
does not need to deal with IP fragmentation and next-hop selec-
tion. The driver scales to multiple cores using receive side scaling
(RSS) [22]. Since the Mux driver only encapsulates the packet with
a new IP header and leaves the inner IP header and its payload in-
tact, it does not need to recalculate TCP checksum and hence it does
not need any sender-side NIC ofﬂoads. For IPv4, each Mux can
hold 20,000 load balanced endpoints and 1.6 million SNAT ports
Figure 11: CPU usage at Mux and Hosts with and without Fast-
path. Once Fastpath is turned on, the hosts take over the encap-
sulation function from Mux. This results in lower CPU at Mux and
CPU increase at every host doing encapsulation.
in its VIP Map with 1GB of memory. Each mux can maintain state
for millions of connections and is only limited by available mem-
ory on the server. CPU performance characteristics of the Mux are
discussed in §5.2.3.
Host Agent: The Host Agent also has a driver component that
runs as an extension of the Windows Hyper-V hypervisor’s virtual
switch. Being in the hypervisor enables us to support unmodiﬁed
VMs running different operating systems. For tenants running with
native (non-VM) conﬁguration, we run a stripped-down version of
the virtual switch inside the host networking stack. This enables us
to reuse the same codebase for both native and VM scenarios.
Upgrading Ananta: Upgrading Ananta is a complex process
that takes place in three phases in order to maintain backwards-
compatibility between various components. First, we update in-
stances of the Ananta Manager, one at a time. During this phase,
AM also adapts its persistent state from previous schema version
to the new version. Schema rollback is currently not supported.
Second, we upgrade the Muxes; and third, the Host Agents.
5. MEASUREMENTS
In this section we ﬁrst present a few micro-benchmarks to evalu-
ate effectiveness and limitations of some of our design choices and
implementation, and then some data from real world deployments.
5.1 Micro-benchmarks
5.1.1 Fastpath
To measure the effectiveness of Fastpath and its impact on host
CPU, we conducted the following experiment. We deployed a 20
VM tenant as the server and two 10 VM tenants as clients. All
the client VMs create up to ten connections to the server and up-
load 1MB of data per connection. We recorded the CPU usage at
the host nodes and one Mux. The results are shown in Figure 11.
We found the host CPU usage to be uniform across all hosts, so
we only show median CPU observed at a representative host. As
expected, as soon as Fastpath is turned on, no new data transfers
happen through the Mux. It only handles the ﬁrst two packets of
any new connection. Once the Mux is out of the way, it also stops
being a bottleneck for data transfer and VMs can exchange data at
the speed allowed by the underlying network.
5.1.2 Tenant Isolation
Tenant isolation is a fundamental requirement of any multi-tenant
In the following we present two different experiments
service.
214Figure 12: SYN-ﬂood Attack Mitigation. Duration of impact
shows the time Ananta takes to detect and black-hole trafﬁc to the
victim VIP on all Muxes.
Figure 13: Impact of heavy SNAT user H on a normal user
N. Heavy user H sees higher latency and higher SYN retransmits.
Normal user N’s performance remains unchanged.
that show Ananta’s ability to isolate inbound packet and outbound
SNAT abuse. For these experiments, we deployed ﬁve different
tenants, each with ten virtual machines, on Ananta.
SYN-ﬂood Isolation: To measure how quickly Ananta can iso-
late a VIP under a SYN-ﬂood attack, we ran the following experi-
ment (other packet rate based attacks, such as a UDP-ﬂood, would
show similar result.) We load Ananta Muxes with a baseline load
and launch a SYN-ﬂood attack using spoofed source IP addresses
on one of the VIPs. We then measure if there is any connection
loss observed by clients of the other tenants. Figure 12 shows the
maximum duration of impact observed over ten trials. As seen in
the chart, Mux can detect and isolate an abusive VIP within 120
seconds when it is running under no load, minimum time being 20
seconds. However, under moderate to heavy load it takes longer to
detect an attack as it gets harder to distinguish between legitimate
and attack trafﬁc. We are working on improving our DoS detection
algorithms to overcome this limitation.
SNAT Performance Isolation: SNAT port allocation at Ananta
Manager could be a subject of abuse by some tenants. It is possi-
ble that a tenant makes a lot of SNAT connections causing impact
on other tenants. To measure the effectiveness of per-VM SNAT
isolation at AM, we conducted the following experiment. A set of
normal use tenants (N) make outbound connections at a steady rate
of 150 connections per minute. Whereas, a heavy SNAT user (H)
keeps increasing its SNAT requests. We measure the rate of SYN
retransmits and the SNAT response time of Ananta at the corre-
sponding HAs. Figure 13 shows the aggregate results over multiple
trials. As seen in the ﬁgure, the normal tenants’ connections keep
succeeding at a steady rate without any SYN loss; and its SNAT
port requests are satisﬁed within 55ms. The heavy user, on the
other hand, starts to see SYN retransmits because Ananta delays its
SNAT requests in favor of N. This shows that Ananta rewards good
behavior by providing faster SNAT response time.
5.1.3 SNAT Optimizations
A key design choice Ananta made is to allocate SNAT ports in
AM and then replicate the port allocations to all Muxes and other
AM replicas to enable scale out and high availability respectively.
Since port allocation takes place for the ﬁrst packet of a connec-
tion, it can add signiﬁcant latency to short connections. Ananta
overcomes these limitations by implementing three optimizations
as mentioned in §3.5.1. To measure the impact of these optimiza-
tions we conducted the following experiment. A client continu-
ously makes outbound TCP connections via SNAT to a remote ser-
vice and records the connection establishment time. The resulting
Figure 14: Connection establishment time experienced by out-
bound connections with and without port demand prediction.
data is partitioned into buckets of 25ms. The minimum connection
establishment time to the remote service (without SNAT) is 75ms.
Figure 14 shows connection establishment times for the following
two optimizations when there is no other load on the system.
Single Port Range: In response to a port request, AM allocates
eight contiguous ports instead of a single port and returns them to
the requesting HA. HA uses these ports for any pending and sub-
sequent connections. The HA keeps any unused ports until a pre-
conﬁgured timeout before returning them to AM. By doing this
optimization, only one in every eight outbound connections ever
results in a request to AM. This is evident from Figure 14 — 88%
connections succeed in the minimum possible time of 75ms. The
remaining 12% connections take longer due to the round-trip to
AM. Without the port range optimization every new connection re-
quest that cannot be satisﬁed using existing already allocated ports
will make a round-trip to AM.
Demand Prediction: When this optimization is turned on, AM
attempts to predict port demand of a DIP based on its recent his-
tory. If a DIP requests new ports within a speciﬁed interval from
its previous requests, AM allocates and returns multiple eight-port
port ranges instead of just one. As shown in Figure 14, with this op-
timization 96% of connections are satisﬁed locally and don’t need
to make a round-trip to AM. Furthermore, since AM handles fewer
requests, its response time is also better than allocating a single port
range for each request.
215Figure 15: CDF of SNAT response latency for the 1% requests
handled by Ananta Manager.
Figure 17: Distribution of VIP conﬁguration time over a 24-hr
period.
Scale
over this period across all test tenants was 99.95%, with a mini-
mum of 99.92% for one tenant and greater than 99.99% for two
tenants. Five of the low availability conditions between Jan 21 and
Jan 26 happened due to Mux overload. The Mux overload events
were primarily caused by SYN-ﬂood attacks on some tenants that
are not protected by our DoS protection service. Two availability
drops were due to wide-area network issues while the rest were
false positives due to update of the test tenants.
5.2.3
For control plane, Ananta ensures that VIP conﬁguration tasks
are not blocked behind other tasks.
In a public cloud environ-
ment, VIP conﬁguration tasks happen at a rapid rate as customers
add, delete or scale their services. Therefore, VIP conﬁguration
time is an important metric. Since Ananta needs to conﬁgure HAs
and Muxes during each VIP conﬁguration change, its programming
time could be delayed due to slow HAs or Muxes. Figure 17 shows
the distribution of time taken by seven instances Ananta to com-
plete VIP conﬁguration tasks over a 24-hr period. The median con-
ﬁguration time was 75ms, while the maximum was 200seconds.
These time vary based on the size of the tenant and the current
health of Muxes. These times ﬁt within our API SLA for VIP con-
ﬁguration tasks.
For data plane scale, Ananta relies on ECMP at the routers to
spread load across Muxes and RSS at the NIC to spread load across
multiple CPU cores. This design implies that the total upload through-
put achieved by a single ﬂow is limited by what the Mux can achieve
using a single CPU core. On our current production hardware, the
Mux can achieve 800Mbps throughput and 220Kpps using a sin-
gle x64, 2.4GHz core. However, Muxes can achieve much higher
aggregate throughput across multiple ﬂows. In our production de-
ployment, we have been able to achieve more than 100Gbps sus-
tained upload throughput for a single VIP. Figure 18 shows band-
width and CPU usage seen over a typical 24-hr period by 14 Muxes
deployed in one instance of Ananta. Here each Mux is running on
a 12-core 2.4Ghz intel R(cid:13) Xeon CPU. This instance of Ananta is
serving 12 VIPs for blob and table storage service. As seen in the
ﬁgure, ECMP is load balancing ﬂows quite evenly across Muxes.
Each Mux is able to achieve 2.4Gbps throughput (for a total of
33.6Gbps) using about 25% of its total processing power.
6. OPERATIONAL EXPERIENCE
We have been offering layer-4 load balancing as part of our cloud
platform for the last three years. The inability of hardware load bal-
ancers to deal with DoS attacks, network elasticity needs, the grow-
Figure 16: Availability of test tenants in seven different data
centers over one month.
5.2 Real World Data
Several instances of Ananta have been deployed in a large pub-
lic cloud with a combined capacity exceeding 1Tbps. It has been
serving Internet and intra-data center trafﬁc needs of very diverse
set of tenants, including blob, table and queue storage services for
over two years. Here we look at some data from real world.
5.2.1 SNAT Response Latency
Based on production data, Ananta serves 99% of the SNAT re-
quests locally by leveraging port reuse and SNAT preallocation as
described above. The remaining requests for tenants initiating a lot
of outbound requests to a few remote destinations require SNAT