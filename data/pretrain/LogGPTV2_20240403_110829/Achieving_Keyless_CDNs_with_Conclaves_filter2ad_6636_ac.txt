29th USENIX Security Symposium    739
However, two important multi-process abstractions that
Graphene does not support with conﬁdentiality and integrity
guarantees are a read-write ﬁlesystem, and shared memory.
Graphene’s sole ﬁlesystem, chrootfs, is modeled as a re-
stricted view of the host’s ﬁlesystem. Graphene does not
support shared memory at all (neither anonymous nor ﬁle-
backed).
Conclaves extend upon this prior design by leaning into
the distributed system nature of it. We implement kernel ser-
vices as kernel servers; applications act as clients, connecting
to and issuing requests to kernel services—via pipes or TLS
network connections. The kernel servers also run atop the
libOS. Our design is effectively that of a multi-server micro-
kernel system, similar to GNU Hurd [58] or Mach-US [59],
in which shared resource abstractions are implemented as a
set of enclaved daemons shared by all processes in the sys-
tem.
4.1.1 Conclave Kernel Servers
Using the NGINX web server as a guide (as software rep-
resentative of a CDN edge server), we identiﬁed ﬁve key
shared resources: ﬁles, shared memory, locks/semaphores,
cryptographic keys, and time. For ﬂexibility in deployment
conﬁgurations, we implement four servers to manage these
resources1:
fsserver The fsserver provides a ﬁle system interface to user
applications. Much like a remote ﬁle system, the fsserver
performs strict access control to restrict access only to the
relevant enclaves. We discuss how this access control is pro-
visioned in §4.2.2. NGINX uses the ﬁle system for storing
cached and persistent web resources.
memserver The memserver provides an interface for cre-
ating, accessing, manipulating, and locking shared mem-
ory. NGINX uses shared memory for storing usage statistics,
metadata for the on-disk HTML caches, and state for TLS
session resumption.
keyserver The keyserver is an SGX enclave rendition of a
hardware-security module (HSM): the keyserver stores pri-
vate keys and performs any private key cryptographic op-
erations. Like Keyless SSL [9], this not only maintains
the conﬁdentiality of the private key with respect to an un-
trusted host, but also isolates the key to an address space dis-
tinct from the application’s, thereby guarding against critical
memory disclosure vulnerabilities, such as Heartbleed [60].
timeserver Given that the components of a conclave must
authenticate one another, we need trusted time to guard
against attacks that trick the conclave into accepting ex-
pired certiﬁcates. Unfortunately, SGX itself does not pro-
vide trusted time. Its SDK [44] provides features [45] for re-
trieving coarse-grained, monotonic time through a protected
clock provided by Intel’s Converged Security and Manage-
ment Engine (CSME), but not all processors support it [61].
Instead of relying on the CSME, we simply design a re-
mote, signed timestamping server. The timestamping server
runs outside of an enclave, on a remote trusted machine (e.g.,
at the CDN’s customer). The timeserver’s purpose is not
to provide ﬁne-grained precision to the conclaved processes,
but rather to serve as an integrity check of the time those pro-
cesses receive from the untrusted host.
In §5, we detail several variants of each of these kernel
servers, covering various trade-offs between performance
and security. While we have found that these four kernel
servers sufﬁce for NGINX—and, we believe, for a wide
range of networked applications—it is possible that other ap-
plications may need more (e.g., for specialized IPC).
4.1.2 Conclave Images
Conclaves bundle the SGX microkernel runtime and applica-
tion suite into a deployable and executable image, reminis-
cent of a traditional container image. When the conclave is
executed, the ﬁrst enclave process that is executed is an init
process, which executes the kernel servers and the speciﬁed
application proper. From that point, the application can fork,
spin up new applications, and so on.
4.2 Phoenix Design
Conclaves provide a multi-process runtime for running
multi-process legacy applications within SGX enclaves.
Phoenix addresses a number of remaining questions concern-
ing how the customer and CDN operator deploy and provi-
sion the combined runtime and application suite.
The core problem Phoenix solves is that the runtime
and application need various assets—in particular, keying
material—in order to successfully and securely execute.
These assets must be delivered in a manner that is shielded
from CDN inspection or tampering. Furthermore, as one of
our goals is to not burden the customer with running addi-
tional services, we, paradoxically, must have the CDN man-
age the provisioning of these assets on behalf of the customer.
Finally, Phoenix’s design must allow for multi-tenant deploy-
ments. We address each of these in turn.
We present a high-level overview of Phoenix’s design in
Figure 1. Its design spans three principles: (1) the CDN cus-
tomer, who must run the origin server as they do today, as
well as an agent for provisioning conclaves, (2) the core CDN
servers, which make and enforce decisions of where exactly
to deploy customers’ content, and (3) the CDN edge server
itself, which receives the majority of the changes.
4.2.1 Bootstrapping Trust
1Due to the common pattern of using locks with shared memory, the
memserver manages both.
We ﬁrst address how the conclave, viewed as a distributed
system, establishes the trust of each member node, whether
740    29th USENIX Security Symposium
USENIX Association
ﬁes a graph of which processes can establish secure channels
with one another.
4.2.2 Provisioning Server and Provisioning Agents
Having bootstrapped trust within the conclave, our next chal-
lenge is the delivery of sensitive assets to the conclave.
Phoenix has the init process spawn a process called the provi-
sioning agent that communicates remotely with a provision-
ing server operated by the CDN. The provisioning agent pe-
riodically beacons to the provisioning server, and downloads
and installs any new conclave assets.
The provisioning agent and server both run in an enclave,
and use essentially the same method for secure channel es-
tablishment as what we described for channel establishment
within the conclave. The main difference is that the quote is
generated and validated using SGX’s remote attestation pro-
tocol, rather than the local attestation protocol.
At this point, we have recursed nearly to the base case;
all that is needed for end-to-end asset encryption is for the
customer to post assets to the provisioning server.
4.2.3 Key Management
The last thing we must address is how Phoenix enables the
CDN to manage its customers’ keys. Today, CDNs manage
their customers’ keys in a handful of ways [6, 7]; customers
can generate their own keys and upload them to the CDN,
or they can delegate all key and certiﬁcate management to
the CDN. When CDNs manage their customers’ certiﬁcates,
they often put multiple customers on a single “cruise-liner
certiﬁcate” [6], under a single key pair.
Phoenix supports all of these conﬁgurations by shifting
them into the (enclaved) provisioning server. When cus-
tomers wish to upload their keys, they establish a secure
connection from their provisioning agent to the CDN’s pro-
visioning server. When the CDN manages its customers’
keys, the provisioning server generates key pairs and runs
Let’s Encrypt’s [4] ACME protocol [63]—from within the
enclave—to request the certiﬁcates. The provisioning server
can then load this data onto edge servers however it sees ﬁt,
by connecting to provisioning agents running in enclaves on
the edge servers (see Figure 1). The end result is that, unlike
today, the CDN will never learn the secret keys. In fact, when
the CDN manages its customers’ keys, no one learns them,
as they will forever reside within one or more enclaves.
4.3 Deployment Scenarios
Phoenix’s conclave-based design permits a diverse range of
deployment options to support varying threat models like
those described in §2.4. There are two dimensions for de-
scribing edge server deployments: First, a deployment can
be single-tenant or multi-tenant, based on whether there is
one or more customers on a given edge server (physical or
Figure 1: Architectural design of Phoenix. Multiple enclaves
(yellow boxes) reside in a logical conclave (red boxes), per-
mitting multiple processes and multi-tenant deployments.
The CDN Edge and Core servers run on untrusted hosts.
kernel server or application process. This is a chicken-and-
egg problem of establishing a secure channel between two
nodes without ﬁrst provisioning these nodes with, say, private
keys and certiﬁcates for mutual authentication.
The standard approach for establishing a secure channel in
an SGX setting is to use SGX as a root of trust and enclave
attestation as a form of authenticated identity, and to merge
this form of attestation into the establishment of the shared
channel secret. To that end, Phoenix follows closely from
the work of Knauth et al. [62], which integrates attestations
with TLS by adding the SGX quote as an X.509 certiﬁcate
extension. This has the effect of making channel establish-
ment and SGX attestation occur together, atomically, with re-
spect to the channel protocol. Certiﬁcate validation can thus
be extended to examine these new extensions.
Adding new certiﬁcate extensions, of course, is not the full
story. In this setup, the enclave generates an ephemeral key
pair. SGX quotes are, mandatorily, over the enclave image,
the enclave signer, non-measurable state, such as the enclave
mode (e.g., debug vs. production), and, optionally, any ad-
ditional data (user data) the enclave wants to associate with
itself. The trick for ensuring the atomicity of attestation and
secure channel establishment is for the enclave to specify as
user data a hash of the ephemeral public key. Since the key
pair is created within the enclave, and since only an enclave
can get a valid quote, such user data binds the key pair to the
enclave. The enclave then generates a self-signed certiﬁcate
for this ephemeral public key, which includes the aforemen-
tioned extensions for the quote and Intel Attestation Service
(IAS) veriﬁcation.
In our conclave setup, the attestation is a local attestation,
and validation of the quote is based on a list of valid attesta-
tion values in the manifest. Speciﬁcally, the manifest speci-
USENIX Association
29th USENIX Security Symposium    741
CDN EdgeCDN CoreGrapheneNGINXengineWAFGrapheneNGINXengineWAFGraphenefsserverGraphenekeyserverGraphenememserverGrapheneProvisioning serverGrapheneProvisioning agentProvisioning agentCustomerGrapheneNGINXEngineWAFContentServerOriginTimeConclave1. Provision    Conclave4. Pull Web Content3. ConﬁgureEnclaves2. DeployConclavevirtual). Second, a given customer’s deployment can be fully-
enclaved or partially-enclaved, based on whether all or just
a speciﬁc subset of components are executed in an enclave.
The provisioning agent and server design handle these use
cases uniformly. Throughout the design of Phoenix, we have
described the single-tenant, fully-enclaved deployment. Be-
low, we discuss two other potential deployments.
Single-tenant, partially-enclaved deployments handle an
honest-but-curious attacker wherein the customer trusts the
CDN with everything but the private key.
In this deploy-
ment, only the keyserver and provisioning agent reside in the
conclave. This conﬁguration is similar to Keyless SSL, but
without requiring modiﬁcations to the application or TLS.
Multi-tenant deployments multiplex customers at one of
three places. First, the CDN operator can trivially place a
proxy server (for example, an HAProxy [64]) on the edge
server; the proxy examines the SNI ﬁeld of the client re-
quest and proxies to the relevant conclave. In other words,
this strategy reduces to running single tenant, fully-enclaved
conclaves for many customers. Second, if the application is
conducive to multiplexing, then the CDN operator can run
an instance of the application in an enclave, with the applica-
tion’s conﬁguration reﬂecting the customer partitions; each
customer then runs their own conclave of kernel servers. As
an example, NGINX can run multiple virtual servers; the re-
sources for each virtual server are mounted on mountpoints
within the application that point to each customer’s respec-
tive kernel servers. Finally, the kernel servers themselves can
multiplex the resources of several customers. These repre-
sent different trade-offs: more multiplexing can increase the
attack surface, but requires less resources to achieve high per-
formance (SGX can incur signiﬁcant overhead in switching
between enclaves on a given CPU).
5 Implementation
We implement conclaves and Phoenix as extensions to the
open-source Graphene SGX libOS [27]. In this section, we
present details of this implementation. We have made our
code and data publicly available so that others can continue
to build off this work.2
5.1 Kernel Servers
We implement the fsserver, memserver, and keyserver as
single-threaded, single-process, event-driven servers that
communicate with the application’s Graphene instances over
a TLS-encrypted stream channel. In the case of a TCP chan-
nel, we disable Nagle’s algorithm due to the request-response
nature of the RPCs. The timeserver uses a datagram channel.
Each server is independent.
fsserver For our ﬁle server, nextfs, we extend lwext4’s [65]
userspace implementation of an ext2 ﬁlesystem into a net-
2Our code may be found at https://phoenix.cs.umd.edu.
worked server. nextfs uses an untrusted host ﬁle as the back-
ing store, similar to a block device. We develop three vari-
ants of this device to accommodate different security pos-
tures, and a fourth for comparison purposes.
• bd-std stores data blocks in plaintext, without integrity
guarantees. This serves as a baseline in our evaluation.
• bd-crypt encrypts each block using AES-256 in XTS
mode, the de facto standard for full-disk encryption [66,
67]. We base each block’s initialization vector on the
block’s ID. This, too, lacks integrity guarantees, and is
thus suitable only for an honest-but-curious attacker.
• bd-vericrypt adds integrity guarantees to bd-crypt, thus
providing authenticated encryption. It does so by main-
taining a Merkle tree over the blocks: a leaf of the tree
is an HMAC of the associated (encrypted) block, and an
internal node the HMAC of its two children. To keep the
memory needs of the enclave small, bd-vericrypt consults
a serialized representation of the tree in a separate ﬁle,
rather than use an in-memory representation. The root of
the Merkle tree exists both on the ﬁle and in enclave mem-
ory; the HMAC key exists only in enclave memory. As an
optimization for reducing reads and writes to the Merkle
tree ﬁle, bd-vericrypt maintains an in-enclave LRU-cache
of the tree nodes. bd-vericrypt is the appropriate choice in
a Byzantine threat model.
memserver We implement shared memory as ﬁlesystems
that implement a reduced set of the ﬁlesystem API3: open,
close, mmap, and advlock (advlock handles both advisory
locking and unlocking). In our shared memory ﬁlesystems,
ﬁles are called memory ﬁles, and either represent a pure,
content-less lock, or a lock with an associated shared mem-
ory segment. Memory ﬁles are non-persistent: they are cre-
ated on the ﬁrst open and destroyed when no process holds a
descriptor to the ﬁle and no process has the associated mem-
ory segment mapped.