of input values [27, 38, 58, 64, 65] and microarchitectural
techniques [11, 22, 63, 66, 69].
Invasive untrusted device attacks typically are carried out
by knowledgeable insiders who have physical access to the
device. These insiders may be able to change the conﬁguration
of the hardware causing system malfunction. Examples of such
attacks include changing the boot ROM, RAM, Disk or more
generally external devices to boot a compromised OS with
backdoors or stealing cryptographic keys using unprotected
JTAG ports [13][56]. A countermeasure is to store data in
encrypted form in untrusted (hardware) entities. Since the ‘80s
there has been signiﬁcant work in this area [61]. Secure co-
processors [28, 35] and Trusted Platform Modules [4] have
been used to secure the boot process. More recently, enabled
by VLSI advances, researchers have proposed continuous pro-
tection of programs and on-chip methods for communication
with memory and I/O integration [29, 40].
A new threat that has recently seen a ﬂurry of activity is
intentional backdoors in hardware. As hardware development
closely resembles software development both in its global
scope and liberal use of third party IP, there is growing interest
and concern in hardware backdoors and their applications to
cyber offense and defense. Broadly speaking, work in this area
can fall into one of three categories: threats and countermea-
sures against malicious designers, threats and countermeasures
against malicious design automation tools, and threats and
countermeasures against malicious foundries. There has been
some work on detecting backdoors inserted by malicious
foundries that typically rely on side-channel information such
as power for detection [12, 16, 17, 24, 41, 54, 57, 70].
There has been no work on providing countermeasures against
malicious designers, which this work aims to address.
There have been a few unconﬁrmed incidents of design-level
hardware attacks [10] and some work in academia on creating
hardware backdoors. Shamir et al.
[20] demonstrate how to
exploit bugs in the hardware implementation of instructions.
King et al.
[36] propose a malicious circuit that can be
embedded inside a general-purpose CPU and can be leveraged
by attack software executing on the same system to launch
a variety of attacks. They demonstrate a number of such
hybrid software/hardware attacks, which operate at a much
higher abstraction level than would generally be possible with
a hardware-only attack. Although they do not discuss any
protection or detection techniques, their work is particularly
illuminating in demonstrating the feasibility and ease of cre-
ating such attacks through concrete constructs.
III. THREAT MODEL
A malicious hardware designer has to be strategic in cre-
ating backdoors because processor development, especially
commercial development, is a carefully controlled process.
Broadly speaking, the attacker has to follow two steps: ﬁrst,
design a backdoor for an attack, and second, build a trigger for
the attack. Just like regular design, the attacker has to handle
trade-offs regarding degrees of deception, time to completion,
veriﬁcation complexity, and programmability. In this section
we discuss these tradeoffs for attack triggers (Section III-B)
and attack backdoors (Section III-C). However, we begin our
discussion by detailing assumptions in our threat model.
A. Assumptions
• Assumption #1: Division of Work Typically, a microproces-
sor team is organized into sub-teams, and each sub-team is
responsible for a portion of the design (e.g., fetch unit or load-
store unit). Microprocessor design is a highly cooperative and
structured activity with tens to hundreds of participants [14].
The latest Intel Atom Processor, for instance, is reported to
have had 205 “Functional Unit Blocks” [3]; a design of a
recent System-on-Chip product from ST Microelectronics is
reported to have required over 200 engineers hierarchically
175
organized into eight units [1]. We assume that any sub-unit
team in a design can be adversarial but that not more than one
of the sub-units can be simultaneously compromised. While
adversarial nation-states could possibly buy out complete
teams to create undetectable malicious designs, it is more
likely that attackers will be a small number of “bad apples.”
• Assumption #2: Access The focus of this work is to detect
the handiwork of malicious microprocessor designers, which
includes chip architects, microarchitects, RTL engineers and
veriﬁers, and circuit designers. These workers have approved
access to the design, privilege to change the design, and an
intricate knowledge of the microprocessor design process and
its workings. A malicious designer will be able to provision
for the backdoor either during the speciﬁcation phase, e.g.,
by allocating “reserved bits” for unnecessary functions, or
by changing the RTL. We assume this will be unnoticed
during the implementation phase and after the code reviews
are complete. Our assumption that code audits will not be
able to catch all backdoors is justiﬁed because audits are not
successful at catching all inadvertent, non-malicious design
bugs.
• Assumption #3: Extent of Changes The malicious designer
is able to insert a backdoor: (i) using only low tens of bits
of storage (latches/ﬂops etc.) (ii) with a very small number
of logic gates and (iii) without cycle level re-pipelining. This
assumption does not restrict the types of attacks allowed. How-
ever, we assume the attacker is clever enough to implement
the changes in this way. This assumption ensures that the ma-
licious designer can slip in the hardware backdoor unnoticed
past traditional audit methods with very high probability.
• Assumption #4: Triggers Although an unintentional bug can
have the same consequences as a malicious backdoor, a critical
difference is that unlike a bug, a backdoor may not be always
active. If the backdoor is always active, there is a high chance
of detection during random, unit-level design testing. To avoid
detection, the malicious designer is likely to carefully control
when the backdoor is triggered.
• Assumption #5: ROMs We assume that ROMs written
during the microprocessor design phase contain correct data. In
particular, we assume that microcoded information is correct.
The reason for this assumption is that the data in ROMs is
statically determined and not altered by the processor’s state.
For this reason, we consider this security issue to be better
solved statically than at runtime.
B. Attack Triggers
An RTL level attacker can use two general strategies for
triggering an attack: a time-based trigger or a data-based
trigger. From the RTL perspective, input data and the passage
of time are the only factors determining the state of the
microprocessor (any attack using environmental factors would
be a side-channel attack; we are concerned with attacks
using digital input signals), so these two strategies or some
combination of them are the only ones possible.
• Trigger #1: Cheat Codes (CC) A malicious designer can
use a sequence of uncommon bits, embedded in either the
instruction or data stream to unlock/lock the backdoor. For
instance, a store instruction to a speciﬁc address and a certain
value (one pairing in a 2128 space for a 64-bit microprocessor)
can be used as a key to unlock a backdoor. Since the search
space is so large, the chance that this trigger is hit by random
veriﬁcation is negligible. King et al. describe a variant of
this attack in which a sequence of instructions in a program
unlocks a trigger. The CC method gives an attacker a very high
degree of control on the backdoor but may require a reasonably
sophisticated state machine to unlock the backdoor. Further, it
requires execution of software that may not be possible due
to access restrictions. This is due to the fact that in order to
ensure the ‘magic’ instruction(s) is issued, the attacker must
execute a program containing that instruction(s). If the attacker
cannot obtain access privileges, then this will not be possible.
• Trigger #2: Ticking Timebomb (TT) An attacker can build
a circuit to turn on the backdoor after the machine has been
powered on for a certain number of cycles. The TT method is
very simple to implement in terms of hardware; for instance, a
simple 40-bit counter that increments once per processor clock
cycle can be used to open a backdoor after roughly 18 minutes
of uptime at 1 GHz. Unlike the CC method, TT triggers do not
require any special software to open the backdoor. However,
like CC triggers, TT triggers can easily escape detection during
design validation because random tests are typically not longer
than millions of cycles.
C. Backdoor Types
While the space of possible attacks is limited only by the
attacker’s creativity and access to the design, attacks can be
broadly classiﬁed into two categories, based on their runtime
characteristics. We observe that an attacker can either create
a hardware backdoor to do more (or less) work than the
uncompromised design would, or he/she can create a backdoor
to do the same amount of work (but work that is different
from that of an uncompromised unit). By work, we mean
the microarchitectural sub-operations or communications that
must be carried out for the execution of an instruction. This
is a complete, binary classiﬁcation.
• Emitter Backdoors (EB) An emitter backdoor in a mi-
croarchitectural unit explicitly sends a different number of
microarchitectural communication than an uncompromised
unit. An example of an emitter backdoor in a memory unit
is one that sends out loads or stores to a shadow address.
When this type of attack is triggered, each memory instruction,
upon accessing the cache subunit, sends out two or more
microarchitectural transactions to downstream memory units
in the hierarchy. Similar attacks can also be orchestrated for
southbridge (I/O control hub) components, such as DMA
and VGA controllers, or other third party IP, to exﬁltrate
conﬁdential data to unauthorized locations.
• Corrupter Backdoors (CB) In this type of attack,
the
attacker changes the results of a microarchitectural operation
without directly changing the number of microarchitectural
176
transactions. We consider two types of corrupter backdoors
— control corrupters and data corrupters.
A control corrupter backdoor alters the type or semantics
of an instruction in ﬂight in a way that changes the number of
microarchitectural transactions somewhere else on-chip (e.g.,
at a later cycle). These attacks are similar to emitter attacks,
except that instead of simply issuing an extra instruction, they
use some part of a legitimate instruction in order to change
the number of transactions happening on-chip. For example,
if a decode unit translates a no op instruction into a store
instruction, this will indirectly cause the cache unit to do
more work than it would in an untampered microprocessor.
However, this change will not manifest itself until a later cycle.
This is different from an emitter attack because the decode
unit does not insert any new transactions directly; it decodes
exactly the same number of instructions in the tampered and
untampered case, but the value it outputs in the tampered case
causes the cache unit to do more work a few cycles later.
Data corrupter backdoors alter only the data being used in
microarchitectural transactions, without in any way altering
the number of events happening on-chip during the life of the
instruction. Examples of this could include changing the value
being written to a register ﬁle or changing the address on a
store request. For instance, an instruction might be maliciously
decoded to turn an addition into a subtraction, causing the
ALU to produce a difference value instead of a sum value. 3
• Emitter vs. Corrupter Trade-offs From the attacker’s point
of view, emitter attacks are easy to implement. Emitter attacks,
such as shadow loads, have very low area and logic require-
ments. They also have the nice property (for the attacker)
that a user may not see any symptoms of hardware emitters
when using applications. This is because they can preserve
the original instruction stream. Often in prior work the term
‘backdoor’ actually means ‘emitter backdoor.’
Corrupter attacks, on the other hand, are more complicated
to design and harder to hide from the user. In fact, a control
corrupter attack requires strictly more logic than a similar
emitter attack because rather then simply sending a trigger, it
must hide the trigger within a live instruction (which involves
extra multiplexing or something equivalent). In these attacks,
rather than simply emitting bogus signals, the user’s own
instructions are altered to invoke the attack. Since the user’s
instructions are being altered, the attacker must have some
knowledge of the binaries being run to change the data without
tipping off the user. If the execution of the backdoor caused
the user’s program to crash, this would violate the secrecy of
the attack. Corrupter attacks also scale poorly with datapath
sizes, since they require decoding of user instructions. In the
case of multi-stage decoders, the backdoor itself may require
latches and execute over multiple cycles.
3Data corrupter backdoors can be used to change program ﬂow, for example
by changing a value in a register, thus changing the result of a future ‘branch-
if-equal’ instruction. However, each individual instruction will still do the
same amount of work as it should. The extra work will not occur until
the corrupt instruction has been committed. Thus each instruction considered
individually will appear to be doing the correct amount of work.
To summarize, the “biggest bang for the buck” for the
attacker is from ticking-timebomb-emitter attacks. They can
be implemented with very little logic, are not dependent on
software or instruction sequences, and can run to completion
unnoticed by users. In the following section, however, we
discuss strategies for defending against all types of backdoors
and triggers.
IV. PRINCIPLES FOR MICROPROCESSOR PROTECTION
We propose as a solution to the untrusted designer prob-
lem an on-chip monitoring system that recognizes malicious
behavior at runtime, regardless of the trigger or unit. Differ-
ent attacks require different defenses. As such, we present
our solution in four ﬂavors. We ﬁrst describe low overhead
solutions for emitter and control corrupter protection, called
TRUSTNET and DATAWATCH. We then describe how a form
of partial duplication, which we call ‘smart duplication’ can
be used against some data corrupters. For data corrupters not
protected by any of the above mechanisms, we recommend
full duplication. For this initial study, we discuss our solutions
in the context of simple microprocessors that do not re-order
instructions.
A. Emitter Backdoor Protection
Emitter backdoors by deﬁnition cause more (or less) mi-
croarchitectural transactions to occur in the corrupted unit
than the instruction speciﬁes. We designed the TRUSTNET
monitoring system to watch the microarchitectural transactions
in each unit and catch this class of attacks. Conceptually,
the system detects violations of deterministic communication
invariants between on-chip units, which are violated by emitter
backdoors.
Toward this end, we designed the prediction/reaction mon-
itor triangle, depicted in Figure 3. A triangle consists of
three different on-chip units - a predictor, a reactor, and a
target (monitored unit in Figure 3). The predictor unit sends
messages to the monitor, predicting events that should come
out of the target unit. If the reactor does not receive a predicted
event, or if the reactor receives an event
that was never
predicted, then the monitor throws an alarm.
The target unit is any unit on-chip. For one example, using
Figure 4 as reference, consider the decode unit (IDU) as a
(cid:5)(cid:22)(cid:24)(cid:28)(cid:27)(cid:1)(cid:16)(cid:29)(cid:16)(cid:22)(cid:27)