>     summary: Too many requests
>     description: There is more than average of 1 requests per second per replica for at least one application
```
我们可以看到，这个表达式几乎与我们在普罗米修斯的图形屏幕中使用的表达式相同。唯一不同的是我们设置的阈值`0.1`。因此，当副本每秒接收请求的速率超过`0.1`时，该警报会通知我们，该速率是在五分钟内计算的(`[5m]`)。正如您可能已经猜到的那样，`0.1`每秒的请求数太低，无法在生产中使用。然而，它将允许我们很容易地触发警报，并看到它在行动。
现在，让我们升级我们的图表，并打开普罗米修斯的警报屏幕。
```
 1  helm upgrade -i prometheus \
 2    stable/prometheus \
 3    --namespace metrics \
 4    --version 7.1.3 \
 5    --set server.ingress.hosts={$PROM_ADDR} \
 6    --set alertmanager.ingress.hosts={$AM_ADDR} \
 7    -f mon/prom-values-latency2.yml
 8
 9  open "http://$PROM_ADDR/alerts"
```
请刷新屏幕，直到出现`TooManyRequests`提醒。
![](img/84c0a44a-b5bd-4f17-a428-6f1513dec3d7.png)
Figure 3-19: Prometheus' alerts screen
接下来，我们将生成一些流量，这样我们就可以看到警报已经生成，并通过 Alertmanager 发送给 Slack。
```
 1  for i in {1..200}; do
 2      curl "http://$GD5_ADDR/demo/hello"
 3  done
 4
 5  open "http://$PROM_ADDR/alerts"
```
我们发送了 200 个请求，重新打开了普罗米修斯的警报屏幕。现在我们应该刷新屏幕，直到`TooManyRequests`警报变为红色。
一旦普罗米修斯发出警报，它就会被发送到警报器管理器，并从那里转发到 Slack。让我们确认一下。
```
 1  open "https://devops20.slack.com/messages/CD8QJA8DS/"
```
我们可以看到`Too many requests`通知，从而证明这个预警的流程是有效的。
![](img/499f9841-7cb1-4865-a6d1-b02dff0a4761.png)
Figure 3-20: Slack with alert messages
接下来，我们将跳转到与错误相关的度量。
# 关于错误相关问题的警报
我们应该始终意识到我们的应用或系统是否正在产生错误。然而，我们不能在第一次出现错误时就惊慌失措，因为这会产生太多的通知，最终我们很可能会忽略这些通知。
错误经常发生，许多错误是由自动修复的问题或我们无法控制的情况引起的。如果我们要对每一个错误都采取行动，我们需要一大群人 24/7 全天候工作，只解决通常不需要解决的问题。例如，因为代码在 500 范围内的单一响应而进入“恐慌”模式几乎肯定会产生永久性危机。相反，我们应该监控与请求总数相比的错误率，并且只有当它超过某个阈值时才做出反应。毕竟，如果错误持续存在，这个比率无疑会增加。另一方面，如果持续较低，则意味着该问题已由系统自动修复(例如，Kubernetes 从故障节点重新计划了 Pods)，或者这是一个不会重复的孤立案例。
我们的下一个任务是检索请求，并根据它们的状态来区分它们。如果我们能做到这一点，我们应该能计算出误差率。
我们将从产生一些流量开始。
```
 1  for i in {1..100}; do
 2      curl "http://$GD5_ADDR/demo/hello"
 3  done
 4
 5  open "http://$PROM_ADDR/graph"
```
我们发送了一百个请求，打开了普罗米修斯的图形屏幕。
让我们看看我们之前使用的`nginx_ingress_controller_requests`度量是否提供了请求的状态。
请键入下面的表达式，然后按“执行”按钮。
```
 1  nginx_ingress_controller_requests
```
我们可以看到普罗米修斯最近刮到的所有数据。如果我们更仔细地观察标签，我们可以看到，除此之外，还有`status`。我们可以用它来计算基于请求总数的错误百分比(例如，500 个范围)。
我们已经看到，我们可以使用`ingress`标签来分隔每个应用的计算，假设我们只对面向公众的应用感兴趣。
![](img/adbe5b3c-eff0-4170-b518-20e9cb90f82d.png)
Figure 3-21: Prometheus' console view with requests entering through Ingress
`go-demo-5`应用有一个特殊的端点`/demo/random-error`，它将生成随机错误响应。大约十分之一的对该地址的请求会产生错误。我们可以用它来测试我们的表情。
```
 1  for i in {1..100}; do
 2    curl "http://$GD5_ADDR/demo/random-error"
 3  done
```
我们向`/demo/random-error`端点发送了 100 个请求，其中大约 10%的响应是错误的(HTTP 状态代码`500`)。
接下来，我们必须等待一会儿，让普罗米修斯刮出新的一批指标。之后，我们可以打开 Graph 屏幕，尝试编写一个表达式来检索应用的错误率。
```
 1  open "http://$PROM_ADDR/graph"
```
请键入下面的表达式，然后按“执行”按钮。
```
 1  sum(rate(
 2    nginx_ingress_controller_requests{
 3      status=~"5.."
 4    }[5m]
 5  ))
 6  by (ingress) /
 7  sum(rate(
 8    nginx_ingress_controller_requests[5m]
 9  ))
10  by (ingress)
```
我们使用`5..` RegEx 来计算有错误的请求的比率，按`ingress`分组，然后用所有请求的比率来划分结果。结果按`ingress`分组。在我的例子中(截图如下)，结果大约是 4%(`0.04`)。普罗米修斯还没有刮除所有的指标，我预计在下一次刮除迭代中，这个数字会接近 10%。
![](img/080bb7c6-42d1-486b-9b17-4b938b5c1624.png)
Figure 3-22: Prometheus' graph screen with the percentage with the requests with error responses
让我们将图表值文件的更新版本与之前使用的版本进行比较。
```
 1  diff mon/prom-values-cpu-memory.yml \
 2      mon/prom-values-errors.yml
```
输出如下。
```
127a128,136
> - name: errors
>   rules:
>   - alert: TooManyErrors
>     expr: sum(rate(nginx_ingress_controller_requests{status=~"5.."}[5m])) by (ingress) / sum(rate(nginx_ingress_controller_requests[5m])) by (ingress) > 0.025
>     labels:
>       severity: error
>     annotations:
>       summary: Too many errors
>       description: At least one application produced more then 5% of error responses
```
如果错误率超过总请求率的 2.5%，则会触发警报。
现在我们可以升级我们的普罗米修斯图表了。
```
 1  helm upgrade -i prometheus \
 2    stable/prometheus \
 3    --namespace metrics \
 4    --version 7.1.3 \
 5    --set server.ingress.hosts={$PROM_ADDR} \
 6    --set alertmanager.ingress.hosts={$AM_ADDR} \
 7    -f mon/prom-values-errors.yml
```
可能不需要确认警报工作。我们已经看到普罗米修斯将所有警报发送给警报管理器，并从那里转发给 Slack。
接下来，我们将讨论饱和指标和警报。
# 关于饱和相关问题的警报
饱和度衡量我们的服务和系统的完善程度。我们应该知道我们服务的副本是否处理了太多的请求，并被迫对其中一些请求进行排队。我们还应该监控 CPU、内存、磁盘和其他资源的使用是否达到了临界限制。
现在，我们将关注 CPU 的使用情况。我们将从打开普罗米修斯的图形屏幕开始。
```
 1  open "http://$PROM_ADDR/graph"
```
我们看看能否得到节点(`instance`)使用 CPU 的速率。我们可以使用`node_cpu_seconds_total`度量。然而，它被分成不同的模式，我们将不得不排除其中的一些模式来获得“真实”的 CPU 使用情况。这些将是`idle`、`iowait`和任何类型的`guest`循环。
请键入下面的表达式，然后按“执行”按钮。
```
 1  sum(rate(
 2    node_cpu_seconds_total{
 3      mode!="idle", 
 4      mode!="iowait", 
 5      mode!~"^(?:guest.*)$"
 6   }[5m]
 7  ))
 8  by (instance)
```
切换到*图形*视图。
输出代表系统中 CPU 的实际使用情况。在我的例子中(截图如下)，除了临时峰值，所有节点使用的 CPU 时间都不到 100 毫秒。
该系统远未面临压力。
![](img/52dcc7f8-4b9f-4e7f-b176-b73855232337.png)
Figure 3-23: Prometheus' graph screen with the rate of used CPU grouped by node instances
正如你已经注意到的，绝对数字很少有用。我们应该尝试发现已用 CPU 的百分比。我们需要找出我们的节点有多少 CPU。我们可以通过计算指标的数量来做到这一点。每个中央处理器都有自己的数据条目，每个模式一个。如果我们把结果限制在单一模式(例如`system`)下，应该可以得到 CPU 的总数。
请键入下面的表达式，然后按“执行”按钮。
```
 1  count(
 2    node_cpu_seconds_total{
 3      mode="system"
 4    }
 5  )
```
在我的例子中(截图如下)，总共有六个内核。如果你用的是 GKE、EKS 或吉斯特的 AKS，你的可能也是 6 个。另一方面，如果您在 Docker for Desktop 或 minikube 中运行集群，结果应该是一个节点。
现在，我们可以将这两个查询结合起来，以获得已用 CPU 的百分比
请键入下面的表达式，然后按“执行”按钮。
```
 1  sum(rate(
 2    node_cpu_seconds_total{
 3      mode!="idle", 
 4      mode!="iowait",
 5      mode!~"^(?:guest.*)$"
 6    }[5m]
 7  )) /
 8  count(
 9    node_cpu_seconds_total{