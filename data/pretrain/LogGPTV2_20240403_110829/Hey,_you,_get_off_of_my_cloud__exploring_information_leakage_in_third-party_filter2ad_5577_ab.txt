where potential targets are located in the cloud and the
instance creation parameters needed to attempt establish-
ing co-residence of an adversarial instance. This will speed
up signiﬁcantly adversarial strategies for placing a malicious
VM on the same machine as a target. In the next section we
will treat the task of conﬁrming when successful co-residence
is achieved.
To map EC2, we begin with the hypothesis that diﬀerent
availability zones are likely to correspond to diﬀerent inter-
nal IP address ranges and the same may be true for instance
types as well. Thus, mapping the use of the EC2 internal
address space allows an adversary to determine which IP ad-
dresses correspond to which creation parameters. Moreover,
since EC2’s DNS service provides a means to map public IP
address to private IP address, an adversary might use such a
map to infer the instance type and availability zone of a tar-
get service — thereby dramatically reducing the number of
instances needed before a co-resident placement is achieved.
We evaluate this theory using two data sets: one created
by enumerating public EC2-based web servers using external
probes and translating responsive public IPs to internal IPs
(via DNS queries within the cloud), and another created by
launching a number of EC2 instances of varying types and
surveying the resulting IP address assigned.
To fully leverage the latter data, we present a heuristic
algorithm that helps label /24 preﬁxes with an estimate of
the availability zone and instance type of the included Inter-
nal IPs. These heuristics utilize several beneﬁcial features
of EC2’s addressing regime. The output of this process is a
map of the internal EC2 address space which allows one to
estimate the availability zone and instance type of any tar-
get public EC2 server. Next, we enumerate a set of public
EC2-based Web servers
Surveying public servers on EC2. Utilizing WHOIS
queries, we identiﬁed four distinct IP address preﬁxes, a /16,
/17, /18, and /19, as being associated with EC2. The last
three contained public IPs observed as assigned to EC2 in-
stances. We had not yet observed EC2 instances with public
IPs in the /16, and therefore did not include it in our sur-
vey. For the remaining IP addresses (57 344 IP addresses),
we performed a TCP connect probe on port 80. This re-
sulted in 11 315 responsive IPs. Of these 9 558 responded
(with some HTTP response) to a follow-up wget on port
80. We also performed a TCP port 443 scan of all 57 344 IP
addresses, which resulted in 8 375 responsive IPs. Via an ap-
propriate DNS lookup from within EC2, we translated each
public IP address that responded to either the port 80 or
port 443 scan into an internal EC2 address. This resulted in
a list of 14 054 unique internal IPs. One of the goals of this
section is to enable identiﬁcation of the instance type and
availability zone of one or more of these potential targets.
2014
6
d
o
m
s
s
e
r
d
d
a
P
I
A
t
n
u
o
c
c
A
B
t
n
u
o
c
c
A
Zone 1
Zone 2
Zone 3
10.249.0.0
10.250.0.0
10.251.0.0
10.252.0.0
10.253.0.0
10.254.0.0
10.255.0.0
Internal IP address
c1.medium
c1.xlarge
m1.large
m1.small
m1.xlarge
64
32
0
64
32
0
64
32
0
10.252.0.0
10.253.0.0
Internal IP address
10.254.0.0
Figure 1: (Top) A plot of the internal IP addresses assigned to instances launched during the initial mapping exper-
iment using Account A. (Bottom) A plot of the internal IP addresses of instances launched in Zone 3 by Account A
and, 39 hours later, by Account B. Fifty-ﬁve of the Account B IPs were repeats of those assigned to instances for
Account A.
Instance placement parameters. Recall that there are
three availability zones and ﬁve instance types in the present
EC2 system. While these parameters could be assigned in-
dependently from the underlying infrastructure, in practice
this is not so. In particular, the Amazon EC2 internal IP ad-
dress space is cleanly partitioned between availability zones
(likely to make it easy to manage separate network con-
nectivity for these zones) and instance types within these
zones also show considerable regularity. Moreover, diﬀerent
accounts exhibit similar placement.
To establish these facts, we iteratively launched 20 in-
stances for each of the 15 availability zone/instance type
pairs. We used a single account, call it “Account A”. The top
graph in Figure 1 depicts a plot of the internal IP address
assigned to each of the 300 instances, partitioned according
to availability zone.
It can be readily seen that the sam-
ples from each zone are assigned IP addresses from disjoint
portions of the observed internal address space. For ex-
ample, samples from Zone 3 were assigned addresses within
10.252.0.0/16 and from discrete preﬁxes within 10.253.0.0/16.
If we make the assumption that internal IP addresses are
statically assigned to physical machines (doing otherwise
would make IP routing far more diﬃcult to implement), this
data supports the assessment that availability zones use sep-
arate physical infrastructure. Indeed, none of the data gath-
ered in the rest of the paper’s described experiments have
cast doubt on this conclusion.
While it is perhaps not surprising that availability zones
enjoy disjoint IP assignment, what about instance type and
accounts? We launched 100 instances (20 of each type, 39
hours after terminating the Account A instances) in Zone
3 from a second account, “Account B”. The bottom graph
in Figure 1 plots the Zone 3 instances from Account A and
Account B, here using distinct labels for instance type. Of
the 100 Account A Zone 3 instances, 92 had unique /24
preﬁxes, while eight /24 preﬁxes each had two instances,
though of the same type. Of the 100 Account B instances,
88 had unique /24 preﬁxes, while six of the /24 preﬁxes had
two instances each. A single /24 had both an m1.large and
an m1.xlarge instance. No IP addresses were ever observed
being assigned to more than one instance type. Of the 100
Acount B IP’s, 55 were repeats of IP addresses assigned to
instances for Acount A.
A fuller map of EC2. We would like to infer the instance
type and availability zone of any public EC2 instance, but
our sampling data is relatively sparse. We could sample
more (and did), but to take full advantage of the sampling
data at hand we should take advantage of the signiﬁcant
regularity of the EC2 addressing regime. For example, the
above data suggests that /24 preﬁxes rarely have IPs as-
signed to distinct instance types. We utilized data from
4 499 instances launched under several accounts under our
control; these instances were also used in many of the exper-
iments described in the rest of the paper. These included
977 unique internal IPs and 611 unique Dom0 IPs associated
with these instances.
Using manual inspection of the resultant data, we derived
a set of heuristics to label /24 preﬁxes with both availability
zone and instance type:
• All IPs from a /16 are from the same availability zone.
• A /24 inherits any included sampled instance type. If there
are multiple instances with distinct types, then we label the
/24 with each distinct type (i.e., it is ambiguous).
• A /24 containing a Dom0 IP address only contains Dom0
IP addresses. We associate to this /24 the type of the
Dom0’s associated instance.
• All /24’s between two consecutive Dom0 /24’s inherit the
former’s associated type.
The last heuristic, which enables us to label /24’s that have
no included instance, is derived from the observation that
Dom0 IPs are consistently assigned a preﬁx that immedi-
ately precedes the instance IPs they are associated with.
(For example, 10.250.8.0/24 contained Dom0 IPs associated
202with m1.small
instances in preﬁxes 10.254.9.0/24 and
10.254.10.0/24.) There were 869 /24’s in the data, and ap-
plying the heuristics resulted in assigning a unique zone and
unique type to 723 of these; a unique zone and two types to
23 of these; and left 123 unlabeled. These last were due to
areas (such as the lower portion of 10.253.0.0/16) for which
we had no sampling data at all.
While the map might contain errors (for example, in areas
of low instance sample numbers), we have yet to encounter
an instance that contradicts the /24 labeling and we used
the map for many of the future experiments. For instance,
we applied it to a subset of the public servers derived from
our survey, those that responded to wget requests with an
HTTP 200 or 206. The resulting 6 057 servers were used as
stand-ins for targets in some of the experiments in Section 7.
Figure 7 in the appendix graphs the result of mapping these
servers.
Preventing cloud cartography. Providers likely have in-
centive to prevent cloud cartography for several reasons, be-
yond the use we outline here (that of exploiting placement
vulnerabilities). Namely, they might wish to hide their in-
frastructure and the amount of use it is enjoying by cus-
tomers. Several features of EC2 made cartography signif-
icantly easier. Paramount is that local IP addresses are
statically (at least over the observed period of time) as-
sociated to availability zone and instance type. Changing
this would likely make administration tasks more challeng-
ing (and costly) for providers. Also, using the map requires
translating a victim instance’s external IP to an internal
IP, and the provider might inhibit this by isolating each
account’s view of the internal IP address space (e.g. via
VLANs and bridging). Even so, this would only appear to
slow down our particular technique for locating an instance
in the LAN — one might instead use ping timing measure-
ments or traceroutes (both discuss more in the next section)
to help “triangulate” on a victim.
6. DETERMINING CO-RESIDENCE
Given a set of targets, the EC2 map from the previous
section educates choice of instance launch parameters for
attempting to achieve placement on the same physical ma-
chine. Recall that we refer to instances that are running
on the same physical machine as being co-resident. In this
section we describe several easy-to-implement co-residence
checks. Looking ahead, our eventual check of choice will be
to compare instances’ Dom0 IP addresses. We conﬁrm the
accuracy of this (and other) co-residence checks by exploit-
ing a hard-disk-based covert channel between EC2 instances.
Network-based co-residence checks. Using our expe-
rience running instances while mapping EC2 and inspect-
ing data collected about them, we identify several poten-
tial methods for checking if two instances are co-resident.
Namely, instances are likely co-resident if they have
(1) matching Dom0 IP address,
(2) small packet round-trip times, or
(3) numerically close internal IP addresses (e.g. within 7).
As mentioned, an instance’s network traﬃc’s ﬁrst hop is the
Dom0 privileged VM. An instance owner can determine its
Dom0 IP from the ﬁrst hop on any route out from the in-
stance. One can determine an uncontrolled instance’s Dom0
IP by performing a TCP SYN traceroute to it (on some open
port) from another instance and inspecting the last hop. For
the second test, we noticed that round-trip times (RTTs) re-
quired a “warm-up”: the ﬁrst reported RTT in any sequence
of probes was almost always an order of magnitude slower
than subsequent probes. Thus for this method we perform
10 probes and just discard the ﬁrst. The third check makes
use of the manner in which internal IP addresses appear to
be assigned by EC2. The same Dom0 IP will be shared by in-
stances with a contiguous sequence of internal IP addresses.
(Note that m1.small instances are reported by CPUID as
having two CPUs each with two cores and these EC2 in-
stance types are limited to 50% core usage, implying that
one such machine could handle eight instances.)
Veracity of the co-residence checks. We verify the cor-
rectness of our network-based co-residence checks using as
ground truth the ability to send messages over a cross-VM
covert channel. That is, if two instances (under our control)
can successfully transmit via the covert channel then they
are co-resident, otherwise not. If the checks above (which do
not require both instances to be under our control) have suf-
ﬁciently low false positive rates relative to this check, then
we can use them for inferring co-residence against arbitrary
victims. We utilized for this experiment a hard-disk-based
covert channel. At a very high level, the channel works as
follows. To send a one bit, the sender instance reads from
random locations on a shared disk volume. To send a zero
bit, the sender does nothing. The receiver times reading
from a ﬁxed location on the disk volume. Longer read times
mean a 1 is being set, shorter read times give a 0.
We performed the following experiment. Three EC2 ac-
counts were utilized: a control, a victim, and a probe. (The
“victim” and “probe” are arbitrary labels, since they were
both under our control.) All instances launched were of
type m1.small. Two instances were launched by the control
account in each of the three availability zones. Then 20 in-
stances on the victim account and 20 instances on the probe
account were launched, all in Zone 3. We determined the
Dom0 IPs of each instance. For each (ordered) pair (A, B)
of these 40 instances, if the Dom0 IPs passed (check 1) then
we had A probe B and each control to determine packet
RTTs and we also sent a 5-bit message from A to B over
the hard-drive covert channel.
We performed three independent trials. These generated,
in total, 31 pairs of instances for which the Dom0 IPs were
equal. The internal IP addresses of each pair were within 7
of each other. Of the 31 (potentially) co-resident instance
pairs, 12 were ‘repeats’ (a pair from a later round had the
same Dom0 as a pair from an earlier round).
The 31 pairs give 62 ordered pairs. The hard-drive covert
channel successfully sent a 5-bit message for 60 of these
pairs. The last two failed due to a single bit error each, and
we point out that these two failures were not for the same
pair of instances (i.e. sending a message in the reverse direc-
tion succeeded). The results of the RTT probes are shown
in Figure 2. The median RTT for co-resident instances was
signiﬁcantly smaller than those to any of the controls. The
RTTs to the controls in the same availability zone as the
probe (Zone 3) and victim instances were also noticeably
smaller than those to other zones.
Discussion. From this experiment we conclude an eﬀec-
tive false positive rate of zero for the Dom0 IP co-residence