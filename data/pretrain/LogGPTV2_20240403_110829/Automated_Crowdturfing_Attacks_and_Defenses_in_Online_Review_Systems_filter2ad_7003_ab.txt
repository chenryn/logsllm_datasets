data. RNNs can learn from a large corpus of natural language text
(character or word sequences), to generate text at different levels
of granularity, i.e. at the character level or word level. We focus
on a character-level RNN due to its recent success on generating
high quality text [14, 28]. Also, the memory and computational cost
required to train a character-level RNN is lower than word-level
RNN since number of words is significantly larger than number of
valid characters in the English Language.
RNN Training. As mentioned before, traditional language models
(e.g., n-gram models) exhibit limited performance when trained
on long text sequences since they are able to look back only a
few steps of the sequence. RNN solves this problem by building a
more sophisticated “memory” model which maintains long term
information about what it has seen so far. In an RNN, “memory” is
a set of high dimensional weights (hidden states) learned during
the training stage to capture information about all characters seen
in the training sequence.
Figure 2 illustrates the training process of an RNN-based text
generation model. At each time step t, a new character xt is fed as
input to a memory unit of the RNN that maintains a hidden state ht,
and provides an output ot. Then it compares the current output ot
with the desired output, which is the next character in the text. The
error between them is computed and the hidden states are updated
towards the direction where the error is minimized. After multiple
iterations of updates, the hidden layer will eventually capture the
relationship between each input character and all characters prior
to it, i.e. the conditional probability distribution P (xt+1|(x1, . . . , xt)).
Text Sampling. After an RNN model is trained, text can be gener-
ated by feeding a character, say ˜x0, to the trained RNN. The RNN
returns a probability distribution that defines which characters are
likely to occur next, i.e. P ( ˜x1| ˜x0). We stochastically sample from
this distribution to obtain the next character ˜x1. Next, by feeding
AttackerHuman WriterAttackerAI ProgramTarget Char SequenceInput Char Sequence‘t’‘h’‘h’‘i’‘s’t = 2t = 3t = 4‘i’‘s’‘ ’‘ ’t = 5‘i’‘i’t = 6‘s’‘s’t = 7‘ ’t = 1Input LayerHidden LayerOutput LayerSession E4:  Adversarial Social Networking CCS’17, October 30-November 3, 2017, Dallas, TX, USA1145Figure 3: Overview of our attack methodology.
˜x1 back in to the RNN, we obtain another probability distribution
predicting the next character, i.e. P ( ˜x2| ˜x0, ˜x1). This process can be
repeated to generate continuous text ˜x0, ˜x1, ˜x2, . . . , ˜xN.
Temperature Control. An important parameter that we can ma-
nipulate during the sampling stage is temperature. Temperature is a
parameter used in the softmax function during the sampling stage
when converting the output vector ot to a probability distribution.
Formally:
P (xt|(x1, . . . , xt-1)) = softmax(ot)
(1)
each ot is a N-dimentional vector where N is the size of the character
vocabulary. The softmax function is defined as:
(cid:80)N
eok
t /T
j=1 eoj
t/T
P (softmax(ot) = k) =
(2)
Here, ok
t represents the component of the output corresponding
to the character class k (in the vocabulary), at time t, for a given
temperature, T .
Temperature controls the “novelty” of generated text. Tempera-
tures lower than 1 amplifies the difference in the sampling probabil-
ity for each character. In other words, this reduces the likelihood of
the RNN to pick characters with lower probabilities, in preference of
more common characters. As a result, this constrains the sampling,
and generates less diverse text, and more potentially repetitive
patterns. As temperature increases, the variation of sampling prob-
ability for each character diminishes, and the RNN will generate
more “novel” and diverse text. But along with diversity comes a
higher risk of mistakes (e.g., spelling errors, context inconsistency
errors etc.).
3 ATTACK METHODOLOGY AND SETUP
We focus our study on Yelp, the most popular site for collecting
and sharing crowdsourcing user reviews. Yelp’s review system is
representative of other review systems, e.g., Amazon or TripAdvi-
sor. In this section, we describe details of our attack methodology,
datasets and training setup.
3.1 Attack Methodology
Our attack methodology is illustrated in Figure 3. At a high level,
the attack consists of two main stages: (1) The first stage starts
by training a generative language model on a review corpus. The
language model is then used to generate a set of initial reviews. (2)
In the second stage, a customization component further modifies
these reviews to capture specific information about the target entity
(e.g., names of dishes in a seafood restaurant), and produces the
final targeted fake review. In our experiments, the customizable
content is extracted from a reference dataset, composed of existing
reviews associated with the target entity. If there are no existing
reviews, an attacker can build a reference dataset using reviews of
entities in the same category (e.g., seafood restaurants) as the target.
Restaurant category metadata is available on Yelp and similar sites,
and can be used to identify similar entities.
Generating Initial Reviews. First, the attacker chooses a training
dataset that matches the domain of the target entity. For example, to
generate reviews targeting restaurants, the attacker would choose a
dataset of restaurant reviews. Next, the attacker trains a generative
RNN model using the dataset. Afterwards, the attacker generates
review text using the sampling procedure in Section 2.4. Note that
the attacker is able to generate reviews at different temperatures.
Review Customization. In general, there is no control over the
topic or context (e.g., name of a food in a restaurant) generated
from the RNN model, since the text is stochastically sampled based
on the character distribution. To better target an entity (e.g., restau-
rant), we further capture the context by customizing the generated
reviews with domain-specific keywords. This is analogous to crowd-
sourced fake review markets, where workers are typically provided
additional information about the target entity for a writing task [59].
The information consists of specific nouns (e.g., names of dishes)
to be included in the written review. Based on this observation, we
propose an automated noun-level word replacement strategy.
Our method works by replacing specific words (nouns) in the
initial review with new words that better capture the context of the
target entity. This involves three main steps:
(1) Choose the type of contextual information to be captured. The
attacker first chooses a keyword C that helps to identify the
context. For example, if the attacker is targeting a restaurant,
the keyword can be “food,” which will capture the food-related
context. If the target is an online electronic accessories store,
then the keyword can be “accessory” or “electronics.”
(2) Identify words in reviews of the reference dataset that capture
context. Next, our method identifies all the nouns in the refer-
ence dataset that are relevant to the keyword C. Relevancy is
estimated by calculating lexical similarity using WordNet [44],
a widely used lexical database that groups English words into
sets of synonyms and measures their concept relatedness [51].
We identify a set of words p in the reference dataset that have
high lexical similarity with the keyword C, using a similarity
Target Review PlatformOnline Review CorpusRNN TrainingInitial Review GenerationTrained RNNUncustomized Review Review CustomizationExisting ReviewsCustomized Review Session E4:  Adversarial Social Networking CCS’17, October 30-November 3, 2017, Dallas, TX, USA1146Dataset
YelpBos [45]
YelpSF [45]
YelpZip [56]
YelpNYC [56]
YelpChi [48]
Total
Restaurants
# of
1,028
3,466
4,204
914
98
9,710
# of Fake
Reviews (%)
28,151 (22.12%)
90,777 (9.94%)
84,484 (13.76%)
37,799 (10.48%)
8,401 (12.83%)
249,612 (11.99%)
# of Real
Reviews (%)
99,117
822,772
529,569
322,858
57,061
1,831,377
Figure 4: Example of review customization.
threshold MINsim. The set of words p captures the context of
the target entity.
(3) Identify words in initial reviews for replacement. Finally, we find
all the nouns in the review set R that are also relevant to C using
the same method in Step 2. We replace them by stochastically
sampling words in p based on the lexical similarity score.
A detailed version of the above algorithm is available in Appen-
dix A. Figure 4 shows an example of customizing an initial review
that has language more suitable for a Japanese restaurant, to a
review more suitable for an Italian restaurant. The nouns to be
replaced in the initial review are marked in green, and replacement
nouns are marked in blue. Note that we choose this noun-level
replacement strategy because of its simplicity, and there is scope
for further improvement of this technique.
3.2 RNN Training and Text Generation
Training Process. For all experiments, we use a Long Short-Term
Memory (LSTM) model [16], an RNN variant that has shown better
performance in practice [22]. We examine multiple RNN training
configurations used in prior work [14, 33] and determine the best
configuration empirically through experiments. The neural network
we used contains 2 hidden layers, each with 1,024 hidden units. For
training, the input string is split into batches of size 256. Training
loss is computed using cross-entropy [12], and weights are updated
using Adam [26] optimization, a common optimization technique
for neural network training. The model is trained for 20 epochs and
the learning rate is set to be 2×10-3 and decays to half of the current
rate every time when the loss increases for 5 successive batches. We
also monitor training loss and inspect generated reviews to avoid
underfitting or overfitting.
We pre-processed the review text by removing all extra white
space and non-ASCII characters. Additionally, we separate the re-
views in the corpus by the delimiter tokens “” (start of review)
and “” (end of review), so the model also learns when to
start and end a review by generating these two tokens. The RNN is
trained using a machine with Intel Core i7 5930K CPU and a Nvidia
TITAN X GPU. The training takes ∼72 hours.
Text Generation. Once we have trained the language model, we
can sample the review text at different temperatures. We generate
reviews at 10 different temperatures between 0 and 1: [0.1, 0.2, ...,
Table 1: Summary of ground-truth dataset.
1]2. To start the text generation process, the model is seeded with
the start of review delimiter token. Conversely, the model identifies
the end of a review by generating the closing delimiter token.
For review customization, we chose the target keyword C to be
“food,” as a large number of nouns unsurprisingly relate to food. We
set the other parameter MINsim to 0.2. After customization, overall,
98.4% of the reviews have at least one word replaced. The reviews
not affected by the customization lack suitable content (or words)
that capture the context, including reviews that rarely mention any
food or dish in the text, e.g., “I love this place! Will be back again!!”
Generated Text Samples. Table 2 shows several examples of re-
views generated at different temperatures. At higher temperatures,
the RNN is more likely to generate novel content, while at lower
temperatures, the RNN produces repetitive patterns. We include
more examples of generated reviews in Appendix B.
3.3 Datasets
For our evaluation, we use different datasets of restaurant reviews
on Yelp. Each review in a dataset contains the text of the review, the
identity of the target restaurant and a desired rating score, ranging
from one to five stars. The rating score determines the sentiment
of the review and the desired textual content.
In the rest of the paper, we present results based on the genera-
tion of reviews tailored towards a five-star rating. This considers
the common scenario of an attack trying to improve the reputation
of a restaurant. Our attack methodology is general and would be
the same for other ratings as well. Appendix B presents examples of
generated reviews tailored towards one-star and three-star ratings.
Three disjoint datasets of Yelp reviews are used for generat-
ing and evaluating the attack: a training, ground-truth, and attack
dataset.
Training Datasets. We use the Yelp Challenge dataset to train the
RNN language model, containing a total of 4.1M reviews by 1M
reviewers, collectively targeting 144K businesses [79]. The dataset
covers restaurants in 11 cities, spread across 4 countries. We extract
reviews corresponding to different ratings, and found 617K reviews
with a five-star rating from 27K restaurants. In total, these five-star
reviews contain 57M words and 304M characters, a sufficiently
large dataset for training an RNN.
2We do not experiment with temperatures beyond 1.0, because the sampling distribu-
tion would significantly diverge from the true distribution learned from the training
corpus, and lead to overly diverse and incoherent text.
I love this place. I love their sushi. The salmon and ramen are also delicious. I will continue to come here anytime I am in town.Review 1: Easily my favorite Italian restaurant. I love the taster menu, everything is amazing on it. I suggest the carpaccio and the asparagus. Sadly it has become more widely known and becoming diffi-cult to get a reservation for prime times.Review 2: I come here every year during Chrismas and I absolutely love the pasta! Well worth the price!Review 3: Excellent pizza, lasagna and some of the best scallops I've had. The dessert was also exten-sive and fantastic.                                                                                 ....I love this place. I love their aspar-agus. The scallops and pasta are also delicious. I will continue to come here anytime I am in town.Initial ReviewCustomized Review Reviews From the Reference Dataset Session E4:  Adversarial Social Networking CCS’17, October 30-November 3, 2017, Dallas, TX, USA1147Ground-truth Dataset. This dataset, listed in Table 1, comprises of
multiple Yelp review datasets released by researchers. By providing
ground-truth information about existing fake and real reviews on
Yelp, this dataset enables us to build machine learning fake review
classifiers to evaluate our attack success (Section 4.1). Similar to
previous work [45, 48, 56], we treat Yelp filtered and unfiltered
reviews as ground-truth information for fake and real reviews. Yelp
attempts to filter reviews that are “fake, shill or malicious” [78],
but acknowledges imperfections in the accuracy of the filter [80].
However, given this is the best information currently available and
used by many prior studies on fake reviews, we use it to establish
ground-truth. In the rest of the paper, we use fake and real reviews
to refer to Yelp filtered and Yelp unfiltered reviews, respectively.
For each dataset, we only consider reviews targeting restaurants3.
The resulting dataset contains restaurants in NYC, Chicago, SF,
Boston, and several cities in NJ, VT, CT, and PA.
Attack Dataset. This dataset contains the reviews generated by
our RNN language model. We use the attack dataset to evaluate
attack performance (Section 4) and defense schemes (Section 5).
The datasets contain similar data to the ground-truth dataset,
except for replacing all fake reviews with our machine-generated
reviews. Using our RNN model, we generate reviews targeting each
restaurant in the ground-truth dataset using different temperatures.
For each temperature, we generate as many reviews as fake re-
views from Yelp for each restaurant, i.e. 249,612 machine-generated
reviews targeting 9,710 restaurants.
4 EVALUATING QUALITY OF
MACHINE-GENERATED REVIEWS