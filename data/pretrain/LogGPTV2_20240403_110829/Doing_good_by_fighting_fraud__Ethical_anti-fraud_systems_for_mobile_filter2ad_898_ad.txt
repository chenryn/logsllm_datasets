faster inference times and reﬁned system design for higher
utilization of the hardware.
1) Improvements in machine learning: The following two
key principles inform our machine learning re-design:
learning
optimization
(1) Optimize machine learning for resource-constrained
phones: Machine
resource-
constrained phones translates to well-provisioned phones as
well but the reverse is not true. Well provisioned phones
often employ hardware acceleration optimized for efﬁcient
machine learning inference. Having this hardware support
means that we can increase the capacity of machine learning
for
models either by adding more parameters or by breaking a
problem into sub-problems each executed with a separate
machine learning model. This has a sub-linear slow-down in
performance, leading to a better speed versus accuracy trade
off. However, resource-constrained phones do not possess
this luxury and adding parameters to the model has at least
a linear slow-down in performance (it was quadratic in our
case).
We thus create a uniﬁed model for OCR and reduce the
number of parameters by half. This leads to a quadratic speed
up on resource-constrained phones and close to a linear speed
up on well-provisioned phones as well. The new model also
occupies half the disk and memory space of the original model,
as an added beneﬁt to memory constrained devices.
In addition to the algorithmic improvements, using a single
model avoids expensive and complex processing to convert
the output of one model into the input of another, leading
to a more efﬁcient implementation with less code needed to
interpret the results.
(2) Optimize machine learning for the common case: Fol-
lowing our previous design principle of using a single model
for OCR implies that we are operating at half the machine
learning capacity as before leading to an inevitable tradeoff
between accuracy and speed. We observe that with a uniﬁed
model for OCR we need to add complex auxiliary layers at
multiple stages in the model to scan all payment card designs.
However, these auxiliary layers add parameters to the model as
well as increase the post processing complexity making them
prohibitively slow on resource-constrained devices.
We thus add native support in the model for the most
common designs and employ system design strategies to ac-
count for less common card designs. This ensures our machine
learning inference is efﬁcient for the common case employing
gated execution of more complex pipeline for less common
cases.
OCR model design: With the above two design principles,
we design and implement a new OCR model to work in a
single pass. Our new model draws on ideas from existing work
on Faster-RCNN [43], SSD [34] and Yolo [42].
We replace Boxer’s detection and recognition stages, which
were implemented using two separate models, with a single
network. The network reasons globally about the entire image
resulting in end-to-end training and faster inference. We im-
plement the model as a fully convolutional MobileNetV2 [45]
with auxiliary features for detection and recognition unifying
separate components of the detection and recognition into a
single network. We append these features to the network at
different layers to account for multi-sized feature maps, like
SSD [34]. This ﬂexibility gives us the ability to operate on
credit cards with varied font sizes.
Our OCR model operates on an input image size of 600x375
pixels, which is close to the aspect ratio of a credit card. As
with any CNN, the feature map shrinks in size and expands in
the depth dimension as the network processes the image. We
add auxiliary layers to the network at two places, one where
the feature map size is 38x24, and another where the feature
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:14:08 UTC from IEEE Xplore.  Restrictions apply. 
1630
map size is 19x12. We ﬁnd adding multi-layer predictions at
these two layers captures the vast majority of credit card fonts.
The activations corresponding to feature map of size 38x24
are useful for small and ﬂat font payment cards while the
activations corresponding to the feature map size 19x12 are
used for embossed cards that have bigger fonts.
At the output feature maps, each activation is responsible for
detecting a digit. To extract the card number from an image,
we need to localize and recognize individual digits. Knowing
the location and value of each digit in the input image aids
in post processing to remove false positives. Accordingly,
each activation in the two output feature maps is mapped
to a regression layer (for localization) and a classiﬁcation
layer (for recognition). We implement the regression layer
with anchor boxes like Faster-RCNN [43], where the possible
output locations are captured with multi-aspect-ratio bounding
boxes. Unlike Faster-RCNN which uses nine anchor boxes
per location, we only use three, since we ﬁnd this to be
sufﬁcient for OCR. We also ﬁne-tune our bounding box scales
for OCR; however, we defer these details to the open source
code we make available. To each output feature map activation,
we append a regression layer that consists of mapping each
input activation to 12 output activations, since we output three
bounding box proposals each containing four coordinates.
Each of these proposals (bounding boxes) can contain a
digit that the classiﬁcation layer detects. The classiﬁcation
layer maps each input activation to 33 output activations, 11
activations (background, 0 to 9) per bounding box.
During inference we apply standard post processing tech-
niques like non-max suppression [12] and heuristic based
reﬁning that is relevant to different credit card designs.
Our OCR model has difﬁculty in localizing small objects
precisely, much like Yolo [42] and SSD [34]. Since each
output activation is responsible for detecting a single digit, if
the corresponding receptive ﬁeld of a single activation spans
multiple digits, the model will only be able to detect a single
digit. In our experience, we found one credit card design
(Brex credit cards) that the model struggles to perform OCR
on. One way to ﬁx this corner case is to make the input
feature map size bigger or add auxiliary layers earlier in the
network where the feature map sizes are bigger. However,
this adds more computation to the machine learning inference
effectively decreasing the frame rates on resource-constrained
devices.
To successfully perform OCR on payment cards with tiny
fonts, we ﬁrst detect the ratio of the size of an individual digit
compared to the size of the input feature map. If it is below our
empirically determined threshold, we pass a zoomed in image
of the input through the machine learning pipeline effectively
mapping a card with small font to one with a relatively bigger
font that the model supports natively. This ﬂow adds latency to
our overall inference pipeline; however, Daredevil only needs
to trigger it sparingly.
We use 1 million real and synthetic card images to train our
OCR model. However, we ﬁnd that, on our internal benchmark
datasets,
is unable to reproduce Boxer OCR’s
this model
precision and recall, owing to the overall reduced number
of parameters. To account for this reduction, we generate an
additional 1.5 million synthetic credit card images. Ultimately,
we train the OCR model with 2.5 million real and synthetic
card images to match Boxer OCR’s baseline on our benchmark
datasets.
2) Improvements in system design: To further increase
the frame rate, we reﬁne our system design to use a pro-
ducer/consumer pipeline with a bounded buffer. We collect
multiple frames from the camera and run machine learning
inference on all of them in parallel. Since running machine
learning inference takes time, this design ensures that fetching
frames from the camera is not blocked, making the entire
system parallel from reading camera frames to completing
machine learning inference.
We ﬁnd that buffering images and running the same in-
ference in parallel leads to speedups of up to 117% for our
workload (see Appendix B for our results). Processing more
frames is critical for improving the end-to-end success rate
for complex machine learning problems that demand high
accuracy as concluded in our measurement study (see Section
III).
VI. EVALUATION
In our evaluation, we answer the following questions:
• Does Daredevil bridge the gap between low- and high-
• Does Daredevil prevent fraud in the wild while remaining
end devices?
ethical?
• What is Daredevil’s false positive rate when scanning real
cards and running anti-fraud models?
• Does our use of redundancy improve overall accuracy?
• What is the impact of back-end networks and data aug-
mentation on overall success rates?
A. Does Daredevil bridge the gap between low- and high-end
devices?
In this section, we measure Daredevil’s performance for its
most complex and carefully designed machine learning model:
OCR. Although OCR is a critical part of our fraud system (see
Section VI-B for real-world results of using Daredevil to stop
fraud), in this experiment we use OCR to help people add
credit and debit cards to an app more effectively by scanning
instead of typing in numbers.
1) Measurement Platform: To measure Daredevil’s perfor-
mance, we perform a correlation study by making it available
to third-party app developers and measuring the success rate
for the users of their live production apps. For Daredevil
Android SDK, we present results from anonymous statistics
sent by 70 apps that deploy our library from December 2019
to late November 2020. For Daredevil iOS SDK, we present
results from anonymous statistics sent by 44 apps that deploy
our library from late July 2020 to late November 2020.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:14:08 UTC from IEEE Xplore.  Restrictions apply. 
1631
Model
Size
Daredevil
Boxer
1.65MB
2.94MB
#
params.
861,242
1,528,919
# 2D
Conv(s)
# Depth-wise
Conv(s)
39
30
25
18
Fig. 8: Comparison of model parameters and architecture of
Daredevil (44% fewer parameters) and Boxer. Developers us-
ing architectures similar to these models for other applications
can expect to see similar frame rates.
2) Testbed: Daredevil Android SDK ran on a total of
477,594 Android devices spanning a total of 722 Android de-
vice types. This included 328,600 Samsung devices spanning
302 Samsung device types, 42,619 Huawei devices spanning
111 Huawei device types, 6,876 Xiaomi devices spanning 38
Xiaomi device types, 22,952 LG devices spanning 78 LG
device types, 31,699 Google devices spanning 17 Google
device types, 18,407 Motorola devices spanning 58 Motorola
device types, 8,751 OnePlus devices spanning 29 OnePlus
device types and tail of 17,690 devices, spanning 89 device
types and 28 vendors. Daredevil iOS SDK ran on a total of
1,102,666 iOS devices spanning a total of 28 iOS device types.
3) Task: As before, Daredevil prompts users to scan their
credit cards. The task, UI and the control ﬂow is identical to
the measurement study (Section III-D)
Daredevil consists of a single-stage OCR where both detec-
tion and recognition happen in a single pass. The input image
from the camera is processed and sent to the OCR model
which outputs a string of digits.
Daredevil uses a fully convolutional MobileNetV2 [45] with
auxiliary features for detection and recognition for OCR and
occupies 1.65MB on disk. The OCR model processes an input
image of size 600x375 and generates 51,300 output values
which are used to detect and localize the information for
extraction. It has a total of 861,242 parameters of which
830,362 are trainable. Daredevil uses 44% fewer parameters
than Boxer. Figure 8 shows a comparison of the model
parameters between Boxer and Daredevil.
We use the same inference engines (CoreML for iOS and
TFLite CPU for Android) for Daredevil as Boxer, detailed in
our measurement study Section III. Like Boxer, we quantize
all our models using 16-bit ﬂoating point weights.
4) Results- Key Performance Metrics: As before, we use
the same deﬁnitions for frame rate and success rate for our
performance metrics as in Section III.
We show the impact that the new Daredevil OCR model
(Section V-F1) has on the overall scanning success rate. Our
informal goal with Daredevil was to improve the success
rates on Android to match Boxer iOS. Daredevil uses algo-
rithmic machine learning improvements, empirical accuracy-
preserving optimizations, high ﬁdelity synthetic data, and an
improved system design to achieve this goal.
Figure 9 shows the results of Daredevil deployed on An-
droid (which we refer to as Daredevil Android) and iOS
(which we refer to as Daredevil iOS) against Boxer iOS.
iOS. Each point
Fig. 9: OCR success rate vs frame rate on Daredevil Android,
Boxer iOS and Daredevil
is the average
success rate and frame rate for a speciﬁc device type. This
ﬁgure shows that by improving our machine learning model
and increasing the frame rate we can achieve higher success
rates. The corresponding plot for Boxer Android is shown in
Figure 1.
Version
Count
Daredevil iOS
Boxer iOS
Daredevil Android
Boxer Android
1,102,666
3,175,912
477,594
329,272
Avg Suc
Avg
Rate
FPS
89.13% 20.00
88.60% 10.00
4.07
88.46%
46.72%
1.30
Avg
Dur (s)
9.37
10.02
10.55
15.45
Fig. 10: Comparison of Daredevil and Boxer. We see, Dare-
devil not only provides over 41% improvement in success rates
on Android but also improves iOS by close to 1%.
This ﬁgure shows that Daredevil’s improvements increase the
success rate on Android to closely match success rates on
Boxer iOS, despite the massive hardware advantages present
on iOS. Seeing the success of Daredevil Android, we ported
it to iOS and observed a more than 2x speedup in frame
rates and a moderate improvement in the success rates as well
(Figure 10). The increase in frame rates also lead to Daredevil
being able to support iPhone 6 and below, which Boxer does
not support.