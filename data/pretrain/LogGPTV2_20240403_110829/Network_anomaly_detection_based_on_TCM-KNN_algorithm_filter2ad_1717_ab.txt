point  in  relation  to  the  rest  of  the  points.  In  our  case  the 
strangeness measure for a point  i  with label  y  is defined as 
α
iy
=
y
ij
D
k
∑
j
1
=
k
D
∑
j
1
=
y
−
ij
(1) 
where k is the number of neighbors used. Thus, our measure for 
strangeness is the ratio of the sum of the k nearest distances from 
the  same  class  to  the  sum  of  the k nearest  distances  from  all 
other classes. This is a natural measure to use, as the strangeness 
of a point increases when the distance from the points of the same 
class becomes bigger or when the distance from the other classes 
becomes smaller [12]. 
Provided with the definition of strangeness, we will use equation 
(2) to compute the p-value as follows: 
}
i
:{#
(2) 
p
(
α
new
)
=
αα
i
n
≥
1
+
new
In  equation  (2),  #  denotes  the  cardinality  of  the  set,  which  is 
newα  is  the 
computed  as  the  number  of  elements  in  finite  set. 
strangeness  value  for  the  test  point  (assuming  there  is  only  one 
test point, or that the test points are processed one at a time), is a 
valid randomness test in the iid case. The proof takes advantage 
of the fact that since our distribution is iid, all permutations of a 
sequence  have  the  same  probability  of  occuring.  If  we  have  a 
newα  is  introduced, 
sequence
then 
newα  can take  any place  in  the  new  (sorted)  sequence  with 
the same probability, as all permutations of the new sequence are 
newα  is  among  the  j 
equiprobable.  Thus,  the  probability  that 
j
 and  a  new  element 
,{
mααα
1
largest occurs with probability of at most 
,...,
. 
}
2
1+n
2
,
}
new
,...,
,  where 
,{
m αααα
1
From the above discussions, we can see that the p-value  for the 
,...,
}
,{
sequence 
 are  the 
mααα
1
strangeness  measures  for  the  training  points  and 
newα  is  the 
strangeness  measure  of  a  new  test  point  with  a  possible 
p α . Figure 1 is the 
classification assigned to it, is the value 
(
classical  TCM-KNN  algorithm  based  on  the  above  strangeness 
and p-values. 
new
)
2
4.  ANOMALY DETECTION BASED ON 
IMPROVED TCM-KNN ALGORITHM 
Although  the  above  TCM-KNN  algorithm  is  good  for  detect 
intrusions according to the network flow because it is a problem 
of  classification  in  essence,  while  it’s  not  very  suitable  for  our 
anomaly detection for the following reasons: 
(i)  In  TCM-KNN,  we  are  always  sure  that  the  point  we  are 
examining belongs to one of the classes. However, in the field of 
anomaly detection, we need not assign a  point  constructed from 
the network traffic (a series of packets or a connection, etc.) to a 
15
c
}
y =
,...,3,2,1{
This definition has been firstly employed by authors in [10] as a 
measure  of  isolation  and  adopted  by  authors  in  [12]  to  detect 
outliers.  In  general  classification  cases,  using  the α values,  we 
can  compute  a  series  of  p-values  for  the  new  point  for  the 
classes
. We call the highest p-value in this series 
maxp
.  This  gives  us  a  way  of  testing  the  fitness  of  point γ for 
each  class y with  a  confidence  of  at  least 
δ −= 1
.  Selecting  a 
τ
τ≤maxp
confidence  level δ (usually  95%),  we  can  test  if 
,  in 
which case, we can declare the point an anomaly. Otherwise, we 
declare it’s normal. 
Specifically for our anomaly detection task, there are no classes 
available,  the  above  test  can  be  administered  to  the  data  as  a 
whole (they all belong to one class, i.e., the normal class). Doing 
that,  of  course,  requires  a  single 
iα  per  point  (as  opposed  to 
computing  one  per  class),  and  the τ used  directly  reflects  the 
confidence level δ that is required. Also,  maxp
 is just the p-value 
of point  i  to be diagnosed computed using all the normal training 
data. The process of our new simplified TCM-KNN algorithm for 
anomaly detection is depicted in Figure 2: 
let  i   as the examples to be determined (as anomaly or not) 
compute the  α  value of point  i   according to equation (3) 
compute the p-value of  i  
if ( τ≤p
) 
declare  i   an anomaly with confidence 1-τ . 
else 
declare that  i   is normal 
Figure 2. Improved TCM-KNN for anomaly detection 
It is worth noting that by using the modified α definition and the 
improved TCM-KNN algorithm, we only risk working with some 
value of α that is larger than it really ought to be. Moreover, the 
anomaly  detection  task  fulfilled  by  our  improved  TCM-KNN 
algorithm  is  not  equivalent  to  running  the  standard  TCM-KNN 
technique just having two classes (normal and abnormal) both in 
theory and practice. The former detects anomalies with only one 
class  (normal)  and  just  need  normal  network  traffic  data  for 
training,  while  the  latter  needs  two  classes  of  data  for  training, 
thus needs the additional workload for data labeling. In essence, 
the latter belongs to supervised misuse detection and the former 
belongs to unsupervised anomaly detection. 
Let  us  analyze  the  computational  complexity  of  our  algorithm. 
Since  the  method  requires  finding  k  nearest  neighbors  on  the 
 distance 
normal  dataset  for  every  test  point,  we  need 
computations, where  n  is the number of data points in the normal 
training  dataset  (Hence,  to  diagnose  s  points,  the  complexity 
would be
) Moreover, to find out the strangeness α for the 
comparisons. We find it 
normal training dataset, we require
is lucky that this step can be done off-line and only once before 
the  detection  of  anomalies  starts.  However,  if  n  and  the 
dimensions of attributes used to distance calculation is very large, 
(nsO
( 2nO
)(nO
)
)
the  off-line  computation  may  still  be  too  costly.  In  this  case,  to 
alleviate  the  complexity,  one  may  sample  the  normal  training 
dataset,  and  perform  comparisons  only  with  the  sampled  data. 
Moreover, reducing the dimensions of attributes used to calculate 
distances is also effective to avoid “curse of dimensionality”. The 
experiments to be discussed in the next section show that they are 
reasonable  methods  to  handle  large  datasets,  with  little  or  no 
significant deterioration of the results. 
5.  EXPERIMENTS AND DISCUSSIONS 
In order to verify the effectiveness of our TCM-KNN algorithm 
for anomaly detection, we take use of the well-known KDD Cup 
1999 Data (KDD 99) [13] to make experiments. 
The main reason we use the dataset is that we need relevant data 
that can easily be shared with other researchers, allowing all kinds 
of methods developed by authors all over the world to be easily 
compared  and  improved  in  the  same  baseline.  The  common 
practice  in  intrusion  detection  to  claim  good  performance  with 
“live  data”  makes  it  difficult  to  verify  and  improve  previous 
research results, as the traffic is never quantified or released for 
privacy concerns. The KDD 99 dataset might have been criticized 
for its problems [14, 15], but it is among the few comprehensive 
data sets that can be shared in intrusion detection nowadays. 
In the following series of experiments, firstly, we make contrast 
experiments  between  our  TCM-KNN  algorithm  and  the  cluster-
based  estimation  algorithm,  unsupervised  KNN  algorithm,  one-
class  SVM  algorithm  proposed  by  authors  in  [8]  using  the 
sampled dataset from KDD 99 since these methods are the most 
well-known  anomaly  detection  methods  in  the  past  decades. 
Secondly,  we  make  experiments  in  order  to  validate  the 
performance of TCM-KNN algorithm when we selected a feature 
subset  from  the  KDD  99  dataset  in  case  of  the  “curse  of 
dimensionality”. 
5.1  Experimental Dataset and Preprocess 
As  our  experimental  dataset,  KDD  99  dataset  contains  24 
different  types  of  attacks  (including  neptune,  land,  ipsweep, 
buffer overflow, etc.) that are broadly categorized in four groups 
such as Probes, DoS (Denial of Service), U2R (User to Root) and 
R2L  (Remote  to  Local).  The  packet  information  in  the  original 
tcpdump files were summarized into connections. This process is 
completed  using  the  Bro  IDS,  resulting  in  41  features  for  each 
connection.  Therefore,  each  instance  of  data  consists  of  41 
features  and  each  instance  of  them  can  be  directly  mapped  into 
the point discussed in TCM-KNN algorithm. 
Before  beginning  our  experiments,  we  preprocessed  the  dataset. 
Firstly, we normalized the dataset. In order to avoid one attribute 
will  dominate  another  for  the  numerical  data,  they  were 
normalized by replacing each attribute value with its distance to 
the mean of all the values for that attribute in the instance space. 
In order to do this, the mean and standard deviation vectors must 
be calculated: 
mean
j
][
1
∑=
i
1
=
1
n
ins
tan
ce
j
][
i
(4) 
s
tan
dard
j
][
=
1
−
1
n
∑
i
1
=
n
ins
(
tan
ce
i
j
][
−
mean
[
j
2])
   (5) 
16
From  this,  the  new  instances  can  be  calculated  by  dividing  the 
difference of the instances with the mean vector by the standard 
deviation vector: 
newins
tan
ce
j
][
=
ins
tan
s
ce
tan
j
][
−
dard
mean
j
][
j
][
  (6) 
This  results  in  rendering  all  numerical  attributes  comparable  to 
each  other  in  terms  of  their  deviation  from  the  norm,  which  is 
exactly what we are seeking in anomaly detection. For discrete or 
categorical  data,  we  represent  a  discrete  value  by  its  frequency. 
That  is,  discrete  values  of  similar  frequency  are  close  to  each 
other,  but  values  of  very  different  frequency  are  far  apart.  As  a 
result, discrete attributes are transformed to continuous attributes. 
Moreover,  the  experiments  employed  Euclidean  distance  metric 
to  evaluate  the  distance  between  two  points.  Although  there  are 
many  other  distance  metrics  available,  such  as  Minkowsky, 
Mahalanobis,  Camberra,  Chebychev,  we  adopt  the  commonly 
used Euclidean distance metric only for convenience. We take it 
for granted that selecting anyone of them for experiments might 
have  little  effect  on  evaluating  the  experimental  results  and  we 
will adopt the others in our future work for distance calculations. 
The Euclidean distance metric  is  defined  as  equation  (7),  where 
1Y  and 
ijY  denotes the jth component 
of 
2Y are two feature vectors, 
iY denotes the length of vector 
iY  and 
iY : 
|
|
dis
tan
YYce
,
(
1
)
=
2